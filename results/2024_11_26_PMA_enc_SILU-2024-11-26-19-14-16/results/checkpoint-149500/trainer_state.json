{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 7.973333333333334,
  "eval_steps": 500,
  "global_step": 149500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0005333333333333334,
      "grad_norm": 1.476143479347229,
      "learning_rate": 4.999666666666667e-05,
      "loss": 0.0125,
      "step": 10
    },
    {
      "epoch": 0.0010666666666666667,
      "grad_norm": 1.368275761604309,
      "learning_rate": 4.9993333333333335e-05,
      "loss": 0.0092,
      "step": 20
    },
    {
      "epoch": 0.0016,
      "grad_norm": 1.1396312713623047,
      "learning_rate": 4.999e-05,
      "loss": 0.0084,
      "step": 30
    },
    {
      "epoch": 0.0021333333333333334,
      "grad_norm": 0.4466232359409332,
      "learning_rate": 4.9986666666666674e-05,
      "loss": 0.0078,
      "step": 40
    },
    {
      "epoch": 0.0026666666666666666,
      "grad_norm": 0.6028664708137512,
      "learning_rate": 4.998333333333334e-05,
      "loss": 0.0063,
      "step": 50
    },
    {
      "epoch": 0.0032,
      "grad_norm": 0.8841772079467773,
      "learning_rate": 4.9980000000000006e-05,
      "loss": 0.0094,
      "step": 60
    },
    {
      "epoch": 0.0037333333333333333,
      "grad_norm": 0.6075733304023743,
      "learning_rate": 4.997666666666667e-05,
      "loss": 0.008,
      "step": 70
    },
    {
      "epoch": 0.004266666666666667,
      "grad_norm": 0.6804388165473938,
      "learning_rate": 4.997333333333333e-05,
      "loss": 0.0076,
      "step": 80
    },
    {
      "epoch": 0.0048,
      "grad_norm": 0.38317161798477173,
      "learning_rate": 4.997e-05,
      "loss": 0.0081,
      "step": 90
    },
    {
      "epoch": 0.005333333333333333,
      "grad_norm": 0.373260498046875,
      "learning_rate": 4.996666666666667e-05,
      "loss": 0.0066,
      "step": 100
    },
    {
      "epoch": 0.005866666666666667,
      "grad_norm": 0.4473348557949066,
      "learning_rate": 4.996333333333334e-05,
      "loss": 0.0056,
      "step": 110
    },
    {
      "epoch": 0.0064,
      "grad_norm": 0.32975009083747864,
      "learning_rate": 4.996e-05,
      "loss": 0.0064,
      "step": 120
    },
    {
      "epoch": 0.006933333333333333,
      "grad_norm": 0.2162148356437683,
      "learning_rate": 4.995666666666667e-05,
      "loss": 0.0076,
      "step": 130
    },
    {
      "epoch": 0.007466666666666667,
      "grad_norm": 0.8168848156929016,
      "learning_rate": 4.9953333333333335e-05,
      "loss": 0.0063,
      "step": 140
    },
    {
      "epoch": 0.008,
      "grad_norm": 0.1223030835390091,
      "learning_rate": 4.995e-05,
      "loss": 0.0052,
      "step": 150
    },
    {
      "epoch": 0.008533333333333334,
      "grad_norm": 0.7527208924293518,
      "learning_rate": 4.994666666666667e-05,
      "loss": 0.0068,
      "step": 160
    },
    {
      "epoch": 0.009066666666666667,
      "grad_norm": 0.029485151171684265,
      "learning_rate": 4.9943333333333333e-05,
      "loss": 0.005,
      "step": 170
    },
    {
      "epoch": 0.0096,
      "grad_norm": 0.9273650646209717,
      "learning_rate": 4.9940000000000006e-05,
      "loss": 0.0088,
      "step": 180
    },
    {
      "epoch": 0.010133333333333333,
      "grad_norm": 0.897864043712616,
      "learning_rate": 4.993666666666667e-05,
      "loss": 0.0075,
      "step": 190
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 0.9899000525474548,
      "learning_rate": 4.993333333333334e-05,
      "loss": 0.0079,
      "step": 200
    },
    {
      "epoch": 0.0112,
      "grad_norm": 0.9741928577423096,
      "learning_rate": 4.9930000000000005e-05,
      "loss": 0.0064,
      "step": 210
    },
    {
      "epoch": 0.011733333333333333,
      "grad_norm": 0.6117143630981445,
      "learning_rate": 4.992666666666667e-05,
      "loss": 0.0056,
      "step": 220
    },
    {
      "epoch": 0.012266666666666667,
      "grad_norm": 0.08639712631702423,
      "learning_rate": 4.992333333333333e-05,
      "loss": 0.0064,
      "step": 230
    },
    {
      "epoch": 0.0128,
      "grad_norm": 0.6924428343772888,
      "learning_rate": 4.992e-05,
      "loss": 0.0073,
      "step": 240
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 0.10285817086696625,
      "learning_rate": 4.991666666666667e-05,
      "loss": 0.0053,
      "step": 250
    },
    {
      "epoch": 0.013866666666666666,
      "grad_norm": 1.1083195209503174,
      "learning_rate": 4.9913333333333335e-05,
      "loss": 0.0068,
      "step": 260
    },
    {
      "epoch": 0.0144,
      "grad_norm": 0.9638701677322388,
      "learning_rate": 4.991e-05,
      "loss": 0.0064,
      "step": 270
    },
    {
      "epoch": 0.014933333333333333,
      "grad_norm": 0.23662294447422028,
      "learning_rate": 4.990666666666667e-05,
      "loss": 0.0058,
      "step": 280
    },
    {
      "epoch": 0.015466666666666667,
      "grad_norm": 0.09258203208446503,
      "learning_rate": 4.9903333333333334e-05,
      "loss": 0.007,
      "step": 290
    },
    {
      "epoch": 0.016,
      "grad_norm": 0.823832094669342,
      "learning_rate": 4.99e-05,
      "loss": 0.0058,
      "step": 300
    },
    {
      "epoch": 0.016533333333333334,
      "grad_norm": 0.2221965193748474,
      "learning_rate": 4.9896666666666666e-05,
      "loss": 0.0056,
      "step": 310
    },
    {
      "epoch": 0.017066666666666667,
      "grad_norm": 0.24376870691776276,
      "learning_rate": 4.989333333333334e-05,
      "loss": 0.0065,
      "step": 320
    },
    {
      "epoch": 0.0176,
      "grad_norm": 0.31491947174072266,
      "learning_rate": 4.9890000000000005e-05,
      "loss": 0.0069,
      "step": 330
    },
    {
      "epoch": 0.018133333333333335,
      "grad_norm": 0.22788725793361664,
      "learning_rate": 4.988666666666667e-05,
      "loss": 0.0063,
      "step": 340
    },
    {
      "epoch": 0.018666666666666668,
      "grad_norm": 0.1645050048828125,
      "learning_rate": 4.988333333333334e-05,
      "loss": 0.0071,
      "step": 350
    },
    {
      "epoch": 0.0192,
      "grad_norm": 0.2054099142551422,
      "learning_rate": 4.9880000000000004e-05,
      "loss": 0.0077,
      "step": 360
    },
    {
      "epoch": 0.019733333333333332,
      "grad_norm": 0.15182942152023315,
      "learning_rate": 4.987666666666667e-05,
      "loss": 0.0069,
      "step": 370
    },
    {
      "epoch": 0.020266666666666665,
      "grad_norm": 0.1024089902639389,
      "learning_rate": 4.9873333333333336e-05,
      "loss": 0.0067,
      "step": 380
    },
    {
      "epoch": 0.0208,
      "grad_norm": 0.6682753562927246,
      "learning_rate": 4.987e-05,
      "loss": 0.0068,
      "step": 390
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 0.08330714702606201,
      "learning_rate": 4.986666666666667e-05,
      "loss": 0.0066,
      "step": 400
    },
    {
      "epoch": 0.021866666666666666,
      "grad_norm": 0.15612760186195374,
      "learning_rate": 4.9863333333333334e-05,
      "loss": 0.0047,
      "step": 410
    },
    {
      "epoch": 0.0224,
      "grad_norm": 0.9685434699058533,
      "learning_rate": 4.986e-05,
      "loss": 0.0061,
      "step": 420
    },
    {
      "epoch": 0.022933333333333333,
      "grad_norm": 0.9496760964393616,
      "learning_rate": 4.9856666666666666e-05,
      "loss": 0.0063,
      "step": 430
    },
    {
      "epoch": 0.023466666666666667,
      "grad_norm": 0.12987658381462097,
      "learning_rate": 4.985333333333333e-05,
      "loss": 0.0055,
      "step": 440
    },
    {
      "epoch": 0.024,
      "grad_norm": 0.16176359355449677,
      "learning_rate": 4.9850000000000006e-05,
      "loss": 0.0053,
      "step": 450
    },
    {
      "epoch": 0.024533333333333334,
      "grad_norm": 0.31152331829071045,
      "learning_rate": 4.984666666666667e-05,
      "loss": 0.005,
      "step": 460
    },
    {
      "epoch": 0.025066666666666668,
      "grad_norm": 0.22309722006320953,
      "learning_rate": 4.984333333333334e-05,
      "loss": 0.0052,
      "step": 470
    },
    {
      "epoch": 0.0256,
      "grad_norm": 0.1691809445619583,
      "learning_rate": 4.9840000000000004e-05,
      "loss": 0.0062,
      "step": 480
    },
    {
      "epoch": 0.026133333333333335,
      "grad_norm": 0.4304395914077759,
      "learning_rate": 4.983666666666667e-05,
      "loss": 0.006,
      "step": 490
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.29669642448425293,
      "learning_rate": 4.9833333333333336e-05,
      "loss": 0.0068,
      "step": 500
    },
    {
      "epoch": 0.0272,
      "grad_norm": 0.14201587438583374,
      "learning_rate": 4.983e-05,
      "loss": 0.0072,
      "step": 510
    },
    {
      "epoch": 0.027733333333333332,
      "grad_norm": 0.84170001745224,
      "learning_rate": 4.982666666666667e-05,
      "loss": 0.0058,
      "step": 520
    },
    {
      "epoch": 0.028266666666666666,
      "grad_norm": 0.31172993779182434,
      "learning_rate": 4.9823333333333335e-05,
      "loss": 0.0056,
      "step": 530
    },
    {
      "epoch": 0.0288,
      "grad_norm": 0.8075461387634277,
      "learning_rate": 4.982e-05,
      "loss": 0.0077,
      "step": 540
    },
    {
      "epoch": 0.029333333333333333,
      "grad_norm": 0.4960422217845917,
      "learning_rate": 4.981666666666667e-05,
      "loss": 0.0071,
      "step": 550
    },
    {
      "epoch": 0.029866666666666666,
      "grad_norm": 0.3742765486240387,
      "learning_rate": 4.981333333333333e-05,
      "loss": 0.0066,
      "step": 560
    },
    {
      "epoch": 0.0304,
      "grad_norm": 0.16044043004512787,
      "learning_rate": 4.981e-05,
      "loss": 0.0072,
      "step": 570
    },
    {
      "epoch": 0.030933333333333334,
      "grad_norm": 1.0152332782745361,
      "learning_rate": 4.9806666666666665e-05,
      "loss": 0.007,
      "step": 580
    },
    {
      "epoch": 0.031466666666666664,
      "grad_norm": 0.9686554074287415,
      "learning_rate": 4.980333333333334e-05,
      "loss": 0.0056,
      "step": 590
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.1005965992808342,
      "learning_rate": 4.9800000000000004e-05,
      "loss": 0.0063,
      "step": 600
    },
    {
      "epoch": 0.03253333333333333,
      "grad_norm": 0.362718790769577,
      "learning_rate": 4.979666666666667e-05,
      "loss": 0.0074,
      "step": 610
    },
    {
      "epoch": 0.03306666666666667,
      "grad_norm": 0.898283839225769,
      "learning_rate": 4.9793333333333337e-05,
      "loss": 0.0067,
      "step": 620
    },
    {
      "epoch": 0.0336,
      "grad_norm": 0.22694821655750275,
      "learning_rate": 4.979e-05,
      "loss": 0.0071,
      "step": 630
    },
    {
      "epoch": 0.034133333333333335,
      "grad_norm": 1.0890779495239258,
      "learning_rate": 4.978666666666667e-05,
      "loss": 0.0067,
      "step": 640
    },
    {
      "epoch": 0.034666666666666665,
      "grad_norm": 0.35850790143013,
      "learning_rate": 4.9783333333333335e-05,
      "loss": 0.0075,
      "step": 650
    },
    {
      "epoch": 0.0352,
      "grad_norm": 0.23166730999946594,
      "learning_rate": 4.978e-05,
      "loss": 0.0059,
      "step": 660
    },
    {
      "epoch": 0.03573333333333333,
      "grad_norm": 0.7122400403022766,
      "learning_rate": 4.9776666666666674e-05,
      "loss": 0.006,
      "step": 670
    },
    {
      "epoch": 0.03626666666666667,
      "grad_norm": 1.1783957481384277,
      "learning_rate": 4.977333333333334e-05,
      "loss": 0.0054,
      "step": 680
    },
    {
      "epoch": 0.0368,
      "grad_norm": 1.0338377952575684,
      "learning_rate": 4.977e-05,
      "loss": 0.0081,
      "step": 690
    },
    {
      "epoch": 0.037333333333333336,
      "grad_norm": 0.37019696831703186,
      "learning_rate": 4.9766666666666666e-05,
      "loss": 0.0076,
      "step": 700
    },
    {
      "epoch": 0.037866666666666667,
      "grad_norm": 0.1478995680809021,
      "learning_rate": 4.976333333333333e-05,
      "loss": 0.0077,
      "step": 710
    },
    {
      "epoch": 0.0384,
      "grad_norm": 0.3834427297115326,
      "learning_rate": 4.976e-05,
      "loss": 0.0043,
      "step": 720
    },
    {
      "epoch": 0.038933333333333334,
      "grad_norm": 0.5721923112869263,
      "learning_rate": 4.975666666666667e-05,
      "loss": 0.0074,
      "step": 730
    },
    {
      "epoch": 0.039466666666666664,
      "grad_norm": 0.1345960646867752,
      "learning_rate": 4.975333333333334e-05,
      "loss": 0.0046,
      "step": 740
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.33318623900413513,
      "learning_rate": 4.975e-05,
      "loss": 0.0056,
      "step": 750
    },
    {
      "epoch": 0.04053333333333333,
      "grad_norm": 0.4841175079345703,
      "learning_rate": 4.974666666666667e-05,
      "loss": 0.0056,
      "step": 760
    },
    {
      "epoch": 0.04106666666666667,
      "grad_norm": 0.4509740471839905,
      "learning_rate": 4.9743333333333335e-05,
      "loss": 0.0054,
      "step": 770
    },
    {
      "epoch": 0.0416,
      "grad_norm": 0.16153639554977417,
      "learning_rate": 4.974e-05,
      "loss": 0.0077,
      "step": 780
    },
    {
      "epoch": 0.042133333333333335,
      "grad_norm": 0.6392838954925537,
      "learning_rate": 4.973666666666667e-05,
      "loss": 0.0059,
      "step": 790
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 0.14666414260864258,
      "learning_rate": 4.973333333333334e-05,
      "loss": 0.0058,
      "step": 800
    },
    {
      "epoch": 0.0432,
      "grad_norm": 0.09886462986469269,
      "learning_rate": 4.973000000000001e-05,
      "loss": 0.009,
      "step": 810
    },
    {
      "epoch": 0.04373333333333333,
      "grad_norm": 0.2899567484855652,
      "learning_rate": 4.972666666666667e-05,
      "loss": 0.0064,
      "step": 820
    },
    {
      "epoch": 0.04426666666666667,
      "grad_norm": 0.36911898851394653,
      "learning_rate": 4.972333333333334e-05,
      "loss": 0.0047,
      "step": 830
    },
    {
      "epoch": 0.0448,
      "grad_norm": 0.23877155780792236,
      "learning_rate": 4.972e-05,
      "loss": 0.006,
      "step": 840
    },
    {
      "epoch": 0.04533333333333334,
      "grad_norm": 0.5807716250419617,
      "learning_rate": 4.9716666666666664e-05,
      "loss": 0.0087,
      "step": 850
    },
    {
      "epoch": 0.04586666666666667,
      "grad_norm": 0.606406569480896,
      "learning_rate": 4.971333333333334e-05,
      "loss": 0.0081,
      "step": 860
    },
    {
      "epoch": 0.0464,
      "grad_norm": 0.667361319065094,
      "learning_rate": 4.9710000000000003e-05,
      "loss": 0.0044,
      "step": 870
    },
    {
      "epoch": 0.046933333333333334,
      "grad_norm": 0.6448737382888794,
      "learning_rate": 4.970666666666667e-05,
      "loss": 0.0067,
      "step": 880
    },
    {
      "epoch": 0.047466666666666664,
      "grad_norm": 1.1069579124450684,
      "learning_rate": 4.9703333333333336e-05,
      "loss": 0.0058,
      "step": 890
    },
    {
      "epoch": 0.048,
      "grad_norm": 1.170319676399231,
      "learning_rate": 4.97e-05,
      "loss": 0.0066,
      "step": 900
    },
    {
      "epoch": 0.04853333333333333,
      "grad_norm": 0.24698221683502197,
      "learning_rate": 4.969666666666667e-05,
      "loss": 0.0047,
      "step": 910
    },
    {
      "epoch": 0.04906666666666667,
      "grad_norm": 0.15038159489631653,
      "learning_rate": 4.9693333333333334e-05,
      "loss": 0.0081,
      "step": 920
    },
    {
      "epoch": 0.0496,
      "grad_norm": 0.07572327554225922,
      "learning_rate": 4.969e-05,
      "loss": 0.0061,
      "step": 930
    },
    {
      "epoch": 0.050133333333333335,
      "grad_norm": 0.10123823583126068,
      "learning_rate": 4.968666666666667e-05,
      "loss": 0.0064,
      "step": 940
    },
    {
      "epoch": 0.050666666666666665,
      "grad_norm": 0.09055168926715851,
      "learning_rate": 4.968333333333334e-05,
      "loss": 0.0046,
      "step": 950
    },
    {
      "epoch": 0.0512,
      "grad_norm": 0.15032480657100677,
      "learning_rate": 4.9680000000000005e-05,
      "loss": 0.0059,
      "step": 960
    },
    {
      "epoch": 0.05173333333333333,
      "grad_norm": 0.1972033828496933,
      "learning_rate": 4.967666666666667e-05,
      "loss": 0.0063,
      "step": 970
    },
    {
      "epoch": 0.05226666666666667,
      "grad_norm": 0.14447100460529327,
      "learning_rate": 4.967333333333334e-05,
      "loss": 0.006,
      "step": 980
    },
    {
      "epoch": 0.0528,
      "grad_norm": 0.642058253288269,
      "learning_rate": 4.967e-05,
      "loss": 0.0053,
      "step": 990
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.22240008413791656,
      "learning_rate": 4.966666666666667e-05,
      "loss": 0.0076,
      "step": 1000
    },
    {
      "epoch": 0.05386666666666667,
      "grad_norm": 0.7249307036399841,
      "learning_rate": 4.9663333333333336e-05,
      "loss": 0.0059,
      "step": 1010
    },
    {
      "epoch": 0.0544,
      "grad_norm": 0.22169074416160583,
      "learning_rate": 4.966e-05,
      "loss": 0.0079,
      "step": 1020
    },
    {
      "epoch": 0.054933333333333334,
      "grad_norm": 0.6844766139984131,
      "learning_rate": 4.965666666666667e-05,
      "loss": 0.0058,
      "step": 1030
    },
    {
      "epoch": 0.055466666666666664,
      "grad_norm": 0.524782121181488,
      "learning_rate": 4.9653333333333335e-05,
      "loss": 0.0066,
      "step": 1040
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.45274123549461365,
      "learning_rate": 4.965e-05,
      "loss": 0.0051,
      "step": 1050
    },
    {
      "epoch": 0.05653333333333333,
      "grad_norm": 0.1988655924797058,
      "learning_rate": 4.964666666666667e-05,
      "loss": 0.0067,
      "step": 1060
    },
    {
      "epoch": 0.05706666666666667,
      "grad_norm": 0.4999016225337982,
      "learning_rate": 4.964333333333333e-05,
      "loss": 0.0048,
      "step": 1070
    },
    {
      "epoch": 0.0576,
      "grad_norm": 0.0818166732788086,
      "learning_rate": 4.9640000000000006e-05,
      "loss": 0.0049,
      "step": 1080
    },
    {
      "epoch": 0.058133333333333335,
      "grad_norm": 0.6665641069412231,
      "learning_rate": 4.963666666666667e-05,
      "loss": 0.0076,
      "step": 1090
    },
    {
      "epoch": 0.058666666666666666,
      "grad_norm": 0.999785304069519,
      "learning_rate": 4.963333333333334e-05,
      "loss": 0.0051,
      "step": 1100
    },
    {
      "epoch": 0.0592,
      "grad_norm": 0.3154140114784241,
      "learning_rate": 4.9630000000000004e-05,
      "loss": 0.0054,
      "step": 1110
    },
    {
      "epoch": 0.05973333333333333,
      "grad_norm": 0.30551138520240784,
      "learning_rate": 4.962666666666667e-05,
      "loss": 0.0065,
      "step": 1120
    },
    {
      "epoch": 0.06026666666666667,
      "grad_norm": 0.42748406529426575,
      "learning_rate": 4.9623333333333337e-05,
      "loss": 0.0047,
      "step": 1130
    },
    {
      "epoch": 0.0608,
      "grad_norm": 0.26889997720718384,
      "learning_rate": 4.962e-05,
      "loss": 0.0048,
      "step": 1140
    },
    {
      "epoch": 0.06133333333333333,
      "grad_norm": 0.6489999294281006,
      "learning_rate": 4.961666666666667e-05,
      "loss": 0.0047,
      "step": 1150
    },
    {
      "epoch": 0.06186666666666667,
      "grad_norm": 0.9420555830001831,
      "learning_rate": 4.9613333333333335e-05,
      "loss": 0.0069,
      "step": 1160
    },
    {
      "epoch": 0.0624,
      "grad_norm": 0.2295909821987152,
      "learning_rate": 4.961e-05,
      "loss": 0.0058,
      "step": 1170
    },
    {
      "epoch": 0.06293333333333333,
      "grad_norm": 0.301768958568573,
      "learning_rate": 4.960666666666667e-05,
      "loss": 0.0048,
      "step": 1180
    },
    {
      "epoch": 0.06346666666666667,
      "grad_norm": 0.43597131967544556,
      "learning_rate": 4.960333333333333e-05,
      "loss": 0.0048,
      "step": 1190
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.25205162167549133,
      "learning_rate": 4.96e-05,
      "loss": 0.0062,
      "step": 1200
    },
    {
      "epoch": 0.06453333333333333,
      "grad_norm": 0.5729703903198242,
      "learning_rate": 4.959666666666667e-05,
      "loss": 0.0042,
      "step": 1210
    },
    {
      "epoch": 0.06506666666666666,
      "grad_norm": 0.7103671431541443,
      "learning_rate": 4.959333333333334e-05,
      "loss": 0.005,
      "step": 1220
    },
    {
      "epoch": 0.0656,
      "grad_norm": 0.2904715836048126,
      "learning_rate": 4.9590000000000005e-05,
      "loss": 0.0061,
      "step": 1230
    },
    {
      "epoch": 0.06613333333333334,
      "grad_norm": 1.172011375427246,
      "learning_rate": 4.958666666666667e-05,
      "loss": 0.0069,
      "step": 1240
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.2289654165506363,
      "learning_rate": 4.958333333333334e-05,
      "loss": 0.0043,
      "step": 1250
    },
    {
      "epoch": 0.0672,
      "grad_norm": 1.0125963687896729,
      "learning_rate": 4.958e-05,
      "loss": 0.006,
      "step": 1260
    },
    {
      "epoch": 0.06773333333333334,
      "grad_norm": 0.4299008846282959,
      "learning_rate": 4.957666666666667e-05,
      "loss": 0.0071,
      "step": 1270
    },
    {
      "epoch": 0.06826666666666667,
      "grad_norm": 0.3823699653148651,
      "learning_rate": 4.9573333333333335e-05,
      "loss": 0.0065,
      "step": 1280
    },
    {
      "epoch": 0.0688,
      "grad_norm": 0.14778254926204681,
      "learning_rate": 4.957e-05,
      "loss": 0.007,
      "step": 1290
    },
    {
      "epoch": 0.06933333333333333,
      "grad_norm": 0.6648220419883728,
      "learning_rate": 4.956666666666667e-05,
      "loss": 0.0067,
      "step": 1300
    },
    {
      "epoch": 0.06986666666666666,
      "grad_norm": 0.7993685007095337,
      "learning_rate": 4.9563333333333334e-05,
      "loss": 0.0056,
      "step": 1310
    },
    {
      "epoch": 0.0704,
      "grad_norm": 0.812652587890625,
      "learning_rate": 4.956e-05,
      "loss": 0.0061,
      "step": 1320
    },
    {
      "epoch": 0.07093333333333333,
      "grad_norm": 0.7139835953712463,
      "learning_rate": 4.9556666666666666e-05,
      "loss": 0.0034,
      "step": 1330
    },
    {
      "epoch": 0.07146666666666666,
      "grad_norm": 0.15650969743728638,
      "learning_rate": 4.955333333333333e-05,
      "loss": 0.006,
      "step": 1340
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.7291403412818909,
      "learning_rate": 4.9550000000000005e-05,
      "loss": 0.0045,
      "step": 1350
    },
    {
      "epoch": 0.07253333333333334,
      "grad_norm": 0.286685049533844,
      "learning_rate": 4.954666666666667e-05,
      "loss": 0.0054,
      "step": 1360
    },
    {
      "epoch": 0.07306666666666667,
      "grad_norm": 0.29467710852622986,
      "learning_rate": 4.954333333333334e-05,
      "loss": 0.0061,
      "step": 1370
    },
    {
      "epoch": 0.0736,
      "grad_norm": 0.29395556449890137,
      "learning_rate": 4.9540000000000003e-05,
      "loss": 0.0056,
      "step": 1380
    },
    {
      "epoch": 0.07413333333333333,
      "grad_norm": 0.5795683860778809,
      "learning_rate": 4.953666666666667e-05,
      "loss": 0.0056,
      "step": 1390
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 0.6513316035270691,
      "learning_rate": 4.9533333333333336e-05,
      "loss": 0.0057,
      "step": 1400
    },
    {
      "epoch": 0.0752,
      "grad_norm": 0.7896528244018555,
      "learning_rate": 4.953e-05,
      "loss": 0.0072,
      "step": 1410
    },
    {
      "epoch": 0.07573333333333333,
      "grad_norm": 0.5738516449928284,
      "learning_rate": 4.952666666666667e-05,
      "loss": 0.0061,
      "step": 1420
    },
    {
      "epoch": 0.07626666666666666,
      "grad_norm": 1.0070241689682007,
      "learning_rate": 4.952333333333334e-05,
      "loss": 0.0069,
      "step": 1430
    },
    {
      "epoch": 0.0768,
      "grad_norm": 0.3010483682155609,
      "learning_rate": 4.952e-05,
      "loss": 0.0062,
      "step": 1440
    },
    {
      "epoch": 0.07733333333333334,
      "grad_norm": 0.5872892141342163,
      "learning_rate": 4.9516666666666666e-05,
      "loss": 0.0063,
      "step": 1450
    },
    {
      "epoch": 0.07786666666666667,
      "grad_norm": 0.5298734307289124,
      "learning_rate": 4.951333333333333e-05,
      "loss": 0.0061,
      "step": 1460
    },
    {
      "epoch": 0.0784,
      "grad_norm": 0.5155373215675354,
      "learning_rate": 4.951e-05,
      "loss": 0.0061,
      "step": 1470
    },
    {
      "epoch": 0.07893333333333333,
      "grad_norm": 0.8907660841941833,
      "learning_rate": 4.9506666666666665e-05,
      "loss": 0.0055,
      "step": 1480
    },
    {
      "epoch": 0.07946666666666667,
      "grad_norm": 0.37116336822509766,
      "learning_rate": 4.950333333333334e-05,
      "loss": 0.0057,
      "step": 1490
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.3706775903701782,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 0.0035,
      "step": 1500
    },
    {
      "epoch": 0.08053333333333333,
      "grad_norm": 0.2389899641275406,
      "learning_rate": 4.949666666666667e-05,
      "loss": 0.0042,
      "step": 1510
    },
    {
      "epoch": 0.08106666666666666,
      "grad_norm": 0.2970165014266968,
      "learning_rate": 4.9493333333333336e-05,
      "loss": 0.0063,
      "step": 1520
    },
    {
      "epoch": 0.0816,
      "grad_norm": 0.3545222282409668,
      "learning_rate": 4.949e-05,
      "loss": 0.0054,
      "step": 1530
    },
    {
      "epoch": 0.08213333333333334,
      "grad_norm": 0.08462034910917282,
      "learning_rate": 4.948666666666667e-05,
      "loss": 0.0045,
      "step": 1540
    },
    {
      "epoch": 0.08266666666666667,
      "grad_norm": 0.3550804853439331,
      "learning_rate": 4.9483333333333334e-05,
      "loss": 0.0055,
      "step": 1550
    },
    {
      "epoch": 0.0832,
      "grad_norm": 0.5092331171035767,
      "learning_rate": 4.948000000000001e-05,
      "loss": 0.0052,
      "step": 1560
    },
    {
      "epoch": 0.08373333333333334,
      "grad_norm": 0.4109792411327362,
      "learning_rate": 4.9476666666666674e-05,
      "loss": 0.005,
      "step": 1570
    },
    {
      "epoch": 0.08426666666666667,
      "grad_norm": 0.7430238127708435,
      "learning_rate": 4.947333333333334e-05,
      "loss": 0.0051,
      "step": 1580
    },
    {
      "epoch": 0.0848,
      "grad_norm": 1.3117080926895142,
      "learning_rate": 4.947e-05,
      "loss": 0.0064,
      "step": 1590
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 1.0441964864730835,
      "learning_rate": 4.9466666666666665e-05,
      "loss": 0.0051,
      "step": 1600
    },
    {
      "epoch": 0.08586666666666666,
      "grad_norm": 0.875782310962677,
      "learning_rate": 4.946333333333333e-05,
      "loss": 0.0045,
      "step": 1610
    },
    {
      "epoch": 0.0864,
      "grad_norm": 0.864429771900177,
      "learning_rate": 4.946e-05,
      "loss": 0.0067,
      "step": 1620
    },
    {
      "epoch": 0.08693333333333333,
      "grad_norm": 0.7291532158851624,
      "learning_rate": 4.945666666666667e-05,
      "loss": 0.005,
      "step": 1630
    },
    {
      "epoch": 0.08746666666666666,
      "grad_norm": 0.20817893743515015,
      "learning_rate": 4.9453333333333336e-05,
      "loss": 0.0039,
      "step": 1640
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.6944065690040588,
      "learning_rate": 4.945e-05,
      "loss": 0.0061,
      "step": 1650
    },
    {
      "epoch": 0.08853333333333334,
      "grad_norm": 0.14892582595348358,
      "learning_rate": 4.944666666666667e-05,
      "loss": 0.0053,
      "step": 1660
    },
    {
      "epoch": 0.08906666666666667,
      "grad_norm": 0.3860059678554535,
      "learning_rate": 4.9443333333333335e-05,
      "loss": 0.005,
      "step": 1670
    },
    {
      "epoch": 0.0896,
      "grad_norm": 0.8114487528800964,
      "learning_rate": 4.944e-05,
      "loss": 0.005,
      "step": 1680
    },
    {
      "epoch": 0.09013333333333333,
      "grad_norm": 0.2166544795036316,
      "learning_rate": 4.943666666666667e-05,
      "loss": 0.0055,
      "step": 1690
    },
    {
      "epoch": 0.09066666666666667,
      "grad_norm": 0.883023202419281,
      "learning_rate": 4.943333333333334e-05,
      "loss": 0.0043,
      "step": 1700
    },
    {
      "epoch": 0.0912,
      "grad_norm": 0.7484508752822876,
      "learning_rate": 4.9430000000000006e-05,
      "loss": 0.0049,
      "step": 1710
    },
    {
      "epoch": 0.09173333333333333,
      "grad_norm": 0.8264963626861572,
      "learning_rate": 4.942666666666667e-05,
      "loss": 0.0037,
      "step": 1720
    },
    {
      "epoch": 0.09226666666666666,
      "grad_norm": 0.25867760181427,
      "learning_rate": 4.942333333333334e-05,
      "loss": 0.0059,
      "step": 1730
    },
    {
      "epoch": 0.0928,
      "grad_norm": 0.39179694652557373,
      "learning_rate": 4.942e-05,
      "loss": 0.0048,
      "step": 1740
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.7615455389022827,
      "learning_rate": 4.9416666666666664e-05,
      "loss": 0.0041,
      "step": 1750
    },
    {
      "epoch": 0.09386666666666667,
      "grad_norm": 0.46353572607040405,
      "learning_rate": 4.941333333333334e-05,
      "loss": 0.0066,
      "step": 1760
    },
    {
      "epoch": 0.0944,
      "grad_norm": 0.8780505061149597,
      "learning_rate": 4.941e-05,
      "loss": 0.0031,
      "step": 1770
    },
    {
      "epoch": 0.09493333333333333,
      "grad_norm": 0.19287580251693726,
      "learning_rate": 4.940666666666667e-05,
      "loss": 0.0053,
      "step": 1780
    },
    {
      "epoch": 0.09546666666666667,
      "grad_norm": 0.30711042881011963,
      "learning_rate": 4.9403333333333335e-05,
      "loss": 0.0049,
      "step": 1790
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.1762695163488388,
      "learning_rate": 4.94e-05,
      "loss": 0.0053,
      "step": 1800
    },
    {
      "epoch": 0.09653333333333333,
      "grad_norm": 0.30120694637298584,
      "learning_rate": 4.939666666666667e-05,
      "loss": 0.0059,
      "step": 1810
    },
    {
      "epoch": 0.09706666666666666,
      "grad_norm": 0.7953425049781799,
      "learning_rate": 4.9393333333333334e-05,
      "loss": 0.0043,
      "step": 1820
    },
    {
      "epoch": 0.0976,
      "grad_norm": 0.5011476874351501,
      "learning_rate": 4.939e-05,
      "loss": 0.0045,
      "step": 1830
    },
    {
      "epoch": 0.09813333333333334,
      "grad_norm": 0.3493785858154297,
      "learning_rate": 4.938666666666667e-05,
      "loss": 0.0046,
      "step": 1840
    },
    {
      "epoch": 0.09866666666666667,
      "grad_norm": 0.5729454755783081,
      "learning_rate": 4.938333333333334e-05,
      "loss": 0.0051,
      "step": 1850
    },
    {
      "epoch": 0.0992,
      "grad_norm": 0.27836766839027405,
      "learning_rate": 4.9380000000000005e-05,
      "loss": 0.0055,
      "step": 1860
    },
    {
      "epoch": 0.09973333333333333,
      "grad_norm": 0.30779829621315,
      "learning_rate": 4.937666666666667e-05,
      "loss": 0.0046,
      "step": 1870
    },
    {
      "epoch": 0.10026666666666667,
      "grad_norm": 0.7405155301094055,
      "learning_rate": 4.937333333333334e-05,
      "loss": 0.0038,
      "step": 1880
    },
    {
      "epoch": 0.1008,
      "grad_norm": 0.22583919763565063,
      "learning_rate": 4.937e-05,
      "loss": 0.0046,
      "step": 1890
    },
    {
      "epoch": 0.10133333333333333,
      "grad_norm": 0.4803673028945923,
      "learning_rate": 4.936666666666667e-05,
      "loss": 0.006,
      "step": 1900
    },
    {
      "epoch": 0.10186666666666666,
      "grad_norm": 0.1136431023478508,
      "learning_rate": 4.9363333333333336e-05,
      "loss": 0.0039,
      "step": 1910
    },
    {
      "epoch": 0.1024,
      "grad_norm": 0.1355164349079132,
      "learning_rate": 4.936e-05,
      "loss": 0.0067,
      "step": 1920
    },
    {
      "epoch": 0.10293333333333334,
      "grad_norm": 0.43504324555397034,
      "learning_rate": 4.935666666666667e-05,
      "loss": 0.0062,
      "step": 1930
    },
    {
      "epoch": 0.10346666666666667,
      "grad_norm": 0.6680998802185059,
      "learning_rate": 4.9353333333333334e-05,
      "loss": 0.0046,
      "step": 1940
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.5360462665557861,
      "learning_rate": 4.935e-05,
      "loss": 0.0034,
      "step": 1950
    },
    {
      "epoch": 0.10453333333333334,
      "grad_norm": 0.5970432162284851,
      "learning_rate": 4.9346666666666666e-05,
      "loss": 0.0053,
      "step": 1960
    },
    {
      "epoch": 0.10506666666666667,
      "grad_norm": 0.8198012113571167,
      "learning_rate": 4.934333333333334e-05,
      "loss": 0.0053,
      "step": 1970
    },
    {
      "epoch": 0.1056,
      "grad_norm": 0.2274406999349594,
      "learning_rate": 4.9340000000000005e-05,
      "loss": 0.0052,
      "step": 1980
    },
    {
      "epoch": 0.10613333333333333,
      "grad_norm": 0.335952490568161,
      "learning_rate": 4.933666666666667e-05,
      "loss": 0.0052,
      "step": 1990
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.7953996062278748,
      "learning_rate": 4.933333333333334e-05,
      "loss": 0.0065,
      "step": 2000
    },
    {
      "epoch": 0.1072,
      "grad_norm": 0.46431106328964233,
      "learning_rate": 4.9330000000000004e-05,
      "loss": 0.0034,
      "step": 2010
    },
    {
      "epoch": 0.10773333333333333,
      "grad_norm": 0.47934097051620483,
      "learning_rate": 4.932666666666667e-05,
      "loss": 0.0049,
      "step": 2020
    },
    {
      "epoch": 0.10826666666666666,
      "grad_norm": 0.4353092610836029,
      "learning_rate": 4.9323333333333336e-05,
      "loss": 0.0049,
      "step": 2030
    },
    {
      "epoch": 0.1088,
      "grad_norm": 0.11098722368478775,
      "learning_rate": 4.932e-05,
      "loss": 0.0054,
      "step": 2040
    },
    {
      "epoch": 0.10933333333333334,
      "grad_norm": 0.8474081754684448,
      "learning_rate": 4.931666666666667e-05,
      "loss": 0.0048,
      "step": 2050
    },
    {
      "epoch": 0.10986666666666667,
      "grad_norm": 1.0242003202438354,
      "learning_rate": 4.9313333333333334e-05,
      "loss": 0.0041,
      "step": 2060
    },
    {
      "epoch": 0.1104,
      "grad_norm": 0.9664261937141418,
      "learning_rate": 4.931e-05,
      "loss": 0.0056,
      "step": 2070
    },
    {
      "epoch": 0.11093333333333333,
      "grad_norm": 0.48524534702301025,
      "learning_rate": 4.930666666666667e-05,
      "loss": 0.0038,
      "step": 2080
    },
    {
      "epoch": 0.11146666666666667,
      "grad_norm": 0.12799908220767975,
      "learning_rate": 4.930333333333333e-05,
      "loss": 0.0045,
      "step": 2090
    },
    {
      "epoch": 0.112,
      "grad_norm": 1.0996631383895874,
      "learning_rate": 4.93e-05,
      "loss": 0.0049,
      "step": 2100
    },
    {
      "epoch": 0.11253333333333333,
      "grad_norm": 0.1742388755083084,
      "learning_rate": 4.929666666666667e-05,
      "loss": 0.0038,
      "step": 2110
    },
    {
      "epoch": 0.11306666666666666,
      "grad_norm": 0.47949284315109253,
      "learning_rate": 4.929333333333334e-05,
      "loss": 0.0054,
      "step": 2120
    },
    {
      "epoch": 0.1136,
      "grad_norm": 0.715582013130188,
      "learning_rate": 4.9290000000000004e-05,
      "loss": 0.0049,
      "step": 2130
    },
    {
      "epoch": 0.11413333333333334,
      "grad_norm": 0.17095915973186493,
      "learning_rate": 4.928666666666667e-05,
      "loss": 0.0052,
      "step": 2140
    },
    {
      "epoch": 0.11466666666666667,
      "grad_norm": 0.23864275217056274,
      "learning_rate": 4.9283333333333336e-05,
      "loss": 0.005,
      "step": 2150
    },
    {
      "epoch": 0.1152,
      "grad_norm": 0.5421948432922363,
      "learning_rate": 4.928e-05,
      "loss": 0.0067,
      "step": 2160
    },
    {
      "epoch": 0.11573333333333333,
      "grad_norm": 1.1257576942443848,
      "learning_rate": 4.927666666666667e-05,
      "loss": 0.0063,
      "step": 2170
    },
    {
      "epoch": 0.11626666666666667,
      "grad_norm": 0.8064901828765869,
      "learning_rate": 4.9273333333333335e-05,
      "loss": 0.0035,
      "step": 2180
    },
    {
      "epoch": 0.1168,
      "grad_norm": 0.5411005616188049,
      "learning_rate": 4.927000000000001e-05,
      "loss": 0.0044,
      "step": 2190
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 1.0229538679122925,
      "learning_rate": 4.926666666666667e-05,
      "loss": 0.004,
      "step": 2200
    },
    {
      "epoch": 0.11786666666666666,
      "grad_norm": 0.48131126165390015,
      "learning_rate": 4.926333333333333e-05,
      "loss": 0.005,
      "step": 2210
    },
    {
      "epoch": 0.1184,
      "grad_norm": 0.17303989827632904,
      "learning_rate": 4.926e-05,
      "loss": 0.0051,
      "step": 2220
    },
    {
      "epoch": 0.11893333333333334,
      "grad_norm": 1.0591102838516235,
      "learning_rate": 4.9256666666666665e-05,
      "loss": 0.0055,
      "step": 2230
    },
    {
      "epoch": 0.11946666666666667,
      "grad_norm": 0.8736317157745361,
      "learning_rate": 4.925333333333333e-05,
      "loss": 0.0051,
      "step": 2240
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.8228188157081604,
      "learning_rate": 4.9250000000000004e-05,
      "loss": 0.0043,
      "step": 2250
    },
    {
      "epoch": 0.12053333333333334,
      "grad_norm": 0.1718667894601822,
      "learning_rate": 4.924666666666667e-05,
      "loss": 0.0047,
      "step": 2260
    },
    {
      "epoch": 0.12106666666666667,
      "grad_norm": 0.3631018102169037,
      "learning_rate": 4.924333333333334e-05,
      "loss": 0.0046,
      "step": 2270
    },
    {
      "epoch": 0.1216,
      "grad_norm": 1.0139509439468384,
      "learning_rate": 4.924e-05,
      "loss": 0.0033,
      "step": 2280
    },
    {
      "epoch": 0.12213333333333333,
      "grad_norm": 0.8868719935417175,
      "learning_rate": 4.923666666666667e-05,
      "loss": 0.0054,
      "step": 2290
    },
    {
      "epoch": 0.12266666666666666,
      "grad_norm": 0.2673349380493164,
      "learning_rate": 4.9233333333333335e-05,
      "loss": 0.0046,
      "step": 2300
    },
    {
      "epoch": 0.1232,
      "grad_norm": 0.6870297789573669,
      "learning_rate": 4.923e-05,
      "loss": 0.0044,
      "step": 2310
    },
    {
      "epoch": 0.12373333333333333,
      "grad_norm": 0.12198811024427414,
      "learning_rate": 4.9226666666666674e-05,
      "loss": 0.0046,
      "step": 2320
    },
    {
      "epoch": 0.12426666666666666,
      "grad_norm": 0.3220297396183014,
      "learning_rate": 4.922333333333334e-05,
      "loss": 0.0042,
      "step": 2330
    },
    {
      "epoch": 0.1248,
      "grad_norm": 0.3050166070461273,
      "learning_rate": 4.9220000000000006e-05,
      "loss": 0.0054,
      "step": 2340
    },
    {
      "epoch": 0.12533333333333332,
      "grad_norm": 0.651063084602356,
      "learning_rate": 4.9216666666666666e-05,
      "loss": 0.005,
      "step": 2350
    },
    {
      "epoch": 0.12586666666666665,
      "grad_norm": 1.2271579504013062,
      "learning_rate": 4.921333333333333e-05,
      "loss": 0.0046,
      "step": 2360
    },
    {
      "epoch": 0.1264,
      "grad_norm": 0.5187836289405823,
      "learning_rate": 4.921e-05,
      "loss": 0.0048,
      "step": 2370
    },
    {
      "epoch": 0.12693333333333334,
      "grad_norm": 0.48483723402023315,
      "learning_rate": 4.9206666666666664e-05,
      "loss": 0.0053,
      "step": 2380
    },
    {
      "epoch": 0.12746666666666667,
      "grad_norm": 0.5890863537788391,
      "learning_rate": 4.920333333333334e-05,
      "loss": 0.004,
      "step": 2390
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.09727080911397934,
      "learning_rate": 4.92e-05,
      "loss": 0.0036,
      "step": 2400
    },
    {
      "epoch": 0.12853333333333333,
      "grad_norm": 0.6992249488830566,
      "learning_rate": 4.919666666666667e-05,
      "loss": 0.0054,
      "step": 2410
    },
    {
      "epoch": 0.12906666666666666,
      "grad_norm": 0.395937979221344,
      "learning_rate": 4.9193333333333336e-05,
      "loss": 0.0035,
      "step": 2420
    },
    {
      "epoch": 0.1296,
      "grad_norm": 0.22770115733146667,
      "learning_rate": 4.919e-05,
      "loss": 0.0045,
      "step": 2430
    },
    {
      "epoch": 0.13013333333333332,
      "grad_norm": 0.19618402421474457,
      "learning_rate": 4.918666666666667e-05,
      "loss": 0.0054,
      "step": 2440
    },
    {
      "epoch": 0.13066666666666665,
      "grad_norm": 0.22891166806221008,
      "learning_rate": 4.9183333333333334e-05,
      "loss": 0.003,
      "step": 2450
    },
    {
      "epoch": 0.1312,
      "grad_norm": 0.34336015582084656,
      "learning_rate": 4.918000000000001e-05,
      "loss": 0.0046,
      "step": 2460
    },
    {
      "epoch": 0.13173333333333334,
      "grad_norm": 1.0206750631332397,
      "learning_rate": 4.917666666666667e-05,
      "loss": 0.0049,
      "step": 2470
    },
    {
      "epoch": 0.13226666666666667,
      "grad_norm": 1.1225688457489014,
      "learning_rate": 4.917333333333334e-05,
      "loss": 0.0051,
      "step": 2480
    },
    {
      "epoch": 0.1328,
      "grad_norm": 1.0942529439926147,
      "learning_rate": 4.9170000000000005e-05,
      "loss": 0.0043,
      "step": 2490
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.44749659299850464,
      "learning_rate": 4.9166666666666665e-05,
      "loss": 0.0049,
      "step": 2500
    },
    {
      "epoch": 0.13386666666666666,
      "grad_norm": 0.5776808261871338,
      "learning_rate": 4.916333333333333e-05,
      "loss": 0.0054,
      "step": 2510
    },
    {
      "epoch": 0.1344,
      "grad_norm": 0.43875110149383545,
      "learning_rate": 4.9160000000000004e-05,
      "loss": 0.0051,
      "step": 2520
    },
    {
      "epoch": 0.13493333333333332,
      "grad_norm": 0.7718151807785034,
      "learning_rate": 4.915666666666667e-05,
      "loss": 0.0078,
      "step": 2530
    },
    {
      "epoch": 0.13546666666666668,
      "grad_norm": 0.5944392085075378,
      "learning_rate": 4.9153333333333336e-05,
      "loss": 0.0043,
      "step": 2540
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.22055073082447052,
      "learning_rate": 4.915e-05,
      "loss": 0.0045,
      "step": 2550
    },
    {
      "epoch": 0.13653333333333334,
      "grad_norm": 0.7740654945373535,
      "learning_rate": 4.914666666666667e-05,
      "loss": 0.0038,
      "step": 2560
    },
    {
      "epoch": 0.13706666666666667,
      "grad_norm": 0.8771333694458008,
      "learning_rate": 4.9143333333333334e-05,
      "loss": 0.0048,
      "step": 2570
    },
    {
      "epoch": 0.1376,
      "grad_norm": 0.1295984387397766,
      "learning_rate": 4.914e-05,
      "loss": 0.0059,
      "step": 2580
    },
    {
      "epoch": 0.13813333333333333,
      "grad_norm": 0.18362465500831604,
      "learning_rate": 4.9136666666666667e-05,
      "loss": 0.0043,
      "step": 2590
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 0.463829904794693,
      "learning_rate": 4.913333333333334e-05,
      "loss": 0.0049,
      "step": 2600
    },
    {
      "epoch": 0.1392,
      "grad_norm": 0.5552162528038025,
      "learning_rate": 4.9130000000000006e-05,
      "loss": 0.0049,
      "step": 2610
    },
    {
      "epoch": 0.13973333333333332,
      "grad_norm": 0.6018144488334656,
      "learning_rate": 4.912666666666667e-05,
      "loss": 0.0041,
      "step": 2620
    },
    {
      "epoch": 0.14026666666666668,
      "grad_norm": 0.22563102841377258,
      "learning_rate": 4.912333333333334e-05,
      "loss": 0.0055,
      "step": 2630
    },
    {
      "epoch": 0.1408,
      "grad_norm": 0.8118315935134888,
      "learning_rate": 4.9120000000000004e-05,
      "loss": 0.005,
      "step": 2640
    },
    {
      "epoch": 0.14133333333333334,
      "grad_norm": 0.8061515092849731,
      "learning_rate": 4.9116666666666663e-05,
      "loss": 0.0056,
      "step": 2650
    },
    {
      "epoch": 0.14186666666666667,
      "grad_norm": 0.7618919014930725,
      "learning_rate": 4.9113333333333336e-05,
      "loss": 0.0049,
      "step": 2660
    },
    {
      "epoch": 0.1424,
      "grad_norm": 0.6583229303359985,
      "learning_rate": 4.911e-05,
      "loss": 0.0062,
      "step": 2670
    },
    {
      "epoch": 0.14293333333333333,
      "grad_norm": 0.31534382700920105,
      "learning_rate": 4.910666666666667e-05,
      "loss": 0.004,
      "step": 2680
    },
    {
      "epoch": 0.14346666666666666,
      "grad_norm": 0.11599146574735641,
      "learning_rate": 4.9103333333333335e-05,
      "loss": 0.0044,
      "step": 2690
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.1060749813914299,
      "learning_rate": 4.91e-05,
      "loss": 0.0039,
      "step": 2700
    },
    {
      "epoch": 0.14453333333333335,
      "grad_norm": 0.21991339325904846,
      "learning_rate": 4.909666666666667e-05,
      "loss": 0.0059,
      "step": 2710
    },
    {
      "epoch": 0.14506666666666668,
      "grad_norm": 0.19651181995868683,
      "learning_rate": 4.909333333333333e-05,
      "loss": 0.0039,
      "step": 2720
    },
    {
      "epoch": 0.1456,
      "grad_norm": 0.2439604103565216,
      "learning_rate": 4.9090000000000006e-05,
      "loss": 0.0053,
      "step": 2730
    },
    {
      "epoch": 0.14613333333333334,
      "grad_norm": 1.1169602870941162,
      "learning_rate": 4.908666666666667e-05,
      "loss": 0.0051,
      "step": 2740
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.37396949529647827,
      "learning_rate": 4.908333333333334e-05,
      "loss": 0.0053,
      "step": 2750
    },
    {
      "epoch": 0.1472,
      "grad_norm": 1.1684131622314453,
      "learning_rate": 4.9080000000000004e-05,
      "loss": 0.0053,
      "step": 2760
    },
    {
      "epoch": 0.14773333333333333,
      "grad_norm": 0.7634432911872864,
      "learning_rate": 4.907666666666667e-05,
      "loss": 0.0043,
      "step": 2770
    },
    {
      "epoch": 0.14826666666666666,
      "grad_norm": 1.0076161623001099,
      "learning_rate": 4.907333333333334e-05,
      "loss": 0.0049,
      "step": 2780
    },
    {
      "epoch": 0.1488,
      "grad_norm": 0.655648946762085,
      "learning_rate": 4.907e-05,
      "loss": 0.0039,
      "step": 2790
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 0.36918190121650696,
      "learning_rate": 4.906666666666667e-05,
      "loss": 0.0053,
      "step": 2800
    },
    {
      "epoch": 0.14986666666666668,
      "grad_norm": 0.13580119609832764,
      "learning_rate": 4.9063333333333335e-05,
      "loss": 0.0057,
      "step": 2810
    },
    {
      "epoch": 0.1504,
      "grad_norm": 0.5700379610061646,
      "learning_rate": 4.906e-05,
      "loss": 0.004,
      "step": 2820
    },
    {
      "epoch": 0.15093333333333334,
      "grad_norm": 0.546344518661499,
      "learning_rate": 4.905666666666667e-05,
      "loss": 0.0045,
      "step": 2830
    },
    {
      "epoch": 0.15146666666666667,
      "grad_norm": 1.0924656391143799,
      "learning_rate": 4.9053333333333333e-05,
      "loss": 0.0045,
      "step": 2840
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.2645128667354584,
      "learning_rate": 4.905e-05,
      "loss": 0.0061,
      "step": 2850
    },
    {
      "epoch": 0.15253333333333333,
      "grad_norm": 0.06649935990571976,
      "learning_rate": 4.9046666666666666e-05,
      "loss": 0.0065,
      "step": 2860
    },
    {
      "epoch": 0.15306666666666666,
      "grad_norm": 0.13849467039108276,
      "learning_rate": 4.904333333333334e-05,
      "loss": 0.0031,
      "step": 2870
    },
    {
      "epoch": 0.1536,
      "grad_norm": 0.5823467969894409,
      "learning_rate": 4.9040000000000005e-05,
      "loss": 0.006,
      "step": 2880
    },
    {
      "epoch": 0.15413333333333334,
      "grad_norm": 0.6999878287315369,
      "learning_rate": 4.903666666666667e-05,
      "loss": 0.0052,
      "step": 2890
    },
    {
      "epoch": 0.15466666666666667,
      "grad_norm": 0.48284006118774414,
      "learning_rate": 4.903333333333334e-05,
      "loss": 0.0043,
      "step": 2900
    },
    {
      "epoch": 0.1552,
      "grad_norm": 0.4179382622241974,
      "learning_rate": 4.903e-05,
      "loss": 0.0045,
      "step": 2910
    },
    {
      "epoch": 0.15573333333333333,
      "grad_norm": 0.4135565161705017,
      "learning_rate": 4.902666666666667e-05,
      "loss": 0.0048,
      "step": 2920
    },
    {
      "epoch": 0.15626666666666666,
      "grad_norm": 0.4374864101409912,
      "learning_rate": 4.9023333333333335e-05,
      "loss": 0.0037,
      "step": 2930
    },
    {
      "epoch": 0.1568,
      "grad_norm": 0.31706503033638,
      "learning_rate": 4.902e-05,
      "loss": 0.0038,
      "step": 2940
    },
    {
      "epoch": 0.15733333333333333,
      "grad_norm": 0.3638320863246918,
      "learning_rate": 4.901666666666667e-05,
      "loss": 0.0059,
      "step": 2950
    },
    {
      "epoch": 0.15786666666666666,
      "grad_norm": 0.6432752013206482,
      "learning_rate": 4.9013333333333334e-05,
      "loss": 0.0058,
      "step": 2960
    },
    {
      "epoch": 0.1584,
      "grad_norm": 0.20054233074188232,
      "learning_rate": 4.901e-05,
      "loss": 0.0039,
      "step": 2970
    },
    {
      "epoch": 0.15893333333333334,
      "grad_norm": 0.22084680199623108,
      "learning_rate": 4.9006666666666666e-05,
      "loss": 0.003,
      "step": 2980
    },
    {
      "epoch": 0.15946666666666667,
      "grad_norm": 0.5488749742507935,
      "learning_rate": 4.900333333333333e-05,
      "loss": 0.004,
      "step": 2990
    },
    {
      "epoch": 0.16,
      "grad_norm": 1.1182461977005005,
      "learning_rate": 4.9e-05,
      "loss": 0.0047,
      "step": 3000
    },
    {
      "epoch": 0.16053333333333333,
      "grad_norm": 0.6715815663337708,
      "learning_rate": 4.899666666666667e-05,
      "loss": 0.0035,
      "step": 3010
    },
    {
      "epoch": 0.16106666666666666,
      "grad_norm": 0.5756721496582031,
      "learning_rate": 4.899333333333334e-05,
      "loss": 0.0048,
      "step": 3020
    },
    {
      "epoch": 0.1616,
      "grad_norm": 0.3862619400024414,
      "learning_rate": 4.8990000000000004e-05,
      "loss": 0.0046,
      "step": 3030
    },
    {
      "epoch": 0.16213333333333332,
      "grad_norm": 0.3779926598072052,
      "learning_rate": 4.898666666666667e-05,
      "loss": 0.0054,
      "step": 3040
    },
    {
      "epoch": 0.16266666666666665,
      "grad_norm": 0.2734333872795105,
      "learning_rate": 4.8983333333333336e-05,
      "loss": 0.0053,
      "step": 3050
    },
    {
      "epoch": 0.1632,
      "grad_norm": 0.6884438991546631,
      "learning_rate": 4.898e-05,
      "loss": 0.0048,
      "step": 3060
    },
    {
      "epoch": 0.16373333333333334,
      "grad_norm": 0.36751702427864075,
      "learning_rate": 4.897666666666667e-05,
      "loss": 0.0046,
      "step": 3070
    },
    {
      "epoch": 0.16426666666666667,
      "grad_norm": 0.18121902644634247,
      "learning_rate": 4.897333333333334e-05,
      "loss": 0.0045,
      "step": 3080
    },
    {
      "epoch": 0.1648,
      "grad_norm": 0.5811387896537781,
      "learning_rate": 4.897000000000001e-05,
      "loss": 0.0044,
      "step": 3090
    },
    {
      "epoch": 0.16533333333333333,
      "grad_norm": 0.29025906324386597,
      "learning_rate": 4.8966666666666667e-05,
      "loss": 0.0053,
      "step": 3100
    },
    {
      "epoch": 0.16586666666666666,
      "grad_norm": 0.9566662907600403,
      "learning_rate": 4.896333333333333e-05,
      "loss": 0.0059,
      "step": 3110
    },
    {
      "epoch": 0.1664,
      "grad_norm": 0.5651314854621887,
      "learning_rate": 4.896e-05,
      "loss": 0.0058,
      "step": 3120
    },
    {
      "epoch": 0.16693333333333332,
      "grad_norm": 0.20115889608860016,
      "learning_rate": 4.8956666666666665e-05,
      "loss": 0.0045,
      "step": 3130
    },
    {
      "epoch": 0.16746666666666668,
      "grad_norm": 0.30669355392456055,
      "learning_rate": 4.895333333333333e-05,
      "loss": 0.0043,
      "step": 3140
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.8952993154525757,
      "learning_rate": 4.8950000000000004e-05,
      "loss": 0.0059,
      "step": 3150
    },
    {
      "epoch": 0.16853333333333334,
      "grad_norm": 0.17055325210094452,
      "learning_rate": 4.894666666666667e-05,
      "loss": 0.0043,
      "step": 3160
    },
    {
      "epoch": 0.16906666666666667,
      "grad_norm": 0.579086422920227,
      "learning_rate": 4.8943333333333336e-05,
      "loss": 0.0047,
      "step": 3170
    },
    {
      "epoch": 0.1696,
      "grad_norm": 0.21143770217895508,
      "learning_rate": 4.894e-05,
      "loss": 0.0042,
      "step": 3180
    },
    {
      "epoch": 0.17013333333333333,
      "grad_norm": 0.6977613568305969,
      "learning_rate": 4.893666666666667e-05,
      "loss": 0.0049,
      "step": 3190
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 1.1741560697555542,
      "learning_rate": 4.8933333333333335e-05,
      "loss": 0.0049,
      "step": 3200
    },
    {
      "epoch": 0.1712,
      "grad_norm": 0.374355673789978,
      "learning_rate": 4.893e-05,
      "loss": 0.0047,
      "step": 3210
    },
    {
      "epoch": 0.17173333333333332,
      "grad_norm": 0.22315263748168945,
      "learning_rate": 4.8926666666666674e-05,
      "loss": 0.0052,
      "step": 3220
    },
    {
      "epoch": 0.17226666666666668,
      "grad_norm": 0.2040109485387802,
      "learning_rate": 4.892333333333334e-05,
      "loss": 0.0038,
      "step": 3230
    },
    {
      "epoch": 0.1728,
      "grad_norm": 0.7230232954025269,
      "learning_rate": 4.8920000000000006e-05,
      "loss": 0.004,
      "step": 3240
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.1983204483985901,
      "learning_rate": 4.891666666666667e-05,
      "loss": 0.0046,
      "step": 3250
    },
    {
      "epoch": 0.17386666666666667,
      "grad_norm": 0.20583505928516388,
      "learning_rate": 4.891333333333333e-05,
      "loss": 0.0046,
      "step": 3260
    },
    {
      "epoch": 0.1744,
      "grad_norm": 0.5131634473800659,
      "learning_rate": 4.891e-05,
      "loss": 0.0056,
      "step": 3270
    },
    {
      "epoch": 0.17493333333333333,
      "grad_norm": 0.5504204034805298,
      "learning_rate": 4.890666666666667e-05,
      "loss": 0.004,
      "step": 3280
    },
    {
      "epoch": 0.17546666666666666,
      "grad_norm": 0.4961860477924347,
      "learning_rate": 4.890333333333334e-05,
      "loss": 0.0054,
      "step": 3290
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.12117765843868256,
      "learning_rate": 4.89e-05,
      "loss": 0.0026,
      "step": 3300
    },
    {
      "epoch": 0.17653333333333332,
      "grad_norm": 0.8634388446807861,
      "learning_rate": 4.889666666666667e-05,
      "loss": 0.0038,
      "step": 3310
    },
    {
      "epoch": 0.17706666666666668,
      "grad_norm": 0.35434356331825256,
      "learning_rate": 4.8893333333333335e-05,
      "loss": 0.0035,
      "step": 3320
    },
    {
      "epoch": 0.1776,
      "grad_norm": 0.6325380206108093,
      "learning_rate": 4.889e-05,
      "loss": 0.0043,
      "step": 3330
    },
    {
      "epoch": 0.17813333333333334,
      "grad_norm": 0.467345654964447,
      "learning_rate": 4.888666666666667e-05,
      "loss": 0.0045,
      "step": 3340
    },
    {
      "epoch": 0.17866666666666667,
      "grad_norm": 0.6424950957298279,
      "learning_rate": 4.8883333333333333e-05,
      "loss": 0.005,
      "step": 3350
    },
    {
      "epoch": 0.1792,
      "grad_norm": 1.004050850868225,
      "learning_rate": 4.8880000000000006e-05,
      "loss": 0.0046,
      "step": 3360
    },
    {
      "epoch": 0.17973333333333333,
      "grad_norm": 1.0435599088668823,
      "learning_rate": 4.887666666666667e-05,
      "loss": 0.0041,
      "step": 3370
    },
    {
      "epoch": 0.18026666666666666,
      "grad_norm": 0.6961759328842163,
      "learning_rate": 4.887333333333334e-05,
      "loss": 0.0033,
      "step": 3380
    },
    {
      "epoch": 0.1808,
      "grad_norm": 0.8614820837974548,
      "learning_rate": 4.8870000000000005e-05,
      "loss": 0.0038,
      "step": 3390
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 0.49992457032203674,
      "learning_rate": 4.886666666666667e-05,
      "loss": 0.0032,
      "step": 3400
    },
    {
      "epoch": 0.18186666666666668,
      "grad_norm": 0.9518705606460571,
      "learning_rate": 4.886333333333333e-05,
      "loss": 0.0064,
      "step": 3410
    },
    {
      "epoch": 0.1824,
      "grad_norm": 0.45476943254470825,
      "learning_rate": 4.886e-05,
      "loss": 0.005,
      "step": 3420
    },
    {
      "epoch": 0.18293333333333334,
      "grad_norm": 0.17212465405464172,
      "learning_rate": 4.885666666666667e-05,
      "loss": 0.0043,
      "step": 3430
    },
    {
      "epoch": 0.18346666666666667,
      "grad_norm": 0.19600944221019745,
      "learning_rate": 4.8853333333333335e-05,
      "loss": 0.0037,
      "step": 3440
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.6803464293479919,
      "learning_rate": 4.885e-05,
      "loss": 0.0044,
      "step": 3450
    },
    {
      "epoch": 0.18453333333333333,
      "grad_norm": 0.688162624835968,
      "learning_rate": 4.884666666666667e-05,
      "loss": 0.0043,
      "step": 3460
    },
    {
      "epoch": 0.18506666666666666,
      "grad_norm": 0.43633973598480225,
      "learning_rate": 4.8843333333333334e-05,
      "loss": 0.004,
      "step": 3470
    },
    {
      "epoch": 0.1856,
      "grad_norm": 0.5185176730155945,
      "learning_rate": 4.884e-05,
      "loss": 0.0047,
      "step": 3480
    },
    {
      "epoch": 0.18613333333333335,
      "grad_norm": 0.15800832211971283,
      "learning_rate": 4.8836666666666666e-05,
      "loss": 0.0054,
      "step": 3490
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.33044907450675964,
      "learning_rate": 4.883333333333334e-05,
      "loss": 0.0034,
      "step": 3500
    },
    {
      "epoch": 0.1872,
      "grad_norm": 0.4452380836009979,
      "learning_rate": 4.8830000000000005e-05,
      "loss": 0.0036,
      "step": 3510
    },
    {
      "epoch": 0.18773333333333334,
      "grad_norm": 0.119183748960495,
      "learning_rate": 4.882666666666667e-05,
      "loss": 0.0044,
      "step": 3520
    },
    {
      "epoch": 0.18826666666666667,
      "grad_norm": 0.5704377889633179,
      "learning_rate": 4.882333333333334e-05,
      "loss": 0.0042,
      "step": 3530
    },
    {
      "epoch": 0.1888,
      "grad_norm": 0.37879931926727295,
      "learning_rate": 4.8820000000000004e-05,
      "loss": 0.0037,
      "step": 3540
    },
    {
      "epoch": 0.18933333333333333,
      "grad_norm": 0.6193032264709473,
      "learning_rate": 4.881666666666667e-05,
      "loss": 0.005,
      "step": 3550
    },
    {
      "epoch": 0.18986666666666666,
      "grad_norm": 0.9452218413352966,
      "learning_rate": 4.8813333333333336e-05,
      "loss": 0.0038,
      "step": 3560
    },
    {
      "epoch": 0.1904,
      "grad_norm": 0.7931993007659912,
      "learning_rate": 4.881e-05,
      "loss": 0.0031,
      "step": 3570
    },
    {
      "epoch": 0.19093333333333334,
      "grad_norm": 0.8064568638801575,
      "learning_rate": 4.880666666666667e-05,
      "loss": 0.0036,
      "step": 3580
    },
    {
      "epoch": 0.19146666666666667,
      "grad_norm": 1.037198543548584,
      "learning_rate": 4.8803333333333334e-05,
      "loss": 0.0046,
      "step": 3590
    },
    {
      "epoch": 0.192,
      "grad_norm": 1.0490080118179321,
      "learning_rate": 4.88e-05,
      "loss": 0.0052,
      "step": 3600
    },
    {
      "epoch": 0.19253333333333333,
      "grad_norm": 0.6381645798683167,
      "learning_rate": 4.8796666666666666e-05,
      "loss": 0.0043,
      "step": 3610
    },
    {
      "epoch": 0.19306666666666666,
      "grad_norm": 0.1600697785615921,
      "learning_rate": 4.879333333333333e-05,
      "loss": 0.0036,
      "step": 3620
    },
    {
      "epoch": 0.1936,
      "grad_norm": 0.5155138969421387,
      "learning_rate": 4.8790000000000006e-05,
      "loss": 0.0036,
      "step": 3630
    },
    {
      "epoch": 0.19413333333333332,
      "grad_norm": 0.13509802520275116,
      "learning_rate": 4.878666666666667e-05,
      "loss": 0.0051,
      "step": 3640
    },
    {
      "epoch": 0.19466666666666665,
      "grad_norm": 0.3618420362472534,
      "learning_rate": 4.878333333333334e-05,
      "loss": 0.0037,
      "step": 3650
    },
    {
      "epoch": 0.1952,
      "grad_norm": 0.8895652890205383,
      "learning_rate": 4.8780000000000004e-05,
      "loss": 0.0042,
      "step": 3660
    },
    {
      "epoch": 0.19573333333333334,
      "grad_norm": 0.33287253975868225,
      "learning_rate": 4.877666666666667e-05,
      "loss": 0.0041,
      "step": 3670
    },
    {
      "epoch": 0.19626666666666667,
      "grad_norm": 0.23035486042499542,
      "learning_rate": 4.8773333333333336e-05,
      "loss": 0.0037,
      "step": 3680
    },
    {
      "epoch": 0.1968,
      "grad_norm": 0.5713796019554138,
      "learning_rate": 4.877e-05,
      "loss": 0.0062,
      "step": 3690
    },
    {
      "epoch": 0.19733333333333333,
      "grad_norm": 0.7250229716300964,
      "learning_rate": 4.876666666666667e-05,
      "loss": 0.0044,
      "step": 3700
    },
    {
      "epoch": 0.19786666666666666,
      "grad_norm": 0.9432428479194641,
      "learning_rate": 4.8763333333333335e-05,
      "loss": 0.004,
      "step": 3710
    },
    {
      "epoch": 0.1984,
      "grad_norm": 0.17793861031532288,
      "learning_rate": 4.876e-05,
      "loss": 0.0045,
      "step": 3720
    },
    {
      "epoch": 0.19893333333333332,
      "grad_norm": 0.36659738421440125,
      "learning_rate": 4.875666666666667e-05,
      "loss": 0.0043,
      "step": 3730
    },
    {
      "epoch": 0.19946666666666665,
      "grad_norm": 0.4592059254646301,
      "learning_rate": 4.875333333333333e-05,
      "loss": 0.0042,
      "step": 3740
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.413236141204834,
      "learning_rate": 4.875e-05,
      "loss": 0.0044,
      "step": 3750
    },
    {
      "epoch": 0.20053333333333334,
      "grad_norm": 0.6175062656402588,
      "learning_rate": 4.8746666666666665e-05,
      "loss": 0.004,
      "step": 3760
    },
    {
      "epoch": 0.20106666666666667,
      "grad_norm": 0.17206674814224243,
      "learning_rate": 4.874333333333334e-05,
      "loss": 0.0063,
      "step": 3770
    },
    {
      "epoch": 0.2016,
      "grad_norm": 0.3980988562107086,
      "learning_rate": 4.8740000000000004e-05,
      "loss": 0.0046,
      "step": 3780
    },
    {
      "epoch": 0.20213333333333333,
      "grad_norm": 0.39057689905166626,
      "learning_rate": 4.873666666666667e-05,
      "loss": 0.0044,
      "step": 3790
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 0.4437687397003174,
      "learning_rate": 4.8733333333333337e-05,
      "loss": 0.0043,
      "step": 3800
    },
    {
      "epoch": 0.2032,
      "grad_norm": 0.2742796838283539,
      "learning_rate": 4.873e-05,
      "loss": 0.0057,
      "step": 3810
    },
    {
      "epoch": 0.20373333333333332,
      "grad_norm": 0.12567055225372314,
      "learning_rate": 4.872666666666667e-05,
      "loss": 0.0048,
      "step": 3820
    },
    {
      "epoch": 0.20426666666666668,
      "grad_norm": 0.478679895401001,
      "learning_rate": 4.8723333333333335e-05,
      "loss": 0.0036,
      "step": 3830
    },
    {
      "epoch": 0.2048,
      "grad_norm": 0.31965070962905884,
      "learning_rate": 4.872000000000001e-05,
      "loss": 0.004,
      "step": 3840
    },
    {
      "epoch": 0.20533333333333334,
      "grad_norm": 0.41846922039985657,
      "learning_rate": 4.8716666666666674e-05,
      "loss": 0.0043,
      "step": 3850
    },
    {
      "epoch": 0.20586666666666667,
      "grad_norm": 0.5280062556266785,
      "learning_rate": 4.871333333333333e-05,
      "loss": 0.0028,
      "step": 3860
    },
    {
      "epoch": 0.2064,
      "grad_norm": 0.8275597095489502,
      "learning_rate": 4.871e-05,
      "loss": 0.0038,
      "step": 3870
    },
    {
      "epoch": 0.20693333333333333,
      "grad_norm": 1.1416537761688232,
      "learning_rate": 4.8706666666666666e-05,
      "loss": 0.0047,
      "step": 3880
    },
    {
      "epoch": 0.20746666666666666,
      "grad_norm": 0.2535112798213959,
      "learning_rate": 4.870333333333333e-05,
      "loss": 0.0041,
      "step": 3890
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.20351940393447876,
      "learning_rate": 4.87e-05,
      "loss": 0.0034,
      "step": 3900
    },
    {
      "epoch": 0.20853333333333332,
      "grad_norm": 0.2979198694229126,
      "learning_rate": 4.869666666666667e-05,
      "loss": 0.0037,
      "step": 3910
    },
    {
      "epoch": 0.20906666666666668,
      "grad_norm": 0.45245736837387085,
      "learning_rate": 4.869333333333334e-05,
      "loss": 0.0045,
      "step": 3920
    },
    {
      "epoch": 0.2096,
      "grad_norm": 0.35413888096809387,
      "learning_rate": 4.869e-05,
      "loss": 0.0049,
      "step": 3930
    },
    {
      "epoch": 0.21013333333333334,
      "grad_norm": 0.1651851087808609,
      "learning_rate": 4.868666666666667e-05,
      "loss": 0.0047,
      "step": 3940
    },
    {
      "epoch": 0.21066666666666667,
      "grad_norm": 0.4772154986858368,
      "learning_rate": 4.8683333333333335e-05,
      "loss": 0.0043,
      "step": 3950
    },
    {
      "epoch": 0.2112,
      "grad_norm": 0.5591704845428467,
      "learning_rate": 4.868e-05,
      "loss": 0.0052,
      "step": 3960
    },
    {
      "epoch": 0.21173333333333333,
      "grad_norm": 0.36149361729621887,
      "learning_rate": 4.867666666666667e-05,
      "loss": 0.0052,
      "step": 3970
    },
    {
      "epoch": 0.21226666666666666,
      "grad_norm": 0.7343998551368713,
      "learning_rate": 4.867333333333334e-05,
      "loss": 0.0041,
      "step": 3980
    },
    {
      "epoch": 0.2128,
      "grad_norm": 0.1291734129190445,
      "learning_rate": 4.867000000000001e-05,
      "loss": 0.0053,
      "step": 3990
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.25609636306762695,
      "learning_rate": 4.866666666666667e-05,
      "loss": 0.0036,
      "step": 4000
    },
    {
      "epoch": 0.21386666666666668,
      "grad_norm": 0.07670514285564423,
      "learning_rate": 4.866333333333333e-05,
      "loss": 0.0046,
      "step": 4010
    },
    {
      "epoch": 0.2144,
      "grad_norm": 0.14419737458229065,
      "learning_rate": 4.866e-05,
      "loss": 0.0042,
      "step": 4020
    },
    {
      "epoch": 0.21493333333333334,
      "grad_norm": 0.17305240035057068,
      "learning_rate": 4.8656666666666664e-05,
      "loss": 0.0031,
      "step": 4030
    },
    {
      "epoch": 0.21546666666666667,
      "grad_norm": 0.6815420985221863,
      "learning_rate": 4.865333333333334e-05,
      "loss": 0.0043,
      "step": 4040
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.24159610271453857,
      "learning_rate": 4.8650000000000003e-05,
      "loss": 0.0041,
      "step": 4050
    },
    {
      "epoch": 0.21653333333333333,
      "grad_norm": 0.10720241814851761,
      "learning_rate": 4.864666666666667e-05,
      "loss": 0.0047,
      "step": 4060
    },
    {
      "epoch": 0.21706666666666666,
      "grad_norm": 0.48783835768699646,
      "learning_rate": 4.8643333333333336e-05,
      "loss": 0.0045,
      "step": 4070
    },
    {
      "epoch": 0.2176,
      "grad_norm": 0.31980016827583313,
      "learning_rate": 4.864e-05,
      "loss": 0.0036,
      "step": 4080
    },
    {
      "epoch": 0.21813333333333335,
      "grad_norm": 0.6541765928268433,
      "learning_rate": 4.863666666666667e-05,
      "loss": 0.0034,
      "step": 4090
    },
    {
      "epoch": 0.21866666666666668,
      "grad_norm": 1.0134810209274292,
      "learning_rate": 4.8633333333333334e-05,
      "loss": 0.0039,
      "step": 4100
    },
    {
      "epoch": 0.2192,
      "grad_norm": 0.40220844745635986,
      "learning_rate": 4.863e-05,
      "loss": 0.004,
      "step": 4110
    },
    {
      "epoch": 0.21973333333333334,
      "grad_norm": 1.1832365989685059,
      "learning_rate": 4.862666666666667e-05,
      "loss": 0.0037,
      "step": 4120
    },
    {
      "epoch": 0.22026666666666667,
      "grad_norm": 0.07029937952756882,
      "learning_rate": 4.862333333333334e-05,
      "loss": 0.0051,
      "step": 4130
    },
    {
      "epoch": 0.2208,
      "grad_norm": 0.28620800375938416,
      "learning_rate": 4.8620000000000005e-05,
      "loss": 0.0037,
      "step": 4140
    },
    {
      "epoch": 0.22133333333333333,
      "grad_norm": 0.17456477880477905,
      "learning_rate": 4.861666666666667e-05,
      "loss": 0.0042,
      "step": 4150
    },
    {
      "epoch": 0.22186666666666666,
      "grad_norm": 0.2796177566051483,
      "learning_rate": 4.861333333333333e-05,
      "loss": 0.004,
      "step": 4160
    },
    {
      "epoch": 0.2224,
      "grad_norm": 0.3976109027862549,
      "learning_rate": 4.861e-05,
      "loss": 0.004,
      "step": 4170
    },
    {
      "epoch": 0.22293333333333334,
      "grad_norm": 0.7235804796218872,
      "learning_rate": 4.860666666666667e-05,
      "loss": 0.0043,
      "step": 4180
    },
    {
      "epoch": 0.22346666666666667,
      "grad_norm": 0.7758132219314575,
      "learning_rate": 4.8603333333333336e-05,
      "loss": 0.0049,
      "step": 4190
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.30174410343170166,
      "learning_rate": 4.86e-05,
      "loss": 0.0036,
      "step": 4200
    },
    {
      "epoch": 0.22453333333333333,
      "grad_norm": 0.2512950599193573,
      "learning_rate": 4.859666666666667e-05,
      "loss": 0.0032,
      "step": 4210
    },
    {
      "epoch": 0.22506666666666666,
      "grad_norm": 0.12249638140201569,
      "learning_rate": 4.8593333333333335e-05,
      "loss": 0.0049,
      "step": 4220
    },
    {
      "epoch": 0.2256,
      "grad_norm": 0.2329864352941513,
      "learning_rate": 4.859e-05,
      "loss": 0.0049,
      "step": 4230
    },
    {
      "epoch": 0.22613333333333333,
      "grad_norm": 0.4670972228050232,
      "learning_rate": 4.858666666666667e-05,
      "loss": 0.0029,
      "step": 4240
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.6586546301841736,
      "learning_rate": 4.858333333333333e-05,
      "loss": 0.0034,
      "step": 4250
    },
    {
      "epoch": 0.2272,
      "grad_norm": 0.14681659638881683,
      "learning_rate": 4.8580000000000006e-05,
      "loss": 0.003,
      "step": 4260
    },
    {
      "epoch": 0.22773333333333334,
      "grad_norm": 0.19344232976436615,
      "learning_rate": 4.857666666666667e-05,
      "loss": 0.0053,
      "step": 4270
    },
    {
      "epoch": 0.22826666666666667,
      "grad_norm": 1.0137296915054321,
      "learning_rate": 4.857333333333334e-05,
      "loss": 0.0043,
      "step": 4280
    },
    {
      "epoch": 0.2288,
      "grad_norm": 0.7661744952201843,
      "learning_rate": 4.8570000000000004e-05,
      "loss": 0.0023,
      "step": 4290
    },
    {
      "epoch": 0.22933333333333333,
      "grad_norm": 0.8748210072517395,
      "learning_rate": 4.856666666666667e-05,
      "loss": 0.0031,
      "step": 4300
    },
    {
      "epoch": 0.22986666666666666,
      "grad_norm": 0.1653861254453659,
      "learning_rate": 4.856333333333333e-05,
      "loss": 0.0042,
      "step": 4310
    },
    {
      "epoch": 0.2304,
      "grad_norm": 0.1304236650466919,
      "learning_rate": 4.856e-05,
      "loss": 0.0038,
      "step": 4320
    },
    {
      "epoch": 0.23093333333333332,
      "grad_norm": 0.7183918356895447,
      "learning_rate": 4.855666666666667e-05,
      "loss": 0.0054,
      "step": 4330
    },
    {
      "epoch": 0.23146666666666665,
      "grad_norm": 0.5032766461372375,
      "learning_rate": 4.8553333333333335e-05,
      "loss": 0.0043,
      "step": 4340
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.1435098946094513,
      "learning_rate": 4.855e-05,
      "loss": 0.0026,
      "step": 4350
    },
    {
      "epoch": 0.23253333333333334,
      "grad_norm": 1.1396316289901733,
      "learning_rate": 4.854666666666667e-05,
      "loss": 0.0049,
      "step": 4360
    },
    {
      "epoch": 0.23306666666666667,
      "grad_norm": 0.7920800447463989,
      "learning_rate": 4.854333333333333e-05,
      "loss": 0.004,
      "step": 4370
    },
    {
      "epoch": 0.2336,
      "grad_norm": 0.7900845408439636,
      "learning_rate": 4.854e-05,
      "loss": 0.0042,
      "step": 4380
    },
    {
      "epoch": 0.23413333333333333,
      "grad_norm": 0.9241494536399841,
      "learning_rate": 4.853666666666667e-05,
      "loss": 0.0046,
      "step": 4390
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 0.3981836140155792,
      "learning_rate": 4.853333333333334e-05,
      "loss": 0.0043,
      "step": 4400
    },
    {
      "epoch": 0.2352,
      "grad_norm": 0.49446237087249756,
      "learning_rate": 4.8530000000000005e-05,
      "loss": 0.0055,
      "step": 4410
    },
    {
      "epoch": 0.23573333333333332,
      "grad_norm": 0.44801485538482666,
      "learning_rate": 4.852666666666667e-05,
      "loss": 0.0038,
      "step": 4420
    },
    {
      "epoch": 0.23626666666666668,
      "grad_norm": 0.5170116424560547,
      "learning_rate": 4.852333333333334e-05,
      "loss": 0.0029,
      "step": 4430
    },
    {
      "epoch": 0.2368,
      "grad_norm": 0.8637663722038269,
      "learning_rate": 4.852e-05,
      "loss": 0.0036,
      "step": 4440
    },
    {
      "epoch": 0.23733333333333334,
      "grad_norm": 0.5001671314239502,
      "learning_rate": 4.851666666666667e-05,
      "loss": 0.0035,
      "step": 4450
    },
    {
      "epoch": 0.23786666666666667,
      "grad_norm": 0.14316491782665253,
      "learning_rate": 4.8513333333333335e-05,
      "loss": 0.0036,
      "step": 4460
    },
    {
      "epoch": 0.2384,
      "grad_norm": 0.6227326989173889,
      "learning_rate": 4.851e-05,
      "loss": 0.0048,
      "step": 4470
    },
    {
      "epoch": 0.23893333333333333,
      "grad_norm": 0.5052451491355896,
      "learning_rate": 4.850666666666667e-05,
      "loss": 0.0054,
      "step": 4480
    },
    {
      "epoch": 0.23946666666666666,
      "grad_norm": 0.2031700313091278,
      "learning_rate": 4.8503333333333334e-05,
      "loss": 0.0051,
      "step": 4490
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.714810311794281,
      "learning_rate": 4.85e-05,
      "loss": 0.0037,
      "step": 4500
    },
    {
      "epoch": 0.24053333333333332,
      "grad_norm": 0.49773967266082764,
      "learning_rate": 4.8496666666666666e-05,
      "loss": 0.0047,
      "step": 4510
    },
    {
      "epoch": 0.24106666666666668,
      "grad_norm": 0.2235703319311142,
      "learning_rate": 4.849333333333333e-05,
      "loss": 0.0045,
      "step": 4520
    },
    {
      "epoch": 0.2416,
      "grad_norm": 0.16233941912651062,
      "learning_rate": 4.8490000000000005e-05,
      "loss": 0.0036,
      "step": 4530
    },
    {
      "epoch": 0.24213333333333334,
      "grad_norm": 0.9757947325706482,
      "learning_rate": 4.848666666666667e-05,
      "loss": 0.0044,
      "step": 4540
    },
    {
      "epoch": 0.24266666666666667,
      "grad_norm": 0.3648791015148163,
      "learning_rate": 4.848333333333334e-05,
      "loss": 0.0036,
      "step": 4550
    },
    {
      "epoch": 0.2432,
      "grad_norm": 0.6519240736961365,
      "learning_rate": 4.8480000000000003e-05,
      "loss": 0.0034,
      "step": 4560
    },
    {
      "epoch": 0.24373333333333333,
      "grad_norm": 0.4187160134315491,
      "learning_rate": 4.847666666666667e-05,
      "loss": 0.0023,
      "step": 4570
    },
    {
      "epoch": 0.24426666666666666,
      "grad_norm": 0.6016877293586731,
      "learning_rate": 4.8473333333333336e-05,
      "loss": 0.004,
      "step": 4580
    },
    {
      "epoch": 0.2448,
      "grad_norm": 1.0126365423202515,
      "learning_rate": 4.847e-05,
      "loss": 0.0045,
      "step": 4590
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 0.8875814080238342,
      "learning_rate": 4.8466666666666675e-05,
      "loss": 0.0025,
      "step": 4600
    },
    {
      "epoch": 0.24586666666666668,
      "grad_norm": 0.3393372893333435,
      "learning_rate": 4.846333333333334e-05,
      "loss": 0.0048,
      "step": 4610
    },
    {
      "epoch": 0.2464,
      "grad_norm": 0.9265812635421753,
      "learning_rate": 4.846e-05,
      "loss": 0.0026,
      "step": 4620
    },
    {
      "epoch": 0.24693333333333334,
      "grad_norm": 0.9316039681434631,
      "learning_rate": 4.8456666666666666e-05,
      "loss": 0.0047,
      "step": 4630
    },
    {
      "epoch": 0.24746666666666667,
      "grad_norm": 0.19160564243793488,
      "learning_rate": 4.845333333333333e-05,
      "loss": 0.0029,
      "step": 4640
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.24210965633392334,
      "learning_rate": 4.845e-05,
      "loss": 0.0041,
      "step": 4650
    },
    {
      "epoch": 0.24853333333333333,
      "grad_norm": 0.25105175375938416,
      "learning_rate": 4.8446666666666665e-05,
      "loss": 0.0043,
      "step": 4660
    },
    {
      "epoch": 0.24906666666666666,
      "grad_norm": 0.18825486302375793,
      "learning_rate": 4.844333333333334e-05,
      "loss": 0.0037,
      "step": 4670
    },
    {
      "epoch": 0.2496,
      "grad_norm": 0.21165458858013153,
      "learning_rate": 4.8440000000000004e-05,
      "loss": 0.0046,
      "step": 4680
    },
    {
      "epoch": 0.2501333333333333,
      "grad_norm": 0.1097099706530571,
      "learning_rate": 4.843666666666667e-05,
      "loss": 0.0037,
      "step": 4690
    },
    {
      "epoch": 0.25066666666666665,
      "grad_norm": 0.9658278226852417,
      "learning_rate": 4.8433333333333336e-05,
      "loss": 0.0049,
      "step": 4700
    },
    {
      "epoch": 0.2512,
      "grad_norm": 0.611493706703186,
      "learning_rate": 4.843e-05,
      "loss": 0.0047,
      "step": 4710
    },
    {
      "epoch": 0.2517333333333333,
      "grad_norm": 1.0660955905914307,
      "learning_rate": 4.842666666666667e-05,
      "loss": 0.003,
      "step": 4720
    },
    {
      "epoch": 0.25226666666666664,
      "grad_norm": 0.8954503536224365,
      "learning_rate": 4.8423333333333334e-05,
      "loss": 0.0037,
      "step": 4730
    },
    {
      "epoch": 0.2528,
      "grad_norm": 0.5162229537963867,
      "learning_rate": 4.842000000000001e-05,
      "loss": 0.0059,
      "step": 4740
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.3696064054965973,
      "learning_rate": 4.8416666666666673e-05,
      "loss": 0.005,
      "step": 4750
    },
    {
      "epoch": 0.2538666666666667,
      "grad_norm": 0.13177871704101562,
      "learning_rate": 4.841333333333334e-05,
      "loss": 0.0029,
      "step": 4760
    },
    {
      "epoch": 0.2544,
      "grad_norm": 0.21871554851531982,
      "learning_rate": 4.841e-05,
      "loss": 0.004,
      "step": 4770
    },
    {
      "epoch": 0.25493333333333335,
      "grad_norm": 0.4356933832168579,
      "learning_rate": 4.8406666666666665e-05,
      "loss": 0.0042,
      "step": 4780
    },
    {
      "epoch": 0.2554666666666667,
      "grad_norm": 0.7737376689910889,
      "learning_rate": 4.840333333333333e-05,
      "loss": 0.0042,
      "step": 4790
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.23338380455970764,
      "learning_rate": 4.8400000000000004e-05,
      "loss": 0.0043,
      "step": 4800
    },
    {
      "epoch": 0.25653333333333334,
      "grad_norm": 0.3329743444919586,
      "learning_rate": 4.839666666666667e-05,
      "loss": 0.0051,
      "step": 4810
    },
    {
      "epoch": 0.25706666666666667,
      "grad_norm": 0.7104965448379517,
      "learning_rate": 4.8393333333333336e-05,
      "loss": 0.0037,
      "step": 4820
    },
    {
      "epoch": 0.2576,
      "grad_norm": 0.23638364672660828,
      "learning_rate": 4.839e-05,
      "loss": 0.0047,
      "step": 4830
    },
    {
      "epoch": 0.2581333333333333,
      "grad_norm": 0.27619868516921997,
      "learning_rate": 4.838666666666667e-05,
      "loss": 0.0034,
      "step": 4840
    },
    {
      "epoch": 0.25866666666666666,
      "grad_norm": 1.068716287612915,
      "learning_rate": 4.8383333333333335e-05,
      "loss": 0.004,
      "step": 4850
    },
    {
      "epoch": 0.2592,
      "grad_norm": 0.24418967962265015,
      "learning_rate": 4.838e-05,
      "loss": 0.0041,
      "step": 4860
    },
    {
      "epoch": 0.2597333333333333,
      "grad_norm": 0.10647380352020264,
      "learning_rate": 4.837666666666667e-05,
      "loss": 0.004,
      "step": 4870
    },
    {
      "epoch": 0.26026666666666665,
      "grad_norm": 0.4381277561187744,
      "learning_rate": 4.837333333333334e-05,
      "loss": 0.0038,
      "step": 4880
    },
    {
      "epoch": 0.2608,
      "grad_norm": 0.2890642583370209,
      "learning_rate": 4.8370000000000006e-05,
      "loss": 0.0036,
      "step": 4890
    },
    {
      "epoch": 0.2613333333333333,
      "grad_norm": 0.2663056254386902,
      "learning_rate": 4.836666666666667e-05,
      "loss": 0.0026,
      "step": 4900
    },
    {
      "epoch": 0.2618666666666667,
      "grad_norm": 0.3535885512828827,
      "learning_rate": 4.836333333333334e-05,
      "loss": 0.0061,
      "step": 4910
    },
    {
      "epoch": 0.2624,
      "grad_norm": 0.23410682380199432,
      "learning_rate": 4.836e-05,
      "loss": 0.0043,
      "step": 4920
    },
    {
      "epoch": 0.26293333333333335,
      "grad_norm": 1.4181362390518188,
      "learning_rate": 4.8356666666666664e-05,
      "loss": 0.0028,
      "step": 4930
    },
    {
      "epoch": 0.2634666666666667,
      "grad_norm": 0.39605942368507385,
      "learning_rate": 4.835333333333334e-05,
      "loss": 0.0047,
      "step": 4940
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.3645607829093933,
      "learning_rate": 4.835e-05,
      "loss": 0.0048,
      "step": 4950
    },
    {
      "epoch": 0.26453333333333334,
      "grad_norm": 0.8911077976226807,
      "learning_rate": 4.834666666666667e-05,
      "loss": 0.005,
      "step": 4960
    },
    {
      "epoch": 0.2650666666666667,
      "grad_norm": 0.28482750058174133,
      "learning_rate": 4.8343333333333335e-05,
      "loss": 0.0025,
      "step": 4970
    },
    {
      "epoch": 0.2656,
      "grad_norm": 0.4948466122150421,
      "learning_rate": 4.834e-05,
      "loss": 0.003,
      "step": 4980
    },
    {
      "epoch": 0.26613333333333333,
      "grad_norm": 0.8546030521392822,
      "learning_rate": 4.833666666666667e-05,
      "loss": 0.0029,
      "step": 4990
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.8085708618164062,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 0.0049,
      "step": 5000
    },
    {
      "epoch": 0.2672,
      "grad_norm": 0.35730865597724915,
      "learning_rate": 4.833e-05,
      "loss": 0.0045,
      "step": 5010
    },
    {
      "epoch": 0.2677333333333333,
      "grad_norm": 0.5348122715950012,
      "learning_rate": 4.832666666666667e-05,
      "loss": 0.0067,
      "step": 5020
    },
    {
      "epoch": 0.26826666666666665,
      "grad_norm": 0.383696049451828,
      "learning_rate": 4.832333333333334e-05,
      "loss": 0.0027,
      "step": 5030
    },
    {
      "epoch": 0.2688,
      "grad_norm": 0.36206409335136414,
      "learning_rate": 4.8320000000000005e-05,
      "loss": 0.0035,
      "step": 5040
    },
    {
      "epoch": 0.2693333333333333,
      "grad_norm": 0.4399647116661072,
      "learning_rate": 4.831666666666667e-05,
      "loss": 0.0041,
      "step": 5050
    },
    {
      "epoch": 0.26986666666666664,
      "grad_norm": 0.5550397634506226,
      "learning_rate": 4.831333333333334e-05,
      "loss": 0.0032,
      "step": 5060
    },
    {
      "epoch": 0.2704,
      "grad_norm": 1.1225930452346802,
      "learning_rate": 4.8309999999999997e-05,
      "loss": 0.0042,
      "step": 5070
    },
    {
      "epoch": 0.27093333333333336,
      "grad_norm": 0.8194698095321655,
      "learning_rate": 4.830666666666667e-05,
      "loss": 0.0032,
      "step": 5080
    },
    {
      "epoch": 0.2714666666666667,
      "grad_norm": 0.4068835973739624,
      "learning_rate": 4.8303333333333336e-05,
      "loss": 0.0047,
      "step": 5090
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.32606345415115356,
      "learning_rate": 4.83e-05,
      "loss": 0.0034,
      "step": 5100
    },
    {
      "epoch": 0.27253333333333335,
      "grad_norm": 0.1217871904373169,
      "learning_rate": 4.829666666666667e-05,
      "loss": 0.0032,
      "step": 5110
    },
    {
      "epoch": 0.2730666666666667,
      "grad_norm": 0.3942126929759979,
      "learning_rate": 4.8293333333333334e-05,
      "loss": 0.0039,
      "step": 5120
    },
    {
      "epoch": 0.2736,
      "grad_norm": 0.4193612337112427,
      "learning_rate": 4.829e-05,
      "loss": 0.0037,
      "step": 5130
    },
    {
      "epoch": 0.27413333333333334,
      "grad_norm": 0.46625980734825134,
      "learning_rate": 4.8286666666666666e-05,
      "loss": 0.0043,
      "step": 5140
    },
    {
      "epoch": 0.27466666666666667,
      "grad_norm": 0.4659423232078552,
      "learning_rate": 4.828333333333334e-05,
      "loss": 0.004,
      "step": 5150
    },
    {
      "epoch": 0.2752,
      "grad_norm": 0.7066212892532349,
      "learning_rate": 4.8280000000000005e-05,
      "loss": 0.0033,
      "step": 5160
    },
    {
      "epoch": 0.27573333333333333,
      "grad_norm": 0.37328654527664185,
      "learning_rate": 4.827666666666667e-05,
      "loss": 0.0038,
      "step": 5170
    },
    {
      "epoch": 0.27626666666666666,
      "grad_norm": 0.2055855691432953,
      "learning_rate": 4.827333333333334e-05,
      "loss": 0.0036,
      "step": 5180
    },
    {
      "epoch": 0.2768,
      "grad_norm": 0.6115662455558777,
      "learning_rate": 4.8270000000000004e-05,
      "loss": 0.0033,
      "step": 5190
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 0.664711058139801,
      "learning_rate": 4.826666666666667e-05,
      "loss": 0.004,
      "step": 5200
    },
    {
      "epoch": 0.27786666666666665,
      "grad_norm": 0.577926754951477,
      "learning_rate": 4.8263333333333336e-05,
      "loss": 0.0033,
      "step": 5210
    },
    {
      "epoch": 0.2784,
      "grad_norm": 0.3209412693977356,
      "learning_rate": 4.826e-05,
      "loss": 0.0038,
      "step": 5220
    },
    {
      "epoch": 0.2789333333333333,
      "grad_norm": 0.49926114082336426,
      "learning_rate": 4.825666666666667e-05,
      "loss": 0.0035,
      "step": 5230
    },
    {
      "epoch": 0.27946666666666664,
      "grad_norm": 1.0469502210617065,
      "learning_rate": 4.8253333333333334e-05,
      "loss": 0.0043,
      "step": 5240
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.12407224625349045,
      "learning_rate": 4.825e-05,
      "loss": 0.0037,
      "step": 5250
    },
    {
      "epoch": 0.28053333333333336,
      "grad_norm": 0.1672029048204422,
      "learning_rate": 4.824666666666667e-05,
      "loss": 0.004,
      "step": 5260
    },
    {
      "epoch": 0.2810666666666667,
      "grad_norm": 0.6765240430831909,
      "learning_rate": 4.824333333333333e-05,
      "loss": 0.003,
      "step": 5270
    },
    {
      "epoch": 0.2816,
      "grad_norm": 0.49141815304756165,
      "learning_rate": 4.824e-05,
      "loss": 0.0036,
      "step": 5280
    },
    {
      "epoch": 0.28213333333333335,
      "grad_norm": 0.4643106162548065,
      "learning_rate": 4.823666666666667e-05,
      "loss": 0.0049,
      "step": 5290
    },
    {
      "epoch": 0.2826666666666667,
      "grad_norm": 0.42121368646621704,
      "learning_rate": 4.823333333333334e-05,
      "loss": 0.0046,
      "step": 5300
    },
    {
      "epoch": 0.2832,
      "grad_norm": 0.12170998007059097,
      "learning_rate": 4.8230000000000004e-05,
      "loss": 0.0046,
      "step": 5310
    },
    {
      "epoch": 0.28373333333333334,
      "grad_norm": 0.47245293855667114,
      "learning_rate": 4.822666666666667e-05,
      "loss": 0.0033,
      "step": 5320
    },
    {
      "epoch": 0.28426666666666667,
      "grad_norm": 0.42286986112594604,
      "learning_rate": 4.8223333333333336e-05,
      "loss": 0.0039,
      "step": 5330
    },
    {
      "epoch": 0.2848,
      "grad_norm": 0.1769176870584488,
      "learning_rate": 4.822e-05,
      "loss": 0.0031,
      "step": 5340
    },
    {
      "epoch": 0.2853333333333333,
      "grad_norm": 0.18265295028686523,
      "learning_rate": 4.821666666666667e-05,
      "loss": 0.0049,
      "step": 5350
    },
    {
      "epoch": 0.28586666666666666,
      "grad_norm": 0.07414132356643677,
      "learning_rate": 4.8213333333333335e-05,
      "loss": 0.0045,
      "step": 5360
    },
    {
      "epoch": 0.2864,
      "grad_norm": 0.2659483551979065,
      "learning_rate": 4.821e-05,
      "loss": 0.0028,
      "step": 5370
    },
    {
      "epoch": 0.2869333333333333,
      "grad_norm": 0.31630846858024597,
      "learning_rate": 4.820666666666667e-05,
      "loss": 0.005,
      "step": 5380
    },
    {
      "epoch": 0.28746666666666665,
      "grad_norm": 0.5432391166687012,
      "learning_rate": 4.820333333333333e-05,
      "loss": 0.0038,
      "step": 5390
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.5479669570922852,
      "learning_rate": 4.82e-05,
      "loss": 0.004,
      "step": 5400
    },
    {
      "epoch": 0.2885333333333333,
      "grad_norm": 0.6016009449958801,
      "learning_rate": 4.8196666666666665e-05,
      "loss": 0.0038,
      "step": 5410
    },
    {
      "epoch": 0.2890666666666667,
      "grad_norm": 0.14745575189590454,
      "learning_rate": 4.819333333333333e-05,
      "loss": 0.0066,
      "step": 5420
    },
    {
      "epoch": 0.2896,
      "grad_norm": 0.08474874496459961,
      "learning_rate": 4.8190000000000004e-05,
      "loss": 0.0047,
      "step": 5430
    },
    {
      "epoch": 0.29013333333333335,
      "grad_norm": 0.631410539150238,
      "learning_rate": 4.818666666666667e-05,
      "loss": 0.0037,
      "step": 5440
    },
    {
      "epoch": 0.2906666666666667,
      "grad_norm": 0.7459933161735535,
      "learning_rate": 4.818333333333334e-05,
      "loss": 0.0041,
      "step": 5450
    },
    {
      "epoch": 0.2912,
      "grad_norm": 0.1438397616147995,
      "learning_rate": 4.818e-05,
      "loss": 0.0025,
      "step": 5460
    },
    {
      "epoch": 0.29173333333333334,
      "grad_norm": 0.5297266244888306,
      "learning_rate": 4.817666666666667e-05,
      "loss": 0.0034,
      "step": 5470
    },
    {
      "epoch": 0.2922666666666667,
      "grad_norm": 0.1435357630252838,
      "learning_rate": 4.8173333333333335e-05,
      "loss": 0.0034,
      "step": 5480
    },
    {
      "epoch": 0.2928,
      "grad_norm": 0.2838439643383026,
      "learning_rate": 4.817e-05,
      "loss": 0.0044,
      "step": 5490
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.5080799460411072,
      "learning_rate": 4.8166666666666674e-05,
      "loss": 0.0048,
      "step": 5500
    },
    {
      "epoch": 0.29386666666666666,
      "grad_norm": 0.18608510494232178,
      "learning_rate": 4.816333333333334e-05,
      "loss": 0.0048,
      "step": 5510
    },
    {
      "epoch": 0.2944,
      "grad_norm": 0.14092279970645905,
      "learning_rate": 4.816e-05,
      "loss": 0.0034,
      "step": 5520
    },
    {
      "epoch": 0.2949333333333333,
      "grad_norm": 0.7771042585372925,
      "learning_rate": 4.8156666666666666e-05,
      "loss": 0.0044,
      "step": 5530
    },
    {
      "epoch": 0.29546666666666666,
      "grad_norm": 0.2670075297355652,
      "learning_rate": 4.815333333333333e-05,
      "loss": 0.0038,
      "step": 5540
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.36741000413894653,
      "learning_rate": 4.815e-05,
      "loss": 0.0046,
      "step": 5550
    },
    {
      "epoch": 0.2965333333333333,
      "grad_norm": 0.7713107466697693,
      "learning_rate": 4.814666666666667e-05,
      "loss": 0.0034,
      "step": 5560
    },
    {
      "epoch": 0.29706666666666665,
      "grad_norm": 0.2155361771583557,
      "learning_rate": 4.814333333333334e-05,
      "loss": 0.0039,
      "step": 5570
    },
    {
      "epoch": 0.2976,
      "grad_norm": 0.11246470361948013,
      "learning_rate": 4.814e-05,
      "loss": 0.0031,
      "step": 5580
    },
    {
      "epoch": 0.2981333333333333,
      "grad_norm": 0.35901620984077454,
      "learning_rate": 4.813666666666667e-05,
      "loss": 0.004,
      "step": 5590
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 0.5645202994346619,
      "learning_rate": 4.8133333333333336e-05,
      "loss": 0.0037,
      "step": 5600
    },
    {
      "epoch": 0.2992,
      "grad_norm": 0.10441219061613083,
      "learning_rate": 4.813e-05,
      "loss": 0.0058,
      "step": 5610
    },
    {
      "epoch": 0.29973333333333335,
      "grad_norm": 0.19999806582927704,
      "learning_rate": 4.812666666666667e-05,
      "loss": 0.004,
      "step": 5620
    },
    {
      "epoch": 0.3002666666666667,
      "grad_norm": 0.525043249130249,
      "learning_rate": 4.8123333333333334e-05,
      "loss": 0.0047,
      "step": 5630
    },
    {
      "epoch": 0.3008,
      "grad_norm": 0.12356038391590118,
      "learning_rate": 4.812000000000001e-05,
      "loss": 0.0027,
      "step": 5640
    },
    {
      "epoch": 0.30133333333333334,
      "grad_norm": 0.5272925496101379,
      "learning_rate": 4.811666666666667e-05,
      "loss": 0.0034,
      "step": 5650
    },
    {
      "epoch": 0.30186666666666667,
      "grad_norm": 0.4988504946231842,
      "learning_rate": 4.811333333333334e-05,
      "loss": 0.0047,
      "step": 5660
    },
    {
      "epoch": 0.3024,
      "grad_norm": 0.46324777603149414,
      "learning_rate": 4.8110000000000005e-05,
      "loss": 0.0039,
      "step": 5670
    },
    {
      "epoch": 0.30293333333333333,
      "grad_norm": 0.3378536105155945,
      "learning_rate": 4.8106666666666665e-05,
      "loss": 0.0031,
      "step": 5680
    },
    {
      "epoch": 0.30346666666666666,
      "grad_norm": 0.29838067293167114,
      "learning_rate": 4.810333333333333e-05,
      "loss": 0.003,
      "step": 5690
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.2168913632631302,
      "learning_rate": 4.8100000000000004e-05,
      "loss": 0.0045,
      "step": 5700
    },
    {
      "epoch": 0.3045333333333333,
      "grad_norm": 0.5650357007980347,
      "learning_rate": 4.809666666666667e-05,
      "loss": 0.0027,
      "step": 5710
    },
    {
      "epoch": 0.30506666666666665,
      "grad_norm": 0.9425517916679382,
      "learning_rate": 4.8093333333333336e-05,
      "loss": 0.0029,
      "step": 5720
    },
    {
      "epoch": 0.3056,
      "grad_norm": 0.15928290784358978,
      "learning_rate": 4.809e-05,
      "loss": 0.0038,
      "step": 5730
    },
    {
      "epoch": 0.3061333333333333,
      "grad_norm": 0.23277804255485535,
      "learning_rate": 4.808666666666667e-05,
      "loss": 0.0026,
      "step": 5740
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.2729388475418091,
      "learning_rate": 4.8083333333333334e-05,
      "loss": 0.0032,
      "step": 5750
    },
    {
      "epoch": 0.3072,
      "grad_norm": 0.1697198450565338,
      "learning_rate": 4.808e-05,
      "loss": 0.0043,
      "step": 5760
    },
    {
      "epoch": 0.30773333333333336,
      "grad_norm": 0.5583190321922302,
      "learning_rate": 4.8076666666666667e-05,
      "loss": 0.0025,
      "step": 5770
    },
    {
      "epoch": 0.3082666666666667,
      "grad_norm": 0.7790123224258423,
      "learning_rate": 4.807333333333334e-05,
      "loss": 0.0048,
      "step": 5780
    },
    {
      "epoch": 0.3088,
      "grad_norm": 0.5642213225364685,
      "learning_rate": 4.8070000000000006e-05,
      "loss": 0.0048,
      "step": 5790
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 0.3010474443435669,
      "learning_rate": 4.806666666666667e-05,
      "loss": 0.0047,
      "step": 5800
    },
    {
      "epoch": 0.3098666666666667,
      "grad_norm": 0.19680330157279968,
      "learning_rate": 4.806333333333334e-05,
      "loss": 0.0044,
      "step": 5810
    },
    {
      "epoch": 0.3104,
      "grad_norm": 0.16554927825927734,
      "learning_rate": 4.8060000000000004e-05,
      "loss": 0.0043,
      "step": 5820
    },
    {
      "epoch": 0.31093333333333334,
      "grad_norm": 0.3373221158981323,
      "learning_rate": 4.805666666666666e-05,
      "loss": 0.0036,
      "step": 5830
    },
    {
      "epoch": 0.31146666666666667,
      "grad_norm": 0.35848015546798706,
      "learning_rate": 4.8053333333333336e-05,
      "loss": 0.0045,
      "step": 5840
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.15078295767307281,
      "learning_rate": 4.805e-05,
      "loss": 0.0035,
      "step": 5850
    },
    {
      "epoch": 0.31253333333333333,
      "grad_norm": 0.610759973526001,
      "learning_rate": 4.804666666666667e-05,
      "loss": 0.0034,
      "step": 5860
    },
    {
      "epoch": 0.31306666666666666,
      "grad_norm": 0.512572169303894,
      "learning_rate": 4.8043333333333335e-05,
      "loss": 0.0036,
      "step": 5870
    },
    {
      "epoch": 0.3136,
      "grad_norm": 0.4664387106895447,
      "learning_rate": 4.804e-05,
      "loss": 0.0041,
      "step": 5880
    },
    {
      "epoch": 0.3141333333333333,
      "grad_norm": 0.7787384390830994,
      "learning_rate": 4.803666666666667e-05,
      "loss": 0.0025,
      "step": 5890
    },
    {
      "epoch": 0.31466666666666665,
      "grad_norm": 0.1360950469970703,
      "learning_rate": 4.803333333333333e-05,
      "loss": 0.0047,
      "step": 5900
    },
    {
      "epoch": 0.3152,
      "grad_norm": 0.47787007689476013,
      "learning_rate": 4.8030000000000006e-05,
      "loss": 0.0039,
      "step": 5910
    },
    {
      "epoch": 0.3157333333333333,
      "grad_norm": 0.7434366345405579,
      "learning_rate": 4.802666666666667e-05,
      "loss": 0.0046,
      "step": 5920
    },
    {
      "epoch": 0.31626666666666664,
      "grad_norm": 0.9474770426750183,
      "learning_rate": 4.802333333333334e-05,
      "loss": 0.0033,
      "step": 5930
    },
    {
      "epoch": 0.3168,
      "grad_norm": 0.07481315732002258,
      "learning_rate": 4.8020000000000004e-05,
      "loss": 0.0036,
      "step": 5940
    },
    {
      "epoch": 0.31733333333333336,
      "grad_norm": 0.5264549851417542,
      "learning_rate": 4.801666666666667e-05,
      "loss": 0.0034,
      "step": 5950
    },
    {
      "epoch": 0.3178666666666667,
      "grad_norm": 1.0406043529510498,
      "learning_rate": 4.801333333333334e-05,
      "loss": 0.0027,
      "step": 5960
    },
    {
      "epoch": 0.3184,
      "grad_norm": 0.7698129415512085,
      "learning_rate": 4.801e-05,
      "loss": 0.0036,
      "step": 5970
    },
    {
      "epoch": 0.31893333333333335,
      "grad_norm": 0.281865656375885,
      "learning_rate": 4.800666666666667e-05,
      "loss": 0.0052,
      "step": 5980
    },
    {
      "epoch": 0.3194666666666667,
      "grad_norm": 0.37178823351860046,
      "learning_rate": 4.8003333333333335e-05,
      "loss": 0.004,
      "step": 5990
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.5579937100410461,
      "learning_rate": 4.8e-05,
      "loss": 0.0033,
      "step": 6000
    },
    {
      "epoch": 0.32053333333333334,
      "grad_norm": 0.7519350647926331,
      "learning_rate": 4.799666666666667e-05,
      "loss": 0.0036,
      "step": 6010
    },
    {
      "epoch": 0.32106666666666667,
      "grad_norm": 0.7128608822822571,
      "learning_rate": 4.7993333333333333e-05,
      "loss": 0.0042,
      "step": 6020
    },
    {
      "epoch": 0.3216,
      "grad_norm": 0.11803863942623138,
      "learning_rate": 4.799e-05,
      "loss": 0.004,
      "step": 6030
    },
    {
      "epoch": 0.3221333333333333,
      "grad_norm": 0.2707199454307556,
      "learning_rate": 4.7986666666666666e-05,
      "loss": 0.0047,
      "step": 6040
    },
    {
      "epoch": 0.32266666666666666,
      "grad_norm": 0.2077436000108719,
      "learning_rate": 4.798333333333334e-05,
      "loss": 0.0041,
      "step": 6050
    },
    {
      "epoch": 0.3232,
      "grad_norm": 0.30337652564048767,
      "learning_rate": 4.7980000000000005e-05,
      "loss": 0.0041,
      "step": 6060
    },
    {
      "epoch": 0.3237333333333333,
      "grad_norm": 0.31328609585762024,
      "learning_rate": 4.797666666666667e-05,
      "loss": 0.0042,
      "step": 6070
    },
    {
      "epoch": 0.32426666666666665,
      "grad_norm": 0.5662345290184021,
      "learning_rate": 4.797333333333334e-05,
      "loss": 0.0035,
      "step": 6080
    },
    {
      "epoch": 0.3248,
      "grad_norm": 0.44664227962493896,
      "learning_rate": 4.797e-05,
      "loss": 0.003,
      "step": 6090
    },
    {
      "epoch": 0.3253333333333333,
      "grad_norm": 0.28192031383514404,
      "learning_rate": 4.796666666666667e-05,
      "loss": 0.0042,
      "step": 6100
    },
    {
      "epoch": 0.3258666666666667,
      "grad_norm": 0.2860792577266693,
      "learning_rate": 4.7963333333333335e-05,
      "loss": 0.0033,
      "step": 6110
    },
    {
      "epoch": 0.3264,
      "grad_norm": 0.2366115152835846,
      "learning_rate": 4.796e-05,
      "loss": 0.0033,
      "step": 6120
    },
    {
      "epoch": 0.32693333333333335,
      "grad_norm": 0.42752066254615784,
      "learning_rate": 4.795666666666667e-05,
      "loss": 0.0033,
      "step": 6130
    },
    {
      "epoch": 0.3274666666666667,
      "grad_norm": 0.47261613607406616,
      "learning_rate": 4.7953333333333334e-05,
      "loss": 0.003,
      "step": 6140
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.5340765118598938,
      "learning_rate": 4.795e-05,
      "loss": 0.0044,
      "step": 6150
    },
    {
      "epoch": 0.32853333333333334,
      "grad_norm": 0.6275371313095093,
      "learning_rate": 4.7946666666666666e-05,
      "loss": 0.0039,
      "step": 6160
    },
    {
      "epoch": 0.3290666666666667,
      "grad_norm": 0.21244776248931885,
      "learning_rate": 4.794333333333333e-05,
      "loss": 0.0046,
      "step": 6170
    },
    {
      "epoch": 0.3296,
      "grad_norm": 0.5923150181770325,
      "learning_rate": 4.794e-05,
      "loss": 0.0046,
      "step": 6180
    },
    {
      "epoch": 0.33013333333333333,
      "grad_norm": 0.16060622036457062,
      "learning_rate": 4.793666666666667e-05,
      "loss": 0.0037,
      "step": 6190
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 0.17215347290039062,
      "learning_rate": 4.793333333333334e-05,
      "loss": 0.004,
      "step": 6200
    },
    {
      "epoch": 0.3312,
      "grad_norm": 0.082954540848732,
      "learning_rate": 4.7930000000000004e-05,
      "loss": 0.0042,
      "step": 6210
    },
    {
      "epoch": 0.3317333333333333,
      "grad_norm": 0.7368201017379761,
      "learning_rate": 4.792666666666667e-05,
      "loss": 0.0045,
      "step": 6220
    },
    {
      "epoch": 0.33226666666666665,
      "grad_norm": 0.16059768199920654,
      "learning_rate": 4.7923333333333336e-05,
      "loss": 0.0034,
      "step": 6230
    },
    {
      "epoch": 0.3328,
      "grad_norm": 0.2839180827140808,
      "learning_rate": 4.792e-05,
      "loss": 0.0027,
      "step": 6240
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.12217416614294052,
      "learning_rate": 4.791666666666667e-05,
      "loss": 0.0028,
      "step": 6250
    },
    {
      "epoch": 0.33386666666666664,
      "grad_norm": 0.05428387597203255,
      "learning_rate": 4.791333333333334e-05,
      "loss": 0.0042,
      "step": 6260
    },
    {
      "epoch": 0.3344,
      "grad_norm": 0.1279366910457611,
      "learning_rate": 4.791000000000001e-05,
      "loss": 0.005,
      "step": 6270
    },
    {
      "epoch": 0.33493333333333336,
      "grad_norm": 0.1634717434644699,
      "learning_rate": 4.7906666666666667e-05,
      "loss": 0.0031,
      "step": 6280
    },
    {
      "epoch": 0.3354666666666667,
      "grad_norm": 0.6832262873649597,
      "learning_rate": 4.790333333333333e-05,
      "loss": 0.0042,
      "step": 6290
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.5090543031692505,
      "learning_rate": 4.79e-05,
      "loss": 0.004,
      "step": 6300
    },
    {
      "epoch": 0.33653333333333335,
      "grad_norm": 0.45016440749168396,
      "learning_rate": 4.7896666666666665e-05,
      "loss": 0.0039,
      "step": 6310
    },
    {
      "epoch": 0.3370666666666667,
      "grad_norm": 0.24494294822216034,
      "learning_rate": 4.789333333333334e-05,
      "loss": 0.0032,
      "step": 6320
    },
    {
      "epoch": 0.3376,
      "grad_norm": 0.13404116034507751,
      "learning_rate": 4.7890000000000004e-05,
      "loss": 0.0038,
      "step": 6330
    },
    {
      "epoch": 0.33813333333333334,
      "grad_norm": 0.22753314673900604,
      "learning_rate": 4.788666666666667e-05,
      "loss": 0.0037,
      "step": 6340
    },
    {
      "epoch": 0.33866666666666667,
      "grad_norm": 0.7059684991836548,
      "learning_rate": 4.7883333333333336e-05,
      "loss": 0.0032,
      "step": 6350
    },
    {
      "epoch": 0.3392,
      "grad_norm": 0.20669221878051758,
      "learning_rate": 4.788e-05,
      "loss": 0.0036,
      "step": 6360
    },
    {
      "epoch": 0.33973333333333333,
      "grad_norm": 0.2557421326637268,
      "learning_rate": 4.787666666666667e-05,
      "loss": 0.0026,
      "step": 6370
    },
    {
      "epoch": 0.34026666666666666,
      "grad_norm": 0.22059713304042816,
      "learning_rate": 4.7873333333333335e-05,
      "loss": 0.0045,
      "step": 6380
    },
    {
      "epoch": 0.3408,
      "grad_norm": 0.06141296774148941,
      "learning_rate": 4.787e-05,
      "loss": 0.0044,
      "step": 6390
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 0.3165772557258606,
      "learning_rate": 4.7866666666666674e-05,
      "loss": 0.0035,
      "step": 6400
    },
    {
      "epoch": 0.34186666666666665,
      "grad_norm": 0.836124062538147,
      "learning_rate": 4.786333333333334e-05,
      "loss": 0.0041,
      "step": 6410
    },
    {
      "epoch": 0.3424,
      "grad_norm": 0.17235849797725677,
      "learning_rate": 4.7860000000000006e-05,
      "loss": 0.0052,
      "step": 6420
    },
    {
      "epoch": 0.3429333333333333,
      "grad_norm": 0.10085279494524002,
      "learning_rate": 4.7856666666666665e-05,
      "loss": 0.0047,
      "step": 6430
    },
    {
      "epoch": 0.34346666666666664,
      "grad_norm": 0.6381098031997681,
      "learning_rate": 4.785333333333333e-05,
      "loss": 0.0036,
      "step": 6440
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.3756939172744751,
      "learning_rate": 4.785e-05,
      "loss": 0.0042,
      "step": 6450
    },
    {
      "epoch": 0.34453333333333336,
      "grad_norm": 0.1965000182390213,
      "learning_rate": 4.784666666666667e-05,
      "loss": 0.0033,
      "step": 6460
    },
    {
      "epoch": 0.3450666666666667,
      "grad_norm": 0.7677521109580994,
      "learning_rate": 4.784333333333334e-05,
      "loss": 0.0037,
      "step": 6470
    },
    {
      "epoch": 0.3456,
      "grad_norm": 0.6065424084663391,
      "learning_rate": 4.784e-05,
      "loss": 0.0037,
      "step": 6480
    },
    {
      "epoch": 0.34613333333333335,
      "grad_norm": 0.35399821400642395,
      "learning_rate": 4.783666666666667e-05,
      "loss": 0.003,
      "step": 6490
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.7125318646430969,
      "learning_rate": 4.7833333333333335e-05,
      "loss": 0.0037,
      "step": 6500
    },
    {
      "epoch": 0.3472,
      "grad_norm": 0.5342181324958801,
      "learning_rate": 4.783e-05,
      "loss": 0.0041,
      "step": 6510
    },
    {
      "epoch": 0.34773333333333334,
      "grad_norm": 0.30830779671669006,
      "learning_rate": 4.782666666666667e-05,
      "loss": 0.0039,
      "step": 6520
    },
    {
      "epoch": 0.34826666666666667,
      "grad_norm": 0.38337618112564087,
      "learning_rate": 4.7823333333333333e-05,
      "loss": 0.0035,
      "step": 6530
    },
    {
      "epoch": 0.3488,
      "grad_norm": 1.0409443378448486,
      "learning_rate": 4.7820000000000006e-05,
      "loss": 0.0046,
      "step": 6540
    },
    {
      "epoch": 0.34933333333333333,
      "grad_norm": 1.2129672765731812,
      "learning_rate": 4.781666666666667e-05,
      "loss": 0.0036,
      "step": 6550
    },
    {
      "epoch": 0.34986666666666666,
      "grad_norm": 0.36524900794029236,
      "learning_rate": 4.781333333333334e-05,
      "loss": 0.0044,
      "step": 6560
    },
    {
      "epoch": 0.3504,
      "grad_norm": 0.5558273196220398,
      "learning_rate": 4.7810000000000005e-05,
      "loss": 0.0029,
      "step": 6570
    },
    {
      "epoch": 0.3509333333333333,
      "grad_norm": 0.1949659287929535,
      "learning_rate": 4.7806666666666664e-05,
      "loss": 0.0025,
      "step": 6580
    },
    {
      "epoch": 0.35146666666666665,
      "grad_norm": 0.3744828402996063,
      "learning_rate": 4.780333333333333e-05,
      "loss": 0.0033,
      "step": 6590
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.11432523280382156,
      "learning_rate": 4.78e-05,
      "loss": 0.0032,
      "step": 6600
    },
    {
      "epoch": 0.3525333333333333,
      "grad_norm": 0.10865651071071625,
      "learning_rate": 4.779666666666667e-05,
      "loss": 0.003,
      "step": 6610
    },
    {
      "epoch": 0.35306666666666664,
      "grad_norm": 0.2009645700454712,
      "learning_rate": 4.7793333333333335e-05,
      "loss": 0.0042,
      "step": 6620
    },
    {
      "epoch": 0.3536,
      "grad_norm": 0.2463858425617218,
      "learning_rate": 4.779e-05,
      "loss": 0.0038,
      "step": 6630
    },
    {
      "epoch": 0.35413333333333336,
      "grad_norm": 0.3291114270687103,
      "learning_rate": 4.778666666666667e-05,
      "loss": 0.0034,
      "step": 6640
    },
    {
      "epoch": 0.3546666666666667,
      "grad_norm": 0.22894088923931122,
      "learning_rate": 4.7783333333333334e-05,
      "loss": 0.0049,
      "step": 6650
    },
    {
      "epoch": 0.3552,
      "grad_norm": 0.1710537075996399,
      "learning_rate": 4.778e-05,
      "loss": 0.004,
      "step": 6660
    },
    {
      "epoch": 0.35573333333333335,
      "grad_norm": 0.35522139072418213,
      "learning_rate": 4.777666666666667e-05,
      "loss": 0.0036,
      "step": 6670
    },
    {
      "epoch": 0.3562666666666667,
      "grad_norm": 0.1480759233236313,
      "learning_rate": 4.777333333333334e-05,
      "loss": 0.0035,
      "step": 6680
    },
    {
      "epoch": 0.3568,
      "grad_norm": 0.15147212147712708,
      "learning_rate": 4.7770000000000005e-05,
      "loss": 0.0045,
      "step": 6690
    },
    {
      "epoch": 0.35733333333333334,
      "grad_norm": 0.1230386346578598,
      "learning_rate": 4.776666666666667e-05,
      "loss": 0.0038,
      "step": 6700
    },
    {
      "epoch": 0.35786666666666667,
      "grad_norm": 0.32284748554229736,
      "learning_rate": 4.776333333333334e-05,
      "loss": 0.0044,
      "step": 6710
    },
    {
      "epoch": 0.3584,
      "grad_norm": 0.289699912071228,
      "learning_rate": 4.7760000000000004e-05,
      "loss": 0.0038,
      "step": 6720
    },
    {
      "epoch": 0.3589333333333333,
      "grad_norm": 0.8280141353607178,
      "learning_rate": 4.775666666666666e-05,
      "loss": 0.0022,
      "step": 6730
    },
    {
      "epoch": 0.35946666666666666,
      "grad_norm": 0.42294642329216003,
      "learning_rate": 4.7753333333333336e-05,
      "loss": 0.0045,
      "step": 6740
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.5980842113494873,
      "learning_rate": 4.775e-05,
      "loss": 0.0038,
      "step": 6750
    },
    {
      "epoch": 0.3605333333333333,
      "grad_norm": 0.7258622050285339,
      "learning_rate": 4.774666666666667e-05,
      "loss": 0.0048,
      "step": 6760
    },
    {
      "epoch": 0.36106666666666665,
      "grad_norm": 0.3076753318309784,
      "learning_rate": 4.7743333333333334e-05,
      "loss": 0.0035,
      "step": 6770
    },
    {
      "epoch": 0.3616,
      "grad_norm": 0.1717953234910965,
      "learning_rate": 4.774e-05,
      "loss": 0.004,
      "step": 6780
    },
    {
      "epoch": 0.3621333333333333,
      "grad_norm": 0.14267176389694214,
      "learning_rate": 4.7736666666666666e-05,
      "loss": 0.0047,
      "step": 6790
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 0.29714977741241455,
      "learning_rate": 4.773333333333333e-05,
      "loss": 0.0052,
      "step": 6800
    },
    {
      "epoch": 0.3632,
      "grad_norm": 0.9159917831420898,
      "learning_rate": 4.7730000000000005e-05,
      "loss": 0.0039,
      "step": 6810
    },
    {
      "epoch": 0.36373333333333335,
      "grad_norm": 0.9354510307312012,
      "learning_rate": 4.772666666666667e-05,
      "loss": 0.0043,
      "step": 6820
    },
    {
      "epoch": 0.3642666666666667,
      "grad_norm": 1.1755452156066895,
      "learning_rate": 4.772333333333334e-05,
      "loss": 0.0036,
      "step": 6830
    },
    {
      "epoch": 0.3648,
      "grad_norm": 0.8661297559738159,
      "learning_rate": 4.7720000000000004e-05,
      "loss": 0.003,
      "step": 6840
    },
    {
      "epoch": 0.36533333333333334,
      "grad_norm": 0.9153942465782166,
      "learning_rate": 4.771666666666667e-05,
      "loss": 0.0046,
      "step": 6850
    },
    {
      "epoch": 0.3658666666666667,
      "grad_norm": 1.0941585302352905,
      "learning_rate": 4.7713333333333336e-05,
      "loss": 0.0028,
      "step": 6860
    },
    {
      "epoch": 0.3664,
      "grad_norm": 0.6865953803062439,
      "learning_rate": 4.771e-05,
      "loss": 0.0036,
      "step": 6870
    },
    {
      "epoch": 0.36693333333333333,
      "grad_norm": 0.7085637450218201,
      "learning_rate": 4.770666666666667e-05,
      "loss": 0.0041,
      "step": 6880
    },
    {
      "epoch": 0.36746666666666666,
      "grad_norm": 0.5570452213287354,
      "learning_rate": 4.7703333333333335e-05,
      "loss": 0.0037,
      "step": 6890
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.12616600096225739,
      "learning_rate": 4.77e-05,
      "loss": 0.0044,
      "step": 6900
    },
    {
      "epoch": 0.3685333333333333,
      "grad_norm": 0.09077522158622742,
      "learning_rate": 4.769666666666667e-05,
      "loss": 0.0027,
      "step": 6910
    },
    {
      "epoch": 0.36906666666666665,
      "grad_norm": 0.38819408416748047,
      "learning_rate": 4.769333333333333e-05,
      "loss": 0.0041,
      "step": 6920
    },
    {
      "epoch": 0.3696,
      "grad_norm": 0.2281941920518875,
      "learning_rate": 4.769e-05,
      "loss": 0.003,
      "step": 6930
    },
    {
      "epoch": 0.3701333333333333,
      "grad_norm": 0.06885231286287308,
      "learning_rate": 4.7686666666666665e-05,
      "loss": 0.0032,
      "step": 6940
    },
    {
      "epoch": 0.37066666666666664,
      "grad_norm": 0.3902123272418976,
      "learning_rate": 4.768333333333334e-05,
      "loss": 0.0032,
      "step": 6950
    },
    {
      "epoch": 0.3712,
      "grad_norm": 0.46509605646133423,
      "learning_rate": 4.7680000000000004e-05,
      "loss": 0.0034,
      "step": 6960
    },
    {
      "epoch": 0.37173333333333336,
      "grad_norm": 0.41679900884628296,
      "learning_rate": 4.767666666666667e-05,
      "loss": 0.0033,
      "step": 6970
    },
    {
      "epoch": 0.3722666666666667,
      "grad_norm": 0.29634103178977966,
      "learning_rate": 4.7673333333333337e-05,
      "loss": 0.0037,
      "step": 6980
    },
    {
      "epoch": 0.3728,
      "grad_norm": 1.1322499513626099,
      "learning_rate": 4.767e-05,
      "loss": 0.0055,
      "step": 6990
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.5830715298652649,
      "learning_rate": 4.766666666666667e-05,
      "loss": 0.0047,
      "step": 7000
    },
    {
      "epoch": 0.3738666666666667,
      "grad_norm": 0.9242744445800781,
      "learning_rate": 4.7663333333333335e-05,
      "loss": 0.0044,
      "step": 7010
    },
    {
      "epoch": 0.3744,
      "grad_norm": 0.8029153943061829,
      "learning_rate": 4.766000000000001e-05,
      "loss": 0.004,
      "step": 7020
    },
    {
      "epoch": 0.37493333333333334,
      "grad_norm": 0.8395413756370544,
      "learning_rate": 4.7656666666666674e-05,
      "loss": 0.0025,
      "step": 7030
    },
    {
      "epoch": 0.37546666666666667,
      "grad_norm": 0.18190203607082367,
      "learning_rate": 4.765333333333333e-05,
      "loss": 0.0036,
      "step": 7040
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.45639336109161377,
      "learning_rate": 4.765e-05,
      "loss": 0.0037,
      "step": 7050
    },
    {
      "epoch": 0.37653333333333333,
      "grad_norm": 0.6250748634338379,
      "learning_rate": 4.7646666666666666e-05,
      "loss": 0.005,
      "step": 7060
    },
    {
      "epoch": 0.37706666666666666,
      "grad_norm": 0.5977830290794373,
      "learning_rate": 4.764333333333333e-05,
      "loss": 0.0035,
      "step": 7070
    },
    {
      "epoch": 0.3776,
      "grad_norm": 0.08376684039831161,
      "learning_rate": 4.7640000000000005e-05,
      "loss": 0.0026,
      "step": 7080
    },
    {
      "epoch": 0.3781333333333333,
      "grad_norm": 0.6254087686538696,
      "learning_rate": 4.763666666666667e-05,
      "loss": 0.0035,
      "step": 7090
    },
    {
      "epoch": 0.37866666666666665,
      "grad_norm": 0.3261668384075165,
      "learning_rate": 4.763333333333334e-05,
      "loss": 0.0042,
      "step": 7100
    },
    {
      "epoch": 0.3792,
      "grad_norm": 0.41735410690307617,
      "learning_rate": 4.763e-05,
      "loss": 0.0022,
      "step": 7110
    },
    {
      "epoch": 0.3797333333333333,
      "grad_norm": 0.547821581363678,
      "learning_rate": 4.762666666666667e-05,
      "loss": 0.0047,
      "step": 7120
    },
    {
      "epoch": 0.38026666666666664,
      "grad_norm": 0.2297699749469757,
      "learning_rate": 4.7623333333333335e-05,
      "loss": 0.0041,
      "step": 7130
    },
    {
      "epoch": 0.3808,
      "grad_norm": 0.792047381401062,
      "learning_rate": 4.762e-05,
      "loss": 0.0041,
      "step": 7140
    },
    {
      "epoch": 0.38133333333333336,
      "grad_norm": 0.3013482987880707,
      "learning_rate": 4.761666666666667e-05,
      "loss": 0.0039,
      "step": 7150
    },
    {
      "epoch": 0.3818666666666667,
      "grad_norm": 0.39970412850379944,
      "learning_rate": 4.761333333333334e-05,
      "loss": 0.003,
      "step": 7160
    },
    {
      "epoch": 0.3824,
      "grad_norm": 0.5211403965950012,
      "learning_rate": 4.761000000000001e-05,
      "loss": 0.0042,
      "step": 7170
    },
    {
      "epoch": 0.38293333333333335,
      "grad_norm": 0.8605020046234131,
      "learning_rate": 4.760666666666667e-05,
      "loss": 0.0035,
      "step": 7180
    },
    {
      "epoch": 0.3834666666666667,
      "grad_norm": 0.5211737155914307,
      "learning_rate": 4.760333333333333e-05,
      "loss": 0.0028,
      "step": 7190
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.16127760708332062,
      "learning_rate": 4.76e-05,
      "loss": 0.0039,
      "step": 7200
    },
    {
      "epoch": 0.38453333333333334,
      "grad_norm": 0.11877124011516571,
      "learning_rate": 4.7596666666666664e-05,
      "loss": 0.0026,
      "step": 7210
    },
    {
      "epoch": 0.38506666666666667,
      "grad_norm": 0.1694820374250412,
      "learning_rate": 4.759333333333334e-05,
      "loss": 0.0032,
      "step": 7220
    },
    {
      "epoch": 0.3856,
      "grad_norm": 0.29258492588996887,
      "learning_rate": 4.7590000000000003e-05,
      "loss": 0.0028,
      "step": 7230
    },
    {
      "epoch": 0.38613333333333333,
      "grad_norm": 0.33037835359573364,
      "learning_rate": 4.758666666666667e-05,
      "loss": 0.0044,
      "step": 7240
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.1899573653936386,
      "learning_rate": 4.7583333333333336e-05,
      "loss": 0.0041,
      "step": 7250
    },
    {
      "epoch": 0.3872,
      "grad_norm": 0.24443869292736053,
      "learning_rate": 4.758e-05,
      "loss": 0.0025,
      "step": 7260
    },
    {
      "epoch": 0.3877333333333333,
      "grad_norm": 0.18020324409008026,
      "learning_rate": 4.757666666666667e-05,
      "loss": 0.0031,
      "step": 7270
    },
    {
      "epoch": 0.38826666666666665,
      "grad_norm": 0.21739384531974792,
      "learning_rate": 4.7573333333333334e-05,
      "loss": 0.0036,
      "step": 7280
    },
    {
      "epoch": 0.3888,
      "grad_norm": 0.4541400671005249,
      "learning_rate": 4.757e-05,
      "loss": 0.0037,
      "step": 7290
    },
    {
      "epoch": 0.3893333333333333,
      "grad_norm": 0.9846683144569397,
      "learning_rate": 4.756666666666667e-05,
      "loss": 0.0036,
      "step": 7300
    },
    {
      "epoch": 0.38986666666666664,
      "grad_norm": 0.6530937552452087,
      "learning_rate": 4.756333333333334e-05,
      "loss": 0.0034,
      "step": 7310
    },
    {
      "epoch": 0.3904,
      "grad_norm": 0.09503703564405441,
      "learning_rate": 4.7560000000000005e-05,
      "loss": 0.0034,
      "step": 7320
    },
    {
      "epoch": 0.39093333333333335,
      "grad_norm": 0.6006080508232117,
      "learning_rate": 4.755666666666667e-05,
      "loss": 0.0043,
      "step": 7330
    },
    {
      "epoch": 0.3914666666666667,
      "grad_norm": 0.7041001319885254,
      "learning_rate": 4.755333333333333e-05,
      "loss": 0.0031,
      "step": 7340
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.23308992385864258,
      "learning_rate": 4.755e-05,
      "loss": 0.0037,
      "step": 7350
    },
    {
      "epoch": 0.39253333333333335,
      "grad_norm": 0.6020098924636841,
      "learning_rate": 4.754666666666667e-05,
      "loss": 0.0047,
      "step": 7360
    },
    {
      "epoch": 0.3930666666666667,
      "grad_norm": 0.42684441804885864,
      "learning_rate": 4.7543333333333336e-05,
      "loss": 0.0035,
      "step": 7370
    },
    {
      "epoch": 0.3936,
      "grad_norm": 0.19453325867652893,
      "learning_rate": 4.754e-05,
      "loss": 0.0041,
      "step": 7380
    },
    {
      "epoch": 0.39413333333333334,
      "grad_norm": 1.23699152469635,
      "learning_rate": 4.753666666666667e-05,
      "loss": 0.0048,
      "step": 7390
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 0.13974185287952423,
      "learning_rate": 4.7533333333333334e-05,
      "loss": 0.003,
      "step": 7400
    },
    {
      "epoch": 0.3952,
      "grad_norm": 0.10724931955337524,
      "learning_rate": 4.753e-05,
      "loss": 0.0034,
      "step": 7410
    },
    {
      "epoch": 0.3957333333333333,
      "grad_norm": 0.39018887281417847,
      "learning_rate": 4.752666666666667e-05,
      "loss": 0.0042,
      "step": 7420
    },
    {
      "epoch": 0.39626666666666666,
      "grad_norm": 0.10334064811468124,
      "learning_rate": 4.752333333333334e-05,
      "loss": 0.0035,
      "step": 7430
    },
    {
      "epoch": 0.3968,
      "grad_norm": 0.3171498775482178,
      "learning_rate": 4.7520000000000006e-05,
      "loss": 0.0039,
      "step": 7440
    },
    {
      "epoch": 0.3973333333333333,
      "grad_norm": 0.6532549858093262,
      "learning_rate": 4.751666666666667e-05,
      "loss": 0.0041,
      "step": 7450
    },
    {
      "epoch": 0.39786666666666665,
      "grad_norm": 1.0002400875091553,
      "learning_rate": 4.751333333333334e-05,
      "loss": 0.0038,
      "step": 7460
    },
    {
      "epoch": 0.3984,
      "grad_norm": 0.8349484205245972,
      "learning_rate": 4.7510000000000004e-05,
      "loss": 0.0037,
      "step": 7470
    },
    {
      "epoch": 0.3989333333333333,
      "grad_norm": 0.2275080680847168,
      "learning_rate": 4.750666666666667e-05,
      "loss": 0.0045,
      "step": 7480
    },
    {
      "epoch": 0.3994666666666667,
      "grad_norm": 0.694898247718811,
      "learning_rate": 4.750333333333333e-05,
      "loss": 0.0036,
      "step": 7490
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.9519541263580322,
      "learning_rate": 4.75e-05,
      "loss": 0.0028,
      "step": 7500
    },
    {
      "epoch": 0.40053333333333335,
      "grad_norm": 0.3546449840068817,
      "learning_rate": 4.749666666666667e-05,
      "loss": 0.0037,
      "step": 7510
    },
    {
      "epoch": 0.4010666666666667,
      "grad_norm": 0.354533851146698,
      "learning_rate": 4.7493333333333335e-05,
      "loss": 0.0038,
      "step": 7520
    },
    {
      "epoch": 0.4016,
      "grad_norm": 0.2211156189441681,
      "learning_rate": 4.749e-05,
      "loss": 0.0041,
      "step": 7530
    },
    {
      "epoch": 0.40213333333333334,
      "grad_norm": 0.4176037609577179,
      "learning_rate": 4.748666666666667e-05,
      "loss": 0.0035,
      "step": 7540
    },
    {
      "epoch": 0.4026666666666667,
      "grad_norm": 0.5209576487541199,
      "learning_rate": 4.748333333333333e-05,
      "loss": 0.0032,
      "step": 7550
    },
    {
      "epoch": 0.4032,
      "grad_norm": 0.2919103503227234,
      "learning_rate": 4.748e-05,
      "loss": 0.0025,
      "step": 7560
    },
    {
      "epoch": 0.40373333333333333,
      "grad_norm": 0.4001545011997223,
      "learning_rate": 4.747666666666667e-05,
      "loss": 0.0046,
      "step": 7570
    },
    {
      "epoch": 0.40426666666666666,
      "grad_norm": 0.7097654938697815,
      "learning_rate": 4.747333333333334e-05,
      "loss": 0.0039,
      "step": 7580
    },
    {
      "epoch": 0.4048,
      "grad_norm": 0.25960248708724976,
      "learning_rate": 4.7470000000000005e-05,
      "loss": 0.003,
      "step": 7590
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 0.09720176458358765,
      "learning_rate": 4.746666666666667e-05,
      "loss": 0.0041,
      "step": 7600
    },
    {
      "epoch": 0.40586666666666665,
      "grad_norm": 0.2740882933139801,
      "learning_rate": 4.746333333333334e-05,
      "loss": 0.0034,
      "step": 7610
    },
    {
      "epoch": 0.4064,
      "grad_norm": 0.38193991780281067,
      "learning_rate": 4.746e-05,
      "loss": 0.0037,
      "step": 7620
    },
    {
      "epoch": 0.4069333333333333,
      "grad_norm": 0.13904473185539246,
      "learning_rate": 4.745666666666667e-05,
      "loss": 0.0049,
      "step": 7630
    },
    {
      "epoch": 0.40746666666666664,
      "grad_norm": 0.2981072664260864,
      "learning_rate": 4.7453333333333335e-05,
      "loss": 0.0031,
      "step": 7640
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.2879525423049927,
      "learning_rate": 4.745e-05,
      "loss": 0.0046,
      "step": 7650
    },
    {
      "epoch": 0.40853333333333336,
      "grad_norm": 0.37784144282341003,
      "learning_rate": 4.744666666666667e-05,
      "loss": 0.0037,
      "step": 7660
    },
    {
      "epoch": 0.4090666666666667,
      "grad_norm": 0.7467372417449951,
      "learning_rate": 4.7443333333333334e-05,
      "loss": 0.0041,
      "step": 7670
    },
    {
      "epoch": 0.4096,
      "grad_norm": 0.38257285952568054,
      "learning_rate": 4.744e-05,
      "loss": 0.003,
      "step": 7680
    },
    {
      "epoch": 0.41013333333333335,
      "grad_norm": 0.3648962080478668,
      "learning_rate": 4.7436666666666666e-05,
      "loss": 0.0043,
      "step": 7690
    },
    {
      "epoch": 0.4106666666666667,
      "grad_norm": 0.45715999603271484,
      "learning_rate": 4.743333333333333e-05,
      "loss": 0.0042,
      "step": 7700
    },
    {
      "epoch": 0.4112,
      "grad_norm": 0.44486474990844727,
      "learning_rate": 4.7430000000000005e-05,
      "loss": 0.0031,
      "step": 7710
    },
    {
      "epoch": 0.41173333333333334,
      "grad_norm": 0.25419023633003235,
      "learning_rate": 4.742666666666667e-05,
      "loss": 0.0032,
      "step": 7720
    },
    {
      "epoch": 0.41226666666666667,
      "grad_norm": 0.29560643434524536,
      "learning_rate": 4.742333333333334e-05,
      "loss": 0.0047,
      "step": 7730
    },
    {
      "epoch": 0.4128,
      "grad_norm": 0.2227238565683365,
      "learning_rate": 4.742e-05,
      "loss": 0.0045,
      "step": 7740
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.7920831441879272,
      "learning_rate": 4.741666666666667e-05,
      "loss": 0.0031,
      "step": 7750
    },
    {
      "epoch": 0.41386666666666666,
      "grad_norm": 0.11386442184448242,
      "learning_rate": 4.7413333333333336e-05,
      "loss": 0.0031,
      "step": 7760
    },
    {
      "epoch": 0.4144,
      "grad_norm": 1.0839745998382568,
      "learning_rate": 4.741e-05,
      "loss": 0.0034,
      "step": 7770
    },
    {
      "epoch": 0.4149333333333333,
      "grad_norm": 0.4303310215473175,
      "learning_rate": 4.7406666666666675e-05,
      "loss": 0.0037,
      "step": 7780
    },
    {
      "epoch": 0.41546666666666665,
      "grad_norm": 0.22755593061447144,
      "learning_rate": 4.7403333333333334e-05,
      "loss": 0.0038,
      "step": 7790
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.818048357963562,
      "learning_rate": 4.74e-05,
      "loss": 0.0043,
      "step": 7800
    },
    {
      "epoch": 0.4165333333333333,
      "grad_norm": 0.21149654686450958,
      "learning_rate": 4.7396666666666666e-05,
      "loss": 0.0042,
      "step": 7810
    },
    {
      "epoch": 0.41706666666666664,
      "grad_norm": 0.5401405096054077,
      "learning_rate": 4.739333333333333e-05,
      "loss": 0.0048,
      "step": 7820
    },
    {
      "epoch": 0.4176,
      "grad_norm": 0.4201733469963074,
      "learning_rate": 4.739e-05,
      "loss": 0.0035,
      "step": 7830
    },
    {
      "epoch": 0.41813333333333336,
      "grad_norm": 0.5118497610092163,
      "learning_rate": 4.7386666666666665e-05,
      "loss": 0.0026,
      "step": 7840
    },
    {
      "epoch": 0.4186666666666667,
      "grad_norm": 0.06987258791923523,
      "learning_rate": 4.738333333333334e-05,
      "loss": 0.0039,
      "step": 7850
    },
    {
      "epoch": 0.4192,
      "grad_norm": 0.33143773674964905,
      "learning_rate": 4.7380000000000004e-05,
      "loss": 0.003,
      "step": 7860
    },
    {
      "epoch": 0.41973333333333335,
      "grad_norm": 0.2543033957481384,
      "learning_rate": 4.737666666666667e-05,
      "loss": 0.0038,
      "step": 7870
    },
    {
      "epoch": 0.4202666666666667,
      "grad_norm": 0.07505074143409729,
      "learning_rate": 4.7373333333333336e-05,
      "loss": 0.0025,
      "step": 7880
    },
    {
      "epoch": 0.4208,
      "grad_norm": 0.4390392601490021,
      "learning_rate": 4.737e-05,
      "loss": 0.0031,
      "step": 7890
    },
    {
      "epoch": 0.42133333333333334,
      "grad_norm": 0.3380448520183563,
      "learning_rate": 4.736666666666667e-05,
      "loss": 0.004,
      "step": 7900
    },
    {
      "epoch": 0.42186666666666667,
      "grad_norm": 0.3440169095993042,
      "learning_rate": 4.7363333333333334e-05,
      "loss": 0.0046,
      "step": 7910
    },
    {
      "epoch": 0.4224,
      "grad_norm": 0.440904825925827,
      "learning_rate": 4.736000000000001e-05,
      "loss": 0.0033,
      "step": 7920
    },
    {
      "epoch": 0.42293333333333333,
      "grad_norm": 0.47130337357521057,
      "learning_rate": 4.7356666666666673e-05,
      "loss": 0.0031,
      "step": 7930
    },
    {
      "epoch": 0.42346666666666666,
      "grad_norm": 0.24639342725276947,
      "learning_rate": 4.735333333333333e-05,
      "loss": 0.0041,
      "step": 7940
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.3782227337360382,
      "learning_rate": 4.735e-05,
      "loss": 0.0038,
      "step": 7950
    },
    {
      "epoch": 0.4245333333333333,
      "grad_norm": 0.3218897879123688,
      "learning_rate": 4.7346666666666665e-05,
      "loss": 0.0036,
      "step": 7960
    },
    {
      "epoch": 0.42506666666666665,
      "grad_norm": 0.1077534630894661,
      "learning_rate": 4.734333333333333e-05,
      "loss": 0.0033,
      "step": 7970
    },
    {
      "epoch": 0.4256,
      "grad_norm": 0.3470708727836609,
      "learning_rate": 4.7340000000000004e-05,
      "loss": 0.0043,
      "step": 7980
    },
    {
      "epoch": 0.4261333333333333,
      "grad_norm": 0.6530834436416626,
      "learning_rate": 4.733666666666667e-05,
      "loss": 0.0038,
      "step": 7990
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.4789567291736603,
      "learning_rate": 4.7333333333333336e-05,
      "loss": 0.0036,
      "step": 8000
    },
    {
      "epoch": 0.4272,
      "grad_norm": 0.4804806113243103,
      "learning_rate": 4.733e-05,
      "loss": 0.0034,
      "step": 8010
    },
    {
      "epoch": 0.42773333333333335,
      "grad_norm": 0.23849202692508698,
      "learning_rate": 4.732666666666667e-05,
      "loss": 0.0045,
      "step": 8020
    },
    {
      "epoch": 0.4282666666666667,
      "grad_norm": 0.28586384654045105,
      "learning_rate": 4.7323333333333335e-05,
      "loss": 0.0046,
      "step": 8030
    },
    {
      "epoch": 0.4288,
      "grad_norm": 0.22271138429641724,
      "learning_rate": 4.732e-05,
      "loss": 0.0035,
      "step": 8040
    },
    {
      "epoch": 0.42933333333333334,
      "grad_norm": 0.08882907032966614,
      "learning_rate": 4.731666666666667e-05,
      "loss": 0.0041,
      "step": 8050
    },
    {
      "epoch": 0.4298666666666667,
      "grad_norm": 0.27042731642723083,
      "learning_rate": 4.731333333333334e-05,
      "loss": 0.003,
      "step": 8060
    },
    {
      "epoch": 0.4304,
      "grad_norm": 0.59272301197052,
      "learning_rate": 4.7310000000000006e-05,
      "loss": 0.0036,
      "step": 8070
    },
    {
      "epoch": 0.43093333333333333,
      "grad_norm": 0.8468230962753296,
      "learning_rate": 4.730666666666667e-05,
      "loss": 0.004,
      "step": 8080
    },
    {
      "epoch": 0.43146666666666667,
      "grad_norm": 0.39353707432746887,
      "learning_rate": 4.730333333333333e-05,
      "loss": 0.0024,
      "step": 8090
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.3048233091831207,
      "learning_rate": 4.73e-05,
      "loss": 0.0035,
      "step": 8100
    },
    {
      "epoch": 0.4325333333333333,
      "grad_norm": 0.7198594808578491,
      "learning_rate": 4.7296666666666664e-05,
      "loss": 0.0046,
      "step": 8110
    },
    {
      "epoch": 0.43306666666666666,
      "grad_norm": 0.22492052614688873,
      "learning_rate": 4.729333333333334e-05,
      "loss": 0.0042,
      "step": 8120
    },
    {
      "epoch": 0.4336,
      "grad_norm": 0.5155749917030334,
      "learning_rate": 4.729e-05,
      "loss": 0.0038,
      "step": 8130
    },
    {
      "epoch": 0.4341333333333333,
      "grad_norm": 0.1696212887763977,
      "learning_rate": 4.728666666666667e-05,
      "loss": 0.005,
      "step": 8140
    },
    {
      "epoch": 0.43466666666666665,
      "grad_norm": 0.15598319470882416,
      "learning_rate": 4.7283333333333335e-05,
      "loss": 0.004,
      "step": 8150
    },
    {
      "epoch": 0.4352,
      "grad_norm": 0.25103074312210083,
      "learning_rate": 4.728e-05,
      "loss": 0.0023,
      "step": 8160
    },
    {
      "epoch": 0.4357333333333333,
      "grad_norm": 0.12966126203536987,
      "learning_rate": 4.727666666666667e-05,
      "loss": 0.0031,
      "step": 8170
    },
    {
      "epoch": 0.4362666666666667,
      "grad_norm": 0.5111042261123657,
      "learning_rate": 4.7273333333333334e-05,
      "loss": 0.0048,
      "step": 8180
    },
    {
      "epoch": 0.4368,
      "grad_norm": 0.0994664654135704,
      "learning_rate": 4.7270000000000007e-05,
      "loss": 0.0039,
      "step": 8190
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 0.1701057106256485,
      "learning_rate": 4.726666666666667e-05,
      "loss": 0.0047,
      "step": 8200
    },
    {
      "epoch": 0.4378666666666667,
      "grad_norm": 0.8395800590515137,
      "learning_rate": 4.726333333333334e-05,
      "loss": 0.0042,
      "step": 8210
    },
    {
      "epoch": 0.4384,
      "grad_norm": 0.25501030683517456,
      "learning_rate": 4.7260000000000005e-05,
      "loss": 0.0028,
      "step": 8220
    },
    {
      "epoch": 0.43893333333333334,
      "grad_norm": 0.4144555628299713,
      "learning_rate": 4.725666666666667e-05,
      "loss": 0.004,
      "step": 8230
    },
    {
      "epoch": 0.43946666666666667,
      "grad_norm": 0.17159079015254974,
      "learning_rate": 4.725333333333334e-05,
      "loss": 0.0032,
      "step": 8240
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.8897305130958557,
      "learning_rate": 4.7249999999999997e-05,
      "loss": 0.0039,
      "step": 8250
    },
    {
      "epoch": 0.44053333333333333,
      "grad_norm": 0.5061072111129761,
      "learning_rate": 4.724666666666667e-05,
      "loss": 0.0032,
      "step": 8260
    },
    {
      "epoch": 0.44106666666666666,
      "grad_norm": 0.8171904683113098,
      "learning_rate": 4.7243333333333336e-05,
      "loss": 0.0043,
      "step": 8270
    },
    {
      "epoch": 0.4416,
      "grad_norm": 0.15437553822994232,
      "learning_rate": 4.724e-05,
      "loss": 0.004,
      "step": 8280
    },
    {
      "epoch": 0.4421333333333333,
      "grad_norm": 0.8807840347290039,
      "learning_rate": 4.723666666666667e-05,
      "loss": 0.0042,
      "step": 8290
    },
    {
      "epoch": 0.44266666666666665,
      "grad_norm": 0.14893081784248352,
      "learning_rate": 4.7233333333333334e-05,
      "loss": 0.0046,
      "step": 8300
    },
    {
      "epoch": 0.4432,
      "grad_norm": 0.4134383499622345,
      "learning_rate": 4.723e-05,
      "loss": 0.0034,
      "step": 8310
    },
    {
      "epoch": 0.4437333333333333,
      "grad_norm": 0.12277045100927353,
      "learning_rate": 4.7226666666666666e-05,
      "loss": 0.0036,
      "step": 8320
    },
    {
      "epoch": 0.44426666666666664,
      "grad_norm": 0.33187007904052734,
      "learning_rate": 4.722333333333334e-05,
      "loss": 0.0035,
      "step": 8330
    },
    {
      "epoch": 0.4448,
      "grad_norm": 0.386401891708374,
      "learning_rate": 4.7220000000000005e-05,
      "loss": 0.0042,
      "step": 8340
    },
    {
      "epoch": 0.44533333333333336,
      "grad_norm": 0.1597309559583664,
      "learning_rate": 4.721666666666667e-05,
      "loss": 0.0023,
      "step": 8350
    },
    {
      "epoch": 0.4458666666666667,
      "grad_norm": 0.27727240324020386,
      "learning_rate": 4.721333333333334e-05,
      "loss": 0.0022,
      "step": 8360
    },
    {
      "epoch": 0.4464,
      "grad_norm": 0.21590940654277802,
      "learning_rate": 4.7210000000000004e-05,
      "loss": 0.0038,
      "step": 8370
    },
    {
      "epoch": 0.44693333333333335,
      "grad_norm": 0.41561973094940186,
      "learning_rate": 4.720666666666667e-05,
      "loss": 0.0026,
      "step": 8380
    },
    {
      "epoch": 0.4474666666666667,
      "grad_norm": 0.2455652952194214,
      "learning_rate": 4.7203333333333336e-05,
      "loss": 0.0027,
      "step": 8390
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.7263681888580322,
      "learning_rate": 4.72e-05,
      "loss": 0.0045,
      "step": 8400
    },
    {
      "epoch": 0.44853333333333334,
      "grad_norm": 0.14884234964847565,
      "learning_rate": 4.719666666666667e-05,
      "loss": 0.0025,
      "step": 8410
    },
    {
      "epoch": 0.44906666666666667,
      "grad_norm": 0.6333105564117432,
      "learning_rate": 4.7193333333333334e-05,
      "loss": 0.0034,
      "step": 8420
    },
    {
      "epoch": 0.4496,
      "grad_norm": 0.4717446565628052,
      "learning_rate": 4.719e-05,
      "loss": 0.0037,
      "step": 8430
    },
    {
      "epoch": 0.45013333333333333,
      "grad_norm": 0.3546680808067322,
      "learning_rate": 4.718666666666667e-05,
      "loss": 0.0035,
      "step": 8440
    },
    {
      "epoch": 0.45066666666666666,
      "grad_norm": 0.1315324306488037,
      "learning_rate": 4.718333333333333e-05,
      "loss": 0.0027,
      "step": 8450
    },
    {
      "epoch": 0.4512,
      "grad_norm": 0.14216040074825287,
      "learning_rate": 4.718e-05,
      "loss": 0.004,
      "step": 8460
    },
    {
      "epoch": 0.4517333333333333,
      "grad_norm": 0.16022154688835144,
      "learning_rate": 4.717666666666667e-05,
      "loss": 0.0033,
      "step": 8470
    },
    {
      "epoch": 0.45226666666666665,
      "grad_norm": 0.4020616114139557,
      "learning_rate": 4.717333333333334e-05,
      "loss": 0.003,
      "step": 8480
    },
    {
      "epoch": 0.4528,
      "grad_norm": 0.20500047504901886,
      "learning_rate": 4.7170000000000004e-05,
      "loss": 0.0046,
      "step": 8490
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.7053279876708984,
      "learning_rate": 4.716666666666667e-05,
      "loss": 0.0039,
      "step": 8500
    },
    {
      "epoch": 0.45386666666666664,
      "grad_norm": 0.2118326872587204,
      "learning_rate": 4.7163333333333336e-05,
      "loss": 0.0045,
      "step": 8510
    },
    {
      "epoch": 0.4544,
      "grad_norm": 0.08771313726902008,
      "learning_rate": 4.716e-05,
      "loss": 0.0024,
      "step": 8520
    },
    {
      "epoch": 0.45493333333333336,
      "grad_norm": 1.0181388854980469,
      "learning_rate": 4.715666666666667e-05,
      "loss": 0.0046,
      "step": 8530
    },
    {
      "epoch": 0.4554666666666667,
      "grad_norm": 0.41143229603767395,
      "learning_rate": 4.715333333333334e-05,
      "loss": 0.003,
      "step": 8540
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.06832235306501389,
      "learning_rate": 4.715e-05,
      "loss": 0.0022,
      "step": 8550
    },
    {
      "epoch": 0.45653333333333335,
      "grad_norm": 0.24716977775096893,
      "learning_rate": 4.714666666666667e-05,
      "loss": 0.0038,
      "step": 8560
    },
    {
      "epoch": 0.4570666666666667,
      "grad_norm": 0.486842542886734,
      "learning_rate": 4.714333333333333e-05,
      "loss": 0.0034,
      "step": 8570
    },
    {
      "epoch": 0.4576,
      "grad_norm": 0.26050475239753723,
      "learning_rate": 4.714e-05,
      "loss": 0.0033,
      "step": 8580
    },
    {
      "epoch": 0.45813333333333334,
      "grad_norm": 0.13420021533966064,
      "learning_rate": 4.7136666666666665e-05,
      "loss": 0.0025,
      "step": 8590
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 0.5553362369537354,
      "learning_rate": 4.713333333333333e-05,
      "loss": 0.004,
      "step": 8600
    },
    {
      "epoch": 0.4592,
      "grad_norm": 0.6259158849716187,
      "learning_rate": 4.7130000000000004e-05,
      "loss": 0.0036,
      "step": 8610
    },
    {
      "epoch": 0.4597333333333333,
      "grad_norm": 0.4771520495414734,
      "learning_rate": 4.712666666666667e-05,
      "loss": 0.0041,
      "step": 8620
    },
    {
      "epoch": 0.46026666666666666,
      "grad_norm": 0.286814421415329,
      "learning_rate": 4.712333333333334e-05,
      "loss": 0.0027,
      "step": 8630
    },
    {
      "epoch": 0.4608,
      "grad_norm": 0.676463782787323,
      "learning_rate": 4.712e-05,
      "loss": 0.0035,
      "step": 8640
    },
    {
      "epoch": 0.4613333333333333,
      "grad_norm": 0.41672220826148987,
      "learning_rate": 4.711666666666667e-05,
      "loss": 0.0027,
      "step": 8650
    },
    {
      "epoch": 0.46186666666666665,
      "grad_norm": 0.4247579872608185,
      "learning_rate": 4.7113333333333335e-05,
      "loss": 0.0033,
      "step": 8660
    },
    {
      "epoch": 0.4624,
      "grad_norm": 0.21088381111621857,
      "learning_rate": 4.711e-05,
      "loss": 0.0025,
      "step": 8670
    },
    {
      "epoch": 0.4629333333333333,
      "grad_norm": 0.17606547474861145,
      "learning_rate": 4.7106666666666674e-05,
      "loss": 0.0048,
      "step": 8680
    },
    {
      "epoch": 0.4634666666666667,
      "grad_norm": 0.4752323031425476,
      "learning_rate": 4.710333333333334e-05,
      "loss": 0.0026,
      "step": 8690
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.2212519347667694,
      "learning_rate": 4.71e-05,
      "loss": 0.0042,
      "step": 8700
    },
    {
      "epoch": 0.46453333333333335,
      "grad_norm": 0.15541382133960724,
      "learning_rate": 4.7096666666666666e-05,
      "loss": 0.0048,
      "step": 8710
    },
    {
      "epoch": 0.4650666666666667,
      "grad_norm": 0.4093964397907257,
      "learning_rate": 4.709333333333333e-05,
      "loss": 0.0037,
      "step": 8720
    },
    {
      "epoch": 0.4656,
      "grad_norm": 0.16191422939300537,
      "learning_rate": 4.709e-05,
      "loss": 0.0038,
      "step": 8730
    },
    {
      "epoch": 0.46613333333333334,
      "grad_norm": 0.35501664876937866,
      "learning_rate": 4.708666666666667e-05,
      "loss": 0.004,
      "step": 8740
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.3500075340270996,
      "learning_rate": 4.708333333333334e-05,
      "loss": 0.0028,
      "step": 8750
    },
    {
      "epoch": 0.4672,
      "grad_norm": 0.18296661972999573,
      "learning_rate": 4.708e-05,
      "loss": 0.0029,
      "step": 8760
    },
    {
      "epoch": 0.46773333333333333,
      "grad_norm": 0.4584367573261261,
      "learning_rate": 4.707666666666667e-05,
      "loss": 0.0027,
      "step": 8770
    },
    {
      "epoch": 0.46826666666666666,
      "grad_norm": 0.9370905160903931,
      "learning_rate": 4.7073333333333336e-05,
      "loss": 0.0043,
      "step": 8780
    },
    {
      "epoch": 0.4688,
      "grad_norm": 0.9418882727622986,
      "learning_rate": 4.707e-05,
      "loss": 0.004,
      "step": 8790
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 0.590410053730011,
      "learning_rate": 4.706666666666667e-05,
      "loss": 0.0028,
      "step": 8800
    },
    {
      "epoch": 0.46986666666666665,
      "grad_norm": 0.15360713005065918,
      "learning_rate": 4.7063333333333334e-05,
      "loss": 0.0047,
      "step": 8810
    },
    {
      "epoch": 0.4704,
      "grad_norm": 0.474336713552475,
      "learning_rate": 4.706000000000001e-05,
      "loss": 0.0041,
      "step": 8820
    },
    {
      "epoch": 0.4709333333333333,
      "grad_norm": 0.3349510133266449,
      "learning_rate": 4.705666666666667e-05,
      "loss": 0.0029,
      "step": 8830
    },
    {
      "epoch": 0.47146666666666665,
      "grad_norm": 1.1046826839447021,
      "learning_rate": 4.705333333333334e-05,
      "loss": 0.0043,
      "step": 8840
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.11272256076335907,
      "learning_rate": 4.705e-05,
      "loss": 0.0034,
      "step": 8850
    },
    {
      "epoch": 0.47253333333333336,
      "grad_norm": 0.06902121007442474,
      "learning_rate": 4.7046666666666665e-05,
      "loss": 0.0056,
      "step": 8860
    },
    {
      "epoch": 0.4730666666666667,
      "grad_norm": 0.2703261375427246,
      "learning_rate": 4.704333333333333e-05,
      "loss": 0.0035,
      "step": 8870
    },
    {
      "epoch": 0.4736,
      "grad_norm": 0.32433611154556274,
      "learning_rate": 4.7040000000000004e-05,
      "loss": 0.003,
      "step": 8880
    },
    {
      "epoch": 0.47413333333333335,
      "grad_norm": 0.16805501282215118,
      "learning_rate": 4.703666666666667e-05,
      "loss": 0.0041,
      "step": 8890
    },
    {
      "epoch": 0.4746666666666667,
      "grad_norm": 0.5641025900840759,
      "learning_rate": 4.7033333333333336e-05,
      "loss": 0.0028,
      "step": 8900
    },
    {
      "epoch": 0.4752,
      "grad_norm": 0.3105975091457367,
      "learning_rate": 4.703e-05,
      "loss": 0.0035,
      "step": 8910
    },
    {
      "epoch": 0.47573333333333334,
      "grad_norm": 0.4012032151222229,
      "learning_rate": 4.702666666666667e-05,
      "loss": 0.0049,
      "step": 8920
    },
    {
      "epoch": 0.47626666666666667,
      "grad_norm": 0.6768286228179932,
      "learning_rate": 4.7023333333333334e-05,
      "loss": 0.004,
      "step": 8930
    },
    {
      "epoch": 0.4768,
      "grad_norm": 0.09580717235803604,
      "learning_rate": 4.702e-05,
      "loss": 0.0049,
      "step": 8940
    },
    {
      "epoch": 0.47733333333333333,
      "grad_norm": 0.7729681134223938,
      "learning_rate": 4.701666666666667e-05,
      "loss": 0.0038,
      "step": 8950
    },
    {
      "epoch": 0.47786666666666666,
      "grad_norm": 0.7058725953102112,
      "learning_rate": 4.701333333333334e-05,
      "loss": 0.0035,
      "step": 8960
    },
    {
      "epoch": 0.4784,
      "grad_norm": 0.3362511396408081,
      "learning_rate": 4.7010000000000006e-05,
      "loss": 0.003,
      "step": 8970
    },
    {
      "epoch": 0.4789333333333333,
      "grad_norm": 0.36624303460121155,
      "learning_rate": 4.700666666666667e-05,
      "loss": 0.0034,
      "step": 8980
    },
    {
      "epoch": 0.47946666666666665,
      "grad_norm": 0.24516157805919647,
      "learning_rate": 4.700333333333334e-05,
      "loss": 0.004,
      "step": 8990
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.5437827110290527,
      "learning_rate": 4.7e-05,
      "loss": 0.0033,
      "step": 9000
    },
    {
      "epoch": 0.4805333333333333,
      "grad_norm": 1.1346971988677979,
      "learning_rate": 4.699666666666666e-05,
      "loss": 0.0028,
      "step": 9010
    },
    {
      "epoch": 0.48106666666666664,
      "grad_norm": 0.3687131404876709,
      "learning_rate": 4.6993333333333336e-05,
      "loss": 0.0038,
      "step": 9020
    },
    {
      "epoch": 0.4816,
      "grad_norm": 0.1946844756603241,
      "learning_rate": 4.699e-05,
      "loss": 0.0033,
      "step": 9030
    },
    {
      "epoch": 0.48213333333333336,
      "grad_norm": 0.0856093317270279,
      "learning_rate": 4.698666666666667e-05,
      "loss": 0.0026,
      "step": 9040
    },
    {
      "epoch": 0.4826666666666667,
      "grad_norm": 0.1314893513917923,
      "learning_rate": 4.6983333333333335e-05,
      "loss": 0.0043,
      "step": 9050
    },
    {
      "epoch": 0.4832,
      "grad_norm": 0.14585047960281372,
      "learning_rate": 4.698e-05,
      "loss": 0.003,
      "step": 9060
    },
    {
      "epoch": 0.48373333333333335,
      "grad_norm": 0.8386632204055786,
      "learning_rate": 4.697666666666667e-05,
      "loss": 0.0044,
      "step": 9070
    },
    {
      "epoch": 0.4842666666666667,
      "grad_norm": 0.31151002645492554,
      "learning_rate": 4.697333333333333e-05,
      "loss": 0.0043,
      "step": 9080
    },
    {
      "epoch": 0.4848,
      "grad_norm": 0.6752403378486633,
      "learning_rate": 4.6970000000000006e-05,
      "loss": 0.005,
      "step": 9090
    },
    {
      "epoch": 0.48533333333333334,
      "grad_norm": 0.9109158515930176,
      "learning_rate": 4.696666666666667e-05,
      "loss": 0.0037,
      "step": 9100
    },
    {
      "epoch": 0.48586666666666667,
      "grad_norm": 0.0754857063293457,
      "learning_rate": 4.696333333333334e-05,
      "loss": 0.0039,
      "step": 9110
    },
    {
      "epoch": 0.4864,
      "grad_norm": 0.19072163105010986,
      "learning_rate": 4.6960000000000004e-05,
      "loss": 0.0044,
      "step": 9120
    },
    {
      "epoch": 0.48693333333333333,
      "grad_norm": 0.7134683132171631,
      "learning_rate": 4.695666666666667e-05,
      "loss": 0.0041,
      "step": 9130
    },
    {
      "epoch": 0.48746666666666666,
      "grad_norm": 0.8445218205451965,
      "learning_rate": 4.695333333333334e-05,
      "loss": 0.003,
      "step": 9140
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.8229686617851257,
      "learning_rate": 4.695e-05,
      "loss": 0.0033,
      "step": 9150
    },
    {
      "epoch": 0.4885333333333333,
      "grad_norm": 0.6511102318763733,
      "learning_rate": 4.694666666666667e-05,
      "loss": 0.0025,
      "step": 9160
    },
    {
      "epoch": 0.48906666666666665,
      "grad_norm": 0.43388089537620544,
      "learning_rate": 4.6943333333333335e-05,
      "loss": 0.0024,
      "step": 9170
    },
    {
      "epoch": 0.4896,
      "grad_norm": 0.594902515411377,
      "learning_rate": 4.694e-05,
      "loss": 0.003,
      "step": 9180
    },
    {
      "epoch": 0.4901333333333333,
      "grad_norm": 0.5042329430580139,
      "learning_rate": 4.693666666666667e-05,
      "loss": 0.0032,
      "step": 9190
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 0.41604796051979065,
      "learning_rate": 4.6933333333333333e-05,
      "loss": 0.005,
      "step": 9200
    },
    {
      "epoch": 0.4912,
      "grad_norm": 0.5189087390899658,
      "learning_rate": 4.693e-05,
      "loss": 0.0036,
      "step": 9210
    },
    {
      "epoch": 0.49173333333333336,
      "grad_norm": 0.4083395302295685,
      "learning_rate": 4.6926666666666666e-05,
      "loss": 0.0037,
      "step": 9220
    },
    {
      "epoch": 0.4922666666666667,
      "grad_norm": 0.1711137294769287,
      "learning_rate": 4.692333333333334e-05,
      "loss": 0.0032,
      "step": 9230
    },
    {
      "epoch": 0.4928,
      "grad_norm": 0.22545257210731506,
      "learning_rate": 4.6920000000000005e-05,
      "loss": 0.0051,
      "step": 9240
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.4391186237335205,
      "learning_rate": 4.691666666666667e-05,
      "loss": 0.003,
      "step": 9250
    },
    {
      "epoch": 0.4938666666666667,
      "grad_norm": 0.16379760205745697,
      "learning_rate": 4.691333333333334e-05,
      "loss": 0.0031,
      "step": 9260
    },
    {
      "epoch": 0.4944,
      "grad_norm": 0.5644305944442749,
      "learning_rate": 4.691e-05,
      "loss": 0.0024,
      "step": 9270
    },
    {
      "epoch": 0.49493333333333334,
      "grad_norm": 0.14263494312763214,
      "learning_rate": 4.690666666666667e-05,
      "loss": 0.0026,
      "step": 9280
    },
    {
      "epoch": 0.49546666666666667,
      "grad_norm": 0.1293472796678543,
      "learning_rate": 4.6903333333333335e-05,
      "loss": 0.0031,
      "step": 9290
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.09474125504493713,
      "learning_rate": 4.69e-05,
      "loss": 0.0029,
      "step": 9300
    },
    {
      "epoch": 0.4965333333333333,
      "grad_norm": 0.2895869016647339,
      "learning_rate": 4.689666666666667e-05,
      "loss": 0.0029,
      "step": 9310
    },
    {
      "epoch": 0.49706666666666666,
      "grad_norm": 0.3065987527370453,
      "learning_rate": 4.6893333333333334e-05,
      "loss": 0.0047,
      "step": 9320
    },
    {
      "epoch": 0.4976,
      "grad_norm": 0.5481570959091187,
      "learning_rate": 4.689e-05,
      "loss": 0.0027,
      "step": 9330
    },
    {
      "epoch": 0.4981333333333333,
      "grad_norm": 0.09498845785856247,
      "learning_rate": 4.6886666666666666e-05,
      "loss": 0.0043,
      "step": 9340
    },
    {
      "epoch": 0.49866666666666665,
      "grad_norm": 0.2129165679216385,
      "learning_rate": 4.688333333333333e-05,
      "loss": 0.0033,
      "step": 9350
    },
    {
      "epoch": 0.4992,
      "grad_norm": 0.20474539697170258,
      "learning_rate": 4.688e-05,
      "loss": 0.0024,
      "step": 9360
    },
    {
      "epoch": 0.4997333333333333,
      "grad_norm": 0.14070920646190643,
      "learning_rate": 4.687666666666667e-05,
      "loss": 0.0039,
      "step": 9370
    },
    {
      "epoch": 0.5002666666666666,
      "grad_norm": 0.41022226214408875,
      "learning_rate": 4.687333333333334e-05,
      "loss": 0.004,
      "step": 9380
    },
    {
      "epoch": 0.5008,
      "grad_norm": 0.20078550279140472,
      "learning_rate": 4.6870000000000004e-05,
      "loss": 0.0044,
      "step": 9390
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 0.1107851043343544,
      "learning_rate": 4.686666666666667e-05,
      "loss": 0.0035,
      "step": 9400
    },
    {
      "epoch": 0.5018666666666667,
      "grad_norm": 0.5367239713668823,
      "learning_rate": 4.6863333333333336e-05,
      "loss": 0.0033,
      "step": 9410
    },
    {
      "epoch": 0.5024,
      "grad_norm": 0.07932576537132263,
      "learning_rate": 4.686e-05,
      "loss": 0.0035,
      "step": 9420
    },
    {
      "epoch": 0.5029333333333333,
      "grad_norm": 0.09707917273044586,
      "learning_rate": 4.685666666666667e-05,
      "loss": 0.0018,
      "step": 9430
    },
    {
      "epoch": 0.5034666666666666,
      "grad_norm": 0.11166400462388992,
      "learning_rate": 4.685333333333334e-05,
      "loss": 0.0042,
      "step": 9440
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.0562257282435894,
      "learning_rate": 4.685000000000001e-05,
      "loss": 0.0034,
      "step": 9450
    },
    {
      "epoch": 0.5045333333333333,
      "grad_norm": 0.14544248580932617,
      "learning_rate": 4.6846666666666667e-05,
      "loss": 0.0039,
      "step": 9460
    },
    {
      "epoch": 0.5050666666666667,
      "grad_norm": 0.06279472261667252,
      "learning_rate": 4.684333333333333e-05,
      "loss": 0.004,
      "step": 9470
    },
    {
      "epoch": 0.5056,
      "grad_norm": 0.48041820526123047,
      "learning_rate": 4.684e-05,
      "loss": 0.0028,
      "step": 9480
    },
    {
      "epoch": 0.5061333333333333,
      "grad_norm": 0.3343426287174225,
      "learning_rate": 4.6836666666666665e-05,
      "loss": 0.0027,
      "step": 9490
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.15963710844516754,
      "learning_rate": 4.683333333333334e-05,
      "loss": 0.0031,
      "step": 9500
    },
    {
      "epoch": 0.5072,
      "grad_norm": 0.19893261790275574,
      "learning_rate": 4.6830000000000004e-05,
      "loss": 0.0027,
      "step": 9510
    },
    {
      "epoch": 0.5077333333333334,
      "grad_norm": 0.34614279866218567,
      "learning_rate": 4.682666666666667e-05,
      "loss": 0.0023,
      "step": 9520
    },
    {
      "epoch": 0.5082666666666666,
      "grad_norm": 0.38210028409957886,
      "learning_rate": 4.6823333333333336e-05,
      "loss": 0.0043,
      "step": 9530
    },
    {
      "epoch": 0.5088,
      "grad_norm": 0.1759752631187439,
      "learning_rate": 4.682e-05,
      "loss": 0.0031,
      "step": 9540
    },
    {
      "epoch": 0.5093333333333333,
      "grad_norm": 0.3459647595882416,
      "learning_rate": 4.681666666666667e-05,
      "loss": 0.0028,
      "step": 9550
    },
    {
      "epoch": 0.5098666666666667,
      "grad_norm": 0.24081361293792725,
      "learning_rate": 4.6813333333333335e-05,
      "loss": 0.0023,
      "step": 9560
    },
    {
      "epoch": 0.5104,
      "grad_norm": 0.733535647392273,
      "learning_rate": 4.681e-05,
      "loss": 0.0037,
      "step": 9570
    },
    {
      "epoch": 0.5109333333333334,
      "grad_norm": 0.25204187631607056,
      "learning_rate": 4.6806666666666674e-05,
      "loss": 0.0034,
      "step": 9580
    },
    {
      "epoch": 0.5114666666666666,
      "grad_norm": 0.5216314792633057,
      "learning_rate": 4.680333333333334e-05,
      "loss": 0.004,
      "step": 9590
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.43411317467689514,
      "learning_rate": 4.6800000000000006e-05,
      "loss": 0.003,
      "step": 9600
    },
    {
      "epoch": 0.5125333333333333,
      "grad_norm": 0.5371316075325012,
      "learning_rate": 4.6796666666666665e-05,
      "loss": 0.0038,
      "step": 9610
    },
    {
      "epoch": 0.5130666666666667,
      "grad_norm": 0.2016514390707016,
      "learning_rate": 4.679333333333333e-05,
      "loss": 0.0032,
      "step": 9620
    },
    {
      "epoch": 0.5136,
      "grad_norm": 0.5399685502052307,
      "learning_rate": 4.679e-05,
      "loss": 0.005,
      "step": 9630
    },
    {
      "epoch": 0.5141333333333333,
      "grad_norm": 0.3744805157184601,
      "learning_rate": 4.678666666666667e-05,
      "loss": 0.0038,
      "step": 9640
    },
    {
      "epoch": 0.5146666666666667,
      "grad_norm": 0.727790117263794,
      "learning_rate": 4.6783333333333337e-05,
      "loss": 0.0033,
      "step": 9650
    },
    {
      "epoch": 0.5152,
      "grad_norm": 0.697109580039978,
      "learning_rate": 4.678e-05,
      "loss": 0.0041,
      "step": 9660
    },
    {
      "epoch": 0.5157333333333334,
      "grad_norm": 0.19822151958942413,
      "learning_rate": 4.677666666666667e-05,
      "loss": 0.0037,
      "step": 9670
    },
    {
      "epoch": 0.5162666666666667,
      "grad_norm": 0.3725863993167877,
      "learning_rate": 4.6773333333333335e-05,
      "loss": 0.0028,
      "step": 9680
    },
    {
      "epoch": 0.5168,
      "grad_norm": 0.09792593121528625,
      "learning_rate": 4.677e-05,
      "loss": 0.0031,
      "step": 9690
    },
    {
      "epoch": 0.5173333333333333,
      "grad_norm": 0.6322158575057983,
      "learning_rate": 4.676666666666667e-05,
      "loss": 0.0034,
      "step": 9700
    },
    {
      "epoch": 0.5178666666666667,
      "grad_norm": 0.4287075698375702,
      "learning_rate": 4.6763333333333333e-05,
      "loss": 0.004,
      "step": 9710
    },
    {
      "epoch": 0.5184,
      "grad_norm": 0.6959139108657837,
      "learning_rate": 4.6760000000000006e-05,
      "loss": 0.0044,
      "step": 9720
    },
    {
      "epoch": 0.5189333333333334,
      "grad_norm": 0.48089081048965454,
      "learning_rate": 4.675666666666667e-05,
      "loss": 0.0033,
      "step": 9730
    },
    {
      "epoch": 0.5194666666666666,
      "grad_norm": 0.7371190190315247,
      "learning_rate": 4.675333333333334e-05,
      "loss": 0.0032,
      "step": 9740
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.5335703492164612,
      "learning_rate": 4.6750000000000005e-05,
      "loss": 0.0025,
      "step": 9750
    },
    {
      "epoch": 0.5205333333333333,
      "grad_norm": 0.2838926613330841,
      "learning_rate": 4.6746666666666664e-05,
      "loss": 0.0041,
      "step": 9760
    },
    {
      "epoch": 0.5210666666666667,
      "grad_norm": 0.26971495151519775,
      "learning_rate": 4.674333333333333e-05,
      "loss": 0.0033,
      "step": 9770
    },
    {
      "epoch": 0.5216,
      "grad_norm": 0.4077702760696411,
      "learning_rate": 4.674e-05,
      "loss": 0.004,
      "step": 9780
    },
    {
      "epoch": 0.5221333333333333,
      "grad_norm": 0.4975432753562927,
      "learning_rate": 4.673666666666667e-05,
      "loss": 0.0034,
      "step": 9790
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 0.536911129951477,
      "learning_rate": 4.6733333333333335e-05,
      "loss": 0.0033,
      "step": 9800
    },
    {
      "epoch": 0.5232,
      "grad_norm": 0.3492647409439087,
      "learning_rate": 4.673e-05,
      "loss": 0.0053,
      "step": 9810
    },
    {
      "epoch": 0.5237333333333334,
      "grad_norm": 0.317965030670166,
      "learning_rate": 4.672666666666667e-05,
      "loss": 0.0035,
      "step": 9820
    },
    {
      "epoch": 0.5242666666666667,
      "grad_norm": 0.548049807548523,
      "learning_rate": 4.6723333333333334e-05,
      "loss": 0.0032,
      "step": 9830
    },
    {
      "epoch": 0.5248,
      "grad_norm": 0.26047810912132263,
      "learning_rate": 4.672e-05,
      "loss": 0.0031,
      "step": 9840
    },
    {
      "epoch": 0.5253333333333333,
      "grad_norm": 0.6322736144065857,
      "learning_rate": 4.671666666666667e-05,
      "loss": 0.0042,
      "step": 9850
    },
    {
      "epoch": 0.5258666666666667,
      "grad_norm": 0.229305699467659,
      "learning_rate": 4.671333333333334e-05,
      "loss": 0.0027,
      "step": 9860
    },
    {
      "epoch": 0.5264,
      "grad_norm": 0.08745791018009186,
      "learning_rate": 4.6710000000000005e-05,
      "loss": 0.0044,
      "step": 9870
    },
    {
      "epoch": 0.5269333333333334,
      "grad_norm": 0.07714467495679855,
      "learning_rate": 4.670666666666667e-05,
      "loss": 0.0033,
      "step": 9880
    },
    {
      "epoch": 0.5274666666666666,
      "grad_norm": 0.07047387957572937,
      "learning_rate": 4.670333333333334e-05,
      "loss": 0.0033,
      "step": 9890
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.6652930378913879,
      "learning_rate": 4.6700000000000003e-05,
      "loss": 0.0031,
      "step": 9900
    },
    {
      "epoch": 0.5285333333333333,
      "grad_norm": 0.5531418323516846,
      "learning_rate": 4.669666666666667e-05,
      "loss": 0.0043,
      "step": 9910
    },
    {
      "epoch": 0.5290666666666667,
      "grad_norm": 0.2438868135213852,
      "learning_rate": 4.6693333333333336e-05,
      "loss": 0.0036,
      "step": 9920
    },
    {
      "epoch": 0.5296,
      "grad_norm": 0.6860945820808411,
      "learning_rate": 4.669e-05,
      "loss": 0.0044,
      "step": 9930
    },
    {
      "epoch": 0.5301333333333333,
      "grad_norm": 0.46404513716697693,
      "learning_rate": 4.668666666666667e-05,
      "loss": 0.0031,
      "step": 9940
    },
    {
      "epoch": 0.5306666666666666,
      "grad_norm": 0.700436532497406,
      "learning_rate": 4.6683333333333334e-05,
      "loss": 0.0043,
      "step": 9950
    },
    {
      "epoch": 0.5312,
      "grad_norm": 0.10641385614871979,
      "learning_rate": 4.668e-05,
      "loss": 0.0044,
      "step": 9960
    },
    {
      "epoch": 0.5317333333333333,
      "grad_norm": 0.1516610085964203,
      "learning_rate": 4.6676666666666666e-05,
      "loss": 0.0021,
      "step": 9970
    },
    {
      "epoch": 0.5322666666666667,
      "grad_norm": 0.4292801320552826,
      "learning_rate": 4.667333333333333e-05,
      "loss": 0.0043,
      "step": 9980
    },
    {
      "epoch": 0.5328,
      "grad_norm": 0.14354626834392548,
      "learning_rate": 4.6670000000000005e-05,
      "loss": 0.0035,
      "step": 9990
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.15508611500263214,
      "learning_rate": 4.666666666666667e-05,
      "loss": 0.0031,
      "step": 10000
    },
    {
      "epoch": 0.5338666666666667,
      "grad_norm": 0.185895174741745,
      "learning_rate": 4.666333333333334e-05,
      "loss": 0.0051,
      "step": 10010
    },
    {
      "epoch": 0.5344,
      "grad_norm": 0.2354123592376709,
      "learning_rate": 4.6660000000000004e-05,
      "loss": 0.0038,
      "step": 10020
    },
    {
      "epoch": 0.5349333333333334,
      "grad_norm": 0.6635781526565552,
      "learning_rate": 4.665666666666667e-05,
      "loss": 0.0047,
      "step": 10030
    },
    {
      "epoch": 0.5354666666666666,
      "grad_norm": 0.24383412301540375,
      "learning_rate": 4.6653333333333336e-05,
      "loss": 0.0033,
      "step": 10040
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.17948636412620544,
      "learning_rate": 4.665e-05,
      "loss": 0.0027,
      "step": 10050
    },
    {
      "epoch": 0.5365333333333333,
      "grad_norm": 0.08465857058763504,
      "learning_rate": 4.664666666666667e-05,
      "loss": 0.0052,
      "step": 10060
    },
    {
      "epoch": 0.5370666666666667,
      "grad_norm": 0.539055585861206,
      "learning_rate": 4.6643333333333335e-05,
      "loss": 0.0027,
      "step": 10070
    },
    {
      "epoch": 0.5376,
      "grad_norm": 0.4984339773654938,
      "learning_rate": 4.664e-05,
      "loss": 0.0044,
      "step": 10080
    },
    {
      "epoch": 0.5381333333333334,
      "grad_norm": 0.43645191192626953,
      "learning_rate": 4.663666666666667e-05,
      "loss": 0.0033,
      "step": 10090
    },
    {
      "epoch": 0.5386666666666666,
      "grad_norm": 0.5704073905944824,
      "learning_rate": 4.663333333333333e-05,
      "loss": 0.0036,
      "step": 10100
    },
    {
      "epoch": 0.5392,
      "grad_norm": 0.29672157764434814,
      "learning_rate": 4.663e-05,
      "loss": 0.0035,
      "step": 10110
    },
    {
      "epoch": 0.5397333333333333,
      "grad_norm": 0.6024973392486572,
      "learning_rate": 4.6626666666666665e-05,
      "loss": 0.0037,
      "step": 10120
    },
    {
      "epoch": 0.5402666666666667,
      "grad_norm": 0.11303526908159256,
      "learning_rate": 4.662333333333334e-05,
      "loss": 0.0029,
      "step": 10130
    },
    {
      "epoch": 0.5408,
      "grad_norm": 0.7339403629302979,
      "learning_rate": 4.6620000000000004e-05,
      "loss": 0.0033,
      "step": 10140
    },
    {
      "epoch": 0.5413333333333333,
      "grad_norm": 0.14784745872020721,
      "learning_rate": 4.661666666666667e-05,
      "loss": 0.0038,
      "step": 10150
    },
    {
      "epoch": 0.5418666666666667,
      "grad_norm": 0.5654539465904236,
      "learning_rate": 4.6613333333333337e-05,
      "loss": 0.0045,
      "step": 10160
    },
    {
      "epoch": 0.5424,
      "grad_norm": 0.63536137342453,
      "learning_rate": 4.661e-05,
      "loss": 0.0033,
      "step": 10170
    },
    {
      "epoch": 0.5429333333333334,
      "grad_norm": 0.36977773904800415,
      "learning_rate": 4.660666666666667e-05,
      "loss": 0.003,
      "step": 10180
    },
    {
      "epoch": 0.5434666666666667,
      "grad_norm": 0.4116145670413971,
      "learning_rate": 4.6603333333333335e-05,
      "loss": 0.0034,
      "step": 10190
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.07698111236095428,
      "learning_rate": 4.660000000000001e-05,
      "loss": 0.0037,
      "step": 10200
    },
    {
      "epoch": 0.5445333333333333,
      "grad_norm": 0.1518673300743103,
      "learning_rate": 4.659666666666667e-05,
      "loss": 0.0022,
      "step": 10210
    },
    {
      "epoch": 0.5450666666666667,
      "grad_norm": 0.19887247681617737,
      "learning_rate": 4.659333333333333e-05,
      "loss": 0.0041,
      "step": 10220
    },
    {
      "epoch": 0.5456,
      "grad_norm": 0.40167033672332764,
      "learning_rate": 4.659e-05,
      "loss": 0.0032,
      "step": 10230
    },
    {
      "epoch": 0.5461333333333334,
      "grad_norm": 0.16756336390972137,
      "learning_rate": 4.6586666666666666e-05,
      "loss": 0.0042,
      "step": 10240
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.2241070717573166,
      "learning_rate": 4.658333333333333e-05,
      "loss": 0.0041,
      "step": 10250
    },
    {
      "epoch": 0.5472,
      "grad_norm": 0.44754117727279663,
      "learning_rate": 4.6580000000000005e-05,
      "loss": 0.0028,
      "step": 10260
    },
    {
      "epoch": 0.5477333333333333,
      "grad_norm": 0.17312480509281158,
      "learning_rate": 4.657666666666667e-05,
      "loss": 0.004,
      "step": 10270
    },
    {
      "epoch": 0.5482666666666667,
      "grad_norm": 0.5722572803497314,
      "learning_rate": 4.657333333333334e-05,
      "loss": 0.0037,
      "step": 10280
    },
    {
      "epoch": 0.5488,
      "grad_norm": 0.17403975129127502,
      "learning_rate": 4.657e-05,
      "loss": 0.003,
      "step": 10290
    },
    {
      "epoch": 0.5493333333333333,
      "grad_norm": 0.4748019874095917,
      "learning_rate": 4.656666666666667e-05,
      "loss": 0.0039,
      "step": 10300
    },
    {
      "epoch": 0.5498666666666666,
      "grad_norm": 0.6288119554519653,
      "learning_rate": 4.6563333333333335e-05,
      "loss": 0.0026,
      "step": 10310
    },
    {
      "epoch": 0.5504,
      "grad_norm": 0.1766960620880127,
      "learning_rate": 4.656e-05,
      "loss": 0.0035,
      "step": 10320
    },
    {
      "epoch": 0.5509333333333334,
      "grad_norm": 0.4691225290298462,
      "learning_rate": 4.655666666666667e-05,
      "loss": 0.0026,
      "step": 10330
    },
    {
      "epoch": 0.5514666666666667,
      "grad_norm": 0.10612945258617401,
      "learning_rate": 4.655333333333334e-05,
      "loss": 0.0032,
      "step": 10340
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.3735535144805908,
      "learning_rate": 4.655000000000001e-05,
      "loss": 0.0027,
      "step": 10350
    },
    {
      "epoch": 0.5525333333333333,
      "grad_norm": 0.13753484189510345,
      "learning_rate": 4.6546666666666666e-05,
      "loss": 0.0028,
      "step": 10360
    },
    {
      "epoch": 0.5530666666666667,
      "grad_norm": 0.7550107836723328,
      "learning_rate": 4.654333333333333e-05,
      "loss": 0.0035,
      "step": 10370
    },
    {
      "epoch": 0.5536,
      "grad_norm": 0.46570178866386414,
      "learning_rate": 4.654e-05,
      "loss": 0.0044,
      "step": 10380
    },
    {
      "epoch": 0.5541333333333334,
      "grad_norm": 0.7968150973320007,
      "learning_rate": 4.6536666666666664e-05,
      "loss": 0.0047,
      "step": 10390
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 0.5040215253829956,
      "learning_rate": 4.653333333333334e-05,
      "loss": 0.0027,
      "step": 10400
    },
    {
      "epoch": 0.5552,
      "grad_norm": 0.433587908744812,
      "learning_rate": 4.6530000000000003e-05,
      "loss": 0.0027,
      "step": 10410
    },
    {
      "epoch": 0.5557333333333333,
      "grad_norm": 0.7373055219650269,
      "learning_rate": 4.652666666666667e-05,
      "loss": 0.0036,
      "step": 10420
    },
    {
      "epoch": 0.5562666666666667,
      "grad_norm": 0.6062015295028687,
      "learning_rate": 4.6523333333333336e-05,
      "loss": 0.0033,
      "step": 10430
    },
    {
      "epoch": 0.5568,
      "grad_norm": 0.5910109281539917,
      "learning_rate": 4.652e-05,
      "loss": 0.0024,
      "step": 10440
    },
    {
      "epoch": 0.5573333333333333,
      "grad_norm": 0.0980505645275116,
      "learning_rate": 4.651666666666667e-05,
      "loss": 0.0042,
      "step": 10450
    },
    {
      "epoch": 0.5578666666666666,
      "grad_norm": 0.3028455376625061,
      "learning_rate": 4.6513333333333334e-05,
      "loss": 0.004,
      "step": 10460
    },
    {
      "epoch": 0.5584,
      "grad_norm": 0.27243462204933167,
      "learning_rate": 4.651e-05,
      "loss": 0.004,
      "step": 10470
    },
    {
      "epoch": 0.5589333333333333,
      "grad_norm": 0.43166157603263855,
      "learning_rate": 4.650666666666667e-05,
      "loss": 0.0024,
      "step": 10480
    },
    {
      "epoch": 0.5594666666666667,
      "grad_norm": 0.4135494530200958,
      "learning_rate": 4.650333333333334e-05,
      "loss": 0.0033,
      "step": 10490
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.6584129929542542,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 0.0042,
      "step": 10500
    },
    {
      "epoch": 0.5605333333333333,
      "grad_norm": 0.39896902441978455,
      "learning_rate": 4.6496666666666665e-05,
      "loss": 0.0031,
      "step": 10510
    },
    {
      "epoch": 0.5610666666666667,
      "grad_norm": 0.3400849997997284,
      "learning_rate": 4.649333333333333e-05,
      "loss": 0.0025,
      "step": 10520
    },
    {
      "epoch": 0.5616,
      "grad_norm": 0.47217288613319397,
      "learning_rate": 4.649e-05,
      "loss": 0.0039,
      "step": 10530
    },
    {
      "epoch": 0.5621333333333334,
      "grad_norm": 0.31468233466148376,
      "learning_rate": 4.648666666666667e-05,
      "loss": 0.0033,
      "step": 10540
    },
    {
      "epoch": 0.5626666666666666,
      "grad_norm": 0.3962661325931549,
      "learning_rate": 4.6483333333333336e-05,
      "loss": 0.0037,
      "step": 10550
    },
    {
      "epoch": 0.5632,
      "grad_norm": 0.49643468856811523,
      "learning_rate": 4.648e-05,
      "loss": 0.0032,
      "step": 10560
    },
    {
      "epoch": 0.5637333333333333,
      "grad_norm": 0.3102041780948639,
      "learning_rate": 4.647666666666667e-05,
      "loss": 0.0024,
      "step": 10570
    },
    {
      "epoch": 0.5642666666666667,
      "grad_norm": 0.18123222887516022,
      "learning_rate": 4.6473333333333334e-05,
      "loss": 0.0048,
      "step": 10580
    },
    {
      "epoch": 0.5648,
      "grad_norm": 0.2857483923435211,
      "learning_rate": 4.647e-05,
      "loss": 0.0024,
      "step": 10590
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 0.6363973617553711,
      "learning_rate": 4.646666666666667e-05,
      "loss": 0.004,
      "step": 10600
    },
    {
      "epoch": 0.5658666666666666,
      "grad_norm": 0.3328787386417389,
      "learning_rate": 4.646333333333334e-05,
      "loss": 0.0037,
      "step": 10610
    },
    {
      "epoch": 0.5664,
      "grad_norm": 0.2754720151424408,
      "learning_rate": 4.6460000000000006e-05,
      "loss": 0.0027,
      "step": 10620
    },
    {
      "epoch": 0.5669333333333333,
      "grad_norm": 0.21915367245674133,
      "learning_rate": 4.645666666666667e-05,
      "loss": 0.0037,
      "step": 10630
    },
    {
      "epoch": 0.5674666666666667,
      "grad_norm": 0.3047081530094147,
      "learning_rate": 4.645333333333334e-05,
      "loss": 0.0038,
      "step": 10640
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.11418233811855316,
      "learning_rate": 4.6450000000000004e-05,
      "loss": 0.0027,
      "step": 10650
    },
    {
      "epoch": 0.5685333333333333,
      "grad_norm": 0.2336825132369995,
      "learning_rate": 4.644666666666667e-05,
      "loss": 0.0024,
      "step": 10660
    },
    {
      "epoch": 0.5690666666666667,
      "grad_norm": 0.1929062008857727,
      "learning_rate": 4.6443333333333336e-05,
      "loss": 0.0039,
      "step": 10670
    },
    {
      "epoch": 0.5696,
      "grad_norm": 0.24487760663032532,
      "learning_rate": 4.644e-05,
      "loss": 0.0033,
      "step": 10680
    },
    {
      "epoch": 0.5701333333333334,
      "grad_norm": 0.1581573188304901,
      "learning_rate": 4.643666666666667e-05,
      "loss": 0.0021,
      "step": 10690
    },
    {
      "epoch": 0.5706666666666667,
      "grad_norm": 0.3072316646575928,
      "learning_rate": 4.6433333333333335e-05,
      "loss": 0.0037,
      "step": 10700
    },
    {
      "epoch": 0.5712,
      "grad_norm": 0.1883924901485443,
      "learning_rate": 4.643e-05,
      "loss": 0.0028,
      "step": 10710
    },
    {
      "epoch": 0.5717333333333333,
      "grad_norm": 0.2547311782836914,
      "learning_rate": 4.642666666666667e-05,
      "loss": 0.0039,
      "step": 10720
    },
    {
      "epoch": 0.5722666666666667,
      "grad_norm": 0.8244625926017761,
      "learning_rate": 4.642333333333333e-05,
      "loss": 0.0043,
      "step": 10730
    },
    {
      "epoch": 0.5728,
      "grad_norm": 0.18061134219169617,
      "learning_rate": 4.642e-05,
      "loss": 0.0031,
      "step": 10740
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.2167230099439621,
      "learning_rate": 4.641666666666667e-05,
      "loss": 0.004,
      "step": 10750
    },
    {
      "epoch": 0.5738666666666666,
      "grad_norm": 0.16642528772354126,
      "learning_rate": 4.641333333333334e-05,
      "loss": 0.0036,
      "step": 10760
    },
    {
      "epoch": 0.5744,
      "grad_norm": 0.24262672662734985,
      "learning_rate": 4.6410000000000005e-05,
      "loss": 0.0034,
      "step": 10770
    },
    {
      "epoch": 0.5749333333333333,
      "grad_norm": 0.4079605042934418,
      "learning_rate": 4.640666666666667e-05,
      "loss": 0.0032,
      "step": 10780
    },
    {
      "epoch": 0.5754666666666667,
      "grad_norm": 0.5641669631004333,
      "learning_rate": 4.640333333333334e-05,
      "loss": 0.0032,
      "step": 10790
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.19581666588783264,
      "learning_rate": 4.64e-05,
      "loss": 0.0055,
      "step": 10800
    },
    {
      "epoch": 0.5765333333333333,
      "grad_norm": 0.14553648233413696,
      "learning_rate": 4.639666666666667e-05,
      "loss": 0.0036,
      "step": 10810
    },
    {
      "epoch": 0.5770666666666666,
      "grad_norm": 0.6306036114692688,
      "learning_rate": 4.6393333333333335e-05,
      "loss": 0.0032,
      "step": 10820
    },
    {
      "epoch": 0.5776,
      "grad_norm": 0.4631524682044983,
      "learning_rate": 4.639e-05,
      "loss": 0.0026,
      "step": 10830
    },
    {
      "epoch": 0.5781333333333334,
      "grad_norm": 0.8251975178718567,
      "learning_rate": 4.638666666666667e-05,
      "loss": 0.0027,
      "step": 10840
    },
    {
      "epoch": 0.5786666666666667,
      "grad_norm": 0.14236325025558472,
      "learning_rate": 4.6383333333333334e-05,
      "loss": 0.0041,
      "step": 10850
    },
    {
      "epoch": 0.5792,
      "grad_norm": 0.20155462622642517,
      "learning_rate": 4.638e-05,
      "loss": 0.0042,
      "step": 10860
    },
    {
      "epoch": 0.5797333333333333,
      "grad_norm": 0.5058411359786987,
      "learning_rate": 4.6376666666666666e-05,
      "loss": 0.0026,
      "step": 10870
    },
    {
      "epoch": 0.5802666666666667,
      "grad_norm": 0.45951882004737854,
      "learning_rate": 4.637333333333333e-05,
      "loss": 0.0038,
      "step": 10880
    },
    {
      "epoch": 0.5808,
      "grad_norm": 0.6216307878494263,
      "learning_rate": 4.6370000000000005e-05,
      "loss": 0.0025,
      "step": 10890
    },
    {
      "epoch": 0.5813333333333334,
      "grad_norm": 0.719870388507843,
      "learning_rate": 4.636666666666667e-05,
      "loss": 0.0026,
      "step": 10900
    },
    {
      "epoch": 0.5818666666666666,
      "grad_norm": 0.5294986367225647,
      "learning_rate": 4.636333333333334e-05,
      "loss": 0.0026,
      "step": 10910
    },
    {
      "epoch": 0.5824,
      "grad_norm": 0.07492811977863312,
      "learning_rate": 4.636e-05,
      "loss": 0.0037,
      "step": 10920
    },
    {
      "epoch": 0.5829333333333333,
      "grad_norm": 0.36539146304130554,
      "learning_rate": 4.635666666666667e-05,
      "loss": 0.0039,
      "step": 10930
    },
    {
      "epoch": 0.5834666666666667,
      "grad_norm": 0.13778717815876007,
      "learning_rate": 4.6353333333333336e-05,
      "loss": 0.003,
      "step": 10940
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.153425931930542,
      "learning_rate": 4.635e-05,
      "loss": 0.0043,
      "step": 10950
    },
    {
      "epoch": 0.5845333333333333,
      "grad_norm": 0.11084965616464615,
      "learning_rate": 4.6346666666666675e-05,
      "loss": 0.0034,
      "step": 10960
    },
    {
      "epoch": 0.5850666666666666,
      "grad_norm": 0.39935290813446045,
      "learning_rate": 4.6343333333333334e-05,
      "loss": 0.0032,
      "step": 10970
    },
    {
      "epoch": 0.5856,
      "grad_norm": 0.5167016983032227,
      "learning_rate": 4.634e-05,
      "loss": 0.0032,
      "step": 10980
    },
    {
      "epoch": 0.5861333333333333,
      "grad_norm": 0.06593068689107895,
      "learning_rate": 4.6336666666666666e-05,
      "loss": 0.0029,
      "step": 10990
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.2676056921482086,
      "learning_rate": 4.633333333333333e-05,
      "loss": 0.003,
      "step": 11000
    },
    {
      "epoch": 0.5872,
      "grad_norm": 0.1856166124343872,
      "learning_rate": 4.633e-05,
      "loss": 0.0037,
      "step": 11010
    },
    {
      "epoch": 0.5877333333333333,
      "grad_norm": 0.4605470895767212,
      "learning_rate": 4.632666666666667e-05,
      "loss": 0.0038,
      "step": 11020
    },
    {
      "epoch": 0.5882666666666667,
      "grad_norm": 0.8006622791290283,
      "learning_rate": 4.632333333333334e-05,
      "loss": 0.0031,
      "step": 11030
    },
    {
      "epoch": 0.5888,
      "grad_norm": 0.8957606554031372,
      "learning_rate": 4.6320000000000004e-05,
      "loss": 0.0038,
      "step": 11040
    },
    {
      "epoch": 0.5893333333333334,
      "grad_norm": 0.5636566877365112,
      "learning_rate": 4.631666666666667e-05,
      "loss": 0.0036,
      "step": 11050
    },
    {
      "epoch": 0.5898666666666667,
      "grad_norm": 0.3013004660606384,
      "learning_rate": 4.6313333333333336e-05,
      "loss": 0.0028,
      "step": 11060
    },
    {
      "epoch": 0.5904,
      "grad_norm": 0.9497659802436829,
      "learning_rate": 4.631e-05,
      "loss": 0.0032,
      "step": 11070
    },
    {
      "epoch": 0.5909333333333333,
      "grad_norm": 0.6079141497612,
      "learning_rate": 4.630666666666667e-05,
      "loss": 0.003,
      "step": 11080
    },
    {
      "epoch": 0.5914666666666667,
      "grad_norm": 0.9830565452575684,
      "learning_rate": 4.6303333333333334e-05,
      "loss": 0.0041,
      "step": 11090
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.4325443208217621,
      "learning_rate": 4.630000000000001e-05,
      "loss": 0.0034,
      "step": 11100
    },
    {
      "epoch": 0.5925333333333334,
      "grad_norm": 0.6907731294631958,
      "learning_rate": 4.6296666666666673e-05,
      "loss": 0.0028,
      "step": 11110
    },
    {
      "epoch": 0.5930666666666666,
      "grad_norm": 0.16354933381080627,
      "learning_rate": 4.629333333333333e-05,
      "loss": 0.003,
      "step": 11120
    },
    {
      "epoch": 0.5936,
      "grad_norm": 0.19075468182563782,
      "learning_rate": 4.629e-05,
      "loss": 0.0038,
      "step": 11130
    },
    {
      "epoch": 0.5941333333333333,
      "grad_norm": 0.7333601117134094,
      "learning_rate": 4.6286666666666665e-05,
      "loss": 0.0038,
      "step": 11140
    },
    {
      "epoch": 0.5946666666666667,
      "grad_norm": 0.11087380349636078,
      "learning_rate": 4.628333333333333e-05,
      "loss": 0.0035,
      "step": 11150
    },
    {
      "epoch": 0.5952,
      "grad_norm": 0.043896764516830444,
      "learning_rate": 4.6280000000000004e-05,
      "loss": 0.004,
      "step": 11160
    },
    {
      "epoch": 0.5957333333333333,
      "grad_norm": 0.9440515637397766,
      "learning_rate": 4.627666666666667e-05,
      "loss": 0.0028,
      "step": 11170
    },
    {
      "epoch": 0.5962666666666666,
      "grad_norm": 0.24474690854549408,
      "learning_rate": 4.6273333333333336e-05,
      "loss": 0.0031,
      "step": 11180
    },
    {
      "epoch": 0.5968,
      "grad_norm": 0.10421498119831085,
      "learning_rate": 4.627e-05,
      "loss": 0.0035,
      "step": 11190
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 0.1300549954175949,
      "learning_rate": 4.626666666666667e-05,
      "loss": 0.0024,
      "step": 11200
    },
    {
      "epoch": 0.5978666666666667,
      "grad_norm": 0.7197967171669006,
      "learning_rate": 4.6263333333333335e-05,
      "loss": 0.0022,
      "step": 11210
    },
    {
      "epoch": 0.5984,
      "grad_norm": 0.5059520602226257,
      "learning_rate": 4.626e-05,
      "loss": 0.0022,
      "step": 11220
    },
    {
      "epoch": 0.5989333333333333,
      "grad_norm": 0.5865587592124939,
      "learning_rate": 4.625666666666667e-05,
      "loss": 0.004,
      "step": 11230
    },
    {
      "epoch": 0.5994666666666667,
      "grad_norm": 0.1762198805809021,
      "learning_rate": 4.625333333333334e-05,
      "loss": 0.0038,
      "step": 11240
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.1868913769721985,
      "learning_rate": 4.6250000000000006e-05,
      "loss": 0.0042,
      "step": 11250
    },
    {
      "epoch": 0.6005333333333334,
      "grad_norm": 0.16106866300106049,
      "learning_rate": 4.624666666666667e-05,
      "loss": 0.0033,
      "step": 11260
    },
    {
      "epoch": 0.6010666666666666,
      "grad_norm": 0.33045676350593567,
      "learning_rate": 4.624333333333333e-05,
      "loss": 0.0026,
      "step": 11270
    },
    {
      "epoch": 0.6016,
      "grad_norm": 0.1425727754831314,
      "learning_rate": 4.624e-05,
      "loss": 0.0033,
      "step": 11280
    },
    {
      "epoch": 0.6021333333333333,
      "grad_norm": 0.3376968801021576,
      "learning_rate": 4.6236666666666664e-05,
      "loss": 0.0051,
      "step": 11290
    },
    {
      "epoch": 0.6026666666666667,
      "grad_norm": 0.25174787640571594,
      "learning_rate": 4.623333333333334e-05,
      "loss": 0.0031,
      "step": 11300
    },
    {
      "epoch": 0.6032,
      "grad_norm": 0.09500729292631149,
      "learning_rate": 4.623e-05,
      "loss": 0.0039,
      "step": 11310
    },
    {
      "epoch": 0.6037333333333333,
      "grad_norm": 0.08395674079656601,
      "learning_rate": 4.622666666666667e-05,
      "loss": 0.0041,
      "step": 11320
    },
    {
      "epoch": 0.6042666666666666,
      "grad_norm": 0.08720865100622177,
      "learning_rate": 4.6223333333333335e-05,
      "loss": 0.0051,
      "step": 11330
    },
    {
      "epoch": 0.6048,
      "grad_norm": 0.5641838908195496,
      "learning_rate": 4.622e-05,
      "loss": 0.0025,
      "step": 11340
    },
    {
      "epoch": 0.6053333333333333,
      "grad_norm": 0.5302805304527283,
      "learning_rate": 4.621666666666667e-05,
      "loss": 0.0031,
      "step": 11350
    },
    {
      "epoch": 0.6058666666666667,
      "grad_norm": 0.5711125731468201,
      "learning_rate": 4.6213333333333334e-05,
      "loss": 0.004,
      "step": 11360
    },
    {
      "epoch": 0.6064,
      "grad_norm": 0.4907444715499878,
      "learning_rate": 4.6210000000000006e-05,
      "loss": 0.0029,
      "step": 11370
    },
    {
      "epoch": 0.6069333333333333,
      "grad_norm": 0.6565074920654297,
      "learning_rate": 4.620666666666667e-05,
      "loss": 0.0027,
      "step": 11380
    },
    {
      "epoch": 0.6074666666666667,
      "grad_norm": 0.4583146870136261,
      "learning_rate": 4.620333333333334e-05,
      "loss": 0.0024,
      "step": 11390
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.09724347293376923,
      "learning_rate": 4.6200000000000005e-05,
      "loss": 0.0023,
      "step": 11400
    },
    {
      "epoch": 0.6085333333333334,
      "grad_norm": 0.15142595767974854,
      "learning_rate": 4.619666666666667e-05,
      "loss": 0.0037,
      "step": 11410
    },
    {
      "epoch": 0.6090666666666666,
      "grad_norm": 0.3136422336101532,
      "learning_rate": 4.619333333333333e-05,
      "loss": 0.0037,
      "step": 11420
    },
    {
      "epoch": 0.6096,
      "grad_norm": 0.4736722409725189,
      "learning_rate": 4.619e-05,
      "loss": 0.0025,
      "step": 11430
    },
    {
      "epoch": 0.6101333333333333,
      "grad_norm": 0.19242048263549805,
      "learning_rate": 4.618666666666667e-05,
      "loss": 0.0038,
      "step": 11440
    },
    {
      "epoch": 0.6106666666666667,
      "grad_norm": 0.5548276305198669,
      "learning_rate": 4.6183333333333336e-05,
      "loss": 0.0033,
      "step": 11450
    },
    {
      "epoch": 0.6112,
      "grad_norm": 0.23826220631599426,
      "learning_rate": 4.618e-05,
      "loss": 0.0032,
      "step": 11460
    },
    {
      "epoch": 0.6117333333333334,
      "grad_norm": 0.5839009881019592,
      "learning_rate": 4.617666666666667e-05,
      "loss": 0.0041,
      "step": 11470
    },
    {
      "epoch": 0.6122666666666666,
      "grad_norm": 0.5596407651901245,
      "learning_rate": 4.6173333333333334e-05,
      "loss": 0.0036,
      "step": 11480
    },
    {
      "epoch": 0.6128,
      "grad_norm": 0.3263987600803375,
      "learning_rate": 4.617e-05,
      "loss": 0.0027,
      "step": 11490
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.4465258717536926,
      "learning_rate": 4.6166666666666666e-05,
      "loss": 0.0046,
      "step": 11500
    },
    {
      "epoch": 0.6138666666666667,
      "grad_norm": 0.8406088948249817,
      "learning_rate": 4.616333333333334e-05,
      "loss": 0.0055,
      "step": 11510
    },
    {
      "epoch": 0.6144,
      "grad_norm": 0.9744279384613037,
      "learning_rate": 4.6160000000000005e-05,
      "loss": 0.0037,
      "step": 11520
    },
    {
      "epoch": 0.6149333333333333,
      "grad_norm": 0.04793841764330864,
      "learning_rate": 4.615666666666667e-05,
      "loss": 0.0027,
      "step": 11530
    },
    {
      "epoch": 0.6154666666666667,
      "grad_norm": 0.484156996011734,
      "learning_rate": 4.615333333333334e-05,
      "loss": 0.0035,
      "step": 11540
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.48495882749557495,
      "learning_rate": 4.6150000000000004e-05,
      "loss": 0.0044,
      "step": 11550
    },
    {
      "epoch": 0.6165333333333334,
      "grad_norm": 0.23329462110996246,
      "learning_rate": 4.614666666666667e-05,
      "loss": 0.003,
      "step": 11560
    },
    {
      "epoch": 0.6170666666666667,
      "grad_norm": 0.2680237293243408,
      "learning_rate": 4.6143333333333336e-05,
      "loss": 0.0049,
      "step": 11570
    },
    {
      "epoch": 0.6176,
      "grad_norm": 0.17392760515213013,
      "learning_rate": 4.614e-05,
      "loss": 0.0034,
      "step": 11580
    },
    {
      "epoch": 0.6181333333333333,
      "grad_norm": 0.15883518755435944,
      "learning_rate": 4.613666666666667e-05,
      "loss": 0.0031,
      "step": 11590
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 0.559364378452301,
      "learning_rate": 4.6133333333333334e-05,
      "loss": 0.0031,
      "step": 11600
    },
    {
      "epoch": 0.6192,
      "grad_norm": 0.2721758186817169,
      "learning_rate": 4.613e-05,
      "loss": 0.0038,
      "step": 11610
    },
    {
      "epoch": 0.6197333333333334,
      "grad_norm": 0.41976433992385864,
      "learning_rate": 4.612666666666667e-05,
      "loss": 0.0037,
      "step": 11620
    },
    {
      "epoch": 0.6202666666666666,
      "grad_norm": 0.297504723072052,
      "learning_rate": 4.612333333333333e-05,
      "loss": 0.004,
      "step": 11630
    },
    {
      "epoch": 0.6208,
      "grad_norm": 0.24124494194984436,
      "learning_rate": 4.612e-05,
      "loss": 0.0051,
      "step": 11640
    },
    {
      "epoch": 0.6213333333333333,
      "grad_norm": 0.3132185935974121,
      "learning_rate": 4.611666666666667e-05,
      "loss": 0.0029,
      "step": 11650
    },
    {
      "epoch": 0.6218666666666667,
      "grad_norm": 0.3680327832698822,
      "learning_rate": 4.611333333333334e-05,
      "loss": 0.0037,
      "step": 11660
    },
    {
      "epoch": 0.6224,
      "grad_norm": 0.18702815473079681,
      "learning_rate": 4.6110000000000004e-05,
      "loss": 0.0034,
      "step": 11670
    },
    {
      "epoch": 0.6229333333333333,
      "grad_norm": 0.2863680422306061,
      "learning_rate": 4.610666666666667e-05,
      "loss": 0.0039,
      "step": 11680
    },
    {
      "epoch": 0.6234666666666666,
      "grad_norm": 0.19956651329994202,
      "learning_rate": 4.6103333333333336e-05,
      "loss": 0.0035,
      "step": 11690
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.3701533079147339,
      "learning_rate": 4.61e-05,
      "loss": 0.0035,
      "step": 11700
    },
    {
      "epoch": 0.6245333333333334,
      "grad_norm": 0.19526053965091705,
      "learning_rate": 4.609666666666667e-05,
      "loss": 0.0038,
      "step": 11710
    },
    {
      "epoch": 0.6250666666666667,
      "grad_norm": 0.614903450012207,
      "learning_rate": 4.6093333333333335e-05,
      "loss": 0.0031,
      "step": 11720
    },
    {
      "epoch": 0.6256,
      "grad_norm": 0.4996366798877716,
      "learning_rate": 4.609e-05,
      "loss": 0.0025,
      "step": 11730
    },
    {
      "epoch": 0.6261333333333333,
      "grad_norm": 0.06266548484563828,
      "learning_rate": 4.608666666666667e-05,
      "loss": 0.0039,
      "step": 11740
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.6093223690986633,
      "learning_rate": 4.608333333333333e-05,
      "loss": 0.0051,
      "step": 11750
    },
    {
      "epoch": 0.6272,
      "grad_norm": 0.2644548714160919,
      "learning_rate": 4.608e-05,
      "loss": 0.0053,
      "step": 11760
    },
    {
      "epoch": 0.6277333333333334,
      "grad_norm": 0.15061794221401215,
      "learning_rate": 4.6076666666666665e-05,
      "loss": 0.003,
      "step": 11770
    },
    {
      "epoch": 0.6282666666666666,
      "grad_norm": 0.20669052004814148,
      "learning_rate": 4.607333333333334e-05,
      "loss": 0.0025,
      "step": 11780
    },
    {
      "epoch": 0.6288,
      "grad_norm": 0.27802354097366333,
      "learning_rate": 4.6070000000000004e-05,
      "loss": 0.0032,
      "step": 11790
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 0.23269489407539368,
      "learning_rate": 4.606666666666667e-05,
      "loss": 0.0025,
      "step": 11800
    },
    {
      "epoch": 0.6298666666666667,
      "grad_norm": 0.7692248225212097,
      "learning_rate": 4.606333333333334e-05,
      "loss": 0.0034,
      "step": 11810
    },
    {
      "epoch": 0.6304,
      "grad_norm": 0.49357548356056213,
      "learning_rate": 4.606e-05,
      "loss": 0.004,
      "step": 11820
    },
    {
      "epoch": 0.6309333333333333,
      "grad_norm": 1.0202908515930176,
      "learning_rate": 4.605666666666667e-05,
      "loss": 0.0028,
      "step": 11830
    },
    {
      "epoch": 0.6314666666666666,
      "grad_norm": 0.24818815290927887,
      "learning_rate": 4.6053333333333335e-05,
      "loss": 0.0023,
      "step": 11840
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.33933302760124207,
      "learning_rate": 4.605e-05,
      "loss": 0.0035,
      "step": 11850
    },
    {
      "epoch": 0.6325333333333333,
      "grad_norm": 0.6150435209274292,
      "learning_rate": 4.6046666666666674e-05,
      "loss": 0.0037,
      "step": 11860
    },
    {
      "epoch": 0.6330666666666667,
      "grad_norm": 0.2147885113954544,
      "learning_rate": 4.6043333333333334e-05,
      "loss": 0.0039,
      "step": 11870
    },
    {
      "epoch": 0.6336,
      "grad_norm": 0.13226714730262756,
      "learning_rate": 4.604e-05,
      "loss": 0.0027,
      "step": 11880
    },
    {
      "epoch": 0.6341333333333333,
      "grad_norm": 0.328993022441864,
      "learning_rate": 4.6036666666666666e-05,
      "loss": 0.0044,
      "step": 11890
    },
    {
      "epoch": 0.6346666666666667,
      "grad_norm": 1.0064733028411865,
      "learning_rate": 4.603333333333333e-05,
      "loss": 0.0034,
      "step": 11900
    },
    {
      "epoch": 0.6352,
      "grad_norm": 0.6010960340499878,
      "learning_rate": 4.603e-05,
      "loss": 0.0033,
      "step": 11910
    },
    {
      "epoch": 0.6357333333333334,
      "grad_norm": 0.42639246582984924,
      "learning_rate": 4.602666666666667e-05,
      "loss": 0.003,
      "step": 11920
    },
    {
      "epoch": 0.6362666666666666,
      "grad_norm": 0.4463433623313904,
      "learning_rate": 4.602333333333334e-05,
      "loss": 0.0028,
      "step": 11930
    },
    {
      "epoch": 0.6368,
      "grad_norm": 0.11528105288743973,
      "learning_rate": 4.602e-05,
      "loss": 0.0031,
      "step": 11940
    },
    {
      "epoch": 0.6373333333333333,
      "grad_norm": 0.3669276237487793,
      "learning_rate": 4.601666666666667e-05,
      "loss": 0.0026,
      "step": 11950
    },
    {
      "epoch": 0.6378666666666667,
      "grad_norm": 0.2092140018939972,
      "learning_rate": 4.6013333333333336e-05,
      "loss": 0.0034,
      "step": 11960
    },
    {
      "epoch": 0.6384,
      "grad_norm": 0.26298579573631287,
      "learning_rate": 4.601e-05,
      "loss": 0.0029,
      "step": 11970
    },
    {
      "epoch": 0.6389333333333334,
      "grad_norm": 0.4259532690048218,
      "learning_rate": 4.600666666666667e-05,
      "loss": 0.0032,
      "step": 11980
    },
    {
      "epoch": 0.6394666666666666,
      "grad_norm": 0.10203711688518524,
      "learning_rate": 4.6003333333333334e-05,
      "loss": 0.0038,
      "step": 11990
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.2015007585287094,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.003,
      "step": 12000
    },
    {
      "epoch": 0.6405333333333333,
      "grad_norm": 0.15051305294036865,
      "learning_rate": 4.599666666666667e-05,
      "loss": 0.0048,
      "step": 12010
    },
    {
      "epoch": 0.6410666666666667,
      "grad_norm": 0.4049772620201111,
      "learning_rate": 4.599333333333334e-05,
      "loss": 0.0025,
      "step": 12020
    },
    {
      "epoch": 0.6416,
      "grad_norm": 0.5621440410614014,
      "learning_rate": 4.599e-05,
      "loss": 0.0033,
      "step": 12030
    },
    {
      "epoch": 0.6421333333333333,
      "grad_norm": 0.5281689763069153,
      "learning_rate": 4.5986666666666665e-05,
      "loss": 0.0036,
      "step": 12040
    },
    {
      "epoch": 0.6426666666666667,
      "grad_norm": 0.29375168681144714,
      "learning_rate": 4.598333333333333e-05,
      "loss": 0.007,
      "step": 12050
    },
    {
      "epoch": 0.6432,
      "grad_norm": 0.15370337665081024,
      "learning_rate": 4.5980000000000004e-05,
      "loss": 0.0036,
      "step": 12060
    },
    {
      "epoch": 0.6437333333333334,
      "grad_norm": 0.3903171420097351,
      "learning_rate": 4.597666666666667e-05,
      "loss": 0.0039,
      "step": 12070
    },
    {
      "epoch": 0.6442666666666667,
      "grad_norm": 0.10996156185865402,
      "learning_rate": 4.5973333333333336e-05,
      "loss": 0.0031,
      "step": 12080
    },
    {
      "epoch": 0.6448,
      "grad_norm": 0.08554508537054062,
      "learning_rate": 4.597e-05,
      "loss": 0.003,
      "step": 12090
    },
    {
      "epoch": 0.6453333333333333,
      "grad_norm": 0.12251411378383636,
      "learning_rate": 4.596666666666667e-05,
      "loss": 0.0035,
      "step": 12100
    },
    {
      "epoch": 0.6458666666666667,
      "grad_norm": 0.7247362732887268,
      "learning_rate": 4.5963333333333334e-05,
      "loss": 0.0031,
      "step": 12110
    },
    {
      "epoch": 0.6464,
      "grad_norm": 0.2951872944831848,
      "learning_rate": 4.596e-05,
      "loss": 0.003,
      "step": 12120
    },
    {
      "epoch": 0.6469333333333334,
      "grad_norm": 0.24973805248737335,
      "learning_rate": 4.595666666666667e-05,
      "loss": 0.0037,
      "step": 12130
    },
    {
      "epoch": 0.6474666666666666,
      "grad_norm": 0.08365827798843384,
      "learning_rate": 4.595333333333334e-05,
      "loss": 0.0028,
      "step": 12140
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.1819106489419937,
      "learning_rate": 4.5950000000000006e-05,
      "loss": 0.0036,
      "step": 12150
    },
    {
      "epoch": 0.6485333333333333,
      "grad_norm": 0.7467895746231079,
      "learning_rate": 4.594666666666667e-05,
      "loss": 0.0036,
      "step": 12160
    },
    {
      "epoch": 0.6490666666666667,
      "grad_norm": 0.24045568704605103,
      "learning_rate": 4.594333333333334e-05,
      "loss": 0.0025,
      "step": 12170
    },
    {
      "epoch": 0.6496,
      "grad_norm": 0.08393393456935883,
      "learning_rate": 4.594e-05,
      "loss": 0.0027,
      "step": 12180
    },
    {
      "epoch": 0.6501333333333333,
      "grad_norm": 0.16313710808753967,
      "learning_rate": 4.593666666666666e-05,
      "loss": 0.0047,
      "step": 12190
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 0.5560476779937744,
      "learning_rate": 4.5933333333333336e-05,
      "loss": 0.0029,
      "step": 12200
    },
    {
      "epoch": 0.6512,
      "grad_norm": 0.07196565717458725,
      "learning_rate": 4.593e-05,
      "loss": 0.0032,
      "step": 12210
    },
    {
      "epoch": 0.6517333333333334,
      "grad_norm": 0.3922479748725891,
      "learning_rate": 4.592666666666667e-05,
      "loss": 0.0031,
      "step": 12220
    },
    {
      "epoch": 0.6522666666666667,
      "grad_norm": 0.09323884546756744,
      "learning_rate": 4.5923333333333335e-05,
      "loss": 0.003,
      "step": 12230
    },
    {
      "epoch": 0.6528,
      "grad_norm": 0.20720329880714417,
      "learning_rate": 4.592e-05,
      "loss": 0.0036,
      "step": 12240
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.15414603054523468,
      "learning_rate": 4.591666666666667e-05,
      "loss": 0.0032,
      "step": 12250
    },
    {
      "epoch": 0.6538666666666667,
      "grad_norm": 0.6499040126800537,
      "learning_rate": 4.591333333333333e-05,
      "loss": 0.004,
      "step": 12260
    },
    {
      "epoch": 0.6544,
      "grad_norm": 0.05890572816133499,
      "learning_rate": 4.5910000000000006e-05,
      "loss": 0.0028,
      "step": 12270
    },
    {
      "epoch": 0.6549333333333334,
      "grad_norm": 0.13371199369430542,
      "learning_rate": 4.590666666666667e-05,
      "loss": 0.0026,
      "step": 12280
    },
    {
      "epoch": 0.6554666666666666,
      "grad_norm": 0.5854426622390747,
      "learning_rate": 4.590333333333334e-05,
      "loss": 0.0022,
      "step": 12290
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.2938501238822937,
      "learning_rate": 4.5900000000000004e-05,
      "loss": 0.0035,
      "step": 12300
    },
    {
      "epoch": 0.6565333333333333,
      "grad_norm": 0.2627682089805603,
      "learning_rate": 4.589666666666667e-05,
      "loss": 0.0024,
      "step": 12310
    },
    {
      "epoch": 0.6570666666666667,
      "grad_norm": 0.327423095703125,
      "learning_rate": 4.589333333333334e-05,
      "loss": 0.0041,
      "step": 12320
    },
    {
      "epoch": 0.6576,
      "grad_norm": 0.29814550280570984,
      "learning_rate": 4.589e-05,
      "loss": 0.003,
      "step": 12330
    },
    {
      "epoch": 0.6581333333333333,
      "grad_norm": 0.35737279057502747,
      "learning_rate": 4.588666666666667e-05,
      "loss": 0.0029,
      "step": 12340
    },
    {
      "epoch": 0.6586666666666666,
      "grad_norm": 0.23128485679626465,
      "learning_rate": 4.5883333333333335e-05,
      "loss": 0.0039,
      "step": 12350
    },
    {
      "epoch": 0.6592,
      "grad_norm": 0.2683408856391907,
      "learning_rate": 4.588e-05,
      "loss": 0.0036,
      "step": 12360
    },
    {
      "epoch": 0.6597333333333333,
      "grad_norm": 0.1712934672832489,
      "learning_rate": 4.587666666666667e-05,
      "loss": 0.0038,
      "step": 12370
    },
    {
      "epoch": 0.6602666666666667,
      "grad_norm": 0.10684045404195786,
      "learning_rate": 4.5873333333333333e-05,
      "loss": 0.0033,
      "step": 12380
    },
    {
      "epoch": 0.6608,
      "grad_norm": 0.18940533697605133,
      "learning_rate": 4.587e-05,
      "loss": 0.0027,
      "step": 12390
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 0.21263843774795532,
      "learning_rate": 4.5866666666666666e-05,
      "loss": 0.0027,
      "step": 12400
    },
    {
      "epoch": 0.6618666666666667,
      "grad_norm": 0.46063661575317383,
      "learning_rate": 4.586333333333334e-05,
      "loss": 0.0038,
      "step": 12410
    },
    {
      "epoch": 0.6624,
      "grad_norm": 0.21092358231544495,
      "learning_rate": 4.5860000000000005e-05,
      "loss": 0.004,
      "step": 12420
    },
    {
      "epoch": 0.6629333333333334,
      "grad_norm": 0.29640689492225647,
      "learning_rate": 4.585666666666667e-05,
      "loss": 0.0022,
      "step": 12430
    },
    {
      "epoch": 0.6634666666666666,
      "grad_norm": 0.3108908236026764,
      "learning_rate": 4.585333333333334e-05,
      "loss": 0.0031,
      "step": 12440
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.20459043979644775,
      "learning_rate": 4.585e-05,
      "loss": 0.0043,
      "step": 12450
    },
    {
      "epoch": 0.6645333333333333,
      "grad_norm": 0.10454706102609634,
      "learning_rate": 4.584666666666667e-05,
      "loss": 0.0037,
      "step": 12460
    },
    {
      "epoch": 0.6650666666666667,
      "grad_norm": 0.07021314650774002,
      "learning_rate": 4.5843333333333335e-05,
      "loss": 0.0032,
      "step": 12470
    },
    {
      "epoch": 0.6656,
      "grad_norm": 0.4516371190547943,
      "learning_rate": 4.584e-05,
      "loss": 0.0035,
      "step": 12480
    },
    {
      "epoch": 0.6661333333333334,
      "grad_norm": 0.0953187644481659,
      "learning_rate": 4.583666666666667e-05,
      "loss": 0.0027,
      "step": 12490
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.517509400844574,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 0.0033,
      "step": 12500
    },
    {
      "epoch": 0.6672,
      "grad_norm": 0.08062810450792313,
      "learning_rate": 4.583e-05,
      "loss": 0.0028,
      "step": 12510
    },
    {
      "epoch": 0.6677333333333333,
      "grad_norm": 0.5305355191230774,
      "learning_rate": 4.5826666666666666e-05,
      "loss": 0.003,
      "step": 12520
    },
    {
      "epoch": 0.6682666666666667,
      "grad_norm": 0.45854517817497253,
      "learning_rate": 4.582333333333333e-05,
      "loss": 0.0032,
      "step": 12530
    },
    {
      "epoch": 0.6688,
      "grad_norm": 0.2162385880947113,
      "learning_rate": 4.5820000000000005e-05,
      "loss": 0.004,
      "step": 12540
    },
    {
      "epoch": 0.6693333333333333,
      "grad_norm": 0.07849369943141937,
      "learning_rate": 4.581666666666667e-05,
      "loss": 0.0032,
      "step": 12550
    },
    {
      "epoch": 0.6698666666666667,
      "grad_norm": 0.3700866997241974,
      "learning_rate": 4.581333333333334e-05,
      "loss": 0.0034,
      "step": 12560
    },
    {
      "epoch": 0.6704,
      "grad_norm": 0.45547181367874146,
      "learning_rate": 4.5810000000000004e-05,
      "loss": 0.003,
      "step": 12570
    },
    {
      "epoch": 0.6709333333333334,
      "grad_norm": 0.07354036718606949,
      "learning_rate": 4.580666666666667e-05,
      "loss": 0.0042,
      "step": 12580
    },
    {
      "epoch": 0.6714666666666667,
      "grad_norm": 0.30444979667663574,
      "learning_rate": 4.5803333333333336e-05,
      "loss": 0.0045,
      "step": 12590
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.3244226574897766,
      "learning_rate": 4.58e-05,
      "loss": 0.0046,
      "step": 12600
    },
    {
      "epoch": 0.6725333333333333,
      "grad_norm": 0.6322777271270752,
      "learning_rate": 4.579666666666667e-05,
      "loss": 0.0035,
      "step": 12610
    },
    {
      "epoch": 0.6730666666666667,
      "grad_norm": 0.22403086721897125,
      "learning_rate": 4.579333333333334e-05,
      "loss": 0.0023,
      "step": 12620
    },
    {
      "epoch": 0.6736,
      "grad_norm": 0.5253896117210388,
      "learning_rate": 4.579e-05,
      "loss": 0.003,
      "step": 12630
    },
    {
      "epoch": 0.6741333333333334,
      "grad_norm": 0.7109001278877258,
      "learning_rate": 4.5786666666666666e-05,
      "loss": 0.0019,
      "step": 12640
    },
    {
      "epoch": 0.6746666666666666,
      "grad_norm": 0.5609694719314575,
      "learning_rate": 4.578333333333333e-05,
      "loss": 0.0036,
      "step": 12650
    },
    {
      "epoch": 0.6752,
      "grad_norm": 0.3937906324863434,
      "learning_rate": 4.578e-05,
      "loss": 0.0038,
      "step": 12660
    },
    {
      "epoch": 0.6757333333333333,
      "grad_norm": 0.3229624629020691,
      "learning_rate": 4.5776666666666665e-05,
      "loss": 0.0032,
      "step": 12670
    },
    {
      "epoch": 0.6762666666666667,
      "grad_norm": 0.4607728123664856,
      "learning_rate": 4.577333333333334e-05,
      "loss": 0.0034,
      "step": 12680
    },
    {
      "epoch": 0.6768,
      "grad_norm": 0.07386045157909393,
      "learning_rate": 4.5770000000000004e-05,
      "loss": 0.0037,
      "step": 12690
    },
    {
      "epoch": 0.6773333333333333,
      "grad_norm": 0.12624193727970123,
      "learning_rate": 4.576666666666667e-05,
      "loss": 0.0024,
      "step": 12700
    },
    {
      "epoch": 0.6778666666666666,
      "grad_norm": 0.3096466064453125,
      "learning_rate": 4.5763333333333336e-05,
      "loss": 0.0033,
      "step": 12710
    },
    {
      "epoch": 0.6784,
      "grad_norm": 0.27909156680107117,
      "learning_rate": 4.576e-05,
      "loss": 0.003,
      "step": 12720
    },
    {
      "epoch": 0.6789333333333334,
      "grad_norm": 0.09199566394090652,
      "learning_rate": 4.575666666666667e-05,
      "loss": 0.0029,
      "step": 12730
    },
    {
      "epoch": 0.6794666666666667,
      "grad_norm": 0.49204888939857483,
      "learning_rate": 4.5753333333333335e-05,
      "loss": 0.0033,
      "step": 12740
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.5199247598648071,
      "learning_rate": 4.575e-05,
      "loss": 0.0034,
      "step": 12750
    },
    {
      "epoch": 0.6805333333333333,
      "grad_norm": 0.0447244793176651,
      "learning_rate": 4.5746666666666674e-05,
      "loss": 0.0031,
      "step": 12760
    },
    {
      "epoch": 0.6810666666666667,
      "grad_norm": 0.29810795187950134,
      "learning_rate": 4.574333333333334e-05,
      "loss": 0.0027,
      "step": 12770
    },
    {
      "epoch": 0.6816,
      "grad_norm": 0.19712021946907043,
      "learning_rate": 4.574e-05,
      "loss": 0.0041,
      "step": 12780
    },
    {
      "epoch": 0.6821333333333334,
      "grad_norm": 0.391665518283844,
      "learning_rate": 4.5736666666666665e-05,
      "loss": 0.004,
      "step": 12790
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 0.16929954290390015,
      "learning_rate": 4.573333333333333e-05,
      "loss": 0.0032,
      "step": 12800
    },
    {
      "epoch": 0.6832,
      "grad_norm": 0.08074866980314255,
      "learning_rate": 4.573e-05,
      "loss": 0.0034,
      "step": 12810
    },
    {
      "epoch": 0.6837333333333333,
      "grad_norm": 0.4730296730995178,
      "learning_rate": 4.572666666666667e-05,
      "loss": 0.0036,
      "step": 12820
    },
    {
      "epoch": 0.6842666666666667,
      "grad_norm": 0.33489447832107544,
      "learning_rate": 4.5723333333333337e-05,
      "loss": 0.0043,
      "step": 12830
    },
    {
      "epoch": 0.6848,
      "grad_norm": 0.7071881294250488,
      "learning_rate": 4.572e-05,
      "loss": 0.0025,
      "step": 12840
    },
    {
      "epoch": 0.6853333333333333,
      "grad_norm": 0.11456087231636047,
      "learning_rate": 4.571666666666667e-05,
      "loss": 0.0043,
      "step": 12850
    },
    {
      "epoch": 0.6858666666666666,
      "grad_norm": 0.42524832487106323,
      "learning_rate": 4.5713333333333335e-05,
      "loss": 0.0042,
      "step": 12860
    },
    {
      "epoch": 0.6864,
      "grad_norm": 0.17616212368011475,
      "learning_rate": 4.571e-05,
      "loss": 0.0029,
      "step": 12870
    },
    {
      "epoch": 0.6869333333333333,
      "grad_norm": 0.9976392984390259,
      "learning_rate": 4.570666666666667e-05,
      "loss": 0.0033,
      "step": 12880
    },
    {
      "epoch": 0.6874666666666667,
      "grad_norm": 0.44817879796028137,
      "learning_rate": 4.570333333333334e-05,
      "loss": 0.0044,
      "step": 12890
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.27090221643447876,
      "learning_rate": 4.5700000000000006e-05,
      "loss": 0.0027,
      "step": 12900
    },
    {
      "epoch": 0.6885333333333333,
      "grad_norm": 0.1558438092470169,
      "learning_rate": 4.569666666666667e-05,
      "loss": 0.0042,
      "step": 12910
    },
    {
      "epoch": 0.6890666666666667,
      "grad_norm": 0.1278866082429886,
      "learning_rate": 4.569333333333334e-05,
      "loss": 0.0024,
      "step": 12920
    },
    {
      "epoch": 0.6896,
      "grad_norm": 0.5514485239982605,
      "learning_rate": 4.569e-05,
      "loss": 0.0036,
      "step": 12930
    },
    {
      "epoch": 0.6901333333333334,
      "grad_norm": 0.12915122509002686,
      "learning_rate": 4.5686666666666664e-05,
      "loss": 0.0031,
      "step": 12940
    },
    {
      "epoch": 0.6906666666666667,
      "grad_norm": 0.08316066116094589,
      "learning_rate": 4.568333333333333e-05,
      "loss": 0.0021,
      "step": 12950
    },
    {
      "epoch": 0.6912,
      "grad_norm": 0.03854706510901451,
      "learning_rate": 4.568e-05,
      "loss": 0.0042,
      "step": 12960
    },
    {
      "epoch": 0.6917333333333333,
      "grad_norm": 0.7093377709388733,
      "learning_rate": 4.567666666666667e-05,
      "loss": 0.0034,
      "step": 12970
    },
    {
      "epoch": 0.6922666666666667,
      "grad_norm": 0.07031486183404922,
      "learning_rate": 4.5673333333333335e-05,
      "loss": 0.0026,
      "step": 12980
    },
    {
      "epoch": 0.6928,
      "grad_norm": 0.2007848024368286,
      "learning_rate": 4.567e-05,
      "loss": 0.0035,
      "step": 12990
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.2753136456012726,
      "learning_rate": 4.566666666666667e-05,
      "loss": 0.0041,
      "step": 13000
    },
    {
      "epoch": 0.6938666666666666,
      "grad_norm": 0.24142929911613464,
      "learning_rate": 4.5663333333333334e-05,
      "loss": 0.0035,
      "step": 13010
    },
    {
      "epoch": 0.6944,
      "grad_norm": 0.7134174704551697,
      "learning_rate": 4.566e-05,
      "loss": 0.0024,
      "step": 13020
    },
    {
      "epoch": 0.6949333333333333,
      "grad_norm": 0.16831329464912415,
      "learning_rate": 4.565666666666667e-05,
      "loss": 0.0025,
      "step": 13030
    },
    {
      "epoch": 0.6954666666666667,
      "grad_norm": 0.19711385667324066,
      "learning_rate": 4.565333333333334e-05,
      "loss": 0.0035,
      "step": 13040
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.6501051187515259,
      "learning_rate": 4.5650000000000005e-05,
      "loss": 0.0032,
      "step": 13050
    },
    {
      "epoch": 0.6965333333333333,
      "grad_norm": 0.21116790175437927,
      "learning_rate": 4.564666666666667e-05,
      "loss": 0.0039,
      "step": 13060
    },
    {
      "epoch": 0.6970666666666666,
      "grad_norm": 0.46666333079338074,
      "learning_rate": 4.564333333333334e-05,
      "loss": 0.0027,
      "step": 13070
    },
    {
      "epoch": 0.6976,
      "grad_norm": 0.20469585061073303,
      "learning_rate": 4.564e-05,
      "loss": 0.003,
      "step": 13080
    },
    {
      "epoch": 0.6981333333333334,
      "grad_norm": 0.7381211519241333,
      "learning_rate": 4.563666666666667e-05,
      "loss": 0.0035,
      "step": 13090
    },
    {
      "epoch": 0.6986666666666667,
      "grad_norm": 0.40635085105895996,
      "learning_rate": 4.5633333333333336e-05,
      "loss": 0.0037,
      "step": 13100
    },
    {
      "epoch": 0.6992,
      "grad_norm": 0.6787230372428894,
      "learning_rate": 4.563e-05,
      "loss": 0.0028,
      "step": 13110
    },
    {
      "epoch": 0.6997333333333333,
      "grad_norm": 0.6018537878990173,
      "learning_rate": 4.562666666666667e-05,
      "loss": 0.0031,
      "step": 13120
    },
    {
      "epoch": 0.7002666666666667,
      "grad_norm": 0.3542638421058655,
      "learning_rate": 4.5623333333333334e-05,
      "loss": 0.0018,
      "step": 13130
    },
    {
      "epoch": 0.7008,
      "grad_norm": 0.2958070635795593,
      "learning_rate": 4.562e-05,
      "loss": 0.0031,
      "step": 13140
    },
    {
      "epoch": 0.7013333333333334,
      "grad_norm": 0.11851686239242554,
      "learning_rate": 4.5616666666666666e-05,
      "loss": 0.0032,
      "step": 13150
    },
    {
      "epoch": 0.7018666666666666,
      "grad_norm": 0.19869138300418854,
      "learning_rate": 4.561333333333333e-05,
      "loss": 0.0028,
      "step": 13160
    },
    {
      "epoch": 0.7024,
      "grad_norm": 0.19339101016521454,
      "learning_rate": 4.5610000000000005e-05,
      "loss": 0.0035,
      "step": 13170
    },
    {
      "epoch": 0.7029333333333333,
      "grad_norm": 0.5121462345123291,
      "learning_rate": 4.560666666666667e-05,
      "loss": 0.0031,
      "step": 13180
    },
    {
      "epoch": 0.7034666666666667,
      "grad_norm": 0.1999703049659729,
      "learning_rate": 4.560333333333334e-05,
      "loss": 0.0046,
      "step": 13190
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.5850855112075806,
      "learning_rate": 4.5600000000000004e-05,
      "loss": 0.0035,
      "step": 13200
    },
    {
      "epoch": 0.7045333333333333,
      "grad_norm": 0.1320990025997162,
      "learning_rate": 4.559666666666667e-05,
      "loss": 0.0028,
      "step": 13210
    },
    {
      "epoch": 0.7050666666666666,
      "grad_norm": 0.8332219123840332,
      "learning_rate": 4.5593333333333336e-05,
      "loss": 0.0026,
      "step": 13220
    },
    {
      "epoch": 0.7056,
      "grad_norm": 0.4879235029220581,
      "learning_rate": 4.559e-05,
      "loss": 0.0036,
      "step": 13230
    },
    {
      "epoch": 0.7061333333333333,
      "grad_norm": 0.46011462807655334,
      "learning_rate": 4.558666666666667e-05,
      "loss": 0.0018,
      "step": 13240
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.5524890422821045,
      "learning_rate": 4.5583333333333335e-05,
      "loss": 0.0025,
      "step": 13250
    },
    {
      "epoch": 0.7072,
      "grad_norm": 0.33961722254753113,
      "learning_rate": 4.558e-05,
      "loss": 0.0036,
      "step": 13260
    },
    {
      "epoch": 0.7077333333333333,
      "grad_norm": 0.37099510431289673,
      "learning_rate": 4.557666666666667e-05,
      "loss": 0.0028,
      "step": 13270
    },
    {
      "epoch": 0.7082666666666667,
      "grad_norm": 0.7495374083518982,
      "learning_rate": 4.557333333333333e-05,
      "loss": 0.004,
      "step": 13280
    },
    {
      "epoch": 0.7088,
      "grad_norm": 0.48619717359542847,
      "learning_rate": 4.557e-05,
      "loss": 0.0035,
      "step": 13290
    },
    {
      "epoch": 0.7093333333333334,
      "grad_norm": 0.4770013391971588,
      "learning_rate": 4.556666666666667e-05,
      "loss": 0.0028,
      "step": 13300
    },
    {
      "epoch": 0.7098666666666666,
      "grad_norm": 0.18569323420524597,
      "learning_rate": 4.556333333333334e-05,
      "loss": 0.0028,
      "step": 13310
    },
    {
      "epoch": 0.7104,
      "grad_norm": 0.5160402059555054,
      "learning_rate": 4.5560000000000004e-05,
      "loss": 0.0039,
      "step": 13320
    },
    {
      "epoch": 0.7109333333333333,
      "grad_norm": 0.1678091138601303,
      "learning_rate": 4.555666666666667e-05,
      "loss": 0.0027,
      "step": 13330
    },
    {
      "epoch": 0.7114666666666667,
      "grad_norm": 0.26269271969795227,
      "learning_rate": 4.5553333333333337e-05,
      "loss": 0.0034,
      "step": 13340
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.5502853989601135,
      "learning_rate": 4.555e-05,
      "loss": 0.0039,
      "step": 13350
    },
    {
      "epoch": 0.7125333333333334,
      "grad_norm": 0.5142942667007446,
      "learning_rate": 4.554666666666667e-05,
      "loss": 0.0036,
      "step": 13360
    },
    {
      "epoch": 0.7130666666666666,
      "grad_norm": 0.26368966698646545,
      "learning_rate": 4.5543333333333335e-05,
      "loss": 0.0036,
      "step": 13370
    },
    {
      "epoch": 0.7136,
      "grad_norm": 0.6391733288764954,
      "learning_rate": 4.554000000000001e-05,
      "loss": 0.0043,
      "step": 13380
    },
    {
      "epoch": 0.7141333333333333,
      "grad_norm": 0.44539156556129456,
      "learning_rate": 4.553666666666667e-05,
      "loss": 0.0046,
      "step": 13390
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 0.5455512404441833,
      "learning_rate": 4.553333333333333e-05,
      "loss": 0.0026,
      "step": 13400
    },
    {
      "epoch": 0.7152,
      "grad_norm": 0.23030613362789154,
      "learning_rate": 4.553e-05,
      "loss": 0.0041,
      "step": 13410
    },
    {
      "epoch": 0.7157333333333333,
      "grad_norm": 0.4236566424369812,
      "learning_rate": 4.5526666666666666e-05,
      "loss": 0.0037,
      "step": 13420
    },
    {
      "epoch": 0.7162666666666667,
      "grad_norm": 0.2104969471693039,
      "learning_rate": 4.552333333333333e-05,
      "loss": 0.0031,
      "step": 13430
    },
    {
      "epoch": 0.7168,
      "grad_norm": 0.2681254744529724,
      "learning_rate": 4.5520000000000005e-05,
      "loss": 0.0034,
      "step": 13440
    },
    {
      "epoch": 0.7173333333333334,
      "grad_norm": 0.2459641993045807,
      "learning_rate": 4.551666666666667e-05,
      "loss": 0.0034,
      "step": 13450
    },
    {
      "epoch": 0.7178666666666667,
      "grad_norm": 0.25497305393218994,
      "learning_rate": 4.551333333333334e-05,
      "loss": 0.002,
      "step": 13460
    },
    {
      "epoch": 0.7184,
      "grad_norm": 0.6224676966667175,
      "learning_rate": 4.551e-05,
      "loss": 0.0021,
      "step": 13470
    },
    {
      "epoch": 0.7189333333333333,
      "grad_norm": 0.6501907110214233,
      "learning_rate": 4.550666666666667e-05,
      "loss": 0.0036,
      "step": 13480
    },
    {
      "epoch": 0.7194666666666667,
      "grad_norm": 0.21059061586856842,
      "learning_rate": 4.5503333333333335e-05,
      "loss": 0.0037,
      "step": 13490
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.12282441556453705,
      "learning_rate": 4.55e-05,
      "loss": 0.0029,
      "step": 13500
    },
    {
      "epoch": 0.7205333333333334,
      "grad_norm": 0.08488161861896515,
      "learning_rate": 4.549666666666667e-05,
      "loss": 0.0033,
      "step": 13510
    },
    {
      "epoch": 0.7210666666666666,
      "grad_norm": 0.38500961661338806,
      "learning_rate": 4.549333333333334e-05,
      "loss": 0.0047,
      "step": 13520
    },
    {
      "epoch": 0.7216,
      "grad_norm": 0.545289158821106,
      "learning_rate": 4.549000000000001e-05,
      "loss": 0.0023,
      "step": 13530
    },
    {
      "epoch": 0.7221333333333333,
      "grad_norm": 0.5467126965522766,
      "learning_rate": 4.5486666666666666e-05,
      "loss": 0.0033,
      "step": 13540
    },
    {
      "epoch": 0.7226666666666667,
      "grad_norm": 0.35336819291114807,
      "learning_rate": 4.548333333333333e-05,
      "loss": 0.0029,
      "step": 13550
    },
    {
      "epoch": 0.7232,
      "grad_norm": 0.3211611211299896,
      "learning_rate": 4.548e-05,
      "loss": 0.0035,
      "step": 13560
    },
    {
      "epoch": 0.7237333333333333,
      "grad_norm": 0.05300593748688698,
      "learning_rate": 4.5476666666666664e-05,
      "loss": 0.0036,
      "step": 13570
    },
    {
      "epoch": 0.7242666666666666,
      "grad_norm": 0.2484586238861084,
      "learning_rate": 4.547333333333334e-05,
      "loss": 0.0028,
      "step": 13580
    },
    {
      "epoch": 0.7248,
      "grad_norm": 0.19407255947589874,
      "learning_rate": 4.5470000000000003e-05,
      "loss": 0.0041,
      "step": 13590
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 0.25900375843048096,
      "learning_rate": 4.546666666666667e-05,
      "loss": 0.0022,
      "step": 13600
    },
    {
      "epoch": 0.7258666666666667,
      "grad_norm": 0.2270510047674179,
      "learning_rate": 4.5463333333333336e-05,
      "loss": 0.0032,
      "step": 13610
    },
    {
      "epoch": 0.7264,
      "grad_norm": 0.1593560129404068,
      "learning_rate": 4.546e-05,
      "loss": 0.0037,
      "step": 13620
    },
    {
      "epoch": 0.7269333333333333,
      "grad_norm": 0.11973842233419418,
      "learning_rate": 4.545666666666667e-05,
      "loss": 0.0035,
      "step": 13630
    },
    {
      "epoch": 0.7274666666666667,
      "grad_norm": 0.6294111013412476,
      "learning_rate": 4.5453333333333334e-05,
      "loss": 0.002,
      "step": 13640
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.14364810287952423,
      "learning_rate": 4.545000000000001e-05,
      "loss": 0.0029,
      "step": 13650
    },
    {
      "epoch": 0.7285333333333334,
      "grad_norm": 0.17813774943351746,
      "learning_rate": 4.544666666666667e-05,
      "loss": 0.0029,
      "step": 13660
    },
    {
      "epoch": 0.7290666666666666,
      "grad_norm": 0.42729002237319946,
      "learning_rate": 4.544333333333334e-05,
      "loss": 0.0034,
      "step": 13670
    },
    {
      "epoch": 0.7296,
      "grad_norm": 0.4203334152698517,
      "learning_rate": 4.5440000000000005e-05,
      "loss": 0.0045,
      "step": 13680
    },
    {
      "epoch": 0.7301333333333333,
      "grad_norm": 0.5750209093093872,
      "learning_rate": 4.5436666666666665e-05,
      "loss": 0.0029,
      "step": 13690
    },
    {
      "epoch": 0.7306666666666667,
      "grad_norm": 0.05238012596964836,
      "learning_rate": 4.543333333333333e-05,
      "loss": 0.003,
      "step": 13700
    },
    {
      "epoch": 0.7312,
      "grad_norm": 0.51966392993927,
      "learning_rate": 4.543e-05,
      "loss": 0.002,
      "step": 13710
    },
    {
      "epoch": 0.7317333333333333,
      "grad_norm": 0.09259005635976791,
      "learning_rate": 4.542666666666667e-05,
      "loss": 0.0026,
      "step": 13720
    },
    {
      "epoch": 0.7322666666666666,
      "grad_norm": 0.16773860156536102,
      "learning_rate": 4.5423333333333336e-05,
      "loss": 0.0017,
      "step": 13730
    },
    {
      "epoch": 0.7328,
      "grad_norm": 0.13546434044837952,
      "learning_rate": 4.542e-05,
      "loss": 0.0028,
      "step": 13740
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.38737258315086365,
      "learning_rate": 4.541666666666667e-05,
      "loss": 0.0036,
      "step": 13750
    },
    {
      "epoch": 0.7338666666666667,
      "grad_norm": 0.3038052022457123,
      "learning_rate": 4.5413333333333334e-05,
      "loss": 0.0034,
      "step": 13760
    },
    {
      "epoch": 0.7344,
      "grad_norm": 0.333885133266449,
      "learning_rate": 4.541e-05,
      "loss": 0.0025,
      "step": 13770
    },
    {
      "epoch": 0.7349333333333333,
      "grad_norm": 0.45315390825271606,
      "learning_rate": 4.540666666666667e-05,
      "loss": 0.0021,
      "step": 13780
    },
    {
      "epoch": 0.7354666666666667,
      "grad_norm": 0.082288958132267,
      "learning_rate": 4.540333333333334e-05,
      "loss": 0.0034,
      "step": 13790
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.32415077090263367,
      "learning_rate": 4.5400000000000006e-05,
      "loss": 0.003,
      "step": 13800
    },
    {
      "epoch": 0.7365333333333334,
      "grad_norm": 0.24915574491024017,
      "learning_rate": 4.539666666666667e-05,
      "loss": 0.0039,
      "step": 13810
    },
    {
      "epoch": 0.7370666666666666,
      "grad_norm": 0.3283126950263977,
      "learning_rate": 4.539333333333334e-05,
      "loss": 0.0035,
      "step": 13820
    },
    {
      "epoch": 0.7376,
      "grad_norm": 0.1638062596321106,
      "learning_rate": 4.5390000000000004e-05,
      "loss": 0.0022,
      "step": 13830
    },
    {
      "epoch": 0.7381333333333333,
      "grad_norm": 0.10233980417251587,
      "learning_rate": 4.5386666666666664e-05,
      "loss": 0.0026,
      "step": 13840
    },
    {
      "epoch": 0.7386666666666667,
      "grad_norm": 0.8655907511711121,
      "learning_rate": 4.5383333333333336e-05,
      "loss": 0.0026,
      "step": 13850
    },
    {
      "epoch": 0.7392,
      "grad_norm": 0.13583363592624664,
      "learning_rate": 4.538e-05,
      "loss": 0.003,
      "step": 13860
    },
    {
      "epoch": 0.7397333333333334,
      "grad_norm": 0.6233576536178589,
      "learning_rate": 4.537666666666667e-05,
      "loss": 0.0026,
      "step": 13870
    },
    {
      "epoch": 0.7402666666666666,
      "grad_norm": 0.23314635455608368,
      "learning_rate": 4.5373333333333335e-05,
      "loss": 0.0026,
      "step": 13880
    },
    {
      "epoch": 0.7408,
      "grad_norm": 0.8988991379737854,
      "learning_rate": 4.537e-05,
      "loss": 0.0028,
      "step": 13890
    },
    {
      "epoch": 0.7413333333333333,
      "grad_norm": 0.5149164199829102,
      "learning_rate": 4.536666666666667e-05,
      "loss": 0.0041,
      "step": 13900
    },
    {
      "epoch": 0.7418666666666667,
      "grad_norm": 0.22275196015834808,
      "learning_rate": 4.536333333333333e-05,
      "loss": 0.0041,
      "step": 13910
    },
    {
      "epoch": 0.7424,
      "grad_norm": 0.8755754828453064,
      "learning_rate": 4.536e-05,
      "loss": 0.003,
      "step": 13920
    },
    {
      "epoch": 0.7429333333333333,
      "grad_norm": 0.707038938999176,
      "learning_rate": 4.535666666666667e-05,
      "loss": 0.003,
      "step": 13930
    },
    {
      "epoch": 0.7434666666666667,
      "grad_norm": 0.5177569389343262,
      "learning_rate": 4.535333333333334e-05,
      "loss": 0.0026,
      "step": 13940
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.3549521267414093,
      "learning_rate": 4.5350000000000005e-05,
      "loss": 0.0028,
      "step": 13950
    },
    {
      "epoch": 0.7445333333333334,
      "grad_norm": 0.5757145285606384,
      "learning_rate": 4.534666666666667e-05,
      "loss": 0.0027,
      "step": 13960
    },
    {
      "epoch": 0.7450666666666667,
      "grad_norm": 0.5409225225448608,
      "learning_rate": 4.534333333333334e-05,
      "loss": 0.0033,
      "step": 13970
    },
    {
      "epoch": 0.7456,
      "grad_norm": 0.20423361659049988,
      "learning_rate": 4.534e-05,
      "loss": 0.0038,
      "step": 13980
    },
    {
      "epoch": 0.7461333333333333,
      "grad_norm": 0.3203679025173187,
      "learning_rate": 4.533666666666667e-05,
      "loss": 0.0037,
      "step": 13990
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.4237651824951172,
      "learning_rate": 4.5333333333333335e-05,
      "loss": 0.0039,
      "step": 14000
    },
    {
      "epoch": 0.7472,
      "grad_norm": 0.3197522759437561,
      "learning_rate": 4.533e-05,
      "loss": 0.0027,
      "step": 14010
    },
    {
      "epoch": 0.7477333333333334,
      "grad_norm": 0.2767338156700134,
      "learning_rate": 4.532666666666667e-05,
      "loss": 0.003,
      "step": 14020
    },
    {
      "epoch": 0.7482666666666666,
      "grad_norm": 0.4822731614112854,
      "learning_rate": 4.5323333333333334e-05,
      "loss": 0.0028,
      "step": 14030
    },
    {
      "epoch": 0.7488,
      "grad_norm": 0.28386324644088745,
      "learning_rate": 4.532e-05,
      "loss": 0.0029,
      "step": 14040
    },
    {
      "epoch": 0.7493333333333333,
      "grad_norm": 0.35931485891342163,
      "learning_rate": 4.5316666666666666e-05,
      "loss": 0.0036,
      "step": 14050
    },
    {
      "epoch": 0.7498666666666667,
      "grad_norm": 0.2213209569454193,
      "learning_rate": 4.531333333333333e-05,
      "loss": 0.0036,
      "step": 14060
    },
    {
      "epoch": 0.7504,
      "grad_norm": 0.11146708577871323,
      "learning_rate": 4.5310000000000005e-05,
      "loss": 0.0022,
      "step": 14070
    },
    {
      "epoch": 0.7509333333333333,
      "grad_norm": 0.10226770490407944,
      "learning_rate": 4.530666666666667e-05,
      "loss": 0.0025,
      "step": 14080
    },
    {
      "epoch": 0.7514666666666666,
      "grad_norm": 0.7016504406929016,
      "learning_rate": 4.530333333333334e-05,
      "loss": 0.0031,
      "step": 14090
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.1515292376279831,
      "learning_rate": 4.53e-05,
      "loss": 0.0031,
      "step": 14100
    },
    {
      "epoch": 0.7525333333333334,
      "grad_norm": 0.11737135797739029,
      "learning_rate": 4.529666666666667e-05,
      "loss": 0.0045,
      "step": 14110
    },
    {
      "epoch": 0.7530666666666667,
      "grad_norm": 0.23402120172977448,
      "learning_rate": 4.5293333333333336e-05,
      "loss": 0.0039,
      "step": 14120
    },
    {
      "epoch": 0.7536,
      "grad_norm": 0.7732112407684326,
      "learning_rate": 4.529e-05,
      "loss": 0.0042,
      "step": 14130
    },
    {
      "epoch": 0.7541333333333333,
      "grad_norm": 0.09455812722444534,
      "learning_rate": 4.528666666666667e-05,
      "loss": 0.0022,
      "step": 14140
    },
    {
      "epoch": 0.7546666666666667,
      "grad_norm": 1.054227590560913,
      "learning_rate": 4.5283333333333334e-05,
      "loss": 0.0032,
      "step": 14150
    },
    {
      "epoch": 0.7552,
      "grad_norm": 0.44969165325164795,
      "learning_rate": 4.528e-05,
      "loss": 0.0028,
      "step": 14160
    },
    {
      "epoch": 0.7557333333333334,
      "grad_norm": 0.13901041448116302,
      "learning_rate": 4.5276666666666666e-05,
      "loss": 0.003,
      "step": 14170
    },
    {
      "epoch": 0.7562666666666666,
      "grad_norm": 0.2339831441640854,
      "learning_rate": 4.527333333333333e-05,
      "loss": 0.0022,
      "step": 14180
    },
    {
      "epoch": 0.7568,
      "grad_norm": 0.2568807899951935,
      "learning_rate": 4.527e-05,
      "loss": 0.0034,
      "step": 14190
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 0.15818479657173157,
      "learning_rate": 4.526666666666667e-05,
      "loss": 0.0047,
      "step": 14200
    },
    {
      "epoch": 0.7578666666666667,
      "grad_norm": 0.7315681576728821,
      "learning_rate": 4.526333333333334e-05,
      "loss": 0.0041,
      "step": 14210
    },
    {
      "epoch": 0.7584,
      "grad_norm": 0.2419188916683197,
      "learning_rate": 4.5260000000000004e-05,
      "loss": 0.0026,
      "step": 14220
    },
    {
      "epoch": 0.7589333333333333,
      "grad_norm": 0.28657013177871704,
      "learning_rate": 4.525666666666667e-05,
      "loss": 0.0033,
      "step": 14230
    },
    {
      "epoch": 0.7594666666666666,
      "grad_norm": 0.060926880687475204,
      "learning_rate": 4.5253333333333336e-05,
      "loss": 0.0024,
      "step": 14240
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.3240782916545868,
      "learning_rate": 4.525e-05,
      "loss": 0.0039,
      "step": 14250
    },
    {
      "epoch": 0.7605333333333333,
      "grad_norm": 0.3212386667728424,
      "learning_rate": 4.524666666666667e-05,
      "loss": 0.0032,
      "step": 14260
    },
    {
      "epoch": 0.7610666666666667,
      "grad_norm": 0.2005634307861328,
      "learning_rate": 4.5243333333333334e-05,
      "loss": 0.0022,
      "step": 14270
    },
    {
      "epoch": 0.7616,
      "grad_norm": 0.10212094336748123,
      "learning_rate": 4.524000000000001e-05,
      "loss": 0.0026,
      "step": 14280
    },
    {
      "epoch": 0.7621333333333333,
      "grad_norm": 0.19503356516361237,
      "learning_rate": 4.523666666666667e-05,
      "loss": 0.0019,
      "step": 14290
    },
    {
      "epoch": 0.7626666666666667,
      "grad_norm": 0.06323835253715515,
      "learning_rate": 4.523333333333333e-05,
      "loss": 0.0038,
      "step": 14300
    },
    {
      "epoch": 0.7632,
      "grad_norm": 0.4472815692424774,
      "learning_rate": 4.523e-05,
      "loss": 0.0028,
      "step": 14310
    },
    {
      "epoch": 0.7637333333333334,
      "grad_norm": 0.33080872893333435,
      "learning_rate": 4.5226666666666665e-05,
      "loss": 0.0034,
      "step": 14320
    },
    {
      "epoch": 0.7642666666666666,
      "grad_norm": 0.12395952641963959,
      "learning_rate": 4.522333333333333e-05,
      "loss": 0.0026,
      "step": 14330
    },
    {
      "epoch": 0.7648,
      "grad_norm": 0.5866529941558838,
      "learning_rate": 4.5220000000000004e-05,
      "loss": 0.0032,
      "step": 14340
    },
    {
      "epoch": 0.7653333333333333,
      "grad_norm": 0.3845774829387665,
      "learning_rate": 4.521666666666667e-05,
      "loss": 0.0041,
      "step": 14350
    },
    {
      "epoch": 0.7658666666666667,
      "grad_norm": 0.27433374524116516,
      "learning_rate": 4.5213333333333336e-05,
      "loss": 0.0037,
      "step": 14360
    },
    {
      "epoch": 0.7664,
      "grad_norm": 0.30518877506256104,
      "learning_rate": 4.521e-05,
      "loss": 0.0056,
      "step": 14370
    },
    {
      "epoch": 0.7669333333333334,
      "grad_norm": 0.42739230394363403,
      "learning_rate": 4.520666666666667e-05,
      "loss": 0.0033,
      "step": 14380
    },
    {
      "epoch": 0.7674666666666666,
      "grad_norm": 0.482359915971756,
      "learning_rate": 4.5203333333333335e-05,
      "loss": 0.0032,
      "step": 14390
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.37557870149612427,
      "learning_rate": 4.52e-05,
      "loss": 0.0024,
      "step": 14400
    },
    {
      "epoch": 0.7685333333333333,
      "grad_norm": 0.4815393090248108,
      "learning_rate": 4.5196666666666674e-05,
      "loss": 0.0041,
      "step": 14410
    },
    {
      "epoch": 0.7690666666666667,
      "grad_norm": 0.6430329084396362,
      "learning_rate": 4.519333333333334e-05,
      "loss": 0.0035,
      "step": 14420
    },
    {
      "epoch": 0.7696,
      "grad_norm": 0.22098687291145325,
      "learning_rate": 4.5190000000000006e-05,
      "loss": 0.003,
      "step": 14430
    },
    {
      "epoch": 0.7701333333333333,
      "grad_norm": 0.1809033304452896,
      "learning_rate": 4.518666666666667e-05,
      "loss": 0.0022,
      "step": 14440
    },
    {
      "epoch": 0.7706666666666667,
      "grad_norm": 0.46167248487472534,
      "learning_rate": 4.518333333333333e-05,
      "loss": 0.003,
      "step": 14450
    },
    {
      "epoch": 0.7712,
      "grad_norm": 0.13413499295711517,
      "learning_rate": 4.518e-05,
      "loss": 0.0026,
      "step": 14460
    },
    {
      "epoch": 0.7717333333333334,
      "grad_norm": 0.3271285891532898,
      "learning_rate": 4.5176666666666664e-05,
      "loss": 0.0036,
      "step": 14470
    },
    {
      "epoch": 0.7722666666666667,
      "grad_norm": 0.35419565439224243,
      "learning_rate": 4.517333333333334e-05,
      "loss": 0.002,
      "step": 14480
    },
    {
      "epoch": 0.7728,
      "grad_norm": 0.29523584246635437,
      "learning_rate": 4.517e-05,
      "loss": 0.0031,
      "step": 14490
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.25754645466804504,
      "learning_rate": 4.516666666666667e-05,
      "loss": 0.0031,
      "step": 14500
    },
    {
      "epoch": 0.7738666666666667,
      "grad_norm": 0.5831186771392822,
      "learning_rate": 4.5163333333333335e-05,
      "loss": 0.0042,
      "step": 14510
    },
    {
      "epoch": 0.7744,
      "grad_norm": 0.10090429335832596,
      "learning_rate": 4.516e-05,
      "loss": 0.0025,
      "step": 14520
    },
    {
      "epoch": 0.7749333333333334,
      "grad_norm": 0.3803465962409973,
      "learning_rate": 4.515666666666667e-05,
      "loss": 0.0043,
      "step": 14530
    },
    {
      "epoch": 0.7754666666666666,
      "grad_norm": 0.3531315326690674,
      "learning_rate": 4.5153333333333334e-05,
      "loss": 0.0038,
      "step": 14540
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.3589571416378021,
      "learning_rate": 4.5150000000000006e-05,
      "loss": 0.0041,
      "step": 14550
    },
    {
      "epoch": 0.7765333333333333,
      "grad_norm": 0.29655569791793823,
      "learning_rate": 4.514666666666667e-05,
      "loss": 0.0043,
      "step": 14560
    },
    {
      "epoch": 0.7770666666666667,
      "grad_norm": 0.30197063088417053,
      "learning_rate": 4.514333333333334e-05,
      "loss": 0.004,
      "step": 14570
    },
    {
      "epoch": 0.7776,
      "grad_norm": 0.24087578058242798,
      "learning_rate": 4.5140000000000005e-05,
      "loss": 0.0037,
      "step": 14580
    },
    {
      "epoch": 0.7781333333333333,
      "grad_norm": 0.2696652412414551,
      "learning_rate": 4.513666666666667e-05,
      "loss": 0.0035,
      "step": 14590
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 0.05891913175582886,
      "learning_rate": 4.513333333333333e-05,
      "loss": 0.0036,
      "step": 14600
    },
    {
      "epoch": 0.7792,
      "grad_norm": 0.47444987297058105,
      "learning_rate": 4.513e-05,
      "loss": 0.0032,
      "step": 14610
    },
    {
      "epoch": 0.7797333333333333,
      "grad_norm": 0.13592515885829926,
      "learning_rate": 4.512666666666667e-05,
      "loss": 0.0043,
      "step": 14620
    },
    {
      "epoch": 0.7802666666666667,
      "grad_norm": 0.08721011132001877,
      "learning_rate": 4.5123333333333336e-05,
      "loss": 0.0047,
      "step": 14630
    },
    {
      "epoch": 0.7808,
      "grad_norm": 0.2932867705821991,
      "learning_rate": 4.512e-05,
      "loss": 0.0027,
      "step": 14640
    },
    {
      "epoch": 0.7813333333333333,
      "grad_norm": 0.3506499230861664,
      "learning_rate": 4.511666666666667e-05,
      "loss": 0.0029,
      "step": 14650
    },
    {
      "epoch": 0.7818666666666667,
      "grad_norm": 0.829807698726654,
      "learning_rate": 4.5113333333333334e-05,
      "loss": 0.0021,
      "step": 14660
    },
    {
      "epoch": 0.7824,
      "grad_norm": 0.2923949360847473,
      "learning_rate": 4.511e-05,
      "loss": 0.0018,
      "step": 14670
    },
    {
      "epoch": 0.7829333333333334,
      "grad_norm": 0.44668784737586975,
      "learning_rate": 4.5106666666666666e-05,
      "loss": 0.0036,
      "step": 14680
    },
    {
      "epoch": 0.7834666666666666,
      "grad_norm": 0.18920530378818512,
      "learning_rate": 4.510333333333334e-05,
      "loss": 0.0031,
      "step": 14690
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.41398003697395325,
      "learning_rate": 4.5100000000000005e-05,
      "loss": 0.0024,
      "step": 14700
    },
    {
      "epoch": 0.7845333333333333,
      "grad_norm": 0.4483768343925476,
      "learning_rate": 4.509666666666667e-05,
      "loss": 0.0029,
      "step": 14710
    },
    {
      "epoch": 0.7850666666666667,
      "grad_norm": 0.10674877464771271,
      "learning_rate": 4.509333333333334e-05,
      "loss": 0.0047,
      "step": 14720
    },
    {
      "epoch": 0.7856,
      "grad_norm": 0.0642167329788208,
      "learning_rate": 4.5090000000000004e-05,
      "loss": 0.0029,
      "step": 14730
    },
    {
      "epoch": 0.7861333333333334,
      "grad_norm": 0.19883541762828827,
      "learning_rate": 4.508666666666667e-05,
      "loss": 0.0026,
      "step": 14740
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.6049943566322327,
      "learning_rate": 4.5083333333333336e-05,
      "loss": 0.0042,
      "step": 14750
    },
    {
      "epoch": 0.7872,
      "grad_norm": 0.34975069761276245,
      "learning_rate": 4.508e-05,
      "loss": 0.0033,
      "step": 14760
    },
    {
      "epoch": 0.7877333333333333,
      "grad_norm": 0.39401787519454956,
      "learning_rate": 4.507666666666667e-05,
      "loss": 0.0036,
      "step": 14770
    },
    {
      "epoch": 0.7882666666666667,
      "grad_norm": 0.0676388069987297,
      "learning_rate": 4.5073333333333334e-05,
      "loss": 0.0047,
      "step": 14780
    },
    {
      "epoch": 0.7888,
      "grad_norm": 0.1942022144794464,
      "learning_rate": 4.507e-05,
      "loss": 0.0035,
      "step": 14790
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 0.5454129576683044,
      "learning_rate": 4.5066666666666667e-05,
      "loss": 0.0028,
      "step": 14800
    },
    {
      "epoch": 0.7898666666666667,
      "grad_norm": 0.7998914122581482,
      "learning_rate": 4.506333333333333e-05,
      "loss": 0.0029,
      "step": 14810
    },
    {
      "epoch": 0.7904,
      "grad_norm": 0.16723766922950745,
      "learning_rate": 4.506e-05,
      "loss": 0.0028,
      "step": 14820
    },
    {
      "epoch": 0.7909333333333334,
      "grad_norm": 0.44705629348754883,
      "learning_rate": 4.505666666666667e-05,
      "loss": 0.0038,
      "step": 14830
    },
    {
      "epoch": 0.7914666666666667,
      "grad_norm": 0.16819585859775543,
      "learning_rate": 4.505333333333334e-05,
      "loss": 0.0025,
      "step": 14840
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.6460814476013184,
      "learning_rate": 4.5050000000000004e-05,
      "loss": 0.0029,
      "step": 14850
    },
    {
      "epoch": 0.7925333333333333,
      "grad_norm": 0.2579900920391083,
      "learning_rate": 4.504666666666667e-05,
      "loss": 0.0043,
      "step": 14860
    },
    {
      "epoch": 0.7930666666666667,
      "grad_norm": 0.610511839389801,
      "learning_rate": 4.5043333333333336e-05,
      "loss": 0.0054,
      "step": 14870
    },
    {
      "epoch": 0.7936,
      "grad_norm": 0.39317840337753296,
      "learning_rate": 4.504e-05,
      "loss": 0.0034,
      "step": 14880
    },
    {
      "epoch": 0.7941333333333334,
      "grad_norm": 0.42364874482154846,
      "learning_rate": 4.503666666666667e-05,
      "loss": 0.0031,
      "step": 14890
    },
    {
      "epoch": 0.7946666666666666,
      "grad_norm": 0.5077797174453735,
      "learning_rate": 4.5033333333333335e-05,
      "loss": 0.0043,
      "step": 14900
    },
    {
      "epoch": 0.7952,
      "grad_norm": 0.3971550762653351,
      "learning_rate": 4.503e-05,
      "loss": 0.0028,
      "step": 14910
    },
    {
      "epoch": 0.7957333333333333,
      "grad_norm": 0.19966794550418854,
      "learning_rate": 4.502666666666667e-05,
      "loss": 0.0026,
      "step": 14920
    },
    {
      "epoch": 0.7962666666666667,
      "grad_norm": 0.2787328362464905,
      "learning_rate": 4.502333333333333e-05,
      "loss": 0.0028,
      "step": 14930
    },
    {
      "epoch": 0.7968,
      "grad_norm": 0.1709291934967041,
      "learning_rate": 4.502e-05,
      "loss": 0.0027,
      "step": 14940
    },
    {
      "epoch": 0.7973333333333333,
      "grad_norm": 0.13838495314121246,
      "learning_rate": 4.5016666666666665e-05,
      "loss": 0.0025,
      "step": 14950
    },
    {
      "epoch": 0.7978666666666666,
      "grad_norm": 0.37030965089797974,
      "learning_rate": 4.501333333333334e-05,
      "loss": 0.003,
      "step": 14960
    },
    {
      "epoch": 0.7984,
      "grad_norm": 0.12100696563720703,
      "learning_rate": 4.5010000000000004e-05,
      "loss": 0.0024,
      "step": 14970
    },
    {
      "epoch": 0.7989333333333334,
      "grad_norm": 0.41342899203300476,
      "learning_rate": 4.500666666666667e-05,
      "loss": 0.0028,
      "step": 14980
    },
    {
      "epoch": 0.7994666666666667,
      "grad_norm": 0.450633704662323,
      "learning_rate": 4.500333333333334e-05,
      "loss": 0.0023,
      "step": 14990
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.2613304853439331,
      "learning_rate": 4.5e-05,
      "loss": 0.0047,
      "step": 15000
    },
    {
      "epoch": 0.8005333333333333,
      "grad_norm": 0.48077380657196045,
      "learning_rate": 4.499666666666667e-05,
      "loss": 0.0041,
      "step": 15010
    },
    {
      "epoch": 0.8010666666666667,
      "grad_norm": 0.35574856400489807,
      "learning_rate": 4.4993333333333335e-05,
      "loss": 0.0024,
      "step": 15020
    },
    {
      "epoch": 0.8016,
      "grad_norm": 0.08239258080720901,
      "learning_rate": 4.499e-05,
      "loss": 0.0028,
      "step": 15030
    },
    {
      "epoch": 0.8021333333333334,
      "grad_norm": 0.08365490287542343,
      "learning_rate": 4.4986666666666674e-05,
      "loss": 0.004,
      "step": 15040
    },
    {
      "epoch": 0.8026666666666666,
      "grad_norm": 0.3541329801082611,
      "learning_rate": 4.4983333333333334e-05,
      "loss": 0.005,
      "step": 15050
    },
    {
      "epoch": 0.8032,
      "grad_norm": 0.26015856862068176,
      "learning_rate": 4.498e-05,
      "loss": 0.0039,
      "step": 15060
    },
    {
      "epoch": 0.8037333333333333,
      "grad_norm": 0.31953370571136475,
      "learning_rate": 4.4976666666666666e-05,
      "loss": 0.0045,
      "step": 15070
    },
    {
      "epoch": 0.8042666666666667,
      "grad_norm": 0.2611840069293976,
      "learning_rate": 4.497333333333333e-05,
      "loss": 0.0034,
      "step": 15080
    },
    {
      "epoch": 0.8048,
      "grad_norm": 0.6048195362091064,
      "learning_rate": 4.497e-05,
      "loss": 0.003,
      "step": 15090
    },
    {
      "epoch": 0.8053333333333333,
      "grad_norm": 0.2859005033969879,
      "learning_rate": 4.496666666666667e-05,
      "loss": 0.0034,
      "step": 15100
    },
    {
      "epoch": 0.8058666666666666,
      "grad_norm": 0.38391414284706116,
      "learning_rate": 4.496333333333334e-05,
      "loss": 0.0042,
      "step": 15110
    },
    {
      "epoch": 0.8064,
      "grad_norm": 0.5686517953872681,
      "learning_rate": 4.496e-05,
      "loss": 0.0028,
      "step": 15120
    },
    {
      "epoch": 0.8069333333333333,
      "grad_norm": 0.2843954265117645,
      "learning_rate": 4.495666666666667e-05,
      "loss": 0.0028,
      "step": 15130
    },
    {
      "epoch": 0.8074666666666667,
      "grad_norm": 0.10280679911375046,
      "learning_rate": 4.4953333333333335e-05,
      "loss": 0.0045,
      "step": 15140
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.4790526032447815,
      "learning_rate": 4.495e-05,
      "loss": 0.0029,
      "step": 15150
    },
    {
      "epoch": 0.8085333333333333,
      "grad_norm": 0.5047419667243958,
      "learning_rate": 4.494666666666667e-05,
      "loss": 0.0036,
      "step": 15160
    },
    {
      "epoch": 0.8090666666666667,
      "grad_norm": 0.6009479761123657,
      "learning_rate": 4.494333333333334e-05,
      "loss": 0.0025,
      "step": 15170
    },
    {
      "epoch": 0.8096,
      "grad_norm": 0.14103129506111145,
      "learning_rate": 4.494000000000001e-05,
      "loss": 0.0026,
      "step": 15180
    },
    {
      "epoch": 0.8101333333333334,
      "grad_norm": 0.10421386361122131,
      "learning_rate": 4.493666666666667e-05,
      "loss": 0.0032,
      "step": 15190
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 0.162700355052948,
      "learning_rate": 4.493333333333333e-05,
      "loss": 0.0029,
      "step": 15200
    },
    {
      "epoch": 0.8112,
      "grad_norm": 0.16480319201946259,
      "learning_rate": 4.493e-05,
      "loss": 0.0028,
      "step": 15210
    },
    {
      "epoch": 0.8117333333333333,
      "grad_norm": 0.16073977947235107,
      "learning_rate": 4.4926666666666665e-05,
      "loss": 0.0035,
      "step": 15220
    },
    {
      "epoch": 0.8122666666666667,
      "grad_norm": 0.4835057854652405,
      "learning_rate": 4.492333333333333e-05,
      "loss": 0.0027,
      "step": 15230
    },
    {
      "epoch": 0.8128,
      "grad_norm": 0.4241768717765808,
      "learning_rate": 4.4920000000000004e-05,
      "loss": 0.0032,
      "step": 15240
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.11792636662721634,
      "learning_rate": 4.491666666666667e-05,
      "loss": 0.0036,
      "step": 15250
    },
    {
      "epoch": 0.8138666666666666,
      "grad_norm": 0.36703941226005554,
      "learning_rate": 4.4913333333333336e-05,
      "loss": 0.0041,
      "step": 15260
    },
    {
      "epoch": 0.8144,
      "grad_norm": 0.3255809247493744,
      "learning_rate": 4.491e-05,
      "loss": 0.0027,
      "step": 15270
    },
    {
      "epoch": 0.8149333333333333,
      "grad_norm": 0.21319325268268585,
      "learning_rate": 4.490666666666667e-05,
      "loss": 0.0031,
      "step": 15280
    },
    {
      "epoch": 0.8154666666666667,
      "grad_norm": 0.7658060193061829,
      "learning_rate": 4.4903333333333334e-05,
      "loss": 0.003,
      "step": 15290
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.3218497633934021,
      "learning_rate": 4.49e-05,
      "loss": 0.0031,
      "step": 15300
    },
    {
      "epoch": 0.8165333333333333,
      "grad_norm": 0.35450419783592224,
      "learning_rate": 4.489666666666667e-05,
      "loss": 0.0026,
      "step": 15310
    },
    {
      "epoch": 0.8170666666666667,
      "grad_norm": 0.38522133231163025,
      "learning_rate": 4.489333333333334e-05,
      "loss": 0.0024,
      "step": 15320
    },
    {
      "epoch": 0.8176,
      "grad_norm": 0.17272204160690308,
      "learning_rate": 4.4890000000000006e-05,
      "loss": 0.0023,
      "step": 15330
    },
    {
      "epoch": 0.8181333333333334,
      "grad_norm": 0.5719824433326721,
      "learning_rate": 4.488666666666667e-05,
      "loss": 0.0033,
      "step": 15340
    },
    {
      "epoch": 0.8186666666666667,
      "grad_norm": 0.2569023072719574,
      "learning_rate": 4.488333333333333e-05,
      "loss": 0.0034,
      "step": 15350
    },
    {
      "epoch": 0.8192,
      "grad_norm": 0.27688974142074585,
      "learning_rate": 4.488e-05,
      "loss": 0.0033,
      "step": 15360
    },
    {
      "epoch": 0.8197333333333333,
      "grad_norm": 0.3561677634716034,
      "learning_rate": 4.487666666666667e-05,
      "loss": 0.0035,
      "step": 15370
    },
    {
      "epoch": 0.8202666666666667,
      "grad_norm": 0.19458986818790436,
      "learning_rate": 4.4873333333333336e-05,
      "loss": 0.0031,
      "step": 15380
    },
    {
      "epoch": 0.8208,
      "grad_norm": 0.2897290289402008,
      "learning_rate": 4.487e-05,
      "loss": 0.0021,
      "step": 15390
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 0.1983305662870407,
      "learning_rate": 4.486666666666667e-05,
      "loss": 0.0039,
      "step": 15400
    },
    {
      "epoch": 0.8218666666666666,
      "grad_norm": 0.7585870027542114,
      "learning_rate": 4.4863333333333335e-05,
      "loss": 0.0039,
      "step": 15410
    },
    {
      "epoch": 0.8224,
      "grad_norm": 0.4159395396709442,
      "learning_rate": 4.486e-05,
      "loss": 0.0024,
      "step": 15420
    },
    {
      "epoch": 0.8229333333333333,
      "grad_norm": 0.12068048864603043,
      "learning_rate": 4.485666666666667e-05,
      "loss": 0.0026,
      "step": 15430
    },
    {
      "epoch": 0.8234666666666667,
      "grad_norm": 0.07489205151796341,
      "learning_rate": 4.485333333333333e-05,
      "loss": 0.0034,
      "step": 15440
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.4524267315864563,
      "learning_rate": 4.4850000000000006e-05,
      "loss": 0.0029,
      "step": 15450
    },
    {
      "epoch": 0.8245333333333333,
      "grad_norm": 0.22537237405776978,
      "learning_rate": 4.484666666666667e-05,
      "loss": 0.0023,
      "step": 15460
    },
    {
      "epoch": 0.8250666666666666,
      "grad_norm": 0.4141671657562256,
      "learning_rate": 4.484333333333334e-05,
      "loss": 0.0032,
      "step": 15470
    },
    {
      "epoch": 0.8256,
      "grad_norm": 0.14384885132312775,
      "learning_rate": 4.4840000000000004e-05,
      "loss": 0.0035,
      "step": 15480
    },
    {
      "epoch": 0.8261333333333334,
      "grad_norm": 0.5556824207305908,
      "learning_rate": 4.483666666666667e-05,
      "loss": 0.0047,
      "step": 15490
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.6056550741195679,
      "learning_rate": 4.483333333333333e-05,
      "loss": 0.0028,
      "step": 15500
    },
    {
      "epoch": 0.8272,
      "grad_norm": 0.24844251573085785,
      "learning_rate": 4.483e-05,
      "loss": 0.0025,
      "step": 15510
    },
    {
      "epoch": 0.8277333333333333,
      "grad_norm": 0.2623482346534729,
      "learning_rate": 4.482666666666667e-05,
      "loss": 0.0031,
      "step": 15520
    },
    {
      "epoch": 0.8282666666666667,
      "grad_norm": 0.4148721694946289,
      "learning_rate": 4.4823333333333335e-05,
      "loss": 0.0031,
      "step": 15530
    },
    {
      "epoch": 0.8288,
      "grad_norm": 0.44768232107162476,
      "learning_rate": 4.482e-05,
      "loss": 0.0031,
      "step": 15540
    },
    {
      "epoch": 0.8293333333333334,
      "grad_norm": 0.7610578536987305,
      "learning_rate": 4.481666666666667e-05,
      "loss": 0.003,
      "step": 15550
    },
    {
      "epoch": 0.8298666666666666,
      "grad_norm": 0.37678590416908264,
      "learning_rate": 4.4813333333333333e-05,
      "loss": 0.0039,
      "step": 15560
    },
    {
      "epoch": 0.8304,
      "grad_norm": 0.38059213757514954,
      "learning_rate": 4.481e-05,
      "loss": 0.0027,
      "step": 15570
    },
    {
      "epoch": 0.8309333333333333,
      "grad_norm": 0.16181141138076782,
      "learning_rate": 4.4806666666666666e-05,
      "loss": 0.0032,
      "step": 15580
    },
    {
      "epoch": 0.8314666666666667,
      "grad_norm": 0.511467695236206,
      "learning_rate": 4.480333333333334e-05,
      "loss": 0.0026,
      "step": 15590
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.2586604952812195,
      "learning_rate": 4.4800000000000005e-05,
      "loss": 0.0022,
      "step": 15600
    },
    {
      "epoch": 0.8325333333333333,
      "grad_norm": 0.31145018339157104,
      "learning_rate": 4.479666666666667e-05,
      "loss": 0.0031,
      "step": 15610
    },
    {
      "epoch": 0.8330666666666666,
      "grad_norm": 0.3161470293998718,
      "learning_rate": 4.479333333333334e-05,
      "loss": 0.003,
      "step": 15620
    },
    {
      "epoch": 0.8336,
      "grad_norm": 0.08776038140058517,
      "learning_rate": 4.479e-05,
      "loss": 0.0027,
      "step": 15630
    },
    {
      "epoch": 0.8341333333333333,
      "grad_norm": 0.44414228200912476,
      "learning_rate": 4.478666666666667e-05,
      "loss": 0.0034,
      "step": 15640
    },
    {
      "epoch": 0.8346666666666667,
      "grad_norm": 0.38871413469314575,
      "learning_rate": 4.4783333333333335e-05,
      "loss": 0.0028,
      "step": 15650
    },
    {
      "epoch": 0.8352,
      "grad_norm": 0.3817400336265564,
      "learning_rate": 4.478e-05,
      "loss": 0.0016,
      "step": 15660
    },
    {
      "epoch": 0.8357333333333333,
      "grad_norm": 0.03897005692124367,
      "learning_rate": 4.477666666666667e-05,
      "loss": 0.0035,
      "step": 15670
    },
    {
      "epoch": 0.8362666666666667,
      "grad_norm": 0.26886364817619324,
      "learning_rate": 4.4773333333333334e-05,
      "loss": 0.0023,
      "step": 15680
    },
    {
      "epoch": 0.8368,
      "grad_norm": 0.32188695669174194,
      "learning_rate": 4.477e-05,
      "loss": 0.0016,
      "step": 15690
    },
    {
      "epoch": 0.8373333333333334,
      "grad_norm": 0.1063229888677597,
      "learning_rate": 4.4766666666666666e-05,
      "loss": 0.0022,
      "step": 15700
    },
    {
      "epoch": 0.8378666666666666,
      "grad_norm": 0.5097092390060425,
      "learning_rate": 4.476333333333333e-05,
      "loss": 0.004,
      "step": 15710
    },
    {
      "epoch": 0.8384,
      "grad_norm": 0.07818345725536346,
      "learning_rate": 4.4760000000000005e-05,
      "loss": 0.0027,
      "step": 15720
    },
    {
      "epoch": 0.8389333333333333,
      "grad_norm": 0.08125085383653641,
      "learning_rate": 4.475666666666667e-05,
      "loss": 0.0041,
      "step": 15730
    },
    {
      "epoch": 0.8394666666666667,
      "grad_norm": 0.4127199351787567,
      "learning_rate": 4.475333333333334e-05,
      "loss": 0.0026,
      "step": 15740
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.2939341366291046,
      "learning_rate": 4.4750000000000004e-05,
      "loss": 0.0028,
      "step": 15750
    },
    {
      "epoch": 0.8405333333333334,
      "grad_norm": 0.44805803894996643,
      "learning_rate": 4.474666666666667e-05,
      "loss": 0.0037,
      "step": 15760
    },
    {
      "epoch": 0.8410666666666666,
      "grad_norm": 0.6393758654594421,
      "learning_rate": 4.4743333333333336e-05,
      "loss": 0.0043,
      "step": 15770
    },
    {
      "epoch": 0.8416,
      "grad_norm": 0.8880063891410828,
      "learning_rate": 4.474e-05,
      "loss": 0.004,
      "step": 15780
    },
    {
      "epoch": 0.8421333333333333,
      "grad_norm": 0.28555068373680115,
      "learning_rate": 4.473666666666667e-05,
      "loss": 0.0037,
      "step": 15790
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 0.7416875958442688,
      "learning_rate": 4.473333333333334e-05,
      "loss": 0.0029,
      "step": 15800
    },
    {
      "epoch": 0.8432,
      "grad_norm": 0.2303636074066162,
      "learning_rate": 4.473e-05,
      "loss": 0.0036,
      "step": 15810
    },
    {
      "epoch": 0.8437333333333333,
      "grad_norm": 0.7409887909889221,
      "learning_rate": 4.4726666666666666e-05,
      "loss": 0.0026,
      "step": 15820
    },
    {
      "epoch": 0.8442666666666667,
      "grad_norm": 0.6026476621627808,
      "learning_rate": 4.472333333333333e-05,
      "loss": 0.0035,
      "step": 15830
    },
    {
      "epoch": 0.8448,
      "grad_norm": 0.7988760471343994,
      "learning_rate": 4.472e-05,
      "loss": 0.0026,
      "step": 15840
    },
    {
      "epoch": 0.8453333333333334,
      "grad_norm": 0.35920774936676025,
      "learning_rate": 4.4716666666666665e-05,
      "loss": 0.003,
      "step": 15850
    },
    {
      "epoch": 0.8458666666666667,
      "grad_norm": 0.1401880383491516,
      "learning_rate": 4.471333333333334e-05,
      "loss": 0.0022,
      "step": 15860
    },
    {
      "epoch": 0.8464,
      "grad_norm": 0.607221782207489,
      "learning_rate": 4.4710000000000004e-05,
      "loss": 0.0029,
      "step": 15870
    },
    {
      "epoch": 0.8469333333333333,
      "grad_norm": 0.41745996475219727,
      "learning_rate": 4.470666666666667e-05,
      "loss": 0.0049,
      "step": 15880
    },
    {
      "epoch": 0.8474666666666667,
      "grad_norm": 0.3915542960166931,
      "learning_rate": 4.4703333333333336e-05,
      "loss": 0.0035,
      "step": 15890
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.20419010519981384,
      "learning_rate": 4.47e-05,
      "loss": 0.0027,
      "step": 15900
    },
    {
      "epoch": 0.8485333333333334,
      "grad_norm": 0.3306940793991089,
      "learning_rate": 4.469666666666667e-05,
      "loss": 0.0033,
      "step": 15910
    },
    {
      "epoch": 0.8490666666666666,
      "grad_norm": 0.1086130291223526,
      "learning_rate": 4.4693333333333335e-05,
      "loss": 0.0032,
      "step": 15920
    },
    {
      "epoch": 0.8496,
      "grad_norm": 0.19551731646060944,
      "learning_rate": 4.469e-05,
      "loss": 0.003,
      "step": 15930
    },
    {
      "epoch": 0.8501333333333333,
      "grad_norm": 0.08360309153795242,
      "learning_rate": 4.4686666666666674e-05,
      "loss": 0.0029,
      "step": 15940
    },
    {
      "epoch": 0.8506666666666667,
      "grad_norm": 0.38461774587631226,
      "learning_rate": 4.468333333333334e-05,
      "loss": 0.0037,
      "step": 15950
    },
    {
      "epoch": 0.8512,
      "grad_norm": 0.6976481080055237,
      "learning_rate": 4.468e-05,
      "loss": 0.0035,
      "step": 15960
    },
    {
      "epoch": 0.8517333333333333,
      "grad_norm": 0.4503575563430786,
      "learning_rate": 4.4676666666666665e-05,
      "loss": 0.0026,
      "step": 15970
    },
    {
      "epoch": 0.8522666666666666,
      "grad_norm": 0.1841738373041153,
      "learning_rate": 4.467333333333333e-05,
      "loss": 0.0021,
      "step": 15980
    },
    {
      "epoch": 0.8528,
      "grad_norm": 0.5069814324378967,
      "learning_rate": 4.467e-05,
      "loss": 0.0036,
      "step": 15990
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.513822615146637,
      "learning_rate": 4.466666666666667e-05,
      "loss": 0.0033,
      "step": 16000
    },
    {
      "epoch": 0.8538666666666667,
      "grad_norm": 0.6213340759277344,
      "learning_rate": 4.4663333333333337e-05,
      "loss": 0.0036,
      "step": 16010
    },
    {
      "epoch": 0.8544,
      "grad_norm": 0.479154109954834,
      "learning_rate": 4.466e-05,
      "loss": 0.0041,
      "step": 16020
    },
    {
      "epoch": 0.8549333333333333,
      "grad_norm": 0.20708218216896057,
      "learning_rate": 4.465666666666667e-05,
      "loss": 0.0032,
      "step": 16030
    },
    {
      "epoch": 0.8554666666666667,
      "grad_norm": 0.14270201325416565,
      "learning_rate": 4.4653333333333335e-05,
      "loss": 0.0023,
      "step": 16040
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.09526605159044266,
      "learning_rate": 4.465e-05,
      "loss": 0.0036,
      "step": 16050
    },
    {
      "epoch": 0.8565333333333334,
      "grad_norm": 0.8254654407501221,
      "learning_rate": 4.464666666666667e-05,
      "loss": 0.0028,
      "step": 16060
    },
    {
      "epoch": 0.8570666666666666,
      "grad_norm": 0.11914201080799103,
      "learning_rate": 4.464333333333334e-05,
      "loss": 0.0029,
      "step": 16070
    },
    {
      "epoch": 0.8576,
      "grad_norm": 0.2280694991350174,
      "learning_rate": 4.4640000000000006e-05,
      "loss": 0.002,
      "step": 16080
    },
    {
      "epoch": 0.8581333333333333,
      "grad_norm": 0.5405904054641724,
      "learning_rate": 4.463666666666667e-05,
      "loss": 0.0029,
      "step": 16090
    },
    {
      "epoch": 0.8586666666666667,
      "grad_norm": 0.5782410502433777,
      "learning_rate": 4.463333333333334e-05,
      "loss": 0.0029,
      "step": 16100
    },
    {
      "epoch": 0.8592,
      "grad_norm": 0.10589169710874557,
      "learning_rate": 4.463e-05,
      "loss": 0.0025,
      "step": 16110
    },
    {
      "epoch": 0.8597333333333333,
      "grad_norm": 0.5099809169769287,
      "learning_rate": 4.4626666666666664e-05,
      "loss": 0.003,
      "step": 16120
    },
    {
      "epoch": 0.8602666666666666,
      "grad_norm": 0.26031407713890076,
      "learning_rate": 4.462333333333334e-05,
      "loss": 0.0041,
      "step": 16130
    },
    {
      "epoch": 0.8608,
      "grad_norm": 0.5458706617355347,
      "learning_rate": 4.462e-05,
      "loss": 0.0029,
      "step": 16140
    },
    {
      "epoch": 0.8613333333333333,
      "grad_norm": 0.5334526300430298,
      "learning_rate": 4.461666666666667e-05,
      "loss": 0.0038,
      "step": 16150
    },
    {
      "epoch": 0.8618666666666667,
      "grad_norm": 0.3810518682003021,
      "learning_rate": 4.4613333333333335e-05,
      "loss": 0.0033,
      "step": 16160
    },
    {
      "epoch": 0.8624,
      "grad_norm": 0.28267133235931396,
      "learning_rate": 4.461e-05,
      "loss": 0.0045,
      "step": 16170
    },
    {
      "epoch": 0.8629333333333333,
      "grad_norm": 0.41444188356399536,
      "learning_rate": 4.460666666666667e-05,
      "loss": 0.0038,
      "step": 16180
    },
    {
      "epoch": 0.8634666666666667,
      "grad_norm": 0.30288851261138916,
      "learning_rate": 4.4603333333333334e-05,
      "loss": 0.0035,
      "step": 16190
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.40934714674949646,
      "learning_rate": 4.46e-05,
      "loss": 0.0037,
      "step": 16200
    },
    {
      "epoch": 0.8645333333333334,
      "grad_norm": 0.19569595158100128,
      "learning_rate": 4.459666666666667e-05,
      "loss": 0.0031,
      "step": 16210
    },
    {
      "epoch": 0.8650666666666667,
      "grad_norm": 0.15568065643310547,
      "learning_rate": 4.459333333333334e-05,
      "loss": 0.0035,
      "step": 16220
    },
    {
      "epoch": 0.8656,
      "grad_norm": 0.6960360407829285,
      "learning_rate": 4.4590000000000005e-05,
      "loss": 0.0026,
      "step": 16230
    },
    {
      "epoch": 0.8661333333333333,
      "grad_norm": 0.20983152091503143,
      "learning_rate": 4.458666666666667e-05,
      "loss": 0.0034,
      "step": 16240
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.320614755153656,
      "learning_rate": 4.458333333333334e-05,
      "loss": 0.0021,
      "step": 16250
    },
    {
      "epoch": 0.8672,
      "grad_norm": 0.9135498404502869,
      "learning_rate": 4.458e-05,
      "loss": 0.0039,
      "step": 16260
    },
    {
      "epoch": 0.8677333333333334,
      "grad_norm": 0.6276389360427856,
      "learning_rate": 4.457666666666667e-05,
      "loss": 0.0037,
      "step": 16270
    },
    {
      "epoch": 0.8682666666666666,
      "grad_norm": 0.10288461297750473,
      "learning_rate": 4.4573333333333336e-05,
      "loss": 0.0035,
      "step": 16280
    },
    {
      "epoch": 0.8688,
      "grad_norm": 0.3582438826560974,
      "learning_rate": 4.457e-05,
      "loss": 0.0027,
      "step": 16290
    },
    {
      "epoch": 0.8693333333333333,
      "grad_norm": 0.5728444457054138,
      "learning_rate": 4.456666666666667e-05,
      "loss": 0.0033,
      "step": 16300
    },
    {
      "epoch": 0.8698666666666667,
      "grad_norm": 0.3198404014110565,
      "learning_rate": 4.4563333333333334e-05,
      "loss": 0.0039,
      "step": 16310
    },
    {
      "epoch": 0.8704,
      "grad_norm": 0.3000037968158722,
      "learning_rate": 4.456e-05,
      "loss": 0.0033,
      "step": 16320
    },
    {
      "epoch": 0.8709333333333333,
      "grad_norm": 0.44019681215286255,
      "learning_rate": 4.4556666666666666e-05,
      "loss": 0.0046,
      "step": 16330
    },
    {
      "epoch": 0.8714666666666666,
      "grad_norm": 0.2877958416938782,
      "learning_rate": 4.455333333333333e-05,
      "loss": 0.003,
      "step": 16340
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.08955499529838562,
      "learning_rate": 4.4550000000000005e-05,
      "loss": 0.0043,
      "step": 16350
    },
    {
      "epoch": 0.8725333333333334,
      "grad_norm": 0.29358211159706116,
      "learning_rate": 4.454666666666667e-05,
      "loss": 0.0033,
      "step": 16360
    },
    {
      "epoch": 0.8730666666666667,
      "grad_norm": 0.3497123420238495,
      "learning_rate": 4.454333333333334e-05,
      "loss": 0.0027,
      "step": 16370
    },
    {
      "epoch": 0.8736,
      "grad_norm": 0.7902060151100159,
      "learning_rate": 4.4540000000000004e-05,
      "loss": 0.002,
      "step": 16380
    },
    {
      "epoch": 0.8741333333333333,
      "grad_norm": 0.2870265543460846,
      "learning_rate": 4.453666666666667e-05,
      "loss": 0.0032,
      "step": 16390
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 0.44446468353271484,
      "learning_rate": 4.4533333333333336e-05,
      "loss": 0.002,
      "step": 16400
    },
    {
      "epoch": 0.8752,
      "grad_norm": 0.07914591580629349,
      "learning_rate": 4.453e-05,
      "loss": 0.0034,
      "step": 16410
    },
    {
      "epoch": 0.8757333333333334,
      "grad_norm": 0.2707536816596985,
      "learning_rate": 4.452666666666667e-05,
      "loss": 0.0036,
      "step": 16420
    },
    {
      "epoch": 0.8762666666666666,
      "grad_norm": 0.3047531843185425,
      "learning_rate": 4.4523333333333335e-05,
      "loss": 0.0032,
      "step": 16430
    },
    {
      "epoch": 0.8768,
      "grad_norm": 0.046289946883916855,
      "learning_rate": 4.452e-05,
      "loss": 0.0034,
      "step": 16440
    },
    {
      "epoch": 0.8773333333333333,
      "grad_norm": 0.28842103481292725,
      "learning_rate": 4.451666666666667e-05,
      "loss": 0.0036,
      "step": 16450
    },
    {
      "epoch": 0.8778666666666667,
      "grad_norm": 0.26750466227531433,
      "learning_rate": 4.451333333333333e-05,
      "loss": 0.0033,
      "step": 16460
    },
    {
      "epoch": 0.8784,
      "grad_norm": 0.19213736057281494,
      "learning_rate": 4.451e-05,
      "loss": 0.0033,
      "step": 16470
    },
    {
      "epoch": 0.8789333333333333,
      "grad_norm": 0.5383824110031128,
      "learning_rate": 4.450666666666667e-05,
      "loss": 0.0033,
      "step": 16480
    },
    {
      "epoch": 0.8794666666666666,
      "grad_norm": 0.20543958246707916,
      "learning_rate": 4.450333333333334e-05,
      "loss": 0.0028,
      "step": 16490
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.6991506218910217,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 0.003,
      "step": 16500
    },
    {
      "epoch": 0.8805333333333333,
      "grad_norm": 0.5573088526725769,
      "learning_rate": 4.449666666666667e-05,
      "loss": 0.0043,
      "step": 16510
    },
    {
      "epoch": 0.8810666666666667,
      "grad_norm": 0.2417207658290863,
      "learning_rate": 4.4493333333333337e-05,
      "loss": 0.0039,
      "step": 16520
    },
    {
      "epoch": 0.8816,
      "grad_norm": 0.5491926074028015,
      "learning_rate": 4.449e-05,
      "loss": 0.0019,
      "step": 16530
    },
    {
      "epoch": 0.8821333333333333,
      "grad_norm": 0.3806832730770111,
      "learning_rate": 4.448666666666667e-05,
      "loss": 0.0031,
      "step": 16540
    },
    {
      "epoch": 0.8826666666666667,
      "grad_norm": 0.38457992672920227,
      "learning_rate": 4.4483333333333335e-05,
      "loss": 0.0043,
      "step": 16550
    },
    {
      "epoch": 0.8832,
      "grad_norm": 0.26673266291618347,
      "learning_rate": 4.448e-05,
      "loss": 0.0032,
      "step": 16560
    },
    {
      "epoch": 0.8837333333333334,
      "grad_norm": 0.20301559567451477,
      "learning_rate": 4.447666666666667e-05,
      "loss": 0.0029,
      "step": 16570
    },
    {
      "epoch": 0.8842666666666666,
      "grad_norm": 0.3222115933895111,
      "learning_rate": 4.447333333333333e-05,
      "loss": 0.0021,
      "step": 16580
    },
    {
      "epoch": 0.8848,
      "grad_norm": 0.5694215893745422,
      "learning_rate": 4.447e-05,
      "loss": 0.0031,
      "step": 16590
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 0.05948510766029358,
      "learning_rate": 4.4466666666666666e-05,
      "loss": 0.003,
      "step": 16600
    },
    {
      "epoch": 0.8858666666666667,
      "grad_norm": 0.07645804435014725,
      "learning_rate": 4.446333333333333e-05,
      "loss": 0.0032,
      "step": 16610
    },
    {
      "epoch": 0.8864,
      "grad_norm": 0.1335064321756363,
      "learning_rate": 4.4460000000000005e-05,
      "loss": 0.0027,
      "step": 16620
    },
    {
      "epoch": 0.8869333333333334,
      "grad_norm": 0.5987204313278198,
      "learning_rate": 4.445666666666667e-05,
      "loss": 0.0026,
      "step": 16630
    },
    {
      "epoch": 0.8874666666666666,
      "grad_norm": 0.7560911774635315,
      "learning_rate": 4.445333333333334e-05,
      "loss": 0.0024,
      "step": 16640
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.7313717007637024,
      "learning_rate": 4.445e-05,
      "loss": 0.0034,
      "step": 16650
    },
    {
      "epoch": 0.8885333333333333,
      "grad_norm": 0.42435160279273987,
      "learning_rate": 4.444666666666667e-05,
      "loss": 0.0021,
      "step": 16660
    },
    {
      "epoch": 0.8890666666666667,
      "grad_norm": 0.5041958689689636,
      "learning_rate": 4.4443333333333335e-05,
      "loss": 0.0026,
      "step": 16670
    },
    {
      "epoch": 0.8896,
      "grad_norm": 0.10630173236131668,
      "learning_rate": 4.444e-05,
      "loss": 0.0032,
      "step": 16680
    },
    {
      "epoch": 0.8901333333333333,
      "grad_norm": 0.37940195202827454,
      "learning_rate": 4.443666666666667e-05,
      "loss": 0.0025,
      "step": 16690
    },
    {
      "epoch": 0.8906666666666667,
      "grad_norm": 0.47708067297935486,
      "learning_rate": 4.443333333333334e-05,
      "loss": 0.0034,
      "step": 16700
    },
    {
      "epoch": 0.8912,
      "grad_norm": 0.06525316834449768,
      "learning_rate": 4.443e-05,
      "loss": 0.0034,
      "step": 16710
    },
    {
      "epoch": 0.8917333333333334,
      "grad_norm": 0.13602499663829803,
      "learning_rate": 4.4426666666666666e-05,
      "loss": 0.003,
      "step": 16720
    },
    {
      "epoch": 0.8922666666666667,
      "grad_norm": 0.12731775641441345,
      "learning_rate": 4.442333333333333e-05,
      "loss": 0.0026,
      "step": 16730
    },
    {
      "epoch": 0.8928,
      "grad_norm": 0.14750750362873077,
      "learning_rate": 4.442e-05,
      "loss": 0.0046,
      "step": 16740
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.5354685187339783,
      "learning_rate": 4.4416666666666664e-05,
      "loss": 0.0023,
      "step": 16750
    },
    {
      "epoch": 0.8938666666666667,
      "grad_norm": 0.25434496998786926,
      "learning_rate": 4.441333333333334e-05,
      "loss": 0.0045,
      "step": 16760
    },
    {
      "epoch": 0.8944,
      "grad_norm": 0.5050122737884521,
      "learning_rate": 4.4410000000000003e-05,
      "loss": 0.002,
      "step": 16770
    },
    {
      "epoch": 0.8949333333333334,
      "grad_norm": 0.5683602690696716,
      "learning_rate": 4.440666666666667e-05,
      "loss": 0.0026,
      "step": 16780
    },
    {
      "epoch": 0.8954666666666666,
      "grad_norm": 0.44040051102638245,
      "learning_rate": 4.4403333333333336e-05,
      "loss": 0.003,
      "step": 16790
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.34975138306617737,
      "learning_rate": 4.44e-05,
      "loss": 0.0028,
      "step": 16800
    },
    {
      "epoch": 0.8965333333333333,
      "grad_norm": 0.7216132879257202,
      "learning_rate": 4.439666666666667e-05,
      "loss": 0.0028,
      "step": 16810
    },
    {
      "epoch": 0.8970666666666667,
      "grad_norm": 0.3099307417869568,
      "learning_rate": 4.4393333333333334e-05,
      "loss": 0.0028,
      "step": 16820
    },
    {
      "epoch": 0.8976,
      "grad_norm": 0.26172247529029846,
      "learning_rate": 4.439000000000001e-05,
      "loss": 0.003,
      "step": 16830
    },
    {
      "epoch": 0.8981333333333333,
      "grad_norm": 0.08196782320737839,
      "learning_rate": 4.438666666666667e-05,
      "loss": 0.0027,
      "step": 16840
    },
    {
      "epoch": 0.8986666666666666,
      "grad_norm": 0.788199245929718,
      "learning_rate": 4.438333333333334e-05,
      "loss": 0.0037,
      "step": 16850
    },
    {
      "epoch": 0.8992,
      "grad_norm": 0.16147153079509735,
      "learning_rate": 4.438e-05,
      "loss": 0.0036,
      "step": 16860
    },
    {
      "epoch": 0.8997333333333334,
      "grad_norm": 0.5966801047325134,
      "learning_rate": 4.4376666666666665e-05,
      "loss": 0.003,
      "step": 16870
    },
    {
      "epoch": 0.9002666666666667,
      "grad_norm": 0.06887597590684891,
      "learning_rate": 4.437333333333333e-05,
      "loss": 0.0025,
      "step": 16880
    },
    {
      "epoch": 0.9008,
      "grad_norm": 0.38213542103767395,
      "learning_rate": 4.4370000000000004e-05,
      "loss": 0.0029,
      "step": 16890
    },
    {
      "epoch": 0.9013333333333333,
      "grad_norm": 0.17018163204193115,
      "learning_rate": 4.436666666666667e-05,
      "loss": 0.0031,
      "step": 16900
    },
    {
      "epoch": 0.9018666666666667,
      "grad_norm": 0.14105287194252014,
      "learning_rate": 4.4363333333333336e-05,
      "loss": 0.0035,
      "step": 16910
    },
    {
      "epoch": 0.9024,
      "grad_norm": 0.34702417254447937,
      "learning_rate": 4.436e-05,
      "loss": 0.0033,
      "step": 16920
    },
    {
      "epoch": 0.9029333333333334,
      "grad_norm": 0.3430821895599365,
      "learning_rate": 4.435666666666667e-05,
      "loss": 0.0046,
      "step": 16930
    },
    {
      "epoch": 0.9034666666666666,
      "grad_norm": 0.34944793581962585,
      "learning_rate": 4.4353333333333334e-05,
      "loss": 0.0031,
      "step": 16940
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.19560779631137848,
      "learning_rate": 4.435e-05,
      "loss": 0.0031,
      "step": 16950
    },
    {
      "epoch": 0.9045333333333333,
      "grad_norm": 0.15977256000041962,
      "learning_rate": 4.434666666666667e-05,
      "loss": 0.003,
      "step": 16960
    },
    {
      "epoch": 0.9050666666666667,
      "grad_norm": 0.28472036123275757,
      "learning_rate": 4.434333333333334e-05,
      "loss": 0.0038,
      "step": 16970
    },
    {
      "epoch": 0.9056,
      "grad_norm": 0.2531229853630066,
      "learning_rate": 4.4340000000000006e-05,
      "loss": 0.003,
      "step": 16980
    },
    {
      "epoch": 0.9061333333333333,
      "grad_norm": 0.350763201713562,
      "learning_rate": 4.433666666666667e-05,
      "loss": 0.0024,
      "step": 16990
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.1143902987241745,
      "learning_rate": 4.433333333333334e-05,
      "loss": 0.0031,
      "step": 17000
    },
    {
      "epoch": 0.9072,
      "grad_norm": 0.19266903400421143,
      "learning_rate": 4.4330000000000004e-05,
      "loss": 0.0027,
      "step": 17010
    },
    {
      "epoch": 0.9077333333333333,
      "grad_norm": 0.18818993866443634,
      "learning_rate": 4.4326666666666664e-05,
      "loss": 0.0028,
      "step": 17020
    },
    {
      "epoch": 0.9082666666666667,
      "grad_norm": 0.29249662160873413,
      "learning_rate": 4.4323333333333336e-05,
      "loss": 0.0037,
      "step": 17030
    },
    {
      "epoch": 0.9088,
      "grad_norm": 0.17438282072544098,
      "learning_rate": 4.432e-05,
      "loss": 0.0038,
      "step": 17040
    },
    {
      "epoch": 0.9093333333333333,
      "grad_norm": 0.06762518733739853,
      "learning_rate": 4.431666666666667e-05,
      "loss": 0.0021,
      "step": 17050
    },
    {
      "epoch": 0.9098666666666667,
      "grad_norm": 0.11146287620067596,
      "learning_rate": 4.4313333333333335e-05,
      "loss": 0.0025,
      "step": 17060
    },
    {
      "epoch": 0.9104,
      "grad_norm": 0.4128721058368683,
      "learning_rate": 4.431e-05,
      "loss": 0.0027,
      "step": 17070
    },
    {
      "epoch": 0.9109333333333334,
      "grad_norm": 0.3483290672302246,
      "learning_rate": 4.430666666666667e-05,
      "loss": 0.0025,
      "step": 17080
    },
    {
      "epoch": 0.9114666666666666,
      "grad_norm": 0.22531850636005402,
      "learning_rate": 4.430333333333333e-05,
      "loss": 0.0041,
      "step": 17090
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.7913601994514465,
      "learning_rate": 4.43e-05,
      "loss": 0.0049,
      "step": 17100
    },
    {
      "epoch": 0.9125333333333333,
      "grad_norm": 0.17938853800296783,
      "learning_rate": 4.429666666666667e-05,
      "loss": 0.0039,
      "step": 17110
    },
    {
      "epoch": 0.9130666666666667,
      "grad_norm": 0.6478679776191711,
      "learning_rate": 4.429333333333334e-05,
      "loss": 0.0041,
      "step": 17120
    },
    {
      "epoch": 0.9136,
      "grad_norm": 0.09508741647005081,
      "learning_rate": 4.4290000000000005e-05,
      "loss": 0.0036,
      "step": 17130
    },
    {
      "epoch": 0.9141333333333334,
      "grad_norm": 0.7154514193534851,
      "learning_rate": 4.428666666666667e-05,
      "loss": 0.0032,
      "step": 17140
    },
    {
      "epoch": 0.9146666666666666,
      "grad_norm": 0.12724192440509796,
      "learning_rate": 4.428333333333334e-05,
      "loss": 0.0025,
      "step": 17150
    },
    {
      "epoch": 0.9152,
      "grad_norm": 0.13367079198360443,
      "learning_rate": 4.428e-05,
      "loss": 0.004,
      "step": 17160
    },
    {
      "epoch": 0.9157333333333333,
      "grad_norm": 0.40927812457084656,
      "learning_rate": 4.427666666666667e-05,
      "loss": 0.0024,
      "step": 17170
    },
    {
      "epoch": 0.9162666666666667,
      "grad_norm": 0.49240460991859436,
      "learning_rate": 4.4273333333333335e-05,
      "loss": 0.004,
      "step": 17180
    },
    {
      "epoch": 0.9168,
      "grad_norm": 0.4681985080242157,
      "learning_rate": 4.427e-05,
      "loss": 0.0025,
      "step": 17190
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 0.2514462172985077,
      "learning_rate": 4.426666666666667e-05,
      "loss": 0.0024,
      "step": 17200
    },
    {
      "epoch": 0.9178666666666667,
      "grad_norm": 0.2875334918498993,
      "learning_rate": 4.4263333333333334e-05,
      "loss": 0.0023,
      "step": 17210
    },
    {
      "epoch": 0.9184,
      "grad_norm": 0.326015830039978,
      "learning_rate": 4.426e-05,
      "loss": 0.0035,
      "step": 17220
    },
    {
      "epoch": 0.9189333333333334,
      "grad_norm": 0.04445462301373482,
      "learning_rate": 4.4256666666666666e-05,
      "loss": 0.0043,
      "step": 17230
    },
    {
      "epoch": 0.9194666666666667,
      "grad_norm": 0.408549427986145,
      "learning_rate": 4.425333333333334e-05,
      "loss": 0.0019,
      "step": 17240
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.3153238594532013,
      "learning_rate": 4.4250000000000005e-05,
      "loss": 0.0044,
      "step": 17250
    },
    {
      "epoch": 0.9205333333333333,
      "grad_norm": 0.22388862073421478,
      "learning_rate": 4.424666666666667e-05,
      "loss": 0.0048,
      "step": 17260
    },
    {
      "epoch": 0.9210666666666667,
      "grad_norm": 0.5131820440292358,
      "learning_rate": 4.424333333333334e-05,
      "loss": 0.0023,
      "step": 17270
    },
    {
      "epoch": 0.9216,
      "grad_norm": 0.21068954467773438,
      "learning_rate": 4.424e-05,
      "loss": 0.0034,
      "step": 17280
    },
    {
      "epoch": 0.9221333333333334,
      "grad_norm": 0.4459497332572937,
      "learning_rate": 4.423666666666667e-05,
      "loss": 0.0045,
      "step": 17290
    },
    {
      "epoch": 0.9226666666666666,
      "grad_norm": 0.04967131093144417,
      "learning_rate": 4.4233333333333336e-05,
      "loss": 0.0034,
      "step": 17300
    },
    {
      "epoch": 0.9232,
      "grad_norm": 0.5701353549957275,
      "learning_rate": 4.423e-05,
      "loss": 0.0044,
      "step": 17310
    },
    {
      "epoch": 0.9237333333333333,
      "grad_norm": 0.47564035654067993,
      "learning_rate": 4.422666666666667e-05,
      "loss": 0.0029,
      "step": 17320
    },
    {
      "epoch": 0.9242666666666667,
      "grad_norm": 0.7300146222114563,
      "learning_rate": 4.4223333333333334e-05,
      "loss": 0.0033,
      "step": 17330
    },
    {
      "epoch": 0.9248,
      "grad_norm": 0.10794825851917267,
      "learning_rate": 4.422e-05,
      "loss": 0.0046,
      "step": 17340
    },
    {
      "epoch": 0.9253333333333333,
      "grad_norm": 0.16846264898777008,
      "learning_rate": 4.4216666666666666e-05,
      "loss": 0.0026,
      "step": 17350
    },
    {
      "epoch": 0.9258666666666666,
      "grad_norm": 0.40172645449638367,
      "learning_rate": 4.421333333333333e-05,
      "loss": 0.0034,
      "step": 17360
    },
    {
      "epoch": 0.9264,
      "grad_norm": 0.1019451841711998,
      "learning_rate": 4.421e-05,
      "loss": 0.0021,
      "step": 17370
    },
    {
      "epoch": 0.9269333333333334,
      "grad_norm": 0.9146417379379272,
      "learning_rate": 4.420666666666667e-05,
      "loss": 0.0034,
      "step": 17380
    },
    {
      "epoch": 0.9274666666666667,
      "grad_norm": 0.05116001516580582,
      "learning_rate": 4.420333333333334e-05,
      "loss": 0.0021,
      "step": 17390
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.22302845120429993,
      "learning_rate": 4.4200000000000004e-05,
      "loss": 0.0033,
      "step": 17400
    },
    {
      "epoch": 0.9285333333333333,
      "grad_norm": 0.050938982516527176,
      "learning_rate": 4.419666666666667e-05,
      "loss": 0.0028,
      "step": 17410
    },
    {
      "epoch": 0.9290666666666667,
      "grad_norm": 0.04482597857713699,
      "learning_rate": 4.4193333333333336e-05,
      "loss": 0.0036,
      "step": 17420
    },
    {
      "epoch": 0.9296,
      "grad_norm": 0.47638627886772156,
      "learning_rate": 4.419e-05,
      "loss": 0.0023,
      "step": 17430
    },
    {
      "epoch": 0.9301333333333334,
      "grad_norm": 0.16301865875720978,
      "learning_rate": 4.418666666666667e-05,
      "loss": 0.0031,
      "step": 17440
    },
    {
      "epoch": 0.9306666666666666,
      "grad_norm": 0.2625531852245331,
      "learning_rate": 4.4183333333333334e-05,
      "loss": 0.0022,
      "step": 17450
    },
    {
      "epoch": 0.9312,
      "grad_norm": 0.3924480378627777,
      "learning_rate": 4.418000000000001e-05,
      "loss": 0.0024,
      "step": 17460
    },
    {
      "epoch": 0.9317333333333333,
      "grad_norm": 0.08588965982198715,
      "learning_rate": 4.417666666666667e-05,
      "loss": 0.0042,
      "step": 17470
    },
    {
      "epoch": 0.9322666666666667,
      "grad_norm": 0.5045193433761597,
      "learning_rate": 4.417333333333333e-05,
      "loss": 0.0034,
      "step": 17480
    },
    {
      "epoch": 0.9328,
      "grad_norm": 0.756203293800354,
      "learning_rate": 4.417e-05,
      "loss": 0.0036,
      "step": 17490
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.13779836893081665,
      "learning_rate": 4.4166666666666665e-05,
      "loss": 0.003,
      "step": 17500
    },
    {
      "epoch": 0.9338666666666666,
      "grad_norm": 0.04122190549969673,
      "learning_rate": 4.416333333333333e-05,
      "loss": 0.0031,
      "step": 17510
    },
    {
      "epoch": 0.9344,
      "grad_norm": 0.08655713498592377,
      "learning_rate": 4.4160000000000004e-05,
      "loss": 0.004,
      "step": 17520
    },
    {
      "epoch": 0.9349333333333333,
      "grad_norm": 0.18823838233947754,
      "learning_rate": 4.415666666666667e-05,
      "loss": 0.0031,
      "step": 17530
    },
    {
      "epoch": 0.9354666666666667,
      "grad_norm": 0.16022054851055145,
      "learning_rate": 4.4153333333333336e-05,
      "loss": 0.0043,
      "step": 17540
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.5435239672660828,
      "learning_rate": 4.415e-05,
      "loss": 0.0026,
      "step": 17550
    },
    {
      "epoch": 0.9365333333333333,
      "grad_norm": 0.30482274293899536,
      "learning_rate": 4.414666666666667e-05,
      "loss": 0.0028,
      "step": 17560
    },
    {
      "epoch": 0.9370666666666667,
      "grad_norm": 0.7190487384796143,
      "learning_rate": 4.4143333333333335e-05,
      "loss": 0.004,
      "step": 17570
    },
    {
      "epoch": 0.9376,
      "grad_norm": 0.5326714515686035,
      "learning_rate": 4.414e-05,
      "loss": 0.0036,
      "step": 17580
    },
    {
      "epoch": 0.9381333333333334,
      "grad_norm": 0.3385555148124695,
      "learning_rate": 4.4136666666666674e-05,
      "loss": 0.0026,
      "step": 17590
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 0.31430429220199585,
      "learning_rate": 4.413333333333334e-05,
      "loss": 0.0033,
      "step": 17600
    },
    {
      "epoch": 0.9392,
      "grad_norm": 0.5360178351402283,
      "learning_rate": 4.4130000000000006e-05,
      "loss": 0.0034,
      "step": 17610
    },
    {
      "epoch": 0.9397333333333333,
      "grad_norm": 0.4762198030948639,
      "learning_rate": 4.4126666666666665e-05,
      "loss": 0.0037,
      "step": 17620
    },
    {
      "epoch": 0.9402666666666667,
      "grad_norm": 0.4089970886707306,
      "learning_rate": 4.412333333333333e-05,
      "loss": 0.0043,
      "step": 17630
    },
    {
      "epoch": 0.9408,
      "grad_norm": 0.5114202499389648,
      "learning_rate": 4.412e-05,
      "loss": 0.0028,
      "step": 17640
    },
    {
      "epoch": 0.9413333333333334,
      "grad_norm": 0.08192755281925201,
      "learning_rate": 4.411666666666667e-05,
      "loss": 0.0034,
      "step": 17650
    },
    {
      "epoch": 0.9418666666666666,
      "grad_norm": 0.5390614867210388,
      "learning_rate": 4.411333333333334e-05,
      "loss": 0.0028,
      "step": 17660
    },
    {
      "epoch": 0.9424,
      "grad_norm": 0.10110815614461899,
      "learning_rate": 4.411e-05,
      "loss": 0.0037,
      "step": 17670
    },
    {
      "epoch": 0.9429333333333333,
      "grad_norm": 0.2834150493144989,
      "learning_rate": 4.410666666666667e-05,
      "loss": 0.0026,
      "step": 17680
    },
    {
      "epoch": 0.9434666666666667,
      "grad_norm": 0.21960261464118958,
      "learning_rate": 4.4103333333333335e-05,
      "loss": 0.0026,
      "step": 17690
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.6342853307723999,
      "learning_rate": 4.41e-05,
      "loss": 0.0025,
      "step": 17700
    },
    {
      "epoch": 0.9445333333333333,
      "grad_norm": 0.31851643323898315,
      "learning_rate": 4.409666666666667e-05,
      "loss": 0.0027,
      "step": 17710
    },
    {
      "epoch": 0.9450666666666667,
      "grad_norm": 0.600731611251831,
      "learning_rate": 4.4093333333333334e-05,
      "loss": 0.0026,
      "step": 17720
    },
    {
      "epoch": 0.9456,
      "grad_norm": 0.14026202261447906,
      "learning_rate": 4.4090000000000006e-05,
      "loss": 0.0039,
      "step": 17730
    },
    {
      "epoch": 0.9461333333333334,
      "grad_norm": 0.18962736427783966,
      "learning_rate": 4.408666666666667e-05,
      "loss": 0.0038,
      "step": 17740
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.5185924768447876,
      "learning_rate": 4.408333333333334e-05,
      "loss": 0.0027,
      "step": 17750
    },
    {
      "epoch": 0.9472,
      "grad_norm": 0.32514488697052,
      "learning_rate": 4.4080000000000005e-05,
      "loss": 0.0031,
      "step": 17760
    },
    {
      "epoch": 0.9477333333333333,
      "grad_norm": 0.2861745357513428,
      "learning_rate": 4.4076666666666664e-05,
      "loss": 0.0039,
      "step": 17770
    },
    {
      "epoch": 0.9482666666666667,
      "grad_norm": 0.483323872089386,
      "learning_rate": 4.407333333333333e-05,
      "loss": 0.0025,
      "step": 17780
    },
    {
      "epoch": 0.9488,
      "grad_norm": 0.13562241196632385,
      "learning_rate": 4.407e-05,
      "loss": 0.0035,
      "step": 17790
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 0.06922826915979385,
      "learning_rate": 4.406666666666667e-05,
      "loss": 0.003,
      "step": 17800
    },
    {
      "epoch": 0.9498666666666666,
      "grad_norm": 0.2847483456134796,
      "learning_rate": 4.4063333333333336e-05,
      "loss": 0.0029,
      "step": 17810
    },
    {
      "epoch": 0.9504,
      "grad_norm": 0.28500896692276,
      "learning_rate": 4.406e-05,
      "loss": 0.0028,
      "step": 17820
    },
    {
      "epoch": 0.9509333333333333,
      "grad_norm": 0.5300678610801697,
      "learning_rate": 4.405666666666667e-05,
      "loss": 0.0023,
      "step": 17830
    },
    {
      "epoch": 0.9514666666666667,
      "grad_norm": 0.1644853800535202,
      "learning_rate": 4.4053333333333334e-05,
      "loss": 0.003,
      "step": 17840
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.04652312770485878,
      "learning_rate": 4.405e-05,
      "loss": 0.0038,
      "step": 17850
    },
    {
      "epoch": 0.9525333333333333,
      "grad_norm": 0.30196642875671387,
      "learning_rate": 4.4046666666666666e-05,
      "loss": 0.0025,
      "step": 17860
    },
    {
      "epoch": 0.9530666666666666,
      "grad_norm": 0.1394810825586319,
      "learning_rate": 4.404333333333334e-05,
      "loss": 0.0032,
      "step": 17870
    },
    {
      "epoch": 0.9536,
      "grad_norm": 0.30587947368621826,
      "learning_rate": 4.4040000000000005e-05,
      "loss": 0.0033,
      "step": 17880
    },
    {
      "epoch": 0.9541333333333334,
      "grad_norm": 0.8707646131515503,
      "learning_rate": 4.403666666666667e-05,
      "loss": 0.0043,
      "step": 17890
    },
    {
      "epoch": 0.9546666666666667,
      "grad_norm": 0.5960128307342529,
      "learning_rate": 4.403333333333334e-05,
      "loss": 0.0046,
      "step": 17900
    },
    {
      "epoch": 0.9552,
      "grad_norm": 0.23296888172626495,
      "learning_rate": 4.4030000000000004e-05,
      "loss": 0.0038,
      "step": 17910
    },
    {
      "epoch": 0.9557333333333333,
      "grad_norm": 0.3742311894893646,
      "learning_rate": 4.402666666666666e-05,
      "loss": 0.0039,
      "step": 17920
    },
    {
      "epoch": 0.9562666666666667,
      "grad_norm": 0.5648189187049866,
      "learning_rate": 4.4023333333333336e-05,
      "loss": 0.0044,
      "step": 17930
    },
    {
      "epoch": 0.9568,
      "grad_norm": 0.07503396272659302,
      "learning_rate": 4.402e-05,
      "loss": 0.0046,
      "step": 17940
    },
    {
      "epoch": 0.9573333333333334,
      "grad_norm": 0.7899371385574341,
      "learning_rate": 4.401666666666667e-05,
      "loss": 0.0016,
      "step": 17950
    },
    {
      "epoch": 0.9578666666666666,
      "grad_norm": 0.13996948301792145,
      "learning_rate": 4.4013333333333334e-05,
      "loss": 0.0031,
      "step": 17960
    },
    {
      "epoch": 0.9584,
      "grad_norm": 0.1915227770805359,
      "learning_rate": 4.401e-05,
      "loss": 0.0026,
      "step": 17970
    },
    {
      "epoch": 0.9589333333333333,
      "grad_norm": 0.37945884466171265,
      "learning_rate": 4.4006666666666667e-05,
      "loss": 0.0027,
      "step": 17980
    },
    {
      "epoch": 0.9594666666666667,
      "grad_norm": 0.06334266066551208,
      "learning_rate": 4.400333333333333e-05,
      "loss": 0.0048,
      "step": 17990
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.0882427766919136,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.0031,
      "step": 18000
    },
    {
      "epoch": 0.9605333333333334,
      "grad_norm": 0.4062078297138214,
      "learning_rate": 4.399666666666667e-05,
      "loss": 0.0023,
      "step": 18010
    },
    {
      "epoch": 0.9610666666666666,
      "grad_norm": 0.7515228986740112,
      "learning_rate": 4.399333333333334e-05,
      "loss": 0.0026,
      "step": 18020
    },
    {
      "epoch": 0.9616,
      "grad_norm": 0.1925925314426422,
      "learning_rate": 4.3990000000000004e-05,
      "loss": 0.0033,
      "step": 18030
    },
    {
      "epoch": 0.9621333333333333,
      "grad_norm": 0.5074363350868225,
      "learning_rate": 4.398666666666667e-05,
      "loss": 0.0023,
      "step": 18040
    },
    {
      "epoch": 0.9626666666666667,
      "grad_norm": 0.1298515498638153,
      "learning_rate": 4.3983333333333336e-05,
      "loss": 0.0026,
      "step": 18050
    },
    {
      "epoch": 0.9632,
      "grad_norm": 0.14371739327907562,
      "learning_rate": 4.398e-05,
      "loss": 0.004,
      "step": 18060
    },
    {
      "epoch": 0.9637333333333333,
      "grad_norm": 0.5403055548667908,
      "learning_rate": 4.397666666666667e-05,
      "loss": 0.0036,
      "step": 18070
    },
    {
      "epoch": 0.9642666666666667,
      "grad_norm": 0.09447634220123291,
      "learning_rate": 4.3973333333333335e-05,
      "loss": 0.0035,
      "step": 18080
    },
    {
      "epoch": 0.9648,
      "grad_norm": 0.3628852069377899,
      "learning_rate": 4.397e-05,
      "loss": 0.0031,
      "step": 18090
    },
    {
      "epoch": 0.9653333333333334,
      "grad_norm": 0.06731195002794266,
      "learning_rate": 4.396666666666667e-05,
      "loss": 0.0027,
      "step": 18100
    },
    {
      "epoch": 0.9658666666666667,
      "grad_norm": 0.44318607449531555,
      "learning_rate": 4.396333333333333e-05,
      "loss": 0.0026,
      "step": 18110
    },
    {
      "epoch": 0.9664,
      "grad_norm": 0.04516857862472534,
      "learning_rate": 4.396e-05,
      "loss": 0.0038,
      "step": 18120
    },
    {
      "epoch": 0.9669333333333333,
      "grad_norm": 0.42465823888778687,
      "learning_rate": 4.3956666666666665e-05,
      "loss": 0.004,
      "step": 18130
    },
    {
      "epoch": 0.9674666666666667,
      "grad_norm": 0.6607658267021179,
      "learning_rate": 4.395333333333334e-05,
      "loss": 0.0031,
      "step": 18140
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.2701997458934784,
      "learning_rate": 4.3950000000000004e-05,
      "loss": 0.0029,
      "step": 18150
    },
    {
      "epoch": 0.9685333333333334,
      "grad_norm": 0.11404460668563843,
      "learning_rate": 4.394666666666667e-05,
      "loss": 0.0026,
      "step": 18160
    },
    {
      "epoch": 0.9690666666666666,
      "grad_norm": 0.8417750000953674,
      "learning_rate": 4.394333333333334e-05,
      "loss": 0.0021,
      "step": 18170
    },
    {
      "epoch": 0.9696,
      "grad_norm": 0.2927400767803192,
      "learning_rate": 4.394e-05,
      "loss": 0.0032,
      "step": 18180
    },
    {
      "epoch": 0.9701333333333333,
      "grad_norm": 0.31821408867836,
      "learning_rate": 4.393666666666667e-05,
      "loss": 0.0031,
      "step": 18190
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 0.22850361466407776,
      "learning_rate": 4.3933333333333335e-05,
      "loss": 0.0034,
      "step": 18200
    },
    {
      "epoch": 0.9712,
      "grad_norm": 0.18933668732643127,
      "learning_rate": 4.393e-05,
      "loss": 0.0027,
      "step": 18210
    },
    {
      "epoch": 0.9717333333333333,
      "grad_norm": 0.12606337666511536,
      "learning_rate": 4.3926666666666674e-05,
      "loss": 0.0031,
      "step": 18220
    },
    {
      "epoch": 0.9722666666666666,
      "grad_norm": 0.23641569912433624,
      "learning_rate": 4.3923333333333333e-05,
      "loss": 0.0032,
      "step": 18230
    },
    {
      "epoch": 0.9728,
      "grad_norm": 0.7511111497879028,
      "learning_rate": 4.392e-05,
      "loss": 0.003,
      "step": 18240
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 0.29773852229118347,
      "learning_rate": 4.3916666666666666e-05,
      "loss": 0.003,
      "step": 18250
    },
    {
      "epoch": 0.9738666666666667,
      "grad_norm": 0.5746281147003174,
      "learning_rate": 4.391333333333333e-05,
      "loss": 0.002,
      "step": 18260
    },
    {
      "epoch": 0.9744,
      "grad_norm": 0.13839267194271088,
      "learning_rate": 4.391e-05,
      "loss": 0.0023,
      "step": 18270
    },
    {
      "epoch": 0.9749333333333333,
      "grad_norm": 0.7189134955406189,
      "learning_rate": 4.390666666666667e-05,
      "loss": 0.0027,
      "step": 18280
    },
    {
      "epoch": 0.9754666666666667,
      "grad_norm": 1.0195482969284058,
      "learning_rate": 4.390333333333334e-05,
      "loss": 0.0034,
      "step": 18290
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.3788963854312897,
      "learning_rate": 4.39e-05,
      "loss": 0.0044,
      "step": 18300
    },
    {
      "epoch": 0.9765333333333334,
      "grad_norm": 0.045500341802835464,
      "learning_rate": 4.389666666666667e-05,
      "loss": 0.0045,
      "step": 18310
    },
    {
      "epoch": 0.9770666666666666,
      "grad_norm": 0.1290951669216156,
      "learning_rate": 4.3893333333333335e-05,
      "loss": 0.0037,
      "step": 18320
    },
    {
      "epoch": 0.9776,
      "grad_norm": 0.2858622670173645,
      "learning_rate": 4.389e-05,
      "loss": 0.0029,
      "step": 18330
    },
    {
      "epoch": 0.9781333333333333,
      "grad_norm": 0.06224769353866577,
      "learning_rate": 4.388666666666667e-05,
      "loss": 0.0025,
      "step": 18340
    },
    {
      "epoch": 0.9786666666666667,
      "grad_norm": 0.34459713101387024,
      "learning_rate": 4.388333333333334e-05,
      "loss": 0.0026,
      "step": 18350
    },
    {
      "epoch": 0.9792,
      "grad_norm": 0.14152461290359497,
      "learning_rate": 4.388000000000001e-05,
      "loss": 0.0021,
      "step": 18360
    },
    {
      "epoch": 0.9797333333333333,
      "grad_norm": 0.12409840524196625,
      "learning_rate": 4.387666666666667e-05,
      "loss": 0.0026,
      "step": 18370
    },
    {
      "epoch": 0.9802666666666666,
      "grad_norm": 0.8138108253479004,
      "learning_rate": 4.387333333333333e-05,
      "loss": 0.0027,
      "step": 18380
    },
    {
      "epoch": 0.9808,
      "grad_norm": 0.12923364341259003,
      "learning_rate": 4.387e-05,
      "loss": 0.0026,
      "step": 18390
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 0.3854863941669464,
      "learning_rate": 4.3866666666666665e-05,
      "loss": 0.003,
      "step": 18400
    },
    {
      "epoch": 0.9818666666666667,
      "grad_norm": 0.22931179404258728,
      "learning_rate": 4.386333333333333e-05,
      "loss": 0.0029,
      "step": 18410
    },
    {
      "epoch": 0.9824,
      "grad_norm": 0.20537707209587097,
      "learning_rate": 4.3860000000000004e-05,
      "loss": 0.0039,
      "step": 18420
    },
    {
      "epoch": 0.9829333333333333,
      "grad_norm": 0.3139480650424957,
      "learning_rate": 4.385666666666667e-05,
      "loss": 0.0022,
      "step": 18430
    },
    {
      "epoch": 0.9834666666666667,
      "grad_norm": 0.03515670448541641,
      "learning_rate": 4.3853333333333336e-05,
      "loss": 0.0035,
      "step": 18440
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.11578255891799927,
      "learning_rate": 4.385e-05,
      "loss": 0.0023,
      "step": 18450
    },
    {
      "epoch": 0.9845333333333334,
      "grad_norm": 0.4980641007423401,
      "learning_rate": 4.384666666666667e-05,
      "loss": 0.0031,
      "step": 18460
    },
    {
      "epoch": 0.9850666666666666,
      "grad_norm": 0.09795204550027847,
      "learning_rate": 4.3843333333333334e-05,
      "loss": 0.0033,
      "step": 18470
    },
    {
      "epoch": 0.9856,
      "grad_norm": 0.846157968044281,
      "learning_rate": 4.384e-05,
      "loss": 0.0027,
      "step": 18480
    },
    {
      "epoch": 0.9861333333333333,
      "grad_norm": 0.598468005657196,
      "learning_rate": 4.383666666666667e-05,
      "loss": 0.0028,
      "step": 18490
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.10469882935285568,
      "learning_rate": 4.383333333333334e-05,
      "loss": 0.0016,
      "step": 18500
    },
    {
      "epoch": 0.9872,
      "grad_norm": 0.2820557951927185,
      "learning_rate": 4.3830000000000006e-05,
      "loss": 0.0043,
      "step": 18510
    },
    {
      "epoch": 0.9877333333333334,
      "grad_norm": 0.15923841297626495,
      "learning_rate": 4.382666666666667e-05,
      "loss": 0.0035,
      "step": 18520
    },
    {
      "epoch": 0.9882666666666666,
      "grad_norm": 0.34683942794799805,
      "learning_rate": 4.382333333333333e-05,
      "loss": 0.0028,
      "step": 18530
    },
    {
      "epoch": 0.9888,
      "grad_norm": 0.4052642583847046,
      "learning_rate": 4.382e-05,
      "loss": 0.0024,
      "step": 18540
    },
    {
      "epoch": 0.9893333333333333,
      "grad_norm": 0.17568707466125488,
      "learning_rate": 4.381666666666667e-05,
      "loss": 0.003,
      "step": 18550
    },
    {
      "epoch": 0.9898666666666667,
      "grad_norm": 0.2175188660621643,
      "learning_rate": 4.3813333333333336e-05,
      "loss": 0.0029,
      "step": 18560
    },
    {
      "epoch": 0.9904,
      "grad_norm": 0.5925824642181396,
      "learning_rate": 4.381e-05,
      "loss": 0.0026,
      "step": 18570
    },
    {
      "epoch": 0.9909333333333333,
      "grad_norm": 0.37424036860466003,
      "learning_rate": 4.380666666666667e-05,
      "loss": 0.0032,
      "step": 18580
    },
    {
      "epoch": 0.9914666666666667,
      "grad_norm": 0.14013011753559113,
      "learning_rate": 4.3803333333333335e-05,
      "loss": 0.0034,
      "step": 18590
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.04277268052101135,
      "learning_rate": 4.38e-05,
      "loss": 0.0032,
      "step": 18600
    },
    {
      "epoch": 0.9925333333333334,
      "grad_norm": 0.08887939900159836,
      "learning_rate": 4.379666666666667e-05,
      "loss": 0.0025,
      "step": 18610
    },
    {
      "epoch": 0.9930666666666667,
      "grad_norm": 0.28421857953071594,
      "learning_rate": 4.379333333333333e-05,
      "loss": 0.0035,
      "step": 18620
    },
    {
      "epoch": 0.9936,
      "grad_norm": 0.6640650629997253,
      "learning_rate": 4.3790000000000006e-05,
      "loss": 0.0022,
      "step": 18630
    },
    {
      "epoch": 0.9941333333333333,
      "grad_norm": 0.6157235503196716,
      "learning_rate": 4.378666666666667e-05,
      "loss": 0.0028,
      "step": 18640
    },
    {
      "epoch": 0.9946666666666667,
      "grad_norm": 0.5120286345481873,
      "learning_rate": 4.378333333333334e-05,
      "loss": 0.0033,
      "step": 18650
    },
    {
      "epoch": 0.9952,
      "grad_norm": 0.31389307975769043,
      "learning_rate": 4.3780000000000004e-05,
      "loss": 0.0038,
      "step": 18660
    },
    {
      "epoch": 0.9957333333333334,
      "grad_norm": 0.7153033018112183,
      "learning_rate": 4.377666666666667e-05,
      "loss": 0.0037,
      "step": 18670
    },
    {
      "epoch": 0.9962666666666666,
      "grad_norm": 0.13455970585346222,
      "learning_rate": 4.377333333333333e-05,
      "loss": 0.0031,
      "step": 18680
    },
    {
      "epoch": 0.9968,
      "grad_norm": 0.37447360157966614,
      "learning_rate": 4.377e-05,
      "loss": 0.0044,
      "step": 18690
    },
    {
      "epoch": 0.9973333333333333,
      "grad_norm": 0.08132661879062653,
      "learning_rate": 4.376666666666667e-05,
      "loss": 0.0024,
      "step": 18700
    },
    {
      "epoch": 0.9978666666666667,
      "grad_norm": 0.23471428453922272,
      "learning_rate": 4.3763333333333335e-05,
      "loss": 0.0028,
      "step": 18710
    },
    {
      "epoch": 0.9984,
      "grad_norm": 0.12889353930950165,
      "learning_rate": 4.376e-05,
      "loss": 0.0031,
      "step": 18720
    },
    {
      "epoch": 0.9989333333333333,
      "grad_norm": 0.10515633970499039,
      "learning_rate": 4.375666666666667e-05,
      "loss": 0.0039,
      "step": 18730
    },
    {
      "epoch": 0.9994666666666666,
      "grad_norm": 0.28343281149864197,
      "learning_rate": 4.3753333333333333e-05,
      "loss": 0.0031,
      "step": 18740
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.17392313480377197,
      "learning_rate": 4.375e-05,
      "loss": 0.0028,
      "step": 18750
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.0031256526708602905,
      "eval_runtime": 159.8857,
      "eval_samples_per_second": 1563.617,
      "eval_steps_per_second": 39.09,
      "step": 18750
    },
    {
      "epoch": 1.0005333333333333,
      "grad_norm": 0.0979364886879921,
      "learning_rate": 4.374666666666667e-05,
      "loss": 0.0038,
      "step": 18760
    },
    {
      "epoch": 1.0010666666666668,
      "grad_norm": 0.5232304930686951,
      "learning_rate": 4.374333333333334e-05,
      "loss": 0.0036,
      "step": 18770
    },
    {
      "epoch": 1.0016,
      "grad_norm": 0.2782911956310272,
      "learning_rate": 4.3740000000000005e-05,
      "loss": 0.0028,
      "step": 18780
    },
    {
      "epoch": 1.0021333333333333,
      "grad_norm": 0.2810579240322113,
      "learning_rate": 4.373666666666667e-05,
      "loss": 0.003,
      "step": 18790
    },
    {
      "epoch": 1.0026666666666666,
      "grad_norm": 0.41028329730033875,
      "learning_rate": 4.373333333333334e-05,
      "loss": 0.0021,
      "step": 18800
    },
    {
      "epoch": 1.0032,
      "grad_norm": 0.08258025348186493,
      "learning_rate": 4.373e-05,
      "loss": 0.0036,
      "step": 18810
    },
    {
      "epoch": 1.0037333333333334,
      "grad_norm": 0.23403994739055634,
      "learning_rate": 4.372666666666667e-05,
      "loss": 0.0022,
      "step": 18820
    },
    {
      "epoch": 1.0042666666666666,
      "grad_norm": 0.2341376692056656,
      "learning_rate": 4.3723333333333335e-05,
      "loss": 0.0033,
      "step": 18830
    },
    {
      "epoch": 1.0048,
      "grad_norm": 0.15461866557598114,
      "learning_rate": 4.372e-05,
      "loss": 0.0041,
      "step": 18840
    },
    {
      "epoch": 1.0053333333333334,
      "grad_norm": 0.7049219608306885,
      "learning_rate": 4.371666666666667e-05,
      "loss": 0.0027,
      "step": 18850
    },
    {
      "epoch": 1.0058666666666667,
      "grad_norm": 0.495125949382782,
      "learning_rate": 4.3713333333333334e-05,
      "loss": 0.0048,
      "step": 18860
    },
    {
      "epoch": 1.0064,
      "grad_norm": 0.44927307963371277,
      "learning_rate": 4.371e-05,
      "loss": 0.002,
      "step": 18870
    },
    {
      "epoch": 1.0069333333333332,
      "grad_norm": 0.3750188648700714,
      "learning_rate": 4.3706666666666666e-05,
      "loss": 0.0033,
      "step": 18880
    },
    {
      "epoch": 1.0074666666666667,
      "grad_norm": 0.3836604356765747,
      "learning_rate": 4.370333333333333e-05,
      "loss": 0.0036,
      "step": 18890
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.12876424193382263,
      "learning_rate": 4.3700000000000005e-05,
      "loss": 0.0022,
      "step": 18900
    },
    {
      "epoch": 1.0085333333333333,
      "grad_norm": 0.28526079654693604,
      "learning_rate": 4.369666666666667e-05,
      "loss": 0.003,
      "step": 18910
    },
    {
      "epoch": 1.0090666666666666,
      "grad_norm": 0.7257473468780518,
      "learning_rate": 4.369333333333334e-05,
      "loss": 0.0031,
      "step": 18920
    },
    {
      "epoch": 1.0096,
      "grad_norm": 0.12352170050144196,
      "learning_rate": 4.3690000000000004e-05,
      "loss": 0.003,
      "step": 18930
    },
    {
      "epoch": 1.0101333333333333,
      "grad_norm": 0.20259049534797668,
      "learning_rate": 4.368666666666667e-05,
      "loss": 0.003,
      "step": 18940
    },
    {
      "epoch": 1.0106666666666666,
      "grad_norm": 0.6391910910606384,
      "learning_rate": 4.3683333333333336e-05,
      "loss": 0.0042,
      "step": 18950
    },
    {
      "epoch": 1.0112,
      "grad_norm": 0.6032987833023071,
      "learning_rate": 4.368e-05,
      "loss": 0.0032,
      "step": 18960
    },
    {
      "epoch": 1.0117333333333334,
      "grad_norm": 0.36188486218452454,
      "learning_rate": 4.367666666666667e-05,
      "loss": 0.0029,
      "step": 18970
    },
    {
      "epoch": 1.0122666666666666,
      "grad_norm": 0.31497666239738464,
      "learning_rate": 4.3673333333333334e-05,
      "loss": 0.0035,
      "step": 18980
    },
    {
      "epoch": 1.0128,
      "grad_norm": 0.11131305247545242,
      "learning_rate": 4.367e-05,
      "loss": 0.0026,
      "step": 18990
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.34255021810531616,
      "learning_rate": 4.3666666666666666e-05,
      "loss": 0.0023,
      "step": 19000
    },
    {
      "epoch": 1.0138666666666667,
      "grad_norm": 0.1579863578081131,
      "learning_rate": 4.366333333333333e-05,
      "loss": 0.0021,
      "step": 19010
    },
    {
      "epoch": 1.0144,
      "grad_norm": 0.4035690426826477,
      "learning_rate": 4.366e-05,
      "loss": 0.0033,
      "step": 19020
    },
    {
      "epoch": 1.0149333333333332,
      "grad_norm": 0.16183137893676758,
      "learning_rate": 4.3656666666666665e-05,
      "loss": 0.0032,
      "step": 19030
    },
    {
      "epoch": 1.0154666666666667,
      "grad_norm": 0.22196051478385925,
      "learning_rate": 4.365333333333334e-05,
      "loss": 0.0028,
      "step": 19040
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.19649501144886017,
      "learning_rate": 4.3650000000000004e-05,
      "loss": 0.0041,
      "step": 19050
    },
    {
      "epoch": 1.0165333333333333,
      "grad_norm": 0.2868988513946533,
      "learning_rate": 4.364666666666667e-05,
      "loss": 0.0022,
      "step": 19060
    },
    {
      "epoch": 1.0170666666666666,
      "grad_norm": 0.13383328914642334,
      "learning_rate": 4.3643333333333336e-05,
      "loss": 0.0022,
      "step": 19070
    },
    {
      "epoch": 1.0176,
      "grad_norm": 0.12883374094963074,
      "learning_rate": 4.364e-05,
      "loss": 0.0022,
      "step": 19080
    },
    {
      "epoch": 1.0181333333333333,
      "grad_norm": 0.12642115354537964,
      "learning_rate": 4.363666666666667e-05,
      "loss": 0.0036,
      "step": 19090
    },
    {
      "epoch": 1.0186666666666666,
      "grad_norm": 0.1659480631351471,
      "learning_rate": 4.3633333333333335e-05,
      "loss": 0.0023,
      "step": 19100
    },
    {
      "epoch": 1.0192,
      "grad_norm": 0.40386858582496643,
      "learning_rate": 4.363000000000001e-05,
      "loss": 0.0043,
      "step": 19110
    },
    {
      "epoch": 1.0197333333333334,
      "grad_norm": 0.3141551911830902,
      "learning_rate": 4.3626666666666674e-05,
      "loss": 0.003,
      "step": 19120
    },
    {
      "epoch": 1.0202666666666667,
      "grad_norm": 0.6549730896949768,
      "learning_rate": 4.362333333333333e-05,
      "loss": 0.003,
      "step": 19130
    },
    {
      "epoch": 1.0208,
      "grad_norm": 0.4423879384994507,
      "learning_rate": 4.362e-05,
      "loss": 0.004,
      "step": 19140
    },
    {
      "epoch": 1.0213333333333334,
      "grad_norm": 0.1606542468070984,
      "learning_rate": 4.3616666666666665e-05,
      "loss": 0.003,
      "step": 19150
    },
    {
      "epoch": 1.0218666666666667,
      "grad_norm": 0.18711218237876892,
      "learning_rate": 4.361333333333333e-05,
      "loss": 0.0017,
      "step": 19160
    },
    {
      "epoch": 1.0224,
      "grad_norm": 0.04678964614868164,
      "learning_rate": 4.361e-05,
      "loss": 0.0025,
      "step": 19170
    },
    {
      "epoch": 1.0229333333333333,
      "grad_norm": 0.05513371154665947,
      "learning_rate": 4.360666666666667e-05,
      "loss": 0.0048,
      "step": 19180
    },
    {
      "epoch": 1.0234666666666667,
      "grad_norm": 0.2547978460788727,
      "learning_rate": 4.3603333333333337e-05,
      "loss": 0.0037,
      "step": 19190
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.28812748193740845,
      "learning_rate": 4.36e-05,
      "loss": 0.004,
      "step": 19200
    },
    {
      "epoch": 1.0245333333333333,
      "grad_norm": 0.239089697599411,
      "learning_rate": 4.359666666666667e-05,
      "loss": 0.0024,
      "step": 19210
    },
    {
      "epoch": 1.0250666666666666,
      "grad_norm": 0.3380855917930603,
      "learning_rate": 4.3593333333333335e-05,
      "loss": 0.0045,
      "step": 19220
    },
    {
      "epoch": 1.0256,
      "grad_norm": 0.43674159049987793,
      "learning_rate": 4.359e-05,
      "loss": 0.0021,
      "step": 19230
    },
    {
      "epoch": 1.0261333333333333,
      "grad_norm": 0.4500325918197632,
      "learning_rate": 4.358666666666667e-05,
      "loss": 0.0024,
      "step": 19240
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 0.12681685388088226,
      "learning_rate": 4.358333333333334e-05,
      "loss": 0.0038,
      "step": 19250
    },
    {
      "epoch": 1.0272,
      "grad_norm": 0.26353657245635986,
      "learning_rate": 4.3580000000000006e-05,
      "loss": 0.0028,
      "step": 19260
    },
    {
      "epoch": 1.0277333333333334,
      "grad_norm": 0.35182228684425354,
      "learning_rate": 4.357666666666667e-05,
      "loss": 0.003,
      "step": 19270
    },
    {
      "epoch": 1.0282666666666667,
      "grad_norm": 0.13902868330478668,
      "learning_rate": 4.357333333333333e-05,
      "loss": 0.0027,
      "step": 19280
    },
    {
      "epoch": 1.0288,
      "grad_norm": 0.1302744597196579,
      "learning_rate": 4.357e-05,
      "loss": 0.0033,
      "step": 19290
    },
    {
      "epoch": 1.0293333333333334,
      "grad_norm": 0.44968381524086,
      "learning_rate": 4.3566666666666664e-05,
      "loss": 0.0031,
      "step": 19300
    },
    {
      "epoch": 1.0298666666666667,
      "grad_norm": 0.31812360882759094,
      "learning_rate": 4.356333333333334e-05,
      "loss": 0.0028,
      "step": 19310
    },
    {
      "epoch": 1.0304,
      "grad_norm": 0.24946892261505127,
      "learning_rate": 4.356e-05,
      "loss": 0.0033,
      "step": 19320
    },
    {
      "epoch": 1.0309333333333333,
      "grad_norm": 0.254244863986969,
      "learning_rate": 4.355666666666667e-05,
      "loss": 0.0027,
      "step": 19330
    },
    {
      "epoch": 1.0314666666666668,
      "grad_norm": 0.43819674849510193,
      "learning_rate": 4.3553333333333335e-05,
      "loss": 0.0031,
      "step": 19340
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.4677448868751526,
      "learning_rate": 4.355e-05,
      "loss": 0.0039,
      "step": 19350
    },
    {
      "epoch": 1.0325333333333333,
      "grad_norm": 0.6260393857955933,
      "learning_rate": 4.354666666666667e-05,
      "loss": 0.004,
      "step": 19360
    },
    {
      "epoch": 1.0330666666666666,
      "grad_norm": 0.07591721415519714,
      "learning_rate": 4.3543333333333334e-05,
      "loss": 0.0026,
      "step": 19370
    },
    {
      "epoch": 1.0336,
      "grad_norm": 0.034526724368333817,
      "learning_rate": 4.354e-05,
      "loss": 0.003,
      "step": 19380
    },
    {
      "epoch": 1.0341333333333333,
      "grad_norm": 0.10093098133802414,
      "learning_rate": 4.353666666666667e-05,
      "loss": 0.0033,
      "step": 19390
    },
    {
      "epoch": 1.0346666666666666,
      "grad_norm": 0.047465287148952484,
      "learning_rate": 4.353333333333334e-05,
      "loss": 0.0041,
      "step": 19400
    },
    {
      "epoch": 1.0352,
      "grad_norm": 0.3869585692882538,
      "learning_rate": 4.3530000000000005e-05,
      "loss": 0.0036,
      "step": 19410
    },
    {
      "epoch": 1.0357333333333334,
      "grad_norm": 0.5889440774917603,
      "learning_rate": 4.352666666666667e-05,
      "loss": 0.0038,
      "step": 19420
    },
    {
      "epoch": 1.0362666666666667,
      "grad_norm": 0.46978238224983215,
      "learning_rate": 4.352333333333334e-05,
      "loss": 0.0018,
      "step": 19430
    },
    {
      "epoch": 1.0368,
      "grad_norm": 0.48307564854621887,
      "learning_rate": 4.352e-05,
      "loss": 0.0027,
      "step": 19440
    },
    {
      "epoch": 1.0373333333333334,
      "grad_norm": 0.06833906471729279,
      "learning_rate": 4.351666666666667e-05,
      "loss": 0.0035,
      "step": 19450
    },
    {
      "epoch": 1.0378666666666667,
      "grad_norm": 0.5273245573043823,
      "learning_rate": 4.3513333333333336e-05,
      "loss": 0.0029,
      "step": 19460
    },
    {
      "epoch": 1.0384,
      "grad_norm": 0.19374501705169678,
      "learning_rate": 4.351e-05,
      "loss": 0.0039,
      "step": 19470
    },
    {
      "epoch": 1.0389333333333333,
      "grad_norm": 0.1165991872549057,
      "learning_rate": 4.350666666666667e-05,
      "loss": 0.0027,
      "step": 19480
    },
    {
      "epoch": 1.0394666666666668,
      "grad_norm": 0.253810852766037,
      "learning_rate": 4.3503333333333334e-05,
      "loss": 0.003,
      "step": 19490
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.5985718965530396,
      "learning_rate": 4.35e-05,
      "loss": 0.0025,
      "step": 19500
    },
    {
      "epoch": 1.0405333333333333,
      "grad_norm": 0.21686358749866486,
      "learning_rate": 4.3496666666666666e-05,
      "loss": 0.0041,
      "step": 19510
    },
    {
      "epoch": 1.0410666666666666,
      "grad_norm": 0.28149670362472534,
      "learning_rate": 4.349333333333334e-05,
      "loss": 0.0026,
      "step": 19520
    },
    {
      "epoch": 1.0416,
      "grad_norm": 0.5633206963539124,
      "learning_rate": 4.3490000000000005e-05,
      "loss": 0.0034,
      "step": 19530
    },
    {
      "epoch": 1.0421333333333334,
      "grad_norm": 0.7488993406295776,
      "learning_rate": 4.348666666666667e-05,
      "loss": 0.0042,
      "step": 19540
    },
    {
      "epoch": 1.0426666666666666,
      "grad_norm": 0.0979340597987175,
      "learning_rate": 4.348333333333334e-05,
      "loss": 0.0037,
      "step": 19550
    },
    {
      "epoch": 1.0432,
      "grad_norm": 0.28113704919815063,
      "learning_rate": 4.3480000000000004e-05,
      "loss": 0.0033,
      "step": 19560
    },
    {
      "epoch": 1.0437333333333334,
      "grad_norm": 0.1547815054655075,
      "learning_rate": 4.347666666666667e-05,
      "loss": 0.004,
      "step": 19570
    },
    {
      "epoch": 1.0442666666666667,
      "grad_norm": 0.4374036490917206,
      "learning_rate": 4.3473333333333336e-05,
      "loss": 0.0027,
      "step": 19580
    },
    {
      "epoch": 1.0448,
      "grad_norm": 0.7809199690818787,
      "learning_rate": 4.347e-05,
      "loss": 0.0042,
      "step": 19590
    },
    {
      "epoch": 1.0453333333333332,
      "grad_norm": 0.4663439393043518,
      "learning_rate": 4.346666666666667e-05,
      "loss": 0.0031,
      "step": 19600
    },
    {
      "epoch": 1.0458666666666667,
      "grad_norm": 0.1895226389169693,
      "learning_rate": 4.3463333333333335e-05,
      "loss": 0.0028,
      "step": 19610
    },
    {
      "epoch": 1.0464,
      "grad_norm": 0.6879382133483887,
      "learning_rate": 4.346e-05,
      "loss": 0.003,
      "step": 19620
    },
    {
      "epoch": 1.0469333333333333,
      "grad_norm": 0.16312433779239655,
      "learning_rate": 4.345666666666667e-05,
      "loss": 0.0019,
      "step": 19630
    },
    {
      "epoch": 1.0474666666666668,
      "grad_norm": 0.0437503382563591,
      "learning_rate": 4.345333333333333e-05,
      "loss": 0.0029,
      "step": 19640
    },
    {
      "epoch": 1.048,
      "grad_norm": 0.6610596179962158,
      "learning_rate": 4.345e-05,
      "loss": 0.0041,
      "step": 19650
    },
    {
      "epoch": 1.0485333333333333,
      "grad_norm": 0.5269341468811035,
      "learning_rate": 4.344666666666667e-05,
      "loss": 0.0029,
      "step": 19660
    },
    {
      "epoch": 1.0490666666666666,
      "grad_norm": 0.6856774091720581,
      "learning_rate": 4.344333333333334e-05,
      "loss": 0.0031,
      "step": 19670
    },
    {
      "epoch": 1.0496,
      "grad_norm": 0.2708025276660919,
      "learning_rate": 4.3440000000000004e-05,
      "loss": 0.003,
      "step": 19680
    },
    {
      "epoch": 1.0501333333333334,
      "grad_norm": 0.23327066004276276,
      "learning_rate": 4.343666666666667e-05,
      "loss": 0.0031,
      "step": 19690
    },
    {
      "epoch": 1.0506666666666666,
      "grad_norm": 0.374569833278656,
      "learning_rate": 4.3433333333333336e-05,
      "loss": 0.0031,
      "step": 19700
    },
    {
      "epoch": 1.0512,
      "grad_norm": 0.6222189664840698,
      "learning_rate": 4.343e-05,
      "loss": 0.0032,
      "step": 19710
    },
    {
      "epoch": 1.0517333333333334,
      "grad_norm": 0.510100245475769,
      "learning_rate": 4.342666666666667e-05,
      "loss": 0.0027,
      "step": 19720
    },
    {
      "epoch": 1.0522666666666667,
      "grad_norm": 0.22881434857845306,
      "learning_rate": 4.3423333333333335e-05,
      "loss": 0.0029,
      "step": 19730
    },
    {
      "epoch": 1.0528,
      "grad_norm": 0.2705937922000885,
      "learning_rate": 4.342e-05,
      "loss": 0.0026,
      "step": 19740
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.6331387162208557,
      "learning_rate": 4.341666666666667e-05,
      "loss": 0.0021,
      "step": 19750
    },
    {
      "epoch": 1.0538666666666667,
      "grad_norm": 0.14591272175312042,
      "learning_rate": 4.341333333333333e-05,
      "loss": 0.0038,
      "step": 19760
    },
    {
      "epoch": 1.0544,
      "grad_norm": 0.29157161712646484,
      "learning_rate": 4.341e-05,
      "loss": 0.0041,
      "step": 19770
    },
    {
      "epoch": 1.0549333333333333,
      "grad_norm": 0.371314138174057,
      "learning_rate": 4.3406666666666666e-05,
      "loss": 0.004,
      "step": 19780
    },
    {
      "epoch": 1.0554666666666668,
      "grad_norm": 0.59228515625,
      "learning_rate": 4.340333333333333e-05,
      "loss": 0.0045,
      "step": 19790
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.08894489705562592,
      "learning_rate": 4.3400000000000005e-05,
      "loss": 0.0038,
      "step": 19800
    },
    {
      "epoch": 1.0565333333333333,
      "grad_norm": 0.1906152218580246,
      "learning_rate": 4.339666666666667e-05,
      "loss": 0.0026,
      "step": 19810
    },
    {
      "epoch": 1.0570666666666666,
      "grad_norm": 0.408199280500412,
      "learning_rate": 4.339333333333334e-05,
      "loss": 0.0033,
      "step": 19820
    },
    {
      "epoch": 1.0576,
      "grad_norm": 0.10855457931756973,
      "learning_rate": 4.339e-05,
      "loss": 0.0026,
      "step": 19830
    },
    {
      "epoch": 1.0581333333333334,
      "grad_norm": 0.5649868845939636,
      "learning_rate": 4.338666666666667e-05,
      "loss": 0.0037,
      "step": 19840
    },
    {
      "epoch": 1.0586666666666666,
      "grad_norm": 0.17617470026016235,
      "learning_rate": 4.3383333333333335e-05,
      "loss": 0.0023,
      "step": 19850
    },
    {
      "epoch": 1.0592,
      "grad_norm": 0.6937511563301086,
      "learning_rate": 4.338e-05,
      "loss": 0.0033,
      "step": 19860
    },
    {
      "epoch": 1.0597333333333334,
      "grad_norm": 0.6545805335044861,
      "learning_rate": 4.3376666666666674e-05,
      "loss": 0.0034,
      "step": 19870
    },
    {
      "epoch": 1.0602666666666667,
      "grad_norm": 0.07348493486642838,
      "learning_rate": 4.337333333333334e-05,
      "loss": 0.0033,
      "step": 19880
    },
    {
      "epoch": 1.0608,
      "grad_norm": 0.5255890488624573,
      "learning_rate": 4.337e-05,
      "loss": 0.0037,
      "step": 19890
    },
    {
      "epoch": 1.0613333333333332,
      "grad_norm": 0.16932973265647888,
      "learning_rate": 4.3366666666666666e-05,
      "loss": 0.0028,
      "step": 19900
    },
    {
      "epoch": 1.0618666666666667,
      "grad_norm": 0.6474075317382812,
      "learning_rate": 4.336333333333333e-05,
      "loss": 0.0034,
      "step": 19910
    },
    {
      "epoch": 1.0624,
      "grad_norm": 0.19137480854988098,
      "learning_rate": 4.336e-05,
      "loss": 0.0031,
      "step": 19920
    },
    {
      "epoch": 1.0629333333333333,
      "grad_norm": 0.09926076978445053,
      "learning_rate": 4.3356666666666664e-05,
      "loss": 0.0028,
      "step": 19930
    },
    {
      "epoch": 1.0634666666666668,
      "grad_norm": 0.4970332682132721,
      "learning_rate": 4.335333333333334e-05,
      "loss": 0.0032,
      "step": 19940
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.06775117665529251,
      "learning_rate": 4.335e-05,
      "loss": 0.0038,
      "step": 19950
    },
    {
      "epoch": 1.0645333333333333,
      "grad_norm": 0.3908933103084564,
      "learning_rate": 4.334666666666667e-05,
      "loss": 0.0034,
      "step": 19960
    },
    {
      "epoch": 1.0650666666666666,
      "grad_norm": 0.06966707855463028,
      "learning_rate": 4.3343333333333336e-05,
      "loss": 0.0032,
      "step": 19970
    },
    {
      "epoch": 1.0656,
      "grad_norm": 0.37137869000434875,
      "learning_rate": 4.334e-05,
      "loss": 0.0036,
      "step": 19980
    },
    {
      "epoch": 1.0661333333333334,
      "grad_norm": 0.28911277651786804,
      "learning_rate": 4.333666666666667e-05,
      "loss": 0.0022,
      "step": 19990
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.10440356284379959,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 0.0029,
      "step": 20000
    },
    {
      "epoch": 1.0672,
      "grad_norm": 0.35493478178977966,
      "learning_rate": 4.333000000000001e-05,
      "loss": 0.0037,
      "step": 20010
    },
    {
      "epoch": 1.0677333333333334,
      "grad_norm": 0.0754459947347641,
      "learning_rate": 4.332666666666667e-05,
      "loss": 0.0036,
      "step": 20020
    },
    {
      "epoch": 1.0682666666666667,
      "grad_norm": 0.19010457396507263,
      "learning_rate": 4.332333333333334e-05,
      "loss": 0.0033,
      "step": 20030
    },
    {
      "epoch": 1.0688,
      "grad_norm": 0.37421509623527527,
      "learning_rate": 4.332e-05,
      "loss": 0.0028,
      "step": 20040
    },
    {
      "epoch": 1.0693333333333332,
      "grad_norm": 0.11814906448125839,
      "learning_rate": 4.3316666666666665e-05,
      "loss": 0.0028,
      "step": 20050
    },
    {
      "epoch": 1.0698666666666667,
      "grad_norm": 0.1268657147884369,
      "learning_rate": 4.331333333333333e-05,
      "loss": 0.0031,
      "step": 20060
    },
    {
      "epoch": 1.0704,
      "grad_norm": 0.2334461510181427,
      "learning_rate": 4.3310000000000004e-05,
      "loss": 0.0035,
      "step": 20070
    },
    {
      "epoch": 1.0709333333333333,
      "grad_norm": 0.3832893669605255,
      "learning_rate": 4.330666666666667e-05,
      "loss": 0.0028,
      "step": 20080
    },
    {
      "epoch": 1.0714666666666666,
      "grad_norm": 0.35068514943122864,
      "learning_rate": 4.3303333333333336e-05,
      "loss": 0.0044,
      "step": 20090
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.3774662911891937,
      "learning_rate": 4.33e-05,
      "loss": 0.0033,
      "step": 20100
    },
    {
      "epoch": 1.0725333333333333,
      "grad_norm": 0.225852370262146,
      "learning_rate": 4.329666666666667e-05,
      "loss": 0.0043,
      "step": 20110
    },
    {
      "epoch": 1.0730666666666666,
      "grad_norm": 0.5914380550384521,
      "learning_rate": 4.3293333333333334e-05,
      "loss": 0.004,
      "step": 20120
    },
    {
      "epoch": 1.0735999999999999,
      "grad_norm": 0.10879062116146088,
      "learning_rate": 4.329e-05,
      "loss": 0.0035,
      "step": 20130
    },
    {
      "epoch": 1.0741333333333334,
      "grad_norm": 0.32540544867515564,
      "learning_rate": 4.328666666666667e-05,
      "loss": 0.0037,
      "step": 20140
    },
    {
      "epoch": 1.0746666666666667,
      "grad_norm": 0.40545424818992615,
      "learning_rate": 4.328333333333334e-05,
      "loss": 0.0023,
      "step": 20150
    },
    {
      "epoch": 1.0752,
      "grad_norm": 0.4967550039291382,
      "learning_rate": 4.3280000000000006e-05,
      "loss": 0.0034,
      "step": 20160
    },
    {
      "epoch": 1.0757333333333334,
      "grad_norm": 0.5586826205253601,
      "learning_rate": 4.327666666666667e-05,
      "loss": 0.0028,
      "step": 20170
    },
    {
      "epoch": 1.0762666666666667,
      "grad_norm": 0.6452155113220215,
      "learning_rate": 4.327333333333334e-05,
      "loss": 0.0031,
      "step": 20180
    },
    {
      "epoch": 1.0768,
      "grad_norm": 0.12101417779922485,
      "learning_rate": 4.327e-05,
      "loss": 0.0026,
      "step": 20190
    },
    {
      "epoch": 1.0773333333333333,
      "grad_norm": 0.15878134965896606,
      "learning_rate": 4.3266666666666664e-05,
      "loss": 0.0035,
      "step": 20200
    },
    {
      "epoch": 1.0778666666666668,
      "grad_norm": 0.09866853803396225,
      "learning_rate": 4.3263333333333336e-05,
      "loss": 0.0019,
      "step": 20210
    },
    {
      "epoch": 1.0784,
      "grad_norm": 0.7206912040710449,
      "learning_rate": 4.326e-05,
      "loss": 0.0026,
      "step": 20220
    },
    {
      "epoch": 1.0789333333333333,
      "grad_norm": 0.04086960852146149,
      "learning_rate": 4.325666666666667e-05,
      "loss": 0.0031,
      "step": 20230
    },
    {
      "epoch": 1.0794666666666666,
      "grad_norm": 0.43116050958633423,
      "learning_rate": 4.3253333333333335e-05,
      "loss": 0.0026,
      "step": 20240
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.5921223163604736,
      "learning_rate": 4.325e-05,
      "loss": 0.0036,
      "step": 20250
    },
    {
      "epoch": 1.0805333333333333,
      "grad_norm": 0.37060174345970154,
      "learning_rate": 4.324666666666667e-05,
      "loss": 0.0029,
      "step": 20260
    },
    {
      "epoch": 1.0810666666666666,
      "grad_norm": 0.09788258373737335,
      "learning_rate": 4.324333333333333e-05,
      "loss": 0.0029,
      "step": 20270
    },
    {
      "epoch": 1.0816,
      "grad_norm": 0.5299778580665588,
      "learning_rate": 4.324e-05,
      "loss": 0.003,
      "step": 20280
    },
    {
      "epoch": 1.0821333333333334,
      "grad_norm": 0.4938657283782959,
      "learning_rate": 4.323666666666667e-05,
      "loss": 0.0032,
      "step": 20290
    },
    {
      "epoch": 1.0826666666666667,
      "grad_norm": 0.1857675313949585,
      "learning_rate": 4.323333333333334e-05,
      "loss": 0.0036,
      "step": 20300
    },
    {
      "epoch": 1.0832,
      "grad_norm": 0.3348947763442993,
      "learning_rate": 4.3230000000000005e-05,
      "loss": 0.0031,
      "step": 20310
    },
    {
      "epoch": 1.0837333333333334,
      "grad_norm": 0.25079262256622314,
      "learning_rate": 4.322666666666667e-05,
      "loss": 0.0038,
      "step": 20320
    },
    {
      "epoch": 1.0842666666666667,
      "grad_norm": 0.18778792023658752,
      "learning_rate": 4.322333333333334e-05,
      "loss": 0.0041,
      "step": 20330
    },
    {
      "epoch": 1.0848,
      "grad_norm": 0.4714503884315491,
      "learning_rate": 4.3219999999999996e-05,
      "loss": 0.0025,
      "step": 20340
    },
    {
      "epoch": 1.0853333333333333,
      "grad_norm": 0.29451993107795715,
      "learning_rate": 4.321666666666667e-05,
      "loss": 0.0036,
      "step": 20350
    },
    {
      "epoch": 1.0858666666666668,
      "grad_norm": 0.3694609999656677,
      "learning_rate": 4.3213333333333335e-05,
      "loss": 0.0033,
      "step": 20360
    },
    {
      "epoch": 1.0864,
      "grad_norm": 0.21713855862617493,
      "learning_rate": 4.321e-05,
      "loss": 0.0026,
      "step": 20370
    },
    {
      "epoch": 1.0869333333333333,
      "grad_norm": 0.287000447511673,
      "learning_rate": 4.320666666666667e-05,
      "loss": 0.0038,
      "step": 20380
    },
    {
      "epoch": 1.0874666666666666,
      "grad_norm": 1.0023202896118164,
      "learning_rate": 4.3203333333333334e-05,
      "loss": 0.0026,
      "step": 20390
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.03864084184169769,
      "learning_rate": 4.32e-05,
      "loss": 0.0047,
      "step": 20400
    },
    {
      "epoch": 1.0885333333333334,
      "grad_norm": 0.21710768342018127,
      "learning_rate": 4.3196666666666666e-05,
      "loss": 0.0031,
      "step": 20410
    },
    {
      "epoch": 1.0890666666666666,
      "grad_norm": 0.37885233759880066,
      "learning_rate": 4.319333333333334e-05,
      "loss": 0.0041,
      "step": 20420
    },
    {
      "epoch": 1.0896,
      "grad_norm": 0.5278370380401611,
      "learning_rate": 4.3190000000000005e-05,
      "loss": 0.0027,
      "step": 20430
    },
    {
      "epoch": 1.0901333333333334,
      "grad_norm": 0.25353091955184937,
      "learning_rate": 4.318666666666667e-05,
      "loss": 0.0024,
      "step": 20440
    },
    {
      "epoch": 1.0906666666666667,
      "grad_norm": 0.41736260056495667,
      "learning_rate": 4.318333333333334e-05,
      "loss": 0.0033,
      "step": 20450
    },
    {
      "epoch": 1.0912,
      "grad_norm": 0.09836455434560776,
      "learning_rate": 4.318e-05,
      "loss": 0.0034,
      "step": 20460
    },
    {
      "epoch": 1.0917333333333334,
      "grad_norm": 0.18669338524341583,
      "learning_rate": 4.317666666666667e-05,
      "loss": 0.0043,
      "step": 20470
    },
    {
      "epoch": 1.0922666666666667,
      "grad_norm": 0.21597036719322205,
      "learning_rate": 4.3173333333333336e-05,
      "loss": 0.0024,
      "step": 20480
    },
    {
      "epoch": 1.0928,
      "grad_norm": 0.03945665806531906,
      "learning_rate": 4.317e-05,
      "loss": 0.0019,
      "step": 20490
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 0.048472993075847626,
      "learning_rate": 4.316666666666667e-05,
      "loss": 0.0027,
      "step": 20500
    },
    {
      "epoch": 1.0938666666666668,
      "grad_norm": 0.10846181958913803,
      "learning_rate": 4.3163333333333334e-05,
      "loss": 0.0039,
      "step": 20510
    },
    {
      "epoch": 1.0944,
      "grad_norm": 1.0015596151351929,
      "learning_rate": 4.316e-05,
      "loss": 0.0043,
      "step": 20520
    },
    {
      "epoch": 1.0949333333333333,
      "grad_norm": 0.6543177962303162,
      "learning_rate": 4.3156666666666666e-05,
      "loss": 0.0039,
      "step": 20530
    },
    {
      "epoch": 1.0954666666666666,
      "grad_norm": 0.3408304452896118,
      "learning_rate": 4.315333333333333e-05,
      "loss": 0.0017,
      "step": 20540
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.4693388044834137,
      "learning_rate": 4.315e-05,
      "loss": 0.0025,
      "step": 20550
    },
    {
      "epoch": 1.0965333333333334,
      "grad_norm": 0.4402487874031067,
      "learning_rate": 4.314666666666667e-05,
      "loss": 0.0027,
      "step": 20560
    },
    {
      "epoch": 1.0970666666666666,
      "grad_norm": 0.3746061325073242,
      "learning_rate": 4.314333333333334e-05,
      "loss": 0.0031,
      "step": 20570
    },
    {
      "epoch": 1.0976,
      "grad_norm": 0.37917959690093994,
      "learning_rate": 4.3140000000000004e-05,
      "loss": 0.0037,
      "step": 20580
    },
    {
      "epoch": 1.0981333333333334,
      "grad_norm": 0.4978186786174774,
      "learning_rate": 4.313666666666667e-05,
      "loss": 0.0046,
      "step": 20590
    },
    {
      "epoch": 1.0986666666666667,
      "grad_norm": 0.2530654966831207,
      "learning_rate": 4.3133333333333336e-05,
      "loss": 0.0022,
      "step": 20600
    },
    {
      "epoch": 1.0992,
      "grad_norm": 0.6865738034248352,
      "learning_rate": 4.313e-05,
      "loss": 0.0033,
      "step": 20610
    },
    {
      "epoch": 1.0997333333333332,
      "grad_norm": 0.22030732035636902,
      "learning_rate": 4.312666666666667e-05,
      "loss": 0.0034,
      "step": 20620
    },
    {
      "epoch": 1.1002666666666667,
      "grad_norm": 0.34312349557876587,
      "learning_rate": 4.312333333333334e-05,
      "loss": 0.0025,
      "step": 20630
    },
    {
      "epoch": 1.1008,
      "grad_norm": 0.21994861960411072,
      "learning_rate": 4.312000000000001e-05,
      "loss": 0.0042,
      "step": 20640
    },
    {
      "epoch": 1.1013333333333333,
      "grad_norm": 0.31420454382896423,
      "learning_rate": 4.311666666666667e-05,
      "loss": 0.0038,
      "step": 20650
    },
    {
      "epoch": 1.1018666666666665,
      "grad_norm": 0.21422302722930908,
      "learning_rate": 4.311333333333333e-05,
      "loss": 0.0037,
      "step": 20660
    },
    {
      "epoch": 1.1024,
      "grad_norm": 0.10663216561079025,
      "learning_rate": 4.311e-05,
      "loss": 0.0033,
      "step": 20670
    },
    {
      "epoch": 1.1029333333333333,
      "grad_norm": 0.1907183974981308,
      "learning_rate": 4.3106666666666665e-05,
      "loss": 0.0033,
      "step": 20680
    },
    {
      "epoch": 1.1034666666666666,
      "grad_norm": 0.18979468941688538,
      "learning_rate": 4.310333333333333e-05,
      "loss": 0.0023,
      "step": 20690
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.46874764561653137,
      "learning_rate": 4.3100000000000004e-05,
      "loss": 0.0024,
      "step": 20700
    },
    {
      "epoch": 1.1045333333333334,
      "grad_norm": 0.16215772926807404,
      "learning_rate": 4.309666666666667e-05,
      "loss": 0.0029,
      "step": 20710
    },
    {
      "epoch": 1.1050666666666666,
      "grad_norm": 0.05021389201283455,
      "learning_rate": 4.3093333333333336e-05,
      "loss": 0.0022,
      "step": 20720
    },
    {
      "epoch": 1.1056,
      "grad_norm": 0.5939459204673767,
      "learning_rate": 4.309e-05,
      "loss": 0.0036,
      "step": 20730
    },
    {
      "epoch": 1.1061333333333334,
      "grad_norm": 0.18691591918468475,
      "learning_rate": 4.308666666666667e-05,
      "loss": 0.0027,
      "step": 20740
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 0.09827221930027008,
      "learning_rate": 4.3083333333333335e-05,
      "loss": 0.0028,
      "step": 20750
    },
    {
      "epoch": 1.1072,
      "grad_norm": 0.5203897356987,
      "learning_rate": 4.308e-05,
      "loss": 0.0033,
      "step": 20760
    },
    {
      "epoch": 1.1077333333333332,
      "grad_norm": 0.33917760848999023,
      "learning_rate": 4.3076666666666674e-05,
      "loss": 0.0027,
      "step": 20770
    },
    {
      "epoch": 1.1082666666666667,
      "grad_norm": 0.24862182140350342,
      "learning_rate": 4.307333333333334e-05,
      "loss": 0.0027,
      "step": 20780
    },
    {
      "epoch": 1.1088,
      "grad_norm": 0.16407790780067444,
      "learning_rate": 4.3070000000000006e-05,
      "loss": 0.0035,
      "step": 20790
    },
    {
      "epoch": 1.1093333333333333,
      "grad_norm": 0.1947394162416458,
      "learning_rate": 4.3066666666666665e-05,
      "loss": 0.0035,
      "step": 20800
    },
    {
      "epoch": 1.1098666666666666,
      "grad_norm": 0.12959714233875275,
      "learning_rate": 4.306333333333333e-05,
      "loss": 0.0043,
      "step": 20810
    },
    {
      "epoch": 1.1104,
      "grad_norm": 0.2160714715719223,
      "learning_rate": 4.306e-05,
      "loss": 0.0026,
      "step": 20820
    },
    {
      "epoch": 1.1109333333333333,
      "grad_norm": 0.24734438955783844,
      "learning_rate": 4.305666666666667e-05,
      "loss": 0.0032,
      "step": 20830
    },
    {
      "epoch": 1.1114666666666666,
      "grad_norm": 0.4926828444004059,
      "learning_rate": 4.305333333333334e-05,
      "loss": 0.0018,
      "step": 20840
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.21721696853637695,
      "learning_rate": 4.305e-05,
      "loss": 0.0021,
      "step": 20850
    },
    {
      "epoch": 1.1125333333333334,
      "grad_norm": 0.34163832664489746,
      "learning_rate": 4.304666666666667e-05,
      "loss": 0.0026,
      "step": 20860
    },
    {
      "epoch": 1.1130666666666666,
      "grad_norm": 0.27977636456489563,
      "learning_rate": 4.3043333333333335e-05,
      "loss": 0.0018,
      "step": 20870
    },
    {
      "epoch": 1.1136,
      "grad_norm": 0.09068465977907181,
      "learning_rate": 4.304e-05,
      "loss": 0.0031,
      "step": 20880
    },
    {
      "epoch": 1.1141333333333334,
      "grad_norm": 0.22676271200180054,
      "learning_rate": 4.303666666666667e-05,
      "loss": 0.0024,
      "step": 20890
    },
    {
      "epoch": 1.1146666666666667,
      "grad_norm": 0.0385039784014225,
      "learning_rate": 4.3033333333333334e-05,
      "loss": 0.0025,
      "step": 20900
    },
    {
      "epoch": 1.1152,
      "grad_norm": 0.0826692134141922,
      "learning_rate": 4.3030000000000006e-05,
      "loss": 0.0025,
      "step": 20910
    },
    {
      "epoch": 1.1157333333333332,
      "grad_norm": 0.06851880252361298,
      "learning_rate": 4.302666666666667e-05,
      "loss": 0.0031,
      "step": 20920
    },
    {
      "epoch": 1.1162666666666667,
      "grad_norm": 0.4049850404262543,
      "learning_rate": 4.302333333333334e-05,
      "loss": 0.003,
      "step": 20930
    },
    {
      "epoch": 1.1168,
      "grad_norm": 0.433769553899765,
      "learning_rate": 4.3020000000000005e-05,
      "loss": 0.0021,
      "step": 20940
    },
    {
      "epoch": 1.1173333333333333,
      "grad_norm": 0.7059372067451477,
      "learning_rate": 4.3016666666666664e-05,
      "loss": 0.0033,
      "step": 20950
    },
    {
      "epoch": 1.1178666666666666,
      "grad_norm": 0.4021257758140564,
      "learning_rate": 4.301333333333333e-05,
      "loss": 0.0027,
      "step": 20960
    },
    {
      "epoch": 1.1184,
      "grad_norm": 0.34193146228790283,
      "learning_rate": 4.301e-05,
      "loss": 0.0038,
      "step": 20970
    },
    {
      "epoch": 1.1189333333333333,
      "grad_norm": 0.0699257180094719,
      "learning_rate": 4.300666666666667e-05,
      "loss": 0.0028,
      "step": 20980
    },
    {
      "epoch": 1.1194666666666666,
      "grad_norm": 0.5590569972991943,
      "learning_rate": 4.3003333333333336e-05,
      "loss": 0.0036,
      "step": 20990
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.10252286493778229,
      "learning_rate": 4.3e-05,
      "loss": 0.0029,
      "step": 21000
    },
    {
      "epoch": 1.1205333333333334,
      "grad_norm": 0.6206605434417725,
      "learning_rate": 4.299666666666667e-05,
      "loss": 0.0044,
      "step": 21010
    },
    {
      "epoch": 1.1210666666666667,
      "grad_norm": 0.4968721568584442,
      "learning_rate": 4.2993333333333334e-05,
      "loss": 0.0033,
      "step": 21020
    },
    {
      "epoch": 1.1216,
      "grad_norm": 0.5074730515480042,
      "learning_rate": 4.299e-05,
      "loss": 0.0038,
      "step": 21030
    },
    {
      "epoch": 1.1221333333333334,
      "grad_norm": 0.3079799711704254,
      "learning_rate": 4.2986666666666666e-05,
      "loss": 0.0026,
      "step": 21040
    },
    {
      "epoch": 1.1226666666666667,
      "grad_norm": 0.3441048860549927,
      "learning_rate": 4.298333333333334e-05,
      "loss": 0.0024,
      "step": 21050
    },
    {
      "epoch": 1.1232,
      "grad_norm": 0.08799657970666885,
      "learning_rate": 4.2980000000000005e-05,
      "loss": 0.0035,
      "step": 21060
    },
    {
      "epoch": 1.1237333333333333,
      "grad_norm": 0.587884247303009,
      "learning_rate": 4.297666666666667e-05,
      "loss": 0.0037,
      "step": 21070
    },
    {
      "epoch": 1.1242666666666667,
      "grad_norm": 0.05124937370419502,
      "learning_rate": 4.297333333333334e-05,
      "loss": 0.0025,
      "step": 21080
    },
    {
      "epoch": 1.1248,
      "grad_norm": 0.05065039545297623,
      "learning_rate": 4.2970000000000004e-05,
      "loss": 0.0038,
      "step": 21090
    },
    {
      "epoch": 1.1253333333333333,
      "grad_norm": 0.04909956082701683,
      "learning_rate": 4.296666666666666e-05,
      "loss": 0.0032,
      "step": 21100
    },
    {
      "epoch": 1.1258666666666666,
      "grad_norm": 0.25348371267318726,
      "learning_rate": 4.2963333333333336e-05,
      "loss": 0.0036,
      "step": 21110
    },
    {
      "epoch": 1.1264,
      "grad_norm": 0.24995501339435577,
      "learning_rate": 4.296e-05,
      "loss": 0.0036,
      "step": 21120
    },
    {
      "epoch": 1.1269333333333333,
      "grad_norm": 0.5687757134437561,
      "learning_rate": 4.295666666666667e-05,
      "loss": 0.0038,
      "step": 21130
    },
    {
      "epoch": 1.1274666666666666,
      "grad_norm": 0.6211948394775391,
      "learning_rate": 4.2953333333333334e-05,
      "loss": 0.0042,
      "step": 21140
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.07783550769090652,
      "learning_rate": 4.295e-05,
      "loss": 0.0033,
      "step": 21150
    },
    {
      "epoch": 1.1285333333333334,
      "grad_norm": 0.13670159876346588,
      "learning_rate": 4.2946666666666667e-05,
      "loss": 0.0037,
      "step": 21160
    },
    {
      "epoch": 1.1290666666666667,
      "grad_norm": 0.1680450588464737,
      "learning_rate": 4.294333333333333e-05,
      "loss": 0.0029,
      "step": 21170
    },
    {
      "epoch": 1.1296,
      "grad_norm": 0.3025587797164917,
      "learning_rate": 4.2940000000000006e-05,
      "loss": 0.0017,
      "step": 21180
    },
    {
      "epoch": 1.1301333333333332,
      "grad_norm": 0.346610963344574,
      "learning_rate": 4.293666666666667e-05,
      "loss": 0.0025,
      "step": 21190
    },
    {
      "epoch": 1.1306666666666667,
      "grad_norm": 0.43289250135421753,
      "learning_rate": 4.293333333333334e-05,
      "loss": 0.0034,
      "step": 21200
    },
    {
      "epoch": 1.1312,
      "grad_norm": 0.5485474467277527,
      "learning_rate": 4.2930000000000004e-05,
      "loss": 0.0034,
      "step": 21210
    },
    {
      "epoch": 1.1317333333333333,
      "grad_norm": 0.06128428876399994,
      "learning_rate": 4.292666666666667e-05,
      "loss": 0.0034,
      "step": 21220
    },
    {
      "epoch": 1.1322666666666668,
      "grad_norm": 0.2533411979675293,
      "learning_rate": 4.2923333333333336e-05,
      "loss": 0.0025,
      "step": 21230
    },
    {
      "epoch": 1.1328,
      "grad_norm": 0.40310946106910706,
      "learning_rate": 4.292e-05,
      "loss": 0.0034,
      "step": 21240
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.16264942288398743,
      "learning_rate": 4.291666666666667e-05,
      "loss": 0.003,
      "step": 21250
    },
    {
      "epoch": 1.1338666666666666,
      "grad_norm": 0.4637143909931183,
      "learning_rate": 4.2913333333333335e-05,
      "loss": 0.0029,
      "step": 21260
    },
    {
      "epoch": 1.1344,
      "grad_norm": 0.2263612449169159,
      "learning_rate": 4.291e-05,
      "loss": 0.0023,
      "step": 21270
    },
    {
      "epoch": 1.1349333333333333,
      "grad_norm": 0.4621272087097168,
      "learning_rate": 4.290666666666667e-05,
      "loss": 0.0036,
      "step": 21280
    },
    {
      "epoch": 1.1354666666666666,
      "grad_norm": 0.10489523410797119,
      "learning_rate": 4.290333333333333e-05,
      "loss": 0.0025,
      "step": 21290
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 0.13124828040599823,
      "learning_rate": 4.29e-05,
      "loss": 0.0021,
      "step": 21300
    },
    {
      "epoch": 1.1365333333333334,
      "grad_norm": 0.09569656848907471,
      "learning_rate": 4.2896666666666665e-05,
      "loss": 0.002,
      "step": 21310
    },
    {
      "epoch": 1.1370666666666667,
      "grad_norm": 0.1310538947582245,
      "learning_rate": 4.289333333333334e-05,
      "loss": 0.0033,
      "step": 21320
    },
    {
      "epoch": 1.1376,
      "grad_norm": 0.05462972819805145,
      "learning_rate": 4.2890000000000004e-05,
      "loss": 0.0032,
      "step": 21330
    },
    {
      "epoch": 1.1381333333333332,
      "grad_norm": 0.05361044034361839,
      "learning_rate": 4.288666666666667e-05,
      "loss": 0.0035,
      "step": 21340
    },
    {
      "epoch": 1.1386666666666667,
      "grad_norm": 0.8051491975784302,
      "learning_rate": 4.288333333333334e-05,
      "loss": 0.0029,
      "step": 21350
    },
    {
      "epoch": 1.1392,
      "grad_norm": 0.09797319024801254,
      "learning_rate": 4.288e-05,
      "loss": 0.0037,
      "step": 21360
    },
    {
      "epoch": 1.1397333333333333,
      "grad_norm": 0.4350631535053253,
      "learning_rate": 4.287666666666667e-05,
      "loss": 0.0029,
      "step": 21370
    },
    {
      "epoch": 1.1402666666666668,
      "grad_norm": 0.2824077010154724,
      "learning_rate": 4.2873333333333335e-05,
      "loss": 0.0032,
      "step": 21380
    },
    {
      "epoch": 1.1408,
      "grad_norm": 0.3134339451789856,
      "learning_rate": 4.287000000000001e-05,
      "loss": 0.0021,
      "step": 21390
    },
    {
      "epoch": 1.1413333333333333,
      "grad_norm": 0.40626123547554016,
      "learning_rate": 4.286666666666667e-05,
      "loss": 0.0016,
      "step": 21400
    },
    {
      "epoch": 1.1418666666666666,
      "grad_norm": 0.09328059107065201,
      "learning_rate": 4.2863333333333333e-05,
      "loss": 0.0024,
      "step": 21410
    },
    {
      "epoch": 1.1424,
      "grad_norm": 0.4103248715400696,
      "learning_rate": 4.286e-05,
      "loss": 0.0026,
      "step": 21420
    },
    {
      "epoch": 1.1429333333333334,
      "grad_norm": 0.5316565632820129,
      "learning_rate": 4.2856666666666666e-05,
      "loss": 0.0042,
      "step": 21430
    },
    {
      "epoch": 1.1434666666666666,
      "grad_norm": 0.4416346848011017,
      "learning_rate": 4.285333333333333e-05,
      "loss": 0.006,
      "step": 21440
    },
    {
      "epoch": 1.144,
      "grad_norm": 0.31473782658576965,
      "learning_rate": 4.285e-05,
      "loss": 0.0039,
      "step": 21450
    },
    {
      "epoch": 1.1445333333333334,
      "grad_norm": 0.4330614507198334,
      "learning_rate": 4.284666666666667e-05,
      "loss": 0.004,
      "step": 21460
    },
    {
      "epoch": 1.1450666666666667,
      "grad_norm": 0.1670588105916977,
      "learning_rate": 4.284333333333334e-05,
      "loss": 0.0025,
      "step": 21470
    },
    {
      "epoch": 1.1456,
      "grad_norm": 0.2564288079738617,
      "learning_rate": 4.284e-05,
      "loss": 0.0033,
      "step": 21480
    },
    {
      "epoch": 1.1461333333333332,
      "grad_norm": 0.6485162377357483,
      "learning_rate": 4.283666666666667e-05,
      "loss": 0.0033,
      "step": 21490
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 0.22335322201251984,
      "learning_rate": 4.2833333333333335e-05,
      "loss": 0.0039,
      "step": 21500
    },
    {
      "epoch": 1.1472,
      "grad_norm": 0.22097332775592804,
      "learning_rate": 4.283e-05,
      "loss": 0.0035,
      "step": 21510
    },
    {
      "epoch": 1.1477333333333333,
      "grad_norm": 0.03745993971824646,
      "learning_rate": 4.282666666666667e-05,
      "loss": 0.0023,
      "step": 21520
    },
    {
      "epoch": 1.1482666666666668,
      "grad_norm": 0.12787188589572906,
      "learning_rate": 4.282333333333334e-05,
      "loss": 0.0043,
      "step": 21530
    },
    {
      "epoch": 1.1488,
      "grad_norm": 0.2699948251247406,
      "learning_rate": 4.282000000000001e-05,
      "loss": 0.0029,
      "step": 21540
    },
    {
      "epoch": 1.1493333333333333,
      "grad_norm": 0.07848737388849258,
      "learning_rate": 4.2816666666666666e-05,
      "loss": 0.0028,
      "step": 21550
    },
    {
      "epoch": 1.1498666666666666,
      "grad_norm": 0.18657033145427704,
      "learning_rate": 4.281333333333333e-05,
      "loss": 0.0028,
      "step": 21560
    },
    {
      "epoch": 1.1504,
      "grad_norm": 0.15551894903182983,
      "learning_rate": 4.281e-05,
      "loss": 0.0031,
      "step": 21570
    },
    {
      "epoch": 1.1509333333333334,
      "grad_norm": 0.34220343828201294,
      "learning_rate": 4.2806666666666665e-05,
      "loss": 0.0044,
      "step": 21580
    },
    {
      "epoch": 1.1514666666666666,
      "grad_norm": 0.5015566349029541,
      "learning_rate": 4.280333333333334e-05,
      "loss": 0.0023,
      "step": 21590
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.24865923821926117,
      "learning_rate": 4.2800000000000004e-05,
      "loss": 0.0029,
      "step": 21600
    },
    {
      "epoch": 1.1525333333333334,
      "grad_norm": 0.5594674348831177,
      "learning_rate": 4.279666666666667e-05,
      "loss": 0.003,
      "step": 21610
    },
    {
      "epoch": 1.1530666666666667,
      "grad_norm": 0.1260339319705963,
      "learning_rate": 4.2793333333333336e-05,
      "loss": 0.0035,
      "step": 21620
    },
    {
      "epoch": 1.1536,
      "grad_norm": 0.4386705458164215,
      "learning_rate": 4.279e-05,
      "loss": 0.0036,
      "step": 21630
    },
    {
      "epoch": 1.1541333333333332,
      "grad_norm": 0.5692106485366821,
      "learning_rate": 4.278666666666667e-05,
      "loss": 0.0029,
      "step": 21640
    },
    {
      "epoch": 1.1546666666666667,
      "grad_norm": 0.8322407007217407,
      "learning_rate": 4.2783333333333334e-05,
      "loss": 0.0025,
      "step": 21650
    },
    {
      "epoch": 1.1552,
      "grad_norm": 0.27917131781578064,
      "learning_rate": 4.278e-05,
      "loss": 0.0043,
      "step": 21660
    },
    {
      "epoch": 1.1557333333333333,
      "grad_norm": 0.0864119604229927,
      "learning_rate": 4.277666666666667e-05,
      "loss": 0.0026,
      "step": 21670
    },
    {
      "epoch": 1.1562666666666668,
      "grad_norm": 0.5228259563446045,
      "learning_rate": 4.277333333333334e-05,
      "loss": 0.0023,
      "step": 21680
    },
    {
      "epoch": 1.1568,
      "grad_norm": 0.2614899277687073,
      "learning_rate": 4.2770000000000006e-05,
      "loss": 0.0027,
      "step": 21690
    },
    {
      "epoch": 1.1573333333333333,
      "grad_norm": 0.19632412493228912,
      "learning_rate": 4.2766666666666665e-05,
      "loss": 0.0033,
      "step": 21700
    },
    {
      "epoch": 1.1578666666666666,
      "grad_norm": 0.5302062034606934,
      "learning_rate": 4.276333333333333e-05,
      "loss": 0.0051,
      "step": 21710
    },
    {
      "epoch": 1.1584,
      "grad_norm": 0.46801334619522095,
      "learning_rate": 4.276e-05,
      "loss": 0.0022,
      "step": 21720
    },
    {
      "epoch": 1.1589333333333334,
      "grad_norm": 0.36909857392311096,
      "learning_rate": 4.275666666666667e-05,
      "loss": 0.0035,
      "step": 21730
    },
    {
      "epoch": 1.1594666666666666,
      "grad_norm": 0.5881981253623962,
      "learning_rate": 4.2753333333333336e-05,
      "loss": 0.0036,
      "step": 21740
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.20432603359222412,
      "learning_rate": 4.275e-05,
      "loss": 0.0028,
      "step": 21750
    },
    {
      "epoch": 1.1605333333333334,
      "grad_norm": 0.6057000160217285,
      "learning_rate": 4.274666666666667e-05,
      "loss": 0.0038,
      "step": 21760
    },
    {
      "epoch": 1.1610666666666667,
      "grad_norm": 0.03251206874847412,
      "learning_rate": 4.2743333333333335e-05,
      "loss": 0.0031,
      "step": 21770
    },
    {
      "epoch": 1.1616,
      "grad_norm": 0.34772005677223206,
      "learning_rate": 4.274e-05,
      "loss": 0.0041,
      "step": 21780
    },
    {
      "epoch": 1.1621333333333332,
      "grad_norm": 0.5324479341506958,
      "learning_rate": 4.273666666666667e-05,
      "loss": 0.0027,
      "step": 21790
    },
    {
      "epoch": 1.1626666666666667,
      "grad_norm": 0.12503322958946228,
      "learning_rate": 4.273333333333333e-05,
      "loss": 0.0039,
      "step": 21800
    },
    {
      "epoch": 1.1632,
      "grad_norm": 0.09543176740407944,
      "learning_rate": 4.2730000000000006e-05,
      "loss": 0.0033,
      "step": 21810
    },
    {
      "epoch": 1.1637333333333333,
      "grad_norm": 0.16167080402374268,
      "learning_rate": 4.272666666666667e-05,
      "loss": 0.0034,
      "step": 21820
    },
    {
      "epoch": 1.1642666666666668,
      "grad_norm": 0.13076165318489075,
      "learning_rate": 4.272333333333334e-05,
      "loss": 0.0024,
      "step": 21830
    },
    {
      "epoch": 1.1648,
      "grad_norm": 0.12428442388772964,
      "learning_rate": 4.2720000000000004e-05,
      "loss": 0.0039,
      "step": 21840
    },
    {
      "epoch": 1.1653333333333333,
      "grad_norm": 0.5327088832855225,
      "learning_rate": 4.2716666666666664e-05,
      "loss": 0.0032,
      "step": 21850
    },
    {
      "epoch": 1.1658666666666666,
      "grad_norm": 0.9577863216400146,
      "learning_rate": 4.271333333333333e-05,
      "loss": 0.0027,
      "step": 21860
    },
    {
      "epoch": 1.1663999999999999,
      "grad_norm": 0.28535372018814087,
      "learning_rate": 4.271e-05,
      "loss": 0.0022,
      "step": 21870
    },
    {
      "epoch": 1.1669333333333334,
      "grad_norm": 0.5907301306724548,
      "learning_rate": 4.270666666666667e-05,
      "loss": 0.0032,
      "step": 21880
    },
    {
      "epoch": 1.1674666666666667,
      "grad_norm": 0.3771934509277344,
      "learning_rate": 4.2703333333333335e-05,
      "loss": 0.0027,
      "step": 21890
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.09570164233446121,
      "learning_rate": 4.27e-05,
      "loss": 0.0038,
      "step": 21900
    },
    {
      "epoch": 1.1685333333333334,
      "grad_norm": 0.3096349537372589,
      "learning_rate": 4.269666666666667e-05,
      "loss": 0.0042,
      "step": 21910
    },
    {
      "epoch": 1.1690666666666667,
      "grad_norm": 0.3754328489303589,
      "learning_rate": 4.2693333333333333e-05,
      "loss": 0.0034,
      "step": 21920
    },
    {
      "epoch": 1.1696,
      "grad_norm": 0.5355135202407837,
      "learning_rate": 4.269e-05,
      "loss": 0.0029,
      "step": 21930
    },
    {
      "epoch": 1.1701333333333332,
      "grad_norm": 0.1585083156824112,
      "learning_rate": 4.268666666666667e-05,
      "loss": 0.0028,
      "step": 21940
    },
    {
      "epoch": 1.1706666666666667,
      "grad_norm": 0.25996750593185425,
      "learning_rate": 4.268333333333334e-05,
      "loss": 0.0035,
      "step": 21950
    },
    {
      "epoch": 1.1712,
      "grad_norm": 0.13708406686782837,
      "learning_rate": 4.2680000000000005e-05,
      "loss": 0.0025,
      "step": 21960
    },
    {
      "epoch": 1.1717333333333333,
      "grad_norm": 0.5044327974319458,
      "learning_rate": 4.267666666666667e-05,
      "loss": 0.0033,
      "step": 21970
    },
    {
      "epoch": 1.1722666666666668,
      "grad_norm": 0.37087324261665344,
      "learning_rate": 4.267333333333334e-05,
      "loss": 0.0043,
      "step": 21980
    },
    {
      "epoch": 1.1728,
      "grad_norm": 0.5890202522277832,
      "learning_rate": 4.267e-05,
      "loss": 0.003,
      "step": 21990
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 0.3747604191303253,
      "learning_rate": 4.266666666666667e-05,
      "loss": 0.0028,
      "step": 22000
    },
    {
      "epoch": 1.1738666666666666,
      "grad_norm": 0.3526748716831207,
      "learning_rate": 4.2663333333333335e-05,
      "loss": 0.0024,
      "step": 22010
    },
    {
      "epoch": 1.1743999999999999,
      "grad_norm": 0.3094615042209625,
      "learning_rate": 4.266e-05,
      "loss": 0.0036,
      "step": 22020
    },
    {
      "epoch": 1.1749333333333334,
      "grad_norm": 0.0478803850710392,
      "learning_rate": 4.265666666666667e-05,
      "loss": 0.0027,
      "step": 22030
    },
    {
      "epoch": 1.1754666666666667,
      "grad_norm": 0.4124492108821869,
      "learning_rate": 4.2653333333333334e-05,
      "loss": 0.0049,
      "step": 22040
    },
    {
      "epoch": 1.176,
      "grad_norm": 0.07967536151409149,
      "learning_rate": 4.265e-05,
      "loss": 0.0037,
      "step": 22050
    },
    {
      "epoch": 1.1765333333333334,
      "grad_norm": 0.5938024520874023,
      "learning_rate": 4.2646666666666666e-05,
      "loss": 0.0034,
      "step": 22060
    },
    {
      "epoch": 1.1770666666666667,
      "grad_norm": 0.0643797367811203,
      "learning_rate": 4.264333333333333e-05,
      "loss": 0.0036,
      "step": 22070
    },
    {
      "epoch": 1.1776,
      "grad_norm": 0.4031015932559967,
      "learning_rate": 4.2640000000000005e-05,
      "loss": 0.003,
      "step": 22080
    },
    {
      "epoch": 1.1781333333333333,
      "grad_norm": 0.33635517954826355,
      "learning_rate": 4.263666666666667e-05,
      "loss": 0.0024,
      "step": 22090
    },
    {
      "epoch": 1.1786666666666668,
      "grad_norm": 0.22425855696201324,
      "learning_rate": 4.263333333333334e-05,
      "loss": 0.0029,
      "step": 22100
    },
    {
      "epoch": 1.1792,
      "grad_norm": 0.35212472081184387,
      "learning_rate": 4.2630000000000004e-05,
      "loss": 0.0026,
      "step": 22110
    },
    {
      "epoch": 1.1797333333333333,
      "grad_norm": 0.224906787276268,
      "learning_rate": 4.262666666666667e-05,
      "loss": 0.0035,
      "step": 22120
    },
    {
      "epoch": 1.1802666666666666,
      "grad_norm": 0.4101911783218384,
      "learning_rate": 4.2623333333333336e-05,
      "loss": 0.0032,
      "step": 22130
    },
    {
      "epoch": 1.1808,
      "grad_norm": 0.19697050750255585,
      "learning_rate": 4.262e-05,
      "loss": 0.0029,
      "step": 22140
    },
    {
      "epoch": 1.1813333333333333,
      "grad_norm": 0.3467850089073181,
      "learning_rate": 4.261666666666667e-05,
      "loss": 0.0034,
      "step": 22150
    },
    {
      "epoch": 1.1818666666666666,
      "grad_norm": 0.49484455585479736,
      "learning_rate": 4.2613333333333334e-05,
      "loss": 0.0032,
      "step": 22160
    },
    {
      "epoch": 1.1824,
      "grad_norm": 0.46206390857696533,
      "learning_rate": 4.261e-05,
      "loss": 0.0026,
      "step": 22170
    },
    {
      "epoch": 1.1829333333333334,
      "grad_norm": 0.16376200318336487,
      "learning_rate": 4.2606666666666666e-05,
      "loss": 0.0027,
      "step": 22180
    },
    {
      "epoch": 1.1834666666666667,
      "grad_norm": 0.5586428046226501,
      "learning_rate": 4.260333333333333e-05,
      "loss": 0.0029,
      "step": 22190
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.34769657254219055,
      "learning_rate": 4.26e-05,
      "loss": 0.0025,
      "step": 22200
    },
    {
      "epoch": 1.1845333333333334,
      "grad_norm": 0.08209618180990219,
      "learning_rate": 4.2596666666666665e-05,
      "loss": 0.0048,
      "step": 22210
    },
    {
      "epoch": 1.1850666666666667,
      "grad_norm": 0.1964930295944214,
      "learning_rate": 4.259333333333334e-05,
      "loss": 0.002,
      "step": 22220
    },
    {
      "epoch": 1.1856,
      "grad_norm": 0.1891043335199356,
      "learning_rate": 4.2590000000000004e-05,
      "loss": 0.0026,
      "step": 22230
    },
    {
      "epoch": 1.1861333333333333,
      "grad_norm": 0.8092895746231079,
      "learning_rate": 4.258666666666667e-05,
      "loss": 0.003,
      "step": 22240
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 0.558785080909729,
      "learning_rate": 4.2583333333333336e-05,
      "loss": 0.0023,
      "step": 22250
    },
    {
      "epoch": 1.1872,
      "grad_norm": 0.31517598032951355,
      "learning_rate": 4.258e-05,
      "loss": 0.0034,
      "step": 22260
    },
    {
      "epoch": 1.1877333333333333,
      "grad_norm": 0.2433006763458252,
      "learning_rate": 4.257666666666667e-05,
      "loss": 0.003,
      "step": 22270
    },
    {
      "epoch": 1.1882666666666666,
      "grad_norm": 0.1272314488887787,
      "learning_rate": 4.2573333333333335e-05,
      "loss": 0.0033,
      "step": 22280
    },
    {
      "epoch": 1.1888,
      "grad_norm": 0.18612611293792725,
      "learning_rate": 4.257000000000001e-05,
      "loss": 0.0039,
      "step": 22290
    },
    {
      "epoch": 1.1893333333333334,
      "grad_norm": 0.18752345442771912,
      "learning_rate": 4.2566666666666674e-05,
      "loss": 0.0031,
      "step": 22300
    },
    {
      "epoch": 1.1898666666666666,
      "grad_norm": 0.31379178166389465,
      "learning_rate": 4.256333333333333e-05,
      "loss": 0.0031,
      "step": 22310
    },
    {
      "epoch": 1.1904,
      "grad_norm": 0.5887129306793213,
      "learning_rate": 4.256e-05,
      "loss": 0.0032,
      "step": 22320
    },
    {
      "epoch": 1.1909333333333334,
      "grad_norm": 0.523858904838562,
      "learning_rate": 4.2556666666666665e-05,
      "loss": 0.0022,
      "step": 22330
    },
    {
      "epoch": 1.1914666666666667,
      "grad_norm": 0.3557352125644684,
      "learning_rate": 4.255333333333333e-05,
      "loss": 0.0028,
      "step": 22340
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.1891246885061264,
      "learning_rate": 4.2550000000000004e-05,
      "loss": 0.003,
      "step": 22350
    },
    {
      "epoch": 1.1925333333333334,
      "grad_norm": 0.2777871787548065,
      "learning_rate": 4.254666666666667e-05,
      "loss": 0.0034,
      "step": 22360
    },
    {
      "epoch": 1.1930666666666667,
      "grad_norm": 0.28328603506088257,
      "learning_rate": 4.2543333333333337e-05,
      "loss": 0.0034,
      "step": 22370
    },
    {
      "epoch": 1.1936,
      "grad_norm": 0.04072074592113495,
      "learning_rate": 4.254e-05,
      "loss": 0.0023,
      "step": 22380
    },
    {
      "epoch": 1.1941333333333333,
      "grad_norm": 0.559218168258667,
      "learning_rate": 4.253666666666667e-05,
      "loss": 0.0036,
      "step": 22390
    },
    {
      "epoch": 1.1946666666666665,
      "grad_norm": 0.8385552167892456,
      "learning_rate": 4.2533333333333335e-05,
      "loss": 0.0038,
      "step": 22400
    },
    {
      "epoch": 1.1952,
      "grad_norm": 0.18503065407276154,
      "learning_rate": 4.253e-05,
      "loss": 0.0042,
      "step": 22410
    },
    {
      "epoch": 1.1957333333333333,
      "grad_norm": 0.9218612313270569,
      "learning_rate": 4.252666666666667e-05,
      "loss": 0.0045,
      "step": 22420
    },
    {
      "epoch": 1.1962666666666666,
      "grad_norm": 0.6288145780563354,
      "learning_rate": 4.252333333333334e-05,
      "loss": 0.0032,
      "step": 22430
    },
    {
      "epoch": 1.1968,
      "grad_norm": 0.06972737610340118,
      "learning_rate": 4.2520000000000006e-05,
      "loss": 0.0037,
      "step": 22440
    },
    {
      "epoch": 1.1973333333333334,
      "grad_norm": 0.5532177686691284,
      "learning_rate": 4.251666666666667e-05,
      "loss": 0.0026,
      "step": 22450
    },
    {
      "epoch": 1.1978666666666666,
      "grad_norm": 0.06923460215330124,
      "learning_rate": 4.251333333333333e-05,
      "loss": 0.003,
      "step": 22460
    },
    {
      "epoch": 1.1984,
      "grad_norm": 0.6424967050552368,
      "learning_rate": 4.251e-05,
      "loss": 0.0029,
      "step": 22470
    },
    {
      "epoch": 1.1989333333333334,
      "grad_norm": 0.24035607278347015,
      "learning_rate": 4.2506666666666664e-05,
      "loss": 0.0033,
      "step": 22480
    },
    {
      "epoch": 1.1994666666666667,
      "grad_norm": 0.11106209456920624,
      "learning_rate": 4.250333333333334e-05,
      "loss": 0.0024,
      "step": 22490
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.37376025319099426,
      "learning_rate": 4.25e-05,
      "loss": 0.0028,
      "step": 22500
    },
    {
      "epoch": 1.2005333333333335,
      "grad_norm": 0.3868759274482727,
      "learning_rate": 4.249666666666667e-05,
      "loss": 0.003,
      "step": 22510
    },
    {
      "epoch": 1.2010666666666667,
      "grad_norm": 0.5230895280838013,
      "learning_rate": 4.2493333333333335e-05,
      "loss": 0.0033,
      "step": 22520
    },
    {
      "epoch": 1.2016,
      "grad_norm": 0.39994919300079346,
      "learning_rate": 4.249e-05,
      "loss": 0.0037,
      "step": 22530
    },
    {
      "epoch": 1.2021333333333333,
      "grad_norm": 0.49969929456710815,
      "learning_rate": 4.248666666666667e-05,
      "loss": 0.0032,
      "step": 22540
    },
    {
      "epoch": 1.2026666666666666,
      "grad_norm": 0.25127142667770386,
      "learning_rate": 4.2483333333333334e-05,
      "loss": 0.0028,
      "step": 22550
    },
    {
      "epoch": 1.2032,
      "grad_norm": 0.1862679272890091,
      "learning_rate": 4.248e-05,
      "loss": 0.0037,
      "step": 22560
    },
    {
      "epoch": 1.2037333333333333,
      "grad_norm": 0.495968222618103,
      "learning_rate": 4.247666666666667e-05,
      "loss": 0.0034,
      "step": 22570
    },
    {
      "epoch": 1.2042666666666666,
      "grad_norm": 0.711556613445282,
      "learning_rate": 4.247333333333334e-05,
      "loss": 0.0032,
      "step": 22580
    },
    {
      "epoch": 1.2048,
      "grad_norm": 0.38381409645080566,
      "learning_rate": 4.2470000000000005e-05,
      "loss": 0.0022,
      "step": 22590
    },
    {
      "epoch": 1.2053333333333334,
      "grad_norm": 0.06979568302631378,
      "learning_rate": 4.246666666666667e-05,
      "loss": 0.002,
      "step": 22600
    },
    {
      "epoch": 1.2058666666666666,
      "grad_norm": 0.20570695400238037,
      "learning_rate": 4.246333333333333e-05,
      "loss": 0.0024,
      "step": 22610
    },
    {
      "epoch": 1.2064,
      "grad_norm": 0.18448972702026367,
      "learning_rate": 4.246e-05,
      "loss": 0.0031,
      "step": 22620
    },
    {
      "epoch": 1.2069333333333334,
      "grad_norm": 0.37296828627586365,
      "learning_rate": 4.245666666666667e-05,
      "loss": 0.0036,
      "step": 22630
    },
    {
      "epoch": 1.2074666666666667,
      "grad_norm": 0.3690924048423767,
      "learning_rate": 4.2453333333333336e-05,
      "loss": 0.0038,
      "step": 22640
    },
    {
      "epoch": 1.208,
      "grad_norm": 0.3401707708835602,
      "learning_rate": 4.245e-05,
      "loss": 0.0031,
      "step": 22650
    },
    {
      "epoch": 1.2085333333333332,
      "grad_norm": 0.10071802884340286,
      "learning_rate": 4.244666666666667e-05,
      "loss": 0.0018,
      "step": 22660
    },
    {
      "epoch": 1.2090666666666667,
      "grad_norm": 0.07871245592832565,
      "learning_rate": 4.2443333333333334e-05,
      "loss": 0.0034,
      "step": 22670
    },
    {
      "epoch": 1.2096,
      "grad_norm": 0.030504753813147545,
      "learning_rate": 4.244e-05,
      "loss": 0.0026,
      "step": 22680
    },
    {
      "epoch": 1.2101333333333333,
      "grad_norm": 0.1903368979692459,
      "learning_rate": 4.2436666666666666e-05,
      "loss": 0.0038,
      "step": 22690
    },
    {
      "epoch": 1.2106666666666666,
      "grad_norm": 0.21417899429798126,
      "learning_rate": 4.243333333333334e-05,
      "loss": 0.0034,
      "step": 22700
    },
    {
      "epoch": 1.2112,
      "grad_norm": 0.4036199152469635,
      "learning_rate": 4.2430000000000005e-05,
      "loss": 0.0017,
      "step": 22710
    },
    {
      "epoch": 1.2117333333333333,
      "grad_norm": 0.5237503051757812,
      "learning_rate": 4.242666666666667e-05,
      "loss": 0.0034,
      "step": 22720
    },
    {
      "epoch": 1.2122666666666666,
      "grad_norm": 0.19308705627918243,
      "learning_rate": 4.242333333333334e-05,
      "loss": 0.0043,
      "step": 22730
    },
    {
      "epoch": 1.2128,
      "grad_norm": 0.06981183588504791,
      "learning_rate": 4.2420000000000004e-05,
      "loss": 0.004,
      "step": 22740
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.24767668545246124,
      "learning_rate": 4.241666666666667e-05,
      "loss": 0.0023,
      "step": 22750
    },
    {
      "epoch": 1.2138666666666666,
      "grad_norm": 0.06765690445899963,
      "learning_rate": 4.241333333333333e-05,
      "loss": 0.0021,
      "step": 22760
    },
    {
      "epoch": 1.2144,
      "grad_norm": 0.09030937403440475,
      "learning_rate": 4.241e-05,
      "loss": 0.0038,
      "step": 22770
    },
    {
      "epoch": 1.2149333333333334,
      "grad_norm": 0.1616586148738861,
      "learning_rate": 4.240666666666667e-05,
      "loss": 0.004,
      "step": 22780
    },
    {
      "epoch": 1.2154666666666667,
      "grad_norm": 0.30725759267807007,
      "learning_rate": 4.2403333333333334e-05,
      "loss": 0.0031,
      "step": 22790
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.10866899788379669,
      "learning_rate": 4.24e-05,
      "loss": 0.0029,
      "step": 22800
    },
    {
      "epoch": 1.2165333333333332,
      "grad_norm": 0.1326127052307129,
      "learning_rate": 4.239666666666667e-05,
      "loss": 0.0034,
      "step": 22810
    },
    {
      "epoch": 1.2170666666666667,
      "grad_norm": 0.5517559051513672,
      "learning_rate": 4.239333333333333e-05,
      "loss": 0.0019,
      "step": 22820
    },
    {
      "epoch": 1.2176,
      "grad_norm": 0.18776048719882965,
      "learning_rate": 4.239e-05,
      "loss": 0.002,
      "step": 22830
    },
    {
      "epoch": 1.2181333333333333,
      "grad_norm": 0.24762962758541107,
      "learning_rate": 4.238666666666667e-05,
      "loss": 0.0027,
      "step": 22840
    },
    {
      "epoch": 1.2186666666666666,
      "grad_norm": 0.4090704917907715,
      "learning_rate": 4.238333333333334e-05,
      "loss": 0.0027,
      "step": 22850
    },
    {
      "epoch": 1.2192,
      "grad_norm": 0.21840673685073853,
      "learning_rate": 4.2380000000000004e-05,
      "loss": 0.0019,
      "step": 22860
    },
    {
      "epoch": 1.2197333333333333,
      "grad_norm": 0.2822084426879883,
      "learning_rate": 4.237666666666667e-05,
      "loss": 0.0042,
      "step": 22870
    },
    {
      "epoch": 1.2202666666666666,
      "grad_norm": 0.2567201554775238,
      "learning_rate": 4.2373333333333336e-05,
      "loss": 0.0031,
      "step": 22880
    },
    {
      "epoch": 1.2208,
      "grad_norm": 0.06616697460412979,
      "learning_rate": 4.237e-05,
      "loss": 0.0041,
      "step": 22890
    },
    {
      "epoch": 1.2213333333333334,
      "grad_norm": 0.5253059267997742,
      "learning_rate": 4.236666666666667e-05,
      "loss": 0.0037,
      "step": 22900
    },
    {
      "epoch": 1.2218666666666667,
      "grad_norm": 0.5641282200813293,
      "learning_rate": 4.2363333333333335e-05,
      "loss": 0.0037,
      "step": 22910
    },
    {
      "epoch": 1.2224,
      "grad_norm": 0.6879724264144897,
      "learning_rate": 4.236e-05,
      "loss": 0.0031,
      "step": 22920
    },
    {
      "epoch": 1.2229333333333334,
      "grad_norm": 0.6491533517837524,
      "learning_rate": 4.235666666666667e-05,
      "loss": 0.004,
      "step": 22930
    },
    {
      "epoch": 1.2234666666666667,
      "grad_norm": 0.5231821537017822,
      "learning_rate": 4.235333333333333e-05,
      "loss": 0.003,
      "step": 22940
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.2789900004863739,
      "learning_rate": 4.235e-05,
      "loss": 0.0033,
      "step": 22950
    },
    {
      "epoch": 1.2245333333333333,
      "grad_norm": 0.4792308807373047,
      "learning_rate": 4.2346666666666666e-05,
      "loss": 0.0036,
      "step": 22960
    },
    {
      "epoch": 1.2250666666666667,
      "grad_norm": 0.40137338638305664,
      "learning_rate": 4.234333333333333e-05,
      "loss": 0.0029,
      "step": 22970
    },
    {
      "epoch": 1.2256,
      "grad_norm": 0.12578299641609192,
      "learning_rate": 4.2340000000000005e-05,
      "loss": 0.0028,
      "step": 22980
    },
    {
      "epoch": 1.2261333333333333,
      "grad_norm": 0.6192460656166077,
      "learning_rate": 4.233666666666667e-05,
      "loss": 0.0036,
      "step": 22990
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 0.4274347722530365,
      "learning_rate": 4.233333333333334e-05,
      "loss": 0.0024,
      "step": 23000
    },
    {
      "epoch": 1.2272,
      "grad_norm": 0.7142932415008545,
      "learning_rate": 4.233e-05,
      "loss": 0.0026,
      "step": 23010
    },
    {
      "epoch": 1.2277333333333333,
      "grad_norm": 0.3076006770133972,
      "learning_rate": 4.232666666666667e-05,
      "loss": 0.0022,
      "step": 23020
    },
    {
      "epoch": 1.2282666666666666,
      "grad_norm": 0.4302191138267517,
      "learning_rate": 4.2323333333333335e-05,
      "loss": 0.0032,
      "step": 23030
    },
    {
      "epoch": 1.2288000000000001,
      "grad_norm": 0.5845514535903931,
      "learning_rate": 4.232e-05,
      "loss": 0.0018,
      "step": 23040
    },
    {
      "epoch": 1.2293333333333334,
      "grad_norm": 0.2145959734916687,
      "learning_rate": 4.2316666666666674e-05,
      "loss": 0.0024,
      "step": 23050
    },
    {
      "epoch": 1.2298666666666667,
      "grad_norm": 0.18980610370635986,
      "learning_rate": 4.2313333333333334e-05,
      "loss": 0.0022,
      "step": 23060
    },
    {
      "epoch": 1.2304,
      "grad_norm": 0.13405318558216095,
      "learning_rate": 4.231e-05,
      "loss": 0.0029,
      "step": 23070
    },
    {
      "epoch": 1.2309333333333332,
      "grad_norm": 0.16314993798732758,
      "learning_rate": 4.2306666666666666e-05,
      "loss": 0.0019,
      "step": 23080
    },
    {
      "epoch": 1.2314666666666667,
      "grad_norm": 0.050836242735385895,
      "learning_rate": 4.230333333333333e-05,
      "loss": 0.0028,
      "step": 23090
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.22256651520729065,
      "learning_rate": 4.23e-05,
      "loss": 0.0029,
      "step": 23100
    },
    {
      "epoch": 1.2325333333333333,
      "grad_norm": 0.0963827446103096,
      "learning_rate": 4.229666666666667e-05,
      "loss": 0.0032,
      "step": 23110
    },
    {
      "epoch": 1.2330666666666668,
      "grad_norm": 0.28338822722435,
      "learning_rate": 4.229333333333334e-05,
      "loss": 0.0025,
      "step": 23120
    },
    {
      "epoch": 1.2336,
      "grad_norm": 0.07989871501922607,
      "learning_rate": 4.229e-05,
      "loss": 0.0025,
      "step": 23130
    },
    {
      "epoch": 1.2341333333333333,
      "grad_norm": 0.2886275053024292,
      "learning_rate": 4.228666666666667e-05,
      "loss": 0.0029,
      "step": 23140
    },
    {
      "epoch": 1.2346666666666666,
      "grad_norm": 0.22109681367874146,
      "learning_rate": 4.2283333333333336e-05,
      "loss": 0.0036,
      "step": 23150
    },
    {
      "epoch": 1.2352,
      "grad_norm": 0.32016733288764954,
      "learning_rate": 4.228e-05,
      "loss": 0.0021,
      "step": 23160
    },
    {
      "epoch": 1.2357333333333334,
      "grad_norm": 0.12848667800426483,
      "learning_rate": 4.227666666666667e-05,
      "loss": 0.0035,
      "step": 23170
    },
    {
      "epoch": 1.2362666666666666,
      "grad_norm": 0.3772551119327545,
      "learning_rate": 4.2273333333333334e-05,
      "loss": 0.0022,
      "step": 23180
    },
    {
      "epoch": 1.2368000000000001,
      "grad_norm": 0.09329389035701752,
      "learning_rate": 4.227000000000001e-05,
      "loss": 0.004,
      "step": 23190
    },
    {
      "epoch": 1.2373333333333334,
      "grad_norm": 0.061360858380794525,
      "learning_rate": 4.226666666666667e-05,
      "loss": 0.0045,
      "step": 23200
    },
    {
      "epoch": 1.2378666666666667,
      "grad_norm": 0.1879032552242279,
      "learning_rate": 4.226333333333334e-05,
      "loss": 0.0035,
      "step": 23210
    },
    {
      "epoch": 1.2384,
      "grad_norm": 0.04456107318401337,
      "learning_rate": 4.226e-05,
      "loss": 0.0038,
      "step": 23220
    },
    {
      "epoch": 1.2389333333333332,
      "grad_norm": 0.25057610869407654,
      "learning_rate": 4.2256666666666665e-05,
      "loss": 0.0021,
      "step": 23230
    },
    {
      "epoch": 1.2394666666666667,
      "grad_norm": 0.4043236970901489,
      "learning_rate": 4.225333333333333e-05,
      "loss": 0.0025,
      "step": 23240
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.43342071771621704,
      "learning_rate": 4.2250000000000004e-05,
      "loss": 0.0034,
      "step": 23250
    },
    {
      "epoch": 1.2405333333333333,
      "grad_norm": 0.09350255131721497,
      "learning_rate": 4.224666666666667e-05,
      "loss": 0.004,
      "step": 23260
    },
    {
      "epoch": 1.2410666666666668,
      "grad_norm": 0.4283146560192108,
      "learning_rate": 4.2243333333333336e-05,
      "loss": 0.0024,
      "step": 23270
    },
    {
      "epoch": 1.2416,
      "grad_norm": 0.38095030188560486,
      "learning_rate": 4.224e-05,
      "loss": 0.0034,
      "step": 23280
    },
    {
      "epoch": 1.2421333333333333,
      "grad_norm": 0.791388750076294,
      "learning_rate": 4.223666666666667e-05,
      "loss": 0.0036,
      "step": 23290
    },
    {
      "epoch": 1.2426666666666666,
      "grad_norm": 0.3404388129711151,
      "learning_rate": 4.2233333333333334e-05,
      "loss": 0.0035,
      "step": 23300
    },
    {
      "epoch": 1.2432,
      "grad_norm": 0.2456217110157013,
      "learning_rate": 4.223e-05,
      "loss": 0.0034,
      "step": 23310
    },
    {
      "epoch": 1.2437333333333334,
      "grad_norm": 0.10484365373849869,
      "learning_rate": 4.222666666666667e-05,
      "loss": 0.0041,
      "step": 23320
    },
    {
      "epoch": 1.2442666666666666,
      "grad_norm": 0.07540454715490341,
      "learning_rate": 4.222333333333334e-05,
      "loss": 0.0041,
      "step": 23330
    },
    {
      "epoch": 1.2448,
      "grad_norm": 0.16575521230697632,
      "learning_rate": 4.2220000000000006e-05,
      "loss": 0.0028,
      "step": 23340
    },
    {
      "epoch": 1.2453333333333334,
      "grad_norm": 0.1276542991399765,
      "learning_rate": 4.221666666666667e-05,
      "loss": 0.0033,
      "step": 23350
    },
    {
      "epoch": 1.2458666666666667,
      "grad_norm": 0.40048277378082275,
      "learning_rate": 4.221333333333334e-05,
      "loss": 0.0022,
      "step": 23360
    },
    {
      "epoch": 1.2464,
      "grad_norm": 0.04449101909995079,
      "learning_rate": 4.221e-05,
      "loss": 0.0031,
      "step": 23370
    },
    {
      "epoch": 1.2469333333333332,
      "grad_norm": 0.10547381639480591,
      "learning_rate": 4.2206666666666663e-05,
      "loss": 0.0026,
      "step": 23380
    },
    {
      "epoch": 1.2474666666666667,
      "grad_norm": 0.8859235644340515,
      "learning_rate": 4.2203333333333336e-05,
      "loss": 0.0023,
      "step": 23390
    },
    {
      "epoch": 1.248,
      "grad_norm": 0.42180588841438293,
      "learning_rate": 4.22e-05,
      "loss": 0.0037,
      "step": 23400
    },
    {
      "epoch": 1.2485333333333333,
      "grad_norm": 0.47399720549583435,
      "learning_rate": 4.219666666666667e-05,
      "loss": 0.0032,
      "step": 23410
    },
    {
      "epoch": 1.2490666666666668,
      "grad_norm": 0.13525499403476715,
      "learning_rate": 4.2193333333333335e-05,
      "loss": 0.0029,
      "step": 23420
    },
    {
      "epoch": 1.2496,
      "grad_norm": 0.15462027490139008,
      "learning_rate": 4.219e-05,
      "loss": 0.0017,
      "step": 23430
    },
    {
      "epoch": 1.2501333333333333,
      "grad_norm": 0.14184121787548065,
      "learning_rate": 4.218666666666667e-05,
      "loss": 0.0027,
      "step": 23440
    },
    {
      "epoch": 1.2506666666666666,
      "grad_norm": 0.1698225438594818,
      "learning_rate": 4.218333333333333e-05,
      "loss": 0.0043,
      "step": 23450
    },
    {
      "epoch": 1.2511999999999999,
      "grad_norm": 0.0460423044860363,
      "learning_rate": 4.2180000000000006e-05,
      "loss": 0.0037,
      "step": 23460
    },
    {
      "epoch": 1.2517333333333334,
      "grad_norm": 0.40284958481788635,
      "learning_rate": 4.217666666666667e-05,
      "loss": 0.0039,
      "step": 23470
    },
    {
      "epoch": 1.2522666666666666,
      "grad_norm": 0.34029456973075867,
      "learning_rate": 4.217333333333334e-05,
      "loss": 0.0028,
      "step": 23480
    },
    {
      "epoch": 1.2528000000000001,
      "grad_norm": 0.1621285229921341,
      "learning_rate": 4.2170000000000005e-05,
      "loss": 0.0029,
      "step": 23490
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 0.133097305893898,
      "learning_rate": 4.216666666666667e-05,
      "loss": 0.0024,
      "step": 23500
    },
    {
      "epoch": 1.2538666666666667,
      "grad_norm": 0.06877102702856064,
      "learning_rate": 4.216333333333334e-05,
      "loss": 0.0032,
      "step": 23510
    },
    {
      "epoch": 1.2544,
      "grad_norm": 0.4938315749168396,
      "learning_rate": 4.2159999999999996e-05,
      "loss": 0.0028,
      "step": 23520
    },
    {
      "epoch": 1.2549333333333332,
      "grad_norm": 0.7094234824180603,
      "learning_rate": 4.215666666666667e-05,
      "loss": 0.0033,
      "step": 23530
    },
    {
      "epoch": 1.2554666666666667,
      "grad_norm": 0.5528478622436523,
      "learning_rate": 4.2153333333333335e-05,
      "loss": 0.0031,
      "step": 23540
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.43259578943252563,
      "learning_rate": 4.215e-05,
      "loss": 0.0029,
      "step": 23550
    },
    {
      "epoch": 1.2565333333333333,
      "grad_norm": 0.15996408462524414,
      "learning_rate": 4.214666666666667e-05,
      "loss": 0.0036,
      "step": 23560
    },
    {
      "epoch": 1.2570666666666668,
      "grad_norm": 0.08190853148698807,
      "learning_rate": 4.2143333333333334e-05,
      "loss": 0.0043,
      "step": 23570
    },
    {
      "epoch": 1.2576,
      "grad_norm": 0.10789518803358078,
      "learning_rate": 4.214e-05,
      "loss": 0.0037,
      "step": 23580
    },
    {
      "epoch": 1.2581333333333333,
      "grad_norm": 0.8931979537010193,
      "learning_rate": 4.2136666666666666e-05,
      "loss": 0.0034,
      "step": 23590
    },
    {
      "epoch": 1.2586666666666666,
      "grad_norm": 0.8943725228309631,
      "learning_rate": 4.213333333333334e-05,
      "loss": 0.0041,
      "step": 23600
    },
    {
      "epoch": 1.2591999999999999,
      "grad_norm": 0.06857367604970932,
      "learning_rate": 4.2130000000000005e-05,
      "loss": 0.0037,
      "step": 23610
    },
    {
      "epoch": 1.2597333333333334,
      "grad_norm": 0.1911422461271286,
      "learning_rate": 4.212666666666667e-05,
      "loss": 0.0023,
      "step": 23620
    },
    {
      "epoch": 1.2602666666666666,
      "grad_norm": 0.4047503173351288,
      "learning_rate": 4.212333333333334e-05,
      "loss": 0.0027,
      "step": 23630
    },
    {
      "epoch": 1.2608,
      "grad_norm": 0.2161945402622223,
      "learning_rate": 4.212e-05,
      "loss": 0.0029,
      "step": 23640
    },
    {
      "epoch": 1.2613333333333334,
      "grad_norm": 0.3691074252128601,
      "learning_rate": 4.211666666666667e-05,
      "loss": 0.0029,
      "step": 23650
    },
    {
      "epoch": 1.2618666666666667,
      "grad_norm": 0.24516648054122925,
      "learning_rate": 4.2113333333333336e-05,
      "loss": 0.0044,
      "step": 23660
    },
    {
      "epoch": 1.2624,
      "grad_norm": 0.06497295200824738,
      "learning_rate": 4.211e-05,
      "loss": 0.0033,
      "step": 23670
    },
    {
      "epoch": 1.2629333333333332,
      "grad_norm": 0.1652476191520691,
      "learning_rate": 4.210666666666667e-05,
      "loss": 0.0028,
      "step": 23680
    },
    {
      "epoch": 1.2634666666666667,
      "grad_norm": 0.45983827114105225,
      "learning_rate": 4.2103333333333334e-05,
      "loss": 0.0043,
      "step": 23690
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.09592880308628082,
      "learning_rate": 4.21e-05,
      "loss": 0.0028,
      "step": 23700
    },
    {
      "epoch": 1.2645333333333333,
      "grad_norm": 0.33448517322540283,
      "learning_rate": 4.2096666666666666e-05,
      "loss": 0.0039,
      "step": 23710
    },
    {
      "epoch": 1.2650666666666668,
      "grad_norm": 0.1059333086013794,
      "learning_rate": 4.209333333333333e-05,
      "loss": 0.0032,
      "step": 23720
    },
    {
      "epoch": 1.2656,
      "grad_norm": 0.2458978295326233,
      "learning_rate": 4.209e-05,
      "loss": 0.0035,
      "step": 23730
    },
    {
      "epoch": 1.2661333333333333,
      "grad_norm": 0.06760294735431671,
      "learning_rate": 4.208666666666667e-05,
      "loss": 0.0029,
      "step": 23740
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.14657683670520782,
      "learning_rate": 4.208333333333334e-05,
      "loss": 0.0029,
      "step": 23750
    },
    {
      "epoch": 1.2671999999999999,
      "grad_norm": 0.11218342185020447,
      "learning_rate": 4.2080000000000004e-05,
      "loss": 0.0017,
      "step": 23760
    },
    {
      "epoch": 1.2677333333333334,
      "grad_norm": 0.2143360674381256,
      "learning_rate": 4.207666666666667e-05,
      "loss": 0.0019,
      "step": 23770
    },
    {
      "epoch": 1.2682666666666667,
      "grad_norm": 0.2666754722595215,
      "learning_rate": 4.2073333333333336e-05,
      "loss": 0.0035,
      "step": 23780
    },
    {
      "epoch": 1.2688,
      "grad_norm": 0.18603259325027466,
      "learning_rate": 4.207e-05,
      "loss": 0.0033,
      "step": 23790
    },
    {
      "epoch": 1.2693333333333334,
      "grad_norm": 0.3115672171115875,
      "learning_rate": 4.206666666666667e-05,
      "loss": 0.0022,
      "step": 23800
    },
    {
      "epoch": 1.2698666666666667,
      "grad_norm": 0.2899734377861023,
      "learning_rate": 4.206333333333334e-05,
      "loss": 0.0026,
      "step": 23810
    },
    {
      "epoch": 1.2704,
      "grad_norm": 0.1659558266401291,
      "learning_rate": 4.206e-05,
      "loss": 0.0024,
      "step": 23820
    },
    {
      "epoch": 1.2709333333333332,
      "grad_norm": 0.40133586525917053,
      "learning_rate": 4.205666666666667e-05,
      "loss": 0.0025,
      "step": 23830
    },
    {
      "epoch": 1.2714666666666667,
      "grad_norm": 0.38109534978866577,
      "learning_rate": 4.205333333333333e-05,
      "loss": 0.0029,
      "step": 23840
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.12822756171226501,
      "learning_rate": 4.205e-05,
      "loss": 0.0031,
      "step": 23850
    },
    {
      "epoch": 1.2725333333333333,
      "grad_norm": 0.5819729566574097,
      "learning_rate": 4.2046666666666665e-05,
      "loss": 0.0022,
      "step": 23860
    },
    {
      "epoch": 1.2730666666666668,
      "grad_norm": 0.05753891542553902,
      "learning_rate": 4.204333333333334e-05,
      "loss": 0.0026,
      "step": 23870
    },
    {
      "epoch": 1.2736,
      "grad_norm": 0.17984308302402496,
      "learning_rate": 4.2040000000000004e-05,
      "loss": 0.003,
      "step": 23880
    },
    {
      "epoch": 1.2741333333333333,
      "grad_norm": 0.1928400844335556,
      "learning_rate": 4.203666666666667e-05,
      "loss": 0.002,
      "step": 23890
    },
    {
      "epoch": 1.2746666666666666,
      "grad_norm": 0.6190452575683594,
      "learning_rate": 4.2033333333333336e-05,
      "loss": 0.0038,
      "step": 23900
    },
    {
      "epoch": 1.2752,
      "grad_norm": 0.5589066743850708,
      "learning_rate": 4.203e-05,
      "loss": 0.0035,
      "step": 23910
    },
    {
      "epoch": 1.2757333333333334,
      "grad_norm": 0.3415296971797943,
      "learning_rate": 4.202666666666667e-05,
      "loss": 0.002,
      "step": 23920
    },
    {
      "epoch": 1.2762666666666667,
      "grad_norm": 0.07396619021892548,
      "learning_rate": 4.2023333333333335e-05,
      "loss": 0.003,
      "step": 23930
    },
    {
      "epoch": 1.2768,
      "grad_norm": 0.37017887830734253,
      "learning_rate": 4.202e-05,
      "loss": 0.0024,
      "step": 23940
    },
    {
      "epoch": 1.2773333333333334,
      "grad_norm": 0.040207695215940475,
      "learning_rate": 4.2016666666666674e-05,
      "loss": 0.0042,
      "step": 23950
    },
    {
      "epoch": 1.2778666666666667,
      "grad_norm": 0.15593545138835907,
      "learning_rate": 4.201333333333334e-05,
      "loss": 0.0034,
      "step": 23960
    },
    {
      "epoch": 1.2784,
      "grad_norm": 0.19293084740638733,
      "learning_rate": 4.201e-05,
      "loss": 0.0034,
      "step": 23970
    },
    {
      "epoch": 1.2789333333333333,
      "grad_norm": 0.257193386554718,
      "learning_rate": 4.2006666666666665e-05,
      "loss": 0.0026,
      "step": 23980
    },
    {
      "epoch": 1.2794666666666665,
      "grad_norm": 0.5519284009933472,
      "learning_rate": 4.200333333333333e-05,
      "loss": 0.0025,
      "step": 23990
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.5920257568359375,
      "learning_rate": 4.2e-05,
      "loss": 0.0034,
      "step": 24000
    },
    {
      "epoch": 1.2805333333333333,
      "grad_norm": 0.2761366665363312,
      "learning_rate": 4.199666666666667e-05,
      "loss": 0.0032,
      "step": 24010
    },
    {
      "epoch": 1.2810666666666668,
      "grad_norm": 0.19097164273262024,
      "learning_rate": 4.199333333333334e-05,
      "loss": 0.003,
      "step": 24020
    },
    {
      "epoch": 1.2816,
      "grad_norm": 0.10477679967880249,
      "learning_rate": 4.199e-05,
      "loss": 0.0029,
      "step": 24030
    },
    {
      "epoch": 1.2821333333333333,
      "grad_norm": 0.1959560215473175,
      "learning_rate": 4.198666666666667e-05,
      "loss": 0.002,
      "step": 24040
    },
    {
      "epoch": 1.2826666666666666,
      "grad_norm": 0.07274018973112106,
      "learning_rate": 4.1983333333333335e-05,
      "loss": 0.0021,
      "step": 24050
    },
    {
      "epoch": 1.2832,
      "grad_norm": 0.34038805961608887,
      "learning_rate": 4.198e-05,
      "loss": 0.0021,
      "step": 24060
    },
    {
      "epoch": 1.2837333333333334,
      "grad_norm": 0.027114035561680794,
      "learning_rate": 4.197666666666667e-05,
      "loss": 0.0039,
      "step": 24070
    },
    {
      "epoch": 1.2842666666666667,
      "grad_norm": 0.4291885197162628,
      "learning_rate": 4.1973333333333334e-05,
      "loss": 0.0026,
      "step": 24080
    },
    {
      "epoch": 1.2848,
      "grad_norm": 0.1897224485874176,
      "learning_rate": 4.1970000000000006e-05,
      "loss": 0.0029,
      "step": 24090
    },
    {
      "epoch": 1.2853333333333334,
      "grad_norm": 0.2767660915851593,
      "learning_rate": 4.196666666666667e-05,
      "loss": 0.0037,
      "step": 24100
    },
    {
      "epoch": 1.2858666666666667,
      "grad_norm": 0.2748540937900543,
      "learning_rate": 4.196333333333334e-05,
      "loss": 0.0025,
      "step": 24110
    },
    {
      "epoch": 1.2864,
      "grad_norm": 0.38267987966537476,
      "learning_rate": 4.196e-05,
      "loss": 0.0031,
      "step": 24120
    },
    {
      "epoch": 1.2869333333333333,
      "grad_norm": 0.39841148257255554,
      "learning_rate": 4.1956666666666664e-05,
      "loss": 0.0034,
      "step": 24130
    },
    {
      "epoch": 1.2874666666666665,
      "grad_norm": 0.8007562160491943,
      "learning_rate": 4.195333333333333e-05,
      "loss": 0.0019,
      "step": 24140
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.2513240575790405,
      "learning_rate": 4.195e-05,
      "loss": 0.0037,
      "step": 24150
    },
    {
      "epoch": 1.2885333333333333,
      "grad_norm": 0.12378456443548203,
      "learning_rate": 4.194666666666667e-05,
      "loss": 0.0016,
      "step": 24160
    },
    {
      "epoch": 1.2890666666666668,
      "grad_norm": 0.40933823585510254,
      "learning_rate": 4.1943333333333336e-05,
      "loss": 0.0023,
      "step": 24170
    },
    {
      "epoch": 1.2896,
      "grad_norm": 0.24766001105308533,
      "learning_rate": 4.194e-05,
      "loss": 0.004,
      "step": 24180
    },
    {
      "epoch": 1.2901333333333334,
      "grad_norm": 0.12547969818115234,
      "learning_rate": 4.193666666666667e-05,
      "loss": 0.0035,
      "step": 24190
    },
    {
      "epoch": 1.2906666666666666,
      "grad_norm": 0.3055196702480316,
      "learning_rate": 4.1933333333333334e-05,
      "loss": 0.0044,
      "step": 24200
    },
    {
      "epoch": 1.2912,
      "grad_norm": 0.3348792791366577,
      "learning_rate": 4.193e-05,
      "loss": 0.0023,
      "step": 24210
    },
    {
      "epoch": 1.2917333333333334,
      "grad_norm": 0.3466421067714691,
      "learning_rate": 4.192666666666667e-05,
      "loss": 0.0027,
      "step": 24220
    },
    {
      "epoch": 1.2922666666666667,
      "grad_norm": 0.16420120000839233,
      "learning_rate": 4.192333333333334e-05,
      "loss": 0.0035,
      "step": 24230
    },
    {
      "epoch": 1.2928,
      "grad_norm": 0.5266749262809753,
      "learning_rate": 4.1920000000000005e-05,
      "loss": 0.0033,
      "step": 24240
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 0.45323264598846436,
      "learning_rate": 4.191666666666667e-05,
      "loss": 0.0041,
      "step": 24250
    },
    {
      "epoch": 1.2938666666666667,
      "grad_norm": 0.4318978488445282,
      "learning_rate": 4.191333333333334e-05,
      "loss": 0.0039,
      "step": 24260
    },
    {
      "epoch": 1.2944,
      "grad_norm": 0.26618972420692444,
      "learning_rate": 4.191e-05,
      "loss": 0.0036,
      "step": 24270
    },
    {
      "epoch": 1.2949333333333333,
      "grad_norm": 0.21671158075332642,
      "learning_rate": 4.190666666666666e-05,
      "loss": 0.0031,
      "step": 24280
    },
    {
      "epoch": 1.2954666666666665,
      "grad_norm": 0.38062652945518494,
      "learning_rate": 4.1903333333333336e-05,
      "loss": 0.0029,
      "step": 24290
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.43244826793670654,
      "learning_rate": 4.19e-05,
      "loss": 0.003,
      "step": 24300
    },
    {
      "epoch": 1.2965333333333333,
      "grad_norm": 0.18647615611553192,
      "learning_rate": 4.189666666666667e-05,
      "loss": 0.0033,
      "step": 24310
    },
    {
      "epoch": 1.2970666666666666,
      "grad_norm": 0.10459782183170319,
      "learning_rate": 4.1893333333333334e-05,
      "loss": 0.0037,
      "step": 24320
    },
    {
      "epoch": 1.2976,
      "grad_norm": 0.2891964614391327,
      "learning_rate": 4.189e-05,
      "loss": 0.0036,
      "step": 24330
    },
    {
      "epoch": 1.2981333333333334,
      "grad_norm": 0.40514957904815674,
      "learning_rate": 4.1886666666666667e-05,
      "loss": 0.0033,
      "step": 24340
    },
    {
      "epoch": 1.2986666666666666,
      "grad_norm": 0.3046868145465851,
      "learning_rate": 4.188333333333333e-05,
      "loss": 0.0031,
      "step": 24350
    },
    {
      "epoch": 1.2992,
      "grad_norm": 0.24992558360099792,
      "learning_rate": 4.1880000000000006e-05,
      "loss": 0.0034,
      "step": 24360
    },
    {
      "epoch": 1.2997333333333334,
      "grad_norm": 0.28105393052101135,
      "learning_rate": 4.187666666666667e-05,
      "loss": 0.0047,
      "step": 24370
    },
    {
      "epoch": 1.3002666666666667,
      "grad_norm": 0.42874789237976074,
      "learning_rate": 4.187333333333334e-05,
      "loss": 0.0028,
      "step": 24380
    },
    {
      "epoch": 1.3008,
      "grad_norm": 0.08229374140501022,
      "learning_rate": 4.1870000000000004e-05,
      "loss": 0.005,
      "step": 24390
    },
    {
      "epoch": 1.3013333333333335,
      "grad_norm": 0.24868692457675934,
      "learning_rate": 4.186666666666667e-05,
      "loss": 0.0032,
      "step": 24400
    },
    {
      "epoch": 1.3018666666666667,
      "grad_norm": 0.07173841446638107,
      "learning_rate": 4.1863333333333336e-05,
      "loss": 0.0033,
      "step": 24410
    },
    {
      "epoch": 1.3024,
      "grad_norm": 0.18379507958889008,
      "learning_rate": 4.186e-05,
      "loss": 0.0032,
      "step": 24420
    },
    {
      "epoch": 1.3029333333333333,
      "grad_norm": 0.055003948509693146,
      "learning_rate": 4.185666666666667e-05,
      "loss": 0.0034,
      "step": 24430
    },
    {
      "epoch": 1.3034666666666666,
      "grad_norm": 0.2491123229265213,
      "learning_rate": 4.1853333333333335e-05,
      "loss": 0.0039,
      "step": 24440
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.21610593795776367,
      "learning_rate": 4.185e-05,
      "loss": 0.0035,
      "step": 24450
    },
    {
      "epoch": 1.3045333333333333,
      "grad_norm": 0.12754613161087036,
      "learning_rate": 4.184666666666667e-05,
      "loss": 0.0029,
      "step": 24460
    },
    {
      "epoch": 1.3050666666666666,
      "grad_norm": 0.04286873713135719,
      "learning_rate": 4.184333333333333e-05,
      "loss": 0.0037,
      "step": 24470
    },
    {
      "epoch": 1.3056,
      "grad_norm": 0.05073963850736618,
      "learning_rate": 4.184e-05,
      "loss": 0.0025,
      "step": 24480
    },
    {
      "epoch": 1.3061333333333334,
      "grad_norm": 0.4040003716945648,
      "learning_rate": 4.1836666666666665e-05,
      "loss": 0.003,
      "step": 24490
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 0.5199592113494873,
      "learning_rate": 4.183333333333334e-05,
      "loss": 0.0026,
      "step": 24500
    },
    {
      "epoch": 1.3072,
      "grad_norm": 0.5543025135993958,
      "learning_rate": 4.1830000000000004e-05,
      "loss": 0.0023,
      "step": 24510
    },
    {
      "epoch": 1.3077333333333334,
      "grad_norm": 0.37236467003822327,
      "learning_rate": 4.182666666666667e-05,
      "loss": 0.0023,
      "step": 24520
    },
    {
      "epoch": 1.3082666666666667,
      "grad_norm": 0.05487271025776863,
      "learning_rate": 4.182333333333334e-05,
      "loss": 0.0042,
      "step": 24530
    },
    {
      "epoch": 1.3088,
      "grad_norm": 0.24580802023410797,
      "learning_rate": 4.182e-05,
      "loss": 0.0027,
      "step": 24540
    },
    {
      "epoch": 1.3093333333333335,
      "grad_norm": 0.40554937720298767,
      "learning_rate": 4.181666666666667e-05,
      "loss": 0.0034,
      "step": 24550
    },
    {
      "epoch": 1.3098666666666667,
      "grad_norm": 0.27433231472969055,
      "learning_rate": 4.1813333333333335e-05,
      "loss": 0.0034,
      "step": 24560
    },
    {
      "epoch": 1.3104,
      "grad_norm": 0.21582458913326263,
      "learning_rate": 4.181000000000001e-05,
      "loss": 0.0028,
      "step": 24570
    },
    {
      "epoch": 1.3109333333333333,
      "grad_norm": 0.4010004699230194,
      "learning_rate": 4.180666666666667e-05,
      "loss": 0.0023,
      "step": 24580
    },
    {
      "epoch": 1.3114666666666666,
      "grad_norm": 0.16188904643058777,
      "learning_rate": 4.1803333333333333e-05,
      "loss": 0.0041,
      "step": 24590
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.24841095507144928,
      "learning_rate": 4.18e-05,
      "loss": 0.0025,
      "step": 24600
    },
    {
      "epoch": 1.3125333333333333,
      "grad_norm": 0.31427615880966187,
      "learning_rate": 4.1796666666666666e-05,
      "loss": 0.0033,
      "step": 24610
    },
    {
      "epoch": 1.3130666666666666,
      "grad_norm": 0.4950694441795349,
      "learning_rate": 4.179333333333333e-05,
      "loss": 0.0026,
      "step": 24620
    },
    {
      "epoch": 1.3136,
      "grad_norm": 0.5035046339035034,
      "learning_rate": 4.179e-05,
      "loss": 0.0025,
      "step": 24630
    },
    {
      "epoch": 1.3141333333333334,
      "grad_norm": 0.22431258857250214,
      "learning_rate": 4.178666666666667e-05,
      "loss": 0.0029,
      "step": 24640
    },
    {
      "epoch": 1.3146666666666667,
      "grad_norm": 0.16351185739040375,
      "learning_rate": 4.178333333333334e-05,
      "loss": 0.003,
      "step": 24650
    },
    {
      "epoch": 1.3152,
      "grad_norm": 0.397960364818573,
      "learning_rate": 4.178e-05,
      "loss": 0.0027,
      "step": 24660
    },
    {
      "epoch": 1.3157333333333332,
      "grad_norm": 0.08939530700445175,
      "learning_rate": 4.177666666666667e-05,
      "loss": 0.003,
      "step": 24670
    },
    {
      "epoch": 1.3162666666666667,
      "grad_norm": 0.7095433473587036,
      "learning_rate": 4.1773333333333335e-05,
      "loss": 0.0019,
      "step": 24680
    },
    {
      "epoch": 1.3168,
      "grad_norm": 0.5004925727844238,
      "learning_rate": 4.177e-05,
      "loss": 0.0036,
      "step": 24690
    },
    {
      "epoch": 1.3173333333333335,
      "grad_norm": 0.21657609939575195,
      "learning_rate": 4.176666666666667e-05,
      "loss": 0.0032,
      "step": 24700
    },
    {
      "epoch": 1.3178666666666667,
      "grad_norm": 0.09635305404663086,
      "learning_rate": 4.176333333333334e-05,
      "loss": 0.0033,
      "step": 24710
    },
    {
      "epoch": 1.3184,
      "grad_norm": 0.1885649561882019,
      "learning_rate": 4.176000000000001e-05,
      "loss": 0.004,
      "step": 24720
    },
    {
      "epoch": 1.3189333333333333,
      "grad_norm": 0.617540180683136,
      "learning_rate": 4.1756666666666666e-05,
      "loss": 0.0026,
      "step": 24730
    },
    {
      "epoch": 1.3194666666666666,
      "grad_norm": 0.40920114517211914,
      "learning_rate": 4.175333333333333e-05,
      "loss": 0.0033,
      "step": 24740
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.30395135283470154,
      "learning_rate": 4.175e-05,
      "loss": 0.0023,
      "step": 24750
    },
    {
      "epoch": 1.3205333333333333,
      "grad_norm": 0.33678358793258667,
      "learning_rate": 4.1746666666666665e-05,
      "loss": 0.0029,
      "step": 24760
    },
    {
      "epoch": 1.3210666666666666,
      "grad_norm": 0.440666526556015,
      "learning_rate": 4.174333333333334e-05,
      "loss": 0.0039,
      "step": 24770
    },
    {
      "epoch": 1.3216,
      "grad_norm": 0.8049057722091675,
      "learning_rate": 4.1740000000000004e-05,
      "loss": 0.0032,
      "step": 24780
    },
    {
      "epoch": 1.3221333333333334,
      "grad_norm": 0.6823384761810303,
      "learning_rate": 4.173666666666667e-05,
      "loss": 0.0031,
      "step": 24790
    },
    {
      "epoch": 1.3226666666666667,
      "grad_norm": 0.12405835837125778,
      "learning_rate": 4.1733333333333336e-05,
      "loss": 0.0028,
      "step": 24800
    },
    {
      "epoch": 1.3232,
      "grad_norm": 0.06328734755516052,
      "learning_rate": 4.173e-05,
      "loss": 0.0035,
      "step": 24810
    },
    {
      "epoch": 1.3237333333333332,
      "grad_norm": 0.6455551385879517,
      "learning_rate": 4.172666666666667e-05,
      "loss": 0.0048,
      "step": 24820
    },
    {
      "epoch": 1.3242666666666667,
      "grad_norm": 0.2813115119934082,
      "learning_rate": 4.1723333333333334e-05,
      "loss": 0.0025,
      "step": 24830
    },
    {
      "epoch": 1.3248,
      "grad_norm": 0.21458232402801514,
      "learning_rate": 4.172e-05,
      "loss": 0.003,
      "step": 24840
    },
    {
      "epoch": 1.3253333333333333,
      "grad_norm": 0.3133639693260193,
      "learning_rate": 4.171666666666667e-05,
      "loss": 0.0025,
      "step": 24850
    },
    {
      "epoch": 1.3258666666666667,
      "grad_norm": 0.42995506525039673,
      "learning_rate": 4.171333333333334e-05,
      "loss": 0.0026,
      "step": 24860
    },
    {
      "epoch": 1.3264,
      "grad_norm": 0.18789230287075043,
      "learning_rate": 4.1710000000000006e-05,
      "loss": 0.0032,
      "step": 24870
    },
    {
      "epoch": 1.3269333333333333,
      "grad_norm": 0.15106096863746643,
      "learning_rate": 4.1706666666666665e-05,
      "loss": 0.0026,
      "step": 24880
    },
    {
      "epoch": 1.3274666666666666,
      "grad_norm": 0.2791714668273926,
      "learning_rate": 4.170333333333333e-05,
      "loss": 0.003,
      "step": 24890
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.35283440351486206,
      "learning_rate": 4.17e-05,
      "loss": 0.0036,
      "step": 24900
    },
    {
      "epoch": 1.3285333333333333,
      "grad_norm": 0.04263635352253914,
      "learning_rate": 4.169666666666667e-05,
      "loss": 0.0026,
      "step": 24910
    },
    {
      "epoch": 1.3290666666666666,
      "grad_norm": 0.24468964338302612,
      "learning_rate": 4.1693333333333336e-05,
      "loss": 0.0026,
      "step": 24920
    },
    {
      "epoch": 1.3296000000000001,
      "grad_norm": 0.3714921176433563,
      "learning_rate": 4.169e-05,
      "loss": 0.003,
      "step": 24930
    },
    {
      "epoch": 1.3301333333333334,
      "grad_norm": 0.21669715642929077,
      "learning_rate": 4.168666666666667e-05,
      "loss": 0.0028,
      "step": 24940
    },
    {
      "epoch": 1.3306666666666667,
      "grad_norm": 0.06936606764793396,
      "learning_rate": 4.1683333333333335e-05,
      "loss": 0.0026,
      "step": 24950
    },
    {
      "epoch": 1.3312,
      "grad_norm": 0.26991721987724304,
      "learning_rate": 4.168e-05,
      "loss": 0.0029,
      "step": 24960
    },
    {
      "epoch": 1.3317333333333332,
      "grad_norm": 0.2750968039035797,
      "learning_rate": 4.167666666666667e-05,
      "loss": 0.0033,
      "step": 24970
    },
    {
      "epoch": 1.3322666666666667,
      "grad_norm": 0.21645191311836243,
      "learning_rate": 4.167333333333334e-05,
      "loss": 0.002,
      "step": 24980
    },
    {
      "epoch": 1.3328,
      "grad_norm": 0.43035486340522766,
      "learning_rate": 4.1670000000000006e-05,
      "loss": 0.0043,
      "step": 24990
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.02633008547127247,
      "learning_rate": 4.166666666666667e-05,
      "loss": 0.0028,
      "step": 25000
    },
    {
      "epoch": 1.3338666666666668,
      "grad_norm": 0.5559951066970825,
      "learning_rate": 4.166333333333334e-05,
      "loss": 0.004,
      "step": 25010
    },
    {
      "epoch": 1.3344,
      "grad_norm": 0.36795854568481445,
      "learning_rate": 4.1660000000000004e-05,
      "loss": 0.0029,
      "step": 25020
    },
    {
      "epoch": 1.3349333333333333,
      "grad_norm": 0.6151572465896606,
      "learning_rate": 4.1656666666666664e-05,
      "loss": 0.0025,
      "step": 25030
    },
    {
      "epoch": 1.3354666666666666,
      "grad_norm": 0.30633217096328735,
      "learning_rate": 4.165333333333333e-05,
      "loss": 0.0047,
      "step": 25040
    },
    {
      "epoch": 1.336,
      "grad_norm": 0.36668264865875244,
      "learning_rate": 4.165e-05,
      "loss": 0.0029,
      "step": 25050
    },
    {
      "epoch": 1.3365333333333334,
      "grad_norm": 0.1847713589668274,
      "learning_rate": 4.164666666666667e-05,
      "loss": 0.0034,
      "step": 25060
    },
    {
      "epoch": 1.3370666666666666,
      "grad_norm": 0.3951870799064636,
      "learning_rate": 4.1643333333333335e-05,
      "loss": 0.0021,
      "step": 25070
    },
    {
      "epoch": 1.3376000000000001,
      "grad_norm": 0.39725401997566223,
      "learning_rate": 4.164e-05,
      "loss": 0.0029,
      "step": 25080
    },
    {
      "epoch": 1.3381333333333334,
      "grad_norm": 0.30477163195610046,
      "learning_rate": 4.163666666666667e-05,
      "loss": 0.0024,
      "step": 25090
    },
    {
      "epoch": 1.3386666666666667,
      "grad_norm": 0.24774764478206635,
      "learning_rate": 4.1633333333333333e-05,
      "loss": 0.0033,
      "step": 25100
    },
    {
      "epoch": 1.3392,
      "grad_norm": 0.08114359527826309,
      "learning_rate": 4.163e-05,
      "loss": 0.0027,
      "step": 25110
    },
    {
      "epoch": 1.3397333333333332,
      "grad_norm": 0.37505459785461426,
      "learning_rate": 4.162666666666667e-05,
      "loss": 0.0035,
      "step": 25120
    },
    {
      "epoch": 1.3402666666666667,
      "grad_norm": 0.5320985913276672,
      "learning_rate": 4.162333333333334e-05,
      "loss": 0.0041,
      "step": 25130
    },
    {
      "epoch": 1.3408,
      "grad_norm": 0.3068748116493225,
      "learning_rate": 4.1620000000000005e-05,
      "loss": 0.0025,
      "step": 25140
    },
    {
      "epoch": 1.3413333333333333,
      "grad_norm": 0.21604736149311066,
      "learning_rate": 4.161666666666667e-05,
      "loss": 0.0033,
      "step": 25150
    },
    {
      "epoch": 1.3418666666666668,
      "grad_norm": 0.24668972194194794,
      "learning_rate": 4.161333333333334e-05,
      "loss": 0.0033,
      "step": 25160
    },
    {
      "epoch": 1.3424,
      "grad_norm": 0.5217235088348389,
      "learning_rate": 4.161e-05,
      "loss": 0.0029,
      "step": 25170
    },
    {
      "epoch": 1.3429333333333333,
      "grad_norm": 0.5008726119995117,
      "learning_rate": 4.160666666666667e-05,
      "loss": 0.0035,
      "step": 25180
    },
    {
      "epoch": 1.3434666666666666,
      "grad_norm": 0.18844619393348694,
      "learning_rate": 4.1603333333333335e-05,
      "loss": 0.0033,
      "step": 25190
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.09833741933107376,
      "learning_rate": 4.16e-05,
      "loss": 0.003,
      "step": 25200
    },
    {
      "epoch": 1.3445333333333334,
      "grad_norm": 0.2948230803012848,
      "learning_rate": 4.159666666666667e-05,
      "loss": 0.0027,
      "step": 25210
    },
    {
      "epoch": 1.3450666666666666,
      "grad_norm": 0.394948273897171,
      "learning_rate": 4.1593333333333334e-05,
      "loss": 0.0029,
      "step": 25220
    },
    {
      "epoch": 1.3456000000000001,
      "grad_norm": 0.24882668256759644,
      "learning_rate": 4.159e-05,
      "loss": 0.0028,
      "step": 25230
    },
    {
      "epoch": 1.3461333333333334,
      "grad_norm": 0.4002765119075775,
      "learning_rate": 4.1586666666666666e-05,
      "loss": 0.0024,
      "step": 25240
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.7669510245323181,
      "learning_rate": 4.158333333333333e-05,
      "loss": 0.0036,
      "step": 25250
    },
    {
      "epoch": 1.3472,
      "grad_norm": 0.3908191919326782,
      "learning_rate": 4.1580000000000005e-05,
      "loss": 0.0035,
      "step": 25260
    },
    {
      "epoch": 1.3477333333333332,
      "grad_norm": 0.46047383546829224,
      "learning_rate": 4.157666666666667e-05,
      "loss": 0.0033,
      "step": 25270
    },
    {
      "epoch": 1.3482666666666667,
      "grad_norm": 0.18585394322872162,
      "learning_rate": 4.157333333333334e-05,
      "loss": 0.0028,
      "step": 25280
    },
    {
      "epoch": 1.3488,
      "grad_norm": 0.15823574364185333,
      "learning_rate": 4.1570000000000003e-05,
      "loss": 0.003,
      "step": 25290
    },
    {
      "epoch": 1.3493333333333333,
      "grad_norm": 0.6108571290969849,
      "learning_rate": 4.156666666666667e-05,
      "loss": 0.0024,
      "step": 25300
    },
    {
      "epoch": 1.3498666666666668,
      "grad_norm": 0.6542538404464722,
      "learning_rate": 4.1563333333333336e-05,
      "loss": 0.0026,
      "step": 25310
    },
    {
      "epoch": 1.3504,
      "grad_norm": 0.39795225858688354,
      "learning_rate": 4.156e-05,
      "loss": 0.004,
      "step": 25320
    },
    {
      "epoch": 1.3509333333333333,
      "grad_norm": 0.39894241094589233,
      "learning_rate": 4.155666666666667e-05,
      "loss": 0.0041,
      "step": 25330
    },
    {
      "epoch": 1.3514666666666666,
      "grad_norm": 0.03454773128032684,
      "learning_rate": 4.1553333333333334e-05,
      "loss": 0.0023,
      "step": 25340
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.640344500541687,
      "learning_rate": 4.155e-05,
      "loss": 0.003,
      "step": 25350
    },
    {
      "epoch": 1.3525333333333334,
      "grad_norm": 0.3373565971851349,
      "learning_rate": 4.1546666666666666e-05,
      "loss": 0.0019,
      "step": 25360
    },
    {
      "epoch": 1.3530666666666666,
      "grad_norm": 0.4054975211620331,
      "learning_rate": 4.154333333333333e-05,
      "loss": 0.0036,
      "step": 25370
    },
    {
      "epoch": 1.3536000000000001,
      "grad_norm": 0.2362578809261322,
      "learning_rate": 4.154e-05,
      "loss": 0.0026,
      "step": 25380
    },
    {
      "epoch": 1.3541333333333334,
      "grad_norm": 0.10379859060049057,
      "learning_rate": 4.1536666666666665e-05,
      "loss": 0.0022,
      "step": 25390
    },
    {
      "epoch": 1.3546666666666667,
      "grad_norm": 0.4333919882774353,
      "learning_rate": 4.153333333333334e-05,
      "loss": 0.002,
      "step": 25400
    },
    {
      "epoch": 1.3552,
      "grad_norm": 0.25368160009384155,
      "learning_rate": 4.1530000000000004e-05,
      "loss": 0.0026,
      "step": 25410
    },
    {
      "epoch": 1.3557333333333332,
      "grad_norm": 0.45784422755241394,
      "learning_rate": 4.152666666666667e-05,
      "loss": 0.0026,
      "step": 25420
    },
    {
      "epoch": 1.3562666666666667,
      "grad_norm": 0.47994476556777954,
      "learning_rate": 4.1523333333333336e-05,
      "loss": 0.0028,
      "step": 25430
    },
    {
      "epoch": 1.3568,
      "grad_norm": 0.5592027306556702,
      "learning_rate": 4.152e-05,
      "loss": 0.002,
      "step": 25440
    },
    {
      "epoch": 1.3573333333333333,
      "grad_norm": 0.6165297627449036,
      "learning_rate": 4.151666666666667e-05,
      "loss": 0.0026,
      "step": 25450
    },
    {
      "epoch": 1.3578666666666668,
      "grad_norm": 0.2198054939508438,
      "learning_rate": 4.1513333333333335e-05,
      "loss": 0.0042,
      "step": 25460
    },
    {
      "epoch": 1.3584,
      "grad_norm": 0.252280592918396,
      "learning_rate": 4.151000000000001e-05,
      "loss": 0.0031,
      "step": 25470
    },
    {
      "epoch": 1.3589333333333333,
      "grad_norm": 0.15553005039691925,
      "learning_rate": 4.150666666666667e-05,
      "loss": 0.0035,
      "step": 25480
    },
    {
      "epoch": 1.3594666666666666,
      "grad_norm": 0.4295996129512787,
      "learning_rate": 4.150333333333333e-05,
      "loss": 0.0019,
      "step": 25490
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.35041701793670654,
      "learning_rate": 4.15e-05,
      "loss": 0.0026,
      "step": 25500
    },
    {
      "epoch": 1.3605333333333334,
      "grad_norm": 0.21478813886642456,
      "learning_rate": 4.1496666666666665e-05,
      "loss": 0.0023,
      "step": 25510
    },
    {
      "epoch": 1.3610666666666666,
      "grad_norm": 0.4596414268016815,
      "learning_rate": 4.149333333333333e-05,
      "loss": 0.0021,
      "step": 25520
    },
    {
      "epoch": 1.3616,
      "grad_norm": 0.05932365357875824,
      "learning_rate": 4.1490000000000004e-05,
      "loss": 0.0032,
      "step": 25530
    },
    {
      "epoch": 1.3621333333333334,
      "grad_norm": 0.045183490961790085,
      "learning_rate": 4.148666666666667e-05,
      "loss": 0.0035,
      "step": 25540
    },
    {
      "epoch": 1.3626666666666667,
      "grad_norm": 0.39841341972351074,
      "learning_rate": 4.1483333333333337e-05,
      "loss": 0.0022,
      "step": 25550
    },
    {
      "epoch": 1.3632,
      "grad_norm": 0.32226210832595825,
      "learning_rate": 4.148e-05,
      "loss": 0.0033,
      "step": 25560
    },
    {
      "epoch": 1.3637333333333332,
      "grad_norm": 0.18799467384815216,
      "learning_rate": 4.147666666666667e-05,
      "loss": 0.0032,
      "step": 25570
    },
    {
      "epoch": 1.3642666666666667,
      "grad_norm": 0.27501991391181946,
      "learning_rate": 4.1473333333333335e-05,
      "loss": 0.0033,
      "step": 25580
    },
    {
      "epoch": 1.3648,
      "grad_norm": 0.048251647502183914,
      "learning_rate": 4.147e-05,
      "loss": 0.0019,
      "step": 25590
    },
    {
      "epoch": 1.3653333333333333,
      "grad_norm": 0.15687139332294464,
      "learning_rate": 4.146666666666667e-05,
      "loss": 0.0028,
      "step": 25600
    },
    {
      "epoch": 1.3658666666666668,
      "grad_norm": 0.4164354205131531,
      "learning_rate": 4.146333333333334e-05,
      "loss": 0.004,
      "step": 25610
    },
    {
      "epoch": 1.3664,
      "grad_norm": 0.2517610192298889,
      "learning_rate": 4.1460000000000006e-05,
      "loss": 0.0038,
      "step": 25620
    },
    {
      "epoch": 1.3669333333333333,
      "grad_norm": 0.1262274980545044,
      "learning_rate": 4.145666666666667e-05,
      "loss": 0.0037,
      "step": 25630
    },
    {
      "epoch": 1.3674666666666666,
      "grad_norm": 0.40779122710227966,
      "learning_rate": 4.145333333333333e-05,
      "loss": 0.004,
      "step": 25640
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 0.3740043044090271,
      "learning_rate": 4.145e-05,
      "loss": 0.0023,
      "step": 25650
    },
    {
      "epoch": 1.3685333333333334,
      "grad_norm": 0.5514621138572693,
      "learning_rate": 4.1446666666666664e-05,
      "loss": 0.0036,
      "step": 25660
    },
    {
      "epoch": 1.3690666666666667,
      "grad_norm": 0.30778858065605164,
      "learning_rate": 4.144333333333334e-05,
      "loss": 0.0035,
      "step": 25670
    },
    {
      "epoch": 1.3696,
      "grad_norm": 0.2163638174533844,
      "learning_rate": 4.144e-05,
      "loss": 0.0034,
      "step": 25680
    },
    {
      "epoch": 1.3701333333333334,
      "grad_norm": 0.15668073296546936,
      "learning_rate": 4.143666666666667e-05,
      "loss": 0.003,
      "step": 25690
    },
    {
      "epoch": 1.3706666666666667,
      "grad_norm": 0.13192100822925568,
      "learning_rate": 4.1433333333333335e-05,
      "loss": 0.0027,
      "step": 25700
    },
    {
      "epoch": 1.3712,
      "grad_norm": 0.04806428402662277,
      "learning_rate": 4.143e-05,
      "loss": 0.0034,
      "step": 25710
    },
    {
      "epoch": 1.3717333333333332,
      "grad_norm": 0.48740941286087036,
      "learning_rate": 4.142666666666667e-05,
      "loss": 0.0033,
      "step": 25720
    },
    {
      "epoch": 1.3722666666666667,
      "grad_norm": 0.27717286348342896,
      "learning_rate": 4.1423333333333334e-05,
      "loss": 0.0034,
      "step": 25730
    },
    {
      "epoch": 1.3728,
      "grad_norm": 0.3703159987926483,
      "learning_rate": 4.142000000000001e-05,
      "loss": 0.0034,
      "step": 25740
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.3035292625427246,
      "learning_rate": 4.141666666666667e-05,
      "loss": 0.003,
      "step": 25750
    },
    {
      "epoch": 1.3738666666666668,
      "grad_norm": 0.32743197679519653,
      "learning_rate": 4.141333333333334e-05,
      "loss": 0.0028,
      "step": 25760
    },
    {
      "epoch": 1.3744,
      "grad_norm": 0.2502487897872925,
      "learning_rate": 4.1410000000000005e-05,
      "loss": 0.0033,
      "step": 25770
    },
    {
      "epoch": 1.3749333333333333,
      "grad_norm": 0.05179893970489502,
      "learning_rate": 4.140666666666667e-05,
      "loss": 0.0035,
      "step": 25780
    },
    {
      "epoch": 1.3754666666666666,
      "grad_norm": 0.40276777744293213,
      "learning_rate": 4.140333333333333e-05,
      "loss": 0.0028,
      "step": 25790
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.18559560179710388,
      "learning_rate": 4.14e-05,
      "loss": 0.002,
      "step": 25800
    },
    {
      "epoch": 1.3765333333333334,
      "grad_norm": 0.4847675859928131,
      "learning_rate": 4.139666666666667e-05,
      "loss": 0.0022,
      "step": 25810
    },
    {
      "epoch": 1.3770666666666667,
      "grad_norm": 0.06496117264032364,
      "learning_rate": 4.1393333333333336e-05,
      "loss": 0.0026,
      "step": 25820
    },
    {
      "epoch": 1.3776,
      "grad_norm": 0.5342244505882263,
      "learning_rate": 4.139e-05,
      "loss": 0.0041,
      "step": 25830
    },
    {
      "epoch": 1.3781333333333334,
      "grad_norm": 0.4551823139190674,
      "learning_rate": 4.138666666666667e-05,
      "loss": 0.0021,
      "step": 25840
    },
    {
      "epoch": 1.3786666666666667,
      "grad_norm": 0.2958625853061676,
      "learning_rate": 4.1383333333333334e-05,
      "loss": 0.0032,
      "step": 25850
    },
    {
      "epoch": 1.3792,
      "grad_norm": 0.1636202335357666,
      "learning_rate": 4.138e-05,
      "loss": 0.0027,
      "step": 25860
    },
    {
      "epoch": 1.3797333333333333,
      "grad_norm": 0.27705082297325134,
      "learning_rate": 4.1376666666666666e-05,
      "loss": 0.0022,
      "step": 25870
    },
    {
      "epoch": 1.3802666666666665,
      "grad_norm": 0.6435522437095642,
      "learning_rate": 4.137333333333334e-05,
      "loss": 0.0035,
      "step": 25880
    },
    {
      "epoch": 1.3808,
      "grad_norm": 0.3139002025127411,
      "learning_rate": 4.1370000000000005e-05,
      "loss": 0.0022,
      "step": 25890
    },
    {
      "epoch": 1.3813333333333333,
      "grad_norm": 0.1559494584798813,
      "learning_rate": 4.136666666666667e-05,
      "loss": 0.0021,
      "step": 25900
    },
    {
      "epoch": 1.3818666666666668,
      "grad_norm": 0.11157985776662827,
      "learning_rate": 4.136333333333334e-05,
      "loss": 0.0028,
      "step": 25910
    },
    {
      "epoch": 1.3824,
      "grad_norm": 0.16187217831611633,
      "learning_rate": 4.1360000000000004e-05,
      "loss": 0.0033,
      "step": 25920
    },
    {
      "epoch": 1.3829333333333333,
      "grad_norm": 0.1866673082113266,
      "learning_rate": 4.135666666666667e-05,
      "loss": 0.0033,
      "step": 25930
    },
    {
      "epoch": 1.3834666666666666,
      "grad_norm": 0.3652384877204895,
      "learning_rate": 4.1353333333333336e-05,
      "loss": 0.0029,
      "step": 25940
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.3595103621482849,
      "learning_rate": 4.135e-05,
      "loss": 0.0023,
      "step": 25950
    },
    {
      "epoch": 1.3845333333333334,
      "grad_norm": 0.48647281527519226,
      "learning_rate": 4.134666666666667e-05,
      "loss": 0.002,
      "step": 25960
    },
    {
      "epoch": 1.3850666666666667,
      "grad_norm": 0.42848172783851624,
      "learning_rate": 4.1343333333333334e-05,
      "loss": 0.0025,
      "step": 25970
    },
    {
      "epoch": 1.3856,
      "grad_norm": 0.42761266231536865,
      "learning_rate": 4.134e-05,
      "loss": 0.0038,
      "step": 25980
    },
    {
      "epoch": 1.3861333333333334,
      "grad_norm": 0.7955700755119324,
      "learning_rate": 4.133666666666667e-05,
      "loss": 0.0024,
      "step": 25990
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.34152233600616455,
      "learning_rate": 4.133333333333333e-05,
      "loss": 0.0028,
      "step": 26000
    },
    {
      "epoch": 1.3872,
      "grad_norm": 0.08614268153905869,
      "learning_rate": 4.133e-05,
      "loss": 0.0031,
      "step": 26010
    },
    {
      "epoch": 1.3877333333333333,
      "grad_norm": 0.2558351457118988,
      "learning_rate": 4.132666666666667e-05,
      "loss": 0.0026,
      "step": 26020
    },
    {
      "epoch": 1.3882666666666665,
      "grad_norm": 0.24418553709983826,
      "learning_rate": 4.132333333333334e-05,
      "loss": 0.0028,
      "step": 26030
    },
    {
      "epoch": 1.3888,
      "grad_norm": 0.6355573534965515,
      "learning_rate": 4.1320000000000004e-05,
      "loss": 0.0039,
      "step": 26040
    },
    {
      "epoch": 1.3893333333333333,
      "grad_norm": 0.2174595147371292,
      "learning_rate": 4.131666666666667e-05,
      "loss": 0.0035,
      "step": 26050
    },
    {
      "epoch": 1.3898666666666666,
      "grad_norm": 0.21444673836231232,
      "learning_rate": 4.1313333333333336e-05,
      "loss": 0.002,
      "step": 26060
    },
    {
      "epoch": 1.3904,
      "grad_norm": 0.45706263184547424,
      "learning_rate": 4.131e-05,
      "loss": 0.0031,
      "step": 26070
    },
    {
      "epoch": 1.3909333333333334,
      "grad_norm": 0.21313786506652832,
      "learning_rate": 4.130666666666667e-05,
      "loss": 0.0028,
      "step": 26080
    },
    {
      "epoch": 1.3914666666666666,
      "grad_norm": 0.07321727275848389,
      "learning_rate": 4.1303333333333335e-05,
      "loss": 0.0038,
      "step": 26090
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.03667822852730751,
      "learning_rate": 4.13e-05,
      "loss": 0.0033,
      "step": 26100
    },
    {
      "epoch": 1.3925333333333334,
      "grad_norm": 0.12894994020462036,
      "learning_rate": 4.129666666666667e-05,
      "loss": 0.0032,
      "step": 26110
    },
    {
      "epoch": 1.3930666666666667,
      "grad_norm": 0.24449510872364044,
      "learning_rate": 4.129333333333333e-05,
      "loss": 0.0032,
      "step": 26120
    },
    {
      "epoch": 1.3936,
      "grad_norm": 0.2875778079032898,
      "learning_rate": 4.129e-05,
      "loss": 0.0029,
      "step": 26130
    },
    {
      "epoch": 1.3941333333333334,
      "grad_norm": 0.4675176739692688,
      "learning_rate": 4.1286666666666666e-05,
      "loss": 0.003,
      "step": 26140
    },
    {
      "epoch": 1.3946666666666667,
      "grad_norm": 0.37526974081993103,
      "learning_rate": 4.128333333333333e-05,
      "loss": 0.0041,
      "step": 26150
    },
    {
      "epoch": 1.3952,
      "grad_norm": 0.25415611267089844,
      "learning_rate": 4.1280000000000005e-05,
      "loss": 0.0027,
      "step": 26160
    },
    {
      "epoch": 1.3957333333333333,
      "grad_norm": 0.2186182290315628,
      "learning_rate": 4.127666666666667e-05,
      "loss": 0.0041,
      "step": 26170
    },
    {
      "epoch": 1.3962666666666665,
      "grad_norm": 0.6939507722854614,
      "learning_rate": 4.127333333333334e-05,
      "loss": 0.003,
      "step": 26180
    },
    {
      "epoch": 1.3968,
      "grad_norm": 0.08066605031490326,
      "learning_rate": 4.127e-05,
      "loss": 0.0028,
      "step": 26190
    },
    {
      "epoch": 1.3973333333333333,
      "grad_norm": 0.42376378178596497,
      "learning_rate": 4.126666666666667e-05,
      "loss": 0.0034,
      "step": 26200
    },
    {
      "epoch": 1.3978666666666666,
      "grad_norm": 0.3105960786342621,
      "learning_rate": 4.1263333333333335e-05,
      "loss": 0.0035,
      "step": 26210
    },
    {
      "epoch": 1.3984,
      "grad_norm": 0.18901678919792175,
      "learning_rate": 4.126e-05,
      "loss": 0.0029,
      "step": 26220
    },
    {
      "epoch": 1.3989333333333334,
      "grad_norm": 0.61481112241745,
      "learning_rate": 4.1256666666666674e-05,
      "loss": 0.0038,
      "step": 26230
    },
    {
      "epoch": 1.3994666666666666,
      "grad_norm": 0.22264496982097626,
      "learning_rate": 4.1253333333333334e-05,
      "loss": 0.0034,
      "step": 26240
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.40298253297805786,
      "learning_rate": 4.125e-05,
      "loss": 0.0029,
      "step": 26250
    },
    {
      "epoch": 1.4005333333333334,
      "grad_norm": 1.004097580909729,
      "learning_rate": 4.1246666666666666e-05,
      "loss": 0.0036,
      "step": 26260
    },
    {
      "epoch": 1.4010666666666667,
      "grad_norm": 0.16777221858501434,
      "learning_rate": 4.124333333333333e-05,
      "loss": 0.0024,
      "step": 26270
    },
    {
      "epoch": 1.4016,
      "grad_norm": 0.4259316027164459,
      "learning_rate": 4.124e-05,
      "loss": 0.0043,
      "step": 26280
    },
    {
      "epoch": 1.4021333333333335,
      "grad_norm": 0.5534092783927917,
      "learning_rate": 4.123666666666667e-05,
      "loss": 0.0031,
      "step": 26290
    },
    {
      "epoch": 1.4026666666666667,
      "grad_norm": 0.2463580071926117,
      "learning_rate": 4.123333333333334e-05,
      "loss": 0.004,
      "step": 26300
    },
    {
      "epoch": 1.4032,
      "grad_norm": 0.6985917687416077,
      "learning_rate": 4.123e-05,
      "loss": 0.0033,
      "step": 26310
    },
    {
      "epoch": 1.4037333333333333,
      "grad_norm": 0.0679246038198471,
      "learning_rate": 4.122666666666667e-05,
      "loss": 0.0025,
      "step": 26320
    },
    {
      "epoch": 1.4042666666666666,
      "grad_norm": 0.7382652759552002,
      "learning_rate": 4.1223333333333336e-05,
      "loss": 0.0025,
      "step": 26330
    },
    {
      "epoch": 1.4048,
      "grad_norm": 0.3379674553871155,
      "learning_rate": 4.122e-05,
      "loss": 0.0027,
      "step": 26340
    },
    {
      "epoch": 1.4053333333333333,
      "grad_norm": 0.17223204672336578,
      "learning_rate": 4.121666666666667e-05,
      "loss": 0.003,
      "step": 26350
    },
    {
      "epoch": 1.4058666666666666,
      "grad_norm": 0.06990662217140198,
      "learning_rate": 4.1213333333333334e-05,
      "loss": 0.004,
      "step": 26360
    },
    {
      "epoch": 1.4064,
      "grad_norm": 0.18399302661418915,
      "learning_rate": 4.121000000000001e-05,
      "loss": 0.003,
      "step": 26370
    },
    {
      "epoch": 1.4069333333333334,
      "grad_norm": 0.5081175565719604,
      "learning_rate": 4.120666666666667e-05,
      "loss": 0.0026,
      "step": 26380
    },
    {
      "epoch": 1.4074666666666666,
      "grad_norm": 0.6728664636611938,
      "learning_rate": 4.120333333333333e-05,
      "loss": 0.0036,
      "step": 26390
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.4958720803260803,
      "learning_rate": 4.12e-05,
      "loss": 0.0036,
      "step": 26400
    },
    {
      "epoch": 1.4085333333333334,
      "grad_norm": 0.19502051174640656,
      "learning_rate": 4.1196666666666665e-05,
      "loss": 0.0032,
      "step": 26410
    },
    {
      "epoch": 1.4090666666666667,
      "grad_norm": 0.20038172602653503,
      "learning_rate": 4.119333333333333e-05,
      "loss": 0.0031,
      "step": 26420
    },
    {
      "epoch": 1.4096,
      "grad_norm": 0.4021376371383667,
      "learning_rate": 4.1190000000000004e-05,
      "loss": 0.005,
      "step": 26430
    },
    {
      "epoch": 1.4101333333333335,
      "grad_norm": 0.33107990026474,
      "learning_rate": 4.118666666666667e-05,
      "loss": 0.0038,
      "step": 26440
    },
    {
      "epoch": 1.4106666666666667,
      "grad_norm": 0.38241732120513916,
      "learning_rate": 4.1183333333333336e-05,
      "loss": 0.0037,
      "step": 26450
    },
    {
      "epoch": 1.4112,
      "grad_norm": 0.08278562873601913,
      "learning_rate": 4.118e-05,
      "loss": 0.0023,
      "step": 26460
    },
    {
      "epoch": 1.4117333333333333,
      "grad_norm": 0.04331143945455551,
      "learning_rate": 4.117666666666667e-05,
      "loss": 0.0028,
      "step": 26470
    },
    {
      "epoch": 1.4122666666666666,
      "grad_norm": 0.0960417091846466,
      "learning_rate": 4.1173333333333334e-05,
      "loss": 0.0024,
      "step": 26480
    },
    {
      "epoch": 1.4128,
      "grad_norm": 0.09239276498556137,
      "learning_rate": 4.117e-05,
      "loss": 0.0033,
      "step": 26490
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 0.04024017974734306,
      "learning_rate": 4.116666666666667e-05,
      "loss": 0.0033,
      "step": 26500
    },
    {
      "epoch": 1.4138666666666666,
      "grad_norm": 0.46386078000068665,
      "learning_rate": 4.116333333333334e-05,
      "loss": 0.0032,
      "step": 26510
    },
    {
      "epoch": 1.4144,
      "grad_norm": 0.31044936180114746,
      "learning_rate": 4.1160000000000006e-05,
      "loss": 0.0037,
      "step": 26520
    },
    {
      "epoch": 1.4149333333333334,
      "grad_norm": 0.6075115203857422,
      "learning_rate": 4.115666666666667e-05,
      "loss": 0.0028,
      "step": 26530
    },
    {
      "epoch": 1.4154666666666667,
      "grad_norm": 0.42508506774902344,
      "learning_rate": 4.115333333333333e-05,
      "loss": 0.002,
      "step": 26540
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.39861205220222473,
      "learning_rate": 4.115e-05,
      "loss": 0.0033,
      "step": 26550
    },
    {
      "epoch": 1.4165333333333332,
      "grad_norm": 0.6860783100128174,
      "learning_rate": 4.1146666666666663e-05,
      "loss": 0.0039,
      "step": 26560
    },
    {
      "epoch": 1.4170666666666667,
      "grad_norm": 0.24379676580429077,
      "learning_rate": 4.1143333333333336e-05,
      "loss": 0.0027,
      "step": 26570
    },
    {
      "epoch": 1.4176,
      "grad_norm": 0.12676893174648285,
      "learning_rate": 4.114e-05,
      "loss": 0.0022,
      "step": 26580
    },
    {
      "epoch": 1.4181333333333335,
      "grad_norm": 0.19526727497577667,
      "learning_rate": 4.113666666666667e-05,
      "loss": 0.0039,
      "step": 26590
    },
    {
      "epoch": 1.4186666666666667,
      "grad_norm": 0.366161972284317,
      "learning_rate": 4.1133333333333335e-05,
      "loss": 0.0027,
      "step": 26600
    },
    {
      "epoch": 1.4192,
      "grad_norm": 0.24387729167938232,
      "learning_rate": 4.113e-05,
      "loss": 0.0035,
      "step": 26610
    },
    {
      "epoch": 1.4197333333333333,
      "grad_norm": 0.30395859479904175,
      "learning_rate": 4.112666666666667e-05,
      "loss": 0.0035,
      "step": 26620
    },
    {
      "epoch": 1.4202666666666666,
      "grad_norm": 0.16139699518680573,
      "learning_rate": 4.112333333333333e-05,
      "loss": 0.004,
      "step": 26630
    },
    {
      "epoch": 1.4208,
      "grad_norm": 0.1075696125626564,
      "learning_rate": 4.1120000000000006e-05,
      "loss": 0.0032,
      "step": 26640
    },
    {
      "epoch": 1.4213333333333333,
      "grad_norm": 0.04494994878768921,
      "learning_rate": 4.111666666666667e-05,
      "loss": 0.0018,
      "step": 26650
    },
    {
      "epoch": 1.4218666666666666,
      "grad_norm": 0.5518093705177307,
      "learning_rate": 4.111333333333334e-05,
      "loss": 0.0028,
      "step": 26660
    },
    {
      "epoch": 1.4224,
      "grad_norm": 0.5626900792121887,
      "learning_rate": 4.1110000000000005e-05,
      "loss": 0.0045,
      "step": 26670
    },
    {
      "epoch": 1.4229333333333334,
      "grad_norm": 0.33752313256263733,
      "learning_rate": 4.110666666666667e-05,
      "loss": 0.0029,
      "step": 26680
    },
    {
      "epoch": 1.4234666666666667,
      "grad_norm": 0.3339795470237732,
      "learning_rate": 4.110333333333333e-05,
      "loss": 0.0032,
      "step": 26690
    },
    {
      "epoch": 1.424,
      "grad_norm": 0.33114445209503174,
      "learning_rate": 4.11e-05,
      "loss": 0.0031,
      "step": 26700
    },
    {
      "epoch": 1.4245333333333332,
      "grad_norm": 0.3670365810394287,
      "learning_rate": 4.109666666666667e-05,
      "loss": 0.0037,
      "step": 26710
    },
    {
      "epoch": 1.4250666666666667,
      "grad_norm": 0.33561816811561584,
      "learning_rate": 4.1093333333333335e-05,
      "loss": 0.003,
      "step": 26720
    },
    {
      "epoch": 1.4256,
      "grad_norm": 0.58924800157547,
      "learning_rate": 4.109e-05,
      "loss": 0.0039,
      "step": 26730
    },
    {
      "epoch": 1.4261333333333333,
      "grad_norm": 0.2743019461631775,
      "learning_rate": 4.108666666666667e-05,
      "loss": 0.0022,
      "step": 26740
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.16304077208042145,
      "learning_rate": 4.1083333333333334e-05,
      "loss": 0.0028,
      "step": 26750
    },
    {
      "epoch": 1.4272,
      "grad_norm": 0.25322210788726807,
      "learning_rate": 4.108e-05,
      "loss": 0.0041,
      "step": 26760
    },
    {
      "epoch": 1.4277333333333333,
      "grad_norm": 0.27692514657974243,
      "learning_rate": 4.1076666666666666e-05,
      "loss": 0.003,
      "step": 26770
    },
    {
      "epoch": 1.4282666666666666,
      "grad_norm": 0.12703949213027954,
      "learning_rate": 4.107333333333334e-05,
      "loss": 0.0033,
      "step": 26780
    },
    {
      "epoch": 1.4288,
      "grad_norm": 0.253390908241272,
      "learning_rate": 4.1070000000000005e-05,
      "loss": 0.0035,
      "step": 26790
    },
    {
      "epoch": 1.4293333333333333,
      "grad_norm": 0.3355768024921417,
      "learning_rate": 4.106666666666667e-05,
      "loss": 0.0028,
      "step": 26800
    },
    {
      "epoch": 1.4298666666666666,
      "grad_norm": 0.4293508231639862,
      "learning_rate": 4.106333333333334e-05,
      "loss": 0.0042,
      "step": 26810
    },
    {
      "epoch": 1.4304000000000001,
      "grad_norm": 0.5771391987800598,
      "learning_rate": 4.106e-05,
      "loss": 0.0028,
      "step": 26820
    },
    {
      "epoch": 1.4309333333333334,
      "grad_norm": 0.2746903598308563,
      "learning_rate": 4.105666666666667e-05,
      "loss": 0.0032,
      "step": 26830
    },
    {
      "epoch": 1.4314666666666667,
      "grad_norm": 0.3119505047798157,
      "learning_rate": 4.1053333333333336e-05,
      "loss": 0.0029,
      "step": 26840
    },
    {
      "epoch": 1.432,
      "grad_norm": 0.06762510538101196,
      "learning_rate": 4.105e-05,
      "loss": 0.002,
      "step": 26850
    },
    {
      "epoch": 1.4325333333333332,
      "grad_norm": 0.1588945984840393,
      "learning_rate": 4.104666666666667e-05,
      "loss": 0.0031,
      "step": 26860
    },
    {
      "epoch": 1.4330666666666667,
      "grad_norm": 0.3390097916126251,
      "learning_rate": 4.1043333333333334e-05,
      "loss": 0.0021,
      "step": 26870
    },
    {
      "epoch": 1.4336,
      "grad_norm": 0.1547040492296219,
      "learning_rate": 4.104e-05,
      "loss": 0.004,
      "step": 26880
    },
    {
      "epoch": 1.4341333333333333,
      "grad_norm": 0.5218948125839233,
      "learning_rate": 4.1036666666666666e-05,
      "loss": 0.0032,
      "step": 26890
    },
    {
      "epoch": 1.4346666666666668,
      "grad_norm": 0.1552564948797226,
      "learning_rate": 4.103333333333333e-05,
      "loss": 0.0027,
      "step": 26900
    },
    {
      "epoch": 1.4352,
      "grad_norm": 0.06791577488183975,
      "learning_rate": 4.103e-05,
      "loss": 0.0038,
      "step": 26910
    },
    {
      "epoch": 1.4357333333333333,
      "grad_norm": 0.19697241485118866,
      "learning_rate": 4.102666666666667e-05,
      "loss": 0.0023,
      "step": 26920
    },
    {
      "epoch": 1.4362666666666666,
      "grad_norm": 0.03885571286082268,
      "learning_rate": 4.102333333333334e-05,
      "loss": 0.0032,
      "step": 26930
    },
    {
      "epoch": 1.4368,
      "grad_norm": 0.40271392464637756,
      "learning_rate": 4.1020000000000004e-05,
      "loss": 0.0024,
      "step": 26940
    },
    {
      "epoch": 1.4373333333333334,
      "grad_norm": 0.3755514919757843,
      "learning_rate": 4.101666666666667e-05,
      "loss": 0.0032,
      "step": 26950
    },
    {
      "epoch": 1.4378666666666666,
      "grad_norm": 0.42648735642433167,
      "learning_rate": 4.1013333333333336e-05,
      "loss": 0.0025,
      "step": 26960
    },
    {
      "epoch": 1.4384000000000001,
      "grad_norm": 0.7311371564865112,
      "learning_rate": 4.101e-05,
      "loss": 0.0026,
      "step": 26970
    },
    {
      "epoch": 1.4389333333333334,
      "grad_norm": 0.3662411868572235,
      "learning_rate": 4.100666666666667e-05,
      "loss": 0.0033,
      "step": 26980
    },
    {
      "epoch": 1.4394666666666667,
      "grad_norm": 0.39160507917404175,
      "learning_rate": 4.100333333333334e-05,
      "loss": 0.0029,
      "step": 26990
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.4571784436702728,
      "learning_rate": 4.1e-05,
      "loss": 0.0022,
      "step": 27000
    },
    {
      "epoch": 1.4405333333333332,
      "grad_norm": 0.30655619502067566,
      "learning_rate": 4.0996666666666667e-05,
      "loss": 0.0029,
      "step": 27010
    },
    {
      "epoch": 1.4410666666666667,
      "grad_norm": 0.07668323069810867,
      "learning_rate": 4.099333333333333e-05,
      "loss": 0.0037,
      "step": 27020
    },
    {
      "epoch": 1.4416,
      "grad_norm": 0.1843070089817047,
      "learning_rate": 4.099e-05,
      "loss": 0.0029,
      "step": 27030
    },
    {
      "epoch": 1.4421333333333333,
      "grad_norm": 0.17640094459056854,
      "learning_rate": 4.0986666666666665e-05,
      "loss": 0.0035,
      "step": 27040
    },
    {
      "epoch": 1.4426666666666668,
      "grad_norm": 0.22706925868988037,
      "learning_rate": 4.098333333333334e-05,
      "loss": 0.0036,
      "step": 27050
    },
    {
      "epoch": 1.4432,
      "grad_norm": 0.09480110555887222,
      "learning_rate": 4.0980000000000004e-05,
      "loss": 0.0034,
      "step": 27060
    },
    {
      "epoch": 1.4437333333333333,
      "grad_norm": 0.07403874397277832,
      "learning_rate": 4.097666666666667e-05,
      "loss": 0.0025,
      "step": 27070
    },
    {
      "epoch": 1.4442666666666666,
      "grad_norm": 0.053518351167440414,
      "learning_rate": 4.0973333333333336e-05,
      "loss": 0.003,
      "step": 27080
    },
    {
      "epoch": 1.4447999999999999,
      "grad_norm": 0.2723151743412018,
      "learning_rate": 4.097e-05,
      "loss": 0.0036,
      "step": 27090
    },
    {
      "epoch": 1.4453333333333334,
      "grad_norm": 0.4958403408527374,
      "learning_rate": 4.096666666666667e-05,
      "loss": 0.0026,
      "step": 27100
    },
    {
      "epoch": 1.4458666666666666,
      "grad_norm": 0.1288972795009613,
      "learning_rate": 4.0963333333333335e-05,
      "loss": 0.0031,
      "step": 27110
    },
    {
      "epoch": 1.4464000000000001,
      "grad_norm": 0.37089642882347107,
      "learning_rate": 4.096e-05,
      "loss": 0.0031,
      "step": 27120
    },
    {
      "epoch": 1.4469333333333334,
      "grad_norm": 0.3676477372646332,
      "learning_rate": 4.0956666666666674e-05,
      "loss": 0.0029,
      "step": 27130
    },
    {
      "epoch": 1.4474666666666667,
      "grad_norm": 0.4325793981552124,
      "learning_rate": 4.095333333333334e-05,
      "loss": 0.0025,
      "step": 27140
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.1880207508802414,
      "learning_rate": 4.095e-05,
      "loss": 0.0025,
      "step": 27150
    },
    {
      "epoch": 1.4485333333333332,
      "grad_norm": 0.10929354280233383,
      "learning_rate": 4.0946666666666665e-05,
      "loss": 0.0036,
      "step": 27160
    },
    {
      "epoch": 1.4490666666666667,
      "grad_norm": 0.1391419619321823,
      "learning_rate": 4.094333333333333e-05,
      "loss": 0.0038,
      "step": 27170
    },
    {
      "epoch": 1.4496,
      "grad_norm": 0.45930764079093933,
      "learning_rate": 4.094e-05,
      "loss": 0.003,
      "step": 27180
    },
    {
      "epoch": 1.4501333333333333,
      "grad_norm": 0.48949071764945984,
      "learning_rate": 4.093666666666667e-05,
      "loss": 0.0028,
      "step": 27190
    },
    {
      "epoch": 1.4506666666666668,
      "grad_norm": 0.5451181530952454,
      "learning_rate": 4.093333333333334e-05,
      "loss": 0.0048,
      "step": 27200
    },
    {
      "epoch": 1.4512,
      "grad_norm": 0.027381474152207375,
      "learning_rate": 4.093e-05,
      "loss": 0.0028,
      "step": 27210
    },
    {
      "epoch": 1.4517333333333333,
      "grad_norm": 0.15784123539924622,
      "learning_rate": 4.092666666666667e-05,
      "loss": 0.0022,
      "step": 27220
    },
    {
      "epoch": 1.4522666666666666,
      "grad_norm": 0.3657166659832001,
      "learning_rate": 4.0923333333333335e-05,
      "loss": 0.0029,
      "step": 27230
    },
    {
      "epoch": 1.4527999999999999,
      "grad_norm": 0.2509177327156067,
      "learning_rate": 4.092e-05,
      "loss": 0.0024,
      "step": 27240
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 0.30916404724121094,
      "learning_rate": 4.091666666666667e-05,
      "loss": 0.0034,
      "step": 27250
    },
    {
      "epoch": 1.4538666666666666,
      "grad_norm": 0.18309074640274048,
      "learning_rate": 4.0913333333333334e-05,
      "loss": 0.0033,
      "step": 27260
    },
    {
      "epoch": 1.4544000000000001,
      "grad_norm": 0.1530807763338089,
      "learning_rate": 4.0910000000000006e-05,
      "loss": 0.0024,
      "step": 27270
    },
    {
      "epoch": 1.4549333333333334,
      "grad_norm": 0.16649651527404785,
      "learning_rate": 4.090666666666667e-05,
      "loss": 0.0047,
      "step": 27280
    },
    {
      "epoch": 1.4554666666666667,
      "grad_norm": 0.06830023974180222,
      "learning_rate": 4.090333333333334e-05,
      "loss": 0.0025,
      "step": 27290
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.05359163135290146,
      "learning_rate": 4.09e-05,
      "loss": 0.003,
      "step": 27300
    },
    {
      "epoch": 1.4565333333333332,
      "grad_norm": 0.3855353593826294,
      "learning_rate": 4.0896666666666664e-05,
      "loss": 0.0024,
      "step": 27310
    },
    {
      "epoch": 1.4570666666666667,
      "grad_norm": 0.9847349524497986,
      "learning_rate": 4.089333333333333e-05,
      "loss": 0.0024,
      "step": 27320
    },
    {
      "epoch": 1.4576,
      "grad_norm": 0.2430363893508911,
      "learning_rate": 4.089e-05,
      "loss": 0.0032,
      "step": 27330
    },
    {
      "epoch": 1.4581333333333333,
      "grad_norm": 0.0945306271314621,
      "learning_rate": 4.088666666666667e-05,
      "loss": 0.003,
      "step": 27340
    },
    {
      "epoch": 1.4586666666666668,
      "grad_norm": 0.2844546437263489,
      "learning_rate": 4.0883333333333335e-05,
      "loss": 0.0029,
      "step": 27350
    },
    {
      "epoch": 1.4592,
      "grad_norm": 0.15805725753307343,
      "learning_rate": 4.088e-05,
      "loss": 0.0031,
      "step": 27360
    },
    {
      "epoch": 1.4597333333333333,
      "grad_norm": 0.24852445721626282,
      "learning_rate": 4.087666666666667e-05,
      "loss": 0.0024,
      "step": 27370
    },
    {
      "epoch": 1.4602666666666666,
      "grad_norm": 0.07546692341566086,
      "learning_rate": 4.0873333333333334e-05,
      "loss": 0.0039,
      "step": 27380
    },
    {
      "epoch": 1.4607999999999999,
      "grad_norm": 0.4405706822872162,
      "learning_rate": 4.087e-05,
      "loss": 0.0023,
      "step": 27390
    },
    {
      "epoch": 1.4613333333333334,
      "grad_norm": 0.7894590497016907,
      "learning_rate": 4.086666666666667e-05,
      "loss": 0.003,
      "step": 27400
    },
    {
      "epoch": 1.4618666666666666,
      "grad_norm": 0.8013002276420593,
      "learning_rate": 4.086333333333334e-05,
      "loss": 0.0036,
      "step": 27410
    },
    {
      "epoch": 1.4624,
      "grad_norm": 0.06721283495426178,
      "learning_rate": 4.0860000000000005e-05,
      "loss": 0.0018,
      "step": 27420
    },
    {
      "epoch": 1.4629333333333334,
      "grad_norm": 0.07950232923030853,
      "learning_rate": 4.085666666666667e-05,
      "loss": 0.0038,
      "step": 27430
    },
    {
      "epoch": 1.4634666666666667,
      "grad_norm": 0.10953734815120697,
      "learning_rate": 4.085333333333334e-05,
      "loss": 0.0025,
      "step": 27440
    },
    {
      "epoch": 1.464,
      "grad_norm": 0.21697407960891724,
      "learning_rate": 4.085e-05,
      "loss": 0.0028,
      "step": 27450
    },
    {
      "epoch": 1.4645333333333332,
      "grad_norm": 0.43213334679603577,
      "learning_rate": 4.084666666666667e-05,
      "loss": 0.0029,
      "step": 27460
    },
    {
      "epoch": 1.4650666666666667,
      "grad_norm": 0.18480104207992554,
      "learning_rate": 4.0843333333333336e-05,
      "loss": 0.0032,
      "step": 27470
    },
    {
      "epoch": 1.4656,
      "grad_norm": 0.16859297454357147,
      "learning_rate": 4.084e-05,
      "loss": 0.0029,
      "step": 27480
    },
    {
      "epoch": 1.4661333333333333,
      "grad_norm": 0.8436818718910217,
      "learning_rate": 4.083666666666667e-05,
      "loss": 0.0028,
      "step": 27490
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.40662461519241333,
      "learning_rate": 4.0833333333333334e-05,
      "loss": 0.0035,
      "step": 27500
    },
    {
      "epoch": 1.4672,
      "grad_norm": 0.12815147638320923,
      "learning_rate": 4.083e-05,
      "loss": 0.0024,
      "step": 27510
    },
    {
      "epoch": 1.4677333333333333,
      "grad_norm": 0.1821340173482895,
      "learning_rate": 4.0826666666666667e-05,
      "loss": 0.0023,
      "step": 27520
    },
    {
      "epoch": 1.4682666666666666,
      "grad_norm": 0.1543845683336258,
      "learning_rate": 4.082333333333333e-05,
      "loss": 0.0031,
      "step": 27530
    },
    {
      "epoch": 1.4687999999999999,
      "grad_norm": 0.26916036009788513,
      "learning_rate": 4.0820000000000006e-05,
      "loss": 0.0025,
      "step": 27540
    },
    {
      "epoch": 1.4693333333333334,
      "grad_norm": 0.03569379448890686,
      "learning_rate": 4.081666666666667e-05,
      "loss": 0.0029,
      "step": 27550
    },
    {
      "epoch": 1.4698666666666667,
      "grad_norm": 0.21272358298301697,
      "learning_rate": 4.081333333333334e-05,
      "loss": 0.0025,
      "step": 27560
    },
    {
      "epoch": 1.4704,
      "grad_norm": 0.1009233370423317,
      "learning_rate": 4.0810000000000004e-05,
      "loss": 0.0026,
      "step": 27570
    },
    {
      "epoch": 1.4709333333333334,
      "grad_norm": 0.5483555197715759,
      "learning_rate": 4.080666666666667e-05,
      "loss": 0.0033,
      "step": 27580
    },
    {
      "epoch": 1.4714666666666667,
      "grad_norm": 0.2810547947883606,
      "learning_rate": 4.0803333333333336e-05,
      "loss": 0.0034,
      "step": 27590
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.4260965883731842,
      "learning_rate": 4.08e-05,
      "loss": 0.004,
      "step": 27600
    },
    {
      "epoch": 1.4725333333333332,
      "grad_norm": 0.06448135524988174,
      "learning_rate": 4.079666666666667e-05,
      "loss": 0.0027,
      "step": 27610
    },
    {
      "epoch": 1.4730666666666667,
      "grad_norm": 0.3682272732257843,
      "learning_rate": 4.0793333333333335e-05,
      "loss": 0.0021,
      "step": 27620
    },
    {
      "epoch": 1.4736,
      "grad_norm": 0.37095123529434204,
      "learning_rate": 4.079e-05,
      "loss": 0.0031,
      "step": 27630
    },
    {
      "epoch": 1.4741333333333333,
      "grad_norm": 0.5485515594482422,
      "learning_rate": 4.078666666666667e-05,
      "loss": 0.0028,
      "step": 27640
    },
    {
      "epoch": 1.4746666666666668,
      "grad_norm": 0.3647623360157013,
      "learning_rate": 4.078333333333333e-05,
      "loss": 0.0024,
      "step": 27650
    },
    {
      "epoch": 1.4752,
      "grad_norm": 0.3956182301044464,
      "learning_rate": 4.078e-05,
      "loss": 0.0036,
      "step": 27660
    },
    {
      "epoch": 1.4757333333333333,
      "grad_norm": 0.14606285095214844,
      "learning_rate": 4.0776666666666665e-05,
      "loss": 0.003,
      "step": 27670
    },
    {
      "epoch": 1.4762666666666666,
      "grad_norm": 0.3409788906574249,
      "learning_rate": 4.077333333333334e-05,
      "loss": 0.0033,
      "step": 27680
    },
    {
      "epoch": 1.4768,
      "grad_norm": 0.3078632056713104,
      "learning_rate": 4.0770000000000004e-05,
      "loss": 0.0021,
      "step": 27690
    },
    {
      "epoch": 1.4773333333333334,
      "grad_norm": 0.4625881612300873,
      "learning_rate": 4.076666666666667e-05,
      "loss": 0.0021,
      "step": 27700
    },
    {
      "epoch": 1.4778666666666667,
      "grad_norm": 0.1639437973499298,
      "learning_rate": 4.076333333333334e-05,
      "loss": 0.0027,
      "step": 27710
    },
    {
      "epoch": 1.4784,
      "grad_norm": 0.4125341475009918,
      "learning_rate": 4.076e-05,
      "loss": 0.004,
      "step": 27720
    },
    {
      "epoch": 1.4789333333333334,
      "grad_norm": 0.05059469863772392,
      "learning_rate": 4.075666666666667e-05,
      "loss": 0.0034,
      "step": 27730
    },
    {
      "epoch": 1.4794666666666667,
      "grad_norm": 0.486959844827652,
      "learning_rate": 4.0753333333333335e-05,
      "loss": 0.0025,
      "step": 27740
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.09746309369802475,
      "learning_rate": 4.075e-05,
      "loss": 0.0028,
      "step": 27750
    },
    {
      "epoch": 1.4805333333333333,
      "grad_norm": 0.24317465722560883,
      "learning_rate": 4.074666666666667e-05,
      "loss": 0.0022,
      "step": 27760
    },
    {
      "epoch": 1.4810666666666665,
      "grad_norm": 0.30232107639312744,
      "learning_rate": 4.0743333333333333e-05,
      "loss": 0.0037,
      "step": 27770
    },
    {
      "epoch": 1.4816,
      "grad_norm": 0.2194015234708786,
      "learning_rate": 4.074e-05,
      "loss": 0.0032,
      "step": 27780
    },
    {
      "epoch": 1.4821333333333333,
      "grad_norm": 0.27798137068748474,
      "learning_rate": 4.0736666666666666e-05,
      "loss": 0.0029,
      "step": 27790
    },
    {
      "epoch": 1.4826666666666668,
      "grad_norm": 0.06776277720928192,
      "learning_rate": 4.073333333333333e-05,
      "loss": 0.0038,
      "step": 27800
    },
    {
      "epoch": 1.4832,
      "grad_norm": 0.15572886168956757,
      "learning_rate": 4.0730000000000005e-05,
      "loss": 0.003,
      "step": 27810
    },
    {
      "epoch": 1.4837333333333333,
      "grad_norm": 0.14585672318935394,
      "learning_rate": 4.072666666666667e-05,
      "loss": 0.0029,
      "step": 27820
    },
    {
      "epoch": 1.4842666666666666,
      "grad_norm": 0.08689530193805695,
      "learning_rate": 4.072333333333334e-05,
      "loss": 0.0037,
      "step": 27830
    },
    {
      "epoch": 1.4848,
      "grad_norm": 0.3081132173538208,
      "learning_rate": 4.072e-05,
      "loss": 0.0033,
      "step": 27840
    },
    {
      "epoch": 1.4853333333333334,
      "grad_norm": 0.1292029172182083,
      "learning_rate": 4.071666666666667e-05,
      "loss": 0.0029,
      "step": 27850
    },
    {
      "epoch": 1.4858666666666667,
      "grad_norm": 0.11380115896463394,
      "learning_rate": 4.0713333333333335e-05,
      "loss": 0.0022,
      "step": 27860
    },
    {
      "epoch": 1.4864,
      "grad_norm": 0.7071843147277832,
      "learning_rate": 4.071e-05,
      "loss": 0.0041,
      "step": 27870
    },
    {
      "epoch": 1.4869333333333334,
      "grad_norm": 0.1792801469564438,
      "learning_rate": 4.070666666666667e-05,
      "loss": 0.0027,
      "step": 27880
    },
    {
      "epoch": 1.4874666666666667,
      "grad_norm": 0.049212340265512466,
      "learning_rate": 4.070333333333334e-05,
      "loss": 0.0019,
      "step": 27890
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.2793963849544525,
      "learning_rate": 4.07e-05,
      "loss": 0.0025,
      "step": 27900
    },
    {
      "epoch": 1.4885333333333333,
      "grad_norm": 0.25455111265182495,
      "learning_rate": 4.0696666666666666e-05,
      "loss": 0.0037,
      "step": 27910
    },
    {
      "epoch": 1.4890666666666665,
      "grad_norm": 0.44159311056137085,
      "learning_rate": 4.069333333333333e-05,
      "loss": 0.0023,
      "step": 27920
    },
    {
      "epoch": 1.4896,
      "grad_norm": 0.07491964846849442,
      "learning_rate": 4.069e-05,
      "loss": 0.0032,
      "step": 27930
    },
    {
      "epoch": 1.4901333333333333,
      "grad_norm": 0.12771014869213104,
      "learning_rate": 4.0686666666666664e-05,
      "loss": 0.0022,
      "step": 27940
    },
    {
      "epoch": 1.4906666666666666,
      "grad_norm": 0.12438453733921051,
      "learning_rate": 4.068333333333334e-05,
      "loss": 0.0036,
      "step": 27950
    },
    {
      "epoch": 1.4912,
      "grad_norm": 0.24815356731414795,
      "learning_rate": 4.0680000000000004e-05,
      "loss": 0.0041,
      "step": 27960
    },
    {
      "epoch": 1.4917333333333334,
      "grad_norm": 0.23089303076267242,
      "learning_rate": 4.067666666666667e-05,
      "loss": 0.0031,
      "step": 27970
    },
    {
      "epoch": 1.4922666666666666,
      "grad_norm": 0.31012192368507385,
      "learning_rate": 4.0673333333333336e-05,
      "loss": 0.0028,
      "step": 27980
    },
    {
      "epoch": 1.4928,
      "grad_norm": 0.2714986801147461,
      "learning_rate": 4.067e-05,
      "loss": 0.0025,
      "step": 27990
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 0.1873786747455597,
      "learning_rate": 4.066666666666667e-05,
      "loss": 0.0035,
      "step": 28000
    },
    {
      "epoch": 1.4938666666666667,
      "grad_norm": 0.30183786153793335,
      "learning_rate": 4.0663333333333334e-05,
      "loss": 0.0029,
      "step": 28010
    },
    {
      "epoch": 1.4944,
      "grad_norm": 0.036008622497320175,
      "learning_rate": 4.066e-05,
      "loss": 0.0017,
      "step": 28020
    },
    {
      "epoch": 1.4949333333333334,
      "grad_norm": 0.21711532771587372,
      "learning_rate": 4.065666666666667e-05,
      "loss": 0.0041,
      "step": 28030
    },
    {
      "epoch": 1.4954666666666667,
      "grad_norm": 0.12536348402500153,
      "learning_rate": 4.065333333333334e-05,
      "loss": 0.0029,
      "step": 28040
    },
    {
      "epoch": 1.496,
      "grad_norm": 0.3747014105319977,
      "learning_rate": 4.065e-05,
      "loss": 0.0028,
      "step": 28050
    },
    {
      "epoch": 1.4965333333333333,
      "grad_norm": 0.10599098354578018,
      "learning_rate": 4.0646666666666665e-05,
      "loss": 0.0035,
      "step": 28060
    },
    {
      "epoch": 1.4970666666666665,
      "grad_norm": 0.1868841052055359,
      "learning_rate": 4.064333333333333e-05,
      "loss": 0.003,
      "step": 28070
    },
    {
      "epoch": 1.4976,
      "grad_norm": 0.7286794185638428,
      "learning_rate": 4.064e-05,
      "loss": 0.0035,
      "step": 28080
    },
    {
      "epoch": 1.4981333333333333,
      "grad_norm": 1.0218340158462524,
      "learning_rate": 4.063666666666667e-05,
      "loss": 0.0027,
      "step": 28090
    },
    {
      "epoch": 1.4986666666666666,
      "grad_norm": 0.09485143423080444,
      "learning_rate": 4.0633333333333336e-05,
      "loss": 0.0035,
      "step": 28100
    },
    {
      "epoch": 1.4992,
      "grad_norm": 0.3341197073459625,
      "learning_rate": 4.063e-05,
      "loss": 0.0026,
      "step": 28110
    },
    {
      "epoch": 1.4997333333333334,
      "grad_norm": 0.1586233675479889,
      "learning_rate": 4.062666666666667e-05,
      "loss": 0.0025,
      "step": 28120
    },
    {
      "epoch": 1.5002666666666666,
      "grad_norm": 0.43447062373161316,
      "learning_rate": 4.0623333333333335e-05,
      "loss": 0.0034,
      "step": 28130
    },
    {
      "epoch": 1.5008,
      "grad_norm": 0.0350821428000927,
      "learning_rate": 4.062e-05,
      "loss": 0.0026,
      "step": 28140
    },
    {
      "epoch": 1.5013333333333332,
      "grad_norm": 0.24363283812999725,
      "learning_rate": 4.061666666666667e-05,
      "loss": 0.0041,
      "step": 28150
    },
    {
      "epoch": 1.5018666666666667,
      "grad_norm": 0.07973136752843857,
      "learning_rate": 4.061333333333334e-05,
      "loss": 0.0034,
      "step": 28160
    },
    {
      "epoch": 1.5024,
      "grad_norm": 0.25787508487701416,
      "learning_rate": 4.0610000000000006e-05,
      "loss": 0.0037,
      "step": 28170
    },
    {
      "epoch": 1.5029333333333335,
      "grad_norm": 0.2108667641878128,
      "learning_rate": 4.060666666666667e-05,
      "loss": 0.0026,
      "step": 28180
    },
    {
      "epoch": 1.5034666666666667,
      "grad_norm": 0.21897897124290466,
      "learning_rate": 4.060333333333334e-05,
      "loss": 0.0036,
      "step": 28190
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.39416971802711487,
      "learning_rate": 4.0600000000000004e-05,
      "loss": 0.0023,
      "step": 28200
    },
    {
      "epoch": 1.5045333333333333,
      "grad_norm": 0.06606093794107437,
      "learning_rate": 4.0596666666666664e-05,
      "loss": 0.0029,
      "step": 28210
    },
    {
      "epoch": 1.5050666666666666,
      "grad_norm": 0.272769033908844,
      "learning_rate": 4.0593333333333337e-05,
      "loss": 0.0034,
      "step": 28220
    },
    {
      "epoch": 1.5056,
      "grad_norm": 0.3631300628185272,
      "learning_rate": 4.059e-05,
      "loss": 0.0031,
      "step": 28230
    },
    {
      "epoch": 1.5061333333333333,
      "grad_norm": 0.35350140929222107,
      "learning_rate": 4.058666666666667e-05,
      "loss": 0.0039,
      "step": 28240
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 0.06187211349606514,
      "learning_rate": 4.0583333333333335e-05,
      "loss": 0.0024,
      "step": 28250
    },
    {
      "epoch": 1.5072,
      "grad_norm": 0.03520508483052254,
      "learning_rate": 4.058e-05,
      "loss": 0.0021,
      "step": 28260
    },
    {
      "epoch": 1.5077333333333334,
      "grad_norm": 0.8507018089294434,
      "learning_rate": 4.057666666666667e-05,
      "loss": 0.0035,
      "step": 28270
    },
    {
      "epoch": 1.5082666666666666,
      "grad_norm": 0.6682233810424805,
      "learning_rate": 4.057333333333333e-05,
      "loss": 0.0025,
      "step": 28280
    },
    {
      "epoch": 1.5088,
      "grad_norm": 0.27819105982780457,
      "learning_rate": 4.057e-05,
      "loss": 0.0039,
      "step": 28290
    },
    {
      "epoch": 1.5093333333333332,
      "grad_norm": 0.06240599974989891,
      "learning_rate": 4.056666666666667e-05,
      "loss": 0.0029,
      "step": 28300
    },
    {
      "epoch": 1.5098666666666667,
      "grad_norm": 0.04086703807115555,
      "learning_rate": 4.056333333333334e-05,
      "loss": 0.0038,
      "step": 28310
    },
    {
      "epoch": 1.5104,
      "grad_norm": 0.28421536087989807,
      "learning_rate": 4.0560000000000005e-05,
      "loss": 0.0033,
      "step": 28320
    },
    {
      "epoch": 1.5109333333333335,
      "grad_norm": 0.15867185592651367,
      "learning_rate": 4.055666666666667e-05,
      "loss": 0.003,
      "step": 28330
    },
    {
      "epoch": 1.5114666666666667,
      "grad_norm": 0.09506413340568542,
      "learning_rate": 4.055333333333334e-05,
      "loss": 0.003,
      "step": 28340
    },
    {
      "epoch": 1.512,
      "grad_norm": 0.3649860620498657,
      "learning_rate": 4.055e-05,
      "loss": 0.0033,
      "step": 28350
    },
    {
      "epoch": 1.5125333333333333,
      "grad_norm": 0.3103150725364685,
      "learning_rate": 4.054666666666667e-05,
      "loss": 0.0028,
      "step": 28360
    },
    {
      "epoch": 1.5130666666666666,
      "grad_norm": 0.21892885863780975,
      "learning_rate": 4.0543333333333335e-05,
      "loss": 0.0037,
      "step": 28370
    },
    {
      "epoch": 1.5135999999999998,
      "grad_norm": 0.6130120754241943,
      "learning_rate": 4.054e-05,
      "loss": 0.0025,
      "step": 28380
    },
    {
      "epoch": 1.5141333333333333,
      "grad_norm": 0.12493178993463516,
      "learning_rate": 4.053666666666667e-05,
      "loss": 0.0034,
      "step": 28390
    },
    {
      "epoch": 1.5146666666666668,
      "grad_norm": 0.3624379336833954,
      "learning_rate": 4.0533333333333334e-05,
      "loss": 0.0051,
      "step": 28400
    },
    {
      "epoch": 1.5152,
      "grad_norm": 0.13400819897651672,
      "learning_rate": 4.053e-05,
      "loss": 0.0027,
      "step": 28410
    },
    {
      "epoch": 1.5157333333333334,
      "grad_norm": 0.48757046461105347,
      "learning_rate": 4.0526666666666666e-05,
      "loss": 0.0025,
      "step": 28420
    },
    {
      "epoch": 1.5162666666666667,
      "grad_norm": 0.18387383222579956,
      "learning_rate": 4.052333333333333e-05,
      "loss": 0.0036,
      "step": 28430
    },
    {
      "epoch": 1.5168,
      "grad_norm": 0.5732826590538025,
      "learning_rate": 4.0520000000000005e-05,
      "loss": 0.0021,
      "step": 28440
    },
    {
      "epoch": 1.5173333333333332,
      "grad_norm": 0.42524391412734985,
      "learning_rate": 4.051666666666667e-05,
      "loss": 0.0035,
      "step": 28450
    },
    {
      "epoch": 1.5178666666666667,
      "grad_norm": 0.040257684886455536,
      "learning_rate": 4.051333333333334e-05,
      "loss": 0.0034,
      "step": 28460
    },
    {
      "epoch": 1.5184,
      "grad_norm": 0.24129363894462585,
      "learning_rate": 4.0510000000000003e-05,
      "loss": 0.0032,
      "step": 28470
    },
    {
      "epoch": 1.5189333333333335,
      "grad_norm": 0.42840760946273804,
      "learning_rate": 4.050666666666667e-05,
      "loss": 0.0034,
      "step": 28480
    },
    {
      "epoch": 1.5194666666666667,
      "grad_norm": 0.0696769431233406,
      "learning_rate": 4.0503333333333336e-05,
      "loss": 0.0027,
      "step": 28490
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.3122842609882355,
      "learning_rate": 4.05e-05,
      "loss": 0.0032,
      "step": 28500
    },
    {
      "epoch": 1.5205333333333333,
      "grad_norm": 0.36761072278022766,
      "learning_rate": 4.049666666666667e-05,
      "loss": 0.003,
      "step": 28510
    },
    {
      "epoch": 1.5210666666666666,
      "grad_norm": 0.39985746145248413,
      "learning_rate": 4.0493333333333334e-05,
      "loss": 0.0022,
      "step": 28520
    },
    {
      "epoch": 1.5215999999999998,
      "grad_norm": 0.3242115080356598,
      "learning_rate": 4.049e-05,
      "loss": 0.0025,
      "step": 28530
    },
    {
      "epoch": 1.5221333333333333,
      "grad_norm": 0.24133005738258362,
      "learning_rate": 4.0486666666666666e-05,
      "loss": 0.0028,
      "step": 28540
    },
    {
      "epoch": 1.5226666666666666,
      "grad_norm": 0.3077941834926605,
      "learning_rate": 4.048333333333333e-05,
      "loss": 0.003,
      "step": 28550
    },
    {
      "epoch": 1.5232,
      "grad_norm": 0.4337654709815979,
      "learning_rate": 4.048e-05,
      "loss": 0.0025,
      "step": 28560
    },
    {
      "epoch": 1.5237333333333334,
      "grad_norm": 0.27415576577186584,
      "learning_rate": 4.047666666666667e-05,
      "loss": 0.0022,
      "step": 28570
    },
    {
      "epoch": 1.5242666666666667,
      "grad_norm": 0.05542527511715889,
      "learning_rate": 4.047333333333334e-05,
      "loss": 0.0022,
      "step": 28580
    },
    {
      "epoch": 1.5248,
      "grad_norm": 0.3338913917541504,
      "learning_rate": 4.0470000000000004e-05,
      "loss": 0.0023,
      "step": 28590
    },
    {
      "epoch": 1.5253333333333332,
      "grad_norm": 0.30843386054039,
      "learning_rate": 4.046666666666667e-05,
      "loss": 0.0029,
      "step": 28600
    },
    {
      "epoch": 1.5258666666666667,
      "grad_norm": 0.05301225185394287,
      "learning_rate": 4.0463333333333336e-05,
      "loss": 0.0027,
      "step": 28610
    },
    {
      "epoch": 1.5264,
      "grad_norm": 0.24608047306537628,
      "learning_rate": 4.046e-05,
      "loss": 0.0025,
      "step": 28620
    },
    {
      "epoch": 1.5269333333333335,
      "grad_norm": 0.1891023963689804,
      "learning_rate": 4.045666666666667e-05,
      "loss": 0.002,
      "step": 28630
    },
    {
      "epoch": 1.5274666666666668,
      "grad_norm": 0.04431546851992607,
      "learning_rate": 4.0453333333333335e-05,
      "loss": 0.0026,
      "step": 28640
    },
    {
      "epoch": 1.528,
      "grad_norm": 0.366440087556839,
      "learning_rate": 4.045000000000001e-05,
      "loss": 0.0022,
      "step": 28650
    },
    {
      "epoch": 1.5285333333333333,
      "grad_norm": 0.2731199860572815,
      "learning_rate": 4.044666666666667e-05,
      "loss": 0.0037,
      "step": 28660
    },
    {
      "epoch": 1.5290666666666666,
      "grad_norm": 0.6121396422386169,
      "learning_rate": 4.044333333333333e-05,
      "loss": 0.0034,
      "step": 28670
    },
    {
      "epoch": 1.5295999999999998,
      "grad_norm": 0.4060087203979492,
      "learning_rate": 4.044e-05,
      "loss": 0.0031,
      "step": 28680
    },
    {
      "epoch": 1.5301333333333333,
      "grad_norm": 0.21503889560699463,
      "learning_rate": 4.0436666666666665e-05,
      "loss": 0.0031,
      "step": 28690
    },
    {
      "epoch": 1.5306666666666666,
      "grad_norm": 0.34242334961891174,
      "learning_rate": 4.043333333333333e-05,
      "loss": 0.0025,
      "step": 28700
    },
    {
      "epoch": 1.5312000000000001,
      "grad_norm": 0.5765365362167358,
      "learning_rate": 4.0430000000000004e-05,
      "loss": 0.0023,
      "step": 28710
    },
    {
      "epoch": 1.5317333333333334,
      "grad_norm": 0.39908120036125183,
      "learning_rate": 4.042666666666667e-05,
      "loss": 0.0028,
      "step": 28720
    },
    {
      "epoch": 1.5322666666666667,
      "grad_norm": 0.1329246610403061,
      "learning_rate": 4.0423333333333337e-05,
      "loss": 0.0029,
      "step": 28730
    },
    {
      "epoch": 1.5328,
      "grad_norm": 0.3102683126926422,
      "learning_rate": 4.042e-05,
      "loss": 0.0031,
      "step": 28740
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.12420961260795593,
      "learning_rate": 4.041666666666667e-05,
      "loss": 0.0049,
      "step": 28750
    },
    {
      "epoch": 1.5338666666666667,
      "grad_norm": 0.05180387943983078,
      "learning_rate": 4.0413333333333335e-05,
      "loss": 0.0028,
      "step": 28760
    },
    {
      "epoch": 1.5344,
      "grad_norm": 0.2568303048610687,
      "learning_rate": 4.041e-05,
      "loss": 0.0048,
      "step": 28770
    },
    {
      "epoch": 1.5349333333333335,
      "grad_norm": 0.42113685607910156,
      "learning_rate": 4.040666666666667e-05,
      "loss": 0.003,
      "step": 28780
    },
    {
      "epoch": 1.5354666666666668,
      "grad_norm": 0.07261152565479279,
      "learning_rate": 4.040333333333334e-05,
      "loss": 0.002,
      "step": 28790
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.040709011256694794,
      "learning_rate": 4.0400000000000006e-05,
      "loss": 0.0022,
      "step": 28800
    },
    {
      "epoch": 1.5365333333333333,
      "grad_norm": 0.3931427597999573,
      "learning_rate": 4.0396666666666666e-05,
      "loss": 0.002,
      "step": 28810
    },
    {
      "epoch": 1.5370666666666666,
      "grad_norm": 0.18478600680828094,
      "learning_rate": 4.039333333333333e-05,
      "loss": 0.0019,
      "step": 28820
    },
    {
      "epoch": 1.5375999999999999,
      "grad_norm": 0.1229877769947052,
      "learning_rate": 4.039e-05,
      "loss": 0.0022,
      "step": 28830
    },
    {
      "epoch": 1.5381333333333334,
      "grad_norm": 0.3694632053375244,
      "learning_rate": 4.0386666666666664e-05,
      "loss": 0.0021,
      "step": 28840
    },
    {
      "epoch": 1.5386666666666666,
      "grad_norm": 0.25519463419914246,
      "learning_rate": 4.038333333333334e-05,
      "loss": 0.0036,
      "step": 28850
    },
    {
      "epoch": 1.5392000000000001,
      "grad_norm": 0.09594111144542694,
      "learning_rate": 4.038e-05,
      "loss": 0.0025,
      "step": 28860
    },
    {
      "epoch": 1.5397333333333334,
      "grad_norm": 0.4198569357395172,
      "learning_rate": 4.037666666666667e-05,
      "loss": 0.0024,
      "step": 28870
    },
    {
      "epoch": 1.5402666666666667,
      "grad_norm": 0.21722792088985443,
      "learning_rate": 4.0373333333333335e-05,
      "loss": 0.0024,
      "step": 28880
    },
    {
      "epoch": 1.5408,
      "grad_norm": 0.5813233852386475,
      "learning_rate": 4.037e-05,
      "loss": 0.0044,
      "step": 28890
    },
    {
      "epoch": 1.5413333333333332,
      "grad_norm": 0.24396172165870667,
      "learning_rate": 4.036666666666667e-05,
      "loss": 0.0028,
      "step": 28900
    },
    {
      "epoch": 1.5418666666666667,
      "grad_norm": 0.20897823572158813,
      "learning_rate": 4.0363333333333334e-05,
      "loss": 0.003,
      "step": 28910
    },
    {
      "epoch": 1.5424,
      "grad_norm": 0.27540677785873413,
      "learning_rate": 4.0360000000000007e-05,
      "loss": 0.0023,
      "step": 28920
    },
    {
      "epoch": 1.5429333333333335,
      "grad_norm": 0.30165690183639526,
      "learning_rate": 4.035666666666667e-05,
      "loss": 0.0025,
      "step": 28930
    },
    {
      "epoch": 1.5434666666666668,
      "grad_norm": 0.13842517137527466,
      "learning_rate": 4.035333333333334e-05,
      "loss": 0.0032,
      "step": 28940
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.33507198095321655,
      "learning_rate": 4.0350000000000005e-05,
      "loss": 0.0032,
      "step": 28950
    },
    {
      "epoch": 1.5445333333333333,
      "grad_norm": 0.39159515500068665,
      "learning_rate": 4.0346666666666664e-05,
      "loss": 0.0031,
      "step": 28960
    },
    {
      "epoch": 1.5450666666666666,
      "grad_norm": 0.12610451877117157,
      "learning_rate": 4.034333333333333e-05,
      "loss": 0.0021,
      "step": 28970
    },
    {
      "epoch": 1.5455999999999999,
      "grad_norm": 0.27323731780052185,
      "learning_rate": 4.034e-05,
      "loss": 0.0043,
      "step": 28980
    },
    {
      "epoch": 1.5461333333333334,
      "grad_norm": 0.21499398350715637,
      "learning_rate": 4.033666666666667e-05,
      "loss": 0.0027,
      "step": 28990
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 0.21194976568222046,
      "learning_rate": 4.0333333333333336e-05,
      "loss": 0.0027,
      "step": 29000
    },
    {
      "epoch": 1.5472000000000001,
      "grad_norm": 0.5473340153694153,
      "learning_rate": 4.033e-05,
      "loss": 0.0032,
      "step": 29010
    },
    {
      "epoch": 1.5477333333333334,
      "grad_norm": 0.5135664939880371,
      "learning_rate": 4.032666666666667e-05,
      "loss": 0.0033,
      "step": 29020
    },
    {
      "epoch": 1.5482666666666667,
      "grad_norm": 0.42327842116355896,
      "learning_rate": 4.0323333333333334e-05,
      "loss": 0.0032,
      "step": 29030
    },
    {
      "epoch": 1.5488,
      "grad_norm": 0.5120813846588135,
      "learning_rate": 4.032e-05,
      "loss": 0.0027,
      "step": 29040
    },
    {
      "epoch": 1.5493333333333332,
      "grad_norm": 0.49733594059944153,
      "learning_rate": 4.0316666666666666e-05,
      "loss": 0.0027,
      "step": 29050
    },
    {
      "epoch": 1.5498666666666665,
      "grad_norm": 0.21114949882030487,
      "learning_rate": 4.031333333333334e-05,
      "loss": 0.0028,
      "step": 29060
    },
    {
      "epoch": 1.5504,
      "grad_norm": 0.04447203502058983,
      "learning_rate": 4.0310000000000005e-05,
      "loss": 0.003,
      "step": 29070
    },
    {
      "epoch": 1.5509333333333335,
      "grad_norm": 0.2918803095817566,
      "learning_rate": 4.030666666666667e-05,
      "loss": 0.0037,
      "step": 29080
    },
    {
      "epoch": 1.5514666666666668,
      "grad_norm": 0.056679364293813705,
      "learning_rate": 4.030333333333334e-05,
      "loss": 0.0039,
      "step": 29090
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.2113496959209442,
      "learning_rate": 4.0300000000000004e-05,
      "loss": 0.0031,
      "step": 29100
    },
    {
      "epoch": 1.5525333333333333,
      "grad_norm": 0.04205937683582306,
      "learning_rate": 4.029666666666666e-05,
      "loss": 0.0024,
      "step": 29110
    },
    {
      "epoch": 1.5530666666666666,
      "grad_norm": 0.043737102299928665,
      "learning_rate": 4.0293333333333336e-05,
      "loss": 0.002,
      "step": 29120
    },
    {
      "epoch": 1.5535999999999999,
      "grad_norm": 0.42204782366752625,
      "learning_rate": 4.029e-05,
      "loss": 0.0031,
      "step": 29130
    },
    {
      "epoch": 1.5541333333333334,
      "grad_norm": 0.21403196454048157,
      "learning_rate": 4.028666666666667e-05,
      "loss": 0.0024,
      "step": 29140
    },
    {
      "epoch": 1.5546666666666666,
      "grad_norm": 0.1530446857213974,
      "learning_rate": 4.0283333333333334e-05,
      "loss": 0.0036,
      "step": 29150
    },
    {
      "epoch": 1.5552000000000001,
      "grad_norm": 0.18525949120521545,
      "learning_rate": 4.028e-05,
      "loss": 0.003,
      "step": 29160
    },
    {
      "epoch": 1.5557333333333334,
      "grad_norm": 0.2896117568016052,
      "learning_rate": 4.027666666666667e-05,
      "loss": 0.0032,
      "step": 29170
    },
    {
      "epoch": 1.5562666666666667,
      "grad_norm": 0.8029144406318665,
      "learning_rate": 4.027333333333333e-05,
      "loss": 0.0034,
      "step": 29180
    },
    {
      "epoch": 1.5568,
      "grad_norm": 0.07651649415493011,
      "learning_rate": 4.027e-05,
      "loss": 0.0032,
      "step": 29190
    },
    {
      "epoch": 1.5573333333333332,
      "grad_norm": 0.24323086440563202,
      "learning_rate": 4.026666666666667e-05,
      "loss": 0.0028,
      "step": 29200
    },
    {
      "epoch": 1.5578666666666665,
      "grad_norm": 0.03990268334746361,
      "learning_rate": 4.026333333333334e-05,
      "loss": 0.0035,
      "step": 29210
    },
    {
      "epoch": 1.5584,
      "grad_norm": 0.24756091833114624,
      "learning_rate": 4.0260000000000004e-05,
      "loss": 0.0024,
      "step": 29220
    },
    {
      "epoch": 1.5589333333333333,
      "grad_norm": 0.2416495829820633,
      "learning_rate": 4.025666666666667e-05,
      "loss": 0.003,
      "step": 29230
    },
    {
      "epoch": 1.5594666666666668,
      "grad_norm": 0.3317575752735138,
      "learning_rate": 4.0253333333333336e-05,
      "loss": 0.0029,
      "step": 29240
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.21251218020915985,
      "learning_rate": 4.025e-05,
      "loss": 0.003,
      "step": 29250
    },
    {
      "epoch": 1.5605333333333333,
      "grad_norm": 0.12258315831422806,
      "learning_rate": 4.024666666666667e-05,
      "loss": 0.0029,
      "step": 29260
    },
    {
      "epoch": 1.5610666666666666,
      "grad_norm": 0.21876369416713715,
      "learning_rate": 4.0243333333333335e-05,
      "loss": 0.0027,
      "step": 29270
    },
    {
      "epoch": 1.5615999999999999,
      "grad_norm": 0.4264537990093231,
      "learning_rate": 4.024e-05,
      "loss": 0.0036,
      "step": 29280
    },
    {
      "epoch": 1.5621333333333334,
      "grad_norm": 0.3047293722629547,
      "learning_rate": 4.023666666666667e-05,
      "loss": 0.0026,
      "step": 29290
    },
    {
      "epoch": 1.5626666666666666,
      "grad_norm": 0.585145890712738,
      "learning_rate": 4.023333333333333e-05,
      "loss": 0.003,
      "step": 29300
    },
    {
      "epoch": 1.5632000000000001,
      "grad_norm": 0.401531457901001,
      "learning_rate": 4.023e-05,
      "loss": 0.0032,
      "step": 29310
    },
    {
      "epoch": 1.5637333333333334,
      "grad_norm": 0.2910335958003998,
      "learning_rate": 4.0226666666666666e-05,
      "loss": 0.0024,
      "step": 29320
    },
    {
      "epoch": 1.5642666666666667,
      "grad_norm": 0.21186953783035278,
      "learning_rate": 4.022333333333334e-05,
      "loss": 0.004,
      "step": 29330
    },
    {
      "epoch": 1.5648,
      "grad_norm": 0.1862100511789322,
      "learning_rate": 4.0220000000000005e-05,
      "loss": 0.0033,
      "step": 29340
    },
    {
      "epoch": 1.5653333333333332,
      "grad_norm": 0.29131126403808594,
      "learning_rate": 4.021666666666667e-05,
      "loss": 0.0034,
      "step": 29350
    },
    {
      "epoch": 1.5658666666666665,
      "grad_norm": 0.3637795150279999,
      "learning_rate": 4.021333333333334e-05,
      "loss": 0.0022,
      "step": 29360
    },
    {
      "epoch": 1.5664,
      "grad_norm": 0.6649981737136841,
      "learning_rate": 4.021e-05,
      "loss": 0.0028,
      "step": 29370
    },
    {
      "epoch": 1.5669333333333333,
      "grad_norm": 0.2796612083911896,
      "learning_rate": 4.020666666666667e-05,
      "loss": 0.0035,
      "step": 29380
    },
    {
      "epoch": 1.5674666666666668,
      "grad_norm": 0.7525631189346313,
      "learning_rate": 4.0203333333333335e-05,
      "loss": 0.004,
      "step": 29390
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.368114709854126,
      "learning_rate": 4.02e-05,
      "loss": 0.002,
      "step": 29400
    },
    {
      "epoch": 1.5685333333333333,
      "grad_norm": 0.27385520935058594,
      "learning_rate": 4.0196666666666674e-05,
      "loss": 0.0027,
      "step": 29410
    },
    {
      "epoch": 1.5690666666666666,
      "grad_norm": 0.21228456497192383,
      "learning_rate": 4.0193333333333334e-05,
      "loss": 0.0023,
      "step": 29420
    },
    {
      "epoch": 1.5695999999999999,
      "grad_norm": 0.0716322809457779,
      "learning_rate": 4.019e-05,
      "loss": 0.0026,
      "step": 29430
    },
    {
      "epoch": 1.5701333333333334,
      "grad_norm": 0.2130982130765915,
      "learning_rate": 4.0186666666666666e-05,
      "loss": 0.0023,
      "step": 29440
    },
    {
      "epoch": 1.5706666666666667,
      "grad_norm": 0.09750070422887802,
      "learning_rate": 4.018333333333333e-05,
      "loss": 0.0031,
      "step": 29450
    },
    {
      "epoch": 1.5712000000000002,
      "grad_norm": 0.21250395476818085,
      "learning_rate": 4.018e-05,
      "loss": 0.004,
      "step": 29460
    },
    {
      "epoch": 1.5717333333333334,
      "grad_norm": 0.3375405967235565,
      "learning_rate": 4.017666666666667e-05,
      "loss": 0.0038,
      "step": 29470
    },
    {
      "epoch": 1.5722666666666667,
      "grad_norm": 0.24646762013435364,
      "learning_rate": 4.017333333333334e-05,
      "loss": 0.0026,
      "step": 29480
    },
    {
      "epoch": 1.5728,
      "grad_norm": 0.15487681329250336,
      "learning_rate": 4.017e-05,
      "loss": 0.0036,
      "step": 29490
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 0.3317982256412506,
      "learning_rate": 4.016666666666667e-05,
      "loss": 0.0025,
      "step": 29500
    },
    {
      "epoch": 1.5738666666666665,
      "grad_norm": 0.09318047761917114,
      "learning_rate": 4.0163333333333336e-05,
      "loss": 0.0028,
      "step": 29510
    },
    {
      "epoch": 1.5744,
      "grad_norm": 0.4851408302783966,
      "learning_rate": 4.016e-05,
      "loss": 0.0023,
      "step": 29520
    },
    {
      "epoch": 1.5749333333333333,
      "grad_norm": 0.27126458287239075,
      "learning_rate": 4.015666666666667e-05,
      "loss": 0.0031,
      "step": 29530
    },
    {
      "epoch": 1.5754666666666668,
      "grad_norm": 0.2709343731403351,
      "learning_rate": 4.0153333333333334e-05,
      "loss": 0.0035,
      "step": 29540
    },
    {
      "epoch": 1.576,
      "grad_norm": 0.4845649302005768,
      "learning_rate": 4.015000000000001e-05,
      "loss": 0.0028,
      "step": 29550
    },
    {
      "epoch": 1.5765333333333333,
      "grad_norm": 0.09474249929189682,
      "learning_rate": 4.014666666666667e-05,
      "loss": 0.0018,
      "step": 29560
    },
    {
      "epoch": 1.5770666666666666,
      "grad_norm": 0.4569144546985626,
      "learning_rate": 4.014333333333333e-05,
      "loss": 0.0033,
      "step": 29570
    },
    {
      "epoch": 1.5776,
      "grad_norm": 0.2150692492723465,
      "learning_rate": 4.014e-05,
      "loss": 0.0021,
      "step": 29580
    },
    {
      "epoch": 1.5781333333333334,
      "grad_norm": 0.06966406106948853,
      "learning_rate": 4.0136666666666665e-05,
      "loss": 0.0027,
      "step": 29590
    },
    {
      "epoch": 1.5786666666666667,
      "grad_norm": 0.15194302797317505,
      "learning_rate": 4.013333333333333e-05,
      "loss": 0.0026,
      "step": 29600
    },
    {
      "epoch": 1.5792000000000002,
      "grad_norm": 0.23267534375190735,
      "learning_rate": 4.0130000000000004e-05,
      "loss": 0.0026,
      "step": 29610
    },
    {
      "epoch": 1.5797333333333334,
      "grad_norm": 0.15276210010051727,
      "learning_rate": 4.012666666666667e-05,
      "loss": 0.0033,
      "step": 29620
    },
    {
      "epoch": 1.5802666666666667,
      "grad_norm": 0.1263246387243271,
      "learning_rate": 4.0123333333333336e-05,
      "loss": 0.0032,
      "step": 29630
    },
    {
      "epoch": 1.5808,
      "grad_norm": 0.3524364233016968,
      "learning_rate": 4.012e-05,
      "loss": 0.0027,
      "step": 29640
    },
    {
      "epoch": 1.5813333333333333,
      "grad_norm": 0.1311257779598236,
      "learning_rate": 4.011666666666667e-05,
      "loss": 0.0037,
      "step": 29650
    },
    {
      "epoch": 1.5818666666666665,
      "grad_norm": 0.16003021597862244,
      "learning_rate": 4.0113333333333334e-05,
      "loss": 0.0023,
      "step": 29660
    },
    {
      "epoch": 1.5824,
      "grad_norm": 0.09854749590158463,
      "learning_rate": 4.011e-05,
      "loss": 0.0023,
      "step": 29670
    },
    {
      "epoch": 1.5829333333333333,
      "grad_norm": 0.04828978702425957,
      "learning_rate": 4.0106666666666673e-05,
      "loss": 0.003,
      "step": 29680
    },
    {
      "epoch": 1.5834666666666668,
      "grad_norm": 0.2709956765174866,
      "learning_rate": 4.010333333333334e-05,
      "loss": 0.0022,
      "step": 29690
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.06828000396490097,
      "learning_rate": 4.0100000000000006e-05,
      "loss": 0.0026,
      "step": 29700
    },
    {
      "epoch": 1.5845333333333333,
      "grad_norm": 0.20137757062911987,
      "learning_rate": 4.009666666666667e-05,
      "loss": 0.004,
      "step": 29710
    },
    {
      "epoch": 1.5850666666666666,
      "grad_norm": 0.8068453073501587,
      "learning_rate": 4.009333333333333e-05,
      "loss": 0.0023,
      "step": 29720
    },
    {
      "epoch": 1.5856,
      "grad_norm": 0.21183563768863678,
      "learning_rate": 4.009e-05,
      "loss": 0.0029,
      "step": 29730
    },
    {
      "epoch": 1.5861333333333332,
      "grad_norm": 0.050082623958587646,
      "learning_rate": 4.0086666666666663e-05,
      "loss": 0.0033,
      "step": 29740
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 0.15215200185775757,
      "learning_rate": 4.0083333333333336e-05,
      "loss": 0.002,
      "step": 29750
    },
    {
      "epoch": 1.5872000000000002,
      "grad_norm": 0.6666367650032043,
      "learning_rate": 4.008e-05,
      "loss": 0.0027,
      "step": 29760
    },
    {
      "epoch": 1.5877333333333334,
      "grad_norm": 0.024303972721099854,
      "learning_rate": 4.007666666666667e-05,
      "loss": 0.0027,
      "step": 29770
    },
    {
      "epoch": 1.5882666666666667,
      "grad_norm": 0.02858172170817852,
      "learning_rate": 4.0073333333333335e-05,
      "loss": 0.0025,
      "step": 29780
    },
    {
      "epoch": 1.5888,
      "grad_norm": 0.06519455462694168,
      "learning_rate": 4.007e-05,
      "loss": 0.0029,
      "step": 29790
    },
    {
      "epoch": 1.5893333333333333,
      "grad_norm": 0.07104817777872086,
      "learning_rate": 4.006666666666667e-05,
      "loss": 0.0034,
      "step": 29800
    },
    {
      "epoch": 1.5898666666666665,
      "grad_norm": 0.21702741086483002,
      "learning_rate": 4.006333333333333e-05,
      "loss": 0.0038,
      "step": 29810
    },
    {
      "epoch": 1.5904,
      "grad_norm": 0.12185470759868622,
      "learning_rate": 4.0060000000000006e-05,
      "loss": 0.003,
      "step": 29820
    },
    {
      "epoch": 1.5909333333333333,
      "grad_norm": 0.8523053526878357,
      "learning_rate": 4.005666666666667e-05,
      "loss": 0.0026,
      "step": 29830
    },
    {
      "epoch": 1.5914666666666668,
      "grad_norm": 0.7254357933998108,
      "learning_rate": 4.005333333333334e-05,
      "loss": 0.0039,
      "step": 29840
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.3319758176803589,
      "learning_rate": 4.0050000000000004e-05,
      "loss": 0.0021,
      "step": 29850
    },
    {
      "epoch": 1.5925333333333334,
      "grad_norm": 0.3049428462982178,
      "learning_rate": 4.004666666666667e-05,
      "loss": 0.0025,
      "step": 29860
    },
    {
      "epoch": 1.5930666666666666,
      "grad_norm": 0.6995810866355896,
      "learning_rate": 4.004333333333333e-05,
      "loss": 0.0045,
      "step": 29870
    },
    {
      "epoch": 1.5936,
      "grad_norm": 0.2505682110786438,
      "learning_rate": 4.004e-05,
      "loss": 0.0045,
      "step": 29880
    },
    {
      "epoch": 1.5941333333333332,
      "grad_norm": 0.42484337091445923,
      "learning_rate": 4.003666666666667e-05,
      "loss": 0.0033,
      "step": 29890
    },
    {
      "epoch": 1.5946666666666667,
      "grad_norm": 0.24153433740139008,
      "learning_rate": 4.0033333333333335e-05,
      "loss": 0.0028,
      "step": 29900
    },
    {
      "epoch": 1.5952,
      "grad_norm": 0.39162689447402954,
      "learning_rate": 4.003e-05,
      "loss": 0.0026,
      "step": 29910
    },
    {
      "epoch": 1.5957333333333334,
      "grad_norm": 0.049932509660720825,
      "learning_rate": 4.002666666666667e-05,
      "loss": 0.0029,
      "step": 29920
    },
    {
      "epoch": 1.5962666666666667,
      "grad_norm": 0.36884456872940063,
      "learning_rate": 4.0023333333333334e-05,
      "loss": 0.0038,
      "step": 29930
    },
    {
      "epoch": 1.5968,
      "grad_norm": 0.6057701110839844,
      "learning_rate": 4.002e-05,
      "loss": 0.0029,
      "step": 29940
    },
    {
      "epoch": 1.5973333333333333,
      "grad_norm": 0.03519221022725105,
      "learning_rate": 4.0016666666666666e-05,
      "loss": 0.0039,
      "step": 29950
    },
    {
      "epoch": 1.5978666666666665,
      "grad_norm": 0.2851630449295044,
      "learning_rate": 4.001333333333334e-05,
      "loss": 0.0029,
      "step": 29960
    },
    {
      "epoch": 1.5984,
      "grad_norm": 0.034738972783088684,
      "learning_rate": 4.0010000000000005e-05,
      "loss": 0.0049,
      "step": 29970
    },
    {
      "epoch": 1.5989333333333333,
      "grad_norm": 0.4246135354042053,
      "learning_rate": 4.000666666666667e-05,
      "loss": 0.0033,
      "step": 29980
    },
    {
      "epoch": 1.5994666666666668,
      "grad_norm": 0.21477395296096802,
      "learning_rate": 4.000333333333334e-05,
      "loss": 0.0045,
      "step": 29990
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.33698275685310364,
      "learning_rate": 4e-05,
      "loss": 0.0037,
      "step": 30000
    },
    {
      "epoch": 1.6005333333333334,
      "grad_norm": 0.09838984161615372,
      "learning_rate": 3.999666666666667e-05,
      "loss": 0.0029,
      "step": 30010
    },
    {
      "epoch": 1.6010666666666666,
      "grad_norm": 0.3018079102039337,
      "learning_rate": 3.9993333333333336e-05,
      "loss": 0.0025,
      "step": 30020
    },
    {
      "epoch": 1.6016,
      "grad_norm": 0.21009765565395355,
      "learning_rate": 3.999e-05,
      "loss": 0.0033,
      "step": 30030
    },
    {
      "epoch": 1.6021333333333332,
      "grad_norm": 0.10245699435472488,
      "learning_rate": 3.998666666666667e-05,
      "loss": 0.0029,
      "step": 30040
    },
    {
      "epoch": 1.6026666666666667,
      "grad_norm": 0.2944447994232178,
      "learning_rate": 3.9983333333333334e-05,
      "loss": 0.0026,
      "step": 30050
    },
    {
      "epoch": 1.6032,
      "grad_norm": 0.30744123458862305,
      "learning_rate": 3.998e-05,
      "loss": 0.0031,
      "step": 30060
    },
    {
      "epoch": 1.6037333333333335,
      "grad_norm": 0.21791136264801025,
      "learning_rate": 3.9976666666666666e-05,
      "loss": 0.0033,
      "step": 30070
    },
    {
      "epoch": 1.6042666666666667,
      "grad_norm": 0.2433714121580124,
      "learning_rate": 3.997333333333333e-05,
      "loss": 0.0022,
      "step": 30080
    },
    {
      "epoch": 1.6048,
      "grad_norm": 0.3394016623497009,
      "learning_rate": 3.9970000000000005e-05,
      "loss": 0.0025,
      "step": 30090
    },
    {
      "epoch": 1.6053333333333333,
      "grad_norm": 0.05393064022064209,
      "learning_rate": 3.996666666666667e-05,
      "loss": 0.0027,
      "step": 30100
    },
    {
      "epoch": 1.6058666666666666,
      "grad_norm": 0.06979279965162277,
      "learning_rate": 3.996333333333334e-05,
      "loss": 0.0024,
      "step": 30110
    },
    {
      "epoch": 1.6064,
      "grad_norm": 0.4209883213043213,
      "learning_rate": 3.9960000000000004e-05,
      "loss": 0.0026,
      "step": 30120
    },
    {
      "epoch": 1.6069333333333333,
      "grad_norm": 0.24351482093334198,
      "learning_rate": 3.995666666666667e-05,
      "loss": 0.0021,
      "step": 30130
    },
    {
      "epoch": 1.6074666666666668,
      "grad_norm": 0.3042537569999695,
      "learning_rate": 3.9953333333333336e-05,
      "loss": 0.0026,
      "step": 30140
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.3665299117565155,
      "learning_rate": 3.995e-05,
      "loss": 0.0019,
      "step": 30150
    },
    {
      "epoch": 1.6085333333333334,
      "grad_norm": 0.3904826045036316,
      "learning_rate": 3.994666666666667e-05,
      "loss": 0.0044,
      "step": 30160
    },
    {
      "epoch": 1.6090666666666666,
      "grad_norm": 0.18184809386730194,
      "learning_rate": 3.9943333333333334e-05,
      "loss": 0.0027,
      "step": 30170
    },
    {
      "epoch": 1.6096,
      "grad_norm": 0.4041610658168793,
      "learning_rate": 3.994e-05,
      "loss": 0.0028,
      "step": 30180
    },
    {
      "epoch": 1.6101333333333332,
      "grad_norm": 0.3698067367076874,
      "learning_rate": 3.9936666666666667e-05,
      "loss": 0.0024,
      "step": 30190
    },
    {
      "epoch": 1.6106666666666667,
      "grad_norm": 0.045150645077228546,
      "learning_rate": 3.993333333333333e-05,
      "loss": 0.004,
      "step": 30200
    },
    {
      "epoch": 1.6112,
      "grad_norm": 0.051591914147138596,
      "learning_rate": 3.993e-05,
      "loss": 0.0028,
      "step": 30210
    },
    {
      "epoch": 1.6117333333333335,
      "grad_norm": 0.30562058091163635,
      "learning_rate": 3.9926666666666665e-05,
      "loss": 0.0029,
      "step": 30220
    },
    {
      "epoch": 1.6122666666666667,
      "grad_norm": 0.21422941982746124,
      "learning_rate": 3.992333333333334e-05,
      "loss": 0.0029,
      "step": 30230
    },
    {
      "epoch": 1.6128,
      "grad_norm": 0.2119266241788864,
      "learning_rate": 3.9920000000000004e-05,
      "loss": 0.0019,
      "step": 30240
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.015261523425579071,
      "learning_rate": 3.991666666666667e-05,
      "loss": 0.0029,
      "step": 30250
    },
    {
      "epoch": 1.6138666666666666,
      "grad_norm": 0.420271635055542,
      "learning_rate": 3.9913333333333336e-05,
      "loss": 0.0024,
      "step": 30260
    },
    {
      "epoch": 1.6143999999999998,
      "grad_norm": 0.5000753402709961,
      "learning_rate": 3.991e-05,
      "loss": 0.0026,
      "step": 30270
    },
    {
      "epoch": 1.6149333333333333,
      "grad_norm": 0.38570261001586914,
      "learning_rate": 3.990666666666667e-05,
      "loss": 0.0026,
      "step": 30280
    },
    {
      "epoch": 1.6154666666666668,
      "grad_norm": 0.23004430532455444,
      "learning_rate": 3.9903333333333335e-05,
      "loss": 0.0029,
      "step": 30290
    },
    {
      "epoch": 1.616,
      "grad_norm": 0.12161607295274734,
      "learning_rate": 3.99e-05,
      "loss": 0.0038,
      "step": 30300
    },
    {
      "epoch": 1.6165333333333334,
      "grad_norm": 0.046604856848716736,
      "learning_rate": 3.9896666666666674e-05,
      "loss": 0.0032,
      "step": 30310
    },
    {
      "epoch": 1.6170666666666667,
      "grad_norm": 0.4198765456676483,
      "learning_rate": 3.989333333333333e-05,
      "loss": 0.0017,
      "step": 30320
    },
    {
      "epoch": 1.6176,
      "grad_norm": 0.3603188395500183,
      "learning_rate": 3.989e-05,
      "loss": 0.0029,
      "step": 30330
    },
    {
      "epoch": 1.6181333333333332,
      "grad_norm": 0.21239730715751648,
      "learning_rate": 3.9886666666666665e-05,
      "loss": 0.0035,
      "step": 30340
    },
    {
      "epoch": 1.6186666666666667,
      "grad_norm": 0.1526118516921997,
      "learning_rate": 3.988333333333333e-05,
      "loss": 0.0032,
      "step": 30350
    },
    {
      "epoch": 1.6192,
      "grad_norm": 0.21413367986679077,
      "learning_rate": 3.988e-05,
      "loss": 0.0022,
      "step": 30360
    },
    {
      "epoch": 1.6197333333333335,
      "grad_norm": 0.0419163703918457,
      "learning_rate": 3.987666666666667e-05,
      "loss": 0.0031,
      "step": 30370
    },
    {
      "epoch": 1.6202666666666667,
      "grad_norm": 0.18545226752758026,
      "learning_rate": 3.987333333333334e-05,
      "loss": 0.0031,
      "step": 30380
    },
    {
      "epoch": 1.6208,
      "grad_norm": 0.48195362091064453,
      "learning_rate": 3.987e-05,
      "loss": 0.0028,
      "step": 30390
    },
    {
      "epoch": 1.6213333333333333,
      "grad_norm": 0.2783132493495941,
      "learning_rate": 3.986666666666667e-05,
      "loss": 0.0036,
      "step": 30400
    },
    {
      "epoch": 1.6218666666666666,
      "grad_norm": 0.40330255031585693,
      "learning_rate": 3.9863333333333335e-05,
      "loss": 0.0032,
      "step": 30410
    },
    {
      "epoch": 1.6223999999999998,
      "grad_norm": 0.0989881232380867,
      "learning_rate": 3.986e-05,
      "loss": 0.0034,
      "step": 30420
    },
    {
      "epoch": 1.6229333333333333,
      "grad_norm": 0.19118128716945648,
      "learning_rate": 3.985666666666667e-05,
      "loss": 0.0024,
      "step": 30430
    },
    {
      "epoch": 1.6234666666666666,
      "grad_norm": 0.18618744611740112,
      "learning_rate": 3.985333333333334e-05,
      "loss": 0.0026,
      "step": 30440
    },
    {
      "epoch": 1.624,
      "grad_norm": 0.45407119393348694,
      "learning_rate": 3.9850000000000006e-05,
      "loss": 0.0028,
      "step": 30450
    },
    {
      "epoch": 1.6245333333333334,
      "grad_norm": 0.30352655053138733,
      "learning_rate": 3.984666666666667e-05,
      "loss": 0.0025,
      "step": 30460
    },
    {
      "epoch": 1.6250666666666667,
      "grad_norm": 0.3694664537906647,
      "learning_rate": 3.984333333333333e-05,
      "loss": 0.002,
      "step": 30470
    },
    {
      "epoch": 1.6256,
      "grad_norm": 0.3066694140434265,
      "learning_rate": 3.984e-05,
      "loss": 0.0021,
      "step": 30480
    },
    {
      "epoch": 1.6261333333333332,
      "grad_norm": 0.3973653018474579,
      "learning_rate": 3.9836666666666664e-05,
      "loss": 0.0038,
      "step": 30490
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 0.36427611112594604,
      "learning_rate": 3.983333333333333e-05,
      "loss": 0.0028,
      "step": 30500
    },
    {
      "epoch": 1.6272,
      "grad_norm": 0.039236243814229965,
      "learning_rate": 3.983e-05,
      "loss": 0.002,
      "step": 30510
    },
    {
      "epoch": 1.6277333333333335,
      "grad_norm": 0.30166926980018616,
      "learning_rate": 3.982666666666667e-05,
      "loss": 0.0026,
      "step": 30520
    },
    {
      "epoch": 1.6282666666666668,
      "grad_norm": 0.15736302733421326,
      "learning_rate": 3.9823333333333335e-05,
      "loss": 0.0028,
      "step": 30530
    },
    {
      "epoch": 1.6288,
      "grad_norm": 0.18052645027637482,
      "learning_rate": 3.982e-05,
      "loss": 0.0023,
      "step": 30540
    },
    {
      "epoch": 1.6293333333333333,
      "grad_norm": 0.5667418837547302,
      "learning_rate": 3.981666666666667e-05,
      "loss": 0.0029,
      "step": 30550
    },
    {
      "epoch": 1.6298666666666666,
      "grad_norm": 0.583565890789032,
      "learning_rate": 3.9813333333333334e-05,
      "loss": 0.0038,
      "step": 30560
    },
    {
      "epoch": 1.6303999999999998,
      "grad_norm": 0.17079967260360718,
      "learning_rate": 3.981e-05,
      "loss": 0.0039,
      "step": 30570
    },
    {
      "epoch": 1.6309333333333333,
      "grad_norm": 0.3980081379413605,
      "learning_rate": 3.980666666666667e-05,
      "loss": 0.0046,
      "step": 30580
    },
    {
      "epoch": 1.6314666666666666,
      "grad_norm": 0.39444783329963684,
      "learning_rate": 3.980333333333334e-05,
      "loss": 0.0037,
      "step": 30590
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.3582000732421875,
      "learning_rate": 3.9800000000000005e-05,
      "loss": 0.0026,
      "step": 30600
    },
    {
      "epoch": 1.6325333333333334,
      "grad_norm": 0.15484003722667694,
      "learning_rate": 3.979666666666667e-05,
      "loss": 0.0034,
      "step": 30610
    },
    {
      "epoch": 1.6330666666666667,
      "grad_norm": 0.34031426906585693,
      "learning_rate": 3.979333333333333e-05,
      "loss": 0.0033,
      "step": 30620
    },
    {
      "epoch": 1.6336,
      "grad_norm": 0.28444433212280273,
      "learning_rate": 3.979e-05,
      "loss": 0.0028,
      "step": 30630
    },
    {
      "epoch": 1.6341333333333332,
      "grad_norm": 0.34974655508995056,
      "learning_rate": 3.978666666666667e-05,
      "loss": 0.0035,
      "step": 30640
    },
    {
      "epoch": 1.6346666666666667,
      "grad_norm": 0.10249479115009308,
      "learning_rate": 3.9783333333333336e-05,
      "loss": 0.0015,
      "step": 30650
    },
    {
      "epoch": 1.6352,
      "grad_norm": 0.15849262475967407,
      "learning_rate": 3.978e-05,
      "loss": 0.0036,
      "step": 30660
    },
    {
      "epoch": 1.6357333333333335,
      "grad_norm": 0.4033641517162323,
      "learning_rate": 3.977666666666667e-05,
      "loss": 0.0034,
      "step": 30670
    },
    {
      "epoch": 1.6362666666666668,
      "grad_norm": 0.15213096141815186,
      "learning_rate": 3.9773333333333334e-05,
      "loss": 0.0034,
      "step": 30680
    },
    {
      "epoch": 1.6368,
      "grad_norm": 0.6770357489585876,
      "learning_rate": 3.977e-05,
      "loss": 0.0036,
      "step": 30690
    },
    {
      "epoch": 1.6373333333333333,
      "grad_norm": 0.045827340334653854,
      "learning_rate": 3.9766666666666667e-05,
      "loss": 0.0024,
      "step": 30700
    },
    {
      "epoch": 1.6378666666666666,
      "grad_norm": 0.09043256938457489,
      "learning_rate": 3.976333333333333e-05,
      "loss": 0.0029,
      "step": 30710
    },
    {
      "epoch": 1.6383999999999999,
      "grad_norm": 0.5725167989730835,
      "learning_rate": 3.9760000000000006e-05,
      "loss": 0.0031,
      "step": 30720
    },
    {
      "epoch": 1.6389333333333334,
      "grad_norm": 0.6894278526306152,
      "learning_rate": 3.975666666666667e-05,
      "loss": 0.0036,
      "step": 30730
    },
    {
      "epoch": 1.6394666666666666,
      "grad_norm": 0.03231559321284294,
      "learning_rate": 3.975333333333334e-05,
      "loss": 0.0035,
      "step": 30740
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.5581649541854858,
      "learning_rate": 3.9750000000000004e-05,
      "loss": 0.0024,
      "step": 30750
    },
    {
      "epoch": 1.6405333333333334,
      "grad_norm": 0.467776358127594,
      "learning_rate": 3.974666666666667e-05,
      "loss": 0.0041,
      "step": 30760
    },
    {
      "epoch": 1.6410666666666667,
      "grad_norm": 0.30434659123420715,
      "learning_rate": 3.9743333333333336e-05,
      "loss": 0.0028,
      "step": 30770
    },
    {
      "epoch": 1.6416,
      "grad_norm": 0.256986141204834,
      "learning_rate": 3.974e-05,
      "loss": 0.0026,
      "step": 30780
    },
    {
      "epoch": 1.6421333333333332,
      "grad_norm": 0.3017522692680359,
      "learning_rate": 3.973666666666667e-05,
      "loss": 0.0028,
      "step": 30790
    },
    {
      "epoch": 1.6426666666666667,
      "grad_norm": 0.27706268429756165,
      "learning_rate": 3.9733333333333335e-05,
      "loss": 0.0022,
      "step": 30800
    },
    {
      "epoch": 1.6432,
      "grad_norm": 0.04679495468735695,
      "learning_rate": 3.973e-05,
      "loss": 0.0045,
      "step": 30810
    },
    {
      "epoch": 1.6437333333333335,
      "grad_norm": 0.48517686128616333,
      "learning_rate": 3.972666666666667e-05,
      "loss": 0.0039,
      "step": 30820
    },
    {
      "epoch": 1.6442666666666668,
      "grad_norm": 0.2820270359516144,
      "learning_rate": 3.972333333333333e-05,
      "loss": 0.0032,
      "step": 30830
    },
    {
      "epoch": 1.6448,
      "grad_norm": 0.2721233665943146,
      "learning_rate": 3.972e-05,
      "loss": 0.003,
      "step": 30840
    },
    {
      "epoch": 1.6453333333333333,
      "grad_norm": 0.19284340739250183,
      "learning_rate": 3.9716666666666665e-05,
      "loss": 0.0016,
      "step": 30850
    },
    {
      "epoch": 1.6458666666666666,
      "grad_norm": 0.3072218596935272,
      "learning_rate": 3.971333333333334e-05,
      "loss": 0.0033,
      "step": 30860
    },
    {
      "epoch": 1.6463999999999999,
      "grad_norm": 0.2208620011806488,
      "learning_rate": 3.9710000000000004e-05,
      "loss": 0.0035,
      "step": 30870
    },
    {
      "epoch": 1.6469333333333334,
      "grad_norm": 0.2425147145986557,
      "learning_rate": 3.970666666666667e-05,
      "loss": 0.0027,
      "step": 30880
    },
    {
      "epoch": 1.6474666666666666,
      "grad_norm": 0.12243202328681946,
      "learning_rate": 3.970333333333334e-05,
      "loss": 0.0024,
      "step": 30890
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.07941367477178574,
      "learning_rate": 3.97e-05,
      "loss": 0.003,
      "step": 30900
    },
    {
      "epoch": 1.6485333333333334,
      "grad_norm": 0.4258778691291809,
      "learning_rate": 3.969666666666667e-05,
      "loss": 0.0032,
      "step": 30910
    },
    {
      "epoch": 1.6490666666666667,
      "grad_norm": 0.10583958029747009,
      "learning_rate": 3.9693333333333335e-05,
      "loss": 0.0031,
      "step": 30920
    },
    {
      "epoch": 1.6496,
      "grad_norm": 0.24247275292873383,
      "learning_rate": 3.969e-05,
      "loss": 0.0028,
      "step": 30930
    },
    {
      "epoch": 1.6501333333333332,
      "grad_norm": 0.05309455841779709,
      "learning_rate": 3.968666666666667e-05,
      "loss": 0.003,
      "step": 30940
    },
    {
      "epoch": 1.6506666666666665,
      "grad_norm": 0.3323560357093811,
      "learning_rate": 3.9683333333333333e-05,
      "loss": 0.005,
      "step": 30950
    },
    {
      "epoch": 1.6512,
      "grad_norm": 0.3024159371852875,
      "learning_rate": 3.968e-05,
      "loss": 0.0033,
      "step": 30960
    },
    {
      "epoch": 1.6517333333333335,
      "grad_norm": 0.31885626912117004,
      "learning_rate": 3.9676666666666666e-05,
      "loss": 0.0029,
      "step": 30970
    },
    {
      "epoch": 1.6522666666666668,
      "grad_norm": 0.22468708455562592,
      "learning_rate": 3.967333333333333e-05,
      "loss": 0.003,
      "step": 30980
    },
    {
      "epoch": 1.6528,
      "grad_norm": 0.09066907316446304,
      "learning_rate": 3.9670000000000005e-05,
      "loss": 0.003,
      "step": 30990
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 0.06359708309173584,
      "learning_rate": 3.966666666666667e-05,
      "loss": 0.0028,
      "step": 31000
    },
    {
      "epoch": 1.6538666666666666,
      "grad_norm": 0.25444450974464417,
      "learning_rate": 3.966333333333334e-05,
      "loss": 0.0025,
      "step": 31010
    },
    {
      "epoch": 1.6543999999999999,
      "grad_norm": 0.04290420934557915,
      "learning_rate": 3.966e-05,
      "loss": 0.0022,
      "step": 31020
    },
    {
      "epoch": 1.6549333333333334,
      "grad_norm": 0.27422642707824707,
      "learning_rate": 3.965666666666667e-05,
      "loss": 0.0025,
      "step": 31030
    },
    {
      "epoch": 1.6554666666666666,
      "grad_norm": 0.3889583647251129,
      "learning_rate": 3.9653333333333335e-05,
      "loss": 0.0017,
      "step": 31040
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 0.42519262433052063,
      "learning_rate": 3.965e-05,
      "loss": 0.0028,
      "step": 31050
    },
    {
      "epoch": 1.6565333333333334,
      "grad_norm": 0.24971774220466614,
      "learning_rate": 3.964666666666667e-05,
      "loss": 0.0039,
      "step": 31060
    },
    {
      "epoch": 1.6570666666666667,
      "grad_norm": 0.5713549256324768,
      "learning_rate": 3.964333333333334e-05,
      "loss": 0.0022,
      "step": 31070
    },
    {
      "epoch": 1.6576,
      "grad_norm": 0.4778133034706116,
      "learning_rate": 3.964e-05,
      "loss": 0.0026,
      "step": 31080
    },
    {
      "epoch": 1.6581333333333332,
      "grad_norm": 0.7530913352966309,
      "learning_rate": 3.9636666666666666e-05,
      "loss": 0.0033,
      "step": 31090
    },
    {
      "epoch": 1.6586666666666665,
      "grad_norm": 0.15203353762626648,
      "learning_rate": 3.963333333333333e-05,
      "loss": 0.0023,
      "step": 31100
    },
    {
      "epoch": 1.6592,
      "grad_norm": 0.42246270179748535,
      "learning_rate": 3.963e-05,
      "loss": 0.0036,
      "step": 31110
    },
    {
      "epoch": 1.6597333333333333,
      "grad_norm": 0.17975425720214844,
      "learning_rate": 3.9626666666666664e-05,
      "loss": 0.0023,
      "step": 31120
    },
    {
      "epoch": 1.6602666666666668,
      "grad_norm": 0.277759850025177,
      "learning_rate": 3.962333333333334e-05,
      "loss": 0.0036,
      "step": 31130
    },
    {
      "epoch": 1.6608,
      "grad_norm": 0.2132662683725357,
      "learning_rate": 3.9620000000000004e-05,
      "loss": 0.0041,
      "step": 31140
    },
    {
      "epoch": 1.6613333333333333,
      "grad_norm": 0.06966067105531693,
      "learning_rate": 3.961666666666667e-05,
      "loss": 0.0019,
      "step": 31150
    },
    {
      "epoch": 1.6618666666666666,
      "grad_norm": 0.24112696945667267,
      "learning_rate": 3.9613333333333336e-05,
      "loss": 0.0024,
      "step": 31160
    },
    {
      "epoch": 1.6623999999999999,
      "grad_norm": 0.2733452618122101,
      "learning_rate": 3.961e-05,
      "loss": 0.003,
      "step": 31170
    },
    {
      "epoch": 1.6629333333333334,
      "grad_norm": 0.09292568266391754,
      "learning_rate": 3.960666666666667e-05,
      "loss": 0.0041,
      "step": 31180
    },
    {
      "epoch": 1.6634666666666666,
      "grad_norm": 0.26977092027664185,
      "learning_rate": 3.9603333333333334e-05,
      "loss": 0.0021,
      "step": 31190
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.3353685140609741,
      "learning_rate": 3.960000000000001e-05,
      "loss": 0.002,
      "step": 31200
    },
    {
      "epoch": 1.6645333333333334,
      "grad_norm": 0.7213655114173889,
      "learning_rate": 3.959666666666667e-05,
      "loss": 0.0035,
      "step": 31210
    },
    {
      "epoch": 1.6650666666666667,
      "grad_norm": 0.16456268727779388,
      "learning_rate": 3.959333333333334e-05,
      "loss": 0.0029,
      "step": 31220
    },
    {
      "epoch": 1.6656,
      "grad_norm": 0.07340759038925171,
      "learning_rate": 3.959e-05,
      "loss": 0.0036,
      "step": 31230
    },
    {
      "epoch": 1.6661333333333332,
      "grad_norm": 0.24396716058254242,
      "learning_rate": 3.9586666666666665e-05,
      "loss": 0.0026,
      "step": 31240
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.07867751270532608,
      "learning_rate": 3.958333333333333e-05,
      "loss": 0.0026,
      "step": 31250
    },
    {
      "epoch": 1.6672,
      "grad_norm": 0.5121370553970337,
      "learning_rate": 3.958e-05,
      "loss": 0.0033,
      "step": 31260
    },
    {
      "epoch": 1.6677333333333333,
      "grad_norm": 0.04891251027584076,
      "learning_rate": 3.957666666666667e-05,
      "loss": 0.0029,
      "step": 31270
    },
    {
      "epoch": 1.6682666666666668,
      "grad_norm": 0.12042620778083801,
      "learning_rate": 3.9573333333333336e-05,
      "loss": 0.0028,
      "step": 31280
    },
    {
      "epoch": 1.6688,
      "grad_norm": 0.307327002286911,
      "learning_rate": 3.957e-05,
      "loss": 0.003,
      "step": 31290
    },
    {
      "epoch": 1.6693333333333333,
      "grad_norm": 0.2149796038866043,
      "learning_rate": 3.956666666666667e-05,
      "loss": 0.0033,
      "step": 31300
    },
    {
      "epoch": 1.6698666666666666,
      "grad_norm": 0.4052995443344116,
      "learning_rate": 3.9563333333333335e-05,
      "loss": 0.0025,
      "step": 31310
    },
    {
      "epoch": 1.6703999999999999,
      "grad_norm": 0.0736321210861206,
      "learning_rate": 3.956e-05,
      "loss": 0.0024,
      "step": 31320
    },
    {
      "epoch": 1.6709333333333334,
      "grad_norm": 0.3004867434501648,
      "learning_rate": 3.955666666666667e-05,
      "loss": 0.0027,
      "step": 31330
    },
    {
      "epoch": 1.6714666666666667,
      "grad_norm": 0.1641009896993637,
      "learning_rate": 3.955333333333334e-05,
      "loss": 0.0031,
      "step": 31340
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.5385472774505615,
      "learning_rate": 3.9550000000000006e-05,
      "loss": 0.0041,
      "step": 31350
    },
    {
      "epoch": 1.6725333333333334,
      "grad_norm": 0.31846562027931213,
      "learning_rate": 3.954666666666667e-05,
      "loss": 0.0024,
      "step": 31360
    },
    {
      "epoch": 1.6730666666666667,
      "grad_norm": 0.3987770974636078,
      "learning_rate": 3.954333333333334e-05,
      "loss": 0.0042,
      "step": 31370
    },
    {
      "epoch": 1.6736,
      "grad_norm": 0.2702581286430359,
      "learning_rate": 3.954e-05,
      "loss": 0.0033,
      "step": 31380
    },
    {
      "epoch": 1.6741333333333333,
      "grad_norm": 0.4964970350265503,
      "learning_rate": 3.9536666666666664e-05,
      "loss": 0.0033,
      "step": 31390
    },
    {
      "epoch": 1.6746666666666665,
      "grad_norm": 0.1294550746679306,
      "learning_rate": 3.9533333333333337e-05,
      "loss": 0.0021,
      "step": 31400
    },
    {
      "epoch": 1.6752,
      "grad_norm": 0.16304542124271393,
      "learning_rate": 3.953e-05,
      "loss": 0.0028,
      "step": 31410
    },
    {
      "epoch": 1.6757333333333333,
      "grad_norm": 0.18221649527549744,
      "learning_rate": 3.952666666666667e-05,
      "loss": 0.0026,
      "step": 31420
    },
    {
      "epoch": 1.6762666666666668,
      "grad_norm": 0.3075654208660126,
      "learning_rate": 3.9523333333333335e-05,
      "loss": 0.0032,
      "step": 31430
    },
    {
      "epoch": 1.6768,
      "grad_norm": 0.4887363016605377,
      "learning_rate": 3.952e-05,
      "loss": 0.0027,
      "step": 31440
    },
    {
      "epoch": 1.6773333333333333,
      "grad_norm": 0.08833317458629608,
      "learning_rate": 3.951666666666667e-05,
      "loss": 0.003,
      "step": 31450
    },
    {
      "epoch": 1.6778666666666666,
      "grad_norm": 0.3122345507144928,
      "learning_rate": 3.951333333333333e-05,
      "loss": 0.0029,
      "step": 31460
    },
    {
      "epoch": 1.6784,
      "grad_norm": 0.39893996715545654,
      "learning_rate": 3.951e-05,
      "loss": 0.0027,
      "step": 31470
    },
    {
      "epoch": 1.6789333333333334,
      "grad_norm": 0.07586731016635895,
      "learning_rate": 3.950666666666667e-05,
      "loss": 0.0026,
      "step": 31480
    },
    {
      "epoch": 1.6794666666666667,
      "grad_norm": 0.30511537194252014,
      "learning_rate": 3.950333333333334e-05,
      "loss": 0.0036,
      "step": 31490
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.6145039200782776,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 0.0037,
      "step": 31500
    },
    {
      "epoch": 1.6805333333333334,
      "grad_norm": 0.6688032746315002,
      "learning_rate": 3.949666666666667e-05,
      "loss": 0.0023,
      "step": 31510
    },
    {
      "epoch": 1.6810666666666667,
      "grad_norm": 0.30271103978157043,
      "learning_rate": 3.949333333333334e-05,
      "loss": 0.0021,
      "step": 31520
    },
    {
      "epoch": 1.6816,
      "grad_norm": 0.4555071294307709,
      "learning_rate": 3.9489999999999996e-05,
      "loss": 0.0028,
      "step": 31530
    },
    {
      "epoch": 1.6821333333333333,
      "grad_norm": 0.5189014077186584,
      "learning_rate": 3.948666666666667e-05,
      "loss": 0.0031,
      "step": 31540
    },
    {
      "epoch": 1.6826666666666665,
      "grad_norm": 0.21473537385463715,
      "learning_rate": 3.9483333333333335e-05,
      "loss": 0.003,
      "step": 31550
    },
    {
      "epoch": 1.6832,
      "grad_norm": 0.12224081158638,
      "learning_rate": 3.948e-05,
      "loss": 0.0024,
      "step": 31560
    },
    {
      "epoch": 1.6837333333333333,
      "grad_norm": 0.24502117931842804,
      "learning_rate": 3.947666666666667e-05,
      "loss": 0.0031,
      "step": 31570
    },
    {
      "epoch": 1.6842666666666668,
      "grad_norm": 0.33651742339134216,
      "learning_rate": 3.9473333333333334e-05,
      "loss": 0.0033,
      "step": 31580
    },
    {
      "epoch": 1.6848,
      "grad_norm": 0.18073508143424988,
      "learning_rate": 3.947e-05,
      "loss": 0.0027,
      "step": 31590
    },
    {
      "epoch": 1.6853333333333333,
      "grad_norm": 0.12431059032678604,
      "learning_rate": 3.9466666666666666e-05,
      "loss": 0.0034,
      "step": 31600
    },
    {
      "epoch": 1.6858666666666666,
      "grad_norm": 0.064970463514328,
      "learning_rate": 3.946333333333333e-05,
      "loss": 0.0024,
      "step": 31610
    },
    {
      "epoch": 1.6864,
      "grad_norm": 0.6322084069252014,
      "learning_rate": 3.9460000000000005e-05,
      "loss": 0.0021,
      "step": 31620
    },
    {
      "epoch": 1.6869333333333332,
      "grad_norm": 0.43268439173698425,
      "learning_rate": 3.945666666666667e-05,
      "loss": 0.0028,
      "step": 31630
    },
    {
      "epoch": 1.6874666666666667,
      "grad_norm": 0.16558285057544708,
      "learning_rate": 3.945333333333334e-05,
      "loss": 0.0019,
      "step": 31640
    },
    {
      "epoch": 1.688,
      "grad_norm": 0.2740289568901062,
      "learning_rate": 3.9450000000000003e-05,
      "loss": 0.0032,
      "step": 31650
    },
    {
      "epoch": 1.6885333333333334,
      "grad_norm": 0.03613712266087532,
      "learning_rate": 3.944666666666667e-05,
      "loss": 0.0021,
      "step": 31660
    },
    {
      "epoch": 1.6890666666666667,
      "grad_norm": 0.4815882444381714,
      "learning_rate": 3.9443333333333336e-05,
      "loss": 0.0034,
      "step": 31670
    },
    {
      "epoch": 1.6896,
      "grad_norm": 0.04794551804661751,
      "learning_rate": 3.944e-05,
      "loss": 0.0032,
      "step": 31680
    },
    {
      "epoch": 1.6901333333333333,
      "grad_norm": 0.0401759073138237,
      "learning_rate": 3.943666666666667e-05,
      "loss": 0.0029,
      "step": 31690
    },
    {
      "epoch": 1.6906666666666665,
      "grad_norm": 0.15156355500221252,
      "learning_rate": 3.9433333333333334e-05,
      "loss": 0.0021,
      "step": 31700
    },
    {
      "epoch": 1.6912,
      "grad_norm": 0.09617392718791962,
      "learning_rate": 3.943e-05,
      "loss": 0.0022,
      "step": 31710
    },
    {
      "epoch": 1.6917333333333333,
      "grad_norm": 0.06746567785739899,
      "learning_rate": 3.9426666666666666e-05,
      "loss": 0.0028,
      "step": 31720
    },
    {
      "epoch": 1.6922666666666668,
      "grad_norm": 0.4584397077560425,
      "learning_rate": 3.942333333333333e-05,
      "loss": 0.0042,
      "step": 31730
    },
    {
      "epoch": 1.6928,
      "grad_norm": 0.1251024454832077,
      "learning_rate": 3.942e-05,
      "loss": 0.0035,
      "step": 31740
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.2402515709400177,
      "learning_rate": 3.941666666666667e-05,
      "loss": 0.0021,
      "step": 31750
    },
    {
      "epoch": 1.6938666666666666,
      "grad_norm": 0.18938295543193817,
      "learning_rate": 3.941333333333334e-05,
      "loss": 0.0027,
      "step": 31760
    },
    {
      "epoch": 1.6944,
      "grad_norm": 0.10069393366575241,
      "learning_rate": 3.9410000000000004e-05,
      "loss": 0.0036,
      "step": 31770
    },
    {
      "epoch": 1.6949333333333332,
      "grad_norm": 0.42227035760879517,
      "learning_rate": 3.940666666666667e-05,
      "loss": 0.003,
      "step": 31780
    },
    {
      "epoch": 1.6954666666666667,
      "grad_norm": 0.44926491379737854,
      "learning_rate": 3.9403333333333336e-05,
      "loss": 0.0036,
      "step": 31790
    },
    {
      "epoch": 1.696,
      "grad_norm": 0.07212620228528976,
      "learning_rate": 3.94e-05,
      "loss": 0.0029,
      "step": 31800
    },
    {
      "epoch": 1.6965333333333334,
      "grad_norm": 0.2126363217830658,
      "learning_rate": 3.939666666666667e-05,
      "loss": 0.0025,
      "step": 31810
    },
    {
      "epoch": 1.6970666666666667,
      "grad_norm": 0.025692639872431755,
      "learning_rate": 3.9393333333333335e-05,
      "loss": 0.0021,
      "step": 31820
    },
    {
      "epoch": 1.6976,
      "grad_norm": 0.27123674750328064,
      "learning_rate": 3.939e-05,
      "loss": 0.0029,
      "step": 31830
    },
    {
      "epoch": 1.6981333333333333,
      "grad_norm": 0.44922104477882385,
      "learning_rate": 3.938666666666667e-05,
      "loss": 0.0023,
      "step": 31840
    },
    {
      "epoch": 1.6986666666666665,
      "grad_norm": 0.4332582652568817,
      "learning_rate": 3.938333333333333e-05,
      "loss": 0.0032,
      "step": 31850
    },
    {
      "epoch": 1.6992,
      "grad_norm": 0.6927719116210938,
      "learning_rate": 3.938e-05,
      "loss": 0.004,
      "step": 31860
    },
    {
      "epoch": 1.6997333333333333,
      "grad_norm": 0.7855706810951233,
      "learning_rate": 3.9376666666666665e-05,
      "loss": 0.0044,
      "step": 31870
    },
    {
      "epoch": 1.7002666666666668,
      "grad_norm": 0.5998712182044983,
      "learning_rate": 3.937333333333333e-05,
      "loss": 0.004,
      "step": 31880
    },
    {
      "epoch": 1.7008,
      "grad_norm": 0.42067259550094604,
      "learning_rate": 3.9370000000000004e-05,
      "loss": 0.003,
      "step": 31890
    },
    {
      "epoch": 1.7013333333333334,
      "grad_norm": 0.47970134019851685,
      "learning_rate": 3.936666666666667e-05,
      "loss": 0.0045,
      "step": 31900
    },
    {
      "epoch": 1.7018666666666666,
      "grad_norm": 0.2698215842247009,
      "learning_rate": 3.9363333333333336e-05,
      "loss": 0.0028,
      "step": 31910
    },
    {
      "epoch": 1.7024,
      "grad_norm": 0.2709486782550812,
      "learning_rate": 3.936e-05,
      "loss": 0.003,
      "step": 31920
    },
    {
      "epoch": 1.7029333333333332,
      "grad_norm": 0.4802957773208618,
      "learning_rate": 3.935666666666667e-05,
      "loss": 0.002,
      "step": 31930
    },
    {
      "epoch": 1.7034666666666667,
      "grad_norm": 0.12210120260715485,
      "learning_rate": 3.9353333333333335e-05,
      "loss": 0.002,
      "step": 31940
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.053308092057704926,
      "learning_rate": 3.935e-05,
      "loss": 0.0037,
      "step": 31950
    },
    {
      "epoch": 1.7045333333333335,
      "grad_norm": 0.04574529081583023,
      "learning_rate": 3.9346666666666674e-05,
      "loss": 0.0032,
      "step": 31960
    },
    {
      "epoch": 1.7050666666666667,
      "grad_norm": 0.41900700330734253,
      "learning_rate": 3.934333333333334e-05,
      "loss": 0.0021,
      "step": 31970
    },
    {
      "epoch": 1.7056,
      "grad_norm": 0.21366435289382935,
      "learning_rate": 3.9340000000000006e-05,
      "loss": 0.0029,
      "step": 31980
    },
    {
      "epoch": 1.7061333333333333,
      "grad_norm": 0.24158747494220734,
      "learning_rate": 3.9336666666666666e-05,
      "loss": 0.0038,
      "step": 31990
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.39199113845825195,
      "learning_rate": 3.933333333333333e-05,
      "loss": 0.0018,
      "step": 32000
    },
    {
      "epoch": 1.7072,
      "grad_norm": 0.36674800515174866,
      "learning_rate": 3.933e-05,
      "loss": 0.0042,
      "step": 32010
    },
    {
      "epoch": 1.7077333333333333,
      "grad_norm": 0.2899552881717682,
      "learning_rate": 3.9326666666666664e-05,
      "loss": 0.0022,
      "step": 32020
    },
    {
      "epoch": 1.7082666666666668,
      "grad_norm": 0.3025219738483429,
      "learning_rate": 3.932333333333334e-05,
      "loss": 0.004,
      "step": 32030
    },
    {
      "epoch": 1.7088,
      "grad_norm": 0.3309989273548126,
      "learning_rate": 3.932e-05,
      "loss": 0.0026,
      "step": 32040
    },
    {
      "epoch": 1.7093333333333334,
      "grad_norm": 0.3044678270816803,
      "learning_rate": 3.931666666666667e-05,
      "loss": 0.0032,
      "step": 32050
    },
    {
      "epoch": 1.7098666666666666,
      "grad_norm": 0.13173146545886993,
      "learning_rate": 3.9313333333333335e-05,
      "loss": 0.0027,
      "step": 32060
    },
    {
      "epoch": 1.7104,
      "grad_norm": 0.05680167302489281,
      "learning_rate": 3.931e-05,
      "loss": 0.0018,
      "step": 32070
    },
    {
      "epoch": 1.7109333333333332,
      "grad_norm": 0.03436967730522156,
      "learning_rate": 3.930666666666667e-05,
      "loss": 0.0044,
      "step": 32080
    },
    {
      "epoch": 1.7114666666666667,
      "grad_norm": 0.12269257754087448,
      "learning_rate": 3.9303333333333334e-05,
      "loss": 0.0026,
      "step": 32090
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.03526126593351364,
      "learning_rate": 3.9300000000000007e-05,
      "loss": 0.003,
      "step": 32100
    },
    {
      "epoch": 1.7125333333333335,
      "grad_norm": 0.15469051897525787,
      "learning_rate": 3.929666666666667e-05,
      "loss": 0.0027,
      "step": 32110
    },
    {
      "epoch": 1.7130666666666667,
      "grad_norm": 0.8165714740753174,
      "learning_rate": 3.929333333333334e-05,
      "loss": 0.0028,
      "step": 32120
    },
    {
      "epoch": 1.7136,
      "grad_norm": 0.39005205035209656,
      "learning_rate": 3.9290000000000005e-05,
      "loss": 0.002,
      "step": 32130
    },
    {
      "epoch": 1.7141333333333333,
      "grad_norm": 0.18823519349098206,
      "learning_rate": 3.9286666666666664e-05,
      "loss": 0.0031,
      "step": 32140
    },
    {
      "epoch": 1.7146666666666666,
      "grad_norm": 0.027551164850592613,
      "learning_rate": 3.928333333333333e-05,
      "loss": 0.004,
      "step": 32150
    },
    {
      "epoch": 1.7151999999999998,
      "grad_norm": 0.2129821628332138,
      "learning_rate": 3.9280000000000003e-05,
      "loss": 0.0031,
      "step": 32160
    },
    {
      "epoch": 1.7157333333333333,
      "grad_norm": 0.3668793737888336,
      "learning_rate": 3.927666666666667e-05,
      "loss": 0.0035,
      "step": 32170
    },
    {
      "epoch": 1.7162666666666668,
      "grad_norm": 0.5772973895072937,
      "learning_rate": 3.9273333333333336e-05,
      "loss": 0.003,
      "step": 32180
    },
    {
      "epoch": 1.7168,
      "grad_norm": 0.36018750071525574,
      "learning_rate": 3.927e-05,
      "loss": 0.0026,
      "step": 32190
    },
    {
      "epoch": 1.7173333333333334,
      "grad_norm": 0.24152809381484985,
      "learning_rate": 3.926666666666667e-05,
      "loss": 0.0021,
      "step": 32200
    },
    {
      "epoch": 1.7178666666666667,
      "grad_norm": 0.041397012770175934,
      "learning_rate": 3.9263333333333334e-05,
      "loss": 0.0026,
      "step": 32210
    },
    {
      "epoch": 1.7184,
      "grad_norm": 0.1852765679359436,
      "learning_rate": 3.926e-05,
      "loss": 0.0024,
      "step": 32220
    },
    {
      "epoch": 1.7189333333333332,
      "grad_norm": 0.07702652364969254,
      "learning_rate": 3.9256666666666666e-05,
      "loss": 0.0031,
      "step": 32230
    },
    {
      "epoch": 1.7194666666666667,
      "grad_norm": 0.216173455119133,
      "learning_rate": 3.925333333333334e-05,
      "loss": 0.0022,
      "step": 32240
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.5204752683639526,
      "learning_rate": 3.9250000000000005e-05,
      "loss": 0.0038,
      "step": 32250
    },
    {
      "epoch": 1.7205333333333335,
      "grad_norm": 0.7064628601074219,
      "learning_rate": 3.924666666666667e-05,
      "loss": 0.0023,
      "step": 32260
    },
    {
      "epoch": 1.7210666666666667,
      "grad_norm": 0.057434119284152985,
      "learning_rate": 3.924333333333334e-05,
      "loss": 0.0056,
      "step": 32270
    },
    {
      "epoch": 1.7216,
      "grad_norm": 0.4583311378955841,
      "learning_rate": 3.9240000000000004e-05,
      "loss": 0.0032,
      "step": 32280
    },
    {
      "epoch": 1.7221333333333333,
      "grad_norm": 0.5098803639411926,
      "learning_rate": 3.923666666666666e-05,
      "loss": 0.0024,
      "step": 32290
    },
    {
      "epoch": 1.7226666666666666,
      "grad_norm": 0.5813932418823242,
      "learning_rate": 3.9233333333333336e-05,
      "loss": 0.0038,
      "step": 32300
    },
    {
      "epoch": 1.7231999999999998,
      "grad_norm": 0.3090464174747467,
      "learning_rate": 3.923e-05,
      "loss": 0.0028,
      "step": 32310
    },
    {
      "epoch": 1.7237333333333333,
      "grad_norm": 0.5449963808059692,
      "learning_rate": 3.922666666666667e-05,
      "loss": 0.0023,
      "step": 32320
    },
    {
      "epoch": 1.7242666666666666,
      "grad_norm": 0.5898717641830444,
      "learning_rate": 3.9223333333333334e-05,
      "loss": 0.0024,
      "step": 32330
    },
    {
      "epoch": 1.7248,
      "grad_norm": 0.36304858326911926,
      "learning_rate": 3.922e-05,
      "loss": 0.0032,
      "step": 32340
    },
    {
      "epoch": 1.7253333333333334,
      "grad_norm": 0.2858288288116455,
      "learning_rate": 3.921666666666667e-05,
      "loss": 0.002,
      "step": 32350
    },
    {
      "epoch": 1.7258666666666667,
      "grad_norm": 0.15037953853607178,
      "learning_rate": 3.921333333333333e-05,
      "loss": 0.0027,
      "step": 32360
    },
    {
      "epoch": 1.7264,
      "grad_norm": 0.12234468013048172,
      "learning_rate": 3.921e-05,
      "loss": 0.0023,
      "step": 32370
    },
    {
      "epoch": 1.7269333333333332,
      "grad_norm": 0.021742964163422585,
      "learning_rate": 3.920666666666667e-05,
      "loss": 0.0024,
      "step": 32380
    },
    {
      "epoch": 1.7274666666666667,
      "grad_norm": 0.696455717086792,
      "learning_rate": 3.920333333333334e-05,
      "loss": 0.0026,
      "step": 32390
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.7815154790878296,
      "learning_rate": 3.9200000000000004e-05,
      "loss": 0.0029,
      "step": 32400
    },
    {
      "epoch": 1.7285333333333335,
      "grad_norm": 0.5552877187728882,
      "learning_rate": 3.919666666666667e-05,
      "loss": 0.003,
      "step": 32410
    },
    {
      "epoch": 1.7290666666666668,
      "grad_norm": 0.2109946608543396,
      "learning_rate": 3.9193333333333336e-05,
      "loss": 0.0048,
      "step": 32420
    },
    {
      "epoch": 1.7296,
      "grad_norm": 0.6349713802337646,
      "learning_rate": 3.919e-05,
      "loss": 0.0034,
      "step": 32430
    },
    {
      "epoch": 1.7301333333333333,
      "grad_norm": 0.45908084511756897,
      "learning_rate": 3.918666666666667e-05,
      "loss": 0.0032,
      "step": 32440
    },
    {
      "epoch": 1.7306666666666666,
      "grad_norm": 0.07311300188302994,
      "learning_rate": 3.9183333333333335e-05,
      "loss": 0.0039,
      "step": 32450
    },
    {
      "epoch": 1.7311999999999999,
      "grad_norm": 0.3618549406528473,
      "learning_rate": 3.918e-05,
      "loss": 0.0037,
      "step": 32460
    },
    {
      "epoch": 1.7317333333333333,
      "grad_norm": 0.12085900455713272,
      "learning_rate": 3.917666666666667e-05,
      "loss": 0.002,
      "step": 32470
    },
    {
      "epoch": 1.7322666666666666,
      "grad_norm": 0.06391891092061996,
      "learning_rate": 3.917333333333333e-05,
      "loss": 0.0029,
      "step": 32480
    },
    {
      "epoch": 1.7328000000000001,
      "grad_norm": 0.3581777513027191,
      "learning_rate": 3.917e-05,
      "loss": 0.004,
      "step": 32490
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.5995410084724426,
      "learning_rate": 3.9166666666666665e-05,
      "loss": 0.0027,
      "step": 32500
    },
    {
      "epoch": 1.7338666666666667,
      "grad_norm": 0.48701879382133484,
      "learning_rate": 3.916333333333334e-05,
      "loss": 0.0029,
      "step": 32510
    },
    {
      "epoch": 1.7344,
      "grad_norm": 0.07198929786682129,
      "learning_rate": 3.9160000000000005e-05,
      "loss": 0.0026,
      "step": 32520
    },
    {
      "epoch": 1.7349333333333332,
      "grad_norm": 0.2106107920408249,
      "learning_rate": 3.915666666666667e-05,
      "loss": 0.0026,
      "step": 32530
    },
    {
      "epoch": 1.7354666666666667,
      "grad_norm": 0.05354355275630951,
      "learning_rate": 3.915333333333334e-05,
      "loss": 0.0023,
      "step": 32540
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.0650874599814415,
      "learning_rate": 3.915e-05,
      "loss": 0.0033,
      "step": 32550
    },
    {
      "epoch": 1.7365333333333335,
      "grad_norm": 0.4050346910953522,
      "learning_rate": 3.914666666666667e-05,
      "loss": 0.0026,
      "step": 32560
    },
    {
      "epoch": 1.7370666666666668,
      "grad_norm": 0.07230307906866074,
      "learning_rate": 3.9143333333333335e-05,
      "loss": 0.0028,
      "step": 32570
    },
    {
      "epoch": 1.7376,
      "grad_norm": 0.4178175926208496,
      "learning_rate": 3.914e-05,
      "loss": 0.003,
      "step": 32580
    },
    {
      "epoch": 1.7381333333333333,
      "grad_norm": 0.2793850898742676,
      "learning_rate": 3.913666666666667e-05,
      "loss": 0.0033,
      "step": 32590
    },
    {
      "epoch": 1.7386666666666666,
      "grad_norm": 0.2515645921230316,
      "learning_rate": 3.9133333333333334e-05,
      "loss": 0.0032,
      "step": 32600
    },
    {
      "epoch": 1.7391999999999999,
      "grad_norm": 0.24180957674980164,
      "learning_rate": 3.913e-05,
      "loss": 0.0029,
      "step": 32610
    },
    {
      "epoch": 1.7397333333333334,
      "grad_norm": 0.7227659225463867,
      "learning_rate": 3.9126666666666666e-05,
      "loss": 0.0019,
      "step": 32620
    },
    {
      "epoch": 1.7402666666666666,
      "grad_norm": 0.4731590151786804,
      "learning_rate": 3.912333333333333e-05,
      "loss": 0.0025,
      "step": 32630
    },
    {
      "epoch": 1.7408000000000001,
      "grad_norm": 0.30540454387664795,
      "learning_rate": 3.912e-05,
      "loss": 0.0026,
      "step": 32640
    },
    {
      "epoch": 1.7413333333333334,
      "grad_norm": 0.15273422002792358,
      "learning_rate": 3.911666666666667e-05,
      "loss": 0.0036,
      "step": 32650
    },
    {
      "epoch": 1.7418666666666667,
      "grad_norm": 0.09500773996114731,
      "learning_rate": 3.911333333333334e-05,
      "loss": 0.0024,
      "step": 32660
    },
    {
      "epoch": 1.7424,
      "grad_norm": 0.36150869727134705,
      "learning_rate": 3.911e-05,
      "loss": 0.0025,
      "step": 32670
    },
    {
      "epoch": 1.7429333333333332,
      "grad_norm": 0.452372670173645,
      "learning_rate": 3.910666666666667e-05,
      "loss": 0.0026,
      "step": 32680
    },
    {
      "epoch": 1.7434666666666667,
      "grad_norm": 0.19508089125156403,
      "learning_rate": 3.9103333333333336e-05,
      "loss": 0.0029,
      "step": 32690
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.24077990651130676,
      "learning_rate": 3.91e-05,
      "loss": 0.0033,
      "step": 32700
    },
    {
      "epoch": 1.7445333333333335,
      "grad_norm": 0.5443674325942993,
      "learning_rate": 3.909666666666667e-05,
      "loss": 0.0031,
      "step": 32710
    },
    {
      "epoch": 1.7450666666666668,
      "grad_norm": 0.03385515511035919,
      "learning_rate": 3.9093333333333334e-05,
      "loss": 0.0026,
      "step": 32720
    },
    {
      "epoch": 1.7456,
      "grad_norm": 0.6562718152999878,
      "learning_rate": 3.909000000000001e-05,
      "loss": 0.002,
      "step": 32730
    },
    {
      "epoch": 1.7461333333333333,
      "grad_norm": 0.15549057722091675,
      "learning_rate": 3.9086666666666666e-05,
      "loss": 0.0025,
      "step": 32740
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 0.21639953553676605,
      "learning_rate": 3.908333333333333e-05,
      "loss": 0.0019,
      "step": 32750
    },
    {
      "epoch": 1.7471999999999999,
      "grad_norm": 0.1501244604587555,
      "learning_rate": 3.908e-05,
      "loss": 0.0029,
      "step": 32760
    },
    {
      "epoch": 1.7477333333333334,
      "grad_norm": 0.01908247172832489,
      "learning_rate": 3.9076666666666665e-05,
      "loss": 0.0043,
      "step": 32770
    },
    {
      "epoch": 1.7482666666666666,
      "grad_norm": 0.06660116463899612,
      "learning_rate": 3.907333333333333e-05,
      "loss": 0.003,
      "step": 32780
    },
    {
      "epoch": 1.7488000000000001,
      "grad_norm": 0.372519850730896,
      "learning_rate": 3.9070000000000004e-05,
      "loss": 0.0046,
      "step": 32790
    },
    {
      "epoch": 1.7493333333333334,
      "grad_norm": 0.39348259568214417,
      "learning_rate": 3.906666666666667e-05,
      "loss": 0.0033,
      "step": 32800
    },
    {
      "epoch": 1.7498666666666667,
      "grad_norm": 0.06666596233844757,
      "learning_rate": 3.9063333333333336e-05,
      "loss": 0.0038,
      "step": 32810
    },
    {
      "epoch": 1.7504,
      "grad_norm": 0.36785247921943665,
      "learning_rate": 3.906e-05,
      "loss": 0.0037,
      "step": 32820
    },
    {
      "epoch": 1.7509333333333332,
      "grad_norm": 0.4694117605686188,
      "learning_rate": 3.905666666666667e-05,
      "loss": 0.0035,
      "step": 32830
    },
    {
      "epoch": 1.7514666666666665,
      "grad_norm": 0.18018676340579987,
      "learning_rate": 3.9053333333333334e-05,
      "loss": 0.0022,
      "step": 32840
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.30362263321876526,
      "learning_rate": 3.905e-05,
      "loss": 0.0028,
      "step": 32850
    },
    {
      "epoch": 1.7525333333333335,
      "grad_norm": 0.24203702807426453,
      "learning_rate": 3.9046666666666673e-05,
      "loss": 0.0036,
      "step": 32860
    },
    {
      "epoch": 1.7530666666666668,
      "grad_norm": 0.5437543392181396,
      "learning_rate": 3.904333333333334e-05,
      "loss": 0.0022,
      "step": 32870
    },
    {
      "epoch": 1.7536,
      "grad_norm": 0.2143629789352417,
      "learning_rate": 3.9040000000000006e-05,
      "loss": 0.0027,
      "step": 32880
    },
    {
      "epoch": 1.7541333333333333,
      "grad_norm": 0.3977632224559784,
      "learning_rate": 3.9036666666666665e-05,
      "loss": 0.0025,
      "step": 32890
    },
    {
      "epoch": 1.7546666666666666,
      "grad_norm": 0.3610660433769226,
      "learning_rate": 3.903333333333333e-05,
      "loss": 0.0025,
      "step": 32900
    },
    {
      "epoch": 1.7551999999999999,
      "grad_norm": 0.07559069246053696,
      "learning_rate": 3.903e-05,
      "loss": 0.0023,
      "step": 32910
    },
    {
      "epoch": 1.7557333333333334,
      "grad_norm": 0.23944005370140076,
      "learning_rate": 3.902666666666667e-05,
      "loss": 0.0021,
      "step": 32920
    },
    {
      "epoch": 1.7562666666666666,
      "grad_norm": 0.12524504959583282,
      "learning_rate": 3.9023333333333336e-05,
      "loss": 0.0027,
      "step": 32930
    },
    {
      "epoch": 1.7568000000000001,
      "grad_norm": 0.27659016847610474,
      "learning_rate": 3.902e-05,
      "loss": 0.0031,
      "step": 32940
    },
    {
      "epoch": 1.7573333333333334,
      "grad_norm": 0.12728463113307953,
      "learning_rate": 3.901666666666667e-05,
      "loss": 0.0028,
      "step": 32950
    },
    {
      "epoch": 1.7578666666666667,
      "grad_norm": 0.24069082736968994,
      "learning_rate": 3.9013333333333335e-05,
      "loss": 0.0029,
      "step": 32960
    },
    {
      "epoch": 1.7584,
      "grad_norm": 0.06392776966094971,
      "learning_rate": 3.901e-05,
      "loss": 0.0014,
      "step": 32970
    },
    {
      "epoch": 1.7589333333333332,
      "grad_norm": 0.1246931403875351,
      "learning_rate": 3.900666666666667e-05,
      "loss": 0.0027,
      "step": 32980
    },
    {
      "epoch": 1.7594666666666665,
      "grad_norm": 0.05278347060084343,
      "learning_rate": 3.900333333333333e-05,
      "loss": 0.0027,
      "step": 32990
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.12621945142745972,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.0022,
      "step": 33000
    },
    {
      "epoch": 1.7605333333333333,
      "grad_norm": 0.10132274031639099,
      "learning_rate": 3.899666666666667e-05,
      "loss": 0.0041,
      "step": 33010
    },
    {
      "epoch": 1.7610666666666668,
      "grad_norm": 0.0985456332564354,
      "learning_rate": 3.899333333333334e-05,
      "loss": 0.0027,
      "step": 33020
    },
    {
      "epoch": 1.7616,
      "grad_norm": 0.179735004901886,
      "learning_rate": 3.8990000000000004e-05,
      "loss": 0.0033,
      "step": 33030
    },
    {
      "epoch": 1.7621333333333333,
      "grad_norm": 0.27514955401420593,
      "learning_rate": 3.8986666666666664e-05,
      "loss": 0.0035,
      "step": 33040
    },
    {
      "epoch": 1.7626666666666666,
      "grad_norm": 0.4352390468120575,
      "learning_rate": 3.898333333333333e-05,
      "loss": 0.0024,
      "step": 33050
    },
    {
      "epoch": 1.7631999999999999,
      "grad_norm": 0.4203823506832123,
      "learning_rate": 3.898e-05,
      "loss": 0.0035,
      "step": 33060
    },
    {
      "epoch": 1.7637333333333334,
      "grad_norm": 0.15527434647083282,
      "learning_rate": 3.897666666666667e-05,
      "loss": 0.0037,
      "step": 33070
    },
    {
      "epoch": 1.7642666666666666,
      "grad_norm": 0.4562031924724579,
      "learning_rate": 3.8973333333333335e-05,
      "loss": 0.0027,
      "step": 33080
    },
    {
      "epoch": 1.7648000000000001,
      "grad_norm": 0.0586368553340435,
      "learning_rate": 3.897e-05,
      "loss": 0.0037,
      "step": 33090
    },
    {
      "epoch": 1.7653333333333334,
      "grad_norm": 0.3361707031726837,
      "learning_rate": 3.896666666666667e-05,
      "loss": 0.0021,
      "step": 33100
    },
    {
      "epoch": 1.7658666666666667,
      "grad_norm": 0.30452415347099304,
      "learning_rate": 3.8963333333333334e-05,
      "loss": 0.0034,
      "step": 33110
    },
    {
      "epoch": 1.7664,
      "grad_norm": 0.03396352753043175,
      "learning_rate": 3.896e-05,
      "loss": 0.0024,
      "step": 33120
    },
    {
      "epoch": 1.7669333333333332,
      "grad_norm": 0.5076159834861755,
      "learning_rate": 3.8956666666666666e-05,
      "loss": 0.0022,
      "step": 33130
    },
    {
      "epoch": 1.7674666666666665,
      "grad_norm": 0.23806963860988617,
      "learning_rate": 3.895333333333334e-05,
      "loss": 0.0029,
      "step": 33140
    },
    {
      "epoch": 1.768,
      "grad_norm": 0.045596715062856674,
      "learning_rate": 3.8950000000000005e-05,
      "loss": 0.0028,
      "step": 33150
    },
    {
      "epoch": 1.7685333333333333,
      "grad_norm": 0.5429355502128601,
      "learning_rate": 3.894666666666667e-05,
      "loss": 0.0032,
      "step": 33160
    },
    {
      "epoch": 1.7690666666666668,
      "grad_norm": 0.0709017664194107,
      "learning_rate": 3.894333333333334e-05,
      "loss": 0.0031,
      "step": 33170
    },
    {
      "epoch": 1.7696,
      "grad_norm": 0.07476284354925156,
      "learning_rate": 3.894e-05,
      "loss": 0.0032,
      "step": 33180
    },
    {
      "epoch": 1.7701333333333333,
      "grad_norm": 0.3582656681537628,
      "learning_rate": 3.893666666666667e-05,
      "loss": 0.0031,
      "step": 33190
    },
    {
      "epoch": 1.7706666666666666,
      "grad_norm": 0.16796347498893738,
      "learning_rate": 3.8933333333333336e-05,
      "loss": 0.0022,
      "step": 33200
    },
    {
      "epoch": 1.7711999999999999,
      "grad_norm": 0.28253650665283203,
      "learning_rate": 3.893e-05,
      "loss": 0.0025,
      "step": 33210
    },
    {
      "epoch": 1.7717333333333334,
      "grad_norm": 0.18027566373348236,
      "learning_rate": 3.892666666666667e-05,
      "loss": 0.0032,
      "step": 33220
    },
    {
      "epoch": 1.7722666666666667,
      "grad_norm": 0.028866766020655632,
      "learning_rate": 3.8923333333333334e-05,
      "loss": 0.0025,
      "step": 33230
    },
    {
      "epoch": 1.7728000000000002,
      "grad_norm": 0.7544668912887573,
      "learning_rate": 3.892e-05,
      "loss": 0.0026,
      "step": 33240
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 0.21120049059391022,
      "learning_rate": 3.8916666666666666e-05,
      "loss": 0.0034,
      "step": 33250
    },
    {
      "epoch": 1.7738666666666667,
      "grad_norm": 0.12411502003669739,
      "learning_rate": 3.891333333333333e-05,
      "loss": 0.0038,
      "step": 33260
    },
    {
      "epoch": 1.7744,
      "grad_norm": 0.15284636616706848,
      "learning_rate": 3.8910000000000005e-05,
      "loss": 0.0028,
      "step": 33270
    },
    {
      "epoch": 1.7749333333333333,
      "grad_norm": 0.2116377204656601,
      "learning_rate": 3.890666666666667e-05,
      "loss": 0.003,
      "step": 33280
    },
    {
      "epoch": 1.7754666666666665,
      "grad_norm": 0.38814252614974976,
      "learning_rate": 3.890333333333334e-05,
      "loss": 0.0039,
      "step": 33290
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.1226353868842125,
      "learning_rate": 3.8900000000000004e-05,
      "loss": 0.003,
      "step": 33300
    },
    {
      "epoch": 1.7765333333333333,
      "grad_norm": 0.16058488190174103,
      "learning_rate": 3.889666666666667e-05,
      "loss": 0.0032,
      "step": 33310
    },
    {
      "epoch": 1.7770666666666668,
      "grad_norm": 0.39209431409835815,
      "learning_rate": 3.8893333333333336e-05,
      "loss": 0.0034,
      "step": 33320
    },
    {
      "epoch": 1.7776,
      "grad_norm": 0.32886242866516113,
      "learning_rate": 3.889e-05,
      "loss": 0.0019,
      "step": 33330
    },
    {
      "epoch": 1.7781333333333333,
      "grad_norm": 0.06733286380767822,
      "learning_rate": 3.888666666666667e-05,
      "loss": 0.0028,
      "step": 33340
    },
    {
      "epoch": 1.7786666666666666,
      "grad_norm": 0.07596472650766373,
      "learning_rate": 3.8883333333333334e-05,
      "loss": 0.0022,
      "step": 33350
    },
    {
      "epoch": 1.7792,
      "grad_norm": 0.06185625493526459,
      "learning_rate": 3.888e-05,
      "loss": 0.0019,
      "step": 33360
    },
    {
      "epoch": 1.7797333333333332,
      "grad_norm": 0.09434133768081665,
      "learning_rate": 3.8876666666666667e-05,
      "loss": 0.0029,
      "step": 33370
    },
    {
      "epoch": 1.7802666666666667,
      "grad_norm": 0.0810478925704956,
      "learning_rate": 3.887333333333333e-05,
      "loss": 0.0021,
      "step": 33380
    },
    {
      "epoch": 1.7808000000000002,
      "grad_norm": 0.3601422607898712,
      "learning_rate": 3.887e-05,
      "loss": 0.0022,
      "step": 33390
    },
    {
      "epoch": 1.7813333333333334,
      "grad_norm": 0.15257875621318817,
      "learning_rate": 3.8866666666666665e-05,
      "loss": 0.0034,
      "step": 33400
    },
    {
      "epoch": 1.7818666666666667,
      "grad_norm": 0.10690611600875854,
      "learning_rate": 3.886333333333334e-05,
      "loss": 0.0031,
      "step": 33410
    },
    {
      "epoch": 1.7824,
      "grad_norm": 0.17902925610542297,
      "learning_rate": 3.8860000000000004e-05,
      "loss": 0.0023,
      "step": 33420
    },
    {
      "epoch": 1.7829333333333333,
      "grad_norm": 0.05702004209160805,
      "learning_rate": 3.885666666666667e-05,
      "loss": 0.0024,
      "step": 33430
    },
    {
      "epoch": 1.7834666666666665,
      "grad_norm": 0.34606459736824036,
      "learning_rate": 3.8853333333333336e-05,
      "loss": 0.0022,
      "step": 33440
    },
    {
      "epoch": 1.784,
      "grad_norm": 0.17447702586650848,
      "learning_rate": 3.885e-05,
      "loss": 0.0032,
      "step": 33450
    },
    {
      "epoch": 1.7845333333333333,
      "grad_norm": 0.4486866295337677,
      "learning_rate": 3.884666666666667e-05,
      "loss": 0.0032,
      "step": 33460
    },
    {
      "epoch": 1.7850666666666668,
      "grad_norm": 0.32799553871154785,
      "learning_rate": 3.8843333333333335e-05,
      "loss": 0.0025,
      "step": 33470
    },
    {
      "epoch": 1.7856,
      "grad_norm": 0.09098414331674576,
      "learning_rate": 3.884e-05,
      "loss": 0.0022,
      "step": 33480
    },
    {
      "epoch": 1.7861333333333334,
      "grad_norm": 0.04384623467922211,
      "learning_rate": 3.8836666666666674e-05,
      "loss": 0.0027,
      "step": 33490
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 0.050781674683094025,
      "learning_rate": 3.883333333333333e-05,
      "loss": 0.002,
      "step": 33500
    },
    {
      "epoch": 1.7872,
      "grad_norm": 0.45301762223243713,
      "learning_rate": 3.883e-05,
      "loss": 0.0035,
      "step": 33510
    },
    {
      "epoch": 1.7877333333333332,
      "grad_norm": 0.021605171263217926,
      "learning_rate": 3.8826666666666665e-05,
      "loss": 0.0016,
      "step": 33520
    },
    {
      "epoch": 1.7882666666666667,
      "grad_norm": 0.39201849699020386,
      "learning_rate": 3.882333333333333e-05,
      "loss": 0.002,
      "step": 33530
    },
    {
      "epoch": 1.7888,
      "grad_norm": 0.18208777904510498,
      "learning_rate": 3.882e-05,
      "loss": 0.0026,
      "step": 33540
    },
    {
      "epoch": 1.7893333333333334,
      "grad_norm": 0.1856195479631424,
      "learning_rate": 3.881666666666667e-05,
      "loss": 0.0019,
      "step": 33550
    },
    {
      "epoch": 1.7898666666666667,
      "grad_norm": 0.18146231770515442,
      "learning_rate": 3.881333333333334e-05,
      "loss": 0.0029,
      "step": 33560
    },
    {
      "epoch": 1.7904,
      "grad_norm": 0.18304578959941864,
      "learning_rate": 3.881e-05,
      "loss": 0.0033,
      "step": 33570
    },
    {
      "epoch": 1.7909333333333333,
      "grad_norm": 0.24260950088500977,
      "learning_rate": 3.880666666666667e-05,
      "loss": 0.0042,
      "step": 33580
    },
    {
      "epoch": 1.7914666666666665,
      "grad_norm": 0.09307993203401566,
      "learning_rate": 3.8803333333333335e-05,
      "loss": 0.0036,
      "step": 33590
    },
    {
      "epoch": 1.792,
      "grad_norm": 0.3579460382461548,
      "learning_rate": 3.88e-05,
      "loss": 0.0031,
      "step": 33600
    },
    {
      "epoch": 1.7925333333333333,
      "grad_norm": 0.064283587038517,
      "learning_rate": 3.879666666666667e-05,
      "loss": 0.0028,
      "step": 33610
    },
    {
      "epoch": 1.7930666666666668,
      "grad_norm": 0.10063914209604263,
      "learning_rate": 3.879333333333334e-05,
      "loss": 0.0023,
      "step": 33620
    },
    {
      "epoch": 1.7936,
      "grad_norm": 0.24589508771896362,
      "learning_rate": 3.8790000000000006e-05,
      "loss": 0.0027,
      "step": 33630
    },
    {
      "epoch": 1.7941333333333334,
      "grad_norm": 0.04084886983036995,
      "learning_rate": 3.878666666666667e-05,
      "loss": 0.0025,
      "step": 33640
    },
    {
      "epoch": 1.7946666666666666,
      "grad_norm": 0.14155937731266022,
      "learning_rate": 3.878333333333333e-05,
      "loss": 0.0023,
      "step": 33650
    },
    {
      "epoch": 1.7952,
      "grad_norm": 0.3354223072528839,
      "learning_rate": 3.878e-05,
      "loss": 0.003,
      "step": 33660
    },
    {
      "epoch": 1.7957333333333332,
      "grad_norm": 0.35144612193107605,
      "learning_rate": 3.8776666666666664e-05,
      "loss": 0.0028,
      "step": 33670
    },
    {
      "epoch": 1.7962666666666667,
      "grad_norm": 0.29229775071144104,
      "learning_rate": 3.877333333333334e-05,
      "loss": 0.003,
      "step": 33680
    },
    {
      "epoch": 1.7968,
      "grad_norm": 0.572289764881134,
      "learning_rate": 3.877e-05,
      "loss": 0.0029,
      "step": 33690
    },
    {
      "epoch": 1.7973333333333334,
      "grad_norm": 0.39351996779441833,
      "learning_rate": 3.876666666666667e-05,
      "loss": 0.0026,
      "step": 33700
    },
    {
      "epoch": 1.7978666666666667,
      "grad_norm": 0.24277614057064056,
      "learning_rate": 3.8763333333333335e-05,
      "loss": 0.0034,
      "step": 33710
    },
    {
      "epoch": 1.7984,
      "grad_norm": 0.15111687779426575,
      "learning_rate": 3.876e-05,
      "loss": 0.0021,
      "step": 33720
    },
    {
      "epoch": 1.7989333333333333,
      "grad_norm": 0.08316648006439209,
      "learning_rate": 3.875666666666667e-05,
      "loss": 0.0026,
      "step": 33730
    },
    {
      "epoch": 1.7994666666666665,
      "grad_norm": 0.10060349106788635,
      "learning_rate": 3.8753333333333334e-05,
      "loss": 0.0027,
      "step": 33740
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.2709836959838867,
      "learning_rate": 3.875e-05,
      "loss": 0.0021,
      "step": 33750
    },
    {
      "epoch": 1.8005333333333333,
      "grad_norm": 0.18294692039489746,
      "learning_rate": 3.874666666666667e-05,
      "loss": 0.0017,
      "step": 33760
    },
    {
      "epoch": 1.8010666666666668,
      "grad_norm": 0.3619256615638733,
      "learning_rate": 3.874333333333334e-05,
      "loss": 0.0033,
      "step": 33770
    },
    {
      "epoch": 1.8016,
      "grad_norm": 0.066792793571949,
      "learning_rate": 3.8740000000000005e-05,
      "loss": 0.0044,
      "step": 33780
    },
    {
      "epoch": 1.8021333333333334,
      "grad_norm": 0.27988070249557495,
      "learning_rate": 3.873666666666667e-05,
      "loss": 0.0032,
      "step": 33790
    },
    {
      "epoch": 1.8026666666666666,
      "grad_norm": 0.3903791904449463,
      "learning_rate": 3.873333333333333e-05,
      "loss": 0.0028,
      "step": 33800
    },
    {
      "epoch": 1.8032,
      "grad_norm": 0.48896893858909607,
      "learning_rate": 3.873e-05,
      "loss": 0.0032,
      "step": 33810
    },
    {
      "epoch": 1.8037333333333332,
      "grad_norm": 0.12612737715244293,
      "learning_rate": 3.872666666666667e-05,
      "loss": 0.003,
      "step": 33820
    },
    {
      "epoch": 1.8042666666666667,
      "grad_norm": 0.16991856694221497,
      "learning_rate": 3.8723333333333336e-05,
      "loss": 0.0026,
      "step": 33830
    },
    {
      "epoch": 1.8048,
      "grad_norm": 0.4489808678627014,
      "learning_rate": 3.872e-05,
      "loss": 0.0027,
      "step": 33840
    },
    {
      "epoch": 1.8053333333333335,
      "grad_norm": 0.3292970657348633,
      "learning_rate": 3.871666666666667e-05,
      "loss": 0.0034,
      "step": 33850
    },
    {
      "epoch": 1.8058666666666667,
      "grad_norm": 0.4178199768066406,
      "learning_rate": 3.8713333333333334e-05,
      "loss": 0.0027,
      "step": 33860
    },
    {
      "epoch": 1.8064,
      "grad_norm": 0.15176908671855927,
      "learning_rate": 3.871e-05,
      "loss": 0.0027,
      "step": 33870
    },
    {
      "epoch": 1.8069333333333333,
      "grad_norm": 0.1799544394016266,
      "learning_rate": 3.8706666666666667e-05,
      "loss": 0.0027,
      "step": 33880
    },
    {
      "epoch": 1.8074666666666666,
      "grad_norm": 0.04149085283279419,
      "learning_rate": 3.870333333333333e-05,
      "loss": 0.002,
      "step": 33890
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.48440325260162354,
      "learning_rate": 3.8700000000000006e-05,
      "loss": 0.0032,
      "step": 33900
    },
    {
      "epoch": 1.8085333333333333,
      "grad_norm": 0.5067534446716309,
      "learning_rate": 3.869666666666667e-05,
      "loss": 0.0033,
      "step": 33910
    },
    {
      "epoch": 1.8090666666666668,
      "grad_norm": 0.30029770731925964,
      "learning_rate": 3.869333333333334e-05,
      "loss": 0.0019,
      "step": 33920
    },
    {
      "epoch": 1.8096,
      "grad_norm": 0.5671793818473816,
      "learning_rate": 3.8690000000000004e-05,
      "loss": 0.0025,
      "step": 33930
    },
    {
      "epoch": 1.8101333333333334,
      "grad_norm": 0.3142039477825165,
      "learning_rate": 3.868666666666667e-05,
      "loss": 0.0024,
      "step": 33940
    },
    {
      "epoch": 1.8106666666666666,
      "grad_norm": 0.3027724325656891,
      "learning_rate": 3.868333333333333e-05,
      "loss": 0.0028,
      "step": 33950
    },
    {
      "epoch": 1.8112,
      "grad_norm": 0.040673717856407166,
      "learning_rate": 3.868e-05,
      "loss": 0.0029,
      "step": 33960
    },
    {
      "epoch": 1.8117333333333332,
      "grad_norm": 0.3323260545730591,
      "learning_rate": 3.867666666666667e-05,
      "loss": 0.0027,
      "step": 33970
    },
    {
      "epoch": 1.8122666666666667,
      "grad_norm": 0.05320107564330101,
      "learning_rate": 3.8673333333333335e-05,
      "loss": 0.0023,
      "step": 33980
    },
    {
      "epoch": 1.8128,
      "grad_norm": 0.4502790868282318,
      "learning_rate": 3.867e-05,
      "loss": 0.0037,
      "step": 33990
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 0.539236843585968,
      "learning_rate": 3.866666666666667e-05,
      "loss": 0.0027,
      "step": 34000
    },
    {
      "epoch": 1.8138666666666667,
      "grad_norm": 0.36242005228996277,
      "learning_rate": 3.866333333333333e-05,
      "loss": 0.003,
      "step": 34010
    },
    {
      "epoch": 1.8144,
      "grad_norm": 0.3576689660549164,
      "learning_rate": 3.866e-05,
      "loss": 0.0029,
      "step": 34020
    },
    {
      "epoch": 1.8149333333333333,
      "grad_norm": 0.10437685996294022,
      "learning_rate": 3.865666666666667e-05,
      "loss": 0.0018,
      "step": 34030
    },
    {
      "epoch": 1.8154666666666666,
      "grad_norm": 0.13329382240772247,
      "learning_rate": 3.865333333333334e-05,
      "loss": 0.0037,
      "step": 34040
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 0.390800803899765,
      "learning_rate": 3.8650000000000004e-05,
      "loss": 0.002,
      "step": 34050
    },
    {
      "epoch": 1.8165333333333333,
      "grad_norm": 0.1515415608882904,
      "learning_rate": 3.864666666666667e-05,
      "loss": 0.0038,
      "step": 34060
    },
    {
      "epoch": 1.8170666666666668,
      "grad_norm": 0.08158610761165619,
      "learning_rate": 3.8643333333333337e-05,
      "loss": 0.0016,
      "step": 34070
    },
    {
      "epoch": 1.8176,
      "grad_norm": 0.7200953364372253,
      "learning_rate": 3.864e-05,
      "loss": 0.0023,
      "step": 34080
    },
    {
      "epoch": 1.8181333333333334,
      "grad_norm": 0.5369111895561218,
      "learning_rate": 3.863666666666667e-05,
      "loss": 0.003,
      "step": 34090
    },
    {
      "epoch": 1.8186666666666667,
      "grad_norm": 0.052444521337747574,
      "learning_rate": 3.8633333333333335e-05,
      "loss": 0.0035,
      "step": 34100
    },
    {
      "epoch": 1.8192,
      "grad_norm": 0.2560691237449646,
      "learning_rate": 3.863e-05,
      "loss": 0.0018,
      "step": 34110
    },
    {
      "epoch": 1.8197333333333332,
      "grad_norm": 0.39047372341156006,
      "learning_rate": 3.862666666666667e-05,
      "loss": 0.0041,
      "step": 34120
    },
    {
      "epoch": 1.8202666666666667,
      "grad_norm": 0.09470111131668091,
      "learning_rate": 3.8623333333333333e-05,
      "loss": 0.0023,
      "step": 34130
    },
    {
      "epoch": 1.8208,
      "grad_norm": 0.03987300395965576,
      "learning_rate": 3.862e-05,
      "loss": 0.0034,
      "step": 34140
    },
    {
      "epoch": 1.8213333333333335,
      "grad_norm": 0.0471344068646431,
      "learning_rate": 3.8616666666666666e-05,
      "loss": 0.0035,
      "step": 34150
    },
    {
      "epoch": 1.8218666666666667,
      "grad_norm": 0.21100901067256927,
      "learning_rate": 3.861333333333333e-05,
      "loss": 0.0032,
      "step": 34160
    },
    {
      "epoch": 1.8224,
      "grad_norm": 0.214187353849411,
      "learning_rate": 3.8610000000000005e-05,
      "loss": 0.0019,
      "step": 34170
    },
    {
      "epoch": 1.8229333333333333,
      "grad_norm": 0.5106772780418396,
      "learning_rate": 3.860666666666667e-05,
      "loss": 0.0025,
      "step": 34180
    },
    {
      "epoch": 1.8234666666666666,
      "grad_norm": 1.021933913230896,
      "learning_rate": 3.860333333333334e-05,
      "loss": 0.0025,
      "step": 34190
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.1577983945608139,
      "learning_rate": 3.86e-05,
      "loss": 0.0026,
      "step": 34200
    },
    {
      "epoch": 1.8245333333333333,
      "grad_norm": 0.3383903205394745,
      "learning_rate": 3.859666666666667e-05,
      "loss": 0.0025,
      "step": 34210
    },
    {
      "epoch": 1.8250666666666666,
      "grad_norm": 0.5821905732154846,
      "learning_rate": 3.8593333333333335e-05,
      "loss": 0.0034,
      "step": 34220
    },
    {
      "epoch": 1.8256000000000001,
      "grad_norm": 0.7364550232887268,
      "learning_rate": 3.859e-05,
      "loss": 0.0026,
      "step": 34230
    },
    {
      "epoch": 1.8261333333333334,
      "grad_norm": 0.3924541473388672,
      "learning_rate": 3.858666666666667e-05,
      "loss": 0.0026,
      "step": 34240
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 0.09191520512104034,
      "learning_rate": 3.8583333333333334e-05,
      "loss": 0.0023,
      "step": 34250
    },
    {
      "epoch": 1.8272,
      "grad_norm": 0.15419098734855652,
      "learning_rate": 3.858e-05,
      "loss": 0.0027,
      "step": 34260
    },
    {
      "epoch": 1.8277333333333332,
      "grad_norm": 0.32810279726982117,
      "learning_rate": 3.8576666666666666e-05,
      "loss": 0.0033,
      "step": 34270
    },
    {
      "epoch": 1.8282666666666667,
      "grad_norm": 0.8560954332351685,
      "learning_rate": 3.857333333333333e-05,
      "loss": 0.0046,
      "step": 34280
    },
    {
      "epoch": 1.8288,
      "grad_norm": 0.5669229030609131,
      "learning_rate": 3.857e-05,
      "loss": 0.0029,
      "step": 34290
    },
    {
      "epoch": 1.8293333333333335,
      "grad_norm": 0.5049384236335754,
      "learning_rate": 3.8566666666666664e-05,
      "loss": 0.0035,
      "step": 34300
    },
    {
      "epoch": 1.8298666666666668,
      "grad_norm": 0.5397831797599792,
      "learning_rate": 3.856333333333334e-05,
      "loss": 0.0034,
      "step": 34310
    },
    {
      "epoch": 1.8304,
      "grad_norm": 0.42862388491630554,
      "learning_rate": 3.8560000000000004e-05,
      "loss": 0.0023,
      "step": 34320
    },
    {
      "epoch": 1.8309333333333333,
      "grad_norm": 0.4837615191936493,
      "learning_rate": 3.855666666666667e-05,
      "loss": 0.0039,
      "step": 34330
    },
    {
      "epoch": 1.8314666666666666,
      "grad_norm": 0.09852828830480576,
      "learning_rate": 3.8553333333333336e-05,
      "loss": 0.0033,
      "step": 34340
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.16284580528736115,
      "learning_rate": 3.855e-05,
      "loss": 0.003,
      "step": 34350
    },
    {
      "epoch": 1.8325333333333333,
      "grad_norm": 0.14990584552288055,
      "learning_rate": 3.854666666666667e-05,
      "loss": 0.0026,
      "step": 34360
    },
    {
      "epoch": 1.8330666666666666,
      "grad_norm": 0.24898761510849,
      "learning_rate": 3.8543333333333334e-05,
      "loss": 0.0026,
      "step": 34370
    },
    {
      "epoch": 1.8336000000000001,
      "grad_norm": 0.7197948098182678,
      "learning_rate": 3.854000000000001e-05,
      "loss": 0.0035,
      "step": 34380
    },
    {
      "epoch": 1.8341333333333334,
      "grad_norm": 0.12247036397457123,
      "learning_rate": 3.853666666666667e-05,
      "loss": 0.0019,
      "step": 34390
    },
    {
      "epoch": 1.8346666666666667,
      "grad_norm": 0.6903699636459351,
      "learning_rate": 3.853333333333334e-05,
      "loss": 0.0026,
      "step": 34400
    },
    {
      "epoch": 1.8352,
      "grad_norm": 0.15162329375743866,
      "learning_rate": 3.853e-05,
      "loss": 0.0024,
      "step": 34410
    },
    {
      "epoch": 1.8357333333333332,
      "grad_norm": 0.3011057674884796,
      "learning_rate": 3.8526666666666665e-05,
      "loss": 0.0013,
      "step": 34420
    },
    {
      "epoch": 1.8362666666666667,
      "grad_norm": 0.3940824270248413,
      "learning_rate": 3.852333333333333e-05,
      "loss": 0.0029,
      "step": 34430
    },
    {
      "epoch": 1.8368,
      "grad_norm": 0.05222417786717415,
      "learning_rate": 3.8520000000000004e-05,
      "loss": 0.0025,
      "step": 34440
    },
    {
      "epoch": 1.8373333333333335,
      "grad_norm": 0.19595149159431458,
      "learning_rate": 3.851666666666667e-05,
      "loss": 0.0027,
      "step": 34450
    },
    {
      "epoch": 1.8378666666666668,
      "grad_norm": 0.3294059634208679,
      "learning_rate": 3.8513333333333336e-05,
      "loss": 0.0031,
      "step": 34460
    },
    {
      "epoch": 1.8384,
      "grad_norm": 0.18189744651317596,
      "learning_rate": 3.851e-05,
      "loss": 0.0024,
      "step": 34470
    },
    {
      "epoch": 1.8389333333333333,
      "grad_norm": 0.6039921641349792,
      "learning_rate": 3.850666666666667e-05,
      "loss": 0.0031,
      "step": 34480
    },
    {
      "epoch": 1.8394666666666666,
      "grad_norm": 0.3893555700778961,
      "learning_rate": 3.8503333333333335e-05,
      "loss": 0.0022,
      "step": 34490
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.18245941400527954,
      "learning_rate": 3.85e-05,
      "loss": 0.0028,
      "step": 34500
    },
    {
      "epoch": 1.8405333333333334,
      "grad_norm": 0.6288983225822449,
      "learning_rate": 3.849666666666667e-05,
      "loss": 0.0035,
      "step": 34510
    },
    {
      "epoch": 1.8410666666666666,
      "grad_norm": 0.4813893735408783,
      "learning_rate": 3.849333333333334e-05,
      "loss": 0.004,
      "step": 34520
    },
    {
      "epoch": 1.8416000000000001,
      "grad_norm": 0.6549579501152039,
      "learning_rate": 3.8490000000000006e-05,
      "loss": 0.0031,
      "step": 34530
    },
    {
      "epoch": 1.8421333333333334,
      "grad_norm": 0.42358046770095825,
      "learning_rate": 3.848666666666667e-05,
      "loss": 0.0036,
      "step": 34540
    },
    {
      "epoch": 1.8426666666666667,
      "grad_norm": 0.11144841462373734,
      "learning_rate": 3.848333333333334e-05,
      "loss": 0.0022,
      "step": 34550
    },
    {
      "epoch": 1.8432,
      "grad_norm": 0.11677683889865875,
      "learning_rate": 3.848e-05,
      "loss": 0.0029,
      "step": 34560
    },
    {
      "epoch": 1.8437333333333332,
      "grad_norm": 0.09552372246980667,
      "learning_rate": 3.8476666666666664e-05,
      "loss": 0.0025,
      "step": 34570
    },
    {
      "epoch": 1.8442666666666667,
      "grad_norm": 0.04016083851456642,
      "learning_rate": 3.8473333333333337e-05,
      "loss": 0.0019,
      "step": 34580
    },
    {
      "epoch": 1.8448,
      "grad_norm": 0.3003275692462921,
      "learning_rate": 3.847e-05,
      "loss": 0.0027,
      "step": 34590
    },
    {
      "epoch": 1.8453333333333335,
      "grad_norm": 0.061957117170095444,
      "learning_rate": 3.846666666666667e-05,
      "loss": 0.0021,
      "step": 34600
    },
    {
      "epoch": 1.8458666666666668,
      "grad_norm": 0.3865770101547241,
      "learning_rate": 3.8463333333333335e-05,
      "loss": 0.0025,
      "step": 34610
    },
    {
      "epoch": 1.8464,
      "grad_norm": 0.0425616018474102,
      "learning_rate": 3.846e-05,
      "loss": 0.0025,
      "step": 34620
    },
    {
      "epoch": 1.8469333333333333,
      "grad_norm": 0.3606351613998413,
      "learning_rate": 3.845666666666667e-05,
      "loss": 0.0021,
      "step": 34630
    },
    {
      "epoch": 1.8474666666666666,
      "grad_norm": 0.29597675800323486,
      "learning_rate": 3.845333333333333e-05,
      "loss": 0.0034,
      "step": 34640
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 0.47185471653938293,
      "learning_rate": 3.845e-05,
      "loss": 0.0035,
      "step": 34650
    },
    {
      "epoch": 1.8485333333333334,
      "grad_norm": 0.6926507353782654,
      "learning_rate": 3.844666666666667e-05,
      "loss": 0.0034,
      "step": 34660
    },
    {
      "epoch": 1.8490666666666666,
      "grad_norm": 0.3294669985771179,
      "learning_rate": 3.844333333333334e-05,
      "loss": 0.0033,
      "step": 34670
    },
    {
      "epoch": 1.8496000000000001,
      "grad_norm": 0.12529882788658142,
      "learning_rate": 3.8440000000000005e-05,
      "loss": 0.0036,
      "step": 34680
    },
    {
      "epoch": 1.8501333333333334,
      "grad_norm": 0.15033955872058868,
      "learning_rate": 3.843666666666667e-05,
      "loss": 0.0019,
      "step": 34690
    },
    {
      "epoch": 1.8506666666666667,
      "grad_norm": 0.5132712125778198,
      "learning_rate": 3.843333333333334e-05,
      "loss": 0.0032,
      "step": 34700
    },
    {
      "epoch": 1.8512,
      "grad_norm": 0.34519392251968384,
      "learning_rate": 3.8429999999999996e-05,
      "loss": 0.0035,
      "step": 34710
    },
    {
      "epoch": 1.8517333333333332,
      "grad_norm": 0.416555792093277,
      "learning_rate": 3.842666666666667e-05,
      "loss": 0.003,
      "step": 34720
    },
    {
      "epoch": 1.8522666666666665,
      "grad_norm": 0.47359389066696167,
      "learning_rate": 3.8423333333333335e-05,
      "loss": 0.0025,
      "step": 34730
    },
    {
      "epoch": 1.8528,
      "grad_norm": 0.09523963928222656,
      "learning_rate": 3.842e-05,
      "loss": 0.0027,
      "step": 34740
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 0.4205961227416992,
      "learning_rate": 3.841666666666667e-05,
      "loss": 0.0023,
      "step": 34750
    },
    {
      "epoch": 1.8538666666666668,
      "grad_norm": 0.0903240367770195,
      "learning_rate": 3.8413333333333334e-05,
      "loss": 0.0032,
      "step": 34760
    },
    {
      "epoch": 1.8544,
      "grad_norm": 0.13215598464012146,
      "learning_rate": 3.841e-05,
      "loss": 0.0025,
      "step": 34770
    },
    {
      "epoch": 1.8549333333333333,
      "grad_norm": 0.2824743092060089,
      "learning_rate": 3.8406666666666666e-05,
      "loss": 0.0028,
      "step": 34780
    },
    {
      "epoch": 1.8554666666666666,
      "grad_norm": 0.4849098324775696,
      "learning_rate": 3.840333333333334e-05,
      "loss": 0.0021,
      "step": 34790
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.09887164831161499,
      "learning_rate": 3.8400000000000005e-05,
      "loss": 0.0019,
      "step": 34800
    },
    {
      "epoch": 1.8565333333333334,
      "grad_norm": 0.019820380955934525,
      "learning_rate": 3.839666666666667e-05,
      "loss": 0.0028,
      "step": 34810
    },
    {
      "epoch": 1.8570666666666666,
      "grad_norm": 0.5704219937324524,
      "learning_rate": 3.839333333333334e-05,
      "loss": 0.0031,
      "step": 34820
    },
    {
      "epoch": 1.8576000000000001,
      "grad_norm": 0.269905686378479,
      "learning_rate": 3.8390000000000003e-05,
      "loss": 0.0024,
      "step": 34830
    },
    {
      "epoch": 1.8581333333333334,
      "grad_norm": 0.14980487525463104,
      "learning_rate": 3.838666666666667e-05,
      "loss": 0.003,
      "step": 34840
    },
    {
      "epoch": 1.8586666666666667,
      "grad_norm": 0.2316274791955948,
      "learning_rate": 3.8383333333333336e-05,
      "loss": 0.0032,
      "step": 34850
    },
    {
      "epoch": 1.8592,
      "grad_norm": 0.329722136259079,
      "learning_rate": 3.838e-05,
      "loss": 0.0028,
      "step": 34860
    },
    {
      "epoch": 1.8597333333333332,
      "grad_norm": 0.09118102490901947,
      "learning_rate": 3.837666666666667e-05,
      "loss": 0.0019,
      "step": 34870
    },
    {
      "epoch": 1.8602666666666665,
      "grad_norm": 0.11692085862159729,
      "learning_rate": 3.8373333333333334e-05,
      "loss": 0.0036,
      "step": 34880
    },
    {
      "epoch": 1.8608,
      "grad_norm": 0.18970680236816406,
      "learning_rate": 3.837e-05,
      "loss": 0.0019,
      "step": 34890
    },
    {
      "epoch": 1.8613333333333333,
      "grad_norm": 0.5513591766357422,
      "learning_rate": 3.8366666666666666e-05,
      "loss": 0.0034,
      "step": 34900
    },
    {
      "epoch": 1.8618666666666668,
      "grad_norm": 0.1539592295885086,
      "learning_rate": 3.836333333333333e-05,
      "loss": 0.0023,
      "step": 34910
    },
    {
      "epoch": 1.8624,
      "grad_norm": 0.2892841696739197,
      "learning_rate": 3.836e-05,
      "loss": 0.0025,
      "step": 34920
    },
    {
      "epoch": 1.8629333333333333,
      "grad_norm": 0.06641997396945953,
      "learning_rate": 3.835666666666667e-05,
      "loss": 0.0024,
      "step": 34930
    },
    {
      "epoch": 1.8634666666666666,
      "grad_norm": 0.6622074842453003,
      "learning_rate": 3.835333333333334e-05,
      "loss": 0.004,
      "step": 34940
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.5104051232337952,
      "learning_rate": 3.8350000000000004e-05,
      "loss": 0.0036,
      "step": 34950
    },
    {
      "epoch": 1.8645333333333334,
      "grad_norm": 0.12153327465057373,
      "learning_rate": 3.834666666666667e-05,
      "loss": 0.0022,
      "step": 34960
    },
    {
      "epoch": 1.8650666666666667,
      "grad_norm": 0.15286487340927124,
      "learning_rate": 3.8343333333333336e-05,
      "loss": 0.0025,
      "step": 34970
    },
    {
      "epoch": 1.8656000000000001,
      "grad_norm": 0.3135187327861786,
      "learning_rate": 3.834e-05,
      "loss": 0.0036,
      "step": 34980
    },
    {
      "epoch": 1.8661333333333334,
      "grad_norm": 0.20944494009017944,
      "learning_rate": 3.833666666666667e-05,
      "loss": 0.0027,
      "step": 34990
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.27591320872306824,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 0.0026,
      "step": 35000
    },
    {
      "epoch": 1.8672,
      "grad_norm": 0.3197178840637207,
      "learning_rate": 3.833e-05,
      "loss": 0.0025,
      "step": 35010
    },
    {
      "epoch": 1.8677333333333332,
      "grad_norm": 0.273986279964447,
      "learning_rate": 3.832666666666667e-05,
      "loss": 0.0028,
      "step": 35020
    },
    {
      "epoch": 1.8682666666666665,
      "grad_norm": 0.15134075284004211,
      "learning_rate": 3.832333333333333e-05,
      "loss": 0.0036,
      "step": 35030
    },
    {
      "epoch": 1.8688,
      "grad_norm": 0.15103521943092346,
      "learning_rate": 3.832e-05,
      "loss": 0.0032,
      "step": 35040
    },
    {
      "epoch": 1.8693333333333333,
      "grad_norm": 0.061448633670806885,
      "learning_rate": 3.8316666666666665e-05,
      "loss": 0.0033,
      "step": 35050
    },
    {
      "epoch": 1.8698666666666668,
      "grad_norm": 0.5467529296875,
      "learning_rate": 3.831333333333333e-05,
      "loss": 0.0034,
      "step": 35060
    },
    {
      "epoch": 1.8704,
      "grad_norm": 0.36447927355766296,
      "learning_rate": 3.8310000000000004e-05,
      "loss": 0.0022,
      "step": 35070
    },
    {
      "epoch": 1.8709333333333333,
      "grad_norm": 0.5428769588470459,
      "learning_rate": 3.830666666666667e-05,
      "loss": 0.0037,
      "step": 35080
    },
    {
      "epoch": 1.8714666666666666,
      "grad_norm": 0.039139214903116226,
      "learning_rate": 3.8303333333333336e-05,
      "loss": 0.0024,
      "step": 35090
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.06822306662797928,
      "learning_rate": 3.83e-05,
      "loss": 0.0026,
      "step": 35100
    },
    {
      "epoch": 1.8725333333333334,
      "grad_norm": 0.5912320613861084,
      "learning_rate": 3.829666666666667e-05,
      "loss": 0.0027,
      "step": 35110
    },
    {
      "epoch": 1.8730666666666667,
      "grad_norm": 0.1793052852153778,
      "learning_rate": 3.8293333333333335e-05,
      "loss": 0.0031,
      "step": 35120
    },
    {
      "epoch": 1.8736000000000002,
      "grad_norm": 0.23864798247814178,
      "learning_rate": 3.829e-05,
      "loss": 0.0034,
      "step": 35130
    },
    {
      "epoch": 1.8741333333333334,
      "grad_norm": 0.4647649824619293,
      "learning_rate": 3.8286666666666674e-05,
      "loss": 0.0022,
      "step": 35140
    },
    {
      "epoch": 1.8746666666666667,
      "grad_norm": 0.12037738412618637,
      "learning_rate": 3.828333333333334e-05,
      "loss": 0.0028,
      "step": 35150
    },
    {
      "epoch": 1.8752,
      "grad_norm": 0.3002385199069977,
      "learning_rate": 3.828e-05,
      "loss": 0.0025,
      "step": 35160
    },
    {
      "epoch": 1.8757333333333333,
      "grad_norm": 0.0718492716550827,
      "learning_rate": 3.8276666666666666e-05,
      "loss": 0.0033,
      "step": 35170
    },
    {
      "epoch": 1.8762666666666665,
      "grad_norm": 0.08582903444766998,
      "learning_rate": 3.827333333333333e-05,
      "loss": 0.0033,
      "step": 35180
    },
    {
      "epoch": 1.8768,
      "grad_norm": 0.4181625545024872,
      "learning_rate": 3.827e-05,
      "loss": 0.0025,
      "step": 35190
    },
    {
      "epoch": 1.8773333333333333,
      "grad_norm": 0.24064244329929352,
      "learning_rate": 3.8266666666666664e-05,
      "loss": 0.0022,
      "step": 35200
    },
    {
      "epoch": 1.8778666666666668,
      "grad_norm": 0.303569495677948,
      "learning_rate": 3.826333333333334e-05,
      "loss": 0.0026,
      "step": 35210
    },
    {
      "epoch": 1.8784,
      "grad_norm": 0.09871159493923187,
      "learning_rate": 3.826e-05,
      "loss": 0.0022,
      "step": 35220
    },
    {
      "epoch": 1.8789333333333333,
      "grad_norm": 0.02324756421148777,
      "learning_rate": 3.825666666666667e-05,
      "loss": 0.0042,
      "step": 35230
    },
    {
      "epoch": 1.8794666666666666,
      "grad_norm": 0.24672538042068481,
      "learning_rate": 3.8253333333333335e-05,
      "loss": 0.0034,
      "step": 35240
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.15787291526794434,
      "learning_rate": 3.825e-05,
      "loss": 0.0025,
      "step": 35250
    },
    {
      "epoch": 1.8805333333333332,
      "grad_norm": 0.1821443736553192,
      "learning_rate": 3.824666666666667e-05,
      "loss": 0.0016,
      "step": 35260
    },
    {
      "epoch": 1.8810666666666667,
      "grad_norm": 0.24078522622585297,
      "learning_rate": 3.8243333333333334e-05,
      "loss": 0.0031,
      "step": 35270
    },
    {
      "epoch": 1.8816000000000002,
      "grad_norm": 0.21025925874710083,
      "learning_rate": 3.8240000000000007e-05,
      "loss": 0.0019,
      "step": 35280
    },
    {
      "epoch": 1.8821333333333334,
      "grad_norm": 0.20855404436588287,
      "learning_rate": 3.823666666666667e-05,
      "loss": 0.0032,
      "step": 35290
    },
    {
      "epoch": 1.8826666666666667,
      "grad_norm": 0.04498422145843506,
      "learning_rate": 3.823333333333334e-05,
      "loss": 0.0027,
      "step": 35300
    },
    {
      "epoch": 1.8832,
      "grad_norm": 0.42310523986816406,
      "learning_rate": 3.823e-05,
      "loss": 0.0045,
      "step": 35310
    },
    {
      "epoch": 1.8837333333333333,
      "grad_norm": 0.20933738350868225,
      "learning_rate": 3.8226666666666664e-05,
      "loss": 0.0029,
      "step": 35320
    },
    {
      "epoch": 1.8842666666666665,
      "grad_norm": 0.15129117667675018,
      "learning_rate": 3.822333333333333e-05,
      "loss": 0.0027,
      "step": 35330
    },
    {
      "epoch": 1.8848,
      "grad_norm": 0.18129540979862213,
      "learning_rate": 3.822e-05,
      "loss": 0.0031,
      "step": 35340
    },
    {
      "epoch": 1.8853333333333333,
      "grad_norm": 0.18103757500648499,
      "learning_rate": 3.821666666666667e-05,
      "loss": 0.0028,
      "step": 35350
    },
    {
      "epoch": 1.8858666666666668,
      "grad_norm": 0.5738167762756348,
      "learning_rate": 3.8213333333333336e-05,
      "loss": 0.003,
      "step": 35360
    },
    {
      "epoch": 1.8864,
      "grad_norm": 0.21069730818271637,
      "learning_rate": 3.821e-05,
      "loss": 0.0028,
      "step": 35370
    },
    {
      "epoch": 1.8869333333333334,
      "grad_norm": 0.5423207879066467,
      "learning_rate": 3.820666666666667e-05,
      "loss": 0.0023,
      "step": 35380
    },
    {
      "epoch": 1.8874666666666666,
      "grad_norm": 0.4940260946750641,
      "learning_rate": 3.8203333333333334e-05,
      "loss": 0.0043,
      "step": 35390
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.35248225927352905,
      "learning_rate": 3.82e-05,
      "loss": 0.004,
      "step": 35400
    },
    {
      "epoch": 1.8885333333333332,
      "grad_norm": 0.18175649642944336,
      "learning_rate": 3.8196666666666666e-05,
      "loss": 0.0021,
      "step": 35410
    },
    {
      "epoch": 1.8890666666666667,
      "grad_norm": 0.06947287172079086,
      "learning_rate": 3.819333333333334e-05,
      "loss": 0.0027,
      "step": 35420
    },
    {
      "epoch": 1.8896,
      "grad_norm": 0.02633562497794628,
      "learning_rate": 3.8190000000000005e-05,
      "loss": 0.0029,
      "step": 35430
    },
    {
      "epoch": 1.8901333333333334,
      "grad_norm": 0.18036597967147827,
      "learning_rate": 3.818666666666667e-05,
      "loss": 0.0038,
      "step": 35440
    },
    {
      "epoch": 1.8906666666666667,
      "grad_norm": 0.5130717754364014,
      "learning_rate": 3.818333333333334e-05,
      "loss": 0.0022,
      "step": 35450
    },
    {
      "epoch": 1.8912,
      "grad_norm": 0.15076394379138947,
      "learning_rate": 3.818e-05,
      "loss": 0.0019,
      "step": 35460
    },
    {
      "epoch": 1.8917333333333333,
      "grad_norm": 0.7400343418121338,
      "learning_rate": 3.817666666666666e-05,
      "loss": 0.003,
      "step": 35470
    },
    {
      "epoch": 1.8922666666666665,
      "grad_norm": 0.12211056053638458,
      "learning_rate": 3.8173333333333336e-05,
      "loss": 0.0029,
      "step": 35480
    },
    {
      "epoch": 1.8928,
      "grad_norm": 0.12140234559774399,
      "learning_rate": 3.817e-05,
      "loss": 0.0021,
      "step": 35490
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 0.06672917306423187,
      "learning_rate": 3.816666666666667e-05,
      "loss": 0.0028,
      "step": 35500
    },
    {
      "epoch": 1.8938666666666668,
      "grad_norm": 0.29884082078933716,
      "learning_rate": 3.8163333333333334e-05,
      "loss": 0.0024,
      "step": 35510
    },
    {
      "epoch": 1.8944,
      "grad_norm": 0.06242569535970688,
      "learning_rate": 3.816e-05,
      "loss": 0.0017,
      "step": 35520
    },
    {
      "epoch": 1.8949333333333334,
      "grad_norm": 0.2672465741634369,
      "learning_rate": 3.815666666666667e-05,
      "loss": 0.0021,
      "step": 35530
    },
    {
      "epoch": 1.8954666666666666,
      "grad_norm": 0.18515950441360474,
      "learning_rate": 3.815333333333333e-05,
      "loss": 0.0029,
      "step": 35540
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.36849722266197205,
      "learning_rate": 3.8150000000000006e-05,
      "loss": 0.0037,
      "step": 35550
    },
    {
      "epoch": 1.8965333333333332,
      "grad_norm": 0.6038175225257874,
      "learning_rate": 3.814666666666667e-05,
      "loss": 0.0021,
      "step": 35560
    },
    {
      "epoch": 1.8970666666666667,
      "grad_norm": 0.06938857585191727,
      "learning_rate": 3.814333333333334e-05,
      "loss": 0.0032,
      "step": 35570
    },
    {
      "epoch": 1.8976,
      "grad_norm": 0.2392609864473343,
      "learning_rate": 3.8140000000000004e-05,
      "loss": 0.0031,
      "step": 35580
    },
    {
      "epoch": 1.8981333333333335,
      "grad_norm": 0.12791651487350464,
      "learning_rate": 3.813666666666667e-05,
      "loss": 0.0023,
      "step": 35590
    },
    {
      "epoch": 1.8986666666666667,
      "grad_norm": 0.39689913392066956,
      "learning_rate": 3.8133333333333336e-05,
      "loss": 0.003,
      "step": 35600
    },
    {
      "epoch": 1.8992,
      "grad_norm": 0.4165274500846863,
      "learning_rate": 3.8129999999999996e-05,
      "loss": 0.0032,
      "step": 35610
    },
    {
      "epoch": 1.8997333333333333,
      "grad_norm": 0.3897627294063568,
      "learning_rate": 3.812666666666667e-05,
      "loss": 0.0025,
      "step": 35620
    },
    {
      "epoch": 1.9002666666666665,
      "grad_norm": 0.09017331153154373,
      "learning_rate": 3.8123333333333335e-05,
      "loss": 0.0034,
      "step": 35630
    },
    {
      "epoch": 1.9008,
      "grad_norm": 0.2671779990196228,
      "learning_rate": 3.812e-05,
      "loss": 0.0033,
      "step": 35640
    },
    {
      "epoch": 1.9013333333333333,
      "grad_norm": 0.5963029861450195,
      "learning_rate": 3.811666666666667e-05,
      "loss": 0.0022,
      "step": 35650
    },
    {
      "epoch": 1.9018666666666668,
      "grad_norm": 0.12148461490869522,
      "learning_rate": 3.811333333333333e-05,
      "loss": 0.002,
      "step": 35660
    },
    {
      "epoch": 1.9024,
      "grad_norm": 0.04269877076148987,
      "learning_rate": 3.811e-05,
      "loss": 0.0015,
      "step": 35670
    },
    {
      "epoch": 1.9029333333333334,
      "grad_norm": 0.15729884803295135,
      "learning_rate": 3.8106666666666665e-05,
      "loss": 0.0039,
      "step": 35680
    },
    {
      "epoch": 1.9034666666666666,
      "grad_norm": 0.3011475205421448,
      "learning_rate": 3.810333333333334e-05,
      "loss": 0.0022,
      "step": 35690
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.18113218247890472,
      "learning_rate": 3.8100000000000005e-05,
      "loss": 0.0025,
      "step": 35700
    },
    {
      "epoch": 1.9045333333333332,
      "grad_norm": 0.9526964426040649,
      "learning_rate": 3.809666666666667e-05,
      "loss": 0.0027,
      "step": 35710
    },
    {
      "epoch": 1.9050666666666667,
      "grad_norm": 0.05128254368901253,
      "learning_rate": 3.809333333333334e-05,
      "loss": 0.0029,
      "step": 35720
    },
    {
      "epoch": 1.9056,
      "grad_norm": 0.10169586539268494,
      "learning_rate": 3.809e-05,
      "loss": 0.0029,
      "step": 35730
    },
    {
      "epoch": 1.9061333333333335,
      "grad_norm": 0.06764209270477295,
      "learning_rate": 3.808666666666667e-05,
      "loss": 0.0015,
      "step": 35740
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.05916929617524147,
      "learning_rate": 3.8083333333333335e-05,
      "loss": 0.0028,
      "step": 35750
    },
    {
      "epoch": 1.9072,
      "grad_norm": 0.6867960691452026,
      "learning_rate": 3.808e-05,
      "loss": 0.0035,
      "step": 35760
    },
    {
      "epoch": 1.9077333333333333,
      "grad_norm": 0.3129231333732605,
      "learning_rate": 3.807666666666667e-05,
      "loss": 0.0023,
      "step": 35770
    },
    {
      "epoch": 1.9082666666666666,
      "grad_norm": 0.3237118422985077,
      "learning_rate": 3.8073333333333334e-05,
      "loss": 0.0038,
      "step": 35780
    },
    {
      "epoch": 1.9088,
      "grad_norm": 0.46843421459198,
      "learning_rate": 3.807e-05,
      "loss": 0.003,
      "step": 35790
    },
    {
      "epoch": 1.9093333333333333,
      "grad_norm": 0.10068655014038086,
      "learning_rate": 3.8066666666666666e-05,
      "loss": 0.0036,
      "step": 35800
    },
    {
      "epoch": 1.9098666666666668,
      "grad_norm": 0.15128162503242493,
      "learning_rate": 3.806333333333333e-05,
      "loss": 0.003,
      "step": 35810
    },
    {
      "epoch": 1.9104,
      "grad_norm": 0.21843411028385162,
      "learning_rate": 3.806e-05,
      "loss": 0.0033,
      "step": 35820
    },
    {
      "epoch": 1.9109333333333334,
      "grad_norm": 0.5339992046356201,
      "learning_rate": 3.805666666666667e-05,
      "loss": 0.0037,
      "step": 35830
    },
    {
      "epoch": 1.9114666666666666,
      "grad_norm": 0.2828770875930786,
      "learning_rate": 3.805333333333334e-05,
      "loss": 0.0034,
      "step": 35840
    },
    {
      "epoch": 1.912,
      "grad_norm": 0.3901256024837494,
      "learning_rate": 3.805e-05,
      "loss": 0.0026,
      "step": 35850
    },
    {
      "epoch": 1.9125333333333332,
      "grad_norm": 0.327923983335495,
      "learning_rate": 3.804666666666667e-05,
      "loss": 0.0027,
      "step": 35860
    },
    {
      "epoch": 1.9130666666666667,
      "grad_norm": 0.3272448182106018,
      "learning_rate": 3.8043333333333336e-05,
      "loss": 0.0023,
      "step": 35870
    },
    {
      "epoch": 1.9136,
      "grad_norm": 0.4169245958328247,
      "learning_rate": 3.804e-05,
      "loss": 0.0042,
      "step": 35880
    },
    {
      "epoch": 1.9141333333333335,
      "grad_norm": 0.5472246408462524,
      "learning_rate": 3.803666666666667e-05,
      "loss": 0.0026,
      "step": 35890
    },
    {
      "epoch": 1.9146666666666667,
      "grad_norm": 0.32400935888290405,
      "learning_rate": 3.803333333333334e-05,
      "loss": 0.0031,
      "step": 35900
    },
    {
      "epoch": 1.9152,
      "grad_norm": 0.1829819679260254,
      "learning_rate": 3.803000000000001e-05,
      "loss": 0.0032,
      "step": 35910
    },
    {
      "epoch": 1.9157333333333333,
      "grad_norm": 0.4746370315551758,
      "learning_rate": 3.8026666666666666e-05,
      "loss": 0.0017,
      "step": 35920
    },
    {
      "epoch": 1.9162666666666666,
      "grad_norm": 0.24322833120822906,
      "learning_rate": 3.802333333333333e-05,
      "loss": 0.0042,
      "step": 35930
    },
    {
      "epoch": 1.9167999999999998,
      "grad_norm": 0.2710714042186737,
      "learning_rate": 3.802e-05,
      "loss": 0.0024,
      "step": 35940
    },
    {
      "epoch": 1.9173333333333333,
      "grad_norm": 0.040412165224552155,
      "learning_rate": 3.8016666666666665e-05,
      "loss": 0.0033,
      "step": 35950
    },
    {
      "epoch": 1.9178666666666668,
      "grad_norm": 0.19272953271865845,
      "learning_rate": 3.801333333333333e-05,
      "loss": 0.0036,
      "step": 35960
    },
    {
      "epoch": 1.9184,
      "grad_norm": 0.12420904636383057,
      "learning_rate": 3.8010000000000004e-05,
      "loss": 0.0039,
      "step": 35970
    },
    {
      "epoch": 1.9189333333333334,
      "grad_norm": 0.4498886168003082,
      "learning_rate": 3.800666666666667e-05,
      "loss": 0.0027,
      "step": 35980
    },
    {
      "epoch": 1.9194666666666667,
      "grad_norm": 0.12135010957717896,
      "learning_rate": 3.8003333333333336e-05,
      "loss": 0.0034,
      "step": 35990
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.13783138990402222,
      "learning_rate": 3.8e-05,
      "loss": 0.0032,
      "step": 36000
    },
    {
      "epoch": 1.9205333333333332,
      "grad_norm": 0.21133075654506683,
      "learning_rate": 3.799666666666667e-05,
      "loss": 0.0023,
      "step": 36010
    },
    {
      "epoch": 1.9210666666666667,
      "grad_norm": 0.47791653871536255,
      "learning_rate": 3.7993333333333334e-05,
      "loss": 0.0024,
      "step": 36020
    },
    {
      "epoch": 1.9216,
      "grad_norm": 0.39370784163475037,
      "learning_rate": 3.799e-05,
      "loss": 0.0042,
      "step": 36030
    },
    {
      "epoch": 1.9221333333333335,
      "grad_norm": 0.06334569305181503,
      "learning_rate": 3.7986666666666673e-05,
      "loss": 0.0033,
      "step": 36040
    },
    {
      "epoch": 1.9226666666666667,
      "grad_norm": 0.15408292412757874,
      "learning_rate": 3.798333333333334e-05,
      "loss": 0.0028,
      "step": 36050
    },
    {
      "epoch": 1.9232,
      "grad_norm": 0.09353026002645493,
      "learning_rate": 3.7980000000000006e-05,
      "loss": 0.0025,
      "step": 36060
    },
    {
      "epoch": 1.9237333333333333,
      "grad_norm": 0.12444502115249634,
      "learning_rate": 3.7976666666666665e-05,
      "loss": 0.0036,
      "step": 36070
    },
    {
      "epoch": 1.9242666666666666,
      "grad_norm": 0.09378864616155624,
      "learning_rate": 3.797333333333333e-05,
      "loss": 0.0028,
      "step": 36080
    },
    {
      "epoch": 1.9247999999999998,
      "grad_norm": 0.33287495374679565,
      "learning_rate": 3.797e-05,
      "loss": 0.0046,
      "step": 36090
    },
    {
      "epoch": 1.9253333333333333,
      "grad_norm": 0.15947356820106506,
      "learning_rate": 3.796666666666667e-05,
      "loss": 0.0039,
      "step": 36100
    },
    {
      "epoch": 1.9258666666666666,
      "grad_norm": 0.08243408054113388,
      "learning_rate": 3.7963333333333336e-05,
      "loss": 0.0023,
      "step": 36110
    },
    {
      "epoch": 1.9264000000000001,
      "grad_norm": 0.1836869716644287,
      "learning_rate": 3.796e-05,
      "loss": 0.0026,
      "step": 36120
    },
    {
      "epoch": 1.9269333333333334,
      "grad_norm": 0.12272726744413376,
      "learning_rate": 3.795666666666667e-05,
      "loss": 0.003,
      "step": 36130
    },
    {
      "epoch": 1.9274666666666667,
      "grad_norm": 0.2998057007789612,
      "learning_rate": 3.7953333333333335e-05,
      "loss": 0.0029,
      "step": 36140
    },
    {
      "epoch": 1.928,
      "grad_norm": 0.298814982175827,
      "learning_rate": 3.795e-05,
      "loss": 0.0034,
      "step": 36150
    },
    {
      "epoch": 1.9285333333333332,
      "grad_norm": 0.2130383551120758,
      "learning_rate": 3.794666666666667e-05,
      "loss": 0.0025,
      "step": 36160
    },
    {
      "epoch": 1.9290666666666667,
      "grad_norm": 0.21503162384033203,
      "learning_rate": 3.794333333333333e-05,
      "loss": 0.0029,
      "step": 36170
    },
    {
      "epoch": 1.9296,
      "grad_norm": 0.21537330746650696,
      "learning_rate": 3.7940000000000006e-05,
      "loss": 0.0025,
      "step": 36180
    },
    {
      "epoch": 1.9301333333333335,
      "grad_norm": 0.19971324503421783,
      "learning_rate": 3.793666666666667e-05,
      "loss": 0.0021,
      "step": 36190
    },
    {
      "epoch": 1.9306666666666668,
      "grad_norm": 0.2096843123435974,
      "learning_rate": 3.793333333333334e-05,
      "loss": 0.0028,
      "step": 36200
    },
    {
      "epoch": 1.9312,
      "grad_norm": 0.09181438386440277,
      "learning_rate": 3.7930000000000004e-05,
      "loss": 0.0032,
      "step": 36210
    },
    {
      "epoch": 1.9317333333333333,
      "grad_norm": 0.4205157160758972,
      "learning_rate": 3.7926666666666664e-05,
      "loss": 0.0021,
      "step": 36220
    },
    {
      "epoch": 1.9322666666666666,
      "grad_norm": 0.046618904918432236,
      "learning_rate": 3.792333333333333e-05,
      "loss": 0.0025,
      "step": 36230
    },
    {
      "epoch": 1.9327999999999999,
      "grad_norm": 0.4473310708999634,
      "learning_rate": 3.792e-05,
      "loss": 0.0042,
      "step": 36240
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.23960617184638977,
      "learning_rate": 3.791666666666667e-05,
      "loss": 0.0032,
      "step": 36250
    },
    {
      "epoch": 1.9338666666666666,
      "grad_norm": 0.0479346327483654,
      "learning_rate": 3.7913333333333335e-05,
      "loss": 0.003,
      "step": 36260
    },
    {
      "epoch": 1.9344000000000001,
      "grad_norm": 0.6886792182922363,
      "learning_rate": 3.791e-05,
      "loss": 0.0022,
      "step": 36270
    },
    {
      "epoch": 1.9349333333333334,
      "grad_norm": 0.47978365421295166,
      "learning_rate": 3.790666666666667e-05,
      "loss": 0.0034,
      "step": 36280
    },
    {
      "epoch": 1.9354666666666667,
      "grad_norm": 0.5125924944877625,
      "learning_rate": 3.7903333333333334e-05,
      "loss": 0.0037,
      "step": 36290
    },
    {
      "epoch": 1.936,
      "grad_norm": 0.2137284129858017,
      "learning_rate": 3.79e-05,
      "loss": 0.0026,
      "step": 36300
    },
    {
      "epoch": 1.9365333333333332,
      "grad_norm": 0.06635713577270508,
      "learning_rate": 3.789666666666667e-05,
      "loss": 0.0025,
      "step": 36310
    },
    {
      "epoch": 1.9370666666666667,
      "grad_norm": 0.30452555418014526,
      "learning_rate": 3.789333333333334e-05,
      "loss": 0.0031,
      "step": 36320
    },
    {
      "epoch": 1.9376,
      "grad_norm": 0.03492048755288124,
      "learning_rate": 3.7890000000000005e-05,
      "loss": 0.0023,
      "step": 36330
    },
    {
      "epoch": 1.9381333333333335,
      "grad_norm": 0.11968289315700531,
      "learning_rate": 3.788666666666667e-05,
      "loss": 0.0029,
      "step": 36340
    },
    {
      "epoch": 1.9386666666666668,
      "grad_norm": 0.2682163119316101,
      "learning_rate": 3.788333333333334e-05,
      "loss": 0.0032,
      "step": 36350
    },
    {
      "epoch": 1.9392,
      "grad_norm": 0.12687774002552032,
      "learning_rate": 3.788e-05,
      "loss": 0.0036,
      "step": 36360
    },
    {
      "epoch": 1.9397333333333333,
      "grad_norm": 0.1858360320329666,
      "learning_rate": 3.787666666666666e-05,
      "loss": 0.0027,
      "step": 36370
    },
    {
      "epoch": 1.9402666666666666,
      "grad_norm": 0.026945946738123894,
      "learning_rate": 3.7873333333333336e-05,
      "loss": 0.0022,
      "step": 36380
    },
    {
      "epoch": 1.9407999999999999,
      "grad_norm": 0.1795409619808197,
      "learning_rate": 3.787e-05,
      "loss": 0.0025,
      "step": 36390
    },
    {
      "epoch": 1.9413333333333334,
      "grad_norm": 0.21434029936790466,
      "learning_rate": 3.786666666666667e-05,
      "loss": 0.0032,
      "step": 36400
    },
    {
      "epoch": 1.9418666666666666,
      "grad_norm": 0.2115693837404251,
      "learning_rate": 3.7863333333333334e-05,
      "loss": 0.0026,
      "step": 36410
    },
    {
      "epoch": 1.9424000000000001,
      "grad_norm": 0.24086304008960724,
      "learning_rate": 3.786e-05,
      "loss": 0.0024,
      "step": 36420
    },
    {
      "epoch": 1.9429333333333334,
      "grad_norm": 0.31395965814590454,
      "learning_rate": 3.7856666666666666e-05,
      "loss": 0.0024,
      "step": 36430
    },
    {
      "epoch": 1.9434666666666667,
      "grad_norm": 0.4177689254283905,
      "learning_rate": 3.785333333333333e-05,
      "loss": 0.0033,
      "step": 36440
    },
    {
      "epoch": 1.944,
      "grad_norm": 0.4191417694091797,
      "learning_rate": 3.7850000000000005e-05,
      "loss": 0.0044,
      "step": 36450
    },
    {
      "epoch": 1.9445333333333332,
      "grad_norm": 0.5683281421661377,
      "learning_rate": 3.784666666666667e-05,
      "loss": 0.0028,
      "step": 36460
    },
    {
      "epoch": 1.9450666666666667,
      "grad_norm": 0.25995782017707825,
      "learning_rate": 3.784333333333334e-05,
      "loss": 0.0039,
      "step": 36470
    },
    {
      "epoch": 1.9456,
      "grad_norm": 0.09909933805465698,
      "learning_rate": 3.7840000000000004e-05,
      "loss": 0.0024,
      "step": 36480
    },
    {
      "epoch": 1.9461333333333335,
      "grad_norm": 0.18992581963539124,
      "learning_rate": 3.783666666666667e-05,
      "loss": 0.0032,
      "step": 36490
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 0.3565102219581604,
      "learning_rate": 3.7833333333333336e-05,
      "loss": 0.0025,
      "step": 36500
    },
    {
      "epoch": 1.9472,
      "grad_norm": 0.12387117743492126,
      "learning_rate": 3.783e-05,
      "loss": 0.0029,
      "step": 36510
    },
    {
      "epoch": 1.9477333333333333,
      "grad_norm": 0.2704993486404419,
      "learning_rate": 3.782666666666667e-05,
      "loss": 0.002,
      "step": 36520
    },
    {
      "epoch": 1.9482666666666666,
      "grad_norm": 0.8344553709030151,
      "learning_rate": 3.7823333333333334e-05,
      "loss": 0.0019,
      "step": 36530
    },
    {
      "epoch": 1.9487999999999999,
      "grad_norm": 0.3994492292404175,
      "learning_rate": 3.782e-05,
      "loss": 0.0036,
      "step": 36540
    },
    {
      "epoch": 1.9493333333333334,
      "grad_norm": 0.09397096931934357,
      "learning_rate": 3.7816666666666667e-05,
      "loss": 0.002,
      "step": 36550
    },
    {
      "epoch": 1.9498666666666666,
      "grad_norm": 0.2384222149848938,
      "learning_rate": 3.781333333333333e-05,
      "loss": 0.0023,
      "step": 36560
    },
    {
      "epoch": 1.9504000000000001,
      "grad_norm": 0.21057578921318054,
      "learning_rate": 3.781e-05,
      "loss": 0.0031,
      "step": 36570
    },
    {
      "epoch": 1.9509333333333334,
      "grad_norm": 0.15291810035705566,
      "learning_rate": 3.7806666666666665e-05,
      "loss": 0.0026,
      "step": 36580
    },
    {
      "epoch": 1.9514666666666667,
      "grad_norm": 0.23326854407787323,
      "learning_rate": 3.780333333333334e-05,
      "loss": 0.0034,
      "step": 36590
    },
    {
      "epoch": 1.952,
      "grad_norm": 0.33037084341049194,
      "learning_rate": 3.7800000000000004e-05,
      "loss": 0.0031,
      "step": 36600
    },
    {
      "epoch": 1.9525333333333332,
      "grad_norm": 0.47987043857574463,
      "learning_rate": 3.779666666666667e-05,
      "loss": 0.0031,
      "step": 36610
    },
    {
      "epoch": 1.9530666666666665,
      "grad_norm": 0.49796485900878906,
      "learning_rate": 3.7793333333333336e-05,
      "loss": 0.0032,
      "step": 36620
    },
    {
      "epoch": 1.9536,
      "grad_norm": 0.48075175285339355,
      "learning_rate": 3.779e-05,
      "loss": 0.0018,
      "step": 36630
    },
    {
      "epoch": 1.9541333333333335,
      "grad_norm": 0.47475388646125793,
      "learning_rate": 3.778666666666667e-05,
      "loss": 0.004,
      "step": 36640
    },
    {
      "epoch": 1.9546666666666668,
      "grad_norm": 0.4471554160118103,
      "learning_rate": 3.7783333333333335e-05,
      "loss": 0.0024,
      "step": 36650
    },
    {
      "epoch": 1.9552,
      "grad_norm": 0.05260477587580681,
      "learning_rate": 3.778000000000001e-05,
      "loss": 0.0026,
      "step": 36660
    },
    {
      "epoch": 1.9557333333333333,
      "grad_norm": 0.18249356746673584,
      "learning_rate": 3.777666666666667e-05,
      "loss": 0.0026,
      "step": 36670
    },
    {
      "epoch": 1.9562666666666666,
      "grad_norm": 0.08135410398244858,
      "learning_rate": 3.777333333333333e-05,
      "loss": 0.0021,
      "step": 36680
    },
    {
      "epoch": 1.9567999999999999,
      "grad_norm": 0.30321162939071655,
      "learning_rate": 3.777e-05,
      "loss": 0.0028,
      "step": 36690
    },
    {
      "epoch": 1.9573333333333334,
      "grad_norm": 0.29051896929740906,
      "learning_rate": 3.7766666666666665e-05,
      "loss": 0.0037,
      "step": 36700
    },
    {
      "epoch": 1.9578666666666666,
      "grad_norm": 0.3176700472831726,
      "learning_rate": 3.776333333333333e-05,
      "loss": 0.0042,
      "step": 36710
    },
    {
      "epoch": 1.9584000000000001,
      "grad_norm": 0.27512046694755554,
      "learning_rate": 3.776e-05,
      "loss": 0.0032,
      "step": 36720
    },
    {
      "epoch": 1.9589333333333334,
      "grad_norm": 0.06747312098741531,
      "learning_rate": 3.775666666666667e-05,
      "loss": 0.0041,
      "step": 36730
    },
    {
      "epoch": 1.9594666666666667,
      "grad_norm": 0.12416008114814758,
      "learning_rate": 3.775333333333334e-05,
      "loss": 0.0026,
      "step": 36740
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.5993200540542603,
      "learning_rate": 3.775e-05,
      "loss": 0.0039,
      "step": 36750
    },
    {
      "epoch": 1.9605333333333332,
      "grad_norm": 0.03650224208831787,
      "learning_rate": 3.774666666666667e-05,
      "loss": 0.005,
      "step": 36760
    },
    {
      "epoch": 1.9610666666666665,
      "grad_norm": 0.06807505339384079,
      "learning_rate": 3.7743333333333335e-05,
      "loss": 0.0026,
      "step": 36770
    },
    {
      "epoch": 1.9616,
      "grad_norm": 0.049953483045101166,
      "learning_rate": 3.774e-05,
      "loss": 0.0031,
      "step": 36780
    },
    {
      "epoch": 1.9621333333333333,
      "grad_norm": 0.2099892646074295,
      "learning_rate": 3.773666666666667e-05,
      "loss": 0.0045,
      "step": 36790
    },
    {
      "epoch": 1.9626666666666668,
      "grad_norm": 0.06540422141551971,
      "learning_rate": 3.773333333333334e-05,
      "loss": 0.0034,
      "step": 36800
    },
    {
      "epoch": 1.9632,
      "grad_norm": 0.124765545129776,
      "learning_rate": 3.7730000000000006e-05,
      "loss": 0.0038,
      "step": 36810
    },
    {
      "epoch": 1.9637333333333333,
      "grad_norm": 0.4011988341808319,
      "learning_rate": 3.7726666666666666e-05,
      "loss": 0.0018,
      "step": 36820
    },
    {
      "epoch": 1.9642666666666666,
      "grad_norm": 0.09312362968921661,
      "learning_rate": 3.772333333333333e-05,
      "loss": 0.0021,
      "step": 36830
    },
    {
      "epoch": 1.9647999999999999,
      "grad_norm": 0.09267780184745789,
      "learning_rate": 3.772e-05,
      "loss": 0.0023,
      "step": 36840
    },
    {
      "epoch": 1.9653333333333334,
      "grad_norm": 0.04738079383969307,
      "learning_rate": 3.7716666666666664e-05,
      "loss": 0.0022,
      "step": 36850
    },
    {
      "epoch": 1.9658666666666667,
      "grad_norm": 0.5668637752532959,
      "learning_rate": 3.771333333333334e-05,
      "loss": 0.0026,
      "step": 36860
    },
    {
      "epoch": 1.9664000000000001,
      "grad_norm": 0.3593748211860657,
      "learning_rate": 3.771e-05,
      "loss": 0.0025,
      "step": 36870
    },
    {
      "epoch": 1.9669333333333334,
      "grad_norm": 0.23862923681735992,
      "learning_rate": 3.770666666666667e-05,
      "loss": 0.0024,
      "step": 36880
    },
    {
      "epoch": 1.9674666666666667,
      "grad_norm": 0.4175572991371155,
      "learning_rate": 3.7703333333333335e-05,
      "loss": 0.0017,
      "step": 36890
    },
    {
      "epoch": 1.968,
      "grad_norm": 0.32733339071273804,
      "learning_rate": 3.77e-05,
      "loss": 0.0039,
      "step": 36900
    },
    {
      "epoch": 1.9685333333333332,
      "grad_norm": 0.2426764816045761,
      "learning_rate": 3.769666666666667e-05,
      "loss": 0.0023,
      "step": 36910
    },
    {
      "epoch": 1.9690666666666665,
      "grad_norm": 0.35666725039482117,
      "learning_rate": 3.7693333333333334e-05,
      "loss": 0.0029,
      "step": 36920
    },
    {
      "epoch": 1.9696,
      "grad_norm": 0.30013415217399597,
      "learning_rate": 3.769e-05,
      "loss": 0.0017,
      "step": 36930
    },
    {
      "epoch": 1.9701333333333333,
      "grad_norm": 0.5354354381561279,
      "learning_rate": 3.768666666666667e-05,
      "loss": 0.0032,
      "step": 36940
    },
    {
      "epoch": 1.9706666666666668,
      "grad_norm": 0.057566892355680466,
      "learning_rate": 3.768333333333334e-05,
      "loss": 0.0026,
      "step": 36950
    },
    {
      "epoch": 1.9712,
      "grad_norm": 0.17875038087368011,
      "learning_rate": 3.7680000000000005e-05,
      "loss": 0.0035,
      "step": 36960
    },
    {
      "epoch": 1.9717333333333333,
      "grad_norm": 0.15264566242694855,
      "learning_rate": 3.767666666666667e-05,
      "loss": 0.0037,
      "step": 36970
    },
    {
      "epoch": 1.9722666666666666,
      "grad_norm": 0.10033589601516724,
      "learning_rate": 3.767333333333333e-05,
      "loss": 0.0031,
      "step": 36980
    },
    {
      "epoch": 1.9727999999999999,
      "grad_norm": 0.10090233385562897,
      "learning_rate": 3.767e-05,
      "loss": 0.0029,
      "step": 36990
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 0.2966471314430237,
      "learning_rate": 3.766666666666667e-05,
      "loss": 0.0034,
      "step": 37000
    },
    {
      "epoch": 1.9738666666666667,
      "grad_norm": 0.06399223208427429,
      "learning_rate": 3.7663333333333336e-05,
      "loss": 0.002,
      "step": 37010
    },
    {
      "epoch": 1.9744000000000002,
      "grad_norm": 0.3258039057254791,
      "learning_rate": 3.766e-05,
      "loss": 0.0026,
      "step": 37020
    },
    {
      "epoch": 1.9749333333333334,
      "grad_norm": 0.7440112829208374,
      "learning_rate": 3.765666666666667e-05,
      "loss": 0.0032,
      "step": 37030
    },
    {
      "epoch": 1.9754666666666667,
      "grad_norm": 0.5056365132331848,
      "learning_rate": 3.7653333333333334e-05,
      "loss": 0.0025,
      "step": 37040
    },
    {
      "epoch": 1.976,
      "grad_norm": 0.17788858711719513,
      "learning_rate": 3.765e-05,
      "loss": 0.0025,
      "step": 37050
    },
    {
      "epoch": 1.9765333333333333,
      "grad_norm": 0.18458861112594604,
      "learning_rate": 3.7646666666666666e-05,
      "loss": 0.002,
      "step": 37060
    },
    {
      "epoch": 1.9770666666666665,
      "grad_norm": 0.03845194727182388,
      "learning_rate": 3.764333333333333e-05,
      "loss": 0.0042,
      "step": 37070
    },
    {
      "epoch": 1.9776,
      "grad_norm": 0.3986296057701111,
      "learning_rate": 3.7640000000000006e-05,
      "loss": 0.0023,
      "step": 37080
    },
    {
      "epoch": 1.9781333333333333,
      "grad_norm": 0.32707446813583374,
      "learning_rate": 3.763666666666667e-05,
      "loss": 0.0025,
      "step": 37090
    },
    {
      "epoch": 1.9786666666666668,
      "grad_norm": 0.3569069504737854,
      "learning_rate": 3.763333333333334e-05,
      "loss": 0.0027,
      "step": 37100
    },
    {
      "epoch": 1.9792,
      "grad_norm": 0.268216997385025,
      "learning_rate": 3.7630000000000004e-05,
      "loss": 0.0022,
      "step": 37110
    },
    {
      "epoch": 1.9797333333333333,
      "grad_norm": 0.18026337027549744,
      "learning_rate": 3.762666666666667e-05,
      "loss": 0.0025,
      "step": 37120
    },
    {
      "epoch": 1.9802666666666666,
      "grad_norm": 0.4180750548839569,
      "learning_rate": 3.762333333333333e-05,
      "loss": 0.0042,
      "step": 37130
    },
    {
      "epoch": 1.9808,
      "grad_norm": 0.4459415674209595,
      "learning_rate": 3.762e-05,
      "loss": 0.0042,
      "step": 37140
    },
    {
      "epoch": 1.9813333333333332,
      "grad_norm": 0.15273116528987885,
      "learning_rate": 3.761666666666667e-05,
      "loss": 0.0023,
      "step": 37150
    },
    {
      "epoch": 1.9818666666666667,
      "grad_norm": 0.35711508989334106,
      "learning_rate": 3.7613333333333335e-05,
      "loss": 0.0023,
      "step": 37160
    },
    {
      "epoch": 1.9824000000000002,
      "grad_norm": 0.33773812651634216,
      "learning_rate": 3.761e-05,
      "loss": 0.0023,
      "step": 37170
    },
    {
      "epoch": 1.9829333333333334,
      "grad_norm": 0.0685306042432785,
      "learning_rate": 3.760666666666667e-05,
      "loss": 0.0017,
      "step": 37180
    },
    {
      "epoch": 1.9834666666666667,
      "grad_norm": 0.18255679309368134,
      "learning_rate": 3.760333333333333e-05,
      "loss": 0.0029,
      "step": 37190
    },
    {
      "epoch": 1.984,
      "grad_norm": 0.09008742868900299,
      "learning_rate": 3.76e-05,
      "loss": 0.0034,
      "step": 37200
    },
    {
      "epoch": 1.9845333333333333,
      "grad_norm": 0.3011404275894165,
      "learning_rate": 3.759666666666667e-05,
      "loss": 0.003,
      "step": 37210
    },
    {
      "epoch": 1.9850666666666665,
      "grad_norm": 0.38966649770736694,
      "learning_rate": 3.759333333333334e-05,
      "loss": 0.0028,
      "step": 37220
    },
    {
      "epoch": 1.9856,
      "grad_norm": 0.37448829412460327,
      "learning_rate": 3.7590000000000004e-05,
      "loss": 0.0045,
      "step": 37230
    },
    {
      "epoch": 1.9861333333333333,
      "grad_norm": 0.04757050797343254,
      "learning_rate": 3.758666666666667e-05,
      "loss": 0.003,
      "step": 37240
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 0.21654634177684784,
      "learning_rate": 3.7583333333333337e-05,
      "loss": 0.0024,
      "step": 37250
    },
    {
      "epoch": 1.9872,
      "grad_norm": 0.2964285910129547,
      "learning_rate": 3.758e-05,
      "loss": 0.0024,
      "step": 37260
    },
    {
      "epoch": 1.9877333333333334,
      "grad_norm": 0.6597530841827393,
      "learning_rate": 3.757666666666667e-05,
      "loss": 0.0031,
      "step": 37270
    },
    {
      "epoch": 1.9882666666666666,
      "grad_norm": 0.14912943542003632,
      "learning_rate": 3.7573333333333335e-05,
      "loss": 0.0019,
      "step": 37280
    },
    {
      "epoch": 1.9888,
      "grad_norm": 0.30874353647232056,
      "learning_rate": 3.757e-05,
      "loss": 0.0029,
      "step": 37290
    },
    {
      "epoch": 1.9893333333333332,
      "grad_norm": 0.4157683849334717,
      "learning_rate": 3.756666666666667e-05,
      "loss": 0.0035,
      "step": 37300
    },
    {
      "epoch": 1.9898666666666667,
      "grad_norm": 0.24920040369033813,
      "learning_rate": 3.7563333333333333e-05,
      "loss": 0.0024,
      "step": 37310
    },
    {
      "epoch": 1.9904,
      "grad_norm": 0.2398475706577301,
      "learning_rate": 3.756e-05,
      "loss": 0.0028,
      "step": 37320
    },
    {
      "epoch": 1.9909333333333334,
      "grad_norm": 0.21351712942123413,
      "learning_rate": 3.7556666666666666e-05,
      "loss": 0.0038,
      "step": 37330
    },
    {
      "epoch": 1.9914666666666667,
      "grad_norm": 0.35461825132369995,
      "learning_rate": 3.755333333333333e-05,
      "loss": 0.0023,
      "step": 37340
    },
    {
      "epoch": 1.992,
      "grad_norm": 0.04314214736223221,
      "learning_rate": 3.7550000000000005e-05,
      "loss": 0.0016,
      "step": 37350
    },
    {
      "epoch": 1.9925333333333333,
      "grad_norm": 0.2726805508136749,
      "learning_rate": 3.754666666666667e-05,
      "loss": 0.002,
      "step": 37360
    },
    {
      "epoch": 1.9930666666666665,
      "grad_norm": 0.09891986846923828,
      "learning_rate": 3.754333333333334e-05,
      "loss": 0.0021,
      "step": 37370
    },
    {
      "epoch": 1.9936,
      "grad_norm": 0.6280538439750671,
      "learning_rate": 3.754e-05,
      "loss": 0.0024,
      "step": 37380
    },
    {
      "epoch": 1.9941333333333333,
      "grad_norm": 0.3052854537963867,
      "learning_rate": 3.753666666666667e-05,
      "loss": 0.0024,
      "step": 37390
    },
    {
      "epoch": 1.9946666666666668,
      "grad_norm": 0.626157283782959,
      "learning_rate": 3.7533333333333335e-05,
      "loss": 0.0023,
      "step": 37400
    },
    {
      "epoch": 1.9952,
      "grad_norm": 0.06664812564849854,
      "learning_rate": 3.753e-05,
      "loss": 0.0027,
      "step": 37410
    },
    {
      "epoch": 1.9957333333333334,
      "grad_norm": 0.12322784960269928,
      "learning_rate": 3.7526666666666674e-05,
      "loss": 0.0031,
      "step": 37420
    },
    {
      "epoch": 1.9962666666666666,
      "grad_norm": 0.09857548773288727,
      "learning_rate": 3.7523333333333334e-05,
      "loss": 0.0025,
      "step": 37430
    },
    {
      "epoch": 1.9968,
      "grad_norm": 0.29671546816825867,
      "learning_rate": 3.752e-05,
      "loss": 0.0026,
      "step": 37440
    },
    {
      "epoch": 1.9973333333333332,
      "grad_norm": 0.055519964545965195,
      "learning_rate": 3.7516666666666666e-05,
      "loss": 0.0028,
      "step": 37450
    },
    {
      "epoch": 1.9978666666666667,
      "grad_norm": 0.21452108025550842,
      "learning_rate": 3.751333333333333e-05,
      "loss": 0.0023,
      "step": 37460
    },
    {
      "epoch": 1.9984,
      "grad_norm": 0.14953117072582245,
      "learning_rate": 3.751e-05,
      "loss": 0.0028,
      "step": 37470
    },
    {
      "epoch": 1.9989333333333335,
      "grad_norm": 0.14923739433288574,
      "learning_rate": 3.7506666666666664e-05,
      "loss": 0.0023,
      "step": 37480
    },
    {
      "epoch": 1.9994666666666667,
      "grad_norm": 0.06889753043651581,
      "learning_rate": 3.750333333333334e-05,
      "loss": 0.0033,
      "step": 37490
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.3205043077468872,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.0022,
      "step": 37500
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.0026801396161317825,
      "eval_runtime": 168.5121,
      "eval_samples_per_second": 1483.573,
      "eval_steps_per_second": 37.089,
      "step": 37500
    },
    {
      "epoch": 2.0005333333333333,
      "grad_norm": 0.04757421091198921,
      "learning_rate": 3.749666666666667e-05,
      "loss": 0.0022,
      "step": 37510
    },
    {
      "epoch": 2.0010666666666665,
      "grad_norm": 0.11995992064476013,
      "learning_rate": 3.7493333333333336e-05,
      "loss": 0.003,
      "step": 37520
    },
    {
      "epoch": 2.0016,
      "grad_norm": 0.14937274158000946,
      "learning_rate": 3.749e-05,
      "loss": 0.003,
      "step": 37530
    },
    {
      "epoch": 2.0021333333333335,
      "grad_norm": 0.6563461422920227,
      "learning_rate": 3.748666666666667e-05,
      "loss": 0.0038,
      "step": 37540
    },
    {
      "epoch": 2.002666666666667,
      "grad_norm": 0.07136818766593933,
      "learning_rate": 3.7483333333333334e-05,
      "loss": 0.0035,
      "step": 37550
    },
    {
      "epoch": 2.0032,
      "grad_norm": 0.19419728219509125,
      "learning_rate": 3.748000000000001e-05,
      "loss": 0.0039,
      "step": 37560
    },
    {
      "epoch": 2.0037333333333334,
      "grad_norm": 0.12943364679813385,
      "learning_rate": 3.747666666666667e-05,
      "loss": 0.0028,
      "step": 37570
    },
    {
      "epoch": 2.0042666666666666,
      "grad_norm": 0.05002082511782646,
      "learning_rate": 3.747333333333333e-05,
      "loss": 0.0026,
      "step": 37580
    },
    {
      "epoch": 2.0048,
      "grad_norm": 0.6272788047790527,
      "learning_rate": 3.747e-05,
      "loss": 0.0037,
      "step": 37590
    },
    {
      "epoch": 2.005333333333333,
      "grad_norm": 0.20996850728988647,
      "learning_rate": 3.7466666666666665e-05,
      "loss": 0.0019,
      "step": 37600
    },
    {
      "epoch": 2.0058666666666665,
      "grad_norm": 0.15439869463443756,
      "learning_rate": 3.746333333333333e-05,
      "loss": 0.003,
      "step": 37610
    },
    {
      "epoch": 2.0064,
      "grad_norm": 0.39291322231292725,
      "learning_rate": 3.7460000000000004e-05,
      "loss": 0.0024,
      "step": 37620
    },
    {
      "epoch": 2.0069333333333335,
      "grad_norm": 0.12027930468320847,
      "learning_rate": 3.745666666666667e-05,
      "loss": 0.0024,
      "step": 37630
    },
    {
      "epoch": 2.0074666666666667,
      "grad_norm": 0.29759514331817627,
      "learning_rate": 3.7453333333333336e-05,
      "loss": 0.0024,
      "step": 37640
    },
    {
      "epoch": 2.008,
      "grad_norm": 0.26861312985420227,
      "learning_rate": 3.745e-05,
      "loss": 0.0021,
      "step": 37650
    },
    {
      "epoch": 2.0085333333333333,
      "grad_norm": 0.04263972491025925,
      "learning_rate": 3.744666666666667e-05,
      "loss": 0.002,
      "step": 37660
    },
    {
      "epoch": 2.0090666666666666,
      "grad_norm": 0.4196048378944397,
      "learning_rate": 3.7443333333333335e-05,
      "loss": 0.0019,
      "step": 37670
    },
    {
      "epoch": 2.0096,
      "grad_norm": 0.5943641066551208,
      "learning_rate": 3.744e-05,
      "loss": 0.0036,
      "step": 37680
    },
    {
      "epoch": 2.0101333333333335,
      "grad_norm": 0.17489828169345856,
      "learning_rate": 3.743666666666667e-05,
      "loss": 0.0025,
      "step": 37690
    },
    {
      "epoch": 2.010666666666667,
      "grad_norm": 0.07575032114982605,
      "learning_rate": 3.743333333333334e-05,
      "loss": 0.0029,
      "step": 37700
    },
    {
      "epoch": 2.0112,
      "grad_norm": 0.20906512439250946,
      "learning_rate": 3.7430000000000006e-05,
      "loss": 0.0032,
      "step": 37710
    },
    {
      "epoch": 2.0117333333333334,
      "grad_norm": 0.29772862792015076,
      "learning_rate": 3.742666666666667e-05,
      "loss": 0.0022,
      "step": 37720
    },
    {
      "epoch": 2.0122666666666666,
      "grad_norm": 0.0999530553817749,
      "learning_rate": 3.742333333333333e-05,
      "loss": 0.0036,
      "step": 37730
    },
    {
      "epoch": 2.0128,
      "grad_norm": 0.1937609612941742,
      "learning_rate": 3.742e-05,
      "loss": 0.0022,
      "step": 37740
    },
    {
      "epoch": 2.013333333333333,
      "grad_norm": 0.2682307958602905,
      "learning_rate": 3.7416666666666664e-05,
      "loss": 0.0025,
      "step": 37750
    },
    {
      "epoch": 2.0138666666666665,
      "grad_norm": 0.2110840082168579,
      "learning_rate": 3.7413333333333337e-05,
      "loss": 0.0025,
      "step": 37760
    },
    {
      "epoch": 2.0144,
      "grad_norm": 0.4468136429786682,
      "learning_rate": 3.741e-05,
      "loss": 0.0021,
      "step": 37770
    },
    {
      "epoch": 2.0149333333333335,
      "grad_norm": 0.23921573162078857,
      "learning_rate": 3.740666666666667e-05,
      "loss": 0.0028,
      "step": 37780
    },
    {
      "epoch": 2.0154666666666667,
      "grad_norm": 0.21750681102275848,
      "learning_rate": 3.7403333333333335e-05,
      "loss": 0.0026,
      "step": 37790
    },
    {
      "epoch": 2.016,
      "grad_norm": 0.1772288978099823,
      "learning_rate": 3.74e-05,
      "loss": 0.0023,
      "step": 37800
    },
    {
      "epoch": 2.0165333333333333,
      "grad_norm": 0.11387764662504196,
      "learning_rate": 3.739666666666667e-05,
      "loss": 0.0028,
      "step": 37810
    },
    {
      "epoch": 2.0170666666666666,
      "grad_norm": 0.3576135039329529,
      "learning_rate": 3.739333333333333e-05,
      "loss": 0.0028,
      "step": 37820
    },
    {
      "epoch": 2.0176,
      "grad_norm": 0.03065885603427887,
      "learning_rate": 3.739e-05,
      "loss": 0.0032,
      "step": 37830
    },
    {
      "epoch": 2.018133333333333,
      "grad_norm": 0.35618099570274353,
      "learning_rate": 3.738666666666667e-05,
      "loss": 0.0018,
      "step": 37840
    },
    {
      "epoch": 2.018666666666667,
      "grad_norm": 0.6011500954627991,
      "learning_rate": 3.738333333333334e-05,
      "loss": 0.0041,
      "step": 37850
    },
    {
      "epoch": 2.0192,
      "grad_norm": 0.06730809062719345,
      "learning_rate": 3.7380000000000005e-05,
      "loss": 0.0044,
      "step": 37860
    },
    {
      "epoch": 2.0197333333333334,
      "grad_norm": 0.6312400698661804,
      "learning_rate": 3.737666666666667e-05,
      "loss": 0.0024,
      "step": 37870
    },
    {
      "epoch": 2.0202666666666667,
      "grad_norm": 0.3651162087917328,
      "learning_rate": 3.737333333333333e-05,
      "loss": 0.0034,
      "step": 37880
    },
    {
      "epoch": 2.0208,
      "grad_norm": 0.09493405371904373,
      "learning_rate": 3.7369999999999996e-05,
      "loss": 0.0028,
      "step": 37890
    },
    {
      "epoch": 2.021333333333333,
      "grad_norm": 0.12422632426023483,
      "learning_rate": 3.736666666666667e-05,
      "loss": 0.0018,
      "step": 37900
    },
    {
      "epoch": 2.0218666666666665,
      "grad_norm": 0.35630708932876587,
      "learning_rate": 3.7363333333333335e-05,
      "loss": 0.0034,
      "step": 37910
    },
    {
      "epoch": 2.0224,
      "grad_norm": 0.12072392553091049,
      "learning_rate": 3.736e-05,
      "loss": 0.0025,
      "step": 37920
    },
    {
      "epoch": 2.0229333333333335,
      "grad_norm": 0.2157091498374939,
      "learning_rate": 3.735666666666667e-05,
      "loss": 0.0031,
      "step": 37930
    },
    {
      "epoch": 2.0234666666666667,
      "grad_norm": 0.15320001542568207,
      "learning_rate": 3.7353333333333334e-05,
      "loss": 0.0026,
      "step": 37940
    },
    {
      "epoch": 2.024,
      "grad_norm": 0.12058453261852264,
      "learning_rate": 3.735e-05,
      "loss": 0.0026,
      "step": 37950
    },
    {
      "epoch": 2.0245333333333333,
      "grad_norm": 0.10176865756511688,
      "learning_rate": 3.7346666666666666e-05,
      "loss": 0.0026,
      "step": 37960
    },
    {
      "epoch": 2.0250666666666666,
      "grad_norm": 0.1071263924241066,
      "learning_rate": 3.734333333333334e-05,
      "loss": 0.0019,
      "step": 37970
    },
    {
      "epoch": 2.0256,
      "grad_norm": 0.7810816168785095,
      "learning_rate": 3.7340000000000005e-05,
      "loss": 0.0026,
      "step": 37980
    },
    {
      "epoch": 2.026133333333333,
      "grad_norm": 0.15368179976940155,
      "learning_rate": 3.733666666666667e-05,
      "loss": 0.003,
      "step": 37990
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 0.11919759213924408,
      "learning_rate": 3.733333333333334e-05,
      "loss": 0.004,
      "step": 38000
    },
    {
      "epoch": 2.0272,
      "grad_norm": 0.09396886080503464,
      "learning_rate": 3.7330000000000003e-05,
      "loss": 0.0029,
      "step": 38010
    },
    {
      "epoch": 2.0277333333333334,
      "grad_norm": 0.3563717305660248,
      "learning_rate": 3.732666666666667e-05,
      "loss": 0.0041,
      "step": 38020
    },
    {
      "epoch": 2.0282666666666667,
      "grad_norm": 0.4067905843257904,
      "learning_rate": 3.7323333333333336e-05,
      "loss": 0.003,
      "step": 38030
    },
    {
      "epoch": 2.0288,
      "grad_norm": 0.6801608204841614,
      "learning_rate": 3.732e-05,
      "loss": 0.0026,
      "step": 38040
    },
    {
      "epoch": 2.029333333333333,
      "grad_norm": 0.2458314597606659,
      "learning_rate": 3.731666666666667e-05,
      "loss": 0.0025,
      "step": 38050
    },
    {
      "epoch": 2.0298666666666665,
      "grad_norm": 0.06357014924287796,
      "learning_rate": 3.7313333333333334e-05,
      "loss": 0.0022,
      "step": 38060
    },
    {
      "epoch": 2.0304,
      "grad_norm": 0.18363459408283234,
      "learning_rate": 3.731e-05,
      "loss": 0.0022,
      "step": 38070
    },
    {
      "epoch": 2.0309333333333335,
      "grad_norm": 0.24197666347026825,
      "learning_rate": 3.7306666666666666e-05,
      "loss": 0.0032,
      "step": 38080
    },
    {
      "epoch": 2.0314666666666668,
      "grad_norm": 0.44791296124458313,
      "learning_rate": 3.730333333333333e-05,
      "loss": 0.0027,
      "step": 38090
    },
    {
      "epoch": 2.032,
      "grad_norm": 0.15237395465373993,
      "learning_rate": 3.73e-05,
      "loss": 0.0019,
      "step": 38100
    },
    {
      "epoch": 2.0325333333333333,
      "grad_norm": 0.41308847069740295,
      "learning_rate": 3.729666666666667e-05,
      "loss": 0.0038,
      "step": 38110
    },
    {
      "epoch": 2.0330666666666666,
      "grad_norm": 0.2976927161216736,
      "learning_rate": 3.729333333333334e-05,
      "loss": 0.0031,
      "step": 38120
    },
    {
      "epoch": 2.0336,
      "grad_norm": 0.3321523666381836,
      "learning_rate": 3.7290000000000004e-05,
      "loss": 0.0031,
      "step": 38130
    },
    {
      "epoch": 2.034133333333333,
      "grad_norm": 0.47638577222824097,
      "learning_rate": 3.728666666666667e-05,
      "loss": 0.002,
      "step": 38140
    },
    {
      "epoch": 2.034666666666667,
      "grad_norm": 0.4501742422580719,
      "learning_rate": 3.7283333333333336e-05,
      "loss": 0.0032,
      "step": 38150
    },
    {
      "epoch": 2.0352,
      "grad_norm": 0.30010390281677246,
      "learning_rate": 3.728e-05,
      "loss": 0.0045,
      "step": 38160
    },
    {
      "epoch": 2.0357333333333334,
      "grad_norm": 0.17460572719573975,
      "learning_rate": 3.727666666666667e-05,
      "loss": 0.0027,
      "step": 38170
    },
    {
      "epoch": 2.0362666666666667,
      "grad_norm": 0.03712334856390953,
      "learning_rate": 3.727333333333334e-05,
      "loss": 0.0019,
      "step": 38180
    },
    {
      "epoch": 2.0368,
      "grad_norm": 0.11999213695526123,
      "learning_rate": 3.727e-05,
      "loss": 0.0027,
      "step": 38190
    },
    {
      "epoch": 2.037333333333333,
      "grad_norm": 0.4764857888221741,
      "learning_rate": 3.726666666666667e-05,
      "loss": 0.0027,
      "step": 38200
    },
    {
      "epoch": 2.0378666666666665,
      "grad_norm": 0.8620314598083496,
      "learning_rate": 3.726333333333333e-05,
      "loss": 0.0037,
      "step": 38210
    },
    {
      "epoch": 2.0384,
      "grad_norm": 0.1491696834564209,
      "learning_rate": 3.726e-05,
      "loss": 0.0025,
      "step": 38220
    },
    {
      "epoch": 2.0389333333333335,
      "grad_norm": 0.044863779097795486,
      "learning_rate": 3.7256666666666665e-05,
      "loss": 0.0028,
      "step": 38230
    },
    {
      "epoch": 2.0394666666666668,
      "grad_norm": 0.2377081960439682,
      "learning_rate": 3.725333333333333e-05,
      "loss": 0.002,
      "step": 38240
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.20907631516456604,
      "learning_rate": 3.7250000000000004e-05,
      "loss": 0.0028,
      "step": 38250
    },
    {
      "epoch": 2.0405333333333333,
      "grad_norm": 0.12500552833080292,
      "learning_rate": 3.724666666666667e-05,
      "loss": 0.0024,
      "step": 38260
    },
    {
      "epoch": 2.0410666666666666,
      "grad_norm": 0.09026958048343658,
      "learning_rate": 3.7243333333333336e-05,
      "loss": 0.002,
      "step": 38270
    },
    {
      "epoch": 2.0416,
      "grad_norm": 0.03928086534142494,
      "learning_rate": 3.724e-05,
      "loss": 0.0021,
      "step": 38280
    },
    {
      "epoch": 2.042133333333333,
      "grad_norm": 0.7230268120765686,
      "learning_rate": 3.723666666666667e-05,
      "loss": 0.0029,
      "step": 38290
    },
    {
      "epoch": 2.042666666666667,
      "grad_norm": 0.30320656299591064,
      "learning_rate": 3.7233333333333335e-05,
      "loss": 0.0027,
      "step": 38300
    },
    {
      "epoch": 2.0432,
      "grad_norm": 0.04464954137802124,
      "learning_rate": 3.723e-05,
      "loss": 0.0031,
      "step": 38310
    },
    {
      "epoch": 2.0437333333333334,
      "grad_norm": 0.33514025807380676,
      "learning_rate": 3.7226666666666674e-05,
      "loss": 0.0024,
      "step": 38320
    },
    {
      "epoch": 2.0442666666666667,
      "grad_norm": 0.5369725823402405,
      "learning_rate": 3.722333333333334e-05,
      "loss": 0.0025,
      "step": 38330
    },
    {
      "epoch": 2.0448,
      "grad_norm": 0.06279025971889496,
      "learning_rate": 3.722e-05,
      "loss": 0.002,
      "step": 38340
    },
    {
      "epoch": 2.0453333333333332,
      "grad_norm": 0.4543887972831726,
      "learning_rate": 3.7216666666666666e-05,
      "loss": 0.0038,
      "step": 38350
    },
    {
      "epoch": 2.0458666666666665,
      "grad_norm": 0.2403220385313034,
      "learning_rate": 3.721333333333333e-05,
      "loss": 0.0025,
      "step": 38360
    },
    {
      "epoch": 2.0464,
      "grad_norm": 0.7475975155830383,
      "learning_rate": 3.721e-05,
      "loss": 0.0015,
      "step": 38370
    },
    {
      "epoch": 2.0469333333333335,
      "grad_norm": 0.47914546728134155,
      "learning_rate": 3.720666666666667e-05,
      "loss": 0.0029,
      "step": 38380
    },
    {
      "epoch": 2.0474666666666668,
      "grad_norm": 0.21154004335403442,
      "learning_rate": 3.720333333333334e-05,
      "loss": 0.0023,
      "step": 38390
    },
    {
      "epoch": 2.048,
      "grad_norm": 0.1953570395708084,
      "learning_rate": 3.72e-05,
      "loss": 0.0022,
      "step": 38400
    },
    {
      "epoch": 2.0485333333333333,
      "grad_norm": 0.541344404220581,
      "learning_rate": 3.719666666666667e-05,
      "loss": 0.0022,
      "step": 38410
    },
    {
      "epoch": 2.0490666666666666,
      "grad_norm": 0.06491631269454956,
      "learning_rate": 3.7193333333333335e-05,
      "loss": 0.0028,
      "step": 38420
    },
    {
      "epoch": 2.0496,
      "grad_norm": 0.04487454518675804,
      "learning_rate": 3.719e-05,
      "loss": 0.0025,
      "step": 38430
    },
    {
      "epoch": 2.050133333333333,
      "grad_norm": 0.27028888463974,
      "learning_rate": 3.718666666666667e-05,
      "loss": 0.0047,
      "step": 38440
    },
    {
      "epoch": 2.050666666666667,
      "grad_norm": 0.559813916683197,
      "learning_rate": 3.7183333333333334e-05,
      "loss": 0.0041,
      "step": 38450
    },
    {
      "epoch": 2.0512,
      "grad_norm": 0.4744553565979004,
      "learning_rate": 3.7180000000000007e-05,
      "loss": 0.0024,
      "step": 38460
    },
    {
      "epoch": 2.0517333333333334,
      "grad_norm": 0.20838896930217743,
      "learning_rate": 3.717666666666667e-05,
      "loss": 0.003,
      "step": 38470
    },
    {
      "epoch": 2.0522666666666667,
      "grad_norm": 0.1538008600473404,
      "learning_rate": 3.717333333333334e-05,
      "loss": 0.0024,
      "step": 38480
    },
    {
      "epoch": 2.0528,
      "grad_norm": 0.41635116934776306,
      "learning_rate": 3.717e-05,
      "loss": 0.0019,
      "step": 38490
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 0.352009654045105,
      "learning_rate": 3.7166666666666664e-05,
      "loss": 0.003,
      "step": 38500
    },
    {
      "epoch": 2.0538666666666665,
      "grad_norm": 0.3576417565345764,
      "learning_rate": 3.716333333333333e-05,
      "loss": 0.0025,
      "step": 38510
    },
    {
      "epoch": 2.0544,
      "grad_norm": 0.3580814301967621,
      "learning_rate": 3.716e-05,
      "loss": 0.0022,
      "step": 38520
    },
    {
      "epoch": 2.0549333333333335,
      "grad_norm": 0.1350453794002533,
      "learning_rate": 3.715666666666667e-05,
      "loss": 0.0029,
      "step": 38530
    },
    {
      "epoch": 2.0554666666666668,
      "grad_norm": 0.18670791387557983,
      "learning_rate": 3.7153333333333336e-05,
      "loss": 0.0026,
      "step": 38540
    },
    {
      "epoch": 2.056,
      "grad_norm": 0.2951796352863312,
      "learning_rate": 3.715e-05,
      "loss": 0.002,
      "step": 38550
    },
    {
      "epoch": 2.0565333333333333,
      "grad_norm": 0.3289995491504669,
      "learning_rate": 3.714666666666667e-05,
      "loss": 0.0044,
      "step": 38560
    },
    {
      "epoch": 2.0570666666666666,
      "grad_norm": 0.15231630206108093,
      "learning_rate": 3.7143333333333334e-05,
      "loss": 0.0029,
      "step": 38570
    },
    {
      "epoch": 2.0576,
      "grad_norm": 0.07422897964715958,
      "learning_rate": 3.714e-05,
      "loss": 0.0026,
      "step": 38580
    },
    {
      "epoch": 2.058133333333333,
      "grad_norm": 0.07516021281480789,
      "learning_rate": 3.7136666666666666e-05,
      "loss": 0.0029,
      "step": 38590
    },
    {
      "epoch": 2.058666666666667,
      "grad_norm": 0.24441330134868622,
      "learning_rate": 3.713333333333334e-05,
      "loss": 0.0023,
      "step": 38600
    },
    {
      "epoch": 2.0592,
      "grad_norm": 0.7147306203842163,
      "learning_rate": 3.7130000000000005e-05,
      "loss": 0.0034,
      "step": 38610
    },
    {
      "epoch": 2.0597333333333334,
      "grad_norm": 0.12575088441371918,
      "learning_rate": 3.712666666666667e-05,
      "loss": 0.0026,
      "step": 38620
    },
    {
      "epoch": 2.0602666666666667,
      "grad_norm": 0.15269039571285248,
      "learning_rate": 3.712333333333334e-05,
      "loss": 0.0017,
      "step": 38630
    },
    {
      "epoch": 2.0608,
      "grad_norm": 0.47506609559059143,
      "learning_rate": 3.712e-05,
      "loss": 0.0023,
      "step": 38640
    },
    {
      "epoch": 2.0613333333333332,
      "grad_norm": 0.24385614693164825,
      "learning_rate": 3.711666666666666e-05,
      "loss": 0.0027,
      "step": 38650
    },
    {
      "epoch": 2.0618666666666665,
      "grad_norm": 0.38489317893981934,
      "learning_rate": 3.7113333333333336e-05,
      "loss": 0.003,
      "step": 38660
    },
    {
      "epoch": 2.0624,
      "grad_norm": 0.2103254497051239,
      "learning_rate": 3.711e-05,
      "loss": 0.0022,
      "step": 38670
    },
    {
      "epoch": 2.0629333333333335,
      "grad_norm": 0.04902864620089531,
      "learning_rate": 3.710666666666667e-05,
      "loss": 0.0034,
      "step": 38680
    },
    {
      "epoch": 2.063466666666667,
      "grad_norm": 0.1871860921382904,
      "learning_rate": 3.7103333333333334e-05,
      "loss": 0.0025,
      "step": 38690
    },
    {
      "epoch": 2.064,
      "grad_norm": 0.2964799702167511,
      "learning_rate": 3.71e-05,
      "loss": 0.0033,
      "step": 38700
    },
    {
      "epoch": 2.0645333333333333,
      "grad_norm": 0.5638788938522339,
      "learning_rate": 3.709666666666667e-05,
      "loss": 0.0029,
      "step": 38710
    },
    {
      "epoch": 2.0650666666666666,
      "grad_norm": 0.7136682868003845,
      "learning_rate": 3.709333333333333e-05,
      "loss": 0.0041,
      "step": 38720
    },
    {
      "epoch": 2.0656,
      "grad_norm": 0.20925159752368927,
      "learning_rate": 3.7090000000000006e-05,
      "loss": 0.0043,
      "step": 38730
    },
    {
      "epoch": 2.066133333333333,
      "grad_norm": 0.563430905342102,
      "learning_rate": 3.708666666666667e-05,
      "loss": 0.0018,
      "step": 38740
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 0.17785179615020752,
      "learning_rate": 3.708333333333334e-05,
      "loss": 0.0024,
      "step": 38750
    },
    {
      "epoch": 2.0672,
      "grad_norm": 0.14899305999279022,
      "learning_rate": 3.7080000000000004e-05,
      "loss": 0.0028,
      "step": 38760
    },
    {
      "epoch": 2.0677333333333334,
      "grad_norm": 0.22443519532680511,
      "learning_rate": 3.707666666666667e-05,
      "loss": 0.0042,
      "step": 38770
    },
    {
      "epoch": 2.0682666666666667,
      "grad_norm": 0.17834924161434174,
      "learning_rate": 3.7073333333333336e-05,
      "loss": 0.0024,
      "step": 38780
    },
    {
      "epoch": 2.0688,
      "grad_norm": 0.24051032960414886,
      "learning_rate": 3.707e-05,
      "loss": 0.0021,
      "step": 38790
    },
    {
      "epoch": 2.0693333333333332,
      "grad_norm": 0.23307056725025177,
      "learning_rate": 3.706666666666667e-05,
      "loss": 0.003,
      "step": 38800
    },
    {
      "epoch": 2.0698666666666665,
      "grad_norm": 0.15143591165542603,
      "learning_rate": 3.7063333333333335e-05,
      "loss": 0.0029,
      "step": 38810
    },
    {
      "epoch": 2.0704,
      "grad_norm": 0.07554716616868973,
      "learning_rate": 3.706e-05,
      "loss": 0.0026,
      "step": 38820
    },
    {
      "epoch": 2.0709333333333335,
      "grad_norm": 0.12648680806159973,
      "learning_rate": 3.705666666666667e-05,
      "loss": 0.0035,
      "step": 38830
    },
    {
      "epoch": 2.071466666666667,
      "grad_norm": 0.3580932915210724,
      "learning_rate": 3.705333333333333e-05,
      "loss": 0.0021,
      "step": 38840
    },
    {
      "epoch": 2.072,
      "grad_norm": 0.5950134992599487,
      "learning_rate": 3.705e-05,
      "loss": 0.0042,
      "step": 38850
    },
    {
      "epoch": 2.0725333333333333,
      "grad_norm": 0.47422629594802856,
      "learning_rate": 3.7046666666666665e-05,
      "loss": 0.0037,
      "step": 38860
    },
    {
      "epoch": 2.0730666666666666,
      "grad_norm": 0.18792130053043365,
      "learning_rate": 3.704333333333334e-05,
      "loss": 0.0021,
      "step": 38870
    },
    {
      "epoch": 2.0736,
      "grad_norm": 0.12634412944316864,
      "learning_rate": 3.7040000000000005e-05,
      "loss": 0.0016,
      "step": 38880
    },
    {
      "epoch": 2.074133333333333,
      "grad_norm": 0.0654192566871643,
      "learning_rate": 3.703666666666667e-05,
      "loss": 0.0031,
      "step": 38890
    },
    {
      "epoch": 2.074666666666667,
      "grad_norm": 0.1198178306221962,
      "learning_rate": 3.703333333333334e-05,
      "loss": 0.0034,
      "step": 38900
    },
    {
      "epoch": 2.0752,
      "grad_norm": 0.06417834758758545,
      "learning_rate": 3.703e-05,
      "loss": 0.0027,
      "step": 38910
    },
    {
      "epoch": 2.0757333333333334,
      "grad_norm": 0.1792069971561432,
      "learning_rate": 3.702666666666667e-05,
      "loss": 0.0029,
      "step": 38920
    },
    {
      "epoch": 2.0762666666666667,
      "grad_norm": 0.06782296299934387,
      "learning_rate": 3.7023333333333335e-05,
      "loss": 0.0014,
      "step": 38930
    },
    {
      "epoch": 2.0768,
      "grad_norm": 0.03552122414112091,
      "learning_rate": 3.702e-05,
      "loss": 0.0041,
      "step": 38940
    },
    {
      "epoch": 2.0773333333333333,
      "grad_norm": 0.209132120013237,
      "learning_rate": 3.701666666666667e-05,
      "loss": 0.0022,
      "step": 38950
    },
    {
      "epoch": 2.0778666666666665,
      "grad_norm": 0.09292884916067123,
      "learning_rate": 3.7013333333333334e-05,
      "loss": 0.0018,
      "step": 38960
    },
    {
      "epoch": 2.0784,
      "grad_norm": 0.3726769983768463,
      "learning_rate": 3.701e-05,
      "loss": 0.0024,
      "step": 38970
    },
    {
      "epoch": 2.0789333333333335,
      "grad_norm": 0.09584955126047134,
      "learning_rate": 3.7006666666666666e-05,
      "loss": 0.0023,
      "step": 38980
    },
    {
      "epoch": 2.079466666666667,
      "grad_norm": 0.1803802251815796,
      "learning_rate": 3.700333333333333e-05,
      "loss": 0.0028,
      "step": 38990
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.03632814809679985,
      "learning_rate": 3.7e-05,
      "loss": 0.0029,
      "step": 39000
    },
    {
      "epoch": 2.0805333333333333,
      "grad_norm": 0.3584998846054077,
      "learning_rate": 3.699666666666667e-05,
      "loss": 0.0018,
      "step": 39010
    },
    {
      "epoch": 2.0810666666666666,
      "grad_norm": 0.17086219787597656,
      "learning_rate": 3.699333333333334e-05,
      "loss": 0.0023,
      "step": 39020
    },
    {
      "epoch": 2.0816,
      "grad_norm": 0.3574058413505554,
      "learning_rate": 3.699e-05,
      "loss": 0.0028,
      "step": 39030
    },
    {
      "epoch": 2.082133333333333,
      "grad_norm": 0.11246822774410248,
      "learning_rate": 3.698666666666667e-05,
      "loss": 0.0024,
      "step": 39040
    },
    {
      "epoch": 2.0826666666666664,
      "grad_norm": 0.1541913002729416,
      "learning_rate": 3.6983333333333336e-05,
      "loss": 0.0036,
      "step": 39050
    },
    {
      "epoch": 2.0832,
      "grad_norm": 0.5096777677536011,
      "learning_rate": 3.698e-05,
      "loss": 0.0018,
      "step": 39060
    },
    {
      "epoch": 2.0837333333333334,
      "grad_norm": 0.2707495391368866,
      "learning_rate": 3.697666666666667e-05,
      "loss": 0.0027,
      "step": 39070
    },
    {
      "epoch": 2.0842666666666667,
      "grad_norm": 0.0962953120470047,
      "learning_rate": 3.697333333333334e-05,
      "loss": 0.0023,
      "step": 39080
    },
    {
      "epoch": 2.0848,
      "grad_norm": 0.09103314578533173,
      "learning_rate": 3.697e-05,
      "loss": 0.0022,
      "step": 39090
    },
    {
      "epoch": 2.0853333333333333,
      "grad_norm": 0.3284280300140381,
      "learning_rate": 3.6966666666666666e-05,
      "loss": 0.0036,
      "step": 39100
    },
    {
      "epoch": 2.0858666666666665,
      "grad_norm": 0.1818159520626068,
      "learning_rate": 3.696333333333333e-05,
      "loss": 0.0028,
      "step": 39110
    },
    {
      "epoch": 2.0864,
      "grad_norm": 0.024723539128899574,
      "learning_rate": 3.696e-05,
      "loss": 0.002,
      "step": 39120
    },
    {
      "epoch": 2.0869333333333335,
      "grad_norm": 0.12827202677726746,
      "learning_rate": 3.6956666666666665e-05,
      "loss": 0.0022,
      "step": 39130
    },
    {
      "epoch": 2.087466666666667,
      "grad_norm": 0.12156512588262558,
      "learning_rate": 3.695333333333334e-05,
      "loss": 0.0018,
      "step": 39140
    },
    {
      "epoch": 2.088,
      "grad_norm": 0.08280482143163681,
      "learning_rate": 3.6950000000000004e-05,
      "loss": 0.0029,
      "step": 39150
    },
    {
      "epoch": 2.0885333333333334,
      "grad_norm": 0.04556947201490402,
      "learning_rate": 3.694666666666667e-05,
      "loss": 0.0019,
      "step": 39160
    },
    {
      "epoch": 2.0890666666666666,
      "grad_norm": 0.309047669172287,
      "learning_rate": 3.6943333333333336e-05,
      "loss": 0.0038,
      "step": 39170
    },
    {
      "epoch": 2.0896,
      "grad_norm": 0.44698336720466614,
      "learning_rate": 3.694e-05,
      "loss": 0.003,
      "step": 39180
    },
    {
      "epoch": 2.090133333333333,
      "grad_norm": 0.24009861052036285,
      "learning_rate": 3.693666666666667e-05,
      "loss": 0.0029,
      "step": 39190
    },
    {
      "epoch": 2.0906666666666665,
      "grad_norm": 0.17890587449073792,
      "learning_rate": 3.6933333333333334e-05,
      "loss": 0.0043,
      "step": 39200
    },
    {
      "epoch": 2.0912,
      "grad_norm": 0.2976250946521759,
      "learning_rate": 3.693e-05,
      "loss": 0.0025,
      "step": 39210
    },
    {
      "epoch": 2.0917333333333334,
      "grad_norm": 0.07336001098155975,
      "learning_rate": 3.6926666666666673e-05,
      "loss": 0.0032,
      "step": 39220
    },
    {
      "epoch": 2.0922666666666667,
      "grad_norm": 0.23970060050487518,
      "learning_rate": 3.692333333333334e-05,
      "loss": 0.0033,
      "step": 39230
    },
    {
      "epoch": 2.0928,
      "grad_norm": 0.06855406612157822,
      "learning_rate": 3.692e-05,
      "loss": 0.0032,
      "step": 39240
    },
    {
      "epoch": 2.0933333333333333,
      "grad_norm": 0.03715754300355911,
      "learning_rate": 3.6916666666666665e-05,
      "loss": 0.0027,
      "step": 39250
    },
    {
      "epoch": 2.0938666666666665,
      "grad_norm": 0.27138492465019226,
      "learning_rate": 3.691333333333333e-05,
      "loss": 0.0019,
      "step": 39260
    },
    {
      "epoch": 2.0944,
      "grad_norm": 0.36105385422706604,
      "learning_rate": 3.691e-05,
      "loss": 0.0022,
      "step": 39270
    },
    {
      "epoch": 2.0949333333333335,
      "grad_norm": 0.4019589126110077,
      "learning_rate": 3.690666666666667e-05,
      "loss": 0.0022,
      "step": 39280
    },
    {
      "epoch": 2.095466666666667,
      "grad_norm": 0.1691516488790512,
      "learning_rate": 3.6903333333333336e-05,
      "loss": 0.0022,
      "step": 39290
    },
    {
      "epoch": 2.096,
      "grad_norm": 0.38480067253112793,
      "learning_rate": 3.69e-05,
      "loss": 0.0022,
      "step": 39300
    },
    {
      "epoch": 2.0965333333333334,
      "grad_norm": 0.1795443594455719,
      "learning_rate": 3.689666666666667e-05,
      "loss": 0.002,
      "step": 39310
    },
    {
      "epoch": 2.0970666666666666,
      "grad_norm": 0.2092500478029251,
      "learning_rate": 3.6893333333333335e-05,
      "loss": 0.0023,
      "step": 39320
    },
    {
      "epoch": 2.0976,
      "grad_norm": 0.3085765838623047,
      "learning_rate": 3.689e-05,
      "loss": 0.0023,
      "step": 39330
    },
    {
      "epoch": 2.098133333333333,
      "grad_norm": 0.44594839215278625,
      "learning_rate": 3.688666666666667e-05,
      "loss": 0.0031,
      "step": 39340
    },
    {
      "epoch": 2.0986666666666665,
      "grad_norm": 0.3006162941455841,
      "learning_rate": 3.688333333333333e-05,
      "loss": 0.0032,
      "step": 39350
    },
    {
      "epoch": 2.0992,
      "grad_norm": 0.1878962367773056,
      "learning_rate": 3.6880000000000006e-05,
      "loss": 0.0027,
      "step": 39360
    },
    {
      "epoch": 2.0997333333333335,
      "grad_norm": 0.09309926629066467,
      "learning_rate": 3.687666666666667e-05,
      "loss": 0.002,
      "step": 39370
    },
    {
      "epoch": 2.1002666666666667,
      "grad_norm": 0.1749168336391449,
      "learning_rate": 3.687333333333334e-05,
      "loss": 0.0025,
      "step": 39380
    },
    {
      "epoch": 2.1008,
      "grad_norm": 0.3834753632545471,
      "learning_rate": 3.6870000000000004e-05,
      "loss": 0.002,
      "step": 39390
    },
    {
      "epoch": 2.1013333333333333,
      "grad_norm": 0.12050983309745789,
      "learning_rate": 3.6866666666666664e-05,
      "loss": 0.0027,
      "step": 39400
    },
    {
      "epoch": 2.1018666666666665,
      "grad_norm": 0.1204725131392479,
      "learning_rate": 3.686333333333333e-05,
      "loss": 0.0028,
      "step": 39410
    },
    {
      "epoch": 2.1024,
      "grad_norm": 0.12365356087684631,
      "learning_rate": 3.686e-05,
      "loss": 0.0048,
      "step": 39420
    },
    {
      "epoch": 2.1029333333333335,
      "grad_norm": 0.4739210903644562,
      "learning_rate": 3.685666666666667e-05,
      "loss": 0.0028,
      "step": 39430
    },
    {
      "epoch": 2.103466666666667,
      "grad_norm": 0.03612392023205757,
      "learning_rate": 3.6853333333333335e-05,
      "loss": 0.0026,
      "step": 39440
    },
    {
      "epoch": 2.104,
      "grad_norm": 0.23990626633167267,
      "learning_rate": 3.685e-05,
      "loss": 0.0026,
      "step": 39450
    },
    {
      "epoch": 2.1045333333333334,
      "grad_norm": 0.18245932459831238,
      "learning_rate": 3.684666666666667e-05,
      "loss": 0.0024,
      "step": 39460
    },
    {
      "epoch": 2.1050666666666666,
      "grad_norm": 0.5653708577156067,
      "learning_rate": 3.6843333333333334e-05,
      "loss": 0.0026,
      "step": 39470
    },
    {
      "epoch": 2.1056,
      "grad_norm": 0.12345369160175323,
      "learning_rate": 3.684e-05,
      "loss": 0.0021,
      "step": 39480
    },
    {
      "epoch": 2.106133333333333,
      "grad_norm": 0.17937259376049042,
      "learning_rate": 3.683666666666667e-05,
      "loss": 0.0029,
      "step": 39490
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 0.270112007856369,
      "learning_rate": 3.683333333333334e-05,
      "loss": 0.0026,
      "step": 39500
    },
    {
      "epoch": 2.1072,
      "grad_norm": 0.1188177764415741,
      "learning_rate": 3.6830000000000005e-05,
      "loss": 0.0016,
      "step": 39510
    },
    {
      "epoch": 2.1077333333333335,
      "grad_norm": 0.3282270133495331,
      "learning_rate": 3.682666666666667e-05,
      "loss": 0.0024,
      "step": 39520
    },
    {
      "epoch": 2.1082666666666667,
      "grad_norm": 0.09202731400728226,
      "learning_rate": 3.682333333333334e-05,
      "loss": 0.0031,
      "step": 39530
    },
    {
      "epoch": 2.1088,
      "grad_norm": 0.12357337027788162,
      "learning_rate": 3.682e-05,
      "loss": 0.0021,
      "step": 39540
    },
    {
      "epoch": 2.1093333333333333,
      "grad_norm": 0.2720469832420349,
      "learning_rate": 3.681666666666667e-05,
      "loss": 0.0033,
      "step": 39550
    },
    {
      "epoch": 2.1098666666666666,
      "grad_norm": 0.5144163370132446,
      "learning_rate": 3.6813333333333335e-05,
      "loss": 0.002,
      "step": 39560
    },
    {
      "epoch": 2.1104,
      "grad_norm": 0.20786060392856598,
      "learning_rate": 3.681e-05,
      "loss": 0.0019,
      "step": 39570
    },
    {
      "epoch": 2.1109333333333336,
      "grad_norm": 0.5116680860519409,
      "learning_rate": 3.680666666666667e-05,
      "loss": 0.003,
      "step": 39580
    },
    {
      "epoch": 2.111466666666667,
      "grad_norm": 0.2958870232105255,
      "learning_rate": 3.6803333333333334e-05,
      "loss": 0.0031,
      "step": 39590
    },
    {
      "epoch": 2.112,
      "grad_norm": 0.06748806685209274,
      "learning_rate": 3.68e-05,
      "loss": 0.0017,
      "step": 39600
    },
    {
      "epoch": 2.1125333333333334,
      "grad_norm": 0.47589990496635437,
      "learning_rate": 3.6796666666666666e-05,
      "loss": 0.0021,
      "step": 39610
    },
    {
      "epoch": 2.1130666666666666,
      "grad_norm": 0.15232522785663605,
      "learning_rate": 3.679333333333333e-05,
      "loss": 0.0028,
      "step": 39620
    },
    {
      "epoch": 2.1136,
      "grad_norm": 0.2097947597503662,
      "learning_rate": 3.6790000000000005e-05,
      "loss": 0.0027,
      "step": 39630
    },
    {
      "epoch": 2.114133333333333,
      "grad_norm": 0.1545056253671646,
      "learning_rate": 3.678666666666667e-05,
      "loss": 0.0029,
      "step": 39640
    },
    {
      "epoch": 2.1146666666666665,
      "grad_norm": 0.29637008905410767,
      "learning_rate": 3.678333333333334e-05,
      "loss": 0.003,
      "step": 39650
    },
    {
      "epoch": 2.1152,
      "grad_norm": 0.8613021969795227,
      "learning_rate": 3.6780000000000004e-05,
      "loss": 0.0032,
      "step": 39660
    },
    {
      "epoch": 2.1157333333333335,
      "grad_norm": 0.09162920713424683,
      "learning_rate": 3.677666666666667e-05,
      "loss": 0.0022,
      "step": 39670
    },
    {
      "epoch": 2.1162666666666667,
      "grad_norm": 0.18581175804138184,
      "learning_rate": 3.6773333333333336e-05,
      "loss": 0.0035,
      "step": 39680
    },
    {
      "epoch": 2.1168,
      "grad_norm": 0.0798506960272789,
      "learning_rate": 3.677e-05,
      "loss": 0.0029,
      "step": 39690
    },
    {
      "epoch": 2.1173333333333333,
      "grad_norm": 0.06228821352124214,
      "learning_rate": 3.676666666666667e-05,
      "loss": 0.0031,
      "step": 39700
    },
    {
      "epoch": 2.1178666666666666,
      "grad_norm": 0.20938672125339508,
      "learning_rate": 3.6763333333333334e-05,
      "loss": 0.0026,
      "step": 39710
    },
    {
      "epoch": 2.1184,
      "grad_norm": 0.15702857077121735,
      "learning_rate": 3.676e-05,
      "loss": 0.0024,
      "step": 39720
    },
    {
      "epoch": 2.1189333333333336,
      "grad_norm": 0.386019766330719,
      "learning_rate": 3.6756666666666667e-05,
      "loss": 0.0019,
      "step": 39730
    },
    {
      "epoch": 2.119466666666667,
      "grad_norm": 0.12184832245111465,
      "learning_rate": 3.675333333333333e-05,
      "loss": 0.0019,
      "step": 39740
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.06843369454145432,
      "learning_rate": 3.675e-05,
      "loss": 0.0022,
      "step": 39750
    },
    {
      "epoch": 2.1205333333333334,
      "grad_norm": 0.24646055698394775,
      "learning_rate": 3.6746666666666665e-05,
      "loss": 0.0042,
      "step": 39760
    },
    {
      "epoch": 2.1210666666666667,
      "grad_norm": 0.1222238540649414,
      "learning_rate": 3.674333333333334e-05,
      "loss": 0.0021,
      "step": 39770
    },
    {
      "epoch": 2.1216,
      "grad_norm": 0.3267938494682312,
      "learning_rate": 3.6740000000000004e-05,
      "loss": 0.0026,
      "step": 39780
    },
    {
      "epoch": 2.122133333333333,
      "grad_norm": 0.3287351131439209,
      "learning_rate": 3.673666666666667e-05,
      "loss": 0.003,
      "step": 39790
    },
    {
      "epoch": 2.1226666666666665,
      "grad_norm": 0.3020031750202179,
      "learning_rate": 3.6733333333333336e-05,
      "loss": 0.0027,
      "step": 39800
    },
    {
      "epoch": 2.1232,
      "grad_norm": 0.32564568519592285,
      "learning_rate": 3.673e-05,
      "loss": 0.0028,
      "step": 39810
    },
    {
      "epoch": 2.1237333333333335,
      "grad_norm": 0.19027447700500488,
      "learning_rate": 3.672666666666667e-05,
      "loss": 0.0032,
      "step": 39820
    },
    {
      "epoch": 2.1242666666666667,
      "grad_norm": 0.3306516110897064,
      "learning_rate": 3.6723333333333335e-05,
      "loss": 0.0023,
      "step": 39830
    },
    {
      "epoch": 2.1248,
      "grad_norm": 0.41489046812057495,
      "learning_rate": 3.672000000000001e-05,
      "loss": 0.0036,
      "step": 39840
    },
    {
      "epoch": 2.1253333333333333,
      "grad_norm": 0.06953533738851547,
      "learning_rate": 3.671666666666667e-05,
      "loss": 0.0034,
      "step": 39850
    },
    {
      "epoch": 2.1258666666666666,
      "grad_norm": 0.04859459400177002,
      "learning_rate": 3.671333333333333e-05,
      "loss": 0.0023,
      "step": 39860
    },
    {
      "epoch": 2.1264,
      "grad_norm": 0.7149311900138855,
      "learning_rate": 3.671e-05,
      "loss": 0.0026,
      "step": 39870
    },
    {
      "epoch": 2.1269333333333336,
      "grad_norm": 0.039067186415195465,
      "learning_rate": 3.6706666666666665e-05,
      "loss": 0.0027,
      "step": 39880
    },
    {
      "epoch": 2.127466666666667,
      "grad_norm": 0.14929039776325226,
      "learning_rate": 3.670333333333333e-05,
      "loss": 0.0022,
      "step": 39890
    },
    {
      "epoch": 2.128,
      "grad_norm": 0.6558533906936646,
      "learning_rate": 3.6700000000000004e-05,
      "loss": 0.0029,
      "step": 39900
    },
    {
      "epoch": 2.1285333333333334,
      "grad_norm": 0.17952010035514832,
      "learning_rate": 3.669666666666667e-05,
      "loss": 0.0041,
      "step": 39910
    },
    {
      "epoch": 2.1290666666666667,
      "grad_norm": 0.12408041954040527,
      "learning_rate": 3.669333333333334e-05,
      "loss": 0.0034,
      "step": 39920
    },
    {
      "epoch": 2.1296,
      "grad_norm": 0.05351986363530159,
      "learning_rate": 3.669e-05,
      "loss": 0.0033,
      "step": 39930
    },
    {
      "epoch": 2.130133333333333,
      "grad_norm": 0.037793975323438644,
      "learning_rate": 3.668666666666667e-05,
      "loss": 0.002,
      "step": 39940
    },
    {
      "epoch": 2.1306666666666665,
      "grad_norm": 0.27127841114997864,
      "learning_rate": 3.6683333333333335e-05,
      "loss": 0.0025,
      "step": 39950
    },
    {
      "epoch": 2.1312,
      "grad_norm": 0.271913081407547,
      "learning_rate": 3.668e-05,
      "loss": 0.0024,
      "step": 39960
    },
    {
      "epoch": 2.1317333333333335,
      "grad_norm": 0.1648610681295395,
      "learning_rate": 3.667666666666667e-05,
      "loss": 0.0021,
      "step": 39970
    },
    {
      "epoch": 2.1322666666666668,
      "grad_norm": 0.09620381891727448,
      "learning_rate": 3.667333333333334e-05,
      "loss": 0.0021,
      "step": 39980
    },
    {
      "epoch": 2.1328,
      "grad_norm": 0.2992451786994934,
      "learning_rate": 3.6670000000000006e-05,
      "loss": 0.0027,
      "step": 39990
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.38741961121559143,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 0.0024,
      "step": 40000
    },
    {
      "epoch": 2.1338666666666666,
      "grad_norm": 0.09202651679515839,
      "learning_rate": 3.666333333333333e-05,
      "loss": 0.002,
      "step": 40010
    },
    {
      "epoch": 2.1344,
      "grad_norm": 0.1839689463376999,
      "learning_rate": 3.666e-05,
      "loss": 0.0023,
      "step": 40020
    },
    {
      "epoch": 2.134933333333333,
      "grad_norm": 0.2401011884212494,
      "learning_rate": 3.6656666666666664e-05,
      "loss": 0.0034,
      "step": 40030
    },
    {
      "epoch": 2.135466666666667,
      "grad_norm": 0.2665145993232727,
      "learning_rate": 3.665333333333334e-05,
      "loss": 0.0035,
      "step": 40040
    },
    {
      "epoch": 2.136,
      "grad_norm": 0.2566753923892975,
      "learning_rate": 3.665e-05,
      "loss": 0.0025,
      "step": 40050
    },
    {
      "epoch": 2.1365333333333334,
      "grad_norm": 0.15509691834449768,
      "learning_rate": 3.664666666666667e-05,
      "loss": 0.0034,
      "step": 40060
    },
    {
      "epoch": 2.1370666666666667,
      "grad_norm": 0.048624709248542786,
      "learning_rate": 3.6643333333333335e-05,
      "loss": 0.0033,
      "step": 40070
    },
    {
      "epoch": 2.1376,
      "grad_norm": 0.4760984778404236,
      "learning_rate": 3.664e-05,
      "loss": 0.0035,
      "step": 40080
    },
    {
      "epoch": 2.138133333333333,
      "grad_norm": 0.12497192621231079,
      "learning_rate": 3.663666666666667e-05,
      "loss": 0.003,
      "step": 40090
    },
    {
      "epoch": 2.1386666666666665,
      "grad_norm": 0.23902210593223572,
      "learning_rate": 3.6633333333333334e-05,
      "loss": 0.0035,
      "step": 40100
    },
    {
      "epoch": 2.1391999999999998,
      "grad_norm": 0.0627877339720726,
      "learning_rate": 3.663e-05,
      "loss": 0.0025,
      "step": 40110
    },
    {
      "epoch": 2.1397333333333335,
      "grad_norm": 0.12693797051906586,
      "learning_rate": 3.662666666666667e-05,
      "loss": 0.0022,
      "step": 40120
    },
    {
      "epoch": 2.1402666666666668,
      "grad_norm": 0.1507801115512848,
      "learning_rate": 3.662333333333334e-05,
      "loss": 0.0028,
      "step": 40130
    },
    {
      "epoch": 2.1408,
      "grad_norm": 0.2695419192314148,
      "learning_rate": 3.6620000000000005e-05,
      "loss": 0.0028,
      "step": 40140
    },
    {
      "epoch": 2.1413333333333333,
      "grad_norm": 0.2950781583786011,
      "learning_rate": 3.6616666666666664e-05,
      "loss": 0.0021,
      "step": 40150
    },
    {
      "epoch": 2.1418666666666666,
      "grad_norm": 0.21219976246356964,
      "learning_rate": 3.661333333333333e-05,
      "loss": 0.0028,
      "step": 40160
    },
    {
      "epoch": 2.1424,
      "grad_norm": 0.44410502910614014,
      "learning_rate": 3.661e-05,
      "loss": 0.0019,
      "step": 40170
    },
    {
      "epoch": 2.142933333333333,
      "grad_norm": 0.1853826493024826,
      "learning_rate": 3.660666666666667e-05,
      "loss": 0.0031,
      "step": 40180
    },
    {
      "epoch": 2.143466666666667,
      "grad_norm": 0.1783725619316101,
      "learning_rate": 3.6603333333333336e-05,
      "loss": 0.0024,
      "step": 40190
    },
    {
      "epoch": 2.144,
      "grad_norm": 0.24783746898174286,
      "learning_rate": 3.66e-05,
      "loss": 0.0023,
      "step": 40200
    },
    {
      "epoch": 2.1445333333333334,
      "grad_norm": 0.24478745460510254,
      "learning_rate": 3.659666666666667e-05,
      "loss": 0.0035,
      "step": 40210
    },
    {
      "epoch": 2.1450666666666667,
      "grad_norm": 0.5499839782714844,
      "learning_rate": 3.6593333333333334e-05,
      "loss": 0.0028,
      "step": 40220
    },
    {
      "epoch": 2.1456,
      "grad_norm": 0.42500603199005127,
      "learning_rate": 3.659e-05,
      "loss": 0.0026,
      "step": 40230
    },
    {
      "epoch": 2.1461333333333332,
      "grad_norm": 0.3648975193500519,
      "learning_rate": 3.6586666666666666e-05,
      "loss": 0.0029,
      "step": 40240
    },
    {
      "epoch": 2.1466666666666665,
      "grad_norm": 0.09400580823421478,
      "learning_rate": 3.658333333333334e-05,
      "loss": 0.0036,
      "step": 40250
    },
    {
      "epoch": 2.1471999999999998,
      "grad_norm": 0.2888021469116211,
      "learning_rate": 3.6580000000000006e-05,
      "loss": 0.0036,
      "step": 40260
    },
    {
      "epoch": 2.1477333333333335,
      "grad_norm": 0.33247309923171997,
      "learning_rate": 3.657666666666667e-05,
      "loss": 0.0033,
      "step": 40270
    },
    {
      "epoch": 2.1482666666666668,
      "grad_norm": 0.3566548228263855,
      "learning_rate": 3.657333333333334e-05,
      "loss": 0.0023,
      "step": 40280
    },
    {
      "epoch": 2.1488,
      "grad_norm": 0.1540110558271408,
      "learning_rate": 3.6570000000000004e-05,
      "loss": 0.0023,
      "step": 40290
    },
    {
      "epoch": 2.1493333333333333,
      "grad_norm": 0.3345295190811157,
      "learning_rate": 3.656666666666666e-05,
      "loss": 0.0032,
      "step": 40300
    },
    {
      "epoch": 2.1498666666666666,
      "grad_norm": 0.5386093854904175,
      "learning_rate": 3.656333333333333e-05,
      "loss": 0.0025,
      "step": 40310
    },
    {
      "epoch": 2.1504,
      "grad_norm": 0.1503414809703827,
      "learning_rate": 3.656e-05,
      "loss": 0.0021,
      "step": 40320
    },
    {
      "epoch": 2.150933333333333,
      "grad_norm": 0.47983765602111816,
      "learning_rate": 3.655666666666667e-05,
      "loss": 0.0035,
      "step": 40330
    },
    {
      "epoch": 2.151466666666667,
      "grad_norm": 0.21020197868347168,
      "learning_rate": 3.6553333333333335e-05,
      "loss": 0.0016,
      "step": 40340
    },
    {
      "epoch": 2.152,
      "grad_norm": 0.3464067876338959,
      "learning_rate": 3.655e-05,
      "loss": 0.0038,
      "step": 40350
    },
    {
      "epoch": 2.1525333333333334,
      "grad_norm": 0.17933551967144012,
      "learning_rate": 3.654666666666667e-05,
      "loss": 0.002,
      "step": 40360
    },
    {
      "epoch": 2.1530666666666667,
      "grad_norm": 0.21334579586982727,
      "learning_rate": 3.654333333333333e-05,
      "loss": 0.0023,
      "step": 40370
    },
    {
      "epoch": 2.1536,
      "grad_norm": 0.09785663336515427,
      "learning_rate": 3.654e-05,
      "loss": 0.0019,
      "step": 40380
    },
    {
      "epoch": 2.1541333333333332,
      "grad_norm": 0.39040178060531616,
      "learning_rate": 3.653666666666667e-05,
      "loss": 0.0027,
      "step": 40390
    },
    {
      "epoch": 2.1546666666666665,
      "grad_norm": 0.6072673797607422,
      "learning_rate": 3.653333333333334e-05,
      "loss": 0.002,
      "step": 40400
    },
    {
      "epoch": 2.1552,
      "grad_norm": 0.06054982542991638,
      "learning_rate": 3.6530000000000004e-05,
      "loss": 0.0023,
      "step": 40410
    },
    {
      "epoch": 2.1557333333333335,
      "grad_norm": 0.12758147716522217,
      "learning_rate": 3.652666666666667e-05,
      "loss": 0.0032,
      "step": 40420
    },
    {
      "epoch": 2.1562666666666668,
      "grad_norm": 0.30164527893066406,
      "learning_rate": 3.6523333333333337e-05,
      "loss": 0.003,
      "step": 40430
    },
    {
      "epoch": 2.1568,
      "grad_norm": 0.20965081453323364,
      "learning_rate": 3.652e-05,
      "loss": 0.002,
      "step": 40440
    },
    {
      "epoch": 2.1573333333333333,
      "grad_norm": 0.3931499123573303,
      "learning_rate": 3.651666666666667e-05,
      "loss": 0.0037,
      "step": 40450
    },
    {
      "epoch": 2.1578666666666666,
      "grad_norm": 0.04485863819718361,
      "learning_rate": 3.6513333333333335e-05,
      "loss": 0.0039,
      "step": 40460
    },
    {
      "epoch": 2.1584,
      "grad_norm": 0.12163713574409485,
      "learning_rate": 3.651e-05,
      "loss": 0.0022,
      "step": 40470
    },
    {
      "epoch": 2.158933333333333,
      "grad_norm": 0.040477603673934937,
      "learning_rate": 3.650666666666667e-05,
      "loss": 0.0019,
      "step": 40480
    },
    {
      "epoch": 2.159466666666667,
      "grad_norm": 0.35717350244522095,
      "learning_rate": 3.650333333333333e-05,
      "loss": 0.0026,
      "step": 40490
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.21097639203071594,
      "learning_rate": 3.65e-05,
      "loss": 0.0026,
      "step": 40500
    },
    {
      "epoch": 2.1605333333333334,
      "grad_norm": 0.45046141743659973,
      "learning_rate": 3.6496666666666666e-05,
      "loss": 0.0028,
      "step": 40510
    },
    {
      "epoch": 2.1610666666666667,
      "grad_norm": 0.14986056089401245,
      "learning_rate": 3.649333333333333e-05,
      "loss": 0.0019,
      "step": 40520
    },
    {
      "epoch": 2.1616,
      "grad_norm": 0.12100392580032349,
      "learning_rate": 3.6490000000000005e-05,
      "loss": 0.0027,
      "step": 40530
    },
    {
      "epoch": 2.1621333333333332,
      "grad_norm": 0.23900432884693146,
      "learning_rate": 3.648666666666667e-05,
      "loss": 0.0041,
      "step": 40540
    },
    {
      "epoch": 2.1626666666666665,
      "grad_norm": 0.22207291424274445,
      "learning_rate": 3.648333333333334e-05,
      "loss": 0.0026,
      "step": 40550
    },
    {
      "epoch": 2.1632,
      "grad_norm": 0.47618669271469116,
      "learning_rate": 3.648e-05,
      "loss": 0.0023,
      "step": 40560
    },
    {
      "epoch": 2.1637333333333335,
      "grad_norm": 0.04551161825656891,
      "learning_rate": 3.647666666666667e-05,
      "loss": 0.0035,
      "step": 40570
    },
    {
      "epoch": 2.164266666666667,
      "grad_norm": 0.4497605264186859,
      "learning_rate": 3.6473333333333335e-05,
      "loss": 0.0029,
      "step": 40580
    },
    {
      "epoch": 2.1648,
      "grad_norm": 0.3049215078353882,
      "learning_rate": 3.647e-05,
      "loss": 0.0031,
      "step": 40590
    },
    {
      "epoch": 2.1653333333333333,
      "grad_norm": 0.023635506629943848,
      "learning_rate": 3.646666666666667e-05,
      "loss": 0.0034,
      "step": 40600
    },
    {
      "epoch": 2.1658666666666666,
      "grad_norm": 0.06278297305107117,
      "learning_rate": 3.6463333333333334e-05,
      "loss": 0.0028,
      "step": 40610
    },
    {
      "epoch": 2.1664,
      "grad_norm": 0.5115240216255188,
      "learning_rate": 3.646e-05,
      "loss": 0.0028,
      "step": 40620
    },
    {
      "epoch": 2.166933333333333,
      "grad_norm": 0.2686251997947693,
      "learning_rate": 3.6456666666666666e-05,
      "loss": 0.0041,
      "step": 40630
    },
    {
      "epoch": 2.167466666666667,
      "grad_norm": 0.38793739676475525,
      "learning_rate": 3.645333333333333e-05,
      "loss": 0.0029,
      "step": 40640
    },
    {
      "epoch": 2.168,
      "grad_norm": 0.09456651657819748,
      "learning_rate": 3.645e-05,
      "loss": 0.0036,
      "step": 40650
    },
    {
      "epoch": 2.1685333333333334,
      "grad_norm": 0.5059112906455994,
      "learning_rate": 3.644666666666667e-05,
      "loss": 0.0014,
      "step": 40660
    },
    {
      "epoch": 2.1690666666666667,
      "grad_norm": 0.23732879757881165,
      "learning_rate": 3.644333333333334e-05,
      "loss": 0.004,
      "step": 40670
    },
    {
      "epoch": 2.1696,
      "grad_norm": 0.3031972348690033,
      "learning_rate": 3.6440000000000003e-05,
      "loss": 0.0042,
      "step": 40680
    },
    {
      "epoch": 2.1701333333333332,
      "grad_norm": 0.024631882086396217,
      "learning_rate": 3.643666666666667e-05,
      "loss": 0.0029,
      "step": 40690
    },
    {
      "epoch": 2.1706666666666665,
      "grad_norm": 0.09575872123241425,
      "learning_rate": 3.6433333333333336e-05,
      "loss": 0.0029,
      "step": 40700
    },
    {
      "epoch": 2.1712,
      "grad_norm": 0.20987243950366974,
      "learning_rate": 3.643e-05,
      "loss": 0.0017,
      "step": 40710
    },
    {
      "epoch": 2.1717333333333335,
      "grad_norm": 0.2370041459798813,
      "learning_rate": 3.642666666666667e-05,
      "loss": 0.0042,
      "step": 40720
    },
    {
      "epoch": 2.172266666666667,
      "grad_norm": 0.3551522195339203,
      "learning_rate": 3.6423333333333334e-05,
      "loss": 0.0027,
      "step": 40730
    },
    {
      "epoch": 2.1728,
      "grad_norm": 0.30346250534057617,
      "learning_rate": 3.642000000000001e-05,
      "loss": 0.0025,
      "step": 40740
    },
    {
      "epoch": 2.1733333333333333,
      "grad_norm": 0.3579239249229431,
      "learning_rate": 3.641666666666667e-05,
      "loss": 0.0018,
      "step": 40750
    },
    {
      "epoch": 2.1738666666666666,
      "grad_norm": 0.06326007097959518,
      "learning_rate": 3.641333333333333e-05,
      "loss": 0.0024,
      "step": 40760
    },
    {
      "epoch": 2.1744,
      "grad_norm": 0.38540297746658325,
      "learning_rate": 3.641e-05,
      "loss": 0.0018,
      "step": 40770
    },
    {
      "epoch": 2.174933333333333,
      "grad_norm": 0.13087217509746552,
      "learning_rate": 3.6406666666666665e-05,
      "loss": 0.0021,
      "step": 40780
    },
    {
      "epoch": 2.175466666666667,
      "grad_norm": 0.13426268100738525,
      "learning_rate": 3.640333333333333e-05,
      "loss": 0.0035,
      "step": 40790
    },
    {
      "epoch": 2.176,
      "grad_norm": 0.0655817911028862,
      "learning_rate": 3.6400000000000004e-05,
      "loss": 0.0031,
      "step": 40800
    },
    {
      "epoch": 2.1765333333333334,
      "grad_norm": 0.2728181481361389,
      "learning_rate": 3.639666666666667e-05,
      "loss": 0.0029,
      "step": 40810
    },
    {
      "epoch": 2.1770666666666667,
      "grad_norm": 0.5946143269538879,
      "learning_rate": 3.6393333333333336e-05,
      "loss": 0.0027,
      "step": 40820
    },
    {
      "epoch": 2.1776,
      "grad_norm": 0.39017048478126526,
      "learning_rate": 3.639e-05,
      "loss": 0.0019,
      "step": 40830
    },
    {
      "epoch": 2.1781333333333333,
      "grad_norm": 0.39001530408859253,
      "learning_rate": 3.638666666666667e-05,
      "loss": 0.0028,
      "step": 40840
    },
    {
      "epoch": 2.1786666666666665,
      "grad_norm": 0.35804489254951477,
      "learning_rate": 3.6383333333333335e-05,
      "loss": 0.0029,
      "step": 40850
    },
    {
      "epoch": 2.1792,
      "grad_norm": 0.15154607594013214,
      "learning_rate": 3.638e-05,
      "loss": 0.0024,
      "step": 40860
    },
    {
      "epoch": 2.1797333333333335,
      "grad_norm": 0.26758936047554016,
      "learning_rate": 3.637666666666667e-05,
      "loss": 0.0042,
      "step": 40870
    },
    {
      "epoch": 2.180266666666667,
      "grad_norm": 0.12018143385648727,
      "learning_rate": 3.637333333333334e-05,
      "loss": 0.002,
      "step": 40880
    },
    {
      "epoch": 2.1808,
      "grad_norm": 0.32705652713775635,
      "learning_rate": 3.6370000000000006e-05,
      "loss": 0.0022,
      "step": 40890
    },
    {
      "epoch": 2.1813333333333333,
      "grad_norm": 0.08064020425081253,
      "learning_rate": 3.636666666666667e-05,
      "loss": 0.0029,
      "step": 40900
    },
    {
      "epoch": 2.1818666666666666,
      "grad_norm": 0.187729611992836,
      "learning_rate": 3.636333333333333e-05,
      "loss": 0.003,
      "step": 40910
    },
    {
      "epoch": 2.1824,
      "grad_norm": 0.12249739468097687,
      "learning_rate": 3.636e-05,
      "loss": 0.003,
      "step": 40920
    },
    {
      "epoch": 2.182933333333333,
      "grad_norm": 0.20998093485832214,
      "learning_rate": 3.6356666666666664e-05,
      "loss": 0.0033,
      "step": 40930
    },
    {
      "epoch": 2.183466666666667,
      "grad_norm": 0.35483285784721375,
      "learning_rate": 3.6353333333333337e-05,
      "loss": 0.0033,
      "step": 40940
    },
    {
      "epoch": 2.184,
      "grad_norm": 0.3277210295200348,
      "learning_rate": 3.635e-05,
      "loss": 0.0027,
      "step": 40950
    },
    {
      "epoch": 2.1845333333333334,
      "grad_norm": 0.20759354531764984,
      "learning_rate": 3.634666666666667e-05,
      "loss": 0.0033,
      "step": 40960
    },
    {
      "epoch": 2.1850666666666667,
      "grad_norm": 0.031130295246839523,
      "learning_rate": 3.6343333333333335e-05,
      "loss": 0.0036,
      "step": 40970
    },
    {
      "epoch": 2.1856,
      "grad_norm": 0.13005346059799194,
      "learning_rate": 3.634e-05,
      "loss": 0.0024,
      "step": 40980
    },
    {
      "epoch": 2.1861333333333333,
      "grad_norm": 0.0654025450348854,
      "learning_rate": 3.633666666666667e-05,
      "loss": 0.0028,
      "step": 40990
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 0.21004226803779602,
      "learning_rate": 3.633333333333333e-05,
      "loss": 0.0022,
      "step": 41000
    },
    {
      "epoch": 2.1872,
      "grad_norm": 0.15095238387584686,
      "learning_rate": 3.6330000000000006e-05,
      "loss": 0.0035,
      "step": 41010
    },
    {
      "epoch": 2.1877333333333335,
      "grad_norm": 0.5315815210342407,
      "learning_rate": 3.632666666666667e-05,
      "loss": 0.0038,
      "step": 41020
    },
    {
      "epoch": 2.188266666666667,
      "grad_norm": 0.327189177274704,
      "learning_rate": 3.632333333333334e-05,
      "loss": 0.0022,
      "step": 41030
    },
    {
      "epoch": 2.1888,
      "grad_norm": 0.12434887140989304,
      "learning_rate": 3.6320000000000005e-05,
      "loss": 0.0031,
      "step": 41040
    },
    {
      "epoch": 2.1893333333333334,
      "grad_norm": 0.4201626181602478,
      "learning_rate": 3.631666666666667e-05,
      "loss": 0.0026,
      "step": 41050
    },
    {
      "epoch": 2.1898666666666666,
      "grad_norm": 0.4469689726829529,
      "learning_rate": 3.631333333333333e-05,
      "loss": 0.003,
      "step": 41060
    },
    {
      "epoch": 2.1904,
      "grad_norm": 0.42921459674835205,
      "learning_rate": 3.6309999999999996e-05,
      "loss": 0.0035,
      "step": 41070
    },
    {
      "epoch": 2.190933333333333,
      "grad_norm": 0.5095183253288269,
      "learning_rate": 3.630666666666667e-05,
      "loss": 0.0041,
      "step": 41080
    },
    {
      "epoch": 2.191466666666667,
      "grad_norm": 0.39119818806648254,
      "learning_rate": 3.6303333333333335e-05,
      "loss": 0.0034,
      "step": 41090
    },
    {
      "epoch": 2.192,
      "grad_norm": 0.07756084948778152,
      "learning_rate": 3.63e-05,
      "loss": 0.0026,
      "step": 41100
    },
    {
      "epoch": 2.1925333333333334,
      "grad_norm": 0.2657313644886017,
      "learning_rate": 3.629666666666667e-05,
      "loss": 0.0039,
      "step": 41110
    },
    {
      "epoch": 2.1930666666666667,
      "grad_norm": 0.1543949693441391,
      "learning_rate": 3.6293333333333334e-05,
      "loss": 0.0034,
      "step": 41120
    },
    {
      "epoch": 2.1936,
      "grad_norm": 0.04599756374955177,
      "learning_rate": 3.629e-05,
      "loss": 0.0024,
      "step": 41130
    },
    {
      "epoch": 2.1941333333333333,
      "grad_norm": 0.09829192608594894,
      "learning_rate": 3.6286666666666666e-05,
      "loss": 0.0026,
      "step": 41140
    },
    {
      "epoch": 2.1946666666666665,
      "grad_norm": 0.4568122327327728,
      "learning_rate": 3.628333333333334e-05,
      "loss": 0.0039,
      "step": 41150
    },
    {
      "epoch": 2.1952,
      "grad_norm": 0.3562604784965515,
      "learning_rate": 3.6280000000000005e-05,
      "loss": 0.0027,
      "step": 41160
    },
    {
      "epoch": 2.1957333333333335,
      "grad_norm": 0.14957287907600403,
      "learning_rate": 3.627666666666667e-05,
      "loss": 0.0033,
      "step": 41170
    },
    {
      "epoch": 2.196266666666667,
      "grad_norm": 0.39839717745780945,
      "learning_rate": 3.627333333333334e-05,
      "loss": 0.0023,
      "step": 41180
    },
    {
      "epoch": 2.1968,
      "grad_norm": 0.29916998744010925,
      "learning_rate": 3.6270000000000003e-05,
      "loss": 0.0027,
      "step": 41190
    },
    {
      "epoch": 2.1973333333333334,
      "grad_norm": 0.8652738928794861,
      "learning_rate": 3.626666666666667e-05,
      "loss": 0.0021,
      "step": 41200
    },
    {
      "epoch": 2.1978666666666666,
      "grad_norm": 0.7458838224411011,
      "learning_rate": 3.6263333333333336e-05,
      "loss": 0.003,
      "step": 41210
    },
    {
      "epoch": 2.1984,
      "grad_norm": 0.4433336853981018,
      "learning_rate": 3.626e-05,
      "loss": 0.0024,
      "step": 41220
    },
    {
      "epoch": 2.198933333333333,
      "grad_norm": 0.03581159934401512,
      "learning_rate": 3.625666666666667e-05,
      "loss": 0.0026,
      "step": 41230
    },
    {
      "epoch": 2.1994666666666665,
      "grad_norm": 0.16146670281887054,
      "learning_rate": 3.6253333333333334e-05,
      "loss": 0.0027,
      "step": 41240
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.25458288192749023,
      "learning_rate": 3.625e-05,
      "loss": 0.0024,
      "step": 41250
    },
    {
      "epoch": 2.2005333333333335,
      "grad_norm": 0.12068302929401398,
      "learning_rate": 3.6246666666666666e-05,
      "loss": 0.003,
      "step": 41260
    },
    {
      "epoch": 2.2010666666666667,
      "grad_norm": 0.09479446709156036,
      "learning_rate": 3.624333333333333e-05,
      "loss": 0.0028,
      "step": 41270
    },
    {
      "epoch": 2.2016,
      "grad_norm": 0.34559890627861023,
      "learning_rate": 3.624e-05,
      "loss": 0.0034,
      "step": 41280
    },
    {
      "epoch": 2.2021333333333333,
      "grad_norm": 0.590559720993042,
      "learning_rate": 3.623666666666667e-05,
      "loss": 0.0028,
      "step": 41290
    },
    {
      "epoch": 2.2026666666666666,
      "grad_norm": 0.06629613041877747,
      "learning_rate": 3.623333333333334e-05,
      "loss": 0.0026,
      "step": 41300
    },
    {
      "epoch": 2.2032,
      "grad_norm": 0.15270566940307617,
      "learning_rate": 3.6230000000000004e-05,
      "loss": 0.0025,
      "step": 41310
    },
    {
      "epoch": 2.203733333333333,
      "grad_norm": 0.04149576649069786,
      "learning_rate": 3.622666666666667e-05,
      "loss": 0.0023,
      "step": 41320
    },
    {
      "epoch": 2.204266666666667,
      "grad_norm": 0.33112749457359314,
      "learning_rate": 3.6223333333333336e-05,
      "loss": 0.0024,
      "step": 41330
    },
    {
      "epoch": 2.2048,
      "grad_norm": 0.12852025032043457,
      "learning_rate": 3.622e-05,
      "loss": 0.0028,
      "step": 41340
    },
    {
      "epoch": 2.2053333333333334,
      "grad_norm": 0.06593003123998642,
      "learning_rate": 3.621666666666667e-05,
      "loss": 0.0025,
      "step": 41350
    },
    {
      "epoch": 2.2058666666666666,
      "grad_norm": 0.29911255836486816,
      "learning_rate": 3.6213333333333334e-05,
      "loss": 0.0029,
      "step": 41360
    },
    {
      "epoch": 2.2064,
      "grad_norm": 0.13207851350307465,
      "learning_rate": 3.621e-05,
      "loss": 0.0029,
      "step": 41370
    },
    {
      "epoch": 2.206933333333333,
      "grad_norm": 0.6573401689529419,
      "learning_rate": 3.620666666666667e-05,
      "loss": 0.0029,
      "step": 41380
    },
    {
      "epoch": 2.2074666666666665,
      "grad_norm": 0.0938759595155716,
      "learning_rate": 3.620333333333333e-05,
      "loss": 0.0018,
      "step": 41390
    },
    {
      "epoch": 2.208,
      "grad_norm": 0.0698109120130539,
      "learning_rate": 3.62e-05,
      "loss": 0.0025,
      "step": 41400
    },
    {
      "epoch": 2.2085333333333335,
      "grad_norm": 0.2970142066478729,
      "learning_rate": 3.6196666666666665e-05,
      "loss": 0.0024,
      "step": 41410
    },
    {
      "epoch": 2.2090666666666667,
      "grad_norm": 0.05125410482287407,
      "learning_rate": 3.619333333333333e-05,
      "loss": 0.0029,
      "step": 41420
    },
    {
      "epoch": 2.2096,
      "grad_norm": 0.18091820180416107,
      "learning_rate": 3.6190000000000004e-05,
      "loss": 0.0013,
      "step": 41430
    },
    {
      "epoch": 2.2101333333333333,
      "grad_norm": 0.20710711181163788,
      "learning_rate": 3.618666666666667e-05,
      "loss": 0.0026,
      "step": 41440
    },
    {
      "epoch": 2.2106666666666666,
      "grad_norm": 0.12263002246618271,
      "learning_rate": 3.6183333333333336e-05,
      "loss": 0.0027,
      "step": 41450
    },
    {
      "epoch": 2.2112,
      "grad_norm": 0.17770268023014069,
      "learning_rate": 3.618e-05,
      "loss": 0.0023,
      "step": 41460
    },
    {
      "epoch": 2.211733333333333,
      "grad_norm": 0.10518322139978409,
      "learning_rate": 3.617666666666667e-05,
      "loss": 0.0034,
      "step": 41470
    },
    {
      "epoch": 2.212266666666667,
      "grad_norm": 0.26888296008110046,
      "learning_rate": 3.6173333333333335e-05,
      "loss": 0.002,
      "step": 41480
    },
    {
      "epoch": 2.2128,
      "grad_norm": 0.15147525072097778,
      "learning_rate": 3.617e-05,
      "loss": 0.003,
      "step": 41490
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 0.09032896906137466,
      "learning_rate": 3.6166666666666674e-05,
      "loss": 0.0029,
      "step": 41500
    },
    {
      "epoch": 2.2138666666666666,
      "grad_norm": 0.15018419921398163,
      "learning_rate": 3.616333333333333e-05,
      "loss": 0.0014,
      "step": 41510
    },
    {
      "epoch": 2.2144,
      "grad_norm": 0.21037185192108154,
      "learning_rate": 3.616e-05,
      "loss": 0.0025,
      "step": 41520
    },
    {
      "epoch": 2.214933333333333,
      "grad_norm": 0.1251223236322403,
      "learning_rate": 3.6156666666666666e-05,
      "loss": 0.0024,
      "step": 41530
    },
    {
      "epoch": 2.2154666666666665,
      "grad_norm": 0.24093499779701233,
      "learning_rate": 3.615333333333333e-05,
      "loss": 0.0028,
      "step": 41540
    },
    {
      "epoch": 2.216,
      "grad_norm": 0.38461005687713623,
      "learning_rate": 3.615e-05,
      "loss": 0.002,
      "step": 41550
    },
    {
      "epoch": 2.2165333333333335,
      "grad_norm": 0.4785977602005005,
      "learning_rate": 3.614666666666667e-05,
      "loss": 0.0032,
      "step": 41560
    },
    {
      "epoch": 2.2170666666666667,
      "grad_norm": 0.03929637372493744,
      "learning_rate": 3.614333333333334e-05,
      "loss": 0.0034,
      "step": 41570
    },
    {
      "epoch": 2.2176,
      "grad_norm": 0.5626082420349121,
      "learning_rate": 3.614e-05,
      "loss": 0.0035,
      "step": 41580
    },
    {
      "epoch": 2.2181333333333333,
      "grad_norm": 0.3552456796169281,
      "learning_rate": 3.613666666666667e-05,
      "loss": 0.0022,
      "step": 41590
    },
    {
      "epoch": 2.2186666666666666,
      "grad_norm": 0.34302201867103577,
      "learning_rate": 3.6133333333333335e-05,
      "loss": 0.0028,
      "step": 41600
    },
    {
      "epoch": 2.2192,
      "grad_norm": 0.44619685411453247,
      "learning_rate": 3.613e-05,
      "loss": 0.0028,
      "step": 41610
    },
    {
      "epoch": 2.219733333333333,
      "grad_norm": 0.36661088466644287,
      "learning_rate": 3.612666666666667e-05,
      "loss": 0.0023,
      "step": 41620
    },
    {
      "epoch": 2.220266666666667,
      "grad_norm": 0.26911765336990356,
      "learning_rate": 3.6123333333333334e-05,
      "loss": 0.0044,
      "step": 41630
    },
    {
      "epoch": 2.2208,
      "grad_norm": 0.6537142992019653,
      "learning_rate": 3.6120000000000007e-05,
      "loss": 0.0023,
      "step": 41640
    },
    {
      "epoch": 2.2213333333333334,
      "grad_norm": 0.23961876332759857,
      "learning_rate": 3.611666666666667e-05,
      "loss": 0.0031,
      "step": 41650
    },
    {
      "epoch": 2.2218666666666667,
      "grad_norm": 0.12277397513389587,
      "learning_rate": 3.611333333333333e-05,
      "loss": 0.0019,
      "step": 41660
    },
    {
      "epoch": 2.2224,
      "grad_norm": 0.6836315393447876,
      "learning_rate": 3.611e-05,
      "loss": 0.0026,
      "step": 41670
    },
    {
      "epoch": 2.222933333333333,
      "grad_norm": 0.14916270971298218,
      "learning_rate": 3.6106666666666664e-05,
      "loss": 0.0027,
      "step": 41680
    },
    {
      "epoch": 2.2234666666666665,
      "grad_norm": 0.44658517837524414,
      "learning_rate": 3.610333333333333e-05,
      "loss": 0.0032,
      "step": 41690
    },
    {
      "epoch": 2.224,
      "grad_norm": 0.2678147852420807,
      "learning_rate": 3.61e-05,
      "loss": 0.0023,
      "step": 41700
    },
    {
      "epoch": 2.2245333333333335,
      "grad_norm": 0.03918631002306938,
      "learning_rate": 3.609666666666667e-05,
      "loss": 0.0033,
      "step": 41710
    },
    {
      "epoch": 2.2250666666666667,
      "grad_norm": 0.26860862970352173,
      "learning_rate": 3.6093333333333336e-05,
      "loss": 0.0026,
      "step": 41720
    },
    {
      "epoch": 2.2256,
      "grad_norm": 0.41962718963623047,
      "learning_rate": 3.609e-05,
      "loss": 0.0042,
      "step": 41730
    },
    {
      "epoch": 2.2261333333333333,
      "grad_norm": 0.04233948886394501,
      "learning_rate": 3.608666666666667e-05,
      "loss": 0.0031,
      "step": 41740
    },
    {
      "epoch": 2.2266666666666666,
      "grad_norm": 0.44194361567497253,
      "learning_rate": 3.6083333333333334e-05,
      "loss": 0.0029,
      "step": 41750
    },
    {
      "epoch": 2.2272,
      "grad_norm": 0.18168973922729492,
      "learning_rate": 3.608e-05,
      "loss": 0.003,
      "step": 41760
    },
    {
      "epoch": 2.227733333333333,
      "grad_norm": 0.36114832758903503,
      "learning_rate": 3.607666666666667e-05,
      "loss": 0.0027,
      "step": 41770
    },
    {
      "epoch": 2.228266666666667,
      "grad_norm": 0.2966634929180145,
      "learning_rate": 3.607333333333334e-05,
      "loss": 0.0028,
      "step": 41780
    },
    {
      "epoch": 2.2288,
      "grad_norm": 0.050739873200654984,
      "learning_rate": 3.6070000000000005e-05,
      "loss": 0.0032,
      "step": 41790
    },
    {
      "epoch": 2.2293333333333334,
      "grad_norm": 0.0908416211605072,
      "learning_rate": 3.606666666666667e-05,
      "loss": 0.0025,
      "step": 41800
    },
    {
      "epoch": 2.2298666666666667,
      "grad_norm": 0.3677148222923279,
      "learning_rate": 3.606333333333333e-05,
      "loss": 0.0029,
      "step": 41810
    },
    {
      "epoch": 2.2304,
      "grad_norm": 0.1597328633069992,
      "learning_rate": 3.606e-05,
      "loss": 0.0023,
      "step": 41820
    },
    {
      "epoch": 2.230933333333333,
      "grad_norm": 0.036469943821430206,
      "learning_rate": 3.605666666666666e-05,
      "loss": 0.0029,
      "step": 41830
    },
    {
      "epoch": 2.2314666666666665,
      "grad_norm": 0.4481055736541748,
      "learning_rate": 3.6053333333333336e-05,
      "loss": 0.002,
      "step": 41840
    },
    {
      "epoch": 2.232,
      "grad_norm": 0.5638430714607239,
      "learning_rate": 3.605e-05,
      "loss": 0.0026,
      "step": 41850
    },
    {
      "epoch": 2.2325333333333335,
      "grad_norm": 0.17959319055080414,
      "learning_rate": 3.604666666666667e-05,
      "loss": 0.0035,
      "step": 41860
    },
    {
      "epoch": 2.2330666666666668,
      "grad_norm": 0.5233615040779114,
      "learning_rate": 3.6043333333333334e-05,
      "loss": 0.0019,
      "step": 41870
    },
    {
      "epoch": 2.2336,
      "grad_norm": 0.27080345153808594,
      "learning_rate": 3.604e-05,
      "loss": 0.0038,
      "step": 41880
    },
    {
      "epoch": 2.2341333333333333,
      "grad_norm": 0.3219373822212219,
      "learning_rate": 3.603666666666667e-05,
      "loss": 0.0027,
      "step": 41890
    },
    {
      "epoch": 2.2346666666666666,
      "grad_norm": 0.302611380815506,
      "learning_rate": 3.603333333333333e-05,
      "loss": 0.0021,
      "step": 41900
    },
    {
      "epoch": 2.2352,
      "grad_norm": 0.47793540358543396,
      "learning_rate": 3.6030000000000006e-05,
      "loss": 0.0028,
      "step": 41910
    },
    {
      "epoch": 2.235733333333333,
      "grad_norm": 0.7411473989486694,
      "learning_rate": 3.602666666666667e-05,
      "loss": 0.0032,
      "step": 41920
    },
    {
      "epoch": 2.236266666666667,
      "grad_norm": 0.5057857036590576,
      "learning_rate": 3.602333333333334e-05,
      "loss": 0.003,
      "step": 41930
    },
    {
      "epoch": 2.2368,
      "grad_norm": 0.2708440124988556,
      "learning_rate": 3.6020000000000004e-05,
      "loss": 0.0023,
      "step": 41940
    },
    {
      "epoch": 2.2373333333333334,
      "grad_norm": 0.2948799133300781,
      "learning_rate": 3.601666666666667e-05,
      "loss": 0.0029,
      "step": 41950
    },
    {
      "epoch": 2.2378666666666667,
      "grad_norm": 0.4794733226299286,
      "learning_rate": 3.6013333333333336e-05,
      "loss": 0.0026,
      "step": 41960
    },
    {
      "epoch": 2.2384,
      "grad_norm": 0.2084469050168991,
      "learning_rate": 3.601e-05,
      "loss": 0.0029,
      "step": 41970
    },
    {
      "epoch": 2.238933333333333,
      "grad_norm": 0.15529003739356995,
      "learning_rate": 3.600666666666667e-05,
      "loss": 0.0021,
      "step": 41980
    },
    {
      "epoch": 2.2394666666666665,
      "grad_norm": 0.18215236067771912,
      "learning_rate": 3.6003333333333335e-05,
      "loss": 0.0028,
      "step": 41990
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.06955572962760925,
      "learning_rate": 3.6e-05,
      "loss": 0.0027,
      "step": 42000
    },
    {
      "epoch": 2.2405333333333335,
      "grad_norm": 0.06911586970090866,
      "learning_rate": 3.599666666666667e-05,
      "loss": 0.0019,
      "step": 42010
    },
    {
      "epoch": 2.2410666666666668,
      "grad_norm": 0.3527635931968689,
      "learning_rate": 3.599333333333333e-05,
      "loss": 0.003,
      "step": 42020
    },
    {
      "epoch": 2.2416,
      "grad_norm": 0.47603970766067505,
      "learning_rate": 3.599e-05,
      "loss": 0.0029,
      "step": 42030
    },
    {
      "epoch": 2.2421333333333333,
      "grad_norm": 0.5643885135650635,
      "learning_rate": 3.5986666666666665e-05,
      "loss": 0.0026,
      "step": 42040
    },
    {
      "epoch": 2.2426666666666666,
      "grad_norm": 0.4380940794944763,
      "learning_rate": 3.598333333333334e-05,
      "loss": 0.0026,
      "step": 42050
    },
    {
      "epoch": 2.2432,
      "grad_norm": 0.21420058608055115,
      "learning_rate": 3.5980000000000004e-05,
      "loss": 0.0032,
      "step": 42060
    },
    {
      "epoch": 2.243733333333333,
      "grad_norm": 0.5317216515541077,
      "learning_rate": 3.597666666666667e-05,
      "loss": 0.0015,
      "step": 42070
    },
    {
      "epoch": 2.244266666666667,
      "grad_norm": 0.20802263915538788,
      "learning_rate": 3.597333333333334e-05,
      "loss": 0.0027,
      "step": 42080
    },
    {
      "epoch": 2.2448,
      "grad_norm": 0.14823807775974274,
      "learning_rate": 3.597e-05,
      "loss": 0.0037,
      "step": 42090
    },
    {
      "epoch": 2.2453333333333334,
      "grad_norm": 0.1872469037771225,
      "learning_rate": 3.596666666666667e-05,
      "loss": 0.0025,
      "step": 42100
    },
    {
      "epoch": 2.2458666666666667,
      "grad_norm": 0.10414465516805649,
      "learning_rate": 3.5963333333333335e-05,
      "loss": 0.0024,
      "step": 42110
    },
    {
      "epoch": 2.2464,
      "grad_norm": 0.5329994559288025,
      "learning_rate": 3.596e-05,
      "loss": 0.0031,
      "step": 42120
    },
    {
      "epoch": 2.2469333333333332,
      "grad_norm": 0.5930740237236023,
      "learning_rate": 3.595666666666667e-05,
      "loss": 0.0033,
      "step": 42130
    },
    {
      "epoch": 2.2474666666666665,
      "grad_norm": 0.45635733008384705,
      "learning_rate": 3.5953333333333334e-05,
      "loss": 0.0037,
      "step": 42140
    },
    {
      "epoch": 2.248,
      "grad_norm": 0.41428810358047485,
      "learning_rate": 3.595e-05,
      "loss": 0.0033,
      "step": 42150
    },
    {
      "epoch": 2.2485333333333335,
      "grad_norm": 0.38898053765296936,
      "learning_rate": 3.5946666666666666e-05,
      "loss": 0.0043,
      "step": 42160
    },
    {
      "epoch": 2.2490666666666668,
      "grad_norm": 0.05676569044589996,
      "learning_rate": 3.594333333333333e-05,
      "loss": 0.0031,
      "step": 42170
    },
    {
      "epoch": 2.2496,
      "grad_norm": 0.27413538098335266,
      "learning_rate": 3.594e-05,
      "loss": 0.0026,
      "step": 42180
    },
    {
      "epoch": 2.2501333333333333,
      "grad_norm": 0.044011440128088,
      "learning_rate": 3.593666666666667e-05,
      "loss": 0.0031,
      "step": 42190
    },
    {
      "epoch": 2.2506666666666666,
      "grad_norm": 0.3282231092453003,
      "learning_rate": 3.593333333333334e-05,
      "loss": 0.0019,
      "step": 42200
    },
    {
      "epoch": 2.2512,
      "grad_norm": 0.09184439480304718,
      "learning_rate": 3.593e-05,
      "loss": 0.0028,
      "step": 42210
    },
    {
      "epoch": 2.251733333333333,
      "grad_norm": 0.2390470653772354,
      "learning_rate": 3.592666666666667e-05,
      "loss": 0.0027,
      "step": 42220
    },
    {
      "epoch": 2.2522666666666664,
      "grad_norm": 0.43477898836135864,
      "learning_rate": 3.5923333333333336e-05,
      "loss": 0.0031,
      "step": 42230
    },
    {
      "epoch": 2.2528,
      "grad_norm": 0.4168539345264435,
      "learning_rate": 3.592e-05,
      "loss": 0.0021,
      "step": 42240
    },
    {
      "epoch": 2.2533333333333334,
      "grad_norm": 0.33205336332321167,
      "learning_rate": 3.591666666666667e-05,
      "loss": 0.0025,
      "step": 42250
    },
    {
      "epoch": 2.2538666666666667,
      "grad_norm": 0.2991376221179962,
      "learning_rate": 3.591333333333334e-05,
      "loss": 0.0023,
      "step": 42260
    },
    {
      "epoch": 2.2544,
      "grad_norm": 0.20113389194011688,
      "learning_rate": 3.591e-05,
      "loss": 0.0026,
      "step": 42270
    },
    {
      "epoch": 2.2549333333333332,
      "grad_norm": 0.12326148897409439,
      "learning_rate": 3.5906666666666666e-05,
      "loss": 0.003,
      "step": 42280
    },
    {
      "epoch": 2.2554666666666665,
      "grad_norm": 0.13112609088420868,
      "learning_rate": 3.590333333333333e-05,
      "loss": 0.0032,
      "step": 42290
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 0.2274390459060669,
      "learning_rate": 3.59e-05,
      "loss": 0.0026,
      "step": 42300
    },
    {
      "epoch": 2.2565333333333335,
      "grad_norm": 0.15337824821472168,
      "learning_rate": 3.5896666666666665e-05,
      "loss": 0.0034,
      "step": 42310
    },
    {
      "epoch": 2.2570666666666668,
      "grad_norm": 0.01957790181040764,
      "learning_rate": 3.589333333333334e-05,
      "loss": 0.002,
      "step": 42320
    },
    {
      "epoch": 2.2576,
      "grad_norm": 0.47569721937179565,
      "learning_rate": 3.5890000000000004e-05,
      "loss": 0.0021,
      "step": 42330
    },
    {
      "epoch": 2.2581333333333333,
      "grad_norm": 0.12436338514089584,
      "learning_rate": 3.588666666666667e-05,
      "loss": 0.0028,
      "step": 42340
    },
    {
      "epoch": 2.2586666666666666,
      "grad_norm": 0.26566705107688904,
      "learning_rate": 3.5883333333333336e-05,
      "loss": 0.0023,
      "step": 42350
    },
    {
      "epoch": 2.2592,
      "grad_norm": 0.3551560044288635,
      "learning_rate": 3.588e-05,
      "loss": 0.0032,
      "step": 42360
    },
    {
      "epoch": 2.259733333333333,
      "grad_norm": 0.24910511076450348,
      "learning_rate": 3.587666666666667e-05,
      "loss": 0.0036,
      "step": 42370
    },
    {
      "epoch": 2.2602666666666664,
      "grad_norm": 0.09209255874156952,
      "learning_rate": 3.5873333333333334e-05,
      "loss": 0.0025,
      "step": 42380
    },
    {
      "epoch": 2.2608,
      "grad_norm": 0.03853562846779823,
      "learning_rate": 3.587e-05,
      "loss": 0.0026,
      "step": 42390
    },
    {
      "epoch": 2.2613333333333334,
      "grad_norm": 0.2072296440601349,
      "learning_rate": 3.586666666666667e-05,
      "loss": 0.004,
      "step": 42400
    },
    {
      "epoch": 2.2618666666666667,
      "grad_norm": 0.06404229253530502,
      "learning_rate": 3.586333333333334e-05,
      "loss": 0.003,
      "step": 42410
    },
    {
      "epoch": 2.2624,
      "grad_norm": 0.3441144824028015,
      "learning_rate": 3.586e-05,
      "loss": 0.0024,
      "step": 42420
    },
    {
      "epoch": 2.2629333333333332,
      "grad_norm": 0.09334106743335724,
      "learning_rate": 3.5856666666666665e-05,
      "loss": 0.0027,
      "step": 42430
    },
    {
      "epoch": 2.2634666666666665,
      "grad_norm": 0.43104979395866394,
      "learning_rate": 3.585333333333333e-05,
      "loss": 0.0021,
      "step": 42440
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 0.06894011050462723,
      "learning_rate": 3.585e-05,
      "loss": 0.0022,
      "step": 42450
    },
    {
      "epoch": 2.2645333333333335,
      "grad_norm": 0.3180490732192993,
      "learning_rate": 3.584666666666667e-05,
      "loss": 0.0021,
      "step": 42460
    },
    {
      "epoch": 2.265066666666667,
      "grad_norm": 0.11887289583683014,
      "learning_rate": 3.5843333333333336e-05,
      "loss": 0.0034,
      "step": 42470
    },
    {
      "epoch": 2.2656,
      "grad_norm": 0.4472586214542389,
      "learning_rate": 3.584e-05,
      "loss": 0.003,
      "step": 42480
    },
    {
      "epoch": 2.2661333333333333,
      "grad_norm": 0.36074841022491455,
      "learning_rate": 3.583666666666667e-05,
      "loss": 0.0034,
      "step": 42490
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.12749485671520233,
      "learning_rate": 3.5833333333333335e-05,
      "loss": 0.003,
      "step": 42500
    },
    {
      "epoch": 2.2672,
      "grad_norm": 0.15188851952552795,
      "learning_rate": 3.583e-05,
      "loss": 0.0029,
      "step": 42510
    },
    {
      "epoch": 2.267733333333333,
      "grad_norm": 0.25779545307159424,
      "learning_rate": 3.582666666666667e-05,
      "loss": 0.004,
      "step": 42520
    },
    {
      "epoch": 2.2682666666666664,
      "grad_norm": 0.239991694688797,
      "learning_rate": 3.582333333333334e-05,
      "loss": 0.003,
      "step": 42530
    },
    {
      "epoch": 2.2688,
      "grad_norm": 0.44258394837379456,
      "learning_rate": 3.5820000000000006e-05,
      "loss": 0.0027,
      "step": 42540
    },
    {
      "epoch": 2.2693333333333334,
      "grad_norm": 0.21952062845230103,
      "learning_rate": 3.581666666666667e-05,
      "loss": 0.003,
      "step": 42550
    },
    {
      "epoch": 2.2698666666666667,
      "grad_norm": 0.47171419858932495,
      "learning_rate": 3.581333333333334e-05,
      "loss": 0.0029,
      "step": 42560
    },
    {
      "epoch": 2.2704,
      "grad_norm": 0.06997092068195343,
      "learning_rate": 3.581e-05,
      "loss": 0.0037,
      "step": 42570
    },
    {
      "epoch": 2.2709333333333332,
      "grad_norm": 0.32618990540504456,
      "learning_rate": 3.5806666666666664e-05,
      "loss": 0.0021,
      "step": 42580
    },
    {
      "epoch": 2.2714666666666665,
      "grad_norm": 0.23932327330112457,
      "learning_rate": 3.580333333333333e-05,
      "loss": 0.0027,
      "step": 42590
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 0.38332417607307434,
      "learning_rate": 3.58e-05,
      "loss": 0.0022,
      "step": 42600
    },
    {
      "epoch": 2.2725333333333335,
      "grad_norm": 0.5360206365585327,
      "learning_rate": 3.579666666666667e-05,
      "loss": 0.0017,
      "step": 42610
    },
    {
      "epoch": 2.273066666666667,
      "grad_norm": 0.20618188381195068,
      "learning_rate": 3.5793333333333335e-05,
      "loss": 0.0028,
      "step": 42620
    },
    {
      "epoch": 2.2736,
      "grad_norm": 0.27027982473373413,
      "learning_rate": 3.579e-05,
      "loss": 0.0023,
      "step": 42630
    },
    {
      "epoch": 2.2741333333333333,
      "grad_norm": 0.5056131482124329,
      "learning_rate": 3.578666666666667e-05,
      "loss": 0.0023,
      "step": 42640
    },
    {
      "epoch": 2.2746666666666666,
      "grad_norm": 0.046781234443187714,
      "learning_rate": 3.5783333333333333e-05,
      "loss": 0.0013,
      "step": 42650
    },
    {
      "epoch": 2.2752,
      "grad_norm": 0.6516932249069214,
      "learning_rate": 3.578e-05,
      "loss": 0.003,
      "step": 42660
    },
    {
      "epoch": 2.275733333333333,
      "grad_norm": 0.3239865303039551,
      "learning_rate": 3.577666666666667e-05,
      "loss": 0.0024,
      "step": 42670
    },
    {
      "epoch": 2.2762666666666664,
      "grad_norm": 0.3244393467903137,
      "learning_rate": 3.577333333333334e-05,
      "loss": 0.0019,
      "step": 42680
    },
    {
      "epoch": 2.2768,
      "grad_norm": 0.38553178310394287,
      "learning_rate": 3.5770000000000005e-05,
      "loss": 0.0019,
      "step": 42690
    },
    {
      "epoch": 2.2773333333333334,
      "grad_norm": 0.3545506000518799,
      "learning_rate": 3.576666666666667e-05,
      "loss": 0.0024,
      "step": 42700
    },
    {
      "epoch": 2.2778666666666667,
      "grad_norm": 0.23603053390979767,
      "learning_rate": 3.576333333333334e-05,
      "loss": 0.0039,
      "step": 42710
    },
    {
      "epoch": 2.2784,
      "grad_norm": 0.1770540177822113,
      "learning_rate": 3.5759999999999996e-05,
      "loss": 0.0024,
      "step": 42720
    },
    {
      "epoch": 2.2789333333333333,
      "grad_norm": 0.18515968322753906,
      "learning_rate": 3.575666666666667e-05,
      "loss": 0.003,
      "step": 42730
    },
    {
      "epoch": 2.2794666666666665,
      "grad_norm": 0.09264443814754486,
      "learning_rate": 3.5753333333333335e-05,
      "loss": 0.002,
      "step": 42740
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.564554750919342,
      "learning_rate": 3.575e-05,
      "loss": 0.0027,
      "step": 42750
    },
    {
      "epoch": 2.2805333333333335,
      "grad_norm": 0.18244707584381104,
      "learning_rate": 3.574666666666667e-05,
      "loss": 0.0029,
      "step": 42760
    },
    {
      "epoch": 2.281066666666667,
      "grad_norm": 0.0659693032503128,
      "learning_rate": 3.5743333333333334e-05,
      "loss": 0.004,
      "step": 42770
    },
    {
      "epoch": 2.2816,
      "grad_norm": 0.34716811776161194,
      "learning_rate": 3.574e-05,
      "loss": 0.0025,
      "step": 42780
    },
    {
      "epoch": 2.2821333333333333,
      "grad_norm": 0.15554426610469818,
      "learning_rate": 3.5736666666666666e-05,
      "loss": 0.0028,
      "step": 42790
    },
    {
      "epoch": 2.2826666666666666,
      "grad_norm": 0.44541826844215393,
      "learning_rate": 3.573333333333333e-05,
      "loss": 0.0039,
      "step": 42800
    },
    {
      "epoch": 2.2832,
      "grad_norm": 0.2984464764595032,
      "learning_rate": 3.5730000000000005e-05,
      "loss": 0.0037,
      "step": 42810
    },
    {
      "epoch": 2.283733333333333,
      "grad_norm": 0.2724313735961914,
      "learning_rate": 3.572666666666667e-05,
      "loss": 0.0029,
      "step": 42820
    },
    {
      "epoch": 2.2842666666666664,
      "grad_norm": 0.030106665566563606,
      "learning_rate": 3.572333333333334e-05,
      "loss": 0.0037,
      "step": 42830
    },
    {
      "epoch": 2.2848,
      "grad_norm": 0.5626253485679626,
      "learning_rate": 3.5720000000000004e-05,
      "loss": 0.0024,
      "step": 42840
    },
    {
      "epoch": 2.2853333333333334,
      "grad_norm": 0.12321457266807556,
      "learning_rate": 3.571666666666667e-05,
      "loss": 0.0037,
      "step": 42850
    },
    {
      "epoch": 2.2858666666666667,
      "grad_norm": 0.2961333990097046,
      "learning_rate": 3.5713333333333336e-05,
      "loss": 0.0026,
      "step": 42860
    },
    {
      "epoch": 2.2864,
      "grad_norm": 0.07865383476018906,
      "learning_rate": 3.571e-05,
      "loss": 0.0034,
      "step": 42870
    },
    {
      "epoch": 2.2869333333333333,
      "grad_norm": 0.12168149650096893,
      "learning_rate": 3.570666666666667e-05,
      "loss": 0.0038,
      "step": 42880
    },
    {
      "epoch": 2.2874666666666665,
      "grad_norm": 0.17951972782611847,
      "learning_rate": 3.5703333333333334e-05,
      "loss": 0.0017,
      "step": 42890
    },
    {
      "epoch": 2.288,
      "grad_norm": 0.09434539824724197,
      "learning_rate": 3.57e-05,
      "loss": 0.0032,
      "step": 42900
    },
    {
      "epoch": 2.2885333333333335,
      "grad_norm": 0.35647621750831604,
      "learning_rate": 3.5696666666666667e-05,
      "loss": 0.0015,
      "step": 42910
    },
    {
      "epoch": 2.289066666666667,
      "grad_norm": 0.3571767508983612,
      "learning_rate": 3.569333333333333e-05,
      "loss": 0.0027,
      "step": 42920
    },
    {
      "epoch": 2.2896,
      "grad_norm": 0.24509719014167786,
      "learning_rate": 3.569e-05,
      "loss": 0.0024,
      "step": 42930
    },
    {
      "epoch": 2.2901333333333334,
      "grad_norm": 0.5942587852478027,
      "learning_rate": 3.5686666666666665e-05,
      "loss": 0.0022,
      "step": 42940
    },
    {
      "epoch": 2.2906666666666666,
      "grad_norm": 0.2858002185821533,
      "learning_rate": 3.568333333333334e-05,
      "loss": 0.0035,
      "step": 42950
    },
    {
      "epoch": 2.2912,
      "grad_norm": 0.06746502220630646,
      "learning_rate": 3.5680000000000004e-05,
      "loss": 0.0022,
      "step": 42960
    },
    {
      "epoch": 2.291733333333333,
      "grad_norm": 0.11961741745471954,
      "learning_rate": 3.567666666666667e-05,
      "loss": 0.0022,
      "step": 42970
    },
    {
      "epoch": 2.2922666666666665,
      "grad_norm": 0.0969201847910881,
      "learning_rate": 3.5673333333333336e-05,
      "loss": 0.0021,
      "step": 42980
    },
    {
      "epoch": 2.2928,
      "grad_norm": 0.41932442784309387,
      "learning_rate": 3.567e-05,
      "loss": 0.0028,
      "step": 42990
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 0.26913338899612427,
      "learning_rate": 3.566666666666667e-05,
      "loss": 0.0031,
      "step": 43000
    },
    {
      "epoch": 2.2938666666666667,
      "grad_norm": 0.24156326055526733,
      "learning_rate": 3.5663333333333335e-05,
      "loss": 0.0027,
      "step": 43010
    },
    {
      "epoch": 2.2944,
      "grad_norm": 0.06942616403102875,
      "learning_rate": 3.566e-05,
      "loss": 0.0022,
      "step": 43020
    },
    {
      "epoch": 2.2949333333333333,
      "grad_norm": 0.336068332195282,
      "learning_rate": 3.565666666666667e-05,
      "loss": 0.0021,
      "step": 43030
    },
    {
      "epoch": 2.2954666666666665,
      "grad_norm": 0.7673672437667847,
      "learning_rate": 3.565333333333333e-05,
      "loss": 0.0036,
      "step": 43040
    },
    {
      "epoch": 2.296,
      "grad_norm": 0.18120552599430084,
      "learning_rate": 3.565e-05,
      "loss": 0.003,
      "step": 43050
    },
    {
      "epoch": 2.2965333333333335,
      "grad_norm": 0.23585635423660278,
      "learning_rate": 3.5646666666666665e-05,
      "loss": 0.0026,
      "step": 43060
    },
    {
      "epoch": 2.297066666666667,
      "grad_norm": 0.09214413911104202,
      "learning_rate": 3.564333333333333e-05,
      "loss": 0.0033,
      "step": 43070
    },
    {
      "epoch": 2.2976,
      "grad_norm": 0.24291543662548065,
      "learning_rate": 3.5640000000000004e-05,
      "loss": 0.0024,
      "step": 43080
    },
    {
      "epoch": 2.2981333333333334,
      "grad_norm": 0.2964542508125305,
      "learning_rate": 3.563666666666667e-05,
      "loss": 0.0029,
      "step": 43090
    },
    {
      "epoch": 2.2986666666666666,
      "grad_norm": 0.12527020275592804,
      "learning_rate": 3.563333333333334e-05,
      "loss": 0.0022,
      "step": 43100
    },
    {
      "epoch": 2.2992,
      "grad_norm": 0.3262409269809723,
      "learning_rate": 3.563e-05,
      "loss": 0.0019,
      "step": 43110
    },
    {
      "epoch": 2.299733333333333,
      "grad_norm": 0.2656228840351105,
      "learning_rate": 3.562666666666667e-05,
      "loss": 0.0027,
      "step": 43120
    },
    {
      "epoch": 2.3002666666666665,
      "grad_norm": 0.06468845158815384,
      "learning_rate": 3.5623333333333335e-05,
      "loss": 0.0025,
      "step": 43130
    },
    {
      "epoch": 2.3008,
      "grad_norm": 0.17953647673130035,
      "learning_rate": 3.562e-05,
      "loss": 0.0024,
      "step": 43140
    },
    {
      "epoch": 2.3013333333333335,
      "grad_norm": 0.2658107578754425,
      "learning_rate": 3.561666666666667e-05,
      "loss": 0.0026,
      "step": 43150
    },
    {
      "epoch": 2.3018666666666667,
      "grad_norm": 0.23768766224384308,
      "learning_rate": 3.561333333333334e-05,
      "loss": 0.0035,
      "step": 43160
    },
    {
      "epoch": 2.3024,
      "grad_norm": 0.35458695888519287,
      "learning_rate": 3.5610000000000006e-05,
      "loss": 0.0035,
      "step": 43170
    },
    {
      "epoch": 2.3029333333333333,
      "grad_norm": 0.5618563890457153,
      "learning_rate": 3.5606666666666666e-05,
      "loss": 0.0016,
      "step": 43180
    },
    {
      "epoch": 2.3034666666666666,
      "grad_norm": 0.12340302765369415,
      "learning_rate": 3.560333333333333e-05,
      "loss": 0.0025,
      "step": 43190
    },
    {
      "epoch": 2.304,
      "grad_norm": 0.2655155658721924,
      "learning_rate": 3.56e-05,
      "loss": 0.0028,
      "step": 43200
    },
    {
      "epoch": 2.3045333333333335,
      "grad_norm": 0.41249963641166687,
      "learning_rate": 3.5596666666666664e-05,
      "loss": 0.003,
      "step": 43210
    },
    {
      "epoch": 2.305066666666667,
      "grad_norm": 0.23785093426704407,
      "learning_rate": 3.559333333333334e-05,
      "loss": 0.0021,
      "step": 43220
    },
    {
      "epoch": 2.3056,
      "grad_norm": 0.1793241947889328,
      "learning_rate": 3.559e-05,
      "loss": 0.0035,
      "step": 43230
    },
    {
      "epoch": 2.3061333333333334,
      "grad_norm": 0.221641406416893,
      "learning_rate": 3.558666666666667e-05,
      "loss": 0.0026,
      "step": 43240
    },
    {
      "epoch": 2.3066666666666666,
      "grad_norm": 0.1780937910079956,
      "learning_rate": 3.5583333333333335e-05,
      "loss": 0.0022,
      "step": 43250
    },
    {
      "epoch": 2.3072,
      "grad_norm": 0.7096734642982483,
      "learning_rate": 3.558e-05,
      "loss": 0.0031,
      "step": 43260
    },
    {
      "epoch": 2.307733333333333,
      "grad_norm": 0.29872676730155945,
      "learning_rate": 3.557666666666667e-05,
      "loss": 0.0022,
      "step": 43270
    },
    {
      "epoch": 2.3082666666666665,
      "grad_norm": 0.20860986411571503,
      "learning_rate": 3.5573333333333334e-05,
      "loss": 0.0027,
      "step": 43280
    },
    {
      "epoch": 2.3088,
      "grad_norm": 0.44593504071235657,
      "learning_rate": 3.557e-05,
      "loss": 0.0026,
      "step": 43290
    },
    {
      "epoch": 2.3093333333333335,
      "grad_norm": 0.157584547996521,
      "learning_rate": 3.556666666666667e-05,
      "loss": 0.004,
      "step": 43300
    },
    {
      "epoch": 2.3098666666666667,
      "grad_norm": 0.17896082997322083,
      "learning_rate": 3.556333333333334e-05,
      "loss": 0.0023,
      "step": 43310
    },
    {
      "epoch": 2.3104,
      "grad_norm": 0.039069607853889465,
      "learning_rate": 3.5560000000000005e-05,
      "loss": 0.0024,
      "step": 43320
    },
    {
      "epoch": 2.3109333333333333,
      "grad_norm": 0.15464343130588531,
      "learning_rate": 3.5556666666666664e-05,
      "loss": 0.0026,
      "step": 43330
    },
    {
      "epoch": 2.3114666666666666,
      "grad_norm": 0.12009400874376297,
      "learning_rate": 3.555333333333333e-05,
      "loss": 0.0027,
      "step": 43340
    },
    {
      "epoch": 2.312,
      "grad_norm": 0.2105148732662201,
      "learning_rate": 3.555e-05,
      "loss": 0.0028,
      "step": 43350
    },
    {
      "epoch": 2.3125333333333336,
      "grad_norm": 0.15036459267139435,
      "learning_rate": 3.554666666666667e-05,
      "loss": 0.002,
      "step": 43360
    },
    {
      "epoch": 2.313066666666667,
      "grad_norm": 0.2699532210826874,
      "learning_rate": 3.5543333333333336e-05,
      "loss": 0.0023,
      "step": 43370
    },
    {
      "epoch": 2.3136,
      "grad_norm": 0.11282751709222794,
      "learning_rate": 3.554e-05,
      "loss": 0.0016,
      "step": 43380
    },
    {
      "epoch": 2.3141333333333334,
      "grad_norm": 0.2620270550251007,
      "learning_rate": 3.553666666666667e-05,
      "loss": 0.0019,
      "step": 43390
    },
    {
      "epoch": 2.3146666666666667,
      "grad_norm": 0.4427412450313568,
      "learning_rate": 3.5533333333333334e-05,
      "loss": 0.0034,
      "step": 43400
    },
    {
      "epoch": 2.3152,
      "grad_norm": 0.2669456899166107,
      "learning_rate": 3.553e-05,
      "loss": 0.0026,
      "step": 43410
    },
    {
      "epoch": 2.315733333333333,
      "grad_norm": 0.15563614666461945,
      "learning_rate": 3.5526666666666666e-05,
      "loss": 0.0031,
      "step": 43420
    },
    {
      "epoch": 2.3162666666666665,
      "grad_norm": 0.15190823376178741,
      "learning_rate": 3.552333333333334e-05,
      "loss": 0.0028,
      "step": 43430
    },
    {
      "epoch": 2.3168,
      "grad_norm": 0.3729434311389923,
      "learning_rate": 3.5520000000000006e-05,
      "loss": 0.0038,
      "step": 43440
    },
    {
      "epoch": 2.3173333333333335,
      "grad_norm": 0.39689525961875916,
      "learning_rate": 3.551666666666667e-05,
      "loss": 0.0019,
      "step": 43450
    },
    {
      "epoch": 2.3178666666666667,
      "grad_norm": 0.12053728103637695,
      "learning_rate": 3.551333333333334e-05,
      "loss": 0.004,
      "step": 43460
    },
    {
      "epoch": 2.3184,
      "grad_norm": 0.09125708788633347,
      "learning_rate": 3.5510000000000004e-05,
      "loss": 0.0025,
      "step": 43470
    },
    {
      "epoch": 2.3189333333333333,
      "grad_norm": 0.12347909063100815,
      "learning_rate": 3.550666666666666e-05,
      "loss": 0.0031,
      "step": 43480
    },
    {
      "epoch": 2.3194666666666666,
      "grad_norm": 0.037892285734415054,
      "learning_rate": 3.5503333333333336e-05,
      "loss": 0.0032,
      "step": 43490
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.38388773798942566,
      "learning_rate": 3.55e-05,
      "loss": 0.0037,
      "step": 43500
    },
    {
      "epoch": 2.3205333333333336,
      "grad_norm": 0.0608166866004467,
      "learning_rate": 3.549666666666667e-05,
      "loss": 0.002,
      "step": 43510
    },
    {
      "epoch": 2.321066666666667,
      "grad_norm": 0.23733483254909515,
      "learning_rate": 3.5493333333333335e-05,
      "loss": 0.0029,
      "step": 43520
    },
    {
      "epoch": 2.3216,
      "grad_norm": 0.2102239578962326,
      "learning_rate": 3.549e-05,
      "loss": 0.0026,
      "step": 43530
    },
    {
      "epoch": 2.3221333333333334,
      "grad_norm": 0.4260464608669281,
      "learning_rate": 3.548666666666667e-05,
      "loss": 0.0028,
      "step": 43540
    },
    {
      "epoch": 2.3226666666666667,
      "grad_norm": 0.2374272644519806,
      "learning_rate": 3.548333333333333e-05,
      "loss": 0.0027,
      "step": 43550
    },
    {
      "epoch": 2.3232,
      "grad_norm": 0.26929640769958496,
      "learning_rate": 3.548e-05,
      "loss": 0.0032,
      "step": 43560
    },
    {
      "epoch": 2.323733333333333,
      "grad_norm": 0.05469908565282822,
      "learning_rate": 3.547666666666667e-05,
      "loss": 0.0043,
      "step": 43570
    },
    {
      "epoch": 2.3242666666666665,
      "grad_norm": 0.357008159160614,
      "learning_rate": 3.547333333333334e-05,
      "loss": 0.0024,
      "step": 43580
    },
    {
      "epoch": 2.3247999999999998,
      "grad_norm": 0.11981252580881119,
      "learning_rate": 3.5470000000000004e-05,
      "loss": 0.003,
      "step": 43590
    },
    {
      "epoch": 2.3253333333333335,
      "grad_norm": 0.15669667720794678,
      "learning_rate": 3.546666666666667e-05,
      "loss": 0.0019,
      "step": 43600
    },
    {
      "epoch": 2.3258666666666667,
      "grad_norm": 0.038564931601285934,
      "learning_rate": 3.5463333333333337e-05,
      "loss": 0.0029,
      "step": 43610
    },
    {
      "epoch": 2.3264,
      "grad_norm": 0.2688193917274475,
      "learning_rate": 3.546e-05,
      "loss": 0.0041,
      "step": 43620
    },
    {
      "epoch": 2.3269333333333333,
      "grad_norm": 0.18414421379566193,
      "learning_rate": 3.545666666666667e-05,
      "loss": 0.0028,
      "step": 43630
    },
    {
      "epoch": 2.3274666666666666,
      "grad_norm": 0.6178911924362183,
      "learning_rate": 3.5453333333333335e-05,
      "loss": 0.0025,
      "step": 43640
    },
    {
      "epoch": 2.328,
      "grad_norm": 0.4450826644897461,
      "learning_rate": 3.545e-05,
      "loss": 0.0026,
      "step": 43650
    },
    {
      "epoch": 2.3285333333333336,
      "grad_norm": 0.12655822932720184,
      "learning_rate": 3.544666666666667e-05,
      "loss": 0.0018,
      "step": 43660
    },
    {
      "epoch": 2.329066666666667,
      "grad_norm": 0.4162488579750061,
      "learning_rate": 3.544333333333333e-05,
      "loss": 0.0023,
      "step": 43670
    },
    {
      "epoch": 2.3296,
      "grad_norm": 0.04934858903288841,
      "learning_rate": 3.544e-05,
      "loss": 0.0023,
      "step": 43680
    },
    {
      "epoch": 2.3301333333333334,
      "grad_norm": 0.44895437359809875,
      "learning_rate": 3.5436666666666666e-05,
      "loss": 0.0027,
      "step": 43690
    },
    {
      "epoch": 2.3306666666666667,
      "grad_norm": 0.2388073354959488,
      "learning_rate": 3.543333333333333e-05,
      "loss": 0.0027,
      "step": 43700
    },
    {
      "epoch": 2.3312,
      "grad_norm": 0.5307276248931885,
      "learning_rate": 3.5430000000000005e-05,
      "loss": 0.0031,
      "step": 43710
    },
    {
      "epoch": 2.331733333333333,
      "grad_norm": 0.32910212874412537,
      "learning_rate": 3.542666666666667e-05,
      "loss": 0.0018,
      "step": 43720
    },
    {
      "epoch": 2.3322666666666665,
      "grad_norm": 0.5364739298820496,
      "learning_rate": 3.542333333333334e-05,
      "loss": 0.0025,
      "step": 43730
    },
    {
      "epoch": 2.3327999999999998,
      "grad_norm": 0.15372690558433533,
      "learning_rate": 3.542e-05,
      "loss": 0.0017,
      "step": 43740
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.2682131826877594,
      "learning_rate": 3.541666666666667e-05,
      "loss": 0.002,
      "step": 43750
    },
    {
      "epoch": 2.3338666666666668,
      "grad_norm": 0.06354214251041412,
      "learning_rate": 3.5413333333333335e-05,
      "loss": 0.0027,
      "step": 43760
    },
    {
      "epoch": 2.3344,
      "grad_norm": 0.3691532015800476,
      "learning_rate": 3.541e-05,
      "loss": 0.0037,
      "step": 43770
    },
    {
      "epoch": 2.3349333333333333,
      "grad_norm": 0.43934109807014465,
      "learning_rate": 3.540666666666667e-05,
      "loss": 0.0039,
      "step": 43780
    },
    {
      "epoch": 2.3354666666666666,
      "grad_norm": 0.29921430349349976,
      "learning_rate": 3.5403333333333334e-05,
      "loss": 0.0032,
      "step": 43790
    },
    {
      "epoch": 2.336,
      "grad_norm": 0.11189562827348709,
      "learning_rate": 3.54e-05,
      "loss": 0.0024,
      "step": 43800
    },
    {
      "epoch": 2.3365333333333336,
      "grad_norm": 0.14778542518615723,
      "learning_rate": 3.5396666666666666e-05,
      "loss": 0.0019,
      "step": 43810
    },
    {
      "epoch": 2.337066666666667,
      "grad_norm": 0.30049094557762146,
      "learning_rate": 3.539333333333333e-05,
      "loss": 0.0023,
      "step": 43820
    },
    {
      "epoch": 2.3376,
      "grad_norm": 0.2886923849582672,
      "learning_rate": 3.539e-05,
      "loss": 0.0027,
      "step": 43830
    },
    {
      "epoch": 2.3381333333333334,
      "grad_norm": 0.12417121231555939,
      "learning_rate": 3.538666666666667e-05,
      "loss": 0.0029,
      "step": 43840
    },
    {
      "epoch": 2.3386666666666667,
      "grad_norm": 0.3609263300895691,
      "learning_rate": 3.538333333333334e-05,
      "loss": 0.0025,
      "step": 43850
    },
    {
      "epoch": 2.3392,
      "grad_norm": 0.3238602578639984,
      "learning_rate": 3.5380000000000003e-05,
      "loss": 0.0029,
      "step": 43860
    },
    {
      "epoch": 2.339733333333333,
      "grad_norm": 0.023577995598316193,
      "learning_rate": 3.537666666666667e-05,
      "loss": 0.0029,
      "step": 43870
    },
    {
      "epoch": 2.3402666666666665,
      "grad_norm": 0.15099793672561646,
      "learning_rate": 3.5373333333333336e-05,
      "loss": 0.003,
      "step": 43880
    },
    {
      "epoch": 2.3407999999999998,
      "grad_norm": 0.12218642234802246,
      "learning_rate": 3.537e-05,
      "loss": 0.0029,
      "step": 43890
    },
    {
      "epoch": 2.3413333333333335,
      "grad_norm": 0.29601114988327026,
      "learning_rate": 3.536666666666667e-05,
      "loss": 0.0028,
      "step": 43900
    },
    {
      "epoch": 2.3418666666666668,
      "grad_norm": 0.22445885837078094,
      "learning_rate": 3.5363333333333334e-05,
      "loss": 0.0022,
      "step": 43910
    },
    {
      "epoch": 2.3424,
      "grad_norm": 0.23790164291858673,
      "learning_rate": 3.536000000000001e-05,
      "loss": 0.0031,
      "step": 43920
    },
    {
      "epoch": 2.3429333333333333,
      "grad_norm": 0.23656803369522095,
      "learning_rate": 3.5356666666666666e-05,
      "loss": 0.0024,
      "step": 43930
    },
    {
      "epoch": 2.3434666666666666,
      "grad_norm": 0.412984162569046,
      "learning_rate": 3.535333333333333e-05,
      "loss": 0.0035,
      "step": 43940
    },
    {
      "epoch": 2.344,
      "grad_norm": 0.17566829919815063,
      "learning_rate": 3.535e-05,
      "loss": 0.003,
      "step": 43950
    },
    {
      "epoch": 2.3445333333333336,
      "grad_norm": 0.6809917688369751,
      "learning_rate": 3.5346666666666665e-05,
      "loss": 0.0031,
      "step": 43960
    },
    {
      "epoch": 2.345066666666667,
      "grad_norm": 0.1493789106607437,
      "learning_rate": 3.534333333333333e-05,
      "loss": 0.0019,
      "step": 43970
    },
    {
      "epoch": 2.3456,
      "grad_norm": 0.44383618235588074,
      "learning_rate": 3.5340000000000004e-05,
      "loss": 0.0013,
      "step": 43980
    },
    {
      "epoch": 2.3461333333333334,
      "grad_norm": 0.3854617774486542,
      "learning_rate": 3.533666666666667e-05,
      "loss": 0.0032,
      "step": 43990
    },
    {
      "epoch": 2.3466666666666667,
      "grad_norm": 0.06461809575557709,
      "learning_rate": 3.5333333333333336e-05,
      "loss": 0.002,
      "step": 44000
    },
    {
      "epoch": 2.3472,
      "grad_norm": 0.1807345747947693,
      "learning_rate": 3.533e-05,
      "loss": 0.0035,
      "step": 44010
    },
    {
      "epoch": 2.3477333333333332,
      "grad_norm": 0.07051663845777512,
      "learning_rate": 3.532666666666667e-05,
      "loss": 0.0024,
      "step": 44020
    },
    {
      "epoch": 2.3482666666666665,
      "grad_norm": 0.09307295829057693,
      "learning_rate": 3.5323333333333335e-05,
      "loss": 0.0027,
      "step": 44030
    },
    {
      "epoch": 2.3487999999999998,
      "grad_norm": 0.042169276624917984,
      "learning_rate": 3.532e-05,
      "loss": 0.0024,
      "step": 44040
    },
    {
      "epoch": 2.3493333333333335,
      "grad_norm": 0.6524714231491089,
      "learning_rate": 3.531666666666667e-05,
      "loss": 0.0038,
      "step": 44050
    },
    {
      "epoch": 2.3498666666666668,
      "grad_norm": 0.21256722509860992,
      "learning_rate": 3.531333333333334e-05,
      "loss": 0.0034,
      "step": 44060
    },
    {
      "epoch": 2.3504,
      "grad_norm": 0.4012366235256195,
      "learning_rate": 3.5310000000000006e-05,
      "loss": 0.003,
      "step": 44070
    },
    {
      "epoch": 2.3509333333333333,
      "grad_norm": 0.08482779562473297,
      "learning_rate": 3.5306666666666665e-05,
      "loss": 0.0028,
      "step": 44080
    },
    {
      "epoch": 2.3514666666666666,
      "grad_norm": 0.12262021005153656,
      "learning_rate": 3.530333333333333e-05,
      "loss": 0.0026,
      "step": 44090
    },
    {
      "epoch": 2.352,
      "grad_norm": 0.12135538458824158,
      "learning_rate": 3.53e-05,
      "loss": 0.0027,
      "step": 44100
    },
    {
      "epoch": 2.352533333333333,
      "grad_norm": 0.47197628021240234,
      "learning_rate": 3.5296666666666664e-05,
      "loss": 0.0025,
      "step": 44110
    },
    {
      "epoch": 2.353066666666667,
      "grad_norm": 0.03969995677471161,
      "learning_rate": 3.5293333333333336e-05,
      "loss": 0.0039,
      "step": 44120
    },
    {
      "epoch": 2.3536,
      "grad_norm": 0.2392062246799469,
      "learning_rate": 3.529e-05,
      "loss": 0.002,
      "step": 44130
    },
    {
      "epoch": 2.3541333333333334,
      "grad_norm": 0.019928311929106712,
      "learning_rate": 3.528666666666667e-05,
      "loss": 0.0021,
      "step": 44140
    },
    {
      "epoch": 2.3546666666666667,
      "grad_norm": 0.15492059290409088,
      "learning_rate": 3.5283333333333335e-05,
      "loss": 0.0019,
      "step": 44150
    },
    {
      "epoch": 2.3552,
      "grad_norm": 0.05674790218472481,
      "learning_rate": 3.528e-05,
      "loss": 0.0018,
      "step": 44160
    },
    {
      "epoch": 2.3557333333333332,
      "grad_norm": 0.41863885521888733,
      "learning_rate": 3.527666666666667e-05,
      "loss": 0.0022,
      "step": 44170
    },
    {
      "epoch": 2.3562666666666665,
      "grad_norm": 0.4201664626598358,
      "learning_rate": 3.527333333333333e-05,
      "loss": 0.0026,
      "step": 44180
    },
    {
      "epoch": 2.3568,
      "grad_norm": 0.17927519977092743,
      "learning_rate": 3.5270000000000006e-05,
      "loss": 0.0029,
      "step": 44190
    },
    {
      "epoch": 2.3573333333333335,
      "grad_norm": 0.3250775635242462,
      "learning_rate": 3.526666666666667e-05,
      "loss": 0.0026,
      "step": 44200
    },
    {
      "epoch": 2.3578666666666668,
      "grad_norm": 0.02527065947651863,
      "learning_rate": 3.526333333333334e-05,
      "loss": 0.0029,
      "step": 44210
    },
    {
      "epoch": 2.3584,
      "grad_norm": 0.05148610100150108,
      "learning_rate": 3.5260000000000005e-05,
      "loss": 0.0026,
      "step": 44220
    },
    {
      "epoch": 2.3589333333333333,
      "grad_norm": 0.46551719307899475,
      "learning_rate": 3.5256666666666664e-05,
      "loss": 0.003,
      "step": 44230
    },
    {
      "epoch": 2.3594666666666666,
      "grad_norm": 0.7401299476623535,
      "learning_rate": 3.525333333333333e-05,
      "loss": 0.0024,
      "step": 44240
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.1540268361568451,
      "learning_rate": 3.525e-05,
      "loss": 0.0025,
      "step": 44250
    },
    {
      "epoch": 2.360533333333333,
      "grad_norm": 0.070946104824543,
      "learning_rate": 3.524666666666667e-05,
      "loss": 0.0015,
      "step": 44260
    },
    {
      "epoch": 2.361066666666667,
      "grad_norm": 0.042865317314863205,
      "learning_rate": 3.5243333333333335e-05,
      "loss": 0.0037,
      "step": 44270
    },
    {
      "epoch": 2.3616,
      "grad_norm": 0.741856038570404,
      "learning_rate": 3.524e-05,
      "loss": 0.0024,
      "step": 44280
    },
    {
      "epoch": 2.3621333333333334,
      "grad_norm": 0.45470306277275085,
      "learning_rate": 3.523666666666667e-05,
      "loss": 0.003,
      "step": 44290
    },
    {
      "epoch": 2.3626666666666667,
      "grad_norm": 0.15342259407043457,
      "learning_rate": 3.5233333333333334e-05,
      "loss": 0.0024,
      "step": 44300
    },
    {
      "epoch": 2.3632,
      "grad_norm": 0.07288706302642822,
      "learning_rate": 3.523e-05,
      "loss": 0.0035,
      "step": 44310
    },
    {
      "epoch": 2.3637333333333332,
      "grad_norm": 0.23509220778942108,
      "learning_rate": 3.5226666666666666e-05,
      "loss": 0.0027,
      "step": 44320
    },
    {
      "epoch": 2.3642666666666665,
      "grad_norm": 0.1799648404121399,
      "learning_rate": 3.522333333333334e-05,
      "loss": 0.002,
      "step": 44330
    },
    {
      "epoch": 2.3648,
      "grad_norm": 0.3761334717273712,
      "learning_rate": 3.5220000000000005e-05,
      "loss": 0.0026,
      "step": 44340
    },
    {
      "epoch": 2.3653333333333335,
      "grad_norm": 0.44300025701522827,
      "learning_rate": 3.521666666666667e-05,
      "loss": 0.002,
      "step": 44350
    },
    {
      "epoch": 2.365866666666667,
      "grad_norm": 0.4082089960575104,
      "learning_rate": 3.521333333333334e-05,
      "loss": 0.0028,
      "step": 44360
    },
    {
      "epoch": 2.3664,
      "grad_norm": 0.0812898725271225,
      "learning_rate": 3.5210000000000003e-05,
      "loss": 0.002,
      "step": 44370
    },
    {
      "epoch": 2.3669333333333333,
      "grad_norm": 0.6324960589408875,
      "learning_rate": 3.520666666666667e-05,
      "loss": 0.0033,
      "step": 44380
    },
    {
      "epoch": 2.3674666666666666,
      "grad_norm": 0.03593657910823822,
      "learning_rate": 3.5203333333333336e-05,
      "loss": 0.0024,
      "step": 44390
    },
    {
      "epoch": 2.368,
      "grad_norm": 0.14976969361305237,
      "learning_rate": 3.52e-05,
      "loss": 0.0023,
      "step": 44400
    },
    {
      "epoch": 2.368533333333333,
      "grad_norm": 0.4728838801383972,
      "learning_rate": 3.519666666666667e-05,
      "loss": 0.0021,
      "step": 44410
    },
    {
      "epoch": 2.369066666666667,
      "grad_norm": 0.05404816195368767,
      "learning_rate": 3.5193333333333334e-05,
      "loss": 0.0043,
      "step": 44420
    },
    {
      "epoch": 2.3696,
      "grad_norm": 0.2980007827281952,
      "learning_rate": 3.519e-05,
      "loss": 0.0025,
      "step": 44430
    },
    {
      "epoch": 2.3701333333333334,
      "grad_norm": 0.12446852773427963,
      "learning_rate": 3.5186666666666666e-05,
      "loss": 0.003,
      "step": 44440
    },
    {
      "epoch": 2.3706666666666667,
      "grad_norm": 0.26478061079978943,
      "learning_rate": 3.518333333333333e-05,
      "loss": 0.0032,
      "step": 44450
    },
    {
      "epoch": 2.3712,
      "grad_norm": 0.06865879148244858,
      "learning_rate": 3.518e-05,
      "loss": 0.0027,
      "step": 44460
    },
    {
      "epoch": 2.3717333333333332,
      "grad_norm": 0.04375947639346123,
      "learning_rate": 3.517666666666667e-05,
      "loss": 0.0025,
      "step": 44470
    },
    {
      "epoch": 2.3722666666666665,
      "grad_norm": 0.1520909070968628,
      "learning_rate": 3.517333333333334e-05,
      "loss": 0.0026,
      "step": 44480
    },
    {
      "epoch": 2.3728,
      "grad_norm": 0.0808444619178772,
      "learning_rate": 3.5170000000000004e-05,
      "loss": 0.0027,
      "step": 44490
    },
    {
      "epoch": 2.3733333333333335,
      "grad_norm": 0.06942743808031082,
      "learning_rate": 3.516666666666667e-05,
      "loss": 0.0023,
      "step": 44500
    },
    {
      "epoch": 2.373866666666667,
      "grad_norm": 0.12269033491611481,
      "learning_rate": 3.5163333333333336e-05,
      "loss": 0.0029,
      "step": 44510
    },
    {
      "epoch": 2.3744,
      "grad_norm": 0.128385528922081,
      "learning_rate": 3.516e-05,
      "loss": 0.0023,
      "step": 44520
    },
    {
      "epoch": 2.3749333333333333,
      "grad_norm": 0.3569243848323822,
      "learning_rate": 3.515666666666667e-05,
      "loss": 0.0021,
      "step": 44530
    },
    {
      "epoch": 2.3754666666666666,
      "grad_norm": 0.044782042503356934,
      "learning_rate": 3.5153333333333334e-05,
      "loss": 0.0026,
      "step": 44540
    },
    {
      "epoch": 2.376,
      "grad_norm": 0.17989614605903625,
      "learning_rate": 3.515e-05,
      "loss": 0.004,
      "step": 44550
    },
    {
      "epoch": 2.376533333333333,
      "grad_norm": 0.23665735125541687,
      "learning_rate": 3.514666666666667e-05,
      "loss": 0.0024,
      "step": 44560
    },
    {
      "epoch": 2.377066666666667,
      "grad_norm": 0.3586716949939728,
      "learning_rate": 3.514333333333333e-05,
      "loss": 0.0029,
      "step": 44570
    },
    {
      "epoch": 2.3776,
      "grad_norm": 0.5311394929885864,
      "learning_rate": 3.514e-05,
      "loss": 0.0022,
      "step": 44580
    },
    {
      "epoch": 2.3781333333333334,
      "grad_norm": 0.2934041917324066,
      "learning_rate": 3.5136666666666665e-05,
      "loss": 0.0029,
      "step": 44590
    },
    {
      "epoch": 2.3786666666666667,
      "grad_norm": 0.6827735304832458,
      "learning_rate": 3.513333333333334e-05,
      "loss": 0.0031,
      "step": 44600
    },
    {
      "epoch": 2.3792,
      "grad_norm": 0.3251441717147827,
      "learning_rate": 3.5130000000000004e-05,
      "loss": 0.0032,
      "step": 44610
    },
    {
      "epoch": 2.3797333333333333,
      "grad_norm": 0.2729361951351166,
      "learning_rate": 3.512666666666667e-05,
      "loss": 0.0029,
      "step": 44620
    },
    {
      "epoch": 2.3802666666666665,
      "grad_norm": 0.03746769204735756,
      "learning_rate": 3.5123333333333336e-05,
      "loss": 0.0028,
      "step": 44630
    },
    {
      "epoch": 2.3808,
      "grad_norm": 0.23787279427051544,
      "learning_rate": 3.512e-05,
      "loss": 0.0021,
      "step": 44640
    },
    {
      "epoch": 2.3813333333333335,
      "grad_norm": 0.23796848952770233,
      "learning_rate": 3.511666666666667e-05,
      "loss": 0.0021,
      "step": 44650
    },
    {
      "epoch": 2.381866666666667,
      "grad_norm": 0.3263959586620331,
      "learning_rate": 3.5113333333333335e-05,
      "loss": 0.0027,
      "step": 44660
    },
    {
      "epoch": 2.3824,
      "grad_norm": 0.2367529720067978,
      "learning_rate": 3.511e-05,
      "loss": 0.0031,
      "step": 44670
    },
    {
      "epoch": 2.3829333333333333,
      "grad_norm": 0.34493759274482727,
      "learning_rate": 3.5106666666666674e-05,
      "loss": 0.0033,
      "step": 44680
    },
    {
      "epoch": 2.3834666666666666,
      "grad_norm": 0.4364943206310272,
      "learning_rate": 3.510333333333333e-05,
      "loss": 0.0029,
      "step": 44690
    },
    {
      "epoch": 2.384,
      "grad_norm": 0.1335282176733017,
      "learning_rate": 3.51e-05,
      "loss": 0.0019,
      "step": 44700
    },
    {
      "epoch": 2.384533333333333,
      "grad_norm": 0.2392166256904602,
      "learning_rate": 3.5096666666666665e-05,
      "loss": 0.0022,
      "step": 44710
    },
    {
      "epoch": 2.385066666666667,
      "grad_norm": 0.04094770550727844,
      "learning_rate": 3.509333333333333e-05,
      "loss": 0.0031,
      "step": 44720
    },
    {
      "epoch": 2.3856,
      "grad_norm": 0.35499483346939087,
      "learning_rate": 3.509e-05,
      "loss": 0.0013,
      "step": 44730
    },
    {
      "epoch": 2.3861333333333334,
      "grad_norm": 0.4184235632419586,
      "learning_rate": 3.508666666666667e-05,
      "loss": 0.0027,
      "step": 44740
    },
    {
      "epoch": 2.3866666666666667,
      "grad_norm": 0.2693416178226471,
      "learning_rate": 3.508333333333334e-05,
      "loss": 0.0016,
      "step": 44750
    },
    {
      "epoch": 2.3872,
      "grad_norm": 0.5073750615119934,
      "learning_rate": 3.508e-05,
      "loss": 0.0021,
      "step": 44760
    },
    {
      "epoch": 2.3877333333333333,
      "grad_norm": 0.39249324798583984,
      "learning_rate": 3.507666666666667e-05,
      "loss": 0.003,
      "step": 44770
    },
    {
      "epoch": 2.3882666666666665,
      "grad_norm": 0.12253788113594055,
      "learning_rate": 3.5073333333333335e-05,
      "loss": 0.0033,
      "step": 44780
    },
    {
      "epoch": 2.3888,
      "grad_norm": 0.5333687663078308,
      "learning_rate": 3.507e-05,
      "loss": 0.0017,
      "step": 44790
    },
    {
      "epoch": 2.389333333333333,
      "grad_norm": 0.24993476271629333,
      "learning_rate": 3.506666666666667e-05,
      "loss": 0.0023,
      "step": 44800
    },
    {
      "epoch": 2.389866666666667,
      "grad_norm": 0.26833575963974,
      "learning_rate": 3.5063333333333334e-05,
      "loss": 0.0022,
      "step": 44810
    },
    {
      "epoch": 2.3904,
      "grad_norm": 0.1519579142332077,
      "learning_rate": 3.5060000000000007e-05,
      "loss": 0.0025,
      "step": 44820
    },
    {
      "epoch": 2.3909333333333334,
      "grad_norm": 0.042617179453372955,
      "learning_rate": 3.505666666666667e-05,
      "loss": 0.0019,
      "step": 44830
    },
    {
      "epoch": 2.3914666666666666,
      "grad_norm": 0.4787948429584503,
      "learning_rate": 3.505333333333333e-05,
      "loss": 0.003,
      "step": 44840
    },
    {
      "epoch": 2.392,
      "grad_norm": 0.04170695319771767,
      "learning_rate": 3.505e-05,
      "loss": 0.0024,
      "step": 44850
    },
    {
      "epoch": 2.392533333333333,
      "grad_norm": 0.033715661615133286,
      "learning_rate": 3.5046666666666664e-05,
      "loss": 0.0024,
      "step": 44860
    },
    {
      "epoch": 2.393066666666667,
      "grad_norm": 0.38667669892311096,
      "learning_rate": 3.504333333333333e-05,
      "loss": 0.0029,
      "step": 44870
    },
    {
      "epoch": 2.3936,
      "grad_norm": 0.306556761264801,
      "learning_rate": 3.504e-05,
      "loss": 0.0033,
      "step": 44880
    },
    {
      "epoch": 2.3941333333333334,
      "grad_norm": 0.2717636227607727,
      "learning_rate": 3.503666666666667e-05,
      "loss": 0.0027,
      "step": 44890
    },
    {
      "epoch": 2.3946666666666667,
      "grad_norm": 0.2282445728778839,
      "learning_rate": 3.5033333333333336e-05,
      "loss": 0.0023,
      "step": 44900
    },
    {
      "epoch": 2.3952,
      "grad_norm": 0.15634916722774506,
      "learning_rate": 3.503e-05,
      "loss": 0.0022,
      "step": 44910
    },
    {
      "epoch": 2.3957333333333333,
      "grad_norm": 0.041564326733350754,
      "learning_rate": 3.502666666666667e-05,
      "loss": 0.004,
      "step": 44920
    },
    {
      "epoch": 2.3962666666666665,
      "grad_norm": 0.10379789769649506,
      "learning_rate": 3.5023333333333334e-05,
      "loss": 0.0035,
      "step": 44930
    },
    {
      "epoch": 2.3968,
      "grad_norm": 0.44434067606925964,
      "learning_rate": 3.502e-05,
      "loss": 0.0019,
      "step": 44940
    },
    {
      "epoch": 2.397333333333333,
      "grad_norm": 0.6535709500312805,
      "learning_rate": 3.501666666666667e-05,
      "loss": 0.0023,
      "step": 44950
    },
    {
      "epoch": 2.397866666666667,
      "grad_norm": 0.037148550152778625,
      "learning_rate": 3.501333333333334e-05,
      "loss": 0.0025,
      "step": 44960
    },
    {
      "epoch": 2.3984,
      "grad_norm": 0.1192818284034729,
      "learning_rate": 3.5010000000000005e-05,
      "loss": 0.0023,
      "step": 44970
    },
    {
      "epoch": 2.3989333333333334,
      "grad_norm": 0.3428669273853302,
      "learning_rate": 3.500666666666667e-05,
      "loss": 0.0024,
      "step": 44980
    },
    {
      "epoch": 2.3994666666666666,
      "grad_norm": 0.4448243975639343,
      "learning_rate": 3.500333333333333e-05,
      "loss": 0.0026,
      "step": 44990
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.5303855538368225,
      "learning_rate": 3.5e-05,
      "loss": 0.0023,
      "step": 45000
    },
    {
      "epoch": 2.400533333333333,
      "grad_norm": 0.24165979027748108,
      "learning_rate": 3.499666666666667e-05,
      "loss": 0.0019,
      "step": 45010
    },
    {
      "epoch": 2.401066666666667,
      "grad_norm": 0.1491544246673584,
      "learning_rate": 3.4993333333333336e-05,
      "loss": 0.0026,
      "step": 45020
    },
    {
      "epoch": 2.4016,
      "grad_norm": 0.5326198935508728,
      "learning_rate": 3.499e-05,
      "loss": 0.0032,
      "step": 45030
    },
    {
      "epoch": 2.4021333333333335,
      "grad_norm": 0.3563496172428131,
      "learning_rate": 3.498666666666667e-05,
      "loss": 0.0019,
      "step": 45040
    },
    {
      "epoch": 2.4026666666666667,
      "grad_norm": 0.3290494680404663,
      "learning_rate": 3.4983333333333334e-05,
      "loss": 0.0025,
      "step": 45050
    },
    {
      "epoch": 2.4032,
      "grad_norm": 0.1842050701379776,
      "learning_rate": 3.498e-05,
      "loss": 0.0022,
      "step": 45060
    },
    {
      "epoch": 2.4037333333333333,
      "grad_norm": 0.05995778366923332,
      "learning_rate": 3.497666666666667e-05,
      "loss": 0.0021,
      "step": 45070
    },
    {
      "epoch": 2.4042666666666666,
      "grad_norm": 0.12124477326869965,
      "learning_rate": 3.497333333333333e-05,
      "loss": 0.0021,
      "step": 45080
    },
    {
      "epoch": 2.4048,
      "grad_norm": 0.022644711658358574,
      "learning_rate": 3.4970000000000006e-05,
      "loss": 0.0026,
      "step": 45090
    },
    {
      "epoch": 2.405333333333333,
      "grad_norm": 0.048315081745386124,
      "learning_rate": 3.496666666666667e-05,
      "loss": 0.0031,
      "step": 45100
    },
    {
      "epoch": 2.405866666666667,
      "grad_norm": 0.29716378450393677,
      "learning_rate": 3.496333333333334e-05,
      "loss": 0.0029,
      "step": 45110
    },
    {
      "epoch": 2.4064,
      "grad_norm": 0.15223054587841034,
      "learning_rate": 3.4960000000000004e-05,
      "loss": 0.0031,
      "step": 45120
    },
    {
      "epoch": 2.4069333333333334,
      "grad_norm": 0.23943254351615906,
      "learning_rate": 3.495666666666667e-05,
      "loss": 0.0039,
      "step": 45130
    },
    {
      "epoch": 2.4074666666666666,
      "grad_norm": 0.23848825693130493,
      "learning_rate": 3.495333333333333e-05,
      "loss": 0.0026,
      "step": 45140
    },
    {
      "epoch": 2.408,
      "grad_norm": 0.5342854857444763,
      "learning_rate": 3.495e-05,
      "loss": 0.0022,
      "step": 45150
    },
    {
      "epoch": 2.408533333333333,
      "grad_norm": 0.1290031522512436,
      "learning_rate": 3.494666666666667e-05,
      "loss": 0.0023,
      "step": 45160
    },
    {
      "epoch": 2.409066666666667,
      "grad_norm": 0.5612722635269165,
      "learning_rate": 3.4943333333333335e-05,
      "loss": 0.0018,
      "step": 45170
    },
    {
      "epoch": 2.4096,
      "grad_norm": 0.35368266701698303,
      "learning_rate": 3.494e-05,
      "loss": 0.0015,
      "step": 45180
    },
    {
      "epoch": 2.4101333333333335,
      "grad_norm": 0.47891321778297424,
      "learning_rate": 3.493666666666667e-05,
      "loss": 0.0026,
      "step": 45190
    },
    {
      "epoch": 2.4106666666666667,
      "grad_norm": 0.2446465641260147,
      "learning_rate": 3.493333333333333e-05,
      "loss": 0.0032,
      "step": 45200
    },
    {
      "epoch": 2.4112,
      "grad_norm": 0.05834006518125534,
      "learning_rate": 3.493e-05,
      "loss": 0.0022,
      "step": 45210
    },
    {
      "epoch": 2.4117333333333333,
      "grad_norm": 0.07867031544446945,
      "learning_rate": 3.4926666666666665e-05,
      "loss": 0.0023,
      "step": 45220
    },
    {
      "epoch": 2.4122666666666666,
      "grad_norm": 0.04637973755598068,
      "learning_rate": 3.492333333333334e-05,
      "loss": 0.0017,
      "step": 45230
    },
    {
      "epoch": 2.4128,
      "grad_norm": 0.5746837854385376,
      "learning_rate": 3.4920000000000004e-05,
      "loss": 0.0024,
      "step": 45240
    },
    {
      "epoch": 2.413333333333333,
      "grad_norm": 0.4448910057544708,
      "learning_rate": 3.491666666666667e-05,
      "loss": 0.0037,
      "step": 45250
    },
    {
      "epoch": 2.413866666666667,
      "grad_norm": 0.06259606033563614,
      "learning_rate": 3.491333333333334e-05,
      "loss": 0.0034,
      "step": 45260
    },
    {
      "epoch": 2.4144,
      "grad_norm": 0.29451414942741394,
      "learning_rate": 3.491e-05,
      "loss": 0.0017,
      "step": 45270
    },
    {
      "epoch": 2.4149333333333334,
      "grad_norm": 0.25360140204429626,
      "learning_rate": 3.490666666666667e-05,
      "loss": 0.0023,
      "step": 45280
    },
    {
      "epoch": 2.4154666666666667,
      "grad_norm": 0.12345439195632935,
      "learning_rate": 3.4903333333333335e-05,
      "loss": 0.0029,
      "step": 45290
    },
    {
      "epoch": 2.416,
      "grad_norm": 0.16321681439876556,
      "learning_rate": 3.49e-05,
      "loss": 0.0029,
      "step": 45300
    },
    {
      "epoch": 2.416533333333333,
      "grad_norm": 0.23676149547100067,
      "learning_rate": 3.489666666666667e-05,
      "loss": 0.0024,
      "step": 45310
    },
    {
      "epoch": 2.4170666666666665,
      "grad_norm": 0.3849226236343384,
      "learning_rate": 3.4893333333333334e-05,
      "loss": 0.0022,
      "step": 45320
    },
    {
      "epoch": 2.4176,
      "grad_norm": 0.15070126950740814,
      "learning_rate": 3.489e-05,
      "loss": 0.0023,
      "step": 45330
    },
    {
      "epoch": 2.4181333333333335,
      "grad_norm": 0.10282628983259201,
      "learning_rate": 3.4886666666666666e-05,
      "loss": 0.0019,
      "step": 45340
    },
    {
      "epoch": 2.4186666666666667,
      "grad_norm": 0.3550461232662201,
      "learning_rate": 3.488333333333333e-05,
      "loss": 0.0028,
      "step": 45350
    },
    {
      "epoch": 2.4192,
      "grad_norm": 0.4611096978187561,
      "learning_rate": 3.4880000000000005e-05,
      "loss": 0.0038,
      "step": 45360
    },
    {
      "epoch": 2.4197333333333333,
      "grad_norm": 0.5065974593162537,
      "learning_rate": 3.487666666666667e-05,
      "loss": 0.0015,
      "step": 45370
    },
    {
      "epoch": 2.4202666666666666,
      "grad_norm": 0.32645976543426514,
      "learning_rate": 3.487333333333334e-05,
      "loss": 0.0033,
      "step": 45380
    },
    {
      "epoch": 2.4208,
      "grad_norm": 0.4145627021789551,
      "learning_rate": 3.487e-05,
      "loss": 0.0029,
      "step": 45390
    },
    {
      "epoch": 2.421333333333333,
      "grad_norm": 0.24634531140327454,
      "learning_rate": 3.486666666666667e-05,
      "loss": 0.0027,
      "step": 45400
    },
    {
      "epoch": 2.421866666666667,
      "grad_norm": 0.4132963716983795,
      "learning_rate": 3.4863333333333336e-05,
      "loss": 0.0025,
      "step": 45410
    },
    {
      "epoch": 2.4224,
      "grad_norm": 0.045881908386945724,
      "learning_rate": 3.486e-05,
      "loss": 0.003,
      "step": 45420
    },
    {
      "epoch": 2.4229333333333334,
      "grad_norm": 0.12290558964014053,
      "learning_rate": 3.485666666666667e-05,
      "loss": 0.0024,
      "step": 45430
    },
    {
      "epoch": 2.4234666666666667,
      "grad_norm": 0.41148197650909424,
      "learning_rate": 3.4853333333333334e-05,
      "loss": 0.0026,
      "step": 45440
    },
    {
      "epoch": 2.424,
      "grad_norm": 0.030881259590387344,
      "learning_rate": 3.485e-05,
      "loss": 0.0022,
      "step": 45450
    },
    {
      "epoch": 2.424533333333333,
      "grad_norm": 0.6621342301368713,
      "learning_rate": 3.4846666666666666e-05,
      "loss": 0.0029,
      "step": 45460
    },
    {
      "epoch": 2.4250666666666665,
      "grad_norm": 0.21730758249759674,
      "learning_rate": 3.484333333333333e-05,
      "loss": 0.0026,
      "step": 45470
    },
    {
      "epoch": 2.4256,
      "grad_norm": 0.11784909665584564,
      "learning_rate": 3.484e-05,
      "loss": 0.0024,
      "step": 45480
    },
    {
      "epoch": 2.4261333333333335,
      "grad_norm": 0.5911895036697388,
      "learning_rate": 3.4836666666666665e-05,
      "loss": 0.0021,
      "step": 45490
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 0.04905145242810249,
      "learning_rate": 3.483333333333334e-05,
      "loss": 0.0023,
      "step": 45500
    },
    {
      "epoch": 2.4272,
      "grad_norm": 0.15563921630382538,
      "learning_rate": 3.4830000000000004e-05,
      "loss": 0.0024,
      "step": 45510
    },
    {
      "epoch": 2.4277333333333333,
      "grad_norm": 0.4421154856681824,
      "learning_rate": 3.482666666666667e-05,
      "loss": 0.0019,
      "step": 45520
    },
    {
      "epoch": 2.4282666666666666,
      "grad_norm": 0.7154667973518372,
      "learning_rate": 3.4823333333333336e-05,
      "loss": 0.002,
      "step": 45530
    },
    {
      "epoch": 2.4288,
      "grad_norm": 0.3741213381290436,
      "learning_rate": 3.482e-05,
      "loss": 0.002,
      "step": 45540
    },
    {
      "epoch": 2.429333333333333,
      "grad_norm": 0.09234727174043655,
      "learning_rate": 3.481666666666667e-05,
      "loss": 0.0018,
      "step": 45550
    },
    {
      "epoch": 2.429866666666667,
      "grad_norm": 0.5935466289520264,
      "learning_rate": 3.4813333333333334e-05,
      "loss": 0.0017,
      "step": 45560
    },
    {
      "epoch": 2.4304,
      "grad_norm": 0.7719458341598511,
      "learning_rate": 3.481e-05,
      "loss": 0.0027,
      "step": 45570
    },
    {
      "epoch": 2.4309333333333334,
      "grad_norm": 0.3567148745059967,
      "learning_rate": 3.480666666666667e-05,
      "loss": 0.002,
      "step": 45580
    },
    {
      "epoch": 2.4314666666666667,
      "grad_norm": 0.3624614477157593,
      "learning_rate": 3.480333333333333e-05,
      "loss": 0.0021,
      "step": 45590
    },
    {
      "epoch": 2.432,
      "grad_norm": 0.08159767091274261,
      "learning_rate": 3.48e-05,
      "loss": 0.0031,
      "step": 45600
    },
    {
      "epoch": 2.432533333333333,
      "grad_norm": 0.32380279898643494,
      "learning_rate": 3.4796666666666665e-05,
      "loss": 0.0023,
      "step": 45610
    },
    {
      "epoch": 2.4330666666666665,
      "grad_norm": 0.03865811228752136,
      "learning_rate": 3.479333333333333e-05,
      "loss": 0.0016,
      "step": 45620
    },
    {
      "epoch": 2.4336,
      "grad_norm": 0.18291029334068298,
      "learning_rate": 3.479e-05,
      "loss": 0.0026,
      "step": 45630
    },
    {
      "epoch": 2.4341333333333335,
      "grad_norm": 0.2732474207878113,
      "learning_rate": 3.478666666666667e-05,
      "loss": 0.0023,
      "step": 45640
    },
    {
      "epoch": 2.4346666666666668,
      "grad_norm": 0.08298767358064651,
      "learning_rate": 3.4783333333333336e-05,
      "loss": 0.002,
      "step": 45650
    },
    {
      "epoch": 2.4352,
      "grad_norm": 0.29855868220329285,
      "learning_rate": 3.478e-05,
      "loss": 0.0015,
      "step": 45660
    },
    {
      "epoch": 2.4357333333333333,
      "grad_norm": 0.12248419970273972,
      "learning_rate": 3.477666666666667e-05,
      "loss": 0.0022,
      "step": 45670
    },
    {
      "epoch": 2.4362666666666666,
      "grad_norm": 0.4494955241680145,
      "learning_rate": 3.4773333333333335e-05,
      "loss": 0.0032,
      "step": 45680
    },
    {
      "epoch": 2.4368,
      "grad_norm": 0.42115911841392517,
      "learning_rate": 3.477e-05,
      "loss": 0.0026,
      "step": 45690
    },
    {
      "epoch": 2.437333333333333,
      "grad_norm": 0.09519725292921066,
      "learning_rate": 3.476666666666667e-05,
      "loss": 0.0018,
      "step": 45700
    },
    {
      "epoch": 2.437866666666667,
      "grad_norm": 0.1764693707227707,
      "learning_rate": 3.476333333333334e-05,
      "loss": 0.0018,
      "step": 45710
    },
    {
      "epoch": 2.4384,
      "grad_norm": 0.06341057270765305,
      "learning_rate": 3.4760000000000006e-05,
      "loss": 0.0021,
      "step": 45720
    },
    {
      "epoch": 2.4389333333333334,
      "grad_norm": 0.03392523527145386,
      "learning_rate": 3.475666666666667e-05,
      "loss": 0.003,
      "step": 45730
    },
    {
      "epoch": 2.4394666666666667,
      "grad_norm": 0.2713678479194641,
      "learning_rate": 3.475333333333334e-05,
      "loss": 0.0044,
      "step": 45740
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.4603412449359894,
      "learning_rate": 3.475e-05,
      "loss": 0.0025,
      "step": 45750
    },
    {
      "epoch": 2.440533333333333,
      "grad_norm": 0.3784325122833252,
      "learning_rate": 3.4746666666666664e-05,
      "loss": 0.0023,
      "step": 45760
    },
    {
      "epoch": 2.4410666666666665,
      "grad_norm": 0.1079215332865715,
      "learning_rate": 3.474333333333334e-05,
      "loss": 0.0032,
      "step": 45770
    },
    {
      "epoch": 2.4416,
      "grad_norm": 0.299675852060318,
      "learning_rate": 3.474e-05,
      "loss": 0.0021,
      "step": 45780
    },
    {
      "epoch": 2.4421333333333335,
      "grad_norm": 0.12422524392604828,
      "learning_rate": 3.473666666666667e-05,
      "loss": 0.0025,
      "step": 45790
    },
    {
      "epoch": 2.4426666666666668,
      "grad_norm": 0.17963135242462158,
      "learning_rate": 3.4733333333333335e-05,
      "loss": 0.0026,
      "step": 45800
    },
    {
      "epoch": 2.4432,
      "grad_norm": 0.3266247510910034,
      "learning_rate": 3.473e-05,
      "loss": 0.002,
      "step": 45810
    },
    {
      "epoch": 2.4437333333333333,
      "grad_norm": 0.03257552906870842,
      "learning_rate": 3.472666666666667e-05,
      "loss": 0.0027,
      "step": 45820
    },
    {
      "epoch": 2.4442666666666666,
      "grad_norm": 0.06212440878152847,
      "learning_rate": 3.4723333333333333e-05,
      "loss": 0.0028,
      "step": 45830
    },
    {
      "epoch": 2.4448,
      "grad_norm": 0.07920913398265839,
      "learning_rate": 3.472e-05,
      "loss": 0.0024,
      "step": 45840
    },
    {
      "epoch": 2.445333333333333,
      "grad_norm": 0.4152486026287079,
      "learning_rate": 3.471666666666667e-05,
      "loss": 0.0021,
      "step": 45850
    },
    {
      "epoch": 2.445866666666667,
      "grad_norm": 0.4215925931930542,
      "learning_rate": 3.471333333333334e-05,
      "loss": 0.0028,
      "step": 45860
    },
    {
      "epoch": 2.4464,
      "grad_norm": 0.03252854943275452,
      "learning_rate": 3.4710000000000005e-05,
      "loss": 0.002,
      "step": 45870
    },
    {
      "epoch": 2.4469333333333334,
      "grad_norm": 0.44722071290016174,
      "learning_rate": 3.470666666666667e-05,
      "loss": 0.0026,
      "step": 45880
    },
    {
      "epoch": 2.4474666666666667,
      "grad_norm": 0.22361703217029572,
      "learning_rate": 3.470333333333334e-05,
      "loss": 0.0021,
      "step": 45890
    },
    {
      "epoch": 2.448,
      "grad_norm": 0.1187492236495018,
      "learning_rate": 3.4699999999999996e-05,
      "loss": 0.0029,
      "step": 45900
    },
    {
      "epoch": 2.4485333333333332,
      "grad_norm": 0.24556808173656464,
      "learning_rate": 3.469666666666667e-05,
      "loss": 0.0026,
      "step": 45910
    },
    {
      "epoch": 2.4490666666666665,
      "grad_norm": 0.04405110701918602,
      "learning_rate": 3.4693333333333335e-05,
      "loss": 0.0026,
      "step": 45920
    },
    {
      "epoch": 2.4496,
      "grad_norm": 0.10260522365570068,
      "learning_rate": 3.469e-05,
      "loss": 0.0028,
      "step": 45930
    },
    {
      "epoch": 2.4501333333333335,
      "grad_norm": 0.27130410075187683,
      "learning_rate": 3.468666666666667e-05,
      "loss": 0.0028,
      "step": 45940
    },
    {
      "epoch": 2.4506666666666668,
      "grad_norm": 0.04754278063774109,
      "learning_rate": 3.4683333333333334e-05,
      "loss": 0.002,
      "step": 45950
    },
    {
      "epoch": 2.4512,
      "grad_norm": 0.06259287148714066,
      "learning_rate": 3.468e-05,
      "loss": 0.0028,
      "step": 45960
    },
    {
      "epoch": 2.4517333333333333,
      "grad_norm": 0.20874349772930145,
      "learning_rate": 3.4676666666666666e-05,
      "loss": 0.0025,
      "step": 45970
    },
    {
      "epoch": 2.4522666666666666,
      "grad_norm": 0.0700637474656105,
      "learning_rate": 3.467333333333333e-05,
      "loss": 0.0025,
      "step": 45980
    },
    {
      "epoch": 2.4528,
      "grad_norm": 0.26946115493774414,
      "learning_rate": 3.4670000000000005e-05,
      "loss": 0.0032,
      "step": 45990
    },
    {
      "epoch": 2.453333333333333,
      "grad_norm": 0.208947092294693,
      "learning_rate": 3.466666666666667e-05,
      "loss": 0.0019,
      "step": 46000
    },
    {
      "epoch": 2.4538666666666664,
      "grad_norm": 0.5961077213287354,
      "learning_rate": 3.466333333333334e-05,
      "loss": 0.0028,
      "step": 46010
    },
    {
      "epoch": 2.4544,
      "grad_norm": 0.050927042961120605,
      "learning_rate": 3.4660000000000004e-05,
      "loss": 0.0019,
      "step": 46020
    },
    {
      "epoch": 2.4549333333333334,
      "grad_norm": 0.5362645983695984,
      "learning_rate": 3.465666666666667e-05,
      "loss": 0.0023,
      "step": 46030
    },
    {
      "epoch": 2.4554666666666667,
      "grad_norm": 0.3110427260398865,
      "learning_rate": 3.4653333333333336e-05,
      "loss": 0.0023,
      "step": 46040
    },
    {
      "epoch": 2.456,
      "grad_norm": 0.7313103079795837,
      "learning_rate": 3.465e-05,
      "loss": 0.0019,
      "step": 46050
    },
    {
      "epoch": 2.4565333333333332,
      "grad_norm": 0.13651429116725922,
      "learning_rate": 3.464666666666667e-05,
      "loss": 0.0025,
      "step": 46060
    },
    {
      "epoch": 2.4570666666666665,
      "grad_norm": 0.1447342485189438,
      "learning_rate": 3.4643333333333334e-05,
      "loss": 0.0029,
      "step": 46070
    },
    {
      "epoch": 2.4576000000000002,
      "grad_norm": 0.07685266435146332,
      "learning_rate": 3.464e-05,
      "loss": 0.0026,
      "step": 46080
    },
    {
      "epoch": 2.4581333333333335,
      "grad_norm": 0.12519478797912598,
      "learning_rate": 3.4636666666666667e-05,
      "loss": 0.0018,
      "step": 46090
    },
    {
      "epoch": 2.458666666666667,
      "grad_norm": 0.045892730355262756,
      "learning_rate": 3.463333333333333e-05,
      "loss": 0.0021,
      "step": 46100
    },
    {
      "epoch": 2.4592,
      "grad_norm": 0.035386402159929276,
      "learning_rate": 3.463e-05,
      "loss": 0.003,
      "step": 46110
    },
    {
      "epoch": 2.4597333333333333,
      "grad_norm": 0.17859268188476562,
      "learning_rate": 3.462666666666667e-05,
      "loss": 0.0027,
      "step": 46120
    },
    {
      "epoch": 2.4602666666666666,
      "grad_norm": 0.25759458541870117,
      "learning_rate": 3.462333333333334e-05,
      "loss": 0.0023,
      "step": 46130
    },
    {
      "epoch": 2.4608,
      "grad_norm": 0.20543991029262543,
      "learning_rate": 3.4620000000000004e-05,
      "loss": 0.0044,
      "step": 46140
    },
    {
      "epoch": 2.461333333333333,
      "grad_norm": 0.3307052254676819,
      "learning_rate": 3.461666666666667e-05,
      "loss": 0.0018,
      "step": 46150
    },
    {
      "epoch": 2.4618666666666664,
      "grad_norm": 0.039824724197387695,
      "learning_rate": 3.4613333333333336e-05,
      "loss": 0.0019,
      "step": 46160
    },
    {
      "epoch": 2.4624,
      "grad_norm": 0.36057835817337036,
      "learning_rate": 3.461e-05,
      "loss": 0.0019,
      "step": 46170
    },
    {
      "epoch": 2.4629333333333334,
      "grad_norm": 0.12338385730981827,
      "learning_rate": 3.460666666666667e-05,
      "loss": 0.0023,
      "step": 46180
    },
    {
      "epoch": 2.4634666666666667,
      "grad_norm": 0.09565604478120804,
      "learning_rate": 3.4603333333333335e-05,
      "loss": 0.0029,
      "step": 46190
    },
    {
      "epoch": 2.464,
      "grad_norm": 0.06946100294589996,
      "learning_rate": 3.46e-05,
      "loss": 0.0039,
      "step": 46200
    },
    {
      "epoch": 2.4645333333333332,
      "grad_norm": 0.15207567811012268,
      "learning_rate": 3.459666666666667e-05,
      "loss": 0.0029,
      "step": 46210
    },
    {
      "epoch": 2.4650666666666665,
      "grad_norm": 0.18581630289554596,
      "learning_rate": 3.459333333333333e-05,
      "loss": 0.0022,
      "step": 46220
    },
    {
      "epoch": 2.4656000000000002,
      "grad_norm": 0.17821833491325378,
      "learning_rate": 3.459e-05,
      "loss": 0.0025,
      "step": 46230
    },
    {
      "epoch": 2.4661333333333335,
      "grad_norm": 0.27393093705177307,
      "learning_rate": 3.4586666666666665e-05,
      "loss": 0.0017,
      "step": 46240
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 0.2452746033668518,
      "learning_rate": 3.458333333333333e-05,
      "loss": 0.0018,
      "step": 46250
    },
    {
      "epoch": 2.4672,
      "grad_norm": 0.09886838495731354,
      "learning_rate": 3.4580000000000004e-05,
      "loss": 0.0017,
      "step": 46260
    },
    {
      "epoch": 2.4677333333333333,
      "grad_norm": 0.10213307291269302,
      "learning_rate": 3.457666666666667e-05,
      "loss": 0.0023,
      "step": 46270
    },
    {
      "epoch": 2.4682666666666666,
      "grad_norm": 0.044124286621809006,
      "learning_rate": 3.4573333333333337e-05,
      "loss": 0.0021,
      "step": 46280
    },
    {
      "epoch": 2.4688,
      "grad_norm": 0.1905427873134613,
      "learning_rate": 3.457e-05,
      "loss": 0.003,
      "step": 46290
    },
    {
      "epoch": 2.469333333333333,
      "grad_norm": 0.08186431229114532,
      "learning_rate": 3.456666666666667e-05,
      "loss": 0.0027,
      "step": 46300
    },
    {
      "epoch": 2.4698666666666664,
      "grad_norm": 0.2746680974960327,
      "learning_rate": 3.4563333333333335e-05,
      "loss": 0.0017,
      "step": 46310
    },
    {
      "epoch": 2.4704,
      "grad_norm": 0.6754391193389893,
      "learning_rate": 3.456e-05,
      "loss": 0.0022,
      "step": 46320
    },
    {
      "epoch": 2.4709333333333334,
      "grad_norm": 0.31612545251846313,
      "learning_rate": 3.455666666666667e-05,
      "loss": 0.0024,
      "step": 46330
    },
    {
      "epoch": 2.4714666666666667,
      "grad_norm": 0.20801562070846558,
      "learning_rate": 3.455333333333334e-05,
      "loss": 0.0024,
      "step": 46340
    },
    {
      "epoch": 2.472,
      "grad_norm": 0.3673346936702728,
      "learning_rate": 3.455e-05,
      "loss": 0.0027,
      "step": 46350
    },
    {
      "epoch": 2.4725333333333332,
      "grad_norm": 0.33000054955482483,
      "learning_rate": 3.4546666666666666e-05,
      "loss": 0.0021,
      "step": 46360
    },
    {
      "epoch": 2.4730666666666665,
      "grad_norm": 0.18151529133319855,
      "learning_rate": 3.454333333333333e-05,
      "loss": 0.0016,
      "step": 46370
    },
    {
      "epoch": 2.4736000000000002,
      "grad_norm": 0.21490232646465302,
      "learning_rate": 3.454e-05,
      "loss": 0.0021,
      "step": 46380
    },
    {
      "epoch": 2.4741333333333335,
      "grad_norm": 0.36199331283569336,
      "learning_rate": 3.4536666666666664e-05,
      "loss": 0.0018,
      "step": 46390
    },
    {
      "epoch": 2.474666666666667,
      "grad_norm": 0.09257511049509048,
      "learning_rate": 3.453333333333334e-05,
      "loss": 0.002,
      "step": 46400
    },
    {
      "epoch": 2.4752,
      "grad_norm": 0.10474196076393127,
      "learning_rate": 3.453e-05,
      "loss": 0.0023,
      "step": 46410
    },
    {
      "epoch": 2.4757333333333333,
      "grad_norm": 0.18207991123199463,
      "learning_rate": 3.452666666666667e-05,
      "loss": 0.0025,
      "step": 46420
    },
    {
      "epoch": 2.4762666666666666,
      "grad_norm": 0.44895586371421814,
      "learning_rate": 3.4523333333333335e-05,
      "loss": 0.0022,
      "step": 46430
    },
    {
      "epoch": 2.4768,
      "grad_norm": 0.09197922796010971,
      "learning_rate": 3.452e-05,
      "loss": 0.0023,
      "step": 46440
    },
    {
      "epoch": 2.477333333333333,
      "grad_norm": 0.25174087285995483,
      "learning_rate": 3.451666666666667e-05,
      "loss": 0.0033,
      "step": 46450
    },
    {
      "epoch": 2.4778666666666664,
      "grad_norm": 0.6602082848548889,
      "learning_rate": 3.4513333333333334e-05,
      "loss": 0.0029,
      "step": 46460
    },
    {
      "epoch": 2.4784,
      "grad_norm": 0.38902440667152405,
      "learning_rate": 3.451000000000001e-05,
      "loss": 0.003,
      "step": 46470
    },
    {
      "epoch": 2.4789333333333334,
      "grad_norm": 0.210113987326622,
      "learning_rate": 3.450666666666667e-05,
      "loss": 0.0028,
      "step": 46480
    },
    {
      "epoch": 2.4794666666666667,
      "grad_norm": 0.08472887426614761,
      "learning_rate": 3.450333333333334e-05,
      "loss": 0.0026,
      "step": 46490
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.2172788828611374,
      "learning_rate": 3.45e-05,
      "loss": 0.0024,
      "step": 46500
    },
    {
      "epoch": 2.4805333333333333,
      "grad_norm": 0.057487860321998596,
      "learning_rate": 3.4496666666666664e-05,
      "loss": 0.0015,
      "step": 46510
    },
    {
      "epoch": 2.4810666666666665,
      "grad_norm": 0.17950339615345,
      "learning_rate": 3.449333333333333e-05,
      "loss": 0.0024,
      "step": 46520
    },
    {
      "epoch": 2.4816,
      "grad_norm": 0.16868461668491364,
      "learning_rate": 3.449e-05,
      "loss": 0.0026,
      "step": 46530
    },
    {
      "epoch": 2.4821333333333335,
      "grad_norm": 0.2156909853219986,
      "learning_rate": 3.448666666666667e-05,
      "loss": 0.0015,
      "step": 46540
    },
    {
      "epoch": 2.482666666666667,
      "grad_norm": 0.07080790400505066,
      "learning_rate": 3.4483333333333336e-05,
      "loss": 0.0027,
      "step": 46550
    },
    {
      "epoch": 2.4832,
      "grad_norm": 0.26653483510017395,
      "learning_rate": 3.448e-05,
      "loss": 0.002,
      "step": 46560
    },
    {
      "epoch": 2.4837333333333333,
      "grad_norm": 0.13574671745300293,
      "learning_rate": 3.447666666666667e-05,
      "loss": 0.0031,
      "step": 46570
    },
    {
      "epoch": 2.4842666666666666,
      "grad_norm": 0.37052229046821594,
      "learning_rate": 3.4473333333333334e-05,
      "loss": 0.0022,
      "step": 46580
    },
    {
      "epoch": 2.4848,
      "grad_norm": 0.344334214925766,
      "learning_rate": 3.447e-05,
      "loss": 0.0031,
      "step": 46590
    },
    {
      "epoch": 2.485333333333333,
      "grad_norm": 0.053939394652843475,
      "learning_rate": 3.4466666666666666e-05,
      "loss": 0.003,
      "step": 46600
    },
    {
      "epoch": 2.4858666666666664,
      "grad_norm": 0.36237627267837524,
      "learning_rate": 3.446333333333334e-05,
      "loss": 0.0026,
      "step": 46610
    },
    {
      "epoch": 2.4864,
      "grad_norm": 0.31262776255607605,
      "learning_rate": 3.4460000000000005e-05,
      "loss": 0.0027,
      "step": 46620
    },
    {
      "epoch": 2.4869333333333334,
      "grad_norm": 0.24748922884464264,
      "learning_rate": 3.445666666666667e-05,
      "loss": 0.0028,
      "step": 46630
    },
    {
      "epoch": 2.4874666666666667,
      "grad_norm": 0.1793956309556961,
      "learning_rate": 3.445333333333334e-05,
      "loss": 0.0028,
      "step": 46640
    },
    {
      "epoch": 2.488,
      "grad_norm": 0.1896449476480484,
      "learning_rate": 3.445e-05,
      "loss": 0.0025,
      "step": 46650
    },
    {
      "epoch": 2.4885333333333333,
      "grad_norm": 0.35994869470596313,
      "learning_rate": 3.444666666666666e-05,
      "loss": 0.0026,
      "step": 46660
    },
    {
      "epoch": 2.4890666666666665,
      "grad_norm": 0.0954030230641365,
      "learning_rate": 3.4443333333333336e-05,
      "loss": 0.0019,
      "step": 46670
    },
    {
      "epoch": 2.4896,
      "grad_norm": 0.24104052782058716,
      "learning_rate": 3.444e-05,
      "loss": 0.0024,
      "step": 46680
    },
    {
      "epoch": 2.4901333333333335,
      "grad_norm": 0.03457169234752655,
      "learning_rate": 3.443666666666667e-05,
      "loss": 0.0025,
      "step": 46690
    },
    {
      "epoch": 2.490666666666667,
      "grad_norm": 0.09368520975112915,
      "learning_rate": 3.4433333333333335e-05,
      "loss": 0.0023,
      "step": 46700
    },
    {
      "epoch": 2.4912,
      "grad_norm": 0.269905149936676,
      "learning_rate": 3.443e-05,
      "loss": 0.0024,
      "step": 46710
    },
    {
      "epoch": 2.4917333333333334,
      "grad_norm": 0.07908332347869873,
      "learning_rate": 3.442666666666667e-05,
      "loss": 0.0028,
      "step": 46720
    },
    {
      "epoch": 2.4922666666666666,
      "grad_norm": 0.5062792301177979,
      "learning_rate": 3.442333333333333e-05,
      "loss": 0.0025,
      "step": 46730
    },
    {
      "epoch": 2.4928,
      "grad_norm": 0.10906761884689331,
      "learning_rate": 3.442e-05,
      "loss": 0.0021,
      "step": 46740
    },
    {
      "epoch": 2.493333333333333,
      "grad_norm": 0.08001066744327545,
      "learning_rate": 3.441666666666667e-05,
      "loss": 0.003,
      "step": 46750
    },
    {
      "epoch": 2.4938666666666665,
      "grad_norm": 0.41869112849235535,
      "learning_rate": 3.441333333333334e-05,
      "loss": 0.0028,
      "step": 46760
    },
    {
      "epoch": 2.4944,
      "grad_norm": 0.04708859696984291,
      "learning_rate": 3.4410000000000004e-05,
      "loss": 0.0025,
      "step": 46770
    },
    {
      "epoch": 2.4949333333333334,
      "grad_norm": 0.20666268467903137,
      "learning_rate": 3.440666666666667e-05,
      "loss": 0.0027,
      "step": 46780
    },
    {
      "epoch": 2.4954666666666667,
      "grad_norm": 0.37112584710121155,
      "learning_rate": 3.4403333333333337e-05,
      "loss": 0.0022,
      "step": 46790
    },
    {
      "epoch": 2.496,
      "grad_norm": 0.10004336386919022,
      "learning_rate": 3.4399999999999996e-05,
      "loss": 0.0027,
      "step": 46800
    },
    {
      "epoch": 2.4965333333333333,
      "grad_norm": 0.12780249118804932,
      "learning_rate": 3.439666666666667e-05,
      "loss": 0.003,
      "step": 46810
    },
    {
      "epoch": 2.4970666666666665,
      "grad_norm": 0.09852530807256699,
      "learning_rate": 3.4393333333333335e-05,
      "loss": 0.0026,
      "step": 46820
    },
    {
      "epoch": 2.4976,
      "grad_norm": 0.266574889421463,
      "learning_rate": 3.439e-05,
      "loss": 0.0031,
      "step": 46830
    },
    {
      "epoch": 2.4981333333333335,
      "grad_norm": 0.14224831759929657,
      "learning_rate": 3.438666666666667e-05,
      "loss": 0.0025,
      "step": 46840
    },
    {
      "epoch": 2.498666666666667,
      "grad_norm": 0.28949132561683655,
      "learning_rate": 3.438333333333333e-05,
      "loss": 0.0031,
      "step": 46850
    },
    {
      "epoch": 2.4992,
      "grad_norm": 0.308321088552475,
      "learning_rate": 3.438e-05,
      "loss": 0.0036,
      "step": 46860
    },
    {
      "epoch": 2.4997333333333334,
      "grad_norm": 0.09781038761138916,
      "learning_rate": 3.4376666666666666e-05,
      "loss": 0.0024,
      "step": 46870
    },
    {
      "epoch": 2.5002666666666666,
      "grad_norm": 0.09574681520462036,
      "learning_rate": 3.437333333333334e-05,
      "loss": 0.003,
      "step": 46880
    },
    {
      "epoch": 2.5008,
      "grad_norm": 0.33656400442123413,
      "learning_rate": 3.4370000000000005e-05,
      "loss": 0.0025,
      "step": 46890
    },
    {
      "epoch": 2.501333333333333,
      "grad_norm": 0.3260124921798706,
      "learning_rate": 3.436666666666667e-05,
      "loss": 0.0027,
      "step": 46900
    },
    {
      "epoch": 2.5018666666666665,
      "grad_norm": 0.4179127812385559,
      "learning_rate": 3.436333333333334e-05,
      "loss": 0.0024,
      "step": 46910
    },
    {
      "epoch": 2.5023999999999997,
      "grad_norm": 0.09965149313211441,
      "learning_rate": 3.436e-05,
      "loss": 0.0028,
      "step": 46920
    },
    {
      "epoch": 2.5029333333333335,
      "grad_norm": 0.2450246959924698,
      "learning_rate": 3.435666666666667e-05,
      "loss": 0.0019,
      "step": 46930
    },
    {
      "epoch": 2.5034666666666667,
      "grad_norm": 0.47811901569366455,
      "learning_rate": 3.4353333333333335e-05,
      "loss": 0.003,
      "step": 46940
    },
    {
      "epoch": 2.504,
      "grad_norm": 0.09512311965227127,
      "learning_rate": 3.435e-05,
      "loss": 0.0025,
      "step": 46950
    },
    {
      "epoch": 2.5045333333333333,
      "grad_norm": 0.1978868991136551,
      "learning_rate": 3.434666666666667e-05,
      "loss": 0.0033,
      "step": 46960
    },
    {
      "epoch": 2.5050666666666666,
      "grad_norm": 0.07312096655368805,
      "learning_rate": 3.4343333333333334e-05,
      "loss": 0.0026,
      "step": 46970
    },
    {
      "epoch": 2.5056000000000003,
      "grad_norm": 0.16742892563343048,
      "learning_rate": 3.434e-05,
      "loss": 0.0023,
      "step": 46980
    },
    {
      "epoch": 2.5061333333333335,
      "grad_norm": 0.25037822127342224,
      "learning_rate": 3.4336666666666666e-05,
      "loss": 0.0033,
      "step": 46990
    },
    {
      "epoch": 2.506666666666667,
      "grad_norm": 0.22400708496570587,
      "learning_rate": 3.433333333333333e-05,
      "loss": 0.0024,
      "step": 47000
    },
    {
      "epoch": 2.5072,
      "grad_norm": 0.3066938817501068,
      "learning_rate": 3.433e-05,
      "loss": 0.0034,
      "step": 47010
    },
    {
      "epoch": 2.5077333333333334,
      "grad_norm": 0.09702402353286743,
      "learning_rate": 3.432666666666667e-05,
      "loss": 0.0039,
      "step": 47020
    },
    {
      "epoch": 2.5082666666666666,
      "grad_norm": 0.13442830741405487,
      "learning_rate": 3.432333333333334e-05,
      "loss": 0.0037,
      "step": 47030
    },
    {
      "epoch": 2.5088,
      "grad_norm": 0.25098076462745667,
      "learning_rate": 3.4320000000000003e-05,
      "loss": 0.0022,
      "step": 47040
    },
    {
      "epoch": 2.509333333333333,
      "grad_norm": 0.24476280808448792,
      "learning_rate": 3.431666666666667e-05,
      "loss": 0.0023,
      "step": 47050
    },
    {
      "epoch": 2.5098666666666665,
      "grad_norm": 0.12533050775527954,
      "learning_rate": 3.4313333333333336e-05,
      "loss": 0.0024,
      "step": 47060
    },
    {
      "epoch": 2.5103999999999997,
      "grad_norm": 0.30596861243247986,
      "learning_rate": 3.431e-05,
      "loss": 0.0026,
      "step": 47070
    },
    {
      "epoch": 2.5109333333333335,
      "grad_norm": 0.1275136023759842,
      "learning_rate": 3.430666666666667e-05,
      "loss": 0.0044,
      "step": 47080
    },
    {
      "epoch": 2.5114666666666667,
      "grad_norm": 0.18819637596607208,
      "learning_rate": 3.4303333333333334e-05,
      "loss": 0.0028,
      "step": 47090
    },
    {
      "epoch": 2.512,
      "grad_norm": 0.24048812687397003,
      "learning_rate": 3.430000000000001e-05,
      "loss": 0.0022,
      "step": 47100
    },
    {
      "epoch": 2.5125333333333333,
      "grad_norm": 0.051761649549007416,
      "learning_rate": 3.4296666666666666e-05,
      "loss": 0.002,
      "step": 47110
    },
    {
      "epoch": 2.5130666666666666,
      "grad_norm": 0.30045220255851746,
      "learning_rate": 3.429333333333333e-05,
      "loss": 0.0026,
      "step": 47120
    },
    {
      "epoch": 2.5136,
      "grad_norm": 0.22995351254940033,
      "learning_rate": 3.429e-05,
      "loss": 0.0038,
      "step": 47130
    },
    {
      "epoch": 2.5141333333333336,
      "grad_norm": 0.36178356409072876,
      "learning_rate": 3.4286666666666665e-05,
      "loss": 0.002,
      "step": 47140
    },
    {
      "epoch": 2.514666666666667,
      "grad_norm": 0.20912444591522217,
      "learning_rate": 3.428333333333333e-05,
      "loss": 0.0021,
      "step": 47150
    },
    {
      "epoch": 2.5152,
      "grad_norm": 0.1518046110868454,
      "learning_rate": 3.4280000000000004e-05,
      "loss": 0.0024,
      "step": 47160
    },
    {
      "epoch": 2.5157333333333334,
      "grad_norm": 0.6004823446273804,
      "learning_rate": 3.427666666666667e-05,
      "loss": 0.0029,
      "step": 47170
    },
    {
      "epoch": 2.5162666666666667,
      "grad_norm": 0.09538085013628006,
      "learning_rate": 3.4273333333333336e-05,
      "loss": 0.0014,
      "step": 47180
    },
    {
      "epoch": 2.5168,
      "grad_norm": 0.09438193589448929,
      "learning_rate": 3.427e-05,
      "loss": 0.0034,
      "step": 47190
    },
    {
      "epoch": 2.517333333333333,
      "grad_norm": 0.4167676568031311,
      "learning_rate": 3.426666666666667e-05,
      "loss": 0.0028,
      "step": 47200
    },
    {
      "epoch": 2.5178666666666665,
      "grad_norm": 0.24054557085037231,
      "learning_rate": 3.4263333333333334e-05,
      "loss": 0.0018,
      "step": 47210
    },
    {
      "epoch": 2.5183999999999997,
      "grad_norm": 0.17701250314712524,
      "learning_rate": 3.426e-05,
      "loss": 0.0023,
      "step": 47220
    },
    {
      "epoch": 2.5189333333333335,
      "grad_norm": 0.1277313083410263,
      "learning_rate": 3.4256666666666674e-05,
      "loss": 0.002,
      "step": 47230
    },
    {
      "epoch": 2.5194666666666667,
      "grad_norm": 0.18009549379348755,
      "learning_rate": 3.425333333333334e-05,
      "loss": 0.0029,
      "step": 47240
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.06898169219493866,
      "learning_rate": 3.4250000000000006e-05,
      "loss": 0.0023,
      "step": 47250
    },
    {
      "epoch": 2.5205333333333333,
      "grad_norm": 0.3301001787185669,
      "learning_rate": 3.4246666666666665e-05,
      "loss": 0.0037,
      "step": 47260
    },
    {
      "epoch": 2.5210666666666666,
      "grad_norm": 0.16304118931293488,
      "learning_rate": 3.424333333333333e-05,
      "loss": 0.0017,
      "step": 47270
    },
    {
      "epoch": 2.5216,
      "grad_norm": 0.2528479993343353,
      "learning_rate": 3.424e-05,
      "loss": 0.0037,
      "step": 47280
    },
    {
      "epoch": 2.5221333333333336,
      "grad_norm": 0.03251872956752777,
      "learning_rate": 3.4236666666666664e-05,
      "loss": 0.0023,
      "step": 47290
    },
    {
      "epoch": 2.522666666666667,
      "grad_norm": 0.09542731940746307,
      "learning_rate": 3.4233333333333336e-05,
      "loss": 0.0025,
      "step": 47300
    },
    {
      "epoch": 2.5232,
      "grad_norm": 0.15564469993114471,
      "learning_rate": 3.423e-05,
      "loss": 0.0027,
      "step": 47310
    },
    {
      "epoch": 2.5237333333333334,
      "grad_norm": 0.21377187967300415,
      "learning_rate": 3.422666666666667e-05,
      "loss": 0.0036,
      "step": 47320
    },
    {
      "epoch": 2.5242666666666667,
      "grad_norm": 0.2763386070728302,
      "learning_rate": 3.4223333333333335e-05,
      "loss": 0.0018,
      "step": 47330
    },
    {
      "epoch": 2.5248,
      "grad_norm": 0.09980782121419907,
      "learning_rate": 3.422e-05,
      "loss": 0.0018,
      "step": 47340
    },
    {
      "epoch": 2.525333333333333,
      "grad_norm": 0.5650920271873474,
      "learning_rate": 3.421666666666667e-05,
      "loss": 0.0027,
      "step": 47350
    },
    {
      "epoch": 2.5258666666666665,
      "grad_norm": 0.07423639297485352,
      "learning_rate": 3.421333333333333e-05,
      "loss": 0.0025,
      "step": 47360
    },
    {
      "epoch": 2.5263999999999998,
      "grad_norm": 0.18782936036586761,
      "learning_rate": 3.4210000000000006e-05,
      "loss": 0.0019,
      "step": 47370
    },
    {
      "epoch": 2.5269333333333335,
      "grad_norm": 0.14952020347118378,
      "learning_rate": 3.420666666666667e-05,
      "loss": 0.0023,
      "step": 47380
    },
    {
      "epoch": 2.5274666666666668,
      "grad_norm": 0.6060665845870972,
      "learning_rate": 3.420333333333334e-05,
      "loss": 0.0028,
      "step": 47390
    },
    {
      "epoch": 2.528,
      "grad_norm": 0.2738899886608124,
      "learning_rate": 3.4200000000000005e-05,
      "loss": 0.0025,
      "step": 47400
    },
    {
      "epoch": 2.5285333333333333,
      "grad_norm": 0.3372479975223541,
      "learning_rate": 3.4196666666666664e-05,
      "loss": 0.0027,
      "step": 47410
    },
    {
      "epoch": 2.5290666666666666,
      "grad_norm": 0.4424903392791748,
      "learning_rate": 3.419333333333333e-05,
      "loss": 0.0021,
      "step": 47420
    },
    {
      "epoch": 2.5296,
      "grad_norm": 0.12486713379621506,
      "learning_rate": 3.419e-05,
      "loss": 0.0022,
      "step": 47430
    },
    {
      "epoch": 2.5301333333333336,
      "grad_norm": 0.16252323985099792,
      "learning_rate": 3.418666666666667e-05,
      "loss": 0.0027,
      "step": 47440
    },
    {
      "epoch": 2.530666666666667,
      "grad_norm": 0.11835050582885742,
      "learning_rate": 3.4183333333333335e-05,
      "loss": 0.0026,
      "step": 47450
    },
    {
      "epoch": 2.5312,
      "grad_norm": 0.06746487319469452,
      "learning_rate": 3.418e-05,
      "loss": 0.0021,
      "step": 47460
    },
    {
      "epoch": 2.5317333333333334,
      "grad_norm": 0.16361770033836365,
      "learning_rate": 3.417666666666667e-05,
      "loss": 0.0026,
      "step": 47470
    },
    {
      "epoch": 2.5322666666666667,
      "grad_norm": 0.357850581407547,
      "learning_rate": 3.4173333333333334e-05,
      "loss": 0.0031,
      "step": 47480
    },
    {
      "epoch": 2.5328,
      "grad_norm": 0.2839110791683197,
      "learning_rate": 3.417e-05,
      "loss": 0.0035,
      "step": 47490
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 0.609961986541748,
      "learning_rate": 3.4166666666666666e-05,
      "loss": 0.003,
      "step": 47500
    },
    {
      "epoch": 2.5338666666666665,
      "grad_norm": 0.42038869857788086,
      "learning_rate": 3.416333333333334e-05,
      "loss": 0.0016,
      "step": 47510
    },
    {
      "epoch": 2.5343999999999998,
      "grad_norm": 0.3246564567089081,
      "learning_rate": 3.4160000000000005e-05,
      "loss": 0.0023,
      "step": 47520
    },
    {
      "epoch": 2.5349333333333335,
      "grad_norm": 0.09797745943069458,
      "learning_rate": 3.415666666666667e-05,
      "loss": 0.0022,
      "step": 47530
    },
    {
      "epoch": 2.5354666666666668,
      "grad_norm": 0.349847674369812,
      "learning_rate": 3.415333333333334e-05,
      "loss": 0.0034,
      "step": 47540
    },
    {
      "epoch": 2.536,
      "grad_norm": 0.2537061274051666,
      "learning_rate": 3.415e-05,
      "loss": 0.0024,
      "step": 47550
    },
    {
      "epoch": 2.5365333333333333,
      "grad_norm": 0.12179561704397202,
      "learning_rate": 3.414666666666666e-05,
      "loss": 0.0027,
      "step": 47560
    },
    {
      "epoch": 2.5370666666666666,
      "grad_norm": 0.14898592233657837,
      "learning_rate": 3.4143333333333336e-05,
      "loss": 0.0029,
      "step": 47570
    },
    {
      "epoch": 2.5376,
      "grad_norm": 0.21411985158920288,
      "learning_rate": 3.414e-05,
      "loss": 0.0028,
      "step": 47580
    },
    {
      "epoch": 2.5381333333333336,
      "grad_norm": 0.18520745635032654,
      "learning_rate": 3.413666666666667e-05,
      "loss": 0.0024,
      "step": 47590
    },
    {
      "epoch": 2.538666666666667,
      "grad_norm": 0.19732236862182617,
      "learning_rate": 3.4133333333333334e-05,
      "loss": 0.0024,
      "step": 47600
    },
    {
      "epoch": 2.5392,
      "grad_norm": 0.2263324111700058,
      "learning_rate": 3.413e-05,
      "loss": 0.0031,
      "step": 47610
    },
    {
      "epoch": 2.5397333333333334,
      "grad_norm": 0.18731176853179932,
      "learning_rate": 3.4126666666666666e-05,
      "loss": 0.0017,
      "step": 47620
    },
    {
      "epoch": 2.5402666666666667,
      "grad_norm": 0.1109812930226326,
      "learning_rate": 3.412333333333333e-05,
      "loss": 0.0021,
      "step": 47630
    },
    {
      "epoch": 2.5408,
      "grad_norm": 0.5547931790351868,
      "learning_rate": 3.412e-05,
      "loss": 0.0021,
      "step": 47640
    },
    {
      "epoch": 2.541333333333333,
      "grad_norm": 0.05845475569367409,
      "learning_rate": 3.411666666666667e-05,
      "loss": 0.0021,
      "step": 47650
    },
    {
      "epoch": 2.5418666666666665,
      "grad_norm": 0.36159548163414,
      "learning_rate": 3.411333333333334e-05,
      "loss": 0.0018,
      "step": 47660
    },
    {
      "epoch": 2.5423999999999998,
      "grad_norm": 0.29739025235176086,
      "learning_rate": 3.4110000000000004e-05,
      "loss": 0.0017,
      "step": 47670
    },
    {
      "epoch": 2.5429333333333335,
      "grad_norm": 0.448309063911438,
      "learning_rate": 3.410666666666667e-05,
      "loss": 0.0024,
      "step": 47680
    },
    {
      "epoch": 2.5434666666666668,
      "grad_norm": 0.5439631342887878,
      "learning_rate": 3.4103333333333336e-05,
      "loss": 0.0037,
      "step": 47690
    },
    {
      "epoch": 2.544,
      "grad_norm": 0.2535706162452698,
      "learning_rate": 3.41e-05,
      "loss": 0.0031,
      "step": 47700
    },
    {
      "epoch": 2.5445333333333333,
      "grad_norm": 0.12066955864429474,
      "learning_rate": 3.409666666666667e-05,
      "loss": 0.002,
      "step": 47710
    },
    {
      "epoch": 2.5450666666666666,
      "grad_norm": 0.046347808092832565,
      "learning_rate": 3.4093333333333334e-05,
      "loss": 0.0029,
      "step": 47720
    },
    {
      "epoch": 2.5456,
      "grad_norm": 0.18478593230247498,
      "learning_rate": 3.409e-05,
      "loss": 0.0025,
      "step": 47730
    },
    {
      "epoch": 2.5461333333333336,
      "grad_norm": 0.3627380132675171,
      "learning_rate": 3.408666666666667e-05,
      "loss": 0.0027,
      "step": 47740
    },
    {
      "epoch": 2.546666666666667,
      "grad_norm": 0.06277815252542496,
      "learning_rate": 3.408333333333333e-05,
      "loss": 0.0017,
      "step": 47750
    },
    {
      "epoch": 2.5472,
      "grad_norm": 0.45577335357666016,
      "learning_rate": 3.408e-05,
      "loss": 0.0025,
      "step": 47760
    },
    {
      "epoch": 2.5477333333333334,
      "grad_norm": 0.2948904037475586,
      "learning_rate": 3.4076666666666665e-05,
      "loss": 0.0023,
      "step": 47770
    },
    {
      "epoch": 2.5482666666666667,
      "grad_norm": 0.6016507148742676,
      "learning_rate": 3.407333333333334e-05,
      "loss": 0.0028,
      "step": 47780
    },
    {
      "epoch": 2.5488,
      "grad_norm": 0.16436971724033356,
      "learning_rate": 3.4070000000000004e-05,
      "loss": 0.0029,
      "step": 47790
    },
    {
      "epoch": 2.5493333333333332,
      "grad_norm": 0.24694451689720154,
      "learning_rate": 3.406666666666667e-05,
      "loss": 0.003,
      "step": 47800
    },
    {
      "epoch": 2.5498666666666665,
      "grad_norm": 0.3643251061439514,
      "learning_rate": 3.4063333333333336e-05,
      "loss": 0.0028,
      "step": 47810
    },
    {
      "epoch": 2.5504,
      "grad_norm": 0.30787307024002075,
      "learning_rate": 3.406e-05,
      "loss": 0.002,
      "step": 47820
    },
    {
      "epoch": 2.5509333333333335,
      "grad_norm": 0.427328884601593,
      "learning_rate": 3.405666666666667e-05,
      "loss": 0.0031,
      "step": 47830
    },
    {
      "epoch": 2.5514666666666668,
      "grad_norm": 0.11953147500753403,
      "learning_rate": 3.4053333333333335e-05,
      "loss": 0.0024,
      "step": 47840
    },
    {
      "epoch": 2.552,
      "grad_norm": 0.0886254534125328,
      "learning_rate": 3.405e-05,
      "loss": 0.0031,
      "step": 47850
    },
    {
      "epoch": 2.5525333333333333,
      "grad_norm": 0.3120012581348419,
      "learning_rate": 3.404666666666667e-05,
      "loss": 0.0022,
      "step": 47860
    },
    {
      "epoch": 2.5530666666666666,
      "grad_norm": 0.1508646011352539,
      "learning_rate": 3.404333333333333e-05,
      "loss": 0.0027,
      "step": 47870
    },
    {
      "epoch": 2.5536,
      "grad_norm": 0.03859013691544533,
      "learning_rate": 3.404e-05,
      "loss": 0.0024,
      "step": 47880
    },
    {
      "epoch": 2.5541333333333336,
      "grad_norm": 0.2681889832019806,
      "learning_rate": 3.4036666666666665e-05,
      "loss": 0.002,
      "step": 47890
    },
    {
      "epoch": 2.554666666666667,
      "grad_norm": 0.5811975002288818,
      "learning_rate": 3.403333333333333e-05,
      "loss": 0.0024,
      "step": 47900
    },
    {
      "epoch": 2.5552,
      "grad_norm": 0.485281765460968,
      "learning_rate": 3.403e-05,
      "loss": 0.0032,
      "step": 47910
    },
    {
      "epoch": 2.5557333333333334,
      "grad_norm": 0.25848084688186646,
      "learning_rate": 3.402666666666667e-05,
      "loss": 0.0028,
      "step": 47920
    },
    {
      "epoch": 2.5562666666666667,
      "grad_norm": 0.1947077214717865,
      "learning_rate": 3.402333333333334e-05,
      "loss": 0.0027,
      "step": 47930
    },
    {
      "epoch": 2.5568,
      "grad_norm": 0.4859209656715393,
      "learning_rate": 3.402e-05,
      "loss": 0.0034,
      "step": 47940
    },
    {
      "epoch": 2.5573333333333332,
      "grad_norm": 0.42598897218704224,
      "learning_rate": 3.401666666666667e-05,
      "loss": 0.0016,
      "step": 47950
    },
    {
      "epoch": 2.5578666666666665,
      "grad_norm": 0.36915159225463867,
      "learning_rate": 3.4013333333333335e-05,
      "loss": 0.0019,
      "step": 47960
    },
    {
      "epoch": 2.5584,
      "grad_norm": 0.633676290512085,
      "learning_rate": 3.401e-05,
      "loss": 0.0018,
      "step": 47970
    },
    {
      "epoch": 2.558933333333333,
      "grad_norm": 0.6741851568222046,
      "learning_rate": 3.400666666666667e-05,
      "loss": 0.0026,
      "step": 47980
    },
    {
      "epoch": 2.559466666666667,
      "grad_norm": 0.3010716438293457,
      "learning_rate": 3.400333333333334e-05,
      "loss": 0.0028,
      "step": 47990
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.2503088116645813,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.0027,
      "step": 48000
    },
    {
      "epoch": 2.5605333333333333,
      "grad_norm": 0.4927116334438324,
      "learning_rate": 3.3996666666666666e-05,
      "loss": 0.0024,
      "step": 48010
    },
    {
      "epoch": 2.5610666666666666,
      "grad_norm": 0.3026227056980133,
      "learning_rate": 3.399333333333333e-05,
      "loss": 0.0026,
      "step": 48020
    },
    {
      "epoch": 2.5616,
      "grad_norm": 0.1338758021593094,
      "learning_rate": 3.399e-05,
      "loss": 0.002,
      "step": 48030
    },
    {
      "epoch": 2.5621333333333336,
      "grad_norm": 0.033121246844530106,
      "learning_rate": 3.3986666666666664e-05,
      "loss": 0.0028,
      "step": 48040
    },
    {
      "epoch": 2.562666666666667,
      "grad_norm": 0.09790142625570297,
      "learning_rate": 3.398333333333333e-05,
      "loss": 0.0017,
      "step": 48050
    },
    {
      "epoch": 2.5632,
      "grad_norm": 0.3700169324874878,
      "learning_rate": 3.398e-05,
      "loss": 0.0026,
      "step": 48060
    },
    {
      "epoch": 2.5637333333333334,
      "grad_norm": 0.2532443106174469,
      "learning_rate": 3.397666666666667e-05,
      "loss": 0.0025,
      "step": 48070
    },
    {
      "epoch": 2.5642666666666667,
      "grad_norm": 0.19234615564346313,
      "learning_rate": 3.3973333333333336e-05,
      "loss": 0.0032,
      "step": 48080
    },
    {
      "epoch": 2.5648,
      "grad_norm": 0.0673648864030838,
      "learning_rate": 3.397e-05,
      "loss": 0.0017,
      "step": 48090
    },
    {
      "epoch": 2.5653333333333332,
      "grad_norm": 0.15067872405052185,
      "learning_rate": 3.396666666666667e-05,
      "loss": 0.0027,
      "step": 48100
    },
    {
      "epoch": 2.5658666666666665,
      "grad_norm": 0.04087807238101959,
      "learning_rate": 3.3963333333333334e-05,
      "loss": 0.0019,
      "step": 48110
    },
    {
      "epoch": 2.5664,
      "grad_norm": 0.34721502661705017,
      "learning_rate": 3.396e-05,
      "loss": 0.0033,
      "step": 48120
    },
    {
      "epoch": 2.566933333333333,
      "grad_norm": 0.0695287361741066,
      "learning_rate": 3.395666666666667e-05,
      "loss": 0.0031,
      "step": 48130
    },
    {
      "epoch": 2.567466666666667,
      "grad_norm": 0.33884745836257935,
      "learning_rate": 3.395333333333334e-05,
      "loss": 0.0023,
      "step": 48140
    },
    {
      "epoch": 2.568,
      "grad_norm": 0.45481181144714355,
      "learning_rate": 3.3950000000000005e-05,
      "loss": 0.0023,
      "step": 48150
    },
    {
      "epoch": 2.5685333333333333,
      "grad_norm": 0.3302634358406067,
      "learning_rate": 3.394666666666667e-05,
      "loss": 0.0019,
      "step": 48160
    },
    {
      "epoch": 2.5690666666666666,
      "grad_norm": 0.30300480127334595,
      "learning_rate": 3.394333333333333e-05,
      "loss": 0.0027,
      "step": 48170
    },
    {
      "epoch": 2.5696,
      "grad_norm": 0.24809308350086212,
      "learning_rate": 3.394e-05,
      "loss": 0.0022,
      "step": 48180
    },
    {
      "epoch": 2.5701333333333336,
      "grad_norm": 0.2852763533592224,
      "learning_rate": 3.393666666666667e-05,
      "loss": 0.0031,
      "step": 48190
    },
    {
      "epoch": 2.570666666666667,
      "grad_norm": 0.25578007102012634,
      "learning_rate": 3.3933333333333336e-05,
      "loss": 0.0019,
      "step": 48200
    },
    {
      "epoch": 2.5712,
      "grad_norm": 0.39277228713035583,
      "learning_rate": 3.393e-05,
      "loss": 0.0024,
      "step": 48210
    },
    {
      "epoch": 2.5717333333333334,
      "grad_norm": 0.6040652990341187,
      "learning_rate": 3.392666666666667e-05,
      "loss": 0.0022,
      "step": 48220
    },
    {
      "epoch": 2.5722666666666667,
      "grad_norm": 0.13703922927379608,
      "learning_rate": 3.3923333333333334e-05,
      "loss": 0.0021,
      "step": 48230
    },
    {
      "epoch": 2.5728,
      "grad_norm": 0.3365476429462433,
      "learning_rate": 3.392e-05,
      "loss": 0.0022,
      "step": 48240
    },
    {
      "epoch": 2.5733333333333333,
      "grad_norm": 0.5611176490783691,
      "learning_rate": 3.391666666666667e-05,
      "loss": 0.0026,
      "step": 48250
    },
    {
      "epoch": 2.5738666666666665,
      "grad_norm": 0.693832516670227,
      "learning_rate": 3.391333333333333e-05,
      "loss": 0.0038,
      "step": 48260
    },
    {
      "epoch": 2.5744,
      "grad_norm": 0.41611990332603455,
      "learning_rate": 3.3910000000000006e-05,
      "loss": 0.0016,
      "step": 48270
    },
    {
      "epoch": 2.574933333333333,
      "grad_norm": 0.06959673762321472,
      "learning_rate": 3.390666666666667e-05,
      "loss": 0.0021,
      "step": 48280
    },
    {
      "epoch": 2.575466666666667,
      "grad_norm": 0.33646371960639954,
      "learning_rate": 3.390333333333334e-05,
      "loss": 0.002,
      "step": 48290
    },
    {
      "epoch": 2.576,
      "grad_norm": 0.1516050100326538,
      "learning_rate": 3.3900000000000004e-05,
      "loss": 0.002,
      "step": 48300
    },
    {
      "epoch": 2.5765333333333333,
      "grad_norm": 0.06470849364995956,
      "learning_rate": 3.389666666666667e-05,
      "loss": 0.003,
      "step": 48310
    },
    {
      "epoch": 2.5770666666666666,
      "grad_norm": 0.273446649312973,
      "learning_rate": 3.389333333333333e-05,
      "loss": 0.0015,
      "step": 48320
    },
    {
      "epoch": 2.5776,
      "grad_norm": 0.4306415319442749,
      "learning_rate": 3.389e-05,
      "loss": 0.0022,
      "step": 48330
    },
    {
      "epoch": 2.5781333333333336,
      "grad_norm": 0.36029359698295593,
      "learning_rate": 3.388666666666667e-05,
      "loss": 0.002,
      "step": 48340
    },
    {
      "epoch": 2.578666666666667,
      "grad_norm": 0.17880690097808838,
      "learning_rate": 3.3883333333333335e-05,
      "loss": 0.0023,
      "step": 48350
    },
    {
      "epoch": 2.5792,
      "grad_norm": 0.053464557975530624,
      "learning_rate": 3.388e-05,
      "loss": 0.0035,
      "step": 48360
    },
    {
      "epoch": 2.5797333333333334,
      "grad_norm": 0.18800385296344757,
      "learning_rate": 3.387666666666667e-05,
      "loss": 0.0023,
      "step": 48370
    },
    {
      "epoch": 2.5802666666666667,
      "grad_norm": 0.246116504073143,
      "learning_rate": 3.387333333333333e-05,
      "loss": 0.0019,
      "step": 48380
    },
    {
      "epoch": 2.5808,
      "grad_norm": 0.21284699440002441,
      "learning_rate": 3.387e-05,
      "loss": 0.0022,
      "step": 48390
    },
    {
      "epoch": 2.5813333333333333,
      "grad_norm": 0.15869714319705963,
      "learning_rate": 3.3866666666666665e-05,
      "loss": 0.0027,
      "step": 48400
    },
    {
      "epoch": 2.5818666666666665,
      "grad_norm": 0.2477012723684311,
      "learning_rate": 3.386333333333334e-05,
      "loss": 0.0025,
      "step": 48410
    },
    {
      "epoch": 2.5824,
      "grad_norm": 0.5210443735122681,
      "learning_rate": 3.3860000000000004e-05,
      "loss": 0.0024,
      "step": 48420
    },
    {
      "epoch": 2.582933333333333,
      "grad_norm": 0.2167825847864151,
      "learning_rate": 3.385666666666667e-05,
      "loss": 0.0022,
      "step": 48430
    },
    {
      "epoch": 2.583466666666667,
      "grad_norm": 0.18264679610729218,
      "learning_rate": 3.385333333333334e-05,
      "loss": 0.0028,
      "step": 48440
    },
    {
      "epoch": 2.584,
      "grad_norm": 0.07151153683662415,
      "learning_rate": 3.385e-05,
      "loss": 0.0013,
      "step": 48450
    },
    {
      "epoch": 2.5845333333333333,
      "grad_norm": 0.10426865518093109,
      "learning_rate": 3.384666666666667e-05,
      "loss": 0.0017,
      "step": 48460
    },
    {
      "epoch": 2.5850666666666666,
      "grad_norm": 0.4792959988117218,
      "learning_rate": 3.3843333333333335e-05,
      "loss": 0.0017,
      "step": 48470
    },
    {
      "epoch": 2.5856,
      "grad_norm": 0.11227895319461823,
      "learning_rate": 3.384e-05,
      "loss": 0.0028,
      "step": 48480
    },
    {
      "epoch": 2.586133333333333,
      "grad_norm": 0.2193872332572937,
      "learning_rate": 3.383666666666667e-05,
      "loss": 0.0035,
      "step": 48490
    },
    {
      "epoch": 2.586666666666667,
      "grad_norm": 0.4149514138698578,
      "learning_rate": 3.3833333333333334e-05,
      "loss": 0.0028,
      "step": 48500
    },
    {
      "epoch": 2.5872,
      "grad_norm": 0.10095536708831787,
      "learning_rate": 3.383e-05,
      "loss": 0.0028,
      "step": 48510
    },
    {
      "epoch": 2.5877333333333334,
      "grad_norm": 0.18967929482460022,
      "learning_rate": 3.3826666666666666e-05,
      "loss": 0.003,
      "step": 48520
    },
    {
      "epoch": 2.5882666666666667,
      "grad_norm": 0.3029199540615082,
      "learning_rate": 3.382333333333333e-05,
      "loss": 0.0028,
      "step": 48530
    },
    {
      "epoch": 2.5888,
      "grad_norm": 0.39166954159736633,
      "learning_rate": 3.3820000000000005e-05,
      "loss": 0.0018,
      "step": 48540
    },
    {
      "epoch": 2.5893333333333333,
      "grad_norm": 0.4239659011363983,
      "learning_rate": 3.381666666666667e-05,
      "loss": 0.0035,
      "step": 48550
    },
    {
      "epoch": 2.5898666666666665,
      "grad_norm": 0.39152470231056213,
      "learning_rate": 3.381333333333334e-05,
      "loss": 0.0029,
      "step": 48560
    },
    {
      "epoch": 2.5904,
      "grad_norm": 0.25419360399246216,
      "learning_rate": 3.381e-05,
      "loss": 0.0023,
      "step": 48570
    },
    {
      "epoch": 2.590933333333333,
      "grad_norm": 0.1795274317264557,
      "learning_rate": 3.380666666666667e-05,
      "loss": 0.002,
      "step": 48580
    },
    {
      "epoch": 2.591466666666667,
      "grad_norm": 0.06817774474620819,
      "learning_rate": 3.3803333333333336e-05,
      "loss": 0.0018,
      "step": 48590
    },
    {
      "epoch": 2.592,
      "grad_norm": 0.5654347538948059,
      "learning_rate": 3.38e-05,
      "loss": 0.0022,
      "step": 48600
    },
    {
      "epoch": 2.5925333333333334,
      "grad_norm": 0.15300855040550232,
      "learning_rate": 3.379666666666667e-05,
      "loss": 0.003,
      "step": 48610
    },
    {
      "epoch": 2.5930666666666666,
      "grad_norm": 0.34762147068977356,
      "learning_rate": 3.3793333333333334e-05,
      "loss": 0.0022,
      "step": 48620
    },
    {
      "epoch": 2.5936,
      "grad_norm": 0.18017174303531647,
      "learning_rate": 3.379e-05,
      "loss": 0.0018,
      "step": 48630
    },
    {
      "epoch": 2.594133333333333,
      "grad_norm": 0.3073400557041168,
      "learning_rate": 3.3786666666666666e-05,
      "loss": 0.0018,
      "step": 48640
    },
    {
      "epoch": 2.594666666666667,
      "grad_norm": 0.15659557282924652,
      "learning_rate": 3.378333333333333e-05,
      "loss": 0.0024,
      "step": 48650
    },
    {
      "epoch": 2.5952,
      "grad_norm": 0.08791084587574005,
      "learning_rate": 3.378e-05,
      "loss": 0.0028,
      "step": 48660
    },
    {
      "epoch": 2.5957333333333334,
      "grad_norm": 0.42029350996017456,
      "learning_rate": 3.3776666666666665e-05,
      "loss": 0.0018,
      "step": 48670
    },
    {
      "epoch": 2.5962666666666667,
      "grad_norm": 0.04664919152855873,
      "learning_rate": 3.377333333333334e-05,
      "loss": 0.0035,
      "step": 48680
    },
    {
      "epoch": 2.5968,
      "grad_norm": 0.08915899693965912,
      "learning_rate": 3.3770000000000004e-05,
      "loss": 0.0026,
      "step": 48690
    },
    {
      "epoch": 2.5973333333333333,
      "grad_norm": 0.462860643863678,
      "learning_rate": 3.376666666666667e-05,
      "loss": 0.0019,
      "step": 48700
    },
    {
      "epoch": 2.5978666666666665,
      "grad_norm": 0.12369339913129807,
      "learning_rate": 3.3763333333333336e-05,
      "loss": 0.0029,
      "step": 48710
    },
    {
      "epoch": 2.5984,
      "grad_norm": 0.43946319818496704,
      "learning_rate": 3.376e-05,
      "loss": 0.0028,
      "step": 48720
    },
    {
      "epoch": 2.598933333333333,
      "grad_norm": 0.08281880617141724,
      "learning_rate": 3.375666666666667e-05,
      "loss": 0.0024,
      "step": 48730
    },
    {
      "epoch": 2.599466666666667,
      "grad_norm": 0.3966078758239746,
      "learning_rate": 3.3753333333333334e-05,
      "loss": 0.0025,
      "step": 48740
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.19182410836219788,
      "learning_rate": 3.375000000000001e-05,
      "loss": 0.0023,
      "step": 48750
    },
    {
      "epoch": 2.6005333333333334,
      "grad_norm": 0.07059032469987869,
      "learning_rate": 3.374666666666667e-05,
      "loss": 0.0032,
      "step": 48760
    },
    {
      "epoch": 2.6010666666666666,
      "grad_norm": 0.23998086154460907,
      "learning_rate": 3.374333333333333e-05,
      "loss": 0.0016,
      "step": 48770
    },
    {
      "epoch": 2.6016,
      "grad_norm": 0.33456504344940186,
      "learning_rate": 3.374e-05,
      "loss": 0.0023,
      "step": 48780
    },
    {
      "epoch": 2.602133333333333,
      "grad_norm": 0.49518319964408875,
      "learning_rate": 3.3736666666666665e-05,
      "loss": 0.003,
      "step": 48790
    },
    {
      "epoch": 2.602666666666667,
      "grad_norm": 0.4232841730117798,
      "learning_rate": 3.373333333333333e-05,
      "loss": 0.0025,
      "step": 48800
    },
    {
      "epoch": 2.6032,
      "grad_norm": 0.632929801940918,
      "learning_rate": 3.373e-05,
      "loss": 0.0021,
      "step": 48810
    },
    {
      "epoch": 2.6037333333333335,
      "grad_norm": 0.6287356615066528,
      "learning_rate": 3.372666666666667e-05,
      "loss": 0.0025,
      "step": 48820
    },
    {
      "epoch": 2.6042666666666667,
      "grad_norm": 0.154465451836586,
      "learning_rate": 3.3723333333333336e-05,
      "loss": 0.0021,
      "step": 48830
    },
    {
      "epoch": 2.6048,
      "grad_norm": 0.16558901965618134,
      "learning_rate": 3.372e-05,
      "loss": 0.0024,
      "step": 48840
    },
    {
      "epoch": 2.6053333333333333,
      "grad_norm": 0.14713770151138306,
      "learning_rate": 3.371666666666667e-05,
      "loss": 0.0024,
      "step": 48850
    },
    {
      "epoch": 2.6058666666666666,
      "grad_norm": 0.25964799523353577,
      "learning_rate": 3.3713333333333335e-05,
      "loss": 0.002,
      "step": 48860
    },
    {
      "epoch": 2.6064,
      "grad_norm": 0.29751116037368774,
      "learning_rate": 3.371e-05,
      "loss": 0.0028,
      "step": 48870
    },
    {
      "epoch": 2.606933333333333,
      "grad_norm": 0.12367486208677292,
      "learning_rate": 3.370666666666667e-05,
      "loss": 0.002,
      "step": 48880
    },
    {
      "epoch": 2.607466666666667,
      "grad_norm": 0.5753615498542786,
      "learning_rate": 3.370333333333334e-05,
      "loss": 0.0028,
      "step": 48890
    },
    {
      "epoch": 2.608,
      "grad_norm": 0.32378533482551575,
      "learning_rate": 3.3700000000000006e-05,
      "loss": 0.0024,
      "step": 48900
    },
    {
      "epoch": 2.6085333333333334,
      "grad_norm": 0.32072076201438904,
      "learning_rate": 3.369666666666667e-05,
      "loss": 0.0023,
      "step": 48910
    },
    {
      "epoch": 2.6090666666666666,
      "grad_norm": 0.34062543511390686,
      "learning_rate": 3.369333333333333e-05,
      "loss": 0.002,
      "step": 48920
    },
    {
      "epoch": 2.6096,
      "grad_norm": 0.5875580906867981,
      "learning_rate": 3.369e-05,
      "loss": 0.0018,
      "step": 48930
    },
    {
      "epoch": 2.610133333333333,
      "grad_norm": 0.2504331171512604,
      "learning_rate": 3.3686666666666664e-05,
      "loss": 0.003,
      "step": 48940
    },
    {
      "epoch": 2.610666666666667,
      "grad_norm": 0.21253813803195953,
      "learning_rate": 3.368333333333334e-05,
      "loss": 0.0021,
      "step": 48950
    },
    {
      "epoch": 2.6112,
      "grad_norm": 0.28143125772476196,
      "learning_rate": 3.368e-05,
      "loss": 0.0031,
      "step": 48960
    },
    {
      "epoch": 2.6117333333333335,
      "grad_norm": 0.09987221658229828,
      "learning_rate": 3.367666666666667e-05,
      "loss": 0.003,
      "step": 48970
    },
    {
      "epoch": 2.6122666666666667,
      "grad_norm": 0.3945766091346741,
      "learning_rate": 3.3673333333333335e-05,
      "loss": 0.0029,
      "step": 48980
    },
    {
      "epoch": 2.6128,
      "grad_norm": 0.7574585676193237,
      "learning_rate": 3.367e-05,
      "loss": 0.0022,
      "step": 48990
    },
    {
      "epoch": 2.6133333333333333,
      "grad_norm": 0.30002474784851074,
      "learning_rate": 3.366666666666667e-05,
      "loss": 0.0022,
      "step": 49000
    },
    {
      "epoch": 2.6138666666666666,
      "grad_norm": 0.039716627448797226,
      "learning_rate": 3.3663333333333333e-05,
      "loss": 0.0028,
      "step": 49010
    },
    {
      "epoch": 2.6144,
      "grad_norm": 0.3628523349761963,
      "learning_rate": 3.366e-05,
      "loss": 0.0018,
      "step": 49020
    },
    {
      "epoch": 2.614933333333333,
      "grad_norm": 0.47852426767349243,
      "learning_rate": 3.365666666666667e-05,
      "loss": 0.002,
      "step": 49030
    },
    {
      "epoch": 2.615466666666667,
      "grad_norm": 0.10604315251111984,
      "learning_rate": 3.365333333333334e-05,
      "loss": 0.0024,
      "step": 49040
    },
    {
      "epoch": 2.616,
      "grad_norm": 0.15456020832061768,
      "learning_rate": 3.3650000000000005e-05,
      "loss": 0.0016,
      "step": 49050
    },
    {
      "epoch": 2.6165333333333334,
      "grad_norm": 0.049659598618745804,
      "learning_rate": 3.364666666666667e-05,
      "loss": 0.0017,
      "step": 49060
    },
    {
      "epoch": 2.6170666666666667,
      "grad_norm": 0.3095615804195404,
      "learning_rate": 3.364333333333333e-05,
      "loss": 0.0037,
      "step": 49070
    },
    {
      "epoch": 2.6176,
      "grad_norm": 0.18416142463684082,
      "learning_rate": 3.3639999999999996e-05,
      "loss": 0.0019,
      "step": 49080
    },
    {
      "epoch": 2.618133333333333,
      "grad_norm": 0.2437703013420105,
      "learning_rate": 3.363666666666667e-05,
      "loss": 0.004,
      "step": 49090
    },
    {
      "epoch": 2.618666666666667,
      "grad_norm": 0.2092449814081192,
      "learning_rate": 3.3633333333333335e-05,
      "loss": 0.0015,
      "step": 49100
    },
    {
      "epoch": 2.6192,
      "grad_norm": 0.1507991999387741,
      "learning_rate": 3.363e-05,
      "loss": 0.0028,
      "step": 49110
    },
    {
      "epoch": 2.6197333333333335,
      "grad_norm": 0.10263343155384064,
      "learning_rate": 3.362666666666667e-05,
      "loss": 0.002,
      "step": 49120
    },
    {
      "epoch": 2.6202666666666667,
      "grad_norm": 0.6448651552200317,
      "learning_rate": 3.3623333333333334e-05,
      "loss": 0.003,
      "step": 49130
    },
    {
      "epoch": 2.6208,
      "grad_norm": 0.3958744704723358,
      "learning_rate": 3.362e-05,
      "loss": 0.002,
      "step": 49140
    },
    {
      "epoch": 2.6213333333333333,
      "grad_norm": 0.36995622515678406,
      "learning_rate": 3.3616666666666666e-05,
      "loss": 0.0033,
      "step": 49150
    },
    {
      "epoch": 2.6218666666666666,
      "grad_norm": 0.0677456185221672,
      "learning_rate": 3.361333333333333e-05,
      "loss": 0.0026,
      "step": 49160
    },
    {
      "epoch": 2.6224,
      "grad_norm": 0.06561018526554108,
      "learning_rate": 3.3610000000000005e-05,
      "loss": 0.0022,
      "step": 49170
    },
    {
      "epoch": 2.622933333333333,
      "grad_norm": 0.07707111537456512,
      "learning_rate": 3.360666666666667e-05,
      "loss": 0.0014,
      "step": 49180
    },
    {
      "epoch": 2.6234666666666664,
      "grad_norm": 0.050406381487846375,
      "learning_rate": 3.360333333333334e-05,
      "loss": 0.0027,
      "step": 49190
    },
    {
      "epoch": 2.624,
      "grad_norm": 0.1875479817390442,
      "learning_rate": 3.3600000000000004e-05,
      "loss": 0.0027,
      "step": 49200
    },
    {
      "epoch": 2.6245333333333334,
      "grad_norm": 0.14997084438800812,
      "learning_rate": 3.359666666666667e-05,
      "loss": 0.0021,
      "step": 49210
    },
    {
      "epoch": 2.6250666666666667,
      "grad_norm": 0.12168968468904495,
      "learning_rate": 3.359333333333333e-05,
      "loss": 0.0026,
      "step": 49220
    },
    {
      "epoch": 2.6256,
      "grad_norm": 0.3109777271747589,
      "learning_rate": 3.359e-05,
      "loss": 0.002,
      "step": 49230
    },
    {
      "epoch": 2.626133333333333,
      "grad_norm": 0.12502527236938477,
      "learning_rate": 3.358666666666667e-05,
      "loss": 0.0023,
      "step": 49240
    },
    {
      "epoch": 2.626666666666667,
      "grad_norm": 0.12379603832960129,
      "learning_rate": 3.3583333333333334e-05,
      "loss": 0.0017,
      "step": 49250
    },
    {
      "epoch": 2.6272,
      "grad_norm": 0.2717691957950592,
      "learning_rate": 3.358e-05,
      "loss": 0.0014,
      "step": 49260
    },
    {
      "epoch": 2.6277333333333335,
      "grad_norm": 0.09388993680477142,
      "learning_rate": 3.3576666666666666e-05,
      "loss": 0.0023,
      "step": 49270
    },
    {
      "epoch": 2.6282666666666668,
      "grad_norm": 0.3048264682292938,
      "learning_rate": 3.357333333333333e-05,
      "loss": 0.0019,
      "step": 49280
    },
    {
      "epoch": 2.6288,
      "grad_norm": 0.12732188403606415,
      "learning_rate": 3.357e-05,
      "loss": 0.002,
      "step": 49290
    },
    {
      "epoch": 2.6293333333333333,
      "grad_norm": 0.42176002264022827,
      "learning_rate": 3.356666666666667e-05,
      "loss": 0.0015,
      "step": 49300
    },
    {
      "epoch": 2.6298666666666666,
      "grad_norm": 0.13532035052776337,
      "learning_rate": 3.356333333333334e-05,
      "loss": 0.0026,
      "step": 49310
    },
    {
      "epoch": 2.6304,
      "grad_norm": 0.489128440618515,
      "learning_rate": 3.3560000000000004e-05,
      "loss": 0.0035,
      "step": 49320
    },
    {
      "epoch": 2.630933333333333,
      "grad_norm": 0.3124198317527771,
      "learning_rate": 3.355666666666667e-05,
      "loss": 0.0013,
      "step": 49330
    },
    {
      "epoch": 2.6314666666666664,
      "grad_norm": 0.42887023091316223,
      "learning_rate": 3.3553333333333336e-05,
      "loss": 0.0025,
      "step": 49340
    },
    {
      "epoch": 2.632,
      "grad_norm": 0.18990540504455566,
      "learning_rate": 3.355e-05,
      "loss": 0.0024,
      "step": 49350
    },
    {
      "epoch": 2.6325333333333334,
      "grad_norm": 0.09347233921289444,
      "learning_rate": 3.354666666666667e-05,
      "loss": 0.0022,
      "step": 49360
    },
    {
      "epoch": 2.6330666666666667,
      "grad_norm": 0.03925572708249092,
      "learning_rate": 3.3543333333333335e-05,
      "loss": 0.0024,
      "step": 49370
    },
    {
      "epoch": 2.6336,
      "grad_norm": 0.12938706576824188,
      "learning_rate": 3.354e-05,
      "loss": 0.0021,
      "step": 49380
    },
    {
      "epoch": 2.634133333333333,
      "grad_norm": 0.0965607538819313,
      "learning_rate": 3.353666666666667e-05,
      "loss": 0.0024,
      "step": 49390
    },
    {
      "epoch": 2.634666666666667,
      "grad_norm": 0.36366215348243713,
      "learning_rate": 3.353333333333333e-05,
      "loss": 0.0024,
      "step": 49400
    },
    {
      "epoch": 2.6352,
      "grad_norm": 0.07194636762142181,
      "learning_rate": 3.353e-05,
      "loss": 0.0021,
      "step": 49410
    },
    {
      "epoch": 2.6357333333333335,
      "grad_norm": 0.2569803297519684,
      "learning_rate": 3.3526666666666665e-05,
      "loss": 0.0024,
      "step": 49420
    },
    {
      "epoch": 2.6362666666666668,
      "grad_norm": 0.3748159408569336,
      "learning_rate": 3.352333333333333e-05,
      "loss": 0.0023,
      "step": 49430
    },
    {
      "epoch": 2.6368,
      "grad_norm": 0.0419035330414772,
      "learning_rate": 3.3520000000000004e-05,
      "loss": 0.0027,
      "step": 49440
    },
    {
      "epoch": 2.6373333333333333,
      "grad_norm": 0.3669184744358063,
      "learning_rate": 3.351666666666667e-05,
      "loss": 0.0021,
      "step": 49450
    },
    {
      "epoch": 2.6378666666666666,
      "grad_norm": 0.27298733592033386,
      "learning_rate": 3.3513333333333337e-05,
      "loss": 0.0032,
      "step": 49460
    },
    {
      "epoch": 2.6384,
      "grad_norm": 0.3366162180900574,
      "learning_rate": 3.351e-05,
      "loss": 0.0029,
      "step": 49470
    },
    {
      "epoch": 2.638933333333333,
      "grad_norm": 0.04437754303216934,
      "learning_rate": 3.350666666666667e-05,
      "loss": 0.0017,
      "step": 49480
    },
    {
      "epoch": 2.6394666666666664,
      "grad_norm": 0.2211206704378128,
      "learning_rate": 3.3503333333333335e-05,
      "loss": 0.0022,
      "step": 49490
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.23052527010440826,
      "learning_rate": 3.35e-05,
      "loss": 0.0017,
      "step": 49500
    },
    {
      "epoch": 2.6405333333333334,
      "grad_norm": 0.3334449231624603,
      "learning_rate": 3.349666666666667e-05,
      "loss": 0.0022,
      "step": 49510
    },
    {
      "epoch": 2.6410666666666667,
      "grad_norm": 0.25440123677253723,
      "learning_rate": 3.349333333333334e-05,
      "loss": 0.0024,
      "step": 49520
    },
    {
      "epoch": 2.6416,
      "grad_norm": 0.12488839775323868,
      "learning_rate": 3.349e-05,
      "loss": 0.0025,
      "step": 49530
    },
    {
      "epoch": 2.6421333333333332,
      "grad_norm": 0.06890912353992462,
      "learning_rate": 3.3486666666666666e-05,
      "loss": 0.0018,
      "step": 49540
    },
    {
      "epoch": 2.642666666666667,
      "grad_norm": 0.4563271105289459,
      "learning_rate": 3.348333333333333e-05,
      "loss": 0.0018,
      "step": 49550
    },
    {
      "epoch": 2.6432,
      "grad_norm": 0.5203071236610413,
      "learning_rate": 3.348e-05,
      "loss": 0.0019,
      "step": 49560
    },
    {
      "epoch": 2.6437333333333335,
      "grad_norm": 0.48553726077079773,
      "learning_rate": 3.3476666666666664e-05,
      "loss": 0.0023,
      "step": 49570
    },
    {
      "epoch": 2.6442666666666668,
      "grad_norm": 0.21082782745361328,
      "learning_rate": 3.347333333333334e-05,
      "loss": 0.0017,
      "step": 49580
    },
    {
      "epoch": 2.6448,
      "grad_norm": 0.2764540910720825,
      "learning_rate": 3.347e-05,
      "loss": 0.0025,
      "step": 49590
    },
    {
      "epoch": 2.6453333333333333,
      "grad_norm": 0.4819934368133545,
      "learning_rate": 3.346666666666667e-05,
      "loss": 0.0035,
      "step": 49600
    },
    {
      "epoch": 2.6458666666666666,
      "grad_norm": 0.23768997192382812,
      "learning_rate": 3.3463333333333335e-05,
      "loss": 0.0017,
      "step": 49610
    },
    {
      "epoch": 2.6464,
      "grad_norm": 0.09260135143995285,
      "learning_rate": 3.346e-05,
      "loss": 0.0023,
      "step": 49620
    },
    {
      "epoch": 2.646933333333333,
      "grad_norm": 0.19847232103347778,
      "learning_rate": 3.345666666666667e-05,
      "loss": 0.0027,
      "step": 49630
    },
    {
      "epoch": 2.6474666666666664,
      "grad_norm": 0.061493296176195145,
      "learning_rate": 3.3453333333333334e-05,
      "loss": 0.0017,
      "step": 49640
    },
    {
      "epoch": 2.648,
      "grad_norm": 0.23183946311473846,
      "learning_rate": 3.345000000000001e-05,
      "loss": 0.0017,
      "step": 49650
    },
    {
      "epoch": 2.6485333333333334,
      "grad_norm": 0.3945356011390686,
      "learning_rate": 3.344666666666667e-05,
      "loss": 0.0021,
      "step": 49660
    },
    {
      "epoch": 2.6490666666666667,
      "grad_norm": 0.3497709631919861,
      "learning_rate": 3.344333333333334e-05,
      "loss": 0.002,
      "step": 49670
    },
    {
      "epoch": 2.6496,
      "grad_norm": 0.13920363783836365,
      "learning_rate": 3.344e-05,
      "loss": 0.0025,
      "step": 49680
    },
    {
      "epoch": 2.6501333333333332,
      "grad_norm": 0.04803990200161934,
      "learning_rate": 3.3436666666666664e-05,
      "loss": 0.0017,
      "step": 49690
    },
    {
      "epoch": 2.6506666666666665,
      "grad_norm": 0.2756423056125641,
      "learning_rate": 3.343333333333333e-05,
      "loss": 0.0022,
      "step": 49700
    },
    {
      "epoch": 2.6512000000000002,
      "grad_norm": 0.4641265273094177,
      "learning_rate": 3.3430000000000003e-05,
      "loss": 0.003,
      "step": 49710
    },
    {
      "epoch": 2.6517333333333335,
      "grad_norm": 0.4308072328567505,
      "learning_rate": 3.342666666666667e-05,
      "loss": 0.002,
      "step": 49720
    },
    {
      "epoch": 2.6522666666666668,
      "grad_norm": 0.43224236369132996,
      "learning_rate": 3.3423333333333336e-05,
      "loss": 0.0024,
      "step": 49730
    },
    {
      "epoch": 2.6528,
      "grad_norm": 0.4363543689250946,
      "learning_rate": 3.342e-05,
      "loss": 0.0019,
      "step": 49740
    },
    {
      "epoch": 2.6533333333333333,
      "grad_norm": 0.16640591621398926,
      "learning_rate": 3.341666666666667e-05,
      "loss": 0.0021,
      "step": 49750
    },
    {
      "epoch": 2.6538666666666666,
      "grad_norm": 0.09747878462076187,
      "learning_rate": 3.3413333333333334e-05,
      "loss": 0.0016,
      "step": 49760
    },
    {
      "epoch": 2.6544,
      "grad_norm": 0.19076119363307953,
      "learning_rate": 3.341e-05,
      "loss": 0.0016,
      "step": 49770
    },
    {
      "epoch": 2.654933333333333,
      "grad_norm": 0.47704213857650757,
      "learning_rate": 3.3406666666666666e-05,
      "loss": 0.0023,
      "step": 49780
    },
    {
      "epoch": 2.6554666666666664,
      "grad_norm": 0.8253069519996643,
      "learning_rate": 3.340333333333334e-05,
      "loss": 0.0034,
      "step": 49790
    },
    {
      "epoch": 2.656,
      "grad_norm": 0.38726988434791565,
      "learning_rate": 3.3400000000000005e-05,
      "loss": 0.0032,
      "step": 49800
    },
    {
      "epoch": 2.6565333333333334,
      "grad_norm": 0.26678720116615295,
      "learning_rate": 3.339666666666667e-05,
      "loss": 0.0025,
      "step": 49810
    },
    {
      "epoch": 2.6570666666666667,
      "grad_norm": 0.12509575486183167,
      "learning_rate": 3.339333333333334e-05,
      "loss": 0.0026,
      "step": 49820
    },
    {
      "epoch": 2.6576,
      "grad_norm": 0.36537694931030273,
      "learning_rate": 3.339e-05,
      "loss": 0.0024,
      "step": 49830
    },
    {
      "epoch": 2.6581333333333332,
      "grad_norm": 0.23827126622200012,
      "learning_rate": 3.338666666666666e-05,
      "loss": 0.002,
      "step": 49840
    },
    {
      "epoch": 2.6586666666666665,
      "grad_norm": 0.2572571337223053,
      "learning_rate": 3.3383333333333336e-05,
      "loss": 0.0025,
      "step": 49850
    },
    {
      "epoch": 2.6592000000000002,
      "grad_norm": 0.12477489560842514,
      "learning_rate": 3.338e-05,
      "loss": 0.003,
      "step": 49860
    },
    {
      "epoch": 2.6597333333333335,
      "grad_norm": 0.4098542630672455,
      "learning_rate": 3.337666666666667e-05,
      "loss": 0.0026,
      "step": 49870
    },
    {
      "epoch": 2.660266666666667,
      "grad_norm": 0.25193679332733154,
      "learning_rate": 3.3373333333333335e-05,
      "loss": 0.002,
      "step": 49880
    },
    {
      "epoch": 2.6608,
      "grad_norm": 0.30465298891067505,
      "learning_rate": 3.337e-05,
      "loss": 0.0018,
      "step": 49890
    },
    {
      "epoch": 2.6613333333333333,
      "grad_norm": 0.33627215027809143,
      "learning_rate": 3.336666666666667e-05,
      "loss": 0.0025,
      "step": 49900
    },
    {
      "epoch": 2.6618666666666666,
      "grad_norm": 0.3435470163822174,
      "learning_rate": 3.336333333333333e-05,
      "loss": 0.0021,
      "step": 49910
    },
    {
      "epoch": 2.6624,
      "grad_norm": 0.07932222634553909,
      "learning_rate": 3.336e-05,
      "loss": 0.0022,
      "step": 49920
    },
    {
      "epoch": 2.662933333333333,
      "grad_norm": 0.5778352618217468,
      "learning_rate": 3.335666666666667e-05,
      "loss": 0.0034,
      "step": 49930
    },
    {
      "epoch": 2.6634666666666664,
      "grad_norm": 0.12041063606739044,
      "learning_rate": 3.335333333333334e-05,
      "loss": 0.0023,
      "step": 49940
    },
    {
      "epoch": 2.664,
      "grad_norm": 0.39631423354148865,
      "learning_rate": 3.3350000000000004e-05,
      "loss": 0.0023,
      "step": 49950
    },
    {
      "epoch": 2.6645333333333334,
      "grad_norm": 0.4003583788871765,
      "learning_rate": 3.334666666666667e-05,
      "loss": 0.0024,
      "step": 49960
    },
    {
      "epoch": 2.6650666666666667,
      "grad_norm": 0.06749232858419418,
      "learning_rate": 3.3343333333333337e-05,
      "loss": 0.0018,
      "step": 49970
    },
    {
      "epoch": 2.6656,
      "grad_norm": 0.45689353346824646,
      "learning_rate": 3.3339999999999996e-05,
      "loss": 0.0018,
      "step": 49980
    },
    {
      "epoch": 2.6661333333333332,
      "grad_norm": 0.2902294397354126,
      "learning_rate": 3.333666666666667e-05,
      "loss": 0.0023,
      "step": 49990
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.2843995690345764,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.0024,
      "step": 50000
    },
    {
      "epoch": 2.6672000000000002,
      "grad_norm": 0.30254921317100525,
      "learning_rate": 3.333e-05,
      "loss": 0.0027,
      "step": 50010
    },
    {
      "epoch": 2.6677333333333335,
      "grad_norm": 0.4896947741508484,
      "learning_rate": 3.332666666666667e-05,
      "loss": 0.0025,
      "step": 50020
    },
    {
      "epoch": 2.668266666666667,
      "grad_norm": 0.38599246740341187,
      "learning_rate": 3.332333333333333e-05,
      "loss": 0.0022,
      "step": 50030
    },
    {
      "epoch": 2.6688,
      "grad_norm": 0.06475389748811722,
      "learning_rate": 3.332e-05,
      "loss": 0.0021,
      "step": 50040
    },
    {
      "epoch": 2.6693333333333333,
      "grad_norm": 0.4387147128582001,
      "learning_rate": 3.3316666666666666e-05,
      "loss": 0.0017,
      "step": 50050
    },
    {
      "epoch": 2.6698666666666666,
      "grad_norm": 0.6719209551811218,
      "learning_rate": 3.331333333333334e-05,
      "loss": 0.0021,
      "step": 50060
    },
    {
      "epoch": 2.6704,
      "grad_norm": 0.18158689141273499,
      "learning_rate": 3.3310000000000005e-05,
      "loss": 0.0025,
      "step": 50070
    },
    {
      "epoch": 2.670933333333333,
      "grad_norm": 0.06462854146957397,
      "learning_rate": 3.330666666666667e-05,
      "loss": 0.0017,
      "step": 50080
    },
    {
      "epoch": 2.6714666666666664,
      "grad_norm": 0.24305632710456848,
      "learning_rate": 3.330333333333334e-05,
      "loss": 0.0027,
      "step": 50090
    },
    {
      "epoch": 2.672,
      "grad_norm": 0.39331841468811035,
      "learning_rate": 3.33e-05,
      "loss": 0.0024,
      "step": 50100
    },
    {
      "epoch": 2.6725333333333334,
      "grad_norm": 0.2516338527202606,
      "learning_rate": 3.329666666666667e-05,
      "loss": 0.0024,
      "step": 50110
    },
    {
      "epoch": 2.6730666666666667,
      "grad_norm": 0.09513525664806366,
      "learning_rate": 3.3293333333333335e-05,
      "loss": 0.0018,
      "step": 50120
    },
    {
      "epoch": 2.6736,
      "grad_norm": 0.20840591192245483,
      "learning_rate": 3.329e-05,
      "loss": 0.0024,
      "step": 50130
    },
    {
      "epoch": 2.6741333333333333,
      "grad_norm": 0.08682448416948318,
      "learning_rate": 3.328666666666667e-05,
      "loss": 0.0021,
      "step": 50140
    },
    {
      "epoch": 2.6746666666666665,
      "grad_norm": 0.12977303564548492,
      "learning_rate": 3.3283333333333334e-05,
      "loss": 0.0026,
      "step": 50150
    },
    {
      "epoch": 2.6752000000000002,
      "grad_norm": 0.2738852798938751,
      "learning_rate": 3.328e-05,
      "loss": 0.0019,
      "step": 50160
    },
    {
      "epoch": 2.6757333333333335,
      "grad_norm": 0.13022588193416595,
      "learning_rate": 3.3276666666666666e-05,
      "loss": 0.0016,
      "step": 50170
    },
    {
      "epoch": 2.676266666666667,
      "grad_norm": 0.15570290386676788,
      "learning_rate": 3.327333333333333e-05,
      "loss": 0.0017,
      "step": 50180
    },
    {
      "epoch": 2.6768,
      "grad_norm": 0.13955669105052948,
      "learning_rate": 3.327e-05,
      "loss": 0.0017,
      "step": 50190
    },
    {
      "epoch": 2.6773333333333333,
      "grad_norm": 0.043932151049375534,
      "learning_rate": 3.326666666666667e-05,
      "loss": 0.0025,
      "step": 50200
    },
    {
      "epoch": 2.6778666666666666,
      "grad_norm": 0.13070979714393616,
      "learning_rate": 3.326333333333334e-05,
      "loss": 0.0022,
      "step": 50210
    },
    {
      "epoch": 2.6784,
      "grad_norm": 0.12294571846723557,
      "learning_rate": 3.3260000000000003e-05,
      "loss": 0.0033,
      "step": 50220
    },
    {
      "epoch": 2.678933333333333,
      "grad_norm": 0.2901412546634674,
      "learning_rate": 3.325666666666667e-05,
      "loss": 0.0023,
      "step": 50230
    },
    {
      "epoch": 2.6794666666666664,
      "grad_norm": 0.09121236205101013,
      "learning_rate": 3.3253333333333336e-05,
      "loss": 0.0025,
      "step": 50240
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.18149782717227936,
      "learning_rate": 3.325e-05,
      "loss": 0.0021,
      "step": 50250
    },
    {
      "epoch": 2.6805333333333334,
      "grad_norm": 0.3872261047363281,
      "learning_rate": 3.324666666666667e-05,
      "loss": 0.003,
      "step": 50260
    },
    {
      "epoch": 2.6810666666666667,
      "grad_norm": 0.23055119812488556,
      "learning_rate": 3.3243333333333334e-05,
      "loss": 0.0022,
      "step": 50270
    },
    {
      "epoch": 2.6816,
      "grad_norm": 0.0948505699634552,
      "learning_rate": 3.324e-05,
      "loss": 0.0025,
      "step": 50280
    },
    {
      "epoch": 2.6821333333333333,
      "grad_norm": 0.10480087250471115,
      "learning_rate": 3.3236666666666666e-05,
      "loss": 0.0021,
      "step": 50290
    },
    {
      "epoch": 2.6826666666666665,
      "grad_norm": 0.14257922768592834,
      "learning_rate": 3.323333333333333e-05,
      "loss": 0.0022,
      "step": 50300
    },
    {
      "epoch": 2.6832000000000003,
      "grad_norm": 0.3247002363204956,
      "learning_rate": 3.323e-05,
      "loss": 0.0027,
      "step": 50310
    },
    {
      "epoch": 2.6837333333333335,
      "grad_norm": 0.2486949861049652,
      "learning_rate": 3.3226666666666665e-05,
      "loss": 0.002,
      "step": 50320
    },
    {
      "epoch": 2.684266666666667,
      "grad_norm": 0.28024446964263916,
      "learning_rate": 3.322333333333333e-05,
      "loss": 0.0023,
      "step": 50330
    },
    {
      "epoch": 2.6848,
      "grad_norm": 0.2184375375509262,
      "learning_rate": 3.3220000000000004e-05,
      "loss": 0.0023,
      "step": 50340
    },
    {
      "epoch": 2.6853333333333333,
      "grad_norm": 0.45495283603668213,
      "learning_rate": 3.321666666666667e-05,
      "loss": 0.0028,
      "step": 50350
    },
    {
      "epoch": 2.6858666666666666,
      "grad_norm": 0.14953501522541046,
      "learning_rate": 3.3213333333333336e-05,
      "loss": 0.0016,
      "step": 50360
    },
    {
      "epoch": 2.6864,
      "grad_norm": 0.30321046710014343,
      "learning_rate": 3.321e-05,
      "loss": 0.0019,
      "step": 50370
    },
    {
      "epoch": 2.686933333333333,
      "grad_norm": 0.20152157545089722,
      "learning_rate": 3.320666666666667e-05,
      "loss": 0.0035,
      "step": 50380
    },
    {
      "epoch": 2.6874666666666664,
      "grad_norm": 0.16214020550251007,
      "learning_rate": 3.3203333333333334e-05,
      "loss": 0.0024,
      "step": 50390
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 0.13623706996440887,
      "learning_rate": 3.32e-05,
      "loss": 0.002,
      "step": 50400
    },
    {
      "epoch": 2.6885333333333334,
      "grad_norm": 0.07906801253557205,
      "learning_rate": 3.3196666666666674e-05,
      "loss": 0.0017,
      "step": 50410
    },
    {
      "epoch": 2.6890666666666667,
      "grad_norm": 0.1270262449979782,
      "learning_rate": 3.319333333333334e-05,
      "loss": 0.0017,
      "step": 50420
    },
    {
      "epoch": 2.6896,
      "grad_norm": 0.05163818970322609,
      "learning_rate": 3.319e-05,
      "loss": 0.0031,
      "step": 50430
    },
    {
      "epoch": 2.6901333333333333,
      "grad_norm": 0.18845199048519135,
      "learning_rate": 3.3186666666666665e-05,
      "loss": 0.0019,
      "step": 50440
    },
    {
      "epoch": 2.6906666666666665,
      "grad_norm": 0.29782217741012573,
      "learning_rate": 3.318333333333333e-05,
      "loss": 0.0017,
      "step": 50450
    },
    {
      "epoch": 2.6912000000000003,
      "grad_norm": 0.08209893107414246,
      "learning_rate": 3.318e-05,
      "loss": 0.0021,
      "step": 50460
    },
    {
      "epoch": 2.6917333333333335,
      "grad_norm": 0.25355324149131775,
      "learning_rate": 3.317666666666667e-05,
      "loss": 0.0031,
      "step": 50470
    },
    {
      "epoch": 2.692266666666667,
      "grad_norm": 0.3036756217479706,
      "learning_rate": 3.3173333333333336e-05,
      "loss": 0.0027,
      "step": 50480
    },
    {
      "epoch": 2.6928,
      "grad_norm": 0.5197533965110779,
      "learning_rate": 3.317e-05,
      "loss": 0.0021,
      "step": 50490
    },
    {
      "epoch": 2.6933333333333334,
      "grad_norm": 0.33972373604774475,
      "learning_rate": 3.316666666666667e-05,
      "loss": 0.003,
      "step": 50500
    },
    {
      "epoch": 2.6938666666666666,
      "grad_norm": 0.1834571212530136,
      "learning_rate": 3.3163333333333335e-05,
      "loss": 0.0011,
      "step": 50510
    },
    {
      "epoch": 2.6944,
      "grad_norm": 0.40070727467536926,
      "learning_rate": 3.316e-05,
      "loss": 0.0014,
      "step": 50520
    },
    {
      "epoch": 2.694933333333333,
      "grad_norm": 0.27809637784957886,
      "learning_rate": 3.315666666666667e-05,
      "loss": 0.0024,
      "step": 50530
    },
    {
      "epoch": 2.6954666666666665,
      "grad_norm": 0.25174829363822937,
      "learning_rate": 3.315333333333333e-05,
      "loss": 0.0022,
      "step": 50540
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 0.3938809931278229,
      "learning_rate": 3.3150000000000006e-05,
      "loss": 0.0022,
      "step": 50550
    },
    {
      "epoch": 2.6965333333333334,
      "grad_norm": 0.19062024354934692,
      "learning_rate": 3.314666666666667e-05,
      "loss": 0.003,
      "step": 50560
    },
    {
      "epoch": 2.6970666666666667,
      "grad_norm": 0.19795183837413788,
      "learning_rate": 3.314333333333334e-05,
      "loss": 0.0022,
      "step": 50570
    },
    {
      "epoch": 2.6976,
      "grad_norm": 0.27308282256126404,
      "learning_rate": 3.314e-05,
      "loss": 0.0023,
      "step": 50580
    },
    {
      "epoch": 2.6981333333333333,
      "grad_norm": 0.3311012387275696,
      "learning_rate": 3.3136666666666664e-05,
      "loss": 0.0015,
      "step": 50590
    },
    {
      "epoch": 2.6986666666666665,
      "grad_norm": 0.3595982789993286,
      "learning_rate": 3.313333333333333e-05,
      "loss": 0.0026,
      "step": 50600
    },
    {
      "epoch": 2.6992000000000003,
      "grad_norm": 0.06144003942608833,
      "learning_rate": 3.313e-05,
      "loss": 0.0023,
      "step": 50610
    },
    {
      "epoch": 2.6997333333333335,
      "grad_norm": 0.040111467242240906,
      "learning_rate": 3.312666666666667e-05,
      "loss": 0.0028,
      "step": 50620
    },
    {
      "epoch": 2.700266666666667,
      "grad_norm": 0.12996375560760498,
      "learning_rate": 3.3123333333333335e-05,
      "loss": 0.0019,
      "step": 50630
    },
    {
      "epoch": 2.7008,
      "grad_norm": 0.3308746814727783,
      "learning_rate": 3.312e-05,
      "loss": 0.0025,
      "step": 50640
    },
    {
      "epoch": 2.7013333333333334,
      "grad_norm": 0.19951468706130981,
      "learning_rate": 3.311666666666667e-05,
      "loss": 0.0021,
      "step": 50650
    },
    {
      "epoch": 2.7018666666666666,
      "grad_norm": 0.17756244540214539,
      "learning_rate": 3.3113333333333334e-05,
      "loss": 0.0015,
      "step": 50660
    },
    {
      "epoch": 2.7024,
      "grad_norm": 0.13621988892555237,
      "learning_rate": 3.311e-05,
      "loss": 0.0015,
      "step": 50670
    },
    {
      "epoch": 2.702933333333333,
      "grad_norm": 0.3692651391029358,
      "learning_rate": 3.3106666666666666e-05,
      "loss": 0.0033,
      "step": 50680
    },
    {
      "epoch": 2.7034666666666665,
      "grad_norm": 0.2711504101753235,
      "learning_rate": 3.310333333333334e-05,
      "loss": 0.0029,
      "step": 50690
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 0.188801571726799,
      "learning_rate": 3.3100000000000005e-05,
      "loss": 0.0023,
      "step": 50700
    },
    {
      "epoch": 2.7045333333333335,
      "grad_norm": 0.48770204186439514,
      "learning_rate": 3.309666666666667e-05,
      "loss": 0.0024,
      "step": 50710
    },
    {
      "epoch": 2.7050666666666667,
      "grad_norm": 0.17907582223415375,
      "learning_rate": 3.309333333333334e-05,
      "loss": 0.003,
      "step": 50720
    },
    {
      "epoch": 2.7056,
      "grad_norm": 0.10253661125898361,
      "learning_rate": 3.309e-05,
      "loss": 0.0029,
      "step": 50730
    },
    {
      "epoch": 2.7061333333333333,
      "grad_norm": 0.2840716540813446,
      "learning_rate": 3.308666666666666e-05,
      "loss": 0.0023,
      "step": 50740
    },
    {
      "epoch": 2.7066666666666666,
      "grad_norm": 0.03963654860854149,
      "learning_rate": 3.3083333333333336e-05,
      "loss": 0.0022,
      "step": 50750
    },
    {
      "epoch": 2.7072000000000003,
      "grad_norm": 0.4037860929965973,
      "learning_rate": 3.308e-05,
      "loss": 0.0036,
      "step": 50760
    },
    {
      "epoch": 2.7077333333333335,
      "grad_norm": 0.05115959420800209,
      "learning_rate": 3.307666666666667e-05,
      "loss": 0.0022,
      "step": 50770
    },
    {
      "epoch": 2.708266666666667,
      "grad_norm": 0.27870887517929077,
      "learning_rate": 3.3073333333333334e-05,
      "loss": 0.0033,
      "step": 50780
    },
    {
      "epoch": 2.7088,
      "grad_norm": 0.11920250952243805,
      "learning_rate": 3.307e-05,
      "loss": 0.0024,
      "step": 50790
    },
    {
      "epoch": 2.7093333333333334,
      "grad_norm": 0.36010003089904785,
      "learning_rate": 3.3066666666666666e-05,
      "loss": 0.0019,
      "step": 50800
    },
    {
      "epoch": 2.7098666666666666,
      "grad_norm": 0.09019333869218826,
      "learning_rate": 3.306333333333333e-05,
      "loss": 0.0026,
      "step": 50810
    },
    {
      "epoch": 2.7104,
      "grad_norm": 0.240315243601799,
      "learning_rate": 3.3060000000000005e-05,
      "loss": 0.0021,
      "step": 50820
    },
    {
      "epoch": 2.710933333333333,
      "grad_norm": 0.060241490602493286,
      "learning_rate": 3.305666666666667e-05,
      "loss": 0.0031,
      "step": 50830
    },
    {
      "epoch": 2.7114666666666665,
      "grad_norm": 0.2710763216018677,
      "learning_rate": 3.305333333333334e-05,
      "loss": 0.0027,
      "step": 50840
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 0.35118722915649414,
      "learning_rate": 3.3050000000000004e-05,
      "loss": 0.0016,
      "step": 50850
    },
    {
      "epoch": 2.7125333333333335,
      "grad_norm": 0.054162297397851944,
      "learning_rate": 3.304666666666667e-05,
      "loss": 0.0027,
      "step": 50860
    },
    {
      "epoch": 2.7130666666666667,
      "grad_norm": 0.045341216027736664,
      "learning_rate": 3.3043333333333336e-05,
      "loss": 0.002,
      "step": 50870
    },
    {
      "epoch": 2.7136,
      "grad_norm": 0.0802038311958313,
      "learning_rate": 3.304e-05,
      "loss": 0.0031,
      "step": 50880
    },
    {
      "epoch": 2.7141333333333333,
      "grad_norm": 0.5169469714164734,
      "learning_rate": 3.303666666666667e-05,
      "loss": 0.0019,
      "step": 50890
    },
    {
      "epoch": 2.7146666666666666,
      "grad_norm": 0.22382861375808716,
      "learning_rate": 3.3033333333333334e-05,
      "loss": 0.0022,
      "step": 50900
    },
    {
      "epoch": 2.7152,
      "grad_norm": 0.6250612139701843,
      "learning_rate": 3.303e-05,
      "loss": 0.0023,
      "step": 50910
    },
    {
      "epoch": 2.7157333333333336,
      "grad_norm": 0.10466916114091873,
      "learning_rate": 3.302666666666667e-05,
      "loss": 0.0022,
      "step": 50920
    },
    {
      "epoch": 2.716266666666667,
      "grad_norm": 0.09150509536266327,
      "learning_rate": 3.302333333333333e-05,
      "loss": 0.002,
      "step": 50930
    },
    {
      "epoch": 2.7168,
      "grad_norm": 0.28195858001708984,
      "learning_rate": 3.302e-05,
      "loss": 0.0018,
      "step": 50940
    },
    {
      "epoch": 2.7173333333333334,
      "grad_norm": 0.31462979316711426,
      "learning_rate": 3.3016666666666665e-05,
      "loss": 0.0023,
      "step": 50950
    },
    {
      "epoch": 2.7178666666666667,
      "grad_norm": 0.15456904470920563,
      "learning_rate": 3.301333333333334e-05,
      "loss": 0.0019,
      "step": 50960
    },
    {
      "epoch": 2.7184,
      "grad_norm": 0.057071760296821594,
      "learning_rate": 3.3010000000000004e-05,
      "loss": 0.0029,
      "step": 50970
    },
    {
      "epoch": 2.718933333333333,
      "grad_norm": 0.329301655292511,
      "learning_rate": 3.300666666666667e-05,
      "loss": 0.0033,
      "step": 50980
    },
    {
      "epoch": 2.7194666666666665,
      "grad_norm": 0.06408434361219406,
      "learning_rate": 3.3003333333333336e-05,
      "loss": 0.0024,
      "step": 50990
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.11753744632005692,
      "learning_rate": 3.3e-05,
      "loss": 0.0023,
      "step": 51000
    },
    {
      "epoch": 2.7205333333333335,
      "grad_norm": 0.20458723604679108,
      "learning_rate": 3.299666666666667e-05,
      "loss": 0.0019,
      "step": 51010
    },
    {
      "epoch": 2.7210666666666667,
      "grad_norm": 0.553759753704071,
      "learning_rate": 3.2993333333333335e-05,
      "loss": 0.0028,
      "step": 51020
    },
    {
      "epoch": 2.7216,
      "grad_norm": 0.8273458480834961,
      "learning_rate": 3.299e-05,
      "loss": 0.0028,
      "step": 51030
    },
    {
      "epoch": 2.7221333333333333,
      "grad_norm": 0.05191950872540474,
      "learning_rate": 3.298666666666667e-05,
      "loss": 0.0019,
      "step": 51040
    },
    {
      "epoch": 2.7226666666666666,
      "grad_norm": 0.3410769999027252,
      "learning_rate": 3.298333333333333e-05,
      "loss": 0.0029,
      "step": 51050
    },
    {
      "epoch": 2.7232,
      "grad_norm": 0.3619121313095093,
      "learning_rate": 3.298e-05,
      "loss": 0.0022,
      "step": 51060
    },
    {
      "epoch": 2.7237333333333336,
      "grad_norm": 0.3977789878845215,
      "learning_rate": 3.2976666666666665e-05,
      "loss": 0.0024,
      "step": 51070
    },
    {
      "epoch": 2.724266666666667,
      "grad_norm": 0.772716224193573,
      "learning_rate": 3.297333333333333e-05,
      "loss": 0.0021,
      "step": 51080
    },
    {
      "epoch": 2.7248,
      "grad_norm": 0.6083506941795349,
      "learning_rate": 3.297e-05,
      "loss": 0.0018,
      "step": 51090
    },
    {
      "epoch": 2.7253333333333334,
      "grad_norm": 0.25705957412719727,
      "learning_rate": 3.296666666666667e-05,
      "loss": 0.0026,
      "step": 51100
    },
    {
      "epoch": 2.7258666666666667,
      "grad_norm": 0.047129593789577484,
      "learning_rate": 3.296333333333334e-05,
      "loss": 0.002,
      "step": 51110
    },
    {
      "epoch": 2.7264,
      "grad_norm": 0.14534078538417816,
      "learning_rate": 3.296e-05,
      "loss": 0.0033,
      "step": 51120
    },
    {
      "epoch": 2.726933333333333,
      "grad_norm": 0.14912495017051697,
      "learning_rate": 3.295666666666667e-05,
      "loss": 0.0026,
      "step": 51130
    },
    {
      "epoch": 2.7274666666666665,
      "grad_norm": 0.1423257440328598,
      "learning_rate": 3.2953333333333335e-05,
      "loss": 0.0024,
      "step": 51140
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 0.26045966148376465,
      "learning_rate": 3.295e-05,
      "loss": 0.0021,
      "step": 51150
    },
    {
      "epoch": 2.7285333333333335,
      "grad_norm": 0.051470186561346054,
      "learning_rate": 3.294666666666667e-05,
      "loss": 0.0025,
      "step": 51160
    },
    {
      "epoch": 2.7290666666666668,
      "grad_norm": 0.25937893986701965,
      "learning_rate": 3.294333333333334e-05,
      "loss": 0.0024,
      "step": 51170
    },
    {
      "epoch": 2.7296,
      "grad_norm": 0.4299509525299072,
      "learning_rate": 3.2940000000000006e-05,
      "loss": 0.003,
      "step": 51180
    },
    {
      "epoch": 2.7301333333333333,
      "grad_norm": 0.291109174489975,
      "learning_rate": 3.2936666666666666e-05,
      "loss": 0.0021,
      "step": 51190
    },
    {
      "epoch": 2.7306666666666666,
      "grad_norm": 0.579608142375946,
      "learning_rate": 3.293333333333333e-05,
      "loss": 0.002,
      "step": 51200
    },
    {
      "epoch": 2.7312,
      "grad_norm": 0.3352499306201935,
      "learning_rate": 3.293e-05,
      "loss": 0.0031,
      "step": 51210
    },
    {
      "epoch": 2.7317333333333336,
      "grad_norm": 0.04761485755443573,
      "learning_rate": 3.2926666666666664e-05,
      "loss": 0.0023,
      "step": 51220
    },
    {
      "epoch": 2.732266666666667,
      "grad_norm": 0.07169181853532791,
      "learning_rate": 3.292333333333334e-05,
      "loss": 0.0023,
      "step": 51230
    },
    {
      "epoch": 2.7328,
      "grad_norm": 0.1607760190963745,
      "learning_rate": 3.292e-05,
      "loss": 0.0027,
      "step": 51240
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 0.15338696539402008,
      "learning_rate": 3.291666666666667e-05,
      "loss": 0.0029,
      "step": 51250
    },
    {
      "epoch": 2.7338666666666667,
      "grad_norm": 0.18417811393737793,
      "learning_rate": 3.2913333333333336e-05,
      "loss": 0.0023,
      "step": 51260
    },
    {
      "epoch": 2.7344,
      "grad_norm": 0.9302047491073608,
      "learning_rate": 3.291e-05,
      "loss": 0.002,
      "step": 51270
    },
    {
      "epoch": 2.734933333333333,
      "grad_norm": 0.4914119839668274,
      "learning_rate": 3.290666666666667e-05,
      "loss": 0.0026,
      "step": 51280
    },
    {
      "epoch": 2.7354666666666665,
      "grad_norm": 0.5500096678733826,
      "learning_rate": 3.2903333333333334e-05,
      "loss": 0.0027,
      "step": 51290
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 0.7052038908004761,
      "learning_rate": 3.29e-05,
      "loss": 0.0021,
      "step": 51300
    },
    {
      "epoch": 2.7365333333333335,
      "grad_norm": 0.09424890577793121,
      "learning_rate": 3.289666666666667e-05,
      "loss": 0.0021,
      "step": 51310
    },
    {
      "epoch": 2.7370666666666668,
      "grad_norm": 0.33305034041404724,
      "learning_rate": 3.289333333333334e-05,
      "loss": 0.0023,
      "step": 51320
    },
    {
      "epoch": 2.7376,
      "grad_norm": 0.19826510548591614,
      "learning_rate": 3.2890000000000005e-05,
      "loss": 0.0028,
      "step": 51330
    },
    {
      "epoch": 2.7381333333333333,
      "grad_norm": 0.36533892154693604,
      "learning_rate": 3.2886666666666665e-05,
      "loss": 0.0031,
      "step": 51340
    },
    {
      "epoch": 2.7386666666666666,
      "grad_norm": 0.4318819046020508,
      "learning_rate": 3.288333333333333e-05,
      "loss": 0.0023,
      "step": 51350
    },
    {
      "epoch": 2.7392,
      "grad_norm": 0.04605439305305481,
      "learning_rate": 3.288e-05,
      "loss": 0.0019,
      "step": 51360
    },
    {
      "epoch": 2.7397333333333336,
      "grad_norm": 0.24181225895881653,
      "learning_rate": 3.287666666666667e-05,
      "loss": 0.0021,
      "step": 51370
    },
    {
      "epoch": 2.740266666666667,
      "grad_norm": 0.2846747636795044,
      "learning_rate": 3.2873333333333336e-05,
      "loss": 0.0021,
      "step": 51380
    },
    {
      "epoch": 2.7408,
      "grad_norm": 0.2885794937610626,
      "learning_rate": 3.287e-05,
      "loss": 0.0028,
      "step": 51390
    },
    {
      "epoch": 2.7413333333333334,
      "grad_norm": 0.07523898035287857,
      "learning_rate": 3.286666666666667e-05,
      "loss": 0.0018,
      "step": 51400
    },
    {
      "epoch": 2.7418666666666667,
      "grad_norm": 0.03823462128639221,
      "learning_rate": 3.2863333333333334e-05,
      "loss": 0.0023,
      "step": 51410
    },
    {
      "epoch": 2.7424,
      "grad_norm": 0.18868637084960938,
      "learning_rate": 3.286e-05,
      "loss": 0.0016,
      "step": 51420
    },
    {
      "epoch": 2.7429333333333332,
      "grad_norm": 0.0899202972650528,
      "learning_rate": 3.285666666666667e-05,
      "loss": 0.0021,
      "step": 51430
    },
    {
      "epoch": 2.7434666666666665,
      "grad_norm": 0.3921280801296234,
      "learning_rate": 3.285333333333333e-05,
      "loss": 0.0022,
      "step": 51440
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 0.2961236238479614,
      "learning_rate": 3.2850000000000006e-05,
      "loss": 0.0032,
      "step": 51450
    },
    {
      "epoch": 2.7445333333333335,
      "grad_norm": 0.24204745888710022,
      "learning_rate": 3.284666666666667e-05,
      "loss": 0.0029,
      "step": 51460
    },
    {
      "epoch": 2.7450666666666668,
      "grad_norm": 0.20763719081878662,
      "learning_rate": 3.284333333333334e-05,
      "loss": 0.0019,
      "step": 51470
    },
    {
      "epoch": 2.7456,
      "grad_norm": 0.2314734160900116,
      "learning_rate": 3.2840000000000004e-05,
      "loss": 0.0021,
      "step": 51480
    },
    {
      "epoch": 2.7461333333333333,
      "grad_norm": 0.33166182041168213,
      "learning_rate": 3.2836666666666663e-05,
      "loss": 0.0023,
      "step": 51490
    },
    {
      "epoch": 2.7466666666666666,
      "grad_norm": 0.10600800067186356,
      "learning_rate": 3.283333333333333e-05,
      "loss": 0.0027,
      "step": 51500
    },
    {
      "epoch": 2.7472,
      "grad_norm": 0.2957325577735901,
      "learning_rate": 3.283e-05,
      "loss": 0.0018,
      "step": 51510
    },
    {
      "epoch": 2.7477333333333336,
      "grad_norm": 0.23145155608654022,
      "learning_rate": 3.282666666666667e-05,
      "loss": 0.0027,
      "step": 51520
    },
    {
      "epoch": 2.748266666666667,
      "grad_norm": 0.3360806405544281,
      "learning_rate": 3.2823333333333335e-05,
      "loss": 0.0023,
      "step": 51530
    },
    {
      "epoch": 2.7488,
      "grad_norm": 0.5893414616584778,
      "learning_rate": 3.282e-05,
      "loss": 0.0032,
      "step": 51540
    },
    {
      "epoch": 2.7493333333333334,
      "grad_norm": 0.16753919422626495,
      "learning_rate": 3.281666666666667e-05,
      "loss": 0.0025,
      "step": 51550
    },
    {
      "epoch": 2.7498666666666667,
      "grad_norm": 0.329926073551178,
      "learning_rate": 3.281333333333333e-05,
      "loss": 0.0027,
      "step": 51560
    },
    {
      "epoch": 2.7504,
      "grad_norm": 0.5217427611351013,
      "learning_rate": 3.281e-05,
      "loss": 0.0027,
      "step": 51570
    },
    {
      "epoch": 2.7509333333333332,
      "grad_norm": 0.36709871888160706,
      "learning_rate": 3.280666666666667e-05,
      "loss": 0.0034,
      "step": 51580
    },
    {
      "epoch": 2.7514666666666665,
      "grad_norm": 0.3426016569137573,
      "learning_rate": 3.280333333333334e-05,
      "loss": 0.0027,
      "step": 51590
    },
    {
      "epoch": 2.752,
      "grad_norm": 0.03956230357289314,
      "learning_rate": 3.2800000000000004e-05,
      "loss": 0.0017,
      "step": 51600
    },
    {
      "epoch": 2.7525333333333335,
      "grad_norm": 0.12719294428825378,
      "learning_rate": 3.279666666666667e-05,
      "loss": 0.0031,
      "step": 51610
    },
    {
      "epoch": 2.7530666666666668,
      "grad_norm": 0.43284329771995544,
      "learning_rate": 3.279333333333334e-05,
      "loss": 0.0027,
      "step": 51620
    },
    {
      "epoch": 2.7536,
      "grad_norm": 0.3158975839614868,
      "learning_rate": 3.279e-05,
      "loss": 0.002,
      "step": 51630
    },
    {
      "epoch": 2.7541333333333333,
      "grad_norm": 0.14862629771232605,
      "learning_rate": 3.278666666666666e-05,
      "loss": 0.0027,
      "step": 51640
    },
    {
      "epoch": 2.7546666666666666,
      "grad_norm": 0.5963631272315979,
      "learning_rate": 3.2783333333333335e-05,
      "loss": 0.0028,
      "step": 51650
    },
    {
      "epoch": 2.7552,
      "grad_norm": 0.10475639253854752,
      "learning_rate": 3.278e-05,
      "loss": 0.002,
      "step": 51660
    },
    {
      "epoch": 2.7557333333333336,
      "grad_norm": 0.14215363562107086,
      "learning_rate": 3.277666666666667e-05,
      "loss": 0.0021,
      "step": 51670
    },
    {
      "epoch": 2.756266666666667,
      "grad_norm": 0.48989754915237427,
      "learning_rate": 3.2773333333333334e-05,
      "loss": 0.0021,
      "step": 51680
    },
    {
      "epoch": 2.7568,
      "grad_norm": 0.13642247021198273,
      "learning_rate": 3.277e-05,
      "loss": 0.0016,
      "step": 51690
    },
    {
      "epoch": 2.7573333333333334,
      "grad_norm": 0.5929235219955444,
      "learning_rate": 3.2766666666666666e-05,
      "loss": 0.0018,
      "step": 51700
    },
    {
      "epoch": 2.7578666666666667,
      "grad_norm": 0.2801702320575714,
      "learning_rate": 3.276333333333333e-05,
      "loss": 0.0021,
      "step": 51710
    },
    {
      "epoch": 2.7584,
      "grad_norm": 0.13729742169380188,
      "learning_rate": 3.2760000000000005e-05,
      "loss": 0.0042,
      "step": 51720
    },
    {
      "epoch": 2.7589333333333332,
      "grad_norm": 0.1466674506664276,
      "learning_rate": 3.275666666666667e-05,
      "loss": 0.0032,
      "step": 51730
    },
    {
      "epoch": 2.7594666666666665,
      "grad_norm": 0.39369624853134155,
      "learning_rate": 3.275333333333334e-05,
      "loss": 0.0021,
      "step": 51740
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.169197216629982,
      "learning_rate": 3.275e-05,
      "loss": 0.0021,
      "step": 51750
    },
    {
      "epoch": 2.760533333333333,
      "grad_norm": 0.07348395138978958,
      "learning_rate": 3.274666666666667e-05,
      "loss": 0.003,
      "step": 51760
    },
    {
      "epoch": 2.761066666666667,
      "grad_norm": 0.18023309111595154,
      "learning_rate": 3.2743333333333335e-05,
      "loss": 0.003,
      "step": 51770
    },
    {
      "epoch": 2.7616,
      "grad_norm": 0.48092466592788696,
      "learning_rate": 3.274e-05,
      "loss": 0.0017,
      "step": 51780
    },
    {
      "epoch": 2.7621333333333333,
      "grad_norm": 0.5010347962379456,
      "learning_rate": 3.273666666666667e-05,
      "loss": 0.0035,
      "step": 51790
    },
    {
      "epoch": 2.7626666666666666,
      "grad_norm": 0.2295241802930832,
      "learning_rate": 3.2733333333333334e-05,
      "loss": 0.0026,
      "step": 51800
    },
    {
      "epoch": 2.7632,
      "grad_norm": 0.42167720198631287,
      "learning_rate": 3.273e-05,
      "loss": 0.0016,
      "step": 51810
    },
    {
      "epoch": 2.7637333333333336,
      "grad_norm": 0.21186812222003937,
      "learning_rate": 3.2726666666666666e-05,
      "loss": 0.0021,
      "step": 51820
    },
    {
      "epoch": 2.764266666666667,
      "grad_norm": 0.4083339273929596,
      "learning_rate": 3.272333333333333e-05,
      "loss": 0.002,
      "step": 51830
    },
    {
      "epoch": 2.7648,
      "grad_norm": 0.4599847197532654,
      "learning_rate": 3.272e-05,
      "loss": 0.0027,
      "step": 51840
    },
    {
      "epoch": 2.7653333333333334,
      "grad_norm": 0.15051564574241638,
      "learning_rate": 3.2716666666666665e-05,
      "loss": 0.0032,
      "step": 51850
    },
    {
      "epoch": 2.7658666666666667,
      "grad_norm": 0.3197031319141388,
      "learning_rate": 3.271333333333334e-05,
      "loss": 0.0029,
      "step": 51860
    },
    {
      "epoch": 2.7664,
      "grad_norm": 0.11432210355997086,
      "learning_rate": 3.2710000000000004e-05,
      "loss": 0.0017,
      "step": 51870
    },
    {
      "epoch": 2.7669333333333332,
      "grad_norm": 0.5204726457595825,
      "learning_rate": 3.270666666666667e-05,
      "loss": 0.0025,
      "step": 51880
    },
    {
      "epoch": 2.7674666666666665,
      "grad_norm": 0.367531418800354,
      "learning_rate": 3.2703333333333336e-05,
      "loss": 0.0025,
      "step": 51890
    },
    {
      "epoch": 2.768,
      "grad_norm": 0.42815133929252625,
      "learning_rate": 3.27e-05,
      "loss": 0.0034,
      "step": 51900
    },
    {
      "epoch": 2.768533333333333,
      "grad_norm": 0.276426762342453,
      "learning_rate": 3.269666666666667e-05,
      "loss": 0.0021,
      "step": 51910
    },
    {
      "epoch": 2.769066666666667,
      "grad_norm": 0.12326297163963318,
      "learning_rate": 3.2693333333333334e-05,
      "loss": 0.0021,
      "step": 51920
    },
    {
      "epoch": 2.7696,
      "grad_norm": 0.10515058785676956,
      "learning_rate": 3.269000000000001e-05,
      "loss": 0.0015,
      "step": 51930
    },
    {
      "epoch": 2.7701333333333333,
      "grad_norm": 0.28858840465545654,
      "learning_rate": 3.268666666666667e-05,
      "loss": 0.0022,
      "step": 51940
    },
    {
      "epoch": 2.7706666666666666,
      "grad_norm": 0.614342451095581,
      "learning_rate": 3.268333333333333e-05,
      "loss": 0.0024,
      "step": 51950
    },
    {
      "epoch": 2.7712,
      "grad_norm": 0.24637757241725922,
      "learning_rate": 3.268e-05,
      "loss": 0.0025,
      "step": 51960
    },
    {
      "epoch": 2.7717333333333336,
      "grad_norm": 0.051161687821149826,
      "learning_rate": 3.2676666666666665e-05,
      "loss": 0.0022,
      "step": 51970
    },
    {
      "epoch": 2.772266666666667,
      "grad_norm": 0.13862138986587524,
      "learning_rate": 3.267333333333333e-05,
      "loss": 0.002,
      "step": 51980
    },
    {
      "epoch": 2.7728,
      "grad_norm": 0.31530702114105225,
      "learning_rate": 3.267e-05,
      "loss": 0.0029,
      "step": 51990
    },
    {
      "epoch": 2.7733333333333334,
      "grad_norm": 0.17673788964748383,
      "learning_rate": 3.266666666666667e-05,
      "loss": 0.0027,
      "step": 52000
    },
    {
      "epoch": 2.7738666666666667,
      "grad_norm": 0.6094231605529785,
      "learning_rate": 3.2663333333333336e-05,
      "loss": 0.0026,
      "step": 52010
    },
    {
      "epoch": 2.7744,
      "grad_norm": 0.40938600897789,
      "learning_rate": 3.266e-05,
      "loss": 0.0028,
      "step": 52020
    },
    {
      "epoch": 2.7749333333333333,
      "grad_norm": 0.827621579170227,
      "learning_rate": 3.265666666666667e-05,
      "loss": 0.0024,
      "step": 52030
    },
    {
      "epoch": 2.7754666666666665,
      "grad_norm": 0.045766156166791916,
      "learning_rate": 3.2653333333333335e-05,
      "loss": 0.0019,
      "step": 52040
    },
    {
      "epoch": 2.776,
      "grad_norm": 0.31434527039527893,
      "learning_rate": 3.265e-05,
      "loss": 0.0019,
      "step": 52050
    },
    {
      "epoch": 2.776533333333333,
      "grad_norm": 0.26770639419555664,
      "learning_rate": 3.264666666666667e-05,
      "loss": 0.0031,
      "step": 52060
    },
    {
      "epoch": 2.777066666666667,
      "grad_norm": 0.14032000303268433,
      "learning_rate": 3.264333333333334e-05,
      "loss": 0.0022,
      "step": 52070
    },
    {
      "epoch": 2.7776,
      "grad_norm": 0.3896466791629791,
      "learning_rate": 3.2640000000000006e-05,
      "loss": 0.0023,
      "step": 52080
    },
    {
      "epoch": 2.7781333333333333,
      "grad_norm": 0.42744219303131104,
      "learning_rate": 3.263666666666667e-05,
      "loss": 0.0016,
      "step": 52090
    },
    {
      "epoch": 2.7786666666666666,
      "grad_norm": 0.21258053183555603,
      "learning_rate": 3.263333333333333e-05,
      "loss": 0.002,
      "step": 52100
    },
    {
      "epoch": 2.7792,
      "grad_norm": 0.5657294392585754,
      "learning_rate": 3.263e-05,
      "loss": 0.0027,
      "step": 52110
    },
    {
      "epoch": 2.779733333333333,
      "grad_norm": 0.3417894244194031,
      "learning_rate": 3.2626666666666664e-05,
      "loss": 0.002,
      "step": 52120
    },
    {
      "epoch": 2.780266666666667,
      "grad_norm": 0.10003570467233658,
      "learning_rate": 3.262333333333334e-05,
      "loss": 0.0021,
      "step": 52130
    },
    {
      "epoch": 2.7808,
      "grad_norm": 0.5897480249404907,
      "learning_rate": 3.262e-05,
      "loss": 0.0034,
      "step": 52140
    },
    {
      "epoch": 2.7813333333333334,
      "grad_norm": 0.21393658220767975,
      "learning_rate": 3.261666666666667e-05,
      "loss": 0.0019,
      "step": 52150
    },
    {
      "epoch": 2.7818666666666667,
      "grad_norm": 0.06803326308727264,
      "learning_rate": 3.2613333333333335e-05,
      "loss": 0.0024,
      "step": 52160
    },
    {
      "epoch": 2.7824,
      "grad_norm": 0.15981298685073853,
      "learning_rate": 3.261e-05,
      "loss": 0.0028,
      "step": 52170
    },
    {
      "epoch": 2.7829333333333333,
      "grad_norm": 0.09868958592414856,
      "learning_rate": 3.260666666666667e-05,
      "loss": 0.0024,
      "step": 52180
    },
    {
      "epoch": 2.7834666666666665,
      "grad_norm": 0.29177308082580566,
      "learning_rate": 3.2603333333333333e-05,
      "loss": 0.002,
      "step": 52190
    },
    {
      "epoch": 2.784,
      "grad_norm": 0.06964530050754547,
      "learning_rate": 3.26e-05,
      "loss": 0.0019,
      "step": 52200
    },
    {
      "epoch": 2.784533333333333,
      "grad_norm": 0.2876344621181488,
      "learning_rate": 3.259666666666667e-05,
      "loss": 0.0021,
      "step": 52210
    },
    {
      "epoch": 2.785066666666667,
      "grad_norm": 0.36376988887786865,
      "learning_rate": 3.259333333333334e-05,
      "loss": 0.003,
      "step": 52220
    },
    {
      "epoch": 2.7856,
      "grad_norm": 0.31063076853752136,
      "learning_rate": 3.2590000000000005e-05,
      "loss": 0.0024,
      "step": 52230
    },
    {
      "epoch": 2.7861333333333334,
      "grad_norm": 0.37559786438941956,
      "learning_rate": 3.258666666666667e-05,
      "loss": 0.0016,
      "step": 52240
    },
    {
      "epoch": 2.7866666666666666,
      "grad_norm": 0.14791782200336456,
      "learning_rate": 3.258333333333333e-05,
      "loss": 0.0024,
      "step": 52250
    },
    {
      "epoch": 2.7872,
      "grad_norm": 0.1951516568660736,
      "learning_rate": 3.2579999999999996e-05,
      "loss": 0.002,
      "step": 52260
    },
    {
      "epoch": 2.787733333333333,
      "grad_norm": 0.16602979600429535,
      "learning_rate": 3.257666666666667e-05,
      "loss": 0.0024,
      "step": 52270
    },
    {
      "epoch": 2.788266666666667,
      "grad_norm": 0.6002779006958008,
      "learning_rate": 3.2573333333333335e-05,
      "loss": 0.0023,
      "step": 52280
    },
    {
      "epoch": 2.7888,
      "grad_norm": 0.6300024390220642,
      "learning_rate": 3.257e-05,
      "loss": 0.0025,
      "step": 52290
    },
    {
      "epoch": 2.7893333333333334,
      "grad_norm": 0.13776253163814545,
      "learning_rate": 3.256666666666667e-05,
      "loss": 0.0018,
      "step": 52300
    },
    {
      "epoch": 2.7898666666666667,
      "grad_norm": 0.5112775564193726,
      "learning_rate": 3.2563333333333334e-05,
      "loss": 0.0026,
      "step": 52310
    },
    {
      "epoch": 2.7904,
      "grad_norm": 0.44082844257354736,
      "learning_rate": 3.256e-05,
      "loss": 0.0024,
      "step": 52320
    },
    {
      "epoch": 2.7909333333333333,
      "grad_norm": 0.22615738213062286,
      "learning_rate": 3.2556666666666666e-05,
      "loss": 0.0019,
      "step": 52330
    },
    {
      "epoch": 2.7914666666666665,
      "grad_norm": 0.13769914209842682,
      "learning_rate": 3.255333333333334e-05,
      "loss": 0.0039,
      "step": 52340
    },
    {
      "epoch": 2.792,
      "grad_norm": 0.17833495140075684,
      "learning_rate": 3.2550000000000005e-05,
      "loss": 0.0018,
      "step": 52350
    },
    {
      "epoch": 2.792533333333333,
      "grad_norm": 0.19436539709568024,
      "learning_rate": 3.254666666666667e-05,
      "loss": 0.0022,
      "step": 52360
    },
    {
      "epoch": 2.793066666666667,
      "grad_norm": 0.544407844543457,
      "learning_rate": 3.254333333333334e-05,
      "loss": 0.0018,
      "step": 52370
    },
    {
      "epoch": 2.7936,
      "grad_norm": 0.22570869326591492,
      "learning_rate": 3.2540000000000004e-05,
      "loss": 0.0018,
      "step": 52380
    },
    {
      "epoch": 2.7941333333333334,
      "grad_norm": 0.5483946800231934,
      "learning_rate": 3.253666666666667e-05,
      "loss": 0.0024,
      "step": 52390
    },
    {
      "epoch": 2.7946666666666666,
      "grad_norm": 0.12757785618305206,
      "learning_rate": 3.253333333333333e-05,
      "loss": 0.0023,
      "step": 52400
    },
    {
      "epoch": 2.7952,
      "grad_norm": 0.27212458848953247,
      "learning_rate": 3.253e-05,
      "loss": 0.0028,
      "step": 52410
    },
    {
      "epoch": 2.795733333333333,
      "grad_norm": 0.34898462891578674,
      "learning_rate": 3.252666666666667e-05,
      "loss": 0.0044,
      "step": 52420
    },
    {
      "epoch": 2.796266666666667,
      "grad_norm": 0.04766366630792618,
      "learning_rate": 3.2523333333333334e-05,
      "loss": 0.0019,
      "step": 52430
    },
    {
      "epoch": 2.7968,
      "grad_norm": 0.07374987751245499,
      "learning_rate": 3.252e-05,
      "loss": 0.003,
      "step": 52440
    },
    {
      "epoch": 2.7973333333333334,
      "grad_norm": 0.04285847768187523,
      "learning_rate": 3.2516666666666666e-05,
      "loss": 0.0029,
      "step": 52450
    },
    {
      "epoch": 2.7978666666666667,
      "grad_norm": 0.0918353721499443,
      "learning_rate": 3.251333333333333e-05,
      "loss": 0.0018,
      "step": 52460
    },
    {
      "epoch": 2.7984,
      "grad_norm": 0.31166476011276245,
      "learning_rate": 3.251e-05,
      "loss": 0.0034,
      "step": 52470
    },
    {
      "epoch": 2.7989333333333333,
      "grad_norm": 0.049177464097738266,
      "learning_rate": 3.250666666666667e-05,
      "loss": 0.0019,
      "step": 52480
    },
    {
      "epoch": 2.7994666666666665,
      "grad_norm": 0.44737961888313293,
      "learning_rate": 3.250333333333334e-05,
      "loss": 0.0015,
      "step": 52490
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.4850170314311981,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 0.0015,
      "step": 52500
    },
    {
      "epoch": 2.800533333333333,
      "grad_norm": 0.4282807409763336,
      "learning_rate": 3.249666666666667e-05,
      "loss": 0.0016,
      "step": 52510
    },
    {
      "epoch": 2.801066666666667,
      "grad_norm": 0.3175000548362732,
      "learning_rate": 3.2493333333333336e-05,
      "loss": 0.0022,
      "step": 52520
    },
    {
      "epoch": 2.8016,
      "grad_norm": 0.21374303102493286,
      "learning_rate": 3.249e-05,
      "loss": 0.0037,
      "step": 52530
    },
    {
      "epoch": 2.8021333333333334,
      "grad_norm": 0.6284618377685547,
      "learning_rate": 3.248666666666667e-05,
      "loss": 0.0027,
      "step": 52540
    },
    {
      "epoch": 2.8026666666666666,
      "grad_norm": 0.04005914553999901,
      "learning_rate": 3.2483333333333335e-05,
      "loss": 0.0018,
      "step": 52550
    },
    {
      "epoch": 2.8032,
      "grad_norm": 0.0587017722427845,
      "learning_rate": 3.248e-05,
      "loss": 0.0021,
      "step": 52560
    },
    {
      "epoch": 2.803733333333333,
      "grad_norm": 0.27808877825737,
      "learning_rate": 3.247666666666667e-05,
      "loss": 0.0031,
      "step": 52570
    },
    {
      "epoch": 2.804266666666667,
      "grad_norm": 0.23099727928638458,
      "learning_rate": 3.247333333333333e-05,
      "loss": 0.0032,
      "step": 52580
    },
    {
      "epoch": 2.8048,
      "grad_norm": 0.04992299526929855,
      "learning_rate": 3.247e-05,
      "loss": 0.0023,
      "step": 52590
    },
    {
      "epoch": 2.8053333333333335,
      "grad_norm": 0.5646626353263855,
      "learning_rate": 3.2466666666666665e-05,
      "loss": 0.002,
      "step": 52600
    },
    {
      "epoch": 2.8058666666666667,
      "grad_norm": 0.5540156364440918,
      "learning_rate": 3.246333333333333e-05,
      "loss": 0.002,
      "step": 52610
    },
    {
      "epoch": 2.8064,
      "grad_norm": 0.4512837827205658,
      "learning_rate": 3.2460000000000004e-05,
      "loss": 0.0023,
      "step": 52620
    },
    {
      "epoch": 2.8069333333333333,
      "grad_norm": 0.360010027885437,
      "learning_rate": 3.245666666666667e-05,
      "loss": 0.002,
      "step": 52630
    },
    {
      "epoch": 2.8074666666666666,
      "grad_norm": 0.12393318861722946,
      "learning_rate": 3.2453333333333337e-05,
      "loss": 0.0022,
      "step": 52640
    },
    {
      "epoch": 2.808,
      "grad_norm": 0.37643033266067505,
      "learning_rate": 3.245e-05,
      "loss": 0.0017,
      "step": 52650
    },
    {
      "epoch": 2.808533333333333,
      "grad_norm": 0.20882496237754822,
      "learning_rate": 3.244666666666667e-05,
      "loss": 0.0027,
      "step": 52660
    },
    {
      "epoch": 2.809066666666667,
      "grad_norm": 0.11957982927560806,
      "learning_rate": 3.2443333333333335e-05,
      "loss": 0.0029,
      "step": 52670
    },
    {
      "epoch": 2.8096,
      "grad_norm": 0.30855798721313477,
      "learning_rate": 3.244e-05,
      "loss": 0.0022,
      "step": 52680
    },
    {
      "epoch": 2.8101333333333334,
      "grad_norm": 0.45590925216674805,
      "learning_rate": 3.2436666666666674e-05,
      "loss": 0.002,
      "step": 52690
    },
    {
      "epoch": 2.8106666666666666,
      "grad_norm": 0.07331492006778717,
      "learning_rate": 3.243333333333333e-05,
      "loss": 0.0018,
      "step": 52700
    },
    {
      "epoch": 2.8112,
      "grad_norm": 0.6828312277793884,
      "learning_rate": 3.243e-05,
      "loss": 0.0019,
      "step": 52710
    },
    {
      "epoch": 2.811733333333333,
      "grad_norm": 0.6006627678871155,
      "learning_rate": 3.2426666666666666e-05,
      "loss": 0.002,
      "step": 52720
    },
    {
      "epoch": 2.812266666666667,
      "grad_norm": 0.4833650290966034,
      "learning_rate": 3.242333333333333e-05,
      "loss": 0.0033,
      "step": 52730
    },
    {
      "epoch": 2.8128,
      "grad_norm": 0.1035839393734932,
      "learning_rate": 3.242e-05,
      "loss": 0.0026,
      "step": 52740
    },
    {
      "epoch": 2.8133333333333335,
      "grad_norm": 0.2830950617790222,
      "learning_rate": 3.2416666666666664e-05,
      "loss": 0.0016,
      "step": 52750
    },
    {
      "epoch": 2.8138666666666667,
      "grad_norm": 0.15400399267673492,
      "learning_rate": 3.241333333333334e-05,
      "loss": 0.0021,
      "step": 52760
    },
    {
      "epoch": 2.8144,
      "grad_norm": 0.30885136127471924,
      "learning_rate": 3.241e-05,
      "loss": 0.0014,
      "step": 52770
    },
    {
      "epoch": 2.8149333333333333,
      "grad_norm": 0.27750205993652344,
      "learning_rate": 3.240666666666667e-05,
      "loss": 0.002,
      "step": 52780
    },
    {
      "epoch": 2.8154666666666666,
      "grad_norm": 0.14289973676204681,
      "learning_rate": 3.2403333333333335e-05,
      "loss": 0.0025,
      "step": 52790
    },
    {
      "epoch": 2.816,
      "grad_norm": 0.1730358749628067,
      "learning_rate": 3.24e-05,
      "loss": 0.0017,
      "step": 52800
    },
    {
      "epoch": 2.816533333333333,
      "grad_norm": 0.33061128854751587,
      "learning_rate": 3.239666666666667e-05,
      "loss": 0.0022,
      "step": 52810
    },
    {
      "epoch": 2.817066666666667,
      "grad_norm": 0.2507798373699188,
      "learning_rate": 3.2393333333333334e-05,
      "loss": 0.0022,
      "step": 52820
    },
    {
      "epoch": 2.8176,
      "grad_norm": 0.33695369958877563,
      "learning_rate": 3.239000000000001e-05,
      "loss": 0.0028,
      "step": 52830
    },
    {
      "epoch": 2.8181333333333334,
      "grad_norm": 0.32659095525741577,
      "learning_rate": 3.238666666666667e-05,
      "loss": 0.0024,
      "step": 52840
    },
    {
      "epoch": 2.8186666666666667,
      "grad_norm": 0.34250590205192566,
      "learning_rate": 3.238333333333333e-05,
      "loss": 0.002,
      "step": 52850
    },
    {
      "epoch": 2.8192,
      "grad_norm": 0.49877578020095825,
      "learning_rate": 3.238e-05,
      "loss": 0.0018,
      "step": 52860
    },
    {
      "epoch": 2.819733333333333,
      "grad_norm": 0.33456099033355713,
      "learning_rate": 3.2376666666666664e-05,
      "loss": 0.0029,
      "step": 52870
    },
    {
      "epoch": 2.820266666666667,
      "grad_norm": 0.07150361686944962,
      "learning_rate": 3.237333333333333e-05,
      "loss": 0.0026,
      "step": 52880
    },
    {
      "epoch": 2.8208,
      "grad_norm": 0.46103063225746155,
      "learning_rate": 3.2370000000000003e-05,
      "loss": 0.0036,
      "step": 52890
    },
    {
      "epoch": 2.8213333333333335,
      "grad_norm": 0.6193074584007263,
      "learning_rate": 3.236666666666667e-05,
      "loss": 0.0026,
      "step": 52900
    },
    {
      "epoch": 2.8218666666666667,
      "grad_norm": 0.7026321887969971,
      "learning_rate": 3.2363333333333336e-05,
      "loss": 0.0024,
      "step": 52910
    },
    {
      "epoch": 2.8224,
      "grad_norm": 0.2851825952529907,
      "learning_rate": 3.236e-05,
      "loss": 0.0019,
      "step": 52920
    },
    {
      "epoch": 2.8229333333333333,
      "grad_norm": 0.18973411619663239,
      "learning_rate": 3.235666666666667e-05,
      "loss": 0.0027,
      "step": 52930
    },
    {
      "epoch": 2.8234666666666666,
      "grad_norm": 0.08100155740976334,
      "learning_rate": 3.2353333333333334e-05,
      "loss": 0.0016,
      "step": 52940
    },
    {
      "epoch": 2.824,
      "grad_norm": 0.5936335325241089,
      "learning_rate": 3.235e-05,
      "loss": 0.0024,
      "step": 52950
    },
    {
      "epoch": 2.824533333333333,
      "grad_norm": 0.22545628249645233,
      "learning_rate": 3.2346666666666666e-05,
      "loss": 0.0022,
      "step": 52960
    },
    {
      "epoch": 2.8250666666666664,
      "grad_norm": 0.5865170955657959,
      "learning_rate": 3.234333333333334e-05,
      "loss": 0.002,
      "step": 52970
    },
    {
      "epoch": 2.8256,
      "grad_norm": 0.37925317883491516,
      "learning_rate": 3.2340000000000005e-05,
      "loss": 0.0017,
      "step": 52980
    },
    {
      "epoch": 2.8261333333333334,
      "grad_norm": 0.12113950401544571,
      "learning_rate": 3.233666666666667e-05,
      "loss": 0.0027,
      "step": 52990
    },
    {
      "epoch": 2.8266666666666667,
      "grad_norm": 0.23066316545009613,
      "learning_rate": 3.233333333333333e-05,
      "loss": 0.0019,
      "step": 53000
    },
    {
      "epoch": 2.8272,
      "grad_norm": 0.19609983265399933,
      "learning_rate": 3.233e-05,
      "loss": 0.0019,
      "step": 53010
    },
    {
      "epoch": 2.827733333333333,
      "grad_norm": 0.3658159375190735,
      "learning_rate": 3.232666666666666e-05,
      "loss": 0.0031,
      "step": 53020
    },
    {
      "epoch": 2.828266666666667,
      "grad_norm": 0.09925035387277603,
      "learning_rate": 3.2323333333333336e-05,
      "loss": 0.0032,
      "step": 53030
    },
    {
      "epoch": 2.8288,
      "grad_norm": 0.14608310163021088,
      "learning_rate": 3.232e-05,
      "loss": 0.0021,
      "step": 53040
    },
    {
      "epoch": 2.8293333333333335,
      "grad_norm": 0.26128020882606506,
      "learning_rate": 3.231666666666667e-05,
      "loss": 0.0017,
      "step": 53050
    },
    {
      "epoch": 2.8298666666666668,
      "grad_norm": 0.06804614514112473,
      "learning_rate": 3.2313333333333335e-05,
      "loss": 0.0015,
      "step": 53060
    },
    {
      "epoch": 2.8304,
      "grad_norm": 0.07196257263422012,
      "learning_rate": 3.231e-05,
      "loss": 0.0017,
      "step": 53070
    },
    {
      "epoch": 2.8309333333333333,
      "grad_norm": 0.24047209322452545,
      "learning_rate": 3.230666666666667e-05,
      "loss": 0.0028,
      "step": 53080
    },
    {
      "epoch": 2.8314666666666666,
      "grad_norm": 0.060481201857328415,
      "learning_rate": 3.230333333333333e-05,
      "loss": 0.0022,
      "step": 53090
    },
    {
      "epoch": 2.832,
      "grad_norm": 0.2221665233373642,
      "learning_rate": 3.2300000000000006e-05,
      "loss": 0.0021,
      "step": 53100
    },
    {
      "epoch": 2.832533333333333,
      "grad_norm": 0.7937105298042297,
      "learning_rate": 3.229666666666667e-05,
      "loss": 0.0022,
      "step": 53110
    },
    {
      "epoch": 2.8330666666666664,
      "grad_norm": 0.1997702717781067,
      "learning_rate": 3.229333333333334e-05,
      "loss": 0.003,
      "step": 53120
    },
    {
      "epoch": 2.8336,
      "grad_norm": 0.14382997155189514,
      "learning_rate": 3.2290000000000004e-05,
      "loss": 0.0022,
      "step": 53130
    },
    {
      "epoch": 2.8341333333333334,
      "grad_norm": 0.5650383830070496,
      "learning_rate": 3.228666666666667e-05,
      "loss": 0.0034,
      "step": 53140
    },
    {
      "epoch": 2.8346666666666667,
      "grad_norm": 0.2138337641954422,
      "learning_rate": 3.2283333333333337e-05,
      "loss": 0.002,
      "step": 53150
    },
    {
      "epoch": 2.8352,
      "grad_norm": 0.49736663699150085,
      "learning_rate": 3.2279999999999996e-05,
      "loss": 0.0023,
      "step": 53160
    },
    {
      "epoch": 2.835733333333333,
      "grad_norm": 0.059495165944099426,
      "learning_rate": 3.227666666666667e-05,
      "loss": 0.0023,
      "step": 53170
    },
    {
      "epoch": 2.836266666666667,
      "grad_norm": 0.25375062227249146,
      "learning_rate": 3.2273333333333335e-05,
      "loss": 0.0024,
      "step": 53180
    },
    {
      "epoch": 2.8368,
      "grad_norm": 0.30131596326828003,
      "learning_rate": 3.227e-05,
      "loss": 0.0029,
      "step": 53190
    },
    {
      "epoch": 2.8373333333333335,
      "grad_norm": 0.09083465486764908,
      "learning_rate": 3.226666666666667e-05,
      "loss": 0.0018,
      "step": 53200
    },
    {
      "epoch": 2.8378666666666668,
      "grad_norm": 0.13335564732551575,
      "learning_rate": 3.226333333333333e-05,
      "loss": 0.0025,
      "step": 53210
    },
    {
      "epoch": 2.8384,
      "grad_norm": 0.09393023699522018,
      "learning_rate": 3.226e-05,
      "loss": 0.0037,
      "step": 53220
    },
    {
      "epoch": 2.8389333333333333,
      "grad_norm": 0.2520960867404938,
      "learning_rate": 3.2256666666666666e-05,
      "loss": 0.003,
      "step": 53230
    },
    {
      "epoch": 2.8394666666666666,
      "grad_norm": 0.04795452207326889,
      "learning_rate": 3.225333333333334e-05,
      "loss": 0.0014,
      "step": 53240
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.08639697730541229,
      "learning_rate": 3.2250000000000005e-05,
      "loss": 0.0021,
      "step": 53250
    },
    {
      "epoch": 2.840533333333333,
      "grad_norm": 0.6805797815322876,
      "learning_rate": 3.224666666666667e-05,
      "loss": 0.0025,
      "step": 53260
    },
    {
      "epoch": 2.8410666666666664,
      "grad_norm": 0.24145765602588654,
      "learning_rate": 3.224333333333334e-05,
      "loss": 0.0025,
      "step": 53270
    },
    {
      "epoch": 2.8416,
      "grad_norm": 0.3083552420139313,
      "learning_rate": 3.224e-05,
      "loss": 0.0017,
      "step": 53280
    },
    {
      "epoch": 2.8421333333333334,
      "grad_norm": 0.10745713859796524,
      "learning_rate": 3.223666666666667e-05,
      "loss": 0.003,
      "step": 53290
    },
    {
      "epoch": 2.8426666666666667,
      "grad_norm": 0.08092915266752243,
      "learning_rate": 3.2233333333333335e-05,
      "loss": 0.0014,
      "step": 53300
    },
    {
      "epoch": 2.8432,
      "grad_norm": 0.42851927876472473,
      "learning_rate": 3.223e-05,
      "loss": 0.0021,
      "step": 53310
    },
    {
      "epoch": 2.8437333333333332,
      "grad_norm": 0.3086411952972412,
      "learning_rate": 3.222666666666667e-05,
      "loss": 0.0035,
      "step": 53320
    },
    {
      "epoch": 2.844266666666667,
      "grad_norm": 0.7394660115242004,
      "learning_rate": 3.2223333333333334e-05,
      "loss": 0.0034,
      "step": 53330
    },
    {
      "epoch": 2.8448,
      "grad_norm": 0.40313124656677246,
      "learning_rate": 3.222e-05,
      "loss": 0.002,
      "step": 53340
    },
    {
      "epoch": 2.8453333333333335,
      "grad_norm": 0.4637759327888489,
      "learning_rate": 3.2216666666666666e-05,
      "loss": 0.003,
      "step": 53350
    },
    {
      "epoch": 2.8458666666666668,
      "grad_norm": 0.1295710951089859,
      "learning_rate": 3.221333333333333e-05,
      "loss": 0.0024,
      "step": 53360
    },
    {
      "epoch": 2.8464,
      "grad_norm": 0.2230442613363266,
      "learning_rate": 3.221e-05,
      "loss": 0.0029,
      "step": 53370
    },
    {
      "epoch": 2.8469333333333333,
      "grad_norm": 0.19603385031223297,
      "learning_rate": 3.220666666666667e-05,
      "loss": 0.0021,
      "step": 53380
    },
    {
      "epoch": 2.8474666666666666,
      "grad_norm": 0.09462405741214752,
      "learning_rate": 3.220333333333334e-05,
      "loss": 0.0022,
      "step": 53390
    },
    {
      "epoch": 2.848,
      "grad_norm": 0.23503626883029938,
      "learning_rate": 3.2200000000000003e-05,
      "loss": 0.0027,
      "step": 53400
    },
    {
      "epoch": 2.848533333333333,
      "grad_norm": 0.1349848508834839,
      "learning_rate": 3.219666666666667e-05,
      "loss": 0.0021,
      "step": 53410
    },
    {
      "epoch": 2.8490666666666664,
      "grad_norm": 0.3659631013870239,
      "learning_rate": 3.2193333333333336e-05,
      "loss": 0.0027,
      "step": 53420
    },
    {
      "epoch": 2.8496,
      "grad_norm": 0.2547522187232971,
      "learning_rate": 3.219e-05,
      "loss": 0.0026,
      "step": 53430
    },
    {
      "epoch": 2.8501333333333334,
      "grad_norm": 0.1648760885000229,
      "learning_rate": 3.218666666666667e-05,
      "loss": 0.0018,
      "step": 53440
    },
    {
      "epoch": 2.8506666666666667,
      "grad_norm": 0.8186788558959961,
      "learning_rate": 3.218333333333334e-05,
      "loss": 0.0022,
      "step": 53450
    },
    {
      "epoch": 2.8512,
      "grad_norm": 0.4148307144641876,
      "learning_rate": 3.218e-05,
      "loss": 0.0019,
      "step": 53460
    },
    {
      "epoch": 2.8517333333333332,
      "grad_norm": 0.5491331219673157,
      "learning_rate": 3.2176666666666666e-05,
      "loss": 0.0017,
      "step": 53470
    },
    {
      "epoch": 2.8522666666666665,
      "grad_norm": 0.3370184004306793,
      "learning_rate": 3.217333333333333e-05,
      "loss": 0.0022,
      "step": 53480
    },
    {
      "epoch": 2.8528000000000002,
      "grad_norm": 0.10311778634786606,
      "learning_rate": 3.217e-05,
      "loss": 0.0018,
      "step": 53490
    },
    {
      "epoch": 2.8533333333333335,
      "grad_norm": 0.18494859337806702,
      "learning_rate": 3.2166666666666665e-05,
      "loss": 0.0019,
      "step": 53500
    },
    {
      "epoch": 2.8538666666666668,
      "grad_norm": 0.15536585450172424,
      "learning_rate": 3.216333333333333e-05,
      "loss": 0.0019,
      "step": 53510
    },
    {
      "epoch": 2.8544,
      "grad_norm": 0.09367106109857559,
      "learning_rate": 3.2160000000000004e-05,
      "loss": 0.0019,
      "step": 53520
    },
    {
      "epoch": 2.8549333333333333,
      "grad_norm": 0.15717023611068726,
      "learning_rate": 3.215666666666667e-05,
      "loss": 0.0025,
      "step": 53530
    },
    {
      "epoch": 2.8554666666666666,
      "grad_norm": 0.09582401067018509,
      "learning_rate": 3.2153333333333336e-05,
      "loss": 0.0018,
      "step": 53540
    },
    {
      "epoch": 2.856,
      "grad_norm": 0.4285554587841034,
      "learning_rate": 3.215e-05,
      "loss": 0.0013,
      "step": 53550
    },
    {
      "epoch": 2.856533333333333,
      "grad_norm": 0.6906474828720093,
      "learning_rate": 3.214666666666667e-05,
      "loss": 0.0018,
      "step": 53560
    },
    {
      "epoch": 2.8570666666666664,
      "grad_norm": 0.6719439029693604,
      "learning_rate": 3.2143333333333334e-05,
      "loss": 0.0035,
      "step": 53570
    },
    {
      "epoch": 2.8576,
      "grad_norm": 0.19794172048568726,
      "learning_rate": 3.214e-05,
      "loss": 0.0022,
      "step": 53580
    },
    {
      "epoch": 2.8581333333333334,
      "grad_norm": 0.7609812617301941,
      "learning_rate": 3.2136666666666674e-05,
      "loss": 0.0025,
      "step": 53590
    },
    {
      "epoch": 2.8586666666666667,
      "grad_norm": 0.15238358080387115,
      "learning_rate": 3.213333333333334e-05,
      "loss": 0.0026,
      "step": 53600
    },
    {
      "epoch": 2.8592,
      "grad_norm": 0.2477969378232956,
      "learning_rate": 3.213e-05,
      "loss": 0.0023,
      "step": 53610
    },
    {
      "epoch": 2.8597333333333332,
      "grad_norm": 0.0977732315659523,
      "learning_rate": 3.2126666666666665e-05,
      "loss": 0.0027,
      "step": 53620
    },
    {
      "epoch": 2.8602666666666665,
      "grad_norm": 0.10486848652362823,
      "learning_rate": 3.212333333333333e-05,
      "loss": 0.0022,
      "step": 53630
    },
    {
      "epoch": 2.8608000000000002,
      "grad_norm": 0.12068828195333481,
      "learning_rate": 3.212e-05,
      "loss": 0.0024,
      "step": 53640
    },
    {
      "epoch": 2.8613333333333335,
      "grad_norm": 0.020586173981428146,
      "learning_rate": 3.211666666666667e-05,
      "loss": 0.0015,
      "step": 53650
    },
    {
      "epoch": 2.861866666666667,
      "grad_norm": 0.2133142501115799,
      "learning_rate": 3.2113333333333336e-05,
      "loss": 0.0021,
      "step": 53660
    },
    {
      "epoch": 2.8624,
      "grad_norm": 0.15393003821372986,
      "learning_rate": 3.211e-05,
      "loss": 0.0029,
      "step": 53670
    },
    {
      "epoch": 2.8629333333333333,
      "grad_norm": 0.5382426977157593,
      "learning_rate": 3.210666666666667e-05,
      "loss": 0.0029,
      "step": 53680
    },
    {
      "epoch": 2.8634666666666666,
      "grad_norm": 0.14922383427619934,
      "learning_rate": 3.2103333333333335e-05,
      "loss": 0.0026,
      "step": 53690
    },
    {
      "epoch": 2.864,
      "grad_norm": 0.31631824374198914,
      "learning_rate": 3.21e-05,
      "loss": 0.0028,
      "step": 53700
    },
    {
      "epoch": 2.864533333333333,
      "grad_norm": 0.06247803941369057,
      "learning_rate": 3.209666666666667e-05,
      "loss": 0.0018,
      "step": 53710
    },
    {
      "epoch": 2.8650666666666664,
      "grad_norm": 0.4370670020580292,
      "learning_rate": 3.209333333333333e-05,
      "loss": 0.003,
      "step": 53720
    },
    {
      "epoch": 2.8656,
      "grad_norm": 0.0369488000869751,
      "learning_rate": 3.2090000000000006e-05,
      "loss": 0.0023,
      "step": 53730
    },
    {
      "epoch": 2.8661333333333334,
      "grad_norm": 0.2489372044801712,
      "learning_rate": 3.208666666666667e-05,
      "loss": 0.0023,
      "step": 53740
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 0.0568092055618763,
      "learning_rate": 3.208333333333334e-05,
      "loss": 0.0022,
      "step": 53750
    },
    {
      "epoch": 2.8672,
      "grad_norm": 0.09292667359113693,
      "learning_rate": 3.208e-05,
      "loss": 0.003,
      "step": 53760
    },
    {
      "epoch": 2.8677333333333332,
      "grad_norm": 0.25473520159721375,
      "learning_rate": 3.2076666666666664e-05,
      "loss": 0.0025,
      "step": 53770
    },
    {
      "epoch": 2.8682666666666665,
      "grad_norm": 0.27897313237190247,
      "learning_rate": 3.207333333333333e-05,
      "loss": 0.0024,
      "step": 53780
    },
    {
      "epoch": 2.8688000000000002,
      "grad_norm": 0.3116423189640045,
      "learning_rate": 3.207e-05,
      "loss": 0.0032,
      "step": 53790
    },
    {
      "epoch": 2.8693333333333335,
      "grad_norm": 0.07410043478012085,
      "learning_rate": 3.206666666666667e-05,
      "loss": 0.0035,
      "step": 53800
    },
    {
      "epoch": 2.869866666666667,
      "grad_norm": 0.3574008643627167,
      "learning_rate": 3.2063333333333335e-05,
      "loss": 0.0021,
      "step": 53810
    },
    {
      "epoch": 2.8704,
      "grad_norm": 0.7574201226234436,
      "learning_rate": 3.206e-05,
      "loss": 0.0022,
      "step": 53820
    },
    {
      "epoch": 2.8709333333333333,
      "grad_norm": 0.38752123713493347,
      "learning_rate": 3.205666666666667e-05,
      "loss": 0.0014,
      "step": 53830
    },
    {
      "epoch": 2.8714666666666666,
      "grad_norm": 0.621644139289856,
      "learning_rate": 3.2053333333333334e-05,
      "loss": 0.0025,
      "step": 53840
    },
    {
      "epoch": 2.872,
      "grad_norm": 0.10565047711133957,
      "learning_rate": 3.205e-05,
      "loss": 0.003,
      "step": 53850
    },
    {
      "epoch": 2.872533333333333,
      "grad_norm": 0.3177052438259125,
      "learning_rate": 3.2046666666666666e-05,
      "loss": 0.0021,
      "step": 53860
    },
    {
      "epoch": 2.8730666666666664,
      "grad_norm": 0.33703750371932983,
      "learning_rate": 3.204333333333334e-05,
      "loss": 0.0035,
      "step": 53870
    },
    {
      "epoch": 2.8736,
      "grad_norm": 0.19203491508960724,
      "learning_rate": 3.2040000000000005e-05,
      "loss": 0.0016,
      "step": 53880
    },
    {
      "epoch": 2.8741333333333334,
      "grad_norm": 0.28733813762664795,
      "learning_rate": 3.203666666666667e-05,
      "loss": 0.002,
      "step": 53890
    },
    {
      "epoch": 2.8746666666666667,
      "grad_norm": 0.4357711672782898,
      "learning_rate": 3.203333333333334e-05,
      "loss": 0.002,
      "step": 53900
    },
    {
      "epoch": 2.8752,
      "grad_norm": 0.2227783501148224,
      "learning_rate": 3.2029999999999997e-05,
      "loss": 0.0022,
      "step": 53910
    },
    {
      "epoch": 2.8757333333333333,
      "grad_norm": 0.10388367623090744,
      "learning_rate": 3.202666666666666e-05,
      "loss": 0.0024,
      "step": 53920
    },
    {
      "epoch": 2.8762666666666665,
      "grad_norm": 0.12296567112207413,
      "learning_rate": 3.2023333333333336e-05,
      "loss": 0.0024,
      "step": 53930
    },
    {
      "epoch": 2.8768000000000002,
      "grad_norm": 0.4508233964443207,
      "learning_rate": 3.202e-05,
      "loss": 0.002,
      "step": 53940
    },
    {
      "epoch": 2.8773333333333335,
      "grad_norm": 0.35151493549346924,
      "learning_rate": 3.201666666666667e-05,
      "loss": 0.0028,
      "step": 53950
    },
    {
      "epoch": 2.877866666666667,
      "grad_norm": 0.13341331481933594,
      "learning_rate": 3.2013333333333334e-05,
      "loss": 0.0024,
      "step": 53960
    },
    {
      "epoch": 2.8784,
      "grad_norm": 0.1932346522808075,
      "learning_rate": 3.201e-05,
      "loss": 0.0027,
      "step": 53970
    },
    {
      "epoch": 2.8789333333333333,
      "grad_norm": 0.1539762020111084,
      "learning_rate": 3.2006666666666666e-05,
      "loss": 0.0021,
      "step": 53980
    },
    {
      "epoch": 2.8794666666666666,
      "grad_norm": 0.10353697836399078,
      "learning_rate": 3.200333333333333e-05,
      "loss": 0.0023,
      "step": 53990
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.3441926836967468,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.002,
      "step": 54000
    },
    {
      "epoch": 2.880533333333333,
      "grad_norm": 0.11465836316347122,
      "learning_rate": 3.199666666666667e-05,
      "loss": 0.0023,
      "step": 54010
    },
    {
      "epoch": 2.8810666666666664,
      "grad_norm": 0.10791071504354477,
      "learning_rate": 3.199333333333334e-05,
      "loss": 0.0036,
      "step": 54020
    },
    {
      "epoch": 2.8816,
      "grad_norm": 0.411950945854187,
      "learning_rate": 3.1990000000000004e-05,
      "loss": 0.0026,
      "step": 54030
    },
    {
      "epoch": 2.8821333333333334,
      "grad_norm": 0.1533239334821701,
      "learning_rate": 3.198666666666667e-05,
      "loss": 0.0037,
      "step": 54040
    },
    {
      "epoch": 2.8826666666666667,
      "grad_norm": 0.6741102337837219,
      "learning_rate": 3.1983333333333336e-05,
      "loss": 0.0028,
      "step": 54050
    },
    {
      "epoch": 2.8832,
      "grad_norm": 0.5111973881721497,
      "learning_rate": 3.198e-05,
      "loss": 0.005,
      "step": 54060
    },
    {
      "epoch": 2.8837333333333333,
      "grad_norm": 0.14953765273094177,
      "learning_rate": 3.197666666666667e-05,
      "loss": 0.0019,
      "step": 54070
    },
    {
      "epoch": 2.8842666666666665,
      "grad_norm": 0.06485075503587723,
      "learning_rate": 3.1973333333333334e-05,
      "loss": 0.0018,
      "step": 54080
    },
    {
      "epoch": 2.8848000000000003,
      "grad_norm": 0.08615535497665405,
      "learning_rate": 3.197e-05,
      "loss": 0.0026,
      "step": 54090
    },
    {
      "epoch": 2.8853333333333335,
      "grad_norm": 0.3277879059314728,
      "learning_rate": 3.196666666666667e-05,
      "loss": 0.0021,
      "step": 54100
    },
    {
      "epoch": 2.885866666666667,
      "grad_norm": 0.2167293131351471,
      "learning_rate": 3.196333333333333e-05,
      "loss": 0.0025,
      "step": 54110
    },
    {
      "epoch": 2.8864,
      "grad_norm": 0.4684445858001709,
      "learning_rate": 3.196e-05,
      "loss": 0.0018,
      "step": 54120
    },
    {
      "epoch": 2.8869333333333334,
      "grad_norm": 0.0478813499212265,
      "learning_rate": 3.1956666666666665e-05,
      "loss": 0.0017,
      "step": 54130
    },
    {
      "epoch": 2.8874666666666666,
      "grad_norm": 0.3571062684059143,
      "learning_rate": 3.195333333333334e-05,
      "loss": 0.0027,
      "step": 54140
    },
    {
      "epoch": 2.888,
      "grad_norm": 0.12325021624565125,
      "learning_rate": 3.1950000000000004e-05,
      "loss": 0.0023,
      "step": 54150
    },
    {
      "epoch": 2.888533333333333,
      "grad_norm": 0.20150361955165863,
      "learning_rate": 3.194666666666667e-05,
      "loss": 0.002,
      "step": 54160
    },
    {
      "epoch": 2.8890666666666664,
      "grad_norm": 0.2904440760612488,
      "learning_rate": 3.1943333333333336e-05,
      "loss": 0.0023,
      "step": 54170
    },
    {
      "epoch": 2.8895999999999997,
      "grad_norm": 0.16926714777946472,
      "learning_rate": 3.194e-05,
      "loss": 0.0012,
      "step": 54180
    },
    {
      "epoch": 2.8901333333333334,
      "grad_norm": 0.20280282199382782,
      "learning_rate": 3.193666666666667e-05,
      "loss": 0.0021,
      "step": 54190
    },
    {
      "epoch": 2.8906666666666667,
      "grad_norm": 0.3483821153640747,
      "learning_rate": 3.1933333333333335e-05,
      "loss": 0.0035,
      "step": 54200
    },
    {
      "epoch": 2.8912,
      "grad_norm": 0.33774805068969727,
      "learning_rate": 3.193e-05,
      "loss": 0.0023,
      "step": 54210
    },
    {
      "epoch": 2.8917333333333333,
      "grad_norm": 0.08883308619260788,
      "learning_rate": 3.192666666666667e-05,
      "loss": 0.0022,
      "step": 54220
    },
    {
      "epoch": 2.8922666666666665,
      "grad_norm": 0.07191738486289978,
      "learning_rate": 3.192333333333333e-05,
      "loss": 0.002,
      "step": 54230
    },
    {
      "epoch": 2.8928000000000003,
      "grad_norm": 0.19452476501464844,
      "learning_rate": 3.192e-05,
      "loss": 0.0036,
      "step": 54240
    },
    {
      "epoch": 2.8933333333333335,
      "grad_norm": 0.0405445396900177,
      "learning_rate": 3.1916666666666665e-05,
      "loss": 0.0015,
      "step": 54250
    },
    {
      "epoch": 2.893866666666667,
      "grad_norm": 0.36385977268218994,
      "learning_rate": 3.191333333333333e-05,
      "loss": 0.0026,
      "step": 54260
    },
    {
      "epoch": 2.8944,
      "grad_norm": 0.13646340370178223,
      "learning_rate": 3.191e-05,
      "loss": 0.0023,
      "step": 54270
    },
    {
      "epoch": 2.8949333333333334,
      "grad_norm": 0.19864031672477722,
      "learning_rate": 3.190666666666667e-05,
      "loss": 0.0022,
      "step": 54280
    },
    {
      "epoch": 2.8954666666666666,
      "grad_norm": 0.16867418587207794,
      "learning_rate": 3.190333333333334e-05,
      "loss": 0.0038,
      "step": 54290
    },
    {
      "epoch": 2.896,
      "grad_norm": 0.4394531548023224,
      "learning_rate": 3.19e-05,
      "loss": 0.0027,
      "step": 54300
    },
    {
      "epoch": 2.896533333333333,
      "grad_norm": 0.1506970375776291,
      "learning_rate": 3.189666666666667e-05,
      "loss": 0.0016,
      "step": 54310
    },
    {
      "epoch": 2.8970666666666665,
      "grad_norm": 0.04768158867955208,
      "learning_rate": 3.1893333333333335e-05,
      "loss": 0.002,
      "step": 54320
    },
    {
      "epoch": 2.8975999999999997,
      "grad_norm": 0.41577476263046265,
      "learning_rate": 3.189e-05,
      "loss": 0.0033,
      "step": 54330
    },
    {
      "epoch": 2.8981333333333335,
      "grad_norm": 0.5793763399124146,
      "learning_rate": 3.188666666666667e-05,
      "loss": 0.0023,
      "step": 54340
    },
    {
      "epoch": 2.8986666666666667,
      "grad_norm": 0.5527758598327637,
      "learning_rate": 3.188333333333334e-05,
      "loss": 0.0023,
      "step": 54350
    },
    {
      "epoch": 2.8992,
      "grad_norm": 0.26084104180336,
      "learning_rate": 3.188e-05,
      "loss": 0.002,
      "step": 54360
    },
    {
      "epoch": 2.8997333333333333,
      "grad_norm": 0.13045458495616913,
      "learning_rate": 3.1876666666666666e-05,
      "loss": 0.0025,
      "step": 54370
    },
    {
      "epoch": 2.9002666666666665,
      "grad_norm": 0.359002947807312,
      "learning_rate": 3.187333333333333e-05,
      "loss": 0.002,
      "step": 54380
    },
    {
      "epoch": 2.9008000000000003,
      "grad_norm": 0.16574357450008392,
      "learning_rate": 3.187e-05,
      "loss": 0.0018,
      "step": 54390
    },
    {
      "epoch": 2.9013333333333335,
      "grad_norm": 0.2836703360080719,
      "learning_rate": 3.1866666666666664e-05,
      "loss": 0.0032,
      "step": 54400
    },
    {
      "epoch": 2.901866666666667,
      "grad_norm": 0.5800252556800842,
      "learning_rate": 3.186333333333334e-05,
      "loss": 0.0028,
      "step": 54410
    },
    {
      "epoch": 2.9024,
      "grad_norm": 0.4299989938735962,
      "learning_rate": 3.186e-05,
      "loss": 0.0031,
      "step": 54420
    },
    {
      "epoch": 2.9029333333333334,
      "grad_norm": 0.3354767858982086,
      "learning_rate": 3.185666666666667e-05,
      "loss": 0.0024,
      "step": 54430
    },
    {
      "epoch": 2.9034666666666666,
      "grad_norm": 0.027224376797676086,
      "learning_rate": 3.1853333333333336e-05,
      "loss": 0.002,
      "step": 54440
    },
    {
      "epoch": 2.904,
      "grad_norm": 0.15158554911613464,
      "learning_rate": 3.185e-05,
      "loss": 0.0026,
      "step": 54450
    },
    {
      "epoch": 2.904533333333333,
      "grad_norm": 0.520432710647583,
      "learning_rate": 3.184666666666667e-05,
      "loss": 0.0022,
      "step": 54460
    },
    {
      "epoch": 2.9050666666666665,
      "grad_norm": 0.10267346352338791,
      "learning_rate": 3.1843333333333334e-05,
      "loss": 0.0022,
      "step": 54470
    },
    {
      "epoch": 2.9055999999999997,
      "grad_norm": 0.1591474860906601,
      "learning_rate": 3.184e-05,
      "loss": 0.0025,
      "step": 54480
    },
    {
      "epoch": 2.9061333333333335,
      "grad_norm": 0.2870628535747528,
      "learning_rate": 3.183666666666667e-05,
      "loss": 0.0026,
      "step": 54490
    },
    {
      "epoch": 2.9066666666666667,
      "grad_norm": 0.20606759190559387,
      "learning_rate": 3.183333333333334e-05,
      "loss": 0.0021,
      "step": 54500
    },
    {
      "epoch": 2.9072,
      "grad_norm": 0.2144843190908432,
      "learning_rate": 3.1830000000000005e-05,
      "loss": 0.0021,
      "step": 54510
    },
    {
      "epoch": 2.9077333333333333,
      "grad_norm": 0.3519439399242401,
      "learning_rate": 3.1826666666666665e-05,
      "loss": 0.0017,
      "step": 54520
    },
    {
      "epoch": 2.9082666666666666,
      "grad_norm": 0.5120943784713745,
      "learning_rate": 3.182333333333333e-05,
      "loss": 0.003,
      "step": 54530
    },
    {
      "epoch": 2.9088000000000003,
      "grad_norm": 0.6649978756904602,
      "learning_rate": 3.182e-05,
      "loss": 0.0031,
      "step": 54540
    },
    {
      "epoch": 2.9093333333333335,
      "grad_norm": 0.1624847948551178,
      "learning_rate": 3.181666666666667e-05,
      "loss": 0.0027,
      "step": 54550
    },
    {
      "epoch": 2.909866666666667,
      "grad_norm": 0.34386175870895386,
      "learning_rate": 3.1813333333333336e-05,
      "loss": 0.0018,
      "step": 54560
    },
    {
      "epoch": 2.9104,
      "grad_norm": 0.2133403718471527,
      "learning_rate": 3.181e-05,
      "loss": 0.002,
      "step": 54570
    },
    {
      "epoch": 2.9109333333333334,
      "grad_norm": 0.42938947677612305,
      "learning_rate": 3.180666666666667e-05,
      "loss": 0.0016,
      "step": 54580
    },
    {
      "epoch": 2.9114666666666666,
      "grad_norm": 0.14949162304401398,
      "learning_rate": 3.1803333333333334e-05,
      "loss": 0.0019,
      "step": 54590
    },
    {
      "epoch": 2.912,
      "grad_norm": 0.5836002230644226,
      "learning_rate": 3.18e-05,
      "loss": 0.002,
      "step": 54600
    },
    {
      "epoch": 2.912533333333333,
      "grad_norm": 0.09475210309028625,
      "learning_rate": 3.1796666666666667e-05,
      "loss": 0.0022,
      "step": 54610
    },
    {
      "epoch": 2.9130666666666665,
      "grad_norm": 0.4385625123977661,
      "learning_rate": 3.179333333333333e-05,
      "loss": 0.002,
      "step": 54620
    },
    {
      "epoch": 2.9135999999999997,
      "grad_norm": 0.10284055769443512,
      "learning_rate": 3.1790000000000006e-05,
      "loss": 0.0021,
      "step": 54630
    },
    {
      "epoch": 2.9141333333333335,
      "grad_norm": 0.0646011233329773,
      "learning_rate": 3.178666666666667e-05,
      "loss": 0.0027,
      "step": 54640
    },
    {
      "epoch": 2.9146666666666667,
      "grad_norm": 0.19017407298088074,
      "learning_rate": 3.178333333333334e-05,
      "loss": 0.0029,
      "step": 54650
    },
    {
      "epoch": 2.9152,
      "grad_norm": 0.5968756675720215,
      "learning_rate": 3.1780000000000004e-05,
      "loss": 0.0027,
      "step": 54660
    },
    {
      "epoch": 2.9157333333333333,
      "grad_norm": 0.18837548792362213,
      "learning_rate": 3.1776666666666663e-05,
      "loss": 0.0018,
      "step": 54670
    },
    {
      "epoch": 2.9162666666666666,
      "grad_norm": 0.4569051265716553,
      "learning_rate": 3.177333333333333e-05,
      "loss": 0.0024,
      "step": 54680
    },
    {
      "epoch": 2.9168,
      "grad_norm": 0.22472304105758667,
      "learning_rate": 3.177e-05,
      "loss": 0.0017,
      "step": 54690
    },
    {
      "epoch": 2.9173333333333336,
      "grad_norm": 0.1287391185760498,
      "learning_rate": 3.176666666666667e-05,
      "loss": 0.0027,
      "step": 54700
    },
    {
      "epoch": 2.917866666666667,
      "grad_norm": 0.7002230882644653,
      "learning_rate": 3.1763333333333335e-05,
      "loss": 0.0026,
      "step": 54710
    },
    {
      "epoch": 2.9184,
      "grad_norm": 0.2848581373691559,
      "learning_rate": 3.176e-05,
      "loss": 0.0025,
      "step": 54720
    },
    {
      "epoch": 2.9189333333333334,
      "grad_norm": 0.21623128652572632,
      "learning_rate": 3.175666666666667e-05,
      "loss": 0.0022,
      "step": 54730
    },
    {
      "epoch": 2.9194666666666667,
      "grad_norm": 0.04042404517531395,
      "learning_rate": 3.175333333333333e-05,
      "loss": 0.0025,
      "step": 54740
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.062109388411045074,
      "learning_rate": 3.175e-05,
      "loss": 0.0026,
      "step": 54750
    },
    {
      "epoch": 2.920533333333333,
      "grad_norm": 0.07362095266580582,
      "learning_rate": 3.174666666666667e-05,
      "loss": 0.002,
      "step": 54760
    },
    {
      "epoch": 2.9210666666666665,
      "grad_norm": 0.20726720988750458,
      "learning_rate": 3.174333333333334e-05,
      "loss": 0.0021,
      "step": 54770
    },
    {
      "epoch": 2.9215999999999998,
      "grad_norm": 0.308412104845047,
      "learning_rate": 3.1740000000000004e-05,
      "loss": 0.0023,
      "step": 54780
    },
    {
      "epoch": 2.9221333333333335,
      "grad_norm": 0.18320050835609436,
      "learning_rate": 3.173666666666667e-05,
      "loss": 0.0025,
      "step": 54790
    },
    {
      "epoch": 2.9226666666666667,
      "grad_norm": 0.5863174200057983,
      "learning_rate": 3.173333333333334e-05,
      "loss": 0.0023,
      "step": 54800
    },
    {
      "epoch": 2.9232,
      "grad_norm": 0.07020766288042068,
      "learning_rate": 3.173e-05,
      "loss": 0.0023,
      "step": 54810
    },
    {
      "epoch": 2.9237333333333333,
      "grad_norm": 0.3797711730003357,
      "learning_rate": 3.172666666666667e-05,
      "loss": 0.0018,
      "step": 54820
    },
    {
      "epoch": 2.9242666666666666,
      "grad_norm": 0.28170913457870483,
      "learning_rate": 3.1723333333333335e-05,
      "loss": 0.0017,
      "step": 54830
    },
    {
      "epoch": 2.9248,
      "grad_norm": 0.18618346750736237,
      "learning_rate": 3.172e-05,
      "loss": 0.0018,
      "step": 54840
    },
    {
      "epoch": 2.9253333333333336,
      "grad_norm": 0.3387559652328491,
      "learning_rate": 3.171666666666667e-05,
      "loss": 0.0019,
      "step": 54850
    },
    {
      "epoch": 2.925866666666667,
      "grad_norm": 0.1534709930419922,
      "learning_rate": 3.1713333333333334e-05,
      "loss": 0.0029,
      "step": 54860
    },
    {
      "epoch": 2.9264,
      "grad_norm": 0.09887241572141647,
      "learning_rate": 3.171e-05,
      "loss": 0.0021,
      "step": 54870
    },
    {
      "epoch": 2.9269333333333334,
      "grad_norm": 0.2136821150779724,
      "learning_rate": 3.1706666666666666e-05,
      "loss": 0.0025,
      "step": 54880
    },
    {
      "epoch": 2.9274666666666667,
      "grad_norm": 0.3500341475009918,
      "learning_rate": 3.170333333333333e-05,
      "loss": 0.0023,
      "step": 54890
    },
    {
      "epoch": 2.928,
      "grad_norm": 0.3471698760986328,
      "learning_rate": 3.1700000000000005e-05,
      "loss": 0.0021,
      "step": 54900
    },
    {
      "epoch": 2.928533333333333,
      "grad_norm": 0.11860901117324829,
      "learning_rate": 3.169666666666667e-05,
      "loss": 0.0018,
      "step": 54910
    },
    {
      "epoch": 2.9290666666666665,
      "grad_norm": 0.2529759109020233,
      "learning_rate": 3.169333333333334e-05,
      "loss": 0.0023,
      "step": 54920
    },
    {
      "epoch": 2.9295999999999998,
      "grad_norm": 0.24160337448120117,
      "learning_rate": 3.169e-05,
      "loss": 0.0036,
      "step": 54930
    },
    {
      "epoch": 2.9301333333333335,
      "grad_norm": 0.3548891842365265,
      "learning_rate": 3.168666666666667e-05,
      "loss": 0.0027,
      "step": 54940
    },
    {
      "epoch": 2.9306666666666668,
      "grad_norm": 0.20711307227611542,
      "learning_rate": 3.1683333333333335e-05,
      "loss": 0.0025,
      "step": 54950
    },
    {
      "epoch": 2.9312,
      "grad_norm": 0.18865564465522766,
      "learning_rate": 3.168e-05,
      "loss": 0.0021,
      "step": 54960
    },
    {
      "epoch": 2.9317333333333333,
      "grad_norm": 0.24595099687576294,
      "learning_rate": 3.167666666666667e-05,
      "loss": 0.0021,
      "step": 54970
    },
    {
      "epoch": 2.9322666666666666,
      "grad_norm": 0.13754454255104065,
      "learning_rate": 3.1673333333333334e-05,
      "loss": 0.0027,
      "step": 54980
    },
    {
      "epoch": 2.9328,
      "grad_norm": 0.18952690064907074,
      "learning_rate": 3.167e-05,
      "loss": 0.0018,
      "step": 54990
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 0.1860511302947998,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 0.0027,
      "step": 55000
    },
    {
      "epoch": 2.933866666666667,
      "grad_norm": 0.37476858496665955,
      "learning_rate": 3.166333333333333e-05,
      "loss": 0.0014,
      "step": 55010
    },
    {
      "epoch": 2.9344,
      "grad_norm": 0.12078168243169785,
      "learning_rate": 3.166e-05,
      "loss": 0.0019,
      "step": 55020
    },
    {
      "epoch": 2.9349333333333334,
      "grad_norm": 0.1400175541639328,
      "learning_rate": 3.1656666666666665e-05,
      "loss": 0.0023,
      "step": 55030
    },
    {
      "epoch": 2.9354666666666667,
      "grad_norm": 0.581122636795044,
      "learning_rate": 3.165333333333334e-05,
      "loss": 0.0027,
      "step": 55040
    },
    {
      "epoch": 2.936,
      "grad_norm": 0.2971174120903015,
      "learning_rate": 3.1650000000000004e-05,
      "loss": 0.0038,
      "step": 55050
    },
    {
      "epoch": 2.936533333333333,
      "grad_norm": 0.343636691570282,
      "learning_rate": 3.164666666666667e-05,
      "loss": 0.0023,
      "step": 55060
    },
    {
      "epoch": 2.9370666666666665,
      "grad_norm": 0.12408839911222458,
      "learning_rate": 3.1643333333333336e-05,
      "loss": 0.0017,
      "step": 55070
    },
    {
      "epoch": 2.9375999999999998,
      "grad_norm": 0.36690863966941833,
      "learning_rate": 3.164e-05,
      "loss": 0.0019,
      "step": 55080
    },
    {
      "epoch": 2.9381333333333335,
      "grad_norm": 0.5152300596237183,
      "learning_rate": 3.163666666666667e-05,
      "loss": 0.0017,
      "step": 55090
    },
    {
      "epoch": 2.9386666666666668,
      "grad_norm": 0.10465764254331589,
      "learning_rate": 3.1633333333333334e-05,
      "loss": 0.0019,
      "step": 55100
    },
    {
      "epoch": 2.9392,
      "grad_norm": 0.5637561678886414,
      "learning_rate": 3.163000000000001e-05,
      "loss": 0.0026,
      "step": 55110
    },
    {
      "epoch": 2.9397333333333333,
      "grad_norm": 0.259810209274292,
      "learning_rate": 3.1626666666666667e-05,
      "loss": 0.0027,
      "step": 55120
    },
    {
      "epoch": 2.9402666666666666,
      "grad_norm": 0.03030581772327423,
      "learning_rate": 3.162333333333333e-05,
      "loss": 0.0016,
      "step": 55130
    },
    {
      "epoch": 2.9408,
      "grad_norm": 0.22707557678222656,
      "learning_rate": 3.162e-05,
      "loss": 0.0014,
      "step": 55140
    },
    {
      "epoch": 2.9413333333333336,
      "grad_norm": 0.12388302385807037,
      "learning_rate": 3.1616666666666665e-05,
      "loss": 0.0026,
      "step": 55150
    },
    {
      "epoch": 2.941866666666667,
      "grad_norm": 0.18499398231506348,
      "learning_rate": 3.161333333333333e-05,
      "loss": 0.0018,
      "step": 55160
    },
    {
      "epoch": 2.9424,
      "grad_norm": 0.13774804770946503,
      "learning_rate": 3.1610000000000004e-05,
      "loss": 0.0028,
      "step": 55170
    },
    {
      "epoch": 2.9429333333333334,
      "grad_norm": 0.29311707615852356,
      "learning_rate": 3.160666666666667e-05,
      "loss": 0.0022,
      "step": 55180
    },
    {
      "epoch": 2.9434666666666667,
      "grad_norm": 0.4771353602409363,
      "learning_rate": 3.1603333333333336e-05,
      "loss": 0.0015,
      "step": 55190
    },
    {
      "epoch": 2.944,
      "grad_norm": 0.030169889330863953,
      "learning_rate": 3.16e-05,
      "loss": 0.0016,
      "step": 55200
    },
    {
      "epoch": 2.9445333333333332,
      "grad_norm": 0.6013757586479187,
      "learning_rate": 3.159666666666667e-05,
      "loss": 0.0026,
      "step": 55210
    },
    {
      "epoch": 2.9450666666666665,
      "grad_norm": 0.3054167330265045,
      "learning_rate": 3.1593333333333335e-05,
      "loss": 0.0022,
      "step": 55220
    },
    {
      "epoch": 2.9455999999999998,
      "grad_norm": 0.5065039992332458,
      "learning_rate": 3.159e-05,
      "loss": 0.0023,
      "step": 55230
    },
    {
      "epoch": 2.9461333333333335,
      "grad_norm": 0.15978854894638062,
      "learning_rate": 3.158666666666667e-05,
      "loss": 0.0034,
      "step": 55240
    },
    {
      "epoch": 2.9466666666666668,
      "grad_norm": 0.0729002058506012,
      "learning_rate": 3.158333333333334e-05,
      "loss": 0.003,
      "step": 55250
    },
    {
      "epoch": 2.9472,
      "grad_norm": 0.5560722351074219,
      "learning_rate": 3.1580000000000006e-05,
      "loss": 0.0022,
      "step": 55260
    },
    {
      "epoch": 2.9477333333333333,
      "grad_norm": 0.2898617088794708,
      "learning_rate": 3.1576666666666665e-05,
      "loss": 0.002,
      "step": 55270
    },
    {
      "epoch": 2.9482666666666666,
      "grad_norm": 0.37306779623031616,
      "learning_rate": 3.157333333333333e-05,
      "loss": 0.0019,
      "step": 55280
    },
    {
      "epoch": 2.9488,
      "grad_norm": 0.25523269176483154,
      "learning_rate": 3.157e-05,
      "loss": 0.0015,
      "step": 55290
    },
    {
      "epoch": 2.9493333333333336,
      "grad_norm": 0.0669424757361412,
      "learning_rate": 3.1566666666666664e-05,
      "loss": 0.0019,
      "step": 55300
    },
    {
      "epoch": 2.949866666666667,
      "grad_norm": 0.02463480271399021,
      "learning_rate": 3.156333333333334e-05,
      "loss": 0.002,
      "step": 55310
    },
    {
      "epoch": 2.9504,
      "grad_norm": 0.14410828053951263,
      "learning_rate": 3.156e-05,
      "loss": 0.0021,
      "step": 55320
    },
    {
      "epoch": 2.9509333333333334,
      "grad_norm": 0.15826453268527985,
      "learning_rate": 3.155666666666667e-05,
      "loss": 0.0017,
      "step": 55330
    },
    {
      "epoch": 2.9514666666666667,
      "grad_norm": 1.0805509090423584,
      "learning_rate": 3.1553333333333335e-05,
      "loss": 0.002,
      "step": 55340
    },
    {
      "epoch": 2.952,
      "grad_norm": 0.6168645024299622,
      "learning_rate": 3.155e-05,
      "loss": 0.0023,
      "step": 55350
    },
    {
      "epoch": 2.9525333333333332,
      "grad_norm": 0.4313531816005707,
      "learning_rate": 3.154666666666667e-05,
      "loss": 0.002,
      "step": 55360
    },
    {
      "epoch": 2.9530666666666665,
      "grad_norm": 0.1844087392091751,
      "learning_rate": 3.1543333333333333e-05,
      "loss": 0.0019,
      "step": 55370
    },
    {
      "epoch": 2.9536,
      "grad_norm": 0.04748795926570892,
      "learning_rate": 3.154e-05,
      "loss": 0.0019,
      "step": 55380
    },
    {
      "epoch": 2.9541333333333335,
      "grad_norm": 0.06892309337854385,
      "learning_rate": 3.153666666666667e-05,
      "loss": 0.0033,
      "step": 55390
    },
    {
      "epoch": 2.9546666666666668,
      "grad_norm": 0.10169921070337296,
      "learning_rate": 3.153333333333334e-05,
      "loss": 0.0022,
      "step": 55400
    },
    {
      "epoch": 2.9552,
      "grad_norm": 0.1846459060907364,
      "learning_rate": 3.1530000000000005e-05,
      "loss": 0.0025,
      "step": 55410
    },
    {
      "epoch": 2.9557333333333333,
      "grad_norm": 0.10141477733850479,
      "learning_rate": 3.1526666666666664e-05,
      "loss": 0.0017,
      "step": 55420
    },
    {
      "epoch": 2.9562666666666666,
      "grad_norm": 0.36802104115486145,
      "learning_rate": 3.152333333333333e-05,
      "loss": 0.0022,
      "step": 55430
    },
    {
      "epoch": 2.9568,
      "grad_norm": 0.49696218967437744,
      "learning_rate": 3.1519999999999996e-05,
      "loss": 0.0016,
      "step": 55440
    },
    {
      "epoch": 2.9573333333333336,
      "grad_norm": 0.3300345838069916,
      "learning_rate": 3.151666666666667e-05,
      "loss": 0.0036,
      "step": 55450
    },
    {
      "epoch": 2.957866666666667,
      "grad_norm": 0.24252840876579285,
      "learning_rate": 3.1513333333333335e-05,
      "loss": 0.0026,
      "step": 55460
    },
    {
      "epoch": 2.9584,
      "grad_norm": 0.5599325299263,
      "learning_rate": 3.151e-05,
      "loss": 0.003,
      "step": 55470
    },
    {
      "epoch": 2.9589333333333334,
      "grad_norm": 0.06873279809951782,
      "learning_rate": 3.150666666666667e-05,
      "loss": 0.0014,
      "step": 55480
    },
    {
      "epoch": 2.9594666666666667,
      "grad_norm": 0.1793833076953888,
      "learning_rate": 3.1503333333333334e-05,
      "loss": 0.0026,
      "step": 55490
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.16745641827583313,
      "learning_rate": 3.15e-05,
      "loss": 0.0023,
      "step": 55500
    },
    {
      "epoch": 2.9605333333333332,
      "grad_norm": 0.2891403138637543,
      "learning_rate": 3.1496666666666666e-05,
      "loss": 0.0022,
      "step": 55510
    },
    {
      "epoch": 2.9610666666666665,
      "grad_norm": 0.343522310256958,
      "learning_rate": 3.149333333333334e-05,
      "loss": 0.0026,
      "step": 55520
    },
    {
      "epoch": 2.9616,
      "grad_norm": 0.05965328589081764,
      "learning_rate": 3.1490000000000005e-05,
      "loss": 0.0031,
      "step": 55530
    },
    {
      "epoch": 2.962133333333333,
      "grad_norm": 0.14217787981033325,
      "learning_rate": 3.148666666666667e-05,
      "loss": 0.0023,
      "step": 55540
    },
    {
      "epoch": 2.962666666666667,
      "grad_norm": 0.07187213003635406,
      "learning_rate": 3.148333333333334e-05,
      "loss": 0.002,
      "step": 55550
    },
    {
      "epoch": 2.9632,
      "grad_norm": 0.13116586208343506,
      "learning_rate": 3.1480000000000004e-05,
      "loss": 0.002,
      "step": 55560
    },
    {
      "epoch": 2.9637333333333333,
      "grad_norm": 0.09511152654886246,
      "learning_rate": 3.147666666666666e-05,
      "loss": 0.0019,
      "step": 55570
    },
    {
      "epoch": 2.9642666666666666,
      "grad_norm": 0.22129002213478088,
      "learning_rate": 3.1473333333333336e-05,
      "loss": 0.0016,
      "step": 55580
    },
    {
      "epoch": 2.9648,
      "grad_norm": 0.42656809091567993,
      "learning_rate": 3.147e-05,
      "loss": 0.0027,
      "step": 55590
    },
    {
      "epoch": 2.9653333333333336,
      "grad_norm": 0.09547800570726395,
      "learning_rate": 3.146666666666667e-05,
      "loss": 0.0023,
      "step": 55600
    },
    {
      "epoch": 2.965866666666667,
      "grad_norm": 0.2347288727760315,
      "learning_rate": 3.1463333333333334e-05,
      "loss": 0.0018,
      "step": 55610
    },
    {
      "epoch": 2.9664,
      "grad_norm": 0.08353181183338165,
      "learning_rate": 3.146e-05,
      "loss": 0.0042,
      "step": 55620
    },
    {
      "epoch": 2.9669333333333334,
      "grad_norm": 0.3747783899307251,
      "learning_rate": 3.1456666666666666e-05,
      "loss": 0.0022,
      "step": 55630
    },
    {
      "epoch": 2.9674666666666667,
      "grad_norm": 0.1707543134689331,
      "learning_rate": 3.145333333333333e-05,
      "loss": 0.0025,
      "step": 55640
    },
    {
      "epoch": 2.968,
      "grad_norm": 0.12230683863162994,
      "learning_rate": 3.145e-05,
      "loss": 0.0022,
      "step": 55650
    },
    {
      "epoch": 2.9685333333333332,
      "grad_norm": 0.26869523525238037,
      "learning_rate": 3.144666666666667e-05,
      "loss": 0.0024,
      "step": 55660
    },
    {
      "epoch": 2.9690666666666665,
      "grad_norm": 0.5689564943313599,
      "learning_rate": 3.144333333333334e-05,
      "loss": 0.0022,
      "step": 55670
    },
    {
      "epoch": 2.9696,
      "grad_norm": 0.15792539715766907,
      "learning_rate": 3.1440000000000004e-05,
      "loss": 0.0024,
      "step": 55680
    },
    {
      "epoch": 2.970133333333333,
      "grad_norm": 0.2502042055130005,
      "learning_rate": 3.143666666666667e-05,
      "loss": 0.002,
      "step": 55690
    },
    {
      "epoch": 2.970666666666667,
      "grad_norm": 0.04407574608922005,
      "learning_rate": 3.1433333333333336e-05,
      "loss": 0.002,
      "step": 55700
    },
    {
      "epoch": 2.9712,
      "grad_norm": 0.5293942093849182,
      "learning_rate": 3.143e-05,
      "loss": 0.0027,
      "step": 55710
    },
    {
      "epoch": 2.9717333333333333,
      "grad_norm": 0.13663755357265472,
      "learning_rate": 3.142666666666667e-05,
      "loss": 0.0021,
      "step": 55720
    },
    {
      "epoch": 2.9722666666666666,
      "grad_norm": 0.07195545732975006,
      "learning_rate": 3.1423333333333335e-05,
      "loss": 0.0015,
      "step": 55730
    },
    {
      "epoch": 2.9728,
      "grad_norm": 0.4441731572151184,
      "learning_rate": 3.142e-05,
      "loss": 0.002,
      "step": 55740
    },
    {
      "epoch": 2.9733333333333336,
      "grad_norm": 0.20261700451374054,
      "learning_rate": 3.141666666666667e-05,
      "loss": 0.0022,
      "step": 55750
    },
    {
      "epoch": 2.973866666666667,
      "grad_norm": 0.036998797208070755,
      "learning_rate": 3.141333333333333e-05,
      "loss": 0.0023,
      "step": 55760
    },
    {
      "epoch": 2.9744,
      "grad_norm": 0.3308648467063904,
      "learning_rate": 3.141e-05,
      "loss": 0.0014,
      "step": 55770
    },
    {
      "epoch": 2.9749333333333334,
      "grad_norm": 0.09289395064115524,
      "learning_rate": 3.1406666666666665e-05,
      "loss": 0.0027,
      "step": 55780
    },
    {
      "epoch": 2.9754666666666667,
      "grad_norm": 0.3096247613430023,
      "learning_rate": 3.140333333333333e-05,
      "loss": 0.0022,
      "step": 55790
    },
    {
      "epoch": 2.976,
      "grad_norm": 0.13168655335903168,
      "learning_rate": 3.1400000000000004e-05,
      "loss": 0.0027,
      "step": 55800
    },
    {
      "epoch": 2.9765333333333333,
      "grad_norm": 0.5902860760688782,
      "learning_rate": 3.139666666666667e-05,
      "loss": 0.0022,
      "step": 55810
    },
    {
      "epoch": 2.9770666666666665,
      "grad_norm": 0.7385880351066589,
      "learning_rate": 3.1393333333333337e-05,
      "loss": 0.0018,
      "step": 55820
    },
    {
      "epoch": 2.9776,
      "grad_norm": 0.03502378612756729,
      "learning_rate": 3.139e-05,
      "loss": 0.0018,
      "step": 55830
    },
    {
      "epoch": 2.978133333333333,
      "grad_norm": 0.4169973134994507,
      "learning_rate": 3.138666666666667e-05,
      "loss": 0.0039,
      "step": 55840
    },
    {
      "epoch": 2.978666666666667,
      "grad_norm": 0.1399422138929367,
      "learning_rate": 3.1383333333333335e-05,
      "loss": 0.0015,
      "step": 55850
    },
    {
      "epoch": 2.9792,
      "grad_norm": 0.3298106789588928,
      "learning_rate": 3.138e-05,
      "loss": 0.0016,
      "step": 55860
    },
    {
      "epoch": 2.9797333333333333,
      "grad_norm": 0.399360328912735,
      "learning_rate": 3.1376666666666674e-05,
      "loss": 0.0017,
      "step": 55870
    },
    {
      "epoch": 2.9802666666666666,
      "grad_norm": 0.12961281836032867,
      "learning_rate": 3.137333333333333e-05,
      "loss": 0.0033,
      "step": 55880
    },
    {
      "epoch": 2.9808,
      "grad_norm": 0.38185933232307434,
      "learning_rate": 3.137e-05,
      "loss": 0.0026,
      "step": 55890
    },
    {
      "epoch": 2.981333333333333,
      "grad_norm": 0.24467429518699646,
      "learning_rate": 3.1366666666666666e-05,
      "loss": 0.0017,
      "step": 55900
    },
    {
      "epoch": 2.981866666666667,
      "grad_norm": 0.4936498701572418,
      "learning_rate": 3.136333333333333e-05,
      "loss": 0.0023,
      "step": 55910
    },
    {
      "epoch": 2.9824,
      "grad_norm": 0.2952466309070587,
      "learning_rate": 3.136e-05,
      "loss": 0.0029,
      "step": 55920
    },
    {
      "epoch": 2.9829333333333334,
      "grad_norm": 0.10202038288116455,
      "learning_rate": 3.135666666666667e-05,
      "loss": 0.0018,
      "step": 55930
    },
    {
      "epoch": 2.9834666666666667,
      "grad_norm": 0.0942557081580162,
      "learning_rate": 3.135333333333334e-05,
      "loss": 0.0024,
      "step": 55940
    },
    {
      "epoch": 2.984,
      "grad_norm": 0.10516789555549622,
      "learning_rate": 3.135e-05,
      "loss": 0.0026,
      "step": 55950
    },
    {
      "epoch": 2.9845333333333333,
      "grad_norm": 0.12371082603931427,
      "learning_rate": 3.134666666666667e-05,
      "loss": 0.0017,
      "step": 55960
    },
    {
      "epoch": 2.9850666666666665,
      "grad_norm": 0.18354730308055878,
      "learning_rate": 3.1343333333333335e-05,
      "loss": 0.0016,
      "step": 55970
    },
    {
      "epoch": 2.9856,
      "grad_norm": 0.6225815415382385,
      "learning_rate": 3.134e-05,
      "loss": 0.0026,
      "step": 55980
    },
    {
      "epoch": 2.986133333333333,
      "grad_norm": 0.16211490333080292,
      "learning_rate": 3.133666666666667e-05,
      "loss": 0.0018,
      "step": 55990
    },
    {
      "epoch": 2.986666666666667,
      "grad_norm": 0.06766680628061295,
      "learning_rate": 3.1333333333333334e-05,
      "loss": 0.0016,
      "step": 56000
    },
    {
      "epoch": 2.9872,
      "grad_norm": 0.4223933219909668,
      "learning_rate": 3.133000000000001e-05,
      "loss": 0.0017,
      "step": 56010
    },
    {
      "epoch": 2.9877333333333334,
      "grad_norm": 0.5199551582336426,
      "learning_rate": 3.132666666666667e-05,
      "loss": 0.0023,
      "step": 56020
    },
    {
      "epoch": 2.9882666666666666,
      "grad_norm": 0.1628057360649109,
      "learning_rate": 3.132333333333333e-05,
      "loss": 0.0024,
      "step": 56030
    },
    {
      "epoch": 2.9888,
      "grad_norm": 0.04724530875682831,
      "learning_rate": 3.132e-05,
      "loss": 0.0021,
      "step": 56040
    },
    {
      "epoch": 2.989333333333333,
      "grad_norm": 0.18881244957447052,
      "learning_rate": 3.1316666666666664e-05,
      "loss": 0.0017,
      "step": 56050
    },
    {
      "epoch": 2.989866666666667,
      "grad_norm": 0.3731338381767273,
      "learning_rate": 3.131333333333333e-05,
      "loss": 0.0017,
      "step": 56060
    },
    {
      "epoch": 2.9904,
      "grad_norm": 0.33437618613243103,
      "learning_rate": 3.1310000000000003e-05,
      "loss": 0.0032,
      "step": 56070
    },
    {
      "epoch": 2.9909333333333334,
      "grad_norm": 0.21926072239875793,
      "learning_rate": 3.130666666666667e-05,
      "loss": 0.0015,
      "step": 56080
    },
    {
      "epoch": 2.9914666666666667,
      "grad_norm": 0.34220871329307556,
      "learning_rate": 3.1303333333333336e-05,
      "loss": 0.0027,
      "step": 56090
    },
    {
      "epoch": 2.992,
      "grad_norm": 0.36217209696769714,
      "learning_rate": 3.13e-05,
      "loss": 0.0021,
      "step": 56100
    },
    {
      "epoch": 2.9925333333333333,
      "grad_norm": 0.14528842270374298,
      "learning_rate": 3.129666666666667e-05,
      "loss": 0.0026,
      "step": 56110
    },
    {
      "epoch": 2.9930666666666665,
      "grad_norm": 0.32641205191612244,
      "learning_rate": 3.1293333333333334e-05,
      "loss": 0.0023,
      "step": 56120
    },
    {
      "epoch": 2.9936,
      "grad_norm": 0.04196827486157417,
      "learning_rate": 3.129e-05,
      "loss": 0.0028,
      "step": 56130
    },
    {
      "epoch": 2.994133333333333,
      "grad_norm": 0.13055148720741272,
      "learning_rate": 3.1286666666666666e-05,
      "loss": 0.0017,
      "step": 56140
    },
    {
      "epoch": 2.994666666666667,
      "grad_norm": 0.1384291797876358,
      "learning_rate": 3.128333333333334e-05,
      "loss": 0.0029,
      "step": 56150
    },
    {
      "epoch": 2.9952,
      "grad_norm": 0.1819327026605606,
      "learning_rate": 3.1280000000000005e-05,
      "loss": 0.0018,
      "step": 56160
    },
    {
      "epoch": 2.9957333333333334,
      "grad_norm": 0.33543726801872253,
      "learning_rate": 3.127666666666667e-05,
      "loss": 0.003,
      "step": 56170
    },
    {
      "epoch": 2.9962666666666666,
      "grad_norm": 0.64729243516922,
      "learning_rate": 3.127333333333333e-05,
      "loss": 0.0018,
      "step": 56180
    },
    {
      "epoch": 2.9968,
      "grad_norm": 0.08111188560724258,
      "learning_rate": 3.127e-05,
      "loss": 0.0024,
      "step": 56190
    },
    {
      "epoch": 2.997333333333333,
      "grad_norm": 0.3373456597328186,
      "learning_rate": 3.126666666666666e-05,
      "loss": 0.0018,
      "step": 56200
    },
    {
      "epoch": 2.997866666666667,
      "grad_norm": 0.13293883204460144,
      "learning_rate": 3.1263333333333336e-05,
      "loss": 0.003,
      "step": 56210
    },
    {
      "epoch": 2.9984,
      "grad_norm": 0.3142778277397156,
      "learning_rate": 3.126e-05,
      "loss": 0.0027,
      "step": 56220
    },
    {
      "epoch": 2.9989333333333335,
      "grad_norm": 0.43037575483322144,
      "learning_rate": 3.125666666666667e-05,
      "loss": 0.0017,
      "step": 56230
    },
    {
      "epoch": 2.9994666666666667,
      "grad_norm": 0.7785449028015137,
      "learning_rate": 3.1253333333333335e-05,
      "loss": 0.0019,
      "step": 56240
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.7365358471870422,
      "learning_rate": 3.125e-05,
      "loss": 0.0028,
      "step": 56250
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.0024636085145175457,
      "eval_runtime": 157.4105,
      "eval_samples_per_second": 1588.204,
      "eval_steps_per_second": 39.705,
      "step": 56250
    },
    {
      "epoch": 3.0005333333333333,
      "grad_norm": 0.18424874544143677,
      "learning_rate": 3.124666666666667e-05,
      "loss": 0.0023,
      "step": 56260
    },
    {
      "epoch": 3.0010666666666665,
      "grad_norm": 0.16439303755760193,
      "learning_rate": 3.124333333333333e-05,
      "loss": 0.0019,
      "step": 56270
    },
    {
      "epoch": 3.0016,
      "grad_norm": 0.34327438473701477,
      "learning_rate": 3.1240000000000006e-05,
      "loss": 0.0024,
      "step": 56280
    },
    {
      "epoch": 3.0021333333333335,
      "grad_norm": 0.28515180945396423,
      "learning_rate": 3.123666666666667e-05,
      "loss": 0.0021,
      "step": 56290
    },
    {
      "epoch": 3.002666666666667,
      "grad_norm": 0.3802311420440674,
      "learning_rate": 3.123333333333334e-05,
      "loss": 0.002,
      "step": 56300
    },
    {
      "epoch": 3.0032,
      "grad_norm": 0.18512964248657227,
      "learning_rate": 3.1230000000000004e-05,
      "loss": 0.0021,
      "step": 56310
    },
    {
      "epoch": 3.0037333333333334,
      "grad_norm": 0.43240687251091003,
      "learning_rate": 3.122666666666667e-05,
      "loss": 0.0024,
      "step": 56320
    },
    {
      "epoch": 3.0042666666666666,
      "grad_norm": 0.17998838424682617,
      "learning_rate": 3.122333333333333e-05,
      "loss": 0.0021,
      "step": 56330
    },
    {
      "epoch": 3.0048,
      "grad_norm": 0.6424177885055542,
      "learning_rate": 3.122e-05,
      "loss": 0.0027,
      "step": 56340
    },
    {
      "epoch": 3.005333333333333,
      "grad_norm": 0.23274484276771545,
      "learning_rate": 3.121666666666667e-05,
      "loss": 0.0014,
      "step": 56350
    },
    {
      "epoch": 3.0058666666666665,
      "grad_norm": 0.19159823656082153,
      "learning_rate": 3.1213333333333335e-05,
      "loss": 0.0016,
      "step": 56360
    },
    {
      "epoch": 3.0064,
      "grad_norm": 0.06878340244293213,
      "learning_rate": 3.121e-05,
      "loss": 0.0015,
      "step": 56370
    },
    {
      "epoch": 3.0069333333333335,
      "grad_norm": 0.06965002417564392,
      "learning_rate": 3.120666666666667e-05,
      "loss": 0.002,
      "step": 56380
    },
    {
      "epoch": 3.0074666666666667,
      "grad_norm": 0.5812629461288452,
      "learning_rate": 3.120333333333333e-05,
      "loss": 0.0028,
      "step": 56390
    },
    {
      "epoch": 3.008,
      "grad_norm": 0.24139238893985748,
      "learning_rate": 3.12e-05,
      "loss": 0.0017,
      "step": 56400
    },
    {
      "epoch": 3.0085333333333333,
      "grad_norm": 0.20477725565433502,
      "learning_rate": 3.1196666666666666e-05,
      "loss": 0.0026,
      "step": 56410
    },
    {
      "epoch": 3.0090666666666666,
      "grad_norm": 0.24582377076148987,
      "learning_rate": 3.119333333333334e-05,
      "loss": 0.0018,
      "step": 56420
    },
    {
      "epoch": 3.0096,
      "grad_norm": 0.3234589397907257,
      "learning_rate": 3.1190000000000005e-05,
      "loss": 0.0029,
      "step": 56430
    },
    {
      "epoch": 3.0101333333333335,
      "grad_norm": 0.22275686264038086,
      "learning_rate": 3.118666666666667e-05,
      "loss": 0.002,
      "step": 56440
    },
    {
      "epoch": 3.010666666666667,
      "grad_norm": 0.1198069304227829,
      "learning_rate": 3.118333333333334e-05,
      "loss": 0.0027,
      "step": 56450
    },
    {
      "epoch": 3.0112,
      "grad_norm": 0.4151551425457001,
      "learning_rate": 3.118e-05,
      "loss": 0.0025,
      "step": 56460
    },
    {
      "epoch": 3.0117333333333334,
      "grad_norm": 0.2821359932422638,
      "learning_rate": 3.117666666666667e-05,
      "loss": 0.0024,
      "step": 56470
    },
    {
      "epoch": 3.0122666666666666,
      "grad_norm": 0.4854682385921478,
      "learning_rate": 3.1173333333333335e-05,
      "loss": 0.0023,
      "step": 56480
    },
    {
      "epoch": 3.0128,
      "grad_norm": 0.35199055075645447,
      "learning_rate": 3.117e-05,
      "loss": 0.0016,
      "step": 56490
    },
    {
      "epoch": 3.013333333333333,
      "grad_norm": 0.3667190968990326,
      "learning_rate": 3.116666666666667e-05,
      "loss": 0.0022,
      "step": 56500
    },
    {
      "epoch": 3.0138666666666665,
      "grad_norm": 0.274349570274353,
      "learning_rate": 3.1163333333333334e-05,
      "loss": 0.0018,
      "step": 56510
    },
    {
      "epoch": 3.0144,
      "grad_norm": 0.11985908448696136,
      "learning_rate": 3.116e-05,
      "loss": 0.0022,
      "step": 56520
    },
    {
      "epoch": 3.0149333333333335,
      "grad_norm": 0.2191421240568161,
      "learning_rate": 3.1156666666666666e-05,
      "loss": 0.0012,
      "step": 56530
    },
    {
      "epoch": 3.0154666666666667,
      "grad_norm": 0.12667542695999146,
      "learning_rate": 3.115333333333333e-05,
      "loss": 0.0018,
      "step": 56540
    },
    {
      "epoch": 3.016,
      "grad_norm": 0.3602454662322998,
      "learning_rate": 3.115e-05,
      "loss": 0.0021,
      "step": 56550
    },
    {
      "epoch": 3.0165333333333333,
      "grad_norm": 0.5150944590568542,
      "learning_rate": 3.114666666666667e-05,
      "loss": 0.0021,
      "step": 56560
    },
    {
      "epoch": 3.0170666666666666,
      "grad_norm": 0.04159534350037575,
      "learning_rate": 3.114333333333334e-05,
      "loss": 0.0017,
      "step": 56570
    },
    {
      "epoch": 3.0176,
      "grad_norm": 0.11419712752103806,
      "learning_rate": 3.1140000000000003e-05,
      "loss": 0.0033,
      "step": 56580
    },
    {
      "epoch": 3.018133333333333,
      "grad_norm": 0.3488211929798126,
      "learning_rate": 3.113666666666667e-05,
      "loss": 0.0016,
      "step": 56590
    },
    {
      "epoch": 3.018666666666667,
      "grad_norm": 0.18983367085456848,
      "learning_rate": 3.1133333333333336e-05,
      "loss": 0.0017,
      "step": 56600
    },
    {
      "epoch": 3.0192,
      "grad_norm": 0.7286065220832825,
      "learning_rate": 3.113e-05,
      "loss": 0.0026,
      "step": 56610
    },
    {
      "epoch": 3.0197333333333334,
      "grad_norm": 0.40651360154151917,
      "learning_rate": 3.112666666666667e-05,
      "loss": 0.0011,
      "step": 56620
    },
    {
      "epoch": 3.0202666666666667,
      "grad_norm": 0.30227696895599365,
      "learning_rate": 3.1123333333333334e-05,
      "loss": 0.0023,
      "step": 56630
    },
    {
      "epoch": 3.0208,
      "grad_norm": 0.2948560416698456,
      "learning_rate": 3.112e-05,
      "loss": 0.0028,
      "step": 56640
    },
    {
      "epoch": 3.021333333333333,
      "grad_norm": 0.48391446471214294,
      "learning_rate": 3.1116666666666666e-05,
      "loss": 0.0032,
      "step": 56650
    },
    {
      "epoch": 3.0218666666666665,
      "grad_norm": 0.09653323888778687,
      "learning_rate": 3.111333333333333e-05,
      "loss": 0.0032,
      "step": 56660
    },
    {
      "epoch": 3.0224,
      "grad_norm": 0.23231129348278046,
      "learning_rate": 3.111e-05,
      "loss": 0.0031,
      "step": 56670
    },
    {
      "epoch": 3.0229333333333335,
      "grad_norm": 0.3434891700744629,
      "learning_rate": 3.1106666666666665e-05,
      "loss": 0.0026,
      "step": 56680
    },
    {
      "epoch": 3.0234666666666667,
      "grad_norm": 0.4164268970489502,
      "learning_rate": 3.110333333333334e-05,
      "loss": 0.0025,
      "step": 56690
    },
    {
      "epoch": 3.024,
      "grad_norm": 0.22863715887069702,
      "learning_rate": 3.1100000000000004e-05,
      "loss": 0.0024,
      "step": 56700
    },
    {
      "epoch": 3.0245333333333333,
      "grad_norm": 0.12788204848766327,
      "learning_rate": 3.109666666666667e-05,
      "loss": 0.0022,
      "step": 56710
    },
    {
      "epoch": 3.0250666666666666,
      "grad_norm": 0.24384264647960663,
      "learning_rate": 3.1093333333333336e-05,
      "loss": 0.0028,
      "step": 56720
    },
    {
      "epoch": 3.0256,
      "grad_norm": 0.2616911828517914,
      "learning_rate": 3.109e-05,
      "loss": 0.0016,
      "step": 56730
    },
    {
      "epoch": 3.026133333333333,
      "grad_norm": 0.07292193174362183,
      "learning_rate": 3.108666666666667e-05,
      "loss": 0.0018,
      "step": 56740
    },
    {
      "epoch": 3.026666666666667,
      "grad_norm": 0.24028261005878448,
      "learning_rate": 3.1083333333333334e-05,
      "loss": 0.0016,
      "step": 56750
    },
    {
      "epoch": 3.0272,
      "grad_norm": 0.0566108338534832,
      "learning_rate": 3.108e-05,
      "loss": 0.0035,
      "step": 56760
    },
    {
      "epoch": 3.0277333333333334,
      "grad_norm": 0.39029982686042786,
      "learning_rate": 3.1076666666666673e-05,
      "loss": 0.002,
      "step": 56770
    },
    {
      "epoch": 3.0282666666666667,
      "grad_norm": 0.5485405921936035,
      "learning_rate": 3.107333333333333e-05,
      "loss": 0.0026,
      "step": 56780
    },
    {
      "epoch": 3.0288,
      "grad_norm": 0.7495409846305847,
      "learning_rate": 3.107e-05,
      "loss": 0.0025,
      "step": 56790
    },
    {
      "epoch": 3.029333333333333,
      "grad_norm": 0.24870352447032928,
      "learning_rate": 3.1066666666666665e-05,
      "loss": 0.0021,
      "step": 56800
    },
    {
      "epoch": 3.0298666666666665,
      "grad_norm": 0.4379667043685913,
      "learning_rate": 3.106333333333333e-05,
      "loss": 0.0039,
      "step": 56810
    },
    {
      "epoch": 3.0304,
      "grad_norm": 0.4582691788673401,
      "learning_rate": 3.106e-05,
      "loss": 0.0022,
      "step": 56820
    },
    {
      "epoch": 3.0309333333333335,
      "grad_norm": 0.5000036358833313,
      "learning_rate": 3.105666666666667e-05,
      "loss": 0.0021,
      "step": 56830
    },
    {
      "epoch": 3.0314666666666668,
      "grad_norm": 0.21344101428985596,
      "learning_rate": 3.1053333333333336e-05,
      "loss": 0.0019,
      "step": 56840
    },
    {
      "epoch": 3.032,
      "grad_norm": 0.12106301635503769,
      "learning_rate": 3.105e-05,
      "loss": 0.0018,
      "step": 56850
    },
    {
      "epoch": 3.0325333333333333,
      "grad_norm": 0.48728063702583313,
      "learning_rate": 3.104666666666667e-05,
      "loss": 0.0022,
      "step": 56860
    },
    {
      "epoch": 3.0330666666666666,
      "grad_norm": 0.33666765689849854,
      "learning_rate": 3.1043333333333335e-05,
      "loss": 0.002,
      "step": 56870
    },
    {
      "epoch": 3.0336,
      "grad_norm": 0.07419488579034805,
      "learning_rate": 3.104e-05,
      "loss": 0.0019,
      "step": 56880
    },
    {
      "epoch": 3.034133333333333,
      "grad_norm": 0.28126707673072815,
      "learning_rate": 3.103666666666667e-05,
      "loss": 0.0026,
      "step": 56890
    },
    {
      "epoch": 3.034666666666667,
      "grad_norm": 0.19466689229011536,
      "learning_rate": 3.103333333333333e-05,
      "loss": 0.0032,
      "step": 56900
    },
    {
      "epoch": 3.0352,
      "grad_norm": 0.05300068482756615,
      "learning_rate": 3.1030000000000006e-05,
      "loss": 0.0024,
      "step": 56910
    },
    {
      "epoch": 3.0357333333333334,
      "grad_norm": 0.18212492763996124,
      "learning_rate": 3.102666666666667e-05,
      "loss": 0.0019,
      "step": 56920
    },
    {
      "epoch": 3.0362666666666667,
      "grad_norm": 0.6432065367698669,
      "learning_rate": 3.102333333333334e-05,
      "loss": 0.0016,
      "step": 56930
    },
    {
      "epoch": 3.0368,
      "grad_norm": 0.04460727795958519,
      "learning_rate": 3.102e-05,
      "loss": 0.0013,
      "step": 56940
    },
    {
      "epoch": 3.037333333333333,
      "grad_norm": 0.5323718786239624,
      "learning_rate": 3.1016666666666664e-05,
      "loss": 0.0028,
      "step": 56950
    },
    {
      "epoch": 3.0378666666666665,
      "grad_norm": 0.4894813001155853,
      "learning_rate": 3.101333333333333e-05,
      "loss": 0.0024,
      "step": 56960
    },
    {
      "epoch": 3.0384,
      "grad_norm": 0.12835480272769928,
      "learning_rate": 3.101e-05,
      "loss": 0.0018,
      "step": 56970
    },
    {
      "epoch": 3.0389333333333335,
      "grad_norm": 0.12377975136041641,
      "learning_rate": 3.100666666666667e-05,
      "loss": 0.0028,
      "step": 56980
    },
    {
      "epoch": 3.0394666666666668,
      "grad_norm": 0.3845282793045044,
      "learning_rate": 3.1003333333333335e-05,
      "loss": 0.0034,
      "step": 56990
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.12380119413137436,
      "learning_rate": 3.1e-05,
      "loss": 0.0026,
      "step": 57000
    },
    {
      "epoch": 3.0405333333333333,
      "grad_norm": 0.18507766723632812,
      "learning_rate": 3.099666666666667e-05,
      "loss": 0.0024,
      "step": 57010
    },
    {
      "epoch": 3.0410666666666666,
      "grad_norm": 0.3394905924797058,
      "learning_rate": 3.0993333333333334e-05,
      "loss": 0.0026,
      "step": 57020
    },
    {
      "epoch": 3.0416,
      "grad_norm": 0.5883399844169617,
      "learning_rate": 3.099e-05,
      "loss": 0.0019,
      "step": 57030
    },
    {
      "epoch": 3.042133333333333,
      "grad_norm": 0.05101635679602623,
      "learning_rate": 3.098666666666667e-05,
      "loss": 0.0019,
      "step": 57040
    },
    {
      "epoch": 3.042666666666667,
      "grad_norm": 0.14365743100643158,
      "learning_rate": 3.098333333333334e-05,
      "loss": 0.0019,
      "step": 57050
    },
    {
      "epoch": 3.0432,
      "grad_norm": 0.5102777481079102,
      "learning_rate": 3.0980000000000005e-05,
      "loss": 0.002,
      "step": 57060
    },
    {
      "epoch": 3.0437333333333334,
      "grad_norm": 0.33254438638687134,
      "learning_rate": 3.097666666666667e-05,
      "loss": 0.0019,
      "step": 57070
    },
    {
      "epoch": 3.0442666666666667,
      "grad_norm": 0.2937605381011963,
      "learning_rate": 3.097333333333334e-05,
      "loss": 0.0027,
      "step": 57080
    },
    {
      "epoch": 3.0448,
      "grad_norm": 0.06908807903528214,
      "learning_rate": 3.0969999999999997e-05,
      "loss": 0.0015,
      "step": 57090
    },
    {
      "epoch": 3.0453333333333332,
      "grad_norm": 0.10665401816368103,
      "learning_rate": 3.096666666666666e-05,
      "loss": 0.0017,
      "step": 57100
    },
    {
      "epoch": 3.0458666666666665,
      "grad_norm": 0.24762608110904694,
      "learning_rate": 3.0963333333333336e-05,
      "loss": 0.0021,
      "step": 57110
    },
    {
      "epoch": 3.0464,
      "grad_norm": 0.46302375197410583,
      "learning_rate": 3.096e-05,
      "loss": 0.0017,
      "step": 57120
    },
    {
      "epoch": 3.0469333333333335,
      "grad_norm": 0.4770626127719879,
      "learning_rate": 3.095666666666667e-05,
      "loss": 0.0032,
      "step": 57130
    },
    {
      "epoch": 3.0474666666666668,
      "grad_norm": 0.21225443482398987,
      "learning_rate": 3.0953333333333334e-05,
      "loss": 0.0018,
      "step": 57140
    },
    {
      "epoch": 3.048,
      "grad_norm": 0.49640247225761414,
      "learning_rate": 3.095e-05,
      "loss": 0.0021,
      "step": 57150
    },
    {
      "epoch": 3.0485333333333333,
      "grad_norm": 0.30167484283447266,
      "learning_rate": 3.0946666666666666e-05,
      "loss": 0.0017,
      "step": 57160
    },
    {
      "epoch": 3.0490666666666666,
      "grad_norm": 0.5441468358039856,
      "learning_rate": 3.094333333333333e-05,
      "loss": 0.0029,
      "step": 57170
    },
    {
      "epoch": 3.0496,
      "grad_norm": 0.0506577305495739,
      "learning_rate": 3.0940000000000005e-05,
      "loss": 0.0028,
      "step": 57180
    },
    {
      "epoch": 3.050133333333333,
      "grad_norm": 0.1040138527750969,
      "learning_rate": 3.093666666666667e-05,
      "loss": 0.0016,
      "step": 57190
    },
    {
      "epoch": 3.050666666666667,
      "grad_norm": 0.18419954180717468,
      "learning_rate": 3.093333333333334e-05,
      "loss": 0.0022,
      "step": 57200
    },
    {
      "epoch": 3.0512,
      "grad_norm": 0.14160454273223877,
      "learning_rate": 3.0930000000000004e-05,
      "loss": 0.0023,
      "step": 57210
    },
    {
      "epoch": 3.0517333333333334,
      "grad_norm": 0.25722700357437134,
      "learning_rate": 3.092666666666667e-05,
      "loss": 0.0026,
      "step": 57220
    },
    {
      "epoch": 3.0522666666666667,
      "grad_norm": 0.19221974909305573,
      "learning_rate": 3.0923333333333336e-05,
      "loss": 0.0022,
      "step": 57230
    },
    {
      "epoch": 3.0528,
      "grad_norm": 0.29834797978401184,
      "learning_rate": 3.092e-05,
      "loss": 0.0025,
      "step": 57240
    },
    {
      "epoch": 3.0533333333333332,
      "grad_norm": 0.19627830386161804,
      "learning_rate": 3.091666666666667e-05,
      "loss": 0.0022,
      "step": 57250
    },
    {
      "epoch": 3.0538666666666665,
      "grad_norm": 0.30907589197158813,
      "learning_rate": 3.0913333333333334e-05,
      "loss": 0.0018,
      "step": 57260
    },
    {
      "epoch": 3.0544,
      "grad_norm": 0.5556084513664246,
      "learning_rate": 3.091e-05,
      "loss": 0.002,
      "step": 57270
    },
    {
      "epoch": 3.0549333333333335,
      "grad_norm": 0.3154664933681488,
      "learning_rate": 3.090666666666667e-05,
      "loss": 0.0018,
      "step": 57280
    },
    {
      "epoch": 3.0554666666666668,
      "grad_norm": 0.08810703456401825,
      "learning_rate": 3.090333333333333e-05,
      "loss": 0.0017,
      "step": 57290
    },
    {
      "epoch": 3.056,
      "grad_norm": 0.06738373637199402,
      "learning_rate": 3.09e-05,
      "loss": 0.0022,
      "step": 57300
    },
    {
      "epoch": 3.0565333333333333,
      "grad_norm": 0.16361969709396362,
      "learning_rate": 3.0896666666666665e-05,
      "loss": 0.0021,
      "step": 57310
    },
    {
      "epoch": 3.0570666666666666,
      "grad_norm": 0.31478238105773926,
      "learning_rate": 3.089333333333334e-05,
      "loss": 0.0017,
      "step": 57320
    },
    {
      "epoch": 3.0576,
      "grad_norm": 0.02375083789229393,
      "learning_rate": 3.0890000000000004e-05,
      "loss": 0.002,
      "step": 57330
    },
    {
      "epoch": 3.058133333333333,
      "grad_norm": 0.7060525417327881,
      "learning_rate": 3.088666666666667e-05,
      "loss": 0.0025,
      "step": 57340
    },
    {
      "epoch": 3.058666666666667,
      "grad_norm": 0.47170504927635193,
      "learning_rate": 3.0883333333333336e-05,
      "loss": 0.0023,
      "step": 57350
    },
    {
      "epoch": 3.0592,
      "grad_norm": 0.18345177173614502,
      "learning_rate": 3.088e-05,
      "loss": 0.0018,
      "step": 57360
    },
    {
      "epoch": 3.0597333333333334,
      "grad_norm": 0.18580640852451324,
      "learning_rate": 3.087666666666667e-05,
      "loss": 0.0022,
      "step": 57370
    },
    {
      "epoch": 3.0602666666666667,
      "grad_norm": 0.41852226853370667,
      "learning_rate": 3.0873333333333335e-05,
      "loss": 0.002,
      "step": 57380
    },
    {
      "epoch": 3.0608,
      "grad_norm": 0.39280465245246887,
      "learning_rate": 3.087e-05,
      "loss": 0.0026,
      "step": 57390
    },
    {
      "epoch": 3.0613333333333332,
      "grad_norm": 0.2270277887582779,
      "learning_rate": 3.086666666666667e-05,
      "loss": 0.0026,
      "step": 57400
    },
    {
      "epoch": 3.0618666666666665,
      "grad_norm": 0.05532550439238548,
      "learning_rate": 3.086333333333333e-05,
      "loss": 0.0023,
      "step": 57410
    },
    {
      "epoch": 3.0624,
      "grad_norm": 0.07731738686561584,
      "learning_rate": 3.086e-05,
      "loss": 0.0016,
      "step": 57420
    },
    {
      "epoch": 3.0629333333333335,
      "grad_norm": 0.17493149638175964,
      "learning_rate": 3.0856666666666665e-05,
      "loss": 0.0022,
      "step": 57430
    },
    {
      "epoch": 3.063466666666667,
      "grad_norm": 0.12587884068489075,
      "learning_rate": 3.085333333333333e-05,
      "loss": 0.0016,
      "step": 57440
    },
    {
      "epoch": 3.064,
      "grad_norm": 0.18365022540092468,
      "learning_rate": 3.0850000000000004e-05,
      "loss": 0.0024,
      "step": 57450
    },
    {
      "epoch": 3.0645333333333333,
      "grad_norm": 0.08192598074674606,
      "learning_rate": 3.084666666666667e-05,
      "loss": 0.0029,
      "step": 57460
    },
    {
      "epoch": 3.0650666666666666,
      "grad_norm": 0.19340327382087708,
      "learning_rate": 3.084333333333334e-05,
      "loss": 0.002,
      "step": 57470
    },
    {
      "epoch": 3.0656,
      "grad_norm": 0.1240912675857544,
      "learning_rate": 3.084e-05,
      "loss": 0.0017,
      "step": 57480
    },
    {
      "epoch": 3.066133333333333,
      "grad_norm": 0.49533072113990784,
      "learning_rate": 3.083666666666667e-05,
      "loss": 0.0014,
      "step": 57490
    },
    {
      "epoch": 3.066666666666667,
      "grad_norm": 0.196843683719635,
      "learning_rate": 3.0833333333333335e-05,
      "loss": 0.0022,
      "step": 57500
    },
    {
      "epoch": 3.0672,
      "grad_norm": 0.04268161952495575,
      "learning_rate": 3.083e-05,
      "loss": 0.0019,
      "step": 57510
    },
    {
      "epoch": 3.0677333333333334,
      "grad_norm": 0.06691114604473114,
      "learning_rate": 3.082666666666667e-05,
      "loss": 0.0022,
      "step": 57520
    },
    {
      "epoch": 3.0682666666666667,
      "grad_norm": 0.04915953800082207,
      "learning_rate": 3.082333333333334e-05,
      "loss": 0.0026,
      "step": 57530
    },
    {
      "epoch": 3.0688,
      "grad_norm": 0.15932419896125793,
      "learning_rate": 3.082e-05,
      "loss": 0.0016,
      "step": 57540
    },
    {
      "epoch": 3.0693333333333332,
      "grad_norm": 0.04787115007638931,
      "learning_rate": 3.0816666666666666e-05,
      "loss": 0.0026,
      "step": 57550
    },
    {
      "epoch": 3.0698666666666665,
      "grad_norm": 0.42617201805114746,
      "learning_rate": 3.081333333333333e-05,
      "loss": 0.0022,
      "step": 57560
    },
    {
      "epoch": 3.0704,
      "grad_norm": 0.13256187736988068,
      "learning_rate": 3.081e-05,
      "loss": 0.003,
      "step": 57570
    },
    {
      "epoch": 3.0709333333333335,
      "grad_norm": 0.6542944312095642,
      "learning_rate": 3.0806666666666664e-05,
      "loss": 0.0027,
      "step": 57580
    },
    {
      "epoch": 3.071466666666667,
      "grad_norm": 0.22636036574840546,
      "learning_rate": 3.080333333333334e-05,
      "loss": 0.0027,
      "step": 57590
    },
    {
      "epoch": 3.072,
      "grad_norm": 0.19696594774723053,
      "learning_rate": 3.08e-05,
      "loss": 0.0019,
      "step": 57600
    },
    {
      "epoch": 3.0725333333333333,
      "grad_norm": 0.3091801106929779,
      "learning_rate": 3.079666666666667e-05,
      "loss": 0.0021,
      "step": 57610
    },
    {
      "epoch": 3.0730666666666666,
      "grad_norm": 0.14569392800331116,
      "learning_rate": 3.0793333333333336e-05,
      "loss": 0.0022,
      "step": 57620
    },
    {
      "epoch": 3.0736,
      "grad_norm": 0.2296430468559265,
      "learning_rate": 3.079e-05,
      "loss": 0.0014,
      "step": 57630
    },
    {
      "epoch": 3.074133333333333,
      "grad_norm": 0.10410264879465103,
      "learning_rate": 3.078666666666667e-05,
      "loss": 0.0021,
      "step": 57640
    },
    {
      "epoch": 3.074666666666667,
      "grad_norm": 0.06519772857427597,
      "learning_rate": 3.0783333333333334e-05,
      "loss": 0.0017,
      "step": 57650
    },
    {
      "epoch": 3.0752,
      "grad_norm": 0.20577891170978546,
      "learning_rate": 3.078e-05,
      "loss": 0.0022,
      "step": 57660
    },
    {
      "epoch": 3.0757333333333334,
      "grad_norm": 0.07025320827960968,
      "learning_rate": 3.077666666666667e-05,
      "loss": 0.0019,
      "step": 57670
    },
    {
      "epoch": 3.0762666666666667,
      "grad_norm": 0.34809616208076477,
      "learning_rate": 3.077333333333334e-05,
      "loss": 0.0015,
      "step": 57680
    },
    {
      "epoch": 3.0768,
      "grad_norm": 0.2874634265899658,
      "learning_rate": 3.077e-05,
      "loss": 0.0024,
      "step": 57690
    },
    {
      "epoch": 3.0773333333333333,
      "grad_norm": 0.3092590868473053,
      "learning_rate": 3.0766666666666665e-05,
      "loss": 0.0014,
      "step": 57700
    },
    {
      "epoch": 3.0778666666666665,
      "grad_norm": 0.3521605134010315,
      "learning_rate": 3.076333333333333e-05,
      "loss": 0.0021,
      "step": 57710
    },
    {
      "epoch": 3.0784,
      "grad_norm": 0.09176105260848999,
      "learning_rate": 3.076e-05,
      "loss": 0.0022,
      "step": 57720
    },
    {
      "epoch": 3.0789333333333335,
      "grad_norm": 0.2790519595146179,
      "learning_rate": 3.075666666666667e-05,
      "loss": 0.0019,
      "step": 57730
    },
    {
      "epoch": 3.079466666666667,
      "grad_norm": 0.3756851851940155,
      "learning_rate": 3.0753333333333336e-05,
      "loss": 0.0027,
      "step": 57740
    },
    {
      "epoch": 3.08,
      "grad_norm": 0.20434261858463287,
      "learning_rate": 3.075e-05,
      "loss": 0.0021,
      "step": 57750
    },
    {
      "epoch": 3.0805333333333333,
      "grad_norm": 0.30571073293685913,
      "learning_rate": 3.074666666666667e-05,
      "loss": 0.0024,
      "step": 57760
    },
    {
      "epoch": 3.0810666666666666,
      "grad_norm": 0.07191893458366394,
      "learning_rate": 3.0743333333333334e-05,
      "loss": 0.0021,
      "step": 57770
    },
    {
      "epoch": 3.0816,
      "grad_norm": 0.20937243103981018,
      "learning_rate": 3.074e-05,
      "loss": 0.002,
      "step": 57780
    },
    {
      "epoch": 3.082133333333333,
      "grad_norm": 0.2784292995929718,
      "learning_rate": 3.0736666666666667e-05,
      "loss": 0.0025,
      "step": 57790
    },
    {
      "epoch": 3.0826666666666664,
      "grad_norm": 0.2346278429031372,
      "learning_rate": 3.073333333333334e-05,
      "loss": 0.0027,
      "step": 57800
    },
    {
      "epoch": 3.0832,
      "grad_norm": 0.26875507831573486,
      "learning_rate": 3.0730000000000006e-05,
      "loss": 0.0018,
      "step": 57810
    },
    {
      "epoch": 3.0837333333333334,
      "grad_norm": 0.16082875430583954,
      "learning_rate": 3.072666666666667e-05,
      "loss": 0.003,
      "step": 57820
    },
    {
      "epoch": 3.0842666666666667,
      "grad_norm": 0.5983192920684814,
      "learning_rate": 3.072333333333334e-05,
      "loss": 0.0025,
      "step": 57830
    },
    {
      "epoch": 3.0848,
      "grad_norm": 0.3051028251647949,
      "learning_rate": 3.072e-05,
      "loss": 0.0031,
      "step": 57840
    },
    {
      "epoch": 3.0853333333333333,
      "grad_norm": 0.1877409964799881,
      "learning_rate": 3.0716666666666663e-05,
      "loss": 0.0029,
      "step": 57850
    },
    {
      "epoch": 3.0858666666666665,
      "grad_norm": 0.24451912939548492,
      "learning_rate": 3.071333333333333e-05,
      "loss": 0.0026,
      "step": 57860
    },
    {
      "epoch": 3.0864,
      "grad_norm": 0.15703533589839935,
      "learning_rate": 3.071e-05,
      "loss": 0.002,
      "step": 57870
    },
    {
      "epoch": 3.0869333333333335,
      "grad_norm": 0.3181372582912445,
      "learning_rate": 3.070666666666667e-05,
      "loss": 0.0029,
      "step": 57880
    },
    {
      "epoch": 3.087466666666667,
      "grad_norm": 0.0429486520588398,
      "learning_rate": 3.0703333333333335e-05,
      "loss": 0.0026,
      "step": 57890
    },
    {
      "epoch": 3.088,
      "grad_norm": 0.8053749799728394,
      "learning_rate": 3.07e-05,
      "loss": 0.0024,
      "step": 57900
    },
    {
      "epoch": 3.0885333333333334,
      "grad_norm": 0.2602476477622986,
      "learning_rate": 3.069666666666667e-05,
      "loss": 0.0028,
      "step": 57910
    },
    {
      "epoch": 3.0890666666666666,
      "grad_norm": 0.09265996515750885,
      "learning_rate": 3.069333333333333e-05,
      "loss": 0.0031,
      "step": 57920
    },
    {
      "epoch": 3.0896,
      "grad_norm": 0.4418599009513855,
      "learning_rate": 3.069e-05,
      "loss": 0.0032,
      "step": 57930
    },
    {
      "epoch": 3.090133333333333,
      "grad_norm": 0.10877774655818939,
      "learning_rate": 3.068666666666667e-05,
      "loss": 0.0035,
      "step": 57940
    },
    {
      "epoch": 3.0906666666666665,
      "grad_norm": 0.34096673130989075,
      "learning_rate": 3.068333333333334e-05,
      "loss": 0.0018,
      "step": 57950
    },
    {
      "epoch": 3.0912,
      "grad_norm": 0.43392452597618103,
      "learning_rate": 3.0680000000000004e-05,
      "loss": 0.0019,
      "step": 57960
    },
    {
      "epoch": 3.0917333333333334,
      "grad_norm": 0.02964591421186924,
      "learning_rate": 3.067666666666667e-05,
      "loss": 0.0021,
      "step": 57970
    },
    {
      "epoch": 3.0922666666666667,
      "grad_norm": 0.049540989100933075,
      "learning_rate": 3.067333333333334e-05,
      "loss": 0.0025,
      "step": 57980
    },
    {
      "epoch": 3.0928,
      "grad_norm": 0.1012946292757988,
      "learning_rate": 3.0669999999999996e-05,
      "loss": 0.0016,
      "step": 57990
    },
    {
      "epoch": 3.0933333333333333,
      "grad_norm": 0.1370386779308319,
      "learning_rate": 3.066666666666667e-05,
      "loss": 0.0029,
      "step": 58000
    },
    {
      "epoch": 3.0938666666666665,
      "grad_norm": 0.061942826956510544,
      "learning_rate": 3.0663333333333335e-05,
      "loss": 0.0022,
      "step": 58010
    },
    {
      "epoch": 3.0944,
      "grad_norm": 0.16102668642997742,
      "learning_rate": 3.066e-05,
      "loss": 0.0018,
      "step": 58020
    },
    {
      "epoch": 3.0949333333333335,
      "grad_norm": 0.21647220849990845,
      "learning_rate": 3.065666666666667e-05,
      "loss": 0.002,
      "step": 58030
    },
    {
      "epoch": 3.095466666666667,
      "grad_norm": 0.7594024538993835,
      "learning_rate": 3.0653333333333333e-05,
      "loss": 0.0025,
      "step": 58040
    },
    {
      "epoch": 3.096,
      "grad_norm": 0.7169950604438782,
      "learning_rate": 3.065e-05,
      "loss": 0.0025,
      "step": 58050
    },
    {
      "epoch": 3.0965333333333334,
      "grad_norm": 0.551823616027832,
      "learning_rate": 3.0646666666666666e-05,
      "loss": 0.0034,
      "step": 58060
    },
    {
      "epoch": 3.0970666666666666,
      "grad_norm": 0.13236458599567413,
      "learning_rate": 3.064333333333333e-05,
      "loss": 0.0024,
      "step": 58070
    },
    {
      "epoch": 3.0976,
      "grad_norm": 0.4260790944099426,
      "learning_rate": 3.0640000000000005e-05,
      "loss": 0.0019,
      "step": 58080
    },
    {
      "epoch": 3.098133333333333,
      "grad_norm": 0.3291226029396057,
      "learning_rate": 3.063666666666667e-05,
      "loss": 0.0026,
      "step": 58090
    },
    {
      "epoch": 3.0986666666666665,
      "grad_norm": 0.3220832645893097,
      "learning_rate": 3.063333333333334e-05,
      "loss": 0.0011,
      "step": 58100
    },
    {
      "epoch": 3.0992,
      "grad_norm": 0.1930377334356308,
      "learning_rate": 3.063e-05,
      "loss": 0.0033,
      "step": 58110
    },
    {
      "epoch": 3.0997333333333335,
      "grad_norm": 0.16611211001873016,
      "learning_rate": 3.062666666666667e-05,
      "loss": 0.0022,
      "step": 58120
    },
    {
      "epoch": 3.1002666666666667,
      "grad_norm": 0.5093697309494019,
      "learning_rate": 3.0623333333333335e-05,
      "loss": 0.0016,
      "step": 58130
    },
    {
      "epoch": 3.1008,
      "grad_norm": 0.6003437638282776,
      "learning_rate": 3.062e-05,
      "loss": 0.0027,
      "step": 58140
    },
    {
      "epoch": 3.1013333333333333,
      "grad_norm": 0.20274539291858673,
      "learning_rate": 3.061666666666667e-05,
      "loss": 0.0028,
      "step": 58150
    },
    {
      "epoch": 3.1018666666666665,
      "grad_norm": 0.4006572663784027,
      "learning_rate": 3.0613333333333334e-05,
      "loss": 0.0029,
      "step": 58160
    },
    {
      "epoch": 3.1024,
      "grad_norm": 0.3119279146194458,
      "learning_rate": 3.061e-05,
      "loss": 0.0022,
      "step": 58170
    },
    {
      "epoch": 3.1029333333333335,
      "grad_norm": 0.21296493709087372,
      "learning_rate": 3.0606666666666666e-05,
      "loss": 0.002,
      "step": 58180
    },
    {
      "epoch": 3.103466666666667,
      "grad_norm": 0.29307952523231506,
      "learning_rate": 3.060333333333333e-05,
      "loss": 0.0016,
      "step": 58190
    },
    {
      "epoch": 3.104,
      "grad_norm": 0.4856809675693512,
      "learning_rate": 3.06e-05,
      "loss": 0.0016,
      "step": 58200
    },
    {
      "epoch": 3.1045333333333334,
      "grad_norm": 0.03366614505648613,
      "learning_rate": 3.0596666666666665e-05,
      "loss": 0.0026,
      "step": 58210
    },
    {
      "epoch": 3.1050666666666666,
      "grad_norm": 0.39854657649993896,
      "learning_rate": 3.059333333333334e-05,
      "loss": 0.0019,
      "step": 58220
    },
    {
      "epoch": 3.1056,
      "grad_norm": 0.5199124217033386,
      "learning_rate": 3.0590000000000004e-05,
      "loss": 0.0021,
      "step": 58230
    },
    {
      "epoch": 3.106133333333333,
      "grad_norm": 0.48825037479400635,
      "learning_rate": 3.058666666666667e-05,
      "loss": 0.0019,
      "step": 58240
    },
    {
      "epoch": 3.1066666666666665,
      "grad_norm": 0.1548605114221573,
      "learning_rate": 3.0583333333333336e-05,
      "loss": 0.0021,
      "step": 58250
    },
    {
      "epoch": 3.1072,
      "grad_norm": 0.16451972723007202,
      "learning_rate": 3.058e-05,
      "loss": 0.0019,
      "step": 58260
    },
    {
      "epoch": 3.1077333333333335,
      "grad_norm": 0.46238428354263306,
      "learning_rate": 3.057666666666667e-05,
      "loss": 0.0022,
      "step": 58270
    },
    {
      "epoch": 3.1082666666666667,
      "grad_norm": 0.07103300839662552,
      "learning_rate": 3.0573333333333334e-05,
      "loss": 0.0021,
      "step": 58280
    },
    {
      "epoch": 3.1088,
      "grad_norm": 0.2767535448074341,
      "learning_rate": 3.057000000000001e-05,
      "loss": 0.0026,
      "step": 58290
    },
    {
      "epoch": 3.1093333333333333,
      "grad_norm": 0.1295485943555832,
      "learning_rate": 3.0566666666666667e-05,
      "loss": 0.0031,
      "step": 58300
    },
    {
      "epoch": 3.1098666666666666,
      "grad_norm": 0.5102651119232178,
      "learning_rate": 3.056333333333333e-05,
      "loss": 0.0017,
      "step": 58310
    },
    {
      "epoch": 3.1104,
      "grad_norm": 0.1704287976026535,
      "learning_rate": 3.056e-05,
      "loss": 0.0021,
      "step": 58320
    },
    {
      "epoch": 3.1109333333333336,
      "grad_norm": 0.272519588470459,
      "learning_rate": 3.0556666666666665e-05,
      "loss": 0.0017,
      "step": 58330
    },
    {
      "epoch": 3.111466666666667,
      "grad_norm": 0.0932174101471901,
      "learning_rate": 3.055333333333333e-05,
      "loss": 0.0019,
      "step": 58340
    },
    {
      "epoch": 3.112,
      "grad_norm": 0.395681768655777,
      "learning_rate": 3.0550000000000004e-05,
      "loss": 0.0031,
      "step": 58350
    },
    {
      "epoch": 3.1125333333333334,
      "grad_norm": 0.3141382336616516,
      "learning_rate": 3.054666666666667e-05,
      "loss": 0.0019,
      "step": 58360
    },
    {
      "epoch": 3.1130666666666666,
      "grad_norm": 0.19436170160770416,
      "learning_rate": 3.0543333333333336e-05,
      "loss": 0.0017,
      "step": 58370
    },
    {
      "epoch": 3.1136,
      "grad_norm": 0.23159141838550568,
      "learning_rate": 3.054e-05,
      "loss": 0.0022,
      "step": 58380
    },
    {
      "epoch": 3.114133333333333,
      "grad_norm": 0.2588140368461609,
      "learning_rate": 3.053666666666667e-05,
      "loss": 0.0016,
      "step": 58390
    },
    {
      "epoch": 3.1146666666666665,
      "grad_norm": 0.0438518151640892,
      "learning_rate": 3.0533333333333335e-05,
      "loss": 0.0024,
      "step": 58400
    },
    {
      "epoch": 3.1152,
      "grad_norm": 0.05135343223810196,
      "learning_rate": 3.053e-05,
      "loss": 0.0016,
      "step": 58410
    },
    {
      "epoch": 3.1157333333333335,
      "grad_norm": 0.13280588388442993,
      "learning_rate": 3.052666666666667e-05,
      "loss": 0.0017,
      "step": 58420
    },
    {
      "epoch": 3.1162666666666667,
      "grad_norm": 0.18766142427921295,
      "learning_rate": 3.052333333333334e-05,
      "loss": 0.0029,
      "step": 58430
    },
    {
      "epoch": 3.1168,
      "grad_norm": 0.23939117789268494,
      "learning_rate": 3.0520000000000006e-05,
      "loss": 0.0029,
      "step": 58440
    },
    {
      "epoch": 3.1173333333333333,
      "grad_norm": 0.4285787045955658,
      "learning_rate": 3.0516666666666665e-05,
      "loss": 0.0023,
      "step": 58450
    },
    {
      "epoch": 3.1178666666666666,
      "grad_norm": 0.4334297478199005,
      "learning_rate": 3.051333333333333e-05,
      "loss": 0.0032,
      "step": 58460
    },
    {
      "epoch": 3.1184,
      "grad_norm": 0.4457307755947113,
      "learning_rate": 3.051e-05,
      "loss": 0.0024,
      "step": 58470
    },
    {
      "epoch": 3.1189333333333336,
      "grad_norm": 0.2844572067260742,
      "learning_rate": 3.0506666666666667e-05,
      "loss": 0.0017,
      "step": 58480
    },
    {
      "epoch": 3.119466666666667,
      "grad_norm": 0.3225293457508087,
      "learning_rate": 3.0503333333333333e-05,
      "loss": 0.002,
      "step": 58490
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.28470107913017273,
      "learning_rate": 3.05e-05,
      "loss": 0.0019,
      "step": 58500
    },
    {
      "epoch": 3.1205333333333334,
      "grad_norm": 0.8745958805084229,
      "learning_rate": 3.049666666666667e-05,
      "loss": 0.0028,
      "step": 58510
    },
    {
      "epoch": 3.1210666666666667,
      "grad_norm": 0.5343183875083923,
      "learning_rate": 3.0493333333333335e-05,
      "loss": 0.0014,
      "step": 58520
    },
    {
      "epoch": 3.1216,
      "grad_norm": 0.47367337346076965,
      "learning_rate": 3.049e-05,
      "loss": 0.0025,
      "step": 58530
    },
    {
      "epoch": 3.122133333333333,
      "grad_norm": 0.049189433455467224,
      "learning_rate": 3.0486666666666667e-05,
      "loss": 0.0028,
      "step": 58540
    },
    {
      "epoch": 3.1226666666666665,
      "grad_norm": 0.1282869577407837,
      "learning_rate": 3.0483333333333337e-05,
      "loss": 0.0026,
      "step": 58550
    },
    {
      "epoch": 3.1232,
      "grad_norm": 0.3244956433773041,
      "learning_rate": 3.0480000000000003e-05,
      "loss": 0.0019,
      "step": 58560
    },
    {
      "epoch": 3.1237333333333335,
      "grad_norm": 0.2419031858444214,
      "learning_rate": 3.047666666666667e-05,
      "loss": 0.0019,
      "step": 58570
    },
    {
      "epoch": 3.1242666666666667,
      "grad_norm": 0.33021116256713867,
      "learning_rate": 3.047333333333334e-05,
      "loss": 0.0017,
      "step": 58580
    },
    {
      "epoch": 3.1248,
      "grad_norm": 0.5063244104385376,
      "learning_rate": 3.0470000000000005e-05,
      "loss": 0.0019,
      "step": 58590
    },
    {
      "epoch": 3.1253333333333333,
      "grad_norm": 0.2212345153093338,
      "learning_rate": 3.0466666666666664e-05,
      "loss": 0.0024,
      "step": 58600
    },
    {
      "epoch": 3.1258666666666666,
      "grad_norm": 0.11209359765052795,
      "learning_rate": 3.0463333333333334e-05,
      "loss": 0.0023,
      "step": 58610
    },
    {
      "epoch": 3.1264,
      "grad_norm": 0.15571652352809906,
      "learning_rate": 3.046e-05,
      "loss": 0.0016,
      "step": 58620
    },
    {
      "epoch": 3.1269333333333336,
      "grad_norm": 0.5576063990592957,
      "learning_rate": 3.0456666666666666e-05,
      "loss": 0.0022,
      "step": 58630
    },
    {
      "epoch": 3.127466666666667,
      "grad_norm": 0.35341712832450867,
      "learning_rate": 3.0453333333333335e-05,
      "loss": 0.0021,
      "step": 58640
    },
    {
      "epoch": 3.128,
      "grad_norm": 0.4646592140197754,
      "learning_rate": 3.045e-05,
      "loss": 0.0021,
      "step": 58650
    },
    {
      "epoch": 3.1285333333333334,
      "grad_norm": 0.36433905363082886,
      "learning_rate": 3.0446666666666668e-05,
      "loss": 0.002,
      "step": 58660
    },
    {
      "epoch": 3.1290666666666667,
      "grad_norm": 0.1694239228963852,
      "learning_rate": 3.0443333333333334e-05,
      "loss": 0.002,
      "step": 58670
    },
    {
      "epoch": 3.1296,
      "grad_norm": 0.24201764166355133,
      "learning_rate": 3.0440000000000003e-05,
      "loss": 0.0023,
      "step": 58680
    },
    {
      "epoch": 3.130133333333333,
      "grad_norm": 0.15076419711112976,
      "learning_rate": 3.043666666666667e-05,
      "loss": 0.0027,
      "step": 58690
    },
    {
      "epoch": 3.1306666666666665,
      "grad_norm": 0.11500529199838638,
      "learning_rate": 3.0433333333333336e-05,
      "loss": 0.0017,
      "step": 58700
    },
    {
      "epoch": 3.1312,
      "grad_norm": 0.10717973113059998,
      "learning_rate": 3.0430000000000002e-05,
      "loss": 0.0016,
      "step": 58710
    },
    {
      "epoch": 3.1317333333333335,
      "grad_norm": 0.053530141711235046,
      "learning_rate": 3.042666666666667e-05,
      "loss": 0.0024,
      "step": 58720
    },
    {
      "epoch": 3.1322666666666668,
      "grad_norm": 0.49913689494132996,
      "learning_rate": 3.0423333333333337e-05,
      "loss": 0.0018,
      "step": 58730
    },
    {
      "epoch": 3.1328,
      "grad_norm": 0.032074637711048126,
      "learning_rate": 3.0420000000000004e-05,
      "loss": 0.0018,
      "step": 58740
    },
    {
      "epoch": 3.1333333333333333,
      "grad_norm": 0.215662881731987,
      "learning_rate": 3.0416666666666666e-05,
      "loss": 0.0017,
      "step": 58750
    },
    {
      "epoch": 3.1338666666666666,
      "grad_norm": 0.6434386968612671,
      "learning_rate": 3.0413333333333332e-05,
      "loss": 0.0022,
      "step": 58760
    },
    {
      "epoch": 3.1344,
      "grad_norm": 0.3352281153202057,
      "learning_rate": 3.041e-05,
      "loss": 0.0019,
      "step": 58770
    },
    {
      "epoch": 3.134933333333333,
      "grad_norm": 0.27977752685546875,
      "learning_rate": 3.0406666666666668e-05,
      "loss": 0.0021,
      "step": 58780
    },
    {
      "epoch": 3.135466666666667,
      "grad_norm": 0.5615038275718689,
      "learning_rate": 3.0403333333333334e-05,
      "loss": 0.0022,
      "step": 58790
    },
    {
      "epoch": 3.136,
      "grad_norm": 0.43239742517471313,
      "learning_rate": 3.04e-05,
      "loss": 0.0019,
      "step": 58800
    },
    {
      "epoch": 3.1365333333333334,
      "grad_norm": 0.33099788427352905,
      "learning_rate": 3.0396666666666666e-05,
      "loss": 0.0014,
      "step": 58810
    },
    {
      "epoch": 3.1370666666666667,
      "grad_norm": 0.10252733528614044,
      "learning_rate": 3.0393333333333336e-05,
      "loss": 0.0027,
      "step": 58820
    },
    {
      "epoch": 3.1376,
      "grad_norm": 0.19497761130332947,
      "learning_rate": 3.0390000000000002e-05,
      "loss": 0.0022,
      "step": 58830
    },
    {
      "epoch": 3.138133333333333,
      "grad_norm": 0.2436622977256775,
      "learning_rate": 3.0386666666666668e-05,
      "loss": 0.0028,
      "step": 58840
    },
    {
      "epoch": 3.1386666666666665,
      "grad_norm": 0.27891257405281067,
      "learning_rate": 3.0383333333333334e-05,
      "loss": 0.0018,
      "step": 58850
    },
    {
      "epoch": 3.1391999999999998,
      "grad_norm": 0.23175758123397827,
      "learning_rate": 3.0380000000000004e-05,
      "loss": 0.002,
      "step": 58860
    },
    {
      "epoch": 3.1397333333333335,
      "grad_norm": 0.2878095805644989,
      "learning_rate": 3.037666666666667e-05,
      "loss": 0.003,
      "step": 58870
    },
    {
      "epoch": 3.1402666666666668,
      "grad_norm": 0.08272003382444382,
      "learning_rate": 3.0373333333333336e-05,
      "loss": 0.0033,
      "step": 58880
    },
    {
      "epoch": 3.1408,
      "grad_norm": 0.6422697305679321,
      "learning_rate": 3.0370000000000006e-05,
      "loss": 0.0022,
      "step": 58890
    },
    {
      "epoch": 3.1413333333333333,
      "grad_norm": 0.3228006064891815,
      "learning_rate": 3.0366666666666665e-05,
      "loss": 0.0035,
      "step": 58900
    },
    {
      "epoch": 3.1418666666666666,
      "grad_norm": 0.1939292997121811,
      "learning_rate": 3.036333333333333e-05,
      "loss": 0.0021,
      "step": 58910
    },
    {
      "epoch": 3.1424,
      "grad_norm": 0.25836628675460815,
      "learning_rate": 3.036e-05,
      "loss": 0.0021,
      "step": 58920
    },
    {
      "epoch": 3.142933333333333,
      "grad_norm": 0.2747792601585388,
      "learning_rate": 3.0356666666666667e-05,
      "loss": 0.002,
      "step": 58930
    },
    {
      "epoch": 3.143466666666667,
      "grad_norm": 0.3395027220249176,
      "learning_rate": 3.0353333333333333e-05,
      "loss": 0.0026,
      "step": 58940
    },
    {
      "epoch": 3.144,
      "grad_norm": 0.7904723882675171,
      "learning_rate": 3.035e-05,
      "loss": 0.0022,
      "step": 58950
    },
    {
      "epoch": 3.1445333333333334,
      "grad_norm": 0.25466200709342957,
      "learning_rate": 3.034666666666667e-05,
      "loss": 0.0024,
      "step": 58960
    },
    {
      "epoch": 3.1450666666666667,
      "grad_norm": 0.3206258714199066,
      "learning_rate": 3.0343333333333335e-05,
      "loss": 0.0028,
      "step": 58970
    },
    {
      "epoch": 3.1456,
      "grad_norm": 0.4199194312095642,
      "learning_rate": 3.034e-05,
      "loss": 0.0022,
      "step": 58980
    },
    {
      "epoch": 3.1461333333333332,
      "grad_norm": 0.7404218316078186,
      "learning_rate": 3.033666666666667e-05,
      "loss": 0.0021,
      "step": 58990
    },
    {
      "epoch": 3.1466666666666665,
      "grad_norm": 0.37704455852508545,
      "learning_rate": 3.0333333333333337e-05,
      "loss": 0.003,
      "step": 59000
    },
    {
      "epoch": 3.1471999999999998,
      "grad_norm": 0.3216022253036499,
      "learning_rate": 3.0330000000000003e-05,
      "loss": 0.0023,
      "step": 59010
    },
    {
      "epoch": 3.1477333333333335,
      "grad_norm": 0.14337702095508575,
      "learning_rate": 3.032666666666667e-05,
      "loss": 0.0024,
      "step": 59020
    },
    {
      "epoch": 3.1482666666666668,
      "grad_norm": 0.16596803069114685,
      "learning_rate": 3.032333333333334e-05,
      "loss": 0.0019,
      "step": 59030
    },
    {
      "epoch": 3.1488,
      "grad_norm": 0.3697643280029297,
      "learning_rate": 3.0320000000000004e-05,
      "loss": 0.0022,
      "step": 59040
    },
    {
      "epoch": 3.1493333333333333,
      "grad_norm": 0.024071503430604935,
      "learning_rate": 3.0316666666666664e-05,
      "loss": 0.0029,
      "step": 59050
    },
    {
      "epoch": 3.1498666666666666,
      "grad_norm": 0.054386455565690994,
      "learning_rate": 3.0313333333333333e-05,
      "loss": 0.0021,
      "step": 59060
    },
    {
      "epoch": 3.1504,
      "grad_norm": 0.10657010227441788,
      "learning_rate": 3.031e-05,
      "loss": 0.002,
      "step": 59070
    },
    {
      "epoch": 3.150933333333333,
      "grad_norm": 0.19925875961780548,
      "learning_rate": 3.0306666666666666e-05,
      "loss": 0.0024,
      "step": 59080
    },
    {
      "epoch": 3.151466666666667,
      "grad_norm": 0.5992729067802429,
      "learning_rate": 3.0303333333333335e-05,
      "loss": 0.0028,
      "step": 59090
    },
    {
      "epoch": 3.152,
      "grad_norm": 0.7679714560508728,
      "learning_rate": 3.03e-05,
      "loss": 0.0017,
      "step": 59100
    },
    {
      "epoch": 3.1525333333333334,
      "grad_norm": 0.24997974932193756,
      "learning_rate": 3.0296666666666667e-05,
      "loss": 0.0021,
      "step": 59110
    },
    {
      "epoch": 3.1530666666666667,
      "grad_norm": 0.19857719540596008,
      "learning_rate": 3.0293333333333334e-05,
      "loss": 0.0028,
      "step": 59120
    },
    {
      "epoch": 3.1536,
      "grad_norm": 0.24374830722808838,
      "learning_rate": 3.0290000000000003e-05,
      "loss": 0.0025,
      "step": 59130
    },
    {
      "epoch": 3.1541333333333332,
      "grad_norm": 0.14247214794158936,
      "learning_rate": 3.028666666666667e-05,
      "loss": 0.0015,
      "step": 59140
    },
    {
      "epoch": 3.1546666666666665,
      "grad_norm": 0.3269755244255066,
      "learning_rate": 3.0283333333333335e-05,
      "loss": 0.0017,
      "step": 59150
    },
    {
      "epoch": 3.1552,
      "grad_norm": 0.2530667185783386,
      "learning_rate": 3.028e-05,
      "loss": 0.0019,
      "step": 59160
    },
    {
      "epoch": 3.1557333333333335,
      "grad_norm": 0.4236816465854645,
      "learning_rate": 3.027666666666667e-05,
      "loss": 0.0017,
      "step": 59170
    },
    {
      "epoch": 3.1562666666666668,
      "grad_norm": 0.5249015092849731,
      "learning_rate": 3.0273333333333337e-05,
      "loss": 0.0018,
      "step": 59180
    },
    {
      "epoch": 3.1568,
      "grad_norm": 0.058989107608795166,
      "learning_rate": 3.0270000000000003e-05,
      "loss": 0.0025,
      "step": 59190
    },
    {
      "epoch": 3.1573333333333333,
      "grad_norm": 0.3701271414756775,
      "learning_rate": 3.0266666666666666e-05,
      "loss": 0.0019,
      "step": 59200
    },
    {
      "epoch": 3.1578666666666666,
      "grad_norm": 0.09011847525835037,
      "learning_rate": 3.0263333333333332e-05,
      "loss": 0.0018,
      "step": 59210
    },
    {
      "epoch": 3.1584,
      "grad_norm": 0.4470360279083252,
      "learning_rate": 3.0259999999999998e-05,
      "loss": 0.0022,
      "step": 59220
    },
    {
      "epoch": 3.158933333333333,
      "grad_norm": 0.08222176879644394,
      "learning_rate": 3.0256666666666668e-05,
      "loss": 0.0019,
      "step": 59230
    },
    {
      "epoch": 3.159466666666667,
      "grad_norm": 0.18189923465251923,
      "learning_rate": 3.0253333333333334e-05,
      "loss": 0.0022,
      "step": 59240
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.2186661958694458,
      "learning_rate": 3.025e-05,
      "loss": 0.0015,
      "step": 59250
    },
    {
      "epoch": 3.1605333333333334,
      "grad_norm": 0.18206451833248138,
      "learning_rate": 3.0246666666666666e-05,
      "loss": 0.0026,
      "step": 59260
    },
    {
      "epoch": 3.1610666666666667,
      "grad_norm": 0.08270654082298279,
      "learning_rate": 3.0243333333333336e-05,
      "loss": 0.0018,
      "step": 59270
    },
    {
      "epoch": 3.1616,
      "grad_norm": 0.09376054257154465,
      "learning_rate": 3.0240000000000002e-05,
      "loss": 0.0017,
      "step": 59280
    },
    {
      "epoch": 3.1621333333333332,
      "grad_norm": 0.05229496583342552,
      "learning_rate": 3.0236666666666668e-05,
      "loss": 0.002,
      "step": 59290
    },
    {
      "epoch": 3.1626666666666665,
      "grad_norm": 0.028549674898386,
      "learning_rate": 3.0233333333333334e-05,
      "loss": 0.0029,
      "step": 59300
    },
    {
      "epoch": 3.1632,
      "grad_norm": 0.16436463594436646,
      "learning_rate": 3.0230000000000004e-05,
      "loss": 0.0022,
      "step": 59310
    },
    {
      "epoch": 3.1637333333333335,
      "grad_norm": 0.0691787451505661,
      "learning_rate": 3.022666666666667e-05,
      "loss": 0.0024,
      "step": 59320
    },
    {
      "epoch": 3.164266666666667,
      "grad_norm": 0.07287923991680145,
      "learning_rate": 3.0223333333333336e-05,
      "loss": 0.0015,
      "step": 59330
    },
    {
      "epoch": 3.1648,
      "grad_norm": 0.12112249433994293,
      "learning_rate": 3.0220000000000005e-05,
      "loss": 0.0024,
      "step": 59340
    },
    {
      "epoch": 3.1653333333333333,
      "grad_norm": 0.30747276544570923,
      "learning_rate": 3.0216666666666665e-05,
      "loss": 0.0025,
      "step": 59350
    },
    {
      "epoch": 3.1658666666666666,
      "grad_norm": 0.3812248706817627,
      "learning_rate": 3.021333333333333e-05,
      "loss": 0.0019,
      "step": 59360
    },
    {
      "epoch": 3.1664,
      "grad_norm": 0.0740562155842781,
      "learning_rate": 3.021e-05,
      "loss": 0.0024,
      "step": 59370
    },
    {
      "epoch": 3.166933333333333,
      "grad_norm": 0.07896427065134048,
      "learning_rate": 3.0206666666666667e-05,
      "loss": 0.0024,
      "step": 59380
    },
    {
      "epoch": 3.167466666666667,
      "grad_norm": 0.40869152545928955,
      "learning_rate": 3.0203333333333333e-05,
      "loss": 0.002,
      "step": 59390
    },
    {
      "epoch": 3.168,
      "grad_norm": 0.13125956058502197,
      "learning_rate": 3.02e-05,
      "loss": 0.0023,
      "step": 59400
    },
    {
      "epoch": 3.1685333333333334,
      "grad_norm": 0.29694047570228577,
      "learning_rate": 3.019666666666667e-05,
      "loss": 0.002,
      "step": 59410
    },
    {
      "epoch": 3.1690666666666667,
      "grad_norm": 0.4105302393436432,
      "learning_rate": 3.0193333333333335e-05,
      "loss": 0.0025,
      "step": 59420
    },
    {
      "epoch": 3.1696,
      "grad_norm": 0.4950287640094757,
      "learning_rate": 3.019e-05,
      "loss": 0.0026,
      "step": 59430
    },
    {
      "epoch": 3.1701333333333332,
      "grad_norm": 0.7561742067337036,
      "learning_rate": 3.018666666666667e-05,
      "loss": 0.0017,
      "step": 59440
    },
    {
      "epoch": 3.1706666666666665,
      "grad_norm": 0.4231611490249634,
      "learning_rate": 3.0183333333333336e-05,
      "loss": 0.0016,
      "step": 59450
    },
    {
      "epoch": 3.1712,
      "grad_norm": 0.19497130811214447,
      "learning_rate": 3.0180000000000002e-05,
      "loss": 0.0025,
      "step": 59460
    },
    {
      "epoch": 3.1717333333333335,
      "grad_norm": 0.6650346517562866,
      "learning_rate": 3.017666666666667e-05,
      "loss": 0.0014,
      "step": 59470
    },
    {
      "epoch": 3.172266666666667,
      "grad_norm": 0.4101206064224243,
      "learning_rate": 3.0173333333333338e-05,
      "loss": 0.0026,
      "step": 59480
    },
    {
      "epoch": 3.1728,
      "grad_norm": 0.05868752300739288,
      "learning_rate": 3.0170000000000004e-05,
      "loss": 0.0021,
      "step": 59490
    },
    {
      "epoch": 3.1733333333333333,
      "grad_norm": 0.22164854407310486,
      "learning_rate": 3.016666666666667e-05,
      "loss": 0.0021,
      "step": 59500
    },
    {
      "epoch": 3.1738666666666666,
      "grad_norm": 0.04202023148536682,
      "learning_rate": 3.0163333333333333e-05,
      "loss": 0.0018,
      "step": 59510
    },
    {
      "epoch": 3.1744,
      "grad_norm": 0.30866482853889465,
      "learning_rate": 3.016e-05,
      "loss": 0.0024,
      "step": 59520
    },
    {
      "epoch": 3.174933333333333,
      "grad_norm": 0.19932739436626434,
      "learning_rate": 3.0156666666666665e-05,
      "loss": 0.0018,
      "step": 59530
    },
    {
      "epoch": 3.175466666666667,
      "grad_norm": 0.13013221323490143,
      "learning_rate": 3.0153333333333335e-05,
      "loss": 0.0023,
      "step": 59540
    },
    {
      "epoch": 3.176,
      "grad_norm": 0.2727717161178589,
      "learning_rate": 3.015e-05,
      "loss": 0.0017,
      "step": 59550
    },
    {
      "epoch": 3.1765333333333334,
      "grad_norm": 0.028896169736981392,
      "learning_rate": 3.0146666666666667e-05,
      "loss": 0.0018,
      "step": 59560
    },
    {
      "epoch": 3.1770666666666667,
      "grad_norm": 0.06702440232038498,
      "learning_rate": 3.0143333333333333e-05,
      "loss": 0.0016,
      "step": 59570
    },
    {
      "epoch": 3.1776,
      "grad_norm": 0.218366801738739,
      "learning_rate": 3.0140000000000003e-05,
      "loss": 0.002,
      "step": 59580
    },
    {
      "epoch": 3.1781333333333333,
      "grad_norm": 0.07379800081253052,
      "learning_rate": 3.013666666666667e-05,
      "loss": 0.0019,
      "step": 59590
    },
    {
      "epoch": 3.1786666666666665,
      "grad_norm": 0.4175618886947632,
      "learning_rate": 3.0133333333333335e-05,
      "loss": 0.0029,
      "step": 59600
    },
    {
      "epoch": 3.1792,
      "grad_norm": 0.2685036361217499,
      "learning_rate": 3.013e-05,
      "loss": 0.0018,
      "step": 59610
    },
    {
      "epoch": 3.1797333333333335,
      "grad_norm": 0.19651538133621216,
      "learning_rate": 3.012666666666667e-05,
      "loss": 0.0018,
      "step": 59620
    },
    {
      "epoch": 3.180266666666667,
      "grad_norm": 0.31435585021972656,
      "learning_rate": 3.0123333333333337e-05,
      "loss": 0.0023,
      "step": 59630
    },
    {
      "epoch": 3.1808,
      "grad_norm": 0.28758811950683594,
      "learning_rate": 3.0120000000000003e-05,
      "loss": 0.0028,
      "step": 59640
    },
    {
      "epoch": 3.1813333333333333,
      "grad_norm": 0.3369525671005249,
      "learning_rate": 3.011666666666667e-05,
      "loss": 0.0017,
      "step": 59650
    },
    {
      "epoch": 3.1818666666666666,
      "grad_norm": 0.3938734233379364,
      "learning_rate": 3.0113333333333332e-05,
      "loss": 0.0017,
      "step": 59660
    },
    {
      "epoch": 3.1824,
      "grad_norm": 0.31813114881515503,
      "learning_rate": 3.0109999999999998e-05,
      "loss": 0.0014,
      "step": 59670
    },
    {
      "epoch": 3.182933333333333,
      "grad_norm": 0.08970429003238678,
      "learning_rate": 3.0106666666666668e-05,
      "loss": 0.0018,
      "step": 59680
    },
    {
      "epoch": 3.183466666666667,
      "grad_norm": 0.1062358021736145,
      "learning_rate": 3.0103333333333334e-05,
      "loss": 0.0029,
      "step": 59690
    },
    {
      "epoch": 3.184,
      "grad_norm": 0.519676685333252,
      "learning_rate": 3.01e-05,
      "loss": 0.0018,
      "step": 59700
    },
    {
      "epoch": 3.1845333333333334,
      "grad_norm": 0.20747502148151398,
      "learning_rate": 3.0096666666666666e-05,
      "loss": 0.0026,
      "step": 59710
    },
    {
      "epoch": 3.1850666666666667,
      "grad_norm": 0.46382763981819153,
      "learning_rate": 3.0093333333333335e-05,
      "loss": 0.0031,
      "step": 59720
    },
    {
      "epoch": 3.1856,
      "grad_norm": 0.11156942695379257,
      "learning_rate": 3.009e-05,
      "loss": 0.0013,
      "step": 59730
    },
    {
      "epoch": 3.1861333333333333,
      "grad_norm": 0.21587666869163513,
      "learning_rate": 3.0086666666666668e-05,
      "loss": 0.0037,
      "step": 59740
    },
    {
      "epoch": 3.1866666666666665,
      "grad_norm": 0.3824552297592163,
      "learning_rate": 3.0083333333333337e-05,
      "loss": 0.0033,
      "step": 59750
    },
    {
      "epoch": 3.1872,
      "grad_norm": 0.04619495943188667,
      "learning_rate": 3.0080000000000003e-05,
      "loss": 0.002,
      "step": 59760
    },
    {
      "epoch": 3.1877333333333335,
      "grad_norm": 0.11152078956365585,
      "learning_rate": 3.007666666666667e-05,
      "loss": 0.0016,
      "step": 59770
    },
    {
      "epoch": 3.188266666666667,
      "grad_norm": 0.5441855788230896,
      "learning_rate": 3.0073333333333336e-05,
      "loss": 0.002,
      "step": 59780
    },
    {
      "epoch": 3.1888,
      "grad_norm": 0.34845513105392456,
      "learning_rate": 3.0070000000000005e-05,
      "loss": 0.003,
      "step": 59790
    },
    {
      "epoch": 3.1893333333333334,
      "grad_norm": 0.4178325831890106,
      "learning_rate": 3.006666666666667e-05,
      "loss": 0.0016,
      "step": 59800
    },
    {
      "epoch": 3.1898666666666666,
      "grad_norm": 0.3849298655986786,
      "learning_rate": 3.006333333333333e-05,
      "loss": 0.0029,
      "step": 59810
    },
    {
      "epoch": 3.1904,
      "grad_norm": 0.1031995341181755,
      "learning_rate": 3.006e-05,
      "loss": 0.0018,
      "step": 59820
    },
    {
      "epoch": 3.190933333333333,
      "grad_norm": 0.6720146536827087,
      "learning_rate": 3.0056666666666666e-05,
      "loss": 0.0017,
      "step": 59830
    },
    {
      "epoch": 3.191466666666667,
      "grad_norm": 0.6113181710243225,
      "learning_rate": 3.0053333333333332e-05,
      "loss": 0.0029,
      "step": 59840
    },
    {
      "epoch": 3.192,
      "grad_norm": 0.08326377719640732,
      "learning_rate": 3.0050000000000002e-05,
      "loss": 0.0015,
      "step": 59850
    },
    {
      "epoch": 3.1925333333333334,
      "grad_norm": 0.4321269989013672,
      "learning_rate": 3.0046666666666668e-05,
      "loss": 0.0024,
      "step": 59860
    },
    {
      "epoch": 3.1930666666666667,
      "grad_norm": 0.44755837321281433,
      "learning_rate": 3.0043333333333334e-05,
      "loss": 0.0021,
      "step": 59870
    },
    {
      "epoch": 3.1936,
      "grad_norm": 0.5970684885978699,
      "learning_rate": 3.004e-05,
      "loss": 0.0018,
      "step": 59880
    },
    {
      "epoch": 3.1941333333333333,
      "grad_norm": 0.18696953356266022,
      "learning_rate": 3.003666666666667e-05,
      "loss": 0.0033,
      "step": 59890
    },
    {
      "epoch": 3.1946666666666665,
      "grad_norm": 0.4596714973449707,
      "learning_rate": 3.0033333333333336e-05,
      "loss": 0.0027,
      "step": 59900
    },
    {
      "epoch": 3.1952,
      "grad_norm": 0.12381722778081894,
      "learning_rate": 3.0030000000000002e-05,
      "loss": 0.002,
      "step": 59910
    },
    {
      "epoch": 3.1957333333333335,
      "grad_norm": 0.10413579642772675,
      "learning_rate": 3.0026666666666668e-05,
      "loss": 0.002,
      "step": 59920
    },
    {
      "epoch": 3.196266666666667,
      "grad_norm": 0.5035181045532227,
      "learning_rate": 3.0023333333333338e-05,
      "loss": 0.0026,
      "step": 59930
    },
    {
      "epoch": 3.1968,
      "grad_norm": 0.06857514381408691,
      "learning_rate": 3.0020000000000004e-05,
      "loss": 0.0032,
      "step": 59940
    },
    {
      "epoch": 3.1973333333333334,
      "grad_norm": 0.10517560690641403,
      "learning_rate": 3.001666666666667e-05,
      "loss": 0.0024,
      "step": 59950
    },
    {
      "epoch": 3.1978666666666666,
      "grad_norm": 0.11421684920787811,
      "learning_rate": 3.0013333333333333e-05,
      "loss": 0.002,
      "step": 59960
    },
    {
      "epoch": 3.1984,
      "grad_norm": 0.08751069754362106,
      "learning_rate": 3.001e-05,
      "loss": 0.0023,
      "step": 59970
    },
    {
      "epoch": 3.198933333333333,
      "grad_norm": 0.040683563798666,
      "learning_rate": 3.0006666666666665e-05,
      "loss": 0.0025,
      "step": 59980
    },
    {
      "epoch": 3.1994666666666665,
      "grad_norm": 0.21433210372924805,
      "learning_rate": 3.0003333333333335e-05,
      "loss": 0.0014,
      "step": 59990
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.29870399832725525,
      "learning_rate": 3e-05,
      "loss": 0.0023,
      "step": 60000
    },
    {
      "epoch": 3.2005333333333335,
      "grad_norm": 0.0359821617603302,
      "learning_rate": 2.9996666666666667e-05,
      "loss": 0.002,
      "step": 60010
    },
    {
      "epoch": 3.2010666666666667,
      "grad_norm": 0.18458078801631927,
      "learning_rate": 2.9993333333333333e-05,
      "loss": 0.0019,
      "step": 60020
    },
    {
      "epoch": 3.2016,
      "grad_norm": 0.13179808855056763,
      "learning_rate": 2.9990000000000003e-05,
      "loss": 0.0021,
      "step": 60030
    },
    {
      "epoch": 3.2021333333333333,
      "grad_norm": 0.42277055978775024,
      "learning_rate": 2.998666666666667e-05,
      "loss": 0.0026,
      "step": 60040
    },
    {
      "epoch": 3.2026666666666666,
      "grad_norm": 0.14018040895462036,
      "learning_rate": 2.9983333333333335e-05,
      "loss": 0.0031,
      "step": 60050
    },
    {
      "epoch": 3.2032,
      "grad_norm": 0.3789895474910736,
      "learning_rate": 2.998e-05,
      "loss": 0.0019,
      "step": 60060
    },
    {
      "epoch": 3.203733333333333,
      "grad_norm": 0.21570460498332977,
      "learning_rate": 2.997666666666667e-05,
      "loss": 0.0025,
      "step": 60070
    },
    {
      "epoch": 3.204266666666667,
      "grad_norm": 0.3783626854419708,
      "learning_rate": 2.9973333333333337e-05,
      "loss": 0.0025,
      "step": 60080
    },
    {
      "epoch": 3.2048,
      "grad_norm": 0.6038054823875427,
      "learning_rate": 2.9970000000000003e-05,
      "loss": 0.0031,
      "step": 60090
    },
    {
      "epoch": 3.2053333333333334,
      "grad_norm": 0.04954664036631584,
      "learning_rate": 2.9966666666666672e-05,
      "loss": 0.003,
      "step": 60100
    },
    {
      "epoch": 3.2058666666666666,
      "grad_norm": 0.22191068530082703,
      "learning_rate": 2.996333333333333e-05,
      "loss": 0.0027,
      "step": 60110
    },
    {
      "epoch": 3.2064,
      "grad_norm": 0.04920932650566101,
      "learning_rate": 2.9959999999999998e-05,
      "loss": 0.0015,
      "step": 60120
    },
    {
      "epoch": 3.206933333333333,
      "grad_norm": 0.33285003900527954,
      "learning_rate": 2.9956666666666667e-05,
      "loss": 0.0019,
      "step": 60130
    },
    {
      "epoch": 3.2074666666666665,
      "grad_norm": 0.1346627175807953,
      "learning_rate": 2.9953333333333333e-05,
      "loss": 0.0024,
      "step": 60140
    },
    {
      "epoch": 3.208,
      "grad_norm": 0.07822954654693604,
      "learning_rate": 2.995e-05,
      "loss": 0.0022,
      "step": 60150
    },
    {
      "epoch": 3.2085333333333335,
      "grad_norm": 0.43405282497406006,
      "learning_rate": 2.9946666666666666e-05,
      "loss": 0.0012,
      "step": 60160
    },
    {
      "epoch": 3.2090666666666667,
      "grad_norm": 0.6738793253898621,
      "learning_rate": 2.9943333333333335e-05,
      "loss": 0.0016,
      "step": 60170
    },
    {
      "epoch": 3.2096,
      "grad_norm": 0.3617696166038513,
      "learning_rate": 2.994e-05,
      "loss": 0.0023,
      "step": 60180
    },
    {
      "epoch": 3.2101333333333333,
      "grad_norm": 0.14061971008777618,
      "learning_rate": 2.9936666666666667e-05,
      "loss": 0.0022,
      "step": 60190
    },
    {
      "epoch": 3.2106666666666666,
      "grad_norm": 0.2584402859210968,
      "learning_rate": 2.9933333333333337e-05,
      "loss": 0.0026,
      "step": 60200
    },
    {
      "epoch": 3.2112,
      "grad_norm": 0.37018120288848877,
      "learning_rate": 2.9930000000000003e-05,
      "loss": 0.0018,
      "step": 60210
    },
    {
      "epoch": 3.211733333333333,
      "grad_norm": 0.12586082518100739,
      "learning_rate": 2.992666666666667e-05,
      "loss": 0.0022,
      "step": 60220
    },
    {
      "epoch": 3.212266666666667,
      "grad_norm": 0.19636934995651245,
      "learning_rate": 2.9923333333333335e-05,
      "loss": 0.0024,
      "step": 60230
    },
    {
      "epoch": 3.2128,
      "grad_norm": 0.2509686350822449,
      "learning_rate": 2.9920000000000005e-05,
      "loss": 0.0017,
      "step": 60240
    },
    {
      "epoch": 3.2133333333333334,
      "grad_norm": 0.3999110758304596,
      "learning_rate": 2.991666666666667e-05,
      "loss": 0.0022,
      "step": 60250
    },
    {
      "epoch": 3.2138666666666666,
      "grad_norm": 0.12693066895008087,
      "learning_rate": 2.991333333333333e-05,
      "loss": 0.003,
      "step": 60260
    },
    {
      "epoch": 3.2144,
      "grad_norm": 0.051316846162080765,
      "learning_rate": 2.991e-05,
      "loss": 0.0015,
      "step": 60270
    },
    {
      "epoch": 3.214933333333333,
      "grad_norm": 0.18725018203258514,
      "learning_rate": 2.9906666666666666e-05,
      "loss": 0.0014,
      "step": 60280
    },
    {
      "epoch": 3.2154666666666665,
      "grad_norm": 0.09808685630559921,
      "learning_rate": 2.9903333333333332e-05,
      "loss": 0.0017,
      "step": 60290
    },
    {
      "epoch": 3.216,
      "grad_norm": 0.04859888181090355,
      "learning_rate": 2.9900000000000002e-05,
      "loss": 0.0019,
      "step": 60300
    },
    {
      "epoch": 3.2165333333333335,
      "grad_norm": 0.2354915291070938,
      "learning_rate": 2.9896666666666668e-05,
      "loss": 0.0012,
      "step": 60310
    },
    {
      "epoch": 3.2170666666666667,
      "grad_norm": 0.21441806852817535,
      "learning_rate": 2.9893333333333334e-05,
      "loss": 0.002,
      "step": 60320
    },
    {
      "epoch": 3.2176,
      "grad_norm": 0.03294743224978447,
      "learning_rate": 2.989e-05,
      "loss": 0.0014,
      "step": 60330
    },
    {
      "epoch": 3.2181333333333333,
      "grad_norm": 0.17922614514827728,
      "learning_rate": 2.988666666666667e-05,
      "loss": 0.002,
      "step": 60340
    },
    {
      "epoch": 3.2186666666666666,
      "grad_norm": 0.39404237270355225,
      "learning_rate": 2.9883333333333336e-05,
      "loss": 0.0029,
      "step": 60350
    },
    {
      "epoch": 3.2192,
      "grad_norm": 0.4306645393371582,
      "learning_rate": 2.9880000000000002e-05,
      "loss": 0.0019,
      "step": 60360
    },
    {
      "epoch": 3.219733333333333,
      "grad_norm": 0.2156112790107727,
      "learning_rate": 2.9876666666666668e-05,
      "loss": 0.002,
      "step": 60370
    },
    {
      "epoch": 3.220266666666667,
      "grad_norm": 0.04434928670525551,
      "learning_rate": 2.9873333333333338e-05,
      "loss": 0.0019,
      "step": 60380
    },
    {
      "epoch": 3.2208,
      "grad_norm": 0.34315159916877747,
      "learning_rate": 2.9870000000000004e-05,
      "loss": 0.0026,
      "step": 60390
    },
    {
      "epoch": 3.2213333333333334,
      "grad_norm": 0.4996277987957001,
      "learning_rate": 2.986666666666667e-05,
      "loss": 0.0018,
      "step": 60400
    },
    {
      "epoch": 3.2218666666666667,
      "grad_norm": 0.07150305807590485,
      "learning_rate": 2.9863333333333333e-05,
      "loss": 0.0023,
      "step": 60410
    },
    {
      "epoch": 3.2224,
      "grad_norm": 0.2986183166503906,
      "learning_rate": 2.986e-05,
      "loss": 0.0014,
      "step": 60420
    },
    {
      "epoch": 3.222933333333333,
      "grad_norm": 0.34822264313697815,
      "learning_rate": 2.9856666666666665e-05,
      "loss": 0.0024,
      "step": 60430
    },
    {
      "epoch": 3.2234666666666665,
      "grad_norm": 0.19616489112377167,
      "learning_rate": 2.9853333333333334e-05,
      "loss": 0.0014,
      "step": 60440
    },
    {
      "epoch": 3.224,
      "grad_norm": 0.6481936573982239,
      "learning_rate": 2.985e-05,
      "loss": 0.0019,
      "step": 60450
    },
    {
      "epoch": 3.2245333333333335,
      "grad_norm": 0.4033343195915222,
      "learning_rate": 2.9846666666666667e-05,
      "loss": 0.0023,
      "step": 60460
    },
    {
      "epoch": 3.2250666666666667,
      "grad_norm": 0.19253821671009064,
      "learning_rate": 2.9843333333333333e-05,
      "loss": 0.0019,
      "step": 60470
    },
    {
      "epoch": 3.2256,
      "grad_norm": 0.21557208895683289,
      "learning_rate": 2.9840000000000002e-05,
      "loss": 0.0015,
      "step": 60480
    },
    {
      "epoch": 3.2261333333333333,
      "grad_norm": 0.21206650137901306,
      "learning_rate": 2.983666666666667e-05,
      "loss": 0.0028,
      "step": 60490
    },
    {
      "epoch": 3.2266666666666666,
      "grad_norm": 0.446878582239151,
      "learning_rate": 2.9833333333333335e-05,
      "loss": 0.0026,
      "step": 60500
    },
    {
      "epoch": 3.2272,
      "grad_norm": 0.3095082640647888,
      "learning_rate": 2.9830000000000004e-05,
      "loss": 0.0037,
      "step": 60510
    },
    {
      "epoch": 3.227733333333333,
      "grad_norm": 0.17700785398483276,
      "learning_rate": 2.982666666666667e-05,
      "loss": 0.0022,
      "step": 60520
    },
    {
      "epoch": 3.228266666666667,
      "grad_norm": 0.31278496980667114,
      "learning_rate": 2.9823333333333336e-05,
      "loss": 0.0014,
      "step": 60530
    },
    {
      "epoch": 3.2288,
      "grad_norm": 0.31193986535072327,
      "learning_rate": 2.9820000000000002e-05,
      "loss": 0.002,
      "step": 60540
    },
    {
      "epoch": 3.2293333333333334,
      "grad_norm": 0.5452530980110168,
      "learning_rate": 2.9816666666666672e-05,
      "loss": 0.0026,
      "step": 60550
    },
    {
      "epoch": 3.2298666666666667,
      "grad_norm": 0.40399619936943054,
      "learning_rate": 2.981333333333333e-05,
      "loss": 0.0013,
      "step": 60560
    },
    {
      "epoch": 3.2304,
      "grad_norm": 0.19251154363155365,
      "learning_rate": 2.9809999999999997e-05,
      "loss": 0.0025,
      "step": 60570
    },
    {
      "epoch": 3.230933333333333,
      "grad_norm": 0.44702237844467163,
      "learning_rate": 2.9806666666666667e-05,
      "loss": 0.0015,
      "step": 60580
    },
    {
      "epoch": 3.2314666666666665,
      "grad_norm": 0.39826324582099915,
      "learning_rate": 2.9803333333333333e-05,
      "loss": 0.0017,
      "step": 60590
    },
    {
      "epoch": 3.232,
      "grad_norm": 0.40054038166999817,
      "learning_rate": 2.98e-05,
      "loss": 0.0019,
      "step": 60600
    },
    {
      "epoch": 3.2325333333333335,
      "grad_norm": 0.09492765367031097,
      "learning_rate": 2.979666666666667e-05,
      "loss": 0.0023,
      "step": 60610
    },
    {
      "epoch": 3.2330666666666668,
      "grad_norm": 0.45578649640083313,
      "learning_rate": 2.9793333333333335e-05,
      "loss": 0.0019,
      "step": 60620
    },
    {
      "epoch": 3.2336,
      "grad_norm": 0.5325664281845093,
      "learning_rate": 2.979e-05,
      "loss": 0.0019,
      "step": 60630
    },
    {
      "epoch": 3.2341333333333333,
      "grad_norm": 0.025941284373402596,
      "learning_rate": 2.9786666666666667e-05,
      "loss": 0.002,
      "step": 60640
    },
    {
      "epoch": 3.2346666666666666,
      "grad_norm": 0.0672263652086258,
      "learning_rate": 2.9783333333333337e-05,
      "loss": 0.0023,
      "step": 60650
    },
    {
      "epoch": 3.2352,
      "grad_norm": 0.03825872018933296,
      "learning_rate": 2.9780000000000003e-05,
      "loss": 0.0018,
      "step": 60660
    },
    {
      "epoch": 3.235733333333333,
      "grad_norm": 0.4481694996356964,
      "learning_rate": 2.977666666666667e-05,
      "loss": 0.0019,
      "step": 60670
    },
    {
      "epoch": 3.236266666666667,
      "grad_norm": 0.39023348689079285,
      "learning_rate": 2.9773333333333335e-05,
      "loss": 0.0022,
      "step": 60680
    },
    {
      "epoch": 3.2368,
      "grad_norm": 0.3450343906879425,
      "learning_rate": 2.9770000000000005e-05,
      "loss": 0.002,
      "step": 60690
    },
    {
      "epoch": 3.2373333333333334,
      "grad_norm": 0.04645228385925293,
      "learning_rate": 2.976666666666667e-05,
      "loss": 0.0025,
      "step": 60700
    },
    {
      "epoch": 3.2378666666666667,
      "grad_norm": 0.049016982316970825,
      "learning_rate": 2.9763333333333337e-05,
      "loss": 0.003,
      "step": 60710
    },
    {
      "epoch": 3.2384,
      "grad_norm": 0.07539299875497818,
      "learning_rate": 2.976e-05,
      "loss": 0.0015,
      "step": 60720
    },
    {
      "epoch": 3.238933333333333,
      "grad_norm": 0.2920253574848175,
      "learning_rate": 2.9756666666666666e-05,
      "loss": 0.0032,
      "step": 60730
    },
    {
      "epoch": 3.2394666666666665,
      "grad_norm": 0.35505664348602295,
      "learning_rate": 2.9753333333333332e-05,
      "loss": 0.0018,
      "step": 60740
    },
    {
      "epoch": 3.24,
      "grad_norm": 0.12538376450538635,
      "learning_rate": 2.975e-05,
      "loss": 0.0028,
      "step": 60750
    },
    {
      "epoch": 3.2405333333333335,
      "grad_norm": 0.07269102334976196,
      "learning_rate": 2.9746666666666668e-05,
      "loss": 0.002,
      "step": 60760
    },
    {
      "epoch": 3.2410666666666668,
      "grad_norm": 0.17742682993412018,
      "learning_rate": 2.9743333333333334e-05,
      "loss": 0.0014,
      "step": 60770
    },
    {
      "epoch": 3.2416,
      "grad_norm": 0.36088672280311584,
      "learning_rate": 2.974e-05,
      "loss": 0.0027,
      "step": 60780
    },
    {
      "epoch": 3.2421333333333333,
      "grad_norm": 0.9332455396652222,
      "learning_rate": 2.973666666666667e-05,
      "loss": 0.0015,
      "step": 60790
    },
    {
      "epoch": 3.2426666666666666,
      "grad_norm": 0.28967177867889404,
      "learning_rate": 2.9733333333333336e-05,
      "loss": 0.0023,
      "step": 60800
    },
    {
      "epoch": 3.2432,
      "grad_norm": 0.397052139043808,
      "learning_rate": 2.973e-05,
      "loss": 0.0021,
      "step": 60810
    },
    {
      "epoch": 3.243733333333333,
      "grad_norm": 0.052220385521650314,
      "learning_rate": 2.9726666666666668e-05,
      "loss": 0.0027,
      "step": 60820
    },
    {
      "epoch": 3.244266666666667,
      "grad_norm": 0.21510529518127441,
      "learning_rate": 2.9723333333333337e-05,
      "loss": 0.0025,
      "step": 60830
    },
    {
      "epoch": 3.2448,
      "grad_norm": 0.050104107707738876,
      "learning_rate": 2.9720000000000003e-05,
      "loss": 0.002,
      "step": 60840
    },
    {
      "epoch": 3.2453333333333334,
      "grad_norm": 0.1468910276889801,
      "learning_rate": 2.971666666666667e-05,
      "loss": 0.0017,
      "step": 60850
    },
    {
      "epoch": 3.2458666666666667,
      "grad_norm": 0.04229413717985153,
      "learning_rate": 2.971333333333334e-05,
      "loss": 0.0021,
      "step": 60860
    },
    {
      "epoch": 3.2464,
      "grad_norm": 0.442324697971344,
      "learning_rate": 2.971e-05,
      "loss": 0.0021,
      "step": 60870
    },
    {
      "epoch": 3.2469333333333332,
      "grad_norm": 0.35903725028038025,
      "learning_rate": 2.9706666666666665e-05,
      "loss": 0.0016,
      "step": 60880
    },
    {
      "epoch": 3.2474666666666665,
      "grad_norm": 0.08416803181171417,
      "learning_rate": 2.9703333333333334e-05,
      "loss": 0.0024,
      "step": 60890
    },
    {
      "epoch": 3.248,
      "grad_norm": 0.35281187295913696,
      "learning_rate": 2.97e-05,
      "loss": 0.0021,
      "step": 60900
    },
    {
      "epoch": 3.2485333333333335,
      "grad_norm": 0.35739538073539734,
      "learning_rate": 2.9696666666666666e-05,
      "loss": 0.0023,
      "step": 60910
    },
    {
      "epoch": 3.2490666666666668,
      "grad_norm": 0.12627480924129486,
      "learning_rate": 2.9693333333333333e-05,
      "loss": 0.0019,
      "step": 60920
    },
    {
      "epoch": 3.2496,
      "grad_norm": 0.3618834614753723,
      "learning_rate": 2.9690000000000002e-05,
      "loss": 0.0015,
      "step": 60930
    },
    {
      "epoch": 3.2501333333333333,
      "grad_norm": 0.17927445471286774,
      "learning_rate": 2.9686666666666668e-05,
      "loss": 0.0021,
      "step": 60940
    },
    {
      "epoch": 3.2506666666666666,
      "grad_norm": 0.3510674834251404,
      "learning_rate": 2.9683333333333334e-05,
      "loss": 0.0024,
      "step": 60950
    },
    {
      "epoch": 3.2512,
      "grad_norm": 0.1320808380842209,
      "learning_rate": 2.9680000000000004e-05,
      "loss": 0.0022,
      "step": 60960
    },
    {
      "epoch": 3.251733333333333,
      "grad_norm": 0.08719927817583084,
      "learning_rate": 2.967666666666667e-05,
      "loss": 0.0018,
      "step": 60970
    },
    {
      "epoch": 3.2522666666666664,
      "grad_norm": 0.405150830745697,
      "learning_rate": 2.9673333333333336e-05,
      "loss": 0.0022,
      "step": 60980
    },
    {
      "epoch": 3.2528,
      "grad_norm": 0.19844776391983032,
      "learning_rate": 2.9670000000000002e-05,
      "loss": 0.0023,
      "step": 60990
    },
    {
      "epoch": 3.2533333333333334,
      "grad_norm": 0.544084370136261,
      "learning_rate": 2.9666666666666672e-05,
      "loss": 0.0029,
      "step": 61000
    },
    {
      "epoch": 3.2538666666666667,
      "grad_norm": 0.38105300068855286,
      "learning_rate": 2.9663333333333338e-05,
      "loss": 0.0029,
      "step": 61010
    },
    {
      "epoch": 3.2544,
      "grad_norm": 0.23215211927890778,
      "learning_rate": 2.9659999999999997e-05,
      "loss": 0.0025,
      "step": 61020
    },
    {
      "epoch": 3.2549333333333332,
      "grad_norm": 0.12199083715677261,
      "learning_rate": 2.9656666666666667e-05,
      "loss": 0.0026,
      "step": 61030
    },
    {
      "epoch": 3.2554666666666665,
      "grad_norm": 0.22325627505779266,
      "learning_rate": 2.9653333333333333e-05,
      "loss": 0.0017,
      "step": 61040
    },
    {
      "epoch": 3.2560000000000002,
      "grad_norm": 0.12685878574848175,
      "learning_rate": 2.965e-05,
      "loss": 0.0035,
      "step": 61050
    },
    {
      "epoch": 3.2565333333333335,
      "grad_norm": 0.0715591087937355,
      "learning_rate": 2.964666666666667e-05,
      "loss": 0.0014,
      "step": 61060
    },
    {
      "epoch": 3.2570666666666668,
      "grad_norm": 0.23434367775917053,
      "learning_rate": 2.9643333333333335e-05,
      "loss": 0.0025,
      "step": 61070
    },
    {
      "epoch": 3.2576,
      "grad_norm": 0.18500801920890808,
      "learning_rate": 2.964e-05,
      "loss": 0.0022,
      "step": 61080
    },
    {
      "epoch": 3.2581333333333333,
      "grad_norm": 0.04635055735707283,
      "learning_rate": 2.9636666666666667e-05,
      "loss": 0.003,
      "step": 61090
    },
    {
      "epoch": 3.2586666666666666,
      "grad_norm": 0.21733352541923523,
      "learning_rate": 2.9633333333333336e-05,
      "loss": 0.0016,
      "step": 61100
    },
    {
      "epoch": 3.2592,
      "grad_norm": 0.04539978504180908,
      "learning_rate": 2.9630000000000003e-05,
      "loss": 0.0015,
      "step": 61110
    },
    {
      "epoch": 3.259733333333333,
      "grad_norm": 0.19714780151844025,
      "learning_rate": 2.962666666666667e-05,
      "loss": 0.0022,
      "step": 61120
    },
    {
      "epoch": 3.2602666666666664,
      "grad_norm": 0.2862502932548523,
      "learning_rate": 2.9623333333333335e-05,
      "loss": 0.0012,
      "step": 61130
    },
    {
      "epoch": 3.2608,
      "grad_norm": 0.4093928039073944,
      "learning_rate": 2.9620000000000004e-05,
      "loss": 0.0016,
      "step": 61140
    },
    {
      "epoch": 3.2613333333333334,
      "grad_norm": 0.070729561150074,
      "learning_rate": 2.961666666666667e-05,
      "loss": 0.0018,
      "step": 61150
    },
    {
      "epoch": 3.2618666666666667,
      "grad_norm": 0.1764833778142929,
      "learning_rate": 2.9613333333333337e-05,
      "loss": 0.0033,
      "step": 61160
    },
    {
      "epoch": 3.2624,
      "grad_norm": 0.15336260199546814,
      "learning_rate": 2.961e-05,
      "loss": 0.0029,
      "step": 61170
    },
    {
      "epoch": 3.2629333333333332,
      "grad_norm": 0.057888325303792953,
      "learning_rate": 2.9606666666666666e-05,
      "loss": 0.0023,
      "step": 61180
    },
    {
      "epoch": 3.2634666666666665,
      "grad_norm": 0.4438176453113556,
      "learning_rate": 2.960333333333333e-05,
      "loss": 0.0017,
      "step": 61190
    },
    {
      "epoch": 3.2640000000000002,
      "grad_norm": 0.43524572253227234,
      "learning_rate": 2.96e-05,
      "loss": 0.0017,
      "step": 61200
    },
    {
      "epoch": 3.2645333333333335,
      "grad_norm": 0.3747427761554718,
      "learning_rate": 2.9596666666666667e-05,
      "loss": 0.0028,
      "step": 61210
    },
    {
      "epoch": 3.265066666666667,
      "grad_norm": 0.24124649167060852,
      "learning_rate": 2.9593333333333333e-05,
      "loss": 0.0032,
      "step": 61220
    },
    {
      "epoch": 3.2656,
      "grad_norm": 0.15379267930984497,
      "learning_rate": 2.959e-05,
      "loss": 0.0021,
      "step": 61230
    },
    {
      "epoch": 3.2661333333333333,
      "grad_norm": 0.8172085285186768,
      "learning_rate": 2.958666666666667e-05,
      "loss": 0.0022,
      "step": 61240
    },
    {
      "epoch": 3.2666666666666666,
      "grad_norm": 0.659682035446167,
      "learning_rate": 2.9583333333333335e-05,
      "loss": 0.0022,
      "step": 61250
    },
    {
      "epoch": 3.2672,
      "grad_norm": 0.13555285334587097,
      "learning_rate": 2.958e-05,
      "loss": 0.0021,
      "step": 61260
    },
    {
      "epoch": 3.267733333333333,
      "grad_norm": 0.3635529577732086,
      "learning_rate": 2.9576666666666668e-05,
      "loss": 0.0024,
      "step": 61270
    },
    {
      "epoch": 3.2682666666666664,
      "grad_norm": 0.3991956114768982,
      "learning_rate": 2.9573333333333337e-05,
      "loss": 0.0015,
      "step": 61280
    },
    {
      "epoch": 3.2688,
      "grad_norm": 0.07304845750331879,
      "learning_rate": 2.9570000000000003e-05,
      "loss": 0.0025,
      "step": 61290
    },
    {
      "epoch": 3.2693333333333334,
      "grad_norm": 0.46207138895988464,
      "learning_rate": 2.956666666666667e-05,
      "loss": 0.0015,
      "step": 61300
    },
    {
      "epoch": 3.2698666666666667,
      "grad_norm": 0.40245187282562256,
      "learning_rate": 2.956333333333334e-05,
      "loss": 0.0018,
      "step": 61310
    },
    {
      "epoch": 3.2704,
      "grad_norm": 0.5146278142929077,
      "learning_rate": 2.9559999999999998e-05,
      "loss": 0.0017,
      "step": 61320
    },
    {
      "epoch": 3.2709333333333332,
      "grad_norm": 0.050254303961992264,
      "learning_rate": 2.9556666666666664e-05,
      "loss": 0.002,
      "step": 61330
    },
    {
      "epoch": 3.2714666666666665,
      "grad_norm": 0.07252536714076996,
      "learning_rate": 2.9553333333333334e-05,
      "loss": 0.0016,
      "step": 61340
    },
    {
      "epoch": 3.2720000000000002,
      "grad_norm": 0.542853593826294,
      "learning_rate": 2.955e-05,
      "loss": 0.0023,
      "step": 61350
    },
    {
      "epoch": 3.2725333333333335,
      "grad_norm": 0.7219184637069702,
      "learning_rate": 2.9546666666666666e-05,
      "loss": 0.0025,
      "step": 61360
    },
    {
      "epoch": 3.273066666666667,
      "grad_norm": 0.39897844195365906,
      "learning_rate": 2.9543333333333336e-05,
      "loss": 0.0017,
      "step": 61370
    },
    {
      "epoch": 3.2736,
      "grad_norm": 0.24298937618732452,
      "learning_rate": 2.9540000000000002e-05,
      "loss": 0.002,
      "step": 61380
    },
    {
      "epoch": 3.2741333333333333,
      "grad_norm": 0.2753438353538513,
      "learning_rate": 2.9536666666666668e-05,
      "loss": 0.0027,
      "step": 61390
    },
    {
      "epoch": 3.2746666666666666,
      "grad_norm": 0.10550136119127274,
      "learning_rate": 2.9533333333333334e-05,
      "loss": 0.0019,
      "step": 61400
    },
    {
      "epoch": 3.2752,
      "grad_norm": 0.29027020931243896,
      "learning_rate": 2.9530000000000004e-05,
      "loss": 0.0018,
      "step": 61410
    },
    {
      "epoch": 3.275733333333333,
      "grad_norm": 0.12865190207958221,
      "learning_rate": 2.952666666666667e-05,
      "loss": 0.002,
      "step": 61420
    },
    {
      "epoch": 3.2762666666666664,
      "grad_norm": 0.161917045712471,
      "learning_rate": 2.9523333333333336e-05,
      "loss": 0.0014,
      "step": 61430
    },
    {
      "epoch": 3.2768,
      "grad_norm": 0.31692007184028625,
      "learning_rate": 2.9520000000000002e-05,
      "loss": 0.0042,
      "step": 61440
    },
    {
      "epoch": 3.2773333333333334,
      "grad_norm": 0.24503812193870544,
      "learning_rate": 2.951666666666667e-05,
      "loss": 0.0024,
      "step": 61450
    },
    {
      "epoch": 3.2778666666666667,
      "grad_norm": 0.6203356981277466,
      "learning_rate": 2.9513333333333338e-05,
      "loss": 0.0021,
      "step": 61460
    },
    {
      "epoch": 3.2784,
      "grad_norm": 0.2846808433532715,
      "learning_rate": 2.951e-05,
      "loss": 0.0024,
      "step": 61470
    },
    {
      "epoch": 3.2789333333333333,
      "grad_norm": 0.05687451735138893,
      "learning_rate": 2.9506666666666667e-05,
      "loss": 0.0015,
      "step": 61480
    },
    {
      "epoch": 3.2794666666666665,
      "grad_norm": 0.46519264578819275,
      "learning_rate": 2.9503333333333333e-05,
      "loss": 0.0017,
      "step": 61490
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 0.2809121310710907,
      "learning_rate": 2.95e-05,
      "loss": 0.0038,
      "step": 61500
    },
    {
      "epoch": 3.2805333333333335,
      "grad_norm": 0.2532518804073334,
      "learning_rate": 2.9496666666666668e-05,
      "loss": 0.0016,
      "step": 61510
    },
    {
      "epoch": 3.281066666666667,
      "grad_norm": 0.24609997868537903,
      "learning_rate": 2.9493333333333334e-05,
      "loss": 0.0023,
      "step": 61520
    },
    {
      "epoch": 3.2816,
      "grad_norm": 0.39858976006507874,
      "learning_rate": 2.949e-05,
      "loss": 0.0028,
      "step": 61530
    },
    {
      "epoch": 3.2821333333333333,
      "grad_norm": 0.3522097170352936,
      "learning_rate": 2.9486666666666667e-05,
      "loss": 0.0032,
      "step": 61540
    },
    {
      "epoch": 3.2826666666666666,
      "grad_norm": 0.07245206832885742,
      "learning_rate": 2.9483333333333336e-05,
      "loss": 0.0021,
      "step": 61550
    },
    {
      "epoch": 3.2832,
      "grad_norm": 0.43284597992897034,
      "learning_rate": 2.9480000000000002e-05,
      "loss": 0.0019,
      "step": 61560
    },
    {
      "epoch": 3.283733333333333,
      "grad_norm": 0.4918428361415863,
      "learning_rate": 2.947666666666667e-05,
      "loss": 0.0018,
      "step": 61570
    },
    {
      "epoch": 3.2842666666666664,
      "grad_norm": 0.09633432328701019,
      "learning_rate": 2.9473333333333335e-05,
      "loss": 0.002,
      "step": 61580
    },
    {
      "epoch": 3.2848,
      "grad_norm": 0.5322898030281067,
      "learning_rate": 2.9470000000000004e-05,
      "loss": 0.0018,
      "step": 61590
    },
    {
      "epoch": 3.2853333333333334,
      "grad_norm": 0.5133650898933411,
      "learning_rate": 2.946666666666667e-05,
      "loss": 0.0027,
      "step": 61600
    },
    {
      "epoch": 3.2858666666666667,
      "grad_norm": 0.10370009392499924,
      "learning_rate": 2.9463333333333336e-05,
      "loss": 0.002,
      "step": 61610
    },
    {
      "epoch": 3.2864,
      "grad_norm": 0.11331979930400848,
      "learning_rate": 2.946e-05,
      "loss": 0.0019,
      "step": 61620
    },
    {
      "epoch": 3.2869333333333333,
      "grad_norm": 0.5464796423912048,
      "learning_rate": 2.9456666666666665e-05,
      "loss": 0.0018,
      "step": 61630
    },
    {
      "epoch": 3.2874666666666665,
      "grad_norm": 0.423650860786438,
      "learning_rate": 2.945333333333333e-05,
      "loss": 0.0022,
      "step": 61640
    },
    {
      "epoch": 3.288,
      "grad_norm": 0.30432409048080444,
      "learning_rate": 2.945e-05,
      "loss": 0.0026,
      "step": 61650
    },
    {
      "epoch": 3.2885333333333335,
      "grad_norm": 0.5712531805038452,
      "learning_rate": 2.9446666666666667e-05,
      "loss": 0.0021,
      "step": 61660
    },
    {
      "epoch": 3.289066666666667,
      "grad_norm": 0.2090492844581604,
      "learning_rate": 2.9443333333333333e-05,
      "loss": 0.0027,
      "step": 61670
    },
    {
      "epoch": 3.2896,
      "grad_norm": 0.46028342843055725,
      "learning_rate": 2.944e-05,
      "loss": 0.0019,
      "step": 61680
    },
    {
      "epoch": 3.2901333333333334,
      "grad_norm": 0.07531300187110901,
      "learning_rate": 2.943666666666667e-05,
      "loss": 0.0023,
      "step": 61690
    },
    {
      "epoch": 3.2906666666666666,
      "grad_norm": 0.13549144566059113,
      "learning_rate": 2.9433333333333335e-05,
      "loss": 0.0024,
      "step": 61700
    },
    {
      "epoch": 3.2912,
      "grad_norm": 0.16977828741073608,
      "learning_rate": 2.943e-05,
      "loss": 0.0018,
      "step": 61710
    },
    {
      "epoch": 3.291733333333333,
      "grad_norm": 0.4134095013141632,
      "learning_rate": 2.942666666666667e-05,
      "loss": 0.0021,
      "step": 61720
    },
    {
      "epoch": 3.2922666666666665,
      "grad_norm": 0.3727358281612396,
      "learning_rate": 2.9423333333333337e-05,
      "loss": 0.0025,
      "step": 61730
    },
    {
      "epoch": 3.2928,
      "grad_norm": 0.3423190414905548,
      "learning_rate": 2.9420000000000003e-05,
      "loss": 0.0025,
      "step": 61740
    },
    {
      "epoch": 3.2933333333333334,
      "grad_norm": 0.26976048946380615,
      "learning_rate": 2.941666666666667e-05,
      "loss": 0.0024,
      "step": 61750
    },
    {
      "epoch": 3.2938666666666667,
      "grad_norm": 0.13740088045597076,
      "learning_rate": 2.941333333333334e-05,
      "loss": 0.002,
      "step": 61760
    },
    {
      "epoch": 3.2944,
      "grad_norm": 0.8874865770339966,
      "learning_rate": 2.9409999999999998e-05,
      "loss": 0.0016,
      "step": 61770
    },
    {
      "epoch": 3.2949333333333333,
      "grad_norm": 0.40303754806518555,
      "learning_rate": 2.9406666666666664e-05,
      "loss": 0.0016,
      "step": 61780
    },
    {
      "epoch": 3.2954666666666665,
      "grad_norm": 0.15432429313659668,
      "learning_rate": 2.9403333333333334e-05,
      "loss": 0.0033,
      "step": 61790
    },
    {
      "epoch": 3.296,
      "grad_norm": 0.7411589622497559,
      "learning_rate": 2.94e-05,
      "loss": 0.0021,
      "step": 61800
    },
    {
      "epoch": 3.2965333333333335,
      "grad_norm": 0.4594637453556061,
      "learning_rate": 2.9396666666666666e-05,
      "loss": 0.0025,
      "step": 61810
    },
    {
      "epoch": 3.297066666666667,
      "grad_norm": 0.4094909727573395,
      "learning_rate": 2.9393333333333335e-05,
      "loss": 0.0029,
      "step": 61820
    },
    {
      "epoch": 3.2976,
      "grad_norm": 0.3082726299762726,
      "learning_rate": 2.939e-05,
      "loss": 0.0026,
      "step": 61830
    },
    {
      "epoch": 3.2981333333333334,
      "grad_norm": 0.21084551513195038,
      "learning_rate": 2.9386666666666668e-05,
      "loss": 0.0014,
      "step": 61840
    },
    {
      "epoch": 3.2986666666666666,
      "grad_norm": 0.18655537068843842,
      "learning_rate": 2.9383333333333334e-05,
      "loss": 0.0025,
      "step": 61850
    },
    {
      "epoch": 3.2992,
      "grad_norm": 0.13191239535808563,
      "learning_rate": 2.9380000000000003e-05,
      "loss": 0.0026,
      "step": 61860
    },
    {
      "epoch": 3.299733333333333,
      "grad_norm": 0.3020552098751068,
      "learning_rate": 2.937666666666667e-05,
      "loss": 0.0019,
      "step": 61870
    },
    {
      "epoch": 3.3002666666666665,
      "grad_norm": 0.15977314114570618,
      "learning_rate": 2.9373333333333336e-05,
      "loss": 0.0022,
      "step": 61880
    },
    {
      "epoch": 3.3008,
      "grad_norm": 0.19122429192066193,
      "learning_rate": 2.9370000000000002e-05,
      "loss": 0.0026,
      "step": 61890
    },
    {
      "epoch": 3.3013333333333335,
      "grad_norm": 0.1240936741232872,
      "learning_rate": 2.936666666666667e-05,
      "loss": 0.002,
      "step": 61900
    },
    {
      "epoch": 3.3018666666666667,
      "grad_norm": 0.18502917885780334,
      "learning_rate": 2.9363333333333337e-05,
      "loss": 0.003,
      "step": 61910
    },
    {
      "epoch": 3.3024,
      "grad_norm": 0.03093370608985424,
      "learning_rate": 2.9360000000000003e-05,
      "loss": 0.0029,
      "step": 61920
    },
    {
      "epoch": 3.3029333333333333,
      "grad_norm": 0.13024230301380157,
      "learning_rate": 2.9356666666666666e-05,
      "loss": 0.0031,
      "step": 61930
    },
    {
      "epoch": 3.3034666666666666,
      "grad_norm": 0.27839818596839905,
      "learning_rate": 2.9353333333333332e-05,
      "loss": 0.0026,
      "step": 61940
    },
    {
      "epoch": 3.304,
      "grad_norm": 0.5179795026779175,
      "learning_rate": 2.935e-05,
      "loss": 0.0021,
      "step": 61950
    },
    {
      "epoch": 3.3045333333333335,
      "grad_norm": 0.34216728806495667,
      "learning_rate": 2.9346666666666668e-05,
      "loss": 0.0027,
      "step": 61960
    },
    {
      "epoch": 3.305066666666667,
      "grad_norm": 0.13630658388137817,
      "learning_rate": 2.9343333333333334e-05,
      "loss": 0.0023,
      "step": 61970
    },
    {
      "epoch": 3.3056,
      "grad_norm": 0.1717476099729538,
      "learning_rate": 2.934e-05,
      "loss": 0.0021,
      "step": 61980
    },
    {
      "epoch": 3.3061333333333334,
      "grad_norm": 0.3712681233882904,
      "learning_rate": 2.9336666666666666e-05,
      "loss": 0.002,
      "step": 61990
    },
    {
      "epoch": 3.3066666666666666,
      "grad_norm": 0.33214566111564636,
      "learning_rate": 2.9333333333333336e-05,
      "loss": 0.002,
      "step": 62000
    },
    {
      "epoch": 3.3072,
      "grad_norm": 0.3363133370876312,
      "learning_rate": 2.9330000000000002e-05,
      "loss": 0.0033,
      "step": 62010
    },
    {
      "epoch": 3.307733333333333,
      "grad_norm": 0.5921139717102051,
      "learning_rate": 2.9326666666666668e-05,
      "loss": 0.0026,
      "step": 62020
    },
    {
      "epoch": 3.3082666666666665,
      "grad_norm": 0.5628932118415833,
      "learning_rate": 2.9323333333333334e-05,
      "loss": 0.0015,
      "step": 62030
    },
    {
      "epoch": 3.3088,
      "grad_norm": 0.3799501955509186,
      "learning_rate": 2.9320000000000004e-05,
      "loss": 0.0027,
      "step": 62040
    },
    {
      "epoch": 3.3093333333333335,
      "grad_norm": 0.5051315426826477,
      "learning_rate": 2.931666666666667e-05,
      "loss": 0.0019,
      "step": 62050
    },
    {
      "epoch": 3.3098666666666667,
      "grad_norm": 0.4073130786418915,
      "learning_rate": 2.9313333333333336e-05,
      "loss": 0.0025,
      "step": 62060
    },
    {
      "epoch": 3.3104,
      "grad_norm": 0.1000007763504982,
      "learning_rate": 2.9310000000000006e-05,
      "loss": 0.0021,
      "step": 62070
    },
    {
      "epoch": 3.3109333333333333,
      "grad_norm": 0.28727835416793823,
      "learning_rate": 2.9306666666666665e-05,
      "loss": 0.0023,
      "step": 62080
    },
    {
      "epoch": 3.3114666666666666,
      "grad_norm": 0.3404744565486908,
      "learning_rate": 2.930333333333333e-05,
      "loss": 0.0019,
      "step": 62090
    },
    {
      "epoch": 3.312,
      "grad_norm": 0.17507772147655487,
      "learning_rate": 2.93e-05,
      "loss": 0.0029,
      "step": 62100
    },
    {
      "epoch": 3.3125333333333336,
      "grad_norm": 0.2722662091255188,
      "learning_rate": 2.9296666666666667e-05,
      "loss": 0.0019,
      "step": 62110
    },
    {
      "epoch": 3.313066666666667,
      "grad_norm": 0.09104496240615845,
      "learning_rate": 2.9293333333333333e-05,
      "loss": 0.0016,
      "step": 62120
    },
    {
      "epoch": 3.3136,
      "grad_norm": 0.11066165566444397,
      "learning_rate": 2.929e-05,
      "loss": 0.0017,
      "step": 62130
    },
    {
      "epoch": 3.3141333333333334,
      "grad_norm": 0.467850923538208,
      "learning_rate": 2.928666666666667e-05,
      "loss": 0.0028,
      "step": 62140
    },
    {
      "epoch": 3.3146666666666667,
      "grad_norm": 0.380597859621048,
      "learning_rate": 2.9283333333333335e-05,
      "loss": 0.0026,
      "step": 62150
    },
    {
      "epoch": 3.3152,
      "grad_norm": 0.39361825585365295,
      "learning_rate": 2.928e-05,
      "loss": 0.0019,
      "step": 62160
    },
    {
      "epoch": 3.315733333333333,
      "grad_norm": 0.5737805962562561,
      "learning_rate": 2.927666666666667e-05,
      "loss": 0.0022,
      "step": 62170
    },
    {
      "epoch": 3.3162666666666665,
      "grad_norm": 0.2768211364746094,
      "learning_rate": 2.9273333333333337e-05,
      "loss": 0.0016,
      "step": 62180
    },
    {
      "epoch": 3.3168,
      "grad_norm": 0.30641618371009827,
      "learning_rate": 2.9270000000000003e-05,
      "loss": 0.0026,
      "step": 62190
    },
    {
      "epoch": 3.3173333333333335,
      "grad_norm": 0.15770691633224487,
      "learning_rate": 2.926666666666667e-05,
      "loss": 0.002,
      "step": 62200
    },
    {
      "epoch": 3.3178666666666667,
      "grad_norm": 0.49749988317489624,
      "learning_rate": 2.926333333333334e-05,
      "loss": 0.0034,
      "step": 62210
    },
    {
      "epoch": 3.3184,
      "grad_norm": 0.050750527530908585,
      "learning_rate": 2.9260000000000004e-05,
      "loss": 0.0019,
      "step": 62220
    },
    {
      "epoch": 3.3189333333333333,
      "grad_norm": 0.4077412784099579,
      "learning_rate": 2.9256666666666667e-05,
      "loss": 0.0026,
      "step": 62230
    },
    {
      "epoch": 3.3194666666666666,
      "grad_norm": 0.09743446111679077,
      "learning_rate": 2.9253333333333333e-05,
      "loss": 0.0017,
      "step": 62240
    },
    {
      "epoch": 3.32,
      "grad_norm": 0.21648624539375305,
      "learning_rate": 2.925e-05,
      "loss": 0.0023,
      "step": 62250
    },
    {
      "epoch": 3.3205333333333336,
      "grad_norm": 0.05515811964869499,
      "learning_rate": 2.9246666666666666e-05,
      "loss": 0.0015,
      "step": 62260
    },
    {
      "epoch": 3.321066666666667,
      "grad_norm": 0.2808219790458679,
      "learning_rate": 2.9243333333333335e-05,
      "loss": 0.0024,
      "step": 62270
    },
    {
      "epoch": 3.3216,
      "grad_norm": 0.588172197341919,
      "learning_rate": 2.924e-05,
      "loss": 0.0025,
      "step": 62280
    },
    {
      "epoch": 3.3221333333333334,
      "grad_norm": 0.05461086332798004,
      "learning_rate": 2.9236666666666667e-05,
      "loss": 0.0017,
      "step": 62290
    },
    {
      "epoch": 3.3226666666666667,
      "grad_norm": 0.40452224016189575,
      "learning_rate": 2.9233333333333334e-05,
      "loss": 0.0025,
      "step": 62300
    },
    {
      "epoch": 3.3232,
      "grad_norm": 0.3615252375602722,
      "learning_rate": 2.9230000000000003e-05,
      "loss": 0.0025,
      "step": 62310
    },
    {
      "epoch": 3.323733333333333,
      "grad_norm": 0.38726112246513367,
      "learning_rate": 2.922666666666667e-05,
      "loss": 0.0023,
      "step": 62320
    },
    {
      "epoch": 3.3242666666666665,
      "grad_norm": 0.6154859662055969,
      "learning_rate": 2.9223333333333335e-05,
      "loss": 0.0026,
      "step": 62330
    },
    {
      "epoch": 3.3247999999999998,
      "grad_norm": 0.2802243232727051,
      "learning_rate": 2.922e-05,
      "loss": 0.0017,
      "step": 62340
    },
    {
      "epoch": 3.3253333333333335,
      "grad_norm": 0.2555798590183258,
      "learning_rate": 2.921666666666667e-05,
      "loss": 0.0027,
      "step": 62350
    },
    {
      "epoch": 3.3258666666666667,
      "grad_norm": 0.35918956995010376,
      "learning_rate": 2.9213333333333337e-05,
      "loss": 0.0021,
      "step": 62360
    },
    {
      "epoch": 3.3264,
      "grad_norm": 0.5595954656600952,
      "learning_rate": 2.9210000000000003e-05,
      "loss": 0.0022,
      "step": 62370
    },
    {
      "epoch": 3.3269333333333333,
      "grad_norm": 0.30844229459762573,
      "learning_rate": 2.9206666666666666e-05,
      "loss": 0.0029,
      "step": 62380
    },
    {
      "epoch": 3.3274666666666666,
      "grad_norm": 0.07831139117479324,
      "learning_rate": 2.9203333333333332e-05,
      "loss": 0.0019,
      "step": 62390
    },
    {
      "epoch": 3.328,
      "grad_norm": 0.5922080874443054,
      "learning_rate": 2.9199999999999998e-05,
      "loss": 0.0027,
      "step": 62400
    },
    {
      "epoch": 3.3285333333333336,
      "grad_norm": 0.06964418292045593,
      "learning_rate": 2.9196666666666668e-05,
      "loss": 0.0022,
      "step": 62410
    },
    {
      "epoch": 3.329066666666667,
      "grad_norm": 0.2952415347099304,
      "learning_rate": 2.9193333333333334e-05,
      "loss": 0.0019,
      "step": 62420
    },
    {
      "epoch": 3.3296,
      "grad_norm": 0.2541029155254364,
      "learning_rate": 2.919e-05,
      "loss": 0.0025,
      "step": 62430
    },
    {
      "epoch": 3.3301333333333334,
      "grad_norm": 0.14041803777217865,
      "learning_rate": 2.9186666666666666e-05,
      "loss": 0.002,
      "step": 62440
    },
    {
      "epoch": 3.3306666666666667,
      "grad_norm": 0.19126610457897186,
      "learning_rate": 2.9183333333333336e-05,
      "loss": 0.0022,
      "step": 62450
    },
    {
      "epoch": 3.3312,
      "grad_norm": 0.316800594329834,
      "learning_rate": 2.9180000000000002e-05,
      "loss": 0.0032,
      "step": 62460
    },
    {
      "epoch": 3.331733333333333,
      "grad_norm": 0.2374878078699112,
      "learning_rate": 2.9176666666666668e-05,
      "loss": 0.0025,
      "step": 62470
    },
    {
      "epoch": 3.3322666666666665,
      "grad_norm": 0.14411212503910065,
      "learning_rate": 2.9173333333333337e-05,
      "loss": 0.0014,
      "step": 62480
    },
    {
      "epoch": 3.3327999999999998,
      "grad_norm": 0.6794043183326721,
      "learning_rate": 2.9170000000000004e-05,
      "loss": 0.0018,
      "step": 62490
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.07272849977016449,
      "learning_rate": 2.916666666666667e-05,
      "loss": 0.0014,
      "step": 62500
    },
    {
      "epoch": 3.3338666666666668,
      "grad_norm": 0.07333358377218246,
      "learning_rate": 2.9163333333333336e-05,
      "loss": 0.0017,
      "step": 62510
    },
    {
      "epoch": 3.3344,
      "grad_norm": 0.04194575548171997,
      "learning_rate": 2.9160000000000005e-05,
      "loss": 0.0019,
      "step": 62520
    },
    {
      "epoch": 3.3349333333333333,
      "grad_norm": 0.02747648023068905,
      "learning_rate": 2.9156666666666665e-05,
      "loss": 0.0024,
      "step": 62530
    },
    {
      "epoch": 3.3354666666666666,
      "grad_norm": 0.12210602313280106,
      "learning_rate": 2.915333333333333e-05,
      "loss": 0.0017,
      "step": 62540
    },
    {
      "epoch": 3.336,
      "grad_norm": 0.1166021004319191,
      "learning_rate": 2.915e-05,
      "loss": 0.002,
      "step": 62550
    },
    {
      "epoch": 3.3365333333333336,
      "grad_norm": 0.43239879608154297,
      "learning_rate": 2.9146666666666667e-05,
      "loss": 0.0018,
      "step": 62560
    },
    {
      "epoch": 3.337066666666667,
      "grad_norm": 0.6688056588172913,
      "learning_rate": 2.9143333333333333e-05,
      "loss": 0.0024,
      "step": 62570
    },
    {
      "epoch": 3.3376,
      "grad_norm": 0.08185464143753052,
      "learning_rate": 2.9140000000000002e-05,
      "loss": 0.0022,
      "step": 62580
    },
    {
      "epoch": 3.3381333333333334,
      "grad_norm": 0.4448419511318207,
      "learning_rate": 2.913666666666667e-05,
      "loss": 0.0022,
      "step": 62590
    },
    {
      "epoch": 3.3386666666666667,
      "grad_norm": 0.498502254486084,
      "learning_rate": 2.9133333333333334e-05,
      "loss": 0.0019,
      "step": 62600
    },
    {
      "epoch": 3.3392,
      "grad_norm": 0.22446739673614502,
      "learning_rate": 2.913e-05,
      "loss": 0.0022,
      "step": 62610
    },
    {
      "epoch": 3.339733333333333,
      "grad_norm": 0.4220462143421173,
      "learning_rate": 2.912666666666667e-05,
      "loss": 0.0019,
      "step": 62620
    },
    {
      "epoch": 3.3402666666666665,
      "grad_norm": 0.8558411002159119,
      "learning_rate": 2.9123333333333336e-05,
      "loss": 0.0018,
      "step": 62630
    },
    {
      "epoch": 3.3407999999999998,
      "grad_norm": 0.34175726771354675,
      "learning_rate": 2.9120000000000002e-05,
      "loss": 0.0034,
      "step": 62640
    },
    {
      "epoch": 3.3413333333333335,
      "grad_norm": 0.14338892698287964,
      "learning_rate": 2.911666666666667e-05,
      "loss": 0.0028,
      "step": 62650
    },
    {
      "epoch": 3.3418666666666668,
      "grad_norm": 0.02964886464178562,
      "learning_rate": 2.9113333333333338e-05,
      "loss": 0.0032,
      "step": 62660
    },
    {
      "epoch": 3.3424,
      "grad_norm": 0.8048822283744812,
      "learning_rate": 2.9110000000000004e-05,
      "loss": 0.0017,
      "step": 62670
    },
    {
      "epoch": 3.3429333333333333,
      "grad_norm": 0.43109238147735596,
      "learning_rate": 2.9106666666666667e-05,
      "loss": 0.0024,
      "step": 62680
    },
    {
      "epoch": 3.3434666666666666,
      "grad_norm": 0.39134252071380615,
      "learning_rate": 2.9103333333333333e-05,
      "loss": 0.0024,
      "step": 62690
    },
    {
      "epoch": 3.344,
      "grad_norm": 0.19183608889579773,
      "learning_rate": 2.91e-05,
      "loss": 0.0031,
      "step": 62700
    },
    {
      "epoch": 3.3445333333333336,
      "grad_norm": 0.2776705324649811,
      "learning_rate": 2.9096666666666665e-05,
      "loss": 0.0028,
      "step": 62710
    },
    {
      "epoch": 3.345066666666667,
      "grad_norm": 0.2826480567455292,
      "learning_rate": 2.9093333333333335e-05,
      "loss": 0.0021,
      "step": 62720
    },
    {
      "epoch": 3.3456,
      "grad_norm": 0.23693647980690002,
      "learning_rate": 2.909e-05,
      "loss": 0.0024,
      "step": 62730
    },
    {
      "epoch": 3.3461333333333334,
      "grad_norm": 0.4500328004360199,
      "learning_rate": 2.9086666666666667e-05,
      "loss": 0.0023,
      "step": 62740
    },
    {
      "epoch": 3.3466666666666667,
      "grad_norm": 0.44725537300109863,
      "learning_rate": 2.9083333333333333e-05,
      "loss": 0.0025,
      "step": 62750
    },
    {
      "epoch": 3.3472,
      "grad_norm": 0.2770296633243561,
      "learning_rate": 2.9080000000000003e-05,
      "loss": 0.0027,
      "step": 62760
    },
    {
      "epoch": 3.3477333333333332,
      "grad_norm": 0.07737013697624207,
      "learning_rate": 2.907666666666667e-05,
      "loss": 0.0013,
      "step": 62770
    },
    {
      "epoch": 3.3482666666666665,
      "grad_norm": 0.06052180379629135,
      "learning_rate": 2.9073333333333335e-05,
      "loss": 0.0024,
      "step": 62780
    },
    {
      "epoch": 3.3487999999999998,
      "grad_norm": 0.12593261897563934,
      "learning_rate": 2.907e-05,
      "loss": 0.0017,
      "step": 62790
    },
    {
      "epoch": 3.3493333333333335,
      "grad_norm": 0.11114094406366348,
      "learning_rate": 2.906666666666667e-05,
      "loss": 0.0027,
      "step": 62800
    },
    {
      "epoch": 3.3498666666666668,
      "grad_norm": 0.41919952630996704,
      "learning_rate": 2.9063333333333337e-05,
      "loss": 0.0019,
      "step": 62810
    },
    {
      "epoch": 3.3504,
      "grad_norm": 0.4943505823612213,
      "learning_rate": 2.9060000000000003e-05,
      "loss": 0.0019,
      "step": 62820
    },
    {
      "epoch": 3.3509333333333333,
      "grad_norm": 0.07627210766077042,
      "learning_rate": 2.9056666666666666e-05,
      "loss": 0.0021,
      "step": 62830
    },
    {
      "epoch": 3.3514666666666666,
      "grad_norm": 0.09827450662851334,
      "learning_rate": 2.9053333333333332e-05,
      "loss": 0.003,
      "step": 62840
    },
    {
      "epoch": 3.352,
      "grad_norm": 0.1501496136188507,
      "learning_rate": 2.9049999999999998e-05,
      "loss": 0.0023,
      "step": 62850
    },
    {
      "epoch": 3.352533333333333,
      "grad_norm": 0.5307121276855469,
      "learning_rate": 2.9046666666666668e-05,
      "loss": 0.0017,
      "step": 62860
    },
    {
      "epoch": 3.353066666666667,
      "grad_norm": 0.1539008766412735,
      "learning_rate": 2.9043333333333334e-05,
      "loss": 0.002,
      "step": 62870
    },
    {
      "epoch": 3.3536,
      "grad_norm": 0.49392592906951904,
      "learning_rate": 2.904e-05,
      "loss": 0.002,
      "step": 62880
    },
    {
      "epoch": 3.3541333333333334,
      "grad_norm": 0.224089577794075,
      "learning_rate": 2.9036666666666666e-05,
      "loss": 0.0018,
      "step": 62890
    },
    {
      "epoch": 3.3546666666666667,
      "grad_norm": 0.5840234756469727,
      "learning_rate": 2.9033333333333335e-05,
      "loss": 0.0021,
      "step": 62900
    },
    {
      "epoch": 3.3552,
      "grad_norm": 0.1655394732952118,
      "learning_rate": 2.903e-05,
      "loss": 0.0026,
      "step": 62910
    },
    {
      "epoch": 3.3557333333333332,
      "grad_norm": 0.4643396735191345,
      "learning_rate": 2.9026666666666668e-05,
      "loss": 0.0017,
      "step": 62920
    },
    {
      "epoch": 3.3562666666666665,
      "grad_norm": 0.09383917599916458,
      "learning_rate": 2.9023333333333337e-05,
      "loss": 0.0026,
      "step": 62930
    },
    {
      "epoch": 3.3568,
      "grad_norm": 0.1015433520078659,
      "learning_rate": 2.9020000000000003e-05,
      "loss": 0.0024,
      "step": 62940
    },
    {
      "epoch": 3.3573333333333335,
      "grad_norm": 0.14907442033290863,
      "learning_rate": 2.901666666666667e-05,
      "loss": 0.0019,
      "step": 62950
    },
    {
      "epoch": 3.3578666666666668,
      "grad_norm": 0.39392587542533875,
      "learning_rate": 2.9013333333333336e-05,
      "loss": 0.002,
      "step": 62960
    },
    {
      "epoch": 3.3584,
      "grad_norm": 0.0656249150633812,
      "learning_rate": 2.9010000000000005e-05,
      "loss": 0.0018,
      "step": 62970
    },
    {
      "epoch": 3.3589333333333333,
      "grad_norm": 0.07050875574350357,
      "learning_rate": 2.9006666666666665e-05,
      "loss": 0.0025,
      "step": 62980
    },
    {
      "epoch": 3.3594666666666666,
      "grad_norm": 0.12637296319007874,
      "learning_rate": 2.9003333333333334e-05,
      "loss": 0.0018,
      "step": 62990
    },
    {
      "epoch": 3.36,
      "grad_norm": 0.289206862449646,
      "learning_rate": 2.9e-05,
      "loss": 0.0015,
      "step": 63000
    },
    {
      "epoch": 3.360533333333333,
      "grad_norm": 0.6153373122215271,
      "learning_rate": 2.8996666666666666e-05,
      "loss": 0.002,
      "step": 63010
    },
    {
      "epoch": 3.361066666666667,
      "grad_norm": 0.15823303163051605,
      "learning_rate": 2.8993333333333332e-05,
      "loss": 0.0018,
      "step": 63020
    },
    {
      "epoch": 3.3616,
      "grad_norm": 0.2478485107421875,
      "learning_rate": 2.8990000000000002e-05,
      "loss": 0.0019,
      "step": 63030
    },
    {
      "epoch": 3.3621333333333334,
      "grad_norm": 0.5797742009162903,
      "learning_rate": 2.8986666666666668e-05,
      "loss": 0.0021,
      "step": 63040
    },
    {
      "epoch": 3.3626666666666667,
      "grad_norm": 0.08811438083648682,
      "learning_rate": 2.8983333333333334e-05,
      "loss": 0.0023,
      "step": 63050
    },
    {
      "epoch": 3.3632,
      "grad_norm": 0.2892358601093292,
      "learning_rate": 2.898e-05,
      "loss": 0.0031,
      "step": 63060
    },
    {
      "epoch": 3.3637333333333332,
      "grad_norm": 0.6022072434425354,
      "learning_rate": 2.897666666666667e-05,
      "loss": 0.0025,
      "step": 63070
    },
    {
      "epoch": 3.3642666666666665,
      "grad_norm": 0.2412245273590088,
      "learning_rate": 2.8973333333333336e-05,
      "loss": 0.0021,
      "step": 63080
    },
    {
      "epoch": 3.3648,
      "grad_norm": 0.18202243745326996,
      "learning_rate": 2.8970000000000002e-05,
      "loss": 0.0019,
      "step": 63090
    },
    {
      "epoch": 3.3653333333333335,
      "grad_norm": 0.438563734292984,
      "learning_rate": 2.8966666666666668e-05,
      "loss": 0.0014,
      "step": 63100
    },
    {
      "epoch": 3.365866666666667,
      "grad_norm": 0.23381949961185455,
      "learning_rate": 2.8963333333333338e-05,
      "loss": 0.0022,
      "step": 63110
    },
    {
      "epoch": 3.3664,
      "grad_norm": 0.507752537727356,
      "learning_rate": 2.8960000000000004e-05,
      "loss": 0.0027,
      "step": 63120
    },
    {
      "epoch": 3.3669333333333333,
      "grad_norm": 0.2650478184223175,
      "learning_rate": 2.895666666666667e-05,
      "loss": 0.0016,
      "step": 63130
    },
    {
      "epoch": 3.3674666666666666,
      "grad_norm": 0.04084029048681259,
      "learning_rate": 2.8953333333333333e-05,
      "loss": 0.0014,
      "step": 63140
    },
    {
      "epoch": 3.368,
      "grad_norm": 0.4519563317298889,
      "learning_rate": 2.895e-05,
      "loss": 0.0019,
      "step": 63150
    },
    {
      "epoch": 3.368533333333333,
      "grad_norm": 0.2771879732608795,
      "learning_rate": 2.8946666666666665e-05,
      "loss": 0.0021,
      "step": 63160
    },
    {
      "epoch": 3.369066666666667,
      "grad_norm": 0.263543039560318,
      "learning_rate": 2.8943333333333335e-05,
      "loss": 0.0019,
      "step": 63170
    },
    {
      "epoch": 3.3696,
      "grad_norm": 0.2980339229106903,
      "learning_rate": 2.894e-05,
      "loss": 0.0026,
      "step": 63180
    },
    {
      "epoch": 3.3701333333333334,
      "grad_norm": 0.2556147277355194,
      "learning_rate": 2.8936666666666667e-05,
      "loss": 0.0017,
      "step": 63190
    },
    {
      "epoch": 3.3706666666666667,
      "grad_norm": 0.3655638098716736,
      "learning_rate": 2.8933333333333333e-05,
      "loss": 0.0018,
      "step": 63200
    },
    {
      "epoch": 3.3712,
      "grad_norm": 0.4159541428089142,
      "learning_rate": 2.8930000000000003e-05,
      "loss": 0.0019,
      "step": 63210
    },
    {
      "epoch": 3.3717333333333332,
      "grad_norm": 0.31143367290496826,
      "learning_rate": 2.892666666666667e-05,
      "loss": 0.0027,
      "step": 63220
    },
    {
      "epoch": 3.3722666666666665,
      "grad_norm": 0.18409030139446259,
      "learning_rate": 2.8923333333333335e-05,
      "loss": 0.0023,
      "step": 63230
    },
    {
      "epoch": 3.3728,
      "grad_norm": 0.22843781113624573,
      "learning_rate": 2.8920000000000004e-05,
      "loss": 0.0025,
      "step": 63240
    },
    {
      "epoch": 3.3733333333333335,
      "grad_norm": 0.19382354617118835,
      "learning_rate": 2.891666666666667e-05,
      "loss": 0.0017,
      "step": 63250
    },
    {
      "epoch": 3.373866666666667,
      "grad_norm": 0.7247433066368103,
      "learning_rate": 2.8913333333333337e-05,
      "loss": 0.0027,
      "step": 63260
    },
    {
      "epoch": 3.3744,
      "grad_norm": 0.5517354607582092,
      "learning_rate": 2.8910000000000003e-05,
      "loss": 0.0021,
      "step": 63270
    },
    {
      "epoch": 3.3749333333333333,
      "grad_norm": 0.49435579776763916,
      "learning_rate": 2.8906666666666672e-05,
      "loss": 0.0017,
      "step": 63280
    },
    {
      "epoch": 3.3754666666666666,
      "grad_norm": 0.19084277749061584,
      "learning_rate": 2.890333333333333e-05,
      "loss": 0.002,
      "step": 63290
    },
    {
      "epoch": 3.376,
      "grad_norm": 0.28581345081329346,
      "learning_rate": 2.8899999999999998e-05,
      "loss": 0.0019,
      "step": 63300
    },
    {
      "epoch": 3.376533333333333,
      "grad_norm": 0.18093782663345337,
      "learning_rate": 2.8896666666666667e-05,
      "loss": 0.0017,
      "step": 63310
    },
    {
      "epoch": 3.377066666666667,
      "grad_norm": 0.7694336175918579,
      "learning_rate": 2.8893333333333333e-05,
      "loss": 0.002,
      "step": 63320
    },
    {
      "epoch": 3.3776,
      "grad_norm": 0.4447105824947357,
      "learning_rate": 2.889e-05,
      "loss": 0.0023,
      "step": 63330
    },
    {
      "epoch": 3.3781333333333334,
      "grad_norm": 0.2806369960308075,
      "learning_rate": 2.888666666666667e-05,
      "loss": 0.0023,
      "step": 63340
    },
    {
      "epoch": 3.3786666666666667,
      "grad_norm": 0.04364542290568352,
      "learning_rate": 2.8883333333333335e-05,
      "loss": 0.0026,
      "step": 63350
    },
    {
      "epoch": 3.3792,
      "grad_norm": 0.5842009782791138,
      "learning_rate": 2.888e-05,
      "loss": 0.002,
      "step": 63360
    },
    {
      "epoch": 3.3797333333333333,
      "grad_norm": 0.0941353440284729,
      "learning_rate": 2.8876666666666667e-05,
      "loss": 0.0018,
      "step": 63370
    },
    {
      "epoch": 3.3802666666666665,
      "grad_norm": 0.1567678302526474,
      "learning_rate": 2.8873333333333337e-05,
      "loss": 0.0013,
      "step": 63380
    },
    {
      "epoch": 3.3808,
      "grad_norm": 0.09499423205852509,
      "learning_rate": 2.8870000000000003e-05,
      "loss": 0.0027,
      "step": 63390
    },
    {
      "epoch": 3.3813333333333335,
      "grad_norm": 0.3639964163303375,
      "learning_rate": 2.886666666666667e-05,
      "loss": 0.0016,
      "step": 63400
    },
    {
      "epoch": 3.381866666666667,
      "grad_norm": 0.2687409520149231,
      "learning_rate": 2.8863333333333335e-05,
      "loss": 0.0019,
      "step": 63410
    },
    {
      "epoch": 3.3824,
      "grad_norm": 0.617728054523468,
      "learning_rate": 2.8860000000000005e-05,
      "loss": 0.0022,
      "step": 63420
    },
    {
      "epoch": 3.3829333333333333,
      "grad_norm": 0.27733859419822693,
      "learning_rate": 2.885666666666667e-05,
      "loss": 0.0013,
      "step": 63430
    },
    {
      "epoch": 3.3834666666666666,
      "grad_norm": 0.04417935758829117,
      "learning_rate": 2.8853333333333334e-05,
      "loss": 0.0017,
      "step": 63440
    },
    {
      "epoch": 3.384,
      "grad_norm": 0.30374079942703247,
      "learning_rate": 2.885e-05,
      "loss": 0.0022,
      "step": 63450
    },
    {
      "epoch": 3.384533333333333,
      "grad_norm": 0.20224730670452118,
      "learning_rate": 2.8846666666666666e-05,
      "loss": 0.002,
      "step": 63460
    },
    {
      "epoch": 3.385066666666667,
      "grad_norm": 0.4193836748600006,
      "learning_rate": 2.8843333333333332e-05,
      "loss": 0.0025,
      "step": 63470
    },
    {
      "epoch": 3.3856,
      "grad_norm": 0.163817897439003,
      "learning_rate": 2.8840000000000002e-05,
      "loss": 0.0031,
      "step": 63480
    },
    {
      "epoch": 3.3861333333333334,
      "grad_norm": 0.09776297211647034,
      "learning_rate": 2.8836666666666668e-05,
      "loss": 0.0021,
      "step": 63490
    },
    {
      "epoch": 3.3866666666666667,
      "grad_norm": 0.18560655415058136,
      "learning_rate": 2.8833333333333334e-05,
      "loss": 0.0026,
      "step": 63500
    },
    {
      "epoch": 3.3872,
      "grad_norm": 0.036597441881895065,
      "learning_rate": 2.883e-05,
      "loss": 0.0028,
      "step": 63510
    },
    {
      "epoch": 3.3877333333333333,
      "grad_norm": 0.3414269685745239,
      "learning_rate": 2.882666666666667e-05,
      "loss": 0.0026,
      "step": 63520
    },
    {
      "epoch": 3.3882666666666665,
      "grad_norm": 0.15777236223220825,
      "learning_rate": 2.8823333333333336e-05,
      "loss": 0.0025,
      "step": 63530
    },
    {
      "epoch": 3.3888,
      "grad_norm": 0.7052040696144104,
      "learning_rate": 2.8820000000000002e-05,
      "loss": 0.0024,
      "step": 63540
    },
    {
      "epoch": 3.389333333333333,
      "grad_norm": 0.3205626606941223,
      "learning_rate": 2.8816666666666668e-05,
      "loss": 0.0021,
      "step": 63550
    },
    {
      "epoch": 3.389866666666667,
      "grad_norm": 0.7855685949325562,
      "learning_rate": 2.8813333333333338e-05,
      "loss": 0.0025,
      "step": 63560
    },
    {
      "epoch": 3.3904,
      "grad_norm": 0.5565182566642761,
      "learning_rate": 2.8810000000000004e-05,
      "loss": 0.0029,
      "step": 63570
    },
    {
      "epoch": 3.3909333333333334,
      "grad_norm": 0.10806342959403992,
      "learning_rate": 2.880666666666667e-05,
      "loss": 0.0018,
      "step": 63580
    },
    {
      "epoch": 3.3914666666666666,
      "grad_norm": 0.30696624517440796,
      "learning_rate": 2.8803333333333333e-05,
      "loss": 0.0016,
      "step": 63590
    },
    {
      "epoch": 3.392,
      "grad_norm": 0.39266350865364075,
      "learning_rate": 2.88e-05,
      "loss": 0.0019,
      "step": 63600
    },
    {
      "epoch": 3.392533333333333,
      "grad_norm": 0.07576259970664978,
      "learning_rate": 2.8796666666666665e-05,
      "loss": 0.0023,
      "step": 63610
    },
    {
      "epoch": 3.393066666666667,
      "grad_norm": 0.13573448359966278,
      "learning_rate": 2.8793333333333334e-05,
      "loss": 0.0024,
      "step": 63620
    },
    {
      "epoch": 3.3936,
      "grad_norm": 0.30420637130737305,
      "learning_rate": 2.879e-05,
      "loss": 0.0017,
      "step": 63630
    },
    {
      "epoch": 3.3941333333333334,
      "grad_norm": 0.5764250159263611,
      "learning_rate": 2.8786666666666667e-05,
      "loss": 0.0024,
      "step": 63640
    },
    {
      "epoch": 3.3946666666666667,
      "grad_norm": 0.07075095921754837,
      "learning_rate": 2.8783333333333333e-05,
      "loss": 0.0023,
      "step": 63650
    },
    {
      "epoch": 3.3952,
      "grad_norm": 0.12007313221693039,
      "learning_rate": 2.8780000000000002e-05,
      "loss": 0.0018,
      "step": 63660
    },
    {
      "epoch": 3.3957333333333333,
      "grad_norm": 0.04822218045592308,
      "learning_rate": 2.877666666666667e-05,
      "loss": 0.0014,
      "step": 63670
    },
    {
      "epoch": 3.3962666666666665,
      "grad_norm": 0.22599352896213531,
      "learning_rate": 2.8773333333333335e-05,
      "loss": 0.0023,
      "step": 63680
    },
    {
      "epoch": 3.3968,
      "grad_norm": 0.2814998924732208,
      "learning_rate": 2.8770000000000004e-05,
      "loss": 0.0017,
      "step": 63690
    },
    {
      "epoch": 3.397333333333333,
      "grad_norm": 0.15095698833465576,
      "learning_rate": 2.876666666666667e-05,
      "loss": 0.0018,
      "step": 63700
    },
    {
      "epoch": 3.397866666666667,
      "grad_norm": 0.40002989768981934,
      "learning_rate": 2.8763333333333336e-05,
      "loss": 0.0013,
      "step": 63710
    },
    {
      "epoch": 3.3984,
      "grad_norm": 0.07405804842710495,
      "learning_rate": 2.8760000000000002e-05,
      "loss": 0.0028,
      "step": 63720
    },
    {
      "epoch": 3.3989333333333334,
      "grad_norm": 0.5974203944206238,
      "learning_rate": 2.8756666666666672e-05,
      "loss": 0.0027,
      "step": 63730
    },
    {
      "epoch": 3.3994666666666666,
      "grad_norm": 0.20120449364185333,
      "learning_rate": 2.875333333333333e-05,
      "loss": 0.0024,
      "step": 63740
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.2785080671310425,
      "learning_rate": 2.8749999999999997e-05,
      "loss": 0.0019,
      "step": 63750
    },
    {
      "epoch": 3.400533333333333,
      "grad_norm": 0.27701103687286377,
      "learning_rate": 2.8746666666666667e-05,
      "loss": 0.0022,
      "step": 63760
    },
    {
      "epoch": 3.401066666666667,
      "grad_norm": 0.33059078454971313,
      "learning_rate": 2.8743333333333333e-05,
      "loss": 0.0022,
      "step": 63770
    },
    {
      "epoch": 3.4016,
      "grad_norm": 0.13210304081439972,
      "learning_rate": 2.874e-05,
      "loss": 0.002,
      "step": 63780
    },
    {
      "epoch": 3.4021333333333335,
      "grad_norm": 0.27768242359161377,
      "learning_rate": 2.873666666666667e-05,
      "loss": 0.0028,
      "step": 63790
    },
    {
      "epoch": 3.4026666666666667,
      "grad_norm": 0.22559332847595215,
      "learning_rate": 2.8733333333333335e-05,
      "loss": 0.0026,
      "step": 63800
    },
    {
      "epoch": 3.4032,
      "grad_norm": 0.029505698010325432,
      "learning_rate": 2.873e-05,
      "loss": 0.0025,
      "step": 63810
    },
    {
      "epoch": 3.4037333333333333,
      "grad_norm": 0.1574973464012146,
      "learning_rate": 2.8726666666666667e-05,
      "loss": 0.0023,
      "step": 63820
    },
    {
      "epoch": 3.4042666666666666,
      "grad_norm": 0.06416086852550507,
      "learning_rate": 2.8723333333333337e-05,
      "loss": 0.002,
      "step": 63830
    },
    {
      "epoch": 3.4048,
      "grad_norm": 0.30932334065437317,
      "learning_rate": 2.8720000000000003e-05,
      "loss": 0.0024,
      "step": 63840
    },
    {
      "epoch": 3.405333333333333,
      "grad_norm": 0.23225320875644684,
      "learning_rate": 2.871666666666667e-05,
      "loss": 0.0028,
      "step": 63850
    },
    {
      "epoch": 3.405866666666667,
      "grad_norm": 0.538644015789032,
      "learning_rate": 2.8713333333333335e-05,
      "loss": 0.0027,
      "step": 63860
    },
    {
      "epoch": 3.4064,
      "grad_norm": 0.09664713591337204,
      "learning_rate": 2.8710000000000005e-05,
      "loss": 0.0014,
      "step": 63870
    },
    {
      "epoch": 3.4069333333333334,
      "grad_norm": 0.7464160323143005,
      "learning_rate": 2.870666666666667e-05,
      "loss": 0.002,
      "step": 63880
    },
    {
      "epoch": 3.4074666666666666,
      "grad_norm": 0.2547674775123596,
      "learning_rate": 2.8703333333333334e-05,
      "loss": 0.0038,
      "step": 63890
    },
    {
      "epoch": 3.408,
      "grad_norm": 0.04275797680020332,
      "learning_rate": 2.87e-05,
      "loss": 0.002,
      "step": 63900
    },
    {
      "epoch": 3.408533333333333,
      "grad_norm": 0.02068360522389412,
      "learning_rate": 2.8696666666666666e-05,
      "loss": 0.0018,
      "step": 63910
    },
    {
      "epoch": 3.409066666666667,
      "grad_norm": 0.41789862513542175,
      "learning_rate": 2.8693333333333332e-05,
      "loss": 0.0019,
      "step": 63920
    },
    {
      "epoch": 3.4096,
      "grad_norm": 0.1018478199839592,
      "learning_rate": 2.869e-05,
      "loss": 0.0014,
      "step": 63930
    },
    {
      "epoch": 3.4101333333333335,
      "grad_norm": 0.5677010416984558,
      "learning_rate": 2.8686666666666668e-05,
      "loss": 0.0026,
      "step": 63940
    },
    {
      "epoch": 3.4106666666666667,
      "grad_norm": 0.5144072771072388,
      "learning_rate": 2.8683333333333334e-05,
      "loss": 0.0021,
      "step": 63950
    },
    {
      "epoch": 3.4112,
      "grad_norm": 0.3387952744960785,
      "learning_rate": 2.868e-05,
      "loss": 0.0031,
      "step": 63960
    },
    {
      "epoch": 3.4117333333333333,
      "grad_norm": 0.27094146609306335,
      "learning_rate": 2.867666666666667e-05,
      "loss": 0.0019,
      "step": 63970
    },
    {
      "epoch": 3.4122666666666666,
      "grad_norm": 0.3726983964443207,
      "learning_rate": 2.8673333333333336e-05,
      "loss": 0.0024,
      "step": 63980
    },
    {
      "epoch": 3.4128,
      "grad_norm": 0.2504098117351532,
      "learning_rate": 2.867e-05,
      "loss": 0.002,
      "step": 63990
    },
    {
      "epoch": 3.413333333333333,
      "grad_norm": 0.42487409710884094,
      "learning_rate": 2.8666666666666668e-05,
      "loss": 0.0018,
      "step": 64000
    },
    {
      "epoch": 3.413866666666667,
      "grad_norm": 0.15215054154396057,
      "learning_rate": 2.8663333333333337e-05,
      "loss": 0.0019,
      "step": 64010
    },
    {
      "epoch": 3.4144,
      "grad_norm": 0.1053209975361824,
      "learning_rate": 2.8660000000000003e-05,
      "loss": 0.002,
      "step": 64020
    },
    {
      "epoch": 3.4149333333333334,
      "grad_norm": 0.09830185770988464,
      "learning_rate": 2.865666666666667e-05,
      "loss": 0.0019,
      "step": 64030
    },
    {
      "epoch": 3.4154666666666667,
      "grad_norm": 0.1244487538933754,
      "learning_rate": 2.8653333333333332e-05,
      "loss": 0.0038,
      "step": 64040
    },
    {
      "epoch": 3.416,
      "grad_norm": 0.24300746619701385,
      "learning_rate": 2.865e-05,
      "loss": 0.002,
      "step": 64050
    },
    {
      "epoch": 3.416533333333333,
      "grad_norm": 0.2775493860244751,
      "learning_rate": 2.8646666666666665e-05,
      "loss": 0.002,
      "step": 64060
    },
    {
      "epoch": 3.4170666666666665,
      "grad_norm": 0.3066927194595337,
      "learning_rate": 2.8643333333333334e-05,
      "loss": 0.002,
      "step": 64070
    },
    {
      "epoch": 3.4176,
      "grad_norm": 0.10230410099029541,
      "learning_rate": 2.864e-05,
      "loss": 0.0016,
      "step": 64080
    },
    {
      "epoch": 3.4181333333333335,
      "grad_norm": 0.1853722184896469,
      "learning_rate": 2.8636666666666666e-05,
      "loss": 0.0022,
      "step": 64090
    },
    {
      "epoch": 3.4186666666666667,
      "grad_norm": 0.24195103347301483,
      "learning_rate": 2.8633333333333336e-05,
      "loss": 0.0023,
      "step": 64100
    },
    {
      "epoch": 3.4192,
      "grad_norm": 0.05050477385520935,
      "learning_rate": 2.8630000000000002e-05,
      "loss": 0.0022,
      "step": 64110
    },
    {
      "epoch": 3.4197333333333333,
      "grad_norm": 0.28550904989242554,
      "learning_rate": 2.8626666666666668e-05,
      "loss": 0.0013,
      "step": 64120
    },
    {
      "epoch": 3.4202666666666666,
      "grad_norm": 0.13121621310710907,
      "learning_rate": 2.8623333333333334e-05,
      "loss": 0.002,
      "step": 64130
    },
    {
      "epoch": 3.4208,
      "grad_norm": 0.05292887985706329,
      "learning_rate": 2.8620000000000004e-05,
      "loss": 0.0018,
      "step": 64140
    },
    {
      "epoch": 3.421333333333333,
      "grad_norm": 0.25214433670043945,
      "learning_rate": 2.861666666666667e-05,
      "loss": 0.0024,
      "step": 64150
    },
    {
      "epoch": 3.421866666666667,
      "grad_norm": 0.5241827964782715,
      "learning_rate": 2.8613333333333336e-05,
      "loss": 0.0028,
      "step": 64160
    },
    {
      "epoch": 3.4224,
      "grad_norm": 0.04609167203307152,
      "learning_rate": 2.8610000000000002e-05,
      "loss": 0.002,
      "step": 64170
    },
    {
      "epoch": 3.4229333333333334,
      "grad_norm": 0.27002766728401184,
      "learning_rate": 2.8606666666666672e-05,
      "loss": 0.0023,
      "step": 64180
    },
    {
      "epoch": 3.4234666666666667,
      "grad_norm": 0.4339165687561035,
      "learning_rate": 2.860333333333333e-05,
      "loss": 0.0017,
      "step": 64190
    },
    {
      "epoch": 3.424,
      "grad_norm": 0.4569756090641022,
      "learning_rate": 2.86e-05,
      "loss": 0.0017,
      "step": 64200
    },
    {
      "epoch": 3.424533333333333,
      "grad_norm": 0.3444909453392029,
      "learning_rate": 2.8596666666666667e-05,
      "loss": 0.0019,
      "step": 64210
    },
    {
      "epoch": 3.4250666666666665,
      "grad_norm": 0.22776153683662415,
      "learning_rate": 2.8593333333333333e-05,
      "loss": 0.002,
      "step": 64220
    },
    {
      "epoch": 3.4256,
      "grad_norm": 0.45738664269447327,
      "learning_rate": 2.859e-05,
      "loss": 0.0019,
      "step": 64230
    },
    {
      "epoch": 3.4261333333333335,
      "grad_norm": 0.3265227675437927,
      "learning_rate": 2.858666666666667e-05,
      "loss": 0.0017,
      "step": 64240
    },
    {
      "epoch": 3.4266666666666667,
      "grad_norm": 0.10507780313491821,
      "learning_rate": 2.8583333333333335e-05,
      "loss": 0.0023,
      "step": 64250
    },
    {
      "epoch": 3.4272,
      "grad_norm": 0.49949583411216736,
      "learning_rate": 2.858e-05,
      "loss": 0.0029,
      "step": 64260
    },
    {
      "epoch": 3.4277333333333333,
      "grad_norm": 0.4336605668067932,
      "learning_rate": 2.8576666666666667e-05,
      "loss": 0.0013,
      "step": 64270
    },
    {
      "epoch": 3.4282666666666666,
      "grad_norm": 0.4154459536075592,
      "learning_rate": 2.8573333333333336e-05,
      "loss": 0.0013,
      "step": 64280
    },
    {
      "epoch": 3.4288,
      "grad_norm": 0.7438987493515015,
      "learning_rate": 2.8570000000000003e-05,
      "loss": 0.0017,
      "step": 64290
    },
    {
      "epoch": 3.429333333333333,
      "grad_norm": 0.3258799910545349,
      "learning_rate": 2.856666666666667e-05,
      "loss": 0.0027,
      "step": 64300
    },
    {
      "epoch": 3.429866666666667,
      "grad_norm": 0.19533541798591614,
      "learning_rate": 2.8563333333333335e-05,
      "loss": 0.0023,
      "step": 64310
    },
    {
      "epoch": 3.4304,
      "grad_norm": 0.24713440239429474,
      "learning_rate": 2.8560000000000004e-05,
      "loss": 0.0012,
      "step": 64320
    },
    {
      "epoch": 3.4309333333333334,
      "grad_norm": 0.25086602568626404,
      "learning_rate": 2.855666666666667e-05,
      "loss": 0.0025,
      "step": 64330
    },
    {
      "epoch": 3.4314666666666667,
      "grad_norm": 0.08249568939208984,
      "learning_rate": 2.8553333333333333e-05,
      "loss": 0.0023,
      "step": 64340
    },
    {
      "epoch": 3.432,
      "grad_norm": 0.39778566360473633,
      "learning_rate": 2.855e-05,
      "loss": 0.0026,
      "step": 64350
    },
    {
      "epoch": 3.432533333333333,
      "grad_norm": 0.19318312406539917,
      "learning_rate": 2.8546666666666666e-05,
      "loss": 0.0026,
      "step": 64360
    },
    {
      "epoch": 3.4330666666666665,
      "grad_norm": 0.338595449924469,
      "learning_rate": 2.854333333333333e-05,
      "loss": 0.0019,
      "step": 64370
    },
    {
      "epoch": 3.4336,
      "grad_norm": 0.3402459919452667,
      "learning_rate": 2.854e-05,
      "loss": 0.0018,
      "step": 64380
    },
    {
      "epoch": 3.4341333333333335,
      "grad_norm": 0.4753818213939667,
      "learning_rate": 2.8536666666666667e-05,
      "loss": 0.0021,
      "step": 64390
    },
    {
      "epoch": 3.4346666666666668,
      "grad_norm": 0.08245010673999786,
      "learning_rate": 2.8533333333333333e-05,
      "loss": 0.0023,
      "step": 64400
    },
    {
      "epoch": 3.4352,
      "grad_norm": 0.2865023612976074,
      "learning_rate": 2.853e-05,
      "loss": 0.0021,
      "step": 64410
    },
    {
      "epoch": 3.4357333333333333,
      "grad_norm": 0.2796967327594757,
      "learning_rate": 2.852666666666667e-05,
      "loss": 0.0035,
      "step": 64420
    },
    {
      "epoch": 3.4362666666666666,
      "grad_norm": 0.20252341032028198,
      "learning_rate": 2.8523333333333335e-05,
      "loss": 0.0023,
      "step": 64430
    },
    {
      "epoch": 3.4368,
      "grad_norm": 0.5466461777687073,
      "learning_rate": 2.852e-05,
      "loss": 0.0022,
      "step": 64440
    },
    {
      "epoch": 3.437333333333333,
      "grad_norm": 0.10399672389030457,
      "learning_rate": 2.851666666666667e-05,
      "loss": 0.0022,
      "step": 64450
    },
    {
      "epoch": 3.437866666666667,
      "grad_norm": 0.4389958381652832,
      "learning_rate": 2.8513333333333337e-05,
      "loss": 0.0025,
      "step": 64460
    },
    {
      "epoch": 3.4384,
      "grad_norm": 0.12203828245401382,
      "learning_rate": 2.8510000000000003e-05,
      "loss": 0.0025,
      "step": 64470
    },
    {
      "epoch": 3.4389333333333334,
      "grad_norm": 0.47661176323890686,
      "learning_rate": 2.850666666666667e-05,
      "loss": 0.0024,
      "step": 64480
    },
    {
      "epoch": 3.4394666666666667,
      "grad_norm": 0.08076515048742294,
      "learning_rate": 2.850333333333334e-05,
      "loss": 0.0015,
      "step": 64490
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.6784414649009705,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 0.0025,
      "step": 64500
    },
    {
      "epoch": 3.440533333333333,
      "grad_norm": 0.29301998019218445,
      "learning_rate": 2.8496666666666664e-05,
      "loss": 0.0017,
      "step": 64510
    },
    {
      "epoch": 3.4410666666666665,
      "grad_norm": 0.15930071473121643,
      "learning_rate": 2.8493333333333334e-05,
      "loss": 0.0028,
      "step": 64520
    },
    {
      "epoch": 3.4416,
      "grad_norm": 0.7008891105651855,
      "learning_rate": 2.849e-05,
      "loss": 0.0019,
      "step": 64530
    },
    {
      "epoch": 3.4421333333333335,
      "grad_norm": 0.4551606774330139,
      "learning_rate": 2.8486666666666666e-05,
      "loss": 0.0019,
      "step": 64540
    },
    {
      "epoch": 3.4426666666666668,
      "grad_norm": 0.20779797434806824,
      "learning_rate": 2.8483333333333336e-05,
      "loss": 0.0021,
      "step": 64550
    },
    {
      "epoch": 3.4432,
      "grad_norm": 0.3559914231300354,
      "learning_rate": 2.8480000000000002e-05,
      "loss": 0.0022,
      "step": 64560
    },
    {
      "epoch": 3.4437333333333333,
      "grad_norm": 0.15753531455993652,
      "learning_rate": 2.8476666666666668e-05,
      "loss": 0.0021,
      "step": 64570
    },
    {
      "epoch": 3.4442666666666666,
      "grad_norm": 0.2164669632911682,
      "learning_rate": 2.8473333333333334e-05,
      "loss": 0.0019,
      "step": 64580
    },
    {
      "epoch": 3.4448,
      "grad_norm": 0.03995949402451515,
      "learning_rate": 2.8470000000000004e-05,
      "loss": 0.0024,
      "step": 64590
    },
    {
      "epoch": 3.445333333333333,
      "grad_norm": 0.14899171888828278,
      "learning_rate": 2.846666666666667e-05,
      "loss": 0.0029,
      "step": 64600
    },
    {
      "epoch": 3.445866666666667,
      "grad_norm": 0.11367446929216385,
      "learning_rate": 2.8463333333333336e-05,
      "loss": 0.0015,
      "step": 64610
    },
    {
      "epoch": 3.4464,
      "grad_norm": 0.27508774399757385,
      "learning_rate": 2.8460000000000002e-05,
      "loss": 0.0019,
      "step": 64620
    },
    {
      "epoch": 3.4469333333333334,
      "grad_norm": 0.3584550619125366,
      "learning_rate": 2.845666666666667e-05,
      "loss": 0.0016,
      "step": 64630
    },
    {
      "epoch": 3.4474666666666667,
      "grad_norm": 0.119101382791996,
      "learning_rate": 2.8453333333333338e-05,
      "loss": 0.0023,
      "step": 64640
    },
    {
      "epoch": 3.448,
      "grad_norm": 0.0550919771194458,
      "learning_rate": 2.845e-05,
      "loss": 0.0014,
      "step": 64650
    },
    {
      "epoch": 3.4485333333333332,
      "grad_norm": 0.18027888238430023,
      "learning_rate": 2.8446666666666666e-05,
      "loss": 0.0022,
      "step": 64660
    },
    {
      "epoch": 3.4490666666666665,
      "grad_norm": 0.43717464804649353,
      "learning_rate": 2.8443333333333333e-05,
      "loss": 0.0019,
      "step": 64670
    },
    {
      "epoch": 3.4496,
      "grad_norm": 0.5126494765281677,
      "learning_rate": 2.844e-05,
      "loss": 0.003,
      "step": 64680
    },
    {
      "epoch": 3.4501333333333335,
      "grad_norm": 0.2883753478527069,
      "learning_rate": 2.8436666666666668e-05,
      "loss": 0.0021,
      "step": 64690
    },
    {
      "epoch": 3.4506666666666668,
      "grad_norm": 0.12261619418859482,
      "learning_rate": 2.8433333333333334e-05,
      "loss": 0.0019,
      "step": 64700
    },
    {
      "epoch": 3.4512,
      "grad_norm": 0.6086454391479492,
      "learning_rate": 2.843e-05,
      "loss": 0.002,
      "step": 64710
    },
    {
      "epoch": 3.4517333333333333,
      "grad_norm": 0.47214454412460327,
      "learning_rate": 2.8426666666666667e-05,
      "loss": 0.002,
      "step": 64720
    },
    {
      "epoch": 3.4522666666666666,
      "grad_norm": 0.24304193258285522,
      "learning_rate": 2.8423333333333336e-05,
      "loss": 0.0015,
      "step": 64730
    },
    {
      "epoch": 3.4528,
      "grad_norm": 0.19349759817123413,
      "learning_rate": 2.8420000000000002e-05,
      "loss": 0.0017,
      "step": 64740
    },
    {
      "epoch": 3.453333333333333,
      "grad_norm": 0.1596921980381012,
      "learning_rate": 2.841666666666667e-05,
      "loss": 0.0018,
      "step": 64750
    },
    {
      "epoch": 3.4538666666666664,
      "grad_norm": 0.3408791422843933,
      "learning_rate": 2.8413333333333335e-05,
      "loss": 0.002,
      "step": 64760
    },
    {
      "epoch": 3.4544,
      "grad_norm": 0.41945260763168335,
      "learning_rate": 2.8410000000000004e-05,
      "loss": 0.0029,
      "step": 64770
    },
    {
      "epoch": 3.4549333333333334,
      "grad_norm": 0.07596616446971893,
      "learning_rate": 2.840666666666667e-05,
      "loss": 0.0021,
      "step": 64780
    },
    {
      "epoch": 3.4554666666666667,
      "grad_norm": 0.058884937316179276,
      "learning_rate": 2.8403333333333336e-05,
      "loss": 0.002,
      "step": 64790
    },
    {
      "epoch": 3.456,
      "grad_norm": 0.2843290865421295,
      "learning_rate": 2.84e-05,
      "loss": 0.0015,
      "step": 64800
    },
    {
      "epoch": 3.4565333333333332,
      "grad_norm": 0.07293468713760376,
      "learning_rate": 2.8396666666666665e-05,
      "loss": 0.002,
      "step": 64810
    },
    {
      "epoch": 3.4570666666666665,
      "grad_norm": 0.5703314542770386,
      "learning_rate": 2.839333333333333e-05,
      "loss": 0.0017,
      "step": 64820
    },
    {
      "epoch": 3.4576000000000002,
      "grad_norm": 0.18056628108024597,
      "learning_rate": 2.839e-05,
      "loss": 0.0023,
      "step": 64830
    },
    {
      "epoch": 3.4581333333333335,
      "grad_norm": 0.12830764055252075,
      "learning_rate": 2.8386666666666667e-05,
      "loss": 0.0016,
      "step": 64840
    },
    {
      "epoch": 3.458666666666667,
      "grad_norm": 0.4550129771232605,
      "learning_rate": 2.8383333333333333e-05,
      "loss": 0.0017,
      "step": 64850
    },
    {
      "epoch": 3.4592,
      "grad_norm": 0.30401888489723206,
      "learning_rate": 2.8380000000000003e-05,
      "loss": 0.0015,
      "step": 64860
    },
    {
      "epoch": 3.4597333333333333,
      "grad_norm": 0.5100793242454529,
      "learning_rate": 2.837666666666667e-05,
      "loss": 0.0025,
      "step": 64870
    },
    {
      "epoch": 3.4602666666666666,
      "grad_norm": 0.6119346618652344,
      "learning_rate": 2.8373333333333335e-05,
      "loss": 0.0021,
      "step": 64880
    },
    {
      "epoch": 3.4608,
      "grad_norm": 0.2510002851486206,
      "learning_rate": 2.837e-05,
      "loss": 0.0012,
      "step": 64890
    },
    {
      "epoch": 3.461333333333333,
      "grad_norm": 0.519375741481781,
      "learning_rate": 2.836666666666667e-05,
      "loss": 0.002,
      "step": 64900
    },
    {
      "epoch": 3.4618666666666664,
      "grad_norm": 0.04269528016448021,
      "learning_rate": 2.8363333333333337e-05,
      "loss": 0.0026,
      "step": 64910
    },
    {
      "epoch": 3.4624,
      "grad_norm": 0.3481706380844116,
      "learning_rate": 2.8360000000000003e-05,
      "loss": 0.0018,
      "step": 64920
    },
    {
      "epoch": 3.4629333333333334,
      "grad_norm": 0.4912929832935333,
      "learning_rate": 2.835666666666667e-05,
      "loss": 0.002,
      "step": 64930
    },
    {
      "epoch": 3.4634666666666667,
      "grad_norm": 0.4130582809448242,
      "learning_rate": 2.835333333333334e-05,
      "loss": 0.0021,
      "step": 64940
    },
    {
      "epoch": 3.464,
      "grad_norm": 0.1269235610961914,
      "learning_rate": 2.8349999999999998e-05,
      "loss": 0.0018,
      "step": 64950
    },
    {
      "epoch": 3.4645333333333332,
      "grad_norm": 0.0321529321372509,
      "learning_rate": 2.8346666666666667e-05,
      "loss": 0.0024,
      "step": 64960
    },
    {
      "epoch": 3.4650666666666665,
      "grad_norm": 0.09572606533765793,
      "learning_rate": 2.8343333333333334e-05,
      "loss": 0.0018,
      "step": 64970
    },
    {
      "epoch": 3.4656000000000002,
      "grad_norm": 0.3489847779273987,
      "learning_rate": 2.834e-05,
      "loss": 0.0027,
      "step": 64980
    },
    {
      "epoch": 3.4661333333333335,
      "grad_norm": 0.3385993242263794,
      "learning_rate": 2.8336666666666666e-05,
      "loss": 0.0015,
      "step": 64990
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 0.17335958778858185,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 0.0012,
      "step": 65000
    },
    {
      "epoch": 3.4672,
      "grad_norm": 0.5501747131347656,
      "learning_rate": 2.833e-05,
      "loss": 0.0017,
      "step": 65010
    },
    {
      "epoch": 3.4677333333333333,
      "grad_norm": 0.1172197014093399,
      "learning_rate": 2.8326666666666668e-05,
      "loss": 0.0014,
      "step": 65020
    },
    {
      "epoch": 3.4682666666666666,
      "grad_norm": 0.12784533202648163,
      "learning_rate": 2.8323333333333334e-05,
      "loss": 0.0021,
      "step": 65030
    },
    {
      "epoch": 3.4688,
      "grad_norm": 0.2414439171552658,
      "learning_rate": 2.8320000000000003e-05,
      "loss": 0.0015,
      "step": 65040
    },
    {
      "epoch": 3.469333333333333,
      "grad_norm": 0.22218084335327148,
      "learning_rate": 2.831666666666667e-05,
      "loss": 0.0021,
      "step": 65050
    },
    {
      "epoch": 3.4698666666666664,
      "grad_norm": 0.12209124863147736,
      "learning_rate": 2.8313333333333336e-05,
      "loss": 0.002,
      "step": 65060
    },
    {
      "epoch": 3.4704,
      "grad_norm": 0.20999184250831604,
      "learning_rate": 2.8310000000000002e-05,
      "loss": 0.0028,
      "step": 65070
    },
    {
      "epoch": 3.4709333333333334,
      "grad_norm": 0.6187706589698792,
      "learning_rate": 2.830666666666667e-05,
      "loss": 0.0025,
      "step": 65080
    },
    {
      "epoch": 3.4714666666666667,
      "grad_norm": 0.08596165478229523,
      "learning_rate": 2.8303333333333337e-05,
      "loss": 0.0015,
      "step": 65090
    },
    {
      "epoch": 3.472,
      "grad_norm": 0.16011717915534973,
      "learning_rate": 2.83e-05,
      "loss": 0.0025,
      "step": 65100
    },
    {
      "epoch": 3.4725333333333332,
      "grad_norm": 0.5809811353683472,
      "learning_rate": 2.8296666666666666e-05,
      "loss": 0.0018,
      "step": 65110
    },
    {
      "epoch": 3.4730666666666665,
      "grad_norm": 0.07348350435495377,
      "learning_rate": 2.8293333333333332e-05,
      "loss": 0.002,
      "step": 65120
    },
    {
      "epoch": 3.4736000000000002,
      "grad_norm": 0.5812293291091919,
      "learning_rate": 2.829e-05,
      "loss": 0.0023,
      "step": 65130
    },
    {
      "epoch": 3.4741333333333335,
      "grad_norm": 0.2593856155872345,
      "learning_rate": 2.8286666666666668e-05,
      "loss": 0.0018,
      "step": 65140
    },
    {
      "epoch": 3.474666666666667,
      "grad_norm": 0.2175738513469696,
      "learning_rate": 2.8283333333333334e-05,
      "loss": 0.0033,
      "step": 65150
    },
    {
      "epoch": 3.4752,
      "grad_norm": 0.09068004041910172,
      "learning_rate": 2.828e-05,
      "loss": 0.0022,
      "step": 65160
    },
    {
      "epoch": 3.4757333333333333,
      "grad_norm": 0.10578508675098419,
      "learning_rate": 2.8276666666666666e-05,
      "loss": 0.0017,
      "step": 65170
    },
    {
      "epoch": 3.4762666666666666,
      "grad_norm": 0.06223025172948837,
      "learning_rate": 2.8273333333333336e-05,
      "loss": 0.002,
      "step": 65180
    },
    {
      "epoch": 3.4768,
      "grad_norm": 0.06684771180152893,
      "learning_rate": 2.8270000000000002e-05,
      "loss": 0.0028,
      "step": 65190
    },
    {
      "epoch": 3.477333333333333,
      "grad_norm": 0.5368995070457458,
      "learning_rate": 2.8266666666666668e-05,
      "loss": 0.0022,
      "step": 65200
    },
    {
      "epoch": 3.4778666666666664,
      "grad_norm": 0.2757854461669922,
      "learning_rate": 2.8263333333333338e-05,
      "loss": 0.0021,
      "step": 65210
    },
    {
      "epoch": 3.4784,
      "grad_norm": 0.18248595297336578,
      "learning_rate": 2.8260000000000004e-05,
      "loss": 0.0017,
      "step": 65220
    },
    {
      "epoch": 3.4789333333333334,
      "grad_norm": 0.20427843928337097,
      "learning_rate": 2.825666666666667e-05,
      "loss": 0.0024,
      "step": 65230
    },
    {
      "epoch": 3.4794666666666667,
      "grad_norm": 0.8280990719795227,
      "learning_rate": 2.8253333333333336e-05,
      "loss": 0.002,
      "step": 65240
    },
    {
      "epoch": 3.48,
      "grad_norm": 0.11628743261098862,
      "learning_rate": 2.825e-05,
      "loss": 0.0031,
      "step": 65250
    },
    {
      "epoch": 3.4805333333333333,
      "grad_norm": 0.5059484839439392,
      "learning_rate": 2.8246666666666665e-05,
      "loss": 0.002,
      "step": 65260
    },
    {
      "epoch": 3.4810666666666665,
      "grad_norm": 0.5535961985588074,
      "learning_rate": 2.824333333333333e-05,
      "loss": 0.0021,
      "step": 65270
    },
    {
      "epoch": 3.4816,
      "grad_norm": 0.07877066731452942,
      "learning_rate": 2.824e-05,
      "loss": 0.0019,
      "step": 65280
    },
    {
      "epoch": 3.4821333333333335,
      "grad_norm": 0.3110809028148651,
      "learning_rate": 2.8236666666666667e-05,
      "loss": 0.002,
      "step": 65290
    },
    {
      "epoch": 3.482666666666667,
      "grad_norm": 0.4243922531604767,
      "learning_rate": 2.8233333333333333e-05,
      "loss": 0.0022,
      "step": 65300
    },
    {
      "epoch": 3.4832,
      "grad_norm": 0.27319949865341187,
      "learning_rate": 2.8230000000000002e-05,
      "loss": 0.0026,
      "step": 65310
    },
    {
      "epoch": 3.4837333333333333,
      "grad_norm": 0.12395717948675156,
      "learning_rate": 2.822666666666667e-05,
      "loss": 0.0016,
      "step": 65320
    },
    {
      "epoch": 3.4842666666666666,
      "grad_norm": 0.6812770366668701,
      "learning_rate": 2.8223333333333335e-05,
      "loss": 0.0024,
      "step": 65330
    },
    {
      "epoch": 3.4848,
      "grad_norm": 0.6805756092071533,
      "learning_rate": 2.822e-05,
      "loss": 0.0022,
      "step": 65340
    },
    {
      "epoch": 3.485333333333333,
      "grad_norm": 0.08746448159217834,
      "learning_rate": 2.821666666666667e-05,
      "loss": 0.002,
      "step": 65350
    },
    {
      "epoch": 3.4858666666666664,
      "grad_norm": 0.14441634714603424,
      "learning_rate": 2.8213333333333337e-05,
      "loss": 0.002,
      "step": 65360
    },
    {
      "epoch": 3.4864,
      "grad_norm": 0.03558497503399849,
      "learning_rate": 2.8210000000000003e-05,
      "loss": 0.0037,
      "step": 65370
    },
    {
      "epoch": 3.4869333333333334,
      "grad_norm": 0.2859375476837158,
      "learning_rate": 2.820666666666667e-05,
      "loss": 0.0019,
      "step": 65380
    },
    {
      "epoch": 3.4874666666666667,
      "grad_norm": 0.46218201518058777,
      "learning_rate": 2.820333333333334e-05,
      "loss": 0.002,
      "step": 65390
    },
    {
      "epoch": 3.488,
      "grad_norm": 0.2140691876411438,
      "learning_rate": 2.8199999999999998e-05,
      "loss": 0.002,
      "step": 65400
    },
    {
      "epoch": 3.4885333333333333,
      "grad_norm": 0.8663532733917236,
      "learning_rate": 2.8196666666666667e-05,
      "loss": 0.0025,
      "step": 65410
    },
    {
      "epoch": 3.4890666666666665,
      "grad_norm": 0.06836852431297302,
      "learning_rate": 2.8193333333333333e-05,
      "loss": 0.0029,
      "step": 65420
    },
    {
      "epoch": 3.4896,
      "grad_norm": 0.1819610893726349,
      "learning_rate": 2.819e-05,
      "loss": 0.0018,
      "step": 65430
    },
    {
      "epoch": 3.4901333333333335,
      "grad_norm": 0.10211007297039032,
      "learning_rate": 2.8186666666666666e-05,
      "loss": 0.0022,
      "step": 65440
    },
    {
      "epoch": 3.490666666666667,
      "grad_norm": 0.33675429224967957,
      "learning_rate": 2.8183333333333335e-05,
      "loss": 0.0026,
      "step": 65450
    },
    {
      "epoch": 3.4912,
      "grad_norm": 0.03252014145255089,
      "learning_rate": 2.818e-05,
      "loss": 0.0022,
      "step": 65460
    },
    {
      "epoch": 3.4917333333333334,
      "grad_norm": 0.15897411108016968,
      "learning_rate": 2.8176666666666667e-05,
      "loss": 0.0019,
      "step": 65470
    },
    {
      "epoch": 3.4922666666666666,
      "grad_norm": 0.5431047677993774,
      "learning_rate": 2.8173333333333334e-05,
      "loss": 0.0023,
      "step": 65480
    },
    {
      "epoch": 3.4928,
      "grad_norm": 0.3964065909385681,
      "learning_rate": 2.8170000000000003e-05,
      "loss": 0.0017,
      "step": 65490
    },
    {
      "epoch": 3.493333333333333,
      "grad_norm": 0.04595223069190979,
      "learning_rate": 2.816666666666667e-05,
      "loss": 0.0026,
      "step": 65500
    },
    {
      "epoch": 3.4938666666666665,
      "grad_norm": 0.3759233355522156,
      "learning_rate": 2.8163333333333335e-05,
      "loss": 0.0019,
      "step": 65510
    },
    {
      "epoch": 3.4944,
      "grad_norm": 0.08634798228740692,
      "learning_rate": 2.816e-05,
      "loss": 0.0025,
      "step": 65520
    },
    {
      "epoch": 3.4949333333333334,
      "grad_norm": 0.10985767841339111,
      "learning_rate": 2.815666666666667e-05,
      "loss": 0.002,
      "step": 65530
    },
    {
      "epoch": 3.4954666666666667,
      "grad_norm": 0.09606285393238068,
      "learning_rate": 2.8153333333333337e-05,
      "loss": 0.0015,
      "step": 65540
    },
    {
      "epoch": 3.496,
      "grad_norm": 0.06501061469316483,
      "learning_rate": 2.815e-05,
      "loss": 0.0018,
      "step": 65550
    },
    {
      "epoch": 3.4965333333333333,
      "grad_norm": 0.8450220823287964,
      "learning_rate": 2.8146666666666666e-05,
      "loss": 0.0019,
      "step": 65560
    },
    {
      "epoch": 3.4970666666666665,
      "grad_norm": 0.16820356249809265,
      "learning_rate": 2.8143333333333332e-05,
      "loss": 0.0018,
      "step": 65570
    },
    {
      "epoch": 3.4976,
      "grad_norm": 0.6072800159454346,
      "learning_rate": 2.8139999999999998e-05,
      "loss": 0.002,
      "step": 65580
    },
    {
      "epoch": 3.4981333333333335,
      "grad_norm": 0.18993335962295532,
      "learning_rate": 2.8136666666666668e-05,
      "loss": 0.0014,
      "step": 65590
    },
    {
      "epoch": 3.498666666666667,
      "grad_norm": 0.27773305773735046,
      "learning_rate": 2.8133333333333334e-05,
      "loss": 0.0016,
      "step": 65600
    },
    {
      "epoch": 3.4992,
      "grad_norm": 0.5472631454467773,
      "learning_rate": 2.813e-05,
      "loss": 0.0023,
      "step": 65610
    },
    {
      "epoch": 3.4997333333333334,
      "grad_norm": 0.12342827767133713,
      "learning_rate": 2.8126666666666666e-05,
      "loss": 0.0012,
      "step": 65620
    },
    {
      "epoch": 3.5002666666666666,
      "grad_norm": 0.3914071321487427,
      "learning_rate": 2.8123333333333336e-05,
      "loss": 0.0026,
      "step": 65630
    },
    {
      "epoch": 3.5008,
      "grad_norm": 0.4744822084903717,
      "learning_rate": 2.8120000000000002e-05,
      "loss": 0.0022,
      "step": 65640
    },
    {
      "epoch": 3.501333333333333,
      "grad_norm": 0.0892593041062355,
      "learning_rate": 2.8116666666666668e-05,
      "loss": 0.0019,
      "step": 65650
    },
    {
      "epoch": 3.5018666666666665,
      "grad_norm": 0.5323475003242493,
      "learning_rate": 2.8113333333333337e-05,
      "loss": 0.0017,
      "step": 65660
    },
    {
      "epoch": 3.5023999999999997,
      "grad_norm": 0.2125980257987976,
      "learning_rate": 2.8110000000000004e-05,
      "loss": 0.0018,
      "step": 65670
    },
    {
      "epoch": 3.5029333333333335,
      "grad_norm": 0.28010377287864685,
      "learning_rate": 2.810666666666667e-05,
      "loss": 0.0017,
      "step": 65680
    },
    {
      "epoch": 3.5034666666666667,
      "grad_norm": 0.16894716024398804,
      "learning_rate": 2.8103333333333336e-05,
      "loss": 0.0016,
      "step": 65690
    },
    {
      "epoch": 3.504,
      "grad_norm": 0.24550554156303406,
      "learning_rate": 2.8100000000000005e-05,
      "loss": 0.002,
      "step": 65700
    },
    {
      "epoch": 3.5045333333333333,
      "grad_norm": 0.062468837946653366,
      "learning_rate": 2.8096666666666665e-05,
      "loss": 0.0023,
      "step": 65710
    },
    {
      "epoch": 3.5050666666666666,
      "grad_norm": 0.1843682825565338,
      "learning_rate": 2.8093333333333334e-05,
      "loss": 0.0027,
      "step": 65720
    },
    {
      "epoch": 3.5056000000000003,
      "grad_norm": 0.12353266775608063,
      "learning_rate": 2.809e-05,
      "loss": 0.0021,
      "step": 65730
    },
    {
      "epoch": 3.5061333333333335,
      "grad_norm": 0.32665178179740906,
      "learning_rate": 2.8086666666666667e-05,
      "loss": 0.0019,
      "step": 65740
    },
    {
      "epoch": 3.506666666666667,
      "grad_norm": 0.10052691400051117,
      "learning_rate": 2.8083333333333333e-05,
      "loss": 0.002,
      "step": 65750
    },
    {
      "epoch": 3.5072,
      "grad_norm": 0.35330867767333984,
      "learning_rate": 2.8080000000000002e-05,
      "loss": 0.0022,
      "step": 65760
    },
    {
      "epoch": 3.5077333333333334,
      "grad_norm": 0.1268995702266693,
      "learning_rate": 2.807666666666667e-05,
      "loss": 0.0014,
      "step": 65770
    },
    {
      "epoch": 3.5082666666666666,
      "grad_norm": 0.3031332194805145,
      "learning_rate": 2.8073333333333334e-05,
      "loss": 0.002,
      "step": 65780
    },
    {
      "epoch": 3.5088,
      "grad_norm": 0.16552618145942688,
      "learning_rate": 2.807e-05,
      "loss": 0.0019,
      "step": 65790
    },
    {
      "epoch": 3.509333333333333,
      "grad_norm": 0.18257127702236176,
      "learning_rate": 2.806666666666667e-05,
      "loss": 0.0018,
      "step": 65800
    },
    {
      "epoch": 3.5098666666666665,
      "grad_norm": 0.4601821005344391,
      "learning_rate": 2.8063333333333336e-05,
      "loss": 0.0021,
      "step": 65810
    },
    {
      "epoch": 3.5103999999999997,
      "grad_norm": 0.22130680084228516,
      "learning_rate": 2.8060000000000002e-05,
      "loss": 0.0023,
      "step": 65820
    },
    {
      "epoch": 3.5109333333333335,
      "grad_norm": 0.05055871978402138,
      "learning_rate": 2.805666666666667e-05,
      "loss": 0.0023,
      "step": 65830
    },
    {
      "epoch": 3.5114666666666667,
      "grad_norm": 0.37369027733802795,
      "learning_rate": 2.8053333333333338e-05,
      "loss": 0.0023,
      "step": 65840
    },
    {
      "epoch": 3.512,
      "grad_norm": 0.07182811200618744,
      "learning_rate": 2.8050000000000004e-05,
      "loss": 0.002,
      "step": 65850
    },
    {
      "epoch": 3.5125333333333333,
      "grad_norm": 0.12345100194215775,
      "learning_rate": 2.8046666666666667e-05,
      "loss": 0.0014,
      "step": 65860
    },
    {
      "epoch": 3.5130666666666666,
      "grad_norm": 0.21828733384609222,
      "learning_rate": 2.8043333333333333e-05,
      "loss": 0.0018,
      "step": 65870
    },
    {
      "epoch": 3.5136,
      "grad_norm": 0.10725836455821991,
      "learning_rate": 2.804e-05,
      "loss": 0.0019,
      "step": 65880
    },
    {
      "epoch": 3.5141333333333336,
      "grad_norm": 0.1509118676185608,
      "learning_rate": 2.8036666666666665e-05,
      "loss": 0.0019,
      "step": 65890
    },
    {
      "epoch": 3.514666666666667,
      "grad_norm": 0.3069218099117279,
      "learning_rate": 2.8033333333333335e-05,
      "loss": 0.0014,
      "step": 65900
    },
    {
      "epoch": 3.5152,
      "grad_norm": 0.16186518967151642,
      "learning_rate": 2.803e-05,
      "loss": 0.0016,
      "step": 65910
    },
    {
      "epoch": 3.5157333333333334,
      "grad_norm": 0.16818290948867798,
      "learning_rate": 2.8026666666666667e-05,
      "loss": 0.0023,
      "step": 65920
    },
    {
      "epoch": 3.5162666666666667,
      "grad_norm": 0.18332889676094055,
      "learning_rate": 2.8023333333333333e-05,
      "loss": 0.0019,
      "step": 65930
    },
    {
      "epoch": 3.5168,
      "grad_norm": 0.06493101269006729,
      "learning_rate": 2.8020000000000003e-05,
      "loss": 0.0028,
      "step": 65940
    },
    {
      "epoch": 3.517333333333333,
      "grad_norm": 0.3295668363571167,
      "learning_rate": 2.801666666666667e-05,
      "loss": 0.0016,
      "step": 65950
    },
    {
      "epoch": 3.5178666666666665,
      "grad_norm": 0.5191821455955505,
      "learning_rate": 2.8013333333333335e-05,
      "loss": 0.0029,
      "step": 65960
    },
    {
      "epoch": 3.5183999999999997,
      "grad_norm": 0.049332763999700546,
      "learning_rate": 2.8010000000000005e-05,
      "loss": 0.0015,
      "step": 65970
    },
    {
      "epoch": 3.5189333333333335,
      "grad_norm": 0.2463427633047104,
      "learning_rate": 2.800666666666667e-05,
      "loss": 0.0017,
      "step": 65980
    },
    {
      "epoch": 3.5194666666666667,
      "grad_norm": 0.32158058881759644,
      "learning_rate": 2.8003333333333337e-05,
      "loss": 0.0027,
      "step": 65990
    },
    {
      "epoch": 3.52,
      "grad_norm": 0.19451244175434113,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.0022,
      "step": 66000
    },
    {
      "epoch": 3.5205333333333333,
      "grad_norm": 0.26448872685432434,
      "learning_rate": 2.7996666666666666e-05,
      "loss": 0.0028,
      "step": 66010
    },
    {
      "epoch": 3.5210666666666666,
      "grad_norm": 0.18496578931808472,
      "learning_rate": 2.7993333333333332e-05,
      "loss": 0.0021,
      "step": 66020
    },
    {
      "epoch": 3.5216,
      "grad_norm": 0.22074958682060242,
      "learning_rate": 2.7989999999999998e-05,
      "loss": 0.0018,
      "step": 66030
    },
    {
      "epoch": 3.5221333333333336,
      "grad_norm": 0.24471625685691833,
      "learning_rate": 2.7986666666666668e-05,
      "loss": 0.0015,
      "step": 66040
    },
    {
      "epoch": 3.522666666666667,
      "grad_norm": 0.2916232943534851,
      "learning_rate": 2.7983333333333334e-05,
      "loss": 0.002,
      "step": 66050
    },
    {
      "epoch": 3.5232,
      "grad_norm": 0.3887365460395813,
      "learning_rate": 2.798e-05,
      "loss": 0.0016,
      "step": 66060
    },
    {
      "epoch": 3.5237333333333334,
      "grad_norm": 0.45249176025390625,
      "learning_rate": 2.797666666666667e-05,
      "loss": 0.0031,
      "step": 66070
    },
    {
      "epoch": 3.5242666666666667,
      "grad_norm": 0.5865463018417358,
      "learning_rate": 2.7973333333333335e-05,
      "loss": 0.0024,
      "step": 66080
    },
    {
      "epoch": 3.5248,
      "grad_norm": 0.4048360288143158,
      "learning_rate": 2.797e-05,
      "loss": 0.0028,
      "step": 66090
    },
    {
      "epoch": 3.525333333333333,
      "grad_norm": 0.588064432144165,
      "learning_rate": 2.7966666666666668e-05,
      "loss": 0.002,
      "step": 66100
    },
    {
      "epoch": 3.5258666666666665,
      "grad_norm": 0.3064439594745636,
      "learning_rate": 2.7963333333333337e-05,
      "loss": 0.0022,
      "step": 66110
    },
    {
      "epoch": 3.5263999999999998,
      "grad_norm": 0.18635575473308563,
      "learning_rate": 2.7960000000000003e-05,
      "loss": 0.0022,
      "step": 66120
    },
    {
      "epoch": 3.5269333333333335,
      "grad_norm": 0.6474094986915588,
      "learning_rate": 2.795666666666667e-05,
      "loss": 0.0015,
      "step": 66130
    },
    {
      "epoch": 3.5274666666666668,
      "grad_norm": 0.06933407485485077,
      "learning_rate": 2.7953333333333336e-05,
      "loss": 0.0026,
      "step": 66140
    },
    {
      "epoch": 3.528,
      "grad_norm": 0.10028308629989624,
      "learning_rate": 2.7950000000000005e-05,
      "loss": 0.0018,
      "step": 66150
    },
    {
      "epoch": 3.5285333333333333,
      "grad_norm": 0.22175830602645874,
      "learning_rate": 2.7946666666666664e-05,
      "loss": 0.002,
      "step": 66160
    },
    {
      "epoch": 3.5290666666666666,
      "grad_norm": 0.7219082117080688,
      "learning_rate": 2.7943333333333334e-05,
      "loss": 0.0024,
      "step": 66170
    },
    {
      "epoch": 3.5296,
      "grad_norm": 0.48670679330825806,
      "learning_rate": 2.794e-05,
      "loss": 0.0016,
      "step": 66180
    },
    {
      "epoch": 3.5301333333333336,
      "grad_norm": 0.10530378669500351,
      "learning_rate": 2.7936666666666666e-05,
      "loss": 0.0019,
      "step": 66190
    },
    {
      "epoch": 3.530666666666667,
      "grad_norm": 0.14704826474189758,
      "learning_rate": 2.7933333333333332e-05,
      "loss": 0.0027,
      "step": 66200
    },
    {
      "epoch": 3.5312,
      "grad_norm": 0.054144661873579025,
      "learning_rate": 2.7930000000000002e-05,
      "loss": 0.002,
      "step": 66210
    },
    {
      "epoch": 3.5317333333333334,
      "grad_norm": 0.5089629292488098,
      "learning_rate": 2.7926666666666668e-05,
      "loss": 0.0014,
      "step": 66220
    },
    {
      "epoch": 3.5322666666666667,
      "grad_norm": 0.07509172707796097,
      "learning_rate": 2.7923333333333334e-05,
      "loss": 0.0017,
      "step": 66230
    },
    {
      "epoch": 3.5328,
      "grad_norm": 0.05469084903597832,
      "learning_rate": 2.792e-05,
      "loss": 0.0018,
      "step": 66240
    },
    {
      "epoch": 3.533333333333333,
      "grad_norm": 0.3845418393611908,
      "learning_rate": 2.791666666666667e-05,
      "loss": 0.0028,
      "step": 66250
    },
    {
      "epoch": 3.5338666666666665,
      "grad_norm": 0.04724724590778351,
      "learning_rate": 2.7913333333333336e-05,
      "loss": 0.0019,
      "step": 66260
    },
    {
      "epoch": 3.5343999999999998,
      "grad_norm": 0.2798387110233307,
      "learning_rate": 2.7910000000000002e-05,
      "loss": 0.0016,
      "step": 66270
    },
    {
      "epoch": 3.5349333333333335,
      "grad_norm": 0.1253369301557541,
      "learning_rate": 2.7906666666666668e-05,
      "loss": 0.0017,
      "step": 66280
    },
    {
      "epoch": 3.5354666666666668,
      "grad_norm": 0.06136664003133774,
      "learning_rate": 2.7903333333333338e-05,
      "loss": 0.0022,
      "step": 66290
    },
    {
      "epoch": 3.536,
      "grad_norm": 0.49237778782844543,
      "learning_rate": 2.7900000000000004e-05,
      "loss": 0.0016,
      "step": 66300
    },
    {
      "epoch": 3.5365333333333333,
      "grad_norm": 0.18981239199638367,
      "learning_rate": 2.7896666666666667e-05,
      "loss": 0.0024,
      "step": 66310
    },
    {
      "epoch": 3.5370666666666666,
      "grad_norm": 0.48540952801704407,
      "learning_rate": 2.7893333333333333e-05,
      "loss": 0.0019,
      "step": 66320
    },
    {
      "epoch": 3.5376,
      "grad_norm": 0.19448906183242798,
      "learning_rate": 2.789e-05,
      "loss": 0.0027,
      "step": 66330
    },
    {
      "epoch": 3.5381333333333336,
      "grad_norm": 0.5973588228225708,
      "learning_rate": 2.7886666666666665e-05,
      "loss": 0.0021,
      "step": 66340
    },
    {
      "epoch": 3.538666666666667,
      "grad_norm": 0.5863747596740723,
      "learning_rate": 2.7883333333333335e-05,
      "loss": 0.0015,
      "step": 66350
    },
    {
      "epoch": 3.5392,
      "grad_norm": 0.046069320291280746,
      "learning_rate": 2.788e-05,
      "loss": 0.0014,
      "step": 66360
    },
    {
      "epoch": 3.5397333333333334,
      "grad_norm": 0.2983607351779938,
      "learning_rate": 2.7876666666666667e-05,
      "loss": 0.0018,
      "step": 66370
    },
    {
      "epoch": 3.5402666666666667,
      "grad_norm": 0.18179601430892944,
      "learning_rate": 2.7873333333333333e-05,
      "loss": 0.0018,
      "step": 66380
    },
    {
      "epoch": 3.5408,
      "grad_norm": 0.35094553232192993,
      "learning_rate": 2.7870000000000003e-05,
      "loss": 0.002,
      "step": 66390
    },
    {
      "epoch": 3.541333333333333,
      "grad_norm": 0.271746426820755,
      "learning_rate": 2.786666666666667e-05,
      "loss": 0.0014,
      "step": 66400
    },
    {
      "epoch": 3.5418666666666665,
      "grad_norm": 0.18347351253032684,
      "learning_rate": 2.7863333333333335e-05,
      "loss": 0.0022,
      "step": 66410
    },
    {
      "epoch": 3.5423999999999998,
      "grad_norm": 0.25662174820899963,
      "learning_rate": 2.7860000000000004e-05,
      "loss": 0.0026,
      "step": 66420
    },
    {
      "epoch": 3.5429333333333335,
      "grad_norm": 0.16100375354290009,
      "learning_rate": 2.785666666666667e-05,
      "loss": 0.0016,
      "step": 66430
    },
    {
      "epoch": 3.5434666666666668,
      "grad_norm": 0.15787282586097717,
      "learning_rate": 2.7853333333333337e-05,
      "loss": 0.0023,
      "step": 66440
    },
    {
      "epoch": 3.544,
      "grad_norm": 0.23320446908473969,
      "learning_rate": 2.7850000000000003e-05,
      "loss": 0.0024,
      "step": 66450
    },
    {
      "epoch": 3.5445333333333333,
      "grad_norm": 0.11836006492376328,
      "learning_rate": 2.7846666666666665e-05,
      "loss": 0.0019,
      "step": 66460
    },
    {
      "epoch": 3.5450666666666666,
      "grad_norm": 0.1805274933576584,
      "learning_rate": 2.784333333333333e-05,
      "loss": 0.0019,
      "step": 66470
    },
    {
      "epoch": 3.5456,
      "grad_norm": 0.2372940182685852,
      "learning_rate": 2.7839999999999998e-05,
      "loss": 0.0014,
      "step": 66480
    },
    {
      "epoch": 3.5461333333333336,
      "grad_norm": 0.07073220610618591,
      "learning_rate": 2.7836666666666667e-05,
      "loss": 0.0012,
      "step": 66490
    },
    {
      "epoch": 3.546666666666667,
      "grad_norm": 0.12496832758188248,
      "learning_rate": 2.7833333333333333e-05,
      "loss": 0.0018,
      "step": 66500
    },
    {
      "epoch": 3.5472,
      "grad_norm": 0.39758118987083435,
      "learning_rate": 2.783e-05,
      "loss": 0.0024,
      "step": 66510
    },
    {
      "epoch": 3.5477333333333334,
      "grad_norm": 0.49846428632736206,
      "learning_rate": 2.782666666666667e-05,
      "loss": 0.0019,
      "step": 66520
    },
    {
      "epoch": 3.5482666666666667,
      "grad_norm": 0.39195162057876587,
      "learning_rate": 2.7823333333333335e-05,
      "loss": 0.0027,
      "step": 66530
    },
    {
      "epoch": 3.5488,
      "grad_norm": 0.12178996205329895,
      "learning_rate": 2.782e-05,
      "loss": 0.0016,
      "step": 66540
    },
    {
      "epoch": 3.5493333333333332,
      "grad_norm": 0.07161199301481247,
      "learning_rate": 2.7816666666666667e-05,
      "loss": 0.0013,
      "step": 66550
    },
    {
      "epoch": 3.5498666666666665,
      "grad_norm": 0.2923877239227295,
      "learning_rate": 2.7813333333333337e-05,
      "loss": 0.0019,
      "step": 66560
    },
    {
      "epoch": 3.5504,
      "grad_norm": 0.32518792152404785,
      "learning_rate": 2.7810000000000003e-05,
      "loss": 0.0015,
      "step": 66570
    },
    {
      "epoch": 3.5509333333333335,
      "grad_norm": 0.0544479675590992,
      "learning_rate": 2.780666666666667e-05,
      "loss": 0.0029,
      "step": 66580
    },
    {
      "epoch": 3.5514666666666668,
      "grad_norm": 0.06855213642120361,
      "learning_rate": 2.7803333333333335e-05,
      "loss": 0.0029,
      "step": 66590
    },
    {
      "epoch": 3.552,
      "grad_norm": 0.6017451882362366,
      "learning_rate": 2.7800000000000005e-05,
      "loss": 0.002,
      "step": 66600
    },
    {
      "epoch": 3.5525333333333333,
      "grad_norm": 0.600311279296875,
      "learning_rate": 2.7796666666666664e-05,
      "loss": 0.0031,
      "step": 66610
    },
    {
      "epoch": 3.5530666666666666,
      "grad_norm": 0.17618213593959808,
      "learning_rate": 2.7793333333333334e-05,
      "loss": 0.0023,
      "step": 66620
    },
    {
      "epoch": 3.5536,
      "grad_norm": 0.18990075588226318,
      "learning_rate": 2.779e-05,
      "loss": 0.0026,
      "step": 66630
    },
    {
      "epoch": 3.5541333333333336,
      "grad_norm": 0.14922361075878143,
      "learning_rate": 2.7786666666666666e-05,
      "loss": 0.0021,
      "step": 66640
    },
    {
      "epoch": 3.554666666666667,
      "grad_norm": 0.4909486472606659,
      "learning_rate": 2.7783333333333332e-05,
      "loss": 0.0031,
      "step": 66650
    },
    {
      "epoch": 3.5552,
      "grad_norm": 0.04914601519703865,
      "learning_rate": 2.778e-05,
      "loss": 0.0019,
      "step": 66660
    },
    {
      "epoch": 3.5557333333333334,
      "grad_norm": 0.18159893155097961,
      "learning_rate": 2.7776666666666668e-05,
      "loss": 0.002,
      "step": 66670
    },
    {
      "epoch": 3.5562666666666667,
      "grad_norm": 0.1519797444343567,
      "learning_rate": 2.7773333333333334e-05,
      "loss": 0.0024,
      "step": 66680
    },
    {
      "epoch": 3.5568,
      "grad_norm": 0.08244390785694122,
      "learning_rate": 2.777e-05,
      "loss": 0.0021,
      "step": 66690
    },
    {
      "epoch": 3.5573333333333332,
      "grad_norm": 0.2160579413175583,
      "learning_rate": 2.776666666666667e-05,
      "loss": 0.0021,
      "step": 66700
    },
    {
      "epoch": 3.5578666666666665,
      "grad_norm": 0.30223819613456726,
      "learning_rate": 2.7763333333333336e-05,
      "loss": 0.0022,
      "step": 66710
    },
    {
      "epoch": 3.5584,
      "grad_norm": 0.46439677476882935,
      "learning_rate": 2.7760000000000002e-05,
      "loss": 0.002,
      "step": 66720
    },
    {
      "epoch": 3.558933333333333,
      "grad_norm": 0.08935704827308655,
      "learning_rate": 2.7756666666666668e-05,
      "loss": 0.003,
      "step": 66730
    },
    {
      "epoch": 3.559466666666667,
      "grad_norm": 0.4311414659023285,
      "learning_rate": 2.7753333333333338e-05,
      "loss": 0.0025,
      "step": 66740
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.258534699678421,
      "learning_rate": 2.7750000000000004e-05,
      "loss": 0.0017,
      "step": 66750
    },
    {
      "epoch": 3.5605333333333333,
      "grad_norm": 0.2165062576532364,
      "learning_rate": 2.7746666666666666e-05,
      "loss": 0.0019,
      "step": 66760
    },
    {
      "epoch": 3.5610666666666666,
      "grad_norm": 0.24596570432186127,
      "learning_rate": 2.7743333333333333e-05,
      "loss": 0.0027,
      "step": 66770
    },
    {
      "epoch": 3.5616,
      "grad_norm": 0.4802418053150177,
      "learning_rate": 2.774e-05,
      "loss": 0.0022,
      "step": 66780
    },
    {
      "epoch": 3.5621333333333336,
      "grad_norm": 0.19004447758197784,
      "learning_rate": 2.7736666666666665e-05,
      "loss": 0.0023,
      "step": 66790
    },
    {
      "epoch": 3.562666666666667,
      "grad_norm": 0.14934860169887543,
      "learning_rate": 2.7733333333333334e-05,
      "loss": 0.0023,
      "step": 66800
    },
    {
      "epoch": 3.5632,
      "grad_norm": 0.14033131301403046,
      "learning_rate": 2.773e-05,
      "loss": 0.003,
      "step": 66810
    },
    {
      "epoch": 3.5637333333333334,
      "grad_norm": 0.35693487524986267,
      "learning_rate": 2.7726666666666667e-05,
      "loss": 0.0025,
      "step": 66820
    },
    {
      "epoch": 3.5642666666666667,
      "grad_norm": 0.10125777870416641,
      "learning_rate": 2.7723333333333336e-05,
      "loss": 0.0025,
      "step": 66830
    },
    {
      "epoch": 3.5648,
      "grad_norm": 0.09244809299707413,
      "learning_rate": 2.7720000000000002e-05,
      "loss": 0.0021,
      "step": 66840
    },
    {
      "epoch": 3.5653333333333332,
      "grad_norm": 0.05670369789004326,
      "learning_rate": 2.771666666666667e-05,
      "loss": 0.0023,
      "step": 66850
    },
    {
      "epoch": 3.5658666666666665,
      "grad_norm": 0.09517581015825272,
      "learning_rate": 2.7713333333333335e-05,
      "loss": 0.0015,
      "step": 66860
    },
    {
      "epoch": 3.5664,
      "grad_norm": 0.12381008267402649,
      "learning_rate": 2.7710000000000004e-05,
      "loss": 0.0016,
      "step": 66870
    },
    {
      "epoch": 3.566933333333333,
      "grad_norm": 0.4590553045272827,
      "learning_rate": 2.770666666666667e-05,
      "loss": 0.0023,
      "step": 66880
    },
    {
      "epoch": 3.567466666666667,
      "grad_norm": 0.05519614741206169,
      "learning_rate": 2.7703333333333336e-05,
      "loss": 0.0019,
      "step": 66890
    },
    {
      "epoch": 3.568,
      "grad_norm": 0.6049659252166748,
      "learning_rate": 2.7700000000000002e-05,
      "loss": 0.0018,
      "step": 66900
    },
    {
      "epoch": 3.5685333333333333,
      "grad_norm": 0.7038334012031555,
      "learning_rate": 2.7696666666666672e-05,
      "loss": 0.0017,
      "step": 66910
    },
    {
      "epoch": 3.5690666666666666,
      "grad_norm": 0.43636229634284973,
      "learning_rate": 2.769333333333333e-05,
      "loss": 0.0014,
      "step": 66920
    },
    {
      "epoch": 3.5696,
      "grad_norm": 0.2818371057510376,
      "learning_rate": 2.769e-05,
      "loss": 0.0028,
      "step": 66930
    },
    {
      "epoch": 3.5701333333333336,
      "grad_norm": 0.16386450827121735,
      "learning_rate": 2.7686666666666667e-05,
      "loss": 0.0018,
      "step": 66940
    },
    {
      "epoch": 3.570666666666667,
      "grad_norm": 0.24564237892627716,
      "learning_rate": 2.7683333333333333e-05,
      "loss": 0.0018,
      "step": 66950
    },
    {
      "epoch": 3.5712,
      "grad_norm": 0.3050113916397095,
      "learning_rate": 2.768e-05,
      "loss": 0.002,
      "step": 66960
    },
    {
      "epoch": 3.5717333333333334,
      "grad_norm": 0.10821425914764404,
      "learning_rate": 2.767666666666667e-05,
      "loss": 0.0016,
      "step": 66970
    },
    {
      "epoch": 3.5722666666666667,
      "grad_norm": 0.2136458456516266,
      "learning_rate": 2.7673333333333335e-05,
      "loss": 0.0011,
      "step": 66980
    },
    {
      "epoch": 3.5728,
      "grad_norm": 0.5340465903282166,
      "learning_rate": 2.767e-05,
      "loss": 0.002,
      "step": 66990
    },
    {
      "epoch": 3.5733333333333333,
      "grad_norm": 0.1343197226524353,
      "learning_rate": 2.7666666666666667e-05,
      "loss": 0.002,
      "step": 67000
    },
    {
      "epoch": 3.5738666666666665,
      "grad_norm": 0.14791305363178253,
      "learning_rate": 2.7663333333333337e-05,
      "loss": 0.0024,
      "step": 67010
    },
    {
      "epoch": 3.5744,
      "grad_norm": 0.19984427094459534,
      "learning_rate": 2.7660000000000003e-05,
      "loss": 0.0019,
      "step": 67020
    },
    {
      "epoch": 3.574933333333333,
      "grad_norm": 0.02655654400587082,
      "learning_rate": 2.765666666666667e-05,
      "loss": 0.0012,
      "step": 67030
    },
    {
      "epoch": 3.575466666666667,
      "grad_norm": 0.3989659249782562,
      "learning_rate": 2.7653333333333335e-05,
      "loss": 0.0019,
      "step": 67040
    },
    {
      "epoch": 3.576,
      "grad_norm": 0.3077537417411804,
      "learning_rate": 2.7650000000000005e-05,
      "loss": 0.0018,
      "step": 67050
    },
    {
      "epoch": 3.5765333333333333,
      "grad_norm": 0.11117549240589142,
      "learning_rate": 2.764666666666667e-05,
      "loss": 0.0016,
      "step": 67060
    },
    {
      "epoch": 3.5770666666666666,
      "grad_norm": 0.18792200088500977,
      "learning_rate": 2.7643333333333334e-05,
      "loss": 0.0019,
      "step": 67070
    },
    {
      "epoch": 3.5776,
      "grad_norm": 0.0502789169549942,
      "learning_rate": 2.764e-05,
      "loss": 0.0031,
      "step": 67080
    },
    {
      "epoch": 3.5781333333333336,
      "grad_norm": 0.1628740131855011,
      "learning_rate": 2.7636666666666666e-05,
      "loss": 0.0015,
      "step": 67090
    },
    {
      "epoch": 3.578666666666667,
      "grad_norm": 0.6955792903900146,
      "learning_rate": 2.7633333333333332e-05,
      "loss": 0.0016,
      "step": 67100
    },
    {
      "epoch": 3.5792,
      "grad_norm": 0.6660277247428894,
      "learning_rate": 2.763e-05,
      "loss": 0.0024,
      "step": 67110
    },
    {
      "epoch": 3.5797333333333334,
      "grad_norm": 0.15178899466991425,
      "learning_rate": 2.7626666666666668e-05,
      "loss": 0.002,
      "step": 67120
    },
    {
      "epoch": 3.5802666666666667,
      "grad_norm": 0.2163628786802292,
      "learning_rate": 2.7623333333333334e-05,
      "loss": 0.0014,
      "step": 67130
    },
    {
      "epoch": 3.5808,
      "grad_norm": 0.4254632294178009,
      "learning_rate": 2.762e-05,
      "loss": 0.0018,
      "step": 67140
    },
    {
      "epoch": 3.5813333333333333,
      "grad_norm": 0.22062090039253235,
      "learning_rate": 2.761666666666667e-05,
      "loss": 0.0014,
      "step": 67150
    },
    {
      "epoch": 3.5818666666666665,
      "grad_norm": 0.4699205458164215,
      "learning_rate": 2.7613333333333335e-05,
      "loss": 0.0018,
      "step": 67160
    },
    {
      "epoch": 3.5824,
      "grad_norm": 0.1827491968870163,
      "learning_rate": 2.761e-05,
      "loss": 0.0021,
      "step": 67170
    },
    {
      "epoch": 3.582933333333333,
      "grad_norm": 0.23135113716125488,
      "learning_rate": 2.760666666666667e-05,
      "loss": 0.0014,
      "step": 67180
    },
    {
      "epoch": 3.583466666666667,
      "grad_norm": 0.09560137987136841,
      "learning_rate": 2.7603333333333337e-05,
      "loss": 0.0029,
      "step": 67190
    },
    {
      "epoch": 3.584,
      "grad_norm": 0.03537880629301071,
      "learning_rate": 2.7600000000000003e-05,
      "loss": 0.0019,
      "step": 67200
    },
    {
      "epoch": 3.5845333333333333,
      "grad_norm": 0.0730833187699318,
      "learning_rate": 2.759666666666667e-05,
      "loss": 0.0013,
      "step": 67210
    },
    {
      "epoch": 3.5850666666666666,
      "grad_norm": 0.3362308442592621,
      "learning_rate": 2.7593333333333332e-05,
      "loss": 0.0026,
      "step": 67220
    },
    {
      "epoch": 3.5856,
      "grad_norm": 0.4183923900127411,
      "learning_rate": 2.759e-05,
      "loss": 0.0019,
      "step": 67230
    },
    {
      "epoch": 3.586133333333333,
      "grad_norm": 0.04967457801103592,
      "learning_rate": 2.7586666666666665e-05,
      "loss": 0.0018,
      "step": 67240
    },
    {
      "epoch": 3.586666666666667,
      "grad_norm": 0.0938117727637291,
      "learning_rate": 2.7583333333333334e-05,
      "loss": 0.0014,
      "step": 67250
    },
    {
      "epoch": 3.5872,
      "grad_norm": 0.15753822028636932,
      "learning_rate": 2.758e-05,
      "loss": 0.0025,
      "step": 67260
    },
    {
      "epoch": 3.5877333333333334,
      "grad_norm": 0.08952704817056656,
      "learning_rate": 2.7576666666666666e-05,
      "loss": 0.0017,
      "step": 67270
    },
    {
      "epoch": 3.5882666666666667,
      "grad_norm": 0.7222671508789062,
      "learning_rate": 2.7573333333333336e-05,
      "loss": 0.0015,
      "step": 67280
    },
    {
      "epoch": 3.5888,
      "grad_norm": 0.717918872833252,
      "learning_rate": 2.7570000000000002e-05,
      "loss": 0.0015,
      "step": 67290
    },
    {
      "epoch": 3.5893333333333333,
      "grad_norm": 0.2158840447664261,
      "learning_rate": 2.7566666666666668e-05,
      "loss": 0.0018,
      "step": 67300
    },
    {
      "epoch": 3.5898666666666665,
      "grad_norm": 0.2689913511276245,
      "learning_rate": 2.7563333333333334e-05,
      "loss": 0.0016,
      "step": 67310
    },
    {
      "epoch": 3.5904,
      "grad_norm": 0.29125848412513733,
      "learning_rate": 2.7560000000000004e-05,
      "loss": 0.0014,
      "step": 67320
    },
    {
      "epoch": 3.590933333333333,
      "grad_norm": 0.3340650200843811,
      "learning_rate": 2.755666666666667e-05,
      "loss": 0.0025,
      "step": 67330
    },
    {
      "epoch": 3.591466666666667,
      "grad_norm": 0.6080120801925659,
      "learning_rate": 2.7553333333333336e-05,
      "loss": 0.0016,
      "step": 67340
    },
    {
      "epoch": 3.592,
      "grad_norm": 0.2455751895904541,
      "learning_rate": 2.7550000000000002e-05,
      "loss": 0.0019,
      "step": 67350
    },
    {
      "epoch": 3.5925333333333334,
      "grad_norm": 0.2977808117866516,
      "learning_rate": 2.7546666666666672e-05,
      "loss": 0.0018,
      "step": 67360
    },
    {
      "epoch": 3.5930666666666666,
      "grad_norm": 0.6143630146980286,
      "learning_rate": 2.754333333333333e-05,
      "loss": 0.0019,
      "step": 67370
    },
    {
      "epoch": 3.5936,
      "grad_norm": 0.33477771282196045,
      "learning_rate": 2.754e-05,
      "loss": 0.0023,
      "step": 67380
    },
    {
      "epoch": 3.594133333333333,
      "grad_norm": 0.18978148698806763,
      "learning_rate": 2.7536666666666667e-05,
      "loss": 0.0023,
      "step": 67390
    },
    {
      "epoch": 3.594666666666667,
      "grad_norm": 0.3026517629623413,
      "learning_rate": 2.7533333333333333e-05,
      "loss": 0.0019,
      "step": 67400
    },
    {
      "epoch": 3.5952,
      "grad_norm": 0.08801795542240143,
      "learning_rate": 2.753e-05,
      "loss": 0.002,
      "step": 67410
    },
    {
      "epoch": 3.5957333333333334,
      "grad_norm": 0.36713477969169617,
      "learning_rate": 2.752666666666667e-05,
      "loss": 0.0019,
      "step": 67420
    },
    {
      "epoch": 3.5962666666666667,
      "grad_norm": 0.07624244689941406,
      "learning_rate": 2.7523333333333335e-05,
      "loss": 0.0021,
      "step": 67430
    },
    {
      "epoch": 3.5968,
      "grad_norm": 0.22001230716705322,
      "learning_rate": 2.752e-05,
      "loss": 0.0026,
      "step": 67440
    },
    {
      "epoch": 3.5973333333333333,
      "grad_norm": 0.24904924631118774,
      "learning_rate": 2.7516666666666667e-05,
      "loss": 0.0014,
      "step": 67450
    },
    {
      "epoch": 3.5978666666666665,
      "grad_norm": 0.37082523107528687,
      "learning_rate": 2.7513333333333336e-05,
      "loss": 0.0016,
      "step": 67460
    },
    {
      "epoch": 3.5984,
      "grad_norm": 0.23346658051013947,
      "learning_rate": 2.7510000000000003e-05,
      "loss": 0.0016,
      "step": 67470
    },
    {
      "epoch": 3.598933333333333,
      "grad_norm": 0.3065383732318878,
      "learning_rate": 2.750666666666667e-05,
      "loss": 0.0023,
      "step": 67480
    },
    {
      "epoch": 3.599466666666667,
      "grad_norm": 0.4277927279472351,
      "learning_rate": 2.7503333333333335e-05,
      "loss": 0.0025,
      "step": 67490
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.30626335740089417,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 0.002,
      "step": 67500
    },
    {
      "epoch": 3.6005333333333334,
      "grad_norm": 0.21634618937969208,
      "learning_rate": 2.749666666666667e-05,
      "loss": 0.0028,
      "step": 67510
    },
    {
      "epoch": 3.6010666666666666,
      "grad_norm": 0.4634913504123688,
      "learning_rate": 2.7493333333333333e-05,
      "loss": 0.0035,
      "step": 67520
    },
    {
      "epoch": 3.6016,
      "grad_norm": 0.45996665954589844,
      "learning_rate": 2.749e-05,
      "loss": 0.0018,
      "step": 67530
    },
    {
      "epoch": 3.602133333333333,
      "grad_norm": 0.3151834011077881,
      "learning_rate": 2.7486666666666666e-05,
      "loss": 0.0015,
      "step": 67540
    },
    {
      "epoch": 3.602666666666667,
      "grad_norm": 0.05410239100456238,
      "learning_rate": 2.748333333333333e-05,
      "loss": 0.0015,
      "step": 67550
    },
    {
      "epoch": 3.6032,
      "grad_norm": 0.25028714537620544,
      "learning_rate": 2.748e-05,
      "loss": 0.002,
      "step": 67560
    },
    {
      "epoch": 3.6037333333333335,
      "grad_norm": 0.1302885115146637,
      "learning_rate": 2.7476666666666667e-05,
      "loss": 0.0017,
      "step": 67570
    },
    {
      "epoch": 3.6042666666666667,
      "grad_norm": 0.19773311913013458,
      "learning_rate": 2.7473333333333333e-05,
      "loss": 0.0017,
      "step": 67580
    },
    {
      "epoch": 3.6048,
      "grad_norm": 0.43988925218582153,
      "learning_rate": 2.7470000000000003e-05,
      "loss": 0.003,
      "step": 67590
    },
    {
      "epoch": 3.6053333333333333,
      "grad_norm": 0.22023959457874298,
      "learning_rate": 2.746666666666667e-05,
      "loss": 0.0016,
      "step": 67600
    },
    {
      "epoch": 3.6058666666666666,
      "grad_norm": 0.41992855072021484,
      "learning_rate": 2.7463333333333335e-05,
      "loss": 0.0022,
      "step": 67610
    },
    {
      "epoch": 3.6064,
      "grad_norm": 0.39939209818840027,
      "learning_rate": 2.746e-05,
      "loss": 0.0018,
      "step": 67620
    },
    {
      "epoch": 3.606933333333333,
      "grad_norm": 0.5586788654327393,
      "learning_rate": 2.745666666666667e-05,
      "loss": 0.0018,
      "step": 67630
    },
    {
      "epoch": 3.607466666666667,
      "grad_norm": 0.19730624556541443,
      "learning_rate": 2.7453333333333337e-05,
      "loss": 0.0018,
      "step": 67640
    },
    {
      "epoch": 3.608,
      "grad_norm": 0.6395372152328491,
      "learning_rate": 2.7450000000000003e-05,
      "loss": 0.003,
      "step": 67650
    },
    {
      "epoch": 3.6085333333333334,
      "grad_norm": 0.24694572389125824,
      "learning_rate": 2.744666666666667e-05,
      "loss": 0.0021,
      "step": 67660
    },
    {
      "epoch": 3.6090666666666666,
      "grad_norm": 0.5337024927139282,
      "learning_rate": 2.7443333333333332e-05,
      "loss": 0.0032,
      "step": 67670
    },
    {
      "epoch": 3.6096,
      "grad_norm": 0.06930765509605408,
      "learning_rate": 2.7439999999999998e-05,
      "loss": 0.0019,
      "step": 67680
    },
    {
      "epoch": 3.610133333333333,
      "grad_norm": 0.231410413980484,
      "learning_rate": 2.7436666666666668e-05,
      "loss": 0.0018,
      "step": 67690
    },
    {
      "epoch": 3.610666666666667,
      "grad_norm": 0.10357394069433212,
      "learning_rate": 2.7433333333333334e-05,
      "loss": 0.0022,
      "step": 67700
    },
    {
      "epoch": 3.6112,
      "grad_norm": 0.07877445966005325,
      "learning_rate": 2.743e-05,
      "loss": 0.0021,
      "step": 67710
    },
    {
      "epoch": 3.6117333333333335,
      "grad_norm": 0.6507529020309448,
      "learning_rate": 2.7426666666666666e-05,
      "loss": 0.0017,
      "step": 67720
    },
    {
      "epoch": 3.6122666666666667,
      "grad_norm": 0.7771192193031311,
      "learning_rate": 2.7423333333333336e-05,
      "loss": 0.0029,
      "step": 67730
    },
    {
      "epoch": 3.6128,
      "grad_norm": 0.3490159511566162,
      "learning_rate": 2.7420000000000002e-05,
      "loss": 0.0023,
      "step": 67740
    },
    {
      "epoch": 3.6133333333333333,
      "grad_norm": 0.40976089239120483,
      "learning_rate": 2.7416666666666668e-05,
      "loss": 0.0022,
      "step": 67750
    },
    {
      "epoch": 3.6138666666666666,
      "grad_norm": 0.3792553246021271,
      "learning_rate": 2.7413333333333334e-05,
      "loss": 0.002,
      "step": 67760
    },
    {
      "epoch": 3.6144,
      "grad_norm": 0.40724945068359375,
      "learning_rate": 2.7410000000000004e-05,
      "loss": 0.0018,
      "step": 67770
    },
    {
      "epoch": 3.614933333333333,
      "grad_norm": 0.27995020151138306,
      "learning_rate": 2.740666666666667e-05,
      "loss": 0.0025,
      "step": 67780
    },
    {
      "epoch": 3.615466666666667,
      "grad_norm": 0.04689428582787514,
      "learning_rate": 2.7403333333333336e-05,
      "loss": 0.0017,
      "step": 67790
    },
    {
      "epoch": 3.616,
      "grad_norm": 0.5808722376823425,
      "learning_rate": 2.7400000000000002e-05,
      "loss": 0.0026,
      "step": 67800
    },
    {
      "epoch": 3.6165333333333334,
      "grad_norm": 0.3596079647541046,
      "learning_rate": 2.739666666666667e-05,
      "loss": 0.0021,
      "step": 67810
    },
    {
      "epoch": 3.6170666666666667,
      "grad_norm": 0.1958238035440445,
      "learning_rate": 2.739333333333333e-05,
      "loss": 0.0022,
      "step": 67820
    },
    {
      "epoch": 3.6176,
      "grad_norm": 0.3521376848220825,
      "learning_rate": 2.739e-05,
      "loss": 0.0029,
      "step": 67830
    },
    {
      "epoch": 3.618133333333333,
      "grad_norm": 0.10711074620485306,
      "learning_rate": 2.7386666666666666e-05,
      "loss": 0.0017,
      "step": 67840
    },
    {
      "epoch": 3.618666666666667,
      "grad_norm": 0.054054904729127884,
      "learning_rate": 2.7383333333333333e-05,
      "loss": 0.0016,
      "step": 67850
    },
    {
      "epoch": 3.6192,
      "grad_norm": 0.3908018469810486,
      "learning_rate": 2.738e-05,
      "loss": 0.0019,
      "step": 67860
    },
    {
      "epoch": 3.6197333333333335,
      "grad_norm": 0.2476453185081482,
      "learning_rate": 2.7376666666666668e-05,
      "loss": 0.0021,
      "step": 67870
    },
    {
      "epoch": 3.6202666666666667,
      "grad_norm": 0.12238889932632446,
      "learning_rate": 2.7373333333333334e-05,
      "loss": 0.0028,
      "step": 67880
    },
    {
      "epoch": 3.6208,
      "grad_norm": 0.18146198987960815,
      "learning_rate": 2.737e-05,
      "loss": 0.002,
      "step": 67890
    },
    {
      "epoch": 3.6213333333333333,
      "grad_norm": 0.07488758116960526,
      "learning_rate": 2.7366666666666667e-05,
      "loss": 0.0015,
      "step": 67900
    },
    {
      "epoch": 3.6218666666666666,
      "grad_norm": 0.49396976828575134,
      "learning_rate": 2.7363333333333336e-05,
      "loss": 0.0016,
      "step": 67910
    },
    {
      "epoch": 3.6224,
      "grad_norm": 0.22623491287231445,
      "learning_rate": 2.7360000000000002e-05,
      "loss": 0.0016,
      "step": 67920
    },
    {
      "epoch": 3.622933333333333,
      "grad_norm": 0.1352609246969223,
      "learning_rate": 2.735666666666667e-05,
      "loss": 0.0019,
      "step": 67930
    },
    {
      "epoch": 3.6234666666666664,
      "grad_norm": 0.12615546584129333,
      "learning_rate": 2.7353333333333338e-05,
      "loss": 0.002,
      "step": 67940
    },
    {
      "epoch": 3.624,
      "grad_norm": 0.30465832352638245,
      "learning_rate": 2.7350000000000004e-05,
      "loss": 0.0015,
      "step": 67950
    },
    {
      "epoch": 3.6245333333333334,
      "grad_norm": 0.2067137062549591,
      "learning_rate": 2.734666666666667e-05,
      "loss": 0.002,
      "step": 67960
    },
    {
      "epoch": 3.6250666666666667,
      "grad_norm": 0.6420855522155762,
      "learning_rate": 2.7343333333333333e-05,
      "loss": 0.0016,
      "step": 67970
    },
    {
      "epoch": 3.6256,
      "grad_norm": 0.343047171831131,
      "learning_rate": 2.734e-05,
      "loss": 0.0014,
      "step": 67980
    },
    {
      "epoch": 3.626133333333333,
      "grad_norm": 0.6106363534927368,
      "learning_rate": 2.7336666666666665e-05,
      "loss": 0.0023,
      "step": 67990
    },
    {
      "epoch": 3.626666666666667,
      "grad_norm": 0.05154085531830788,
      "learning_rate": 2.733333333333333e-05,
      "loss": 0.0018,
      "step": 68000
    },
    {
      "epoch": 3.6272,
      "grad_norm": 0.23400653898715973,
      "learning_rate": 2.733e-05,
      "loss": 0.0017,
      "step": 68010
    },
    {
      "epoch": 3.6277333333333335,
      "grad_norm": 0.37853386998176575,
      "learning_rate": 2.7326666666666667e-05,
      "loss": 0.0025,
      "step": 68020
    },
    {
      "epoch": 3.6282666666666668,
      "grad_norm": 0.42534518241882324,
      "learning_rate": 2.7323333333333333e-05,
      "loss": 0.0015,
      "step": 68030
    },
    {
      "epoch": 3.6288,
      "grad_norm": 0.14361445605754852,
      "learning_rate": 2.7320000000000003e-05,
      "loss": 0.0015,
      "step": 68040
    },
    {
      "epoch": 3.6293333333333333,
      "grad_norm": 0.3439420461654663,
      "learning_rate": 2.731666666666667e-05,
      "loss": 0.0021,
      "step": 68050
    },
    {
      "epoch": 3.6298666666666666,
      "grad_norm": 0.517672061920166,
      "learning_rate": 2.7313333333333335e-05,
      "loss": 0.0017,
      "step": 68060
    },
    {
      "epoch": 3.6304,
      "grad_norm": 0.4016914963722229,
      "learning_rate": 2.731e-05,
      "loss": 0.0036,
      "step": 68070
    },
    {
      "epoch": 3.630933333333333,
      "grad_norm": 0.3203059434890747,
      "learning_rate": 2.730666666666667e-05,
      "loss": 0.0023,
      "step": 68080
    },
    {
      "epoch": 3.6314666666666664,
      "grad_norm": 0.8019082546234131,
      "learning_rate": 2.7303333333333337e-05,
      "loss": 0.0023,
      "step": 68090
    },
    {
      "epoch": 3.632,
      "grad_norm": 0.05298537760972977,
      "learning_rate": 2.7300000000000003e-05,
      "loss": 0.0016,
      "step": 68100
    },
    {
      "epoch": 3.6325333333333334,
      "grad_norm": 0.10257930308580399,
      "learning_rate": 2.729666666666667e-05,
      "loss": 0.0026,
      "step": 68110
    },
    {
      "epoch": 3.6330666666666667,
      "grad_norm": 0.15885420143604279,
      "learning_rate": 2.7293333333333332e-05,
      "loss": 0.0018,
      "step": 68120
    },
    {
      "epoch": 3.6336,
      "grad_norm": 0.5908697843551636,
      "learning_rate": 2.7289999999999998e-05,
      "loss": 0.0014,
      "step": 68130
    },
    {
      "epoch": 3.634133333333333,
      "grad_norm": 0.3228282034397125,
      "learning_rate": 2.7286666666666667e-05,
      "loss": 0.003,
      "step": 68140
    },
    {
      "epoch": 3.634666666666667,
      "grad_norm": 0.3135402798652649,
      "learning_rate": 2.7283333333333334e-05,
      "loss": 0.0013,
      "step": 68150
    },
    {
      "epoch": 3.6352,
      "grad_norm": 0.1745036095380783,
      "learning_rate": 2.728e-05,
      "loss": 0.0017,
      "step": 68160
    },
    {
      "epoch": 3.6357333333333335,
      "grad_norm": 0.3703031837940216,
      "learning_rate": 2.7276666666666666e-05,
      "loss": 0.0014,
      "step": 68170
    },
    {
      "epoch": 3.6362666666666668,
      "grad_norm": 0.13133373856544495,
      "learning_rate": 2.7273333333333335e-05,
      "loss": 0.0025,
      "step": 68180
    },
    {
      "epoch": 3.6368,
      "grad_norm": 0.4346809983253479,
      "learning_rate": 2.727e-05,
      "loss": 0.002,
      "step": 68190
    },
    {
      "epoch": 3.6373333333333333,
      "grad_norm": 0.5511459112167358,
      "learning_rate": 2.7266666666666668e-05,
      "loss": 0.0017,
      "step": 68200
    },
    {
      "epoch": 3.6378666666666666,
      "grad_norm": 0.17203041911125183,
      "learning_rate": 2.7263333333333334e-05,
      "loss": 0.0021,
      "step": 68210
    },
    {
      "epoch": 3.6384,
      "grad_norm": 0.458098828792572,
      "learning_rate": 2.7260000000000003e-05,
      "loss": 0.0024,
      "step": 68220
    },
    {
      "epoch": 3.638933333333333,
      "grad_norm": 0.5577214360237122,
      "learning_rate": 2.725666666666667e-05,
      "loss": 0.002,
      "step": 68230
    },
    {
      "epoch": 3.6394666666666664,
      "grad_norm": 0.15960276126861572,
      "learning_rate": 2.7253333333333336e-05,
      "loss": 0.0015,
      "step": 68240
    },
    {
      "epoch": 3.64,
      "grad_norm": 0.03675525262951851,
      "learning_rate": 2.725e-05,
      "loss": 0.0018,
      "step": 68250
    },
    {
      "epoch": 3.6405333333333334,
      "grad_norm": 0.0913914367556572,
      "learning_rate": 2.724666666666667e-05,
      "loss": 0.0019,
      "step": 68260
    },
    {
      "epoch": 3.6410666666666667,
      "grad_norm": 0.06409482657909393,
      "learning_rate": 2.7243333333333337e-05,
      "loss": 0.0019,
      "step": 68270
    },
    {
      "epoch": 3.6416,
      "grad_norm": 0.06499385833740234,
      "learning_rate": 2.724e-05,
      "loss": 0.0018,
      "step": 68280
    },
    {
      "epoch": 3.6421333333333332,
      "grad_norm": 0.5788322687149048,
      "learning_rate": 2.7236666666666666e-05,
      "loss": 0.0016,
      "step": 68290
    },
    {
      "epoch": 3.642666666666667,
      "grad_norm": 0.6775698661804199,
      "learning_rate": 2.7233333333333332e-05,
      "loss": 0.0019,
      "step": 68300
    },
    {
      "epoch": 3.6432,
      "grad_norm": 0.4082503616809845,
      "learning_rate": 2.723e-05,
      "loss": 0.0027,
      "step": 68310
    },
    {
      "epoch": 3.6437333333333335,
      "grad_norm": 0.09494560211896896,
      "learning_rate": 2.7226666666666668e-05,
      "loss": 0.0016,
      "step": 68320
    },
    {
      "epoch": 3.6442666666666668,
      "grad_norm": 0.6265217065811157,
      "learning_rate": 2.7223333333333334e-05,
      "loss": 0.002,
      "step": 68330
    },
    {
      "epoch": 3.6448,
      "grad_norm": 0.17910023033618927,
      "learning_rate": 2.722e-05,
      "loss": 0.0021,
      "step": 68340
    },
    {
      "epoch": 3.6453333333333333,
      "grad_norm": 0.09400081634521484,
      "learning_rate": 2.7216666666666666e-05,
      "loss": 0.0017,
      "step": 68350
    },
    {
      "epoch": 3.6458666666666666,
      "grad_norm": 0.31115952134132385,
      "learning_rate": 2.7213333333333336e-05,
      "loss": 0.0017,
      "step": 68360
    },
    {
      "epoch": 3.6464,
      "grad_norm": 0.440221905708313,
      "learning_rate": 2.7210000000000002e-05,
      "loss": 0.0014,
      "step": 68370
    },
    {
      "epoch": 3.646933333333333,
      "grad_norm": 0.1765512079000473,
      "learning_rate": 2.7206666666666668e-05,
      "loss": 0.0019,
      "step": 68380
    },
    {
      "epoch": 3.6474666666666664,
      "grad_norm": 0.8039218187332153,
      "learning_rate": 2.7203333333333338e-05,
      "loss": 0.0026,
      "step": 68390
    },
    {
      "epoch": 3.648,
      "grad_norm": 0.24784433841705322,
      "learning_rate": 2.7200000000000004e-05,
      "loss": 0.0016,
      "step": 68400
    },
    {
      "epoch": 3.6485333333333334,
      "grad_norm": 0.43219879269599915,
      "learning_rate": 2.719666666666667e-05,
      "loss": 0.0024,
      "step": 68410
    },
    {
      "epoch": 3.6490666666666667,
      "grad_norm": 0.11486111581325531,
      "learning_rate": 2.7193333333333336e-05,
      "loss": 0.0026,
      "step": 68420
    },
    {
      "epoch": 3.6496,
      "grad_norm": 0.4623588025569916,
      "learning_rate": 2.719e-05,
      "loss": 0.0021,
      "step": 68430
    },
    {
      "epoch": 3.6501333333333332,
      "grad_norm": 0.0952606052160263,
      "learning_rate": 2.7186666666666665e-05,
      "loss": 0.0027,
      "step": 68440
    },
    {
      "epoch": 3.6506666666666665,
      "grad_norm": 0.09710858017206192,
      "learning_rate": 2.7183333333333335e-05,
      "loss": 0.0027,
      "step": 68450
    },
    {
      "epoch": 3.6512000000000002,
      "grad_norm": 0.36820900440216064,
      "learning_rate": 2.718e-05,
      "loss": 0.0021,
      "step": 68460
    },
    {
      "epoch": 3.6517333333333335,
      "grad_norm": 0.23579835891723633,
      "learning_rate": 2.7176666666666667e-05,
      "loss": 0.0018,
      "step": 68470
    },
    {
      "epoch": 3.6522666666666668,
      "grad_norm": 0.10505940765142441,
      "learning_rate": 2.7173333333333333e-05,
      "loss": 0.0025,
      "step": 68480
    },
    {
      "epoch": 3.6528,
      "grad_norm": 0.10214939713478088,
      "learning_rate": 2.7170000000000002e-05,
      "loss": 0.0017,
      "step": 68490
    },
    {
      "epoch": 3.6533333333333333,
      "grad_norm": 0.7072027921676636,
      "learning_rate": 2.716666666666667e-05,
      "loss": 0.0025,
      "step": 68500
    },
    {
      "epoch": 3.6538666666666666,
      "grad_norm": 0.15252459049224854,
      "learning_rate": 2.7163333333333335e-05,
      "loss": 0.0024,
      "step": 68510
    },
    {
      "epoch": 3.6544,
      "grad_norm": 0.10403265804052353,
      "learning_rate": 2.716e-05,
      "loss": 0.002,
      "step": 68520
    },
    {
      "epoch": 3.654933333333333,
      "grad_norm": 0.2173173576593399,
      "learning_rate": 2.715666666666667e-05,
      "loss": 0.003,
      "step": 68530
    },
    {
      "epoch": 3.6554666666666664,
      "grad_norm": 0.29027995467185974,
      "learning_rate": 2.7153333333333337e-05,
      "loss": 0.0022,
      "step": 68540
    },
    {
      "epoch": 3.656,
      "grad_norm": 0.18083475530147552,
      "learning_rate": 2.7150000000000003e-05,
      "loss": 0.0028,
      "step": 68550
    },
    {
      "epoch": 3.6565333333333334,
      "grad_norm": 0.20693419873714447,
      "learning_rate": 2.714666666666667e-05,
      "loss": 0.002,
      "step": 68560
    },
    {
      "epoch": 3.6570666666666667,
      "grad_norm": 0.39485228061676025,
      "learning_rate": 2.7143333333333338e-05,
      "loss": 0.0038,
      "step": 68570
    },
    {
      "epoch": 3.6576,
      "grad_norm": 0.07778863608837128,
      "learning_rate": 2.7139999999999998e-05,
      "loss": 0.0014,
      "step": 68580
    },
    {
      "epoch": 3.6581333333333332,
      "grad_norm": 0.3026336431503296,
      "learning_rate": 2.7136666666666667e-05,
      "loss": 0.0021,
      "step": 68590
    },
    {
      "epoch": 3.6586666666666665,
      "grad_norm": 0.19419467449188232,
      "learning_rate": 2.7133333333333333e-05,
      "loss": 0.0019,
      "step": 68600
    },
    {
      "epoch": 3.6592000000000002,
      "grad_norm": 0.6110245585441589,
      "learning_rate": 2.713e-05,
      "loss": 0.0016,
      "step": 68610
    },
    {
      "epoch": 3.6597333333333335,
      "grad_norm": 0.46941953897476196,
      "learning_rate": 2.7126666666666666e-05,
      "loss": 0.0022,
      "step": 68620
    },
    {
      "epoch": 3.660266666666667,
      "grad_norm": 0.09831754863262177,
      "learning_rate": 2.7123333333333335e-05,
      "loss": 0.0019,
      "step": 68630
    },
    {
      "epoch": 3.6608,
      "grad_norm": 0.15743166208267212,
      "learning_rate": 2.712e-05,
      "loss": 0.0023,
      "step": 68640
    },
    {
      "epoch": 3.6613333333333333,
      "grad_norm": 0.0771210566163063,
      "learning_rate": 2.7116666666666667e-05,
      "loss": 0.0033,
      "step": 68650
    },
    {
      "epoch": 3.6618666666666666,
      "grad_norm": 0.40789729356765747,
      "learning_rate": 2.7113333333333333e-05,
      "loss": 0.002,
      "step": 68660
    },
    {
      "epoch": 3.6624,
      "grad_norm": 0.09614310413599014,
      "learning_rate": 2.7110000000000003e-05,
      "loss": 0.0025,
      "step": 68670
    },
    {
      "epoch": 3.662933333333333,
      "grad_norm": 0.1914311945438385,
      "learning_rate": 2.710666666666667e-05,
      "loss": 0.002,
      "step": 68680
    },
    {
      "epoch": 3.6634666666666664,
      "grad_norm": 0.22639939188957214,
      "learning_rate": 2.7103333333333335e-05,
      "loss": 0.0016,
      "step": 68690
    },
    {
      "epoch": 3.664,
      "grad_norm": 0.2746144235134125,
      "learning_rate": 2.7100000000000005e-05,
      "loss": 0.0017,
      "step": 68700
    },
    {
      "epoch": 3.6645333333333334,
      "grad_norm": 0.40635594725608826,
      "learning_rate": 2.709666666666667e-05,
      "loss": 0.0026,
      "step": 68710
    },
    {
      "epoch": 3.6650666666666667,
      "grad_norm": 0.7536097764968872,
      "learning_rate": 2.7093333333333337e-05,
      "loss": 0.0015,
      "step": 68720
    },
    {
      "epoch": 3.6656,
      "grad_norm": 0.28514862060546875,
      "learning_rate": 2.709e-05,
      "loss": 0.0011,
      "step": 68730
    },
    {
      "epoch": 3.6661333333333332,
      "grad_norm": 0.15620270371437073,
      "learning_rate": 2.7086666666666666e-05,
      "loss": 0.0022,
      "step": 68740
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 0.33220404386520386,
      "learning_rate": 2.7083333333333332e-05,
      "loss": 0.0023,
      "step": 68750
    },
    {
      "epoch": 3.6672000000000002,
      "grad_norm": 0.6257303953170776,
      "learning_rate": 2.7079999999999998e-05,
      "loss": 0.0016,
      "step": 68760
    },
    {
      "epoch": 3.6677333333333335,
      "grad_norm": 0.41677412390708923,
      "learning_rate": 2.7076666666666668e-05,
      "loss": 0.002,
      "step": 68770
    },
    {
      "epoch": 3.668266666666667,
      "grad_norm": 0.14112909138202667,
      "learning_rate": 2.7073333333333334e-05,
      "loss": 0.0026,
      "step": 68780
    },
    {
      "epoch": 3.6688,
      "grad_norm": 0.10194713622331619,
      "learning_rate": 2.707e-05,
      "loss": 0.0028,
      "step": 68790
    },
    {
      "epoch": 3.6693333333333333,
      "grad_norm": 0.16249020397663116,
      "learning_rate": 2.706666666666667e-05,
      "loss": 0.0018,
      "step": 68800
    },
    {
      "epoch": 3.6698666666666666,
      "grad_norm": 0.4301467537879944,
      "learning_rate": 2.7063333333333336e-05,
      "loss": 0.002,
      "step": 68810
    },
    {
      "epoch": 3.6704,
      "grad_norm": 0.07120528817176819,
      "learning_rate": 2.7060000000000002e-05,
      "loss": 0.0013,
      "step": 68820
    },
    {
      "epoch": 3.670933333333333,
      "grad_norm": 0.21466484665870667,
      "learning_rate": 2.7056666666666668e-05,
      "loss": 0.0018,
      "step": 68830
    },
    {
      "epoch": 3.6714666666666664,
      "grad_norm": 0.1840752512216568,
      "learning_rate": 2.7053333333333337e-05,
      "loss": 0.0018,
      "step": 68840
    },
    {
      "epoch": 3.672,
      "grad_norm": 0.21349391341209412,
      "learning_rate": 2.7050000000000004e-05,
      "loss": 0.0016,
      "step": 68850
    },
    {
      "epoch": 3.6725333333333334,
      "grad_norm": 0.13470865786075592,
      "learning_rate": 2.704666666666667e-05,
      "loss": 0.0027,
      "step": 68860
    },
    {
      "epoch": 3.6730666666666667,
      "grad_norm": 0.15718278288841248,
      "learning_rate": 2.7043333333333336e-05,
      "loss": 0.0024,
      "step": 68870
    },
    {
      "epoch": 3.6736,
      "grad_norm": 0.10848156362771988,
      "learning_rate": 2.704e-05,
      "loss": 0.0019,
      "step": 68880
    },
    {
      "epoch": 3.6741333333333333,
      "grad_norm": 0.04913216084241867,
      "learning_rate": 2.7036666666666665e-05,
      "loss": 0.0018,
      "step": 68890
    },
    {
      "epoch": 3.6746666666666665,
      "grad_norm": 0.16874109208583832,
      "learning_rate": 2.7033333333333334e-05,
      "loss": 0.0017,
      "step": 68900
    },
    {
      "epoch": 3.6752000000000002,
      "grad_norm": 0.24727362394332886,
      "learning_rate": 2.703e-05,
      "loss": 0.0024,
      "step": 68910
    },
    {
      "epoch": 3.6757333333333335,
      "grad_norm": 0.14973032474517822,
      "learning_rate": 2.7026666666666667e-05,
      "loss": 0.0017,
      "step": 68920
    },
    {
      "epoch": 3.676266666666667,
      "grad_norm": 0.1217338889837265,
      "learning_rate": 2.7023333333333333e-05,
      "loss": 0.0013,
      "step": 68930
    },
    {
      "epoch": 3.6768,
      "grad_norm": 0.252691388130188,
      "learning_rate": 2.7020000000000002e-05,
      "loss": 0.003,
      "step": 68940
    },
    {
      "epoch": 3.6773333333333333,
      "grad_norm": 0.23502561450004578,
      "learning_rate": 2.701666666666667e-05,
      "loss": 0.0018,
      "step": 68950
    },
    {
      "epoch": 3.6778666666666666,
      "grad_norm": 0.4045865833759308,
      "learning_rate": 2.7013333333333334e-05,
      "loss": 0.0027,
      "step": 68960
    },
    {
      "epoch": 3.6784,
      "grad_norm": 0.21692265570163727,
      "learning_rate": 2.701e-05,
      "loss": 0.0016,
      "step": 68970
    },
    {
      "epoch": 3.678933333333333,
      "grad_norm": 0.12912477552890778,
      "learning_rate": 2.700666666666667e-05,
      "loss": 0.002,
      "step": 68980
    },
    {
      "epoch": 3.6794666666666664,
      "grad_norm": 0.3153664171695709,
      "learning_rate": 2.7003333333333336e-05,
      "loss": 0.0016,
      "step": 68990
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.21843117475509644,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.0017,
      "step": 69000
    },
    {
      "epoch": 3.6805333333333334,
      "grad_norm": 0.412184476852417,
      "learning_rate": 2.699666666666667e-05,
      "loss": 0.0015,
      "step": 69010
    },
    {
      "epoch": 3.6810666666666667,
      "grad_norm": 0.2739752531051636,
      "learning_rate": 2.6993333333333338e-05,
      "loss": 0.0023,
      "step": 69020
    },
    {
      "epoch": 3.6816,
      "grad_norm": 0.041447609663009644,
      "learning_rate": 2.6989999999999997e-05,
      "loss": 0.002,
      "step": 69030
    },
    {
      "epoch": 3.6821333333333333,
      "grad_norm": 0.30754444003105164,
      "learning_rate": 2.6986666666666667e-05,
      "loss": 0.0019,
      "step": 69040
    },
    {
      "epoch": 3.6826666666666665,
      "grad_norm": 0.20474053919315338,
      "learning_rate": 2.6983333333333333e-05,
      "loss": 0.0021,
      "step": 69050
    },
    {
      "epoch": 3.6832000000000003,
      "grad_norm": 0.1091138944029808,
      "learning_rate": 2.698e-05,
      "loss": 0.0022,
      "step": 69060
    },
    {
      "epoch": 3.6837333333333335,
      "grad_norm": 0.27740374207496643,
      "learning_rate": 2.6976666666666665e-05,
      "loss": 0.002,
      "step": 69070
    },
    {
      "epoch": 3.684266666666667,
      "grad_norm": 0.45602139830589294,
      "learning_rate": 2.6973333333333335e-05,
      "loss": 0.0023,
      "step": 69080
    },
    {
      "epoch": 3.6848,
      "grad_norm": 0.052404649555683136,
      "learning_rate": 2.697e-05,
      "loss": 0.0015,
      "step": 69090
    },
    {
      "epoch": 3.6853333333333333,
      "grad_norm": 0.15720629692077637,
      "learning_rate": 2.6966666666666667e-05,
      "loss": 0.003,
      "step": 69100
    },
    {
      "epoch": 3.6858666666666666,
      "grad_norm": 0.1642170399427414,
      "learning_rate": 2.6963333333333333e-05,
      "loss": 0.0016,
      "step": 69110
    },
    {
      "epoch": 3.6864,
      "grad_norm": 0.47574877738952637,
      "learning_rate": 2.6960000000000003e-05,
      "loss": 0.0027,
      "step": 69120
    },
    {
      "epoch": 3.686933333333333,
      "grad_norm": 0.31232285499572754,
      "learning_rate": 2.695666666666667e-05,
      "loss": 0.0037,
      "step": 69130
    },
    {
      "epoch": 3.6874666666666664,
      "grad_norm": 0.32511767745018005,
      "learning_rate": 2.6953333333333335e-05,
      "loss": 0.0021,
      "step": 69140
    },
    {
      "epoch": 3.6879999999999997,
      "grad_norm": 0.13928039371967316,
      "learning_rate": 2.6950000000000005e-05,
      "loss": 0.0014,
      "step": 69150
    },
    {
      "epoch": 3.6885333333333334,
      "grad_norm": 0.032704465091228485,
      "learning_rate": 2.694666666666667e-05,
      "loss": 0.0022,
      "step": 69160
    },
    {
      "epoch": 3.6890666666666667,
      "grad_norm": 0.5188413262367249,
      "learning_rate": 2.6943333333333337e-05,
      "loss": 0.0027,
      "step": 69170
    },
    {
      "epoch": 3.6896,
      "grad_norm": 0.1692112386226654,
      "learning_rate": 2.694e-05,
      "loss": 0.0018,
      "step": 69180
    },
    {
      "epoch": 3.6901333333333333,
      "grad_norm": 0.25680139660835266,
      "learning_rate": 2.6936666666666666e-05,
      "loss": 0.0022,
      "step": 69190
    },
    {
      "epoch": 3.6906666666666665,
      "grad_norm": 0.29179272055625916,
      "learning_rate": 2.6933333333333332e-05,
      "loss": 0.0025,
      "step": 69200
    },
    {
      "epoch": 3.6912000000000003,
      "grad_norm": 0.1286700814962387,
      "learning_rate": 2.693e-05,
      "loss": 0.0018,
      "step": 69210
    },
    {
      "epoch": 3.6917333333333335,
      "grad_norm": 0.3915548026561737,
      "learning_rate": 2.6926666666666667e-05,
      "loss": 0.0022,
      "step": 69220
    },
    {
      "epoch": 3.692266666666667,
      "grad_norm": 0.42011570930480957,
      "learning_rate": 2.6923333333333334e-05,
      "loss": 0.0022,
      "step": 69230
    },
    {
      "epoch": 3.6928,
      "grad_norm": 0.3844933807849884,
      "learning_rate": 2.692e-05,
      "loss": 0.002,
      "step": 69240
    },
    {
      "epoch": 3.6933333333333334,
      "grad_norm": 0.2210436314344406,
      "learning_rate": 2.691666666666667e-05,
      "loss": 0.0014,
      "step": 69250
    },
    {
      "epoch": 3.6938666666666666,
      "grad_norm": 0.18990486860275269,
      "learning_rate": 2.6913333333333335e-05,
      "loss": 0.0016,
      "step": 69260
    },
    {
      "epoch": 3.6944,
      "grad_norm": 0.2123173177242279,
      "learning_rate": 2.691e-05,
      "loss": 0.0022,
      "step": 69270
    },
    {
      "epoch": 3.694933333333333,
      "grad_norm": 0.4486854374408722,
      "learning_rate": 2.6906666666666668e-05,
      "loss": 0.0026,
      "step": 69280
    },
    {
      "epoch": 3.6954666666666665,
      "grad_norm": 0.31389185786247253,
      "learning_rate": 2.6903333333333337e-05,
      "loss": 0.0026,
      "step": 69290
    },
    {
      "epoch": 3.6959999999999997,
      "grad_norm": 0.18041977286338806,
      "learning_rate": 2.6900000000000003e-05,
      "loss": 0.0022,
      "step": 69300
    },
    {
      "epoch": 3.6965333333333334,
      "grad_norm": 0.2059115767478943,
      "learning_rate": 2.689666666666667e-05,
      "loss": 0.0018,
      "step": 69310
    },
    {
      "epoch": 3.6970666666666667,
      "grad_norm": 0.32293951511383057,
      "learning_rate": 2.6893333333333336e-05,
      "loss": 0.003,
      "step": 69320
    },
    {
      "epoch": 3.6976,
      "grad_norm": 0.3112178444862366,
      "learning_rate": 2.689e-05,
      "loss": 0.0021,
      "step": 69330
    },
    {
      "epoch": 3.6981333333333333,
      "grad_norm": 0.27693456411361694,
      "learning_rate": 2.6886666666666664e-05,
      "loss": 0.0015,
      "step": 69340
    },
    {
      "epoch": 3.6986666666666665,
      "grad_norm": 0.2099011391401291,
      "learning_rate": 2.6883333333333334e-05,
      "loss": 0.0031,
      "step": 69350
    },
    {
      "epoch": 3.6992000000000003,
      "grad_norm": 0.5472267270088196,
      "learning_rate": 2.688e-05,
      "loss": 0.0018,
      "step": 69360
    },
    {
      "epoch": 3.6997333333333335,
      "grad_norm": 0.40223968029022217,
      "learning_rate": 2.6876666666666666e-05,
      "loss": 0.0015,
      "step": 69370
    },
    {
      "epoch": 3.700266666666667,
      "grad_norm": 0.411395400762558,
      "learning_rate": 2.6873333333333332e-05,
      "loss": 0.0021,
      "step": 69380
    },
    {
      "epoch": 3.7008,
      "grad_norm": 0.4975278079509735,
      "learning_rate": 2.6870000000000002e-05,
      "loss": 0.0035,
      "step": 69390
    },
    {
      "epoch": 3.7013333333333334,
      "grad_norm": 0.32135888934135437,
      "learning_rate": 2.6866666666666668e-05,
      "loss": 0.0035,
      "step": 69400
    },
    {
      "epoch": 3.7018666666666666,
      "grad_norm": 0.18849678337574005,
      "learning_rate": 2.6863333333333334e-05,
      "loss": 0.003,
      "step": 69410
    },
    {
      "epoch": 3.7024,
      "grad_norm": 0.266209214925766,
      "learning_rate": 2.686e-05,
      "loss": 0.0012,
      "step": 69420
    },
    {
      "epoch": 3.702933333333333,
      "grad_norm": 0.474247008562088,
      "learning_rate": 2.685666666666667e-05,
      "loss": 0.0022,
      "step": 69430
    },
    {
      "epoch": 3.7034666666666665,
      "grad_norm": 0.30624857544898987,
      "learning_rate": 2.6853333333333336e-05,
      "loss": 0.0016,
      "step": 69440
    },
    {
      "epoch": 3.7039999999999997,
      "grad_norm": 0.06233500689268112,
      "learning_rate": 2.6850000000000002e-05,
      "loss": 0.003,
      "step": 69450
    },
    {
      "epoch": 3.7045333333333335,
      "grad_norm": 0.4849315583705902,
      "learning_rate": 2.684666666666667e-05,
      "loss": 0.0015,
      "step": 69460
    },
    {
      "epoch": 3.7050666666666667,
      "grad_norm": 0.13503317534923553,
      "learning_rate": 2.6843333333333338e-05,
      "loss": 0.0025,
      "step": 69470
    },
    {
      "epoch": 3.7056,
      "grad_norm": 0.1727338284254074,
      "learning_rate": 2.6840000000000004e-05,
      "loss": 0.0019,
      "step": 69480
    },
    {
      "epoch": 3.7061333333333333,
      "grad_norm": 0.3046344816684723,
      "learning_rate": 2.6836666666666667e-05,
      "loss": 0.0021,
      "step": 69490
    },
    {
      "epoch": 3.7066666666666666,
      "grad_norm": 0.19777068495750427,
      "learning_rate": 2.6833333333333333e-05,
      "loss": 0.0019,
      "step": 69500
    },
    {
      "epoch": 3.7072000000000003,
      "grad_norm": 0.18577969074249268,
      "learning_rate": 2.683e-05,
      "loss": 0.0016,
      "step": 69510
    },
    {
      "epoch": 3.7077333333333335,
      "grad_norm": 0.14185123145580292,
      "learning_rate": 2.6826666666666665e-05,
      "loss": 0.0028,
      "step": 69520
    },
    {
      "epoch": 3.708266666666667,
      "grad_norm": 0.2848234176635742,
      "learning_rate": 2.6823333333333335e-05,
      "loss": 0.0027,
      "step": 69530
    },
    {
      "epoch": 3.7088,
      "grad_norm": 0.1591903418302536,
      "learning_rate": 2.682e-05,
      "loss": 0.0017,
      "step": 69540
    },
    {
      "epoch": 3.7093333333333334,
      "grad_norm": 0.15015071630477905,
      "learning_rate": 2.6816666666666667e-05,
      "loss": 0.0021,
      "step": 69550
    },
    {
      "epoch": 3.7098666666666666,
      "grad_norm": 0.21107079088687897,
      "learning_rate": 2.6813333333333336e-05,
      "loss": 0.002,
      "step": 69560
    },
    {
      "epoch": 3.7104,
      "grad_norm": 0.055256448686122894,
      "learning_rate": 2.6810000000000003e-05,
      "loss": 0.0019,
      "step": 69570
    },
    {
      "epoch": 3.710933333333333,
      "grad_norm": 0.24781689047813416,
      "learning_rate": 2.680666666666667e-05,
      "loss": 0.0028,
      "step": 69580
    },
    {
      "epoch": 3.7114666666666665,
      "grad_norm": 0.3948380947113037,
      "learning_rate": 2.6803333333333335e-05,
      "loss": 0.0024,
      "step": 69590
    },
    {
      "epoch": 3.7119999999999997,
      "grad_norm": 0.22222338616847992,
      "learning_rate": 2.6800000000000004e-05,
      "loss": 0.0023,
      "step": 69600
    },
    {
      "epoch": 3.7125333333333335,
      "grad_norm": 0.16528645157814026,
      "learning_rate": 2.679666666666667e-05,
      "loss": 0.0016,
      "step": 69610
    },
    {
      "epoch": 3.7130666666666667,
      "grad_norm": 0.07877788692712784,
      "learning_rate": 2.6793333333333337e-05,
      "loss": 0.0018,
      "step": 69620
    },
    {
      "epoch": 3.7136,
      "grad_norm": 0.27562645077705383,
      "learning_rate": 2.6790000000000003e-05,
      "loss": 0.0018,
      "step": 69630
    },
    {
      "epoch": 3.7141333333333333,
      "grad_norm": 0.5562787652015686,
      "learning_rate": 2.6786666666666665e-05,
      "loss": 0.0027,
      "step": 69640
    },
    {
      "epoch": 3.7146666666666666,
      "grad_norm": 0.5250059366226196,
      "learning_rate": 2.678333333333333e-05,
      "loss": 0.0021,
      "step": 69650
    },
    {
      "epoch": 3.7152,
      "grad_norm": 0.08879081159830093,
      "learning_rate": 2.678e-05,
      "loss": 0.0026,
      "step": 69660
    },
    {
      "epoch": 3.7157333333333336,
      "grad_norm": 0.13132011890411377,
      "learning_rate": 2.6776666666666667e-05,
      "loss": 0.0017,
      "step": 69670
    },
    {
      "epoch": 3.716266666666667,
      "grad_norm": 0.24901069700717926,
      "learning_rate": 2.6773333333333333e-05,
      "loss": 0.0019,
      "step": 69680
    },
    {
      "epoch": 3.7168,
      "grad_norm": 0.2917031943798065,
      "learning_rate": 2.677e-05,
      "loss": 0.0016,
      "step": 69690
    },
    {
      "epoch": 3.7173333333333334,
      "grad_norm": 0.4347941279411316,
      "learning_rate": 2.676666666666667e-05,
      "loss": 0.0027,
      "step": 69700
    },
    {
      "epoch": 3.7178666666666667,
      "grad_norm": 0.30253106355667114,
      "learning_rate": 2.6763333333333335e-05,
      "loss": 0.0014,
      "step": 69710
    },
    {
      "epoch": 3.7184,
      "grad_norm": 0.2884555757045746,
      "learning_rate": 2.676e-05,
      "loss": 0.0017,
      "step": 69720
    },
    {
      "epoch": 3.718933333333333,
      "grad_norm": 0.45047685503959656,
      "learning_rate": 2.6756666666666667e-05,
      "loss": 0.0022,
      "step": 69730
    },
    {
      "epoch": 3.7194666666666665,
      "grad_norm": 0.4915091097354889,
      "learning_rate": 2.6753333333333337e-05,
      "loss": 0.0017,
      "step": 69740
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 0.4676307439804077,
      "learning_rate": 2.6750000000000003e-05,
      "loss": 0.0031,
      "step": 69750
    },
    {
      "epoch": 3.7205333333333335,
      "grad_norm": 0.07544724643230438,
      "learning_rate": 2.674666666666667e-05,
      "loss": 0.0025,
      "step": 69760
    },
    {
      "epoch": 3.7210666666666667,
      "grad_norm": 0.03626573830842972,
      "learning_rate": 2.6743333333333335e-05,
      "loss": 0.0023,
      "step": 69770
    },
    {
      "epoch": 3.7216,
      "grad_norm": 0.12400492280721664,
      "learning_rate": 2.6740000000000005e-05,
      "loss": 0.0019,
      "step": 69780
    },
    {
      "epoch": 3.7221333333333333,
      "grad_norm": 0.12439678609371185,
      "learning_rate": 2.6736666666666664e-05,
      "loss": 0.0017,
      "step": 69790
    },
    {
      "epoch": 3.7226666666666666,
      "grad_norm": 0.529912531375885,
      "learning_rate": 2.6733333333333334e-05,
      "loss": 0.0016,
      "step": 69800
    },
    {
      "epoch": 3.7232,
      "grad_norm": 0.12706154584884644,
      "learning_rate": 2.673e-05,
      "loss": 0.0023,
      "step": 69810
    },
    {
      "epoch": 3.7237333333333336,
      "grad_norm": 0.12735451757907867,
      "learning_rate": 2.6726666666666666e-05,
      "loss": 0.0029,
      "step": 69820
    },
    {
      "epoch": 3.724266666666667,
      "grad_norm": 0.0695568397641182,
      "learning_rate": 2.6723333333333332e-05,
      "loss": 0.0031,
      "step": 69830
    },
    {
      "epoch": 3.7248,
      "grad_norm": 0.04363492503762245,
      "learning_rate": 2.672e-05,
      "loss": 0.0024,
      "step": 69840
    },
    {
      "epoch": 3.7253333333333334,
      "grad_norm": 0.542111873626709,
      "learning_rate": 2.6716666666666668e-05,
      "loss": 0.0021,
      "step": 69850
    },
    {
      "epoch": 3.7258666666666667,
      "grad_norm": 0.1694955974817276,
      "learning_rate": 2.6713333333333334e-05,
      "loss": 0.0019,
      "step": 69860
    },
    {
      "epoch": 3.7264,
      "grad_norm": 0.19131478667259216,
      "learning_rate": 2.671e-05,
      "loss": 0.0022,
      "step": 69870
    },
    {
      "epoch": 3.726933333333333,
      "grad_norm": 0.6698827743530273,
      "learning_rate": 2.670666666666667e-05,
      "loss": 0.0022,
      "step": 69880
    },
    {
      "epoch": 3.7274666666666665,
      "grad_norm": 0.22125670313835144,
      "learning_rate": 2.6703333333333336e-05,
      "loss": 0.0021,
      "step": 69890
    },
    {
      "epoch": 3.7279999999999998,
      "grad_norm": 0.3598477840423584,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 0.0015,
      "step": 69900
    },
    {
      "epoch": 3.7285333333333335,
      "grad_norm": 0.05651426315307617,
      "learning_rate": 2.669666666666667e-05,
      "loss": 0.0022,
      "step": 69910
    },
    {
      "epoch": 3.7290666666666668,
      "grad_norm": 0.12868382036685944,
      "learning_rate": 2.6693333333333338e-05,
      "loss": 0.0022,
      "step": 69920
    },
    {
      "epoch": 3.7296,
      "grad_norm": 0.16740548610687256,
      "learning_rate": 2.6690000000000004e-05,
      "loss": 0.0022,
      "step": 69930
    },
    {
      "epoch": 3.7301333333333333,
      "grad_norm": 0.1851321905851364,
      "learning_rate": 2.6686666666666666e-05,
      "loss": 0.0018,
      "step": 69940
    },
    {
      "epoch": 3.7306666666666666,
      "grad_norm": 0.13030566275119781,
      "learning_rate": 2.6683333333333333e-05,
      "loss": 0.0012,
      "step": 69950
    },
    {
      "epoch": 3.7312,
      "grad_norm": 0.13376286625862122,
      "learning_rate": 2.668e-05,
      "loss": 0.0015,
      "step": 69960
    },
    {
      "epoch": 3.7317333333333336,
      "grad_norm": 0.2704647481441498,
      "learning_rate": 2.6676666666666665e-05,
      "loss": 0.0015,
      "step": 69970
    },
    {
      "epoch": 3.732266666666667,
      "grad_norm": 0.41442909836769104,
      "learning_rate": 2.6673333333333334e-05,
      "loss": 0.0017,
      "step": 69980
    },
    {
      "epoch": 3.7328,
      "grad_norm": 0.32789531350135803,
      "learning_rate": 2.667e-05,
      "loss": 0.0018,
      "step": 69990
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 0.30751290917396545,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.0021,
      "step": 70000
    },
    {
      "epoch": 3.7338666666666667,
      "grad_norm": 0.36781230568885803,
      "learning_rate": 2.6663333333333336e-05,
      "loss": 0.0017,
      "step": 70010
    },
    {
      "epoch": 3.7344,
      "grad_norm": 0.1767997294664383,
      "learning_rate": 2.6660000000000002e-05,
      "loss": 0.0029,
      "step": 70020
    },
    {
      "epoch": 3.734933333333333,
      "grad_norm": 0.3946233093738556,
      "learning_rate": 2.665666666666667e-05,
      "loss": 0.0021,
      "step": 70030
    },
    {
      "epoch": 3.7354666666666665,
      "grad_norm": 0.26124927401542664,
      "learning_rate": 2.6653333333333335e-05,
      "loss": 0.0014,
      "step": 70040
    },
    {
      "epoch": 3.7359999999999998,
      "grad_norm": 0.18747299909591675,
      "learning_rate": 2.6650000000000004e-05,
      "loss": 0.0024,
      "step": 70050
    },
    {
      "epoch": 3.7365333333333335,
      "grad_norm": 0.26994988322257996,
      "learning_rate": 2.664666666666667e-05,
      "loss": 0.0024,
      "step": 70060
    },
    {
      "epoch": 3.7370666666666668,
      "grad_norm": 0.06809864938259125,
      "learning_rate": 2.6643333333333336e-05,
      "loss": 0.0019,
      "step": 70070
    },
    {
      "epoch": 3.7376,
      "grad_norm": 0.14100538194179535,
      "learning_rate": 2.6640000000000002e-05,
      "loss": 0.003,
      "step": 70080
    },
    {
      "epoch": 3.7381333333333333,
      "grad_norm": 0.28862670063972473,
      "learning_rate": 2.6636666666666665e-05,
      "loss": 0.002,
      "step": 70090
    },
    {
      "epoch": 3.7386666666666666,
      "grad_norm": 0.28001996874809265,
      "learning_rate": 2.663333333333333e-05,
      "loss": 0.0017,
      "step": 70100
    },
    {
      "epoch": 3.7392,
      "grad_norm": 0.2027965635061264,
      "learning_rate": 2.663e-05,
      "loss": 0.0027,
      "step": 70110
    },
    {
      "epoch": 3.7397333333333336,
      "grad_norm": 0.3354910612106323,
      "learning_rate": 2.6626666666666667e-05,
      "loss": 0.0017,
      "step": 70120
    },
    {
      "epoch": 3.740266666666667,
      "grad_norm": 0.30802321434020996,
      "learning_rate": 2.6623333333333333e-05,
      "loss": 0.0015,
      "step": 70130
    },
    {
      "epoch": 3.7408,
      "grad_norm": 0.3314603269100189,
      "learning_rate": 2.662e-05,
      "loss": 0.0016,
      "step": 70140
    },
    {
      "epoch": 3.7413333333333334,
      "grad_norm": 0.3602684438228607,
      "learning_rate": 2.661666666666667e-05,
      "loss": 0.0015,
      "step": 70150
    },
    {
      "epoch": 3.7418666666666667,
      "grad_norm": 0.23070885241031647,
      "learning_rate": 2.6613333333333335e-05,
      "loss": 0.0019,
      "step": 70160
    },
    {
      "epoch": 3.7424,
      "grad_norm": 0.05254890397191048,
      "learning_rate": 2.661e-05,
      "loss": 0.0017,
      "step": 70170
    },
    {
      "epoch": 3.7429333333333332,
      "grad_norm": 0.3977481424808502,
      "learning_rate": 2.6606666666666667e-05,
      "loss": 0.002,
      "step": 70180
    },
    {
      "epoch": 3.7434666666666665,
      "grad_norm": 0.036356642842292786,
      "learning_rate": 2.6603333333333337e-05,
      "loss": 0.0017,
      "step": 70190
    },
    {
      "epoch": 3.7439999999999998,
      "grad_norm": 0.3717591464519501,
      "learning_rate": 2.6600000000000003e-05,
      "loss": 0.0017,
      "step": 70200
    },
    {
      "epoch": 3.7445333333333335,
      "grad_norm": 0.26717883348464966,
      "learning_rate": 2.659666666666667e-05,
      "loss": 0.0017,
      "step": 70210
    },
    {
      "epoch": 3.7450666666666668,
      "grad_norm": 0.3836669325828552,
      "learning_rate": 2.6593333333333335e-05,
      "loss": 0.002,
      "step": 70220
    },
    {
      "epoch": 3.7456,
      "grad_norm": 0.31481748819351196,
      "learning_rate": 2.6590000000000005e-05,
      "loss": 0.0035,
      "step": 70230
    },
    {
      "epoch": 3.7461333333333333,
      "grad_norm": 0.3894377052783966,
      "learning_rate": 2.6586666666666664e-05,
      "loss": 0.0021,
      "step": 70240
    },
    {
      "epoch": 3.7466666666666666,
      "grad_norm": 0.46238458156585693,
      "learning_rate": 2.6583333333333333e-05,
      "loss": 0.0015,
      "step": 70250
    },
    {
      "epoch": 3.7472,
      "grad_norm": 0.07486391812562943,
      "learning_rate": 2.658e-05,
      "loss": 0.0015,
      "step": 70260
    },
    {
      "epoch": 3.7477333333333336,
      "grad_norm": 0.038764871656894684,
      "learning_rate": 2.6576666666666666e-05,
      "loss": 0.0032,
      "step": 70270
    },
    {
      "epoch": 3.748266666666667,
      "grad_norm": 0.15585705637931824,
      "learning_rate": 2.6573333333333332e-05,
      "loss": 0.0015,
      "step": 70280
    },
    {
      "epoch": 3.7488,
      "grad_norm": 0.21975497901439667,
      "learning_rate": 2.657e-05,
      "loss": 0.0019,
      "step": 70290
    },
    {
      "epoch": 3.7493333333333334,
      "grad_norm": 0.3618876338005066,
      "learning_rate": 2.6566666666666668e-05,
      "loss": 0.0019,
      "step": 70300
    },
    {
      "epoch": 3.7498666666666667,
      "grad_norm": 0.35612112283706665,
      "learning_rate": 2.6563333333333334e-05,
      "loss": 0.0018,
      "step": 70310
    },
    {
      "epoch": 3.7504,
      "grad_norm": 0.5186931490898132,
      "learning_rate": 2.6560000000000003e-05,
      "loss": 0.0013,
      "step": 70320
    },
    {
      "epoch": 3.7509333333333332,
      "grad_norm": 0.7573511600494385,
      "learning_rate": 2.655666666666667e-05,
      "loss": 0.0016,
      "step": 70330
    },
    {
      "epoch": 3.7514666666666665,
      "grad_norm": 0.28294843435287476,
      "learning_rate": 2.6553333333333335e-05,
      "loss": 0.0014,
      "step": 70340
    },
    {
      "epoch": 3.752,
      "grad_norm": 0.05612395703792572,
      "learning_rate": 2.655e-05,
      "loss": 0.0019,
      "step": 70350
    },
    {
      "epoch": 3.7525333333333335,
      "grad_norm": 0.08987803757190704,
      "learning_rate": 2.654666666666667e-05,
      "loss": 0.0017,
      "step": 70360
    },
    {
      "epoch": 3.7530666666666668,
      "grad_norm": 0.37690356373786926,
      "learning_rate": 2.6543333333333337e-05,
      "loss": 0.0021,
      "step": 70370
    },
    {
      "epoch": 3.7536,
      "grad_norm": 0.4005516767501831,
      "learning_rate": 2.6540000000000003e-05,
      "loss": 0.002,
      "step": 70380
    },
    {
      "epoch": 3.7541333333333333,
      "grad_norm": 0.2380698174238205,
      "learning_rate": 2.6536666666666666e-05,
      "loss": 0.0025,
      "step": 70390
    },
    {
      "epoch": 3.7546666666666666,
      "grad_norm": 0.6745268106460571,
      "learning_rate": 2.6533333333333332e-05,
      "loss": 0.0023,
      "step": 70400
    },
    {
      "epoch": 3.7552,
      "grad_norm": 0.43203309178352356,
      "learning_rate": 2.653e-05,
      "loss": 0.0023,
      "step": 70410
    },
    {
      "epoch": 3.7557333333333336,
      "grad_norm": 0.12972034513950348,
      "learning_rate": 2.6526666666666668e-05,
      "loss": 0.0017,
      "step": 70420
    },
    {
      "epoch": 3.756266666666667,
      "grad_norm": 0.31219297647476196,
      "learning_rate": 2.6523333333333334e-05,
      "loss": 0.0017,
      "step": 70430
    },
    {
      "epoch": 3.7568,
      "grad_norm": 0.37361961603164673,
      "learning_rate": 2.652e-05,
      "loss": 0.0015,
      "step": 70440
    },
    {
      "epoch": 3.7573333333333334,
      "grad_norm": 0.12157181650400162,
      "learning_rate": 2.6516666666666666e-05,
      "loss": 0.0019,
      "step": 70450
    },
    {
      "epoch": 3.7578666666666667,
      "grad_norm": 0.37595996260643005,
      "learning_rate": 2.6513333333333336e-05,
      "loss": 0.002,
      "step": 70460
    },
    {
      "epoch": 3.7584,
      "grad_norm": 0.28158342838287354,
      "learning_rate": 2.6510000000000002e-05,
      "loss": 0.0017,
      "step": 70470
    },
    {
      "epoch": 3.7589333333333332,
      "grad_norm": 0.19344627857208252,
      "learning_rate": 2.6506666666666668e-05,
      "loss": 0.0026,
      "step": 70480
    },
    {
      "epoch": 3.7594666666666665,
      "grad_norm": 0.02762695774435997,
      "learning_rate": 2.6503333333333334e-05,
      "loss": 0.0015,
      "step": 70490
    },
    {
      "epoch": 3.76,
      "grad_norm": 0.12445865571498871,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 0.002,
      "step": 70500
    },
    {
      "epoch": 3.760533333333333,
      "grad_norm": 0.12689344584941864,
      "learning_rate": 2.649666666666667e-05,
      "loss": 0.0018,
      "step": 70510
    },
    {
      "epoch": 3.761066666666667,
      "grad_norm": 0.06651566177606583,
      "learning_rate": 2.6493333333333336e-05,
      "loss": 0.0021,
      "step": 70520
    },
    {
      "epoch": 3.7616,
      "grad_norm": 0.09861090034246445,
      "learning_rate": 2.6490000000000002e-05,
      "loss": 0.0016,
      "step": 70530
    },
    {
      "epoch": 3.7621333333333333,
      "grad_norm": 0.3088257312774658,
      "learning_rate": 2.6486666666666665e-05,
      "loss": 0.002,
      "step": 70540
    },
    {
      "epoch": 3.7626666666666666,
      "grad_norm": 0.12776881456375122,
      "learning_rate": 2.648333333333333e-05,
      "loss": 0.0013,
      "step": 70550
    },
    {
      "epoch": 3.7632,
      "grad_norm": 0.2860194444656372,
      "learning_rate": 2.648e-05,
      "loss": 0.0024,
      "step": 70560
    },
    {
      "epoch": 3.7637333333333336,
      "grad_norm": 0.054933663457632065,
      "learning_rate": 2.6476666666666667e-05,
      "loss": 0.0023,
      "step": 70570
    },
    {
      "epoch": 3.764266666666667,
      "grad_norm": 0.34204360842704773,
      "learning_rate": 2.6473333333333333e-05,
      "loss": 0.002,
      "step": 70580
    },
    {
      "epoch": 3.7648,
      "grad_norm": 0.07938038557767868,
      "learning_rate": 2.647e-05,
      "loss": 0.0022,
      "step": 70590
    },
    {
      "epoch": 3.7653333333333334,
      "grad_norm": 0.259756475687027,
      "learning_rate": 2.646666666666667e-05,
      "loss": 0.0014,
      "step": 70600
    },
    {
      "epoch": 3.7658666666666667,
      "grad_norm": 0.035429567098617554,
      "learning_rate": 2.6463333333333335e-05,
      "loss": 0.0023,
      "step": 70610
    },
    {
      "epoch": 3.7664,
      "grad_norm": 0.49091479182243347,
      "learning_rate": 2.646e-05,
      "loss": 0.0019,
      "step": 70620
    },
    {
      "epoch": 3.7669333333333332,
      "grad_norm": 0.3649960160255432,
      "learning_rate": 2.6456666666666667e-05,
      "loss": 0.0021,
      "step": 70630
    },
    {
      "epoch": 3.7674666666666665,
      "grad_norm": 0.2780568599700928,
      "learning_rate": 2.6453333333333336e-05,
      "loss": 0.0024,
      "step": 70640
    },
    {
      "epoch": 3.768,
      "grad_norm": 0.2166571319103241,
      "learning_rate": 2.6450000000000003e-05,
      "loss": 0.0027,
      "step": 70650
    },
    {
      "epoch": 3.768533333333333,
      "grad_norm": 0.4900037348270416,
      "learning_rate": 2.644666666666667e-05,
      "loss": 0.002,
      "step": 70660
    },
    {
      "epoch": 3.769066666666667,
      "grad_norm": 0.4254576563835144,
      "learning_rate": 2.6443333333333338e-05,
      "loss": 0.0021,
      "step": 70670
    },
    {
      "epoch": 3.7696,
      "grad_norm": 0.3345372974872589,
      "learning_rate": 2.6440000000000004e-05,
      "loss": 0.0024,
      "step": 70680
    },
    {
      "epoch": 3.7701333333333333,
      "grad_norm": 0.6325808167457581,
      "learning_rate": 2.643666666666667e-05,
      "loss": 0.0013,
      "step": 70690
    },
    {
      "epoch": 3.7706666666666666,
      "grad_norm": 0.29279258847236633,
      "learning_rate": 2.6433333333333333e-05,
      "loss": 0.0037,
      "step": 70700
    },
    {
      "epoch": 3.7712,
      "grad_norm": 0.4028036296367645,
      "learning_rate": 2.643e-05,
      "loss": 0.0029,
      "step": 70710
    },
    {
      "epoch": 3.7717333333333336,
      "grad_norm": 0.37889862060546875,
      "learning_rate": 2.6426666666666665e-05,
      "loss": 0.0022,
      "step": 70720
    },
    {
      "epoch": 3.772266666666667,
      "grad_norm": 0.4983237683773041,
      "learning_rate": 2.642333333333333e-05,
      "loss": 0.0027,
      "step": 70730
    },
    {
      "epoch": 3.7728,
      "grad_norm": 0.3402232527732849,
      "learning_rate": 2.642e-05,
      "loss": 0.0024,
      "step": 70740
    },
    {
      "epoch": 3.7733333333333334,
      "grad_norm": 0.036590203642845154,
      "learning_rate": 2.6416666666666667e-05,
      "loss": 0.0019,
      "step": 70750
    },
    {
      "epoch": 3.7738666666666667,
      "grad_norm": 0.21900537610054016,
      "learning_rate": 2.6413333333333333e-05,
      "loss": 0.0025,
      "step": 70760
    },
    {
      "epoch": 3.7744,
      "grad_norm": 0.24414777755737305,
      "learning_rate": 2.6410000000000003e-05,
      "loss": 0.0021,
      "step": 70770
    },
    {
      "epoch": 3.7749333333333333,
      "grad_norm": 0.2137000411748886,
      "learning_rate": 2.640666666666667e-05,
      "loss": 0.0022,
      "step": 70780
    },
    {
      "epoch": 3.7754666666666665,
      "grad_norm": 0.27883774042129517,
      "learning_rate": 2.6403333333333335e-05,
      "loss": 0.0028,
      "step": 70790
    },
    {
      "epoch": 3.776,
      "grad_norm": 0.35565951466560364,
      "learning_rate": 2.64e-05,
      "loss": 0.0031,
      "step": 70800
    },
    {
      "epoch": 3.776533333333333,
      "grad_norm": 0.48618683218955994,
      "learning_rate": 2.639666666666667e-05,
      "loss": 0.0018,
      "step": 70810
    },
    {
      "epoch": 3.777066666666667,
      "grad_norm": 0.12285470217466354,
      "learning_rate": 2.6393333333333337e-05,
      "loss": 0.0018,
      "step": 70820
    },
    {
      "epoch": 3.7776,
      "grad_norm": 0.44366925954818726,
      "learning_rate": 2.6390000000000003e-05,
      "loss": 0.0019,
      "step": 70830
    },
    {
      "epoch": 3.7781333333333333,
      "grad_norm": 0.05268967151641846,
      "learning_rate": 2.638666666666667e-05,
      "loss": 0.0019,
      "step": 70840
    },
    {
      "epoch": 3.7786666666666666,
      "grad_norm": 0.24177846312522888,
      "learning_rate": 2.6383333333333332e-05,
      "loss": 0.0014,
      "step": 70850
    },
    {
      "epoch": 3.7792,
      "grad_norm": 0.17029772698879242,
      "learning_rate": 2.6379999999999998e-05,
      "loss": 0.0028,
      "step": 70860
    },
    {
      "epoch": 3.779733333333333,
      "grad_norm": 0.3692314028739929,
      "learning_rate": 2.6376666666666668e-05,
      "loss": 0.0025,
      "step": 70870
    },
    {
      "epoch": 3.780266666666667,
      "grad_norm": 0.5620247721672058,
      "learning_rate": 2.6373333333333334e-05,
      "loss": 0.0016,
      "step": 70880
    },
    {
      "epoch": 3.7808,
      "grad_norm": 0.18941909074783325,
      "learning_rate": 2.637e-05,
      "loss": 0.0018,
      "step": 70890
    },
    {
      "epoch": 3.7813333333333334,
      "grad_norm": 0.17243218421936035,
      "learning_rate": 2.6366666666666666e-05,
      "loss": 0.003,
      "step": 70900
    },
    {
      "epoch": 3.7818666666666667,
      "grad_norm": 0.12679043412208557,
      "learning_rate": 2.6363333333333336e-05,
      "loss": 0.0022,
      "step": 70910
    },
    {
      "epoch": 3.7824,
      "grad_norm": 0.11846872419118881,
      "learning_rate": 2.6360000000000002e-05,
      "loss": 0.0023,
      "step": 70920
    },
    {
      "epoch": 3.7829333333333333,
      "grad_norm": 0.04657216742634773,
      "learning_rate": 2.6356666666666668e-05,
      "loss": 0.0024,
      "step": 70930
    },
    {
      "epoch": 3.7834666666666665,
      "grad_norm": 0.22454242408275604,
      "learning_rate": 2.6353333333333334e-05,
      "loss": 0.0015,
      "step": 70940
    },
    {
      "epoch": 3.784,
      "grad_norm": 0.2549021542072296,
      "learning_rate": 2.6350000000000004e-05,
      "loss": 0.002,
      "step": 70950
    },
    {
      "epoch": 3.784533333333333,
      "grad_norm": 0.18522344529628754,
      "learning_rate": 2.634666666666667e-05,
      "loss": 0.0018,
      "step": 70960
    },
    {
      "epoch": 3.785066666666667,
      "grad_norm": 0.45813947916030884,
      "learning_rate": 2.6343333333333336e-05,
      "loss": 0.0021,
      "step": 70970
    },
    {
      "epoch": 3.7856,
      "grad_norm": 0.15986120700836182,
      "learning_rate": 2.6340000000000002e-05,
      "loss": 0.0015,
      "step": 70980
    },
    {
      "epoch": 3.7861333333333334,
      "grad_norm": 0.042568475008010864,
      "learning_rate": 2.633666666666667e-05,
      "loss": 0.0014,
      "step": 70990
    },
    {
      "epoch": 3.7866666666666666,
      "grad_norm": 0.21475304663181305,
      "learning_rate": 2.633333333333333e-05,
      "loss": 0.0019,
      "step": 71000
    },
    {
      "epoch": 3.7872,
      "grad_norm": 0.12175918370485306,
      "learning_rate": 2.633e-05,
      "loss": 0.0031,
      "step": 71010
    },
    {
      "epoch": 3.787733333333333,
      "grad_norm": 0.3923223316669464,
      "learning_rate": 2.6326666666666666e-05,
      "loss": 0.0023,
      "step": 71020
    },
    {
      "epoch": 3.788266666666667,
      "grad_norm": 0.29871484637260437,
      "learning_rate": 2.6323333333333333e-05,
      "loss": 0.0017,
      "step": 71030
    },
    {
      "epoch": 3.7888,
      "grad_norm": 0.07234887033700943,
      "learning_rate": 2.632e-05,
      "loss": 0.0015,
      "step": 71040
    },
    {
      "epoch": 3.7893333333333334,
      "grad_norm": 0.0664139911532402,
      "learning_rate": 2.6316666666666668e-05,
      "loss": 0.0031,
      "step": 71050
    },
    {
      "epoch": 3.7898666666666667,
      "grad_norm": 0.4530313313007355,
      "learning_rate": 2.6313333333333334e-05,
      "loss": 0.0022,
      "step": 71060
    },
    {
      "epoch": 3.7904,
      "grad_norm": 0.31251516938209534,
      "learning_rate": 2.631e-05,
      "loss": 0.0027,
      "step": 71070
    },
    {
      "epoch": 3.7909333333333333,
      "grad_norm": 0.23991894721984863,
      "learning_rate": 2.630666666666667e-05,
      "loss": 0.0024,
      "step": 71080
    },
    {
      "epoch": 3.7914666666666665,
      "grad_norm": 0.16957908868789673,
      "learning_rate": 2.6303333333333336e-05,
      "loss": 0.0018,
      "step": 71090
    },
    {
      "epoch": 3.792,
      "grad_norm": 0.39609503746032715,
      "learning_rate": 2.6300000000000002e-05,
      "loss": 0.0015,
      "step": 71100
    },
    {
      "epoch": 3.792533333333333,
      "grad_norm": 0.32001790404319763,
      "learning_rate": 2.629666666666667e-05,
      "loss": 0.0018,
      "step": 71110
    },
    {
      "epoch": 3.793066666666667,
      "grad_norm": 0.27801522612571716,
      "learning_rate": 2.6293333333333338e-05,
      "loss": 0.0018,
      "step": 71120
    },
    {
      "epoch": 3.7936,
      "grad_norm": 0.06685854494571686,
      "learning_rate": 2.6290000000000004e-05,
      "loss": 0.0034,
      "step": 71130
    },
    {
      "epoch": 3.7941333333333334,
      "grad_norm": 0.1641141176223755,
      "learning_rate": 2.628666666666667e-05,
      "loss": 0.0021,
      "step": 71140
    },
    {
      "epoch": 3.7946666666666666,
      "grad_norm": 0.19154146313667297,
      "learning_rate": 2.6283333333333333e-05,
      "loss": 0.0015,
      "step": 71150
    },
    {
      "epoch": 3.7952,
      "grad_norm": 0.09600194543600082,
      "learning_rate": 2.628e-05,
      "loss": 0.0025,
      "step": 71160
    },
    {
      "epoch": 3.795733333333333,
      "grad_norm": 0.6244239807128906,
      "learning_rate": 2.6276666666666665e-05,
      "loss": 0.0027,
      "step": 71170
    },
    {
      "epoch": 3.796266666666667,
      "grad_norm": 0.0958944633603096,
      "learning_rate": 2.6273333333333335e-05,
      "loss": 0.0014,
      "step": 71180
    },
    {
      "epoch": 3.7968,
      "grad_norm": 0.3661406338214874,
      "learning_rate": 2.627e-05,
      "loss": 0.0027,
      "step": 71190
    },
    {
      "epoch": 3.7973333333333334,
      "grad_norm": 0.631893515586853,
      "learning_rate": 2.6266666666666667e-05,
      "loss": 0.0027,
      "step": 71200
    },
    {
      "epoch": 3.7978666666666667,
      "grad_norm": 0.08274605870246887,
      "learning_rate": 2.6263333333333333e-05,
      "loss": 0.0025,
      "step": 71210
    },
    {
      "epoch": 3.7984,
      "grad_norm": 0.4785029888153076,
      "learning_rate": 2.6260000000000003e-05,
      "loss": 0.0026,
      "step": 71220
    },
    {
      "epoch": 3.7989333333333333,
      "grad_norm": 0.2645234763622284,
      "learning_rate": 2.625666666666667e-05,
      "loss": 0.0031,
      "step": 71230
    },
    {
      "epoch": 3.7994666666666665,
      "grad_norm": 0.4799751937389374,
      "learning_rate": 2.6253333333333335e-05,
      "loss": 0.0025,
      "step": 71240
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.6708475947380066,
      "learning_rate": 2.625e-05,
      "loss": 0.0014,
      "step": 71250
    },
    {
      "epoch": 3.800533333333333,
      "grad_norm": 0.6761407256126404,
      "learning_rate": 2.624666666666667e-05,
      "loss": 0.0016,
      "step": 71260
    },
    {
      "epoch": 3.801066666666667,
      "grad_norm": 0.12847523391246796,
      "learning_rate": 2.6243333333333337e-05,
      "loss": 0.002,
      "step": 71270
    },
    {
      "epoch": 3.8016,
      "grad_norm": 0.42628711462020874,
      "learning_rate": 2.6240000000000003e-05,
      "loss": 0.0013,
      "step": 71280
    },
    {
      "epoch": 3.8021333333333334,
      "grad_norm": 0.15682671964168549,
      "learning_rate": 2.623666666666667e-05,
      "loss": 0.002,
      "step": 71290
    },
    {
      "epoch": 3.8026666666666666,
      "grad_norm": 0.5480965971946716,
      "learning_rate": 2.6233333333333332e-05,
      "loss": 0.0025,
      "step": 71300
    },
    {
      "epoch": 3.8032,
      "grad_norm": 0.1340760439634323,
      "learning_rate": 2.6229999999999998e-05,
      "loss": 0.002,
      "step": 71310
    },
    {
      "epoch": 3.803733333333333,
      "grad_norm": 0.18162646889686584,
      "learning_rate": 2.6226666666666667e-05,
      "loss": 0.0015,
      "step": 71320
    },
    {
      "epoch": 3.804266666666667,
      "grad_norm": 0.4657674729824066,
      "learning_rate": 2.6223333333333334e-05,
      "loss": 0.002,
      "step": 71330
    },
    {
      "epoch": 3.8048,
      "grad_norm": 0.3220260441303253,
      "learning_rate": 2.622e-05,
      "loss": 0.0022,
      "step": 71340
    },
    {
      "epoch": 3.8053333333333335,
      "grad_norm": 0.23803269863128662,
      "learning_rate": 2.6216666666666666e-05,
      "loss": 0.0031,
      "step": 71350
    },
    {
      "epoch": 3.8058666666666667,
      "grad_norm": 0.4020334780216217,
      "learning_rate": 2.6213333333333335e-05,
      "loss": 0.0017,
      "step": 71360
    },
    {
      "epoch": 3.8064,
      "grad_norm": 0.1116064190864563,
      "learning_rate": 2.621e-05,
      "loss": 0.0023,
      "step": 71370
    },
    {
      "epoch": 3.8069333333333333,
      "grad_norm": 0.31597140431404114,
      "learning_rate": 2.6206666666666668e-05,
      "loss": 0.0025,
      "step": 71380
    },
    {
      "epoch": 3.8074666666666666,
      "grad_norm": 0.25904741883277893,
      "learning_rate": 2.6203333333333334e-05,
      "loss": 0.0021,
      "step": 71390
    },
    {
      "epoch": 3.808,
      "grad_norm": 0.0638006404042244,
      "learning_rate": 2.6200000000000003e-05,
      "loss": 0.0021,
      "step": 71400
    },
    {
      "epoch": 3.808533333333333,
      "grad_norm": 0.28172898292541504,
      "learning_rate": 2.619666666666667e-05,
      "loss": 0.002,
      "step": 71410
    },
    {
      "epoch": 3.809066666666667,
      "grad_norm": 0.40501224994659424,
      "learning_rate": 2.6193333333333336e-05,
      "loss": 0.0013,
      "step": 71420
    },
    {
      "epoch": 3.8096,
      "grad_norm": 0.02787296287715435,
      "learning_rate": 2.6190000000000005e-05,
      "loss": 0.0018,
      "step": 71430
    },
    {
      "epoch": 3.8101333333333334,
      "grad_norm": 0.06809800118207932,
      "learning_rate": 2.618666666666667e-05,
      "loss": 0.0016,
      "step": 71440
    },
    {
      "epoch": 3.8106666666666666,
      "grad_norm": 0.09402455389499664,
      "learning_rate": 2.618333333333333e-05,
      "loss": 0.002,
      "step": 71450
    },
    {
      "epoch": 3.8112,
      "grad_norm": 0.10289468616247177,
      "learning_rate": 2.618e-05,
      "loss": 0.0027,
      "step": 71460
    },
    {
      "epoch": 3.811733333333333,
      "grad_norm": 0.38641560077667236,
      "learning_rate": 2.6176666666666666e-05,
      "loss": 0.0021,
      "step": 71470
    },
    {
      "epoch": 3.812266666666667,
      "grad_norm": 0.10568590462207794,
      "learning_rate": 2.6173333333333332e-05,
      "loss": 0.0012,
      "step": 71480
    },
    {
      "epoch": 3.8128,
      "grad_norm": 0.5021885633468628,
      "learning_rate": 2.617e-05,
      "loss": 0.0012,
      "step": 71490
    },
    {
      "epoch": 3.8133333333333335,
      "grad_norm": 0.4272676408290863,
      "learning_rate": 2.6166666666666668e-05,
      "loss": 0.0027,
      "step": 71500
    },
    {
      "epoch": 3.8138666666666667,
      "grad_norm": 0.3676024079322815,
      "learning_rate": 2.6163333333333334e-05,
      "loss": 0.0019,
      "step": 71510
    },
    {
      "epoch": 3.8144,
      "grad_norm": 0.45361995697021484,
      "learning_rate": 2.616e-05,
      "loss": 0.0024,
      "step": 71520
    },
    {
      "epoch": 3.8149333333333333,
      "grad_norm": 0.16032208502292633,
      "learning_rate": 2.615666666666667e-05,
      "loss": 0.0021,
      "step": 71530
    },
    {
      "epoch": 3.8154666666666666,
      "grad_norm": 0.2223072052001953,
      "learning_rate": 2.6153333333333336e-05,
      "loss": 0.002,
      "step": 71540
    },
    {
      "epoch": 3.816,
      "grad_norm": 0.3166355490684509,
      "learning_rate": 2.6150000000000002e-05,
      "loss": 0.0021,
      "step": 71550
    },
    {
      "epoch": 3.816533333333333,
      "grad_norm": 0.2412416636943817,
      "learning_rate": 2.6146666666666668e-05,
      "loss": 0.0015,
      "step": 71560
    },
    {
      "epoch": 3.817066666666667,
      "grad_norm": 0.6177965998649597,
      "learning_rate": 2.6143333333333338e-05,
      "loss": 0.0019,
      "step": 71570
    },
    {
      "epoch": 3.8176,
      "grad_norm": 0.10560430586338043,
      "learning_rate": 2.6140000000000004e-05,
      "loss": 0.0022,
      "step": 71580
    },
    {
      "epoch": 3.8181333333333334,
      "grad_norm": 0.2872036397457123,
      "learning_rate": 2.613666666666667e-05,
      "loss": 0.0018,
      "step": 71590
    },
    {
      "epoch": 3.8186666666666667,
      "grad_norm": 0.09707343578338623,
      "learning_rate": 2.6133333333333333e-05,
      "loss": 0.0026,
      "step": 71600
    },
    {
      "epoch": 3.8192,
      "grad_norm": 0.4003753364086151,
      "learning_rate": 2.613e-05,
      "loss": 0.003,
      "step": 71610
    },
    {
      "epoch": 3.819733333333333,
      "grad_norm": 0.43627020716667175,
      "learning_rate": 2.6126666666666665e-05,
      "loss": 0.0028,
      "step": 71620
    },
    {
      "epoch": 3.820266666666667,
      "grad_norm": 0.13054470717906952,
      "learning_rate": 2.6123333333333335e-05,
      "loss": 0.0016,
      "step": 71630
    },
    {
      "epoch": 3.8208,
      "grad_norm": 0.5429148077964783,
      "learning_rate": 2.612e-05,
      "loss": 0.002,
      "step": 71640
    },
    {
      "epoch": 3.8213333333333335,
      "grad_norm": 0.20204350352287292,
      "learning_rate": 2.6116666666666667e-05,
      "loss": 0.0027,
      "step": 71650
    },
    {
      "epoch": 3.8218666666666667,
      "grad_norm": 0.1376882642507553,
      "learning_rate": 2.6113333333333333e-05,
      "loss": 0.0026,
      "step": 71660
    },
    {
      "epoch": 3.8224,
      "grad_norm": 0.24763424694538116,
      "learning_rate": 2.6110000000000002e-05,
      "loss": 0.0014,
      "step": 71670
    },
    {
      "epoch": 3.8229333333333333,
      "grad_norm": 0.3676057457923889,
      "learning_rate": 2.610666666666667e-05,
      "loss": 0.0018,
      "step": 71680
    },
    {
      "epoch": 3.8234666666666666,
      "grad_norm": 0.08177071809768677,
      "learning_rate": 2.6103333333333335e-05,
      "loss": 0.0017,
      "step": 71690
    },
    {
      "epoch": 3.824,
      "grad_norm": 0.18704473972320557,
      "learning_rate": 2.61e-05,
      "loss": 0.0028,
      "step": 71700
    },
    {
      "epoch": 3.824533333333333,
      "grad_norm": 0.2146112322807312,
      "learning_rate": 2.609666666666667e-05,
      "loss": 0.0015,
      "step": 71710
    },
    {
      "epoch": 3.8250666666666664,
      "grad_norm": 0.17002291977405548,
      "learning_rate": 2.6093333333333336e-05,
      "loss": 0.0023,
      "step": 71720
    },
    {
      "epoch": 3.8256,
      "grad_norm": 0.2636675536632538,
      "learning_rate": 2.6090000000000003e-05,
      "loss": 0.0025,
      "step": 71730
    },
    {
      "epoch": 3.8261333333333334,
      "grad_norm": 0.24421945214271545,
      "learning_rate": 2.608666666666667e-05,
      "loss": 0.0023,
      "step": 71740
    },
    {
      "epoch": 3.8266666666666667,
      "grad_norm": 0.6736758351325989,
      "learning_rate": 2.608333333333333e-05,
      "loss": 0.002,
      "step": 71750
    },
    {
      "epoch": 3.8272,
      "grad_norm": 0.15541431307792664,
      "learning_rate": 2.6079999999999998e-05,
      "loss": 0.0018,
      "step": 71760
    },
    {
      "epoch": 3.827733333333333,
      "grad_norm": 0.24457679688930511,
      "learning_rate": 2.6076666666666667e-05,
      "loss": 0.0022,
      "step": 71770
    },
    {
      "epoch": 3.828266666666667,
      "grad_norm": 0.3623661994934082,
      "learning_rate": 2.6073333333333333e-05,
      "loss": 0.0022,
      "step": 71780
    },
    {
      "epoch": 3.8288,
      "grad_norm": 0.10315524786710739,
      "learning_rate": 2.607e-05,
      "loss": 0.0013,
      "step": 71790
    },
    {
      "epoch": 3.8293333333333335,
      "grad_norm": 0.13595151901245117,
      "learning_rate": 2.6066666666666666e-05,
      "loss": 0.0015,
      "step": 71800
    },
    {
      "epoch": 3.8298666666666668,
      "grad_norm": 0.370451420545578,
      "learning_rate": 2.6063333333333335e-05,
      "loss": 0.0028,
      "step": 71810
    },
    {
      "epoch": 3.8304,
      "grad_norm": 0.18791379034519196,
      "learning_rate": 2.606e-05,
      "loss": 0.0019,
      "step": 71820
    },
    {
      "epoch": 3.8309333333333333,
      "grad_norm": 0.38011518120765686,
      "learning_rate": 2.6056666666666667e-05,
      "loss": 0.0025,
      "step": 71830
    },
    {
      "epoch": 3.8314666666666666,
      "grad_norm": 0.536284327507019,
      "learning_rate": 2.6053333333333333e-05,
      "loss": 0.0022,
      "step": 71840
    },
    {
      "epoch": 3.832,
      "grad_norm": 0.0915096253156662,
      "learning_rate": 2.6050000000000003e-05,
      "loss": 0.0018,
      "step": 71850
    },
    {
      "epoch": 3.832533333333333,
      "grad_norm": 0.5733880996704102,
      "learning_rate": 2.604666666666667e-05,
      "loss": 0.0017,
      "step": 71860
    },
    {
      "epoch": 3.8330666666666664,
      "grad_norm": 0.3099702298641205,
      "learning_rate": 2.6043333333333335e-05,
      "loss": 0.0024,
      "step": 71870
    },
    {
      "epoch": 3.8336,
      "grad_norm": 0.058486152440309525,
      "learning_rate": 2.6040000000000005e-05,
      "loss": 0.0023,
      "step": 71880
    },
    {
      "epoch": 3.8341333333333334,
      "grad_norm": 0.29253819584846497,
      "learning_rate": 2.603666666666667e-05,
      "loss": 0.0021,
      "step": 71890
    },
    {
      "epoch": 3.8346666666666667,
      "grad_norm": 0.09633563458919525,
      "learning_rate": 2.6033333333333337e-05,
      "loss": 0.0021,
      "step": 71900
    },
    {
      "epoch": 3.8352,
      "grad_norm": 0.13046929240226746,
      "learning_rate": 2.603e-05,
      "loss": 0.0024,
      "step": 71910
    },
    {
      "epoch": 3.835733333333333,
      "grad_norm": 0.168080136179924,
      "learning_rate": 2.6026666666666666e-05,
      "loss": 0.0025,
      "step": 71920
    },
    {
      "epoch": 3.836266666666667,
      "grad_norm": 0.25337547063827515,
      "learning_rate": 2.6023333333333332e-05,
      "loss": 0.0022,
      "step": 71930
    },
    {
      "epoch": 3.8368,
      "grad_norm": 0.05920206382870674,
      "learning_rate": 2.602e-05,
      "loss": 0.0035,
      "step": 71940
    },
    {
      "epoch": 3.8373333333333335,
      "grad_norm": 0.06557238101959229,
      "learning_rate": 2.6016666666666668e-05,
      "loss": 0.0012,
      "step": 71950
    },
    {
      "epoch": 3.8378666666666668,
      "grad_norm": 0.5921658277511597,
      "learning_rate": 2.6013333333333334e-05,
      "loss": 0.0017,
      "step": 71960
    },
    {
      "epoch": 3.8384,
      "grad_norm": 0.06376083940267563,
      "learning_rate": 2.601e-05,
      "loss": 0.0026,
      "step": 71970
    },
    {
      "epoch": 3.8389333333333333,
      "grad_norm": 0.16467492282390594,
      "learning_rate": 2.600666666666667e-05,
      "loss": 0.0022,
      "step": 71980
    },
    {
      "epoch": 3.8394666666666666,
      "grad_norm": 0.558197021484375,
      "learning_rate": 2.6003333333333336e-05,
      "loss": 0.0019,
      "step": 71990
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.3037889897823334,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.0017,
      "step": 72000
    },
    {
      "epoch": 3.840533333333333,
      "grad_norm": 0.07497355341911316,
      "learning_rate": 2.5996666666666668e-05,
      "loss": 0.0024,
      "step": 72010
    },
    {
      "epoch": 3.8410666666666664,
      "grad_norm": 0.08853614330291748,
      "learning_rate": 2.5993333333333337e-05,
      "loss": 0.0018,
      "step": 72020
    },
    {
      "epoch": 3.8416,
      "grad_norm": 0.050402067601680756,
      "learning_rate": 2.5990000000000004e-05,
      "loss": 0.0029,
      "step": 72030
    },
    {
      "epoch": 3.8421333333333334,
      "grad_norm": 0.18977543711662292,
      "learning_rate": 2.598666666666667e-05,
      "loss": 0.0019,
      "step": 72040
    },
    {
      "epoch": 3.8426666666666667,
      "grad_norm": 0.41611433029174805,
      "learning_rate": 2.5983333333333336e-05,
      "loss": 0.0025,
      "step": 72050
    },
    {
      "epoch": 3.8432,
      "grad_norm": 0.1611674726009369,
      "learning_rate": 2.598e-05,
      "loss": 0.0017,
      "step": 72060
    },
    {
      "epoch": 3.8437333333333332,
      "grad_norm": 0.3022983968257904,
      "learning_rate": 2.5976666666666665e-05,
      "loss": 0.0028,
      "step": 72070
    },
    {
      "epoch": 3.844266666666667,
      "grad_norm": 0.3611113727092743,
      "learning_rate": 2.5973333333333334e-05,
      "loss": 0.0019,
      "step": 72080
    },
    {
      "epoch": 3.8448,
      "grad_norm": 0.45058926939964294,
      "learning_rate": 2.597e-05,
      "loss": 0.0015,
      "step": 72090
    },
    {
      "epoch": 3.8453333333333335,
      "grad_norm": 0.454945832490921,
      "learning_rate": 2.5966666666666667e-05,
      "loss": 0.0017,
      "step": 72100
    },
    {
      "epoch": 3.8458666666666668,
      "grad_norm": 0.04383297637104988,
      "learning_rate": 2.5963333333333333e-05,
      "loss": 0.0017,
      "step": 72110
    },
    {
      "epoch": 3.8464,
      "grad_norm": 0.6438879370689392,
      "learning_rate": 2.5960000000000002e-05,
      "loss": 0.002,
      "step": 72120
    },
    {
      "epoch": 3.8469333333333333,
      "grad_norm": 0.3031798303127289,
      "learning_rate": 2.5956666666666668e-05,
      "loss": 0.0019,
      "step": 72130
    },
    {
      "epoch": 3.8474666666666666,
      "grad_norm": 0.12395291030406952,
      "learning_rate": 2.5953333333333334e-05,
      "loss": 0.0017,
      "step": 72140
    },
    {
      "epoch": 3.848,
      "grad_norm": 0.15441101789474487,
      "learning_rate": 2.595e-05,
      "loss": 0.0019,
      "step": 72150
    },
    {
      "epoch": 3.848533333333333,
      "grad_norm": 0.3258591890335083,
      "learning_rate": 2.594666666666667e-05,
      "loss": 0.0012,
      "step": 72160
    },
    {
      "epoch": 3.8490666666666664,
      "grad_norm": 0.22044900059700012,
      "learning_rate": 2.5943333333333336e-05,
      "loss": 0.0015,
      "step": 72170
    },
    {
      "epoch": 3.8496,
      "grad_norm": 0.3737777769565582,
      "learning_rate": 2.5940000000000002e-05,
      "loss": 0.0024,
      "step": 72180
    },
    {
      "epoch": 3.8501333333333334,
      "grad_norm": 0.8069257736206055,
      "learning_rate": 2.5936666666666672e-05,
      "loss": 0.0014,
      "step": 72190
    },
    {
      "epoch": 3.8506666666666667,
      "grad_norm": 0.6765418648719788,
      "learning_rate": 2.5933333333333338e-05,
      "loss": 0.0021,
      "step": 72200
    },
    {
      "epoch": 3.8512,
      "grad_norm": 0.3323296904563904,
      "learning_rate": 2.5929999999999997e-05,
      "loss": 0.0015,
      "step": 72210
    },
    {
      "epoch": 3.8517333333333332,
      "grad_norm": 0.41649335622787476,
      "learning_rate": 2.5926666666666667e-05,
      "loss": 0.002,
      "step": 72220
    },
    {
      "epoch": 3.8522666666666665,
      "grad_norm": 0.37272170186042786,
      "learning_rate": 2.5923333333333333e-05,
      "loss": 0.002,
      "step": 72230
    },
    {
      "epoch": 3.8528000000000002,
      "grad_norm": 0.41316309571266174,
      "learning_rate": 2.592e-05,
      "loss": 0.0027,
      "step": 72240
    },
    {
      "epoch": 3.8533333333333335,
      "grad_norm": 0.22939592599868774,
      "learning_rate": 2.5916666666666665e-05,
      "loss": 0.0022,
      "step": 72250
    },
    {
      "epoch": 3.8538666666666668,
      "grad_norm": 0.07877432554960251,
      "learning_rate": 2.5913333333333335e-05,
      "loss": 0.0014,
      "step": 72260
    },
    {
      "epoch": 3.8544,
      "grad_norm": 0.3688115179538727,
      "learning_rate": 2.591e-05,
      "loss": 0.0019,
      "step": 72270
    },
    {
      "epoch": 3.8549333333333333,
      "grad_norm": 0.061597004532814026,
      "learning_rate": 2.5906666666666667e-05,
      "loss": 0.0021,
      "step": 72280
    },
    {
      "epoch": 3.8554666666666666,
      "grad_norm": 0.46515658497810364,
      "learning_rate": 2.5903333333333337e-05,
      "loss": 0.0019,
      "step": 72290
    },
    {
      "epoch": 3.856,
      "grad_norm": 0.24871455132961273,
      "learning_rate": 2.5900000000000003e-05,
      "loss": 0.0015,
      "step": 72300
    },
    {
      "epoch": 3.856533333333333,
      "grad_norm": 0.03002438321709633,
      "learning_rate": 2.589666666666667e-05,
      "loss": 0.0022,
      "step": 72310
    },
    {
      "epoch": 3.8570666666666664,
      "grad_norm": 0.30364981293678284,
      "learning_rate": 2.5893333333333335e-05,
      "loss": 0.0015,
      "step": 72320
    },
    {
      "epoch": 3.8576,
      "grad_norm": 0.17279453575611115,
      "learning_rate": 2.5890000000000005e-05,
      "loss": 0.0023,
      "step": 72330
    },
    {
      "epoch": 3.8581333333333334,
      "grad_norm": 0.21568624675273895,
      "learning_rate": 2.588666666666667e-05,
      "loss": 0.002,
      "step": 72340
    },
    {
      "epoch": 3.8586666666666667,
      "grad_norm": 0.10720925033092499,
      "learning_rate": 2.5883333333333337e-05,
      "loss": 0.0021,
      "step": 72350
    },
    {
      "epoch": 3.8592,
      "grad_norm": 0.3666566014289856,
      "learning_rate": 2.588e-05,
      "loss": 0.0019,
      "step": 72360
    },
    {
      "epoch": 3.8597333333333332,
      "grad_norm": 0.36045578122138977,
      "learning_rate": 2.5876666666666666e-05,
      "loss": 0.0016,
      "step": 72370
    },
    {
      "epoch": 3.8602666666666665,
      "grad_norm": 0.21189247071743011,
      "learning_rate": 2.5873333333333332e-05,
      "loss": 0.0017,
      "step": 72380
    },
    {
      "epoch": 3.8608000000000002,
      "grad_norm": 0.09966012835502625,
      "learning_rate": 2.587e-05,
      "loss": 0.0014,
      "step": 72390
    },
    {
      "epoch": 3.8613333333333335,
      "grad_norm": 0.05713281035423279,
      "learning_rate": 2.5866666666666667e-05,
      "loss": 0.0012,
      "step": 72400
    },
    {
      "epoch": 3.861866666666667,
      "grad_norm": 0.32018861174583435,
      "learning_rate": 2.5863333333333334e-05,
      "loss": 0.0012,
      "step": 72410
    },
    {
      "epoch": 3.8624,
      "grad_norm": 0.0581418052315712,
      "learning_rate": 2.586e-05,
      "loss": 0.0013,
      "step": 72420
    },
    {
      "epoch": 3.8629333333333333,
      "grad_norm": 0.22220948338508606,
      "learning_rate": 2.585666666666667e-05,
      "loss": 0.0017,
      "step": 72430
    },
    {
      "epoch": 3.8634666666666666,
      "grad_norm": 0.21724700927734375,
      "learning_rate": 2.5853333333333335e-05,
      "loss": 0.0018,
      "step": 72440
    },
    {
      "epoch": 3.864,
      "grad_norm": 0.03663932532072067,
      "learning_rate": 2.585e-05,
      "loss": 0.0024,
      "step": 72450
    },
    {
      "epoch": 3.864533333333333,
      "grad_norm": 0.41650959849357605,
      "learning_rate": 2.5846666666666668e-05,
      "loss": 0.0017,
      "step": 72460
    },
    {
      "epoch": 3.8650666666666664,
      "grad_norm": 0.03708699345588684,
      "learning_rate": 2.5843333333333337e-05,
      "loss": 0.0013,
      "step": 72470
    },
    {
      "epoch": 3.8656,
      "grad_norm": 0.208967387676239,
      "learning_rate": 2.5840000000000003e-05,
      "loss": 0.002,
      "step": 72480
    },
    {
      "epoch": 3.8661333333333334,
      "grad_norm": 0.2490706890821457,
      "learning_rate": 2.583666666666667e-05,
      "loss": 0.0026,
      "step": 72490
    },
    {
      "epoch": 3.8666666666666667,
      "grad_norm": 0.561120867729187,
      "learning_rate": 2.5833333333333336e-05,
      "loss": 0.0017,
      "step": 72500
    },
    {
      "epoch": 3.8672,
      "grad_norm": 0.2584211528301239,
      "learning_rate": 2.583e-05,
      "loss": 0.0029,
      "step": 72510
    },
    {
      "epoch": 3.8677333333333332,
      "grad_norm": 0.12649019062519073,
      "learning_rate": 2.5826666666666664e-05,
      "loss": 0.0025,
      "step": 72520
    },
    {
      "epoch": 3.8682666666666665,
      "grad_norm": 0.3325943946838379,
      "learning_rate": 2.5823333333333334e-05,
      "loss": 0.0017,
      "step": 72530
    },
    {
      "epoch": 3.8688000000000002,
      "grad_norm": 0.045790378004312515,
      "learning_rate": 2.582e-05,
      "loss": 0.002,
      "step": 72540
    },
    {
      "epoch": 3.8693333333333335,
      "grad_norm": 0.2547420859336853,
      "learning_rate": 2.5816666666666666e-05,
      "loss": 0.0018,
      "step": 72550
    },
    {
      "epoch": 3.869866666666667,
      "grad_norm": 0.1899537444114685,
      "learning_rate": 2.5813333333333332e-05,
      "loss": 0.0014,
      "step": 72560
    },
    {
      "epoch": 3.8704,
      "grad_norm": 0.09758792072534561,
      "learning_rate": 2.5810000000000002e-05,
      "loss": 0.0022,
      "step": 72570
    },
    {
      "epoch": 3.8709333333333333,
      "grad_norm": 0.044677577912807465,
      "learning_rate": 2.5806666666666668e-05,
      "loss": 0.0027,
      "step": 72580
    },
    {
      "epoch": 3.8714666666666666,
      "grad_norm": 0.2269715666770935,
      "learning_rate": 2.5803333333333334e-05,
      "loss": 0.0025,
      "step": 72590
    },
    {
      "epoch": 3.872,
      "grad_norm": 0.3609383702278137,
      "learning_rate": 2.58e-05,
      "loss": 0.0017,
      "step": 72600
    },
    {
      "epoch": 3.872533333333333,
      "grad_norm": 0.2057778686285019,
      "learning_rate": 2.579666666666667e-05,
      "loss": 0.0029,
      "step": 72610
    },
    {
      "epoch": 3.8730666666666664,
      "grad_norm": 0.10055617243051529,
      "learning_rate": 2.5793333333333336e-05,
      "loss": 0.0024,
      "step": 72620
    },
    {
      "epoch": 3.8736,
      "grad_norm": 0.08319812268018723,
      "learning_rate": 2.5790000000000002e-05,
      "loss": 0.0021,
      "step": 72630
    },
    {
      "epoch": 3.8741333333333334,
      "grad_norm": 0.5113651752471924,
      "learning_rate": 2.578666666666667e-05,
      "loss": 0.0027,
      "step": 72640
    },
    {
      "epoch": 3.8746666666666667,
      "grad_norm": 0.36301189661026,
      "learning_rate": 2.5783333333333338e-05,
      "loss": 0.0023,
      "step": 72650
    },
    {
      "epoch": 3.8752,
      "grad_norm": 0.15007524192333221,
      "learning_rate": 2.5779999999999997e-05,
      "loss": 0.0018,
      "step": 72660
    },
    {
      "epoch": 3.8757333333333333,
      "grad_norm": 0.19450914859771729,
      "learning_rate": 2.5776666666666667e-05,
      "loss": 0.0013,
      "step": 72670
    },
    {
      "epoch": 3.8762666666666665,
      "grad_norm": 0.04756425321102142,
      "learning_rate": 2.5773333333333333e-05,
      "loss": 0.0024,
      "step": 72680
    },
    {
      "epoch": 3.8768000000000002,
      "grad_norm": 0.27531030774116516,
      "learning_rate": 2.577e-05,
      "loss": 0.0022,
      "step": 72690
    },
    {
      "epoch": 3.8773333333333335,
      "grad_norm": 0.4787197411060333,
      "learning_rate": 2.5766666666666665e-05,
      "loss": 0.0018,
      "step": 72700
    },
    {
      "epoch": 3.877866666666667,
      "grad_norm": 0.4105377197265625,
      "learning_rate": 2.5763333333333335e-05,
      "loss": 0.0026,
      "step": 72710
    },
    {
      "epoch": 3.8784,
      "grad_norm": 0.27872994542121887,
      "learning_rate": 2.576e-05,
      "loss": 0.0016,
      "step": 72720
    },
    {
      "epoch": 3.8789333333333333,
      "grad_norm": 0.621268630027771,
      "learning_rate": 2.5756666666666667e-05,
      "loss": 0.0021,
      "step": 72730
    },
    {
      "epoch": 3.8794666666666666,
      "grad_norm": 0.5404229760169983,
      "learning_rate": 2.5753333333333336e-05,
      "loss": 0.0017,
      "step": 72740
    },
    {
      "epoch": 3.88,
      "grad_norm": 0.11072706431150436,
      "learning_rate": 2.5750000000000002e-05,
      "loss": 0.0028,
      "step": 72750
    },
    {
      "epoch": 3.880533333333333,
      "grad_norm": 0.10153575241565704,
      "learning_rate": 2.574666666666667e-05,
      "loss": 0.0017,
      "step": 72760
    },
    {
      "epoch": 3.8810666666666664,
      "grad_norm": 0.17222730815410614,
      "learning_rate": 2.5743333333333335e-05,
      "loss": 0.0018,
      "step": 72770
    },
    {
      "epoch": 3.8816,
      "grad_norm": 0.21004365384578705,
      "learning_rate": 2.5740000000000004e-05,
      "loss": 0.0015,
      "step": 72780
    },
    {
      "epoch": 3.8821333333333334,
      "grad_norm": 0.22988921403884888,
      "learning_rate": 2.573666666666667e-05,
      "loss": 0.0021,
      "step": 72790
    },
    {
      "epoch": 3.8826666666666667,
      "grad_norm": 0.08960992097854614,
      "learning_rate": 2.5733333333333337e-05,
      "loss": 0.0027,
      "step": 72800
    },
    {
      "epoch": 3.8832,
      "grad_norm": 0.4302302896976471,
      "learning_rate": 2.573e-05,
      "loss": 0.0023,
      "step": 72810
    },
    {
      "epoch": 3.8837333333333333,
      "grad_norm": 0.1343899965286255,
      "learning_rate": 2.5726666666666665e-05,
      "loss": 0.002,
      "step": 72820
    },
    {
      "epoch": 3.8842666666666665,
      "grad_norm": 0.07297954708337784,
      "learning_rate": 2.572333333333333e-05,
      "loss": 0.0018,
      "step": 72830
    },
    {
      "epoch": 3.8848000000000003,
      "grad_norm": 0.27727335691452026,
      "learning_rate": 2.572e-05,
      "loss": 0.002,
      "step": 72840
    },
    {
      "epoch": 3.8853333333333335,
      "grad_norm": 0.07329784333705902,
      "learning_rate": 2.5716666666666667e-05,
      "loss": 0.0023,
      "step": 72850
    },
    {
      "epoch": 3.885866666666667,
      "grad_norm": 0.20086506009101868,
      "learning_rate": 2.5713333333333333e-05,
      "loss": 0.0019,
      "step": 72860
    },
    {
      "epoch": 3.8864,
      "grad_norm": 0.07338026165962219,
      "learning_rate": 2.571e-05,
      "loss": 0.0017,
      "step": 72870
    },
    {
      "epoch": 3.8869333333333334,
      "grad_norm": 0.3118388056755066,
      "learning_rate": 2.570666666666667e-05,
      "loss": 0.0022,
      "step": 72880
    },
    {
      "epoch": 3.8874666666666666,
      "grad_norm": 0.3021927773952484,
      "learning_rate": 2.5703333333333335e-05,
      "loss": 0.0021,
      "step": 72890
    },
    {
      "epoch": 3.888,
      "grad_norm": 0.3700195550918579,
      "learning_rate": 2.57e-05,
      "loss": 0.0023,
      "step": 72900
    },
    {
      "epoch": 3.888533333333333,
      "grad_norm": 0.24606594443321228,
      "learning_rate": 2.5696666666666667e-05,
      "loss": 0.0016,
      "step": 72910
    },
    {
      "epoch": 3.8890666666666664,
      "grad_norm": 0.43755728006362915,
      "learning_rate": 2.5693333333333337e-05,
      "loss": 0.0026,
      "step": 72920
    },
    {
      "epoch": 3.8895999999999997,
      "grad_norm": 0.19663769006729126,
      "learning_rate": 2.5690000000000003e-05,
      "loss": 0.002,
      "step": 72930
    },
    {
      "epoch": 3.8901333333333334,
      "grad_norm": 0.6235021948814392,
      "learning_rate": 2.568666666666667e-05,
      "loss": 0.0017,
      "step": 72940
    },
    {
      "epoch": 3.8906666666666667,
      "grad_norm": 0.09846559166908264,
      "learning_rate": 2.5683333333333335e-05,
      "loss": 0.0027,
      "step": 72950
    },
    {
      "epoch": 3.8912,
      "grad_norm": 0.16061928868293762,
      "learning_rate": 2.5679999999999998e-05,
      "loss": 0.0024,
      "step": 72960
    },
    {
      "epoch": 3.8917333333333333,
      "grad_norm": 0.11810483783483505,
      "learning_rate": 2.5676666666666664e-05,
      "loss": 0.0014,
      "step": 72970
    },
    {
      "epoch": 3.8922666666666665,
      "grad_norm": 0.06299803406000137,
      "learning_rate": 2.5673333333333334e-05,
      "loss": 0.0016,
      "step": 72980
    },
    {
      "epoch": 3.8928000000000003,
      "grad_norm": 0.22114168107509613,
      "learning_rate": 2.567e-05,
      "loss": 0.0015,
      "step": 72990
    },
    {
      "epoch": 3.8933333333333335,
      "grad_norm": 0.20913894474506378,
      "learning_rate": 2.5666666666666666e-05,
      "loss": 0.0017,
      "step": 73000
    },
    {
      "epoch": 3.893866666666667,
      "grad_norm": 0.18766820430755615,
      "learning_rate": 2.5663333333333332e-05,
      "loss": 0.0019,
      "step": 73010
    },
    {
      "epoch": 3.8944,
      "grad_norm": 0.09011705219745636,
      "learning_rate": 2.566e-05,
      "loss": 0.0024,
      "step": 73020
    },
    {
      "epoch": 3.8949333333333334,
      "grad_norm": 0.21778249740600586,
      "learning_rate": 2.5656666666666668e-05,
      "loss": 0.0019,
      "step": 73030
    },
    {
      "epoch": 3.8954666666666666,
      "grad_norm": 0.3520147204399109,
      "learning_rate": 2.5653333333333334e-05,
      "loss": 0.0016,
      "step": 73040
    },
    {
      "epoch": 3.896,
      "grad_norm": 0.3108234405517578,
      "learning_rate": 2.5650000000000003e-05,
      "loss": 0.0021,
      "step": 73050
    },
    {
      "epoch": 3.896533333333333,
      "grad_norm": 0.07813718169927597,
      "learning_rate": 2.564666666666667e-05,
      "loss": 0.002,
      "step": 73060
    },
    {
      "epoch": 3.8970666666666665,
      "grad_norm": 0.44417646527290344,
      "learning_rate": 2.5643333333333336e-05,
      "loss": 0.0018,
      "step": 73070
    },
    {
      "epoch": 3.8975999999999997,
      "grad_norm": 0.1353168934583664,
      "learning_rate": 2.5640000000000002e-05,
      "loss": 0.0018,
      "step": 73080
    },
    {
      "epoch": 3.8981333333333335,
      "grad_norm": 0.2263648509979248,
      "learning_rate": 2.563666666666667e-05,
      "loss": 0.0025,
      "step": 73090
    },
    {
      "epoch": 3.8986666666666667,
      "grad_norm": 0.28862565755844116,
      "learning_rate": 2.5633333333333338e-05,
      "loss": 0.0015,
      "step": 73100
    },
    {
      "epoch": 3.8992,
      "grad_norm": 0.1267763376235962,
      "learning_rate": 2.5629999999999997e-05,
      "loss": 0.002,
      "step": 73110
    },
    {
      "epoch": 3.8997333333333333,
      "grad_norm": 0.30888429284095764,
      "learning_rate": 2.5626666666666666e-05,
      "loss": 0.0031,
      "step": 73120
    },
    {
      "epoch": 3.9002666666666665,
      "grad_norm": 0.17985665798187256,
      "learning_rate": 2.5623333333333333e-05,
      "loss": 0.0016,
      "step": 73130
    },
    {
      "epoch": 3.9008000000000003,
      "grad_norm": 0.05628550797700882,
      "learning_rate": 2.562e-05,
      "loss": 0.0021,
      "step": 73140
    },
    {
      "epoch": 3.9013333333333335,
      "grad_norm": 0.27686241269111633,
      "learning_rate": 2.5616666666666668e-05,
      "loss": 0.0023,
      "step": 73150
    },
    {
      "epoch": 3.901866666666667,
      "grad_norm": 0.19369952380657196,
      "learning_rate": 2.5613333333333334e-05,
      "loss": 0.002,
      "step": 73160
    },
    {
      "epoch": 3.9024,
      "grad_norm": 0.16643638908863068,
      "learning_rate": 2.561e-05,
      "loss": 0.0019,
      "step": 73170
    },
    {
      "epoch": 3.9029333333333334,
      "grad_norm": 0.2763637900352478,
      "learning_rate": 2.5606666666666667e-05,
      "loss": 0.0022,
      "step": 73180
    },
    {
      "epoch": 3.9034666666666666,
      "grad_norm": 0.07851814478635788,
      "learning_rate": 2.5603333333333336e-05,
      "loss": 0.0018,
      "step": 73190
    },
    {
      "epoch": 3.904,
      "grad_norm": 0.7175031900405884,
      "learning_rate": 2.5600000000000002e-05,
      "loss": 0.0025,
      "step": 73200
    },
    {
      "epoch": 3.904533333333333,
      "grad_norm": 0.28353849053382874,
      "learning_rate": 2.559666666666667e-05,
      "loss": 0.0024,
      "step": 73210
    },
    {
      "epoch": 3.9050666666666665,
      "grad_norm": 0.1469177007675171,
      "learning_rate": 2.5593333333333334e-05,
      "loss": 0.0015,
      "step": 73220
    },
    {
      "epoch": 3.9055999999999997,
      "grad_norm": 0.11934313923120499,
      "learning_rate": 2.5590000000000004e-05,
      "loss": 0.0023,
      "step": 73230
    },
    {
      "epoch": 3.9061333333333335,
      "grad_norm": 0.14864997565746307,
      "learning_rate": 2.558666666666667e-05,
      "loss": 0.003,
      "step": 73240
    },
    {
      "epoch": 3.9066666666666667,
      "grad_norm": 0.2802167534828186,
      "learning_rate": 2.5583333333333336e-05,
      "loss": 0.0017,
      "step": 73250
    },
    {
      "epoch": 3.9072,
      "grad_norm": 0.34072595834732056,
      "learning_rate": 2.5580000000000002e-05,
      "loss": 0.0035,
      "step": 73260
    },
    {
      "epoch": 3.9077333333333333,
      "grad_norm": 0.39174869656562805,
      "learning_rate": 2.5576666666666665e-05,
      "loss": 0.0017,
      "step": 73270
    },
    {
      "epoch": 3.9082666666666666,
      "grad_norm": 0.059696346521377563,
      "learning_rate": 2.557333333333333e-05,
      "loss": 0.0017,
      "step": 73280
    },
    {
      "epoch": 3.9088000000000003,
      "grad_norm": 0.4180399179458618,
      "learning_rate": 2.557e-05,
      "loss": 0.0021,
      "step": 73290
    },
    {
      "epoch": 3.9093333333333335,
      "grad_norm": 0.4795379936695099,
      "learning_rate": 2.5566666666666667e-05,
      "loss": 0.0028,
      "step": 73300
    },
    {
      "epoch": 3.909866666666667,
      "grad_norm": 0.6511055827140808,
      "learning_rate": 2.5563333333333333e-05,
      "loss": 0.0018,
      "step": 73310
    },
    {
      "epoch": 3.9104,
      "grad_norm": 0.4283462166786194,
      "learning_rate": 2.556e-05,
      "loss": 0.0018,
      "step": 73320
    },
    {
      "epoch": 3.9109333333333334,
      "grad_norm": 0.046171560883522034,
      "learning_rate": 2.555666666666667e-05,
      "loss": 0.0018,
      "step": 73330
    },
    {
      "epoch": 3.9114666666666666,
      "grad_norm": 0.15964914858341217,
      "learning_rate": 2.5553333333333335e-05,
      "loss": 0.0014,
      "step": 73340
    },
    {
      "epoch": 3.912,
      "grad_norm": 0.0795951560139656,
      "learning_rate": 2.555e-05,
      "loss": 0.0019,
      "step": 73350
    },
    {
      "epoch": 3.912533333333333,
      "grad_norm": 0.19580066204071045,
      "learning_rate": 2.5546666666666667e-05,
      "loss": 0.0023,
      "step": 73360
    },
    {
      "epoch": 3.9130666666666665,
      "grad_norm": 0.18319079279899597,
      "learning_rate": 2.5543333333333337e-05,
      "loss": 0.0017,
      "step": 73370
    },
    {
      "epoch": 3.9135999999999997,
      "grad_norm": 0.39532139897346497,
      "learning_rate": 2.5540000000000003e-05,
      "loss": 0.0024,
      "step": 73380
    },
    {
      "epoch": 3.9141333333333335,
      "grad_norm": 0.36852148175239563,
      "learning_rate": 2.553666666666667e-05,
      "loss": 0.0031,
      "step": 73390
    },
    {
      "epoch": 3.9146666666666667,
      "grad_norm": 0.2661796510219574,
      "learning_rate": 2.553333333333334e-05,
      "loss": 0.0022,
      "step": 73400
    },
    {
      "epoch": 3.9152,
      "grad_norm": 0.14353208243846893,
      "learning_rate": 2.5530000000000005e-05,
      "loss": 0.0018,
      "step": 73410
    },
    {
      "epoch": 3.9157333333333333,
      "grad_norm": 0.280830055475235,
      "learning_rate": 2.5526666666666664e-05,
      "loss": 0.0019,
      "step": 73420
    },
    {
      "epoch": 3.9162666666666666,
      "grad_norm": 0.09637222439050674,
      "learning_rate": 2.5523333333333333e-05,
      "loss": 0.0013,
      "step": 73430
    },
    {
      "epoch": 3.9168,
      "grad_norm": 0.5473899841308594,
      "learning_rate": 2.552e-05,
      "loss": 0.002,
      "step": 73440
    },
    {
      "epoch": 3.9173333333333336,
      "grad_norm": 0.4882226884365082,
      "learning_rate": 2.5516666666666666e-05,
      "loss": 0.0029,
      "step": 73450
    },
    {
      "epoch": 3.917866666666667,
      "grad_norm": 0.30061593651771545,
      "learning_rate": 2.5513333333333332e-05,
      "loss": 0.0021,
      "step": 73460
    },
    {
      "epoch": 3.9184,
      "grad_norm": 0.044567566365003586,
      "learning_rate": 2.551e-05,
      "loss": 0.0024,
      "step": 73470
    },
    {
      "epoch": 3.9189333333333334,
      "grad_norm": 0.09605889767408371,
      "learning_rate": 2.5506666666666668e-05,
      "loss": 0.0021,
      "step": 73480
    },
    {
      "epoch": 3.9194666666666667,
      "grad_norm": 0.170262411236763,
      "learning_rate": 2.5503333333333334e-05,
      "loss": 0.0014,
      "step": 73490
    },
    {
      "epoch": 3.92,
      "grad_norm": 0.18495862185955048,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 0.0025,
      "step": 73500
    },
    {
      "epoch": 3.920533333333333,
      "grad_norm": 0.06674399971961975,
      "learning_rate": 2.549666666666667e-05,
      "loss": 0.0029,
      "step": 73510
    },
    {
      "epoch": 3.9210666666666665,
      "grad_norm": 0.39866262674331665,
      "learning_rate": 2.5493333333333335e-05,
      "loss": 0.0021,
      "step": 73520
    },
    {
      "epoch": 3.9215999999999998,
      "grad_norm": 0.3654364347457886,
      "learning_rate": 2.549e-05,
      "loss": 0.0021,
      "step": 73530
    },
    {
      "epoch": 3.9221333333333335,
      "grad_norm": 0.20053833723068237,
      "learning_rate": 2.548666666666667e-05,
      "loss": 0.0021,
      "step": 73540
    },
    {
      "epoch": 3.9226666666666667,
      "grad_norm": 0.5230967402458191,
      "learning_rate": 2.5483333333333337e-05,
      "loss": 0.0015,
      "step": 73550
    },
    {
      "epoch": 3.9232,
      "grad_norm": 0.19480764865875244,
      "learning_rate": 2.5480000000000003e-05,
      "loss": 0.0022,
      "step": 73560
    },
    {
      "epoch": 3.9237333333333333,
      "grad_norm": 0.18981578946113586,
      "learning_rate": 2.5476666666666666e-05,
      "loss": 0.0028,
      "step": 73570
    },
    {
      "epoch": 3.9242666666666666,
      "grad_norm": 0.3050960600376129,
      "learning_rate": 2.5473333333333332e-05,
      "loss": 0.0025,
      "step": 73580
    },
    {
      "epoch": 3.9248,
      "grad_norm": 0.2844991683959961,
      "learning_rate": 2.547e-05,
      "loss": 0.0016,
      "step": 73590
    },
    {
      "epoch": 3.9253333333333336,
      "grad_norm": 0.09271221607923508,
      "learning_rate": 2.5466666666666668e-05,
      "loss": 0.0015,
      "step": 73600
    },
    {
      "epoch": 3.925866666666667,
      "grad_norm": 0.18278254568576813,
      "learning_rate": 2.5463333333333334e-05,
      "loss": 0.002,
      "step": 73610
    },
    {
      "epoch": 3.9264,
      "grad_norm": 0.6636500358581543,
      "learning_rate": 2.546e-05,
      "loss": 0.0016,
      "step": 73620
    },
    {
      "epoch": 3.9269333333333334,
      "grad_norm": 0.123885877430439,
      "learning_rate": 2.5456666666666666e-05,
      "loss": 0.0018,
      "step": 73630
    },
    {
      "epoch": 3.9274666666666667,
      "grad_norm": 0.22760295867919922,
      "learning_rate": 2.5453333333333336e-05,
      "loss": 0.0013,
      "step": 73640
    },
    {
      "epoch": 3.928,
      "grad_norm": 0.16628074645996094,
      "learning_rate": 2.5450000000000002e-05,
      "loss": 0.0019,
      "step": 73650
    },
    {
      "epoch": 3.928533333333333,
      "grad_norm": 0.2048054039478302,
      "learning_rate": 2.5446666666666668e-05,
      "loss": 0.0016,
      "step": 73660
    },
    {
      "epoch": 3.9290666666666665,
      "grad_norm": 0.18585288524627686,
      "learning_rate": 2.5443333333333334e-05,
      "loss": 0.0024,
      "step": 73670
    },
    {
      "epoch": 3.9295999999999998,
      "grad_norm": 0.4077847898006439,
      "learning_rate": 2.5440000000000004e-05,
      "loss": 0.0018,
      "step": 73680
    },
    {
      "epoch": 3.9301333333333335,
      "grad_norm": 0.04761967435479164,
      "learning_rate": 2.543666666666667e-05,
      "loss": 0.0016,
      "step": 73690
    },
    {
      "epoch": 3.9306666666666668,
      "grad_norm": 0.5962885022163391,
      "learning_rate": 2.5433333333333336e-05,
      "loss": 0.0028,
      "step": 73700
    },
    {
      "epoch": 3.9312,
      "grad_norm": 0.3853219449520111,
      "learning_rate": 2.5430000000000002e-05,
      "loss": 0.0021,
      "step": 73710
    },
    {
      "epoch": 3.9317333333333333,
      "grad_norm": 0.11522547900676727,
      "learning_rate": 2.5426666666666665e-05,
      "loss": 0.0023,
      "step": 73720
    },
    {
      "epoch": 3.9322666666666666,
      "grad_norm": 0.5021898150444031,
      "learning_rate": 2.542333333333333e-05,
      "loss": 0.0022,
      "step": 73730
    },
    {
      "epoch": 3.9328,
      "grad_norm": 0.5352611541748047,
      "learning_rate": 2.542e-05,
      "loss": 0.0015,
      "step": 73740
    },
    {
      "epoch": 3.9333333333333336,
      "grad_norm": 0.059257809072732925,
      "learning_rate": 2.5416666666666667e-05,
      "loss": 0.0014,
      "step": 73750
    },
    {
      "epoch": 3.933866666666667,
      "grad_norm": 0.031571805477142334,
      "learning_rate": 2.5413333333333333e-05,
      "loss": 0.0018,
      "step": 73760
    },
    {
      "epoch": 3.9344,
      "grad_norm": 0.18676044046878815,
      "learning_rate": 2.541e-05,
      "loss": 0.0015,
      "step": 73770
    },
    {
      "epoch": 3.9349333333333334,
      "grad_norm": 0.33973172307014465,
      "learning_rate": 2.540666666666667e-05,
      "loss": 0.0023,
      "step": 73780
    },
    {
      "epoch": 3.9354666666666667,
      "grad_norm": 0.15584363043308258,
      "learning_rate": 2.5403333333333335e-05,
      "loss": 0.0022,
      "step": 73790
    },
    {
      "epoch": 3.936,
      "grad_norm": 0.5810901522636414,
      "learning_rate": 2.54e-05,
      "loss": 0.0021,
      "step": 73800
    },
    {
      "epoch": 3.936533333333333,
      "grad_norm": 0.25934261083602905,
      "learning_rate": 2.539666666666667e-05,
      "loss": 0.0021,
      "step": 73810
    },
    {
      "epoch": 3.9370666666666665,
      "grad_norm": 0.38022804260253906,
      "learning_rate": 2.5393333333333336e-05,
      "loss": 0.0016,
      "step": 73820
    },
    {
      "epoch": 3.9375999999999998,
      "grad_norm": 0.34571677446365356,
      "learning_rate": 2.5390000000000003e-05,
      "loss": 0.0016,
      "step": 73830
    },
    {
      "epoch": 3.9381333333333335,
      "grad_norm": 0.1943604201078415,
      "learning_rate": 2.538666666666667e-05,
      "loss": 0.0017,
      "step": 73840
    },
    {
      "epoch": 3.9386666666666668,
      "grad_norm": 0.12252818793058395,
      "learning_rate": 2.5383333333333338e-05,
      "loss": 0.0014,
      "step": 73850
    },
    {
      "epoch": 3.9392,
      "grad_norm": 0.38205116987228394,
      "learning_rate": 2.5380000000000004e-05,
      "loss": 0.0025,
      "step": 73860
    },
    {
      "epoch": 3.9397333333333333,
      "grad_norm": 0.21471185982227325,
      "learning_rate": 2.5376666666666664e-05,
      "loss": 0.0022,
      "step": 73870
    },
    {
      "epoch": 3.9402666666666666,
      "grad_norm": 0.04324996471405029,
      "learning_rate": 2.5373333333333333e-05,
      "loss": 0.0027,
      "step": 73880
    },
    {
      "epoch": 3.9408,
      "grad_norm": 0.09281183034181595,
      "learning_rate": 2.537e-05,
      "loss": 0.002,
      "step": 73890
    },
    {
      "epoch": 3.9413333333333336,
      "grad_norm": 0.1949855238199234,
      "learning_rate": 2.5366666666666665e-05,
      "loss": 0.0017,
      "step": 73900
    },
    {
      "epoch": 3.941866666666667,
      "grad_norm": 0.21353822946548462,
      "learning_rate": 2.5363333333333335e-05,
      "loss": 0.0024,
      "step": 73910
    },
    {
      "epoch": 3.9424,
      "grad_norm": 0.18450815975666046,
      "learning_rate": 2.536e-05,
      "loss": 0.0023,
      "step": 73920
    },
    {
      "epoch": 3.9429333333333334,
      "grad_norm": 0.3911765515804291,
      "learning_rate": 2.5356666666666667e-05,
      "loss": 0.002,
      "step": 73930
    },
    {
      "epoch": 3.9434666666666667,
      "grad_norm": 0.38545164465904236,
      "learning_rate": 2.5353333333333333e-05,
      "loss": 0.0013,
      "step": 73940
    },
    {
      "epoch": 3.944,
      "grad_norm": 0.040218252688646317,
      "learning_rate": 2.5350000000000003e-05,
      "loss": 0.0025,
      "step": 73950
    },
    {
      "epoch": 3.9445333333333332,
      "grad_norm": 0.059953924268484116,
      "learning_rate": 2.534666666666667e-05,
      "loss": 0.0015,
      "step": 73960
    },
    {
      "epoch": 3.9450666666666665,
      "grad_norm": 0.3693430721759796,
      "learning_rate": 2.5343333333333335e-05,
      "loss": 0.0013,
      "step": 73970
    },
    {
      "epoch": 3.9455999999999998,
      "grad_norm": 0.12673319876194,
      "learning_rate": 2.534e-05,
      "loss": 0.0022,
      "step": 73980
    },
    {
      "epoch": 3.9461333333333335,
      "grad_norm": 0.09138493239879608,
      "learning_rate": 2.533666666666667e-05,
      "loss": 0.0018,
      "step": 73990
    },
    {
      "epoch": 3.9466666666666668,
      "grad_norm": 0.5574827194213867,
      "learning_rate": 2.5333333333333337e-05,
      "loss": 0.0021,
      "step": 74000
    },
    {
      "epoch": 3.9472,
      "grad_norm": 0.6592774987220764,
      "learning_rate": 2.5330000000000003e-05,
      "loss": 0.002,
      "step": 74010
    },
    {
      "epoch": 3.9477333333333333,
      "grad_norm": 0.03552810847759247,
      "learning_rate": 2.5326666666666666e-05,
      "loss": 0.0016,
      "step": 74020
    },
    {
      "epoch": 3.9482666666666666,
      "grad_norm": 0.1268719583749771,
      "learning_rate": 2.5323333333333332e-05,
      "loss": 0.0026,
      "step": 74030
    },
    {
      "epoch": 3.9488,
      "grad_norm": 0.552230715751648,
      "learning_rate": 2.5319999999999998e-05,
      "loss": 0.0021,
      "step": 74040
    },
    {
      "epoch": 3.9493333333333336,
      "grad_norm": 0.3663022220134735,
      "learning_rate": 2.5316666666666668e-05,
      "loss": 0.0021,
      "step": 74050
    },
    {
      "epoch": 3.949866666666667,
      "grad_norm": 0.4319116473197937,
      "learning_rate": 2.5313333333333334e-05,
      "loss": 0.0025,
      "step": 74060
    },
    {
      "epoch": 3.9504,
      "grad_norm": 0.07984410971403122,
      "learning_rate": 2.531e-05,
      "loss": 0.0018,
      "step": 74070
    },
    {
      "epoch": 3.9509333333333334,
      "grad_norm": 0.10666367411613464,
      "learning_rate": 2.5306666666666666e-05,
      "loss": 0.0014,
      "step": 74080
    },
    {
      "epoch": 3.9514666666666667,
      "grad_norm": 0.3010062277317047,
      "learning_rate": 2.5303333333333336e-05,
      "loss": 0.0021,
      "step": 74090
    },
    {
      "epoch": 3.952,
      "grad_norm": 0.3933168947696686,
      "learning_rate": 2.5300000000000002e-05,
      "loss": 0.0025,
      "step": 74100
    },
    {
      "epoch": 3.9525333333333332,
      "grad_norm": 0.1262117326259613,
      "learning_rate": 2.5296666666666668e-05,
      "loss": 0.0018,
      "step": 74110
    },
    {
      "epoch": 3.9530666666666665,
      "grad_norm": 0.48567885160446167,
      "learning_rate": 2.5293333333333334e-05,
      "loss": 0.0024,
      "step": 74120
    },
    {
      "epoch": 3.9536,
      "grad_norm": 0.0724903792142868,
      "learning_rate": 2.5290000000000004e-05,
      "loss": 0.0016,
      "step": 74130
    },
    {
      "epoch": 3.9541333333333335,
      "grad_norm": 0.09732257574796677,
      "learning_rate": 2.528666666666667e-05,
      "loss": 0.0018,
      "step": 74140
    },
    {
      "epoch": 3.9546666666666668,
      "grad_norm": 0.2805677056312561,
      "learning_rate": 2.5283333333333336e-05,
      "loss": 0.0014,
      "step": 74150
    },
    {
      "epoch": 3.9552,
      "grad_norm": 0.30914628505706787,
      "learning_rate": 2.5280000000000005e-05,
      "loss": 0.002,
      "step": 74160
    },
    {
      "epoch": 3.9557333333333333,
      "grad_norm": 0.1843804568052292,
      "learning_rate": 2.5276666666666665e-05,
      "loss": 0.0016,
      "step": 74170
    },
    {
      "epoch": 3.9562666666666666,
      "grad_norm": 0.39550378918647766,
      "learning_rate": 2.527333333333333e-05,
      "loss": 0.0019,
      "step": 74180
    },
    {
      "epoch": 3.9568,
      "grad_norm": 0.21021941304206848,
      "learning_rate": 2.527e-05,
      "loss": 0.0015,
      "step": 74190
    },
    {
      "epoch": 3.9573333333333336,
      "grad_norm": 0.09656316041946411,
      "learning_rate": 2.5266666666666666e-05,
      "loss": 0.0016,
      "step": 74200
    },
    {
      "epoch": 3.957866666666667,
      "grad_norm": 0.3370872139930725,
      "learning_rate": 2.5263333333333333e-05,
      "loss": 0.0023,
      "step": 74210
    },
    {
      "epoch": 3.9584,
      "grad_norm": 0.532397985458374,
      "learning_rate": 2.526e-05,
      "loss": 0.0023,
      "step": 74220
    },
    {
      "epoch": 3.9589333333333334,
      "grad_norm": 0.08254753798246384,
      "learning_rate": 2.5256666666666668e-05,
      "loss": 0.0032,
      "step": 74230
    },
    {
      "epoch": 3.9594666666666667,
      "grad_norm": 0.04221585392951965,
      "learning_rate": 2.5253333333333334e-05,
      "loss": 0.0021,
      "step": 74240
    },
    {
      "epoch": 3.96,
      "grad_norm": 0.05782754719257355,
      "learning_rate": 2.525e-05,
      "loss": 0.0023,
      "step": 74250
    },
    {
      "epoch": 3.9605333333333332,
      "grad_norm": 0.24473820626735687,
      "learning_rate": 2.524666666666667e-05,
      "loss": 0.0022,
      "step": 74260
    },
    {
      "epoch": 3.9610666666666665,
      "grad_norm": 0.36786675453186035,
      "learning_rate": 2.5243333333333336e-05,
      "loss": 0.0014,
      "step": 74270
    },
    {
      "epoch": 3.9616,
      "grad_norm": 0.07134320586919785,
      "learning_rate": 2.5240000000000002e-05,
      "loss": 0.0013,
      "step": 74280
    },
    {
      "epoch": 3.962133333333333,
      "grad_norm": 0.07453875243663788,
      "learning_rate": 2.523666666666667e-05,
      "loss": 0.0024,
      "step": 74290
    },
    {
      "epoch": 3.962666666666667,
      "grad_norm": 0.21009959280490875,
      "learning_rate": 2.5233333333333338e-05,
      "loss": 0.0026,
      "step": 74300
    },
    {
      "epoch": 3.9632,
      "grad_norm": 0.6896380186080933,
      "learning_rate": 2.5230000000000004e-05,
      "loss": 0.0015,
      "step": 74310
    },
    {
      "epoch": 3.9637333333333333,
      "grad_norm": 0.25673389434814453,
      "learning_rate": 2.5226666666666663e-05,
      "loss": 0.0012,
      "step": 74320
    },
    {
      "epoch": 3.9642666666666666,
      "grad_norm": 0.05074359476566315,
      "learning_rate": 2.5223333333333333e-05,
      "loss": 0.0029,
      "step": 74330
    },
    {
      "epoch": 3.9648,
      "grad_norm": 0.4844329357147217,
      "learning_rate": 2.522e-05,
      "loss": 0.0032,
      "step": 74340
    },
    {
      "epoch": 3.9653333333333336,
      "grad_norm": 0.2460431456565857,
      "learning_rate": 2.5216666666666665e-05,
      "loss": 0.0016,
      "step": 74350
    },
    {
      "epoch": 3.965866666666667,
      "grad_norm": 0.6139451861381531,
      "learning_rate": 2.5213333333333335e-05,
      "loss": 0.0021,
      "step": 74360
    },
    {
      "epoch": 3.9664,
      "grad_norm": 0.13465666770935059,
      "learning_rate": 2.521e-05,
      "loss": 0.0017,
      "step": 74370
    },
    {
      "epoch": 3.9669333333333334,
      "grad_norm": 0.6724286675453186,
      "learning_rate": 2.5206666666666667e-05,
      "loss": 0.0021,
      "step": 74380
    },
    {
      "epoch": 3.9674666666666667,
      "grad_norm": 0.34390395879745483,
      "learning_rate": 2.5203333333333333e-05,
      "loss": 0.0017,
      "step": 74390
    },
    {
      "epoch": 3.968,
      "grad_norm": 0.07923498004674911,
      "learning_rate": 2.5200000000000003e-05,
      "loss": 0.002,
      "step": 74400
    },
    {
      "epoch": 3.9685333333333332,
      "grad_norm": 0.1051853820681572,
      "learning_rate": 2.519666666666667e-05,
      "loss": 0.0016,
      "step": 74410
    },
    {
      "epoch": 3.9690666666666665,
      "grad_norm": 0.16036829352378845,
      "learning_rate": 2.5193333333333335e-05,
      "loss": 0.0016,
      "step": 74420
    },
    {
      "epoch": 3.9696,
      "grad_norm": 0.37866535782814026,
      "learning_rate": 2.519e-05,
      "loss": 0.0016,
      "step": 74430
    },
    {
      "epoch": 3.970133333333333,
      "grad_norm": 0.09059467911720276,
      "learning_rate": 2.518666666666667e-05,
      "loss": 0.0023,
      "step": 74440
    },
    {
      "epoch": 3.970666666666667,
      "grad_norm": 0.26957380771636963,
      "learning_rate": 2.5183333333333337e-05,
      "loss": 0.0028,
      "step": 74450
    },
    {
      "epoch": 3.9712,
      "grad_norm": 0.03793923929333687,
      "learning_rate": 2.5180000000000003e-05,
      "loss": 0.0016,
      "step": 74460
    },
    {
      "epoch": 3.9717333333333333,
      "grad_norm": 0.4046986699104309,
      "learning_rate": 2.517666666666667e-05,
      "loss": 0.0015,
      "step": 74470
    },
    {
      "epoch": 3.9722666666666666,
      "grad_norm": 0.10206333547830582,
      "learning_rate": 2.5173333333333332e-05,
      "loss": 0.0022,
      "step": 74480
    },
    {
      "epoch": 3.9728,
      "grad_norm": 0.1877981275320053,
      "learning_rate": 2.5169999999999998e-05,
      "loss": 0.0015,
      "step": 74490
    },
    {
      "epoch": 3.9733333333333336,
      "grad_norm": 0.48619794845581055,
      "learning_rate": 2.5166666666666667e-05,
      "loss": 0.0018,
      "step": 74500
    },
    {
      "epoch": 3.973866666666667,
      "grad_norm": 0.09070096909999847,
      "learning_rate": 2.5163333333333334e-05,
      "loss": 0.0018,
      "step": 74510
    },
    {
      "epoch": 3.9744,
      "grad_norm": 0.1916443556547165,
      "learning_rate": 2.516e-05,
      "loss": 0.0016,
      "step": 74520
    },
    {
      "epoch": 3.9749333333333334,
      "grad_norm": 0.060534581542015076,
      "learning_rate": 2.5156666666666666e-05,
      "loss": 0.0019,
      "step": 74530
    },
    {
      "epoch": 3.9754666666666667,
      "grad_norm": 0.03804866597056389,
      "learning_rate": 2.5153333333333335e-05,
      "loss": 0.0019,
      "step": 74540
    },
    {
      "epoch": 3.976,
      "grad_norm": 0.09954853355884552,
      "learning_rate": 2.515e-05,
      "loss": 0.0027,
      "step": 74550
    },
    {
      "epoch": 3.9765333333333333,
      "grad_norm": 0.13005249202251434,
      "learning_rate": 2.5146666666666668e-05,
      "loss": 0.0014,
      "step": 74560
    },
    {
      "epoch": 3.9770666666666665,
      "grad_norm": 0.2869936525821686,
      "learning_rate": 2.5143333333333334e-05,
      "loss": 0.0015,
      "step": 74570
    },
    {
      "epoch": 3.9776,
      "grad_norm": 0.09684017300605774,
      "learning_rate": 2.5140000000000003e-05,
      "loss": 0.0022,
      "step": 74580
    },
    {
      "epoch": 3.978133333333333,
      "grad_norm": 0.03949108347296715,
      "learning_rate": 2.513666666666667e-05,
      "loss": 0.0022,
      "step": 74590
    },
    {
      "epoch": 3.978666666666667,
      "grad_norm": 0.08198844641447067,
      "learning_rate": 2.5133333333333336e-05,
      "loss": 0.0015,
      "step": 74600
    },
    {
      "epoch": 3.9792,
      "grad_norm": 0.48456525802612305,
      "learning_rate": 2.5130000000000005e-05,
      "loss": 0.002,
      "step": 74610
    },
    {
      "epoch": 3.9797333333333333,
      "grad_norm": 0.17237313091754913,
      "learning_rate": 2.512666666666667e-05,
      "loss": 0.0014,
      "step": 74620
    },
    {
      "epoch": 3.9802666666666666,
      "grad_norm": 0.6095788478851318,
      "learning_rate": 2.512333333333333e-05,
      "loss": 0.0028,
      "step": 74630
    },
    {
      "epoch": 3.9808,
      "grad_norm": 0.2135990858078003,
      "learning_rate": 2.512e-05,
      "loss": 0.0023,
      "step": 74640
    },
    {
      "epoch": 3.981333333333333,
      "grad_norm": 0.6173709034919739,
      "learning_rate": 2.5116666666666666e-05,
      "loss": 0.003,
      "step": 74650
    },
    {
      "epoch": 3.981866666666667,
      "grad_norm": 0.22773334383964539,
      "learning_rate": 2.5113333333333332e-05,
      "loss": 0.0023,
      "step": 74660
    },
    {
      "epoch": 3.9824,
      "grad_norm": 0.3453250229358673,
      "learning_rate": 2.5110000000000002e-05,
      "loss": 0.0018,
      "step": 74670
    },
    {
      "epoch": 3.9829333333333334,
      "grad_norm": 0.692835807800293,
      "learning_rate": 2.5106666666666668e-05,
      "loss": 0.0017,
      "step": 74680
    },
    {
      "epoch": 3.9834666666666667,
      "grad_norm": 0.4244310259819031,
      "learning_rate": 2.5103333333333334e-05,
      "loss": 0.0019,
      "step": 74690
    },
    {
      "epoch": 3.984,
      "grad_norm": 0.24704872071743011,
      "learning_rate": 2.51e-05,
      "loss": 0.0016,
      "step": 74700
    },
    {
      "epoch": 3.9845333333333333,
      "grad_norm": 0.600681722164154,
      "learning_rate": 2.509666666666667e-05,
      "loss": 0.0015,
      "step": 74710
    },
    {
      "epoch": 3.9850666666666665,
      "grad_norm": 0.11161071807146072,
      "learning_rate": 2.5093333333333336e-05,
      "loss": 0.0017,
      "step": 74720
    },
    {
      "epoch": 3.9856,
      "grad_norm": 0.42126646637916565,
      "learning_rate": 2.5090000000000002e-05,
      "loss": 0.0025,
      "step": 74730
    },
    {
      "epoch": 3.986133333333333,
      "grad_norm": 0.2501145601272583,
      "learning_rate": 2.5086666666666668e-05,
      "loss": 0.0018,
      "step": 74740
    },
    {
      "epoch": 3.986666666666667,
      "grad_norm": 0.4053356945514679,
      "learning_rate": 2.5083333333333338e-05,
      "loss": 0.002,
      "step": 74750
    },
    {
      "epoch": 3.9872,
      "grad_norm": 0.10640912503004074,
      "learning_rate": 2.5080000000000004e-05,
      "loss": 0.0016,
      "step": 74760
    },
    {
      "epoch": 3.9877333333333334,
      "grad_norm": 0.3484877347946167,
      "learning_rate": 2.507666666666667e-05,
      "loss": 0.0022,
      "step": 74770
    },
    {
      "epoch": 3.9882666666666666,
      "grad_norm": 0.4602094888687134,
      "learning_rate": 2.5073333333333333e-05,
      "loss": 0.0018,
      "step": 74780
    },
    {
      "epoch": 3.9888,
      "grad_norm": 0.22484157979488373,
      "learning_rate": 2.507e-05,
      "loss": 0.0019,
      "step": 74790
    },
    {
      "epoch": 3.989333333333333,
      "grad_norm": 0.10405959188938141,
      "learning_rate": 2.5066666666666665e-05,
      "loss": 0.0027,
      "step": 74800
    },
    {
      "epoch": 3.989866666666667,
      "grad_norm": 0.18875683844089508,
      "learning_rate": 2.5063333333333334e-05,
      "loss": 0.002,
      "step": 74810
    },
    {
      "epoch": 3.9904,
      "grad_norm": 0.4936274290084839,
      "learning_rate": 2.506e-05,
      "loss": 0.0017,
      "step": 74820
    },
    {
      "epoch": 3.9909333333333334,
      "grad_norm": 0.16126297414302826,
      "learning_rate": 2.5056666666666667e-05,
      "loss": 0.0027,
      "step": 74830
    },
    {
      "epoch": 3.9914666666666667,
      "grad_norm": 0.3114400804042816,
      "learning_rate": 2.5053333333333333e-05,
      "loss": 0.002,
      "step": 74840
    },
    {
      "epoch": 3.992,
      "grad_norm": 0.2886026203632355,
      "learning_rate": 2.5050000000000002e-05,
      "loss": 0.0027,
      "step": 74850
    },
    {
      "epoch": 3.9925333333333333,
      "grad_norm": 0.4064948856830597,
      "learning_rate": 2.504666666666667e-05,
      "loss": 0.0018,
      "step": 74860
    },
    {
      "epoch": 3.9930666666666665,
      "grad_norm": 0.3284454941749573,
      "learning_rate": 2.5043333333333335e-05,
      "loss": 0.0015,
      "step": 74870
    },
    {
      "epoch": 3.9936,
      "grad_norm": 0.0810447484254837,
      "learning_rate": 2.504e-05,
      "loss": 0.0025,
      "step": 74880
    },
    {
      "epoch": 3.994133333333333,
      "grad_norm": 0.19160082936286926,
      "learning_rate": 2.503666666666667e-05,
      "loss": 0.0017,
      "step": 74890
    },
    {
      "epoch": 3.994666666666667,
      "grad_norm": 0.22420769929885864,
      "learning_rate": 2.5033333333333336e-05,
      "loss": 0.002,
      "step": 74900
    },
    {
      "epoch": 3.9952,
      "grad_norm": 0.06542094796895981,
      "learning_rate": 2.5030000000000003e-05,
      "loss": 0.0015,
      "step": 74910
    },
    {
      "epoch": 3.9957333333333334,
      "grad_norm": 0.17998111248016357,
      "learning_rate": 2.5026666666666672e-05,
      "loss": 0.0017,
      "step": 74920
    },
    {
      "epoch": 3.9962666666666666,
      "grad_norm": 0.2465365082025528,
      "learning_rate": 2.502333333333333e-05,
      "loss": 0.0027,
      "step": 74930
    },
    {
      "epoch": 3.9968,
      "grad_norm": 0.08390919119119644,
      "learning_rate": 2.5019999999999998e-05,
      "loss": 0.0023,
      "step": 74940
    },
    {
      "epoch": 3.997333333333333,
      "grad_norm": 0.32611680030822754,
      "learning_rate": 2.5016666666666667e-05,
      "loss": 0.0018,
      "step": 74950
    },
    {
      "epoch": 3.997866666666667,
      "grad_norm": 0.2079695761203766,
      "learning_rate": 2.5013333333333333e-05,
      "loss": 0.0025,
      "step": 74960
    },
    {
      "epoch": 3.9984,
      "grad_norm": 0.2918411195278168,
      "learning_rate": 2.501e-05,
      "loss": 0.0022,
      "step": 74970
    },
    {
      "epoch": 3.9989333333333335,
      "grad_norm": 0.1356297880411148,
      "learning_rate": 2.5006666666666666e-05,
      "loss": 0.0023,
      "step": 74980
    },
    {
      "epoch": 3.9994666666666667,
      "grad_norm": 0.1698458045721054,
      "learning_rate": 2.5003333333333335e-05,
      "loss": 0.0021,
      "step": 74990
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.49550509452819824,
      "learning_rate": 2.5e-05,
      "loss": 0.0015,
      "step": 75000
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.002093804534524679,
      "eval_runtime": 163.566,
      "eval_samples_per_second": 1528.435,
      "eval_steps_per_second": 38.211,
      "step": 75000
    },
    {
      "epoch": 4.000533333333333,
      "grad_norm": 0.34729790687561035,
      "learning_rate": 2.4996666666666667e-05,
      "loss": 0.0019,
      "step": 75010
    },
    {
      "epoch": 4.0010666666666665,
      "grad_norm": 0.24221841990947723,
      "learning_rate": 2.4993333333333337e-05,
      "loss": 0.0023,
      "step": 75020
    },
    {
      "epoch": 4.0016,
      "grad_norm": 0.07286053150892258,
      "learning_rate": 2.4990000000000003e-05,
      "loss": 0.0016,
      "step": 75030
    },
    {
      "epoch": 4.002133333333333,
      "grad_norm": 0.11656486988067627,
      "learning_rate": 2.4986666666666666e-05,
      "loss": 0.0017,
      "step": 75040
    },
    {
      "epoch": 4.002666666666666,
      "grad_norm": 0.5260258316993713,
      "learning_rate": 2.4983333333333335e-05,
      "loss": 0.0017,
      "step": 75050
    },
    {
      "epoch": 4.0032,
      "grad_norm": 0.07194425165653229,
      "learning_rate": 2.498e-05,
      "loss": 0.0023,
      "step": 75060
    },
    {
      "epoch": 4.003733333333333,
      "grad_norm": 0.1808452159166336,
      "learning_rate": 2.4976666666666668e-05,
      "loss": 0.0029,
      "step": 75070
    },
    {
      "epoch": 4.004266666666667,
      "grad_norm": 0.38957369327545166,
      "learning_rate": 2.4973333333333334e-05,
      "loss": 0.0026,
      "step": 75080
    },
    {
      "epoch": 4.0048,
      "grad_norm": 0.13000300526618958,
      "learning_rate": 2.4970000000000003e-05,
      "loss": 0.002,
      "step": 75090
    },
    {
      "epoch": 4.005333333333334,
      "grad_norm": 0.17574524879455566,
      "learning_rate": 2.496666666666667e-05,
      "loss": 0.0028,
      "step": 75100
    },
    {
      "epoch": 4.005866666666667,
      "grad_norm": 0.057101212441921234,
      "learning_rate": 2.4963333333333335e-05,
      "loss": 0.0026,
      "step": 75110
    },
    {
      "epoch": 4.0064,
      "grad_norm": 0.27429407835006714,
      "learning_rate": 2.496e-05,
      "loss": 0.002,
      "step": 75120
    },
    {
      "epoch": 4.0069333333333335,
      "grad_norm": 0.07426179200410843,
      "learning_rate": 2.4956666666666668e-05,
      "loss": 0.0023,
      "step": 75130
    },
    {
      "epoch": 4.007466666666667,
      "grad_norm": 0.15736843645572662,
      "learning_rate": 2.4953333333333334e-05,
      "loss": 0.002,
      "step": 75140
    },
    {
      "epoch": 4.008,
      "grad_norm": 0.16327708959579468,
      "learning_rate": 2.495e-05,
      "loss": 0.0021,
      "step": 75150
    },
    {
      "epoch": 4.008533333333333,
      "grad_norm": 0.39396438002586365,
      "learning_rate": 2.494666666666667e-05,
      "loss": 0.0018,
      "step": 75160
    },
    {
      "epoch": 4.009066666666667,
      "grad_norm": 0.21428042650222778,
      "learning_rate": 2.4943333333333336e-05,
      "loss": 0.0025,
      "step": 75170
    },
    {
      "epoch": 4.0096,
      "grad_norm": 0.18363584578037262,
      "learning_rate": 2.4940000000000002e-05,
      "loss": 0.0015,
      "step": 75180
    },
    {
      "epoch": 4.010133333333333,
      "grad_norm": 0.18669486045837402,
      "learning_rate": 2.4936666666666668e-05,
      "loss": 0.0017,
      "step": 75190
    },
    {
      "epoch": 4.010666666666666,
      "grad_norm": 0.24209554493427277,
      "learning_rate": 2.4933333333333334e-05,
      "loss": 0.0023,
      "step": 75200
    },
    {
      "epoch": 4.0112,
      "grad_norm": 0.25227344036102295,
      "learning_rate": 2.493e-05,
      "loss": 0.0021,
      "step": 75210
    },
    {
      "epoch": 4.011733333333333,
      "grad_norm": 0.307204931974411,
      "learning_rate": 2.4926666666666666e-05,
      "loss": 0.0022,
      "step": 75220
    },
    {
      "epoch": 4.012266666666667,
      "grad_norm": 0.3461341857910156,
      "learning_rate": 2.4923333333333336e-05,
      "loss": 0.0025,
      "step": 75230
    },
    {
      "epoch": 4.0128,
      "grad_norm": 0.3434274196624756,
      "learning_rate": 2.4920000000000002e-05,
      "loss": 0.0018,
      "step": 75240
    },
    {
      "epoch": 4.013333333333334,
      "grad_norm": 0.695091962814331,
      "learning_rate": 2.4916666666666668e-05,
      "loss": 0.0024,
      "step": 75250
    },
    {
      "epoch": 4.013866666666667,
      "grad_norm": 0.26670143008232117,
      "learning_rate": 2.4913333333333334e-05,
      "loss": 0.0014,
      "step": 75260
    },
    {
      "epoch": 4.0144,
      "grad_norm": 0.06488796323537827,
      "learning_rate": 2.491e-05,
      "loss": 0.0014,
      "step": 75270
    },
    {
      "epoch": 4.0149333333333335,
      "grad_norm": 0.1527494490146637,
      "learning_rate": 2.4906666666666666e-05,
      "loss": 0.0025,
      "step": 75280
    },
    {
      "epoch": 4.015466666666667,
      "grad_norm": 0.10130396485328674,
      "learning_rate": 2.4903333333333333e-05,
      "loss": 0.0013,
      "step": 75290
    },
    {
      "epoch": 4.016,
      "grad_norm": 0.7427918910980225,
      "learning_rate": 2.4900000000000002e-05,
      "loss": 0.0025,
      "step": 75300
    },
    {
      "epoch": 4.016533333333333,
      "grad_norm": 0.8782077431678772,
      "learning_rate": 2.4896666666666668e-05,
      "loss": 0.0026,
      "step": 75310
    },
    {
      "epoch": 4.017066666666667,
      "grad_norm": 0.17647448182106018,
      "learning_rate": 2.4893333333333334e-05,
      "loss": 0.0015,
      "step": 75320
    },
    {
      "epoch": 4.0176,
      "grad_norm": 0.41257020831108093,
      "learning_rate": 2.489e-05,
      "loss": 0.0017,
      "step": 75330
    },
    {
      "epoch": 4.018133333333333,
      "grad_norm": 0.3177199363708496,
      "learning_rate": 2.488666666666667e-05,
      "loss": 0.0017,
      "step": 75340
    },
    {
      "epoch": 4.018666666666666,
      "grad_norm": 0.03929709643125534,
      "learning_rate": 2.4883333333333333e-05,
      "loss": 0.0019,
      "step": 75350
    },
    {
      "epoch": 4.0192,
      "grad_norm": 0.40346047282218933,
      "learning_rate": 2.488e-05,
      "loss": 0.0019,
      "step": 75360
    },
    {
      "epoch": 4.019733333333333,
      "grad_norm": 0.39502012729644775,
      "learning_rate": 2.487666666666667e-05,
      "loss": 0.002,
      "step": 75370
    },
    {
      "epoch": 4.020266666666667,
      "grad_norm": 0.5200619697570801,
      "learning_rate": 2.4873333333333335e-05,
      "loss": 0.0014,
      "step": 75380
    },
    {
      "epoch": 4.0208,
      "grad_norm": 0.3279939293861389,
      "learning_rate": 2.487e-05,
      "loss": 0.0016,
      "step": 75390
    },
    {
      "epoch": 4.021333333333334,
      "grad_norm": 0.09392452239990234,
      "learning_rate": 2.486666666666667e-05,
      "loss": 0.0018,
      "step": 75400
    },
    {
      "epoch": 4.021866666666667,
      "grad_norm": 0.15091964602470398,
      "learning_rate": 2.4863333333333336e-05,
      "loss": 0.0017,
      "step": 75410
    },
    {
      "epoch": 4.0224,
      "grad_norm": 0.20849476754665375,
      "learning_rate": 2.486e-05,
      "loss": 0.0022,
      "step": 75420
    },
    {
      "epoch": 4.0229333333333335,
      "grad_norm": 0.50172358751297,
      "learning_rate": 2.485666666666667e-05,
      "loss": 0.0037,
      "step": 75430
    },
    {
      "epoch": 4.023466666666667,
      "grad_norm": 0.15766946971416473,
      "learning_rate": 2.4853333333333335e-05,
      "loss": 0.0021,
      "step": 75440
    },
    {
      "epoch": 4.024,
      "grad_norm": 0.3286948800086975,
      "learning_rate": 2.485e-05,
      "loss": 0.0016,
      "step": 75450
    },
    {
      "epoch": 4.024533333333333,
      "grad_norm": 0.16523796319961548,
      "learning_rate": 2.4846666666666667e-05,
      "loss": 0.0024,
      "step": 75460
    },
    {
      "epoch": 4.025066666666667,
      "grad_norm": 0.10547591745853424,
      "learning_rate": 2.4843333333333337e-05,
      "loss": 0.0015,
      "step": 75470
    },
    {
      "epoch": 4.0256,
      "grad_norm": 0.09592804312705994,
      "learning_rate": 2.4840000000000003e-05,
      "loss": 0.0019,
      "step": 75480
    },
    {
      "epoch": 4.026133333333333,
      "grad_norm": 0.2695643901824951,
      "learning_rate": 2.483666666666667e-05,
      "loss": 0.0021,
      "step": 75490
    },
    {
      "epoch": 4.026666666666666,
      "grad_norm": 0.2554875612258911,
      "learning_rate": 2.4833333333333335e-05,
      "loss": 0.0022,
      "step": 75500
    },
    {
      "epoch": 4.0272,
      "grad_norm": 0.3075452148914337,
      "learning_rate": 2.483e-05,
      "loss": 0.0018,
      "step": 75510
    },
    {
      "epoch": 4.027733333333333,
      "grad_norm": 0.2460237741470337,
      "learning_rate": 2.4826666666666667e-05,
      "loss": 0.0015,
      "step": 75520
    },
    {
      "epoch": 4.028266666666667,
      "grad_norm": 0.12065498530864716,
      "learning_rate": 2.4823333333333333e-05,
      "loss": 0.0022,
      "step": 75530
    },
    {
      "epoch": 4.0288,
      "grad_norm": 0.12097055464982986,
      "learning_rate": 2.4820000000000003e-05,
      "loss": 0.0024,
      "step": 75540
    },
    {
      "epoch": 4.029333333333334,
      "grad_norm": 0.16138756275177002,
      "learning_rate": 2.481666666666667e-05,
      "loss": 0.0015,
      "step": 75550
    },
    {
      "epoch": 4.029866666666667,
      "grad_norm": 0.07202281057834625,
      "learning_rate": 2.4813333333333335e-05,
      "loss": 0.0021,
      "step": 75560
    },
    {
      "epoch": 4.0304,
      "grad_norm": 0.2167789489030838,
      "learning_rate": 2.481e-05,
      "loss": 0.0012,
      "step": 75570
    },
    {
      "epoch": 4.0309333333333335,
      "grad_norm": 0.3224775195121765,
      "learning_rate": 2.4806666666666667e-05,
      "loss": 0.0028,
      "step": 75580
    },
    {
      "epoch": 4.031466666666667,
      "grad_norm": 0.16327328979969025,
      "learning_rate": 2.4803333333333334e-05,
      "loss": 0.002,
      "step": 75590
    },
    {
      "epoch": 4.032,
      "grad_norm": 0.11344464868307114,
      "learning_rate": 2.48e-05,
      "loss": 0.0016,
      "step": 75600
    },
    {
      "epoch": 4.032533333333333,
      "grad_norm": 0.48710721731185913,
      "learning_rate": 2.479666666666667e-05,
      "loss": 0.0018,
      "step": 75610
    },
    {
      "epoch": 4.033066666666667,
      "grad_norm": 0.5235387086868286,
      "learning_rate": 2.4793333333333335e-05,
      "loss": 0.0019,
      "step": 75620
    },
    {
      "epoch": 4.0336,
      "grad_norm": 0.5799944996833801,
      "learning_rate": 2.479e-05,
      "loss": 0.0028,
      "step": 75630
    },
    {
      "epoch": 4.034133333333333,
      "grad_norm": 0.3927339017391205,
      "learning_rate": 2.4786666666666668e-05,
      "loss": 0.0022,
      "step": 75640
    },
    {
      "epoch": 4.034666666666666,
      "grad_norm": 0.3658115863800049,
      "learning_rate": 2.4783333333333334e-05,
      "loss": 0.0015,
      "step": 75650
    },
    {
      "epoch": 4.0352,
      "grad_norm": 0.22310546040534973,
      "learning_rate": 2.478e-05,
      "loss": 0.0019,
      "step": 75660
    },
    {
      "epoch": 4.035733333333333,
      "grad_norm": 0.21790838241577148,
      "learning_rate": 2.4776666666666666e-05,
      "loss": 0.0016,
      "step": 75670
    },
    {
      "epoch": 4.036266666666666,
      "grad_norm": 0.4627845585346222,
      "learning_rate": 2.4773333333333336e-05,
      "loss": 0.0016,
      "step": 75680
    },
    {
      "epoch": 4.0368,
      "grad_norm": 0.17768912017345428,
      "learning_rate": 2.4770000000000002e-05,
      "loss": 0.0021,
      "step": 75690
    },
    {
      "epoch": 4.037333333333334,
      "grad_norm": 0.07095494121313095,
      "learning_rate": 2.4766666666666668e-05,
      "loss": 0.0021,
      "step": 75700
    },
    {
      "epoch": 4.037866666666667,
      "grad_norm": 0.3176708519458771,
      "learning_rate": 2.4763333333333334e-05,
      "loss": 0.0016,
      "step": 75710
    },
    {
      "epoch": 4.0384,
      "grad_norm": 0.22808828949928284,
      "learning_rate": 2.476e-05,
      "loss": 0.0023,
      "step": 75720
    },
    {
      "epoch": 4.0389333333333335,
      "grad_norm": 0.0405331514775753,
      "learning_rate": 2.4756666666666666e-05,
      "loss": 0.0014,
      "step": 75730
    },
    {
      "epoch": 4.039466666666667,
      "grad_norm": 0.2154708057641983,
      "learning_rate": 2.4753333333333332e-05,
      "loss": 0.002,
      "step": 75740
    },
    {
      "epoch": 4.04,
      "grad_norm": 0.07305663824081421,
      "learning_rate": 2.4750000000000002e-05,
      "loss": 0.0019,
      "step": 75750
    },
    {
      "epoch": 4.040533333333333,
      "grad_norm": 0.26653018593788147,
      "learning_rate": 2.4746666666666668e-05,
      "loss": 0.0019,
      "step": 75760
    },
    {
      "epoch": 4.041066666666667,
      "grad_norm": 0.3354076147079468,
      "learning_rate": 2.4743333333333334e-05,
      "loss": 0.0018,
      "step": 75770
    },
    {
      "epoch": 4.0416,
      "grad_norm": 0.566406786441803,
      "learning_rate": 2.4740000000000004e-05,
      "loss": 0.0026,
      "step": 75780
    },
    {
      "epoch": 4.042133333333333,
      "grad_norm": 0.18456979095935822,
      "learning_rate": 2.473666666666667e-05,
      "loss": 0.0021,
      "step": 75790
    },
    {
      "epoch": 4.042666666666666,
      "grad_norm": 0.37131643295288086,
      "learning_rate": 2.4733333333333333e-05,
      "loss": 0.0015,
      "step": 75800
    },
    {
      "epoch": 4.0432,
      "grad_norm": 0.04930073395371437,
      "learning_rate": 2.473e-05,
      "loss": 0.0018,
      "step": 75810
    },
    {
      "epoch": 4.043733333333333,
      "grad_norm": 0.16718798875808716,
      "learning_rate": 2.4726666666666668e-05,
      "loss": 0.0022,
      "step": 75820
    },
    {
      "epoch": 4.044266666666666,
      "grad_norm": 0.12933789193630219,
      "learning_rate": 2.4723333333333334e-05,
      "loss": 0.0018,
      "step": 75830
    },
    {
      "epoch": 4.0448,
      "grad_norm": 0.4557058811187744,
      "learning_rate": 2.472e-05,
      "loss": 0.0019,
      "step": 75840
    },
    {
      "epoch": 4.045333333333334,
      "grad_norm": 0.2299949675798416,
      "learning_rate": 2.471666666666667e-05,
      "loss": 0.0018,
      "step": 75850
    },
    {
      "epoch": 4.045866666666667,
      "grad_norm": 0.24964570999145508,
      "learning_rate": 2.4713333333333336e-05,
      "loss": 0.0016,
      "step": 75860
    },
    {
      "epoch": 4.0464,
      "grad_norm": 0.3575781285762787,
      "learning_rate": 2.471e-05,
      "loss": 0.0019,
      "step": 75870
    },
    {
      "epoch": 4.0469333333333335,
      "grad_norm": 0.36738184094429016,
      "learning_rate": 2.470666666666667e-05,
      "loss": 0.0021,
      "step": 75880
    },
    {
      "epoch": 4.047466666666667,
      "grad_norm": 0.23340380191802979,
      "learning_rate": 2.4703333333333335e-05,
      "loss": 0.002,
      "step": 75890
    },
    {
      "epoch": 4.048,
      "grad_norm": 0.12924255430698395,
      "learning_rate": 2.47e-05,
      "loss": 0.0012,
      "step": 75900
    },
    {
      "epoch": 4.048533333333333,
      "grad_norm": 0.5772848129272461,
      "learning_rate": 2.4696666666666667e-05,
      "loss": 0.0018,
      "step": 75910
    },
    {
      "epoch": 4.049066666666667,
      "grad_norm": 0.25152671337127686,
      "learning_rate": 2.4693333333333336e-05,
      "loss": 0.0017,
      "step": 75920
    },
    {
      "epoch": 4.0496,
      "grad_norm": 0.39313778281211853,
      "learning_rate": 2.4690000000000002e-05,
      "loss": 0.0019,
      "step": 75930
    },
    {
      "epoch": 4.050133333333333,
      "grad_norm": 0.22795024514198303,
      "learning_rate": 2.468666666666667e-05,
      "loss": 0.0016,
      "step": 75940
    },
    {
      "epoch": 4.050666666666666,
      "grad_norm": 0.11476820707321167,
      "learning_rate": 2.4683333333333335e-05,
      "loss": 0.0023,
      "step": 75950
    },
    {
      "epoch": 4.0512,
      "grad_norm": 0.2118326872587204,
      "learning_rate": 2.468e-05,
      "loss": 0.0023,
      "step": 75960
    },
    {
      "epoch": 4.051733333333333,
      "grad_norm": 0.019775880500674248,
      "learning_rate": 2.4676666666666667e-05,
      "loss": 0.002,
      "step": 75970
    },
    {
      "epoch": 4.052266666666666,
      "grad_norm": 0.3295876085758209,
      "learning_rate": 2.4673333333333333e-05,
      "loss": 0.003,
      "step": 75980
    },
    {
      "epoch": 4.0528,
      "grad_norm": 0.08035539090633392,
      "learning_rate": 2.4670000000000003e-05,
      "loss": 0.002,
      "step": 75990
    },
    {
      "epoch": 4.053333333333334,
      "grad_norm": 0.05003594607114792,
      "learning_rate": 2.466666666666667e-05,
      "loss": 0.0018,
      "step": 76000
    },
    {
      "epoch": 4.053866666666667,
      "grad_norm": 0.18033628165721893,
      "learning_rate": 2.4663333333333335e-05,
      "loss": 0.0018,
      "step": 76010
    },
    {
      "epoch": 4.0544,
      "grad_norm": 0.1980798989534378,
      "learning_rate": 2.466e-05,
      "loss": 0.0017,
      "step": 76020
    },
    {
      "epoch": 4.0549333333333335,
      "grad_norm": 0.5025194883346558,
      "learning_rate": 2.4656666666666667e-05,
      "loss": 0.0021,
      "step": 76030
    },
    {
      "epoch": 4.055466666666667,
      "grad_norm": 0.7464620471000671,
      "learning_rate": 2.4653333333333333e-05,
      "loss": 0.0022,
      "step": 76040
    },
    {
      "epoch": 4.056,
      "grad_norm": 0.05875831097364426,
      "learning_rate": 2.465e-05,
      "loss": 0.0025,
      "step": 76050
    },
    {
      "epoch": 4.056533333333333,
      "grad_norm": 0.10349531471729279,
      "learning_rate": 2.464666666666667e-05,
      "loss": 0.0017,
      "step": 76060
    },
    {
      "epoch": 4.057066666666667,
      "grad_norm": 0.24590542912483215,
      "learning_rate": 2.4643333333333335e-05,
      "loss": 0.0017,
      "step": 76070
    },
    {
      "epoch": 4.0576,
      "grad_norm": 0.1601877361536026,
      "learning_rate": 2.464e-05,
      "loss": 0.0022,
      "step": 76080
    },
    {
      "epoch": 4.058133333333333,
      "grad_norm": 0.12583918869495392,
      "learning_rate": 2.4636666666666667e-05,
      "loss": 0.0021,
      "step": 76090
    },
    {
      "epoch": 4.058666666666666,
      "grad_norm": 0.20679645240306854,
      "learning_rate": 2.4633333333333334e-05,
      "loss": 0.002,
      "step": 76100
    },
    {
      "epoch": 4.0592,
      "grad_norm": 0.34111538529396057,
      "learning_rate": 2.463e-05,
      "loss": 0.0018,
      "step": 76110
    },
    {
      "epoch": 4.059733333333333,
      "grad_norm": 0.21608781814575195,
      "learning_rate": 2.4626666666666666e-05,
      "loss": 0.0027,
      "step": 76120
    },
    {
      "epoch": 4.060266666666666,
      "grad_norm": 0.20760168135166168,
      "learning_rate": 2.4623333333333335e-05,
      "loss": 0.0018,
      "step": 76130
    },
    {
      "epoch": 4.0608,
      "grad_norm": 0.1558627039194107,
      "learning_rate": 2.462e-05,
      "loss": 0.0024,
      "step": 76140
    },
    {
      "epoch": 4.061333333333334,
      "grad_norm": 0.05819734185934067,
      "learning_rate": 2.4616666666666668e-05,
      "loss": 0.0015,
      "step": 76150
    },
    {
      "epoch": 4.061866666666667,
      "grad_norm": 0.20855852961540222,
      "learning_rate": 2.4613333333333337e-05,
      "loss": 0.0018,
      "step": 76160
    },
    {
      "epoch": 4.0624,
      "grad_norm": 0.21559427678585052,
      "learning_rate": 2.4610000000000003e-05,
      "loss": 0.0016,
      "step": 76170
    },
    {
      "epoch": 4.0629333333333335,
      "grad_norm": 0.22633199393749237,
      "learning_rate": 2.4606666666666666e-05,
      "loss": 0.0014,
      "step": 76180
    },
    {
      "epoch": 4.063466666666667,
      "grad_norm": 0.27029702067375183,
      "learning_rate": 2.4603333333333332e-05,
      "loss": 0.0022,
      "step": 76190
    },
    {
      "epoch": 4.064,
      "grad_norm": 0.6912575960159302,
      "learning_rate": 2.46e-05,
      "loss": 0.0018,
      "step": 76200
    },
    {
      "epoch": 4.064533333333333,
      "grad_norm": 0.047936223447322845,
      "learning_rate": 2.4596666666666668e-05,
      "loss": 0.0018,
      "step": 76210
    },
    {
      "epoch": 4.065066666666667,
      "grad_norm": 0.10737261176109314,
      "learning_rate": 2.4593333333333334e-05,
      "loss": 0.0016,
      "step": 76220
    },
    {
      "epoch": 4.0656,
      "grad_norm": 0.33839505910873413,
      "learning_rate": 2.4590000000000003e-05,
      "loss": 0.003,
      "step": 76230
    },
    {
      "epoch": 4.066133333333333,
      "grad_norm": 0.10308092832565308,
      "learning_rate": 2.458666666666667e-05,
      "loss": 0.0021,
      "step": 76240
    },
    {
      "epoch": 4.066666666666666,
      "grad_norm": 0.09191276133060455,
      "learning_rate": 2.4583333333333332e-05,
      "loss": 0.0019,
      "step": 76250
    },
    {
      "epoch": 4.0672,
      "grad_norm": 0.09347391128540039,
      "learning_rate": 2.4580000000000002e-05,
      "loss": 0.0021,
      "step": 76260
    },
    {
      "epoch": 4.067733333333333,
      "grad_norm": 0.46108776330947876,
      "learning_rate": 2.4576666666666668e-05,
      "loss": 0.0018,
      "step": 76270
    },
    {
      "epoch": 4.068266666666666,
      "grad_norm": 0.4866998791694641,
      "learning_rate": 2.4573333333333334e-05,
      "loss": 0.0016,
      "step": 76280
    },
    {
      "epoch": 4.0688,
      "grad_norm": 0.49482351541519165,
      "learning_rate": 2.457e-05,
      "loss": 0.0018,
      "step": 76290
    },
    {
      "epoch": 4.069333333333334,
      "grad_norm": 0.23650044202804565,
      "learning_rate": 2.456666666666667e-05,
      "loss": 0.0022,
      "step": 76300
    },
    {
      "epoch": 4.069866666666667,
      "grad_norm": 0.1409151405096054,
      "learning_rate": 2.4563333333333336e-05,
      "loss": 0.0019,
      "step": 76310
    },
    {
      "epoch": 4.0704,
      "grad_norm": 0.6861010193824768,
      "learning_rate": 2.4560000000000002e-05,
      "loss": 0.0023,
      "step": 76320
    },
    {
      "epoch": 4.0709333333333335,
      "grad_norm": 0.5396726727485657,
      "learning_rate": 2.4556666666666668e-05,
      "loss": 0.0025,
      "step": 76330
    },
    {
      "epoch": 4.071466666666667,
      "grad_norm": 0.4603121876716614,
      "learning_rate": 2.4553333333333334e-05,
      "loss": 0.0031,
      "step": 76340
    },
    {
      "epoch": 4.072,
      "grad_norm": 0.34114059805870056,
      "learning_rate": 2.455e-05,
      "loss": 0.0024,
      "step": 76350
    },
    {
      "epoch": 4.072533333333333,
      "grad_norm": 0.4894583225250244,
      "learning_rate": 2.4546666666666667e-05,
      "loss": 0.0017,
      "step": 76360
    },
    {
      "epoch": 4.073066666666667,
      "grad_norm": 0.415693461894989,
      "learning_rate": 2.4543333333333336e-05,
      "loss": 0.0017,
      "step": 76370
    },
    {
      "epoch": 4.0736,
      "grad_norm": 0.19463837146759033,
      "learning_rate": 2.4540000000000002e-05,
      "loss": 0.0024,
      "step": 76380
    },
    {
      "epoch": 4.074133333333333,
      "grad_norm": 0.05264491215348244,
      "learning_rate": 2.453666666666667e-05,
      "loss": 0.0017,
      "step": 76390
    },
    {
      "epoch": 4.074666666666666,
      "grad_norm": 0.6122503280639648,
      "learning_rate": 2.4533333333333334e-05,
      "loss": 0.0021,
      "step": 76400
    },
    {
      "epoch": 4.0752,
      "grad_norm": 0.6814589500427246,
      "learning_rate": 2.453e-05,
      "loss": 0.0022,
      "step": 76410
    },
    {
      "epoch": 4.075733333333333,
      "grad_norm": 0.38639286160469055,
      "learning_rate": 2.4526666666666667e-05,
      "loss": 0.0022,
      "step": 76420
    },
    {
      "epoch": 4.076266666666666,
      "grad_norm": 0.1621314287185669,
      "learning_rate": 2.4523333333333333e-05,
      "loss": 0.0022,
      "step": 76430
    },
    {
      "epoch": 4.0768,
      "grad_norm": 0.20029090344905853,
      "learning_rate": 2.4520000000000002e-05,
      "loss": 0.0021,
      "step": 76440
    },
    {
      "epoch": 4.077333333333334,
      "grad_norm": 0.20864681899547577,
      "learning_rate": 2.451666666666667e-05,
      "loss": 0.0025,
      "step": 76450
    },
    {
      "epoch": 4.077866666666667,
      "grad_norm": 0.38094228506088257,
      "learning_rate": 2.4513333333333335e-05,
      "loss": 0.0019,
      "step": 76460
    },
    {
      "epoch": 4.0784,
      "grad_norm": 0.5510994791984558,
      "learning_rate": 2.451e-05,
      "loss": 0.0014,
      "step": 76470
    },
    {
      "epoch": 4.0789333333333335,
      "grad_norm": 0.26522642374038696,
      "learning_rate": 2.4506666666666667e-05,
      "loss": 0.0021,
      "step": 76480
    },
    {
      "epoch": 4.079466666666667,
      "grad_norm": 0.022704245522618294,
      "learning_rate": 2.4503333333333333e-05,
      "loss": 0.0017,
      "step": 76490
    },
    {
      "epoch": 4.08,
      "grad_norm": 0.5140257477760315,
      "learning_rate": 2.45e-05,
      "loss": 0.0017,
      "step": 76500
    },
    {
      "epoch": 4.080533333333333,
      "grad_norm": 0.14557094871997833,
      "learning_rate": 2.449666666666667e-05,
      "loss": 0.002,
      "step": 76510
    },
    {
      "epoch": 4.081066666666667,
      "grad_norm": 0.2222481518983841,
      "learning_rate": 2.4493333333333335e-05,
      "loss": 0.0017,
      "step": 76520
    },
    {
      "epoch": 4.0816,
      "grad_norm": 0.2414526492357254,
      "learning_rate": 2.449e-05,
      "loss": 0.002,
      "step": 76530
    },
    {
      "epoch": 4.082133333333333,
      "grad_norm": 0.09604188799858093,
      "learning_rate": 2.448666666666667e-05,
      "loss": 0.0021,
      "step": 76540
    },
    {
      "epoch": 4.082666666666666,
      "grad_norm": 0.1323520690202713,
      "learning_rate": 2.4483333333333333e-05,
      "loss": 0.0016,
      "step": 76550
    },
    {
      "epoch": 4.0832,
      "grad_norm": 0.24917946755886078,
      "learning_rate": 2.448e-05,
      "loss": 0.0013,
      "step": 76560
    },
    {
      "epoch": 4.083733333333333,
      "grad_norm": 0.48901042342185974,
      "learning_rate": 2.4476666666666666e-05,
      "loss": 0.002,
      "step": 76570
    },
    {
      "epoch": 4.084266666666666,
      "grad_norm": 0.5808920860290527,
      "learning_rate": 2.4473333333333335e-05,
      "loss": 0.0029,
      "step": 76580
    },
    {
      "epoch": 4.0848,
      "grad_norm": 0.12629762291908264,
      "learning_rate": 2.447e-05,
      "loss": 0.0019,
      "step": 76590
    },
    {
      "epoch": 4.085333333333334,
      "grad_norm": 0.1529775857925415,
      "learning_rate": 2.4466666666666667e-05,
      "loss": 0.002,
      "step": 76600
    },
    {
      "epoch": 4.085866666666667,
      "grad_norm": 0.09471911936998367,
      "learning_rate": 2.4463333333333337e-05,
      "loss": 0.0021,
      "step": 76610
    },
    {
      "epoch": 4.0864,
      "grad_norm": 0.055679306387901306,
      "learning_rate": 2.4460000000000003e-05,
      "loss": 0.0022,
      "step": 76620
    },
    {
      "epoch": 4.0869333333333335,
      "grad_norm": 0.18149156868457794,
      "learning_rate": 2.4456666666666666e-05,
      "loss": 0.0016,
      "step": 76630
    },
    {
      "epoch": 4.087466666666667,
      "grad_norm": 0.6446589231491089,
      "learning_rate": 2.4453333333333335e-05,
      "loss": 0.0015,
      "step": 76640
    },
    {
      "epoch": 4.088,
      "grad_norm": 0.08131510019302368,
      "learning_rate": 2.445e-05,
      "loss": 0.0017,
      "step": 76650
    },
    {
      "epoch": 4.088533333333333,
      "grad_norm": 0.3954276442527771,
      "learning_rate": 2.4446666666666668e-05,
      "loss": 0.0019,
      "step": 76660
    },
    {
      "epoch": 4.089066666666667,
      "grad_norm": 0.48978227376937866,
      "learning_rate": 2.4443333333333334e-05,
      "loss": 0.0023,
      "step": 76670
    },
    {
      "epoch": 4.0896,
      "grad_norm": 0.18230460584163666,
      "learning_rate": 2.4440000000000003e-05,
      "loss": 0.0024,
      "step": 76680
    },
    {
      "epoch": 4.090133333333333,
      "grad_norm": 0.30262449383735657,
      "learning_rate": 2.443666666666667e-05,
      "loss": 0.0023,
      "step": 76690
    },
    {
      "epoch": 4.0906666666666665,
      "grad_norm": 0.3074647784233093,
      "learning_rate": 2.4433333333333335e-05,
      "loss": 0.0027,
      "step": 76700
    },
    {
      "epoch": 4.0912,
      "grad_norm": 0.39860978722572327,
      "learning_rate": 2.443e-05,
      "loss": 0.0015,
      "step": 76710
    },
    {
      "epoch": 4.091733333333333,
      "grad_norm": 0.2583889961242676,
      "learning_rate": 2.4426666666666668e-05,
      "loss": 0.0015,
      "step": 76720
    },
    {
      "epoch": 4.092266666666666,
      "grad_norm": 0.18279555439949036,
      "learning_rate": 2.4423333333333334e-05,
      "loss": 0.002,
      "step": 76730
    },
    {
      "epoch": 4.0928,
      "grad_norm": 0.07203555107116699,
      "learning_rate": 2.442e-05,
      "loss": 0.0026,
      "step": 76740
    },
    {
      "epoch": 4.093333333333334,
      "grad_norm": 0.4827948808670044,
      "learning_rate": 2.441666666666667e-05,
      "loss": 0.0022,
      "step": 76750
    },
    {
      "epoch": 4.093866666666667,
      "grad_norm": 0.09667763859033585,
      "learning_rate": 2.4413333333333336e-05,
      "loss": 0.0022,
      "step": 76760
    },
    {
      "epoch": 4.0944,
      "grad_norm": 0.18170715868473053,
      "learning_rate": 2.4410000000000002e-05,
      "loss": 0.0012,
      "step": 76770
    },
    {
      "epoch": 4.0949333333333335,
      "grad_norm": 0.383596271276474,
      "learning_rate": 2.4406666666666668e-05,
      "loss": 0.0019,
      "step": 76780
    },
    {
      "epoch": 4.095466666666667,
      "grad_norm": 0.3746018409729004,
      "learning_rate": 2.4403333333333334e-05,
      "loss": 0.002,
      "step": 76790
    },
    {
      "epoch": 4.096,
      "grad_norm": 0.12495001405477524,
      "learning_rate": 2.44e-05,
      "loss": 0.002,
      "step": 76800
    },
    {
      "epoch": 4.096533333333333,
      "grad_norm": 0.4752902388572693,
      "learning_rate": 2.4396666666666666e-05,
      "loss": 0.0016,
      "step": 76810
    },
    {
      "epoch": 4.097066666666667,
      "grad_norm": 0.46401602029800415,
      "learning_rate": 2.4393333333333336e-05,
      "loss": 0.0017,
      "step": 76820
    },
    {
      "epoch": 4.0976,
      "grad_norm": 0.22486922144889832,
      "learning_rate": 2.4390000000000002e-05,
      "loss": 0.0021,
      "step": 76830
    },
    {
      "epoch": 4.098133333333333,
      "grad_norm": 0.5129730105400085,
      "learning_rate": 2.4386666666666668e-05,
      "loss": 0.0019,
      "step": 76840
    },
    {
      "epoch": 4.0986666666666665,
      "grad_norm": 0.11334407329559326,
      "learning_rate": 2.4383333333333334e-05,
      "loss": 0.0019,
      "step": 76850
    },
    {
      "epoch": 4.0992,
      "grad_norm": 0.4659087657928467,
      "learning_rate": 2.438e-05,
      "loss": 0.0016,
      "step": 76860
    },
    {
      "epoch": 4.099733333333333,
      "grad_norm": 0.29951947927474976,
      "learning_rate": 2.4376666666666666e-05,
      "loss": 0.0029,
      "step": 76870
    },
    {
      "epoch": 4.100266666666666,
      "grad_norm": 0.04973683878779411,
      "learning_rate": 2.4373333333333333e-05,
      "loss": 0.0021,
      "step": 76880
    },
    {
      "epoch": 4.1008,
      "grad_norm": 0.34499984979629517,
      "learning_rate": 2.4370000000000002e-05,
      "loss": 0.0019,
      "step": 76890
    },
    {
      "epoch": 4.101333333333334,
      "grad_norm": 0.7099077701568604,
      "learning_rate": 2.4366666666666668e-05,
      "loss": 0.002,
      "step": 76900
    },
    {
      "epoch": 4.101866666666667,
      "grad_norm": 0.28698357939720154,
      "learning_rate": 2.4363333333333334e-05,
      "loss": 0.0018,
      "step": 76910
    },
    {
      "epoch": 4.1024,
      "grad_norm": 0.2582884430885315,
      "learning_rate": 2.4360000000000004e-05,
      "loss": 0.0029,
      "step": 76920
    },
    {
      "epoch": 4.1029333333333335,
      "grad_norm": 0.04996870085597038,
      "learning_rate": 2.4356666666666667e-05,
      "loss": 0.0019,
      "step": 76930
    },
    {
      "epoch": 4.103466666666667,
      "grad_norm": 0.0628284215927124,
      "learning_rate": 2.4353333333333333e-05,
      "loss": 0.0029,
      "step": 76940
    },
    {
      "epoch": 4.104,
      "grad_norm": 0.18870341777801514,
      "learning_rate": 2.435e-05,
      "loss": 0.0027,
      "step": 76950
    },
    {
      "epoch": 4.104533333333333,
      "grad_norm": 0.15392719209194183,
      "learning_rate": 2.434666666666667e-05,
      "loss": 0.0025,
      "step": 76960
    },
    {
      "epoch": 4.105066666666667,
      "grad_norm": 0.09245642274618149,
      "learning_rate": 2.4343333333333335e-05,
      "loss": 0.0017,
      "step": 76970
    },
    {
      "epoch": 4.1056,
      "grad_norm": 0.1465030014514923,
      "learning_rate": 2.434e-05,
      "loss": 0.0026,
      "step": 76980
    },
    {
      "epoch": 4.106133333333333,
      "grad_norm": 0.15748007595539093,
      "learning_rate": 2.433666666666667e-05,
      "loss": 0.0023,
      "step": 76990
    },
    {
      "epoch": 4.1066666666666665,
      "grad_norm": 0.6961912512779236,
      "learning_rate": 2.4333333333333336e-05,
      "loss": 0.0018,
      "step": 77000
    },
    {
      "epoch": 4.1072,
      "grad_norm": 0.2831169068813324,
      "learning_rate": 2.433e-05,
      "loss": 0.0017,
      "step": 77010
    },
    {
      "epoch": 4.107733333333333,
      "grad_norm": 0.04898321256041527,
      "learning_rate": 2.432666666666667e-05,
      "loss": 0.0014,
      "step": 77020
    },
    {
      "epoch": 4.108266666666666,
      "grad_norm": 0.3391101062297821,
      "learning_rate": 2.4323333333333335e-05,
      "loss": 0.0021,
      "step": 77030
    },
    {
      "epoch": 4.1088,
      "grad_norm": 0.04235918074846268,
      "learning_rate": 2.432e-05,
      "loss": 0.0016,
      "step": 77040
    },
    {
      "epoch": 4.109333333333334,
      "grad_norm": 0.1805344820022583,
      "learning_rate": 2.4316666666666667e-05,
      "loss": 0.0031,
      "step": 77050
    },
    {
      "epoch": 4.109866666666667,
      "grad_norm": 0.33945149183273315,
      "learning_rate": 2.4313333333333337e-05,
      "loss": 0.002,
      "step": 77060
    },
    {
      "epoch": 4.1104,
      "grad_norm": 0.08843517303466797,
      "learning_rate": 2.4310000000000003e-05,
      "loss": 0.0021,
      "step": 77070
    },
    {
      "epoch": 4.1109333333333336,
      "grad_norm": 0.10943181067705154,
      "learning_rate": 2.4306666666666665e-05,
      "loss": 0.0015,
      "step": 77080
    },
    {
      "epoch": 4.111466666666667,
      "grad_norm": 0.09898030012845993,
      "learning_rate": 2.4303333333333335e-05,
      "loss": 0.0021,
      "step": 77090
    },
    {
      "epoch": 4.112,
      "grad_norm": 0.3551686406135559,
      "learning_rate": 2.43e-05,
      "loss": 0.0017,
      "step": 77100
    },
    {
      "epoch": 4.112533333333333,
      "grad_norm": 0.23263657093048096,
      "learning_rate": 2.4296666666666667e-05,
      "loss": 0.0021,
      "step": 77110
    },
    {
      "epoch": 4.113066666666667,
      "grad_norm": 0.5840871334075928,
      "learning_rate": 2.4293333333333333e-05,
      "loss": 0.0019,
      "step": 77120
    },
    {
      "epoch": 4.1136,
      "grad_norm": 0.4251299202442169,
      "learning_rate": 2.4290000000000003e-05,
      "loss": 0.0029,
      "step": 77130
    },
    {
      "epoch": 4.114133333333333,
      "grad_norm": 0.14925478398799896,
      "learning_rate": 2.428666666666667e-05,
      "loss": 0.0026,
      "step": 77140
    },
    {
      "epoch": 4.1146666666666665,
      "grad_norm": 0.6358363628387451,
      "learning_rate": 2.4283333333333335e-05,
      "loss": 0.0014,
      "step": 77150
    },
    {
      "epoch": 4.1152,
      "grad_norm": 0.04435043781995773,
      "learning_rate": 2.428e-05,
      "loss": 0.0018,
      "step": 77160
    },
    {
      "epoch": 4.115733333333333,
      "grad_norm": 0.16457566618919373,
      "learning_rate": 2.4276666666666667e-05,
      "loss": 0.0027,
      "step": 77170
    },
    {
      "epoch": 4.116266666666666,
      "grad_norm": 0.15229766070842743,
      "learning_rate": 2.4273333333333334e-05,
      "loss": 0.0019,
      "step": 77180
    },
    {
      "epoch": 4.1168,
      "grad_norm": 0.12345727533102036,
      "learning_rate": 2.427e-05,
      "loss": 0.0022,
      "step": 77190
    },
    {
      "epoch": 4.117333333333334,
      "grad_norm": 0.24438361823558807,
      "learning_rate": 2.426666666666667e-05,
      "loss": 0.0022,
      "step": 77200
    },
    {
      "epoch": 4.117866666666667,
      "grad_norm": 0.5537459850311279,
      "learning_rate": 2.4263333333333335e-05,
      "loss": 0.0012,
      "step": 77210
    },
    {
      "epoch": 4.1184,
      "grad_norm": 0.12836854159832,
      "learning_rate": 2.426e-05,
      "loss": 0.0013,
      "step": 77220
    },
    {
      "epoch": 4.118933333333334,
      "grad_norm": 0.46976810693740845,
      "learning_rate": 2.4256666666666668e-05,
      "loss": 0.0021,
      "step": 77230
    },
    {
      "epoch": 4.119466666666667,
      "grad_norm": 0.4834281802177429,
      "learning_rate": 2.4253333333333334e-05,
      "loss": 0.0024,
      "step": 77240
    },
    {
      "epoch": 4.12,
      "grad_norm": 0.40947026014328003,
      "learning_rate": 2.425e-05,
      "loss": 0.004,
      "step": 77250
    },
    {
      "epoch": 4.120533333333333,
      "grad_norm": 0.38950228691101074,
      "learning_rate": 2.4246666666666666e-05,
      "loss": 0.0026,
      "step": 77260
    },
    {
      "epoch": 4.121066666666667,
      "grad_norm": 0.11190078407526016,
      "learning_rate": 2.4243333333333336e-05,
      "loss": 0.0015,
      "step": 77270
    },
    {
      "epoch": 4.1216,
      "grad_norm": 0.5283551812171936,
      "learning_rate": 2.4240000000000002e-05,
      "loss": 0.0016,
      "step": 77280
    },
    {
      "epoch": 4.122133333333333,
      "grad_norm": 0.291194885969162,
      "learning_rate": 2.4236666666666668e-05,
      "loss": 0.0019,
      "step": 77290
    },
    {
      "epoch": 4.1226666666666665,
      "grad_norm": 0.49074241518974304,
      "learning_rate": 2.4233333333333337e-05,
      "loss": 0.0018,
      "step": 77300
    },
    {
      "epoch": 4.1232,
      "grad_norm": 0.1858779937028885,
      "learning_rate": 2.423e-05,
      "loss": 0.0017,
      "step": 77310
    },
    {
      "epoch": 4.123733333333333,
      "grad_norm": 0.10050830245018005,
      "learning_rate": 2.4226666666666666e-05,
      "loss": 0.0019,
      "step": 77320
    },
    {
      "epoch": 4.124266666666666,
      "grad_norm": 0.19685280323028564,
      "learning_rate": 2.4223333333333332e-05,
      "loss": 0.0018,
      "step": 77330
    },
    {
      "epoch": 4.1248,
      "grad_norm": 0.36136895418167114,
      "learning_rate": 2.4220000000000002e-05,
      "loss": 0.0027,
      "step": 77340
    },
    {
      "epoch": 4.125333333333334,
      "grad_norm": 0.07692628353834152,
      "learning_rate": 2.4216666666666668e-05,
      "loss": 0.0018,
      "step": 77350
    },
    {
      "epoch": 4.125866666666667,
      "grad_norm": 0.3368534743785858,
      "learning_rate": 2.4213333333333334e-05,
      "loss": 0.0023,
      "step": 77360
    },
    {
      "epoch": 4.1264,
      "grad_norm": 0.1903054565191269,
      "learning_rate": 2.4210000000000004e-05,
      "loss": 0.0013,
      "step": 77370
    },
    {
      "epoch": 4.126933333333334,
      "grad_norm": 0.2758775055408478,
      "learning_rate": 2.420666666666667e-05,
      "loss": 0.0019,
      "step": 77380
    },
    {
      "epoch": 4.127466666666667,
      "grad_norm": 0.39955949783325195,
      "learning_rate": 2.4203333333333333e-05,
      "loss": 0.0016,
      "step": 77390
    },
    {
      "epoch": 4.128,
      "grad_norm": 0.12382279336452484,
      "learning_rate": 2.4200000000000002e-05,
      "loss": 0.0019,
      "step": 77400
    },
    {
      "epoch": 4.128533333333333,
      "grad_norm": 0.5857236385345459,
      "learning_rate": 2.4196666666666668e-05,
      "loss": 0.0033,
      "step": 77410
    },
    {
      "epoch": 4.129066666666667,
      "grad_norm": 0.3693845570087433,
      "learning_rate": 2.4193333333333334e-05,
      "loss": 0.0024,
      "step": 77420
    },
    {
      "epoch": 4.1296,
      "grad_norm": 0.08855508267879486,
      "learning_rate": 2.419e-05,
      "loss": 0.002,
      "step": 77430
    },
    {
      "epoch": 4.130133333333333,
      "grad_norm": 0.12304375320672989,
      "learning_rate": 2.418666666666667e-05,
      "loss": 0.0022,
      "step": 77440
    },
    {
      "epoch": 4.1306666666666665,
      "grad_norm": 0.12371306121349335,
      "learning_rate": 2.4183333333333336e-05,
      "loss": 0.0016,
      "step": 77450
    },
    {
      "epoch": 4.1312,
      "grad_norm": 0.048104193061590195,
      "learning_rate": 2.418e-05,
      "loss": 0.0015,
      "step": 77460
    },
    {
      "epoch": 4.131733333333333,
      "grad_norm": 0.07763342559337616,
      "learning_rate": 2.417666666666667e-05,
      "loss": 0.0016,
      "step": 77470
    },
    {
      "epoch": 4.132266666666666,
      "grad_norm": 0.187244713306427,
      "learning_rate": 2.4173333333333335e-05,
      "loss": 0.0025,
      "step": 77480
    },
    {
      "epoch": 4.1328,
      "grad_norm": 0.5524266362190247,
      "learning_rate": 2.417e-05,
      "loss": 0.002,
      "step": 77490
    },
    {
      "epoch": 4.133333333333334,
      "grad_norm": 0.3723105788230896,
      "learning_rate": 2.4166666666666667e-05,
      "loss": 0.0026,
      "step": 77500
    },
    {
      "epoch": 4.133866666666667,
      "grad_norm": 0.39265796542167664,
      "learning_rate": 2.4163333333333336e-05,
      "loss": 0.0025,
      "step": 77510
    },
    {
      "epoch": 4.1344,
      "grad_norm": 0.07065198570489883,
      "learning_rate": 2.4160000000000002e-05,
      "loss": 0.0013,
      "step": 77520
    },
    {
      "epoch": 4.134933333333334,
      "grad_norm": 0.22817087173461914,
      "learning_rate": 2.415666666666667e-05,
      "loss": 0.0019,
      "step": 77530
    },
    {
      "epoch": 4.135466666666667,
      "grad_norm": 0.058314528316259384,
      "learning_rate": 2.4153333333333335e-05,
      "loss": 0.0026,
      "step": 77540
    },
    {
      "epoch": 4.136,
      "grad_norm": 0.07616214454174042,
      "learning_rate": 2.415e-05,
      "loss": 0.002,
      "step": 77550
    },
    {
      "epoch": 4.136533333333333,
      "grad_norm": 0.12869225442409515,
      "learning_rate": 2.4146666666666667e-05,
      "loss": 0.002,
      "step": 77560
    },
    {
      "epoch": 4.137066666666667,
      "grad_norm": 0.15517589449882507,
      "learning_rate": 2.4143333333333333e-05,
      "loss": 0.0023,
      "step": 77570
    },
    {
      "epoch": 4.1376,
      "grad_norm": 0.16351915895938873,
      "learning_rate": 2.4140000000000003e-05,
      "loss": 0.0018,
      "step": 77580
    },
    {
      "epoch": 4.138133333333333,
      "grad_norm": 0.2816367447376251,
      "learning_rate": 2.413666666666667e-05,
      "loss": 0.0025,
      "step": 77590
    },
    {
      "epoch": 4.1386666666666665,
      "grad_norm": 0.2146308869123459,
      "learning_rate": 2.4133333333333335e-05,
      "loss": 0.0019,
      "step": 77600
    },
    {
      "epoch": 4.1392,
      "grad_norm": 0.2118224948644638,
      "learning_rate": 2.413e-05,
      "loss": 0.0018,
      "step": 77610
    },
    {
      "epoch": 4.139733333333333,
      "grad_norm": 0.20579859614372253,
      "learning_rate": 2.4126666666666667e-05,
      "loss": 0.0023,
      "step": 77620
    },
    {
      "epoch": 4.140266666666666,
      "grad_norm": 0.43099454045295715,
      "learning_rate": 2.4123333333333333e-05,
      "loss": 0.0014,
      "step": 77630
    },
    {
      "epoch": 4.1408,
      "grad_norm": 0.3373347520828247,
      "learning_rate": 2.412e-05,
      "loss": 0.0024,
      "step": 77640
    },
    {
      "epoch": 4.141333333333334,
      "grad_norm": 0.18084152042865753,
      "learning_rate": 2.411666666666667e-05,
      "loss": 0.0011,
      "step": 77650
    },
    {
      "epoch": 4.141866666666667,
      "grad_norm": 0.43594101071357727,
      "learning_rate": 2.4113333333333335e-05,
      "loss": 0.0024,
      "step": 77660
    },
    {
      "epoch": 4.1424,
      "grad_norm": 0.2479114979505539,
      "learning_rate": 2.411e-05,
      "loss": 0.0014,
      "step": 77670
    },
    {
      "epoch": 4.142933333333334,
      "grad_norm": 0.2779957950115204,
      "learning_rate": 2.4106666666666667e-05,
      "loss": 0.0017,
      "step": 77680
    },
    {
      "epoch": 4.143466666666667,
      "grad_norm": 0.4504459798336029,
      "learning_rate": 2.4103333333333334e-05,
      "loss": 0.0017,
      "step": 77690
    },
    {
      "epoch": 4.144,
      "grad_norm": 0.26849856972694397,
      "learning_rate": 2.41e-05,
      "loss": 0.0045,
      "step": 77700
    },
    {
      "epoch": 4.144533333333333,
      "grad_norm": 0.09955562651157379,
      "learning_rate": 2.4096666666666666e-05,
      "loss": 0.0018,
      "step": 77710
    },
    {
      "epoch": 4.145066666666667,
      "grad_norm": 0.25652217864990234,
      "learning_rate": 2.4093333333333335e-05,
      "loss": 0.0023,
      "step": 77720
    },
    {
      "epoch": 4.1456,
      "grad_norm": 0.3975338339805603,
      "learning_rate": 2.409e-05,
      "loss": 0.0022,
      "step": 77730
    },
    {
      "epoch": 4.146133333333333,
      "grad_norm": 0.09442448616027832,
      "learning_rate": 2.4086666666666668e-05,
      "loss": 0.0017,
      "step": 77740
    },
    {
      "epoch": 4.1466666666666665,
      "grad_norm": 0.54176265001297,
      "learning_rate": 2.4083333333333337e-05,
      "loss": 0.0017,
      "step": 77750
    },
    {
      "epoch": 4.1472,
      "grad_norm": 0.1323021948337555,
      "learning_rate": 2.408e-05,
      "loss": 0.002,
      "step": 77760
    },
    {
      "epoch": 4.147733333333333,
      "grad_norm": 0.21694263815879822,
      "learning_rate": 2.4076666666666666e-05,
      "loss": 0.0021,
      "step": 77770
    },
    {
      "epoch": 4.148266666666666,
      "grad_norm": 0.1009548231959343,
      "learning_rate": 2.4073333333333335e-05,
      "loss": 0.0016,
      "step": 77780
    },
    {
      "epoch": 4.1488,
      "grad_norm": 0.5096234083175659,
      "learning_rate": 2.407e-05,
      "loss": 0.0022,
      "step": 77790
    },
    {
      "epoch": 4.149333333333334,
      "grad_norm": 0.8382269740104675,
      "learning_rate": 2.4066666666666668e-05,
      "loss": 0.003,
      "step": 77800
    },
    {
      "epoch": 4.149866666666667,
      "grad_norm": 0.5231385827064514,
      "learning_rate": 2.4063333333333334e-05,
      "loss": 0.0019,
      "step": 77810
    },
    {
      "epoch": 4.1504,
      "grad_norm": 0.5240901112556458,
      "learning_rate": 2.4060000000000003e-05,
      "loss": 0.0019,
      "step": 77820
    },
    {
      "epoch": 4.150933333333334,
      "grad_norm": 0.6589846611022949,
      "learning_rate": 2.405666666666667e-05,
      "loss": 0.0019,
      "step": 77830
    },
    {
      "epoch": 4.151466666666667,
      "grad_norm": 0.18074212968349457,
      "learning_rate": 2.4053333333333332e-05,
      "loss": 0.0016,
      "step": 77840
    },
    {
      "epoch": 4.152,
      "grad_norm": 0.19026319682598114,
      "learning_rate": 2.4050000000000002e-05,
      "loss": 0.0026,
      "step": 77850
    },
    {
      "epoch": 4.152533333333333,
      "grad_norm": 0.06525655835866928,
      "learning_rate": 2.4046666666666668e-05,
      "loss": 0.0016,
      "step": 77860
    },
    {
      "epoch": 4.153066666666667,
      "grad_norm": 0.344985693693161,
      "learning_rate": 2.4043333333333334e-05,
      "loss": 0.003,
      "step": 77870
    },
    {
      "epoch": 4.1536,
      "grad_norm": 0.08492149412631989,
      "learning_rate": 2.404e-05,
      "loss": 0.002,
      "step": 77880
    },
    {
      "epoch": 4.154133333333333,
      "grad_norm": 0.105200856924057,
      "learning_rate": 2.403666666666667e-05,
      "loss": 0.0032,
      "step": 77890
    },
    {
      "epoch": 4.1546666666666665,
      "grad_norm": 0.25204241275787354,
      "learning_rate": 2.4033333333333336e-05,
      "loss": 0.0019,
      "step": 77900
    },
    {
      "epoch": 4.1552,
      "grad_norm": 0.4834980070590973,
      "learning_rate": 2.4030000000000002e-05,
      "loss": 0.0032,
      "step": 77910
    },
    {
      "epoch": 4.155733333333333,
      "grad_norm": 0.05295281857252121,
      "learning_rate": 2.4026666666666668e-05,
      "loss": 0.0024,
      "step": 77920
    },
    {
      "epoch": 4.156266666666666,
      "grad_norm": 0.4373815953731537,
      "learning_rate": 2.4023333333333334e-05,
      "loss": 0.002,
      "step": 77930
    },
    {
      "epoch": 4.1568,
      "grad_norm": 0.24510164558887482,
      "learning_rate": 2.402e-05,
      "loss": 0.0014,
      "step": 77940
    },
    {
      "epoch": 4.157333333333334,
      "grad_norm": 0.1889503002166748,
      "learning_rate": 2.4016666666666667e-05,
      "loss": 0.0025,
      "step": 77950
    },
    {
      "epoch": 4.157866666666667,
      "grad_norm": 0.20351924002170563,
      "learning_rate": 2.4013333333333336e-05,
      "loss": 0.0027,
      "step": 77960
    },
    {
      "epoch": 4.1584,
      "grad_norm": 0.15839628875255585,
      "learning_rate": 2.4010000000000002e-05,
      "loss": 0.0026,
      "step": 77970
    },
    {
      "epoch": 4.158933333333334,
      "grad_norm": 0.24187004566192627,
      "learning_rate": 2.400666666666667e-05,
      "loss": 0.0016,
      "step": 77980
    },
    {
      "epoch": 4.159466666666667,
      "grad_norm": 0.1558040976524353,
      "learning_rate": 2.4003333333333334e-05,
      "loss": 0.0027,
      "step": 77990
    },
    {
      "epoch": 4.16,
      "grad_norm": 0.08251389861106873,
      "learning_rate": 2.4e-05,
      "loss": 0.0015,
      "step": 78000
    },
    {
      "epoch": 4.160533333333333,
      "grad_norm": 0.3316171169281006,
      "learning_rate": 2.3996666666666667e-05,
      "loss": 0.0023,
      "step": 78010
    },
    {
      "epoch": 4.161066666666667,
      "grad_norm": 0.4899500906467438,
      "learning_rate": 2.3993333333333333e-05,
      "loss": 0.0012,
      "step": 78020
    },
    {
      "epoch": 4.1616,
      "grad_norm": 0.06741854548454285,
      "learning_rate": 2.3990000000000002e-05,
      "loss": 0.0015,
      "step": 78030
    },
    {
      "epoch": 4.162133333333333,
      "grad_norm": 0.43008771538734436,
      "learning_rate": 2.398666666666667e-05,
      "loss": 0.0016,
      "step": 78040
    },
    {
      "epoch": 4.1626666666666665,
      "grad_norm": 0.30889996886253357,
      "learning_rate": 2.3983333333333335e-05,
      "loss": 0.0027,
      "step": 78050
    },
    {
      "epoch": 4.1632,
      "grad_norm": 0.7671626210212708,
      "learning_rate": 2.398e-05,
      "loss": 0.0023,
      "step": 78060
    },
    {
      "epoch": 4.163733333333333,
      "grad_norm": 0.14961810410022736,
      "learning_rate": 2.3976666666666667e-05,
      "loss": 0.0019,
      "step": 78070
    },
    {
      "epoch": 4.164266666666666,
      "grad_norm": 0.7498353719711304,
      "learning_rate": 2.3973333333333333e-05,
      "loss": 0.0024,
      "step": 78080
    },
    {
      "epoch": 4.1648,
      "grad_norm": 0.1921411007642746,
      "learning_rate": 2.397e-05,
      "loss": 0.0019,
      "step": 78090
    },
    {
      "epoch": 4.165333333333333,
      "grad_norm": 0.2643834948539734,
      "learning_rate": 2.396666666666667e-05,
      "loss": 0.0016,
      "step": 78100
    },
    {
      "epoch": 4.165866666666667,
      "grad_norm": 0.25463372468948364,
      "learning_rate": 2.3963333333333335e-05,
      "loss": 0.0023,
      "step": 78110
    },
    {
      "epoch": 4.1664,
      "grad_norm": 0.07516560703516006,
      "learning_rate": 2.396e-05,
      "loss": 0.0021,
      "step": 78120
    },
    {
      "epoch": 4.166933333333334,
      "grad_norm": 0.04197311028838158,
      "learning_rate": 2.395666666666667e-05,
      "loss": 0.0018,
      "step": 78130
    },
    {
      "epoch": 4.167466666666667,
      "grad_norm": 0.06197847053408623,
      "learning_rate": 2.3953333333333333e-05,
      "loss": 0.0019,
      "step": 78140
    },
    {
      "epoch": 4.168,
      "grad_norm": 0.42594781517982483,
      "learning_rate": 2.395e-05,
      "loss": 0.002,
      "step": 78150
    },
    {
      "epoch": 4.168533333333333,
      "grad_norm": 0.5552187561988831,
      "learning_rate": 2.394666666666667e-05,
      "loss": 0.0013,
      "step": 78160
    },
    {
      "epoch": 4.169066666666667,
      "grad_norm": 0.15236426889896393,
      "learning_rate": 2.3943333333333335e-05,
      "loss": 0.0019,
      "step": 78170
    },
    {
      "epoch": 4.1696,
      "grad_norm": 0.5466216802597046,
      "learning_rate": 2.394e-05,
      "loss": 0.003,
      "step": 78180
    },
    {
      "epoch": 4.170133333333333,
      "grad_norm": 0.14273913204669952,
      "learning_rate": 2.3936666666666667e-05,
      "loss": 0.0025,
      "step": 78190
    },
    {
      "epoch": 4.1706666666666665,
      "grad_norm": 0.38800951838493347,
      "learning_rate": 2.3933333333333337e-05,
      "loss": 0.0021,
      "step": 78200
    },
    {
      "epoch": 4.1712,
      "grad_norm": 0.5047090649604797,
      "learning_rate": 2.3930000000000003e-05,
      "loss": 0.0027,
      "step": 78210
    },
    {
      "epoch": 4.171733333333333,
      "grad_norm": 0.27213460206985474,
      "learning_rate": 2.3926666666666666e-05,
      "loss": 0.0015,
      "step": 78220
    },
    {
      "epoch": 4.172266666666666,
      "grad_norm": 0.47042638063430786,
      "learning_rate": 2.3923333333333335e-05,
      "loss": 0.0021,
      "step": 78230
    },
    {
      "epoch": 4.1728,
      "grad_norm": 0.551540195941925,
      "learning_rate": 2.392e-05,
      "loss": 0.0016,
      "step": 78240
    },
    {
      "epoch": 4.173333333333334,
      "grad_norm": 0.30209705233573914,
      "learning_rate": 2.3916666666666668e-05,
      "loss": 0.0014,
      "step": 78250
    },
    {
      "epoch": 4.173866666666667,
      "grad_norm": 0.38545721769332886,
      "learning_rate": 2.3913333333333334e-05,
      "loss": 0.0022,
      "step": 78260
    },
    {
      "epoch": 4.1744,
      "grad_norm": 0.20830340683460236,
      "learning_rate": 2.3910000000000003e-05,
      "loss": 0.0017,
      "step": 78270
    },
    {
      "epoch": 4.174933333333334,
      "grad_norm": 0.09830188006162643,
      "learning_rate": 2.390666666666667e-05,
      "loss": 0.0028,
      "step": 78280
    },
    {
      "epoch": 4.175466666666667,
      "grad_norm": 0.18419024348258972,
      "learning_rate": 2.3903333333333332e-05,
      "loss": 0.0013,
      "step": 78290
    },
    {
      "epoch": 4.176,
      "grad_norm": 0.5610714554786682,
      "learning_rate": 2.39e-05,
      "loss": 0.0017,
      "step": 78300
    },
    {
      "epoch": 4.176533333333333,
      "grad_norm": 0.4581269919872284,
      "learning_rate": 2.3896666666666668e-05,
      "loss": 0.0014,
      "step": 78310
    },
    {
      "epoch": 4.177066666666667,
      "grad_norm": 0.2803709805011749,
      "learning_rate": 2.3893333333333334e-05,
      "loss": 0.0021,
      "step": 78320
    },
    {
      "epoch": 4.1776,
      "grad_norm": 0.1479467749595642,
      "learning_rate": 2.389e-05,
      "loss": 0.0025,
      "step": 78330
    },
    {
      "epoch": 4.178133333333333,
      "grad_norm": 0.05392284318804741,
      "learning_rate": 2.388666666666667e-05,
      "loss": 0.0019,
      "step": 78340
    },
    {
      "epoch": 4.1786666666666665,
      "grad_norm": 0.33141833543777466,
      "learning_rate": 2.3883333333333336e-05,
      "loss": 0.0022,
      "step": 78350
    },
    {
      "epoch": 4.1792,
      "grad_norm": 0.4387127757072449,
      "learning_rate": 2.3880000000000002e-05,
      "loss": 0.0018,
      "step": 78360
    },
    {
      "epoch": 4.179733333333333,
      "grad_norm": 0.3085434138774872,
      "learning_rate": 2.3876666666666668e-05,
      "loss": 0.0025,
      "step": 78370
    },
    {
      "epoch": 4.180266666666666,
      "grad_norm": 0.5036158561706543,
      "learning_rate": 2.3873333333333334e-05,
      "loss": 0.0019,
      "step": 78380
    },
    {
      "epoch": 4.1808,
      "grad_norm": 0.43592485785484314,
      "learning_rate": 2.387e-05,
      "loss": 0.0016,
      "step": 78390
    },
    {
      "epoch": 4.181333333333333,
      "grad_norm": 0.6147910356521606,
      "learning_rate": 2.3866666666666666e-05,
      "loss": 0.0023,
      "step": 78400
    },
    {
      "epoch": 4.181866666666667,
      "grad_norm": 0.212395578622818,
      "learning_rate": 2.3863333333333336e-05,
      "loss": 0.0018,
      "step": 78410
    },
    {
      "epoch": 4.1824,
      "grad_norm": 0.33985111117362976,
      "learning_rate": 2.3860000000000002e-05,
      "loss": 0.0014,
      "step": 78420
    },
    {
      "epoch": 4.182933333333334,
      "grad_norm": 0.05568591132760048,
      "learning_rate": 2.3856666666666668e-05,
      "loss": 0.0021,
      "step": 78430
    },
    {
      "epoch": 4.183466666666667,
      "grad_norm": 0.12125685065984726,
      "learning_rate": 2.3853333333333334e-05,
      "loss": 0.0021,
      "step": 78440
    },
    {
      "epoch": 4.184,
      "grad_norm": 0.19167964160442352,
      "learning_rate": 2.385e-05,
      "loss": 0.0023,
      "step": 78450
    },
    {
      "epoch": 4.184533333333333,
      "grad_norm": 0.18766991794109344,
      "learning_rate": 2.3846666666666666e-05,
      "loss": 0.0023,
      "step": 78460
    },
    {
      "epoch": 4.185066666666667,
      "grad_norm": 0.22721250355243683,
      "learning_rate": 2.3843333333333333e-05,
      "loss": 0.0032,
      "step": 78470
    },
    {
      "epoch": 4.1856,
      "grad_norm": 0.45532873272895813,
      "learning_rate": 2.3840000000000002e-05,
      "loss": 0.0025,
      "step": 78480
    },
    {
      "epoch": 4.186133333333333,
      "grad_norm": 0.3251906633377075,
      "learning_rate": 2.3836666666666668e-05,
      "loss": 0.0015,
      "step": 78490
    },
    {
      "epoch": 4.1866666666666665,
      "grad_norm": 0.0678229033946991,
      "learning_rate": 2.3833333333333334e-05,
      "loss": 0.002,
      "step": 78500
    },
    {
      "epoch": 4.1872,
      "grad_norm": 0.399370014667511,
      "learning_rate": 2.3830000000000004e-05,
      "loss": 0.0025,
      "step": 78510
    },
    {
      "epoch": 4.187733333333333,
      "grad_norm": 0.22433142364025116,
      "learning_rate": 2.3826666666666667e-05,
      "loss": 0.0016,
      "step": 78520
    },
    {
      "epoch": 4.188266666666666,
      "grad_norm": 0.2584679424762726,
      "learning_rate": 2.3823333333333333e-05,
      "loss": 0.0023,
      "step": 78530
    },
    {
      "epoch": 4.1888,
      "grad_norm": 0.279977411031723,
      "learning_rate": 2.3820000000000002e-05,
      "loss": 0.0018,
      "step": 78540
    },
    {
      "epoch": 4.189333333333333,
      "grad_norm": 0.45249682664871216,
      "learning_rate": 2.381666666666667e-05,
      "loss": 0.0026,
      "step": 78550
    },
    {
      "epoch": 4.189866666666667,
      "grad_norm": 0.24420222640037537,
      "learning_rate": 2.3813333333333335e-05,
      "loss": 0.0024,
      "step": 78560
    },
    {
      "epoch": 4.1904,
      "grad_norm": 0.0481744222342968,
      "learning_rate": 2.381e-05,
      "loss": 0.0019,
      "step": 78570
    },
    {
      "epoch": 4.190933333333334,
      "grad_norm": 0.2173723578453064,
      "learning_rate": 2.380666666666667e-05,
      "loss": 0.0026,
      "step": 78580
    },
    {
      "epoch": 4.191466666666667,
      "grad_norm": 0.4015507996082306,
      "learning_rate": 2.3803333333333336e-05,
      "loss": 0.002,
      "step": 78590
    },
    {
      "epoch": 4.192,
      "grad_norm": 0.10764613747596741,
      "learning_rate": 2.38e-05,
      "loss": 0.002,
      "step": 78600
    },
    {
      "epoch": 4.1925333333333334,
      "grad_norm": 0.5732651352882385,
      "learning_rate": 2.379666666666667e-05,
      "loss": 0.0018,
      "step": 78610
    },
    {
      "epoch": 4.193066666666667,
      "grad_norm": 0.1322924792766571,
      "learning_rate": 2.3793333333333335e-05,
      "loss": 0.0021,
      "step": 78620
    },
    {
      "epoch": 4.1936,
      "grad_norm": 0.3387722074985504,
      "learning_rate": 2.379e-05,
      "loss": 0.0024,
      "step": 78630
    },
    {
      "epoch": 4.194133333333333,
      "grad_norm": 0.21797648072242737,
      "learning_rate": 2.3786666666666667e-05,
      "loss": 0.0014,
      "step": 78640
    },
    {
      "epoch": 4.1946666666666665,
      "grad_norm": 0.5071637630462646,
      "learning_rate": 2.3783333333333337e-05,
      "loss": 0.0015,
      "step": 78650
    },
    {
      "epoch": 4.1952,
      "grad_norm": 0.3105790615081787,
      "learning_rate": 2.3780000000000003e-05,
      "loss": 0.0018,
      "step": 78660
    },
    {
      "epoch": 4.195733333333333,
      "grad_norm": 0.28098466992378235,
      "learning_rate": 2.3776666666666665e-05,
      "loss": 0.0032,
      "step": 78670
    },
    {
      "epoch": 4.196266666666666,
      "grad_norm": 0.09792939573526382,
      "learning_rate": 2.3773333333333335e-05,
      "loss": 0.0013,
      "step": 78680
    },
    {
      "epoch": 4.1968,
      "grad_norm": 0.21684344112873077,
      "learning_rate": 2.377e-05,
      "loss": 0.0023,
      "step": 78690
    },
    {
      "epoch": 4.197333333333333,
      "grad_norm": 0.2512597441673279,
      "learning_rate": 2.3766666666666667e-05,
      "loss": 0.0013,
      "step": 78700
    },
    {
      "epoch": 4.197866666666667,
      "grad_norm": 0.07045536488294601,
      "learning_rate": 2.3763333333333333e-05,
      "loss": 0.0017,
      "step": 78710
    },
    {
      "epoch": 4.1984,
      "grad_norm": 0.42290884256362915,
      "learning_rate": 2.3760000000000003e-05,
      "loss": 0.0017,
      "step": 78720
    },
    {
      "epoch": 4.198933333333334,
      "grad_norm": 0.3442397117614746,
      "learning_rate": 2.375666666666667e-05,
      "loss": 0.0025,
      "step": 78730
    },
    {
      "epoch": 4.199466666666667,
      "grad_norm": 0.06766649335622787,
      "learning_rate": 2.3753333333333335e-05,
      "loss": 0.0023,
      "step": 78740
    },
    {
      "epoch": 4.2,
      "grad_norm": 0.15438327193260193,
      "learning_rate": 2.375e-05,
      "loss": 0.0023,
      "step": 78750
    },
    {
      "epoch": 4.2005333333333335,
      "grad_norm": 0.47709596157073975,
      "learning_rate": 2.3746666666666667e-05,
      "loss": 0.0018,
      "step": 78760
    },
    {
      "epoch": 4.201066666666667,
      "grad_norm": 0.2200719565153122,
      "learning_rate": 2.3743333333333334e-05,
      "loss": 0.0018,
      "step": 78770
    },
    {
      "epoch": 4.2016,
      "grad_norm": 0.5717551708221436,
      "learning_rate": 2.374e-05,
      "loss": 0.0024,
      "step": 78780
    },
    {
      "epoch": 4.202133333333333,
      "grad_norm": 0.049533501267433167,
      "learning_rate": 2.373666666666667e-05,
      "loss": 0.0023,
      "step": 78790
    },
    {
      "epoch": 4.2026666666666666,
      "grad_norm": 0.14493286609649658,
      "learning_rate": 2.3733333333333335e-05,
      "loss": 0.0019,
      "step": 78800
    },
    {
      "epoch": 4.2032,
      "grad_norm": 0.36092719435691833,
      "learning_rate": 2.373e-05,
      "loss": 0.0014,
      "step": 78810
    },
    {
      "epoch": 4.203733333333333,
      "grad_norm": 0.24942517280578613,
      "learning_rate": 2.3726666666666668e-05,
      "loss": 0.0021,
      "step": 78820
    },
    {
      "epoch": 4.204266666666666,
      "grad_norm": 0.18927520513534546,
      "learning_rate": 2.3723333333333334e-05,
      "loss": 0.0022,
      "step": 78830
    },
    {
      "epoch": 4.2048,
      "grad_norm": 0.17985016107559204,
      "learning_rate": 2.372e-05,
      "loss": 0.0024,
      "step": 78840
    },
    {
      "epoch": 4.205333333333333,
      "grad_norm": 0.33813658356666565,
      "learning_rate": 2.3716666666666666e-05,
      "loss": 0.0016,
      "step": 78850
    },
    {
      "epoch": 4.205866666666667,
      "grad_norm": 0.1540689617395401,
      "learning_rate": 2.3713333333333336e-05,
      "loss": 0.0027,
      "step": 78860
    },
    {
      "epoch": 4.2064,
      "grad_norm": 0.1972048282623291,
      "learning_rate": 2.371e-05,
      "loss": 0.0024,
      "step": 78870
    },
    {
      "epoch": 4.206933333333334,
      "grad_norm": 0.09509799629449844,
      "learning_rate": 2.3706666666666668e-05,
      "loss": 0.0018,
      "step": 78880
    },
    {
      "epoch": 4.207466666666667,
      "grad_norm": 0.15050721168518066,
      "learning_rate": 2.3703333333333337e-05,
      "loss": 0.0021,
      "step": 78890
    },
    {
      "epoch": 4.208,
      "grad_norm": 0.16524185240268707,
      "learning_rate": 2.37e-05,
      "loss": 0.0016,
      "step": 78900
    },
    {
      "epoch": 4.2085333333333335,
      "grad_norm": 0.13095292448997498,
      "learning_rate": 2.3696666666666666e-05,
      "loss": 0.0019,
      "step": 78910
    },
    {
      "epoch": 4.209066666666667,
      "grad_norm": 0.138590008020401,
      "learning_rate": 2.3693333333333332e-05,
      "loss": 0.0024,
      "step": 78920
    },
    {
      "epoch": 4.2096,
      "grad_norm": 0.16128800809383392,
      "learning_rate": 2.3690000000000002e-05,
      "loss": 0.0016,
      "step": 78930
    },
    {
      "epoch": 4.210133333333333,
      "grad_norm": 0.07690908759832382,
      "learning_rate": 2.3686666666666668e-05,
      "loss": 0.0029,
      "step": 78940
    },
    {
      "epoch": 4.210666666666667,
      "grad_norm": 0.7081965804100037,
      "learning_rate": 2.3683333333333334e-05,
      "loss": 0.0027,
      "step": 78950
    },
    {
      "epoch": 4.2112,
      "grad_norm": 0.6814103722572327,
      "learning_rate": 2.3680000000000004e-05,
      "loss": 0.0016,
      "step": 78960
    },
    {
      "epoch": 4.211733333333333,
      "grad_norm": 0.33387044072151184,
      "learning_rate": 2.3676666666666666e-05,
      "loss": 0.0029,
      "step": 78970
    },
    {
      "epoch": 4.212266666666666,
      "grad_norm": 0.5615130066871643,
      "learning_rate": 2.3673333333333333e-05,
      "loss": 0.0024,
      "step": 78980
    },
    {
      "epoch": 4.2128,
      "grad_norm": 0.2912229597568512,
      "learning_rate": 2.3670000000000002e-05,
      "loss": 0.0023,
      "step": 78990
    },
    {
      "epoch": 4.213333333333333,
      "grad_norm": 0.24248106777668,
      "learning_rate": 2.3666666666666668e-05,
      "loss": 0.0034,
      "step": 79000
    },
    {
      "epoch": 4.213866666666667,
      "grad_norm": 0.3085378110408783,
      "learning_rate": 2.3663333333333334e-05,
      "loss": 0.0028,
      "step": 79010
    },
    {
      "epoch": 4.2144,
      "grad_norm": 0.1888592392206192,
      "learning_rate": 2.366e-05,
      "loss": 0.0018,
      "step": 79020
    },
    {
      "epoch": 4.214933333333334,
      "grad_norm": 0.34215837717056274,
      "learning_rate": 2.365666666666667e-05,
      "loss": 0.0022,
      "step": 79030
    },
    {
      "epoch": 4.215466666666667,
      "grad_norm": 0.22118917107582092,
      "learning_rate": 2.3653333333333336e-05,
      "loss": 0.0025,
      "step": 79040
    },
    {
      "epoch": 4.216,
      "grad_norm": 0.4635956883430481,
      "learning_rate": 2.365e-05,
      "loss": 0.0025,
      "step": 79050
    },
    {
      "epoch": 4.2165333333333335,
      "grad_norm": 0.3059322237968445,
      "learning_rate": 2.364666666666667e-05,
      "loss": 0.002,
      "step": 79060
    },
    {
      "epoch": 4.217066666666667,
      "grad_norm": 0.354012668132782,
      "learning_rate": 2.3643333333333335e-05,
      "loss": 0.0021,
      "step": 79070
    },
    {
      "epoch": 4.2176,
      "grad_norm": 0.505835771560669,
      "learning_rate": 2.364e-05,
      "loss": 0.002,
      "step": 79080
    },
    {
      "epoch": 4.218133333333333,
      "grad_norm": 0.23247076570987701,
      "learning_rate": 2.3636666666666667e-05,
      "loss": 0.003,
      "step": 79090
    },
    {
      "epoch": 4.218666666666667,
      "grad_norm": 0.5178696513175964,
      "learning_rate": 2.3633333333333336e-05,
      "loss": 0.0026,
      "step": 79100
    },
    {
      "epoch": 4.2192,
      "grad_norm": 0.09495539963245392,
      "learning_rate": 2.3630000000000002e-05,
      "loss": 0.0016,
      "step": 79110
    },
    {
      "epoch": 4.219733333333333,
      "grad_norm": 0.21410351991653442,
      "learning_rate": 2.362666666666667e-05,
      "loss": 0.0026,
      "step": 79120
    },
    {
      "epoch": 4.220266666666666,
      "grad_norm": 0.18587477505207062,
      "learning_rate": 2.3623333333333335e-05,
      "loss": 0.0017,
      "step": 79130
    },
    {
      "epoch": 4.2208,
      "grad_norm": 0.5301737189292908,
      "learning_rate": 2.362e-05,
      "loss": 0.0019,
      "step": 79140
    },
    {
      "epoch": 4.221333333333333,
      "grad_norm": 0.14770343899726868,
      "learning_rate": 2.3616666666666667e-05,
      "loss": 0.0014,
      "step": 79150
    },
    {
      "epoch": 4.221866666666667,
      "grad_norm": 0.06429698318243027,
      "learning_rate": 2.3613333333333333e-05,
      "loss": 0.0019,
      "step": 79160
    },
    {
      "epoch": 4.2224,
      "grad_norm": 0.23857589066028595,
      "learning_rate": 2.3610000000000003e-05,
      "loss": 0.0031,
      "step": 79170
    },
    {
      "epoch": 4.222933333333334,
      "grad_norm": 0.3713729679584503,
      "learning_rate": 2.360666666666667e-05,
      "loss": 0.0017,
      "step": 79180
    },
    {
      "epoch": 4.223466666666667,
      "grad_norm": 0.05922548100352287,
      "learning_rate": 2.3603333333333335e-05,
      "loss": 0.0016,
      "step": 79190
    },
    {
      "epoch": 4.224,
      "grad_norm": 0.1965140700340271,
      "learning_rate": 2.36e-05,
      "loss": 0.0019,
      "step": 79200
    },
    {
      "epoch": 4.2245333333333335,
      "grad_norm": 0.251391738653183,
      "learning_rate": 2.3596666666666667e-05,
      "loss": 0.0017,
      "step": 79210
    },
    {
      "epoch": 4.225066666666667,
      "grad_norm": 0.41639095544815063,
      "learning_rate": 2.3593333333333333e-05,
      "loss": 0.0025,
      "step": 79220
    },
    {
      "epoch": 4.2256,
      "grad_norm": 0.07249442487955093,
      "learning_rate": 2.359e-05,
      "loss": 0.0014,
      "step": 79230
    },
    {
      "epoch": 4.226133333333333,
      "grad_norm": 0.15678173303604126,
      "learning_rate": 2.358666666666667e-05,
      "loss": 0.0022,
      "step": 79240
    },
    {
      "epoch": 4.226666666666667,
      "grad_norm": 0.1505713164806366,
      "learning_rate": 2.3583333333333335e-05,
      "loss": 0.0021,
      "step": 79250
    },
    {
      "epoch": 4.2272,
      "grad_norm": 0.3363250195980072,
      "learning_rate": 2.358e-05,
      "loss": 0.0031,
      "step": 79260
    },
    {
      "epoch": 4.227733333333333,
      "grad_norm": 0.15590786933898926,
      "learning_rate": 2.357666666666667e-05,
      "loss": 0.0025,
      "step": 79270
    },
    {
      "epoch": 4.228266666666666,
      "grad_norm": 0.6616342663764954,
      "learning_rate": 2.3573333333333334e-05,
      "loss": 0.002,
      "step": 79280
    },
    {
      "epoch": 4.2288,
      "grad_norm": 0.1596456915140152,
      "learning_rate": 2.357e-05,
      "loss": 0.0012,
      "step": 79290
    },
    {
      "epoch": 4.229333333333333,
      "grad_norm": 0.5247638821601868,
      "learning_rate": 2.3566666666666666e-05,
      "loss": 0.0017,
      "step": 79300
    },
    {
      "epoch": 4.229866666666666,
      "grad_norm": 0.3666025698184967,
      "learning_rate": 2.3563333333333335e-05,
      "loss": 0.0015,
      "step": 79310
    },
    {
      "epoch": 4.2304,
      "grad_norm": 0.3818349242210388,
      "learning_rate": 2.356e-05,
      "loss": 0.002,
      "step": 79320
    },
    {
      "epoch": 4.230933333333334,
      "grad_norm": 0.10744981467723846,
      "learning_rate": 2.3556666666666668e-05,
      "loss": 0.0013,
      "step": 79330
    },
    {
      "epoch": 4.231466666666667,
      "grad_norm": 0.11925232410430908,
      "learning_rate": 2.3553333333333337e-05,
      "loss": 0.0019,
      "step": 79340
    },
    {
      "epoch": 4.232,
      "grad_norm": 0.4763190448284149,
      "learning_rate": 2.355e-05,
      "loss": 0.0027,
      "step": 79350
    },
    {
      "epoch": 4.2325333333333335,
      "grad_norm": 0.5538992881774902,
      "learning_rate": 2.3546666666666666e-05,
      "loss": 0.0023,
      "step": 79360
    },
    {
      "epoch": 4.233066666666667,
      "grad_norm": 0.6941999197006226,
      "learning_rate": 2.3543333333333335e-05,
      "loss": 0.0028,
      "step": 79370
    },
    {
      "epoch": 4.2336,
      "grad_norm": 0.18160846829414368,
      "learning_rate": 2.354e-05,
      "loss": 0.0015,
      "step": 79380
    },
    {
      "epoch": 4.234133333333333,
      "grad_norm": 0.04540874436497688,
      "learning_rate": 2.3536666666666668e-05,
      "loss": 0.0018,
      "step": 79390
    },
    {
      "epoch": 4.234666666666667,
      "grad_norm": 0.18512925505638123,
      "learning_rate": 2.3533333333333334e-05,
      "loss": 0.0022,
      "step": 79400
    },
    {
      "epoch": 4.2352,
      "grad_norm": 0.1922491192817688,
      "learning_rate": 2.3530000000000003e-05,
      "loss": 0.0013,
      "step": 79410
    },
    {
      "epoch": 4.235733333333333,
      "grad_norm": 0.27052953839302063,
      "learning_rate": 2.352666666666667e-05,
      "loss": 0.0029,
      "step": 79420
    },
    {
      "epoch": 4.236266666666666,
      "grad_norm": 0.056173596531152725,
      "learning_rate": 2.3523333333333332e-05,
      "loss": 0.0022,
      "step": 79430
    },
    {
      "epoch": 4.2368,
      "grad_norm": 0.0546274296939373,
      "learning_rate": 2.3520000000000002e-05,
      "loss": 0.0027,
      "step": 79440
    },
    {
      "epoch": 4.237333333333333,
      "grad_norm": 0.1729622185230255,
      "learning_rate": 2.3516666666666668e-05,
      "loss": 0.0032,
      "step": 79450
    },
    {
      "epoch": 4.237866666666667,
      "grad_norm": 0.32382524013519287,
      "learning_rate": 2.3513333333333334e-05,
      "loss": 0.0023,
      "step": 79460
    },
    {
      "epoch": 4.2384,
      "grad_norm": 0.3342689573764801,
      "learning_rate": 2.351e-05,
      "loss": 0.002,
      "step": 79470
    },
    {
      "epoch": 4.238933333333334,
      "grad_norm": 0.8279695510864258,
      "learning_rate": 2.350666666666667e-05,
      "loss": 0.0024,
      "step": 79480
    },
    {
      "epoch": 4.239466666666667,
      "grad_norm": 0.3422965109348297,
      "learning_rate": 2.3503333333333336e-05,
      "loss": 0.0018,
      "step": 79490
    },
    {
      "epoch": 4.24,
      "grad_norm": 0.3052406907081604,
      "learning_rate": 2.35e-05,
      "loss": 0.0018,
      "step": 79500
    },
    {
      "epoch": 4.2405333333333335,
      "grad_norm": 0.2570561170578003,
      "learning_rate": 2.3496666666666668e-05,
      "loss": 0.0018,
      "step": 79510
    },
    {
      "epoch": 4.241066666666667,
      "grad_norm": 0.15695105493068695,
      "learning_rate": 2.3493333333333334e-05,
      "loss": 0.0017,
      "step": 79520
    },
    {
      "epoch": 4.2416,
      "grad_norm": 0.33385998010635376,
      "learning_rate": 2.349e-05,
      "loss": 0.0011,
      "step": 79530
    },
    {
      "epoch": 4.242133333333333,
      "grad_norm": 0.174870565533638,
      "learning_rate": 2.3486666666666667e-05,
      "loss": 0.0017,
      "step": 79540
    },
    {
      "epoch": 4.242666666666667,
      "grad_norm": 0.16233381628990173,
      "learning_rate": 2.3483333333333336e-05,
      "loss": 0.0036,
      "step": 79550
    },
    {
      "epoch": 4.2432,
      "grad_norm": 0.5444600582122803,
      "learning_rate": 2.3480000000000002e-05,
      "loss": 0.0017,
      "step": 79560
    },
    {
      "epoch": 4.243733333333333,
      "grad_norm": 0.611110508441925,
      "learning_rate": 2.347666666666667e-05,
      "loss": 0.0017,
      "step": 79570
    },
    {
      "epoch": 4.244266666666666,
      "grad_norm": 0.43122515082359314,
      "learning_rate": 2.3473333333333334e-05,
      "loss": 0.002,
      "step": 79580
    },
    {
      "epoch": 4.2448,
      "grad_norm": 0.387285977602005,
      "learning_rate": 2.347e-05,
      "loss": 0.0017,
      "step": 79590
    },
    {
      "epoch": 4.245333333333333,
      "grad_norm": 0.2492886483669281,
      "learning_rate": 2.3466666666666667e-05,
      "loss": 0.0025,
      "step": 79600
    },
    {
      "epoch": 4.245866666666666,
      "grad_norm": 0.15474285185337067,
      "learning_rate": 2.3463333333333333e-05,
      "loss": 0.0027,
      "step": 79610
    },
    {
      "epoch": 4.2464,
      "grad_norm": 0.3272544741630554,
      "learning_rate": 2.3460000000000002e-05,
      "loss": 0.0029,
      "step": 79620
    },
    {
      "epoch": 4.246933333333334,
      "grad_norm": 0.09818707406520844,
      "learning_rate": 2.345666666666667e-05,
      "loss": 0.0019,
      "step": 79630
    },
    {
      "epoch": 4.247466666666667,
      "grad_norm": 0.22260220348834991,
      "learning_rate": 2.3453333333333335e-05,
      "loss": 0.0023,
      "step": 79640
    },
    {
      "epoch": 4.248,
      "grad_norm": 0.2470032274723053,
      "learning_rate": 2.345e-05,
      "loss": 0.0017,
      "step": 79650
    },
    {
      "epoch": 4.2485333333333335,
      "grad_norm": 0.24289868772029877,
      "learning_rate": 2.3446666666666667e-05,
      "loss": 0.0019,
      "step": 79660
    },
    {
      "epoch": 4.249066666666667,
      "grad_norm": 0.48533591628074646,
      "learning_rate": 2.3443333333333333e-05,
      "loss": 0.0021,
      "step": 79670
    },
    {
      "epoch": 4.2496,
      "grad_norm": 0.27335965633392334,
      "learning_rate": 2.344e-05,
      "loss": 0.0014,
      "step": 79680
    },
    {
      "epoch": 4.250133333333333,
      "grad_norm": 0.33993712067604065,
      "learning_rate": 2.343666666666667e-05,
      "loss": 0.0018,
      "step": 79690
    },
    {
      "epoch": 4.250666666666667,
      "grad_norm": 0.09869769960641861,
      "learning_rate": 2.3433333333333335e-05,
      "loss": 0.0018,
      "step": 79700
    },
    {
      "epoch": 4.2512,
      "grad_norm": 0.06701154261827469,
      "learning_rate": 2.343e-05,
      "loss": 0.0012,
      "step": 79710
    },
    {
      "epoch": 4.251733333333333,
      "grad_norm": 0.7545527815818787,
      "learning_rate": 2.342666666666667e-05,
      "loss": 0.002,
      "step": 79720
    },
    {
      "epoch": 4.252266666666666,
      "grad_norm": 0.19015707075595856,
      "learning_rate": 2.3423333333333333e-05,
      "loss": 0.0016,
      "step": 79730
    },
    {
      "epoch": 4.2528,
      "grad_norm": 0.08857500553131104,
      "learning_rate": 2.342e-05,
      "loss": 0.0022,
      "step": 79740
    },
    {
      "epoch": 4.253333333333333,
      "grad_norm": 0.08683443069458008,
      "learning_rate": 2.341666666666667e-05,
      "loss": 0.0015,
      "step": 79750
    },
    {
      "epoch": 4.253866666666667,
      "grad_norm": 0.23993517458438873,
      "learning_rate": 2.3413333333333335e-05,
      "loss": 0.0017,
      "step": 79760
    },
    {
      "epoch": 4.2544,
      "grad_norm": 0.27356842160224915,
      "learning_rate": 2.341e-05,
      "loss": 0.0022,
      "step": 79770
    },
    {
      "epoch": 4.254933333333334,
      "grad_norm": 0.23295125365257263,
      "learning_rate": 2.3406666666666667e-05,
      "loss": 0.0016,
      "step": 79780
    },
    {
      "epoch": 4.255466666666667,
      "grad_norm": 0.4386069178581238,
      "learning_rate": 2.3403333333333337e-05,
      "loss": 0.0023,
      "step": 79790
    },
    {
      "epoch": 4.256,
      "grad_norm": 0.07391209900379181,
      "learning_rate": 2.3400000000000003e-05,
      "loss": 0.0017,
      "step": 79800
    },
    {
      "epoch": 4.2565333333333335,
      "grad_norm": 0.1587364226579666,
      "learning_rate": 2.3396666666666666e-05,
      "loss": 0.0023,
      "step": 79810
    },
    {
      "epoch": 4.257066666666667,
      "grad_norm": 0.44346874952316284,
      "learning_rate": 2.3393333333333335e-05,
      "loss": 0.0022,
      "step": 79820
    },
    {
      "epoch": 4.2576,
      "grad_norm": 0.20840278267860413,
      "learning_rate": 2.339e-05,
      "loss": 0.0026,
      "step": 79830
    },
    {
      "epoch": 4.258133333333333,
      "grad_norm": 0.18229444324970245,
      "learning_rate": 2.3386666666666668e-05,
      "loss": 0.0022,
      "step": 79840
    },
    {
      "epoch": 4.258666666666667,
      "grad_norm": 0.6418907046318054,
      "learning_rate": 2.3383333333333334e-05,
      "loss": 0.0017,
      "step": 79850
    },
    {
      "epoch": 4.2592,
      "grad_norm": 0.13394378125667572,
      "learning_rate": 2.3380000000000003e-05,
      "loss": 0.0026,
      "step": 79860
    },
    {
      "epoch": 4.259733333333333,
      "grad_norm": 0.27027174830436707,
      "learning_rate": 2.337666666666667e-05,
      "loss": 0.0023,
      "step": 79870
    },
    {
      "epoch": 4.260266666666666,
      "grad_norm": 0.18489345908164978,
      "learning_rate": 2.3373333333333332e-05,
      "loss": 0.0019,
      "step": 79880
    },
    {
      "epoch": 4.2608,
      "grad_norm": 0.09433632344007492,
      "learning_rate": 2.337e-05,
      "loss": 0.0018,
      "step": 79890
    },
    {
      "epoch": 4.261333333333333,
      "grad_norm": 0.1958065927028656,
      "learning_rate": 2.3366666666666668e-05,
      "loss": 0.0016,
      "step": 79900
    },
    {
      "epoch": 4.261866666666666,
      "grad_norm": 0.13924260437488556,
      "learning_rate": 2.3363333333333334e-05,
      "loss": 0.0024,
      "step": 79910
    },
    {
      "epoch": 4.2624,
      "grad_norm": 0.26679232716560364,
      "learning_rate": 2.336e-05,
      "loss": 0.0021,
      "step": 79920
    },
    {
      "epoch": 4.262933333333334,
      "grad_norm": 0.18433670699596405,
      "learning_rate": 2.335666666666667e-05,
      "loss": 0.0018,
      "step": 79930
    },
    {
      "epoch": 4.263466666666667,
      "grad_norm": 0.6049301624298096,
      "learning_rate": 2.3353333333333336e-05,
      "loss": 0.0017,
      "step": 79940
    },
    {
      "epoch": 4.264,
      "grad_norm": 0.21730917692184448,
      "learning_rate": 2.3350000000000002e-05,
      "loss": 0.0015,
      "step": 79950
    },
    {
      "epoch": 4.2645333333333335,
      "grad_norm": 0.2237311154603958,
      "learning_rate": 2.3346666666666668e-05,
      "loss": 0.0021,
      "step": 79960
    },
    {
      "epoch": 4.265066666666667,
      "grad_norm": 0.11968135088682175,
      "learning_rate": 2.3343333333333334e-05,
      "loss": 0.0018,
      "step": 79970
    },
    {
      "epoch": 4.2656,
      "grad_norm": 0.09692924469709396,
      "learning_rate": 2.334e-05,
      "loss": 0.0013,
      "step": 79980
    },
    {
      "epoch": 4.266133333333333,
      "grad_norm": 0.04249197617173195,
      "learning_rate": 2.3336666666666666e-05,
      "loss": 0.002,
      "step": 79990
    },
    {
      "epoch": 4.266666666666667,
      "grad_norm": 0.08957591652870178,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 0.0022,
      "step": 80000
    },
    {
      "epoch": 4.2672,
      "grad_norm": 0.266596257686615,
      "learning_rate": 2.3330000000000002e-05,
      "loss": 0.002,
      "step": 80010
    },
    {
      "epoch": 4.267733333333333,
      "grad_norm": 0.3249865174293518,
      "learning_rate": 2.3326666666666668e-05,
      "loss": 0.0017,
      "step": 80020
    },
    {
      "epoch": 4.268266666666666,
      "grad_norm": 0.05655520409345627,
      "learning_rate": 2.3323333333333334e-05,
      "loss": 0.0021,
      "step": 80030
    },
    {
      "epoch": 4.2688,
      "grad_norm": 0.09677529335021973,
      "learning_rate": 2.332e-05,
      "loss": 0.0023,
      "step": 80040
    },
    {
      "epoch": 4.269333333333333,
      "grad_norm": 0.31972503662109375,
      "learning_rate": 2.3316666666666666e-05,
      "loss": 0.002,
      "step": 80050
    },
    {
      "epoch": 4.269866666666666,
      "grad_norm": 0.29529687762260437,
      "learning_rate": 2.3313333333333333e-05,
      "loss": 0.0024,
      "step": 80060
    },
    {
      "epoch": 4.2704,
      "grad_norm": 0.06472664326429367,
      "learning_rate": 2.3310000000000002e-05,
      "loss": 0.0025,
      "step": 80070
    },
    {
      "epoch": 4.270933333333334,
      "grad_norm": 0.038141924887895584,
      "learning_rate": 2.3306666666666668e-05,
      "loss": 0.0016,
      "step": 80080
    },
    {
      "epoch": 4.271466666666667,
      "grad_norm": 0.24214234948158264,
      "learning_rate": 2.3303333333333334e-05,
      "loss": 0.0017,
      "step": 80090
    },
    {
      "epoch": 4.272,
      "grad_norm": 0.22937051951885223,
      "learning_rate": 2.3300000000000004e-05,
      "loss": 0.0026,
      "step": 80100
    },
    {
      "epoch": 4.2725333333333335,
      "grad_norm": 0.0963006466627121,
      "learning_rate": 2.3296666666666667e-05,
      "loss": 0.0022,
      "step": 80110
    },
    {
      "epoch": 4.273066666666667,
      "grad_norm": 0.2552340626716614,
      "learning_rate": 2.3293333333333333e-05,
      "loss": 0.0024,
      "step": 80120
    },
    {
      "epoch": 4.2736,
      "grad_norm": 0.4680193364620209,
      "learning_rate": 2.3290000000000002e-05,
      "loss": 0.0017,
      "step": 80130
    },
    {
      "epoch": 4.274133333333333,
      "grad_norm": 0.08241181075572968,
      "learning_rate": 2.328666666666667e-05,
      "loss": 0.0022,
      "step": 80140
    },
    {
      "epoch": 4.274666666666667,
      "grad_norm": 0.08083396404981613,
      "learning_rate": 2.3283333333333335e-05,
      "loss": 0.0025,
      "step": 80150
    },
    {
      "epoch": 4.2752,
      "grad_norm": 0.21667444705963135,
      "learning_rate": 2.328e-05,
      "loss": 0.0018,
      "step": 80160
    },
    {
      "epoch": 4.275733333333333,
      "grad_norm": 0.6689413785934448,
      "learning_rate": 2.327666666666667e-05,
      "loss": 0.0016,
      "step": 80170
    },
    {
      "epoch": 4.276266666666666,
      "grad_norm": 0.4658529758453369,
      "learning_rate": 2.3273333333333333e-05,
      "loss": 0.0024,
      "step": 80180
    },
    {
      "epoch": 4.2768,
      "grad_norm": 0.2623012065887451,
      "learning_rate": 2.327e-05,
      "loss": 0.0021,
      "step": 80190
    },
    {
      "epoch": 4.277333333333333,
      "grad_norm": 0.03999001532793045,
      "learning_rate": 2.326666666666667e-05,
      "loss": 0.0018,
      "step": 80200
    },
    {
      "epoch": 4.277866666666666,
      "grad_norm": 0.3651481866836548,
      "learning_rate": 2.3263333333333335e-05,
      "loss": 0.0022,
      "step": 80210
    },
    {
      "epoch": 4.2783999999999995,
      "grad_norm": 0.6665841937065125,
      "learning_rate": 2.326e-05,
      "loss": 0.0018,
      "step": 80220
    },
    {
      "epoch": 4.278933333333334,
      "grad_norm": 0.13736051321029663,
      "learning_rate": 2.3256666666666667e-05,
      "loss": 0.0022,
      "step": 80230
    },
    {
      "epoch": 4.279466666666667,
      "grad_norm": 0.04688888415694237,
      "learning_rate": 2.3253333333333337e-05,
      "loss": 0.003,
      "step": 80240
    },
    {
      "epoch": 4.28,
      "grad_norm": 0.3335336148738861,
      "learning_rate": 2.3250000000000003e-05,
      "loss": 0.0017,
      "step": 80250
    },
    {
      "epoch": 4.2805333333333335,
      "grad_norm": 0.49533945322036743,
      "learning_rate": 2.3246666666666665e-05,
      "loss": 0.002,
      "step": 80260
    },
    {
      "epoch": 4.281066666666667,
      "grad_norm": 0.17104148864746094,
      "learning_rate": 2.3243333333333335e-05,
      "loss": 0.0018,
      "step": 80270
    },
    {
      "epoch": 4.2816,
      "grad_norm": 0.4659157991409302,
      "learning_rate": 2.324e-05,
      "loss": 0.0022,
      "step": 80280
    },
    {
      "epoch": 4.282133333333333,
      "grad_norm": 0.1315043866634369,
      "learning_rate": 2.3236666666666667e-05,
      "loss": 0.0014,
      "step": 80290
    },
    {
      "epoch": 4.282666666666667,
      "grad_norm": 0.20904585719108582,
      "learning_rate": 2.3233333333333333e-05,
      "loss": 0.002,
      "step": 80300
    },
    {
      "epoch": 4.2832,
      "grad_norm": 0.20790937542915344,
      "learning_rate": 2.3230000000000003e-05,
      "loss": 0.0019,
      "step": 80310
    },
    {
      "epoch": 4.283733333333333,
      "grad_norm": 0.4213270843029022,
      "learning_rate": 2.322666666666667e-05,
      "loss": 0.0025,
      "step": 80320
    },
    {
      "epoch": 4.2842666666666664,
      "grad_norm": 0.05734488368034363,
      "learning_rate": 2.3223333333333335e-05,
      "loss": 0.0025,
      "step": 80330
    },
    {
      "epoch": 4.2848,
      "grad_norm": 0.28945937752723694,
      "learning_rate": 2.322e-05,
      "loss": 0.0021,
      "step": 80340
    },
    {
      "epoch": 4.285333333333333,
      "grad_norm": 0.20079179108142853,
      "learning_rate": 2.3216666666666667e-05,
      "loss": 0.0015,
      "step": 80350
    },
    {
      "epoch": 4.285866666666666,
      "grad_norm": 0.6873865127563477,
      "learning_rate": 2.3213333333333334e-05,
      "loss": 0.0021,
      "step": 80360
    },
    {
      "epoch": 4.2864,
      "grad_norm": 0.032865047454833984,
      "learning_rate": 2.321e-05,
      "loss": 0.0028,
      "step": 80370
    },
    {
      "epoch": 4.286933333333334,
      "grad_norm": 0.1253783106803894,
      "learning_rate": 2.320666666666667e-05,
      "loss": 0.0024,
      "step": 80380
    },
    {
      "epoch": 4.287466666666667,
      "grad_norm": 0.2562938928604126,
      "learning_rate": 2.3203333333333335e-05,
      "loss": 0.0012,
      "step": 80390
    },
    {
      "epoch": 4.288,
      "grad_norm": 0.09707648307085037,
      "learning_rate": 2.32e-05,
      "loss": 0.002,
      "step": 80400
    },
    {
      "epoch": 4.2885333333333335,
      "grad_norm": 0.08345888555049896,
      "learning_rate": 2.3196666666666668e-05,
      "loss": 0.0016,
      "step": 80410
    },
    {
      "epoch": 4.289066666666667,
      "grad_norm": 0.09157129377126694,
      "learning_rate": 2.3193333333333334e-05,
      "loss": 0.002,
      "step": 80420
    },
    {
      "epoch": 4.2896,
      "grad_norm": 0.27773135900497437,
      "learning_rate": 2.319e-05,
      "loss": 0.002,
      "step": 80430
    },
    {
      "epoch": 4.290133333333333,
      "grad_norm": 0.054008789360523224,
      "learning_rate": 2.3186666666666666e-05,
      "loss": 0.002,
      "step": 80440
    },
    {
      "epoch": 4.290666666666667,
      "grad_norm": 0.15423691272735596,
      "learning_rate": 2.3183333333333336e-05,
      "loss": 0.0024,
      "step": 80450
    },
    {
      "epoch": 4.2912,
      "grad_norm": 0.15180917084217072,
      "learning_rate": 2.318e-05,
      "loss": 0.0021,
      "step": 80460
    },
    {
      "epoch": 4.291733333333333,
      "grad_norm": 0.25742802023887634,
      "learning_rate": 2.3176666666666668e-05,
      "loss": 0.002,
      "step": 80470
    },
    {
      "epoch": 4.2922666666666665,
      "grad_norm": 0.024800051003694534,
      "learning_rate": 2.3173333333333337e-05,
      "loss": 0.0016,
      "step": 80480
    },
    {
      "epoch": 4.2928,
      "grad_norm": 0.17942281067371368,
      "learning_rate": 2.317e-05,
      "loss": 0.0021,
      "step": 80490
    },
    {
      "epoch": 4.293333333333333,
      "grad_norm": 0.07039763033390045,
      "learning_rate": 2.3166666666666666e-05,
      "loss": 0.0026,
      "step": 80500
    },
    {
      "epoch": 4.293866666666666,
      "grad_norm": 0.18436765670776367,
      "learning_rate": 2.3163333333333336e-05,
      "loss": 0.002,
      "step": 80510
    },
    {
      "epoch": 4.2943999999999996,
      "grad_norm": 0.07697071880102158,
      "learning_rate": 2.3160000000000002e-05,
      "loss": 0.0023,
      "step": 80520
    },
    {
      "epoch": 4.294933333333334,
      "grad_norm": 0.28224149346351624,
      "learning_rate": 2.3156666666666668e-05,
      "loss": 0.0023,
      "step": 80530
    },
    {
      "epoch": 4.295466666666667,
      "grad_norm": 0.1902400255203247,
      "learning_rate": 2.3153333333333334e-05,
      "loss": 0.0015,
      "step": 80540
    },
    {
      "epoch": 4.296,
      "grad_norm": 0.3662657141685486,
      "learning_rate": 2.3150000000000004e-05,
      "loss": 0.0021,
      "step": 80550
    },
    {
      "epoch": 4.2965333333333335,
      "grad_norm": 0.4962998330593109,
      "learning_rate": 2.3146666666666666e-05,
      "loss": 0.002,
      "step": 80560
    },
    {
      "epoch": 4.297066666666667,
      "grad_norm": 0.4322766959667206,
      "learning_rate": 2.3143333333333333e-05,
      "loss": 0.002,
      "step": 80570
    },
    {
      "epoch": 4.2976,
      "grad_norm": 0.15243537724018097,
      "learning_rate": 2.3140000000000002e-05,
      "loss": 0.0017,
      "step": 80580
    },
    {
      "epoch": 4.298133333333333,
      "grad_norm": 0.2759358286857605,
      "learning_rate": 2.3136666666666668e-05,
      "loss": 0.0015,
      "step": 80590
    },
    {
      "epoch": 4.298666666666667,
      "grad_norm": 0.30345848202705383,
      "learning_rate": 2.3133333333333334e-05,
      "loss": 0.0018,
      "step": 80600
    },
    {
      "epoch": 4.2992,
      "grad_norm": 0.2226720154285431,
      "learning_rate": 2.313e-05,
      "loss": 0.0017,
      "step": 80610
    },
    {
      "epoch": 4.299733333333333,
      "grad_norm": 0.04533243551850319,
      "learning_rate": 2.312666666666667e-05,
      "loss": 0.0021,
      "step": 80620
    },
    {
      "epoch": 4.3002666666666665,
      "grad_norm": 0.12904880940914154,
      "learning_rate": 2.3123333333333336e-05,
      "loss": 0.002,
      "step": 80630
    },
    {
      "epoch": 4.3008,
      "grad_norm": 0.17539571225643158,
      "learning_rate": 2.312e-05,
      "loss": 0.0014,
      "step": 80640
    },
    {
      "epoch": 4.301333333333333,
      "grad_norm": 0.2396114468574524,
      "learning_rate": 2.311666666666667e-05,
      "loss": 0.0026,
      "step": 80650
    },
    {
      "epoch": 4.301866666666666,
      "grad_norm": 0.1274174600839615,
      "learning_rate": 2.3113333333333335e-05,
      "loss": 0.0021,
      "step": 80660
    },
    {
      "epoch": 4.3024000000000004,
      "grad_norm": 0.1450948268175125,
      "learning_rate": 2.311e-05,
      "loss": 0.0015,
      "step": 80670
    },
    {
      "epoch": 4.302933333333334,
      "grad_norm": 0.15393905341625214,
      "learning_rate": 2.3106666666666667e-05,
      "loss": 0.0018,
      "step": 80680
    },
    {
      "epoch": 4.303466666666667,
      "grad_norm": 0.2814747989177704,
      "learning_rate": 2.3103333333333336e-05,
      "loss": 0.0015,
      "step": 80690
    },
    {
      "epoch": 4.304,
      "grad_norm": 0.06916891038417816,
      "learning_rate": 2.3100000000000002e-05,
      "loss": 0.0028,
      "step": 80700
    },
    {
      "epoch": 4.3045333333333335,
      "grad_norm": 0.06997910887002945,
      "learning_rate": 2.3096666666666665e-05,
      "loss": 0.0014,
      "step": 80710
    },
    {
      "epoch": 4.305066666666667,
      "grad_norm": 0.04694955050945282,
      "learning_rate": 2.3093333333333335e-05,
      "loss": 0.0017,
      "step": 80720
    },
    {
      "epoch": 4.3056,
      "grad_norm": 0.3299921751022339,
      "learning_rate": 2.309e-05,
      "loss": 0.0019,
      "step": 80730
    },
    {
      "epoch": 4.306133333333333,
      "grad_norm": 0.1419488787651062,
      "learning_rate": 2.3086666666666667e-05,
      "loss": 0.0022,
      "step": 80740
    },
    {
      "epoch": 4.306666666666667,
      "grad_norm": 0.2263849377632141,
      "learning_rate": 2.3083333333333333e-05,
      "loss": 0.0026,
      "step": 80750
    },
    {
      "epoch": 4.3072,
      "grad_norm": 0.051762327551841736,
      "learning_rate": 2.3080000000000003e-05,
      "loss": 0.0024,
      "step": 80760
    },
    {
      "epoch": 4.307733333333333,
      "grad_norm": 0.06021150201559067,
      "learning_rate": 2.307666666666667e-05,
      "loss": 0.0015,
      "step": 80770
    },
    {
      "epoch": 4.3082666666666665,
      "grad_norm": 0.045618508011102676,
      "learning_rate": 2.3073333333333335e-05,
      "loss": 0.0021,
      "step": 80780
    },
    {
      "epoch": 4.3088,
      "grad_norm": 0.12961897253990173,
      "learning_rate": 2.307e-05,
      "loss": 0.0023,
      "step": 80790
    },
    {
      "epoch": 4.309333333333333,
      "grad_norm": 0.3065939247608185,
      "learning_rate": 2.3066666666666667e-05,
      "loss": 0.0019,
      "step": 80800
    },
    {
      "epoch": 4.309866666666666,
      "grad_norm": 0.2439226508140564,
      "learning_rate": 2.3063333333333333e-05,
      "loss": 0.003,
      "step": 80810
    },
    {
      "epoch": 4.3104,
      "grad_norm": 0.2710798978805542,
      "learning_rate": 2.306e-05,
      "loss": 0.0022,
      "step": 80820
    },
    {
      "epoch": 4.310933333333334,
      "grad_norm": 0.3941435217857361,
      "learning_rate": 2.305666666666667e-05,
      "loss": 0.0018,
      "step": 80830
    },
    {
      "epoch": 4.311466666666667,
      "grad_norm": 0.6300156712532043,
      "learning_rate": 2.3053333333333335e-05,
      "loss": 0.002,
      "step": 80840
    },
    {
      "epoch": 4.312,
      "grad_norm": 0.19412165880203247,
      "learning_rate": 2.305e-05,
      "loss": 0.0015,
      "step": 80850
    },
    {
      "epoch": 4.3125333333333336,
      "grad_norm": 0.3383466601371765,
      "learning_rate": 2.3046666666666667e-05,
      "loss": 0.0017,
      "step": 80860
    },
    {
      "epoch": 4.313066666666667,
      "grad_norm": 0.06823847442865372,
      "learning_rate": 2.3043333333333334e-05,
      "loss": 0.0023,
      "step": 80870
    },
    {
      "epoch": 4.3136,
      "grad_norm": 0.048998381942510605,
      "learning_rate": 2.304e-05,
      "loss": 0.002,
      "step": 80880
    },
    {
      "epoch": 4.314133333333333,
      "grad_norm": 0.16930197179317474,
      "learning_rate": 2.303666666666667e-05,
      "loss": 0.0025,
      "step": 80890
    },
    {
      "epoch": 4.314666666666667,
      "grad_norm": 0.31940358877182007,
      "learning_rate": 2.3033333333333335e-05,
      "loss": 0.0021,
      "step": 80900
    },
    {
      "epoch": 4.3152,
      "grad_norm": 0.46907487511634827,
      "learning_rate": 2.303e-05,
      "loss": 0.0016,
      "step": 80910
    },
    {
      "epoch": 4.315733333333333,
      "grad_norm": 0.15218563377857208,
      "learning_rate": 2.3026666666666668e-05,
      "loss": 0.002,
      "step": 80920
    },
    {
      "epoch": 4.3162666666666665,
      "grad_norm": 0.260759174823761,
      "learning_rate": 2.3023333333333337e-05,
      "loss": 0.0024,
      "step": 80930
    },
    {
      "epoch": 4.3168,
      "grad_norm": 0.3270706534385681,
      "learning_rate": 2.302e-05,
      "loss": 0.0022,
      "step": 80940
    },
    {
      "epoch": 4.317333333333333,
      "grad_norm": 0.06191717088222504,
      "learning_rate": 2.3016666666666666e-05,
      "loss": 0.0021,
      "step": 80950
    },
    {
      "epoch": 4.317866666666666,
      "grad_norm": 0.045647986233234406,
      "learning_rate": 2.3013333333333335e-05,
      "loss": 0.0021,
      "step": 80960
    },
    {
      "epoch": 4.3184000000000005,
      "grad_norm": 0.19502486288547516,
      "learning_rate": 2.301e-05,
      "loss": 0.0021,
      "step": 80970
    },
    {
      "epoch": 4.318933333333334,
      "grad_norm": 0.18919311463832855,
      "learning_rate": 2.3006666666666668e-05,
      "loss": 0.0014,
      "step": 80980
    },
    {
      "epoch": 4.319466666666667,
      "grad_norm": 0.12578219175338745,
      "learning_rate": 2.3003333333333334e-05,
      "loss": 0.0019,
      "step": 80990
    },
    {
      "epoch": 4.32,
      "grad_norm": 0.125013068318367,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.003,
      "step": 81000
    },
    {
      "epoch": 4.320533333333334,
      "grad_norm": 0.29730620980262756,
      "learning_rate": 2.299666666666667e-05,
      "loss": 0.0022,
      "step": 81010
    },
    {
      "epoch": 4.321066666666667,
      "grad_norm": 0.13535436987876892,
      "learning_rate": 2.2993333333333332e-05,
      "loss": 0.0014,
      "step": 81020
    },
    {
      "epoch": 4.3216,
      "grad_norm": 0.45639050006866455,
      "learning_rate": 2.2990000000000002e-05,
      "loss": 0.0024,
      "step": 81030
    },
    {
      "epoch": 4.322133333333333,
      "grad_norm": 0.04802655056118965,
      "learning_rate": 2.2986666666666668e-05,
      "loss": 0.0018,
      "step": 81040
    },
    {
      "epoch": 4.322666666666667,
      "grad_norm": 0.34072238206863403,
      "learning_rate": 2.2983333333333334e-05,
      "loss": 0.0016,
      "step": 81050
    },
    {
      "epoch": 4.3232,
      "grad_norm": 0.10482148081064224,
      "learning_rate": 2.298e-05,
      "loss": 0.0021,
      "step": 81060
    },
    {
      "epoch": 4.323733333333333,
      "grad_norm": 0.2462465465068817,
      "learning_rate": 2.297666666666667e-05,
      "loss": 0.0017,
      "step": 81070
    },
    {
      "epoch": 4.3242666666666665,
      "grad_norm": 0.3632104992866516,
      "learning_rate": 2.2973333333333336e-05,
      "loss": 0.0025,
      "step": 81080
    },
    {
      "epoch": 4.3248,
      "grad_norm": 0.42870911955833435,
      "learning_rate": 2.297e-05,
      "loss": 0.0017,
      "step": 81090
    },
    {
      "epoch": 4.325333333333333,
      "grad_norm": 0.3417927324771881,
      "learning_rate": 2.2966666666666668e-05,
      "loss": 0.0016,
      "step": 81100
    },
    {
      "epoch": 4.325866666666666,
      "grad_norm": 0.34588658809661865,
      "learning_rate": 2.2963333333333334e-05,
      "loss": 0.0016,
      "step": 81110
    },
    {
      "epoch": 4.3264,
      "grad_norm": 0.192000150680542,
      "learning_rate": 2.296e-05,
      "loss": 0.0027,
      "step": 81120
    },
    {
      "epoch": 4.326933333333334,
      "grad_norm": 0.05591795593500137,
      "learning_rate": 2.2956666666666667e-05,
      "loss": 0.0019,
      "step": 81130
    },
    {
      "epoch": 4.327466666666667,
      "grad_norm": 0.08915382623672485,
      "learning_rate": 2.2953333333333336e-05,
      "loss": 0.0021,
      "step": 81140
    },
    {
      "epoch": 4.328,
      "grad_norm": 0.485745370388031,
      "learning_rate": 2.2950000000000002e-05,
      "loss": 0.0025,
      "step": 81150
    },
    {
      "epoch": 4.328533333333334,
      "grad_norm": 0.10042509436607361,
      "learning_rate": 2.294666666666667e-05,
      "loss": 0.0025,
      "step": 81160
    },
    {
      "epoch": 4.329066666666667,
      "grad_norm": 0.7386131286621094,
      "learning_rate": 2.2943333333333334e-05,
      "loss": 0.0022,
      "step": 81170
    },
    {
      "epoch": 4.3296,
      "grad_norm": 0.549216091632843,
      "learning_rate": 2.294e-05,
      "loss": 0.0028,
      "step": 81180
    },
    {
      "epoch": 4.330133333333333,
      "grad_norm": 0.11869150400161743,
      "learning_rate": 2.2936666666666667e-05,
      "loss": 0.0019,
      "step": 81190
    },
    {
      "epoch": 4.330666666666667,
      "grad_norm": 0.24521541595458984,
      "learning_rate": 2.2933333333333333e-05,
      "loss": 0.0021,
      "step": 81200
    },
    {
      "epoch": 4.3312,
      "grad_norm": 0.08051641285419464,
      "learning_rate": 2.2930000000000002e-05,
      "loss": 0.0022,
      "step": 81210
    },
    {
      "epoch": 4.331733333333333,
      "grad_norm": 0.43306368589401245,
      "learning_rate": 2.292666666666667e-05,
      "loss": 0.002,
      "step": 81220
    },
    {
      "epoch": 4.3322666666666665,
      "grad_norm": 0.10008205473423004,
      "learning_rate": 2.2923333333333335e-05,
      "loss": 0.0021,
      "step": 81230
    },
    {
      "epoch": 4.3328,
      "grad_norm": 0.18405666947364807,
      "learning_rate": 2.292e-05,
      "loss": 0.0018,
      "step": 81240
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 0.3001258075237274,
      "learning_rate": 2.2916666666666667e-05,
      "loss": 0.0017,
      "step": 81250
    },
    {
      "epoch": 4.333866666666666,
      "grad_norm": 0.5509470105171204,
      "learning_rate": 2.2913333333333333e-05,
      "loss": 0.0028,
      "step": 81260
    },
    {
      "epoch": 4.3344,
      "grad_norm": 0.021308721974492073,
      "learning_rate": 2.2910000000000003e-05,
      "loss": 0.0016,
      "step": 81270
    },
    {
      "epoch": 4.334933333333334,
      "grad_norm": 0.16605494916439056,
      "learning_rate": 2.290666666666667e-05,
      "loss": 0.0016,
      "step": 81280
    },
    {
      "epoch": 4.335466666666667,
      "grad_norm": 0.37190771102905273,
      "learning_rate": 2.2903333333333335e-05,
      "loss": 0.0019,
      "step": 81290
    },
    {
      "epoch": 4.336,
      "grad_norm": 0.24449655413627625,
      "learning_rate": 2.29e-05,
      "loss": 0.0017,
      "step": 81300
    },
    {
      "epoch": 4.336533333333334,
      "grad_norm": 0.19878418743610382,
      "learning_rate": 2.289666666666667e-05,
      "loss": 0.0023,
      "step": 81310
    },
    {
      "epoch": 4.337066666666667,
      "grad_norm": 0.08839312940835953,
      "learning_rate": 2.2893333333333333e-05,
      "loss": 0.0022,
      "step": 81320
    },
    {
      "epoch": 4.3376,
      "grad_norm": 0.33394524455070496,
      "learning_rate": 2.289e-05,
      "loss": 0.0017,
      "step": 81330
    },
    {
      "epoch": 4.338133333333333,
      "grad_norm": 0.09070959687232971,
      "learning_rate": 2.288666666666667e-05,
      "loss": 0.0024,
      "step": 81340
    },
    {
      "epoch": 4.338666666666667,
      "grad_norm": 0.08162816613912582,
      "learning_rate": 2.2883333333333335e-05,
      "loss": 0.0022,
      "step": 81350
    },
    {
      "epoch": 4.3392,
      "grad_norm": 0.2762814164161682,
      "learning_rate": 2.288e-05,
      "loss": 0.002,
      "step": 81360
    },
    {
      "epoch": 4.339733333333333,
      "grad_norm": 0.2713507115840912,
      "learning_rate": 2.2876666666666667e-05,
      "loss": 0.0031,
      "step": 81370
    },
    {
      "epoch": 4.3402666666666665,
      "grad_norm": 0.5416070222854614,
      "learning_rate": 2.2873333333333337e-05,
      "loss": 0.004,
      "step": 81380
    },
    {
      "epoch": 4.3408,
      "grad_norm": 0.6093692779541016,
      "learning_rate": 2.287e-05,
      "loss": 0.0019,
      "step": 81390
    },
    {
      "epoch": 4.341333333333333,
      "grad_norm": 0.36642491817474365,
      "learning_rate": 2.2866666666666666e-05,
      "loss": 0.0018,
      "step": 81400
    },
    {
      "epoch": 4.341866666666666,
      "grad_norm": 0.5469039082527161,
      "learning_rate": 2.2863333333333335e-05,
      "loss": 0.0015,
      "step": 81410
    },
    {
      "epoch": 4.3424,
      "grad_norm": 0.07563536614179611,
      "learning_rate": 2.286e-05,
      "loss": 0.0012,
      "step": 81420
    },
    {
      "epoch": 4.342933333333333,
      "grad_norm": 0.29330500960350037,
      "learning_rate": 2.2856666666666667e-05,
      "loss": 0.0022,
      "step": 81430
    },
    {
      "epoch": 4.343466666666667,
      "grad_norm": 0.27478793263435364,
      "learning_rate": 2.2853333333333334e-05,
      "loss": 0.0013,
      "step": 81440
    },
    {
      "epoch": 4.344,
      "grad_norm": 0.05338320508599281,
      "learning_rate": 2.2850000000000003e-05,
      "loss": 0.0012,
      "step": 81450
    },
    {
      "epoch": 4.344533333333334,
      "grad_norm": 0.23731489479541779,
      "learning_rate": 2.284666666666667e-05,
      "loss": 0.0018,
      "step": 81460
    },
    {
      "epoch": 4.345066666666667,
      "grad_norm": 0.3347468078136444,
      "learning_rate": 2.2843333333333332e-05,
      "loss": 0.0017,
      "step": 81470
    },
    {
      "epoch": 4.3456,
      "grad_norm": 0.17904333770275116,
      "learning_rate": 2.284e-05,
      "loss": 0.0018,
      "step": 81480
    },
    {
      "epoch": 4.346133333333333,
      "grad_norm": 0.09646438807249069,
      "learning_rate": 2.2836666666666668e-05,
      "loss": 0.0014,
      "step": 81490
    },
    {
      "epoch": 4.346666666666667,
      "grad_norm": 0.6958910226821899,
      "learning_rate": 2.2833333333333334e-05,
      "loss": 0.002,
      "step": 81500
    },
    {
      "epoch": 4.3472,
      "grad_norm": 0.4492321312427521,
      "learning_rate": 2.283e-05,
      "loss": 0.0018,
      "step": 81510
    },
    {
      "epoch": 4.347733333333333,
      "grad_norm": 0.09373549371957779,
      "learning_rate": 2.282666666666667e-05,
      "loss": 0.0018,
      "step": 81520
    },
    {
      "epoch": 4.3482666666666665,
      "grad_norm": 0.05118459463119507,
      "learning_rate": 2.2823333333333336e-05,
      "loss": 0.0016,
      "step": 81530
    },
    {
      "epoch": 4.3488,
      "grad_norm": 0.18565835058689117,
      "learning_rate": 2.282e-05,
      "loss": 0.0025,
      "step": 81540
    },
    {
      "epoch": 4.349333333333333,
      "grad_norm": 0.10072140395641327,
      "learning_rate": 2.2816666666666668e-05,
      "loss": 0.0023,
      "step": 81550
    },
    {
      "epoch": 4.349866666666666,
      "grad_norm": 0.4232065975666046,
      "learning_rate": 2.2813333333333334e-05,
      "loss": 0.0017,
      "step": 81560
    },
    {
      "epoch": 4.3504,
      "grad_norm": 0.6104631423950195,
      "learning_rate": 2.281e-05,
      "loss": 0.0025,
      "step": 81570
    },
    {
      "epoch": 4.350933333333334,
      "grad_norm": 0.42312461137771606,
      "learning_rate": 2.2806666666666666e-05,
      "loss": 0.0021,
      "step": 81580
    },
    {
      "epoch": 4.351466666666667,
      "grad_norm": 0.04372262582182884,
      "learning_rate": 2.2803333333333336e-05,
      "loss": 0.0022,
      "step": 81590
    },
    {
      "epoch": 4.352,
      "grad_norm": 0.5465866327285767,
      "learning_rate": 2.2800000000000002e-05,
      "loss": 0.0016,
      "step": 81600
    },
    {
      "epoch": 4.352533333333334,
      "grad_norm": 0.3636684715747833,
      "learning_rate": 2.2796666666666668e-05,
      "loss": 0.0027,
      "step": 81610
    },
    {
      "epoch": 4.353066666666667,
      "grad_norm": 0.32528719305992126,
      "learning_rate": 2.2793333333333334e-05,
      "loss": 0.0024,
      "step": 81620
    },
    {
      "epoch": 4.3536,
      "grad_norm": 0.35488778352737427,
      "learning_rate": 2.279e-05,
      "loss": 0.0022,
      "step": 81630
    },
    {
      "epoch": 4.354133333333333,
      "grad_norm": 0.5255041122436523,
      "learning_rate": 2.2786666666666666e-05,
      "loss": 0.002,
      "step": 81640
    },
    {
      "epoch": 4.354666666666667,
      "grad_norm": 0.09474016726016998,
      "learning_rate": 2.2783333333333336e-05,
      "loss": 0.0016,
      "step": 81650
    },
    {
      "epoch": 4.3552,
      "grad_norm": 0.07366423308849335,
      "learning_rate": 2.2780000000000002e-05,
      "loss": 0.0027,
      "step": 81660
    },
    {
      "epoch": 4.355733333333333,
      "grad_norm": 0.16267937421798706,
      "learning_rate": 2.2776666666666668e-05,
      "loss": 0.0018,
      "step": 81670
    },
    {
      "epoch": 4.3562666666666665,
      "grad_norm": 0.07070706784725189,
      "learning_rate": 2.2773333333333334e-05,
      "loss": 0.0018,
      "step": 81680
    },
    {
      "epoch": 4.3568,
      "grad_norm": 0.15798939764499664,
      "learning_rate": 2.2770000000000004e-05,
      "loss": 0.0017,
      "step": 81690
    },
    {
      "epoch": 4.357333333333333,
      "grad_norm": 0.11494329571723938,
      "learning_rate": 2.2766666666666667e-05,
      "loss": 0.0017,
      "step": 81700
    },
    {
      "epoch": 4.357866666666666,
      "grad_norm": 0.4625793993473053,
      "learning_rate": 2.2763333333333333e-05,
      "loss": 0.0025,
      "step": 81710
    },
    {
      "epoch": 4.3584,
      "grad_norm": 0.24454915523529053,
      "learning_rate": 2.2760000000000002e-05,
      "loss": 0.0016,
      "step": 81720
    },
    {
      "epoch": 4.358933333333333,
      "grad_norm": 0.3580460548400879,
      "learning_rate": 2.275666666666667e-05,
      "loss": 0.0018,
      "step": 81730
    },
    {
      "epoch": 4.359466666666667,
      "grad_norm": 0.16758723556995392,
      "learning_rate": 2.2753333333333335e-05,
      "loss": 0.0019,
      "step": 81740
    },
    {
      "epoch": 4.36,
      "grad_norm": 0.34393924474716187,
      "learning_rate": 2.275e-05,
      "loss": 0.0022,
      "step": 81750
    },
    {
      "epoch": 4.360533333333334,
      "grad_norm": 0.07045633345842361,
      "learning_rate": 2.274666666666667e-05,
      "loss": 0.0023,
      "step": 81760
    },
    {
      "epoch": 4.361066666666667,
      "grad_norm": 0.3617088496685028,
      "learning_rate": 2.2743333333333333e-05,
      "loss": 0.002,
      "step": 81770
    },
    {
      "epoch": 4.3616,
      "grad_norm": 0.4388534426689148,
      "learning_rate": 2.274e-05,
      "loss": 0.0028,
      "step": 81780
    },
    {
      "epoch": 4.362133333333333,
      "grad_norm": 0.38005757331848145,
      "learning_rate": 2.273666666666667e-05,
      "loss": 0.0018,
      "step": 81790
    },
    {
      "epoch": 4.362666666666667,
      "grad_norm": 0.03871484845876694,
      "learning_rate": 2.2733333333333335e-05,
      "loss": 0.0025,
      "step": 81800
    },
    {
      "epoch": 4.3632,
      "grad_norm": 0.06440326571464539,
      "learning_rate": 2.273e-05,
      "loss": 0.0026,
      "step": 81810
    },
    {
      "epoch": 4.363733333333333,
      "grad_norm": 0.22221265733242035,
      "learning_rate": 2.2726666666666667e-05,
      "loss": 0.0015,
      "step": 81820
    },
    {
      "epoch": 4.3642666666666665,
      "grad_norm": 0.2727481424808502,
      "learning_rate": 2.2723333333333337e-05,
      "loss": 0.0023,
      "step": 81830
    },
    {
      "epoch": 4.3648,
      "grad_norm": 0.09852972626686096,
      "learning_rate": 2.2720000000000003e-05,
      "loss": 0.0023,
      "step": 81840
    },
    {
      "epoch": 4.365333333333333,
      "grad_norm": 0.06544086337089539,
      "learning_rate": 2.2716666666666665e-05,
      "loss": 0.0013,
      "step": 81850
    },
    {
      "epoch": 4.365866666666666,
      "grad_norm": 0.046126242727041245,
      "learning_rate": 2.2713333333333335e-05,
      "loss": 0.0021,
      "step": 81860
    },
    {
      "epoch": 4.3664,
      "grad_norm": 0.44112828373908997,
      "learning_rate": 2.271e-05,
      "loss": 0.0025,
      "step": 81870
    },
    {
      "epoch": 4.366933333333334,
      "grad_norm": 0.15310350060462952,
      "learning_rate": 2.2706666666666667e-05,
      "loss": 0.0026,
      "step": 81880
    },
    {
      "epoch": 4.367466666666667,
      "grad_norm": 0.07303479313850403,
      "learning_rate": 2.2703333333333333e-05,
      "loss": 0.0018,
      "step": 81890
    },
    {
      "epoch": 4.368,
      "grad_norm": 0.2316429764032364,
      "learning_rate": 2.2700000000000003e-05,
      "loss": 0.0014,
      "step": 81900
    },
    {
      "epoch": 4.368533333333334,
      "grad_norm": 0.365841805934906,
      "learning_rate": 2.269666666666667e-05,
      "loss": 0.0014,
      "step": 81910
    },
    {
      "epoch": 4.369066666666667,
      "grad_norm": 0.052721716463565826,
      "learning_rate": 2.2693333333333332e-05,
      "loss": 0.0015,
      "step": 81920
    },
    {
      "epoch": 4.3696,
      "grad_norm": 0.24467074871063232,
      "learning_rate": 2.269e-05,
      "loss": 0.0014,
      "step": 81930
    },
    {
      "epoch": 4.370133333333333,
      "grad_norm": 0.15576770901679993,
      "learning_rate": 2.2686666666666667e-05,
      "loss": 0.0016,
      "step": 81940
    },
    {
      "epoch": 4.370666666666667,
      "grad_norm": 0.0923406109213829,
      "learning_rate": 2.2683333333333334e-05,
      "loss": 0.0022,
      "step": 81950
    },
    {
      "epoch": 4.3712,
      "grad_norm": 0.20005255937576294,
      "learning_rate": 2.268e-05,
      "loss": 0.0015,
      "step": 81960
    },
    {
      "epoch": 4.371733333333333,
      "grad_norm": 0.2963472008705139,
      "learning_rate": 2.267666666666667e-05,
      "loss": 0.002,
      "step": 81970
    },
    {
      "epoch": 4.3722666666666665,
      "grad_norm": 0.05676931142807007,
      "learning_rate": 2.2673333333333335e-05,
      "loss": 0.0025,
      "step": 81980
    },
    {
      "epoch": 4.3728,
      "grad_norm": 0.16483758389949799,
      "learning_rate": 2.267e-05,
      "loss": 0.0014,
      "step": 81990
    },
    {
      "epoch": 4.373333333333333,
      "grad_norm": 0.30686822533607483,
      "learning_rate": 2.2666666666666668e-05,
      "loss": 0.0021,
      "step": 82000
    },
    {
      "epoch": 4.373866666666666,
      "grad_norm": 0.3322147727012634,
      "learning_rate": 2.2663333333333334e-05,
      "loss": 0.0017,
      "step": 82010
    },
    {
      "epoch": 4.3744,
      "grad_norm": 0.0644579604268074,
      "learning_rate": 2.266e-05,
      "loss": 0.0017,
      "step": 82020
    },
    {
      "epoch": 4.374933333333333,
      "grad_norm": 0.13262133300304413,
      "learning_rate": 2.2656666666666666e-05,
      "loss": 0.0015,
      "step": 82030
    },
    {
      "epoch": 4.375466666666667,
      "grad_norm": 0.5045192241668701,
      "learning_rate": 2.2653333333333336e-05,
      "loss": 0.0016,
      "step": 82040
    },
    {
      "epoch": 4.376,
      "grad_norm": 0.34134340286254883,
      "learning_rate": 2.265e-05,
      "loss": 0.0016,
      "step": 82050
    },
    {
      "epoch": 4.376533333333334,
      "grad_norm": 0.38741278648376465,
      "learning_rate": 2.2646666666666668e-05,
      "loss": 0.0018,
      "step": 82060
    },
    {
      "epoch": 4.377066666666667,
      "grad_norm": 0.3070186674594879,
      "learning_rate": 2.2643333333333334e-05,
      "loss": 0.0025,
      "step": 82070
    },
    {
      "epoch": 4.3776,
      "grad_norm": 0.16223067045211792,
      "learning_rate": 2.264e-05,
      "loss": 0.0019,
      "step": 82080
    },
    {
      "epoch": 4.378133333333333,
      "grad_norm": 0.610443651676178,
      "learning_rate": 2.2636666666666666e-05,
      "loss": 0.0018,
      "step": 82090
    },
    {
      "epoch": 4.378666666666667,
      "grad_norm": 0.396746963262558,
      "learning_rate": 2.2633333333333336e-05,
      "loss": 0.0018,
      "step": 82100
    },
    {
      "epoch": 4.3792,
      "grad_norm": 0.3755634129047394,
      "learning_rate": 2.2630000000000002e-05,
      "loss": 0.002,
      "step": 82110
    },
    {
      "epoch": 4.379733333333333,
      "grad_norm": 0.13058127462863922,
      "learning_rate": 2.2626666666666668e-05,
      "loss": 0.0018,
      "step": 82120
    },
    {
      "epoch": 4.3802666666666665,
      "grad_norm": 0.08242141455411911,
      "learning_rate": 2.2623333333333334e-05,
      "loss": 0.0018,
      "step": 82130
    },
    {
      "epoch": 4.3808,
      "grad_norm": 0.057840343564748764,
      "learning_rate": 2.2620000000000004e-05,
      "loss": 0.0021,
      "step": 82140
    },
    {
      "epoch": 4.381333333333333,
      "grad_norm": 0.5096540451049805,
      "learning_rate": 2.2616666666666666e-05,
      "loss": 0.0021,
      "step": 82150
    },
    {
      "epoch": 4.381866666666666,
      "grad_norm": 0.06877774000167847,
      "learning_rate": 2.2613333333333333e-05,
      "loss": 0.0018,
      "step": 82160
    },
    {
      "epoch": 4.3824,
      "grad_norm": 0.0359848327934742,
      "learning_rate": 2.2610000000000002e-05,
      "loss": 0.0018,
      "step": 82170
    },
    {
      "epoch": 4.382933333333334,
      "grad_norm": 0.07343018800020218,
      "learning_rate": 2.2606666666666668e-05,
      "loss": 0.0027,
      "step": 82180
    },
    {
      "epoch": 4.383466666666667,
      "grad_norm": 0.04144246131181717,
      "learning_rate": 2.2603333333333334e-05,
      "loss": 0.0014,
      "step": 82190
    },
    {
      "epoch": 4.384,
      "grad_norm": 0.045199815183877945,
      "learning_rate": 2.26e-05,
      "loss": 0.0018,
      "step": 82200
    },
    {
      "epoch": 4.384533333333334,
      "grad_norm": 0.29498767852783203,
      "learning_rate": 2.259666666666667e-05,
      "loss": 0.0019,
      "step": 82210
    },
    {
      "epoch": 4.385066666666667,
      "grad_norm": 0.38717934489250183,
      "learning_rate": 2.2593333333333336e-05,
      "loss": 0.0018,
      "step": 82220
    },
    {
      "epoch": 4.3856,
      "grad_norm": 0.12866997718811035,
      "learning_rate": 2.259e-05,
      "loss": 0.0026,
      "step": 82230
    },
    {
      "epoch": 4.386133333333333,
      "grad_norm": 0.05137547105550766,
      "learning_rate": 2.258666666666667e-05,
      "loss": 0.0015,
      "step": 82240
    },
    {
      "epoch": 4.386666666666667,
      "grad_norm": 0.1189369335770607,
      "learning_rate": 2.2583333333333335e-05,
      "loss": 0.0021,
      "step": 82250
    },
    {
      "epoch": 4.3872,
      "grad_norm": 0.16601905226707458,
      "learning_rate": 2.258e-05,
      "loss": 0.0025,
      "step": 82260
    },
    {
      "epoch": 4.387733333333333,
      "grad_norm": 0.4577074944972992,
      "learning_rate": 2.2576666666666667e-05,
      "loss": 0.0021,
      "step": 82270
    },
    {
      "epoch": 4.3882666666666665,
      "grad_norm": 0.08962132781744003,
      "learning_rate": 2.2573333333333336e-05,
      "loss": 0.0021,
      "step": 82280
    },
    {
      "epoch": 4.3888,
      "grad_norm": 0.31173717975616455,
      "learning_rate": 2.2570000000000002e-05,
      "loss": 0.0021,
      "step": 82290
    },
    {
      "epoch": 4.389333333333333,
      "grad_norm": 0.3329823315143585,
      "learning_rate": 2.2566666666666665e-05,
      "loss": 0.0023,
      "step": 82300
    },
    {
      "epoch": 4.389866666666666,
      "grad_norm": 0.4259507358074188,
      "learning_rate": 2.2563333333333335e-05,
      "loss": 0.0016,
      "step": 82310
    },
    {
      "epoch": 4.3904,
      "grad_norm": 0.10580462217330933,
      "learning_rate": 2.256e-05,
      "loss": 0.0018,
      "step": 82320
    },
    {
      "epoch": 4.390933333333333,
      "grad_norm": 0.2125290334224701,
      "learning_rate": 2.2556666666666667e-05,
      "loss": 0.0023,
      "step": 82330
    },
    {
      "epoch": 4.391466666666667,
      "grad_norm": 0.5068131685256958,
      "learning_rate": 2.2553333333333333e-05,
      "loss": 0.0016,
      "step": 82340
    },
    {
      "epoch": 4.392,
      "grad_norm": 0.12837111949920654,
      "learning_rate": 2.2550000000000003e-05,
      "loss": 0.0024,
      "step": 82350
    },
    {
      "epoch": 4.392533333333334,
      "grad_norm": 0.3589121997356415,
      "learning_rate": 2.254666666666667e-05,
      "loss": 0.0015,
      "step": 82360
    },
    {
      "epoch": 4.393066666666667,
      "grad_norm": 0.2016826868057251,
      "learning_rate": 2.2543333333333335e-05,
      "loss": 0.0023,
      "step": 82370
    },
    {
      "epoch": 4.3936,
      "grad_norm": 0.5445936322212219,
      "learning_rate": 2.254e-05,
      "loss": 0.0018,
      "step": 82380
    },
    {
      "epoch": 4.3941333333333334,
      "grad_norm": 0.19630195200443268,
      "learning_rate": 2.2536666666666667e-05,
      "loss": 0.0021,
      "step": 82390
    },
    {
      "epoch": 4.394666666666667,
      "grad_norm": 0.28845539689064026,
      "learning_rate": 2.2533333333333333e-05,
      "loss": 0.0016,
      "step": 82400
    },
    {
      "epoch": 4.3952,
      "grad_norm": 0.5625816583633423,
      "learning_rate": 2.253e-05,
      "loss": 0.0029,
      "step": 82410
    },
    {
      "epoch": 4.395733333333333,
      "grad_norm": 0.5480157732963562,
      "learning_rate": 2.252666666666667e-05,
      "loss": 0.002,
      "step": 82420
    },
    {
      "epoch": 4.3962666666666665,
      "grad_norm": 0.07516244798898697,
      "learning_rate": 2.2523333333333335e-05,
      "loss": 0.0015,
      "step": 82430
    },
    {
      "epoch": 4.3968,
      "grad_norm": 0.3092406988143921,
      "learning_rate": 2.252e-05,
      "loss": 0.0013,
      "step": 82440
    },
    {
      "epoch": 4.397333333333333,
      "grad_norm": 0.19895558059215546,
      "learning_rate": 2.2516666666666667e-05,
      "loss": 0.0026,
      "step": 82450
    },
    {
      "epoch": 4.397866666666666,
      "grad_norm": 0.2630491852760315,
      "learning_rate": 2.2513333333333333e-05,
      "loss": 0.0017,
      "step": 82460
    },
    {
      "epoch": 4.3984,
      "grad_norm": 0.259335994720459,
      "learning_rate": 2.251e-05,
      "loss": 0.0023,
      "step": 82470
    },
    {
      "epoch": 4.398933333333333,
      "grad_norm": 0.40355241298675537,
      "learning_rate": 2.250666666666667e-05,
      "loss": 0.0021,
      "step": 82480
    },
    {
      "epoch": 4.399466666666667,
      "grad_norm": 0.31378114223480225,
      "learning_rate": 2.2503333333333335e-05,
      "loss": 0.0029,
      "step": 82490
    },
    {
      "epoch": 4.4,
      "grad_norm": 0.35015130043029785,
      "learning_rate": 2.25e-05,
      "loss": 0.0027,
      "step": 82500
    },
    {
      "epoch": 4.400533333333334,
      "grad_norm": 0.19332380592823029,
      "learning_rate": 2.2496666666666668e-05,
      "loss": 0.0025,
      "step": 82510
    },
    {
      "epoch": 4.401066666666667,
      "grad_norm": 0.4219301640987396,
      "learning_rate": 2.2493333333333337e-05,
      "loss": 0.0016,
      "step": 82520
    },
    {
      "epoch": 4.4016,
      "grad_norm": 0.4023423194885254,
      "learning_rate": 2.249e-05,
      "loss": 0.0021,
      "step": 82530
    },
    {
      "epoch": 4.4021333333333335,
      "grad_norm": 0.2653791606426239,
      "learning_rate": 2.2486666666666666e-05,
      "loss": 0.0019,
      "step": 82540
    },
    {
      "epoch": 4.402666666666667,
      "grad_norm": 0.4258524179458618,
      "learning_rate": 2.2483333333333335e-05,
      "loss": 0.0016,
      "step": 82550
    },
    {
      "epoch": 4.4032,
      "grad_norm": 0.3018045127391815,
      "learning_rate": 2.248e-05,
      "loss": 0.0033,
      "step": 82560
    },
    {
      "epoch": 4.403733333333333,
      "grad_norm": 0.07919223606586456,
      "learning_rate": 2.2476666666666668e-05,
      "loss": 0.0018,
      "step": 82570
    },
    {
      "epoch": 4.4042666666666666,
      "grad_norm": 0.1782810389995575,
      "learning_rate": 2.2473333333333334e-05,
      "loss": 0.0016,
      "step": 82580
    },
    {
      "epoch": 4.4048,
      "grad_norm": 0.20357169210910797,
      "learning_rate": 2.2470000000000003e-05,
      "loss": 0.0016,
      "step": 82590
    },
    {
      "epoch": 4.405333333333333,
      "grad_norm": 0.24220144748687744,
      "learning_rate": 2.2466666666666666e-05,
      "loss": 0.0015,
      "step": 82600
    },
    {
      "epoch": 4.405866666666666,
      "grad_norm": 0.20828589797019958,
      "learning_rate": 2.2463333333333332e-05,
      "loss": 0.0025,
      "step": 82610
    },
    {
      "epoch": 4.4064,
      "grad_norm": 0.34131062030792236,
      "learning_rate": 2.2460000000000002e-05,
      "loss": 0.0022,
      "step": 82620
    },
    {
      "epoch": 4.406933333333333,
      "grad_norm": 0.2070034146308899,
      "learning_rate": 2.2456666666666668e-05,
      "loss": 0.0018,
      "step": 82630
    },
    {
      "epoch": 4.407466666666666,
      "grad_norm": 0.3208949863910675,
      "learning_rate": 2.2453333333333334e-05,
      "loss": 0.0019,
      "step": 82640
    },
    {
      "epoch": 4.408,
      "grad_norm": 0.3464091122150421,
      "learning_rate": 2.245e-05,
      "loss": 0.0015,
      "step": 82650
    },
    {
      "epoch": 4.408533333333334,
      "grad_norm": 0.11302598565816879,
      "learning_rate": 2.244666666666667e-05,
      "loss": 0.0023,
      "step": 82660
    },
    {
      "epoch": 4.409066666666667,
      "grad_norm": 0.12618078291416168,
      "learning_rate": 2.2443333333333336e-05,
      "loss": 0.0018,
      "step": 82670
    },
    {
      "epoch": 4.4096,
      "grad_norm": 0.26533979177474976,
      "learning_rate": 2.244e-05,
      "loss": 0.0028,
      "step": 82680
    },
    {
      "epoch": 4.4101333333333335,
      "grad_norm": 0.5294424295425415,
      "learning_rate": 2.2436666666666668e-05,
      "loss": 0.0018,
      "step": 82690
    },
    {
      "epoch": 4.410666666666667,
      "grad_norm": 0.36604538559913635,
      "learning_rate": 2.2433333333333334e-05,
      "loss": 0.0016,
      "step": 82700
    },
    {
      "epoch": 4.4112,
      "grad_norm": 0.21294313669204712,
      "learning_rate": 2.243e-05,
      "loss": 0.0018,
      "step": 82710
    },
    {
      "epoch": 4.411733333333333,
      "grad_norm": 0.0659259632229805,
      "learning_rate": 2.2426666666666667e-05,
      "loss": 0.0013,
      "step": 82720
    },
    {
      "epoch": 4.412266666666667,
      "grad_norm": 0.15878626704216003,
      "learning_rate": 2.2423333333333336e-05,
      "loss": 0.0014,
      "step": 82730
    },
    {
      "epoch": 4.4128,
      "grad_norm": 0.1109170913696289,
      "learning_rate": 2.2420000000000002e-05,
      "loss": 0.0014,
      "step": 82740
    },
    {
      "epoch": 4.413333333333333,
      "grad_norm": 0.2541288733482361,
      "learning_rate": 2.2416666666666665e-05,
      "loss": 0.0013,
      "step": 82750
    },
    {
      "epoch": 4.413866666666666,
      "grad_norm": 0.2390139251947403,
      "learning_rate": 2.2413333333333334e-05,
      "loss": 0.0016,
      "step": 82760
    },
    {
      "epoch": 4.4144,
      "grad_norm": 0.09557223320007324,
      "learning_rate": 2.241e-05,
      "loss": 0.0021,
      "step": 82770
    },
    {
      "epoch": 4.414933333333333,
      "grad_norm": 0.05547710880637169,
      "learning_rate": 2.2406666666666667e-05,
      "loss": 0.0017,
      "step": 82780
    },
    {
      "epoch": 4.415466666666667,
      "grad_norm": 0.3677801191806793,
      "learning_rate": 2.2403333333333333e-05,
      "loss": 0.0011,
      "step": 82790
    },
    {
      "epoch": 4.416,
      "grad_norm": 0.11057376861572266,
      "learning_rate": 2.2400000000000002e-05,
      "loss": 0.0028,
      "step": 82800
    },
    {
      "epoch": 4.416533333333334,
      "grad_norm": 0.08643321692943573,
      "learning_rate": 2.239666666666667e-05,
      "loss": 0.0016,
      "step": 82810
    },
    {
      "epoch": 4.417066666666667,
      "grad_norm": 0.3075213134288788,
      "learning_rate": 2.2393333333333335e-05,
      "loss": 0.0028,
      "step": 82820
    },
    {
      "epoch": 4.4176,
      "grad_norm": 0.0710785910487175,
      "learning_rate": 2.239e-05,
      "loss": 0.0015,
      "step": 82830
    },
    {
      "epoch": 4.4181333333333335,
      "grad_norm": 0.2777548134326935,
      "learning_rate": 2.2386666666666667e-05,
      "loss": 0.0028,
      "step": 82840
    },
    {
      "epoch": 4.418666666666667,
      "grad_norm": 0.4965144097805023,
      "learning_rate": 2.2383333333333333e-05,
      "loss": 0.0018,
      "step": 82850
    },
    {
      "epoch": 4.4192,
      "grad_norm": 0.18173250555992126,
      "learning_rate": 2.2380000000000003e-05,
      "loss": 0.002,
      "step": 82860
    },
    {
      "epoch": 4.419733333333333,
      "grad_norm": 0.28743433952331543,
      "learning_rate": 2.237666666666667e-05,
      "loss": 0.0025,
      "step": 82870
    },
    {
      "epoch": 4.420266666666667,
      "grad_norm": 0.10060623288154602,
      "learning_rate": 2.2373333333333335e-05,
      "loss": 0.0018,
      "step": 82880
    },
    {
      "epoch": 4.4208,
      "grad_norm": 0.10837411135435104,
      "learning_rate": 2.237e-05,
      "loss": 0.0034,
      "step": 82890
    },
    {
      "epoch": 4.421333333333333,
      "grad_norm": 0.16792601346969604,
      "learning_rate": 2.236666666666667e-05,
      "loss": 0.0033,
      "step": 82900
    },
    {
      "epoch": 4.421866666666666,
      "grad_norm": 0.1236577257514,
      "learning_rate": 2.2363333333333333e-05,
      "loss": 0.0017,
      "step": 82910
    },
    {
      "epoch": 4.4224,
      "grad_norm": 0.0683598741889,
      "learning_rate": 2.236e-05,
      "loss": 0.0013,
      "step": 82920
    },
    {
      "epoch": 4.422933333333333,
      "grad_norm": 0.3330293595790863,
      "learning_rate": 2.235666666666667e-05,
      "loss": 0.0016,
      "step": 82930
    },
    {
      "epoch": 4.423466666666666,
      "grad_norm": 0.5151059627532959,
      "learning_rate": 2.2353333333333335e-05,
      "loss": 0.0021,
      "step": 82940
    },
    {
      "epoch": 4.424,
      "grad_norm": 0.3069387972354889,
      "learning_rate": 2.235e-05,
      "loss": 0.0016,
      "step": 82950
    },
    {
      "epoch": 4.424533333333334,
      "grad_norm": 0.33971163630485535,
      "learning_rate": 2.2346666666666667e-05,
      "loss": 0.0017,
      "step": 82960
    },
    {
      "epoch": 4.425066666666667,
      "grad_norm": 0.3022695481777191,
      "learning_rate": 2.2343333333333337e-05,
      "loss": 0.0014,
      "step": 82970
    },
    {
      "epoch": 4.4256,
      "grad_norm": 0.04231136292219162,
      "learning_rate": 2.234e-05,
      "loss": 0.0021,
      "step": 82980
    },
    {
      "epoch": 4.4261333333333335,
      "grad_norm": 0.2741515040397644,
      "learning_rate": 2.2336666666666666e-05,
      "loss": 0.0031,
      "step": 82990
    },
    {
      "epoch": 4.426666666666667,
      "grad_norm": 0.0949573740363121,
      "learning_rate": 2.2333333333333335e-05,
      "loss": 0.0016,
      "step": 83000
    },
    {
      "epoch": 4.4272,
      "grad_norm": 0.2257583737373352,
      "learning_rate": 2.233e-05,
      "loss": 0.0024,
      "step": 83010
    },
    {
      "epoch": 4.427733333333333,
      "grad_norm": 0.4050753116607666,
      "learning_rate": 2.2326666666666667e-05,
      "loss": 0.002,
      "step": 83020
    },
    {
      "epoch": 4.428266666666667,
      "grad_norm": 0.03784676641225815,
      "learning_rate": 2.2323333333333334e-05,
      "loss": 0.0022,
      "step": 83030
    },
    {
      "epoch": 4.4288,
      "grad_norm": 0.34378933906555176,
      "learning_rate": 2.2320000000000003e-05,
      "loss": 0.0017,
      "step": 83040
    },
    {
      "epoch": 4.429333333333333,
      "grad_norm": 0.2082611620426178,
      "learning_rate": 2.231666666666667e-05,
      "loss": 0.0019,
      "step": 83050
    },
    {
      "epoch": 4.429866666666666,
      "grad_norm": 0.7296587824821472,
      "learning_rate": 2.2313333333333332e-05,
      "loss": 0.0017,
      "step": 83060
    },
    {
      "epoch": 4.4304,
      "grad_norm": 0.30684441328048706,
      "learning_rate": 2.231e-05,
      "loss": 0.002,
      "step": 83070
    },
    {
      "epoch": 4.430933333333333,
      "grad_norm": 0.40537768602371216,
      "learning_rate": 2.2306666666666668e-05,
      "loss": 0.0016,
      "step": 83080
    },
    {
      "epoch": 4.431466666666667,
      "grad_norm": 0.44835105538368225,
      "learning_rate": 2.2303333333333334e-05,
      "loss": 0.0012,
      "step": 83090
    },
    {
      "epoch": 4.432,
      "grad_norm": 0.47552841901779175,
      "learning_rate": 2.23e-05,
      "loss": 0.0016,
      "step": 83100
    },
    {
      "epoch": 4.432533333333334,
      "grad_norm": 0.2638135850429535,
      "learning_rate": 2.229666666666667e-05,
      "loss": 0.0015,
      "step": 83110
    },
    {
      "epoch": 4.433066666666667,
      "grad_norm": 0.2294382005929947,
      "learning_rate": 2.2293333333333336e-05,
      "loss": 0.0031,
      "step": 83120
    },
    {
      "epoch": 4.4336,
      "grad_norm": 0.28495386242866516,
      "learning_rate": 2.229e-05,
      "loss": 0.0014,
      "step": 83130
    },
    {
      "epoch": 4.4341333333333335,
      "grad_norm": 0.15455955266952515,
      "learning_rate": 2.2286666666666668e-05,
      "loss": 0.002,
      "step": 83140
    },
    {
      "epoch": 4.434666666666667,
      "grad_norm": 0.1278368979692459,
      "learning_rate": 2.2283333333333334e-05,
      "loss": 0.002,
      "step": 83150
    },
    {
      "epoch": 4.4352,
      "grad_norm": 0.09551648795604706,
      "learning_rate": 2.228e-05,
      "loss": 0.0021,
      "step": 83160
    },
    {
      "epoch": 4.435733333333333,
      "grad_norm": 0.32155147194862366,
      "learning_rate": 2.2276666666666666e-05,
      "loss": 0.0015,
      "step": 83170
    },
    {
      "epoch": 4.436266666666667,
      "grad_norm": 0.32919034361839294,
      "learning_rate": 2.2273333333333336e-05,
      "loss": 0.0022,
      "step": 83180
    },
    {
      "epoch": 4.4368,
      "grad_norm": 0.4199368953704834,
      "learning_rate": 2.2270000000000002e-05,
      "loss": 0.0018,
      "step": 83190
    },
    {
      "epoch": 4.437333333333333,
      "grad_norm": 0.24046440422534943,
      "learning_rate": 2.2266666666666668e-05,
      "loss": 0.0033,
      "step": 83200
    },
    {
      "epoch": 4.437866666666666,
      "grad_norm": 0.028391703963279724,
      "learning_rate": 2.2263333333333334e-05,
      "loss": 0.003,
      "step": 83210
    },
    {
      "epoch": 4.4384,
      "grad_norm": 0.2778925895690918,
      "learning_rate": 2.226e-05,
      "loss": 0.0017,
      "step": 83220
    },
    {
      "epoch": 4.438933333333333,
      "grad_norm": 0.11435895413160324,
      "learning_rate": 2.2256666666666666e-05,
      "loss": 0.0026,
      "step": 83230
    },
    {
      "epoch": 4.439466666666666,
      "grad_norm": 0.535169780254364,
      "learning_rate": 2.2253333333333336e-05,
      "loss": 0.0015,
      "step": 83240
    },
    {
      "epoch": 4.44,
      "grad_norm": 0.2215014547109604,
      "learning_rate": 2.2250000000000002e-05,
      "loss": 0.0017,
      "step": 83250
    },
    {
      "epoch": 4.440533333333334,
      "grad_norm": 0.14321087300777435,
      "learning_rate": 2.2246666666666668e-05,
      "loss": 0.002,
      "step": 83260
    },
    {
      "epoch": 4.441066666666667,
      "grad_norm": 0.05647532641887665,
      "learning_rate": 2.2243333333333334e-05,
      "loss": 0.0022,
      "step": 83270
    },
    {
      "epoch": 4.4416,
      "grad_norm": 0.07309994846582413,
      "learning_rate": 2.224e-05,
      "loss": 0.0021,
      "step": 83280
    },
    {
      "epoch": 4.4421333333333335,
      "grad_norm": 0.8024176955223083,
      "learning_rate": 2.2236666666666667e-05,
      "loss": 0.0017,
      "step": 83290
    },
    {
      "epoch": 4.442666666666667,
      "grad_norm": 0.30046346783638,
      "learning_rate": 2.2233333333333333e-05,
      "loss": 0.0015,
      "step": 83300
    },
    {
      "epoch": 4.4432,
      "grad_norm": 0.07302628457546234,
      "learning_rate": 2.2230000000000002e-05,
      "loss": 0.0019,
      "step": 83310
    },
    {
      "epoch": 4.443733333333333,
      "grad_norm": 0.5203888416290283,
      "learning_rate": 2.222666666666667e-05,
      "loss": 0.0017,
      "step": 83320
    },
    {
      "epoch": 4.444266666666667,
      "grad_norm": 0.333359032869339,
      "learning_rate": 2.2223333333333335e-05,
      "loss": 0.0018,
      "step": 83330
    },
    {
      "epoch": 4.4448,
      "grad_norm": 0.24574898183345795,
      "learning_rate": 2.222e-05,
      "loss": 0.0014,
      "step": 83340
    },
    {
      "epoch": 4.445333333333333,
      "grad_norm": 0.619535505771637,
      "learning_rate": 2.221666666666667e-05,
      "loss": 0.0022,
      "step": 83350
    },
    {
      "epoch": 4.445866666666666,
      "grad_norm": 0.06278613209724426,
      "learning_rate": 2.2213333333333333e-05,
      "loss": 0.0021,
      "step": 83360
    },
    {
      "epoch": 4.4464,
      "grad_norm": 0.10073473304510117,
      "learning_rate": 2.221e-05,
      "loss": 0.0023,
      "step": 83370
    },
    {
      "epoch": 4.446933333333333,
      "grad_norm": 0.25687283277511597,
      "learning_rate": 2.220666666666667e-05,
      "loss": 0.0016,
      "step": 83380
    },
    {
      "epoch": 4.447466666666667,
      "grad_norm": 0.13083666563034058,
      "learning_rate": 2.2203333333333335e-05,
      "loss": 0.0014,
      "step": 83390
    },
    {
      "epoch": 4.448,
      "grad_norm": 0.30591580271720886,
      "learning_rate": 2.22e-05,
      "loss": 0.0016,
      "step": 83400
    },
    {
      "epoch": 4.448533333333334,
      "grad_norm": 0.4702615737915039,
      "learning_rate": 2.2196666666666667e-05,
      "loss": 0.0018,
      "step": 83410
    },
    {
      "epoch": 4.449066666666667,
      "grad_norm": 0.6032059192657471,
      "learning_rate": 2.2193333333333337e-05,
      "loss": 0.0017,
      "step": 83420
    },
    {
      "epoch": 4.4496,
      "grad_norm": 0.24231375753879547,
      "learning_rate": 2.219e-05,
      "loss": 0.0021,
      "step": 83430
    },
    {
      "epoch": 4.4501333333333335,
      "grad_norm": 0.2659410834312439,
      "learning_rate": 2.2186666666666665e-05,
      "loss": 0.0021,
      "step": 83440
    },
    {
      "epoch": 4.450666666666667,
      "grad_norm": 0.1934117078781128,
      "learning_rate": 2.2183333333333335e-05,
      "loss": 0.0018,
      "step": 83450
    },
    {
      "epoch": 4.4512,
      "grad_norm": 0.6391382217407227,
      "learning_rate": 2.218e-05,
      "loss": 0.0017,
      "step": 83460
    },
    {
      "epoch": 4.451733333333333,
      "grad_norm": 0.09539929777383804,
      "learning_rate": 2.2176666666666667e-05,
      "loss": 0.0028,
      "step": 83470
    },
    {
      "epoch": 4.452266666666667,
      "grad_norm": 0.5536896586418152,
      "learning_rate": 2.2173333333333333e-05,
      "loss": 0.0021,
      "step": 83480
    },
    {
      "epoch": 4.4528,
      "grad_norm": 0.09720224887132645,
      "learning_rate": 2.2170000000000003e-05,
      "loss": 0.0019,
      "step": 83490
    },
    {
      "epoch": 4.453333333333333,
      "grad_norm": 0.1412639617919922,
      "learning_rate": 2.216666666666667e-05,
      "loss": 0.0028,
      "step": 83500
    },
    {
      "epoch": 4.453866666666666,
      "grad_norm": 0.3647919297218323,
      "learning_rate": 2.2163333333333332e-05,
      "loss": 0.0023,
      "step": 83510
    },
    {
      "epoch": 4.4544,
      "grad_norm": 0.18484468758106232,
      "learning_rate": 2.216e-05,
      "loss": 0.0017,
      "step": 83520
    },
    {
      "epoch": 4.454933333333333,
      "grad_norm": 0.4544515311717987,
      "learning_rate": 2.2156666666666667e-05,
      "loss": 0.0021,
      "step": 83530
    },
    {
      "epoch": 4.455466666666666,
      "grad_norm": 0.13307799398899078,
      "learning_rate": 2.2153333333333334e-05,
      "loss": 0.0026,
      "step": 83540
    },
    {
      "epoch": 4.456,
      "grad_norm": 0.09586440026760101,
      "learning_rate": 2.215e-05,
      "loss": 0.0018,
      "step": 83550
    },
    {
      "epoch": 4.456533333333334,
      "grad_norm": 0.45734262466430664,
      "learning_rate": 2.214666666666667e-05,
      "loss": 0.003,
      "step": 83560
    },
    {
      "epoch": 4.457066666666667,
      "grad_norm": 0.1988500952720642,
      "learning_rate": 2.2143333333333335e-05,
      "loss": 0.0029,
      "step": 83570
    },
    {
      "epoch": 4.4576,
      "grad_norm": 0.24411721527576447,
      "learning_rate": 2.214e-05,
      "loss": 0.002,
      "step": 83580
    },
    {
      "epoch": 4.4581333333333335,
      "grad_norm": 0.43752238154411316,
      "learning_rate": 2.2136666666666668e-05,
      "loss": 0.0023,
      "step": 83590
    },
    {
      "epoch": 4.458666666666667,
      "grad_norm": 0.16531683504581451,
      "learning_rate": 2.2133333333333334e-05,
      "loss": 0.0019,
      "step": 83600
    },
    {
      "epoch": 4.4592,
      "grad_norm": 0.0701298713684082,
      "learning_rate": 2.213e-05,
      "loss": 0.0017,
      "step": 83610
    },
    {
      "epoch": 4.459733333333333,
      "grad_norm": 0.5429127812385559,
      "learning_rate": 2.212666666666667e-05,
      "loss": 0.0023,
      "step": 83620
    },
    {
      "epoch": 4.460266666666667,
      "grad_norm": 0.7000826597213745,
      "learning_rate": 2.2123333333333336e-05,
      "loss": 0.0028,
      "step": 83630
    },
    {
      "epoch": 4.4608,
      "grad_norm": 0.24637076258659363,
      "learning_rate": 2.212e-05,
      "loss": 0.0014,
      "step": 83640
    },
    {
      "epoch": 4.461333333333333,
      "grad_norm": 0.2741200029850006,
      "learning_rate": 2.2116666666666668e-05,
      "loss": 0.0022,
      "step": 83650
    },
    {
      "epoch": 4.461866666666666,
      "grad_norm": 0.2167162001132965,
      "learning_rate": 2.2113333333333334e-05,
      "loss": 0.0022,
      "step": 83660
    },
    {
      "epoch": 4.4624,
      "grad_norm": 0.2686336040496826,
      "learning_rate": 2.211e-05,
      "loss": 0.002,
      "step": 83670
    },
    {
      "epoch": 4.462933333333333,
      "grad_norm": 0.274528443813324,
      "learning_rate": 2.2106666666666666e-05,
      "loss": 0.0031,
      "step": 83680
    },
    {
      "epoch": 4.463466666666667,
      "grad_norm": 0.3998347520828247,
      "learning_rate": 2.2103333333333336e-05,
      "loss": 0.002,
      "step": 83690
    },
    {
      "epoch": 4.464,
      "grad_norm": 0.4574102461338043,
      "learning_rate": 2.2100000000000002e-05,
      "loss": 0.0027,
      "step": 83700
    },
    {
      "epoch": 4.464533333333334,
      "grad_norm": 0.23934291303157806,
      "learning_rate": 2.2096666666666668e-05,
      "loss": 0.0017,
      "step": 83710
    },
    {
      "epoch": 4.465066666666667,
      "grad_norm": 0.29900678992271423,
      "learning_rate": 2.2093333333333334e-05,
      "loss": 0.0015,
      "step": 83720
    },
    {
      "epoch": 4.4656,
      "grad_norm": 0.5140767693519592,
      "learning_rate": 2.2090000000000004e-05,
      "loss": 0.0018,
      "step": 83730
    },
    {
      "epoch": 4.4661333333333335,
      "grad_norm": 0.12390121072530746,
      "learning_rate": 2.2086666666666666e-05,
      "loss": 0.002,
      "step": 83740
    },
    {
      "epoch": 4.466666666666667,
      "grad_norm": 0.19600728154182434,
      "learning_rate": 2.2083333333333333e-05,
      "loss": 0.0024,
      "step": 83750
    },
    {
      "epoch": 4.4672,
      "grad_norm": 0.29962635040283203,
      "learning_rate": 2.2080000000000002e-05,
      "loss": 0.0027,
      "step": 83760
    },
    {
      "epoch": 4.467733333333333,
      "grad_norm": 0.211909681558609,
      "learning_rate": 2.2076666666666668e-05,
      "loss": 0.0024,
      "step": 83770
    },
    {
      "epoch": 4.468266666666667,
      "grad_norm": 0.0991310402750969,
      "learning_rate": 2.2073333333333334e-05,
      "loss": 0.0017,
      "step": 83780
    },
    {
      "epoch": 4.4688,
      "grad_norm": 0.23247970640659332,
      "learning_rate": 2.207e-05,
      "loss": 0.0015,
      "step": 83790
    },
    {
      "epoch": 4.469333333333333,
      "grad_norm": 0.13214649260044098,
      "learning_rate": 2.206666666666667e-05,
      "loss": 0.0023,
      "step": 83800
    },
    {
      "epoch": 4.469866666666666,
      "grad_norm": 0.35449323058128357,
      "learning_rate": 2.2063333333333333e-05,
      "loss": 0.0016,
      "step": 83810
    },
    {
      "epoch": 4.4704,
      "grad_norm": 0.4332997500896454,
      "learning_rate": 2.206e-05,
      "loss": 0.0024,
      "step": 83820
    },
    {
      "epoch": 4.470933333333333,
      "grad_norm": 0.17951007187366486,
      "learning_rate": 2.205666666666667e-05,
      "loss": 0.0024,
      "step": 83830
    },
    {
      "epoch": 4.471466666666666,
      "grad_norm": 0.4346162676811218,
      "learning_rate": 2.2053333333333335e-05,
      "loss": 0.0023,
      "step": 83840
    },
    {
      "epoch": 4.4719999999999995,
      "grad_norm": 0.06219381093978882,
      "learning_rate": 2.205e-05,
      "loss": 0.0029,
      "step": 83850
    },
    {
      "epoch": 4.472533333333334,
      "grad_norm": 0.0727376714348793,
      "learning_rate": 2.2046666666666667e-05,
      "loss": 0.0012,
      "step": 83860
    },
    {
      "epoch": 4.473066666666667,
      "grad_norm": 0.09597615897655487,
      "learning_rate": 2.2043333333333336e-05,
      "loss": 0.0018,
      "step": 83870
    },
    {
      "epoch": 4.4736,
      "grad_norm": 0.04648899659514427,
      "learning_rate": 2.2040000000000002e-05,
      "loss": 0.0016,
      "step": 83880
    },
    {
      "epoch": 4.4741333333333335,
      "grad_norm": 0.3538358211517334,
      "learning_rate": 2.2036666666666665e-05,
      "loss": 0.0024,
      "step": 83890
    },
    {
      "epoch": 4.474666666666667,
      "grad_norm": 0.29371243715286255,
      "learning_rate": 2.2033333333333335e-05,
      "loss": 0.0024,
      "step": 83900
    },
    {
      "epoch": 4.4752,
      "grad_norm": 0.257302850484848,
      "learning_rate": 2.203e-05,
      "loss": 0.0017,
      "step": 83910
    },
    {
      "epoch": 4.475733333333333,
      "grad_norm": 0.4089545011520386,
      "learning_rate": 2.2026666666666667e-05,
      "loss": 0.002,
      "step": 83920
    },
    {
      "epoch": 4.476266666666667,
      "grad_norm": 0.13367348909378052,
      "learning_rate": 2.2023333333333333e-05,
      "loss": 0.0013,
      "step": 83930
    },
    {
      "epoch": 4.4768,
      "grad_norm": 0.4216022789478302,
      "learning_rate": 2.2020000000000003e-05,
      "loss": 0.0022,
      "step": 83940
    },
    {
      "epoch": 4.477333333333333,
      "grad_norm": 0.11345955729484558,
      "learning_rate": 2.201666666666667e-05,
      "loss": 0.0014,
      "step": 83950
    },
    {
      "epoch": 4.477866666666666,
      "grad_norm": 0.33556079864501953,
      "learning_rate": 2.201333333333333e-05,
      "loss": 0.0028,
      "step": 83960
    },
    {
      "epoch": 4.4784,
      "grad_norm": 0.3940085470676422,
      "learning_rate": 2.201e-05,
      "loss": 0.0013,
      "step": 83970
    },
    {
      "epoch": 4.478933333333333,
      "grad_norm": 0.1328047811985016,
      "learning_rate": 2.2006666666666667e-05,
      "loss": 0.0024,
      "step": 83980
    },
    {
      "epoch": 4.479466666666666,
      "grad_norm": 0.15244624018669128,
      "learning_rate": 2.2003333333333333e-05,
      "loss": 0.0022,
      "step": 83990
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.31498491764068604,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.0025,
      "step": 84000
    },
    {
      "epoch": 4.480533333333334,
      "grad_norm": 0.06954293698072433,
      "learning_rate": 2.199666666666667e-05,
      "loss": 0.0019,
      "step": 84010
    },
    {
      "epoch": 4.481066666666667,
      "grad_norm": 0.3106367588043213,
      "learning_rate": 2.1993333333333335e-05,
      "loss": 0.0014,
      "step": 84020
    },
    {
      "epoch": 4.4816,
      "grad_norm": 0.3297761380672455,
      "learning_rate": 2.199e-05,
      "loss": 0.002,
      "step": 84030
    },
    {
      "epoch": 4.4821333333333335,
      "grad_norm": 0.033950332552194595,
      "learning_rate": 2.1986666666666667e-05,
      "loss": 0.0017,
      "step": 84040
    },
    {
      "epoch": 4.482666666666667,
      "grad_norm": 0.27282583713531494,
      "learning_rate": 2.1983333333333333e-05,
      "loss": 0.0019,
      "step": 84050
    },
    {
      "epoch": 4.4832,
      "grad_norm": 0.3057008683681488,
      "learning_rate": 2.198e-05,
      "loss": 0.0015,
      "step": 84060
    },
    {
      "epoch": 4.483733333333333,
      "grad_norm": 0.33736321330070496,
      "learning_rate": 2.197666666666667e-05,
      "loss": 0.0012,
      "step": 84070
    },
    {
      "epoch": 4.484266666666667,
      "grad_norm": 0.25497111678123474,
      "learning_rate": 2.1973333333333335e-05,
      "loss": 0.0026,
      "step": 84080
    },
    {
      "epoch": 4.4848,
      "grad_norm": 0.16515041887760162,
      "learning_rate": 2.197e-05,
      "loss": 0.002,
      "step": 84090
    },
    {
      "epoch": 4.485333333333333,
      "grad_norm": 0.200748473405838,
      "learning_rate": 2.1966666666666668e-05,
      "loss": 0.002,
      "step": 84100
    },
    {
      "epoch": 4.4858666666666664,
      "grad_norm": 0.07142274081707001,
      "learning_rate": 2.1963333333333337e-05,
      "loss": 0.0019,
      "step": 84110
    },
    {
      "epoch": 4.4864,
      "grad_norm": 0.27587074041366577,
      "learning_rate": 2.196e-05,
      "loss": 0.0025,
      "step": 84120
    },
    {
      "epoch": 4.486933333333333,
      "grad_norm": 0.1124495267868042,
      "learning_rate": 2.1956666666666666e-05,
      "loss": 0.0017,
      "step": 84130
    },
    {
      "epoch": 4.487466666666666,
      "grad_norm": 0.8218943476676941,
      "learning_rate": 2.1953333333333335e-05,
      "loss": 0.0026,
      "step": 84140
    },
    {
      "epoch": 4.4879999999999995,
      "grad_norm": 0.4867941439151764,
      "learning_rate": 2.195e-05,
      "loss": 0.0019,
      "step": 84150
    },
    {
      "epoch": 4.488533333333334,
      "grad_norm": 0.34598487615585327,
      "learning_rate": 2.1946666666666668e-05,
      "loss": 0.0015,
      "step": 84160
    },
    {
      "epoch": 4.489066666666667,
      "grad_norm": 0.34237974882125854,
      "learning_rate": 2.1943333333333334e-05,
      "loss": 0.0021,
      "step": 84170
    },
    {
      "epoch": 4.4896,
      "grad_norm": 0.12571585178375244,
      "learning_rate": 2.1940000000000003e-05,
      "loss": 0.0012,
      "step": 84180
    },
    {
      "epoch": 4.4901333333333335,
      "grad_norm": 0.047052763402462006,
      "learning_rate": 2.1936666666666666e-05,
      "loss": 0.0033,
      "step": 84190
    },
    {
      "epoch": 4.490666666666667,
      "grad_norm": 0.49654626846313477,
      "learning_rate": 2.1933333333333332e-05,
      "loss": 0.0019,
      "step": 84200
    },
    {
      "epoch": 4.4912,
      "grad_norm": 0.13435958325862885,
      "learning_rate": 2.1930000000000002e-05,
      "loss": 0.0015,
      "step": 84210
    },
    {
      "epoch": 4.491733333333333,
      "grad_norm": 0.17016401886940002,
      "learning_rate": 2.1926666666666668e-05,
      "loss": 0.0014,
      "step": 84220
    },
    {
      "epoch": 4.492266666666667,
      "grad_norm": 0.24520757794380188,
      "learning_rate": 2.1923333333333334e-05,
      "loss": 0.0022,
      "step": 84230
    },
    {
      "epoch": 4.4928,
      "grad_norm": 0.26883235573768616,
      "learning_rate": 2.192e-05,
      "loss": 0.0016,
      "step": 84240
    },
    {
      "epoch": 4.493333333333333,
      "grad_norm": 0.46027326583862305,
      "learning_rate": 2.191666666666667e-05,
      "loss": 0.0017,
      "step": 84250
    },
    {
      "epoch": 4.4938666666666665,
      "grad_norm": 0.4192467927932739,
      "learning_rate": 2.1913333333333336e-05,
      "loss": 0.0016,
      "step": 84260
    },
    {
      "epoch": 4.4944,
      "grad_norm": 0.18436460196971893,
      "learning_rate": 2.191e-05,
      "loss": 0.0014,
      "step": 84270
    },
    {
      "epoch": 4.494933333333333,
      "grad_norm": 0.045089397579431534,
      "learning_rate": 2.1906666666666668e-05,
      "loss": 0.0015,
      "step": 84280
    },
    {
      "epoch": 4.495466666666666,
      "grad_norm": 0.06383637338876724,
      "learning_rate": 2.1903333333333334e-05,
      "loss": 0.0017,
      "step": 84290
    },
    {
      "epoch": 4.496,
      "grad_norm": 0.32514244318008423,
      "learning_rate": 2.19e-05,
      "loss": 0.0021,
      "step": 84300
    },
    {
      "epoch": 4.496533333333334,
      "grad_norm": 0.039230842143297195,
      "learning_rate": 2.1896666666666667e-05,
      "loss": 0.0016,
      "step": 84310
    },
    {
      "epoch": 4.497066666666667,
      "grad_norm": 0.37475690245628357,
      "learning_rate": 2.1893333333333336e-05,
      "loss": 0.0013,
      "step": 84320
    },
    {
      "epoch": 4.4976,
      "grad_norm": 0.19707386195659637,
      "learning_rate": 2.1890000000000002e-05,
      "loss": 0.0018,
      "step": 84330
    },
    {
      "epoch": 4.4981333333333335,
      "grad_norm": 0.05974031984806061,
      "learning_rate": 2.1886666666666665e-05,
      "loss": 0.0019,
      "step": 84340
    },
    {
      "epoch": 4.498666666666667,
      "grad_norm": 0.04587341845035553,
      "learning_rate": 2.1883333333333334e-05,
      "loss": 0.0018,
      "step": 84350
    },
    {
      "epoch": 4.4992,
      "grad_norm": 0.09340687096118927,
      "learning_rate": 2.188e-05,
      "loss": 0.0012,
      "step": 84360
    },
    {
      "epoch": 4.499733333333333,
      "grad_norm": 0.3020581901073456,
      "learning_rate": 2.1876666666666667e-05,
      "loss": 0.0016,
      "step": 84370
    },
    {
      "epoch": 4.500266666666667,
      "grad_norm": 0.2836166322231293,
      "learning_rate": 2.1873333333333336e-05,
      "loss": 0.0014,
      "step": 84380
    },
    {
      "epoch": 4.5008,
      "grad_norm": 0.1220061331987381,
      "learning_rate": 2.1870000000000002e-05,
      "loss": 0.0031,
      "step": 84390
    },
    {
      "epoch": 4.501333333333333,
      "grad_norm": 0.09206664562225342,
      "learning_rate": 2.186666666666667e-05,
      "loss": 0.0023,
      "step": 84400
    },
    {
      "epoch": 4.5018666666666665,
      "grad_norm": 0.48204970359802246,
      "learning_rate": 2.1863333333333335e-05,
      "loss": 0.0016,
      "step": 84410
    },
    {
      "epoch": 4.5024,
      "grad_norm": 0.5245974063873291,
      "learning_rate": 2.186e-05,
      "loss": 0.0025,
      "step": 84420
    },
    {
      "epoch": 4.502933333333333,
      "grad_norm": 0.37593087553977966,
      "learning_rate": 2.1856666666666667e-05,
      "loss": 0.0019,
      "step": 84430
    },
    {
      "epoch": 4.503466666666666,
      "grad_norm": 0.39444106817245483,
      "learning_rate": 2.1853333333333333e-05,
      "loss": 0.0021,
      "step": 84440
    },
    {
      "epoch": 4.504,
      "grad_norm": 0.3840206265449524,
      "learning_rate": 2.1850000000000003e-05,
      "loss": 0.0018,
      "step": 84450
    },
    {
      "epoch": 4.504533333333333,
      "grad_norm": 0.30249398946762085,
      "learning_rate": 2.184666666666667e-05,
      "loss": 0.002,
      "step": 84460
    },
    {
      "epoch": 4.505066666666667,
      "grad_norm": 0.07969456166028976,
      "learning_rate": 2.1843333333333335e-05,
      "loss": 0.0026,
      "step": 84470
    },
    {
      "epoch": 4.5056,
      "grad_norm": 0.09250756353139877,
      "learning_rate": 2.184e-05,
      "loss": 0.0014,
      "step": 84480
    },
    {
      "epoch": 4.5061333333333335,
      "grad_norm": 0.2244085669517517,
      "learning_rate": 2.1836666666666667e-05,
      "loss": 0.0026,
      "step": 84490
    },
    {
      "epoch": 4.506666666666667,
      "grad_norm": 0.08060463517904282,
      "learning_rate": 2.1833333333333333e-05,
      "loss": 0.0017,
      "step": 84500
    },
    {
      "epoch": 4.5072,
      "grad_norm": 0.5873799920082092,
      "learning_rate": 2.183e-05,
      "loss": 0.0014,
      "step": 84510
    },
    {
      "epoch": 4.507733333333333,
      "grad_norm": 0.15546081960201263,
      "learning_rate": 2.182666666666667e-05,
      "loss": 0.0017,
      "step": 84520
    },
    {
      "epoch": 4.508266666666667,
      "grad_norm": 0.24253156781196594,
      "learning_rate": 2.1823333333333335e-05,
      "loss": 0.0017,
      "step": 84530
    },
    {
      "epoch": 4.5088,
      "grad_norm": 0.04708606004714966,
      "learning_rate": 2.182e-05,
      "loss": 0.0023,
      "step": 84540
    },
    {
      "epoch": 4.509333333333333,
      "grad_norm": 0.1227881908416748,
      "learning_rate": 2.1816666666666667e-05,
      "loss": 0.0021,
      "step": 84550
    },
    {
      "epoch": 4.5098666666666665,
      "grad_norm": 0.18818598985671997,
      "learning_rate": 2.1813333333333337e-05,
      "loss": 0.0022,
      "step": 84560
    },
    {
      "epoch": 4.5104,
      "grad_norm": 0.1922825574874878,
      "learning_rate": 2.181e-05,
      "loss": 0.0022,
      "step": 84570
    },
    {
      "epoch": 4.510933333333333,
      "grad_norm": 0.35788291692733765,
      "learning_rate": 2.1806666666666666e-05,
      "loss": 0.002,
      "step": 84580
    },
    {
      "epoch": 4.511466666666666,
      "grad_norm": 0.0779675766825676,
      "learning_rate": 2.1803333333333335e-05,
      "loss": 0.0013,
      "step": 84590
    },
    {
      "epoch": 4.5120000000000005,
      "grad_norm": 0.09375271201133728,
      "learning_rate": 2.18e-05,
      "loss": 0.0014,
      "step": 84600
    },
    {
      "epoch": 4.512533333333334,
      "grad_norm": 0.43850943446159363,
      "learning_rate": 2.1796666666666667e-05,
      "loss": 0.0027,
      "step": 84610
    },
    {
      "epoch": 4.513066666666667,
      "grad_norm": 0.20994044840335846,
      "learning_rate": 2.1793333333333334e-05,
      "loss": 0.0019,
      "step": 84620
    },
    {
      "epoch": 4.5136,
      "grad_norm": 0.33290210366249084,
      "learning_rate": 2.1790000000000003e-05,
      "loss": 0.002,
      "step": 84630
    },
    {
      "epoch": 4.5141333333333336,
      "grad_norm": 0.1871979683637619,
      "learning_rate": 2.1786666666666666e-05,
      "loss": 0.0013,
      "step": 84640
    },
    {
      "epoch": 4.514666666666667,
      "grad_norm": 0.2171853482723236,
      "learning_rate": 2.1783333333333332e-05,
      "loss": 0.002,
      "step": 84650
    },
    {
      "epoch": 4.5152,
      "grad_norm": 0.3737449049949646,
      "learning_rate": 2.178e-05,
      "loss": 0.0017,
      "step": 84660
    },
    {
      "epoch": 4.515733333333333,
      "grad_norm": 0.041128143668174744,
      "learning_rate": 2.1776666666666668e-05,
      "loss": 0.0016,
      "step": 84670
    },
    {
      "epoch": 4.516266666666667,
      "grad_norm": 0.1885540783405304,
      "learning_rate": 2.1773333333333334e-05,
      "loss": 0.0012,
      "step": 84680
    },
    {
      "epoch": 4.5168,
      "grad_norm": 0.050681959837675095,
      "learning_rate": 2.177e-05,
      "loss": 0.0016,
      "step": 84690
    },
    {
      "epoch": 4.517333333333333,
      "grad_norm": 0.3845500946044922,
      "learning_rate": 2.176666666666667e-05,
      "loss": 0.0028,
      "step": 84700
    },
    {
      "epoch": 4.5178666666666665,
      "grad_norm": 0.2578534483909607,
      "learning_rate": 2.1763333333333336e-05,
      "loss": 0.002,
      "step": 84710
    },
    {
      "epoch": 4.5184,
      "grad_norm": 0.4298627972602844,
      "learning_rate": 2.176e-05,
      "loss": 0.0015,
      "step": 84720
    },
    {
      "epoch": 4.518933333333333,
      "grad_norm": 0.07516811788082123,
      "learning_rate": 2.1756666666666668e-05,
      "loss": 0.002,
      "step": 84730
    },
    {
      "epoch": 4.519466666666666,
      "grad_norm": 0.12709416449069977,
      "learning_rate": 2.1753333333333334e-05,
      "loss": 0.0019,
      "step": 84740
    },
    {
      "epoch": 4.52,
      "grad_norm": 0.37462544441223145,
      "learning_rate": 2.175e-05,
      "loss": 0.0022,
      "step": 84750
    },
    {
      "epoch": 4.520533333333333,
      "grad_norm": 0.15161634981632233,
      "learning_rate": 2.174666666666667e-05,
      "loss": 0.0021,
      "step": 84760
    },
    {
      "epoch": 4.521066666666667,
      "grad_norm": 0.3295733630657196,
      "learning_rate": 2.1743333333333336e-05,
      "loss": 0.0018,
      "step": 84770
    },
    {
      "epoch": 4.5216,
      "grad_norm": 0.27028167247772217,
      "learning_rate": 2.1740000000000002e-05,
      "loss": 0.0014,
      "step": 84780
    },
    {
      "epoch": 4.522133333333334,
      "grad_norm": 0.39819270372390747,
      "learning_rate": 2.1736666666666668e-05,
      "loss": 0.0016,
      "step": 84790
    },
    {
      "epoch": 4.522666666666667,
      "grad_norm": 0.21673518419265747,
      "learning_rate": 2.1733333333333334e-05,
      "loss": 0.002,
      "step": 84800
    },
    {
      "epoch": 4.5232,
      "grad_norm": 0.21313826739788055,
      "learning_rate": 2.173e-05,
      "loss": 0.0024,
      "step": 84810
    },
    {
      "epoch": 4.523733333333333,
      "grad_norm": 0.1707819402217865,
      "learning_rate": 2.1726666666666666e-05,
      "loss": 0.0015,
      "step": 84820
    },
    {
      "epoch": 4.524266666666667,
      "grad_norm": 0.3734923005104065,
      "learning_rate": 2.1723333333333336e-05,
      "loss": 0.0031,
      "step": 84830
    },
    {
      "epoch": 4.5248,
      "grad_norm": 0.43789777159690857,
      "learning_rate": 2.1720000000000002e-05,
      "loss": 0.0018,
      "step": 84840
    },
    {
      "epoch": 4.525333333333333,
      "grad_norm": 0.047647103667259216,
      "learning_rate": 2.1716666666666668e-05,
      "loss": 0.0015,
      "step": 84850
    },
    {
      "epoch": 4.5258666666666665,
      "grad_norm": 0.280507355928421,
      "learning_rate": 2.1713333333333334e-05,
      "loss": 0.0021,
      "step": 84860
    },
    {
      "epoch": 4.5264,
      "grad_norm": 0.44701218605041504,
      "learning_rate": 2.171e-05,
      "loss": 0.0025,
      "step": 84870
    },
    {
      "epoch": 4.526933333333333,
      "grad_norm": 0.21719616651535034,
      "learning_rate": 2.1706666666666667e-05,
      "loss": 0.0018,
      "step": 84880
    },
    {
      "epoch": 4.527466666666666,
      "grad_norm": 0.36847904324531555,
      "learning_rate": 2.1703333333333333e-05,
      "loss": 0.0014,
      "step": 84890
    },
    {
      "epoch": 4.5280000000000005,
      "grad_norm": 0.15829378366470337,
      "learning_rate": 2.1700000000000002e-05,
      "loss": 0.0019,
      "step": 84900
    },
    {
      "epoch": 4.528533333333334,
      "grad_norm": 0.19979093968868256,
      "learning_rate": 2.169666666666667e-05,
      "loss": 0.0021,
      "step": 84910
    },
    {
      "epoch": 4.529066666666667,
      "grad_norm": 0.3039591312408447,
      "learning_rate": 2.1693333333333335e-05,
      "loss": 0.0018,
      "step": 84920
    },
    {
      "epoch": 4.5296,
      "grad_norm": 0.3368578553199768,
      "learning_rate": 2.169e-05,
      "loss": 0.0018,
      "step": 84930
    },
    {
      "epoch": 4.530133333333334,
      "grad_norm": 0.40972524881362915,
      "learning_rate": 2.168666666666667e-05,
      "loss": 0.0021,
      "step": 84940
    },
    {
      "epoch": 4.530666666666667,
      "grad_norm": 0.6313273906707764,
      "learning_rate": 2.1683333333333333e-05,
      "loss": 0.0021,
      "step": 84950
    },
    {
      "epoch": 4.5312,
      "grad_norm": 0.2701058089733124,
      "learning_rate": 2.168e-05,
      "loss": 0.0021,
      "step": 84960
    },
    {
      "epoch": 4.531733333333333,
      "grad_norm": 0.28155654668807983,
      "learning_rate": 2.167666666666667e-05,
      "loss": 0.0021,
      "step": 84970
    },
    {
      "epoch": 4.532266666666667,
      "grad_norm": 0.3848329186439514,
      "learning_rate": 2.1673333333333335e-05,
      "loss": 0.0016,
      "step": 84980
    },
    {
      "epoch": 4.5328,
      "grad_norm": 0.39343056082725525,
      "learning_rate": 2.167e-05,
      "loss": 0.0017,
      "step": 84990
    },
    {
      "epoch": 4.533333333333333,
      "grad_norm": 0.2692908048629761,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 0.0025,
      "step": 85000
    },
    {
      "epoch": 4.5338666666666665,
      "grad_norm": 0.07628366351127625,
      "learning_rate": 2.1663333333333337e-05,
      "loss": 0.0023,
      "step": 85010
    },
    {
      "epoch": 4.5344,
      "grad_norm": 0.23840093612670898,
      "learning_rate": 2.166e-05,
      "loss": 0.0017,
      "step": 85020
    },
    {
      "epoch": 4.534933333333333,
      "grad_norm": 0.3029427230358124,
      "learning_rate": 2.1656666666666665e-05,
      "loss": 0.0017,
      "step": 85030
    },
    {
      "epoch": 4.535466666666666,
      "grad_norm": 0.5473023056983948,
      "learning_rate": 2.1653333333333335e-05,
      "loss": 0.0023,
      "step": 85040
    },
    {
      "epoch": 4.536,
      "grad_norm": 0.66799396276474,
      "learning_rate": 2.165e-05,
      "loss": 0.0014,
      "step": 85050
    },
    {
      "epoch": 4.536533333333333,
      "grad_norm": 0.6014246344566345,
      "learning_rate": 2.1646666666666667e-05,
      "loss": 0.0015,
      "step": 85060
    },
    {
      "epoch": 4.537066666666667,
      "grad_norm": 0.3663026988506317,
      "learning_rate": 2.1643333333333333e-05,
      "loss": 0.0016,
      "step": 85070
    },
    {
      "epoch": 4.5376,
      "grad_norm": 0.22193500399589539,
      "learning_rate": 2.1640000000000003e-05,
      "loss": 0.0017,
      "step": 85080
    },
    {
      "epoch": 4.538133333333334,
      "grad_norm": 0.3236522376537323,
      "learning_rate": 2.163666666666667e-05,
      "loss": 0.0017,
      "step": 85090
    },
    {
      "epoch": 4.538666666666667,
      "grad_norm": 0.19076426327228546,
      "learning_rate": 2.1633333333333332e-05,
      "loss": 0.0021,
      "step": 85100
    },
    {
      "epoch": 4.5392,
      "grad_norm": 0.12873013317584991,
      "learning_rate": 2.163e-05,
      "loss": 0.0014,
      "step": 85110
    },
    {
      "epoch": 4.539733333333333,
      "grad_norm": 0.28963544964790344,
      "learning_rate": 2.1626666666666667e-05,
      "loss": 0.0015,
      "step": 85120
    },
    {
      "epoch": 4.540266666666667,
      "grad_norm": 0.27890485525131226,
      "learning_rate": 2.1623333333333334e-05,
      "loss": 0.0019,
      "step": 85130
    },
    {
      "epoch": 4.5408,
      "grad_norm": 0.3189389109611511,
      "learning_rate": 2.162e-05,
      "loss": 0.0016,
      "step": 85140
    },
    {
      "epoch": 4.541333333333333,
      "grad_norm": 0.18705269694328308,
      "learning_rate": 2.161666666666667e-05,
      "loss": 0.0019,
      "step": 85150
    },
    {
      "epoch": 4.5418666666666665,
      "grad_norm": 0.577625036239624,
      "learning_rate": 2.1613333333333335e-05,
      "loss": 0.0022,
      "step": 85160
    },
    {
      "epoch": 4.5424,
      "grad_norm": 0.5644416809082031,
      "learning_rate": 2.1609999999999998e-05,
      "loss": 0.002,
      "step": 85170
    },
    {
      "epoch": 4.542933333333333,
      "grad_norm": 0.21425072848796844,
      "learning_rate": 2.1606666666666668e-05,
      "loss": 0.0031,
      "step": 85180
    },
    {
      "epoch": 4.543466666666666,
      "grad_norm": 0.11563999205827713,
      "learning_rate": 2.1603333333333334e-05,
      "loss": 0.0022,
      "step": 85190
    },
    {
      "epoch": 4.5440000000000005,
      "grad_norm": 0.38760411739349365,
      "learning_rate": 2.16e-05,
      "loss": 0.0024,
      "step": 85200
    },
    {
      "epoch": 4.544533333333334,
      "grad_norm": 0.1532355099916458,
      "learning_rate": 2.159666666666667e-05,
      "loss": 0.0013,
      "step": 85210
    },
    {
      "epoch": 4.545066666666667,
      "grad_norm": 0.04450678825378418,
      "learning_rate": 2.1593333333333336e-05,
      "loss": 0.0015,
      "step": 85220
    },
    {
      "epoch": 4.5456,
      "grad_norm": 0.45734062790870667,
      "learning_rate": 2.159e-05,
      "loss": 0.0019,
      "step": 85230
    },
    {
      "epoch": 4.546133333333334,
      "grad_norm": 0.27402952313423157,
      "learning_rate": 2.1586666666666668e-05,
      "loss": 0.0018,
      "step": 85240
    },
    {
      "epoch": 4.546666666666667,
      "grad_norm": 0.2515870928764343,
      "learning_rate": 2.1583333333333334e-05,
      "loss": 0.0018,
      "step": 85250
    },
    {
      "epoch": 4.5472,
      "grad_norm": 0.13978435099124908,
      "learning_rate": 2.158e-05,
      "loss": 0.002,
      "step": 85260
    },
    {
      "epoch": 4.547733333333333,
      "grad_norm": 0.35996371507644653,
      "learning_rate": 2.1576666666666666e-05,
      "loss": 0.002,
      "step": 85270
    },
    {
      "epoch": 4.548266666666667,
      "grad_norm": 0.23597252368927002,
      "learning_rate": 2.1573333333333336e-05,
      "loss": 0.0021,
      "step": 85280
    },
    {
      "epoch": 4.5488,
      "grad_norm": 0.06820718199014664,
      "learning_rate": 2.1570000000000002e-05,
      "loss": 0.0023,
      "step": 85290
    },
    {
      "epoch": 4.549333333333333,
      "grad_norm": 0.1880427896976471,
      "learning_rate": 2.1566666666666668e-05,
      "loss": 0.0021,
      "step": 85300
    },
    {
      "epoch": 4.5498666666666665,
      "grad_norm": 0.20514832437038422,
      "learning_rate": 2.1563333333333334e-05,
      "loss": 0.0017,
      "step": 85310
    },
    {
      "epoch": 4.5504,
      "grad_norm": 0.12776058912277222,
      "learning_rate": 2.1560000000000004e-05,
      "loss": 0.002,
      "step": 85320
    },
    {
      "epoch": 4.550933333333333,
      "grad_norm": 0.1382117122411728,
      "learning_rate": 2.1556666666666666e-05,
      "loss": 0.0016,
      "step": 85330
    },
    {
      "epoch": 4.551466666666666,
      "grad_norm": 0.09643746167421341,
      "learning_rate": 2.1553333333333333e-05,
      "loss": 0.0025,
      "step": 85340
    },
    {
      "epoch": 4.552,
      "grad_norm": 0.09768740832805634,
      "learning_rate": 2.1550000000000002e-05,
      "loss": 0.0023,
      "step": 85350
    },
    {
      "epoch": 4.552533333333333,
      "grad_norm": 0.21645773947238922,
      "learning_rate": 2.1546666666666668e-05,
      "loss": 0.0017,
      "step": 85360
    },
    {
      "epoch": 4.553066666666667,
      "grad_norm": 0.18420374393463135,
      "learning_rate": 2.1543333333333334e-05,
      "loss": 0.0028,
      "step": 85370
    },
    {
      "epoch": 4.5536,
      "grad_norm": 0.056791312992572784,
      "learning_rate": 2.154e-05,
      "loss": 0.0022,
      "step": 85380
    },
    {
      "epoch": 4.554133333333334,
      "grad_norm": 0.24493296444416046,
      "learning_rate": 2.153666666666667e-05,
      "loss": 0.0018,
      "step": 85390
    },
    {
      "epoch": 4.554666666666667,
      "grad_norm": 0.26778852939605713,
      "learning_rate": 2.1533333333333333e-05,
      "loss": 0.0019,
      "step": 85400
    },
    {
      "epoch": 4.5552,
      "grad_norm": 0.11203251034021378,
      "learning_rate": 2.153e-05,
      "loss": 0.0017,
      "step": 85410
    },
    {
      "epoch": 4.555733333333333,
      "grad_norm": 0.18209151923656464,
      "learning_rate": 2.152666666666667e-05,
      "loss": 0.0019,
      "step": 85420
    },
    {
      "epoch": 4.556266666666667,
      "grad_norm": 0.14306031167507172,
      "learning_rate": 2.1523333333333335e-05,
      "loss": 0.0019,
      "step": 85430
    },
    {
      "epoch": 4.5568,
      "grad_norm": 0.30132120847702026,
      "learning_rate": 2.152e-05,
      "loss": 0.0019,
      "step": 85440
    },
    {
      "epoch": 4.557333333333333,
      "grad_norm": 0.24701154232025146,
      "learning_rate": 2.1516666666666667e-05,
      "loss": 0.003,
      "step": 85450
    },
    {
      "epoch": 4.5578666666666665,
      "grad_norm": 0.19773578643798828,
      "learning_rate": 2.1513333333333336e-05,
      "loss": 0.0017,
      "step": 85460
    },
    {
      "epoch": 4.5584,
      "grad_norm": 0.5170403718948364,
      "learning_rate": 2.1510000000000002e-05,
      "loss": 0.0016,
      "step": 85470
    },
    {
      "epoch": 4.558933333333333,
      "grad_norm": 0.18569234013557434,
      "learning_rate": 2.1506666666666665e-05,
      "loss": 0.0019,
      "step": 85480
    },
    {
      "epoch": 4.559466666666666,
      "grad_norm": 0.20977182686328888,
      "learning_rate": 2.1503333333333335e-05,
      "loss": 0.0026,
      "step": 85490
    },
    {
      "epoch": 4.5600000000000005,
      "grad_norm": 0.20211541652679443,
      "learning_rate": 2.15e-05,
      "loss": 0.0022,
      "step": 85500
    },
    {
      "epoch": 4.560533333333334,
      "grad_norm": 0.1272129863500595,
      "learning_rate": 2.1496666666666667e-05,
      "loss": 0.0016,
      "step": 85510
    },
    {
      "epoch": 4.561066666666667,
      "grad_norm": 0.3243272304534912,
      "learning_rate": 2.1493333333333333e-05,
      "loss": 0.0022,
      "step": 85520
    },
    {
      "epoch": 4.5616,
      "grad_norm": 0.09955349564552307,
      "learning_rate": 2.1490000000000003e-05,
      "loss": 0.0016,
      "step": 85530
    },
    {
      "epoch": 4.562133333333334,
      "grad_norm": 0.28729650378227234,
      "learning_rate": 2.148666666666667e-05,
      "loss": 0.0018,
      "step": 85540
    },
    {
      "epoch": 4.562666666666667,
      "grad_norm": 0.2713785469532013,
      "learning_rate": 2.148333333333333e-05,
      "loss": 0.0023,
      "step": 85550
    },
    {
      "epoch": 4.5632,
      "grad_norm": 0.06537145376205444,
      "learning_rate": 2.148e-05,
      "loss": 0.0025,
      "step": 85560
    },
    {
      "epoch": 4.563733333333333,
      "grad_norm": 0.2804948389530182,
      "learning_rate": 2.1476666666666667e-05,
      "loss": 0.0015,
      "step": 85570
    },
    {
      "epoch": 4.564266666666667,
      "grad_norm": 0.041820455342531204,
      "learning_rate": 2.1473333333333333e-05,
      "loss": 0.0021,
      "step": 85580
    },
    {
      "epoch": 4.5648,
      "grad_norm": 0.6173093914985657,
      "learning_rate": 2.1470000000000003e-05,
      "loss": 0.0019,
      "step": 85590
    },
    {
      "epoch": 4.565333333333333,
      "grad_norm": 0.4196532964706421,
      "learning_rate": 2.146666666666667e-05,
      "loss": 0.0029,
      "step": 85600
    },
    {
      "epoch": 4.5658666666666665,
      "grad_norm": 0.357541561126709,
      "learning_rate": 2.1463333333333335e-05,
      "loss": 0.0022,
      "step": 85610
    },
    {
      "epoch": 4.5664,
      "grad_norm": 0.366386353969574,
      "learning_rate": 2.146e-05,
      "loss": 0.0021,
      "step": 85620
    },
    {
      "epoch": 4.566933333333333,
      "grad_norm": 0.21413275599479675,
      "learning_rate": 2.1456666666666667e-05,
      "loss": 0.002,
      "step": 85630
    },
    {
      "epoch": 4.567466666666666,
      "grad_norm": 0.36212873458862305,
      "learning_rate": 2.1453333333333333e-05,
      "loss": 0.0021,
      "step": 85640
    },
    {
      "epoch": 4.568,
      "grad_norm": 0.9489712715148926,
      "learning_rate": 2.145e-05,
      "loss": 0.0021,
      "step": 85650
    },
    {
      "epoch": 4.568533333333333,
      "grad_norm": 0.7065200805664062,
      "learning_rate": 2.144666666666667e-05,
      "loss": 0.0018,
      "step": 85660
    },
    {
      "epoch": 4.569066666666667,
      "grad_norm": 0.24834682047367096,
      "learning_rate": 2.1443333333333335e-05,
      "loss": 0.0021,
      "step": 85670
    },
    {
      "epoch": 4.5696,
      "grad_norm": 0.4080584943294525,
      "learning_rate": 2.144e-05,
      "loss": 0.0014,
      "step": 85680
    },
    {
      "epoch": 4.570133333333334,
      "grad_norm": 0.30060574412345886,
      "learning_rate": 2.1436666666666668e-05,
      "loss": 0.0023,
      "step": 85690
    },
    {
      "epoch": 4.570666666666667,
      "grad_norm": 0.11042288690805435,
      "learning_rate": 2.1433333333333334e-05,
      "loss": 0.002,
      "step": 85700
    },
    {
      "epoch": 4.5712,
      "grad_norm": 0.2992032468318939,
      "learning_rate": 2.143e-05,
      "loss": 0.0019,
      "step": 85710
    },
    {
      "epoch": 4.571733333333333,
      "grad_norm": 0.281471848487854,
      "learning_rate": 2.1426666666666666e-05,
      "loss": 0.0035,
      "step": 85720
    },
    {
      "epoch": 4.572266666666667,
      "grad_norm": 0.07255449146032333,
      "learning_rate": 2.1423333333333335e-05,
      "loss": 0.0014,
      "step": 85730
    },
    {
      "epoch": 4.5728,
      "grad_norm": 0.15230722725391388,
      "learning_rate": 2.142e-05,
      "loss": 0.002,
      "step": 85740
    },
    {
      "epoch": 4.573333333333333,
      "grad_norm": 0.08962967246770859,
      "learning_rate": 2.1416666666666668e-05,
      "loss": 0.0018,
      "step": 85750
    },
    {
      "epoch": 4.5738666666666665,
      "grad_norm": 0.15126745402812958,
      "learning_rate": 2.1413333333333334e-05,
      "loss": 0.002,
      "step": 85760
    },
    {
      "epoch": 4.5744,
      "grad_norm": 0.3177260756492615,
      "learning_rate": 2.1410000000000003e-05,
      "loss": 0.0028,
      "step": 85770
    },
    {
      "epoch": 4.574933333333333,
      "grad_norm": 0.3149137794971466,
      "learning_rate": 2.1406666666666666e-05,
      "loss": 0.0017,
      "step": 85780
    },
    {
      "epoch": 4.575466666666666,
      "grad_norm": 0.37876883149147034,
      "learning_rate": 2.1403333333333332e-05,
      "loss": 0.002,
      "step": 85790
    },
    {
      "epoch": 4.576,
      "grad_norm": 0.1451309621334076,
      "learning_rate": 2.1400000000000002e-05,
      "loss": 0.0015,
      "step": 85800
    },
    {
      "epoch": 4.576533333333334,
      "grad_norm": 0.24860630929470062,
      "learning_rate": 2.1396666666666668e-05,
      "loss": 0.0016,
      "step": 85810
    },
    {
      "epoch": 4.577066666666667,
      "grad_norm": 0.28630998730659485,
      "learning_rate": 2.1393333333333334e-05,
      "loss": 0.0021,
      "step": 85820
    },
    {
      "epoch": 4.5776,
      "grad_norm": 0.13291916251182556,
      "learning_rate": 2.139e-05,
      "loss": 0.0021,
      "step": 85830
    },
    {
      "epoch": 4.578133333333334,
      "grad_norm": 0.43978333473205566,
      "learning_rate": 2.138666666666667e-05,
      "loss": 0.0021,
      "step": 85840
    },
    {
      "epoch": 4.578666666666667,
      "grad_norm": 0.317073255777359,
      "learning_rate": 2.1383333333333332e-05,
      "loss": 0.0026,
      "step": 85850
    },
    {
      "epoch": 4.5792,
      "grad_norm": 0.05053265392780304,
      "learning_rate": 2.138e-05,
      "loss": 0.0016,
      "step": 85860
    },
    {
      "epoch": 4.579733333333333,
      "grad_norm": 0.15990248322486877,
      "learning_rate": 2.1376666666666668e-05,
      "loss": 0.0014,
      "step": 85870
    },
    {
      "epoch": 4.580266666666667,
      "grad_norm": 0.32989203929901123,
      "learning_rate": 2.1373333333333334e-05,
      "loss": 0.0021,
      "step": 85880
    },
    {
      "epoch": 4.5808,
      "grad_norm": 0.20986926555633545,
      "learning_rate": 2.137e-05,
      "loss": 0.002,
      "step": 85890
    },
    {
      "epoch": 4.581333333333333,
      "grad_norm": 0.3888155221939087,
      "learning_rate": 2.1366666666666667e-05,
      "loss": 0.002,
      "step": 85900
    },
    {
      "epoch": 4.5818666666666665,
      "grad_norm": 0.03555202856659889,
      "learning_rate": 2.1363333333333336e-05,
      "loss": 0.0021,
      "step": 85910
    },
    {
      "epoch": 4.5824,
      "grad_norm": 0.5504375100135803,
      "learning_rate": 2.1360000000000002e-05,
      "loss": 0.0024,
      "step": 85920
    },
    {
      "epoch": 4.582933333333333,
      "grad_norm": 0.6225420832633972,
      "learning_rate": 2.1356666666666665e-05,
      "loss": 0.0025,
      "step": 85930
    },
    {
      "epoch": 4.583466666666666,
      "grad_norm": 0.12077128887176514,
      "learning_rate": 2.1353333333333334e-05,
      "loss": 0.002,
      "step": 85940
    },
    {
      "epoch": 4.584,
      "grad_norm": 0.7053940296173096,
      "learning_rate": 2.135e-05,
      "loss": 0.0018,
      "step": 85950
    },
    {
      "epoch": 4.584533333333333,
      "grad_norm": 0.4580218493938446,
      "learning_rate": 2.1346666666666667e-05,
      "loss": 0.0019,
      "step": 85960
    },
    {
      "epoch": 4.585066666666666,
      "grad_norm": 0.2203696221113205,
      "learning_rate": 2.1343333333333336e-05,
      "loss": 0.0017,
      "step": 85970
    },
    {
      "epoch": 4.5856,
      "grad_norm": 0.21109269559383392,
      "learning_rate": 2.1340000000000002e-05,
      "loss": 0.0018,
      "step": 85980
    },
    {
      "epoch": 4.586133333333334,
      "grad_norm": 0.21078172326087952,
      "learning_rate": 2.133666666666667e-05,
      "loss": 0.0018,
      "step": 85990
    },
    {
      "epoch": 4.586666666666667,
      "grad_norm": 0.07646600157022476,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 0.0017,
      "step": 86000
    },
    {
      "epoch": 4.5872,
      "grad_norm": 0.09110191464424133,
      "learning_rate": 2.133e-05,
      "loss": 0.0013,
      "step": 86010
    },
    {
      "epoch": 4.587733333333333,
      "grad_norm": 0.02477085404098034,
      "learning_rate": 2.1326666666666667e-05,
      "loss": 0.002,
      "step": 86020
    },
    {
      "epoch": 4.588266666666667,
      "grad_norm": 0.49000808596611023,
      "learning_rate": 2.1323333333333333e-05,
      "loss": 0.0024,
      "step": 86030
    },
    {
      "epoch": 4.5888,
      "grad_norm": 0.04215382784605026,
      "learning_rate": 2.1320000000000003e-05,
      "loss": 0.0017,
      "step": 86040
    },
    {
      "epoch": 4.589333333333333,
      "grad_norm": 0.3746631145477295,
      "learning_rate": 2.131666666666667e-05,
      "loss": 0.0022,
      "step": 86050
    },
    {
      "epoch": 4.5898666666666665,
      "grad_norm": 0.30729252099990845,
      "learning_rate": 2.1313333333333335e-05,
      "loss": 0.0016,
      "step": 86060
    },
    {
      "epoch": 4.5904,
      "grad_norm": 0.047415200620889664,
      "learning_rate": 2.131e-05,
      "loss": 0.0016,
      "step": 86070
    },
    {
      "epoch": 4.590933333333333,
      "grad_norm": 0.2389119267463684,
      "learning_rate": 2.1306666666666667e-05,
      "loss": 0.0012,
      "step": 86080
    },
    {
      "epoch": 4.591466666666666,
      "grad_norm": 0.05861647054553032,
      "learning_rate": 2.1303333333333333e-05,
      "loss": 0.0016,
      "step": 86090
    },
    {
      "epoch": 4.592,
      "grad_norm": 0.19499768316745758,
      "learning_rate": 2.13e-05,
      "loss": 0.0017,
      "step": 86100
    },
    {
      "epoch": 4.592533333333334,
      "grad_norm": 0.182893306016922,
      "learning_rate": 2.129666666666667e-05,
      "loss": 0.0022,
      "step": 86110
    },
    {
      "epoch": 4.593066666666667,
      "grad_norm": 0.03253120556473732,
      "learning_rate": 2.1293333333333335e-05,
      "loss": 0.0021,
      "step": 86120
    },
    {
      "epoch": 4.5936,
      "grad_norm": 0.15158239006996155,
      "learning_rate": 2.129e-05,
      "loss": 0.0023,
      "step": 86130
    },
    {
      "epoch": 4.594133333333334,
      "grad_norm": 0.4103197157382965,
      "learning_rate": 2.1286666666666667e-05,
      "loss": 0.002,
      "step": 86140
    },
    {
      "epoch": 4.594666666666667,
      "grad_norm": 0.14945614337921143,
      "learning_rate": 2.1283333333333337e-05,
      "loss": 0.0018,
      "step": 86150
    },
    {
      "epoch": 4.5952,
      "grad_norm": 0.21908500790596008,
      "learning_rate": 2.128e-05,
      "loss": 0.0024,
      "step": 86160
    },
    {
      "epoch": 4.5957333333333334,
      "grad_norm": 0.27352428436279297,
      "learning_rate": 2.1276666666666666e-05,
      "loss": 0.0021,
      "step": 86170
    },
    {
      "epoch": 4.596266666666667,
      "grad_norm": 0.2068263292312622,
      "learning_rate": 2.1273333333333335e-05,
      "loss": 0.002,
      "step": 86180
    },
    {
      "epoch": 4.5968,
      "grad_norm": 0.28414154052734375,
      "learning_rate": 2.127e-05,
      "loss": 0.002,
      "step": 86190
    },
    {
      "epoch": 4.597333333333333,
      "grad_norm": 0.3960528075695038,
      "learning_rate": 2.1266666666666667e-05,
      "loss": 0.0016,
      "step": 86200
    },
    {
      "epoch": 4.5978666666666665,
      "grad_norm": 0.19550587236881256,
      "learning_rate": 2.1263333333333334e-05,
      "loss": 0.0021,
      "step": 86210
    },
    {
      "epoch": 4.5984,
      "grad_norm": 0.04923897236585617,
      "learning_rate": 2.1260000000000003e-05,
      "loss": 0.0026,
      "step": 86220
    },
    {
      "epoch": 4.598933333333333,
      "grad_norm": 0.4566866457462311,
      "learning_rate": 2.1256666666666666e-05,
      "loss": 0.0022,
      "step": 86230
    },
    {
      "epoch": 4.599466666666666,
      "grad_norm": 0.5966909527778625,
      "learning_rate": 2.1253333333333332e-05,
      "loss": 0.0024,
      "step": 86240
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.1452246755361557,
      "learning_rate": 2.125e-05,
      "loss": 0.002,
      "step": 86250
    },
    {
      "epoch": 4.600533333333333,
      "grad_norm": 0.16897830367088318,
      "learning_rate": 2.1246666666666668e-05,
      "loss": 0.0014,
      "step": 86260
    },
    {
      "epoch": 4.601066666666666,
      "grad_norm": 0.1510995775461197,
      "learning_rate": 2.1243333333333334e-05,
      "loss": 0.0018,
      "step": 86270
    },
    {
      "epoch": 4.6016,
      "grad_norm": 0.273738294839859,
      "learning_rate": 2.124e-05,
      "loss": 0.0018,
      "step": 86280
    },
    {
      "epoch": 4.602133333333334,
      "grad_norm": 0.591504693031311,
      "learning_rate": 2.123666666666667e-05,
      "loss": 0.0022,
      "step": 86290
    },
    {
      "epoch": 4.602666666666667,
      "grad_norm": 0.36338475346565247,
      "learning_rate": 2.1233333333333336e-05,
      "loss": 0.0037,
      "step": 86300
    },
    {
      "epoch": 4.6032,
      "grad_norm": 0.1570429503917694,
      "learning_rate": 2.123e-05,
      "loss": 0.0023,
      "step": 86310
    },
    {
      "epoch": 4.6037333333333335,
      "grad_norm": 0.3742818832397461,
      "learning_rate": 2.1226666666666668e-05,
      "loss": 0.0015,
      "step": 86320
    },
    {
      "epoch": 4.604266666666667,
      "grad_norm": 0.32926684617996216,
      "learning_rate": 2.1223333333333334e-05,
      "loss": 0.0014,
      "step": 86330
    },
    {
      "epoch": 4.6048,
      "grad_norm": 0.33875370025634766,
      "learning_rate": 2.122e-05,
      "loss": 0.0019,
      "step": 86340
    },
    {
      "epoch": 4.605333333333333,
      "grad_norm": 0.15512767434120178,
      "learning_rate": 2.121666666666667e-05,
      "loss": 0.002,
      "step": 86350
    },
    {
      "epoch": 4.6058666666666666,
      "grad_norm": 0.0629585012793541,
      "learning_rate": 2.1213333333333336e-05,
      "loss": 0.0018,
      "step": 86360
    },
    {
      "epoch": 4.6064,
      "grad_norm": 0.12992720305919647,
      "learning_rate": 2.1210000000000002e-05,
      "loss": 0.0014,
      "step": 86370
    },
    {
      "epoch": 4.606933333333333,
      "grad_norm": 0.04651501402258873,
      "learning_rate": 2.1206666666666665e-05,
      "loss": 0.0022,
      "step": 86380
    },
    {
      "epoch": 4.607466666666666,
      "grad_norm": 0.16933481395244598,
      "learning_rate": 2.1203333333333334e-05,
      "loss": 0.0017,
      "step": 86390
    },
    {
      "epoch": 4.608,
      "grad_norm": 0.14809361100196838,
      "learning_rate": 2.12e-05,
      "loss": 0.0018,
      "step": 86400
    },
    {
      "epoch": 4.608533333333334,
      "grad_norm": 0.5024840831756592,
      "learning_rate": 2.1196666666666666e-05,
      "loss": 0.0015,
      "step": 86410
    },
    {
      "epoch": 4.609066666666667,
      "grad_norm": 0.09389395266771317,
      "learning_rate": 2.1193333333333336e-05,
      "loss": 0.0018,
      "step": 86420
    },
    {
      "epoch": 4.6096,
      "grad_norm": 0.2920418381690979,
      "learning_rate": 2.1190000000000002e-05,
      "loss": 0.0017,
      "step": 86430
    },
    {
      "epoch": 4.610133333333334,
      "grad_norm": 0.33136147260665894,
      "learning_rate": 2.1186666666666668e-05,
      "loss": 0.0018,
      "step": 86440
    },
    {
      "epoch": 4.610666666666667,
      "grad_norm": 0.15434423089027405,
      "learning_rate": 2.1183333333333334e-05,
      "loss": 0.0015,
      "step": 86450
    },
    {
      "epoch": 4.6112,
      "grad_norm": 0.37490159273147583,
      "learning_rate": 2.118e-05,
      "loss": 0.0014,
      "step": 86460
    },
    {
      "epoch": 4.6117333333333335,
      "grad_norm": 0.047381967306137085,
      "learning_rate": 2.1176666666666667e-05,
      "loss": 0.0021,
      "step": 86470
    },
    {
      "epoch": 4.612266666666667,
      "grad_norm": 0.3722468316555023,
      "learning_rate": 2.1173333333333333e-05,
      "loss": 0.0012,
      "step": 86480
    },
    {
      "epoch": 4.6128,
      "grad_norm": 0.09565886855125427,
      "learning_rate": 2.1170000000000002e-05,
      "loss": 0.0013,
      "step": 86490
    },
    {
      "epoch": 4.613333333333333,
      "grad_norm": 0.04640643298625946,
      "learning_rate": 2.116666666666667e-05,
      "loss": 0.0021,
      "step": 86500
    },
    {
      "epoch": 4.613866666666667,
      "grad_norm": 0.12558889389038086,
      "learning_rate": 2.1163333333333335e-05,
      "loss": 0.0016,
      "step": 86510
    },
    {
      "epoch": 4.6144,
      "grad_norm": 0.04008916765451431,
      "learning_rate": 2.116e-05,
      "loss": 0.0015,
      "step": 86520
    },
    {
      "epoch": 4.614933333333333,
      "grad_norm": 0.39582470059394836,
      "learning_rate": 2.1156666666666667e-05,
      "loss": 0.0014,
      "step": 86530
    },
    {
      "epoch": 4.615466666666666,
      "grad_norm": 0.2932276129722595,
      "learning_rate": 2.1153333333333333e-05,
      "loss": 0.0016,
      "step": 86540
    },
    {
      "epoch": 4.616,
      "grad_norm": 0.28220799565315247,
      "learning_rate": 2.115e-05,
      "loss": 0.0027,
      "step": 86550
    },
    {
      "epoch": 4.616533333333333,
      "grad_norm": 0.1089872345328331,
      "learning_rate": 2.114666666666667e-05,
      "loss": 0.0019,
      "step": 86560
    },
    {
      "epoch": 4.617066666666666,
      "grad_norm": 0.03573836386203766,
      "learning_rate": 2.1143333333333335e-05,
      "loss": 0.002,
      "step": 86570
    },
    {
      "epoch": 4.6176,
      "grad_norm": 0.26991039514541626,
      "learning_rate": 2.114e-05,
      "loss": 0.0014,
      "step": 86580
    },
    {
      "epoch": 4.618133333333334,
      "grad_norm": 0.452584832906723,
      "learning_rate": 2.1136666666666667e-05,
      "loss": 0.0015,
      "step": 86590
    },
    {
      "epoch": 4.618666666666667,
      "grad_norm": 0.5345543622970581,
      "learning_rate": 2.1133333333333337e-05,
      "loss": 0.0021,
      "step": 86600
    },
    {
      "epoch": 4.6192,
      "grad_norm": 0.04420042783021927,
      "learning_rate": 2.113e-05,
      "loss": 0.0018,
      "step": 86610
    },
    {
      "epoch": 4.6197333333333335,
      "grad_norm": 0.04551723226904869,
      "learning_rate": 2.1126666666666665e-05,
      "loss": 0.0018,
      "step": 86620
    },
    {
      "epoch": 4.620266666666667,
      "grad_norm": 0.41460537910461426,
      "learning_rate": 2.1123333333333335e-05,
      "loss": 0.0033,
      "step": 86630
    },
    {
      "epoch": 4.6208,
      "grad_norm": 0.047670938074588776,
      "learning_rate": 2.112e-05,
      "loss": 0.0019,
      "step": 86640
    },
    {
      "epoch": 4.621333333333333,
      "grad_norm": 0.2162346988916397,
      "learning_rate": 2.1116666666666667e-05,
      "loss": 0.0019,
      "step": 86650
    },
    {
      "epoch": 4.621866666666667,
      "grad_norm": 0.2347070872783661,
      "learning_rate": 2.1113333333333333e-05,
      "loss": 0.0015,
      "step": 86660
    },
    {
      "epoch": 4.6224,
      "grad_norm": 0.3874635100364685,
      "learning_rate": 2.1110000000000003e-05,
      "loss": 0.0015,
      "step": 86670
    },
    {
      "epoch": 4.622933333333333,
      "grad_norm": 0.24299517273902893,
      "learning_rate": 2.110666666666667e-05,
      "loss": 0.0027,
      "step": 86680
    },
    {
      "epoch": 4.623466666666666,
      "grad_norm": 0.5588781237602234,
      "learning_rate": 2.1103333333333332e-05,
      "loss": 0.0033,
      "step": 86690
    },
    {
      "epoch": 4.624,
      "grad_norm": 0.6832817196846008,
      "learning_rate": 2.11e-05,
      "loss": 0.0018,
      "step": 86700
    },
    {
      "epoch": 4.624533333333334,
      "grad_norm": 0.23594102263450623,
      "learning_rate": 2.1096666666666667e-05,
      "loss": 0.0027,
      "step": 86710
    },
    {
      "epoch": 4.625066666666667,
      "grad_norm": 0.16323868930339813,
      "learning_rate": 2.1093333333333334e-05,
      "loss": 0.0018,
      "step": 86720
    },
    {
      "epoch": 4.6256,
      "grad_norm": 0.24926941096782684,
      "learning_rate": 2.1090000000000003e-05,
      "loss": 0.002,
      "step": 86730
    },
    {
      "epoch": 4.626133333333334,
      "grad_norm": 0.33173829317092896,
      "learning_rate": 2.108666666666667e-05,
      "loss": 0.0028,
      "step": 86740
    },
    {
      "epoch": 4.626666666666667,
      "grad_norm": 0.21734073758125305,
      "learning_rate": 2.1083333333333335e-05,
      "loss": 0.0026,
      "step": 86750
    },
    {
      "epoch": 4.6272,
      "grad_norm": 0.36870089173316956,
      "learning_rate": 2.1079999999999998e-05,
      "loss": 0.002,
      "step": 86760
    },
    {
      "epoch": 4.6277333333333335,
      "grad_norm": 0.3200952112674713,
      "learning_rate": 2.1076666666666668e-05,
      "loss": 0.0016,
      "step": 86770
    },
    {
      "epoch": 4.628266666666667,
      "grad_norm": 0.1115342229604721,
      "learning_rate": 2.1073333333333334e-05,
      "loss": 0.0023,
      "step": 86780
    },
    {
      "epoch": 4.6288,
      "grad_norm": 0.4429246783256531,
      "learning_rate": 2.107e-05,
      "loss": 0.0017,
      "step": 86790
    },
    {
      "epoch": 4.629333333333333,
      "grad_norm": 0.37675586342811584,
      "learning_rate": 2.106666666666667e-05,
      "loss": 0.0024,
      "step": 86800
    },
    {
      "epoch": 4.629866666666667,
      "grad_norm": 0.3654562830924988,
      "learning_rate": 2.1063333333333336e-05,
      "loss": 0.0017,
      "step": 86810
    },
    {
      "epoch": 4.6304,
      "grad_norm": 0.19584886729717255,
      "learning_rate": 2.106e-05,
      "loss": 0.0028,
      "step": 86820
    },
    {
      "epoch": 4.630933333333333,
      "grad_norm": 0.26532042026519775,
      "learning_rate": 2.1056666666666668e-05,
      "loss": 0.0027,
      "step": 86830
    },
    {
      "epoch": 4.631466666666666,
      "grad_norm": 0.08261698484420776,
      "learning_rate": 2.1053333333333334e-05,
      "loss": 0.0031,
      "step": 86840
    },
    {
      "epoch": 4.632,
      "grad_norm": 0.19254133105278015,
      "learning_rate": 2.105e-05,
      "loss": 0.0025,
      "step": 86850
    },
    {
      "epoch": 4.632533333333333,
      "grad_norm": 0.049495864659547806,
      "learning_rate": 2.1046666666666666e-05,
      "loss": 0.0022,
      "step": 86860
    },
    {
      "epoch": 4.633066666666666,
      "grad_norm": 0.16832509636878967,
      "learning_rate": 2.1043333333333336e-05,
      "loss": 0.0018,
      "step": 86870
    },
    {
      "epoch": 4.6336,
      "grad_norm": 0.22497500479221344,
      "learning_rate": 2.1040000000000002e-05,
      "loss": 0.0023,
      "step": 86880
    },
    {
      "epoch": 4.634133333333334,
      "grad_norm": 0.1917467564344406,
      "learning_rate": 2.1036666666666668e-05,
      "loss": 0.0017,
      "step": 86890
    },
    {
      "epoch": 4.634666666666667,
      "grad_norm": 0.2793203890323639,
      "learning_rate": 2.1033333333333334e-05,
      "loss": 0.0019,
      "step": 86900
    },
    {
      "epoch": 4.6352,
      "grad_norm": 0.34441953897476196,
      "learning_rate": 2.103e-05,
      "loss": 0.0015,
      "step": 86910
    },
    {
      "epoch": 4.6357333333333335,
      "grad_norm": 0.39144015312194824,
      "learning_rate": 2.1026666666666666e-05,
      "loss": 0.0025,
      "step": 86920
    },
    {
      "epoch": 4.636266666666667,
      "grad_norm": 0.07452093809843063,
      "learning_rate": 2.1023333333333333e-05,
      "loss": 0.0031,
      "step": 86930
    },
    {
      "epoch": 4.6368,
      "grad_norm": 0.07185304909944534,
      "learning_rate": 2.1020000000000002e-05,
      "loss": 0.0018,
      "step": 86940
    },
    {
      "epoch": 4.637333333333333,
      "grad_norm": 0.3453754484653473,
      "learning_rate": 2.1016666666666668e-05,
      "loss": 0.0016,
      "step": 86950
    },
    {
      "epoch": 4.637866666666667,
      "grad_norm": 0.1674308031797409,
      "learning_rate": 2.1013333333333334e-05,
      "loss": 0.002,
      "step": 86960
    },
    {
      "epoch": 4.6384,
      "grad_norm": 0.36253440380096436,
      "learning_rate": 2.101e-05,
      "loss": 0.0019,
      "step": 86970
    },
    {
      "epoch": 4.638933333333333,
      "grad_norm": 0.48936668038368225,
      "learning_rate": 2.100666666666667e-05,
      "loss": 0.0017,
      "step": 86980
    },
    {
      "epoch": 4.639466666666666,
      "grad_norm": 0.09921462088823318,
      "learning_rate": 2.1003333333333333e-05,
      "loss": 0.0028,
      "step": 86990
    },
    {
      "epoch": 4.64,
      "grad_norm": 0.15852634608745575,
      "learning_rate": 2.1e-05,
      "loss": 0.0019,
      "step": 87000
    },
    {
      "epoch": 4.640533333333333,
      "grad_norm": 0.09195413440465927,
      "learning_rate": 2.099666666666667e-05,
      "loss": 0.0015,
      "step": 87010
    },
    {
      "epoch": 4.641066666666667,
      "grad_norm": 0.41779422760009766,
      "learning_rate": 2.0993333333333334e-05,
      "loss": 0.0017,
      "step": 87020
    },
    {
      "epoch": 4.6416,
      "grad_norm": 0.37468627095222473,
      "learning_rate": 2.099e-05,
      "loss": 0.0025,
      "step": 87030
    },
    {
      "epoch": 4.642133333333334,
      "grad_norm": 0.3062112033367157,
      "learning_rate": 2.0986666666666667e-05,
      "loss": 0.0021,
      "step": 87040
    },
    {
      "epoch": 4.642666666666667,
      "grad_norm": 0.2531811594963074,
      "learning_rate": 2.0983333333333336e-05,
      "loss": 0.0014,
      "step": 87050
    },
    {
      "epoch": 4.6432,
      "grad_norm": 0.1102682501077652,
      "learning_rate": 2.098e-05,
      "loss": 0.0023,
      "step": 87060
    },
    {
      "epoch": 4.6437333333333335,
      "grad_norm": 0.08142205327749252,
      "learning_rate": 2.0976666666666665e-05,
      "loss": 0.0023,
      "step": 87070
    },
    {
      "epoch": 4.644266666666667,
      "grad_norm": 0.09664766490459442,
      "learning_rate": 2.0973333333333335e-05,
      "loss": 0.002,
      "step": 87080
    },
    {
      "epoch": 4.6448,
      "grad_norm": 0.21530888974666595,
      "learning_rate": 2.097e-05,
      "loss": 0.0021,
      "step": 87090
    },
    {
      "epoch": 4.645333333333333,
      "grad_norm": 0.14065252244472504,
      "learning_rate": 2.0966666666666667e-05,
      "loss": 0.0012,
      "step": 87100
    },
    {
      "epoch": 4.645866666666667,
      "grad_norm": 0.28438839316368103,
      "learning_rate": 2.0963333333333336e-05,
      "loss": 0.0021,
      "step": 87110
    },
    {
      "epoch": 4.6464,
      "grad_norm": 0.4574996829032898,
      "learning_rate": 2.0960000000000003e-05,
      "loss": 0.0019,
      "step": 87120
    },
    {
      "epoch": 4.646933333333333,
      "grad_norm": 0.18868416547775269,
      "learning_rate": 2.095666666666667e-05,
      "loss": 0.0028,
      "step": 87130
    },
    {
      "epoch": 4.647466666666666,
      "grad_norm": 0.32284054160118103,
      "learning_rate": 2.095333333333333e-05,
      "loss": 0.002,
      "step": 87140
    },
    {
      "epoch": 4.648,
      "grad_norm": 0.24292950332164764,
      "learning_rate": 2.095e-05,
      "loss": 0.002,
      "step": 87150
    },
    {
      "epoch": 4.648533333333333,
      "grad_norm": 0.3117590844631195,
      "learning_rate": 2.0946666666666667e-05,
      "loss": 0.0019,
      "step": 87160
    },
    {
      "epoch": 4.649066666666666,
      "grad_norm": 0.47891294956207275,
      "learning_rate": 2.0943333333333333e-05,
      "loss": 0.0017,
      "step": 87170
    },
    {
      "epoch": 4.6495999999999995,
      "grad_norm": 0.6639277935028076,
      "learning_rate": 2.0940000000000003e-05,
      "loss": 0.0013,
      "step": 87180
    },
    {
      "epoch": 4.650133333333334,
      "grad_norm": 0.3763294816017151,
      "learning_rate": 2.093666666666667e-05,
      "loss": 0.0017,
      "step": 87190
    },
    {
      "epoch": 4.650666666666667,
      "grad_norm": 0.03755074366927147,
      "learning_rate": 2.0933333333333335e-05,
      "loss": 0.0013,
      "step": 87200
    },
    {
      "epoch": 4.6512,
      "grad_norm": 0.17808137834072113,
      "learning_rate": 2.093e-05,
      "loss": 0.0018,
      "step": 87210
    },
    {
      "epoch": 4.6517333333333335,
      "grad_norm": 0.22468508780002594,
      "learning_rate": 2.0926666666666667e-05,
      "loss": 0.0021,
      "step": 87220
    },
    {
      "epoch": 4.652266666666667,
      "grad_norm": 0.09020928293466568,
      "learning_rate": 2.0923333333333333e-05,
      "loss": 0.0022,
      "step": 87230
    },
    {
      "epoch": 4.6528,
      "grad_norm": 0.0998510792851448,
      "learning_rate": 2.092e-05,
      "loss": 0.0024,
      "step": 87240
    },
    {
      "epoch": 4.653333333333333,
      "grad_norm": 0.4445215165615082,
      "learning_rate": 2.091666666666667e-05,
      "loss": 0.0023,
      "step": 87250
    },
    {
      "epoch": 4.653866666666667,
      "grad_norm": 0.08927712589502335,
      "learning_rate": 2.0913333333333335e-05,
      "loss": 0.0015,
      "step": 87260
    },
    {
      "epoch": 4.6544,
      "grad_norm": 0.629485547542572,
      "learning_rate": 2.091e-05,
      "loss": 0.0019,
      "step": 87270
    },
    {
      "epoch": 4.654933333333333,
      "grad_norm": 0.04997634142637253,
      "learning_rate": 2.0906666666666668e-05,
      "loss": 0.0017,
      "step": 87280
    },
    {
      "epoch": 4.655466666666666,
      "grad_norm": 0.46470561623573303,
      "learning_rate": 2.0903333333333334e-05,
      "loss": 0.0016,
      "step": 87290
    },
    {
      "epoch": 4.656,
      "grad_norm": 0.06478040665388107,
      "learning_rate": 2.09e-05,
      "loss": 0.0017,
      "step": 87300
    },
    {
      "epoch": 4.656533333333333,
      "grad_norm": 0.4309035539627075,
      "learning_rate": 2.0896666666666666e-05,
      "loss": 0.0024,
      "step": 87310
    },
    {
      "epoch": 4.657066666666667,
      "grad_norm": 0.29575538635253906,
      "learning_rate": 2.0893333333333335e-05,
      "loss": 0.0015,
      "step": 87320
    },
    {
      "epoch": 4.6576,
      "grad_norm": 0.24583686888217926,
      "learning_rate": 2.089e-05,
      "loss": 0.0016,
      "step": 87330
    },
    {
      "epoch": 4.658133333333334,
      "grad_norm": 0.09401912242174149,
      "learning_rate": 2.0886666666666668e-05,
      "loss": 0.0013,
      "step": 87340
    },
    {
      "epoch": 4.658666666666667,
      "grad_norm": 0.16376419365406036,
      "learning_rate": 2.0883333333333334e-05,
      "loss": 0.0017,
      "step": 87350
    },
    {
      "epoch": 4.6592,
      "grad_norm": 0.06459008157253265,
      "learning_rate": 2.0880000000000003e-05,
      "loss": 0.0022,
      "step": 87360
    },
    {
      "epoch": 4.6597333333333335,
      "grad_norm": 0.2809271812438965,
      "learning_rate": 2.0876666666666666e-05,
      "loss": 0.0021,
      "step": 87370
    },
    {
      "epoch": 4.660266666666667,
      "grad_norm": 0.09433723986148834,
      "learning_rate": 2.0873333333333332e-05,
      "loss": 0.0015,
      "step": 87380
    },
    {
      "epoch": 4.6608,
      "grad_norm": 0.2780993580818176,
      "learning_rate": 2.0870000000000002e-05,
      "loss": 0.0027,
      "step": 87390
    },
    {
      "epoch": 4.661333333333333,
      "grad_norm": 0.12925614416599274,
      "learning_rate": 2.0866666666666668e-05,
      "loss": 0.0028,
      "step": 87400
    },
    {
      "epoch": 4.661866666666667,
      "grad_norm": 0.2091415524482727,
      "learning_rate": 2.0863333333333334e-05,
      "loss": 0.0018,
      "step": 87410
    },
    {
      "epoch": 4.6624,
      "grad_norm": 0.30472925305366516,
      "learning_rate": 2.086e-05,
      "loss": 0.0018,
      "step": 87420
    },
    {
      "epoch": 4.662933333333333,
      "grad_norm": 0.06807959079742432,
      "learning_rate": 2.085666666666667e-05,
      "loss": 0.0021,
      "step": 87430
    },
    {
      "epoch": 4.663466666666666,
      "grad_norm": 0.2168445736169815,
      "learning_rate": 2.0853333333333332e-05,
      "loss": 0.0016,
      "step": 87440
    },
    {
      "epoch": 4.664,
      "grad_norm": 0.3213984966278076,
      "learning_rate": 2.085e-05,
      "loss": 0.0025,
      "step": 87450
    },
    {
      "epoch": 4.664533333333333,
      "grad_norm": 0.04768979549407959,
      "learning_rate": 2.0846666666666668e-05,
      "loss": 0.0023,
      "step": 87460
    },
    {
      "epoch": 4.665066666666666,
      "grad_norm": 0.16153177618980408,
      "learning_rate": 2.0843333333333334e-05,
      "loss": 0.0013,
      "step": 87470
    },
    {
      "epoch": 4.6655999999999995,
      "grad_norm": 0.4547049105167389,
      "learning_rate": 2.084e-05,
      "loss": 0.0021,
      "step": 87480
    },
    {
      "epoch": 4.666133333333334,
      "grad_norm": 0.24646563827991486,
      "learning_rate": 2.083666666666667e-05,
      "loss": 0.0022,
      "step": 87490
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 0.40122002363204956,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 0.0019,
      "step": 87500
    },
    {
      "epoch": 4.6672,
      "grad_norm": 0.27025723457336426,
      "learning_rate": 2.0830000000000002e-05,
      "loss": 0.0025,
      "step": 87510
    },
    {
      "epoch": 4.6677333333333335,
      "grad_norm": 0.05339264124631882,
      "learning_rate": 2.0826666666666665e-05,
      "loss": 0.0024,
      "step": 87520
    },
    {
      "epoch": 4.668266666666667,
      "grad_norm": 0.14881673455238342,
      "learning_rate": 2.0823333333333334e-05,
      "loss": 0.002,
      "step": 87530
    },
    {
      "epoch": 4.6688,
      "grad_norm": 0.4784773588180542,
      "learning_rate": 2.082e-05,
      "loss": 0.0017,
      "step": 87540
    },
    {
      "epoch": 4.669333333333333,
      "grad_norm": 0.2123151421546936,
      "learning_rate": 2.0816666666666667e-05,
      "loss": 0.0026,
      "step": 87550
    },
    {
      "epoch": 4.669866666666667,
      "grad_norm": 0.24834957718849182,
      "learning_rate": 2.0813333333333336e-05,
      "loss": 0.0018,
      "step": 87560
    },
    {
      "epoch": 4.6704,
      "grad_norm": 0.22479726374149323,
      "learning_rate": 2.0810000000000002e-05,
      "loss": 0.0018,
      "step": 87570
    },
    {
      "epoch": 4.670933333333333,
      "grad_norm": 0.4846683740615845,
      "learning_rate": 2.080666666666667e-05,
      "loss": 0.0012,
      "step": 87580
    },
    {
      "epoch": 4.671466666666666,
      "grad_norm": 0.3639652132987976,
      "learning_rate": 2.0803333333333335e-05,
      "loss": 0.0021,
      "step": 87590
    },
    {
      "epoch": 4.672,
      "grad_norm": 0.46924665570259094,
      "learning_rate": 2.08e-05,
      "loss": 0.0016,
      "step": 87600
    },
    {
      "epoch": 4.672533333333333,
      "grad_norm": 0.2211305797100067,
      "learning_rate": 2.0796666666666667e-05,
      "loss": 0.0023,
      "step": 87610
    },
    {
      "epoch": 4.673066666666667,
      "grad_norm": 0.3721696734428406,
      "learning_rate": 2.0793333333333333e-05,
      "loss": 0.0018,
      "step": 87620
    },
    {
      "epoch": 4.6736,
      "grad_norm": 0.23831233382225037,
      "learning_rate": 2.0790000000000003e-05,
      "loss": 0.002,
      "step": 87630
    },
    {
      "epoch": 4.674133333333334,
      "grad_norm": 0.4305877387523651,
      "learning_rate": 2.078666666666667e-05,
      "loss": 0.002,
      "step": 87640
    },
    {
      "epoch": 4.674666666666667,
      "grad_norm": 0.1428833305835724,
      "learning_rate": 2.0783333333333335e-05,
      "loss": 0.002,
      "step": 87650
    },
    {
      "epoch": 4.6752,
      "grad_norm": 0.3612554669380188,
      "learning_rate": 2.078e-05,
      "loss": 0.0014,
      "step": 87660
    },
    {
      "epoch": 4.6757333333333335,
      "grad_norm": 0.21644151210784912,
      "learning_rate": 2.0776666666666667e-05,
      "loss": 0.0018,
      "step": 87670
    },
    {
      "epoch": 4.676266666666667,
      "grad_norm": 0.2834792733192444,
      "learning_rate": 2.0773333333333333e-05,
      "loss": 0.0028,
      "step": 87680
    },
    {
      "epoch": 4.6768,
      "grad_norm": 0.13297022879123688,
      "learning_rate": 2.077e-05,
      "loss": 0.0024,
      "step": 87690
    },
    {
      "epoch": 4.677333333333333,
      "grad_norm": 0.18492425978183746,
      "learning_rate": 2.076666666666667e-05,
      "loss": 0.0026,
      "step": 87700
    },
    {
      "epoch": 4.677866666666667,
      "grad_norm": 0.1763208955526352,
      "learning_rate": 2.0763333333333335e-05,
      "loss": 0.0019,
      "step": 87710
    },
    {
      "epoch": 4.6784,
      "grad_norm": 0.30240365862846375,
      "learning_rate": 2.076e-05,
      "loss": 0.0019,
      "step": 87720
    },
    {
      "epoch": 4.678933333333333,
      "grad_norm": 0.2688964605331421,
      "learning_rate": 2.0756666666666667e-05,
      "loss": 0.0017,
      "step": 87730
    },
    {
      "epoch": 4.679466666666666,
      "grad_norm": 0.07209035009145737,
      "learning_rate": 2.0753333333333333e-05,
      "loss": 0.0017,
      "step": 87740
    },
    {
      "epoch": 4.68,
      "grad_norm": 0.19170229136943817,
      "learning_rate": 2.075e-05,
      "loss": 0.0018,
      "step": 87750
    },
    {
      "epoch": 4.680533333333333,
      "grad_norm": 0.2690335810184479,
      "learning_rate": 2.0746666666666666e-05,
      "loss": 0.0018,
      "step": 87760
    },
    {
      "epoch": 4.681066666666666,
      "grad_norm": 0.3399674892425537,
      "learning_rate": 2.0743333333333335e-05,
      "loss": 0.0023,
      "step": 87770
    },
    {
      "epoch": 4.6815999999999995,
      "grad_norm": 0.4565548598766327,
      "learning_rate": 2.074e-05,
      "loss": 0.0021,
      "step": 87780
    },
    {
      "epoch": 4.682133333333334,
      "grad_norm": 0.12975771725177765,
      "learning_rate": 2.0736666666666667e-05,
      "loss": 0.0015,
      "step": 87790
    },
    {
      "epoch": 4.682666666666667,
      "grad_norm": 0.3427254855632782,
      "learning_rate": 2.0733333333333334e-05,
      "loss": 0.002,
      "step": 87800
    },
    {
      "epoch": 4.6832,
      "grad_norm": 0.221397265791893,
      "learning_rate": 2.0730000000000003e-05,
      "loss": 0.0013,
      "step": 87810
    },
    {
      "epoch": 4.6837333333333335,
      "grad_norm": 0.03287772089242935,
      "learning_rate": 2.0726666666666666e-05,
      "loss": 0.0024,
      "step": 87820
    },
    {
      "epoch": 4.684266666666667,
      "grad_norm": 0.25246208906173706,
      "learning_rate": 2.0723333333333332e-05,
      "loss": 0.0014,
      "step": 87830
    },
    {
      "epoch": 4.6848,
      "grad_norm": 0.3413930833339691,
      "learning_rate": 2.072e-05,
      "loss": 0.0014,
      "step": 87840
    },
    {
      "epoch": 4.685333333333333,
      "grad_norm": 0.04376356303691864,
      "learning_rate": 2.0716666666666668e-05,
      "loss": 0.0027,
      "step": 87850
    },
    {
      "epoch": 4.685866666666667,
      "grad_norm": 0.25874537229537964,
      "learning_rate": 2.0713333333333334e-05,
      "loss": 0.0026,
      "step": 87860
    },
    {
      "epoch": 4.6864,
      "grad_norm": 0.13190528750419617,
      "learning_rate": 2.0710000000000003e-05,
      "loss": 0.0014,
      "step": 87870
    },
    {
      "epoch": 4.686933333333333,
      "grad_norm": 0.6665406823158264,
      "learning_rate": 2.070666666666667e-05,
      "loss": 0.0013,
      "step": 87880
    },
    {
      "epoch": 4.6874666666666664,
      "grad_norm": 0.37707024812698364,
      "learning_rate": 2.0703333333333336e-05,
      "loss": 0.0016,
      "step": 87890
    },
    {
      "epoch": 4.688,
      "grad_norm": 0.18533258140087128,
      "learning_rate": 2.07e-05,
      "loss": 0.0027,
      "step": 87900
    },
    {
      "epoch": 4.688533333333333,
      "grad_norm": 0.1574293076992035,
      "learning_rate": 2.0696666666666668e-05,
      "loss": 0.002,
      "step": 87910
    },
    {
      "epoch": 4.689066666666667,
      "grad_norm": 0.2831048369407654,
      "learning_rate": 2.0693333333333334e-05,
      "loss": 0.0011,
      "step": 87920
    },
    {
      "epoch": 4.6896,
      "grad_norm": 0.15676037967205048,
      "learning_rate": 2.069e-05,
      "loss": 0.0012,
      "step": 87930
    },
    {
      "epoch": 4.690133333333334,
      "grad_norm": 0.183731347322464,
      "learning_rate": 2.068666666666667e-05,
      "loss": 0.0021,
      "step": 87940
    },
    {
      "epoch": 4.690666666666667,
      "grad_norm": 0.1643119901418686,
      "learning_rate": 2.0683333333333336e-05,
      "loss": 0.0018,
      "step": 87950
    },
    {
      "epoch": 4.6912,
      "grad_norm": 0.10129618644714355,
      "learning_rate": 2.0680000000000002e-05,
      "loss": 0.003,
      "step": 87960
    },
    {
      "epoch": 4.6917333333333335,
      "grad_norm": 0.2623424232006073,
      "learning_rate": 2.0676666666666668e-05,
      "loss": 0.0019,
      "step": 87970
    },
    {
      "epoch": 4.692266666666667,
      "grad_norm": 0.502510130405426,
      "learning_rate": 2.0673333333333334e-05,
      "loss": 0.0018,
      "step": 87980
    },
    {
      "epoch": 4.6928,
      "grad_norm": 0.0556931272149086,
      "learning_rate": 2.067e-05,
      "loss": 0.0021,
      "step": 87990
    },
    {
      "epoch": 4.693333333333333,
      "grad_norm": 0.1895260065793991,
      "learning_rate": 2.0666666666666666e-05,
      "loss": 0.0023,
      "step": 88000
    },
    {
      "epoch": 4.693866666666667,
      "grad_norm": 0.372852087020874,
      "learning_rate": 2.0663333333333336e-05,
      "loss": 0.0023,
      "step": 88010
    },
    {
      "epoch": 4.6944,
      "grad_norm": 0.07343049347400665,
      "learning_rate": 2.0660000000000002e-05,
      "loss": 0.0022,
      "step": 88020
    },
    {
      "epoch": 4.694933333333333,
      "grad_norm": 0.48240745067596436,
      "learning_rate": 2.0656666666666668e-05,
      "loss": 0.0022,
      "step": 88030
    },
    {
      "epoch": 4.6954666666666665,
      "grad_norm": 0.13931883871555328,
      "learning_rate": 2.0653333333333334e-05,
      "loss": 0.0019,
      "step": 88040
    },
    {
      "epoch": 4.696,
      "grad_norm": 0.09743713587522507,
      "learning_rate": 2.065e-05,
      "loss": 0.0021,
      "step": 88050
    },
    {
      "epoch": 4.696533333333333,
      "grad_norm": 0.07224448770284653,
      "learning_rate": 2.0646666666666667e-05,
      "loss": 0.0016,
      "step": 88060
    },
    {
      "epoch": 4.697066666666666,
      "grad_norm": 0.1576254814863205,
      "learning_rate": 2.0643333333333333e-05,
      "loss": 0.0021,
      "step": 88070
    },
    {
      "epoch": 4.6975999999999996,
      "grad_norm": 0.16206388175487518,
      "learning_rate": 2.0640000000000002e-05,
      "loss": 0.0018,
      "step": 88080
    },
    {
      "epoch": 4.698133333333334,
      "grad_norm": 0.4439248740673065,
      "learning_rate": 2.063666666666667e-05,
      "loss": 0.0022,
      "step": 88090
    },
    {
      "epoch": 4.698666666666667,
      "grad_norm": 0.210256889462471,
      "learning_rate": 2.0633333333333335e-05,
      "loss": 0.002,
      "step": 88100
    },
    {
      "epoch": 4.6992,
      "grad_norm": 0.337836354970932,
      "learning_rate": 2.063e-05,
      "loss": 0.0024,
      "step": 88110
    },
    {
      "epoch": 4.6997333333333335,
      "grad_norm": 0.3058619797229767,
      "learning_rate": 2.0626666666666667e-05,
      "loss": 0.0021,
      "step": 88120
    },
    {
      "epoch": 4.700266666666667,
      "grad_norm": 0.041838403791189194,
      "learning_rate": 2.0623333333333333e-05,
      "loss": 0.0021,
      "step": 88130
    },
    {
      "epoch": 4.7008,
      "grad_norm": 0.4833449125289917,
      "learning_rate": 2.062e-05,
      "loss": 0.0013,
      "step": 88140
    },
    {
      "epoch": 4.701333333333333,
      "grad_norm": 0.7201255559921265,
      "learning_rate": 2.061666666666667e-05,
      "loss": 0.0017,
      "step": 88150
    },
    {
      "epoch": 4.701866666666667,
      "grad_norm": 0.08982115238904953,
      "learning_rate": 2.0613333333333335e-05,
      "loss": 0.0019,
      "step": 88160
    },
    {
      "epoch": 4.7024,
      "grad_norm": 0.22541606426239014,
      "learning_rate": 2.061e-05,
      "loss": 0.0017,
      "step": 88170
    },
    {
      "epoch": 4.702933333333333,
      "grad_norm": 0.22917930781841278,
      "learning_rate": 2.0606666666666667e-05,
      "loss": 0.0019,
      "step": 88180
    },
    {
      "epoch": 4.7034666666666665,
      "grad_norm": 0.3000410199165344,
      "learning_rate": 2.0603333333333337e-05,
      "loss": 0.0028,
      "step": 88190
    },
    {
      "epoch": 4.704,
      "grad_norm": 0.22032369673252106,
      "learning_rate": 2.06e-05,
      "loss": 0.0021,
      "step": 88200
    },
    {
      "epoch": 4.704533333333333,
      "grad_norm": 0.3869827687740326,
      "learning_rate": 2.0596666666666665e-05,
      "loss": 0.0017,
      "step": 88210
    },
    {
      "epoch": 4.705066666666666,
      "grad_norm": 0.32130205631256104,
      "learning_rate": 2.0593333333333335e-05,
      "loss": 0.0022,
      "step": 88220
    },
    {
      "epoch": 4.7056000000000004,
      "grad_norm": 0.5645259618759155,
      "learning_rate": 2.059e-05,
      "loss": 0.0026,
      "step": 88230
    },
    {
      "epoch": 4.706133333333334,
      "grad_norm": 0.26854610443115234,
      "learning_rate": 2.0586666666666667e-05,
      "loss": 0.0015,
      "step": 88240
    },
    {
      "epoch": 4.706666666666667,
      "grad_norm": 0.4251454770565033,
      "learning_rate": 2.0583333333333333e-05,
      "loss": 0.0023,
      "step": 88250
    },
    {
      "epoch": 4.7072,
      "grad_norm": 0.252555251121521,
      "learning_rate": 2.0580000000000003e-05,
      "loss": 0.0018,
      "step": 88260
    },
    {
      "epoch": 4.7077333333333335,
      "grad_norm": 0.10922404378652573,
      "learning_rate": 2.0576666666666666e-05,
      "loss": 0.0023,
      "step": 88270
    },
    {
      "epoch": 4.708266666666667,
      "grad_norm": 0.02672000601887703,
      "learning_rate": 2.0573333333333332e-05,
      "loss": 0.0019,
      "step": 88280
    },
    {
      "epoch": 4.7088,
      "grad_norm": 0.3072793483734131,
      "learning_rate": 2.057e-05,
      "loss": 0.0019,
      "step": 88290
    },
    {
      "epoch": 4.709333333333333,
      "grad_norm": 0.2429308146238327,
      "learning_rate": 2.0566666666666667e-05,
      "loss": 0.0023,
      "step": 88300
    },
    {
      "epoch": 4.709866666666667,
      "grad_norm": 0.4883706867694855,
      "learning_rate": 2.0563333333333334e-05,
      "loss": 0.0019,
      "step": 88310
    },
    {
      "epoch": 4.7104,
      "grad_norm": 0.09239750355482101,
      "learning_rate": 2.0560000000000003e-05,
      "loss": 0.0016,
      "step": 88320
    },
    {
      "epoch": 4.710933333333333,
      "grad_norm": 0.5104691982269287,
      "learning_rate": 2.055666666666667e-05,
      "loss": 0.0019,
      "step": 88330
    },
    {
      "epoch": 4.7114666666666665,
      "grad_norm": 0.5101544260978699,
      "learning_rate": 2.0553333333333335e-05,
      "loss": 0.0016,
      "step": 88340
    },
    {
      "epoch": 4.712,
      "grad_norm": 0.23524008691310883,
      "learning_rate": 2.055e-05,
      "loss": 0.0016,
      "step": 88350
    },
    {
      "epoch": 4.712533333333333,
      "grad_norm": 0.4094924330711365,
      "learning_rate": 2.0546666666666668e-05,
      "loss": 0.0024,
      "step": 88360
    },
    {
      "epoch": 4.713066666666666,
      "grad_norm": 0.06979642063379288,
      "learning_rate": 2.0543333333333334e-05,
      "loss": 0.0018,
      "step": 88370
    },
    {
      "epoch": 4.7136,
      "grad_norm": 0.4223090708255768,
      "learning_rate": 2.054e-05,
      "loss": 0.002,
      "step": 88380
    },
    {
      "epoch": 4.714133333333333,
      "grad_norm": 0.32566386461257935,
      "learning_rate": 2.053666666666667e-05,
      "loss": 0.0022,
      "step": 88390
    },
    {
      "epoch": 4.714666666666667,
      "grad_norm": 0.048266686499118805,
      "learning_rate": 2.0533333333333336e-05,
      "loss": 0.002,
      "step": 88400
    },
    {
      "epoch": 4.7152,
      "grad_norm": 0.06522712856531143,
      "learning_rate": 2.053e-05,
      "loss": 0.0012,
      "step": 88410
    },
    {
      "epoch": 4.7157333333333336,
      "grad_norm": 0.3336848318576813,
      "learning_rate": 2.0526666666666668e-05,
      "loss": 0.0019,
      "step": 88420
    },
    {
      "epoch": 4.716266666666667,
      "grad_norm": 0.3303777575492859,
      "learning_rate": 2.0523333333333334e-05,
      "loss": 0.0021,
      "step": 88430
    },
    {
      "epoch": 4.7168,
      "grad_norm": 0.2555195391178131,
      "learning_rate": 2.052e-05,
      "loss": 0.0022,
      "step": 88440
    },
    {
      "epoch": 4.717333333333333,
      "grad_norm": 0.30250465869903564,
      "learning_rate": 2.0516666666666666e-05,
      "loss": 0.0022,
      "step": 88450
    },
    {
      "epoch": 4.717866666666667,
      "grad_norm": 0.2162466049194336,
      "learning_rate": 2.0513333333333336e-05,
      "loss": 0.0019,
      "step": 88460
    },
    {
      "epoch": 4.7184,
      "grad_norm": 0.4702281653881073,
      "learning_rate": 2.0510000000000002e-05,
      "loss": 0.0015,
      "step": 88470
    },
    {
      "epoch": 4.718933333333333,
      "grad_norm": 0.133124440908432,
      "learning_rate": 2.0506666666666668e-05,
      "loss": 0.0021,
      "step": 88480
    },
    {
      "epoch": 4.7194666666666665,
      "grad_norm": 0.3702195882797241,
      "learning_rate": 2.0503333333333334e-05,
      "loss": 0.0018,
      "step": 88490
    },
    {
      "epoch": 4.72,
      "grad_norm": 0.14920181035995483,
      "learning_rate": 2.05e-05,
      "loss": 0.003,
      "step": 88500
    },
    {
      "epoch": 4.720533333333333,
      "grad_norm": 0.1653536707162857,
      "learning_rate": 2.0496666666666666e-05,
      "loss": 0.0017,
      "step": 88510
    },
    {
      "epoch": 4.721066666666666,
      "grad_norm": 0.1998150497674942,
      "learning_rate": 2.0493333333333333e-05,
      "loss": 0.0014,
      "step": 88520
    },
    {
      "epoch": 4.7216000000000005,
      "grad_norm": 0.5512151718139648,
      "learning_rate": 2.0490000000000002e-05,
      "loss": 0.0023,
      "step": 88530
    },
    {
      "epoch": 4.722133333333334,
      "grad_norm": 0.2751333713531494,
      "learning_rate": 2.0486666666666668e-05,
      "loss": 0.0021,
      "step": 88540
    },
    {
      "epoch": 4.722666666666667,
      "grad_norm": 0.21446822583675385,
      "learning_rate": 2.0483333333333334e-05,
      "loss": 0.0026,
      "step": 88550
    },
    {
      "epoch": 4.7232,
      "grad_norm": 0.2665110230445862,
      "learning_rate": 2.048e-05,
      "loss": 0.0021,
      "step": 88560
    },
    {
      "epoch": 4.723733333333334,
      "grad_norm": 0.09849365055561066,
      "learning_rate": 2.047666666666667e-05,
      "loss": 0.0018,
      "step": 88570
    },
    {
      "epoch": 4.724266666666667,
      "grad_norm": 0.27594175934791565,
      "learning_rate": 2.0473333333333333e-05,
      "loss": 0.0016,
      "step": 88580
    },
    {
      "epoch": 4.7248,
      "grad_norm": 0.04905293509364128,
      "learning_rate": 2.047e-05,
      "loss": 0.0023,
      "step": 88590
    },
    {
      "epoch": 4.725333333333333,
      "grad_norm": 0.30698278546333313,
      "learning_rate": 2.046666666666667e-05,
      "loss": 0.002,
      "step": 88600
    },
    {
      "epoch": 4.725866666666667,
      "grad_norm": 0.31244194507598877,
      "learning_rate": 2.0463333333333334e-05,
      "loss": 0.0017,
      "step": 88610
    },
    {
      "epoch": 4.7264,
      "grad_norm": 0.1437627524137497,
      "learning_rate": 2.046e-05,
      "loss": 0.0017,
      "step": 88620
    },
    {
      "epoch": 4.726933333333333,
      "grad_norm": 0.07497724145650864,
      "learning_rate": 2.0456666666666667e-05,
      "loss": 0.0017,
      "step": 88630
    },
    {
      "epoch": 4.7274666666666665,
      "grad_norm": 0.22928255796432495,
      "learning_rate": 2.0453333333333336e-05,
      "loss": 0.0016,
      "step": 88640
    },
    {
      "epoch": 4.728,
      "grad_norm": 0.2162604182958603,
      "learning_rate": 2.045e-05,
      "loss": 0.0023,
      "step": 88650
    },
    {
      "epoch": 4.728533333333333,
      "grad_norm": 0.06765618175268173,
      "learning_rate": 2.0446666666666665e-05,
      "loss": 0.0017,
      "step": 88660
    },
    {
      "epoch": 4.729066666666666,
      "grad_norm": 0.15046902000904083,
      "learning_rate": 2.0443333333333335e-05,
      "loss": 0.002,
      "step": 88670
    },
    {
      "epoch": 4.7296,
      "grad_norm": 0.2988191843032837,
      "learning_rate": 2.044e-05,
      "loss": 0.0018,
      "step": 88680
    },
    {
      "epoch": 4.730133333333333,
      "grad_norm": 0.16317550837993622,
      "learning_rate": 2.0436666666666667e-05,
      "loss": 0.0032,
      "step": 88690
    },
    {
      "epoch": 4.730666666666667,
      "grad_norm": 0.3095683157444,
      "learning_rate": 2.0433333333333336e-05,
      "loss": 0.002,
      "step": 88700
    },
    {
      "epoch": 4.7312,
      "grad_norm": 0.2235831469297409,
      "learning_rate": 2.0430000000000003e-05,
      "loss": 0.0019,
      "step": 88710
    },
    {
      "epoch": 4.731733333333334,
      "grad_norm": 0.36991581320762634,
      "learning_rate": 2.042666666666667e-05,
      "loss": 0.0025,
      "step": 88720
    },
    {
      "epoch": 4.732266666666667,
      "grad_norm": 0.06534701585769653,
      "learning_rate": 2.0423333333333335e-05,
      "loss": 0.002,
      "step": 88730
    },
    {
      "epoch": 4.7328,
      "grad_norm": 0.18364308774471283,
      "learning_rate": 2.042e-05,
      "loss": 0.0017,
      "step": 88740
    },
    {
      "epoch": 4.733333333333333,
      "grad_norm": 0.2303057312965393,
      "learning_rate": 2.0416666666666667e-05,
      "loss": 0.0017,
      "step": 88750
    },
    {
      "epoch": 4.733866666666667,
      "grad_norm": 0.04522612318396568,
      "learning_rate": 2.0413333333333333e-05,
      "loss": 0.0022,
      "step": 88760
    },
    {
      "epoch": 4.7344,
      "grad_norm": 0.3585090637207031,
      "learning_rate": 2.0410000000000003e-05,
      "loss": 0.0014,
      "step": 88770
    },
    {
      "epoch": 4.734933333333333,
      "grad_norm": 0.18820105493068695,
      "learning_rate": 2.040666666666667e-05,
      "loss": 0.0022,
      "step": 88780
    },
    {
      "epoch": 4.7354666666666665,
      "grad_norm": 0.2148953229188919,
      "learning_rate": 2.0403333333333335e-05,
      "loss": 0.0018,
      "step": 88790
    },
    {
      "epoch": 4.736,
      "grad_norm": 0.2671886086463928,
      "learning_rate": 2.04e-05,
      "loss": 0.0019,
      "step": 88800
    },
    {
      "epoch": 4.736533333333333,
      "grad_norm": 0.05382174998521805,
      "learning_rate": 2.0396666666666667e-05,
      "loss": 0.0015,
      "step": 88810
    },
    {
      "epoch": 4.737066666666666,
      "grad_norm": 0.2017473429441452,
      "learning_rate": 2.0393333333333333e-05,
      "loss": 0.0019,
      "step": 88820
    },
    {
      "epoch": 4.7376000000000005,
      "grad_norm": 0.3101624846458435,
      "learning_rate": 2.039e-05,
      "loss": 0.0021,
      "step": 88830
    },
    {
      "epoch": 4.738133333333334,
      "grad_norm": 0.37764593958854675,
      "learning_rate": 2.038666666666667e-05,
      "loss": 0.0024,
      "step": 88840
    },
    {
      "epoch": 4.738666666666667,
      "grad_norm": 0.39379459619522095,
      "learning_rate": 2.0383333333333335e-05,
      "loss": 0.0017,
      "step": 88850
    },
    {
      "epoch": 4.7392,
      "grad_norm": 0.6426466703414917,
      "learning_rate": 2.038e-05,
      "loss": 0.0021,
      "step": 88860
    },
    {
      "epoch": 4.739733333333334,
      "grad_norm": 0.1100044772028923,
      "learning_rate": 2.0376666666666668e-05,
      "loss": 0.0018,
      "step": 88870
    },
    {
      "epoch": 4.740266666666667,
      "grad_norm": 0.14135681092739105,
      "learning_rate": 2.0373333333333334e-05,
      "loss": 0.0016,
      "step": 88880
    },
    {
      "epoch": 4.7408,
      "grad_norm": 0.4478584825992584,
      "learning_rate": 2.037e-05,
      "loss": 0.0018,
      "step": 88890
    },
    {
      "epoch": 4.741333333333333,
      "grad_norm": 0.04740186780691147,
      "learning_rate": 2.0366666666666666e-05,
      "loss": 0.0018,
      "step": 88900
    },
    {
      "epoch": 4.741866666666667,
      "grad_norm": 0.32255423069000244,
      "learning_rate": 2.0363333333333335e-05,
      "loss": 0.002,
      "step": 88910
    },
    {
      "epoch": 4.7424,
      "grad_norm": 0.04124478995800018,
      "learning_rate": 2.036e-05,
      "loss": 0.0021,
      "step": 88920
    },
    {
      "epoch": 4.742933333333333,
      "grad_norm": 0.18823207914829254,
      "learning_rate": 2.0356666666666668e-05,
      "loss": 0.002,
      "step": 88930
    },
    {
      "epoch": 4.7434666666666665,
      "grad_norm": 0.09206730872392654,
      "learning_rate": 2.0353333333333334e-05,
      "loss": 0.0031,
      "step": 88940
    },
    {
      "epoch": 4.744,
      "grad_norm": 0.25128090381622314,
      "learning_rate": 2.035e-05,
      "loss": 0.0015,
      "step": 88950
    },
    {
      "epoch": 4.744533333333333,
      "grad_norm": 0.4601304829120636,
      "learning_rate": 2.0346666666666666e-05,
      "loss": 0.0028,
      "step": 88960
    },
    {
      "epoch": 4.745066666666666,
      "grad_norm": 0.05607868731021881,
      "learning_rate": 2.0343333333333332e-05,
      "loss": 0.0022,
      "step": 88970
    },
    {
      "epoch": 4.7456,
      "grad_norm": 0.048744067549705505,
      "learning_rate": 2.0340000000000002e-05,
      "loss": 0.0022,
      "step": 88980
    },
    {
      "epoch": 4.746133333333333,
      "grad_norm": 0.34994572401046753,
      "learning_rate": 2.0336666666666668e-05,
      "loss": 0.0015,
      "step": 88990
    },
    {
      "epoch": 4.746666666666667,
      "grad_norm": 0.047567564994096756,
      "learning_rate": 2.0333333333333334e-05,
      "loss": 0.0017,
      "step": 89000
    },
    {
      "epoch": 4.7472,
      "grad_norm": 0.07191261649131775,
      "learning_rate": 2.033e-05,
      "loss": 0.0022,
      "step": 89010
    },
    {
      "epoch": 4.747733333333334,
      "grad_norm": 0.2534809112548828,
      "learning_rate": 2.032666666666667e-05,
      "loss": 0.0017,
      "step": 89020
    },
    {
      "epoch": 4.748266666666667,
      "grad_norm": 0.24192696809768677,
      "learning_rate": 2.0323333333333332e-05,
      "loss": 0.002,
      "step": 89030
    },
    {
      "epoch": 4.7488,
      "grad_norm": 0.024351244792342186,
      "learning_rate": 2.032e-05,
      "loss": 0.0013,
      "step": 89040
    },
    {
      "epoch": 4.749333333333333,
      "grad_norm": 0.21362435817718506,
      "learning_rate": 2.0316666666666668e-05,
      "loss": 0.0018,
      "step": 89050
    },
    {
      "epoch": 4.749866666666667,
      "grad_norm": 0.4201669692993164,
      "learning_rate": 2.0313333333333334e-05,
      "loss": 0.0019,
      "step": 89060
    },
    {
      "epoch": 4.7504,
      "grad_norm": 0.18183791637420654,
      "learning_rate": 2.031e-05,
      "loss": 0.0031,
      "step": 89070
    },
    {
      "epoch": 4.750933333333333,
      "grad_norm": 0.21687467396259308,
      "learning_rate": 2.030666666666667e-05,
      "loss": 0.0014,
      "step": 89080
    },
    {
      "epoch": 4.7514666666666665,
      "grad_norm": 0.2301301211118698,
      "learning_rate": 2.0303333333333336e-05,
      "loss": 0.0021,
      "step": 89090
    },
    {
      "epoch": 4.752,
      "grad_norm": 0.15091420710086823,
      "learning_rate": 2.0300000000000002e-05,
      "loss": 0.0022,
      "step": 89100
    },
    {
      "epoch": 4.752533333333333,
      "grad_norm": 0.043298304080963135,
      "learning_rate": 2.0296666666666668e-05,
      "loss": 0.0016,
      "step": 89110
    },
    {
      "epoch": 4.753066666666666,
      "grad_norm": 0.09458401054143906,
      "learning_rate": 2.0293333333333334e-05,
      "loss": 0.0021,
      "step": 89120
    },
    {
      "epoch": 4.7536000000000005,
      "grad_norm": 0.5370059609413147,
      "learning_rate": 2.029e-05,
      "loss": 0.0015,
      "step": 89130
    },
    {
      "epoch": 4.754133333333334,
      "grad_norm": 0.13354742527008057,
      "learning_rate": 2.0286666666666667e-05,
      "loss": 0.0021,
      "step": 89140
    },
    {
      "epoch": 4.754666666666667,
      "grad_norm": 0.26578038930892944,
      "learning_rate": 2.0283333333333336e-05,
      "loss": 0.0014,
      "step": 89150
    },
    {
      "epoch": 4.7552,
      "grad_norm": 0.05848175659775734,
      "learning_rate": 2.0280000000000002e-05,
      "loss": 0.002,
      "step": 89160
    },
    {
      "epoch": 4.755733333333334,
      "grad_norm": 0.28481534123420715,
      "learning_rate": 2.027666666666667e-05,
      "loss": 0.0019,
      "step": 89170
    },
    {
      "epoch": 4.756266666666667,
      "grad_norm": 0.05602302402257919,
      "learning_rate": 2.0273333333333335e-05,
      "loss": 0.0019,
      "step": 89180
    },
    {
      "epoch": 4.7568,
      "grad_norm": 0.17665201425552368,
      "learning_rate": 2.027e-05,
      "loss": 0.0021,
      "step": 89190
    },
    {
      "epoch": 4.757333333333333,
      "grad_norm": 0.12920396029949188,
      "learning_rate": 2.0266666666666667e-05,
      "loss": 0.0018,
      "step": 89200
    },
    {
      "epoch": 4.757866666666667,
      "grad_norm": 0.1499214470386505,
      "learning_rate": 2.0263333333333333e-05,
      "loss": 0.002,
      "step": 89210
    },
    {
      "epoch": 4.7584,
      "grad_norm": 0.1631559431552887,
      "learning_rate": 2.0260000000000003e-05,
      "loss": 0.002,
      "step": 89220
    },
    {
      "epoch": 4.758933333333333,
      "grad_norm": 0.3823780119419098,
      "learning_rate": 2.025666666666667e-05,
      "loss": 0.002,
      "step": 89230
    },
    {
      "epoch": 4.7594666666666665,
      "grad_norm": 0.35953477025032043,
      "learning_rate": 2.0253333333333335e-05,
      "loss": 0.0034,
      "step": 89240
    },
    {
      "epoch": 4.76,
      "grad_norm": 0.3271811604499817,
      "learning_rate": 2.025e-05,
      "loss": 0.0018,
      "step": 89250
    },
    {
      "epoch": 4.760533333333333,
      "grad_norm": 0.29970839619636536,
      "learning_rate": 2.0246666666666667e-05,
      "loss": 0.0022,
      "step": 89260
    },
    {
      "epoch": 4.761066666666666,
      "grad_norm": 0.21369776129722595,
      "learning_rate": 2.0243333333333333e-05,
      "loss": 0.0027,
      "step": 89270
    },
    {
      "epoch": 4.7616,
      "grad_norm": 0.39307600259780884,
      "learning_rate": 2.024e-05,
      "loss": 0.0016,
      "step": 89280
    },
    {
      "epoch": 4.762133333333333,
      "grad_norm": 0.4239940643310547,
      "learning_rate": 2.023666666666667e-05,
      "loss": 0.0015,
      "step": 89290
    },
    {
      "epoch": 4.762666666666667,
      "grad_norm": 0.3288866877555847,
      "learning_rate": 2.0233333333333335e-05,
      "loss": 0.0016,
      "step": 89300
    },
    {
      "epoch": 4.7632,
      "grad_norm": 0.06096668541431427,
      "learning_rate": 2.023e-05,
      "loss": 0.0026,
      "step": 89310
    },
    {
      "epoch": 4.763733333333334,
      "grad_norm": 0.1619865745306015,
      "learning_rate": 2.0226666666666667e-05,
      "loss": 0.0016,
      "step": 89320
    },
    {
      "epoch": 4.764266666666667,
      "grad_norm": 0.7398710250854492,
      "learning_rate": 2.0223333333333333e-05,
      "loss": 0.0019,
      "step": 89330
    },
    {
      "epoch": 4.7648,
      "grad_norm": 0.32637259364128113,
      "learning_rate": 2.022e-05,
      "loss": 0.0021,
      "step": 89340
    },
    {
      "epoch": 4.765333333333333,
      "grad_norm": 0.10544060915708542,
      "learning_rate": 2.0216666666666666e-05,
      "loss": 0.0013,
      "step": 89350
    },
    {
      "epoch": 4.765866666666667,
      "grad_norm": 0.4146493077278137,
      "learning_rate": 2.0213333333333335e-05,
      "loss": 0.0022,
      "step": 89360
    },
    {
      "epoch": 4.7664,
      "grad_norm": 0.34643861651420593,
      "learning_rate": 2.021e-05,
      "loss": 0.0021,
      "step": 89370
    },
    {
      "epoch": 4.766933333333333,
      "grad_norm": 0.1490616500377655,
      "learning_rate": 2.0206666666666667e-05,
      "loss": 0.0024,
      "step": 89380
    },
    {
      "epoch": 4.7674666666666665,
      "grad_norm": 0.05517999827861786,
      "learning_rate": 2.0203333333333334e-05,
      "loss": 0.0018,
      "step": 89390
    },
    {
      "epoch": 4.768,
      "grad_norm": 0.646135151386261,
      "learning_rate": 2.0200000000000003e-05,
      "loss": 0.0025,
      "step": 89400
    },
    {
      "epoch": 4.768533333333333,
      "grad_norm": 0.09804313629865646,
      "learning_rate": 2.0196666666666666e-05,
      "loss": 0.002,
      "step": 89410
    },
    {
      "epoch": 4.769066666666666,
      "grad_norm": 0.13482914865016937,
      "learning_rate": 2.0193333333333332e-05,
      "loss": 0.0017,
      "step": 89420
    },
    {
      "epoch": 4.7696,
      "grad_norm": 0.5152158141136169,
      "learning_rate": 2.019e-05,
      "loss": 0.0017,
      "step": 89430
    },
    {
      "epoch": 4.770133333333334,
      "grad_norm": 0.2257976233959198,
      "learning_rate": 2.0186666666666668e-05,
      "loss": 0.0018,
      "step": 89440
    },
    {
      "epoch": 4.770666666666667,
      "grad_norm": 0.5883901715278625,
      "learning_rate": 2.0183333333333334e-05,
      "loss": 0.0019,
      "step": 89450
    },
    {
      "epoch": 4.7712,
      "grad_norm": 0.2898911237716675,
      "learning_rate": 2.0180000000000003e-05,
      "loss": 0.0015,
      "step": 89460
    },
    {
      "epoch": 4.771733333333334,
      "grad_norm": 0.26551058888435364,
      "learning_rate": 2.017666666666667e-05,
      "loss": 0.0016,
      "step": 89470
    },
    {
      "epoch": 4.772266666666667,
      "grad_norm": 0.1085733026266098,
      "learning_rate": 2.0173333333333332e-05,
      "loss": 0.0015,
      "step": 89480
    },
    {
      "epoch": 4.7728,
      "grad_norm": 0.369404137134552,
      "learning_rate": 2.017e-05,
      "loss": 0.0027,
      "step": 89490
    },
    {
      "epoch": 4.773333333333333,
      "grad_norm": 0.17646759748458862,
      "learning_rate": 2.0166666666666668e-05,
      "loss": 0.0018,
      "step": 89500
    },
    {
      "epoch": 4.773866666666667,
      "grad_norm": 0.3379965126514435,
      "learning_rate": 2.0163333333333334e-05,
      "loss": 0.0023,
      "step": 89510
    },
    {
      "epoch": 4.7744,
      "grad_norm": 0.045292820781469345,
      "learning_rate": 2.016e-05,
      "loss": 0.0025,
      "step": 89520
    },
    {
      "epoch": 4.774933333333333,
      "grad_norm": 0.21056821942329407,
      "learning_rate": 2.015666666666667e-05,
      "loss": 0.0018,
      "step": 89530
    },
    {
      "epoch": 4.7754666666666665,
      "grad_norm": 0.35694923996925354,
      "learning_rate": 2.0153333333333336e-05,
      "loss": 0.0022,
      "step": 89540
    },
    {
      "epoch": 4.776,
      "grad_norm": 0.062424447387456894,
      "learning_rate": 2.0150000000000002e-05,
      "loss": 0.0016,
      "step": 89550
    },
    {
      "epoch": 4.776533333333333,
      "grad_norm": 0.15583603084087372,
      "learning_rate": 2.0146666666666668e-05,
      "loss": 0.0022,
      "step": 89560
    },
    {
      "epoch": 4.777066666666666,
      "grad_norm": 0.3780965209007263,
      "learning_rate": 2.0143333333333334e-05,
      "loss": 0.0027,
      "step": 89570
    },
    {
      "epoch": 4.7776,
      "grad_norm": 0.35660845041275024,
      "learning_rate": 2.014e-05,
      "loss": 0.0014,
      "step": 89580
    },
    {
      "epoch": 4.778133333333333,
      "grad_norm": 0.454141229391098,
      "learning_rate": 2.0136666666666666e-05,
      "loss": 0.0016,
      "step": 89590
    },
    {
      "epoch": 4.778666666666666,
      "grad_norm": 0.11254850029945374,
      "learning_rate": 2.0133333333333336e-05,
      "loss": 0.0016,
      "step": 89600
    },
    {
      "epoch": 4.7792,
      "grad_norm": 0.6948797702789307,
      "learning_rate": 2.0130000000000002e-05,
      "loss": 0.0022,
      "step": 89610
    },
    {
      "epoch": 4.779733333333334,
      "grad_norm": 0.18441225588321686,
      "learning_rate": 2.0126666666666668e-05,
      "loss": 0.0025,
      "step": 89620
    },
    {
      "epoch": 4.780266666666667,
      "grad_norm": 0.18837842345237732,
      "learning_rate": 2.0123333333333334e-05,
      "loss": 0.0023,
      "step": 89630
    },
    {
      "epoch": 4.7808,
      "grad_norm": 0.31884896755218506,
      "learning_rate": 2.012e-05,
      "loss": 0.0024,
      "step": 89640
    },
    {
      "epoch": 4.781333333333333,
      "grad_norm": 0.26909464597702026,
      "learning_rate": 2.0116666666666667e-05,
      "loss": 0.003,
      "step": 89650
    },
    {
      "epoch": 4.781866666666667,
      "grad_norm": 0.5240956544876099,
      "learning_rate": 2.0113333333333333e-05,
      "loss": 0.0022,
      "step": 89660
    },
    {
      "epoch": 4.7824,
      "grad_norm": 0.0817909985780716,
      "learning_rate": 2.0110000000000002e-05,
      "loss": 0.0014,
      "step": 89670
    },
    {
      "epoch": 4.782933333333333,
      "grad_norm": 0.13564957678318024,
      "learning_rate": 2.010666666666667e-05,
      "loss": 0.0018,
      "step": 89680
    },
    {
      "epoch": 4.7834666666666665,
      "grad_norm": 0.06257208436727524,
      "learning_rate": 2.0103333333333335e-05,
      "loss": 0.0013,
      "step": 89690
    },
    {
      "epoch": 4.784,
      "grad_norm": 0.18764184415340424,
      "learning_rate": 2.01e-05,
      "loss": 0.0017,
      "step": 89700
    },
    {
      "epoch": 4.784533333333333,
      "grad_norm": 0.42272815108299255,
      "learning_rate": 2.0096666666666667e-05,
      "loss": 0.0017,
      "step": 89710
    },
    {
      "epoch": 4.785066666666666,
      "grad_norm": 0.12000765651464462,
      "learning_rate": 2.0093333333333333e-05,
      "loss": 0.0026,
      "step": 89720
    },
    {
      "epoch": 4.7856,
      "grad_norm": 0.33256086707115173,
      "learning_rate": 2.009e-05,
      "loss": 0.0016,
      "step": 89730
    },
    {
      "epoch": 4.786133333333334,
      "grad_norm": 0.17716114223003387,
      "learning_rate": 2.008666666666667e-05,
      "loss": 0.0018,
      "step": 89740
    },
    {
      "epoch": 4.786666666666667,
      "grad_norm": 0.360715389251709,
      "learning_rate": 2.0083333333333335e-05,
      "loss": 0.0019,
      "step": 89750
    },
    {
      "epoch": 4.7872,
      "grad_norm": 0.26069989800453186,
      "learning_rate": 2.008e-05,
      "loss": 0.0014,
      "step": 89760
    },
    {
      "epoch": 4.787733333333334,
      "grad_norm": 0.2298370748758316,
      "learning_rate": 2.0076666666666667e-05,
      "loss": 0.0019,
      "step": 89770
    },
    {
      "epoch": 4.788266666666667,
      "grad_norm": 0.32685163617134094,
      "learning_rate": 2.0073333333333337e-05,
      "loss": 0.002,
      "step": 89780
    },
    {
      "epoch": 4.7888,
      "grad_norm": 0.12217026948928833,
      "learning_rate": 2.007e-05,
      "loss": 0.0022,
      "step": 89790
    },
    {
      "epoch": 4.789333333333333,
      "grad_norm": 0.10836759954690933,
      "learning_rate": 2.0066666666666665e-05,
      "loss": 0.0017,
      "step": 89800
    },
    {
      "epoch": 4.789866666666667,
      "grad_norm": 0.24415543675422668,
      "learning_rate": 2.0063333333333335e-05,
      "loss": 0.0012,
      "step": 89810
    },
    {
      "epoch": 4.7904,
      "grad_norm": 0.5796481370925903,
      "learning_rate": 2.006e-05,
      "loss": 0.0019,
      "step": 89820
    },
    {
      "epoch": 4.790933333333333,
      "grad_norm": 0.46349847316741943,
      "learning_rate": 2.0056666666666667e-05,
      "loss": 0.0017,
      "step": 89830
    },
    {
      "epoch": 4.7914666666666665,
      "grad_norm": 0.21380485594272614,
      "learning_rate": 2.0053333333333337e-05,
      "loss": 0.0019,
      "step": 89840
    },
    {
      "epoch": 4.792,
      "grad_norm": 0.15411710739135742,
      "learning_rate": 2.0050000000000003e-05,
      "loss": 0.0018,
      "step": 89850
    },
    {
      "epoch": 4.792533333333333,
      "grad_norm": 0.16865046322345734,
      "learning_rate": 2.0046666666666666e-05,
      "loss": 0.0013,
      "step": 89860
    },
    {
      "epoch": 4.793066666666666,
      "grad_norm": 0.17238910496234894,
      "learning_rate": 2.0043333333333332e-05,
      "loss": 0.0021,
      "step": 89870
    },
    {
      "epoch": 4.7936,
      "grad_norm": 0.271325021982193,
      "learning_rate": 2.004e-05,
      "loss": 0.0015,
      "step": 89880
    },
    {
      "epoch": 4.794133333333333,
      "grad_norm": 0.5573483109474182,
      "learning_rate": 2.0036666666666667e-05,
      "loss": 0.0024,
      "step": 89890
    },
    {
      "epoch": 4.794666666666666,
      "grad_norm": 0.041994765400886536,
      "learning_rate": 2.0033333333333334e-05,
      "loss": 0.0016,
      "step": 89900
    },
    {
      "epoch": 4.7952,
      "grad_norm": 0.11357641220092773,
      "learning_rate": 2.0030000000000003e-05,
      "loss": 0.0017,
      "step": 89910
    },
    {
      "epoch": 4.795733333333334,
      "grad_norm": 0.3784923851490021,
      "learning_rate": 2.002666666666667e-05,
      "loss": 0.0021,
      "step": 89920
    },
    {
      "epoch": 4.796266666666667,
      "grad_norm": 0.035884562879800797,
      "learning_rate": 2.0023333333333335e-05,
      "loss": 0.0013,
      "step": 89930
    },
    {
      "epoch": 4.7968,
      "grad_norm": 0.2072090208530426,
      "learning_rate": 2.002e-05,
      "loss": 0.002,
      "step": 89940
    },
    {
      "epoch": 4.7973333333333334,
      "grad_norm": 0.06899912655353546,
      "learning_rate": 2.0016666666666668e-05,
      "loss": 0.002,
      "step": 89950
    },
    {
      "epoch": 4.797866666666667,
      "grad_norm": 0.06726355105638504,
      "learning_rate": 2.0013333333333334e-05,
      "loss": 0.0019,
      "step": 89960
    },
    {
      "epoch": 4.7984,
      "grad_norm": 0.3233734965324402,
      "learning_rate": 2.001e-05,
      "loss": 0.0016,
      "step": 89970
    },
    {
      "epoch": 4.798933333333333,
      "grad_norm": 0.33246392011642456,
      "learning_rate": 2.000666666666667e-05,
      "loss": 0.0017,
      "step": 89980
    },
    {
      "epoch": 4.7994666666666665,
      "grad_norm": 0.03740676864981651,
      "learning_rate": 2.0003333333333336e-05,
      "loss": 0.0024,
      "step": 89990
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.16132371127605438,
      "learning_rate": 2e-05,
      "loss": 0.0025,
      "step": 90000
    },
    {
      "epoch": 4.800533333333333,
      "grad_norm": 0.5269225239753723,
      "learning_rate": 1.9996666666666668e-05,
      "loss": 0.0022,
      "step": 90010
    },
    {
      "epoch": 4.801066666666666,
      "grad_norm": 0.2452288717031479,
      "learning_rate": 1.9993333333333334e-05,
      "loss": 0.0015,
      "step": 90020
    },
    {
      "epoch": 4.8016,
      "grad_norm": 0.4137701988220215,
      "learning_rate": 1.999e-05,
      "loss": 0.0021,
      "step": 90030
    },
    {
      "epoch": 4.802133333333334,
      "grad_norm": 0.18922275304794312,
      "learning_rate": 1.9986666666666666e-05,
      "loss": 0.0015,
      "step": 90040
    },
    {
      "epoch": 4.802666666666667,
      "grad_norm": 0.3311213552951813,
      "learning_rate": 1.9983333333333336e-05,
      "loss": 0.0029,
      "step": 90050
    },
    {
      "epoch": 4.8032,
      "grad_norm": 0.1590394228696823,
      "learning_rate": 1.9980000000000002e-05,
      "loss": 0.0014,
      "step": 90060
    },
    {
      "epoch": 4.803733333333334,
      "grad_norm": 0.28116941452026367,
      "learning_rate": 1.9976666666666668e-05,
      "loss": 0.0017,
      "step": 90070
    },
    {
      "epoch": 4.804266666666667,
      "grad_norm": 0.10436206310987473,
      "learning_rate": 1.9973333333333334e-05,
      "loss": 0.0016,
      "step": 90080
    },
    {
      "epoch": 4.8048,
      "grad_norm": 0.22475863993167877,
      "learning_rate": 1.997e-05,
      "loss": 0.0022,
      "step": 90090
    },
    {
      "epoch": 4.8053333333333335,
      "grad_norm": 0.41994884610176086,
      "learning_rate": 1.9966666666666666e-05,
      "loss": 0.0012,
      "step": 90100
    },
    {
      "epoch": 4.805866666666667,
      "grad_norm": 0.2523019313812256,
      "learning_rate": 1.9963333333333332e-05,
      "loss": 0.0022,
      "step": 90110
    },
    {
      "epoch": 4.8064,
      "grad_norm": 0.3021932542324066,
      "learning_rate": 1.9960000000000002e-05,
      "loss": 0.0019,
      "step": 90120
    },
    {
      "epoch": 4.806933333333333,
      "grad_norm": 0.27769550681114197,
      "learning_rate": 1.9956666666666668e-05,
      "loss": 0.0015,
      "step": 90130
    },
    {
      "epoch": 4.8074666666666666,
      "grad_norm": 0.16934511065483093,
      "learning_rate": 1.9953333333333334e-05,
      "loss": 0.0027,
      "step": 90140
    },
    {
      "epoch": 4.808,
      "grad_norm": 0.5674548745155334,
      "learning_rate": 1.995e-05,
      "loss": 0.002,
      "step": 90150
    },
    {
      "epoch": 4.808533333333333,
      "grad_norm": 0.05951085314154625,
      "learning_rate": 1.9946666666666667e-05,
      "loss": 0.0022,
      "step": 90160
    },
    {
      "epoch": 4.809066666666666,
      "grad_norm": 0.14224332571029663,
      "learning_rate": 1.9943333333333333e-05,
      "loss": 0.0026,
      "step": 90170
    },
    {
      "epoch": 4.8096,
      "grad_norm": 0.39039334654808044,
      "learning_rate": 1.994e-05,
      "loss": 0.0018,
      "step": 90180
    },
    {
      "epoch": 4.810133333333333,
      "grad_norm": 0.09091205149888992,
      "learning_rate": 1.993666666666667e-05,
      "loss": 0.0018,
      "step": 90190
    },
    {
      "epoch": 4.810666666666666,
      "grad_norm": 0.11323965340852737,
      "learning_rate": 1.9933333333333334e-05,
      "loss": 0.0015,
      "step": 90200
    },
    {
      "epoch": 4.8112,
      "grad_norm": 0.10774823278188705,
      "learning_rate": 1.993e-05,
      "loss": 0.0027,
      "step": 90210
    },
    {
      "epoch": 4.811733333333334,
      "grad_norm": 0.13567906618118286,
      "learning_rate": 1.992666666666667e-05,
      "loss": 0.0024,
      "step": 90220
    },
    {
      "epoch": 4.812266666666667,
      "grad_norm": 0.06745043396949768,
      "learning_rate": 1.9923333333333336e-05,
      "loss": 0.0026,
      "step": 90230
    },
    {
      "epoch": 4.8128,
      "grad_norm": 0.05207356438040733,
      "learning_rate": 1.992e-05,
      "loss": 0.0027,
      "step": 90240
    },
    {
      "epoch": 4.8133333333333335,
      "grad_norm": 0.15349698066711426,
      "learning_rate": 1.9916666666666665e-05,
      "loss": 0.0029,
      "step": 90250
    },
    {
      "epoch": 4.813866666666667,
      "grad_norm": 0.5824820399284363,
      "learning_rate": 1.9913333333333335e-05,
      "loss": 0.0022,
      "step": 90260
    },
    {
      "epoch": 4.8144,
      "grad_norm": 0.0458645224571228,
      "learning_rate": 1.991e-05,
      "loss": 0.002,
      "step": 90270
    },
    {
      "epoch": 4.814933333333333,
      "grad_norm": 0.30552205443382263,
      "learning_rate": 1.9906666666666667e-05,
      "loss": 0.0019,
      "step": 90280
    },
    {
      "epoch": 4.815466666666667,
      "grad_norm": 0.040730755776166916,
      "learning_rate": 1.9903333333333336e-05,
      "loss": 0.0029,
      "step": 90290
    },
    {
      "epoch": 4.816,
      "grad_norm": 0.09144124388694763,
      "learning_rate": 1.9900000000000003e-05,
      "loss": 0.0012,
      "step": 90300
    },
    {
      "epoch": 4.816533333333333,
      "grad_norm": 0.1000169962644577,
      "learning_rate": 1.9896666666666665e-05,
      "loss": 0.0016,
      "step": 90310
    },
    {
      "epoch": 4.817066666666666,
      "grad_norm": 0.19683775305747986,
      "learning_rate": 1.9893333333333335e-05,
      "loss": 0.0022,
      "step": 90320
    },
    {
      "epoch": 4.8176,
      "grad_norm": 0.0629701241850853,
      "learning_rate": 1.989e-05,
      "loss": 0.0012,
      "step": 90330
    },
    {
      "epoch": 4.818133333333334,
      "grad_norm": 0.09382070600986481,
      "learning_rate": 1.9886666666666667e-05,
      "loss": 0.0021,
      "step": 90340
    },
    {
      "epoch": 4.818666666666667,
      "grad_norm": 0.44917911291122437,
      "learning_rate": 1.9883333333333333e-05,
      "loss": 0.0015,
      "step": 90350
    },
    {
      "epoch": 4.8192,
      "grad_norm": 0.09736597537994385,
      "learning_rate": 1.9880000000000003e-05,
      "loss": 0.0021,
      "step": 90360
    },
    {
      "epoch": 4.819733333333334,
      "grad_norm": 0.3061823844909668,
      "learning_rate": 1.987666666666667e-05,
      "loss": 0.0016,
      "step": 90370
    },
    {
      "epoch": 4.820266666666667,
      "grad_norm": 0.18045449256896973,
      "learning_rate": 1.9873333333333335e-05,
      "loss": 0.0015,
      "step": 90380
    },
    {
      "epoch": 4.8208,
      "grad_norm": 0.18304048478603363,
      "learning_rate": 1.987e-05,
      "loss": 0.0019,
      "step": 90390
    },
    {
      "epoch": 4.8213333333333335,
      "grad_norm": 0.11252669990062714,
      "learning_rate": 1.9866666666666667e-05,
      "loss": 0.0015,
      "step": 90400
    },
    {
      "epoch": 4.821866666666667,
      "grad_norm": 0.06704971194267273,
      "learning_rate": 1.9863333333333333e-05,
      "loss": 0.0017,
      "step": 90410
    },
    {
      "epoch": 4.8224,
      "grad_norm": 0.38518527150154114,
      "learning_rate": 1.986e-05,
      "loss": 0.0015,
      "step": 90420
    },
    {
      "epoch": 4.822933333333333,
      "grad_norm": 0.5516108274459839,
      "learning_rate": 1.985666666666667e-05,
      "loss": 0.0021,
      "step": 90430
    },
    {
      "epoch": 4.823466666666667,
      "grad_norm": 0.2577395737171173,
      "learning_rate": 1.9853333333333335e-05,
      "loss": 0.0029,
      "step": 90440
    },
    {
      "epoch": 4.824,
      "grad_norm": 0.3550693392753601,
      "learning_rate": 1.985e-05,
      "loss": 0.0018,
      "step": 90450
    },
    {
      "epoch": 4.824533333333333,
      "grad_norm": 0.06721127778291702,
      "learning_rate": 1.9846666666666668e-05,
      "loss": 0.0025,
      "step": 90460
    },
    {
      "epoch": 4.825066666666666,
      "grad_norm": 0.044851720333099365,
      "learning_rate": 1.9843333333333334e-05,
      "loss": 0.0018,
      "step": 90470
    },
    {
      "epoch": 4.8256,
      "grad_norm": 0.08059720695018768,
      "learning_rate": 1.984e-05,
      "loss": 0.0025,
      "step": 90480
    },
    {
      "epoch": 4.826133333333333,
      "grad_norm": 0.20616818964481354,
      "learning_rate": 1.9836666666666666e-05,
      "loss": 0.0019,
      "step": 90490
    },
    {
      "epoch": 4.826666666666666,
      "grad_norm": 0.15361957252025604,
      "learning_rate": 1.9833333333333335e-05,
      "loss": 0.0021,
      "step": 90500
    },
    {
      "epoch": 4.8272,
      "grad_norm": 0.14987903833389282,
      "learning_rate": 1.983e-05,
      "loss": 0.0013,
      "step": 90510
    },
    {
      "epoch": 4.827733333333334,
      "grad_norm": 0.15922099351882935,
      "learning_rate": 1.9826666666666668e-05,
      "loss": 0.0015,
      "step": 90520
    },
    {
      "epoch": 4.828266666666667,
      "grad_norm": 0.04240958392620087,
      "learning_rate": 1.9823333333333334e-05,
      "loss": 0.0026,
      "step": 90530
    },
    {
      "epoch": 4.8288,
      "grad_norm": 0.16454598307609558,
      "learning_rate": 1.982e-05,
      "loss": 0.0024,
      "step": 90540
    },
    {
      "epoch": 4.8293333333333335,
      "grad_norm": 0.1207362487912178,
      "learning_rate": 1.9816666666666666e-05,
      "loss": 0.0014,
      "step": 90550
    },
    {
      "epoch": 4.829866666666667,
      "grad_norm": 0.3322394788265228,
      "learning_rate": 1.9813333333333332e-05,
      "loss": 0.0024,
      "step": 90560
    },
    {
      "epoch": 4.8304,
      "grad_norm": 0.3636585474014282,
      "learning_rate": 1.9810000000000002e-05,
      "loss": 0.0021,
      "step": 90570
    },
    {
      "epoch": 4.830933333333333,
      "grad_norm": 0.1649230569601059,
      "learning_rate": 1.9806666666666668e-05,
      "loss": 0.0015,
      "step": 90580
    },
    {
      "epoch": 4.831466666666667,
      "grad_norm": 0.2694936692714691,
      "learning_rate": 1.9803333333333334e-05,
      "loss": 0.0018,
      "step": 90590
    },
    {
      "epoch": 4.832,
      "grad_norm": 0.2646248936653137,
      "learning_rate": 1.9800000000000004e-05,
      "loss": 0.0012,
      "step": 90600
    },
    {
      "epoch": 4.832533333333333,
      "grad_norm": 0.036773983389139175,
      "learning_rate": 1.979666666666667e-05,
      "loss": 0.0018,
      "step": 90610
    },
    {
      "epoch": 4.833066666666666,
      "grad_norm": 0.0504632405936718,
      "learning_rate": 1.9793333333333332e-05,
      "loss": 0.0023,
      "step": 90620
    },
    {
      "epoch": 4.8336,
      "grad_norm": 0.2883307933807373,
      "learning_rate": 1.979e-05,
      "loss": 0.0017,
      "step": 90630
    },
    {
      "epoch": 4.834133333333333,
      "grad_norm": 0.2660141587257385,
      "learning_rate": 1.9786666666666668e-05,
      "loss": 0.002,
      "step": 90640
    },
    {
      "epoch": 4.834666666666667,
      "grad_norm": 0.3774701952934265,
      "learning_rate": 1.9783333333333334e-05,
      "loss": 0.0024,
      "step": 90650
    },
    {
      "epoch": 4.8352,
      "grad_norm": 0.27738475799560547,
      "learning_rate": 1.978e-05,
      "loss": 0.0027,
      "step": 90660
    },
    {
      "epoch": 4.835733333333334,
      "grad_norm": 0.25207656621932983,
      "learning_rate": 1.977666666666667e-05,
      "loss": 0.0017,
      "step": 90670
    },
    {
      "epoch": 4.836266666666667,
      "grad_norm": 0.1728232055902481,
      "learning_rate": 1.9773333333333336e-05,
      "loss": 0.0019,
      "step": 90680
    },
    {
      "epoch": 4.8368,
      "grad_norm": 0.31868788599967957,
      "learning_rate": 1.977e-05,
      "loss": 0.0017,
      "step": 90690
    },
    {
      "epoch": 4.8373333333333335,
      "grad_norm": 0.06774488836526871,
      "learning_rate": 1.9766666666666668e-05,
      "loss": 0.0021,
      "step": 90700
    },
    {
      "epoch": 4.837866666666667,
      "grad_norm": 0.4210051894187927,
      "learning_rate": 1.9763333333333334e-05,
      "loss": 0.0022,
      "step": 90710
    },
    {
      "epoch": 4.8384,
      "grad_norm": 0.2118673473596573,
      "learning_rate": 1.976e-05,
      "loss": 0.0016,
      "step": 90720
    },
    {
      "epoch": 4.838933333333333,
      "grad_norm": 0.2716740369796753,
      "learning_rate": 1.9756666666666667e-05,
      "loss": 0.0016,
      "step": 90730
    },
    {
      "epoch": 4.839466666666667,
      "grad_norm": 0.24577859044075012,
      "learning_rate": 1.9753333333333336e-05,
      "loss": 0.002,
      "step": 90740
    },
    {
      "epoch": 4.84,
      "grad_norm": 0.1187494695186615,
      "learning_rate": 1.9750000000000002e-05,
      "loss": 0.0021,
      "step": 90750
    },
    {
      "epoch": 4.840533333333333,
      "grad_norm": 0.16288107633590698,
      "learning_rate": 1.974666666666667e-05,
      "loss": 0.0018,
      "step": 90760
    },
    {
      "epoch": 4.841066666666666,
      "grad_norm": 0.0961182564496994,
      "learning_rate": 1.9743333333333335e-05,
      "loss": 0.002,
      "step": 90770
    },
    {
      "epoch": 4.8416,
      "grad_norm": 0.04187173768877983,
      "learning_rate": 1.974e-05,
      "loss": 0.0015,
      "step": 90780
    },
    {
      "epoch": 4.842133333333333,
      "grad_norm": 0.05293795093894005,
      "learning_rate": 1.9736666666666667e-05,
      "loss": 0.0018,
      "step": 90790
    },
    {
      "epoch": 4.842666666666666,
      "grad_norm": 0.06342118978500366,
      "learning_rate": 1.9733333333333333e-05,
      "loss": 0.0022,
      "step": 90800
    },
    {
      "epoch": 4.8431999999999995,
      "grad_norm": 0.14873403310775757,
      "learning_rate": 1.9730000000000003e-05,
      "loss": 0.0021,
      "step": 90810
    },
    {
      "epoch": 4.843733333333334,
      "grad_norm": 0.17153677344322205,
      "learning_rate": 1.972666666666667e-05,
      "loss": 0.0021,
      "step": 90820
    },
    {
      "epoch": 4.844266666666667,
      "grad_norm": 0.1529831439256668,
      "learning_rate": 1.9723333333333335e-05,
      "loss": 0.0021,
      "step": 90830
    },
    {
      "epoch": 4.8448,
      "grad_norm": 0.33800143003463745,
      "learning_rate": 1.972e-05,
      "loss": 0.0014,
      "step": 90840
    },
    {
      "epoch": 4.8453333333333335,
      "grad_norm": 0.1550324261188507,
      "learning_rate": 1.9716666666666667e-05,
      "loss": 0.0014,
      "step": 90850
    },
    {
      "epoch": 4.845866666666667,
      "grad_norm": 0.09405290335416794,
      "learning_rate": 1.9713333333333333e-05,
      "loss": 0.0016,
      "step": 90860
    },
    {
      "epoch": 4.8464,
      "grad_norm": 0.6616918444633484,
      "learning_rate": 1.971e-05,
      "loss": 0.002,
      "step": 90870
    },
    {
      "epoch": 4.846933333333333,
      "grad_norm": 0.2742466330528259,
      "learning_rate": 1.970666666666667e-05,
      "loss": 0.0019,
      "step": 90880
    },
    {
      "epoch": 4.847466666666667,
      "grad_norm": 0.2993842363357544,
      "learning_rate": 1.9703333333333335e-05,
      "loss": 0.0015,
      "step": 90890
    },
    {
      "epoch": 4.848,
      "grad_norm": 0.05395621061325073,
      "learning_rate": 1.97e-05,
      "loss": 0.0023,
      "step": 90900
    },
    {
      "epoch": 4.848533333333333,
      "grad_norm": 0.07593753933906555,
      "learning_rate": 1.9696666666666667e-05,
      "loss": 0.0019,
      "step": 90910
    },
    {
      "epoch": 4.849066666666666,
      "grad_norm": 0.2822025716304779,
      "learning_rate": 1.9693333333333333e-05,
      "loss": 0.0021,
      "step": 90920
    },
    {
      "epoch": 4.8496,
      "grad_norm": 0.18475499749183655,
      "learning_rate": 1.969e-05,
      "loss": 0.0017,
      "step": 90930
    },
    {
      "epoch": 4.850133333333333,
      "grad_norm": 0.3157312572002411,
      "learning_rate": 1.9686666666666666e-05,
      "loss": 0.0023,
      "step": 90940
    },
    {
      "epoch": 4.850666666666667,
      "grad_norm": 0.3516731858253479,
      "learning_rate": 1.9683333333333335e-05,
      "loss": 0.0021,
      "step": 90950
    },
    {
      "epoch": 4.8512,
      "grad_norm": 0.4936395287513733,
      "learning_rate": 1.968e-05,
      "loss": 0.0019,
      "step": 90960
    },
    {
      "epoch": 4.851733333333334,
      "grad_norm": 0.4047597646713257,
      "learning_rate": 1.9676666666666667e-05,
      "loss": 0.0024,
      "step": 90970
    },
    {
      "epoch": 4.852266666666667,
      "grad_norm": 0.5188755393028259,
      "learning_rate": 1.9673333333333337e-05,
      "loss": 0.0021,
      "step": 90980
    },
    {
      "epoch": 4.8528,
      "grad_norm": 0.35610899329185486,
      "learning_rate": 1.9670000000000003e-05,
      "loss": 0.0018,
      "step": 90990
    },
    {
      "epoch": 4.8533333333333335,
      "grad_norm": 0.15425752103328705,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 0.0014,
      "step": 91000
    },
    {
      "epoch": 4.853866666666667,
      "grad_norm": 0.24879062175750732,
      "learning_rate": 1.9663333333333332e-05,
      "loss": 0.0025,
      "step": 91010
    },
    {
      "epoch": 4.8544,
      "grad_norm": 0.21384844183921814,
      "learning_rate": 1.966e-05,
      "loss": 0.0028,
      "step": 91020
    },
    {
      "epoch": 4.854933333333333,
      "grad_norm": 0.3989838659763336,
      "learning_rate": 1.9656666666666668e-05,
      "loss": 0.0019,
      "step": 91030
    },
    {
      "epoch": 4.855466666666667,
      "grad_norm": 0.06990722566843033,
      "learning_rate": 1.9653333333333334e-05,
      "loss": 0.0018,
      "step": 91040
    },
    {
      "epoch": 4.856,
      "grad_norm": 0.18490929901599884,
      "learning_rate": 1.9650000000000003e-05,
      "loss": 0.0018,
      "step": 91050
    },
    {
      "epoch": 4.856533333333333,
      "grad_norm": 0.3349374234676361,
      "learning_rate": 1.964666666666667e-05,
      "loss": 0.0018,
      "step": 91060
    },
    {
      "epoch": 4.857066666666666,
      "grad_norm": 0.07900494337081909,
      "learning_rate": 1.9643333333333332e-05,
      "loss": 0.0023,
      "step": 91070
    },
    {
      "epoch": 4.8576,
      "grad_norm": 0.1806216984987259,
      "learning_rate": 1.9640000000000002e-05,
      "loss": 0.0021,
      "step": 91080
    },
    {
      "epoch": 4.858133333333333,
      "grad_norm": 0.16985008120536804,
      "learning_rate": 1.9636666666666668e-05,
      "loss": 0.0024,
      "step": 91090
    },
    {
      "epoch": 4.858666666666666,
      "grad_norm": 0.06595706939697266,
      "learning_rate": 1.9633333333333334e-05,
      "loss": 0.0018,
      "step": 91100
    },
    {
      "epoch": 4.8591999999999995,
      "grad_norm": 0.09262876957654953,
      "learning_rate": 1.963e-05,
      "loss": 0.0016,
      "step": 91110
    },
    {
      "epoch": 4.859733333333334,
      "grad_norm": 0.2495674043893814,
      "learning_rate": 1.962666666666667e-05,
      "loss": 0.0015,
      "step": 91120
    },
    {
      "epoch": 4.860266666666667,
      "grad_norm": 0.4044032394886017,
      "learning_rate": 1.9623333333333336e-05,
      "loss": 0.0024,
      "step": 91130
    },
    {
      "epoch": 4.8608,
      "grad_norm": 0.21959418058395386,
      "learning_rate": 1.9620000000000002e-05,
      "loss": 0.0014,
      "step": 91140
    },
    {
      "epoch": 4.8613333333333335,
      "grad_norm": 0.024165332317352295,
      "learning_rate": 1.9616666666666668e-05,
      "loss": 0.0015,
      "step": 91150
    },
    {
      "epoch": 4.861866666666667,
      "grad_norm": 0.3631645143032074,
      "learning_rate": 1.9613333333333334e-05,
      "loss": 0.0016,
      "step": 91160
    },
    {
      "epoch": 4.8624,
      "grad_norm": 0.43030914664268494,
      "learning_rate": 1.961e-05,
      "loss": 0.0016,
      "step": 91170
    },
    {
      "epoch": 4.862933333333333,
      "grad_norm": 0.157918319106102,
      "learning_rate": 1.9606666666666666e-05,
      "loss": 0.0013,
      "step": 91180
    },
    {
      "epoch": 4.863466666666667,
      "grad_norm": 0.3310827314853668,
      "learning_rate": 1.9603333333333336e-05,
      "loss": 0.002,
      "step": 91190
    },
    {
      "epoch": 4.864,
      "grad_norm": 0.30070143938064575,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 0.0019,
      "step": 91200
    },
    {
      "epoch": 4.864533333333333,
      "grad_norm": 0.35189729928970337,
      "learning_rate": 1.9596666666666668e-05,
      "loss": 0.0018,
      "step": 91210
    },
    {
      "epoch": 4.865066666666666,
      "grad_norm": 0.406643271446228,
      "learning_rate": 1.9593333333333334e-05,
      "loss": 0.0016,
      "step": 91220
    },
    {
      "epoch": 4.8656,
      "grad_norm": 0.12835344672203064,
      "learning_rate": 1.959e-05,
      "loss": 0.0011,
      "step": 91230
    },
    {
      "epoch": 4.866133333333333,
      "grad_norm": 0.3626936078071594,
      "learning_rate": 1.9586666666666667e-05,
      "loss": 0.0013,
      "step": 91240
    },
    {
      "epoch": 4.866666666666667,
      "grad_norm": 0.24008040130138397,
      "learning_rate": 1.9583333333333333e-05,
      "loss": 0.0021,
      "step": 91250
    },
    {
      "epoch": 4.8672,
      "grad_norm": 0.07529888302087784,
      "learning_rate": 1.9580000000000002e-05,
      "loss": 0.002,
      "step": 91260
    },
    {
      "epoch": 4.867733333333334,
      "grad_norm": 0.07937190681695938,
      "learning_rate": 1.957666666666667e-05,
      "loss": 0.0018,
      "step": 91270
    },
    {
      "epoch": 4.868266666666667,
      "grad_norm": 0.05150402709841728,
      "learning_rate": 1.9573333333333335e-05,
      "loss": 0.0023,
      "step": 91280
    },
    {
      "epoch": 4.8688,
      "grad_norm": 0.2830810546875,
      "learning_rate": 1.957e-05,
      "loss": 0.0018,
      "step": 91290
    },
    {
      "epoch": 4.8693333333333335,
      "grad_norm": 0.2128332406282425,
      "learning_rate": 1.9566666666666667e-05,
      "loss": 0.002,
      "step": 91300
    },
    {
      "epoch": 4.869866666666667,
      "grad_norm": 0.46477144956588745,
      "learning_rate": 1.9563333333333333e-05,
      "loss": 0.0019,
      "step": 91310
    },
    {
      "epoch": 4.8704,
      "grad_norm": 0.16956840455532074,
      "learning_rate": 1.956e-05,
      "loss": 0.0017,
      "step": 91320
    },
    {
      "epoch": 4.870933333333333,
      "grad_norm": 0.3982757329940796,
      "learning_rate": 1.955666666666667e-05,
      "loss": 0.0021,
      "step": 91330
    },
    {
      "epoch": 4.871466666666667,
      "grad_norm": 0.22054557502269745,
      "learning_rate": 1.9553333333333335e-05,
      "loss": 0.0018,
      "step": 91340
    },
    {
      "epoch": 4.872,
      "grad_norm": 0.058298442512750626,
      "learning_rate": 1.955e-05,
      "loss": 0.0021,
      "step": 91350
    },
    {
      "epoch": 4.872533333333333,
      "grad_norm": 0.23724587261676788,
      "learning_rate": 1.9546666666666667e-05,
      "loss": 0.0012,
      "step": 91360
    },
    {
      "epoch": 4.873066666666666,
      "grad_norm": 0.3387935161590576,
      "learning_rate": 1.9543333333333333e-05,
      "loss": 0.0017,
      "step": 91370
    },
    {
      "epoch": 4.8736,
      "grad_norm": 0.05603410303592682,
      "learning_rate": 1.954e-05,
      "loss": 0.0023,
      "step": 91380
    },
    {
      "epoch": 4.874133333333333,
      "grad_norm": 0.5813132524490356,
      "learning_rate": 1.9536666666666665e-05,
      "loss": 0.0017,
      "step": 91390
    },
    {
      "epoch": 4.874666666666666,
      "grad_norm": 0.04271376505494118,
      "learning_rate": 1.9533333333333335e-05,
      "loss": 0.0023,
      "step": 91400
    },
    {
      "epoch": 4.8751999999999995,
      "grad_norm": 0.12371170520782471,
      "learning_rate": 1.953e-05,
      "loss": 0.0013,
      "step": 91410
    },
    {
      "epoch": 4.875733333333334,
      "grad_norm": 0.394234836101532,
      "learning_rate": 1.9526666666666667e-05,
      "loss": 0.0022,
      "step": 91420
    },
    {
      "epoch": 4.876266666666667,
      "grad_norm": 0.3671663701534271,
      "learning_rate": 1.9523333333333337e-05,
      "loss": 0.0019,
      "step": 91430
    },
    {
      "epoch": 4.8768,
      "grad_norm": 0.06726831942796707,
      "learning_rate": 1.9520000000000003e-05,
      "loss": 0.003,
      "step": 91440
    },
    {
      "epoch": 4.8773333333333335,
      "grad_norm": 0.48752012848854065,
      "learning_rate": 1.9516666666666666e-05,
      "loss": 0.0018,
      "step": 91450
    },
    {
      "epoch": 4.877866666666667,
      "grad_norm": 0.24555574357509613,
      "learning_rate": 1.9513333333333335e-05,
      "loss": 0.0019,
      "step": 91460
    },
    {
      "epoch": 4.8784,
      "grad_norm": 0.5167382955551147,
      "learning_rate": 1.951e-05,
      "loss": 0.0022,
      "step": 91470
    },
    {
      "epoch": 4.878933333333333,
      "grad_norm": 0.030974246561527252,
      "learning_rate": 1.9506666666666667e-05,
      "loss": 0.0023,
      "step": 91480
    },
    {
      "epoch": 4.879466666666667,
      "grad_norm": 0.09664890915155411,
      "learning_rate": 1.9503333333333334e-05,
      "loss": 0.0023,
      "step": 91490
    },
    {
      "epoch": 4.88,
      "grad_norm": 0.44812971353530884,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 0.002,
      "step": 91500
    },
    {
      "epoch": 4.880533333333333,
      "grad_norm": 0.16410672664642334,
      "learning_rate": 1.949666666666667e-05,
      "loss": 0.0013,
      "step": 91510
    },
    {
      "epoch": 4.881066666666666,
      "grad_norm": 0.1880500167608261,
      "learning_rate": 1.9493333333333332e-05,
      "loss": 0.0021,
      "step": 91520
    },
    {
      "epoch": 4.8816,
      "grad_norm": 0.09677937626838684,
      "learning_rate": 1.949e-05,
      "loss": 0.0024,
      "step": 91530
    },
    {
      "epoch": 4.882133333333333,
      "grad_norm": 0.3123518228530884,
      "learning_rate": 1.9486666666666668e-05,
      "loss": 0.002,
      "step": 91540
    },
    {
      "epoch": 4.882666666666667,
      "grad_norm": 0.3176248073577881,
      "learning_rate": 1.9483333333333334e-05,
      "loss": 0.0012,
      "step": 91550
    },
    {
      "epoch": 4.8832,
      "grad_norm": 0.5193802118301392,
      "learning_rate": 1.948e-05,
      "loss": 0.0015,
      "step": 91560
    },
    {
      "epoch": 4.883733333333334,
      "grad_norm": 0.08521416783332825,
      "learning_rate": 1.947666666666667e-05,
      "loss": 0.0021,
      "step": 91570
    },
    {
      "epoch": 4.884266666666667,
      "grad_norm": 0.3085513412952423,
      "learning_rate": 1.9473333333333335e-05,
      "loss": 0.0018,
      "step": 91580
    },
    {
      "epoch": 4.8848,
      "grad_norm": 0.09523646533489227,
      "learning_rate": 1.947e-05,
      "loss": 0.0016,
      "step": 91590
    },
    {
      "epoch": 4.8853333333333335,
      "grad_norm": 0.2474316954612732,
      "learning_rate": 1.9466666666666668e-05,
      "loss": 0.0021,
      "step": 91600
    },
    {
      "epoch": 4.885866666666667,
      "grad_norm": 0.18456751108169556,
      "learning_rate": 1.9463333333333334e-05,
      "loss": 0.0023,
      "step": 91610
    },
    {
      "epoch": 4.8864,
      "grad_norm": 0.36045241355895996,
      "learning_rate": 1.946e-05,
      "loss": 0.0029,
      "step": 91620
    },
    {
      "epoch": 4.886933333333333,
      "grad_norm": 0.4231676757335663,
      "learning_rate": 1.9456666666666666e-05,
      "loss": 0.0012,
      "step": 91630
    },
    {
      "epoch": 4.887466666666667,
      "grad_norm": 0.3416139483451843,
      "learning_rate": 1.9453333333333336e-05,
      "loss": 0.0025,
      "step": 91640
    },
    {
      "epoch": 4.888,
      "grad_norm": 0.3015632629394531,
      "learning_rate": 1.9450000000000002e-05,
      "loss": 0.0015,
      "step": 91650
    },
    {
      "epoch": 4.888533333333333,
      "grad_norm": 0.089594267308712,
      "learning_rate": 1.9446666666666668e-05,
      "loss": 0.0016,
      "step": 91660
    },
    {
      "epoch": 4.8890666666666664,
      "grad_norm": 0.15726150572299957,
      "learning_rate": 1.9443333333333334e-05,
      "loss": 0.0017,
      "step": 91670
    },
    {
      "epoch": 4.8896,
      "grad_norm": 0.23986850678920746,
      "learning_rate": 1.944e-05,
      "loss": 0.0016,
      "step": 91680
    },
    {
      "epoch": 4.890133333333333,
      "grad_norm": 0.29360759258270264,
      "learning_rate": 1.9436666666666666e-05,
      "loss": 0.0023,
      "step": 91690
    },
    {
      "epoch": 4.890666666666666,
      "grad_norm": 0.1318877786397934,
      "learning_rate": 1.9433333333333332e-05,
      "loss": 0.0015,
      "step": 91700
    },
    {
      "epoch": 4.8911999999999995,
      "grad_norm": 0.18307501077651978,
      "learning_rate": 1.9430000000000002e-05,
      "loss": 0.0034,
      "step": 91710
    },
    {
      "epoch": 4.891733333333334,
      "grad_norm": 0.22241568565368652,
      "learning_rate": 1.9426666666666668e-05,
      "loss": 0.0012,
      "step": 91720
    },
    {
      "epoch": 4.892266666666667,
      "grad_norm": 0.09232326596975327,
      "learning_rate": 1.9423333333333334e-05,
      "loss": 0.0017,
      "step": 91730
    },
    {
      "epoch": 4.8928,
      "grad_norm": 0.29851776361465454,
      "learning_rate": 1.942e-05,
      "loss": 0.0026,
      "step": 91740
    },
    {
      "epoch": 4.8933333333333335,
      "grad_norm": 0.05214899405837059,
      "learning_rate": 1.9416666666666667e-05,
      "loss": 0.002,
      "step": 91750
    },
    {
      "epoch": 4.893866666666667,
      "grad_norm": 0.06069637089967728,
      "learning_rate": 1.9413333333333333e-05,
      "loss": 0.0014,
      "step": 91760
    },
    {
      "epoch": 4.8944,
      "grad_norm": 0.33372172713279724,
      "learning_rate": 1.941e-05,
      "loss": 0.0021,
      "step": 91770
    },
    {
      "epoch": 4.894933333333333,
      "grad_norm": 0.09458045661449432,
      "learning_rate": 1.940666666666667e-05,
      "loss": 0.0021,
      "step": 91780
    },
    {
      "epoch": 4.895466666666667,
      "grad_norm": 0.33959299325942993,
      "learning_rate": 1.9403333333333334e-05,
      "loss": 0.0018,
      "step": 91790
    },
    {
      "epoch": 4.896,
      "grad_norm": 0.12867121398448944,
      "learning_rate": 1.94e-05,
      "loss": 0.0022,
      "step": 91800
    },
    {
      "epoch": 4.896533333333333,
      "grad_norm": 0.31963270902633667,
      "learning_rate": 1.939666666666667e-05,
      "loss": 0.0012,
      "step": 91810
    },
    {
      "epoch": 4.8970666666666665,
      "grad_norm": 0.18618613481521606,
      "learning_rate": 1.9393333333333336e-05,
      "loss": 0.002,
      "step": 91820
    },
    {
      "epoch": 4.8976,
      "grad_norm": 0.3954605162143707,
      "learning_rate": 1.939e-05,
      "loss": 0.002,
      "step": 91830
    },
    {
      "epoch": 4.898133333333333,
      "grad_norm": 0.3192700445652008,
      "learning_rate": 1.938666666666667e-05,
      "loss": 0.0018,
      "step": 91840
    },
    {
      "epoch": 4.898666666666666,
      "grad_norm": 0.33034786581993103,
      "learning_rate": 1.9383333333333335e-05,
      "loss": 0.0025,
      "step": 91850
    },
    {
      "epoch": 4.8992,
      "grad_norm": 0.09982885420322418,
      "learning_rate": 1.938e-05,
      "loss": 0.0019,
      "step": 91860
    },
    {
      "epoch": 4.899733333333334,
      "grad_norm": 0.2717623710632324,
      "learning_rate": 1.9376666666666667e-05,
      "loss": 0.0017,
      "step": 91870
    },
    {
      "epoch": 4.900266666666667,
      "grad_norm": 0.09645255655050278,
      "learning_rate": 1.9373333333333336e-05,
      "loss": 0.0022,
      "step": 91880
    },
    {
      "epoch": 4.9008,
      "grad_norm": 0.2971900701522827,
      "learning_rate": 1.9370000000000003e-05,
      "loss": 0.0014,
      "step": 91890
    },
    {
      "epoch": 4.9013333333333335,
      "grad_norm": 0.12454069405794144,
      "learning_rate": 1.9366666666666665e-05,
      "loss": 0.0019,
      "step": 91900
    },
    {
      "epoch": 4.901866666666667,
      "grad_norm": 0.37913456559181213,
      "learning_rate": 1.9363333333333335e-05,
      "loss": 0.0027,
      "step": 91910
    },
    {
      "epoch": 4.9024,
      "grad_norm": 0.18817850947380066,
      "learning_rate": 1.936e-05,
      "loss": 0.0015,
      "step": 91920
    },
    {
      "epoch": 4.902933333333333,
      "grad_norm": 0.11143362522125244,
      "learning_rate": 1.9356666666666667e-05,
      "loss": 0.0016,
      "step": 91930
    },
    {
      "epoch": 4.903466666666667,
      "grad_norm": 0.15674588084220886,
      "learning_rate": 1.9353333333333333e-05,
      "loss": 0.0025,
      "step": 91940
    },
    {
      "epoch": 4.904,
      "grad_norm": 0.0530218780040741,
      "learning_rate": 1.9350000000000003e-05,
      "loss": 0.002,
      "step": 91950
    },
    {
      "epoch": 4.904533333333333,
      "grad_norm": 0.5780177712440491,
      "learning_rate": 1.934666666666667e-05,
      "loss": 0.0027,
      "step": 91960
    },
    {
      "epoch": 4.9050666666666665,
      "grad_norm": 0.07081202417612076,
      "learning_rate": 1.9343333333333335e-05,
      "loss": 0.003,
      "step": 91970
    },
    {
      "epoch": 4.9056,
      "grad_norm": 0.07906468212604523,
      "learning_rate": 1.934e-05,
      "loss": 0.0015,
      "step": 91980
    },
    {
      "epoch": 4.906133333333333,
      "grad_norm": 0.269168883562088,
      "learning_rate": 1.9336666666666667e-05,
      "loss": 0.0019,
      "step": 91990
    },
    {
      "epoch": 4.906666666666666,
      "grad_norm": 0.2947779595851898,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 0.0016,
      "step": 92000
    },
    {
      "epoch": 4.9072,
      "grad_norm": 0.451434850692749,
      "learning_rate": 1.933e-05,
      "loss": 0.0015,
      "step": 92010
    },
    {
      "epoch": 4.907733333333333,
      "grad_norm": 0.1288757473230362,
      "learning_rate": 1.932666666666667e-05,
      "loss": 0.0016,
      "step": 92020
    },
    {
      "epoch": 4.908266666666667,
      "grad_norm": 0.22502464056015015,
      "learning_rate": 1.9323333333333335e-05,
      "loss": 0.0016,
      "step": 92030
    },
    {
      "epoch": 4.9088,
      "grad_norm": 0.29765063524246216,
      "learning_rate": 1.932e-05,
      "loss": 0.0023,
      "step": 92040
    },
    {
      "epoch": 4.9093333333333335,
      "grad_norm": 0.18215623497962952,
      "learning_rate": 1.9316666666666668e-05,
      "loss": 0.002,
      "step": 92050
    },
    {
      "epoch": 4.909866666666667,
      "grad_norm": 0.33596694469451904,
      "learning_rate": 1.9313333333333334e-05,
      "loss": 0.0015,
      "step": 92060
    },
    {
      "epoch": 4.9104,
      "grad_norm": 0.38439515233039856,
      "learning_rate": 1.931e-05,
      "loss": 0.0017,
      "step": 92070
    },
    {
      "epoch": 4.910933333333333,
      "grad_norm": 0.13199535012245178,
      "learning_rate": 1.9306666666666666e-05,
      "loss": 0.0016,
      "step": 92080
    },
    {
      "epoch": 4.911466666666667,
      "grad_norm": 0.4326167702674866,
      "learning_rate": 1.9303333333333335e-05,
      "loss": 0.0019,
      "step": 92090
    },
    {
      "epoch": 4.912,
      "grad_norm": 0.3226240575313568,
      "learning_rate": 1.93e-05,
      "loss": 0.0024,
      "step": 92100
    },
    {
      "epoch": 4.912533333333333,
      "grad_norm": 0.24362820386886597,
      "learning_rate": 1.9296666666666668e-05,
      "loss": 0.0014,
      "step": 92110
    },
    {
      "epoch": 4.9130666666666665,
      "grad_norm": 0.33387064933776855,
      "learning_rate": 1.9293333333333334e-05,
      "loss": 0.0014,
      "step": 92120
    },
    {
      "epoch": 4.9136,
      "grad_norm": 0.3874594271183014,
      "learning_rate": 1.929e-05,
      "loss": 0.0014,
      "step": 92130
    },
    {
      "epoch": 4.914133333333333,
      "grad_norm": 0.0660446286201477,
      "learning_rate": 1.9286666666666666e-05,
      "loss": 0.0019,
      "step": 92140
    },
    {
      "epoch": 4.914666666666666,
      "grad_norm": 0.23213209211826324,
      "learning_rate": 1.9283333333333332e-05,
      "loss": 0.0023,
      "step": 92150
    },
    {
      "epoch": 4.9152000000000005,
      "grad_norm": 0.1468898355960846,
      "learning_rate": 1.9280000000000002e-05,
      "loss": 0.0022,
      "step": 92160
    },
    {
      "epoch": 4.915733333333334,
      "grad_norm": 0.0689259022474289,
      "learning_rate": 1.9276666666666668e-05,
      "loss": 0.003,
      "step": 92170
    },
    {
      "epoch": 4.916266666666667,
      "grad_norm": 0.06398195028305054,
      "learning_rate": 1.9273333333333334e-05,
      "loss": 0.0024,
      "step": 92180
    },
    {
      "epoch": 4.9168,
      "grad_norm": 0.28136199712753296,
      "learning_rate": 1.9270000000000004e-05,
      "loss": 0.0019,
      "step": 92190
    },
    {
      "epoch": 4.917333333333334,
      "grad_norm": 0.19272252917289734,
      "learning_rate": 1.926666666666667e-05,
      "loss": 0.0012,
      "step": 92200
    },
    {
      "epoch": 4.917866666666667,
      "grad_norm": 0.12184736132621765,
      "learning_rate": 1.9263333333333332e-05,
      "loss": 0.0026,
      "step": 92210
    },
    {
      "epoch": 4.9184,
      "grad_norm": 0.09944671392440796,
      "learning_rate": 1.9260000000000002e-05,
      "loss": 0.0017,
      "step": 92220
    },
    {
      "epoch": 4.918933333333333,
      "grad_norm": 0.12935896217823029,
      "learning_rate": 1.9256666666666668e-05,
      "loss": 0.0022,
      "step": 92230
    },
    {
      "epoch": 4.919466666666667,
      "grad_norm": 0.2703341245651245,
      "learning_rate": 1.9253333333333334e-05,
      "loss": 0.0028,
      "step": 92240
    },
    {
      "epoch": 4.92,
      "grad_norm": 0.26976701617240906,
      "learning_rate": 1.925e-05,
      "loss": 0.0024,
      "step": 92250
    },
    {
      "epoch": 4.920533333333333,
      "grad_norm": 0.18499766290187836,
      "learning_rate": 1.924666666666667e-05,
      "loss": 0.0024,
      "step": 92260
    },
    {
      "epoch": 4.9210666666666665,
      "grad_norm": 0.33016717433929443,
      "learning_rate": 1.9243333333333336e-05,
      "loss": 0.0018,
      "step": 92270
    },
    {
      "epoch": 4.9216,
      "grad_norm": 0.30397316813468933,
      "learning_rate": 1.924e-05,
      "loss": 0.0025,
      "step": 92280
    },
    {
      "epoch": 4.922133333333333,
      "grad_norm": 0.10709524154663086,
      "learning_rate": 1.9236666666666668e-05,
      "loss": 0.0015,
      "step": 92290
    },
    {
      "epoch": 4.922666666666666,
      "grad_norm": 0.049717336893081665,
      "learning_rate": 1.9233333333333334e-05,
      "loss": 0.0035,
      "step": 92300
    },
    {
      "epoch": 4.9232,
      "grad_norm": 0.12188771367073059,
      "learning_rate": 1.923e-05,
      "loss": 0.0021,
      "step": 92310
    },
    {
      "epoch": 4.923733333333333,
      "grad_norm": 0.044055040925741196,
      "learning_rate": 1.9226666666666667e-05,
      "loss": 0.0013,
      "step": 92320
    },
    {
      "epoch": 4.924266666666667,
      "grad_norm": 0.11199556291103363,
      "learning_rate": 1.9223333333333336e-05,
      "loss": 0.0017,
      "step": 92330
    },
    {
      "epoch": 4.9248,
      "grad_norm": 0.5024791955947876,
      "learning_rate": 1.9220000000000002e-05,
      "loss": 0.0019,
      "step": 92340
    },
    {
      "epoch": 4.925333333333334,
      "grad_norm": 0.1535826325416565,
      "learning_rate": 1.921666666666667e-05,
      "loss": 0.0016,
      "step": 92350
    },
    {
      "epoch": 4.925866666666667,
      "grad_norm": 0.3777942657470703,
      "learning_rate": 1.9213333333333335e-05,
      "loss": 0.0024,
      "step": 92360
    },
    {
      "epoch": 4.9264,
      "grad_norm": 0.04866761714220047,
      "learning_rate": 1.921e-05,
      "loss": 0.002,
      "step": 92370
    },
    {
      "epoch": 4.926933333333333,
      "grad_norm": 0.12914791703224182,
      "learning_rate": 1.9206666666666667e-05,
      "loss": 0.0018,
      "step": 92380
    },
    {
      "epoch": 4.927466666666667,
      "grad_norm": 0.15448805689811707,
      "learning_rate": 1.9203333333333333e-05,
      "loss": 0.0014,
      "step": 92390
    },
    {
      "epoch": 4.928,
      "grad_norm": 0.24119636416435242,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.0029,
      "step": 92400
    },
    {
      "epoch": 4.928533333333333,
      "grad_norm": 0.28316202759742737,
      "learning_rate": 1.919666666666667e-05,
      "loss": 0.0024,
      "step": 92410
    },
    {
      "epoch": 4.9290666666666665,
      "grad_norm": 0.44058385491371155,
      "learning_rate": 1.9193333333333335e-05,
      "loss": 0.0023,
      "step": 92420
    },
    {
      "epoch": 4.9296,
      "grad_norm": 0.6400454044342041,
      "learning_rate": 1.919e-05,
      "loss": 0.0015,
      "step": 92430
    },
    {
      "epoch": 4.930133333333333,
      "grad_norm": 0.09105352312326431,
      "learning_rate": 1.9186666666666667e-05,
      "loss": 0.0017,
      "step": 92440
    },
    {
      "epoch": 4.930666666666666,
      "grad_norm": 0.36152833700180054,
      "learning_rate": 1.9183333333333333e-05,
      "loss": 0.0022,
      "step": 92450
    },
    {
      "epoch": 4.9312000000000005,
      "grad_norm": 0.1548180729150772,
      "learning_rate": 1.918e-05,
      "loss": 0.0013,
      "step": 92460
    },
    {
      "epoch": 4.931733333333334,
      "grad_norm": 0.4875885844230652,
      "learning_rate": 1.917666666666667e-05,
      "loss": 0.0018,
      "step": 92470
    },
    {
      "epoch": 4.932266666666667,
      "grad_norm": 0.43368375301361084,
      "learning_rate": 1.9173333333333335e-05,
      "loss": 0.0023,
      "step": 92480
    },
    {
      "epoch": 4.9328,
      "grad_norm": 0.13096024096012115,
      "learning_rate": 1.917e-05,
      "loss": 0.0021,
      "step": 92490
    },
    {
      "epoch": 4.933333333333334,
      "grad_norm": 0.2253447026014328,
      "learning_rate": 1.9166666666666667e-05,
      "loss": 0.0013,
      "step": 92500
    },
    {
      "epoch": 4.933866666666667,
      "grad_norm": 0.25184187293052673,
      "learning_rate": 1.9163333333333333e-05,
      "loss": 0.0024,
      "step": 92510
    },
    {
      "epoch": 4.9344,
      "grad_norm": 0.32998576760292053,
      "learning_rate": 1.916e-05,
      "loss": 0.0016,
      "step": 92520
    },
    {
      "epoch": 4.934933333333333,
      "grad_norm": 0.08469472825527191,
      "learning_rate": 1.9156666666666666e-05,
      "loss": 0.0018,
      "step": 92530
    },
    {
      "epoch": 4.935466666666667,
      "grad_norm": 0.2110234647989273,
      "learning_rate": 1.9153333333333335e-05,
      "loss": 0.0018,
      "step": 92540
    },
    {
      "epoch": 4.936,
      "grad_norm": 0.10448189824819565,
      "learning_rate": 1.915e-05,
      "loss": 0.0018,
      "step": 92550
    },
    {
      "epoch": 4.936533333333333,
      "grad_norm": 0.1572723537683487,
      "learning_rate": 1.9146666666666667e-05,
      "loss": 0.0014,
      "step": 92560
    },
    {
      "epoch": 4.9370666666666665,
      "grad_norm": 0.0942501574754715,
      "learning_rate": 1.9143333333333337e-05,
      "loss": 0.0018,
      "step": 92570
    },
    {
      "epoch": 4.9376,
      "grad_norm": 0.25461429357528687,
      "learning_rate": 1.914e-05,
      "loss": 0.0018,
      "step": 92580
    },
    {
      "epoch": 4.938133333333333,
      "grad_norm": 0.45750054717063904,
      "learning_rate": 1.9136666666666666e-05,
      "loss": 0.0015,
      "step": 92590
    },
    {
      "epoch": 4.938666666666666,
      "grad_norm": 0.3294839859008789,
      "learning_rate": 1.9133333333333332e-05,
      "loss": 0.0027,
      "step": 92600
    },
    {
      "epoch": 4.9392,
      "grad_norm": 0.050158821046352386,
      "learning_rate": 1.913e-05,
      "loss": 0.0017,
      "step": 92610
    },
    {
      "epoch": 4.939733333333333,
      "grad_norm": 0.5438222289085388,
      "learning_rate": 1.9126666666666668e-05,
      "loss": 0.0025,
      "step": 92620
    },
    {
      "epoch": 4.940266666666667,
      "grad_norm": 0.30688607692718506,
      "learning_rate": 1.9123333333333334e-05,
      "loss": 0.002,
      "step": 92630
    },
    {
      "epoch": 4.9408,
      "grad_norm": 0.2104349136352539,
      "learning_rate": 1.9120000000000003e-05,
      "loss": 0.0019,
      "step": 92640
    },
    {
      "epoch": 4.941333333333334,
      "grad_norm": 0.15042303502559662,
      "learning_rate": 1.911666666666667e-05,
      "loss": 0.0019,
      "step": 92650
    },
    {
      "epoch": 4.941866666666667,
      "grad_norm": 0.1662530153989792,
      "learning_rate": 1.9113333333333332e-05,
      "loss": 0.0015,
      "step": 92660
    },
    {
      "epoch": 4.9424,
      "grad_norm": 0.4593481719493866,
      "learning_rate": 1.911e-05,
      "loss": 0.0022,
      "step": 92670
    },
    {
      "epoch": 4.942933333333333,
      "grad_norm": 0.04490352421998978,
      "learning_rate": 1.9106666666666668e-05,
      "loss": 0.0013,
      "step": 92680
    },
    {
      "epoch": 4.943466666666667,
      "grad_norm": 0.12581400573253632,
      "learning_rate": 1.9103333333333334e-05,
      "loss": 0.0017,
      "step": 92690
    },
    {
      "epoch": 4.944,
      "grad_norm": 0.37286749482154846,
      "learning_rate": 1.91e-05,
      "loss": 0.0015,
      "step": 92700
    },
    {
      "epoch": 4.944533333333333,
      "grad_norm": 0.06550367921590805,
      "learning_rate": 1.909666666666667e-05,
      "loss": 0.0019,
      "step": 92710
    },
    {
      "epoch": 4.9450666666666665,
      "grad_norm": 0.275113046169281,
      "learning_rate": 1.9093333333333336e-05,
      "loss": 0.0022,
      "step": 92720
    },
    {
      "epoch": 4.9456,
      "grad_norm": 0.09137079864740372,
      "learning_rate": 1.909e-05,
      "loss": 0.003,
      "step": 92730
    },
    {
      "epoch": 4.946133333333333,
      "grad_norm": 0.11568423360586166,
      "learning_rate": 1.9086666666666668e-05,
      "loss": 0.0014,
      "step": 92740
    },
    {
      "epoch": 4.946666666666666,
      "grad_norm": 0.1262606978416443,
      "learning_rate": 1.9083333333333334e-05,
      "loss": 0.0027,
      "step": 92750
    },
    {
      "epoch": 4.9472000000000005,
      "grad_norm": 0.42145636677742004,
      "learning_rate": 1.908e-05,
      "loss": 0.0018,
      "step": 92760
    },
    {
      "epoch": 4.947733333333334,
      "grad_norm": 0.37136390805244446,
      "learning_rate": 1.9076666666666666e-05,
      "loss": 0.0019,
      "step": 92770
    },
    {
      "epoch": 4.948266666666667,
      "grad_norm": 0.12513719499111176,
      "learning_rate": 1.9073333333333336e-05,
      "loss": 0.002,
      "step": 92780
    },
    {
      "epoch": 4.9488,
      "grad_norm": 0.39447060227394104,
      "learning_rate": 1.9070000000000002e-05,
      "loss": 0.0016,
      "step": 92790
    },
    {
      "epoch": 4.949333333333334,
      "grad_norm": 0.15497541427612305,
      "learning_rate": 1.9066666666666668e-05,
      "loss": 0.0017,
      "step": 92800
    },
    {
      "epoch": 4.949866666666667,
      "grad_norm": 0.4623021185398102,
      "learning_rate": 1.9063333333333334e-05,
      "loss": 0.0016,
      "step": 92810
    },
    {
      "epoch": 4.9504,
      "grad_norm": 0.21335020661354065,
      "learning_rate": 1.906e-05,
      "loss": 0.0017,
      "step": 92820
    },
    {
      "epoch": 4.950933333333333,
      "grad_norm": 0.2144850343465805,
      "learning_rate": 1.9056666666666667e-05,
      "loss": 0.0016,
      "step": 92830
    },
    {
      "epoch": 4.951466666666667,
      "grad_norm": 0.2726844847202301,
      "learning_rate": 1.9053333333333333e-05,
      "loss": 0.0019,
      "step": 92840
    },
    {
      "epoch": 4.952,
      "grad_norm": 0.09312919527292252,
      "learning_rate": 1.9050000000000002e-05,
      "loss": 0.0015,
      "step": 92850
    },
    {
      "epoch": 4.952533333333333,
      "grad_norm": 0.18442732095718384,
      "learning_rate": 1.904666666666667e-05,
      "loss": 0.0019,
      "step": 92860
    },
    {
      "epoch": 4.9530666666666665,
      "grad_norm": 0.15059959888458252,
      "learning_rate": 1.9043333333333335e-05,
      "loss": 0.0015,
      "step": 92870
    },
    {
      "epoch": 4.9536,
      "grad_norm": 0.10737332701683044,
      "learning_rate": 1.904e-05,
      "loss": 0.002,
      "step": 92880
    },
    {
      "epoch": 4.954133333333333,
      "grad_norm": 0.22108392417430878,
      "learning_rate": 1.9036666666666667e-05,
      "loss": 0.0016,
      "step": 92890
    },
    {
      "epoch": 4.954666666666666,
      "grad_norm": 0.05943545699119568,
      "learning_rate": 1.9033333333333333e-05,
      "loss": 0.0026,
      "step": 92900
    },
    {
      "epoch": 4.9552,
      "grad_norm": 0.2043771892786026,
      "learning_rate": 1.903e-05,
      "loss": 0.0014,
      "step": 92910
    },
    {
      "epoch": 4.955733333333333,
      "grad_norm": 0.15915407240390778,
      "learning_rate": 1.902666666666667e-05,
      "loss": 0.0015,
      "step": 92920
    },
    {
      "epoch": 4.956266666666667,
      "grad_norm": 0.3638029396533966,
      "learning_rate": 1.9023333333333335e-05,
      "loss": 0.0021,
      "step": 92930
    },
    {
      "epoch": 4.9568,
      "grad_norm": 0.2145298570394516,
      "learning_rate": 1.902e-05,
      "loss": 0.0024,
      "step": 92940
    },
    {
      "epoch": 4.957333333333334,
      "grad_norm": 0.3886145353317261,
      "learning_rate": 1.901666666666667e-05,
      "loss": 0.0019,
      "step": 92950
    },
    {
      "epoch": 4.957866666666667,
      "grad_norm": 0.5074895024299622,
      "learning_rate": 1.9013333333333333e-05,
      "loss": 0.0023,
      "step": 92960
    },
    {
      "epoch": 4.9584,
      "grad_norm": 0.1528981775045395,
      "learning_rate": 1.901e-05,
      "loss": 0.0021,
      "step": 92970
    },
    {
      "epoch": 4.958933333333333,
      "grad_norm": 0.6546749472618103,
      "learning_rate": 1.9006666666666665e-05,
      "loss": 0.0025,
      "step": 92980
    },
    {
      "epoch": 4.959466666666667,
      "grad_norm": 0.3017544448375702,
      "learning_rate": 1.9003333333333335e-05,
      "loss": 0.0024,
      "step": 92990
    },
    {
      "epoch": 4.96,
      "grad_norm": 0.0366319976747036,
      "learning_rate": 1.9e-05,
      "loss": 0.0018,
      "step": 93000
    },
    {
      "epoch": 4.960533333333333,
      "grad_norm": 0.057739898562431335,
      "learning_rate": 1.8996666666666667e-05,
      "loss": 0.002,
      "step": 93010
    },
    {
      "epoch": 4.9610666666666665,
      "grad_norm": 0.1560191959142685,
      "learning_rate": 1.8993333333333337e-05,
      "loss": 0.0024,
      "step": 93020
    },
    {
      "epoch": 4.9616,
      "grad_norm": 0.36034902930259705,
      "learning_rate": 1.8990000000000003e-05,
      "loss": 0.0016,
      "step": 93030
    },
    {
      "epoch": 4.962133333333333,
      "grad_norm": 0.43470749258995056,
      "learning_rate": 1.8986666666666666e-05,
      "loss": 0.0019,
      "step": 93040
    },
    {
      "epoch": 4.962666666666666,
      "grad_norm": 0.45500674843788147,
      "learning_rate": 1.8983333333333335e-05,
      "loss": 0.0024,
      "step": 93050
    },
    {
      "epoch": 4.9632,
      "grad_norm": 0.1729891449213028,
      "learning_rate": 1.898e-05,
      "loss": 0.0019,
      "step": 93060
    },
    {
      "epoch": 4.963733333333334,
      "grad_norm": 0.1009242907166481,
      "learning_rate": 1.8976666666666667e-05,
      "loss": 0.0022,
      "step": 93070
    },
    {
      "epoch": 4.964266666666667,
      "grad_norm": 0.5188496112823486,
      "learning_rate": 1.8973333333333334e-05,
      "loss": 0.0018,
      "step": 93080
    },
    {
      "epoch": 4.9648,
      "grad_norm": 0.49141815304756165,
      "learning_rate": 1.8970000000000003e-05,
      "loss": 0.0024,
      "step": 93090
    },
    {
      "epoch": 4.965333333333334,
      "grad_norm": 0.350774884223938,
      "learning_rate": 1.896666666666667e-05,
      "loss": 0.0015,
      "step": 93100
    },
    {
      "epoch": 4.965866666666667,
      "grad_norm": 0.07162677496671677,
      "learning_rate": 1.8963333333333332e-05,
      "loss": 0.0018,
      "step": 93110
    },
    {
      "epoch": 4.9664,
      "grad_norm": 0.10385551303625107,
      "learning_rate": 1.896e-05,
      "loss": 0.0016,
      "step": 93120
    },
    {
      "epoch": 4.966933333333333,
      "grad_norm": 0.2034311592578888,
      "learning_rate": 1.8956666666666668e-05,
      "loss": 0.0015,
      "step": 93130
    },
    {
      "epoch": 4.967466666666667,
      "grad_norm": 0.09547574073076248,
      "learning_rate": 1.8953333333333334e-05,
      "loss": 0.0026,
      "step": 93140
    },
    {
      "epoch": 4.968,
      "grad_norm": 0.42583802342414856,
      "learning_rate": 1.895e-05,
      "loss": 0.0014,
      "step": 93150
    },
    {
      "epoch": 4.968533333333333,
      "grad_norm": 0.47374647855758667,
      "learning_rate": 1.894666666666667e-05,
      "loss": 0.0017,
      "step": 93160
    },
    {
      "epoch": 4.9690666666666665,
      "grad_norm": 0.10221138596534729,
      "learning_rate": 1.8943333333333335e-05,
      "loss": 0.0018,
      "step": 93170
    },
    {
      "epoch": 4.9696,
      "grad_norm": 0.14747682213783264,
      "learning_rate": 1.894e-05,
      "loss": 0.0016,
      "step": 93180
    },
    {
      "epoch": 4.970133333333333,
      "grad_norm": 0.567936897277832,
      "learning_rate": 1.8936666666666668e-05,
      "loss": 0.0017,
      "step": 93190
    },
    {
      "epoch": 4.970666666666666,
      "grad_norm": 0.4657515585422516,
      "learning_rate": 1.8933333333333334e-05,
      "loss": 0.0018,
      "step": 93200
    },
    {
      "epoch": 4.9712,
      "grad_norm": 0.18195316195487976,
      "learning_rate": 1.893e-05,
      "loss": 0.0014,
      "step": 93210
    },
    {
      "epoch": 4.971733333333333,
      "grad_norm": 0.02601628378033638,
      "learning_rate": 1.8926666666666666e-05,
      "loss": 0.0015,
      "step": 93220
    },
    {
      "epoch": 4.972266666666666,
      "grad_norm": 0.1896459311246872,
      "learning_rate": 1.8923333333333336e-05,
      "loss": 0.0023,
      "step": 93230
    },
    {
      "epoch": 4.9728,
      "grad_norm": 0.436598002910614,
      "learning_rate": 1.8920000000000002e-05,
      "loss": 0.0022,
      "step": 93240
    },
    {
      "epoch": 4.973333333333334,
      "grad_norm": 0.09973561018705368,
      "learning_rate": 1.8916666666666668e-05,
      "loss": 0.0016,
      "step": 93250
    },
    {
      "epoch": 4.973866666666667,
      "grad_norm": 0.09613360464572906,
      "learning_rate": 1.8913333333333334e-05,
      "loss": 0.0018,
      "step": 93260
    },
    {
      "epoch": 4.9744,
      "grad_norm": 0.2463689148426056,
      "learning_rate": 1.891e-05,
      "loss": 0.0017,
      "step": 93270
    },
    {
      "epoch": 4.974933333333333,
      "grad_norm": 0.32945242524147034,
      "learning_rate": 1.8906666666666666e-05,
      "loss": 0.0013,
      "step": 93280
    },
    {
      "epoch": 4.975466666666667,
      "grad_norm": 0.21877507865428925,
      "learning_rate": 1.8903333333333332e-05,
      "loss": 0.002,
      "step": 93290
    },
    {
      "epoch": 4.976,
      "grad_norm": 0.09362135827541351,
      "learning_rate": 1.8900000000000002e-05,
      "loss": 0.002,
      "step": 93300
    },
    {
      "epoch": 4.976533333333333,
      "grad_norm": 0.2430478036403656,
      "learning_rate": 1.8896666666666668e-05,
      "loss": 0.0018,
      "step": 93310
    },
    {
      "epoch": 4.9770666666666665,
      "grad_norm": 0.29737767577171326,
      "learning_rate": 1.8893333333333334e-05,
      "loss": 0.0021,
      "step": 93320
    },
    {
      "epoch": 4.9776,
      "grad_norm": 0.24595105648040771,
      "learning_rate": 1.8890000000000004e-05,
      "loss": 0.0019,
      "step": 93330
    },
    {
      "epoch": 4.978133333333333,
      "grad_norm": 0.12887676060199738,
      "learning_rate": 1.8886666666666667e-05,
      "loss": 0.0029,
      "step": 93340
    },
    {
      "epoch": 4.978666666666666,
      "grad_norm": 0.20622505247592926,
      "learning_rate": 1.8883333333333333e-05,
      "loss": 0.0023,
      "step": 93350
    },
    {
      "epoch": 4.9792,
      "grad_norm": 0.046242136508226395,
      "learning_rate": 1.888e-05,
      "loss": 0.0015,
      "step": 93360
    },
    {
      "epoch": 4.979733333333334,
      "grad_norm": 0.28599900007247925,
      "learning_rate": 1.887666666666667e-05,
      "loss": 0.0017,
      "step": 93370
    },
    {
      "epoch": 4.980266666666667,
      "grad_norm": 0.18395264446735382,
      "learning_rate": 1.8873333333333334e-05,
      "loss": 0.0017,
      "step": 93380
    },
    {
      "epoch": 4.9808,
      "grad_norm": 0.15360936522483826,
      "learning_rate": 1.887e-05,
      "loss": 0.0018,
      "step": 93390
    },
    {
      "epoch": 4.981333333333334,
      "grad_norm": 0.21285982429981232,
      "learning_rate": 1.886666666666667e-05,
      "loss": 0.0019,
      "step": 93400
    },
    {
      "epoch": 4.981866666666667,
      "grad_norm": 0.2042359858751297,
      "learning_rate": 1.8863333333333333e-05,
      "loss": 0.0018,
      "step": 93410
    },
    {
      "epoch": 4.9824,
      "grad_norm": 0.096707783639431,
      "learning_rate": 1.886e-05,
      "loss": 0.0021,
      "step": 93420
    },
    {
      "epoch": 4.982933333333333,
      "grad_norm": 0.2355998456478119,
      "learning_rate": 1.885666666666667e-05,
      "loss": 0.0014,
      "step": 93430
    },
    {
      "epoch": 4.983466666666667,
      "grad_norm": 0.33940091729164124,
      "learning_rate": 1.8853333333333335e-05,
      "loss": 0.0019,
      "step": 93440
    },
    {
      "epoch": 4.984,
      "grad_norm": 0.5374872088432312,
      "learning_rate": 1.885e-05,
      "loss": 0.0018,
      "step": 93450
    },
    {
      "epoch": 4.984533333333333,
      "grad_norm": 0.27123624086380005,
      "learning_rate": 1.8846666666666667e-05,
      "loss": 0.0017,
      "step": 93460
    },
    {
      "epoch": 4.9850666666666665,
      "grad_norm": 0.10180342942476273,
      "learning_rate": 1.8843333333333336e-05,
      "loss": 0.0013,
      "step": 93470
    },
    {
      "epoch": 4.9856,
      "grad_norm": 0.18088507652282715,
      "learning_rate": 1.8840000000000003e-05,
      "loss": 0.0018,
      "step": 93480
    },
    {
      "epoch": 4.986133333333333,
      "grad_norm": 0.21333934366703033,
      "learning_rate": 1.8836666666666665e-05,
      "loss": 0.0013,
      "step": 93490
    },
    {
      "epoch": 4.986666666666666,
      "grad_norm": 0.18004605174064636,
      "learning_rate": 1.8833333333333335e-05,
      "loss": 0.0023,
      "step": 93500
    },
    {
      "epoch": 4.9872,
      "grad_norm": 0.7847415208816528,
      "learning_rate": 1.883e-05,
      "loss": 0.002,
      "step": 93510
    },
    {
      "epoch": 4.987733333333333,
      "grad_norm": 0.3339855372905731,
      "learning_rate": 1.8826666666666667e-05,
      "loss": 0.0022,
      "step": 93520
    },
    {
      "epoch": 4.988266666666666,
      "grad_norm": 0.3463036119937897,
      "learning_rate": 1.8823333333333333e-05,
      "loss": 0.0022,
      "step": 93530
    },
    {
      "epoch": 4.9888,
      "grad_norm": 0.05774366855621338,
      "learning_rate": 1.8820000000000003e-05,
      "loss": 0.0023,
      "step": 93540
    },
    {
      "epoch": 4.989333333333334,
      "grad_norm": 0.12342933565378189,
      "learning_rate": 1.881666666666667e-05,
      "loss": 0.0025,
      "step": 93550
    },
    {
      "epoch": 4.989866666666667,
      "grad_norm": 0.5254160165786743,
      "learning_rate": 1.8813333333333335e-05,
      "loss": 0.002,
      "step": 93560
    },
    {
      "epoch": 4.9904,
      "grad_norm": 0.19596903026103973,
      "learning_rate": 1.881e-05,
      "loss": 0.0017,
      "step": 93570
    },
    {
      "epoch": 4.990933333333333,
      "grad_norm": 0.3515086770057678,
      "learning_rate": 1.8806666666666667e-05,
      "loss": 0.0023,
      "step": 93580
    },
    {
      "epoch": 4.991466666666667,
      "grad_norm": 0.3398911654949188,
      "learning_rate": 1.8803333333333333e-05,
      "loss": 0.002,
      "step": 93590
    },
    {
      "epoch": 4.992,
      "grad_norm": 0.24808068573474884,
      "learning_rate": 1.88e-05,
      "loss": 0.0023,
      "step": 93600
    },
    {
      "epoch": 4.992533333333333,
      "grad_norm": 0.5797995328903198,
      "learning_rate": 1.879666666666667e-05,
      "loss": 0.0017,
      "step": 93610
    },
    {
      "epoch": 4.9930666666666665,
      "grad_norm": 0.19522395730018616,
      "learning_rate": 1.8793333333333335e-05,
      "loss": 0.0015,
      "step": 93620
    },
    {
      "epoch": 4.9936,
      "grad_norm": 0.31405389308929443,
      "learning_rate": 1.879e-05,
      "loss": 0.0017,
      "step": 93630
    },
    {
      "epoch": 4.994133333333333,
      "grad_norm": 0.3391334116458893,
      "learning_rate": 1.8786666666666667e-05,
      "loss": 0.0018,
      "step": 93640
    },
    {
      "epoch": 4.994666666666666,
      "grad_norm": 0.07814683765172958,
      "learning_rate": 1.8783333333333334e-05,
      "loss": 0.0012,
      "step": 93650
    },
    {
      "epoch": 4.9952,
      "grad_norm": 0.24747413396835327,
      "learning_rate": 1.878e-05,
      "loss": 0.0015,
      "step": 93660
    },
    {
      "epoch": 4.995733333333334,
      "grad_norm": 0.5923555493354797,
      "learning_rate": 1.8776666666666666e-05,
      "loss": 0.0022,
      "step": 93670
    },
    {
      "epoch": 4.996266666666667,
      "grad_norm": 0.2451850324869156,
      "learning_rate": 1.8773333333333335e-05,
      "loss": 0.0017,
      "step": 93680
    },
    {
      "epoch": 4.9968,
      "grad_norm": 0.25035059452056885,
      "learning_rate": 1.877e-05,
      "loss": 0.0023,
      "step": 93690
    },
    {
      "epoch": 4.997333333333334,
      "grad_norm": 0.5488756895065308,
      "learning_rate": 1.8766666666666668e-05,
      "loss": 0.0026,
      "step": 93700
    },
    {
      "epoch": 4.997866666666667,
      "grad_norm": 0.32132869958877563,
      "learning_rate": 1.8763333333333337e-05,
      "loss": 0.002,
      "step": 93710
    },
    {
      "epoch": 4.9984,
      "grad_norm": 0.12858723104000092,
      "learning_rate": 1.876e-05,
      "loss": 0.0026,
      "step": 93720
    },
    {
      "epoch": 4.9989333333333335,
      "grad_norm": 0.13954289257526398,
      "learning_rate": 1.8756666666666666e-05,
      "loss": 0.0026,
      "step": 93730
    },
    {
      "epoch": 4.999466666666667,
      "grad_norm": 0.24744674563407898,
      "learning_rate": 1.8753333333333332e-05,
      "loss": 0.0022,
      "step": 93740
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.11970950663089752,
      "learning_rate": 1.8750000000000002e-05,
      "loss": 0.002,
      "step": 93750
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.0018871877109631896,
      "eval_runtime": 155.2528,
      "eval_samples_per_second": 1610.277,
      "eval_steps_per_second": 40.257,
      "step": 93750
    },
    {
      "epoch": 5.000533333333333,
      "grad_norm": 0.15356722474098206,
      "learning_rate": 1.8746666666666668e-05,
      "loss": 0.0019,
      "step": 93760
    },
    {
      "epoch": 5.0010666666666665,
      "grad_norm": 0.3121262490749359,
      "learning_rate": 1.8743333333333334e-05,
      "loss": 0.002,
      "step": 93770
    },
    {
      "epoch": 5.0016,
      "grad_norm": 0.6737223267555237,
      "learning_rate": 1.8740000000000004e-05,
      "loss": 0.0021,
      "step": 93780
    },
    {
      "epoch": 5.002133333333333,
      "grad_norm": 0.22464390099048615,
      "learning_rate": 1.8736666666666666e-05,
      "loss": 0.0017,
      "step": 93790
    },
    {
      "epoch": 5.002666666666666,
      "grad_norm": 0.24811628460884094,
      "learning_rate": 1.8733333333333332e-05,
      "loss": 0.0016,
      "step": 93800
    },
    {
      "epoch": 5.0032,
      "grad_norm": 0.09577392041683197,
      "learning_rate": 1.8730000000000002e-05,
      "loss": 0.0024,
      "step": 93810
    },
    {
      "epoch": 5.003733333333333,
      "grad_norm": 0.07383544743061066,
      "learning_rate": 1.8726666666666668e-05,
      "loss": 0.0022,
      "step": 93820
    },
    {
      "epoch": 5.004266666666667,
      "grad_norm": 0.18519678711891174,
      "learning_rate": 1.8723333333333334e-05,
      "loss": 0.0021,
      "step": 93830
    },
    {
      "epoch": 5.0048,
      "grad_norm": 0.0964517816901207,
      "learning_rate": 1.872e-05,
      "loss": 0.0022,
      "step": 93840
    },
    {
      "epoch": 5.005333333333334,
      "grad_norm": 0.04780100658535957,
      "learning_rate": 1.871666666666667e-05,
      "loss": 0.002,
      "step": 93850
    },
    {
      "epoch": 5.005866666666667,
      "grad_norm": 0.31043434143066406,
      "learning_rate": 1.8713333333333336e-05,
      "loss": 0.0021,
      "step": 93860
    },
    {
      "epoch": 5.0064,
      "grad_norm": 0.10485626012086868,
      "learning_rate": 1.871e-05,
      "loss": 0.0018,
      "step": 93870
    },
    {
      "epoch": 5.0069333333333335,
      "grad_norm": 0.37453916668891907,
      "learning_rate": 1.8706666666666668e-05,
      "loss": 0.0028,
      "step": 93880
    },
    {
      "epoch": 5.007466666666667,
      "grad_norm": 0.0689486637711525,
      "learning_rate": 1.8703333333333334e-05,
      "loss": 0.0014,
      "step": 93890
    },
    {
      "epoch": 5.008,
      "grad_norm": 0.05737445503473282,
      "learning_rate": 1.87e-05,
      "loss": 0.0017,
      "step": 93900
    },
    {
      "epoch": 5.008533333333333,
      "grad_norm": 0.26710212230682373,
      "learning_rate": 1.8696666666666667e-05,
      "loss": 0.0018,
      "step": 93910
    },
    {
      "epoch": 5.009066666666667,
      "grad_norm": 0.1574140042066574,
      "learning_rate": 1.8693333333333336e-05,
      "loss": 0.0019,
      "step": 93920
    },
    {
      "epoch": 5.0096,
      "grad_norm": 0.1568395346403122,
      "learning_rate": 1.8690000000000002e-05,
      "loss": 0.0011,
      "step": 93930
    },
    {
      "epoch": 5.010133333333333,
      "grad_norm": 0.27353784441947937,
      "learning_rate": 1.8686666666666665e-05,
      "loss": 0.0019,
      "step": 93940
    },
    {
      "epoch": 5.010666666666666,
      "grad_norm": 0.4268781542778015,
      "learning_rate": 1.8683333333333335e-05,
      "loss": 0.0028,
      "step": 93950
    },
    {
      "epoch": 5.0112,
      "grad_norm": 0.08483228832483292,
      "learning_rate": 1.868e-05,
      "loss": 0.0022,
      "step": 93960
    },
    {
      "epoch": 5.011733333333333,
      "grad_norm": 0.5161383748054504,
      "learning_rate": 1.8676666666666667e-05,
      "loss": 0.0033,
      "step": 93970
    },
    {
      "epoch": 5.012266666666667,
      "grad_norm": 0.18316292762756348,
      "learning_rate": 1.8673333333333333e-05,
      "loss": 0.0015,
      "step": 93980
    },
    {
      "epoch": 5.0128,
      "grad_norm": 0.25462934374809265,
      "learning_rate": 1.8670000000000003e-05,
      "loss": 0.0014,
      "step": 93990
    },
    {
      "epoch": 5.013333333333334,
      "grad_norm": 0.24248844385147095,
      "learning_rate": 1.866666666666667e-05,
      "loss": 0.0013,
      "step": 94000
    },
    {
      "epoch": 5.013866666666667,
      "grad_norm": 0.310977578163147,
      "learning_rate": 1.8663333333333335e-05,
      "loss": 0.0015,
      "step": 94010
    },
    {
      "epoch": 5.0144,
      "grad_norm": 0.5106120705604553,
      "learning_rate": 1.866e-05,
      "loss": 0.0018,
      "step": 94020
    },
    {
      "epoch": 5.0149333333333335,
      "grad_norm": 0.638608455657959,
      "learning_rate": 1.8656666666666667e-05,
      "loss": 0.0027,
      "step": 94030
    },
    {
      "epoch": 5.015466666666667,
      "grad_norm": 0.2448612004518509,
      "learning_rate": 1.8653333333333333e-05,
      "loss": 0.0015,
      "step": 94040
    },
    {
      "epoch": 5.016,
      "grad_norm": 0.1071680560708046,
      "learning_rate": 1.865e-05,
      "loss": 0.0017,
      "step": 94050
    },
    {
      "epoch": 5.016533333333333,
      "grad_norm": 0.0538041815161705,
      "learning_rate": 1.864666666666667e-05,
      "loss": 0.0017,
      "step": 94060
    },
    {
      "epoch": 5.017066666666667,
      "grad_norm": 0.4589105546474457,
      "learning_rate": 1.8643333333333335e-05,
      "loss": 0.0014,
      "step": 94070
    },
    {
      "epoch": 5.0176,
      "grad_norm": 0.0917259156703949,
      "learning_rate": 1.864e-05,
      "loss": 0.0018,
      "step": 94080
    },
    {
      "epoch": 5.018133333333333,
      "grad_norm": 0.27334684133529663,
      "learning_rate": 1.863666666666667e-05,
      "loss": 0.0017,
      "step": 94090
    },
    {
      "epoch": 5.018666666666666,
      "grad_norm": 0.13567841053009033,
      "learning_rate": 1.8633333333333333e-05,
      "loss": 0.0016,
      "step": 94100
    },
    {
      "epoch": 5.0192,
      "grad_norm": 0.07481012493371964,
      "learning_rate": 1.863e-05,
      "loss": 0.0024,
      "step": 94110
    },
    {
      "epoch": 5.019733333333333,
      "grad_norm": 0.32565173506736755,
      "learning_rate": 1.8626666666666666e-05,
      "loss": 0.0014,
      "step": 94120
    },
    {
      "epoch": 5.020266666666667,
      "grad_norm": 0.21655938029289246,
      "learning_rate": 1.8623333333333335e-05,
      "loss": 0.0012,
      "step": 94130
    },
    {
      "epoch": 5.0208,
      "grad_norm": 0.41710546612739563,
      "learning_rate": 1.862e-05,
      "loss": 0.0022,
      "step": 94140
    },
    {
      "epoch": 5.021333333333334,
      "grad_norm": 0.0684816911816597,
      "learning_rate": 1.8616666666666667e-05,
      "loss": 0.0015,
      "step": 94150
    },
    {
      "epoch": 5.021866666666667,
      "grad_norm": 0.07411443442106247,
      "learning_rate": 1.8613333333333337e-05,
      "loss": 0.0016,
      "step": 94160
    },
    {
      "epoch": 5.0224,
      "grad_norm": 0.5732529759407043,
      "learning_rate": 1.861e-05,
      "loss": 0.0028,
      "step": 94170
    },
    {
      "epoch": 5.0229333333333335,
      "grad_norm": 0.048786960542201996,
      "learning_rate": 1.8606666666666666e-05,
      "loss": 0.0017,
      "step": 94180
    },
    {
      "epoch": 5.023466666666667,
      "grad_norm": 0.23549316823482513,
      "learning_rate": 1.8603333333333335e-05,
      "loss": 0.0027,
      "step": 94190
    },
    {
      "epoch": 5.024,
      "grad_norm": 0.12693053483963013,
      "learning_rate": 1.86e-05,
      "loss": 0.0024,
      "step": 94200
    },
    {
      "epoch": 5.024533333333333,
      "grad_norm": 0.05991559475660324,
      "learning_rate": 1.8596666666666668e-05,
      "loss": 0.0014,
      "step": 94210
    },
    {
      "epoch": 5.025066666666667,
      "grad_norm": 0.15535926818847656,
      "learning_rate": 1.8593333333333334e-05,
      "loss": 0.0019,
      "step": 94220
    },
    {
      "epoch": 5.0256,
      "grad_norm": 0.538663387298584,
      "learning_rate": 1.8590000000000003e-05,
      "loss": 0.0013,
      "step": 94230
    },
    {
      "epoch": 5.026133333333333,
      "grad_norm": 0.18311738967895508,
      "learning_rate": 1.858666666666667e-05,
      "loss": 0.0011,
      "step": 94240
    },
    {
      "epoch": 5.026666666666666,
      "grad_norm": 0.05506051704287529,
      "learning_rate": 1.8583333333333332e-05,
      "loss": 0.0026,
      "step": 94250
    },
    {
      "epoch": 5.0272,
      "grad_norm": 0.1237640306353569,
      "learning_rate": 1.858e-05,
      "loss": 0.0014,
      "step": 94260
    },
    {
      "epoch": 5.027733333333333,
      "grad_norm": 0.24874600768089294,
      "learning_rate": 1.8576666666666668e-05,
      "loss": 0.0019,
      "step": 94270
    },
    {
      "epoch": 5.028266666666667,
      "grad_norm": 0.18711042404174805,
      "learning_rate": 1.8573333333333334e-05,
      "loss": 0.0019,
      "step": 94280
    },
    {
      "epoch": 5.0288,
      "grad_norm": 0.7083992958068848,
      "learning_rate": 1.857e-05,
      "loss": 0.0016,
      "step": 94290
    },
    {
      "epoch": 5.029333333333334,
      "grad_norm": 0.4364768862724304,
      "learning_rate": 1.856666666666667e-05,
      "loss": 0.0017,
      "step": 94300
    },
    {
      "epoch": 5.029866666666667,
      "grad_norm": 0.07305686175823212,
      "learning_rate": 1.8563333333333336e-05,
      "loss": 0.0022,
      "step": 94310
    },
    {
      "epoch": 5.0304,
      "grad_norm": 0.20661196112632751,
      "learning_rate": 1.856e-05,
      "loss": 0.0015,
      "step": 94320
    },
    {
      "epoch": 5.0309333333333335,
      "grad_norm": 0.04460586607456207,
      "learning_rate": 1.8556666666666668e-05,
      "loss": 0.0017,
      "step": 94330
    },
    {
      "epoch": 5.031466666666667,
      "grad_norm": 0.21343551576137543,
      "learning_rate": 1.8553333333333334e-05,
      "loss": 0.0015,
      "step": 94340
    },
    {
      "epoch": 5.032,
      "grad_norm": 0.3224638104438782,
      "learning_rate": 1.855e-05,
      "loss": 0.0015,
      "step": 94350
    },
    {
      "epoch": 5.032533333333333,
      "grad_norm": 0.04265182092785835,
      "learning_rate": 1.8546666666666666e-05,
      "loss": 0.0025,
      "step": 94360
    },
    {
      "epoch": 5.033066666666667,
      "grad_norm": 0.5480154156684875,
      "learning_rate": 1.8543333333333336e-05,
      "loss": 0.0012,
      "step": 94370
    },
    {
      "epoch": 5.0336,
      "grad_norm": 0.6666380763053894,
      "learning_rate": 1.8540000000000002e-05,
      "loss": 0.0021,
      "step": 94380
    },
    {
      "epoch": 5.034133333333333,
      "grad_norm": 0.0962957888841629,
      "learning_rate": 1.8536666666666668e-05,
      "loss": 0.0018,
      "step": 94390
    },
    {
      "epoch": 5.034666666666666,
      "grad_norm": 0.3531278371810913,
      "learning_rate": 1.8533333333333334e-05,
      "loss": 0.0014,
      "step": 94400
    },
    {
      "epoch": 5.0352,
      "grad_norm": 0.06480016559362411,
      "learning_rate": 1.853e-05,
      "loss": 0.0014,
      "step": 94410
    },
    {
      "epoch": 5.035733333333333,
      "grad_norm": 0.603504478931427,
      "learning_rate": 1.8526666666666667e-05,
      "loss": 0.003,
      "step": 94420
    },
    {
      "epoch": 5.036266666666666,
      "grad_norm": 0.35879480838775635,
      "learning_rate": 1.8523333333333333e-05,
      "loss": 0.0015,
      "step": 94430
    },
    {
      "epoch": 5.0368,
      "grad_norm": 0.28983020782470703,
      "learning_rate": 1.8520000000000002e-05,
      "loss": 0.002,
      "step": 94440
    },
    {
      "epoch": 5.037333333333334,
      "grad_norm": 0.09257715195417404,
      "learning_rate": 1.851666666666667e-05,
      "loss": 0.0018,
      "step": 94450
    },
    {
      "epoch": 5.037866666666667,
      "grad_norm": 0.2690976560115814,
      "learning_rate": 1.8513333333333335e-05,
      "loss": 0.0016,
      "step": 94460
    },
    {
      "epoch": 5.0384,
      "grad_norm": 0.32618066668510437,
      "learning_rate": 1.851e-05,
      "loss": 0.0016,
      "step": 94470
    },
    {
      "epoch": 5.0389333333333335,
      "grad_norm": 0.15060186386108398,
      "learning_rate": 1.8506666666666667e-05,
      "loss": 0.0023,
      "step": 94480
    },
    {
      "epoch": 5.039466666666667,
      "grad_norm": 0.07327122241258621,
      "learning_rate": 1.8503333333333333e-05,
      "loss": 0.0015,
      "step": 94490
    },
    {
      "epoch": 5.04,
      "grad_norm": 0.0624508261680603,
      "learning_rate": 1.85e-05,
      "loss": 0.0025,
      "step": 94500
    },
    {
      "epoch": 5.040533333333333,
      "grad_norm": 0.06271955370903015,
      "learning_rate": 1.849666666666667e-05,
      "loss": 0.0015,
      "step": 94510
    },
    {
      "epoch": 5.041066666666667,
      "grad_norm": 0.2213338315486908,
      "learning_rate": 1.8493333333333335e-05,
      "loss": 0.0018,
      "step": 94520
    },
    {
      "epoch": 5.0416,
      "grad_norm": 0.23995202779769897,
      "learning_rate": 1.849e-05,
      "loss": 0.0022,
      "step": 94530
    },
    {
      "epoch": 5.042133333333333,
      "grad_norm": 0.20515474677085876,
      "learning_rate": 1.848666666666667e-05,
      "loss": 0.0021,
      "step": 94540
    },
    {
      "epoch": 5.042666666666666,
      "grad_norm": 0.2712540924549103,
      "learning_rate": 1.8483333333333333e-05,
      "loss": 0.0015,
      "step": 94550
    },
    {
      "epoch": 5.0432,
      "grad_norm": 0.09316860884428024,
      "learning_rate": 1.848e-05,
      "loss": 0.0018,
      "step": 94560
    },
    {
      "epoch": 5.043733333333333,
      "grad_norm": 0.5411694049835205,
      "learning_rate": 1.847666666666667e-05,
      "loss": 0.0029,
      "step": 94570
    },
    {
      "epoch": 5.044266666666666,
      "grad_norm": 0.12175974249839783,
      "learning_rate": 1.8473333333333335e-05,
      "loss": 0.0016,
      "step": 94580
    },
    {
      "epoch": 5.0448,
      "grad_norm": 0.2375359684228897,
      "learning_rate": 1.847e-05,
      "loss": 0.0016,
      "step": 94590
    },
    {
      "epoch": 5.045333333333334,
      "grad_norm": 0.23361952602863312,
      "learning_rate": 1.8466666666666667e-05,
      "loss": 0.0016,
      "step": 94600
    },
    {
      "epoch": 5.045866666666667,
      "grad_norm": 0.29895490407943726,
      "learning_rate": 1.8463333333333337e-05,
      "loss": 0.0016,
      "step": 94610
    },
    {
      "epoch": 5.0464,
      "grad_norm": 0.4940820038318634,
      "learning_rate": 1.846e-05,
      "loss": 0.0038,
      "step": 94620
    },
    {
      "epoch": 5.0469333333333335,
      "grad_norm": 0.14338648319244385,
      "learning_rate": 1.8456666666666666e-05,
      "loss": 0.0017,
      "step": 94630
    },
    {
      "epoch": 5.047466666666667,
      "grad_norm": 0.08213143795728683,
      "learning_rate": 1.8453333333333335e-05,
      "loss": 0.0014,
      "step": 94640
    },
    {
      "epoch": 5.048,
      "grad_norm": 0.3732650578022003,
      "learning_rate": 1.845e-05,
      "loss": 0.0014,
      "step": 94650
    },
    {
      "epoch": 5.048533333333333,
      "grad_norm": 0.4348812997341156,
      "learning_rate": 1.8446666666666667e-05,
      "loss": 0.0016,
      "step": 94660
    },
    {
      "epoch": 5.049066666666667,
      "grad_norm": 0.37880203127861023,
      "learning_rate": 1.8443333333333333e-05,
      "loss": 0.0016,
      "step": 94670
    },
    {
      "epoch": 5.0496,
      "grad_norm": 0.12744435667991638,
      "learning_rate": 1.8440000000000003e-05,
      "loss": 0.0018,
      "step": 94680
    },
    {
      "epoch": 5.050133333333333,
      "grad_norm": 0.08596204221248627,
      "learning_rate": 1.843666666666667e-05,
      "loss": 0.0018,
      "step": 94690
    },
    {
      "epoch": 5.050666666666666,
      "grad_norm": 0.4202876389026642,
      "learning_rate": 1.8433333333333332e-05,
      "loss": 0.0018,
      "step": 94700
    },
    {
      "epoch": 5.0512,
      "grad_norm": 0.035369012504816055,
      "learning_rate": 1.843e-05,
      "loss": 0.0018,
      "step": 94710
    },
    {
      "epoch": 5.051733333333333,
      "grad_norm": 0.09135863184928894,
      "learning_rate": 1.8426666666666668e-05,
      "loss": 0.002,
      "step": 94720
    },
    {
      "epoch": 5.052266666666666,
      "grad_norm": 0.18012724816799164,
      "learning_rate": 1.8423333333333334e-05,
      "loss": 0.0018,
      "step": 94730
    },
    {
      "epoch": 5.0528,
      "grad_norm": 0.21738161146640778,
      "learning_rate": 1.842e-05,
      "loss": 0.0024,
      "step": 94740
    },
    {
      "epoch": 5.053333333333334,
      "grad_norm": 0.168190598487854,
      "learning_rate": 1.841666666666667e-05,
      "loss": 0.0021,
      "step": 94750
    },
    {
      "epoch": 5.053866666666667,
      "grad_norm": 0.03767472505569458,
      "learning_rate": 1.8413333333333335e-05,
      "loss": 0.0016,
      "step": 94760
    },
    {
      "epoch": 5.0544,
      "grad_norm": 0.060617152601480484,
      "learning_rate": 1.841e-05,
      "loss": 0.0021,
      "step": 94770
    },
    {
      "epoch": 5.0549333333333335,
      "grad_norm": 0.041158758103847504,
      "learning_rate": 1.8406666666666668e-05,
      "loss": 0.0029,
      "step": 94780
    },
    {
      "epoch": 5.055466666666667,
      "grad_norm": 0.06618178635835648,
      "learning_rate": 1.8403333333333334e-05,
      "loss": 0.0032,
      "step": 94790
    },
    {
      "epoch": 5.056,
      "grad_norm": 0.1197664886713028,
      "learning_rate": 1.84e-05,
      "loss": 0.002,
      "step": 94800
    },
    {
      "epoch": 5.056533333333333,
      "grad_norm": 0.4239594638347626,
      "learning_rate": 1.8396666666666666e-05,
      "loss": 0.0012,
      "step": 94810
    },
    {
      "epoch": 5.057066666666667,
      "grad_norm": 0.1564093977212906,
      "learning_rate": 1.8393333333333336e-05,
      "loss": 0.0025,
      "step": 94820
    },
    {
      "epoch": 5.0576,
      "grad_norm": 0.3151001036167145,
      "learning_rate": 1.8390000000000002e-05,
      "loss": 0.0014,
      "step": 94830
    },
    {
      "epoch": 5.058133333333333,
      "grad_norm": 0.1768372654914856,
      "learning_rate": 1.8386666666666668e-05,
      "loss": 0.0018,
      "step": 94840
    },
    {
      "epoch": 5.058666666666666,
      "grad_norm": 0.06892108917236328,
      "learning_rate": 1.8383333333333334e-05,
      "loss": 0.0024,
      "step": 94850
    },
    {
      "epoch": 5.0592,
      "grad_norm": 0.23168262839317322,
      "learning_rate": 1.838e-05,
      "loss": 0.0015,
      "step": 94860
    },
    {
      "epoch": 5.059733333333333,
      "grad_norm": 0.079839788377285,
      "learning_rate": 1.8376666666666666e-05,
      "loss": 0.0012,
      "step": 94870
    },
    {
      "epoch": 5.060266666666666,
      "grad_norm": 0.08340904116630554,
      "learning_rate": 1.8373333333333332e-05,
      "loss": 0.0013,
      "step": 94880
    },
    {
      "epoch": 5.0608,
      "grad_norm": 0.15281713008880615,
      "learning_rate": 1.8370000000000002e-05,
      "loss": 0.0018,
      "step": 94890
    },
    {
      "epoch": 5.061333333333334,
      "grad_norm": 0.5945088863372803,
      "learning_rate": 1.8366666666666668e-05,
      "loss": 0.0023,
      "step": 94900
    },
    {
      "epoch": 5.061866666666667,
      "grad_norm": 0.5985348224639893,
      "learning_rate": 1.8363333333333334e-05,
      "loss": 0.0033,
      "step": 94910
    },
    {
      "epoch": 5.0624,
      "grad_norm": 0.3297441005706787,
      "learning_rate": 1.8360000000000004e-05,
      "loss": 0.0022,
      "step": 94920
    },
    {
      "epoch": 5.0629333333333335,
      "grad_norm": 0.19250550866127014,
      "learning_rate": 1.8356666666666667e-05,
      "loss": 0.0022,
      "step": 94930
    },
    {
      "epoch": 5.063466666666667,
      "grad_norm": 0.41967710852622986,
      "learning_rate": 1.8353333333333333e-05,
      "loss": 0.002,
      "step": 94940
    },
    {
      "epoch": 5.064,
      "grad_norm": 0.29515576362609863,
      "learning_rate": 1.8350000000000002e-05,
      "loss": 0.0016,
      "step": 94950
    },
    {
      "epoch": 5.064533333333333,
      "grad_norm": 0.2425839900970459,
      "learning_rate": 1.834666666666667e-05,
      "loss": 0.0025,
      "step": 94960
    },
    {
      "epoch": 5.065066666666667,
      "grad_norm": 0.27109822630882263,
      "learning_rate": 1.8343333333333334e-05,
      "loss": 0.0027,
      "step": 94970
    },
    {
      "epoch": 5.0656,
      "grad_norm": 0.4411375820636749,
      "learning_rate": 1.834e-05,
      "loss": 0.0014,
      "step": 94980
    },
    {
      "epoch": 5.066133333333333,
      "grad_norm": 0.18291938304901123,
      "learning_rate": 1.833666666666667e-05,
      "loss": 0.0018,
      "step": 94990
    },
    {
      "epoch": 5.066666666666666,
      "grad_norm": 0.042604297399520874,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 0.002,
      "step": 95000
    },
    {
      "epoch": 5.0672,
      "grad_norm": 0.33722203969955444,
      "learning_rate": 1.833e-05,
      "loss": 0.0021,
      "step": 95010
    },
    {
      "epoch": 5.067733333333333,
      "grad_norm": 0.08620291948318481,
      "learning_rate": 1.832666666666667e-05,
      "loss": 0.0017,
      "step": 95020
    },
    {
      "epoch": 5.068266666666666,
      "grad_norm": 0.04390183091163635,
      "learning_rate": 1.8323333333333335e-05,
      "loss": 0.0019,
      "step": 95030
    },
    {
      "epoch": 5.0688,
      "grad_norm": 0.19008426368236542,
      "learning_rate": 1.832e-05,
      "loss": 0.0015,
      "step": 95040
    },
    {
      "epoch": 5.069333333333334,
      "grad_norm": 0.04802703484892845,
      "learning_rate": 1.8316666666666667e-05,
      "loss": 0.0016,
      "step": 95050
    },
    {
      "epoch": 5.069866666666667,
      "grad_norm": 0.08830201625823975,
      "learning_rate": 1.8313333333333336e-05,
      "loss": 0.0019,
      "step": 95060
    },
    {
      "epoch": 5.0704,
      "grad_norm": 0.4365331828594208,
      "learning_rate": 1.8310000000000003e-05,
      "loss": 0.0027,
      "step": 95070
    },
    {
      "epoch": 5.0709333333333335,
      "grad_norm": 0.297221839427948,
      "learning_rate": 1.8306666666666665e-05,
      "loss": 0.0021,
      "step": 95080
    },
    {
      "epoch": 5.071466666666667,
      "grad_norm": 0.36240559816360474,
      "learning_rate": 1.8303333333333335e-05,
      "loss": 0.0025,
      "step": 95090
    },
    {
      "epoch": 5.072,
      "grad_norm": 0.09193892031908035,
      "learning_rate": 1.83e-05,
      "loss": 0.0022,
      "step": 95100
    },
    {
      "epoch": 5.072533333333333,
      "grad_norm": 0.15434862673282623,
      "learning_rate": 1.8296666666666667e-05,
      "loss": 0.0019,
      "step": 95110
    },
    {
      "epoch": 5.073066666666667,
      "grad_norm": 0.16143837571144104,
      "learning_rate": 1.8293333333333333e-05,
      "loss": 0.0019,
      "step": 95120
    },
    {
      "epoch": 5.0736,
      "grad_norm": 0.573058545589447,
      "learning_rate": 1.8290000000000003e-05,
      "loss": 0.0026,
      "step": 95130
    },
    {
      "epoch": 5.074133333333333,
      "grad_norm": 0.2154138684272766,
      "learning_rate": 1.828666666666667e-05,
      "loss": 0.0027,
      "step": 95140
    },
    {
      "epoch": 5.074666666666666,
      "grad_norm": 0.2527778446674347,
      "learning_rate": 1.828333333333333e-05,
      "loss": 0.0018,
      "step": 95150
    },
    {
      "epoch": 5.0752,
      "grad_norm": 0.03692377358675003,
      "learning_rate": 1.828e-05,
      "loss": 0.0023,
      "step": 95160
    },
    {
      "epoch": 5.075733333333333,
      "grad_norm": 0.06865980476140976,
      "learning_rate": 1.8276666666666667e-05,
      "loss": 0.0012,
      "step": 95170
    },
    {
      "epoch": 5.076266666666666,
      "grad_norm": 0.15976925194263458,
      "learning_rate": 1.8273333333333333e-05,
      "loss": 0.0022,
      "step": 95180
    },
    {
      "epoch": 5.0768,
      "grad_norm": 0.1192183718085289,
      "learning_rate": 1.827e-05,
      "loss": 0.0019,
      "step": 95190
    },
    {
      "epoch": 5.077333333333334,
      "grad_norm": 0.03185661882162094,
      "learning_rate": 1.826666666666667e-05,
      "loss": 0.0023,
      "step": 95200
    },
    {
      "epoch": 5.077866666666667,
      "grad_norm": 0.17680275440216064,
      "learning_rate": 1.8263333333333335e-05,
      "loss": 0.002,
      "step": 95210
    },
    {
      "epoch": 5.0784,
      "grad_norm": 0.0900593250989914,
      "learning_rate": 1.826e-05,
      "loss": 0.0022,
      "step": 95220
    },
    {
      "epoch": 5.0789333333333335,
      "grad_norm": 0.5541577935218811,
      "learning_rate": 1.8256666666666667e-05,
      "loss": 0.0024,
      "step": 95230
    },
    {
      "epoch": 5.079466666666667,
      "grad_norm": 0.41264402866363525,
      "learning_rate": 1.8253333333333334e-05,
      "loss": 0.0018,
      "step": 95240
    },
    {
      "epoch": 5.08,
      "grad_norm": 0.06826416403055191,
      "learning_rate": 1.825e-05,
      "loss": 0.002,
      "step": 95250
    },
    {
      "epoch": 5.080533333333333,
      "grad_norm": 0.13647711277008057,
      "learning_rate": 1.8246666666666666e-05,
      "loss": 0.0023,
      "step": 95260
    },
    {
      "epoch": 5.081066666666667,
      "grad_norm": 0.051331985741853714,
      "learning_rate": 1.8243333333333335e-05,
      "loss": 0.0019,
      "step": 95270
    },
    {
      "epoch": 5.0816,
      "grad_norm": 0.12571294605731964,
      "learning_rate": 1.824e-05,
      "loss": 0.003,
      "step": 95280
    },
    {
      "epoch": 5.082133333333333,
      "grad_norm": 0.13469298183918,
      "learning_rate": 1.8236666666666668e-05,
      "loss": 0.0015,
      "step": 95290
    },
    {
      "epoch": 5.082666666666666,
      "grad_norm": 0.452292263507843,
      "learning_rate": 1.8233333333333334e-05,
      "loss": 0.0026,
      "step": 95300
    },
    {
      "epoch": 5.0832,
      "grad_norm": 0.42778176069259644,
      "learning_rate": 1.823e-05,
      "loss": 0.0013,
      "step": 95310
    },
    {
      "epoch": 5.083733333333333,
      "grad_norm": 0.13645118474960327,
      "learning_rate": 1.8226666666666666e-05,
      "loss": 0.0021,
      "step": 95320
    },
    {
      "epoch": 5.084266666666666,
      "grad_norm": 0.15198005735874176,
      "learning_rate": 1.8223333333333336e-05,
      "loss": 0.002,
      "step": 95330
    },
    {
      "epoch": 5.0848,
      "grad_norm": 0.11200188845396042,
      "learning_rate": 1.8220000000000002e-05,
      "loss": 0.0013,
      "step": 95340
    },
    {
      "epoch": 5.085333333333334,
      "grad_norm": 0.35538917779922485,
      "learning_rate": 1.8216666666666668e-05,
      "loss": 0.0019,
      "step": 95350
    },
    {
      "epoch": 5.085866666666667,
      "grad_norm": 0.22869625687599182,
      "learning_rate": 1.8213333333333334e-05,
      "loss": 0.0021,
      "step": 95360
    },
    {
      "epoch": 5.0864,
      "grad_norm": 0.2765291631221771,
      "learning_rate": 1.8210000000000004e-05,
      "loss": 0.0022,
      "step": 95370
    },
    {
      "epoch": 5.0869333333333335,
      "grad_norm": 0.3877466320991516,
      "learning_rate": 1.8206666666666666e-05,
      "loss": 0.0019,
      "step": 95380
    },
    {
      "epoch": 5.087466666666667,
      "grad_norm": 0.17193284630775452,
      "learning_rate": 1.8203333333333332e-05,
      "loss": 0.0012,
      "step": 95390
    },
    {
      "epoch": 5.088,
      "grad_norm": 0.2594763934612274,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 0.0017,
      "step": 95400
    },
    {
      "epoch": 5.088533333333333,
      "grad_norm": 0.3143511712551117,
      "learning_rate": 1.8196666666666668e-05,
      "loss": 0.0016,
      "step": 95410
    },
    {
      "epoch": 5.089066666666667,
      "grad_norm": 0.03891146183013916,
      "learning_rate": 1.8193333333333334e-05,
      "loss": 0.0024,
      "step": 95420
    },
    {
      "epoch": 5.0896,
      "grad_norm": 0.12446561455726624,
      "learning_rate": 1.819e-05,
      "loss": 0.0022,
      "step": 95430
    },
    {
      "epoch": 5.090133333333333,
      "grad_norm": 0.34306278824806213,
      "learning_rate": 1.818666666666667e-05,
      "loss": 0.0018,
      "step": 95440
    },
    {
      "epoch": 5.0906666666666665,
      "grad_norm": 0.054433200508356094,
      "learning_rate": 1.8183333333333336e-05,
      "loss": 0.0018,
      "step": 95450
    },
    {
      "epoch": 5.0912,
      "grad_norm": 0.03948898985981941,
      "learning_rate": 1.818e-05,
      "loss": 0.0028,
      "step": 95460
    },
    {
      "epoch": 5.091733333333333,
      "grad_norm": 0.2594536244869232,
      "learning_rate": 1.8176666666666668e-05,
      "loss": 0.0022,
      "step": 95470
    },
    {
      "epoch": 5.092266666666666,
      "grad_norm": 0.21600066125392914,
      "learning_rate": 1.8173333333333334e-05,
      "loss": 0.0025,
      "step": 95480
    },
    {
      "epoch": 5.0928,
      "grad_norm": 0.33714759349823,
      "learning_rate": 1.817e-05,
      "loss": 0.0017,
      "step": 95490
    },
    {
      "epoch": 5.093333333333334,
      "grad_norm": 0.13476181030273438,
      "learning_rate": 1.8166666666666667e-05,
      "loss": 0.0035,
      "step": 95500
    },
    {
      "epoch": 5.093866666666667,
      "grad_norm": 0.39668792486190796,
      "learning_rate": 1.8163333333333336e-05,
      "loss": 0.0013,
      "step": 95510
    },
    {
      "epoch": 5.0944,
      "grad_norm": 0.12190433591604233,
      "learning_rate": 1.8160000000000002e-05,
      "loss": 0.0015,
      "step": 95520
    },
    {
      "epoch": 5.0949333333333335,
      "grad_norm": 0.21009862422943115,
      "learning_rate": 1.8156666666666665e-05,
      "loss": 0.002,
      "step": 95530
    },
    {
      "epoch": 5.095466666666667,
      "grad_norm": 0.15507228672504425,
      "learning_rate": 1.8153333333333335e-05,
      "loss": 0.003,
      "step": 95540
    },
    {
      "epoch": 5.096,
      "grad_norm": 0.04398902505636215,
      "learning_rate": 1.815e-05,
      "loss": 0.0017,
      "step": 95550
    },
    {
      "epoch": 5.096533333333333,
      "grad_norm": 0.6234189867973328,
      "learning_rate": 1.8146666666666667e-05,
      "loss": 0.0017,
      "step": 95560
    },
    {
      "epoch": 5.097066666666667,
      "grad_norm": 0.07370445132255554,
      "learning_rate": 1.8143333333333333e-05,
      "loss": 0.0017,
      "step": 95570
    },
    {
      "epoch": 5.0976,
      "grad_norm": 0.16762559115886688,
      "learning_rate": 1.8140000000000003e-05,
      "loss": 0.0017,
      "step": 95580
    },
    {
      "epoch": 5.098133333333333,
      "grad_norm": 0.1938023865222931,
      "learning_rate": 1.813666666666667e-05,
      "loss": 0.0015,
      "step": 95590
    },
    {
      "epoch": 5.0986666666666665,
      "grad_norm": 0.23837777972221375,
      "learning_rate": 1.8133333333333335e-05,
      "loss": 0.0022,
      "step": 95600
    },
    {
      "epoch": 5.0992,
      "grad_norm": 0.029645143076777458,
      "learning_rate": 1.813e-05,
      "loss": 0.0012,
      "step": 95610
    },
    {
      "epoch": 5.099733333333333,
      "grad_norm": 0.15332214534282684,
      "learning_rate": 1.8126666666666667e-05,
      "loss": 0.0022,
      "step": 95620
    },
    {
      "epoch": 5.100266666666666,
      "grad_norm": 0.12148784101009369,
      "learning_rate": 1.8123333333333333e-05,
      "loss": 0.002,
      "step": 95630
    },
    {
      "epoch": 5.1008,
      "grad_norm": 0.26685044169425964,
      "learning_rate": 1.812e-05,
      "loss": 0.0017,
      "step": 95640
    },
    {
      "epoch": 5.101333333333334,
      "grad_norm": 0.38513344526290894,
      "learning_rate": 1.811666666666667e-05,
      "loss": 0.0013,
      "step": 95650
    },
    {
      "epoch": 5.101866666666667,
      "grad_norm": 0.03866742178797722,
      "learning_rate": 1.8113333333333335e-05,
      "loss": 0.0017,
      "step": 95660
    },
    {
      "epoch": 5.1024,
      "grad_norm": 0.06803252547979355,
      "learning_rate": 1.811e-05,
      "loss": 0.002,
      "step": 95670
    },
    {
      "epoch": 5.1029333333333335,
      "grad_norm": 0.16661366820335388,
      "learning_rate": 1.8106666666666667e-05,
      "loss": 0.0022,
      "step": 95680
    },
    {
      "epoch": 5.103466666666667,
      "grad_norm": 0.0769304484128952,
      "learning_rate": 1.8103333333333333e-05,
      "loss": 0.0019,
      "step": 95690
    },
    {
      "epoch": 5.104,
      "grad_norm": 0.3502582013607025,
      "learning_rate": 1.81e-05,
      "loss": 0.0016,
      "step": 95700
    },
    {
      "epoch": 5.104533333333333,
      "grad_norm": 0.6720865964889526,
      "learning_rate": 1.8096666666666666e-05,
      "loss": 0.0012,
      "step": 95710
    },
    {
      "epoch": 5.105066666666667,
      "grad_norm": 0.18673598766326904,
      "learning_rate": 1.8093333333333335e-05,
      "loss": 0.0029,
      "step": 95720
    },
    {
      "epoch": 5.1056,
      "grad_norm": 0.22187988460063934,
      "learning_rate": 1.809e-05,
      "loss": 0.0022,
      "step": 95730
    },
    {
      "epoch": 5.106133333333333,
      "grad_norm": 0.10310810059309006,
      "learning_rate": 1.8086666666666667e-05,
      "loss": 0.002,
      "step": 95740
    },
    {
      "epoch": 5.1066666666666665,
      "grad_norm": 0.1042112186551094,
      "learning_rate": 1.8083333333333337e-05,
      "loss": 0.0029,
      "step": 95750
    },
    {
      "epoch": 5.1072,
      "grad_norm": 0.44015780091285706,
      "learning_rate": 1.808e-05,
      "loss": 0.0023,
      "step": 95760
    },
    {
      "epoch": 5.107733333333333,
      "grad_norm": 0.36583009362220764,
      "learning_rate": 1.8076666666666666e-05,
      "loss": 0.0023,
      "step": 95770
    },
    {
      "epoch": 5.108266666666666,
      "grad_norm": 0.1076166108250618,
      "learning_rate": 1.8073333333333335e-05,
      "loss": 0.0018,
      "step": 95780
    },
    {
      "epoch": 5.1088,
      "grad_norm": 0.25737395882606506,
      "learning_rate": 1.807e-05,
      "loss": 0.002,
      "step": 95790
    },
    {
      "epoch": 5.109333333333334,
      "grad_norm": 0.3883969187736511,
      "learning_rate": 1.8066666666666668e-05,
      "loss": 0.0017,
      "step": 95800
    },
    {
      "epoch": 5.109866666666667,
      "grad_norm": 0.23910295963287354,
      "learning_rate": 1.8063333333333334e-05,
      "loss": 0.0018,
      "step": 95810
    },
    {
      "epoch": 5.1104,
      "grad_norm": 0.03187365084886551,
      "learning_rate": 1.8060000000000003e-05,
      "loss": 0.0016,
      "step": 95820
    },
    {
      "epoch": 5.1109333333333336,
      "grad_norm": 0.29806169867515564,
      "learning_rate": 1.8056666666666666e-05,
      "loss": 0.0018,
      "step": 95830
    },
    {
      "epoch": 5.111466666666667,
      "grad_norm": 0.4634086787700653,
      "learning_rate": 1.8053333333333332e-05,
      "loss": 0.0018,
      "step": 95840
    },
    {
      "epoch": 5.112,
      "grad_norm": 0.18353454768657684,
      "learning_rate": 1.805e-05,
      "loss": 0.0012,
      "step": 95850
    },
    {
      "epoch": 5.112533333333333,
      "grad_norm": 0.039071518927812576,
      "learning_rate": 1.8046666666666668e-05,
      "loss": 0.0018,
      "step": 95860
    },
    {
      "epoch": 5.113066666666667,
      "grad_norm": 0.1571010947227478,
      "learning_rate": 1.8043333333333334e-05,
      "loss": 0.0021,
      "step": 95870
    },
    {
      "epoch": 5.1136,
      "grad_norm": 0.12685257196426392,
      "learning_rate": 1.804e-05,
      "loss": 0.0017,
      "step": 95880
    },
    {
      "epoch": 5.114133333333333,
      "grad_norm": 0.12677298486232758,
      "learning_rate": 1.803666666666667e-05,
      "loss": 0.0017,
      "step": 95890
    },
    {
      "epoch": 5.1146666666666665,
      "grad_norm": 0.26611563563346863,
      "learning_rate": 1.8033333333333336e-05,
      "loss": 0.0014,
      "step": 95900
    },
    {
      "epoch": 5.1152,
      "grad_norm": 0.16513291001319885,
      "learning_rate": 1.803e-05,
      "loss": 0.0019,
      "step": 95910
    },
    {
      "epoch": 5.115733333333333,
      "grad_norm": 0.12600967288017273,
      "learning_rate": 1.8026666666666668e-05,
      "loss": 0.0014,
      "step": 95920
    },
    {
      "epoch": 5.116266666666666,
      "grad_norm": 0.31205472350120544,
      "learning_rate": 1.8023333333333334e-05,
      "loss": 0.0019,
      "step": 95930
    },
    {
      "epoch": 5.1168,
      "grad_norm": 0.5192288756370544,
      "learning_rate": 1.802e-05,
      "loss": 0.0027,
      "step": 95940
    },
    {
      "epoch": 5.117333333333334,
      "grad_norm": 0.11580820381641388,
      "learning_rate": 1.8016666666666666e-05,
      "loss": 0.0019,
      "step": 95950
    },
    {
      "epoch": 5.117866666666667,
      "grad_norm": 0.189114511013031,
      "learning_rate": 1.8013333333333336e-05,
      "loss": 0.0019,
      "step": 95960
    },
    {
      "epoch": 5.1184,
      "grad_norm": 0.23429065942764282,
      "learning_rate": 1.8010000000000002e-05,
      "loss": 0.0022,
      "step": 95970
    },
    {
      "epoch": 5.118933333333334,
      "grad_norm": 0.5622376799583435,
      "learning_rate": 1.8006666666666668e-05,
      "loss": 0.0014,
      "step": 95980
    },
    {
      "epoch": 5.119466666666667,
      "grad_norm": 0.10498057305812836,
      "learning_rate": 1.8003333333333334e-05,
      "loss": 0.0021,
      "step": 95990
    },
    {
      "epoch": 5.12,
      "grad_norm": 0.46220847964286804,
      "learning_rate": 1.8e-05,
      "loss": 0.0016,
      "step": 96000
    },
    {
      "epoch": 5.120533333333333,
      "grad_norm": 0.15634490549564362,
      "learning_rate": 1.7996666666666667e-05,
      "loss": 0.0019,
      "step": 96010
    },
    {
      "epoch": 5.121066666666667,
      "grad_norm": 0.03534013032913208,
      "learning_rate": 1.7993333333333333e-05,
      "loss": 0.0027,
      "step": 96020
    },
    {
      "epoch": 5.1216,
      "grad_norm": 0.45057550072669983,
      "learning_rate": 1.7990000000000002e-05,
      "loss": 0.0023,
      "step": 96030
    },
    {
      "epoch": 5.122133333333333,
      "grad_norm": 0.4209825396537781,
      "learning_rate": 1.798666666666667e-05,
      "loss": 0.002,
      "step": 96040
    },
    {
      "epoch": 5.1226666666666665,
      "grad_norm": 0.27377817034721375,
      "learning_rate": 1.7983333333333335e-05,
      "loss": 0.0016,
      "step": 96050
    },
    {
      "epoch": 5.1232,
      "grad_norm": 0.45620232820510864,
      "learning_rate": 1.798e-05,
      "loss": 0.002,
      "step": 96060
    },
    {
      "epoch": 5.123733333333333,
      "grad_norm": 0.043645039200782776,
      "learning_rate": 1.7976666666666667e-05,
      "loss": 0.0015,
      "step": 96070
    },
    {
      "epoch": 5.124266666666666,
      "grad_norm": 0.03966246172785759,
      "learning_rate": 1.7973333333333333e-05,
      "loss": 0.0012,
      "step": 96080
    },
    {
      "epoch": 5.1248,
      "grad_norm": 0.5511131286621094,
      "learning_rate": 1.797e-05,
      "loss": 0.0024,
      "step": 96090
    },
    {
      "epoch": 5.125333333333334,
      "grad_norm": 0.08111464977264404,
      "learning_rate": 1.796666666666667e-05,
      "loss": 0.0021,
      "step": 96100
    },
    {
      "epoch": 5.125866666666667,
      "grad_norm": 0.2059522569179535,
      "learning_rate": 1.7963333333333335e-05,
      "loss": 0.0016,
      "step": 96110
    },
    {
      "epoch": 5.1264,
      "grad_norm": 0.2360692024230957,
      "learning_rate": 1.796e-05,
      "loss": 0.0018,
      "step": 96120
    },
    {
      "epoch": 5.126933333333334,
      "grad_norm": 0.7847246527671814,
      "learning_rate": 1.795666666666667e-05,
      "loss": 0.002,
      "step": 96130
    },
    {
      "epoch": 5.127466666666667,
      "grad_norm": 0.24423301219940186,
      "learning_rate": 1.7953333333333333e-05,
      "loss": 0.0015,
      "step": 96140
    },
    {
      "epoch": 5.128,
      "grad_norm": 0.18547067046165466,
      "learning_rate": 1.795e-05,
      "loss": 0.0019,
      "step": 96150
    },
    {
      "epoch": 5.128533333333333,
      "grad_norm": 0.6111763119697571,
      "learning_rate": 1.794666666666667e-05,
      "loss": 0.0015,
      "step": 96160
    },
    {
      "epoch": 5.129066666666667,
      "grad_norm": 0.12006339430809021,
      "learning_rate": 1.7943333333333335e-05,
      "loss": 0.0017,
      "step": 96170
    },
    {
      "epoch": 5.1296,
      "grad_norm": 0.0435945950448513,
      "learning_rate": 1.794e-05,
      "loss": 0.0014,
      "step": 96180
    },
    {
      "epoch": 5.130133333333333,
      "grad_norm": 0.47530433535575867,
      "learning_rate": 1.7936666666666667e-05,
      "loss": 0.0013,
      "step": 96190
    },
    {
      "epoch": 5.1306666666666665,
      "grad_norm": 0.26173630356788635,
      "learning_rate": 1.7933333333333337e-05,
      "loss": 0.0021,
      "step": 96200
    },
    {
      "epoch": 5.1312,
      "grad_norm": 0.05668410658836365,
      "learning_rate": 1.793e-05,
      "loss": 0.0017,
      "step": 96210
    },
    {
      "epoch": 5.131733333333333,
      "grad_norm": 0.3677918612957001,
      "learning_rate": 1.7926666666666666e-05,
      "loss": 0.002,
      "step": 96220
    },
    {
      "epoch": 5.132266666666666,
      "grad_norm": 0.21286356449127197,
      "learning_rate": 1.7923333333333335e-05,
      "loss": 0.0014,
      "step": 96230
    },
    {
      "epoch": 5.1328,
      "grad_norm": 0.2346954494714737,
      "learning_rate": 1.792e-05,
      "loss": 0.0017,
      "step": 96240
    },
    {
      "epoch": 5.133333333333334,
      "grad_norm": 0.13055911660194397,
      "learning_rate": 1.7916666666666667e-05,
      "loss": 0.002,
      "step": 96250
    },
    {
      "epoch": 5.133866666666667,
      "grad_norm": 0.07587853819131851,
      "learning_rate": 1.7913333333333333e-05,
      "loss": 0.0015,
      "step": 96260
    },
    {
      "epoch": 5.1344,
      "grad_norm": 0.04939693957567215,
      "learning_rate": 1.7910000000000003e-05,
      "loss": 0.0015,
      "step": 96270
    },
    {
      "epoch": 5.134933333333334,
      "grad_norm": 0.050517212599515915,
      "learning_rate": 1.790666666666667e-05,
      "loss": 0.002,
      "step": 96280
    },
    {
      "epoch": 5.135466666666667,
      "grad_norm": 0.1593257188796997,
      "learning_rate": 1.7903333333333332e-05,
      "loss": 0.0022,
      "step": 96290
    },
    {
      "epoch": 5.136,
      "grad_norm": 0.38391417264938354,
      "learning_rate": 1.79e-05,
      "loss": 0.0017,
      "step": 96300
    },
    {
      "epoch": 5.136533333333333,
      "grad_norm": 0.41283169388771057,
      "learning_rate": 1.7896666666666668e-05,
      "loss": 0.0023,
      "step": 96310
    },
    {
      "epoch": 5.137066666666667,
      "grad_norm": 0.052400775253772736,
      "learning_rate": 1.7893333333333334e-05,
      "loss": 0.0015,
      "step": 96320
    },
    {
      "epoch": 5.1376,
      "grad_norm": 0.2949471175670624,
      "learning_rate": 1.789e-05,
      "loss": 0.0019,
      "step": 96330
    },
    {
      "epoch": 5.138133333333333,
      "grad_norm": 0.24421554803848267,
      "learning_rate": 1.788666666666667e-05,
      "loss": 0.0017,
      "step": 96340
    },
    {
      "epoch": 5.1386666666666665,
      "grad_norm": 0.7488387227058411,
      "learning_rate": 1.7883333333333335e-05,
      "loss": 0.0019,
      "step": 96350
    },
    {
      "epoch": 5.1392,
      "grad_norm": 0.4237319827079773,
      "learning_rate": 1.7879999999999998e-05,
      "loss": 0.0014,
      "step": 96360
    },
    {
      "epoch": 5.139733333333333,
      "grad_norm": 0.19997021555900574,
      "learning_rate": 1.7876666666666668e-05,
      "loss": 0.002,
      "step": 96370
    },
    {
      "epoch": 5.140266666666666,
      "grad_norm": 0.21923258900642395,
      "learning_rate": 1.7873333333333334e-05,
      "loss": 0.0014,
      "step": 96380
    },
    {
      "epoch": 5.1408,
      "grad_norm": 0.3295765817165375,
      "learning_rate": 1.787e-05,
      "loss": 0.0029,
      "step": 96390
    },
    {
      "epoch": 5.141333333333334,
      "grad_norm": 0.29903846979141235,
      "learning_rate": 1.7866666666666666e-05,
      "loss": 0.0017,
      "step": 96400
    },
    {
      "epoch": 5.141866666666667,
      "grad_norm": 0.3621586263179779,
      "learning_rate": 1.7863333333333336e-05,
      "loss": 0.0018,
      "step": 96410
    },
    {
      "epoch": 5.1424,
      "grad_norm": 0.5441592931747437,
      "learning_rate": 1.7860000000000002e-05,
      "loss": 0.0018,
      "step": 96420
    },
    {
      "epoch": 5.142933333333334,
      "grad_norm": 0.06981632858514786,
      "learning_rate": 1.7856666666666668e-05,
      "loss": 0.0032,
      "step": 96430
    },
    {
      "epoch": 5.143466666666667,
      "grad_norm": 0.1046162098646164,
      "learning_rate": 1.7853333333333334e-05,
      "loss": 0.0025,
      "step": 96440
    },
    {
      "epoch": 5.144,
      "grad_norm": 0.0966043546795845,
      "learning_rate": 1.785e-05,
      "loss": 0.0018,
      "step": 96450
    },
    {
      "epoch": 5.144533333333333,
      "grad_norm": 0.16132183372974396,
      "learning_rate": 1.7846666666666666e-05,
      "loss": 0.0018,
      "step": 96460
    },
    {
      "epoch": 5.145066666666667,
      "grad_norm": 0.37234556674957275,
      "learning_rate": 1.7843333333333332e-05,
      "loss": 0.0023,
      "step": 96470
    },
    {
      "epoch": 5.1456,
      "grad_norm": 0.06619451195001602,
      "learning_rate": 1.7840000000000002e-05,
      "loss": 0.0015,
      "step": 96480
    },
    {
      "epoch": 5.146133333333333,
      "grad_norm": 0.09396161884069443,
      "learning_rate": 1.7836666666666668e-05,
      "loss": 0.002,
      "step": 96490
    },
    {
      "epoch": 5.1466666666666665,
      "grad_norm": 0.17937448620796204,
      "learning_rate": 1.7833333333333334e-05,
      "loss": 0.0017,
      "step": 96500
    },
    {
      "epoch": 5.1472,
      "grad_norm": 0.27509167790412903,
      "learning_rate": 1.783e-05,
      "loss": 0.0017,
      "step": 96510
    },
    {
      "epoch": 5.147733333333333,
      "grad_norm": 0.21144583821296692,
      "learning_rate": 1.7826666666666667e-05,
      "loss": 0.0019,
      "step": 96520
    },
    {
      "epoch": 5.148266666666666,
      "grad_norm": 0.12375516444444656,
      "learning_rate": 1.7823333333333333e-05,
      "loss": 0.0015,
      "step": 96530
    },
    {
      "epoch": 5.1488,
      "grad_norm": 0.5366703867912292,
      "learning_rate": 1.7820000000000002e-05,
      "loss": 0.0013,
      "step": 96540
    },
    {
      "epoch": 5.149333333333334,
      "grad_norm": 0.15771470963954926,
      "learning_rate": 1.781666666666667e-05,
      "loss": 0.0031,
      "step": 96550
    },
    {
      "epoch": 5.149866666666667,
      "grad_norm": 0.5324869155883789,
      "learning_rate": 1.7813333333333334e-05,
      "loss": 0.0013,
      "step": 96560
    },
    {
      "epoch": 5.1504,
      "grad_norm": 0.3736018240451813,
      "learning_rate": 1.781e-05,
      "loss": 0.002,
      "step": 96570
    },
    {
      "epoch": 5.150933333333334,
      "grad_norm": 0.466739684343338,
      "learning_rate": 1.780666666666667e-05,
      "loss": 0.0022,
      "step": 96580
    },
    {
      "epoch": 5.151466666666667,
      "grad_norm": 0.3482950031757355,
      "learning_rate": 1.7803333333333333e-05,
      "loss": 0.0015,
      "step": 96590
    },
    {
      "epoch": 5.152,
      "grad_norm": 0.1216488927602768,
      "learning_rate": 1.78e-05,
      "loss": 0.0014,
      "step": 96600
    },
    {
      "epoch": 5.152533333333333,
      "grad_norm": 0.14201337099075317,
      "learning_rate": 1.779666666666667e-05,
      "loss": 0.0028,
      "step": 96610
    },
    {
      "epoch": 5.153066666666667,
      "grad_norm": 0.15418949723243713,
      "learning_rate": 1.7793333333333335e-05,
      "loss": 0.0014,
      "step": 96620
    },
    {
      "epoch": 5.1536,
      "grad_norm": 0.29757246375083923,
      "learning_rate": 1.779e-05,
      "loss": 0.0024,
      "step": 96630
    },
    {
      "epoch": 5.154133333333333,
      "grad_norm": 0.27242162823677063,
      "learning_rate": 1.7786666666666667e-05,
      "loss": 0.0015,
      "step": 96640
    },
    {
      "epoch": 5.1546666666666665,
      "grad_norm": 0.053341399878263474,
      "learning_rate": 1.7783333333333336e-05,
      "loss": 0.0017,
      "step": 96650
    },
    {
      "epoch": 5.1552,
      "grad_norm": 0.25212377309799194,
      "learning_rate": 1.7780000000000003e-05,
      "loss": 0.0012,
      "step": 96660
    },
    {
      "epoch": 5.155733333333333,
      "grad_norm": 0.5095481276512146,
      "learning_rate": 1.7776666666666665e-05,
      "loss": 0.0023,
      "step": 96670
    },
    {
      "epoch": 5.156266666666666,
      "grad_norm": 0.5053922533988953,
      "learning_rate": 1.7773333333333335e-05,
      "loss": 0.0017,
      "step": 96680
    },
    {
      "epoch": 5.1568,
      "grad_norm": 0.051478542387485504,
      "learning_rate": 1.777e-05,
      "loss": 0.0023,
      "step": 96690
    },
    {
      "epoch": 5.157333333333334,
      "grad_norm": 0.11885464936494827,
      "learning_rate": 1.7766666666666667e-05,
      "loss": 0.0024,
      "step": 96700
    },
    {
      "epoch": 5.157866666666667,
      "grad_norm": 0.18344809114933014,
      "learning_rate": 1.7763333333333333e-05,
      "loss": 0.0016,
      "step": 96710
    },
    {
      "epoch": 5.1584,
      "grad_norm": 0.29615122079849243,
      "learning_rate": 1.7760000000000003e-05,
      "loss": 0.0028,
      "step": 96720
    },
    {
      "epoch": 5.158933333333334,
      "grad_norm": 0.18852181732654572,
      "learning_rate": 1.775666666666667e-05,
      "loss": 0.0019,
      "step": 96730
    },
    {
      "epoch": 5.159466666666667,
      "grad_norm": 0.41519486904144287,
      "learning_rate": 1.775333333333333e-05,
      "loss": 0.002,
      "step": 96740
    },
    {
      "epoch": 5.16,
      "grad_norm": 0.12041240930557251,
      "learning_rate": 1.775e-05,
      "loss": 0.0021,
      "step": 96750
    },
    {
      "epoch": 5.160533333333333,
      "grad_norm": 0.23472854495048523,
      "learning_rate": 1.7746666666666667e-05,
      "loss": 0.0016,
      "step": 96760
    },
    {
      "epoch": 5.161066666666667,
      "grad_norm": 0.460332989692688,
      "learning_rate": 1.7743333333333333e-05,
      "loss": 0.0017,
      "step": 96770
    },
    {
      "epoch": 5.1616,
      "grad_norm": 0.24434532225131989,
      "learning_rate": 1.774e-05,
      "loss": 0.0015,
      "step": 96780
    },
    {
      "epoch": 5.162133333333333,
      "grad_norm": 0.21726271510124207,
      "learning_rate": 1.773666666666667e-05,
      "loss": 0.0023,
      "step": 96790
    },
    {
      "epoch": 5.1626666666666665,
      "grad_norm": 0.18565869331359863,
      "learning_rate": 1.7733333333333335e-05,
      "loss": 0.0021,
      "step": 96800
    },
    {
      "epoch": 5.1632,
      "grad_norm": 0.2457992285490036,
      "learning_rate": 1.773e-05,
      "loss": 0.0015,
      "step": 96810
    },
    {
      "epoch": 5.163733333333333,
      "grad_norm": 0.6175633072853088,
      "learning_rate": 1.7726666666666667e-05,
      "loss": 0.0016,
      "step": 96820
    },
    {
      "epoch": 5.164266666666666,
      "grad_norm": 0.48774483799934387,
      "learning_rate": 1.7723333333333334e-05,
      "loss": 0.0018,
      "step": 96830
    },
    {
      "epoch": 5.1648,
      "grad_norm": 0.24224917590618134,
      "learning_rate": 1.772e-05,
      "loss": 0.0023,
      "step": 96840
    },
    {
      "epoch": 5.165333333333333,
      "grad_norm": 0.3468726575374603,
      "learning_rate": 1.7716666666666666e-05,
      "loss": 0.0019,
      "step": 96850
    },
    {
      "epoch": 5.165866666666667,
      "grad_norm": 0.18553417921066284,
      "learning_rate": 1.7713333333333335e-05,
      "loss": 0.0016,
      "step": 96860
    },
    {
      "epoch": 5.1664,
      "grad_norm": 0.32422104477882385,
      "learning_rate": 1.771e-05,
      "loss": 0.0017,
      "step": 96870
    },
    {
      "epoch": 5.166933333333334,
      "grad_norm": 0.5172094106674194,
      "learning_rate": 1.7706666666666668e-05,
      "loss": 0.0016,
      "step": 96880
    },
    {
      "epoch": 5.167466666666667,
      "grad_norm": 0.24371160566806793,
      "learning_rate": 1.7703333333333334e-05,
      "loss": 0.0019,
      "step": 96890
    },
    {
      "epoch": 5.168,
      "grad_norm": 0.17210106551647186,
      "learning_rate": 1.77e-05,
      "loss": 0.0016,
      "step": 96900
    },
    {
      "epoch": 5.168533333333333,
      "grad_norm": 0.21122591197490692,
      "learning_rate": 1.7696666666666666e-05,
      "loss": 0.002,
      "step": 96910
    },
    {
      "epoch": 5.169066666666667,
      "grad_norm": 0.07906219363212585,
      "learning_rate": 1.7693333333333336e-05,
      "loss": 0.0014,
      "step": 96920
    },
    {
      "epoch": 5.1696,
      "grad_norm": 0.07381808757781982,
      "learning_rate": 1.7690000000000002e-05,
      "loss": 0.0022,
      "step": 96930
    },
    {
      "epoch": 5.170133333333333,
      "grad_norm": 0.2130642682313919,
      "learning_rate": 1.7686666666666668e-05,
      "loss": 0.0013,
      "step": 96940
    },
    {
      "epoch": 5.1706666666666665,
      "grad_norm": 0.1317300796508789,
      "learning_rate": 1.7683333333333334e-05,
      "loss": 0.0014,
      "step": 96950
    },
    {
      "epoch": 5.1712,
      "grad_norm": 0.2862776517868042,
      "learning_rate": 1.7680000000000004e-05,
      "loss": 0.0024,
      "step": 96960
    },
    {
      "epoch": 5.171733333333333,
      "grad_norm": 0.047856640070676804,
      "learning_rate": 1.7676666666666666e-05,
      "loss": 0.0023,
      "step": 96970
    },
    {
      "epoch": 5.172266666666666,
      "grad_norm": 0.39552056789398193,
      "learning_rate": 1.7673333333333332e-05,
      "loss": 0.0019,
      "step": 96980
    },
    {
      "epoch": 5.1728,
      "grad_norm": 0.06928583979606628,
      "learning_rate": 1.7670000000000002e-05,
      "loss": 0.0018,
      "step": 96990
    },
    {
      "epoch": 5.173333333333334,
      "grad_norm": 0.09639408439397812,
      "learning_rate": 1.7666666666666668e-05,
      "loss": 0.0023,
      "step": 97000
    },
    {
      "epoch": 5.173866666666667,
      "grad_norm": 0.4952493906021118,
      "learning_rate": 1.7663333333333334e-05,
      "loss": 0.0024,
      "step": 97010
    },
    {
      "epoch": 5.1744,
      "grad_norm": 0.19114094972610474,
      "learning_rate": 1.766e-05,
      "loss": 0.002,
      "step": 97020
    },
    {
      "epoch": 5.174933333333334,
      "grad_norm": 0.6137229800224304,
      "learning_rate": 1.765666666666667e-05,
      "loss": 0.0032,
      "step": 97030
    },
    {
      "epoch": 5.175466666666667,
      "grad_norm": 0.09059295803308487,
      "learning_rate": 1.7653333333333333e-05,
      "loss": 0.0025,
      "step": 97040
    },
    {
      "epoch": 5.176,
      "grad_norm": 0.15724517405033112,
      "learning_rate": 1.765e-05,
      "loss": 0.003,
      "step": 97050
    },
    {
      "epoch": 5.176533333333333,
      "grad_norm": 0.12856091558933258,
      "learning_rate": 1.7646666666666668e-05,
      "loss": 0.0019,
      "step": 97060
    },
    {
      "epoch": 5.177066666666667,
      "grad_norm": 0.4080386459827423,
      "learning_rate": 1.7643333333333334e-05,
      "loss": 0.0013,
      "step": 97070
    },
    {
      "epoch": 5.1776,
      "grad_norm": 0.4676203727722168,
      "learning_rate": 1.764e-05,
      "loss": 0.0013,
      "step": 97080
    },
    {
      "epoch": 5.178133333333333,
      "grad_norm": 0.17694689333438873,
      "learning_rate": 1.7636666666666667e-05,
      "loss": 0.0018,
      "step": 97090
    },
    {
      "epoch": 5.1786666666666665,
      "grad_norm": 0.4566520154476166,
      "learning_rate": 1.7633333333333336e-05,
      "loss": 0.0013,
      "step": 97100
    },
    {
      "epoch": 5.1792,
      "grad_norm": 0.30151113867759705,
      "learning_rate": 1.7630000000000002e-05,
      "loss": 0.0015,
      "step": 97110
    },
    {
      "epoch": 5.179733333333333,
      "grad_norm": 0.13787776231765747,
      "learning_rate": 1.7626666666666665e-05,
      "loss": 0.0013,
      "step": 97120
    },
    {
      "epoch": 5.180266666666666,
      "grad_norm": 0.3611331284046173,
      "learning_rate": 1.7623333333333335e-05,
      "loss": 0.0017,
      "step": 97130
    },
    {
      "epoch": 5.1808,
      "grad_norm": 0.028027545660734177,
      "learning_rate": 1.762e-05,
      "loss": 0.0014,
      "step": 97140
    },
    {
      "epoch": 5.181333333333333,
      "grad_norm": 0.22147363424301147,
      "learning_rate": 1.7616666666666667e-05,
      "loss": 0.0015,
      "step": 97150
    },
    {
      "epoch": 5.181866666666667,
      "grad_norm": 0.1219450905919075,
      "learning_rate": 1.7613333333333333e-05,
      "loss": 0.0018,
      "step": 97160
    },
    {
      "epoch": 5.1824,
      "grad_norm": 0.1877492368221283,
      "learning_rate": 1.7610000000000002e-05,
      "loss": 0.0024,
      "step": 97170
    },
    {
      "epoch": 5.182933333333334,
      "grad_norm": 0.03836427628993988,
      "learning_rate": 1.760666666666667e-05,
      "loss": 0.0026,
      "step": 97180
    },
    {
      "epoch": 5.183466666666667,
      "grad_norm": 0.47768986225128174,
      "learning_rate": 1.7603333333333335e-05,
      "loss": 0.0018,
      "step": 97190
    },
    {
      "epoch": 5.184,
      "grad_norm": 0.39823445677757263,
      "learning_rate": 1.76e-05,
      "loss": 0.0018,
      "step": 97200
    },
    {
      "epoch": 5.184533333333333,
      "grad_norm": 0.09874056279659271,
      "learning_rate": 1.7596666666666667e-05,
      "loss": 0.0016,
      "step": 97210
    },
    {
      "epoch": 5.185066666666667,
      "grad_norm": 0.48562249541282654,
      "learning_rate": 1.7593333333333333e-05,
      "loss": 0.0031,
      "step": 97220
    },
    {
      "epoch": 5.1856,
      "grad_norm": 0.37739840149879456,
      "learning_rate": 1.759e-05,
      "loss": 0.002,
      "step": 97230
    },
    {
      "epoch": 5.186133333333333,
      "grad_norm": 0.4471258819103241,
      "learning_rate": 1.758666666666667e-05,
      "loss": 0.0022,
      "step": 97240
    },
    {
      "epoch": 5.1866666666666665,
      "grad_norm": 0.521705687046051,
      "learning_rate": 1.7583333333333335e-05,
      "loss": 0.0016,
      "step": 97250
    },
    {
      "epoch": 5.1872,
      "grad_norm": 0.44762590527534485,
      "learning_rate": 1.758e-05,
      "loss": 0.0024,
      "step": 97260
    },
    {
      "epoch": 5.187733333333333,
      "grad_norm": 0.32560086250305176,
      "learning_rate": 1.7576666666666667e-05,
      "loss": 0.0028,
      "step": 97270
    },
    {
      "epoch": 5.188266666666666,
      "grad_norm": 0.05168164148926735,
      "learning_rate": 1.7573333333333333e-05,
      "loss": 0.0018,
      "step": 97280
    },
    {
      "epoch": 5.1888,
      "grad_norm": 0.039670128375291824,
      "learning_rate": 1.757e-05,
      "loss": 0.002,
      "step": 97290
    },
    {
      "epoch": 5.189333333333333,
      "grad_norm": 0.5853010416030884,
      "learning_rate": 1.756666666666667e-05,
      "loss": 0.0029,
      "step": 97300
    },
    {
      "epoch": 5.189866666666667,
      "grad_norm": 0.17030024528503418,
      "learning_rate": 1.7563333333333335e-05,
      "loss": 0.002,
      "step": 97310
    },
    {
      "epoch": 5.1904,
      "grad_norm": 0.24758076667785645,
      "learning_rate": 1.756e-05,
      "loss": 0.0021,
      "step": 97320
    },
    {
      "epoch": 5.190933333333334,
      "grad_norm": 0.10558046400547028,
      "learning_rate": 1.7556666666666667e-05,
      "loss": 0.0018,
      "step": 97330
    },
    {
      "epoch": 5.191466666666667,
      "grad_norm": 0.12901687622070312,
      "learning_rate": 1.7553333333333337e-05,
      "loss": 0.0017,
      "step": 97340
    },
    {
      "epoch": 5.192,
      "grad_norm": 0.07014055550098419,
      "learning_rate": 1.755e-05,
      "loss": 0.0017,
      "step": 97350
    },
    {
      "epoch": 5.1925333333333334,
      "grad_norm": 0.0807972177863121,
      "learning_rate": 1.7546666666666666e-05,
      "loss": 0.0018,
      "step": 97360
    },
    {
      "epoch": 5.193066666666667,
      "grad_norm": 0.1627533882856369,
      "learning_rate": 1.7543333333333335e-05,
      "loss": 0.0019,
      "step": 97370
    },
    {
      "epoch": 5.1936,
      "grad_norm": 0.0480315275490284,
      "learning_rate": 1.754e-05,
      "loss": 0.0016,
      "step": 97380
    },
    {
      "epoch": 5.194133333333333,
      "grad_norm": 0.3876364231109619,
      "learning_rate": 1.7536666666666668e-05,
      "loss": 0.0021,
      "step": 97390
    },
    {
      "epoch": 5.1946666666666665,
      "grad_norm": 0.7207362055778503,
      "learning_rate": 1.7533333333333334e-05,
      "loss": 0.0037,
      "step": 97400
    },
    {
      "epoch": 5.1952,
      "grad_norm": 0.06627537310123444,
      "learning_rate": 1.7530000000000003e-05,
      "loss": 0.0019,
      "step": 97410
    },
    {
      "epoch": 5.195733333333333,
      "grad_norm": 0.2432660013437271,
      "learning_rate": 1.7526666666666666e-05,
      "loss": 0.0013,
      "step": 97420
    },
    {
      "epoch": 5.196266666666666,
      "grad_norm": 0.6456621885299683,
      "learning_rate": 1.7523333333333332e-05,
      "loss": 0.0026,
      "step": 97430
    },
    {
      "epoch": 5.1968,
      "grad_norm": 0.9416528344154358,
      "learning_rate": 1.752e-05,
      "loss": 0.0022,
      "step": 97440
    },
    {
      "epoch": 5.197333333333333,
      "grad_norm": 0.0918952077627182,
      "learning_rate": 1.7516666666666668e-05,
      "loss": 0.0023,
      "step": 97450
    },
    {
      "epoch": 5.197866666666667,
      "grad_norm": 0.08503840118646622,
      "learning_rate": 1.7513333333333334e-05,
      "loss": 0.002,
      "step": 97460
    },
    {
      "epoch": 5.1984,
      "grad_norm": 0.48716962337493896,
      "learning_rate": 1.751e-05,
      "loss": 0.0018,
      "step": 97470
    },
    {
      "epoch": 5.198933333333334,
      "grad_norm": 0.35391807556152344,
      "learning_rate": 1.750666666666667e-05,
      "loss": 0.0014,
      "step": 97480
    },
    {
      "epoch": 5.199466666666667,
      "grad_norm": 0.11300980299711227,
      "learning_rate": 1.7503333333333336e-05,
      "loss": 0.0013,
      "step": 97490
    },
    {
      "epoch": 5.2,
      "grad_norm": 0.2800208628177643,
      "learning_rate": 1.75e-05,
      "loss": 0.0026,
      "step": 97500
    },
    {
      "epoch": 5.2005333333333335,
      "grad_norm": 0.2990211844444275,
      "learning_rate": 1.7496666666666668e-05,
      "loss": 0.002,
      "step": 97510
    },
    {
      "epoch": 5.201066666666667,
      "grad_norm": 0.5103642344474792,
      "learning_rate": 1.7493333333333334e-05,
      "loss": 0.0014,
      "step": 97520
    },
    {
      "epoch": 5.2016,
      "grad_norm": 0.20960550010204315,
      "learning_rate": 1.749e-05,
      "loss": 0.0022,
      "step": 97530
    },
    {
      "epoch": 5.202133333333333,
      "grad_norm": 0.4240800440311432,
      "learning_rate": 1.7486666666666666e-05,
      "loss": 0.0025,
      "step": 97540
    },
    {
      "epoch": 5.2026666666666666,
      "grad_norm": 0.24707892537117004,
      "learning_rate": 1.7483333333333336e-05,
      "loss": 0.002,
      "step": 97550
    },
    {
      "epoch": 5.2032,
      "grad_norm": 0.1916114091873169,
      "learning_rate": 1.7480000000000002e-05,
      "loss": 0.0015,
      "step": 97560
    },
    {
      "epoch": 5.203733333333333,
      "grad_norm": 0.16210857033729553,
      "learning_rate": 1.7476666666666665e-05,
      "loss": 0.0018,
      "step": 97570
    },
    {
      "epoch": 5.204266666666666,
      "grad_norm": 0.28102365136146545,
      "learning_rate": 1.7473333333333334e-05,
      "loss": 0.0022,
      "step": 97580
    },
    {
      "epoch": 5.2048,
      "grad_norm": 0.33861008286476135,
      "learning_rate": 1.747e-05,
      "loss": 0.0018,
      "step": 97590
    },
    {
      "epoch": 5.205333333333333,
      "grad_norm": 0.7305597066879272,
      "learning_rate": 1.7466666666666667e-05,
      "loss": 0.0018,
      "step": 97600
    },
    {
      "epoch": 5.205866666666667,
      "grad_norm": 0.2758674621582031,
      "learning_rate": 1.7463333333333333e-05,
      "loss": 0.0019,
      "step": 97610
    },
    {
      "epoch": 5.2064,
      "grad_norm": 0.6306828856468201,
      "learning_rate": 1.7460000000000002e-05,
      "loss": 0.0023,
      "step": 97620
    },
    {
      "epoch": 5.206933333333334,
      "grad_norm": 0.3900148570537567,
      "learning_rate": 1.745666666666667e-05,
      "loss": 0.0015,
      "step": 97630
    },
    {
      "epoch": 5.207466666666667,
      "grad_norm": 0.09834177792072296,
      "learning_rate": 1.7453333333333335e-05,
      "loss": 0.0017,
      "step": 97640
    },
    {
      "epoch": 5.208,
      "grad_norm": 0.1931861937046051,
      "learning_rate": 1.745e-05,
      "loss": 0.0019,
      "step": 97650
    },
    {
      "epoch": 5.2085333333333335,
      "grad_norm": 0.20783475041389465,
      "learning_rate": 1.7446666666666667e-05,
      "loss": 0.0016,
      "step": 97660
    },
    {
      "epoch": 5.209066666666667,
      "grad_norm": 0.2993789315223694,
      "learning_rate": 1.7443333333333333e-05,
      "loss": 0.002,
      "step": 97670
    },
    {
      "epoch": 5.2096,
      "grad_norm": 0.07934629917144775,
      "learning_rate": 1.7440000000000002e-05,
      "loss": 0.0021,
      "step": 97680
    },
    {
      "epoch": 5.210133333333333,
      "grad_norm": 0.2436794638633728,
      "learning_rate": 1.743666666666667e-05,
      "loss": 0.0012,
      "step": 97690
    },
    {
      "epoch": 5.210666666666667,
      "grad_norm": 0.05009689927101135,
      "learning_rate": 1.7433333333333335e-05,
      "loss": 0.0022,
      "step": 97700
    },
    {
      "epoch": 5.2112,
      "grad_norm": 0.39587774872779846,
      "learning_rate": 1.743e-05,
      "loss": 0.0026,
      "step": 97710
    },
    {
      "epoch": 5.211733333333333,
      "grad_norm": 0.4488930106163025,
      "learning_rate": 1.7426666666666667e-05,
      "loss": 0.002,
      "step": 97720
    },
    {
      "epoch": 5.212266666666666,
      "grad_norm": 0.03490700200200081,
      "learning_rate": 1.7423333333333333e-05,
      "loss": 0.002,
      "step": 97730
    },
    {
      "epoch": 5.2128,
      "grad_norm": 0.362371563911438,
      "learning_rate": 1.742e-05,
      "loss": 0.0028,
      "step": 97740
    },
    {
      "epoch": 5.213333333333333,
      "grad_norm": 0.0458042174577713,
      "learning_rate": 1.741666666666667e-05,
      "loss": 0.0013,
      "step": 97750
    },
    {
      "epoch": 5.213866666666667,
      "grad_norm": 0.09423858672380447,
      "learning_rate": 1.7413333333333335e-05,
      "loss": 0.0012,
      "step": 97760
    },
    {
      "epoch": 5.2144,
      "grad_norm": 0.1572588086128235,
      "learning_rate": 1.741e-05,
      "loss": 0.002,
      "step": 97770
    },
    {
      "epoch": 5.214933333333334,
      "grad_norm": 0.42168980836868286,
      "learning_rate": 1.7406666666666667e-05,
      "loss": 0.0022,
      "step": 97780
    },
    {
      "epoch": 5.215466666666667,
      "grad_norm": 0.19930574297904968,
      "learning_rate": 1.7403333333333337e-05,
      "loss": 0.0028,
      "step": 97790
    },
    {
      "epoch": 5.216,
      "grad_norm": 0.3987657427787781,
      "learning_rate": 1.74e-05,
      "loss": 0.0014,
      "step": 97800
    },
    {
      "epoch": 5.2165333333333335,
      "grad_norm": 0.1370822638273239,
      "learning_rate": 1.7396666666666666e-05,
      "loss": 0.0023,
      "step": 97810
    },
    {
      "epoch": 5.217066666666667,
      "grad_norm": 0.389267235994339,
      "learning_rate": 1.7393333333333335e-05,
      "loss": 0.0018,
      "step": 97820
    },
    {
      "epoch": 5.2176,
      "grad_norm": 0.24593323469161987,
      "learning_rate": 1.739e-05,
      "loss": 0.0024,
      "step": 97830
    },
    {
      "epoch": 5.218133333333333,
      "grad_norm": 0.18509824573993683,
      "learning_rate": 1.7386666666666667e-05,
      "loss": 0.0017,
      "step": 97840
    },
    {
      "epoch": 5.218666666666667,
      "grad_norm": 0.47398895025253296,
      "learning_rate": 1.7383333333333333e-05,
      "loss": 0.002,
      "step": 97850
    },
    {
      "epoch": 5.2192,
      "grad_norm": 0.2985357940196991,
      "learning_rate": 1.7380000000000003e-05,
      "loss": 0.0022,
      "step": 97860
    },
    {
      "epoch": 5.219733333333333,
      "grad_norm": 0.36610162258148193,
      "learning_rate": 1.737666666666667e-05,
      "loss": 0.0019,
      "step": 97870
    },
    {
      "epoch": 5.220266666666666,
      "grad_norm": 0.10385547578334808,
      "learning_rate": 1.7373333333333332e-05,
      "loss": 0.0013,
      "step": 97880
    },
    {
      "epoch": 5.2208,
      "grad_norm": 0.2100922167301178,
      "learning_rate": 1.737e-05,
      "loss": 0.0017,
      "step": 97890
    },
    {
      "epoch": 5.221333333333333,
      "grad_norm": 0.39591190218925476,
      "learning_rate": 1.7366666666666668e-05,
      "loss": 0.002,
      "step": 97900
    },
    {
      "epoch": 5.221866666666667,
      "grad_norm": 0.3352566659450531,
      "learning_rate": 1.7363333333333334e-05,
      "loss": 0.0014,
      "step": 97910
    },
    {
      "epoch": 5.2224,
      "grad_norm": 0.1475597321987152,
      "learning_rate": 1.736e-05,
      "loss": 0.0016,
      "step": 97920
    },
    {
      "epoch": 5.222933333333334,
      "grad_norm": 0.47424423694610596,
      "learning_rate": 1.735666666666667e-05,
      "loss": 0.0016,
      "step": 97930
    },
    {
      "epoch": 5.223466666666667,
      "grad_norm": 0.05224372446537018,
      "learning_rate": 1.7353333333333335e-05,
      "loss": 0.0022,
      "step": 97940
    },
    {
      "epoch": 5.224,
      "grad_norm": 0.3711502254009247,
      "learning_rate": 1.7349999999999998e-05,
      "loss": 0.0022,
      "step": 97950
    },
    {
      "epoch": 5.2245333333333335,
      "grad_norm": 0.07360616326332092,
      "learning_rate": 1.7346666666666668e-05,
      "loss": 0.002,
      "step": 97960
    },
    {
      "epoch": 5.225066666666667,
      "grad_norm": 0.37025436758995056,
      "learning_rate": 1.7343333333333334e-05,
      "loss": 0.0016,
      "step": 97970
    },
    {
      "epoch": 5.2256,
      "grad_norm": 0.288174569606781,
      "learning_rate": 1.734e-05,
      "loss": 0.0014,
      "step": 97980
    },
    {
      "epoch": 5.226133333333333,
      "grad_norm": 0.07228690385818481,
      "learning_rate": 1.7336666666666666e-05,
      "loss": 0.0015,
      "step": 97990
    },
    {
      "epoch": 5.226666666666667,
      "grad_norm": 0.31428271532058716,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 0.0017,
      "step": 98000
    },
    {
      "epoch": 5.2272,
      "grad_norm": 0.14582562446594238,
      "learning_rate": 1.7330000000000002e-05,
      "loss": 0.0029,
      "step": 98010
    },
    {
      "epoch": 5.227733333333333,
      "grad_norm": 0.04580660164356232,
      "learning_rate": 1.7326666666666668e-05,
      "loss": 0.0016,
      "step": 98020
    },
    {
      "epoch": 5.228266666666666,
      "grad_norm": 0.08566031605005264,
      "learning_rate": 1.7323333333333334e-05,
      "loss": 0.0021,
      "step": 98030
    },
    {
      "epoch": 5.2288,
      "grad_norm": 0.056196000427007675,
      "learning_rate": 1.732e-05,
      "loss": 0.0014,
      "step": 98040
    },
    {
      "epoch": 5.229333333333333,
      "grad_norm": 0.09615301340818405,
      "learning_rate": 1.7316666666666666e-05,
      "loss": 0.0016,
      "step": 98050
    },
    {
      "epoch": 5.229866666666666,
      "grad_norm": 0.1354425847530365,
      "learning_rate": 1.7313333333333336e-05,
      "loss": 0.0015,
      "step": 98060
    },
    {
      "epoch": 5.2304,
      "grad_norm": 0.1220184862613678,
      "learning_rate": 1.7310000000000002e-05,
      "loss": 0.0015,
      "step": 98070
    },
    {
      "epoch": 5.230933333333334,
      "grad_norm": 0.0517958365380764,
      "learning_rate": 1.7306666666666668e-05,
      "loss": 0.0015,
      "step": 98080
    },
    {
      "epoch": 5.231466666666667,
      "grad_norm": 0.060176145285367966,
      "learning_rate": 1.7303333333333334e-05,
      "loss": 0.0013,
      "step": 98090
    },
    {
      "epoch": 5.232,
      "grad_norm": 0.23952767252922058,
      "learning_rate": 1.73e-05,
      "loss": 0.0014,
      "step": 98100
    },
    {
      "epoch": 5.2325333333333335,
      "grad_norm": 0.15143242478370667,
      "learning_rate": 1.7296666666666667e-05,
      "loss": 0.0018,
      "step": 98110
    },
    {
      "epoch": 5.233066666666667,
      "grad_norm": 0.38785260915756226,
      "learning_rate": 1.7293333333333333e-05,
      "loss": 0.0023,
      "step": 98120
    },
    {
      "epoch": 5.2336,
      "grad_norm": 0.1843632310628891,
      "learning_rate": 1.7290000000000002e-05,
      "loss": 0.0015,
      "step": 98130
    },
    {
      "epoch": 5.234133333333333,
      "grad_norm": 0.44769802689552307,
      "learning_rate": 1.7286666666666668e-05,
      "loss": 0.0027,
      "step": 98140
    },
    {
      "epoch": 5.234666666666667,
      "grad_norm": 0.2047799825668335,
      "learning_rate": 1.7283333333333334e-05,
      "loss": 0.0023,
      "step": 98150
    },
    {
      "epoch": 5.2352,
      "grad_norm": 0.0724906399846077,
      "learning_rate": 1.728e-05,
      "loss": 0.0016,
      "step": 98160
    },
    {
      "epoch": 5.235733333333333,
      "grad_norm": 0.09032440185546875,
      "learning_rate": 1.727666666666667e-05,
      "loss": 0.0021,
      "step": 98170
    },
    {
      "epoch": 5.236266666666666,
      "grad_norm": 0.29880309104919434,
      "learning_rate": 1.7273333333333333e-05,
      "loss": 0.0015,
      "step": 98180
    },
    {
      "epoch": 5.2368,
      "grad_norm": 0.3391019403934479,
      "learning_rate": 1.727e-05,
      "loss": 0.0016,
      "step": 98190
    },
    {
      "epoch": 5.237333333333333,
      "grad_norm": 0.36849984526634216,
      "learning_rate": 1.726666666666667e-05,
      "loss": 0.0017,
      "step": 98200
    },
    {
      "epoch": 5.237866666666667,
      "grad_norm": 0.10897039622068405,
      "learning_rate": 1.7263333333333335e-05,
      "loss": 0.0022,
      "step": 98210
    },
    {
      "epoch": 5.2384,
      "grad_norm": 0.28084269165992737,
      "learning_rate": 1.726e-05,
      "loss": 0.0017,
      "step": 98220
    },
    {
      "epoch": 5.238933333333334,
      "grad_norm": 0.3560801148414612,
      "learning_rate": 1.7256666666666667e-05,
      "loss": 0.0026,
      "step": 98230
    },
    {
      "epoch": 5.239466666666667,
      "grad_norm": 0.18535374104976654,
      "learning_rate": 1.7253333333333336e-05,
      "loss": 0.0018,
      "step": 98240
    },
    {
      "epoch": 5.24,
      "grad_norm": 0.22996866703033447,
      "learning_rate": 1.725e-05,
      "loss": 0.0017,
      "step": 98250
    },
    {
      "epoch": 5.2405333333333335,
      "grad_norm": 0.03781348839402199,
      "learning_rate": 1.7246666666666665e-05,
      "loss": 0.0024,
      "step": 98260
    },
    {
      "epoch": 5.241066666666667,
      "grad_norm": 0.18161463737487793,
      "learning_rate": 1.7243333333333335e-05,
      "loss": 0.0015,
      "step": 98270
    },
    {
      "epoch": 5.2416,
      "grad_norm": 0.25509655475616455,
      "learning_rate": 1.724e-05,
      "loss": 0.0015,
      "step": 98280
    },
    {
      "epoch": 5.242133333333333,
      "grad_norm": 0.03011038713157177,
      "learning_rate": 1.7236666666666667e-05,
      "loss": 0.0022,
      "step": 98290
    },
    {
      "epoch": 5.242666666666667,
      "grad_norm": 0.060931332409381866,
      "learning_rate": 1.7233333333333333e-05,
      "loss": 0.0023,
      "step": 98300
    },
    {
      "epoch": 5.2432,
      "grad_norm": 0.10177340358495712,
      "learning_rate": 1.7230000000000003e-05,
      "loss": 0.0024,
      "step": 98310
    },
    {
      "epoch": 5.243733333333333,
      "grad_norm": 0.16102705895900726,
      "learning_rate": 1.722666666666667e-05,
      "loss": 0.0021,
      "step": 98320
    },
    {
      "epoch": 5.244266666666666,
      "grad_norm": 0.48784711956977844,
      "learning_rate": 1.722333333333333e-05,
      "loss": 0.0016,
      "step": 98330
    },
    {
      "epoch": 5.2448,
      "grad_norm": 0.2170020043849945,
      "learning_rate": 1.722e-05,
      "loss": 0.0026,
      "step": 98340
    },
    {
      "epoch": 5.245333333333333,
      "grad_norm": 0.0595761276781559,
      "learning_rate": 1.7216666666666667e-05,
      "loss": 0.0015,
      "step": 98350
    },
    {
      "epoch": 5.245866666666666,
      "grad_norm": 0.04442259296774864,
      "learning_rate": 1.7213333333333333e-05,
      "loss": 0.0015,
      "step": 98360
    },
    {
      "epoch": 5.2464,
      "grad_norm": 0.24878227710723877,
      "learning_rate": 1.721e-05,
      "loss": 0.0026,
      "step": 98370
    },
    {
      "epoch": 5.246933333333334,
      "grad_norm": 0.2587442994117737,
      "learning_rate": 1.720666666666667e-05,
      "loss": 0.0027,
      "step": 98380
    },
    {
      "epoch": 5.247466666666667,
      "grad_norm": 0.24519579112529755,
      "learning_rate": 1.7203333333333335e-05,
      "loss": 0.0022,
      "step": 98390
    },
    {
      "epoch": 5.248,
      "grad_norm": 0.40319693088531494,
      "learning_rate": 1.7199999999999998e-05,
      "loss": 0.003,
      "step": 98400
    },
    {
      "epoch": 5.2485333333333335,
      "grad_norm": 0.42988428473472595,
      "learning_rate": 1.7196666666666667e-05,
      "loss": 0.0025,
      "step": 98410
    },
    {
      "epoch": 5.249066666666667,
      "grad_norm": 0.20771098136901855,
      "learning_rate": 1.7193333333333334e-05,
      "loss": 0.0014,
      "step": 98420
    },
    {
      "epoch": 5.2496,
      "grad_norm": 0.4996088445186615,
      "learning_rate": 1.719e-05,
      "loss": 0.0021,
      "step": 98430
    },
    {
      "epoch": 5.250133333333333,
      "grad_norm": 0.08003924787044525,
      "learning_rate": 1.718666666666667e-05,
      "loss": 0.0019,
      "step": 98440
    },
    {
      "epoch": 5.250666666666667,
      "grad_norm": 0.15836425125598907,
      "learning_rate": 1.7183333333333335e-05,
      "loss": 0.0015,
      "step": 98450
    },
    {
      "epoch": 5.2512,
      "grad_norm": 0.1268434077501297,
      "learning_rate": 1.718e-05,
      "loss": 0.0021,
      "step": 98460
    },
    {
      "epoch": 5.251733333333333,
      "grad_norm": 0.15443448722362518,
      "learning_rate": 1.7176666666666668e-05,
      "loss": 0.0016,
      "step": 98470
    },
    {
      "epoch": 5.252266666666666,
      "grad_norm": 0.1186317428946495,
      "learning_rate": 1.7173333333333334e-05,
      "loss": 0.0018,
      "step": 98480
    },
    {
      "epoch": 5.2528,
      "grad_norm": 0.1133003905415535,
      "learning_rate": 1.717e-05,
      "loss": 0.0019,
      "step": 98490
    },
    {
      "epoch": 5.253333333333333,
      "grad_norm": 0.05439605563879013,
      "learning_rate": 1.7166666666666666e-05,
      "loss": 0.0026,
      "step": 98500
    },
    {
      "epoch": 5.253866666666667,
      "grad_norm": 0.2733789384365082,
      "learning_rate": 1.7163333333333336e-05,
      "loss": 0.0016,
      "step": 98510
    },
    {
      "epoch": 5.2544,
      "grad_norm": 0.07205968350172043,
      "learning_rate": 1.7160000000000002e-05,
      "loss": 0.0017,
      "step": 98520
    },
    {
      "epoch": 5.254933333333334,
      "grad_norm": 0.5488356351852417,
      "learning_rate": 1.7156666666666668e-05,
      "loss": 0.0016,
      "step": 98530
    },
    {
      "epoch": 5.255466666666667,
      "grad_norm": 0.23944821953773499,
      "learning_rate": 1.7153333333333334e-05,
      "loss": 0.0016,
      "step": 98540
    },
    {
      "epoch": 5.256,
      "grad_norm": 0.13093730807304382,
      "learning_rate": 1.7150000000000004e-05,
      "loss": 0.0025,
      "step": 98550
    },
    {
      "epoch": 5.2565333333333335,
      "grad_norm": 0.18344822525978088,
      "learning_rate": 1.7146666666666666e-05,
      "loss": 0.0014,
      "step": 98560
    },
    {
      "epoch": 5.257066666666667,
      "grad_norm": 0.20585745573043823,
      "learning_rate": 1.7143333333333332e-05,
      "loss": 0.0016,
      "step": 98570
    },
    {
      "epoch": 5.2576,
      "grad_norm": 0.09342470020055771,
      "learning_rate": 1.7140000000000002e-05,
      "loss": 0.0017,
      "step": 98580
    },
    {
      "epoch": 5.258133333333333,
      "grad_norm": 0.03774416074156761,
      "learning_rate": 1.7136666666666668e-05,
      "loss": 0.0022,
      "step": 98590
    },
    {
      "epoch": 5.258666666666667,
      "grad_norm": 0.07077808678150177,
      "learning_rate": 1.7133333333333334e-05,
      "loss": 0.0013,
      "step": 98600
    },
    {
      "epoch": 5.2592,
      "grad_norm": 0.48153066635131836,
      "learning_rate": 1.713e-05,
      "loss": 0.0015,
      "step": 98610
    },
    {
      "epoch": 5.259733333333333,
      "grad_norm": 0.09231600165367126,
      "learning_rate": 1.712666666666667e-05,
      "loss": 0.003,
      "step": 98620
    },
    {
      "epoch": 5.260266666666666,
      "grad_norm": 0.4191984534263611,
      "learning_rate": 1.7123333333333333e-05,
      "loss": 0.0016,
      "step": 98630
    },
    {
      "epoch": 5.2608,
      "grad_norm": 0.14098547399044037,
      "learning_rate": 1.712e-05,
      "loss": 0.0015,
      "step": 98640
    },
    {
      "epoch": 5.261333333333333,
      "grad_norm": 0.05377284437417984,
      "learning_rate": 1.7116666666666668e-05,
      "loss": 0.0027,
      "step": 98650
    },
    {
      "epoch": 5.261866666666666,
      "grad_norm": 0.08699380606412888,
      "learning_rate": 1.7113333333333334e-05,
      "loss": 0.0017,
      "step": 98660
    },
    {
      "epoch": 5.2624,
      "grad_norm": 0.04524475336074829,
      "learning_rate": 1.711e-05,
      "loss": 0.0019,
      "step": 98670
    },
    {
      "epoch": 5.262933333333334,
      "grad_norm": 0.04196183383464813,
      "learning_rate": 1.7106666666666667e-05,
      "loss": 0.0011,
      "step": 98680
    },
    {
      "epoch": 5.263466666666667,
      "grad_norm": 0.09694638848304749,
      "learning_rate": 1.7103333333333336e-05,
      "loss": 0.0019,
      "step": 98690
    },
    {
      "epoch": 5.264,
      "grad_norm": 0.21151083707809448,
      "learning_rate": 1.7100000000000002e-05,
      "loss": 0.0022,
      "step": 98700
    },
    {
      "epoch": 5.2645333333333335,
      "grad_norm": 0.18489894270896912,
      "learning_rate": 1.7096666666666665e-05,
      "loss": 0.0015,
      "step": 98710
    },
    {
      "epoch": 5.265066666666667,
      "grad_norm": 0.3504278361797333,
      "learning_rate": 1.7093333333333335e-05,
      "loss": 0.0019,
      "step": 98720
    },
    {
      "epoch": 5.2656,
      "grad_norm": 0.4007889926433563,
      "learning_rate": 1.709e-05,
      "loss": 0.0018,
      "step": 98730
    },
    {
      "epoch": 5.266133333333333,
      "grad_norm": 0.09584242850542068,
      "learning_rate": 1.7086666666666667e-05,
      "loss": 0.0018,
      "step": 98740
    },
    {
      "epoch": 5.266666666666667,
      "grad_norm": 0.24380062520503998,
      "learning_rate": 1.7083333333333333e-05,
      "loss": 0.0019,
      "step": 98750
    },
    {
      "epoch": 5.2672,
      "grad_norm": 0.15921780467033386,
      "learning_rate": 1.7080000000000002e-05,
      "loss": 0.002,
      "step": 98760
    },
    {
      "epoch": 5.267733333333333,
      "grad_norm": 0.07142133265733719,
      "learning_rate": 1.707666666666667e-05,
      "loss": 0.0021,
      "step": 98770
    },
    {
      "epoch": 5.268266666666666,
      "grad_norm": 0.2376602739095688,
      "learning_rate": 1.707333333333333e-05,
      "loss": 0.0014,
      "step": 98780
    },
    {
      "epoch": 5.2688,
      "grad_norm": 0.17927128076553345,
      "learning_rate": 1.707e-05,
      "loss": 0.0014,
      "step": 98790
    },
    {
      "epoch": 5.269333333333333,
      "grad_norm": 0.4623725712299347,
      "learning_rate": 1.7066666666666667e-05,
      "loss": 0.002,
      "step": 98800
    },
    {
      "epoch": 5.269866666666666,
      "grad_norm": 0.15228937566280365,
      "learning_rate": 1.7063333333333333e-05,
      "loss": 0.0012,
      "step": 98810
    },
    {
      "epoch": 5.2704,
      "grad_norm": 0.15379662811756134,
      "learning_rate": 1.706e-05,
      "loss": 0.0013,
      "step": 98820
    },
    {
      "epoch": 5.270933333333334,
      "grad_norm": 0.6420380473136902,
      "learning_rate": 1.705666666666667e-05,
      "loss": 0.0026,
      "step": 98830
    },
    {
      "epoch": 5.271466666666667,
      "grad_norm": 0.04824068397283554,
      "learning_rate": 1.7053333333333335e-05,
      "loss": 0.0015,
      "step": 98840
    },
    {
      "epoch": 5.272,
      "grad_norm": 0.03860243409872055,
      "learning_rate": 1.705e-05,
      "loss": 0.0021,
      "step": 98850
    },
    {
      "epoch": 5.2725333333333335,
      "grad_norm": 0.02565591409802437,
      "learning_rate": 1.7046666666666667e-05,
      "loss": 0.0016,
      "step": 98860
    },
    {
      "epoch": 5.273066666666667,
      "grad_norm": 0.21525950729846954,
      "learning_rate": 1.7043333333333333e-05,
      "loss": 0.0019,
      "step": 98870
    },
    {
      "epoch": 5.2736,
      "grad_norm": 0.21716827154159546,
      "learning_rate": 1.704e-05,
      "loss": 0.0023,
      "step": 98880
    },
    {
      "epoch": 5.274133333333333,
      "grad_norm": 0.23088273406028748,
      "learning_rate": 1.703666666666667e-05,
      "loss": 0.0023,
      "step": 98890
    },
    {
      "epoch": 5.274666666666667,
      "grad_norm": 0.12695330381393433,
      "learning_rate": 1.7033333333333335e-05,
      "loss": 0.0018,
      "step": 98900
    },
    {
      "epoch": 5.2752,
      "grad_norm": 0.334168016910553,
      "learning_rate": 1.703e-05,
      "loss": 0.0015,
      "step": 98910
    },
    {
      "epoch": 5.275733333333333,
      "grad_norm": 0.15182611346244812,
      "learning_rate": 1.7026666666666667e-05,
      "loss": 0.002,
      "step": 98920
    },
    {
      "epoch": 5.276266666666666,
      "grad_norm": 0.559786319732666,
      "learning_rate": 1.7023333333333334e-05,
      "loss": 0.0022,
      "step": 98930
    },
    {
      "epoch": 5.2768,
      "grad_norm": 0.15373510122299194,
      "learning_rate": 1.702e-05,
      "loss": 0.0024,
      "step": 98940
    },
    {
      "epoch": 5.277333333333333,
      "grad_norm": 0.06543979793787003,
      "learning_rate": 1.7016666666666666e-05,
      "loss": 0.0012,
      "step": 98950
    },
    {
      "epoch": 5.277866666666666,
      "grad_norm": 0.10102479159832001,
      "learning_rate": 1.7013333333333335e-05,
      "loss": 0.0022,
      "step": 98960
    },
    {
      "epoch": 5.2783999999999995,
      "grad_norm": 0.362705260515213,
      "learning_rate": 1.701e-05,
      "loss": 0.0016,
      "step": 98970
    },
    {
      "epoch": 5.278933333333334,
      "grad_norm": 0.4399101138114929,
      "learning_rate": 1.7006666666666668e-05,
      "loss": 0.002,
      "step": 98980
    },
    {
      "epoch": 5.279466666666667,
      "grad_norm": 0.361611545085907,
      "learning_rate": 1.7003333333333334e-05,
      "loss": 0.0027,
      "step": 98990
    },
    {
      "epoch": 5.28,
      "grad_norm": 0.09300577640533447,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.0022,
      "step": 99000
    },
    {
      "epoch": 5.2805333333333335,
      "grad_norm": 0.09198421984910965,
      "learning_rate": 1.6996666666666666e-05,
      "loss": 0.0018,
      "step": 99010
    },
    {
      "epoch": 5.281066666666667,
      "grad_norm": 0.40534448623657227,
      "learning_rate": 1.6993333333333332e-05,
      "loss": 0.0019,
      "step": 99020
    },
    {
      "epoch": 5.2816,
      "grad_norm": 0.1509595364332199,
      "learning_rate": 1.699e-05,
      "loss": 0.0015,
      "step": 99030
    },
    {
      "epoch": 5.282133333333333,
      "grad_norm": 0.19419154524803162,
      "learning_rate": 1.6986666666666668e-05,
      "loss": 0.0021,
      "step": 99040
    },
    {
      "epoch": 5.282666666666667,
      "grad_norm": 0.20308741927146912,
      "learning_rate": 1.6983333333333334e-05,
      "loss": 0.0017,
      "step": 99050
    },
    {
      "epoch": 5.2832,
      "grad_norm": 0.16521486639976501,
      "learning_rate": 1.698e-05,
      "loss": 0.0015,
      "step": 99060
    },
    {
      "epoch": 5.283733333333333,
      "grad_norm": 0.03801577910780907,
      "learning_rate": 1.697666666666667e-05,
      "loss": 0.0018,
      "step": 99070
    },
    {
      "epoch": 5.2842666666666664,
      "grad_norm": 0.1222773939371109,
      "learning_rate": 1.6973333333333336e-05,
      "loss": 0.0018,
      "step": 99080
    },
    {
      "epoch": 5.2848,
      "grad_norm": 0.04165777564048767,
      "learning_rate": 1.697e-05,
      "loss": 0.0013,
      "step": 99090
    },
    {
      "epoch": 5.285333333333333,
      "grad_norm": 0.05088283494114876,
      "learning_rate": 1.6966666666666668e-05,
      "loss": 0.0013,
      "step": 99100
    },
    {
      "epoch": 5.285866666666666,
      "grad_norm": 0.3298015892505646,
      "learning_rate": 1.6963333333333334e-05,
      "loss": 0.0022,
      "step": 99110
    },
    {
      "epoch": 5.2864,
      "grad_norm": 0.3097880482673645,
      "learning_rate": 1.696e-05,
      "loss": 0.0028,
      "step": 99120
    },
    {
      "epoch": 5.286933333333334,
      "grad_norm": 0.10940075665712357,
      "learning_rate": 1.6956666666666666e-05,
      "loss": 0.0018,
      "step": 99130
    },
    {
      "epoch": 5.287466666666667,
      "grad_norm": 0.5256005525588989,
      "learning_rate": 1.6953333333333336e-05,
      "loss": 0.0016,
      "step": 99140
    },
    {
      "epoch": 5.288,
      "grad_norm": 0.206640362739563,
      "learning_rate": 1.6950000000000002e-05,
      "loss": 0.002,
      "step": 99150
    },
    {
      "epoch": 5.2885333333333335,
      "grad_norm": 0.17997422814369202,
      "learning_rate": 1.6946666666666665e-05,
      "loss": 0.0016,
      "step": 99160
    },
    {
      "epoch": 5.289066666666667,
      "grad_norm": 0.24341966211795807,
      "learning_rate": 1.6943333333333334e-05,
      "loss": 0.0027,
      "step": 99170
    },
    {
      "epoch": 5.2896,
      "grad_norm": 0.1306462287902832,
      "learning_rate": 1.694e-05,
      "loss": 0.0014,
      "step": 99180
    },
    {
      "epoch": 5.290133333333333,
      "grad_norm": 0.1592080444097519,
      "learning_rate": 1.6936666666666667e-05,
      "loss": 0.002,
      "step": 99190
    },
    {
      "epoch": 5.290666666666667,
      "grad_norm": 0.24037031829357147,
      "learning_rate": 1.6933333333333333e-05,
      "loss": 0.0027,
      "step": 99200
    },
    {
      "epoch": 5.2912,
      "grad_norm": 0.16981731355190277,
      "learning_rate": 1.6930000000000002e-05,
      "loss": 0.0023,
      "step": 99210
    },
    {
      "epoch": 5.291733333333333,
      "grad_norm": 0.7075090408325195,
      "learning_rate": 1.692666666666667e-05,
      "loss": 0.0018,
      "step": 99220
    },
    {
      "epoch": 5.2922666666666665,
      "grad_norm": 0.043956208974123,
      "learning_rate": 1.6923333333333334e-05,
      "loss": 0.0012,
      "step": 99230
    },
    {
      "epoch": 5.2928,
      "grad_norm": 0.12756885588169098,
      "learning_rate": 1.692e-05,
      "loss": 0.0023,
      "step": 99240
    },
    {
      "epoch": 5.293333333333333,
      "grad_norm": 0.4585053026676178,
      "learning_rate": 1.6916666666666667e-05,
      "loss": 0.0015,
      "step": 99250
    },
    {
      "epoch": 5.293866666666666,
      "grad_norm": 0.4197179675102234,
      "learning_rate": 1.6913333333333333e-05,
      "loss": 0.0023,
      "step": 99260
    },
    {
      "epoch": 5.2943999999999996,
      "grad_norm": 0.0663733258843422,
      "learning_rate": 1.6910000000000002e-05,
      "loss": 0.0014,
      "step": 99270
    },
    {
      "epoch": 5.294933333333334,
      "grad_norm": 0.21270565688610077,
      "learning_rate": 1.690666666666667e-05,
      "loss": 0.0015,
      "step": 99280
    },
    {
      "epoch": 5.295466666666667,
      "grad_norm": 0.14911356568336487,
      "learning_rate": 1.6903333333333335e-05,
      "loss": 0.0019,
      "step": 99290
    },
    {
      "epoch": 5.296,
      "grad_norm": 0.08219878375530243,
      "learning_rate": 1.69e-05,
      "loss": 0.0019,
      "step": 99300
    },
    {
      "epoch": 5.2965333333333335,
      "grad_norm": 0.2654118537902832,
      "learning_rate": 1.6896666666666667e-05,
      "loss": 0.0021,
      "step": 99310
    },
    {
      "epoch": 5.297066666666667,
      "grad_norm": 0.12567299604415894,
      "learning_rate": 1.6893333333333333e-05,
      "loss": 0.0021,
      "step": 99320
    },
    {
      "epoch": 5.2976,
      "grad_norm": 0.4560006558895111,
      "learning_rate": 1.689e-05,
      "loss": 0.0022,
      "step": 99330
    },
    {
      "epoch": 5.298133333333333,
      "grad_norm": 0.4229799807071686,
      "learning_rate": 1.688666666666667e-05,
      "loss": 0.0016,
      "step": 99340
    },
    {
      "epoch": 5.298666666666667,
      "grad_norm": 0.8049671649932861,
      "learning_rate": 1.6883333333333335e-05,
      "loss": 0.0019,
      "step": 99350
    },
    {
      "epoch": 5.2992,
      "grad_norm": 0.07044895738363266,
      "learning_rate": 1.688e-05,
      "loss": 0.0023,
      "step": 99360
    },
    {
      "epoch": 5.299733333333333,
      "grad_norm": 0.04740836098790169,
      "learning_rate": 1.6876666666666667e-05,
      "loss": 0.0022,
      "step": 99370
    },
    {
      "epoch": 5.3002666666666665,
      "grad_norm": 0.3851998746395111,
      "learning_rate": 1.6873333333333337e-05,
      "loss": 0.0021,
      "step": 99380
    },
    {
      "epoch": 5.3008,
      "grad_norm": 0.42418819665908813,
      "learning_rate": 1.687e-05,
      "loss": 0.0016,
      "step": 99390
    },
    {
      "epoch": 5.301333333333333,
      "grad_norm": 0.12231951206922531,
      "learning_rate": 1.6866666666666666e-05,
      "loss": 0.002,
      "step": 99400
    },
    {
      "epoch": 5.301866666666666,
      "grad_norm": 0.2525116205215454,
      "learning_rate": 1.6863333333333335e-05,
      "loss": 0.0018,
      "step": 99410
    },
    {
      "epoch": 5.3024000000000004,
      "grad_norm": 0.21153871715068817,
      "learning_rate": 1.686e-05,
      "loss": 0.0026,
      "step": 99420
    },
    {
      "epoch": 5.302933333333334,
      "grad_norm": 0.196033775806427,
      "learning_rate": 1.6856666666666667e-05,
      "loss": 0.0014,
      "step": 99430
    },
    {
      "epoch": 5.303466666666667,
      "grad_norm": 0.09437486529350281,
      "learning_rate": 1.6853333333333333e-05,
      "loss": 0.0021,
      "step": 99440
    },
    {
      "epoch": 5.304,
      "grad_norm": 0.1814199835062027,
      "learning_rate": 1.6850000000000003e-05,
      "loss": 0.0024,
      "step": 99450
    },
    {
      "epoch": 5.3045333333333335,
      "grad_norm": 0.1004684641957283,
      "learning_rate": 1.6846666666666666e-05,
      "loss": 0.0015,
      "step": 99460
    },
    {
      "epoch": 5.305066666666667,
      "grad_norm": 0.0881013497710228,
      "learning_rate": 1.6843333333333332e-05,
      "loss": 0.0015,
      "step": 99470
    },
    {
      "epoch": 5.3056,
      "grad_norm": 0.40160849690437317,
      "learning_rate": 1.684e-05,
      "loss": 0.0026,
      "step": 99480
    },
    {
      "epoch": 5.306133333333333,
      "grad_norm": 0.13338561356067657,
      "learning_rate": 1.6836666666666668e-05,
      "loss": 0.0015,
      "step": 99490
    },
    {
      "epoch": 5.306666666666667,
      "grad_norm": 0.23043876886367798,
      "learning_rate": 1.6833333333333334e-05,
      "loss": 0.0026,
      "step": 99500
    },
    {
      "epoch": 5.3072,
      "grad_norm": 0.10536978393793106,
      "learning_rate": 1.683e-05,
      "loss": 0.0013,
      "step": 99510
    },
    {
      "epoch": 5.307733333333333,
      "grad_norm": 0.3331191837787628,
      "learning_rate": 1.682666666666667e-05,
      "loss": 0.0019,
      "step": 99520
    },
    {
      "epoch": 5.3082666666666665,
      "grad_norm": 0.4461826980113983,
      "learning_rate": 1.6823333333333335e-05,
      "loss": 0.0013,
      "step": 99530
    },
    {
      "epoch": 5.3088,
      "grad_norm": 0.33874639868736267,
      "learning_rate": 1.6819999999999998e-05,
      "loss": 0.002,
      "step": 99540
    },
    {
      "epoch": 5.309333333333333,
      "grad_norm": 0.24352926015853882,
      "learning_rate": 1.6816666666666668e-05,
      "loss": 0.002,
      "step": 99550
    },
    {
      "epoch": 5.309866666666666,
      "grad_norm": 0.46731433272361755,
      "learning_rate": 1.6813333333333334e-05,
      "loss": 0.0019,
      "step": 99560
    },
    {
      "epoch": 5.3104,
      "grad_norm": 0.09864391386508942,
      "learning_rate": 1.681e-05,
      "loss": 0.0018,
      "step": 99570
    },
    {
      "epoch": 5.310933333333334,
      "grad_norm": 0.24168163537979126,
      "learning_rate": 1.6806666666666666e-05,
      "loss": 0.0014,
      "step": 99580
    },
    {
      "epoch": 5.311466666666667,
      "grad_norm": 0.4276854991912842,
      "learning_rate": 1.6803333333333336e-05,
      "loss": 0.0015,
      "step": 99590
    },
    {
      "epoch": 5.312,
      "grad_norm": 0.22769208252429962,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 0.0019,
      "step": 99600
    },
    {
      "epoch": 5.3125333333333336,
      "grad_norm": 0.48495975136756897,
      "learning_rate": 1.6796666666666665e-05,
      "loss": 0.0036,
      "step": 99610
    },
    {
      "epoch": 5.313066666666667,
      "grad_norm": 0.47581833600997925,
      "learning_rate": 1.6793333333333334e-05,
      "loss": 0.0015,
      "step": 99620
    },
    {
      "epoch": 5.3136,
      "grad_norm": 0.21926400065422058,
      "learning_rate": 1.679e-05,
      "loss": 0.0019,
      "step": 99630
    },
    {
      "epoch": 5.314133333333333,
      "grad_norm": 0.45262354612350464,
      "learning_rate": 1.6786666666666666e-05,
      "loss": 0.0017,
      "step": 99640
    },
    {
      "epoch": 5.314666666666667,
      "grad_norm": 0.12554512917995453,
      "learning_rate": 1.6783333333333336e-05,
      "loss": 0.0019,
      "step": 99650
    },
    {
      "epoch": 5.3152,
      "grad_norm": 0.13219672441482544,
      "learning_rate": 1.6780000000000002e-05,
      "loss": 0.0015,
      "step": 99660
    },
    {
      "epoch": 5.315733333333333,
      "grad_norm": 0.10232904553413391,
      "learning_rate": 1.6776666666666668e-05,
      "loss": 0.0021,
      "step": 99670
    },
    {
      "epoch": 5.3162666666666665,
      "grad_norm": 0.2961088716983795,
      "learning_rate": 1.6773333333333334e-05,
      "loss": 0.0015,
      "step": 99680
    },
    {
      "epoch": 5.3168,
      "grad_norm": 0.05299647897481918,
      "learning_rate": 1.677e-05,
      "loss": 0.002,
      "step": 99690
    },
    {
      "epoch": 5.317333333333333,
      "grad_norm": 0.04130669683218002,
      "learning_rate": 1.6766666666666667e-05,
      "loss": 0.0021,
      "step": 99700
    },
    {
      "epoch": 5.317866666666666,
      "grad_norm": 0.45623061060905457,
      "learning_rate": 1.6763333333333333e-05,
      "loss": 0.0018,
      "step": 99710
    },
    {
      "epoch": 5.3184000000000005,
      "grad_norm": 0.15085285902023315,
      "learning_rate": 1.6760000000000002e-05,
      "loss": 0.0025,
      "step": 99720
    },
    {
      "epoch": 5.318933333333334,
      "grad_norm": 0.22616612911224365,
      "learning_rate": 1.6756666666666668e-05,
      "loss": 0.0016,
      "step": 99730
    },
    {
      "epoch": 5.319466666666667,
      "grad_norm": 0.06980253756046295,
      "learning_rate": 1.6753333333333334e-05,
      "loss": 0.0014,
      "step": 99740
    },
    {
      "epoch": 5.32,
      "grad_norm": 0.15745823085308075,
      "learning_rate": 1.675e-05,
      "loss": 0.0018,
      "step": 99750
    },
    {
      "epoch": 5.320533333333334,
      "grad_norm": 0.33157896995544434,
      "learning_rate": 1.674666666666667e-05,
      "loss": 0.0021,
      "step": 99760
    },
    {
      "epoch": 5.321066666666667,
      "grad_norm": 0.09520505368709564,
      "learning_rate": 1.6743333333333333e-05,
      "loss": 0.0022,
      "step": 99770
    },
    {
      "epoch": 5.3216,
      "grad_norm": 0.22460706532001495,
      "learning_rate": 1.674e-05,
      "loss": 0.0023,
      "step": 99780
    },
    {
      "epoch": 5.322133333333333,
      "grad_norm": 0.22601556777954102,
      "learning_rate": 1.673666666666667e-05,
      "loss": 0.0016,
      "step": 99790
    },
    {
      "epoch": 5.322666666666667,
      "grad_norm": 0.09397491812705994,
      "learning_rate": 1.6733333333333335e-05,
      "loss": 0.0026,
      "step": 99800
    },
    {
      "epoch": 5.3232,
      "grad_norm": 0.13174539804458618,
      "learning_rate": 1.673e-05,
      "loss": 0.0015,
      "step": 99810
    },
    {
      "epoch": 5.323733333333333,
      "grad_norm": 0.12496628612279892,
      "learning_rate": 1.6726666666666667e-05,
      "loss": 0.0024,
      "step": 99820
    },
    {
      "epoch": 5.3242666666666665,
      "grad_norm": 0.12244731187820435,
      "learning_rate": 1.6723333333333336e-05,
      "loss": 0.0016,
      "step": 99830
    },
    {
      "epoch": 5.3248,
      "grad_norm": 0.32402363419532776,
      "learning_rate": 1.672e-05,
      "loss": 0.0025,
      "step": 99840
    },
    {
      "epoch": 5.325333333333333,
      "grad_norm": 0.039072781801223755,
      "learning_rate": 1.6716666666666665e-05,
      "loss": 0.0028,
      "step": 99850
    },
    {
      "epoch": 5.325866666666666,
      "grad_norm": 0.2745138108730316,
      "learning_rate": 1.6713333333333335e-05,
      "loss": 0.0023,
      "step": 99860
    },
    {
      "epoch": 5.3264,
      "grad_norm": 0.23997534811496735,
      "learning_rate": 1.671e-05,
      "loss": 0.0018,
      "step": 99870
    },
    {
      "epoch": 5.326933333333334,
      "grad_norm": 0.0734797790646553,
      "learning_rate": 1.6706666666666667e-05,
      "loss": 0.0018,
      "step": 99880
    },
    {
      "epoch": 5.327466666666667,
      "grad_norm": 0.0991005226969719,
      "learning_rate": 1.6703333333333333e-05,
      "loss": 0.0015,
      "step": 99890
    },
    {
      "epoch": 5.328,
      "grad_norm": 0.4518127143383026,
      "learning_rate": 1.6700000000000003e-05,
      "loss": 0.0018,
      "step": 99900
    },
    {
      "epoch": 5.328533333333334,
      "grad_norm": 0.4888148009777069,
      "learning_rate": 1.669666666666667e-05,
      "loss": 0.0017,
      "step": 99910
    },
    {
      "epoch": 5.329066666666667,
      "grad_norm": 0.2802325487136841,
      "learning_rate": 1.669333333333333e-05,
      "loss": 0.0015,
      "step": 99920
    },
    {
      "epoch": 5.3296,
      "grad_norm": 0.23381927609443665,
      "learning_rate": 1.669e-05,
      "loss": 0.002,
      "step": 99930
    },
    {
      "epoch": 5.330133333333333,
      "grad_norm": 0.3794836103916168,
      "learning_rate": 1.6686666666666667e-05,
      "loss": 0.0016,
      "step": 99940
    },
    {
      "epoch": 5.330666666666667,
      "grad_norm": 0.4598221182823181,
      "learning_rate": 1.6683333333333333e-05,
      "loss": 0.0028,
      "step": 99950
    },
    {
      "epoch": 5.3312,
      "grad_norm": 0.0692964494228363,
      "learning_rate": 1.668e-05,
      "loss": 0.0019,
      "step": 99960
    },
    {
      "epoch": 5.331733333333333,
      "grad_norm": 0.5412449240684509,
      "learning_rate": 1.667666666666667e-05,
      "loss": 0.0016,
      "step": 99970
    },
    {
      "epoch": 5.3322666666666665,
      "grad_norm": 0.18421044945716858,
      "learning_rate": 1.6673333333333335e-05,
      "loss": 0.0019,
      "step": 99980
    },
    {
      "epoch": 5.3328,
      "grad_norm": 0.2355508655309677,
      "learning_rate": 1.6669999999999998e-05,
      "loss": 0.0025,
      "step": 99990
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 0.4590553045272827,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.0021,
      "step": 100000
    },
    {
      "epoch": 5.333866666666666,
      "grad_norm": 0.1444307565689087,
      "learning_rate": 1.6663333333333334e-05,
      "loss": 0.0016,
      "step": 100010
    },
    {
      "epoch": 5.3344,
      "grad_norm": 0.40045684576034546,
      "learning_rate": 1.666e-05,
      "loss": 0.0011,
      "step": 100020
    },
    {
      "epoch": 5.334933333333334,
      "grad_norm": 0.22922739386558533,
      "learning_rate": 1.665666666666667e-05,
      "loss": 0.0022,
      "step": 100030
    },
    {
      "epoch": 5.335466666666667,
      "grad_norm": 0.15084253251552582,
      "learning_rate": 1.6653333333333335e-05,
      "loss": 0.0016,
      "step": 100040
    },
    {
      "epoch": 5.336,
      "grad_norm": 0.2711585760116577,
      "learning_rate": 1.665e-05,
      "loss": 0.002,
      "step": 100050
    },
    {
      "epoch": 5.336533333333334,
      "grad_norm": 0.3090931177139282,
      "learning_rate": 1.6646666666666668e-05,
      "loss": 0.0022,
      "step": 100060
    },
    {
      "epoch": 5.337066666666667,
      "grad_norm": 0.22928553819656372,
      "learning_rate": 1.6643333333333334e-05,
      "loss": 0.0015,
      "step": 100070
    },
    {
      "epoch": 5.3376,
      "grad_norm": 0.037556156516075134,
      "learning_rate": 1.664e-05,
      "loss": 0.0019,
      "step": 100080
    },
    {
      "epoch": 5.338133333333333,
      "grad_norm": 0.2995840907096863,
      "learning_rate": 1.6636666666666666e-05,
      "loss": 0.0014,
      "step": 100090
    },
    {
      "epoch": 5.338666666666667,
      "grad_norm": 0.22349220514297485,
      "learning_rate": 1.6633333333333336e-05,
      "loss": 0.0022,
      "step": 100100
    },
    {
      "epoch": 5.3392,
      "grad_norm": 0.27143698930740356,
      "learning_rate": 1.6630000000000002e-05,
      "loss": 0.0013,
      "step": 100110
    },
    {
      "epoch": 5.339733333333333,
      "grad_norm": 0.0466434583067894,
      "learning_rate": 1.6626666666666668e-05,
      "loss": 0.0025,
      "step": 100120
    },
    {
      "epoch": 5.3402666666666665,
      "grad_norm": 0.15320366621017456,
      "learning_rate": 1.6623333333333334e-05,
      "loss": 0.0019,
      "step": 100130
    },
    {
      "epoch": 5.3408,
      "grad_norm": 0.25206705927848816,
      "learning_rate": 1.662e-05,
      "loss": 0.0017,
      "step": 100140
    },
    {
      "epoch": 5.341333333333333,
      "grad_norm": 0.1945524364709854,
      "learning_rate": 1.6616666666666666e-05,
      "loss": 0.0015,
      "step": 100150
    },
    {
      "epoch": 5.341866666666666,
      "grad_norm": 0.3317722976207733,
      "learning_rate": 1.6613333333333332e-05,
      "loss": 0.0015,
      "step": 100160
    },
    {
      "epoch": 5.3424,
      "grad_norm": 0.10003869980573654,
      "learning_rate": 1.6610000000000002e-05,
      "loss": 0.002,
      "step": 100170
    },
    {
      "epoch": 5.342933333333333,
      "grad_norm": 0.24858404695987701,
      "learning_rate": 1.6606666666666668e-05,
      "loss": 0.002,
      "step": 100180
    },
    {
      "epoch": 5.343466666666667,
      "grad_norm": 0.2917449474334717,
      "learning_rate": 1.6603333333333334e-05,
      "loss": 0.0018,
      "step": 100190
    },
    {
      "epoch": 5.344,
      "grad_norm": 0.2252403199672699,
      "learning_rate": 1.66e-05,
      "loss": 0.0019,
      "step": 100200
    },
    {
      "epoch": 5.344533333333334,
      "grad_norm": 0.6294715404510498,
      "learning_rate": 1.659666666666667e-05,
      "loss": 0.0015,
      "step": 100210
    },
    {
      "epoch": 5.345066666666667,
      "grad_norm": 0.33521631360054016,
      "learning_rate": 1.6593333333333333e-05,
      "loss": 0.0019,
      "step": 100220
    },
    {
      "epoch": 5.3456,
      "grad_norm": 0.10396270453929901,
      "learning_rate": 1.659e-05,
      "loss": 0.0018,
      "step": 100230
    },
    {
      "epoch": 5.346133333333333,
      "grad_norm": 0.0431685671210289,
      "learning_rate": 1.6586666666666668e-05,
      "loss": 0.0021,
      "step": 100240
    },
    {
      "epoch": 5.346666666666667,
      "grad_norm": 0.5925564169883728,
      "learning_rate": 1.6583333333333334e-05,
      "loss": 0.0026,
      "step": 100250
    },
    {
      "epoch": 5.3472,
      "grad_norm": 0.34513431787490845,
      "learning_rate": 1.658e-05,
      "loss": 0.0021,
      "step": 100260
    },
    {
      "epoch": 5.347733333333333,
      "grad_norm": 0.33309686183929443,
      "learning_rate": 1.6576666666666667e-05,
      "loss": 0.0016,
      "step": 100270
    },
    {
      "epoch": 5.3482666666666665,
      "grad_norm": 0.3530563712120056,
      "learning_rate": 1.6573333333333336e-05,
      "loss": 0.0026,
      "step": 100280
    },
    {
      "epoch": 5.3488,
      "grad_norm": 0.3639697730541229,
      "learning_rate": 1.657e-05,
      "loss": 0.0024,
      "step": 100290
    },
    {
      "epoch": 5.349333333333333,
      "grad_norm": 0.07622203975915909,
      "learning_rate": 1.6566666666666665e-05,
      "loss": 0.0014,
      "step": 100300
    },
    {
      "epoch": 5.349866666666666,
      "grad_norm": 0.2351282685995102,
      "learning_rate": 1.6563333333333335e-05,
      "loss": 0.002,
      "step": 100310
    },
    {
      "epoch": 5.3504,
      "grad_norm": 0.07869723439216614,
      "learning_rate": 1.656e-05,
      "loss": 0.0019,
      "step": 100320
    },
    {
      "epoch": 5.350933333333334,
      "grad_norm": 0.06862055510282516,
      "learning_rate": 1.6556666666666667e-05,
      "loss": 0.0018,
      "step": 100330
    },
    {
      "epoch": 5.351466666666667,
      "grad_norm": 0.032680705189704895,
      "learning_rate": 1.6553333333333333e-05,
      "loss": 0.0016,
      "step": 100340
    },
    {
      "epoch": 5.352,
      "grad_norm": 0.09881080687046051,
      "learning_rate": 1.6550000000000002e-05,
      "loss": 0.0024,
      "step": 100350
    },
    {
      "epoch": 5.352533333333334,
      "grad_norm": 0.22437803447246552,
      "learning_rate": 1.654666666666667e-05,
      "loss": 0.0017,
      "step": 100360
    },
    {
      "epoch": 5.353066666666667,
      "grad_norm": 0.22153805196285248,
      "learning_rate": 1.654333333333333e-05,
      "loss": 0.002,
      "step": 100370
    },
    {
      "epoch": 5.3536,
      "grad_norm": 0.2523333430290222,
      "learning_rate": 1.654e-05,
      "loss": 0.0016,
      "step": 100380
    },
    {
      "epoch": 5.354133333333333,
      "grad_norm": 0.5500044822692871,
      "learning_rate": 1.6536666666666667e-05,
      "loss": 0.0017,
      "step": 100390
    },
    {
      "epoch": 5.354666666666667,
      "grad_norm": 0.06426641345024109,
      "learning_rate": 1.6533333333333333e-05,
      "loss": 0.0019,
      "step": 100400
    },
    {
      "epoch": 5.3552,
      "grad_norm": 0.3874584436416626,
      "learning_rate": 1.6530000000000003e-05,
      "loss": 0.0018,
      "step": 100410
    },
    {
      "epoch": 5.355733333333333,
      "grad_norm": 0.16499198973178864,
      "learning_rate": 1.652666666666667e-05,
      "loss": 0.0018,
      "step": 100420
    },
    {
      "epoch": 5.3562666666666665,
      "grad_norm": 0.13769449293613434,
      "learning_rate": 1.6523333333333335e-05,
      "loss": 0.0028,
      "step": 100430
    },
    {
      "epoch": 5.3568,
      "grad_norm": 0.27123188972473145,
      "learning_rate": 1.652e-05,
      "loss": 0.0013,
      "step": 100440
    },
    {
      "epoch": 5.357333333333333,
      "grad_norm": 0.04602406546473503,
      "learning_rate": 1.6516666666666667e-05,
      "loss": 0.0017,
      "step": 100450
    },
    {
      "epoch": 5.357866666666666,
      "grad_norm": 0.4709983766078949,
      "learning_rate": 1.6513333333333333e-05,
      "loss": 0.0024,
      "step": 100460
    },
    {
      "epoch": 5.3584,
      "grad_norm": 0.26917558908462524,
      "learning_rate": 1.651e-05,
      "loss": 0.0025,
      "step": 100470
    },
    {
      "epoch": 5.358933333333333,
      "grad_norm": 0.33141255378723145,
      "learning_rate": 1.650666666666667e-05,
      "loss": 0.0019,
      "step": 100480
    },
    {
      "epoch": 5.359466666666667,
      "grad_norm": 0.32151463627815247,
      "learning_rate": 1.6503333333333335e-05,
      "loss": 0.002,
      "step": 100490
    },
    {
      "epoch": 5.36,
      "grad_norm": 0.04208267852663994,
      "learning_rate": 1.65e-05,
      "loss": 0.0017,
      "step": 100500
    },
    {
      "epoch": 5.360533333333334,
      "grad_norm": 0.10487271845340729,
      "learning_rate": 1.6496666666666667e-05,
      "loss": 0.0018,
      "step": 100510
    },
    {
      "epoch": 5.361066666666667,
      "grad_norm": 0.23977594077587128,
      "learning_rate": 1.6493333333333334e-05,
      "loss": 0.0022,
      "step": 100520
    },
    {
      "epoch": 5.3616,
      "grad_norm": 0.14807137846946716,
      "learning_rate": 1.649e-05,
      "loss": 0.0016,
      "step": 100530
    },
    {
      "epoch": 5.362133333333333,
      "grad_norm": 0.3633482754230499,
      "learning_rate": 1.6486666666666666e-05,
      "loss": 0.0023,
      "step": 100540
    },
    {
      "epoch": 5.362666666666667,
      "grad_norm": 0.03025798313319683,
      "learning_rate": 1.6483333333333335e-05,
      "loss": 0.0017,
      "step": 100550
    },
    {
      "epoch": 5.3632,
      "grad_norm": 0.36611130833625793,
      "learning_rate": 1.648e-05,
      "loss": 0.0027,
      "step": 100560
    },
    {
      "epoch": 5.363733333333333,
      "grad_norm": 0.2612602114677429,
      "learning_rate": 1.6476666666666668e-05,
      "loss": 0.0017,
      "step": 100570
    },
    {
      "epoch": 5.3642666666666665,
      "grad_norm": 0.35671621561050415,
      "learning_rate": 1.6473333333333334e-05,
      "loss": 0.0024,
      "step": 100580
    },
    {
      "epoch": 5.3648,
      "grad_norm": 0.10223101079463959,
      "learning_rate": 1.6470000000000003e-05,
      "loss": 0.0019,
      "step": 100590
    },
    {
      "epoch": 5.365333333333333,
      "grad_norm": 0.3036833703517914,
      "learning_rate": 1.6466666666666666e-05,
      "loss": 0.0011,
      "step": 100600
    },
    {
      "epoch": 5.365866666666666,
      "grad_norm": 0.1659502536058426,
      "learning_rate": 1.6463333333333332e-05,
      "loss": 0.0022,
      "step": 100610
    },
    {
      "epoch": 5.3664,
      "grad_norm": 0.12912262976169586,
      "learning_rate": 1.646e-05,
      "loss": 0.0017,
      "step": 100620
    },
    {
      "epoch": 5.366933333333334,
      "grad_norm": 0.1817089021205902,
      "learning_rate": 1.6456666666666668e-05,
      "loss": 0.002,
      "step": 100630
    },
    {
      "epoch": 5.367466666666667,
      "grad_norm": 0.07330254465341568,
      "learning_rate": 1.6453333333333334e-05,
      "loss": 0.0014,
      "step": 100640
    },
    {
      "epoch": 5.368,
      "grad_norm": 0.21252159774303436,
      "learning_rate": 1.645e-05,
      "loss": 0.0022,
      "step": 100650
    },
    {
      "epoch": 5.368533333333334,
      "grad_norm": 0.05755149945616722,
      "learning_rate": 1.644666666666667e-05,
      "loss": 0.002,
      "step": 100660
    },
    {
      "epoch": 5.369066666666667,
      "grad_norm": 0.5418134331703186,
      "learning_rate": 1.6443333333333332e-05,
      "loss": 0.0021,
      "step": 100670
    },
    {
      "epoch": 5.3696,
      "grad_norm": 0.07558362931013107,
      "learning_rate": 1.644e-05,
      "loss": 0.0027,
      "step": 100680
    },
    {
      "epoch": 5.370133333333333,
      "grad_norm": 0.07647141069173813,
      "learning_rate": 1.6436666666666668e-05,
      "loss": 0.0015,
      "step": 100690
    },
    {
      "epoch": 5.370666666666667,
      "grad_norm": 0.21083475649356842,
      "learning_rate": 1.6433333333333334e-05,
      "loss": 0.003,
      "step": 100700
    },
    {
      "epoch": 5.3712,
      "grad_norm": 0.13553692400455475,
      "learning_rate": 1.643e-05,
      "loss": 0.0023,
      "step": 100710
    },
    {
      "epoch": 5.371733333333333,
      "grad_norm": 0.047866735607385635,
      "learning_rate": 1.6426666666666666e-05,
      "loss": 0.0014,
      "step": 100720
    },
    {
      "epoch": 5.3722666666666665,
      "grad_norm": 0.27975374460220337,
      "learning_rate": 1.6423333333333336e-05,
      "loss": 0.0019,
      "step": 100730
    },
    {
      "epoch": 5.3728,
      "grad_norm": 0.044081564992666245,
      "learning_rate": 1.6420000000000002e-05,
      "loss": 0.0017,
      "step": 100740
    },
    {
      "epoch": 5.373333333333333,
      "grad_norm": 0.5212996602058411,
      "learning_rate": 1.6416666666666665e-05,
      "loss": 0.0026,
      "step": 100750
    },
    {
      "epoch": 5.373866666666666,
      "grad_norm": 0.21664300560951233,
      "learning_rate": 1.6413333333333334e-05,
      "loss": 0.0017,
      "step": 100760
    },
    {
      "epoch": 5.3744,
      "grad_norm": 0.15064853429794312,
      "learning_rate": 1.641e-05,
      "loss": 0.0022,
      "step": 100770
    },
    {
      "epoch": 5.374933333333333,
      "grad_norm": 0.12236916273832321,
      "learning_rate": 1.6406666666666667e-05,
      "loss": 0.002,
      "step": 100780
    },
    {
      "epoch": 5.375466666666667,
      "grad_norm": 0.539456307888031,
      "learning_rate": 1.6403333333333336e-05,
      "loss": 0.0017,
      "step": 100790
    },
    {
      "epoch": 5.376,
      "grad_norm": 0.45972058176994324,
      "learning_rate": 1.6400000000000002e-05,
      "loss": 0.0031,
      "step": 100800
    },
    {
      "epoch": 5.376533333333334,
      "grad_norm": 0.36577939987182617,
      "learning_rate": 1.639666666666667e-05,
      "loss": 0.0021,
      "step": 100810
    },
    {
      "epoch": 5.377066666666667,
      "grad_norm": 0.585010290145874,
      "learning_rate": 1.639333333333333e-05,
      "loss": 0.0021,
      "step": 100820
    },
    {
      "epoch": 5.3776,
      "grad_norm": 0.035566870123147964,
      "learning_rate": 1.639e-05,
      "loss": 0.0016,
      "step": 100830
    },
    {
      "epoch": 5.378133333333333,
      "grad_norm": 0.39211854338645935,
      "learning_rate": 1.6386666666666667e-05,
      "loss": 0.002,
      "step": 100840
    },
    {
      "epoch": 5.378666666666667,
      "grad_norm": 0.10667289793491364,
      "learning_rate": 1.6383333333333333e-05,
      "loss": 0.0017,
      "step": 100850
    },
    {
      "epoch": 5.3792,
      "grad_norm": 0.5744972229003906,
      "learning_rate": 1.6380000000000002e-05,
      "loss": 0.0016,
      "step": 100860
    },
    {
      "epoch": 5.379733333333333,
      "grad_norm": 0.06384947150945663,
      "learning_rate": 1.637666666666667e-05,
      "loss": 0.0015,
      "step": 100870
    },
    {
      "epoch": 5.3802666666666665,
      "grad_norm": 0.45439398288726807,
      "learning_rate": 1.6373333333333335e-05,
      "loss": 0.0014,
      "step": 100880
    },
    {
      "epoch": 5.3808,
      "grad_norm": 0.09212549775838852,
      "learning_rate": 1.637e-05,
      "loss": 0.0032,
      "step": 100890
    },
    {
      "epoch": 5.381333333333333,
      "grad_norm": 0.1589798927307129,
      "learning_rate": 1.6366666666666667e-05,
      "loss": 0.0025,
      "step": 100900
    },
    {
      "epoch": 5.381866666666666,
      "grad_norm": 0.26750293374061584,
      "learning_rate": 1.6363333333333333e-05,
      "loss": 0.0017,
      "step": 100910
    },
    {
      "epoch": 5.3824,
      "grad_norm": 0.47099539637565613,
      "learning_rate": 1.636e-05,
      "loss": 0.0014,
      "step": 100920
    },
    {
      "epoch": 5.382933333333334,
      "grad_norm": 0.7966102957725525,
      "learning_rate": 1.635666666666667e-05,
      "loss": 0.0015,
      "step": 100930
    },
    {
      "epoch": 5.383466666666667,
      "grad_norm": 0.08430250734090805,
      "learning_rate": 1.6353333333333335e-05,
      "loss": 0.0028,
      "step": 100940
    },
    {
      "epoch": 5.384,
      "grad_norm": 0.34279054403305054,
      "learning_rate": 1.635e-05,
      "loss": 0.0016,
      "step": 100950
    },
    {
      "epoch": 5.384533333333334,
      "grad_norm": 0.2872384786605835,
      "learning_rate": 1.6346666666666667e-05,
      "loss": 0.0027,
      "step": 100960
    },
    {
      "epoch": 5.385066666666667,
      "grad_norm": 0.09708093106746674,
      "learning_rate": 1.6343333333333337e-05,
      "loss": 0.0019,
      "step": 100970
    },
    {
      "epoch": 5.3856,
      "grad_norm": 0.10402198880910873,
      "learning_rate": 1.634e-05,
      "loss": 0.002,
      "step": 100980
    },
    {
      "epoch": 5.386133333333333,
      "grad_norm": 0.23887880146503448,
      "learning_rate": 1.6336666666666666e-05,
      "loss": 0.0015,
      "step": 100990
    },
    {
      "epoch": 5.386666666666667,
      "grad_norm": 0.15940578281879425,
      "learning_rate": 1.6333333333333335e-05,
      "loss": 0.0026,
      "step": 101000
    },
    {
      "epoch": 5.3872,
      "grad_norm": 0.294451504945755,
      "learning_rate": 1.633e-05,
      "loss": 0.0018,
      "step": 101010
    },
    {
      "epoch": 5.387733333333333,
      "grad_norm": 0.15110936760902405,
      "learning_rate": 1.6326666666666667e-05,
      "loss": 0.0011,
      "step": 101020
    },
    {
      "epoch": 5.3882666666666665,
      "grad_norm": 0.08502493053674698,
      "learning_rate": 1.6323333333333333e-05,
      "loss": 0.0021,
      "step": 101030
    },
    {
      "epoch": 5.3888,
      "grad_norm": 0.09236322343349457,
      "learning_rate": 1.6320000000000003e-05,
      "loss": 0.0019,
      "step": 101040
    },
    {
      "epoch": 5.389333333333333,
      "grad_norm": 0.36899176239967346,
      "learning_rate": 1.6316666666666666e-05,
      "loss": 0.0033,
      "step": 101050
    },
    {
      "epoch": 5.389866666666666,
      "grad_norm": 0.1361066699028015,
      "learning_rate": 1.6313333333333332e-05,
      "loss": 0.0025,
      "step": 101060
    },
    {
      "epoch": 5.3904,
      "grad_norm": 0.2803233563899994,
      "learning_rate": 1.631e-05,
      "loss": 0.0015,
      "step": 101070
    },
    {
      "epoch": 5.390933333333333,
      "grad_norm": 0.08881204575300217,
      "learning_rate": 1.6306666666666668e-05,
      "loss": 0.0016,
      "step": 101080
    },
    {
      "epoch": 5.391466666666667,
      "grad_norm": 0.03291831538081169,
      "learning_rate": 1.6303333333333334e-05,
      "loss": 0.002,
      "step": 101090
    },
    {
      "epoch": 5.392,
      "grad_norm": 0.08132075518369675,
      "learning_rate": 1.63e-05,
      "loss": 0.0022,
      "step": 101100
    },
    {
      "epoch": 5.392533333333334,
      "grad_norm": 0.26728275418281555,
      "learning_rate": 1.629666666666667e-05,
      "loss": 0.0016,
      "step": 101110
    },
    {
      "epoch": 5.393066666666667,
      "grad_norm": 0.12600184977054596,
      "learning_rate": 1.6293333333333335e-05,
      "loss": 0.0013,
      "step": 101120
    },
    {
      "epoch": 5.3936,
      "grad_norm": 0.13081036508083344,
      "learning_rate": 1.6289999999999998e-05,
      "loss": 0.0023,
      "step": 101130
    },
    {
      "epoch": 5.3941333333333334,
      "grad_norm": 0.30477362871170044,
      "learning_rate": 1.6286666666666668e-05,
      "loss": 0.0019,
      "step": 101140
    },
    {
      "epoch": 5.394666666666667,
      "grad_norm": 0.4799273610115051,
      "learning_rate": 1.6283333333333334e-05,
      "loss": 0.0025,
      "step": 101150
    },
    {
      "epoch": 5.3952,
      "grad_norm": 0.10151077061891556,
      "learning_rate": 1.628e-05,
      "loss": 0.0019,
      "step": 101160
    },
    {
      "epoch": 5.395733333333333,
      "grad_norm": 0.5060661435127258,
      "learning_rate": 1.627666666666667e-05,
      "loss": 0.0018,
      "step": 101170
    },
    {
      "epoch": 5.3962666666666665,
      "grad_norm": 0.0685107484459877,
      "learning_rate": 1.6273333333333336e-05,
      "loss": 0.0021,
      "step": 101180
    },
    {
      "epoch": 5.3968,
      "grad_norm": 0.3466325104236603,
      "learning_rate": 1.6270000000000002e-05,
      "loss": 0.0017,
      "step": 101190
    },
    {
      "epoch": 5.397333333333333,
      "grad_norm": 0.22192752361297607,
      "learning_rate": 1.6266666666666665e-05,
      "loss": 0.0024,
      "step": 101200
    },
    {
      "epoch": 5.397866666666666,
      "grad_norm": 0.20651358366012573,
      "learning_rate": 1.6263333333333334e-05,
      "loss": 0.0015,
      "step": 101210
    },
    {
      "epoch": 5.3984,
      "grad_norm": 0.12265516817569733,
      "learning_rate": 1.626e-05,
      "loss": 0.0022,
      "step": 101220
    },
    {
      "epoch": 5.398933333333333,
      "grad_norm": 0.434965580701828,
      "learning_rate": 1.6256666666666666e-05,
      "loss": 0.0017,
      "step": 101230
    },
    {
      "epoch": 5.399466666666667,
      "grad_norm": 0.17952702939510345,
      "learning_rate": 1.6253333333333336e-05,
      "loss": 0.0023,
      "step": 101240
    },
    {
      "epoch": 5.4,
      "grad_norm": 0.4021604359149933,
      "learning_rate": 1.6250000000000002e-05,
      "loss": 0.0026,
      "step": 101250
    },
    {
      "epoch": 5.400533333333334,
      "grad_norm": 0.31543564796447754,
      "learning_rate": 1.6246666666666668e-05,
      "loss": 0.0025,
      "step": 101260
    },
    {
      "epoch": 5.401066666666667,
      "grad_norm": 0.35544630885124207,
      "learning_rate": 1.6243333333333334e-05,
      "loss": 0.0023,
      "step": 101270
    },
    {
      "epoch": 5.4016,
      "grad_norm": 0.13450098037719727,
      "learning_rate": 1.624e-05,
      "loss": 0.0012,
      "step": 101280
    },
    {
      "epoch": 5.4021333333333335,
      "grad_norm": 0.09763509780168533,
      "learning_rate": 1.6236666666666667e-05,
      "loss": 0.0015,
      "step": 101290
    },
    {
      "epoch": 5.402666666666667,
      "grad_norm": 0.26636171340942383,
      "learning_rate": 1.6233333333333333e-05,
      "loss": 0.0032,
      "step": 101300
    },
    {
      "epoch": 5.4032,
      "grad_norm": 0.09646682441234589,
      "learning_rate": 1.6230000000000002e-05,
      "loss": 0.0017,
      "step": 101310
    },
    {
      "epoch": 5.403733333333333,
      "grad_norm": 0.09537697583436966,
      "learning_rate": 1.6226666666666668e-05,
      "loss": 0.0022,
      "step": 101320
    },
    {
      "epoch": 5.4042666666666666,
      "grad_norm": 0.390179306268692,
      "learning_rate": 1.6223333333333334e-05,
      "loss": 0.0026,
      "step": 101330
    },
    {
      "epoch": 5.4048,
      "grad_norm": 0.04483986273407936,
      "learning_rate": 1.622e-05,
      "loss": 0.0017,
      "step": 101340
    },
    {
      "epoch": 5.405333333333333,
      "grad_norm": 0.12147979438304901,
      "learning_rate": 1.6216666666666667e-05,
      "loss": 0.0023,
      "step": 101350
    },
    {
      "epoch": 5.405866666666666,
      "grad_norm": 0.13916003704071045,
      "learning_rate": 1.6213333333333333e-05,
      "loss": 0.0018,
      "step": 101360
    },
    {
      "epoch": 5.4064,
      "grad_norm": 0.42669275403022766,
      "learning_rate": 1.621e-05,
      "loss": 0.0019,
      "step": 101370
    },
    {
      "epoch": 5.406933333333333,
      "grad_norm": 0.20532569289207458,
      "learning_rate": 1.620666666666667e-05,
      "loss": 0.002,
      "step": 101380
    },
    {
      "epoch": 5.407466666666666,
      "grad_norm": 0.09728524088859558,
      "learning_rate": 1.6203333333333335e-05,
      "loss": 0.0025,
      "step": 101390
    },
    {
      "epoch": 5.408,
      "grad_norm": 0.4005827009677887,
      "learning_rate": 1.62e-05,
      "loss": 0.0018,
      "step": 101400
    },
    {
      "epoch": 5.408533333333334,
      "grad_norm": 0.25455644726753235,
      "learning_rate": 1.6196666666666667e-05,
      "loss": 0.0018,
      "step": 101410
    },
    {
      "epoch": 5.409066666666667,
      "grad_norm": 0.4023042619228363,
      "learning_rate": 1.6193333333333336e-05,
      "loss": 0.0019,
      "step": 101420
    },
    {
      "epoch": 5.4096,
      "grad_norm": 0.295081228017807,
      "learning_rate": 1.619e-05,
      "loss": 0.0017,
      "step": 101430
    },
    {
      "epoch": 5.4101333333333335,
      "grad_norm": 0.04725773259997368,
      "learning_rate": 1.6186666666666665e-05,
      "loss": 0.0019,
      "step": 101440
    },
    {
      "epoch": 5.410666666666667,
      "grad_norm": 0.07201575487852097,
      "learning_rate": 1.6183333333333335e-05,
      "loss": 0.0011,
      "step": 101450
    },
    {
      "epoch": 5.4112,
      "grad_norm": 0.4184989631175995,
      "learning_rate": 1.618e-05,
      "loss": 0.0019,
      "step": 101460
    },
    {
      "epoch": 5.411733333333333,
      "grad_norm": 0.5467093586921692,
      "learning_rate": 1.6176666666666667e-05,
      "loss": 0.0022,
      "step": 101470
    },
    {
      "epoch": 5.412266666666667,
      "grad_norm": 0.31120818853378296,
      "learning_rate": 1.6173333333333333e-05,
      "loss": 0.002,
      "step": 101480
    },
    {
      "epoch": 5.4128,
      "grad_norm": 0.14459672570228577,
      "learning_rate": 1.6170000000000003e-05,
      "loss": 0.0027,
      "step": 101490
    },
    {
      "epoch": 5.413333333333333,
      "grad_norm": 0.24198849499225616,
      "learning_rate": 1.6166666666666665e-05,
      "loss": 0.0023,
      "step": 101500
    },
    {
      "epoch": 5.413866666666666,
      "grad_norm": 0.05186072736978531,
      "learning_rate": 1.616333333333333e-05,
      "loss": 0.0016,
      "step": 101510
    },
    {
      "epoch": 5.4144,
      "grad_norm": 0.5442561507225037,
      "learning_rate": 1.616e-05,
      "loss": 0.0016,
      "step": 101520
    },
    {
      "epoch": 5.414933333333333,
      "grad_norm": 0.26862668991088867,
      "learning_rate": 1.6156666666666667e-05,
      "loss": 0.0031,
      "step": 101530
    },
    {
      "epoch": 5.415466666666667,
      "grad_norm": 0.3304837942123413,
      "learning_rate": 1.6153333333333333e-05,
      "loss": 0.0029,
      "step": 101540
    },
    {
      "epoch": 5.416,
      "grad_norm": 0.10791189223527908,
      "learning_rate": 1.6150000000000003e-05,
      "loss": 0.0016,
      "step": 101550
    },
    {
      "epoch": 5.416533333333334,
      "grad_norm": 0.10220412164926529,
      "learning_rate": 1.614666666666667e-05,
      "loss": 0.0012,
      "step": 101560
    },
    {
      "epoch": 5.417066666666667,
      "grad_norm": 0.3710462152957916,
      "learning_rate": 1.6143333333333335e-05,
      "loss": 0.0019,
      "step": 101570
    },
    {
      "epoch": 5.4176,
      "grad_norm": 0.05736757442355156,
      "learning_rate": 1.6139999999999998e-05,
      "loss": 0.0014,
      "step": 101580
    },
    {
      "epoch": 5.4181333333333335,
      "grad_norm": 0.08299082517623901,
      "learning_rate": 1.6136666666666667e-05,
      "loss": 0.0022,
      "step": 101590
    },
    {
      "epoch": 5.418666666666667,
      "grad_norm": 0.6785427331924438,
      "learning_rate": 1.6133333333333334e-05,
      "loss": 0.0019,
      "step": 101600
    },
    {
      "epoch": 5.4192,
      "grad_norm": 0.2701888084411621,
      "learning_rate": 1.613e-05,
      "loss": 0.0033,
      "step": 101610
    },
    {
      "epoch": 5.419733333333333,
      "grad_norm": 0.330099493265152,
      "learning_rate": 1.612666666666667e-05,
      "loss": 0.0014,
      "step": 101620
    },
    {
      "epoch": 5.420266666666667,
      "grad_norm": 0.10475965589284897,
      "learning_rate": 1.6123333333333335e-05,
      "loss": 0.0018,
      "step": 101630
    },
    {
      "epoch": 5.4208,
      "grad_norm": 0.38879823684692383,
      "learning_rate": 1.612e-05,
      "loss": 0.0021,
      "step": 101640
    },
    {
      "epoch": 5.421333333333333,
      "grad_norm": 0.052630964666604996,
      "learning_rate": 1.6116666666666668e-05,
      "loss": 0.0016,
      "step": 101650
    },
    {
      "epoch": 5.421866666666666,
      "grad_norm": 0.3252328336238861,
      "learning_rate": 1.6113333333333334e-05,
      "loss": 0.0018,
      "step": 101660
    },
    {
      "epoch": 5.4224,
      "grad_norm": 0.19333812594413757,
      "learning_rate": 1.611e-05,
      "loss": 0.0016,
      "step": 101670
    },
    {
      "epoch": 5.422933333333333,
      "grad_norm": 0.23613274097442627,
      "learning_rate": 1.6106666666666666e-05,
      "loss": 0.0018,
      "step": 101680
    },
    {
      "epoch": 5.423466666666666,
      "grad_norm": 0.06618215888738632,
      "learning_rate": 1.6103333333333336e-05,
      "loss": 0.0017,
      "step": 101690
    },
    {
      "epoch": 5.424,
      "grad_norm": 0.2826862633228302,
      "learning_rate": 1.6100000000000002e-05,
      "loss": 0.0013,
      "step": 101700
    },
    {
      "epoch": 5.424533333333334,
      "grad_norm": 0.11871436983346939,
      "learning_rate": 1.6096666666666668e-05,
      "loss": 0.0023,
      "step": 101710
    },
    {
      "epoch": 5.425066666666667,
      "grad_norm": 0.157720148563385,
      "learning_rate": 1.6093333333333334e-05,
      "loss": 0.0021,
      "step": 101720
    },
    {
      "epoch": 5.4256,
      "grad_norm": 0.5734934210777283,
      "learning_rate": 1.609e-05,
      "loss": 0.0024,
      "step": 101730
    },
    {
      "epoch": 5.4261333333333335,
      "grad_norm": 0.4830048382282257,
      "learning_rate": 1.6086666666666666e-05,
      "loss": 0.0018,
      "step": 101740
    },
    {
      "epoch": 5.426666666666667,
      "grad_norm": 0.12267068028450012,
      "learning_rate": 1.6083333333333332e-05,
      "loss": 0.0024,
      "step": 101750
    },
    {
      "epoch": 5.4272,
      "grad_norm": 0.2490149736404419,
      "learning_rate": 1.6080000000000002e-05,
      "loss": 0.0016,
      "step": 101760
    },
    {
      "epoch": 5.427733333333333,
      "grad_norm": 0.5042776465415955,
      "learning_rate": 1.6076666666666668e-05,
      "loss": 0.0017,
      "step": 101770
    },
    {
      "epoch": 5.428266666666667,
      "grad_norm": 0.18291568756103516,
      "learning_rate": 1.6073333333333334e-05,
      "loss": 0.0014,
      "step": 101780
    },
    {
      "epoch": 5.4288,
      "grad_norm": 0.09777753800153732,
      "learning_rate": 1.607e-05,
      "loss": 0.002,
      "step": 101790
    },
    {
      "epoch": 5.429333333333333,
      "grad_norm": 0.29236969351768494,
      "learning_rate": 1.606666666666667e-05,
      "loss": 0.0022,
      "step": 101800
    },
    {
      "epoch": 5.429866666666666,
      "grad_norm": 0.4801250398159027,
      "learning_rate": 1.6063333333333333e-05,
      "loss": 0.0019,
      "step": 101810
    },
    {
      "epoch": 5.4304,
      "grad_norm": 0.6895854473114014,
      "learning_rate": 1.606e-05,
      "loss": 0.0024,
      "step": 101820
    },
    {
      "epoch": 5.430933333333333,
      "grad_norm": 0.23868729174137115,
      "learning_rate": 1.6056666666666668e-05,
      "loss": 0.0018,
      "step": 101830
    },
    {
      "epoch": 5.431466666666667,
      "grad_norm": 0.21586942672729492,
      "learning_rate": 1.6053333333333334e-05,
      "loss": 0.0033,
      "step": 101840
    },
    {
      "epoch": 5.432,
      "grad_norm": 0.21088796854019165,
      "learning_rate": 1.605e-05,
      "loss": 0.0016,
      "step": 101850
    },
    {
      "epoch": 5.432533333333334,
      "grad_norm": 0.08128318935632706,
      "learning_rate": 1.6046666666666667e-05,
      "loss": 0.0023,
      "step": 101860
    },
    {
      "epoch": 5.433066666666667,
      "grad_norm": 0.4355980455875397,
      "learning_rate": 1.6043333333333336e-05,
      "loss": 0.0021,
      "step": 101870
    },
    {
      "epoch": 5.4336,
      "grad_norm": 0.28089573979377747,
      "learning_rate": 1.604e-05,
      "loss": 0.0025,
      "step": 101880
    },
    {
      "epoch": 5.4341333333333335,
      "grad_norm": 0.11554905027151108,
      "learning_rate": 1.6036666666666665e-05,
      "loss": 0.0026,
      "step": 101890
    },
    {
      "epoch": 5.434666666666667,
      "grad_norm": 0.24610303342342377,
      "learning_rate": 1.6033333333333335e-05,
      "loss": 0.002,
      "step": 101900
    },
    {
      "epoch": 5.4352,
      "grad_norm": 0.13481977581977844,
      "learning_rate": 1.603e-05,
      "loss": 0.0018,
      "step": 101910
    },
    {
      "epoch": 5.435733333333333,
      "grad_norm": 0.06224677711725235,
      "learning_rate": 1.6026666666666667e-05,
      "loss": 0.0015,
      "step": 101920
    },
    {
      "epoch": 5.436266666666667,
      "grad_norm": 0.43521568179130554,
      "learning_rate": 1.6023333333333333e-05,
      "loss": 0.0027,
      "step": 101930
    },
    {
      "epoch": 5.4368,
      "grad_norm": 0.3418772220611572,
      "learning_rate": 1.6020000000000002e-05,
      "loss": 0.0013,
      "step": 101940
    },
    {
      "epoch": 5.437333333333333,
      "grad_norm": 0.6260779500007629,
      "learning_rate": 1.601666666666667e-05,
      "loss": 0.0019,
      "step": 101950
    },
    {
      "epoch": 5.437866666666666,
      "grad_norm": 0.2730454206466675,
      "learning_rate": 1.601333333333333e-05,
      "loss": 0.0017,
      "step": 101960
    },
    {
      "epoch": 5.4384,
      "grad_norm": 0.25028181076049805,
      "learning_rate": 1.601e-05,
      "loss": 0.0015,
      "step": 101970
    },
    {
      "epoch": 5.438933333333333,
      "grad_norm": 0.5075660943984985,
      "learning_rate": 1.6006666666666667e-05,
      "loss": 0.0017,
      "step": 101980
    },
    {
      "epoch": 5.439466666666666,
      "grad_norm": 0.09169039875268936,
      "learning_rate": 1.6003333333333333e-05,
      "loss": 0.0016,
      "step": 101990
    },
    {
      "epoch": 5.44,
      "grad_norm": 0.15782880783081055,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.0012,
      "step": 102000
    },
    {
      "epoch": 5.440533333333334,
      "grad_norm": 0.6329401135444641,
      "learning_rate": 1.599666666666667e-05,
      "loss": 0.0018,
      "step": 102010
    },
    {
      "epoch": 5.441066666666667,
      "grad_norm": 0.12230958789587021,
      "learning_rate": 1.5993333333333335e-05,
      "loss": 0.0018,
      "step": 102020
    },
    {
      "epoch": 5.4416,
      "grad_norm": 0.20535995066165924,
      "learning_rate": 1.599e-05,
      "loss": 0.003,
      "step": 102030
    },
    {
      "epoch": 5.4421333333333335,
      "grad_norm": 0.4735965132713318,
      "learning_rate": 1.5986666666666667e-05,
      "loss": 0.0016,
      "step": 102040
    },
    {
      "epoch": 5.442666666666667,
      "grad_norm": 0.0491727776825428,
      "learning_rate": 1.5983333333333333e-05,
      "loss": 0.0019,
      "step": 102050
    },
    {
      "epoch": 5.4432,
      "grad_norm": 0.5438739657402039,
      "learning_rate": 1.598e-05,
      "loss": 0.0018,
      "step": 102060
    },
    {
      "epoch": 5.443733333333333,
      "grad_norm": 0.21879249811172485,
      "learning_rate": 1.597666666666667e-05,
      "loss": 0.0026,
      "step": 102070
    },
    {
      "epoch": 5.444266666666667,
      "grad_norm": 0.1511564552783966,
      "learning_rate": 1.5973333333333335e-05,
      "loss": 0.0018,
      "step": 102080
    },
    {
      "epoch": 5.4448,
      "grad_norm": 0.667046308517456,
      "learning_rate": 1.597e-05,
      "loss": 0.0024,
      "step": 102090
    },
    {
      "epoch": 5.445333333333333,
      "grad_norm": 0.058723967522382736,
      "learning_rate": 1.5966666666666667e-05,
      "loss": 0.0014,
      "step": 102100
    },
    {
      "epoch": 5.445866666666666,
      "grad_norm": 0.31256765127182007,
      "learning_rate": 1.5963333333333334e-05,
      "loss": 0.0019,
      "step": 102110
    },
    {
      "epoch": 5.4464,
      "grad_norm": 0.4537058174610138,
      "learning_rate": 1.596e-05,
      "loss": 0.0016,
      "step": 102120
    },
    {
      "epoch": 5.446933333333333,
      "grad_norm": 0.10573472827672958,
      "learning_rate": 1.5956666666666666e-05,
      "loss": 0.0024,
      "step": 102130
    },
    {
      "epoch": 5.447466666666667,
      "grad_norm": 0.8710996508598328,
      "learning_rate": 1.5953333333333335e-05,
      "loss": 0.0024,
      "step": 102140
    },
    {
      "epoch": 5.448,
      "grad_norm": 0.5687692165374756,
      "learning_rate": 1.595e-05,
      "loss": 0.0028,
      "step": 102150
    },
    {
      "epoch": 5.448533333333334,
      "grad_norm": 0.07055115699768066,
      "learning_rate": 1.5946666666666668e-05,
      "loss": 0.0022,
      "step": 102160
    },
    {
      "epoch": 5.449066666666667,
      "grad_norm": 0.11234107613563538,
      "learning_rate": 1.5943333333333334e-05,
      "loss": 0.0025,
      "step": 102170
    },
    {
      "epoch": 5.4496,
      "grad_norm": 0.17337782680988312,
      "learning_rate": 1.594e-05,
      "loss": 0.0024,
      "step": 102180
    },
    {
      "epoch": 5.4501333333333335,
      "grad_norm": 0.1208920031785965,
      "learning_rate": 1.5936666666666666e-05,
      "loss": 0.0013,
      "step": 102190
    },
    {
      "epoch": 5.450666666666667,
      "grad_norm": 0.05645861476659775,
      "learning_rate": 1.5933333333333332e-05,
      "loss": 0.0017,
      "step": 102200
    },
    {
      "epoch": 5.4512,
      "grad_norm": 0.10267060995101929,
      "learning_rate": 1.593e-05,
      "loss": 0.0019,
      "step": 102210
    },
    {
      "epoch": 5.451733333333333,
      "grad_norm": 0.02888873778283596,
      "learning_rate": 1.5926666666666668e-05,
      "loss": 0.0015,
      "step": 102220
    },
    {
      "epoch": 5.452266666666667,
      "grad_norm": 0.14909259974956512,
      "learning_rate": 1.5923333333333334e-05,
      "loss": 0.0016,
      "step": 102230
    },
    {
      "epoch": 5.4528,
      "grad_norm": 0.032811470329761505,
      "learning_rate": 1.592e-05,
      "loss": 0.0026,
      "step": 102240
    },
    {
      "epoch": 5.453333333333333,
      "grad_norm": 0.11504629254341125,
      "learning_rate": 1.591666666666667e-05,
      "loss": 0.002,
      "step": 102250
    },
    {
      "epoch": 5.453866666666666,
      "grad_norm": 0.04260944202542305,
      "learning_rate": 1.5913333333333332e-05,
      "loss": 0.0028,
      "step": 102260
    },
    {
      "epoch": 5.4544,
      "grad_norm": 0.3421383798122406,
      "learning_rate": 1.591e-05,
      "loss": 0.0018,
      "step": 102270
    },
    {
      "epoch": 5.454933333333333,
      "grad_norm": 0.020021779462695122,
      "learning_rate": 1.5906666666666668e-05,
      "loss": 0.002,
      "step": 102280
    },
    {
      "epoch": 5.455466666666666,
      "grad_norm": 0.2936314046382904,
      "learning_rate": 1.5903333333333334e-05,
      "loss": 0.0014,
      "step": 102290
    },
    {
      "epoch": 5.456,
      "grad_norm": 0.2173982411623001,
      "learning_rate": 1.59e-05,
      "loss": 0.0014,
      "step": 102300
    },
    {
      "epoch": 5.456533333333334,
      "grad_norm": 0.09122499078512192,
      "learning_rate": 1.5896666666666666e-05,
      "loss": 0.0011,
      "step": 102310
    },
    {
      "epoch": 5.457066666666667,
      "grad_norm": 0.3059743344783783,
      "learning_rate": 1.5893333333333336e-05,
      "loss": 0.0021,
      "step": 102320
    },
    {
      "epoch": 5.4576,
      "grad_norm": 0.15540888905525208,
      "learning_rate": 1.5890000000000002e-05,
      "loss": 0.0019,
      "step": 102330
    },
    {
      "epoch": 5.4581333333333335,
      "grad_norm": 0.22106480598449707,
      "learning_rate": 1.5886666666666665e-05,
      "loss": 0.0012,
      "step": 102340
    },
    {
      "epoch": 5.458666666666667,
      "grad_norm": 0.06794743984937668,
      "learning_rate": 1.5883333333333334e-05,
      "loss": 0.0016,
      "step": 102350
    },
    {
      "epoch": 5.4592,
      "grad_norm": 0.40526464581489563,
      "learning_rate": 1.588e-05,
      "loss": 0.0012,
      "step": 102360
    },
    {
      "epoch": 5.459733333333333,
      "grad_norm": 0.3382686376571655,
      "learning_rate": 1.5876666666666667e-05,
      "loss": 0.0014,
      "step": 102370
    },
    {
      "epoch": 5.460266666666667,
      "grad_norm": 0.4622918665409088,
      "learning_rate": 1.5873333333333336e-05,
      "loss": 0.0017,
      "step": 102380
    },
    {
      "epoch": 5.4608,
      "grad_norm": 0.14438647031784058,
      "learning_rate": 1.5870000000000002e-05,
      "loss": 0.002,
      "step": 102390
    },
    {
      "epoch": 5.461333333333333,
      "grad_norm": 0.16243508458137512,
      "learning_rate": 1.586666666666667e-05,
      "loss": 0.0017,
      "step": 102400
    },
    {
      "epoch": 5.461866666666666,
      "grad_norm": 0.18101702630519867,
      "learning_rate": 1.5863333333333334e-05,
      "loss": 0.0012,
      "step": 102410
    },
    {
      "epoch": 5.4624,
      "grad_norm": 0.2753883898258209,
      "learning_rate": 1.586e-05,
      "loss": 0.0017,
      "step": 102420
    },
    {
      "epoch": 5.462933333333333,
      "grad_norm": 0.2209344357252121,
      "learning_rate": 1.5856666666666667e-05,
      "loss": 0.0021,
      "step": 102430
    },
    {
      "epoch": 5.463466666666667,
      "grad_norm": 0.17555244266986847,
      "learning_rate": 1.5853333333333333e-05,
      "loss": 0.0018,
      "step": 102440
    },
    {
      "epoch": 5.464,
      "grad_norm": 0.035199280828237534,
      "learning_rate": 1.5850000000000002e-05,
      "loss": 0.0014,
      "step": 102450
    },
    {
      "epoch": 5.464533333333334,
      "grad_norm": 0.12094499170780182,
      "learning_rate": 1.584666666666667e-05,
      "loss": 0.0012,
      "step": 102460
    },
    {
      "epoch": 5.465066666666667,
      "grad_norm": 0.5381212830543518,
      "learning_rate": 1.5843333333333335e-05,
      "loss": 0.0016,
      "step": 102470
    },
    {
      "epoch": 5.4656,
      "grad_norm": 0.07053867727518082,
      "learning_rate": 1.584e-05,
      "loss": 0.0018,
      "step": 102480
    },
    {
      "epoch": 5.4661333333333335,
      "grad_norm": 0.2685505151748657,
      "learning_rate": 1.5836666666666667e-05,
      "loss": 0.0022,
      "step": 102490
    },
    {
      "epoch": 5.466666666666667,
      "grad_norm": 0.15328381955623627,
      "learning_rate": 1.5833333333333333e-05,
      "loss": 0.0015,
      "step": 102500
    },
    {
      "epoch": 5.4672,
      "grad_norm": 0.2737398147583008,
      "learning_rate": 1.583e-05,
      "loss": 0.0029,
      "step": 102510
    },
    {
      "epoch": 5.467733333333333,
      "grad_norm": 0.35742342472076416,
      "learning_rate": 1.582666666666667e-05,
      "loss": 0.0021,
      "step": 102520
    },
    {
      "epoch": 5.468266666666667,
      "grad_norm": 0.14072351157665253,
      "learning_rate": 1.5823333333333335e-05,
      "loss": 0.0023,
      "step": 102530
    },
    {
      "epoch": 5.4688,
      "grad_norm": 0.12779474258422852,
      "learning_rate": 1.582e-05,
      "loss": 0.002,
      "step": 102540
    },
    {
      "epoch": 5.469333333333333,
      "grad_norm": 0.5492400527000427,
      "learning_rate": 1.5816666666666667e-05,
      "loss": 0.0018,
      "step": 102550
    },
    {
      "epoch": 5.469866666666666,
      "grad_norm": 0.21254737675189972,
      "learning_rate": 1.5813333333333333e-05,
      "loss": 0.002,
      "step": 102560
    },
    {
      "epoch": 5.4704,
      "grad_norm": 0.2962838113307953,
      "learning_rate": 1.581e-05,
      "loss": 0.0018,
      "step": 102570
    },
    {
      "epoch": 5.470933333333333,
      "grad_norm": 0.05136871710419655,
      "learning_rate": 1.5806666666666666e-05,
      "loss": 0.0021,
      "step": 102580
    },
    {
      "epoch": 5.471466666666666,
      "grad_norm": 0.32435792684555054,
      "learning_rate": 1.5803333333333335e-05,
      "loss": 0.0025,
      "step": 102590
    },
    {
      "epoch": 5.4719999999999995,
      "grad_norm": 0.47469431161880493,
      "learning_rate": 1.58e-05,
      "loss": 0.0017,
      "step": 102600
    },
    {
      "epoch": 5.472533333333334,
      "grad_norm": 0.11897474527359009,
      "learning_rate": 1.5796666666666667e-05,
      "loss": 0.0021,
      "step": 102610
    },
    {
      "epoch": 5.473066666666667,
      "grad_norm": 0.22170722484588623,
      "learning_rate": 1.5793333333333333e-05,
      "loss": 0.0021,
      "step": 102620
    },
    {
      "epoch": 5.4736,
      "grad_norm": 0.06960080564022064,
      "learning_rate": 1.5790000000000003e-05,
      "loss": 0.0012,
      "step": 102630
    },
    {
      "epoch": 5.4741333333333335,
      "grad_norm": 0.29888808727264404,
      "learning_rate": 1.5786666666666666e-05,
      "loss": 0.0022,
      "step": 102640
    },
    {
      "epoch": 5.474666666666667,
      "grad_norm": 0.06802587956190109,
      "learning_rate": 1.5783333333333332e-05,
      "loss": 0.0022,
      "step": 102650
    },
    {
      "epoch": 5.4752,
      "grad_norm": 0.08690998703241348,
      "learning_rate": 1.578e-05,
      "loss": 0.002,
      "step": 102660
    },
    {
      "epoch": 5.475733333333333,
      "grad_norm": 0.23918232321739197,
      "learning_rate": 1.5776666666666668e-05,
      "loss": 0.0019,
      "step": 102670
    },
    {
      "epoch": 5.476266666666667,
      "grad_norm": 0.1522141844034195,
      "learning_rate": 1.5773333333333334e-05,
      "loss": 0.0018,
      "step": 102680
    },
    {
      "epoch": 5.4768,
      "grad_norm": 0.055852729827165604,
      "learning_rate": 1.577e-05,
      "loss": 0.0016,
      "step": 102690
    },
    {
      "epoch": 5.477333333333333,
      "grad_norm": 0.2575231194496155,
      "learning_rate": 1.576666666666667e-05,
      "loss": 0.0024,
      "step": 102700
    },
    {
      "epoch": 5.477866666666666,
      "grad_norm": 0.13524289429187775,
      "learning_rate": 1.5763333333333332e-05,
      "loss": 0.0017,
      "step": 102710
    },
    {
      "epoch": 5.4784,
      "grad_norm": 0.17859582602977753,
      "learning_rate": 1.5759999999999998e-05,
      "loss": 0.0031,
      "step": 102720
    },
    {
      "epoch": 5.478933333333333,
      "grad_norm": 0.5400649309158325,
      "learning_rate": 1.5756666666666668e-05,
      "loss": 0.0017,
      "step": 102730
    },
    {
      "epoch": 5.479466666666666,
      "grad_norm": 0.06349082291126251,
      "learning_rate": 1.5753333333333334e-05,
      "loss": 0.0029,
      "step": 102740
    },
    {
      "epoch": 5.48,
      "grad_norm": 0.23719172179698944,
      "learning_rate": 1.575e-05,
      "loss": 0.0017,
      "step": 102750
    },
    {
      "epoch": 5.480533333333334,
      "grad_norm": 0.23532263934612274,
      "learning_rate": 1.574666666666667e-05,
      "loss": 0.0022,
      "step": 102760
    },
    {
      "epoch": 5.481066666666667,
      "grad_norm": 0.12057335674762726,
      "learning_rate": 1.5743333333333336e-05,
      "loss": 0.0021,
      "step": 102770
    },
    {
      "epoch": 5.4816,
      "grad_norm": 0.286616712808609,
      "learning_rate": 1.5740000000000002e-05,
      "loss": 0.0025,
      "step": 102780
    },
    {
      "epoch": 5.4821333333333335,
      "grad_norm": 0.052752118557691574,
      "learning_rate": 1.5736666666666668e-05,
      "loss": 0.002,
      "step": 102790
    },
    {
      "epoch": 5.482666666666667,
      "grad_norm": 0.03796462342143059,
      "learning_rate": 1.5733333333333334e-05,
      "loss": 0.0019,
      "step": 102800
    },
    {
      "epoch": 5.4832,
      "grad_norm": 0.49749523401260376,
      "learning_rate": 1.573e-05,
      "loss": 0.0014,
      "step": 102810
    },
    {
      "epoch": 5.483733333333333,
      "grad_norm": 0.23972442746162415,
      "learning_rate": 1.5726666666666666e-05,
      "loss": 0.0019,
      "step": 102820
    },
    {
      "epoch": 5.484266666666667,
      "grad_norm": 0.06302820146083832,
      "learning_rate": 1.5723333333333336e-05,
      "loss": 0.0024,
      "step": 102830
    },
    {
      "epoch": 5.4848,
      "grad_norm": 0.050546057522296906,
      "learning_rate": 1.5720000000000002e-05,
      "loss": 0.0013,
      "step": 102840
    },
    {
      "epoch": 5.485333333333333,
      "grad_norm": 0.4655192792415619,
      "learning_rate": 1.5716666666666668e-05,
      "loss": 0.0019,
      "step": 102850
    },
    {
      "epoch": 5.4858666666666664,
      "grad_norm": 0.28859272599220276,
      "learning_rate": 1.5713333333333334e-05,
      "loss": 0.0015,
      "step": 102860
    },
    {
      "epoch": 5.4864,
      "grad_norm": 0.10141440480947495,
      "learning_rate": 1.571e-05,
      "loss": 0.0016,
      "step": 102870
    },
    {
      "epoch": 5.486933333333333,
      "grad_norm": 0.1468946784734726,
      "learning_rate": 1.5706666666666666e-05,
      "loss": 0.0015,
      "step": 102880
    },
    {
      "epoch": 5.487466666666666,
      "grad_norm": 0.047541458159685135,
      "learning_rate": 1.5703333333333333e-05,
      "loss": 0.0015,
      "step": 102890
    },
    {
      "epoch": 5.4879999999999995,
      "grad_norm": 0.28456825017929077,
      "learning_rate": 1.5700000000000002e-05,
      "loss": 0.0024,
      "step": 102900
    },
    {
      "epoch": 5.488533333333334,
      "grad_norm": 0.08940498530864716,
      "learning_rate": 1.5696666666666668e-05,
      "loss": 0.0014,
      "step": 102910
    },
    {
      "epoch": 5.489066666666667,
      "grad_norm": 0.09998274594545364,
      "learning_rate": 1.5693333333333334e-05,
      "loss": 0.0017,
      "step": 102920
    },
    {
      "epoch": 5.4896,
      "grad_norm": 0.06642542034387589,
      "learning_rate": 1.569e-05,
      "loss": 0.0016,
      "step": 102930
    },
    {
      "epoch": 5.4901333333333335,
      "grad_norm": 0.16530539095401764,
      "learning_rate": 1.5686666666666667e-05,
      "loss": 0.002,
      "step": 102940
    },
    {
      "epoch": 5.490666666666667,
      "grad_norm": 0.2304062694311142,
      "learning_rate": 1.5683333333333333e-05,
      "loss": 0.0015,
      "step": 102950
    },
    {
      "epoch": 5.4912,
      "grad_norm": 0.46664902567863464,
      "learning_rate": 1.568e-05,
      "loss": 0.002,
      "step": 102960
    },
    {
      "epoch": 5.491733333333333,
      "grad_norm": 0.09681116044521332,
      "learning_rate": 1.567666666666667e-05,
      "loss": 0.0033,
      "step": 102970
    },
    {
      "epoch": 5.492266666666667,
      "grad_norm": 0.19425086677074432,
      "learning_rate": 1.5673333333333335e-05,
      "loss": 0.0023,
      "step": 102980
    },
    {
      "epoch": 5.4928,
      "grad_norm": 0.5653215646743774,
      "learning_rate": 1.567e-05,
      "loss": 0.0028,
      "step": 102990
    },
    {
      "epoch": 5.493333333333333,
      "grad_norm": 0.13667415082454681,
      "learning_rate": 1.5666666666666667e-05,
      "loss": 0.002,
      "step": 103000
    },
    {
      "epoch": 5.4938666666666665,
      "grad_norm": 0.0649963915348053,
      "learning_rate": 1.5663333333333336e-05,
      "loss": 0.0021,
      "step": 103010
    },
    {
      "epoch": 5.4944,
      "grad_norm": 0.26621946692466736,
      "learning_rate": 1.566e-05,
      "loss": 0.0021,
      "step": 103020
    },
    {
      "epoch": 5.494933333333333,
      "grad_norm": 0.5987725257873535,
      "learning_rate": 1.5656666666666665e-05,
      "loss": 0.0015,
      "step": 103030
    },
    {
      "epoch": 5.495466666666666,
      "grad_norm": 0.12655434012413025,
      "learning_rate": 1.5653333333333335e-05,
      "loss": 0.0022,
      "step": 103040
    },
    {
      "epoch": 5.496,
      "grad_norm": 0.4765244126319885,
      "learning_rate": 1.565e-05,
      "loss": 0.0021,
      "step": 103050
    },
    {
      "epoch": 5.496533333333334,
      "grad_norm": 0.2624359726905823,
      "learning_rate": 1.5646666666666667e-05,
      "loss": 0.0031,
      "step": 103060
    },
    {
      "epoch": 5.497066666666667,
      "grad_norm": 0.195739284157753,
      "learning_rate": 1.5643333333333333e-05,
      "loss": 0.0023,
      "step": 103070
    },
    {
      "epoch": 5.4976,
      "grad_norm": 0.20669911801815033,
      "learning_rate": 1.5640000000000003e-05,
      "loss": 0.0017,
      "step": 103080
    },
    {
      "epoch": 5.4981333333333335,
      "grad_norm": 0.30873215198516846,
      "learning_rate": 1.5636666666666665e-05,
      "loss": 0.0023,
      "step": 103090
    },
    {
      "epoch": 5.498666666666667,
      "grad_norm": 0.33866533637046814,
      "learning_rate": 1.563333333333333e-05,
      "loss": 0.0021,
      "step": 103100
    },
    {
      "epoch": 5.4992,
      "grad_norm": 0.09144440293312073,
      "learning_rate": 1.563e-05,
      "loss": 0.0018,
      "step": 103110
    },
    {
      "epoch": 5.499733333333333,
      "grad_norm": 0.28134480118751526,
      "learning_rate": 1.5626666666666667e-05,
      "loss": 0.0021,
      "step": 103120
    },
    {
      "epoch": 5.500266666666667,
      "grad_norm": 0.3055674135684967,
      "learning_rate": 1.5623333333333333e-05,
      "loss": 0.0024,
      "step": 103130
    },
    {
      "epoch": 5.5008,
      "grad_norm": 0.3266049921512604,
      "learning_rate": 1.5620000000000003e-05,
      "loss": 0.0015,
      "step": 103140
    },
    {
      "epoch": 5.501333333333333,
      "grad_norm": 0.3054908215999603,
      "learning_rate": 1.561666666666667e-05,
      "loss": 0.0018,
      "step": 103150
    },
    {
      "epoch": 5.5018666666666665,
      "grad_norm": 0.20594029128551483,
      "learning_rate": 1.5613333333333335e-05,
      "loss": 0.0013,
      "step": 103160
    },
    {
      "epoch": 5.5024,
      "grad_norm": 0.39385610818862915,
      "learning_rate": 1.561e-05,
      "loss": 0.0019,
      "step": 103170
    },
    {
      "epoch": 5.502933333333333,
      "grad_norm": 0.06662563234567642,
      "learning_rate": 1.5606666666666667e-05,
      "loss": 0.0014,
      "step": 103180
    },
    {
      "epoch": 5.503466666666666,
      "grad_norm": 0.18171720206737518,
      "learning_rate": 1.5603333333333334e-05,
      "loss": 0.0035,
      "step": 103190
    },
    {
      "epoch": 5.504,
      "grad_norm": 0.21973279118537903,
      "learning_rate": 1.56e-05,
      "loss": 0.0014,
      "step": 103200
    },
    {
      "epoch": 5.504533333333333,
      "grad_norm": 0.035158395767211914,
      "learning_rate": 1.559666666666667e-05,
      "loss": 0.002,
      "step": 103210
    },
    {
      "epoch": 5.505066666666667,
      "grad_norm": 0.09941495954990387,
      "learning_rate": 1.5593333333333335e-05,
      "loss": 0.0023,
      "step": 103220
    },
    {
      "epoch": 5.5056,
      "grad_norm": 0.3751937448978424,
      "learning_rate": 1.559e-05,
      "loss": 0.0019,
      "step": 103230
    },
    {
      "epoch": 5.5061333333333335,
      "grad_norm": 0.4797665476799011,
      "learning_rate": 1.5586666666666668e-05,
      "loss": 0.0017,
      "step": 103240
    },
    {
      "epoch": 5.506666666666667,
      "grad_norm": 0.24261237680912018,
      "learning_rate": 1.5583333333333334e-05,
      "loss": 0.0019,
      "step": 103250
    },
    {
      "epoch": 5.5072,
      "grad_norm": 0.1594071239233017,
      "learning_rate": 1.558e-05,
      "loss": 0.0021,
      "step": 103260
    },
    {
      "epoch": 5.507733333333333,
      "grad_norm": 0.2558354139328003,
      "learning_rate": 1.5576666666666666e-05,
      "loss": 0.0016,
      "step": 103270
    },
    {
      "epoch": 5.508266666666667,
      "grad_norm": 0.06471741199493408,
      "learning_rate": 1.5573333333333336e-05,
      "loss": 0.0021,
      "step": 103280
    },
    {
      "epoch": 5.5088,
      "grad_norm": 0.36374637484550476,
      "learning_rate": 1.5570000000000002e-05,
      "loss": 0.0027,
      "step": 103290
    },
    {
      "epoch": 5.509333333333333,
      "grad_norm": 0.19487422704696655,
      "learning_rate": 1.5566666666666668e-05,
      "loss": 0.0015,
      "step": 103300
    },
    {
      "epoch": 5.5098666666666665,
      "grad_norm": 0.038470558822155,
      "learning_rate": 1.5563333333333334e-05,
      "loss": 0.0013,
      "step": 103310
    },
    {
      "epoch": 5.5104,
      "grad_norm": 0.1925441324710846,
      "learning_rate": 1.556e-05,
      "loss": 0.0019,
      "step": 103320
    },
    {
      "epoch": 5.510933333333333,
      "grad_norm": 0.3202771246433258,
      "learning_rate": 1.5556666666666666e-05,
      "loss": 0.0017,
      "step": 103330
    },
    {
      "epoch": 5.511466666666666,
      "grad_norm": 0.29558923840522766,
      "learning_rate": 1.5553333333333332e-05,
      "loss": 0.0025,
      "step": 103340
    },
    {
      "epoch": 5.5120000000000005,
      "grad_norm": 0.5918017625808716,
      "learning_rate": 1.5550000000000002e-05,
      "loss": 0.0019,
      "step": 103350
    },
    {
      "epoch": 5.512533333333334,
      "grad_norm": 0.13147829473018646,
      "learning_rate": 1.5546666666666668e-05,
      "loss": 0.0017,
      "step": 103360
    },
    {
      "epoch": 5.513066666666667,
      "grad_norm": 0.09375929087400436,
      "learning_rate": 1.5543333333333334e-05,
      "loss": 0.0013,
      "step": 103370
    },
    {
      "epoch": 5.5136,
      "grad_norm": 0.06347205489873886,
      "learning_rate": 1.554e-05,
      "loss": 0.0021,
      "step": 103380
    },
    {
      "epoch": 5.5141333333333336,
      "grad_norm": 0.3298008143901825,
      "learning_rate": 1.5536666666666666e-05,
      "loss": 0.0016,
      "step": 103390
    },
    {
      "epoch": 5.514666666666667,
      "grad_norm": 0.21516427397727966,
      "learning_rate": 1.5533333333333333e-05,
      "loss": 0.002,
      "step": 103400
    },
    {
      "epoch": 5.5152,
      "grad_norm": 0.34637606143951416,
      "learning_rate": 1.553e-05,
      "loss": 0.0013,
      "step": 103410
    },
    {
      "epoch": 5.515733333333333,
      "grad_norm": 0.3787885010242462,
      "learning_rate": 1.5526666666666668e-05,
      "loss": 0.0015,
      "step": 103420
    },
    {
      "epoch": 5.516266666666667,
      "grad_norm": 0.1218174546957016,
      "learning_rate": 1.5523333333333334e-05,
      "loss": 0.0016,
      "step": 103430
    },
    {
      "epoch": 5.5168,
      "grad_norm": 0.1372455656528473,
      "learning_rate": 1.552e-05,
      "loss": 0.0016,
      "step": 103440
    },
    {
      "epoch": 5.517333333333333,
      "grad_norm": 0.22126175463199615,
      "learning_rate": 1.5516666666666667e-05,
      "loss": 0.0028,
      "step": 103450
    },
    {
      "epoch": 5.5178666666666665,
      "grad_norm": 0.06713486462831497,
      "learning_rate": 1.5513333333333336e-05,
      "loss": 0.0012,
      "step": 103460
    },
    {
      "epoch": 5.5184,
      "grad_norm": 0.09296107292175293,
      "learning_rate": 1.551e-05,
      "loss": 0.0024,
      "step": 103470
    },
    {
      "epoch": 5.518933333333333,
      "grad_norm": 0.1834602952003479,
      "learning_rate": 1.5506666666666665e-05,
      "loss": 0.0019,
      "step": 103480
    },
    {
      "epoch": 5.519466666666666,
      "grad_norm": 0.061692725867033005,
      "learning_rate": 1.5503333333333335e-05,
      "loss": 0.0013,
      "step": 103490
    },
    {
      "epoch": 5.52,
      "grad_norm": 0.3016456961631775,
      "learning_rate": 1.55e-05,
      "loss": 0.0019,
      "step": 103500
    },
    {
      "epoch": 5.520533333333333,
      "grad_norm": 0.03831998258829117,
      "learning_rate": 1.5496666666666667e-05,
      "loss": 0.0012,
      "step": 103510
    },
    {
      "epoch": 5.521066666666667,
      "grad_norm": 0.20533603429794312,
      "learning_rate": 1.5493333333333336e-05,
      "loss": 0.0019,
      "step": 103520
    },
    {
      "epoch": 5.5216,
      "grad_norm": 0.1508024036884308,
      "learning_rate": 1.5490000000000002e-05,
      "loss": 0.0019,
      "step": 103530
    },
    {
      "epoch": 5.522133333333334,
      "grad_norm": 0.1351214200258255,
      "learning_rate": 1.548666666666667e-05,
      "loss": 0.0016,
      "step": 103540
    },
    {
      "epoch": 5.522666666666667,
      "grad_norm": 0.22671449184417725,
      "learning_rate": 1.548333333333333e-05,
      "loss": 0.0017,
      "step": 103550
    },
    {
      "epoch": 5.5232,
      "grad_norm": 0.9021813869476318,
      "learning_rate": 1.548e-05,
      "loss": 0.0014,
      "step": 103560
    },
    {
      "epoch": 5.523733333333333,
      "grad_norm": 0.05515396595001221,
      "learning_rate": 1.5476666666666667e-05,
      "loss": 0.0019,
      "step": 103570
    },
    {
      "epoch": 5.524266666666667,
      "grad_norm": 0.2759699523448944,
      "learning_rate": 1.5473333333333333e-05,
      "loss": 0.002,
      "step": 103580
    },
    {
      "epoch": 5.5248,
      "grad_norm": 0.17230461537837982,
      "learning_rate": 1.5470000000000003e-05,
      "loss": 0.0018,
      "step": 103590
    },
    {
      "epoch": 5.525333333333333,
      "grad_norm": 0.13423873484134674,
      "learning_rate": 1.546666666666667e-05,
      "loss": 0.0015,
      "step": 103600
    },
    {
      "epoch": 5.5258666666666665,
      "grad_norm": 0.07713330537080765,
      "learning_rate": 1.5463333333333335e-05,
      "loss": 0.0017,
      "step": 103610
    },
    {
      "epoch": 5.5264,
      "grad_norm": 0.25170817971229553,
      "learning_rate": 1.546e-05,
      "loss": 0.0021,
      "step": 103620
    },
    {
      "epoch": 5.526933333333333,
      "grad_norm": 0.04123920574784279,
      "learning_rate": 1.5456666666666667e-05,
      "loss": 0.0023,
      "step": 103630
    },
    {
      "epoch": 5.527466666666666,
      "grad_norm": 0.10318316519260406,
      "learning_rate": 1.5453333333333333e-05,
      "loss": 0.0011,
      "step": 103640
    },
    {
      "epoch": 5.5280000000000005,
      "grad_norm": 0.10015534609556198,
      "learning_rate": 1.545e-05,
      "loss": 0.0014,
      "step": 103650
    },
    {
      "epoch": 5.528533333333334,
      "grad_norm": 0.09190323203802109,
      "learning_rate": 1.544666666666667e-05,
      "loss": 0.0019,
      "step": 103660
    },
    {
      "epoch": 5.529066666666667,
      "grad_norm": 0.11285682767629623,
      "learning_rate": 1.5443333333333335e-05,
      "loss": 0.0013,
      "step": 103670
    },
    {
      "epoch": 5.5296,
      "grad_norm": 0.1560785472393036,
      "learning_rate": 1.544e-05,
      "loss": 0.0016,
      "step": 103680
    },
    {
      "epoch": 5.530133333333334,
      "grad_norm": 0.5702099204063416,
      "learning_rate": 1.5436666666666667e-05,
      "loss": 0.002,
      "step": 103690
    },
    {
      "epoch": 5.530666666666667,
      "grad_norm": 0.07255590707063675,
      "learning_rate": 1.5433333333333334e-05,
      "loss": 0.0014,
      "step": 103700
    },
    {
      "epoch": 5.5312,
      "grad_norm": 0.026610495522618294,
      "learning_rate": 1.543e-05,
      "loss": 0.002,
      "step": 103710
    },
    {
      "epoch": 5.531733333333333,
      "grad_norm": 0.26355719566345215,
      "learning_rate": 1.5426666666666666e-05,
      "loss": 0.0019,
      "step": 103720
    },
    {
      "epoch": 5.532266666666667,
      "grad_norm": 0.12124670296907425,
      "learning_rate": 1.5423333333333335e-05,
      "loss": 0.003,
      "step": 103730
    },
    {
      "epoch": 5.5328,
      "grad_norm": 0.5039131045341492,
      "learning_rate": 1.542e-05,
      "loss": 0.0019,
      "step": 103740
    },
    {
      "epoch": 5.533333333333333,
      "grad_norm": 0.3600583076477051,
      "learning_rate": 1.5416666666666668e-05,
      "loss": 0.0015,
      "step": 103750
    },
    {
      "epoch": 5.5338666666666665,
      "grad_norm": 0.12111257761716843,
      "learning_rate": 1.5413333333333334e-05,
      "loss": 0.0019,
      "step": 103760
    },
    {
      "epoch": 5.5344,
      "grad_norm": 0.653656005859375,
      "learning_rate": 1.541e-05,
      "loss": 0.0017,
      "step": 103770
    },
    {
      "epoch": 5.534933333333333,
      "grad_norm": 0.38222718238830566,
      "learning_rate": 1.5406666666666666e-05,
      "loss": 0.0012,
      "step": 103780
    },
    {
      "epoch": 5.535466666666666,
      "grad_norm": 0.13496336340904236,
      "learning_rate": 1.5403333333333332e-05,
      "loss": 0.0018,
      "step": 103790
    },
    {
      "epoch": 5.536,
      "grad_norm": 0.07732774317264557,
      "learning_rate": 1.54e-05,
      "loss": 0.0019,
      "step": 103800
    },
    {
      "epoch": 5.536533333333333,
      "grad_norm": 0.30697140097618103,
      "learning_rate": 1.5396666666666668e-05,
      "loss": 0.0023,
      "step": 103810
    },
    {
      "epoch": 5.537066666666667,
      "grad_norm": 0.19222259521484375,
      "learning_rate": 1.5393333333333334e-05,
      "loss": 0.0018,
      "step": 103820
    },
    {
      "epoch": 5.5376,
      "grad_norm": 0.20606139302253723,
      "learning_rate": 1.539e-05,
      "loss": 0.0029,
      "step": 103830
    },
    {
      "epoch": 5.538133333333334,
      "grad_norm": 0.15417294204235077,
      "learning_rate": 1.538666666666667e-05,
      "loss": 0.0017,
      "step": 103840
    },
    {
      "epoch": 5.538666666666667,
      "grad_norm": 0.643319308757782,
      "learning_rate": 1.5383333333333332e-05,
      "loss": 0.0012,
      "step": 103850
    },
    {
      "epoch": 5.5392,
      "grad_norm": 0.59747314453125,
      "learning_rate": 1.538e-05,
      "loss": 0.0016,
      "step": 103860
    },
    {
      "epoch": 5.539733333333333,
      "grad_norm": 0.3030117452144623,
      "learning_rate": 1.5376666666666668e-05,
      "loss": 0.0016,
      "step": 103870
    },
    {
      "epoch": 5.540266666666667,
      "grad_norm": 0.576892614364624,
      "learning_rate": 1.5373333333333334e-05,
      "loss": 0.0016,
      "step": 103880
    },
    {
      "epoch": 5.5408,
      "grad_norm": 0.182389035820961,
      "learning_rate": 1.537e-05,
      "loss": 0.0016,
      "step": 103890
    },
    {
      "epoch": 5.541333333333333,
      "grad_norm": 0.0638912096619606,
      "learning_rate": 1.536666666666667e-05,
      "loss": 0.0016,
      "step": 103900
    },
    {
      "epoch": 5.5418666666666665,
      "grad_norm": 0.10263098031282425,
      "learning_rate": 1.5363333333333336e-05,
      "loss": 0.0021,
      "step": 103910
    },
    {
      "epoch": 5.5424,
      "grad_norm": 0.19207823276519775,
      "learning_rate": 1.536e-05,
      "loss": 0.0018,
      "step": 103920
    },
    {
      "epoch": 5.542933333333333,
      "grad_norm": 0.3135109841823578,
      "learning_rate": 1.5356666666666665e-05,
      "loss": 0.0015,
      "step": 103930
    },
    {
      "epoch": 5.543466666666666,
      "grad_norm": 0.15396039187908173,
      "learning_rate": 1.5353333333333334e-05,
      "loss": 0.0017,
      "step": 103940
    },
    {
      "epoch": 5.5440000000000005,
      "grad_norm": 0.28790464997291565,
      "learning_rate": 1.535e-05,
      "loss": 0.0027,
      "step": 103950
    },
    {
      "epoch": 5.544533333333334,
      "grad_norm": 0.0540829636156559,
      "learning_rate": 1.5346666666666667e-05,
      "loss": 0.0017,
      "step": 103960
    },
    {
      "epoch": 5.545066666666667,
      "grad_norm": 0.09409917145967484,
      "learning_rate": 1.5343333333333336e-05,
      "loss": 0.0015,
      "step": 103970
    },
    {
      "epoch": 5.5456,
      "grad_norm": 0.32251426577568054,
      "learning_rate": 1.5340000000000002e-05,
      "loss": 0.0022,
      "step": 103980
    },
    {
      "epoch": 5.546133333333334,
      "grad_norm": 0.12615548074245453,
      "learning_rate": 1.533666666666667e-05,
      "loss": 0.0031,
      "step": 103990
    },
    {
      "epoch": 5.546666666666667,
      "grad_norm": 0.09836461395025253,
      "learning_rate": 1.5333333333333334e-05,
      "loss": 0.0021,
      "step": 104000
    },
    {
      "epoch": 5.5472,
      "grad_norm": 0.07362254709005356,
      "learning_rate": 1.533e-05,
      "loss": 0.0014,
      "step": 104010
    },
    {
      "epoch": 5.547733333333333,
      "grad_norm": 0.1392703652381897,
      "learning_rate": 1.5326666666666667e-05,
      "loss": 0.0013,
      "step": 104020
    },
    {
      "epoch": 5.548266666666667,
      "grad_norm": 0.07217118889093399,
      "learning_rate": 1.5323333333333333e-05,
      "loss": 0.0015,
      "step": 104030
    },
    {
      "epoch": 5.5488,
      "grad_norm": 0.06155623123049736,
      "learning_rate": 1.5320000000000002e-05,
      "loss": 0.0015,
      "step": 104040
    },
    {
      "epoch": 5.549333333333333,
      "grad_norm": 0.22024692595005035,
      "learning_rate": 1.531666666666667e-05,
      "loss": 0.0024,
      "step": 104050
    },
    {
      "epoch": 5.5498666666666665,
      "grad_norm": 0.1559225469827652,
      "learning_rate": 1.5313333333333335e-05,
      "loss": 0.0017,
      "step": 104060
    },
    {
      "epoch": 5.5504,
      "grad_norm": 0.05078669264912605,
      "learning_rate": 1.531e-05,
      "loss": 0.0016,
      "step": 104070
    },
    {
      "epoch": 5.550933333333333,
      "grad_norm": 0.23781269788742065,
      "learning_rate": 1.5306666666666667e-05,
      "loss": 0.0023,
      "step": 104080
    },
    {
      "epoch": 5.551466666666666,
      "grad_norm": 0.10196094214916229,
      "learning_rate": 1.5303333333333333e-05,
      "loss": 0.0016,
      "step": 104090
    },
    {
      "epoch": 5.552,
      "grad_norm": 0.6116535663604736,
      "learning_rate": 1.53e-05,
      "loss": 0.0013,
      "step": 104100
    },
    {
      "epoch": 5.552533333333333,
      "grad_norm": 0.2192862182855606,
      "learning_rate": 1.529666666666667e-05,
      "loss": 0.0025,
      "step": 104110
    },
    {
      "epoch": 5.553066666666667,
      "grad_norm": 0.09845008701086044,
      "learning_rate": 1.5293333333333335e-05,
      "loss": 0.0015,
      "step": 104120
    },
    {
      "epoch": 5.5536,
      "grad_norm": 0.13933436572551727,
      "learning_rate": 1.529e-05,
      "loss": 0.003,
      "step": 104130
    },
    {
      "epoch": 5.554133333333334,
      "grad_norm": 0.2419549822807312,
      "learning_rate": 1.5286666666666667e-05,
      "loss": 0.0013,
      "step": 104140
    },
    {
      "epoch": 5.554666666666667,
      "grad_norm": 0.13243702054023743,
      "learning_rate": 1.5283333333333333e-05,
      "loss": 0.0017,
      "step": 104150
    },
    {
      "epoch": 5.5552,
      "grad_norm": 0.0689643993973732,
      "learning_rate": 1.528e-05,
      "loss": 0.0019,
      "step": 104160
    },
    {
      "epoch": 5.555733333333333,
      "grad_norm": 0.2407018095254898,
      "learning_rate": 1.5276666666666666e-05,
      "loss": 0.0017,
      "step": 104170
    },
    {
      "epoch": 5.556266666666667,
      "grad_norm": 0.2711469531059265,
      "learning_rate": 1.5273333333333335e-05,
      "loss": 0.0014,
      "step": 104180
    },
    {
      "epoch": 5.5568,
      "grad_norm": 0.1211593896150589,
      "learning_rate": 1.527e-05,
      "loss": 0.0028,
      "step": 104190
    },
    {
      "epoch": 5.557333333333333,
      "grad_norm": 0.07406560331583023,
      "learning_rate": 1.5266666666666667e-05,
      "loss": 0.0012,
      "step": 104200
    },
    {
      "epoch": 5.5578666666666665,
      "grad_norm": 0.14037497341632843,
      "learning_rate": 1.5263333333333333e-05,
      "loss": 0.0018,
      "step": 104210
    },
    {
      "epoch": 5.5584,
      "grad_norm": 0.418917179107666,
      "learning_rate": 1.5260000000000003e-05,
      "loss": 0.0029,
      "step": 104220
    },
    {
      "epoch": 5.558933333333333,
      "grad_norm": 0.10642235726118088,
      "learning_rate": 1.5256666666666666e-05,
      "loss": 0.0024,
      "step": 104230
    },
    {
      "epoch": 5.559466666666666,
      "grad_norm": 0.27441003918647766,
      "learning_rate": 1.5253333333333334e-05,
      "loss": 0.0017,
      "step": 104240
    },
    {
      "epoch": 5.5600000000000005,
      "grad_norm": 0.026975592598319054,
      "learning_rate": 1.525e-05,
      "loss": 0.0012,
      "step": 104250
    },
    {
      "epoch": 5.560533333333334,
      "grad_norm": 0.037449829280376434,
      "learning_rate": 1.5246666666666668e-05,
      "loss": 0.0023,
      "step": 104260
    },
    {
      "epoch": 5.561066666666667,
      "grad_norm": 0.2658727169036865,
      "learning_rate": 1.5243333333333334e-05,
      "loss": 0.0021,
      "step": 104270
    },
    {
      "epoch": 5.5616,
      "grad_norm": 0.12673397362232208,
      "learning_rate": 1.5240000000000001e-05,
      "loss": 0.0022,
      "step": 104280
    },
    {
      "epoch": 5.562133333333334,
      "grad_norm": 0.4820869565010071,
      "learning_rate": 1.523666666666667e-05,
      "loss": 0.0015,
      "step": 104290
    },
    {
      "epoch": 5.562666666666667,
      "grad_norm": 0.22004394233226776,
      "learning_rate": 1.5233333333333332e-05,
      "loss": 0.002,
      "step": 104300
    },
    {
      "epoch": 5.5632,
      "grad_norm": 0.2674037516117096,
      "learning_rate": 1.523e-05,
      "loss": 0.002,
      "step": 104310
    },
    {
      "epoch": 5.563733333333333,
      "grad_norm": 0.3275616466999054,
      "learning_rate": 1.5226666666666668e-05,
      "loss": 0.0028,
      "step": 104320
    },
    {
      "epoch": 5.564266666666667,
      "grad_norm": 0.2777062952518463,
      "learning_rate": 1.5223333333333334e-05,
      "loss": 0.0016,
      "step": 104330
    },
    {
      "epoch": 5.5648,
      "grad_norm": 0.15248584747314453,
      "learning_rate": 1.5220000000000002e-05,
      "loss": 0.0017,
      "step": 104340
    },
    {
      "epoch": 5.565333333333333,
      "grad_norm": 0.30699434876441956,
      "learning_rate": 1.5216666666666668e-05,
      "loss": 0.0017,
      "step": 104350
    },
    {
      "epoch": 5.5658666666666665,
      "grad_norm": 0.30241698026657104,
      "learning_rate": 1.5213333333333336e-05,
      "loss": 0.0019,
      "step": 104360
    },
    {
      "epoch": 5.5664,
      "grad_norm": 0.29541826248168945,
      "learning_rate": 1.5210000000000002e-05,
      "loss": 0.0016,
      "step": 104370
    },
    {
      "epoch": 5.566933333333333,
      "grad_norm": 0.10694742202758789,
      "learning_rate": 1.5206666666666666e-05,
      "loss": 0.0015,
      "step": 104380
    },
    {
      "epoch": 5.567466666666666,
      "grad_norm": 0.12505468726158142,
      "learning_rate": 1.5203333333333334e-05,
      "loss": 0.0024,
      "step": 104390
    },
    {
      "epoch": 5.568,
      "grad_norm": 0.385181188583374,
      "learning_rate": 1.52e-05,
      "loss": 0.002,
      "step": 104400
    },
    {
      "epoch": 5.568533333333333,
      "grad_norm": 0.27896466851234436,
      "learning_rate": 1.5196666666666668e-05,
      "loss": 0.0022,
      "step": 104410
    },
    {
      "epoch": 5.569066666666667,
      "grad_norm": 0.1568412333726883,
      "learning_rate": 1.5193333333333334e-05,
      "loss": 0.0019,
      "step": 104420
    },
    {
      "epoch": 5.5696,
      "grad_norm": 0.045668888837099075,
      "learning_rate": 1.5190000000000002e-05,
      "loss": 0.0019,
      "step": 104430
    },
    {
      "epoch": 5.570133333333334,
      "grad_norm": 0.0645735040307045,
      "learning_rate": 1.5186666666666668e-05,
      "loss": 0.0023,
      "step": 104440
    },
    {
      "epoch": 5.570666666666667,
      "grad_norm": 0.5098708271980286,
      "learning_rate": 1.5183333333333333e-05,
      "loss": 0.0015,
      "step": 104450
    },
    {
      "epoch": 5.5712,
      "grad_norm": 0.5120403170585632,
      "learning_rate": 1.518e-05,
      "loss": 0.0015,
      "step": 104460
    },
    {
      "epoch": 5.571733333333333,
      "grad_norm": 0.1306324005126953,
      "learning_rate": 1.5176666666666666e-05,
      "loss": 0.0021,
      "step": 104470
    },
    {
      "epoch": 5.572266666666667,
      "grad_norm": 0.16243109107017517,
      "learning_rate": 1.5173333333333334e-05,
      "loss": 0.0018,
      "step": 104480
    },
    {
      "epoch": 5.5728,
      "grad_norm": 0.2703656256198883,
      "learning_rate": 1.517e-05,
      "loss": 0.0015,
      "step": 104490
    },
    {
      "epoch": 5.573333333333333,
      "grad_norm": 0.5730525255203247,
      "learning_rate": 1.5166666666666668e-05,
      "loss": 0.0011,
      "step": 104500
    },
    {
      "epoch": 5.5738666666666665,
      "grad_norm": 0.06034190207719803,
      "learning_rate": 1.5163333333333334e-05,
      "loss": 0.002,
      "step": 104510
    },
    {
      "epoch": 5.5744,
      "grad_norm": 0.3574543297290802,
      "learning_rate": 1.5160000000000002e-05,
      "loss": 0.0015,
      "step": 104520
    },
    {
      "epoch": 5.574933333333333,
      "grad_norm": 0.5010174512863159,
      "learning_rate": 1.5156666666666667e-05,
      "loss": 0.0021,
      "step": 104530
    },
    {
      "epoch": 5.575466666666666,
      "grad_norm": 0.22577638924121857,
      "learning_rate": 1.5153333333333333e-05,
      "loss": 0.0029,
      "step": 104540
    },
    {
      "epoch": 5.576,
      "grad_norm": 0.1465720236301422,
      "learning_rate": 1.515e-05,
      "loss": 0.0028,
      "step": 104550
    },
    {
      "epoch": 5.576533333333334,
      "grad_norm": 0.3540440499782562,
      "learning_rate": 1.5146666666666667e-05,
      "loss": 0.0021,
      "step": 104560
    },
    {
      "epoch": 5.577066666666667,
      "grad_norm": 0.15728211402893066,
      "learning_rate": 1.5143333333333335e-05,
      "loss": 0.0014,
      "step": 104570
    },
    {
      "epoch": 5.5776,
      "grad_norm": 0.06869816035032272,
      "learning_rate": 1.514e-05,
      "loss": 0.0021,
      "step": 104580
    },
    {
      "epoch": 5.578133333333334,
      "grad_norm": 0.2443292737007141,
      "learning_rate": 1.5136666666666669e-05,
      "loss": 0.0016,
      "step": 104590
    },
    {
      "epoch": 5.578666666666667,
      "grad_norm": 0.05063917487859726,
      "learning_rate": 1.5133333333333333e-05,
      "loss": 0.0015,
      "step": 104600
    },
    {
      "epoch": 5.5792,
      "grad_norm": 0.22237923741340637,
      "learning_rate": 1.5129999999999999e-05,
      "loss": 0.0018,
      "step": 104610
    },
    {
      "epoch": 5.579733333333333,
      "grad_norm": 0.3557804524898529,
      "learning_rate": 1.5126666666666667e-05,
      "loss": 0.0017,
      "step": 104620
    },
    {
      "epoch": 5.580266666666667,
      "grad_norm": 0.04192577302455902,
      "learning_rate": 1.5123333333333333e-05,
      "loss": 0.0026,
      "step": 104630
    },
    {
      "epoch": 5.5808,
      "grad_norm": 0.6436235904693604,
      "learning_rate": 1.5120000000000001e-05,
      "loss": 0.0021,
      "step": 104640
    },
    {
      "epoch": 5.581333333333333,
      "grad_norm": 0.3031909763813019,
      "learning_rate": 1.5116666666666667e-05,
      "loss": 0.0018,
      "step": 104650
    },
    {
      "epoch": 5.5818666666666665,
      "grad_norm": 0.09432429820299149,
      "learning_rate": 1.5113333333333335e-05,
      "loss": 0.0024,
      "step": 104660
    },
    {
      "epoch": 5.5824,
      "grad_norm": 0.3173019587993622,
      "learning_rate": 1.5110000000000003e-05,
      "loss": 0.0019,
      "step": 104670
    },
    {
      "epoch": 5.582933333333333,
      "grad_norm": 0.4262429475784302,
      "learning_rate": 1.5106666666666665e-05,
      "loss": 0.0019,
      "step": 104680
    },
    {
      "epoch": 5.583466666666666,
      "grad_norm": 0.18894371390342712,
      "learning_rate": 1.5103333333333333e-05,
      "loss": 0.002,
      "step": 104690
    },
    {
      "epoch": 5.584,
      "grad_norm": 0.1570105254650116,
      "learning_rate": 1.51e-05,
      "loss": 0.0015,
      "step": 104700
    },
    {
      "epoch": 5.584533333333333,
      "grad_norm": 0.1017695814371109,
      "learning_rate": 1.5096666666666667e-05,
      "loss": 0.0024,
      "step": 104710
    },
    {
      "epoch": 5.585066666666666,
      "grad_norm": 0.09503886103630066,
      "learning_rate": 1.5093333333333335e-05,
      "loss": 0.003,
      "step": 104720
    },
    {
      "epoch": 5.5856,
      "grad_norm": 0.06240350008010864,
      "learning_rate": 1.5090000000000001e-05,
      "loss": 0.0019,
      "step": 104730
    },
    {
      "epoch": 5.586133333333334,
      "grad_norm": 0.10293964296579361,
      "learning_rate": 1.5086666666666669e-05,
      "loss": 0.0015,
      "step": 104740
    },
    {
      "epoch": 5.586666666666667,
      "grad_norm": 0.15733370184898376,
      "learning_rate": 1.5083333333333335e-05,
      "loss": 0.0018,
      "step": 104750
    },
    {
      "epoch": 5.5872,
      "grad_norm": 0.7584469318389893,
      "learning_rate": 1.508e-05,
      "loss": 0.0019,
      "step": 104760
    },
    {
      "epoch": 5.587733333333333,
      "grad_norm": 0.32506442070007324,
      "learning_rate": 1.5076666666666667e-05,
      "loss": 0.0023,
      "step": 104770
    },
    {
      "epoch": 5.588266666666667,
      "grad_norm": 0.03895607218146324,
      "learning_rate": 1.5073333333333334e-05,
      "loss": 0.0028,
      "step": 104780
    },
    {
      "epoch": 5.5888,
      "grad_norm": 0.23613111674785614,
      "learning_rate": 1.5070000000000001e-05,
      "loss": 0.0012,
      "step": 104790
    },
    {
      "epoch": 5.589333333333333,
      "grad_norm": 0.22607162594795227,
      "learning_rate": 1.5066666666666668e-05,
      "loss": 0.0015,
      "step": 104800
    },
    {
      "epoch": 5.5898666666666665,
      "grad_norm": 0.2530786991119385,
      "learning_rate": 1.5063333333333335e-05,
      "loss": 0.0013,
      "step": 104810
    },
    {
      "epoch": 5.5904,
      "grad_norm": 0.4774279296398163,
      "learning_rate": 1.5060000000000001e-05,
      "loss": 0.0016,
      "step": 104820
    },
    {
      "epoch": 5.590933333333333,
      "grad_norm": 0.05423246696591377,
      "learning_rate": 1.5056666666666666e-05,
      "loss": 0.0015,
      "step": 104830
    },
    {
      "epoch": 5.591466666666666,
      "grad_norm": 0.08413833379745483,
      "learning_rate": 1.5053333333333334e-05,
      "loss": 0.0021,
      "step": 104840
    },
    {
      "epoch": 5.592,
      "grad_norm": 0.09786663949489594,
      "learning_rate": 1.505e-05,
      "loss": 0.0015,
      "step": 104850
    },
    {
      "epoch": 5.592533333333334,
      "grad_norm": 0.23098118603229523,
      "learning_rate": 1.5046666666666668e-05,
      "loss": 0.0027,
      "step": 104860
    },
    {
      "epoch": 5.593066666666667,
      "grad_norm": 0.11721091717481613,
      "learning_rate": 1.5043333333333334e-05,
      "loss": 0.0014,
      "step": 104870
    },
    {
      "epoch": 5.5936,
      "grad_norm": 0.3323340117931366,
      "learning_rate": 1.5040000000000002e-05,
      "loss": 0.0021,
      "step": 104880
    },
    {
      "epoch": 5.594133333333334,
      "grad_norm": 0.28600794076919556,
      "learning_rate": 1.5036666666666668e-05,
      "loss": 0.0022,
      "step": 104890
    },
    {
      "epoch": 5.594666666666667,
      "grad_norm": 0.07863076031208038,
      "learning_rate": 1.5033333333333336e-05,
      "loss": 0.0016,
      "step": 104900
    },
    {
      "epoch": 5.5952,
      "grad_norm": 0.23777903616428375,
      "learning_rate": 1.503e-05,
      "loss": 0.0015,
      "step": 104910
    },
    {
      "epoch": 5.5957333333333334,
      "grad_norm": 0.18763770163059235,
      "learning_rate": 1.5026666666666666e-05,
      "loss": 0.0013,
      "step": 104920
    },
    {
      "epoch": 5.596266666666667,
      "grad_norm": 0.318549245595932,
      "learning_rate": 1.5023333333333334e-05,
      "loss": 0.0017,
      "step": 104930
    },
    {
      "epoch": 5.5968,
      "grad_norm": 0.15937179327011108,
      "learning_rate": 1.502e-05,
      "loss": 0.0016,
      "step": 104940
    },
    {
      "epoch": 5.597333333333333,
      "grad_norm": 0.21189172565937042,
      "learning_rate": 1.5016666666666668e-05,
      "loss": 0.0021,
      "step": 104950
    },
    {
      "epoch": 5.5978666666666665,
      "grad_norm": 0.09375608712434769,
      "learning_rate": 1.5013333333333334e-05,
      "loss": 0.0016,
      "step": 104960
    },
    {
      "epoch": 5.5984,
      "grad_norm": 0.07059724628925323,
      "learning_rate": 1.5010000000000002e-05,
      "loss": 0.0015,
      "step": 104970
    },
    {
      "epoch": 5.598933333333333,
      "grad_norm": 0.17688795924186707,
      "learning_rate": 1.5006666666666666e-05,
      "loss": 0.002,
      "step": 104980
    },
    {
      "epoch": 5.599466666666666,
      "grad_norm": 0.2947385311126709,
      "learning_rate": 1.5003333333333333e-05,
      "loss": 0.0019,
      "step": 104990
    },
    {
      "epoch": 5.6,
      "grad_norm": 0.15378902852535248,
      "learning_rate": 1.5e-05,
      "loss": 0.0018,
      "step": 105000
    },
    {
      "epoch": 5.600533333333333,
      "grad_norm": 0.16725458204746246,
      "learning_rate": 1.4996666666666667e-05,
      "loss": 0.0015,
      "step": 105010
    },
    {
      "epoch": 5.601066666666666,
      "grad_norm": 0.10737612098455429,
      "learning_rate": 1.4993333333333334e-05,
      "loss": 0.0014,
      "step": 105020
    },
    {
      "epoch": 5.6016,
      "grad_norm": 0.27187642455101013,
      "learning_rate": 1.499e-05,
      "loss": 0.0021,
      "step": 105030
    },
    {
      "epoch": 5.602133333333334,
      "grad_norm": 0.15734823048114777,
      "learning_rate": 1.4986666666666668e-05,
      "loss": 0.0016,
      "step": 105040
    },
    {
      "epoch": 5.602666666666667,
      "grad_norm": 0.10440703481435776,
      "learning_rate": 1.4983333333333336e-05,
      "loss": 0.0014,
      "step": 105050
    },
    {
      "epoch": 5.6032,
      "grad_norm": 0.04219911992549896,
      "learning_rate": 1.4979999999999999e-05,
      "loss": 0.0027,
      "step": 105060
    },
    {
      "epoch": 5.6037333333333335,
      "grad_norm": 0.14932723343372345,
      "learning_rate": 1.4976666666666667e-05,
      "loss": 0.0021,
      "step": 105070
    },
    {
      "epoch": 5.604266666666667,
      "grad_norm": 0.2135499268770218,
      "learning_rate": 1.4973333333333333e-05,
      "loss": 0.0022,
      "step": 105080
    },
    {
      "epoch": 5.6048,
      "grad_norm": 0.21407823264598846,
      "learning_rate": 1.497e-05,
      "loss": 0.0016,
      "step": 105090
    },
    {
      "epoch": 5.605333333333333,
      "grad_norm": 0.4146539270877838,
      "learning_rate": 1.4966666666666668e-05,
      "loss": 0.0018,
      "step": 105100
    },
    {
      "epoch": 5.6058666666666666,
      "grad_norm": 0.27360832691192627,
      "learning_rate": 1.4963333333333335e-05,
      "loss": 0.0018,
      "step": 105110
    },
    {
      "epoch": 5.6064,
      "grad_norm": 0.09060182422399521,
      "learning_rate": 1.4960000000000002e-05,
      "loss": 0.0027,
      "step": 105120
    },
    {
      "epoch": 5.606933333333333,
      "grad_norm": 0.23316113650798798,
      "learning_rate": 1.4956666666666665e-05,
      "loss": 0.002,
      "step": 105130
    },
    {
      "epoch": 5.607466666666666,
      "grad_norm": 0.12380947917699814,
      "learning_rate": 1.4953333333333333e-05,
      "loss": 0.0013,
      "step": 105140
    },
    {
      "epoch": 5.608,
      "grad_norm": 0.0395226776599884,
      "learning_rate": 1.4950000000000001e-05,
      "loss": 0.0019,
      "step": 105150
    },
    {
      "epoch": 5.608533333333334,
      "grad_norm": 0.31801530718803406,
      "learning_rate": 1.4946666666666667e-05,
      "loss": 0.0015,
      "step": 105160
    },
    {
      "epoch": 5.609066666666667,
      "grad_norm": 0.052643466740846634,
      "learning_rate": 1.4943333333333335e-05,
      "loss": 0.0016,
      "step": 105170
    },
    {
      "epoch": 5.6096,
      "grad_norm": 0.10093088448047638,
      "learning_rate": 1.4940000000000001e-05,
      "loss": 0.0014,
      "step": 105180
    },
    {
      "epoch": 5.610133333333334,
      "grad_norm": 0.5745264291763306,
      "learning_rate": 1.4936666666666669e-05,
      "loss": 0.0021,
      "step": 105190
    },
    {
      "epoch": 5.610666666666667,
      "grad_norm": 0.42373743653297424,
      "learning_rate": 1.4933333333333335e-05,
      "loss": 0.0015,
      "step": 105200
    },
    {
      "epoch": 5.6112,
      "grad_norm": 0.30252841114997864,
      "learning_rate": 1.493e-05,
      "loss": 0.0022,
      "step": 105210
    },
    {
      "epoch": 5.6117333333333335,
      "grad_norm": 0.2145129144191742,
      "learning_rate": 1.4926666666666667e-05,
      "loss": 0.0019,
      "step": 105220
    },
    {
      "epoch": 5.612266666666667,
      "grad_norm": 0.30626004934310913,
      "learning_rate": 1.4923333333333333e-05,
      "loss": 0.0016,
      "step": 105230
    },
    {
      "epoch": 5.6128,
      "grad_norm": 0.4622250497341156,
      "learning_rate": 1.4920000000000001e-05,
      "loss": 0.0025,
      "step": 105240
    },
    {
      "epoch": 5.613333333333333,
      "grad_norm": 0.17300105094909668,
      "learning_rate": 1.4916666666666667e-05,
      "loss": 0.0013,
      "step": 105250
    },
    {
      "epoch": 5.613866666666667,
      "grad_norm": 0.44528505206108093,
      "learning_rate": 1.4913333333333335e-05,
      "loss": 0.0027,
      "step": 105260
    },
    {
      "epoch": 5.6144,
      "grad_norm": 0.46729862689971924,
      "learning_rate": 1.4910000000000001e-05,
      "loss": 0.0018,
      "step": 105270
    },
    {
      "epoch": 5.614933333333333,
      "grad_norm": 0.2636357843875885,
      "learning_rate": 1.4906666666666666e-05,
      "loss": 0.0019,
      "step": 105280
    },
    {
      "epoch": 5.615466666666666,
      "grad_norm": 0.5806771516799927,
      "learning_rate": 1.4903333333333334e-05,
      "loss": 0.0019,
      "step": 105290
    },
    {
      "epoch": 5.616,
      "grad_norm": 0.31400448083877563,
      "learning_rate": 1.49e-05,
      "loss": 0.0018,
      "step": 105300
    },
    {
      "epoch": 5.616533333333333,
      "grad_norm": 0.12562839686870575,
      "learning_rate": 1.4896666666666667e-05,
      "loss": 0.0028,
      "step": 105310
    },
    {
      "epoch": 5.617066666666666,
      "grad_norm": 0.456598162651062,
      "learning_rate": 1.4893333333333334e-05,
      "loss": 0.0017,
      "step": 105320
    },
    {
      "epoch": 5.6176,
      "grad_norm": 0.4317808449268341,
      "learning_rate": 1.4890000000000001e-05,
      "loss": 0.0022,
      "step": 105330
    },
    {
      "epoch": 5.618133333333334,
      "grad_norm": 0.2378968596458435,
      "learning_rate": 1.4886666666666668e-05,
      "loss": 0.002,
      "step": 105340
    },
    {
      "epoch": 5.618666666666667,
      "grad_norm": 0.22676150500774384,
      "learning_rate": 1.4883333333333335e-05,
      "loss": 0.0021,
      "step": 105350
    },
    {
      "epoch": 5.6192,
      "grad_norm": 0.5396950840950012,
      "learning_rate": 1.488e-05,
      "loss": 0.0014,
      "step": 105360
    },
    {
      "epoch": 5.6197333333333335,
      "grad_norm": 0.6512134671211243,
      "learning_rate": 1.4876666666666666e-05,
      "loss": 0.003,
      "step": 105370
    },
    {
      "epoch": 5.620266666666667,
      "grad_norm": 0.2439804971218109,
      "learning_rate": 1.4873333333333334e-05,
      "loss": 0.0019,
      "step": 105380
    },
    {
      "epoch": 5.6208,
      "grad_norm": 0.4797206223011017,
      "learning_rate": 1.487e-05,
      "loss": 0.0014,
      "step": 105390
    },
    {
      "epoch": 5.621333333333333,
      "grad_norm": 0.06923036277294159,
      "learning_rate": 1.4866666666666668e-05,
      "loss": 0.002,
      "step": 105400
    },
    {
      "epoch": 5.621866666666667,
      "grad_norm": 0.05044294148683548,
      "learning_rate": 1.4863333333333334e-05,
      "loss": 0.0022,
      "step": 105410
    },
    {
      "epoch": 5.6224,
      "grad_norm": 0.11710135638713837,
      "learning_rate": 1.4860000000000002e-05,
      "loss": 0.002,
      "step": 105420
    },
    {
      "epoch": 5.622933333333333,
      "grad_norm": 0.21074159443378448,
      "learning_rate": 1.485666666666667e-05,
      "loss": 0.0012,
      "step": 105430
    },
    {
      "epoch": 5.623466666666666,
      "grad_norm": 0.19037145376205444,
      "learning_rate": 1.4853333333333332e-05,
      "loss": 0.0019,
      "step": 105440
    },
    {
      "epoch": 5.624,
      "grad_norm": 0.20884431898593903,
      "learning_rate": 1.485e-05,
      "loss": 0.0014,
      "step": 105450
    },
    {
      "epoch": 5.624533333333334,
      "grad_norm": 0.18531052768230438,
      "learning_rate": 1.4846666666666666e-05,
      "loss": 0.0017,
      "step": 105460
    },
    {
      "epoch": 5.625066666666667,
      "grad_norm": 0.178880974650383,
      "learning_rate": 1.4843333333333334e-05,
      "loss": 0.0021,
      "step": 105470
    },
    {
      "epoch": 5.6256,
      "grad_norm": 0.3585430383682251,
      "learning_rate": 1.4840000000000002e-05,
      "loss": 0.0017,
      "step": 105480
    },
    {
      "epoch": 5.626133333333334,
      "grad_norm": 0.43311432003974915,
      "learning_rate": 1.4836666666666668e-05,
      "loss": 0.0017,
      "step": 105490
    },
    {
      "epoch": 5.626666666666667,
      "grad_norm": 0.48530498147010803,
      "learning_rate": 1.4833333333333336e-05,
      "loss": 0.0025,
      "step": 105500
    },
    {
      "epoch": 5.6272,
      "grad_norm": 0.18251529335975647,
      "learning_rate": 1.4829999999999999e-05,
      "loss": 0.0016,
      "step": 105510
    },
    {
      "epoch": 5.6277333333333335,
      "grad_norm": 0.23634770512580872,
      "learning_rate": 1.4826666666666666e-05,
      "loss": 0.0036,
      "step": 105520
    },
    {
      "epoch": 5.628266666666667,
      "grad_norm": 0.11227060109376907,
      "learning_rate": 1.4823333333333334e-05,
      "loss": 0.0018,
      "step": 105530
    },
    {
      "epoch": 5.6288,
      "grad_norm": 0.23878802359104156,
      "learning_rate": 1.482e-05,
      "loss": 0.002,
      "step": 105540
    },
    {
      "epoch": 5.629333333333333,
      "grad_norm": 0.12859699130058289,
      "learning_rate": 1.4816666666666668e-05,
      "loss": 0.0015,
      "step": 105550
    },
    {
      "epoch": 5.629866666666667,
      "grad_norm": 0.2324378490447998,
      "learning_rate": 1.4813333333333334e-05,
      "loss": 0.0017,
      "step": 105560
    },
    {
      "epoch": 5.6304,
      "grad_norm": 0.2069883793592453,
      "learning_rate": 1.4810000000000002e-05,
      "loss": 0.0014,
      "step": 105570
    },
    {
      "epoch": 5.630933333333333,
      "grad_norm": 0.1867944449186325,
      "learning_rate": 1.4806666666666668e-05,
      "loss": 0.0019,
      "step": 105580
    },
    {
      "epoch": 5.631466666666666,
      "grad_norm": 0.23502951860427856,
      "learning_rate": 1.4803333333333333e-05,
      "loss": 0.0026,
      "step": 105590
    },
    {
      "epoch": 5.632,
      "grad_norm": 0.23737268149852753,
      "learning_rate": 1.48e-05,
      "loss": 0.0014,
      "step": 105600
    },
    {
      "epoch": 5.632533333333333,
      "grad_norm": 0.0931951254606247,
      "learning_rate": 1.4796666666666667e-05,
      "loss": 0.0015,
      "step": 105610
    },
    {
      "epoch": 5.633066666666666,
      "grad_norm": 0.07667873054742813,
      "learning_rate": 1.4793333333333335e-05,
      "loss": 0.0019,
      "step": 105620
    },
    {
      "epoch": 5.6336,
      "grad_norm": 0.5312696695327759,
      "learning_rate": 1.479e-05,
      "loss": 0.0012,
      "step": 105630
    },
    {
      "epoch": 5.634133333333334,
      "grad_norm": 0.12410643696784973,
      "learning_rate": 1.4786666666666669e-05,
      "loss": 0.0019,
      "step": 105640
    },
    {
      "epoch": 5.634666666666667,
      "grad_norm": 0.06703951209783554,
      "learning_rate": 1.4783333333333335e-05,
      "loss": 0.0018,
      "step": 105650
    },
    {
      "epoch": 5.6352,
      "grad_norm": 0.3621881902217865,
      "learning_rate": 1.4779999999999999e-05,
      "loss": 0.002,
      "step": 105660
    },
    {
      "epoch": 5.6357333333333335,
      "grad_norm": 0.14548920094966888,
      "learning_rate": 1.4776666666666667e-05,
      "loss": 0.0019,
      "step": 105670
    },
    {
      "epoch": 5.636266666666667,
      "grad_norm": 0.10717933624982834,
      "learning_rate": 1.4773333333333333e-05,
      "loss": 0.0014,
      "step": 105680
    },
    {
      "epoch": 5.6368,
      "grad_norm": 0.20441317558288574,
      "learning_rate": 1.4770000000000001e-05,
      "loss": 0.0026,
      "step": 105690
    },
    {
      "epoch": 5.637333333333333,
      "grad_norm": 0.5769063830375671,
      "learning_rate": 1.4766666666666667e-05,
      "loss": 0.0023,
      "step": 105700
    },
    {
      "epoch": 5.637866666666667,
      "grad_norm": 0.2975727319717407,
      "learning_rate": 1.4763333333333335e-05,
      "loss": 0.0021,
      "step": 105710
    },
    {
      "epoch": 5.6384,
      "grad_norm": 0.2193121612071991,
      "learning_rate": 1.4760000000000001e-05,
      "loss": 0.0016,
      "step": 105720
    },
    {
      "epoch": 5.638933333333333,
      "grad_norm": 0.38098272681236267,
      "learning_rate": 1.4756666666666669e-05,
      "loss": 0.0021,
      "step": 105730
    },
    {
      "epoch": 5.639466666666666,
      "grad_norm": 0.04300945997238159,
      "learning_rate": 1.4753333333333333e-05,
      "loss": 0.0021,
      "step": 105740
    },
    {
      "epoch": 5.64,
      "grad_norm": 0.1361941695213318,
      "learning_rate": 1.475e-05,
      "loss": 0.0017,
      "step": 105750
    },
    {
      "epoch": 5.640533333333333,
      "grad_norm": 0.2962237298488617,
      "learning_rate": 1.4746666666666667e-05,
      "loss": 0.0013,
      "step": 105760
    },
    {
      "epoch": 5.641066666666667,
      "grad_norm": 0.29320472478866577,
      "learning_rate": 1.4743333333333333e-05,
      "loss": 0.003,
      "step": 105770
    },
    {
      "epoch": 5.6416,
      "grad_norm": 0.11928965896368027,
      "learning_rate": 1.4740000000000001e-05,
      "loss": 0.0017,
      "step": 105780
    },
    {
      "epoch": 5.642133333333334,
      "grad_norm": 0.21797513961791992,
      "learning_rate": 1.4736666666666667e-05,
      "loss": 0.0017,
      "step": 105790
    },
    {
      "epoch": 5.642666666666667,
      "grad_norm": 0.21022866666316986,
      "learning_rate": 1.4733333333333335e-05,
      "loss": 0.0019,
      "step": 105800
    },
    {
      "epoch": 5.6432,
      "grad_norm": 0.2531101107597351,
      "learning_rate": 1.473e-05,
      "loss": 0.0013,
      "step": 105810
    },
    {
      "epoch": 5.6437333333333335,
      "grad_norm": 0.07342420518398285,
      "learning_rate": 1.4726666666666666e-05,
      "loss": 0.0014,
      "step": 105820
    },
    {
      "epoch": 5.644266666666667,
      "grad_norm": 0.08741097152233124,
      "learning_rate": 1.4723333333333334e-05,
      "loss": 0.0019,
      "step": 105830
    },
    {
      "epoch": 5.6448,
      "grad_norm": 0.48621684312820435,
      "learning_rate": 1.472e-05,
      "loss": 0.0016,
      "step": 105840
    },
    {
      "epoch": 5.645333333333333,
      "grad_norm": 0.07260268926620483,
      "learning_rate": 1.4716666666666668e-05,
      "loss": 0.0021,
      "step": 105850
    },
    {
      "epoch": 5.645866666666667,
      "grad_norm": 0.4500138759613037,
      "learning_rate": 1.4713333333333335e-05,
      "loss": 0.0021,
      "step": 105860
    },
    {
      "epoch": 5.6464,
      "grad_norm": 0.19563372433185577,
      "learning_rate": 1.4710000000000001e-05,
      "loss": 0.0015,
      "step": 105870
    },
    {
      "epoch": 5.646933333333333,
      "grad_norm": 0.38794517517089844,
      "learning_rate": 1.470666666666667e-05,
      "loss": 0.0015,
      "step": 105880
    },
    {
      "epoch": 5.647466666666666,
      "grad_norm": 0.6564239263534546,
      "learning_rate": 1.4703333333333332e-05,
      "loss": 0.002,
      "step": 105890
    },
    {
      "epoch": 5.648,
      "grad_norm": 0.13866259157657623,
      "learning_rate": 1.47e-05,
      "loss": 0.0017,
      "step": 105900
    },
    {
      "epoch": 5.648533333333333,
      "grad_norm": 0.09536203742027283,
      "learning_rate": 1.4696666666666668e-05,
      "loss": 0.0013,
      "step": 105910
    },
    {
      "epoch": 5.649066666666666,
      "grad_norm": 0.3242329955101013,
      "learning_rate": 1.4693333333333334e-05,
      "loss": 0.0024,
      "step": 105920
    },
    {
      "epoch": 5.6495999999999995,
      "grad_norm": 0.028337450698018074,
      "learning_rate": 1.4690000000000002e-05,
      "loss": 0.0013,
      "step": 105930
    },
    {
      "epoch": 5.650133333333334,
      "grad_norm": 0.12042143940925598,
      "learning_rate": 1.4686666666666668e-05,
      "loss": 0.0019,
      "step": 105940
    },
    {
      "epoch": 5.650666666666667,
      "grad_norm": 0.23830001056194305,
      "learning_rate": 1.4683333333333336e-05,
      "loss": 0.002,
      "step": 105950
    },
    {
      "epoch": 5.6512,
      "grad_norm": 0.3145427107810974,
      "learning_rate": 1.4680000000000002e-05,
      "loss": 0.0025,
      "step": 105960
    },
    {
      "epoch": 5.6517333333333335,
      "grad_norm": 0.2411329299211502,
      "learning_rate": 1.4676666666666666e-05,
      "loss": 0.0018,
      "step": 105970
    },
    {
      "epoch": 5.652266666666667,
      "grad_norm": 0.20465430617332458,
      "learning_rate": 1.4673333333333334e-05,
      "loss": 0.0019,
      "step": 105980
    },
    {
      "epoch": 5.6528,
      "grad_norm": 0.14958633482456207,
      "learning_rate": 1.467e-05,
      "loss": 0.0016,
      "step": 105990
    },
    {
      "epoch": 5.653333333333333,
      "grad_norm": 0.39287811517715454,
      "learning_rate": 1.4666666666666668e-05,
      "loss": 0.0019,
      "step": 106000
    },
    {
      "epoch": 5.653866666666667,
      "grad_norm": 0.45220792293548584,
      "learning_rate": 1.4663333333333334e-05,
      "loss": 0.0019,
      "step": 106010
    },
    {
      "epoch": 5.6544,
      "grad_norm": 0.473957359790802,
      "learning_rate": 1.4660000000000002e-05,
      "loss": 0.0018,
      "step": 106020
    },
    {
      "epoch": 5.654933333333333,
      "grad_norm": 0.1266961693763733,
      "learning_rate": 1.4656666666666668e-05,
      "loss": 0.0018,
      "step": 106030
    },
    {
      "epoch": 5.655466666666666,
      "grad_norm": 0.3321707844734192,
      "learning_rate": 1.4653333333333333e-05,
      "loss": 0.0019,
      "step": 106040
    },
    {
      "epoch": 5.656,
      "grad_norm": 0.27407100796699524,
      "learning_rate": 1.465e-05,
      "loss": 0.0016,
      "step": 106050
    },
    {
      "epoch": 5.656533333333333,
      "grad_norm": 0.361280232667923,
      "learning_rate": 1.4646666666666666e-05,
      "loss": 0.0019,
      "step": 106060
    },
    {
      "epoch": 5.657066666666667,
      "grad_norm": 0.11918856203556061,
      "learning_rate": 1.4643333333333334e-05,
      "loss": 0.002,
      "step": 106070
    },
    {
      "epoch": 5.6576,
      "grad_norm": 0.08858179301023483,
      "learning_rate": 1.464e-05,
      "loss": 0.0012,
      "step": 106080
    },
    {
      "epoch": 5.658133333333334,
      "grad_norm": 0.4635438919067383,
      "learning_rate": 1.4636666666666668e-05,
      "loss": 0.0021,
      "step": 106090
    },
    {
      "epoch": 5.658666666666667,
      "grad_norm": 0.5520601272583008,
      "learning_rate": 1.4633333333333334e-05,
      "loss": 0.0021,
      "step": 106100
    },
    {
      "epoch": 5.6592,
      "grad_norm": 0.3108944892883301,
      "learning_rate": 1.4630000000000002e-05,
      "loss": 0.0029,
      "step": 106110
    },
    {
      "epoch": 5.6597333333333335,
      "grad_norm": 0.17152464389801025,
      "learning_rate": 1.4626666666666667e-05,
      "loss": 0.0015,
      "step": 106120
    },
    {
      "epoch": 5.660266666666667,
      "grad_norm": 0.44970500469207764,
      "learning_rate": 1.4623333333333333e-05,
      "loss": 0.0021,
      "step": 106130
    },
    {
      "epoch": 5.6608,
      "grad_norm": 0.055959105491638184,
      "learning_rate": 1.462e-05,
      "loss": 0.0015,
      "step": 106140
    },
    {
      "epoch": 5.661333333333333,
      "grad_norm": 0.15777358412742615,
      "learning_rate": 1.4616666666666667e-05,
      "loss": 0.0013,
      "step": 106150
    },
    {
      "epoch": 5.661866666666667,
      "grad_norm": 0.33920028805732727,
      "learning_rate": 1.4613333333333335e-05,
      "loss": 0.0023,
      "step": 106160
    },
    {
      "epoch": 5.6624,
      "grad_norm": 0.12874747812747955,
      "learning_rate": 1.461e-05,
      "loss": 0.0015,
      "step": 106170
    },
    {
      "epoch": 5.662933333333333,
      "grad_norm": 0.6253153681755066,
      "learning_rate": 1.4606666666666669e-05,
      "loss": 0.0017,
      "step": 106180
    },
    {
      "epoch": 5.663466666666666,
      "grad_norm": 0.0451740063726902,
      "learning_rate": 1.4603333333333333e-05,
      "loss": 0.0016,
      "step": 106190
    },
    {
      "epoch": 5.664,
      "grad_norm": 0.3998948633670807,
      "learning_rate": 1.4599999999999999e-05,
      "loss": 0.0022,
      "step": 106200
    },
    {
      "epoch": 5.664533333333333,
      "grad_norm": 0.5174877047538757,
      "learning_rate": 1.4596666666666667e-05,
      "loss": 0.0022,
      "step": 106210
    },
    {
      "epoch": 5.665066666666666,
      "grad_norm": 0.19088496267795563,
      "learning_rate": 1.4593333333333333e-05,
      "loss": 0.0013,
      "step": 106220
    },
    {
      "epoch": 5.6655999999999995,
      "grad_norm": 0.08483053743839264,
      "learning_rate": 1.4590000000000001e-05,
      "loss": 0.0016,
      "step": 106230
    },
    {
      "epoch": 5.666133333333334,
      "grad_norm": 0.39769840240478516,
      "learning_rate": 1.4586666666666669e-05,
      "loss": 0.0011,
      "step": 106240
    },
    {
      "epoch": 5.666666666666667,
      "grad_norm": 0.11992516368627548,
      "learning_rate": 1.4583333333333335e-05,
      "loss": 0.0021,
      "step": 106250
    },
    {
      "epoch": 5.6672,
      "grad_norm": 0.35534194111824036,
      "learning_rate": 1.4580000000000003e-05,
      "loss": 0.0017,
      "step": 106260
    },
    {
      "epoch": 5.6677333333333335,
      "grad_norm": 0.12798404693603516,
      "learning_rate": 1.4576666666666665e-05,
      "loss": 0.0023,
      "step": 106270
    },
    {
      "epoch": 5.668266666666667,
      "grad_norm": 0.24571599066257477,
      "learning_rate": 1.4573333333333333e-05,
      "loss": 0.0027,
      "step": 106280
    },
    {
      "epoch": 5.6688,
      "grad_norm": 0.3261471390724182,
      "learning_rate": 1.4570000000000001e-05,
      "loss": 0.0016,
      "step": 106290
    },
    {
      "epoch": 5.669333333333333,
      "grad_norm": 0.2394382357597351,
      "learning_rate": 1.4566666666666667e-05,
      "loss": 0.0023,
      "step": 106300
    },
    {
      "epoch": 5.669866666666667,
      "grad_norm": 0.1563093066215515,
      "learning_rate": 1.4563333333333335e-05,
      "loss": 0.0018,
      "step": 106310
    },
    {
      "epoch": 5.6704,
      "grad_norm": 0.25648656487464905,
      "learning_rate": 1.4560000000000001e-05,
      "loss": 0.0023,
      "step": 106320
    },
    {
      "epoch": 5.670933333333333,
      "grad_norm": 0.21293751895427704,
      "learning_rate": 1.4556666666666669e-05,
      "loss": 0.0025,
      "step": 106330
    },
    {
      "epoch": 5.671466666666666,
      "grad_norm": 0.15895316004753113,
      "learning_rate": 1.4553333333333333e-05,
      "loss": 0.0014,
      "step": 106340
    },
    {
      "epoch": 5.672,
      "grad_norm": 0.33182623982429504,
      "learning_rate": 1.455e-05,
      "loss": 0.0015,
      "step": 106350
    },
    {
      "epoch": 5.672533333333333,
      "grad_norm": 0.15474833548069,
      "learning_rate": 1.4546666666666667e-05,
      "loss": 0.0023,
      "step": 106360
    },
    {
      "epoch": 5.673066666666667,
      "grad_norm": 0.07275447994470596,
      "learning_rate": 1.4543333333333334e-05,
      "loss": 0.0013,
      "step": 106370
    },
    {
      "epoch": 5.6736,
      "grad_norm": 0.10016689449548721,
      "learning_rate": 1.4540000000000001e-05,
      "loss": 0.0013,
      "step": 106380
    },
    {
      "epoch": 5.674133333333334,
      "grad_norm": 0.18548975884914398,
      "learning_rate": 1.4536666666666668e-05,
      "loss": 0.002,
      "step": 106390
    },
    {
      "epoch": 5.674666666666667,
      "grad_norm": 0.5711215734481812,
      "learning_rate": 1.4533333333333335e-05,
      "loss": 0.0018,
      "step": 106400
    },
    {
      "epoch": 5.6752,
      "grad_norm": 0.13267934322357178,
      "learning_rate": 1.4530000000000001e-05,
      "loss": 0.0022,
      "step": 106410
    },
    {
      "epoch": 5.6757333333333335,
      "grad_norm": 0.1263572871685028,
      "learning_rate": 1.4526666666666666e-05,
      "loss": 0.0016,
      "step": 106420
    },
    {
      "epoch": 5.676266666666667,
      "grad_norm": 0.39491066336631775,
      "learning_rate": 1.4523333333333334e-05,
      "loss": 0.0026,
      "step": 106430
    },
    {
      "epoch": 5.6768,
      "grad_norm": 0.590218722820282,
      "learning_rate": 1.452e-05,
      "loss": 0.0017,
      "step": 106440
    },
    {
      "epoch": 5.677333333333333,
      "grad_norm": 0.11927381902933121,
      "learning_rate": 1.4516666666666668e-05,
      "loss": 0.0024,
      "step": 106450
    },
    {
      "epoch": 5.677866666666667,
      "grad_norm": 0.3359225392341614,
      "learning_rate": 1.4513333333333334e-05,
      "loss": 0.0016,
      "step": 106460
    },
    {
      "epoch": 5.6784,
      "grad_norm": 0.15557770431041718,
      "learning_rate": 1.4510000000000002e-05,
      "loss": 0.0015,
      "step": 106470
    },
    {
      "epoch": 5.678933333333333,
      "grad_norm": 0.6104657053947449,
      "learning_rate": 1.4506666666666668e-05,
      "loss": 0.0021,
      "step": 106480
    },
    {
      "epoch": 5.679466666666666,
      "grad_norm": 0.08575623482465744,
      "learning_rate": 1.4503333333333332e-05,
      "loss": 0.0023,
      "step": 106490
    },
    {
      "epoch": 5.68,
      "grad_norm": 0.1197023093700409,
      "learning_rate": 1.45e-05,
      "loss": 0.0022,
      "step": 106500
    },
    {
      "epoch": 5.680533333333333,
      "grad_norm": 0.24749577045440674,
      "learning_rate": 1.4496666666666666e-05,
      "loss": 0.0014,
      "step": 106510
    },
    {
      "epoch": 5.681066666666666,
      "grad_norm": 0.015068743377923965,
      "learning_rate": 1.4493333333333334e-05,
      "loss": 0.0022,
      "step": 106520
    },
    {
      "epoch": 5.6815999999999995,
      "grad_norm": 0.5429767966270447,
      "learning_rate": 1.449e-05,
      "loss": 0.0024,
      "step": 106530
    },
    {
      "epoch": 5.682133333333334,
      "grad_norm": 0.32030734419822693,
      "learning_rate": 1.4486666666666668e-05,
      "loss": 0.0018,
      "step": 106540
    },
    {
      "epoch": 5.682666666666667,
      "grad_norm": 0.05547545105218887,
      "learning_rate": 1.4483333333333334e-05,
      "loss": 0.0014,
      "step": 106550
    },
    {
      "epoch": 5.6832,
      "grad_norm": 0.2644974887371063,
      "learning_rate": 1.4480000000000002e-05,
      "loss": 0.003,
      "step": 106560
    },
    {
      "epoch": 5.6837333333333335,
      "grad_norm": 0.4860353469848633,
      "learning_rate": 1.4476666666666666e-05,
      "loss": 0.0018,
      "step": 106570
    },
    {
      "epoch": 5.684266666666667,
      "grad_norm": 0.14747770130634308,
      "learning_rate": 1.4473333333333333e-05,
      "loss": 0.0014,
      "step": 106580
    },
    {
      "epoch": 5.6848,
      "grad_norm": 0.06420988589525223,
      "learning_rate": 1.447e-05,
      "loss": 0.0019,
      "step": 106590
    },
    {
      "epoch": 5.685333333333333,
      "grad_norm": 0.0918503850698471,
      "learning_rate": 1.4466666666666667e-05,
      "loss": 0.0018,
      "step": 106600
    },
    {
      "epoch": 5.685866666666667,
      "grad_norm": 0.18527790904045105,
      "learning_rate": 1.4463333333333334e-05,
      "loss": 0.0026,
      "step": 106610
    },
    {
      "epoch": 5.6864,
      "grad_norm": 0.20701710879802704,
      "learning_rate": 1.4460000000000002e-05,
      "loss": 0.0018,
      "step": 106620
    },
    {
      "epoch": 5.686933333333333,
      "grad_norm": 0.38285696506500244,
      "learning_rate": 1.4456666666666668e-05,
      "loss": 0.002,
      "step": 106630
    },
    {
      "epoch": 5.6874666666666664,
      "grad_norm": 0.07354898750782013,
      "learning_rate": 1.4453333333333336e-05,
      "loss": 0.0018,
      "step": 106640
    },
    {
      "epoch": 5.688,
      "grad_norm": 0.07733963429927826,
      "learning_rate": 1.4449999999999999e-05,
      "loss": 0.0024,
      "step": 106650
    },
    {
      "epoch": 5.688533333333333,
      "grad_norm": 0.25164249539375305,
      "learning_rate": 1.4446666666666667e-05,
      "loss": 0.0013,
      "step": 106660
    },
    {
      "epoch": 5.689066666666667,
      "grad_norm": 0.2942892909049988,
      "learning_rate": 1.4443333333333335e-05,
      "loss": 0.0014,
      "step": 106670
    },
    {
      "epoch": 5.6896,
      "grad_norm": 0.1836795061826706,
      "learning_rate": 1.444e-05,
      "loss": 0.0017,
      "step": 106680
    },
    {
      "epoch": 5.690133333333334,
      "grad_norm": 0.2386588156223297,
      "learning_rate": 1.4436666666666668e-05,
      "loss": 0.002,
      "step": 106690
    },
    {
      "epoch": 5.690666666666667,
      "grad_norm": 0.15018287301063538,
      "learning_rate": 1.4433333333333335e-05,
      "loss": 0.0019,
      "step": 106700
    },
    {
      "epoch": 5.6912,
      "grad_norm": 0.18206894397735596,
      "learning_rate": 1.4430000000000002e-05,
      "loss": 0.0015,
      "step": 106710
    },
    {
      "epoch": 5.6917333333333335,
      "grad_norm": 0.22074027359485626,
      "learning_rate": 1.4426666666666667e-05,
      "loss": 0.0027,
      "step": 106720
    },
    {
      "epoch": 5.692266666666667,
      "grad_norm": 0.1498902440071106,
      "learning_rate": 1.4423333333333333e-05,
      "loss": 0.0018,
      "step": 106730
    },
    {
      "epoch": 5.6928,
      "grad_norm": 0.3673408627510071,
      "learning_rate": 1.4420000000000001e-05,
      "loss": 0.0022,
      "step": 106740
    },
    {
      "epoch": 5.693333333333333,
      "grad_norm": 0.2711266577243805,
      "learning_rate": 1.4416666666666667e-05,
      "loss": 0.0017,
      "step": 106750
    },
    {
      "epoch": 5.693866666666667,
      "grad_norm": 0.49826928973197937,
      "learning_rate": 1.4413333333333335e-05,
      "loss": 0.0025,
      "step": 106760
    },
    {
      "epoch": 5.6944,
      "grad_norm": 0.22118254005908966,
      "learning_rate": 1.4410000000000001e-05,
      "loss": 0.0021,
      "step": 106770
    },
    {
      "epoch": 5.694933333333333,
      "grad_norm": 0.18842637538909912,
      "learning_rate": 1.4406666666666669e-05,
      "loss": 0.0015,
      "step": 106780
    },
    {
      "epoch": 5.6954666666666665,
      "grad_norm": 0.13809239864349365,
      "learning_rate": 1.4403333333333335e-05,
      "loss": 0.0016,
      "step": 106790
    },
    {
      "epoch": 5.696,
      "grad_norm": 0.2237234264612198,
      "learning_rate": 1.44e-05,
      "loss": 0.0017,
      "step": 106800
    },
    {
      "epoch": 5.696533333333333,
      "grad_norm": 0.07535850256681442,
      "learning_rate": 1.4396666666666667e-05,
      "loss": 0.0019,
      "step": 106810
    },
    {
      "epoch": 5.697066666666666,
      "grad_norm": 0.32782718539237976,
      "learning_rate": 1.4393333333333333e-05,
      "loss": 0.0017,
      "step": 106820
    },
    {
      "epoch": 5.6975999999999996,
      "grad_norm": 0.10977644473314285,
      "learning_rate": 1.4390000000000001e-05,
      "loss": 0.0016,
      "step": 106830
    },
    {
      "epoch": 5.698133333333334,
      "grad_norm": 0.2466781586408615,
      "learning_rate": 1.4386666666666667e-05,
      "loss": 0.002,
      "step": 106840
    },
    {
      "epoch": 5.698666666666667,
      "grad_norm": 0.2616124451160431,
      "learning_rate": 1.4383333333333335e-05,
      "loss": 0.0033,
      "step": 106850
    },
    {
      "epoch": 5.6992,
      "grad_norm": 0.1259208619594574,
      "learning_rate": 1.4380000000000001e-05,
      "loss": 0.0015,
      "step": 106860
    },
    {
      "epoch": 5.6997333333333335,
      "grad_norm": 0.2734353840351105,
      "learning_rate": 1.4376666666666666e-05,
      "loss": 0.0018,
      "step": 106870
    },
    {
      "epoch": 5.700266666666667,
      "grad_norm": 0.25000959634780884,
      "learning_rate": 1.4373333333333334e-05,
      "loss": 0.0021,
      "step": 106880
    },
    {
      "epoch": 5.7008,
      "grad_norm": 0.16422848403453827,
      "learning_rate": 1.437e-05,
      "loss": 0.0014,
      "step": 106890
    },
    {
      "epoch": 5.701333333333333,
      "grad_norm": 0.05937360227108002,
      "learning_rate": 1.4366666666666667e-05,
      "loss": 0.0018,
      "step": 106900
    },
    {
      "epoch": 5.701866666666667,
      "grad_norm": 0.1796298325061798,
      "learning_rate": 1.4363333333333334e-05,
      "loss": 0.0014,
      "step": 106910
    },
    {
      "epoch": 5.7024,
      "grad_norm": 0.09186261892318726,
      "learning_rate": 1.4360000000000001e-05,
      "loss": 0.0019,
      "step": 106920
    },
    {
      "epoch": 5.702933333333333,
      "grad_norm": 0.09351552277803421,
      "learning_rate": 1.4356666666666668e-05,
      "loss": 0.0026,
      "step": 106930
    },
    {
      "epoch": 5.7034666666666665,
      "grad_norm": 0.1753433346748352,
      "learning_rate": 1.4353333333333335e-05,
      "loss": 0.0016,
      "step": 106940
    },
    {
      "epoch": 5.704,
      "grad_norm": 0.36299172043800354,
      "learning_rate": 1.435e-05,
      "loss": 0.0014,
      "step": 106950
    },
    {
      "epoch": 5.704533333333333,
      "grad_norm": 0.21225740015506744,
      "learning_rate": 1.4346666666666666e-05,
      "loss": 0.0019,
      "step": 106960
    },
    {
      "epoch": 5.705066666666666,
      "grad_norm": 0.2414369285106659,
      "learning_rate": 1.4343333333333334e-05,
      "loss": 0.0018,
      "step": 106970
    },
    {
      "epoch": 5.7056000000000004,
      "grad_norm": 0.2245543748140335,
      "learning_rate": 1.434e-05,
      "loss": 0.003,
      "step": 106980
    },
    {
      "epoch": 5.706133333333334,
      "grad_norm": 0.18794319033622742,
      "learning_rate": 1.4336666666666668e-05,
      "loss": 0.0013,
      "step": 106990
    },
    {
      "epoch": 5.706666666666667,
      "grad_norm": 0.10247649252414703,
      "learning_rate": 1.4333333333333334e-05,
      "loss": 0.0017,
      "step": 107000
    },
    {
      "epoch": 5.7072,
      "grad_norm": 0.2870473265647888,
      "learning_rate": 1.4330000000000002e-05,
      "loss": 0.0025,
      "step": 107010
    },
    {
      "epoch": 5.7077333333333335,
      "grad_norm": 0.31648769974708557,
      "learning_rate": 1.4326666666666666e-05,
      "loss": 0.0021,
      "step": 107020
    },
    {
      "epoch": 5.708266666666667,
      "grad_norm": 0.2743801474571228,
      "learning_rate": 1.4323333333333332e-05,
      "loss": 0.0013,
      "step": 107030
    },
    {
      "epoch": 5.7088,
      "grad_norm": 0.10724339634180069,
      "learning_rate": 1.432e-05,
      "loss": 0.0015,
      "step": 107040
    },
    {
      "epoch": 5.709333333333333,
      "grad_norm": 0.1766604483127594,
      "learning_rate": 1.4316666666666668e-05,
      "loss": 0.0013,
      "step": 107050
    },
    {
      "epoch": 5.709866666666667,
      "grad_norm": 0.4477766454219818,
      "learning_rate": 1.4313333333333334e-05,
      "loss": 0.002,
      "step": 107060
    },
    {
      "epoch": 5.7104,
      "grad_norm": 0.24503250420093536,
      "learning_rate": 1.4310000000000002e-05,
      "loss": 0.0013,
      "step": 107070
    },
    {
      "epoch": 5.710933333333333,
      "grad_norm": 0.07812640070915222,
      "learning_rate": 1.4306666666666668e-05,
      "loss": 0.0019,
      "step": 107080
    },
    {
      "epoch": 5.7114666666666665,
      "grad_norm": 0.38779357075691223,
      "learning_rate": 1.4303333333333336e-05,
      "loss": 0.0023,
      "step": 107090
    },
    {
      "epoch": 5.712,
      "grad_norm": 0.045015934854745865,
      "learning_rate": 1.43e-05,
      "loss": 0.002,
      "step": 107100
    },
    {
      "epoch": 5.712533333333333,
      "grad_norm": 0.09324408322572708,
      "learning_rate": 1.4296666666666666e-05,
      "loss": 0.0012,
      "step": 107110
    },
    {
      "epoch": 5.713066666666666,
      "grad_norm": 0.2710127532482147,
      "learning_rate": 1.4293333333333334e-05,
      "loss": 0.0016,
      "step": 107120
    },
    {
      "epoch": 5.7136,
      "grad_norm": 0.38738927245140076,
      "learning_rate": 1.429e-05,
      "loss": 0.0023,
      "step": 107130
    },
    {
      "epoch": 5.714133333333333,
      "grad_norm": 0.03564028814435005,
      "learning_rate": 1.4286666666666668e-05,
      "loss": 0.0022,
      "step": 107140
    },
    {
      "epoch": 5.714666666666667,
      "grad_norm": 0.19073821604251862,
      "learning_rate": 1.4283333333333334e-05,
      "loss": 0.0016,
      "step": 107150
    },
    {
      "epoch": 5.7152,
      "grad_norm": 0.23813892900943756,
      "learning_rate": 1.4280000000000002e-05,
      "loss": 0.0021,
      "step": 107160
    },
    {
      "epoch": 5.7157333333333336,
      "grad_norm": 0.20826618373394012,
      "learning_rate": 1.4276666666666667e-05,
      "loss": 0.0021,
      "step": 107170
    },
    {
      "epoch": 5.716266666666667,
      "grad_norm": 0.3083328306674957,
      "learning_rate": 1.4273333333333333e-05,
      "loss": 0.0021,
      "step": 107180
    },
    {
      "epoch": 5.7168,
      "grad_norm": 0.5318537354469299,
      "learning_rate": 1.427e-05,
      "loss": 0.002,
      "step": 107190
    },
    {
      "epoch": 5.717333333333333,
      "grad_norm": 0.3691353499889374,
      "learning_rate": 1.4266666666666667e-05,
      "loss": 0.0019,
      "step": 107200
    },
    {
      "epoch": 5.717866666666667,
      "grad_norm": 0.10979859530925751,
      "learning_rate": 1.4263333333333335e-05,
      "loss": 0.0014,
      "step": 107210
    },
    {
      "epoch": 5.7184,
      "grad_norm": 0.09987029433250427,
      "learning_rate": 1.426e-05,
      "loss": 0.0015,
      "step": 107220
    },
    {
      "epoch": 5.718933333333333,
      "grad_norm": 0.25136837363243103,
      "learning_rate": 1.4256666666666669e-05,
      "loss": 0.0019,
      "step": 107230
    },
    {
      "epoch": 5.7194666666666665,
      "grad_norm": 0.40929773449897766,
      "learning_rate": 1.4253333333333335e-05,
      "loss": 0.0022,
      "step": 107240
    },
    {
      "epoch": 5.72,
      "grad_norm": 0.18247747421264648,
      "learning_rate": 1.4249999999999999e-05,
      "loss": 0.0021,
      "step": 107250
    },
    {
      "epoch": 5.720533333333333,
      "grad_norm": 0.0760442316532135,
      "learning_rate": 1.4246666666666667e-05,
      "loss": 0.0021,
      "step": 107260
    },
    {
      "epoch": 5.721066666666666,
      "grad_norm": 0.3226505517959595,
      "learning_rate": 1.4243333333333333e-05,
      "loss": 0.0021,
      "step": 107270
    },
    {
      "epoch": 5.7216000000000005,
      "grad_norm": 0.45941099524497986,
      "learning_rate": 1.4240000000000001e-05,
      "loss": 0.0015,
      "step": 107280
    },
    {
      "epoch": 5.722133333333334,
      "grad_norm": 0.3578025698661804,
      "learning_rate": 1.4236666666666667e-05,
      "loss": 0.0023,
      "step": 107290
    },
    {
      "epoch": 5.722666666666667,
      "grad_norm": 0.2057526409626007,
      "learning_rate": 1.4233333333333335e-05,
      "loss": 0.0019,
      "step": 107300
    },
    {
      "epoch": 5.7232,
      "grad_norm": 0.11181597411632538,
      "learning_rate": 1.4230000000000001e-05,
      "loss": 0.0013,
      "step": 107310
    },
    {
      "epoch": 5.723733333333334,
      "grad_norm": 0.1794048696756363,
      "learning_rate": 1.4226666666666669e-05,
      "loss": 0.0019,
      "step": 107320
    },
    {
      "epoch": 5.724266666666667,
      "grad_norm": 0.07381027191877365,
      "learning_rate": 1.4223333333333333e-05,
      "loss": 0.0012,
      "step": 107330
    },
    {
      "epoch": 5.7248,
      "grad_norm": 0.3527795672416687,
      "learning_rate": 1.422e-05,
      "loss": 0.0025,
      "step": 107340
    },
    {
      "epoch": 5.725333333333333,
      "grad_norm": 0.11976032704114914,
      "learning_rate": 1.4216666666666667e-05,
      "loss": 0.0019,
      "step": 107350
    },
    {
      "epoch": 5.725866666666667,
      "grad_norm": 0.04725465551018715,
      "learning_rate": 1.4213333333333333e-05,
      "loss": 0.0017,
      "step": 107360
    },
    {
      "epoch": 5.7264,
      "grad_norm": 0.29385390877723694,
      "learning_rate": 1.4210000000000001e-05,
      "loss": 0.0016,
      "step": 107370
    },
    {
      "epoch": 5.726933333333333,
      "grad_norm": 0.36727985739707947,
      "learning_rate": 1.4206666666666667e-05,
      "loss": 0.0017,
      "step": 107380
    },
    {
      "epoch": 5.7274666666666665,
      "grad_norm": 0.5228645205497742,
      "learning_rate": 1.4203333333333335e-05,
      "loss": 0.0013,
      "step": 107390
    },
    {
      "epoch": 5.728,
      "grad_norm": 0.18573759496212006,
      "learning_rate": 1.42e-05,
      "loss": 0.0015,
      "step": 107400
    },
    {
      "epoch": 5.728533333333333,
      "grad_norm": 0.26813751459121704,
      "learning_rate": 1.4196666666666666e-05,
      "loss": 0.0021,
      "step": 107410
    },
    {
      "epoch": 5.729066666666666,
      "grad_norm": 0.04786773771047592,
      "learning_rate": 1.4193333333333334e-05,
      "loss": 0.0016,
      "step": 107420
    },
    {
      "epoch": 5.7296,
      "grad_norm": 0.27149564027786255,
      "learning_rate": 1.4190000000000001e-05,
      "loss": 0.0016,
      "step": 107430
    },
    {
      "epoch": 5.730133333333333,
      "grad_norm": 0.5075079798698425,
      "learning_rate": 1.4186666666666667e-05,
      "loss": 0.0015,
      "step": 107440
    },
    {
      "epoch": 5.730666666666667,
      "grad_norm": 0.395119845867157,
      "learning_rate": 1.4183333333333335e-05,
      "loss": 0.0018,
      "step": 107450
    },
    {
      "epoch": 5.7312,
      "grad_norm": 0.3930792510509491,
      "learning_rate": 1.4180000000000001e-05,
      "loss": 0.0019,
      "step": 107460
    },
    {
      "epoch": 5.731733333333334,
      "grad_norm": 0.1624062955379486,
      "learning_rate": 1.417666666666667e-05,
      "loss": 0.002,
      "step": 107470
    },
    {
      "epoch": 5.732266666666667,
      "grad_norm": 0.14783985912799835,
      "learning_rate": 1.4173333333333334e-05,
      "loss": 0.0028,
      "step": 107480
    },
    {
      "epoch": 5.7328,
      "grad_norm": 0.122437484562397,
      "learning_rate": 1.417e-05,
      "loss": 0.0016,
      "step": 107490
    },
    {
      "epoch": 5.733333333333333,
      "grad_norm": 0.04657984897494316,
      "learning_rate": 1.4166666666666668e-05,
      "loss": 0.0019,
      "step": 107500
    },
    {
      "epoch": 5.733866666666667,
      "grad_norm": 0.15990722179412842,
      "learning_rate": 1.4163333333333334e-05,
      "loss": 0.0014,
      "step": 107510
    },
    {
      "epoch": 5.7344,
      "grad_norm": 0.46829965710639954,
      "learning_rate": 1.4160000000000002e-05,
      "loss": 0.0019,
      "step": 107520
    },
    {
      "epoch": 5.734933333333333,
      "grad_norm": 0.11917773634195328,
      "learning_rate": 1.4156666666666668e-05,
      "loss": 0.0017,
      "step": 107530
    },
    {
      "epoch": 5.7354666666666665,
      "grad_norm": 0.5205274224281311,
      "learning_rate": 1.4153333333333336e-05,
      "loss": 0.0024,
      "step": 107540
    },
    {
      "epoch": 5.736,
      "grad_norm": 0.09950914978981018,
      "learning_rate": 1.415e-05,
      "loss": 0.002,
      "step": 107550
    },
    {
      "epoch": 5.736533333333333,
      "grad_norm": 0.41743943095207214,
      "learning_rate": 1.4146666666666666e-05,
      "loss": 0.0013,
      "step": 107560
    },
    {
      "epoch": 5.737066666666666,
      "grad_norm": 0.18151018023490906,
      "learning_rate": 1.4143333333333334e-05,
      "loss": 0.0014,
      "step": 107570
    },
    {
      "epoch": 5.7376000000000005,
      "grad_norm": 0.3824615776538849,
      "learning_rate": 1.414e-05,
      "loss": 0.0019,
      "step": 107580
    },
    {
      "epoch": 5.738133333333334,
      "grad_norm": 0.3306458592414856,
      "learning_rate": 1.4136666666666668e-05,
      "loss": 0.0015,
      "step": 107590
    },
    {
      "epoch": 5.738666666666667,
      "grad_norm": 0.09227165579795837,
      "learning_rate": 1.4133333333333334e-05,
      "loss": 0.0021,
      "step": 107600
    },
    {
      "epoch": 5.7392,
      "grad_norm": 0.16483987867832184,
      "learning_rate": 1.4130000000000002e-05,
      "loss": 0.0021,
      "step": 107610
    },
    {
      "epoch": 5.739733333333334,
      "grad_norm": 0.12546615302562714,
      "learning_rate": 1.4126666666666668e-05,
      "loss": 0.0012,
      "step": 107620
    },
    {
      "epoch": 5.740266666666667,
      "grad_norm": 0.07082002609968185,
      "learning_rate": 1.4123333333333333e-05,
      "loss": 0.0019,
      "step": 107630
    },
    {
      "epoch": 5.7408,
      "grad_norm": 0.09475553780794144,
      "learning_rate": 1.412e-05,
      "loss": 0.0014,
      "step": 107640
    },
    {
      "epoch": 5.741333333333333,
      "grad_norm": 0.49394285678863525,
      "learning_rate": 1.4116666666666666e-05,
      "loss": 0.002,
      "step": 107650
    },
    {
      "epoch": 5.741866666666667,
      "grad_norm": 0.2920070290565491,
      "learning_rate": 1.4113333333333334e-05,
      "loss": 0.0013,
      "step": 107660
    },
    {
      "epoch": 5.7424,
      "grad_norm": 0.5338016748428345,
      "learning_rate": 1.411e-05,
      "loss": 0.0021,
      "step": 107670
    },
    {
      "epoch": 5.742933333333333,
      "grad_norm": 0.27309396862983704,
      "learning_rate": 1.4106666666666668e-05,
      "loss": 0.0015,
      "step": 107680
    },
    {
      "epoch": 5.7434666666666665,
      "grad_norm": 0.06455826014280319,
      "learning_rate": 1.4103333333333334e-05,
      "loss": 0.0021,
      "step": 107690
    },
    {
      "epoch": 5.744,
      "grad_norm": 0.13514500856399536,
      "learning_rate": 1.4099999999999999e-05,
      "loss": 0.0019,
      "step": 107700
    },
    {
      "epoch": 5.744533333333333,
      "grad_norm": 0.29902184009552,
      "learning_rate": 1.4096666666666667e-05,
      "loss": 0.0012,
      "step": 107710
    },
    {
      "epoch": 5.745066666666666,
      "grad_norm": 0.43335622549057007,
      "learning_rate": 1.4093333333333333e-05,
      "loss": 0.0017,
      "step": 107720
    },
    {
      "epoch": 5.7456,
      "grad_norm": 0.11980486661195755,
      "learning_rate": 1.409e-05,
      "loss": 0.0019,
      "step": 107730
    },
    {
      "epoch": 5.746133333333333,
      "grad_norm": 0.19112378358840942,
      "learning_rate": 1.4086666666666667e-05,
      "loss": 0.0021,
      "step": 107740
    },
    {
      "epoch": 5.746666666666667,
      "grad_norm": 0.05643894150853157,
      "learning_rate": 1.4083333333333335e-05,
      "loss": 0.0019,
      "step": 107750
    },
    {
      "epoch": 5.7472,
      "grad_norm": 0.22205452620983124,
      "learning_rate": 1.408e-05,
      "loss": 0.0028,
      "step": 107760
    },
    {
      "epoch": 5.747733333333334,
      "grad_norm": 0.11873603612184525,
      "learning_rate": 1.4076666666666669e-05,
      "loss": 0.0015,
      "step": 107770
    },
    {
      "epoch": 5.748266666666667,
      "grad_norm": 0.45617809891700745,
      "learning_rate": 1.4073333333333333e-05,
      "loss": 0.0018,
      "step": 107780
    },
    {
      "epoch": 5.7488,
      "grad_norm": 0.26388272643089294,
      "learning_rate": 1.4069999999999999e-05,
      "loss": 0.0019,
      "step": 107790
    },
    {
      "epoch": 5.749333333333333,
      "grad_norm": 0.5136191844940186,
      "learning_rate": 1.4066666666666667e-05,
      "loss": 0.0021,
      "step": 107800
    },
    {
      "epoch": 5.749866666666667,
      "grad_norm": 0.2583148777484894,
      "learning_rate": 1.4063333333333333e-05,
      "loss": 0.0017,
      "step": 107810
    },
    {
      "epoch": 5.7504,
      "grad_norm": 0.300898939371109,
      "learning_rate": 1.4060000000000001e-05,
      "loss": 0.0018,
      "step": 107820
    },
    {
      "epoch": 5.750933333333333,
      "grad_norm": 0.4047713875770569,
      "learning_rate": 1.4056666666666669e-05,
      "loss": 0.0014,
      "step": 107830
    },
    {
      "epoch": 5.7514666666666665,
      "grad_norm": 0.44701069593429565,
      "learning_rate": 1.4053333333333335e-05,
      "loss": 0.0019,
      "step": 107840
    },
    {
      "epoch": 5.752,
      "grad_norm": 0.07006585597991943,
      "learning_rate": 1.4050000000000003e-05,
      "loss": 0.0015,
      "step": 107850
    },
    {
      "epoch": 5.752533333333333,
      "grad_norm": 0.12908749282360077,
      "learning_rate": 1.4046666666666667e-05,
      "loss": 0.0036,
      "step": 107860
    },
    {
      "epoch": 5.753066666666666,
      "grad_norm": 0.1149786040186882,
      "learning_rate": 1.4043333333333333e-05,
      "loss": 0.0014,
      "step": 107870
    },
    {
      "epoch": 5.7536000000000005,
      "grad_norm": 0.11904724687337875,
      "learning_rate": 1.4040000000000001e-05,
      "loss": 0.0021,
      "step": 107880
    },
    {
      "epoch": 5.754133333333334,
      "grad_norm": 0.20459215342998505,
      "learning_rate": 1.4036666666666667e-05,
      "loss": 0.0024,
      "step": 107890
    },
    {
      "epoch": 5.754666666666667,
      "grad_norm": 0.4753692150115967,
      "learning_rate": 1.4033333333333335e-05,
      "loss": 0.002,
      "step": 107900
    },
    {
      "epoch": 5.7552,
      "grad_norm": 0.22605258226394653,
      "learning_rate": 1.4030000000000001e-05,
      "loss": 0.0016,
      "step": 107910
    },
    {
      "epoch": 5.755733333333334,
      "grad_norm": 0.16626042127609253,
      "learning_rate": 1.4026666666666669e-05,
      "loss": 0.0015,
      "step": 107920
    },
    {
      "epoch": 5.756266666666667,
      "grad_norm": 0.40332484245300293,
      "learning_rate": 1.4023333333333333e-05,
      "loss": 0.0018,
      "step": 107930
    },
    {
      "epoch": 5.7568,
      "grad_norm": 0.24048210680484772,
      "learning_rate": 1.402e-05,
      "loss": 0.0019,
      "step": 107940
    },
    {
      "epoch": 5.757333333333333,
      "grad_norm": 0.06638773530721664,
      "learning_rate": 1.4016666666666667e-05,
      "loss": 0.0031,
      "step": 107950
    },
    {
      "epoch": 5.757866666666667,
      "grad_norm": 0.27197960019111633,
      "learning_rate": 1.4013333333333334e-05,
      "loss": 0.0022,
      "step": 107960
    },
    {
      "epoch": 5.7584,
      "grad_norm": 0.03831715136766434,
      "learning_rate": 1.4010000000000001e-05,
      "loss": 0.0015,
      "step": 107970
    },
    {
      "epoch": 5.758933333333333,
      "grad_norm": 0.3256470561027527,
      "learning_rate": 1.4006666666666668e-05,
      "loss": 0.0016,
      "step": 107980
    },
    {
      "epoch": 5.7594666666666665,
      "grad_norm": 0.046853888779878616,
      "learning_rate": 1.4003333333333335e-05,
      "loss": 0.0013,
      "step": 107990
    },
    {
      "epoch": 5.76,
      "grad_norm": 0.7626672983169556,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.0024,
      "step": 108000
    },
    {
      "epoch": 5.760533333333333,
      "grad_norm": 0.18688809871673584,
      "learning_rate": 1.3996666666666666e-05,
      "loss": 0.0021,
      "step": 108010
    },
    {
      "epoch": 5.761066666666666,
      "grad_norm": 0.06309278309345245,
      "learning_rate": 1.3993333333333334e-05,
      "loss": 0.0018,
      "step": 108020
    },
    {
      "epoch": 5.7616,
      "grad_norm": 0.300197571516037,
      "learning_rate": 1.399e-05,
      "loss": 0.0023,
      "step": 108030
    },
    {
      "epoch": 5.762133333333333,
      "grad_norm": 0.22906535863876343,
      "learning_rate": 1.3986666666666668e-05,
      "loss": 0.0025,
      "step": 108040
    },
    {
      "epoch": 5.762666666666667,
      "grad_norm": 0.45222949981689453,
      "learning_rate": 1.3983333333333334e-05,
      "loss": 0.0027,
      "step": 108050
    },
    {
      "epoch": 5.7632,
      "grad_norm": 0.2691980302333832,
      "learning_rate": 1.3980000000000002e-05,
      "loss": 0.0024,
      "step": 108060
    },
    {
      "epoch": 5.763733333333334,
      "grad_norm": 0.0669938251376152,
      "learning_rate": 1.3976666666666668e-05,
      "loss": 0.0024,
      "step": 108070
    },
    {
      "epoch": 5.764266666666667,
      "grad_norm": 0.45371299982070923,
      "learning_rate": 1.3973333333333332e-05,
      "loss": 0.0012,
      "step": 108080
    },
    {
      "epoch": 5.7648,
      "grad_norm": 0.033173996955156326,
      "learning_rate": 1.397e-05,
      "loss": 0.0021,
      "step": 108090
    },
    {
      "epoch": 5.765333333333333,
      "grad_norm": 0.3656204044818878,
      "learning_rate": 1.3966666666666666e-05,
      "loss": 0.0015,
      "step": 108100
    },
    {
      "epoch": 5.765866666666667,
      "grad_norm": 0.2827010750770569,
      "learning_rate": 1.3963333333333334e-05,
      "loss": 0.0021,
      "step": 108110
    },
    {
      "epoch": 5.7664,
      "grad_norm": 0.16002681851387024,
      "learning_rate": 1.396e-05,
      "loss": 0.0016,
      "step": 108120
    },
    {
      "epoch": 5.766933333333333,
      "grad_norm": 0.04228364676237106,
      "learning_rate": 1.3956666666666668e-05,
      "loss": 0.0017,
      "step": 108130
    },
    {
      "epoch": 5.7674666666666665,
      "grad_norm": 0.041826747357845306,
      "learning_rate": 1.3953333333333334e-05,
      "loss": 0.0014,
      "step": 108140
    },
    {
      "epoch": 5.768,
      "grad_norm": 0.4900096356868744,
      "learning_rate": 1.3950000000000002e-05,
      "loss": 0.0022,
      "step": 108150
    },
    {
      "epoch": 5.768533333333333,
      "grad_norm": 0.052009981125593185,
      "learning_rate": 1.3946666666666666e-05,
      "loss": 0.0014,
      "step": 108160
    },
    {
      "epoch": 5.769066666666666,
      "grad_norm": 0.30011796951293945,
      "learning_rate": 1.3943333333333333e-05,
      "loss": 0.0023,
      "step": 108170
    },
    {
      "epoch": 5.7696,
      "grad_norm": 0.07691117376089096,
      "learning_rate": 1.394e-05,
      "loss": 0.0016,
      "step": 108180
    },
    {
      "epoch": 5.770133333333334,
      "grad_norm": 0.3034808933734894,
      "learning_rate": 1.3936666666666666e-05,
      "loss": 0.0019,
      "step": 108190
    },
    {
      "epoch": 5.770666666666667,
      "grad_norm": 0.0826340764760971,
      "learning_rate": 1.3933333333333334e-05,
      "loss": 0.002,
      "step": 108200
    },
    {
      "epoch": 5.7712,
      "grad_norm": 0.29067328572273254,
      "learning_rate": 1.3930000000000002e-05,
      "loss": 0.0018,
      "step": 108210
    },
    {
      "epoch": 5.771733333333334,
      "grad_norm": 0.06013714522123337,
      "learning_rate": 1.3926666666666668e-05,
      "loss": 0.0022,
      "step": 108220
    },
    {
      "epoch": 5.772266666666667,
      "grad_norm": 0.24328011274337769,
      "learning_rate": 1.3923333333333333e-05,
      "loss": 0.0021,
      "step": 108230
    },
    {
      "epoch": 5.7728,
      "grad_norm": 0.17864643037319183,
      "learning_rate": 1.3919999999999999e-05,
      "loss": 0.0026,
      "step": 108240
    },
    {
      "epoch": 5.773333333333333,
      "grad_norm": 0.10260400176048279,
      "learning_rate": 1.3916666666666667e-05,
      "loss": 0.0016,
      "step": 108250
    },
    {
      "epoch": 5.773866666666667,
      "grad_norm": 0.1653573215007782,
      "learning_rate": 1.3913333333333335e-05,
      "loss": 0.0017,
      "step": 108260
    },
    {
      "epoch": 5.7744,
      "grad_norm": 0.14681848883628845,
      "learning_rate": 1.391e-05,
      "loss": 0.0024,
      "step": 108270
    },
    {
      "epoch": 5.774933333333333,
      "grad_norm": 0.43023282289505005,
      "learning_rate": 1.3906666666666668e-05,
      "loss": 0.0013,
      "step": 108280
    },
    {
      "epoch": 5.7754666666666665,
      "grad_norm": 0.26800110936164856,
      "learning_rate": 1.3903333333333335e-05,
      "loss": 0.0016,
      "step": 108290
    },
    {
      "epoch": 5.776,
      "grad_norm": 0.12076172232627869,
      "learning_rate": 1.3900000000000002e-05,
      "loss": 0.0023,
      "step": 108300
    },
    {
      "epoch": 5.776533333333333,
      "grad_norm": 0.0428842268884182,
      "learning_rate": 1.3896666666666667e-05,
      "loss": 0.0022,
      "step": 108310
    },
    {
      "epoch": 5.777066666666666,
      "grad_norm": 0.182252898812294,
      "learning_rate": 1.3893333333333333e-05,
      "loss": 0.0021,
      "step": 108320
    },
    {
      "epoch": 5.7776,
      "grad_norm": 0.17728759348392487,
      "learning_rate": 1.389e-05,
      "loss": 0.0024,
      "step": 108330
    },
    {
      "epoch": 5.778133333333333,
      "grad_norm": 0.2992166578769684,
      "learning_rate": 1.3886666666666667e-05,
      "loss": 0.0015,
      "step": 108340
    },
    {
      "epoch": 5.778666666666666,
      "grad_norm": 0.2117464542388916,
      "learning_rate": 1.3883333333333335e-05,
      "loss": 0.0018,
      "step": 108350
    },
    {
      "epoch": 5.7792,
      "grad_norm": 0.21532170474529266,
      "learning_rate": 1.3880000000000001e-05,
      "loss": 0.0025,
      "step": 108360
    },
    {
      "epoch": 5.779733333333334,
      "grad_norm": 0.12687675654888153,
      "learning_rate": 1.3876666666666669e-05,
      "loss": 0.0022,
      "step": 108370
    },
    {
      "epoch": 5.780266666666667,
      "grad_norm": 0.21661028265953064,
      "learning_rate": 1.3873333333333333e-05,
      "loss": 0.0011,
      "step": 108380
    },
    {
      "epoch": 5.7808,
      "grad_norm": 0.16284963488578796,
      "learning_rate": 1.387e-05,
      "loss": 0.0019,
      "step": 108390
    },
    {
      "epoch": 5.781333333333333,
      "grad_norm": 0.2184576541185379,
      "learning_rate": 1.3866666666666667e-05,
      "loss": 0.0019,
      "step": 108400
    },
    {
      "epoch": 5.781866666666667,
      "grad_norm": 0.07146075367927551,
      "learning_rate": 1.3863333333333333e-05,
      "loss": 0.0014,
      "step": 108410
    },
    {
      "epoch": 5.7824,
      "grad_norm": 0.20562897622585297,
      "learning_rate": 1.3860000000000001e-05,
      "loss": 0.0016,
      "step": 108420
    },
    {
      "epoch": 5.782933333333333,
      "grad_norm": 0.3879989683628082,
      "learning_rate": 1.3856666666666667e-05,
      "loss": 0.0019,
      "step": 108430
    },
    {
      "epoch": 5.7834666666666665,
      "grad_norm": 0.0315810926258564,
      "learning_rate": 1.3853333333333335e-05,
      "loss": 0.0019,
      "step": 108440
    },
    {
      "epoch": 5.784,
      "grad_norm": 0.039438311010599136,
      "learning_rate": 1.3850000000000001e-05,
      "loss": 0.0016,
      "step": 108450
    },
    {
      "epoch": 5.784533333333333,
      "grad_norm": 0.13020028173923492,
      "learning_rate": 1.3846666666666666e-05,
      "loss": 0.0015,
      "step": 108460
    },
    {
      "epoch": 5.785066666666666,
      "grad_norm": 0.5033594369888306,
      "learning_rate": 1.3843333333333333e-05,
      "loss": 0.0014,
      "step": 108470
    },
    {
      "epoch": 5.7856,
      "grad_norm": 0.3887867331504822,
      "learning_rate": 1.384e-05,
      "loss": 0.002,
      "step": 108480
    },
    {
      "epoch": 5.786133333333334,
      "grad_norm": 0.04253076761960983,
      "learning_rate": 1.3836666666666667e-05,
      "loss": 0.0015,
      "step": 108490
    },
    {
      "epoch": 5.786666666666667,
      "grad_norm": 0.07868857681751251,
      "learning_rate": 1.3833333333333334e-05,
      "loss": 0.002,
      "step": 108500
    },
    {
      "epoch": 5.7872,
      "grad_norm": 0.03291730210185051,
      "learning_rate": 1.3830000000000001e-05,
      "loss": 0.0018,
      "step": 108510
    },
    {
      "epoch": 5.787733333333334,
      "grad_norm": 0.16558200120925903,
      "learning_rate": 1.3826666666666668e-05,
      "loss": 0.0023,
      "step": 108520
    },
    {
      "epoch": 5.788266666666667,
      "grad_norm": 0.21866783499717712,
      "learning_rate": 1.3823333333333335e-05,
      "loss": 0.0014,
      "step": 108530
    },
    {
      "epoch": 5.7888,
      "grad_norm": 0.11391093581914902,
      "learning_rate": 1.382e-05,
      "loss": 0.002,
      "step": 108540
    },
    {
      "epoch": 5.789333333333333,
      "grad_norm": 0.2729485332965851,
      "learning_rate": 1.3816666666666666e-05,
      "loss": 0.0023,
      "step": 108550
    },
    {
      "epoch": 5.789866666666667,
      "grad_norm": 0.14571882784366608,
      "learning_rate": 1.3813333333333334e-05,
      "loss": 0.0014,
      "step": 108560
    },
    {
      "epoch": 5.7904,
      "grad_norm": 0.23632369935512543,
      "learning_rate": 1.381e-05,
      "loss": 0.0023,
      "step": 108570
    },
    {
      "epoch": 5.790933333333333,
      "grad_norm": 0.12905830144882202,
      "learning_rate": 1.3806666666666668e-05,
      "loss": 0.0014,
      "step": 108580
    },
    {
      "epoch": 5.7914666666666665,
      "grad_norm": 0.1802307665348053,
      "learning_rate": 1.3803333333333336e-05,
      "loss": 0.002,
      "step": 108590
    },
    {
      "epoch": 5.792,
      "grad_norm": 0.29503390192985535,
      "learning_rate": 1.3800000000000002e-05,
      "loss": 0.002,
      "step": 108600
    },
    {
      "epoch": 5.792533333333333,
      "grad_norm": 0.20905503630638123,
      "learning_rate": 1.3796666666666666e-05,
      "loss": 0.0025,
      "step": 108610
    },
    {
      "epoch": 5.793066666666666,
      "grad_norm": 0.3943948447704315,
      "learning_rate": 1.3793333333333332e-05,
      "loss": 0.0018,
      "step": 108620
    },
    {
      "epoch": 5.7936,
      "grad_norm": 0.38186410069465637,
      "learning_rate": 1.379e-05,
      "loss": 0.0018,
      "step": 108630
    },
    {
      "epoch": 5.794133333333333,
      "grad_norm": 0.21416713297367096,
      "learning_rate": 1.3786666666666668e-05,
      "loss": 0.0021,
      "step": 108640
    },
    {
      "epoch": 5.794666666666666,
      "grad_norm": 0.08553769439458847,
      "learning_rate": 1.3783333333333334e-05,
      "loss": 0.0021,
      "step": 108650
    },
    {
      "epoch": 5.7952,
      "grad_norm": 0.07587612420320511,
      "learning_rate": 1.3780000000000002e-05,
      "loss": 0.0026,
      "step": 108660
    },
    {
      "epoch": 5.795733333333334,
      "grad_norm": 0.19257979094982147,
      "learning_rate": 1.3776666666666668e-05,
      "loss": 0.0023,
      "step": 108670
    },
    {
      "epoch": 5.796266666666667,
      "grad_norm": 0.20632009208202362,
      "learning_rate": 1.3773333333333336e-05,
      "loss": 0.0017,
      "step": 108680
    },
    {
      "epoch": 5.7968,
      "grad_norm": 0.0930359810590744,
      "learning_rate": 1.377e-05,
      "loss": 0.002,
      "step": 108690
    },
    {
      "epoch": 5.7973333333333334,
      "grad_norm": 0.041032057255506516,
      "learning_rate": 1.3766666666666666e-05,
      "loss": 0.002,
      "step": 108700
    },
    {
      "epoch": 5.797866666666667,
      "grad_norm": 0.2508038282394409,
      "learning_rate": 1.3763333333333334e-05,
      "loss": 0.0023,
      "step": 108710
    },
    {
      "epoch": 5.7984,
      "grad_norm": 0.18708179891109467,
      "learning_rate": 1.376e-05,
      "loss": 0.0016,
      "step": 108720
    },
    {
      "epoch": 5.798933333333333,
      "grad_norm": 0.26931333541870117,
      "learning_rate": 1.3756666666666668e-05,
      "loss": 0.0014,
      "step": 108730
    },
    {
      "epoch": 5.7994666666666665,
      "grad_norm": 0.09371043741703033,
      "learning_rate": 1.3753333333333334e-05,
      "loss": 0.0016,
      "step": 108740
    },
    {
      "epoch": 5.8,
      "grad_norm": 0.05710002779960632,
      "learning_rate": 1.3750000000000002e-05,
      "loss": 0.0016,
      "step": 108750
    },
    {
      "epoch": 5.800533333333333,
      "grad_norm": 0.24929451942443848,
      "learning_rate": 1.3746666666666667e-05,
      "loss": 0.0016,
      "step": 108760
    },
    {
      "epoch": 5.801066666666666,
      "grad_norm": 0.2730632424354553,
      "learning_rate": 1.3743333333333333e-05,
      "loss": 0.0025,
      "step": 108770
    },
    {
      "epoch": 5.8016,
      "grad_norm": 0.04563285410404205,
      "learning_rate": 1.374e-05,
      "loss": 0.0022,
      "step": 108780
    },
    {
      "epoch": 5.802133333333334,
      "grad_norm": 0.33772987127304077,
      "learning_rate": 1.3736666666666667e-05,
      "loss": 0.0016,
      "step": 108790
    },
    {
      "epoch": 5.802666666666667,
      "grad_norm": 0.35830289125442505,
      "learning_rate": 1.3733333333333335e-05,
      "loss": 0.0027,
      "step": 108800
    },
    {
      "epoch": 5.8032,
      "grad_norm": 0.25437241792678833,
      "learning_rate": 1.373e-05,
      "loss": 0.0025,
      "step": 108810
    },
    {
      "epoch": 5.803733333333334,
      "grad_norm": 0.4273146688938141,
      "learning_rate": 1.3726666666666669e-05,
      "loss": 0.0014,
      "step": 108820
    },
    {
      "epoch": 5.804266666666667,
      "grad_norm": 0.5954852104187012,
      "learning_rate": 1.3723333333333335e-05,
      "loss": 0.0015,
      "step": 108830
    },
    {
      "epoch": 5.8048,
      "grad_norm": 0.06818423420190811,
      "learning_rate": 1.3719999999999999e-05,
      "loss": 0.0018,
      "step": 108840
    },
    {
      "epoch": 5.8053333333333335,
      "grad_norm": 0.06820009648799896,
      "learning_rate": 1.3716666666666667e-05,
      "loss": 0.0019,
      "step": 108850
    },
    {
      "epoch": 5.805866666666667,
      "grad_norm": 0.11679317057132721,
      "learning_rate": 1.3713333333333333e-05,
      "loss": 0.0015,
      "step": 108860
    },
    {
      "epoch": 5.8064,
      "grad_norm": 0.2514593005180359,
      "learning_rate": 1.3710000000000001e-05,
      "loss": 0.0018,
      "step": 108870
    },
    {
      "epoch": 5.806933333333333,
      "grad_norm": 0.18323522806167603,
      "learning_rate": 1.3706666666666667e-05,
      "loss": 0.0019,
      "step": 108880
    },
    {
      "epoch": 5.8074666666666666,
      "grad_norm": 0.2766777575016022,
      "learning_rate": 1.3703333333333335e-05,
      "loss": 0.0018,
      "step": 108890
    },
    {
      "epoch": 5.808,
      "grad_norm": 0.1801094263792038,
      "learning_rate": 1.3700000000000001e-05,
      "loss": 0.0016,
      "step": 108900
    },
    {
      "epoch": 5.808533333333333,
      "grad_norm": 0.18869981169700623,
      "learning_rate": 1.3696666666666665e-05,
      "loss": 0.0017,
      "step": 108910
    },
    {
      "epoch": 5.809066666666666,
      "grad_norm": 0.15252335369586945,
      "learning_rate": 1.3693333333333333e-05,
      "loss": 0.0023,
      "step": 108920
    },
    {
      "epoch": 5.8096,
      "grad_norm": 0.1528691202402115,
      "learning_rate": 1.369e-05,
      "loss": 0.0014,
      "step": 108930
    },
    {
      "epoch": 5.810133333333333,
      "grad_norm": 0.28505030274391174,
      "learning_rate": 1.3686666666666667e-05,
      "loss": 0.0028,
      "step": 108940
    },
    {
      "epoch": 5.810666666666666,
      "grad_norm": 0.044585440307855606,
      "learning_rate": 1.3683333333333333e-05,
      "loss": 0.0022,
      "step": 108950
    },
    {
      "epoch": 5.8112,
      "grad_norm": 0.38454610109329224,
      "learning_rate": 1.3680000000000001e-05,
      "loss": 0.0017,
      "step": 108960
    },
    {
      "epoch": 5.811733333333334,
      "grad_norm": 0.26810741424560547,
      "learning_rate": 1.3676666666666669e-05,
      "loss": 0.0026,
      "step": 108970
    },
    {
      "epoch": 5.812266666666667,
      "grad_norm": 0.030395476147532463,
      "learning_rate": 1.3673333333333335e-05,
      "loss": 0.0016,
      "step": 108980
    },
    {
      "epoch": 5.8128,
      "grad_norm": 0.474945992231369,
      "learning_rate": 1.367e-05,
      "loss": 0.0026,
      "step": 108990
    },
    {
      "epoch": 5.8133333333333335,
      "grad_norm": 0.2705167829990387,
      "learning_rate": 1.3666666666666666e-05,
      "loss": 0.0023,
      "step": 109000
    },
    {
      "epoch": 5.813866666666667,
      "grad_norm": 0.06946566700935364,
      "learning_rate": 1.3663333333333334e-05,
      "loss": 0.0023,
      "step": 109010
    },
    {
      "epoch": 5.8144,
      "grad_norm": 0.0666506215929985,
      "learning_rate": 1.3660000000000001e-05,
      "loss": 0.0026,
      "step": 109020
    },
    {
      "epoch": 5.814933333333333,
      "grad_norm": 0.25055766105651855,
      "learning_rate": 1.3656666666666667e-05,
      "loss": 0.0012,
      "step": 109030
    },
    {
      "epoch": 5.815466666666667,
      "grad_norm": 0.1284114569425583,
      "learning_rate": 1.3653333333333335e-05,
      "loss": 0.0012,
      "step": 109040
    },
    {
      "epoch": 5.816,
      "grad_norm": 0.04504424333572388,
      "learning_rate": 1.3650000000000001e-05,
      "loss": 0.0019,
      "step": 109050
    },
    {
      "epoch": 5.816533333333333,
      "grad_norm": 0.09475220739841461,
      "learning_rate": 1.3646666666666666e-05,
      "loss": 0.0018,
      "step": 109060
    },
    {
      "epoch": 5.817066666666666,
      "grad_norm": 0.2144814282655716,
      "learning_rate": 1.3643333333333334e-05,
      "loss": 0.0022,
      "step": 109070
    },
    {
      "epoch": 5.8176,
      "grad_norm": 0.2764716148376465,
      "learning_rate": 1.364e-05,
      "loss": 0.0019,
      "step": 109080
    },
    {
      "epoch": 5.818133333333334,
      "grad_norm": 0.6053416728973389,
      "learning_rate": 1.3636666666666668e-05,
      "loss": 0.0016,
      "step": 109090
    },
    {
      "epoch": 5.818666666666667,
      "grad_norm": 0.5040051341056824,
      "learning_rate": 1.3633333333333334e-05,
      "loss": 0.0013,
      "step": 109100
    },
    {
      "epoch": 5.8192,
      "grad_norm": 0.4577985405921936,
      "learning_rate": 1.3630000000000002e-05,
      "loss": 0.0022,
      "step": 109110
    },
    {
      "epoch": 5.819733333333334,
      "grad_norm": 0.2502383589744568,
      "learning_rate": 1.3626666666666668e-05,
      "loss": 0.0015,
      "step": 109120
    },
    {
      "epoch": 5.820266666666667,
      "grad_norm": 0.3887823820114136,
      "learning_rate": 1.3623333333333336e-05,
      "loss": 0.0019,
      "step": 109130
    },
    {
      "epoch": 5.8208,
      "grad_norm": 0.12238887697458267,
      "learning_rate": 1.362e-05,
      "loss": 0.0021,
      "step": 109140
    },
    {
      "epoch": 5.8213333333333335,
      "grad_norm": 0.06226419284939766,
      "learning_rate": 1.3616666666666666e-05,
      "loss": 0.0019,
      "step": 109150
    },
    {
      "epoch": 5.821866666666667,
      "grad_norm": 0.12321029603481293,
      "learning_rate": 1.3613333333333334e-05,
      "loss": 0.0018,
      "step": 109160
    },
    {
      "epoch": 5.8224,
      "grad_norm": 0.3575044870376587,
      "learning_rate": 1.361e-05,
      "loss": 0.0018,
      "step": 109170
    },
    {
      "epoch": 5.822933333333333,
      "grad_norm": 0.09444503486156464,
      "learning_rate": 1.3606666666666668e-05,
      "loss": 0.002,
      "step": 109180
    },
    {
      "epoch": 5.823466666666667,
      "grad_norm": 0.14954662322998047,
      "learning_rate": 1.3603333333333334e-05,
      "loss": 0.0017,
      "step": 109190
    },
    {
      "epoch": 5.824,
      "grad_norm": 0.32777392864227295,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 0.0016,
      "step": 109200
    },
    {
      "epoch": 5.824533333333333,
      "grad_norm": 0.2381477802991867,
      "learning_rate": 1.3596666666666668e-05,
      "loss": 0.0028,
      "step": 109210
    },
    {
      "epoch": 5.825066666666666,
      "grad_norm": 0.31149864196777344,
      "learning_rate": 1.3593333333333332e-05,
      "loss": 0.0014,
      "step": 109220
    },
    {
      "epoch": 5.8256,
      "grad_norm": 0.033044591546058655,
      "learning_rate": 1.359e-05,
      "loss": 0.0018,
      "step": 109230
    },
    {
      "epoch": 5.826133333333333,
      "grad_norm": 0.036088015884160995,
      "learning_rate": 1.3586666666666666e-05,
      "loss": 0.0021,
      "step": 109240
    },
    {
      "epoch": 5.826666666666666,
      "grad_norm": 0.4846785366535187,
      "learning_rate": 1.3583333333333334e-05,
      "loss": 0.0021,
      "step": 109250
    },
    {
      "epoch": 5.8272,
      "grad_norm": 0.3551422655582428,
      "learning_rate": 1.358e-05,
      "loss": 0.0025,
      "step": 109260
    },
    {
      "epoch": 5.827733333333334,
      "grad_norm": 0.37937095761299133,
      "learning_rate": 1.3576666666666668e-05,
      "loss": 0.0024,
      "step": 109270
    },
    {
      "epoch": 5.828266666666667,
      "grad_norm": 0.17396406829357147,
      "learning_rate": 1.3573333333333334e-05,
      "loss": 0.0026,
      "step": 109280
    },
    {
      "epoch": 5.8288,
      "grad_norm": 0.38942116498947144,
      "learning_rate": 1.3569999999999999e-05,
      "loss": 0.0028,
      "step": 109290
    },
    {
      "epoch": 5.8293333333333335,
      "grad_norm": 0.1553754359483719,
      "learning_rate": 1.3566666666666667e-05,
      "loss": 0.0018,
      "step": 109300
    },
    {
      "epoch": 5.829866666666667,
      "grad_norm": 0.2349775731563568,
      "learning_rate": 1.3563333333333333e-05,
      "loss": 0.0016,
      "step": 109310
    },
    {
      "epoch": 5.8304,
      "grad_norm": 0.1484895795583725,
      "learning_rate": 1.356e-05,
      "loss": 0.0014,
      "step": 109320
    },
    {
      "epoch": 5.830933333333333,
      "grad_norm": 0.40519216656684875,
      "learning_rate": 1.3556666666666667e-05,
      "loss": 0.0022,
      "step": 109330
    },
    {
      "epoch": 5.831466666666667,
      "grad_norm": 0.44666990637779236,
      "learning_rate": 1.3553333333333335e-05,
      "loss": 0.0026,
      "step": 109340
    },
    {
      "epoch": 5.832,
      "grad_norm": 0.3186498284339905,
      "learning_rate": 1.3550000000000002e-05,
      "loss": 0.0016,
      "step": 109350
    },
    {
      "epoch": 5.832533333333333,
      "grad_norm": 0.2610059380531311,
      "learning_rate": 1.3546666666666669e-05,
      "loss": 0.0014,
      "step": 109360
    },
    {
      "epoch": 5.833066666666666,
      "grad_norm": 0.09429803490638733,
      "learning_rate": 1.3543333333333333e-05,
      "loss": 0.0016,
      "step": 109370
    },
    {
      "epoch": 5.8336,
      "grad_norm": 0.039650749415159225,
      "learning_rate": 1.3539999999999999e-05,
      "loss": 0.0025,
      "step": 109380
    },
    {
      "epoch": 5.834133333333333,
      "grad_norm": 0.5246439576148987,
      "learning_rate": 1.3536666666666667e-05,
      "loss": 0.0018,
      "step": 109390
    },
    {
      "epoch": 5.834666666666667,
      "grad_norm": 0.13393940031528473,
      "learning_rate": 1.3533333333333335e-05,
      "loss": 0.0021,
      "step": 109400
    },
    {
      "epoch": 5.8352,
      "grad_norm": 0.05267399922013283,
      "learning_rate": 1.3530000000000001e-05,
      "loss": 0.0016,
      "step": 109410
    },
    {
      "epoch": 5.835733333333334,
      "grad_norm": 0.05656738951802254,
      "learning_rate": 1.3526666666666669e-05,
      "loss": 0.0017,
      "step": 109420
    },
    {
      "epoch": 5.836266666666667,
      "grad_norm": 0.06414264440536499,
      "learning_rate": 1.3523333333333335e-05,
      "loss": 0.0022,
      "step": 109430
    },
    {
      "epoch": 5.8368,
      "grad_norm": 0.47113311290740967,
      "learning_rate": 1.352e-05,
      "loss": 0.0019,
      "step": 109440
    },
    {
      "epoch": 5.8373333333333335,
      "grad_norm": 0.0463993139564991,
      "learning_rate": 1.3516666666666667e-05,
      "loss": 0.002,
      "step": 109450
    },
    {
      "epoch": 5.837866666666667,
      "grad_norm": 0.48605605959892273,
      "learning_rate": 1.3513333333333333e-05,
      "loss": 0.0013,
      "step": 109460
    },
    {
      "epoch": 5.8384,
      "grad_norm": 0.237155482172966,
      "learning_rate": 1.3510000000000001e-05,
      "loss": 0.0017,
      "step": 109470
    },
    {
      "epoch": 5.838933333333333,
      "grad_norm": 0.035884905606508255,
      "learning_rate": 1.3506666666666667e-05,
      "loss": 0.0019,
      "step": 109480
    },
    {
      "epoch": 5.839466666666667,
      "grad_norm": 0.21202243864536285,
      "learning_rate": 1.3503333333333335e-05,
      "loss": 0.0017,
      "step": 109490
    },
    {
      "epoch": 5.84,
      "grad_norm": 0.12688124179840088,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 0.0022,
      "step": 109500
    },
    {
      "epoch": 5.840533333333333,
      "grad_norm": 0.15791849792003632,
      "learning_rate": 1.3496666666666669e-05,
      "loss": 0.0018,
      "step": 109510
    },
    {
      "epoch": 5.841066666666666,
      "grad_norm": 0.04366244003176689,
      "learning_rate": 1.3493333333333333e-05,
      "loss": 0.002,
      "step": 109520
    },
    {
      "epoch": 5.8416,
      "grad_norm": 0.27370649576187134,
      "learning_rate": 1.349e-05,
      "loss": 0.0015,
      "step": 109530
    },
    {
      "epoch": 5.842133333333333,
      "grad_norm": 0.2966577112674713,
      "learning_rate": 1.3486666666666667e-05,
      "loss": 0.002,
      "step": 109540
    },
    {
      "epoch": 5.842666666666666,
      "grad_norm": 0.3259945213794708,
      "learning_rate": 1.3483333333333334e-05,
      "loss": 0.0032,
      "step": 109550
    },
    {
      "epoch": 5.8431999999999995,
      "grad_norm": 0.07005073875188828,
      "learning_rate": 1.3480000000000001e-05,
      "loss": 0.0016,
      "step": 109560
    },
    {
      "epoch": 5.843733333333334,
      "grad_norm": 0.1695929765701294,
      "learning_rate": 1.3476666666666668e-05,
      "loss": 0.0033,
      "step": 109570
    },
    {
      "epoch": 5.844266666666667,
      "grad_norm": 0.2077377289533615,
      "learning_rate": 1.3473333333333335e-05,
      "loss": 0.0016,
      "step": 109580
    },
    {
      "epoch": 5.8448,
      "grad_norm": 0.29575619101524353,
      "learning_rate": 1.347e-05,
      "loss": 0.003,
      "step": 109590
    },
    {
      "epoch": 5.8453333333333335,
      "grad_norm": 0.18551667034626007,
      "learning_rate": 1.3466666666666666e-05,
      "loss": 0.0014,
      "step": 109600
    },
    {
      "epoch": 5.845866666666667,
      "grad_norm": 0.4970536530017853,
      "learning_rate": 1.3463333333333334e-05,
      "loss": 0.0023,
      "step": 109610
    },
    {
      "epoch": 5.8464,
      "grad_norm": 0.19043360650539398,
      "learning_rate": 1.346e-05,
      "loss": 0.0018,
      "step": 109620
    },
    {
      "epoch": 5.846933333333333,
      "grad_norm": 0.21490047872066498,
      "learning_rate": 1.3456666666666668e-05,
      "loss": 0.0023,
      "step": 109630
    },
    {
      "epoch": 5.847466666666667,
      "grad_norm": 0.050197482109069824,
      "learning_rate": 1.3453333333333334e-05,
      "loss": 0.0014,
      "step": 109640
    },
    {
      "epoch": 5.848,
      "grad_norm": 0.3354837894439697,
      "learning_rate": 1.3450000000000002e-05,
      "loss": 0.0017,
      "step": 109650
    },
    {
      "epoch": 5.848533333333333,
      "grad_norm": 0.09654891490936279,
      "learning_rate": 1.3446666666666668e-05,
      "loss": 0.0015,
      "step": 109660
    },
    {
      "epoch": 5.849066666666666,
      "grad_norm": 0.21169181168079376,
      "learning_rate": 1.3443333333333332e-05,
      "loss": 0.0021,
      "step": 109670
    },
    {
      "epoch": 5.8496,
      "grad_norm": 0.15502645075321198,
      "learning_rate": 1.344e-05,
      "loss": 0.0021,
      "step": 109680
    },
    {
      "epoch": 5.850133333333333,
      "grad_norm": 0.2694542706012726,
      "learning_rate": 1.3436666666666666e-05,
      "loss": 0.0017,
      "step": 109690
    },
    {
      "epoch": 5.850666666666667,
      "grad_norm": 0.04345905780792236,
      "learning_rate": 1.3433333333333334e-05,
      "loss": 0.0019,
      "step": 109700
    },
    {
      "epoch": 5.8512,
      "grad_norm": 0.30354589223861694,
      "learning_rate": 1.343e-05,
      "loss": 0.0025,
      "step": 109710
    },
    {
      "epoch": 5.851733333333334,
      "grad_norm": 0.04472735524177551,
      "learning_rate": 1.3426666666666668e-05,
      "loss": 0.002,
      "step": 109720
    },
    {
      "epoch": 5.852266666666667,
      "grad_norm": 0.28111428022384644,
      "learning_rate": 1.3423333333333336e-05,
      "loss": 0.0013,
      "step": 109730
    },
    {
      "epoch": 5.8528,
      "grad_norm": 0.1276795119047165,
      "learning_rate": 1.3420000000000002e-05,
      "loss": 0.0021,
      "step": 109740
    },
    {
      "epoch": 5.8533333333333335,
      "grad_norm": 0.451585978269577,
      "learning_rate": 1.3416666666666666e-05,
      "loss": 0.0021,
      "step": 109750
    },
    {
      "epoch": 5.853866666666667,
      "grad_norm": 0.09116248786449432,
      "learning_rate": 1.3413333333333333e-05,
      "loss": 0.0015,
      "step": 109760
    },
    {
      "epoch": 5.8544,
      "grad_norm": 0.21248845756053925,
      "learning_rate": 1.341e-05,
      "loss": 0.0019,
      "step": 109770
    },
    {
      "epoch": 5.854933333333333,
      "grad_norm": 0.1246861070394516,
      "learning_rate": 1.3406666666666668e-05,
      "loss": 0.0032,
      "step": 109780
    },
    {
      "epoch": 5.855466666666667,
      "grad_norm": 0.13082736730575562,
      "learning_rate": 1.3403333333333334e-05,
      "loss": 0.0022,
      "step": 109790
    },
    {
      "epoch": 5.856,
      "grad_norm": 0.04540920630097389,
      "learning_rate": 1.3400000000000002e-05,
      "loss": 0.0018,
      "step": 109800
    },
    {
      "epoch": 5.856533333333333,
      "grad_norm": 0.0617985725402832,
      "learning_rate": 1.3396666666666668e-05,
      "loss": 0.0021,
      "step": 109810
    },
    {
      "epoch": 5.857066666666666,
      "grad_norm": 0.2046584039926529,
      "learning_rate": 1.3393333333333333e-05,
      "loss": 0.0016,
      "step": 109820
    },
    {
      "epoch": 5.8576,
      "grad_norm": 0.3083840608596802,
      "learning_rate": 1.339e-05,
      "loss": 0.0016,
      "step": 109830
    },
    {
      "epoch": 5.858133333333333,
      "grad_norm": 0.24330918490886688,
      "learning_rate": 1.3386666666666667e-05,
      "loss": 0.0018,
      "step": 109840
    },
    {
      "epoch": 5.858666666666666,
      "grad_norm": 0.5063393712043762,
      "learning_rate": 1.3383333333333335e-05,
      "loss": 0.002,
      "step": 109850
    },
    {
      "epoch": 5.8591999999999995,
      "grad_norm": 0.24634303152561188,
      "learning_rate": 1.338e-05,
      "loss": 0.0013,
      "step": 109860
    },
    {
      "epoch": 5.859733333333334,
      "grad_norm": 0.20801085233688354,
      "learning_rate": 1.3376666666666668e-05,
      "loss": 0.0019,
      "step": 109870
    },
    {
      "epoch": 5.860266666666667,
      "grad_norm": 0.5485246181488037,
      "learning_rate": 1.3373333333333335e-05,
      "loss": 0.0018,
      "step": 109880
    },
    {
      "epoch": 5.8608,
      "grad_norm": 0.06707214564085007,
      "learning_rate": 1.3370000000000002e-05,
      "loss": 0.0027,
      "step": 109890
    },
    {
      "epoch": 5.8613333333333335,
      "grad_norm": 0.36432668566703796,
      "learning_rate": 1.3366666666666667e-05,
      "loss": 0.002,
      "step": 109900
    },
    {
      "epoch": 5.861866666666667,
      "grad_norm": 0.19565129280090332,
      "learning_rate": 1.3363333333333333e-05,
      "loss": 0.002,
      "step": 109910
    },
    {
      "epoch": 5.8624,
      "grad_norm": 0.03948662430047989,
      "learning_rate": 1.336e-05,
      "loss": 0.0015,
      "step": 109920
    },
    {
      "epoch": 5.862933333333333,
      "grad_norm": 0.3868493139743805,
      "learning_rate": 1.3356666666666667e-05,
      "loss": 0.002,
      "step": 109930
    },
    {
      "epoch": 5.863466666666667,
      "grad_norm": 0.07787556201219559,
      "learning_rate": 1.3353333333333335e-05,
      "loss": 0.0017,
      "step": 109940
    },
    {
      "epoch": 5.864,
      "grad_norm": 0.2451428771018982,
      "learning_rate": 1.3350000000000001e-05,
      "loss": 0.0027,
      "step": 109950
    },
    {
      "epoch": 5.864533333333333,
      "grad_norm": 0.11754138022661209,
      "learning_rate": 1.3346666666666669e-05,
      "loss": 0.004,
      "step": 109960
    },
    {
      "epoch": 5.865066666666666,
      "grad_norm": 0.18075452744960785,
      "learning_rate": 1.3343333333333333e-05,
      "loss": 0.0015,
      "step": 109970
    },
    {
      "epoch": 5.8656,
      "grad_norm": 0.21591730415821075,
      "learning_rate": 1.334e-05,
      "loss": 0.0012,
      "step": 109980
    },
    {
      "epoch": 5.866133333333333,
      "grad_norm": 0.19203683733940125,
      "learning_rate": 1.3336666666666667e-05,
      "loss": 0.0018,
      "step": 109990
    },
    {
      "epoch": 5.866666666666667,
      "grad_norm": 0.4422464370727539,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.0016,
      "step": 110000
    },
    {
      "epoch": 5.8672,
      "grad_norm": 0.23845365643501282,
      "learning_rate": 1.3330000000000001e-05,
      "loss": 0.0018,
      "step": 110010
    },
    {
      "epoch": 5.867733333333334,
      "grad_norm": 0.1753806322813034,
      "learning_rate": 1.3326666666666667e-05,
      "loss": 0.0017,
      "step": 110020
    },
    {
      "epoch": 5.868266666666667,
      "grad_norm": 0.12481062114238739,
      "learning_rate": 1.3323333333333335e-05,
      "loss": 0.0017,
      "step": 110030
    },
    {
      "epoch": 5.8688,
      "grad_norm": 0.06323882192373276,
      "learning_rate": 1.3320000000000001e-05,
      "loss": 0.0019,
      "step": 110040
    },
    {
      "epoch": 5.8693333333333335,
      "grad_norm": 0.3050728738307953,
      "learning_rate": 1.3316666666666666e-05,
      "loss": 0.0024,
      "step": 110050
    },
    {
      "epoch": 5.869866666666667,
      "grad_norm": 0.16345173120498657,
      "learning_rate": 1.3313333333333333e-05,
      "loss": 0.0015,
      "step": 110060
    },
    {
      "epoch": 5.8704,
      "grad_norm": 0.41286295652389526,
      "learning_rate": 1.331e-05,
      "loss": 0.0015,
      "step": 110070
    },
    {
      "epoch": 5.870933333333333,
      "grad_norm": 0.2744429409503937,
      "learning_rate": 1.3306666666666667e-05,
      "loss": 0.0018,
      "step": 110080
    },
    {
      "epoch": 5.871466666666667,
      "grad_norm": 0.2668410539627075,
      "learning_rate": 1.3303333333333334e-05,
      "loss": 0.0015,
      "step": 110090
    },
    {
      "epoch": 5.872,
      "grad_norm": 0.3275725841522217,
      "learning_rate": 1.3300000000000001e-05,
      "loss": 0.0015,
      "step": 110100
    },
    {
      "epoch": 5.872533333333333,
      "grad_norm": 0.09651484340429306,
      "learning_rate": 1.3296666666666668e-05,
      "loss": 0.0026,
      "step": 110110
    },
    {
      "epoch": 5.873066666666666,
      "grad_norm": 0.0495045967400074,
      "learning_rate": 1.3293333333333332e-05,
      "loss": 0.0017,
      "step": 110120
    },
    {
      "epoch": 5.8736,
      "grad_norm": 0.026161639019846916,
      "learning_rate": 1.329e-05,
      "loss": 0.0016,
      "step": 110130
    },
    {
      "epoch": 5.874133333333333,
      "grad_norm": 0.13160216808319092,
      "learning_rate": 1.3286666666666666e-05,
      "loss": 0.0016,
      "step": 110140
    },
    {
      "epoch": 5.874666666666666,
      "grad_norm": 0.21368497610092163,
      "learning_rate": 1.3283333333333334e-05,
      "loss": 0.0022,
      "step": 110150
    },
    {
      "epoch": 5.8751999999999995,
      "grad_norm": 0.11697438359260559,
      "learning_rate": 1.3280000000000002e-05,
      "loss": 0.0016,
      "step": 110160
    },
    {
      "epoch": 5.875733333333334,
      "grad_norm": 0.23517590761184692,
      "learning_rate": 1.3276666666666668e-05,
      "loss": 0.0021,
      "step": 110170
    },
    {
      "epoch": 5.876266666666667,
      "grad_norm": 0.41875699162483215,
      "learning_rate": 1.3273333333333336e-05,
      "loss": 0.0026,
      "step": 110180
    },
    {
      "epoch": 5.8768,
      "grad_norm": 0.07792848348617554,
      "learning_rate": 1.3270000000000002e-05,
      "loss": 0.002,
      "step": 110190
    },
    {
      "epoch": 5.8773333333333335,
      "grad_norm": 0.25490817427635193,
      "learning_rate": 1.3266666666666666e-05,
      "loss": 0.0015,
      "step": 110200
    },
    {
      "epoch": 5.877866666666667,
      "grad_norm": 0.0575670525431633,
      "learning_rate": 1.3263333333333334e-05,
      "loss": 0.0022,
      "step": 110210
    },
    {
      "epoch": 5.8784,
      "grad_norm": 0.504477322101593,
      "learning_rate": 1.326e-05,
      "loss": 0.0028,
      "step": 110220
    },
    {
      "epoch": 5.878933333333333,
      "grad_norm": 0.3221058249473572,
      "learning_rate": 1.3256666666666668e-05,
      "loss": 0.0021,
      "step": 110230
    },
    {
      "epoch": 5.879466666666667,
      "grad_norm": 0.15858110785484314,
      "learning_rate": 1.3253333333333334e-05,
      "loss": 0.002,
      "step": 110240
    },
    {
      "epoch": 5.88,
      "grad_norm": 0.1742144227027893,
      "learning_rate": 1.3250000000000002e-05,
      "loss": 0.002,
      "step": 110250
    },
    {
      "epoch": 5.880533333333333,
      "grad_norm": 0.23590394854545593,
      "learning_rate": 1.3246666666666668e-05,
      "loss": 0.0015,
      "step": 110260
    },
    {
      "epoch": 5.881066666666666,
      "grad_norm": 0.29880136251449585,
      "learning_rate": 1.3243333333333332e-05,
      "loss": 0.0016,
      "step": 110270
    },
    {
      "epoch": 5.8816,
      "grad_norm": 0.5390759110450745,
      "learning_rate": 1.324e-05,
      "loss": 0.0029,
      "step": 110280
    },
    {
      "epoch": 5.882133333333333,
      "grad_norm": 0.19902096688747406,
      "learning_rate": 1.3236666666666666e-05,
      "loss": 0.0022,
      "step": 110290
    },
    {
      "epoch": 5.882666666666667,
      "grad_norm": 0.12823711335659027,
      "learning_rate": 1.3233333333333334e-05,
      "loss": 0.0031,
      "step": 110300
    },
    {
      "epoch": 5.8832,
      "grad_norm": 0.3800051510334015,
      "learning_rate": 1.323e-05,
      "loss": 0.0014,
      "step": 110310
    },
    {
      "epoch": 5.883733333333334,
      "grad_norm": 0.2378818541765213,
      "learning_rate": 1.3226666666666668e-05,
      "loss": 0.0018,
      "step": 110320
    },
    {
      "epoch": 5.884266666666667,
      "grad_norm": 0.12362446635961533,
      "learning_rate": 1.3223333333333334e-05,
      "loss": 0.0015,
      "step": 110330
    },
    {
      "epoch": 5.8848,
      "grad_norm": 0.3253416121006012,
      "learning_rate": 1.3220000000000002e-05,
      "loss": 0.0023,
      "step": 110340
    },
    {
      "epoch": 5.8853333333333335,
      "grad_norm": 0.212874174118042,
      "learning_rate": 1.3216666666666667e-05,
      "loss": 0.0016,
      "step": 110350
    },
    {
      "epoch": 5.885866666666667,
      "grad_norm": 0.41778564453125,
      "learning_rate": 1.3213333333333333e-05,
      "loss": 0.0023,
      "step": 110360
    },
    {
      "epoch": 5.8864,
      "grad_norm": 0.07783479243516922,
      "learning_rate": 1.321e-05,
      "loss": 0.0018,
      "step": 110370
    },
    {
      "epoch": 5.886933333333333,
      "grad_norm": 0.26699402928352356,
      "learning_rate": 1.3206666666666667e-05,
      "loss": 0.0026,
      "step": 110380
    },
    {
      "epoch": 5.887466666666667,
      "grad_norm": 0.04551920294761658,
      "learning_rate": 1.3203333333333335e-05,
      "loss": 0.0017,
      "step": 110390
    },
    {
      "epoch": 5.888,
      "grad_norm": 0.043116193264722824,
      "learning_rate": 1.32e-05,
      "loss": 0.0026,
      "step": 110400
    },
    {
      "epoch": 5.888533333333333,
      "grad_norm": 0.18447260558605194,
      "learning_rate": 1.3196666666666669e-05,
      "loss": 0.0016,
      "step": 110410
    },
    {
      "epoch": 5.8890666666666664,
      "grad_norm": 0.3079850673675537,
      "learning_rate": 1.3193333333333335e-05,
      "loss": 0.0023,
      "step": 110420
    },
    {
      "epoch": 5.8896,
      "grad_norm": 0.5623345971107483,
      "learning_rate": 1.3189999999999999e-05,
      "loss": 0.0014,
      "step": 110430
    },
    {
      "epoch": 5.890133333333333,
      "grad_norm": 0.3000492751598358,
      "learning_rate": 1.3186666666666667e-05,
      "loss": 0.0015,
      "step": 110440
    },
    {
      "epoch": 5.890666666666666,
      "grad_norm": 0.2612086832523346,
      "learning_rate": 1.3183333333333333e-05,
      "loss": 0.002,
      "step": 110450
    },
    {
      "epoch": 5.8911999999999995,
      "grad_norm": 0.19570820033550262,
      "learning_rate": 1.3180000000000001e-05,
      "loss": 0.0016,
      "step": 110460
    },
    {
      "epoch": 5.891733333333334,
      "grad_norm": 0.159832164645195,
      "learning_rate": 1.3176666666666667e-05,
      "loss": 0.0017,
      "step": 110470
    },
    {
      "epoch": 5.892266666666667,
      "grad_norm": 0.07194562256336212,
      "learning_rate": 1.3173333333333335e-05,
      "loss": 0.0014,
      "step": 110480
    },
    {
      "epoch": 5.8928,
      "grad_norm": 0.35645297169685364,
      "learning_rate": 1.3170000000000001e-05,
      "loss": 0.0015,
      "step": 110490
    },
    {
      "epoch": 5.8933333333333335,
      "grad_norm": 0.07771290093660355,
      "learning_rate": 1.3166666666666665e-05,
      "loss": 0.0014,
      "step": 110500
    },
    {
      "epoch": 5.893866666666667,
      "grad_norm": 0.23654364049434662,
      "learning_rate": 1.3163333333333333e-05,
      "loss": 0.0021,
      "step": 110510
    },
    {
      "epoch": 5.8944,
      "grad_norm": 0.29626134037971497,
      "learning_rate": 1.316e-05,
      "loss": 0.0015,
      "step": 110520
    },
    {
      "epoch": 5.894933333333333,
      "grad_norm": 0.11067894101142883,
      "learning_rate": 1.3156666666666667e-05,
      "loss": 0.0016,
      "step": 110530
    },
    {
      "epoch": 5.895466666666667,
      "grad_norm": 0.23935887217521667,
      "learning_rate": 1.3153333333333335e-05,
      "loss": 0.0013,
      "step": 110540
    },
    {
      "epoch": 5.896,
      "grad_norm": 0.05030205473303795,
      "learning_rate": 1.3150000000000001e-05,
      "loss": 0.0016,
      "step": 110550
    },
    {
      "epoch": 5.896533333333333,
      "grad_norm": 0.038102079182863235,
      "learning_rate": 1.3146666666666669e-05,
      "loss": 0.0017,
      "step": 110560
    },
    {
      "epoch": 5.8970666666666665,
      "grad_norm": 0.07402344793081284,
      "learning_rate": 1.3143333333333335e-05,
      "loss": 0.0022,
      "step": 110570
    },
    {
      "epoch": 5.8976,
      "grad_norm": 0.6150380969047546,
      "learning_rate": 1.314e-05,
      "loss": 0.0011,
      "step": 110580
    },
    {
      "epoch": 5.898133333333333,
      "grad_norm": 0.0660477951169014,
      "learning_rate": 1.3136666666666667e-05,
      "loss": 0.0015,
      "step": 110590
    },
    {
      "epoch": 5.898666666666666,
      "grad_norm": 0.03376166522502899,
      "learning_rate": 1.3133333333333334e-05,
      "loss": 0.0016,
      "step": 110600
    },
    {
      "epoch": 5.8992,
      "grad_norm": 0.21719148755073547,
      "learning_rate": 1.3130000000000001e-05,
      "loss": 0.0015,
      "step": 110610
    },
    {
      "epoch": 5.899733333333334,
      "grad_norm": 0.3304314613342285,
      "learning_rate": 1.3126666666666667e-05,
      "loss": 0.0019,
      "step": 110620
    },
    {
      "epoch": 5.900266666666667,
      "grad_norm": 0.1666189432144165,
      "learning_rate": 1.3123333333333335e-05,
      "loss": 0.0015,
      "step": 110630
    },
    {
      "epoch": 5.9008,
      "grad_norm": 0.09908975660800934,
      "learning_rate": 1.3120000000000001e-05,
      "loss": 0.0021,
      "step": 110640
    },
    {
      "epoch": 5.9013333333333335,
      "grad_norm": 0.07339973002672195,
      "learning_rate": 1.3116666666666666e-05,
      "loss": 0.0031,
      "step": 110650
    },
    {
      "epoch": 5.901866666666667,
      "grad_norm": 0.07074083387851715,
      "learning_rate": 1.3113333333333334e-05,
      "loss": 0.0022,
      "step": 110660
    },
    {
      "epoch": 5.9024,
      "grad_norm": 0.27417755126953125,
      "learning_rate": 1.311e-05,
      "loss": 0.0024,
      "step": 110670
    },
    {
      "epoch": 5.902933333333333,
      "grad_norm": 0.27138298749923706,
      "learning_rate": 1.3106666666666668e-05,
      "loss": 0.0013,
      "step": 110680
    },
    {
      "epoch": 5.903466666666667,
      "grad_norm": 0.2446143925189972,
      "learning_rate": 1.3103333333333334e-05,
      "loss": 0.0017,
      "step": 110690
    },
    {
      "epoch": 5.904,
      "grad_norm": 0.039331771433353424,
      "learning_rate": 1.3100000000000002e-05,
      "loss": 0.0022,
      "step": 110700
    },
    {
      "epoch": 5.904533333333333,
      "grad_norm": 0.3238687515258789,
      "learning_rate": 1.3096666666666668e-05,
      "loss": 0.0024,
      "step": 110710
    },
    {
      "epoch": 5.9050666666666665,
      "grad_norm": 0.09497479349374771,
      "learning_rate": 1.3093333333333336e-05,
      "loss": 0.0021,
      "step": 110720
    },
    {
      "epoch": 5.9056,
      "grad_norm": 0.09899744391441345,
      "learning_rate": 1.309e-05,
      "loss": 0.0034,
      "step": 110730
    },
    {
      "epoch": 5.906133333333333,
      "grad_norm": 0.45341622829437256,
      "learning_rate": 1.3086666666666666e-05,
      "loss": 0.0014,
      "step": 110740
    },
    {
      "epoch": 5.906666666666666,
      "grad_norm": 0.17582353949546814,
      "learning_rate": 1.3083333333333334e-05,
      "loss": 0.0015,
      "step": 110750
    },
    {
      "epoch": 5.9072,
      "grad_norm": 0.029454119503498077,
      "learning_rate": 1.308e-05,
      "loss": 0.0016,
      "step": 110760
    },
    {
      "epoch": 5.907733333333333,
      "grad_norm": 0.21668462455272675,
      "learning_rate": 1.3076666666666668e-05,
      "loss": 0.0027,
      "step": 110770
    },
    {
      "epoch": 5.908266666666667,
      "grad_norm": 0.1535412222146988,
      "learning_rate": 1.3073333333333334e-05,
      "loss": 0.0021,
      "step": 110780
    },
    {
      "epoch": 5.9088,
      "grad_norm": 0.21031540632247925,
      "learning_rate": 1.3070000000000002e-05,
      "loss": 0.0014,
      "step": 110790
    },
    {
      "epoch": 5.9093333333333335,
      "grad_norm": 0.19186986982822418,
      "learning_rate": 1.3066666666666666e-05,
      "loss": 0.0025,
      "step": 110800
    },
    {
      "epoch": 5.909866666666667,
      "grad_norm": 0.1932850331068039,
      "learning_rate": 1.3063333333333332e-05,
      "loss": 0.0015,
      "step": 110810
    },
    {
      "epoch": 5.9104,
      "grad_norm": 0.08953285962343216,
      "learning_rate": 1.306e-05,
      "loss": 0.0018,
      "step": 110820
    },
    {
      "epoch": 5.910933333333333,
      "grad_norm": 0.23150397837162018,
      "learning_rate": 1.3056666666666666e-05,
      "loss": 0.0022,
      "step": 110830
    },
    {
      "epoch": 5.911466666666667,
      "grad_norm": 0.2025473713874817,
      "learning_rate": 1.3053333333333334e-05,
      "loss": 0.0014,
      "step": 110840
    },
    {
      "epoch": 5.912,
      "grad_norm": 0.1289971023797989,
      "learning_rate": 1.305e-05,
      "loss": 0.0024,
      "step": 110850
    },
    {
      "epoch": 5.912533333333333,
      "grad_norm": 0.2773949205875397,
      "learning_rate": 1.3046666666666668e-05,
      "loss": 0.0024,
      "step": 110860
    },
    {
      "epoch": 5.9130666666666665,
      "grad_norm": 0.39564934372901917,
      "learning_rate": 1.3043333333333334e-05,
      "loss": 0.0019,
      "step": 110870
    },
    {
      "epoch": 5.9136,
      "grad_norm": 0.42545241117477417,
      "learning_rate": 1.3039999999999999e-05,
      "loss": 0.0017,
      "step": 110880
    },
    {
      "epoch": 5.914133333333333,
      "grad_norm": 0.26744839549064636,
      "learning_rate": 1.3036666666666667e-05,
      "loss": 0.0021,
      "step": 110890
    },
    {
      "epoch": 5.914666666666666,
      "grad_norm": 0.14467790722846985,
      "learning_rate": 1.3033333333333333e-05,
      "loss": 0.0025,
      "step": 110900
    },
    {
      "epoch": 5.9152000000000005,
      "grad_norm": 0.3346119523048401,
      "learning_rate": 1.303e-05,
      "loss": 0.0018,
      "step": 110910
    },
    {
      "epoch": 5.915733333333334,
      "grad_norm": 0.3144387900829315,
      "learning_rate": 1.3026666666666667e-05,
      "loss": 0.0015,
      "step": 110920
    },
    {
      "epoch": 5.916266666666667,
      "grad_norm": 0.03844910487532616,
      "learning_rate": 1.3023333333333335e-05,
      "loss": 0.0022,
      "step": 110930
    },
    {
      "epoch": 5.9168,
      "grad_norm": 0.29852405190467834,
      "learning_rate": 1.3020000000000002e-05,
      "loss": 0.0017,
      "step": 110940
    },
    {
      "epoch": 5.917333333333334,
      "grad_norm": 0.505902886390686,
      "learning_rate": 1.3016666666666669e-05,
      "loss": 0.0015,
      "step": 110950
    },
    {
      "epoch": 5.917866666666667,
      "grad_norm": 0.09360364824533463,
      "learning_rate": 1.3013333333333333e-05,
      "loss": 0.0017,
      "step": 110960
    },
    {
      "epoch": 5.9184,
      "grad_norm": 0.4144514799118042,
      "learning_rate": 1.301e-05,
      "loss": 0.0014,
      "step": 110970
    },
    {
      "epoch": 5.918933333333333,
      "grad_norm": 0.26714441180229187,
      "learning_rate": 1.3006666666666667e-05,
      "loss": 0.0023,
      "step": 110980
    },
    {
      "epoch": 5.919466666666667,
      "grad_norm": 0.06534332036972046,
      "learning_rate": 1.3003333333333335e-05,
      "loss": 0.0026,
      "step": 110990
    },
    {
      "epoch": 5.92,
      "grad_norm": 0.19998638331890106,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.0016,
      "step": 111000
    },
    {
      "epoch": 5.920533333333333,
      "grad_norm": 0.19336968660354614,
      "learning_rate": 1.2996666666666669e-05,
      "loss": 0.0017,
      "step": 111010
    },
    {
      "epoch": 5.9210666666666665,
      "grad_norm": 0.11518527567386627,
      "learning_rate": 1.2993333333333335e-05,
      "loss": 0.0027,
      "step": 111020
    },
    {
      "epoch": 5.9216,
      "grad_norm": 0.4703483581542969,
      "learning_rate": 1.299e-05,
      "loss": 0.0027,
      "step": 111030
    },
    {
      "epoch": 5.922133333333333,
      "grad_norm": 0.3530919551849365,
      "learning_rate": 1.2986666666666667e-05,
      "loss": 0.0017,
      "step": 111040
    },
    {
      "epoch": 5.922666666666666,
      "grad_norm": 0.3344876170158386,
      "learning_rate": 1.2983333333333333e-05,
      "loss": 0.0013,
      "step": 111050
    },
    {
      "epoch": 5.9232,
      "grad_norm": 0.26714569330215454,
      "learning_rate": 1.2980000000000001e-05,
      "loss": 0.0011,
      "step": 111060
    },
    {
      "epoch": 5.923733333333333,
      "grad_norm": 0.06697802990674973,
      "learning_rate": 1.2976666666666667e-05,
      "loss": 0.0021,
      "step": 111070
    },
    {
      "epoch": 5.924266666666667,
      "grad_norm": 0.22965481877326965,
      "learning_rate": 1.2973333333333335e-05,
      "loss": 0.0014,
      "step": 111080
    },
    {
      "epoch": 5.9248,
      "grad_norm": 0.2200465202331543,
      "learning_rate": 1.2970000000000001e-05,
      "loss": 0.0016,
      "step": 111090
    },
    {
      "epoch": 5.925333333333334,
      "grad_norm": 0.3926846385002136,
      "learning_rate": 1.2966666666666669e-05,
      "loss": 0.0026,
      "step": 111100
    },
    {
      "epoch": 5.925866666666667,
      "grad_norm": 0.2726169228553772,
      "learning_rate": 1.2963333333333333e-05,
      "loss": 0.002,
      "step": 111110
    },
    {
      "epoch": 5.9264,
      "grad_norm": 0.5232968926429749,
      "learning_rate": 1.296e-05,
      "loss": 0.0024,
      "step": 111120
    },
    {
      "epoch": 5.926933333333333,
      "grad_norm": 0.07423227280378342,
      "learning_rate": 1.2956666666666667e-05,
      "loss": 0.0021,
      "step": 111130
    },
    {
      "epoch": 5.927466666666667,
      "grad_norm": 0.2482430785894394,
      "learning_rate": 1.2953333333333334e-05,
      "loss": 0.0018,
      "step": 111140
    },
    {
      "epoch": 5.928,
      "grad_norm": 0.03783327713608742,
      "learning_rate": 1.2950000000000001e-05,
      "loss": 0.0019,
      "step": 111150
    },
    {
      "epoch": 5.928533333333333,
      "grad_norm": 0.6414557695388794,
      "learning_rate": 1.2946666666666668e-05,
      "loss": 0.0027,
      "step": 111160
    },
    {
      "epoch": 5.9290666666666665,
      "grad_norm": 0.6412798762321472,
      "learning_rate": 1.2943333333333335e-05,
      "loss": 0.0015,
      "step": 111170
    },
    {
      "epoch": 5.9296,
      "grad_norm": 0.08818110078573227,
      "learning_rate": 1.294e-05,
      "loss": 0.0019,
      "step": 111180
    },
    {
      "epoch": 5.930133333333333,
      "grad_norm": 0.32794633507728577,
      "learning_rate": 1.2936666666666666e-05,
      "loss": 0.0014,
      "step": 111190
    },
    {
      "epoch": 5.930666666666666,
      "grad_norm": 0.12845613062381744,
      "learning_rate": 1.2933333333333334e-05,
      "loss": 0.002,
      "step": 111200
    },
    {
      "epoch": 5.9312000000000005,
      "grad_norm": 0.11851504445075989,
      "learning_rate": 1.293e-05,
      "loss": 0.0021,
      "step": 111210
    },
    {
      "epoch": 5.931733333333334,
      "grad_norm": 0.5129865407943726,
      "learning_rate": 1.2926666666666668e-05,
      "loss": 0.0015,
      "step": 111220
    },
    {
      "epoch": 5.932266666666667,
      "grad_norm": 0.6951216459274292,
      "learning_rate": 1.2923333333333334e-05,
      "loss": 0.0011,
      "step": 111230
    },
    {
      "epoch": 5.9328,
      "grad_norm": 0.04789622873067856,
      "learning_rate": 1.2920000000000002e-05,
      "loss": 0.0012,
      "step": 111240
    },
    {
      "epoch": 5.933333333333334,
      "grad_norm": 0.18087467551231384,
      "learning_rate": 1.2916666666666668e-05,
      "loss": 0.0015,
      "step": 111250
    },
    {
      "epoch": 5.933866666666667,
      "grad_norm": 0.42026928067207336,
      "learning_rate": 1.2913333333333332e-05,
      "loss": 0.0023,
      "step": 111260
    },
    {
      "epoch": 5.9344,
      "grad_norm": 0.7450054883956909,
      "learning_rate": 1.291e-05,
      "loss": 0.0016,
      "step": 111270
    },
    {
      "epoch": 5.934933333333333,
      "grad_norm": 0.5142657160758972,
      "learning_rate": 1.2906666666666666e-05,
      "loss": 0.0015,
      "step": 111280
    },
    {
      "epoch": 5.935466666666667,
      "grad_norm": 0.06252426654100418,
      "learning_rate": 1.2903333333333334e-05,
      "loss": 0.0015,
      "step": 111290
    },
    {
      "epoch": 5.936,
      "grad_norm": 0.13071224093437195,
      "learning_rate": 1.29e-05,
      "loss": 0.0021,
      "step": 111300
    },
    {
      "epoch": 5.936533333333333,
      "grad_norm": 0.33032384514808655,
      "learning_rate": 1.2896666666666668e-05,
      "loss": 0.0016,
      "step": 111310
    },
    {
      "epoch": 5.9370666666666665,
      "grad_norm": 0.03574584424495697,
      "learning_rate": 1.2893333333333336e-05,
      "loss": 0.0014,
      "step": 111320
    },
    {
      "epoch": 5.9376,
      "grad_norm": 0.2073611617088318,
      "learning_rate": 1.2889999999999999e-05,
      "loss": 0.0017,
      "step": 111330
    },
    {
      "epoch": 5.938133333333333,
      "grad_norm": 0.4688190817832947,
      "learning_rate": 1.2886666666666666e-05,
      "loss": 0.0027,
      "step": 111340
    },
    {
      "epoch": 5.938666666666666,
      "grad_norm": 0.10345694422721863,
      "learning_rate": 1.2883333333333333e-05,
      "loss": 0.0018,
      "step": 111350
    },
    {
      "epoch": 5.9392,
      "grad_norm": 0.21036113798618317,
      "learning_rate": 1.288e-05,
      "loss": 0.002,
      "step": 111360
    },
    {
      "epoch": 5.939733333333333,
      "grad_norm": 0.17963607609272003,
      "learning_rate": 1.2876666666666668e-05,
      "loss": 0.0017,
      "step": 111370
    },
    {
      "epoch": 5.940266666666667,
      "grad_norm": 0.21594616770744324,
      "learning_rate": 1.2873333333333334e-05,
      "loss": 0.0016,
      "step": 111380
    },
    {
      "epoch": 5.9408,
      "grad_norm": 0.18253619968891144,
      "learning_rate": 1.2870000000000002e-05,
      "loss": 0.0016,
      "step": 111390
    },
    {
      "epoch": 5.941333333333334,
      "grad_norm": 0.5086433291435242,
      "learning_rate": 1.2866666666666668e-05,
      "loss": 0.0023,
      "step": 111400
    },
    {
      "epoch": 5.941866666666667,
      "grad_norm": 0.09162290394306183,
      "learning_rate": 1.2863333333333333e-05,
      "loss": 0.002,
      "step": 111410
    },
    {
      "epoch": 5.9424,
      "grad_norm": 0.1537596881389618,
      "learning_rate": 1.286e-05,
      "loss": 0.0013,
      "step": 111420
    },
    {
      "epoch": 5.942933333333333,
      "grad_norm": 0.4390130639076233,
      "learning_rate": 1.2856666666666667e-05,
      "loss": 0.0022,
      "step": 111430
    },
    {
      "epoch": 5.943466666666667,
      "grad_norm": 0.1757301241159439,
      "learning_rate": 1.2853333333333335e-05,
      "loss": 0.0015,
      "step": 111440
    },
    {
      "epoch": 5.944,
      "grad_norm": 0.35784292221069336,
      "learning_rate": 1.285e-05,
      "loss": 0.0017,
      "step": 111450
    },
    {
      "epoch": 5.944533333333333,
      "grad_norm": 0.049291737377643585,
      "learning_rate": 1.2846666666666668e-05,
      "loss": 0.0013,
      "step": 111460
    },
    {
      "epoch": 5.9450666666666665,
      "grad_norm": 0.05869868025183678,
      "learning_rate": 1.2843333333333335e-05,
      "loss": 0.002,
      "step": 111470
    },
    {
      "epoch": 5.9456,
      "grad_norm": 0.15291820466518402,
      "learning_rate": 1.2839999999999999e-05,
      "loss": 0.0015,
      "step": 111480
    },
    {
      "epoch": 5.946133333333333,
      "grad_norm": 0.0433201789855957,
      "learning_rate": 1.2836666666666667e-05,
      "loss": 0.0018,
      "step": 111490
    },
    {
      "epoch": 5.946666666666666,
      "grad_norm": 0.38679787516593933,
      "learning_rate": 1.2833333333333333e-05,
      "loss": 0.0014,
      "step": 111500
    },
    {
      "epoch": 5.9472000000000005,
      "grad_norm": 0.04782547801733017,
      "learning_rate": 1.283e-05,
      "loss": 0.002,
      "step": 111510
    },
    {
      "epoch": 5.947733333333334,
      "grad_norm": 0.04813471809029579,
      "learning_rate": 1.2826666666666667e-05,
      "loss": 0.0024,
      "step": 111520
    },
    {
      "epoch": 5.948266666666667,
      "grad_norm": 0.4369685649871826,
      "learning_rate": 1.2823333333333335e-05,
      "loss": 0.002,
      "step": 111530
    },
    {
      "epoch": 5.9488,
      "grad_norm": 0.26808783411979675,
      "learning_rate": 1.2820000000000001e-05,
      "loss": 0.0015,
      "step": 111540
    },
    {
      "epoch": 5.949333333333334,
      "grad_norm": 0.033150289207696915,
      "learning_rate": 1.2816666666666669e-05,
      "loss": 0.0013,
      "step": 111550
    },
    {
      "epoch": 5.949866666666667,
      "grad_norm": 0.19070278108119965,
      "learning_rate": 1.2813333333333333e-05,
      "loss": 0.0017,
      "step": 111560
    },
    {
      "epoch": 5.9504,
      "grad_norm": 0.20350882411003113,
      "learning_rate": 1.281e-05,
      "loss": 0.0013,
      "step": 111570
    },
    {
      "epoch": 5.950933333333333,
      "grad_norm": 0.7446208000183105,
      "learning_rate": 1.2806666666666667e-05,
      "loss": 0.0015,
      "step": 111580
    },
    {
      "epoch": 5.951466666666667,
      "grad_norm": 0.08379148691892624,
      "learning_rate": 1.2803333333333333e-05,
      "loss": 0.0016,
      "step": 111590
    },
    {
      "epoch": 5.952,
      "grad_norm": 0.265577495098114,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 0.0015,
      "step": 111600
    },
    {
      "epoch": 5.952533333333333,
      "grad_norm": 0.36315059661865234,
      "learning_rate": 1.2796666666666667e-05,
      "loss": 0.002,
      "step": 111610
    },
    {
      "epoch": 5.9530666666666665,
      "grad_norm": 0.2744681239128113,
      "learning_rate": 1.2793333333333335e-05,
      "loss": 0.0021,
      "step": 111620
    },
    {
      "epoch": 5.9536,
      "grad_norm": 0.1291297823190689,
      "learning_rate": 1.2790000000000001e-05,
      "loss": 0.002,
      "step": 111630
    },
    {
      "epoch": 5.954133333333333,
      "grad_norm": 0.30692365765571594,
      "learning_rate": 1.2786666666666666e-05,
      "loss": 0.0024,
      "step": 111640
    },
    {
      "epoch": 5.954666666666666,
      "grad_norm": 0.22730572521686554,
      "learning_rate": 1.2783333333333333e-05,
      "loss": 0.0033,
      "step": 111650
    },
    {
      "epoch": 5.9552,
      "grad_norm": 0.0691903606057167,
      "learning_rate": 1.278e-05,
      "loss": 0.002,
      "step": 111660
    },
    {
      "epoch": 5.955733333333333,
      "grad_norm": 0.10195830464363098,
      "learning_rate": 1.2776666666666667e-05,
      "loss": 0.0015,
      "step": 111670
    },
    {
      "epoch": 5.956266666666667,
      "grad_norm": 0.19192302227020264,
      "learning_rate": 1.2773333333333334e-05,
      "loss": 0.0018,
      "step": 111680
    },
    {
      "epoch": 5.9568,
      "grad_norm": 0.3313818573951721,
      "learning_rate": 1.2770000000000001e-05,
      "loss": 0.0024,
      "step": 111690
    },
    {
      "epoch": 5.957333333333334,
      "grad_norm": 0.0791521891951561,
      "learning_rate": 1.276666666666667e-05,
      "loss": 0.0011,
      "step": 111700
    },
    {
      "epoch": 5.957866666666667,
      "grad_norm": 0.2671138048171997,
      "learning_rate": 1.2763333333333332e-05,
      "loss": 0.0014,
      "step": 111710
    },
    {
      "epoch": 5.9584,
      "grad_norm": 0.28295233845710754,
      "learning_rate": 1.276e-05,
      "loss": 0.0021,
      "step": 111720
    },
    {
      "epoch": 5.958933333333333,
      "grad_norm": 0.5068745613098145,
      "learning_rate": 1.2756666666666666e-05,
      "loss": 0.0015,
      "step": 111730
    },
    {
      "epoch": 5.959466666666667,
      "grad_norm": 0.45832982659339905,
      "learning_rate": 1.2753333333333334e-05,
      "loss": 0.0017,
      "step": 111740
    },
    {
      "epoch": 5.96,
      "grad_norm": 0.4559631645679474,
      "learning_rate": 1.2750000000000002e-05,
      "loss": 0.0026,
      "step": 111750
    },
    {
      "epoch": 5.960533333333333,
      "grad_norm": 0.2734655737876892,
      "learning_rate": 1.2746666666666668e-05,
      "loss": 0.0015,
      "step": 111760
    },
    {
      "epoch": 5.9610666666666665,
      "grad_norm": 0.15126055479049683,
      "learning_rate": 1.2743333333333336e-05,
      "loss": 0.0017,
      "step": 111770
    },
    {
      "epoch": 5.9616,
      "grad_norm": 0.23650139570236206,
      "learning_rate": 1.2740000000000002e-05,
      "loss": 0.0017,
      "step": 111780
    },
    {
      "epoch": 5.962133333333333,
      "grad_norm": 0.06644102931022644,
      "learning_rate": 1.2736666666666666e-05,
      "loss": 0.0024,
      "step": 111790
    },
    {
      "epoch": 5.962666666666666,
      "grad_norm": 0.19112573564052582,
      "learning_rate": 1.2733333333333334e-05,
      "loss": 0.0015,
      "step": 111800
    },
    {
      "epoch": 5.9632,
      "grad_norm": 0.19206924736499786,
      "learning_rate": 1.273e-05,
      "loss": 0.002,
      "step": 111810
    },
    {
      "epoch": 5.963733333333334,
      "grad_norm": 0.12703093886375427,
      "learning_rate": 1.2726666666666668e-05,
      "loss": 0.0018,
      "step": 111820
    },
    {
      "epoch": 5.964266666666667,
      "grad_norm": 0.14811356365680695,
      "learning_rate": 1.2723333333333334e-05,
      "loss": 0.0021,
      "step": 111830
    },
    {
      "epoch": 5.9648,
      "grad_norm": 0.026067344471812248,
      "learning_rate": 1.2720000000000002e-05,
      "loss": 0.0015,
      "step": 111840
    },
    {
      "epoch": 5.965333333333334,
      "grad_norm": 0.26001331210136414,
      "learning_rate": 1.2716666666666668e-05,
      "loss": 0.002,
      "step": 111850
    },
    {
      "epoch": 5.965866666666667,
      "grad_norm": 0.07360266894102097,
      "learning_rate": 1.2713333333333332e-05,
      "loss": 0.002,
      "step": 111860
    },
    {
      "epoch": 5.9664,
      "grad_norm": 0.09727013856172562,
      "learning_rate": 1.271e-05,
      "loss": 0.0026,
      "step": 111870
    },
    {
      "epoch": 5.966933333333333,
      "grad_norm": 0.15295790135860443,
      "learning_rate": 1.2706666666666666e-05,
      "loss": 0.0021,
      "step": 111880
    },
    {
      "epoch": 5.967466666666667,
      "grad_norm": 0.3559914827346802,
      "learning_rate": 1.2703333333333334e-05,
      "loss": 0.002,
      "step": 111890
    },
    {
      "epoch": 5.968,
      "grad_norm": 0.3184156119823456,
      "learning_rate": 1.27e-05,
      "loss": 0.0022,
      "step": 111900
    },
    {
      "epoch": 5.968533333333333,
      "grad_norm": 0.29564571380615234,
      "learning_rate": 1.2696666666666668e-05,
      "loss": 0.0018,
      "step": 111910
    },
    {
      "epoch": 5.9690666666666665,
      "grad_norm": 0.15047520399093628,
      "learning_rate": 1.2693333333333334e-05,
      "loss": 0.0021,
      "step": 111920
    },
    {
      "epoch": 5.9696,
      "grad_norm": 0.05173494666814804,
      "learning_rate": 1.2690000000000002e-05,
      "loss": 0.0015,
      "step": 111930
    },
    {
      "epoch": 5.970133333333333,
      "grad_norm": 0.11379382759332657,
      "learning_rate": 1.2686666666666667e-05,
      "loss": 0.0024,
      "step": 111940
    },
    {
      "epoch": 5.970666666666666,
      "grad_norm": 0.3477673828601837,
      "learning_rate": 1.2683333333333333e-05,
      "loss": 0.0015,
      "step": 111950
    },
    {
      "epoch": 5.9712,
      "grad_norm": 0.37026259303092957,
      "learning_rate": 1.268e-05,
      "loss": 0.0018,
      "step": 111960
    },
    {
      "epoch": 5.971733333333333,
      "grad_norm": 0.23603419959545135,
      "learning_rate": 1.2676666666666667e-05,
      "loss": 0.0014,
      "step": 111970
    },
    {
      "epoch": 5.972266666666666,
      "grad_norm": 0.11042863875627518,
      "learning_rate": 1.2673333333333335e-05,
      "loss": 0.0025,
      "step": 111980
    },
    {
      "epoch": 5.9728,
      "grad_norm": 0.2474089115858078,
      "learning_rate": 1.267e-05,
      "loss": 0.0019,
      "step": 111990
    },
    {
      "epoch": 5.973333333333334,
      "grad_norm": 0.5858457684516907,
      "learning_rate": 1.2666666666666668e-05,
      "loss": 0.0014,
      "step": 112000
    },
    {
      "epoch": 5.973866666666667,
      "grad_norm": 0.42326441407203674,
      "learning_rate": 1.2663333333333333e-05,
      "loss": 0.0017,
      "step": 112010
    },
    {
      "epoch": 5.9744,
      "grad_norm": 0.3052210509777069,
      "learning_rate": 1.2659999999999999e-05,
      "loss": 0.0019,
      "step": 112020
    },
    {
      "epoch": 5.974933333333333,
      "grad_norm": 0.19301164150238037,
      "learning_rate": 1.2656666666666667e-05,
      "loss": 0.0015,
      "step": 112030
    },
    {
      "epoch": 5.975466666666667,
      "grad_norm": 0.30501195788383484,
      "learning_rate": 1.2653333333333333e-05,
      "loss": 0.0022,
      "step": 112040
    },
    {
      "epoch": 5.976,
      "grad_norm": 0.18463894724845886,
      "learning_rate": 1.2650000000000001e-05,
      "loss": 0.0016,
      "step": 112050
    },
    {
      "epoch": 5.976533333333333,
      "grad_norm": 0.44228750467300415,
      "learning_rate": 1.2646666666666667e-05,
      "loss": 0.0013,
      "step": 112060
    },
    {
      "epoch": 5.9770666666666665,
      "grad_norm": 0.15026092529296875,
      "learning_rate": 1.2643333333333335e-05,
      "loss": 0.0016,
      "step": 112070
    },
    {
      "epoch": 5.9776,
      "grad_norm": 0.10390528291463852,
      "learning_rate": 1.2640000000000003e-05,
      "loss": 0.0026,
      "step": 112080
    },
    {
      "epoch": 5.978133333333333,
      "grad_norm": 0.4513208568096161,
      "learning_rate": 1.2636666666666665e-05,
      "loss": 0.0016,
      "step": 112090
    },
    {
      "epoch": 5.978666666666666,
      "grad_norm": 0.0857294499874115,
      "learning_rate": 1.2633333333333333e-05,
      "loss": 0.0015,
      "step": 112100
    },
    {
      "epoch": 5.9792,
      "grad_norm": 0.03941695764660835,
      "learning_rate": 1.263e-05,
      "loss": 0.0035,
      "step": 112110
    },
    {
      "epoch": 5.979733333333334,
      "grad_norm": 0.4694368839263916,
      "learning_rate": 1.2626666666666667e-05,
      "loss": 0.0014,
      "step": 112120
    },
    {
      "epoch": 5.980266666666667,
      "grad_norm": 0.3591471016407013,
      "learning_rate": 1.2623333333333335e-05,
      "loss": 0.0021,
      "step": 112130
    },
    {
      "epoch": 5.9808,
      "grad_norm": 0.30120161175727844,
      "learning_rate": 1.2620000000000001e-05,
      "loss": 0.0022,
      "step": 112140
    },
    {
      "epoch": 5.981333333333334,
      "grad_norm": 0.130263552069664,
      "learning_rate": 1.2616666666666669e-05,
      "loss": 0.0028,
      "step": 112150
    },
    {
      "epoch": 5.981866666666667,
      "grad_norm": 0.1602039337158203,
      "learning_rate": 1.2613333333333332e-05,
      "loss": 0.0016,
      "step": 112160
    },
    {
      "epoch": 5.9824,
      "grad_norm": 0.3191657066345215,
      "learning_rate": 1.261e-05,
      "loss": 0.0015,
      "step": 112170
    },
    {
      "epoch": 5.982933333333333,
      "grad_norm": 0.10090699046850204,
      "learning_rate": 1.2606666666666667e-05,
      "loss": 0.0013,
      "step": 112180
    },
    {
      "epoch": 5.983466666666667,
      "grad_norm": 0.29769641160964966,
      "learning_rate": 1.2603333333333334e-05,
      "loss": 0.0025,
      "step": 112190
    },
    {
      "epoch": 5.984,
      "grad_norm": 0.12009928375482559,
      "learning_rate": 1.2600000000000001e-05,
      "loss": 0.0019,
      "step": 112200
    },
    {
      "epoch": 5.984533333333333,
      "grad_norm": 0.14921575784683228,
      "learning_rate": 1.2596666666666667e-05,
      "loss": 0.0024,
      "step": 112210
    },
    {
      "epoch": 5.9850666666666665,
      "grad_norm": 0.2701275050640106,
      "learning_rate": 1.2593333333333335e-05,
      "loss": 0.0019,
      "step": 112220
    },
    {
      "epoch": 5.9856,
      "grad_norm": 0.362214595079422,
      "learning_rate": 1.2590000000000001e-05,
      "loss": 0.0017,
      "step": 112230
    },
    {
      "epoch": 5.986133333333333,
      "grad_norm": 0.060968004167079926,
      "learning_rate": 1.2586666666666666e-05,
      "loss": 0.0019,
      "step": 112240
    },
    {
      "epoch": 5.986666666666666,
      "grad_norm": 0.1561616212129593,
      "learning_rate": 1.2583333333333334e-05,
      "loss": 0.0016,
      "step": 112250
    },
    {
      "epoch": 5.9872,
      "grad_norm": 0.21295978128910065,
      "learning_rate": 1.258e-05,
      "loss": 0.0017,
      "step": 112260
    },
    {
      "epoch": 5.987733333333333,
      "grad_norm": 0.3398405611515045,
      "learning_rate": 1.2576666666666668e-05,
      "loss": 0.0017,
      "step": 112270
    },
    {
      "epoch": 5.988266666666666,
      "grad_norm": 0.04128912091255188,
      "learning_rate": 1.2573333333333334e-05,
      "loss": 0.0013,
      "step": 112280
    },
    {
      "epoch": 5.9888,
      "grad_norm": 0.29524537920951843,
      "learning_rate": 1.2570000000000002e-05,
      "loss": 0.0014,
      "step": 112290
    },
    {
      "epoch": 5.989333333333334,
      "grad_norm": 0.3311338424682617,
      "learning_rate": 1.2566666666666668e-05,
      "loss": 0.0025,
      "step": 112300
    },
    {
      "epoch": 5.989866666666667,
      "grad_norm": 0.36064502596855164,
      "learning_rate": 1.2563333333333336e-05,
      "loss": 0.0021,
      "step": 112310
    },
    {
      "epoch": 5.9904,
      "grad_norm": 0.3574298918247223,
      "learning_rate": 1.256e-05,
      "loss": 0.0014,
      "step": 112320
    },
    {
      "epoch": 5.990933333333333,
      "grad_norm": 0.21181614696979523,
      "learning_rate": 1.2556666666666666e-05,
      "loss": 0.002,
      "step": 112330
    },
    {
      "epoch": 5.991466666666667,
      "grad_norm": 0.30284053087234497,
      "learning_rate": 1.2553333333333334e-05,
      "loss": 0.0015,
      "step": 112340
    },
    {
      "epoch": 5.992,
      "grad_norm": 0.1480262726545334,
      "learning_rate": 1.255e-05,
      "loss": 0.0015,
      "step": 112350
    },
    {
      "epoch": 5.992533333333333,
      "grad_norm": 0.24337151646614075,
      "learning_rate": 1.2546666666666668e-05,
      "loss": 0.0034,
      "step": 112360
    },
    {
      "epoch": 5.9930666666666665,
      "grad_norm": 0.45889386534690857,
      "learning_rate": 1.2543333333333334e-05,
      "loss": 0.0018,
      "step": 112370
    },
    {
      "epoch": 5.9936,
      "grad_norm": 0.4323100745677948,
      "learning_rate": 1.2540000000000002e-05,
      "loss": 0.0018,
      "step": 112380
    },
    {
      "epoch": 5.994133333333333,
      "grad_norm": 0.13702429831027985,
      "learning_rate": 1.2536666666666666e-05,
      "loss": 0.0013,
      "step": 112390
    },
    {
      "epoch": 5.994666666666666,
      "grad_norm": 0.025142205879092216,
      "learning_rate": 1.2533333333333332e-05,
      "loss": 0.0017,
      "step": 112400
    },
    {
      "epoch": 5.9952,
      "grad_norm": 0.3291412591934204,
      "learning_rate": 1.253e-05,
      "loss": 0.0016,
      "step": 112410
    },
    {
      "epoch": 5.995733333333334,
      "grad_norm": 0.4473327696323395,
      "learning_rate": 1.2526666666666666e-05,
      "loss": 0.0019,
      "step": 112420
    },
    {
      "epoch": 5.996266666666667,
      "grad_norm": 0.31169071793556213,
      "learning_rate": 1.2523333333333334e-05,
      "loss": 0.0018,
      "step": 112430
    },
    {
      "epoch": 5.9968,
      "grad_norm": 0.3591363728046417,
      "learning_rate": 1.252e-05,
      "loss": 0.0018,
      "step": 112440
    },
    {
      "epoch": 5.997333333333334,
      "grad_norm": 0.3339695632457733,
      "learning_rate": 1.2516666666666668e-05,
      "loss": 0.0017,
      "step": 112450
    },
    {
      "epoch": 5.997866666666667,
      "grad_norm": 0.5808278918266296,
      "learning_rate": 1.2513333333333336e-05,
      "loss": 0.0018,
      "step": 112460
    },
    {
      "epoch": 5.9984,
      "grad_norm": 0.25186359882354736,
      "learning_rate": 1.2509999999999999e-05,
      "loss": 0.0025,
      "step": 112470
    },
    {
      "epoch": 5.9989333333333335,
      "grad_norm": 0.24374766647815704,
      "learning_rate": 1.2506666666666667e-05,
      "loss": 0.0018,
      "step": 112480
    },
    {
      "epoch": 5.999466666666667,
      "grad_norm": 0.2930762469768524,
      "learning_rate": 1.2503333333333333e-05,
      "loss": 0.0013,
      "step": 112490
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.21686328947544098,
      "learning_rate": 1.25e-05,
      "loss": 0.0017,
      "step": 112500
    },
    {
      "epoch": 6.0,
      "eval_loss": 0.0018986270297318697,
      "eval_runtime": 152.9054,
      "eval_samples_per_second": 1634.997,
      "eval_steps_per_second": 40.875,
      "step": 112500
    },
    {
      "epoch": 6.000533333333333,
      "grad_norm": 0.1768018901348114,
      "learning_rate": 1.2496666666666668e-05,
      "loss": 0.0023,
      "step": 112510
    },
    {
      "epoch": 6.0010666666666665,
      "grad_norm": 0.07661456614732742,
      "learning_rate": 1.2493333333333333e-05,
      "loss": 0.002,
      "step": 112520
    },
    {
      "epoch": 6.0016,
      "grad_norm": 0.3136388063430786,
      "learning_rate": 1.249e-05,
      "loss": 0.0019,
      "step": 112530
    },
    {
      "epoch": 6.002133333333333,
      "grad_norm": 0.7096573710441589,
      "learning_rate": 1.2486666666666667e-05,
      "loss": 0.002,
      "step": 112540
    },
    {
      "epoch": 6.002666666666666,
      "grad_norm": 0.029917847365140915,
      "learning_rate": 1.2483333333333335e-05,
      "loss": 0.002,
      "step": 112550
    },
    {
      "epoch": 6.0032,
      "grad_norm": 0.28312256932258606,
      "learning_rate": 1.248e-05,
      "loss": 0.0022,
      "step": 112560
    },
    {
      "epoch": 6.003733333333333,
      "grad_norm": 0.3237706124782562,
      "learning_rate": 1.2476666666666667e-05,
      "loss": 0.0031,
      "step": 112570
    },
    {
      "epoch": 6.004266666666667,
      "grad_norm": 0.5497846603393555,
      "learning_rate": 1.2473333333333335e-05,
      "loss": 0.0016,
      "step": 112580
    },
    {
      "epoch": 6.0048,
      "grad_norm": 0.3006684184074402,
      "learning_rate": 1.2470000000000001e-05,
      "loss": 0.0024,
      "step": 112590
    },
    {
      "epoch": 6.005333333333334,
      "grad_norm": 0.2967742085456848,
      "learning_rate": 1.2466666666666667e-05,
      "loss": 0.0023,
      "step": 112600
    },
    {
      "epoch": 6.005866666666667,
      "grad_norm": 0.1073877215385437,
      "learning_rate": 1.2463333333333333e-05,
      "loss": 0.0014,
      "step": 112610
    },
    {
      "epoch": 6.0064,
      "grad_norm": 0.2871899902820587,
      "learning_rate": 1.2460000000000001e-05,
      "loss": 0.0021,
      "step": 112620
    },
    {
      "epoch": 6.0069333333333335,
      "grad_norm": 0.4652293920516968,
      "learning_rate": 1.2456666666666667e-05,
      "loss": 0.002,
      "step": 112630
    },
    {
      "epoch": 6.007466666666667,
      "grad_norm": 0.1191163957118988,
      "learning_rate": 1.2453333333333333e-05,
      "loss": 0.0015,
      "step": 112640
    },
    {
      "epoch": 6.008,
      "grad_norm": 0.09056300669908524,
      "learning_rate": 1.2450000000000001e-05,
      "loss": 0.0021,
      "step": 112650
    },
    {
      "epoch": 6.008533333333333,
      "grad_norm": 0.25996068120002747,
      "learning_rate": 1.2446666666666667e-05,
      "loss": 0.0022,
      "step": 112660
    },
    {
      "epoch": 6.009066666666667,
      "grad_norm": 0.15425194799900055,
      "learning_rate": 1.2443333333333335e-05,
      "loss": 0.0027,
      "step": 112670
    },
    {
      "epoch": 6.0096,
      "grad_norm": 0.03281421214342117,
      "learning_rate": 1.244e-05,
      "loss": 0.0013,
      "step": 112680
    },
    {
      "epoch": 6.010133333333333,
      "grad_norm": 0.10231704264879227,
      "learning_rate": 1.2436666666666667e-05,
      "loss": 0.0012,
      "step": 112690
    },
    {
      "epoch": 6.010666666666666,
      "grad_norm": 0.06883188337087631,
      "learning_rate": 1.2433333333333335e-05,
      "loss": 0.0023,
      "step": 112700
    },
    {
      "epoch": 6.0112,
      "grad_norm": 0.34561219811439514,
      "learning_rate": 1.243e-05,
      "loss": 0.0021,
      "step": 112710
    },
    {
      "epoch": 6.011733333333333,
      "grad_norm": 0.17843559384346008,
      "learning_rate": 1.2426666666666667e-05,
      "loss": 0.0019,
      "step": 112720
    },
    {
      "epoch": 6.012266666666667,
      "grad_norm": 0.2874581217765808,
      "learning_rate": 1.2423333333333334e-05,
      "loss": 0.0019,
      "step": 112730
    },
    {
      "epoch": 6.0128,
      "grad_norm": 0.39620351791381836,
      "learning_rate": 1.2420000000000001e-05,
      "loss": 0.002,
      "step": 112740
    },
    {
      "epoch": 6.013333333333334,
      "grad_norm": 0.6239565014839172,
      "learning_rate": 1.2416666666666667e-05,
      "loss": 0.0019,
      "step": 112750
    },
    {
      "epoch": 6.013866666666667,
      "grad_norm": 0.10150983184576035,
      "learning_rate": 1.2413333333333334e-05,
      "loss": 0.0021,
      "step": 112760
    },
    {
      "epoch": 6.0144,
      "grad_norm": 0.0401834100484848,
      "learning_rate": 1.2410000000000001e-05,
      "loss": 0.0024,
      "step": 112770
    },
    {
      "epoch": 6.0149333333333335,
      "grad_norm": 0.16243720054626465,
      "learning_rate": 1.2406666666666668e-05,
      "loss": 0.0019,
      "step": 112780
    },
    {
      "epoch": 6.015466666666667,
      "grad_norm": 0.2763560712337494,
      "learning_rate": 1.2403333333333334e-05,
      "loss": 0.0018,
      "step": 112790
    },
    {
      "epoch": 6.016,
      "grad_norm": 0.21094273030757904,
      "learning_rate": 1.24e-05,
      "loss": 0.0021,
      "step": 112800
    },
    {
      "epoch": 6.016533333333333,
      "grad_norm": 0.3517882227897644,
      "learning_rate": 1.2396666666666668e-05,
      "loss": 0.0017,
      "step": 112810
    },
    {
      "epoch": 6.017066666666667,
      "grad_norm": 0.18180052936077118,
      "learning_rate": 1.2393333333333334e-05,
      "loss": 0.0021,
      "step": 112820
    },
    {
      "epoch": 6.0176,
      "grad_norm": 0.13892465829849243,
      "learning_rate": 1.239e-05,
      "loss": 0.0026,
      "step": 112830
    },
    {
      "epoch": 6.018133333333333,
      "grad_norm": 0.14550119638442993,
      "learning_rate": 1.2386666666666668e-05,
      "loss": 0.0012,
      "step": 112840
    },
    {
      "epoch": 6.018666666666666,
      "grad_norm": 0.09067744761705399,
      "learning_rate": 1.2383333333333334e-05,
      "loss": 0.0017,
      "step": 112850
    },
    {
      "epoch": 6.0192,
      "grad_norm": 0.18432986736297607,
      "learning_rate": 1.238e-05,
      "loss": 0.0026,
      "step": 112860
    },
    {
      "epoch": 6.019733333333333,
      "grad_norm": 0.07295816391706467,
      "learning_rate": 1.2376666666666666e-05,
      "loss": 0.0017,
      "step": 112870
    },
    {
      "epoch": 6.020266666666667,
      "grad_norm": 0.19629447162151337,
      "learning_rate": 1.2373333333333334e-05,
      "loss": 0.003,
      "step": 112880
    },
    {
      "epoch": 6.0208,
      "grad_norm": 0.38344570994377136,
      "learning_rate": 1.2370000000000002e-05,
      "loss": 0.0018,
      "step": 112890
    },
    {
      "epoch": 6.021333333333334,
      "grad_norm": 0.20031322538852692,
      "learning_rate": 1.2366666666666666e-05,
      "loss": 0.0014,
      "step": 112900
    },
    {
      "epoch": 6.021866666666667,
      "grad_norm": 0.06287878006696701,
      "learning_rate": 1.2363333333333334e-05,
      "loss": 0.0019,
      "step": 112910
    },
    {
      "epoch": 6.0224,
      "grad_norm": 0.47504374384880066,
      "learning_rate": 1.236e-05,
      "loss": 0.0017,
      "step": 112920
    },
    {
      "epoch": 6.0229333333333335,
      "grad_norm": 0.06515348702669144,
      "learning_rate": 1.2356666666666668e-05,
      "loss": 0.0015,
      "step": 112930
    },
    {
      "epoch": 6.023466666666667,
      "grad_norm": 0.3310546875,
      "learning_rate": 1.2353333333333334e-05,
      "loss": 0.0019,
      "step": 112940
    },
    {
      "epoch": 6.024,
      "grad_norm": 0.17763668298721313,
      "learning_rate": 1.235e-05,
      "loss": 0.0013,
      "step": 112950
    },
    {
      "epoch": 6.024533333333333,
      "grad_norm": 0.12576669454574585,
      "learning_rate": 1.2346666666666668e-05,
      "loss": 0.0015,
      "step": 112960
    },
    {
      "epoch": 6.025066666666667,
      "grad_norm": 0.2451285421848297,
      "learning_rate": 1.2343333333333334e-05,
      "loss": 0.002,
      "step": 112970
    },
    {
      "epoch": 6.0256,
      "grad_norm": 0.12336859852075577,
      "learning_rate": 1.234e-05,
      "loss": 0.0016,
      "step": 112980
    },
    {
      "epoch": 6.026133333333333,
      "grad_norm": 0.12291837483644485,
      "learning_rate": 1.2336666666666667e-05,
      "loss": 0.0026,
      "step": 112990
    },
    {
      "epoch": 6.026666666666666,
      "grad_norm": 0.40765950083732605,
      "learning_rate": 1.2333333333333334e-05,
      "loss": 0.0022,
      "step": 113000
    },
    {
      "epoch": 6.0272,
      "grad_norm": 0.09450610727071762,
      "learning_rate": 1.233e-05,
      "loss": 0.002,
      "step": 113010
    },
    {
      "epoch": 6.027733333333333,
      "grad_norm": 0.2074243128299713,
      "learning_rate": 1.2326666666666667e-05,
      "loss": 0.0016,
      "step": 113020
    },
    {
      "epoch": 6.028266666666667,
      "grad_norm": 0.12182534486055374,
      "learning_rate": 1.2323333333333334e-05,
      "loss": 0.0017,
      "step": 113030
    },
    {
      "epoch": 6.0288,
      "grad_norm": 0.23769837617874146,
      "learning_rate": 1.232e-05,
      "loss": 0.0021,
      "step": 113040
    },
    {
      "epoch": 6.029333333333334,
      "grad_norm": 0.3552166819572449,
      "learning_rate": 1.2316666666666667e-05,
      "loss": 0.0015,
      "step": 113050
    },
    {
      "epoch": 6.029866666666667,
      "grad_norm": 0.39116984605789185,
      "learning_rate": 1.2313333333333333e-05,
      "loss": 0.0021,
      "step": 113060
    },
    {
      "epoch": 6.0304,
      "grad_norm": 0.03606988117098808,
      "learning_rate": 1.231e-05,
      "loss": 0.0017,
      "step": 113070
    },
    {
      "epoch": 6.0309333333333335,
      "grad_norm": 0.207114577293396,
      "learning_rate": 1.2306666666666669e-05,
      "loss": 0.0014,
      "step": 113080
    },
    {
      "epoch": 6.031466666666667,
      "grad_norm": 0.052639737725257874,
      "learning_rate": 1.2303333333333333e-05,
      "loss": 0.002,
      "step": 113090
    },
    {
      "epoch": 6.032,
      "grad_norm": 0.22741158306598663,
      "learning_rate": 1.23e-05,
      "loss": 0.0022,
      "step": 113100
    },
    {
      "epoch": 6.032533333333333,
      "grad_norm": 0.0953478142619133,
      "learning_rate": 1.2296666666666667e-05,
      "loss": 0.0017,
      "step": 113110
    },
    {
      "epoch": 6.033066666666667,
      "grad_norm": 0.09412923455238342,
      "learning_rate": 1.2293333333333335e-05,
      "loss": 0.0014,
      "step": 113120
    },
    {
      "epoch": 6.0336,
      "grad_norm": 0.35674700140953064,
      "learning_rate": 1.2290000000000001e-05,
      "loss": 0.0021,
      "step": 113130
    },
    {
      "epoch": 6.034133333333333,
      "grad_norm": 0.2089817374944687,
      "learning_rate": 1.2286666666666667e-05,
      "loss": 0.0014,
      "step": 113140
    },
    {
      "epoch": 6.034666666666666,
      "grad_norm": 0.20893043279647827,
      "learning_rate": 1.2283333333333335e-05,
      "loss": 0.0014,
      "step": 113150
    },
    {
      "epoch": 6.0352,
      "grad_norm": 0.10430628061294556,
      "learning_rate": 1.2280000000000001e-05,
      "loss": 0.0014,
      "step": 113160
    },
    {
      "epoch": 6.035733333333333,
      "grad_norm": 0.38389506936073303,
      "learning_rate": 1.2276666666666667e-05,
      "loss": 0.0012,
      "step": 113170
    },
    {
      "epoch": 6.036266666666666,
      "grad_norm": 0.31561654806137085,
      "learning_rate": 1.2273333333333333e-05,
      "loss": 0.0021,
      "step": 113180
    },
    {
      "epoch": 6.0368,
      "grad_norm": 0.0770905390381813,
      "learning_rate": 1.2270000000000001e-05,
      "loss": 0.0019,
      "step": 113190
    },
    {
      "epoch": 6.037333333333334,
      "grad_norm": 0.32864370942115784,
      "learning_rate": 1.2266666666666667e-05,
      "loss": 0.0026,
      "step": 113200
    },
    {
      "epoch": 6.037866666666667,
      "grad_norm": 0.26924678683280945,
      "learning_rate": 1.2263333333333333e-05,
      "loss": 0.0023,
      "step": 113210
    },
    {
      "epoch": 6.0384,
      "grad_norm": 0.08882025629281998,
      "learning_rate": 1.2260000000000001e-05,
      "loss": 0.0022,
      "step": 113220
    },
    {
      "epoch": 6.0389333333333335,
      "grad_norm": 0.12581346929073334,
      "learning_rate": 1.2256666666666667e-05,
      "loss": 0.002,
      "step": 113230
    },
    {
      "epoch": 6.039466666666667,
      "grad_norm": 0.1203036904335022,
      "learning_rate": 1.2253333333333333e-05,
      "loss": 0.0023,
      "step": 113240
    },
    {
      "epoch": 6.04,
      "grad_norm": 0.187762051820755,
      "learning_rate": 1.225e-05,
      "loss": 0.0018,
      "step": 113250
    },
    {
      "epoch": 6.040533333333333,
      "grad_norm": 0.031315967440605164,
      "learning_rate": 1.2246666666666667e-05,
      "loss": 0.0014,
      "step": 113260
    },
    {
      "epoch": 6.041066666666667,
      "grad_norm": 0.33160412311553955,
      "learning_rate": 1.2243333333333335e-05,
      "loss": 0.0026,
      "step": 113270
    },
    {
      "epoch": 6.0416,
      "grad_norm": 0.1773996502161026,
      "learning_rate": 1.224e-05,
      "loss": 0.0021,
      "step": 113280
    },
    {
      "epoch": 6.042133333333333,
      "grad_norm": 0.331481009721756,
      "learning_rate": 1.2236666666666668e-05,
      "loss": 0.0032,
      "step": 113290
    },
    {
      "epoch": 6.042666666666666,
      "grad_norm": 0.2202407866716385,
      "learning_rate": 1.2233333333333334e-05,
      "loss": 0.0015,
      "step": 113300
    },
    {
      "epoch": 6.0432,
      "grad_norm": 0.3359583020210266,
      "learning_rate": 1.2230000000000001e-05,
      "loss": 0.0013,
      "step": 113310
    },
    {
      "epoch": 6.043733333333333,
      "grad_norm": 0.1300499439239502,
      "learning_rate": 1.2226666666666668e-05,
      "loss": 0.0016,
      "step": 113320
    },
    {
      "epoch": 6.044266666666666,
      "grad_norm": 0.18177130818367004,
      "learning_rate": 1.2223333333333334e-05,
      "loss": 0.0022,
      "step": 113330
    },
    {
      "epoch": 6.0448,
      "grad_norm": 0.41514670848846436,
      "learning_rate": 1.2220000000000002e-05,
      "loss": 0.002,
      "step": 113340
    },
    {
      "epoch": 6.045333333333334,
      "grad_norm": 0.26435843110084534,
      "learning_rate": 1.2216666666666668e-05,
      "loss": 0.0028,
      "step": 113350
    },
    {
      "epoch": 6.045866666666667,
      "grad_norm": 0.04341757670044899,
      "learning_rate": 1.2213333333333334e-05,
      "loss": 0.0014,
      "step": 113360
    },
    {
      "epoch": 6.0464,
      "grad_norm": 0.2490752935409546,
      "learning_rate": 1.221e-05,
      "loss": 0.0014,
      "step": 113370
    },
    {
      "epoch": 6.0469333333333335,
      "grad_norm": 0.1988232135772705,
      "learning_rate": 1.2206666666666668e-05,
      "loss": 0.0017,
      "step": 113380
    },
    {
      "epoch": 6.047466666666667,
      "grad_norm": 0.04850838705897331,
      "learning_rate": 1.2203333333333334e-05,
      "loss": 0.0018,
      "step": 113390
    },
    {
      "epoch": 6.048,
      "grad_norm": 0.31660401821136475,
      "learning_rate": 1.22e-05,
      "loss": 0.0028,
      "step": 113400
    },
    {
      "epoch": 6.048533333333333,
      "grad_norm": 0.30582770705223083,
      "learning_rate": 1.2196666666666668e-05,
      "loss": 0.0019,
      "step": 113410
    },
    {
      "epoch": 6.049066666666667,
      "grad_norm": 0.2734019160270691,
      "learning_rate": 1.2193333333333334e-05,
      "loss": 0.0018,
      "step": 113420
    },
    {
      "epoch": 6.0496,
      "grad_norm": 0.07578223943710327,
      "learning_rate": 1.219e-05,
      "loss": 0.0023,
      "step": 113430
    },
    {
      "epoch": 6.050133333333333,
      "grad_norm": 0.07008053362369537,
      "learning_rate": 1.2186666666666666e-05,
      "loss": 0.0021,
      "step": 113440
    },
    {
      "epoch": 6.050666666666666,
      "grad_norm": 0.29604387283325195,
      "learning_rate": 1.2183333333333334e-05,
      "loss": 0.002,
      "step": 113450
    },
    {
      "epoch": 6.0512,
      "grad_norm": 0.12682154774665833,
      "learning_rate": 1.2180000000000002e-05,
      "loss": 0.0019,
      "step": 113460
    },
    {
      "epoch": 6.051733333333333,
      "grad_norm": 0.36408597230911255,
      "learning_rate": 1.2176666666666666e-05,
      "loss": 0.0013,
      "step": 113470
    },
    {
      "epoch": 6.052266666666666,
      "grad_norm": 0.22448626160621643,
      "learning_rate": 1.2173333333333334e-05,
      "loss": 0.002,
      "step": 113480
    },
    {
      "epoch": 6.0528,
      "grad_norm": 0.0503707081079483,
      "learning_rate": 1.217e-05,
      "loss": 0.0015,
      "step": 113490
    },
    {
      "epoch": 6.053333333333334,
      "grad_norm": 0.041880618780851364,
      "learning_rate": 1.2166666666666668e-05,
      "loss": 0.0015,
      "step": 113500
    },
    {
      "epoch": 6.053866666666667,
      "grad_norm": 0.17633381485939026,
      "learning_rate": 1.2163333333333334e-05,
      "loss": 0.0028,
      "step": 113510
    },
    {
      "epoch": 6.0544,
      "grad_norm": 0.2951136529445648,
      "learning_rate": 1.216e-05,
      "loss": 0.0022,
      "step": 113520
    },
    {
      "epoch": 6.0549333333333335,
      "grad_norm": 0.07891228795051575,
      "learning_rate": 1.2156666666666668e-05,
      "loss": 0.0026,
      "step": 113530
    },
    {
      "epoch": 6.055466666666667,
      "grad_norm": 0.18041329085826874,
      "learning_rate": 1.2153333333333333e-05,
      "loss": 0.0024,
      "step": 113540
    },
    {
      "epoch": 6.056,
      "grad_norm": 0.4495674669742584,
      "learning_rate": 1.215e-05,
      "loss": 0.0016,
      "step": 113550
    },
    {
      "epoch": 6.056533333333333,
      "grad_norm": 0.0734209269285202,
      "learning_rate": 1.2146666666666667e-05,
      "loss": 0.0021,
      "step": 113560
    },
    {
      "epoch": 6.057066666666667,
      "grad_norm": 0.20798690617084503,
      "learning_rate": 1.2143333333333335e-05,
      "loss": 0.0024,
      "step": 113570
    },
    {
      "epoch": 6.0576,
      "grad_norm": 0.24249938130378723,
      "learning_rate": 1.214e-05,
      "loss": 0.0014,
      "step": 113580
    },
    {
      "epoch": 6.058133333333333,
      "grad_norm": 0.24810485541820526,
      "learning_rate": 1.2136666666666667e-05,
      "loss": 0.0016,
      "step": 113590
    },
    {
      "epoch": 6.058666666666666,
      "grad_norm": 0.06011214852333069,
      "learning_rate": 1.2133333333333335e-05,
      "loss": 0.0021,
      "step": 113600
    },
    {
      "epoch": 6.0592,
      "grad_norm": 0.0359213650226593,
      "learning_rate": 1.213e-05,
      "loss": 0.0014,
      "step": 113610
    },
    {
      "epoch": 6.059733333333333,
      "grad_norm": 0.09972111880779266,
      "learning_rate": 1.2126666666666667e-05,
      "loss": 0.0014,
      "step": 113620
    },
    {
      "epoch": 6.060266666666666,
      "grad_norm": 0.21526241302490234,
      "learning_rate": 1.2123333333333333e-05,
      "loss": 0.0021,
      "step": 113630
    },
    {
      "epoch": 6.0608,
      "grad_norm": 0.03464493528008461,
      "learning_rate": 1.2120000000000001e-05,
      "loss": 0.0026,
      "step": 113640
    },
    {
      "epoch": 6.061333333333334,
      "grad_norm": 0.23585186898708344,
      "learning_rate": 1.2116666666666669e-05,
      "loss": 0.0025,
      "step": 113650
    },
    {
      "epoch": 6.061866666666667,
      "grad_norm": 0.12379352748394012,
      "learning_rate": 1.2113333333333333e-05,
      "loss": 0.0013,
      "step": 113660
    },
    {
      "epoch": 6.0624,
      "grad_norm": 0.5013842582702637,
      "learning_rate": 1.2110000000000001e-05,
      "loss": 0.0019,
      "step": 113670
    },
    {
      "epoch": 6.0629333333333335,
      "grad_norm": 0.2941814363002777,
      "learning_rate": 1.2106666666666667e-05,
      "loss": 0.0017,
      "step": 113680
    },
    {
      "epoch": 6.063466666666667,
      "grad_norm": 0.10191994905471802,
      "learning_rate": 1.2103333333333335e-05,
      "loss": 0.0015,
      "step": 113690
    },
    {
      "epoch": 6.064,
      "grad_norm": 0.36187615990638733,
      "learning_rate": 1.2100000000000001e-05,
      "loss": 0.0024,
      "step": 113700
    },
    {
      "epoch": 6.064533333333333,
      "grad_norm": 0.28819262981414795,
      "learning_rate": 1.2096666666666667e-05,
      "loss": 0.0019,
      "step": 113710
    },
    {
      "epoch": 6.065066666666667,
      "grad_norm": 0.2432294636964798,
      "learning_rate": 1.2093333333333335e-05,
      "loss": 0.0024,
      "step": 113720
    },
    {
      "epoch": 6.0656,
      "grad_norm": 0.39092564582824707,
      "learning_rate": 1.209e-05,
      "loss": 0.0023,
      "step": 113730
    },
    {
      "epoch": 6.066133333333333,
      "grad_norm": 0.2945045530796051,
      "learning_rate": 1.2086666666666667e-05,
      "loss": 0.0019,
      "step": 113740
    },
    {
      "epoch": 6.066666666666666,
      "grad_norm": 0.16531001031398773,
      "learning_rate": 1.2083333333333333e-05,
      "loss": 0.0017,
      "step": 113750
    },
    {
      "epoch": 6.0672,
      "grad_norm": 0.17047277092933655,
      "learning_rate": 1.2080000000000001e-05,
      "loss": 0.0014,
      "step": 113760
    },
    {
      "epoch": 6.067733333333333,
      "grad_norm": 0.07647164165973663,
      "learning_rate": 1.2076666666666667e-05,
      "loss": 0.0012,
      "step": 113770
    },
    {
      "epoch": 6.068266666666666,
      "grad_norm": 0.20737384259700775,
      "learning_rate": 1.2073333333333333e-05,
      "loss": 0.0024,
      "step": 113780
    },
    {
      "epoch": 6.0688,
      "grad_norm": 0.06378877907991409,
      "learning_rate": 1.2070000000000001e-05,
      "loss": 0.0013,
      "step": 113790
    },
    {
      "epoch": 6.069333333333334,
      "grad_norm": 0.21921981871128082,
      "learning_rate": 1.2066666666666667e-05,
      "loss": 0.0019,
      "step": 113800
    },
    {
      "epoch": 6.069866666666667,
      "grad_norm": 0.3753975033760071,
      "learning_rate": 1.2063333333333334e-05,
      "loss": 0.0025,
      "step": 113810
    },
    {
      "epoch": 6.0704,
      "grad_norm": 0.1814781129360199,
      "learning_rate": 1.206e-05,
      "loss": 0.0014,
      "step": 113820
    },
    {
      "epoch": 6.0709333333333335,
      "grad_norm": 0.12289272993803024,
      "learning_rate": 1.2056666666666668e-05,
      "loss": 0.0022,
      "step": 113830
    },
    {
      "epoch": 6.071466666666667,
      "grad_norm": 0.1538383960723877,
      "learning_rate": 1.2053333333333334e-05,
      "loss": 0.0027,
      "step": 113840
    },
    {
      "epoch": 6.072,
      "grad_norm": 0.3674180805683136,
      "learning_rate": 1.205e-05,
      "loss": 0.0018,
      "step": 113850
    },
    {
      "epoch": 6.072533333333333,
      "grad_norm": 0.34922417998313904,
      "learning_rate": 1.2046666666666668e-05,
      "loss": 0.0021,
      "step": 113860
    },
    {
      "epoch": 6.073066666666667,
      "grad_norm": 0.10157852619886398,
      "learning_rate": 1.2043333333333334e-05,
      "loss": 0.0012,
      "step": 113870
    },
    {
      "epoch": 6.0736,
      "grad_norm": 0.23940861225128174,
      "learning_rate": 1.204e-05,
      "loss": 0.0016,
      "step": 113880
    },
    {
      "epoch": 6.074133333333333,
      "grad_norm": 0.06901729106903076,
      "learning_rate": 1.2036666666666668e-05,
      "loss": 0.0019,
      "step": 113890
    },
    {
      "epoch": 6.074666666666666,
      "grad_norm": 0.05337601155042648,
      "learning_rate": 1.2033333333333334e-05,
      "loss": 0.0022,
      "step": 113900
    },
    {
      "epoch": 6.0752,
      "grad_norm": 0.1668979972600937,
      "learning_rate": 1.2030000000000002e-05,
      "loss": 0.0016,
      "step": 113910
    },
    {
      "epoch": 6.075733333333333,
      "grad_norm": 0.17317800223827362,
      "learning_rate": 1.2026666666666666e-05,
      "loss": 0.0019,
      "step": 113920
    },
    {
      "epoch": 6.076266666666666,
      "grad_norm": 0.09098715335130692,
      "learning_rate": 1.2023333333333334e-05,
      "loss": 0.0018,
      "step": 113930
    },
    {
      "epoch": 6.0768,
      "grad_norm": 0.2709342837333679,
      "learning_rate": 1.202e-05,
      "loss": 0.0019,
      "step": 113940
    },
    {
      "epoch": 6.077333333333334,
      "grad_norm": 0.12207619100809097,
      "learning_rate": 1.2016666666666668e-05,
      "loss": 0.0025,
      "step": 113950
    },
    {
      "epoch": 6.077866666666667,
      "grad_norm": 0.16382257640361786,
      "learning_rate": 1.2013333333333334e-05,
      "loss": 0.0018,
      "step": 113960
    },
    {
      "epoch": 6.0784,
      "grad_norm": 0.12704572081565857,
      "learning_rate": 1.201e-05,
      "loss": 0.0017,
      "step": 113970
    },
    {
      "epoch": 6.0789333333333335,
      "grad_norm": 0.12906549870967865,
      "learning_rate": 1.2006666666666668e-05,
      "loss": 0.0019,
      "step": 113980
    },
    {
      "epoch": 6.079466666666667,
      "grad_norm": 0.2701438069343567,
      "learning_rate": 1.2003333333333334e-05,
      "loss": 0.0023,
      "step": 113990
    },
    {
      "epoch": 6.08,
      "grad_norm": 0.12368232011795044,
      "learning_rate": 1.2e-05,
      "loss": 0.0021,
      "step": 114000
    },
    {
      "epoch": 6.080533333333333,
      "grad_norm": 0.12416518479585648,
      "learning_rate": 1.1996666666666666e-05,
      "loss": 0.0013,
      "step": 114010
    },
    {
      "epoch": 6.081066666666667,
      "grad_norm": 0.2396668791770935,
      "learning_rate": 1.1993333333333334e-05,
      "loss": 0.002,
      "step": 114020
    },
    {
      "epoch": 6.0816,
      "grad_norm": 0.09277251362800598,
      "learning_rate": 1.199e-05,
      "loss": 0.0017,
      "step": 114030
    },
    {
      "epoch": 6.082133333333333,
      "grad_norm": 0.28180521726608276,
      "learning_rate": 1.1986666666666667e-05,
      "loss": 0.0022,
      "step": 114040
    },
    {
      "epoch": 6.082666666666666,
      "grad_norm": 0.12469913065433502,
      "learning_rate": 1.1983333333333334e-05,
      "loss": 0.002,
      "step": 114050
    },
    {
      "epoch": 6.0832,
      "grad_norm": 0.03973314166069031,
      "learning_rate": 1.198e-05,
      "loss": 0.0021,
      "step": 114060
    },
    {
      "epoch": 6.083733333333333,
      "grad_norm": 0.026804838329553604,
      "learning_rate": 1.1976666666666667e-05,
      "loss": 0.0015,
      "step": 114070
    },
    {
      "epoch": 6.084266666666666,
      "grad_norm": 0.11748802661895752,
      "learning_rate": 1.1973333333333334e-05,
      "loss": 0.0013,
      "step": 114080
    },
    {
      "epoch": 6.0848,
      "grad_norm": 0.4456839859485626,
      "learning_rate": 1.197e-05,
      "loss": 0.0022,
      "step": 114090
    },
    {
      "epoch": 6.085333333333334,
      "grad_norm": 0.5043550133705139,
      "learning_rate": 1.1966666666666668e-05,
      "loss": 0.0016,
      "step": 114100
    },
    {
      "epoch": 6.085866666666667,
      "grad_norm": 0.5916968584060669,
      "learning_rate": 1.1963333333333333e-05,
      "loss": 0.002,
      "step": 114110
    },
    {
      "epoch": 6.0864,
      "grad_norm": 0.2084810882806778,
      "learning_rate": 1.196e-05,
      "loss": 0.0023,
      "step": 114120
    },
    {
      "epoch": 6.0869333333333335,
      "grad_norm": 0.39629384875297546,
      "learning_rate": 1.1956666666666667e-05,
      "loss": 0.0018,
      "step": 114130
    },
    {
      "epoch": 6.087466666666667,
      "grad_norm": 0.21105743944644928,
      "learning_rate": 1.1953333333333335e-05,
      "loss": 0.0023,
      "step": 114140
    },
    {
      "epoch": 6.088,
      "grad_norm": 0.2894602119922638,
      "learning_rate": 1.195e-05,
      "loss": 0.0016,
      "step": 114150
    },
    {
      "epoch": 6.088533333333333,
      "grad_norm": 0.0406179316341877,
      "learning_rate": 1.1946666666666667e-05,
      "loss": 0.0018,
      "step": 114160
    },
    {
      "epoch": 6.089066666666667,
      "grad_norm": 0.046456459909677505,
      "learning_rate": 1.1943333333333335e-05,
      "loss": 0.0022,
      "step": 114170
    },
    {
      "epoch": 6.0896,
      "grad_norm": 0.2693626582622528,
      "learning_rate": 1.1940000000000001e-05,
      "loss": 0.0017,
      "step": 114180
    },
    {
      "epoch": 6.090133333333333,
      "grad_norm": 0.3310561180114746,
      "learning_rate": 1.1936666666666667e-05,
      "loss": 0.0023,
      "step": 114190
    },
    {
      "epoch": 6.0906666666666665,
      "grad_norm": 0.22128912806510925,
      "learning_rate": 1.1933333333333333e-05,
      "loss": 0.0021,
      "step": 114200
    },
    {
      "epoch": 6.0912,
      "grad_norm": 0.5457518100738525,
      "learning_rate": 1.1930000000000001e-05,
      "loss": 0.0013,
      "step": 114210
    },
    {
      "epoch": 6.091733333333333,
      "grad_norm": 0.19731517136096954,
      "learning_rate": 1.1926666666666667e-05,
      "loss": 0.0025,
      "step": 114220
    },
    {
      "epoch": 6.092266666666666,
      "grad_norm": 0.15023964643478394,
      "learning_rate": 1.1923333333333333e-05,
      "loss": 0.002,
      "step": 114230
    },
    {
      "epoch": 6.0928,
      "grad_norm": 0.3560504615306854,
      "learning_rate": 1.1920000000000001e-05,
      "loss": 0.0015,
      "step": 114240
    },
    {
      "epoch": 6.093333333333334,
      "grad_norm": 0.45466065406799316,
      "learning_rate": 1.1916666666666667e-05,
      "loss": 0.0017,
      "step": 114250
    },
    {
      "epoch": 6.093866666666667,
      "grad_norm": 0.33480337262153625,
      "learning_rate": 1.1913333333333333e-05,
      "loss": 0.0017,
      "step": 114260
    },
    {
      "epoch": 6.0944,
      "grad_norm": 0.2455798089504242,
      "learning_rate": 1.1910000000000001e-05,
      "loss": 0.0019,
      "step": 114270
    },
    {
      "epoch": 6.0949333333333335,
      "grad_norm": 0.14917728304862976,
      "learning_rate": 1.1906666666666667e-05,
      "loss": 0.002,
      "step": 114280
    },
    {
      "epoch": 6.095466666666667,
      "grad_norm": 0.21303147077560425,
      "learning_rate": 1.1903333333333335e-05,
      "loss": 0.0022,
      "step": 114290
    },
    {
      "epoch": 6.096,
      "grad_norm": 0.20432721078395844,
      "learning_rate": 1.19e-05,
      "loss": 0.0017,
      "step": 114300
    },
    {
      "epoch": 6.096533333333333,
      "grad_norm": 0.12016499787569046,
      "learning_rate": 1.1896666666666667e-05,
      "loss": 0.0019,
      "step": 114310
    },
    {
      "epoch": 6.097066666666667,
      "grad_norm": 0.19873444736003876,
      "learning_rate": 1.1893333333333334e-05,
      "loss": 0.0015,
      "step": 114320
    },
    {
      "epoch": 6.0976,
      "grad_norm": 0.24640633165836334,
      "learning_rate": 1.1890000000000001e-05,
      "loss": 0.0015,
      "step": 114330
    },
    {
      "epoch": 6.098133333333333,
      "grad_norm": 0.12454342842102051,
      "learning_rate": 1.1886666666666667e-05,
      "loss": 0.0013,
      "step": 114340
    },
    {
      "epoch": 6.0986666666666665,
      "grad_norm": 0.07377702742815018,
      "learning_rate": 1.1883333333333334e-05,
      "loss": 0.002,
      "step": 114350
    },
    {
      "epoch": 6.0992,
      "grad_norm": 0.11966543644666672,
      "learning_rate": 1.1880000000000001e-05,
      "loss": 0.0018,
      "step": 114360
    },
    {
      "epoch": 6.099733333333333,
      "grad_norm": 0.3600972890853882,
      "learning_rate": 1.1876666666666668e-05,
      "loss": 0.0015,
      "step": 114370
    },
    {
      "epoch": 6.100266666666666,
      "grad_norm": 0.020290432497859,
      "learning_rate": 1.1873333333333334e-05,
      "loss": 0.0021,
      "step": 114380
    },
    {
      "epoch": 6.1008,
      "grad_norm": 0.03569866344332695,
      "learning_rate": 1.187e-05,
      "loss": 0.0021,
      "step": 114390
    },
    {
      "epoch": 6.101333333333334,
      "grad_norm": 0.23992642760276794,
      "learning_rate": 1.1866666666666668e-05,
      "loss": 0.0014,
      "step": 114400
    },
    {
      "epoch": 6.101866666666667,
      "grad_norm": 0.2823409140110016,
      "learning_rate": 1.1863333333333334e-05,
      "loss": 0.0015,
      "step": 114410
    },
    {
      "epoch": 6.1024,
      "grad_norm": 0.20660163462162018,
      "learning_rate": 1.186e-05,
      "loss": 0.0018,
      "step": 114420
    },
    {
      "epoch": 6.1029333333333335,
      "grad_norm": 0.15641535818576813,
      "learning_rate": 1.1856666666666668e-05,
      "loss": 0.0016,
      "step": 114430
    },
    {
      "epoch": 6.103466666666667,
      "grad_norm": 0.23525740206241608,
      "learning_rate": 1.1853333333333334e-05,
      "loss": 0.0024,
      "step": 114440
    },
    {
      "epoch": 6.104,
      "grad_norm": 0.032838188111782074,
      "learning_rate": 1.185e-05,
      "loss": 0.0025,
      "step": 114450
    },
    {
      "epoch": 6.104533333333333,
      "grad_norm": 0.24926325678825378,
      "learning_rate": 1.1846666666666666e-05,
      "loss": 0.0018,
      "step": 114460
    },
    {
      "epoch": 6.105066666666667,
      "grad_norm": 0.07018526643514633,
      "learning_rate": 1.1843333333333334e-05,
      "loss": 0.0017,
      "step": 114470
    },
    {
      "epoch": 6.1056,
      "grad_norm": 0.11257864534854889,
      "learning_rate": 1.1840000000000002e-05,
      "loss": 0.0014,
      "step": 114480
    },
    {
      "epoch": 6.106133333333333,
      "grad_norm": 0.06176919490098953,
      "learning_rate": 1.1836666666666666e-05,
      "loss": 0.0016,
      "step": 114490
    },
    {
      "epoch": 6.1066666666666665,
      "grad_norm": 0.047482866793870926,
      "learning_rate": 1.1833333333333334e-05,
      "loss": 0.0018,
      "step": 114500
    },
    {
      "epoch": 6.1072,
      "grad_norm": 0.044395819306373596,
      "learning_rate": 1.183e-05,
      "loss": 0.0021,
      "step": 114510
    },
    {
      "epoch": 6.107733333333333,
      "grad_norm": 0.21100613474845886,
      "learning_rate": 1.1826666666666668e-05,
      "loss": 0.0014,
      "step": 114520
    },
    {
      "epoch": 6.108266666666666,
      "grad_norm": 0.18128705024719238,
      "learning_rate": 1.1823333333333334e-05,
      "loss": 0.0028,
      "step": 114530
    },
    {
      "epoch": 6.1088,
      "grad_norm": 0.08973090350627899,
      "learning_rate": 1.182e-05,
      "loss": 0.0016,
      "step": 114540
    },
    {
      "epoch": 6.109333333333334,
      "grad_norm": 0.16748739778995514,
      "learning_rate": 1.1816666666666668e-05,
      "loss": 0.0011,
      "step": 114550
    },
    {
      "epoch": 6.109866666666667,
      "grad_norm": 0.05810817703604698,
      "learning_rate": 1.1813333333333334e-05,
      "loss": 0.0015,
      "step": 114560
    },
    {
      "epoch": 6.1104,
      "grad_norm": 0.18673932552337646,
      "learning_rate": 1.181e-05,
      "loss": 0.0021,
      "step": 114570
    },
    {
      "epoch": 6.1109333333333336,
      "grad_norm": 0.3551497161388397,
      "learning_rate": 1.1806666666666667e-05,
      "loss": 0.0022,
      "step": 114580
    },
    {
      "epoch": 6.111466666666667,
      "grad_norm": 0.1555841565132141,
      "learning_rate": 1.1803333333333334e-05,
      "loss": 0.0018,
      "step": 114590
    },
    {
      "epoch": 6.112,
      "grad_norm": 0.09541413187980652,
      "learning_rate": 1.18e-05,
      "loss": 0.0019,
      "step": 114600
    },
    {
      "epoch": 6.112533333333333,
      "grad_norm": 0.065706267952919,
      "learning_rate": 1.1796666666666667e-05,
      "loss": 0.0018,
      "step": 114610
    },
    {
      "epoch": 6.113066666666667,
      "grad_norm": 0.1002885103225708,
      "learning_rate": 1.1793333333333334e-05,
      "loss": 0.0036,
      "step": 114620
    },
    {
      "epoch": 6.1136,
      "grad_norm": 0.044666264206171036,
      "learning_rate": 1.179e-05,
      "loss": 0.002,
      "step": 114630
    },
    {
      "epoch": 6.114133333333333,
      "grad_norm": 0.06037699431180954,
      "learning_rate": 1.1786666666666667e-05,
      "loss": 0.0023,
      "step": 114640
    },
    {
      "epoch": 6.1146666666666665,
      "grad_norm": 0.12275976687669754,
      "learning_rate": 1.1783333333333333e-05,
      "loss": 0.0017,
      "step": 114650
    },
    {
      "epoch": 6.1152,
      "grad_norm": 0.21412523090839386,
      "learning_rate": 1.178e-05,
      "loss": 0.0022,
      "step": 114660
    },
    {
      "epoch": 6.115733333333333,
      "grad_norm": 0.19304345548152924,
      "learning_rate": 1.1776666666666669e-05,
      "loss": 0.0017,
      "step": 114670
    },
    {
      "epoch": 6.116266666666666,
      "grad_norm": 0.18039563298225403,
      "learning_rate": 1.1773333333333333e-05,
      "loss": 0.0011,
      "step": 114680
    },
    {
      "epoch": 6.1168,
      "grad_norm": 0.12145266681909561,
      "learning_rate": 1.177e-05,
      "loss": 0.0023,
      "step": 114690
    },
    {
      "epoch": 6.117333333333334,
      "grad_norm": 0.2999080419540405,
      "learning_rate": 1.1766666666666667e-05,
      "loss": 0.0012,
      "step": 114700
    },
    {
      "epoch": 6.117866666666667,
      "grad_norm": 0.36284759640693665,
      "learning_rate": 1.1763333333333335e-05,
      "loss": 0.0019,
      "step": 114710
    },
    {
      "epoch": 6.1184,
      "grad_norm": 0.3572664260864258,
      "learning_rate": 1.1760000000000001e-05,
      "loss": 0.0028,
      "step": 114720
    },
    {
      "epoch": 6.118933333333334,
      "grad_norm": 0.3658667206764221,
      "learning_rate": 1.1756666666666667e-05,
      "loss": 0.0012,
      "step": 114730
    },
    {
      "epoch": 6.119466666666667,
      "grad_norm": 0.03824631869792938,
      "learning_rate": 1.1753333333333335e-05,
      "loss": 0.0014,
      "step": 114740
    },
    {
      "epoch": 6.12,
      "grad_norm": 0.07077348977327347,
      "learning_rate": 1.175e-05,
      "loss": 0.0012,
      "step": 114750
    },
    {
      "epoch": 6.120533333333333,
      "grad_norm": 0.08065373450517654,
      "learning_rate": 1.1746666666666667e-05,
      "loss": 0.0014,
      "step": 114760
    },
    {
      "epoch": 6.121066666666667,
      "grad_norm": 0.3875885307788849,
      "learning_rate": 1.1743333333333333e-05,
      "loss": 0.0024,
      "step": 114770
    },
    {
      "epoch": 6.1216,
      "grad_norm": 0.17625202238559723,
      "learning_rate": 1.1740000000000001e-05,
      "loss": 0.0016,
      "step": 114780
    },
    {
      "epoch": 6.122133333333333,
      "grad_norm": 0.12312496453523636,
      "learning_rate": 1.1736666666666667e-05,
      "loss": 0.0017,
      "step": 114790
    },
    {
      "epoch": 6.1226666666666665,
      "grad_norm": 0.0733485296368599,
      "learning_rate": 1.1733333333333333e-05,
      "loss": 0.0017,
      "step": 114800
    },
    {
      "epoch": 6.1232,
      "grad_norm": 0.07430334389209747,
      "learning_rate": 1.1730000000000001e-05,
      "loss": 0.0021,
      "step": 114810
    },
    {
      "epoch": 6.123733333333333,
      "grad_norm": 0.028375888243317604,
      "learning_rate": 1.1726666666666667e-05,
      "loss": 0.0017,
      "step": 114820
    },
    {
      "epoch": 6.124266666666666,
      "grad_norm": 0.3127792477607727,
      "learning_rate": 1.1723333333333333e-05,
      "loss": 0.0017,
      "step": 114830
    },
    {
      "epoch": 6.1248,
      "grad_norm": 0.3306635618209839,
      "learning_rate": 1.172e-05,
      "loss": 0.0018,
      "step": 114840
    },
    {
      "epoch": 6.125333333333334,
      "grad_norm": 0.34312403202056885,
      "learning_rate": 1.1716666666666667e-05,
      "loss": 0.0017,
      "step": 114850
    },
    {
      "epoch": 6.125866666666667,
      "grad_norm": 0.16869167983531952,
      "learning_rate": 1.1713333333333335e-05,
      "loss": 0.0016,
      "step": 114860
    },
    {
      "epoch": 6.1264,
      "grad_norm": 0.07165320962667465,
      "learning_rate": 1.171e-05,
      "loss": 0.0017,
      "step": 114870
    },
    {
      "epoch": 6.126933333333334,
      "grad_norm": 0.3515077829360962,
      "learning_rate": 1.1706666666666668e-05,
      "loss": 0.0014,
      "step": 114880
    },
    {
      "epoch": 6.127466666666667,
      "grad_norm": 0.03946994990110397,
      "learning_rate": 1.1703333333333334e-05,
      "loss": 0.0014,
      "step": 114890
    },
    {
      "epoch": 6.128,
      "grad_norm": 0.11108748614788055,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 0.0023,
      "step": 114900
    },
    {
      "epoch": 6.128533333333333,
      "grad_norm": 0.2698705196380615,
      "learning_rate": 1.1696666666666668e-05,
      "loss": 0.0013,
      "step": 114910
    },
    {
      "epoch": 6.129066666666667,
      "grad_norm": 0.09002145379781723,
      "learning_rate": 1.1693333333333334e-05,
      "loss": 0.0014,
      "step": 114920
    },
    {
      "epoch": 6.1296,
      "grad_norm": 0.19975124299526215,
      "learning_rate": 1.1690000000000002e-05,
      "loss": 0.0013,
      "step": 114930
    },
    {
      "epoch": 6.130133333333333,
      "grad_norm": 0.44199860095977783,
      "learning_rate": 1.1686666666666666e-05,
      "loss": 0.002,
      "step": 114940
    },
    {
      "epoch": 6.1306666666666665,
      "grad_norm": 0.35928651690483093,
      "learning_rate": 1.1683333333333334e-05,
      "loss": 0.0013,
      "step": 114950
    },
    {
      "epoch": 6.1312,
      "grad_norm": 0.12695834040641785,
      "learning_rate": 1.168e-05,
      "loss": 0.0014,
      "step": 114960
    },
    {
      "epoch": 6.131733333333333,
      "grad_norm": 0.4067336320877075,
      "learning_rate": 1.1676666666666668e-05,
      "loss": 0.0025,
      "step": 114970
    },
    {
      "epoch": 6.132266666666666,
      "grad_norm": 0.2060900777578354,
      "learning_rate": 1.1673333333333334e-05,
      "loss": 0.0017,
      "step": 114980
    },
    {
      "epoch": 6.1328,
      "grad_norm": 0.06584537774324417,
      "learning_rate": 1.167e-05,
      "loss": 0.0015,
      "step": 114990
    },
    {
      "epoch": 6.133333333333334,
      "grad_norm": 0.20524834096431732,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 0.0018,
      "step": 115000
    },
    {
      "epoch": 6.133866666666667,
      "grad_norm": 0.2087666392326355,
      "learning_rate": 1.1663333333333334e-05,
      "loss": 0.0017,
      "step": 115010
    },
    {
      "epoch": 6.1344,
      "grad_norm": 0.23714222013950348,
      "learning_rate": 1.166e-05,
      "loss": 0.0012,
      "step": 115020
    },
    {
      "epoch": 6.134933333333334,
      "grad_norm": 0.411208838224411,
      "learning_rate": 1.1656666666666666e-05,
      "loss": 0.0025,
      "step": 115030
    },
    {
      "epoch": 6.135466666666667,
      "grad_norm": 0.07756474614143372,
      "learning_rate": 1.1653333333333334e-05,
      "loss": 0.0016,
      "step": 115040
    },
    {
      "epoch": 6.136,
      "grad_norm": 0.06600534915924072,
      "learning_rate": 1.1650000000000002e-05,
      "loss": 0.0012,
      "step": 115050
    },
    {
      "epoch": 6.136533333333333,
      "grad_norm": 0.29316526651382446,
      "learning_rate": 1.1646666666666666e-05,
      "loss": 0.0018,
      "step": 115060
    },
    {
      "epoch": 6.137066666666667,
      "grad_norm": 0.20737415552139282,
      "learning_rate": 1.1643333333333334e-05,
      "loss": 0.0016,
      "step": 115070
    },
    {
      "epoch": 6.1376,
      "grad_norm": 0.3235016465187073,
      "learning_rate": 1.164e-05,
      "loss": 0.0014,
      "step": 115080
    },
    {
      "epoch": 6.138133333333333,
      "grad_norm": 0.08406607061624527,
      "learning_rate": 1.1636666666666666e-05,
      "loss": 0.0017,
      "step": 115090
    },
    {
      "epoch": 6.1386666666666665,
      "grad_norm": 0.1775110363960266,
      "learning_rate": 1.1633333333333334e-05,
      "loss": 0.0012,
      "step": 115100
    },
    {
      "epoch": 6.1392,
      "grad_norm": 0.35338377952575684,
      "learning_rate": 1.163e-05,
      "loss": 0.0014,
      "step": 115110
    },
    {
      "epoch": 6.139733333333333,
      "grad_norm": 0.17707674205303192,
      "learning_rate": 1.1626666666666668e-05,
      "loss": 0.0017,
      "step": 115120
    },
    {
      "epoch": 6.140266666666666,
      "grad_norm": 0.359647661447525,
      "learning_rate": 1.1623333333333333e-05,
      "loss": 0.0019,
      "step": 115130
    },
    {
      "epoch": 6.1408,
      "grad_norm": 0.09446590393781662,
      "learning_rate": 1.162e-05,
      "loss": 0.0017,
      "step": 115140
    },
    {
      "epoch": 6.141333333333334,
      "grad_norm": 0.20847974717617035,
      "learning_rate": 1.1616666666666667e-05,
      "loss": 0.0015,
      "step": 115150
    },
    {
      "epoch": 6.141866666666667,
      "grad_norm": 0.3038121163845062,
      "learning_rate": 1.1613333333333335e-05,
      "loss": 0.0019,
      "step": 115160
    },
    {
      "epoch": 6.1424,
      "grad_norm": 0.28561997413635254,
      "learning_rate": 1.161e-05,
      "loss": 0.0014,
      "step": 115170
    },
    {
      "epoch": 6.142933333333334,
      "grad_norm": 0.08058039844036102,
      "learning_rate": 1.1606666666666667e-05,
      "loss": 0.0014,
      "step": 115180
    },
    {
      "epoch": 6.143466666666667,
      "grad_norm": 0.29444974660873413,
      "learning_rate": 1.1603333333333335e-05,
      "loss": 0.0021,
      "step": 115190
    },
    {
      "epoch": 6.144,
      "grad_norm": 0.21264362335205078,
      "learning_rate": 1.16e-05,
      "loss": 0.003,
      "step": 115200
    },
    {
      "epoch": 6.144533333333333,
      "grad_norm": 0.1317192018032074,
      "learning_rate": 1.1596666666666667e-05,
      "loss": 0.0016,
      "step": 115210
    },
    {
      "epoch": 6.145066666666667,
      "grad_norm": 0.05854841321706772,
      "learning_rate": 1.1593333333333333e-05,
      "loss": 0.0013,
      "step": 115220
    },
    {
      "epoch": 6.1456,
      "grad_norm": 0.03548283129930496,
      "learning_rate": 1.159e-05,
      "loss": 0.0013,
      "step": 115230
    },
    {
      "epoch": 6.146133333333333,
      "grad_norm": 0.20883621275424957,
      "learning_rate": 1.1586666666666669e-05,
      "loss": 0.0021,
      "step": 115240
    },
    {
      "epoch": 6.1466666666666665,
      "grad_norm": 0.10191681236028671,
      "learning_rate": 1.1583333333333333e-05,
      "loss": 0.0018,
      "step": 115250
    },
    {
      "epoch": 6.1472,
      "grad_norm": 0.037384890019893646,
      "learning_rate": 1.1580000000000001e-05,
      "loss": 0.0017,
      "step": 115260
    },
    {
      "epoch": 6.147733333333333,
      "grad_norm": 0.23213878273963928,
      "learning_rate": 1.1576666666666667e-05,
      "loss": 0.0019,
      "step": 115270
    },
    {
      "epoch": 6.148266666666666,
      "grad_norm": 0.21476905047893524,
      "learning_rate": 1.1573333333333333e-05,
      "loss": 0.0017,
      "step": 115280
    },
    {
      "epoch": 6.1488,
      "grad_norm": 0.0382392480969429,
      "learning_rate": 1.1570000000000001e-05,
      "loss": 0.0018,
      "step": 115290
    },
    {
      "epoch": 6.149333333333334,
      "grad_norm": 0.2642311155796051,
      "learning_rate": 1.1566666666666667e-05,
      "loss": 0.003,
      "step": 115300
    },
    {
      "epoch": 6.149866666666667,
      "grad_norm": 0.030376970767974854,
      "learning_rate": 1.1563333333333335e-05,
      "loss": 0.0024,
      "step": 115310
    },
    {
      "epoch": 6.1504,
      "grad_norm": 0.23747055232524872,
      "learning_rate": 1.156e-05,
      "loss": 0.0024,
      "step": 115320
    },
    {
      "epoch": 6.150933333333334,
      "grad_norm": 0.26450255513191223,
      "learning_rate": 1.1556666666666667e-05,
      "loss": 0.0026,
      "step": 115330
    },
    {
      "epoch": 6.151466666666667,
      "grad_norm": 0.18494249880313873,
      "learning_rate": 1.1553333333333333e-05,
      "loss": 0.0017,
      "step": 115340
    },
    {
      "epoch": 6.152,
      "grad_norm": 0.09486929327249527,
      "learning_rate": 1.1550000000000001e-05,
      "loss": 0.0018,
      "step": 115350
    },
    {
      "epoch": 6.152533333333333,
      "grad_norm": 0.11272846907377243,
      "learning_rate": 1.1546666666666667e-05,
      "loss": 0.0019,
      "step": 115360
    },
    {
      "epoch": 6.153066666666667,
      "grad_norm": 0.3897002637386322,
      "learning_rate": 1.1543333333333333e-05,
      "loss": 0.0017,
      "step": 115370
    },
    {
      "epoch": 6.1536,
      "grad_norm": 0.2844242453575134,
      "learning_rate": 1.1540000000000001e-05,
      "loss": 0.0015,
      "step": 115380
    },
    {
      "epoch": 6.154133333333333,
      "grad_norm": 0.21272782981395721,
      "learning_rate": 1.1536666666666667e-05,
      "loss": 0.0014,
      "step": 115390
    },
    {
      "epoch": 6.1546666666666665,
      "grad_norm": 0.1247672438621521,
      "learning_rate": 1.1533333333333334e-05,
      "loss": 0.0012,
      "step": 115400
    },
    {
      "epoch": 6.1552,
      "grad_norm": 0.2069273293018341,
      "learning_rate": 1.153e-05,
      "loss": 0.0024,
      "step": 115410
    },
    {
      "epoch": 6.155733333333333,
      "grad_norm": 0.1161147952079773,
      "learning_rate": 1.1526666666666668e-05,
      "loss": 0.0016,
      "step": 115420
    },
    {
      "epoch": 6.156266666666666,
      "grad_norm": 0.27603697776794434,
      "learning_rate": 1.1523333333333334e-05,
      "loss": 0.0019,
      "step": 115430
    },
    {
      "epoch": 6.1568,
      "grad_norm": 0.20721712708473206,
      "learning_rate": 1.152e-05,
      "loss": 0.0018,
      "step": 115440
    },
    {
      "epoch": 6.157333333333334,
      "grad_norm": 0.24365662038326263,
      "learning_rate": 1.1516666666666668e-05,
      "loss": 0.0017,
      "step": 115450
    },
    {
      "epoch": 6.157866666666667,
      "grad_norm": 0.5511999726295471,
      "learning_rate": 1.1513333333333334e-05,
      "loss": 0.0024,
      "step": 115460
    },
    {
      "epoch": 6.1584,
      "grad_norm": 0.29649171233177185,
      "learning_rate": 1.151e-05,
      "loss": 0.0019,
      "step": 115470
    },
    {
      "epoch": 6.158933333333334,
      "grad_norm": 0.213149756193161,
      "learning_rate": 1.1506666666666668e-05,
      "loss": 0.0012,
      "step": 115480
    },
    {
      "epoch": 6.159466666666667,
      "grad_norm": 0.23926164209842682,
      "learning_rate": 1.1503333333333334e-05,
      "loss": 0.0014,
      "step": 115490
    },
    {
      "epoch": 6.16,
      "grad_norm": 0.08269444853067398,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 0.0014,
      "step": 115500
    },
    {
      "epoch": 6.160533333333333,
      "grad_norm": 0.13027125597000122,
      "learning_rate": 1.1496666666666666e-05,
      "loss": 0.0015,
      "step": 115510
    },
    {
      "epoch": 6.161066666666667,
      "grad_norm": 0.12852513790130615,
      "learning_rate": 1.1493333333333334e-05,
      "loss": 0.0013,
      "step": 115520
    },
    {
      "epoch": 6.1616,
      "grad_norm": 0.22290563583374023,
      "learning_rate": 1.149e-05,
      "loss": 0.0016,
      "step": 115530
    },
    {
      "epoch": 6.162133333333333,
      "grad_norm": 0.13608016073703766,
      "learning_rate": 1.1486666666666668e-05,
      "loss": 0.0013,
      "step": 115540
    },
    {
      "epoch": 6.1626666666666665,
      "grad_norm": 0.26333165168762207,
      "learning_rate": 1.1483333333333334e-05,
      "loss": 0.0018,
      "step": 115550
    },
    {
      "epoch": 6.1632,
      "grad_norm": 0.18773044645786285,
      "learning_rate": 1.148e-05,
      "loss": 0.0016,
      "step": 115560
    },
    {
      "epoch": 6.163733333333333,
      "grad_norm": 0.537642240524292,
      "learning_rate": 1.1476666666666668e-05,
      "loss": 0.0017,
      "step": 115570
    },
    {
      "epoch": 6.164266666666666,
      "grad_norm": 0.2983132004737854,
      "learning_rate": 1.1473333333333334e-05,
      "loss": 0.0014,
      "step": 115580
    },
    {
      "epoch": 6.1648,
      "grad_norm": 0.18395614624023438,
      "learning_rate": 1.147e-05,
      "loss": 0.0019,
      "step": 115590
    },
    {
      "epoch": 6.165333333333333,
      "grad_norm": 0.11774981021881104,
      "learning_rate": 1.1466666666666666e-05,
      "loss": 0.0021,
      "step": 115600
    },
    {
      "epoch": 6.165866666666667,
      "grad_norm": 0.3287437856197357,
      "learning_rate": 1.1463333333333334e-05,
      "loss": 0.0023,
      "step": 115610
    },
    {
      "epoch": 6.1664,
      "grad_norm": 0.029469558969140053,
      "learning_rate": 1.146e-05,
      "loss": 0.0018,
      "step": 115620
    },
    {
      "epoch": 6.166933333333334,
      "grad_norm": 0.23987168073654175,
      "learning_rate": 1.1456666666666667e-05,
      "loss": 0.0019,
      "step": 115630
    },
    {
      "epoch": 6.167466666666667,
      "grad_norm": 0.09081012010574341,
      "learning_rate": 1.1453333333333334e-05,
      "loss": 0.0025,
      "step": 115640
    },
    {
      "epoch": 6.168,
      "grad_norm": 0.0958058163523674,
      "learning_rate": 1.145e-05,
      "loss": 0.0018,
      "step": 115650
    },
    {
      "epoch": 6.168533333333333,
      "grad_norm": 0.06948204338550568,
      "learning_rate": 1.1446666666666667e-05,
      "loss": 0.0018,
      "step": 115660
    },
    {
      "epoch": 6.169066666666667,
      "grad_norm": 0.1287146955728531,
      "learning_rate": 1.1443333333333334e-05,
      "loss": 0.0017,
      "step": 115670
    },
    {
      "epoch": 6.1696,
      "grad_norm": 0.06231064721941948,
      "learning_rate": 1.144e-05,
      "loss": 0.0013,
      "step": 115680
    },
    {
      "epoch": 6.170133333333333,
      "grad_norm": 0.023821881040930748,
      "learning_rate": 1.1436666666666668e-05,
      "loss": 0.0026,
      "step": 115690
    },
    {
      "epoch": 6.1706666666666665,
      "grad_norm": 0.4439353942871094,
      "learning_rate": 1.1433333333333333e-05,
      "loss": 0.0012,
      "step": 115700
    },
    {
      "epoch": 6.1712,
      "grad_norm": 0.30820944905281067,
      "learning_rate": 1.143e-05,
      "loss": 0.0024,
      "step": 115710
    },
    {
      "epoch": 6.171733333333333,
      "grad_norm": 0.08950762450695038,
      "learning_rate": 1.1426666666666667e-05,
      "loss": 0.0013,
      "step": 115720
    },
    {
      "epoch": 6.172266666666666,
      "grad_norm": 0.09209150075912476,
      "learning_rate": 1.1423333333333335e-05,
      "loss": 0.0017,
      "step": 115730
    },
    {
      "epoch": 6.1728,
      "grad_norm": 0.09400217235088348,
      "learning_rate": 1.142e-05,
      "loss": 0.0017,
      "step": 115740
    },
    {
      "epoch": 6.173333333333334,
      "grad_norm": 0.033936433494091034,
      "learning_rate": 1.1416666666666667e-05,
      "loss": 0.0027,
      "step": 115750
    },
    {
      "epoch": 6.173866666666667,
      "grad_norm": 0.06373994052410126,
      "learning_rate": 1.1413333333333335e-05,
      "loss": 0.0019,
      "step": 115760
    },
    {
      "epoch": 6.1744,
      "grad_norm": 0.31001099944114685,
      "learning_rate": 1.141e-05,
      "loss": 0.0017,
      "step": 115770
    },
    {
      "epoch": 6.174933333333334,
      "grad_norm": 0.2146133929491043,
      "learning_rate": 1.1406666666666667e-05,
      "loss": 0.0017,
      "step": 115780
    },
    {
      "epoch": 6.175466666666667,
      "grad_norm": 0.04808666557073593,
      "learning_rate": 1.1403333333333333e-05,
      "loss": 0.0014,
      "step": 115790
    },
    {
      "epoch": 6.176,
      "grad_norm": 0.03640293702483177,
      "learning_rate": 1.1400000000000001e-05,
      "loss": 0.0022,
      "step": 115800
    },
    {
      "epoch": 6.176533333333333,
      "grad_norm": 0.07773078233003616,
      "learning_rate": 1.1396666666666667e-05,
      "loss": 0.0013,
      "step": 115810
    },
    {
      "epoch": 6.177066666666667,
      "grad_norm": 0.03469749167561531,
      "learning_rate": 1.1393333333333333e-05,
      "loss": 0.0013,
      "step": 115820
    },
    {
      "epoch": 6.1776,
      "grad_norm": 0.474221408367157,
      "learning_rate": 1.1390000000000001e-05,
      "loss": 0.0027,
      "step": 115830
    },
    {
      "epoch": 6.178133333333333,
      "grad_norm": 0.3424077332019806,
      "learning_rate": 1.1386666666666667e-05,
      "loss": 0.0015,
      "step": 115840
    },
    {
      "epoch": 6.1786666666666665,
      "grad_norm": 0.13190028071403503,
      "learning_rate": 1.1383333333333333e-05,
      "loss": 0.0019,
      "step": 115850
    },
    {
      "epoch": 6.1792,
      "grad_norm": 0.12315529584884644,
      "learning_rate": 1.1380000000000001e-05,
      "loss": 0.0017,
      "step": 115860
    },
    {
      "epoch": 6.179733333333333,
      "grad_norm": 0.11823948472738266,
      "learning_rate": 1.1376666666666667e-05,
      "loss": 0.0016,
      "step": 115870
    },
    {
      "epoch": 6.180266666666666,
      "grad_norm": 0.09653114527463913,
      "learning_rate": 1.1373333333333335e-05,
      "loss": 0.0035,
      "step": 115880
    },
    {
      "epoch": 6.1808,
      "grad_norm": 0.20575575530529022,
      "learning_rate": 1.137e-05,
      "loss": 0.0018,
      "step": 115890
    },
    {
      "epoch": 6.181333333333333,
      "grad_norm": 0.21264255046844482,
      "learning_rate": 1.1366666666666667e-05,
      "loss": 0.0019,
      "step": 115900
    },
    {
      "epoch": 6.181866666666667,
      "grad_norm": 0.1225026398897171,
      "learning_rate": 1.1363333333333334e-05,
      "loss": 0.0015,
      "step": 115910
    },
    {
      "epoch": 6.1824,
      "grad_norm": 0.03753318265080452,
      "learning_rate": 1.1360000000000001e-05,
      "loss": 0.0021,
      "step": 115920
    },
    {
      "epoch": 6.182933333333334,
      "grad_norm": 0.3386159837245941,
      "learning_rate": 1.1356666666666667e-05,
      "loss": 0.0022,
      "step": 115930
    },
    {
      "epoch": 6.183466666666667,
      "grad_norm": 0.15129078924655914,
      "learning_rate": 1.1353333333333334e-05,
      "loss": 0.0013,
      "step": 115940
    },
    {
      "epoch": 6.184,
      "grad_norm": 0.1222577840089798,
      "learning_rate": 1.1350000000000001e-05,
      "loss": 0.0013,
      "step": 115950
    },
    {
      "epoch": 6.184533333333333,
      "grad_norm": 0.30784720182418823,
      "learning_rate": 1.1346666666666666e-05,
      "loss": 0.0014,
      "step": 115960
    },
    {
      "epoch": 6.185066666666667,
      "grad_norm": 0.24107655882835388,
      "learning_rate": 1.1343333333333334e-05,
      "loss": 0.0013,
      "step": 115970
    },
    {
      "epoch": 6.1856,
      "grad_norm": 0.11317895352840424,
      "learning_rate": 1.134e-05,
      "loss": 0.0012,
      "step": 115980
    },
    {
      "epoch": 6.186133333333333,
      "grad_norm": 0.08593247085809708,
      "learning_rate": 1.1336666666666668e-05,
      "loss": 0.0015,
      "step": 115990
    },
    {
      "epoch": 6.1866666666666665,
      "grad_norm": 0.36778315901756287,
      "learning_rate": 1.1333333333333334e-05,
      "loss": 0.0021,
      "step": 116000
    },
    {
      "epoch": 6.1872,
      "grad_norm": 0.3047173321247101,
      "learning_rate": 1.133e-05,
      "loss": 0.0023,
      "step": 116010
    },
    {
      "epoch": 6.187733333333333,
      "grad_norm": 0.15109692513942719,
      "learning_rate": 1.1326666666666668e-05,
      "loss": 0.0022,
      "step": 116020
    },
    {
      "epoch": 6.188266666666666,
      "grad_norm": 0.09194373339414597,
      "learning_rate": 1.1323333333333334e-05,
      "loss": 0.0017,
      "step": 116030
    },
    {
      "epoch": 6.1888,
      "grad_norm": 0.19561035931110382,
      "learning_rate": 1.132e-05,
      "loss": 0.0017,
      "step": 116040
    },
    {
      "epoch": 6.189333333333333,
      "grad_norm": 0.41694560647010803,
      "learning_rate": 1.1316666666666668e-05,
      "loss": 0.0013,
      "step": 116050
    },
    {
      "epoch": 6.189866666666667,
      "grad_norm": 0.2945539057254791,
      "learning_rate": 1.1313333333333334e-05,
      "loss": 0.0014,
      "step": 116060
    },
    {
      "epoch": 6.1904,
      "grad_norm": 0.07074835151433945,
      "learning_rate": 1.1310000000000002e-05,
      "loss": 0.002,
      "step": 116070
    },
    {
      "epoch": 6.190933333333334,
      "grad_norm": 0.030856262892484665,
      "learning_rate": 1.1306666666666666e-05,
      "loss": 0.0021,
      "step": 116080
    },
    {
      "epoch": 6.191466666666667,
      "grad_norm": 0.1811385601758957,
      "learning_rate": 1.1303333333333334e-05,
      "loss": 0.0025,
      "step": 116090
    },
    {
      "epoch": 6.192,
      "grad_norm": 0.14631971716880798,
      "learning_rate": 1.13e-05,
      "loss": 0.0023,
      "step": 116100
    },
    {
      "epoch": 6.1925333333333334,
      "grad_norm": 0.09561244398355484,
      "learning_rate": 1.1296666666666668e-05,
      "loss": 0.0017,
      "step": 116110
    },
    {
      "epoch": 6.193066666666667,
      "grad_norm": 0.03148529306054115,
      "learning_rate": 1.1293333333333334e-05,
      "loss": 0.0016,
      "step": 116120
    },
    {
      "epoch": 6.1936,
      "grad_norm": 0.2326638400554657,
      "learning_rate": 1.129e-05,
      "loss": 0.0027,
      "step": 116130
    },
    {
      "epoch": 6.194133333333333,
      "grad_norm": 0.33803480863571167,
      "learning_rate": 1.1286666666666668e-05,
      "loss": 0.0017,
      "step": 116140
    },
    {
      "epoch": 6.1946666666666665,
      "grad_norm": 0.45365414023399353,
      "learning_rate": 1.1283333333333333e-05,
      "loss": 0.0013,
      "step": 116150
    },
    {
      "epoch": 6.1952,
      "grad_norm": 0.16857652366161346,
      "learning_rate": 1.128e-05,
      "loss": 0.0028,
      "step": 116160
    },
    {
      "epoch": 6.195733333333333,
      "grad_norm": 0.68406742811203,
      "learning_rate": 1.1276666666666667e-05,
      "loss": 0.0022,
      "step": 116170
    },
    {
      "epoch": 6.196266666666666,
      "grad_norm": 0.06722115725278854,
      "learning_rate": 1.1273333333333334e-05,
      "loss": 0.0017,
      "step": 116180
    },
    {
      "epoch": 6.1968,
      "grad_norm": 0.10233685374259949,
      "learning_rate": 1.127e-05,
      "loss": 0.0016,
      "step": 116190
    },
    {
      "epoch": 6.197333333333333,
      "grad_norm": 0.18244294822216034,
      "learning_rate": 1.1266666666666667e-05,
      "loss": 0.0016,
      "step": 116200
    },
    {
      "epoch": 6.197866666666667,
      "grad_norm": 0.08968669921159744,
      "learning_rate": 1.1263333333333334e-05,
      "loss": 0.002,
      "step": 116210
    },
    {
      "epoch": 6.1984,
      "grad_norm": 0.08953520655632019,
      "learning_rate": 1.126e-05,
      "loss": 0.002,
      "step": 116220
    },
    {
      "epoch": 6.198933333333334,
      "grad_norm": 0.4555511772632599,
      "learning_rate": 1.1256666666666667e-05,
      "loss": 0.002,
      "step": 116230
    },
    {
      "epoch": 6.199466666666667,
      "grad_norm": 0.2672077715396881,
      "learning_rate": 1.1253333333333335e-05,
      "loss": 0.0016,
      "step": 116240
    },
    {
      "epoch": 6.2,
      "grad_norm": 0.2644859552383423,
      "learning_rate": 1.125e-05,
      "loss": 0.0012,
      "step": 116250
    },
    {
      "epoch": 6.2005333333333335,
      "grad_norm": 0.15966570377349854,
      "learning_rate": 1.1246666666666669e-05,
      "loss": 0.0021,
      "step": 116260
    },
    {
      "epoch": 6.201066666666667,
      "grad_norm": 0.17877095937728882,
      "learning_rate": 1.1243333333333333e-05,
      "loss": 0.0012,
      "step": 116270
    },
    {
      "epoch": 6.2016,
      "grad_norm": 0.6031163930892944,
      "learning_rate": 1.124e-05,
      "loss": 0.0023,
      "step": 116280
    },
    {
      "epoch": 6.202133333333333,
      "grad_norm": 0.07861289381980896,
      "learning_rate": 1.1236666666666667e-05,
      "loss": 0.0021,
      "step": 116290
    },
    {
      "epoch": 6.2026666666666666,
      "grad_norm": 0.35978665947914124,
      "learning_rate": 1.1233333333333333e-05,
      "loss": 0.0021,
      "step": 116300
    },
    {
      "epoch": 6.2032,
      "grad_norm": 0.1823141872882843,
      "learning_rate": 1.1230000000000001e-05,
      "loss": 0.0018,
      "step": 116310
    },
    {
      "epoch": 6.203733333333333,
      "grad_norm": 0.4281955063343048,
      "learning_rate": 1.1226666666666667e-05,
      "loss": 0.0029,
      "step": 116320
    },
    {
      "epoch": 6.204266666666666,
      "grad_norm": 0.0551074743270874,
      "learning_rate": 1.1223333333333335e-05,
      "loss": 0.0022,
      "step": 116330
    },
    {
      "epoch": 6.2048,
      "grad_norm": 0.23924192786216736,
      "learning_rate": 1.122e-05,
      "loss": 0.0022,
      "step": 116340
    },
    {
      "epoch": 6.205333333333333,
      "grad_norm": 0.07823475450277328,
      "learning_rate": 1.1216666666666667e-05,
      "loss": 0.002,
      "step": 116350
    },
    {
      "epoch": 6.205866666666667,
      "grad_norm": 0.21690601110458374,
      "learning_rate": 1.1213333333333333e-05,
      "loss": 0.0021,
      "step": 116360
    },
    {
      "epoch": 6.2064,
      "grad_norm": 0.06475265324115753,
      "learning_rate": 1.1210000000000001e-05,
      "loss": 0.0014,
      "step": 116370
    },
    {
      "epoch": 6.206933333333334,
      "grad_norm": 0.07172925770282745,
      "learning_rate": 1.1206666666666667e-05,
      "loss": 0.002,
      "step": 116380
    },
    {
      "epoch": 6.207466666666667,
      "grad_norm": 0.09916327148675919,
      "learning_rate": 1.1203333333333333e-05,
      "loss": 0.0017,
      "step": 116390
    },
    {
      "epoch": 6.208,
      "grad_norm": 0.1162908598780632,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 0.002,
      "step": 116400
    },
    {
      "epoch": 6.2085333333333335,
      "grad_norm": 0.09161151945590973,
      "learning_rate": 1.1196666666666667e-05,
      "loss": 0.0016,
      "step": 116410
    },
    {
      "epoch": 6.209066666666667,
      "grad_norm": 0.06771419942378998,
      "learning_rate": 1.1193333333333333e-05,
      "loss": 0.0013,
      "step": 116420
    },
    {
      "epoch": 6.2096,
      "grad_norm": 0.0915447250008583,
      "learning_rate": 1.1190000000000001e-05,
      "loss": 0.0014,
      "step": 116430
    },
    {
      "epoch": 6.210133333333333,
      "grad_norm": 0.07377183437347412,
      "learning_rate": 1.1186666666666667e-05,
      "loss": 0.0018,
      "step": 116440
    },
    {
      "epoch": 6.210666666666667,
      "grad_norm": 0.27284032106399536,
      "learning_rate": 1.1183333333333335e-05,
      "loss": 0.0019,
      "step": 116450
    },
    {
      "epoch": 6.2112,
      "grad_norm": 0.21490052342414856,
      "learning_rate": 1.118e-05,
      "loss": 0.0014,
      "step": 116460
    },
    {
      "epoch": 6.211733333333333,
      "grad_norm": 0.12909391522407532,
      "learning_rate": 1.1176666666666668e-05,
      "loss": 0.0016,
      "step": 116470
    },
    {
      "epoch": 6.212266666666666,
      "grad_norm": 0.24060215055942535,
      "learning_rate": 1.1173333333333334e-05,
      "loss": 0.0019,
      "step": 116480
    },
    {
      "epoch": 6.2128,
      "grad_norm": 0.23643861711025238,
      "learning_rate": 1.117e-05,
      "loss": 0.0022,
      "step": 116490
    },
    {
      "epoch": 6.213333333333333,
      "grad_norm": 0.3533320426940918,
      "learning_rate": 1.1166666666666668e-05,
      "loss": 0.0026,
      "step": 116500
    },
    {
      "epoch": 6.213866666666667,
      "grad_norm": 0.06725580245256424,
      "learning_rate": 1.1163333333333334e-05,
      "loss": 0.0014,
      "step": 116510
    },
    {
      "epoch": 6.2144,
      "grad_norm": 0.09513240307569504,
      "learning_rate": 1.1160000000000002e-05,
      "loss": 0.0025,
      "step": 116520
    },
    {
      "epoch": 6.214933333333334,
      "grad_norm": 0.19022880494594574,
      "learning_rate": 1.1156666666666666e-05,
      "loss": 0.0017,
      "step": 116530
    },
    {
      "epoch": 6.215466666666667,
      "grad_norm": 0.3257993757724762,
      "learning_rate": 1.1153333333333334e-05,
      "loss": 0.0015,
      "step": 116540
    },
    {
      "epoch": 6.216,
      "grad_norm": 0.6645559668540955,
      "learning_rate": 1.115e-05,
      "loss": 0.0024,
      "step": 116550
    },
    {
      "epoch": 6.2165333333333335,
      "grad_norm": 0.10055883973836899,
      "learning_rate": 1.1146666666666668e-05,
      "loss": 0.0016,
      "step": 116560
    },
    {
      "epoch": 6.217066666666667,
      "grad_norm": 0.042340751737356186,
      "learning_rate": 1.1143333333333334e-05,
      "loss": 0.0019,
      "step": 116570
    },
    {
      "epoch": 6.2176,
      "grad_norm": 0.1464833766222,
      "learning_rate": 1.114e-05,
      "loss": 0.0013,
      "step": 116580
    },
    {
      "epoch": 6.218133333333333,
      "grad_norm": 0.38294702768325806,
      "learning_rate": 1.1136666666666668e-05,
      "loss": 0.0015,
      "step": 116590
    },
    {
      "epoch": 6.218666666666667,
      "grad_norm": 0.4489426612854004,
      "learning_rate": 1.1133333333333334e-05,
      "loss": 0.0015,
      "step": 116600
    },
    {
      "epoch": 6.2192,
      "grad_norm": 0.1660008579492569,
      "learning_rate": 1.113e-05,
      "loss": 0.0012,
      "step": 116610
    },
    {
      "epoch": 6.219733333333333,
      "grad_norm": 1.0024504661560059,
      "learning_rate": 1.1126666666666668e-05,
      "loss": 0.0018,
      "step": 116620
    },
    {
      "epoch": 6.220266666666666,
      "grad_norm": 0.15159818530082703,
      "learning_rate": 1.1123333333333334e-05,
      "loss": 0.0018,
      "step": 116630
    },
    {
      "epoch": 6.2208,
      "grad_norm": 0.17402055859565735,
      "learning_rate": 1.112e-05,
      "loss": 0.0022,
      "step": 116640
    },
    {
      "epoch": 6.221333333333333,
      "grad_norm": 0.10546091198921204,
      "learning_rate": 1.1116666666666666e-05,
      "loss": 0.0019,
      "step": 116650
    },
    {
      "epoch": 6.221866666666667,
      "grad_norm": 0.4120717942714691,
      "learning_rate": 1.1113333333333334e-05,
      "loss": 0.0023,
      "step": 116660
    },
    {
      "epoch": 6.2224,
      "grad_norm": 0.06125279515981674,
      "learning_rate": 1.111e-05,
      "loss": 0.002,
      "step": 116670
    },
    {
      "epoch": 6.222933333333334,
      "grad_norm": 0.27227500081062317,
      "learning_rate": 1.1106666666666666e-05,
      "loss": 0.0025,
      "step": 116680
    },
    {
      "epoch": 6.223466666666667,
      "grad_norm": 0.09405988454818726,
      "learning_rate": 1.1103333333333334e-05,
      "loss": 0.0013,
      "step": 116690
    },
    {
      "epoch": 6.224,
      "grad_norm": 0.36007028818130493,
      "learning_rate": 1.11e-05,
      "loss": 0.0016,
      "step": 116700
    },
    {
      "epoch": 6.2245333333333335,
      "grad_norm": 0.5691099762916565,
      "learning_rate": 1.1096666666666668e-05,
      "loss": 0.0022,
      "step": 116710
    },
    {
      "epoch": 6.225066666666667,
      "grad_norm": 0.09095659852027893,
      "learning_rate": 1.1093333333333333e-05,
      "loss": 0.0031,
      "step": 116720
    },
    {
      "epoch": 6.2256,
      "grad_norm": 0.09978783875703812,
      "learning_rate": 1.109e-05,
      "loss": 0.0028,
      "step": 116730
    },
    {
      "epoch": 6.226133333333333,
      "grad_norm": 0.17581519484519958,
      "learning_rate": 1.1086666666666667e-05,
      "loss": 0.0027,
      "step": 116740
    },
    {
      "epoch": 6.226666666666667,
      "grad_norm": 0.1799841821193695,
      "learning_rate": 1.1083333333333335e-05,
      "loss": 0.0022,
      "step": 116750
    },
    {
      "epoch": 6.2272,
      "grad_norm": 0.33903390169143677,
      "learning_rate": 1.108e-05,
      "loss": 0.0027,
      "step": 116760
    },
    {
      "epoch": 6.227733333333333,
      "grad_norm": 0.45249465107917786,
      "learning_rate": 1.1076666666666667e-05,
      "loss": 0.0019,
      "step": 116770
    },
    {
      "epoch": 6.228266666666666,
      "grad_norm": 0.19764915108680725,
      "learning_rate": 1.1073333333333335e-05,
      "loss": 0.0018,
      "step": 116780
    },
    {
      "epoch": 6.2288,
      "grad_norm": 0.40549570322036743,
      "learning_rate": 1.107e-05,
      "loss": 0.0013,
      "step": 116790
    },
    {
      "epoch": 6.229333333333333,
      "grad_norm": 0.0486220121383667,
      "learning_rate": 1.1066666666666667e-05,
      "loss": 0.0016,
      "step": 116800
    },
    {
      "epoch": 6.229866666666666,
      "grad_norm": 0.305191308259964,
      "learning_rate": 1.1063333333333335e-05,
      "loss": 0.0013,
      "step": 116810
    },
    {
      "epoch": 6.2304,
      "grad_norm": 0.21491533517837524,
      "learning_rate": 1.106e-05,
      "loss": 0.0017,
      "step": 116820
    },
    {
      "epoch": 6.230933333333334,
      "grad_norm": 0.23781636357307434,
      "learning_rate": 1.1056666666666667e-05,
      "loss": 0.0017,
      "step": 116830
    },
    {
      "epoch": 6.231466666666667,
      "grad_norm": 0.39239373803138733,
      "learning_rate": 1.1053333333333333e-05,
      "loss": 0.0033,
      "step": 116840
    },
    {
      "epoch": 6.232,
      "grad_norm": 0.19913458824157715,
      "learning_rate": 1.1050000000000001e-05,
      "loss": 0.0015,
      "step": 116850
    },
    {
      "epoch": 6.2325333333333335,
      "grad_norm": 0.17863397300243378,
      "learning_rate": 1.1046666666666667e-05,
      "loss": 0.0015,
      "step": 116860
    },
    {
      "epoch": 6.233066666666667,
      "grad_norm": 0.29360678791999817,
      "learning_rate": 1.1043333333333333e-05,
      "loss": 0.002,
      "step": 116870
    },
    {
      "epoch": 6.2336,
      "grad_norm": 0.1586984097957611,
      "learning_rate": 1.1040000000000001e-05,
      "loss": 0.0021,
      "step": 116880
    },
    {
      "epoch": 6.234133333333333,
      "grad_norm": 0.10131438076496124,
      "learning_rate": 1.1036666666666667e-05,
      "loss": 0.003,
      "step": 116890
    },
    {
      "epoch": 6.234666666666667,
      "grad_norm": 0.096173956990242,
      "learning_rate": 1.1033333333333335e-05,
      "loss": 0.0021,
      "step": 116900
    },
    {
      "epoch": 6.2352,
      "grad_norm": 0.026298826560378075,
      "learning_rate": 1.103e-05,
      "loss": 0.0019,
      "step": 116910
    },
    {
      "epoch": 6.235733333333333,
      "grad_norm": 0.15744194388389587,
      "learning_rate": 1.1026666666666667e-05,
      "loss": 0.0015,
      "step": 116920
    },
    {
      "epoch": 6.236266666666666,
      "grad_norm": 0.38832077383995056,
      "learning_rate": 1.1023333333333333e-05,
      "loss": 0.002,
      "step": 116930
    },
    {
      "epoch": 6.2368,
      "grad_norm": 0.26916444301605225,
      "learning_rate": 1.1020000000000001e-05,
      "loss": 0.003,
      "step": 116940
    },
    {
      "epoch": 6.237333333333333,
      "grad_norm": 0.161519393324852,
      "learning_rate": 1.1016666666666667e-05,
      "loss": 0.0013,
      "step": 116950
    },
    {
      "epoch": 6.237866666666667,
      "grad_norm": 0.21223270893096924,
      "learning_rate": 1.1013333333333333e-05,
      "loss": 0.0015,
      "step": 116960
    },
    {
      "epoch": 6.2384,
      "grad_norm": 0.23620755970478058,
      "learning_rate": 1.1010000000000001e-05,
      "loss": 0.0014,
      "step": 116970
    },
    {
      "epoch": 6.238933333333334,
      "grad_norm": 0.30505338311195374,
      "learning_rate": 1.1006666666666666e-05,
      "loss": 0.0015,
      "step": 116980
    },
    {
      "epoch": 6.239466666666667,
      "grad_norm": 0.04613468423485756,
      "learning_rate": 1.1003333333333334e-05,
      "loss": 0.0019,
      "step": 116990
    },
    {
      "epoch": 6.24,
      "grad_norm": 0.34019550681114197,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.0019,
      "step": 117000
    },
    {
      "epoch": 6.2405333333333335,
      "grad_norm": 0.1271718442440033,
      "learning_rate": 1.0996666666666668e-05,
      "loss": 0.0027,
      "step": 117010
    },
    {
      "epoch": 6.241066666666667,
      "grad_norm": 0.15871617197990417,
      "learning_rate": 1.0993333333333334e-05,
      "loss": 0.0012,
      "step": 117020
    },
    {
      "epoch": 6.2416,
      "grad_norm": 0.10652260482311249,
      "learning_rate": 1.099e-05,
      "loss": 0.0015,
      "step": 117030
    },
    {
      "epoch": 6.242133333333333,
      "grad_norm": 0.0692848190665245,
      "learning_rate": 1.0986666666666668e-05,
      "loss": 0.0016,
      "step": 117040
    },
    {
      "epoch": 6.242666666666667,
      "grad_norm": 0.367561936378479,
      "learning_rate": 1.0983333333333334e-05,
      "loss": 0.0026,
      "step": 117050
    },
    {
      "epoch": 6.2432,
      "grad_norm": 0.1635177731513977,
      "learning_rate": 1.098e-05,
      "loss": 0.0014,
      "step": 117060
    },
    {
      "epoch": 6.243733333333333,
      "grad_norm": 0.055277250707149506,
      "learning_rate": 1.0976666666666668e-05,
      "loss": 0.0015,
      "step": 117070
    },
    {
      "epoch": 6.244266666666666,
      "grad_norm": 0.4739718735218048,
      "learning_rate": 1.0973333333333334e-05,
      "loss": 0.002,
      "step": 117080
    },
    {
      "epoch": 6.2448,
      "grad_norm": 0.2760367691516876,
      "learning_rate": 1.0970000000000002e-05,
      "loss": 0.0018,
      "step": 117090
    },
    {
      "epoch": 6.245333333333333,
      "grad_norm": 0.5906147956848145,
      "learning_rate": 1.0966666666666666e-05,
      "loss": 0.0014,
      "step": 117100
    },
    {
      "epoch": 6.245866666666666,
      "grad_norm": 0.05896485969424248,
      "learning_rate": 1.0963333333333334e-05,
      "loss": 0.0015,
      "step": 117110
    },
    {
      "epoch": 6.2464,
      "grad_norm": 0.06670030951499939,
      "learning_rate": 1.096e-05,
      "loss": 0.0024,
      "step": 117120
    },
    {
      "epoch": 6.246933333333334,
      "grad_norm": 0.23771968483924866,
      "learning_rate": 1.0956666666666668e-05,
      "loss": 0.0021,
      "step": 117130
    },
    {
      "epoch": 6.247466666666667,
      "grad_norm": 0.20415985584259033,
      "learning_rate": 1.0953333333333334e-05,
      "loss": 0.0014,
      "step": 117140
    },
    {
      "epoch": 6.248,
      "grad_norm": 0.12316520512104034,
      "learning_rate": 1.095e-05,
      "loss": 0.0013,
      "step": 117150
    },
    {
      "epoch": 6.2485333333333335,
      "grad_norm": 0.35054081678390503,
      "learning_rate": 1.0946666666666668e-05,
      "loss": 0.0015,
      "step": 117160
    },
    {
      "epoch": 6.249066666666667,
      "grad_norm": 0.04792654141783714,
      "learning_rate": 1.0943333333333332e-05,
      "loss": 0.002,
      "step": 117170
    },
    {
      "epoch": 6.2496,
      "grad_norm": 0.36154499650001526,
      "learning_rate": 1.094e-05,
      "loss": 0.0015,
      "step": 117180
    },
    {
      "epoch": 6.250133333333333,
      "grad_norm": 0.2336873561143875,
      "learning_rate": 1.0936666666666668e-05,
      "loss": 0.0017,
      "step": 117190
    },
    {
      "epoch": 6.250666666666667,
      "grad_norm": 0.0999085083603859,
      "learning_rate": 1.0933333333333334e-05,
      "loss": 0.0017,
      "step": 117200
    },
    {
      "epoch": 6.2512,
      "grad_norm": 0.03548240661621094,
      "learning_rate": 1.093e-05,
      "loss": 0.0022,
      "step": 117210
    },
    {
      "epoch": 6.251733333333333,
      "grad_norm": 0.25553861260414124,
      "learning_rate": 1.0926666666666667e-05,
      "loss": 0.0014,
      "step": 117220
    },
    {
      "epoch": 6.252266666666666,
      "grad_norm": 0.07571253180503845,
      "learning_rate": 1.0923333333333334e-05,
      "loss": 0.0013,
      "step": 117230
    },
    {
      "epoch": 6.2528,
      "grad_norm": 0.15074032545089722,
      "learning_rate": 1.092e-05,
      "loss": 0.0012,
      "step": 117240
    },
    {
      "epoch": 6.253333333333333,
      "grad_norm": 0.26212096214294434,
      "learning_rate": 1.0916666666666667e-05,
      "loss": 0.0017,
      "step": 117250
    },
    {
      "epoch": 6.253866666666667,
      "grad_norm": 0.06417394429445267,
      "learning_rate": 1.0913333333333334e-05,
      "loss": 0.0014,
      "step": 117260
    },
    {
      "epoch": 6.2544,
      "grad_norm": 0.3591610789299011,
      "learning_rate": 1.091e-05,
      "loss": 0.0014,
      "step": 117270
    },
    {
      "epoch": 6.254933333333334,
      "grad_norm": 0.12487152218818665,
      "learning_rate": 1.0906666666666668e-05,
      "loss": 0.0014,
      "step": 117280
    },
    {
      "epoch": 6.255466666666667,
      "grad_norm": 0.06636850535869598,
      "learning_rate": 1.0903333333333333e-05,
      "loss": 0.0013,
      "step": 117290
    },
    {
      "epoch": 6.256,
      "grad_norm": 0.3066028654575348,
      "learning_rate": 1.09e-05,
      "loss": 0.0023,
      "step": 117300
    },
    {
      "epoch": 6.2565333333333335,
      "grad_norm": 0.14816610515117645,
      "learning_rate": 1.0896666666666667e-05,
      "loss": 0.0019,
      "step": 117310
    },
    {
      "epoch": 6.257066666666667,
      "grad_norm": 0.13001343607902527,
      "learning_rate": 1.0893333333333333e-05,
      "loss": 0.0023,
      "step": 117320
    },
    {
      "epoch": 6.2576,
      "grad_norm": 0.15131428837776184,
      "learning_rate": 1.089e-05,
      "loss": 0.0022,
      "step": 117330
    },
    {
      "epoch": 6.258133333333333,
      "grad_norm": 0.13870880007743835,
      "learning_rate": 1.0886666666666667e-05,
      "loss": 0.0014,
      "step": 117340
    },
    {
      "epoch": 6.258666666666667,
      "grad_norm": 0.15179532766342163,
      "learning_rate": 1.0883333333333335e-05,
      "loss": 0.0021,
      "step": 117350
    },
    {
      "epoch": 6.2592,
      "grad_norm": 0.09150522947311401,
      "learning_rate": 1.088e-05,
      "loss": 0.0014,
      "step": 117360
    },
    {
      "epoch": 6.259733333333333,
      "grad_norm": 0.3793705403804779,
      "learning_rate": 1.0876666666666667e-05,
      "loss": 0.0015,
      "step": 117370
    },
    {
      "epoch": 6.260266666666666,
      "grad_norm": 0.24921007454395294,
      "learning_rate": 1.0873333333333335e-05,
      "loss": 0.0014,
      "step": 117380
    },
    {
      "epoch": 6.2608,
      "grad_norm": 0.21806327998638153,
      "learning_rate": 1.0870000000000001e-05,
      "loss": 0.0014,
      "step": 117390
    },
    {
      "epoch": 6.261333333333333,
      "grad_norm": 0.24865947663784027,
      "learning_rate": 1.0866666666666667e-05,
      "loss": 0.0024,
      "step": 117400
    },
    {
      "epoch": 6.261866666666666,
      "grad_norm": 0.2988450229167938,
      "learning_rate": 1.0863333333333333e-05,
      "loss": 0.0017,
      "step": 117410
    },
    {
      "epoch": 6.2624,
      "grad_norm": 0.06517540663480759,
      "learning_rate": 1.0860000000000001e-05,
      "loss": 0.0013,
      "step": 117420
    },
    {
      "epoch": 6.262933333333334,
      "grad_norm": 0.04303130879998207,
      "learning_rate": 1.0856666666666667e-05,
      "loss": 0.0018,
      "step": 117430
    },
    {
      "epoch": 6.263466666666667,
      "grad_norm": 0.4785282611846924,
      "learning_rate": 1.0853333333333333e-05,
      "loss": 0.0023,
      "step": 117440
    },
    {
      "epoch": 6.264,
      "grad_norm": 0.18033446371555328,
      "learning_rate": 1.0850000000000001e-05,
      "loss": 0.0015,
      "step": 117450
    },
    {
      "epoch": 6.2645333333333335,
      "grad_norm": 0.18435236811637878,
      "learning_rate": 1.0846666666666667e-05,
      "loss": 0.002,
      "step": 117460
    },
    {
      "epoch": 6.265066666666667,
      "grad_norm": 0.037490975111722946,
      "learning_rate": 1.0843333333333335e-05,
      "loss": 0.0015,
      "step": 117470
    },
    {
      "epoch": 6.2656,
      "grad_norm": 0.12227284163236618,
      "learning_rate": 1.084e-05,
      "loss": 0.0015,
      "step": 117480
    },
    {
      "epoch": 6.266133333333333,
      "grad_norm": 0.1282932311296463,
      "learning_rate": 1.0836666666666667e-05,
      "loss": 0.0027,
      "step": 117490
    },
    {
      "epoch": 6.266666666666667,
      "grad_norm": 0.18589390814304352,
      "learning_rate": 1.0833333333333334e-05,
      "loss": 0.0015,
      "step": 117500
    },
    {
      "epoch": 6.2672,
      "grad_norm": 0.06246786564588547,
      "learning_rate": 1.083e-05,
      "loss": 0.0018,
      "step": 117510
    },
    {
      "epoch": 6.267733333333333,
      "grad_norm": 0.13454186916351318,
      "learning_rate": 1.0826666666666667e-05,
      "loss": 0.002,
      "step": 117520
    },
    {
      "epoch": 6.268266666666666,
      "grad_norm": 0.0395086444914341,
      "learning_rate": 1.0823333333333334e-05,
      "loss": 0.0015,
      "step": 117530
    },
    {
      "epoch": 6.2688,
      "grad_norm": 0.24112586677074432,
      "learning_rate": 1.0820000000000001e-05,
      "loss": 0.0014,
      "step": 117540
    },
    {
      "epoch": 6.269333333333333,
      "grad_norm": 0.17723149061203003,
      "learning_rate": 1.0816666666666666e-05,
      "loss": 0.0017,
      "step": 117550
    },
    {
      "epoch": 6.269866666666666,
      "grad_norm": 0.21795377135276794,
      "learning_rate": 1.0813333333333334e-05,
      "loss": 0.0036,
      "step": 117560
    },
    {
      "epoch": 6.2704,
      "grad_norm": 0.42453426122665405,
      "learning_rate": 1.081e-05,
      "loss": 0.0019,
      "step": 117570
    },
    {
      "epoch": 6.270933333333334,
      "grad_norm": 0.16176319122314453,
      "learning_rate": 1.0806666666666668e-05,
      "loss": 0.0016,
      "step": 117580
    },
    {
      "epoch": 6.271466666666667,
      "grad_norm": 0.24648888409137726,
      "learning_rate": 1.0803333333333334e-05,
      "loss": 0.0017,
      "step": 117590
    },
    {
      "epoch": 6.272,
      "grad_norm": 0.06261862069368362,
      "learning_rate": 1.08e-05,
      "loss": 0.0016,
      "step": 117600
    },
    {
      "epoch": 6.2725333333333335,
      "grad_norm": 0.4455604553222656,
      "learning_rate": 1.0796666666666668e-05,
      "loss": 0.0012,
      "step": 117610
    },
    {
      "epoch": 6.273066666666667,
      "grad_norm": 0.23610709607601166,
      "learning_rate": 1.0793333333333334e-05,
      "loss": 0.0012,
      "step": 117620
    },
    {
      "epoch": 6.2736,
      "grad_norm": 0.03349124267697334,
      "learning_rate": 1.079e-05,
      "loss": 0.0029,
      "step": 117630
    },
    {
      "epoch": 6.274133333333333,
      "grad_norm": 0.1812497079372406,
      "learning_rate": 1.0786666666666668e-05,
      "loss": 0.0029,
      "step": 117640
    },
    {
      "epoch": 6.274666666666667,
      "grad_norm": 0.25177833437919617,
      "learning_rate": 1.0783333333333334e-05,
      "loss": 0.0023,
      "step": 117650
    },
    {
      "epoch": 6.2752,
      "grad_norm": 0.1179841160774231,
      "learning_rate": 1.0780000000000002e-05,
      "loss": 0.0019,
      "step": 117660
    },
    {
      "epoch": 6.275733333333333,
      "grad_norm": 0.24673037230968475,
      "learning_rate": 1.0776666666666666e-05,
      "loss": 0.0019,
      "step": 117670
    },
    {
      "epoch": 6.276266666666666,
      "grad_norm": 0.31168362498283386,
      "learning_rate": 1.0773333333333334e-05,
      "loss": 0.0022,
      "step": 117680
    },
    {
      "epoch": 6.2768,
      "grad_norm": 0.2148839682340622,
      "learning_rate": 1.077e-05,
      "loss": 0.0018,
      "step": 117690
    },
    {
      "epoch": 6.277333333333333,
      "grad_norm": 0.050944067537784576,
      "learning_rate": 1.0766666666666666e-05,
      "loss": 0.0019,
      "step": 117700
    },
    {
      "epoch": 6.277866666666666,
      "grad_norm": 0.47493085265159607,
      "learning_rate": 1.0763333333333334e-05,
      "loss": 0.0014,
      "step": 117710
    },
    {
      "epoch": 6.2783999999999995,
      "grad_norm": 0.16651375591754913,
      "learning_rate": 1.076e-05,
      "loss": 0.0019,
      "step": 117720
    },
    {
      "epoch": 6.278933333333334,
      "grad_norm": 0.295408695936203,
      "learning_rate": 1.0756666666666668e-05,
      "loss": 0.0023,
      "step": 117730
    },
    {
      "epoch": 6.279466666666667,
      "grad_norm": 0.14886142313480377,
      "learning_rate": 1.0753333333333333e-05,
      "loss": 0.0016,
      "step": 117740
    },
    {
      "epoch": 6.28,
      "grad_norm": 0.17332705855369568,
      "learning_rate": 1.075e-05,
      "loss": 0.0018,
      "step": 117750
    },
    {
      "epoch": 6.2805333333333335,
      "grad_norm": 0.17502310872077942,
      "learning_rate": 1.0746666666666667e-05,
      "loss": 0.0021,
      "step": 117760
    },
    {
      "epoch": 6.281066666666667,
      "grad_norm": 0.35228657722473145,
      "learning_rate": 1.0743333333333334e-05,
      "loss": 0.0023,
      "step": 117770
    },
    {
      "epoch": 6.2816,
      "grad_norm": 0.35057613253593445,
      "learning_rate": 1.074e-05,
      "loss": 0.0019,
      "step": 117780
    },
    {
      "epoch": 6.282133333333333,
      "grad_norm": 0.10194269567728043,
      "learning_rate": 1.0736666666666667e-05,
      "loss": 0.0024,
      "step": 117790
    },
    {
      "epoch": 6.282666666666667,
      "grad_norm": 0.4097626507282257,
      "learning_rate": 1.0733333333333334e-05,
      "loss": 0.0023,
      "step": 117800
    },
    {
      "epoch": 6.2832,
      "grad_norm": 0.2664473056793213,
      "learning_rate": 1.073e-05,
      "loss": 0.0014,
      "step": 117810
    },
    {
      "epoch": 6.283733333333333,
      "grad_norm": 0.3411892056465149,
      "learning_rate": 1.0726666666666667e-05,
      "loss": 0.0021,
      "step": 117820
    },
    {
      "epoch": 6.2842666666666664,
      "grad_norm": 0.09919600933790207,
      "learning_rate": 1.0723333333333335e-05,
      "loss": 0.0026,
      "step": 117830
    },
    {
      "epoch": 6.2848,
      "grad_norm": 0.17636461555957794,
      "learning_rate": 1.072e-05,
      "loss": 0.0021,
      "step": 117840
    },
    {
      "epoch": 6.285333333333333,
      "grad_norm": 0.0896729975938797,
      "learning_rate": 1.0716666666666667e-05,
      "loss": 0.0018,
      "step": 117850
    },
    {
      "epoch": 6.285866666666666,
      "grad_norm": 0.026431553065776825,
      "learning_rate": 1.0713333333333333e-05,
      "loss": 0.0013,
      "step": 117860
    },
    {
      "epoch": 6.2864,
      "grad_norm": 0.14129459857940674,
      "learning_rate": 1.071e-05,
      "loss": 0.0015,
      "step": 117870
    },
    {
      "epoch": 6.286933333333334,
      "grad_norm": 0.09955916553735733,
      "learning_rate": 1.0706666666666667e-05,
      "loss": 0.0018,
      "step": 117880
    },
    {
      "epoch": 6.287466666666667,
      "grad_norm": 0.4621739387512207,
      "learning_rate": 1.0703333333333333e-05,
      "loss": 0.0024,
      "step": 117890
    },
    {
      "epoch": 6.288,
      "grad_norm": 0.09057430922985077,
      "learning_rate": 1.0700000000000001e-05,
      "loss": 0.0019,
      "step": 117900
    },
    {
      "epoch": 6.2885333333333335,
      "grad_norm": 0.20047801733016968,
      "learning_rate": 1.0696666666666667e-05,
      "loss": 0.002,
      "step": 117910
    },
    {
      "epoch": 6.289066666666667,
      "grad_norm": 0.20982897281646729,
      "learning_rate": 1.0693333333333335e-05,
      "loss": 0.0018,
      "step": 117920
    },
    {
      "epoch": 6.2896,
      "grad_norm": 0.22474123537540436,
      "learning_rate": 1.069e-05,
      "loss": 0.0014,
      "step": 117930
    },
    {
      "epoch": 6.290133333333333,
      "grad_norm": 0.1494138389825821,
      "learning_rate": 1.0686666666666667e-05,
      "loss": 0.0036,
      "step": 117940
    },
    {
      "epoch": 6.290666666666667,
      "grad_norm": 0.23486492037773132,
      "learning_rate": 1.0683333333333333e-05,
      "loss": 0.0023,
      "step": 117950
    },
    {
      "epoch": 6.2912,
      "grad_norm": 0.040902890264987946,
      "learning_rate": 1.0680000000000001e-05,
      "loss": 0.003,
      "step": 117960
    },
    {
      "epoch": 6.291733333333333,
      "grad_norm": 0.39724934101104736,
      "learning_rate": 1.0676666666666667e-05,
      "loss": 0.0014,
      "step": 117970
    },
    {
      "epoch": 6.2922666666666665,
      "grad_norm": 0.06257433444261551,
      "learning_rate": 1.0673333333333333e-05,
      "loss": 0.0018,
      "step": 117980
    },
    {
      "epoch": 6.2928,
      "grad_norm": 0.07358957827091217,
      "learning_rate": 1.0670000000000001e-05,
      "loss": 0.0018,
      "step": 117990
    },
    {
      "epoch": 6.293333333333333,
      "grad_norm": 0.29296621680259705,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 0.0025,
      "step": 118000
    },
    {
      "epoch": 6.293866666666666,
      "grad_norm": 0.379495769739151,
      "learning_rate": 1.0663333333333333e-05,
      "loss": 0.0016,
      "step": 118010
    },
    {
      "epoch": 6.2943999999999996,
      "grad_norm": 0.18020661175251007,
      "learning_rate": 1.0660000000000001e-05,
      "loss": 0.001,
      "step": 118020
    },
    {
      "epoch": 6.294933333333334,
      "grad_norm": 0.16001391410827637,
      "learning_rate": 1.0656666666666667e-05,
      "loss": 0.0022,
      "step": 118030
    },
    {
      "epoch": 6.295466666666667,
      "grad_norm": 0.2069087028503418,
      "learning_rate": 1.0653333333333334e-05,
      "loss": 0.0013,
      "step": 118040
    },
    {
      "epoch": 6.296,
      "grad_norm": 0.3036099672317505,
      "learning_rate": 1.065e-05,
      "loss": 0.0012,
      "step": 118050
    },
    {
      "epoch": 6.2965333333333335,
      "grad_norm": 0.06296809017658234,
      "learning_rate": 1.0646666666666668e-05,
      "loss": 0.0015,
      "step": 118060
    },
    {
      "epoch": 6.297066666666667,
      "grad_norm": 0.17841210961341858,
      "learning_rate": 1.0643333333333334e-05,
      "loss": 0.0017,
      "step": 118070
    },
    {
      "epoch": 6.2976,
      "grad_norm": 0.22143609821796417,
      "learning_rate": 1.064e-05,
      "loss": 0.0016,
      "step": 118080
    },
    {
      "epoch": 6.298133333333333,
      "grad_norm": 0.02387925237417221,
      "learning_rate": 1.0636666666666668e-05,
      "loss": 0.0021,
      "step": 118090
    },
    {
      "epoch": 6.298666666666667,
      "grad_norm": 0.09517120569944382,
      "learning_rate": 1.0633333333333334e-05,
      "loss": 0.0016,
      "step": 118100
    },
    {
      "epoch": 6.2992,
      "grad_norm": 0.49775922298431396,
      "learning_rate": 1.0630000000000002e-05,
      "loss": 0.0015,
      "step": 118110
    },
    {
      "epoch": 6.299733333333333,
      "grad_norm": 0.2499135583639145,
      "learning_rate": 1.0626666666666666e-05,
      "loss": 0.0023,
      "step": 118120
    },
    {
      "epoch": 6.3002666666666665,
      "grad_norm": 0.18569226562976837,
      "learning_rate": 1.0623333333333334e-05,
      "loss": 0.0014,
      "step": 118130
    },
    {
      "epoch": 6.3008,
      "grad_norm": 0.1449883133172989,
      "learning_rate": 1.062e-05,
      "loss": 0.0031,
      "step": 118140
    },
    {
      "epoch": 6.301333333333333,
      "grad_norm": 0.2131383717060089,
      "learning_rate": 1.0616666666666668e-05,
      "loss": 0.0022,
      "step": 118150
    },
    {
      "epoch": 6.301866666666666,
      "grad_norm": 0.10758504271507263,
      "learning_rate": 1.0613333333333334e-05,
      "loss": 0.0015,
      "step": 118160
    },
    {
      "epoch": 6.3024000000000004,
      "grad_norm": 0.5032253265380859,
      "learning_rate": 1.061e-05,
      "loss": 0.0014,
      "step": 118170
    },
    {
      "epoch": 6.302933333333334,
      "grad_norm": 0.1431247442960739,
      "learning_rate": 1.0606666666666668e-05,
      "loss": 0.0021,
      "step": 118180
    },
    {
      "epoch": 6.303466666666667,
      "grad_norm": 0.379878968000412,
      "learning_rate": 1.0603333333333332e-05,
      "loss": 0.0013,
      "step": 118190
    },
    {
      "epoch": 6.304,
      "grad_norm": 0.27208054065704346,
      "learning_rate": 1.06e-05,
      "loss": 0.0022,
      "step": 118200
    },
    {
      "epoch": 6.3045333333333335,
      "grad_norm": 0.0630190521478653,
      "learning_rate": 1.0596666666666668e-05,
      "loss": 0.0016,
      "step": 118210
    },
    {
      "epoch": 6.305066666666667,
      "grad_norm": 0.03853759169578552,
      "learning_rate": 1.0593333333333334e-05,
      "loss": 0.0015,
      "step": 118220
    },
    {
      "epoch": 6.3056,
      "grad_norm": 0.1880677044391632,
      "learning_rate": 1.059e-05,
      "loss": 0.0015,
      "step": 118230
    },
    {
      "epoch": 6.306133333333333,
      "grad_norm": 0.19148939847946167,
      "learning_rate": 1.0586666666666666e-05,
      "loss": 0.0021,
      "step": 118240
    },
    {
      "epoch": 6.306666666666667,
      "grad_norm": 0.15676183998584747,
      "learning_rate": 1.0583333333333334e-05,
      "loss": 0.0019,
      "step": 118250
    },
    {
      "epoch": 6.3072,
      "grad_norm": 0.3368787467479706,
      "learning_rate": 1.058e-05,
      "loss": 0.0015,
      "step": 118260
    },
    {
      "epoch": 6.307733333333333,
      "grad_norm": 0.20367395877838135,
      "learning_rate": 1.0576666666666666e-05,
      "loss": 0.0021,
      "step": 118270
    },
    {
      "epoch": 6.3082666666666665,
      "grad_norm": 0.04747256636619568,
      "learning_rate": 1.0573333333333334e-05,
      "loss": 0.0033,
      "step": 118280
    },
    {
      "epoch": 6.3088,
      "grad_norm": 0.08893052488565445,
      "learning_rate": 1.057e-05,
      "loss": 0.0023,
      "step": 118290
    },
    {
      "epoch": 6.309333333333333,
      "grad_norm": 0.13628438115119934,
      "learning_rate": 1.0566666666666668e-05,
      "loss": 0.0012,
      "step": 118300
    },
    {
      "epoch": 6.309866666666666,
      "grad_norm": 0.3049548268318176,
      "learning_rate": 1.0563333333333333e-05,
      "loss": 0.0015,
      "step": 118310
    },
    {
      "epoch": 6.3104,
      "grad_norm": 0.0795774832367897,
      "learning_rate": 1.056e-05,
      "loss": 0.0023,
      "step": 118320
    },
    {
      "epoch": 6.310933333333334,
      "grad_norm": 0.04563404247164726,
      "learning_rate": 1.0556666666666667e-05,
      "loss": 0.0013,
      "step": 118330
    },
    {
      "epoch": 6.311466666666667,
      "grad_norm": 0.04805110767483711,
      "learning_rate": 1.0553333333333335e-05,
      "loss": 0.0018,
      "step": 118340
    },
    {
      "epoch": 6.312,
      "grad_norm": 0.08701132982969284,
      "learning_rate": 1.055e-05,
      "loss": 0.0036,
      "step": 118350
    },
    {
      "epoch": 6.3125333333333336,
      "grad_norm": 0.07638771086931229,
      "learning_rate": 1.0546666666666667e-05,
      "loss": 0.0021,
      "step": 118360
    },
    {
      "epoch": 6.313066666666667,
      "grad_norm": 0.04261361435055733,
      "learning_rate": 1.0543333333333335e-05,
      "loss": 0.0012,
      "step": 118370
    },
    {
      "epoch": 6.3136,
      "grad_norm": 0.4747820496559143,
      "learning_rate": 1.0539999999999999e-05,
      "loss": 0.0016,
      "step": 118380
    },
    {
      "epoch": 6.314133333333333,
      "grad_norm": 0.20656444132328033,
      "learning_rate": 1.0536666666666667e-05,
      "loss": 0.0016,
      "step": 118390
    },
    {
      "epoch": 6.314666666666667,
      "grad_norm": 0.12505081295967102,
      "learning_rate": 1.0533333333333335e-05,
      "loss": 0.0017,
      "step": 118400
    },
    {
      "epoch": 6.3152,
      "grad_norm": 0.18092641234397888,
      "learning_rate": 1.053e-05,
      "loss": 0.0014,
      "step": 118410
    },
    {
      "epoch": 6.315733333333333,
      "grad_norm": 0.3533475697040558,
      "learning_rate": 1.0526666666666667e-05,
      "loss": 0.0019,
      "step": 118420
    },
    {
      "epoch": 6.3162666666666665,
      "grad_norm": 0.06689354032278061,
      "learning_rate": 1.0523333333333333e-05,
      "loss": 0.0012,
      "step": 118430
    },
    {
      "epoch": 6.3168,
      "grad_norm": 0.0894937589764595,
      "learning_rate": 1.0520000000000001e-05,
      "loss": 0.0016,
      "step": 118440
    },
    {
      "epoch": 6.317333333333333,
      "grad_norm": 0.30762916803359985,
      "learning_rate": 1.0516666666666667e-05,
      "loss": 0.0013,
      "step": 118450
    },
    {
      "epoch": 6.317866666666666,
      "grad_norm": 0.056282564997673035,
      "learning_rate": 1.0513333333333333e-05,
      "loss": 0.0015,
      "step": 118460
    },
    {
      "epoch": 6.3184000000000005,
      "grad_norm": 0.035979848355054855,
      "learning_rate": 1.0510000000000001e-05,
      "loss": 0.0019,
      "step": 118470
    },
    {
      "epoch": 6.318933333333334,
      "grad_norm": 0.15028026700019836,
      "learning_rate": 1.0506666666666667e-05,
      "loss": 0.0025,
      "step": 118480
    },
    {
      "epoch": 6.319466666666667,
      "grad_norm": 0.17973510921001434,
      "learning_rate": 1.0503333333333335e-05,
      "loss": 0.0019,
      "step": 118490
    },
    {
      "epoch": 6.32,
      "grad_norm": 0.27375227212905884,
      "learning_rate": 1.05e-05,
      "loss": 0.002,
      "step": 118500
    },
    {
      "epoch": 6.320533333333334,
      "grad_norm": 0.09162880480289459,
      "learning_rate": 1.0496666666666667e-05,
      "loss": 0.0012,
      "step": 118510
    },
    {
      "epoch": 6.321066666666667,
      "grad_norm": 0.04614029824733734,
      "learning_rate": 1.0493333333333333e-05,
      "loss": 0.0022,
      "step": 118520
    },
    {
      "epoch": 6.3216,
      "grad_norm": 0.06041054055094719,
      "learning_rate": 1.049e-05,
      "loss": 0.0011,
      "step": 118530
    },
    {
      "epoch": 6.322133333333333,
      "grad_norm": 0.097519151866436,
      "learning_rate": 1.0486666666666667e-05,
      "loss": 0.0016,
      "step": 118540
    },
    {
      "epoch": 6.322666666666667,
      "grad_norm": 0.1566617637872696,
      "learning_rate": 1.0483333333333333e-05,
      "loss": 0.0015,
      "step": 118550
    },
    {
      "epoch": 6.3232,
      "grad_norm": 0.10028723627328873,
      "learning_rate": 1.0480000000000001e-05,
      "loss": 0.0027,
      "step": 118560
    },
    {
      "epoch": 6.323733333333333,
      "grad_norm": 0.061498720198869705,
      "learning_rate": 1.0476666666666666e-05,
      "loss": 0.0018,
      "step": 118570
    },
    {
      "epoch": 6.3242666666666665,
      "grad_norm": 0.2060258835554123,
      "learning_rate": 1.0473333333333334e-05,
      "loss": 0.0022,
      "step": 118580
    },
    {
      "epoch": 6.3248,
      "grad_norm": 0.4654238224029541,
      "learning_rate": 1.0470000000000001e-05,
      "loss": 0.0016,
      "step": 118590
    },
    {
      "epoch": 6.325333333333333,
      "grad_norm": 0.219783216714859,
      "learning_rate": 1.0466666666666668e-05,
      "loss": 0.0027,
      "step": 118600
    },
    {
      "epoch": 6.325866666666666,
      "grad_norm": 0.38838711380958557,
      "learning_rate": 1.0463333333333334e-05,
      "loss": 0.0027,
      "step": 118610
    },
    {
      "epoch": 6.3264,
      "grad_norm": 0.31715691089630127,
      "learning_rate": 1.046e-05,
      "loss": 0.0016,
      "step": 118620
    },
    {
      "epoch": 6.326933333333334,
      "grad_norm": 0.20952092111110687,
      "learning_rate": 1.0456666666666668e-05,
      "loss": 0.0025,
      "step": 118630
    },
    {
      "epoch": 6.327466666666667,
      "grad_norm": 0.33609524369239807,
      "learning_rate": 1.0453333333333334e-05,
      "loss": 0.0014,
      "step": 118640
    },
    {
      "epoch": 6.328,
      "grad_norm": 0.4797344207763672,
      "learning_rate": 1.045e-05,
      "loss": 0.0023,
      "step": 118650
    },
    {
      "epoch": 6.328533333333334,
      "grad_norm": 0.3870801031589508,
      "learning_rate": 1.0446666666666668e-05,
      "loss": 0.0013,
      "step": 118660
    },
    {
      "epoch": 6.329066666666667,
      "grad_norm": 0.5954798460006714,
      "learning_rate": 1.0443333333333334e-05,
      "loss": 0.0033,
      "step": 118670
    },
    {
      "epoch": 6.3296,
      "grad_norm": 0.4180637300014496,
      "learning_rate": 1.0440000000000002e-05,
      "loss": 0.0018,
      "step": 118680
    },
    {
      "epoch": 6.330133333333333,
      "grad_norm": 0.21785850822925568,
      "learning_rate": 1.0436666666666666e-05,
      "loss": 0.0025,
      "step": 118690
    },
    {
      "epoch": 6.330666666666667,
      "grad_norm": 0.03248438984155655,
      "learning_rate": 1.0433333333333334e-05,
      "loss": 0.0029,
      "step": 118700
    },
    {
      "epoch": 6.3312,
      "grad_norm": 0.16326391696929932,
      "learning_rate": 1.043e-05,
      "loss": 0.0016,
      "step": 118710
    },
    {
      "epoch": 6.331733333333333,
      "grad_norm": 0.3328668177127838,
      "learning_rate": 1.0426666666666666e-05,
      "loss": 0.0014,
      "step": 118720
    },
    {
      "epoch": 6.3322666666666665,
      "grad_norm": 0.21323421597480774,
      "learning_rate": 1.0423333333333334e-05,
      "loss": 0.0016,
      "step": 118730
    },
    {
      "epoch": 6.3328,
      "grad_norm": 0.29471808671951294,
      "learning_rate": 1.042e-05,
      "loss": 0.0014,
      "step": 118740
    },
    {
      "epoch": 6.333333333333333,
      "grad_norm": 0.4985799491405487,
      "learning_rate": 1.0416666666666668e-05,
      "loss": 0.0014,
      "step": 118750
    },
    {
      "epoch": 6.333866666666666,
      "grad_norm": 0.2070222645998001,
      "learning_rate": 1.0413333333333332e-05,
      "loss": 0.0021,
      "step": 118760
    },
    {
      "epoch": 6.3344,
      "grad_norm": 0.12989583611488342,
      "learning_rate": 1.041e-05,
      "loss": 0.0014,
      "step": 118770
    },
    {
      "epoch": 6.334933333333334,
      "grad_norm": 0.03675634786486626,
      "learning_rate": 1.0406666666666668e-05,
      "loss": 0.0019,
      "step": 118780
    },
    {
      "epoch": 6.335466666666667,
      "grad_norm": 0.4740010201931,
      "learning_rate": 1.0403333333333334e-05,
      "loss": 0.0024,
      "step": 118790
    },
    {
      "epoch": 6.336,
      "grad_norm": 0.29221007227897644,
      "learning_rate": 1.04e-05,
      "loss": 0.0017,
      "step": 118800
    },
    {
      "epoch": 6.336533333333334,
      "grad_norm": 0.07148778438568115,
      "learning_rate": 1.0396666666666667e-05,
      "loss": 0.002,
      "step": 118810
    },
    {
      "epoch": 6.337066666666667,
      "grad_norm": 0.05121065676212311,
      "learning_rate": 1.0393333333333334e-05,
      "loss": 0.0014,
      "step": 118820
    },
    {
      "epoch": 6.3376,
      "grad_norm": 0.14676415920257568,
      "learning_rate": 1.039e-05,
      "loss": 0.0023,
      "step": 118830
    },
    {
      "epoch": 6.338133333333333,
      "grad_norm": 0.07709135115146637,
      "learning_rate": 1.0386666666666667e-05,
      "loss": 0.002,
      "step": 118840
    },
    {
      "epoch": 6.338666666666667,
      "grad_norm": 0.21116141974925995,
      "learning_rate": 1.0383333333333334e-05,
      "loss": 0.0013,
      "step": 118850
    },
    {
      "epoch": 6.3392,
      "grad_norm": 0.27077341079711914,
      "learning_rate": 1.038e-05,
      "loss": 0.002,
      "step": 118860
    },
    {
      "epoch": 6.339733333333333,
      "grad_norm": 0.47258085012435913,
      "learning_rate": 1.0376666666666667e-05,
      "loss": 0.0017,
      "step": 118870
    },
    {
      "epoch": 6.3402666666666665,
      "grad_norm": 0.04454570636153221,
      "learning_rate": 1.0373333333333333e-05,
      "loss": 0.0015,
      "step": 118880
    },
    {
      "epoch": 6.3408,
      "grad_norm": 0.04286791384220123,
      "learning_rate": 1.037e-05,
      "loss": 0.0017,
      "step": 118890
    },
    {
      "epoch": 6.341333333333333,
      "grad_norm": 0.15596894919872284,
      "learning_rate": 1.0366666666666667e-05,
      "loss": 0.0019,
      "step": 118900
    },
    {
      "epoch": 6.341866666666666,
      "grad_norm": 0.21704666316509247,
      "learning_rate": 1.0363333333333333e-05,
      "loss": 0.0021,
      "step": 118910
    },
    {
      "epoch": 6.3424,
      "grad_norm": 0.02823760360479355,
      "learning_rate": 1.036e-05,
      "loss": 0.0022,
      "step": 118920
    },
    {
      "epoch": 6.342933333333333,
      "grad_norm": 0.24750599265098572,
      "learning_rate": 1.0356666666666667e-05,
      "loss": 0.0026,
      "step": 118930
    },
    {
      "epoch": 6.343466666666667,
      "grad_norm": 0.2727985084056854,
      "learning_rate": 1.0353333333333335e-05,
      "loss": 0.0018,
      "step": 118940
    },
    {
      "epoch": 6.344,
      "grad_norm": 0.2441006302833557,
      "learning_rate": 1.035e-05,
      "loss": 0.0026,
      "step": 118950
    },
    {
      "epoch": 6.344533333333334,
      "grad_norm": 0.06268490105867386,
      "learning_rate": 1.0346666666666667e-05,
      "loss": 0.0015,
      "step": 118960
    },
    {
      "epoch": 6.345066666666667,
      "grad_norm": 0.23724232614040375,
      "learning_rate": 1.0343333333333335e-05,
      "loss": 0.0017,
      "step": 118970
    },
    {
      "epoch": 6.3456,
      "grad_norm": 0.10566247999668121,
      "learning_rate": 1.0340000000000001e-05,
      "loss": 0.0018,
      "step": 118980
    },
    {
      "epoch": 6.346133333333333,
      "grad_norm": 0.3666701316833496,
      "learning_rate": 1.0336666666666667e-05,
      "loss": 0.0016,
      "step": 118990
    },
    {
      "epoch": 6.346666666666667,
      "grad_norm": 0.03059951215982437,
      "learning_rate": 1.0333333333333333e-05,
      "loss": 0.0021,
      "step": 119000
    },
    {
      "epoch": 6.3472,
      "grad_norm": 0.05424101650714874,
      "learning_rate": 1.0330000000000001e-05,
      "loss": 0.0022,
      "step": 119010
    },
    {
      "epoch": 6.347733333333333,
      "grad_norm": 0.24113008379936218,
      "learning_rate": 1.0326666666666667e-05,
      "loss": 0.0012,
      "step": 119020
    },
    {
      "epoch": 6.3482666666666665,
      "grad_norm": 0.07638619095087051,
      "learning_rate": 1.0323333333333333e-05,
      "loss": 0.0021,
      "step": 119030
    },
    {
      "epoch": 6.3488,
      "grad_norm": 0.12051171064376831,
      "learning_rate": 1.0320000000000001e-05,
      "loss": 0.0018,
      "step": 119040
    },
    {
      "epoch": 6.349333333333333,
      "grad_norm": 0.42591893672943115,
      "learning_rate": 1.0316666666666667e-05,
      "loss": 0.0016,
      "step": 119050
    },
    {
      "epoch": 6.349866666666666,
      "grad_norm": 0.4420446753501892,
      "learning_rate": 1.0313333333333333e-05,
      "loss": 0.002,
      "step": 119060
    },
    {
      "epoch": 6.3504,
      "grad_norm": 0.38962143659591675,
      "learning_rate": 1.031e-05,
      "loss": 0.0014,
      "step": 119070
    },
    {
      "epoch": 6.350933333333334,
      "grad_norm": 0.19303342700004578,
      "learning_rate": 1.0306666666666667e-05,
      "loss": 0.0015,
      "step": 119080
    },
    {
      "epoch": 6.351466666666667,
      "grad_norm": 0.29205751419067383,
      "learning_rate": 1.0303333333333334e-05,
      "loss": 0.0015,
      "step": 119090
    },
    {
      "epoch": 6.352,
      "grad_norm": 0.5421563982963562,
      "learning_rate": 1.03e-05,
      "loss": 0.0014,
      "step": 119100
    },
    {
      "epoch": 6.352533333333334,
      "grad_norm": 0.5169797539710999,
      "learning_rate": 1.0296666666666667e-05,
      "loss": 0.0016,
      "step": 119110
    },
    {
      "epoch": 6.353066666666667,
      "grad_norm": 0.3327144980430603,
      "learning_rate": 1.0293333333333334e-05,
      "loss": 0.0016,
      "step": 119120
    },
    {
      "epoch": 6.3536,
      "grad_norm": 0.41495630145072937,
      "learning_rate": 1.0290000000000001e-05,
      "loss": 0.0019,
      "step": 119130
    },
    {
      "epoch": 6.354133333333333,
      "grad_norm": 0.1908913552761078,
      "learning_rate": 1.0286666666666666e-05,
      "loss": 0.002,
      "step": 119140
    },
    {
      "epoch": 6.354666666666667,
      "grad_norm": 0.1696331799030304,
      "learning_rate": 1.0283333333333334e-05,
      "loss": 0.0025,
      "step": 119150
    },
    {
      "epoch": 6.3552,
      "grad_norm": 0.06298698484897614,
      "learning_rate": 1.0280000000000002e-05,
      "loss": 0.0019,
      "step": 119160
    },
    {
      "epoch": 6.355733333333333,
      "grad_norm": 0.21611802279949188,
      "learning_rate": 1.0276666666666668e-05,
      "loss": 0.0019,
      "step": 119170
    },
    {
      "epoch": 6.3562666666666665,
      "grad_norm": 0.3323191702365875,
      "learning_rate": 1.0273333333333334e-05,
      "loss": 0.0018,
      "step": 119180
    },
    {
      "epoch": 6.3568,
      "grad_norm": 0.22203226387500763,
      "learning_rate": 1.027e-05,
      "loss": 0.0016,
      "step": 119190
    },
    {
      "epoch": 6.357333333333333,
      "grad_norm": 0.5911291241645813,
      "learning_rate": 1.0266666666666668e-05,
      "loss": 0.0016,
      "step": 119200
    },
    {
      "epoch": 6.357866666666666,
      "grad_norm": 0.23813506960868835,
      "learning_rate": 1.0263333333333334e-05,
      "loss": 0.0019,
      "step": 119210
    },
    {
      "epoch": 6.3584,
      "grad_norm": 0.15409159660339355,
      "learning_rate": 1.026e-05,
      "loss": 0.0017,
      "step": 119220
    },
    {
      "epoch": 6.358933333333333,
      "grad_norm": 0.035922735929489136,
      "learning_rate": 1.0256666666666668e-05,
      "loss": 0.0013,
      "step": 119230
    },
    {
      "epoch": 6.359466666666667,
      "grad_norm": 0.04565891623497009,
      "learning_rate": 1.0253333333333334e-05,
      "loss": 0.0018,
      "step": 119240
    },
    {
      "epoch": 6.36,
      "grad_norm": 0.3010762631893158,
      "learning_rate": 1.025e-05,
      "loss": 0.0016,
      "step": 119250
    },
    {
      "epoch": 6.360533333333334,
      "grad_norm": 0.1521335244178772,
      "learning_rate": 1.0246666666666666e-05,
      "loss": 0.0015,
      "step": 119260
    },
    {
      "epoch": 6.361066666666667,
      "grad_norm": 0.24203695356845856,
      "learning_rate": 1.0243333333333334e-05,
      "loss": 0.0022,
      "step": 119270
    },
    {
      "epoch": 6.3616,
      "grad_norm": 0.2691097855567932,
      "learning_rate": 1.024e-05,
      "loss": 0.0017,
      "step": 119280
    },
    {
      "epoch": 6.362133333333333,
      "grad_norm": 0.262368381023407,
      "learning_rate": 1.0236666666666666e-05,
      "loss": 0.0018,
      "step": 119290
    },
    {
      "epoch": 6.362666666666667,
      "grad_norm": 0.06940719485282898,
      "learning_rate": 1.0233333333333334e-05,
      "loss": 0.0034,
      "step": 119300
    },
    {
      "epoch": 6.3632,
      "grad_norm": 0.1140679195523262,
      "learning_rate": 1.023e-05,
      "loss": 0.0013,
      "step": 119310
    },
    {
      "epoch": 6.363733333333333,
      "grad_norm": 0.0702500194311142,
      "learning_rate": 1.0226666666666668e-05,
      "loss": 0.0014,
      "step": 119320
    },
    {
      "epoch": 6.3642666666666665,
      "grad_norm": 0.10100836306810379,
      "learning_rate": 1.0223333333333333e-05,
      "loss": 0.0015,
      "step": 119330
    },
    {
      "epoch": 6.3648,
      "grad_norm": 0.3884254992008209,
      "learning_rate": 1.022e-05,
      "loss": 0.0021,
      "step": 119340
    },
    {
      "epoch": 6.365333333333333,
      "grad_norm": 0.23680700361728668,
      "learning_rate": 1.0216666666666668e-05,
      "loss": 0.0022,
      "step": 119350
    },
    {
      "epoch": 6.365866666666666,
      "grad_norm": 0.1989433914422989,
      "learning_rate": 1.0213333333333334e-05,
      "loss": 0.0024,
      "step": 119360
    },
    {
      "epoch": 6.3664,
      "grad_norm": 0.34114179015159607,
      "learning_rate": 1.021e-05,
      "loss": 0.002,
      "step": 119370
    },
    {
      "epoch": 6.366933333333334,
      "grad_norm": 0.15458035469055176,
      "learning_rate": 1.0206666666666667e-05,
      "loss": 0.0018,
      "step": 119380
    },
    {
      "epoch": 6.367466666666667,
      "grad_norm": 0.15256813168525696,
      "learning_rate": 1.0203333333333334e-05,
      "loss": 0.0017,
      "step": 119390
    },
    {
      "epoch": 6.368,
      "grad_norm": 0.04483630508184433,
      "learning_rate": 1.02e-05,
      "loss": 0.0018,
      "step": 119400
    },
    {
      "epoch": 6.368533333333334,
      "grad_norm": 0.44001680612564087,
      "learning_rate": 1.0196666666666667e-05,
      "loss": 0.0016,
      "step": 119410
    },
    {
      "epoch": 6.369066666666667,
      "grad_norm": 0.11887804418802261,
      "learning_rate": 1.0193333333333335e-05,
      "loss": 0.0014,
      "step": 119420
    },
    {
      "epoch": 6.3696,
      "grad_norm": 0.16244637966156006,
      "learning_rate": 1.019e-05,
      "loss": 0.0016,
      "step": 119430
    },
    {
      "epoch": 6.370133333333333,
      "grad_norm": 0.07799912989139557,
      "learning_rate": 1.0186666666666667e-05,
      "loss": 0.0015,
      "step": 119440
    },
    {
      "epoch": 6.370666666666667,
      "grad_norm": 0.32942691445350647,
      "learning_rate": 1.0183333333333333e-05,
      "loss": 0.0013,
      "step": 119450
    },
    {
      "epoch": 6.3712,
      "grad_norm": 0.12460412830114365,
      "learning_rate": 1.018e-05,
      "loss": 0.0016,
      "step": 119460
    },
    {
      "epoch": 6.371733333333333,
      "grad_norm": 0.1472695916891098,
      "learning_rate": 1.0176666666666667e-05,
      "loss": 0.0021,
      "step": 119470
    },
    {
      "epoch": 6.3722666666666665,
      "grad_norm": 0.18693415820598602,
      "learning_rate": 1.0173333333333333e-05,
      "loss": 0.0018,
      "step": 119480
    },
    {
      "epoch": 6.3728,
      "grad_norm": 0.060269564390182495,
      "learning_rate": 1.0170000000000001e-05,
      "loss": 0.0018,
      "step": 119490
    },
    {
      "epoch": 6.373333333333333,
      "grad_norm": 0.30089640617370605,
      "learning_rate": 1.0166666666666667e-05,
      "loss": 0.0021,
      "step": 119500
    },
    {
      "epoch": 6.373866666666666,
      "grad_norm": 0.09904579818248749,
      "learning_rate": 1.0163333333333335e-05,
      "loss": 0.0019,
      "step": 119510
    },
    {
      "epoch": 6.3744,
      "grad_norm": 0.23360216617584229,
      "learning_rate": 1.016e-05,
      "loss": 0.0022,
      "step": 119520
    },
    {
      "epoch": 6.374933333333333,
      "grad_norm": 0.18203353881835938,
      "learning_rate": 1.0156666666666667e-05,
      "loss": 0.0017,
      "step": 119530
    },
    {
      "epoch": 6.375466666666667,
      "grad_norm": 0.11314252018928528,
      "learning_rate": 1.0153333333333335e-05,
      "loss": 0.0018,
      "step": 119540
    },
    {
      "epoch": 6.376,
      "grad_norm": 0.17388513684272766,
      "learning_rate": 1.0150000000000001e-05,
      "loss": 0.0021,
      "step": 119550
    },
    {
      "epoch": 6.376533333333334,
      "grad_norm": 0.05555647239089012,
      "learning_rate": 1.0146666666666667e-05,
      "loss": 0.0013,
      "step": 119560
    },
    {
      "epoch": 6.377066666666667,
      "grad_norm": 0.5869051218032837,
      "learning_rate": 1.0143333333333333e-05,
      "loss": 0.0022,
      "step": 119570
    },
    {
      "epoch": 6.3776,
      "grad_norm": 0.149699866771698,
      "learning_rate": 1.0140000000000001e-05,
      "loss": 0.0023,
      "step": 119580
    },
    {
      "epoch": 6.378133333333333,
      "grad_norm": 0.20778684318065643,
      "learning_rate": 1.0136666666666667e-05,
      "loss": 0.0014,
      "step": 119590
    },
    {
      "epoch": 6.378666666666667,
      "grad_norm": 0.14828290045261383,
      "learning_rate": 1.0133333333333333e-05,
      "loss": 0.0023,
      "step": 119600
    },
    {
      "epoch": 6.3792,
      "grad_norm": 0.119199737906456,
      "learning_rate": 1.0130000000000001e-05,
      "loss": 0.002,
      "step": 119610
    },
    {
      "epoch": 6.379733333333333,
      "grad_norm": 0.09338397532701492,
      "learning_rate": 1.0126666666666667e-05,
      "loss": 0.0019,
      "step": 119620
    },
    {
      "epoch": 6.3802666666666665,
      "grad_norm": 0.09753216803073883,
      "learning_rate": 1.0123333333333334e-05,
      "loss": 0.0024,
      "step": 119630
    },
    {
      "epoch": 6.3808,
      "grad_norm": 0.11968090385198593,
      "learning_rate": 1.012e-05,
      "loss": 0.0013,
      "step": 119640
    },
    {
      "epoch": 6.381333333333333,
      "grad_norm": 0.07216531783342361,
      "learning_rate": 1.0116666666666667e-05,
      "loss": 0.0019,
      "step": 119650
    },
    {
      "epoch": 6.381866666666666,
      "grad_norm": 0.14858733117580414,
      "learning_rate": 1.0113333333333334e-05,
      "loss": 0.0016,
      "step": 119660
    },
    {
      "epoch": 6.3824,
      "grad_norm": 0.290841281414032,
      "learning_rate": 1.011e-05,
      "loss": 0.0018,
      "step": 119670
    },
    {
      "epoch": 6.382933333333334,
      "grad_norm": 0.2956758439540863,
      "learning_rate": 1.0106666666666668e-05,
      "loss": 0.0018,
      "step": 119680
    },
    {
      "epoch": 6.383466666666667,
      "grad_norm": 0.14936032891273499,
      "learning_rate": 1.0103333333333334e-05,
      "loss": 0.003,
      "step": 119690
    },
    {
      "epoch": 6.384,
      "grad_norm": 0.13307158648967743,
      "learning_rate": 1.0100000000000002e-05,
      "loss": 0.0014,
      "step": 119700
    },
    {
      "epoch": 6.384533333333334,
      "grad_norm": 0.03583233803510666,
      "learning_rate": 1.0096666666666666e-05,
      "loss": 0.0024,
      "step": 119710
    },
    {
      "epoch": 6.385066666666667,
      "grad_norm": 0.034784551709890366,
      "learning_rate": 1.0093333333333334e-05,
      "loss": 0.0018,
      "step": 119720
    },
    {
      "epoch": 6.3856,
      "grad_norm": 0.27704688906669617,
      "learning_rate": 1.0090000000000002e-05,
      "loss": 0.0015,
      "step": 119730
    },
    {
      "epoch": 6.386133333333333,
      "grad_norm": 0.0482194721698761,
      "learning_rate": 1.0086666666666666e-05,
      "loss": 0.0018,
      "step": 119740
    },
    {
      "epoch": 6.386666666666667,
      "grad_norm": 0.0512520857155323,
      "learning_rate": 1.0083333333333334e-05,
      "loss": 0.0018,
      "step": 119750
    },
    {
      "epoch": 6.3872,
      "grad_norm": 0.30408960580825806,
      "learning_rate": 1.008e-05,
      "loss": 0.0014,
      "step": 119760
    },
    {
      "epoch": 6.387733333333333,
      "grad_norm": 0.2541016638278961,
      "learning_rate": 1.0076666666666668e-05,
      "loss": 0.0015,
      "step": 119770
    },
    {
      "epoch": 6.3882666666666665,
      "grad_norm": 0.4866214096546173,
      "learning_rate": 1.0073333333333334e-05,
      "loss": 0.0018,
      "step": 119780
    },
    {
      "epoch": 6.3888,
      "grad_norm": 0.18055744469165802,
      "learning_rate": 1.007e-05,
      "loss": 0.0015,
      "step": 119790
    },
    {
      "epoch": 6.389333333333333,
      "grad_norm": 0.03928179293870926,
      "learning_rate": 1.0066666666666668e-05,
      "loss": 0.0021,
      "step": 119800
    },
    {
      "epoch": 6.389866666666666,
      "grad_norm": 0.07742106914520264,
      "learning_rate": 1.0063333333333334e-05,
      "loss": 0.0018,
      "step": 119810
    },
    {
      "epoch": 6.3904,
      "grad_norm": 0.89708012342453,
      "learning_rate": 1.006e-05,
      "loss": 0.0027,
      "step": 119820
    },
    {
      "epoch": 6.390933333333333,
      "grad_norm": 0.04909032583236694,
      "learning_rate": 1.0056666666666666e-05,
      "loss": 0.0027,
      "step": 119830
    },
    {
      "epoch": 6.391466666666667,
      "grad_norm": 0.08054324239492416,
      "learning_rate": 1.0053333333333334e-05,
      "loss": 0.0013,
      "step": 119840
    },
    {
      "epoch": 6.392,
      "grad_norm": 0.12157673388719559,
      "learning_rate": 1.005e-05,
      "loss": 0.0019,
      "step": 119850
    },
    {
      "epoch": 6.392533333333334,
      "grad_norm": 0.41977229714393616,
      "learning_rate": 1.0046666666666666e-05,
      "loss": 0.0017,
      "step": 119860
    },
    {
      "epoch": 6.393066666666667,
      "grad_norm": 0.09373901784420013,
      "learning_rate": 1.0043333333333334e-05,
      "loss": 0.0013,
      "step": 119870
    },
    {
      "epoch": 6.3936,
      "grad_norm": 0.2144433706998825,
      "learning_rate": 1.004e-05,
      "loss": 0.0013,
      "step": 119880
    },
    {
      "epoch": 6.3941333333333334,
      "grad_norm": 0.47567182779312134,
      "learning_rate": 1.0036666666666668e-05,
      "loss": 0.002,
      "step": 119890
    },
    {
      "epoch": 6.394666666666667,
      "grad_norm": 0.4812152087688446,
      "learning_rate": 1.0033333333333333e-05,
      "loss": 0.0017,
      "step": 119900
    },
    {
      "epoch": 6.3952,
      "grad_norm": 0.4513505697250366,
      "learning_rate": 1.003e-05,
      "loss": 0.0018,
      "step": 119910
    },
    {
      "epoch": 6.395733333333333,
      "grad_norm": 0.1017119437456131,
      "learning_rate": 1.0026666666666668e-05,
      "loss": 0.0017,
      "step": 119920
    },
    {
      "epoch": 6.3962666666666665,
      "grad_norm": 0.04523199424147606,
      "learning_rate": 1.0023333333333333e-05,
      "loss": 0.0028,
      "step": 119930
    },
    {
      "epoch": 6.3968,
      "grad_norm": 0.2822076976299286,
      "learning_rate": 1.002e-05,
      "loss": 0.0027,
      "step": 119940
    },
    {
      "epoch": 6.397333333333333,
      "grad_norm": 0.06918282806873322,
      "learning_rate": 1.0016666666666667e-05,
      "loss": 0.0014,
      "step": 119950
    },
    {
      "epoch": 6.397866666666666,
      "grad_norm": 0.42369163036346436,
      "learning_rate": 1.0013333333333335e-05,
      "loss": 0.0015,
      "step": 119960
    },
    {
      "epoch": 6.3984,
      "grad_norm": 0.09357145428657532,
      "learning_rate": 1.001e-05,
      "loss": 0.0016,
      "step": 119970
    },
    {
      "epoch": 6.398933333333333,
      "grad_norm": 0.4512649476528168,
      "learning_rate": 1.0006666666666667e-05,
      "loss": 0.0025,
      "step": 119980
    },
    {
      "epoch": 6.399466666666667,
      "grad_norm": 0.15271368622779846,
      "learning_rate": 1.0003333333333335e-05,
      "loss": 0.0015,
      "step": 119990
    },
    {
      "epoch": 6.4,
      "grad_norm": 0.30513787269592285,
      "learning_rate": 1e-05,
      "loss": 0.0019,
      "step": 120000
    },
    {
      "epoch": 6.400533333333334,
      "grad_norm": 0.20908598601818085,
      "learning_rate": 9.996666666666667e-06,
      "loss": 0.0017,
      "step": 120010
    },
    {
      "epoch": 6.401066666666667,
      "grad_norm": 0.44621384143829346,
      "learning_rate": 9.993333333333333e-06,
      "loss": 0.0021,
      "step": 120020
    },
    {
      "epoch": 6.4016,
      "grad_norm": 0.24290351569652557,
      "learning_rate": 9.990000000000001e-06,
      "loss": 0.0021,
      "step": 120030
    },
    {
      "epoch": 6.4021333333333335,
      "grad_norm": 0.363947331905365,
      "learning_rate": 9.986666666666667e-06,
      "loss": 0.0018,
      "step": 120040
    },
    {
      "epoch": 6.402666666666667,
      "grad_norm": 0.3197658061981201,
      "learning_rate": 9.983333333333333e-06,
      "loss": 0.0014,
      "step": 120050
    },
    {
      "epoch": 6.4032,
      "grad_norm": 0.32903391122817993,
      "learning_rate": 9.980000000000001e-06,
      "loss": 0.0022,
      "step": 120060
    },
    {
      "epoch": 6.403733333333333,
      "grad_norm": 0.2985970675945282,
      "learning_rate": 9.976666666666667e-06,
      "loss": 0.0016,
      "step": 120070
    },
    {
      "epoch": 6.4042666666666666,
      "grad_norm": 0.3481125235557556,
      "learning_rate": 9.973333333333333e-06,
      "loss": 0.002,
      "step": 120080
    },
    {
      "epoch": 6.4048,
      "grad_norm": 0.32124343514442444,
      "learning_rate": 9.97e-06,
      "loss": 0.0015,
      "step": 120090
    },
    {
      "epoch": 6.405333333333333,
      "grad_norm": 0.2028602808713913,
      "learning_rate": 9.966666666666667e-06,
      "loss": 0.0014,
      "step": 120100
    },
    {
      "epoch": 6.405866666666666,
      "grad_norm": 0.08074143528938293,
      "learning_rate": 9.963333333333335e-06,
      "loss": 0.0022,
      "step": 120110
    },
    {
      "epoch": 6.4064,
      "grad_norm": 0.2605033814907074,
      "learning_rate": 9.96e-06,
      "loss": 0.0013,
      "step": 120120
    },
    {
      "epoch": 6.406933333333333,
      "grad_norm": 0.13512307405471802,
      "learning_rate": 9.956666666666667e-06,
      "loss": 0.0016,
      "step": 120130
    },
    {
      "epoch": 6.407466666666666,
      "grad_norm": 0.09326816350221634,
      "learning_rate": 9.953333333333333e-06,
      "loss": 0.0019,
      "step": 120140
    },
    {
      "epoch": 6.408,
      "grad_norm": 0.0911514088511467,
      "learning_rate": 9.950000000000001e-06,
      "loss": 0.0023,
      "step": 120150
    },
    {
      "epoch": 6.408533333333334,
      "grad_norm": 0.21445561945438385,
      "learning_rate": 9.946666666666667e-06,
      "loss": 0.0012,
      "step": 120160
    },
    {
      "epoch": 6.409066666666667,
      "grad_norm": 0.04381910711526871,
      "learning_rate": 9.943333333333334e-06,
      "loss": 0.0016,
      "step": 120170
    },
    {
      "epoch": 6.4096,
      "grad_norm": 0.03798440098762512,
      "learning_rate": 9.940000000000001e-06,
      "loss": 0.0015,
      "step": 120180
    },
    {
      "epoch": 6.4101333333333335,
      "grad_norm": 0.38693615794181824,
      "learning_rate": 9.936666666666668e-06,
      "loss": 0.0016,
      "step": 120190
    },
    {
      "epoch": 6.410666666666667,
      "grad_norm": 0.03402417153120041,
      "learning_rate": 9.933333333333334e-06,
      "loss": 0.0018,
      "step": 120200
    },
    {
      "epoch": 6.4112,
      "grad_norm": 0.30041226744651794,
      "learning_rate": 9.93e-06,
      "loss": 0.0017,
      "step": 120210
    },
    {
      "epoch": 6.411733333333333,
      "grad_norm": 0.054404519498348236,
      "learning_rate": 9.926666666666668e-06,
      "loss": 0.0018,
      "step": 120220
    },
    {
      "epoch": 6.412266666666667,
      "grad_norm": 0.1371012181043625,
      "learning_rate": 9.923333333333334e-06,
      "loss": 0.0018,
      "step": 120230
    },
    {
      "epoch": 6.4128,
      "grad_norm": 0.18116231262683868,
      "learning_rate": 9.92e-06,
      "loss": 0.0016,
      "step": 120240
    },
    {
      "epoch": 6.413333333333333,
      "grad_norm": 0.35062986612319946,
      "learning_rate": 9.916666666666668e-06,
      "loss": 0.0029,
      "step": 120250
    },
    {
      "epoch": 6.413866666666666,
      "grad_norm": 0.04488067328929901,
      "learning_rate": 9.913333333333334e-06,
      "loss": 0.0018,
      "step": 120260
    },
    {
      "epoch": 6.4144,
      "grad_norm": 0.3235316872596741,
      "learning_rate": 9.91e-06,
      "loss": 0.002,
      "step": 120270
    },
    {
      "epoch": 6.414933333333333,
      "grad_norm": 0.417738139629364,
      "learning_rate": 9.906666666666666e-06,
      "loss": 0.002,
      "step": 120280
    },
    {
      "epoch": 6.415466666666667,
      "grad_norm": 0.08945057541131973,
      "learning_rate": 9.903333333333334e-06,
      "loss": 0.0015,
      "step": 120290
    },
    {
      "epoch": 6.416,
      "grad_norm": 0.12719763815402985,
      "learning_rate": 9.900000000000002e-06,
      "loss": 0.0027,
      "step": 120300
    },
    {
      "epoch": 6.416533333333334,
      "grad_norm": 0.14771708846092224,
      "learning_rate": 9.896666666666666e-06,
      "loss": 0.0018,
      "step": 120310
    },
    {
      "epoch": 6.417066666666667,
      "grad_norm": 0.205842062830925,
      "learning_rate": 9.893333333333334e-06,
      "loss": 0.0021,
      "step": 120320
    },
    {
      "epoch": 6.4176,
      "grad_norm": 0.10346320271492004,
      "learning_rate": 9.89e-06,
      "loss": 0.0017,
      "step": 120330
    },
    {
      "epoch": 6.4181333333333335,
      "grad_norm": 0.2953452467918396,
      "learning_rate": 9.886666666666668e-06,
      "loss": 0.0018,
      "step": 120340
    },
    {
      "epoch": 6.418666666666667,
      "grad_norm": 0.44005438685417175,
      "learning_rate": 9.883333333333334e-06,
      "loss": 0.0017,
      "step": 120350
    },
    {
      "epoch": 6.4192,
      "grad_norm": 0.09847252815961838,
      "learning_rate": 9.88e-06,
      "loss": 0.0014,
      "step": 120360
    },
    {
      "epoch": 6.419733333333333,
      "grad_norm": 0.10102059692144394,
      "learning_rate": 9.876666666666668e-06,
      "loss": 0.0014,
      "step": 120370
    },
    {
      "epoch": 6.420266666666667,
      "grad_norm": 0.2688003182411194,
      "learning_rate": 9.873333333333334e-06,
      "loss": 0.0017,
      "step": 120380
    },
    {
      "epoch": 6.4208,
      "grad_norm": 0.3749062418937683,
      "learning_rate": 9.87e-06,
      "loss": 0.002,
      "step": 120390
    },
    {
      "epoch": 6.421333333333333,
      "grad_norm": 0.27608737349510193,
      "learning_rate": 9.866666666666667e-06,
      "loss": 0.0018,
      "step": 120400
    },
    {
      "epoch": 6.421866666666666,
      "grad_norm": 0.12245262414216995,
      "learning_rate": 9.863333333333334e-06,
      "loss": 0.0012,
      "step": 120410
    },
    {
      "epoch": 6.4224,
      "grad_norm": 0.05425238981842995,
      "learning_rate": 9.86e-06,
      "loss": 0.0024,
      "step": 120420
    },
    {
      "epoch": 6.422933333333333,
      "grad_norm": 0.0897374376654625,
      "learning_rate": 9.856666666666667e-06,
      "loss": 0.0014,
      "step": 120430
    },
    {
      "epoch": 6.423466666666666,
      "grad_norm": 0.33062127232551575,
      "learning_rate": 9.853333333333334e-06,
      "loss": 0.003,
      "step": 120440
    },
    {
      "epoch": 6.424,
      "grad_norm": 0.29204511642456055,
      "learning_rate": 9.85e-06,
      "loss": 0.0014,
      "step": 120450
    },
    {
      "epoch": 6.424533333333334,
      "grad_norm": 0.18371915817260742,
      "learning_rate": 9.846666666666667e-06,
      "loss": 0.0024,
      "step": 120460
    },
    {
      "epoch": 6.425066666666667,
      "grad_norm": 0.12023293972015381,
      "learning_rate": 9.843333333333333e-06,
      "loss": 0.0014,
      "step": 120470
    },
    {
      "epoch": 6.4256,
      "grad_norm": 0.08313857764005661,
      "learning_rate": 9.84e-06,
      "loss": 0.0014,
      "step": 120480
    },
    {
      "epoch": 6.4261333333333335,
      "grad_norm": 0.2947433292865753,
      "learning_rate": 9.836666666666668e-06,
      "loss": 0.0015,
      "step": 120490
    },
    {
      "epoch": 6.426666666666667,
      "grad_norm": 0.17715640366077423,
      "learning_rate": 9.833333333333333e-06,
      "loss": 0.0021,
      "step": 120500
    },
    {
      "epoch": 6.4272,
      "grad_norm": 0.4016270637512207,
      "learning_rate": 9.83e-06,
      "loss": 0.0015,
      "step": 120510
    },
    {
      "epoch": 6.427733333333333,
      "grad_norm": 0.12763291597366333,
      "learning_rate": 9.826666666666667e-06,
      "loss": 0.0015,
      "step": 120520
    },
    {
      "epoch": 6.428266666666667,
      "grad_norm": 0.1751364767551422,
      "learning_rate": 9.823333333333335e-06,
      "loss": 0.0013,
      "step": 120530
    },
    {
      "epoch": 6.4288,
      "grad_norm": 0.18798451125621796,
      "learning_rate": 9.820000000000001e-06,
      "loss": 0.0013,
      "step": 120540
    },
    {
      "epoch": 6.429333333333333,
      "grad_norm": 0.4724961221218109,
      "learning_rate": 9.816666666666667e-06,
      "loss": 0.0015,
      "step": 120550
    },
    {
      "epoch": 6.429866666666666,
      "grad_norm": 0.290031373500824,
      "learning_rate": 9.813333333333335e-06,
      "loss": 0.0016,
      "step": 120560
    },
    {
      "epoch": 6.4304,
      "grad_norm": 0.2139872908592224,
      "learning_rate": 9.810000000000001e-06,
      "loss": 0.002,
      "step": 120570
    },
    {
      "epoch": 6.430933333333333,
      "grad_norm": 0.09293539822101593,
      "learning_rate": 9.806666666666667e-06,
      "loss": 0.0015,
      "step": 120580
    },
    {
      "epoch": 6.431466666666667,
      "grad_norm": 0.07576664537191391,
      "learning_rate": 9.803333333333333e-06,
      "loss": 0.0015,
      "step": 120590
    },
    {
      "epoch": 6.432,
      "grad_norm": 0.030256114900112152,
      "learning_rate": 9.800000000000001e-06,
      "loss": 0.0018,
      "step": 120600
    },
    {
      "epoch": 6.432533333333334,
      "grad_norm": 0.23995305597782135,
      "learning_rate": 9.796666666666667e-06,
      "loss": 0.0014,
      "step": 120610
    },
    {
      "epoch": 6.433066666666667,
      "grad_norm": 0.04607761278748512,
      "learning_rate": 9.793333333333333e-06,
      "loss": 0.0021,
      "step": 120620
    },
    {
      "epoch": 6.4336,
      "grad_norm": 0.2412722259759903,
      "learning_rate": 9.790000000000001e-06,
      "loss": 0.0016,
      "step": 120630
    },
    {
      "epoch": 6.4341333333333335,
      "grad_norm": 0.23178987205028534,
      "learning_rate": 9.786666666666667e-06,
      "loss": 0.0017,
      "step": 120640
    },
    {
      "epoch": 6.434666666666667,
      "grad_norm": 0.21013985574245453,
      "learning_rate": 9.783333333333333e-06,
      "loss": 0.0016,
      "step": 120650
    },
    {
      "epoch": 6.4352,
      "grad_norm": 0.06904183328151703,
      "learning_rate": 9.78e-06,
      "loss": 0.0019,
      "step": 120660
    },
    {
      "epoch": 6.435733333333333,
      "grad_norm": 0.14779576659202576,
      "learning_rate": 9.776666666666667e-06,
      "loss": 0.0015,
      "step": 120670
    },
    {
      "epoch": 6.436266666666667,
      "grad_norm": 0.3077095150947571,
      "learning_rate": 9.773333333333333e-06,
      "loss": 0.0015,
      "step": 120680
    },
    {
      "epoch": 6.4368,
      "grad_norm": 0.06350010633468628,
      "learning_rate": 9.77e-06,
      "loss": 0.0014,
      "step": 120690
    },
    {
      "epoch": 6.437333333333333,
      "grad_norm": 0.3523349463939667,
      "learning_rate": 9.766666666666667e-06,
      "loss": 0.0021,
      "step": 120700
    },
    {
      "epoch": 6.437866666666666,
      "grad_norm": 0.1902824491262436,
      "learning_rate": 9.763333333333334e-06,
      "loss": 0.0019,
      "step": 120710
    },
    {
      "epoch": 6.4384,
      "grad_norm": 0.3211330771446228,
      "learning_rate": 9.760000000000001e-06,
      "loss": 0.0016,
      "step": 120720
    },
    {
      "epoch": 6.438933333333333,
      "grad_norm": 0.07133567333221436,
      "learning_rate": 9.756666666666668e-06,
      "loss": 0.0022,
      "step": 120730
    },
    {
      "epoch": 6.439466666666666,
      "grad_norm": 0.16124586760997772,
      "learning_rate": 9.753333333333334e-06,
      "loss": 0.0018,
      "step": 120740
    },
    {
      "epoch": 6.44,
      "grad_norm": 0.04825829714536667,
      "learning_rate": 9.750000000000002e-06,
      "loss": 0.0014,
      "step": 120750
    },
    {
      "epoch": 6.440533333333334,
      "grad_norm": 0.0879446342587471,
      "learning_rate": 9.746666666666666e-06,
      "loss": 0.0023,
      "step": 120760
    },
    {
      "epoch": 6.441066666666667,
      "grad_norm": 0.04314444959163666,
      "learning_rate": 9.743333333333334e-06,
      "loss": 0.0029,
      "step": 120770
    },
    {
      "epoch": 6.4416,
      "grad_norm": 0.31282711029052734,
      "learning_rate": 9.74e-06,
      "loss": 0.0017,
      "step": 120780
    },
    {
      "epoch": 6.4421333333333335,
      "grad_norm": 0.031359780579805374,
      "learning_rate": 9.736666666666668e-06,
      "loss": 0.0017,
      "step": 120790
    },
    {
      "epoch": 6.442666666666667,
      "grad_norm": 0.14300242066383362,
      "learning_rate": 9.733333333333334e-06,
      "loss": 0.0032,
      "step": 120800
    },
    {
      "epoch": 6.4432,
      "grad_norm": 0.21441033482551575,
      "learning_rate": 9.73e-06,
      "loss": 0.0025,
      "step": 120810
    },
    {
      "epoch": 6.443733333333333,
      "grad_norm": 0.14996761083602905,
      "learning_rate": 9.726666666666668e-06,
      "loss": 0.0028,
      "step": 120820
    },
    {
      "epoch": 6.444266666666667,
      "grad_norm": 0.06297853589057922,
      "learning_rate": 9.723333333333334e-06,
      "loss": 0.0021,
      "step": 120830
    },
    {
      "epoch": 6.4448,
      "grad_norm": 0.1802539974451065,
      "learning_rate": 9.72e-06,
      "loss": 0.0017,
      "step": 120840
    },
    {
      "epoch": 6.445333333333333,
      "grad_norm": 0.38206982612609863,
      "learning_rate": 9.716666666666666e-06,
      "loss": 0.0014,
      "step": 120850
    },
    {
      "epoch": 6.445866666666666,
      "grad_norm": 0.165458083152771,
      "learning_rate": 9.713333333333334e-06,
      "loss": 0.0017,
      "step": 120860
    },
    {
      "epoch": 6.4464,
      "grad_norm": 0.20778951048851013,
      "learning_rate": 9.71e-06,
      "loss": 0.0016,
      "step": 120870
    },
    {
      "epoch": 6.446933333333333,
      "grad_norm": 0.21011967957019806,
      "learning_rate": 9.706666666666666e-06,
      "loss": 0.0024,
      "step": 120880
    },
    {
      "epoch": 6.447466666666667,
      "grad_norm": 0.4266935884952545,
      "learning_rate": 9.703333333333334e-06,
      "loss": 0.0014,
      "step": 120890
    },
    {
      "epoch": 6.448,
      "grad_norm": 0.055011969059705734,
      "learning_rate": 9.7e-06,
      "loss": 0.0014,
      "step": 120900
    },
    {
      "epoch": 6.448533333333334,
      "grad_norm": 0.20871654152870178,
      "learning_rate": 9.696666666666668e-06,
      "loss": 0.0019,
      "step": 120910
    },
    {
      "epoch": 6.449066666666667,
      "grad_norm": 0.0795910507440567,
      "learning_rate": 9.693333333333334e-06,
      "loss": 0.0018,
      "step": 120920
    },
    {
      "epoch": 6.4496,
      "grad_norm": 0.20490624010562897,
      "learning_rate": 9.69e-06,
      "loss": 0.0025,
      "step": 120930
    },
    {
      "epoch": 6.4501333333333335,
      "grad_norm": 0.2646094560623169,
      "learning_rate": 9.686666666666668e-06,
      "loss": 0.0027,
      "step": 120940
    },
    {
      "epoch": 6.450666666666667,
      "grad_norm": 0.040117956697940826,
      "learning_rate": 9.683333333333333e-06,
      "loss": 0.0016,
      "step": 120950
    },
    {
      "epoch": 6.4512,
      "grad_norm": 0.1747310310602188,
      "learning_rate": 9.68e-06,
      "loss": 0.0013,
      "step": 120960
    },
    {
      "epoch": 6.451733333333333,
      "grad_norm": 0.0358654148876667,
      "learning_rate": 9.676666666666667e-06,
      "loss": 0.0017,
      "step": 120970
    },
    {
      "epoch": 6.452266666666667,
      "grad_norm": 0.037365395575761795,
      "learning_rate": 9.673333333333334e-06,
      "loss": 0.0022,
      "step": 120980
    },
    {
      "epoch": 6.4528,
      "grad_norm": 0.18453584611415863,
      "learning_rate": 9.67e-06,
      "loss": 0.0015,
      "step": 120990
    },
    {
      "epoch": 6.453333333333333,
      "grad_norm": 0.2654391825199127,
      "learning_rate": 9.666666666666667e-06,
      "loss": 0.0015,
      "step": 121000
    },
    {
      "epoch": 6.453866666666666,
      "grad_norm": 0.4588133692741394,
      "learning_rate": 9.663333333333335e-06,
      "loss": 0.0018,
      "step": 121010
    },
    {
      "epoch": 6.4544,
      "grad_norm": 0.22655712068080902,
      "learning_rate": 9.66e-06,
      "loss": 0.0018,
      "step": 121020
    },
    {
      "epoch": 6.454933333333333,
      "grad_norm": 0.5554046034812927,
      "learning_rate": 9.656666666666667e-06,
      "loss": 0.0022,
      "step": 121030
    },
    {
      "epoch": 6.455466666666666,
      "grad_norm": 0.35958951711654663,
      "learning_rate": 9.653333333333333e-06,
      "loss": 0.0023,
      "step": 121040
    },
    {
      "epoch": 6.456,
      "grad_norm": 0.23502466082572937,
      "learning_rate": 9.65e-06,
      "loss": 0.0023,
      "step": 121050
    },
    {
      "epoch": 6.456533333333334,
      "grad_norm": 0.16286952793598175,
      "learning_rate": 9.646666666666667e-06,
      "loss": 0.0013,
      "step": 121060
    },
    {
      "epoch": 6.457066666666667,
      "grad_norm": 0.2401532083749771,
      "learning_rate": 9.643333333333333e-06,
      "loss": 0.002,
      "step": 121070
    },
    {
      "epoch": 6.4576,
      "grad_norm": 0.2934047281742096,
      "learning_rate": 9.640000000000001e-06,
      "loss": 0.0012,
      "step": 121080
    },
    {
      "epoch": 6.4581333333333335,
      "grad_norm": 0.14003108441829681,
      "learning_rate": 9.636666666666667e-06,
      "loss": 0.002,
      "step": 121090
    },
    {
      "epoch": 6.458666666666667,
      "grad_norm": 0.09354150295257568,
      "learning_rate": 9.633333333333335e-06,
      "loss": 0.0017,
      "step": 121100
    },
    {
      "epoch": 6.4592,
      "grad_norm": 0.19987985491752625,
      "learning_rate": 9.630000000000001e-06,
      "loss": 0.0019,
      "step": 121110
    },
    {
      "epoch": 6.459733333333333,
      "grad_norm": 0.029935302212834358,
      "learning_rate": 9.626666666666667e-06,
      "loss": 0.0018,
      "step": 121120
    },
    {
      "epoch": 6.460266666666667,
      "grad_norm": 0.3226625919342041,
      "learning_rate": 9.623333333333335e-06,
      "loss": 0.0018,
      "step": 121130
    },
    {
      "epoch": 6.4608,
      "grad_norm": 0.24139627814292908,
      "learning_rate": 9.62e-06,
      "loss": 0.0032,
      "step": 121140
    },
    {
      "epoch": 6.461333333333333,
      "grad_norm": 0.06437859684228897,
      "learning_rate": 9.616666666666667e-06,
      "loss": 0.0011,
      "step": 121150
    },
    {
      "epoch": 6.461866666666666,
      "grad_norm": 0.3233692944049835,
      "learning_rate": 9.613333333333333e-06,
      "loss": 0.0017,
      "step": 121160
    },
    {
      "epoch": 6.4624,
      "grad_norm": 0.09535381942987442,
      "learning_rate": 9.610000000000001e-06,
      "loss": 0.0024,
      "step": 121170
    },
    {
      "epoch": 6.462933333333333,
      "grad_norm": 0.5153428316116333,
      "learning_rate": 9.606666666666667e-06,
      "loss": 0.0019,
      "step": 121180
    },
    {
      "epoch": 6.463466666666667,
      "grad_norm": 0.059118904173374176,
      "learning_rate": 9.603333333333333e-06,
      "loss": 0.002,
      "step": 121190
    },
    {
      "epoch": 6.464,
      "grad_norm": 0.04331754148006439,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.0016,
      "step": 121200
    },
    {
      "epoch": 6.464533333333334,
      "grad_norm": 0.21700608730316162,
      "learning_rate": 9.596666666666667e-06,
      "loss": 0.0019,
      "step": 121210
    },
    {
      "epoch": 6.465066666666667,
      "grad_norm": 0.60303795337677,
      "learning_rate": 9.593333333333334e-06,
      "loss": 0.0022,
      "step": 121220
    },
    {
      "epoch": 6.4656,
      "grad_norm": 0.026497185230255127,
      "learning_rate": 9.59e-06,
      "loss": 0.0019,
      "step": 121230
    },
    {
      "epoch": 6.4661333333333335,
      "grad_norm": 0.06546880304813385,
      "learning_rate": 9.586666666666667e-06,
      "loss": 0.0024,
      "step": 121240
    },
    {
      "epoch": 6.466666666666667,
      "grad_norm": 0.39312297105789185,
      "learning_rate": 9.583333333333334e-06,
      "loss": 0.002,
      "step": 121250
    },
    {
      "epoch": 6.4672,
      "grad_norm": 0.3388802707195282,
      "learning_rate": 9.58e-06,
      "loss": 0.0015,
      "step": 121260
    },
    {
      "epoch": 6.467733333333333,
      "grad_norm": 0.17664982378482819,
      "learning_rate": 9.576666666666668e-06,
      "loss": 0.0017,
      "step": 121270
    },
    {
      "epoch": 6.468266666666667,
      "grad_norm": 0.20519870519638062,
      "learning_rate": 9.573333333333334e-06,
      "loss": 0.0022,
      "step": 121280
    },
    {
      "epoch": 6.4688,
      "grad_norm": 0.16542166471481323,
      "learning_rate": 9.57e-06,
      "loss": 0.0022,
      "step": 121290
    },
    {
      "epoch": 6.469333333333333,
      "grad_norm": 0.3430657982826233,
      "learning_rate": 9.566666666666666e-06,
      "loss": 0.0015,
      "step": 121300
    },
    {
      "epoch": 6.469866666666666,
      "grad_norm": 0.02321925014257431,
      "learning_rate": 9.563333333333334e-06,
      "loss": 0.0017,
      "step": 121310
    },
    {
      "epoch": 6.4704,
      "grad_norm": 0.29366031289100647,
      "learning_rate": 9.560000000000002e-06,
      "loss": 0.0012,
      "step": 121320
    },
    {
      "epoch": 6.470933333333333,
      "grad_norm": 0.08720598369836807,
      "learning_rate": 9.556666666666666e-06,
      "loss": 0.0023,
      "step": 121330
    },
    {
      "epoch": 6.471466666666666,
      "grad_norm": 0.18155759572982788,
      "learning_rate": 9.553333333333334e-06,
      "loss": 0.0018,
      "step": 121340
    },
    {
      "epoch": 6.4719999999999995,
      "grad_norm": 0.3199780285358429,
      "learning_rate": 9.55e-06,
      "loss": 0.0014,
      "step": 121350
    },
    {
      "epoch": 6.472533333333334,
      "grad_norm": 0.06753233075141907,
      "learning_rate": 9.546666666666668e-06,
      "loss": 0.0014,
      "step": 121360
    },
    {
      "epoch": 6.473066666666667,
      "grad_norm": 0.12851104140281677,
      "learning_rate": 9.543333333333334e-06,
      "loss": 0.0014,
      "step": 121370
    },
    {
      "epoch": 6.4736,
      "grad_norm": 0.3110496699810028,
      "learning_rate": 9.54e-06,
      "loss": 0.0019,
      "step": 121380
    },
    {
      "epoch": 6.4741333333333335,
      "grad_norm": 0.1557035893201828,
      "learning_rate": 9.536666666666668e-06,
      "loss": 0.0019,
      "step": 121390
    },
    {
      "epoch": 6.474666666666667,
      "grad_norm": 0.41834887862205505,
      "learning_rate": 9.533333333333334e-06,
      "loss": 0.0021,
      "step": 121400
    },
    {
      "epoch": 6.4752,
      "grad_norm": 0.09782855212688446,
      "learning_rate": 9.53e-06,
      "loss": 0.0021,
      "step": 121410
    },
    {
      "epoch": 6.475733333333333,
      "grad_norm": 0.35118094086647034,
      "learning_rate": 9.526666666666666e-06,
      "loss": 0.0023,
      "step": 121420
    },
    {
      "epoch": 6.476266666666667,
      "grad_norm": 0.043342702090740204,
      "learning_rate": 9.523333333333334e-06,
      "loss": 0.0013,
      "step": 121430
    },
    {
      "epoch": 6.4768,
      "grad_norm": 0.26015686988830566,
      "learning_rate": 9.52e-06,
      "loss": 0.0015,
      "step": 121440
    },
    {
      "epoch": 6.477333333333333,
      "grad_norm": 0.09935881197452545,
      "learning_rate": 9.516666666666666e-06,
      "loss": 0.0016,
      "step": 121450
    },
    {
      "epoch": 6.477866666666666,
      "grad_norm": 0.26736393570899963,
      "learning_rate": 9.513333333333334e-06,
      "loss": 0.0017,
      "step": 121460
    },
    {
      "epoch": 6.4784,
      "grad_norm": 0.20807617902755737,
      "learning_rate": 9.51e-06,
      "loss": 0.0014,
      "step": 121470
    },
    {
      "epoch": 6.478933333333333,
      "grad_norm": 0.49595943093299866,
      "learning_rate": 9.506666666666667e-06,
      "loss": 0.0018,
      "step": 121480
    },
    {
      "epoch": 6.479466666666666,
      "grad_norm": 0.2177043855190277,
      "learning_rate": 9.503333333333333e-06,
      "loss": 0.0019,
      "step": 121490
    },
    {
      "epoch": 6.48,
      "grad_norm": 0.1871977150440216,
      "learning_rate": 9.5e-06,
      "loss": 0.0022,
      "step": 121500
    },
    {
      "epoch": 6.480533333333334,
      "grad_norm": 0.15091757476329803,
      "learning_rate": 9.496666666666668e-06,
      "loss": 0.0013,
      "step": 121510
    },
    {
      "epoch": 6.481066666666667,
      "grad_norm": 0.10961537808179855,
      "learning_rate": 9.493333333333333e-06,
      "loss": 0.0016,
      "step": 121520
    },
    {
      "epoch": 6.4816,
      "grad_norm": 0.3577263653278351,
      "learning_rate": 9.49e-06,
      "loss": 0.002,
      "step": 121530
    },
    {
      "epoch": 6.4821333333333335,
      "grad_norm": 0.06391175836324692,
      "learning_rate": 9.486666666666667e-06,
      "loss": 0.0024,
      "step": 121540
    },
    {
      "epoch": 6.482666666666667,
      "grad_norm": 0.4836333096027374,
      "learning_rate": 9.483333333333335e-06,
      "loss": 0.0015,
      "step": 121550
    },
    {
      "epoch": 6.4832,
      "grad_norm": 0.1597093790769577,
      "learning_rate": 9.48e-06,
      "loss": 0.0023,
      "step": 121560
    },
    {
      "epoch": 6.483733333333333,
      "grad_norm": 0.1801120489835739,
      "learning_rate": 9.476666666666667e-06,
      "loss": 0.0026,
      "step": 121570
    },
    {
      "epoch": 6.484266666666667,
      "grad_norm": 0.21544453501701355,
      "learning_rate": 9.473333333333335e-06,
      "loss": 0.0016,
      "step": 121580
    },
    {
      "epoch": 6.4848,
      "grad_norm": 0.1638956516981125,
      "learning_rate": 9.47e-06,
      "loss": 0.0014,
      "step": 121590
    },
    {
      "epoch": 6.485333333333333,
      "grad_norm": 0.1823205202817917,
      "learning_rate": 9.466666666666667e-06,
      "loss": 0.0014,
      "step": 121600
    },
    {
      "epoch": 6.4858666666666664,
      "grad_norm": 0.14985713362693787,
      "learning_rate": 9.463333333333333e-06,
      "loss": 0.0016,
      "step": 121610
    },
    {
      "epoch": 6.4864,
      "grad_norm": 0.1314494013786316,
      "learning_rate": 9.460000000000001e-06,
      "loss": 0.0023,
      "step": 121620
    },
    {
      "epoch": 6.486933333333333,
      "grad_norm": 0.47315648198127747,
      "learning_rate": 9.456666666666667e-06,
      "loss": 0.0016,
      "step": 121630
    },
    {
      "epoch": 6.487466666666666,
      "grad_norm": 0.10845090448856354,
      "learning_rate": 9.453333333333333e-06,
      "loss": 0.002,
      "step": 121640
    },
    {
      "epoch": 6.4879999999999995,
      "grad_norm": 0.5000731348991394,
      "learning_rate": 9.450000000000001e-06,
      "loss": 0.0015,
      "step": 121650
    },
    {
      "epoch": 6.488533333333334,
      "grad_norm": 0.4203176498413086,
      "learning_rate": 9.446666666666667e-06,
      "loss": 0.0025,
      "step": 121660
    },
    {
      "epoch": 6.489066666666667,
      "grad_norm": 0.08074143528938293,
      "learning_rate": 9.443333333333333e-06,
      "loss": 0.0019,
      "step": 121670
    },
    {
      "epoch": 6.4896,
      "grad_norm": 0.04598128795623779,
      "learning_rate": 9.44e-06,
      "loss": 0.0023,
      "step": 121680
    },
    {
      "epoch": 6.4901333333333335,
      "grad_norm": 0.11991989612579346,
      "learning_rate": 9.436666666666667e-06,
      "loss": 0.0013,
      "step": 121690
    },
    {
      "epoch": 6.490666666666667,
      "grad_norm": 0.3654523491859436,
      "learning_rate": 9.433333333333335e-06,
      "loss": 0.0017,
      "step": 121700
    },
    {
      "epoch": 6.4912,
      "grad_norm": 0.07394837588071823,
      "learning_rate": 9.43e-06,
      "loss": 0.0021,
      "step": 121710
    },
    {
      "epoch": 6.491733333333333,
      "grad_norm": 0.061955682933330536,
      "learning_rate": 9.426666666666667e-06,
      "loss": 0.0018,
      "step": 121720
    },
    {
      "epoch": 6.492266666666667,
      "grad_norm": 0.12088523060083389,
      "learning_rate": 9.423333333333333e-06,
      "loss": 0.0016,
      "step": 121730
    },
    {
      "epoch": 6.4928,
      "grad_norm": 0.2386537492275238,
      "learning_rate": 9.420000000000001e-06,
      "loss": 0.0019,
      "step": 121740
    },
    {
      "epoch": 6.493333333333333,
      "grad_norm": 0.41493797302246094,
      "learning_rate": 9.416666666666667e-06,
      "loss": 0.0021,
      "step": 121750
    },
    {
      "epoch": 6.4938666666666665,
      "grad_norm": 0.06703652441501617,
      "learning_rate": 9.413333333333334e-06,
      "loss": 0.0027,
      "step": 121760
    },
    {
      "epoch": 6.4944,
      "grad_norm": 0.12337322533130646,
      "learning_rate": 9.410000000000001e-06,
      "loss": 0.0023,
      "step": 121770
    },
    {
      "epoch": 6.494933333333333,
      "grad_norm": 0.20768578350543976,
      "learning_rate": 9.406666666666668e-06,
      "loss": 0.0017,
      "step": 121780
    },
    {
      "epoch": 6.495466666666666,
      "grad_norm": 0.1550404131412506,
      "learning_rate": 9.403333333333334e-06,
      "loss": 0.0017,
      "step": 121790
    },
    {
      "epoch": 6.496,
      "grad_norm": 0.03624827414751053,
      "learning_rate": 9.4e-06,
      "loss": 0.0016,
      "step": 121800
    },
    {
      "epoch": 6.496533333333334,
      "grad_norm": 0.19197343289852142,
      "learning_rate": 9.396666666666668e-06,
      "loss": 0.0014,
      "step": 121810
    },
    {
      "epoch": 6.497066666666667,
      "grad_norm": 0.09661485999822617,
      "learning_rate": 9.393333333333334e-06,
      "loss": 0.0023,
      "step": 121820
    },
    {
      "epoch": 6.4976,
      "grad_norm": 0.31999287009239197,
      "learning_rate": 9.39e-06,
      "loss": 0.0014,
      "step": 121830
    },
    {
      "epoch": 6.4981333333333335,
      "grad_norm": 0.027316898107528687,
      "learning_rate": 9.386666666666668e-06,
      "loss": 0.0015,
      "step": 121840
    },
    {
      "epoch": 6.498666666666667,
      "grad_norm": 0.21126611530780792,
      "learning_rate": 9.383333333333334e-06,
      "loss": 0.002,
      "step": 121850
    },
    {
      "epoch": 6.4992,
      "grad_norm": 0.29124149680137634,
      "learning_rate": 9.38e-06,
      "loss": 0.0013,
      "step": 121860
    },
    {
      "epoch": 6.499733333333333,
      "grad_norm": 0.41251492500305176,
      "learning_rate": 9.376666666666666e-06,
      "loss": 0.0011,
      "step": 121870
    },
    {
      "epoch": 6.500266666666667,
      "grad_norm": 0.038718678057193756,
      "learning_rate": 9.373333333333334e-06,
      "loss": 0.0022,
      "step": 121880
    },
    {
      "epoch": 6.5008,
      "grad_norm": 0.12836164236068726,
      "learning_rate": 9.370000000000002e-06,
      "loss": 0.0016,
      "step": 121890
    },
    {
      "epoch": 6.501333333333333,
      "grad_norm": 0.13857696950435638,
      "learning_rate": 9.366666666666666e-06,
      "loss": 0.0014,
      "step": 121900
    },
    {
      "epoch": 6.5018666666666665,
      "grad_norm": 0.09853710234165192,
      "learning_rate": 9.363333333333334e-06,
      "loss": 0.0023,
      "step": 121910
    },
    {
      "epoch": 6.5024,
      "grad_norm": 0.1589287519454956,
      "learning_rate": 9.36e-06,
      "loss": 0.0019,
      "step": 121920
    },
    {
      "epoch": 6.502933333333333,
      "grad_norm": 0.050177451223134995,
      "learning_rate": 9.356666666666668e-06,
      "loss": 0.0021,
      "step": 121930
    },
    {
      "epoch": 6.503466666666666,
      "grad_norm": 0.3764704763889313,
      "learning_rate": 9.353333333333334e-06,
      "loss": 0.0025,
      "step": 121940
    },
    {
      "epoch": 6.504,
      "grad_norm": 0.10374417155981064,
      "learning_rate": 9.35e-06,
      "loss": 0.0014,
      "step": 121950
    },
    {
      "epoch": 6.504533333333333,
      "grad_norm": 0.2127978354692459,
      "learning_rate": 9.346666666666668e-06,
      "loss": 0.0019,
      "step": 121960
    },
    {
      "epoch": 6.505066666666667,
      "grad_norm": 0.22007855772972107,
      "learning_rate": 9.343333333333333e-06,
      "loss": 0.0017,
      "step": 121970
    },
    {
      "epoch": 6.5056,
      "grad_norm": 0.09215933084487915,
      "learning_rate": 9.34e-06,
      "loss": 0.0023,
      "step": 121980
    },
    {
      "epoch": 6.5061333333333335,
      "grad_norm": 0.24515259265899658,
      "learning_rate": 9.336666666666666e-06,
      "loss": 0.0016,
      "step": 121990
    },
    {
      "epoch": 6.506666666666667,
      "grad_norm": 0.39901086688041687,
      "learning_rate": 9.333333333333334e-06,
      "loss": 0.0019,
      "step": 122000
    },
    {
      "epoch": 6.5072,
      "grad_norm": 0.2255229651927948,
      "learning_rate": 9.33e-06,
      "loss": 0.0013,
      "step": 122010
    },
    {
      "epoch": 6.507733333333333,
      "grad_norm": 0.19245344400405884,
      "learning_rate": 9.326666666666667e-06,
      "loss": 0.0016,
      "step": 122020
    },
    {
      "epoch": 6.508266666666667,
      "grad_norm": 0.06879038363695145,
      "learning_rate": 9.323333333333334e-06,
      "loss": 0.0018,
      "step": 122030
    },
    {
      "epoch": 6.5088,
      "grad_norm": 0.38928645849227905,
      "learning_rate": 9.32e-06,
      "loss": 0.0015,
      "step": 122040
    },
    {
      "epoch": 6.509333333333333,
      "grad_norm": 0.2582128047943115,
      "learning_rate": 9.316666666666667e-06,
      "loss": 0.0018,
      "step": 122050
    },
    {
      "epoch": 6.5098666666666665,
      "grad_norm": 0.15108978748321533,
      "learning_rate": 9.313333333333333e-06,
      "loss": 0.0014,
      "step": 122060
    },
    {
      "epoch": 6.5104,
      "grad_norm": 0.2706511616706848,
      "learning_rate": 9.31e-06,
      "loss": 0.0018,
      "step": 122070
    },
    {
      "epoch": 6.510933333333333,
      "grad_norm": 0.15359534323215485,
      "learning_rate": 9.306666666666668e-06,
      "loss": 0.0011,
      "step": 122080
    },
    {
      "epoch": 6.511466666666666,
      "grad_norm": 0.1292286366224289,
      "learning_rate": 9.303333333333333e-06,
      "loss": 0.0016,
      "step": 122090
    },
    {
      "epoch": 6.5120000000000005,
      "grad_norm": 0.1489454209804535,
      "learning_rate": 9.3e-06,
      "loss": 0.0015,
      "step": 122100
    },
    {
      "epoch": 6.512533333333334,
      "grad_norm": 0.3284968137741089,
      "learning_rate": 9.296666666666667e-06,
      "loss": 0.0028,
      "step": 122110
    },
    {
      "epoch": 6.513066666666667,
      "grad_norm": 0.17590849101543427,
      "learning_rate": 9.293333333333335e-06,
      "loss": 0.0018,
      "step": 122120
    },
    {
      "epoch": 6.5136,
      "grad_norm": 0.0935538113117218,
      "learning_rate": 9.29e-06,
      "loss": 0.0017,
      "step": 122130
    },
    {
      "epoch": 6.5141333333333336,
      "grad_norm": 0.04234772175550461,
      "learning_rate": 9.286666666666667e-06,
      "loss": 0.0026,
      "step": 122140
    },
    {
      "epoch": 6.514666666666667,
      "grad_norm": 0.34796467423439026,
      "learning_rate": 9.283333333333335e-06,
      "loss": 0.0017,
      "step": 122150
    },
    {
      "epoch": 6.5152,
      "grad_norm": 0.3428664207458496,
      "learning_rate": 9.28e-06,
      "loss": 0.0022,
      "step": 122160
    },
    {
      "epoch": 6.515733333333333,
      "grad_norm": 0.20514453947544098,
      "learning_rate": 9.276666666666667e-06,
      "loss": 0.0021,
      "step": 122170
    },
    {
      "epoch": 6.516266666666667,
      "grad_norm": 0.2396586835384369,
      "learning_rate": 9.273333333333333e-06,
      "loss": 0.0016,
      "step": 122180
    },
    {
      "epoch": 6.5168,
      "grad_norm": 0.14475640654563904,
      "learning_rate": 9.270000000000001e-06,
      "loss": 0.0012,
      "step": 122190
    },
    {
      "epoch": 6.517333333333333,
      "grad_norm": 0.03718141093850136,
      "learning_rate": 9.266666666666667e-06,
      "loss": 0.0017,
      "step": 122200
    },
    {
      "epoch": 6.5178666666666665,
      "grad_norm": 0.5384551882743835,
      "learning_rate": 9.263333333333333e-06,
      "loss": 0.0019,
      "step": 122210
    },
    {
      "epoch": 6.5184,
      "grad_norm": 0.04527789354324341,
      "learning_rate": 9.260000000000001e-06,
      "loss": 0.0021,
      "step": 122220
    },
    {
      "epoch": 6.518933333333333,
      "grad_norm": 0.19365021586418152,
      "learning_rate": 9.256666666666667e-06,
      "loss": 0.0021,
      "step": 122230
    },
    {
      "epoch": 6.519466666666666,
      "grad_norm": 0.11722326278686523,
      "learning_rate": 9.253333333333333e-06,
      "loss": 0.0016,
      "step": 122240
    },
    {
      "epoch": 6.52,
      "grad_norm": 0.058666031807661057,
      "learning_rate": 9.25e-06,
      "loss": 0.0024,
      "step": 122250
    },
    {
      "epoch": 6.520533333333333,
      "grad_norm": 0.04101360961794853,
      "learning_rate": 9.246666666666667e-06,
      "loss": 0.0025,
      "step": 122260
    },
    {
      "epoch": 6.521066666666667,
      "grad_norm": 0.31752386689186096,
      "learning_rate": 9.243333333333335e-06,
      "loss": 0.0023,
      "step": 122270
    },
    {
      "epoch": 6.5216,
      "grad_norm": 0.23733095824718475,
      "learning_rate": 9.24e-06,
      "loss": 0.0013,
      "step": 122280
    },
    {
      "epoch": 6.522133333333334,
      "grad_norm": 0.24288690090179443,
      "learning_rate": 9.236666666666667e-06,
      "loss": 0.0013,
      "step": 122290
    },
    {
      "epoch": 6.522666666666667,
      "grad_norm": 0.2650890052318573,
      "learning_rate": 9.233333333333334e-06,
      "loss": 0.0015,
      "step": 122300
    },
    {
      "epoch": 6.5232,
      "grad_norm": 0.10527652502059937,
      "learning_rate": 9.23e-06,
      "loss": 0.0024,
      "step": 122310
    },
    {
      "epoch": 6.523733333333333,
      "grad_norm": 0.19508600234985352,
      "learning_rate": 9.226666666666668e-06,
      "loss": 0.0013,
      "step": 122320
    },
    {
      "epoch": 6.524266666666667,
      "grad_norm": 0.148778036236763,
      "learning_rate": 9.223333333333334e-06,
      "loss": 0.0018,
      "step": 122330
    },
    {
      "epoch": 6.5248,
      "grad_norm": 0.17954574525356293,
      "learning_rate": 9.220000000000002e-06,
      "loss": 0.0021,
      "step": 122340
    },
    {
      "epoch": 6.525333333333333,
      "grad_norm": 0.2288825362920761,
      "learning_rate": 9.216666666666666e-06,
      "loss": 0.0018,
      "step": 122350
    },
    {
      "epoch": 6.5258666666666665,
      "grad_norm": 0.27821648120880127,
      "learning_rate": 9.213333333333334e-06,
      "loss": 0.0017,
      "step": 122360
    },
    {
      "epoch": 6.5264,
      "grad_norm": 0.20758022367954254,
      "learning_rate": 9.21e-06,
      "loss": 0.0029,
      "step": 122370
    },
    {
      "epoch": 6.526933333333333,
      "grad_norm": 0.03432825580239296,
      "learning_rate": 9.206666666666668e-06,
      "loss": 0.0019,
      "step": 122380
    },
    {
      "epoch": 6.527466666666666,
      "grad_norm": 0.04423142597079277,
      "learning_rate": 9.203333333333334e-06,
      "loss": 0.0015,
      "step": 122390
    },
    {
      "epoch": 6.5280000000000005,
      "grad_norm": 0.15144862234592438,
      "learning_rate": 9.2e-06,
      "loss": 0.0017,
      "step": 122400
    },
    {
      "epoch": 6.528533333333334,
      "grad_norm": 0.46059584617614746,
      "learning_rate": 9.196666666666668e-06,
      "loss": 0.002,
      "step": 122410
    },
    {
      "epoch": 6.529066666666667,
      "grad_norm": 0.1783577799797058,
      "learning_rate": 9.193333333333334e-06,
      "loss": 0.0018,
      "step": 122420
    },
    {
      "epoch": 6.5296,
      "grad_norm": 0.17826689779758453,
      "learning_rate": 9.19e-06,
      "loss": 0.0015,
      "step": 122430
    },
    {
      "epoch": 6.530133333333334,
      "grad_norm": 0.04393479600548744,
      "learning_rate": 9.186666666666666e-06,
      "loss": 0.002,
      "step": 122440
    },
    {
      "epoch": 6.530666666666667,
      "grad_norm": 0.09590639919042587,
      "learning_rate": 9.183333333333334e-06,
      "loss": 0.0019,
      "step": 122450
    },
    {
      "epoch": 6.5312,
      "grad_norm": 0.07125405967235565,
      "learning_rate": 9.180000000000002e-06,
      "loss": 0.0014,
      "step": 122460
    },
    {
      "epoch": 6.531733333333333,
      "grad_norm": 0.07696538418531418,
      "learning_rate": 9.176666666666666e-06,
      "loss": 0.0011,
      "step": 122470
    },
    {
      "epoch": 6.532266666666667,
      "grad_norm": 0.10451840609312057,
      "learning_rate": 9.173333333333334e-06,
      "loss": 0.0012,
      "step": 122480
    },
    {
      "epoch": 6.5328,
      "grad_norm": 0.4301164448261261,
      "learning_rate": 9.17e-06,
      "loss": 0.0013,
      "step": 122490
    },
    {
      "epoch": 6.533333333333333,
      "grad_norm": 0.09546154737472534,
      "learning_rate": 9.166666666666666e-06,
      "loss": 0.0019,
      "step": 122500
    },
    {
      "epoch": 6.5338666666666665,
      "grad_norm": 0.1226286068558693,
      "learning_rate": 9.163333333333334e-06,
      "loss": 0.0018,
      "step": 122510
    },
    {
      "epoch": 6.5344,
      "grad_norm": 0.2477860003709793,
      "learning_rate": 9.16e-06,
      "loss": 0.0017,
      "step": 122520
    },
    {
      "epoch": 6.534933333333333,
      "grad_norm": 0.1001753881573677,
      "learning_rate": 9.156666666666668e-06,
      "loss": 0.0015,
      "step": 122530
    },
    {
      "epoch": 6.535466666666666,
      "grad_norm": 0.11798372119665146,
      "learning_rate": 9.153333333333333e-06,
      "loss": 0.0018,
      "step": 122540
    },
    {
      "epoch": 6.536,
      "grad_norm": 0.12342651933431625,
      "learning_rate": 9.15e-06,
      "loss": 0.002,
      "step": 122550
    },
    {
      "epoch": 6.536533333333333,
      "grad_norm": 0.20811259746551514,
      "learning_rate": 9.146666666666667e-06,
      "loss": 0.0018,
      "step": 122560
    },
    {
      "epoch": 6.537066666666667,
      "grad_norm": 0.04145665094256401,
      "learning_rate": 9.143333333333334e-06,
      "loss": 0.0016,
      "step": 122570
    },
    {
      "epoch": 6.5376,
      "grad_norm": 0.23587025701999664,
      "learning_rate": 9.14e-06,
      "loss": 0.0033,
      "step": 122580
    },
    {
      "epoch": 6.538133333333334,
      "grad_norm": 0.06772806495428085,
      "learning_rate": 9.136666666666667e-06,
      "loss": 0.0025,
      "step": 122590
    },
    {
      "epoch": 6.538666666666667,
      "grad_norm": 0.06533364206552505,
      "learning_rate": 9.133333333333335e-06,
      "loss": 0.0012,
      "step": 122600
    },
    {
      "epoch": 6.5392,
      "grad_norm": 0.38507917523384094,
      "learning_rate": 9.13e-06,
      "loss": 0.0018,
      "step": 122610
    },
    {
      "epoch": 6.539733333333333,
      "grad_norm": 0.06228775531053543,
      "learning_rate": 9.126666666666667e-06,
      "loss": 0.0013,
      "step": 122620
    },
    {
      "epoch": 6.540266666666667,
      "grad_norm": 0.15263061225414276,
      "learning_rate": 9.123333333333333e-06,
      "loss": 0.0022,
      "step": 122630
    },
    {
      "epoch": 6.5408,
      "grad_norm": 0.15267422795295715,
      "learning_rate": 9.12e-06,
      "loss": 0.0026,
      "step": 122640
    },
    {
      "epoch": 6.541333333333333,
      "grad_norm": 0.2692074775695801,
      "learning_rate": 9.116666666666667e-06,
      "loss": 0.0013,
      "step": 122650
    },
    {
      "epoch": 6.5418666666666665,
      "grad_norm": 0.037239931523799896,
      "learning_rate": 9.113333333333333e-06,
      "loss": 0.0016,
      "step": 122660
    },
    {
      "epoch": 6.5424,
      "grad_norm": 0.23906178772449493,
      "learning_rate": 9.110000000000001e-06,
      "loss": 0.002,
      "step": 122670
    },
    {
      "epoch": 6.542933333333333,
      "grad_norm": 0.04305480793118477,
      "learning_rate": 9.106666666666667e-06,
      "loss": 0.0022,
      "step": 122680
    },
    {
      "epoch": 6.543466666666666,
      "grad_norm": 0.09435568004846573,
      "learning_rate": 9.103333333333333e-06,
      "loss": 0.0013,
      "step": 122690
    },
    {
      "epoch": 6.5440000000000005,
      "grad_norm": 0.07563240081071854,
      "learning_rate": 9.100000000000001e-06,
      "loss": 0.0022,
      "step": 122700
    },
    {
      "epoch": 6.544533333333334,
      "grad_norm": 0.10688038170337677,
      "learning_rate": 9.096666666666667e-06,
      "loss": 0.0017,
      "step": 122710
    },
    {
      "epoch": 6.545066666666667,
      "grad_norm": 0.10444747656583786,
      "learning_rate": 9.093333333333335e-06,
      "loss": 0.0017,
      "step": 122720
    },
    {
      "epoch": 6.5456,
      "grad_norm": 0.06468706578016281,
      "learning_rate": 9.09e-06,
      "loss": 0.0019,
      "step": 122730
    },
    {
      "epoch": 6.546133333333334,
      "grad_norm": 0.5774025917053223,
      "learning_rate": 9.086666666666667e-06,
      "loss": 0.0018,
      "step": 122740
    },
    {
      "epoch": 6.546666666666667,
      "grad_norm": 0.2657940685749054,
      "learning_rate": 9.083333333333333e-06,
      "loss": 0.0015,
      "step": 122750
    },
    {
      "epoch": 6.5472,
      "grad_norm": 0.17050619423389435,
      "learning_rate": 9.080000000000001e-06,
      "loss": 0.0016,
      "step": 122760
    },
    {
      "epoch": 6.547733333333333,
      "grad_norm": 0.23489128053188324,
      "learning_rate": 9.076666666666667e-06,
      "loss": 0.0014,
      "step": 122770
    },
    {
      "epoch": 6.548266666666667,
      "grad_norm": 0.12595725059509277,
      "learning_rate": 9.073333333333333e-06,
      "loss": 0.0014,
      "step": 122780
    },
    {
      "epoch": 6.5488,
      "grad_norm": 0.10351023823022842,
      "learning_rate": 9.070000000000001e-06,
      "loss": 0.002,
      "step": 122790
    },
    {
      "epoch": 6.549333333333333,
      "grad_norm": 0.1772579401731491,
      "learning_rate": 9.066666666666667e-06,
      "loss": 0.0018,
      "step": 122800
    },
    {
      "epoch": 6.5498666666666665,
      "grad_norm": 0.15179769694805145,
      "learning_rate": 9.063333333333334e-06,
      "loss": 0.0015,
      "step": 122810
    },
    {
      "epoch": 6.5504,
      "grad_norm": 0.07128492742776871,
      "learning_rate": 9.06e-06,
      "loss": 0.0022,
      "step": 122820
    },
    {
      "epoch": 6.550933333333333,
      "grad_norm": 0.025040103122591972,
      "learning_rate": 9.056666666666667e-06,
      "loss": 0.0026,
      "step": 122830
    },
    {
      "epoch": 6.551466666666666,
      "grad_norm": 0.06761215627193451,
      "learning_rate": 9.053333333333334e-06,
      "loss": 0.0015,
      "step": 122840
    },
    {
      "epoch": 6.552,
      "grad_norm": 0.2121564894914627,
      "learning_rate": 9.05e-06,
      "loss": 0.0017,
      "step": 122850
    },
    {
      "epoch": 6.552533333333333,
      "grad_norm": 0.3339568078517914,
      "learning_rate": 9.046666666666668e-06,
      "loss": 0.002,
      "step": 122860
    },
    {
      "epoch": 6.553066666666667,
      "grad_norm": 0.32911351323127747,
      "learning_rate": 9.043333333333334e-06,
      "loss": 0.0019,
      "step": 122870
    },
    {
      "epoch": 6.5536,
      "grad_norm": 0.05700751394033432,
      "learning_rate": 9.04e-06,
      "loss": 0.0018,
      "step": 122880
    },
    {
      "epoch": 6.554133333333334,
      "grad_norm": 0.052864570170640945,
      "learning_rate": 9.036666666666668e-06,
      "loss": 0.0025,
      "step": 122890
    },
    {
      "epoch": 6.554666666666667,
      "grad_norm": 0.13577212393283844,
      "learning_rate": 9.033333333333334e-06,
      "loss": 0.0021,
      "step": 122900
    },
    {
      "epoch": 6.5552,
      "grad_norm": 0.37041905522346497,
      "learning_rate": 9.030000000000002e-06,
      "loss": 0.0024,
      "step": 122910
    },
    {
      "epoch": 6.555733333333333,
      "grad_norm": 0.1787191778421402,
      "learning_rate": 9.026666666666666e-06,
      "loss": 0.0014,
      "step": 122920
    },
    {
      "epoch": 6.556266666666667,
      "grad_norm": 0.038095757365226746,
      "learning_rate": 9.023333333333334e-06,
      "loss": 0.0017,
      "step": 122930
    },
    {
      "epoch": 6.5568,
      "grad_norm": 0.28980177640914917,
      "learning_rate": 9.02e-06,
      "loss": 0.0013,
      "step": 122940
    },
    {
      "epoch": 6.557333333333333,
      "grad_norm": 0.09707405418157578,
      "learning_rate": 9.016666666666668e-06,
      "loss": 0.0016,
      "step": 122950
    },
    {
      "epoch": 6.5578666666666665,
      "grad_norm": 0.17805494368076324,
      "learning_rate": 9.013333333333334e-06,
      "loss": 0.0014,
      "step": 122960
    },
    {
      "epoch": 6.5584,
      "grad_norm": 0.12082981318235397,
      "learning_rate": 9.01e-06,
      "loss": 0.0016,
      "step": 122970
    },
    {
      "epoch": 6.558933333333333,
      "grad_norm": 0.13466623425483704,
      "learning_rate": 9.006666666666668e-06,
      "loss": 0.0018,
      "step": 122980
    },
    {
      "epoch": 6.559466666666666,
      "grad_norm": 0.026790186762809753,
      "learning_rate": 9.003333333333334e-06,
      "loss": 0.0015,
      "step": 122990
    },
    {
      "epoch": 6.5600000000000005,
      "grad_norm": 0.09544645249843597,
      "learning_rate": 9e-06,
      "loss": 0.0015,
      "step": 123000
    },
    {
      "epoch": 6.560533333333334,
      "grad_norm": 0.41098615527153015,
      "learning_rate": 8.996666666666666e-06,
      "loss": 0.0016,
      "step": 123010
    },
    {
      "epoch": 6.561066666666667,
      "grad_norm": 0.3349500298500061,
      "learning_rate": 8.993333333333334e-06,
      "loss": 0.0019,
      "step": 123020
    },
    {
      "epoch": 6.5616,
      "grad_norm": 0.05090408772230148,
      "learning_rate": 8.99e-06,
      "loss": 0.0013,
      "step": 123030
    },
    {
      "epoch": 6.562133333333334,
      "grad_norm": 0.12050826847553253,
      "learning_rate": 8.986666666666666e-06,
      "loss": 0.0015,
      "step": 123040
    },
    {
      "epoch": 6.562666666666667,
      "grad_norm": 0.24005381762981415,
      "learning_rate": 8.983333333333334e-06,
      "loss": 0.0024,
      "step": 123050
    },
    {
      "epoch": 6.5632,
      "grad_norm": 0.11852411925792694,
      "learning_rate": 8.98e-06,
      "loss": 0.0028,
      "step": 123060
    },
    {
      "epoch": 6.563733333333333,
      "grad_norm": 0.5334477424621582,
      "learning_rate": 8.976666666666667e-06,
      "loss": 0.0024,
      "step": 123070
    },
    {
      "epoch": 6.564266666666667,
      "grad_norm": 0.027718108147382736,
      "learning_rate": 8.973333333333334e-06,
      "loss": 0.0019,
      "step": 123080
    },
    {
      "epoch": 6.5648,
      "grad_norm": 0.03948577865958214,
      "learning_rate": 8.97e-06,
      "loss": 0.0021,
      "step": 123090
    },
    {
      "epoch": 6.565333333333333,
      "grad_norm": 0.12278345227241516,
      "learning_rate": 8.966666666666668e-06,
      "loss": 0.0017,
      "step": 123100
    },
    {
      "epoch": 6.5658666666666665,
      "grad_norm": 0.07524067163467407,
      "learning_rate": 8.963333333333333e-06,
      "loss": 0.0025,
      "step": 123110
    },
    {
      "epoch": 6.5664,
      "grad_norm": 0.06812290847301483,
      "learning_rate": 8.96e-06,
      "loss": 0.0016,
      "step": 123120
    },
    {
      "epoch": 6.566933333333333,
      "grad_norm": 0.32063671946525574,
      "learning_rate": 8.956666666666667e-06,
      "loss": 0.0014,
      "step": 123130
    },
    {
      "epoch": 6.567466666666666,
      "grad_norm": 0.12271949648857117,
      "learning_rate": 8.953333333333335e-06,
      "loss": 0.002,
      "step": 123140
    },
    {
      "epoch": 6.568,
      "grad_norm": 0.23638023436069489,
      "learning_rate": 8.95e-06,
      "loss": 0.0015,
      "step": 123150
    },
    {
      "epoch": 6.568533333333333,
      "grad_norm": 0.09009969979524612,
      "learning_rate": 8.946666666666667e-06,
      "loss": 0.0016,
      "step": 123160
    },
    {
      "epoch": 6.569066666666667,
      "grad_norm": 0.07416804879903793,
      "learning_rate": 8.943333333333335e-06,
      "loss": 0.0017,
      "step": 123170
    },
    {
      "epoch": 6.5696,
      "grad_norm": 0.3701598346233368,
      "learning_rate": 8.939999999999999e-06,
      "loss": 0.0014,
      "step": 123180
    },
    {
      "epoch": 6.570133333333334,
      "grad_norm": 0.39016294479370117,
      "learning_rate": 8.936666666666667e-06,
      "loss": 0.0011,
      "step": 123190
    },
    {
      "epoch": 6.570666666666667,
      "grad_norm": 0.16765102744102478,
      "learning_rate": 8.933333333333333e-06,
      "loss": 0.0028,
      "step": 123200
    },
    {
      "epoch": 6.5712,
      "grad_norm": 0.33213427662849426,
      "learning_rate": 8.930000000000001e-06,
      "loss": 0.002,
      "step": 123210
    },
    {
      "epoch": 6.571733333333333,
      "grad_norm": 0.11791960150003433,
      "learning_rate": 8.926666666666667e-06,
      "loss": 0.0017,
      "step": 123220
    },
    {
      "epoch": 6.572266666666667,
      "grad_norm": 0.2185477465391159,
      "learning_rate": 8.923333333333333e-06,
      "loss": 0.0024,
      "step": 123230
    },
    {
      "epoch": 6.5728,
      "grad_norm": 0.23944999277591705,
      "learning_rate": 8.920000000000001e-06,
      "loss": 0.0018,
      "step": 123240
    },
    {
      "epoch": 6.573333333333333,
      "grad_norm": 0.06672825664281845,
      "learning_rate": 8.916666666666667e-06,
      "loss": 0.0026,
      "step": 123250
    },
    {
      "epoch": 6.5738666666666665,
      "grad_norm": 0.2692600190639496,
      "learning_rate": 8.913333333333333e-06,
      "loss": 0.002,
      "step": 123260
    },
    {
      "epoch": 6.5744,
      "grad_norm": 0.2648389935493469,
      "learning_rate": 8.910000000000001e-06,
      "loss": 0.002,
      "step": 123270
    },
    {
      "epoch": 6.574933333333333,
      "grad_norm": 0.22888511419296265,
      "learning_rate": 8.906666666666667e-06,
      "loss": 0.002,
      "step": 123280
    },
    {
      "epoch": 6.575466666666666,
      "grad_norm": 0.157855823636055,
      "learning_rate": 8.903333333333335e-06,
      "loss": 0.0014,
      "step": 123290
    },
    {
      "epoch": 6.576,
      "grad_norm": 0.32537150382995605,
      "learning_rate": 8.9e-06,
      "loss": 0.0023,
      "step": 123300
    },
    {
      "epoch": 6.576533333333334,
      "grad_norm": 0.21847133338451385,
      "learning_rate": 8.896666666666667e-06,
      "loss": 0.0021,
      "step": 123310
    },
    {
      "epoch": 6.577066666666667,
      "grad_norm": 0.1797247678041458,
      "learning_rate": 8.893333333333333e-06,
      "loss": 0.0016,
      "step": 123320
    },
    {
      "epoch": 6.5776,
      "grad_norm": 0.06943569332361221,
      "learning_rate": 8.890000000000001e-06,
      "loss": 0.0025,
      "step": 123330
    },
    {
      "epoch": 6.578133333333334,
      "grad_norm": 0.047744497656822205,
      "learning_rate": 8.886666666666667e-06,
      "loss": 0.0022,
      "step": 123340
    },
    {
      "epoch": 6.578666666666667,
      "grad_norm": 0.1511276215314865,
      "learning_rate": 8.883333333333334e-06,
      "loss": 0.0012,
      "step": 123350
    },
    {
      "epoch": 6.5792,
      "grad_norm": 0.16086305677890778,
      "learning_rate": 8.880000000000001e-06,
      "loss": 0.0014,
      "step": 123360
    },
    {
      "epoch": 6.579733333333333,
      "grad_norm": 0.11826471984386444,
      "learning_rate": 8.876666666666666e-06,
      "loss": 0.0024,
      "step": 123370
    },
    {
      "epoch": 6.580266666666667,
      "grad_norm": 0.05729925259947777,
      "learning_rate": 8.873333333333334e-06,
      "loss": 0.0018,
      "step": 123380
    },
    {
      "epoch": 6.5808,
      "grad_norm": 0.47792524099349976,
      "learning_rate": 8.87e-06,
      "loss": 0.0028,
      "step": 123390
    },
    {
      "epoch": 6.581333333333333,
      "grad_norm": 0.3338601887226105,
      "learning_rate": 8.866666666666668e-06,
      "loss": 0.0013,
      "step": 123400
    },
    {
      "epoch": 6.5818666666666665,
      "grad_norm": 0.2936890423297882,
      "learning_rate": 8.863333333333334e-06,
      "loss": 0.0015,
      "step": 123410
    },
    {
      "epoch": 6.5824,
      "grad_norm": 0.16857142746448517,
      "learning_rate": 8.86e-06,
      "loss": 0.0014,
      "step": 123420
    },
    {
      "epoch": 6.582933333333333,
      "grad_norm": 0.12642453610897064,
      "learning_rate": 8.856666666666668e-06,
      "loss": 0.0029,
      "step": 123430
    },
    {
      "epoch": 6.583466666666666,
      "grad_norm": 0.2821503281593323,
      "learning_rate": 8.853333333333334e-06,
      "loss": 0.0014,
      "step": 123440
    },
    {
      "epoch": 6.584,
      "grad_norm": 0.5115873217582703,
      "learning_rate": 8.85e-06,
      "loss": 0.0014,
      "step": 123450
    },
    {
      "epoch": 6.584533333333333,
      "grad_norm": 0.24999243021011353,
      "learning_rate": 8.846666666666668e-06,
      "loss": 0.0014,
      "step": 123460
    },
    {
      "epoch": 6.585066666666666,
      "grad_norm": 0.2982843220233917,
      "learning_rate": 8.843333333333334e-06,
      "loss": 0.0024,
      "step": 123470
    },
    {
      "epoch": 6.5856,
      "grad_norm": 0.12092415988445282,
      "learning_rate": 8.840000000000002e-06,
      "loss": 0.0018,
      "step": 123480
    },
    {
      "epoch": 6.586133333333334,
      "grad_norm": 0.26741674542427063,
      "learning_rate": 8.836666666666666e-06,
      "loss": 0.0031,
      "step": 123490
    },
    {
      "epoch": 6.586666666666667,
      "grad_norm": 0.12461263686418533,
      "learning_rate": 8.833333333333334e-06,
      "loss": 0.002,
      "step": 123500
    },
    {
      "epoch": 6.5872,
      "grad_norm": 0.17479655146598816,
      "learning_rate": 8.83e-06,
      "loss": 0.0017,
      "step": 123510
    },
    {
      "epoch": 6.587733333333333,
      "grad_norm": 0.07081353664398193,
      "learning_rate": 8.826666666666666e-06,
      "loss": 0.0016,
      "step": 123520
    },
    {
      "epoch": 6.588266666666667,
      "grad_norm": 0.15079762041568756,
      "learning_rate": 8.823333333333334e-06,
      "loss": 0.0014,
      "step": 123530
    },
    {
      "epoch": 6.5888,
      "grad_norm": 0.24126332998275757,
      "learning_rate": 8.82e-06,
      "loss": 0.0015,
      "step": 123540
    },
    {
      "epoch": 6.589333333333333,
      "grad_norm": 0.12657982110977173,
      "learning_rate": 8.816666666666668e-06,
      "loss": 0.0013,
      "step": 123550
    },
    {
      "epoch": 6.5898666666666665,
      "grad_norm": 0.18513886630535126,
      "learning_rate": 8.813333333333333e-06,
      "loss": 0.0016,
      "step": 123560
    },
    {
      "epoch": 6.5904,
      "grad_norm": 0.2921423614025116,
      "learning_rate": 8.81e-06,
      "loss": 0.0018,
      "step": 123570
    },
    {
      "epoch": 6.590933333333333,
      "grad_norm": 0.15364782512187958,
      "learning_rate": 8.806666666666666e-06,
      "loss": 0.0015,
      "step": 123580
    },
    {
      "epoch": 6.591466666666666,
      "grad_norm": 0.14801453053951263,
      "learning_rate": 8.803333333333334e-06,
      "loss": 0.0019,
      "step": 123590
    },
    {
      "epoch": 6.592,
      "grad_norm": 0.30098509788513184,
      "learning_rate": 8.8e-06,
      "loss": 0.0015,
      "step": 123600
    },
    {
      "epoch": 6.592533333333334,
      "grad_norm": 0.27444812655448914,
      "learning_rate": 8.796666666666667e-06,
      "loss": 0.0012,
      "step": 123610
    },
    {
      "epoch": 6.593066666666667,
      "grad_norm": 0.22531579434871674,
      "learning_rate": 8.793333333333334e-06,
      "loss": 0.0015,
      "step": 123620
    },
    {
      "epoch": 6.5936,
      "grad_norm": 0.05441080778837204,
      "learning_rate": 8.79e-06,
      "loss": 0.0019,
      "step": 123630
    },
    {
      "epoch": 6.594133333333334,
      "grad_norm": 0.05083828419446945,
      "learning_rate": 8.786666666666667e-06,
      "loss": 0.0025,
      "step": 123640
    },
    {
      "epoch": 6.594666666666667,
      "grad_norm": 0.32785436511039734,
      "learning_rate": 8.783333333333335e-06,
      "loss": 0.0025,
      "step": 123650
    },
    {
      "epoch": 6.5952,
      "grad_norm": 0.12799032032489777,
      "learning_rate": 8.78e-06,
      "loss": 0.0022,
      "step": 123660
    },
    {
      "epoch": 6.5957333333333334,
      "grad_norm": 0.04970746114850044,
      "learning_rate": 8.776666666666668e-06,
      "loss": 0.0016,
      "step": 123670
    },
    {
      "epoch": 6.596266666666667,
      "grad_norm": 0.07817359268665314,
      "learning_rate": 8.773333333333333e-06,
      "loss": 0.0019,
      "step": 123680
    },
    {
      "epoch": 6.5968,
      "grad_norm": 0.22453837096691132,
      "learning_rate": 8.77e-06,
      "loss": 0.0013,
      "step": 123690
    },
    {
      "epoch": 6.597333333333333,
      "grad_norm": 0.15219911932945251,
      "learning_rate": 8.766666666666667e-06,
      "loss": 0.002,
      "step": 123700
    },
    {
      "epoch": 6.5978666666666665,
      "grad_norm": 0.2794734537601471,
      "learning_rate": 8.763333333333333e-06,
      "loss": 0.0028,
      "step": 123710
    },
    {
      "epoch": 6.5984,
      "grad_norm": 0.039650142192840576,
      "learning_rate": 8.76e-06,
      "loss": 0.0014,
      "step": 123720
    },
    {
      "epoch": 6.598933333333333,
      "grad_norm": 0.23346033692359924,
      "learning_rate": 8.756666666666667e-06,
      "loss": 0.0023,
      "step": 123730
    },
    {
      "epoch": 6.599466666666666,
      "grad_norm": 0.21335040032863617,
      "learning_rate": 8.753333333333335e-06,
      "loss": 0.0024,
      "step": 123740
    },
    {
      "epoch": 6.6,
      "grad_norm": 0.14691631495952606,
      "learning_rate": 8.75e-06,
      "loss": 0.0019,
      "step": 123750
    },
    {
      "epoch": 6.600533333333333,
      "grad_norm": 0.07153832912445068,
      "learning_rate": 8.746666666666667e-06,
      "loss": 0.0022,
      "step": 123760
    },
    {
      "epoch": 6.601066666666666,
      "grad_norm": 0.36279794573783875,
      "learning_rate": 8.743333333333333e-06,
      "loss": 0.0025,
      "step": 123770
    },
    {
      "epoch": 6.6016,
      "grad_norm": 0.12321734428405762,
      "learning_rate": 8.740000000000001e-06,
      "loss": 0.0013,
      "step": 123780
    },
    {
      "epoch": 6.602133333333334,
      "grad_norm": 0.07469496876001358,
      "learning_rate": 8.736666666666667e-06,
      "loss": 0.0015,
      "step": 123790
    },
    {
      "epoch": 6.602666666666667,
      "grad_norm": 0.05660161003470421,
      "learning_rate": 8.733333333333333e-06,
      "loss": 0.0015,
      "step": 123800
    },
    {
      "epoch": 6.6032,
      "grad_norm": 0.27326375246047974,
      "learning_rate": 8.730000000000001e-06,
      "loss": 0.0023,
      "step": 123810
    },
    {
      "epoch": 6.6037333333333335,
      "grad_norm": 0.13925839960575104,
      "learning_rate": 8.726666666666667e-06,
      "loss": 0.0019,
      "step": 123820
    },
    {
      "epoch": 6.604266666666667,
      "grad_norm": 0.09032756090164185,
      "learning_rate": 8.723333333333333e-06,
      "loss": 0.0016,
      "step": 123830
    },
    {
      "epoch": 6.6048,
      "grad_norm": 0.21588455140590668,
      "learning_rate": 8.720000000000001e-06,
      "loss": 0.0025,
      "step": 123840
    },
    {
      "epoch": 6.605333333333333,
      "grad_norm": 0.2926998436450958,
      "learning_rate": 8.716666666666667e-06,
      "loss": 0.0023,
      "step": 123850
    },
    {
      "epoch": 6.6058666666666666,
      "grad_norm": 0.10061126202344894,
      "learning_rate": 8.713333333333333e-06,
      "loss": 0.0023,
      "step": 123860
    },
    {
      "epoch": 6.6064,
      "grad_norm": 0.10030725598335266,
      "learning_rate": 8.71e-06,
      "loss": 0.0016,
      "step": 123870
    },
    {
      "epoch": 6.606933333333333,
      "grad_norm": 0.17780616879463196,
      "learning_rate": 8.706666666666667e-06,
      "loss": 0.0014,
      "step": 123880
    },
    {
      "epoch": 6.607466666666666,
      "grad_norm": 0.4555478096008301,
      "learning_rate": 8.703333333333334e-06,
      "loss": 0.0013,
      "step": 123890
    },
    {
      "epoch": 6.608,
      "grad_norm": 0.041887592524290085,
      "learning_rate": 8.7e-06,
      "loss": 0.003,
      "step": 123900
    },
    {
      "epoch": 6.608533333333334,
      "grad_norm": 0.3562317490577698,
      "learning_rate": 8.696666666666668e-06,
      "loss": 0.003,
      "step": 123910
    },
    {
      "epoch": 6.609066666666667,
      "grad_norm": 0.44549888372421265,
      "learning_rate": 8.693333333333334e-06,
      "loss": 0.0015,
      "step": 123920
    },
    {
      "epoch": 6.6096,
      "grad_norm": 0.26347145438194275,
      "learning_rate": 8.690000000000002e-06,
      "loss": 0.0014,
      "step": 123930
    },
    {
      "epoch": 6.610133333333334,
      "grad_norm": 0.559065043926239,
      "learning_rate": 8.686666666666666e-06,
      "loss": 0.0019,
      "step": 123940
    },
    {
      "epoch": 6.610666666666667,
      "grad_norm": 0.06406563520431519,
      "learning_rate": 8.683333333333334e-06,
      "loss": 0.0012,
      "step": 123950
    },
    {
      "epoch": 6.6112,
      "grad_norm": 0.26806092262268066,
      "learning_rate": 8.68e-06,
      "loss": 0.0016,
      "step": 123960
    },
    {
      "epoch": 6.6117333333333335,
      "grad_norm": 0.20750947296619415,
      "learning_rate": 8.676666666666668e-06,
      "loss": 0.0024,
      "step": 123970
    },
    {
      "epoch": 6.612266666666667,
      "grad_norm": 0.3619806468486786,
      "learning_rate": 8.673333333333334e-06,
      "loss": 0.0015,
      "step": 123980
    },
    {
      "epoch": 6.6128,
      "grad_norm": 0.09280088543891907,
      "learning_rate": 8.67e-06,
      "loss": 0.0015,
      "step": 123990
    },
    {
      "epoch": 6.613333333333333,
      "grad_norm": 0.0943344309926033,
      "learning_rate": 8.666666666666668e-06,
      "loss": 0.0016,
      "step": 124000
    },
    {
      "epoch": 6.613866666666667,
      "grad_norm": 0.42305371165275574,
      "learning_rate": 8.663333333333334e-06,
      "loss": 0.0022,
      "step": 124010
    },
    {
      "epoch": 6.6144,
      "grad_norm": 0.6258882284164429,
      "learning_rate": 8.66e-06,
      "loss": 0.0015,
      "step": 124020
    },
    {
      "epoch": 6.614933333333333,
      "grad_norm": 0.191786527633667,
      "learning_rate": 8.656666666666668e-06,
      "loss": 0.0016,
      "step": 124030
    },
    {
      "epoch": 6.615466666666666,
      "grad_norm": 0.35503584146499634,
      "learning_rate": 8.653333333333334e-06,
      "loss": 0.0014,
      "step": 124040
    },
    {
      "epoch": 6.616,
      "grad_norm": 0.12261415272951126,
      "learning_rate": 8.65e-06,
      "loss": 0.002,
      "step": 124050
    },
    {
      "epoch": 6.616533333333333,
      "grad_norm": 0.28192293643951416,
      "learning_rate": 8.646666666666666e-06,
      "loss": 0.0017,
      "step": 124060
    },
    {
      "epoch": 6.617066666666666,
      "grad_norm": 0.12183257937431335,
      "learning_rate": 8.643333333333334e-06,
      "loss": 0.0017,
      "step": 124070
    },
    {
      "epoch": 6.6176,
      "grad_norm": 0.157167449593544,
      "learning_rate": 8.64e-06,
      "loss": 0.0026,
      "step": 124080
    },
    {
      "epoch": 6.618133333333334,
      "grad_norm": 0.0582757294178009,
      "learning_rate": 8.636666666666666e-06,
      "loss": 0.002,
      "step": 124090
    },
    {
      "epoch": 6.618666666666667,
      "grad_norm": 0.06021083891391754,
      "learning_rate": 8.633333333333334e-06,
      "loss": 0.0017,
      "step": 124100
    },
    {
      "epoch": 6.6192,
      "grad_norm": 0.08887191861867905,
      "learning_rate": 8.63e-06,
      "loss": 0.0013,
      "step": 124110
    },
    {
      "epoch": 6.6197333333333335,
      "grad_norm": 0.3614582121372223,
      "learning_rate": 8.626666666666668e-06,
      "loss": 0.0014,
      "step": 124120
    },
    {
      "epoch": 6.620266666666667,
      "grad_norm": 0.34445589780807495,
      "learning_rate": 8.623333333333333e-06,
      "loss": 0.0012,
      "step": 124130
    },
    {
      "epoch": 6.6208,
      "grad_norm": 0.036354437470436096,
      "learning_rate": 8.62e-06,
      "loss": 0.0014,
      "step": 124140
    },
    {
      "epoch": 6.621333333333333,
      "grad_norm": 0.18818306922912598,
      "learning_rate": 8.616666666666667e-06,
      "loss": 0.0014,
      "step": 124150
    },
    {
      "epoch": 6.621866666666667,
      "grad_norm": 0.16428303718566895,
      "learning_rate": 8.613333333333334e-06,
      "loss": 0.0019,
      "step": 124160
    },
    {
      "epoch": 6.6224,
      "grad_norm": 0.020324580371379852,
      "learning_rate": 8.61e-06,
      "loss": 0.0019,
      "step": 124170
    },
    {
      "epoch": 6.622933333333333,
      "grad_norm": 0.706756055355072,
      "learning_rate": 8.606666666666667e-06,
      "loss": 0.002,
      "step": 124180
    },
    {
      "epoch": 6.623466666666666,
      "grad_norm": 0.03577810525894165,
      "learning_rate": 8.603333333333335e-06,
      "loss": 0.0028,
      "step": 124190
    },
    {
      "epoch": 6.624,
      "grad_norm": 0.2133476585149765,
      "learning_rate": 8.599999999999999e-06,
      "loss": 0.0013,
      "step": 124200
    },
    {
      "epoch": 6.624533333333334,
      "grad_norm": 0.1430310755968094,
      "learning_rate": 8.596666666666667e-06,
      "loss": 0.0017,
      "step": 124210
    },
    {
      "epoch": 6.625066666666667,
      "grad_norm": 0.07719386368989944,
      "learning_rate": 8.593333333333335e-06,
      "loss": 0.0014,
      "step": 124220
    },
    {
      "epoch": 6.6256,
      "grad_norm": 0.2302473932504654,
      "learning_rate": 8.59e-06,
      "loss": 0.0021,
      "step": 124230
    },
    {
      "epoch": 6.626133333333334,
      "grad_norm": 0.16497300565242767,
      "learning_rate": 8.586666666666667e-06,
      "loss": 0.0019,
      "step": 124240
    },
    {
      "epoch": 6.626666666666667,
      "grad_norm": 0.29435452818870544,
      "learning_rate": 8.583333333333333e-06,
      "loss": 0.0016,
      "step": 124250
    },
    {
      "epoch": 6.6272,
      "grad_norm": 0.1472538858652115,
      "learning_rate": 8.580000000000001e-06,
      "loss": 0.0019,
      "step": 124260
    },
    {
      "epoch": 6.6277333333333335,
      "grad_norm": 0.34670165181159973,
      "learning_rate": 8.576666666666667e-06,
      "loss": 0.0016,
      "step": 124270
    },
    {
      "epoch": 6.628266666666667,
      "grad_norm": 0.20554997026920319,
      "learning_rate": 8.573333333333333e-06,
      "loss": 0.0022,
      "step": 124280
    },
    {
      "epoch": 6.6288,
      "grad_norm": 0.362196147441864,
      "learning_rate": 8.570000000000001e-06,
      "loss": 0.0016,
      "step": 124290
    },
    {
      "epoch": 6.629333333333333,
      "grad_norm": 0.11181265115737915,
      "learning_rate": 8.566666666666667e-06,
      "loss": 0.0015,
      "step": 124300
    },
    {
      "epoch": 6.629866666666667,
      "grad_norm": 0.13232427835464478,
      "learning_rate": 8.563333333333335e-06,
      "loss": 0.0016,
      "step": 124310
    },
    {
      "epoch": 6.6304,
      "grad_norm": 0.06564866006374359,
      "learning_rate": 8.56e-06,
      "loss": 0.0018,
      "step": 124320
    },
    {
      "epoch": 6.630933333333333,
      "grad_norm": 0.21312279999256134,
      "learning_rate": 8.556666666666667e-06,
      "loss": 0.0016,
      "step": 124330
    },
    {
      "epoch": 6.631466666666666,
      "grad_norm": 0.037091802805662155,
      "learning_rate": 8.553333333333333e-06,
      "loss": 0.0025,
      "step": 124340
    },
    {
      "epoch": 6.632,
      "grad_norm": 0.16073304414749146,
      "learning_rate": 8.550000000000001e-06,
      "loss": 0.0022,
      "step": 124350
    },
    {
      "epoch": 6.632533333333333,
      "grad_norm": 0.04745720699429512,
      "learning_rate": 8.546666666666667e-06,
      "loss": 0.0018,
      "step": 124360
    },
    {
      "epoch": 6.633066666666666,
      "grad_norm": 0.12264202535152435,
      "learning_rate": 8.543333333333333e-06,
      "loss": 0.0021,
      "step": 124370
    },
    {
      "epoch": 6.6336,
      "grad_norm": 0.1863117814064026,
      "learning_rate": 8.540000000000001e-06,
      "loss": 0.0021,
      "step": 124380
    },
    {
      "epoch": 6.634133333333334,
      "grad_norm": 0.11979428678750992,
      "learning_rate": 8.536666666666666e-06,
      "loss": 0.0019,
      "step": 124390
    },
    {
      "epoch": 6.634666666666667,
      "grad_norm": 0.06941864639520645,
      "learning_rate": 8.533333333333334e-06,
      "loss": 0.0018,
      "step": 124400
    },
    {
      "epoch": 6.6352,
      "grad_norm": 0.127034991979599,
      "learning_rate": 8.53e-06,
      "loss": 0.0015,
      "step": 124410
    },
    {
      "epoch": 6.6357333333333335,
      "grad_norm": 0.15271465480327606,
      "learning_rate": 8.526666666666667e-06,
      "loss": 0.0013,
      "step": 124420
    },
    {
      "epoch": 6.636266666666667,
      "grad_norm": 0.13629372417926788,
      "learning_rate": 8.523333333333334e-06,
      "loss": 0.0018,
      "step": 124430
    },
    {
      "epoch": 6.6368,
      "grad_norm": 0.09927757829427719,
      "learning_rate": 8.52e-06,
      "loss": 0.0024,
      "step": 124440
    },
    {
      "epoch": 6.637333333333333,
      "grad_norm": 0.16337667405605316,
      "learning_rate": 8.516666666666668e-06,
      "loss": 0.0013,
      "step": 124450
    },
    {
      "epoch": 6.637866666666667,
      "grad_norm": 0.25252681970596313,
      "learning_rate": 8.513333333333334e-06,
      "loss": 0.0027,
      "step": 124460
    },
    {
      "epoch": 6.6384,
      "grad_norm": 0.4514455497264862,
      "learning_rate": 8.51e-06,
      "loss": 0.002,
      "step": 124470
    },
    {
      "epoch": 6.638933333333333,
      "grad_norm": 0.31668615341186523,
      "learning_rate": 8.506666666666668e-06,
      "loss": 0.0024,
      "step": 124480
    },
    {
      "epoch": 6.639466666666666,
      "grad_norm": 0.13027387857437134,
      "learning_rate": 8.503333333333334e-06,
      "loss": 0.0016,
      "step": 124490
    },
    {
      "epoch": 6.64,
      "grad_norm": 0.09009508788585663,
      "learning_rate": 8.500000000000002e-06,
      "loss": 0.0015,
      "step": 124500
    },
    {
      "epoch": 6.640533333333333,
      "grad_norm": 0.6429030299186707,
      "learning_rate": 8.496666666666666e-06,
      "loss": 0.0023,
      "step": 124510
    },
    {
      "epoch": 6.641066666666667,
      "grad_norm": 0.28034695982933044,
      "learning_rate": 8.493333333333334e-06,
      "loss": 0.0017,
      "step": 124520
    },
    {
      "epoch": 6.6416,
      "grad_norm": 0.18476244807243347,
      "learning_rate": 8.49e-06,
      "loss": 0.0012,
      "step": 124530
    },
    {
      "epoch": 6.642133333333334,
      "grad_norm": 0.29876652359962463,
      "learning_rate": 8.486666666666668e-06,
      "loss": 0.0026,
      "step": 124540
    },
    {
      "epoch": 6.642666666666667,
      "grad_norm": 0.05836772546172142,
      "learning_rate": 8.483333333333334e-06,
      "loss": 0.0012,
      "step": 124550
    },
    {
      "epoch": 6.6432,
      "grad_norm": 0.3293241560459137,
      "learning_rate": 8.48e-06,
      "loss": 0.0016,
      "step": 124560
    },
    {
      "epoch": 6.6437333333333335,
      "grad_norm": 0.39417368173599243,
      "learning_rate": 8.476666666666668e-06,
      "loss": 0.002,
      "step": 124570
    },
    {
      "epoch": 6.644266666666667,
      "grad_norm": 0.538123607635498,
      "learning_rate": 8.473333333333332e-06,
      "loss": 0.0018,
      "step": 124580
    },
    {
      "epoch": 6.6448,
      "grad_norm": 0.12045648694038391,
      "learning_rate": 8.47e-06,
      "loss": 0.0015,
      "step": 124590
    },
    {
      "epoch": 6.645333333333333,
      "grad_norm": 0.5721270442008972,
      "learning_rate": 8.466666666666666e-06,
      "loss": 0.0014,
      "step": 124600
    },
    {
      "epoch": 6.645866666666667,
      "grad_norm": 0.4545625150203705,
      "learning_rate": 8.463333333333334e-06,
      "loss": 0.0016,
      "step": 124610
    },
    {
      "epoch": 6.6464,
      "grad_norm": 0.2855965495109558,
      "learning_rate": 8.46e-06,
      "loss": 0.0015,
      "step": 124620
    },
    {
      "epoch": 6.646933333333333,
      "grad_norm": 0.22072912752628326,
      "learning_rate": 8.456666666666666e-06,
      "loss": 0.0026,
      "step": 124630
    },
    {
      "epoch": 6.647466666666666,
      "grad_norm": 0.08057896047830582,
      "learning_rate": 8.453333333333334e-06,
      "loss": 0.0017,
      "step": 124640
    },
    {
      "epoch": 6.648,
      "grad_norm": 0.06156720221042633,
      "learning_rate": 8.45e-06,
      "loss": 0.0021,
      "step": 124650
    },
    {
      "epoch": 6.648533333333333,
      "grad_norm": 0.1772586852312088,
      "learning_rate": 8.446666666666667e-06,
      "loss": 0.0027,
      "step": 124660
    },
    {
      "epoch": 6.649066666666666,
      "grad_norm": 0.27273207902908325,
      "learning_rate": 8.443333333333334e-06,
      "loss": 0.0014,
      "step": 124670
    },
    {
      "epoch": 6.6495999999999995,
      "grad_norm": 0.16067436337471008,
      "learning_rate": 8.44e-06,
      "loss": 0.0026,
      "step": 124680
    },
    {
      "epoch": 6.650133333333334,
      "grad_norm": 0.12402736395597458,
      "learning_rate": 8.436666666666668e-06,
      "loss": 0.002,
      "step": 124690
    },
    {
      "epoch": 6.650666666666667,
      "grad_norm": 0.1563521772623062,
      "learning_rate": 8.433333333333333e-06,
      "loss": 0.0014,
      "step": 124700
    },
    {
      "epoch": 6.6512,
      "grad_norm": 0.04459882900118828,
      "learning_rate": 8.43e-06,
      "loss": 0.0024,
      "step": 124710
    },
    {
      "epoch": 6.6517333333333335,
      "grad_norm": 0.038848649710416794,
      "learning_rate": 8.426666666666667e-06,
      "loss": 0.0014,
      "step": 124720
    },
    {
      "epoch": 6.652266666666667,
      "grad_norm": 0.1642892211675644,
      "learning_rate": 8.423333333333333e-06,
      "loss": 0.0026,
      "step": 124730
    },
    {
      "epoch": 6.6528,
      "grad_norm": 0.17086154222488403,
      "learning_rate": 8.42e-06,
      "loss": 0.0016,
      "step": 124740
    },
    {
      "epoch": 6.653333333333333,
      "grad_norm": 0.2731121778488159,
      "learning_rate": 8.416666666666667e-06,
      "loss": 0.0023,
      "step": 124750
    },
    {
      "epoch": 6.653866666666667,
      "grad_norm": 0.3037722706794739,
      "learning_rate": 8.413333333333335e-06,
      "loss": 0.0015,
      "step": 124760
    },
    {
      "epoch": 6.6544,
      "grad_norm": 0.11789244413375854,
      "learning_rate": 8.409999999999999e-06,
      "loss": 0.0018,
      "step": 124770
    },
    {
      "epoch": 6.654933333333333,
      "grad_norm": 0.5070626139640808,
      "learning_rate": 8.406666666666667e-06,
      "loss": 0.0014,
      "step": 124780
    },
    {
      "epoch": 6.655466666666666,
      "grad_norm": 0.3204635977745056,
      "learning_rate": 8.403333333333333e-06,
      "loss": 0.0014,
      "step": 124790
    },
    {
      "epoch": 6.656,
      "grad_norm": 0.06547339260578156,
      "learning_rate": 8.400000000000001e-06,
      "loss": 0.0015,
      "step": 124800
    },
    {
      "epoch": 6.656533333333333,
      "grad_norm": 0.24995553493499756,
      "learning_rate": 8.396666666666667e-06,
      "loss": 0.0015,
      "step": 124810
    },
    {
      "epoch": 6.657066666666667,
      "grad_norm": 0.25614991784095764,
      "learning_rate": 8.393333333333333e-06,
      "loss": 0.0023,
      "step": 124820
    },
    {
      "epoch": 6.6576,
      "grad_norm": 0.14762510359287262,
      "learning_rate": 8.390000000000001e-06,
      "loss": 0.0018,
      "step": 124830
    },
    {
      "epoch": 6.658133333333334,
      "grad_norm": 0.23478825390338898,
      "learning_rate": 8.386666666666667e-06,
      "loss": 0.0028,
      "step": 124840
    },
    {
      "epoch": 6.658666666666667,
      "grad_norm": 0.12022112309932709,
      "learning_rate": 8.383333333333333e-06,
      "loss": 0.0022,
      "step": 124850
    },
    {
      "epoch": 6.6592,
      "grad_norm": 0.16798675060272217,
      "learning_rate": 8.380000000000001e-06,
      "loss": 0.0015,
      "step": 124860
    },
    {
      "epoch": 6.6597333333333335,
      "grad_norm": 0.20151962339878082,
      "learning_rate": 8.376666666666667e-06,
      "loss": 0.0019,
      "step": 124870
    },
    {
      "epoch": 6.660266666666667,
      "grad_norm": 0.37404224276542664,
      "learning_rate": 8.373333333333335e-06,
      "loss": 0.0025,
      "step": 124880
    },
    {
      "epoch": 6.6608,
      "grad_norm": 0.13247783482074738,
      "learning_rate": 8.37e-06,
      "loss": 0.0021,
      "step": 124890
    },
    {
      "epoch": 6.661333333333333,
      "grad_norm": 0.18898314237594604,
      "learning_rate": 8.366666666666667e-06,
      "loss": 0.0015,
      "step": 124900
    },
    {
      "epoch": 6.661866666666667,
      "grad_norm": 0.2672244608402252,
      "learning_rate": 8.363333333333333e-06,
      "loss": 0.0015,
      "step": 124910
    },
    {
      "epoch": 6.6624,
      "grad_norm": 0.07499519735574722,
      "learning_rate": 8.36e-06,
      "loss": 0.0019,
      "step": 124920
    },
    {
      "epoch": 6.662933333333333,
      "grad_norm": 0.0451592318713665,
      "learning_rate": 8.356666666666667e-06,
      "loss": 0.0017,
      "step": 124930
    },
    {
      "epoch": 6.663466666666666,
      "grad_norm": 0.047640953212976456,
      "learning_rate": 8.353333333333334e-06,
      "loss": 0.0015,
      "step": 124940
    },
    {
      "epoch": 6.664,
      "grad_norm": 0.44218799471855164,
      "learning_rate": 8.350000000000001e-06,
      "loss": 0.0018,
      "step": 124950
    },
    {
      "epoch": 6.664533333333333,
      "grad_norm": 0.5182879567146301,
      "learning_rate": 8.346666666666666e-06,
      "loss": 0.0021,
      "step": 124960
    },
    {
      "epoch": 6.665066666666666,
      "grad_norm": 0.10301331430673599,
      "learning_rate": 8.343333333333334e-06,
      "loss": 0.0015,
      "step": 124970
    },
    {
      "epoch": 6.6655999999999995,
      "grad_norm": 0.2754994332790375,
      "learning_rate": 8.34e-06,
      "loss": 0.0023,
      "step": 124980
    },
    {
      "epoch": 6.666133333333334,
      "grad_norm": 0.093905508518219,
      "learning_rate": 8.336666666666668e-06,
      "loss": 0.0017,
      "step": 124990
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 0.4719846844673157,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.0014,
      "step": 125000
    },
    {
      "epoch": 6.6672,
      "grad_norm": 0.2423582375049591,
      "learning_rate": 8.33e-06,
      "loss": 0.0015,
      "step": 125010
    },
    {
      "epoch": 6.6677333333333335,
      "grad_norm": 0.1550130695104599,
      "learning_rate": 8.326666666666668e-06,
      "loss": 0.002,
      "step": 125020
    },
    {
      "epoch": 6.668266666666667,
      "grad_norm": 0.3552606701850891,
      "learning_rate": 8.323333333333334e-06,
      "loss": 0.0027,
      "step": 125030
    },
    {
      "epoch": 6.6688,
      "grad_norm": 0.09407113492488861,
      "learning_rate": 8.32e-06,
      "loss": 0.0018,
      "step": 125040
    },
    {
      "epoch": 6.669333333333333,
      "grad_norm": 0.0905035212635994,
      "learning_rate": 8.316666666666668e-06,
      "loss": 0.0027,
      "step": 125050
    },
    {
      "epoch": 6.669866666666667,
      "grad_norm": 0.1988341361284256,
      "learning_rate": 8.313333333333334e-06,
      "loss": 0.0021,
      "step": 125060
    },
    {
      "epoch": 6.6704,
      "grad_norm": 0.15051387250423431,
      "learning_rate": 8.31e-06,
      "loss": 0.0025,
      "step": 125070
    },
    {
      "epoch": 6.670933333333333,
      "grad_norm": 0.13237692415714264,
      "learning_rate": 8.306666666666666e-06,
      "loss": 0.0016,
      "step": 125080
    },
    {
      "epoch": 6.671466666666666,
      "grad_norm": 0.17686007916927338,
      "learning_rate": 8.303333333333334e-06,
      "loss": 0.0016,
      "step": 125090
    },
    {
      "epoch": 6.672,
      "grad_norm": 0.06631670892238617,
      "learning_rate": 8.3e-06,
      "loss": 0.0017,
      "step": 125100
    },
    {
      "epoch": 6.672533333333333,
      "grad_norm": 0.17559991776943207,
      "learning_rate": 8.296666666666666e-06,
      "loss": 0.0017,
      "step": 125110
    },
    {
      "epoch": 6.673066666666667,
      "grad_norm": 0.04616188257932663,
      "learning_rate": 8.293333333333334e-06,
      "loss": 0.0016,
      "step": 125120
    },
    {
      "epoch": 6.6736,
      "grad_norm": 0.18870486319065094,
      "learning_rate": 8.29e-06,
      "loss": 0.0013,
      "step": 125130
    },
    {
      "epoch": 6.674133333333334,
      "grad_norm": 0.09893504530191422,
      "learning_rate": 8.286666666666668e-06,
      "loss": 0.0024,
      "step": 125140
    },
    {
      "epoch": 6.674666666666667,
      "grad_norm": 0.0906192883849144,
      "learning_rate": 8.283333333333333e-06,
      "loss": 0.0021,
      "step": 125150
    },
    {
      "epoch": 6.6752,
      "grad_norm": 0.6955137252807617,
      "learning_rate": 8.28e-06,
      "loss": 0.0019,
      "step": 125160
    },
    {
      "epoch": 6.6757333333333335,
      "grad_norm": 0.19373422861099243,
      "learning_rate": 8.276666666666666e-06,
      "loss": 0.0024,
      "step": 125170
    },
    {
      "epoch": 6.676266666666667,
      "grad_norm": 0.506314754486084,
      "learning_rate": 8.273333333333334e-06,
      "loss": 0.0021,
      "step": 125180
    },
    {
      "epoch": 6.6768,
      "grad_norm": 0.06309690326452255,
      "learning_rate": 8.27e-06,
      "loss": 0.0014,
      "step": 125190
    },
    {
      "epoch": 6.677333333333333,
      "grad_norm": 0.29569146037101746,
      "learning_rate": 8.266666666666667e-06,
      "loss": 0.0021,
      "step": 125200
    },
    {
      "epoch": 6.677866666666667,
      "grad_norm": 0.3150799572467804,
      "learning_rate": 8.263333333333334e-06,
      "loss": 0.0017,
      "step": 125210
    },
    {
      "epoch": 6.6784,
      "grad_norm": 0.09708252549171448,
      "learning_rate": 8.26e-06,
      "loss": 0.0016,
      "step": 125220
    },
    {
      "epoch": 6.678933333333333,
      "grad_norm": 0.2389034777879715,
      "learning_rate": 8.256666666666667e-06,
      "loss": 0.0016,
      "step": 125230
    },
    {
      "epoch": 6.679466666666666,
      "grad_norm": 0.20968683063983917,
      "learning_rate": 8.253333333333334e-06,
      "loss": 0.0016,
      "step": 125240
    },
    {
      "epoch": 6.68,
      "grad_norm": 0.17604102194309235,
      "learning_rate": 8.25e-06,
      "loss": 0.0022,
      "step": 125250
    },
    {
      "epoch": 6.680533333333333,
      "grad_norm": 0.18657904863357544,
      "learning_rate": 8.246666666666667e-06,
      "loss": 0.0017,
      "step": 125260
    },
    {
      "epoch": 6.681066666666666,
      "grad_norm": 0.1086900606751442,
      "learning_rate": 8.243333333333333e-06,
      "loss": 0.0018,
      "step": 125270
    },
    {
      "epoch": 6.6815999999999995,
      "grad_norm": 0.35581234097480774,
      "learning_rate": 8.24e-06,
      "loss": 0.0019,
      "step": 125280
    },
    {
      "epoch": 6.682133333333334,
      "grad_norm": 0.07019025832414627,
      "learning_rate": 8.236666666666667e-06,
      "loss": 0.0015,
      "step": 125290
    },
    {
      "epoch": 6.682666666666667,
      "grad_norm": 0.504017174243927,
      "learning_rate": 8.233333333333333e-06,
      "loss": 0.0015,
      "step": 125300
    },
    {
      "epoch": 6.6832,
      "grad_norm": 0.06623498350381851,
      "learning_rate": 8.23e-06,
      "loss": 0.0018,
      "step": 125310
    },
    {
      "epoch": 6.6837333333333335,
      "grad_norm": 0.29226604104042053,
      "learning_rate": 8.226666666666667e-06,
      "loss": 0.0015,
      "step": 125320
    },
    {
      "epoch": 6.684266666666667,
      "grad_norm": 0.24579109251499176,
      "learning_rate": 8.223333333333335e-06,
      "loss": 0.0019,
      "step": 125330
    },
    {
      "epoch": 6.6848,
      "grad_norm": 0.07845467329025269,
      "learning_rate": 8.22e-06,
      "loss": 0.0019,
      "step": 125340
    },
    {
      "epoch": 6.685333333333333,
      "grad_norm": 0.04810680076479912,
      "learning_rate": 8.216666666666667e-06,
      "loss": 0.0018,
      "step": 125350
    },
    {
      "epoch": 6.685866666666667,
      "grad_norm": 0.23778429627418518,
      "learning_rate": 8.213333333333333e-06,
      "loss": 0.0034,
      "step": 125360
    },
    {
      "epoch": 6.6864,
      "grad_norm": 0.040851060301065445,
      "learning_rate": 8.210000000000001e-06,
      "loss": 0.0018,
      "step": 125370
    },
    {
      "epoch": 6.686933333333333,
      "grad_norm": 0.13001297414302826,
      "learning_rate": 8.206666666666667e-06,
      "loss": 0.002,
      "step": 125380
    },
    {
      "epoch": 6.6874666666666664,
      "grad_norm": 0.15884554386138916,
      "learning_rate": 8.203333333333333e-06,
      "loss": 0.0017,
      "step": 125390
    },
    {
      "epoch": 6.688,
      "grad_norm": 0.21279875934123993,
      "learning_rate": 8.200000000000001e-06,
      "loss": 0.0015,
      "step": 125400
    },
    {
      "epoch": 6.688533333333333,
      "grad_norm": 0.2424832433462143,
      "learning_rate": 8.196666666666666e-06,
      "loss": 0.0015,
      "step": 125410
    },
    {
      "epoch": 6.689066666666667,
      "grad_norm": 0.47093823552131653,
      "learning_rate": 8.193333333333333e-06,
      "loss": 0.0026,
      "step": 125420
    },
    {
      "epoch": 6.6896,
      "grad_norm": 0.29541635513305664,
      "learning_rate": 8.190000000000001e-06,
      "loss": 0.002,
      "step": 125430
    },
    {
      "epoch": 6.690133333333334,
      "grad_norm": 0.18036550283432007,
      "learning_rate": 8.186666666666667e-06,
      "loss": 0.0027,
      "step": 125440
    },
    {
      "epoch": 6.690666666666667,
      "grad_norm": 0.3680950105190277,
      "learning_rate": 8.183333333333333e-06,
      "loss": 0.0023,
      "step": 125450
    },
    {
      "epoch": 6.6912,
      "grad_norm": 0.3718554377555847,
      "learning_rate": 8.18e-06,
      "loss": 0.0025,
      "step": 125460
    },
    {
      "epoch": 6.6917333333333335,
      "grad_norm": 0.17003582417964935,
      "learning_rate": 8.176666666666667e-06,
      "loss": 0.0025,
      "step": 125470
    },
    {
      "epoch": 6.692266666666667,
      "grad_norm": 0.3542307913303375,
      "learning_rate": 8.173333333333334e-06,
      "loss": 0.0023,
      "step": 125480
    },
    {
      "epoch": 6.6928,
      "grad_norm": 0.0971328541636467,
      "learning_rate": 8.17e-06,
      "loss": 0.0022,
      "step": 125490
    },
    {
      "epoch": 6.693333333333333,
      "grad_norm": 0.09279479831457138,
      "learning_rate": 8.166666666666668e-06,
      "loss": 0.003,
      "step": 125500
    },
    {
      "epoch": 6.693866666666667,
      "grad_norm": 0.41538938879966736,
      "learning_rate": 8.163333333333334e-06,
      "loss": 0.0021,
      "step": 125510
    },
    {
      "epoch": 6.6944,
      "grad_norm": 0.09324739128351212,
      "learning_rate": 8.160000000000001e-06,
      "loss": 0.0022,
      "step": 125520
    },
    {
      "epoch": 6.694933333333333,
      "grad_norm": 0.17857955396175385,
      "learning_rate": 8.156666666666666e-06,
      "loss": 0.0014,
      "step": 125530
    },
    {
      "epoch": 6.6954666666666665,
      "grad_norm": 0.09419578313827515,
      "learning_rate": 8.153333333333334e-06,
      "loss": 0.0018,
      "step": 125540
    },
    {
      "epoch": 6.696,
      "grad_norm": 0.3262668550014496,
      "learning_rate": 8.15e-06,
      "loss": 0.0014,
      "step": 125550
    },
    {
      "epoch": 6.696533333333333,
      "grad_norm": 0.1548047512769699,
      "learning_rate": 8.146666666666668e-06,
      "loss": 0.002,
      "step": 125560
    },
    {
      "epoch": 6.697066666666666,
      "grad_norm": 0.21186114847660065,
      "learning_rate": 8.143333333333334e-06,
      "loss": 0.0014,
      "step": 125570
    },
    {
      "epoch": 6.6975999999999996,
      "grad_norm": 0.20190899074077606,
      "learning_rate": 8.14e-06,
      "loss": 0.0018,
      "step": 125580
    },
    {
      "epoch": 6.698133333333334,
      "grad_norm": 0.20648516714572906,
      "learning_rate": 8.136666666666668e-06,
      "loss": 0.0019,
      "step": 125590
    },
    {
      "epoch": 6.698666666666667,
      "grad_norm": 0.26691433787345886,
      "learning_rate": 8.133333333333332e-06,
      "loss": 0.0015,
      "step": 125600
    },
    {
      "epoch": 6.6992,
      "grad_norm": 0.12020914256572723,
      "learning_rate": 8.13e-06,
      "loss": 0.0021,
      "step": 125610
    },
    {
      "epoch": 6.6997333333333335,
      "grad_norm": 0.2992284297943115,
      "learning_rate": 8.126666666666668e-06,
      "loss": 0.0024,
      "step": 125620
    },
    {
      "epoch": 6.700266666666667,
      "grad_norm": 0.28343674540519714,
      "learning_rate": 8.123333333333334e-06,
      "loss": 0.0028,
      "step": 125630
    },
    {
      "epoch": 6.7008,
      "grad_norm": 0.09224393963813782,
      "learning_rate": 8.12e-06,
      "loss": 0.0017,
      "step": 125640
    },
    {
      "epoch": 6.701333333333333,
      "grad_norm": 0.09271778166294098,
      "learning_rate": 8.116666666666666e-06,
      "loss": 0.0013,
      "step": 125650
    },
    {
      "epoch": 6.701866666666667,
      "grad_norm": 0.11625197529792786,
      "learning_rate": 8.113333333333334e-06,
      "loss": 0.0029,
      "step": 125660
    },
    {
      "epoch": 6.7024,
      "grad_norm": 0.1498292088508606,
      "learning_rate": 8.11e-06,
      "loss": 0.0016,
      "step": 125670
    },
    {
      "epoch": 6.702933333333333,
      "grad_norm": 0.5887145400047302,
      "learning_rate": 8.106666666666666e-06,
      "loss": 0.0016,
      "step": 125680
    },
    {
      "epoch": 6.7034666666666665,
      "grad_norm": 0.20528936386108398,
      "learning_rate": 8.103333333333334e-06,
      "loss": 0.0015,
      "step": 125690
    },
    {
      "epoch": 6.704,
      "grad_norm": 0.08858554810285568,
      "learning_rate": 8.1e-06,
      "loss": 0.0018,
      "step": 125700
    },
    {
      "epoch": 6.704533333333333,
      "grad_norm": 0.03560688719153404,
      "learning_rate": 8.096666666666668e-06,
      "loss": 0.0027,
      "step": 125710
    },
    {
      "epoch": 6.705066666666666,
      "grad_norm": 0.21067090332508087,
      "learning_rate": 8.093333333333333e-06,
      "loss": 0.0023,
      "step": 125720
    },
    {
      "epoch": 6.7056000000000004,
      "grad_norm": 0.12035009264945984,
      "learning_rate": 8.09e-06,
      "loss": 0.0013,
      "step": 125730
    },
    {
      "epoch": 6.706133333333334,
      "grad_norm": 0.3049760162830353,
      "learning_rate": 8.086666666666667e-06,
      "loss": 0.0019,
      "step": 125740
    },
    {
      "epoch": 6.706666666666667,
      "grad_norm": 0.1815391480922699,
      "learning_rate": 8.083333333333333e-06,
      "loss": 0.0017,
      "step": 125750
    },
    {
      "epoch": 6.7072,
      "grad_norm": 0.060755256563425064,
      "learning_rate": 8.08e-06,
      "loss": 0.0015,
      "step": 125760
    },
    {
      "epoch": 6.7077333333333335,
      "grad_norm": 0.26505738496780396,
      "learning_rate": 8.076666666666667e-06,
      "loss": 0.0015,
      "step": 125770
    },
    {
      "epoch": 6.708266666666667,
      "grad_norm": 0.08465509861707687,
      "learning_rate": 8.073333333333335e-06,
      "loss": 0.002,
      "step": 125780
    },
    {
      "epoch": 6.7088,
      "grad_norm": 0.02300380729138851,
      "learning_rate": 8.069999999999999e-06,
      "loss": 0.0018,
      "step": 125790
    },
    {
      "epoch": 6.709333333333333,
      "grad_norm": 0.044443536549806595,
      "learning_rate": 8.066666666666667e-06,
      "loss": 0.0031,
      "step": 125800
    },
    {
      "epoch": 6.709866666666667,
      "grad_norm": 0.14842671155929565,
      "learning_rate": 8.063333333333335e-06,
      "loss": 0.0032,
      "step": 125810
    },
    {
      "epoch": 6.7104,
      "grad_norm": 0.4466322064399719,
      "learning_rate": 8.06e-06,
      "loss": 0.0022,
      "step": 125820
    },
    {
      "epoch": 6.710933333333333,
      "grad_norm": 0.08012213557958603,
      "learning_rate": 8.056666666666667e-06,
      "loss": 0.0015,
      "step": 125830
    },
    {
      "epoch": 6.7114666666666665,
      "grad_norm": 0.04757936671376228,
      "learning_rate": 8.053333333333333e-06,
      "loss": 0.0025,
      "step": 125840
    },
    {
      "epoch": 6.712,
      "grad_norm": 0.3520192503929138,
      "learning_rate": 8.050000000000001e-06,
      "loss": 0.002,
      "step": 125850
    },
    {
      "epoch": 6.712533333333333,
      "grad_norm": 0.1885414719581604,
      "learning_rate": 8.046666666666667e-06,
      "loss": 0.0018,
      "step": 125860
    },
    {
      "epoch": 6.713066666666666,
      "grad_norm": 0.12392207980155945,
      "learning_rate": 8.043333333333333e-06,
      "loss": 0.0017,
      "step": 125870
    },
    {
      "epoch": 6.7136,
      "grad_norm": 0.07184454053640366,
      "learning_rate": 8.040000000000001e-06,
      "loss": 0.0013,
      "step": 125880
    },
    {
      "epoch": 6.714133333333333,
      "grad_norm": 0.21607768535614014,
      "learning_rate": 8.036666666666667e-06,
      "loss": 0.0019,
      "step": 125890
    },
    {
      "epoch": 6.714666666666667,
      "grad_norm": 0.05189383774995804,
      "learning_rate": 8.033333333333335e-06,
      "loss": 0.0021,
      "step": 125900
    },
    {
      "epoch": 6.7152,
      "grad_norm": 0.1492745280265808,
      "learning_rate": 8.03e-06,
      "loss": 0.0018,
      "step": 125910
    },
    {
      "epoch": 6.7157333333333336,
      "grad_norm": 0.14953306317329407,
      "learning_rate": 8.026666666666667e-06,
      "loss": 0.0018,
      "step": 125920
    },
    {
      "epoch": 6.716266666666667,
      "grad_norm": 0.1474122256040573,
      "learning_rate": 8.023333333333333e-06,
      "loss": 0.0018,
      "step": 125930
    },
    {
      "epoch": 6.7168,
      "grad_norm": 0.2933376431465149,
      "learning_rate": 8.02e-06,
      "loss": 0.0019,
      "step": 125940
    },
    {
      "epoch": 6.717333333333333,
      "grad_norm": 0.04901254549622536,
      "learning_rate": 8.016666666666667e-06,
      "loss": 0.002,
      "step": 125950
    },
    {
      "epoch": 6.717866666666667,
      "grad_norm": 0.16512492299079895,
      "learning_rate": 8.013333333333333e-06,
      "loss": 0.0012,
      "step": 125960
    },
    {
      "epoch": 6.7184,
      "grad_norm": 0.6250929236412048,
      "learning_rate": 8.010000000000001e-06,
      "loss": 0.0023,
      "step": 125970
    },
    {
      "epoch": 6.718933333333333,
      "grad_norm": 0.05110679194331169,
      "learning_rate": 8.006666666666666e-06,
      "loss": 0.0022,
      "step": 125980
    },
    {
      "epoch": 6.7194666666666665,
      "grad_norm": 0.06701762974262238,
      "learning_rate": 8.003333333333334e-06,
      "loss": 0.0015,
      "step": 125990
    },
    {
      "epoch": 6.72,
      "grad_norm": 0.33147427439689636,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.002,
      "step": 126000
    },
    {
      "epoch": 6.720533333333333,
      "grad_norm": 0.41065648198127747,
      "learning_rate": 7.996666666666667e-06,
      "loss": 0.0016,
      "step": 126010
    },
    {
      "epoch": 6.721066666666666,
      "grad_norm": 0.028843825682997704,
      "learning_rate": 7.993333333333334e-06,
      "loss": 0.0014,
      "step": 126020
    },
    {
      "epoch": 6.7216000000000005,
      "grad_norm": 0.3312660753726959,
      "learning_rate": 7.99e-06,
      "loss": 0.0014,
      "step": 126030
    },
    {
      "epoch": 6.722133333333334,
      "grad_norm": 0.35752594470977783,
      "learning_rate": 7.986666666666668e-06,
      "loss": 0.0017,
      "step": 126040
    },
    {
      "epoch": 6.722666666666667,
      "grad_norm": 0.04518565535545349,
      "learning_rate": 7.983333333333334e-06,
      "loss": 0.0014,
      "step": 126050
    },
    {
      "epoch": 6.7232,
      "grad_norm": 0.4122001826763153,
      "learning_rate": 7.98e-06,
      "loss": 0.0019,
      "step": 126060
    },
    {
      "epoch": 6.723733333333334,
      "grad_norm": 0.20982813835144043,
      "learning_rate": 7.976666666666668e-06,
      "loss": 0.0014,
      "step": 126070
    },
    {
      "epoch": 6.724266666666667,
      "grad_norm": 0.18612787127494812,
      "learning_rate": 7.973333333333334e-06,
      "loss": 0.0016,
      "step": 126080
    },
    {
      "epoch": 6.7248,
      "grad_norm": 0.30755412578582764,
      "learning_rate": 7.97e-06,
      "loss": 0.0017,
      "step": 126090
    },
    {
      "epoch": 6.725333333333333,
      "grad_norm": 0.11898679286241531,
      "learning_rate": 7.966666666666666e-06,
      "loss": 0.0027,
      "step": 126100
    },
    {
      "epoch": 6.725866666666667,
      "grad_norm": 0.23983925580978394,
      "learning_rate": 7.963333333333334e-06,
      "loss": 0.0015,
      "step": 126110
    },
    {
      "epoch": 6.7264,
      "grad_norm": 0.26380205154418945,
      "learning_rate": 7.96e-06,
      "loss": 0.0024,
      "step": 126120
    },
    {
      "epoch": 6.726933333333333,
      "grad_norm": 0.09587068110704422,
      "learning_rate": 7.956666666666666e-06,
      "loss": 0.0024,
      "step": 126130
    },
    {
      "epoch": 6.7274666666666665,
      "grad_norm": 0.12601062655448914,
      "learning_rate": 7.953333333333334e-06,
      "loss": 0.0015,
      "step": 126140
    },
    {
      "epoch": 6.728,
      "grad_norm": 0.1250893473625183,
      "learning_rate": 7.95e-06,
      "loss": 0.0015,
      "step": 126150
    },
    {
      "epoch": 6.728533333333333,
      "grad_norm": 0.33340439200401306,
      "learning_rate": 7.946666666666668e-06,
      "loss": 0.0014,
      "step": 126160
    },
    {
      "epoch": 6.729066666666666,
      "grad_norm": 0.15063416957855225,
      "learning_rate": 7.943333333333332e-06,
      "loss": 0.0014,
      "step": 126170
    },
    {
      "epoch": 6.7296,
      "grad_norm": 0.1718272566795349,
      "learning_rate": 7.94e-06,
      "loss": 0.0022,
      "step": 126180
    },
    {
      "epoch": 6.730133333333333,
      "grad_norm": 0.49816086888313293,
      "learning_rate": 7.936666666666668e-06,
      "loss": 0.002,
      "step": 126190
    },
    {
      "epoch": 6.730666666666667,
      "grad_norm": 0.2594135105609894,
      "learning_rate": 7.933333333333334e-06,
      "loss": 0.0017,
      "step": 126200
    },
    {
      "epoch": 6.7312,
      "grad_norm": 0.2793024778366089,
      "learning_rate": 7.93e-06,
      "loss": 0.0015,
      "step": 126210
    },
    {
      "epoch": 6.731733333333334,
      "grad_norm": 0.07065022736787796,
      "learning_rate": 7.926666666666666e-06,
      "loss": 0.002,
      "step": 126220
    },
    {
      "epoch": 6.732266666666667,
      "grad_norm": 0.20795905590057373,
      "learning_rate": 7.923333333333334e-06,
      "loss": 0.0017,
      "step": 126230
    },
    {
      "epoch": 6.7328,
      "grad_norm": 0.1568479835987091,
      "learning_rate": 7.92e-06,
      "loss": 0.0019,
      "step": 126240
    },
    {
      "epoch": 6.733333333333333,
      "grad_norm": 0.04108494892716408,
      "learning_rate": 7.916666666666667e-06,
      "loss": 0.0013,
      "step": 126250
    },
    {
      "epoch": 6.733866666666667,
      "grad_norm": 0.07414715737104416,
      "learning_rate": 7.913333333333334e-06,
      "loss": 0.0016,
      "step": 126260
    },
    {
      "epoch": 6.7344,
      "grad_norm": 0.361603707075119,
      "learning_rate": 7.91e-06,
      "loss": 0.0019,
      "step": 126270
    },
    {
      "epoch": 6.734933333333333,
      "grad_norm": 0.22794583439826965,
      "learning_rate": 7.906666666666667e-06,
      "loss": 0.0014,
      "step": 126280
    },
    {
      "epoch": 6.7354666666666665,
      "grad_norm": 0.13039112091064453,
      "learning_rate": 7.903333333333333e-06,
      "loss": 0.0019,
      "step": 126290
    },
    {
      "epoch": 6.736,
      "grad_norm": 0.3548998236656189,
      "learning_rate": 7.9e-06,
      "loss": 0.0016,
      "step": 126300
    },
    {
      "epoch": 6.736533333333333,
      "grad_norm": 0.2648167014122009,
      "learning_rate": 7.896666666666667e-06,
      "loss": 0.002,
      "step": 126310
    },
    {
      "epoch": 6.737066666666666,
      "grad_norm": 0.06608579307794571,
      "learning_rate": 7.893333333333333e-06,
      "loss": 0.0018,
      "step": 126320
    },
    {
      "epoch": 6.7376000000000005,
      "grad_norm": 0.24774886667728424,
      "learning_rate": 7.89e-06,
      "loss": 0.0029,
      "step": 126330
    },
    {
      "epoch": 6.738133333333334,
      "grad_norm": 0.11918671429157257,
      "learning_rate": 7.886666666666667e-06,
      "loss": 0.002,
      "step": 126340
    },
    {
      "epoch": 6.738666666666667,
      "grad_norm": 0.07432132214307785,
      "learning_rate": 7.883333333333335e-06,
      "loss": 0.0027,
      "step": 126350
    },
    {
      "epoch": 6.7392,
      "grad_norm": 0.15588678419589996,
      "learning_rate": 7.879999999999999e-06,
      "loss": 0.0022,
      "step": 126360
    },
    {
      "epoch": 6.739733333333334,
      "grad_norm": 0.321231484413147,
      "learning_rate": 7.876666666666667e-06,
      "loss": 0.0015,
      "step": 126370
    },
    {
      "epoch": 6.740266666666667,
      "grad_norm": 0.12900954484939575,
      "learning_rate": 7.873333333333335e-06,
      "loss": 0.0011,
      "step": 126380
    },
    {
      "epoch": 6.7408,
      "grad_norm": 0.14750495553016663,
      "learning_rate": 7.870000000000001e-06,
      "loss": 0.0017,
      "step": 126390
    },
    {
      "epoch": 6.741333333333333,
      "grad_norm": 0.2041996568441391,
      "learning_rate": 7.866666666666667e-06,
      "loss": 0.0027,
      "step": 126400
    },
    {
      "epoch": 6.741866666666667,
      "grad_norm": 0.09240809082984924,
      "learning_rate": 7.863333333333333e-06,
      "loss": 0.0018,
      "step": 126410
    },
    {
      "epoch": 6.7424,
      "grad_norm": 0.18482619524002075,
      "learning_rate": 7.860000000000001e-06,
      "loss": 0.002,
      "step": 126420
    },
    {
      "epoch": 6.742933333333333,
      "grad_norm": 0.3192805051803589,
      "learning_rate": 7.856666666666667e-06,
      "loss": 0.0013,
      "step": 126430
    },
    {
      "epoch": 6.7434666666666665,
      "grad_norm": 0.10300765931606293,
      "learning_rate": 7.853333333333333e-06,
      "loss": 0.0016,
      "step": 126440
    },
    {
      "epoch": 6.744,
      "grad_norm": 0.06620389968156815,
      "learning_rate": 7.850000000000001e-06,
      "loss": 0.002,
      "step": 126450
    },
    {
      "epoch": 6.744533333333333,
      "grad_norm": 0.06492062658071518,
      "learning_rate": 7.846666666666667e-06,
      "loss": 0.0019,
      "step": 126460
    },
    {
      "epoch": 6.745066666666666,
      "grad_norm": 0.08807721734046936,
      "learning_rate": 7.843333333333333e-06,
      "loss": 0.0027,
      "step": 126470
    },
    {
      "epoch": 6.7456,
      "grad_norm": 0.2059776335954666,
      "learning_rate": 7.84e-06,
      "loss": 0.0013,
      "step": 126480
    },
    {
      "epoch": 6.746133333333333,
      "grad_norm": 0.08797024935483932,
      "learning_rate": 7.836666666666667e-06,
      "loss": 0.0022,
      "step": 126490
    },
    {
      "epoch": 6.746666666666667,
      "grad_norm": 0.1525142788887024,
      "learning_rate": 7.833333333333333e-06,
      "loss": 0.0019,
      "step": 126500
    },
    {
      "epoch": 6.7472,
      "grad_norm": 0.1306445151567459,
      "learning_rate": 7.83e-06,
      "loss": 0.0017,
      "step": 126510
    },
    {
      "epoch": 6.747733333333334,
      "grad_norm": 0.16282235085964203,
      "learning_rate": 7.826666666666667e-06,
      "loss": 0.002,
      "step": 126520
    },
    {
      "epoch": 6.748266666666667,
      "grad_norm": 0.13566677272319794,
      "learning_rate": 7.823333333333334e-06,
      "loss": 0.0015,
      "step": 126530
    },
    {
      "epoch": 6.7488,
      "grad_norm": 0.04970209673047066,
      "learning_rate": 7.820000000000001e-06,
      "loss": 0.0012,
      "step": 126540
    },
    {
      "epoch": 6.749333333333333,
      "grad_norm": 0.12035229057073593,
      "learning_rate": 7.816666666666666e-06,
      "loss": 0.0019,
      "step": 126550
    },
    {
      "epoch": 6.749866666666667,
      "grad_norm": 0.40434929728507996,
      "learning_rate": 7.813333333333334e-06,
      "loss": 0.0024,
      "step": 126560
    },
    {
      "epoch": 6.7504,
      "grad_norm": 0.23232625424861908,
      "learning_rate": 7.810000000000001e-06,
      "loss": 0.0014,
      "step": 126570
    },
    {
      "epoch": 6.750933333333333,
      "grad_norm": 0.06570848822593689,
      "learning_rate": 7.806666666666668e-06,
      "loss": 0.0015,
      "step": 126580
    },
    {
      "epoch": 6.7514666666666665,
      "grad_norm": 0.3730129301548004,
      "learning_rate": 7.803333333333334e-06,
      "loss": 0.0013,
      "step": 126590
    },
    {
      "epoch": 6.752,
      "grad_norm": 0.42202526330947876,
      "learning_rate": 7.8e-06,
      "loss": 0.0019,
      "step": 126600
    },
    {
      "epoch": 6.752533333333333,
      "grad_norm": 0.29921913146972656,
      "learning_rate": 7.796666666666668e-06,
      "loss": 0.002,
      "step": 126610
    },
    {
      "epoch": 6.753066666666666,
      "grad_norm": 0.3624500632286072,
      "learning_rate": 7.793333333333334e-06,
      "loss": 0.0018,
      "step": 126620
    },
    {
      "epoch": 6.7536000000000005,
      "grad_norm": 0.28142809867858887,
      "learning_rate": 7.79e-06,
      "loss": 0.002,
      "step": 126630
    },
    {
      "epoch": 6.754133333333334,
      "grad_norm": 0.0948057696223259,
      "learning_rate": 7.786666666666668e-06,
      "loss": 0.0016,
      "step": 126640
    },
    {
      "epoch": 6.754666666666667,
      "grad_norm": 0.11699549853801727,
      "learning_rate": 7.783333333333334e-06,
      "loss": 0.0016,
      "step": 126650
    },
    {
      "epoch": 6.7552,
      "grad_norm": 0.11907782405614853,
      "learning_rate": 7.78e-06,
      "loss": 0.0016,
      "step": 126660
    },
    {
      "epoch": 6.755733333333334,
      "grad_norm": 0.1281151920557022,
      "learning_rate": 7.776666666666666e-06,
      "loss": 0.0017,
      "step": 126670
    },
    {
      "epoch": 6.756266666666667,
      "grad_norm": 0.3852115571498871,
      "learning_rate": 7.773333333333334e-06,
      "loss": 0.0022,
      "step": 126680
    },
    {
      "epoch": 6.7568,
      "grad_norm": 0.3983968496322632,
      "learning_rate": 7.77e-06,
      "loss": 0.0018,
      "step": 126690
    },
    {
      "epoch": 6.757333333333333,
      "grad_norm": 0.27747678756713867,
      "learning_rate": 7.766666666666666e-06,
      "loss": 0.0016,
      "step": 126700
    },
    {
      "epoch": 6.757866666666667,
      "grad_norm": 0.06258021295070648,
      "learning_rate": 7.763333333333334e-06,
      "loss": 0.0021,
      "step": 126710
    },
    {
      "epoch": 6.7584,
      "grad_norm": 0.17924726009368896,
      "learning_rate": 7.76e-06,
      "loss": 0.0038,
      "step": 126720
    },
    {
      "epoch": 6.758933333333333,
      "grad_norm": 0.1513417512178421,
      "learning_rate": 7.756666666666668e-06,
      "loss": 0.0015,
      "step": 126730
    },
    {
      "epoch": 6.7594666666666665,
      "grad_norm": 0.10204004496335983,
      "learning_rate": 7.753333333333333e-06,
      "loss": 0.0014,
      "step": 126740
    },
    {
      "epoch": 6.76,
      "grad_norm": 0.16174061596393585,
      "learning_rate": 7.75e-06,
      "loss": 0.0016,
      "step": 126750
    },
    {
      "epoch": 6.760533333333333,
      "grad_norm": 0.13505858182907104,
      "learning_rate": 7.746666666666668e-06,
      "loss": 0.0021,
      "step": 126760
    },
    {
      "epoch": 6.761066666666666,
      "grad_norm": 0.4055764079093933,
      "learning_rate": 7.743333333333334e-06,
      "loss": 0.0023,
      "step": 126770
    },
    {
      "epoch": 6.7616,
      "grad_norm": 0.06905309110879898,
      "learning_rate": 7.74e-06,
      "loss": 0.0019,
      "step": 126780
    },
    {
      "epoch": 6.762133333333333,
      "grad_norm": 0.09727489203214645,
      "learning_rate": 7.736666666666667e-06,
      "loss": 0.0012,
      "step": 126790
    },
    {
      "epoch": 6.762666666666667,
      "grad_norm": 0.09129440039396286,
      "learning_rate": 7.733333333333334e-06,
      "loss": 0.0019,
      "step": 126800
    },
    {
      "epoch": 6.7632,
      "grad_norm": 0.17773152887821198,
      "learning_rate": 7.73e-06,
      "loss": 0.0021,
      "step": 126810
    },
    {
      "epoch": 6.763733333333334,
      "grad_norm": 0.0963931754231453,
      "learning_rate": 7.726666666666667e-06,
      "loss": 0.0014,
      "step": 126820
    },
    {
      "epoch": 6.764266666666667,
      "grad_norm": 0.3214661478996277,
      "learning_rate": 7.723333333333334e-06,
      "loss": 0.0017,
      "step": 126830
    },
    {
      "epoch": 6.7648,
      "grad_norm": 0.2084561586380005,
      "learning_rate": 7.72e-06,
      "loss": 0.0015,
      "step": 126840
    },
    {
      "epoch": 6.765333333333333,
      "grad_norm": 0.3571958839893341,
      "learning_rate": 7.716666666666667e-06,
      "loss": 0.0023,
      "step": 126850
    },
    {
      "epoch": 6.765866666666667,
      "grad_norm": 0.4641117751598358,
      "learning_rate": 7.713333333333333e-06,
      "loss": 0.0019,
      "step": 126860
    },
    {
      "epoch": 6.7664,
      "grad_norm": 0.2047567516565323,
      "learning_rate": 7.71e-06,
      "loss": 0.0023,
      "step": 126870
    },
    {
      "epoch": 6.766933333333333,
      "grad_norm": 0.04382852837443352,
      "learning_rate": 7.706666666666667e-06,
      "loss": 0.0014,
      "step": 126880
    },
    {
      "epoch": 6.7674666666666665,
      "grad_norm": 0.2173243910074234,
      "learning_rate": 7.703333333333333e-06,
      "loss": 0.0014,
      "step": 126890
    },
    {
      "epoch": 6.768,
      "grad_norm": 0.5340271592140198,
      "learning_rate": 7.7e-06,
      "loss": 0.0018,
      "step": 126900
    },
    {
      "epoch": 6.768533333333333,
      "grad_norm": 0.07736852765083313,
      "learning_rate": 7.696666666666667e-06,
      "loss": 0.0019,
      "step": 126910
    },
    {
      "epoch": 6.769066666666666,
      "grad_norm": 0.15019887685775757,
      "learning_rate": 7.693333333333335e-06,
      "loss": 0.0014,
      "step": 126920
    },
    {
      "epoch": 6.7696,
      "grad_norm": 0.28902655839920044,
      "learning_rate": 7.69e-06,
      "loss": 0.0029,
      "step": 126930
    },
    {
      "epoch": 6.770133333333334,
      "grad_norm": 0.04248105734586716,
      "learning_rate": 7.686666666666667e-06,
      "loss": 0.002,
      "step": 126940
    },
    {
      "epoch": 6.770666666666667,
      "grad_norm": 0.20920638740062714,
      "learning_rate": 7.683333333333335e-06,
      "loss": 0.0014,
      "step": 126950
    },
    {
      "epoch": 6.7712,
      "grad_norm": 0.25019174814224243,
      "learning_rate": 7.68e-06,
      "loss": 0.0016,
      "step": 126960
    },
    {
      "epoch": 6.771733333333334,
      "grad_norm": 0.26439616084098816,
      "learning_rate": 7.676666666666667e-06,
      "loss": 0.0017,
      "step": 126970
    },
    {
      "epoch": 6.772266666666667,
      "grad_norm": 0.45005515217781067,
      "learning_rate": 7.673333333333333e-06,
      "loss": 0.0018,
      "step": 126980
    },
    {
      "epoch": 6.7728,
      "grad_norm": 0.40621960163116455,
      "learning_rate": 7.670000000000001e-06,
      "loss": 0.0021,
      "step": 126990
    },
    {
      "epoch": 6.773333333333333,
      "grad_norm": 0.1738186627626419,
      "learning_rate": 7.666666666666667e-06,
      "loss": 0.0027,
      "step": 127000
    },
    {
      "epoch": 6.773866666666667,
      "grad_norm": 0.1372481733560562,
      "learning_rate": 7.663333333333333e-06,
      "loss": 0.0018,
      "step": 127010
    },
    {
      "epoch": 6.7744,
      "grad_norm": 0.15226136147975922,
      "learning_rate": 7.660000000000001e-06,
      "loss": 0.0022,
      "step": 127020
    },
    {
      "epoch": 6.774933333333333,
      "grad_norm": 0.1973574161529541,
      "learning_rate": 7.656666666666667e-06,
      "loss": 0.0019,
      "step": 127030
    },
    {
      "epoch": 6.7754666666666665,
      "grad_norm": 0.15264154970645905,
      "learning_rate": 7.653333333333333e-06,
      "loss": 0.0018,
      "step": 127040
    },
    {
      "epoch": 6.776,
      "grad_norm": 0.4461374282836914,
      "learning_rate": 7.65e-06,
      "loss": 0.0031,
      "step": 127050
    },
    {
      "epoch": 6.776533333333333,
      "grad_norm": 0.25256165862083435,
      "learning_rate": 7.646666666666667e-06,
      "loss": 0.0026,
      "step": 127060
    },
    {
      "epoch": 6.777066666666666,
      "grad_norm": 0.05353084206581116,
      "learning_rate": 7.643333333333334e-06,
      "loss": 0.0015,
      "step": 127070
    },
    {
      "epoch": 6.7776,
      "grad_norm": 0.24621997773647308,
      "learning_rate": 7.64e-06,
      "loss": 0.0022,
      "step": 127080
    },
    {
      "epoch": 6.778133333333333,
      "grad_norm": 0.19316783547401428,
      "learning_rate": 7.636666666666668e-06,
      "loss": 0.0017,
      "step": 127090
    },
    {
      "epoch": 6.778666666666666,
      "grad_norm": 0.48097577691078186,
      "learning_rate": 7.633333333333334e-06,
      "loss": 0.002,
      "step": 127100
    },
    {
      "epoch": 6.7792,
      "grad_norm": 0.07417800277471542,
      "learning_rate": 7.630000000000001e-06,
      "loss": 0.0017,
      "step": 127110
    },
    {
      "epoch": 6.779733333333334,
      "grad_norm": 0.1095404177904129,
      "learning_rate": 7.626666666666667e-06,
      "loss": 0.0016,
      "step": 127120
    },
    {
      "epoch": 6.780266666666667,
      "grad_norm": 0.4106461703777313,
      "learning_rate": 7.623333333333334e-06,
      "loss": 0.0014,
      "step": 127130
    },
    {
      "epoch": 6.7808,
      "grad_norm": 0.2357558161020279,
      "learning_rate": 7.620000000000001e-06,
      "loss": 0.0019,
      "step": 127140
    },
    {
      "epoch": 6.781333333333333,
      "grad_norm": 0.3281973600387573,
      "learning_rate": 7.616666666666666e-06,
      "loss": 0.0019,
      "step": 127150
    },
    {
      "epoch": 6.781866666666667,
      "grad_norm": 0.27909088134765625,
      "learning_rate": 7.613333333333334e-06,
      "loss": 0.0022,
      "step": 127160
    },
    {
      "epoch": 6.7824,
      "grad_norm": 0.16486644744873047,
      "learning_rate": 7.610000000000001e-06,
      "loss": 0.0015,
      "step": 127170
    },
    {
      "epoch": 6.782933333333333,
      "grad_norm": 0.3338766396045685,
      "learning_rate": 7.606666666666668e-06,
      "loss": 0.0022,
      "step": 127180
    },
    {
      "epoch": 6.7834666666666665,
      "grad_norm": 0.5055137872695923,
      "learning_rate": 7.603333333333333e-06,
      "loss": 0.0013,
      "step": 127190
    },
    {
      "epoch": 6.784,
      "grad_norm": 0.08858965337276459,
      "learning_rate": 7.6e-06,
      "loss": 0.0014,
      "step": 127200
    },
    {
      "epoch": 6.784533333333333,
      "grad_norm": 0.21584422886371613,
      "learning_rate": 7.596666666666667e-06,
      "loss": 0.002,
      "step": 127210
    },
    {
      "epoch": 6.785066666666666,
      "grad_norm": 0.10373623669147491,
      "learning_rate": 7.593333333333334e-06,
      "loss": 0.0018,
      "step": 127220
    },
    {
      "epoch": 6.7856,
      "grad_norm": 0.09844526648521423,
      "learning_rate": 7.59e-06,
      "loss": 0.0015,
      "step": 127230
    },
    {
      "epoch": 6.786133333333334,
      "grad_norm": 0.5054330825805664,
      "learning_rate": 7.586666666666667e-06,
      "loss": 0.0016,
      "step": 127240
    },
    {
      "epoch": 6.786666666666667,
      "grad_norm": 0.30071020126342773,
      "learning_rate": 7.583333333333334e-06,
      "loss": 0.0017,
      "step": 127250
    },
    {
      "epoch": 6.7872,
      "grad_norm": 0.14639641344547272,
      "learning_rate": 7.580000000000001e-06,
      "loss": 0.0015,
      "step": 127260
    },
    {
      "epoch": 6.787733333333334,
      "grad_norm": 0.07048877328634262,
      "learning_rate": 7.576666666666666e-06,
      "loss": 0.0022,
      "step": 127270
    },
    {
      "epoch": 6.788266666666667,
      "grad_norm": 0.0417528972029686,
      "learning_rate": 7.573333333333333e-06,
      "loss": 0.0014,
      "step": 127280
    },
    {
      "epoch": 6.7888,
      "grad_norm": 0.17798569798469543,
      "learning_rate": 7.57e-06,
      "loss": 0.0018,
      "step": 127290
    },
    {
      "epoch": 6.789333333333333,
      "grad_norm": 0.12088410556316376,
      "learning_rate": 7.5666666666666665e-06,
      "loss": 0.0021,
      "step": 127300
    },
    {
      "epoch": 6.789866666666667,
      "grad_norm": 0.05020357668399811,
      "learning_rate": 7.5633333333333335e-06,
      "loss": 0.0012,
      "step": 127310
    },
    {
      "epoch": 6.7904,
      "grad_norm": 0.07347037643194199,
      "learning_rate": 7.5600000000000005e-06,
      "loss": 0.0016,
      "step": 127320
    },
    {
      "epoch": 6.790933333333333,
      "grad_norm": 0.2636435031890869,
      "learning_rate": 7.5566666666666674e-06,
      "loss": 0.0019,
      "step": 127330
    },
    {
      "epoch": 6.7914666666666665,
      "grad_norm": 0.05392080172896385,
      "learning_rate": 7.553333333333333e-06,
      "loss": 0.0013,
      "step": 127340
    },
    {
      "epoch": 6.792,
      "grad_norm": 0.21012039482593536,
      "learning_rate": 7.55e-06,
      "loss": 0.0016,
      "step": 127350
    },
    {
      "epoch": 6.792533333333333,
      "grad_norm": 0.23082321882247925,
      "learning_rate": 7.5466666666666675e-06,
      "loss": 0.0021,
      "step": 127360
    },
    {
      "epoch": 6.793066666666666,
      "grad_norm": 0.32681024074554443,
      "learning_rate": 7.5433333333333345e-06,
      "loss": 0.0018,
      "step": 127370
    },
    {
      "epoch": 6.7936,
      "grad_norm": 0.33039140701293945,
      "learning_rate": 7.54e-06,
      "loss": 0.0016,
      "step": 127380
    },
    {
      "epoch": 6.794133333333333,
      "grad_norm": 0.24044688045978546,
      "learning_rate": 7.536666666666667e-06,
      "loss": 0.0021,
      "step": 127390
    },
    {
      "epoch": 6.794666666666666,
      "grad_norm": 0.24536541104316711,
      "learning_rate": 7.533333333333334e-06,
      "loss": 0.0015,
      "step": 127400
    },
    {
      "epoch": 6.7952,
      "grad_norm": 0.3984299898147583,
      "learning_rate": 7.530000000000001e-06,
      "loss": 0.0018,
      "step": 127410
    },
    {
      "epoch": 6.795733333333334,
      "grad_norm": 0.26322871446609497,
      "learning_rate": 7.526666666666667e-06,
      "loss": 0.0019,
      "step": 127420
    },
    {
      "epoch": 6.796266666666667,
      "grad_norm": 0.35644036531448364,
      "learning_rate": 7.523333333333334e-06,
      "loss": 0.0027,
      "step": 127430
    },
    {
      "epoch": 6.7968,
      "grad_norm": 0.18059778213500977,
      "learning_rate": 7.520000000000001e-06,
      "loss": 0.0013,
      "step": 127440
    },
    {
      "epoch": 6.7973333333333334,
      "grad_norm": 0.24414174258708954,
      "learning_rate": 7.516666666666668e-06,
      "loss": 0.0016,
      "step": 127450
    },
    {
      "epoch": 6.797866666666667,
      "grad_norm": 0.06106943264603615,
      "learning_rate": 7.513333333333333e-06,
      "loss": 0.0022,
      "step": 127460
    },
    {
      "epoch": 6.7984,
      "grad_norm": 0.36381494998931885,
      "learning_rate": 7.51e-06,
      "loss": 0.002,
      "step": 127470
    },
    {
      "epoch": 6.798933333333333,
      "grad_norm": 0.15009932219982147,
      "learning_rate": 7.506666666666667e-06,
      "loss": 0.0013,
      "step": 127480
    },
    {
      "epoch": 6.7994666666666665,
      "grad_norm": 0.17557267844676971,
      "learning_rate": 7.503333333333333e-06,
      "loss": 0.002,
      "step": 127490
    },
    {
      "epoch": 6.8,
      "grad_norm": 0.16697345674037933,
      "learning_rate": 7.5e-06,
      "loss": 0.0019,
      "step": 127500
    },
    {
      "epoch": 6.800533333333333,
      "grad_norm": 0.06263522803783417,
      "learning_rate": 7.496666666666667e-06,
      "loss": 0.0021,
      "step": 127510
    },
    {
      "epoch": 6.801066666666666,
      "grad_norm": 0.023200813680887222,
      "learning_rate": 7.493333333333334e-06,
      "loss": 0.0016,
      "step": 127520
    },
    {
      "epoch": 6.8016,
      "grad_norm": 0.18228797614574432,
      "learning_rate": 7.4899999999999994e-06,
      "loss": 0.002,
      "step": 127530
    },
    {
      "epoch": 6.802133333333334,
      "grad_norm": 0.22934937477111816,
      "learning_rate": 7.486666666666666e-06,
      "loss": 0.0019,
      "step": 127540
    },
    {
      "epoch": 6.802666666666667,
      "grad_norm": 0.2342575043439865,
      "learning_rate": 7.483333333333334e-06,
      "loss": 0.002,
      "step": 127550
    },
    {
      "epoch": 6.8032,
      "grad_norm": 0.273131787776947,
      "learning_rate": 7.480000000000001e-06,
      "loss": 0.0019,
      "step": 127560
    },
    {
      "epoch": 6.803733333333334,
      "grad_norm": 0.04216974601149559,
      "learning_rate": 7.4766666666666665e-06,
      "loss": 0.0015,
      "step": 127570
    },
    {
      "epoch": 6.804266666666667,
      "grad_norm": 0.24594953656196594,
      "learning_rate": 7.4733333333333335e-06,
      "loss": 0.0015,
      "step": 127580
    },
    {
      "epoch": 6.8048,
      "grad_norm": 0.24486158788204193,
      "learning_rate": 7.4700000000000005e-06,
      "loss": 0.0022,
      "step": 127590
    },
    {
      "epoch": 6.8053333333333335,
      "grad_norm": 0.2954450845718384,
      "learning_rate": 7.4666666666666675e-06,
      "loss": 0.0017,
      "step": 127600
    },
    {
      "epoch": 6.805866666666667,
      "grad_norm": 0.20095521211624146,
      "learning_rate": 7.463333333333334e-06,
      "loss": 0.0015,
      "step": 127610
    },
    {
      "epoch": 6.8064,
      "grad_norm": 0.11847329139709473,
      "learning_rate": 7.4600000000000006e-06,
      "loss": 0.0019,
      "step": 127620
    },
    {
      "epoch": 6.806933333333333,
      "grad_norm": 0.09819482266902924,
      "learning_rate": 7.4566666666666676e-06,
      "loss": 0.0014,
      "step": 127630
    },
    {
      "epoch": 6.8074666666666666,
      "grad_norm": 0.09513389319181442,
      "learning_rate": 7.453333333333333e-06,
      "loss": 0.0019,
      "step": 127640
    },
    {
      "epoch": 6.808,
      "grad_norm": 0.06166653335094452,
      "learning_rate": 7.45e-06,
      "loss": 0.0025,
      "step": 127650
    },
    {
      "epoch": 6.808533333333333,
      "grad_norm": 0.2759440243244171,
      "learning_rate": 7.446666666666667e-06,
      "loss": 0.0017,
      "step": 127660
    },
    {
      "epoch": 6.809066666666666,
      "grad_norm": 0.26424941420555115,
      "learning_rate": 7.443333333333334e-06,
      "loss": 0.0015,
      "step": 127670
    },
    {
      "epoch": 6.8096,
      "grad_norm": 0.18139205873012543,
      "learning_rate": 7.44e-06,
      "loss": 0.0024,
      "step": 127680
    },
    {
      "epoch": 6.810133333333333,
      "grad_norm": 0.1590791791677475,
      "learning_rate": 7.436666666666667e-06,
      "loss": 0.0014,
      "step": 127690
    },
    {
      "epoch": 6.810666666666666,
      "grad_norm": 0.17387522757053375,
      "learning_rate": 7.433333333333334e-06,
      "loss": 0.0017,
      "step": 127700
    },
    {
      "epoch": 6.8112,
      "grad_norm": 0.730846107006073,
      "learning_rate": 7.430000000000001e-06,
      "loss": 0.0016,
      "step": 127710
    },
    {
      "epoch": 6.811733333333334,
      "grad_norm": 0.18255814909934998,
      "learning_rate": 7.426666666666666e-06,
      "loss": 0.0026,
      "step": 127720
    },
    {
      "epoch": 6.812266666666667,
      "grad_norm": 0.12620271742343903,
      "learning_rate": 7.423333333333333e-06,
      "loss": 0.0013,
      "step": 127730
    },
    {
      "epoch": 6.8128,
      "grad_norm": 0.03717890754342079,
      "learning_rate": 7.420000000000001e-06,
      "loss": 0.0014,
      "step": 127740
    },
    {
      "epoch": 6.8133333333333335,
      "grad_norm": 0.0928102359175682,
      "learning_rate": 7.416666666666668e-06,
      "loss": 0.0026,
      "step": 127750
    },
    {
      "epoch": 6.813866666666667,
      "grad_norm": 0.2715108096599579,
      "learning_rate": 7.413333333333333e-06,
      "loss": 0.0015,
      "step": 127760
    },
    {
      "epoch": 6.8144,
      "grad_norm": 0.06787347793579102,
      "learning_rate": 7.41e-06,
      "loss": 0.0021,
      "step": 127770
    },
    {
      "epoch": 6.814933333333333,
      "grad_norm": 0.42101794481277466,
      "learning_rate": 7.406666666666667e-06,
      "loss": 0.0015,
      "step": 127780
    },
    {
      "epoch": 6.815466666666667,
      "grad_norm": 0.1479385495185852,
      "learning_rate": 7.403333333333334e-06,
      "loss": 0.0017,
      "step": 127790
    },
    {
      "epoch": 6.816,
      "grad_norm": 0.19455638527870178,
      "learning_rate": 7.4e-06,
      "loss": 0.0025,
      "step": 127800
    },
    {
      "epoch": 6.816533333333333,
      "grad_norm": 0.16534601151943207,
      "learning_rate": 7.396666666666667e-06,
      "loss": 0.0016,
      "step": 127810
    },
    {
      "epoch": 6.817066666666666,
      "grad_norm": 0.04790164530277252,
      "learning_rate": 7.393333333333334e-06,
      "loss": 0.0027,
      "step": 127820
    },
    {
      "epoch": 6.8176,
      "grad_norm": 0.1847248077392578,
      "learning_rate": 7.3899999999999995e-06,
      "loss": 0.0022,
      "step": 127830
    },
    {
      "epoch": 6.818133333333334,
      "grad_norm": 0.059426162391901016,
      "learning_rate": 7.3866666666666665e-06,
      "loss": 0.0025,
      "step": 127840
    },
    {
      "epoch": 6.818666666666667,
      "grad_norm": 0.29222559928894043,
      "learning_rate": 7.3833333333333335e-06,
      "loss": 0.0027,
      "step": 127850
    },
    {
      "epoch": 6.8192,
      "grad_norm": 0.4509516656398773,
      "learning_rate": 7.3800000000000005e-06,
      "loss": 0.0021,
      "step": 127860
    },
    {
      "epoch": 6.819733333333334,
      "grad_norm": 0.2413158118724823,
      "learning_rate": 7.376666666666667e-06,
      "loss": 0.0031,
      "step": 127870
    },
    {
      "epoch": 6.820266666666667,
      "grad_norm": 0.21316996216773987,
      "learning_rate": 7.373333333333334e-06,
      "loss": 0.0013,
      "step": 127880
    },
    {
      "epoch": 6.8208,
      "grad_norm": 0.09638478606939316,
      "learning_rate": 7.370000000000001e-06,
      "loss": 0.0016,
      "step": 127890
    },
    {
      "epoch": 6.8213333333333335,
      "grad_norm": 0.24362176656723022,
      "learning_rate": 7.3666666666666676e-06,
      "loss": 0.0025,
      "step": 127900
    },
    {
      "epoch": 6.821866666666667,
      "grad_norm": 0.09909268468618393,
      "learning_rate": 7.363333333333333e-06,
      "loss": 0.0027,
      "step": 127910
    },
    {
      "epoch": 6.8224,
      "grad_norm": 0.1483522653579712,
      "learning_rate": 7.36e-06,
      "loss": 0.0022,
      "step": 127920
    },
    {
      "epoch": 6.822933333333333,
      "grad_norm": 0.20598572492599487,
      "learning_rate": 7.356666666666668e-06,
      "loss": 0.0013,
      "step": 127930
    },
    {
      "epoch": 6.823466666666667,
      "grad_norm": 0.12447527796030045,
      "learning_rate": 7.353333333333335e-06,
      "loss": 0.0016,
      "step": 127940
    },
    {
      "epoch": 6.824,
      "grad_norm": 0.4097958505153656,
      "learning_rate": 7.35e-06,
      "loss": 0.0028,
      "step": 127950
    },
    {
      "epoch": 6.824533333333333,
      "grad_norm": 0.5190454125404358,
      "learning_rate": 7.346666666666667e-06,
      "loss": 0.0023,
      "step": 127960
    },
    {
      "epoch": 6.825066666666666,
      "grad_norm": 0.040998317301273346,
      "learning_rate": 7.343333333333334e-06,
      "loss": 0.0019,
      "step": 127970
    },
    {
      "epoch": 6.8256,
      "grad_norm": 0.10350655019283295,
      "learning_rate": 7.340000000000001e-06,
      "loss": 0.0018,
      "step": 127980
    },
    {
      "epoch": 6.826133333333333,
      "grad_norm": 0.15272045135498047,
      "learning_rate": 7.336666666666667e-06,
      "loss": 0.0013,
      "step": 127990
    },
    {
      "epoch": 6.826666666666666,
      "grad_norm": 0.1756991147994995,
      "learning_rate": 7.333333333333334e-06,
      "loss": 0.0017,
      "step": 128000
    },
    {
      "epoch": 6.8272,
      "grad_norm": 0.06621330976486206,
      "learning_rate": 7.330000000000001e-06,
      "loss": 0.002,
      "step": 128010
    },
    {
      "epoch": 6.827733333333334,
      "grad_norm": 0.02302822284400463,
      "learning_rate": 7.326666666666666e-06,
      "loss": 0.0014,
      "step": 128020
    },
    {
      "epoch": 6.828266666666667,
      "grad_norm": 0.03542370721697807,
      "learning_rate": 7.323333333333333e-06,
      "loss": 0.0018,
      "step": 128030
    },
    {
      "epoch": 6.8288,
      "grad_norm": 0.0774116963148117,
      "learning_rate": 7.32e-06,
      "loss": 0.0027,
      "step": 128040
    },
    {
      "epoch": 6.8293333333333335,
      "grad_norm": 0.06494374573230743,
      "learning_rate": 7.316666666666667e-06,
      "loss": 0.0014,
      "step": 128050
    },
    {
      "epoch": 6.829866666666667,
      "grad_norm": 0.09051784873008728,
      "learning_rate": 7.313333333333333e-06,
      "loss": 0.0017,
      "step": 128060
    },
    {
      "epoch": 6.8304,
      "grad_norm": 0.18131837248802185,
      "learning_rate": 7.31e-06,
      "loss": 0.0016,
      "step": 128070
    },
    {
      "epoch": 6.830933333333333,
      "grad_norm": 0.021719083189964294,
      "learning_rate": 7.306666666666667e-06,
      "loss": 0.0021,
      "step": 128080
    },
    {
      "epoch": 6.831466666666667,
      "grad_norm": 0.2631871700286865,
      "learning_rate": 7.303333333333334e-06,
      "loss": 0.0018,
      "step": 128090
    },
    {
      "epoch": 6.832,
      "grad_norm": 0.30152082443237305,
      "learning_rate": 7.2999999999999996e-06,
      "loss": 0.0021,
      "step": 128100
    },
    {
      "epoch": 6.832533333333333,
      "grad_norm": 0.09750273823738098,
      "learning_rate": 7.2966666666666665e-06,
      "loss": 0.0014,
      "step": 128110
    },
    {
      "epoch": 6.833066666666666,
      "grad_norm": 0.32296910881996155,
      "learning_rate": 7.293333333333334e-06,
      "loss": 0.0021,
      "step": 128120
    },
    {
      "epoch": 6.8336,
      "grad_norm": 0.23413248360157013,
      "learning_rate": 7.290000000000001e-06,
      "loss": 0.0017,
      "step": 128130
    },
    {
      "epoch": 6.834133333333333,
      "grad_norm": 0.2637133300304413,
      "learning_rate": 7.286666666666667e-06,
      "loss": 0.003,
      "step": 128140
    },
    {
      "epoch": 6.834666666666667,
      "grad_norm": 0.052549079060554504,
      "learning_rate": 7.283333333333334e-06,
      "loss": 0.0014,
      "step": 128150
    },
    {
      "epoch": 6.8352,
      "grad_norm": 0.1634441763162613,
      "learning_rate": 7.280000000000001e-06,
      "loss": 0.0024,
      "step": 128160
    },
    {
      "epoch": 6.835733333333334,
      "grad_norm": 0.2172875553369522,
      "learning_rate": 7.276666666666667e-06,
      "loss": 0.0016,
      "step": 128170
    },
    {
      "epoch": 6.836266666666667,
      "grad_norm": 0.20502792298793793,
      "learning_rate": 7.273333333333334e-06,
      "loss": 0.002,
      "step": 128180
    },
    {
      "epoch": 6.8368,
      "grad_norm": 0.33336225152015686,
      "learning_rate": 7.270000000000001e-06,
      "loss": 0.0017,
      "step": 128190
    },
    {
      "epoch": 6.8373333333333335,
      "grad_norm": 0.1536148339509964,
      "learning_rate": 7.266666666666668e-06,
      "loss": 0.0015,
      "step": 128200
    },
    {
      "epoch": 6.837866666666667,
      "grad_norm": 0.12073595076799393,
      "learning_rate": 7.263333333333333e-06,
      "loss": 0.0014,
      "step": 128210
    },
    {
      "epoch": 6.8384,
      "grad_norm": 0.29787325859069824,
      "learning_rate": 7.26e-06,
      "loss": 0.002,
      "step": 128220
    },
    {
      "epoch": 6.838933333333333,
      "grad_norm": 0.09093793481588364,
      "learning_rate": 7.256666666666667e-06,
      "loss": 0.0014,
      "step": 128230
    },
    {
      "epoch": 6.839466666666667,
      "grad_norm": 0.25557664036750793,
      "learning_rate": 7.253333333333334e-06,
      "loss": 0.0019,
      "step": 128240
    },
    {
      "epoch": 6.84,
      "grad_norm": 0.17954370379447937,
      "learning_rate": 7.25e-06,
      "loss": 0.0012,
      "step": 128250
    },
    {
      "epoch": 6.840533333333333,
      "grad_norm": 0.26671603322029114,
      "learning_rate": 7.246666666666667e-06,
      "loss": 0.0033,
      "step": 128260
    },
    {
      "epoch": 6.841066666666666,
      "grad_norm": 0.09155704081058502,
      "learning_rate": 7.243333333333334e-06,
      "loss": 0.0017,
      "step": 128270
    },
    {
      "epoch": 6.8416,
      "grad_norm": 0.1556602418422699,
      "learning_rate": 7.240000000000001e-06,
      "loss": 0.0016,
      "step": 128280
    },
    {
      "epoch": 6.842133333333333,
      "grad_norm": 0.1576097309589386,
      "learning_rate": 7.236666666666666e-06,
      "loss": 0.0024,
      "step": 128290
    },
    {
      "epoch": 6.842666666666666,
      "grad_norm": 0.1229703277349472,
      "learning_rate": 7.233333333333333e-06,
      "loss": 0.0017,
      "step": 128300
    },
    {
      "epoch": 6.8431999999999995,
      "grad_norm": 0.4069574475288391,
      "learning_rate": 7.230000000000001e-06,
      "loss": 0.0014,
      "step": 128310
    },
    {
      "epoch": 6.843733333333334,
      "grad_norm": 0.5921602845191956,
      "learning_rate": 7.226666666666668e-06,
      "loss": 0.002,
      "step": 128320
    },
    {
      "epoch": 6.844266666666667,
      "grad_norm": 0.18327026069164276,
      "learning_rate": 7.223333333333333e-06,
      "loss": 0.0026,
      "step": 128330
    },
    {
      "epoch": 6.8448,
      "grad_norm": 0.10126864165067673,
      "learning_rate": 7.22e-06,
      "loss": 0.0014,
      "step": 128340
    },
    {
      "epoch": 6.8453333333333335,
      "grad_norm": 0.1514226794242859,
      "learning_rate": 7.216666666666667e-06,
      "loss": 0.0016,
      "step": 128350
    },
    {
      "epoch": 6.845866666666667,
      "grad_norm": 0.05389101803302765,
      "learning_rate": 7.2133333333333334e-06,
      "loss": 0.0015,
      "step": 128360
    },
    {
      "epoch": 6.8464,
      "grad_norm": 0.023016033694148064,
      "learning_rate": 7.2100000000000004e-06,
      "loss": 0.0019,
      "step": 128370
    },
    {
      "epoch": 6.846933333333333,
      "grad_norm": 0.2968887686729431,
      "learning_rate": 7.206666666666667e-06,
      "loss": 0.0029,
      "step": 128380
    },
    {
      "epoch": 6.847466666666667,
      "grad_norm": 0.5304802060127258,
      "learning_rate": 7.203333333333334e-06,
      "loss": 0.0016,
      "step": 128390
    },
    {
      "epoch": 6.848,
      "grad_norm": 0.3844018280506134,
      "learning_rate": 7.2e-06,
      "loss": 0.0024,
      "step": 128400
    },
    {
      "epoch": 6.848533333333333,
      "grad_norm": 0.4209502637386322,
      "learning_rate": 7.196666666666667e-06,
      "loss": 0.0024,
      "step": 128410
    },
    {
      "epoch": 6.849066666666666,
      "grad_norm": 0.1778600662946701,
      "learning_rate": 7.193333333333334e-06,
      "loss": 0.0018,
      "step": 128420
    },
    {
      "epoch": 6.8496,
      "grad_norm": 0.2630998194217682,
      "learning_rate": 7.190000000000001e-06,
      "loss": 0.0013,
      "step": 128430
    },
    {
      "epoch": 6.850133333333333,
      "grad_norm": 0.06383124738931656,
      "learning_rate": 7.186666666666667e-06,
      "loss": 0.0014,
      "step": 128440
    },
    {
      "epoch": 6.850666666666667,
      "grad_norm": 0.15006405115127563,
      "learning_rate": 7.183333333333334e-06,
      "loss": 0.0015,
      "step": 128450
    },
    {
      "epoch": 6.8512,
      "grad_norm": 0.09029431641101837,
      "learning_rate": 7.180000000000001e-06,
      "loss": 0.002,
      "step": 128460
    },
    {
      "epoch": 6.851733333333334,
      "grad_norm": 0.3306884765625,
      "learning_rate": 7.176666666666668e-06,
      "loss": 0.0023,
      "step": 128470
    },
    {
      "epoch": 6.852266666666667,
      "grad_norm": 0.18974748253822327,
      "learning_rate": 7.173333333333333e-06,
      "loss": 0.0021,
      "step": 128480
    },
    {
      "epoch": 6.8528,
      "grad_norm": 0.15937988460063934,
      "learning_rate": 7.17e-06,
      "loss": 0.0028,
      "step": 128490
    },
    {
      "epoch": 6.8533333333333335,
      "grad_norm": 0.1754148304462433,
      "learning_rate": 7.166666666666667e-06,
      "loss": 0.0022,
      "step": 128500
    },
    {
      "epoch": 6.853866666666667,
      "grad_norm": 0.15028400719165802,
      "learning_rate": 7.163333333333333e-06,
      "loss": 0.0018,
      "step": 128510
    },
    {
      "epoch": 6.8544,
      "grad_norm": 0.5977974534034729,
      "learning_rate": 7.16e-06,
      "loss": 0.0017,
      "step": 128520
    },
    {
      "epoch": 6.854933333333333,
      "grad_norm": 0.05082259327173233,
      "learning_rate": 7.156666666666667e-06,
      "loss": 0.0025,
      "step": 128530
    },
    {
      "epoch": 6.855466666666667,
      "grad_norm": 0.04020378366112709,
      "learning_rate": 7.153333333333334e-06,
      "loss": 0.0015,
      "step": 128540
    },
    {
      "epoch": 6.856,
      "grad_norm": 0.26407313346862793,
      "learning_rate": 7.15e-06,
      "loss": 0.0014,
      "step": 128550
    },
    {
      "epoch": 6.856533333333333,
      "grad_norm": 0.12252207100391388,
      "learning_rate": 7.146666666666667e-06,
      "loss": 0.0011,
      "step": 128560
    },
    {
      "epoch": 6.857066666666666,
      "grad_norm": 0.07506413012742996,
      "learning_rate": 7.143333333333334e-06,
      "loss": 0.0012,
      "step": 128570
    },
    {
      "epoch": 6.8576,
      "grad_norm": 0.18934816122055054,
      "learning_rate": 7.140000000000001e-06,
      "loss": 0.0015,
      "step": 128580
    },
    {
      "epoch": 6.858133333333333,
      "grad_norm": 0.12337154895067215,
      "learning_rate": 7.136666666666666e-06,
      "loss": 0.0012,
      "step": 128590
    },
    {
      "epoch": 6.858666666666666,
      "grad_norm": 0.17630212008953094,
      "learning_rate": 7.133333333333333e-06,
      "loss": 0.0014,
      "step": 128600
    },
    {
      "epoch": 6.8591999999999995,
      "grad_norm": 0.24482062458992004,
      "learning_rate": 7.13e-06,
      "loss": 0.0013,
      "step": 128610
    },
    {
      "epoch": 6.859733333333334,
      "grad_norm": 0.23530347645282745,
      "learning_rate": 7.126666666666667e-06,
      "loss": 0.0023,
      "step": 128620
    },
    {
      "epoch": 6.860266666666667,
      "grad_norm": 0.2734889090061188,
      "learning_rate": 7.1233333333333335e-06,
      "loss": 0.002,
      "step": 128630
    },
    {
      "epoch": 6.8608,
      "grad_norm": 0.5743515491485596,
      "learning_rate": 7.1200000000000004e-06,
      "loss": 0.003,
      "step": 128640
    },
    {
      "epoch": 6.8613333333333335,
      "grad_norm": 0.3694200813770294,
      "learning_rate": 7.116666666666667e-06,
      "loss": 0.002,
      "step": 128650
    },
    {
      "epoch": 6.861866666666667,
      "grad_norm": 0.08610118925571442,
      "learning_rate": 7.113333333333334e-06,
      "loss": 0.0013,
      "step": 128660
    },
    {
      "epoch": 6.8624,
      "grad_norm": 0.2050997018814087,
      "learning_rate": 7.11e-06,
      "loss": 0.0011,
      "step": 128670
    },
    {
      "epoch": 6.862933333333333,
      "grad_norm": 0.10102622210979462,
      "learning_rate": 7.106666666666667e-06,
      "loss": 0.0019,
      "step": 128680
    },
    {
      "epoch": 6.863466666666667,
      "grad_norm": 0.18326851725578308,
      "learning_rate": 7.103333333333334e-06,
      "loss": 0.0017,
      "step": 128690
    },
    {
      "epoch": 6.864,
      "grad_norm": 0.10353945195674896,
      "learning_rate": 7.1e-06,
      "loss": 0.0017,
      "step": 128700
    },
    {
      "epoch": 6.864533333333333,
      "grad_norm": 0.18084457516670227,
      "learning_rate": 7.096666666666667e-06,
      "loss": 0.002,
      "step": 128710
    },
    {
      "epoch": 6.865066666666666,
      "grad_norm": 0.09629335254430771,
      "learning_rate": 7.093333333333334e-06,
      "loss": 0.0015,
      "step": 128720
    },
    {
      "epoch": 6.8656,
      "grad_norm": 0.03272230178117752,
      "learning_rate": 7.090000000000001e-06,
      "loss": 0.0019,
      "step": 128730
    },
    {
      "epoch": 6.866133333333333,
      "grad_norm": 0.20030660927295685,
      "learning_rate": 7.086666666666667e-06,
      "loss": 0.002,
      "step": 128740
    },
    {
      "epoch": 6.866666666666667,
      "grad_norm": 0.20951013267040253,
      "learning_rate": 7.083333333333334e-06,
      "loss": 0.0017,
      "step": 128750
    },
    {
      "epoch": 6.8672,
      "grad_norm": 0.32275599241256714,
      "learning_rate": 7.080000000000001e-06,
      "loss": 0.0024,
      "step": 128760
    },
    {
      "epoch": 6.867733333333334,
      "grad_norm": 0.08815542608499527,
      "learning_rate": 7.076666666666668e-06,
      "loss": 0.0019,
      "step": 128770
    },
    {
      "epoch": 6.868266666666667,
      "grad_norm": 0.15658465027809143,
      "learning_rate": 7.073333333333333e-06,
      "loss": 0.0013,
      "step": 128780
    },
    {
      "epoch": 6.8688,
      "grad_norm": 0.11929573863744736,
      "learning_rate": 7.07e-06,
      "loss": 0.0021,
      "step": 128790
    },
    {
      "epoch": 6.8693333333333335,
      "grad_norm": 0.15344585478305817,
      "learning_rate": 7.066666666666667e-06,
      "loss": 0.0022,
      "step": 128800
    },
    {
      "epoch": 6.869866666666667,
      "grad_norm": 0.3549048900604248,
      "learning_rate": 7.063333333333334e-06,
      "loss": 0.002,
      "step": 128810
    },
    {
      "epoch": 6.8704,
      "grad_norm": 0.26565811038017273,
      "learning_rate": 7.06e-06,
      "loss": 0.0012,
      "step": 128820
    },
    {
      "epoch": 6.870933333333333,
      "grad_norm": 0.44676920771598816,
      "learning_rate": 7.056666666666667e-06,
      "loss": 0.0017,
      "step": 128830
    },
    {
      "epoch": 6.871466666666667,
      "grad_norm": 0.15448151528835297,
      "learning_rate": 7.053333333333334e-06,
      "loss": 0.0016,
      "step": 128840
    },
    {
      "epoch": 6.872,
      "grad_norm": 0.0731164887547493,
      "learning_rate": 7.049999999999999e-06,
      "loss": 0.0023,
      "step": 128850
    },
    {
      "epoch": 6.872533333333333,
      "grad_norm": 0.1415238231420517,
      "learning_rate": 7.046666666666666e-06,
      "loss": 0.0013,
      "step": 128860
    },
    {
      "epoch": 6.873066666666666,
      "grad_norm": 0.12817981839179993,
      "learning_rate": 7.043333333333333e-06,
      "loss": 0.0024,
      "step": 128870
    },
    {
      "epoch": 6.8736,
      "grad_norm": 0.19185703992843628,
      "learning_rate": 7.04e-06,
      "loss": 0.003,
      "step": 128880
    },
    {
      "epoch": 6.874133333333333,
      "grad_norm": 0.2645520567893982,
      "learning_rate": 7.0366666666666665e-06,
      "loss": 0.0017,
      "step": 128890
    },
    {
      "epoch": 6.874666666666666,
      "grad_norm": 0.23653005063533783,
      "learning_rate": 7.0333333333333335e-06,
      "loss": 0.0016,
      "step": 128900
    },
    {
      "epoch": 6.8751999999999995,
      "grad_norm": 0.27679499983787537,
      "learning_rate": 7.0300000000000005e-06,
      "loss": 0.0019,
      "step": 128910
    },
    {
      "epoch": 6.875733333333334,
      "grad_norm": 0.231097012758255,
      "learning_rate": 7.0266666666666674e-06,
      "loss": 0.0015,
      "step": 128920
    },
    {
      "epoch": 6.876266666666667,
      "grad_norm": 0.1498735398054123,
      "learning_rate": 7.0233333333333336e-06,
      "loss": 0.0018,
      "step": 128930
    },
    {
      "epoch": 6.8768,
      "grad_norm": 0.14923322200775146,
      "learning_rate": 7.0200000000000006e-06,
      "loss": 0.0018,
      "step": 128940
    },
    {
      "epoch": 6.8773333333333335,
      "grad_norm": 0.15979117155075073,
      "learning_rate": 7.0166666666666675e-06,
      "loss": 0.0015,
      "step": 128950
    },
    {
      "epoch": 6.877866666666667,
      "grad_norm": 0.09931924194097519,
      "learning_rate": 7.0133333333333345e-06,
      "loss": 0.002,
      "step": 128960
    },
    {
      "epoch": 6.8784,
      "grad_norm": 0.3858144283294678,
      "learning_rate": 7.01e-06,
      "loss": 0.0015,
      "step": 128970
    },
    {
      "epoch": 6.878933333333333,
      "grad_norm": 0.12566007673740387,
      "learning_rate": 7.006666666666667e-06,
      "loss": 0.0019,
      "step": 128980
    },
    {
      "epoch": 6.879466666666667,
      "grad_norm": 0.03587299585342407,
      "learning_rate": 7.003333333333334e-06,
      "loss": 0.0019,
      "step": 128990
    },
    {
      "epoch": 6.88,
      "grad_norm": 0.027977891266345978,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.0015,
      "step": 129000
    },
    {
      "epoch": 6.880533333333333,
      "grad_norm": 0.18366271257400513,
      "learning_rate": 6.996666666666667e-06,
      "loss": 0.0016,
      "step": 129010
    },
    {
      "epoch": 6.881066666666666,
      "grad_norm": 0.28109273314476013,
      "learning_rate": 6.993333333333334e-06,
      "loss": 0.002,
      "step": 129020
    },
    {
      "epoch": 6.8816,
      "grad_norm": 0.07433663308620453,
      "learning_rate": 6.990000000000001e-06,
      "loss": 0.0016,
      "step": 129030
    },
    {
      "epoch": 6.882133333333333,
      "grad_norm": 0.18710823357105255,
      "learning_rate": 6.986666666666666e-06,
      "loss": 0.0013,
      "step": 129040
    },
    {
      "epoch": 6.882666666666667,
      "grad_norm": 0.05293037369847298,
      "learning_rate": 6.983333333333333e-06,
      "loss": 0.0019,
      "step": 129050
    },
    {
      "epoch": 6.8832,
      "grad_norm": 0.38846299052238464,
      "learning_rate": 6.98e-06,
      "loss": 0.0029,
      "step": 129060
    },
    {
      "epoch": 6.883733333333334,
      "grad_norm": 0.32566869258880615,
      "learning_rate": 6.976666666666667e-06,
      "loss": 0.0026,
      "step": 129070
    },
    {
      "epoch": 6.884266666666667,
      "grad_norm": 0.07018360495567322,
      "learning_rate": 6.973333333333333e-06,
      "loss": 0.0015,
      "step": 129080
    },
    {
      "epoch": 6.8848,
      "grad_norm": 0.32319605350494385,
      "learning_rate": 6.97e-06,
      "loss": 0.002,
      "step": 129090
    },
    {
      "epoch": 6.8853333333333335,
      "grad_norm": 0.09681417047977448,
      "learning_rate": 6.966666666666667e-06,
      "loss": 0.0022,
      "step": 129100
    },
    {
      "epoch": 6.885866666666667,
      "grad_norm": 0.4897111654281616,
      "learning_rate": 6.963333333333334e-06,
      "loss": 0.0013,
      "step": 129110
    },
    {
      "epoch": 6.8864,
      "grad_norm": 0.2724406123161316,
      "learning_rate": 6.9599999999999994e-06,
      "loss": 0.0014,
      "step": 129120
    },
    {
      "epoch": 6.886933333333333,
      "grad_norm": 0.12464819103479385,
      "learning_rate": 6.956666666666667e-06,
      "loss": 0.0015,
      "step": 129130
    },
    {
      "epoch": 6.887466666666667,
      "grad_norm": 0.30402669310569763,
      "learning_rate": 6.953333333333334e-06,
      "loss": 0.0017,
      "step": 129140
    },
    {
      "epoch": 6.888,
      "grad_norm": 0.19014546275138855,
      "learning_rate": 6.950000000000001e-06,
      "loss": 0.002,
      "step": 129150
    },
    {
      "epoch": 6.888533333333333,
      "grad_norm": 0.2644828259944916,
      "learning_rate": 6.9466666666666665e-06,
      "loss": 0.0024,
      "step": 129160
    },
    {
      "epoch": 6.8890666666666664,
      "grad_norm": 0.1831786334514618,
      "learning_rate": 6.9433333333333335e-06,
      "loss": 0.0021,
      "step": 129170
    },
    {
      "epoch": 6.8896,
      "grad_norm": 0.07313352078199387,
      "learning_rate": 6.9400000000000005e-06,
      "loss": 0.0021,
      "step": 129180
    },
    {
      "epoch": 6.890133333333333,
      "grad_norm": 0.23544390499591827,
      "learning_rate": 6.936666666666667e-06,
      "loss": 0.0016,
      "step": 129190
    },
    {
      "epoch": 6.890666666666666,
      "grad_norm": 0.09405701607465744,
      "learning_rate": 6.933333333333334e-06,
      "loss": 0.0012,
      "step": 129200
    },
    {
      "epoch": 6.8911999999999995,
      "grad_norm": 0.23214587569236755,
      "learning_rate": 6.9300000000000006e-06,
      "loss": 0.0015,
      "step": 129210
    },
    {
      "epoch": 6.891733333333334,
      "grad_norm": 0.09245453029870987,
      "learning_rate": 6.9266666666666675e-06,
      "loss": 0.0016,
      "step": 129220
    },
    {
      "epoch": 6.892266666666667,
      "grad_norm": 0.24334126710891724,
      "learning_rate": 6.923333333333333e-06,
      "loss": 0.0018,
      "step": 129230
    },
    {
      "epoch": 6.8928,
      "grad_norm": 0.09036479890346527,
      "learning_rate": 6.92e-06,
      "loss": 0.003,
      "step": 129240
    },
    {
      "epoch": 6.8933333333333335,
      "grad_norm": 0.18264442682266235,
      "learning_rate": 6.916666666666667e-06,
      "loss": 0.002,
      "step": 129250
    },
    {
      "epoch": 6.893866666666667,
      "grad_norm": 0.19629895687103271,
      "learning_rate": 6.913333333333334e-06,
      "loss": 0.0014,
      "step": 129260
    },
    {
      "epoch": 6.8944,
      "grad_norm": 0.12901277840137482,
      "learning_rate": 6.91e-06,
      "loss": 0.0016,
      "step": 129270
    },
    {
      "epoch": 6.894933333333333,
      "grad_norm": 0.29671022295951843,
      "learning_rate": 6.906666666666667e-06,
      "loss": 0.0024,
      "step": 129280
    },
    {
      "epoch": 6.895466666666667,
      "grad_norm": 0.3010287582874298,
      "learning_rate": 6.903333333333334e-06,
      "loss": 0.0016,
      "step": 129290
    },
    {
      "epoch": 6.896,
      "grad_norm": 0.3583032488822937,
      "learning_rate": 6.900000000000001e-06,
      "loss": 0.0017,
      "step": 129300
    },
    {
      "epoch": 6.896533333333333,
      "grad_norm": 0.06733859330415726,
      "learning_rate": 6.896666666666666e-06,
      "loss": 0.0017,
      "step": 129310
    },
    {
      "epoch": 6.8970666666666665,
      "grad_norm": 0.3565923571586609,
      "learning_rate": 6.893333333333334e-06,
      "loss": 0.0018,
      "step": 129320
    },
    {
      "epoch": 6.8976,
      "grad_norm": 0.26540181040763855,
      "learning_rate": 6.890000000000001e-06,
      "loss": 0.0015,
      "step": 129330
    },
    {
      "epoch": 6.898133333333333,
      "grad_norm": 0.38362735509872437,
      "learning_rate": 6.886666666666668e-06,
      "loss": 0.0016,
      "step": 129340
    },
    {
      "epoch": 6.898666666666666,
      "grad_norm": 0.1660265475511551,
      "learning_rate": 6.883333333333333e-06,
      "loss": 0.0022,
      "step": 129350
    },
    {
      "epoch": 6.8992,
      "grad_norm": 0.06760978698730469,
      "learning_rate": 6.88e-06,
      "loss": 0.0021,
      "step": 129360
    },
    {
      "epoch": 6.899733333333334,
      "grad_norm": 0.03324474021792412,
      "learning_rate": 6.876666666666667e-06,
      "loss": 0.0016,
      "step": 129370
    },
    {
      "epoch": 6.900266666666667,
      "grad_norm": 0.0701812282204628,
      "learning_rate": 6.873333333333333e-06,
      "loss": 0.0018,
      "step": 129380
    },
    {
      "epoch": 6.9008,
      "grad_norm": 0.08999510854482651,
      "learning_rate": 6.87e-06,
      "loss": 0.002,
      "step": 129390
    },
    {
      "epoch": 6.9013333333333335,
      "grad_norm": 0.37039458751678467,
      "learning_rate": 6.866666666666667e-06,
      "loss": 0.0026,
      "step": 129400
    },
    {
      "epoch": 6.901866666666667,
      "grad_norm": 0.11800523847341537,
      "learning_rate": 6.863333333333334e-06,
      "loss": 0.0019,
      "step": 129410
    },
    {
      "epoch": 6.9024,
      "grad_norm": 0.14608685672283173,
      "learning_rate": 6.8599999999999995e-06,
      "loss": 0.0015,
      "step": 129420
    },
    {
      "epoch": 6.902933333333333,
      "grad_norm": 0.20715148746967316,
      "learning_rate": 6.8566666666666665e-06,
      "loss": 0.0012,
      "step": 129430
    },
    {
      "epoch": 6.903466666666667,
      "grad_norm": 0.5903745293617249,
      "learning_rate": 6.8533333333333335e-06,
      "loss": 0.0014,
      "step": 129440
    },
    {
      "epoch": 6.904,
      "grad_norm": 0.06403538584709167,
      "learning_rate": 6.8500000000000005e-06,
      "loss": 0.0013,
      "step": 129450
    },
    {
      "epoch": 6.904533333333333,
      "grad_norm": 0.039031002670526505,
      "learning_rate": 6.846666666666667e-06,
      "loss": 0.0015,
      "step": 129460
    },
    {
      "epoch": 6.9050666666666665,
      "grad_norm": 0.1525551974773407,
      "learning_rate": 6.843333333333334e-06,
      "loss": 0.0011,
      "step": 129470
    },
    {
      "epoch": 6.9056,
      "grad_norm": 0.08709994703531265,
      "learning_rate": 6.840000000000001e-06,
      "loss": 0.0013,
      "step": 129480
    },
    {
      "epoch": 6.906133333333333,
      "grad_norm": 0.6004329919815063,
      "learning_rate": 6.8366666666666676e-06,
      "loss": 0.0017,
      "step": 129490
    },
    {
      "epoch": 6.906666666666666,
      "grad_norm": 0.2154545783996582,
      "learning_rate": 6.833333333333333e-06,
      "loss": 0.0019,
      "step": 129500
    },
    {
      "epoch": 6.9072,
      "grad_norm": 0.29179614782333374,
      "learning_rate": 6.830000000000001e-06,
      "loss": 0.0028,
      "step": 129510
    },
    {
      "epoch": 6.907733333333333,
      "grad_norm": 0.17560604214668274,
      "learning_rate": 6.826666666666668e-06,
      "loss": 0.0013,
      "step": 129520
    },
    {
      "epoch": 6.908266666666667,
      "grad_norm": 0.05909350886940956,
      "learning_rate": 6.823333333333333e-06,
      "loss": 0.0021,
      "step": 129530
    },
    {
      "epoch": 6.9088,
      "grad_norm": 0.3959811329841614,
      "learning_rate": 6.82e-06,
      "loss": 0.0014,
      "step": 129540
    },
    {
      "epoch": 6.9093333333333335,
      "grad_norm": 0.26834139227867126,
      "learning_rate": 6.816666666666667e-06,
      "loss": 0.0017,
      "step": 129550
    },
    {
      "epoch": 6.909866666666667,
      "grad_norm": 0.07022754102945328,
      "learning_rate": 6.813333333333334e-06,
      "loss": 0.0016,
      "step": 129560
    },
    {
      "epoch": 6.9104,
      "grad_norm": 0.06265415251255035,
      "learning_rate": 6.81e-06,
      "loss": 0.0014,
      "step": 129570
    },
    {
      "epoch": 6.910933333333333,
      "grad_norm": 0.07974404096603394,
      "learning_rate": 6.806666666666667e-06,
      "loss": 0.0019,
      "step": 129580
    },
    {
      "epoch": 6.911466666666667,
      "grad_norm": 0.3114526569843292,
      "learning_rate": 6.803333333333334e-06,
      "loss": 0.0012,
      "step": 129590
    },
    {
      "epoch": 6.912,
      "grad_norm": 0.2202185094356537,
      "learning_rate": 6.800000000000001e-06,
      "loss": 0.0018,
      "step": 129600
    },
    {
      "epoch": 6.912533333333333,
      "grad_norm": 0.20686164498329163,
      "learning_rate": 6.796666666666666e-06,
      "loss": 0.0023,
      "step": 129610
    },
    {
      "epoch": 6.9130666666666665,
      "grad_norm": 0.2937186360359192,
      "learning_rate": 6.793333333333333e-06,
      "loss": 0.002,
      "step": 129620
    },
    {
      "epoch": 6.9136,
      "grad_norm": 0.36677417159080505,
      "learning_rate": 6.79e-06,
      "loss": 0.0012,
      "step": 129630
    },
    {
      "epoch": 6.914133333333333,
      "grad_norm": 0.2938331961631775,
      "learning_rate": 6.786666666666667e-06,
      "loss": 0.0014,
      "step": 129640
    },
    {
      "epoch": 6.914666666666666,
      "grad_norm": 0.13811004161834717,
      "learning_rate": 6.783333333333333e-06,
      "loss": 0.0015,
      "step": 129650
    },
    {
      "epoch": 6.9152000000000005,
      "grad_norm": 0.06402356177568436,
      "learning_rate": 6.78e-06,
      "loss": 0.0018,
      "step": 129660
    },
    {
      "epoch": 6.915733333333334,
      "grad_norm": 0.4673953950405121,
      "learning_rate": 6.776666666666667e-06,
      "loss": 0.002,
      "step": 129670
    },
    {
      "epoch": 6.916266666666667,
      "grad_norm": 0.12515991926193237,
      "learning_rate": 6.773333333333334e-06,
      "loss": 0.0025,
      "step": 129680
    },
    {
      "epoch": 6.9168,
      "grad_norm": 0.5732965469360352,
      "learning_rate": 6.7699999999999996e-06,
      "loss": 0.0017,
      "step": 129690
    },
    {
      "epoch": 6.917333333333334,
      "grad_norm": 0.21013468503952026,
      "learning_rate": 6.766666666666667e-06,
      "loss": 0.0015,
      "step": 129700
    },
    {
      "epoch": 6.917866666666667,
      "grad_norm": 0.20454028248786926,
      "learning_rate": 6.763333333333334e-06,
      "loss": 0.0015,
      "step": 129710
    },
    {
      "epoch": 6.9184,
      "grad_norm": 0.21049126982688904,
      "learning_rate": 6.76e-06,
      "loss": 0.0023,
      "step": 129720
    },
    {
      "epoch": 6.918933333333333,
      "grad_norm": 0.0956583246588707,
      "learning_rate": 6.756666666666667e-06,
      "loss": 0.0017,
      "step": 129730
    },
    {
      "epoch": 6.919466666666667,
      "grad_norm": 0.058667153120040894,
      "learning_rate": 6.753333333333334e-06,
      "loss": 0.0016,
      "step": 129740
    },
    {
      "epoch": 6.92,
      "grad_norm": 0.0968843400478363,
      "learning_rate": 6.750000000000001e-06,
      "loss": 0.0016,
      "step": 129750
    },
    {
      "epoch": 6.920533333333333,
      "grad_norm": 0.049853045493364334,
      "learning_rate": 6.746666666666667e-06,
      "loss": 0.0023,
      "step": 129760
    },
    {
      "epoch": 6.9210666666666665,
      "grad_norm": 0.13140098750591278,
      "learning_rate": 6.743333333333334e-06,
      "loss": 0.0027,
      "step": 129770
    },
    {
      "epoch": 6.9216,
      "grad_norm": 0.19304943084716797,
      "learning_rate": 6.740000000000001e-06,
      "loss": 0.0017,
      "step": 129780
    },
    {
      "epoch": 6.922133333333333,
      "grad_norm": 0.09252546727657318,
      "learning_rate": 6.736666666666668e-06,
      "loss": 0.0018,
      "step": 129790
    },
    {
      "epoch": 6.922666666666666,
      "grad_norm": 0.2278708517551422,
      "learning_rate": 6.733333333333333e-06,
      "loss": 0.0014,
      "step": 129800
    },
    {
      "epoch": 6.9232,
      "grad_norm": 0.18022356927394867,
      "learning_rate": 6.73e-06,
      "loss": 0.0016,
      "step": 129810
    },
    {
      "epoch": 6.923733333333333,
      "grad_norm": 0.20750385522842407,
      "learning_rate": 6.726666666666667e-06,
      "loss": 0.0028,
      "step": 129820
    },
    {
      "epoch": 6.924266666666667,
      "grad_norm": 0.23724812269210815,
      "learning_rate": 6.723333333333334e-06,
      "loss": 0.0021,
      "step": 129830
    },
    {
      "epoch": 6.9248,
      "grad_norm": 0.33090123534202576,
      "learning_rate": 6.72e-06,
      "loss": 0.0018,
      "step": 129840
    },
    {
      "epoch": 6.925333333333334,
      "grad_norm": 0.2778758406639099,
      "learning_rate": 6.716666666666667e-06,
      "loss": 0.0026,
      "step": 129850
    },
    {
      "epoch": 6.925866666666667,
      "grad_norm": 0.2948439419269562,
      "learning_rate": 6.713333333333334e-06,
      "loss": 0.0012,
      "step": 129860
    },
    {
      "epoch": 6.9264,
      "grad_norm": 0.18497702479362488,
      "learning_rate": 6.710000000000001e-06,
      "loss": 0.0021,
      "step": 129870
    },
    {
      "epoch": 6.926933333333333,
      "grad_norm": 0.12586699426174164,
      "learning_rate": 6.706666666666666e-06,
      "loss": 0.002,
      "step": 129880
    },
    {
      "epoch": 6.927466666666667,
      "grad_norm": 0.09490279853343964,
      "learning_rate": 6.703333333333334e-06,
      "loss": 0.0027,
      "step": 129890
    },
    {
      "epoch": 6.928,
      "grad_norm": 0.03589419275522232,
      "learning_rate": 6.700000000000001e-06,
      "loss": 0.0018,
      "step": 129900
    },
    {
      "epoch": 6.928533333333333,
      "grad_norm": 0.054575949907302856,
      "learning_rate": 6.696666666666666e-06,
      "loss": 0.0018,
      "step": 129910
    },
    {
      "epoch": 6.9290666666666665,
      "grad_norm": 0.2943454086780548,
      "learning_rate": 6.693333333333333e-06,
      "loss": 0.0016,
      "step": 129920
    },
    {
      "epoch": 6.9296,
      "grad_norm": 0.22467435896396637,
      "learning_rate": 6.69e-06,
      "loss": 0.0015,
      "step": 129930
    },
    {
      "epoch": 6.930133333333333,
      "grad_norm": 0.0756000205874443,
      "learning_rate": 6.686666666666667e-06,
      "loss": 0.0012,
      "step": 129940
    },
    {
      "epoch": 6.930666666666666,
      "grad_norm": 0.1534341424703598,
      "learning_rate": 6.6833333333333334e-06,
      "loss": 0.0012,
      "step": 129950
    },
    {
      "epoch": 6.9312000000000005,
      "grad_norm": 0.24228350818157196,
      "learning_rate": 6.68e-06,
      "loss": 0.0022,
      "step": 129960
    },
    {
      "epoch": 6.931733333333334,
      "grad_norm": 0.06111060082912445,
      "learning_rate": 6.676666666666667e-06,
      "loss": 0.0016,
      "step": 129970
    },
    {
      "epoch": 6.932266666666667,
      "grad_norm": 0.2413094937801361,
      "learning_rate": 6.673333333333334e-06,
      "loss": 0.0027,
      "step": 129980
    },
    {
      "epoch": 6.9328,
      "grad_norm": 0.2441130131483078,
      "learning_rate": 6.67e-06,
      "loss": 0.0019,
      "step": 129990
    },
    {
      "epoch": 6.933333333333334,
      "grad_norm": 0.28222671151161194,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.0015,
      "step": 130000
    },
    {
      "epoch": 6.933866666666667,
      "grad_norm": 0.45829540491104126,
      "learning_rate": 6.663333333333334e-06,
      "loss": 0.0029,
      "step": 130010
    },
    {
      "epoch": 6.9344,
      "grad_norm": 0.1783602088689804,
      "learning_rate": 6.660000000000001e-06,
      "loss": 0.0014,
      "step": 130020
    },
    {
      "epoch": 6.934933333333333,
      "grad_norm": 0.050441257655620575,
      "learning_rate": 6.656666666666667e-06,
      "loss": 0.0014,
      "step": 130030
    },
    {
      "epoch": 6.935466666666667,
      "grad_norm": 0.10857376456260681,
      "learning_rate": 6.653333333333334e-06,
      "loss": 0.0014,
      "step": 130040
    },
    {
      "epoch": 6.936,
      "grad_norm": 0.27140599489212036,
      "learning_rate": 6.650000000000001e-06,
      "loss": 0.0013,
      "step": 130050
    },
    {
      "epoch": 6.936533333333333,
      "grad_norm": 0.2415202260017395,
      "learning_rate": 6.646666666666666e-06,
      "loss": 0.0019,
      "step": 130060
    },
    {
      "epoch": 6.9370666666666665,
      "grad_norm": 0.44417402148246765,
      "learning_rate": 6.643333333333333e-06,
      "loss": 0.0022,
      "step": 130070
    },
    {
      "epoch": 6.9376,
      "grad_norm": 0.09515545517206192,
      "learning_rate": 6.640000000000001e-06,
      "loss": 0.0016,
      "step": 130080
    },
    {
      "epoch": 6.938133333333333,
      "grad_norm": 0.06342986971139908,
      "learning_rate": 6.636666666666668e-06,
      "loss": 0.0013,
      "step": 130090
    },
    {
      "epoch": 6.938666666666666,
      "grad_norm": 0.1309284120798111,
      "learning_rate": 6.633333333333333e-06,
      "loss": 0.0015,
      "step": 130100
    },
    {
      "epoch": 6.9392,
      "grad_norm": 0.0952795073390007,
      "learning_rate": 6.63e-06,
      "loss": 0.002,
      "step": 130110
    },
    {
      "epoch": 6.939733333333333,
      "grad_norm": 0.33632782101631165,
      "learning_rate": 6.626666666666667e-06,
      "loss": 0.0015,
      "step": 130120
    },
    {
      "epoch": 6.940266666666667,
      "grad_norm": 0.407848596572876,
      "learning_rate": 6.623333333333334e-06,
      "loss": 0.0022,
      "step": 130130
    },
    {
      "epoch": 6.9408,
      "grad_norm": 0.41000258922576904,
      "learning_rate": 6.62e-06,
      "loss": 0.0014,
      "step": 130140
    },
    {
      "epoch": 6.941333333333334,
      "grad_norm": 0.236082524061203,
      "learning_rate": 6.616666666666667e-06,
      "loss": 0.0017,
      "step": 130150
    },
    {
      "epoch": 6.941866666666667,
      "grad_norm": 0.21394391357898712,
      "learning_rate": 6.613333333333334e-06,
      "loss": 0.0021,
      "step": 130160
    },
    {
      "epoch": 6.9424,
      "grad_norm": 0.06546418368816376,
      "learning_rate": 6.610000000000001e-06,
      "loss": 0.0018,
      "step": 130170
    },
    {
      "epoch": 6.942933333333333,
      "grad_norm": 0.10382858663797379,
      "learning_rate": 6.606666666666666e-06,
      "loss": 0.0024,
      "step": 130180
    },
    {
      "epoch": 6.943466666666667,
      "grad_norm": 0.08428292721509933,
      "learning_rate": 6.603333333333333e-06,
      "loss": 0.0016,
      "step": 130190
    },
    {
      "epoch": 6.944,
      "grad_norm": 0.2753985524177551,
      "learning_rate": 6.6e-06,
      "loss": 0.002,
      "step": 130200
    },
    {
      "epoch": 6.944533333333333,
      "grad_norm": 0.07947973161935806,
      "learning_rate": 6.596666666666667e-06,
      "loss": 0.0024,
      "step": 130210
    },
    {
      "epoch": 6.9450666666666665,
      "grad_norm": 0.05343850702047348,
      "learning_rate": 6.5933333333333335e-06,
      "loss": 0.0026,
      "step": 130220
    },
    {
      "epoch": 6.9456,
      "grad_norm": 0.1901794970035553,
      "learning_rate": 6.5900000000000004e-06,
      "loss": 0.0013,
      "step": 130230
    },
    {
      "epoch": 6.946133333333333,
      "grad_norm": 0.08883433789014816,
      "learning_rate": 6.586666666666667e-06,
      "loss": 0.0016,
      "step": 130240
    },
    {
      "epoch": 6.946666666666666,
      "grad_norm": 0.1765352487564087,
      "learning_rate": 6.583333333333333e-06,
      "loss": 0.0018,
      "step": 130250
    },
    {
      "epoch": 6.9472000000000005,
      "grad_norm": 0.0562080554664135,
      "learning_rate": 6.58e-06,
      "loss": 0.0014,
      "step": 130260
    },
    {
      "epoch": 6.947733333333334,
      "grad_norm": 0.22314758598804474,
      "learning_rate": 6.5766666666666675e-06,
      "loss": 0.0015,
      "step": 130270
    },
    {
      "epoch": 6.948266666666667,
      "grad_norm": 0.30868881940841675,
      "learning_rate": 6.5733333333333345e-06,
      "loss": 0.0017,
      "step": 130280
    },
    {
      "epoch": 6.9488,
      "grad_norm": 0.32187148928642273,
      "learning_rate": 6.57e-06,
      "loss": 0.0015,
      "step": 130290
    },
    {
      "epoch": 6.949333333333334,
      "grad_norm": 0.24956555664539337,
      "learning_rate": 6.566666666666667e-06,
      "loss": 0.0024,
      "step": 130300
    },
    {
      "epoch": 6.949866666666667,
      "grad_norm": 0.09944459050893784,
      "learning_rate": 6.563333333333334e-06,
      "loss": 0.0021,
      "step": 130310
    },
    {
      "epoch": 6.9504,
      "grad_norm": 0.04257296398282051,
      "learning_rate": 6.560000000000001e-06,
      "loss": 0.0022,
      "step": 130320
    },
    {
      "epoch": 6.950933333333333,
      "grad_norm": 0.1501888483762741,
      "learning_rate": 6.556666666666667e-06,
      "loss": 0.0018,
      "step": 130330
    },
    {
      "epoch": 6.951466666666667,
      "grad_norm": 0.4124990999698639,
      "learning_rate": 6.553333333333334e-06,
      "loss": 0.0018,
      "step": 130340
    },
    {
      "epoch": 6.952,
      "grad_norm": 0.18048129975795746,
      "learning_rate": 6.550000000000001e-06,
      "loss": 0.0024,
      "step": 130350
    },
    {
      "epoch": 6.952533333333333,
      "grad_norm": 0.18314576148986816,
      "learning_rate": 6.546666666666668e-06,
      "loss": 0.0013,
      "step": 130360
    },
    {
      "epoch": 6.9530666666666665,
      "grad_norm": 0.20691503584384918,
      "learning_rate": 6.543333333333333e-06,
      "loss": 0.0026,
      "step": 130370
    },
    {
      "epoch": 6.9536,
      "grad_norm": 0.07648642361164093,
      "learning_rate": 6.54e-06,
      "loss": 0.0011,
      "step": 130380
    },
    {
      "epoch": 6.954133333333333,
      "grad_norm": 0.09423229098320007,
      "learning_rate": 6.536666666666667e-06,
      "loss": 0.0028,
      "step": 130390
    },
    {
      "epoch": 6.954666666666666,
      "grad_norm": 0.12553922832012177,
      "learning_rate": 6.533333333333333e-06,
      "loss": 0.0013,
      "step": 130400
    },
    {
      "epoch": 6.9552,
      "grad_norm": 0.5622996687889099,
      "learning_rate": 6.53e-06,
      "loss": 0.0024,
      "step": 130410
    },
    {
      "epoch": 6.955733333333333,
      "grad_norm": 0.04081399738788605,
      "learning_rate": 6.526666666666667e-06,
      "loss": 0.0017,
      "step": 130420
    },
    {
      "epoch": 6.956266666666667,
      "grad_norm": 0.18256258964538574,
      "learning_rate": 6.523333333333334e-06,
      "loss": 0.0024,
      "step": 130430
    },
    {
      "epoch": 6.9568,
      "grad_norm": 0.3009380102157593,
      "learning_rate": 6.519999999999999e-06,
      "loss": 0.0022,
      "step": 130440
    },
    {
      "epoch": 6.957333333333334,
      "grad_norm": 0.15730683505535126,
      "learning_rate": 6.516666666666666e-06,
      "loss": 0.0014,
      "step": 130450
    },
    {
      "epoch": 6.957866666666667,
      "grad_norm": 0.39076176285743713,
      "learning_rate": 6.513333333333333e-06,
      "loss": 0.0021,
      "step": 130460
    },
    {
      "epoch": 6.9584,
      "grad_norm": 0.29652440547943115,
      "learning_rate": 6.510000000000001e-06,
      "loss": 0.0016,
      "step": 130470
    },
    {
      "epoch": 6.958933333333333,
      "grad_norm": 0.04095685854554176,
      "learning_rate": 6.5066666666666665e-06,
      "loss": 0.0022,
      "step": 130480
    },
    {
      "epoch": 6.959466666666667,
      "grad_norm": 0.2773071527481079,
      "learning_rate": 6.5033333333333335e-06,
      "loss": 0.002,
      "step": 130490
    },
    {
      "epoch": 6.96,
      "grad_norm": 0.09070070832967758,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 0.0023,
      "step": 130500
    },
    {
      "epoch": 6.960533333333333,
      "grad_norm": 0.029530104249715805,
      "learning_rate": 6.4966666666666674e-06,
      "loss": 0.0013,
      "step": 130510
    },
    {
      "epoch": 6.9610666666666665,
      "grad_norm": 0.13287313282489777,
      "learning_rate": 6.4933333333333336e-06,
      "loss": 0.0018,
      "step": 130520
    },
    {
      "epoch": 6.9616,
      "grad_norm": 0.08923612534999847,
      "learning_rate": 6.4900000000000005e-06,
      "loss": 0.002,
      "step": 130530
    },
    {
      "epoch": 6.962133333333333,
      "grad_norm": 0.2660672068595886,
      "learning_rate": 6.4866666666666675e-06,
      "loss": 0.0014,
      "step": 130540
    },
    {
      "epoch": 6.962666666666666,
      "grad_norm": 0.20883050560951233,
      "learning_rate": 6.4833333333333345e-06,
      "loss": 0.0016,
      "step": 130550
    },
    {
      "epoch": 6.9632,
      "grad_norm": 0.23914948105812073,
      "learning_rate": 6.48e-06,
      "loss": 0.0012,
      "step": 130560
    },
    {
      "epoch": 6.963733333333334,
      "grad_norm": 0.06991379708051682,
      "learning_rate": 6.476666666666667e-06,
      "loss": 0.0012,
      "step": 130570
    },
    {
      "epoch": 6.964266666666667,
      "grad_norm": 0.43963560461997986,
      "learning_rate": 6.473333333333334e-06,
      "loss": 0.0024,
      "step": 130580
    },
    {
      "epoch": 6.9648,
      "grad_norm": 0.11769998073577881,
      "learning_rate": 6.47e-06,
      "loss": 0.0015,
      "step": 130590
    },
    {
      "epoch": 6.965333333333334,
      "grad_norm": 0.09357785433530807,
      "learning_rate": 6.466666666666667e-06,
      "loss": 0.002,
      "step": 130600
    },
    {
      "epoch": 6.965866666666667,
      "grad_norm": 0.24359148740768433,
      "learning_rate": 6.463333333333334e-06,
      "loss": 0.0023,
      "step": 130610
    },
    {
      "epoch": 6.9664,
      "grad_norm": 0.3042260706424713,
      "learning_rate": 6.460000000000001e-06,
      "loss": 0.0023,
      "step": 130620
    },
    {
      "epoch": 6.966933333333333,
      "grad_norm": 0.0743032693862915,
      "learning_rate": 6.456666666666666e-06,
      "loss": 0.002,
      "step": 130630
    },
    {
      "epoch": 6.967466666666667,
      "grad_norm": 0.17748886346817017,
      "learning_rate": 6.453333333333333e-06,
      "loss": 0.0013,
      "step": 130640
    },
    {
      "epoch": 6.968,
      "grad_norm": 0.061735156923532486,
      "learning_rate": 6.45e-06,
      "loss": 0.0018,
      "step": 130650
    },
    {
      "epoch": 6.968533333333333,
      "grad_norm": 0.30121907591819763,
      "learning_rate": 6.446666666666668e-06,
      "loss": 0.0024,
      "step": 130660
    },
    {
      "epoch": 6.9690666666666665,
      "grad_norm": 0.06191360577940941,
      "learning_rate": 6.443333333333333e-06,
      "loss": 0.0013,
      "step": 130670
    },
    {
      "epoch": 6.9696,
      "grad_norm": 0.06473515927791595,
      "learning_rate": 6.44e-06,
      "loss": 0.0012,
      "step": 130680
    },
    {
      "epoch": 6.970133333333333,
      "grad_norm": 0.0653499886393547,
      "learning_rate": 6.436666666666667e-06,
      "loss": 0.0015,
      "step": 130690
    },
    {
      "epoch": 6.970666666666666,
      "grad_norm": 0.047479040920734406,
      "learning_rate": 6.433333333333334e-06,
      "loss": 0.0019,
      "step": 130700
    },
    {
      "epoch": 6.9712,
      "grad_norm": 0.2976524233818054,
      "learning_rate": 6.43e-06,
      "loss": 0.0022,
      "step": 130710
    },
    {
      "epoch": 6.971733333333333,
      "grad_norm": 0.5625807642936707,
      "learning_rate": 6.426666666666667e-06,
      "loss": 0.0012,
      "step": 130720
    },
    {
      "epoch": 6.972266666666666,
      "grad_norm": 0.298294335603714,
      "learning_rate": 6.423333333333334e-06,
      "loss": 0.001,
      "step": 130730
    },
    {
      "epoch": 6.9728,
      "grad_norm": 0.04772220924496651,
      "learning_rate": 6.4199999999999995e-06,
      "loss": 0.0018,
      "step": 130740
    },
    {
      "epoch": 6.973333333333334,
      "grad_norm": 0.06783489882946014,
      "learning_rate": 6.4166666666666665e-06,
      "loss": 0.0021,
      "step": 130750
    },
    {
      "epoch": 6.973866666666667,
      "grad_norm": 0.45563963055610657,
      "learning_rate": 6.4133333333333335e-06,
      "loss": 0.0016,
      "step": 130760
    },
    {
      "epoch": 6.9744,
      "grad_norm": 0.04964076355099678,
      "learning_rate": 6.4100000000000005e-06,
      "loss": 0.0013,
      "step": 130770
    },
    {
      "epoch": 6.974933333333333,
      "grad_norm": 0.07223666459321976,
      "learning_rate": 6.406666666666667e-06,
      "loss": 0.0013,
      "step": 130780
    },
    {
      "epoch": 6.975466666666667,
      "grad_norm": 0.18370293080806732,
      "learning_rate": 6.403333333333334e-06,
      "loss": 0.0018,
      "step": 130790
    },
    {
      "epoch": 6.976,
      "grad_norm": 0.1487111896276474,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 0.0017,
      "step": 130800
    },
    {
      "epoch": 6.976533333333333,
      "grad_norm": 0.17541033029556274,
      "learning_rate": 6.3966666666666675e-06,
      "loss": 0.0022,
      "step": 130810
    },
    {
      "epoch": 6.9770666666666665,
      "grad_norm": 0.1755581498146057,
      "learning_rate": 6.393333333333333e-06,
      "loss": 0.0015,
      "step": 130820
    },
    {
      "epoch": 6.9776,
      "grad_norm": 0.18292920291423798,
      "learning_rate": 6.39e-06,
      "loss": 0.0015,
      "step": 130830
    },
    {
      "epoch": 6.978133333333333,
      "grad_norm": 0.2052394300699234,
      "learning_rate": 6.386666666666667e-06,
      "loss": 0.0017,
      "step": 130840
    },
    {
      "epoch": 6.978666666666666,
      "grad_norm": 0.20879331231117249,
      "learning_rate": 6.383333333333335e-06,
      "loss": 0.0023,
      "step": 130850
    },
    {
      "epoch": 6.9792,
      "grad_norm": 0.07119633257389069,
      "learning_rate": 6.38e-06,
      "loss": 0.0014,
      "step": 130860
    },
    {
      "epoch": 6.979733333333334,
      "grad_norm": 0.09323574602603912,
      "learning_rate": 6.376666666666667e-06,
      "loss": 0.0013,
      "step": 130870
    },
    {
      "epoch": 6.980266666666667,
      "grad_norm": 0.28749823570251465,
      "learning_rate": 6.373333333333334e-06,
      "loss": 0.0014,
      "step": 130880
    },
    {
      "epoch": 6.9808,
      "grad_norm": 0.20531252026557922,
      "learning_rate": 6.370000000000001e-06,
      "loss": 0.0012,
      "step": 130890
    },
    {
      "epoch": 6.981333333333334,
      "grad_norm": 0.18035836517810822,
      "learning_rate": 6.366666666666667e-06,
      "loss": 0.0016,
      "step": 130900
    },
    {
      "epoch": 6.981866666666667,
      "grad_norm": 0.3491499423980713,
      "learning_rate": 6.363333333333334e-06,
      "loss": 0.0014,
      "step": 130910
    },
    {
      "epoch": 6.9824,
      "grad_norm": 0.2640390694141388,
      "learning_rate": 6.360000000000001e-06,
      "loss": 0.0016,
      "step": 130920
    },
    {
      "epoch": 6.982933333333333,
      "grad_norm": 0.1795227974653244,
      "learning_rate": 6.356666666666666e-06,
      "loss": 0.0011,
      "step": 130930
    },
    {
      "epoch": 6.983466666666667,
      "grad_norm": 0.07843319326639175,
      "learning_rate": 6.353333333333333e-06,
      "loss": 0.0021,
      "step": 130940
    },
    {
      "epoch": 6.984,
      "grad_norm": 0.2955707609653473,
      "learning_rate": 6.35e-06,
      "loss": 0.0014,
      "step": 130950
    },
    {
      "epoch": 6.984533333333333,
      "grad_norm": 0.11901199072599411,
      "learning_rate": 6.346666666666667e-06,
      "loss": 0.0012,
      "step": 130960
    },
    {
      "epoch": 6.9850666666666665,
      "grad_norm": 0.3933403193950653,
      "learning_rate": 6.343333333333333e-06,
      "loss": 0.0024,
      "step": 130970
    },
    {
      "epoch": 6.9856,
      "grad_norm": 0.3254408836364746,
      "learning_rate": 6.34e-06,
      "loss": 0.0023,
      "step": 130980
    },
    {
      "epoch": 6.986133333333333,
      "grad_norm": 0.09997240453958511,
      "learning_rate": 6.336666666666667e-06,
      "loss": 0.0015,
      "step": 130990
    },
    {
      "epoch": 6.986666666666666,
      "grad_norm": 0.20999592542648315,
      "learning_rate": 6.333333333333334e-06,
      "loss": 0.0017,
      "step": 131000
    },
    {
      "epoch": 6.9872,
      "grad_norm": 0.431878924369812,
      "learning_rate": 6.3299999999999995e-06,
      "loss": 0.0019,
      "step": 131010
    },
    {
      "epoch": 6.987733333333333,
      "grad_norm": 0.1254638284444809,
      "learning_rate": 6.3266666666666665e-06,
      "loss": 0.002,
      "step": 131020
    },
    {
      "epoch": 6.988266666666666,
      "grad_norm": 0.12575004994869232,
      "learning_rate": 6.3233333333333335e-06,
      "loss": 0.0015,
      "step": 131030
    },
    {
      "epoch": 6.9888,
      "grad_norm": 0.3765493929386139,
      "learning_rate": 6.320000000000001e-06,
      "loss": 0.0019,
      "step": 131040
    },
    {
      "epoch": 6.989333333333334,
      "grad_norm": 0.4000896215438843,
      "learning_rate": 6.316666666666667e-06,
      "loss": 0.0022,
      "step": 131050
    },
    {
      "epoch": 6.989866666666667,
      "grad_norm": 0.44146138429641724,
      "learning_rate": 6.313333333333334e-06,
      "loss": 0.0014,
      "step": 131060
    },
    {
      "epoch": 6.9904,
      "grad_norm": 0.4451608955860138,
      "learning_rate": 6.3100000000000006e-06,
      "loss": 0.0016,
      "step": 131070
    },
    {
      "epoch": 6.990933333333333,
      "grad_norm": 0.31934767961502075,
      "learning_rate": 6.306666666666666e-06,
      "loss": 0.0021,
      "step": 131080
    },
    {
      "epoch": 6.991466666666667,
      "grad_norm": 0.09256032109260559,
      "learning_rate": 6.303333333333334e-06,
      "loss": 0.0012,
      "step": 131090
    },
    {
      "epoch": 6.992,
      "grad_norm": 0.06566731631755829,
      "learning_rate": 6.300000000000001e-06,
      "loss": 0.0025,
      "step": 131100
    },
    {
      "epoch": 6.992533333333333,
      "grad_norm": 0.12301812320947647,
      "learning_rate": 6.296666666666668e-06,
      "loss": 0.0019,
      "step": 131110
    },
    {
      "epoch": 6.9930666666666665,
      "grad_norm": 0.07510717958211899,
      "learning_rate": 6.293333333333333e-06,
      "loss": 0.0013,
      "step": 131120
    },
    {
      "epoch": 6.9936,
      "grad_norm": 0.06548718363046646,
      "learning_rate": 6.29e-06,
      "loss": 0.0015,
      "step": 131130
    },
    {
      "epoch": 6.994133333333333,
      "grad_norm": 0.09509779512882233,
      "learning_rate": 6.286666666666667e-06,
      "loss": 0.0013,
      "step": 131140
    },
    {
      "epoch": 6.994666666666666,
      "grad_norm": 0.3087198734283447,
      "learning_rate": 6.283333333333334e-06,
      "loss": 0.0016,
      "step": 131150
    },
    {
      "epoch": 6.9952,
      "grad_norm": 0.07396116852760315,
      "learning_rate": 6.28e-06,
      "loss": 0.0015,
      "step": 131160
    },
    {
      "epoch": 6.995733333333334,
      "grad_norm": 0.16953803598880768,
      "learning_rate": 6.276666666666667e-06,
      "loss": 0.0026,
      "step": 131170
    },
    {
      "epoch": 6.996266666666667,
      "grad_norm": 0.5803447365760803,
      "learning_rate": 6.273333333333334e-06,
      "loss": 0.0025,
      "step": 131180
    },
    {
      "epoch": 6.9968,
      "grad_norm": 0.19566553831100464,
      "learning_rate": 6.270000000000001e-06,
      "loss": 0.0023,
      "step": 131190
    },
    {
      "epoch": 6.997333333333334,
      "grad_norm": 0.03093365579843521,
      "learning_rate": 6.266666666666666e-06,
      "loss": 0.0013,
      "step": 131200
    },
    {
      "epoch": 6.997866666666667,
      "grad_norm": 0.18056480586528778,
      "learning_rate": 6.263333333333333e-06,
      "loss": 0.002,
      "step": 131210
    },
    {
      "epoch": 6.9984,
      "grad_norm": 0.3054444193840027,
      "learning_rate": 6.26e-06,
      "loss": 0.0015,
      "step": 131220
    },
    {
      "epoch": 6.9989333333333335,
      "grad_norm": 0.06485389173030853,
      "learning_rate": 6.256666666666668e-06,
      "loss": 0.0032,
      "step": 131230
    },
    {
      "epoch": 6.999466666666667,
      "grad_norm": 0.0613456591963768,
      "learning_rate": 6.253333333333333e-06,
      "loss": 0.0021,
      "step": 131240
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.0695352777838707,
      "learning_rate": 6.25e-06,
      "loss": 0.0024,
      "step": 131250
    },
    {
      "epoch": 7.0,
      "eval_loss": 0.0018222325015813112,
      "eval_runtime": 162.225,
      "eval_samples_per_second": 1541.07,
      "eval_steps_per_second": 38.527,
      "step": 131250
    },
    {
      "epoch": 7.000533333333333,
      "grad_norm": 0.1834699809551239,
      "learning_rate": 6.2466666666666664e-06,
      "loss": 0.0014,
      "step": 131260
    },
    {
      "epoch": 7.0010666666666665,
      "grad_norm": 0.24907299876213074,
      "learning_rate": 6.243333333333333e-06,
      "loss": 0.003,
      "step": 131270
    },
    {
      "epoch": 7.0016,
      "grad_norm": 0.2639768719673157,
      "learning_rate": 6.24e-06,
      "loss": 0.0015,
      "step": 131280
    },
    {
      "epoch": 7.002133333333333,
      "grad_norm": 0.17505909502506256,
      "learning_rate": 6.236666666666667e-06,
      "loss": 0.0014,
      "step": 131290
    },
    {
      "epoch": 7.002666666666666,
      "grad_norm": 0.13883180916309357,
      "learning_rate": 6.2333333333333335e-06,
      "loss": 0.0018,
      "step": 131300
    },
    {
      "epoch": 7.0032,
      "grad_norm": 0.1880282610654831,
      "learning_rate": 6.2300000000000005e-06,
      "loss": 0.0015,
      "step": 131310
    },
    {
      "epoch": 7.003733333333333,
      "grad_norm": 0.05728210136294365,
      "learning_rate": 6.226666666666667e-06,
      "loss": 0.0013,
      "step": 131320
    },
    {
      "epoch": 7.004266666666667,
      "grad_norm": 0.062385443598032,
      "learning_rate": 6.223333333333334e-06,
      "loss": 0.0018,
      "step": 131330
    },
    {
      "epoch": 7.0048,
      "grad_norm": 0.16337136924266815,
      "learning_rate": 6.22e-06,
      "loss": 0.0012,
      "step": 131340
    },
    {
      "epoch": 7.005333333333334,
      "grad_norm": 0.27364012598991394,
      "learning_rate": 6.2166666666666676e-06,
      "loss": 0.0021,
      "step": 131350
    },
    {
      "epoch": 7.005866666666667,
      "grad_norm": 0.35409101843833923,
      "learning_rate": 6.213333333333334e-06,
      "loss": 0.0017,
      "step": 131360
    },
    {
      "epoch": 7.0064,
      "grad_norm": 0.26295551657676697,
      "learning_rate": 6.210000000000001e-06,
      "loss": 0.003,
      "step": 131370
    },
    {
      "epoch": 7.0069333333333335,
      "grad_norm": 0.09195737540721893,
      "learning_rate": 6.206666666666667e-06,
      "loss": 0.0024,
      "step": 131380
    },
    {
      "epoch": 7.007466666666667,
      "grad_norm": 0.4155154824256897,
      "learning_rate": 6.203333333333334e-06,
      "loss": 0.0022,
      "step": 131390
    },
    {
      "epoch": 7.008,
      "grad_norm": 0.29264941811561584,
      "learning_rate": 6.2e-06,
      "loss": 0.0017,
      "step": 131400
    },
    {
      "epoch": 7.008533333333333,
      "grad_norm": 0.3020302951335907,
      "learning_rate": 6.196666666666667e-06,
      "loss": 0.0013,
      "step": 131410
    },
    {
      "epoch": 7.009066666666667,
      "grad_norm": 0.038084447383880615,
      "learning_rate": 6.193333333333334e-06,
      "loss": 0.0015,
      "step": 131420
    },
    {
      "epoch": 7.0096,
      "grad_norm": 0.0976315587759018,
      "learning_rate": 6.19e-06,
      "loss": 0.0014,
      "step": 131430
    },
    {
      "epoch": 7.010133333333333,
      "grad_norm": 0.3001352548599243,
      "learning_rate": 6.186666666666667e-06,
      "loss": 0.0014,
      "step": 131440
    },
    {
      "epoch": 7.010666666666666,
      "grad_norm": 0.06783271580934525,
      "learning_rate": 6.183333333333333e-06,
      "loss": 0.0018,
      "step": 131450
    },
    {
      "epoch": 7.0112,
      "grad_norm": 0.09268898516893387,
      "learning_rate": 6.18e-06,
      "loss": 0.0028,
      "step": 131460
    },
    {
      "epoch": 7.011733333333333,
      "grad_norm": 0.26891231536865234,
      "learning_rate": 6.176666666666667e-06,
      "loss": 0.0021,
      "step": 131470
    },
    {
      "epoch": 7.012266666666667,
      "grad_norm": 0.3987951874732971,
      "learning_rate": 6.173333333333334e-06,
      "loss": 0.0016,
      "step": 131480
    },
    {
      "epoch": 7.0128,
      "grad_norm": 0.29035741090774536,
      "learning_rate": 6.17e-06,
      "loss": 0.0016,
      "step": 131490
    },
    {
      "epoch": 7.013333333333334,
      "grad_norm": 0.15031304955482483,
      "learning_rate": 6.166666666666667e-06,
      "loss": 0.0018,
      "step": 131500
    },
    {
      "epoch": 7.013866666666667,
      "grad_norm": 0.23296217620372772,
      "learning_rate": 6.163333333333333e-06,
      "loss": 0.0018,
      "step": 131510
    },
    {
      "epoch": 7.0144,
      "grad_norm": 0.12041956931352615,
      "learning_rate": 6.16e-06,
      "loss": 0.0019,
      "step": 131520
    },
    {
      "epoch": 7.0149333333333335,
      "grad_norm": 0.24549905955791473,
      "learning_rate": 6.1566666666666664e-06,
      "loss": 0.002,
      "step": 131530
    },
    {
      "epoch": 7.015466666666667,
      "grad_norm": 0.043602023273706436,
      "learning_rate": 6.153333333333334e-06,
      "loss": 0.0015,
      "step": 131540
    },
    {
      "epoch": 7.016,
      "grad_norm": 0.1741829663515091,
      "learning_rate": 6.15e-06,
      "loss": 0.0017,
      "step": 131550
    },
    {
      "epoch": 7.016533333333333,
      "grad_norm": 0.047418009489774704,
      "learning_rate": 6.146666666666667e-06,
      "loss": 0.0019,
      "step": 131560
    },
    {
      "epoch": 7.017066666666667,
      "grad_norm": 0.29863303899765015,
      "learning_rate": 6.1433333333333335e-06,
      "loss": 0.0045,
      "step": 131570
    },
    {
      "epoch": 7.0176,
      "grad_norm": 0.05408143624663353,
      "learning_rate": 6.1400000000000005e-06,
      "loss": 0.0015,
      "step": 131580
    },
    {
      "epoch": 7.018133333333333,
      "grad_norm": 0.3070977032184601,
      "learning_rate": 6.136666666666667e-06,
      "loss": 0.0023,
      "step": 131590
    },
    {
      "epoch": 7.018666666666666,
      "grad_norm": 0.12943734228610992,
      "learning_rate": 6.133333333333334e-06,
      "loss": 0.0022,
      "step": 131600
    },
    {
      "epoch": 7.0192,
      "grad_norm": 0.20499595999717712,
      "learning_rate": 6.130000000000001e-06,
      "loss": 0.0013,
      "step": 131610
    },
    {
      "epoch": 7.019733333333333,
      "grad_norm": 0.18541628122329712,
      "learning_rate": 6.126666666666667e-06,
      "loss": 0.0026,
      "step": 131620
    },
    {
      "epoch": 7.020266666666667,
      "grad_norm": 0.1738937944173813,
      "learning_rate": 6.123333333333334e-06,
      "loss": 0.0021,
      "step": 131630
    },
    {
      "epoch": 7.0208,
      "grad_norm": 0.08129570633172989,
      "learning_rate": 6.12e-06,
      "loss": 0.0021,
      "step": 131640
    },
    {
      "epoch": 7.021333333333334,
      "grad_norm": 0.3396872878074646,
      "learning_rate": 6.116666666666667e-06,
      "loss": 0.0026,
      "step": 131650
    },
    {
      "epoch": 7.021866666666667,
      "grad_norm": 0.28255385160446167,
      "learning_rate": 6.113333333333334e-06,
      "loss": 0.0022,
      "step": 131660
    },
    {
      "epoch": 7.0224,
      "grad_norm": 0.05881589278578758,
      "learning_rate": 6.110000000000001e-06,
      "loss": 0.002,
      "step": 131670
    },
    {
      "epoch": 7.0229333333333335,
      "grad_norm": 0.3089195191860199,
      "learning_rate": 6.106666666666667e-06,
      "loss": 0.0024,
      "step": 131680
    },
    {
      "epoch": 7.023466666666667,
      "grad_norm": 0.4184836149215698,
      "learning_rate": 6.103333333333334e-06,
      "loss": 0.0031,
      "step": 131690
    },
    {
      "epoch": 7.024,
      "grad_norm": 0.09020247310400009,
      "learning_rate": 6.1e-06,
      "loss": 0.0024,
      "step": 131700
    },
    {
      "epoch": 7.024533333333333,
      "grad_norm": 0.12488973885774612,
      "learning_rate": 6.096666666666667e-06,
      "loss": 0.0015,
      "step": 131710
    },
    {
      "epoch": 7.025066666666667,
      "grad_norm": 0.12119149416685104,
      "learning_rate": 6.093333333333333e-06,
      "loss": 0.0015,
      "step": 131720
    },
    {
      "epoch": 7.0256,
      "grad_norm": 0.11941627413034439,
      "learning_rate": 6.090000000000001e-06,
      "loss": 0.0023,
      "step": 131730
    },
    {
      "epoch": 7.026133333333333,
      "grad_norm": 0.2336588203907013,
      "learning_rate": 6.086666666666667e-06,
      "loss": 0.0017,
      "step": 131740
    },
    {
      "epoch": 7.026666666666666,
      "grad_norm": 0.18402764201164246,
      "learning_rate": 6.083333333333334e-06,
      "loss": 0.0013,
      "step": 131750
    },
    {
      "epoch": 7.0272,
      "grad_norm": 0.23708757758140564,
      "learning_rate": 6.08e-06,
      "loss": 0.002,
      "step": 131760
    },
    {
      "epoch": 7.027733333333333,
      "grad_norm": 0.0447661466896534,
      "learning_rate": 6.076666666666666e-06,
      "loss": 0.0019,
      "step": 131770
    },
    {
      "epoch": 7.028266666666667,
      "grad_norm": 0.40868091583251953,
      "learning_rate": 6.073333333333333e-06,
      "loss": 0.0013,
      "step": 131780
    },
    {
      "epoch": 7.0288,
      "grad_norm": 0.05038219317793846,
      "learning_rate": 6.07e-06,
      "loss": 0.0016,
      "step": 131790
    },
    {
      "epoch": 7.029333333333334,
      "grad_norm": 0.1741480529308319,
      "learning_rate": 6.066666666666667e-06,
      "loss": 0.0017,
      "step": 131800
    },
    {
      "epoch": 7.029866666666667,
      "grad_norm": 0.23610077798366547,
      "learning_rate": 6.0633333333333334e-06,
      "loss": 0.0012,
      "step": 131810
    },
    {
      "epoch": 7.0304,
      "grad_norm": 0.26634126901626587,
      "learning_rate": 6.0600000000000004e-06,
      "loss": 0.0018,
      "step": 131820
    },
    {
      "epoch": 7.0309333333333335,
      "grad_norm": 0.2455114722251892,
      "learning_rate": 6.0566666666666666e-06,
      "loss": 0.0014,
      "step": 131830
    },
    {
      "epoch": 7.031466666666667,
      "grad_norm": 0.2271609604358673,
      "learning_rate": 6.0533333333333335e-06,
      "loss": 0.0019,
      "step": 131840
    },
    {
      "epoch": 7.032,
      "grad_norm": 0.152311772108078,
      "learning_rate": 6.0500000000000005e-06,
      "loss": 0.0017,
      "step": 131850
    },
    {
      "epoch": 7.032533333333333,
      "grad_norm": 0.04466011002659798,
      "learning_rate": 6.0466666666666675e-06,
      "loss": 0.0013,
      "step": 131860
    },
    {
      "epoch": 7.033066666666667,
      "grad_norm": 0.2938096523284912,
      "learning_rate": 6.043333333333334e-06,
      "loss": 0.002,
      "step": 131870
    },
    {
      "epoch": 7.0336,
      "grad_norm": 0.2651185691356659,
      "learning_rate": 6.040000000000001e-06,
      "loss": 0.0017,
      "step": 131880
    },
    {
      "epoch": 7.034133333333333,
      "grad_norm": 0.1515478938817978,
      "learning_rate": 6.036666666666667e-06,
      "loss": 0.0016,
      "step": 131890
    },
    {
      "epoch": 7.034666666666666,
      "grad_norm": 0.11336594820022583,
      "learning_rate": 6.033333333333334e-06,
      "loss": 0.0019,
      "step": 131900
    },
    {
      "epoch": 7.0352,
      "grad_norm": 0.2147808074951172,
      "learning_rate": 6.03e-06,
      "loss": 0.0014,
      "step": 131910
    },
    {
      "epoch": 7.035733333333333,
      "grad_norm": 0.03508210927248001,
      "learning_rate": 6.026666666666667e-06,
      "loss": 0.0021,
      "step": 131920
    },
    {
      "epoch": 7.036266666666666,
      "grad_norm": 0.03520619124174118,
      "learning_rate": 6.023333333333334e-06,
      "loss": 0.003,
      "step": 131930
    },
    {
      "epoch": 7.0368,
      "grad_norm": 0.09643799811601639,
      "learning_rate": 6.02e-06,
      "loss": 0.0022,
      "step": 131940
    },
    {
      "epoch": 7.037333333333334,
      "grad_norm": 0.30990058183670044,
      "learning_rate": 6.016666666666667e-06,
      "loss": 0.0017,
      "step": 131950
    },
    {
      "epoch": 7.037866666666667,
      "grad_norm": 0.06897524744272232,
      "learning_rate": 6.013333333333333e-06,
      "loss": 0.0014,
      "step": 131960
    },
    {
      "epoch": 7.0384,
      "grad_norm": 0.050625354051589966,
      "learning_rate": 6.01e-06,
      "loss": 0.0022,
      "step": 131970
    },
    {
      "epoch": 7.0389333333333335,
      "grad_norm": 0.07840694487094879,
      "learning_rate": 6.006666666666667e-06,
      "loss": 0.0022,
      "step": 131980
    },
    {
      "epoch": 7.039466666666667,
      "grad_norm": 0.4127279222011566,
      "learning_rate": 6.003333333333334e-06,
      "loss": 0.0014,
      "step": 131990
    },
    {
      "epoch": 7.04,
      "grad_norm": 0.09004464745521545,
      "learning_rate": 6e-06,
      "loss": 0.0016,
      "step": 132000
    },
    {
      "epoch": 7.040533333333333,
      "grad_norm": 0.3288283050060272,
      "learning_rate": 5.996666666666667e-06,
      "loss": 0.0018,
      "step": 132010
    },
    {
      "epoch": 7.041066666666667,
      "grad_norm": 0.12576858699321747,
      "learning_rate": 5.993333333333333e-06,
      "loss": 0.0013,
      "step": 132020
    },
    {
      "epoch": 7.0416,
      "grad_norm": 0.054279353469610214,
      "learning_rate": 5.99e-06,
      "loss": 0.0018,
      "step": 132030
    },
    {
      "epoch": 7.042133333333333,
      "grad_norm": 0.18151840567588806,
      "learning_rate": 5.986666666666667e-06,
      "loss": 0.0014,
      "step": 132040
    },
    {
      "epoch": 7.042666666666666,
      "grad_norm": 0.1530667543411255,
      "learning_rate": 5.983333333333334e-06,
      "loss": 0.0019,
      "step": 132050
    },
    {
      "epoch": 7.0432,
      "grad_norm": 0.07433290779590607,
      "learning_rate": 5.98e-06,
      "loss": 0.0021,
      "step": 132060
    },
    {
      "epoch": 7.043733333333333,
      "grad_norm": 0.23688596487045288,
      "learning_rate": 5.976666666666667e-06,
      "loss": 0.0022,
      "step": 132070
    },
    {
      "epoch": 7.044266666666666,
      "grad_norm": 0.3558729887008667,
      "learning_rate": 5.9733333333333335e-06,
      "loss": 0.0013,
      "step": 132080
    },
    {
      "epoch": 7.0448,
      "grad_norm": 0.35410985350608826,
      "learning_rate": 5.9700000000000004e-06,
      "loss": 0.0021,
      "step": 132090
    },
    {
      "epoch": 7.045333333333334,
      "grad_norm": 0.3492675721645355,
      "learning_rate": 5.9666666666666666e-06,
      "loss": 0.0017,
      "step": 132100
    },
    {
      "epoch": 7.045866666666667,
      "grad_norm": 0.26755228638648987,
      "learning_rate": 5.9633333333333336e-06,
      "loss": 0.0015,
      "step": 132110
    },
    {
      "epoch": 7.0464,
      "grad_norm": 0.15753796696662903,
      "learning_rate": 5.9600000000000005e-06,
      "loss": 0.0016,
      "step": 132120
    },
    {
      "epoch": 7.0469333333333335,
      "grad_norm": 0.09343873709440231,
      "learning_rate": 5.956666666666667e-06,
      "loss": 0.0012,
      "step": 132130
    },
    {
      "epoch": 7.047466666666667,
      "grad_norm": 0.09750832617282867,
      "learning_rate": 5.953333333333334e-06,
      "loss": 0.0013,
      "step": 132140
    },
    {
      "epoch": 7.048,
      "grad_norm": 0.23252932727336884,
      "learning_rate": 5.95e-06,
      "loss": 0.0027,
      "step": 132150
    },
    {
      "epoch": 7.048533333333333,
      "grad_norm": 0.1470489799976349,
      "learning_rate": 5.946666666666667e-06,
      "loss": 0.0017,
      "step": 132160
    },
    {
      "epoch": 7.049066666666667,
      "grad_norm": 0.11880817264318466,
      "learning_rate": 5.943333333333334e-06,
      "loss": 0.0014,
      "step": 132170
    },
    {
      "epoch": 7.0496,
      "grad_norm": 0.040198709815740585,
      "learning_rate": 5.940000000000001e-06,
      "loss": 0.0014,
      "step": 132180
    },
    {
      "epoch": 7.050133333333333,
      "grad_norm": 0.12160521745681763,
      "learning_rate": 5.936666666666667e-06,
      "loss": 0.0019,
      "step": 132190
    },
    {
      "epoch": 7.050666666666666,
      "grad_norm": 0.2085447609424591,
      "learning_rate": 5.933333333333334e-06,
      "loss": 0.0024,
      "step": 132200
    },
    {
      "epoch": 7.0512,
      "grad_norm": 0.29321596026420593,
      "learning_rate": 5.93e-06,
      "loss": 0.0014,
      "step": 132210
    },
    {
      "epoch": 7.051733333333333,
      "grad_norm": 0.2126898169517517,
      "learning_rate": 5.926666666666667e-06,
      "loss": 0.0021,
      "step": 132220
    },
    {
      "epoch": 7.052266666666666,
      "grad_norm": 0.17861591279506683,
      "learning_rate": 5.923333333333333e-06,
      "loss": 0.0014,
      "step": 132230
    },
    {
      "epoch": 7.0528,
      "grad_norm": 0.06440238654613495,
      "learning_rate": 5.920000000000001e-06,
      "loss": 0.0016,
      "step": 132240
    },
    {
      "epoch": 7.053333333333334,
      "grad_norm": 0.28006401658058167,
      "learning_rate": 5.916666666666667e-06,
      "loss": 0.0028,
      "step": 132250
    },
    {
      "epoch": 7.053866666666667,
      "grad_norm": 0.12800563871860504,
      "learning_rate": 5.913333333333334e-06,
      "loss": 0.0021,
      "step": 132260
    },
    {
      "epoch": 7.0544,
      "grad_norm": 0.3261324167251587,
      "learning_rate": 5.91e-06,
      "loss": 0.0016,
      "step": 132270
    },
    {
      "epoch": 7.0549333333333335,
      "grad_norm": 0.1539844125509262,
      "learning_rate": 5.906666666666667e-06,
      "loss": 0.0015,
      "step": 132280
    },
    {
      "epoch": 7.055466666666667,
      "grad_norm": 0.12846720218658447,
      "learning_rate": 5.903333333333333e-06,
      "loss": 0.0021,
      "step": 132290
    },
    {
      "epoch": 7.056,
      "grad_norm": 0.13856106996536255,
      "learning_rate": 5.9e-06,
      "loss": 0.0019,
      "step": 132300
    },
    {
      "epoch": 7.056533333333333,
      "grad_norm": 0.23959104716777802,
      "learning_rate": 5.896666666666667e-06,
      "loss": 0.0012,
      "step": 132310
    },
    {
      "epoch": 7.057066666666667,
      "grad_norm": 0.08924750983715057,
      "learning_rate": 5.893333333333333e-06,
      "loss": 0.0015,
      "step": 132320
    },
    {
      "epoch": 7.0576,
      "grad_norm": 0.34779882431030273,
      "learning_rate": 5.89e-06,
      "loss": 0.0023,
      "step": 132330
    },
    {
      "epoch": 7.058133333333333,
      "grad_norm": 0.28623533248901367,
      "learning_rate": 5.8866666666666665e-06,
      "loss": 0.0019,
      "step": 132340
    },
    {
      "epoch": 7.058666666666666,
      "grad_norm": 0.1319981813430786,
      "learning_rate": 5.8833333333333335e-06,
      "loss": 0.002,
      "step": 132350
    },
    {
      "epoch": 7.0592,
      "grad_norm": 0.10098240524530411,
      "learning_rate": 5.8800000000000005e-06,
      "loss": 0.0016,
      "step": 132360
    },
    {
      "epoch": 7.059733333333333,
      "grad_norm": 0.08842030167579651,
      "learning_rate": 5.8766666666666674e-06,
      "loss": 0.0014,
      "step": 132370
    },
    {
      "epoch": 7.060266666666666,
      "grad_norm": 0.1702355593442917,
      "learning_rate": 5.8733333333333336e-06,
      "loss": 0.0019,
      "step": 132380
    },
    {
      "epoch": 7.0608,
      "grad_norm": 0.3404886722564697,
      "learning_rate": 5.8700000000000005e-06,
      "loss": 0.0028,
      "step": 132390
    },
    {
      "epoch": 7.061333333333334,
      "grad_norm": 0.2471754252910614,
      "learning_rate": 5.866666666666667e-06,
      "loss": 0.002,
      "step": 132400
    },
    {
      "epoch": 7.061866666666667,
      "grad_norm": 0.17472410202026367,
      "learning_rate": 5.863333333333334e-06,
      "loss": 0.0016,
      "step": 132410
    },
    {
      "epoch": 7.0624,
      "grad_norm": 0.10188878327608109,
      "learning_rate": 5.86e-06,
      "loss": 0.0019,
      "step": 132420
    },
    {
      "epoch": 7.0629333333333335,
      "grad_norm": 0.15262936055660248,
      "learning_rate": 5.856666666666668e-06,
      "loss": 0.0011,
      "step": 132430
    },
    {
      "epoch": 7.063466666666667,
      "grad_norm": 0.05621802434325218,
      "learning_rate": 5.853333333333334e-06,
      "loss": 0.0015,
      "step": 132440
    },
    {
      "epoch": 7.064,
      "grad_norm": 0.0999864786863327,
      "learning_rate": 5.850000000000001e-06,
      "loss": 0.0014,
      "step": 132450
    },
    {
      "epoch": 7.064533333333333,
      "grad_norm": 0.1950434148311615,
      "learning_rate": 5.846666666666667e-06,
      "loss": 0.002,
      "step": 132460
    },
    {
      "epoch": 7.065066666666667,
      "grad_norm": 0.2916560769081116,
      "learning_rate": 5.843333333333333e-06,
      "loss": 0.0017,
      "step": 132470
    },
    {
      "epoch": 7.0656,
      "grad_norm": 0.218471497297287,
      "learning_rate": 5.84e-06,
      "loss": 0.0024,
      "step": 132480
    },
    {
      "epoch": 7.066133333333333,
      "grad_norm": 0.2961582839488983,
      "learning_rate": 5.836666666666667e-06,
      "loss": 0.0017,
      "step": 132490
    },
    {
      "epoch": 7.066666666666666,
      "grad_norm": 0.27105847001075745,
      "learning_rate": 5.833333333333334e-06,
      "loss": 0.0012,
      "step": 132500
    },
    {
      "epoch": 7.0672,
      "grad_norm": 0.32017093896865845,
      "learning_rate": 5.83e-06,
      "loss": 0.0024,
      "step": 132510
    },
    {
      "epoch": 7.067733333333333,
      "grad_norm": 0.21573977172374725,
      "learning_rate": 5.826666666666667e-06,
      "loss": 0.0014,
      "step": 132520
    },
    {
      "epoch": 7.068266666666666,
      "grad_norm": 0.20492953062057495,
      "learning_rate": 5.823333333333333e-06,
      "loss": 0.0023,
      "step": 132530
    },
    {
      "epoch": 7.0688,
      "grad_norm": 0.2713410556316376,
      "learning_rate": 5.82e-06,
      "loss": 0.0024,
      "step": 132540
    },
    {
      "epoch": 7.069333333333334,
      "grad_norm": 0.32994067668914795,
      "learning_rate": 5.816666666666667e-06,
      "loss": 0.0034,
      "step": 132550
    },
    {
      "epoch": 7.069866666666667,
      "grad_norm": 0.15051186084747314,
      "learning_rate": 5.813333333333334e-06,
      "loss": 0.0015,
      "step": 132560
    },
    {
      "epoch": 7.0704,
      "grad_norm": 0.38940829038619995,
      "learning_rate": 5.81e-06,
      "loss": 0.0015,
      "step": 132570
    },
    {
      "epoch": 7.0709333333333335,
      "grad_norm": 0.3489730954170227,
      "learning_rate": 5.806666666666667e-06,
      "loss": 0.002,
      "step": 132580
    },
    {
      "epoch": 7.071466666666667,
      "grad_norm": 0.2657254636287689,
      "learning_rate": 5.803333333333333e-06,
      "loss": 0.0013,
      "step": 132590
    },
    {
      "epoch": 7.072,
      "grad_norm": 0.06561799347400665,
      "learning_rate": 5.8e-06,
      "loss": 0.0018,
      "step": 132600
    },
    {
      "epoch": 7.072533333333333,
      "grad_norm": 0.09132098406553268,
      "learning_rate": 5.7966666666666665e-06,
      "loss": 0.0015,
      "step": 132610
    },
    {
      "epoch": 7.073066666666667,
      "grad_norm": 0.12257495522499084,
      "learning_rate": 5.793333333333334e-06,
      "loss": 0.0014,
      "step": 132620
    },
    {
      "epoch": 7.0736,
      "grad_norm": 0.3465768098831177,
      "learning_rate": 5.7900000000000005e-06,
      "loss": 0.0014,
      "step": 132630
    },
    {
      "epoch": 7.074133333333333,
      "grad_norm": 0.3851794898509979,
      "learning_rate": 5.786666666666667e-06,
      "loss": 0.0015,
      "step": 132640
    },
    {
      "epoch": 7.074666666666666,
      "grad_norm": 0.14831477403640747,
      "learning_rate": 5.783333333333334e-06,
      "loss": 0.0021,
      "step": 132650
    },
    {
      "epoch": 7.0752,
      "grad_norm": 0.28917160630226135,
      "learning_rate": 5.78e-06,
      "loss": 0.0017,
      "step": 132660
    },
    {
      "epoch": 7.075733333333333,
      "grad_norm": 0.17562299966812134,
      "learning_rate": 5.776666666666667e-06,
      "loss": 0.0014,
      "step": 132670
    },
    {
      "epoch": 7.076266666666666,
      "grad_norm": 0.04063279181718826,
      "learning_rate": 5.773333333333334e-06,
      "loss": 0.0017,
      "step": 132680
    },
    {
      "epoch": 7.0768,
      "grad_norm": 0.264752596616745,
      "learning_rate": 5.770000000000001e-06,
      "loss": 0.0013,
      "step": 132690
    },
    {
      "epoch": 7.077333333333334,
      "grad_norm": 0.051850467920303345,
      "learning_rate": 5.766666666666667e-06,
      "loss": 0.0022,
      "step": 132700
    },
    {
      "epoch": 7.077866666666667,
      "grad_norm": 0.05262802541255951,
      "learning_rate": 5.763333333333334e-06,
      "loss": 0.0031,
      "step": 132710
    },
    {
      "epoch": 7.0784,
      "grad_norm": 0.41760531067848206,
      "learning_rate": 5.76e-06,
      "loss": 0.0021,
      "step": 132720
    },
    {
      "epoch": 7.0789333333333335,
      "grad_norm": 0.13181281089782715,
      "learning_rate": 5.756666666666667e-06,
      "loss": 0.0017,
      "step": 132730
    },
    {
      "epoch": 7.079466666666667,
      "grad_norm": 0.21925604343414307,
      "learning_rate": 5.753333333333334e-06,
      "loss": 0.0019,
      "step": 132740
    },
    {
      "epoch": 7.08,
      "grad_norm": 0.14932487905025482,
      "learning_rate": 5.750000000000001e-06,
      "loss": 0.0016,
      "step": 132750
    },
    {
      "epoch": 7.080533333333333,
      "grad_norm": 0.30629947781562805,
      "learning_rate": 5.746666666666667e-06,
      "loss": 0.0019,
      "step": 132760
    },
    {
      "epoch": 7.081066666666667,
      "grad_norm": 0.5637415647506714,
      "learning_rate": 5.743333333333334e-06,
      "loss": 0.0015,
      "step": 132770
    },
    {
      "epoch": 7.0816,
      "grad_norm": 0.14842535555362701,
      "learning_rate": 5.74e-06,
      "loss": 0.0018,
      "step": 132780
    },
    {
      "epoch": 7.082133333333333,
      "grad_norm": 0.328948438167572,
      "learning_rate": 5.736666666666667e-06,
      "loss": 0.0019,
      "step": 132790
    },
    {
      "epoch": 7.082666666666666,
      "grad_norm": 0.2684570550918579,
      "learning_rate": 5.733333333333333e-06,
      "loss": 0.0015,
      "step": 132800
    },
    {
      "epoch": 7.0832,
      "grad_norm": 0.04792384058237076,
      "learning_rate": 5.73e-06,
      "loss": 0.0015,
      "step": 132810
    },
    {
      "epoch": 7.083733333333333,
      "grad_norm": 0.17338038980960846,
      "learning_rate": 5.726666666666667e-06,
      "loss": 0.0014,
      "step": 132820
    },
    {
      "epoch": 7.084266666666666,
      "grad_norm": 0.35185107588768005,
      "learning_rate": 5.723333333333333e-06,
      "loss": 0.0017,
      "step": 132830
    },
    {
      "epoch": 7.0848,
      "grad_norm": 0.09220033138990402,
      "learning_rate": 5.72e-06,
      "loss": 0.0015,
      "step": 132840
    },
    {
      "epoch": 7.085333333333334,
      "grad_norm": 0.18410177528858185,
      "learning_rate": 5.7166666666666664e-06,
      "loss": 0.0018,
      "step": 132850
    },
    {
      "epoch": 7.085866666666667,
      "grad_norm": 0.5055019855499268,
      "learning_rate": 5.713333333333333e-06,
      "loss": 0.0023,
      "step": 132860
    },
    {
      "epoch": 7.0864,
      "grad_norm": 0.17819708585739136,
      "learning_rate": 5.71e-06,
      "loss": 0.0015,
      "step": 132870
    },
    {
      "epoch": 7.0869333333333335,
      "grad_norm": 0.10544086992740631,
      "learning_rate": 5.706666666666667e-06,
      "loss": 0.0015,
      "step": 132880
    },
    {
      "epoch": 7.087466666666667,
      "grad_norm": 0.1219506785273552,
      "learning_rate": 5.7033333333333335e-06,
      "loss": 0.0021,
      "step": 132890
    },
    {
      "epoch": 7.088,
      "grad_norm": 0.0688694640994072,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 0.0012,
      "step": 132900
    },
    {
      "epoch": 7.088533333333333,
      "grad_norm": 0.2618687152862549,
      "learning_rate": 5.696666666666667e-06,
      "loss": 0.0029,
      "step": 132910
    },
    {
      "epoch": 7.089066666666667,
      "grad_norm": 0.27705061435699463,
      "learning_rate": 5.693333333333334e-06,
      "loss": 0.0019,
      "step": 132920
    },
    {
      "epoch": 7.0896,
      "grad_norm": 0.17461274564266205,
      "learning_rate": 5.690000000000001e-06,
      "loss": 0.0016,
      "step": 132930
    },
    {
      "epoch": 7.090133333333333,
      "grad_norm": 0.2092556357383728,
      "learning_rate": 5.6866666666666676e-06,
      "loss": 0.0019,
      "step": 132940
    },
    {
      "epoch": 7.0906666666666665,
      "grad_norm": 0.2647194266319275,
      "learning_rate": 5.683333333333334e-06,
      "loss": 0.0019,
      "step": 132950
    },
    {
      "epoch": 7.0912,
      "grad_norm": 0.3898734450340271,
      "learning_rate": 5.680000000000001e-06,
      "loss": 0.0021,
      "step": 132960
    },
    {
      "epoch": 7.091733333333333,
      "grad_norm": 0.09323756396770477,
      "learning_rate": 5.676666666666667e-06,
      "loss": 0.0016,
      "step": 132970
    },
    {
      "epoch": 7.092266666666666,
      "grad_norm": 0.03706709295511246,
      "learning_rate": 5.673333333333333e-06,
      "loss": 0.0024,
      "step": 132980
    },
    {
      "epoch": 7.0928,
      "grad_norm": 0.18149761855602264,
      "learning_rate": 5.67e-06,
      "loss": 0.0012,
      "step": 132990
    },
    {
      "epoch": 7.093333333333334,
      "grad_norm": 0.20697306096553802,
      "learning_rate": 5.666666666666667e-06,
      "loss": 0.0018,
      "step": 133000
    },
    {
      "epoch": 7.093866666666667,
      "grad_norm": 0.3448503315448761,
      "learning_rate": 5.663333333333334e-06,
      "loss": 0.0021,
      "step": 133010
    },
    {
      "epoch": 7.0944,
      "grad_norm": 0.27612394094467163,
      "learning_rate": 5.66e-06,
      "loss": 0.0016,
      "step": 133020
    },
    {
      "epoch": 7.0949333333333335,
      "grad_norm": 0.364958256483078,
      "learning_rate": 5.656666666666667e-06,
      "loss": 0.0014,
      "step": 133030
    },
    {
      "epoch": 7.095466666666667,
      "grad_norm": 0.23117956519126892,
      "learning_rate": 5.653333333333333e-06,
      "loss": 0.002,
      "step": 133040
    },
    {
      "epoch": 7.096,
      "grad_norm": 0.14767716825008392,
      "learning_rate": 5.65e-06,
      "loss": 0.0016,
      "step": 133050
    },
    {
      "epoch": 7.096533333333333,
      "grad_norm": 0.19459176063537598,
      "learning_rate": 5.646666666666667e-06,
      "loss": 0.0022,
      "step": 133060
    },
    {
      "epoch": 7.097066666666667,
      "grad_norm": 0.06620391458272934,
      "learning_rate": 5.643333333333334e-06,
      "loss": 0.0018,
      "step": 133070
    },
    {
      "epoch": 7.0976,
      "grad_norm": 0.36213502287864685,
      "learning_rate": 5.64e-06,
      "loss": 0.0017,
      "step": 133080
    },
    {
      "epoch": 7.098133333333333,
      "grad_norm": 0.4528809189796448,
      "learning_rate": 5.636666666666667e-06,
      "loss": 0.0018,
      "step": 133090
    },
    {
      "epoch": 7.0986666666666665,
      "grad_norm": 0.29150068759918213,
      "learning_rate": 5.633333333333333e-06,
      "loss": 0.0012,
      "step": 133100
    },
    {
      "epoch": 7.0992,
      "grad_norm": 0.32590818405151367,
      "learning_rate": 5.63e-06,
      "loss": 0.0017,
      "step": 133110
    },
    {
      "epoch": 7.099733333333333,
      "grad_norm": 0.6496902704238892,
      "learning_rate": 5.626666666666667e-06,
      "loss": 0.0012,
      "step": 133120
    },
    {
      "epoch": 7.100266666666666,
      "grad_norm": 0.4988601803779602,
      "learning_rate": 5.623333333333334e-06,
      "loss": 0.0028,
      "step": 133130
    },
    {
      "epoch": 7.1008,
      "grad_norm": 0.04227453097701073,
      "learning_rate": 5.62e-06,
      "loss": 0.0013,
      "step": 133140
    },
    {
      "epoch": 7.101333333333334,
      "grad_norm": 0.042985569685697556,
      "learning_rate": 5.6166666666666665e-06,
      "loss": 0.0017,
      "step": 133150
    },
    {
      "epoch": 7.101866666666667,
      "grad_norm": 0.5230503678321838,
      "learning_rate": 5.6133333333333335e-06,
      "loss": 0.0015,
      "step": 133160
    },
    {
      "epoch": 7.1024,
      "grad_norm": 0.09384208172559738,
      "learning_rate": 5.61e-06,
      "loss": 0.0015,
      "step": 133170
    },
    {
      "epoch": 7.1029333333333335,
      "grad_norm": 0.07544991374015808,
      "learning_rate": 5.606666666666667e-06,
      "loss": 0.0014,
      "step": 133180
    },
    {
      "epoch": 7.103466666666667,
      "grad_norm": 0.442913681268692,
      "learning_rate": 5.603333333333334e-06,
      "loss": 0.0016,
      "step": 133190
    },
    {
      "epoch": 7.104,
      "grad_norm": 0.1772042065858841,
      "learning_rate": 5.600000000000001e-06,
      "loss": 0.0015,
      "step": 133200
    },
    {
      "epoch": 7.104533333333333,
      "grad_norm": 0.26399245858192444,
      "learning_rate": 5.596666666666667e-06,
      "loss": 0.0025,
      "step": 133210
    },
    {
      "epoch": 7.105066666666667,
      "grad_norm": 0.20679479837417603,
      "learning_rate": 5.593333333333334e-06,
      "loss": 0.0021,
      "step": 133220
    },
    {
      "epoch": 7.1056,
      "grad_norm": 0.34887251257896423,
      "learning_rate": 5.59e-06,
      "loss": 0.0015,
      "step": 133230
    },
    {
      "epoch": 7.106133333333333,
      "grad_norm": 0.356250524520874,
      "learning_rate": 5.586666666666667e-06,
      "loss": 0.003,
      "step": 133240
    },
    {
      "epoch": 7.1066666666666665,
      "grad_norm": 0.22373341023921967,
      "learning_rate": 5.583333333333334e-06,
      "loss": 0.0015,
      "step": 133250
    },
    {
      "epoch": 7.1072,
      "grad_norm": 0.1836724877357483,
      "learning_rate": 5.580000000000001e-06,
      "loss": 0.0015,
      "step": 133260
    },
    {
      "epoch": 7.107733333333333,
      "grad_norm": 0.055034440010786057,
      "learning_rate": 5.576666666666667e-06,
      "loss": 0.0026,
      "step": 133270
    },
    {
      "epoch": 7.108266666666666,
      "grad_norm": 0.15501752495765686,
      "learning_rate": 5.573333333333334e-06,
      "loss": 0.0014,
      "step": 133280
    },
    {
      "epoch": 7.1088,
      "grad_norm": 0.2996613383293152,
      "learning_rate": 5.57e-06,
      "loss": 0.0018,
      "step": 133290
    },
    {
      "epoch": 7.109333333333334,
      "grad_norm": 0.22640402615070343,
      "learning_rate": 5.566666666666667e-06,
      "loss": 0.0022,
      "step": 133300
    },
    {
      "epoch": 7.109866666666667,
      "grad_norm": 0.38042151927948,
      "learning_rate": 5.563333333333334e-06,
      "loss": 0.0033,
      "step": 133310
    },
    {
      "epoch": 7.1104,
      "grad_norm": 0.046947650611400604,
      "learning_rate": 5.56e-06,
      "loss": 0.002,
      "step": 133320
    },
    {
      "epoch": 7.1109333333333336,
      "grad_norm": 0.06898414343595505,
      "learning_rate": 5.556666666666667e-06,
      "loss": 0.0013,
      "step": 133330
    },
    {
      "epoch": 7.111466666666667,
      "grad_norm": 0.18344646692276,
      "learning_rate": 5.553333333333333e-06,
      "loss": 0.0013,
      "step": 133340
    },
    {
      "epoch": 7.112,
      "grad_norm": 0.24180151522159576,
      "learning_rate": 5.55e-06,
      "loss": 0.0017,
      "step": 133350
    },
    {
      "epoch": 7.112533333333333,
      "grad_norm": 0.2685747444629669,
      "learning_rate": 5.546666666666666e-06,
      "loss": 0.0016,
      "step": 133360
    },
    {
      "epoch": 7.113066666666667,
      "grad_norm": 0.061976999044418335,
      "learning_rate": 5.543333333333333e-06,
      "loss": 0.0018,
      "step": 133370
    },
    {
      "epoch": 7.1136,
      "grad_norm": 0.23060853779315948,
      "learning_rate": 5.54e-06,
      "loss": 0.0016,
      "step": 133380
    },
    {
      "epoch": 7.114133333333333,
      "grad_norm": 0.2950456738471985,
      "learning_rate": 5.536666666666667e-06,
      "loss": 0.0014,
      "step": 133390
    },
    {
      "epoch": 7.1146666666666665,
      "grad_norm": 0.1003713607788086,
      "learning_rate": 5.5333333333333334e-06,
      "loss": 0.0018,
      "step": 133400
    },
    {
      "epoch": 7.1152,
      "grad_norm": 0.12933573126792908,
      "learning_rate": 5.53e-06,
      "loss": 0.0016,
      "step": 133410
    },
    {
      "epoch": 7.115733333333333,
      "grad_norm": 0.14926737546920776,
      "learning_rate": 5.5266666666666666e-06,
      "loss": 0.0015,
      "step": 133420
    },
    {
      "epoch": 7.116266666666666,
      "grad_norm": 0.3754527270793915,
      "learning_rate": 5.5233333333333335e-06,
      "loss": 0.0015,
      "step": 133430
    },
    {
      "epoch": 7.1168,
      "grad_norm": 0.2134435772895813,
      "learning_rate": 5.5200000000000005e-06,
      "loss": 0.0018,
      "step": 133440
    },
    {
      "epoch": 7.117333333333334,
      "grad_norm": 0.06510977447032928,
      "learning_rate": 5.5166666666666675e-06,
      "loss": 0.0013,
      "step": 133450
    },
    {
      "epoch": 7.117866666666667,
      "grad_norm": 0.09098628163337708,
      "learning_rate": 5.513333333333334e-06,
      "loss": 0.0015,
      "step": 133460
    },
    {
      "epoch": 7.1184,
      "grad_norm": 0.11972790211439133,
      "learning_rate": 5.510000000000001e-06,
      "loss": 0.0022,
      "step": 133470
    },
    {
      "epoch": 7.118933333333334,
      "grad_norm": 0.15820468962192535,
      "learning_rate": 5.506666666666667e-06,
      "loss": 0.0017,
      "step": 133480
    },
    {
      "epoch": 7.119466666666667,
      "grad_norm": 0.1025446355342865,
      "learning_rate": 5.503333333333333e-06,
      "loss": 0.0015,
      "step": 133490
    },
    {
      "epoch": 7.12,
      "grad_norm": 0.20279379189014435,
      "learning_rate": 5.500000000000001e-06,
      "loss": 0.0017,
      "step": 133500
    },
    {
      "epoch": 7.120533333333333,
      "grad_norm": 0.07502911239862442,
      "learning_rate": 5.496666666666667e-06,
      "loss": 0.0019,
      "step": 133510
    },
    {
      "epoch": 7.121066666666667,
      "grad_norm": 0.12425792217254639,
      "learning_rate": 5.493333333333334e-06,
      "loss": 0.0025,
      "step": 133520
    },
    {
      "epoch": 7.1216,
      "grad_norm": 0.508466362953186,
      "learning_rate": 5.49e-06,
      "loss": 0.0021,
      "step": 133530
    },
    {
      "epoch": 7.122133333333333,
      "grad_norm": 0.39211755990982056,
      "learning_rate": 5.486666666666667e-06,
      "loss": 0.0027,
      "step": 133540
    },
    {
      "epoch": 7.1226666666666665,
      "grad_norm": 0.06334707140922546,
      "learning_rate": 5.483333333333333e-06,
      "loss": 0.0014,
      "step": 133550
    },
    {
      "epoch": 7.1232,
      "grad_norm": 0.18155622482299805,
      "learning_rate": 5.48e-06,
      "loss": 0.0013,
      "step": 133560
    },
    {
      "epoch": 7.123733333333333,
      "grad_norm": 0.18150374293327332,
      "learning_rate": 5.476666666666667e-06,
      "loss": 0.0018,
      "step": 133570
    },
    {
      "epoch": 7.124266666666666,
      "grad_norm": 0.2662831246852875,
      "learning_rate": 5.473333333333334e-06,
      "loss": 0.0013,
      "step": 133580
    },
    {
      "epoch": 7.1248,
      "grad_norm": 0.33312854170799255,
      "learning_rate": 5.47e-06,
      "loss": 0.0015,
      "step": 133590
    },
    {
      "epoch": 7.125333333333334,
      "grad_norm": 0.3270905017852783,
      "learning_rate": 5.466666666666667e-06,
      "loss": 0.0016,
      "step": 133600
    },
    {
      "epoch": 7.125866666666667,
      "grad_norm": 0.16855117678642273,
      "learning_rate": 5.463333333333333e-06,
      "loss": 0.002,
      "step": 133610
    },
    {
      "epoch": 7.1264,
      "grad_norm": 0.06711261719465256,
      "learning_rate": 5.46e-06,
      "loss": 0.002,
      "step": 133620
    },
    {
      "epoch": 7.126933333333334,
      "grad_norm": 0.2692781388759613,
      "learning_rate": 5.456666666666667e-06,
      "loss": 0.0014,
      "step": 133630
    },
    {
      "epoch": 7.127466666666667,
      "grad_norm": 0.11666260659694672,
      "learning_rate": 5.453333333333334e-06,
      "loss": 0.0018,
      "step": 133640
    },
    {
      "epoch": 7.128,
      "grad_norm": 0.09982786327600479,
      "learning_rate": 5.45e-06,
      "loss": 0.0015,
      "step": 133650
    },
    {
      "epoch": 7.128533333333333,
      "grad_norm": 0.23365384340286255,
      "learning_rate": 5.4466666666666665e-06,
      "loss": 0.0023,
      "step": 133660
    },
    {
      "epoch": 7.129066666666667,
      "grad_norm": 0.15215493738651276,
      "learning_rate": 5.4433333333333335e-06,
      "loss": 0.0018,
      "step": 133670
    },
    {
      "epoch": 7.1296,
      "grad_norm": 0.05925162881612778,
      "learning_rate": 5.44e-06,
      "loss": 0.0015,
      "step": 133680
    },
    {
      "epoch": 7.130133333333333,
      "grad_norm": 0.2784488797187805,
      "learning_rate": 5.436666666666667e-06,
      "loss": 0.0014,
      "step": 133690
    },
    {
      "epoch": 7.1306666666666665,
      "grad_norm": 0.36018985509872437,
      "learning_rate": 5.4333333333333335e-06,
      "loss": 0.0016,
      "step": 133700
    },
    {
      "epoch": 7.1312,
      "grad_norm": 0.043719880282878876,
      "learning_rate": 5.4300000000000005e-06,
      "loss": 0.0019,
      "step": 133710
    },
    {
      "epoch": 7.131733333333333,
      "grad_norm": 0.17916123569011688,
      "learning_rate": 5.426666666666667e-06,
      "loss": 0.0014,
      "step": 133720
    },
    {
      "epoch": 7.132266666666666,
      "grad_norm": 0.28743499517440796,
      "learning_rate": 5.423333333333334e-06,
      "loss": 0.0013,
      "step": 133730
    },
    {
      "epoch": 7.1328,
      "grad_norm": 0.24393108487129211,
      "learning_rate": 5.42e-06,
      "loss": 0.0014,
      "step": 133740
    },
    {
      "epoch": 7.133333333333334,
      "grad_norm": 0.06336177885532379,
      "learning_rate": 5.416666666666667e-06,
      "loss": 0.0024,
      "step": 133750
    },
    {
      "epoch": 7.133866666666667,
      "grad_norm": 0.23642902076244354,
      "learning_rate": 5.413333333333334e-06,
      "loss": 0.0015,
      "step": 133760
    },
    {
      "epoch": 7.1344,
      "grad_norm": 0.10821645706892014,
      "learning_rate": 5.410000000000001e-06,
      "loss": 0.0022,
      "step": 133770
    },
    {
      "epoch": 7.134933333333334,
      "grad_norm": 0.46935391426086426,
      "learning_rate": 5.406666666666667e-06,
      "loss": 0.002,
      "step": 133780
    },
    {
      "epoch": 7.135466666666667,
      "grad_norm": 0.10427586734294891,
      "learning_rate": 5.403333333333334e-06,
      "loss": 0.0016,
      "step": 133790
    },
    {
      "epoch": 7.136,
      "grad_norm": 0.04075118526816368,
      "learning_rate": 5.4e-06,
      "loss": 0.0014,
      "step": 133800
    },
    {
      "epoch": 7.136533333333333,
      "grad_norm": 0.3855312466621399,
      "learning_rate": 5.396666666666667e-06,
      "loss": 0.0016,
      "step": 133810
    },
    {
      "epoch": 7.137066666666667,
      "grad_norm": 0.20350529253482819,
      "learning_rate": 5.393333333333334e-06,
      "loss": 0.002,
      "step": 133820
    },
    {
      "epoch": 7.1376,
      "grad_norm": 0.10358333587646484,
      "learning_rate": 5.390000000000001e-06,
      "loss": 0.0022,
      "step": 133830
    },
    {
      "epoch": 7.138133333333333,
      "grad_norm": 0.39671486616134644,
      "learning_rate": 5.386666666666667e-06,
      "loss": 0.0018,
      "step": 133840
    },
    {
      "epoch": 7.1386666666666665,
      "grad_norm": 0.07112111151218414,
      "learning_rate": 5.383333333333333e-06,
      "loss": 0.0021,
      "step": 133850
    },
    {
      "epoch": 7.1392,
      "grad_norm": 0.2864094376564026,
      "learning_rate": 5.38e-06,
      "loss": 0.0013,
      "step": 133860
    },
    {
      "epoch": 7.139733333333333,
      "grad_norm": 0.5696161985397339,
      "learning_rate": 5.376666666666666e-06,
      "loss": 0.0017,
      "step": 133870
    },
    {
      "epoch": 7.140266666666666,
      "grad_norm": 0.3961288332939148,
      "learning_rate": 5.373333333333333e-06,
      "loss": 0.0012,
      "step": 133880
    },
    {
      "epoch": 7.1408,
      "grad_norm": 0.06761017441749573,
      "learning_rate": 5.37e-06,
      "loss": 0.002,
      "step": 133890
    },
    {
      "epoch": 7.141333333333334,
      "grad_norm": 0.18285243213176727,
      "learning_rate": 5.366666666666667e-06,
      "loss": 0.0021,
      "step": 133900
    },
    {
      "epoch": 7.141866666666667,
      "grad_norm": 0.26315125823020935,
      "learning_rate": 5.363333333333333e-06,
      "loss": 0.0017,
      "step": 133910
    },
    {
      "epoch": 7.1424,
      "grad_norm": 0.4300243556499481,
      "learning_rate": 5.36e-06,
      "loss": 0.0025,
      "step": 133920
    },
    {
      "epoch": 7.142933333333334,
      "grad_norm": 0.5018575191497803,
      "learning_rate": 5.3566666666666665e-06,
      "loss": 0.0014,
      "step": 133930
    },
    {
      "epoch": 7.143466666666667,
      "grad_norm": 0.1403689682483673,
      "learning_rate": 5.3533333333333335e-06,
      "loss": 0.0012,
      "step": 133940
    },
    {
      "epoch": 7.144,
      "grad_norm": 0.3184257745742798,
      "learning_rate": 5.3500000000000004e-06,
      "loss": 0.0013,
      "step": 133950
    },
    {
      "epoch": 7.144533333333333,
      "grad_norm": 0.321829229593277,
      "learning_rate": 5.3466666666666674e-06,
      "loss": 0.0021,
      "step": 133960
    },
    {
      "epoch": 7.145066666666667,
      "grad_norm": 0.05758001655340195,
      "learning_rate": 5.3433333333333336e-06,
      "loss": 0.0014,
      "step": 133970
    },
    {
      "epoch": 7.1456,
      "grad_norm": 0.11960629373788834,
      "learning_rate": 5.3400000000000005e-06,
      "loss": 0.0015,
      "step": 133980
    },
    {
      "epoch": 7.146133333333333,
      "grad_norm": 0.15650315582752228,
      "learning_rate": 5.336666666666667e-06,
      "loss": 0.002,
      "step": 133990
    },
    {
      "epoch": 7.1466666666666665,
      "grad_norm": 0.17436262965202332,
      "learning_rate": 5.333333333333334e-06,
      "loss": 0.0014,
      "step": 134000
    },
    {
      "epoch": 7.1472,
      "grad_norm": 0.09129578620195389,
      "learning_rate": 5.330000000000001e-06,
      "loss": 0.0018,
      "step": 134010
    },
    {
      "epoch": 7.147733333333333,
      "grad_norm": 0.24498078227043152,
      "learning_rate": 5.326666666666667e-06,
      "loss": 0.0016,
      "step": 134020
    },
    {
      "epoch": 7.148266666666666,
      "grad_norm": 0.1496131718158722,
      "learning_rate": 5.323333333333334e-06,
      "loss": 0.0022,
      "step": 134030
    },
    {
      "epoch": 7.1488,
      "grad_norm": 0.1953449845314026,
      "learning_rate": 5.32e-06,
      "loss": 0.0016,
      "step": 134040
    },
    {
      "epoch": 7.149333333333334,
      "grad_norm": 0.2088211476802826,
      "learning_rate": 5.316666666666667e-06,
      "loss": 0.0015,
      "step": 134050
    },
    {
      "epoch": 7.149866666666667,
      "grad_norm": 0.13581061363220215,
      "learning_rate": 5.313333333333333e-06,
      "loss": 0.0025,
      "step": 134060
    },
    {
      "epoch": 7.1504,
      "grad_norm": 0.10455432534217834,
      "learning_rate": 5.31e-06,
      "loss": 0.0017,
      "step": 134070
    },
    {
      "epoch": 7.150933333333334,
      "grad_norm": 0.032331060618162155,
      "learning_rate": 5.306666666666667e-06,
      "loss": 0.0016,
      "step": 134080
    },
    {
      "epoch": 7.151466666666667,
      "grad_norm": 0.08841001242399216,
      "learning_rate": 5.303333333333334e-06,
      "loss": 0.0018,
      "step": 134090
    },
    {
      "epoch": 7.152,
      "grad_norm": 0.11472343653440475,
      "learning_rate": 5.3e-06,
      "loss": 0.0012,
      "step": 134100
    },
    {
      "epoch": 7.152533333333333,
      "grad_norm": 0.12105982005596161,
      "learning_rate": 5.296666666666667e-06,
      "loss": 0.0016,
      "step": 134110
    },
    {
      "epoch": 7.153066666666667,
      "grad_norm": 0.1573062688112259,
      "learning_rate": 5.293333333333333e-06,
      "loss": 0.002,
      "step": 134120
    },
    {
      "epoch": 7.1536,
      "grad_norm": 0.03829812631011009,
      "learning_rate": 5.29e-06,
      "loss": 0.0025,
      "step": 134130
    },
    {
      "epoch": 7.154133333333333,
      "grad_norm": 0.29788798093795776,
      "learning_rate": 5.286666666666667e-06,
      "loss": 0.0016,
      "step": 134140
    },
    {
      "epoch": 7.1546666666666665,
      "grad_norm": 0.07088585942983627,
      "learning_rate": 5.283333333333334e-06,
      "loss": 0.0013,
      "step": 134150
    },
    {
      "epoch": 7.1552,
      "grad_norm": 0.06740289181470871,
      "learning_rate": 5.28e-06,
      "loss": 0.0015,
      "step": 134160
    },
    {
      "epoch": 7.155733333333333,
      "grad_norm": 0.048642728477716446,
      "learning_rate": 5.276666666666667e-06,
      "loss": 0.002,
      "step": 134170
    },
    {
      "epoch": 7.156266666666666,
      "grad_norm": 0.3533373177051544,
      "learning_rate": 5.273333333333333e-06,
      "loss": 0.0011,
      "step": 134180
    },
    {
      "epoch": 7.1568,
      "grad_norm": 0.06774700433015823,
      "learning_rate": 5.2699999999999995e-06,
      "loss": 0.0017,
      "step": 134190
    },
    {
      "epoch": 7.157333333333334,
      "grad_norm": 0.24076233804225922,
      "learning_rate": 5.266666666666667e-06,
      "loss": 0.0015,
      "step": 134200
    },
    {
      "epoch": 7.157866666666667,
      "grad_norm": 0.20782160758972168,
      "learning_rate": 5.2633333333333335e-06,
      "loss": 0.0014,
      "step": 134210
    },
    {
      "epoch": 7.1584,
      "grad_norm": 0.2615617513656616,
      "learning_rate": 5.2600000000000005e-06,
      "loss": 0.0016,
      "step": 134220
    },
    {
      "epoch": 7.158933333333334,
      "grad_norm": 0.46911224722862244,
      "learning_rate": 5.256666666666667e-06,
      "loss": 0.0025,
      "step": 134230
    },
    {
      "epoch": 7.159466666666667,
      "grad_norm": 0.04596514627337456,
      "learning_rate": 5.2533333333333336e-06,
      "loss": 0.0017,
      "step": 134240
    },
    {
      "epoch": 7.16,
      "grad_norm": 0.12358034402132034,
      "learning_rate": 5.25e-06,
      "loss": 0.0013,
      "step": 134250
    },
    {
      "epoch": 7.160533333333333,
      "grad_norm": 0.2776228189468384,
      "learning_rate": 5.246666666666667e-06,
      "loss": 0.0021,
      "step": 134260
    },
    {
      "epoch": 7.161066666666667,
      "grad_norm": 0.06326978653669357,
      "learning_rate": 5.243333333333334e-06,
      "loss": 0.0019,
      "step": 134270
    },
    {
      "epoch": 7.1616,
      "grad_norm": 0.21902509033679962,
      "learning_rate": 5.240000000000001e-06,
      "loss": 0.0014,
      "step": 134280
    },
    {
      "epoch": 7.162133333333333,
      "grad_norm": 0.0608668327331543,
      "learning_rate": 5.236666666666667e-06,
      "loss": 0.0018,
      "step": 134290
    },
    {
      "epoch": 7.1626666666666665,
      "grad_norm": 0.12166330963373184,
      "learning_rate": 5.233333333333334e-06,
      "loss": 0.002,
      "step": 134300
    },
    {
      "epoch": 7.1632,
      "grad_norm": 0.08770190924406052,
      "learning_rate": 5.23e-06,
      "loss": 0.0016,
      "step": 134310
    },
    {
      "epoch": 7.163733333333333,
      "grad_norm": 0.150009423494339,
      "learning_rate": 5.226666666666667e-06,
      "loss": 0.0017,
      "step": 134320
    },
    {
      "epoch": 7.164266666666666,
      "grad_norm": 0.0412958599627018,
      "learning_rate": 5.223333333333334e-06,
      "loss": 0.0023,
      "step": 134330
    },
    {
      "epoch": 7.1648,
      "grad_norm": 0.30081385374069214,
      "learning_rate": 5.220000000000001e-06,
      "loss": 0.0019,
      "step": 134340
    },
    {
      "epoch": 7.165333333333333,
      "grad_norm": 0.29844093322753906,
      "learning_rate": 5.216666666666667e-06,
      "loss": 0.0015,
      "step": 134350
    },
    {
      "epoch": 7.165866666666667,
      "grad_norm": 0.028780395165085793,
      "learning_rate": 5.213333333333333e-06,
      "loss": 0.0015,
      "step": 134360
    },
    {
      "epoch": 7.1664,
      "grad_norm": 0.5698899626731873,
      "learning_rate": 5.21e-06,
      "loss": 0.0011,
      "step": 134370
    },
    {
      "epoch": 7.166933333333334,
      "grad_norm": 0.14500080049037933,
      "learning_rate": 5.206666666666666e-06,
      "loss": 0.0016,
      "step": 134380
    },
    {
      "epoch": 7.167466666666667,
      "grad_norm": 0.2337518185377121,
      "learning_rate": 5.203333333333334e-06,
      "loss": 0.0018,
      "step": 134390
    },
    {
      "epoch": 7.168,
      "grad_norm": 0.34235793352127075,
      "learning_rate": 5.2e-06,
      "loss": 0.0038,
      "step": 134400
    },
    {
      "epoch": 7.168533333333333,
      "grad_norm": 0.12665940821170807,
      "learning_rate": 5.196666666666667e-06,
      "loss": 0.0015,
      "step": 134410
    },
    {
      "epoch": 7.169066666666667,
      "grad_norm": 0.040432967245578766,
      "learning_rate": 5.193333333333333e-06,
      "loss": 0.0019,
      "step": 134420
    },
    {
      "epoch": 7.1696,
      "grad_norm": 0.21195107698440552,
      "learning_rate": 5.19e-06,
      "loss": 0.0017,
      "step": 134430
    },
    {
      "epoch": 7.170133333333333,
      "grad_norm": 0.10037785768508911,
      "learning_rate": 5.186666666666666e-06,
      "loss": 0.0014,
      "step": 134440
    },
    {
      "epoch": 7.1706666666666665,
      "grad_norm": 0.3774751126766205,
      "learning_rate": 5.183333333333333e-06,
      "loss": 0.0015,
      "step": 134450
    },
    {
      "epoch": 7.1712,
      "grad_norm": 0.21679730713367462,
      "learning_rate": 5.18e-06,
      "loss": 0.0014,
      "step": 134460
    },
    {
      "epoch": 7.171733333333333,
      "grad_norm": 0.1261502057313919,
      "learning_rate": 5.176666666666667e-06,
      "loss": 0.0019,
      "step": 134470
    },
    {
      "epoch": 7.172266666666666,
      "grad_norm": 0.05292661488056183,
      "learning_rate": 5.1733333333333335e-06,
      "loss": 0.0012,
      "step": 134480
    },
    {
      "epoch": 7.1728,
      "grad_norm": 0.4208683371543884,
      "learning_rate": 5.1700000000000005e-06,
      "loss": 0.0016,
      "step": 134490
    },
    {
      "epoch": 7.173333333333334,
      "grad_norm": 0.26249176263809204,
      "learning_rate": 5.166666666666667e-06,
      "loss": 0.0022,
      "step": 134500
    },
    {
      "epoch": 7.173866666666667,
      "grad_norm": 0.29422396421432495,
      "learning_rate": 5.163333333333334e-06,
      "loss": 0.0033,
      "step": 134510
    },
    {
      "epoch": 7.1744,
      "grad_norm": 0.2690863609313965,
      "learning_rate": 5.1600000000000006e-06,
      "loss": 0.0026,
      "step": 134520
    },
    {
      "epoch": 7.174933333333334,
      "grad_norm": 0.2514350712299347,
      "learning_rate": 5.156666666666667e-06,
      "loss": 0.0016,
      "step": 134530
    },
    {
      "epoch": 7.175466666666667,
      "grad_norm": 0.052569326013326645,
      "learning_rate": 5.153333333333334e-06,
      "loss": 0.0025,
      "step": 134540
    },
    {
      "epoch": 7.176,
      "grad_norm": 0.07044056057929993,
      "learning_rate": 5.15e-06,
      "loss": 0.0013,
      "step": 134550
    },
    {
      "epoch": 7.176533333333333,
      "grad_norm": 0.12148548662662506,
      "learning_rate": 5.146666666666667e-06,
      "loss": 0.0029,
      "step": 134560
    },
    {
      "epoch": 7.177066666666667,
      "grad_norm": 0.03156958892941475,
      "learning_rate": 5.143333333333333e-06,
      "loss": 0.0015,
      "step": 134570
    },
    {
      "epoch": 7.1776,
      "grad_norm": 0.3327895700931549,
      "learning_rate": 5.140000000000001e-06,
      "loss": 0.0017,
      "step": 134580
    },
    {
      "epoch": 7.178133333333333,
      "grad_norm": 0.16009153425693512,
      "learning_rate": 5.136666666666667e-06,
      "loss": 0.0021,
      "step": 134590
    },
    {
      "epoch": 7.1786666666666665,
      "grad_norm": 0.17170977592468262,
      "learning_rate": 5.133333333333334e-06,
      "loss": 0.0014,
      "step": 134600
    },
    {
      "epoch": 7.1792,
      "grad_norm": 0.44445446133613586,
      "learning_rate": 5.13e-06,
      "loss": 0.002,
      "step": 134610
    },
    {
      "epoch": 7.179733333333333,
      "grad_norm": 0.3474918007850647,
      "learning_rate": 5.126666666666667e-06,
      "loss": 0.0016,
      "step": 134620
    },
    {
      "epoch": 7.180266666666666,
      "grad_norm": 0.03647253289818764,
      "learning_rate": 5.123333333333333e-06,
      "loss": 0.0023,
      "step": 134630
    },
    {
      "epoch": 7.1808,
      "grad_norm": 0.04258577525615692,
      "learning_rate": 5.12e-06,
      "loss": 0.0012,
      "step": 134640
    },
    {
      "epoch": 7.181333333333333,
      "grad_norm": 0.15452641248703003,
      "learning_rate": 5.116666666666667e-06,
      "loss": 0.0023,
      "step": 134650
    },
    {
      "epoch": 7.181866666666667,
      "grad_norm": 0.20459683239459991,
      "learning_rate": 5.113333333333334e-06,
      "loss": 0.0024,
      "step": 134660
    },
    {
      "epoch": 7.1824,
      "grad_norm": 0.23419144749641418,
      "learning_rate": 5.11e-06,
      "loss": 0.0024,
      "step": 134670
    },
    {
      "epoch": 7.182933333333334,
      "grad_norm": 0.2079918384552002,
      "learning_rate": 5.106666666666667e-06,
      "loss": 0.0013,
      "step": 134680
    },
    {
      "epoch": 7.183466666666667,
      "grad_norm": 0.14466674625873566,
      "learning_rate": 5.103333333333333e-06,
      "loss": 0.0018,
      "step": 134690
    },
    {
      "epoch": 7.184,
      "grad_norm": 0.27384284138679504,
      "learning_rate": 5.1e-06,
      "loss": 0.0017,
      "step": 134700
    },
    {
      "epoch": 7.184533333333333,
      "grad_norm": 0.41322627663612366,
      "learning_rate": 5.096666666666667e-06,
      "loss": 0.0014,
      "step": 134710
    },
    {
      "epoch": 7.185066666666667,
      "grad_norm": 0.14276094734668732,
      "learning_rate": 5.093333333333333e-06,
      "loss": 0.0013,
      "step": 134720
    },
    {
      "epoch": 7.1856,
      "grad_norm": 0.08471857011318207,
      "learning_rate": 5.09e-06,
      "loss": 0.0014,
      "step": 134730
    },
    {
      "epoch": 7.186133333333333,
      "grad_norm": 0.12547944486141205,
      "learning_rate": 5.0866666666666665e-06,
      "loss": 0.0017,
      "step": 134740
    },
    {
      "epoch": 7.1866666666666665,
      "grad_norm": 0.42850348353385925,
      "learning_rate": 5.0833333333333335e-06,
      "loss": 0.0015,
      "step": 134750
    },
    {
      "epoch": 7.1872,
      "grad_norm": 0.22809822857379913,
      "learning_rate": 5.08e-06,
      "loss": 0.0018,
      "step": 134760
    },
    {
      "epoch": 7.187733333333333,
      "grad_norm": 0.20559647679328918,
      "learning_rate": 5.0766666666666675e-06,
      "loss": 0.0019,
      "step": 134770
    },
    {
      "epoch": 7.188266666666666,
      "grad_norm": 0.4411657452583313,
      "learning_rate": 5.073333333333334e-06,
      "loss": 0.0016,
      "step": 134780
    },
    {
      "epoch": 7.1888,
      "grad_norm": 0.4662027955055237,
      "learning_rate": 5.070000000000001e-06,
      "loss": 0.0018,
      "step": 134790
    },
    {
      "epoch": 7.189333333333333,
      "grad_norm": 0.3250219225883484,
      "learning_rate": 5.066666666666667e-06,
      "loss": 0.0017,
      "step": 134800
    },
    {
      "epoch": 7.189866666666667,
      "grad_norm": 0.15721140801906586,
      "learning_rate": 5.063333333333334e-06,
      "loss": 0.0023,
      "step": 134810
    },
    {
      "epoch": 7.1904,
      "grad_norm": 0.040765441954135895,
      "learning_rate": 5.06e-06,
      "loss": 0.0016,
      "step": 134820
    },
    {
      "epoch": 7.190933333333334,
      "grad_norm": 0.15153229236602783,
      "learning_rate": 5.056666666666667e-06,
      "loss": 0.0012,
      "step": 134830
    },
    {
      "epoch": 7.191466666666667,
      "grad_norm": 0.05785290524363518,
      "learning_rate": 5.053333333333334e-06,
      "loss": 0.0012,
      "step": 134840
    },
    {
      "epoch": 7.192,
      "grad_norm": 0.4085991382598877,
      "learning_rate": 5.050000000000001e-06,
      "loss": 0.0021,
      "step": 134850
    },
    {
      "epoch": 7.1925333333333334,
      "grad_norm": 0.240712970495224,
      "learning_rate": 5.046666666666667e-06,
      "loss": 0.0015,
      "step": 134860
    },
    {
      "epoch": 7.193066666666667,
      "grad_norm": 0.024217700585722923,
      "learning_rate": 5.043333333333333e-06,
      "loss": 0.0015,
      "step": 134870
    },
    {
      "epoch": 7.1936,
      "grad_norm": 0.06430160999298096,
      "learning_rate": 5.04e-06,
      "loss": 0.0026,
      "step": 134880
    },
    {
      "epoch": 7.194133333333333,
      "grad_norm": 0.461994469165802,
      "learning_rate": 5.036666666666667e-06,
      "loss": 0.0015,
      "step": 134890
    },
    {
      "epoch": 7.1946666666666665,
      "grad_norm": 0.32883721590042114,
      "learning_rate": 5.033333333333334e-06,
      "loss": 0.0015,
      "step": 134900
    },
    {
      "epoch": 7.1952,
      "grad_norm": 0.15171614289283752,
      "learning_rate": 5.03e-06,
      "loss": 0.0016,
      "step": 134910
    },
    {
      "epoch": 7.195733333333333,
      "grad_norm": 0.3491039276123047,
      "learning_rate": 5.026666666666667e-06,
      "loss": 0.0017,
      "step": 134920
    },
    {
      "epoch": 7.196266666666666,
      "grad_norm": 0.04901627451181412,
      "learning_rate": 5.023333333333333e-06,
      "loss": 0.0017,
      "step": 134930
    },
    {
      "epoch": 7.1968,
      "grad_norm": 0.20919501781463623,
      "learning_rate": 5.02e-06,
      "loss": 0.0016,
      "step": 134940
    },
    {
      "epoch": 7.197333333333333,
      "grad_norm": 0.30597028136253357,
      "learning_rate": 5.016666666666666e-06,
      "loss": 0.0014,
      "step": 134950
    },
    {
      "epoch": 7.197866666666667,
      "grad_norm": 0.11910316348075867,
      "learning_rate": 5.013333333333334e-06,
      "loss": 0.0017,
      "step": 134960
    },
    {
      "epoch": 7.1984,
      "grad_norm": 0.057033441960811615,
      "learning_rate": 5.01e-06,
      "loss": 0.002,
      "step": 134970
    },
    {
      "epoch": 7.198933333333334,
      "grad_norm": 0.293032169342041,
      "learning_rate": 5.006666666666667e-06,
      "loss": 0.002,
      "step": 134980
    },
    {
      "epoch": 7.199466666666667,
      "grad_norm": 0.069489024579525,
      "learning_rate": 5.0033333333333334e-06,
      "loss": 0.0028,
      "step": 134990
    },
    {
      "epoch": 7.2,
      "grad_norm": 0.04160524532198906,
      "learning_rate": 5e-06,
      "loss": 0.0021,
      "step": 135000
    },
    {
      "epoch": 7.2005333333333335,
      "grad_norm": 0.04762857407331467,
      "learning_rate": 4.9966666666666665e-06,
      "loss": 0.0023,
      "step": 135010
    },
    {
      "epoch": 7.201066666666667,
      "grad_norm": 0.23831318318843842,
      "learning_rate": 4.9933333333333335e-06,
      "loss": 0.0015,
      "step": 135020
    },
    {
      "epoch": 7.2016,
      "grad_norm": 0.03449435532093048,
      "learning_rate": 4.9900000000000005e-06,
      "loss": 0.0013,
      "step": 135030
    },
    {
      "epoch": 7.202133333333333,
      "grad_norm": 0.15250103175640106,
      "learning_rate": 4.986666666666667e-06,
      "loss": 0.0018,
      "step": 135040
    },
    {
      "epoch": 7.2026666666666666,
      "grad_norm": 0.14649470150470734,
      "learning_rate": 4.983333333333334e-06,
      "loss": 0.0013,
      "step": 135050
    },
    {
      "epoch": 7.2032,
      "grad_norm": 0.17686231434345245,
      "learning_rate": 4.98e-06,
      "loss": 0.0017,
      "step": 135060
    },
    {
      "epoch": 7.203733333333333,
      "grad_norm": 0.06733552366495132,
      "learning_rate": 4.976666666666667e-06,
      "loss": 0.0014,
      "step": 135070
    },
    {
      "epoch": 7.204266666666666,
      "grad_norm": 0.03435109183192253,
      "learning_rate": 4.973333333333334e-06,
      "loss": 0.0014,
      "step": 135080
    },
    {
      "epoch": 7.2048,
      "grad_norm": 0.19871929287910461,
      "learning_rate": 4.970000000000001e-06,
      "loss": 0.0012,
      "step": 135090
    },
    {
      "epoch": 7.205333333333333,
      "grad_norm": 0.07087387889623642,
      "learning_rate": 4.966666666666667e-06,
      "loss": 0.0015,
      "step": 135100
    },
    {
      "epoch": 7.205866666666667,
      "grad_norm": 0.17923341691493988,
      "learning_rate": 4.963333333333334e-06,
      "loss": 0.0014,
      "step": 135110
    },
    {
      "epoch": 7.2064,
      "grad_norm": 0.4848254919052124,
      "learning_rate": 4.96e-06,
      "loss": 0.0016,
      "step": 135120
    },
    {
      "epoch": 7.206933333333334,
      "grad_norm": 0.26215988397598267,
      "learning_rate": 4.956666666666667e-06,
      "loss": 0.0015,
      "step": 135130
    },
    {
      "epoch": 7.207466666666667,
      "grad_norm": 0.17370198667049408,
      "learning_rate": 4.953333333333333e-06,
      "loss": 0.0022,
      "step": 135140
    },
    {
      "epoch": 7.208,
      "grad_norm": 0.2640719711780548,
      "learning_rate": 4.950000000000001e-06,
      "loss": 0.0019,
      "step": 135150
    },
    {
      "epoch": 7.2085333333333335,
      "grad_norm": 0.40778297185897827,
      "learning_rate": 4.946666666666667e-06,
      "loss": 0.0016,
      "step": 135160
    },
    {
      "epoch": 7.209066666666667,
      "grad_norm": 0.07603923231363297,
      "learning_rate": 4.943333333333334e-06,
      "loss": 0.0022,
      "step": 135170
    },
    {
      "epoch": 7.2096,
      "grad_norm": 0.177681103348732,
      "learning_rate": 4.94e-06,
      "loss": 0.0015,
      "step": 135180
    },
    {
      "epoch": 7.210133333333333,
      "grad_norm": 0.21075518429279327,
      "learning_rate": 4.936666666666667e-06,
      "loss": 0.002,
      "step": 135190
    },
    {
      "epoch": 7.210666666666667,
      "grad_norm": 0.14947062730789185,
      "learning_rate": 4.933333333333333e-06,
      "loss": 0.0016,
      "step": 135200
    },
    {
      "epoch": 7.2112,
      "grad_norm": 0.06723491102457047,
      "learning_rate": 4.93e-06,
      "loss": 0.0018,
      "step": 135210
    },
    {
      "epoch": 7.211733333333333,
      "grad_norm": 0.1835741102695465,
      "learning_rate": 4.926666666666667e-06,
      "loss": 0.0023,
      "step": 135220
    },
    {
      "epoch": 7.212266666666666,
      "grad_norm": 0.3251085877418518,
      "learning_rate": 4.923333333333333e-06,
      "loss": 0.0013,
      "step": 135230
    },
    {
      "epoch": 7.2128,
      "grad_norm": 0.04408103600144386,
      "learning_rate": 4.92e-06,
      "loss": 0.0013,
      "step": 135240
    },
    {
      "epoch": 7.213333333333333,
      "grad_norm": 0.46615439653396606,
      "learning_rate": 4.9166666666666665e-06,
      "loss": 0.0025,
      "step": 135250
    },
    {
      "epoch": 7.213866666666667,
      "grad_norm": 0.23492158949375153,
      "learning_rate": 4.9133333333333334e-06,
      "loss": 0.0014,
      "step": 135260
    },
    {
      "epoch": 7.2144,
      "grad_norm": 0.17554442584514618,
      "learning_rate": 4.9100000000000004e-06,
      "loss": 0.0021,
      "step": 135270
    },
    {
      "epoch": 7.214933333333334,
      "grad_norm": 0.4134570062160492,
      "learning_rate": 4.906666666666667e-06,
      "loss": 0.0023,
      "step": 135280
    },
    {
      "epoch": 7.215466666666667,
      "grad_norm": 0.15930281579494476,
      "learning_rate": 4.9033333333333335e-06,
      "loss": 0.0025,
      "step": 135290
    },
    {
      "epoch": 7.216,
      "grad_norm": 0.15730971097946167,
      "learning_rate": 4.9000000000000005e-06,
      "loss": 0.0016,
      "step": 135300
    },
    {
      "epoch": 7.2165333333333335,
      "grad_norm": 0.30503037571907043,
      "learning_rate": 4.896666666666667e-06,
      "loss": 0.0014,
      "step": 135310
    },
    {
      "epoch": 7.217066666666667,
      "grad_norm": 0.11860445886850357,
      "learning_rate": 4.893333333333334e-06,
      "loss": 0.0018,
      "step": 135320
    },
    {
      "epoch": 7.2176,
      "grad_norm": 0.04968668147921562,
      "learning_rate": 4.89e-06,
      "loss": 0.0016,
      "step": 135330
    },
    {
      "epoch": 7.218133333333333,
      "grad_norm": 0.060595739632844925,
      "learning_rate": 4.886666666666667e-06,
      "loss": 0.0026,
      "step": 135340
    },
    {
      "epoch": 7.218666666666667,
      "grad_norm": 0.2735968828201294,
      "learning_rate": 4.883333333333334e-06,
      "loss": 0.0014,
      "step": 135350
    },
    {
      "epoch": 7.2192,
      "grad_norm": 0.0789586529135704,
      "learning_rate": 4.880000000000001e-06,
      "loss": 0.0017,
      "step": 135360
    },
    {
      "epoch": 7.219733333333333,
      "grad_norm": 0.06386993080377579,
      "learning_rate": 4.876666666666667e-06,
      "loss": 0.002,
      "step": 135370
    },
    {
      "epoch": 7.220266666666666,
      "grad_norm": 0.18627198040485382,
      "learning_rate": 4.873333333333333e-06,
      "loss": 0.0016,
      "step": 135380
    },
    {
      "epoch": 7.2208,
      "grad_norm": 0.29419997334480286,
      "learning_rate": 4.87e-06,
      "loss": 0.002,
      "step": 135390
    },
    {
      "epoch": 7.221333333333333,
      "grad_norm": 0.05089635029435158,
      "learning_rate": 4.866666666666667e-06,
      "loss": 0.0025,
      "step": 135400
    },
    {
      "epoch": 7.221866666666667,
      "grad_norm": 0.042093850672245026,
      "learning_rate": 4.863333333333334e-06,
      "loss": 0.0014,
      "step": 135410
    },
    {
      "epoch": 7.2224,
      "grad_norm": 0.12155220657587051,
      "learning_rate": 4.86e-06,
      "loss": 0.002,
      "step": 135420
    },
    {
      "epoch": 7.222933333333334,
      "grad_norm": 0.17702455818653107,
      "learning_rate": 4.856666666666667e-06,
      "loss": 0.0018,
      "step": 135430
    },
    {
      "epoch": 7.223466666666667,
      "grad_norm": 0.5784508585929871,
      "learning_rate": 4.853333333333333e-06,
      "loss": 0.0019,
      "step": 135440
    },
    {
      "epoch": 7.224,
      "grad_norm": 0.07632532715797424,
      "learning_rate": 4.85e-06,
      "loss": 0.0013,
      "step": 135450
    },
    {
      "epoch": 7.2245333333333335,
      "grad_norm": 0.15589837729930878,
      "learning_rate": 4.846666666666667e-06,
      "loss": 0.0014,
      "step": 135460
    },
    {
      "epoch": 7.225066666666667,
      "grad_norm": 0.09154673665761948,
      "learning_rate": 4.843333333333334e-06,
      "loss": 0.0013,
      "step": 135470
    },
    {
      "epoch": 7.2256,
      "grad_norm": 0.5627484321594238,
      "learning_rate": 4.84e-06,
      "loss": 0.0036,
      "step": 135480
    },
    {
      "epoch": 7.226133333333333,
      "grad_norm": 0.18411467969417572,
      "learning_rate": 4.836666666666667e-06,
      "loss": 0.0012,
      "step": 135490
    },
    {
      "epoch": 7.226666666666667,
      "grad_norm": 0.11875712126493454,
      "learning_rate": 4.833333333333333e-06,
      "loss": 0.0013,
      "step": 135500
    },
    {
      "epoch": 7.2272,
      "grad_norm": 0.21496763825416565,
      "learning_rate": 4.83e-06,
      "loss": 0.0017,
      "step": 135510
    },
    {
      "epoch": 7.227733333333333,
      "grad_norm": 0.11899597942829132,
      "learning_rate": 4.8266666666666665e-06,
      "loss": 0.0014,
      "step": 135520
    },
    {
      "epoch": 7.228266666666666,
      "grad_norm": 0.09374841302633286,
      "learning_rate": 4.8233333333333335e-06,
      "loss": 0.0013,
      "step": 135530
    },
    {
      "epoch": 7.2288,
      "grad_norm": 0.25185123085975647,
      "learning_rate": 4.8200000000000004e-06,
      "loss": 0.0018,
      "step": 135540
    },
    {
      "epoch": 7.229333333333333,
      "grad_norm": 0.18315230309963226,
      "learning_rate": 4.816666666666667e-06,
      "loss": 0.0014,
      "step": 135550
    },
    {
      "epoch": 7.229866666666666,
      "grad_norm": 0.23426587879657745,
      "learning_rate": 4.8133333333333336e-06,
      "loss": 0.0015,
      "step": 135560
    },
    {
      "epoch": 7.2304,
      "grad_norm": 0.2502687871456146,
      "learning_rate": 4.81e-06,
      "loss": 0.0015,
      "step": 135570
    },
    {
      "epoch": 7.230933333333334,
      "grad_norm": 0.2667505741119385,
      "learning_rate": 4.806666666666667e-06,
      "loss": 0.002,
      "step": 135580
    },
    {
      "epoch": 7.231466666666667,
      "grad_norm": 0.3818022608757019,
      "learning_rate": 4.803333333333334e-06,
      "loss": 0.0016,
      "step": 135590
    },
    {
      "epoch": 7.232,
      "grad_norm": 0.1853141486644745,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.0024,
      "step": 135600
    },
    {
      "epoch": 7.2325333333333335,
      "grad_norm": 0.15254944562911987,
      "learning_rate": 4.796666666666667e-06,
      "loss": 0.002,
      "step": 135610
    },
    {
      "epoch": 7.233066666666667,
      "grad_norm": 0.07989699393510818,
      "learning_rate": 4.793333333333334e-06,
      "loss": 0.0025,
      "step": 135620
    },
    {
      "epoch": 7.2336,
      "grad_norm": 0.14610181748867035,
      "learning_rate": 4.79e-06,
      "loss": 0.0017,
      "step": 135630
    },
    {
      "epoch": 7.234133333333333,
      "grad_norm": 0.24033300578594208,
      "learning_rate": 4.786666666666667e-06,
      "loss": 0.0025,
      "step": 135640
    },
    {
      "epoch": 7.234666666666667,
      "grad_norm": 0.06888798624277115,
      "learning_rate": 4.783333333333333e-06,
      "loss": 0.0012,
      "step": 135650
    },
    {
      "epoch": 7.2352,
      "grad_norm": 0.17804333567619324,
      "learning_rate": 4.780000000000001e-06,
      "loss": 0.0016,
      "step": 135660
    },
    {
      "epoch": 7.235733333333333,
      "grad_norm": 0.1876494437456131,
      "learning_rate": 4.776666666666667e-06,
      "loss": 0.0016,
      "step": 135670
    },
    {
      "epoch": 7.236266666666666,
      "grad_norm": 0.17702993750572205,
      "learning_rate": 4.773333333333334e-06,
      "loss": 0.0016,
      "step": 135680
    },
    {
      "epoch": 7.2368,
      "grad_norm": 0.10700774192810059,
      "learning_rate": 4.77e-06,
      "loss": 0.002,
      "step": 135690
    },
    {
      "epoch": 7.237333333333333,
      "grad_norm": 0.1218833327293396,
      "learning_rate": 4.766666666666667e-06,
      "loss": 0.0012,
      "step": 135700
    },
    {
      "epoch": 7.237866666666667,
      "grad_norm": 0.0947871282696724,
      "learning_rate": 4.763333333333333e-06,
      "loss": 0.0018,
      "step": 135710
    },
    {
      "epoch": 7.2384,
      "grad_norm": 0.4074114263057709,
      "learning_rate": 4.76e-06,
      "loss": 0.0012,
      "step": 135720
    },
    {
      "epoch": 7.238933333333334,
      "grad_norm": 0.07568694651126862,
      "learning_rate": 4.756666666666667e-06,
      "loss": 0.0015,
      "step": 135730
    },
    {
      "epoch": 7.239466666666667,
      "grad_norm": 0.27335938811302185,
      "learning_rate": 4.753333333333333e-06,
      "loss": 0.0012,
      "step": 135740
    },
    {
      "epoch": 7.24,
      "grad_norm": 0.06744224578142166,
      "learning_rate": 4.75e-06,
      "loss": 0.002,
      "step": 135750
    },
    {
      "epoch": 7.2405333333333335,
      "grad_norm": 0.06126107648015022,
      "learning_rate": 4.746666666666666e-06,
      "loss": 0.0017,
      "step": 135760
    },
    {
      "epoch": 7.241066666666667,
      "grad_norm": 0.14973807334899902,
      "learning_rate": 4.743333333333333e-06,
      "loss": 0.0014,
      "step": 135770
    },
    {
      "epoch": 7.2416,
      "grad_norm": 0.027530070394277573,
      "learning_rate": 4.74e-06,
      "loss": 0.0017,
      "step": 135780
    },
    {
      "epoch": 7.242133333333333,
      "grad_norm": 0.3654947876930237,
      "learning_rate": 4.736666666666667e-06,
      "loss": 0.0023,
      "step": 135790
    },
    {
      "epoch": 7.242666666666667,
      "grad_norm": 0.3567057251930237,
      "learning_rate": 4.7333333333333335e-06,
      "loss": 0.0018,
      "step": 135800
    },
    {
      "epoch": 7.2432,
      "grad_norm": 0.06304673850536346,
      "learning_rate": 4.7300000000000005e-06,
      "loss": 0.0019,
      "step": 135810
    },
    {
      "epoch": 7.243733333333333,
      "grad_norm": 0.12877340614795685,
      "learning_rate": 4.726666666666667e-06,
      "loss": 0.0014,
      "step": 135820
    },
    {
      "epoch": 7.244266666666666,
      "grad_norm": 0.10452906787395477,
      "learning_rate": 4.7233333333333336e-06,
      "loss": 0.0014,
      "step": 135830
    },
    {
      "epoch": 7.2448,
      "grad_norm": 0.1666889786720276,
      "learning_rate": 4.72e-06,
      "loss": 0.0015,
      "step": 135840
    },
    {
      "epoch": 7.245333333333333,
      "grad_norm": 0.20689164102077484,
      "learning_rate": 4.7166666666666675e-06,
      "loss": 0.0021,
      "step": 135850
    },
    {
      "epoch": 7.245866666666666,
      "grad_norm": 0.0420323982834816,
      "learning_rate": 4.713333333333334e-06,
      "loss": 0.0015,
      "step": 135860
    },
    {
      "epoch": 7.2464,
      "grad_norm": 0.07093147188425064,
      "learning_rate": 4.710000000000001e-06,
      "loss": 0.0012,
      "step": 135870
    },
    {
      "epoch": 7.246933333333334,
      "grad_norm": 0.024920841678977013,
      "learning_rate": 4.706666666666667e-06,
      "loss": 0.0017,
      "step": 135880
    },
    {
      "epoch": 7.247466666666667,
      "grad_norm": 0.06241151690483093,
      "learning_rate": 4.703333333333334e-06,
      "loss": 0.0019,
      "step": 135890
    },
    {
      "epoch": 7.248,
      "grad_norm": 0.29443544149398804,
      "learning_rate": 4.7e-06,
      "loss": 0.0013,
      "step": 135900
    },
    {
      "epoch": 7.2485333333333335,
      "grad_norm": 0.11053702235221863,
      "learning_rate": 4.696666666666667e-06,
      "loss": 0.0018,
      "step": 135910
    },
    {
      "epoch": 7.249066666666667,
      "grad_norm": 0.1225886344909668,
      "learning_rate": 4.693333333333334e-06,
      "loss": 0.0023,
      "step": 135920
    },
    {
      "epoch": 7.2496,
      "grad_norm": 0.23586708307266235,
      "learning_rate": 4.69e-06,
      "loss": 0.0013,
      "step": 135930
    },
    {
      "epoch": 7.250133333333333,
      "grad_norm": 0.3315622806549072,
      "learning_rate": 4.686666666666667e-06,
      "loss": 0.0017,
      "step": 135940
    },
    {
      "epoch": 7.250666666666667,
      "grad_norm": 0.29764556884765625,
      "learning_rate": 4.683333333333333e-06,
      "loss": 0.0022,
      "step": 135950
    },
    {
      "epoch": 7.2512,
      "grad_norm": 0.0854685977101326,
      "learning_rate": 4.68e-06,
      "loss": 0.002,
      "step": 135960
    },
    {
      "epoch": 7.251733333333333,
      "grad_norm": 0.17972390353679657,
      "learning_rate": 4.676666666666667e-06,
      "loss": 0.0019,
      "step": 135970
    },
    {
      "epoch": 7.252266666666666,
      "grad_norm": 0.12540091574192047,
      "learning_rate": 4.673333333333334e-06,
      "loss": 0.0024,
      "step": 135980
    },
    {
      "epoch": 7.2528,
      "grad_norm": 0.08232364058494568,
      "learning_rate": 4.67e-06,
      "loss": 0.0019,
      "step": 135990
    },
    {
      "epoch": 7.253333333333333,
      "grad_norm": 0.06465011090040207,
      "learning_rate": 4.666666666666667e-06,
      "loss": 0.002,
      "step": 136000
    },
    {
      "epoch": 7.253866666666667,
      "grad_norm": 0.21400728821754456,
      "learning_rate": 4.663333333333333e-06,
      "loss": 0.0016,
      "step": 136010
    },
    {
      "epoch": 7.2544,
      "grad_norm": 0.12293675541877747,
      "learning_rate": 4.66e-06,
      "loss": 0.0016,
      "step": 136020
    },
    {
      "epoch": 7.254933333333334,
      "grad_norm": 0.10052325576543808,
      "learning_rate": 4.656666666666666e-06,
      "loss": 0.0015,
      "step": 136030
    },
    {
      "epoch": 7.255466666666667,
      "grad_norm": 0.12757371366024017,
      "learning_rate": 4.653333333333334e-06,
      "loss": 0.0014,
      "step": 136040
    },
    {
      "epoch": 7.256,
      "grad_norm": 0.20675092935562134,
      "learning_rate": 4.65e-06,
      "loss": 0.002,
      "step": 136050
    },
    {
      "epoch": 7.2565333333333335,
      "grad_norm": 0.3268304467201233,
      "learning_rate": 4.646666666666667e-06,
      "loss": 0.0016,
      "step": 136060
    },
    {
      "epoch": 7.257066666666667,
      "grad_norm": 0.2364369034767151,
      "learning_rate": 4.6433333333333335e-06,
      "loss": 0.0018,
      "step": 136070
    },
    {
      "epoch": 7.2576,
      "grad_norm": 0.3129628300666809,
      "learning_rate": 4.64e-06,
      "loss": 0.0017,
      "step": 136080
    },
    {
      "epoch": 7.258133333333333,
      "grad_norm": 0.23497042059898376,
      "learning_rate": 4.636666666666667e-06,
      "loss": 0.0015,
      "step": 136090
    },
    {
      "epoch": 7.258666666666667,
      "grad_norm": 0.2774655520915985,
      "learning_rate": 4.633333333333334e-06,
      "loss": 0.0016,
      "step": 136100
    },
    {
      "epoch": 7.2592,
      "grad_norm": 0.2699565291404724,
      "learning_rate": 4.6300000000000006e-06,
      "loss": 0.0013,
      "step": 136110
    },
    {
      "epoch": 7.259733333333333,
      "grad_norm": 0.08438601344823837,
      "learning_rate": 4.626666666666667e-06,
      "loss": 0.0015,
      "step": 136120
    },
    {
      "epoch": 7.260266666666666,
      "grad_norm": 0.5611459612846375,
      "learning_rate": 4.623333333333334e-06,
      "loss": 0.0021,
      "step": 136130
    },
    {
      "epoch": 7.2608,
      "grad_norm": 0.2697431743144989,
      "learning_rate": 4.62e-06,
      "loss": 0.002,
      "step": 136140
    },
    {
      "epoch": 7.261333333333333,
      "grad_norm": 0.04315514490008354,
      "learning_rate": 4.616666666666667e-06,
      "loss": 0.0016,
      "step": 136150
    },
    {
      "epoch": 7.261866666666666,
      "grad_norm": 0.12337463349103928,
      "learning_rate": 4.613333333333334e-06,
      "loss": 0.0023,
      "step": 136160
    },
    {
      "epoch": 7.2624,
      "grad_norm": 0.08330641686916351,
      "learning_rate": 4.610000000000001e-06,
      "loss": 0.0015,
      "step": 136170
    },
    {
      "epoch": 7.262933333333334,
      "grad_norm": 0.1819762885570526,
      "learning_rate": 4.606666666666667e-06,
      "loss": 0.0016,
      "step": 136180
    },
    {
      "epoch": 7.263466666666667,
      "grad_norm": 0.20870420336723328,
      "learning_rate": 4.603333333333334e-06,
      "loss": 0.0018,
      "step": 136190
    },
    {
      "epoch": 7.264,
      "grad_norm": 0.12444117665290833,
      "learning_rate": 4.6e-06,
      "loss": 0.0016,
      "step": 136200
    },
    {
      "epoch": 7.2645333333333335,
      "grad_norm": 0.19449995458126068,
      "learning_rate": 4.596666666666667e-06,
      "loss": 0.0018,
      "step": 136210
    },
    {
      "epoch": 7.265066666666667,
      "grad_norm": 0.09492899477481842,
      "learning_rate": 4.593333333333333e-06,
      "loss": 0.0016,
      "step": 136220
    },
    {
      "epoch": 7.2656,
      "grad_norm": 0.26544612646102905,
      "learning_rate": 4.590000000000001e-06,
      "loss": 0.0018,
      "step": 136230
    },
    {
      "epoch": 7.266133333333333,
      "grad_norm": 0.13174384832382202,
      "learning_rate": 4.586666666666667e-06,
      "loss": 0.0016,
      "step": 136240
    },
    {
      "epoch": 7.266666666666667,
      "grad_norm": 0.17807695269584656,
      "learning_rate": 4.583333333333333e-06,
      "loss": 0.002,
      "step": 136250
    },
    {
      "epoch": 7.2672,
      "grad_norm": 0.3340757489204407,
      "learning_rate": 4.58e-06,
      "loss": 0.0017,
      "step": 136260
    },
    {
      "epoch": 7.267733333333333,
      "grad_norm": 0.26999422907829285,
      "learning_rate": 4.576666666666666e-06,
      "loss": 0.0016,
      "step": 136270
    },
    {
      "epoch": 7.268266666666666,
      "grad_norm": 0.10127835720777512,
      "learning_rate": 4.573333333333333e-06,
      "loss": 0.0016,
      "step": 136280
    },
    {
      "epoch": 7.2688,
      "grad_norm": 0.19984939694404602,
      "learning_rate": 4.57e-06,
      "loss": 0.0015,
      "step": 136290
    },
    {
      "epoch": 7.269333333333333,
      "grad_norm": 0.0404474139213562,
      "learning_rate": 4.566666666666667e-06,
      "loss": 0.0014,
      "step": 136300
    },
    {
      "epoch": 7.269866666666666,
      "grad_norm": 0.06516501307487488,
      "learning_rate": 4.563333333333333e-06,
      "loss": 0.0018,
      "step": 136310
    },
    {
      "epoch": 7.2704,
      "grad_norm": 0.533635139465332,
      "learning_rate": 4.56e-06,
      "loss": 0.0024,
      "step": 136320
    },
    {
      "epoch": 7.270933333333334,
      "grad_norm": 0.3453828692436218,
      "learning_rate": 4.5566666666666665e-06,
      "loss": 0.0016,
      "step": 136330
    },
    {
      "epoch": 7.271466666666667,
      "grad_norm": 0.478514701128006,
      "learning_rate": 4.5533333333333335e-06,
      "loss": 0.0014,
      "step": 136340
    },
    {
      "epoch": 7.272,
      "grad_norm": 0.4377203583717346,
      "learning_rate": 4.5500000000000005e-06,
      "loss": 0.0017,
      "step": 136350
    },
    {
      "epoch": 7.2725333333333335,
      "grad_norm": 0.06114247813820839,
      "learning_rate": 4.5466666666666675e-06,
      "loss": 0.0019,
      "step": 136360
    },
    {
      "epoch": 7.273066666666667,
      "grad_norm": 0.3200131058692932,
      "learning_rate": 4.543333333333334e-06,
      "loss": 0.002,
      "step": 136370
    },
    {
      "epoch": 7.2736,
      "grad_norm": 0.26980218291282654,
      "learning_rate": 4.540000000000001e-06,
      "loss": 0.0021,
      "step": 136380
    },
    {
      "epoch": 7.274133333333333,
      "grad_norm": 0.05547217279672623,
      "learning_rate": 4.536666666666667e-06,
      "loss": 0.0022,
      "step": 136390
    },
    {
      "epoch": 7.274666666666667,
      "grad_norm": 0.32885369658470154,
      "learning_rate": 4.533333333333334e-06,
      "loss": 0.0013,
      "step": 136400
    },
    {
      "epoch": 7.2752,
      "grad_norm": 0.2708074450492859,
      "learning_rate": 4.53e-06,
      "loss": 0.0021,
      "step": 136410
    },
    {
      "epoch": 7.275733333333333,
      "grad_norm": 0.29303815960884094,
      "learning_rate": 4.526666666666667e-06,
      "loss": 0.0019,
      "step": 136420
    },
    {
      "epoch": 7.276266666666666,
      "grad_norm": 0.06894735991954803,
      "learning_rate": 4.523333333333334e-06,
      "loss": 0.0014,
      "step": 136430
    },
    {
      "epoch": 7.2768,
      "grad_norm": 0.1480007767677307,
      "learning_rate": 4.52e-06,
      "loss": 0.0014,
      "step": 136440
    },
    {
      "epoch": 7.277333333333333,
      "grad_norm": 0.0699339434504509,
      "learning_rate": 4.516666666666667e-06,
      "loss": 0.0018,
      "step": 136450
    },
    {
      "epoch": 7.277866666666666,
      "grad_norm": 0.21877875924110413,
      "learning_rate": 4.513333333333333e-06,
      "loss": 0.0019,
      "step": 136460
    },
    {
      "epoch": 7.2783999999999995,
      "grad_norm": 0.07322680205106735,
      "learning_rate": 4.51e-06,
      "loss": 0.0027,
      "step": 136470
    },
    {
      "epoch": 7.278933333333334,
      "grad_norm": 0.12373615056276321,
      "learning_rate": 4.506666666666667e-06,
      "loss": 0.0015,
      "step": 136480
    },
    {
      "epoch": 7.279466666666667,
      "grad_norm": 0.12079543620347977,
      "learning_rate": 4.503333333333334e-06,
      "loss": 0.0011,
      "step": 136490
    },
    {
      "epoch": 7.28,
      "grad_norm": 0.2678130865097046,
      "learning_rate": 4.5e-06,
      "loss": 0.0018,
      "step": 136500
    },
    {
      "epoch": 7.2805333333333335,
      "grad_norm": 0.048341505229473114,
      "learning_rate": 4.496666666666667e-06,
      "loss": 0.0019,
      "step": 136510
    },
    {
      "epoch": 7.281066666666667,
      "grad_norm": 0.349087655544281,
      "learning_rate": 4.493333333333333e-06,
      "loss": 0.0014,
      "step": 136520
    },
    {
      "epoch": 7.2816,
      "grad_norm": 0.05791131407022476,
      "learning_rate": 4.49e-06,
      "loss": 0.0019,
      "step": 136530
    },
    {
      "epoch": 7.282133333333333,
      "grad_norm": 0.1552789807319641,
      "learning_rate": 4.486666666666667e-06,
      "loss": 0.0019,
      "step": 136540
    },
    {
      "epoch": 7.282666666666667,
      "grad_norm": 0.1773259937763214,
      "learning_rate": 4.483333333333334e-06,
      "loss": 0.0014,
      "step": 136550
    },
    {
      "epoch": 7.2832,
      "grad_norm": 0.23491200804710388,
      "learning_rate": 4.48e-06,
      "loss": 0.0026,
      "step": 136560
    },
    {
      "epoch": 7.283733333333333,
      "grad_norm": 0.07391161471605301,
      "learning_rate": 4.476666666666667e-06,
      "loss": 0.0012,
      "step": 136570
    },
    {
      "epoch": 7.2842666666666664,
      "grad_norm": 0.2914124131202698,
      "learning_rate": 4.473333333333333e-06,
      "loss": 0.0013,
      "step": 136580
    },
    {
      "epoch": 7.2848,
      "grad_norm": 0.09011808782815933,
      "learning_rate": 4.4699999999999996e-06,
      "loss": 0.0014,
      "step": 136590
    },
    {
      "epoch": 7.285333333333333,
      "grad_norm": 0.05724748224020004,
      "learning_rate": 4.4666666666666665e-06,
      "loss": 0.0014,
      "step": 136600
    },
    {
      "epoch": 7.285866666666666,
      "grad_norm": 0.43506014347076416,
      "learning_rate": 4.4633333333333335e-06,
      "loss": 0.0019,
      "step": 136610
    },
    {
      "epoch": 7.2864,
      "grad_norm": 0.1215711310505867,
      "learning_rate": 4.4600000000000005e-06,
      "loss": 0.0021,
      "step": 136620
    },
    {
      "epoch": 7.286933333333334,
      "grad_norm": 0.12774774432182312,
      "learning_rate": 4.456666666666667e-06,
      "loss": 0.0015,
      "step": 136630
    },
    {
      "epoch": 7.287466666666667,
      "grad_norm": 0.28957799077033997,
      "learning_rate": 4.453333333333334e-06,
      "loss": 0.0012,
      "step": 136640
    },
    {
      "epoch": 7.288,
      "grad_norm": 0.15427787601947784,
      "learning_rate": 4.45e-06,
      "loss": 0.0027,
      "step": 136650
    },
    {
      "epoch": 7.2885333333333335,
      "grad_norm": 0.053453799337148666,
      "learning_rate": 4.446666666666667e-06,
      "loss": 0.0018,
      "step": 136660
    },
    {
      "epoch": 7.289066666666667,
      "grad_norm": 0.28350457549095154,
      "learning_rate": 4.443333333333334e-06,
      "loss": 0.0017,
      "step": 136670
    },
    {
      "epoch": 7.2896,
      "grad_norm": 0.09151830524206161,
      "learning_rate": 4.440000000000001e-06,
      "loss": 0.0018,
      "step": 136680
    },
    {
      "epoch": 7.290133333333333,
      "grad_norm": 0.12455857545137405,
      "learning_rate": 4.436666666666667e-06,
      "loss": 0.0019,
      "step": 136690
    },
    {
      "epoch": 7.290666666666667,
      "grad_norm": 0.47405460476875305,
      "learning_rate": 4.433333333333334e-06,
      "loss": 0.0021,
      "step": 136700
    },
    {
      "epoch": 7.2912,
      "grad_norm": 0.044828545302152634,
      "learning_rate": 4.43e-06,
      "loss": 0.0016,
      "step": 136710
    },
    {
      "epoch": 7.291733333333333,
      "grad_norm": 0.12284500151872635,
      "learning_rate": 4.426666666666667e-06,
      "loss": 0.0025,
      "step": 136720
    },
    {
      "epoch": 7.2922666666666665,
      "grad_norm": 0.20150870084762573,
      "learning_rate": 4.423333333333334e-06,
      "loss": 0.0015,
      "step": 136730
    },
    {
      "epoch": 7.2928,
      "grad_norm": 0.10559704899787903,
      "learning_rate": 4.420000000000001e-06,
      "loss": 0.0025,
      "step": 136740
    },
    {
      "epoch": 7.293333333333333,
      "grad_norm": 0.06445228308439255,
      "learning_rate": 4.416666666666667e-06,
      "loss": 0.0011,
      "step": 136750
    },
    {
      "epoch": 7.293866666666666,
      "grad_norm": 0.23373030126094818,
      "learning_rate": 4.413333333333333e-06,
      "loss": 0.0017,
      "step": 136760
    },
    {
      "epoch": 7.2943999999999996,
      "grad_norm": 0.05431804060935974,
      "learning_rate": 4.41e-06,
      "loss": 0.0012,
      "step": 136770
    },
    {
      "epoch": 7.294933333333334,
      "grad_norm": 0.0458657331764698,
      "learning_rate": 4.406666666666666e-06,
      "loss": 0.0018,
      "step": 136780
    },
    {
      "epoch": 7.295466666666667,
      "grad_norm": 0.2790472209453583,
      "learning_rate": 4.403333333333333e-06,
      "loss": 0.0016,
      "step": 136790
    },
    {
      "epoch": 7.296,
      "grad_norm": 0.3476521670818329,
      "learning_rate": 4.4e-06,
      "loss": 0.0015,
      "step": 136800
    },
    {
      "epoch": 7.2965333333333335,
      "grad_norm": 0.24894344806671143,
      "learning_rate": 4.396666666666667e-06,
      "loss": 0.0017,
      "step": 136810
    },
    {
      "epoch": 7.297066666666667,
      "grad_norm": 0.14731000363826752,
      "learning_rate": 4.393333333333333e-06,
      "loss": 0.0017,
      "step": 136820
    },
    {
      "epoch": 7.2976,
      "grad_norm": 0.17354823648929596,
      "learning_rate": 4.39e-06,
      "loss": 0.0017,
      "step": 136830
    },
    {
      "epoch": 7.298133333333333,
      "grad_norm": 0.1551555097103119,
      "learning_rate": 4.3866666666666665e-06,
      "loss": 0.0019,
      "step": 136840
    },
    {
      "epoch": 7.298666666666667,
      "grad_norm": 0.029324641451239586,
      "learning_rate": 4.3833333333333334e-06,
      "loss": 0.0019,
      "step": 136850
    },
    {
      "epoch": 7.2992,
      "grad_norm": 0.15314136445522308,
      "learning_rate": 4.38e-06,
      "loss": 0.0015,
      "step": 136860
    },
    {
      "epoch": 7.299733333333333,
      "grad_norm": 0.12952125072479248,
      "learning_rate": 4.376666666666667e-06,
      "loss": 0.0022,
      "step": 136870
    },
    {
      "epoch": 7.3002666666666665,
      "grad_norm": 0.07165122032165527,
      "learning_rate": 4.3733333333333335e-06,
      "loss": 0.0015,
      "step": 136880
    },
    {
      "epoch": 7.3008,
      "grad_norm": 0.11877121776342392,
      "learning_rate": 4.3700000000000005e-06,
      "loss": 0.002,
      "step": 136890
    },
    {
      "epoch": 7.301333333333333,
      "grad_norm": 0.2395969033241272,
      "learning_rate": 4.366666666666667e-06,
      "loss": 0.0014,
      "step": 136900
    },
    {
      "epoch": 7.301866666666666,
      "grad_norm": 0.33109205961227417,
      "learning_rate": 4.363333333333334e-06,
      "loss": 0.0014,
      "step": 136910
    },
    {
      "epoch": 7.3024000000000004,
      "grad_norm": 0.1835886836051941,
      "learning_rate": 4.360000000000001e-06,
      "loss": 0.0025,
      "step": 136920
    },
    {
      "epoch": 7.302933333333334,
      "grad_norm": 0.04894721880555153,
      "learning_rate": 4.356666666666667e-06,
      "loss": 0.0014,
      "step": 136930
    },
    {
      "epoch": 7.303466666666667,
      "grad_norm": 0.35972142219543457,
      "learning_rate": 4.353333333333334e-06,
      "loss": 0.0019,
      "step": 136940
    },
    {
      "epoch": 7.304,
      "grad_norm": 0.06193854659795761,
      "learning_rate": 4.35e-06,
      "loss": 0.0013,
      "step": 136950
    },
    {
      "epoch": 7.3045333333333335,
      "grad_norm": 0.09737703204154968,
      "learning_rate": 4.346666666666667e-06,
      "loss": 0.0021,
      "step": 136960
    },
    {
      "epoch": 7.305066666666667,
      "grad_norm": 0.260092169046402,
      "learning_rate": 4.343333333333333e-06,
      "loss": 0.0025,
      "step": 136970
    },
    {
      "epoch": 7.3056,
      "grad_norm": 0.17455534636974335,
      "learning_rate": 4.34e-06,
      "loss": 0.0015,
      "step": 136980
    },
    {
      "epoch": 7.306133333333333,
      "grad_norm": 0.17132429778575897,
      "learning_rate": 4.336666666666667e-06,
      "loss": 0.0027,
      "step": 136990
    },
    {
      "epoch": 7.306666666666667,
      "grad_norm": 0.20300571620464325,
      "learning_rate": 4.333333333333334e-06,
      "loss": 0.0025,
      "step": 137000
    },
    {
      "epoch": 7.3072,
      "grad_norm": 0.23759298026561737,
      "learning_rate": 4.33e-06,
      "loss": 0.0023,
      "step": 137010
    },
    {
      "epoch": 7.307733333333333,
      "grad_norm": 0.2519400119781494,
      "learning_rate": 4.326666666666667e-06,
      "loss": 0.0019,
      "step": 137020
    },
    {
      "epoch": 7.3082666666666665,
      "grad_norm": 0.05115627124905586,
      "learning_rate": 4.323333333333333e-06,
      "loss": 0.0025,
      "step": 137030
    },
    {
      "epoch": 7.3088,
      "grad_norm": 0.047852106392383575,
      "learning_rate": 4.32e-06,
      "loss": 0.0012,
      "step": 137040
    },
    {
      "epoch": 7.309333333333333,
      "grad_norm": 0.17687849700450897,
      "learning_rate": 4.316666666666667e-06,
      "loss": 0.0019,
      "step": 137050
    },
    {
      "epoch": 7.309866666666666,
      "grad_norm": 0.0950218141078949,
      "learning_rate": 4.313333333333334e-06,
      "loss": 0.0017,
      "step": 137060
    },
    {
      "epoch": 7.3104,
      "grad_norm": 0.14926178753376007,
      "learning_rate": 4.31e-06,
      "loss": 0.0016,
      "step": 137070
    },
    {
      "epoch": 7.310933333333334,
      "grad_norm": 0.3258070647716522,
      "learning_rate": 4.306666666666667e-06,
      "loss": 0.0013,
      "step": 137080
    },
    {
      "epoch": 7.311466666666667,
      "grad_norm": 0.19914795458316803,
      "learning_rate": 4.303333333333333e-06,
      "loss": 0.0015,
      "step": 137090
    },
    {
      "epoch": 7.312,
      "grad_norm": 0.08925216645002365,
      "learning_rate": 4.2999999999999995e-06,
      "loss": 0.0018,
      "step": 137100
    },
    {
      "epoch": 7.3125333333333336,
      "grad_norm": 0.18175008893013,
      "learning_rate": 4.296666666666667e-06,
      "loss": 0.0018,
      "step": 137110
    },
    {
      "epoch": 7.313066666666667,
      "grad_norm": 0.34010517597198486,
      "learning_rate": 4.2933333333333334e-06,
      "loss": 0.0017,
      "step": 137120
    },
    {
      "epoch": 7.3136,
      "grad_norm": 0.13139459490776062,
      "learning_rate": 4.2900000000000004e-06,
      "loss": 0.0017,
      "step": 137130
    },
    {
      "epoch": 7.314133333333333,
      "grad_norm": 0.3820539116859436,
      "learning_rate": 4.2866666666666666e-06,
      "loss": 0.0023,
      "step": 137140
    },
    {
      "epoch": 7.314666666666667,
      "grad_norm": 0.4108353555202484,
      "learning_rate": 4.2833333333333335e-06,
      "loss": 0.0016,
      "step": 137150
    },
    {
      "epoch": 7.3152,
      "grad_norm": 0.06628347188234329,
      "learning_rate": 4.28e-06,
      "loss": 0.0024,
      "step": 137160
    },
    {
      "epoch": 7.315733333333333,
      "grad_norm": 0.05657769739627838,
      "learning_rate": 4.276666666666667e-06,
      "loss": 0.0014,
      "step": 137170
    },
    {
      "epoch": 7.3162666666666665,
      "grad_norm": 0.0715666338801384,
      "learning_rate": 4.273333333333334e-06,
      "loss": 0.0023,
      "step": 137180
    },
    {
      "epoch": 7.3168,
      "grad_norm": 0.1749076247215271,
      "learning_rate": 4.270000000000001e-06,
      "loss": 0.002,
      "step": 137190
    },
    {
      "epoch": 7.317333333333333,
      "grad_norm": 0.0962504893541336,
      "learning_rate": 4.266666666666667e-06,
      "loss": 0.0016,
      "step": 137200
    },
    {
      "epoch": 7.317866666666666,
      "grad_norm": 0.2689460515975952,
      "learning_rate": 4.263333333333334e-06,
      "loss": 0.002,
      "step": 137210
    },
    {
      "epoch": 7.3184000000000005,
      "grad_norm": 0.2661586105823517,
      "learning_rate": 4.26e-06,
      "loss": 0.0023,
      "step": 137220
    },
    {
      "epoch": 7.318933333333334,
      "grad_norm": 0.325684517621994,
      "learning_rate": 4.256666666666667e-06,
      "loss": 0.0019,
      "step": 137230
    },
    {
      "epoch": 7.319466666666667,
      "grad_norm": 0.04762966185808182,
      "learning_rate": 4.253333333333334e-06,
      "loss": 0.002,
      "step": 137240
    },
    {
      "epoch": 7.32,
      "grad_norm": 0.07282111048698425,
      "learning_rate": 4.250000000000001e-06,
      "loss": 0.0023,
      "step": 137250
    },
    {
      "epoch": 7.320533333333334,
      "grad_norm": 0.12768849730491638,
      "learning_rate": 4.246666666666667e-06,
      "loss": 0.0014,
      "step": 137260
    },
    {
      "epoch": 7.321066666666667,
      "grad_norm": 0.27787867188453674,
      "learning_rate": 4.243333333333334e-06,
      "loss": 0.0013,
      "step": 137270
    },
    {
      "epoch": 7.3216,
      "grad_norm": 0.5311899185180664,
      "learning_rate": 4.24e-06,
      "loss": 0.0017,
      "step": 137280
    },
    {
      "epoch": 7.322133333333333,
      "grad_norm": 0.05452527478337288,
      "learning_rate": 4.236666666666666e-06,
      "loss": 0.0017,
      "step": 137290
    },
    {
      "epoch": 7.322666666666667,
      "grad_norm": 0.07102757692337036,
      "learning_rate": 4.233333333333333e-06,
      "loss": 0.0019,
      "step": 137300
    },
    {
      "epoch": 7.3232,
      "grad_norm": 0.045512713491916656,
      "learning_rate": 4.23e-06,
      "loss": 0.0016,
      "step": 137310
    },
    {
      "epoch": 7.323733333333333,
      "grad_norm": 0.12226574122905731,
      "learning_rate": 4.226666666666667e-06,
      "loss": 0.0021,
      "step": 137320
    },
    {
      "epoch": 7.3242666666666665,
      "grad_norm": 0.23852816224098206,
      "learning_rate": 4.223333333333333e-06,
      "loss": 0.0022,
      "step": 137330
    },
    {
      "epoch": 7.3248,
      "grad_norm": 0.4419763684272766,
      "learning_rate": 4.22e-06,
      "loss": 0.0015,
      "step": 137340
    },
    {
      "epoch": 7.325333333333333,
      "grad_norm": 0.11863002181053162,
      "learning_rate": 4.216666666666666e-06,
      "loss": 0.0013,
      "step": 137350
    },
    {
      "epoch": 7.325866666666666,
      "grad_norm": 0.04882440343499184,
      "learning_rate": 4.213333333333333e-06,
      "loss": 0.0023,
      "step": 137360
    },
    {
      "epoch": 7.3264,
      "grad_norm": 0.07890095561742783,
      "learning_rate": 4.21e-06,
      "loss": 0.0018,
      "step": 137370
    },
    {
      "epoch": 7.326933333333334,
      "grad_norm": 0.44846582412719727,
      "learning_rate": 4.206666666666667e-06,
      "loss": 0.0012,
      "step": 137380
    },
    {
      "epoch": 7.327466666666667,
      "grad_norm": 0.36616644263267517,
      "learning_rate": 4.2033333333333335e-06,
      "loss": 0.0013,
      "step": 137390
    },
    {
      "epoch": 7.328,
      "grad_norm": 0.12658075988292694,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 0.0013,
      "step": 137400
    },
    {
      "epoch": 7.328533333333334,
      "grad_norm": 0.15521852672100067,
      "learning_rate": 4.196666666666667e-06,
      "loss": 0.0016,
      "step": 137410
    },
    {
      "epoch": 7.329066666666667,
      "grad_norm": 0.15373800694942474,
      "learning_rate": 4.1933333333333336e-06,
      "loss": 0.0018,
      "step": 137420
    },
    {
      "epoch": 7.3296,
      "grad_norm": 0.03795650973916054,
      "learning_rate": 4.1900000000000005e-06,
      "loss": 0.0019,
      "step": 137430
    },
    {
      "epoch": 7.330133333333333,
      "grad_norm": 0.3298248052597046,
      "learning_rate": 4.1866666666666675e-06,
      "loss": 0.0015,
      "step": 137440
    },
    {
      "epoch": 7.330666666666667,
      "grad_norm": 0.07022407650947571,
      "learning_rate": 4.183333333333334e-06,
      "loss": 0.0016,
      "step": 137450
    },
    {
      "epoch": 7.3312,
      "grad_norm": 0.05337683483958244,
      "learning_rate": 4.18e-06,
      "loss": 0.0019,
      "step": 137460
    },
    {
      "epoch": 7.331733333333333,
      "grad_norm": 0.21896156668663025,
      "learning_rate": 4.176666666666667e-06,
      "loss": 0.0015,
      "step": 137470
    },
    {
      "epoch": 7.3322666666666665,
      "grad_norm": 0.31199777126312256,
      "learning_rate": 4.173333333333333e-06,
      "loss": 0.0023,
      "step": 137480
    },
    {
      "epoch": 7.3328,
      "grad_norm": 0.17945903539657593,
      "learning_rate": 4.17e-06,
      "loss": 0.0016,
      "step": 137490
    },
    {
      "epoch": 7.333333333333333,
      "grad_norm": 0.04663959890604019,
      "learning_rate": 4.166666666666667e-06,
      "loss": 0.0016,
      "step": 137500
    },
    {
      "epoch": 7.333866666666666,
      "grad_norm": 0.1207473874092102,
      "learning_rate": 4.163333333333334e-06,
      "loss": 0.0021,
      "step": 137510
    },
    {
      "epoch": 7.3344,
      "grad_norm": 0.0907215029001236,
      "learning_rate": 4.16e-06,
      "loss": 0.0017,
      "step": 137520
    },
    {
      "epoch": 7.334933333333334,
      "grad_norm": 0.05893917754292488,
      "learning_rate": 4.156666666666667e-06,
      "loss": 0.0014,
      "step": 137530
    },
    {
      "epoch": 7.335466666666667,
      "grad_norm": 0.22428618371486664,
      "learning_rate": 4.153333333333333e-06,
      "loss": 0.0018,
      "step": 137540
    },
    {
      "epoch": 7.336,
      "grad_norm": 0.26582804322242737,
      "learning_rate": 4.15e-06,
      "loss": 0.0021,
      "step": 137550
    },
    {
      "epoch": 7.336533333333334,
      "grad_norm": 0.08711061626672745,
      "learning_rate": 4.146666666666667e-06,
      "loss": 0.0013,
      "step": 137560
    },
    {
      "epoch": 7.337066666666667,
      "grad_norm": 0.25890204310417175,
      "learning_rate": 4.143333333333334e-06,
      "loss": 0.0015,
      "step": 137570
    },
    {
      "epoch": 7.3376,
      "grad_norm": 0.2518005967140198,
      "learning_rate": 4.14e-06,
      "loss": 0.0027,
      "step": 137580
    },
    {
      "epoch": 7.338133333333333,
      "grad_norm": 0.21132804453372955,
      "learning_rate": 4.136666666666667e-06,
      "loss": 0.0016,
      "step": 137590
    },
    {
      "epoch": 7.338666666666667,
      "grad_norm": 0.20949752628803253,
      "learning_rate": 4.133333333333333e-06,
      "loss": 0.0015,
      "step": 137600
    },
    {
      "epoch": 7.3392,
      "grad_norm": 0.2758065164089203,
      "learning_rate": 4.13e-06,
      "loss": 0.0029,
      "step": 137610
    },
    {
      "epoch": 7.339733333333333,
      "grad_norm": 0.08133354783058167,
      "learning_rate": 4.126666666666667e-06,
      "loss": 0.0012,
      "step": 137620
    },
    {
      "epoch": 7.3402666666666665,
      "grad_norm": 0.048530805855989456,
      "learning_rate": 4.123333333333333e-06,
      "loss": 0.0014,
      "step": 137630
    },
    {
      "epoch": 7.3408,
      "grad_norm": 0.32179588079452515,
      "learning_rate": 4.12e-06,
      "loss": 0.0022,
      "step": 137640
    },
    {
      "epoch": 7.341333333333333,
      "grad_norm": 0.17621445655822754,
      "learning_rate": 4.1166666666666665e-06,
      "loss": 0.0015,
      "step": 137650
    },
    {
      "epoch": 7.341866666666666,
      "grad_norm": 0.380919486284256,
      "learning_rate": 4.1133333333333335e-06,
      "loss": 0.0013,
      "step": 137660
    },
    {
      "epoch": 7.3424,
      "grad_norm": 0.29132080078125,
      "learning_rate": 4.11e-06,
      "loss": 0.0022,
      "step": 137670
    },
    {
      "epoch": 7.342933333333333,
      "grad_norm": 0.29125508666038513,
      "learning_rate": 4.106666666666667e-06,
      "loss": 0.0028,
      "step": 137680
    },
    {
      "epoch": 7.343466666666667,
      "grad_norm": 0.2658509910106659,
      "learning_rate": 4.1033333333333336e-06,
      "loss": 0.0027,
      "step": 137690
    },
    {
      "epoch": 7.344,
      "grad_norm": 0.1458497792482376,
      "learning_rate": 4.1000000000000006e-06,
      "loss": 0.002,
      "step": 137700
    },
    {
      "epoch": 7.344533333333334,
      "grad_norm": 0.26960986852645874,
      "learning_rate": 4.096666666666667e-06,
      "loss": 0.0022,
      "step": 137710
    },
    {
      "epoch": 7.345066666666667,
      "grad_norm": 0.38799214363098145,
      "learning_rate": 4.093333333333334e-06,
      "loss": 0.0015,
      "step": 137720
    },
    {
      "epoch": 7.3456,
      "grad_norm": 0.04114554077386856,
      "learning_rate": 4.09e-06,
      "loss": 0.0016,
      "step": 137730
    },
    {
      "epoch": 7.346133333333333,
      "grad_norm": 0.33273905515670776,
      "learning_rate": 4.086666666666667e-06,
      "loss": 0.0026,
      "step": 137740
    },
    {
      "epoch": 7.346666666666667,
      "grad_norm": 0.12916944921016693,
      "learning_rate": 4.083333333333334e-06,
      "loss": 0.0016,
      "step": 137750
    },
    {
      "epoch": 7.3472,
      "grad_norm": 0.0483548603951931,
      "learning_rate": 4.080000000000001e-06,
      "loss": 0.0019,
      "step": 137760
    },
    {
      "epoch": 7.347733333333333,
      "grad_norm": 0.11968062072992325,
      "learning_rate": 4.076666666666667e-06,
      "loss": 0.0019,
      "step": 137770
    },
    {
      "epoch": 7.3482666666666665,
      "grad_norm": 0.08898446708917618,
      "learning_rate": 4.073333333333334e-06,
      "loss": 0.0017,
      "step": 137780
    },
    {
      "epoch": 7.3488,
      "grad_norm": 0.07100358605384827,
      "learning_rate": 4.07e-06,
      "loss": 0.0024,
      "step": 137790
    },
    {
      "epoch": 7.349333333333333,
      "grad_norm": 0.048035163432359695,
      "learning_rate": 4.066666666666666e-06,
      "loss": 0.0018,
      "step": 137800
    },
    {
      "epoch": 7.349866666666666,
      "grad_norm": 0.18255697190761566,
      "learning_rate": 4.063333333333334e-06,
      "loss": 0.0018,
      "step": 137810
    },
    {
      "epoch": 7.3504,
      "grad_norm": 0.052469778805971146,
      "learning_rate": 4.06e-06,
      "loss": 0.0017,
      "step": 137820
    },
    {
      "epoch": 7.350933333333334,
      "grad_norm": 0.04842463135719299,
      "learning_rate": 4.056666666666667e-06,
      "loss": 0.0018,
      "step": 137830
    },
    {
      "epoch": 7.351466666666667,
      "grad_norm": 0.05246942862868309,
      "learning_rate": 4.053333333333333e-06,
      "loss": 0.0014,
      "step": 137840
    },
    {
      "epoch": 7.352,
      "grad_norm": 0.21300870180130005,
      "learning_rate": 4.05e-06,
      "loss": 0.0017,
      "step": 137850
    },
    {
      "epoch": 7.352533333333334,
      "grad_norm": 0.07595577090978622,
      "learning_rate": 4.046666666666666e-06,
      "loss": 0.0015,
      "step": 137860
    },
    {
      "epoch": 7.353066666666667,
      "grad_norm": 0.25589147210121155,
      "learning_rate": 4.043333333333333e-06,
      "loss": 0.0019,
      "step": 137870
    },
    {
      "epoch": 7.3536,
      "grad_norm": 0.4391859173774719,
      "learning_rate": 4.04e-06,
      "loss": 0.0017,
      "step": 137880
    },
    {
      "epoch": 7.354133333333333,
      "grad_norm": 0.24348244071006775,
      "learning_rate": 4.036666666666667e-06,
      "loss": 0.0028,
      "step": 137890
    },
    {
      "epoch": 7.354666666666667,
      "grad_norm": 0.41249793767929077,
      "learning_rate": 4.033333333333333e-06,
      "loss": 0.0018,
      "step": 137900
    },
    {
      "epoch": 7.3552,
      "grad_norm": 0.39617660641670227,
      "learning_rate": 4.03e-06,
      "loss": 0.0019,
      "step": 137910
    },
    {
      "epoch": 7.355733333333333,
      "grad_norm": 0.04527126997709274,
      "learning_rate": 4.0266666666666665e-06,
      "loss": 0.0025,
      "step": 137920
    },
    {
      "epoch": 7.3562666666666665,
      "grad_norm": 0.052986323833465576,
      "learning_rate": 4.0233333333333335e-06,
      "loss": 0.0015,
      "step": 137930
    },
    {
      "epoch": 7.3568,
      "grad_norm": 0.36957916617393494,
      "learning_rate": 4.0200000000000005e-06,
      "loss": 0.0019,
      "step": 137940
    },
    {
      "epoch": 7.357333333333333,
      "grad_norm": 0.6214361786842346,
      "learning_rate": 4.0166666666666675e-06,
      "loss": 0.0026,
      "step": 137950
    },
    {
      "epoch": 7.357866666666666,
      "grad_norm": 0.09984686225652695,
      "learning_rate": 4.013333333333334e-06,
      "loss": 0.0015,
      "step": 137960
    },
    {
      "epoch": 7.3584,
      "grad_norm": 0.2644233703613281,
      "learning_rate": 4.01e-06,
      "loss": 0.0018,
      "step": 137970
    },
    {
      "epoch": 7.358933333333333,
      "grad_norm": 0.24476824700832367,
      "learning_rate": 4.006666666666667e-06,
      "loss": 0.0019,
      "step": 137980
    },
    {
      "epoch": 7.359466666666667,
      "grad_norm": 0.23266352713108063,
      "learning_rate": 4.003333333333333e-06,
      "loss": 0.0014,
      "step": 137990
    },
    {
      "epoch": 7.36,
      "grad_norm": 0.2918425500392914,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.0025,
      "step": 138000
    },
    {
      "epoch": 7.360533333333334,
      "grad_norm": 0.21609622240066528,
      "learning_rate": 3.996666666666667e-06,
      "loss": 0.0024,
      "step": 138010
    },
    {
      "epoch": 7.361066666666667,
      "grad_norm": 0.380076140165329,
      "learning_rate": 3.993333333333334e-06,
      "loss": 0.0018,
      "step": 138020
    },
    {
      "epoch": 7.3616,
      "grad_norm": 0.4056524336338043,
      "learning_rate": 3.99e-06,
      "loss": 0.0019,
      "step": 138030
    },
    {
      "epoch": 7.362133333333333,
      "grad_norm": 0.11832781881093979,
      "learning_rate": 3.986666666666667e-06,
      "loss": 0.0025,
      "step": 138040
    },
    {
      "epoch": 7.362666666666667,
      "grad_norm": 0.12460130453109741,
      "learning_rate": 3.983333333333333e-06,
      "loss": 0.0016,
      "step": 138050
    },
    {
      "epoch": 7.3632,
      "grad_norm": 0.3674527406692505,
      "learning_rate": 3.98e-06,
      "loss": 0.0018,
      "step": 138060
    },
    {
      "epoch": 7.363733333333333,
      "grad_norm": 0.04509081691503525,
      "learning_rate": 3.976666666666667e-06,
      "loss": 0.0012,
      "step": 138070
    },
    {
      "epoch": 7.3642666666666665,
      "grad_norm": 0.20332877337932587,
      "learning_rate": 3.973333333333334e-06,
      "loss": 0.0025,
      "step": 138080
    },
    {
      "epoch": 7.3648,
      "grad_norm": 0.18546058237552643,
      "learning_rate": 3.97e-06,
      "loss": 0.0013,
      "step": 138090
    },
    {
      "epoch": 7.365333333333333,
      "grad_norm": 0.04287965968251228,
      "learning_rate": 3.966666666666667e-06,
      "loss": 0.0033,
      "step": 138100
    },
    {
      "epoch": 7.365866666666666,
      "grad_norm": 0.06765282154083252,
      "learning_rate": 3.963333333333333e-06,
      "loss": 0.0017,
      "step": 138110
    },
    {
      "epoch": 7.3664,
      "grad_norm": 0.06771186739206314,
      "learning_rate": 3.96e-06,
      "loss": 0.0014,
      "step": 138120
    },
    {
      "epoch": 7.366933333333334,
      "grad_norm": 0.35219642519950867,
      "learning_rate": 3.956666666666667e-06,
      "loss": 0.0022,
      "step": 138130
    },
    {
      "epoch": 7.367466666666667,
      "grad_norm": 0.04762406647205353,
      "learning_rate": 3.953333333333333e-06,
      "loss": 0.0012,
      "step": 138140
    },
    {
      "epoch": 7.368,
      "grad_norm": 0.32138681411743164,
      "learning_rate": 3.95e-06,
      "loss": 0.0013,
      "step": 138150
    },
    {
      "epoch": 7.368533333333334,
      "grad_norm": 0.2943112254142761,
      "learning_rate": 3.9466666666666664e-06,
      "loss": 0.0019,
      "step": 138160
    },
    {
      "epoch": 7.369066666666667,
      "grad_norm": 0.23492459952831268,
      "learning_rate": 3.943333333333333e-06,
      "loss": 0.0015,
      "step": 138170
    },
    {
      "epoch": 7.3696,
      "grad_norm": 0.15062938630580902,
      "learning_rate": 3.9399999999999995e-06,
      "loss": 0.0013,
      "step": 138180
    },
    {
      "epoch": 7.370133333333333,
      "grad_norm": 0.14986367523670197,
      "learning_rate": 3.936666666666667e-06,
      "loss": 0.0015,
      "step": 138190
    },
    {
      "epoch": 7.370666666666667,
      "grad_norm": 0.4950984716415405,
      "learning_rate": 3.9333333333333335e-06,
      "loss": 0.0018,
      "step": 138200
    },
    {
      "epoch": 7.3712,
      "grad_norm": 0.23304148018360138,
      "learning_rate": 3.9300000000000005e-06,
      "loss": 0.0021,
      "step": 138210
    },
    {
      "epoch": 7.371733333333333,
      "grad_norm": 0.23810473084449768,
      "learning_rate": 3.926666666666667e-06,
      "loss": 0.002,
      "step": 138220
    },
    {
      "epoch": 7.3722666666666665,
      "grad_norm": 0.16641129553318024,
      "learning_rate": 3.923333333333334e-06,
      "loss": 0.002,
      "step": 138230
    },
    {
      "epoch": 7.3728,
      "grad_norm": 0.03858267888426781,
      "learning_rate": 3.92e-06,
      "loss": 0.0021,
      "step": 138240
    },
    {
      "epoch": 7.373333333333333,
      "grad_norm": 0.12393324822187424,
      "learning_rate": 3.916666666666667e-06,
      "loss": 0.0024,
      "step": 138250
    },
    {
      "epoch": 7.373866666666666,
      "grad_norm": 0.09990718960762024,
      "learning_rate": 3.913333333333334e-06,
      "loss": 0.0013,
      "step": 138260
    },
    {
      "epoch": 7.3744,
      "grad_norm": 0.09290885925292969,
      "learning_rate": 3.910000000000001e-06,
      "loss": 0.0018,
      "step": 138270
    },
    {
      "epoch": 7.374933333333333,
      "grad_norm": 0.24291552603244781,
      "learning_rate": 3.906666666666667e-06,
      "loss": 0.0026,
      "step": 138280
    },
    {
      "epoch": 7.375466666666667,
      "grad_norm": 0.23818087577819824,
      "learning_rate": 3.903333333333334e-06,
      "loss": 0.0013,
      "step": 138290
    },
    {
      "epoch": 7.376,
      "grad_norm": 0.22395232319831848,
      "learning_rate": 3.9e-06,
      "loss": 0.0026,
      "step": 138300
    },
    {
      "epoch": 7.376533333333334,
      "grad_norm": 0.18552671372890472,
      "learning_rate": 3.896666666666667e-06,
      "loss": 0.002,
      "step": 138310
    },
    {
      "epoch": 7.377066666666667,
      "grad_norm": 0.17469699680805206,
      "learning_rate": 3.893333333333334e-06,
      "loss": 0.0015,
      "step": 138320
    },
    {
      "epoch": 7.3776,
      "grad_norm": 0.3539024889469147,
      "learning_rate": 3.89e-06,
      "loss": 0.0022,
      "step": 138330
    },
    {
      "epoch": 7.378133333333333,
      "grad_norm": 0.3542870581150055,
      "learning_rate": 3.886666666666667e-06,
      "loss": 0.0016,
      "step": 138340
    },
    {
      "epoch": 7.378666666666667,
      "grad_norm": 0.2944057583808899,
      "learning_rate": 3.883333333333333e-06,
      "loss": 0.002,
      "step": 138350
    },
    {
      "epoch": 7.3792,
      "grad_norm": 0.37478509545326233,
      "learning_rate": 3.88e-06,
      "loss": 0.0019,
      "step": 138360
    },
    {
      "epoch": 7.379733333333333,
      "grad_norm": 0.15967248380184174,
      "learning_rate": 3.876666666666666e-06,
      "loss": 0.0013,
      "step": 138370
    },
    {
      "epoch": 7.3802666666666665,
      "grad_norm": 0.26213786005973816,
      "learning_rate": 3.873333333333334e-06,
      "loss": 0.0012,
      "step": 138380
    },
    {
      "epoch": 7.3808,
      "grad_norm": 0.2539442479610443,
      "learning_rate": 3.87e-06,
      "loss": 0.0014,
      "step": 138390
    },
    {
      "epoch": 7.381333333333333,
      "grad_norm": 0.16788160800933838,
      "learning_rate": 3.866666666666667e-06,
      "loss": 0.0014,
      "step": 138400
    },
    {
      "epoch": 7.381866666666666,
      "grad_norm": 0.023165298625826836,
      "learning_rate": 3.863333333333333e-06,
      "loss": 0.0015,
      "step": 138410
    },
    {
      "epoch": 7.3824,
      "grad_norm": 0.1554493010044098,
      "learning_rate": 3.86e-06,
      "loss": 0.0014,
      "step": 138420
    },
    {
      "epoch": 7.382933333333334,
      "grad_norm": 0.24107731878757477,
      "learning_rate": 3.8566666666666664e-06,
      "loss": 0.0014,
      "step": 138430
    },
    {
      "epoch": 7.383466666666667,
      "grad_norm": 0.3841032385826111,
      "learning_rate": 3.8533333333333334e-06,
      "loss": 0.0016,
      "step": 138440
    },
    {
      "epoch": 7.384,
      "grad_norm": 0.5140222907066345,
      "learning_rate": 3.85e-06,
      "loss": 0.0014,
      "step": 138450
    },
    {
      "epoch": 7.384533333333334,
      "grad_norm": 0.13829179108142853,
      "learning_rate": 3.846666666666667e-06,
      "loss": 0.002,
      "step": 138460
    },
    {
      "epoch": 7.385066666666667,
      "grad_norm": 0.039820779114961624,
      "learning_rate": 3.8433333333333335e-06,
      "loss": 0.0019,
      "step": 138470
    },
    {
      "epoch": 7.3856,
      "grad_norm": 0.045486874878406525,
      "learning_rate": 3.84e-06,
      "loss": 0.002,
      "step": 138480
    },
    {
      "epoch": 7.386133333333333,
      "grad_norm": 0.23838956654071808,
      "learning_rate": 3.836666666666667e-06,
      "loss": 0.0024,
      "step": 138490
    },
    {
      "epoch": 7.386666666666667,
      "grad_norm": 0.14304250478744507,
      "learning_rate": 3.833333333333334e-06,
      "loss": 0.002,
      "step": 138500
    },
    {
      "epoch": 7.3872,
      "grad_norm": 0.16547603905200958,
      "learning_rate": 3.830000000000001e-06,
      "loss": 0.0016,
      "step": 138510
    },
    {
      "epoch": 7.387733333333333,
      "grad_norm": 0.0987677052617073,
      "learning_rate": 3.826666666666667e-06,
      "loss": 0.0027,
      "step": 138520
    },
    {
      "epoch": 7.3882666666666665,
      "grad_norm": 0.39582347869873047,
      "learning_rate": 3.823333333333334e-06,
      "loss": 0.0019,
      "step": 138530
    },
    {
      "epoch": 7.3888,
      "grad_norm": 0.10218274593353271,
      "learning_rate": 3.82e-06,
      "loss": 0.0019,
      "step": 138540
    },
    {
      "epoch": 7.389333333333333,
      "grad_norm": 0.03999737650156021,
      "learning_rate": 3.816666666666667e-06,
      "loss": 0.0019,
      "step": 138550
    },
    {
      "epoch": 7.389866666666666,
      "grad_norm": 0.43806156516075134,
      "learning_rate": 3.8133333333333334e-06,
      "loss": 0.0016,
      "step": 138560
    },
    {
      "epoch": 7.3904,
      "grad_norm": 0.25072070956230164,
      "learning_rate": 3.8100000000000004e-06,
      "loss": 0.0021,
      "step": 138570
    },
    {
      "epoch": 7.390933333333333,
      "grad_norm": 0.1816490739583969,
      "learning_rate": 3.806666666666667e-06,
      "loss": 0.0017,
      "step": 138580
    },
    {
      "epoch": 7.391466666666667,
      "grad_norm": 0.24021030962467194,
      "learning_rate": 3.803333333333334e-06,
      "loss": 0.0022,
      "step": 138590
    },
    {
      "epoch": 7.392,
      "grad_norm": 0.44914510846138,
      "learning_rate": 3.8e-06,
      "loss": 0.0024,
      "step": 138600
    },
    {
      "epoch": 7.392533333333334,
      "grad_norm": 0.12897571921348572,
      "learning_rate": 3.796666666666667e-06,
      "loss": 0.0016,
      "step": 138610
    },
    {
      "epoch": 7.393066666666667,
      "grad_norm": 0.05923718214035034,
      "learning_rate": 3.7933333333333336e-06,
      "loss": 0.0026,
      "step": 138620
    },
    {
      "epoch": 7.3936,
      "grad_norm": 0.1113022118806839,
      "learning_rate": 3.7900000000000006e-06,
      "loss": 0.0013,
      "step": 138630
    },
    {
      "epoch": 7.3941333333333334,
      "grad_norm": 0.32997071743011475,
      "learning_rate": 3.7866666666666667e-06,
      "loss": 0.0012,
      "step": 138640
    },
    {
      "epoch": 7.394666666666667,
      "grad_norm": 0.13473300635814667,
      "learning_rate": 3.7833333333333333e-06,
      "loss": 0.0023,
      "step": 138650
    },
    {
      "epoch": 7.3952,
      "grad_norm": 0.2357555627822876,
      "learning_rate": 3.7800000000000002e-06,
      "loss": 0.0025,
      "step": 138660
    },
    {
      "epoch": 7.395733333333333,
      "grad_norm": 0.26703667640686035,
      "learning_rate": 3.7766666666666664e-06,
      "loss": 0.0016,
      "step": 138670
    },
    {
      "epoch": 7.3962666666666665,
      "grad_norm": 0.07197421044111252,
      "learning_rate": 3.7733333333333338e-06,
      "loss": 0.0018,
      "step": 138680
    },
    {
      "epoch": 7.3968,
      "grad_norm": 0.17681999504566193,
      "learning_rate": 3.77e-06,
      "loss": 0.0019,
      "step": 138690
    },
    {
      "epoch": 7.397333333333333,
      "grad_norm": 0.2896319627761841,
      "learning_rate": 3.766666666666667e-06,
      "loss": 0.0018,
      "step": 138700
    },
    {
      "epoch": 7.397866666666666,
      "grad_norm": 0.2944904863834381,
      "learning_rate": 3.7633333333333334e-06,
      "loss": 0.0015,
      "step": 138710
    },
    {
      "epoch": 7.3984,
      "grad_norm": 0.2562841475009918,
      "learning_rate": 3.7600000000000004e-06,
      "loss": 0.0018,
      "step": 138720
    },
    {
      "epoch": 7.398933333333333,
      "grad_norm": 0.1516999453306198,
      "learning_rate": 3.7566666666666666e-06,
      "loss": 0.0029,
      "step": 138730
    },
    {
      "epoch": 7.399466666666667,
      "grad_norm": 0.32292500138282776,
      "learning_rate": 3.7533333333333335e-06,
      "loss": 0.002,
      "step": 138740
    },
    {
      "epoch": 7.4,
      "grad_norm": 0.25244373083114624,
      "learning_rate": 3.75e-06,
      "loss": 0.0017,
      "step": 138750
    },
    {
      "epoch": 7.400533333333334,
      "grad_norm": 0.12466280162334442,
      "learning_rate": 3.746666666666667e-06,
      "loss": 0.0022,
      "step": 138760
    },
    {
      "epoch": 7.401066666666667,
      "grad_norm": 0.1900748908519745,
      "learning_rate": 3.743333333333333e-06,
      "loss": 0.0016,
      "step": 138770
    },
    {
      "epoch": 7.4016,
      "grad_norm": 0.41668424010276794,
      "learning_rate": 3.7400000000000006e-06,
      "loss": 0.0016,
      "step": 138780
    },
    {
      "epoch": 7.4021333333333335,
      "grad_norm": 0.12808053195476532,
      "learning_rate": 3.7366666666666667e-06,
      "loss": 0.0022,
      "step": 138790
    },
    {
      "epoch": 7.402666666666667,
      "grad_norm": 0.17833472788333893,
      "learning_rate": 3.7333333333333337e-06,
      "loss": 0.0015,
      "step": 138800
    },
    {
      "epoch": 7.4032,
      "grad_norm": 0.2621500492095947,
      "learning_rate": 3.7300000000000003e-06,
      "loss": 0.002,
      "step": 138810
    },
    {
      "epoch": 7.403733333333333,
      "grad_norm": 0.1392010599374771,
      "learning_rate": 3.7266666666666664e-06,
      "loss": 0.0021,
      "step": 138820
    },
    {
      "epoch": 7.4042666666666666,
      "grad_norm": 0.1479172259569168,
      "learning_rate": 3.7233333333333334e-06,
      "loss": 0.002,
      "step": 138830
    },
    {
      "epoch": 7.4048,
      "grad_norm": 0.1026638075709343,
      "learning_rate": 3.72e-06,
      "loss": 0.0018,
      "step": 138840
    },
    {
      "epoch": 7.405333333333333,
      "grad_norm": 0.106102354824543,
      "learning_rate": 3.716666666666667e-06,
      "loss": 0.0013,
      "step": 138850
    },
    {
      "epoch": 7.405866666666666,
      "grad_norm": 0.3698985278606415,
      "learning_rate": 3.713333333333333e-06,
      "loss": 0.002,
      "step": 138860
    },
    {
      "epoch": 7.4064,
      "grad_norm": 0.4695797264575958,
      "learning_rate": 3.7100000000000005e-06,
      "loss": 0.0015,
      "step": 138870
    },
    {
      "epoch": 7.406933333333333,
      "grad_norm": 0.17723441123962402,
      "learning_rate": 3.7066666666666666e-06,
      "loss": 0.0016,
      "step": 138880
    },
    {
      "epoch": 7.407466666666666,
      "grad_norm": 0.19406116008758545,
      "learning_rate": 3.7033333333333336e-06,
      "loss": 0.0029,
      "step": 138890
    },
    {
      "epoch": 7.408,
      "grad_norm": 0.2647969424724579,
      "learning_rate": 3.7e-06,
      "loss": 0.0014,
      "step": 138900
    },
    {
      "epoch": 7.408533333333334,
      "grad_norm": 0.1699148416519165,
      "learning_rate": 3.696666666666667e-06,
      "loss": 0.0014,
      "step": 138910
    },
    {
      "epoch": 7.409066666666667,
      "grad_norm": 0.14219793677330017,
      "learning_rate": 3.6933333333333333e-06,
      "loss": 0.0019,
      "step": 138920
    },
    {
      "epoch": 7.4096,
      "grad_norm": 0.40807288885116577,
      "learning_rate": 3.6900000000000002e-06,
      "loss": 0.0018,
      "step": 138930
    },
    {
      "epoch": 7.4101333333333335,
      "grad_norm": 0.09502732008695602,
      "learning_rate": 3.686666666666667e-06,
      "loss": 0.0019,
      "step": 138940
    },
    {
      "epoch": 7.410666666666667,
      "grad_norm": 0.08898288756608963,
      "learning_rate": 3.6833333333333338e-06,
      "loss": 0.0027,
      "step": 138950
    },
    {
      "epoch": 7.4112,
      "grad_norm": 0.14960741996765137,
      "learning_rate": 3.68e-06,
      "loss": 0.0016,
      "step": 138960
    },
    {
      "epoch": 7.411733333333333,
      "grad_norm": 0.27156227827072144,
      "learning_rate": 3.6766666666666673e-06,
      "loss": 0.0026,
      "step": 138970
    },
    {
      "epoch": 7.412266666666667,
      "grad_norm": 0.5042106509208679,
      "learning_rate": 3.6733333333333335e-06,
      "loss": 0.002,
      "step": 138980
    },
    {
      "epoch": 7.4128,
      "grad_norm": 0.06324642896652222,
      "learning_rate": 3.6700000000000004e-06,
      "loss": 0.0023,
      "step": 138990
    },
    {
      "epoch": 7.413333333333333,
      "grad_norm": 0.062258947640657425,
      "learning_rate": 3.666666666666667e-06,
      "loss": 0.0019,
      "step": 139000
    },
    {
      "epoch": 7.413866666666666,
      "grad_norm": 0.12705671787261963,
      "learning_rate": 3.663333333333333e-06,
      "loss": 0.0017,
      "step": 139010
    },
    {
      "epoch": 7.4144,
      "grad_norm": 0.1594032347202301,
      "learning_rate": 3.66e-06,
      "loss": 0.0026,
      "step": 139020
    },
    {
      "epoch": 7.414933333333333,
      "grad_norm": 0.1572200059890747,
      "learning_rate": 3.6566666666666667e-06,
      "loss": 0.0013,
      "step": 139030
    },
    {
      "epoch": 7.415466666666667,
      "grad_norm": 0.10745750367641449,
      "learning_rate": 3.6533333333333336e-06,
      "loss": 0.0015,
      "step": 139040
    },
    {
      "epoch": 7.416,
      "grad_norm": 0.15546852350234985,
      "learning_rate": 3.6499999999999998e-06,
      "loss": 0.0016,
      "step": 139050
    },
    {
      "epoch": 7.416533333333334,
      "grad_norm": 0.15455982089042664,
      "learning_rate": 3.646666666666667e-06,
      "loss": 0.0012,
      "step": 139060
    },
    {
      "epoch": 7.417066666666667,
      "grad_norm": 0.2092777043581009,
      "learning_rate": 3.6433333333333333e-06,
      "loss": 0.0021,
      "step": 139070
    },
    {
      "epoch": 7.4176,
      "grad_norm": 0.29868316650390625,
      "learning_rate": 3.6400000000000003e-06,
      "loss": 0.002,
      "step": 139080
    },
    {
      "epoch": 7.4181333333333335,
      "grad_norm": 0.3824131190776825,
      "learning_rate": 3.636666666666667e-06,
      "loss": 0.0025,
      "step": 139090
    },
    {
      "epoch": 7.418666666666667,
      "grad_norm": 0.2036750316619873,
      "learning_rate": 3.633333333333334e-06,
      "loss": 0.0017,
      "step": 139100
    },
    {
      "epoch": 7.4192,
      "grad_norm": 0.09364285320043564,
      "learning_rate": 3.63e-06,
      "loss": 0.0021,
      "step": 139110
    },
    {
      "epoch": 7.419733333333333,
      "grad_norm": 0.21930482983589172,
      "learning_rate": 3.626666666666667e-06,
      "loss": 0.0013,
      "step": 139120
    },
    {
      "epoch": 7.420266666666667,
      "grad_norm": 0.26300325989723206,
      "learning_rate": 3.6233333333333335e-06,
      "loss": 0.0015,
      "step": 139130
    },
    {
      "epoch": 7.4208,
      "grad_norm": 0.23509913682937622,
      "learning_rate": 3.6200000000000005e-06,
      "loss": 0.0014,
      "step": 139140
    },
    {
      "epoch": 7.421333333333333,
      "grad_norm": 0.05799994245171547,
      "learning_rate": 3.6166666666666666e-06,
      "loss": 0.0017,
      "step": 139150
    },
    {
      "epoch": 7.421866666666666,
      "grad_norm": 0.3531200587749481,
      "learning_rate": 3.613333333333334e-06,
      "loss": 0.0014,
      "step": 139160
    },
    {
      "epoch": 7.4224,
      "grad_norm": 0.2388516217470169,
      "learning_rate": 3.61e-06,
      "loss": 0.0019,
      "step": 139170
    },
    {
      "epoch": 7.422933333333333,
      "grad_norm": 0.3029521703720093,
      "learning_rate": 3.6066666666666667e-06,
      "loss": 0.0016,
      "step": 139180
    },
    {
      "epoch": 7.423466666666666,
      "grad_norm": 0.1496688574552536,
      "learning_rate": 3.6033333333333337e-06,
      "loss": 0.0015,
      "step": 139190
    },
    {
      "epoch": 7.424,
      "grad_norm": 0.3872252404689789,
      "learning_rate": 3.6e-06,
      "loss": 0.0013,
      "step": 139200
    },
    {
      "epoch": 7.424533333333334,
      "grad_norm": 0.1018555536866188,
      "learning_rate": 3.596666666666667e-06,
      "loss": 0.0021,
      "step": 139210
    },
    {
      "epoch": 7.425066666666667,
      "grad_norm": 0.09577101469039917,
      "learning_rate": 3.5933333333333334e-06,
      "loss": 0.002,
      "step": 139220
    },
    {
      "epoch": 7.4256,
      "grad_norm": 0.10435856878757477,
      "learning_rate": 3.5900000000000004e-06,
      "loss": 0.0015,
      "step": 139230
    },
    {
      "epoch": 7.4261333333333335,
      "grad_norm": 0.10179216414690018,
      "learning_rate": 3.5866666666666665e-06,
      "loss": 0.0032,
      "step": 139240
    },
    {
      "epoch": 7.426666666666667,
      "grad_norm": 0.12134113162755966,
      "learning_rate": 3.5833333333333335e-06,
      "loss": 0.0034,
      "step": 139250
    },
    {
      "epoch": 7.4272,
      "grad_norm": 0.35283151268959045,
      "learning_rate": 3.58e-06,
      "loss": 0.0014,
      "step": 139260
    },
    {
      "epoch": 7.427733333333333,
      "grad_norm": 0.04074731841683388,
      "learning_rate": 3.576666666666667e-06,
      "loss": 0.0017,
      "step": 139270
    },
    {
      "epoch": 7.428266666666667,
      "grad_norm": 0.40877750515937805,
      "learning_rate": 3.5733333333333336e-06,
      "loss": 0.0019,
      "step": 139280
    },
    {
      "epoch": 7.4288,
      "grad_norm": 0.17027647793293,
      "learning_rate": 3.5700000000000005e-06,
      "loss": 0.0016,
      "step": 139290
    },
    {
      "epoch": 7.429333333333333,
      "grad_norm": 0.06920302659273148,
      "learning_rate": 3.5666666666666667e-06,
      "loss": 0.0017,
      "step": 139300
    },
    {
      "epoch": 7.429866666666666,
      "grad_norm": 0.3424133360385895,
      "learning_rate": 3.5633333333333337e-06,
      "loss": 0.0013,
      "step": 139310
    },
    {
      "epoch": 7.4304,
      "grad_norm": 0.12264396995306015,
      "learning_rate": 3.5600000000000002e-06,
      "loss": 0.0017,
      "step": 139320
    },
    {
      "epoch": 7.430933333333333,
      "grad_norm": 0.3912886083126068,
      "learning_rate": 3.556666666666667e-06,
      "loss": 0.0019,
      "step": 139330
    },
    {
      "epoch": 7.431466666666667,
      "grad_norm": 0.41222718358039856,
      "learning_rate": 3.5533333333333333e-06,
      "loss": 0.0023,
      "step": 139340
    },
    {
      "epoch": 7.432,
      "grad_norm": 0.15921153128147125,
      "learning_rate": 3.55e-06,
      "loss": 0.0019,
      "step": 139350
    },
    {
      "epoch": 7.432533333333334,
      "grad_norm": 0.4393710792064667,
      "learning_rate": 3.546666666666667e-06,
      "loss": 0.0016,
      "step": 139360
    },
    {
      "epoch": 7.433066666666667,
      "grad_norm": 0.04171622917056084,
      "learning_rate": 3.5433333333333334e-06,
      "loss": 0.0017,
      "step": 139370
    },
    {
      "epoch": 7.4336,
      "grad_norm": 0.10282981395721436,
      "learning_rate": 3.5400000000000004e-06,
      "loss": 0.0012,
      "step": 139380
    },
    {
      "epoch": 7.4341333333333335,
      "grad_norm": 0.03985113278031349,
      "learning_rate": 3.5366666666666665e-06,
      "loss": 0.002,
      "step": 139390
    },
    {
      "epoch": 7.434666666666667,
      "grad_norm": 0.31347012519836426,
      "learning_rate": 3.5333333333333335e-06,
      "loss": 0.0019,
      "step": 139400
    },
    {
      "epoch": 7.4352,
      "grad_norm": 0.30228671431541443,
      "learning_rate": 3.53e-06,
      "loss": 0.0018,
      "step": 139410
    },
    {
      "epoch": 7.435733333333333,
      "grad_norm": 0.12051849067211151,
      "learning_rate": 3.526666666666667e-06,
      "loss": 0.0015,
      "step": 139420
    },
    {
      "epoch": 7.436266666666667,
      "grad_norm": 0.041823938488960266,
      "learning_rate": 3.523333333333333e-06,
      "loss": 0.0016,
      "step": 139430
    },
    {
      "epoch": 7.4368,
      "grad_norm": 0.26799190044403076,
      "learning_rate": 3.52e-06,
      "loss": 0.0015,
      "step": 139440
    },
    {
      "epoch": 7.437333333333333,
      "grad_norm": 0.2378089725971222,
      "learning_rate": 3.5166666666666667e-06,
      "loss": 0.002,
      "step": 139450
    },
    {
      "epoch": 7.437866666666666,
      "grad_norm": 0.1833425909280777,
      "learning_rate": 3.5133333333333337e-06,
      "loss": 0.0026,
      "step": 139460
    },
    {
      "epoch": 7.4384,
      "grad_norm": 0.07162515819072723,
      "learning_rate": 3.5100000000000003e-06,
      "loss": 0.0029,
      "step": 139470
    },
    {
      "epoch": 7.438933333333333,
      "grad_norm": 0.17836712300777435,
      "learning_rate": 3.5066666666666673e-06,
      "loss": 0.0015,
      "step": 139480
    },
    {
      "epoch": 7.439466666666666,
      "grad_norm": 0.03856920078396797,
      "learning_rate": 3.5033333333333334e-06,
      "loss": 0.0014,
      "step": 139490
    },
    {
      "epoch": 7.44,
      "grad_norm": 0.19611942768096924,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 0.0017,
      "step": 139500
    },
    {
      "epoch": 7.440533333333334,
      "grad_norm": 0.04863036051392555,
      "learning_rate": 3.496666666666667e-06,
      "loss": 0.0014,
      "step": 139510
    },
    {
      "epoch": 7.441066666666667,
      "grad_norm": 0.13223113119602203,
      "learning_rate": 3.493333333333333e-06,
      "loss": 0.002,
      "step": 139520
    },
    {
      "epoch": 7.4416,
      "grad_norm": 0.09833429753780365,
      "learning_rate": 3.49e-06,
      "loss": 0.0015,
      "step": 139530
    },
    {
      "epoch": 7.4421333333333335,
      "grad_norm": 0.06619489938020706,
      "learning_rate": 3.4866666666666666e-06,
      "loss": 0.0017,
      "step": 139540
    },
    {
      "epoch": 7.442666666666667,
      "grad_norm": 0.12403351813554764,
      "learning_rate": 3.4833333333333336e-06,
      "loss": 0.0023,
      "step": 139550
    },
    {
      "epoch": 7.4432,
      "grad_norm": 0.12929658591747284,
      "learning_rate": 3.4799999999999997e-06,
      "loss": 0.0011,
      "step": 139560
    },
    {
      "epoch": 7.443733333333333,
      "grad_norm": 0.4774084985256195,
      "learning_rate": 3.476666666666667e-06,
      "loss": 0.0018,
      "step": 139570
    },
    {
      "epoch": 7.444266666666667,
      "grad_norm": 0.05401584133505821,
      "learning_rate": 3.4733333333333333e-06,
      "loss": 0.0013,
      "step": 139580
    },
    {
      "epoch": 7.4448,
      "grad_norm": 0.026585344225168228,
      "learning_rate": 3.4700000000000002e-06,
      "loss": 0.0014,
      "step": 139590
    },
    {
      "epoch": 7.445333333333333,
      "grad_norm": 0.05540134757757187,
      "learning_rate": 3.466666666666667e-06,
      "loss": 0.0032,
      "step": 139600
    },
    {
      "epoch": 7.445866666666666,
      "grad_norm": 0.04036978632211685,
      "learning_rate": 3.4633333333333338e-06,
      "loss": 0.0021,
      "step": 139610
    },
    {
      "epoch": 7.4464,
      "grad_norm": 0.12071257829666138,
      "learning_rate": 3.46e-06,
      "loss": 0.0019,
      "step": 139620
    },
    {
      "epoch": 7.446933333333333,
      "grad_norm": 0.055134642869234085,
      "learning_rate": 3.456666666666667e-06,
      "loss": 0.0019,
      "step": 139630
    },
    {
      "epoch": 7.447466666666667,
      "grad_norm": 0.2628301680088043,
      "learning_rate": 3.4533333333333334e-06,
      "loss": 0.0016,
      "step": 139640
    },
    {
      "epoch": 7.448,
      "grad_norm": 0.34909602999687195,
      "learning_rate": 3.4500000000000004e-06,
      "loss": 0.0014,
      "step": 139650
    },
    {
      "epoch": 7.448533333333334,
      "grad_norm": 0.4469672441482544,
      "learning_rate": 3.446666666666667e-06,
      "loss": 0.0017,
      "step": 139660
    },
    {
      "epoch": 7.449066666666667,
      "grad_norm": 0.05155019834637642,
      "learning_rate": 3.443333333333334e-06,
      "loss": 0.0018,
      "step": 139670
    },
    {
      "epoch": 7.4496,
      "grad_norm": 0.0639934241771698,
      "learning_rate": 3.44e-06,
      "loss": 0.0021,
      "step": 139680
    },
    {
      "epoch": 7.4501333333333335,
      "grad_norm": 0.07051277905702591,
      "learning_rate": 3.4366666666666667e-06,
      "loss": 0.0024,
      "step": 139690
    },
    {
      "epoch": 7.450666666666667,
      "grad_norm": 0.059078194200992584,
      "learning_rate": 3.4333333333333336e-06,
      "loss": 0.0017,
      "step": 139700
    },
    {
      "epoch": 7.4512,
      "grad_norm": 0.14018462598323822,
      "learning_rate": 3.4299999999999998e-06,
      "loss": 0.0017,
      "step": 139710
    },
    {
      "epoch": 7.451733333333333,
      "grad_norm": 0.04698793590068817,
      "learning_rate": 3.4266666666666668e-06,
      "loss": 0.0027,
      "step": 139720
    },
    {
      "epoch": 7.452266666666667,
      "grad_norm": 0.05417764186859131,
      "learning_rate": 3.4233333333333333e-06,
      "loss": 0.0013,
      "step": 139730
    },
    {
      "epoch": 7.4528,
      "grad_norm": 0.18599432706832886,
      "learning_rate": 3.4200000000000003e-06,
      "loss": 0.0018,
      "step": 139740
    },
    {
      "epoch": 7.453333333333333,
      "grad_norm": 0.38565701246261597,
      "learning_rate": 3.4166666666666664e-06,
      "loss": 0.0015,
      "step": 139750
    },
    {
      "epoch": 7.453866666666666,
      "grad_norm": 0.23078803718090057,
      "learning_rate": 3.413333333333334e-06,
      "loss": 0.0017,
      "step": 139760
    },
    {
      "epoch": 7.4544,
      "grad_norm": 0.09245504438877106,
      "learning_rate": 3.41e-06,
      "loss": 0.0018,
      "step": 139770
    },
    {
      "epoch": 7.454933333333333,
      "grad_norm": 0.05614122375845909,
      "learning_rate": 3.406666666666667e-06,
      "loss": 0.0016,
      "step": 139780
    },
    {
      "epoch": 7.455466666666666,
      "grad_norm": 0.3238108456134796,
      "learning_rate": 3.4033333333333335e-06,
      "loss": 0.0018,
      "step": 139790
    },
    {
      "epoch": 7.456,
      "grad_norm": 0.053120970726013184,
      "learning_rate": 3.4000000000000005e-06,
      "loss": 0.0033,
      "step": 139800
    },
    {
      "epoch": 7.456533333333334,
      "grad_norm": 0.15017154812812805,
      "learning_rate": 3.3966666666666666e-06,
      "loss": 0.0015,
      "step": 139810
    },
    {
      "epoch": 7.457066666666667,
      "grad_norm": 0.18415676057338715,
      "learning_rate": 3.3933333333333336e-06,
      "loss": 0.0022,
      "step": 139820
    },
    {
      "epoch": 7.4576,
      "grad_norm": 0.09935325384140015,
      "learning_rate": 3.39e-06,
      "loss": 0.0015,
      "step": 139830
    },
    {
      "epoch": 7.4581333333333335,
      "grad_norm": 0.29256314039230347,
      "learning_rate": 3.386666666666667e-06,
      "loss": 0.0015,
      "step": 139840
    },
    {
      "epoch": 7.458666666666667,
      "grad_norm": 0.5028969645500183,
      "learning_rate": 3.3833333333333337e-06,
      "loss": 0.0019,
      "step": 139850
    },
    {
      "epoch": 7.4592,
      "grad_norm": 0.0730765089392662,
      "learning_rate": 3.38e-06,
      "loss": 0.0047,
      "step": 139860
    },
    {
      "epoch": 7.459733333333333,
      "grad_norm": 0.18683546781539917,
      "learning_rate": 3.376666666666667e-06,
      "loss": 0.0013,
      "step": 139870
    },
    {
      "epoch": 7.460266666666667,
      "grad_norm": 0.07420767098665237,
      "learning_rate": 3.3733333333333334e-06,
      "loss": 0.0012,
      "step": 139880
    },
    {
      "epoch": 7.4608,
      "grad_norm": 0.12101854383945465,
      "learning_rate": 3.3700000000000003e-06,
      "loss": 0.0021,
      "step": 139890
    },
    {
      "epoch": 7.461333333333333,
      "grad_norm": 0.18055610358715057,
      "learning_rate": 3.3666666666666665e-06,
      "loss": 0.002,
      "step": 139900
    },
    {
      "epoch": 7.461866666666666,
      "grad_norm": 0.08121126890182495,
      "learning_rate": 3.3633333333333335e-06,
      "loss": 0.0019,
      "step": 139910
    },
    {
      "epoch": 7.4624,
      "grad_norm": 0.16095595061779022,
      "learning_rate": 3.36e-06,
      "loss": 0.0018,
      "step": 139920
    },
    {
      "epoch": 7.462933333333333,
      "grad_norm": 0.26197245717048645,
      "learning_rate": 3.356666666666667e-06,
      "loss": 0.0022,
      "step": 139930
    },
    {
      "epoch": 7.463466666666667,
      "grad_norm": 0.14728714525699615,
      "learning_rate": 3.353333333333333e-06,
      "loss": 0.0022,
      "step": 139940
    },
    {
      "epoch": 7.464,
      "grad_norm": 0.0493202768266201,
      "learning_rate": 3.3500000000000005e-06,
      "loss": 0.0018,
      "step": 139950
    },
    {
      "epoch": 7.464533333333334,
      "grad_norm": 0.06090174615383148,
      "learning_rate": 3.3466666666666667e-06,
      "loss": 0.0021,
      "step": 139960
    },
    {
      "epoch": 7.465066666666667,
      "grad_norm": 0.09601031988859177,
      "learning_rate": 3.3433333333333337e-06,
      "loss": 0.001,
      "step": 139970
    },
    {
      "epoch": 7.4656,
      "grad_norm": 0.03562287613749504,
      "learning_rate": 3.34e-06,
      "loss": 0.0016,
      "step": 139980
    },
    {
      "epoch": 7.4661333333333335,
      "grad_norm": 0.3268304765224457,
      "learning_rate": 3.336666666666667e-06,
      "loss": 0.0024,
      "step": 139990
    },
    {
      "epoch": 7.466666666666667,
      "grad_norm": 0.07451765239238739,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.0014,
      "step": 140000
    },
    {
      "epoch": 7.4672,
      "grad_norm": 0.12379579991102219,
      "learning_rate": 3.3300000000000003e-06,
      "loss": 0.0015,
      "step": 140010
    },
    {
      "epoch": 7.467733333333333,
      "grad_norm": 0.16956780850887299,
      "learning_rate": 3.326666666666667e-06,
      "loss": 0.0013,
      "step": 140020
    },
    {
      "epoch": 7.468266666666667,
      "grad_norm": 0.11296143382787704,
      "learning_rate": 3.323333333333333e-06,
      "loss": 0.0016,
      "step": 140030
    },
    {
      "epoch": 7.4688,
      "grad_norm": 0.2186489850282669,
      "learning_rate": 3.3200000000000004e-06,
      "loss": 0.0015,
      "step": 140040
    },
    {
      "epoch": 7.469333333333333,
      "grad_norm": 0.13115611672401428,
      "learning_rate": 3.3166666666666665e-06,
      "loss": 0.0019,
      "step": 140050
    },
    {
      "epoch": 7.469866666666666,
      "grad_norm": 0.18358542025089264,
      "learning_rate": 3.3133333333333335e-06,
      "loss": 0.0011,
      "step": 140060
    },
    {
      "epoch": 7.4704,
      "grad_norm": 0.03687046468257904,
      "learning_rate": 3.31e-06,
      "loss": 0.0015,
      "step": 140070
    },
    {
      "epoch": 7.470933333333333,
      "grad_norm": 0.1484680324792862,
      "learning_rate": 3.306666666666667e-06,
      "loss": 0.0026,
      "step": 140080
    },
    {
      "epoch": 7.471466666666666,
      "grad_norm": 0.25212356448173523,
      "learning_rate": 3.303333333333333e-06,
      "loss": 0.0014,
      "step": 140090
    },
    {
      "epoch": 7.4719999999999995,
      "grad_norm": 0.14634884893894196,
      "learning_rate": 3.3e-06,
      "loss": 0.0021,
      "step": 140100
    },
    {
      "epoch": 7.472533333333334,
      "grad_norm": 0.17816995084285736,
      "learning_rate": 3.2966666666666667e-06,
      "loss": 0.0017,
      "step": 140110
    },
    {
      "epoch": 7.473066666666667,
      "grad_norm": 0.26092687249183655,
      "learning_rate": 3.2933333333333337e-06,
      "loss": 0.0012,
      "step": 140120
    },
    {
      "epoch": 7.4736,
      "grad_norm": 0.24244281649589539,
      "learning_rate": 3.29e-06,
      "loss": 0.0018,
      "step": 140130
    },
    {
      "epoch": 7.4741333333333335,
      "grad_norm": 0.5499227643013,
      "learning_rate": 3.2866666666666672e-06,
      "loss": 0.0015,
      "step": 140140
    },
    {
      "epoch": 7.474666666666667,
      "grad_norm": 0.5240321159362793,
      "learning_rate": 3.2833333333333334e-06,
      "loss": 0.0021,
      "step": 140150
    },
    {
      "epoch": 7.4752,
      "grad_norm": 0.5211528539657593,
      "learning_rate": 3.2800000000000004e-06,
      "loss": 0.0019,
      "step": 140160
    },
    {
      "epoch": 7.475733333333333,
      "grad_norm": 0.27313902974128723,
      "learning_rate": 3.276666666666667e-06,
      "loss": 0.0014,
      "step": 140170
    },
    {
      "epoch": 7.476266666666667,
      "grad_norm": 0.25275734066963196,
      "learning_rate": 3.273333333333334e-06,
      "loss": 0.0016,
      "step": 140180
    },
    {
      "epoch": 7.4768,
      "grad_norm": 0.6278407573699951,
      "learning_rate": 3.27e-06,
      "loss": 0.0015,
      "step": 140190
    },
    {
      "epoch": 7.477333333333333,
      "grad_norm": 0.1518608182668686,
      "learning_rate": 3.2666666666666666e-06,
      "loss": 0.0021,
      "step": 140200
    },
    {
      "epoch": 7.477866666666666,
      "grad_norm": 0.11462002247571945,
      "learning_rate": 3.2633333333333336e-06,
      "loss": 0.0017,
      "step": 140210
    },
    {
      "epoch": 7.4784,
      "grad_norm": 0.1774289309978485,
      "learning_rate": 3.2599999999999997e-06,
      "loss": 0.0017,
      "step": 140220
    },
    {
      "epoch": 7.478933333333333,
      "grad_norm": 0.1476186364889145,
      "learning_rate": 3.2566666666666667e-06,
      "loss": 0.0013,
      "step": 140230
    },
    {
      "epoch": 7.479466666666666,
      "grad_norm": 0.19570991396903992,
      "learning_rate": 3.2533333333333332e-06,
      "loss": 0.0025,
      "step": 140240
    },
    {
      "epoch": 7.48,
      "grad_norm": 0.32182493805885315,
      "learning_rate": 3.2500000000000002e-06,
      "loss": 0.0014,
      "step": 140250
    },
    {
      "epoch": 7.480533333333334,
      "grad_norm": 0.4696212112903595,
      "learning_rate": 3.2466666666666668e-06,
      "loss": 0.0019,
      "step": 140260
    },
    {
      "epoch": 7.481066666666667,
      "grad_norm": 0.17774349451065063,
      "learning_rate": 3.2433333333333338e-06,
      "loss": 0.0019,
      "step": 140270
    },
    {
      "epoch": 7.4816,
      "grad_norm": 0.44801998138427734,
      "learning_rate": 3.24e-06,
      "loss": 0.0014,
      "step": 140280
    },
    {
      "epoch": 7.4821333333333335,
      "grad_norm": 0.07168912887573242,
      "learning_rate": 3.236666666666667e-06,
      "loss": 0.003,
      "step": 140290
    },
    {
      "epoch": 7.482666666666667,
      "grad_norm": 0.10224039852619171,
      "learning_rate": 3.2333333333333334e-06,
      "loss": 0.0015,
      "step": 140300
    },
    {
      "epoch": 7.4832,
      "grad_norm": 0.06543931365013123,
      "learning_rate": 3.2300000000000004e-06,
      "loss": 0.0014,
      "step": 140310
    },
    {
      "epoch": 7.483733333333333,
      "grad_norm": 0.2476681023836136,
      "learning_rate": 3.2266666666666665e-06,
      "loss": 0.0023,
      "step": 140320
    },
    {
      "epoch": 7.484266666666667,
      "grad_norm": 0.24153578281402588,
      "learning_rate": 3.223333333333334e-06,
      "loss": 0.0012,
      "step": 140330
    },
    {
      "epoch": 7.4848,
      "grad_norm": 0.07187923043966293,
      "learning_rate": 3.22e-06,
      "loss": 0.0017,
      "step": 140340
    },
    {
      "epoch": 7.485333333333333,
      "grad_norm": 0.11560779064893723,
      "learning_rate": 3.216666666666667e-06,
      "loss": 0.002,
      "step": 140350
    },
    {
      "epoch": 7.4858666666666664,
      "grad_norm": 0.2791348099708557,
      "learning_rate": 3.2133333333333336e-06,
      "loss": 0.0025,
      "step": 140360
    },
    {
      "epoch": 7.4864,
      "grad_norm": 0.06601675599813461,
      "learning_rate": 3.2099999999999998e-06,
      "loss": 0.0012,
      "step": 140370
    },
    {
      "epoch": 7.486933333333333,
      "grad_norm": 0.10579188168048859,
      "learning_rate": 3.2066666666666667e-06,
      "loss": 0.0018,
      "step": 140380
    },
    {
      "epoch": 7.487466666666666,
      "grad_norm": 0.15815936028957367,
      "learning_rate": 3.2033333333333333e-06,
      "loss": 0.0017,
      "step": 140390
    },
    {
      "epoch": 7.4879999999999995,
      "grad_norm": 0.47694122791290283,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 0.0016,
      "step": 140400
    },
    {
      "epoch": 7.488533333333334,
      "grad_norm": 0.15198637545108795,
      "learning_rate": 3.1966666666666664e-06,
      "loss": 0.0015,
      "step": 140410
    },
    {
      "epoch": 7.489066666666667,
      "grad_norm": 0.12495303153991699,
      "learning_rate": 3.1933333333333334e-06,
      "loss": 0.0013,
      "step": 140420
    },
    {
      "epoch": 7.4896,
      "grad_norm": 0.37755724787712097,
      "learning_rate": 3.19e-06,
      "loss": 0.0016,
      "step": 140430
    },
    {
      "epoch": 7.4901333333333335,
      "grad_norm": 0.07928016036748886,
      "learning_rate": 3.186666666666667e-06,
      "loss": 0.0018,
      "step": 140440
    },
    {
      "epoch": 7.490666666666667,
      "grad_norm": 0.15556104481220245,
      "learning_rate": 3.1833333333333335e-06,
      "loss": 0.0015,
      "step": 140450
    },
    {
      "epoch": 7.4912,
      "grad_norm": 0.4636928141117096,
      "learning_rate": 3.1800000000000005e-06,
      "loss": 0.0027,
      "step": 140460
    },
    {
      "epoch": 7.491733333333333,
      "grad_norm": 0.2753589153289795,
      "learning_rate": 3.1766666666666666e-06,
      "loss": 0.0025,
      "step": 140470
    },
    {
      "epoch": 7.492266666666667,
      "grad_norm": 0.20480655133724213,
      "learning_rate": 3.1733333333333336e-06,
      "loss": 0.0012,
      "step": 140480
    },
    {
      "epoch": 7.4928,
      "grad_norm": 0.26116761565208435,
      "learning_rate": 3.17e-06,
      "loss": 0.0015,
      "step": 140490
    },
    {
      "epoch": 7.493333333333333,
      "grad_norm": 0.09515976160764694,
      "learning_rate": 3.166666666666667e-06,
      "loss": 0.0015,
      "step": 140500
    },
    {
      "epoch": 7.4938666666666665,
      "grad_norm": 0.13609735667705536,
      "learning_rate": 3.1633333333333333e-06,
      "loss": 0.0019,
      "step": 140510
    },
    {
      "epoch": 7.4944,
      "grad_norm": 0.37107545137405396,
      "learning_rate": 3.1600000000000007e-06,
      "loss": 0.002,
      "step": 140520
    },
    {
      "epoch": 7.494933333333333,
      "grad_norm": 0.23288483917713165,
      "learning_rate": 3.156666666666667e-06,
      "loss": 0.0023,
      "step": 140530
    },
    {
      "epoch": 7.495466666666666,
      "grad_norm": 0.32497522234916687,
      "learning_rate": 3.153333333333333e-06,
      "loss": 0.0017,
      "step": 140540
    },
    {
      "epoch": 7.496,
      "grad_norm": 0.053389064967632294,
      "learning_rate": 3.1500000000000003e-06,
      "loss": 0.0012,
      "step": 140550
    },
    {
      "epoch": 7.496533333333334,
      "grad_norm": 0.46759846806526184,
      "learning_rate": 3.1466666666666665e-06,
      "loss": 0.002,
      "step": 140560
    },
    {
      "epoch": 7.497066666666667,
      "grad_norm": 0.26290664076805115,
      "learning_rate": 3.1433333333333334e-06,
      "loss": 0.002,
      "step": 140570
    },
    {
      "epoch": 7.4976,
      "grad_norm": 0.07188735902309418,
      "learning_rate": 3.14e-06,
      "loss": 0.0022,
      "step": 140580
    },
    {
      "epoch": 7.4981333333333335,
      "grad_norm": 0.09890203922986984,
      "learning_rate": 3.136666666666667e-06,
      "loss": 0.002,
      "step": 140590
    },
    {
      "epoch": 7.498666666666667,
      "grad_norm": 0.32359248399734497,
      "learning_rate": 3.133333333333333e-06,
      "loss": 0.0013,
      "step": 140600
    },
    {
      "epoch": 7.4992,
      "grad_norm": 0.147713765501976,
      "learning_rate": 3.13e-06,
      "loss": 0.0032,
      "step": 140610
    },
    {
      "epoch": 7.499733333333333,
      "grad_norm": 0.4060601592063904,
      "learning_rate": 3.1266666666666667e-06,
      "loss": 0.0016,
      "step": 140620
    },
    {
      "epoch": 7.500266666666667,
      "grad_norm": 0.040171068161726,
      "learning_rate": 3.1233333333333332e-06,
      "loss": 0.0018,
      "step": 140630
    },
    {
      "epoch": 7.5008,
      "grad_norm": 0.22839026153087616,
      "learning_rate": 3.12e-06,
      "loss": 0.0012,
      "step": 140640
    },
    {
      "epoch": 7.501333333333333,
      "grad_norm": 0.15138086676597595,
      "learning_rate": 3.1166666666666668e-06,
      "loss": 0.0024,
      "step": 140650
    },
    {
      "epoch": 7.5018666666666665,
      "grad_norm": 0.060740139335393906,
      "learning_rate": 3.1133333333333333e-06,
      "loss": 0.0013,
      "step": 140660
    },
    {
      "epoch": 7.5024,
      "grad_norm": 0.09142084419727325,
      "learning_rate": 3.11e-06,
      "loss": 0.0018,
      "step": 140670
    },
    {
      "epoch": 7.502933333333333,
      "grad_norm": 0.1259308159351349,
      "learning_rate": 3.106666666666667e-06,
      "loss": 0.002,
      "step": 140680
    },
    {
      "epoch": 7.503466666666666,
      "grad_norm": 0.10328113287687302,
      "learning_rate": 3.1033333333333334e-06,
      "loss": 0.002,
      "step": 140690
    },
    {
      "epoch": 7.504,
      "grad_norm": 0.3798830807209015,
      "learning_rate": 3.1e-06,
      "loss": 0.0017,
      "step": 140700
    },
    {
      "epoch": 7.504533333333333,
      "grad_norm": 0.3897729218006134,
      "learning_rate": 3.096666666666667e-06,
      "loss": 0.0018,
      "step": 140710
    },
    {
      "epoch": 7.505066666666667,
      "grad_norm": 0.17657536268234253,
      "learning_rate": 3.0933333333333335e-06,
      "loss": 0.0015,
      "step": 140720
    },
    {
      "epoch": 7.5056,
      "grad_norm": 0.2387702763080597,
      "learning_rate": 3.09e-06,
      "loss": 0.0019,
      "step": 140730
    },
    {
      "epoch": 7.5061333333333335,
      "grad_norm": 0.09356880933046341,
      "learning_rate": 3.086666666666667e-06,
      "loss": 0.0014,
      "step": 140740
    },
    {
      "epoch": 7.506666666666667,
      "grad_norm": 0.04352398216724396,
      "learning_rate": 3.0833333333333336e-06,
      "loss": 0.0021,
      "step": 140750
    },
    {
      "epoch": 7.5072,
      "grad_norm": 0.15304216742515564,
      "learning_rate": 3.08e-06,
      "loss": 0.0023,
      "step": 140760
    },
    {
      "epoch": 7.507733333333333,
      "grad_norm": 0.06396497786045074,
      "learning_rate": 3.076666666666667e-06,
      "loss": 0.0018,
      "step": 140770
    },
    {
      "epoch": 7.508266666666667,
      "grad_norm": 0.14898976683616638,
      "learning_rate": 3.0733333333333337e-06,
      "loss": 0.0013,
      "step": 140780
    },
    {
      "epoch": 7.5088,
      "grad_norm": 0.12284751236438751,
      "learning_rate": 3.0700000000000003e-06,
      "loss": 0.0019,
      "step": 140790
    },
    {
      "epoch": 7.509333333333333,
      "grad_norm": 0.03819842264056206,
      "learning_rate": 3.066666666666667e-06,
      "loss": 0.0018,
      "step": 140800
    },
    {
      "epoch": 7.5098666666666665,
      "grad_norm": 0.2037661075592041,
      "learning_rate": 3.0633333333333334e-06,
      "loss": 0.0016,
      "step": 140810
    },
    {
      "epoch": 7.5104,
      "grad_norm": 0.11729411780834198,
      "learning_rate": 3.06e-06,
      "loss": 0.0017,
      "step": 140820
    },
    {
      "epoch": 7.510933333333333,
      "grad_norm": 0.14715203642845154,
      "learning_rate": 3.056666666666667e-06,
      "loss": 0.0011,
      "step": 140830
    },
    {
      "epoch": 7.511466666666666,
      "grad_norm": 0.0775248110294342,
      "learning_rate": 3.0533333333333335e-06,
      "loss": 0.0013,
      "step": 140840
    },
    {
      "epoch": 7.5120000000000005,
      "grad_norm": 0.12345867604017258,
      "learning_rate": 3.05e-06,
      "loss": 0.0023,
      "step": 140850
    },
    {
      "epoch": 7.512533333333334,
      "grad_norm": 0.4395439028739929,
      "learning_rate": 3.0466666666666666e-06,
      "loss": 0.0017,
      "step": 140860
    },
    {
      "epoch": 7.513066666666667,
      "grad_norm": 0.379543662071228,
      "learning_rate": 3.0433333333333336e-06,
      "loss": 0.0024,
      "step": 140870
    },
    {
      "epoch": 7.5136,
      "grad_norm": 0.12357467412948608,
      "learning_rate": 3.04e-06,
      "loss": 0.0017,
      "step": 140880
    },
    {
      "epoch": 7.5141333333333336,
      "grad_norm": 0.3894924819469452,
      "learning_rate": 3.0366666666666667e-06,
      "loss": 0.0016,
      "step": 140890
    },
    {
      "epoch": 7.514666666666667,
      "grad_norm": 0.24566400051116943,
      "learning_rate": 3.0333333333333337e-06,
      "loss": 0.0021,
      "step": 140900
    },
    {
      "epoch": 7.5152,
      "grad_norm": 0.06608562916517258,
      "learning_rate": 3.0300000000000002e-06,
      "loss": 0.0015,
      "step": 140910
    },
    {
      "epoch": 7.515733333333333,
      "grad_norm": 0.27319520711898804,
      "learning_rate": 3.0266666666666668e-06,
      "loss": 0.0011,
      "step": 140920
    },
    {
      "epoch": 7.516266666666667,
      "grad_norm": 0.12762074172496796,
      "learning_rate": 3.0233333333333338e-06,
      "loss": 0.0016,
      "step": 140930
    },
    {
      "epoch": 7.5168,
      "grad_norm": 0.3940688669681549,
      "learning_rate": 3.0200000000000003e-06,
      "loss": 0.0017,
      "step": 140940
    },
    {
      "epoch": 7.517333333333333,
      "grad_norm": 0.3827386796474457,
      "learning_rate": 3.016666666666667e-06,
      "loss": 0.0016,
      "step": 140950
    },
    {
      "epoch": 7.5178666666666665,
      "grad_norm": 0.0333654023706913,
      "learning_rate": 3.0133333333333334e-06,
      "loss": 0.0013,
      "step": 140960
    },
    {
      "epoch": 7.5184,
      "grad_norm": 0.18062624335289001,
      "learning_rate": 3.01e-06,
      "loss": 0.0019,
      "step": 140970
    },
    {
      "epoch": 7.518933333333333,
      "grad_norm": 0.14590564370155334,
      "learning_rate": 3.0066666666666665e-06,
      "loss": 0.0027,
      "step": 140980
    },
    {
      "epoch": 7.519466666666666,
      "grad_norm": 0.4409947991371155,
      "learning_rate": 3.0033333333333335e-06,
      "loss": 0.0014,
      "step": 140990
    },
    {
      "epoch": 7.52,
      "grad_norm": 0.07958512008190155,
      "learning_rate": 3e-06,
      "loss": 0.0013,
      "step": 141000
    },
    {
      "epoch": 7.520533333333333,
      "grad_norm": 0.3020772337913513,
      "learning_rate": 2.9966666666666666e-06,
      "loss": 0.002,
      "step": 141010
    },
    {
      "epoch": 7.521066666666667,
      "grad_norm": 0.06087041273713112,
      "learning_rate": 2.9933333333333336e-06,
      "loss": 0.0013,
      "step": 141020
    },
    {
      "epoch": 7.5216,
      "grad_norm": 0.3212195336818695,
      "learning_rate": 2.99e-06,
      "loss": 0.0021,
      "step": 141030
    },
    {
      "epoch": 7.522133333333334,
      "grad_norm": 0.056348394602537155,
      "learning_rate": 2.9866666666666667e-06,
      "loss": 0.0016,
      "step": 141040
    },
    {
      "epoch": 7.522666666666667,
      "grad_norm": 0.2917163074016571,
      "learning_rate": 2.9833333333333333e-06,
      "loss": 0.0016,
      "step": 141050
    },
    {
      "epoch": 7.5232,
      "grad_norm": 0.03549949824810028,
      "learning_rate": 2.9800000000000003e-06,
      "loss": 0.0019,
      "step": 141060
    },
    {
      "epoch": 7.523733333333333,
      "grad_norm": 0.15332065522670746,
      "learning_rate": 2.976666666666667e-06,
      "loss": 0.0019,
      "step": 141070
    },
    {
      "epoch": 7.524266666666667,
      "grad_norm": 0.20264442265033722,
      "learning_rate": 2.9733333333333334e-06,
      "loss": 0.0021,
      "step": 141080
    },
    {
      "epoch": 7.5248,
      "grad_norm": 0.15463295578956604,
      "learning_rate": 2.9700000000000004e-06,
      "loss": 0.0017,
      "step": 141090
    },
    {
      "epoch": 7.525333333333333,
      "grad_norm": 0.14904922246932983,
      "learning_rate": 2.966666666666667e-06,
      "loss": 0.0018,
      "step": 141100
    },
    {
      "epoch": 7.5258666666666665,
      "grad_norm": 0.2996070086956024,
      "learning_rate": 2.9633333333333335e-06,
      "loss": 0.0019,
      "step": 141110
    },
    {
      "epoch": 7.5264,
      "grad_norm": 0.075217105448246,
      "learning_rate": 2.9600000000000005e-06,
      "loss": 0.0014,
      "step": 141120
    },
    {
      "epoch": 7.526933333333333,
      "grad_norm": 0.24030733108520508,
      "learning_rate": 2.956666666666667e-06,
      "loss": 0.0021,
      "step": 141130
    },
    {
      "epoch": 7.527466666666666,
      "grad_norm": 0.28326207399368286,
      "learning_rate": 2.9533333333333336e-06,
      "loss": 0.0022,
      "step": 141140
    },
    {
      "epoch": 7.5280000000000005,
      "grad_norm": 0.039093777537345886,
      "learning_rate": 2.95e-06,
      "loss": 0.0016,
      "step": 141150
    },
    {
      "epoch": 7.528533333333334,
      "grad_norm": 0.35414478182792664,
      "learning_rate": 2.9466666666666667e-06,
      "loss": 0.0017,
      "step": 141160
    },
    {
      "epoch": 7.529066666666667,
      "grad_norm": 0.3298569917678833,
      "learning_rate": 2.9433333333333332e-06,
      "loss": 0.0016,
      "step": 141170
    },
    {
      "epoch": 7.5296,
      "grad_norm": 0.1840425729751587,
      "learning_rate": 2.9400000000000002e-06,
      "loss": 0.0013,
      "step": 141180
    },
    {
      "epoch": 7.530133333333334,
      "grad_norm": 0.089243583381176,
      "learning_rate": 2.9366666666666668e-06,
      "loss": 0.0013,
      "step": 141190
    },
    {
      "epoch": 7.530666666666667,
      "grad_norm": 0.08211801946163177,
      "learning_rate": 2.9333333333333333e-06,
      "loss": 0.0016,
      "step": 141200
    },
    {
      "epoch": 7.5312,
      "grad_norm": 0.15525349974632263,
      "learning_rate": 2.93e-06,
      "loss": 0.0022,
      "step": 141210
    },
    {
      "epoch": 7.531733333333333,
      "grad_norm": 0.13502754271030426,
      "learning_rate": 2.926666666666667e-06,
      "loss": 0.0012,
      "step": 141220
    },
    {
      "epoch": 7.532266666666667,
      "grad_norm": 0.09448059648275375,
      "learning_rate": 2.9233333333333334e-06,
      "loss": 0.0032,
      "step": 141230
    },
    {
      "epoch": 7.5328,
      "grad_norm": 0.09710956364870071,
      "learning_rate": 2.92e-06,
      "loss": 0.0017,
      "step": 141240
    },
    {
      "epoch": 7.533333333333333,
      "grad_norm": 0.18038935959339142,
      "learning_rate": 2.916666666666667e-06,
      "loss": 0.0017,
      "step": 141250
    },
    {
      "epoch": 7.5338666666666665,
      "grad_norm": 0.12023170292377472,
      "learning_rate": 2.9133333333333335e-06,
      "loss": 0.002,
      "step": 141260
    },
    {
      "epoch": 7.5344,
      "grad_norm": 0.218454048037529,
      "learning_rate": 2.91e-06,
      "loss": 0.0018,
      "step": 141270
    },
    {
      "epoch": 7.534933333333333,
      "grad_norm": 0.2265191674232483,
      "learning_rate": 2.906666666666667e-06,
      "loss": 0.0018,
      "step": 141280
    },
    {
      "epoch": 7.535466666666666,
      "grad_norm": 0.05044509470462799,
      "learning_rate": 2.9033333333333336e-06,
      "loss": 0.0018,
      "step": 141290
    },
    {
      "epoch": 7.536,
      "grad_norm": 0.39384204149246216,
      "learning_rate": 2.9e-06,
      "loss": 0.0016,
      "step": 141300
    },
    {
      "epoch": 7.536533333333333,
      "grad_norm": 0.06754229962825775,
      "learning_rate": 2.896666666666667e-06,
      "loss": 0.0017,
      "step": 141310
    },
    {
      "epoch": 7.537066666666667,
      "grad_norm": 0.09299614280462265,
      "learning_rate": 2.8933333333333333e-06,
      "loss": 0.0015,
      "step": 141320
    },
    {
      "epoch": 7.5376,
      "grad_norm": 0.07475180923938751,
      "learning_rate": 2.89e-06,
      "loss": 0.0015,
      "step": 141330
    },
    {
      "epoch": 7.538133333333334,
      "grad_norm": 0.2832162380218506,
      "learning_rate": 2.886666666666667e-06,
      "loss": 0.0022,
      "step": 141340
    },
    {
      "epoch": 7.538666666666667,
      "grad_norm": 0.16910134255886078,
      "learning_rate": 2.8833333333333334e-06,
      "loss": 0.0015,
      "step": 141350
    },
    {
      "epoch": 7.5392,
      "grad_norm": 0.04156029224395752,
      "learning_rate": 2.88e-06,
      "loss": 0.0016,
      "step": 141360
    },
    {
      "epoch": 7.539733333333333,
      "grad_norm": 0.25409138202667236,
      "learning_rate": 2.876666666666667e-06,
      "loss": 0.0017,
      "step": 141370
    },
    {
      "epoch": 7.540266666666667,
      "grad_norm": 0.07317334413528442,
      "learning_rate": 2.8733333333333335e-06,
      "loss": 0.0017,
      "step": 141380
    },
    {
      "epoch": 7.5408,
      "grad_norm": 0.3397517502307892,
      "learning_rate": 2.87e-06,
      "loss": 0.0019,
      "step": 141390
    },
    {
      "epoch": 7.541333333333333,
      "grad_norm": 0.04780855402350426,
      "learning_rate": 2.8666666666666666e-06,
      "loss": 0.002,
      "step": 141400
    },
    {
      "epoch": 7.5418666666666665,
      "grad_norm": 0.11647611111402512,
      "learning_rate": 2.8633333333333336e-06,
      "loss": 0.0015,
      "step": 141410
    },
    {
      "epoch": 7.5424,
      "grad_norm": 0.25039830803871155,
      "learning_rate": 2.86e-06,
      "loss": 0.0013,
      "step": 141420
    },
    {
      "epoch": 7.542933333333333,
      "grad_norm": 0.2634347975254059,
      "learning_rate": 2.8566666666666667e-06,
      "loss": 0.0023,
      "step": 141430
    },
    {
      "epoch": 7.543466666666666,
      "grad_norm": 0.29154402017593384,
      "learning_rate": 2.8533333333333337e-06,
      "loss": 0.0019,
      "step": 141440
    },
    {
      "epoch": 7.5440000000000005,
      "grad_norm": 0.021387984976172447,
      "learning_rate": 2.8500000000000002e-06,
      "loss": 0.0015,
      "step": 141450
    },
    {
      "epoch": 7.544533333333334,
      "grad_norm": 0.15931132435798645,
      "learning_rate": 2.846666666666667e-06,
      "loss": 0.0018,
      "step": 141460
    },
    {
      "epoch": 7.545066666666667,
      "grad_norm": 0.37973490357398987,
      "learning_rate": 2.8433333333333338e-06,
      "loss": 0.0014,
      "step": 141470
    },
    {
      "epoch": 7.5456,
      "grad_norm": 0.12389860302209854,
      "learning_rate": 2.8400000000000003e-06,
      "loss": 0.0014,
      "step": 141480
    },
    {
      "epoch": 7.546133333333334,
      "grad_norm": 0.17562206089496613,
      "learning_rate": 2.8366666666666665e-06,
      "loss": 0.0024,
      "step": 141490
    },
    {
      "epoch": 7.546666666666667,
      "grad_norm": 0.23355266451835632,
      "learning_rate": 2.8333333333333335e-06,
      "loss": 0.0013,
      "step": 141500
    },
    {
      "epoch": 7.5472,
      "grad_norm": 0.1854160875082016,
      "learning_rate": 2.83e-06,
      "loss": 0.0022,
      "step": 141510
    },
    {
      "epoch": 7.547733333333333,
      "grad_norm": 0.17496225237846375,
      "learning_rate": 2.8266666666666666e-06,
      "loss": 0.0022,
      "step": 141520
    },
    {
      "epoch": 7.548266666666667,
      "grad_norm": 0.12310324609279633,
      "learning_rate": 2.8233333333333335e-06,
      "loss": 0.0016,
      "step": 141530
    },
    {
      "epoch": 7.5488,
      "grad_norm": 0.23488231003284454,
      "learning_rate": 2.82e-06,
      "loss": 0.0015,
      "step": 141540
    },
    {
      "epoch": 7.549333333333333,
      "grad_norm": 0.0737747997045517,
      "learning_rate": 2.8166666666666667e-06,
      "loss": 0.0027,
      "step": 141550
    },
    {
      "epoch": 7.5498666666666665,
      "grad_norm": 0.10202334821224213,
      "learning_rate": 2.8133333333333336e-06,
      "loss": 0.0017,
      "step": 141560
    },
    {
      "epoch": 7.5504,
      "grad_norm": 0.15763050317764282,
      "learning_rate": 2.81e-06,
      "loss": 0.0018,
      "step": 141570
    },
    {
      "epoch": 7.550933333333333,
      "grad_norm": 0.204290971159935,
      "learning_rate": 2.8066666666666668e-06,
      "loss": 0.0021,
      "step": 141580
    },
    {
      "epoch": 7.551466666666666,
      "grad_norm": 0.21196052432060242,
      "learning_rate": 2.8033333333333333e-06,
      "loss": 0.0022,
      "step": 141590
    },
    {
      "epoch": 7.552,
      "grad_norm": 0.24591590464115143,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 0.002,
      "step": 141600
    },
    {
      "epoch": 7.552533333333333,
      "grad_norm": 0.03971906378865242,
      "learning_rate": 2.796666666666667e-06,
      "loss": 0.002,
      "step": 141610
    },
    {
      "epoch": 7.553066666666667,
      "grad_norm": 0.050112076103687286,
      "learning_rate": 2.7933333333333334e-06,
      "loss": 0.0015,
      "step": 141620
    },
    {
      "epoch": 7.5536,
      "grad_norm": 0.2908298671245575,
      "learning_rate": 2.7900000000000004e-06,
      "loss": 0.0017,
      "step": 141630
    },
    {
      "epoch": 7.554133333333334,
      "grad_norm": 0.14888082444667816,
      "learning_rate": 2.786666666666667e-06,
      "loss": 0.0012,
      "step": 141640
    },
    {
      "epoch": 7.554666666666667,
      "grad_norm": 0.03280593082308769,
      "learning_rate": 2.7833333333333335e-06,
      "loss": 0.0016,
      "step": 141650
    },
    {
      "epoch": 7.5552,
      "grad_norm": 0.11885590106248856,
      "learning_rate": 2.78e-06,
      "loss": 0.0025,
      "step": 141660
    },
    {
      "epoch": 7.555733333333333,
      "grad_norm": 0.044515836983919144,
      "learning_rate": 2.7766666666666666e-06,
      "loss": 0.0029,
      "step": 141670
    },
    {
      "epoch": 7.556266666666667,
      "grad_norm": 0.06195642054080963,
      "learning_rate": 2.773333333333333e-06,
      "loss": 0.0018,
      "step": 141680
    },
    {
      "epoch": 7.5568,
      "grad_norm": 0.24630801379680634,
      "learning_rate": 2.77e-06,
      "loss": 0.0022,
      "step": 141690
    },
    {
      "epoch": 7.557333333333333,
      "grad_norm": 0.08397066593170166,
      "learning_rate": 2.7666666666666667e-06,
      "loss": 0.002,
      "step": 141700
    },
    {
      "epoch": 7.5578666666666665,
      "grad_norm": 0.12070725858211517,
      "learning_rate": 2.7633333333333333e-06,
      "loss": 0.0016,
      "step": 141710
    },
    {
      "epoch": 7.5584,
      "grad_norm": 0.2570999264717102,
      "learning_rate": 2.7600000000000003e-06,
      "loss": 0.0024,
      "step": 141720
    },
    {
      "epoch": 7.558933333333333,
      "grad_norm": 0.049591563642024994,
      "learning_rate": 2.756666666666667e-06,
      "loss": 0.0022,
      "step": 141730
    },
    {
      "epoch": 7.559466666666666,
      "grad_norm": 0.21198952198028564,
      "learning_rate": 2.7533333333333334e-06,
      "loss": 0.0015,
      "step": 141740
    },
    {
      "epoch": 7.5600000000000005,
      "grad_norm": 0.29189398884773254,
      "learning_rate": 2.7500000000000004e-06,
      "loss": 0.0014,
      "step": 141750
    },
    {
      "epoch": 7.560533333333334,
      "grad_norm": 0.03687845543026924,
      "learning_rate": 2.746666666666667e-06,
      "loss": 0.002,
      "step": 141760
    },
    {
      "epoch": 7.561066666666667,
      "grad_norm": 0.08950081467628479,
      "learning_rate": 2.7433333333333335e-06,
      "loss": 0.0025,
      "step": 141770
    },
    {
      "epoch": 7.5616,
      "grad_norm": 0.0975804254412651,
      "learning_rate": 2.74e-06,
      "loss": 0.0014,
      "step": 141780
    },
    {
      "epoch": 7.562133333333334,
      "grad_norm": 0.15832188725471497,
      "learning_rate": 2.736666666666667e-06,
      "loss": 0.0021,
      "step": 141790
    },
    {
      "epoch": 7.562666666666667,
      "grad_norm": 0.12348000705242157,
      "learning_rate": 2.7333333333333336e-06,
      "loss": 0.0024,
      "step": 141800
    },
    {
      "epoch": 7.5632,
      "grad_norm": 0.30572327971458435,
      "learning_rate": 2.73e-06,
      "loss": 0.0013,
      "step": 141810
    },
    {
      "epoch": 7.563733333333333,
      "grad_norm": 0.18232397735118866,
      "learning_rate": 2.726666666666667e-06,
      "loss": 0.0018,
      "step": 141820
    },
    {
      "epoch": 7.564266666666667,
      "grad_norm": 0.08353599905967712,
      "learning_rate": 2.7233333333333332e-06,
      "loss": 0.0021,
      "step": 141830
    },
    {
      "epoch": 7.5648,
      "grad_norm": 0.32552650570869446,
      "learning_rate": 2.72e-06,
      "loss": 0.0021,
      "step": 141840
    },
    {
      "epoch": 7.565333333333333,
      "grad_norm": 0.13132542371749878,
      "learning_rate": 2.7166666666666668e-06,
      "loss": 0.0013,
      "step": 141850
    },
    {
      "epoch": 7.5658666666666665,
      "grad_norm": 0.24912896752357483,
      "learning_rate": 2.7133333333333333e-06,
      "loss": 0.0017,
      "step": 141860
    },
    {
      "epoch": 7.5664,
      "grad_norm": 0.3393976390361786,
      "learning_rate": 2.71e-06,
      "loss": 0.0012,
      "step": 141870
    },
    {
      "epoch": 7.566933333333333,
      "grad_norm": 0.06437648087739944,
      "learning_rate": 2.706666666666667e-06,
      "loss": 0.0015,
      "step": 141880
    },
    {
      "epoch": 7.567466666666666,
      "grad_norm": 0.305219441652298,
      "learning_rate": 2.7033333333333334e-06,
      "loss": 0.0018,
      "step": 141890
    },
    {
      "epoch": 7.568,
      "grad_norm": 0.061240315437316895,
      "learning_rate": 2.7e-06,
      "loss": 0.0018,
      "step": 141900
    },
    {
      "epoch": 7.568533333333333,
      "grad_norm": 0.2111152857542038,
      "learning_rate": 2.696666666666667e-06,
      "loss": 0.0011,
      "step": 141910
    },
    {
      "epoch": 7.569066666666667,
      "grad_norm": 0.4598327875137329,
      "learning_rate": 2.6933333333333335e-06,
      "loss": 0.0019,
      "step": 141920
    },
    {
      "epoch": 7.5696,
      "grad_norm": 0.0646243691444397,
      "learning_rate": 2.69e-06,
      "loss": 0.0018,
      "step": 141930
    },
    {
      "epoch": 7.570133333333334,
      "grad_norm": 0.07364543527364731,
      "learning_rate": 2.6866666666666666e-06,
      "loss": 0.003,
      "step": 141940
    },
    {
      "epoch": 7.570666666666667,
      "grad_norm": 0.14050105214118958,
      "learning_rate": 2.6833333333333336e-06,
      "loss": 0.0018,
      "step": 141950
    },
    {
      "epoch": 7.5712,
      "grad_norm": 0.22704561054706573,
      "learning_rate": 2.68e-06,
      "loss": 0.002,
      "step": 141960
    },
    {
      "epoch": 7.571733333333333,
      "grad_norm": 0.3845520317554474,
      "learning_rate": 2.6766666666666667e-06,
      "loss": 0.0019,
      "step": 141970
    },
    {
      "epoch": 7.572266666666667,
      "grad_norm": 0.10269128531217575,
      "learning_rate": 2.6733333333333337e-06,
      "loss": 0.0016,
      "step": 141980
    },
    {
      "epoch": 7.5728,
      "grad_norm": 0.09854969382286072,
      "learning_rate": 2.6700000000000003e-06,
      "loss": 0.0014,
      "step": 141990
    },
    {
      "epoch": 7.573333333333333,
      "grad_norm": 0.3331631124019623,
      "learning_rate": 2.666666666666667e-06,
      "loss": 0.0019,
      "step": 142000
    },
    {
      "epoch": 7.5738666666666665,
      "grad_norm": 0.18880029022693634,
      "learning_rate": 2.6633333333333334e-06,
      "loss": 0.0024,
      "step": 142010
    },
    {
      "epoch": 7.5744,
      "grad_norm": 0.07271787524223328,
      "learning_rate": 2.66e-06,
      "loss": 0.002,
      "step": 142020
    },
    {
      "epoch": 7.574933333333333,
      "grad_norm": 0.08144329488277435,
      "learning_rate": 2.6566666666666665e-06,
      "loss": 0.0015,
      "step": 142030
    },
    {
      "epoch": 7.575466666666666,
      "grad_norm": 0.14547045528888702,
      "learning_rate": 2.6533333333333335e-06,
      "loss": 0.0014,
      "step": 142040
    },
    {
      "epoch": 7.576,
      "grad_norm": 0.4733736217021942,
      "learning_rate": 2.65e-06,
      "loss": 0.0017,
      "step": 142050
    },
    {
      "epoch": 7.576533333333334,
      "grad_norm": 0.35437333583831787,
      "learning_rate": 2.6466666666666666e-06,
      "loss": 0.0032,
      "step": 142060
    },
    {
      "epoch": 7.577066666666667,
      "grad_norm": 0.3032277822494507,
      "learning_rate": 2.6433333333333336e-06,
      "loss": 0.0013,
      "step": 142070
    },
    {
      "epoch": 7.5776,
      "grad_norm": 0.12180851399898529,
      "learning_rate": 2.64e-06,
      "loss": 0.0015,
      "step": 142080
    },
    {
      "epoch": 7.578133333333334,
      "grad_norm": 0.2679997682571411,
      "learning_rate": 2.6366666666666667e-06,
      "loss": 0.0014,
      "step": 142090
    },
    {
      "epoch": 7.578666666666667,
      "grad_norm": 0.06720362603664398,
      "learning_rate": 2.6333333333333337e-06,
      "loss": 0.0014,
      "step": 142100
    },
    {
      "epoch": 7.5792,
      "grad_norm": 0.3137960433959961,
      "learning_rate": 2.6300000000000002e-06,
      "loss": 0.0014,
      "step": 142110
    },
    {
      "epoch": 7.579733333333333,
      "grad_norm": 0.26807576417922974,
      "learning_rate": 2.6266666666666668e-06,
      "loss": 0.0021,
      "step": 142120
    },
    {
      "epoch": 7.580266666666667,
      "grad_norm": 0.03839874640107155,
      "learning_rate": 2.6233333333333333e-06,
      "loss": 0.0021,
      "step": 142130
    },
    {
      "epoch": 7.5808,
      "grad_norm": 0.4007831811904907,
      "learning_rate": 2.6200000000000003e-06,
      "loss": 0.0014,
      "step": 142140
    },
    {
      "epoch": 7.581333333333333,
      "grad_norm": 0.414103239774704,
      "learning_rate": 2.616666666666667e-06,
      "loss": 0.0014,
      "step": 142150
    },
    {
      "epoch": 7.5818666666666665,
      "grad_norm": 0.4954266846179962,
      "learning_rate": 2.6133333333333334e-06,
      "loss": 0.002,
      "step": 142160
    },
    {
      "epoch": 7.5824,
      "grad_norm": 0.21771520376205444,
      "learning_rate": 2.6100000000000004e-06,
      "loss": 0.003,
      "step": 142170
    },
    {
      "epoch": 7.582933333333333,
      "grad_norm": 0.29035642743110657,
      "learning_rate": 2.6066666666666666e-06,
      "loss": 0.0015,
      "step": 142180
    },
    {
      "epoch": 7.583466666666666,
      "grad_norm": 0.4199609160423279,
      "learning_rate": 2.603333333333333e-06,
      "loss": 0.0022,
      "step": 142190
    },
    {
      "epoch": 7.584,
      "grad_norm": 0.09147345274686813,
      "learning_rate": 2.6e-06,
      "loss": 0.0028,
      "step": 142200
    },
    {
      "epoch": 7.584533333333333,
      "grad_norm": 0.2903316915035248,
      "learning_rate": 2.5966666666666667e-06,
      "loss": 0.0019,
      "step": 142210
    },
    {
      "epoch": 7.585066666666666,
      "grad_norm": 0.1797621250152588,
      "learning_rate": 2.593333333333333e-06,
      "loss": 0.0023,
      "step": 142220
    },
    {
      "epoch": 7.5856,
      "grad_norm": 0.2703254520893097,
      "learning_rate": 2.59e-06,
      "loss": 0.0023,
      "step": 142230
    },
    {
      "epoch": 7.586133333333334,
      "grad_norm": 0.1295788288116455,
      "learning_rate": 2.5866666666666667e-06,
      "loss": 0.0018,
      "step": 142240
    },
    {
      "epoch": 7.586666666666667,
      "grad_norm": 0.34996291995048523,
      "learning_rate": 2.5833333333333333e-06,
      "loss": 0.0012,
      "step": 142250
    },
    {
      "epoch": 7.5872,
      "grad_norm": 0.11947013437747955,
      "learning_rate": 2.5800000000000003e-06,
      "loss": 0.0015,
      "step": 142260
    },
    {
      "epoch": 7.587733333333333,
      "grad_norm": 0.18284384906291962,
      "learning_rate": 2.576666666666667e-06,
      "loss": 0.0019,
      "step": 142270
    },
    {
      "epoch": 7.588266666666667,
      "grad_norm": 0.1760232001543045,
      "learning_rate": 2.5733333333333334e-06,
      "loss": 0.0015,
      "step": 142280
    },
    {
      "epoch": 7.5888,
      "grad_norm": 0.03389457240700722,
      "learning_rate": 2.5700000000000004e-06,
      "loss": 0.0016,
      "step": 142290
    },
    {
      "epoch": 7.589333333333333,
      "grad_norm": 0.21016782522201538,
      "learning_rate": 2.566666666666667e-06,
      "loss": 0.0013,
      "step": 142300
    },
    {
      "epoch": 7.5898666666666665,
      "grad_norm": 0.20487439632415771,
      "learning_rate": 2.5633333333333335e-06,
      "loss": 0.0014,
      "step": 142310
    },
    {
      "epoch": 7.5904,
      "grad_norm": 0.04608483985066414,
      "learning_rate": 2.56e-06,
      "loss": 0.0014,
      "step": 142320
    },
    {
      "epoch": 7.590933333333333,
      "grad_norm": 0.2656662166118622,
      "learning_rate": 2.556666666666667e-06,
      "loss": 0.0013,
      "step": 142330
    },
    {
      "epoch": 7.591466666666666,
      "grad_norm": 0.07863440364599228,
      "learning_rate": 2.5533333333333336e-06,
      "loss": 0.0016,
      "step": 142340
    },
    {
      "epoch": 7.592,
      "grad_norm": 0.10209792852401733,
      "learning_rate": 2.55e-06,
      "loss": 0.0022,
      "step": 142350
    },
    {
      "epoch": 7.592533333333334,
      "grad_norm": 0.06628484278917313,
      "learning_rate": 2.5466666666666667e-06,
      "loss": 0.0019,
      "step": 142360
    },
    {
      "epoch": 7.593066666666667,
      "grad_norm": 0.2923906147480011,
      "learning_rate": 2.5433333333333333e-06,
      "loss": 0.0014,
      "step": 142370
    },
    {
      "epoch": 7.5936,
      "grad_norm": 0.049145620316267014,
      "learning_rate": 2.54e-06,
      "loss": 0.0015,
      "step": 142380
    },
    {
      "epoch": 7.594133333333334,
      "grad_norm": 0.1565518081188202,
      "learning_rate": 2.536666666666667e-06,
      "loss": 0.0014,
      "step": 142390
    },
    {
      "epoch": 7.594666666666667,
      "grad_norm": 0.19413097202777863,
      "learning_rate": 2.5333333333333334e-06,
      "loss": 0.0015,
      "step": 142400
    },
    {
      "epoch": 7.5952,
      "grad_norm": 0.06455281376838684,
      "learning_rate": 2.53e-06,
      "loss": 0.0012,
      "step": 142410
    },
    {
      "epoch": 7.5957333333333334,
      "grad_norm": 0.720630943775177,
      "learning_rate": 2.526666666666667e-06,
      "loss": 0.0016,
      "step": 142420
    },
    {
      "epoch": 7.596266666666667,
      "grad_norm": 0.29070737957954407,
      "learning_rate": 2.5233333333333335e-06,
      "loss": 0.0013,
      "step": 142430
    },
    {
      "epoch": 7.5968,
      "grad_norm": 0.049434784799814224,
      "learning_rate": 2.52e-06,
      "loss": 0.0013,
      "step": 142440
    },
    {
      "epoch": 7.597333333333333,
      "grad_norm": 0.04529137536883354,
      "learning_rate": 2.516666666666667e-06,
      "loss": 0.0014,
      "step": 142450
    },
    {
      "epoch": 7.5978666666666665,
      "grad_norm": 0.2628084719181061,
      "learning_rate": 2.5133333333333336e-06,
      "loss": 0.0016,
      "step": 142460
    },
    {
      "epoch": 7.5984,
      "grad_norm": 0.2118045538663864,
      "learning_rate": 2.51e-06,
      "loss": 0.0012,
      "step": 142470
    },
    {
      "epoch": 7.598933333333333,
      "grad_norm": 0.046879541128873825,
      "learning_rate": 2.506666666666667e-06,
      "loss": 0.0022,
      "step": 142480
    },
    {
      "epoch": 7.599466666666666,
      "grad_norm": 0.19442835450172424,
      "learning_rate": 2.5033333333333336e-06,
      "loss": 0.0013,
      "step": 142490
    },
    {
      "epoch": 7.6,
      "grad_norm": 0.23776665329933167,
      "learning_rate": 2.5e-06,
      "loss": 0.0023,
      "step": 142500
    },
    {
      "epoch": 7.600533333333333,
      "grad_norm": 0.1307458132505417,
      "learning_rate": 2.4966666666666668e-06,
      "loss": 0.0015,
      "step": 142510
    },
    {
      "epoch": 7.601066666666666,
      "grad_norm": 0.07151826471090317,
      "learning_rate": 2.4933333333333333e-06,
      "loss": 0.0025,
      "step": 142520
    },
    {
      "epoch": 7.6016,
      "grad_norm": 0.14566540718078613,
      "learning_rate": 2.49e-06,
      "loss": 0.0015,
      "step": 142530
    },
    {
      "epoch": 7.602133333333334,
      "grad_norm": 0.16744673252105713,
      "learning_rate": 2.486666666666667e-06,
      "loss": 0.0015,
      "step": 142540
    },
    {
      "epoch": 7.602666666666667,
      "grad_norm": 0.06758888810873032,
      "learning_rate": 2.4833333333333334e-06,
      "loss": 0.0017,
      "step": 142550
    },
    {
      "epoch": 7.6032,
      "grad_norm": 0.20534437894821167,
      "learning_rate": 2.48e-06,
      "loss": 0.0012,
      "step": 142560
    },
    {
      "epoch": 7.6037333333333335,
      "grad_norm": 0.05438432842493057,
      "learning_rate": 2.4766666666666665e-06,
      "loss": 0.0014,
      "step": 142570
    },
    {
      "epoch": 7.604266666666667,
      "grad_norm": 0.24443171918392181,
      "learning_rate": 2.4733333333333335e-06,
      "loss": 0.0018,
      "step": 142580
    },
    {
      "epoch": 7.6048,
      "grad_norm": 0.26955804228782654,
      "learning_rate": 2.47e-06,
      "loss": 0.0019,
      "step": 142590
    },
    {
      "epoch": 7.605333333333333,
      "grad_norm": 0.13011282682418823,
      "learning_rate": 2.4666666666666666e-06,
      "loss": 0.0016,
      "step": 142600
    },
    {
      "epoch": 7.6058666666666666,
      "grad_norm": 0.0387912280857563,
      "learning_rate": 2.4633333333333336e-06,
      "loss": 0.0013,
      "step": 142610
    },
    {
      "epoch": 7.6064,
      "grad_norm": 0.05671408399939537,
      "learning_rate": 2.46e-06,
      "loss": 0.0013,
      "step": 142620
    },
    {
      "epoch": 7.606933333333333,
      "grad_norm": 0.39897608757019043,
      "learning_rate": 2.4566666666666667e-06,
      "loss": 0.0015,
      "step": 142630
    },
    {
      "epoch": 7.607466666666666,
      "grad_norm": 0.09726765006780624,
      "learning_rate": 2.4533333333333337e-06,
      "loss": 0.0013,
      "step": 142640
    },
    {
      "epoch": 7.608,
      "grad_norm": 0.06608634442090988,
      "learning_rate": 2.4500000000000003e-06,
      "loss": 0.0017,
      "step": 142650
    },
    {
      "epoch": 7.608533333333334,
      "grad_norm": 0.2881722152233124,
      "learning_rate": 2.446666666666667e-06,
      "loss": 0.0025,
      "step": 142660
    },
    {
      "epoch": 7.609066666666667,
      "grad_norm": 0.04119396209716797,
      "learning_rate": 2.4433333333333334e-06,
      "loss": 0.002,
      "step": 142670
    },
    {
      "epoch": 7.6096,
      "grad_norm": 0.15146593749523163,
      "learning_rate": 2.4400000000000004e-06,
      "loss": 0.0013,
      "step": 142680
    },
    {
      "epoch": 7.610133333333334,
      "grad_norm": 0.19446787238121033,
      "learning_rate": 2.4366666666666665e-06,
      "loss": 0.0015,
      "step": 142690
    },
    {
      "epoch": 7.610666666666667,
      "grad_norm": 0.15804935991764069,
      "learning_rate": 2.4333333333333335e-06,
      "loss": 0.0029,
      "step": 142700
    },
    {
      "epoch": 7.6112,
      "grad_norm": 0.09778128564357758,
      "learning_rate": 2.43e-06,
      "loss": 0.0018,
      "step": 142710
    },
    {
      "epoch": 7.6117333333333335,
      "grad_norm": 0.07571332901716232,
      "learning_rate": 2.4266666666666666e-06,
      "loss": 0.0016,
      "step": 142720
    },
    {
      "epoch": 7.612266666666667,
      "grad_norm": 0.2932055592536926,
      "learning_rate": 2.4233333333333336e-06,
      "loss": 0.0015,
      "step": 142730
    },
    {
      "epoch": 7.6128,
      "grad_norm": 0.11870633810758591,
      "learning_rate": 2.42e-06,
      "loss": 0.0017,
      "step": 142740
    },
    {
      "epoch": 7.613333333333333,
      "grad_norm": 0.1961331069469452,
      "learning_rate": 2.4166666666666667e-06,
      "loss": 0.0019,
      "step": 142750
    },
    {
      "epoch": 7.613866666666667,
      "grad_norm": 0.21520517766475677,
      "learning_rate": 2.4133333333333332e-06,
      "loss": 0.0016,
      "step": 142760
    },
    {
      "epoch": 7.6144,
      "grad_norm": 0.32146239280700684,
      "learning_rate": 2.4100000000000002e-06,
      "loss": 0.0022,
      "step": 142770
    },
    {
      "epoch": 7.614933333333333,
      "grad_norm": 0.21002256870269775,
      "learning_rate": 2.4066666666666668e-06,
      "loss": 0.0015,
      "step": 142780
    },
    {
      "epoch": 7.615466666666666,
      "grad_norm": 0.1277122050523758,
      "learning_rate": 2.4033333333333333e-06,
      "loss": 0.002,
      "step": 142790
    },
    {
      "epoch": 7.616,
      "grad_norm": 0.23847638070583344,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 0.002,
      "step": 142800
    },
    {
      "epoch": 7.616533333333333,
      "grad_norm": 0.24337248504161835,
      "learning_rate": 2.396666666666667e-06,
      "loss": 0.0027,
      "step": 142810
    },
    {
      "epoch": 7.617066666666666,
      "grad_norm": 0.295928418636322,
      "learning_rate": 2.3933333333333334e-06,
      "loss": 0.0026,
      "step": 142820
    },
    {
      "epoch": 7.6176,
      "grad_norm": 0.11753956228494644,
      "learning_rate": 2.3900000000000004e-06,
      "loss": 0.0011,
      "step": 142830
    },
    {
      "epoch": 7.618133333333334,
      "grad_norm": 0.41923439502716064,
      "learning_rate": 2.386666666666667e-06,
      "loss": 0.0021,
      "step": 142840
    },
    {
      "epoch": 7.618666666666667,
      "grad_norm": 0.5295662879943848,
      "learning_rate": 2.3833333333333335e-06,
      "loss": 0.0024,
      "step": 142850
    },
    {
      "epoch": 7.6192,
      "grad_norm": 0.08477512747049332,
      "learning_rate": 2.38e-06,
      "loss": 0.0011,
      "step": 142860
    },
    {
      "epoch": 7.6197333333333335,
      "grad_norm": 0.05311726778745651,
      "learning_rate": 2.3766666666666666e-06,
      "loss": 0.0013,
      "step": 142870
    },
    {
      "epoch": 7.620266666666667,
      "grad_norm": 0.12033752351999283,
      "learning_rate": 2.373333333333333e-06,
      "loss": 0.0016,
      "step": 142880
    },
    {
      "epoch": 7.6208,
      "grad_norm": 0.23761990666389465,
      "learning_rate": 2.37e-06,
      "loss": 0.0012,
      "step": 142890
    },
    {
      "epoch": 7.621333333333333,
      "grad_norm": 0.1928909420967102,
      "learning_rate": 2.3666666666666667e-06,
      "loss": 0.002,
      "step": 142900
    },
    {
      "epoch": 7.621866666666667,
      "grad_norm": 0.06447067856788635,
      "learning_rate": 2.3633333333333333e-06,
      "loss": 0.0026,
      "step": 142910
    },
    {
      "epoch": 7.6224,
      "grad_norm": 0.34892627596855164,
      "learning_rate": 2.36e-06,
      "loss": 0.0026,
      "step": 142920
    },
    {
      "epoch": 7.622933333333333,
      "grad_norm": 0.32841217517852783,
      "learning_rate": 2.356666666666667e-06,
      "loss": 0.0021,
      "step": 142930
    },
    {
      "epoch": 7.623466666666666,
      "grad_norm": 0.17290791869163513,
      "learning_rate": 2.3533333333333334e-06,
      "loss": 0.0021,
      "step": 142940
    },
    {
      "epoch": 7.624,
      "grad_norm": 0.24287953972816467,
      "learning_rate": 2.35e-06,
      "loss": 0.0024,
      "step": 142950
    },
    {
      "epoch": 7.624533333333334,
      "grad_norm": 0.18563809990882874,
      "learning_rate": 2.346666666666667e-06,
      "loss": 0.0012,
      "step": 142960
    },
    {
      "epoch": 7.625066666666667,
      "grad_norm": 0.14719247817993164,
      "learning_rate": 2.3433333333333335e-06,
      "loss": 0.002,
      "step": 142970
    },
    {
      "epoch": 7.6256,
      "grad_norm": 0.14665859937667847,
      "learning_rate": 2.34e-06,
      "loss": 0.0021,
      "step": 142980
    },
    {
      "epoch": 7.626133333333334,
      "grad_norm": 0.045366909354925156,
      "learning_rate": 2.336666666666667e-06,
      "loss": 0.0021,
      "step": 142990
    },
    {
      "epoch": 7.626666666666667,
      "grad_norm": 0.12205979228019714,
      "learning_rate": 2.3333333333333336e-06,
      "loss": 0.0013,
      "step": 143000
    },
    {
      "epoch": 7.6272,
      "grad_norm": 0.13678152859210968,
      "learning_rate": 2.33e-06,
      "loss": 0.0017,
      "step": 143010
    },
    {
      "epoch": 7.6277333333333335,
      "grad_norm": 0.33753108978271484,
      "learning_rate": 2.326666666666667e-06,
      "loss": 0.0014,
      "step": 143020
    },
    {
      "epoch": 7.628266666666667,
      "grad_norm": 0.24199211597442627,
      "learning_rate": 2.3233333333333337e-06,
      "loss": 0.0018,
      "step": 143030
    },
    {
      "epoch": 7.6288,
      "grad_norm": 0.3035167157649994,
      "learning_rate": 2.32e-06,
      "loss": 0.0015,
      "step": 143040
    },
    {
      "epoch": 7.629333333333333,
      "grad_norm": 0.2728695273399353,
      "learning_rate": 2.316666666666667e-06,
      "loss": 0.0018,
      "step": 143050
    },
    {
      "epoch": 7.629866666666667,
      "grad_norm": 0.12139206379652023,
      "learning_rate": 2.3133333333333333e-06,
      "loss": 0.0015,
      "step": 143060
    },
    {
      "epoch": 7.6304,
      "grad_norm": 0.2153567373752594,
      "learning_rate": 2.31e-06,
      "loss": 0.0017,
      "step": 143070
    },
    {
      "epoch": 7.630933333333333,
      "grad_norm": 0.10402146726846695,
      "learning_rate": 2.306666666666667e-06,
      "loss": 0.002,
      "step": 143080
    },
    {
      "epoch": 7.631466666666666,
      "grad_norm": 0.2275317907333374,
      "learning_rate": 2.3033333333333334e-06,
      "loss": 0.0019,
      "step": 143090
    },
    {
      "epoch": 7.632,
      "grad_norm": 0.0642869845032692,
      "learning_rate": 2.3e-06,
      "loss": 0.0018,
      "step": 143100
    },
    {
      "epoch": 7.632533333333333,
      "grad_norm": 0.18640108406543732,
      "learning_rate": 2.2966666666666666e-06,
      "loss": 0.0019,
      "step": 143110
    },
    {
      "epoch": 7.633066666666666,
      "grad_norm": 0.43084725737571716,
      "learning_rate": 2.2933333333333335e-06,
      "loss": 0.0016,
      "step": 143120
    },
    {
      "epoch": 7.6336,
      "grad_norm": 0.2668900191783905,
      "learning_rate": 2.29e-06,
      "loss": 0.0019,
      "step": 143130
    },
    {
      "epoch": 7.634133333333334,
      "grad_norm": 0.1493319869041443,
      "learning_rate": 2.2866666666666667e-06,
      "loss": 0.002,
      "step": 143140
    },
    {
      "epoch": 7.634666666666667,
      "grad_norm": 0.33410876989364624,
      "learning_rate": 2.2833333333333336e-06,
      "loss": 0.0015,
      "step": 143150
    },
    {
      "epoch": 7.6352,
      "grad_norm": 0.35282596945762634,
      "learning_rate": 2.28e-06,
      "loss": 0.0012,
      "step": 143160
    },
    {
      "epoch": 7.6357333333333335,
      "grad_norm": 0.19904972612857819,
      "learning_rate": 2.2766666666666668e-06,
      "loss": 0.0017,
      "step": 143170
    },
    {
      "epoch": 7.636266666666667,
      "grad_norm": 0.3303945064544678,
      "learning_rate": 2.2733333333333337e-06,
      "loss": 0.0023,
      "step": 143180
    },
    {
      "epoch": 7.6368,
      "grad_norm": 0.1698630452156067,
      "learning_rate": 2.2700000000000003e-06,
      "loss": 0.0023,
      "step": 143190
    },
    {
      "epoch": 7.637333333333333,
      "grad_norm": 0.11896093189716339,
      "learning_rate": 2.266666666666667e-06,
      "loss": 0.0016,
      "step": 143200
    },
    {
      "epoch": 7.637866666666667,
      "grad_norm": 0.18459436297416687,
      "learning_rate": 2.2633333333333334e-06,
      "loss": 0.0021,
      "step": 143210
    },
    {
      "epoch": 7.6384,
      "grad_norm": 0.5409947037696838,
      "learning_rate": 2.26e-06,
      "loss": 0.002,
      "step": 143220
    },
    {
      "epoch": 7.638933333333333,
      "grad_norm": 0.25571927428245544,
      "learning_rate": 2.2566666666666665e-06,
      "loss": 0.002,
      "step": 143230
    },
    {
      "epoch": 7.639466666666666,
      "grad_norm": 0.1370282918214798,
      "learning_rate": 2.2533333333333335e-06,
      "loss": 0.0016,
      "step": 143240
    },
    {
      "epoch": 7.64,
      "grad_norm": 0.041246019303798676,
      "learning_rate": 2.25e-06,
      "loss": 0.0021,
      "step": 143250
    },
    {
      "epoch": 7.640533333333333,
      "grad_norm": 0.17456597089767456,
      "learning_rate": 2.2466666666666666e-06,
      "loss": 0.0016,
      "step": 143260
    },
    {
      "epoch": 7.641066666666667,
      "grad_norm": 0.40486884117126465,
      "learning_rate": 2.2433333333333336e-06,
      "loss": 0.002,
      "step": 143270
    },
    {
      "epoch": 7.6416,
      "grad_norm": 0.21027177572250366,
      "learning_rate": 2.24e-06,
      "loss": 0.0015,
      "step": 143280
    },
    {
      "epoch": 7.642133333333334,
      "grad_norm": 0.3531660735607147,
      "learning_rate": 2.2366666666666667e-06,
      "loss": 0.0017,
      "step": 143290
    },
    {
      "epoch": 7.642666666666667,
      "grad_norm": 0.06378520280122757,
      "learning_rate": 2.2333333333333333e-06,
      "loss": 0.0017,
      "step": 143300
    },
    {
      "epoch": 7.6432,
      "grad_norm": 0.0956917256116867,
      "learning_rate": 2.2300000000000002e-06,
      "loss": 0.0011,
      "step": 143310
    },
    {
      "epoch": 7.6437333333333335,
      "grad_norm": 0.15591207146644592,
      "learning_rate": 2.226666666666667e-06,
      "loss": 0.0013,
      "step": 143320
    },
    {
      "epoch": 7.644266666666667,
      "grad_norm": 0.11857141554355621,
      "learning_rate": 2.2233333333333334e-06,
      "loss": 0.0014,
      "step": 143330
    },
    {
      "epoch": 7.6448,
      "grad_norm": 0.41287127137184143,
      "learning_rate": 2.2200000000000003e-06,
      "loss": 0.0014,
      "step": 143340
    },
    {
      "epoch": 7.645333333333333,
      "grad_norm": 0.0620715394616127,
      "learning_rate": 2.216666666666667e-06,
      "loss": 0.002,
      "step": 143350
    },
    {
      "epoch": 7.645866666666667,
      "grad_norm": 0.0769384428858757,
      "learning_rate": 2.2133333333333335e-06,
      "loss": 0.0014,
      "step": 143360
    },
    {
      "epoch": 7.6464,
      "grad_norm": 0.09958058595657349,
      "learning_rate": 2.2100000000000004e-06,
      "loss": 0.0013,
      "step": 143370
    },
    {
      "epoch": 7.646933333333333,
      "grad_norm": 0.35201627016067505,
      "learning_rate": 2.2066666666666666e-06,
      "loss": 0.0023,
      "step": 143380
    },
    {
      "epoch": 7.647466666666666,
      "grad_norm": 0.35511514544487,
      "learning_rate": 2.203333333333333e-06,
      "loss": 0.0012,
      "step": 143390
    },
    {
      "epoch": 7.648,
      "grad_norm": 0.1461666226387024,
      "learning_rate": 2.2e-06,
      "loss": 0.0017,
      "step": 143400
    },
    {
      "epoch": 7.648533333333333,
      "grad_norm": 0.22733651101589203,
      "learning_rate": 2.1966666666666667e-06,
      "loss": 0.0016,
      "step": 143410
    },
    {
      "epoch": 7.649066666666666,
      "grad_norm": 0.07081855833530426,
      "learning_rate": 2.1933333333333332e-06,
      "loss": 0.0031,
      "step": 143420
    },
    {
      "epoch": 7.6495999999999995,
      "grad_norm": 0.496695876121521,
      "learning_rate": 2.19e-06,
      "loss": 0.0017,
      "step": 143430
    },
    {
      "epoch": 7.650133333333334,
      "grad_norm": 0.1292043924331665,
      "learning_rate": 2.1866666666666668e-06,
      "loss": 0.0014,
      "step": 143440
    },
    {
      "epoch": 7.650666666666667,
      "grad_norm": 0.22600017488002777,
      "learning_rate": 2.1833333333333333e-06,
      "loss": 0.0021,
      "step": 143450
    },
    {
      "epoch": 7.6512,
      "grad_norm": 0.24199584126472473,
      "learning_rate": 2.1800000000000003e-06,
      "loss": 0.0017,
      "step": 143460
    },
    {
      "epoch": 7.6517333333333335,
      "grad_norm": 0.1751183718442917,
      "learning_rate": 2.176666666666667e-06,
      "loss": 0.0013,
      "step": 143470
    },
    {
      "epoch": 7.652266666666667,
      "grad_norm": 0.043176669627428055,
      "learning_rate": 2.1733333333333334e-06,
      "loss": 0.0014,
      "step": 143480
    },
    {
      "epoch": 7.6528,
      "grad_norm": 0.2434610277414322,
      "learning_rate": 2.17e-06,
      "loss": 0.0015,
      "step": 143490
    },
    {
      "epoch": 7.653333333333333,
      "grad_norm": 0.17713625729084015,
      "learning_rate": 2.166666666666667e-06,
      "loss": 0.0014,
      "step": 143500
    },
    {
      "epoch": 7.653866666666667,
      "grad_norm": 0.03894171118736267,
      "learning_rate": 2.1633333333333335e-06,
      "loss": 0.0024,
      "step": 143510
    },
    {
      "epoch": 7.6544,
      "grad_norm": 0.13604256510734558,
      "learning_rate": 2.16e-06,
      "loss": 0.002,
      "step": 143520
    },
    {
      "epoch": 7.654933333333333,
      "grad_norm": 0.1538066267967224,
      "learning_rate": 2.156666666666667e-06,
      "loss": 0.0014,
      "step": 143530
    },
    {
      "epoch": 7.655466666666666,
      "grad_norm": 0.14930805563926697,
      "learning_rate": 2.1533333333333336e-06,
      "loss": 0.0025,
      "step": 143540
    },
    {
      "epoch": 7.656,
      "grad_norm": 0.08370815217494965,
      "learning_rate": 2.1499999999999997e-06,
      "loss": 0.0018,
      "step": 143550
    },
    {
      "epoch": 7.656533333333333,
      "grad_norm": 0.2914300262928009,
      "learning_rate": 2.1466666666666667e-06,
      "loss": 0.0016,
      "step": 143560
    },
    {
      "epoch": 7.657066666666667,
      "grad_norm": 0.6772177219390869,
      "learning_rate": 2.1433333333333333e-06,
      "loss": 0.0023,
      "step": 143570
    },
    {
      "epoch": 7.6576,
      "grad_norm": 0.20605947077274323,
      "learning_rate": 2.14e-06,
      "loss": 0.0014,
      "step": 143580
    },
    {
      "epoch": 7.658133333333334,
      "grad_norm": 0.09525535255670547,
      "learning_rate": 2.136666666666667e-06,
      "loss": 0.0019,
      "step": 143590
    },
    {
      "epoch": 7.658666666666667,
      "grad_norm": 0.0776389092206955,
      "learning_rate": 2.1333333333333334e-06,
      "loss": 0.0026,
      "step": 143600
    },
    {
      "epoch": 7.6592,
      "grad_norm": 0.21119281649589539,
      "learning_rate": 2.13e-06,
      "loss": 0.0012,
      "step": 143610
    },
    {
      "epoch": 7.6597333333333335,
      "grad_norm": 0.04305337741971016,
      "learning_rate": 2.126666666666667e-06,
      "loss": 0.0019,
      "step": 143620
    },
    {
      "epoch": 7.660266666666667,
      "grad_norm": 0.1480547934770584,
      "learning_rate": 2.1233333333333335e-06,
      "loss": 0.0019,
      "step": 143630
    },
    {
      "epoch": 7.6608,
      "grad_norm": 0.09946326911449432,
      "learning_rate": 2.12e-06,
      "loss": 0.0014,
      "step": 143640
    },
    {
      "epoch": 7.661333333333333,
      "grad_norm": 0.06766004115343094,
      "learning_rate": 2.1166666666666666e-06,
      "loss": 0.0018,
      "step": 143650
    },
    {
      "epoch": 7.661866666666667,
      "grad_norm": 0.23416577279567719,
      "learning_rate": 2.1133333333333336e-06,
      "loss": 0.0017,
      "step": 143660
    },
    {
      "epoch": 7.6624,
      "grad_norm": 0.06561445444822311,
      "learning_rate": 2.11e-06,
      "loss": 0.0018,
      "step": 143670
    },
    {
      "epoch": 7.662933333333333,
      "grad_norm": 0.10698065906763077,
      "learning_rate": 2.1066666666666667e-06,
      "loss": 0.0021,
      "step": 143680
    },
    {
      "epoch": 7.663466666666666,
      "grad_norm": 0.24484962224960327,
      "learning_rate": 2.1033333333333337e-06,
      "loss": 0.0012,
      "step": 143690
    },
    {
      "epoch": 7.664,
      "grad_norm": 0.10027361661195755,
      "learning_rate": 2.1000000000000002e-06,
      "loss": 0.002,
      "step": 143700
    },
    {
      "epoch": 7.664533333333333,
      "grad_norm": 0.056229785084724426,
      "learning_rate": 2.0966666666666668e-06,
      "loss": 0.0019,
      "step": 143710
    },
    {
      "epoch": 7.665066666666666,
      "grad_norm": 0.09146469831466675,
      "learning_rate": 2.0933333333333338e-06,
      "loss": 0.0017,
      "step": 143720
    },
    {
      "epoch": 7.6655999999999995,
      "grad_norm": 0.2619967460632324,
      "learning_rate": 2.09e-06,
      "loss": 0.0022,
      "step": 143730
    },
    {
      "epoch": 7.666133333333334,
      "grad_norm": 0.2972627282142639,
      "learning_rate": 2.0866666666666665e-06,
      "loss": 0.0013,
      "step": 143740
    },
    {
      "epoch": 7.666666666666667,
      "grad_norm": 0.07193318009376526,
      "learning_rate": 2.0833333333333334e-06,
      "loss": 0.0024,
      "step": 143750
    },
    {
      "epoch": 7.6672,
      "grad_norm": 0.19497741758823395,
      "learning_rate": 2.08e-06,
      "loss": 0.0012,
      "step": 143760
    },
    {
      "epoch": 7.6677333333333335,
      "grad_norm": 0.09800609201192856,
      "learning_rate": 2.0766666666666665e-06,
      "loss": 0.0023,
      "step": 143770
    },
    {
      "epoch": 7.668266666666667,
      "grad_norm": 0.09657811373472214,
      "learning_rate": 2.0733333333333335e-06,
      "loss": 0.0022,
      "step": 143780
    },
    {
      "epoch": 7.6688,
      "grad_norm": 0.0623965784907341,
      "learning_rate": 2.07e-06,
      "loss": 0.0016,
      "step": 143790
    },
    {
      "epoch": 7.669333333333333,
      "grad_norm": 0.19530324637889862,
      "learning_rate": 2.0666666666666666e-06,
      "loss": 0.0019,
      "step": 143800
    },
    {
      "epoch": 7.669866666666667,
      "grad_norm": 0.31815001368522644,
      "learning_rate": 2.0633333333333336e-06,
      "loss": 0.0015,
      "step": 143810
    },
    {
      "epoch": 7.6704,
      "grad_norm": 0.4133133590221405,
      "learning_rate": 2.06e-06,
      "loss": 0.0014,
      "step": 143820
    },
    {
      "epoch": 7.670933333333333,
      "grad_norm": 0.30118829011917114,
      "learning_rate": 2.0566666666666667e-06,
      "loss": 0.0023,
      "step": 143830
    },
    {
      "epoch": 7.671466666666666,
      "grad_norm": 0.1752796471118927,
      "learning_rate": 2.0533333333333333e-06,
      "loss": 0.0022,
      "step": 143840
    },
    {
      "epoch": 7.672,
      "grad_norm": 0.09082939475774765,
      "learning_rate": 2.0500000000000003e-06,
      "loss": 0.0018,
      "step": 143850
    },
    {
      "epoch": 7.672533333333333,
      "grad_norm": 0.18245573341846466,
      "learning_rate": 2.046666666666667e-06,
      "loss": 0.0014,
      "step": 143860
    },
    {
      "epoch": 7.673066666666667,
      "grad_norm": 0.1866234987974167,
      "learning_rate": 2.0433333333333334e-06,
      "loss": 0.0013,
      "step": 143870
    },
    {
      "epoch": 7.6736,
      "grad_norm": 0.042515963315963745,
      "learning_rate": 2.0400000000000004e-06,
      "loss": 0.002,
      "step": 143880
    },
    {
      "epoch": 7.674133333333334,
      "grad_norm": 0.07148576527833939,
      "learning_rate": 2.036666666666667e-06,
      "loss": 0.002,
      "step": 143890
    },
    {
      "epoch": 7.674666666666667,
      "grad_norm": 0.264899343252182,
      "learning_rate": 2.033333333333333e-06,
      "loss": 0.0013,
      "step": 143900
    },
    {
      "epoch": 7.6752,
      "grad_norm": 0.18602751195430756,
      "learning_rate": 2.03e-06,
      "loss": 0.0023,
      "step": 143910
    },
    {
      "epoch": 7.6757333333333335,
      "grad_norm": 0.424905389547348,
      "learning_rate": 2.0266666666666666e-06,
      "loss": 0.0011,
      "step": 143920
    },
    {
      "epoch": 7.676266666666667,
      "grad_norm": 0.08236177265644073,
      "learning_rate": 2.023333333333333e-06,
      "loss": 0.0028,
      "step": 143930
    },
    {
      "epoch": 7.6768,
      "grad_norm": 0.1738141030073166,
      "learning_rate": 2.02e-06,
      "loss": 0.002,
      "step": 143940
    },
    {
      "epoch": 7.677333333333333,
      "grad_norm": 0.09498830884695053,
      "learning_rate": 2.0166666666666667e-06,
      "loss": 0.002,
      "step": 143950
    },
    {
      "epoch": 7.677866666666667,
      "grad_norm": 0.15253469347953796,
      "learning_rate": 2.0133333333333333e-06,
      "loss": 0.0015,
      "step": 143960
    },
    {
      "epoch": 7.6784,
      "grad_norm": 0.24845203757286072,
      "learning_rate": 2.0100000000000002e-06,
      "loss": 0.0021,
      "step": 143970
    },
    {
      "epoch": 7.678933333333333,
      "grad_norm": 0.05811362341046333,
      "learning_rate": 2.006666666666667e-06,
      "loss": 0.0016,
      "step": 143980
    },
    {
      "epoch": 7.679466666666666,
      "grad_norm": 0.15956711769104004,
      "learning_rate": 2.0033333333333334e-06,
      "loss": 0.0013,
      "step": 143990
    },
    {
      "epoch": 7.68,
      "grad_norm": 0.06355183571577072,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.0013,
      "step": 144000
    },
    {
      "epoch": 7.680533333333333,
      "grad_norm": 0.26495248079299927,
      "learning_rate": 1.996666666666667e-06,
      "loss": 0.0016,
      "step": 144010
    },
    {
      "epoch": 7.681066666666666,
      "grad_norm": 0.29409515857696533,
      "learning_rate": 1.9933333333333334e-06,
      "loss": 0.0015,
      "step": 144020
    },
    {
      "epoch": 7.6815999999999995,
      "grad_norm": 0.1972210854291916,
      "learning_rate": 1.99e-06,
      "loss": 0.0024,
      "step": 144030
    },
    {
      "epoch": 7.682133333333334,
      "grad_norm": 0.1229543387889862,
      "learning_rate": 1.986666666666667e-06,
      "loss": 0.0022,
      "step": 144040
    },
    {
      "epoch": 7.682666666666667,
      "grad_norm": 0.06434547901153564,
      "learning_rate": 1.9833333333333335e-06,
      "loss": 0.0016,
      "step": 144050
    },
    {
      "epoch": 7.6832,
      "grad_norm": 0.09824709594249725,
      "learning_rate": 1.98e-06,
      "loss": 0.0013,
      "step": 144060
    },
    {
      "epoch": 7.6837333333333335,
      "grad_norm": 0.19734106957912445,
      "learning_rate": 1.9766666666666667e-06,
      "loss": 0.0025,
      "step": 144070
    },
    {
      "epoch": 7.684266666666667,
      "grad_norm": 0.123783640563488,
      "learning_rate": 1.9733333333333332e-06,
      "loss": 0.0025,
      "step": 144080
    },
    {
      "epoch": 7.6848,
      "grad_norm": 0.08056090772151947,
      "learning_rate": 1.9699999999999998e-06,
      "loss": 0.0022,
      "step": 144090
    },
    {
      "epoch": 7.685333333333333,
      "grad_norm": 0.14926467835903168,
      "learning_rate": 1.9666666666666668e-06,
      "loss": 0.0014,
      "step": 144100
    },
    {
      "epoch": 7.685866666666667,
      "grad_norm": 0.0227399542927742,
      "learning_rate": 1.9633333333333333e-06,
      "loss": 0.0017,
      "step": 144110
    },
    {
      "epoch": 7.6864,
      "grad_norm": 0.4831474721431732,
      "learning_rate": 1.96e-06,
      "loss": 0.0015,
      "step": 144120
    },
    {
      "epoch": 7.686933333333333,
      "grad_norm": 0.03643815219402313,
      "learning_rate": 1.956666666666667e-06,
      "loss": 0.0016,
      "step": 144130
    },
    {
      "epoch": 7.6874666666666664,
      "grad_norm": 0.539707601070404,
      "learning_rate": 1.9533333333333334e-06,
      "loss": 0.0014,
      "step": 144140
    },
    {
      "epoch": 7.688,
      "grad_norm": 0.14875754714012146,
      "learning_rate": 1.95e-06,
      "loss": 0.0019,
      "step": 144150
    },
    {
      "epoch": 7.688533333333333,
      "grad_norm": 0.30601298809051514,
      "learning_rate": 1.946666666666667e-06,
      "loss": 0.0015,
      "step": 144160
    },
    {
      "epoch": 7.689066666666667,
      "grad_norm": 0.17692427337169647,
      "learning_rate": 1.9433333333333335e-06,
      "loss": 0.0021,
      "step": 144170
    },
    {
      "epoch": 7.6896,
      "grad_norm": 0.047678254544734955,
      "learning_rate": 1.94e-06,
      "loss": 0.0022,
      "step": 144180
    },
    {
      "epoch": 7.690133333333334,
      "grad_norm": 0.07557477056980133,
      "learning_rate": 1.936666666666667e-06,
      "loss": 0.0024,
      "step": 144190
    },
    {
      "epoch": 7.690666666666667,
      "grad_norm": 0.23489966988563538,
      "learning_rate": 1.9333333333333336e-06,
      "loss": 0.0023,
      "step": 144200
    },
    {
      "epoch": 7.6912,
      "grad_norm": 0.1263553351163864,
      "learning_rate": 1.93e-06,
      "loss": 0.0013,
      "step": 144210
    },
    {
      "epoch": 7.6917333333333335,
      "grad_norm": 0.2391621619462967,
      "learning_rate": 1.9266666666666667e-06,
      "loss": 0.0012,
      "step": 144220
    },
    {
      "epoch": 7.692266666666667,
      "grad_norm": 0.20804905891418457,
      "learning_rate": 1.9233333333333337e-06,
      "loss": 0.0016,
      "step": 144230
    },
    {
      "epoch": 7.6928,
      "grad_norm": 0.469356507062912,
      "learning_rate": 1.92e-06,
      "loss": 0.0021,
      "step": 144240
    },
    {
      "epoch": 7.693333333333333,
      "grad_norm": 0.06403779238462448,
      "learning_rate": 1.916666666666667e-06,
      "loss": 0.0023,
      "step": 144250
    },
    {
      "epoch": 7.693866666666667,
      "grad_norm": 0.11794870346784592,
      "learning_rate": 1.9133333333333334e-06,
      "loss": 0.0016,
      "step": 144260
    },
    {
      "epoch": 7.6944,
      "grad_norm": 0.2360452264547348,
      "learning_rate": 1.91e-06,
      "loss": 0.0013,
      "step": 144270
    },
    {
      "epoch": 7.694933333333333,
      "grad_norm": 0.29415929317474365,
      "learning_rate": 1.9066666666666667e-06,
      "loss": 0.0013,
      "step": 144280
    },
    {
      "epoch": 7.6954666666666665,
      "grad_norm": 0.2708536684513092,
      "learning_rate": 1.9033333333333335e-06,
      "loss": 0.002,
      "step": 144290
    },
    {
      "epoch": 7.696,
      "grad_norm": 0.13764455914497375,
      "learning_rate": 1.9e-06,
      "loss": 0.002,
      "step": 144300
    },
    {
      "epoch": 7.696533333333333,
      "grad_norm": 0.35530802607536316,
      "learning_rate": 1.8966666666666668e-06,
      "loss": 0.0015,
      "step": 144310
    },
    {
      "epoch": 7.697066666666666,
      "grad_norm": 0.15186944603919983,
      "learning_rate": 1.8933333333333333e-06,
      "loss": 0.0017,
      "step": 144320
    },
    {
      "epoch": 7.6975999999999996,
      "grad_norm": 0.14966531097888947,
      "learning_rate": 1.8900000000000001e-06,
      "loss": 0.0016,
      "step": 144330
    },
    {
      "epoch": 7.698133333333334,
      "grad_norm": 0.048699889332056046,
      "learning_rate": 1.8866666666666669e-06,
      "loss": 0.0018,
      "step": 144340
    },
    {
      "epoch": 7.698666666666667,
      "grad_norm": 0.06515370309352875,
      "learning_rate": 1.8833333333333334e-06,
      "loss": 0.0015,
      "step": 144350
    },
    {
      "epoch": 7.6992,
      "grad_norm": 0.1366795301437378,
      "learning_rate": 1.8800000000000002e-06,
      "loss": 0.0012,
      "step": 144360
    },
    {
      "epoch": 7.6997333333333335,
      "grad_norm": 0.03741486743092537,
      "learning_rate": 1.8766666666666668e-06,
      "loss": 0.0013,
      "step": 144370
    },
    {
      "epoch": 7.700266666666667,
      "grad_norm": 0.03523215651512146,
      "learning_rate": 1.8733333333333335e-06,
      "loss": 0.0023,
      "step": 144380
    },
    {
      "epoch": 7.7008,
      "grad_norm": 0.23432542383670807,
      "learning_rate": 1.8700000000000003e-06,
      "loss": 0.0019,
      "step": 144390
    },
    {
      "epoch": 7.701333333333333,
      "grad_norm": 0.25380560755729675,
      "learning_rate": 1.8666666666666669e-06,
      "loss": 0.0015,
      "step": 144400
    },
    {
      "epoch": 7.701866666666667,
      "grad_norm": 0.32057449221611023,
      "learning_rate": 1.8633333333333332e-06,
      "loss": 0.0023,
      "step": 144410
    },
    {
      "epoch": 7.7024,
      "grad_norm": 0.1458943486213684,
      "learning_rate": 1.86e-06,
      "loss": 0.0013,
      "step": 144420
    },
    {
      "epoch": 7.702933333333333,
      "grad_norm": 0.12921887636184692,
      "learning_rate": 1.8566666666666665e-06,
      "loss": 0.0013,
      "step": 144430
    },
    {
      "epoch": 7.7034666666666665,
      "grad_norm": 0.0719393938779831,
      "learning_rate": 1.8533333333333333e-06,
      "loss": 0.0028,
      "step": 144440
    },
    {
      "epoch": 7.704,
      "grad_norm": 0.4845241606235504,
      "learning_rate": 1.85e-06,
      "loss": 0.0012,
      "step": 144450
    },
    {
      "epoch": 7.704533333333333,
      "grad_norm": 0.20443092286586761,
      "learning_rate": 1.8466666666666666e-06,
      "loss": 0.0016,
      "step": 144460
    },
    {
      "epoch": 7.705066666666666,
      "grad_norm": 0.09227146208286285,
      "learning_rate": 1.8433333333333334e-06,
      "loss": 0.0014,
      "step": 144470
    },
    {
      "epoch": 7.7056000000000004,
      "grad_norm": 0.09249408543109894,
      "learning_rate": 1.84e-06,
      "loss": 0.0017,
      "step": 144480
    },
    {
      "epoch": 7.706133333333334,
      "grad_norm": 0.0684128850698471,
      "learning_rate": 1.8366666666666667e-06,
      "loss": 0.0014,
      "step": 144490
    },
    {
      "epoch": 7.706666666666667,
      "grad_norm": 0.2687772512435913,
      "learning_rate": 1.8333333333333335e-06,
      "loss": 0.0015,
      "step": 144500
    },
    {
      "epoch": 7.7072,
      "grad_norm": 0.06720126420259476,
      "learning_rate": 1.83e-06,
      "loss": 0.0016,
      "step": 144510
    },
    {
      "epoch": 7.7077333333333335,
      "grad_norm": 0.02807302586734295,
      "learning_rate": 1.8266666666666668e-06,
      "loss": 0.0025,
      "step": 144520
    },
    {
      "epoch": 7.708266666666667,
      "grad_norm": 0.3259946405887604,
      "learning_rate": 1.8233333333333336e-06,
      "loss": 0.0016,
      "step": 144530
    },
    {
      "epoch": 7.7088,
      "grad_norm": 0.08352254331111908,
      "learning_rate": 1.8200000000000002e-06,
      "loss": 0.0014,
      "step": 144540
    },
    {
      "epoch": 7.709333333333333,
      "grad_norm": 0.19320881366729736,
      "learning_rate": 1.816666666666667e-06,
      "loss": 0.0015,
      "step": 144550
    },
    {
      "epoch": 7.709866666666667,
      "grad_norm": 0.040744882076978683,
      "learning_rate": 1.8133333333333335e-06,
      "loss": 0.0019,
      "step": 144560
    },
    {
      "epoch": 7.7104,
      "grad_norm": 0.1058853343129158,
      "learning_rate": 1.8100000000000002e-06,
      "loss": 0.0021,
      "step": 144570
    },
    {
      "epoch": 7.710933333333333,
      "grad_norm": 0.13317708671092987,
      "learning_rate": 1.806666666666667e-06,
      "loss": 0.0026,
      "step": 144580
    },
    {
      "epoch": 7.7114666666666665,
      "grad_norm": 0.11920032650232315,
      "learning_rate": 1.8033333333333334e-06,
      "loss": 0.0017,
      "step": 144590
    },
    {
      "epoch": 7.712,
      "grad_norm": 0.19902896881103516,
      "learning_rate": 1.8e-06,
      "loss": 0.0022,
      "step": 144600
    },
    {
      "epoch": 7.712533333333333,
      "grad_norm": 0.04353475198149681,
      "learning_rate": 1.7966666666666667e-06,
      "loss": 0.0014,
      "step": 144610
    },
    {
      "epoch": 7.713066666666666,
      "grad_norm": 0.33338791131973267,
      "learning_rate": 1.7933333333333332e-06,
      "loss": 0.0024,
      "step": 144620
    },
    {
      "epoch": 7.7136,
      "grad_norm": 0.21072986721992493,
      "learning_rate": 1.79e-06,
      "loss": 0.0018,
      "step": 144630
    },
    {
      "epoch": 7.714133333333333,
      "grad_norm": 0.11919474601745605,
      "learning_rate": 1.7866666666666668e-06,
      "loss": 0.0014,
      "step": 144640
    },
    {
      "epoch": 7.714666666666667,
      "grad_norm": 0.069693922996521,
      "learning_rate": 1.7833333333333333e-06,
      "loss": 0.002,
      "step": 144650
    },
    {
      "epoch": 7.7152,
      "grad_norm": 0.027686946094036102,
      "learning_rate": 1.7800000000000001e-06,
      "loss": 0.0023,
      "step": 144660
    },
    {
      "epoch": 7.7157333333333336,
      "grad_norm": 0.34895002841949463,
      "learning_rate": 1.7766666666666667e-06,
      "loss": 0.002,
      "step": 144670
    },
    {
      "epoch": 7.716266666666667,
      "grad_norm": 0.08998331427574158,
      "learning_rate": 1.7733333333333334e-06,
      "loss": 0.0013,
      "step": 144680
    },
    {
      "epoch": 7.7168,
      "grad_norm": 0.18109504878520966,
      "learning_rate": 1.7700000000000002e-06,
      "loss": 0.0017,
      "step": 144690
    },
    {
      "epoch": 7.717333333333333,
      "grad_norm": 0.06079133227467537,
      "learning_rate": 1.7666666666666668e-06,
      "loss": 0.0018,
      "step": 144700
    },
    {
      "epoch": 7.717866666666667,
      "grad_norm": 0.15021315217018127,
      "learning_rate": 1.7633333333333335e-06,
      "loss": 0.0024,
      "step": 144710
    },
    {
      "epoch": 7.7184,
      "grad_norm": 0.04343843460083008,
      "learning_rate": 1.76e-06,
      "loss": 0.0013,
      "step": 144720
    },
    {
      "epoch": 7.718933333333333,
      "grad_norm": 0.09940141439437866,
      "learning_rate": 1.7566666666666669e-06,
      "loss": 0.0024,
      "step": 144730
    },
    {
      "epoch": 7.7194666666666665,
      "grad_norm": 0.17675310373306274,
      "learning_rate": 1.7533333333333336e-06,
      "loss": 0.0026,
      "step": 144740
    },
    {
      "epoch": 7.72,
      "grad_norm": 0.1299935132265091,
      "learning_rate": 1.7500000000000002e-06,
      "loss": 0.0012,
      "step": 144750
    },
    {
      "epoch": 7.720533333333333,
      "grad_norm": 0.24278336763381958,
      "learning_rate": 1.7466666666666665e-06,
      "loss": 0.0015,
      "step": 144760
    },
    {
      "epoch": 7.721066666666666,
      "grad_norm": 0.17848314344882965,
      "learning_rate": 1.7433333333333333e-06,
      "loss": 0.0014,
      "step": 144770
    },
    {
      "epoch": 7.7216000000000005,
      "grad_norm": 0.02612593024969101,
      "learning_rate": 1.7399999999999999e-06,
      "loss": 0.0021,
      "step": 144780
    },
    {
      "epoch": 7.722133333333334,
      "grad_norm": 0.15895913541316986,
      "learning_rate": 1.7366666666666666e-06,
      "loss": 0.0026,
      "step": 144790
    },
    {
      "epoch": 7.722666666666667,
      "grad_norm": 0.04267194867134094,
      "learning_rate": 1.7333333333333334e-06,
      "loss": 0.0015,
      "step": 144800
    },
    {
      "epoch": 7.7232,
      "grad_norm": 0.046582333743572235,
      "learning_rate": 1.73e-06,
      "loss": 0.0024,
      "step": 144810
    },
    {
      "epoch": 7.723733333333334,
      "grad_norm": 0.1309100240468979,
      "learning_rate": 1.7266666666666667e-06,
      "loss": 0.0016,
      "step": 144820
    },
    {
      "epoch": 7.724266666666667,
      "grad_norm": 0.17878545820713043,
      "learning_rate": 1.7233333333333335e-06,
      "loss": 0.0016,
      "step": 144830
    },
    {
      "epoch": 7.7248,
      "grad_norm": 0.044516053050756454,
      "learning_rate": 1.72e-06,
      "loss": 0.0014,
      "step": 144840
    },
    {
      "epoch": 7.725333333333333,
      "grad_norm": 0.3442760109901428,
      "learning_rate": 1.7166666666666668e-06,
      "loss": 0.0015,
      "step": 144850
    },
    {
      "epoch": 7.725866666666667,
      "grad_norm": 0.06240830570459366,
      "learning_rate": 1.7133333333333334e-06,
      "loss": 0.0015,
      "step": 144860
    },
    {
      "epoch": 7.7264,
      "grad_norm": 0.12445182353258133,
      "learning_rate": 1.7100000000000001e-06,
      "loss": 0.0018,
      "step": 144870
    },
    {
      "epoch": 7.726933333333333,
      "grad_norm": 0.46976011991500854,
      "learning_rate": 1.706666666666667e-06,
      "loss": 0.0017,
      "step": 144880
    },
    {
      "epoch": 7.7274666666666665,
      "grad_norm": 0.10419173538684845,
      "learning_rate": 1.7033333333333335e-06,
      "loss": 0.0015,
      "step": 144890
    },
    {
      "epoch": 7.728,
      "grad_norm": 0.13089774549007416,
      "learning_rate": 1.7000000000000002e-06,
      "loss": 0.0012,
      "step": 144900
    },
    {
      "epoch": 7.728533333333333,
      "grad_norm": 0.1330435425043106,
      "learning_rate": 1.6966666666666668e-06,
      "loss": 0.0018,
      "step": 144910
    },
    {
      "epoch": 7.729066666666666,
      "grad_norm": 0.16461750864982605,
      "learning_rate": 1.6933333333333336e-06,
      "loss": 0.0026,
      "step": 144920
    },
    {
      "epoch": 7.7296,
      "grad_norm": 0.06735369563102722,
      "learning_rate": 1.69e-06,
      "loss": 0.0018,
      "step": 144930
    },
    {
      "epoch": 7.730133333333333,
      "grad_norm": 0.33591926097869873,
      "learning_rate": 1.6866666666666667e-06,
      "loss": 0.0012,
      "step": 144940
    },
    {
      "epoch": 7.730666666666667,
      "grad_norm": 0.0854666456580162,
      "learning_rate": 1.6833333333333332e-06,
      "loss": 0.0015,
      "step": 144950
    },
    {
      "epoch": 7.7312,
      "grad_norm": 0.1489523947238922,
      "learning_rate": 1.68e-06,
      "loss": 0.0017,
      "step": 144960
    },
    {
      "epoch": 7.731733333333334,
      "grad_norm": 0.07030584663152695,
      "learning_rate": 1.6766666666666666e-06,
      "loss": 0.0019,
      "step": 144970
    },
    {
      "epoch": 7.732266666666667,
      "grad_norm": 0.20877213776111603,
      "learning_rate": 1.6733333333333333e-06,
      "loss": 0.0017,
      "step": 144980
    },
    {
      "epoch": 7.7328,
      "grad_norm": 0.18290702998638153,
      "learning_rate": 1.67e-06,
      "loss": 0.0018,
      "step": 144990
    },
    {
      "epoch": 7.733333333333333,
      "grad_norm": 0.22089916467666626,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 0.0019,
      "step": 145000
    },
    {
      "epoch": 7.733866666666667,
      "grad_norm": 0.19968010485172272,
      "learning_rate": 1.6633333333333334e-06,
      "loss": 0.0017,
      "step": 145010
    },
    {
      "epoch": 7.7344,
      "grad_norm": 0.3305431306362152,
      "learning_rate": 1.6600000000000002e-06,
      "loss": 0.0016,
      "step": 145020
    },
    {
      "epoch": 7.734933333333333,
      "grad_norm": 0.09664671868085861,
      "learning_rate": 1.6566666666666668e-06,
      "loss": 0.0018,
      "step": 145030
    },
    {
      "epoch": 7.7354666666666665,
      "grad_norm": 0.2638876438140869,
      "learning_rate": 1.6533333333333335e-06,
      "loss": 0.0015,
      "step": 145040
    },
    {
      "epoch": 7.736,
      "grad_norm": 0.148209810256958,
      "learning_rate": 1.65e-06,
      "loss": 0.0013,
      "step": 145050
    },
    {
      "epoch": 7.736533333333333,
      "grad_norm": 0.280049204826355,
      "learning_rate": 1.6466666666666669e-06,
      "loss": 0.0017,
      "step": 145060
    },
    {
      "epoch": 7.737066666666666,
      "grad_norm": 0.15270750224590302,
      "learning_rate": 1.6433333333333336e-06,
      "loss": 0.0016,
      "step": 145070
    },
    {
      "epoch": 7.7376000000000005,
      "grad_norm": 0.2726190686225891,
      "learning_rate": 1.6400000000000002e-06,
      "loss": 0.0011,
      "step": 145080
    },
    {
      "epoch": 7.738133333333334,
      "grad_norm": 0.6624289155006409,
      "learning_rate": 1.636666666666667e-06,
      "loss": 0.0017,
      "step": 145090
    },
    {
      "epoch": 7.738666666666667,
      "grad_norm": 0.030173560604453087,
      "learning_rate": 1.6333333333333333e-06,
      "loss": 0.0026,
      "step": 145100
    },
    {
      "epoch": 7.7392,
      "grad_norm": 0.24179746210575104,
      "learning_rate": 1.6299999999999999e-06,
      "loss": 0.0016,
      "step": 145110
    },
    {
      "epoch": 7.739733333333334,
      "grad_norm": 0.22684088349342346,
      "learning_rate": 1.6266666666666666e-06,
      "loss": 0.0015,
      "step": 145120
    },
    {
      "epoch": 7.740266666666667,
      "grad_norm": 0.06349200755357742,
      "learning_rate": 1.6233333333333334e-06,
      "loss": 0.0013,
      "step": 145130
    },
    {
      "epoch": 7.7408,
      "grad_norm": 0.23415324091911316,
      "learning_rate": 1.62e-06,
      "loss": 0.0017,
      "step": 145140
    },
    {
      "epoch": 7.741333333333333,
      "grad_norm": 0.06085024029016495,
      "learning_rate": 1.6166666666666667e-06,
      "loss": 0.0014,
      "step": 145150
    },
    {
      "epoch": 7.741866666666667,
      "grad_norm": 0.4086345136165619,
      "learning_rate": 1.6133333333333333e-06,
      "loss": 0.0017,
      "step": 145160
    },
    {
      "epoch": 7.7424,
      "grad_norm": 0.04633964225649834,
      "learning_rate": 1.61e-06,
      "loss": 0.0019,
      "step": 145170
    },
    {
      "epoch": 7.742933333333333,
      "grad_norm": 0.18463654816150665,
      "learning_rate": 1.6066666666666668e-06,
      "loss": 0.0022,
      "step": 145180
    },
    {
      "epoch": 7.7434666666666665,
      "grad_norm": 0.20640696585178375,
      "learning_rate": 1.6033333333333334e-06,
      "loss": 0.0023,
      "step": 145190
    },
    {
      "epoch": 7.744,
      "grad_norm": 0.15477630496025085,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 0.0017,
      "step": 145200
    },
    {
      "epoch": 7.744533333333333,
      "grad_norm": 0.06619392335414886,
      "learning_rate": 1.5966666666666667e-06,
      "loss": 0.0022,
      "step": 145210
    },
    {
      "epoch": 7.745066666666666,
      "grad_norm": 0.17740464210510254,
      "learning_rate": 1.5933333333333335e-06,
      "loss": 0.0021,
      "step": 145220
    },
    {
      "epoch": 7.7456,
      "grad_norm": 0.17871518433094025,
      "learning_rate": 1.5900000000000002e-06,
      "loss": 0.0017,
      "step": 145230
    },
    {
      "epoch": 7.746133333333333,
      "grad_norm": 0.4586527645587921,
      "learning_rate": 1.5866666666666668e-06,
      "loss": 0.0014,
      "step": 145240
    },
    {
      "epoch": 7.746666666666667,
      "grad_norm": 0.2894364297389984,
      "learning_rate": 1.5833333333333336e-06,
      "loss": 0.0032,
      "step": 145250
    },
    {
      "epoch": 7.7472,
      "grad_norm": 0.3212737441062927,
      "learning_rate": 1.5800000000000003e-06,
      "loss": 0.0015,
      "step": 145260
    },
    {
      "epoch": 7.747733333333334,
      "grad_norm": 0.06619973480701447,
      "learning_rate": 1.5766666666666665e-06,
      "loss": 0.0016,
      "step": 145270
    },
    {
      "epoch": 7.748266666666667,
      "grad_norm": 0.24401724338531494,
      "learning_rate": 1.5733333333333332e-06,
      "loss": 0.0021,
      "step": 145280
    },
    {
      "epoch": 7.7488,
      "grad_norm": 0.18328937888145447,
      "learning_rate": 1.57e-06,
      "loss": 0.0024,
      "step": 145290
    },
    {
      "epoch": 7.749333333333333,
      "grad_norm": 0.06503955274820328,
      "learning_rate": 1.5666666666666666e-06,
      "loss": 0.0017,
      "step": 145300
    },
    {
      "epoch": 7.749866666666667,
      "grad_norm": 0.2032700479030609,
      "learning_rate": 1.5633333333333333e-06,
      "loss": 0.0013,
      "step": 145310
    },
    {
      "epoch": 7.7504,
      "grad_norm": 0.7747040390968323,
      "learning_rate": 1.56e-06,
      "loss": 0.002,
      "step": 145320
    },
    {
      "epoch": 7.750933333333333,
      "grad_norm": 0.06961096078157425,
      "learning_rate": 1.5566666666666667e-06,
      "loss": 0.0011,
      "step": 145330
    },
    {
      "epoch": 7.7514666666666665,
      "grad_norm": 0.4274856150150299,
      "learning_rate": 1.5533333333333334e-06,
      "loss": 0.0019,
      "step": 145340
    },
    {
      "epoch": 7.752,
      "grad_norm": 0.23797458410263062,
      "learning_rate": 1.55e-06,
      "loss": 0.0027,
      "step": 145350
    },
    {
      "epoch": 7.752533333333333,
      "grad_norm": 0.3571227788925171,
      "learning_rate": 1.5466666666666668e-06,
      "loss": 0.0019,
      "step": 145360
    },
    {
      "epoch": 7.753066666666666,
      "grad_norm": 0.09562531858682632,
      "learning_rate": 1.5433333333333335e-06,
      "loss": 0.0021,
      "step": 145370
    },
    {
      "epoch": 7.7536000000000005,
      "grad_norm": 0.11844184994697571,
      "learning_rate": 1.54e-06,
      "loss": 0.0025,
      "step": 145380
    },
    {
      "epoch": 7.754133333333334,
      "grad_norm": 0.06542227417230606,
      "learning_rate": 1.5366666666666668e-06,
      "loss": 0.0015,
      "step": 145390
    },
    {
      "epoch": 7.754666666666667,
      "grad_norm": 0.15113140642642975,
      "learning_rate": 1.5333333333333334e-06,
      "loss": 0.0013,
      "step": 145400
    },
    {
      "epoch": 7.7552,
      "grad_norm": 0.2910740375518799,
      "learning_rate": 1.53e-06,
      "loss": 0.0026,
      "step": 145410
    },
    {
      "epoch": 7.755733333333334,
      "grad_norm": 0.06815159320831299,
      "learning_rate": 1.5266666666666667e-06,
      "loss": 0.0017,
      "step": 145420
    },
    {
      "epoch": 7.756266666666667,
      "grad_norm": 0.15309692919254303,
      "learning_rate": 1.5233333333333333e-06,
      "loss": 0.0019,
      "step": 145430
    },
    {
      "epoch": 7.7568,
      "grad_norm": 0.03811457008123398,
      "learning_rate": 1.52e-06,
      "loss": 0.0016,
      "step": 145440
    },
    {
      "epoch": 7.757333333333333,
      "grad_norm": 0.4152801036834717,
      "learning_rate": 1.5166666666666668e-06,
      "loss": 0.0028,
      "step": 145450
    },
    {
      "epoch": 7.757866666666667,
      "grad_norm": 0.14756327867507935,
      "learning_rate": 1.5133333333333334e-06,
      "loss": 0.0018,
      "step": 145460
    },
    {
      "epoch": 7.7584,
      "grad_norm": 0.08865893632173538,
      "learning_rate": 1.5100000000000002e-06,
      "loss": 0.0013,
      "step": 145470
    },
    {
      "epoch": 7.758933333333333,
      "grad_norm": 0.12180911749601364,
      "learning_rate": 1.5066666666666667e-06,
      "loss": 0.0021,
      "step": 145480
    },
    {
      "epoch": 7.7594666666666665,
      "grad_norm": 0.06830662488937378,
      "learning_rate": 1.5033333333333333e-06,
      "loss": 0.0022,
      "step": 145490
    },
    {
      "epoch": 7.76,
      "grad_norm": 0.05014738813042641,
      "learning_rate": 1.5e-06,
      "loss": 0.0012,
      "step": 145500
    },
    {
      "epoch": 7.760533333333333,
      "grad_norm": 0.05956810340285301,
      "learning_rate": 1.4966666666666668e-06,
      "loss": 0.0016,
      "step": 145510
    },
    {
      "epoch": 7.761066666666666,
      "grad_norm": 0.0914960578083992,
      "learning_rate": 1.4933333333333334e-06,
      "loss": 0.0019,
      "step": 145520
    },
    {
      "epoch": 7.7616,
      "grad_norm": 0.06330377608537674,
      "learning_rate": 1.4900000000000001e-06,
      "loss": 0.0024,
      "step": 145530
    },
    {
      "epoch": 7.762133333333333,
      "grad_norm": 0.19478905200958252,
      "learning_rate": 1.4866666666666667e-06,
      "loss": 0.0022,
      "step": 145540
    },
    {
      "epoch": 7.762666666666667,
      "grad_norm": 0.03491455689072609,
      "learning_rate": 1.4833333333333335e-06,
      "loss": 0.0021,
      "step": 145550
    },
    {
      "epoch": 7.7632,
      "grad_norm": 0.02277648076415062,
      "learning_rate": 1.4800000000000002e-06,
      "loss": 0.0025,
      "step": 145560
    },
    {
      "epoch": 7.763733333333334,
      "grad_norm": 0.19008563458919525,
      "learning_rate": 1.4766666666666668e-06,
      "loss": 0.0016,
      "step": 145570
    },
    {
      "epoch": 7.764266666666667,
      "grad_norm": 0.3768995702266693,
      "learning_rate": 1.4733333333333333e-06,
      "loss": 0.0025,
      "step": 145580
    },
    {
      "epoch": 7.7648,
      "grad_norm": 0.07788721472024918,
      "learning_rate": 1.4700000000000001e-06,
      "loss": 0.0015,
      "step": 145590
    },
    {
      "epoch": 7.765333333333333,
      "grad_norm": 0.2846689820289612,
      "learning_rate": 1.4666666666666667e-06,
      "loss": 0.0014,
      "step": 145600
    },
    {
      "epoch": 7.765866666666667,
      "grad_norm": 0.4182722568511963,
      "learning_rate": 1.4633333333333334e-06,
      "loss": 0.0017,
      "step": 145610
    },
    {
      "epoch": 7.7664,
      "grad_norm": 0.2354416400194168,
      "learning_rate": 1.46e-06,
      "loss": 0.0017,
      "step": 145620
    },
    {
      "epoch": 7.766933333333333,
      "grad_norm": 0.17769598960876465,
      "learning_rate": 1.4566666666666668e-06,
      "loss": 0.0019,
      "step": 145630
    },
    {
      "epoch": 7.7674666666666665,
      "grad_norm": 0.0642058476805687,
      "learning_rate": 1.4533333333333335e-06,
      "loss": 0.002,
      "step": 145640
    },
    {
      "epoch": 7.768,
      "grad_norm": 0.200798898935318,
      "learning_rate": 1.45e-06,
      "loss": 0.0017,
      "step": 145650
    },
    {
      "epoch": 7.768533333333333,
      "grad_norm": 0.32591134309768677,
      "learning_rate": 1.4466666666666667e-06,
      "loss": 0.0017,
      "step": 145660
    },
    {
      "epoch": 7.769066666666666,
      "grad_norm": 0.07197305560112,
      "learning_rate": 1.4433333333333334e-06,
      "loss": 0.0016,
      "step": 145670
    },
    {
      "epoch": 7.7696,
      "grad_norm": 0.38381972908973694,
      "learning_rate": 1.44e-06,
      "loss": 0.0014,
      "step": 145680
    },
    {
      "epoch": 7.770133333333334,
      "grad_norm": 0.04493355005979538,
      "learning_rate": 1.4366666666666667e-06,
      "loss": 0.0023,
      "step": 145690
    },
    {
      "epoch": 7.770666666666667,
      "grad_norm": 0.2643950283527374,
      "learning_rate": 1.4333333333333333e-06,
      "loss": 0.002,
      "step": 145700
    },
    {
      "epoch": 7.7712,
      "grad_norm": 0.14816482365131378,
      "learning_rate": 1.43e-06,
      "loss": 0.0023,
      "step": 145710
    },
    {
      "epoch": 7.771733333333334,
      "grad_norm": 0.17599335312843323,
      "learning_rate": 1.4266666666666668e-06,
      "loss": 0.0013,
      "step": 145720
    },
    {
      "epoch": 7.772266666666667,
      "grad_norm": 0.16101160645484924,
      "learning_rate": 1.4233333333333334e-06,
      "loss": 0.0016,
      "step": 145730
    },
    {
      "epoch": 7.7728,
      "grad_norm": 0.1215725988149643,
      "learning_rate": 1.4200000000000002e-06,
      "loss": 0.0018,
      "step": 145740
    },
    {
      "epoch": 7.773333333333333,
      "grad_norm": 0.20809681713581085,
      "learning_rate": 1.4166666666666667e-06,
      "loss": 0.0015,
      "step": 145750
    },
    {
      "epoch": 7.773866666666667,
      "grad_norm": 0.05405399948358536,
      "learning_rate": 1.4133333333333333e-06,
      "loss": 0.0016,
      "step": 145760
    },
    {
      "epoch": 7.7744,
      "grad_norm": 0.3277832567691803,
      "learning_rate": 1.41e-06,
      "loss": 0.002,
      "step": 145770
    },
    {
      "epoch": 7.774933333333333,
      "grad_norm": 0.06382961571216583,
      "learning_rate": 1.4066666666666668e-06,
      "loss": 0.0018,
      "step": 145780
    },
    {
      "epoch": 7.7754666666666665,
      "grad_norm": 0.0993124321103096,
      "learning_rate": 1.4033333333333334e-06,
      "loss": 0.0019,
      "step": 145790
    },
    {
      "epoch": 7.776,
      "grad_norm": 0.11944372206926346,
      "learning_rate": 1.4000000000000001e-06,
      "loss": 0.0021,
      "step": 145800
    },
    {
      "epoch": 7.776533333333333,
      "grad_norm": 0.3806069493293762,
      "learning_rate": 1.3966666666666667e-06,
      "loss": 0.0024,
      "step": 145810
    },
    {
      "epoch": 7.777066666666666,
      "grad_norm": 0.08334846794605255,
      "learning_rate": 1.3933333333333335e-06,
      "loss": 0.0014,
      "step": 145820
    },
    {
      "epoch": 7.7776,
      "grad_norm": 0.2031187117099762,
      "learning_rate": 1.39e-06,
      "loss": 0.0018,
      "step": 145830
    },
    {
      "epoch": 7.778133333333333,
      "grad_norm": 0.366286039352417,
      "learning_rate": 1.3866666666666666e-06,
      "loss": 0.002,
      "step": 145840
    },
    {
      "epoch": 7.778666666666666,
      "grad_norm": 0.32798564434051514,
      "learning_rate": 1.3833333333333334e-06,
      "loss": 0.0019,
      "step": 145850
    },
    {
      "epoch": 7.7792,
      "grad_norm": 0.5116115212440491,
      "learning_rate": 1.3800000000000001e-06,
      "loss": 0.0013,
      "step": 145860
    },
    {
      "epoch": 7.779733333333334,
      "grad_norm": 0.20448258519172668,
      "learning_rate": 1.3766666666666667e-06,
      "loss": 0.0025,
      "step": 145870
    },
    {
      "epoch": 7.780266666666667,
      "grad_norm": 0.25426146388053894,
      "learning_rate": 1.3733333333333335e-06,
      "loss": 0.0017,
      "step": 145880
    },
    {
      "epoch": 7.7808,
      "grad_norm": 0.2660757601261139,
      "learning_rate": 1.37e-06,
      "loss": 0.0017,
      "step": 145890
    },
    {
      "epoch": 7.781333333333333,
      "grad_norm": 0.5224989056587219,
      "learning_rate": 1.3666666666666668e-06,
      "loss": 0.0015,
      "step": 145900
    },
    {
      "epoch": 7.781866666666667,
      "grad_norm": 0.41658520698547363,
      "learning_rate": 1.3633333333333336e-06,
      "loss": 0.0017,
      "step": 145910
    },
    {
      "epoch": 7.7824,
      "grad_norm": 0.0772821381688118,
      "learning_rate": 1.36e-06,
      "loss": 0.003,
      "step": 145920
    },
    {
      "epoch": 7.782933333333333,
      "grad_norm": 0.06955642253160477,
      "learning_rate": 1.3566666666666667e-06,
      "loss": 0.0018,
      "step": 145930
    },
    {
      "epoch": 7.7834666666666665,
      "grad_norm": 0.22644564509391785,
      "learning_rate": 1.3533333333333334e-06,
      "loss": 0.0016,
      "step": 145940
    },
    {
      "epoch": 7.784,
      "grad_norm": 0.17982494831085205,
      "learning_rate": 1.35e-06,
      "loss": 0.0014,
      "step": 145950
    },
    {
      "epoch": 7.784533333333333,
      "grad_norm": 0.2929648458957672,
      "learning_rate": 1.3466666666666668e-06,
      "loss": 0.0013,
      "step": 145960
    },
    {
      "epoch": 7.785066666666666,
      "grad_norm": 0.18324075639247894,
      "learning_rate": 1.3433333333333333e-06,
      "loss": 0.0018,
      "step": 145970
    },
    {
      "epoch": 7.7856,
      "grad_norm": 0.12419861555099487,
      "learning_rate": 1.34e-06,
      "loss": 0.0016,
      "step": 145980
    },
    {
      "epoch": 7.786133333333334,
      "grad_norm": 0.17113983631134033,
      "learning_rate": 1.3366666666666669e-06,
      "loss": 0.0028,
      "step": 145990
    },
    {
      "epoch": 7.786666666666667,
      "grad_norm": 0.3821636438369751,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 0.0014,
      "step": 146000
    },
    {
      "epoch": 7.7872,
      "grad_norm": 0.21726864576339722,
      "learning_rate": 1.33e-06,
      "loss": 0.0018,
      "step": 146010
    },
    {
      "epoch": 7.787733333333334,
      "grad_norm": 0.18744350969791412,
      "learning_rate": 1.3266666666666667e-06,
      "loss": 0.0014,
      "step": 146020
    },
    {
      "epoch": 7.788266666666667,
      "grad_norm": 0.03973446413874626,
      "learning_rate": 1.3233333333333333e-06,
      "loss": 0.0017,
      "step": 146030
    },
    {
      "epoch": 7.7888,
      "grad_norm": 0.037640590220689774,
      "learning_rate": 1.32e-06,
      "loss": 0.0016,
      "step": 146040
    },
    {
      "epoch": 7.789333333333333,
      "grad_norm": 0.26689210534095764,
      "learning_rate": 1.3166666666666668e-06,
      "loss": 0.0022,
      "step": 146050
    },
    {
      "epoch": 7.789866666666667,
      "grad_norm": 0.49503374099731445,
      "learning_rate": 1.3133333333333334e-06,
      "loss": 0.0018,
      "step": 146060
    },
    {
      "epoch": 7.7904,
      "grad_norm": 0.1708420217037201,
      "learning_rate": 1.3100000000000002e-06,
      "loss": 0.0021,
      "step": 146070
    },
    {
      "epoch": 7.790933333333333,
      "grad_norm": 0.038112133741378784,
      "learning_rate": 1.3066666666666667e-06,
      "loss": 0.0019,
      "step": 146080
    },
    {
      "epoch": 7.7914666666666665,
      "grad_norm": 0.10052570700645447,
      "learning_rate": 1.3033333333333333e-06,
      "loss": 0.0014,
      "step": 146090
    },
    {
      "epoch": 7.792,
      "grad_norm": 0.06491077691316605,
      "learning_rate": 1.3e-06,
      "loss": 0.0017,
      "step": 146100
    },
    {
      "epoch": 7.792533333333333,
      "grad_norm": 0.12833265960216522,
      "learning_rate": 1.2966666666666666e-06,
      "loss": 0.0012,
      "step": 146110
    },
    {
      "epoch": 7.793066666666666,
      "grad_norm": 0.29289689660072327,
      "learning_rate": 1.2933333333333334e-06,
      "loss": 0.0012,
      "step": 146120
    },
    {
      "epoch": 7.7936,
      "grad_norm": 0.32667413353919983,
      "learning_rate": 1.2900000000000001e-06,
      "loss": 0.0012,
      "step": 146130
    },
    {
      "epoch": 7.794133333333333,
      "grad_norm": 0.40812787413597107,
      "learning_rate": 1.2866666666666667e-06,
      "loss": 0.0023,
      "step": 146140
    },
    {
      "epoch": 7.794666666666666,
      "grad_norm": 0.30317190289497375,
      "learning_rate": 1.2833333333333335e-06,
      "loss": 0.0017,
      "step": 146150
    },
    {
      "epoch": 7.7952,
      "grad_norm": 0.22843456268310547,
      "learning_rate": 1.28e-06,
      "loss": 0.0016,
      "step": 146160
    },
    {
      "epoch": 7.795733333333334,
      "grad_norm": 0.4169537127017975,
      "learning_rate": 1.2766666666666668e-06,
      "loss": 0.0015,
      "step": 146170
    },
    {
      "epoch": 7.796266666666667,
      "grad_norm": 0.17628712952136993,
      "learning_rate": 1.2733333333333334e-06,
      "loss": 0.0016,
      "step": 146180
    },
    {
      "epoch": 7.7968,
      "grad_norm": 0.2669387757778168,
      "learning_rate": 1.27e-06,
      "loss": 0.0017,
      "step": 146190
    },
    {
      "epoch": 7.7973333333333334,
      "grad_norm": 0.06201745942234993,
      "learning_rate": 1.2666666666666667e-06,
      "loss": 0.0015,
      "step": 146200
    },
    {
      "epoch": 7.797866666666667,
      "grad_norm": 0.20401428639888763,
      "learning_rate": 1.2633333333333334e-06,
      "loss": 0.0018,
      "step": 146210
    },
    {
      "epoch": 7.7984,
      "grad_norm": 0.3203486502170563,
      "learning_rate": 1.26e-06,
      "loss": 0.0021,
      "step": 146220
    },
    {
      "epoch": 7.798933333333333,
      "grad_norm": 0.23255743086338043,
      "learning_rate": 1.2566666666666668e-06,
      "loss": 0.0017,
      "step": 146230
    },
    {
      "epoch": 7.7994666666666665,
      "grad_norm": 0.1288110613822937,
      "learning_rate": 1.2533333333333335e-06,
      "loss": 0.0013,
      "step": 146240
    },
    {
      "epoch": 7.8,
      "grad_norm": 0.048276811838150024,
      "learning_rate": 1.25e-06,
      "loss": 0.0012,
      "step": 146250
    },
    {
      "epoch": 7.800533333333333,
      "grad_norm": 0.022508129477500916,
      "learning_rate": 1.2466666666666667e-06,
      "loss": 0.0013,
      "step": 146260
    },
    {
      "epoch": 7.801066666666666,
      "grad_norm": 0.1819048970937729,
      "learning_rate": 1.2433333333333334e-06,
      "loss": 0.0013,
      "step": 146270
    },
    {
      "epoch": 7.8016,
      "grad_norm": 0.04841272905468941,
      "learning_rate": 1.24e-06,
      "loss": 0.0021,
      "step": 146280
    },
    {
      "epoch": 7.802133333333334,
      "grad_norm": 0.10594068467617035,
      "learning_rate": 1.2366666666666668e-06,
      "loss": 0.0016,
      "step": 146290
    },
    {
      "epoch": 7.802666666666667,
      "grad_norm": 0.26550742983818054,
      "learning_rate": 1.2333333333333333e-06,
      "loss": 0.0015,
      "step": 146300
    },
    {
      "epoch": 7.8032,
      "grad_norm": 0.04951601102948189,
      "learning_rate": 1.23e-06,
      "loss": 0.0014,
      "step": 146310
    },
    {
      "epoch": 7.803733333333334,
      "grad_norm": 0.16279372572898865,
      "learning_rate": 1.2266666666666669e-06,
      "loss": 0.0014,
      "step": 146320
    },
    {
      "epoch": 7.804266666666667,
      "grad_norm": 0.2149219810962677,
      "learning_rate": 1.2233333333333334e-06,
      "loss": 0.0014,
      "step": 146330
    },
    {
      "epoch": 7.8048,
      "grad_norm": 0.17245079576969147,
      "learning_rate": 1.2200000000000002e-06,
      "loss": 0.0019,
      "step": 146340
    },
    {
      "epoch": 7.8053333333333335,
      "grad_norm": 0.06125570833683014,
      "learning_rate": 1.2166666666666667e-06,
      "loss": 0.0021,
      "step": 146350
    },
    {
      "epoch": 7.805866666666667,
      "grad_norm": 0.2577761709690094,
      "learning_rate": 1.2133333333333333e-06,
      "loss": 0.0013,
      "step": 146360
    },
    {
      "epoch": 7.8064,
      "grad_norm": 0.2663309574127197,
      "learning_rate": 1.21e-06,
      "loss": 0.0016,
      "step": 146370
    },
    {
      "epoch": 7.806933333333333,
      "grad_norm": 0.26749294996261597,
      "learning_rate": 1.2066666666666666e-06,
      "loss": 0.0015,
      "step": 146380
    },
    {
      "epoch": 7.8074666666666666,
      "grad_norm": 0.15196098387241364,
      "learning_rate": 1.2033333333333334e-06,
      "loss": 0.0018,
      "step": 146390
    },
    {
      "epoch": 7.808,
      "grad_norm": 0.09612633287906647,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 0.0012,
      "step": 146400
    },
    {
      "epoch": 7.808533333333333,
      "grad_norm": 0.09933248162269592,
      "learning_rate": 1.1966666666666667e-06,
      "loss": 0.0027,
      "step": 146410
    },
    {
      "epoch": 7.809066666666666,
      "grad_norm": 0.17325463891029358,
      "learning_rate": 1.1933333333333335e-06,
      "loss": 0.0014,
      "step": 146420
    },
    {
      "epoch": 7.8096,
      "grad_norm": 0.3641093671321869,
      "learning_rate": 1.19e-06,
      "loss": 0.0015,
      "step": 146430
    },
    {
      "epoch": 7.810133333333333,
      "grad_norm": 0.1734790802001953,
      "learning_rate": 1.1866666666666666e-06,
      "loss": 0.0024,
      "step": 146440
    },
    {
      "epoch": 7.810666666666666,
      "grad_norm": 0.14418599009513855,
      "learning_rate": 1.1833333333333334e-06,
      "loss": 0.0015,
      "step": 146450
    },
    {
      "epoch": 7.8112,
      "grad_norm": 0.3294084370136261,
      "learning_rate": 1.18e-06,
      "loss": 0.002,
      "step": 146460
    },
    {
      "epoch": 7.811733333333334,
      "grad_norm": 0.12140574306249619,
      "learning_rate": 1.1766666666666667e-06,
      "loss": 0.0018,
      "step": 146470
    },
    {
      "epoch": 7.812266666666667,
      "grad_norm": 0.3925039768218994,
      "learning_rate": 1.1733333333333335e-06,
      "loss": 0.002,
      "step": 146480
    },
    {
      "epoch": 7.8128,
      "grad_norm": 0.20995469391345978,
      "learning_rate": 1.17e-06,
      "loss": 0.0017,
      "step": 146490
    },
    {
      "epoch": 7.8133333333333335,
      "grad_norm": 0.37796133756637573,
      "learning_rate": 1.1666666666666668e-06,
      "loss": 0.0015,
      "step": 146500
    },
    {
      "epoch": 7.813866666666667,
      "grad_norm": 0.1480642408132553,
      "learning_rate": 1.1633333333333336e-06,
      "loss": 0.0019,
      "step": 146510
    },
    {
      "epoch": 7.8144,
      "grad_norm": 0.17451462149620056,
      "learning_rate": 1.16e-06,
      "loss": 0.002,
      "step": 146520
    },
    {
      "epoch": 7.814933333333333,
      "grad_norm": 0.05087083578109741,
      "learning_rate": 1.1566666666666667e-06,
      "loss": 0.0016,
      "step": 146530
    },
    {
      "epoch": 7.815466666666667,
      "grad_norm": 0.07055492699146271,
      "learning_rate": 1.1533333333333334e-06,
      "loss": 0.0018,
      "step": 146540
    },
    {
      "epoch": 7.816,
      "grad_norm": 0.36139801144599915,
      "learning_rate": 1.15e-06,
      "loss": 0.0019,
      "step": 146550
    },
    {
      "epoch": 7.816533333333333,
      "grad_norm": 0.032894812524318695,
      "learning_rate": 1.1466666666666668e-06,
      "loss": 0.0012,
      "step": 146560
    },
    {
      "epoch": 7.817066666666666,
      "grad_norm": 0.3165426254272461,
      "learning_rate": 1.1433333333333333e-06,
      "loss": 0.002,
      "step": 146570
    },
    {
      "epoch": 7.8176,
      "grad_norm": 0.09346423298120499,
      "learning_rate": 1.14e-06,
      "loss": 0.0018,
      "step": 146580
    },
    {
      "epoch": 7.818133333333334,
      "grad_norm": 0.09021410346031189,
      "learning_rate": 1.1366666666666669e-06,
      "loss": 0.0017,
      "step": 146590
    },
    {
      "epoch": 7.818666666666667,
      "grad_norm": 0.04021809250116348,
      "learning_rate": 1.1333333333333334e-06,
      "loss": 0.0021,
      "step": 146600
    },
    {
      "epoch": 7.8192,
      "grad_norm": 0.2035519927740097,
      "learning_rate": 1.13e-06,
      "loss": 0.003,
      "step": 146610
    },
    {
      "epoch": 7.819733333333334,
      "grad_norm": 0.2566367983818054,
      "learning_rate": 1.1266666666666667e-06,
      "loss": 0.0015,
      "step": 146620
    },
    {
      "epoch": 7.820266666666667,
      "grad_norm": 0.09176425635814667,
      "learning_rate": 1.1233333333333333e-06,
      "loss": 0.0017,
      "step": 146630
    },
    {
      "epoch": 7.8208,
      "grad_norm": 0.03123997151851654,
      "learning_rate": 1.12e-06,
      "loss": 0.0018,
      "step": 146640
    },
    {
      "epoch": 7.8213333333333335,
      "grad_norm": 0.11987128108739853,
      "learning_rate": 1.1166666666666666e-06,
      "loss": 0.0013,
      "step": 146650
    },
    {
      "epoch": 7.821866666666667,
      "grad_norm": 0.528928279876709,
      "learning_rate": 1.1133333333333334e-06,
      "loss": 0.0017,
      "step": 146660
    },
    {
      "epoch": 7.8224,
      "grad_norm": 0.09016207605600357,
      "learning_rate": 1.1100000000000002e-06,
      "loss": 0.0017,
      "step": 146670
    },
    {
      "epoch": 7.822933333333333,
      "grad_norm": 0.19752895832061768,
      "learning_rate": 1.1066666666666667e-06,
      "loss": 0.0012,
      "step": 146680
    },
    {
      "epoch": 7.823466666666667,
      "grad_norm": 0.18197979032993317,
      "learning_rate": 1.1033333333333333e-06,
      "loss": 0.001,
      "step": 146690
    },
    {
      "epoch": 7.824,
      "grad_norm": 0.3299700915813446,
      "learning_rate": 1.1e-06,
      "loss": 0.0021,
      "step": 146700
    },
    {
      "epoch": 7.824533333333333,
      "grad_norm": 0.09748507291078568,
      "learning_rate": 1.0966666666666666e-06,
      "loss": 0.0018,
      "step": 146710
    },
    {
      "epoch": 7.825066666666666,
      "grad_norm": 0.06146872788667679,
      "learning_rate": 1.0933333333333334e-06,
      "loss": 0.0021,
      "step": 146720
    },
    {
      "epoch": 7.8256,
      "grad_norm": 0.2927224636077881,
      "learning_rate": 1.0900000000000002e-06,
      "loss": 0.0018,
      "step": 146730
    },
    {
      "epoch": 7.826133333333333,
      "grad_norm": 0.1819007843732834,
      "learning_rate": 1.0866666666666667e-06,
      "loss": 0.0019,
      "step": 146740
    },
    {
      "epoch": 7.826666666666666,
      "grad_norm": 0.17751608788967133,
      "learning_rate": 1.0833333333333335e-06,
      "loss": 0.0012,
      "step": 146750
    },
    {
      "epoch": 7.8272,
      "grad_norm": 0.1903291940689087,
      "learning_rate": 1.08e-06,
      "loss": 0.0022,
      "step": 146760
    },
    {
      "epoch": 7.827733333333334,
      "grad_norm": 0.3202505111694336,
      "learning_rate": 1.0766666666666668e-06,
      "loss": 0.0016,
      "step": 146770
    },
    {
      "epoch": 7.828266666666667,
      "grad_norm": 0.048087168484926224,
      "learning_rate": 1.0733333333333334e-06,
      "loss": 0.0025,
      "step": 146780
    },
    {
      "epoch": 7.8288,
      "grad_norm": 0.04306134209036827,
      "learning_rate": 1.07e-06,
      "loss": 0.0017,
      "step": 146790
    },
    {
      "epoch": 7.8293333333333335,
      "grad_norm": 0.40691494941711426,
      "learning_rate": 1.0666666666666667e-06,
      "loss": 0.0016,
      "step": 146800
    },
    {
      "epoch": 7.829866666666667,
      "grad_norm": 0.2399800568819046,
      "learning_rate": 1.0633333333333335e-06,
      "loss": 0.0019,
      "step": 146810
    },
    {
      "epoch": 7.8304,
      "grad_norm": 0.4879695475101471,
      "learning_rate": 1.06e-06,
      "loss": 0.002,
      "step": 146820
    },
    {
      "epoch": 7.830933333333333,
      "grad_norm": 0.07381140440702438,
      "learning_rate": 1.0566666666666668e-06,
      "loss": 0.0018,
      "step": 146830
    },
    {
      "epoch": 7.831466666666667,
      "grad_norm": 0.0636378601193428,
      "learning_rate": 1.0533333333333333e-06,
      "loss": 0.0016,
      "step": 146840
    },
    {
      "epoch": 7.832,
      "grad_norm": 0.12433268129825592,
      "learning_rate": 1.0500000000000001e-06,
      "loss": 0.0013,
      "step": 146850
    },
    {
      "epoch": 7.832533333333333,
      "grad_norm": 0.20442508161067963,
      "learning_rate": 1.0466666666666669e-06,
      "loss": 0.0013,
      "step": 146860
    },
    {
      "epoch": 7.833066666666666,
      "grad_norm": 0.09281550347805023,
      "learning_rate": 1.0433333333333332e-06,
      "loss": 0.0024,
      "step": 146870
    },
    {
      "epoch": 7.8336,
      "grad_norm": 0.18927787244319916,
      "learning_rate": 1.04e-06,
      "loss": 0.0014,
      "step": 146880
    },
    {
      "epoch": 7.834133333333333,
      "grad_norm": 0.39905259013175964,
      "learning_rate": 1.0366666666666668e-06,
      "loss": 0.0014,
      "step": 146890
    },
    {
      "epoch": 7.834666666666667,
      "grad_norm": 0.06586351990699768,
      "learning_rate": 1.0333333333333333e-06,
      "loss": 0.0018,
      "step": 146900
    },
    {
      "epoch": 7.8352,
      "grad_norm": 0.17783202230930328,
      "learning_rate": 1.03e-06,
      "loss": 0.0013,
      "step": 146910
    },
    {
      "epoch": 7.835733333333334,
      "grad_norm": 0.19511747360229492,
      "learning_rate": 1.0266666666666666e-06,
      "loss": 0.002,
      "step": 146920
    },
    {
      "epoch": 7.836266666666667,
      "grad_norm": 0.032028790563344955,
      "learning_rate": 1.0233333333333334e-06,
      "loss": 0.0016,
      "step": 146930
    },
    {
      "epoch": 7.8368,
      "grad_norm": 0.3245587944984436,
      "learning_rate": 1.0200000000000002e-06,
      "loss": 0.0016,
      "step": 146940
    },
    {
      "epoch": 7.8373333333333335,
      "grad_norm": 0.23958897590637207,
      "learning_rate": 1.0166666666666665e-06,
      "loss": 0.0014,
      "step": 146950
    },
    {
      "epoch": 7.837866666666667,
      "grad_norm": 0.09278795123100281,
      "learning_rate": 1.0133333333333333e-06,
      "loss": 0.0024,
      "step": 146960
    },
    {
      "epoch": 7.8384,
      "grad_norm": 0.37948331236839294,
      "learning_rate": 1.01e-06,
      "loss": 0.0016,
      "step": 146970
    },
    {
      "epoch": 7.838933333333333,
      "grad_norm": 0.12740090489387512,
      "learning_rate": 1.0066666666666666e-06,
      "loss": 0.0026,
      "step": 146980
    },
    {
      "epoch": 7.839466666666667,
      "grad_norm": 0.2413080632686615,
      "learning_rate": 1.0033333333333334e-06,
      "loss": 0.0022,
      "step": 146990
    },
    {
      "epoch": 7.84,
      "grad_norm": 0.49399515986442566,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.0022,
      "step": 147000
    },
    {
      "epoch": 7.840533333333333,
      "grad_norm": 0.2861507534980774,
      "learning_rate": 9.966666666666667e-07,
      "loss": 0.0022,
      "step": 147010
    },
    {
      "epoch": 7.841066666666666,
      "grad_norm": 0.23482228815555573,
      "learning_rate": 9.933333333333335e-07,
      "loss": 0.0026,
      "step": 147020
    },
    {
      "epoch": 7.8416,
      "grad_norm": 0.05099756270647049,
      "learning_rate": 9.9e-07,
      "loss": 0.0022,
      "step": 147030
    },
    {
      "epoch": 7.842133333333333,
      "grad_norm": 0.3509308099746704,
      "learning_rate": 9.866666666666666e-07,
      "loss": 0.0019,
      "step": 147040
    },
    {
      "epoch": 7.842666666666666,
      "grad_norm": 0.1878906488418579,
      "learning_rate": 9.833333333333334e-07,
      "loss": 0.0014,
      "step": 147050
    },
    {
      "epoch": 7.8431999999999995,
      "grad_norm": 0.35617902874946594,
      "learning_rate": 9.8e-07,
      "loss": 0.0022,
      "step": 147060
    },
    {
      "epoch": 7.843733333333334,
      "grad_norm": 0.12574613094329834,
      "learning_rate": 9.766666666666667e-07,
      "loss": 0.0012,
      "step": 147070
    },
    {
      "epoch": 7.844266666666667,
      "grad_norm": 0.14861959218978882,
      "learning_rate": 9.733333333333335e-07,
      "loss": 0.002,
      "step": 147080
    },
    {
      "epoch": 7.8448,
      "grad_norm": 0.05261467024683952,
      "learning_rate": 9.7e-07,
      "loss": 0.0018,
      "step": 147090
    },
    {
      "epoch": 7.8453333333333335,
      "grad_norm": 0.2524416446685791,
      "learning_rate": 9.666666666666668e-07,
      "loss": 0.0015,
      "step": 147100
    },
    {
      "epoch": 7.845866666666667,
      "grad_norm": 0.06954589486122131,
      "learning_rate": 9.633333333333334e-07,
      "loss": 0.0016,
      "step": 147110
    },
    {
      "epoch": 7.8464,
      "grad_norm": 0.3932543694972992,
      "learning_rate": 9.6e-07,
      "loss": 0.0013,
      "step": 147120
    },
    {
      "epoch": 7.846933333333333,
      "grad_norm": 0.1226128339767456,
      "learning_rate": 9.566666666666667e-07,
      "loss": 0.0023,
      "step": 147130
    },
    {
      "epoch": 7.847466666666667,
      "grad_norm": 0.15072743594646454,
      "learning_rate": 9.533333333333333e-07,
      "loss": 0.0013,
      "step": 147140
    },
    {
      "epoch": 7.848,
      "grad_norm": 0.22803154587745667,
      "learning_rate": 9.5e-07,
      "loss": 0.0014,
      "step": 147150
    },
    {
      "epoch": 7.848533333333333,
      "grad_norm": 0.11887041479349136,
      "learning_rate": 9.466666666666667e-07,
      "loss": 0.0022,
      "step": 147160
    },
    {
      "epoch": 7.849066666666666,
      "grad_norm": 0.05158238857984543,
      "learning_rate": 9.433333333333334e-07,
      "loss": 0.0022,
      "step": 147170
    },
    {
      "epoch": 7.8496,
      "grad_norm": 0.06521391868591309,
      "learning_rate": 9.400000000000001e-07,
      "loss": 0.0029,
      "step": 147180
    },
    {
      "epoch": 7.850133333333333,
      "grad_norm": 0.06678319722414017,
      "learning_rate": 9.366666666666668e-07,
      "loss": 0.0019,
      "step": 147190
    },
    {
      "epoch": 7.850666666666667,
      "grad_norm": 0.17479179799556732,
      "learning_rate": 9.333333333333334e-07,
      "loss": 0.0025,
      "step": 147200
    },
    {
      "epoch": 7.8512,
      "grad_norm": 0.11822117865085602,
      "learning_rate": 9.3e-07,
      "loss": 0.0012,
      "step": 147210
    },
    {
      "epoch": 7.851733333333334,
      "grad_norm": 0.30799537897109985,
      "learning_rate": 9.266666666666667e-07,
      "loss": 0.0016,
      "step": 147220
    },
    {
      "epoch": 7.852266666666667,
      "grad_norm": 0.04366658627986908,
      "learning_rate": 9.233333333333333e-07,
      "loss": 0.0018,
      "step": 147230
    },
    {
      "epoch": 7.8528,
      "grad_norm": 0.27682068943977356,
      "learning_rate": 9.2e-07,
      "loss": 0.0016,
      "step": 147240
    },
    {
      "epoch": 7.8533333333333335,
      "grad_norm": 0.12117815762758255,
      "learning_rate": 9.166666666666667e-07,
      "loss": 0.0013,
      "step": 147250
    },
    {
      "epoch": 7.853866666666667,
      "grad_norm": 0.34842944145202637,
      "learning_rate": 9.133333333333334e-07,
      "loss": 0.0018,
      "step": 147260
    },
    {
      "epoch": 7.8544,
      "grad_norm": 0.29804620146751404,
      "learning_rate": 9.100000000000001e-07,
      "loss": 0.0022,
      "step": 147270
    },
    {
      "epoch": 7.854933333333333,
      "grad_norm": 0.24690496921539307,
      "learning_rate": 9.066666666666667e-07,
      "loss": 0.0019,
      "step": 147280
    },
    {
      "epoch": 7.855466666666667,
      "grad_norm": 0.21856240928173065,
      "learning_rate": 9.033333333333335e-07,
      "loss": 0.0012,
      "step": 147290
    },
    {
      "epoch": 7.856,
      "grad_norm": 0.06761828809976578,
      "learning_rate": 9e-07,
      "loss": 0.0023,
      "step": 147300
    },
    {
      "epoch": 7.856533333333333,
      "grad_norm": 0.24413180351257324,
      "learning_rate": 8.966666666666666e-07,
      "loss": 0.0013,
      "step": 147310
    },
    {
      "epoch": 7.857066666666666,
      "grad_norm": 0.44099166989326477,
      "learning_rate": 8.933333333333334e-07,
      "loss": 0.0016,
      "step": 147320
    },
    {
      "epoch": 7.8576,
      "grad_norm": 0.18965232372283936,
      "learning_rate": 8.900000000000001e-07,
      "loss": 0.0016,
      "step": 147330
    },
    {
      "epoch": 7.858133333333333,
      "grad_norm": 0.039770253002643585,
      "learning_rate": 8.866666666666667e-07,
      "loss": 0.0015,
      "step": 147340
    },
    {
      "epoch": 7.858666666666666,
      "grad_norm": 0.3825129568576813,
      "learning_rate": 8.833333333333334e-07,
      "loss": 0.0017,
      "step": 147350
    },
    {
      "epoch": 7.8591999999999995,
      "grad_norm": 0.09604570269584656,
      "learning_rate": 8.8e-07,
      "loss": 0.0024,
      "step": 147360
    },
    {
      "epoch": 7.859733333333334,
      "grad_norm": 0.4646586775779724,
      "learning_rate": 8.766666666666668e-07,
      "loss": 0.0023,
      "step": 147370
    },
    {
      "epoch": 7.860266666666667,
      "grad_norm": 0.10080210864543915,
      "learning_rate": 8.733333333333333e-07,
      "loss": 0.0021,
      "step": 147380
    },
    {
      "epoch": 7.8608,
      "grad_norm": 0.06721626967191696,
      "learning_rate": 8.699999999999999e-07,
      "loss": 0.0019,
      "step": 147390
    },
    {
      "epoch": 7.8613333333333335,
      "grad_norm": 0.3775261342525482,
      "learning_rate": 8.666666666666667e-07,
      "loss": 0.0017,
      "step": 147400
    },
    {
      "epoch": 7.861866666666667,
      "grad_norm": 0.11037377268075943,
      "learning_rate": 8.633333333333334e-07,
      "loss": 0.0017,
      "step": 147410
    },
    {
      "epoch": 7.8624,
      "grad_norm": 0.29745420813560486,
      "learning_rate": 8.6e-07,
      "loss": 0.0022,
      "step": 147420
    },
    {
      "epoch": 7.862933333333333,
      "grad_norm": 0.19308710098266602,
      "learning_rate": 8.566666666666667e-07,
      "loss": 0.0022,
      "step": 147430
    },
    {
      "epoch": 7.863466666666667,
      "grad_norm": 0.06386994570493698,
      "learning_rate": 8.533333333333335e-07,
      "loss": 0.0015,
      "step": 147440
    },
    {
      "epoch": 7.864,
      "grad_norm": 0.42009785771369934,
      "learning_rate": 8.500000000000001e-07,
      "loss": 0.0013,
      "step": 147450
    },
    {
      "epoch": 7.864533333333333,
      "grad_norm": 0.06100236624479294,
      "learning_rate": 8.466666666666668e-07,
      "loss": 0.0015,
      "step": 147460
    },
    {
      "epoch": 7.865066666666666,
      "grad_norm": 0.566462516784668,
      "learning_rate": 8.433333333333333e-07,
      "loss": 0.0013,
      "step": 147470
    },
    {
      "epoch": 7.8656,
      "grad_norm": 0.03857831656932831,
      "learning_rate": 8.4e-07,
      "loss": 0.0017,
      "step": 147480
    },
    {
      "epoch": 7.866133333333333,
      "grad_norm": 0.4703543782234192,
      "learning_rate": 8.366666666666667e-07,
      "loss": 0.0024,
      "step": 147490
    },
    {
      "epoch": 7.866666666666667,
      "grad_norm": 0.29203563928604126,
      "learning_rate": 8.333333333333333e-07,
      "loss": 0.0014,
      "step": 147500
    },
    {
      "epoch": 7.8672,
      "grad_norm": 0.12157842516899109,
      "learning_rate": 8.300000000000001e-07,
      "loss": 0.0023,
      "step": 147510
    },
    {
      "epoch": 7.867733333333334,
      "grad_norm": 0.12543731927871704,
      "learning_rate": 8.266666666666668e-07,
      "loss": 0.0016,
      "step": 147520
    },
    {
      "epoch": 7.868266666666667,
      "grad_norm": 0.3186546266078949,
      "learning_rate": 8.233333333333334e-07,
      "loss": 0.0016,
      "step": 147530
    },
    {
      "epoch": 7.8688,
      "grad_norm": 0.5235740542411804,
      "learning_rate": 8.200000000000001e-07,
      "loss": 0.002,
      "step": 147540
    },
    {
      "epoch": 7.8693333333333335,
      "grad_norm": 0.09272906929254532,
      "learning_rate": 8.166666666666666e-07,
      "loss": 0.0016,
      "step": 147550
    },
    {
      "epoch": 7.869866666666667,
      "grad_norm": 0.0533854216337204,
      "learning_rate": 8.133333333333333e-07,
      "loss": 0.0023,
      "step": 147560
    },
    {
      "epoch": 7.8704,
      "grad_norm": 0.210626021027565,
      "learning_rate": 8.1e-07,
      "loss": 0.0014,
      "step": 147570
    },
    {
      "epoch": 7.870933333333333,
      "grad_norm": 0.19241490960121155,
      "learning_rate": 8.066666666666666e-07,
      "loss": 0.0012,
      "step": 147580
    },
    {
      "epoch": 7.871466666666667,
      "grad_norm": 0.15175685286521912,
      "learning_rate": 8.033333333333334e-07,
      "loss": 0.0016,
      "step": 147590
    },
    {
      "epoch": 7.872,
      "grad_norm": 0.175615593791008,
      "learning_rate": 8.000000000000001e-07,
      "loss": 0.0013,
      "step": 147600
    },
    {
      "epoch": 7.872533333333333,
      "grad_norm": 0.060491111129522324,
      "learning_rate": 7.966666666666667e-07,
      "loss": 0.0017,
      "step": 147610
    },
    {
      "epoch": 7.873066666666666,
      "grad_norm": 0.05169365927577019,
      "learning_rate": 7.933333333333334e-07,
      "loss": 0.0018,
      "step": 147620
    },
    {
      "epoch": 7.8736,
      "grad_norm": 0.0670996829867363,
      "learning_rate": 7.900000000000002e-07,
      "loss": 0.0018,
      "step": 147630
    },
    {
      "epoch": 7.874133333333333,
      "grad_norm": 0.20543131232261658,
      "learning_rate": 7.866666666666666e-07,
      "loss": 0.002,
      "step": 147640
    },
    {
      "epoch": 7.874666666666666,
      "grad_norm": 0.037375904619693756,
      "learning_rate": 7.833333333333333e-07,
      "loss": 0.002,
      "step": 147650
    },
    {
      "epoch": 7.8751999999999995,
      "grad_norm": 0.153036430478096,
      "learning_rate": 7.8e-07,
      "loss": 0.0018,
      "step": 147660
    },
    {
      "epoch": 7.875733333333334,
      "grad_norm": 0.20794722437858582,
      "learning_rate": 7.766666666666667e-07,
      "loss": 0.0018,
      "step": 147670
    },
    {
      "epoch": 7.876266666666667,
      "grad_norm": 0.18063358962535858,
      "learning_rate": 7.733333333333334e-07,
      "loss": 0.0019,
      "step": 147680
    },
    {
      "epoch": 7.8768,
      "grad_norm": 0.07906944304704666,
      "learning_rate": 7.7e-07,
      "loss": 0.0015,
      "step": 147690
    },
    {
      "epoch": 7.8773333333333335,
      "grad_norm": 0.2378825843334198,
      "learning_rate": 7.666666666666667e-07,
      "loss": 0.0015,
      "step": 147700
    },
    {
      "epoch": 7.877866666666667,
      "grad_norm": 0.18230924010276794,
      "learning_rate": 7.633333333333334e-07,
      "loss": 0.0017,
      "step": 147710
    },
    {
      "epoch": 7.8784,
      "grad_norm": 0.09165194630622864,
      "learning_rate": 7.6e-07,
      "loss": 0.0013,
      "step": 147720
    },
    {
      "epoch": 7.878933333333333,
      "grad_norm": 0.21003510057926178,
      "learning_rate": 7.566666666666667e-07,
      "loss": 0.0013,
      "step": 147730
    },
    {
      "epoch": 7.879466666666667,
      "grad_norm": 0.350910484790802,
      "learning_rate": 7.533333333333334e-07,
      "loss": 0.0014,
      "step": 147740
    },
    {
      "epoch": 7.88,
      "grad_norm": 0.09842823445796967,
      "learning_rate": 7.5e-07,
      "loss": 0.0022,
      "step": 147750
    },
    {
      "epoch": 7.880533333333333,
      "grad_norm": 0.26727011799812317,
      "learning_rate": 7.466666666666667e-07,
      "loss": 0.0013,
      "step": 147760
    },
    {
      "epoch": 7.881066666666666,
      "grad_norm": 0.043878618627786636,
      "learning_rate": 7.433333333333333e-07,
      "loss": 0.002,
      "step": 147770
    },
    {
      "epoch": 7.8816,
      "grad_norm": 0.3322180211544037,
      "learning_rate": 7.400000000000001e-07,
      "loss": 0.0015,
      "step": 147780
    },
    {
      "epoch": 7.882133333333333,
      "grad_norm": 0.26226285099983215,
      "learning_rate": 7.366666666666667e-07,
      "loss": 0.0021,
      "step": 147790
    },
    {
      "epoch": 7.882666666666667,
      "grad_norm": 0.06610142439603806,
      "learning_rate": 7.333333333333333e-07,
      "loss": 0.0015,
      "step": 147800
    },
    {
      "epoch": 7.8832,
      "grad_norm": 0.18321673572063446,
      "learning_rate": 7.3e-07,
      "loss": 0.0013,
      "step": 147810
    },
    {
      "epoch": 7.883733333333334,
      "grad_norm": 0.20861934125423431,
      "learning_rate": 7.266666666666668e-07,
      "loss": 0.0024,
      "step": 147820
    },
    {
      "epoch": 7.884266666666667,
      "grad_norm": 0.04958318918943405,
      "learning_rate": 7.233333333333333e-07,
      "loss": 0.0022,
      "step": 147830
    },
    {
      "epoch": 7.8848,
      "grad_norm": 0.20416660606861115,
      "learning_rate": 7.2e-07,
      "loss": 0.0016,
      "step": 147840
    },
    {
      "epoch": 7.8853333333333335,
      "grad_norm": 0.29061904549598694,
      "learning_rate": 7.166666666666667e-07,
      "loss": 0.0012,
      "step": 147850
    },
    {
      "epoch": 7.885866666666667,
      "grad_norm": 0.1101575717329979,
      "learning_rate": 7.133333333333334e-07,
      "loss": 0.0018,
      "step": 147860
    },
    {
      "epoch": 7.8864,
      "grad_norm": 0.26805344223976135,
      "learning_rate": 7.100000000000001e-07,
      "loss": 0.002,
      "step": 147870
    },
    {
      "epoch": 7.886933333333333,
      "grad_norm": 0.25899946689605713,
      "learning_rate": 7.066666666666666e-07,
      "loss": 0.0017,
      "step": 147880
    },
    {
      "epoch": 7.887466666666667,
      "grad_norm": 0.14875993132591248,
      "learning_rate": 7.033333333333334e-07,
      "loss": 0.0012,
      "step": 147890
    },
    {
      "epoch": 7.888,
      "grad_norm": 0.18660400807857513,
      "learning_rate": 7.000000000000001e-07,
      "loss": 0.0015,
      "step": 147900
    },
    {
      "epoch": 7.888533333333333,
      "grad_norm": 0.052546050399541855,
      "learning_rate": 6.966666666666667e-07,
      "loss": 0.0019,
      "step": 147910
    },
    {
      "epoch": 7.8890666666666664,
      "grad_norm": 0.24169564247131348,
      "learning_rate": 6.933333333333333e-07,
      "loss": 0.002,
      "step": 147920
    },
    {
      "epoch": 7.8896,
      "grad_norm": 0.14839811623096466,
      "learning_rate": 6.900000000000001e-07,
      "loss": 0.0022,
      "step": 147930
    },
    {
      "epoch": 7.890133333333333,
      "grad_norm": 0.0931071937084198,
      "learning_rate": 6.866666666666667e-07,
      "loss": 0.0017,
      "step": 147940
    },
    {
      "epoch": 7.890666666666666,
      "grad_norm": 0.035962846130132675,
      "learning_rate": 6.833333333333334e-07,
      "loss": 0.0024,
      "step": 147950
    },
    {
      "epoch": 7.8911999999999995,
      "grad_norm": 0.2091042846441269,
      "learning_rate": 6.8e-07,
      "loss": 0.0013,
      "step": 147960
    },
    {
      "epoch": 7.891733333333334,
      "grad_norm": 0.14620886743068695,
      "learning_rate": 6.766666666666667e-07,
      "loss": 0.0014,
      "step": 147970
    },
    {
      "epoch": 7.892266666666667,
      "grad_norm": 0.2213907539844513,
      "learning_rate": 6.733333333333334e-07,
      "loss": 0.0021,
      "step": 147980
    },
    {
      "epoch": 7.8928,
      "grad_norm": 0.3837982714176178,
      "learning_rate": 6.7e-07,
      "loss": 0.0015,
      "step": 147990
    },
    {
      "epoch": 7.8933333333333335,
      "grad_norm": 0.2375464290380478,
      "learning_rate": 6.666666666666667e-07,
      "loss": 0.0014,
      "step": 148000
    },
    {
      "epoch": 7.893866666666667,
      "grad_norm": 0.23600083589553833,
      "learning_rate": 6.633333333333334e-07,
      "loss": 0.0018,
      "step": 148010
    },
    {
      "epoch": 7.8944,
      "grad_norm": 0.14584888517856598,
      "learning_rate": 6.6e-07,
      "loss": 0.0019,
      "step": 148020
    },
    {
      "epoch": 7.894933333333333,
      "grad_norm": 0.1860339343547821,
      "learning_rate": 6.566666666666667e-07,
      "loss": 0.0014,
      "step": 148030
    },
    {
      "epoch": 7.895466666666667,
      "grad_norm": 0.14809539914131165,
      "learning_rate": 6.533333333333334e-07,
      "loss": 0.0014,
      "step": 148040
    },
    {
      "epoch": 7.896,
      "grad_norm": 0.12441742420196533,
      "learning_rate": 6.5e-07,
      "loss": 0.0023,
      "step": 148050
    },
    {
      "epoch": 7.896533333333333,
      "grad_norm": 0.051969870924949646,
      "learning_rate": 6.466666666666667e-07,
      "loss": 0.0022,
      "step": 148060
    },
    {
      "epoch": 7.8970666666666665,
      "grad_norm": 0.27139317989349365,
      "learning_rate": 6.433333333333334e-07,
      "loss": 0.0021,
      "step": 148070
    },
    {
      "epoch": 7.8976,
      "grad_norm": 0.07162979245185852,
      "learning_rate": 6.4e-07,
      "loss": 0.0023,
      "step": 148080
    },
    {
      "epoch": 7.898133333333333,
      "grad_norm": 0.33688318729400635,
      "learning_rate": 6.366666666666667e-07,
      "loss": 0.0028,
      "step": 148090
    },
    {
      "epoch": 7.898666666666666,
      "grad_norm": 0.0932471752166748,
      "learning_rate": 6.333333333333333e-07,
      "loss": 0.0017,
      "step": 148100
    },
    {
      "epoch": 7.8992,
      "grad_norm": 0.29840806126594543,
      "learning_rate": 6.3e-07,
      "loss": 0.0015,
      "step": 148110
    },
    {
      "epoch": 7.899733333333334,
      "grad_norm": 0.2037428617477417,
      "learning_rate": 6.266666666666668e-07,
      "loss": 0.0012,
      "step": 148120
    },
    {
      "epoch": 7.900266666666667,
      "grad_norm": 0.48420268297195435,
      "learning_rate": 6.233333333333333e-07,
      "loss": 0.0015,
      "step": 148130
    },
    {
      "epoch": 7.9008,
      "grad_norm": 0.05236734449863434,
      "learning_rate": 6.2e-07,
      "loss": 0.0021,
      "step": 148140
    },
    {
      "epoch": 7.9013333333333335,
      "grad_norm": 0.21390104293823242,
      "learning_rate": 6.166666666666667e-07,
      "loss": 0.0011,
      "step": 148150
    },
    {
      "epoch": 7.901866666666667,
      "grad_norm": 0.27101004123687744,
      "learning_rate": 6.133333333333334e-07,
      "loss": 0.0019,
      "step": 148160
    },
    {
      "epoch": 7.9024,
      "grad_norm": 0.20853933691978455,
      "learning_rate": 6.100000000000001e-07,
      "loss": 0.0022,
      "step": 148170
    },
    {
      "epoch": 7.902933333333333,
      "grad_norm": 0.16458000242710114,
      "learning_rate": 6.066666666666666e-07,
      "loss": 0.0018,
      "step": 148180
    },
    {
      "epoch": 7.903466666666667,
      "grad_norm": 0.08498701453208923,
      "learning_rate": 6.033333333333333e-07,
      "loss": 0.0013,
      "step": 148190
    },
    {
      "epoch": 7.904,
      "grad_norm": 0.311319500207901,
      "learning_rate": 6.000000000000001e-07,
      "loss": 0.0013,
      "step": 148200
    },
    {
      "epoch": 7.904533333333333,
      "grad_norm": 0.18959937989711761,
      "learning_rate": 5.966666666666667e-07,
      "loss": 0.0015,
      "step": 148210
    },
    {
      "epoch": 7.9050666666666665,
      "grad_norm": 0.23736143112182617,
      "learning_rate": 5.933333333333333e-07,
      "loss": 0.0018,
      "step": 148220
    },
    {
      "epoch": 7.9056,
      "grad_norm": 0.08992770314216614,
      "learning_rate": 5.9e-07,
      "loss": 0.0024,
      "step": 148230
    },
    {
      "epoch": 7.906133333333333,
      "grad_norm": 0.12575426697731018,
      "learning_rate": 5.866666666666667e-07,
      "loss": 0.0023,
      "step": 148240
    },
    {
      "epoch": 7.906666666666666,
      "grad_norm": 0.2682135999202728,
      "learning_rate": 5.833333333333334e-07,
      "loss": 0.0018,
      "step": 148250
    },
    {
      "epoch": 7.9072,
      "grad_norm": 0.442508339881897,
      "learning_rate": 5.8e-07,
      "loss": 0.0018,
      "step": 148260
    },
    {
      "epoch": 7.907733333333333,
      "grad_norm": 0.15375429391860962,
      "learning_rate": 5.766666666666667e-07,
      "loss": 0.0016,
      "step": 148270
    },
    {
      "epoch": 7.908266666666667,
      "grad_norm": 0.09184019267559052,
      "learning_rate": 5.733333333333334e-07,
      "loss": 0.002,
      "step": 148280
    },
    {
      "epoch": 7.9088,
      "grad_norm": 0.16417111456394196,
      "learning_rate": 5.7e-07,
      "loss": 0.0021,
      "step": 148290
    },
    {
      "epoch": 7.9093333333333335,
      "grad_norm": 0.1794751137495041,
      "learning_rate": 5.666666666666667e-07,
      "loss": 0.0018,
      "step": 148300
    },
    {
      "epoch": 7.909866666666667,
      "grad_norm": 0.3331472873687744,
      "learning_rate": 5.633333333333334e-07,
      "loss": 0.0022,
      "step": 148310
    },
    {
      "epoch": 7.9104,
      "grad_norm": 0.204657644033432,
      "learning_rate": 5.6e-07,
      "loss": 0.0021,
      "step": 148320
    },
    {
      "epoch": 7.910933333333333,
      "grad_norm": 0.13459186255931854,
      "learning_rate": 5.566666666666667e-07,
      "loss": 0.0027,
      "step": 148330
    },
    {
      "epoch": 7.911466666666667,
      "grad_norm": 0.210987389087677,
      "learning_rate": 5.533333333333334e-07,
      "loss": 0.0016,
      "step": 148340
    },
    {
      "epoch": 7.912,
      "grad_norm": 0.12010885775089264,
      "learning_rate": 5.5e-07,
      "loss": 0.0023,
      "step": 148350
    },
    {
      "epoch": 7.912533333333333,
      "grad_norm": 0.12100916355848312,
      "learning_rate": 5.466666666666667e-07,
      "loss": 0.0017,
      "step": 148360
    },
    {
      "epoch": 7.9130666666666665,
      "grad_norm": 0.06340615451335907,
      "learning_rate": 5.433333333333334e-07,
      "loss": 0.0021,
      "step": 148370
    },
    {
      "epoch": 7.9136,
      "grad_norm": 0.4583783447742462,
      "learning_rate": 5.4e-07,
      "loss": 0.0012,
      "step": 148380
    },
    {
      "epoch": 7.914133333333333,
      "grad_norm": 0.5852351784706116,
      "learning_rate": 5.366666666666667e-07,
      "loss": 0.0014,
      "step": 148390
    },
    {
      "epoch": 7.914666666666666,
      "grad_norm": 0.041045740246772766,
      "learning_rate": 5.333333333333333e-07,
      "loss": 0.0014,
      "step": 148400
    },
    {
      "epoch": 7.9152000000000005,
      "grad_norm": 0.1507699340581894,
      "learning_rate": 5.3e-07,
      "loss": 0.0011,
      "step": 148410
    },
    {
      "epoch": 7.915733333333334,
      "grad_norm": 0.20434445142745972,
      "learning_rate": 5.266666666666667e-07,
      "loss": 0.0013,
      "step": 148420
    },
    {
      "epoch": 7.916266666666667,
      "grad_norm": 0.3183591067790985,
      "learning_rate": 5.233333333333334e-07,
      "loss": 0.002,
      "step": 148430
    },
    {
      "epoch": 7.9168,
      "grad_norm": 0.17857332527637482,
      "learning_rate": 5.2e-07,
      "loss": 0.0024,
      "step": 148440
    },
    {
      "epoch": 7.917333333333334,
      "grad_norm": 0.10055225342512131,
      "learning_rate": 5.166666666666667e-07,
      "loss": 0.0027,
      "step": 148450
    },
    {
      "epoch": 7.917866666666667,
      "grad_norm": 0.06790436804294586,
      "learning_rate": 5.133333333333333e-07,
      "loss": 0.0016,
      "step": 148460
    },
    {
      "epoch": 7.9184,
      "grad_norm": 0.18594755232334137,
      "learning_rate": 5.100000000000001e-07,
      "loss": 0.0018,
      "step": 148470
    },
    {
      "epoch": 7.918933333333333,
      "grad_norm": 0.26110029220581055,
      "learning_rate": 5.066666666666667e-07,
      "loss": 0.0019,
      "step": 148480
    },
    {
      "epoch": 7.919466666666667,
      "grad_norm": 0.2764717936515808,
      "learning_rate": 5.033333333333333e-07,
      "loss": 0.0017,
      "step": 148490
    },
    {
      "epoch": 7.92,
      "grad_norm": 0.4600212275981903,
      "learning_rate": 5.000000000000001e-07,
      "loss": 0.0014,
      "step": 148500
    },
    {
      "epoch": 7.920533333333333,
      "grad_norm": 0.2160813808441162,
      "learning_rate": 4.966666666666667e-07,
      "loss": 0.0014,
      "step": 148510
    },
    {
      "epoch": 7.9210666666666665,
      "grad_norm": 0.1188584491610527,
      "learning_rate": 4.933333333333333e-07,
      "loss": 0.0011,
      "step": 148520
    },
    {
      "epoch": 7.9216,
      "grad_norm": 0.19032470881938934,
      "learning_rate": 4.9e-07,
      "loss": 0.0014,
      "step": 148530
    },
    {
      "epoch": 7.922133333333333,
      "grad_norm": 0.18528485298156738,
      "learning_rate": 4.866666666666667e-07,
      "loss": 0.0012,
      "step": 148540
    },
    {
      "epoch": 7.922666666666666,
      "grad_norm": 0.42349010705947876,
      "learning_rate": 4.833333333333334e-07,
      "loss": 0.0019,
      "step": 148550
    },
    {
      "epoch": 7.9232,
      "grad_norm": 0.13047422468662262,
      "learning_rate": 4.8e-07,
      "loss": 0.0019,
      "step": 148560
    },
    {
      "epoch": 7.923733333333333,
      "grad_norm": 0.3361952006816864,
      "learning_rate": 4.7666666666666667e-07,
      "loss": 0.0019,
      "step": 148570
    },
    {
      "epoch": 7.924266666666667,
      "grad_norm": 0.3937302231788635,
      "learning_rate": 4.7333333333333334e-07,
      "loss": 0.0014,
      "step": 148580
    },
    {
      "epoch": 7.9248,
      "grad_norm": 0.20661063492298126,
      "learning_rate": 4.7000000000000005e-07,
      "loss": 0.0014,
      "step": 148590
    },
    {
      "epoch": 7.925333333333334,
      "grad_norm": 0.3323538303375244,
      "learning_rate": 4.666666666666667e-07,
      "loss": 0.0017,
      "step": 148600
    },
    {
      "epoch": 7.925866666666667,
      "grad_norm": 0.1215822622179985,
      "learning_rate": 4.6333333333333333e-07,
      "loss": 0.0017,
      "step": 148610
    },
    {
      "epoch": 7.9264,
      "grad_norm": 0.21348418295383453,
      "learning_rate": 4.6e-07,
      "loss": 0.0018,
      "step": 148620
    },
    {
      "epoch": 7.926933333333333,
      "grad_norm": 0.38858211040496826,
      "learning_rate": 4.566666666666667e-07,
      "loss": 0.0015,
      "step": 148630
    },
    {
      "epoch": 7.927466666666667,
      "grad_norm": 0.2375132441520691,
      "learning_rate": 4.5333333333333337e-07,
      "loss": 0.002,
      "step": 148640
    },
    {
      "epoch": 7.928,
      "grad_norm": 0.1653793752193451,
      "learning_rate": 4.5e-07,
      "loss": 0.0016,
      "step": 148650
    },
    {
      "epoch": 7.928533333333333,
      "grad_norm": 0.06534509360790253,
      "learning_rate": 4.466666666666667e-07,
      "loss": 0.0018,
      "step": 148660
    },
    {
      "epoch": 7.9290666666666665,
      "grad_norm": 0.05515885353088379,
      "learning_rate": 4.4333333333333336e-07,
      "loss": 0.0015,
      "step": 148670
    },
    {
      "epoch": 7.9296,
      "grad_norm": 0.09870189428329468,
      "learning_rate": 4.4e-07,
      "loss": 0.0016,
      "step": 148680
    },
    {
      "epoch": 7.930133333333333,
      "grad_norm": 0.6640517115592957,
      "learning_rate": 4.3666666666666663e-07,
      "loss": 0.0015,
      "step": 148690
    },
    {
      "epoch": 7.930666666666666,
      "grad_norm": 0.04515386372804642,
      "learning_rate": 4.3333333333333335e-07,
      "loss": 0.0025,
      "step": 148700
    },
    {
      "epoch": 7.9312000000000005,
      "grad_norm": 0.06773391366004944,
      "learning_rate": 4.3e-07,
      "loss": 0.0023,
      "step": 148710
    },
    {
      "epoch": 7.931733333333334,
      "grad_norm": 0.13292841613292694,
      "learning_rate": 4.2666666666666673e-07,
      "loss": 0.0014,
      "step": 148720
    },
    {
      "epoch": 7.932266666666667,
      "grad_norm": 0.11659013479948044,
      "learning_rate": 4.233333333333334e-07,
      "loss": 0.0019,
      "step": 148730
    },
    {
      "epoch": 7.9328,
      "grad_norm": 0.10456334799528122,
      "learning_rate": 4.2e-07,
      "loss": 0.0019,
      "step": 148740
    },
    {
      "epoch": 7.933333333333334,
      "grad_norm": 0.12090451270341873,
      "learning_rate": 4.1666666666666667e-07,
      "loss": 0.0017,
      "step": 148750
    },
    {
      "epoch": 7.933866666666667,
      "grad_norm": 0.17343789339065552,
      "learning_rate": 4.133333333333334e-07,
      "loss": 0.0012,
      "step": 148760
    },
    {
      "epoch": 7.9344,
      "grad_norm": 0.06787741184234619,
      "learning_rate": 4.1000000000000004e-07,
      "loss": 0.0023,
      "step": 148770
    },
    {
      "epoch": 7.934933333333333,
      "grad_norm": 0.37921276688575745,
      "learning_rate": 4.0666666666666666e-07,
      "loss": 0.0014,
      "step": 148780
    },
    {
      "epoch": 7.935466666666667,
      "grad_norm": 0.14699698984622955,
      "learning_rate": 4.033333333333333e-07,
      "loss": 0.0019,
      "step": 148790
    },
    {
      "epoch": 7.936,
      "grad_norm": 0.07185245305299759,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 0.0019,
      "step": 148800
    },
    {
      "epoch": 7.936533333333333,
      "grad_norm": 0.07887547463178635,
      "learning_rate": 3.966666666666667e-07,
      "loss": 0.0022,
      "step": 148810
    },
    {
      "epoch": 7.9370666666666665,
      "grad_norm": 0.3274560868740082,
      "learning_rate": 3.933333333333333e-07,
      "loss": 0.0024,
      "step": 148820
    },
    {
      "epoch": 7.9376,
      "grad_norm": 0.24603457748889923,
      "learning_rate": 3.9e-07,
      "loss": 0.0013,
      "step": 148830
    },
    {
      "epoch": 7.938133333333333,
      "grad_norm": 0.094007708132267,
      "learning_rate": 3.866666666666667e-07,
      "loss": 0.0018,
      "step": 148840
    },
    {
      "epoch": 7.938666666666666,
      "grad_norm": 0.2401503324508667,
      "learning_rate": 3.8333333333333335e-07,
      "loss": 0.0021,
      "step": 148850
    },
    {
      "epoch": 7.9392,
      "grad_norm": 0.09384576231241226,
      "learning_rate": 3.8e-07,
      "loss": 0.002,
      "step": 148860
    },
    {
      "epoch": 7.939733333333333,
      "grad_norm": 0.06914713233709335,
      "learning_rate": 3.766666666666667e-07,
      "loss": 0.0013,
      "step": 148870
    },
    {
      "epoch": 7.940266666666667,
      "grad_norm": 0.2616962194442749,
      "learning_rate": 3.7333333333333334e-07,
      "loss": 0.0023,
      "step": 148880
    },
    {
      "epoch": 7.9408,
      "grad_norm": 0.4264448881149292,
      "learning_rate": 3.7000000000000006e-07,
      "loss": 0.0017,
      "step": 148890
    },
    {
      "epoch": 7.941333333333334,
      "grad_norm": 0.04157789424061775,
      "learning_rate": 3.6666666666666667e-07,
      "loss": 0.0018,
      "step": 148900
    },
    {
      "epoch": 7.941866666666667,
      "grad_norm": 0.12132415920495987,
      "learning_rate": 3.633333333333334e-07,
      "loss": 0.0013,
      "step": 148910
    },
    {
      "epoch": 7.9424,
      "grad_norm": 0.2683047652244568,
      "learning_rate": 3.6e-07,
      "loss": 0.0018,
      "step": 148920
    },
    {
      "epoch": 7.942933333333333,
      "grad_norm": 0.23846162855625153,
      "learning_rate": 3.566666666666667e-07,
      "loss": 0.0018,
      "step": 148930
    },
    {
      "epoch": 7.943466666666667,
      "grad_norm": 0.09852659702301025,
      "learning_rate": 3.533333333333333e-07,
      "loss": 0.0015,
      "step": 148940
    },
    {
      "epoch": 7.944,
      "grad_norm": 0.153094083070755,
      "learning_rate": 3.5000000000000004e-07,
      "loss": 0.0013,
      "step": 148950
    },
    {
      "epoch": 7.944533333333333,
      "grad_norm": 0.19984178245067596,
      "learning_rate": 3.4666666666666665e-07,
      "loss": 0.0019,
      "step": 148960
    },
    {
      "epoch": 7.9450666666666665,
      "grad_norm": 0.1195458471775055,
      "learning_rate": 3.4333333333333336e-07,
      "loss": 0.002,
      "step": 148970
    },
    {
      "epoch": 7.9456,
      "grad_norm": 0.06791196018457413,
      "learning_rate": 3.4e-07,
      "loss": 0.0016,
      "step": 148980
    },
    {
      "epoch": 7.946133333333333,
      "grad_norm": 0.08319259434938431,
      "learning_rate": 3.366666666666667e-07,
      "loss": 0.0019,
      "step": 148990
    },
    {
      "epoch": 7.946666666666666,
      "grad_norm": 0.062484901398420334,
      "learning_rate": 3.3333333333333335e-07,
      "loss": 0.0023,
      "step": 149000
    },
    {
      "epoch": 7.9472000000000005,
      "grad_norm": 0.2611315846443176,
      "learning_rate": 3.3e-07,
      "loss": 0.0019,
      "step": 149010
    },
    {
      "epoch": 7.947733333333334,
      "grad_norm": 0.08760181069374084,
      "learning_rate": 3.266666666666667e-07,
      "loss": 0.0015,
      "step": 149020
    },
    {
      "epoch": 7.948266666666667,
      "grad_norm": 0.14526964724063873,
      "learning_rate": 3.2333333333333334e-07,
      "loss": 0.0011,
      "step": 149030
    },
    {
      "epoch": 7.9488,
      "grad_norm": 0.20838111639022827,
      "learning_rate": 3.2e-07,
      "loss": 0.0022,
      "step": 149040
    },
    {
      "epoch": 7.949333333333334,
      "grad_norm": 0.043937984853982925,
      "learning_rate": 3.1666666666666667e-07,
      "loss": 0.0011,
      "step": 149050
    },
    {
      "epoch": 7.949866666666667,
      "grad_norm": 0.22933538258075714,
      "learning_rate": 3.133333333333334e-07,
      "loss": 0.0016,
      "step": 149060
    },
    {
      "epoch": 7.9504,
      "grad_norm": 0.08420988917350769,
      "learning_rate": 3.1e-07,
      "loss": 0.0019,
      "step": 149070
    },
    {
      "epoch": 7.950933333333333,
      "grad_norm": 0.06786930561065674,
      "learning_rate": 3.066666666666667e-07,
      "loss": 0.0027,
      "step": 149080
    },
    {
      "epoch": 7.951466666666667,
      "grad_norm": 0.26366329193115234,
      "learning_rate": 3.033333333333333e-07,
      "loss": 0.0013,
      "step": 149090
    },
    {
      "epoch": 7.952,
      "grad_norm": 0.12253779917955399,
      "learning_rate": 3.0000000000000004e-07,
      "loss": 0.0019,
      "step": 149100
    },
    {
      "epoch": 7.952533333333333,
      "grad_norm": 0.24752593040466309,
      "learning_rate": 2.9666666666666665e-07,
      "loss": 0.0024,
      "step": 149110
    },
    {
      "epoch": 7.9530666666666665,
      "grad_norm": 0.025971319526433945,
      "learning_rate": 2.9333333333333337e-07,
      "loss": 0.0012,
      "step": 149120
    },
    {
      "epoch": 7.9536,
      "grad_norm": 0.15856149792671204,
      "learning_rate": 2.9e-07,
      "loss": 0.0014,
      "step": 149130
    },
    {
      "epoch": 7.954133333333333,
      "grad_norm": 0.2548120617866516,
      "learning_rate": 2.866666666666667e-07,
      "loss": 0.0016,
      "step": 149140
    },
    {
      "epoch": 7.954666666666666,
      "grad_norm": 0.03166598454117775,
      "learning_rate": 2.8333333333333336e-07,
      "loss": 0.002,
      "step": 149150
    },
    {
      "epoch": 7.9552,
      "grad_norm": 0.1473492830991745,
      "learning_rate": 2.8e-07,
      "loss": 0.0014,
      "step": 149160
    },
    {
      "epoch": 7.955733333333333,
      "grad_norm": 0.10428180545568466,
      "learning_rate": 2.766666666666667e-07,
      "loss": 0.0027,
      "step": 149170
    },
    {
      "epoch": 7.956266666666667,
      "grad_norm": 0.06670451909303665,
      "learning_rate": 2.7333333333333335e-07,
      "loss": 0.0015,
      "step": 149180
    },
    {
      "epoch": 7.9568,
      "grad_norm": 0.12306962162256241,
      "learning_rate": 2.7e-07,
      "loss": 0.003,
      "step": 149190
    },
    {
      "epoch": 7.957333333333334,
      "grad_norm": 0.2632503807544708,
      "learning_rate": 2.6666666666666667e-07,
      "loss": 0.0014,
      "step": 149200
    },
    {
      "epoch": 7.957866666666667,
      "grad_norm": 0.5303655862808228,
      "learning_rate": 2.6333333333333334e-07,
      "loss": 0.0019,
      "step": 149210
    },
    {
      "epoch": 7.9584,
      "grad_norm": 0.37831446528434753,
      "learning_rate": 2.6e-07,
      "loss": 0.0029,
      "step": 149220
    },
    {
      "epoch": 7.958933333333333,
      "grad_norm": 0.12032776325941086,
      "learning_rate": 2.5666666666666666e-07,
      "loss": 0.0032,
      "step": 149230
    },
    {
      "epoch": 7.959466666666667,
      "grad_norm": 0.3562966585159302,
      "learning_rate": 2.533333333333333e-07,
      "loss": 0.002,
      "step": 149240
    },
    {
      "epoch": 7.96,
      "grad_norm": 0.4126602113246918,
      "learning_rate": 2.5000000000000004e-07,
      "loss": 0.0026,
      "step": 149250
    },
    {
      "epoch": 7.960533333333333,
      "grad_norm": 0.43875765800476074,
      "learning_rate": 2.4666666666666665e-07,
      "loss": 0.0017,
      "step": 149260
    },
    {
      "epoch": 7.9610666666666665,
      "grad_norm": 0.06244521960616112,
      "learning_rate": 2.4333333333333337e-07,
      "loss": 0.0021,
      "step": 149270
    },
    {
      "epoch": 7.9616,
      "grad_norm": 0.06313624978065491,
      "learning_rate": 2.4e-07,
      "loss": 0.002,
      "step": 149280
    },
    {
      "epoch": 7.962133333333333,
      "grad_norm": 0.10584309697151184,
      "learning_rate": 2.3666666666666667e-07,
      "loss": 0.0023,
      "step": 149290
    },
    {
      "epoch": 7.962666666666666,
      "grad_norm": 0.2091602087020874,
      "learning_rate": 2.3333333333333336e-07,
      "loss": 0.0014,
      "step": 149300
    },
    {
      "epoch": 7.9632,
      "grad_norm": 0.07372667640447617,
      "learning_rate": 2.3e-07,
      "loss": 0.0014,
      "step": 149310
    },
    {
      "epoch": 7.963733333333334,
      "grad_norm": 0.07011473178863525,
      "learning_rate": 2.2666666666666668e-07,
      "loss": 0.002,
      "step": 149320
    },
    {
      "epoch": 7.964266666666667,
      "grad_norm": 0.29429638385772705,
      "learning_rate": 2.2333333333333335e-07,
      "loss": 0.0034,
      "step": 149330
    },
    {
      "epoch": 7.9648,
      "grad_norm": 0.13239116966724396,
      "learning_rate": 2.2e-07,
      "loss": 0.0015,
      "step": 149340
    },
    {
      "epoch": 7.965333333333334,
      "grad_norm": 0.3886571526527405,
      "learning_rate": 2.1666666666666667e-07,
      "loss": 0.0019,
      "step": 149350
    },
    {
      "epoch": 7.965866666666667,
      "grad_norm": 0.06461688131093979,
      "learning_rate": 2.1333333333333336e-07,
      "loss": 0.0014,
      "step": 149360
    },
    {
      "epoch": 7.9664,
      "grad_norm": 0.04179084300994873,
      "learning_rate": 2.1e-07,
      "loss": 0.0011,
      "step": 149370
    },
    {
      "epoch": 7.966933333333333,
      "grad_norm": 0.2038729339838028,
      "learning_rate": 2.066666666666667e-07,
      "loss": 0.0022,
      "step": 149380
    },
    {
      "epoch": 7.967466666666667,
      "grad_norm": 0.14646200835704803,
      "learning_rate": 2.0333333333333333e-07,
      "loss": 0.0019,
      "step": 149390
    },
    {
      "epoch": 7.968,
      "grad_norm": 0.29829469323158264,
      "learning_rate": 2.0000000000000002e-07,
      "loss": 0.0015,
      "step": 149400
    },
    {
      "epoch": 7.968533333333333,
      "grad_norm": 0.1795111745595932,
      "learning_rate": 1.9666666666666665e-07,
      "loss": 0.0014,
      "step": 149410
    },
    {
      "epoch": 7.9690666666666665,
      "grad_norm": 0.04116585850715637,
      "learning_rate": 1.9333333333333334e-07,
      "loss": 0.0019,
      "step": 149420
    },
    {
      "epoch": 7.9696,
      "grad_norm": 0.03417382016777992,
      "learning_rate": 1.9e-07,
      "loss": 0.0013,
      "step": 149430
    },
    {
      "epoch": 7.970133333333333,
      "grad_norm": 0.4312576949596405,
      "learning_rate": 1.8666666666666667e-07,
      "loss": 0.0016,
      "step": 149440
    },
    {
      "epoch": 7.970666666666666,
      "grad_norm": 0.051019515842199326,
      "learning_rate": 1.8333333333333333e-07,
      "loss": 0.0017,
      "step": 149450
    },
    {
      "epoch": 7.9712,
      "grad_norm": 0.293491393327713,
      "learning_rate": 1.8e-07,
      "loss": 0.0021,
      "step": 149460
    },
    {
      "epoch": 7.971733333333333,
      "grad_norm": 0.09079502522945404,
      "learning_rate": 1.7666666666666666e-07,
      "loss": 0.0023,
      "step": 149470
    },
    {
      "epoch": 7.972266666666666,
      "grad_norm": 0.33669066429138184,
      "learning_rate": 1.7333333333333332e-07,
      "loss": 0.0019,
      "step": 149480
    },
    {
      "epoch": 7.9728,
      "grad_norm": 0.26136502623558044,
      "learning_rate": 1.7e-07,
      "loss": 0.0016,
      "step": 149490
    },
    {
      "epoch": 7.973333333333334,
      "grad_norm": 0.14761774241924286,
      "learning_rate": 1.6666666666666668e-07,
      "loss": 0.0013,
      "step": 149500
    }
  ],
  "logging_steps": 10,
  "max_steps": 150000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 8,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 40,
  "trial_name": null,
  "trial_params": null
}

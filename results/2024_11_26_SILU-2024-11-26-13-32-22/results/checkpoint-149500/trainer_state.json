{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 7.973333333333334,
  "eval_steps": 500,
  "global_step": 149500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0005333333333333334,
      "grad_norm": 1.326286792755127,
      "learning_rate": 4.999666666666667e-05,
      "loss": 0.0328,
      "step": 10
    },
    {
      "epoch": 0.0010666666666666667,
      "grad_norm": 0.8520083427429199,
      "learning_rate": 4.9993333333333335e-05,
      "loss": 0.0156,
      "step": 20
    },
    {
      "epoch": 0.0016,
      "grad_norm": 1.271254539489746,
      "learning_rate": 4.999e-05,
      "loss": 0.0102,
      "step": 30
    },
    {
      "epoch": 0.0021333333333333334,
      "grad_norm": 0.913791298866272,
      "learning_rate": 4.9986666666666674e-05,
      "loss": 0.0084,
      "step": 40
    },
    {
      "epoch": 0.0026666666666666666,
      "grad_norm": 0.2139165699481964,
      "learning_rate": 4.998333333333334e-05,
      "loss": 0.0062,
      "step": 50
    },
    {
      "epoch": 0.0032,
      "grad_norm": 0.34764158725738525,
      "learning_rate": 4.9980000000000006e-05,
      "loss": 0.0089,
      "step": 60
    },
    {
      "epoch": 0.0037333333333333333,
      "grad_norm": 0.48827385902404785,
      "learning_rate": 4.997666666666667e-05,
      "loss": 0.0075,
      "step": 70
    },
    {
      "epoch": 0.004266666666666667,
      "grad_norm": 0.2836458384990692,
      "learning_rate": 4.997333333333333e-05,
      "loss": 0.0074,
      "step": 80
    },
    {
      "epoch": 0.0048,
      "grad_norm": 0.5590843558311462,
      "learning_rate": 4.997e-05,
      "loss": 0.0075,
      "step": 90
    },
    {
      "epoch": 0.005333333333333333,
      "grad_norm": 0.41888150572776794,
      "learning_rate": 4.996666666666667e-05,
      "loss": 0.0064,
      "step": 100
    },
    {
      "epoch": 0.005866666666666667,
      "grad_norm": 0.6947826147079468,
      "learning_rate": 4.996333333333334e-05,
      "loss": 0.0057,
      "step": 110
    },
    {
      "epoch": 0.0064,
      "grad_norm": 0.9058571457862854,
      "learning_rate": 4.996e-05,
      "loss": 0.0067,
      "step": 120
    },
    {
      "epoch": 0.006933333333333333,
      "grad_norm": 0.6253117918968201,
      "learning_rate": 4.995666666666667e-05,
      "loss": 0.0077,
      "step": 130
    },
    {
      "epoch": 0.007466666666666667,
      "grad_norm": 0.1554277092218399,
      "learning_rate": 4.9953333333333335e-05,
      "loss": 0.0064,
      "step": 140
    },
    {
      "epoch": 0.008,
      "grad_norm": 0.8287565112113953,
      "learning_rate": 4.995e-05,
      "loss": 0.0056,
      "step": 150
    },
    {
      "epoch": 0.008533333333333334,
      "grad_norm": 0.4155031442642212,
      "learning_rate": 4.994666666666667e-05,
      "loss": 0.0068,
      "step": 160
    },
    {
      "epoch": 0.009066666666666667,
      "grad_norm": 0.20916999876499176,
      "learning_rate": 4.9943333333333333e-05,
      "loss": 0.0049,
      "step": 170
    },
    {
      "epoch": 0.0096,
      "grad_norm": 0.6917047500610352,
      "learning_rate": 4.9940000000000006e-05,
      "loss": 0.0087,
      "step": 180
    },
    {
      "epoch": 0.010133333333333333,
      "grad_norm": 0.7583746910095215,
      "learning_rate": 4.993666666666667e-05,
      "loss": 0.0074,
      "step": 190
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 1.0385884046554565,
      "learning_rate": 4.993333333333334e-05,
      "loss": 0.0077,
      "step": 200
    },
    {
      "epoch": 0.0112,
      "grad_norm": 0.1536990851163864,
      "learning_rate": 4.9930000000000005e-05,
      "loss": 0.0061,
      "step": 210
    },
    {
      "epoch": 0.011733333333333333,
      "grad_norm": 0.21333634853363037,
      "learning_rate": 4.992666666666667e-05,
      "loss": 0.0056,
      "step": 220
    },
    {
      "epoch": 0.012266666666666667,
      "grad_norm": 0.28335022926330566,
      "learning_rate": 4.992333333333333e-05,
      "loss": 0.0064,
      "step": 230
    },
    {
      "epoch": 0.0128,
      "grad_norm": 0.762286365032196,
      "learning_rate": 4.992e-05,
      "loss": 0.0073,
      "step": 240
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 0.2762136459350586,
      "learning_rate": 4.991666666666667e-05,
      "loss": 0.0053,
      "step": 250
    },
    {
      "epoch": 0.013866666666666666,
      "grad_norm": 0.6875383853912354,
      "learning_rate": 4.9913333333333335e-05,
      "loss": 0.0067,
      "step": 260
    },
    {
      "epoch": 0.0144,
      "grad_norm": 0.6199691891670227,
      "learning_rate": 4.991e-05,
      "loss": 0.0063,
      "step": 270
    },
    {
      "epoch": 0.014933333333333333,
      "grad_norm": 0.5599243640899658,
      "learning_rate": 4.990666666666667e-05,
      "loss": 0.0059,
      "step": 280
    },
    {
      "epoch": 0.015466666666666667,
      "grad_norm": 0.8275566101074219,
      "learning_rate": 4.9903333333333334e-05,
      "loss": 0.007,
      "step": 290
    },
    {
      "epoch": 0.016,
      "grad_norm": 0.6267344951629639,
      "learning_rate": 4.99e-05,
      "loss": 0.0058,
      "step": 300
    },
    {
      "epoch": 0.016533333333333334,
      "grad_norm": 0.41533026099205017,
      "learning_rate": 4.9896666666666666e-05,
      "loss": 0.0058,
      "step": 310
    },
    {
      "epoch": 0.017066666666666667,
      "grad_norm": 0.6210664510726929,
      "learning_rate": 4.989333333333334e-05,
      "loss": 0.0067,
      "step": 320
    },
    {
      "epoch": 0.0176,
      "grad_norm": 0.6918396949768066,
      "learning_rate": 4.9890000000000005e-05,
      "loss": 0.0072,
      "step": 330
    },
    {
      "epoch": 0.018133333333333335,
      "grad_norm": 0.8264748454093933,
      "learning_rate": 4.988666666666667e-05,
      "loss": 0.0066,
      "step": 340
    },
    {
      "epoch": 0.018666666666666668,
      "grad_norm": 0.35232317447662354,
      "learning_rate": 4.988333333333334e-05,
      "loss": 0.0073,
      "step": 350
    },
    {
      "epoch": 0.0192,
      "grad_norm": 0.42194968461990356,
      "learning_rate": 4.9880000000000004e-05,
      "loss": 0.0079,
      "step": 360
    },
    {
      "epoch": 0.019733333333333332,
      "grad_norm": 0.04717164859175682,
      "learning_rate": 4.987666666666667e-05,
      "loss": 0.007,
      "step": 370
    },
    {
      "epoch": 0.020266666666666665,
      "grad_norm": 0.21519680321216583,
      "learning_rate": 4.9873333333333336e-05,
      "loss": 0.0067,
      "step": 380
    },
    {
      "epoch": 0.0208,
      "grad_norm": 0.34410813450813293,
      "learning_rate": 4.987e-05,
      "loss": 0.0068,
      "step": 390
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 0.2098083198070526,
      "learning_rate": 4.986666666666667e-05,
      "loss": 0.0067,
      "step": 400
    },
    {
      "epoch": 0.021866666666666666,
      "grad_norm": 0.895215630531311,
      "learning_rate": 4.9863333333333334e-05,
      "loss": 0.0048,
      "step": 410
    },
    {
      "epoch": 0.0224,
      "grad_norm": 0.8921656012535095,
      "learning_rate": 4.986e-05,
      "loss": 0.0061,
      "step": 420
    },
    {
      "epoch": 0.022933333333333333,
      "grad_norm": 0.6815988421440125,
      "learning_rate": 4.9856666666666666e-05,
      "loss": 0.0062,
      "step": 430
    },
    {
      "epoch": 0.023466666666666667,
      "grad_norm": 0.5447400808334351,
      "learning_rate": 4.985333333333333e-05,
      "loss": 0.0054,
      "step": 440
    },
    {
      "epoch": 0.024,
      "grad_norm": 0.6137309074401855,
      "learning_rate": 4.9850000000000006e-05,
      "loss": 0.0055,
      "step": 450
    },
    {
      "epoch": 0.024533333333333334,
      "grad_norm": 0.20375055074691772,
      "learning_rate": 4.984666666666667e-05,
      "loss": 0.005,
      "step": 460
    },
    {
      "epoch": 0.025066666666666668,
      "grad_norm": 0.28023335337638855,
      "learning_rate": 4.984333333333334e-05,
      "loss": 0.0052,
      "step": 470
    },
    {
      "epoch": 0.0256,
      "grad_norm": 0.42132097482681274,
      "learning_rate": 4.9840000000000004e-05,
      "loss": 0.0063,
      "step": 480
    },
    {
      "epoch": 0.026133333333333335,
      "grad_norm": 0.07890752702951431,
      "learning_rate": 4.983666666666667e-05,
      "loss": 0.0059,
      "step": 490
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.08093194663524628,
      "learning_rate": 4.9833333333333336e-05,
      "loss": 0.0068,
      "step": 500
    },
    {
      "epoch": 0.0272,
      "grad_norm": 0.27271872758865356,
      "learning_rate": 4.983e-05,
      "loss": 0.0071,
      "step": 510
    },
    {
      "epoch": 0.027733333333333332,
      "grad_norm": 0.7510265111923218,
      "learning_rate": 4.982666666666667e-05,
      "loss": 0.006,
      "step": 520
    },
    {
      "epoch": 0.028266666666666666,
      "grad_norm": 0.5476630926132202,
      "learning_rate": 4.9823333333333335e-05,
      "loss": 0.006,
      "step": 530
    },
    {
      "epoch": 0.0288,
      "grad_norm": 0.4087041914463043,
      "learning_rate": 4.982e-05,
      "loss": 0.0076,
      "step": 540
    },
    {
      "epoch": 0.029333333333333333,
      "grad_norm": 0.0909612700343132,
      "learning_rate": 4.981666666666667e-05,
      "loss": 0.0071,
      "step": 550
    },
    {
      "epoch": 0.029866666666666666,
      "grad_norm": 0.6104921102523804,
      "learning_rate": 4.981333333333333e-05,
      "loss": 0.0067,
      "step": 560
    },
    {
      "epoch": 0.0304,
      "grad_norm": 0.3442166745662689,
      "learning_rate": 4.981e-05,
      "loss": 0.0075,
      "step": 570
    },
    {
      "epoch": 0.030933333333333334,
      "grad_norm": 0.6149042248725891,
      "learning_rate": 4.9806666666666665e-05,
      "loss": 0.007,
      "step": 580
    },
    {
      "epoch": 0.031466666666666664,
      "grad_norm": 0.34529075026512146,
      "learning_rate": 4.980333333333334e-05,
      "loss": 0.0051,
      "step": 590
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.20682619512081146,
      "learning_rate": 4.9800000000000004e-05,
      "loss": 0.0063,
      "step": 600
    },
    {
      "epoch": 0.03253333333333333,
      "grad_norm": 0.07998719811439514,
      "learning_rate": 4.979666666666667e-05,
      "loss": 0.0074,
      "step": 610
    },
    {
      "epoch": 0.03306666666666667,
      "grad_norm": 0.1393258422613144,
      "learning_rate": 4.9793333333333337e-05,
      "loss": 0.0068,
      "step": 620
    },
    {
      "epoch": 0.0336,
      "grad_norm": 0.20977428555488586,
      "learning_rate": 4.979e-05,
      "loss": 0.0071,
      "step": 630
    },
    {
      "epoch": 0.034133333333333335,
      "grad_norm": 0.4828236699104309,
      "learning_rate": 4.978666666666667e-05,
      "loss": 0.0064,
      "step": 640
    },
    {
      "epoch": 0.034666666666666665,
      "grad_norm": 0.0828016847372055,
      "learning_rate": 4.9783333333333335e-05,
      "loss": 0.0076,
      "step": 650
    },
    {
      "epoch": 0.0352,
      "grad_norm": 0.2056284099817276,
      "learning_rate": 4.978e-05,
      "loss": 0.0062,
      "step": 660
    },
    {
      "epoch": 0.03573333333333333,
      "grad_norm": 0.04328853264451027,
      "learning_rate": 4.9776666666666674e-05,
      "loss": 0.0063,
      "step": 670
    },
    {
      "epoch": 0.03626666666666667,
      "grad_norm": 0.8191661238670349,
      "learning_rate": 4.977333333333334e-05,
      "loss": 0.0052,
      "step": 680
    },
    {
      "epoch": 0.0368,
      "grad_norm": 0.15248972177505493,
      "learning_rate": 4.977e-05,
      "loss": 0.0077,
      "step": 690
    },
    {
      "epoch": 0.037333333333333336,
      "grad_norm": 0.1412602812051773,
      "learning_rate": 4.9766666666666666e-05,
      "loss": 0.0079,
      "step": 700
    },
    {
      "epoch": 0.037866666666666667,
      "grad_norm": 0.4743388593196869,
      "learning_rate": 4.976333333333333e-05,
      "loss": 0.0079,
      "step": 710
    },
    {
      "epoch": 0.0384,
      "grad_norm": 0.3529849052429199,
      "learning_rate": 4.976e-05,
      "loss": 0.0044,
      "step": 720
    },
    {
      "epoch": 0.038933333333333334,
      "grad_norm": 0.4778462052345276,
      "learning_rate": 4.975666666666667e-05,
      "loss": 0.0074,
      "step": 730
    },
    {
      "epoch": 0.039466666666666664,
      "grad_norm": 0.08742348849773407,
      "learning_rate": 4.975333333333334e-05,
      "loss": 0.0045,
      "step": 740
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.09783915430307388,
      "learning_rate": 4.975e-05,
      "loss": 0.0055,
      "step": 750
    },
    {
      "epoch": 0.04053333333333333,
      "grad_norm": 0.47898605465888977,
      "learning_rate": 4.974666666666667e-05,
      "loss": 0.0056,
      "step": 760
    },
    {
      "epoch": 0.04106666666666667,
      "grad_norm": 0.48042798042297363,
      "learning_rate": 4.9743333333333335e-05,
      "loss": 0.0055,
      "step": 770
    },
    {
      "epoch": 0.0416,
      "grad_norm": 0.27239906787872314,
      "learning_rate": 4.974e-05,
      "loss": 0.0076,
      "step": 780
    },
    {
      "epoch": 0.042133333333333335,
      "grad_norm": 0.34125444293022156,
      "learning_rate": 4.973666666666667e-05,
      "loss": 0.0061,
      "step": 790
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 0.34027159214019775,
      "learning_rate": 4.973333333333334e-05,
      "loss": 0.0062,
      "step": 800
    },
    {
      "epoch": 0.0432,
      "grad_norm": 0.7438278198242188,
      "learning_rate": 4.973000000000001e-05,
      "loss": 0.0096,
      "step": 810
    },
    {
      "epoch": 0.04373333333333333,
      "grad_norm": 0.7391599416732788,
      "learning_rate": 4.972666666666667e-05,
      "loss": 0.0068,
      "step": 820
    },
    {
      "epoch": 0.04426666666666667,
      "grad_norm": 0.5429314970970154,
      "learning_rate": 4.972333333333334e-05,
      "loss": 0.005,
      "step": 830
    },
    {
      "epoch": 0.0448,
      "grad_norm": 0.7417153120040894,
      "learning_rate": 4.972e-05,
      "loss": 0.0063,
      "step": 840
    },
    {
      "epoch": 0.04533333333333334,
      "grad_norm": 0.037482865154743195,
      "learning_rate": 4.9716666666666664e-05,
      "loss": 0.0088,
      "step": 850
    },
    {
      "epoch": 0.04586666666666667,
      "grad_norm": 0.6054004430770874,
      "learning_rate": 4.971333333333334e-05,
      "loss": 0.0083,
      "step": 860
    },
    {
      "epoch": 0.0464,
      "grad_norm": 0.4062991440296173,
      "learning_rate": 4.9710000000000003e-05,
      "loss": 0.004,
      "step": 870
    },
    {
      "epoch": 0.046933333333333334,
      "grad_norm": 0.139036625623703,
      "learning_rate": 4.970666666666667e-05,
      "loss": 0.0069,
      "step": 880
    },
    {
      "epoch": 0.047466666666666664,
      "grad_norm": 0.8838128447532654,
      "learning_rate": 4.9703333333333336e-05,
      "loss": 0.0055,
      "step": 890
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.07875414937734604,
      "learning_rate": 4.97e-05,
      "loss": 0.0064,
      "step": 900
    },
    {
      "epoch": 0.04853333333333333,
      "grad_norm": 0.5394332408905029,
      "learning_rate": 4.969666666666667e-05,
      "loss": 0.0041,
      "step": 910
    },
    {
      "epoch": 0.04906666666666667,
      "grad_norm": 0.2127261906862259,
      "learning_rate": 4.9693333333333334e-05,
      "loss": 0.0085,
      "step": 920
    },
    {
      "epoch": 0.0496,
      "grad_norm": 0.20187877118587494,
      "learning_rate": 4.969e-05,
      "loss": 0.0063,
      "step": 930
    },
    {
      "epoch": 0.050133333333333335,
      "grad_norm": 0.080885149538517,
      "learning_rate": 4.968666666666667e-05,
      "loss": 0.0067,
      "step": 940
    },
    {
      "epoch": 0.050666666666666665,
      "grad_norm": 0.13996809720993042,
      "learning_rate": 4.968333333333334e-05,
      "loss": 0.0046,
      "step": 950
    },
    {
      "epoch": 0.0512,
      "grad_norm": 0.14560472965240479,
      "learning_rate": 4.9680000000000005e-05,
      "loss": 0.0061,
      "step": 960
    },
    {
      "epoch": 0.05173333333333333,
      "grad_norm": 1.0115516185760498,
      "learning_rate": 4.967666666666667e-05,
      "loss": 0.007,
      "step": 970
    },
    {
      "epoch": 0.05226666666666667,
      "grad_norm": 0.6057088375091553,
      "learning_rate": 4.967333333333334e-05,
      "loss": 0.0063,
      "step": 980
    },
    {
      "epoch": 0.0528,
      "grad_norm": 0.403579443693161,
      "learning_rate": 4.967e-05,
      "loss": 0.0056,
      "step": 990
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.20197351276874542,
      "learning_rate": 4.966666666666667e-05,
      "loss": 0.0077,
      "step": 1000
    },
    {
      "epoch": 0.05386666666666667,
      "grad_norm": 0.6030053496360779,
      "learning_rate": 4.9663333333333336e-05,
      "loss": 0.0058,
      "step": 1010
    },
    {
      "epoch": 0.0544,
      "grad_norm": 0.40961334109306335,
      "learning_rate": 4.966e-05,
      "loss": 0.0082,
      "step": 1020
    },
    {
      "epoch": 0.054933333333333334,
      "grad_norm": 0.1504836082458496,
      "learning_rate": 4.965666666666667e-05,
      "loss": 0.0057,
      "step": 1030
    },
    {
      "epoch": 0.055466666666666664,
      "grad_norm": 0.3372681736946106,
      "learning_rate": 4.9653333333333335e-05,
      "loss": 0.0068,
      "step": 1040
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.6028397679328918,
      "learning_rate": 4.965e-05,
      "loss": 0.0048,
      "step": 1050
    },
    {
      "epoch": 0.05653333333333333,
      "grad_norm": 0.08705084770917892,
      "learning_rate": 4.964666666666667e-05,
      "loss": 0.0069,
      "step": 1060
    },
    {
      "epoch": 0.05706666666666667,
      "grad_norm": 0.6701157689094543,
      "learning_rate": 4.964333333333333e-05,
      "loss": 0.0051,
      "step": 1070
    },
    {
      "epoch": 0.0576,
      "grad_norm": 0.7418094277381897,
      "learning_rate": 4.9640000000000006e-05,
      "loss": 0.0052,
      "step": 1080
    },
    {
      "epoch": 0.058133333333333335,
      "grad_norm": 0.6710177063941956,
      "learning_rate": 4.963666666666667e-05,
      "loss": 0.0078,
      "step": 1090
    },
    {
      "epoch": 0.058666666666666666,
      "grad_norm": 0.2716754078865051,
      "learning_rate": 4.963333333333334e-05,
      "loss": 0.0055,
      "step": 1100
    },
    {
      "epoch": 0.0592,
      "grad_norm": 0.6052359342575073,
      "learning_rate": 4.9630000000000004e-05,
      "loss": 0.0057,
      "step": 1110
    },
    {
      "epoch": 0.05973333333333333,
      "grad_norm": 0.09946262836456299,
      "learning_rate": 4.962666666666667e-05,
      "loss": 0.0069,
      "step": 1120
    },
    {
      "epoch": 0.06026666666666667,
      "grad_norm": 0.2850485146045685,
      "learning_rate": 4.9623333333333337e-05,
      "loss": 0.0048,
      "step": 1130
    },
    {
      "epoch": 0.0608,
      "grad_norm": 0.2717741131782532,
      "learning_rate": 4.962e-05,
      "loss": 0.0051,
      "step": 1140
    },
    {
      "epoch": 0.06133333333333333,
      "grad_norm": 0.27276554703712463,
      "learning_rate": 4.961666666666667e-05,
      "loss": 0.0048,
      "step": 1150
    },
    {
      "epoch": 0.06186666666666667,
      "grad_norm": 0.22835306823253632,
      "learning_rate": 4.9613333333333335e-05,
      "loss": 0.0069,
      "step": 1160
    },
    {
      "epoch": 0.0624,
      "grad_norm": 0.4711141586303711,
      "learning_rate": 4.961e-05,
      "loss": 0.0061,
      "step": 1170
    },
    {
      "epoch": 0.06293333333333333,
      "grad_norm": 0.2738047242164612,
      "learning_rate": 4.960666666666667e-05,
      "loss": 0.0051,
      "step": 1180
    },
    {
      "epoch": 0.06346666666666667,
      "grad_norm": 0.47872158885002136,
      "learning_rate": 4.960333333333333e-05,
      "loss": 0.0049,
      "step": 1190
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.21131063997745514,
      "learning_rate": 4.96e-05,
      "loss": 0.0068,
      "step": 1200
    },
    {
      "epoch": 0.06453333333333333,
      "grad_norm": 0.8123726844787598,
      "learning_rate": 4.959666666666667e-05,
      "loss": 0.0041,
      "step": 1210
    },
    {
      "epoch": 0.06506666666666666,
      "grad_norm": 0.33778125047683716,
      "learning_rate": 4.959333333333334e-05,
      "loss": 0.0053,
      "step": 1220
    },
    {
      "epoch": 0.0656,
      "grad_norm": 0.7363412380218506,
      "learning_rate": 4.9590000000000005e-05,
      "loss": 0.0063,
      "step": 1230
    },
    {
      "epoch": 0.06613333333333334,
      "grad_norm": 0.6052125096321106,
      "learning_rate": 4.958666666666667e-05,
      "loss": 0.0073,
      "step": 1240
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.543308436870575,
      "learning_rate": 4.958333333333334e-05,
      "loss": 0.0048,
      "step": 1250
    },
    {
      "epoch": 0.0672,
      "grad_norm": 0.087959885597229,
      "learning_rate": 4.958e-05,
      "loss": 0.0063,
      "step": 1260
    },
    {
      "epoch": 0.06773333333333334,
      "grad_norm": 0.40228256583213806,
      "learning_rate": 4.957666666666667e-05,
      "loss": 0.0071,
      "step": 1270
    },
    {
      "epoch": 0.06826666666666667,
      "grad_norm": 0.6719772815704346,
      "learning_rate": 4.9573333333333335e-05,
      "loss": 0.0066,
      "step": 1280
    },
    {
      "epoch": 0.0688,
      "grad_norm": 0.33891499042510986,
      "learning_rate": 4.957e-05,
      "loss": 0.0079,
      "step": 1290
    },
    {
      "epoch": 0.06933333333333333,
      "grad_norm": 0.6698617339134216,
      "learning_rate": 4.956666666666667e-05,
      "loss": 0.0073,
      "step": 1300
    },
    {
      "epoch": 0.06986666666666666,
      "grad_norm": 0.7377138733863831,
      "learning_rate": 4.9563333333333334e-05,
      "loss": 0.0062,
      "step": 1310
    },
    {
      "epoch": 0.0704,
      "grad_norm": 0.8011636734008789,
      "learning_rate": 4.956e-05,
      "loss": 0.0064,
      "step": 1320
    },
    {
      "epoch": 0.07093333333333333,
      "grad_norm": 0.8061768412590027,
      "learning_rate": 4.9556666666666666e-05,
      "loss": 0.0039,
      "step": 1330
    },
    {
      "epoch": 0.07146666666666666,
      "grad_norm": 0.13461236655712128,
      "learning_rate": 4.955333333333333e-05,
      "loss": 0.0066,
      "step": 1340
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.20423443615436554,
      "learning_rate": 4.9550000000000005e-05,
      "loss": 0.0048,
      "step": 1350
    },
    {
      "epoch": 0.07253333333333334,
      "grad_norm": 0.10265576839447021,
      "learning_rate": 4.954666666666667e-05,
      "loss": 0.0053,
      "step": 1360
    },
    {
      "epoch": 0.07306666666666667,
      "grad_norm": 0.08945770561695099,
      "learning_rate": 4.954333333333334e-05,
      "loss": 0.0062,
      "step": 1370
    },
    {
      "epoch": 0.0736,
      "grad_norm": 0.27787724137306213,
      "learning_rate": 4.9540000000000003e-05,
      "loss": 0.0062,
      "step": 1380
    },
    {
      "epoch": 0.07413333333333333,
      "grad_norm": 0.2811300754547119,
      "learning_rate": 4.953666666666667e-05,
      "loss": 0.0061,
      "step": 1390
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 0.1394784152507782,
      "learning_rate": 4.9533333333333336e-05,
      "loss": 0.0062,
      "step": 1400
    },
    {
      "epoch": 0.0752,
      "grad_norm": 0.2087305188179016,
      "learning_rate": 4.953e-05,
      "loss": 0.0076,
      "step": 1410
    },
    {
      "epoch": 0.07573333333333333,
      "grad_norm": 0.6052834987640381,
      "learning_rate": 4.952666666666667e-05,
      "loss": 0.0065,
      "step": 1420
    },
    {
      "epoch": 0.07626666666666666,
      "grad_norm": 0.3358778655529022,
      "learning_rate": 4.952333333333334e-05,
      "loss": 0.0072,
      "step": 1430
    },
    {
      "epoch": 0.0768,
      "grad_norm": 0.07519077509641647,
      "learning_rate": 4.952e-05,
      "loss": 0.0067,
      "step": 1440
    },
    {
      "epoch": 0.07733333333333334,
      "grad_norm": 0.4045533239841461,
      "learning_rate": 4.9516666666666666e-05,
      "loss": 0.0064,
      "step": 1450
    },
    {
      "epoch": 0.07786666666666667,
      "grad_norm": 0.6067157983779907,
      "learning_rate": 4.951333333333333e-05,
      "loss": 0.0066,
      "step": 1460
    },
    {
      "epoch": 0.0784,
      "grad_norm": 0.14231865108013153,
      "learning_rate": 4.951e-05,
      "loss": 0.0064,
      "step": 1470
    },
    {
      "epoch": 0.07893333333333333,
      "grad_norm": 0.16332049667835236,
      "learning_rate": 4.9506666666666665e-05,
      "loss": 0.0057,
      "step": 1480
    },
    {
      "epoch": 0.07946666666666667,
      "grad_norm": 0.480396568775177,
      "learning_rate": 4.950333333333334e-05,
      "loss": 0.0062,
      "step": 1490
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.08188173174858093,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 0.004,
      "step": 1500
    },
    {
      "epoch": 0.08053333333333333,
      "grad_norm": 0.8139398097991943,
      "learning_rate": 4.949666666666667e-05,
      "loss": 0.0043,
      "step": 1510
    },
    {
      "epoch": 0.08106666666666666,
      "grad_norm": 0.40368708968162537,
      "learning_rate": 4.9493333333333336e-05,
      "loss": 0.0069,
      "step": 1520
    },
    {
      "epoch": 0.0816,
      "grad_norm": 0.08298102021217346,
      "learning_rate": 4.949e-05,
      "loss": 0.0055,
      "step": 1530
    },
    {
      "epoch": 0.08213333333333334,
      "grad_norm": 0.27515047788619995,
      "learning_rate": 4.948666666666667e-05,
      "loss": 0.0049,
      "step": 1540
    },
    {
      "epoch": 0.08266666666666667,
      "grad_norm": 0.6742215752601624,
      "learning_rate": 4.9483333333333334e-05,
      "loss": 0.0057,
      "step": 1550
    },
    {
      "epoch": 0.0832,
      "grad_norm": 0.6743526458740234,
      "learning_rate": 4.948000000000001e-05,
      "loss": 0.0062,
      "step": 1560
    },
    {
      "epoch": 0.08373333333333334,
      "grad_norm": 0.40796294808387756,
      "learning_rate": 4.9476666666666674e-05,
      "loss": 0.0058,
      "step": 1570
    },
    {
      "epoch": 0.08426666666666667,
      "grad_norm": 0.6113724708557129,
      "learning_rate": 4.947333333333334e-05,
      "loss": 0.0061,
      "step": 1580
    },
    {
      "epoch": 0.0848,
      "grad_norm": 0.948424756526947,
      "learning_rate": 4.947e-05,
      "loss": 0.0067,
      "step": 1590
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 1.007697343826294,
      "learning_rate": 4.9466666666666665e-05,
      "loss": 0.0055,
      "step": 1600
    },
    {
      "epoch": 0.08586666666666666,
      "grad_norm": 0.8113321661949158,
      "learning_rate": 4.946333333333333e-05,
      "loss": 0.0049,
      "step": 1610
    },
    {
      "epoch": 0.0864,
      "grad_norm": 0.6792780160903931,
      "learning_rate": 4.946e-05,
      "loss": 0.0073,
      "step": 1620
    },
    {
      "epoch": 0.08693333333333333,
      "grad_norm": 0.7428282499313354,
      "learning_rate": 4.945666666666667e-05,
      "loss": 0.0054,
      "step": 1630
    },
    {
      "epoch": 0.08746666666666666,
      "grad_norm": 0.27121487259864807,
      "learning_rate": 4.9453333333333336e-05,
      "loss": 0.0045,
      "step": 1640
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.805193305015564,
      "learning_rate": 4.945e-05,
      "loss": 0.0063,
      "step": 1650
    },
    {
      "epoch": 0.08853333333333334,
      "grad_norm": 0.09800968319177628,
      "learning_rate": 4.944666666666667e-05,
      "loss": 0.0054,
      "step": 1660
    },
    {
      "epoch": 0.08906666666666667,
      "grad_norm": 0.47541171312332153,
      "learning_rate": 4.9443333333333335e-05,
      "loss": 0.0051,
      "step": 1670
    },
    {
      "epoch": 0.0896,
      "grad_norm": 0.611329197883606,
      "learning_rate": 4.944e-05,
      "loss": 0.0056,
      "step": 1680
    },
    {
      "epoch": 0.09013333333333333,
      "grad_norm": 0.07974846661090851,
      "learning_rate": 4.943666666666667e-05,
      "loss": 0.0064,
      "step": 1690
    },
    {
      "epoch": 0.09066666666666667,
      "grad_norm": 0.08137771487236023,
      "learning_rate": 4.943333333333334e-05,
      "loss": 0.0045,
      "step": 1700
    },
    {
      "epoch": 0.0912,
      "grad_norm": 0.5417373180389404,
      "learning_rate": 4.9430000000000006e-05,
      "loss": 0.0052,
      "step": 1710
    },
    {
      "epoch": 0.09173333333333333,
      "grad_norm": 0.6071906685829163,
      "learning_rate": 4.942666666666667e-05,
      "loss": 0.0039,
      "step": 1720
    },
    {
      "epoch": 0.09226666666666666,
      "grad_norm": 0.4790255129337311,
      "learning_rate": 4.942333333333334e-05,
      "loss": 0.0067,
      "step": 1730
    },
    {
      "epoch": 0.0928,
      "grad_norm": 0.0751751959323883,
      "learning_rate": 4.942e-05,
      "loss": 0.0044,
      "step": 1740
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.294764906167984,
      "learning_rate": 4.9416666666666664e-05,
      "loss": 0.0051,
      "step": 1750
    },
    {
      "epoch": 0.09386666666666667,
      "grad_norm": 0.2760619521141052,
      "learning_rate": 4.941333333333334e-05,
      "loss": 0.0074,
      "step": 1760
    },
    {
      "epoch": 0.0944,
      "grad_norm": 0.339802622795105,
      "learning_rate": 4.941e-05,
      "loss": 0.0033,
      "step": 1770
    },
    {
      "epoch": 0.09493333333333333,
      "grad_norm": 0.5400802493095398,
      "learning_rate": 4.940666666666667e-05,
      "loss": 0.0062,
      "step": 1780
    },
    {
      "epoch": 0.09546666666666667,
      "grad_norm": 0.4783189594745636,
      "learning_rate": 4.9403333333333335e-05,
      "loss": 0.0057,
      "step": 1790
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.08212310820817947,
      "learning_rate": 4.94e-05,
      "loss": 0.0056,
      "step": 1800
    },
    {
      "epoch": 0.09653333333333333,
      "grad_norm": 0.4057469069957733,
      "learning_rate": 4.939666666666667e-05,
      "loss": 0.0058,
      "step": 1810
    },
    {
      "epoch": 0.09706666666666666,
      "grad_norm": 0.1319849044084549,
      "learning_rate": 4.9393333333333334e-05,
      "loss": 0.0046,
      "step": 1820
    },
    {
      "epoch": 0.0976,
      "grad_norm": 0.15199926495552063,
      "learning_rate": 4.939e-05,
      "loss": 0.0044,
      "step": 1830
    },
    {
      "epoch": 0.09813333333333334,
      "grad_norm": 0.4842655062675476,
      "learning_rate": 4.938666666666667e-05,
      "loss": 0.0051,
      "step": 1840
    },
    {
      "epoch": 0.09866666666666667,
      "grad_norm": 0.8126041889190674,
      "learning_rate": 4.938333333333334e-05,
      "loss": 0.0057,
      "step": 1850
    },
    {
      "epoch": 0.0992,
      "grad_norm": 0.09071362018585205,
      "learning_rate": 4.9380000000000005e-05,
      "loss": 0.0058,
      "step": 1860
    },
    {
      "epoch": 0.09973333333333333,
      "grad_norm": 0.6138917803764343,
      "learning_rate": 4.937666666666667e-05,
      "loss": 0.0053,
      "step": 1870
    },
    {
      "epoch": 0.10026666666666667,
      "grad_norm": 0.6325111389160156,
      "learning_rate": 4.937333333333334e-05,
      "loss": 0.0036,
      "step": 1880
    },
    {
      "epoch": 0.1008,
      "grad_norm": 0.4781475365161896,
      "learning_rate": 4.937e-05,
      "loss": 0.0049,
      "step": 1890
    },
    {
      "epoch": 0.10133333333333333,
      "grad_norm": 0.183474600315094,
      "learning_rate": 4.936666666666667e-05,
      "loss": 0.0068,
      "step": 1900
    },
    {
      "epoch": 0.10186666666666666,
      "grad_norm": 0.541714072227478,
      "learning_rate": 4.9363333333333336e-05,
      "loss": 0.0046,
      "step": 1910
    },
    {
      "epoch": 0.1024,
      "grad_norm": 0.6572766900062561,
      "learning_rate": 4.936e-05,
      "loss": 0.0066,
      "step": 1920
    },
    {
      "epoch": 0.10293333333333334,
      "grad_norm": 0.7507768273353577,
      "learning_rate": 4.935666666666667e-05,
      "loss": 0.0068,
      "step": 1930
    },
    {
      "epoch": 0.10346666666666667,
      "grad_norm": 0.7135235071182251,
      "learning_rate": 4.9353333333333334e-05,
      "loss": 0.0055,
      "step": 1940
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.21629761159420013,
      "learning_rate": 4.935e-05,
      "loss": 0.0038,
      "step": 1950
    },
    {
      "epoch": 0.10453333333333334,
      "grad_norm": 0.15048803389072418,
      "learning_rate": 4.9346666666666666e-05,
      "loss": 0.0055,
      "step": 1960
    },
    {
      "epoch": 0.10506666666666667,
      "grad_norm": 0.5911638736724854,
      "learning_rate": 4.934333333333334e-05,
      "loss": 0.0052,
      "step": 1970
    },
    {
      "epoch": 0.1056,
      "grad_norm": 0.41177132725715637,
      "learning_rate": 4.9340000000000005e-05,
      "loss": 0.0055,
      "step": 1980
    },
    {
      "epoch": 0.10613333333333333,
      "grad_norm": 0.2727329134941101,
      "learning_rate": 4.933666666666667e-05,
      "loss": 0.0054,
      "step": 1990
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.8870088458061218,
      "learning_rate": 4.933333333333334e-05,
      "loss": 0.0066,
      "step": 2000
    },
    {
      "epoch": 0.1072,
      "grad_norm": 0.33941948413848877,
      "learning_rate": 4.9330000000000004e-05,
      "loss": 0.0034,
      "step": 2010
    },
    {
      "epoch": 0.10773333333333333,
      "grad_norm": 0.40950140357017517,
      "learning_rate": 4.932666666666667e-05,
      "loss": 0.0051,
      "step": 2020
    },
    {
      "epoch": 0.10826666666666666,
      "grad_norm": 0.884047269821167,
      "learning_rate": 4.9323333333333336e-05,
      "loss": 0.0052,
      "step": 2030
    },
    {
      "epoch": 0.1088,
      "grad_norm": 0.6497365236282349,
      "learning_rate": 4.932e-05,
      "loss": 0.006,
      "step": 2040
    },
    {
      "epoch": 0.10933333333333334,
      "grad_norm": 0.7260444760322571,
      "learning_rate": 4.931666666666667e-05,
      "loss": 0.0051,
      "step": 2050
    },
    {
      "epoch": 0.10986666666666667,
      "grad_norm": 0.8943228125572205,
      "learning_rate": 4.9313333333333334e-05,
      "loss": 0.0043,
      "step": 2060
    },
    {
      "epoch": 0.1104,
      "grad_norm": 1.063677191734314,
      "learning_rate": 4.931e-05,
      "loss": 0.0062,
      "step": 2070
    },
    {
      "epoch": 0.11093333333333333,
      "grad_norm": 1.1594359874725342,
      "learning_rate": 4.930666666666667e-05,
      "loss": 0.004,
      "step": 2080
    },
    {
      "epoch": 0.11146666666666667,
      "grad_norm": 0.259613573551178,
      "learning_rate": 4.930333333333333e-05,
      "loss": 0.005,
      "step": 2090
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.14259761571884155,
      "learning_rate": 4.93e-05,
      "loss": 0.0052,
      "step": 2100
    },
    {
      "epoch": 0.11253333333333333,
      "grad_norm": 0.3400188386440277,
      "learning_rate": 4.929666666666667e-05,
      "loss": 0.0048,
      "step": 2110
    },
    {
      "epoch": 0.11306666666666666,
      "grad_norm": 0.3156033754348755,
      "learning_rate": 4.929333333333334e-05,
      "loss": 0.0058,
      "step": 2120
    },
    {
      "epoch": 0.1136,
      "grad_norm": 0.7722402811050415,
      "learning_rate": 4.9290000000000004e-05,
      "loss": 0.0055,
      "step": 2130
    },
    {
      "epoch": 0.11413333333333334,
      "grad_norm": 0.47764039039611816,
      "learning_rate": 4.928666666666667e-05,
      "loss": 0.0058,
      "step": 2140
    },
    {
      "epoch": 0.11466666666666667,
      "grad_norm": 0.8264932036399841,
      "learning_rate": 4.9283333333333336e-05,
      "loss": 0.0051,
      "step": 2150
    },
    {
      "epoch": 0.1152,
      "grad_norm": 0.9021633267402649,
      "learning_rate": 4.928e-05,
      "loss": 0.0069,
      "step": 2160
    },
    {
      "epoch": 0.11573333333333333,
      "grad_norm": 0.34699317812919617,
      "learning_rate": 4.927666666666667e-05,
      "loss": 0.0063,
      "step": 2170
    },
    {
      "epoch": 0.11626666666666667,
      "grad_norm": 0.756406307220459,
      "learning_rate": 4.9273333333333335e-05,
      "loss": 0.0035,
      "step": 2180
    },
    {
      "epoch": 0.1168,
      "grad_norm": 0.11055672913789749,
      "learning_rate": 4.927000000000001e-05,
      "loss": 0.0047,
      "step": 2190
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 0.4815675616264343,
      "learning_rate": 4.926666666666667e-05,
      "loss": 0.0041,
      "step": 2200
    },
    {
      "epoch": 0.11786666666666666,
      "grad_norm": 0.7749120593070984,
      "learning_rate": 4.926333333333333e-05,
      "loss": 0.0059,
      "step": 2210
    },
    {
      "epoch": 0.1184,
      "grad_norm": 1.0666760206222534,
      "learning_rate": 4.926e-05,
      "loss": 0.0055,
      "step": 2220
    },
    {
      "epoch": 0.11893333333333334,
      "grad_norm": 0.35588306188583374,
      "learning_rate": 4.9256666666666665e-05,
      "loss": 0.0058,
      "step": 2230
    },
    {
      "epoch": 0.11946666666666667,
      "grad_norm": 1.171299695968628,
      "learning_rate": 4.925333333333333e-05,
      "loss": 0.0058,
      "step": 2240
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.8972267508506775,
      "learning_rate": 4.9250000000000004e-05,
      "loss": 0.0044,
      "step": 2250
    },
    {
      "epoch": 0.12053333333333334,
      "grad_norm": 0.8179242014884949,
      "learning_rate": 4.924666666666667e-05,
      "loss": 0.0051,
      "step": 2260
    },
    {
      "epoch": 0.12106666666666667,
      "grad_norm": 0.9597476720809937,
      "learning_rate": 4.924333333333334e-05,
      "loss": 0.0046,
      "step": 2270
    },
    {
      "epoch": 0.1216,
      "grad_norm": 0.964652955532074,
      "learning_rate": 4.924e-05,
      "loss": 0.0036,
      "step": 2280
    },
    {
      "epoch": 0.12213333333333333,
      "grad_norm": 0.9624130129814148,
      "learning_rate": 4.923666666666667e-05,
      "loss": 0.0059,
      "step": 2290
    },
    {
      "epoch": 0.12266666666666666,
      "grad_norm": 0.11406456679105759,
      "learning_rate": 4.9233333333333335e-05,
      "loss": 0.0048,
      "step": 2300
    },
    {
      "epoch": 0.1232,
      "grad_norm": 0.3448071777820587,
      "learning_rate": 4.923e-05,
      "loss": 0.0046,
      "step": 2310
    },
    {
      "epoch": 0.12373333333333333,
      "grad_norm": 0.7226150631904602,
      "learning_rate": 4.9226666666666674e-05,
      "loss": 0.0049,
      "step": 2320
    },
    {
      "epoch": 0.12426666666666666,
      "grad_norm": 0.7863613963127136,
      "learning_rate": 4.922333333333334e-05,
      "loss": 0.0047,
      "step": 2330
    },
    {
      "epoch": 0.1248,
      "grad_norm": 0.2880108952522278,
      "learning_rate": 4.9220000000000006e-05,
      "loss": 0.0054,
      "step": 2340
    },
    {
      "epoch": 0.12533333333333332,
      "grad_norm": 0.289527952671051,
      "learning_rate": 4.9216666666666666e-05,
      "loss": 0.0046,
      "step": 2350
    },
    {
      "epoch": 0.12586666666666665,
      "grad_norm": 1.0266001224517822,
      "learning_rate": 4.921333333333333e-05,
      "loss": 0.0043,
      "step": 2360
    },
    {
      "epoch": 0.1264,
      "grad_norm": 0.19428694248199463,
      "learning_rate": 4.921e-05,
      "loss": 0.0044,
      "step": 2370
    },
    {
      "epoch": 0.12693333333333334,
      "grad_norm": 0.7909882068634033,
      "learning_rate": 4.9206666666666664e-05,
      "loss": 0.0054,
      "step": 2380
    },
    {
      "epoch": 0.12746666666666667,
      "grad_norm": 0.0954485833644867,
      "learning_rate": 4.920333333333334e-05,
      "loss": 0.0041,
      "step": 2390
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.25999927520751953,
      "learning_rate": 4.92e-05,
      "loss": 0.0035,
      "step": 2400
    },
    {
      "epoch": 0.12853333333333333,
      "grad_norm": 0.3190436065196991,
      "learning_rate": 4.919666666666667e-05,
      "loss": 0.005,
      "step": 2410
    },
    {
      "epoch": 0.12906666666666666,
      "grad_norm": 1.0345048904418945,
      "learning_rate": 4.9193333333333336e-05,
      "loss": 0.0038,
      "step": 2420
    },
    {
      "epoch": 0.1296,
      "grad_norm": 0.26667749881744385,
      "learning_rate": 4.919e-05,
      "loss": 0.0047,
      "step": 2430
    },
    {
      "epoch": 0.13013333333333332,
      "grad_norm": 0.4000234305858612,
      "learning_rate": 4.918666666666667e-05,
      "loss": 0.0056,
      "step": 2440
    },
    {
      "epoch": 0.13066666666666665,
      "grad_norm": 0.5496715903282166,
      "learning_rate": 4.9183333333333334e-05,
      "loss": 0.0031,
      "step": 2450
    },
    {
      "epoch": 0.1312,
      "grad_norm": 0.36408165097236633,
      "learning_rate": 4.918000000000001e-05,
      "loss": 0.005,
      "step": 2460
    },
    {
      "epoch": 0.13173333333333334,
      "grad_norm": 0.9728846549987793,
      "learning_rate": 4.917666666666667e-05,
      "loss": 0.0052,
      "step": 2470
    },
    {
      "epoch": 0.13226666666666667,
      "grad_norm": 0.7922030091285706,
      "learning_rate": 4.917333333333334e-05,
      "loss": 0.0051,
      "step": 2480
    },
    {
      "epoch": 0.1328,
      "grad_norm": 0.10079832375049591,
      "learning_rate": 4.9170000000000005e-05,
      "loss": 0.0042,
      "step": 2490
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.08172625303268433,
      "learning_rate": 4.9166666666666665e-05,
      "loss": 0.0046,
      "step": 2500
    },
    {
      "epoch": 0.13386666666666666,
      "grad_norm": 0.3154939115047455,
      "learning_rate": 4.916333333333333e-05,
      "loss": 0.0057,
      "step": 2510
    },
    {
      "epoch": 0.1344,
      "grad_norm": 0.08870277553796768,
      "learning_rate": 4.9160000000000004e-05,
      "loss": 0.0051,
      "step": 2520
    },
    {
      "epoch": 0.13493333333333332,
      "grad_norm": 0.17644187808036804,
      "learning_rate": 4.915666666666667e-05,
      "loss": 0.0076,
      "step": 2530
    },
    {
      "epoch": 0.13546666666666668,
      "grad_norm": 0.11070982366800308,
      "learning_rate": 4.9153333333333336e-05,
      "loss": 0.0039,
      "step": 2540
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.7785358428955078,
      "learning_rate": 4.915e-05,
      "loss": 0.0042,
      "step": 2550
    },
    {
      "epoch": 0.13653333333333334,
      "grad_norm": 0.7334301471710205,
      "learning_rate": 4.914666666666667e-05,
      "loss": 0.0039,
      "step": 2560
    },
    {
      "epoch": 0.13706666666666667,
      "grad_norm": 0.34586459398269653,
      "learning_rate": 4.9143333333333334e-05,
      "loss": 0.0048,
      "step": 2570
    },
    {
      "epoch": 0.1376,
      "grad_norm": 0.38697803020477295,
      "learning_rate": 4.914e-05,
      "loss": 0.006,
      "step": 2580
    },
    {
      "epoch": 0.13813333333333333,
      "grad_norm": 0.1846243292093277,
      "learning_rate": 4.9136666666666667e-05,
      "loss": 0.0039,
      "step": 2590
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 0.08152909576892853,
      "learning_rate": 4.913333333333334e-05,
      "loss": 0.005,
      "step": 2600
    },
    {
      "epoch": 0.1392,
      "grad_norm": 0.9579745531082153,
      "learning_rate": 4.9130000000000006e-05,
      "loss": 0.0045,
      "step": 2610
    },
    {
      "epoch": 0.13973333333333332,
      "grad_norm": 0.6919130682945251,
      "learning_rate": 4.912666666666667e-05,
      "loss": 0.0045,
      "step": 2620
    },
    {
      "epoch": 0.14026666666666668,
      "grad_norm": 0.8243271112442017,
      "learning_rate": 4.912333333333334e-05,
      "loss": 0.0055,
      "step": 2630
    },
    {
      "epoch": 0.1408,
      "grad_norm": 0.6698204874992371,
      "learning_rate": 4.9120000000000004e-05,
      "loss": 0.005,
      "step": 2640
    },
    {
      "epoch": 0.14133333333333334,
      "grad_norm": 0.34314003586769104,
      "learning_rate": 4.9116666666666663e-05,
      "loss": 0.0057,
      "step": 2650
    },
    {
      "epoch": 0.14186666666666667,
      "grad_norm": 0.4487423598766327,
      "learning_rate": 4.9113333333333336e-05,
      "loss": 0.0049,
      "step": 2660
    },
    {
      "epoch": 0.1424,
      "grad_norm": 0.45791998505592346,
      "learning_rate": 4.911e-05,
      "loss": 0.0058,
      "step": 2670
    },
    {
      "epoch": 0.14293333333333333,
      "grad_norm": 0.09850160032510757,
      "learning_rate": 4.910666666666667e-05,
      "loss": 0.0041,
      "step": 2680
    },
    {
      "epoch": 0.14346666666666666,
      "grad_norm": 0.6239685416221619,
      "learning_rate": 4.9103333333333335e-05,
      "loss": 0.0042,
      "step": 2690
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.3457573354244232,
      "learning_rate": 4.91e-05,
      "loss": 0.0043,
      "step": 2700
    },
    {
      "epoch": 0.14453333333333335,
      "grad_norm": 1.0029598474502563,
      "learning_rate": 4.909666666666667e-05,
      "loss": 0.0061,
      "step": 2710
    },
    {
      "epoch": 0.14506666666666668,
      "grad_norm": 1.070399522781372,
      "learning_rate": 4.909333333333333e-05,
      "loss": 0.0043,
      "step": 2720
    },
    {
      "epoch": 0.1456,
      "grad_norm": 0.6977572441101074,
      "learning_rate": 4.9090000000000006e-05,
      "loss": 0.0058,
      "step": 2730
    },
    {
      "epoch": 0.14613333333333334,
      "grad_norm": 0.3124645948410034,
      "learning_rate": 4.908666666666667e-05,
      "loss": 0.005,
      "step": 2740
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.48474404215812683,
      "learning_rate": 4.908333333333334e-05,
      "loss": 0.0046,
      "step": 2750
    },
    {
      "epoch": 0.1472,
      "grad_norm": 1.1064627170562744,
      "learning_rate": 4.9080000000000004e-05,
      "loss": 0.0057,
      "step": 2760
    },
    {
      "epoch": 0.14773333333333333,
      "grad_norm": 0.7534384727478027,
      "learning_rate": 4.907666666666667e-05,
      "loss": 0.0043,
      "step": 2770
    },
    {
      "epoch": 0.14826666666666666,
      "grad_norm": 0.8886580467224121,
      "learning_rate": 4.907333333333334e-05,
      "loss": 0.0048,
      "step": 2780
    },
    {
      "epoch": 0.1488,
      "grad_norm": 0.4545586109161377,
      "learning_rate": 4.907e-05,
      "loss": 0.0036,
      "step": 2790
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 0.6637972593307495,
      "learning_rate": 4.906666666666667e-05,
      "loss": 0.005,
      "step": 2800
    },
    {
      "epoch": 0.14986666666666668,
      "grad_norm": 0.24602845311164856,
      "learning_rate": 4.9063333333333335e-05,
      "loss": 0.0059,
      "step": 2810
    },
    {
      "epoch": 0.1504,
      "grad_norm": 0.21230201423168182,
      "learning_rate": 4.906e-05,
      "loss": 0.004,
      "step": 2820
    },
    {
      "epoch": 0.15093333333333334,
      "grad_norm": 0.3565533757209778,
      "learning_rate": 4.905666666666667e-05,
      "loss": 0.0047,
      "step": 2830
    },
    {
      "epoch": 0.15146666666666667,
      "grad_norm": 1.1373361349105835,
      "learning_rate": 4.9053333333333333e-05,
      "loss": 0.0045,
      "step": 2840
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.34418657422065735,
      "learning_rate": 4.905e-05,
      "loss": 0.0061,
      "step": 2850
    },
    {
      "epoch": 0.15253333333333333,
      "grad_norm": 0.4802513122558594,
      "learning_rate": 4.9046666666666666e-05,
      "loss": 0.0063,
      "step": 2860
    },
    {
      "epoch": 0.15306666666666666,
      "grad_norm": 0.23131708800792694,
      "learning_rate": 4.904333333333334e-05,
      "loss": 0.0031,
      "step": 2870
    },
    {
      "epoch": 0.1536,
      "grad_norm": 0.6256725788116455,
      "learning_rate": 4.9040000000000005e-05,
      "loss": 0.006,
      "step": 2880
    },
    {
      "epoch": 0.15413333333333334,
      "grad_norm": 0.6499080657958984,
      "learning_rate": 4.903666666666667e-05,
      "loss": 0.005,
      "step": 2890
    },
    {
      "epoch": 0.15466666666666667,
      "grad_norm": 0.34286630153656006,
      "learning_rate": 4.903333333333334e-05,
      "loss": 0.0044,
      "step": 2900
    },
    {
      "epoch": 0.1552,
      "grad_norm": 0.08817964792251587,
      "learning_rate": 4.903e-05,
      "loss": 0.0044,
      "step": 2910
    },
    {
      "epoch": 0.15573333333333333,
      "grad_norm": 0.8291511535644531,
      "learning_rate": 4.902666666666667e-05,
      "loss": 0.0046,
      "step": 2920
    },
    {
      "epoch": 0.15626666666666666,
      "grad_norm": 0.699780285358429,
      "learning_rate": 4.9023333333333335e-05,
      "loss": 0.0035,
      "step": 2930
    },
    {
      "epoch": 0.1568,
      "grad_norm": 0.283041387796402,
      "learning_rate": 4.902e-05,
      "loss": 0.0039,
      "step": 2940
    },
    {
      "epoch": 0.15733333333333333,
      "grad_norm": 0.09735222160816193,
      "learning_rate": 4.901666666666667e-05,
      "loss": 0.0057,
      "step": 2950
    },
    {
      "epoch": 0.15786666666666666,
      "grad_norm": 0.17428132891654968,
      "learning_rate": 4.9013333333333334e-05,
      "loss": 0.0057,
      "step": 2960
    },
    {
      "epoch": 0.1584,
      "grad_norm": 0.4430614113807678,
      "learning_rate": 4.901e-05,
      "loss": 0.0039,
      "step": 2970
    },
    {
      "epoch": 0.15893333333333334,
      "grad_norm": 0.43384239077568054,
      "learning_rate": 4.9006666666666666e-05,
      "loss": 0.003,
      "step": 2980
    },
    {
      "epoch": 0.15946666666666667,
      "grad_norm": 0.3157234787940979,
      "learning_rate": 4.900333333333333e-05,
      "loss": 0.0039,
      "step": 2990
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.5168129205703735,
      "learning_rate": 4.9e-05,
      "loss": 0.0046,
      "step": 3000
    },
    {
      "epoch": 0.16053333333333333,
      "grad_norm": 0.40682125091552734,
      "learning_rate": 4.899666666666667e-05,
      "loss": 0.0033,
      "step": 3010
    },
    {
      "epoch": 0.16106666666666666,
      "grad_norm": 0.4846067428588867,
      "learning_rate": 4.899333333333334e-05,
      "loss": 0.0047,
      "step": 3020
    },
    {
      "epoch": 0.1616,
      "grad_norm": 0.548721194267273,
      "learning_rate": 4.8990000000000004e-05,
      "loss": 0.0043,
      "step": 3030
    },
    {
      "epoch": 0.16213333333333332,
      "grad_norm": 0.36623701453208923,
      "learning_rate": 4.898666666666667e-05,
      "loss": 0.0055,
      "step": 3040
    },
    {
      "epoch": 0.16266666666666665,
      "grad_norm": 0.82411128282547,
      "learning_rate": 4.8983333333333336e-05,
      "loss": 0.0053,
      "step": 3050
    },
    {
      "epoch": 0.1632,
      "grad_norm": 1.0695441961288452,
      "learning_rate": 4.898e-05,
      "loss": 0.0049,
      "step": 3060
    },
    {
      "epoch": 0.16373333333333334,
      "grad_norm": 1.0207210779190063,
      "learning_rate": 4.897666666666667e-05,
      "loss": 0.0046,
      "step": 3070
    },
    {
      "epoch": 0.16426666666666667,
      "grad_norm": 0.5447377562522888,
      "learning_rate": 4.897333333333334e-05,
      "loss": 0.0045,
      "step": 3080
    },
    {
      "epoch": 0.1648,
      "grad_norm": 0.19323857128620148,
      "learning_rate": 4.897000000000001e-05,
      "loss": 0.0041,
      "step": 3090
    },
    {
      "epoch": 0.16533333333333333,
      "grad_norm": 0.21101504564285278,
      "learning_rate": 4.8966666666666667e-05,
      "loss": 0.0057,
      "step": 3100
    },
    {
      "epoch": 0.16586666666666666,
      "grad_norm": 0.9322696924209595,
      "learning_rate": 4.896333333333333e-05,
      "loss": 0.006,
      "step": 3110
    },
    {
      "epoch": 0.1664,
      "grad_norm": 0.8301446437835693,
      "learning_rate": 4.896e-05,
      "loss": 0.0055,
      "step": 3120
    },
    {
      "epoch": 0.16693333333333332,
      "grad_norm": 0.4650363326072693,
      "learning_rate": 4.8956666666666665e-05,
      "loss": 0.0046,
      "step": 3130
    },
    {
      "epoch": 0.16746666666666668,
      "grad_norm": 0.8926517963409424,
      "learning_rate": 4.895333333333333e-05,
      "loss": 0.0043,
      "step": 3140
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.38660597801208496,
      "learning_rate": 4.8950000000000004e-05,
      "loss": 0.0056,
      "step": 3150
    },
    {
      "epoch": 0.16853333333333334,
      "grad_norm": 0.11457764357328415,
      "learning_rate": 4.894666666666667e-05,
      "loss": 0.0042,
      "step": 3160
    },
    {
      "epoch": 0.16906666666666667,
      "grad_norm": 0.6638051867485046,
      "learning_rate": 4.8943333333333336e-05,
      "loss": 0.0044,
      "step": 3170
    },
    {
      "epoch": 0.1696,
      "grad_norm": 0.08828578144311905,
      "learning_rate": 4.894e-05,
      "loss": 0.004,
      "step": 3180
    },
    {
      "epoch": 0.17013333333333333,
      "grad_norm": 0.10005605965852737,
      "learning_rate": 4.893666666666667e-05,
      "loss": 0.0044,
      "step": 3190
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 1.039574146270752,
      "learning_rate": 4.8933333333333335e-05,
      "loss": 0.0046,
      "step": 3200
    },
    {
      "epoch": 0.1712,
      "grad_norm": 0.14636534452438354,
      "learning_rate": 4.893e-05,
      "loss": 0.0045,
      "step": 3210
    },
    {
      "epoch": 0.17173333333333332,
      "grad_norm": 0.1931355595588684,
      "learning_rate": 4.8926666666666674e-05,
      "loss": 0.005,
      "step": 3220
    },
    {
      "epoch": 0.17226666666666668,
      "grad_norm": 0.588801383972168,
      "learning_rate": 4.892333333333334e-05,
      "loss": 0.0037,
      "step": 3230
    },
    {
      "epoch": 0.1728,
      "grad_norm": 0.181003600358963,
      "learning_rate": 4.8920000000000006e-05,
      "loss": 0.004,
      "step": 3240
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.2094963937997818,
      "learning_rate": 4.891666666666667e-05,
      "loss": 0.0039,
      "step": 3250
    },
    {
      "epoch": 0.17386666666666667,
      "grad_norm": 0.4594152867794037,
      "learning_rate": 4.891333333333333e-05,
      "loss": 0.0043,
      "step": 3260
    },
    {
      "epoch": 0.1744,
      "grad_norm": 0.49388131499290466,
      "learning_rate": 4.891e-05,
      "loss": 0.0053,
      "step": 3270
    },
    {
      "epoch": 0.17493333333333333,
      "grad_norm": 0.17716015875339508,
      "learning_rate": 4.890666666666667e-05,
      "loss": 0.0034,
      "step": 3280
    },
    {
      "epoch": 0.17546666666666666,
      "grad_norm": 0.2660449743270874,
      "learning_rate": 4.890333333333334e-05,
      "loss": 0.0053,
      "step": 3290
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.2798554599285126,
      "learning_rate": 4.89e-05,
      "loss": 0.0027,
      "step": 3300
    },
    {
      "epoch": 0.17653333333333332,
      "grad_norm": 0.6887646913528442,
      "learning_rate": 4.889666666666667e-05,
      "loss": 0.0037,
      "step": 3310
    },
    {
      "epoch": 0.17706666666666668,
      "grad_norm": 0.6524987816810608,
      "learning_rate": 4.8893333333333335e-05,
      "loss": 0.0036,
      "step": 3320
    },
    {
      "epoch": 0.1776,
      "grad_norm": 0.4550289809703827,
      "learning_rate": 4.889e-05,
      "loss": 0.0046,
      "step": 3330
    },
    {
      "epoch": 0.17813333333333334,
      "grad_norm": 0.18121089041233063,
      "learning_rate": 4.888666666666667e-05,
      "loss": 0.0046,
      "step": 3340
    },
    {
      "epoch": 0.17866666666666667,
      "grad_norm": 0.3563600182533264,
      "learning_rate": 4.8883333333333333e-05,
      "loss": 0.005,
      "step": 3350
    },
    {
      "epoch": 0.1792,
      "grad_norm": 0.20599615573883057,
      "learning_rate": 4.8880000000000006e-05,
      "loss": 0.0047,
      "step": 3360
    },
    {
      "epoch": 0.17973333333333333,
      "grad_norm": 0.2519637644290924,
      "learning_rate": 4.887666666666667e-05,
      "loss": 0.0037,
      "step": 3370
    },
    {
      "epoch": 0.18026666666666666,
      "grad_norm": 0.256262868642807,
      "learning_rate": 4.887333333333334e-05,
      "loss": 0.0034,
      "step": 3380
    },
    {
      "epoch": 0.1808,
      "grad_norm": 0.04014057293534279,
      "learning_rate": 4.8870000000000005e-05,
      "loss": 0.0038,
      "step": 3390
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 0.11745784431695938,
      "learning_rate": 4.886666666666667e-05,
      "loss": 0.003,
      "step": 3400
    },
    {
      "epoch": 0.18186666666666668,
      "grad_norm": 0.43703117966651917,
      "learning_rate": 4.886333333333333e-05,
      "loss": 0.0064,
      "step": 3410
    },
    {
      "epoch": 0.1824,
      "grad_norm": 0.5635658502578735,
      "learning_rate": 4.886e-05,
      "loss": 0.0051,
      "step": 3420
    },
    {
      "epoch": 0.18293333333333334,
      "grad_norm": 0.43544232845306396,
      "learning_rate": 4.885666666666667e-05,
      "loss": 0.0044,
      "step": 3430
    },
    {
      "epoch": 0.18346666666666667,
      "grad_norm": 0.5205020904541016,
      "learning_rate": 4.8853333333333335e-05,
      "loss": 0.0037,
      "step": 3440
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.3119141161441803,
      "learning_rate": 4.885e-05,
      "loss": 0.0044,
      "step": 3450
    },
    {
      "epoch": 0.18453333333333333,
      "grad_norm": 0.8887044787406921,
      "learning_rate": 4.884666666666667e-05,
      "loss": 0.004,
      "step": 3460
    },
    {
      "epoch": 0.18506666666666666,
      "grad_norm": 0.146171435713768,
      "learning_rate": 4.8843333333333334e-05,
      "loss": 0.004,
      "step": 3470
    },
    {
      "epoch": 0.1856,
      "grad_norm": 0.6857966184616089,
      "learning_rate": 4.884e-05,
      "loss": 0.0044,
      "step": 3480
    },
    {
      "epoch": 0.18613333333333335,
      "grad_norm": 0.21436814963817596,
      "learning_rate": 4.8836666666666666e-05,
      "loss": 0.0052,
      "step": 3490
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.21602974832057953,
      "learning_rate": 4.883333333333334e-05,
      "loss": 0.0032,
      "step": 3500
    },
    {
      "epoch": 0.1872,
      "grad_norm": 0.341647744178772,
      "learning_rate": 4.8830000000000005e-05,
      "loss": 0.0035,
      "step": 3510
    },
    {
      "epoch": 0.18773333333333334,
      "grad_norm": 0.6855489015579224,
      "learning_rate": 4.882666666666667e-05,
      "loss": 0.0045,
      "step": 3520
    },
    {
      "epoch": 0.18826666666666667,
      "grad_norm": 0.48792850971221924,
      "learning_rate": 4.882333333333334e-05,
      "loss": 0.0041,
      "step": 3530
    },
    {
      "epoch": 0.1888,
      "grad_norm": 0.18468090891838074,
      "learning_rate": 4.8820000000000004e-05,
      "loss": 0.0036,
      "step": 3540
    },
    {
      "epoch": 0.18933333333333333,
      "grad_norm": 0.1026604026556015,
      "learning_rate": 4.881666666666667e-05,
      "loss": 0.0049,
      "step": 3550
    },
    {
      "epoch": 0.18986666666666666,
      "grad_norm": 0.08739206194877625,
      "learning_rate": 4.8813333333333336e-05,
      "loss": 0.0035,
      "step": 3560
    },
    {
      "epoch": 0.1904,
      "grad_norm": 0.6304397583007812,
      "learning_rate": 4.881e-05,
      "loss": 0.0029,
      "step": 3570
    },
    {
      "epoch": 0.19093333333333334,
      "grad_norm": 0.14062708616256714,
      "learning_rate": 4.880666666666667e-05,
      "loss": 0.0032,
      "step": 3580
    },
    {
      "epoch": 0.19146666666666667,
      "grad_norm": 0.2244725227355957,
      "learning_rate": 4.8803333333333334e-05,
      "loss": 0.0038,
      "step": 3590
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.23176735639572144,
      "learning_rate": 4.88e-05,
      "loss": 0.0047,
      "step": 3600
    },
    {
      "epoch": 0.19253333333333333,
      "grad_norm": 0.5632209777832031,
      "learning_rate": 4.8796666666666666e-05,
      "loss": 0.004,
      "step": 3610
    },
    {
      "epoch": 0.19306666666666666,
      "grad_norm": 0.22227448225021362,
      "learning_rate": 4.879333333333333e-05,
      "loss": 0.0033,
      "step": 3620
    },
    {
      "epoch": 0.1936,
      "grad_norm": 0.6237385869026184,
      "learning_rate": 4.8790000000000006e-05,
      "loss": 0.0036,
      "step": 3630
    },
    {
      "epoch": 0.19413333333333332,
      "grad_norm": 0.12063624709844589,
      "learning_rate": 4.878666666666667e-05,
      "loss": 0.0048,
      "step": 3640
    },
    {
      "epoch": 0.19466666666666665,
      "grad_norm": 0.14261098206043243,
      "learning_rate": 4.878333333333334e-05,
      "loss": 0.0038,
      "step": 3650
    },
    {
      "epoch": 0.1952,
      "grad_norm": 0.7578006982803345,
      "learning_rate": 4.8780000000000004e-05,
      "loss": 0.0039,
      "step": 3660
    },
    {
      "epoch": 0.19573333333333334,
      "grad_norm": 0.6649764180183411,
      "learning_rate": 4.877666666666667e-05,
      "loss": 0.0044,
      "step": 3670
    },
    {
      "epoch": 0.19626666666666667,
      "grad_norm": 0.14340586960315704,
      "learning_rate": 4.8773333333333336e-05,
      "loss": 0.0036,
      "step": 3680
    },
    {
      "epoch": 0.1968,
      "grad_norm": 0.2302430272102356,
      "learning_rate": 4.877e-05,
      "loss": 0.0055,
      "step": 3690
    },
    {
      "epoch": 0.19733333333333333,
      "grad_norm": 1.0262492895126343,
      "learning_rate": 4.876666666666667e-05,
      "loss": 0.0044,
      "step": 3700
    },
    {
      "epoch": 0.19786666666666666,
      "grad_norm": 1.022761583328247,
      "learning_rate": 4.8763333333333335e-05,
      "loss": 0.0041,
      "step": 3710
    },
    {
      "epoch": 0.1984,
      "grad_norm": 0.6113282442092896,
      "learning_rate": 4.876e-05,
      "loss": 0.0048,
      "step": 3720
    },
    {
      "epoch": 0.19893333333333332,
      "grad_norm": 0.8593100905418396,
      "learning_rate": 4.875666666666667e-05,
      "loss": 0.0042,
      "step": 3730
    },
    {
      "epoch": 0.19946666666666665,
      "grad_norm": 0.13845717906951904,
      "learning_rate": 4.875333333333333e-05,
      "loss": 0.0043,
      "step": 3740
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.17652492225170135,
      "learning_rate": 4.875e-05,
      "loss": 0.0044,
      "step": 3750
    },
    {
      "epoch": 0.20053333333333334,
      "grad_norm": 0.11784832924604416,
      "learning_rate": 4.8746666666666665e-05,
      "loss": 0.0037,
      "step": 3760
    },
    {
      "epoch": 0.20106666666666667,
      "grad_norm": 0.44936084747314453,
      "learning_rate": 4.874333333333334e-05,
      "loss": 0.0062,
      "step": 3770
    },
    {
      "epoch": 0.2016,
      "grad_norm": 0.2211054414510727,
      "learning_rate": 4.8740000000000004e-05,
      "loss": 0.0042,
      "step": 3780
    },
    {
      "epoch": 0.20213333333333333,
      "grad_norm": 0.42489704489707947,
      "learning_rate": 4.873666666666667e-05,
      "loss": 0.0043,
      "step": 3790
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 0.19974109530448914,
      "learning_rate": 4.8733333333333337e-05,
      "loss": 0.0047,
      "step": 3800
    },
    {
      "epoch": 0.2032,
      "grad_norm": 0.906423807144165,
      "learning_rate": 4.873e-05,
      "loss": 0.0058,
      "step": 3810
    },
    {
      "epoch": 0.20373333333333332,
      "grad_norm": 0.8603700995445251,
      "learning_rate": 4.872666666666667e-05,
      "loss": 0.0052,
      "step": 3820
    },
    {
      "epoch": 0.20426666666666668,
      "grad_norm": 0.6884249448776245,
      "learning_rate": 4.8723333333333335e-05,
      "loss": 0.0036,
      "step": 3830
    },
    {
      "epoch": 0.2048,
      "grad_norm": 0.3796045780181885,
      "learning_rate": 4.872000000000001e-05,
      "loss": 0.0038,
      "step": 3840
    },
    {
      "epoch": 0.20533333333333334,
      "grad_norm": 0.49045509099960327,
      "learning_rate": 4.8716666666666674e-05,
      "loss": 0.0042,
      "step": 3850
    },
    {
      "epoch": 0.20586666666666667,
      "grad_norm": 0.07642591744661331,
      "learning_rate": 4.871333333333333e-05,
      "loss": 0.0029,
      "step": 3860
    },
    {
      "epoch": 0.2064,
      "grad_norm": 0.15425936877727509,
      "learning_rate": 4.871e-05,
      "loss": 0.0035,
      "step": 3870
    },
    {
      "epoch": 0.20693333333333333,
      "grad_norm": 0.5511066913604736,
      "learning_rate": 4.8706666666666666e-05,
      "loss": 0.0044,
      "step": 3880
    },
    {
      "epoch": 0.20746666666666666,
      "grad_norm": 0.46110257506370544,
      "learning_rate": 4.870333333333333e-05,
      "loss": 0.0039,
      "step": 3890
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.20680558681488037,
      "learning_rate": 4.87e-05,
      "loss": 0.003,
      "step": 3900
    },
    {
      "epoch": 0.20853333333333332,
      "grad_norm": 0.05564236640930176,
      "learning_rate": 4.869666666666667e-05,
      "loss": 0.0037,
      "step": 3910
    },
    {
      "epoch": 0.20906666666666668,
      "grad_norm": 0.6305209398269653,
      "learning_rate": 4.869333333333334e-05,
      "loss": 0.0044,
      "step": 3920
    },
    {
      "epoch": 0.2096,
      "grad_norm": 0.7171710729598999,
      "learning_rate": 4.869e-05,
      "loss": 0.0045,
      "step": 3930
    },
    {
      "epoch": 0.21013333333333334,
      "grad_norm": 0.6134994626045227,
      "learning_rate": 4.868666666666667e-05,
      "loss": 0.0047,
      "step": 3940
    },
    {
      "epoch": 0.21066666666666667,
      "grad_norm": 0.7303315997123718,
      "learning_rate": 4.8683333333333335e-05,
      "loss": 0.0038,
      "step": 3950
    },
    {
      "epoch": 0.2112,
      "grad_norm": 0.08901873230934143,
      "learning_rate": 4.868e-05,
      "loss": 0.0047,
      "step": 3960
    },
    {
      "epoch": 0.21173333333333333,
      "grad_norm": 0.6844189763069153,
      "learning_rate": 4.867666666666667e-05,
      "loss": 0.0054,
      "step": 3970
    },
    {
      "epoch": 0.21226666666666666,
      "grad_norm": 0.14283542335033417,
      "learning_rate": 4.867333333333334e-05,
      "loss": 0.0041,
      "step": 3980
    },
    {
      "epoch": 0.2128,
      "grad_norm": 0.36861151456832886,
      "learning_rate": 4.867000000000001e-05,
      "loss": 0.0052,
      "step": 3990
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.5826926231384277,
      "learning_rate": 4.866666666666667e-05,
      "loss": 0.0035,
      "step": 4000
    },
    {
      "epoch": 0.21386666666666668,
      "grad_norm": 0.36293578147888184,
      "learning_rate": 4.866333333333333e-05,
      "loss": 0.0046,
      "step": 4010
    },
    {
      "epoch": 0.2144,
      "grad_norm": 0.24389925599098206,
      "learning_rate": 4.866e-05,
      "loss": 0.0039,
      "step": 4020
    },
    {
      "epoch": 0.21493333333333334,
      "grad_norm": 0.5020787715911865,
      "learning_rate": 4.8656666666666664e-05,
      "loss": 0.0031,
      "step": 4030
    },
    {
      "epoch": 0.21546666666666667,
      "grad_norm": 0.38719961047172546,
      "learning_rate": 4.865333333333334e-05,
      "loss": 0.0044,
      "step": 4040
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.06144500896334648,
      "learning_rate": 4.8650000000000003e-05,
      "loss": 0.0042,
      "step": 4050
    },
    {
      "epoch": 0.21653333333333333,
      "grad_norm": 0.40681084990501404,
      "learning_rate": 4.864666666666667e-05,
      "loss": 0.0044,
      "step": 4060
    },
    {
      "epoch": 0.21706666666666666,
      "grad_norm": 0.14493100345134735,
      "learning_rate": 4.8643333333333336e-05,
      "loss": 0.0043,
      "step": 4070
    },
    {
      "epoch": 0.2176,
      "grad_norm": 0.3004770874977112,
      "learning_rate": 4.864e-05,
      "loss": 0.0035,
      "step": 4080
    },
    {
      "epoch": 0.21813333333333335,
      "grad_norm": 0.07686250656843185,
      "learning_rate": 4.863666666666667e-05,
      "loss": 0.0033,
      "step": 4090
    },
    {
      "epoch": 0.21866666666666668,
      "grad_norm": 0.5701009035110474,
      "learning_rate": 4.8633333333333334e-05,
      "loss": 0.0037,
      "step": 4100
    },
    {
      "epoch": 0.2192,
      "grad_norm": 0.37350142002105713,
      "learning_rate": 4.863e-05,
      "loss": 0.004,
      "step": 4110
    },
    {
      "epoch": 0.21973333333333334,
      "grad_norm": 0.43098151683807373,
      "learning_rate": 4.862666666666667e-05,
      "loss": 0.0034,
      "step": 4120
    },
    {
      "epoch": 0.22026666666666667,
      "grad_norm": 0.07158565521240234,
      "learning_rate": 4.862333333333334e-05,
      "loss": 0.0046,
      "step": 4130
    },
    {
      "epoch": 0.2208,
      "grad_norm": 0.27613237500190735,
      "learning_rate": 4.8620000000000005e-05,
      "loss": 0.0039,
      "step": 4140
    },
    {
      "epoch": 0.22133333333333333,
      "grad_norm": 0.49171319603919983,
      "learning_rate": 4.861666666666667e-05,
      "loss": 0.0042,
      "step": 4150
    },
    {
      "epoch": 0.22186666666666666,
      "grad_norm": 0.5819414258003235,
      "learning_rate": 4.861333333333333e-05,
      "loss": 0.0042,
      "step": 4160
    },
    {
      "epoch": 0.2224,
      "grad_norm": 1.0579158067703247,
      "learning_rate": 4.861e-05,
      "loss": 0.004,
      "step": 4170
    },
    {
      "epoch": 0.22293333333333334,
      "grad_norm": 1.0192979574203491,
      "learning_rate": 4.860666666666667e-05,
      "loss": 0.0043,
      "step": 4180
    },
    {
      "epoch": 0.22346666666666667,
      "grad_norm": 0.5535839200019836,
      "learning_rate": 4.8603333333333336e-05,
      "loss": 0.0046,
      "step": 4190
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.55046546459198,
      "learning_rate": 4.86e-05,
      "loss": 0.0034,
      "step": 4200
    },
    {
      "epoch": 0.22453333333333333,
      "grad_norm": 0.6513857841491699,
      "learning_rate": 4.859666666666667e-05,
      "loss": 0.0036,
      "step": 4210
    },
    {
      "epoch": 0.22506666666666666,
      "grad_norm": 0.36104142665863037,
      "learning_rate": 4.8593333333333335e-05,
      "loss": 0.0051,
      "step": 4220
    },
    {
      "epoch": 0.2256,
      "grad_norm": 0.2482440173625946,
      "learning_rate": 4.859e-05,
      "loss": 0.0049,
      "step": 4230
    },
    {
      "epoch": 0.22613333333333333,
      "grad_norm": 0.447270929813385,
      "learning_rate": 4.858666666666667e-05,
      "loss": 0.0028,
      "step": 4240
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.4844151437282562,
      "learning_rate": 4.858333333333333e-05,
      "loss": 0.0035,
      "step": 4250
    },
    {
      "epoch": 0.2272,
      "grad_norm": 0.5118101835250854,
      "learning_rate": 4.8580000000000006e-05,
      "loss": 0.0029,
      "step": 4260
    },
    {
      "epoch": 0.22773333333333334,
      "grad_norm": 0.3891803026199341,
      "learning_rate": 4.857666666666667e-05,
      "loss": 0.0051,
      "step": 4270
    },
    {
      "epoch": 0.22826666666666667,
      "grad_norm": 0.6131083369255066,
      "learning_rate": 4.857333333333334e-05,
      "loss": 0.0038,
      "step": 4280
    },
    {
      "epoch": 0.2288,
      "grad_norm": 0.589374840259552,
      "learning_rate": 4.8570000000000004e-05,
      "loss": 0.002,
      "step": 4290
    },
    {
      "epoch": 0.22933333333333333,
      "grad_norm": 0.3645341694355011,
      "learning_rate": 4.856666666666667e-05,
      "loss": 0.0029,
      "step": 4300
    },
    {
      "epoch": 0.22986666666666666,
      "grad_norm": 0.6198914647102356,
      "learning_rate": 4.856333333333333e-05,
      "loss": 0.004,
      "step": 4310
    },
    {
      "epoch": 0.2304,
      "grad_norm": 0.18911275267601013,
      "learning_rate": 4.856e-05,
      "loss": 0.0037,
      "step": 4320
    },
    {
      "epoch": 0.23093333333333332,
      "grad_norm": 0.34791332483291626,
      "learning_rate": 4.855666666666667e-05,
      "loss": 0.0054,
      "step": 4330
    },
    {
      "epoch": 0.23146666666666665,
      "grad_norm": 0.7444477677345276,
      "learning_rate": 4.8553333333333335e-05,
      "loss": 0.0043,
      "step": 4340
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.6429399847984314,
      "learning_rate": 4.855e-05,
      "loss": 0.0026,
      "step": 4350
    },
    {
      "epoch": 0.23253333333333334,
      "grad_norm": 1.0854536294937134,
      "learning_rate": 4.854666666666667e-05,
      "loss": 0.0045,
      "step": 4360
    },
    {
      "epoch": 0.23306666666666667,
      "grad_norm": 0.8099024295806885,
      "learning_rate": 4.854333333333333e-05,
      "loss": 0.0041,
      "step": 4370
    },
    {
      "epoch": 0.2336,
      "grad_norm": 0.8480098843574524,
      "learning_rate": 4.854e-05,
      "loss": 0.0043,
      "step": 4380
    },
    {
      "epoch": 0.23413333333333333,
      "grad_norm": 0.9955126047134399,
      "learning_rate": 4.853666666666667e-05,
      "loss": 0.0041,
      "step": 4390
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 0.383511483669281,
      "learning_rate": 4.853333333333334e-05,
      "loss": 0.0043,
      "step": 4400
    },
    {
      "epoch": 0.2352,
      "grad_norm": 0.27394965291023254,
      "learning_rate": 4.8530000000000005e-05,
      "loss": 0.0057,
      "step": 4410
    },
    {
      "epoch": 0.23573333333333332,
      "grad_norm": 0.28553423285484314,
      "learning_rate": 4.852666666666667e-05,
      "loss": 0.0036,
      "step": 4420
    },
    {
      "epoch": 0.23626666666666668,
      "grad_norm": 0.6814544200897217,
      "learning_rate": 4.852333333333334e-05,
      "loss": 0.0026,
      "step": 4430
    },
    {
      "epoch": 0.2368,
      "grad_norm": 0.7600984573364258,
      "learning_rate": 4.852e-05,
      "loss": 0.0035,
      "step": 4440
    },
    {
      "epoch": 0.23733333333333334,
      "grad_norm": 0.136496901512146,
      "learning_rate": 4.851666666666667e-05,
      "loss": 0.0034,
      "step": 4450
    },
    {
      "epoch": 0.23786666666666667,
      "grad_norm": 0.12896159291267395,
      "learning_rate": 4.8513333333333335e-05,
      "loss": 0.0039,
      "step": 4460
    },
    {
      "epoch": 0.2384,
      "grad_norm": 0.5807177424430847,
      "learning_rate": 4.851e-05,
      "loss": 0.0045,
      "step": 4470
    },
    {
      "epoch": 0.23893333333333333,
      "grad_norm": 0.4056117832660675,
      "learning_rate": 4.850666666666667e-05,
      "loss": 0.0053,
      "step": 4480
    },
    {
      "epoch": 0.23946666666666666,
      "grad_norm": 0.5332998037338257,
      "learning_rate": 4.8503333333333334e-05,
      "loss": 0.0047,
      "step": 4490
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.8130604028701782,
      "learning_rate": 4.85e-05,
      "loss": 0.0035,
      "step": 4500
    },
    {
      "epoch": 0.24053333333333332,
      "grad_norm": 0.2161215841770172,
      "learning_rate": 4.8496666666666666e-05,
      "loss": 0.0044,
      "step": 4510
    },
    {
      "epoch": 0.24106666666666668,
      "grad_norm": 0.7540025115013123,
      "learning_rate": 4.849333333333333e-05,
      "loss": 0.0042,
      "step": 4520
    },
    {
      "epoch": 0.2416,
      "grad_norm": 0.12353335320949554,
      "learning_rate": 4.8490000000000005e-05,
      "loss": 0.0037,
      "step": 4530
    },
    {
      "epoch": 0.24213333333333334,
      "grad_norm": 0.860395610332489,
      "learning_rate": 4.848666666666667e-05,
      "loss": 0.0043,
      "step": 4540
    },
    {
      "epoch": 0.24266666666666667,
      "grad_norm": 0.34881943464279175,
      "learning_rate": 4.848333333333334e-05,
      "loss": 0.0034,
      "step": 4550
    },
    {
      "epoch": 0.2432,
      "grad_norm": 0.80735844373703,
      "learning_rate": 4.8480000000000003e-05,
      "loss": 0.0033,
      "step": 4560
    },
    {
      "epoch": 0.24373333333333333,
      "grad_norm": 0.6517484784126282,
      "learning_rate": 4.847666666666667e-05,
      "loss": 0.0021,
      "step": 4570
    },
    {
      "epoch": 0.24426666666666666,
      "grad_norm": 0.33929646015167236,
      "learning_rate": 4.8473333333333336e-05,
      "loss": 0.004,
      "step": 4580
    },
    {
      "epoch": 0.2448,
      "grad_norm": 0.21816009283065796,
      "learning_rate": 4.847e-05,
      "loss": 0.0038,
      "step": 4590
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 0.9184127449989319,
      "learning_rate": 4.8466666666666675e-05,
      "loss": 0.0021,
      "step": 4600
    },
    {
      "epoch": 0.24586666666666668,
      "grad_norm": 0.1949206292629242,
      "learning_rate": 4.846333333333334e-05,
      "loss": 0.0047,
      "step": 4610
    },
    {
      "epoch": 0.2464,
      "grad_norm": 0.9499459266662598,
      "learning_rate": 4.846e-05,
      "loss": 0.0025,
      "step": 4620
    },
    {
      "epoch": 0.24693333333333334,
      "grad_norm": 0.5513360500335693,
      "learning_rate": 4.8456666666666666e-05,
      "loss": 0.0044,
      "step": 4630
    },
    {
      "epoch": 0.24746666666666667,
      "grad_norm": 0.4766518175601959,
      "learning_rate": 4.845333333333333e-05,
      "loss": 0.0029,
      "step": 4640
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.19559885561466217,
      "learning_rate": 4.845e-05,
      "loss": 0.0041,
      "step": 4650
    },
    {
      "epoch": 0.24853333333333333,
      "grad_norm": 0.23109327256679535,
      "learning_rate": 4.8446666666666665e-05,
      "loss": 0.0044,
      "step": 4660
    },
    {
      "epoch": 0.24906666666666666,
      "grad_norm": 0.6150892376899719,
      "learning_rate": 4.844333333333334e-05,
      "loss": 0.0037,
      "step": 4670
    },
    {
      "epoch": 0.2496,
      "grad_norm": 0.7230464220046997,
      "learning_rate": 4.8440000000000004e-05,
      "loss": 0.0048,
      "step": 4680
    },
    {
      "epoch": 0.2501333333333333,
      "grad_norm": 0.8297184705734253,
      "learning_rate": 4.843666666666667e-05,
      "loss": 0.0037,
      "step": 4690
    },
    {
      "epoch": 0.25066666666666665,
      "grad_norm": 0.915379524230957,
      "learning_rate": 4.8433333333333336e-05,
      "loss": 0.0052,
      "step": 4700
    },
    {
      "epoch": 0.2512,
      "grad_norm": 0.2263597846031189,
      "learning_rate": 4.843e-05,
      "loss": 0.0047,
      "step": 4710
    },
    {
      "epoch": 0.2517333333333333,
      "grad_norm": 0.2748524248600006,
      "learning_rate": 4.842666666666667e-05,
      "loss": 0.003,
      "step": 4720
    },
    {
      "epoch": 0.25226666666666664,
      "grad_norm": 0.36046087741851807,
      "learning_rate": 4.8423333333333334e-05,
      "loss": 0.0033,
      "step": 4730
    },
    {
      "epoch": 0.2528,
      "grad_norm": 0.5442243218421936,
      "learning_rate": 4.842000000000001e-05,
      "loss": 0.0056,
      "step": 4740
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.3791087567806244,
      "learning_rate": 4.8416666666666673e-05,
      "loss": 0.005,
      "step": 4750
    },
    {
      "epoch": 0.2538666666666667,
      "grad_norm": 0.15178045630455017,
      "learning_rate": 4.841333333333334e-05,
      "loss": 0.003,
      "step": 4760
    },
    {
      "epoch": 0.2544,
      "grad_norm": 0.6825566291809082,
      "learning_rate": 4.841e-05,
      "loss": 0.0039,
      "step": 4770
    },
    {
      "epoch": 0.25493333333333335,
      "grad_norm": 0.17669406533241272,
      "learning_rate": 4.8406666666666665e-05,
      "loss": 0.004,
      "step": 4780
    },
    {
      "epoch": 0.2554666666666667,
      "grad_norm": 0.22966749966144562,
      "learning_rate": 4.840333333333333e-05,
      "loss": 0.004,
      "step": 4790
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.42497438192367554,
      "learning_rate": 4.8400000000000004e-05,
      "loss": 0.004,
      "step": 4800
    },
    {
      "epoch": 0.25653333333333334,
      "grad_norm": 0.8468451499938965,
      "learning_rate": 4.839666666666667e-05,
      "loss": 0.0051,
      "step": 4810
    },
    {
      "epoch": 0.25706666666666667,
      "grad_norm": 0.4755278527736664,
      "learning_rate": 4.8393333333333336e-05,
      "loss": 0.0038,
      "step": 4820
    },
    {
      "epoch": 0.2576,
      "grad_norm": 0.21965789794921875,
      "learning_rate": 4.839e-05,
      "loss": 0.0046,
      "step": 4830
    },
    {
      "epoch": 0.2581333333333333,
      "grad_norm": 0.419849693775177,
      "learning_rate": 4.838666666666667e-05,
      "loss": 0.0032,
      "step": 4840
    },
    {
      "epoch": 0.25866666666666666,
      "grad_norm": 1.0131136178970337,
      "learning_rate": 4.8383333333333335e-05,
      "loss": 0.0038,
      "step": 4850
    },
    {
      "epoch": 0.2592,
      "grad_norm": 0.20584125816822052,
      "learning_rate": 4.838e-05,
      "loss": 0.0038,
      "step": 4860
    },
    {
      "epoch": 0.2597333333333333,
      "grad_norm": 0.31901058554649353,
      "learning_rate": 4.837666666666667e-05,
      "loss": 0.0038,
      "step": 4870
    },
    {
      "epoch": 0.26026666666666665,
      "grad_norm": 0.8883111476898193,
      "learning_rate": 4.837333333333334e-05,
      "loss": 0.0038,
      "step": 4880
    },
    {
      "epoch": 0.2608,
      "grad_norm": 0.9562267065048218,
      "learning_rate": 4.8370000000000006e-05,
      "loss": 0.0036,
      "step": 4890
    },
    {
      "epoch": 0.2613333333333333,
      "grad_norm": 0.37424686551094055,
      "learning_rate": 4.836666666666667e-05,
      "loss": 0.0027,
      "step": 4900
    },
    {
      "epoch": 0.2618666666666667,
      "grad_norm": 1.0775089263916016,
      "learning_rate": 4.836333333333334e-05,
      "loss": 0.0061,
      "step": 4910
    },
    {
      "epoch": 0.2624,
      "grad_norm": 0.8819240927696228,
      "learning_rate": 4.836e-05,
      "loss": 0.004,
      "step": 4920
    },
    {
      "epoch": 0.26293333333333335,
      "grad_norm": 1.0854804515838623,
      "learning_rate": 4.8356666666666664e-05,
      "loss": 0.0023,
      "step": 4930
    },
    {
      "epoch": 0.2634666666666667,
      "grad_norm": 0.7789454460144043,
      "learning_rate": 4.835333333333334e-05,
      "loss": 0.0043,
      "step": 4940
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.6007401943206787,
      "learning_rate": 4.835e-05,
      "loss": 0.005,
      "step": 4950
    },
    {
      "epoch": 0.26453333333333334,
      "grad_norm": 0.9739281535148621,
      "learning_rate": 4.834666666666667e-05,
      "loss": 0.0049,
      "step": 4960
    },
    {
      "epoch": 0.2650666666666667,
      "grad_norm": 0.19138380885124207,
      "learning_rate": 4.8343333333333335e-05,
      "loss": 0.0026,
      "step": 4970
    },
    {
      "epoch": 0.2656,
      "grad_norm": 0.6728407144546509,
      "learning_rate": 4.834e-05,
      "loss": 0.0031,
      "step": 4980
    },
    {
      "epoch": 0.26613333333333333,
      "grad_norm": 0.3377443552017212,
      "learning_rate": 4.833666666666667e-05,
      "loss": 0.0026,
      "step": 4990
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.11016953736543655,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 0.0047,
      "step": 5000
    },
    {
      "epoch": 0.2672,
      "grad_norm": 0.7915570735931396,
      "learning_rate": 4.833e-05,
      "loss": 0.0043,
      "step": 5010
    },
    {
      "epoch": 0.2677333333333333,
      "grad_norm": 0.8405789136886597,
      "learning_rate": 4.832666666666667e-05,
      "loss": 0.0066,
      "step": 5020
    },
    {
      "epoch": 0.26826666666666665,
      "grad_norm": 0.15262405574321747,
      "learning_rate": 4.832333333333334e-05,
      "loss": 0.0027,
      "step": 5030
    },
    {
      "epoch": 0.2688,
      "grad_norm": 0.36371272802352905,
      "learning_rate": 4.8320000000000005e-05,
      "loss": 0.0036,
      "step": 5040
    },
    {
      "epoch": 0.2693333333333333,
      "grad_norm": 0.08437097817659378,
      "learning_rate": 4.831666666666667e-05,
      "loss": 0.004,
      "step": 5050
    },
    {
      "epoch": 0.26986666666666664,
      "grad_norm": 0.16796867549419403,
      "learning_rate": 4.831333333333334e-05,
      "loss": 0.0033,
      "step": 5060
    },
    {
      "epoch": 0.2704,
      "grad_norm": 0.3471043109893799,
      "learning_rate": 4.8309999999999997e-05,
      "loss": 0.0041,
      "step": 5070
    },
    {
      "epoch": 0.27093333333333336,
      "grad_norm": 0.15289509296417236,
      "learning_rate": 4.830666666666667e-05,
      "loss": 0.0027,
      "step": 5080
    },
    {
      "epoch": 0.2714666666666667,
      "grad_norm": 0.5830383896827698,
      "learning_rate": 4.8303333333333336e-05,
      "loss": 0.0044,
      "step": 5090
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.419699102640152,
      "learning_rate": 4.83e-05,
      "loss": 0.0033,
      "step": 5100
    },
    {
      "epoch": 0.27253333333333335,
      "grad_norm": 0.32693934440612793,
      "learning_rate": 4.829666666666667e-05,
      "loss": 0.0032,
      "step": 5110
    },
    {
      "epoch": 0.2730666666666667,
      "grad_norm": 0.31248822808265686,
      "learning_rate": 4.8293333333333334e-05,
      "loss": 0.0038,
      "step": 5120
    },
    {
      "epoch": 0.2736,
      "grad_norm": 0.4046057164669037,
      "learning_rate": 4.829e-05,
      "loss": 0.0036,
      "step": 5130
    },
    {
      "epoch": 0.27413333333333334,
      "grad_norm": 0.37362363934516907,
      "learning_rate": 4.8286666666666666e-05,
      "loss": 0.0042,
      "step": 5140
    },
    {
      "epoch": 0.27466666666666667,
      "grad_norm": 0.12482045590877533,
      "learning_rate": 4.828333333333334e-05,
      "loss": 0.0039,
      "step": 5150
    },
    {
      "epoch": 0.2752,
      "grad_norm": 0.14330294728279114,
      "learning_rate": 4.8280000000000005e-05,
      "loss": 0.0034,
      "step": 5160
    },
    {
      "epoch": 0.27573333333333333,
      "grad_norm": 0.14167532324790955,
      "learning_rate": 4.827666666666667e-05,
      "loss": 0.0033,
      "step": 5170
    },
    {
      "epoch": 0.27626666666666666,
      "grad_norm": 0.334403932094574,
      "learning_rate": 4.827333333333334e-05,
      "loss": 0.0036,
      "step": 5180
    },
    {
      "epoch": 0.2768,
      "grad_norm": 0.25675466656684875,
      "learning_rate": 4.8270000000000004e-05,
      "loss": 0.0031,
      "step": 5190
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 0.6056532263755798,
      "learning_rate": 4.826666666666667e-05,
      "loss": 0.0038,
      "step": 5200
    },
    {
      "epoch": 0.27786666666666665,
      "grad_norm": 0.22814126312732697,
      "learning_rate": 4.8263333333333336e-05,
      "loss": 0.0032,
      "step": 5210
    },
    {
      "epoch": 0.2784,
      "grad_norm": 0.2361849844455719,
      "learning_rate": 4.826e-05,
      "loss": 0.0036,
      "step": 5220
    },
    {
      "epoch": 0.2789333333333333,
      "grad_norm": 0.23929668962955475,
      "learning_rate": 4.825666666666667e-05,
      "loss": 0.0029,
      "step": 5230
    },
    {
      "epoch": 0.27946666666666664,
      "grad_norm": 1.073900818824768,
      "learning_rate": 4.8253333333333334e-05,
      "loss": 0.0046,
      "step": 5240
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.16992945969104767,
      "learning_rate": 4.825e-05,
      "loss": 0.0039,
      "step": 5250
    },
    {
      "epoch": 0.28053333333333336,
      "grad_norm": 0.1281588226556778,
      "learning_rate": 4.824666666666667e-05,
      "loss": 0.0035,
      "step": 5260
    },
    {
      "epoch": 0.2810666666666667,
      "grad_norm": 0.15435653924942017,
      "learning_rate": 4.824333333333333e-05,
      "loss": 0.0027,
      "step": 5270
    },
    {
      "epoch": 0.2816,
      "grad_norm": 0.5434532761573792,
      "learning_rate": 4.824e-05,
      "loss": 0.0034,
      "step": 5280
    },
    {
      "epoch": 0.28213333333333335,
      "grad_norm": 0.32468608021736145,
      "learning_rate": 4.823666666666667e-05,
      "loss": 0.0045,
      "step": 5290
    },
    {
      "epoch": 0.2826666666666667,
      "grad_norm": 0.4613442122936249,
      "learning_rate": 4.823333333333334e-05,
      "loss": 0.0043,
      "step": 5300
    },
    {
      "epoch": 0.2832,
      "grad_norm": 0.23162946105003357,
      "learning_rate": 4.8230000000000004e-05,
      "loss": 0.0045,
      "step": 5310
    },
    {
      "epoch": 0.28373333333333334,
      "grad_norm": 0.702645480632782,
      "learning_rate": 4.822666666666667e-05,
      "loss": 0.0031,
      "step": 5320
    },
    {
      "epoch": 0.28426666666666667,
      "grad_norm": 0.45757707953453064,
      "learning_rate": 4.8223333333333336e-05,
      "loss": 0.0037,
      "step": 5330
    },
    {
      "epoch": 0.2848,
      "grad_norm": 0.14741291105747223,
      "learning_rate": 4.822e-05,
      "loss": 0.0028,
      "step": 5340
    },
    {
      "epoch": 0.2853333333333333,
      "grad_norm": 0.1068873181939125,
      "learning_rate": 4.821666666666667e-05,
      "loss": 0.0045,
      "step": 5350
    },
    {
      "epoch": 0.28586666666666666,
      "grad_norm": 0.50813227891922,
      "learning_rate": 4.8213333333333335e-05,
      "loss": 0.0044,
      "step": 5360
    },
    {
      "epoch": 0.2864,
      "grad_norm": 0.16846303641796112,
      "learning_rate": 4.821e-05,
      "loss": 0.0026,
      "step": 5370
    },
    {
      "epoch": 0.2869333333333333,
      "grad_norm": 0.1685994565486908,
      "learning_rate": 4.820666666666667e-05,
      "loss": 0.0048,
      "step": 5380
    },
    {
      "epoch": 0.28746666666666665,
      "grad_norm": 0.07594466209411621,
      "learning_rate": 4.820333333333333e-05,
      "loss": 0.0034,
      "step": 5390
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.10613700747489929,
      "learning_rate": 4.82e-05,
      "loss": 0.004,
      "step": 5400
    },
    {
      "epoch": 0.2885333333333333,
      "grad_norm": 1.171213150024414,
      "learning_rate": 4.8196666666666665e-05,
      "loss": 0.0037,
      "step": 5410
    },
    {
      "epoch": 0.2890666666666667,
      "grad_norm": 0.1727750599384308,
      "learning_rate": 4.819333333333333e-05,
      "loss": 0.007,
      "step": 5420
    },
    {
      "epoch": 0.2896,
      "grad_norm": 0.5026355385780334,
      "learning_rate": 4.8190000000000004e-05,
      "loss": 0.0044,
      "step": 5430
    },
    {
      "epoch": 0.29013333333333335,
      "grad_norm": 0.738430917263031,
      "learning_rate": 4.818666666666667e-05,
      "loss": 0.0035,
      "step": 5440
    },
    {
      "epoch": 0.2906666666666667,
      "grad_norm": 0.9125544428825378,
      "learning_rate": 4.818333333333334e-05,
      "loss": 0.004,
      "step": 5450
    },
    {
      "epoch": 0.2912,
      "grad_norm": 0.6349237561225891,
      "learning_rate": 4.818e-05,
      "loss": 0.0026,
      "step": 5460
    },
    {
      "epoch": 0.29173333333333334,
      "grad_norm": 0.12013033777475357,
      "learning_rate": 4.817666666666667e-05,
      "loss": 0.0033,
      "step": 5470
    },
    {
      "epoch": 0.2922666666666667,
      "grad_norm": 0.34633275866508484,
      "learning_rate": 4.8173333333333335e-05,
      "loss": 0.0034,
      "step": 5480
    },
    {
      "epoch": 0.2928,
      "grad_norm": 0.464428573846817,
      "learning_rate": 4.817e-05,
      "loss": 0.0044,
      "step": 5490
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.745073676109314,
      "learning_rate": 4.8166666666666674e-05,
      "loss": 0.0046,
      "step": 5500
    },
    {
      "epoch": 0.29386666666666666,
      "grad_norm": 0.09494279325008392,
      "learning_rate": 4.816333333333334e-05,
      "loss": 0.0047,
      "step": 5510
    },
    {
      "epoch": 0.2944,
      "grad_norm": 0.23760986328125,
      "learning_rate": 4.816e-05,
      "loss": 0.0033,
      "step": 5520
    },
    {
      "epoch": 0.2949333333333333,
      "grad_norm": 0.6814008355140686,
      "learning_rate": 4.8156666666666666e-05,
      "loss": 0.0043,
      "step": 5530
    },
    {
      "epoch": 0.29546666666666666,
      "grad_norm": 0.3736833930015564,
      "learning_rate": 4.815333333333333e-05,
      "loss": 0.0034,
      "step": 5540
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.2025085985660553,
      "learning_rate": 4.815e-05,
      "loss": 0.0045,
      "step": 5550
    },
    {
      "epoch": 0.2965333333333333,
      "grad_norm": 0.40057840943336487,
      "learning_rate": 4.814666666666667e-05,
      "loss": 0.0032,
      "step": 5560
    },
    {
      "epoch": 0.29706666666666665,
      "grad_norm": 0.13559584319591522,
      "learning_rate": 4.814333333333334e-05,
      "loss": 0.0039,
      "step": 5570
    },
    {
      "epoch": 0.2976,
      "grad_norm": 0.40389272570610046,
      "learning_rate": 4.814e-05,
      "loss": 0.003,
      "step": 5580
    },
    {
      "epoch": 0.2981333333333333,
      "grad_norm": 0.8786988854408264,
      "learning_rate": 4.813666666666667e-05,
      "loss": 0.004,
      "step": 5590
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 0.2127317190170288,
      "learning_rate": 4.8133333333333336e-05,
      "loss": 0.0038,
      "step": 5600
    },
    {
      "epoch": 0.2992,
      "grad_norm": 0.8032946586608887,
      "learning_rate": 4.813e-05,
      "loss": 0.0061,
      "step": 5610
    },
    {
      "epoch": 0.29973333333333335,
      "grad_norm": 0.579953670501709,
      "learning_rate": 4.812666666666667e-05,
      "loss": 0.0043,
      "step": 5620
    },
    {
      "epoch": 0.3002666666666667,
      "grad_norm": 0.7441024780273438,
      "learning_rate": 4.8123333333333334e-05,
      "loss": 0.0053,
      "step": 5630
    },
    {
      "epoch": 0.3008,
      "grad_norm": 0.4430989921092987,
      "learning_rate": 4.812000000000001e-05,
      "loss": 0.0024,
      "step": 5640
    },
    {
      "epoch": 0.30133333333333334,
      "grad_norm": 0.8718439936637878,
      "learning_rate": 4.811666666666667e-05,
      "loss": 0.0031,
      "step": 5650
    },
    {
      "epoch": 0.30186666666666667,
      "grad_norm": 0.560989260673523,
      "learning_rate": 4.811333333333334e-05,
      "loss": 0.0049,
      "step": 5660
    },
    {
      "epoch": 0.3024,
      "grad_norm": 0.11598052084445953,
      "learning_rate": 4.8110000000000005e-05,
      "loss": 0.0032,
      "step": 5670
    },
    {
      "epoch": 0.30293333333333333,
      "grad_norm": 0.26385506987571716,
      "learning_rate": 4.8106666666666665e-05,
      "loss": 0.0031,
      "step": 5680
    },
    {
      "epoch": 0.30346666666666666,
      "grad_norm": 0.46862536668777466,
      "learning_rate": 4.810333333333333e-05,
      "loss": 0.0029,
      "step": 5690
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.8056520819664001,
      "learning_rate": 4.8100000000000004e-05,
      "loss": 0.0046,
      "step": 5700
    },
    {
      "epoch": 0.3045333333333333,
      "grad_norm": 0.3475581109523773,
      "learning_rate": 4.809666666666667e-05,
      "loss": 0.0029,
      "step": 5710
    },
    {
      "epoch": 0.30506666666666665,
      "grad_norm": 1.03525972366333,
      "learning_rate": 4.8093333333333336e-05,
      "loss": 0.0033,
      "step": 5720
    },
    {
      "epoch": 0.3056,
      "grad_norm": 0.1219342052936554,
      "learning_rate": 4.809e-05,
      "loss": 0.0036,
      "step": 5730
    },
    {
      "epoch": 0.3061333333333333,
      "grad_norm": 0.12892314791679382,
      "learning_rate": 4.808666666666667e-05,
      "loss": 0.0026,
      "step": 5740
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.17687273025512695,
      "learning_rate": 4.8083333333333334e-05,
      "loss": 0.0033,
      "step": 5750
    },
    {
      "epoch": 0.3072,
      "grad_norm": 0.6376466751098633,
      "learning_rate": 4.808e-05,
      "loss": 0.004,
      "step": 5760
    },
    {
      "epoch": 0.30773333333333336,
      "grad_norm": 0.8012415766716003,
      "learning_rate": 4.8076666666666667e-05,
      "loss": 0.0026,
      "step": 5770
    },
    {
      "epoch": 0.3082666666666667,
      "grad_norm": 0.4038814902305603,
      "learning_rate": 4.807333333333334e-05,
      "loss": 0.0047,
      "step": 5780
    },
    {
      "epoch": 0.3088,
      "grad_norm": 0.2854893207550049,
      "learning_rate": 4.8070000000000006e-05,
      "loss": 0.0047,
      "step": 5790
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 0.8672474026679993,
      "learning_rate": 4.806666666666667e-05,
      "loss": 0.0043,
      "step": 5800
    },
    {
      "epoch": 0.3098666666666667,
      "grad_norm": 0.5165225267410278,
      "learning_rate": 4.806333333333334e-05,
      "loss": 0.0046,
      "step": 5810
    },
    {
      "epoch": 0.3104,
      "grad_norm": 0.1532367616891861,
      "learning_rate": 4.8060000000000004e-05,
      "loss": 0.0042,
      "step": 5820
    },
    {
      "epoch": 0.31093333333333334,
      "grad_norm": 0.26819097995758057,
      "learning_rate": 4.805666666666666e-05,
      "loss": 0.0036,
      "step": 5830
    },
    {
      "epoch": 0.31146666666666667,
      "grad_norm": 0.20573683083057404,
      "learning_rate": 4.8053333333333336e-05,
      "loss": 0.0045,
      "step": 5840
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.12630632519721985,
      "learning_rate": 4.805e-05,
      "loss": 0.0037,
      "step": 5850
    },
    {
      "epoch": 0.31253333333333333,
      "grad_norm": 0.7951568961143494,
      "learning_rate": 4.804666666666667e-05,
      "loss": 0.0031,
      "step": 5860
    },
    {
      "epoch": 0.31306666666666666,
      "grad_norm": 0.541014552116394,
      "learning_rate": 4.8043333333333335e-05,
      "loss": 0.0034,
      "step": 5870
    },
    {
      "epoch": 0.3136,
      "grad_norm": 0.4368927776813507,
      "learning_rate": 4.804e-05,
      "loss": 0.0042,
      "step": 5880
    },
    {
      "epoch": 0.3141333333333333,
      "grad_norm": 0.485323041677475,
      "learning_rate": 4.803666666666667e-05,
      "loss": 0.0026,
      "step": 5890
    },
    {
      "epoch": 0.31466666666666665,
      "grad_norm": 0.5081470012664795,
      "learning_rate": 4.803333333333333e-05,
      "loss": 0.0048,
      "step": 5900
    },
    {
      "epoch": 0.3152,
      "grad_norm": 0.21188507974147797,
      "learning_rate": 4.8030000000000006e-05,
      "loss": 0.004,
      "step": 5910
    },
    {
      "epoch": 0.3157333333333333,
      "grad_norm": 0.701241135597229,
      "learning_rate": 4.802666666666667e-05,
      "loss": 0.0045,
      "step": 5920
    },
    {
      "epoch": 0.31626666666666664,
      "grad_norm": 0.9031947255134583,
      "learning_rate": 4.802333333333334e-05,
      "loss": 0.0033,
      "step": 5930
    },
    {
      "epoch": 0.3168,
      "grad_norm": 0.7000089883804321,
      "learning_rate": 4.8020000000000004e-05,
      "loss": 0.0037,
      "step": 5940
    },
    {
      "epoch": 0.31733333333333336,
      "grad_norm": 1.0438485145568848,
      "learning_rate": 4.801666666666667e-05,
      "loss": 0.0034,
      "step": 5950
    },
    {
      "epoch": 0.3178666666666667,
      "grad_norm": 0.9360888600349426,
      "learning_rate": 4.801333333333334e-05,
      "loss": 0.0027,
      "step": 5960
    },
    {
      "epoch": 0.3184,
      "grad_norm": 0.41642314195632935,
      "learning_rate": 4.801e-05,
      "loss": 0.0036,
      "step": 5970
    },
    {
      "epoch": 0.31893333333333335,
      "grad_norm": 0.466038316488266,
      "learning_rate": 4.800666666666667e-05,
      "loss": 0.005,
      "step": 5980
    },
    {
      "epoch": 0.3194666666666667,
      "grad_norm": 0.41241317987442017,
      "learning_rate": 4.8003333333333335e-05,
      "loss": 0.0037,
      "step": 5990
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.27485039830207825,
      "learning_rate": 4.8e-05,
      "loss": 0.0029,
      "step": 6000
    },
    {
      "epoch": 0.32053333333333334,
      "grad_norm": 0.763052225112915,
      "learning_rate": 4.799666666666667e-05,
      "loss": 0.0032,
      "step": 6010
    },
    {
      "epoch": 0.32106666666666667,
      "grad_norm": 0.4277970790863037,
      "learning_rate": 4.7993333333333333e-05,
      "loss": 0.0042,
      "step": 6020
    },
    {
      "epoch": 0.3216,
      "grad_norm": 0.15121856331825256,
      "learning_rate": 4.799e-05,
      "loss": 0.0039,
      "step": 6030
    },
    {
      "epoch": 0.3221333333333333,
      "grad_norm": 0.15882259607315063,
      "learning_rate": 4.7986666666666666e-05,
      "loss": 0.0043,
      "step": 6040
    },
    {
      "epoch": 0.32266666666666666,
      "grad_norm": 0.2633260190486908,
      "learning_rate": 4.798333333333334e-05,
      "loss": 0.0036,
      "step": 6050
    },
    {
      "epoch": 0.3232,
      "grad_norm": 0.5645442605018616,
      "learning_rate": 4.7980000000000005e-05,
      "loss": 0.0038,
      "step": 6060
    },
    {
      "epoch": 0.3237333333333333,
      "grad_norm": 0.7399417161941528,
      "learning_rate": 4.797666666666667e-05,
      "loss": 0.004,
      "step": 6070
    },
    {
      "epoch": 0.32426666666666665,
      "grad_norm": 0.6597625613212585,
      "learning_rate": 4.797333333333334e-05,
      "loss": 0.0033,
      "step": 6080
    },
    {
      "epoch": 0.3248,
      "grad_norm": 0.6023238897323608,
      "learning_rate": 4.797e-05,
      "loss": 0.0031,
      "step": 6090
    },
    {
      "epoch": 0.3253333333333333,
      "grad_norm": 0.4632626175880432,
      "learning_rate": 4.796666666666667e-05,
      "loss": 0.0043,
      "step": 6100
    },
    {
      "epoch": 0.3258666666666667,
      "grad_norm": 0.7993950247764587,
      "learning_rate": 4.7963333333333335e-05,
      "loss": 0.0034,
      "step": 6110
    },
    {
      "epoch": 0.3264,
      "grad_norm": 0.3375624716281891,
      "learning_rate": 4.796e-05,
      "loss": 0.0033,
      "step": 6120
    },
    {
      "epoch": 0.32693333333333335,
      "grad_norm": 0.10758538544178009,
      "learning_rate": 4.795666666666667e-05,
      "loss": 0.0031,
      "step": 6130
    },
    {
      "epoch": 0.3274666666666667,
      "grad_norm": 0.5668873190879822,
      "learning_rate": 4.7953333333333334e-05,
      "loss": 0.0028,
      "step": 6140
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.19903810322284698,
      "learning_rate": 4.795e-05,
      "loss": 0.004,
      "step": 6150
    },
    {
      "epoch": 0.32853333333333334,
      "grad_norm": 0.5616738796234131,
      "learning_rate": 4.7946666666666666e-05,
      "loss": 0.0037,
      "step": 6160
    },
    {
      "epoch": 0.3290666666666667,
      "grad_norm": 0.5015725493431091,
      "learning_rate": 4.794333333333333e-05,
      "loss": 0.0046,
      "step": 6170
    },
    {
      "epoch": 0.3296,
      "grad_norm": 0.8695777058601379,
      "learning_rate": 4.794e-05,
      "loss": 0.0046,
      "step": 6180
    },
    {
      "epoch": 0.33013333333333333,
      "grad_norm": 0.08935315161943436,
      "learning_rate": 4.793666666666667e-05,
      "loss": 0.0036,
      "step": 6190
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 0.30585724115371704,
      "learning_rate": 4.793333333333334e-05,
      "loss": 0.0038,
      "step": 6200
    },
    {
      "epoch": 0.3312,
      "grad_norm": 0.06702983379364014,
      "learning_rate": 4.7930000000000004e-05,
      "loss": 0.0041,
      "step": 6210
    },
    {
      "epoch": 0.3317333333333333,
      "grad_norm": 0.057648103684186935,
      "learning_rate": 4.792666666666667e-05,
      "loss": 0.0042,
      "step": 6220
    },
    {
      "epoch": 0.33226666666666665,
      "grad_norm": 0.2015252709388733,
      "learning_rate": 4.7923333333333336e-05,
      "loss": 0.0031,
      "step": 6230
    },
    {
      "epoch": 0.3328,
      "grad_norm": 0.5298692584037781,
      "learning_rate": 4.792e-05,
      "loss": 0.0026,
      "step": 6240
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.21351756155490875,
      "learning_rate": 4.791666666666667e-05,
      "loss": 0.0027,
      "step": 6250
    },
    {
      "epoch": 0.33386666666666664,
      "grad_norm": 0.5622604489326477,
      "learning_rate": 4.791333333333334e-05,
      "loss": 0.0039,
      "step": 6260
    },
    {
      "epoch": 0.3344,
      "grad_norm": 0.3121306002140045,
      "learning_rate": 4.791000000000001e-05,
      "loss": 0.0046,
      "step": 6270
    },
    {
      "epoch": 0.33493333333333336,
      "grad_norm": 0.12796218693256378,
      "learning_rate": 4.7906666666666667e-05,
      "loss": 0.0033,
      "step": 6280
    },
    {
      "epoch": 0.3354666666666667,
      "grad_norm": 0.5946971774101257,
      "learning_rate": 4.790333333333333e-05,
      "loss": 0.0044,
      "step": 6290
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.44419679045677185,
      "learning_rate": 4.79e-05,
      "loss": 0.0039,
      "step": 6300
    },
    {
      "epoch": 0.33653333333333335,
      "grad_norm": 0.11502142250537872,
      "learning_rate": 4.7896666666666665e-05,
      "loss": 0.0036,
      "step": 6310
    },
    {
      "epoch": 0.3370666666666667,
      "grad_norm": 0.40814512968063354,
      "learning_rate": 4.789333333333334e-05,
      "loss": 0.0029,
      "step": 6320
    },
    {
      "epoch": 0.3376,
      "grad_norm": 0.34371218085289,
      "learning_rate": 4.7890000000000004e-05,
      "loss": 0.0038,
      "step": 6330
    },
    {
      "epoch": 0.33813333333333334,
      "grad_norm": 0.20748090744018555,
      "learning_rate": 4.788666666666667e-05,
      "loss": 0.0036,
      "step": 6340
    },
    {
      "epoch": 0.33866666666666667,
      "grad_norm": 0.08567244559526443,
      "learning_rate": 4.7883333333333336e-05,
      "loss": 0.0032,
      "step": 6350
    },
    {
      "epoch": 0.3392,
      "grad_norm": 0.25291070342063904,
      "learning_rate": 4.788e-05,
      "loss": 0.0034,
      "step": 6360
    },
    {
      "epoch": 0.33973333333333333,
      "grad_norm": 0.4300432503223419,
      "learning_rate": 4.787666666666667e-05,
      "loss": 0.0025,
      "step": 6370
    },
    {
      "epoch": 0.34026666666666666,
      "grad_norm": 0.1382138580083847,
      "learning_rate": 4.7873333333333335e-05,
      "loss": 0.0044,
      "step": 6380
    },
    {
      "epoch": 0.3408,
      "grad_norm": 0.06797310709953308,
      "learning_rate": 4.787e-05,
      "loss": 0.0045,
      "step": 6390
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 0.7963870167732239,
      "learning_rate": 4.7866666666666674e-05,
      "loss": 0.0038,
      "step": 6400
    },
    {
      "epoch": 0.34186666666666665,
      "grad_norm": 0.22045734524726868,
      "learning_rate": 4.786333333333334e-05,
      "loss": 0.0042,
      "step": 6410
    },
    {
      "epoch": 0.3424,
      "grad_norm": 0.619143545627594,
      "learning_rate": 4.7860000000000006e-05,
      "loss": 0.0051,
      "step": 6420
    },
    {
      "epoch": 0.3429333333333333,
      "grad_norm": 0.11998546123504639,
      "learning_rate": 4.7856666666666665e-05,
      "loss": 0.0046,
      "step": 6430
    },
    {
      "epoch": 0.34346666666666664,
      "grad_norm": 0.5282598733901978,
      "learning_rate": 4.785333333333333e-05,
      "loss": 0.0032,
      "step": 6440
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.3703174293041229,
      "learning_rate": 4.785e-05,
      "loss": 0.004,
      "step": 6450
    },
    {
      "epoch": 0.34453333333333336,
      "grad_norm": 0.30451497435569763,
      "learning_rate": 4.784666666666667e-05,
      "loss": 0.0032,
      "step": 6460
    },
    {
      "epoch": 0.3450666666666667,
      "grad_norm": 0.3341459035873413,
      "learning_rate": 4.784333333333334e-05,
      "loss": 0.0034,
      "step": 6470
    },
    {
      "epoch": 0.3456,
      "grad_norm": 0.6620323657989502,
      "learning_rate": 4.784e-05,
      "loss": 0.0034,
      "step": 6480
    },
    {
      "epoch": 0.34613333333333335,
      "grad_norm": 0.596446692943573,
      "learning_rate": 4.783666666666667e-05,
      "loss": 0.0028,
      "step": 6490
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.41678982973098755,
      "learning_rate": 4.7833333333333335e-05,
      "loss": 0.0036,
      "step": 6500
    },
    {
      "epoch": 0.3472,
      "grad_norm": 0.21171900629997253,
      "learning_rate": 4.783e-05,
      "loss": 0.004,
      "step": 6510
    },
    {
      "epoch": 0.34773333333333334,
      "grad_norm": 0.3019782602787018,
      "learning_rate": 4.782666666666667e-05,
      "loss": 0.0035,
      "step": 6520
    },
    {
      "epoch": 0.34826666666666667,
      "grad_norm": 0.12491938471794128,
      "learning_rate": 4.7823333333333333e-05,
      "loss": 0.0034,
      "step": 6530
    },
    {
      "epoch": 0.3488,
      "grad_norm": 0.1616710126399994,
      "learning_rate": 4.7820000000000006e-05,
      "loss": 0.0044,
      "step": 6540
    },
    {
      "epoch": 0.34933333333333333,
      "grad_norm": 0.11262165009975433,
      "learning_rate": 4.781666666666667e-05,
      "loss": 0.0036,
      "step": 6550
    },
    {
      "epoch": 0.34986666666666666,
      "grad_norm": 0.1199444979429245,
      "learning_rate": 4.781333333333334e-05,
      "loss": 0.0044,
      "step": 6560
    },
    {
      "epoch": 0.3504,
      "grad_norm": 0.1440456360578537,
      "learning_rate": 4.7810000000000005e-05,
      "loss": 0.0028,
      "step": 6570
    },
    {
      "epoch": 0.3509333333333333,
      "grad_norm": 0.43387874960899353,
      "learning_rate": 4.7806666666666664e-05,
      "loss": 0.0023,
      "step": 6580
    },
    {
      "epoch": 0.35146666666666665,
      "grad_norm": 0.26690152287483215,
      "learning_rate": 4.780333333333333e-05,
      "loss": 0.0031,
      "step": 6590
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.46753576397895813,
      "learning_rate": 4.78e-05,
      "loss": 0.0033,
      "step": 6600
    },
    {
      "epoch": 0.3525333333333333,
      "grad_norm": 0.3391183018684387,
      "learning_rate": 4.779666666666667e-05,
      "loss": 0.003,
      "step": 6610
    },
    {
      "epoch": 0.35306666666666664,
      "grad_norm": 0.08891868591308594,
      "learning_rate": 4.7793333333333335e-05,
      "loss": 0.0041,
      "step": 6620
    },
    {
      "epoch": 0.3536,
      "grad_norm": 0.1688060462474823,
      "learning_rate": 4.779e-05,
      "loss": 0.0037,
      "step": 6630
    },
    {
      "epoch": 0.35413333333333336,
      "grad_norm": 0.08002327382564545,
      "learning_rate": 4.778666666666667e-05,
      "loss": 0.0032,
      "step": 6640
    },
    {
      "epoch": 0.3546666666666667,
      "grad_norm": 0.39663055539131165,
      "learning_rate": 4.7783333333333334e-05,
      "loss": 0.0046,
      "step": 6650
    },
    {
      "epoch": 0.3552,
      "grad_norm": 0.15619449317455292,
      "learning_rate": 4.778e-05,
      "loss": 0.004,
      "step": 6660
    },
    {
      "epoch": 0.35573333333333335,
      "grad_norm": 0.3930307924747467,
      "learning_rate": 4.777666666666667e-05,
      "loss": 0.0033,
      "step": 6670
    },
    {
      "epoch": 0.3562666666666667,
      "grad_norm": 0.4625493884086609,
      "learning_rate": 4.777333333333334e-05,
      "loss": 0.0031,
      "step": 6680
    },
    {
      "epoch": 0.3568,
      "grad_norm": 0.20614247024059296,
      "learning_rate": 4.7770000000000005e-05,
      "loss": 0.0043,
      "step": 6690
    },
    {
      "epoch": 0.35733333333333334,
      "grad_norm": 0.09222175180912018,
      "learning_rate": 4.776666666666667e-05,
      "loss": 0.0037,
      "step": 6700
    },
    {
      "epoch": 0.35786666666666667,
      "grad_norm": 0.17030192911624908,
      "learning_rate": 4.776333333333334e-05,
      "loss": 0.0044,
      "step": 6710
    },
    {
      "epoch": 0.3584,
      "grad_norm": 0.4601621627807617,
      "learning_rate": 4.7760000000000004e-05,
      "loss": 0.004,
      "step": 6720
    },
    {
      "epoch": 0.3589333333333333,
      "grad_norm": 0.6706621050834656,
      "learning_rate": 4.775666666666666e-05,
      "loss": 0.0023,
      "step": 6730
    },
    {
      "epoch": 0.35946666666666666,
      "grad_norm": 0.1572890430688858,
      "learning_rate": 4.7753333333333336e-05,
      "loss": 0.0037,
      "step": 6740
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.1422586590051651,
      "learning_rate": 4.775e-05,
      "loss": 0.0036,
      "step": 6750
    },
    {
      "epoch": 0.3605333333333333,
      "grad_norm": 0.29695966839790344,
      "learning_rate": 4.774666666666667e-05,
      "loss": 0.0045,
      "step": 6760
    },
    {
      "epoch": 0.36106666666666665,
      "grad_norm": 0.41321784257888794,
      "learning_rate": 4.7743333333333334e-05,
      "loss": 0.0038,
      "step": 6770
    },
    {
      "epoch": 0.3616,
      "grad_norm": 0.42212042212486267,
      "learning_rate": 4.774e-05,
      "loss": 0.0041,
      "step": 6780
    },
    {
      "epoch": 0.3621333333333333,
      "grad_norm": 0.5114170908927917,
      "learning_rate": 4.7736666666666666e-05,
      "loss": 0.0044,
      "step": 6790
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 0.7835506796836853,
      "learning_rate": 4.773333333333333e-05,
      "loss": 0.0053,
      "step": 6800
    },
    {
      "epoch": 0.3632,
      "grad_norm": 0.6978720426559448,
      "learning_rate": 4.7730000000000005e-05,
      "loss": 0.0038,
      "step": 6810
    },
    {
      "epoch": 0.36373333333333335,
      "grad_norm": 0.43690958619117737,
      "learning_rate": 4.772666666666667e-05,
      "loss": 0.0039,
      "step": 6820
    },
    {
      "epoch": 0.3642666666666667,
      "grad_norm": 0.39834174513816833,
      "learning_rate": 4.772333333333334e-05,
      "loss": 0.0032,
      "step": 6830
    },
    {
      "epoch": 0.3648,
      "grad_norm": 0.1286812722682953,
      "learning_rate": 4.7720000000000004e-05,
      "loss": 0.0024,
      "step": 6840
    },
    {
      "epoch": 0.36533333333333334,
      "grad_norm": 0.15859639644622803,
      "learning_rate": 4.771666666666667e-05,
      "loss": 0.0045,
      "step": 6850
    },
    {
      "epoch": 0.3658666666666667,
      "grad_norm": 0.32308152318000793,
      "learning_rate": 4.7713333333333336e-05,
      "loss": 0.0025,
      "step": 6860
    },
    {
      "epoch": 0.3664,
      "grad_norm": 0.06148657575249672,
      "learning_rate": 4.771e-05,
      "loss": 0.0032,
      "step": 6870
    },
    {
      "epoch": 0.36693333333333333,
      "grad_norm": 0.3586938977241516,
      "learning_rate": 4.770666666666667e-05,
      "loss": 0.0037,
      "step": 6880
    },
    {
      "epoch": 0.36746666666666666,
      "grad_norm": 0.35512372851371765,
      "learning_rate": 4.7703333333333335e-05,
      "loss": 0.0035,
      "step": 6890
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.5598438382148743,
      "learning_rate": 4.77e-05,
      "loss": 0.0041,
      "step": 6900
    },
    {
      "epoch": 0.3685333333333333,
      "grad_norm": 0.22968964278697968,
      "learning_rate": 4.769666666666667e-05,
      "loss": 0.0026,
      "step": 6910
    },
    {
      "epoch": 0.36906666666666665,
      "grad_norm": 0.10663732886314392,
      "learning_rate": 4.769333333333333e-05,
      "loss": 0.0039,
      "step": 6920
    },
    {
      "epoch": 0.3696,
      "grad_norm": 0.07980483025312424,
      "learning_rate": 4.769e-05,
      "loss": 0.0026,
      "step": 6930
    },
    {
      "epoch": 0.3701333333333333,
      "grad_norm": 0.4293298125267029,
      "learning_rate": 4.7686666666666665e-05,
      "loss": 0.0031,
      "step": 6940
    },
    {
      "epoch": 0.37066666666666664,
      "grad_norm": 0.6309968829154968,
      "learning_rate": 4.768333333333334e-05,
      "loss": 0.0029,
      "step": 6950
    },
    {
      "epoch": 0.3712,
      "grad_norm": 0.09297820925712585,
      "learning_rate": 4.7680000000000004e-05,
      "loss": 0.0032,
      "step": 6960
    },
    {
      "epoch": 0.37173333333333336,
      "grad_norm": 0.12482664734125137,
      "learning_rate": 4.767666666666667e-05,
      "loss": 0.0031,
      "step": 6970
    },
    {
      "epoch": 0.3722666666666667,
      "grad_norm": 0.22225260734558105,
      "learning_rate": 4.7673333333333337e-05,
      "loss": 0.0035,
      "step": 6980
    },
    {
      "epoch": 0.3728,
      "grad_norm": 0.8250199556350708,
      "learning_rate": 4.767e-05,
      "loss": 0.0053,
      "step": 6990
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.6726706624031067,
      "learning_rate": 4.766666666666667e-05,
      "loss": 0.004,
      "step": 7000
    },
    {
      "epoch": 0.3738666666666667,
      "grad_norm": 0.694728434085846,
      "learning_rate": 4.7663333333333335e-05,
      "loss": 0.0042,
      "step": 7010
    },
    {
      "epoch": 0.3744,
      "grad_norm": 0.24243810772895813,
      "learning_rate": 4.766000000000001e-05,
      "loss": 0.0039,
      "step": 7020
    },
    {
      "epoch": 0.37493333333333334,
      "grad_norm": 0.7970618009567261,
      "learning_rate": 4.7656666666666674e-05,
      "loss": 0.0027,
      "step": 7030
    },
    {
      "epoch": 0.37546666666666667,
      "grad_norm": 0.5400095582008362,
      "learning_rate": 4.765333333333333e-05,
      "loss": 0.0035,
      "step": 7040
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.05528039485216141,
      "learning_rate": 4.765e-05,
      "loss": 0.0033,
      "step": 7050
    },
    {
      "epoch": 0.37653333333333333,
      "grad_norm": 0.33586394786834717,
      "learning_rate": 4.7646666666666666e-05,
      "loss": 0.0049,
      "step": 7060
    },
    {
      "epoch": 0.37706666666666666,
      "grad_norm": 0.5630941390991211,
      "learning_rate": 4.764333333333333e-05,
      "loss": 0.0031,
      "step": 7070
    },
    {
      "epoch": 0.3776,
      "grad_norm": 0.28101763129234314,
      "learning_rate": 4.7640000000000005e-05,
      "loss": 0.0022,
      "step": 7080
    },
    {
      "epoch": 0.3781333333333333,
      "grad_norm": 0.5905329585075378,
      "learning_rate": 4.763666666666667e-05,
      "loss": 0.0031,
      "step": 7090
    },
    {
      "epoch": 0.37866666666666665,
      "grad_norm": 0.3411867320537567,
      "learning_rate": 4.763333333333334e-05,
      "loss": 0.0043,
      "step": 7100
    },
    {
      "epoch": 0.3792,
      "grad_norm": 0.23950882256031036,
      "learning_rate": 4.763e-05,
      "loss": 0.0023,
      "step": 7110
    },
    {
      "epoch": 0.3797333333333333,
      "grad_norm": 0.12451291084289551,
      "learning_rate": 4.762666666666667e-05,
      "loss": 0.0048,
      "step": 7120
    },
    {
      "epoch": 0.38026666666666664,
      "grad_norm": 0.24435126781463623,
      "learning_rate": 4.7623333333333335e-05,
      "loss": 0.004,
      "step": 7130
    },
    {
      "epoch": 0.3808,
      "grad_norm": 1.021760106086731,
      "learning_rate": 4.762e-05,
      "loss": 0.0036,
      "step": 7140
    },
    {
      "epoch": 0.38133333333333336,
      "grad_norm": 0.44481226801872253,
      "learning_rate": 4.761666666666667e-05,
      "loss": 0.0036,
      "step": 7150
    },
    {
      "epoch": 0.3818666666666667,
      "grad_norm": 0.8186056017875671,
      "learning_rate": 4.761333333333334e-05,
      "loss": 0.0029,
      "step": 7160
    },
    {
      "epoch": 0.3824,
      "grad_norm": 0.5104255080223083,
      "learning_rate": 4.761000000000001e-05,
      "loss": 0.0038,
      "step": 7170
    },
    {
      "epoch": 0.38293333333333335,
      "grad_norm": 0.6334372758865356,
      "learning_rate": 4.760666666666667e-05,
      "loss": 0.0035,
      "step": 7180
    },
    {
      "epoch": 0.3834666666666667,
      "grad_norm": 0.34461212158203125,
      "learning_rate": 4.760333333333333e-05,
      "loss": 0.0024,
      "step": 7190
    },
    {
      "epoch": 0.384,
      "grad_norm": 1.0513241291046143,
      "learning_rate": 4.76e-05,
      "loss": 0.0041,
      "step": 7200
    },
    {
      "epoch": 0.38453333333333334,
      "grad_norm": 0.4949932098388672,
      "learning_rate": 4.7596666666666664e-05,
      "loss": 0.0027,
      "step": 7210
    },
    {
      "epoch": 0.38506666666666667,
      "grad_norm": 0.3972596824169159,
      "learning_rate": 4.759333333333334e-05,
      "loss": 0.0033,
      "step": 7220
    },
    {
      "epoch": 0.3856,
      "grad_norm": 0.22970537841320038,
      "learning_rate": 4.7590000000000003e-05,
      "loss": 0.0027,
      "step": 7230
    },
    {
      "epoch": 0.38613333333333333,
      "grad_norm": 0.08887067437171936,
      "learning_rate": 4.758666666666667e-05,
      "loss": 0.0047,
      "step": 7240
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.1495639830827713,
      "learning_rate": 4.7583333333333336e-05,
      "loss": 0.0042,
      "step": 7250
    },
    {
      "epoch": 0.3872,
      "grad_norm": 0.22099235653877258,
      "learning_rate": 4.758e-05,
      "loss": 0.0022,
      "step": 7260
    },
    {
      "epoch": 0.3877333333333333,
      "grad_norm": 0.15157613158226013,
      "learning_rate": 4.757666666666667e-05,
      "loss": 0.0028,
      "step": 7270
    },
    {
      "epoch": 0.38826666666666665,
      "grad_norm": 0.24888676404953003,
      "learning_rate": 4.7573333333333334e-05,
      "loss": 0.0033,
      "step": 7280
    },
    {
      "epoch": 0.3888,
      "grad_norm": 0.3828562796115875,
      "learning_rate": 4.757e-05,
      "loss": 0.0036,
      "step": 7290
    },
    {
      "epoch": 0.3893333333333333,
      "grad_norm": 0.30086269974708557,
      "learning_rate": 4.756666666666667e-05,
      "loss": 0.0035,
      "step": 7300
    },
    {
      "epoch": 0.38986666666666664,
      "grad_norm": 0.18769074976444244,
      "learning_rate": 4.756333333333334e-05,
      "loss": 0.0031,
      "step": 7310
    },
    {
      "epoch": 0.3904,
      "grad_norm": 0.3708134591579437,
      "learning_rate": 4.7560000000000005e-05,
      "loss": 0.0034,
      "step": 7320
    },
    {
      "epoch": 0.39093333333333335,
      "grad_norm": 0.4445447027683258,
      "learning_rate": 4.755666666666667e-05,
      "loss": 0.0041,
      "step": 7330
    },
    {
      "epoch": 0.3914666666666667,
      "grad_norm": 0.7549997568130493,
      "learning_rate": 4.755333333333333e-05,
      "loss": 0.0026,
      "step": 7340
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.3515404164791107,
      "learning_rate": 4.755e-05,
      "loss": 0.0035,
      "step": 7350
    },
    {
      "epoch": 0.39253333333333335,
      "grad_norm": 0.20360781252384186,
      "learning_rate": 4.754666666666667e-05,
      "loss": 0.0045,
      "step": 7360
    },
    {
      "epoch": 0.3930666666666667,
      "grad_norm": 0.12063392996788025,
      "learning_rate": 4.7543333333333336e-05,
      "loss": 0.0033,
      "step": 7370
    },
    {
      "epoch": 0.3936,
      "grad_norm": 0.1496015340089798,
      "learning_rate": 4.754e-05,
      "loss": 0.004,
      "step": 7380
    },
    {
      "epoch": 0.39413333333333334,
      "grad_norm": 0.8575964570045471,
      "learning_rate": 4.753666666666667e-05,
      "loss": 0.0044,
      "step": 7390
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 0.16409552097320557,
      "learning_rate": 4.7533333333333334e-05,
      "loss": 0.0028,
      "step": 7400
    },
    {
      "epoch": 0.3952,
      "grad_norm": 0.07508152723312378,
      "learning_rate": 4.753e-05,
      "loss": 0.0033,
      "step": 7410
    },
    {
      "epoch": 0.3957333333333333,
      "grad_norm": 0.20503604412078857,
      "learning_rate": 4.752666666666667e-05,
      "loss": 0.0038,
      "step": 7420
    },
    {
      "epoch": 0.39626666666666666,
      "grad_norm": 0.40114906430244446,
      "learning_rate": 4.752333333333334e-05,
      "loss": 0.0036,
      "step": 7430
    },
    {
      "epoch": 0.3968,
      "grad_norm": 0.11778029054403305,
      "learning_rate": 4.7520000000000006e-05,
      "loss": 0.0037,
      "step": 7440
    },
    {
      "epoch": 0.3973333333333333,
      "grad_norm": 0.7559804320335388,
      "learning_rate": 4.751666666666667e-05,
      "loss": 0.0039,
      "step": 7450
    },
    {
      "epoch": 0.39786666666666665,
      "grad_norm": 0.17204037308692932,
      "learning_rate": 4.751333333333334e-05,
      "loss": 0.003,
      "step": 7460
    },
    {
      "epoch": 0.3984,
      "grad_norm": 0.35450878739356995,
      "learning_rate": 4.7510000000000004e-05,
      "loss": 0.003,
      "step": 7470
    },
    {
      "epoch": 0.3989333333333333,
      "grad_norm": 0.21078810095787048,
      "learning_rate": 4.750666666666667e-05,
      "loss": 0.0041,
      "step": 7480
    },
    {
      "epoch": 0.3994666666666667,
      "grad_norm": 0.22187548875808716,
      "learning_rate": 4.750333333333333e-05,
      "loss": 0.0029,
      "step": 7490
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.05618368089199066,
      "learning_rate": 4.75e-05,
      "loss": 0.0023,
      "step": 7500
    },
    {
      "epoch": 0.40053333333333335,
      "grad_norm": 0.147655189037323,
      "learning_rate": 4.749666666666667e-05,
      "loss": 0.0036,
      "step": 7510
    },
    {
      "epoch": 0.4010666666666667,
      "grad_norm": 0.24525213241577148,
      "learning_rate": 4.7493333333333335e-05,
      "loss": 0.0038,
      "step": 7520
    },
    {
      "epoch": 0.4016,
      "grad_norm": 0.1579916626214981,
      "learning_rate": 4.749e-05,
      "loss": 0.0043,
      "step": 7530
    },
    {
      "epoch": 0.40213333333333334,
      "grad_norm": 0.17169612646102905,
      "learning_rate": 4.748666666666667e-05,
      "loss": 0.0034,
      "step": 7540
    },
    {
      "epoch": 0.4026666666666667,
      "grad_norm": 0.5054346919059753,
      "learning_rate": 4.748333333333333e-05,
      "loss": 0.0035,
      "step": 7550
    },
    {
      "epoch": 0.4032,
      "grad_norm": 0.20321683585643768,
      "learning_rate": 4.748e-05,
      "loss": 0.0029,
      "step": 7560
    },
    {
      "epoch": 0.40373333333333333,
      "grad_norm": 0.2310834527015686,
      "learning_rate": 4.747666666666667e-05,
      "loss": 0.0051,
      "step": 7570
    },
    {
      "epoch": 0.40426666666666666,
      "grad_norm": 0.5685632824897766,
      "learning_rate": 4.747333333333334e-05,
      "loss": 0.0041,
      "step": 7580
    },
    {
      "epoch": 0.4048,
      "grad_norm": 0.8233743906021118,
      "learning_rate": 4.7470000000000005e-05,
      "loss": 0.003,
      "step": 7590
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 0.16485898196697235,
      "learning_rate": 4.746666666666667e-05,
      "loss": 0.0038,
      "step": 7600
    },
    {
      "epoch": 0.40586666666666665,
      "grad_norm": 0.2439536452293396,
      "learning_rate": 4.746333333333334e-05,
      "loss": 0.0035,
      "step": 7610
    },
    {
      "epoch": 0.4064,
      "grad_norm": 0.3669203817844391,
      "learning_rate": 4.746e-05,
      "loss": 0.0036,
      "step": 7620
    },
    {
      "epoch": 0.4069333333333333,
      "grad_norm": 0.26725906133651733,
      "learning_rate": 4.745666666666667e-05,
      "loss": 0.0043,
      "step": 7630
    },
    {
      "epoch": 0.40746666666666664,
      "grad_norm": 0.10606136173009872,
      "learning_rate": 4.7453333333333335e-05,
      "loss": 0.0031,
      "step": 7640
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.19365867972373962,
      "learning_rate": 4.745e-05,
      "loss": 0.0046,
      "step": 7650
    },
    {
      "epoch": 0.40853333333333336,
      "grad_norm": 0.6175088286399841,
      "learning_rate": 4.744666666666667e-05,
      "loss": 0.0038,
      "step": 7660
    },
    {
      "epoch": 0.4090666666666667,
      "grad_norm": 0.7262380719184875,
      "learning_rate": 4.7443333333333334e-05,
      "loss": 0.004,
      "step": 7670
    },
    {
      "epoch": 0.4096,
      "grad_norm": 0.1745513379573822,
      "learning_rate": 4.744e-05,
      "loss": 0.0031,
      "step": 7680
    },
    {
      "epoch": 0.41013333333333335,
      "grad_norm": 0.4979569911956787,
      "learning_rate": 4.7436666666666666e-05,
      "loss": 0.0045,
      "step": 7690
    },
    {
      "epoch": 0.4106666666666667,
      "grad_norm": 0.6885814666748047,
      "learning_rate": 4.743333333333333e-05,
      "loss": 0.0038,
      "step": 7700
    },
    {
      "epoch": 0.4112,
      "grad_norm": 0.182551309466362,
      "learning_rate": 4.7430000000000005e-05,
      "loss": 0.0032,
      "step": 7710
    },
    {
      "epoch": 0.41173333333333334,
      "grad_norm": 0.17910829186439514,
      "learning_rate": 4.742666666666667e-05,
      "loss": 0.0031,
      "step": 7720
    },
    {
      "epoch": 0.41226666666666667,
      "grad_norm": 0.3349897265434265,
      "learning_rate": 4.742333333333334e-05,
      "loss": 0.0047,
      "step": 7730
    },
    {
      "epoch": 0.4128,
      "grad_norm": 0.1026495024561882,
      "learning_rate": 4.742e-05,
      "loss": 0.0043,
      "step": 7740
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.43244993686676025,
      "learning_rate": 4.741666666666667e-05,
      "loss": 0.0029,
      "step": 7750
    },
    {
      "epoch": 0.41386666666666666,
      "grad_norm": 0.5775122046470642,
      "learning_rate": 4.7413333333333336e-05,
      "loss": 0.0032,
      "step": 7760
    },
    {
      "epoch": 0.4144,
      "grad_norm": 1.3117880821228027,
      "learning_rate": 4.741e-05,
      "loss": 0.0036,
      "step": 7770
    },
    {
      "epoch": 0.4149333333333333,
      "grad_norm": 0.16016997396945953,
      "learning_rate": 4.7406666666666675e-05,
      "loss": 0.0036,
      "step": 7780
    },
    {
      "epoch": 0.41546666666666665,
      "grad_norm": 0.03532904386520386,
      "learning_rate": 4.7403333333333334e-05,
      "loss": 0.0038,
      "step": 7790
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.0909101665019989,
      "learning_rate": 4.74e-05,
      "loss": 0.0044,
      "step": 7800
    },
    {
      "epoch": 0.4165333333333333,
      "grad_norm": 0.653403103351593,
      "learning_rate": 4.7396666666666666e-05,
      "loss": 0.0042,
      "step": 7810
    },
    {
      "epoch": 0.41706666666666664,
      "grad_norm": 0.45840269327163696,
      "learning_rate": 4.739333333333333e-05,
      "loss": 0.0049,
      "step": 7820
    },
    {
      "epoch": 0.4176,
      "grad_norm": 0.5310042500495911,
      "learning_rate": 4.739e-05,
      "loss": 0.0034,
      "step": 7830
    },
    {
      "epoch": 0.41813333333333336,
      "grad_norm": 0.24088643491268158,
      "learning_rate": 4.7386666666666665e-05,
      "loss": 0.0026,
      "step": 7840
    },
    {
      "epoch": 0.4186666666666667,
      "grad_norm": 0.05061550438404083,
      "learning_rate": 4.738333333333334e-05,
      "loss": 0.0037,
      "step": 7850
    },
    {
      "epoch": 0.4192,
      "grad_norm": 0.10203050822019577,
      "learning_rate": 4.7380000000000004e-05,
      "loss": 0.003,
      "step": 7860
    },
    {
      "epoch": 0.41973333333333335,
      "grad_norm": 0.42388612031936646,
      "learning_rate": 4.737666666666667e-05,
      "loss": 0.0037,
      "step": 7870
    },
    {
      "epoch": 0.4202666666666667,
      "grad_norm": 0.6160476803779602,
      "learning_rate": 4.7373333333333336e-05,
      "loss": 0.0025,
      "step": 7880
    },
    {
      "epoch": 0.4208,
      "grad_norm": 0.08473096787929535,
      "learning_rate": 4.737e-05,
      "loss": 0.003,
      "step": 7890
    },
    {
      "epoch": 0.42133333333333334,
      "grad_norm": 0.27635979652404785,
      "learning_rate": 4.736666666666667e-05,
      "loss": 0.004,
      "step": 7900
    },
    {
      "epoch": 0.42186666666666667,
      "grad_norm": 0.32506781816482544,
      "learning_rate": 4.7363333333333334e-05,
      "loss": 0.0046,
      "step": 7910
    },
    {
      "epoch": 0.4224,
      "grad_norm": 0.2000185251235962,
      "learning_rate": 4.736000000000001e-05,
      "loss": 0.0031,
      "step": 7920
    },
    {
      "epoch": 0.42293333333333333,
      "grad_norm": 0.10662133991718292,
      "learning_rate": 4.7356666666666673e-05,
      "loss": 0.0025,
      "step": 7930
    },
    {
      "epoch": 0.42346666666666666,
      "grad_norm": 0.42406752705574036,
      "learning_rate": 4.735333333333333e-05,
      "loss": 0.0041,
      "step": 7940
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.49214106798171997,
      "learning_rate": 4.735e-05,
      "loss": 0.0038,
      "step": 7950
    },
    {
      "epoch": 0.4245333333333333,
      "grad_norm": 0.21453718841075897,
      "learning_rate": 4.7346666666666665e-05,
      "loss": 0.0036,
      "step": 7960
    },
    {
      "epoch": 0.42506666666666665,
      "grad_norm": 0.7496076822280884,
      "learning_rate": 4.734333333333333e-05,
      "loss": 0.0033,
      "step": 7970
    },
    {
      "epoch": 0.4256,
      "grad_norm": 0.4965914189815521,
      "learning_rate": 4.7340000000000004e-05,
      "loss": 0.0043,
      "step": 7980
    },
    {
      "epoch": 0.4261333333333333,
      "grad_norm": 0.29624250531196594,
      "learning_rate": 4.733666666666667e-05,
      "loss": 0.0031,
      "step": 7990
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.24762462079524994,
      "learning_rate": 4.7333333333333336e-05,
      "loss": 0.0033,
      "step": 8000
    },
    {
      "epoch": 0.4272,
      "grad_norm": 0.5864242315292358,
      "learning_rate": 4.733e-05,
      "loss": 0.0036,
      "step": 8010
    },
    {
      "epoch": 0.42773333333333335,
      "grad_norm": 0.49055948853492737,
      "learning_rate": 4.732666666666667e-05,
      "loss": 0.0045,
      "step": 8020
    },
    {
      "epoch": 0.4282666666666667,
      "grad_norm": 0.11497455090284348,
      "learning_rate": 4.7323333333333335e-05,
      "loss": 0.0046,
      "step": 8030
    },
    {
      "epoch": 0.4288,
      "grad_norm": 0.26452580094337463,
      "learning_rate": 4.732e-05,
      "loss": 0.0036,
      "step": 8040
    },
    {
      "epoch": 0.42933333333333334,
      "grad_norm": 0.2928515076637268,
      "learning_rate": 4.731666666666667e-05,
      "loss": 0.0039,
      "step": 8050
    },
    {
      "epoch": 0.4298666666666667,
      "grad_norm": 0.29736194014549255,
      "learning_rate": 4.731333333333334e-05,
      "loss": 0.003,
      "step": 8060
    },
    {
      "epoch": 0.4304,
      "grad_norm": 0.06555313616991043,
      "learning_rate": 4.7310000000000006e-05,
      "loss": 0.0034,
      "step": 8070
    },
    {
      "epoch": 0.43093333333333333,
      "grad_norm": 0.27872124314308167,
      "learning_rate": 4.730666666666667e-05,
      "loss": 0.0036,
      "step": 8080
    },
    {
      "epoch": 0.43146666666666667,
      "grad_norm": 0.66259765625,
      "learning_rate": 4.730333333333333e-05,
      "loss": 0.0023,
      "step": 8090
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.2915038466453552,
      "learning_rate": 4.73e-05,
      "loss": 0.0034,
      "step": 8100
    },
    {
      "epoch": 0.4325333333333333,
      "grad_norm": 0.18398921191692352,
      "learning_rate": 4.7296666666666664e-05,
      "loss": 0.0045,
      "step": 8110
    },
    {
      "epoch": 0.43306666666666666,
      "grad_norm": 0.5261145234107971,
      "learning_rate": 4.729333333333334e-05,
      "loss": 0.004,
      "step": 8120
    },
    {
      "epoch": 0.4336,
      "grad_norm": 0.1722346544265747,
      "learning_rate": 4.729e-05,
      "loss": 0.0042,
      "step": 8130
    },
    {
      "epoch": 0.4341333333333333,
      "grad_norm": 0.29626142978668213,
      "learning_rate": 4.728666666666667e-05,
      "loss": 0.0047,
      "step": 8140
    },
    {
      "epoch": 0.43466666666666665,
      "grad_norm": 0.11965449899435043,
      "learning_rate": 4.7283333333333335e-05,
      "loss": 0.0038,
      "step": 8150
    },
    {
      "epoch": 0.4352,
      "grad_norm": 0.1515921801328659,
      "learning_rate": 4.728e-05,
      "loss": 0.0023,
      "step": 8160
    },
    {
      "epoch": 0.4357333333333333,
      "grad_norm": 0.12566372752189636,
      "learning_rate": 4.727666666666667e-05,
      "loss": 0.0031,
      "step": 8170
    },
    {
      "epoch": 0.4362666666666667,
      "grad_norm": 0.2656841278076172,
      "learning_rate": 4.7273333333333334e-05,
      "loss": 0.0046,
      "step": 8180
    },
    {
      "epoch": 0.4368,
      "grad_norm": 0.2705440819263458,
      "learning_rate": 4.7270000000000007e-05,
      "loss": 0.0039,
      "step": 8190
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 0.07859417051076889,
      "learning_rate": 4.726666666666667e-05,
      "loss": 0.0043,
      "step": 8200
    },
    {
      "epoch": 0.4378666666666667,
      "grad_norm": 0.42072221636772156,
      "learning_rate": 4.726333333333334e-05,
      "loss": 0.004,
      "step": 8210
    },
    {
      "epoch": 0.4384,
      "grad_norm": 0.11109709739685059,
      "learning_rate": 4.7260000000000005e-05,
      "loss": 0.003,
      "step": 8220
    },
    {
      "epoch": 0.43893333333333334,
      "grad_norm": 0.4695993661880493,
      "learning_rate": 4.725666666666667e-05,
      "loss": 0.004,
      "step": 8230
    },
    {
      "epoch": 0.43946666666666667,
      "grad_norm": 0.04494535177946091,
      "learning_rate": 4.725333333333334e-05,
      "loss": 0.0032,
      "step": 8240
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.804667592048645,
      "learning_rate": 4.7249999999999997e-05,
      "loss": 0.0039,
      "step": 8250
    },
    {
      "epoch": 0.44053333333333333,
      "grad_norm": 0.7560836672782898,
      "learning_rate": 4.724666666666667e-05,
      "loss": 0.003,
      "step": 8260
    },
    {
      "epoch": 0.44106666666666666,
      "grad_norm": 0.8128677606582642,
      "learning_rate": 4.7243333333333336e-05,
      "loss": 0.0041,
      "step": 8270
    },
    {
      "epoch": 0.4416,
      "grad_norm": 0.9120518565177917,
      "learning_rate": 4.724e-05,
      "loss": 0.0041,
      "step": 8280
    },
    {
      "epoch": 0.4421333333333333,
      "grad_norm": 0.6540539860725403,
      "learning_rate": 4.723666666666667e-05,
      "loss": 0.0039,
      "step": 8290
    },
    {
      "epoch": 0.44266666666666665,
      "grad_norm": 0.19951888918876648,
      "learning_rate": 4.7233333333333334e-05,
      "loss": 0.0044,
      "step": 8300
    },
    {
      "epoch": 0.4432,
      "grad_norm": 0.22691386938095093,
      "learning_rate": 4.723e-05,
      "loss": 0.0035,
      "step": 8310
    },
    {
      "epoch": 0.4437333333333333,
      "grad_norm": 0.41646575927734375,
      "learning_rate": 4.7226666666666666e-05,
      "loss": 0.0036,
      "step": 8320
    },
    {
      "epoch": 0.44426666666666664,
      "grad_norm": 0.2921767830848694,
      "learning_rate": 4.722333333333334e-05,
      "loss": 0.0034,
      "step": 8330
    },
    {
      "epoch": 0.4448,
      "grad_norm": 0.14867337048053741,
      "learning_rate": 4.7220000000000005e-05,
      "loss": 0.0039,
      "step": 8340
    },
    {
      "epoch": 0.44533333333333336,
      "grad_norm": 0.14668089151382446,
      "learning_rate": 4.721666666666667e-05,
      "loss": 0.0022,
      "step": 8350
    },
    {
      "epoch": 0.4458666666666667,
      "grad_norm": 0.30243760347366333,
      "learning_rate": 4.721333333333334e-05,
      "loss": 0.0021,
      "step": 8360
    },
    {
      "epoch": 0.4464,
      "grad_norm": 0.39332762360572815,
      "learning_rate": 4.7210000000000004e-05,
      "loss": 0.0039,
      "step": 8370
    },
    {
      "epoch": 0.44693333333333335,
      "grad_norm": 0.5222969055175781,
      "learning_rate": 4.720666666666667e-05,
      "loss": 0.0028,
      "step": 8380
    },
    {
      "epoch": 0.4474666666666667,
      "grad_norm": 0.23368942737579346,
      "learning_rate": 4.7203333333333336e-05,
      "loss": 0.0026,
      "step": 8390
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.7563446164131165,
      "learning_rate": 4.72e-05,
      "loss": 0.0043,
      "step": 8400
    },
    {
      "epoch": 0.44853333333333334,
      "grad_norm": 0.8170983791351318,
      "learning_rate": 4.719666666666667e-05,
      "loss": 0.0024,
      "step": 8410
    },
    {
      "epoch": 0.44906666666666667,
      "grad_norm": 0.4558444619178772,
      "learning_rate": 4.7193333333333334e-05,
      "loss": 0.0033,
      "step": 8420
    },
    {
      "epoch": 0.4496,
      "grad_norm": 0.2317408174276352,
      "learning_rate": 4.719e-05,
      "loss": 0.0035,
      "step": 8430
    },
    {
      "epoch": 0.45013333333333333,
      "grad_norm": 0.13259096443653107,
      "learning_rate": 4.718666666666667e-05,
      "loss": 0.0031,
      "step": 8440
    },
    {
      "epoch": 0.45066666666666666,
      "grad_norm": 0.1509445458650589,
      "learning_rate": 4.718333333333333e-05,
      "loss": 0.0027,
      "step": 8450
    },
    {
      "epoch": 0.4512,
      "grad_norm": 0.21635064482688904,
      "learning_rate": 4.718e-05,
      "loss": 0.0042,
      "step": 8460
    },
    {
      "epoch": 0.4517333333333333,
      "grad_norm": 0.197567418217659,
      "learning_rate": 4.717666666666667e-05,
      "loss": 0.0033,
      "step": 8470
    },
    {
      "epoch": 0.45226666666666665,
      "grad_norm": 0.3330605924129486,
      "learning_rate": 4.717333333333334e-05,
      "loss": 0.003,
      "step": 8480
    },
    {
      "epoch": 0.4528,
      "grad_norm": 0.10525999963283539,
      "learning_rate": 4.7170000000000004e-05,
      "loss": 0.0045,
      "step": 8490
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.6776398420333862,
      "learning_rate": 4.716666666666667e-05,
      "loss": 0.0036,
      "step": 8500
    },
    {
      "epoch": 0.45386666666666664,
      "grad_norm": 0.320993036031723,
      "learning_rate": 4.7163333333333336e-05,
      "loss": 0.0042,
      "step": 8510
    },
    {
      "epoch": 0.4544,
      "grad_norm": 0.4532236158847809,
      "learning_rate": 4.716e-05,
      "loss": 0.0022,
      "step": 8520
    },
    {
      "epoch": 0.45493333333333336,
      "grad_norm": 0.917411744594574,
      "learning_rate": 4.715666666666667e-05,
      "loss": 0.0044,
      "step": 8530
    },
    {
      "epoch": 0.4554666666666667,
      "grad_norm": 0.20636330544948578,
      "learning_rate": 4.715333333333334e-05,
      "loss": 0.0028,
      "step": 8540
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.20193138718605042,
      "learning_rate": 4.715e-05,
      "loss": 0.0018,
      "step": 8550
    },
    {
      "epoch": 0.45653333333333335,
      "grad_norm": 0.11093652993440628,
      "learning_rate": 4.714666666666667e-05,
      "loss": 0.0036,
      "step": 8560
    },
    {
      "epoch": 0.4570666666666667,
      "grad_norm": 0.27379488945007324,
      "learning_rate": 4.714333333333333e-05,
      "loss": 0.0034,
      "step": 8570
    },
    {
      "epoch": 0.4576,
      "grad_norm": 0.15494827926158905,
      "learning_rate": 4.714e-05,
      "loss": 0.003,
      "step": 8580
    },
    {
      "epoch": 0.45813333333333334,
      "grad_norm": 0.2982659637928009,
      "learning_rate": 4.7136666666666665e-05,
      "loss": 0.0024,
      "step": 8590
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 0.11929178982973099,
      "learning_rate": 4.713333333333333e-05,
      "loss": 0.0035,
      "step": 8600
    },
    {
      "epoch": 0.4592,
      "grad_norm": 0.4529756009578705,
      "learning_rate": 4.7130000000000004e-05,
      "loss": 0.0037,
      "step": 8610
    },
    {
      "epoch": 0.4597333333333333,
      "grad_norm": 0.3932432532310486,
      "learning_rate": 4.712666666666667e-05,
      "loss": 0.0042,
      "step": 8620
    },
    {
      "epoch": 0.46026666666666666,
      "grad_norm": 0.2839241623878479,
      "learning_rate": 4.712333333333334e-05,
      "loss": 0.0026,
      "step": 8630
    },
    {
      "epoch": 0.4608,
      "grad_norm": 0.7131819725036621,
      "learning_rate": 4.712e-05,
      "loss": 0.0033,
      "step": 8640
    },
    {
      "epoch": 0.4613333333333333,
      "grad_norm": 0.1979561299085617,
      "learning_rate": 4.711666666666667e-05,
      "loss": 0.0029,
      "step": 8650
    },
    {
      "epoch": 0.46186666666666665,
      "grad_norm": 0.4668329954147339,
      "learning_rate": 4.7113333333333335e-05,
      "loss": 0.003,
      "step": 8660
    },
    {
      "epoch": 0.4624,
      "grad_norm": 0.1511358618736267,
      "learning_rate": 4.711e-05,
      "loss": 0.0025,
      "step": 8670
    },
    {
      "epoch": 0.4629333333333333,
      "grad_norm": 0.22073259949684143,
      "learning_rate": 4.7106666666666674e-05,
      "loss": 0.0048,
      "step": 8680
    },
    {
      "epoch": 0.4634666666666667,
      "grad_norm": 0.15653984248638153,
      "learning_rate": 4.710333333333334e-05,
      "loss": 0.0026,
      "step": 8690
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.07964897900819778,
      "learning_rate": 4.71e-05,
      "loss": 0.0043,
      "step": 8700
    },
    {
      "epoch": 0.46453333333333335,
      "grad_norm": 0.2276606261730194,
      "learning_rate": 4.7096666666666666e-05,
      "loss": 0.0044,
      "step": 8710
    },
    {
      "epoch": 0.4650666666666667,
      "grad_norm": 0.22989794611930847,
      "learning_rate": 4.709333333333333e-05,
      "loss": 0.0033,
      "step": 8720
    },
    {
      "epoch": 0.4656,
      "grad_norm": 0.5153733491897583,
      "learning_rate": 4.709e-05,
      "loss": 0.0038,
      "step": 8730
    },
    {
      "epoch": 0.46613333333333334,
      "grad_norm": 0.2342797815799713,
      "learning_rate": 4.708666666666667e-05,
      "loss": 0.0039,
      "step": 8740
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.3902888894081116,
      "learning_rate": 4.708333333333334e-05,
      "loss": 0.0029,
      "step": 8750
    },
    {
      "epoch": 0.4672,
      "grad_norm": 0.2030041366815567,
      "learning_rate": 4.708e-05,
      "loss": 0.0029,
      "step": 8760
    },
    {
      "epoch": 0.46773333333333333,
      "grad_norm": 0.3675091862678528,
      "learning_rate": 4.707666666666667e-05,
      "loss": 0.0023,
      "step": 8770
    },
    {
      "epoch": 0.46826666666666666,
      "grad_norm": 0.38665682077407837,
      "learning_rate": 4.7073333333333336e-05,
      "loss": 0.0041,
      "step": 8780
    },
    {
      "epoch": 0.4688,
      "grad_norm": 0.26479852199554443,
      "learning_rate": 4.707e-05,
      "loss": 0.0037,
      "step": 8790
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 0.3337792754173279,
      "learning_rate": 4.706666666666667e-05,
      "loss": 0.0025,
      "step": 8800
    },
    {
      "epoch": 0.46986666666666665,
      "grad_norm": 0.263110488653183,
      "learning_rate": 4.7063333333333334e-05,
      "loss": 0.0048,
      "step": 8810
    },
    {
      "epoch": 0.4704,
      "grad_norm": 0.40785831212997437,
      "learning_rate": 4.706000000000001e-05,
      "loss": 0.0043,
      "step": 8820
    },
    {
      "epoch": 0.4709333333333333,
      "grad_norm": 0.3978665769100189,
      "learning_rate": 4.705666666666667e-05,
      "loss": 0.0029,
      "step": 8830
    },
    {
      "epoch": 0.47146666666666665,
      "grad_norm": 1.0123026371002197,
      "learning_rate": 4.705333333333334e-05,
      "loss": 0.0043,
      "step": 8840
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.09021924436092377,
      "learning_rate": 4.705e-05,
      "loss": 0.0032,
      "step": 8850
    },
    {
      "epoch": 0.47253333333333336,
      "grad_norm": 0.58575838804245,
      "learning_rate": 4.7046666666666665e-05,
      "loss": 0.0056,
      "step": 8860
    },
    {
      "epoch": 0.4730666666666667,
      "grad_norm": 0.645840585231781,
      "learning_rate": 4.704333333333333e-05,
      "loss": 0.0036,
      "step": 8870
    },
    {
      "epoch": 0.4736,
      "grad_norm": 0.5219795107841492,
      "learning_rate": 4.7040000000000004e-05,
      "loss": 0.0031,
      "step": 8880
    },
    {
      "epoch": 0.47413333333333335,
      "grad_norm": 0.1764177531003952,
      "learning_rate": 4.703666666666667e-05,
      "loss": 0.0042,
      "step": 8890
    },
    {
      "epoch": 0.4746666666666667,
      "grad_norm": 0.47685137391090393,
      "learning_rate": 4.7033333333333336e-05,
      "loss": 0.003,
      "step": 8900
    },
    {
      "epoch": 0.4752,
      "grad_norm": 0.11724688112735748,
      "learning_rate": 4.703e-05,
      "loss": 0.0034,
      "step": 8910
    },
    {
      "epoch": 0.47573333333333334,
      "grad_norm": 0.0658380389213562,
      "learning_rate": 4.702666666666667e-05,
      "loss": 0.0047,
      "step": 8920
    },
    {
      "epoch": 0.47626666666666667,
      "grad_norm": 0.3293645679950714,
      "learning_rate": 4.7023333333333334e-05,
      "loss": 0.0039,
      "step": 8930
    },
    {
      "epoch": 0.4768,
      "grad_norm": 0.6541424989700317,
      "learning_rate": 4.702e-05,
      "loss": 0.0045,
      "step": 8940
    },
    {
      "epoch": 0.47733333333333333,
      "grad_norm": 0.8801184296607971,
      "learning_rate": 4.701666666666667e-05,
      "loss": 0.0036,
      "step": 8950
    },
    {
      "epoch": 0.47786666666666666,
      "grad_norm": 0.6147475838661194,
      "learning_rate": 4.701333333333334e-05,
      "loss": 0.0034,
      "step": 8960
    },
    {
      "epoch": 0.4784,
      "grad_norm": 0.3883498013019562,
      "learning_rate": 4.7010000000000006e-05,
      "loss": 0.0029,
      "step": 8970
    },
    {
      "epoch": 0.4789333333333333,
      "grad_norm": 0.12741626799106598,
      "learning_rate": 4.700666666666667e-05,
      "loss": 0.0035,
      "step": 8980
    },
    {
      "epoch": 0.47946666666666665,
      "grad_norm": 0.20816653966903687,
      "learning_rate": 4.700333333333334e-05,
      "loss": 0.004,
      "step": 8990
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.10438498854637146,
      "learning_rate": 4.7e-05,
      "loss": 0.003,
      "step": 9000
    },
    {
      "epoch": 0.4805333333333333,
      "grad_norm": 0.8427887558937073,
      "learning_rate": 4.699666666666666e-05,
      "loss": 0.0025,
      "step": 9010
    },
    {
      "epoch": 0.48106666666666664,
      "grad_norm": 0.42715004086494446,
      "learning_rate": 4.6993333333333336e-05,
      "loss": 0.0037,
      "step": 9020
    },
    {
      "epoch": 0.4816,
      "grad_norm": 0.18906421959400177,
      "learning_rate": 4.699e-05,
      "loss": 0.003,
      "step": 9030
    },
    {
      "epoch": 0.48213333333333336,
      "grad_norm": 0.36041033267974854,
      "learning_rate": 4.698666666666667e-05,
      "loss": 0.0021,
      "step": 9040
    },
    {
      "epoch": 0.4826666666666667,
      "grad_norm": 0.4202917814254761,
      "learning_rate": 4.6983333333333335e-05,
      "loss": 0.0037,
      "step": 9050
    },
    {
      "epoch": 0.4832,
      "grad_norm": 0.6449715495109558,
      "learning_rate": 4.698e-05,
      "loss": 0.0031,
      "step": 9060
    },
    {
      "epoch": 0.48373333333333335,
      "grad_norm": 0.6268922090530396,
      "learning_rate": 4.697666666666667e-05,
      "loss": 0.0042,
      "step": 9070
    },
    {
      "epoch": 0.4842666666666667,
      "grad_norm": 0.4553627073764801,
      "learning_rate": 4.697333333333333e-05,
      "loss": 0.0041,
      "step": 9080
    },
    {
      "epoch": 0.4848,
      "grad_norm": 0.7796072363853455,
      "learning_rate": 4.6970000000000006e-05,
      "loss": 0.0048,
      "step": 9090
    },
    {
      "epoch": 0.48533333333333334,
      "grad_norm": 1.0062779188156128,
      "learning_rate": 4.696666666666667e-05,
      "loss": 0.0036,
      "step": 9100
    },
    {
      "epoch": 0.48586666666666667,
      "grad_norm": 0.056903231889009476,
      "learning_rate": 4.696333333333334e-05,
      "loss": 0.0037,
      "step": 9110
    },
    {
      "epoch": 0.4864,
      "grad_norm": 0.428602397441864,
      "learning_rate": 4.6960000000000004e-05,
      "loss": 0.0042,
      "step": 9120
    },
    {
      "epoch": 0.48693333333333333,
      "grad_norm": 0.6181354522705078,
      "learning_rate": 4.695666666666667e-05,
      "loss": 0.0042,
      "step": 9130
    },
    {
      "epoch": 0.48746666666666666,
      "grad_norm": 0.8760754466056824,
      "learning_rate": 4.695333333333334e-05,
      "loss": 0.0033,
      "step": 9140
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.5211920738220215,
      "learning_rate": 4.695e-05,
      "loss": 0.0031,
      "step": 9150
    },
    {
      "epoch": 0.4885333333333333,
      "grad_norm": 0.6252188086509705,
      "learning_rate": 4.694666666666667e-05,
      "loss": 0.0024,
      "step": 9160
    },
    {
      "epoch": 0.48906666666666665,
      "grad_norm": 0.18605390191078186,
      "learning_rate": 4.6943333333333335e-05,
      "loss": 0.0022,
      "step": 9170
    },
    {
      "epoch": 0.4896,
      "grad_norm": 0.37660545110702515,
      "learning_rate": 4.694e-05,
      "loss": 0.0025,
      "step": 9180
    },
    {
      "epoch": 0.4901333333333333,
      "grad_norm": 0.16628840565681458,
      "learning_rate": 4.693666666666667e-05,
      "loss": 0.003,
      "step": 9190
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 0.10957401990890503,
      "learning_rate": 4.6933333333333333e-05,
      "loss": 0.0045,
      "step": 9200
    },
    {
      "epoch": 0.4912,
      "grad_norm": 0.23121272027492523,
      "learning_rate": 4.693e-05,
      "loss": 0.0035,
      "step": 9210
    },
    {
      "epoch": 0.49173333333333336,
      "grad_norm": 0.5787172913551331,
      "learning_rate": 4.6926666666666666e-05,
      "loss": 0.0038,
      "step": 9220
    },
    {
      "epoch": 0.4922666666666667,
      "grad_norm": 0.3076721727848053,
      "learning_rate": 4.692333333333334e-05,
      "loss": 0.0033,
      "step": 9230
    },
    {
      "epoch": 0.4928,
      "grad_norm": 0.17314255237579346,
      "learning_rate": 4.6920000000000005e-05,
      "loss": 0.0052,
      "step": 9240
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.5908600091934204,
      "learning_rate": 4.691666666666667e-05,
      "loss": 0.0028,
      "step": 9250
    },
    {
      "epoch": 0.4938666666666667,
      "grad_norm": 0.3960444927215576,
      "learning_rate": 4.691333333333334e-05,
      "loss": 0.0029,
      "step": 9260
    },
    {
      "epoch": 0.4944,
      "grad_norm": 0.12039933353662491,
      "learning_rate": 4.691e-05,
      "loss": 0.0023,
      "step": 9270
    },
    {
      "epoch": 0.49493333333333334,
      "grad_norm": 0.07820872962474823,
      "learning_rate": 4.690666666666667e-05,
      "loss": 0.0028,
      "step": 9280
    },
    {
      "epoch": 0.49546666666666667,
      "grad_norm": 0.23389294743537903,
      "learning_rate": 4.6903333333333335e-05,
      "loss": 0.0032,
      "step": 9290
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.19834475219249725,
      "learning_rate": 4.69e-05,
      "loss": 0.003,
      "step": 9300
    },
    {
      "epoch": 0.4965333333333333,
      "grad_norm": 0.11416389048099518,
      "learning_rate": 4.689666666666667e-05,
      "loss": 0.0029,
      "step": 9310
    },
    {
      "epoch": 0.49706666666666666,
      "grad_norm": 0.22793889045715332,
      "learning_rate": 4.6893333333333334e-05,
      "loss": 0.0052,
      "step": 9320
    },
    {
      "epoch": 0.4976,
      "grad_norm": 0.5832070112228394,
      "learning_rate": 4.689e-05,
      "loss": 0.0026,
      "step": 9330
    },
    {
      "epoch": 0.4981333333333333,
      "grad_norm": 0.4652194380760193,
      "learning_rate": 4.6886666666666666e-05,
      "loss": 0.0041,
      "step": 9340
    },
    {
      "epoch": 0.49866666666666665,
      "grad_norm": 0.33744293451309204,
      "learning_rate": 4.688333333333333e-05,
      "loss": 0.0029,
      "step": 9350
    },
    {
      "epoch": 0.4992,
      "grad_norm": 0.29938751459121704,
      "learning_rate": 4.688e-05,
      "loss": 0.0022,
      "step": 9360
    },
    {
      "epoch": 0.4997333333333333,
      "grad_norm": 0.21804270148277283,
      "learning_rate": 4.687666666666667e-05,
      "loss": 0.004,
      "step": 9370
    },
    {
      "epoch": 0.5002666666666666,
      "grad_norm": 0.12315016984939575,
      "learning_rate": 4.687333333333334e-05,
      "loss": 0.0034,
      "step": 9380
    },
    {
      "epoch": 0.5008,
      "grad_norm": 0.5036309361457825,
      "learning_rate": 4.6870000000000004e-05,
      "loss": 0.004,
      "step": 9390
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 0.23689374327659607,
      "learning_rate": 4.686666666666667e-05,
      "loss": 0.0029,
      "step": 9400
    },
    {
      "epoch": 0.5018666666666667,
      "grad_norm": 0.6553374528884888,
      "learning_rate": 4.6863333333333336e-05,
      "loss": 0.003,
      "step": 9410
    },
    {
      "epoch": 0.5024,
      "grad_norm": 0.24321775138378143,
      "learning_rate": 4.686e-05,
      "loss": 0.0035,
      "step": 9420
    },
    {
      "epoch": 0.5029333333333333,
      "grad_norm": 0.168487548828125,
      "learning_rate": 4.685666666666667e-05,
      "loss": 0.0019,
      "step": 9430
    },
    {
      "epoch": 0.5034666666666666,
      "grad_norm": 0.4866848587989807,
      "learning_rate": 4.685333333333334e-05,
      "loss": 0.004,
      "step": 9440
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.3523558974266052,
      "learning_rate": 4.685000000000001e-05,
      "loss": 0.0032,
      "step": 9450
    },
    {
      "epoch": 0.5045333333333333,
      "grad_norm": 0.4841015636920929,
      "learning_rate": 4.6846666666666667e-05,
      "loss": 0.0037,
      "step": 9460
    },
    {
      "epoch": 0.5050666666666667,
      "grad_norm": 0.13274376094341278,
      "learning_rate": 4.684333333333333e-05,
      "loss": 0.0039,
      "step": 9470
    },
    {
      "epoch": 0.5056,
      "grad_norm": 0.23949488997459412,
      "learning_rate": 4.684e-05,
      "loss": 0.0023,
      "step": 9480
    },
    {
      "epoch": 0.5061333333333333,
      "grad_norm": 0.3938952684402466,
      "learning_rate": 4.6836666666666665e-05,
      "loss": 0.0022,
      "step": 9490
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.40462177991867065,
      "learning_rate": 4.683333333333334e-05,
      "loss": 0.0029,
      "step": 9500
    },
    {
      "epoch": 0.5072,
      "grad_norm": 0.21259373426437378,
      "learning_rate": 4.6830000000000004e-05,
      "loss": 0.0026,
      "step": 9510
    },
    {
      "epoch": 0.5077333333333334,
      "grad_norm": 0.08911401033401489,
      "learning_rate": 4.682666666666667e-05,
      "loss": 0.0022,
      "step": 9520
    },
    {
      "epoch": 0.5082666666666666,
      "grad_norm": 0.5114434957504272,
      "learning_rate": 4.6823333333333336e-05,
      "loss": 0.0042,
      "step": 9530
    },
    {
      "epoch": 0.5088,
      "grad_norm": 0.22795476019382477,
      "learning_rate": 4.682e-05,
      "loss": 0.0032,
      "step": 9540
    },
    {
      "epoch": 0.5093333333333333,
      "grad_norm": 0.06997847557067871,
      "learning_rate": 4.681666666666667e-05,
      "loss": 0.0024,
      "step": 9550
    },
    {
      "epoch": 0.5098666666666667,
      "grad_norm": 0.4689229130744934,
      "learning_rate": 4.6813333333333335e-05,
      "loss": 0.0024,
      "step": 9560
    },
    {
      "epoch": 0.5104,
      "grad_norm": 0.14341610670089722,
      "learning_rate": 4.681e-05,
      "loss": 0.0037,
      "step": 9570
    },
    {
      "epoch": 0.5109333333333334,
      "grad_norm": 0.34633833169937134,
      "learning_rate": 4.6806666666666674e-05,
      "loss": 0.0034,
      "step": 9580
    },
    {
      "epoch": 0.5114666666666666,
      "grad_norm": 0.20116133987903595,
      "learning_rate": 4.680333333333334e-05,
      "loss": 0.0039,
      "step": 9590
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.5093703866004944,
      "learning_rate": 4.6800000000000006e-05,
      "loss": 0.003,
      "step": 9600
    },
    {
      "epoch": 0.5125333333333333,
      "grad_norm": 0.5448896288871765,
      "learning_rate": 4.6796666666666665e-05,
      "loss": 0.0033,
      "step": 9610
    },
    {
      "epoch": 0.5130666666666667,
      "grad_norm": 0.16334159672260284,
      "learning_rate": 4.679333333333333e-05,
      "loss": 0.0031,
      "step": 9620
    },
    {
      "epoch": 0.5136,
      "grad_norm": 0.3526427447795868,
      "learning_rate": 4.679e-05,
      "loss": 0.005,
      "step": 9630
    },
    {
      "epoch": 0.5141333333333333,
      "grad_norm": 0.13307328522205353,
      "learning_rate": 4.678666666666667e-05,
      "loss": 0.0037,
      "step": 9640
    },
    {
      "epoch": 0.5146666666666667,
      "grad_norm": 0.6407579183578491,
      "learning_rate": 4.6783333333333337e-05,
      "loss": 0.0034,
      "step": 9650
    },
    {
      "epoch": 0.5152,
      "grad_norm": 0.6078985333442688,
      "learning_rate": 4.678e-05,
      "loss": 0.0036,
      "step": 9660
    },
    {
      "epoch": 0.5157333333333334,
      "grad_norm": 0.18709257245063782,
      "learning_rate": 4.677666666666667e-05,
      "loss": 0.0037,
      "step": 9670
    },
    {
      "epoch": 0.5162666666666667,
      "grad_norm": 0.3274381458759308,
      "learning_rate": 4.6773333333333335e-05,
      "loss": 0.0028,
      "step": 9680
    },
    {
      "epoch": 0.5168,
      "grad_norm": 0.025655290111899376,
      "learning_rate": 4.677e-05,
      "loss": 0.0028,
      "step": 9690
    },
    {
      "epoch": 0.5173333333333333,
      "grad_norm": 0.7980331778526306,
      "learning_rate": 4.676666666666667e-05,
      "loss": 0.0036,
      "step": 9700
    },
    {
      "epoch": 0.5178666666666667,
      "grad_norm": 0.48380985856056213,
      "learning_rate": 4.6763333333333333e-05,
      "loss": 0.0042,
      "step": 9710
    },
    {
      "epoch": 0.5184,
      "grad_norm": 0.43211230635643005,
      "learning_rate": 4.6760000000000006e-05,
      "loss": 0.0043,
      "step": 9720
    },
    {
      "epoch": 0.5189333333333334,
      "grad_norm": 0.08439990133047104,
      "learning_rate": 4.675666666666667e-05,
      "loss": 0.0034,
      "step": 9730
    },
    {
      "epoch": 0.5194666666666666,
      "grad_norm": 0.451124906539917,
      "learning_rate": 4.675333333333334e-05,
      "loss": 0.0032,
      "step": 9740
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.3956979513168335,
      "learning_rate": 4.6750000000000005e-05,
      "loss": 0.0026,
      "step": 9750
    },
    {
      "epoch": 0.5205333333333333,
      "grad_norm": 0.49233415722846985,
      "learning_rate": 4.6746666666666664e-05,
      "loss": 0.0036,
      "step": 9760
    },
    {
      "epoch": 0.5210666666666667,
      "grad_norm": 0.3857923746109009,
      "learning_rate": 4.674333333333333e-05,
      "loss": 0.0032,
      "step": 9770
    },
    {
      "epoch": 0.5216,
      "grad_norm": 0.11853018403053284,
      "learning_rate": 4.674e-05,
      "loss": 0.0038,
      "step": 9780
    },
    {
      "epoch": 0.5221333333333333,
      "grad_norm": 0.32488569617271423,
      "learning_rate": 4.673666666666667e-05,
      "loss": 0.0032,
      "step": 9790
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 0.5906074047088623,
      "learning_rate": 4.6733333333333335e-05,
      "loss": 0.0028,
      "step": 9800
    },
    {
      "epoch": 0.5232,
      "grad_norm": 0.5944461226463318,
      "learning_rate": 4.673e-05,
      "loss": 0.0051,
      "step": 9810
    },
    {
      "epoch": 0.5237333333333334,
      "grad_norm": 0.0774756371974945,
      "learning_rate": 4.672666666666667e-05,
      "loss": 0.0034,
      "step": 9820
    },
    {
      "epoch": 0.5242666666666667,
      "grad_norm": 0.27295002341270447,
      "learning_rate": 4.6723333333333334e-05,
      "loss": 0.0031,
      "step": 9830
    },
    {
      "epoch": 0.5248,
      "grad_norm": 0.09987303614616394,
      "learning_rate": 4.672e-05,
      "loss": 0.003,
      "step": 9840
    },
    {
      "epoch": 0.5253333333333333,
      "grad_norm": 0.36942142248153687,
      "learning_rate": 4.671666666666667e-05,
      "loss": 0.0039,
      "step": 9850
    },
    {
      "epoch": 0.5258666666666667,
      "grad_norm": 0.7205540537834167,
      "learning_rate": 4.671333333333334e-05,
      "loss": 0.0024,
      "step": 9860
    },
    {
      "epoch": 0.5264,
      "grad_norm": 0.1275411993265152,
      "learning_rate": 4.6710000000000005e-05,
      "loss": 0.0041,
      "step": 9870
    },
    {
      "epoch": 0.5269333333333334,
      "grad_norm": 0.08320952206850052,
      "learning_rate": 4.670666666666667e-05,
      "loss": 0.0032,
      "step": 9880
    },
    {
      "epoch": 0.5274666666666666,
      "grad_norm": 0.6263197064399719,
      "learning_rate": 4.670333333333334e-05,
      "loss": 0.0033,
      "step": 9890
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.40055739879608154,
      "learning_rate": 4.6700000000000003e-05,
      "loss": 0.0031,
      "step": 9900
    },
    {
      "epoch": 0.5285333333333333,
      "grad_norm": 0.5333586931228638,
      "learning_rate": 4.669666666666667e-05,
      "loss": 0.0044,
      "step": 9910
    },
    {
      "epoch": 0.5290666666666667,
      "grad_norm": 0.6692814826965332,
      "learning_rate": 4.6693333333333336e-05,
      "loss": 0.003,
      "step": 9920
    },
    {
      "epoch": 0.5296,
      "grad_norm": 0.6555508375167847,
      "learning_rate": 4.669e-05,
      "loss": 0.0043,
      "step": 9930
    },
    {
      "epoch": 0.5301333333333333,
      "grad_norm": 0.10854639858007431,
      "learning_rate": 4.668666666666667e-05,
      "loss": 0.0028,
      "step": 9940
    },
    {
      "epoch": 0.5306666666666666,
      "grad_norm": 0.6768669486045837,
      "learning_rate": 4.6683333333333334e-05,
      "loss": 0.0039,
      "step": 9950
    },
    {
      "epoch": 0.5312,
      "grad_norm": 0.1324596405029297,
      "learning_rate": 4.668e-05,
      "loss": 0.0041,
      "step": 9960
    },
    {
      "epoch": 0.5317333333333333,
      "grad_norm": 0.0761444941163063,
      "learning_rate": 4.6676666666666666e-05,
      "loss": 0.002,
      "step": 9970
    },
    {
      "epoch": 0.5322666666666667,
      "grad_norm": 0.7406746745109558,
      "learning_rate": 4.667333333333333e-05,
      "loss": 0.0043,
      "step": 9980
    },
    {
      "epoch": 0.5328,
      "grad_norm": 0.11739495396614075,
      "learning_rate": 4.6670000000000005e-05,
      "loss": 0.0038,
      "step": 9990
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.32084617018699646,
      "learning_rate": 4.666666666666667e-05,
      "loss": 0.0026,
      "step": 10000
    },
    {
      "epoch": 0.5338666666666667,
      "grad_norm": 0.170763298869133,
      "learning_rate": 4.666333333333334e-05,
      "loss": 0.0048,
      "step": 10010
    },
    {
      "epoch": 0.5344,
      "grad_norm": 0.30074653029441833,
      "learning_rate": 4.6660000000000004e-05,
      "loss": 0.0035,
      "step": 10020
    },
    {
      "epoch": 0.5349333333333334,
      "grad_norm": 0.5823793411254883,
      "learning_rate": 4.665666666666667e-05,
      "loss": 0.0047,
      "step": 10030
    },
    {
      "epoch": 0.5354666666666666,
      "grad_norm": 0.5489091873168945,
      "learning_rate": 4.6653333333333336e-05,
      "loss": 0.0032,
      "step": 10040
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.23724070191383362,
      "learning_rate": 4.665e-05,
      "loss": 0.0026,
      "step": 10050
    },
    {
      "epoch": 0.5365333333333333,
      "grad_norm": 0.15030498802661896,
      "learning_rate": 4.664666666666667e-05,
      "loss": 0.0051,
      "step": 10060
    },
    {
      "epoch": 0.5370666666666667,
      "grad_norm": 0.4486851096153259,
      "learning_rate": 4.6643333333333335e-05,
      "loss": 0.0026,
      "step": 10070
    },
    {
      "epoch": 0.5376,
      "grad_norm": 0.6282883286476135,
      "learning_rate": 4.664e-05,
      "loss": 0.0044,
      "step": 10080
    },
    {
      "epoch": 0.5381333333333334,
      "grad_norm": 0.5588703155517578,
      "learning_rate": 4.663666666666667e-05,
      "loss": 0.0034,
      "step": 10090
    },
    {
      "epoch": 0.5386666666666666,
      "grad_norm": 0.6750127673149109,
      "learning_rate": 4.663333333333333e-05,
      "loss": 0.0036,
      "step": 10100
    },
    {
      "epoch": 0.5392,
      "grad_norm": 0.29173925518989563,
      "learning_rate": 4.663e-05,
      "loss": 0.0034,
      "step": 10110
    },
    {
      "epoch": 0.5397333333333333,
      "grad_norm": 0.4525909125804901,
      "learning_rate": 4.6626666666666665e-05,
      "loss": 0.0035,
      "step": 10120
    },
    {
      "epoch": 0.5402666666666667,
      "grad_norm": 0.31233617663383484,
      "learning_rate": 4.662333333333334e-05,
      "loss": 0.0026,
      "step": 10130
    },
    {
      "epoch": 0.5408,
      "grad_norm": 0.4492012560367584,
      "learning_rate": 4.6620000000000004e-05,
      "loss": 0.003,
      "step": 10140
    },
    {
      "epoch": 0.5413333333333333,
      "grad_norm": 0.4161638915538788,
      "learning_rate": 4.661666666666667e-05,
      "loss": 0.0036,
      "step": 10150
    },
    {
      "epoch": 0.5418666666666667,
      "grad_norm": 0.05953606590628624,
      "learning_rate": 4.6613333333333337e-05,
      "loss": 0.0046,
      "step": 10160
    },
    {
      "epoch": 0.5424,
      "grad_norm": 0.19017179310321808,
      "learning_rate": 4.661e-05,
      "loss": 0.0034,
      "step": 10170
    },
    {
      "epoch": 0.5429333333333334,
      "grad_norm": 0.046683941036462784,
      "learning_rate": 4.660666666666667e-05,
      "loss": 0.0032,
      "step": 10180
    },
    {
      "epoch": 0.5434666666666667,
      "grad_norm": 0.3540116548538208,
      "learning_rate": 4.6603333333333335e-05,
      "loss": 0.0033,
      "step": 10190
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.254875123500824,
      "learning_rate": 4.660000000000001e-05,
      "loss": 0.0037,
      "step": 10200
    },
    {
      "epoch": 0.5445333333333333,
      "grad_norm": 0.5488044023513794,
      "learning_rate": 4.659666666666667e-05,
      "loss": 0.0024,
      "step": 10210
    },
    {
      "epoch": 0.5450666666666667,
      "grad_norm": 0.40361344814300537,
      "learning_rate": 4.659333333333333e-05,
      "loss": 0.0041,
      "step": 10220
    },
    {
      "epoch": 0.5456,
      "grad_norm": 0.8371507525444031,
      "learning_rate": 4.659e-05,
      "loss": 0.0032,
      "step": 10230
    },
    {
      "epoch": 0.5461333333333334,
      "grad_norm": 0.32309192419052124,
      "learning_rate": 4.6586666666666666e-05,
      "loss": 0.0043,
      "step": 10240
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.41206011176109314,
      "learning_rate": 4.658333333333333e-05,
      "loss": 0.004,
      "step": 10250
    },
    {
      "epoch": 0.5472,
      "grad_norm": 0.5001273155212402,
      "learning_rate": 4.6580000000000005e-05,
      "loss": 0.0027,
      "step": 10260
    },
    {
      "epoch": 0.5477333333333333,
      "grad_norm": 0.10135843604803085,
      "learning_rate": 4.657666666666667e-05,
      "loss": 0.0039,
      "step": 10270
    },
    {
      "epoch": 0.5482666666666667,
      "grad_norm": 0.8115418553352356,
      "learning_rate": 4.657333333333334e-05,
      "loss": 0.0037,
      "step": 10280
    },
    {
      "epoch": 0.5488,
      "grad_norm": 0.5543102025985718,
      "learning_rate": 4.657e-05,
      "loss": 0.0029,
      "step": 10290
    },
    {
      "epoch": 0.5493333333333333,
      "grad_norm": 0.45645672082901,
      "learning_rate": 4.656666666666667e-05,
      "loss": 0.0041,
      "step": 10300
    },
    {
      "epoch": 0.5498666666666666,
      "grad_norm": 0.3565230071544647,
      "learning_rate": 4.6563333333333335e-05,
      "loss": 0.0027,
      "step": 10310
    },
    {
      "epoch": 0.5504,
      "grad_norm": 0.4380258619785309,
      "learning_rate": 4.656e-05,
      "loss": 0.0034,
      "step": 10320
    },
    {
      "epoch": 0.5509333333333334,
      "grad_norm": 0.04123830795288086,
      "learning_rate": 4.655666666666667e-05,
      "loss": 0.0026,
      "step": 10330
    },
    {
      "epoch": 0.5514666666666667,
      "grad_norm": 0.23289720714092255,
      "learning_rate": 4.655333333333334e-05,
      "loss": 0.003,
      "step": 10340
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.26414769887924194,
      "learning_rate": 4.655000000000001e-05,
      "loss": 0.0026,
      "step": 10350
    },
    {
      "epoch": 0.5525333333333333,
      "grad_norm": 0.16038478910923004,
      "learning_rate": 4.6546666666666666e-05,
      "loss": 0.0029,
      "step": 10360
    },
    {
      "epoch": 0.5530666666666667,
      "grad_norm": 0.5481619834899902,
      "learning_rate": 4.654333333333333e-05,
      "loss": 0.0035,
      "step": 10370
    },
    {
      "epoch": 0.5536,
      "grad_norm": 0.08787070959806442,
      "learning_rate": 4.654e-05,
      "loss": 0.0043,
      "step": 10380
    },
    {
      "epoch": 0.5541333333333334,
      "grad_norm": 0.7177360653877258,
      "learning_rate": 4.6536666666666664e-05,
      "loss": 0.0046,
      "step": 10390
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 0.14420925080776215,
      "learning_rate": 4.653333333333334e-05,
      "loss": 0.0028,
      "step": 10400
    },
    {
      "epoch": 0.5552,
      "grad_norm": 0.8693001866340637,
      "learning_rate": 4.6530000000000003e-05,
      "loss": 0.0028,
      "step": 10410
    },
    {
      "epoch": 0.5557333333333333,
      "grad_norm": 0.14052370190620422,
      "learning_rate": 4.652666666666667e-05,
      "loss": 0.0035,
      "step": 10420
    },
    {
      "epoch": 0.5562666666666667,
      "grad_norm": 0.12184029817581177,
      "learning_rate": 4.6523333333333336e-05,
      "loss": 0.003,
      "step": 10430
    },
    {
      "epoch": 0.5568,
      "grad_norm": 0.38356661796569824,
      "learning_rate": 4.652e-05,
      "loss": 0.0021,
      "step": 10440
    },
    {
      "epoch": 0.5573333333333333,
      "grad_norm": 0.15367548167705536,
      "learning_rate": 4.651666666666667e-05,
      "loss": 0.004,
      "step": 10450
    },
    {
      "epoch": 0.5578666666666666,
      "grad_norm": 0.6053415536880493,
      "learning_rate": 4.6513333333333334e-05,
      "loss": 0.004,
      "step": 10460
    },
    {
      "epoch": 0.5584,
      "grad_norm": 0.19911807775497437,
      "learning_rate": 4.651e-05,
      "loss": 0.004,
      "step": 10470
    },
    {
      "epoch": 0.5589333333333333,
      "grad_norm": 0.06820634007453918,
      "learning_rate": 4.650666666666667e-05,
      "loss": 0.0026,
      "step": 10480
    },
    {
      "epoch": 0.5594666666666667,
      "grad_norm": 0.903422474861145,
      "learning_rate": 4.650333333333334e-05,
      "loss": 0.0035,
      "step": 10490
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.46521690487861633,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 0.0042,
      "step": 10500
    },
    {
      "epoch": 0.5605333333333333,
      "grad_norm": 0.1004655510187149,
      "learning_rate": 4.6496666666666665e-05,
      "loss": 0.003,
      "step": 10510
    },
    {
      "epoch": 0.5610666666666667,
      "grad_norm": 0.2707628011703491,
      "learning_rate": 4.649333333333333e-05,
      "loss": 0.0023,
      "step": 10520
    },
    {
      "epoch": 0.5616,
      "grad_norm": 0.2839217782020569,
      "learning_rate": 4.649e-05,
      "loss": 0.0036,
      "step": 10530
    },
    {
      "epoch": 0.5621333333333334,
      "grad_norm": 0.3509231507778168,
      "learning_rate": 4.648666666666667e-05,
      "loss": 0.003,
      "step": 10540
    },
    {
      "epoch": 0.5626666666666666,
      "grad_norm": 0.13655723631381989,
      "learning_rate": 4.6483333333333336e-05,
      "loss": 0.0039,
      "step": 10550
    },
    {
      "epoch": 0.5632,
      "grad_norm": 0.5342738032341003,
      "learning_rate": 4.648e-05,
      "loss": 0.0036,
      "step": 10560
    },
    {
      "epoch": 0.5637333333333333,
      "grad_norm": 0.6726572513580322,
      "learning_rate": 4.647666666666667e-05,
      "loss": 0.0026,
      "step": 10570
    },
    {
      "epoch": 0.5642666666666667,
      "grad_norm": 0.5067091584205627,
      "learning_rate": 4.6473333333333334e-05,
      "loss": 0.005,
      "step": 10580
    },
    {
      "epoch": 0.5648,
      "grad_norm": 0.333026260137558,
      "learning_rate": 4.647e-05,
      "loss": 0.0026,
      "step": 10590
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 0.8050078749656677,
      "learning_rate": 4.646666666666667e-05,
      "loss": 0.0041,
      "step": 10600
    },
    {
      "epoch": 0.5658666666666666,
      "grad_norm": 0.5773023366928101,
      "learning_rate": 4.646333333333334e-05,
      "loss": 0.004,
      "step": 10610
    },
    {
      "epoch": 0.5664,
      "grad_norm": 0.577086329460144,
      "learning_rate": 4.6460000000000006e-05,
      "loss": 0.0027,
      "step": 10620
    },
    {
      "epoch": 0.5669333333333333,
      "grad_norm": 0.33276525139808655,
      "learning_rate": 4.645666666666667e-05,
      "loss": 0.0036,
      "step": 10630
    },
    {
      "epoch": 0.5674666666666667,
      "grad_norm": 0.15162056684494019,
      "learning_rate": 4.645333333333334e-05,
      "loss": 0.004,
      "step": 10640
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.10228750854730606,
      "learning_rate": 4.6450000000000004e-05,
      "loss": 0.0029,
      "step": 10650
    },
    {
      "epoch": 0.5685333333333333,
      "grad_norm": 0.22314272820949554,
      "learning_rate": 4.644666666666667e-05,
      "loss": 0.0025,
      "step": 10660
    },
    {
      "epoch": 0.5690666666666667,
      "grad_norm": 0.29322579503059387,
      "learning_rate": 4.6443333333333336e-05,
      "loss": 0.0041,
      "step": 10670
    },
    {
      "epoch": 0.5696,
      "grad_norm": 0.10646264255046844,
      "learning_rate": 4.644e-05,
      "loss": 0.0036,
      "step": 10680
    },
    {
      "epoch": 0.5701333333333334,
      "grad_norm": 0.2931655943393707,
      "learning_rate": 4.643666666666667e-05,
      "loss": 0.0019,
      "step": 10690
    },
    {
      "epoch": 0.5706666666666667,
      "grad_norm": 0.6303362250328064,
      "learning_rate": 4.6433333333333335e-05,
      "loss": 0.0036,
      "step": 10700
    },
    {
      "epoch": 0.5712,
      "grad_norm": 0.330615371465683,
      "learning_rate": 4.643e-05,
      "loss": 0.0027,
      "step": 10710
    },
    {
      "epoch": 0.5717333333333333,
      "grad_norm": 0.26931244134902954,
      "learning_rate": 4.642666666666667e-05,
      "loss": 0.0041,
      "step": 10720
    },
    {
      "epoch": 0.5722666666666667,
      "grad_norm": 0.3268794119358063,
      "learning_rate": 4.642333333333333e-05,
      "loss": 0.0039,
      "step": 10730
    },
    {
      "epoch": 0.5728,
      "grad_norm": 0.19242170453071594,
      "learning_rate": 4.642e-05,
      "loss": 0.0032,
      "step": 10740
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.29508495330810547,
      "learning_rate": 4.641666666666667e-05,
      "loss": 0.0035,
      "step": 10750
    },
    {
      "epoch": 0.5738666666666666,
      "grad_norm": 0.08910146355628967,
      "learning_rate": 4.641333333333334e-05,
      "loss": 0.0035,
      "step": 10760
    },
    {
      "epoch": 0.5744,
      "grad_norm": 0.23415380716323853,
      "learning_rate": 4.6410000000000005e-05,
      "loss": 0.0034,
      "step": 10770
    },
    {
      "epoch": 0.5749333333333333,
      "grad_norm": 0.18697382509708405,
      "learning_rate": 4.640666666666667e-05,
      "loss": 0.0033,
      "step": 10780
    },
    {
      "epoch": 0.5754666666666667,
      "grad_norm": 0.48233523964881897,
      "learning_rate": 4.640333333333334e-05,
      "loss": 0.0034,
      "step": 10790
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.4887343645095825,
      "learning_rate": 4.64e-05,
      "loss": 0.0054,
      "step": 10800
    },
    {
      "epoch": 0.5765333333333333,
      "grad_norm": 0.18187269568443298,
      "learning_rate": 4.639666666666667e-05,
      "loss": 0.0037,
      "step": 10810
    },
    {
      "epoch": 0.5770666666666666,
      "grad_norm": 0.22386574745178223,
      "learning_rate": 4.6393333333333335e-05,
      "loss": 0.0039,
      "step": 10820
    },
    {
      "epoch": 0.5776,
      "grad_norm": 0.3900526463985443,
      "learning_rate": 4.639e-05,
      "loss": 0.0028,
      "step": 10830
    },
    {
      "epoch": 0.5781333333333334,
      "grad_norm": 0.12049583345651627,
      "learning_rate": 4.638666666666667e-05,
      "loss": 0.0029,
      "step": 10840
    },
    {
      "epoch": 0.5786666666666667,
      "grad_norm": 0.45085418224334717,
      "learning_rate": 4.6383333333333334e-05,
      "loss": 0.0039,
      "step": 10850
    },
    {
      "epoch": 0.5792,
      "grad_norm": 0.1902993619441986,
      "learning_rate": 4.638e-05,
      "loss": 0.0041,
      "step": 10860
    },
    {
      "epoch": 0.5797333333333333,
      "grad_norm": 0.1626802682876587,
      "learning_rate": 4.6376666666666666e-05,
      "loss": 0.0027,
      "step": 10870
    },
    {
      "epoch": 0.5802666666666667,
      "grad_norm": 0.16764698922634125,
      "learning_rate": 4.637333333333333e-05,
      "loss": 0.0039,
      "step": 10880
    },
    {
      "epoch": 0.5808,
      "grad_norm": 0.5106242299079895,
      "learning_rate": 4.6370000000000005e-05,
      "loss": 0.0025,
      "step": 10890
    },
    {
      "epoch": 0.5813333333333334,
      "grad_norm": 0.2020801454782486,
      "learning_rate": 4.636666666666667e-05,
      "loss": 0.0028,
      "step": 10900
    },
    {
      "epoch": 0.5818666666666666,
      "grad_norm": 0.4567205309867859,
      "learning_rate": 4.636333333333334e-05,
      "loss": 0.0033,
      "step": 10910
    },
    {
      "epoch": 0.5824,
      "grad_norm": 0.4777730703353882,
      "learning_rate": 4.636e-05,
      "loss": 0.0033,
      "step": 10920
    },
    {
      "epoch": 0.5829333333333333,
      "grad_norm": 0.5422713160514832,
      "learning_rate": 4.635666666666667e-05,
      "loss": 0.0038,
      "step": 10930
    },
    {
      "epoch": 0.5834666666666667,
      "grad_norm": 0.19157429039478302,
      "learning_rate": 4.6353333333333336e-05,
      "loss": 0.0032,
      "step": 10940
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.2921255826950073,
      "learning_rate": 4.635e-05,
      "loss": 0.0042,
      "step": 10950
    },
    {
      "epoch": 0.5845333333333333,
      "grad_norm": 0.6843345165252686,
      "learning_rate": 4.6346666666666675e-05,
      "loss": 0.0037,
      "step": 10960
    },
    {
      "epoch": 0.5850666666666666,
      "grad_norm": 0.26222899556159973,
      "learning_rate": 4.6343333333333334e-05,
      "loss": 0.0034,
      "step": 10970
    },
    {
      "epoch": 0.5856,
      "grad_norm": 0.1036999374628067,
      "learning_rate": 4.634e-05,
      "loss": 0.0031,
      "step": 10980
    },
    {
      "epoch": 0.5861333333333333,
      "grad_norm": 0.16650064289569855,
      "learning_rate": 4.6336666666666666e-05,
      "loss": 0.003,
      "step": 10990
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.2740883231163025,
      "learning_rate": 4.633333333333333e-05,
      "loss": 0.003,
      "step": 11000
    },
    {
      "epoch": 0.5872,
      "grad_norm": 0.6783810257911682,
      "learning_rate": 4.633e-05,
      "loss": 0.0035,
      "step": 11010
    },
    {
      "epoch": 0.5877333333333333,
      "grad_norm": 0.31724849343299866,
      "learning_rate": 4.632666666666667e-05,
      "loss": 0.0038,
      "step": 11020
    },
    {
      "epoch": 0.5882666666666667,
      "grad_norm": 0.5731291770935059,
      "learning_rate": 4.632333333333334e-05,
      "loss": 0.0031,
      "step": 11030
    },
    {
      "epoch": 0.5888,
      "grad_norm": 0.6622605919837952,
      "learning_rate": 4.6320000000000004e-05,
      "loss": 0.0037,
      "step": 11040
    },
    {
      "epoch": 0.5893333333333334,
      "grad_norm": 0.788550615310669,
      "learning_rate": 4.631666666666667e-05,
      "loss": 0.0033,
      "step": 11050
    },
    {
      "epoch": 0.5898666666666667,
      "grad_norm": 0.20846524834632874,
      "learning_rate": 4.6313333333333336e-05,
      "loss": 0.0029,
      "step": 11060
    },
    {
      "epoch": 0.5904,
      "grad_norm": 0.7495117783546448,
      "learning_rate": 4.631e-05,
      "loss": 0.0031,
      "step": 11070
    },
    {
      "epoch": 0.5909333333333333,
      "grad_norm": 0.579387366771698,
      "learning_rate": 4.630666666666667e-05,
      "loss": 0.0029,
      "step": 11080
    },
    {
      "epoch": 0.5914666666666667,
      "grad_norm": 0.8225381970405579,
      "learning_rate": 4.6303333333333334e-05,
      "loss": 0.0041,
      "step": 11090
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.601044774055481,
      "learning_rate": 4.630000000000001e-05,
      "loss": 0.0032,
      "step": 11100
    },
    {
      "epoch": 0.5925333333333334,
      "grad_norm": 0.36142802238464355,
      "learning_rate": 4.6296666666666673e-05,
      "loss": 0.0027,
      "step": 11110
    },
    {
      "epoch": 0.5930666666666666,
      "grad_norm": 0.445991188287735,
      "learning_rate": 4.629333333333333e-05,
      "loss": 0.0028,
      "step": 11120
    },
    {
      "epoch": 0.5936,
      "grad_norm": 0.12557870149612427,
      "learning_rate": 4.629e-05,
      "loss": 0.0038,
      "step": 11130
    },
    {
      "epoch": 0.5941333333333333,
      "grad_norm": 0.7024943828582764,
      "learning_rate": 4.6286666666666665e-05,
      "loss": 0.0037,
      "step": 11140
    },
    {
      "epoch": 0.5946666666666667,
      "grad_norm": 0.2136068344116211,
      "learning_rate": 4.628333333333333e-05,
      "loss": 0.0031,
      "step": 11150
    },
    {
      "epoch": 0.5952,
      "grad_norm": 0.2624160647392273,
      "learning_rate": 4.6280000000000004e-05,
      "loss": 0.0035,
      "step": 11160
    },
    {
      "epoch": 0.5957333333333333,
      "grad_norm": 0.788835346698761,
      "learning_rate": 4.627666666666667e-05,
      "loss": 0.0027,
      "step": 11170
    },
    {
      "epoch": 0.5962666666666666,
      "grad_norm": 0.47755616903305054,
      "learning_rate": 4.6273333333333336e-05,
      "loss": 0.003,
      "step": 11180
    },
    {
      "epoch": 0.5968,
      "grad_norm": 0.16916581988334656,
      "learning_rate": 4.627e-05,
      "loss": 0.0036,
      "step": 11190
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 0.33835694193840027,
      "learning_rate": 4.626666666666667e-05,
      "loss": 0.0025,
      "step": 11200
    },
    {
      "epoch": 0.5978666666666667,
      "grad_norm": 0.3225102424621582,
      "learning_rate": 4.6263333333333335e-05,
      "loss": 0.0021,
      "step": 11210
    },
    {
      "epoch": 0.5984,
      "grad_norm": 0.48056477308273315,
      "learning_rate": 4.626e-05,
      "loss": 0.0022,
      "step": 11220
    },
    {
      "epoch": 0.5989333333333333,
      "grad_norm": 0.2551313042640686,
      "learning_rate": 4.625666666666667e-05,
      "loss": 0.0037,
      "step": 11230
    },
    {
      "epoch": 0.5994666666666667,
      "grad_norm": 0.32706451416015625,
      "learning_rate": 4.625333333333334e-05,
      "loss": 0.0037,
      "step": 11240
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.19030991196632385,
      "learning_rate": 4.6250000000000006e-05,
      "loss": 0.0037,
      "step": 11250
    },
    {
      "epoch": 0.6005333333333334,
      "grad_norm": 0.1928173303604126,
      "learning_rate": 4.624666666666667e-05,
      "loss": 0.0034,
      "step": 11260
    },
    {
      "epoch": 0.6010666666666666,
      "grad_norm": 0.3796032965183258,
      "learning_rate": 4.624333333333333e-05,
      "loss": 0.0024,
      "step": 11270
    },
    {
      "epoch": 0.6016,
      "grad_norm": 0.4177241623401642,
      "learning_rate": 4.624e-05,
      "loss": 0.0033,
      "step": 11280
    },
    {
      "epoch": 0.6021333333333333,
      "grad_norm": 0.4039512574672699,
      "learning_rate": 4.6236666666666664e-05,
      "loss": 0.005,
      "step": 11290
    },
    {
      "epoch": 0.6026666666666667,
      "grad_norm": 0.4726099967956543,
      "learning_rate": 4.623333333333334e-05,
      "loss": 0.003,
      "step": 11300
    },
    {
      "epoch": 0.6032,
      "grad_norm": 0.08527056127786636,
      "learning_rate": 4.623e-05,
      "loss": 0.004,
      "step": 11310
    },
    {
      "epoch": 0.6037333333333333,
      "grad_norm": 0.16102349758148193,
      "learning_rate": 4.622666666666667e-05,
      "loss": 0.0041,
      "step": 11320
    },
    {
      "epoch": 0.6042666666666666,
      "grad_norm": 0.16420893371105194,
      "learning_rate": 4.6223333333333335e-05,
      "loss": 0.005,
      "step": 11330
    },
    {
      "epoch": 0.6048,
      "grad_norm": 0.5442339181900024,
      "learning_rate": 4.622e-05,
      "loss": 0.0025,
      "step": 11340
    },
    {
      "epoch": 0.6053333333333333,
      "grad_norm": 0.31950345635414124,
      "learning_rate": 4.621666666666667e-05,
      "loss": 0.0031,
      "step": 11350
    },
    {
      "epoch": 0.6058666666666667,
      "grad_norm": 0.1866229772567749,
      "learning_rate": 4.6213333333333334e-05,
      "loss": 0.004,
      "step": 11360
    },
    {
      "epoch": 0.6064,
      "grad_norm": 0.7267523407936096,
      "learning_rate": 4.6210000000000006e-05,
      "loss": 0.0031,
      "step": 11370
    },
    {
      "epoch": 0.6069333333333333,
      "grad_norm": 0.38249021768569946,
      "learning_rate": 4.620666666666667e-05,
      "loss": 0.0027,
      "step": 11380
    },
    {
      "epoch": 0.6074666666666667,
      "grad_norm": 0.3789384961128235,
      "learning_rate": 4.620333333333334e-05,
      "loss": 0.0022,
      "step": 11390
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.5055909156799316,
      "learning_rate": 4.6200000000000005e-05,
      "loss": 0.0018,
      "step": 11400
    },
    {
      "epoch": 0.6085333333333334,
      "grad_norm": 0.09542352706193924,
      "learning_rate": 4.619666666666667e-05,
      "loss": 0.0038,
      "step": 11410
    },
    {
      "epoch": 0.6090666666666666,
      "grad_norm": 0.17013411223888397,
      "learning_rate": 4.619333333333333e-05,
      "loss": 0.0035,
      "step": 11420
    },
    {
      "epoch": 0.6096,
      "grad_norm": 0.35162487626075745,
      "learning_rate": 4.619e-05,
      "loss": 0.0025,
      "step": 11430
    },
    {
      "epoch": 0.6101333333333333,
      "grad_norm": 0.23008862137794495,
      "learning_rate": 4.618666666666667e-05,
      "loss": 0.0037,
      "step": 11440
    },
    {
      "epoch": 0.6106666666666667,
      "grad_norm": 0.5388930439949036,
      "learning_rate": 4.6183333333333336e-05,
      "loss": 0.0033,
      "step": 11450
    },
    {
      "epoch": 0.6112,
      "grad_norm": 0.18430869281291962,
      "learning_rate": 4.618e-05,
      "loss": 0.0031,
      "step": 11460
    },
    {
      "epoch": 0.6117333333333334,
      "grad_norm": 0.2068147510290146,
      "learning_rate": 4.617666666666667e-05,
      "loss": 0.0037,
      "step": 11470
    },
    {
      "epoch": 0.6122666666666666,
      "grad_norm": 0.5501202344894409,
      "learning_rate": 4.6173333333333334e-05,
      "loss": 0.0035,
      "step": 11480
    },
    {
      "epoch": 0.6128,
      "grad_norm": 0.7091712951660156,
      "learning_rate": 4.617e-05,
      "loss": 0.0024,
      "step": 11490
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.5637416243553162,
      "learning_rate": 4.6166666666666666e-05,
      "loss": 0.0041,
      "step": 11500
    },
    {
      "epoch": 0.6138666666666667,
      "grad_norm": 0.1384609043598175,
      "learning_rate": 4.616333333333334e-05,
      "loss": 0.0055,
      "step": 11510
    },
    {
      "epoch": 0.6144,
      "grad_norm": 0.637832760810852,
      "learning_rate": 4.6160000000000005e-05,
      "loss": 0.0033,
      "step": 11520
    },
    {
      "epoch": 0.6149333333333333,
      "grad_norm": 0.2586362659931183,
      "learning_rate": 4.615666666666667e-05,
      "loss": 0.0025,
      "step": 11530
    },
    {
      "epoch": 0.6154666666666667,
      "grad_norm": 0.08160419762134552,
      "learning_rate": 4.615333333333334e-05,
      "loss": 0.0037,
      "step": 11540
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.11536362022161484,
      "learning_rate": 4.6150000000000004e-05,
      "loss": 0.0043,
      "step": 11550
    },
    {
      "epoch": 0.6165333333333334,
      "grad_norm": 0.05946339666843414,
      "learning_rate": 4.614666666666667e-05,
      "loss": 0.0031,
      "step": 11560
    },
    {
      "epoch": 0.6170666666666667,
      "grad_norm": 0.5655708909034729,
      "learning_rate": 4.6143333333333336e-05,
      "loss": 0.0049,
      "step": 11570
    },
    {
      "epoch": 0.6176,
      "grad_norm": 0.61268550157547,
      "learning_rate": 4.614e-05,
      "loss": 0.0033,
      "step": 11580
    },
    {
      "epoch": 0.6181333333333333,
      "grad_norm": 0.6326516270637512,
      "learning_rate": 4.613666666666667e-05,
      "loss": 0.0033,
      "step": 11590
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 0.2560026943683624,
      "learning_rate": 4.6133333333333334e-05,
      "loss": 0.0035,
      "step": 11600
    },
    {
      "epoch": 0.6192,
      "grad_norm": 0.4027813971042633,
      "learning_rate": 4.613e-05,
      "loss": 0.0031,
      "step": 11610
    },
    {
      "epoch": 0.6197333333333334,
      "grad_norm": 0.11327386647462845,
      "learning_rate": 4.612666666666667e-05,
      "loss": 0.0037,
      "step": 11620
    },
    {
      "epoch": 0.6202666666666666,
      "grad_norm": 0.2974121868610382,
      "learning_rate": 4.612333333333333e-05,
      "loss": 0.0038,
      "step": 11630
    },
    {
      "epoch": 0.6208,
      "grad_norm": 0.27833765745162964,
      "learning_rate": 4.612e-05,
      "loss": 0.0052,
      "step": 11640
    },
    {
      "epoch": 0.6213333333333333,
      "grad_norm": 0.3568713665008545,
      "learning_rate": 4.611666666666667e-05,
      "loss": 0.0024,
      "step": 11650
    },
    {
      "epoch": 0.6218666666666667,
      "grad_norm": 0.1691589057445526,
      "learning_rate": 4.611333333333334e-05,
      "loss": 0.0035,
      "step": 11660
    },
    {
      "epoch": 0.6224,
      "grad_norm": 0.6596904397010803,
      "learning_rate": 4.6110000000000004e-05,
      "loss": 0.0032,
      "step": 11670
    },
    {
      "epoch": 0.6229333333333333,
      "grad_norm": 0.2539803385734558,
      "learning_rate": 4.610666666666667e-05,
      "loss": 0.004,
      "step": 11680
    },
    {
      "epoch": 0.6234666666666666,
      "grad_norm": 0.2600986063480377,
      "learning_rate": 4.6103333333333336e-05,
      "loss": 0.0035,
      "step": 11690
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.289725661277771,
      "learning_rate": 4.61e-05,
      "loss": 0.0032,
      "step": 11700
    },
    {
      "epoch": 0.6245333333333334,
      "grad_norm": 0.39997678995132446,
      "learning_rate": 4.609666666666667e-05,
      "loss": 0.0039,
      "step": 11710
    },
    {
      "epoch": 0.6250666666666667,
      "grad_norm": 0.7310696244239807,
      "learning_rate": 4.6093333333333335e-05,
      "loss": 0.0031,
      "step": 11720
    },
    {
      "epoch": 0.6256,
      "grad_norm": 0.4937230348587036,
      "learning_rate": 4.609e-05,
      "loss": 0.0028,
      "step": 11730
    },
    {
      "epoch": 0.6261333333333333,
      "grad_norm": 0.1927422285079956,
      "learning_rate": 4.608666666666667e-05,
      "loss": 0.004,
      "step": 11740
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.7302675247192383,
      "learning_rate": 4.608333333333333e-05,
      "loss": 0.0051,
      "step": 11750
    },
    {
      "epoch": 0.6272,
      "grad_norm": 0.21621130406856537,
      "learning_rate": 4.608e-05,
      "loss": 0.0049,
      "step": 11760
    },
    {
      "epoch": 0.6277333333333334,
      "grad_norm": 0.10024944692850113,
      "learning_rate": 4.6076666666666665e-05,
      "loss": 0.0033,
      "step": 11770
    },
    {
      "epoch": 0.6282666666666666,
      "grad_norm": 0.0689442902803421,
      "learning_rate": 4.607333333333334e-05,
      "loss": 0.0022,
      "step": 11780
    },
    {
      "epoch": 0.6288,
      "grad_norm": 0.29313504695892334,
      "learning_rate": 4.6070000000000004e-05,
      "loss": 0.0032,
      "step": 11790
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 0.052753303200006485,
      "learning_rate": 4.606666666666667e-05,
      "loss": 0.0022,
      "step": 11800
    },
    {
      "epoch": 0.6298666666666667,
      "grad_norm": 0.8537288308143616,
      "learning_rate": 4.606333333333334e-05,
      "loss": 0.0031,
      "step": 11810
    },
    {
      "epoch": 0.6304,
      "grad_norm": 0.7763237953186035,
      "learning_rate": 4.606e-05,
      "loss": 0.0036,
      "step": 11820
    },
    {
      "epoch": 0.6309333333333333,
      "grad_norm": 0.6042749285697937,
      "learning_rate": 4.605666666666667e-05,
      "loss": 0.0023,
      "step": 11830
    },
    {
      "epoch": 0.6314666666666666,
      "grad_norm": 0.16803060472011566,
      "learning_rate": 4.6053333333333335e-05,
      "loss": 0.0021,
      "step": 11840
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.2780646085739136,
      "learning_rate": 4.605e-05,
      "loss": 0.0033,
      "step": 11850
    },
    {
      "epoch": 0.6325333333333333,
      "grad_norm": 0.911952555179596,
      "learning_rate": 4.6046666666666674e-05,
      "loss": 0.0036,
      "step": 11860
    },
    {
      "epoch": 0.6330666666666667,
      "grad_norm": 0.22230751812458038,
      "learning_rate": 4.6043333333333334e-05,
      "loss": 0.0037,
      "step": 11870
    },
    {
      "epoch": 0.6336,
      "grad_norm": 0.19204312562942505,
      "learning_rate": 4.604e-05,
      "loss": 0.0026,
      "step": 11880
    },
    {
      "epoch": 0.6341333333333333,
      "grad_norm": 0.1950429230928421,
      "learning_rate": 4.6036666666666666e-05,
      "loss": 0.0039,
      "step": 11890
    },
    {
      "epoch": 0.6346666666666667,
      "grad_norm": 0.3521488606929779,
      "learning_rate": 4.603333333333333e-05,
      "loss": 0.0032,
      "step": 11900
    },
    {
      "epoch": 0.6352,
      "grad_norm": 0.31312769651412964,
      "learning_rate": 4.603e-05,
      "loss": 0.0032,
      "step": 11910
    },
    {
      "epoch": 0.6357333333333334,
      "grad_norm": 0.647944986820221,
      "learning_rate": 4.602666666666667e-05,
      "loss": 0.0032,
      "step": 11920
    },
    {
      "epoch": 0.6362666666666666,
      "grad_norm": 0.600684642791748,
      "learning_rate": 4.602333333333334e-05,
      "loss": 0.003,
      "step": 11930
    },
    {
      "epoch": 0.6368,
      "grad_norm": 0.19394995272159576,
      "learning_rate": 4.602e-05,
      "loss": 0.0029,
      "step": 11940
    },
    {
      "epoch": 0.6373333333333333,
      "grad_norm": 0.5436695218086243,
      "learning_rate": 4.601666666666667e-05,
      "loss": 0.0025,
      "step": 11950
    },
    {
      "epoch": 0.6378666666666667,
      "grad_norm": 0.13015905022621155,
      "learning_rate": 4.6013333333333336e-05,
      "loss": 0.0034,
      "step": 11960
    },
    {
      "epoch": 0.6384,
      "grad_norm": 0.3944017291069031,
      "learning_rate": 4.601e-05,
      "loss": 0.0031,
      "step": 11970
    },
    {
      "epoch": 0.6389333333333334,
      "grad_norm": 0.6085227727890015,
      "learning_rate": 4.600666666666667e-05,
      "loss": 0.0032,
      "step": 11980
    },
    {
      "epoch": 0.6394666666666666,
      "grad_norm": 0.3821263611316681,
      "learning_rate": 4.6003333333333334e-05,
      "loss": 0.004,
      "step": 11990
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.3378139138221741,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.0033,
      "step": 12000
    },
    {
      "epoch": 0.6405333333333333,
      "grad_norm": 0.20252999663352966,
      "learning_rate": 4.599666666666667e-05,
      "loss": 0.0046,
      "step": 12010
    },
    {
      "epoch": 0.6410666666666667,
      "grad_norm": 0.4536338448524475,
      "learning_rate": 4.599333333333334e-05,
      "loss": 0.0025,
      "step": 12020
    },
    {
      "epoch": 0.6416,
      "grad_norm": 0.41489341855049133,
      "learning_rate": 4.599e-05,
      "loss": 0.0034,
      "step": 12030
    },
    {
      "epoch": 0.6421333333333333,
      "grad_norm": 0.4427882432937622,
      "learning_rate": 4.5986666666666665e-05,
      "loss": 0.0029,
      "step": 12040
    },
    {
      "epoch": 0.6426666666666667,
      "grad_norm": 0.10634912550449371,
      "learning_rate": 4.598333333333333e-05,
      "loss": 0.0069,
      "step": 12050
    },
    {
      "epoch": 0.6432,
      "grad_norm": 0.2672288715839386,
      "learning_rate": 4.5980000000000004e-05,
      "loss": 0.0034,
      "step": 12060
    },
    {
      "epoch": 0.6437333333333334,
      "grad_norm": 0.20519717037677765,
      "learning_rate": 4.597666666666667e-05,
      "loss": 0.0039,
      "step": 12070
    },
    {
      "epoch": 0.6442666666666667,
      "grad_norm": 0.32391229271888733,
      "learning_rate": 4.5973333333333336e-05,
      "loss": 0.0031,
      "step": 12080
    },
    {
      "epoch": 0.6448,
      "grad_norm": 0.4526263177394867,
      "learning_rate": 4.597e-05,
      "loss": 0.0033,
      "step": 12090
    },
    {
      "epoch": 0.6453333333333333,
      "grad_norm": 0.15220648050308228,
      "learning_rate": 4.596666666666667e-05,
      "loss": 0.0035,
      "step": 12100
    },
    {
      "epoch": 0.6458666666666667,
      "grad_norm": 0.5813409090042114,
      "learning_rate": 4.5963333333333334e-05,
      "loss": 0.0033,
      "step": 12110
    },
    {
      "epoch": 0.6464,
      "grad_norm": 0.10775167495012283,
      "learning_rate": 4.596e-05,
      "loss": 0.0031,
      "step": 12120
    },
    {
      "epoch": 0.6469333333333334,
      "grad_norm": 0.3705975413322449,
      "learning_rate": 4.595666666666667e-05,
      "loss": 0.0038,
      "step": 12130
    },
    {
      "epoch": 0.6474666666666666,
      "grad_norm": 0.07239927351474762,
      "learning_rate": 4.595333333333334e-05,
      "loss": 0.003,
      "step": 12140
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.4716232120990753,
      "learning_rate": 4.5950000000000006e-05,
      "loss": 0.0039,
      "step": 12150
    },
    {
      "epoch": 0.6485333333333333,
      "grad_norm": 0.41457971930503845,
      "learning_rate": 4.594666666666667e-05,
      "loss": 0.0035,
      "step": 12160
    },
    {
      "epoch": 0.6490666666666667,
      "grad_norm": 0.10344463586807251,
      "learning_rate": 4.594333333333334e-05,
      "loss": 0.0023,
      "step": 12170
    },
    {
      "epoch": 0.6496,
      "grad_norm": 0.3224533796310425,
      "learning_rate": 4.594e-05,
      "loss": 0.0025,
      "step": 12180
    },
    {
      "epoch": 0.6501333333333333,
      "grad_norm": 0.14513644576072693,
      "learning_rate": 4.593666666666666e-05,
      "loss": 0.0045,
      "step": 12190
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 0.4890555441379547,
      "learning_rate": 4.5933333333333336e-05,
      "loss": 0.0029,
      "step": 12200
    },
    {
      "epoch": 0.6512,
      "grad_norm": 0.08333543688058853,
      "learning_rate": 4.593e-05,
      "loss": 0.0031,
      "step": 12210
    },
    {
      "epoch": 0.6517333333333334,
      "grad_norm": 0.3203607201576233,
      "learning_rate": 4.592666666666667e-05,
      "loss": 0.0031,
      "step": 12220
    },
    {
      "epoch": 0.6522666666666667,
      "grad_norm": 0.1272805631160736,
      "learning_rate": 4.5923333333333335e-05,
      "loss": 0.003,
      "step": 12230
    },
    {
      "epoch": 0.6528,
      "grad_norm": 0.13681015372276306,
      "learning_rate": 4.592e-05,
      "loss": 0.003,
      "step": 12240
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.39369454979896545,
      "learning_rate": 4.591666666666667e-05,
      "loss": 0.0032,
      "step": 12250
    },
    {
      "epoch": 0.6538666666666667,
      "grad_norm": 0.3792438805103302,
      "learning_rate": 4.591333333333333e-05,
      "loss": 0.004,
      "step": 12260
    },
    {
      "epoch": 0.6544,
      "grad_norm": 0.046981312334537506,
      "learning_rate": 4.5910000000000006e-05,
      "loss": 0.0029,
      "step": 12270
    },
    {
      "epoch": 0.6549333333333334,
      "grad_norm": 0.43272149562835693,
      "learning_rate": 4.590666666666667e-05,
      "loss": 0.0024,
      "step": 12280
    },
    {
      "epoch": 0.6554666666666666,
      "grad_norm": 0.5681096911430359,
      "learning_rate": 4.590333333333334e-05,
      "loss": 0.0023,
      "step": 12290
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.3432215452194214,
      "learning_rate": 4.5900000000000004e-05,
      "loss": 0.0033,
      "step": 12300
    },
    {
      "epoch": 0.6565333333333333,
      "grad_norm": 0.07319851219654083,
      "learning_rate": 4.589666666666667e-05,
      "loss": 0.0024,
      "step": 12310
    },
    {
      "epoch": 0.6570666666666667,
      "grad_norm": 0.1625121831893921,
      "learning_rate": 4.589333333333334e-05,
      "loss": 0.004,
      "step": 12320
    },
    {
      "epoch": 0.6576,
      "grad_norm": 0.18891389667987823,
      "learning_rate": 4.589e-05,
      "loss": 0.0032,
      "step": 12330
    },
    {
      "epoch": 0.6581333333333333,
      "grad_norm": 0.4727197587490082,
      "learning_rate": 4.588666666666667e-05,
      "loss": 0.0023,
      "step": 12340
    },
    {
      "epoch": 0.6586666666666666,
      "grad_norm": 0.34616053104400635,
      "learning_rate": 4.5883333333333335e-05,
      "loss": 0.0031,
      "step": 12350
    },
    {
      "epoch": 0.6592,
      "grad_norm": 0.4107781648635864,
      "learning_rate": 4.588e-05,
      "loss": 0.003,
      "step": 12360
    },
    {
      "epoch": 0.6597333333333333,
      "grad_norm": 0.5446025729179382,
      "learning_rate": 4.587666666666667e-05,
      "loss": 0.0035,
      "step": 12370
    },
    {
      "epoch": 0.6602666666666667,
      "grad_norm": 0.1607835441827774,
      "learning_rate": 4.5873333333333333e-05,
      "loss": 0.0032,
      "step": 12380
    },
    {
      "epoch": 0.6608,
      "grad_norm": 0.15875081717967987,
      "learning_rate": 4.587e-05,
      "loss": 0.0027,
      "step": 12390
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 0.3795107305049896,
      "learning_rate": 4.5866666666666666e-05,
      "loss": 0.0027,
      "step": 12400
    },
    {
      "epoch": 0.6618666666666667,
      "grad_norm": 0.4390068054199219,
      "learning_rate": 4.586333333333334e-05,
      "loss": 0.0036,
      "step": 12410
    },
    {
      "epoch": 0.6624,
      "grad_norm": 0.4721652567386627,
      "learning_rate": 4.5860000000000005e-05,
      "loss": 0.0038,
      "step": 12420
    },
    {
      "epoch": 0.6629333333333334,
      "grad_norm": 0.4652058482170105,
      "learning_rate": 4.585666666666667e-05,
      "loss": 0.0024,
      "step": 12430
    },
    {
      "epoch": 0.6634666666666666,
      "grad_norm": 0.2779693901538849,
      "learning_rate": 4.585333333333334e-05,
      "loss": 0.0031,
      "step": 12440
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.5073001980781555,
      "learning_rate": 4.585e-05,
      "loss": 0.0041,
      "step": 12450
    },
    {
      "epoch": 0.6645333333333333,
      "grad_norm": 0.23051869869232178,
      "learning_rate": 4.584666666666667e-05,
      "loss": 0.0035,
      "step": 12460
    },
    {
      "epoch": 0.6650666666666667,
      "grad_norm": 0.32689011096954346,
      "learning_rate": 4.5843333333333335e-05,
      "loss": 0.0033,
      "step": 12470
    },
    {
      "epoch": 0.6656,
      "grad_norm": 0.6262940168380737,
      "learning_rate": 4.584e-05,
      "loss": 0.0034,
      "step": 12480
    },
    {
      "epoch": 0.6661333333333334,
      "grad_norm": 0.19957809150218964,
      "learning_rate": 4.583666666666667e-05,
      "loss": 0.0025,
      "step": 12490
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.3785153031349182,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 0.0033,
      "step": 12500
    },
    {
      "epoch": 0.6672,
      "grad_norm": 0.2547285258769989,
      "learning_rate": 4.583e-05,
      "loss": 0.0028,
      "step": 12510
    },
    {
      "epoch": 0.6677333333333333,
      "grad_norm": 0.34037068486213684,
      "learning_rate": 4.5826666666666666e-05,
      "loss": 0.0031,
      "step": 12520
    },
    {
      "epoch": 0.6682666666666667,
      "grad_norm": 0.6395370960235596,
      "learning_rate": 4.582333333333333e-05,
      "loss": 0.003,
      "step": 12530
    },
    {
      "epoch": 0.6688,
      "grad_norm": 0.2644577622413635,
      "learning_rate": 4.5820000000000005e-05,
      "loss": 0.0043,
      "step": 12540
    },
    {
      "epoch": 0.6693333333333333,
      "grad_norm": 0.5774354934692383,
      "learning_rate": 4.581666666666667e-05,
      "loss": 0.0035,
      "step": 12550
    },
    {
      "epoch": 0.6698666666666667,
      "grad_norm": 0.35513943433761597,
      "learning_rate": 4.581333333333334e-05,
      "loss": 0.0034,
      "step": 12560
    },
    {
      "epoch": 0.6704,
      "grad_norm": 0.08492807298898697,
      "learning_rate": 4.5810000000000004e-05,
      "loss": 0.0029,
      "step": 12570
    },
    {
      "epoch": 0.6709333333333334,
      "grad_norm": 0.16588589549064636,
      "learning_rate": 4.580666666666667e-05,
      "loss": 0.004,
      "step": 12580
    },
    {
      "epoch": 0.6714666666666667,
      "grad_norm": 0.1654191017150879,
      "learning_rate": 4.5803333333333336e-05,
      "loss": 0.0039,
      "step": 12590
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.10721736401319504,
      "learning_rate": 4.58e-05,
      "loss": 0.0046,
      "step": 12600
    },
    {
      "epoch": 0.6725333333333333,
      "grad_norm": 0.5934855937957764,
      "learning_rate": 4.579666666666667e-05,
      "loss": 0.0035,
      "step": 12610
    },
    {
      "epoch": 0.6730666666666667,
      "grad_norm": 0.28316566348075867,
      "learning_rate": 4.579333333333334e-05,
      "loss": 0.0023,
      "step": 12620
    },
    {
      "epoch": 0.6736,
      "grad_norm": 0.2000541090965271,
      "learning_rate": 4.579e-05,
      "loss": 0.0029,
      "step": 12630
    },
    {
      "epoch": 0.6741333333333334,
      "grad_norm": 0.6314473152160645,
      "learning_rate": 4.5786666666666666e-05,
      "loss": 0.0018,
      "step": 12640
    },
    {
      "epoch": 0.6746666666666666,
      "grad_norm": 0.43448513746261597,
      "learning_rate": 4.578333333333333e-05,
      "loss": 0.0033,
      "step": 12650
    },
    {
      "epoch": 0.6752,
      "grad_norm": 0.0767899751663208,
      "learning_rate": 4.578e-05,
      "loss": 0.0034,
      "step": 12660
    },
    {
      "epoch": 0.6757333333333333,
      "grad_norm": 0.25373268127441406,
      "learning_rate": 4.5776666666666665e-05,
      "loss": 0.0032,
      "step": 12670
    },
    {
      "epoch": 0.6762666666666667,
      "grad_norm": 0.3194153308868408,
      "learning_rate": 4.577333333333334e-05,
      "loss": 0.0038,
      "step": 12680
    },
    {
      "epoch": 0.6768,
      "grad_norm": 0.16787748038768768,
      "learning_rate": 4.5770000000000004e-05,
      "loss": 0.0036,
      "step": 12690
    },
    {
      "epoch": 0.6773333333333333,
      "grad_norm": 0.16046303510665894,
      "learning_rate": 4.576666666666667e-05,
      "loss": 0.0027,
      "step": 12700
    },
    {
      "epoch": 0.6778666666666666,
      "grad_norm": 0.7877793312072754,
      "learning_rate": 4.5763333333333336e-05,
      "loss": 0.0033,
      "step": 12710
    },
    {
      "epoch": 0.6784,
      "grad_norm": 0.5020349621772766,
      "learning_rate": 4.576e-05,
      "loss": 0.0033,
      "step": 12720
    },
    {
      "epoch": 0.6789333333333334,
      "grad_norm": 0.6418772339820862,
      "learning_rate": 4.575666666666667e-05,
      "loss": 0.003,
      "step": 12730
    },
    {
      "epoch": 0.6794666666666667,
      "grad_norm": 0.36868271231651306,
      "learning_rate": 4.5753333333333335e-05,
      "loss": 0.0031,
      "step": 12740
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.08855456113815308,
      "learning_rate": 4.575e-05,
      "loss": 0.0032,
      "step": 12750
    },
    {
      "epoch": 0.6805333333333333,
      "grad_norm": 0.05302276089787483,
      "learning_rate": 4.5746666666666674e-05,
      "loss": 0.0031,
      "step": 12760
    },
    {
      "epoch": 0.6810666666666667,
      "grad_norm": 0.07976816594600677,
      "learning_rate": 4.574333333333334e-05,
      "loss": 0.0026,
      "step": 12770
    },
    {
      "epoch": 0.6816,
      "grad_norm": 0.31748050451278687,
      "learning_rate": 4.574e-05,
      "loss": 0.0041,
      "step": 12780
    },
    {
      "epoch": 0.6821333333333334,
      "grad_norm": 0.10522699356079102,
      "learning_rate": 4.5736666666666665e-05,
      "loss": 0.0038,
      "step": 12790
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 0.2837797999382019,
      "learning_rate": 4.573333333333333e-05,
      "loss": 0.003,
      "step": 12800
    },
    {
      "epoch": 0.6832,
      "grad_norm": 0.13210876286029816,
      "learning_rate": 4.573e-05,
      "loss": 0.0033,
      "step": 12810
    },
    {
      "epoch": 0.6837333333333333,
      "grad_norm": 0.49809086322784424,
      "learning_rate": 4.572666666666667e-05,
      "loss": 0.0035,
      "step": 12820
    },
    {
      "epoch": 0.6842666666666667,
      "grad_norm": 0.3288451135158539,
      "learning_rate": 4.5723333333333337e-05,
      "loss": 0.0043,
      "step": 12830
    },
    {
      "epoch": 0.6848,
      "grad_norm": 0.6892905235290527,
      "learning_rate": 4.572e-05,
      "loss": 0.0025,
      "step": 12840
    },
    {
      "epoch": 0.6853333333333333,
      "grad_norm": 0.284923791885376,
      "learning_rate": 4.571666666666667e-05,
      "loss": 0.0045,
      "step": 12850
    },
    {
      "epoch": 0.6858666666666666,
      "grad_norm": 0.4226991534233093,
      "learning_rate": 4.5713333333333335e-05,
      "loss": 0.0043,
      "step": 12860
    },
    {
      "epoch": 0.6864,
      "grad_norm": 0.10290176421403885,
      "learning_rate": 4.571e-05,
      "loss": 0.0029,
      "step": 12870
    },
    {
      "epoch": 0.6869333333333333,
      "grad_norm": 0.9092681407928467,
      "learning_rate": 4.570666666666667e-05,
      "loss": 0.003,
      "step": 12880
    },
    {
      "epoch": 0.6874666666666667,
      "grad_norm": 0.24807746708393097,
      "learning_rate": 4.570333333333334e-05,
      "loss": 0.0042,
      "step": 12890
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.6863496899604797,
      "learning_rate": 4.5700000000000006e-05,
      "loss": 0.0025,
      "step": 12900
    },
    {
      "epoch": 0.6885333333333333,
      "grad_norm": 0.22849731147289276,
      "learning_rate": 4.569666666666667e-05,
      "loss": 0.0043,
      "step": 12910
    },
    {
      "epoch": 0.6890666666666667,
      "grad_norm": 0.10865671187639236,
      "learning_rate": 4.569333333333334e-05,
      "loss": 0.0022,
      "step": 12920
    },
    {
      "epoch": 0.6896,
      "grad_norm": 0.315403550863266,
      "learning_rate": 4.569e-05,
      "loss": 0.0035,
      "step": 12930
    },
    {
      "epoch": 0.6901333333333334,
      "grad_norm": 0.2790036201477051,
      "learning_rate": 4.5686666666666664e-05,
      "loss": 0.0031,
      "step": 12940
    },
    {
      "epoch": 0.6906666666666667,
      "grad_norm": 0.07699155807495117,
      "learning_rate": 4.568333333333333e-05,
      "loss": 0.0017,
      "step": 12950
    },
    {
      "epoch": 0.6912,
      "grad_norm": 0.28380510210990906,
      "learning_rate": 4.568e-05,
      "loss": 0.004,
      "step": 12960
    },
    {
      "epoch": 0.6917333333333333,
      "grad_norm": 0.3802500367164612,
      "learning_rate": 4.567666666666667e-05,
      "loss": 0.0034,
      "step": 12970
    },
    {
      "epoch": 0.6922666666666667,
      "grad_norm": 0.13836626708507538,
      "learning_rate": 4.5673333333333335e-05,
      "loss": 0.0024,
      "step": 12980
    },
    {
      "epoch": 0.6928,
      "grad_norm": 0.2520383298397064,
      "learning_rate": 4.567e-05,
      "loss": 0.0034,
      "step": 12990
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.5668237209320068,
      "learning_rate": 4.566666666666667e-05,
      "loss": 0.0039,
      "step": 13000
    },
    {
      "epoch": 0.6938666666666666,
      "grad_norm": 0.5971145629882812,
      "learning_rate": 4.5663333333333334e-05,
      "loss": 0.0035,
      "step": 13010
    },
    {
      "epoch": 0.6944,
      "grad_norm": 0.5035805702209473,
      "learning_rate": 4.566e-05,
      "loss": 0.0027,
      "step": 13020
    },
    {
      "epoch": 0.6949333333333333,
      "grad_norm": 0.536020040512085,
      "learning_rate": 4.565666666666667e-05,
      "loss": 0.0026,
      "step": 13030
    },
    {
      "epoch": 0.6954666666666667,
      "grad_norm": 0.19930212199687958,
      "learning_rate": 4.565333333333334e-05,
      "loss": 0.0035,
      "step": 13040
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.18199610710144043,
      "learning_rate": 4.5650000000000005e-05,
      "loss": 0.003,
      "step": 13050
    },
    {
      "epoch": 0.6965333333333333,
      "grad_norm": 0.22636184096336365,
      "learning_rate": 4.564666666666667e-05,
      "loss": 0.0037,
      "step": 13060
    },
    {
      "epoch": 0.6970666666666666,
      "grad_norm": 0.14836636185646057,
      "learning_rate": 4.564333333333334e-05,
      "loss": 0.0033,
      "step": 13070
    },
    {
      "epoch": 0.6976,
      "grad_norm": 0.507106602191925,
      "learning_rate": 4.564e-05,
      "loss": 0.0032,
      "step": 13080
    },
    {
      "epoch": 0.6981333333333334,
      "grad_norm": 0.7946434020996094,
      "learning_rate": 4.563666666666667e-05,
      "loss": 0.0036,
      "step": 13090
    },
    {
      "epoch": 0.6986666666666667,
      "grad_norm": 0.2252139002084732,
      "learning_rate": 4.5633333333333336e-05,
      "loss": 0.0041,
      "step": 13100
    },
    {
      "epoch": 0.6992,
      "grad_norm": 0.42268893122673035,
      "learning_rate": 4.563e-05,
      "loss": 0.0029,
      "step": 13110
    },
    {
      "epoch": 0.6997333333333333,
      "grad_norm": 0.20996078848838806,
      "learning_rate": 4.562666666666667e-05,
      "loss": 0.0032,
      "step": 13120
    },
    {
      "epoch": 0.7002666666666667,
      "grad_norm": 0.12160264700651169,
      "learning_rate": 4.5623333333333334e-05,
      "loss": 0.0018,
      "step": 13130
    },
    {
      "epoch": 0.7008,
      "grad_norm": 0.4139308035373688,
      "learning_rate": 4.562e-05,
      "loss": 0.0033,
      "step": 13140
    },
    {
      "epoch": 0.7013333333333334,
      "grad_norm": 0.40539997816085815,
      "learning_rate": 4.5616666666666666e-05,
      "loss": 0.0034,
      "step": 13150
    },
    {
      "epoch": 0.7018666666666666,
      "grad_norm": 0.5700068473815918,
      "learning_rate": 4.561333333333333e-05,
      "loss": 0.0029,
      "step": 13160
    },
    {
      "epoch": 0.7024,
      "grad_norm": 0.07145141065120697,
      "learning_rate": 4.5610000000000005e-05,
      "loss": 0.0038,
      "step": 13170
    },
    {
      "epoch": 0.7029333333333333,
      "grad_norm": 0.44365859031677246,
      "learning_rate": 4.560666666666667e-05,
      "loss": 0.0035,
      "step": 13180
    },
    {
      "epoch": 0.7034666666666667,
      "grad_norm": 0.11826878786087036,
      "learning_rate": 4.560333333333334e-05,
      "loss": 0.0045,
      "step": 13190
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.2807934880256653,
      "learning_rate": 4.5600000000000004e-05,
      "loss": 0.0038,
      "step": 13200
    },
    {
      "epoch": 0.7045333333333333,
      "grad_norm": 0.6551566123962402,
      "learning_rate": 4.559666666666667e-05,
      "loss": 0.0029,
      "step": 13210
    },
    {
      "epoch": 0.7050666666666666,
      "grad_norm": 0.5559682250022888,
      "learning_rate": 4.5593333333333336e-05,
      "loss": 0.0025,
      "step": 13220
    },
    {
      "epoch": 0.7056,
      "grad_norm": 0.15877290070056915,
      "learning_rate": 4.559e-05,
      "loss": 0.0032,
      "step": 13230
    },
    {
      "epoch": 0.7061333333333333,
      "grad_norm": 0.3167770504951477,
      "learning_rate": 4.558666666666667e-05,
      "loss": 0.0018,
      "step": 13240
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.04182402044534683,
      "learning_rate": 4.5583333333333335e-05,
      "loss": 0.0024,
      "step": 13250
    },
    {
      "epoch": 0.7072,
      "grad_norm": 0.49522048234939575,
      "learning_rate": 4.558e-05,
      "loss": 0.0037,
      "step": 13260
    },
    {
      "epoch": 0.7077333333333333,
      "grad_norm": 0.12631100416183472,
      "learning_rate": 4.557666666666667e-05,
      "loss": 0.0028,
      "step": 13270
    },
    {
      "epoch": 0.7082666666666667,
      "grad_norm": 0.3759636878967285,
      "learning_rate": 4.557333333333333e-05,
      "loss": 0.0038,
      "step": 13280
    },
    {
      "epoch": 0.7088,
      "grad_norm": 0.21983282268047333,
      "learning_rate": 4.557e-05,
      "loss": 0.0033,
      "step": 13290
    },
    {
      "epoch": 0.7093333333333334,
      "grad_norm": 0.2898027002811432,
      "learning_rate": 4.556666666666667e-05,
      "loss": 0.0027,
      "step": 13300
    },
    {
      "epoch": 0.7098666666666666,
      "grad_norm": 0.1728016883134842,
      "learning_rate": 4.556333333333334e-05,
      "loss": 0.0029,
      "step": 13310
    },
    {
      "epoch": 0.7104,
      "grad_norm": 0.32429391145706177,
      "learning_rate": 4.5560000000000004e-05,
      "loss": 0.0038,
      "step": 13320
    },
    {
      "epoch": 0.7109333333333333,
      "grad_norm": 0.28424298763275146,
      "learning_rate": 4.555666666666667e-05,
      "loss": 0.0027,
      "step": 13330
    },
    {
      "epoch": 0.7114666666666667,
      "grad_norm": 0.22317419946193695,
      "learning_rate": 4.5553333333333337e-05,
      "loss": 0.0034,
      "step": 13340
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.2230515033006668,
      "learning_rate": 4.555e-05,
      "loss": 0.0039,
      "step": 13350
    },
    {
      "epoch": 0.7125333333333334,
      "grad_norm": 0.6234027743339539,
      "learning_rate": 4.554666666666667e-05,
      "loss": 0.0033,
      "step": 13360
    },
    {
      "epoch": 0.7130666666666666,
      "grad_norm": 0.12551230192184448,
      "learning_rate": 4.5543333333333335e-05,
      "loss": 0.0034,
      "step": 13370
    },
    {
      "epoch": 0.7136,
      "grad_norm": 0.3213205635547638,
      "learning_rate": 4.554000000000001e-05,
      "loss": 0.0044,
      "step": 13380
    },
    {
      "epoch": 0.7141333333333333,
      "grad_norm": 0.17000900208950043,
      "learning_rate": 4.553666666666667e-05,
      "loss": 0.0042,
      "step": 13390
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 0.11560969799757004,
      "learning_rate": 4.553333333333333e-05,
      "loss": 0.0026,
      "step": 13400
    },
    {
      "epoch": 0.7152,
      "grad_norm": 0.34561246633529663,
      "learning_rate": 4.553e-05,
      "loss": 0.0042,
      "step": 13410
    },
    {
      "epoch": 0.7157333333333333,
      "grad_norm": 0.22269220650196075,
      "learning_rate": 4.5526666666666666e-05,
      "loss": 0.0037,
      "step": 13420
    },
    {
      "epoch": 0.7162666666666667,
      "grad_norm": 0.49020808935165405,
      "learning_rate": 4.552333333333333e-05,
      "loss": 0.0028,
      "step": 13430
    },
    {
      "epoch": 0.7168,
      "grad_norm": 0.28563016653060913,
      "learning_rate": 4.5520000000000005e-05,
      "loss": 0.0035,
      "step": 13440
    },
    {
      "epoch": 0.7173333333333334,
      "grad_norm": 0.1430394947528839,
      "learning_rate": 4.551666666666667e-05,
      "loss": 0.0033,
      "step": 13450
    },
    {
      "epoch": 0.7178666666666667,
      "grad_norm": 0.3737770915031433,
      "learning_rate": 4.551333333333334e-05,
      "loss": 0.0022,
      "step": 13460
    },
    {
      "epoch": 0.7184,
      "grad_norm": 0.49931445717811584,
      "learning_rate": 4.551e-05,
      "loss": 0.0021,
      "step": 13470
    },
    {
      "epoch": 0.7189333333333333,
      "grad_norm": 0.19941097497940063,
      "learning_rate": 4.550666666666667e-05,
      "loss": 0.0036,
      "step": 13480
    },
    {
      "epoch": 0.7194666666666667,
      "grad_norm": 0.26061081886291504,
      "learning_rate": 4.5503333333333335e-05,
      "loss": 0.0037,
      "step": 13490
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.19345852732658386,
      "learning_rate": 4.55e-05,
      "loss": 0.0029,
      "step": 13500
    },
    {
      "epoch": 0.7205333333333334,
      "grad_norm": 0.22107698023319244,
      "learning_rate": 4.549666666666667e-05,
      "loss": 0.0033,
      "step": 13510
    },
    {
      "epoch": 0.7210666666666666,
      "grad_norm": 0.2539893090724945,
      "learning_rate": 4.549333333333334e-05,
      "loss": 0.0047,
      "step": 13520
    },
    {
      "epoch": 0.7216,
      "grad_norm": 0.19078142940998077,
      "learning_rate": 4.549000000000001e-05,
      "loss": 0.0022,
      "step": 13530
    },
    {
      "epoch": 0.7221333333333333,
      "grad_norm": 0.4240736663341522,
      "learning_rate": 4.5486666666666666e-05,
      "loss": 0.003,
      "step": 13540
    },
    {
      "epoch": 0.7226666666666667,
      "grad_norm": 0.2265671342611313,
      "learning_rate": 4.548333333333333e-05,
      "loss": 0.0026,
      "step": 13550
    },
    {
      "epoch": 0.7232,
      "grad_norm": 0.19058085978031158,
      "learning_rate": 4.548e-05,
      "loss": 0.0031,
      "step": 13560
    },
    {
      "epoch": 0.7237333333333333,
      "grad_norm": 0.03786725923418999,
      "learning_rate": 4.5476666666666664e-05,
      "loss": 0.0033,
      "step": 13570
    },
    {
      "epoch": 0.7242666666666666,
      "grad_norm": 0.21775318682193756,
      "learning_rate": 4.547333333333334e-05,
      "loss": 0.0032,
      "step": 13580
    },
    {
      "epoch": 0.7248,
      "grad_norm": 0.08257025480270386,
      "learning_rate": 4.5470000000000003e-05,
      "loss": 0.0043,
      "step": 13590
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 0.1306721419095993,
      "learning_rate": 4.546666666666667e-05,
      "loss": 0.0023,
      "step": 13600
    },
    {
      "epoch": 0.7258666666666667,
      "grad_norm": 0.28215116262435913,
      "learning_rate": 4.5463333333333336e-05,
      "loss": 0.0032,
      "step": 13610
    },
    {
      "epoch": 0.7264,
      "grad_norm": 0.06251756846904755,
      "learning_rate": 4.546e-05,
      "loss": 0.0037,
      "step": 13620
    },
    {
      "epoch": 0.7269333333333333,
      "grad_norm": 0.2006615400314331,
      "learning_rate": 4.545666666666667e-05,
      "loss": 0.003,
      "step": 13630
    },
    {
      "epoch": 0.7274666666666667,
      "grad_norm": 0.669889509677887,
      "learning_rate": 4.5453333333333334e-05,
      "loss": 0.0021,
      "step": 13640
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.40855565667152405,
      "learning_rate": 4.545000000000001e-05,
      "loss": 0.003,
      "step": 13650
    },
    {
      "epoch": 0.7285333333333334,
      "grad_norm": 0.295136421918869,
      "learning_rate": 4.544666666666667e-05,
      "loss": 0.0028,
      "step": 13660
    },
    {
      "epoch": 0.7290666666666666,
      "grad_norm": 0.1941126137971878,
      "learning_rate": 4.544333333333334e-05,
      "loss": 0.0032,
      "step": 13670
    },
    {
      "epoch": 0.7296,
      "grad_norm": 0.2867310643196106,
      "learning_rate": 4.5440000000000005e-05,
      "loss": 0.0044,
      "step": 13680
    },
    {
      "epoch": 0.7301333333333333,
      "grad_norm": 0.3789399266242981,
      "learning_rate": 4.5436666666666665e-05,
      "loss": 0.0031,
      "step": 13690
    },
    {
      "epoch": 0.7306666666666667,
      "grad_norm": 0.5930031538009644,
      "learning_rate": 4.543333333333333e-05,
      "loss": 0.0031,
      "step": 13700
    },
    {
      "epoch": 0.7312,
      "grad_norm": 0.8820328712463379,
      "learning_rate": 4.543e-05,
      "loss": 0.002,
      "step": 13710
    },
    {
      "epoch": 0.7317333333333333,
      "grad_norm": 0.08328195661306381,
      "learning_rate": 4.542666666666667e-05,
      "loss": 0.0026,
      "step": 13720
    },
    {
      "epoch": 0.7322666666666666,
      "grad_norm": 0.5280849933624268,
      "learning_rate": 4.5423333333333336e-05,
      "loss": 0.0018,
      "step": 13730
    },
    {
      "epoch": 0.7328,
      "grad_norm": 0.22058337926864624,
      "learning_rate": 4.542e-05,
      "loss": 0.0029,
      "step": 13740
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.2005082368850708,
      "learning_rate": 4.541666666666667e-05,
      "loss": 0.0034,
      "step": 13750
    },
    {
      "epoch": 0.7338666666666667,
      "grad_norm": 0.42418172955513,
      "learning_rate": 4.5413333333333334e-05,
      "loss": 0.0033,
      "step": 13760
    },
    {
      "epoch": 0.7344,
      "grad_norm": 0.21272093057632446,
      "learning_rate": 4.541e-05,
      "loss": 0.0024,
      "step": 13770
    },
    {
      "epoch": 0.7349333333333333,
      "grad_norm": 0.31576699018478394,
      "learning_rate": 4.540666666666667e-05,
      "loss": 0.002,
      "step": 13780
    },
    {
      "epoch": 0.7354666666666667,
      "grad_norm": 0.13392668962478638,
      "learning_rate": 4.540333333333334e-05,
      "loss": 0.0033,
      "step": 13790
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.23155447840690613,
      "learning_rate": 4.5400000000000006e-05,
      "loss": 0.0028,
      "step": 13800
    },
    {
      "epoch": 0.7365333333333334,
      "grad_norm": 0.5603584051132202,
      "learning_rate": 4.539666666666667e-05,
      "loss": 0.0036,
      "step": 13810
    },
    {
      "epoch": 0.7370666666666666,
      "grad_norm": 0.08569227159023285,
      "learning_rate": 4.539333333333334e-05,
      "loss": 0.0033,
      "step": 13820
    },
    {
      "epoch": 0.7376,
      "grad_norm": 0.06102145090699196,
      "learning_rate": 4.5390000000000004e-05,
      "loss": 0.0025,
      "step": 13830
    },
    {
      "epoch": 0.7381333333333333,
      "grad_norm": 0.4949948489665985,
      "learning_rate": 4.5386666666666664e-05,
      "loss": 0.0026,
      "step": 13840
    },
    {
      "epoch": 0.7386666666666667,
      "grad_norm": 0.7788864374160767,
      "learning_rate": 4.5383333333333336e-05,
      "loss": 0.0025,
      "step": 13850
    },
    {
      "epoch": 0.7392,
      "grad_norm": 0.2050676792860031,
      "learning_rate": 4.538e-05,
      "loss": 0.0028,
      "step": 13860
    },
    {
      "epoch": 0.7397333333333334,
      "grad_norm": 0.45137327909469604,
      "learning_rate": 4.537666666666667e-05,
      "loss": 0.0025,
      "step": 13870
    },
    {
      "epoch": 0.7402666666666666,
      "grad_norm": 0.5347622036933899,
      "learning_rate": 4.5373333333333335e-05,
      "loss": 0.0025,
      "step": 13880
    },
    {
      "epoch": 0.7408,
      "grad_norm": 0.32150915265083313,
      "learning_rate": 4.537e-05,
      "loss": 0.0023,
      "step": 13890
    },
    {
      "epoch": 0.7413333333333333,
      "grad_norm": 0.1953504979610443,
      "learning_rate": 4.536666666666667e-05,
      "loss": 0.0038,
      "step": 13900
    },
    {
      "epoch": 0.7418666666666667,
      "grad_norm": 0.35458555817604065,
      "learning_rate": 4.536333333333333e-05,
      "loss": 0.004,
      "step": 13910
    },
    {
      "epoch": 0.7424,
      "grad_norm": 0.5071206092834473,
      "learning_rate": 4.536e-05,
      "loss": 0.0027,
      "step": 13920
    },
    {
      "epoch": 0.7429333333333333,
      "grad_norm": 0.31041648983955383,
      "learning_rate": 4.535666666666667e-05,
      "loss": 0.003,
      "step": 13930
    },
    {
      "epoch": 0.7434666666666667,
      "grad_norm": 0.25292375683784485,
      "learning_rate": 4.535333333333334e-05,
      "loss": 0.0027,
      "step": 13940
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.09884993731975555,
      "learning_rate": 4.5350000000000005e-05,
      "loss": 0.0026,
      "step": 13950
    },
    {
      "epoch": 0.7445333333333334,
      "grad_norm": 0.03753456473350525,
      "learning_rate": 4.534666666666667e-05,
      "loss": 0.0026,
      "step": 13960
    },
    {
      "epoch": 0.7450666666666667,
      "grad_norm": 0.4799598455429077,
      "learning_rate": 4.534333333333334e-05,
      "loss": 0.0032,
      "step": 13970
    },
    {
      "epoch": 0.7456,
      "grad_norm": 0.5576474070549011,
      "learning_rate": 4.534e-05,
      "loss": 0.0038,
      "step": 13980
    },
    {
      "epoch": 0.7461333333333333,
      "grad_norm": 0.5359472036361694,
      "learning_rate": 4.533666666666667e-05,
      "loss": 0.0039,
      "step": 13990
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.30735185742378235,
      "learning_rate": 4.5333333333333335e-05,
      "loss": 0.0037,
      "step": 14000
    },
    {
      "epoch": 0.7472,
      "grad_norm": 0.5632789134979248,
      "learning_rate": 4.533e-05,
      "loss": 0.0029,
      "step": 14010
    },
    {
      "epoch": 0.7477333333333334,
      "grad_norm": 0.5632405877113342,
      "learning_rate": 4.532666666666667e-05,
      "loss": 0.0034,
      "step": 14020
    },
    {
      "epoch": 0.7482666666666666,
      "grad_norm": 0.6036527156829834,
      "learning_rate": 4.5323333333333334e-05,
      "loss": 0.0029,
      "step": 14030
    },
    {
      "epoch": 0.7488,
      "grad_norm": 0.08593053370714188,
      "learning_rate": 4.532e-05,
      "loss": 0.0031,
      "step": 14040
    },
    {
      "epoch": 0.7493333333333333,
      "grad_norm": 0.10779257863759995,
      "learning_rate": 4.5316666666666666e-05,
      "loss": 0.0037,
      "step": 14050
    },
    {
      "epoch": 0.7498666666666667,
      "grad_norm": 0.248526930809021,
      "learning_rate": 4.531333333333333e-05,
      "loss": 0.0037,
      "step": 14060
    },
    {
      "epoch": 0.7504,
      "grad_norm": 0.28419774770736694,
      "learning_rate": 4.5310000000000005e-05,
      "loss": 0.0018,
      "step": 14070
    },
    {
      "epoch": 0.7509333333333333,
      "grad_norm": 0.04229147732257843,
      "learning_rate": 4.530666666666667e-05,
      "loss": 0.0028,
      "step": 14080
    },
    {
      "epoch": 0.7514666666666666,
      "grad_norm": 0.6315987706184387,
      "learning_rate": 4.530333333333334e-05,
      "loss": 0.0031,
      "step": 14090
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.2518647611141205,
      "learning_rate": 4.53e-05,
      "loss": 0.0033,
      "step": 14100
    },
    {
      "epoch": 0.7525333333333334,
      "grad_norm": 0.28931599855422974,
      "learning_rate": 4.529666666666667e-05,
      "loss": 0.0047,
      "step": 14110
    },
    {
      "epoch": 0.7530666666666667,
      "grad_norm": 0.3498842716217041,
      "learning_rate": 4.5293333333333336e-05,
      "loss": 0.0042,
      "step": 14120
    },
    {
      "epoch": 0.7536,
      "grad_norm": 0.3909144997596741,
      "learning_rate": 4.529e-05,
      "loss": 0.0047,
      "step": 14130
    },
    {
      "epoch": 0.7541333333333333,
      "grad_norm": 0.3080044388771057,
      "learning_rate": 4.528666666666667e-05,
      "loss": 0.0024,
      "step": 14140
    },
    {
      "epoch": 0.7546666666666667,
      "grad_norm": 0.12780173122882843,
      "learning_rate": 4.5283333333333334e-05,
      "loss": 0.0031,
      "step": 14150
    },
    {
      "epoch": 0.7552,
      "grad_norm": 0.184600830078125,
      "learning_rate": 4.528e-05,
      "loss": 0.0028,
      "step": 14160
    },
    {
      "epoch": 0.7557333333333334,
      "grad_norm": 0.19259028136730194,
      "learning_rate": 4.5276666666666666e-05,
      "loss": 0.0029,
      "step": 14170
    },
    {
      "epoch": 0.7562666666666666,
      "grad_norm": 0.055422671139240265,
      "learning_rate": 4.527333333333333e-05,
      "loss": 0.0021,
      "step": 14180
    },
    {
      "epoch": 0.7568,
      "grad_norm": 0.39510953426361084,
      "learning_rate": 4.527e-05,
      "loss": 0.0037,
      "step": 14190
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 0.5023471713066101,
      "learning_rate": 4.526666666666667e-05,
      "loss": 0.0047,
      "step": 14200
    },
    {
      "epoch": 0.7578666666666667,
      "grad_norm": 0.6497491598129272,
      "learning_rate": 4.526333333333334e-05,
      "loss": 0.0041,
      "step": 14210
    },
    {
      "epoch": 0.7584,
      "grad_norm": 0.2870180904865265,
      "learning_rate": 4.5260000000000004e-05,
      "loss": 0.0024,
      "step": 14220
    },
    {
      "epoch": 0.7589333333333333,
      "grad_norm": 0.41095343232154846,
      "learning_rate": 4.525666666666667e-05,
      "loss": 0.0033,
      "step": 14230
    },
    {
      "epoch": 0.7594666666666666,
      "grad_norm": 0.223765566945076,
      "learning_rate": 4.5253333333333336e-05,
      "loss": 0.0022,
      "step": 14240
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.24965980648994446,
      "learning_rate": 4.525e-05,
      "loss": 0.0039,
      "step": 14250
    },
    {
      "epoch": 0.7605333333333333,
      "grad_norm": 0.39697229862213135,
      "learning_rate": 4.524666666666667e-05,
      "loss": 0.003,
      "step": 14260
    },
    {
      "epoch": 0.7610666666666667,
      "grad_norm": 0.03543340042233467,
      "learning_rate": 4.5243333333333334e-05,
      "loss": 0.0022,
      "step": 14270
    },
    {
      "epoch": 0.7616,
      "grad_norm": 0.05883244425058365,
      "learning_rate": 4.524000000000001e-05,
      "loss": 0.0025,
      "step": 14280
    },
    {
      "epoch": 0.7621333333333333,
      "grad_norm": 0.1758200228214264,
      "learning_rate": 4.523666666666667e-05,
      "loss": 0.0022,
      "step": 14290
    },
    {
      "epoch": 0.7626666666666667,
      "grad_norm": 0.09845621883869171,
      "learning_rate": 4.523333333333333e-05,
      "loss": 0.0034,
      "step": 14300
    },
    {
      "epoch": 0.7632,
      "grad_norm": 0.6214565634727478,
      "learning_rate": 4.523e-05,
      "loss": 0.0032,
      "step": 14310
    },
    {
      "epoch": 0.7637333333333334,
      "grad_norm": 0.27136412262916565,
      "learning_rate": 4.5226666666666665e-05,
      "loss": 0.0033,
      "step": 14320
    },
    {
      "epoch": 0.7642666666666666,
      "grad_norm": 0.6868230104446411,
      "learning_rate": 4.522333333333333e-05,
      "loss": 0.003,
      "step": 14330
    },
    {
      "epoch": 0.7648,
      "grad_norm": 0.1795167326927185,
      "learning_rate": 4.5220000000000004e-05,
      "loss": 0.0033,
      "step": 14340
    },
    {
      "epoch": 0.7653333333333333,
      "grad_norm": 0.1321561336517334,
      "learning_rate": 4.521666666666667e-05,
      "loss": 0.0044,
      "step": 14350
    },
    {
      "epoch": 0.7658666666666667,
      "grad_norm": 0.15311363339424133,
      "learning_rate": 4.5213333333333336e-05,
      "loss": 0.0036,
      "step": 14360
    },
    {
      "epoch": 0.7664,
      "grad_norm": 0.31513258814811707,
      "learning_rate": 4.521e-05,
      "loss": 0.0054,
      "step": 14370
    },
    {
      "epoch": 0.7669333333333334,
      "grad_norm": 0.10224351286888123,
      "learning_rate": 4.520666666666667e-05,
      "loss": 0.0034,
      "step": 14380
    },
    {
      "epoch": 0.7674666666666666,
      "grad_norm": 0.410838782787323,
      "learning_rate": 4.5203333333333335e-05,
      "loss": 0.003,
      "step": 14390
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.5100261569023132,
      "learning_rate": 4.52e-05,
      "loss": 0.0024,
      "step": 14400
    },
    {
      "epoch": 0.7685333333333333,
      "grad_norm": 0.21937909722328186,
      "learning_rate": 4.5196666666666674e-05,
      "loss": 0.0041,
      "step": 14410
    },
    {
      "epoch": 0.7690666666666667,
      "grad_norm": 0.49845391511917114,
      "learning_rate": 4.519333333333334e-05,
      "loss": 0.0035,
      "step": 14420
    },
    {
      "epoch": 0.7696,
      "grad_norm": 0.2509442865848541,
      "learning_rate": 4.5190000000000006e-05,
      "loss": 0.0031,
      "step": 14430
    },
    {
      "epoch": 0.7701333333333333,
      "grad_norm": 0.23710010945796967,
      "learning_rate": 4.518666666666667e-05,
      "loss": 0.0021,
      "step": 14440
    },
    {
      "epoch": 0.7706666666666667,
      "grad_norm": 0.361966073513031,
      "learning_rate": 4.518333333333333e-05,
      "loss": 0.0029,
      "step": 14450
    },
    {
      "epoch": 0.7712,
      "grad_norm": 0.18741895258426666,
      "learning_rate": 4.518e-05,
      "loss": 0.0026,
      "step": 14460
    },
    {
      "epoch": 0.7717333333333334,
      "grad_norm": 0.26122158765792847,
      "learning_rate": 4.5176666666666664e-05,
      "loss": 0.0031,
      "step": 14470
    },
    {
      "epoch": 0.7722666666666667,
      "grad_norm": 0.143912672996521,
      "learning_rate": 4.517333333333334e-05,
      "loss": 0.0022,
      "step": 14480
    },
    {
      "epoch": 0.7728,
      "grad_norm": 0.06099250912666321,
      "learning_rate": 4.517e-05,
      "loss": 0.0028,
      "step": 14490
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.1043107807636261,
      "learning_rate": 4.516666666666667e-05,
      "loss": 0.0031,
      "step": 14500
    },
    {
      "epoch": 0.7738666666666667,
      "grad_norm": 0.3126104176044464,
      "learning_rate": 4.5163333333333335e-05,
      "loss": 0.004,
      "step": 14510
    },
    {
      "epoch": 0.7744,
      "grad_norm": 0.1896328330039978,
      "learning_rate": 4.516e-05,
      "loss": 0.0022,
      "step": 14520
    },
    {
      "epoch": 0.7749333333333334,
      "grad_norm": 0.2955605983734131,
      "learning_rate": 4.515666666666667e-05,
      "loss": 0.0039,
      "step": 14530
    },
    {
      "epoch": 0.7754666666666666,
      "grad_norm": 0.060429349541664124,
      "learning_rate": 4.5153333333333334e-05,
      "loss": 0.0042,
      "step": 14540
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.15581919252872467,
      "learning_rate": 4.5150000000000006e-05,
      "loss": 0.004,
      "step": 14550
    },
    {
      "epoch": 0.7765333333333333,
      "grad_norm": 0.04285253584384918,
      "learning_rate": 4.514666666666667e-05,
      "loss": 0.004,
      "step": 14560
    },
    {
      "epoch": 0.7770666666666667,
      "grad_norm": 0.0906505361199379,
      "learning_rate": 4.514333333333334e-05,
      "loss": 0.0038,
      "step": 14570
    },
    {
      "epoch": 0.7776,
      "grad_norm": 0.20928855240345,
      "learning_rate": 4.5140000000000005e-05,
      "loss": 0.0032,
      "step": 14580
    },
    {
      "epoch": 0.7781333333333333,
      "grad_norm": 0.07944244891405106,
      "learning_rate": 4.513666666666667e-05,
      "loss": 0.0035,
      "step": 14590
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 0.8956117630004883,
      "learning_rate": 4.513333333333333e-05,
      "loss": 0.003,
      "step": 14600
    },
    {
      "epoch": 0.7792,
      "grad_norm": 0.6304009556770325,
      "learning_rate": 4.513e-05,
      "loss": 0.0029,
      "step": 14610
    },
    {
      "epoch": 0.7797333333333333,
      "grad_norm": 0.2762249708175659,
      "learning_rate": 4.512666666666667e-05,
      "loss": 0.0041,
      "step": 14620
    },
    {
      "epoch": 0.7802666666666667,
      "grad_norm": 0.15261590480804443,
      "learning_rate": 4.5123333333333336e-05,
      "loss": 0.0045,
      "step": 14630
    },
    {
      "epoch": 0.7808,
      "grad_norm": 0.07719315588474274,
      "learning_rate": 4.512e-05,
      "loss": 0.0026,
      "step": 14640
    },
    {
      "epoch": 0.7813333333333333,
      "grad_norm": 0.24026885628700256,
      "learning_rate": 4.511666666666667e-05,
      "loss": 0.0029,
      "step": 14650
    },
    {
      "epoch": 0.7818666666666667,
      "grad_norm": 0.5751348733901978,
      "learning_rate": 4.5113333333333334e-05,
      "loss": 0.0022,
      "step": 14660
    },
    {
      "epoch": 0.7824,
      "grad_norm": 0.3536686897277832,
      "learning_rate": 4.511e-05,
      "loss": 0.0018,
      "step": 14670
    },
    {
      "epoch": 0.7829333333333334,
      "grad_norm": 0.5046758651733398,
      "learning_rate": 4.5106666666666666e-05,
      "loss": 0.0035,
      "step": 14680
    },
    {
      "epoch": 0.7834666666666666,
      "grad_norm": 0.19216008484363556,
      "learning_rate": 4.510333333333334e-05,
      "loss": 0.0036,
      "step": 14690
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.39537280797958374,
      "learning_rate": 4.5100000000000005e-05,
      "loss": 0.0028,
      "step": 14700
    },
    {
      "epoch": 0.7845333333333333,
      "grad_norm": 0.31333690881729126,
      "learning_rate": 4.509666666666667e-05,
      "loss": 0.0035,
      "step": 14710
    },
    {
      "epoch": 0.7850666666666667,
      "grad_norm": 0.2798629701137543,
      "learning_rate": 4.509333333333334e-05,
      "loss": 0.0041,
      "step": 14720
    },
    {
      "epoch": 0.7856,
      "grad_norm": 0.13638491928577423,
      "learning_rate": 4.5090000000000004e-05,
      "loss": 0.0029,
      "step": 14730
    },
    {
      "epoch": 0.7861333333333334,
      "grad_norm": 0.20370449125766754,
      "learning_rate": 4.508666666666667e-05,
      "loss": 0.0029,
      "step": 14740
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.1589658111333847,
      "learning_rate": 4.5083333333333336e-05,
      "loss": 0.004,
      "step": 14750
    },
    {
      "epoch": 0.7872,
      "grad_norm": 0.16006919741630554,
      "learning_rate": 4.508e-05,
      "loss": 0.0038,
      "step": 14760
    },
    {
      "epoch": 0.7877333333333333,
      "grad_norm": 0.13052771985530853,
      "learning_rate": 4.507666666666667e-05,
      "loss": 0.0038,
      "step": 14770
    },
    {
      "epoch": 0.7882666666666667,
      "grad_norm": 0.25698909163475037,
      "learning_rate": 4.5073333333333334e-05,
      "loss": 0.0048,
      "step": 14780
    },
    {
      "epoch": 0.7888,
      "grad_norm": 0.18561935424804688,
      "learning_rate": 4.507e-05,
      "loss": 0.0035,
      "step": 14790
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 0.051991187036037445,
      "learning_rate": 4.5066666666666667e-05,
      "loss": 0.0027,
      "step": 14800
    },
    {
      "epoch": 0.7898666666666667,
      "grad_norm": 0.720360517501831,
      "learning_rate": 4.506333333333333e-05,
      "loss": 0.0033,
      "step": 14810
    },
    {
      "epoch": 0.7904,
      "grad_norm": 0.6541054248809814,
      "learning_rate": 4.506e-05,
      "loss": 0.0027,
      "step": 14820
    },
    {
      "epoch": 0.7909333333333334,
      "grad_norm": 0.26176348328590393,
      "learning_rate": 4.505666666666667e-05,
      "loss": 0.0035,
      "step": 14830
    },
    {
      "epoch": 0.7914666666666667,
      "grad_norm": 0.09862294793128967,
      "learning_rate": 4.505333333333334e-05,
      "loss": 0.0026,
      "step": 14840
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.5825381278991699,
      "learning_rate": 4.5050000000000004e-05,
      "loss": 0.0028,
      "step": 14850
    },
    {
      "epoch": 0.7925333333333333,
      "grad_norm": 0.3752652704715729,
      "learning_rate": 4.504666666666667e-05,
      "loss": 0.004,
      "step": 14860
    },
    {
      "epoch": 0.7930666666666667,
      "grad_norm": 0.4480818808078766,
      "learning_rate": 4.5043333333333336e-05,
      "loss": 0.0046,
      "step": 14870
    },
    {
      "epoch": 0.7936,
      "grad_norm": 0.8104575276374817,
      "learning_rate": 4.504e-05,
      "loss": 0.0031,
      "step": 14880
    },
    {
      "epoch": 0.7941333333333334,
      "grad_norm": 0.31012219190597534,
      "learning_rate": 4.503666666666667e-05,
      "loss": 0.0029,
      "step": 14890
    },
    {
      "epoch": 0.7946666666666666,
      "grad_norm": 0.19153361022472382,
      "learning_rate": 4.5033333333333335e-05,
      "loss": 0.0042,
      "step": 14900
    },
    {
      "epoch": 0.7952,
      "grad_norm": 0.42951348423957825,
      "learning_rate": 4.503e-05,
      "loss": 0.0026,
      "step": 14910
    },
    {
      "epoch": 0.7957333333333333,
      "grad_norm": 0.4439031779766083,
      "learning_rate": 4.502666666666667e-05,
      "loss": 0.0026,
      "step": 14920
    },
    {
      "epoch": 0.7962666666666667,
      "grad_norm": 0.5336490869522095,
      "learning_rate": 4.502333333333333e-05,
      "loss": 0.0029,
      "step": 14930
    },
    {
      "epoch": 0.7968,
      "grad_norm": 0.587760865688324,
      "learning_rate": 4.502e-05,
      "loss": 0.0026,
      "step": 14940
    },
    {
      "epoch": 0.7973333333333333,
      "grad_norm": 0.44122830033302307,
      "learning_rate": 4.5016666666666665e-05,
      "loss": 0.0024,
      "step": 14950
    },
    {
      "epoch": 0.7978666666666666,
      "grad_norm": 0.5355491042137146,
      "learning_rate": 4.501333333333334e-05,
      "loss": 0.003,
      "step": 14960
    },
    {
      "epoch": 0.7984,
      "grad_norm": 0.7051874399185181,
      "learning_rate": 4.5010000000000004e-05,
      "loss": 0.0026,
      "step": 14970
    },
    {
      "epoch": 0.7989333333333334,
      "grad_norm": 0.21728704869747162,
      "learning_rate": 4.500666666666667e-05,
      "loss": 0.0028,
      "step": 14980
    },
    {
      "epoch": 0.7994666666666667,
      "grad_norm": 0.7381712198257446,
      "learning_rate": 4.500333333333334e-05,
      "loss": 0.0022,
      "step": 14990
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.8134600520133972,
      "learning_rate": 4.5e-05,
      "loss": 0.005,
      "step": 15000
    },
    {
      "epoch": 0.8005333333333333,
      "grad_norm": 0.5871949195861816,
      "learning_rate": 4.499666666666667e-05,
      "loss": 0.0041,
      "step": 15010
    },
    {
      "epoch": 0.8010666666666667,
      "grad_norm": 0.16695085167884827,
      "learning_rate": 4.4993333333333335e-05,
      "loss": 0.0022,
      "step": 15020
    },
    {
      "epoch": 0.8016,
      "grad_norm": 0.08439274877309799,
      "learning_rate": 4.499e-05,
      "loss": 0.0026,
      "step": 15030
    },
    {
      "epoch": 0.8021333333333334,
      "grad_norm": 0.21671421825885773,
      "learning_rate": 4.4986666666666674e-05,
      "loss": 0.0038,
      "step": 15040
    },
    {
      "epoch": 0.8026666666666666,
      "grad_norm": 0.3099672198295593,
      "learning_rate": 4.4983333333333334e-05,
      "loss": 0.0051,
      "step": 15050
    },
    {
      "epoch": 0.8032,
      "grad_norm": 0.5750340819358826,
      "learning_rate": 4.498e-05,
      "loss": 0.004,
      "step": 15060
    },
    {
      "epoch": 0.8037333333333333,
      "grad_norm": 0.4311639666557312,
      "learning_rate": 4.4976666666666666e-05,
      "loss": 0.0044,
      "step": 15070
    },
    {
      "epoch": 0.8042666666666667,
      "grad_norm": 0.5408666729927063,
      "learning_rate": 4.497333333333333e-05,
      "loss": 0.0032,
      "step": 15080
    },
    {
      "epoch": 0.8048,
      "grad_norm": 0.40554895997047424,
      "learning_rate": 4.497e-05,
      "loss": 0.0025,
      "step": 15090
    },
    {
      "epoch": 0.8053333333333333,
      "grad_norm": 0.05502712354063988,
      "learning_rate": 4.496666666666667e-05,
      "loss": 0.0029,
      "step": 15100
    },
    {
      "epoch": 0.8058666666666666,
      "grad_norm": 0.6557668447494507,
      "learning_rate": 4.496333333333334e-05,
      "loss": 0.0042,
      "step": 15110
    },
    {
      "epoch": 0.8064,
      "grad_norm": 0.5615587830543518,
      "learning_rate": 4.496e-05,
      "loss": 0.0027,
      "step": 15120
    },
    {
      "epoch": 0.8069333333333333,
      "grad_norm": 0.25217121839523315,
      "learning_rate": 4.495666666666667e-05,
      "loss": 0.0028,
      "step": 15130
    },
    {
      "epoch": 0.8074666666666667,
      "grad_norm": 0.19825318455696106,
      "learning_rate": 4.4953333333333335e-05,
      "loss": 0.0047,
      "step": 15140
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.7203643918037415,
      "learning_rate": 4.495e-05,
      "loss": 0.0029,
      "step": 15150
    },
    {
      "epoch": 0.8085333333333333,
      "grad_norm": 0.6249129176139832,
      "learning_rate": 4.494666666666667e-05,
      "loss": 0.0036,
      "step": 15160
    },
    {
      "epoch": 0.8090666666666667,
      "grad_norm": 0.10883703827857971,
      "learning_rate": 4.494333333333334e-05,
      "loss": 0.0027,
      "step": 15170
    },
    {
      "epoch": 0.8096,
      "grad_norm": 0.1340256929397583,
      "learning_rate": 4.494000000000001e-05,
      "loss": 0.0025,
      "step": 15180
    },
    {
      "epoch": 0.8101333333333334,
      "grad_norm": 0.08008553832769394,
      "learning_rate": 4.493666666666667e-05,
      "loss": 0.0033,
      "step": 15190
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 0.3725329041481018,
      "learning_rate": 4.493333333333333e-05,
      "loss": 0.0031,
      "step": 15200
    },
    {
      "epoch": 0.8112,
      "grad_norm": 0.28790462017059326,
      "learning_rate": 4.493e-05,
      "loss": 0.0028,
      "step": 15210
    },
    {
      "epoch": 0.8117333333333333,
      "grad_norm": 0.1591503918170929,
      "learning_rate": 4.4926666666666665e-05,
      "loss": 0.0036,
      "step": 15220
    },
    {
      "epoch": 0.8122666666666667,
      "grad_norm": 0.5353676676750183,
      "learning_rate": 4.492333333333333e-05,
      "loss": 0.0025,
      "step": 15230
    },
    {
      "epoch": 0.8128,
      "grad_norm": 0.5381026864051819,
      "learning_rate": 4.4920000000000004e-05,
      "loss": 0.0031,
      "step": 15240
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.27275705337524414,
      "learning_rate": 4.491666666666667e-05,
      "loss": 0.0036,
      "step": 15250
    },
    {
      "epoch": 0.8138666666666666,
      "grad_norm": 0.12620168924331665,
      "learning_rate": 4.4913333333333336e-05,
      "loss": 0.004,
      "step": 15260
    },
    {
      "epoch": 0.8144,
      "grad_norm": 0.09354742616415024,
      "learning_rate": 4.491e-05,
      "loss": 0.003,
      "step": 15270
    },
    {
      "epoch": 0.8149333333333333,
      "grad_norm": 0.2659383714199066,
      "learning_rate": 4.490666666666667e-05,
      "loss": 0.003,
      "step": 15280
    },
    {
      "epoch": 0.8154666666666667,
      "grad_norm": 0.741836667060852,
      "learning_rate": 4.4903333333333334e-05,
      "loss": 0.0031,
      "step": 15290
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.10959891229867935,
      "learning_rate": 4.49e-05,
      "loss": 0.0031,
      "step": 15300
    },
    {
      "epoch": 0.8165333333333333,
      "grad_norm": 0.7060784697532654,
      "learning_rate": 4.489666666666667e-05,
      "loss": 0.0026,
      "step": 15310
    },
    {
      "epoch": 0.8170666666666667,
      "grad_norm": 0.4355226159095764,
      "learning_rate": 4.489333333333334e-05,
      "loss": 0.0023,
      "step": 15320
    },
    {
      "epoch": 0.8176,
      "grad_norm": 0.28387513756752014,
      "learning_rate": 4.4890000000000006e-05,
      "loss": 0.0024,
      "step": 15330
    },
    {
      "epoch": 0.8181333333333334,
      "grad_norm": 0.5539824366569519,
      "learning_rate": 4.488666666666667e-05,
      "loss": 0.0033,
      "step": 15340
    },
    {
      "epoch": 0.8186666666666667,
      "grad_norm": 0.37117403745651245,
      "learning_rate": 4.488333333333333e-05,
      "loss": 0.0034,
      "step": 15350
    },
    {
      "epoch": 0.8192,
      "grad_norm": 0.18939681351184845,
      "learning_rate": 4.488e-05,
      "loss": 0.0034,
      "step": 15360
    },
    {
      "epoch": 0.8197333333333333,
      "grad_norm": 0.4141559898853302,
      "learning_rate": 4.487666666666667e-05,
      "loss": 0.004,
      "step": 15370
    },
    {
      "epoch": 0.8202666666666667,
      "grad_norm": 0.3742889165878296,
      "learning_rate": 4.4873333333333336e-05,
      "loss": 0.0035,
      "step": 15380
    },
    {
      "epoch": 0.8208,
      "grad_norm": 0.7177234292030334,
      "learning_rate": 4.487e-05,
      "loss": 0.0023,
      "step": 15390
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 0.8643690347671509,
      "learning_rate": 4.486666666666667e-05,
      "loss": 0.0043,
      "step": 15400
    },
    {
      "epoch": 0.8218666666666666,
      "grad_norm": 0.8212355971336365,
      "learning_rate": 4.4863333333333335e-05,
      "loss": 0.0041,
      "step": 15410
    },
    {
      "epoch": 0.8224,
      "grad_norm": 0.2626599371433258,
      "learning_rate": 4.486e-05,
      "loss": 0.0021,
      "step": 15420
    },
    {
      "epoch": 0.8229333333333333,
      "grad_norm": 0.07936197519302368,
      "learning_rate": 4.485666666666667e-05,
      "loss": 0.0025,
      "step": 15430
    },
    {
      "epoch": 0.8234666666666667,
      "grad_norm": 0.3097483515739441,
      "learning_rate": 4.485333333333333e-05,
      "loss": 0.0031,
      "step": 15440
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.12522681057453156,
      "learning_rate": 4.4850000000000006e-05,
      "loss": 0.0031,
      "step": 15450
    },
    {
      "epoch": 0.8245333333333333,
      "grad_norm": 0.047332651913166046,
      "learning_rate": 4.484666666666667e-05,
      "loss": 0.0022,
      "step": 15460
    },
    {
      "epoch": 0.8250666666666666,
      "grad_norm": 0.30804696679115295,
      "learning_rate": 4.484333333333334e-05,
      "loss": 0.0032,
      "step": 15470
    },
    {
      "epoch": 0.8256,
      "grad_norm": 0.5070022344589233,
      "learning_rate": 4.4840000000000004e-05,
      "loss": 0.0038,
      "step": 15480
    },
    {
      "epoch": 0.8261333333333334,
      "grad_norm": 0.34148073196411133,
      "learning_rate": 4.483666666666667e-05,
      "loss": 0.0044,
      "step": 15490
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.7283865809440613,
      "learning_rate": 4.483333333333333e-05,
      "loss": 0.0025,
      "step": 15500
    },
    {
      "epoch": 0.8272,
      "grad_norm": 0.15923470258712769,
      "learning_rate": 4.483e-05,
      "loss": 0.0025,
      "step": 15510
    },
    {
      "epoch": 0.8277333333333333,
      "grad_norm": 0.14582188427448273,
      "learning_rate": 4.482666666666667e-05,
      "loss": 0.0028,
      "step": 15520
    },
    {
      "epoch": 0.8282666666666667,
      "grad_norm": 0.24014073610305786,
      "learning_rate": 4.4823333333333335e-05,
      "loss": 0.0029,
      "step": 15530
    },
    {
      "epoch": 0.8288,
      "grad_norm": 0.5704312324523926,
      "learning_rate": 4.482e-05,
      "loss": 0.003,
      "step": 15540
    },
    {
      "epoch": 0.8293333333333334,
      "grad_norm": 0.18664434552192688,
      "learning_rate": 4.481666666666667e-05,
      "loss": 0.003,
      "step": 15550
    },
    {
      "epoch": 0.8298666666666666,
      "grad_norm": 0.4670615494251251,
      "learning_rate": 4.4813333333333333e-05,
      "loss": 0.004,
      "step": 15560
    },
    {
      "epoch": 0.8304,
      "grad_norm": 0.2318590134382248,
      "learning_rate": 4.481e-05,
      "loss": 0.0031,
      "step": 15570
    },
    {
      "epoch": 0.8309333333333333,
      "grad_norm": 0.483045369386673,
      "learning_rate": 4.4806666666666666e-05,
      "loss": 0.0031,
      "step": 15580
    },
    {
      "epoch": 0.8314666666666667,
      "grad_norm": 0.13275666534900665,
      "learning_rate": 4.480333333333334e-05,
      "loss": 0.0025,
      "step": 15590
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.02290559746325016,
      "learning_rate": 4.4800000000000005e-05,
      "loss": 0.0023,
      "step": 15600
    },
    {
      "epoch": 0.8325333333333333,
      "grad_norm": 0.8039500713348389,
      "learning_rate": 4.479666666666667e-05,
      "loss": 0.0034,
      "step": 15610
    },
    {
      "epoch": 0.8330666666666666,
      "grad_norm": 0.3687186539173126,
      "learning_rate": 4.479333333333334e-05,
      "loss": 0.0031,
      "step": 15620
    },
    {
      "epoch": 0.8336,
      "grad_norm": 0.3774689733982086,
      "learning_rate": 4.479e-05,
      "loss": 0.0028,
      "step": 15630
    },
    {
      "epoch": 0.8341333333333333,
      "grad_norm": 0.06243596598505974,
      "learning_rate": 4.478666666666667e-05,
      "loss": 0.0033,
      "step": 15640
    },
    {
      "epoch": 0.8346666666666667,
      "grad_norm": 0.18685977160930634,
      "learning_rate": 4.4783333333333335e-05,
      "loss": 0.0028,
      "step": 15650
    },
    {
      "epoch": 0.8352,
      "grad_norm": 0.49757009744644165,
      "learning_rate": 4.478e-05,
      "loss": 0.0016,
      "step": 15660
    },
    {
      "epoch": 0.8357333333333333,
      "grad_norm": 0.051802486181259155,
      "learning_rate": 4.477666666666667e-05,
      "loss": 0.0035,
      "step": 15670
    },
    {
      "epoch": 0.8362666666666667,
      "grad_norm": 0.1429518461227417,
      "learning_rate": 4.4773333333333334e-05,
      "loss": 0.0024,
      "step": 15680
    },
    {
      "epoch": 0.8368,
      "grad_norm": 0.3418331742286682,
      "learning_rate": 4.477e-05,
      "loss": 0.002,
      "step": 15690
    },
    {
      "epoch": 0.8373333333333334,
      "grad_norm": 0.4027830958366394,
      "learning_rate": 4.4766666666666666e-05,
      "loss": 0.0022,
      "step": 15700
    },
    {
      "epoch": 0.8378666666666666,
      "grad_norm": 0.05543530359864235,
      "learning_rate": 4.476333333333333e-05,
      "loss": 0.0038,
      "step": 15710
    },
    {
      "epoch": 0.8384,
      "grad_norm": 0.13117791712284088,
      "learning_rate": 4.4760000000000005e-05,
      "loss": 0.0027,
      "step": 15720
    },
    {
      "epoch": 0.8389333333333333,
      "grad_norm": 0.10796713083982468,
      "learning_rate": 4.475666666666667e-05,
      "loss": 0.0039,
      "step": 15730
    },
    {
      "epoch": 0.8394666666666667,
      "grad_norm": 0.5283876061439514,
      "learning_rate": 4.475333333333334e-05,
      "loss": 0.0027,
      "step": 15740
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.26666879653930664,
      "learning_rate": 4.4750000000000004e-05,
      "loss": 0.0032,
      "step": 15750
    },
    {
      "epoch": 0.8405333333333334,
      "grad_norm": 0.33191096782684326,
      "learning_rate": 4.474666666666667e-05,
      "loss": 0.0038,
      "step": 15760
    },
    {
      "epoch": 0.8410666666666666,
      "grad_norm": 0.622778594493866,
      "learning_rate": 4.4743333333333336e-05,
      "loss": 0.0041,
      "step": 15770
    },
    {
      "epoch": 0.8416,
      "grad_norm": 0.49829524755477905,
      "learning_rate": 4.474e-05,
      "loss": 0.0038,
      "step": 15780
    },
    {
      "epoch": 0.8421333333333333,
      "grad_norm": 0.05591713637113571,
      "learning_rate": 4.473666666666667e-05,
      "loss": 0.0038,
      "step": 15790
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 0.5971029996871948,
      "learning_rate": 4.473333333333334e-05,
      "loss": 0.0031,
      "step": 15800
    },
    {
      "epoch": 0.8432,
      "grad_norm": 0.591315507888794,
      "learning_rate": 4.473e-05,
      "loss": 0.0032,
      "step": 15810
    },
    {
      "epoch": 0.8437333333333333,
      "grad_norm": 0.5554574728012085,
      "learning_rate": 4.4726666666666666e-05,
      "loss": 0.0028,
      "step": 15820
    },
    {
      "epoch": 0.8442666666666667,
      "grad_norm": 0.2777542173862457,
      "learning_rate": 4.472333333333333e-05,
      "loss": 0.0034,
      "step": 15830
    },
    {
      "epoch": 0.8448,
      "grad_norm": 0.723019003868103,
      "learning_rate": 4.472e-05,
      "loss": 0.0029,
      "step": 15840
    },
    {
      "epoch": 0.8453333333333334,
      "grad_norm": 0.2733772397041321,
      "learning_rate": 4.4716666666666665e-05,
      "loss": 0.0031,
      "step": 15850
    },
    {
      "epoch": 0.8458666666666667,
      "grad_norm": 0.05321359634399414,
      "learning_rate": 4.471333333333334e-05,
      "loss": 0.0018,
      "step": 15860
    },
    {
      "epoch": 0.8464,
      "grad_norm": 0.19140760600566864,
      "learning_rate": 4.4710000000000004e-05,
      "loss": 0.0028,
      "step": 15870
    },
    {
      "epoch": 0.8469333333333333,
      "grad_norm": 0.534395158290863,
      "learning_rate": 4.470666666666667e-05,
      "loss": 0.005,
      "step": 15880
    },
    {
      "epoch": 0.8474666666666667,
      "grad_norm": 0.4551081359386444,
      "learning_rate": 4.4703333333333336e-05,
      "loss": 0.0035,
      "step": 15890
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.27780580520629883,
      "learning_rate": 4.47e-05,
      "loss": 0.0028,
      "step": 15900
    },
    {
      "epoch": 0.8485333333333334,
      "grad_norm": 0.5023908019065857,
      "learning_rate": 4.469666666666667e-05,
      "loss": 0.0035,
      "step": 15910
    },
    {
      "epoch": 0.8490666666666666,
      "grad_norm": 0.2780563235282898,
      "learning_rate": 4.4693333333333335e-05,
      "loss": 0.0031,
      "step": 15920
    },
    {
      "epoch": 0.8496,
      "grad_norm": 0.5606116652488708,
      "learning_rate": 4.469e-05,
      "loss": 0.0029,
      "step": 15930
    },
    {
      "epoch": 0.8501333333333333,
      "grad_norm": 0.06862438470125198,
      "learning_rate": 4.4686666666666674e-05,
      "loss": 0.0033,
      "step": 15940
    },
    {
      "epoch": 0.8506666666666667,
      "grad_norm": 0.9546715617179871,
      "learning_rate": 4.468333333333334e-05,
      "loss": 0.0038,
      "step": 15950
    },
    {
      "epoch": 0.8512,
      "grad_norm": 0.6837767958641052,
      "learning_rate": 4.468e-05,
      "loss": 0.0038,
      "step": 15960
    },
    {
      "epoch": 0.8517333333333333,
      "grad_norm": 1.0400681495666504,
      "learning_rate": 4.4676666666666665e-05,
      "loss": 0.0031,
      "step": 15970
    },
    {
      "epoch": 0.8522666666666666,
      "grad_norm": 0.1612147092819214,
      "learning_rate": 4.467333333333333e-05,
      "loss": 0.0025,
      "step": 15980
    },
    {
      "epoch": 0.8528,
      "grad_norm": 0.3252052664756775,
      "learning_rate": 4.467e-05,
      "loss": 0.0043,
      "step": 15990
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.5065069198608398,
      "learning_rate": 4.466666666666667e-05,
      "loss": 0.0039,
      "step": 16000
    },
    {
      "epoch": 0.8538666666666667,
      "grad_norm": 0.39651861786842346,
      "learning_rate": 4.4663333333333337e-05,
      "loss": 0.0033,
      "step": 16010
    },
    {
      "epoch": 0.8544,
      "grad_norm": 0.5899062752723694,
      "learning_rate": 4.466e-05,
      "loss": 0.004,
      "step": 16020
    },
    {
      "epoch": 0.8549333333333333,
      "grad_norm": 0.3215391933917999,
      "learning_rate": 4.465666666666667e-05,
      "loss": 0.0032,
      "step": 16030
    },
    {
      "epoch": 0.8554666666666667,
      "grad_norm": 0.146616131067276,
      "learning_rate": 4.4653333333333335e-05,
      "loss": 0.0026,
      "step": 16040
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.5632112622261047,
      "learning_rate": 4.465e-05,
      "loss": 0.0037,
      "step": 16050
    },
    {
      "epoch": 0.8565333333333334,
      "grad_norm": 0.4476063847541809,
      "learning_rate": 4.464666666666667e-05,
      "loss": 0.0031,
      "step": 16060
    },
    {
      "epoch": 0.8570666666666666,
      "grad_norm": 0.464338093996048,
      "learning_rate": 4.464333333333334e-05,
      "loss": 0.0029,
      "step": 16070
    },
    {
      "epoch": 0.8576,
      "grad_norm": 0.40921422839164734,
      "learning_rate": 4.4640000000000006e-05,
      "loss": 0.002,
      "step": 16080
    },
    {
      "epoch": 0.8581333333333333,
      "grad_norm": 0.5709475874900818,
      "learning_rate": 4.463666666666667e-05,
      "loss": 0.0026,
      "step": 16090
    },
    {
      "epoch": 0.8586666666666667,
      "grad_norm": 0.4398830235004425,
      "learning_rate": 4.463333333333334e-05,
      "loss": 0.0028,
      "step": 16100
    },
    {
      "epoch": 0.8592,
      "grad_norm": 0.2982447147369385,
      "learning_rate": 4.463e-05,
      "loss": 0.0027,
      "step": 16110
    },
    {
      "epoch": 0.8597333333333333,
      "grad_norm": 0.39537498354911804,
      "learning_rate": 4.4626666666666664e-05,
      "loss": 0.003,
      "step": 16120
    },
    {
      "epoch": 0.8602666666666666,
      "grad_norm": 0.32131409645080566,
      "learning_rate": 4.462333333333334e-05,
      "loss": 0.004,
      "step": 16130
    },
    {
      "epoch": 0.8608,
      "grad_norm": 0.47897908091545105,
      "learning_rate": 4.462e-05,
      "loss": 0.0027,
      "step": 16140
    },
    {
      "epoch": 0.8613333333333333,
      "grad_norm": 0.14847153425216675,
      "learning_rate": 4.461666666666667e-05,
      "loss": 0.0036,
      "step": 16150
    },
    {
      "epoch": 0.8618666666666667,
      "grad_norm": 0.308109313249588,
      "learning_rate": 4.4613333333333335e-05,
      "loss": 0.0034,
      "step": 16160
    },
    {
      "epoch": 0.8624,
      "grad_norm": 0.36012253165245056,
      "learning_rate": 4.461e-05,
      "loss": 0.0044,
      "step": 16170
    },
    {
      "epoch": 0.8629333333333333,
      "grad_norm": 0.10559509694576263,
      "learning_rate": 4.460666666666667e-05,
      "loss": 0.004,
      "step": 16180
    },
    {
      "epoch": 0.8634666666666667,
      "grad_norm": 0.5640010237693787,
      "learning_rate": 4.4603333333333334e-05,
      "loss": 0.0037,
      "step": 16190
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.21788284182548523,
      "learning_rate": 4.46e-05,
      "loss": 0.0037,
      "step": 16200
    },
    {
      "epoch": 0.8645333333333334,
      "grad_norm": 0.6297928094863892,
      "learning_rate": 4.459666666666667e-05,
      "loss": 0.0032,
      "step": 16210
    },
    {
      "epoch": 0.8650666666666667,
      "grad_norm": 0.39965346455574036,
      "learning_rate": 4.459333333333334e-05,
      "loss": 0.0034,
      "step": 16220
    },
    {
      "epoch": 0.8656,
      "grad_norm": 0.6803938746452332,
      "learning_rate": 4.4590000000000005e-05,
      "loss": 0.0026,
      "step": 16230
    },
    {
      "epoch": 0.8661333333333333,
      "grad_norm": 0.14582011103630066,
      "learning_rate": 4.458666666666667e-05,
      "loss": 0.0035,
      "step": 16240
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.15865229070186615,
      "learning_rate": 4.458333333333334e-05,
      "loss": 0.0021,
      "step": 16250
    },
    {
      "epoch": 0.8672,
      "grad_norm": 0.2773405909538269,
      "learning_rate": 4.458e-05,
      "loss": 0.0037,
      "step": 16260
    },
    {
      "epoch": 0.8677333333333334,
      "grad_norm": 0.4302789866924286,
      "learning_rate": 4.457666666666667e-05,
      "loss": 0.0035,
      "step": 16270
    },
    {
      "epoch": 0.8682666666666666,
      "grad_norm": 0.09725689888000488,
      "learning_rate": 4.4573333333333336e-05,
      "loss": 0.0032,
      "step": 16280
    },
    {
      "epoch": 0.8688,
      "grad_norm": 0.34173086285591125,
      "learning_rate": 4.457e-05,
      "loss": 0.002,
      "step": 16290
    },
    {
      "epoch": 0.8693333333333333,
      "grad_norm": 0.37631550431251526,
      "learning_rate": 4.456666666666667e-05,
      "loss": 0.0032,
      "step": 16300
    },
    {
      "epoch": 0.8698666666666667,
      "grad_norm": 0.24840889871120453,
      "learning_rate": 4.4563333333333334e-05,
      "loss": 0.0038,
      "step": 16310
    },
    {
      "epoch": 0.8704,
      "grad_norm": 0.2895369827747345,
      "learning_rate": 4.456e-05,
      "loss": 0.0035,
      "step": 16320
    },
    {
      "epoch": 0.8709333333333333,
      "grad_norm": 0.24669697880744934,
      "learning_rate": 4.4556666666666666e-05,
      "loss": 0.0045,
      "step": 16330
    },
    {
      "epoch": 0.8714666666666666,
      "grad_norm": 0.22440119087696075,
      "learning_rate": 4.455333333333333e-05,
      "loss": 0.0029,
      "step": 16340
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.37213239073753357,
      "learning_rate": 4.4550000000000005e-05,
      "loss": 0.0043,
      "step": 16350
    },
    {
      "epoch": 0.8725333333333334,
      "grad_norm": 0.5249194502830505,
      "learning_rate": 4.454666666666667e-05,
      "loss": 0.0035,
      "step": 16360
    },
    {
      "epoch": 0.8730666666666667,
      "grad_norm": 0.8274199962615967,
      "learning_rate": 4.454333333333334e-05,
      "loss": 0.0029,
      "step": 16370
    },
    {
      "epoch": 0.8736,
      "grad_norm": 0.4658367335796356,
      "learning_rate": 4.4540000000000004e-05,
      "loss": 0.0018,
      "step": 16380
    },
    {
      "epoch": 0.8741333333333333,
      "grad_norm": 0.70524662733078,
      "learning_rate": 4.453666666666667e-05,
      "loss": 0.0033,
      "step": 16390
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 0.25578901171684265,
      "learning_rate": 4.4533333333333336e-05,
      "loss": 0.0024,
      "step": 16400
    },
    {
      "epoch": 0.8752,
      "grad_norm": 0.15818706154823303,
      "learning_rate": 4.453e-05,
      "loss": 0.0035,
      "step": 16410
    },
    {
      "epoch": 0.8757333333333334,
      "grad_norm": 0.5164750218391418,
      "learning_rate": 4.452666666666667e-05,
      "loss": 0.0036,
      "step": 16420
    },
    {
      "epoch": 0.8762666666666666,
      "grad_norm": 0.6329954862594604,
      "learning_rate": 4.4523333333333335e-05,
      "loss": 0.0036,
      "step": 16430
    },
    {
      "epoch": 0.8768,
      "grad_norm": 0.33302438259124756,
      "learning_rate": 4.452e-05,
      "loss": 0.0031,
      "step": 16440
    },
    {
      "epoch": 0.8773333333333333,
      "grad_norm": 0.08796746283769608,
      "learning_rate": 4.451666666666667e-05,
      "loss": 0.0038,
      "step": 16450
    },
    {
      "epoch": 0.8778666666666667,
      "grad_norm": 0.5511781573295593,
      "learning_rate": 4.451333333333333e-05,
      "loss": 0.0032,
      "step": 16460
    },
    {
      "epoch": 0.8784,
      "grad_norm": 0.13498768210411072,
      "learning_rate": 4.451e-05,
      "loss": 0.0032,
      "step": 16470
    },
    {
      "epoch": 0.8789333333333333,
      "grad_norm": 0.4536336064338684,
      "learning_rate": 4.450666666666667e-05,
      "loss": 0.0033,
      "step": 16480
    },
    {
      "epoch": 0.8794666666666666,
      "grad_norm": 0.11333608627319336,
      "learning_rate": 4.450333333333334e-05,
      "loss": 0.0026,
      "step": 16490
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.5575371384620667,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 0.0029,
      "step": 16500
    },
    {
      "epoch": 0.8805333333333333,
      "grad_norm": 0.5240333080291748,
      "learning_rate": 4.449666666666667e-05,
      "loss": 0.0042,
      "step": 16510
    },
    {
      "epoch": 0.8810666666666667,
      "grad_norm": 0.22623540461063385,
      "learning_rate": 4.4493333333333337e-05,
      "loss": 0.0042,
      "step": 16520
    },
    {
      "epoch": 0.8816,
      "grad_norm": 0.602626383304596,
      "learning_rate": 4.449e-05,
      "loss": 0.0022,
      "step": 16530
    },
    {
      "epoch": 0.8821333333333333,
      "grad_norm": 0.7545419931411743,
      "learning_rate": 4.448666666666667e-05,
      "loss": 0.0031,
      "step": 16540
    },
    {
      "epoch": 0.8826666666666667,
      "grad_norm": 0.4335154891014099,
      "learning_rate": 4.4483333333333335e-05,
      "loss": 0.0044,
      "step": 16550
    },
    {
      "epoch": 0.8832,
      "grad_norm": 0.2805161476135254,
      "learning_rate": 4.448e-05,
      "loss": 0.0033,
      "step": 16560
    },
    {
      "epoch": 0.8837333333333334,
      "grad_norm": 0.01909993402659893,
      "learning_rate": 4.447666666666667e-05,
      "loss": 0.0033,
      "step": 16570
    },
    {
      "epoch": 0.8842666666666666,
      "grad_norm": 0.22084718942642212,
      "learning_rate": 4.447333333333333e-05,
      "loss": 0.0022,
      "step": 16580
    },
    {
      "epoch": 0.8848,
      "grad_norm": 0.7436741590499878,
      "learning_rate": 4.447e-05,
      "loss": 0.0033,
      "step": 16590
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 0.2174808830022812,
      "learning_rate": 4.4466666666666666e-05,
      "loss": 0.003,
      "step": 16600
    },
    {
      "epoch": 0.8858666666666667,
      "grad_norm": 0.370515912771225,
      "learning_rate": 4.446333333333333e-05,
      "loss": 0.0034,
      "step": 16610
    },
    {
      "epoch": 0.8864,
      "grad_norm": 0.023636531084775925,
      "learning_rate": 4.4460000000000005e-05,
      "loss": 0.0029,
      "step": 16620
    },
    {
      "epoch": 0.8869333333333334,
      "grad_norm": 0.09669714421033859,
      "learning_rate": 4.445666666666667e-05,
      "loss": 0.0027,
      "step": 16630
    },
    {
      "epoch": 0.8874666666666666,
      "grad_norm": 0.7494212985038757,
      "learning_rate": 4.445333333333334e-05,
      "loss": 0.0026,
      "step": 16640
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.4129166901111603,
      "learning_rate": 4.445e-05,
      "loss": 0.0033,
      "step": 16650
    },
    {
      "epoch": 0.8885333333333333,
      "grad_norm": 0.3407944142818451,
      "learning_rate": 4.444666666666667e-05,
      "loss": 0.0017,
      "step": 16660
    },
    {
      "epoch": 0.8890666666666667,
      "grad_norm": 0.3981251120567322,
      "learning_rate": 4.4443333333333335e-05,
      "loss": 0.0018,
      "step": 16670
    },
    {
      "epoch": 0.8896,
      "grad_norm": 0.11959251761436462,
      "learning_rate": 4.444e-05,
      "loss": 0.0031,
      "step": 16680
    },
    {
      "epoch": 0.8901333333333333,
      "grad_norm": 0.19860020279884338,
      "learning_rate": 4.443666666666667e-05,
      "loss": 0.0024,
      "step": 16690
    },
    {
      "epoch": 0.8906666666666667,
      "grad_norm": 0.407025545835495,
      "learning_rate": 4.443333333333334e-05,
      "loss": 0.0035,
      "step": 16700
    },
    {
      "epoch": 0.8912,
      "grad_norm": 0.3049463927745819,
      "learning_rate": 4.443e-05,
      "loss": 0.0032,
      "step": 16710
    },
    {
      "epoch": 0.8917333333333334,
      "grad_norm": 0.21797727048397064,
      "learning_rate": 4.4426666666666666e-05,
      "loss": 0.0028,
      "step": 16720
    },
    {
      "epoch": 0.8922666666666667,
      "grad_norm": 0.18515077233314514,
      "learning_rate": 4.442333333333333e-05,
      "loss": 0.0026,
      "step": 16730
    },
    {
      "epoch": 0.8928,
      "grad_norm": 0.20140846073627472,
      "learning_rate": 4.442e-05,
      "loss": 0.0044,
      "step": 16740
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.48559656739234924,
      "learning_rate": 4.4416666666666664e-05,
      "loss": 0.0023,
      "step": 16750
    },
    {
      "epoch": 0.8938666666666667,
      "grad_norm": 0.3031703233718872,
      "learning_rate": 4.441333333333334e-05,
      "loss": 0.0043,
      "step": 16760
    },
    {
      "epoch": 0.8944,
      "grad_norm": 0.38270843029022217,
      "learning_rate": 4.4410000000000003e-05,
      "loss": 0.002,
      "step": 16770
    },
    {
      "epoch": 0.8949333333333334,
      "grad_norm": 0.6354351043701172,
      "learning_rate": 4.440666666666667e-05,
      "loss": 0.0028,
      "step": 16780
    },
    {
      "epoch": 0.8954666666666666,
      "grad_norm": 0.672886312007904,
      "learning_rate": 4.4403333333333336e-05,
      "loss": 0.003,
      "step": 16790
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.4614854156970978,
      "learning_rate": 4.44e-05,
      "loss": 0.0029,
      "step": 16800
    },
    {
      "epoch": 0.8965333333333333,
      "grad_norm": 0.33444929122924805,
      "learning_rate": 4.439666666666667e-05,
      "loss": 0.0028,
      "step": 16810
    },
    {
      "epoch": 0.8970666666666667,
      "grad_norm": 0.33091217279434204,
      "learning_rate": 4.4393333333333334e-05,
      "loss": 0.0031,
      "step": 16820
    },
    {
      "epoch": 0.8976,
      "grad_norm": 0.3781048655509949,
      "learning_rate": 4.439000000000001e-05,
      "loss": 0.0031,
      "step": 16830
    },
    {
      "epoch": 0.8981333333333333,
      "grad_norm": 0.3068999648094177,
      "learning_rate": 4.438666666666667e-05,
      "loss": 0.0025,
      "step": 16840
    },
    {
      "epoch": 0.8986666666666666,
      "grad_norm": 0.7799836993217468,
      "learning_rate": 4.438333333333334e-05,
      "loss": 0.0038,
      "step": 16850
    },
    {
      "epoch": 0.8992,
      "grad_norm": 0.10470671206712723,
      "learning_rate": 4.438e-05,
      "loss": 0.0038,
      "step": 16860
    },
    {
      "epoch": 0.8997333333333334,
      "grad_norm": 0.2248164415359497,
      "learning_rate": 4.4376666666666665e-05,
      "loss": 0.0029,
      "step": 16870
    },
    {
      "epoch": 0.9002666666666667,
      "grad_norm": 0.36602312326431274,
      "learning_rate": 4.437333333333333e-05,
      "loss": 0.0026,
      "step": 16880
    },
    {
      "epoch": 0.9008,
      "grad_norm": 0.2492227405309677,
      "learning_rate": 4.4370000000000004e-05,
      "loss": 0.0027,
      "step": 16890
    },
    {
      "epoch": 0.9013333333333333,
      "grad_norm": 0.2812238335609436,
      "learning_rate": 4.436666666666667e-05,
      "loss": 0.0033,
      "step": 16900
    },
    {
      "epoch": 0.9018666666666667,
      "grad_norm": 0.6583443284034729,
      "learning_rate": 4.4363333333333336e-05,
      "loss": 0.0036,
      "step": 16910
    },
    {
      "epoch": 0.9024,
      "grad_norm": 0.16865308582782745,
      "learning_rate": 4.436e-05,
      "loss": 0.0036,
      "step": 16920
    },
    {
      "epoch": 0.9029333333333334,
      "grad_norm": 0.4141579866409302,
      "learning_rate": 4.435666666666667e-05,
      "loss": 0.0046,
      "step": 16930
    },
    {
      "epoch": 0.9034666666666666,
      "grad_norm": 0.40310436487197876,
      "learning_rate": 4.4353333333333334e-05,
      "loss": 0.003,
      "step": 16940
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.3534223139286041,
      "learning_rate": 4.435e-05,
      "loss": 0.0031,
      "step": 16950
    },
    {
      "epoch": 0.9045333333333333,
      "grad_norm": 0.34570178389549255,
      "learning_rate": 4.434666666666667e-05,
      "loss": 0.0032,
      "step": 16960
    },
    {
      "epoch": 0.9050666666666667,
      "grad_norm": 0.1605907678604126,
      "learning_rate": 4.434333333333334e-05,
      "loss": 0.0038,
      "step": 16970
    },
    {
      "epoch": 0.9056,
      "grad_norm": 0.6890589594841003,
      "learning_rate": 4.4340000000000006e-05,
      "loss": 0.0033,
      "step": 16980
    },
    {
      "epoch": 0.9061333333333333,
      "grad_norm": 0.21975988149642944,
      "learning_rate": 4.433666666666667e-05,
      "loss": 0.0024,
      "step": 16990
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.10027985274791718,
      "learning_rate": 4.433333333333334e-05,
      "loss": 0.003,
      "step": 17000
    },
    {
      "epoch": 0.9072,
      "grad_norm": 0.3200995624065399,
      "learning_rate": 4.4330000000000004e-05,
      "loss": 0.0026,
      "step": 17010
    },
    {
      "epoch": 0.9077333333333333,
      "grad_norm": 0.06834316998720169,
      "learning_rate": 4.4326666666666664e-05,
      "loss": 0.0029,
      "step": 17020
    },
    {
      "epoch": 0.9082666666666667,
      "grad_norm": 0.10298775881528854,
      "learning_rate": 4.4323333333333336e-05,
      "loss": 0.0034,
      "step": 17030
    },
    {
      "epoch": 0.9088,
      "grad_norm": 0.21203544735908508,
      "learning_rate": 4.432e-05,
      "loss": 0.0037,
      "step": 17040
    },
    {
      "epoch": 0.9093333333333333,
      "grad_norm": 0.3043654263019562,
      "learning_rate": 4.431666666666667e-05,
      "loss": 0.002,
      "step": 17050
    },
    {
      "epoch": 0.9098666666666667,
      "grad_norm": 0.1689993143081665,
      "learning_rate": 4.4313333333333335e-05,
      "loss": 0.0027,
      "step": 17060
    },
    {
      "epoch": 0.9104,
      "grad_norm": 0.5291264653205872,
      "learning_rate": 4.431e-05,
      "loss": 0.0028,
      "step": 17070
    },
    {
      "epoch": 0.9109333333333334,
      "grad_norm": 0.10666678100824356,
      "learning_rate": 4.430666666666667e-05,
      "loss": 0.0026,
      "step": 17080
    },
    {
      "epoch": 0.9114666666666666,
      "grad_norm": 0.5513824820518494,
      "learning_rate": 4.430333333333333e-05,
      "loss": 0.0037,
      "step": 17090
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.6429265737533569,
      "learning_rate": 4.43e-05,
      "loss": 0.0049,
      "step": 17100
    },
    {
      "epoch": 0.9125333333333333,
      "grad_norm": 0.7148327231407166,
      "learning_rate": 4.429666666666667e-05,
      "loss": 0.0034,
      "step": 17110
    },
    {
      "epoch": 0.9130666666666667,
      "grad_norm": 0.1877305507659912,
      "learning_rate": 4.429333333333334e-05,
      "loss": 0.0043,
      "step": 17120
    },
    {
      "epoch": 0.9136,
      "grad_norm": 0.22198714315891266,
      "learning_rate": 4.4290000000000005e-05,
      "loss": 0.0038,
      "step": 17130
    },
    {
      "epoch": 0.9141333333333334,
      "grad_norm": 0.2528303563594818,
      "learning_rate": 4.428666666666667e-05,
      "loss": 0.0029,
      "step": 17140
    },
    {
      "epoch": 0.9146666666666666,
      "grad_norm": 0.06882309913635254,
      "learning_rate": 4.428333333333334e-05,
      "loss": 0.0024,
      "step": 17150
    },
    {
      "epoch": 0.9152,
      "grad_norm": 0.1247212365269661,
      "learning_rate": 4.428e-05,
      "loss": 0.0039,
      "step": 17160
    },
    {
      "epoch": 0.9157333333333333,
      "grad_norm": 0.36714479327201843,
      "learning_rate": 4.427666666666667e-05,
      "loss": 0.0025,
      "step": 17170
    },
    {
      "epoch": 0.9162666666666667,
      "grad_norm": 0.5604548454284668,
      "learning_rate": 4.4273333333333335e-05,
      "loss": 0.0037,
      "step": 17180
    },
    {
      "epoch": 0.9168,
      "grad_norm": 0.6618608832359314,
      "learning_rate": 4.427e-05,
      "loss": 0.0029,
      "step": 17190
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 0.068216972053051,
      "learning_rate": 4.426666666666667e-05,
      "loss": 0.0021,
      "step": 17200
    },
    {
      "epoch": 0.9178666666666667,
      "grad_norm": 0.2204660326242447,
      "learning_rate": 4.4263333333333334e-05,
      "loss": 0.0023,
      "step": 17210
    },
    {
      "epoch": 0.9184,
      "grad_norm": 0.18036089837551117,
      "learning_rate": 4.426e-05,
      "loss": 0.0037,
      "step": 17220
    },
    {
      "epoch": 0.9189333333333334,
      "grad_norm": 0.3264493942260742,
      "learning_rate": 4.4256666666666666e-05,
      "loss": 0.0043,
      "step": 17230
    },
    {
      "epoch": 0.9194666666666667,
      "grad_norm": 0.4666137397289276,
      "learning_rate": 4.425333333333334e-05,
      "loss": 0.0018,
      "step": 17240
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.255609929561615,
      "learning_rate": 4.4250000000000005e-05,
      "loss": 0.0049,
      "step": 17250
    },
    {
      "epoch": 0.9205333333333333,
      "grad_norm": 0.1409013718366623,
      "learning_rate": 4.424666666666667e-05,
      "loss": 0.0045,
      "step": 17260
    },
    {
      "epoch": 0.9210666666666667,
      "grad_norm": 0.25936344265937805,
      "learning_rate": 4.424333333333334e-05,
      "loss": 0.0022,
      "step": 17270
    },
    {
      "epoch": 0.9216,
      "grad_norm": 0.2749612629413605,
      "learning_rate": 4.424e-05,
      "loss": 0.0033,
      "step": 17280
    },
    {
      "epoch": 0.9221333333333334,
      "grad_norm": 0.3691612184047699,
      "learning_rate": 4.423666666666667e-05,
      "loss": 0.0045,
      "step": 17290
    },
    {
      "epoch": 0.9226666666666666,
      "grad_norm": 0.5499182939529419,
      "learning_rate": 4.4233333333333336e-05,
      "loss": 0.0033,
      "step": 17300
    },
    {
      "epoch": 0.9232,
      "grad_norm": 0.15161268413066864,
      "learning_rate": 4.423e-05,
      "loss": 0.0039,
      "step": 17310
    },
    {
      "epoch": 0.9237333333333333,
      "grad_norm": 0.21338294446468353,
      "learning_rate": 4.422666666666667e-05,
      "loss": 0.0024,
      "step": 17320
    },
    {
      "epoch": 0.9242666666666667,
      "grad_norm": 0.34571048617362976,
      "learning_rate": 4.4223333333333334e-05,
      "loss": 0.0031,
      "step": 17330
    },
    {
      "epoch": 0.9248,
      "grad_norm": 0.2928015887737274,
      "learning_rate": 4.422e-05,
      "loss": 0.0036,
      "step": 17340
    },
    {
      "epoch": 0.9253333333333333,
      "grad_norm": 0.0663209930062294,
      "learning_rate": 4.4216666666666666e-05,
      "loss": 0.0027,
      "step": 17350
    },
    {
      "epoch": 0.9258666666666666,
      "grad_norm": 0.22561849653720856,
      "learning_rate": 4.421333333333333e-05,
      "loss": 0.0034,
      "step": 17360
    },
    {
      "epoch": 0.9264,
      "grad_norm": 0.21458055078983307,
      "learning_rate": 4.421e-05,
      "loss": 0.0021,
      "step": 17370
    },
    {
      "epoch": 0.9269333333333334,
      "grad_norm": 0.7482039332389832,
      "learning_rate": 4.420666666666667e-05,
      "loss": 0.0032,
      "step": 17380
    },
    {
      "epoch": 0.9274666666666667,
      "grad_norm": 0.19157257676124573,
      "learning_rate": 4.420333333333334e-05,
      "loss": 0.0024,
      "step": 17390
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.0545092411339283,
      "learning_rate": 4.4200000000000004e-05,
      "loss": 0.0034,
      "step": 17400
    },
    {
      "epoch": 0.9285333333333333,
      "grad_norm": 0.4920167326927185,
      "learning_rate": 4.419666666666667e-05,
      "loss": 0.0028,
      "step": 17410
    },
    {
      "epoch": 0.9290666666666667,
      "grad_norm": 0.2786208689212799,
      "learning_rate": 4.4193333333333336e-05,
      "loss": 0.0034,
      "step": 17420
    },
    {
      "epoch": 0.9296,
      "grad_norm": 0.16535049676895142,
      "learning_rate": 4.419e-05,
      "loss": 0.0022,
      "step": 17430
    },
    {
      "epoch": 0.9301333333333334,
      "grad_norm": 0.334960401058197,
      "learning_rate": 4.418666666666667e-05,
      "loss": 0.0028,
      "step": 17440
    },
    {
      "epoch": 0.9306666666666666,
      "grad_norm": 0.24866914749145508,
      "learning_rate": 4.4183333333333334e-05,
      "loss": 0.0023,
      "step": 17450
    },
    {
      "epoch": 0.9312,
      "grad_norm": 0.1926220953464508,
      "learning_rate": 4.418000000000001e-05,
      "loss": 0.0027,
      "step": 17460
    },
    {
      "epoch": 0.9317333333333333,
      "grad_norm": 0.061688926070928574,
      "learning_rate": 4.417666666666667e-05,
      "loss": 0.0041,
      "step": 17470
    },
    {
      "epoch": 0.9322666666666667,
      "grad_norm": 0.07929787039756775,
      "learning_rate": 4.417333333333333e-05,
      "loss": 0.0033,
      "step": 17480
    },
    {
      "epoch": 0.9328,
      "grad_norm": 0.6763215065002441,
      "learning_rate": 4.417e-05,
      "loss": 0.0033,
      "step": 17490
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.055666133761405945,
      "learning_rate": 4.4166666666666665e-05,
      "loss": 0.0028,
      "step": 17500
    },
    {
      "epoch": 0.9338666666666666,
      "grad_norm": 0.18208208680152893,
      "learning_rate": 4.416333333333333e-05,
      "loss": 0.003,
      "step": 17510
    },
    {
      "epoch": 0.9344,
      "grad_norm": 0.31622910499572754,
      "learning_rate": 4.4160000000000004e-05,
      "loss": 0.0037,
      "step": 17520
    },
    {
      "epoch": 0.9349333333333333,
      "grad_norm": 0.2477790266275406,
      "learning_rate": 4.415666666666667e-05,
      "loss": 0.0033,
      "step": 17530
    },
    {
      "epoch": 0.9354666666666667,
      "grad_norm": 0.44699832797050476,
      "learning_rate": 4.4153333333333336e-05,
      "loss": 0.0045,
      "step": 17540
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.15761370956897736,
      "learning_rate": 4.415e-05,
      "loss": 0.0031,
      "step": 17550
    },
    {
      "epoch": 0.9365333333333333,
      "grad_norm": 0.40401414036750793,
      "learning_rate": 4.414666666666667e-05,
      "loss": 0.0026,
      "step": 17560
    },
    {
      "epoch": 0.9370666666666667,
      "grad_norm": 0.06616265326738358,
      "learning_rate": 4.4143333333333335e-05,
      "loss": 0.0041,
      "step": 17570
    },
    {
      "epoch": 0.9376,
      "grad_norm": 0.05857870355248451,
      "learning_rate": 4.414e-05,
      "loss": 0.0038,
      "step": 17580
    },
    {
      "epoch": 0.9381333333333334,
      "grad_norm": 0.41822436451911926,
      "learning_rate": 4.4136666666666674e-05,
      "loss": 0.0024,
      "step": 17590
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 0.1860063225030899,
      "learning_rate": 4.413333333333334e-05,
      "loss": 0.0032,
      "step": 17600
    },
    {
      "epoch": 0.9392,
      "grad_norm": 0.4603993594646454,
      "learning_rate": 4.4130000000000006e-05,
      "loss": 0.0034,
      "step": 17610
    },
    {
      "epoch": 0.9397333333333333,
      "grad_norm": 0.28926146030426025,
      "learning_rate": 4.4126666666666665e-05,
      "loss": 0.0037,
      "step": 17620
    },
    {
      "epoch": 0.9402666666666667,
      "grad_norm": 0.03926633670926094,
      "learning_rate": 4.412333333333333e-05,
      "loss": 0.0043,
      "step": 17630
    },
    {
      "epoch": 0.9408,
      "grad_norm": 0.8510746955871582,
      "learning_rate": 4.412e-05,
      "loss": 0.0027,
      "step": 17640
    },
    {
      "epoch": 0.9413333333333334,
      "grad_norm": 0.09178582578897476,
      "learning_rate": 4.411666666666667e-05,
      "loss": 0.0034,
      "step": 17650
    },
    {
      "epoch": 0.9418666666666666,
      "grad_norm": 0.06391407549381256,
      "learning_rate": 4.411333333333334e-05,
      "loss": 0.0029,
      "step": 17660
    },
    {
      "epoch": 0.9424,
      "grad_norm": 0.0928279384970665,
      "learning_rate": 4.411e-05,
      "loss": 0.004,
      "step": 17670
    },
    {
      "epoch": 0.9429333333333333,
      "grad_norm": 0.22299066185951233,
      "learning_rate": 4.410666666666667e-05,
      "loss": 0.0026,
      "step": 17680
    },
    {
      "epoch": 0.9434666666666667,
      "grad_norm": 0.03891215845942497,
      "learning_rate": 4.4103333333333335e-05,
      "loss": 0.0026,
      "step": 17690
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.07665640860795975,
      "learning_rate": 4.41e-05,
      "loss": 0.0026,
      "step": 17700
    },
    {
      "epoch": 0.9445333333333333,
      "grad_norm": 0.13327328860759735,
      "learning_rate": 4.409666666666667e-05,
      "loss": 0.0028,
      "step": 17710
    },
    {
      "epoch": 0.9450666666666667,
      "grad_norm": 0.5256360173225403,
      "learning_rate": 4.4093333333333334e-05,
      "loss": 0.0026,
      "step": 17720
    },
    {
      "epoch": 0.9456,
      "grad_norm": 0.18380221724510193,
      "learning_rate": 4.4090000000000006e-05,
      "loss": 0.0034,
      "step": 17730
    },
    {
      "epoch": 0.9461333333333334,
      "grad_norm": 0.25315943360328674,
      "learning_rate": 4.408666666666667e-05,
      "loss": 0.0038,
      "step": 17740
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.24040676653385162,
      "learning_rate": 4.408333333333334e-05,
      "loss": 0.0023,
      "step": 17750
    },
    {
      "epoch": 0.9472,
      "grad_norm": 0.12303777039051056,
      "learning_rate": 4.4080000000000005e-05,
      "loss": 0.0029,
      "step": 17760
    },
    {
      "epoch": 0.9477333333333333,
      "grad_norm": 0.22209106385707855,
      "learning_rate": 4.4076666666666664e-05,
      "loss": 0.0035,
      "step": 17770
    },
    {
      "epoch": 0.9482666666666667,
      "grad_norm": 0.2182932198047638,
      "learning_rate": 4.407333333333333e-05,
      "loss": 0.0022,
      "step": 17780
    },
    {
      "epoch": 0.9488,
      "grad_norm": 0.19403687119483948,
      "learning_rate": 4.407e-05,
      "loss": 0.0031,
      "step": 17790
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 0.07050736248493195,
      "learning_rate": 4.406666666666667e-05,
      "loss": 0.003,
      "step": 17800
    },
    {
      "epoch": 0.9498666666666666,
      "grad_norm": 0.27579760551452637,
      "learning_rate": 4.4063333333333336e-05,
      "loss": 0.0027,
      "step": 17810
    },
    {
      "epoch": 0.9504,
      "grad_norm": 0.15504105389118195,
      "learning_rate": 4.406e-05,
      "loss": 0.0027,
      "step": 17820
    },
    {
      "epoch": 0.9509333333333333,
      "grad_norm": 0.24837398529052734,
      "learning_rate": 4.405666666666667e-05,
      "loss": 0.0021,
      "step": 17830
    },
    {
      "epoch": 0.9514666666666667,
      "grad_norm": 0.18666018545627594,
      "learning_rate": 4.4053333333333334e-05,
      "loss": 0.0031,
      "step": 17840
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.399314820766449,
      "learning_rate": 4.405e-05,
      "loss": 0.0036,
      "step": 17850
    },
    {
      "epoch": 0.9525333333333333,
      "grad_norm": 0.09239135682582855,
      "learning_rate": 4.4046666666666666e-05,
      "loss": 0.0025,
      "step": 17860
    },
    {
      "epoch": 0.9530666666666666,
      "grad_norm": 0.2646988034248352,
      "learning_rate": 4.404333333333334e-05,
      "loss": 0.0031,
      "step": 17870
    },
    {
      "epoch": 0.9536,
      "grad_norm": 0.13660696148872375,
      "learning_rate": 4.4040000000000005e-05,
      "loss": 0.0029,
      "step": 17880
    },
    {
      "epoch": 0.9541333333333334,
      "grad_norm": 0.9032781720161438,
      "learning_rate": 4.403666666666667e-05,
      "loss": 0.0042,
      "step": 17890
    },
    {
      "epoch": 0.9546666666666667,
      "grad_norm": 0.1277812421321869,
      "learning_rate": 4.403333333333334e-05,
      "loss": 0.0045,
      "step": 17900
    },
    {
      "epoch": 0.9552,
      "grad_norm": 0.4683713912963867,
      "learning_rate": 4.4030000000000004e-05,
      "loss": 0.0035,
      "step": 17910
    },
    {
      "epoch": 0.9557333333333333,
      "grad_norm": 0.3061996400356293,
      "learning_rate": 4.402666666666666e-05,
      "loss": 0.0035,
      "step": 17920
    },
    {
      "epoch": 0.9562666666666667,
      "grad_norm": 0.3712524175643921,
      "learning_rate": 4.4023333333333336e-05,
      "loss": 0.004,
      "step": 17930
    },
    {
      "epoch": 0.9568,
      "grad_norm": 0.2492106556892395,
      "learning_rate": 4.402e-05,
      "loss": 0.0046,
      "step": 17940
    },
    {
      "epoch": 0.9573333333333334,
      "grad_norm": 0.6987282037734985,
      "learning_rate": 4.401666666666667e-05,
      "loss": 0.0017,
      "step": 17950
    },
    {
      "epoch": 0.9578666666666666,
      "grad_norm": 0.44111475348472595,
      "learning_rate": 4.4013333333333334e-05,
      "loss": 0.0033,
      "step": 17960
    },
    {
      "epoch": 0.9584,
      "grad_norm": 0.4309259057044983,
      "learning_rate": 4.401e-05,
      "loss": 0.0027,
      "step": 17970
    },
    {
      "epoch": 0.9589333333333333,
      "grad_norm": 0.4381794333457947,
      "learning_rate": 4.4006666666666667e-05,
      "loss": 0.0027,
      "step": 17980
    },
    {
      "epoch": 0.9594666666666667,
      "grad_norm": 0.49182868003845215,
      "learning_rate": 4.400333333333333e-05,
      "loss": 0.0046,
      "step": 17990
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.686741292476654,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.0032,
      "step": 18000
    },
    {
      "epoch": 0.9605333333333334,
      "grad_norm": 0.5165054202079773,
      "learning_rate": 4.399666666666667e-05,
      "loss": 0.0023,
      "step": 18010
    },
    {
      "epoch": 0.9610666666666666,
      "grad_norm": 0.6214606165885925,
      "learning_rate": 4.399333333333334e-05,
      "loss": 0.0027,
      "step": 18020
    },
    {
      "epoch": 0.9616,
      "grad_norm": 0.18324287235736847,
      "learning_rate": 4.3990000000000004e-05,
      "loss": 0.0035,
      "step": 18030
    },
    {
      "epoch": 0.9621333333333333,
      "grad_norm": 0.6368140578269958,
      "learning_rate": 4.398666666666667e-05,
      "loss": 0.0027,
      "step": 18040
    },
    {
      "epoch": 0.9626666666666667,
      "grad_norm": 0.13492929935455322,
      "learning_rate": 4.3983333333333336e-05,
      "loss": 0.0026,
      "step": 18050
    },
    {
      "epoch": 0.9632,
      "grad_norm": 0.625331699848175,
      "learning_rate": 4.398e-05,
      "loss": 0.0042,
      "step": 18060
    },
    {
      "epoch": 0.9637333333333333,
      "grad_norm": 1.0196776390075684,
      "learning_rate": 4.397666666666667e-05,
      "loss": 0.0036,
      "step": 18070
    },
    {
      "epoch": 0.9642666666666667,
      "grad_norm": 0.3324970006942749,
      "learning_rate": 4.3973333333333335e-05,
      "loss": 0.0038,
      "step": 18080
    },
    {
      "epoch": 0.9648,
      "grad_norm": 0.3756411075592041,
      "learning_rate": 4.397e-05,
      "loss": 0.0031,
      "step": 18090
    },
    {
      "epoch": 0.9653333333333334,
      "grad_norm": 0.057553909718990326,
      "learning_rate": 4.396666666666667e-05,
      "loss": 0.0028,
      "step": 18100
    },
    {
      "epoch": 0.9658666666666667,
      "grad_norm": 0.7569223046302795,
      "learning_rate": 4.396333333333333e-05,
      "loss": 0.0028,
      "step": 18110
    },
    {
      "epoch": 0.9664,
      "grad_norm": 0.3404032588005066,
      "learning_rate": 4.396e-05,
      "loss": 0.0035,
      "step": 18120
    },
    {
      "epoch": 0.9669333333333333,
      "grad_norm": 0.03759157285094261,
      "learning_rate": 4.3956666666666665e-05,
      "loss": 0.004,
      "step": 18130
    },
    {
      "epoch": 0.9674666666666667,
      "grad_norm": 0.4027980864048004,
      "learning_rate": 4.395333333333334e-05,
      "loss": 0.0029,
      "step": 18140
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.24786075949668884,
      "learning_rate": 4.3950000000000004e-05,
      "loss": 0.0031,
      "step": 18150
    },
    {
      "epoch": 0.9685333333333334,
      "grad_norm": 0.0477655827999115,
      "learning_rate": 4.394666666666667e-05,
      "loss": 0.0026,
      "step": 18160
    },
    {
      "epoch": 0.9690666666666666,
      "grad_norm": 0.2871629595756531,
      "learning_rate": 4.394333333333334e-05,
      "loss": 0.0021,
      "step": 18170
    },
    {
      "epoch": 0.9696,
      "grad_norm": 0.26753613352775574,
      "learning_rate": 4.394e-05,
      "loss": 0.0034,
      "step": 18180
    },
    {
      "epoch": 0.9701333333333333,
      "grad_norm": 0.07941371947526932,
      "learning_rate": 4.393666666666667e-05,
      "loss": 0.0029,
      "step": 18190
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 0.15211597084999084,
      "learning_rate": 4.3933333333333335e-05,
      "loss": 0.0031,
      "step": 18200
    },
    {
      "epoch": 0.9712,
      "grad_norm": 0.1204652339220047,
      "learning_rate": 4.393e-05,
      "loss": 0.0027,
      "step": 18210
    },
    {
      "epoch": 0.9717333333333333,
      "grad_norm": 0.06278591603040695,
      "learning_rate": 4.3926666666666674e-05,
      "loss": 0.0029,
      "step": 18220
    },
    {
      "epoch": 0.9722666666666666,
      "grad_norm": 0.3406834006309509,
      "learning_rate": 4.3923333333333333e-05,
      "loss": 0.0034,
      "step": 18230
    },
    {
      "epoch": 0.9728,
      "grad_norm": 0.5509621500968933,
      "learning_rate": 4.392e-05,
      "loss": 0.0029,
      "step": 18240
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 0.40036964416503906,
      "learning_rate": 4.3916666666666666e-05,
      "loss": 0.0027,
      "step": 18250
    },
    {
      "epoch": 0.9738666666666667,
      "grad_norm": 0.5501179099082947,
      "learning_rate": 4.391333333333333e-05,
      "loss": 0.0019,
      "step": 18260
    },
    {
      "epoch": 0.9744,
      "grad_norm": 0.3500217795372009,
      "learning_rate": 4.391e-05,
      "loss": 0.0028,
      "step": 18270
    },
    {
      "epoch": 0.9749333333333333,
      "grad_norm": 0.6695248484611511,
      "learning_rate": 4.390666666666667e-05,
      "loss": 0.0029,
      "step": 18280
    },
    {
      "epoch": 0.9754666666666667,
      "grad_norm": 0.49816522002220154,
      "learning_rate": 4.390333333333334e-05,
      "loss": 0.0036,
      "step": 18290
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.06687051057815552,
      "learning_rate": 4.39e-05,
      "loss": 0.0043,
      "step": 18300
    },
    {
      "epoch": 0.9765333333333334,
      "grad_norm": 0.15864871442317963,
      "learning_rate": 4.389666666666667e-05,
      "loss": 0.0046,
      "step": 18310
    },
    {
      "epoch": 0.9770666666666666,
      "grad_norm": 0.06444007903337479,
      "learning_rate": 4.3893333333333335e-05,
      "loss": 0.0038,
      "step": 18320
    },
    {
      "epoch": 0.9776,
      "grad_norm": 0.3994511365890503,
      "learning_rate": 4.389e-05,
      "loss": 0.003,
      "step": 18330
    },
    {
      "epoch": 0.9781333333333333,
      "grad_norm": 0.253980427980423,
      "learning_rate": 4.388666666666667e-05,
      "loss": 0.0031,
      "step": 18340
    },
    {
      "epoch": 0.9786666666666667,
      "grad_norm": 0.09421908110380173,
      "learning_rate": 4.388333333333334e-05,
      "loss": 0.0027,
      "step": 18350
    },
    {
      "epoch": 0.9792,
      "grad_norm": 0.1826484203338623,
      "learning_rate": 4.388000000000001e-05,
      "loss": 0.0028,
      "step": 18360
    },
    {
      "epoch": 0.9797333333333333,
      "grad_norm": 0.16444747149944305,
      "learning_rate": 4.387666666666667e-05,
      "loss": 0.0028,
      "step": 18370
    },
    {
      "epoch": 0.9802666666666666,
      "grad_norm": 0.550788402557373,
      "learning_rate": 4.387333333333333e-05,
      "loss": 0.0024,
      "step": 18380
    },
    {
      "epoch": 0.9808,
      "grad_norm": 0.32148584723472595,
      "learning_rate": 4.387e-05,
      "loss": 0.0027,
      "step": 18390
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 0.5842958092689514,
      "learning_rate": 4.3866666666666665e-05,
      "loss": 0.0033,
      "step": 18400
    },
    {
      "epoch": 0.9818666666666667,
      "grad_norm": 0.09184355288743973,
      "learning_rate": 4.386333333333333e-05,
      "loss": 0.003,
      "step": 18410
    },
    {
      "epoch": 0.9824,
      "grad_norm": 0.5601971745491028,
      "learning_rate": 4.3860000000000004e-05,
      "loss": 0.0039,
      "step": 18420
    },
    {
      "epoch": 0.9829333333333333,
      "grad_norm": 0.2518005669116974,
      "learning_rate": 4.385666666666667e-05,
      "loss": 0.0022,
      "step": 18430
    },
    {
      "epoch": 0.9834666666666667,
      "grad_norm": 0.22449111938476562,
      "learning_rate": 4.3853333333333336e-05,
      "loss": 0.0038,
      "step": 18440
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.05801733583211899,
      "learning_rate": 4.385e-05,
      "loss": 0.0024,
      "step": 18450
    },
    {
      "epoch": 0.9845333333333334,
      "grad_norm": 0.6134693026542664,
      "learning_rate": 4.384666666666667e-05,
      "loss": 0.0035,
      "step": 18460
    },
    {
      "epoch": 0.9850666666666666,
      "grad_norm": 0.19035224616527557,
      "learning_rate": 4.3843333333333334e-05,
      "loss": 0.0033,
      "step": 18470
    },
    {
      "epoch": 0.9856,
      "grad_norm": 0.594687819480896,
      "learning_rate": 4.384e-05,
      "loss": 0.0028,
      "step": 18480
    },
    {
      "epoch": 0.9861333333333333,
      "grad_norm": 0.42944541573524475,
      "learning_rate": 4.383666666666667e-05,
      "loss": 0.0028,
      "step": 18490
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.2724234461784363,
      "learning_rate": 4.383333333333334e-05,
      "loss": 0.0017,
      "step": 18500
    },
    {
      "epoch": 0.9872,
      "grad_norm": 0.2166833132505417,
      "learning_rate": 4.3830000000000006e-05,
      "loss": 0.0042,
      "step": 18510
    },
    {
      "epoch": 0.9877333333333334,
      "grad_norm": 0.21365951001644135,
      "learning_rate": 4.382666666666667e-05,
      "loss": 0.0036,
      "step": 18520
    },
    {
      "epoch": 0.9882666666666666,
      "grad_norm": 0.40725234150886536,
      "learning_rate": 4.382333333333333e-05,
      "loss": 0.0028,
      "step": 18530
    },
    {
      "epoch": 0.9888,
      "grad_norm": 0.5450185537338257,
      "learning_rate": 4.382e-05,
      "loss": 0.0024,
      "step": 18540
    },
    {
      "epoch": 0.9893333333333333,
      "grad_norm": 0.15220224857330322,
      "learning_rate": 4.381666666666667e-05,
      "loss": 0.003,
      "step": 18550
    },
    {
      "epoch": 0.9898666666666667,
      "grad_norm": 0.27343839406967163,
      "learning_rate": 4.3813333333333336e-05,
      "loss": 0.0027,
      "step": 18560
    },
    {
      "epoch": 0.9904,
      "grad_norm": 0.525724470615387,
      "learning_rate": 4.381e-05,
      "loss": 0.0029,
      "step": 18570
    },
    {
      "epoch": 0.9909333333333333,
      "grad_norm": 0.5432611703872681,
      "learning_rate": 4.380666666666667e-05,
      "loss": 0.0037,
      "step": 18580
    },
    {
      "epoch": 0.9914666666666667,
      "grad_norm": 0.023685215041041374,
      "learning_rate": 4.3803333333333335e-05,
      "loss": 0.0039,
      "step": 18590
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.18236349523067474,
      "learning_rate": 4.38e-05,
      "loss": 0.0034,
      "step": 18600
    },
    {
      "epoch": 0.9925333333333334,
      "grad_norm": 0.3232143819332123,
      "learning_rate": 4.379666666666667e-05,
      "loss": 0.0025,
      "step": 18610
    },
    {
      "epoch": 0.9930666666666667,
      "grad_norm": 0.2749393582344055,
      "learning_rate": 4.379333333333333e-05,
      "loss": 0.0037,
      "step": 18620
    },
    {
      "epoch": 0.9936,
      "grad_norm": 0.6670064926147461,
      "learning_rate": 4.3790000000000006e-05,
      "loss": 0.0023,
      "step": 18630
    },
    {
      "epoch": 0.9941333333333333,
      "grad_norm": 0.1662471741437912,
      "learning_rate": 4.378666666666667e-05,
      "loss": 0.0029,
      "step": 18640
    },
    {
      "epoch": 0.9946666666666667,
      "grad_norm": 0.08123083412647247,
      "learning_rate": 4.378333333333334e-05,
      "loss": 0.0031,
      "step": 18650
    },
    {
      "epoch": 0.9952,
      "grad_norm": 0.3729075789451599,
      "learning_rate": 4.3780000000000004e-05,
      "loss": 0.0038,
      "step": 18660
    },
    {
      "epoch": 0.9957333333333334,
      "grad_norm": 0.2918183207511902,
      "learning_rate": 4.377666666666667e-05,
      "loss": 0.0036,
      "step": 18670
    },
    {
      "epoch": 0.9962666666666666,
      "grad_norm": 0.5474648475646973,
      "learning_rate": 4.377333333333333e-05,
      "loss": 0.0033,
      "step": 18680
    },
    {
      "epoch": 0.9968,
      "grad_norm": 0.24906526505947113,
      "learning_rate": 4.377e-05,
      "loss": 0.0037,
      "step": 18690
    },
    {
      "epoch": 0.9973333333333333,
      "grad_norm": 0.12208084017038345,
      "learning_rate": 4.376666666666667e-05,
      "loss": 0.0026,
      "step": 18700
    },
    {
      "epoch": 0.9978666666666667,
      "grad_norm": 0.18698690831661224,
      "learning_rate": 4.3763333333333335e-05,
      "loss": 0.0025,
      "step": 18710
    },
    {
      "epoch": 0.9984,
      "grad_norm": 0.3075794279575348,
      "learning_rate": 4.376e-05,
      "loss": 0.0031,
      "step": 18720
    },
    {
      "epoch": 0.9989333333333333,
      "grad_norm": 0.16044877469539642,
      "learning_rate": 4.375666666666667e-05,
      "loss": 0.0042,
      "step": 18730
    },
    {
      "epoch": 0.9994666666666666,
      "grad_norm": 0.13062003254890442,
      "learning_rate": 4.3753333333333333e-05,
      "loss": 0.0032,
      "step": 18740
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.22408592700958252,
      "learning_rate": 4.375e-05,
      "loss": 0.0023,
      "step": 18750
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.0030052883084863424,
      "eval_runtime": 165.816,
      "eval_samples_per_second": 1507.695,
      "eval_steps_per_second": 37.692,
      "step": 18750
    },
    {
      "epoch": 1.0005333333333333,
      "grad_norm": 0.1923009306192398,
      "learning_rate": 4.374666666666667e-05,
      "loss": 0.0037,
      "step": 18760
    },
    {
      "epoch": 1.0010666666666668,
      "grad_norm": 0.7382591962814331,
      "learning_rate": 4.374333333333334e-05,
      "loss": 0.0031,
      "step": 18770
    },
    {
      "epoch": 1.0016,
      "grad_norm": 0.28060388565063477,
      "learning_rate": 4.3740000000000005e-05,
      "loss": 0.0028,
      "step": 18780
    },
    {
      "epoch": 1.0021333333333333,
      "grad_norm": 0.7028530836105347,
      "learning_rate": 4.373666666666667e-05,
      "loss": 0.0028,
      "step": 18790
    },
    {
      "epoch": 1.0026666666666666,
      "grad_norm": 0.46020635962486267,
      "learning_rate": 4.373333333333334e-05,
      "loss": 0.0022,
      "step": 18800
    },
    {
      "epoch": 1.0032,
      "grad_norm": 0.12756432592868805,
      "learning_rate": 4.373e-05,
      "loss": 0.0036,
      "step": 18810
    },
    {
      "epoch": 1.0037333333333334,
      "grad_norm": 0.06363651156425476,
      "learning_rate": 4.372666666666667e-05,
      "loss": 0.0024,
      "step": 18820
    },
    {
      "epoch": 1.0042666666666666,
      "grad_norm": 0.0532558374106884,
      "learning_rate": 4.3723333333333335e-05,
      "loss": 0.0029,
      "step": 18830
    },
    {
      "epoch": 1.0048,
      "grad_norm": 0.4786093235015869,
      "learning_rate": 4.372e-05,
      "loss": 0.0036,
      "step": 18840
    },
    {
      "epoch": 1.0053333333333334,
      "grad_norm": 0.6676138043403625,
      "learning_rate": 4.371666666666667e-05,
      "loss": 0.0023,
      "step": 18850
    },
    {
      "epoch": 1.0058666666666667,
      "grad_norm": 0.05347860977053642,
      "learning_rate": 4.3713333333333334e-05,
      "loss": 0.0041,
      "step": 18860
    },
    {
      "epoch": 1.0064,
      "grad_norm": 0.5558857321739197,
      "learning_rate": 4.371e-05,
      "loss": 0.0018,
      "step": 18870
    },
    {
      "epoch": 1.0069333333333332,
      "grad_norm": 0.1315057873725891,
      "learning_rate": 4.3706666666666666e-05,
      "loss": 0.0035,
      "step": 18880
    },
    {
      "epoch": 1.0074666666666667,
      "grad_norm": 0.2265521138906479,
      "learning_rate": 4.370333333333333e-05,
      "loss": 0.0034,
      "step": 18890
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.3050956726074219,
      "learning_rate": 4.3700000000000005e-05,
      "loss": 0.0023,
      "step": 18900
    },
    {
      "epoch": 1.0085333333333333,
      "grad_norm": 0.21890029311180115,
      "learning_rate": 4.369666666666667e-05,
      "loss": 0.003,
      "step": 18910
    },
    {
      "epoch": 1.0090666666666666,
      "grad_norm": 0.8274590373039246,
      "learning_rate": 4.369333333333334e-05,
      "loss": 0.0033,
      "step": 18920
    },
    {
      "epoch": 1.0096,
      "grad_norm": 0.31515297293663025,
      "learning_rate": 4.3690000000000004e-05,
      "loss": 0.0031,
      "step": 18930
    },
    {
      "epoch": 1.0101333333333333,
      "grad_norm": 0.24202807247638702,
      "learning_rate": 4.368666666666667e-05,
      "loss": 0.0032,
      "step": 18940
    },
    {
      "epoch": 1.0106666666666666,
      "grad_norm": 0.435037761926651,
      "learning_rate": 4.3683333333333336e-05,
      "loss": 0.0037,
      "step": 18950
    },
    {
      "epoch": 1.0112,
      "grad_norm": 0.10406164079904556,
      "learning_rate": 4.368e-05,
      "loss": 0.0033,
      "step": 18960
    },
    {
      "epoch": 1.0117333333333334,
      "grad_norm": 0.5839012861251831,
      "learning_rate": 4.367666666666667e-05,
      "loss": 0.0029,
      "step": 18970
    },
    {
      "epoch": 1.0122666666666666,
      "grad_norm": 0.4881952106952667,
      "learning_rate": 4.3673333333333334e-05,
      "loss": 0.0034,
      "step": 18980
    },
    {
      "epoch": 1.0128,
      "grad_norm": 0.1714302897453308,
      "learning_rate": 4.367e-05,
      "loss": 0.0028,
      "step": 18990
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.10684707760810852,
      "learning_rate": 4.3666666666666666e-05,
      "loss": 0.0023,
      "step": 19000
    },
    {
      "epoch": 1.0138666666666667,
      "grad_norm": 0.5788412690162659,
      "learning_rate": 4.366333333333333e-05,
      "loss": 0.0021,
      "step": 19010
    },
    {
      "epoch": 1.0144,
      "grad_norm": 0.4882923662662506,
      "learning_rate": 4.366e-05,
      "loss": 0.0029,
      "step": 19020
    },
    {
      "epoch": 1.0149333333333332,
      "grad_norm": 0.34036847949028015,
      "learning_rate": 4.3656666666666665e-05,
      "loss": 0.0033,
      "step": 19030
    },
    {
      "epoch": 1.0154666666666667,
      "grad_norm": 0.35884156823158264,
      "learning_rate": 4.365333333333334e-05,
      "loss": 0.003,
      "step": 19040
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.29688283801078796,
      "learning_rate": 4.3650000000000004e-05,
      "loss": 0.0042,
      "step": 19050
    },
    {
      "epoch": 1.0165333333333333,
      "grad_norm": 0.7648351192474365,
      "learning_rate": 4.364666666666667e-05,
      "loss": 0.0021,
      "step": 19060
    },
    {
      "epoch": 1.0170666666666666,
      "grad_norm": 0.2484065145254135,
      "learning_rate": 4.3643333333333336e-05,
      "loss": 0.0022,
      "step": 19070
    },
    {
      "epoch": 1.0176,
      "grad_norm": 0.2545064389705658,
      "learning_rate": 4.364e-05,
      "loss": 0.0024,
      "step": 19080
    },
    {
      "epoch": 1.0181333333333333,
      "grad_norm": 0.06547784805297852,
      "learning_rate": 4.363666666666667e-05,
      "loss": 0.0038,
      "step": 19090
    },
    {
      "epoch": 1.0186666666666666,
      "grad_norm": 0.27951163053512573,
      "learning_rate": 4.3633333333333335e-05,
      "loss": 0.0024,
      "step": 19100
    },
    {
      "epoch": 1.0192,
      "grad_norm": 0.3338427245616913,
      "learning_rate": 4.363000000000001e-05,
      "loss": 0.0042,
      "step": 19110
    },
    {
      "epoch": 1.0197333333333334,
      "grad_norm": 0.03552774339914322,
      "learning_rate": 4.3626666666666674e-05,
      "loss": 0.0032,
      "step": 19120
    },
    {
      "epoch": 1.0202666666666667,
      "grad_norm": 0.09340377151966095,
      "learning_rate": 4.362333333333333e-05,
      "loss": 0.0031,
      "step": 19130
    },
    {
      "epoch": 1.0208,
      "grad_norm": 0.1346437931060791,
      "learning_rate": 4.362e-05,
      "loss": 0.0038,
      "step": 19140
    },
    {
      "epoch": 1.0213333333333334,
      "grad_norm": 0.4618057310581207,
      "learning_rate": 4.3616666666666665e-05,
      "loss": 0.0032,
      "step": 19150
    },
    {
      "epoch": 1.0218666666666667,
      "grad_norm": 0.30717042088508606,
      "learning_rate": 4.361333333333333e-05,
      "loss": 0.0017,
      "step": 19160
    },
    {
      "epoch": 1.0224,
      "grad_norm": 0.40380269289016724,
      "learning_rate": 4.361e-05,
      "loss": 0.003,
      "step": 19170
    },
    {
      "epoch": 1.0229333333333333,
      "grad_norm": 0.217926025390625,
      "learning_rate": 4.360666666666667e-05,
      "loss": 0.0047,
      "step": 19180
    },
    {
      "epoch": 1.0234666666666667,
      "grad_norm": 0.19069959223270416,
      "learning_rate": 4.3603333333333337e-05,
      "loss": 0.0033,
      "step": 19190
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.5140453577041626,
      "learning_rate": 4.36e-05,
      "loss": 0.0034,
      "step": 19200
    },
    {
      "epoch": 1.0245333333333333,
      "grad_norm": 0.19102168083190918,
      "learning_rate": 4.359666666666667e-05,
      "loss": 0.0025,
      "step": 19210
    },
    {
      "epoch": 1.0250666666666666,
      "grad_norm": 0.19417665898799896,
      "learning_rate": 4.3593333333333335e-05,
      "loss": 0.0049,
      "step": 19220
    },
    {
      "epoch": 1.0256,
      "grad_norm": 0.18474382162094116,
      "learning_rate": 4.359e-05,
      "loss": 0.0022,
      "step": 19230
    },
    {
      "epoch": 1.0261333333333333,
      "grad_norm": 0.12114864587783813,
      "learning_rate": 4.358666666666667e-05,
      "loss": 0.0029,
      "step": 19240
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 0.12085841596126556,
      "learning_rate": 4.358333333333334e-05,
      "loss": 0.0037,
      "step": 19250
    },
    {
      "epoch": 1.0272,
      "grad_norm": 0.27002882957458496,
      "learning_rate": 4.3580000000000006e-05,
      "loss": 0.0031,
      "step": 19260
    },
    {
      "epoch": 1.0277333333333334,
      "grad_norm": 0.0993526428937912,
      "learning_rate": 4.357666666666667e-05,
      "loss": 0.003,
      "step": 19270
    },
    {
      "epoch": 1.0282666666666667,
      "grad_norm": 0.3159332871437073,
      "learning_rate": 4.357333333333333e-05,
      "loss": 0.0029,
      "step": 19280
    },
    {
      "epoch": 1.0288,
      "grad_norm": 0.08260144293308258,
      "learning_rate": 4.357e-05,
      "loss": 0.0033,
      "step": 19290
    },
    {
      "epoch": 1.0293333333333334,
      "grad_norm": 0.42820972204208374,
      "learning_rate": 4.3566666666666664e-05,
      "loss": 0.0028,
      "step": 19300
    },
    {
      "epoch": 1.0298666666666667,
      "grad_norm": 0.12995997071266174,
      "learning_rate": 4.356333333333334e-05,
      "loss": 0.0032,
      "step": 19310
    },
    {
      "epoch": 1.0304,
      "grad_norm": 0.18535563349723816,
      "learning_rate": 4.356e-05,
      "loss": 0.0031,
      "step": 19320
    },
    {
      "epoch": 1.0309333333333333,
      "grad_norm": 0.12696968019008636,
      "learning_rate": 4.355666666666667e-05,
      "loss": 0.003,
      "step": 19330
    },
    {
      "epoch": 1.0314666666666668,
      "grad_norm": 0.3012247383594513,
      "learning_rate": 4.3553333333333335e-05,
      "loss": 0.0032,
      "step": 19340
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.1330152302980423,
      "learning_rate": 4.355e-05,
      "loss": 0.0043,
      "step": 19350
    },
    {
      "epoch": 1.0325333333333333,
      "grad_norm": 0.4933018982410431,
      "learning_rate": 4.354666666666667e-05,
      "loss": 0.0047,
      "step": 19360
    },
    {
      "epoch": 1.0330666666666666,
      "grad_norm": 0.597838282585144,
      "learning_rate": 4.3543333333333334e-05,
      "loss": 0.0025,
      "step": 19370
    },
    {
      "epoch": 1.0336,
      "grad_norm": 0.12386026978492737,
      "learning_rate": 4.354e-05,
      "loss": 0.0031,
      "step": 19380
    },
    {
      "epoch": 1.0341333333333333,
      "grad_norm": 0.21600817143917084,
      "learning_rate": 4.353666666666667e-05,
      "loss": 0.0035,
      "step": 19390
    },
    {
      "epoch": 1.0346666666666666,
      "grad_norm": 0.25409412384033203,
      "learning_rate": 4.353333333333334e-05,
      "loss": 0.0042,
      "step": 19400
    },
    {
      "epoch": 1.0352,
      "grad_norm": 0.43355247378349304,
      "learning_rate": 4.3530000000000005e-05,
      "loss": 0.0034,
      "step": 19410
    },
    {
      "epoch": 1.0357333333333334,
      "grad_norm": 0.514552652835846,
      "learning_rate": 4.352666666666667e-05,
      "loss": 0.0037,
      "step": 19420
    },
    {
      "epoch": 1.0362666666666667,
      "grad_norm": 0.4134940207004547,
      "learning_rate": 4.352333333333334e-05,
      "loss": 0.0022,
      "step": 19430
    },
    {
      "epoch": 1.0368,
      "grad_norm": 0.6400214433670044,
      "learning_rate": 4.352e-05,
      "loss": 0.0029,
      "step": 19440
    },
    {
      "epoch": 1.0373333333333334,
      "grad_norm": 0.12870711088180542,
      "learning_rate": 4.351666666666667e-05,
      "loss": 0.0039,
      "step": 19450
    },
    {
      "epoch": 1.0378666666666667,
      "grad_norm": 0.36779454350471497,
      "learning_rate": 4.3513333333333336e-05,
      "loss": 0.0033,
      "step": 19460
    },
    {
      "epoch": 1.0384,
      "grad_norm": 0.1817908138036728,
      "learning_rate": 4.351e-05,
      "loss": 0.0036,
      "step": 19470
    },
    {
      "epoch": 1.0389333333333333,
      "grad_norm": 0.459845632314682,
      "learning_rate": 4.350666666666667e-05,
      "loss": 0.0026,
      "step": 19480
    },
    {
      "epoch": 1.0394666666666668,
      "grad_norm": 0.18474555015563965,
      "learning_rate": 4.3503333333333334e-05,
      "loss": 0.0028,
      "step": 19490
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.578342080116272,
      "learning_rate": 4.35e-05,
      "loss": 0.0025,
      "step": 19500
    },
    {
      "epoch": 1.0405333333333333,
      "grad_norm": 0.04678688198328018,
      "learning_rate": 4.3496666666666666e-05,
      "loss": 0.0039,
      "step": 19510
    },
    {
      "epoch": 1.0410666666666666,
      "grad_norm": 0.21778443455696106,
      "learning_rate": 4.349333333333334e-05,
      "loss": 0.0026,
      "step": 19520
    },
    {
      "epoch": 1.0416,
      "grad_norm": 0.550157904624939,
      "learning_rate": 4.3490000000000005e-05,
      "loss": 0.0037,
      "step": 19530
    },
    {
      "epoch": 1.0421333333333334,
      "grad_norm": 0.5127987265586853,
      "learning_rate": 4.348666666666667e-05,
      "loss": 0.0044,
      "step": 19540
    },
    {
      "epoch": 1.0426666666666666,
      "grad_norm": 0.7319621443748474,
      "learning_rate": 4.348333333333334e-05,
      "loss": 0.0039,
      "step": 19550
    },
    {
      "epoch": 1.0432,
      "grad_norm": 0.6440523862838745,
      "learning_rate": 4.3480000000000004e-05,
      "loss": 0.0032,
      "step": 19560
    },
    {
      "epoch": 1.0437333333333334,
      "grad_norm": 0.031534504145383835,
      "learning_rate": 4.347666666666667e-05,
      "loss": 0.0047,
      "step": 19570
    },
    {
      "epoch": 1.0442666666666667,
      "grad_norm": 0.48941564559936523,
      "learning_rate": 4.3473333333333336e-05,
      "loss": 0.0025,
      "step": 19580
    },
    {
      "epoch": 1.0448,
      "grad_norm": 0.46957218647003174,
      "learning_rate": 4.347e-05,
      "loss": 0.0041,
      "step": 19590
    },
    {
      "epoch": 1.0453333333333332,
      "grad_norm": 0.09644626826047897,
      "learning_rate": 4.346666666666667e-05,
      "loss": 0.0035,
      "step": 19600
    },
    {
      "epoch": 1.0458666666666667,
      "grad_norm": 0.37028512358665466,
      "learning_rate": 4.3463333333333335e-05,
      "loss": 0.0029,
      "step": 19610
    },
    {
      "epoch": 1.0464,
      "grad_norm": 0.7298440337181091,
      "learning_rate": 4.346e-05,
      "loss": 0.003,
      "step": 19620
    },
    {
      "epoch": 1.0469333333333333,
      "grad_norm": 0.27627280354499817,
      "learning_rate": 4.345666666666667e-05,
      "loss": 0.0017,
      "step": 19630
    },
    {
      "epoch": 1.0474666666666668,
      "grad_norm": 0.21375983953475952,
      "learning_rate": 4.345333333333333e-05,
      "loss": 0.0027,
      "step": 19640
    },
    {
      "epoch": 1.048,
      "grad_norm": 0.19236400723457336,
      "learning_rate": 4.345e-05,
      "loss": 0.0043,
      "step": 19650
    },
    {
      "epoch": 1.0485333333333333,
      "grad_norm": 0.5853214859962463,
      "learning_rate": 4.344666666666667e-05,
      "loss": 0.0027,
      "step": 19660
    },
    {
      "epoch": 1.0490666666666666,
      "grad_norm": 0.48747149109840393,
      "learning_rate": 4.344333333333334e-05,
      "loss": 0.0034,
      "step": 19670
    },
    {
      "epoch": 1.0496,
      "grad_norm": 0.15127816796302795,
      "learning_rate": 4.3440000000000004e-05,
      "loss": 0.0033,
      "step": 19680
    },
    {
      "epoch": 1.0501333333333334,
      "grad_norm": 0.2408134639263153,
      "learning_rate": 4.343666666666667e-05,
      "loss": 0.0031,
      "step": 19690
    },
    {
      "epoch": 1.0506666666666666,
      "grad_norm": 0.24869763851165771,
      "learning_rate": 4.3433333333333336e-05,
      "loss": 0.0031,
      "step": 19700
    },
    {
      "epoch": 1.0512,
      "grad_norm": 0.24269111454486847,
      "learning_rate": 4.343e-05,
      "loss": 0.0034,
      "step": 19710
    },
    {
      "epoch": 1.0517333333333334,
      "grad_norm": 0.07495997846126556,
      "learning_rate": 4.342666666666667e-05,
      "loss": 0.0026,
      "step": 19720
    },
    {
      "epoch": 1.0522666666666667,
      "grad_norm": 0.06126057356595993,
      "learning_rate": 4.3423333333333335e-05,
      "loss": 0.0028,
      "step": 19730
    },
    {
      "epoch": 1.0528,
      "grad_norm": 0.09099997580051422,
      "learning_rate": 4.342e-05,
      "loss": 0.0023,
      "step": 19740
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.2605595886707306,
      "learning_rate": 4.341666666666667e-05,
      "loss": 0.0021,
      "step": 19750
    },
    {
      "epoch": 1.0538666666666667,
      "grad_norm": 0.10011176764965057,
      "learning_rate": 4.341333333333333e-05,
      "loss": 0.0027,
      "step": 19760
    },
    {
      "epoch": 1.0544,
      "grad_norm": 0.10319016128778458,
      "learning_rate": 4.341e-05,
      "loss": 0.0037,
      "step": 19770
    },
    {
      "epoch": 1.0549333333333333,
      "grad_norm": 0.05452517047524452,
      "learning_rate": 4.3406666666666666e-05,
      "loss": 0.0036,
      "step": 19780
    },
    {
      "epoch": 1.0554666666666668,
      "grad_norm": 0.18467527627944946,
      "learning_rate": 4.340333333333333e-05,
      "loss": 0.0043,
      "step": 19790
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.09539172798395157,
      "learning_rate": 4.3400000000000005e-05,
      "loss": 0.0031,
      "step": 19800
    },
    {
      "epoch": 1.0565333333333333,
      "grad_norm": 0.25282326340675354,
      "learning_rate": 4.339666666666667e-05,
      "loss": 0.0027,
      "step": 19810
    },
    {
      "epoch": 1.0570666666666666,
      "grad_norm": 0.09129011631011963,
      "learning_rate": 4.339333333333334e-05,
      "loss": 0.0032,
      "step": 19820
    },
    {
      "epoch": 1.0576,
      "grad_norm": 0.5811737179756165,
      "learning_rate": 4.339e-05,
      "loss": 0.0025,
      "step": 19830
    },
    {
      "epoch": 1.0581333333333334,
      "grad_norm": 0.5511903762817383,
      "learning_rate": 4.338666666666667e-05,
      "loss": 0.0037,
      "step": 19840
    },
    {
      "epoch": 1.0586666666666666,
      "grad_norm": 0.09607737511396408,
      "learning_rate": 4.3383333333333335e-05,
      "loss": 0.0022,
      "step": 19850
    },
    {
      "epoch": 1.0592,
      "grad_norm": 0.2604624032974243,
      "learning_rate": 4.338e-05,
      "loss": 0.003,
      "step": 19860
    },
    {
      "epoch": 1.0597333333333334,
      "grad_norm": 0.1538117527961731,
      "learning_rate": 4.3376666666666674e-05,
      "loss": 0.0033,
      "step": 19870
    },
    {
      "epoch": 1.0602666666666667,
      "grad_norm": 0.13073007762432098,
      "learning_rate": 4.337333333333334e-05,
      "loss": 0.0032,
      "step": 19880
    },
    {
      "epoch": 1.0608,
      "grad_norm": 0.514761209487915,
      "learning_rate": 4.337e-05,
      "loss": 0.0037,
      "step": 19890
    },
    {
      "epoch": 1.0613333333333332,
      "grad_norm": 0.15152637660503387,
      "learning_rate": 4.3366666666666666e-05,
      "loss": 0.0026,
      "step": 19900
    },
    {
      "epoch": 1.0618666666666667,
      "grad_norm": 0.7550262808799744,
      "learning_rate": 4.336333333333333e-05,
      "loss": 0.0033,
      "step": 19910
    },
    {
      "epoch": 1.0624,
      "grad_norm": 0.18315893411636353,
      "learning_rate": 4.336e-05,
      "loss": 0.0031,
      "step": 19920
    },
    {
      "epoch": 1.0629333333333333,
      "grad_norm": 0.1901169717311859,
      "learning_rate": 4.3356666666666664e-05,
      "loss": 0.0029,
      "step": 19930
    },
    {
      "epoch": 1.0634666666666668,
      "grad_norm": 0.36277487874031067,
      "learning_rate": 4.335333333333334e-05,
      "loss": 0.0032,
      "step": 19940
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.18418465554714203,
      "learning_rate": 4.335e-05,
      "loss": 0.0037,
      "step": 19950
    },
    {
      "epoch": 1.0645333333333333,
      "grad_norm": 0.16604219377040863,
      "learning_rate": 4.334666666666667e-05,
      "loss": 0.0032,
      "step": 19960
    },
    {
      "epoch": 1.0650666666666666,
      "grad_norm": 0.40150827169418335,
      "learning_rate": 4.3343333333333336e-05,
      "loss": 0.003,
      "step": 19970
    },
    {
      "epoch": 1.0656,
      "grad_norm": 0.3023439645767212,
      "learning_rate": 4.334e-05,
      "loss": 0.0036,
      "step": 19980
    },
    {
      "epoch": 1.0661333333333334,
      "grad_norm": 0.4545976519584656,
      "learning_rate": 4.333666666666667e-05,
      "loss": 0.0023,
      "step": 19990
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.13037990033626556,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 0.0026,
      "step": 20000
    },
    {
      "epoch": 1.0672,
      "grad_norm": 0.07028298825025558,
      "learning_rate": 4.333000000000001e-05,
      "loss": 0.0038,
      "step": 20010
    },
    {
      "epoch": 1.0677333333333334,
      "grad_norm": 0.12201914191246033,
      "learning_rate": 4.332666666666667e-05,
      "loss": 0.0034,
      "step": 20020
    },
    {
      "epoch": 1.0682666666666667,
      "grad_norm": 0.030655985698103905,
      "learning_rate": 4.332333333333334e-05,
      "loss": 0.0029,
      "step": 20030
    },
    {
      "epoch": 1.0688,
      "grad_norm": 0.24388715624809265,
      "learning_rate": 4.332e-05,
      "loss": 0.0029,
      "step": 20040
    },
    {
      "epoch": 1.0693333333333332,
      "grad_norm": 0.232471764087677,
      "learning_rate": 4.3316666666666665e-05,
      "loss": 0.0027,
      "step": 20050
    },
    {
      "epoch": 1.0698666666666667,
      "grad_norm": 0.24916715919971466,
      "learning_rate": 4.331333333333333e-05,
      "loss": 0.0028,
      "step": 20060
    },
    {
      "epoch": 1.0704,
      "grad_norm": 0.10871872305870056,
      "learning_rate": 4.3310000000000004e-05,
      "loss": 0.0034,
      "step": 20070
    },
    {
      "epoch": 1.0709333333333333,
      "grad_norm": 0.4238196015357971,
      "learning_rate": 4.330666666666667e-05,
      "loss": 0.0022,
      "step": 20080
    },
    {
      "epoch": 1.0714666666666666,
      "grad_norm": 0.22749538719654083,
      "learning_rate": 4.3303333333333336e-05,
      "loss": 0.0043,
      "step": 20090
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.2410288155078888,
      "learning_rate": 4.33e-05,
      "loss": 0.0032,
      "step": 20100
    },
    {
      "epoch": 1.0725333333333333,
      "grad_norm": 0.09884606301784515,
      "learning_rate": 4.329666666666667e-05,
      "loss": 0.0039,
      "step": 20110
    },
    {
      "epoch": 1.0730666666666666,
      "grad_norm": 0.15229253470897675,
      "learning_rate": 4.3293333333333334e-05,
      "loss": 0.0038,
      "step": 20120
    },
    {
      "epoch": 1.0735999999999999,
      "grad_norm": 0.2150198072195053,
      "learning_rate": 4.329e-05,
      "loss": 0.0034,
      "step": 20130
    },
    {
      "epoch": 1.0741333333333334,
      "grad_norm": 0.0776536613702774,
      "learning_rate": 4.328666666666667e-05,
      "loss": 0.004,
      "step": 20140
    },
    {
      "epoch": 1.0746666666666667,
      "grad_norm": 0.21231700479984283,
      "learning_rate": 4.328333333333334e-05,
      "loss": 0.0021,
      "step": 20150
    },
    {
      "epoch": 1.0752,
      "grad_norm": 0.3587462902069092,
      "learning_rate": 4.3280000000000006e-05,
      "loss": 0.0032,
      "step": 20160
    },
    {
      "epoch": 1.0757333333333334,
      "grad_norm": 0.07604555040597916,
      "learning_rate": 4.327666666666667e-05,
      "loss": 0.0027,
      "step": 20170
    },
    {
      "epoch": 1.0762666666666667,
      "grad_norm": 0.18988510966300964,
      "learning_rate": 4.327333333333334e-05,
      "loss": 0.0029,
      "step": 20180
    },
    {
      "epoch": 1.0768,
      "grad_norm": 0.09245513379573822,
      "learning_rate": 4.327e-05,
      "loss": 0.0028,
      "step": 20190
    },
    {
      "epoch": 1.0773333333333333,
      "grad_norm": 0.12015104293823242,
      "learning_rate": 4.3266666666666664e-05,
      "loss": 0.0037,
      "step": 20200
    },
    {
      "epoch": 1.0778666666666668,
      "grad_norm": 0.213181272149086,
      "learning_rate": 4.3263333333333336e-05,
      "loss": 0.002,
      "step": 20210
    },
    {
      "epoch": 1.0784,
      "grad_norm": 0.6172943711280823,
      "learning_rate": 4.326e-05,
      "loss": 0.0036,
      "step": 20220
    },
    {
      "epoch": 1.0789333333333333,
      "grad_norm": 0.1965155303478241,
      "learning_rate": 4.325666666666667e-05,
      "loss": 0.0032,
      "step": 20230
    },
    {
      "epoch": 1.0794666666666666,
      "grad_norm": 0.41680100560188293,
      "learning_rate": 4.3253333333333335e-05,
      "loss": 0.0028,
      "step": 20240
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.5378122925758362,
      "learning_rate": 4.325e-05,
      "loss": 0.0036,
      "step": 20250
    },
    {
      "epoch": 1.0805333333333333,
      "grad_norm": 0.30307313799858093,
      "learning_rate": 4.324666666666667e-05,
      "loss": 0.0027,
      "step": 20260
    },
    {
      "epoch": 1.0810666666666666,
      "grad_norm": 0.18369388580322266,
      "learning_rate": 4.324333333333333e-05,
      "loss": 0.0025,
      "step": 20270
    },
    {
      "epoch": 1.0816,
      "grad_norm": 0.18768265843391418,
      "learning_rate": 4.324e-05,
      "loss": 0.0031,
      "step": 20280
    },
    {
      "epoch": 1.0821333333333334,
      "grad_norm": 0.42411577701568604,
      "learning_rate": 4.323666666666667e-05,
      "loss": 0.0031,
      "step": 20290
    },
    {
      "epoch": 1.0826666666666667,
      "grad_norm": 0.2501218616962433,
      "learning_rate": 4.323333333333334e-05,
      "loss": 0.0033,
      "step": 20300
    },
    {
      "epoch": 1.0832,
      "grad_norm": 0.4128997325897217,
      "learning_rate": 4.3230000000000005e-05,
      "loss": 0.0031,
      "step": 20310
    },
    {
      "epoch": 1.0837333333333334,
      "grad_norm": 0.1969550997018814,
      "learning_rate": 4.322666666666667e-05,
      "loss": 0.0035,
      "step": 20320
    },
    {
      "epoch": 1.0842666666666667,
      "grad_norm": 0.12756755948066711,
      "learning_rate": 4.322333333333334e-05,
      "loss": 0.0039,
      "step": 20330
    },
    {
      "epoch": 1.0848,
      "grad_norm": 0.34228378534317017,
      "learning_rate": 4.3219999999999996e-05,
      "loss": 0.0025,
      "step": 20340
    },
    {
      "epoch": 1.0853333333333333,
      "grad_norm": 0.21357667446136475,
      "learning_rate": 4.321666666666667e-05,
      "loss": 0.0033,
      "step": 20350
    },
    {
      "epoch": 1.0858666666666668,
      "grad_norm": 0.5883886814117432,
      "learning_rate": 4.3213333333333335e-05,
      "loss": 0.0039,
      "step": 20360
    },
    {
      "epoch": 1.0864,
      "grad_norm": 0.689712405204773,
      "learning_rate": 4.321e-05,
      "loss": 0.0027,
      "step": 20370
    },
    {
      "epoch": 1.0869333333333333,
      "grad_norm": 0.24442392587661743,
      "learning_rate": 4.320666666666667e-05,
      "loss": 0.004,
      "step": 20380
    },
    {
      "epoch": 1.0874666666666666,
      "grad_norm": 0.34688520431518555,
      "learning_rate": 4.3203333333333334e-05,
      "loss": 0.0026,
      "step": 20390
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.2147870659828186,
      "learning_rate": 4.32e-05,
      "loss": 0.0048,
      "step": 20400
    },
    {
      "epoch": 1.0885333333333334,
      "grad_norm": 0.09399358928203583,
      "learning_rate": 4.3196666666666666e-05,
      "loss": 0.0032,
      "step": 20410
    },
    {
      "epoch": 1.0890666666666666,
      "grad_norm": 0.26101481914520264,
      "learning_rate": 4.319333333333334e-05,
      "loss": 0.0036,
      "step": 20420
    },
    {
      "epoch": 1.0896,
      "grad_norm": 0.46238797903060913,
      "learning_rate": 4.3190000000000005e-05,
      "loss": 0.003,
      "step": 20430
    },
    {
      "epoch": 1.0901333333333334,
      "grad_norm": 0.23869633674621582,
      "learning_rate": 4.318666666666667e-05,
      "loss": 0.0021,
      "step": 20440
    },
    {
      "epoch": 1.0906666666666667,
      "grad_norm": 0.14168912172317505,
      "learning_rate": 4.318333333333334e-05,
      "loss": 0.0031,
      "step": 20450
    },
    {
      "epoch": 1.0912,
      "grad_norm": 0.09218528866767883,
      "learning_rate": 4.318e-05,
      "loss": 0.0032,
      "step": 20460
    },
    {
      "epoch": 1.0917333333333334,
      "grad_norm": 0.17973195016384125,
      "learning_rate": 4.317666666666667e-05,
      "loss": 0.0041,
      "step": 20470
    },
    {
      "epoch": 1.0922666666666667,
      "grad_norm": 0.3150334060192108,
      "learning_rate": 4.3173333333333336e-05,
      "loss": 0.0024,
      "step": 20480
    },
    {
      "epoch": 1.0928,
      "grad_norm": 0.12417226284742355,
      "learning_rate": 4.317e-05,
      "loss": 0.0019,
      "step": 20490
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 0.27390429377555847,
      "learning_rate": 4.316666666666667e-05,
      "loss": 0.0027,
      "step": 20500
    },
    {
      "epoch": 1.0938666666666668,
      "grad_norm": 0.38236337900161743,
      "learning_rate": 4.3163333333333334e-05,
      "loss": 0.0037,
      "step": 20510
    },
    {
      "epoch": 1.0944,
      "grad_norm": 0.975135087966919,
      "learning_rate": 4.316e-05,
      "loss": 0.0041,
      "step": 20520
    },
    {
      "epoch": 1.0949333333333333,
      "grad_norm": 0.5160665512084961,
      "learning_rate": 4.3156666666666666e-05,
      "loss": 0.0037,
      "step": 20530
    },
    {
      "epoch": 1.0954666666666666,
      "grad_norm": 0.27912989258766174,
      "learning_rate": 4.315333333333333e-05,
      "loss": 0.0016,
      "step": 20540
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.276268869638443,
      "learning_rate": 4.315e-05,
      "loss": 0.0022,
      "step": 20550
    },
    {
      "epoch": 1.0965333333333334,
      "grad_norm": 0.19572411477565765,
      "learning_rate": 4.314666666666667e-05,
      "loss": 0.0026,
      "step": 20560
    },
    {
      "epoch": 1.0970666666666666,
      "grad_norm": 0.36198127269744873,
      "learning_rate": 4.314333333333334e-05,
      "loss": 0.0029,
      "step": 20570
    },
    {
      "epoch": 1.0976,
      "grad_norm": 0.6658644080162048,
      "learning_rate": 4.3140000000000004e-05,
      "loss": 0.0038,
      "step": 20580
    },
    {
      "epoch": 1.0981333333333334,
      "grad_norm": 0.18272481858730316,
      "learning_rate": 4.313666666666667e-05,
      "loss": 0.0048,
      "step": 20590
    },
    {
      "epoch": 1.0986666666666667,
      "grad_norm": 0.13009876012802124,
      "learning_rate": 4.3133333333333336e-05,
      "loss": 0.0021,
      "step": 20600
    },
    {
      "epoch": 1.0992,
      "grad_norm": 0.3790337145328522,
      "learning_rate": 4.313e-05,
      "loss": 0.0034,
      "step": 20610
    },
    {
      "epoch": 1.0997333333333332,
      "grad_norm": 0.15383586287498474,
      "learning_rate": 4.312666666666667e-05,
      "loss": 0.0034,
      "step": 20620
    },
    {
      "epoch": 1.1002666666666667,
      "grad_norm": 0.521950900554657,
      "learning_rate": 4.312333333333334e-05,
      "loss": 0.0024,
      "step": 20630
    },
    {
      "epoch": 1.1008,
      "grad_norm": 0.3802568316459656,
      "learning_rate": 4.312000000000001e-05,
      "loss": 0.0042,
      "step": 20640
    },
    {
      "epoch": 1.1013333333333333,
      "grad_norm": 0.08306275308132172,
      "learning_rate": 4.311666666666667e-05,
      "loss": 0.0037,
      "step": 20650
    },
    {
      "epoch": 1.1018666666666665,
      "grad_norm": 0.07468444108963013,
      "learning_rate": 4.311333333333333e-05,
      "loss": 0.0046,
      "step": 20660
    },
    {
      "epoch": 1.1024,
      "grad_norm": 0.15588738024234772,
      "learning_rate": 4.311e-05,
      "loss": 0.0037,
      "step": 20670
    },
    {
      "epoch": 1.1029333333333333,
      "grad_norm": 0.2704843580722809,
      "learning_rate": 4.3106666666666665e-05,
      "loss": 0.0031,
      "step": 20680
    },
    {
      "epoch": 1.1034666666666666,
      "grad_norm": 0.3045489192008972,
      "learning_rate": 4.310333333333333e-05,
      "loss": 0.0025,
      "step": 20690
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.236115500330925,
      "learning_rate": 4.3100000000000004e-05,
      "loss": 0.0029,
      "step": 20700
    },
    {
      "epoch": 1.1045333333333334,
      "grad_norm": 0.07140371203422546,
      "learning_rate": 4.309666666666667e-05,
      "loss": 0.0029,
      "step": 20710
    },
    {
      "epoch": 1.1050666666666666,
      "grad_norm": 0.2783322334289551,
      "learning_rate": 4.3093333333333336e-05,
      "loss": 0.0024,
      "step": 20720
    },
    {
      "epoch": 1.1056,
      "grad_norm": 0.4595562517642975,
      "learning_rate": 4.309e-05,
      "loss": 0.0039,
      "step": 20730
    },
    {
      "epoch": 1.1061333333333334,
      "grad_norm": 0.37014418840408325,
      "learning_rate": 4.308666666666667e-05,
      "loss": 0.0027,
      "step": 20740
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 0.1302221715450287,
      "learning_rate": 4.3083333333333335e-05,
      "loss": 0.0032,
      "step": 20750
    },
    {
      "epoch": 1.1072,
      "grad_norm": 0.12093634903430939,
      "learning_rate": 4.308e-05,
      "loss": 0.003,
      "step": 20760
    },
    {
      "epoch": 1.1077333333333332,
      "grad_norm": 0.15401506423950195,
      "learning_rate": 4.3076666666666674e-05,
      "loss": 0.0027,
      "step": 20770
    },
    {
      "epoch": 1.1082666666666667,
      "grad_norm": 0.23698648810386658,
      "learning_rate": 4.307333333333334e-05,
      "loss": 0.0027,
      "step": 20780
    },
    {
      "epoch": 1.1088,
      "grad_norm": 0.15337926149368286,
      "learning_rate": 4.3070000000000006e-05,
      "loss": 0.0032,
      "step": 20790
    },
    {
      "epoch": 1.1093333333333333,
      "grad_norm": 0.10010641813278198,
      "learning_rate": 4.3066666666666665e-05,
      "loss": 0.0032,
      "step": 20800
    },
    {
      "epoch": 1.1098666666666666,
      "grad_norm": 0.19191060960292816,
      "learning_rate": 4.306333333333333e-05,
      "loss": 0.0038,
      "step": 20810
    },
    {
      "epoch": 1.1104,
      "grad_norm": 0.3344472646713257,
      "learning_rate": 4.306e-05,
      "loss": 0.0024,
      "step": 20820
    },
    {
      "epoch": 1.1109333333333333,
      "grad_norm": 0.4259639084339142,
      "learning_rate": 4.305666666666667e-05,
      "loss": 0.0026,
      "step": 20830
    },
    {
      "epoch": 1.1114666666666666,
      "grad_norm": 0.17923244833946228,
      "learning_rate": 4.305333333333334e-05,
      "loss": 0.0022,
      "step": 20840
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.4549869894981384,
      "learning_rate": 4.305e-05,
      "loss": 0.0021,
      "step": 20850
    },
    {
      "epoch": 1.1125333333333334,
      "grad_norm": 0.24139808118343353,
      "learning_rate": 4.304666666666667e-05,
      "loss": 0.0027,
      "step": 20860
    },
    {
      "epoch": 1.1130666666666666,
      "grad_norm": 0.400942862033844,
      "learning_rate": 4.3043333333333335e-05,
      "loss": 0.002,
      "step": 20870
    },
    {
      "epoch": 1.1136,
      "grad_norm": 0.304037481546402,
      "learning_rate": 4.304e-05,
      "loss": 0.0029,
      "step": 20880
    },
    {
      "epoch": 1.1141333333333334,
      "grad_norm": 0.5878768563270569,
      "learning_rate": 4.303666666666667e-05,
      "loss": 0.0024,
      "step": 20890
    },
    {
      "epoch": 1.1146666666666667,
      "grad_norm": 0.31144917011260986,
      "learning_rate": 4.3033333333333334e-05,
      "loss": 0.0026,
      "step": 20900
    },
    {
      "epoch": 1.1152,
      "grad_norm": 0.30724188685417175,
      "learning_rate": 4.3030000000000006e-05,
      "loss": 0.0027,
      "step": 20910
    },
    {
      "epoch": 1.1157333333333332,
      "grad_norm": 0.33649346232414246,
      "learning_rate": 4.302666666666667e-05,
      "loss": 0.0033,
      "step": 20920
    },
    {
      "epoch": 1.1162666666666667,
      "grad_norm": 0.2774832844734192,
      "learning_rate": 4.302333333333334e-05,
      "loss": 0.0029,
      "step": 20930
    },
    {
      "epoch": 1.1168,
      "grad_norm": 0.7309026122093201,
      "learning_rate": 4.3020000000000005e-05,
      "loss": 0.0022,
      "step": 20940
    },
    {
      "epoch": 1.1173333333333333,
      "grad_norm": 0.23564524948596954,
      "learning_rate": 4.3016666666666664e-05,
      "loss": 0.0033,
      "step": 20950
    },
    {
      "epoch": 1.1178666666666666,
      "grad_norm": 0.10217559337615967,
      "learning_rate": 4.301333333333333e-05,
      "loss": 0.0025,
      "step": 20960
    },
    {
      "epoch": 1.1184,
      "grad_norm": 0.14079582691192627,
      "learning_rate": 4.301e-05,
      "loss": 0.0041,
      "step": 20970
    },
    {
      "epoch": 1.1189333333333333,
      "grad_norm": 0.30566540360450745,
      "learning_rate": 4.300666666666667e-05,
      "loss": 0.0028,
      "step": 20980
    },
    {
      "epoch": 1.1194666666666666,
      "grad_norm": 0.28972840309143066,
      "learning_rate": 4.3003333333333336e-05,
      "loss": 0.0037,
      "step": 20990
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.05715927481651306,
      "learning_rate": 4.3e-05,
      "loss": 0.0027,
      "step": 21000
    },
    {
      "epoch": 1.1205333333333334,
      "grad_norm": 0.2665213942527771,
      "learning_rate": 4.299666666666667e-05,
      "loss": 0.0042,
      "step": 21010
    },
    {
      "epoch": 1.1210666666666667,
      "grad_norm": 0.3062872886657715,
      "learning_rate": 4.2993333333333334e-05,
      "loss": 0.0032,
      "step": 21020
    },
    {
      "epoch": 1.1216,
      "grad_norm": 0.04057672992348671,
      "learning_rate": 4.299e-05,
      "loss": 0.0039,
      "step": 21030
    },
    {
      "epoch": 1.1221333333333334,
      "grad_norm": 0.09333483129739761,
      "learning_rate": 4.2986666666666666e-05,
      "loss": 0.0026,
      "step": 21040
    },
    {
      "epoch": 1.1226666666666667,
      "grad_norm": 0.4540908634662628,
      "learning_rate": 4.298333333333334e-05,
      "loss": 0.0024,
      "step": 21050
    },
    {
      "epoch": 1.1232,
      "grad_norm": 0.4559960961341858,
      "learning_rate": 4.2980000000000005e-05,
      "loss": 0.0033,
      "step": 21060
    },
    {
      "epoch": 1.1237333333333333,
      "grad_norm": 0.6380528211593628,
      "learning_rate": 4.297666666666667e-05,
      "loss": 0.0037,
      "step": 21070
    },
    {
      "epoch": 1.1242666666666667,
      "grad_norm": 0.06007076054811478,
      "learning_rate": 4.297333333333334e-05,
      "loss": 0.0024,
      "step": 21080
    },
    {
      "epoch": 1.1248,
      "grad_norm": 0.21372097730636597,
      "learning_rate": 4.2970000000000004e-05,
      "loss": 0.0039,
      "step": 21090
    },
    {
      "epoch": 1.1253333333333333,
      "grad_norm": 0.2728216052055359,
      "learning_rate": 4.296666666666666e-05,
      "loss": 0.0033,
      "step": 21100
    },
    {
      "epoch": 1.1258666666666666,
      "grad_norm": 0.16167935729026794,
      "learning_rate": 4.2963333333333336e-05,
      "loss": 0.0035,
      "step": 21110
    },
    {
      "epoch": 1.1264,
      "grad_norm": 0.7796145081520081,
      "learning_rate": 4.296e-05,
      "loss": 0.0034,
      "step": 21120
    },
    {
      "epoch": 1.1269333333333333,
      "grad_norm": 0.6715255975723267,
      "learning_rate": 4.295666666666667e-05,
      "loss": 0.0039,
      "step": 21130
    },
    {
      "epoch": 1.1274666666666666,
      "grad_norm": 0.7872591614723206,
      "learning_rate": 4.2953333333333334e-05,
      "loss": 0.004,
      "step": 21140
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.08283163607120514,
      "learning_rate": 4.295e-05,
      "loss": 0.003,
      "step": 21150
    },
    {
      "epoch": 1.1285333333333334,
      "grad_norm": 0.06665614247322083,
      "learning_rate": 4.2946666666666667e-05,
      "loss": 0.0038,
      "step": 21160
    },
    {
      "epoch": 1.1290666666666667,
      "grad_norm": 0.2446402907371521,
      "learning_rate": 4.294333333333333e-05,
      "loss": 0.0028,
      "step": 21170
    },
    {
      "epoch": 1.1296,
      "grad_norm": 0.5116444826126099,
      "learning_rate": 4.2940000000000006e-05,
      "loss": 0.002,
      "step": 21180
    },
    {
      "epoch": 1.1301333333333332,
      "grad_norm": 0.15980684757232666,
      "learning_rate": 4.293666666666667e-05,
      "loss": 0.0025,
      "step": 21190
    },
    {
      "epoch": 1.1306666666666667,
      "grad_norm": 0.36288824677467346,
      "learning_rate": 4.293333333333334e-05,
      "loss": 0.0034,
      "step": 21200
    },
    {
      "epoch": 1.1312,
      "grad_norm": 0.4369213581085205,
      "learning_rate": 4.2930000000000004e-05,
      "loss": 0.0031,
      "step": 21210
    },
    {
      "epoch": 1.1317333333333333,
      "grad_norm": 0.2638397812843323,
      "learning_rate": 4.292666666666667e-05,
      "loss": 0.0038,
      "step": 21220
    },
    {
      "epoch": 1.1322666666666668,
      "grad_norm": 0.49241554737091064,
      "learning_rate": 4.2923333333333336e-05,
      "loss": 0.0025,
      "step": 21230
    },
    {
      "epoch": 1.1328,
      "grad_norm": 0.3240763247013092,
      "learning_rate": 4.292e-05,
      "loss": 0.0042,
      "step": 21240
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.33644410967826843,
      "learning_rate": 4.291666666666667e-05,
      "loss": 0.0034,
      "step": 21250
    },
    {
      "epoch": 1.1338666666666666,
      "grad_norm": 0.6883401274681091,
      "learning_rate": 4.2913333333333335e-05,
      "loss": 0.0031,
      "step": 21260
    },
    {
      "epoch": 1.1344,
      "grad_norm": 0.33630797266960144,
      "learning_rate": 4.291e-05,
      "loss": 0.0022,
      "step": 21270
    },
    {
      "epoch": 1.1349333333333333,
      "grad_norm": 0.6930574178695679,
      "learning_rate": 4.290666666666667e-05,
      "loss": 0.0035,
      "step": 21280
    },
    {
      "epoch": 1.1354666666666666,
      "grad_norm": 0.22189466655254364,
      "learning_rate": 4.290333333333333e-05,
      "loss": 0.0029,
      "step": 21290
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 0.18564918637275696,
      "learning_rate": 4.29e-05,
      "loss": 0.0019,
      "step": 21300
    },
    {
      "epoch": 1.1365333333333334,
      "grad_norm": 0.33885329961776733,
      "learning_rate": 4.2896666666666665e-05,
      "loss": 0.002,
      "step": 21310
    },
    {
      "epoch": 1.1370666666666667,
      "grad_norm": 0.18244437873363495,
      "learning_rate": 4.289333333333334e-05,
      "loss": 0.0035,
      "step": 21320
    },
    {
      "epoch": 1.1376,
      "grad_norm": 0.33300212025642395,
      "learning_rate": 4.2890000000000004e-05,
      "loss": 0.0034,
      "step": 21330
    },
    {
      "epoch": 1.1381333333333332,
      "grad_norm": 0.14853541553020477,
      "learning_rate": 4.288666666666667e-05,
      "loss": 0.0037,
      "step": 21340
    },
    {
      "epoch": 1.1386666666666667,
      "grad_norm": 0.7868087887763977,
      "learning_rate": 4.288333333333334e-05,
      "loss": 0.0025,
      "step": 21350
    },
    {
      "epoch": 1.1392,
      "grad_norm": 0.428301066160202,
      "learning_rate": 4.288e-05,
      "loss": 0.0042,
      "step": 21360
    },
    {
      "epoch": 1.1397333333333333,
      "grad_norm": 0.18187640607357025,
      "learning_rate": 4.287666666666667e-05,
      "loss": 0.003,
      "step": 21370
    },
    {
      "epoch": 1.1402666666666668,
      "grad_norm": 0.10060639679431915,
      "learning_rate": 4.2873333333333335e-05,
      "loss": 0.0031,
      "step": 21380
    },
    {
      "epoch": 1.1408,
      "grad_norm": 0.24530625343322754,
      "learning_rate": 4.287000000000001e-05,
      "loss": 0.0021,
      "step": 21390
    },
    {
      "epoch": 1.1413333333333333,
      "grad_norm": 0.15076583623886108,
      "learning_rate": 4.286666666666667e-05,
      "loss": 0.0018,
      "step": 21400
    },
    {
      "epoch": 1.1418666666666666,
      "grad_norm": 0.08092742413282394,
      "learning_rate": 4.2863333333333333e-05,
      "loss": 0.0022,
      "step": 21410
    },
    {
      "epoch": 1.1424,
      "grad_norm": 0.3872433304786682,
      "learning_rate": 4.286e-05,
      "loss": 0.0028,
      "step": 21420
    },
    {
      "epoch": 1.1429333333333334,
      "grad_norm": 0.43097612261772156,
      "learning_rate": 4.2856666666666666e-05,
      "loss": 0.0043,
      "step": 21430
    },
    {
      "epoch": 1.1434666666666666,
      "grad_norm": 0.6704984307289124,
      "learning_rate": 4.285333333333333e-05,
      "loss": 0.0057,
      "step": 21440
    },
    {
      "epoch": 1.144,
      "grad_norm": 0.2801513075828552,
      "learning_rate": 4.285e-05,
      "loss": 0.0038,
      "step": 21450
    },
    {
      "epoch": 1.1445333333333334,
      "grad_norm": 0.23698493838310242,
      "learning_rate": 4.284666666666667e-05,
      "loss": 0.0033,
      "step": 21460
    },
    {
      "epoch": 1.1450666666666667,
      "grad_norm": 0.2705037593841553,
      "learning_rate": 4.284333333333334e-05,
      "loss": 0.0023,
      "step": 21470
    },
    {
      "epoch": 1.1456,
      "grad_norm": 0.425765722990036,
      "learning_rate": 4.284e-05,
      "loss": 0.0028,
      "step": 21480
    },
    {
      "epoch": 1.1461333333333332,
      "grad_norm": 0.10080991685390472,
      "learning_rate": 4.283666666666667e-05,
      "loss": 0.0025,
      "step": 21490
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 0.05837017297744751,
      "learning_rate": 4.2833333333333335e-05,
      "loss": 0.0038,
      "step": 21500
    },
    {
      "epoch": 1.1472,
      "grad_norm": 0.33469900488853455,
      "learning_rate": 4.283e-05,
      "loss": 0.0033,
      "step": 21510
    },
    {
      "epoch": 1.1477333333333333,
      "grad_norm": 0.41846126317977905,
      "learning_rate": 4.282666666666667e-05,
      "loss": 0.0028,
      "step": 21520
    },
    {
      "epoch": 1.1482666666666668,
      "grad_norm": 0.3592173755168915,
      "learning_rate": 4.282333333333334e-05,
      "loss": 0.0041,
      "step": 21530
    },
    {
      "epoch": 1.1488,
      "grad_norm": 0.3883386552333832,
      "learning_rate": 4.282000000000001e-05,
      "loss": 0.0032,
      "step": 21540
    },
    {
      "epoch": 1.1493333333333333,
      "grad_norm": 0.2399158626794815,
      "learning_rate": 4.2816666666666666e-05,
      "loss": 0.0029,
      "step": 21550
    },
    {
      "epoch": 1.1498666666666666,
      "grad_norm": 0.13113334774971008,
      "learning_rate": 4.281333333333333e-05,
      "loss": 0.0027,
      "step": 21560
    },
    {
      "epoch": 1.1504,
      "grad_norm": 0.12059734761714935,
      "learning_rate": 4.281e-05,
      "loss": 0.0032,
      "step": 21570
    },
    {
      "epoch": 1.1509333333333334,
      "grad_norm": 0.6316697001457214,
      "learning_rate": 4.2806666666666665e-05,
      "loss": 0.0045,
      "step": 21580
    },
    {
      "epoch": 1.1514666666666666,
      "grad_norm": 0.4305117130279541,
      "learning_rate": 4.280333333333334e-05,
      "loss": 0.0031,
      "step": 21590
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.06435243040323257,
      "learning_rate": 4.2800000000000004e-05,
      "loss": 0.0029,
      "step": 21600
    },
    {
      "epoch": 1.1525333333333334,
      "grad_norm": 0.13462112843990326,
      "learning_rate": 4.279666666666667e-05,
      "loss": 0.0027,
      "step": 21610
    },
    {
      "epoch": 1.1530666666666667,
      "grad_norm": 0.01916956715285778,
      "learning_rate": 4.2793333333333336e-05,
      "loss": 0.0033,
      "step": 21620
    },
    {
      "epoch": 1.1536,
      "grad_norm": 0.6601003408432007,
      "learning_rate": 4.279e-05,
      "loss": 0.0038,
      "step": 21630
    },
    {
      "epoch": 1.1541333333333332,
      "grad_norm": 0.21608786284923553,
      "learning_rate": 4.278666666666667e-05,
      "loss": 0.0032,
      "step": 21640
    },
    {
      "epoch": 1.1546666666666667,
      "grad_norm": 0.18600548803806305,
      "learning_rate": 4.2783333333333334e-05,
      "loss": 0.0026,
      "step": 21650
    },
    {
      "epoch": 1.1552,
      "grad_norm": 0.34204527735710144,
      "learning_rate": 4.278e-05,
      "loss": 0.0043,
      "step": 21660
    },
    {
      "epoch": 1.1557333333333333,
      "grad_norm": 0.058928027749061584,
      "learning_rate": 4.277666666666667e-05,
      "loss": 0.0026,
      "step": 21670
    },
    {
      "epoch": 1.1562666666666668,
      "grad_norm": 0.24185632169246674,
      "learning_rate": 4.277333333333334e-05,
      "loss": 0.0021,
      "step": 21680
    },
    {
      "epoch": 1.1568,
      "grad_norm": 0.18072737753391266,
      "learning_rate": 4.2770000000000006e-05,
      "loss": 0.0029,
      "step": 21690
    },
    {
      "epoch": 1.1573333333333333,
      "grad_norm": 0.3675469756126404,
      "learning_rate": 4.2766666666666665e-05,
      "loss": 0.0033,
      "step": 21700
    },
    {
      "epoch": 1.1578666666666666,
      "grad_norm": 0.2180044949054718,
      "learning_rate": 4.276333333333333e-05,
      "loss": 0.0056,
      "step": 21710
    },
    {
      "epoch": 1.1584,
      "grad_norm": 0.3160800337791443,
      "learning_rate": 4.276e-05,
      "loss": 0.0024,
      "step": 21720
    },
    {
      "epoch": 1.1589333333333334,
      "grad_norm": 0.42163869738578796,
      "learning_rate": 4.275666666666667e-05,
      "loss": 0.0037,
      "step": 21730
    },
    {
      "epoch": 1.1594666666666666,
      "grad_norm": 0.33838921785354614,
      "learning_rate": 4.2753333333333336e-05,
      "loss": 0.0037,
      "step": 21740
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.07473665475845337,
      "learning_rate": 4.275e-05,
      "loss": 0.0027,
      "step": 21750
    },
    {
      "epoch": 1.1605333333333334,
      "grad_norm": 0.8929467797279358,
      "learning_rate": 4.274666666666667e-05,
      "loss": 0.0039,
      "step": 21760
    },
    {
      "epoch": 1.1610666666666667,
      "grad_norm": 0.045605819672346115,
      "learning_rate": 4.2743333333333335e-05,
      "loss": 0.0036,
      "step": 21770
    },
    {
      "epoch": 1.1616,
      "grad_norm": 0.26921066641807556,
      "learning_rate": 4.274e-05,
      "loss": 0.0039,
      "step": 21780
    },
    {
      "epoch": 1.1621333333333332,
      "grad_norm": 0.5815369486808777,
      "learning_rate": 4.273666666666667e-05,
      "loss": 0.0028,
      "step": 21790
    },
    {
      "epoch": 1.1626666666666667,
      "grad_norm": 0.35869690775871277,
      "learning_rate": 4.273333333333333e-05,
      "loss": 0.004,
      "step": 21800
    },
    {
      "epoch": 1.1632,
      "grad_norm": 0.4830814301967621,
      "learning_rate": 4.2730000000000006e-05,
      "loss": 0.0033,
      "step": 21810
    },
    {
      "epoch": 1.1637333333333333,
      "grad_norm": 0.28125056624412537,
      "learning_rate": 4.272666666666667e-05,
      "loss": 0.0033,
      "step": 21820
    },
    {
      "epoch": 1.1642666666666668,
      "grad_norm": 0.09033272415399551,
      "learning_rate": 4.272333333333334e-05,
      "loss": 0.0026,
      "step": 21830
    },
    {
      "epoch": 1.1648,
      "grad_norm": 0.029246632009744644,
      "learning_rate": 4.2720000000000004e-05,
      "loss": 0.0034,
      "step": 21840
    },
    {
      "epoch": 1.1653333333333333,
      "grad_norm": 0.5494803190231323,
      "learning_rate": 4.2716666666666664e-05,
      "loss": 0.0033,
      "step": 21850
    },
    {
      "epoch": 1.1658666666666666,
      "grad_norm": 0.6967208981513977,
      "learning_rate": 4.271333333333333e-05,
      "loss": 0.0025,
      "step": 21860
    },
    {
      "epoch": 1.1663999999999999,
      "grad_norm": 0.3329440653324127,
      "learning_rate": 4.271e-05,
      "loss": 0.0022,
      "step": 21870
    },
    {
      "epoch": 1.1669333333333334,
      "grad_norm": 0.16768653690814972,
      "learning_rate": 4.270666666666667e-05,
      "loss": 0.0029,
      "step": 21880
    },
    {
      "epoch": 1.1674666666666667,
      "grad_norm": 0.12382413446903229,
      "learning_rate": 4.2703333333333335e-05,
      "loss": 0.0024,
      "step": 21890
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.3399336338043213,
      "learning_rate": 4.27e-05,
      "loss": 0.0039,
      "step": 21900
    },
    {
      "epoch": 1.1685333333333334,
      "grad_norm": 0.07383006066083908,
      "learning_rate": 4.269666666666667e-05,
      "loss": 0.0045,
      "step": 21910
    },
    {
      "epoch": 1.1690666666666667,
      "grad_norm": 0.6670297384262085,
      "learning_rate": 4.2693333333333333e-05,
      "loss": 0.0034,
      "step": 21920
    },
    {
      "epoch": 1.1696,
      "grad_norm": 0.10292352735996246,
      "learning_rate": 4.269e-05,
      "loss": 0.0028,
      "step": 21930
    },
    {
      "epoch": 1.1701333333333332,
      "grad_norm": 0.16023565828800201,
      "learning_rate": 4.268666666666667e-05,
      "loss": 0.0027,
      "step": 21940
    },
    {
      "epoch": 1.1706666666666667,
      "grad_norm": 0.30982476472854614,
      "learning_rate": 4.268333333333334e-05,
      "loss": 0.0029,
      "step": 21950
    },
    {
      "epoch": 1.1712,
      "grad_norm": 0.48725271224975586,
      "learning_rate": 4.2680000000000005e-05,
      "loss": 0.0024,
      "step": 21960
    },
    {
      "epoch": 1.1717333333333333,
      "grad_norm": 0.3746832013130188,
      "learning_rate": 4.267666666666667e-05,
      "loss": 0.0031,
      "step": 21970
    },
    {
      "epoch": 1.1722666666666668,
      "grad_norm": 0.13297735154628754,
      "learning_rate": 4.267333333333334e-05,
      "loss": 0.0042,
      "step": 21980
    },
    {
      "epoch": 1.1728,
      "grad_norm": 0.16401998698711395,
      "learning_rate": 4.267e-05,
      "loss": 0.0031,
      "step": 21990
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 0.07833925634622574,
      "learning_rate": 4.266666666666667e-05,
      "loss": 0.0033,
      "step": 22000
    },
    {
      "epoch": 1.1738666666666666,
      "grad_norm": 0.6736773252487183,
      "learning_rate": 4.2663333333333335e-05,
      "loss": 0.0028,
      "step": 22010
    },
    {
      "epoch": 1.1743999999999999,
      "grad_norm": 0.16060712933540344,
      "learning_rate": 4.266e-05,
      "loss": 0.0043,
      "step": 22020
    },
    {
      "epoch": 1.1749333333333334,
      "grad_norm": 0.2239343672990799,
      "learning_rate": 4.265666666666667e-05,
      "loss": 0.0027,
      "step": 22030
    },
    {
      "epoch": 1.1754666666666667,
      "grad_norm": 0.27507442235946655,
      "learning_rate": 4.2653333333333334e-05,
      "loss": 0.0046,
      "step": 22040
    },
    {
      "epoch": 1.176,
      "grad_norm": 0.3599807620048523,
      "learning_rate": 4.265e-05,
      "loss": 0.004,
      "step": 22050
    },
    {
      "epoch": 1.1765333333333334,
      "grad_norm": 0.21376262605190277,
      "learning_rate": 4.2646666666666666e-05,
      "loss": 0.0036,
      "step": 22060
    },
    {
      "epoch": 1.1770666666666667,
      "grad_norm": 0.07742009311914444,
      "learning_rate": 4.264333333333333e-05,
      "loss": 0.0032,
      "step": 22070
    },
    {
      "epoch": 1.1776,
      "grad_norm": 0.4574008584022522,
      "learning_rate": 4.2640000000000005e-05,
      "loss": 0.0028,
      "step": 22080
    },
    {
      "epoch": 1.1781333333333333,
      "grad_norm": 0.09864640235900879,
      "learning_rate": 4.263666666666667e-05,
      "loss": 0.0023,
      "step": 22090
    },
    {
      "epoch": 1.1786666666666668,
      "grad_norm": 0.26886534690856934,
      "learning_rate": 4.263333333333334e-05,
      "loss": 0.0031,
      "step": 22100
    },
    {
      "epoch": 1.1792,
      "grad_norm": 0.06435258686542511,
      "learning_rate": 4.2630000000000004e-05,
      "loss": 0.0029,
      "step": 22110
    },
    {
      "epoch": 1.1797333333333333,
      "grad_norm": 0.3408639430999756,
      "learning_rate": 4.262666666666667e-05,
      "loss": 0.0031,
      "step": 22120
    },
    {
      "epoch": 1.1802666666666666,
      "grad_norm": 0.28468549251556396,
      "learning_rate": 4.2623333333333336e-05,
      "loss": 0.0038,
      "step": 22130
    },
    {
      "epoch": 1.1808,
      "grad_norm": 0.2521466910839081,
      "learning_rate": 4.262e-05,
      "loss": 0.003,
      "step": 22140
    },
    {
      "epoch": 1.1813333333333333,
      "grad_norm": 0.5754685401916504,
      "learning_rate": 4.261666666666667e-05,
      "loss": 0.0033,
      "step": 22150
    },
    {
      "epoch": 1.1818666666666666,
      "grad_norm": 0.7283944487571716,
      "learning_rate": 4.2613333333333334e-05,
      "loss": 0.0031,
      "step": 22160
    },
    {
      "epoch": 1.1824,
      "grad_norm": 0.4521067440509796,
      "learning_rate": 4.261e-05,
      "loss": 0.0028,
      "step": 22170
    },
    {
      "epoch": 1.1829333333333334,
      "grad_norm": 0.09805943816900253,
      "learning_rate": 4.2606666666666666e-05,
      "loss": 0.0028,
      "step": 22180
    },
    {
      "epoch": 1.1834666666666667,
      "grad_norm": 0.3236614465713501,
      "learning_rate": 4.260333333333333e-05,
      "loss": 0.0029,
      "step": 22190
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.06505287438631058,
      "learning_rate": 4.26e-05,
      "loss": 0.0026,
      "step": 22200
    },
    {
      "epoch": 1.1845333333333334,
      "grad_norm": 0.24252720177173615,
      "learning_rate": 4.2596666666666665e-05,
      "loss": 0.0036,
      "step": 22210
    },
    {
      "epoch": 1.1850666666666667,
      "grad_norm": 0.09025108069181442,
      "learning_rate": 4.259333333333334e-05,
      "loss": 0.0022,
      "step": 22220
    },
    {
      "epoch": 1.1856,
      "grad_norm": 0.24087855219841003,
      "learning_rate": 4.2590000000000004e-05,
      "loss": 0.0023,
      "step": 22230
    },
    {
      "epoch": 1.1861333333333333,
      "grad_norm": 0.5795333981513977,
      "learning_rate": 4.258666666666667e-05,
      "loss": 0.0035,
      "step": 22240
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 0.3359937369823456,
      "learning_rate": 4.2583333333333336e-05,
      "loss": 0.0023,
      "step": 22250
    },
    {
      "epoch": 1.1872,
      "grad_norm": 0.18560734391212463,
      "learning_rate": 4.258e-05,
      "loss": 0.0031,
      "step": 22260
    },
    {
      "epoch": 1.1877333333333333,
      "grad_norm": 0.13327665627002716,
      "learning_rate": 4.257666666666667e-05,
      "loss": 0.0025,
      "step": 22270
    },
    {
      "epoch": 1.1882666666666666,
      "grad_norm": 0.48453930020332336,
      "learning_rate": 4.2573333333333335e-05,
      "loss": 0.0033,
      "step": 22280
    },
    {
      "epoch": 1.1888,
      "grad_norm": 0.12454158067703247,
      "learning_rate": 4.257000000000001e-05,
      "loss": 0.0039,
      "step": 22290
    },
    {
      "epoch": 1.1893333333333334,
      "grad_norm": 0.07535689324140549,
      "learning_rate": 4.2566666666666674e-05,
      "loss": 0.0032,
      "step": 22300
    },
    {
      "epoch": 1.1898666666666666,
      "grad_norm": 0.18615217506885529,
      "learning_rate": 4.256333333333333e-05,
      "loss": 0.0031,
      "step": 22310
    },
    {
      "epoch": 1.1904,
      "grad_norm": 0.5819521546363831,
      "learning_rate": 4.256e-05,
      "loss": 0.0034,
      "step": 22320
    },
    {
      "epoch": 1.1909333333333334,
      "grad_norm": 0.3323449194431305,
      "learning_rate": 4.2556666666666665e-05,
      "loss": 0.0023,
      "step": 22330
    },
    {
      "epoch": 1.1914666666666667,
      "grad_norm": 0.24233488738536835,
      "learning_rate": 4.255333333333333e-05,
      "loss": 0.0028,
      "step": 22340
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.29735130071640015,
      "learning_rate": 4.2550000000000004e-05,
      "loss": 0.0033,
      "step": 22350
    },
    {
      "epoch": 1.1925333333333334,
      "grad_norm": 0.21033959090709686,
      "learning_rate": 4.254666666666667e-05,
      "loss": 0.0034,
      "step": 22360
    },
    {
      "epoch": 1.1930666666666667,
      "grad_norm": 0.10237633436918259,
      "learning_rate": 4.2543333333333337e-05,
      "loss": 0.0034,
      "step": 22370
    },
    {
      "epoch": 1.1936,
      "grad_norm": 0.17775166034698486,
      "learning_rate": 4.254e-05,
      "loss": 0.0026,
      "step": 22380
    },
    {
      "epoch": 1.1941333333333333,
      "grad_norm": 0.24363338947296143,
      "learning_rate": 4.253666666666667e-05,
      "loss": 0.0031,
      "step": 22390
    },
    {
      "epoch": 1.1946666666666665,
      "grad_norm": 0.9546889066696167,
      "learning_rate": 4.2533333333333335e-05,
      "loss": 0.0037,
      "step": 22400
    },
    {
      "epoch": 1.1952,
      "grad_norm": 0.2388860285282135,
      "learning_rate": 4.253e-05,
      "loss": 0.0042,
      "step": 22410
    },
    {
      "epoch": 1.1957333333333333,
      "grad_norm": 0.24006018042564392,
      "learning_rate": 4.252666666666667e-05,
      "loss": 0.0046,
      "step": 22420
    },
    {
      "epoch": 1.1962666666666666,
      "grad_norm": 0.30443811416625977,
      "learning_rate": 4.252333333333334e-05,
      "loss": 0.0034,
      "step": 22430
    },
    {
      "epoch": 1.1968,
      "grad_norm": 0.1785999834537506,
      "learning_rate": 4.2520000000000006e-05,
      "loss": 0.0037,
      "step": 22440
    },
    {
      "epoch": 1.1973333333333334,
      "grad_norm": 0.24080489575862885,
      "learning_rate": 4.251666666666667e-05,
      "loss": 0.0025,
      "step": 22450
    },
    {
      "epoch": 1.1978666666666666,
      "grad_norm": 0.2446303516626358,
      "learning_rate": 4.251333333333333e-05,
      "loss": 0.003,
      "step": 22460
    },
    {
      "epoch": 1.1984,
      "grad_norm": 0.053660739213228226,
      "learning_rate": 4.251e-05,
      "loss": 0.003,
      "step": 22470
    },
    {
      "epoch": 1.1989333333333334,
      "grad_norm": 0.3311496675014496,
      "learning_rate": 4.2506666666666664e-05,
      "loss": 0.0034,
      "step": 22480
    },
    {
      "epoch": 1.1994666666666667,
      "grad_norm": 0.10791223496198654,
      "learning_rate": 4.250333333333334e-05,
      "loss": 0.0021,
      "step": 22490
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.1873202919960022,
      "learning_rate": 4.25e-05,
      "loss": 0.0028,
      "step": 22500
    },
    {
      "epoch": 1.2005333333333335,
      "grad_norm": 0.36547622084617615,
      "learning_rate": 4.249666666666667e-05,
      "loss": 0.0025,
      "step": 22510
    },
    {
      "epoch": 1.2010666666666667,
      "grad_norm": 0.4525693356990814,
      "learning_rate": 4.2493333333333335e-05,
      "loss": 0.0032,
      "step": 22520
    },
    {
      "epoch": 1.2016,
      "grad_norm": 0.5104882717132568,
      "learning_rate": 4.249e-05,
      "loss": 0.0036,
      "step": 22530
    },
    {
      "epoch": 1.2021333333333333,
      "grad_norm": 0.7563179731369019,
      "learning_rate": 4.248666666666667e-05,
      "loss": 0.0034,
      "step": 22540
    },
    {
      "epoch": 1.2026666666666666,
      "grad_norm": 0.36308032274246216,
      "learning_rate": 4.2483333333333334e-05,
      "loss": 0.003,
      "step": 22550
    },
    {
      "epoch": 1.2032,
      "grad_norm": 0.30051079392433167,
      "learning_rate": 4.248e-05,
      "loss": 0.0036,
      "step": 22560
    },
    {
      "epoch": 1.2037333333333333,
      "grad_norm": 0.36170411109924316,
      "learning_rate": 4.247666666666667e-05,
      "loss": 0.0034,
      "step": 22570
    },
    {
      "epoch": 1.2042666666666666,
      "grad_norm": 0.1529475301504135,
      "learning_rate": 4.247333333333334e-05,
      "loss": 0.0031,
      "step": 22580
    },
    {
      "epoch": 1.2048,
      "grad_norm": 0.2389703094959259,
      "learning_rate": 4.2470000000000005e-05,
      "loss": 0.0018,
      "step": 22590
    },
    {
      "epoch": 1.2053333333333334,
      "grad_norm": 0.09054843336343765,
      "learning_rate": 4.246666666666667e-05,
      "loss": 0.002,
      "step": 22600
    },
    {
      "epoch": 1.2058666666666666,
      "grad_norm": 0.4804922044277191,
      "learning_rate": 4.246333333333333e-05,
      "loss": 0.0023,
      "step": 22610
    },
    {
      "epoch": 1.2064,
      "grad_norm": 0.42421990633010864,
      "learning_rate": 4.246e-05,
      "loss": 0.003,
      "step": 22620
    },
    {
      "epoch": 1.2069333333333334,
      "grad_norm": 0.1816020905971527,
      "learning_rate": 4.245666666666667e-05,
      "loss": 0.0031,
      "step": 22630
    },
    {
      "epoch": 1.2074666666666667,
      "grad_norm": 0.35917922854423523,
      "learning_rate": 4.2453333333333336e-05,
      "loss": 0.003,
      "step": 22640
    },
    {
      "epoch": 1.208,
      "grad_norm": 0.10941314697265625,
      "learning_rate": 4.245e-05,
      "loss": 0.0031,
      "step": 22650
    },
    {
      "epoch": 1.2085333333333332,
      "grad_norm": 0.09868761152029037,
      "learning_rate": 4.244666666666667e-05,
      "loss": 0.0018,
      "step": 22660
    },
    {
      "epoch": 1.2090666666666667,
      "grad_norm": 0.4226851165294647,
      "learning_rate": 4.2443333333333334e-05,
      "loss": 0.0035,
      "step": 22670
    },
    {
      "epoch": 1.2096,
      "grad_norm": 0.06136052682995796,
      "learning_rate": 4.244e-05,
      "loss": 0.0029,
      "step": 22680
    },
    {
      "epoch": 1.2101333333333333,
      "grad_norm": 0.3521692454814911,
      "learning_rate": 4.2436666666666666e-05,
      "loss": 0.0038,
      "step": 22690
    },
    {
      "epoch": 1.2106666666666666,
      "grad_norm": 0.2563140094280243,
      "learning_rate": 4.243333333333334e-05,
      "loss": 0.0036,
      "step": 22700
    },
    {
      "epoch": 1.2112,
      "grad_norm": 0.460658997297287,
      "learning_rate": 4.2430000000000005e-05,
      "loss": 0.0018,
      "step": 22710
    },
    {
      "epoch": 1.2117333333333333,
      "grad_norm": 0.10477740317583084,
      "learning_rate": 4.242666666666667e-05,
      "loss": 0.0038,
      "step": 22720
    },
    {
      "epoch": 1.2122666666666666,
      "grad_norm": 0.4228264391422272,
      "learning_rate": 4.242333333333334e-05,
      "loss": 0.0043,
      "step": 22730
    },
    {
      "epoch": 1.2128,
      "grad_norm": 0.27268970012664795,
      "learning_rate": 4.2420000000000004e-05,
      "loss": 0.0044,
      "step": 22740
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.28533273935317993,
      "learning_rate": 4.241666666666667e-05,
      "loss": 0.0026,
      "step": 22750
    },
    {
      "epoch": 1.2138666666666666,
      "grad_norm": 0.46420586109161377,
      "learning_rate": 4.241333333333333e-05,
      "loss": 0.0021,
      "step": 22760
    },
    {
      "epoch": 1.2144,
      "grad_norm": 0.3996847867965698,
      "learning_rate": 4.241e-05,
      "loss": 0.0041,
      "step": 22770
    },
    {
      "epoch": 1.2149333333333334,
      "grad_norm": 0.757112979888916,
      "learning_rate": 4.240666666666667e-05,
      "loss": 0.004,
      "step": 22780
    },
    {
      "epoch": 1.2154666666666667,
      "grad_norm": 0.5487896203994751,
      "learning_rate": 4.2403333333333334e-05,
      "loss": 0.0031,
      "step": 22790
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.4284445643424988,
      "learning_rate": 4.24e-05,
      "loss": 0.0033,
      "step": 22800
    },
    {
      "epoch": 1.2165333333333332,
      "grad_norm": 0.31686756014823914,
      "learning_rate": 4.239666666666667e-05,
      "loss": 0.0035,
      "step": 22810
    },
    {
      "epoch": 1.2170666666666667,
      "grad_norm": 0.41488876938819885,
      "learning_rate": 4.239333333333333e-05,
      "loss": 0.0023,
      "step": 22820
    },
    {
      "epoch": 1.2176,
      "grad_norm": 0.01909656450152397,
      "learning_rate": 4.239e-05,
      "loss": 0.0019,
      "step": 22830
    },
    {
      "epoch": 1.2181333333333333,
      "grad_norm": 0.16113711893558502,
      "learning_rate": 4.238666666666667e-05,
      "loss": 0.0026,
      "step": 22840
    },
    {
      "epoch": 1.2186666666666666,
      "grad_norm": 0.39601173996925354,
      "learning_rate": 4.238333333333334e-05,
      "loss": 0.0026,
      "step": 22850
    },
    {
      "epoch": 1.2192,
      "grad_norm": 0.04431730508804321,
      "learning_rate": 4.2380000000000004e-05,
      "loss": 0.0018,
      "step": 22860
    },
    {
      "epoch": 1.2197333333333333,
      "grad_norm": 0.05234583467245102,
      "learning_rate": 4.237666666666667e-05,
      "loss": 0.0044,
      "step": 22870
    },
    {
      "epoch": 1.2202666666666666,
      "grad_norm": 0.2128704935312271,
      "learning_rate": 4.2373333333333336e-05,
      "loss": 0.0034,
      "step": 22880
    },
    {
      "epoch": 1.2208,
      "grad_norm": 0.09287243336439133,
      "learning_rate": 4.237e-05,
      "loss": 0.0033,
      "step": 22890
    },
    {
      "epoch": 1.2213333333333334,
      "grad_norm": 0.6334138512611389,
      "learning_rate": 4.236666666666667e-05,
      "loss": 0.0037,
      "step": 22900
    },
    {
      "epoch": 1.2218666666666667,
      "grad_norm": 0.6016534566879272,
      "learning_rate": 4.2363333333333335e-05,
      "loss": 0.004,
      "step": 22910
    },
    {
      "epoch": 1.2224,
      "grad_norm": 0.506952166557312,
      "learning_rate": 4.236e-05,
      "loss": 0.0031,
      "step": 22920
    },
    {
      "epoch": 1.2229333333333334,
      "grad_norm": 0.5727031826972961,
      "learning_rate": 4.235666666666667e-05,
      "loss": 0.0038,
      "step": 22930
    },
    {
      "epoch": 1.2234666666666667,
      "grad_norm": 0.5185043215751648,
      "learning_rate": 4.235333333333333e-05,
      "loss": 0.0027,
      "step": 22940
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.6352130174636841,
      "learning_rate": 4.235e-05,
      "loss": 0.003,
      "step": 22950
    },
    {
      "epoch": 1.2245333333333333,
      "grad_norm": 0.32855021953582764,
      "learning_rate": 4.2346666666666666e-05,
      "loss": 0.0034,
      "step": 22960
    },
    {
      "epoch": 1.2250666666666667,
      "grad_norm": 0.10417517274618149,
      "learning_rate": 4.234333333333333e-05,
      "loss": 0.0028,
      "step": 22970
    },
    {
      "epoch": 1.2256,
      "grad_norm": 0.3053022027015686,
      "learning_rate": 4.2340000000000005e-05,
      "loss": 0.0028,
      "step": 22980
    },
    {
      "epoch": 1.2261333333333333,
      "grad_norm": 0.18415318429470062,
      "learning_rate": 4.233666666666667e-05,
      "loss": 0.0041,
      "step": 22990
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 0.2997473478317261,
      "learning_rate": 4.233333333333334e-05,
      "loss": 0.0024,
      "step": 23000
    },
    {
      "epoch": 1.2272,
      "grad_norm": 0.7536218166351318,
      "learning_rate": 4.233e-05,
      "loss": 0.0024,
      "step": 23010
    },
    {
      "epoch": 1.2277333333333333,
      "grad_norm": 0.35764002799987793,
      "learning_rate": 4.232666666666667e-05,
      "loss": 0.0027,
      "step": 23020
    },
    {
      "epoch": 1.2282666666666666,
      "grad_norm": 0.6031818985939026,
      "learning_rate": 4.2323333333333335e-05,
      "loss": 0.0033,
      "step": 23030
    },
    {
      "epoch": 1.2288000000000001,
      "grad_norm": 0.27075472474098206,
      "learning_rate": 4.232e-05,
      "loss": 0.0019,
      "step": 23040
    },
    {
      "epoch": 1.2293333333333334,
      "grad_norm": 0.06461838632822037,
      "learning_rate": 4.2316666666666674e-05,
      "loss": 0.0025,
      "step": 23050
    },
    {
      "epoch": 1.2298666666666667,
      "grad_norm": 0.06031610071659088,
      "learning_rate": 4.2313333333333334e-05,
      "loss": 0.002,
      "step": 23060
    },
    {
      "epoch": 1.2304,
      "grad_norm": 0.4774131774902344,
      "learning_rate": 4.231e-05,
      "loss": 0.0027,
      "step": 23070
    },
    {
      "epoch": 1.2309333333333332,
      "grad_norm": 0.35640889406204224,
      "learning_rate": 4.2306666666666666e-05,
      "loss": 0.0019,
      "step": 23080
    },
    {
      "epoch": 1.2314666666666667,
      "grad_norm": 0.09799690544605255,
      "learning_rate": 4.230333333333333e-05,
      "loss": 0.0027,
      "step": 23090
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.14832374453544617,
      "learning_rate": 4.23e-05,
      "loss": 0.0029,
      "step": 23100
    },
    {
      "epoch": 1.2325333333333333,
      "grad_norm": 0.045429326593875885,
      "learning_rate": 4.229666666666667e-05,
      "loss": 0.0034,
      "step": 23110
    },
    {
      "epoch": 1.2330666666666668,
      "grad_norm": 0.23640966415405273,
      "learning_rate": 4.229333333333334e-05,
      "loss": 0.0029,
      "step": 23120
    },
    {
      "epoch": 1.2336,
      "grad_norm": 0.08297383785247803,
      "learning_rate": 4.229e-05,
      "loss": 0.0027,
      "step": 23130
    },
    {
      "epoch": 1.2341333333333333,
      "grad_norm": 0.22459788620471954,
      "learning_rate": 4.228666666666667e-05,
      "loss": 0.0032,
      "step": 23140
    },
    {
      "epoch": 1.2346666666666666,
      "grad_norm": 0.4540616571903229,
      "learning_rate": 4.2283333333333336e-05,
      "loss": 0.0043,
      "step": 23150
    },
    {
      "epoch": 1.2352,
      "grad_norm": 0.13567867875099182,
      "learning_rate": 4.228e-05,
      "loss": 0.0023,
      "step": 23160
    },
    {
      "epoch": 1.2357333333333334,
      "grad_norm": 0.07901781797409058,
      "learning_rate": 4.227666666666667e-05,
      "loss": 0.0034,
      "step": 23170
    },
    {
      "epoch": 1.2362666666666666,
      "grad_norm": 0.28939175605773926,
      "learning_rate": 4.2273333333333334e-05,
      "loss": 0.0031,
      "step": 23180
    },
    {
      "epoch": 1.2368000000000001,
      "grad_norm": 0.1323583871126175,
      "learning_rate": 4.227000000000001e-05,
      "loss": 0.0043,
      "step": 23190
    },
    {
      "epoch": 1.2373333333333334,
      "grad_norm": 0.18511134386062622,
      "learning_rate": 4.226666666666667e-05,
      "loss": 0.0044,
      "step": 23200
    },
    {
      "epoch": 1.2378666666666667,
      "grad_norm": 0.027738502249121666,
      "learning_rate": 4.226333333333334e-05,
      "loss": 0.0043,
      "step": 23210
    },
    {
      "epoch": 1.2384,
      "grad_norm": 0.23756197094917297,
      "learning_rate": 4.226e-05,
      "loss": 0.0039,
      "step": 23220
    },
    {
      "epoch": 1.2389333333333332,
      "grad_norm": 0.42680469155311584,
      "learning_rate": 4.2256666666666665e-05,
      "loss": 0.002,
      "step": 23230
    },
    {
      "epoch": 1.2394666666666667,
      "grad_norm": 0.4620043933391571,
      "learning_rate": 4.225333333333333e-05,
      "loss": 0.0027,
      "step": 23240
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.45360609889030457,
      "learning_rate": 4.2250000000000004e-05,
      "loss": 0.0032,
      "step": 23250
    },
    {
      "epoch": 1.2405333333333333,
      "grad_norm": 0.22532476484775543,
      "learning_rate": 4.224666666666667e-05,
      "loss": 0.0039,
      "step": 23260
    },
    {
      "epoch": 1.2410666666666668,
      "grad_norm": 0.362122118473053,
      "learning_rate": 4.2243333333333336e-05,
      "loss": 0.0024,
      "step": 23270
    },
    {
      "epoch": 1.2416,
      "grad_norm": 0.371853232383728,
      "learning_rate": 4.224e-05,
      "loss": 0.003,
      "step": 23280
    },
    {
      "epoch": 1.2421333333333333,
      "grad_norm": 0.5597253441810608,
      "learning_rate": 4.223666666666667e-05,
      "loss": 0.0037,
      "step": 23290
    },
    {
      "epoch": 1.2426666666666666,
      "grad_norm": 0.0917108952999115,
      "learning_rate": 4.2233333333333334e-05,
      "loss": 0.0034,
      "step": 23300
    },
    {
      "epoch": 1.2432,
      "grad_norm": 0.26670974493026733,
      "learning_rate": 4.223e-05,
      "loss": 0.0032,
      "step": 23310
    },
    {
      "epoch": 1.2437333333333334,
      "grad_norm": 0.31574079394340515,
      "learning_rate": 4.222666666666667e-05,
      "loss": 0.0042,
      "step": 23320
    },
    {
      "epoch": 1.2442666666666666,
      "grad_norm": 0.25758272409439087,
      "learning_rate": 4.222333333333334e-05,
      "loss": 0.0038,
      "step": 23330
    },
    {
      "epoch": 1.2448,
      "grad_norm": 0.05208355560898781,
      "learning_rate": 4.2220000000000006e-05,
      "loss": 0.0029,
      "step": 23340
    },
    {
      "epoch": 1.2453333333333334,
      "grad_norm": 0.4889764189720154,
      "learning_rate": 4.221666666666667e-05,
      "loss": 0.0032,
      "step": 23350
    },
    {
      "epoch": 1.2458666666666667,
      "grad_norm": 0.1522703468799591,
      "learning_rate": 4.221333333333334e-05,
      "loss": 0.0024,
      "step": 23360
    },
    {
      "epoch": 1.2464,
      "grad_norm": 0.27478551864624023,
      "learning_rate": 4.221e-05,
      "loss": 0.003,
      "step": 23370
    },
    {
      "epoch": 1.2469333333333332,
      "grad_norm": 0.2776215076446533,
      "learning_rate": 4.2206666666666663e-05,
      "loss": 0.0026,
      "step": 23380
    },
    {
      "epoch": 1.2474666666666667,
      "grad_norm": 0.7934750914573669,
      "learning_rate": 4.2203333333333336e-05,
      "loss": 0.0025,
      "step": 23390
    },
    {
      "epoch": 1.248,
      "grad_norm": 0.6726871728897095,
      "learning_rate": 4.22e-05,
      "loss": 0.0041,
      "step": 23400
    },
    {
      "epoch": 1.2485333333333333,
      "grad_norm": 0.27940285205841064,
      "learning_rate": 4.219666666666667e-05,
      "loss": 0.0037,
      "step": 23410
    },
    {
      "epoch": 1.2490666666666668,
      "grad_norm": 0.15440905094146729,
      "learning_rate": 4.2193333333333335e-05,
      "loss": 0.003,
      "step": 23420
    },
    {
      "epoch": 1.2496,
      "grad_norm": 0.09276391565799713,
      "learning_rate": 4.219e-05,
      "loss": 0.0018,
      "step": 23430
    },
    {
      "epoch": 1.2501333333333333,
      "grad_norm": 0.345998078584671,
      "learning_rate": 4.218666666666667e-05,
      "loss": 0.0029,
      "step": 23440
    },
    {
      "epoch": 1.2506666666666666,
      "grad_norm": 0.12643736600875854,
      "learning_rate": 4.218333333333333e-05,
      "loss": 0.0051,
      "step": 23450
    },
    {
      "epoch": 1.2511999999999999,
      "grad_norm": 0.07285187393426895,
      "learning_rate": 4.2180000000000006e-05,
      "loss": 0.0036,
      "step": 23460
    },
    {
      "epoch": 1.2517333333333334,
      "grad_norm": 0.2215534746646881,
      "learning_rate": 4.217666666666667e-05,
      "loss": 0.0037,
      "step": 23470
    },
    {
      "epoch": 1.2522666666666666,
      "grad_norm": 0.1571033000946045,
      "learning_rate": 4.217333333333334e-05,
      "loss": 0.0026,
      "step": 23480
    },
    {
      "epoch": 1.2528000000000001,
      "grad_norm": 0.4658967852592468,
      "learning_rate": 4.2170000000000005e-05,
      "loss": 0.0034,
      "step": 23490
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 0.12714098393917084,
      "learning_rate": 4.216666666666667e-05,
      "loss": 0.0025,
      "step": 23500
    },
    {
      "epoch": 1.2538666666666667,
      "grad_norm": 0.129648819565773,
      "learning_rate": 4.216333333333334e-05,
      "loss": 0.0032,
      "step": 23510
    },
    {
      "epoch": 1.2544,
      "grad_norm": 0.3080475628376007,
      "learning_rate": 4.2159999999999996e-05,
      "loss": 0.0028,
      "step": 23520
    },
    {
      "epoch": 1.2549333333333332,
      "grad_norm": 0.6359144449234009,
      "learning_rate": 4.215666666666667e-05,
      "loss": 0.003,
      "step": 23530
    },
    {
      "epoch": 1.2554666666666667,
      "grad_norm": 0.47880661487579346,
      "learning_rate": 4.2153333333333335e-05,
      "loss": 0.0029,
      "step": 23540
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.2720094919204712,
      "learning_rate": 4.215e-05,
      "loss": 0.0029,
      "step": 23550
    },
    {
      "epoch": 1.2565333333333333,
      "grad_norm": 0.2111516147851944,
      "learning_rate": 4.214666666666667e-05,
      "loss": 0.0033,
      "step": 23560
    },
    {
      "epoch": 1.2570666666666668,
      "grad_norm": 0.416644424200058,
      "learning_rate": 4.2143333333333334e-05,
      "loss": 0.0041,
      "step": 23570
    },
    {
      "epoch": 1.2576,
      "grad_norm": 0.24496152997016907,
      "learning_rate": 4.214e-05,
      "loss": 0.0034,
      "step": 23580
    },
    {
      "epoch": 1.2581333333333333,
      "grad_norm": 0.5640864968299866,
      "learning_rate": 4.2136666666666666e-05,
      "loss": 0.0034,
      "step": 23590
    },
    {
      "epoch": 1.2586666666666666,
      "grad_norm": 0.328288197517395,
      "learning_rate": 4.213333333333334e-05,
      "loss": 0.0036,
      "step": 23600
    },
    {
      "epoch": 1.2591999999999999,
      "grad_norm": 0.18302254378795624,
      "learning_rate": 4.2130000000000005e-05,
      "loss": 0.0039,
      "step": 23610
    },
    {
      "epoch": 1.2597333333333334,
      "grad_norm": 0.12286310642957687,
      "learning_rate": 4.212666666666667e-05,
      "loss": 0.0023,
      "step": 23620
    },
    {
      "epoch": 1.2602666666666666,
      "grad_norm": 0.5382181406021118,
      "learning_rate": 4.212333333333334e-05,
      "loss": 0.0027,
      "step": 23630
    },
    {
      "epoch": 1.2608,
      "grad_norm": 0.4423075318336487,
      "learning_rate": 4.212e-05,
      "loss": 0.0029,
      "step": 23640
    },
    {
      "epoch": 1.2613333333333334,
      "grad_norm": 0.0364486426115036,
      "learning_rate": 4.211666666666667e-05,
      "loss": 0.0025,
      "step": 23650
    },
    {
      "epoch": 1.2618666666666667,
      "grad_norm": 0.29618170857429504,
      "learning_rate": 4.2113333333333336e-05,
      "loss": 0.0043,
      "step": 23660
    },
    {
      "epoch": 1.2624,
      "grad_norm": 0.33553189039230347,
      "learning_rate": 4.211e-05,
      "loss": 0.0037,
      "step": 23670
    },
    {
      "epoch": 1.2629333333333332,
      "grad_norm": 0.3467106819152832,
      "learning_rate": 4.210666666666667e-05,
      "loss": 0.0028,
      "step": 23680
    },
    {
      "epoch": 1.2634666666666667,
      "grad_norm": 0.4458425045013428,
      "learning_rate": 4.2103333333333334e-05,
      "loss": 0.0043,
      "step": 23690
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.09032145887613297,
      "learning_rate": 4.21e-05,
      "loss": 0.0026,
      "step": 23700
    },
    {
      "epoch": 1.2645333333333333,
      "grad_norm": 0.3978344798088074,
      "learning_rate": 4.2096666666666666e-05,
      "loss": 0.0038,
      "step": 23710
    },
    {
      "epoch": 1.2650666666666668,
      "grad_norm": 0.27513518929481506,
      "learning_rate": 4.209333333333333e-05,
      "loss": 0.0031,
      "step": 23720
    },
    {
      "epoch": 1.2656,
      "grad_norm": 0.23173126578330994,
      "learning_rate": 4.209e-05,
      "loss": 0.0034,
      "step": 23730
    },
    {
      "epoch": 1.2661333333333333,
      "grad_norm": 0.09311164915561676,
      "learning_rate": 4.208666666666667e-05,
      "loss": 0.0029,
      "step": 23740
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.13464799523353577,
      "learning_rate": 4.208333333333334e-05,
      "loss": 0.003,
      "step": 23750
    },
    {
      "epoch": 1.2671999999999999,
      "grad_norm": 0.09186787903308868,
      "learning_rate": 4.2080000000000004e-05,
      "loss": 0.0017,
      "step": 23760
    },
    {
      "epoch": 1.2677333333333334,
      "grad_norm": 0.2800455391407013,
      "learning_rate": 4.207666666666667e-05,
      "loss": 0.0025,
      "step": 23770
    },
    {
      "epoch": 1.2682666666666667,
      "grad_norm": 0.3558928370475769,
      "learning_rate": 4.2073333333333336e-05,
      "loss": 0.0036,
      "step": 23780
    },
    {
      "epoch": 1.2688,
      "grad_norm": 0.030557410791516304,
      "learning_rate": 4.207e-05,
      "loss": 0.0033,
      "step": 23790
    },
    {
      "epoch": 1.2693333333333334,
      "grad_norm": 0.2755817770957947,
      "learning_rate": 4.206666666666667e-05,
      "loss": 0.0024,
      "step": 23800
    },
    {
      "epoch": 1.2698666666666667,
      "grad_norm": 0.16637355089187622,
      "learning_rate": 4.206333333333334e-05,
      "loss": 0.0026,
      "step": 23810
    },
    {
      "epoch": 1.2704,
      "grad_norm": 0.26322296261787415,
      "learning_rate": 4.206e-05,
      "loss": 0.0026,
      "step": 23820
    },
    {
      "epoch": 1.2709333333333332,
      "grad_norm": 0.635650098323822,
      "learning_rate": 4.205666666666667e-05,
      "loss": 0.0027,
      "step": 23830
    },
    {
      "epoch": 1.2714666666666667,
      "grad_norm": 0.052071742713451385,
      "learning_rate": 4.205333333333333e-05,
      "loss": 0.0032,
      "step": 23840
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.18010468780994415,
      "learning_rate": 4.205e-05,
      "loss": 0.0034,
      "step": 23850
    },
    {
      "epoch": 1.2725333333333333,
      "grad_norm": 0.3911316990852356,
      "learning_rate": 4.2046666666666665e-05,
      "loss": 0.0021,
      "step": 23860
    },
    {
      "epoch": 1.2730666666666668,
      "grad_norm": 0.2115338146686554,
      "learning_rate": 4.204333333333334e-05,
      "loss": 0.0027,
      "step": 23870
    },
    {
      "epoch": 1.2736,
      "grad_norm": 0.39096200466156006,
      "learning_rate": 4.2040000000000004e-05,
      "loss": 0.0027,
      "step": 23880
    },
    {
      "epoch": 1.2741333333333333,
      "grad_norm": 0.061318833380937576,
      "learning_rate": 4.203666666666667e-05,
      "loss": 0.0015,
      "step": 23890
    },
    {
      "epoch": 1.2746666666666666,
      "grad_norm": 0.41894397139549255,
      "learning_rate": 4.2033333333333336e-05,
      "loss": 0.0036,
      "step": 23900
    },
    {
      "epoch": 1.2752,
      "grad_norm": 0.478004515171051,
      "learning_rate": 4.203e-05,
      "loss": 0.0034,
      "step": 23910
    },
    {
      "epoch": 1.2757333333333334,
      "grad_norm": 0.30997151136398315,
      "learning_rate": 4.202666666666667e-05,
      "loss": 0.002,
      "step": 23920
    },
    {
      "epoch": 1.2762666666666667,
      "grad_norm": 0.593405544757843,
      "learning_rate": 4.2023333333333335e-05,
      "loss": 0.003,
      "step": 23930
    },
    {
      "epoch": 1.2768,
      "grad_norm": 0.8408013582229614,
      "learning_rate": 4.202e-05,
      "loss": 0.0026,
      "step": 23940
    },
    {
      "epoch": 1.2773333333333334,
      "grad_norm": 0.4576554298400879,
      "learning_rate": 4.2016666666666674e-05,
      "loss": 0.0042,
      "step": 23950
    },
    {
      "epoch": 1.2778666666666667,
      "grad_norm": 0.5039870142936707,
      "learning_rate": 4.201333333333334e-05,
      "loss": 0.0036,
      "step": 23960
    },
    {
      "epoch": 1.2784,
      "grad_norm": 0.18504449725151062,
      "learning_rate": 4.201e-05,
      "loss": 0.0033,
      "step": 23970
    },
    {
      "epoch": 1.2789333333333333,
      "grad_norm": 0.4790736436843872,
      "learning_rate": 4.2006666666666665e-05,
      "loss": 0.0032,
      "step": 23980
    },
    {
      "epoch": 1.2794666666666665,
      "grad_norm": 0.520769476890564,
      "learning_rate": 4.200333333333333e-05,
      "loss": 0.0023,
      "step": 23990
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.06676176935434341,
      "learning_rate": 4.2e-05,
      "loss": 0.0031,
      "step": 24000
    },
    {
      "epoch": 1.2805333333333333,
      "grad_norm": 0.24777625501155853,
      "learning_rate": 4.199666666666667e-05,
      "loss": 0.0028,
      "step": 24010
    },
    {
      "epoch": 1.2810666666666668,
      "grad_norm": 0.24698224663734436,
      "learning_rate": 4.199333333333334e-05,
      "loss": 0.0027,
      "step": 24020
    },
    {
      "epoch": 1.2816,
      "grad_norm": 0.0988069698214531,
      "learning_rate": 4.199e-05,
      "loss": 0.0029,
      "step": 24030
    },
    {
      "epoch": 1.2821333333333333,
      "grad_norm": 0.2556568682193756,
      "learning_rate": 4.198666666666667e-05,
      "loss": 0.0021,
      "step": 24040
    },
    {
      "epoch": 1.2826666666666666,
      "grad_norm": 0.23862850666046143,
      "learning_rate": 4.1983333333333335e-05,
      "loss": 0.0019,
      "step": 24050
    },
    {
      "epoch": 1.2832,
      "grad_norm": 0.33283454179763794,
      "learning_rate": 4.198e-05,
      "loss": 0.0021,
      "step": 24060
    },
    {
      "epoch": 1.2837333333333334,
      "grad_norm": 0.1784517765045166,
      "learning_rate": 4.197666666666667e-05,
      "loss": 0.0042,
      "step": 24070
    },
    {
      "epoch": 1.2842666666666667,
      "grad_norm": 0.21715016663074493,
      "learning_rate": 4.1973333333333334e-05,
      "loss": 0.0029,
      "step": 24080
    },
    {
      "epoch": 1.2848,
      "grad_norm": 0.6640933156013489,
      "learning_rate": 4.1970000000000006e-05,
      "loss": 0.0032,
      "step": 24090
    },
    {
      "epoch": 1.2853333333333334,
      "grad_norm": 0.1793823391199112,
      "learning_rate": 4.196666666666667e-05,
      "loss": 0.0037,
      "step": 24100
    },
    {
      "epoch": 1.2858666666666667,
      "grad_norm": 0.5860533118247986,
      "learning_rate": 4.196333333333334e-05,
      "loss": 0.0026,
      "step": 24110
    },
    {
      "epoch": 1.2864,
      "grad_norm": 0.1860744059085846,
      "learning_rate": 4.196e-05,
      "loss": 0.003,
      "step": 24120
    },
    {
      "epoch": 1.2869333333333333,
      "grad_norm": 0.14950062334537506,
      "learning_rate": 4.1956666666666664e-05,
      "loss": 0.0032,
      "step": 24130
    },
    {
      "epoch": 1.2874666666666665,
      "grad_norm": 0.4847046434879303,
      "learning_rate": 4.195333333333333e-05,
      "loss": 0.0019,
      "step": 24140
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.06354909390211105,
      "learning_rate": 4.195e-05,
      "loss": 0.004,
      "step": 24150
    },
    {
      "epoch": 1.2885333333333333,
      "grad_norm": 0.03547462448477745,
      "learning_rate": 4.194666666666667e-05,
      "loss": 0.0015,
      "step": 24160
    },
    {
      "epoch": 1.2890666666666668,
      "grad_norm": 0.2189953774213791,
      "learning_rate": 4.1943333333333336e-05,
      "loss": 0.0022,
      "step": 24170
    },
    {
      "epoch": 1.2896,
      "grad_norm": 0.3011954426765442,
      "learning_rate": 4.194e-05,
      "loss": 0.0038,
      "step": 24180
    },
    {
      "epoch": 1.2901333333333334,
      "grad_norm": 0.01680898107588291,
      "learning_rate": 4.193666666666667e-05,
      "loss": 0.0034,
      "step": 24190
    },
    {
      "epoch": 1.2906666666666666,
      "grad_norm": 0.6677437424659729,
      "learning_rate": 4.1933333333333334e-05,
      "loss": 0.005,
      "step": 24200
    },
    {
      "epoch": 1.2912,
      "grad_norm": 0.2722669541835785,
      "learning_rate": 4.193e-05,
      "loss": 0.0024,
      "step": 24210
    },
    {
      "epoch": 1.2917333333333334,
      "grad_norm": 0.09265026450157166,
      "learning_rate": 4.192666666666667e-05,
      "loss": 0.0029,
      "step": 24220
    },
    {
      "epoch": 1.2922666666666667,
      "grad_norm": 0.16962939500808716,
      "learning_rate": 4.192333333333334e-05,
      "loss": 0.0031,
      "step": 24230
    },
    {
      "epoch": 1.2928,
      "grad_norm": 0.5762390494346619,
      "learning_rate": 4.1920000000000005e-05,
      "loss": 0.0033,
      "step": 24240
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 0.45305708050727844,
      "learning_rate": 4.191666666666667e-05,
      "loss": 0.0043,
      "step": 24250
    },
    {
      "epoch": 1.2938666666666667,
      "grad_norm": 0.36910712718963623,
      "learning_rate": 4.191333333333334e-05,
      "loss": 0.0041,
      "step": 24260
    },
    {
      "epoch": 1.2944,
      "grad_norm": 0.271597683429718,
      "learning_rate": 4.191e-05,
      "loss": 0.0036,
      "step": 24270
    },
    {
      "epoch": 1.2949333333333333,
      "grad_norm": 0.20954342186450958,
      "learning_rate": 4.190666666666666e-05,
      "loss": 0.0031,
      "step": 24280
    },
    {
      "epoch": 1.2954666666666665,
      "grad_norm": 0.3753986954689026,
      "learning_rate": 4.1903333333333336e-05,
      "loss": 0.0027,
      "step": 24290
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.8733798861503601,
      "learning_rate": 4.19e-05,
      "loss": 0.0036,
      "step": 24300
    },
    {
      "epoch": 1.2965333333333333,
      "grad_norm": 0.2423468381166458,
      "learning_rate": 4.189666666666667e-05,
      "loss": 0.0034,
      "step": 24310
    },
    {
      "epoch": 1.2970666666666666,
      "grad_norm": 0.1586552858352661,
      "learning_rate": 4.1893333333333334e-05,
      "loss": 0.0042,
      "step": 24320
    },
    {
      "epoch": 1.2976,
      "grad_norm": 0.04040572792291641,
      "learning_rate": 4.189e-05,
      "loss": 0.0036,
      "step": 24330
    },
    {
      "epoch": 1.2981333333333334,
      "grad_norm": 0.051299020648002625,
      "learning_rate": 4.1886666666666667e-05,
      "loss": 0.0031,
      "step": 24340
    },
    {
      "epoch": 1.2986666666666666,
      "grad_norm": 0.12561431527137756,
      "learning_rate": 4.188333333333333e-05,
      "loss": 0.003,
      "step": 24350
    },
    {
      "epoch": 1.2992,
      "grad_norm": 0.43993788957595825,
      "learning_rate": 4.1880000000000006e-05,
      "loss": 0.004,
      "step": 24360
    },
    {
      "epoch": 1.2997333333333334,
      "grad_norm": 0.3619249165058136,
      "learning_rate": 4.187666666666667e-05,
      "loss": 0.005,
      "step": 24370
    },
    {
      "epoch": 1.3002666666666667,
      "grad_norm": 0.6703935861587524,
      "learning_rate": 4.187333333333334e-05,
      "loss": 0.0027,
      "step": 24380
    },
    {
      "epoch": 1.3008,
      "grad_norm": 0.6895641088485718,
      "learning_rate": 4.1870000000000004e-05,
      "loss": 0.0055,
      "step": 24390
    },
    {
      "epoch": 1.3013333333333335,
      "grad_norm": 0.5735527873039246,
      "learning_rate": 4.186666666666667e-05,
      "loss": 0.0028,
      "step": 24400
    },
    {
      "epoch": 1.3018666666666667,
      "grad_norm": 0.24538156390190125,
      "learning_rate": 4.1863333333333336e-05,
      "loss": 0.0029,
      "step": 24410
    },
    {
      "epoch": 1.3024,
      "grad_norm": 0.06375986337661743,
      "learning_rate": 4.186e-05,
      "loss": 0.003,
      "step": 24420
    },
    {
      "epoch": 1.3029333333333333,
      "grad_norm": 0.2525610029697418,
      "learning_rate": 4.185666666666667e-05,
      "loss": 0.0036,
      "step": 24430
    },
    {
      "epoch": 1.3034666666666666,
      "grad_norm": 0.06558695435523987,
      "learning_rate": 4.1853333333333335e-05,
      "loss": 0.0039,
      "step": 24440
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.03971016779541969,
      "learning_rate": 4.185e-05,
      "loss": 0.0034,
      "step": 24450
    },
    {
      "epoch": 1.3045333333333333,
      "grad_norm": 0.08618505299091339,
      "learning_rate": 4.184666666666667e-05,
      "loss": 0.0024,
      "step": 24460
    },
    {
      "epoch": 1.3050666666666666,
      "grad_norm": 0.5197070240974426,
      "learning_rate": 4.184333333333333e-05,
      "loss": 0.0029,
      "step": 24470
    },
    {
      "epoch": 1.3056,
      "grad_norm": 0.23116368055343628,
      "learning_rate": 4.184e-05,
      "loss": 0.002,
      "step": 24480
    },
    {
      "epoch": 1.3061333333333334,
      "grad_norm": 0.15221188962459564,
      "learning_rate": 4.1836666666666665e-05,
      "loss": 0.0032,
      "step": 24490
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 0.47445446252822876,
      "learning_rate": 4.183333333333334e-05,
      "loss": 0.0026,
      "step": 24500
    },
    {
      "epoch": 1.3072,
      "grad_norm": 0.8311378359794617,
      "learning_rate": 4.1830000000000004e-05,
      "loss": 0.0023,
      "step": 24510
    },
    {
      "epoch": 1.3077333333333334,
      "grad_norm": 0.26567575335502625,
      "learning_rate": 4.182666666666667e-05,
      "loss": 0.0023,
      "step": 24520
    },
    {
      "epoch": 1.3082666666666667,
      "grad_norm": 0.11162658780813217,
      "learning_rate": 4.182333333333334e-05,
      "loss": 0.004,
      "step": 24530
    },
    {
      "epoch": 1.3088,
      "grad_norm": 0.1261458843946457,
      "learning_rate": 4.182e-05,
      "loss": 0.0024,
      "step": 24540
    },
    {
      "epoch": 1.3093333333333335,
      "grad_norm": 0.26544028520584106,
      "learning_rate": 4.181666666666667e-05,
      "loss": 0.0034,
      "step": 24550
    },
    {
      "epoch": 1.3098666666666667,
      "grad_norm": 0.0968935489654541,
      "learning_rate": 4.1813333333333335e-05,
      "loss": 0.0027,
      "step": 24560
    },
    {
      "epoch": 1.3104,
      "grad_norm": 0.1803537905216217,
      "learning_rate": 4.181000000000001e-05,
      "loss": 0.0027,
      "step": 24570
    },
    {
      "epoch": 1.3109333333333333,
      "grad_norm": 0.33049923181533813,
      "learning_rate": 4.180666666666667e-05,
      "loss": 0.0021,
      "step": 24580
    },
    {
      "epoch": 1.3114666666666666,
      "grad_norm": 0.2092248797416687,
      "learning_rate": 4.1803333333333333e-05,
      "loss": 0.0037,
      "step": 24590
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.3593742549419403,
      "learning_rate": 4.18e-05,
      "loss": 0.0024,
      "step": 24600
    },
    {
      "epoch": 1.3125333333333333,
      "grad_norm": 0.030551530420780182,
      "learning_rate": 4.1796666666666666e-05,
      "loss": 0.003,
      "step": 24610
    },
    {
      "epoch": 1.3130666666666666,
      "grad_norm": 0.31394463777542114,
      "learning_rate": 4.179333333333333e-05,
      "loss": 0.0023,
      "step": 24620
    },
    {
      "epoch": 1.3136,
      "grad_norm": 0.10038008540868759,
      "learning_rate": 4.179e-05,
      "loss": 0.0025,
      "step": 24630
    },
    {
      "epoch": 1.3141333333333334,
      "grad_norm": 0.5790499448776245,
      "learning_rate": 4.178666666666667e-05,
      "loss": 0.0028,
      "step": 24640
    },
    {
      "epoch": 1.3146666666666667,
      "grad_norm": 0.6864331364631653,
      "learning_rate": 4.178333333333334e-05,
      "loss": 0.0032,
      "step": 24650
    },
    {
      "epoch": 1.3152,
      "grad_norm": 0.1704864203929901,
      "learning_rate": 4.178e-05,
      "loss": 0.0025,
      "step": 24660
    },
    {
      "epoch": 1.3157333333333332,
      "grad_norm": 0.07865557074546814,
      "learning_rate": 4.177666666666667e-05,
      "loss": 0.0036,
      "step": 24670
    },
    {
      "epoch": 1.3162666666666667,
      "grad_norm": 0.4531044363975525,
      "learning_rate": 4.1773333333333335e-05,
      "loss": 0.0022,
      "step": 24680
    },
    {
      "epoch": 1.3168,
      "grad_norm": 0.18634603917598724,
      "learning_rate": 4.177e-05,
      "loss": 0.0039,
      "step": 24690
    },
    {
      "epoch": 1.3173333333333335,
      "grad_norm": 0.45032942295074463,
      "learning_rate": 4.176666666666667e-05,
      "loss": 0.0035,
      "step": 24700
    },
    {
      "epoch": 1.3178666666666667,
      "grad_norm": 0.20865237712860107,
      "learning_rate": 4.176333333333334e-05,
      "loss": 0.0035,
      "step": 24710
    },
    {
      "epoch": 1.3184,
      "grad_norm": 0.06528595089912415,
      "learning_rate": 4.176000000000001e-05,
      "loss": 0.004,
      "step": 24720
    },
    {
      "epoch": 1.3189333333333333,
      "grad_norm": 0.13643240928649902,
      "learning_rate": 4.1756666666666666e-05,
      "loss": 0.0028,
      "step": 24730
    },
    {
      "epoch": 1.3194666666666666,
      "grad_norm": 0.3320055603981018,
      "learning_rate": 4.175333333333333e-05,
      "loss": 0.0034,
      "step": 24740
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.17911845445632935,
      "learning_rate": 4.175e-05,
      "loss": 0.0024,
      "step": 24750
    },
    {
      "epoch": 1.3205333333333333,
      "grad_norm": 0.03251858055591583,
      "learning_rate": 4.1746666666666665e-05,
      "loss": 0.0028,
      "step": 24760
    },
    {
      "epoch": 1.3210666666666666,
      "grad_norm": 0.9016792178153992,
      "learning_rate": 4.174333333333334e-05,
      "loss": 0.0035,
      "step": 24770
    },
    {
      "epoch": 1.3216,
      "grad_norm": 0.6015541553497314,
      "learning_rate": 4.1740000000000004e-05,
      "loss": 0.0033,
      "step": 24780
    },
    {
      "epoch": 1.3221333333333334,
      "grad_norm": 0.5147659182548523,
      "learning_rate": 4.173666666666667e-05,
      "loss": 0.0027,
      "step": 24790
    },
    {
      "epoch": 1.3226666666666667,
      "grad_norm": 0.1758679449558258,
      "learning_rate": 4.1733333333333336e-05,
      "loss": 0.0032,
      "step": 24800
    },
    {
      "epoch": 1.3232,
      "grad_norm": 0.2469043880701065,
      "learning_rate": 4.173e-05,
      "loss": 0.0031,
      "step": 24810
    },
    {
      "epoch": 1.3237333333333332,
      "grad_norm": 0.5105165839195251,
      "learning_rate": 4.172666666666667e-05,
      "loss": 0.0044,
      "step": 24820
    },
    {
      "epoch": 1.3242666666666667,
      "grad_norm": 0.335858017206192,
      "learning_rate": 4.1723333333333334e-05,
      "loss": 0.003,
      "step": 24830
    },
    {
      "epoch": 1.3248,
      "grad_norm": 0.2183023989200592,
      "learning_rate": 4.172e-05,
      "loss": 0.0036,
      "step": 24840
    },
    {
      "epoch": 1.3253333333333333,
      "grad_norm": 0.3630632162094116,
      "learning_rate": 4.171666666666667e-05,
      "loss": 0.0026,
      "step": 24850
    },
    {
      "epoch": 1.3258666666666667,
      "grad_norm": 0.30406996607780457,
      "learning_rate": 4.171333333333334e-05,
      "loss": 0.0026,
      "step": 24860
    },
    {
      "epoch": 1.3264,
      "grad_norm": 0.1853097528219223,
      "learning_rate": 4.1710000000000006e-05,
      "loss": 0.0034,
      "step": 24870
    },
    {
      "epoch": 1.3269333333333333,
      "grad_norm": 0.025463271886110306,
      "learning_rate": 4.1706666666666665e-05,
      "loss": 0.0028,
      "step": 24880
    },
    {
      "epoch": 1.3274666666666666,
      "grad_norm": 0.38452455401420593,
      "learning_rate": 4.170333333333333e-05,
      "loss": 0.0027,
      "step": 24890
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.18184147775173187,
      "learning_rate": 4.17e-05,
      "loss": 0.0034,
      "step": 24900
    },
    {
      "epoch": 1.3285333333333333,
      "grad_norm": 0.25639986991882324,
      "learning_rate": 4.169666666666667e-05,
      "loss": 0.0024,
      "step": 24910
    },
    {
      "epoch": 1.3290666666666666,
      "grad_norm": 0.39452847838401794,
      "learning_rate": 4.1693333333333336e-05,
      "loss": 0.0028,
      "step": 24920
    },
    {
      "epoch": 1.3296000000000001,
      "grad_norm": 0.2955365777015686,
      "learning_rate": 4.169e-05,
      "loss": 0.0032,
      "step": 24930
    },
    {
      "epoch": 1.3301333333333334,
      "grad_norm": 0.6291749477386475,
      "learning_rate": 4.168666666666667e-05,
      "loss": 0.0031,
      "step": 24940
    },
    {
      "epoch": 1.3306666666666667,
      "grad_norm": 0.015895258635282516,
      "learning_rate": 4.1683333333333335e-05,
      "loss": 0.0029,
      "step": 24950
    },
    {
      "epoch": 1.3312,
      "grad_norm": 0.24290470778942108,
      "learning_rate": 4.168e-05,
      "loss": 0.0034,
      "step": 24960
    },
    {
      "epoch": 1.3317333333333332,
      "grad_norm": 0.28902468085289,
      "learning_rate": 4.167666666666667e-05,
      "loss": 0.0033,
      "step": 24970
    },
    {
      "epoch": 1.3322666666666667,
      "grad_norm": 0.043244004249572754,
      "learning_rate": 4.167333333333334e-05,
      "loss": 0.0023,
      "step": 24980
    },
    {
      "epoch": 1.3328,
      "grad_norm": 0.24473898112773895,
      "learning_rate": 4.1670000000000006e-05,
      "loss": 0.0048,
      "step": 24990
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.02260655164718628,
      "learning_rate": 4.166666666666667e-05,
      "loss": 0.0027,
      "step": 25000
    },
    {
      "epoch": 1.3338666666666668,
      "grad_norm": 0.479513019323349,
      "learning_rate": 4.166333333333334e-05,
      "loss": 0.0043,
      "step": 25010
    },
    {
      "epoch": 1.3344,
      "grad_norm": 0.3675288259983063,
      "learning_rate": 4.1660000000000004e-05,
      "loss": 0.0028,
      "step": 25020
    },
    {
      "epoch": 1.3349333333333333,
      "grad_norm": 0.47846877574920654,
      "learning_rate": 4.1656666666666664e-05,
      "loss": 0.0026,
      "step": 25030
    },
    {
      "epoch": 1.3354666666666666,
      "grad_norm": 0.06305120140314102,
      "learning_rate": 4.165333333333333e-05,
      "loss": 0.0052,
      "step": 25040
    },
    {
      "epoch": 1.336,
      "grad_norm": 0.23849396407604218,
      "learning_rate": 4.165e-05,
      "loss": 0.0029,
      "step": 25050
    },
    {
      "epoch": 1.3365333333333334,
      "grad_norm": 0.23631200194358826,
      "learning_rate": 4.164666666666667e-05,
      "loss": 0.0036,
      "step": 25060
    },
    {
      "epoch": 1.3370666666666666,
      "grad_norm": 0.5243529081344604,
      "learning_rate": 4.1643333333333335e-05,
      "loss": 0.0023,
      "step": 25070
    },
    {
      "epoch": 1.3376000000000001,
      "grad_norm": 0.32523107528686523,
      "learning_rate": 4.164e-05,
      "loss": 0.0029,
      "step": 25080
    },
    {
      "epoch": 1.3381333333333334,
      "grad_norm": 0.38078421354293823,
      "learning_rate": 4.163666666666667e-05,
      "loss": 0.0026,
      "step": 25090
    },
    {
      "epoch": 1.3386666666666667,
      "grad_norm": 0.03253703936934471,
      "learning_rate": 4.1633333333333333e-05,
      "loss": 0.0036,
      "step": 25100
    },
    {
      "epoch": 1.3392,
      "grad_norm": 0.127655029296875,
      "learning_rate": 4.163e-05,
      "loss": 0.0027,
      "step": 25110
    },
    {
      "epoch": 1.3397333333333332,
      "grad_norm": 0.29590484499931335,
      "learning_rate": 4.162666666666667e-05,
      "loss": 0.0041,
      "step": 25120
    },
    {
      "epoch": 1.3402666666666667,
      "grad_norm": 0.20493540167808533,
      "learning_rate": 4.162333333333334e-05,
      "loss": 0.0042,
      "step": 25130
    },
    {
      "epoch": 1.3408,
      "grad_norm": 0.3605985641479492,
      "learning_rate": 4.1620000000000005e-05,
      "loss": 0.0023,
      "step": 25140
    },
    {
      "epoch": 1.3413333333333333,
      "grad_norm": 0.05649561062455177,
      "learning_rate": 4.161666666666667e-05,
      "loss": 0.0029,
      "step": 25150
    },
    {
      "epoch": 1.3418666666666668,
      "grad_norm": 0.4782433807849884,
      "learning_rate": 4.161333333333334e-05,
      "loss": 0.0034,
      "step": 25160
    },
    {
      "epoch": 1.3424,
      "grad_norm": 0.3368692100048065,
      "learning_rate": 4.161e-05,
      "loss": 0.0025,
      "step": 25170
    },
    {
      "epoch": 1.3429333333333333,
      "grad_norm": 0.6040979623794556,
      "learning_rate": 4.160666666666667e-05,
      "loss": 0.0038,
      "step": 25180
    },
    {
      "epoch": 1.3434666666666666,
      "grad_norm": 0.2772095799446106,
      "learning_rate": 4.1603333333333335e-05,
      "loss": 0.003,
      "step": 25190
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.384784072637558,
      "learning_rate": 4.16e-05,
      "loss": 0.0031,
      "step": 25200
    },
    {
      "epoch": 1.3445333333333334,
      "grad_norm": 0.3854370713233948,
      "learning_rate": 4.159666666666667e-05,
      "loss": 0.0033,
      "step": 25210
    },
    {
      "epoch": 1.3450666666666666,
      "grad_norm": 0.38865017890930176,
      "learning_rate": 4.1593333333333334e-05,
      "loss": 0.0028,
      "step": 25220
    },
    {
      "epoch": 1.3456000000000001,
      "grad_norm": 0.6372561454772949,
      "learning_rate": 4.159e-05,
      "loss": 0.0025,
      "step": 25230
    },
    {
      "epoch": 1.3461333333333334,
      "grad_norm": 0.5670603513717651,
      "learning_rate": 4.1586666666666666e-05,
      "loss": 0.0027,
      "step": 25240
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.3864452838897705,
      "learning_rate": 4.158333333333333e-05,
      "loss": 0.0037,
      "step": 25250
    },
    {
      "epoch": 1.3472,
      "grad_norm": 0.48301607370376587,
      "learning_rate": 4.1580000000000005e-05,
      "loss": 0.004,
      "step": 25260
    },
    {
      "epoch": 1.3477333333333332,
      "grad_norm": 0.04597172513604164,
      "learning_rate": 4.157666666666667e-05,
      "loss": 0.0032,
      "step": 25270
    },
    {
      "epoch": 1.3482666666666667,
      "grad_norm": 0.29814377427101135,
      "learning_rate": 4.157333333333334e-05,
      "loss": 0.0028,
      "step": 25280
    },
    {
      "epoch": 1.3488,
      "grad_norm": 0.2138485312461853,
      "learning_rate": 4.1570000000000003e-05,
      "loss": 0.0027,
      "step": 25290
    },
    {
      "epoch": 1.3493333333333333,
      "grad_norm": 0.839535117149353,
      "learning_rate": 4.156666666666667e-05,
      "loss": 0.0025,
      "step": 25300
    },
    {
      "epoch": 1.3498666666666668,
      "grad_norm": 0.27406781911849976,
      "learning_rate": 4.1563333333333336e-05,
      "loss": 0.0026,
      "step": 25310
    },
    {
      "epoch": 1.3504,
      "grad_norm": 0.08958864212036133,
      "learning_rate": 4.156e-05,
      "loss": 0.0041,
      "step": 25320
    },
    {
      "epoch": 1.3509333333333333,
      "grad_norm": 0.506010115146637,
      "learning_rate": 4.155666666666667e-05,
      "loss": 0.0038,
      "step": 25330
    },
    {
      "epoch": 1.3514666666666666,
      "grad_norm": 0.16072048246860504,
      "learning_rate": 4.1553333333333334e-05,
      "loss": 0.0022,
      "step": 25340
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.6260450482368469,
      "learning_rate": 4.155e-05,
      "loss": 0.0029,
      "step": 25350
    },
    {
      "epoch": 1.3525333333333334,
      "grad_norm": 0.26555120944976807,
      "learning_rate": 4.1546666666666666e-05,
      "loss": 0.0018,
      "step": 25360
    },
    {
      "epoch": 1.3530666666666666,
      "grad_norm": 0.15096472203731537,
      "learning_rate": 4.154333333333333e-05,
      "loss": 0.0036,
      "step": 25370
    },
    {
      "epoch": 1.3536000000000001,
      "grad_norm": 0.49183139204978943,
      "learning_rate": 4.154e-05,
      "loss": 0.0026,
      "step": 25380
    },
    {
      "epoch": 1.3541333333333334,
      "grad_norm": 0.060289569199085236,
      "learning_rate": 4.1536666666666665e-05,
      "loss": 0.0023,
      "step": 25390
    },
    {
      "epoch": 1.3546666666666667,
      "grad_norm": 0.23762694001197815,
      "learning_rate": 4.153333333333334e-05,
      "loss": 0.0022,
      "step": 25400
    },
    {
      "epoch": 1.3552,
      "grad_norm": 0.39317041635513306,
      "learning_rate": 4.1530000000000004e-05,
      "loss": 0.0028,
      "step": 25410
    },
    {
      "epoch": 1.3557333333333332,
      "grad_norm": 0.5079315900802612,
      "learning_rate": 4.152666666666667e-05,
      "loss": 0.0027,
      "step": 25420
    },
    {
      "epoch": 1.3562666666666667,
      "grad_norm": 0.2703225612640381,
      "learning_rate": 4.1523333333333336e-05,
      "loss": 0.0027,
      "step": 25430
    },
    {
      "epoch": 1.3568,
      "grad_norm": 0.13203656673431396,
      "learning_rate": 4.152e-05,
      "loss": 0.0018,
      "step": 25440
    },
    {
      "epoch": 1.3573333333333333,
      "grad_norm": 0.5362046957015991,
      "learning_rate": 4.151666666666667e-05,
      "loss": 0.0021,
      "step": 25450
    },
    {
      "epoch": 1.3578666666666668,
      "grad_norm": 0.09577399492263794,
      "learning_rate": 4.1513333333333335e-05,
      "loss": 0.0041,
      "step": 25460
    },
    {
      "epoch": 1.3584,
      "grad_norm": 0.5091995000839233,
      "learning_rate": 4.151000000000001e-05,
      "loss": 0.0029,
      "step": 25470
    },
    {
      "epoch": 1.3589333333333333,
      "grad_norm": 0.15957026183605194,
      "learning_rate": 4.150666666666667e-05,
      "loss": 0.0035,
      "step": 25480
    },
    {
      "epoch": 1.3594666666666666,
      "grad_norm": 0.4951070547103882,
      "learning_rate": 4.150333333333333e-05,
      "loss": 0.0026,
      "step": 25490
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.24998900294303894,
      "learning_rate": 4.15e-05,
      "loss": 0.0027,
      "step": 25500
    },
    {
      "epoch": 1.3605333333333334,
      "grad_norm": 0.4828096926212311,
      "learning_rate": 4.1496666666666665e-05,
      "loss": 0.0026,
      "step": 25510
    },
    {
      "epoch": 1.3610666666666666,
      "grad_norm": 0.033154234290122986,
      "learning_rate": 4.149333333333333e-05,
      "loss": 0.0024,
      "step": 25520
    },
    {
      "epoch": 1.3616,
      "grad_norm": 0.0409037321805954,
      "learning_rate": 4.1490000000000004e-05,
      "loss": 0.0032,
      "step": 25530
    },
    {
      "epoch": 1.3621333333333334,
      "grad_norm": 0.09210044145584106,
      "learning_rate": 4.148666666666667e-05,
      "loss": 0.0035,
      "step": 25540
    },
    {
      "epoch": 1.3626666666666667,
      "grad_norm": 0.26765885949134827,
      "learning_rate": 4.1483333333333337e-05,
      "loss": 0.0021,
      "step": 25550
    },
    {
      "epoch": 1.3632,
      "grad_norm": 0.17912188172340393,
      "learning_rate": 4.148e-05,
      "loss": 0.0039,
      "step": 25560
    },
    {
      "epoch": 1.3637333333333332,
      "grad_norm": 0.24019111692905426,
      "learning_rate": 4.147666666666667e-05,
      "loss": 0.0031,
      "step": 25570
    },
    {
      "epoch": 1.3642666666666667,
      "grad_norm": 0.20884118974208832,
      "learning_rate": 4.1473333333333335e-05,
      "loss": 0.0032,
      "step": 25580
    },
    {
      "epoch": 1.3648,
      "grad_norm": 0.09297595173120499,
      "learning_rate": 4.147e-05,
      "loss": 0.0018,
      "step": 25590
    },
    {
      "epoch": 1.3653333333333333,
      "grad_norm": 0.10276015847921371,
      "learning_rate": 4.146666666666667e-05,
      "loss": 0.0028,
      "step": 25600
    },
    {
      "epoch": 1.3658666666666668,
      "grad_norm": 0.38090306520462036,
      "learning_rate": 4.146333333333334e-05,
      "loss": 0.0037,
      "step": 25610
    },
    {
      "epoch": 1.3664,
      "grad_norm": 0.0719347596168518,
      "learning_rate": 4.1460000000000006e-05,
      "loss": 0.0038,
      "step": 25620
    },
    {
      "epoch": 1.3669333333333333,
      "grad_norm": 0.1844412237405777,
      "learning_rate": 4.145666666666667e-05,
      "loss": 0.0033,
      "step": 25630
    },
    {
      "epoch": 1.3674666666666666,
      "grad_norm": 0.5724583864212036,
      "learning_rate": 4.145333333333333e-05,
      "loss": 0.004,
      "step": 25640
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 0.2540964186191559,
      "learning_rate": 4.145e-05,
      "loss": 0.0022,
      "step": 25650
    },
    {
      "epoch": 1.3685333333333334,
      "grad_norm": 0.5388217568397522,
      "learning_rate": 4.1446666666666664e-05,
      "loss": 0.0031,
      "step": 25660
    },
    {
      "epoch": 1.3690666666666667,
      "grad_norm": 0.4240253269672394,
      "learning_rate": 4.144333333333334e-05,
      "loss": 0.0032,
      "step": 25670
    },
    {
      "epoch": 1.3696,
      "grad_norm": 0.31845760345458984,
      "learning_rate": 4.144e-05,
      "loss": 0.0031,
      "step": 25680
    },
    {
      "epoch": 1.3701333333333334,
      "grad_norm": 0.3020355999469757,
      "learning_rate": 4.143666666666667e-05,
      "loss": 0.003,
      "step": 25690
    },
    {
      "epoch": 1.3706666666666667,
      "grad_norm": 0.29965364933013916,
      "learning_rate": 4.1433333333333335e-05,
      "loss": 0.0027,
      "step": 25700
    },
    {
      "epoch": 1.3712,
      "grad_norm": 0.05145435035228729,
      "learning_rate": 4.143e-05,
      "loss": 0.0039,
      "step": 25710
    },
    {
      "epoch": 1.3717333333333332,
      "grad_norm": 0.595892608165741,
      "learning_rate": 4.142666666666667e-05,
      "loss": 0.0032,
      "step": 25720
    },
    {
      "epoch": 1.3722666666666667,
      "grad_norm": 0.035764824599027634,
      "learning_rate": 4.1423333333333334e-05,
      "loss": 0.0033,
      "step": 25730
    },
    {
      "epoch": 1.3728,
      "grad_norm": 0.5375877022743225,
      "learning_rate": 4.142000000000001e-05,
      "loss": 0.0033,
      "step": 25740
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.055215202271938324,
      "learning_rate": 4.141666666666667e-05,
      "loss": 0.0037,
      "step": 25750
    },
    {
      "epoch": 1.3738666666666668,
      "grad_norm": 0.05863776430487633,
      "learning_rate": 4.141333333333334e-05,
      "loss": 0.0027,
      "step": 25760
    },
    {
      "epoch": 1.3744,
      "grad_norm": 0.17876021564006805,
      "learning_rate": 4.1410000000000005e-05,
      "loss": 0.0037,
      "step": 25770
    },
    {
      "epoch": 1.3749333333333333,
      "grad_norm": 0.30132704973220825,
      "learning_rate": 4.140666666666667e-05,
      "loss": 0.0033,
      "step": 25780
    },
    {
      "epoch": 1.3754666666666666,
      "grad_norm": 0.3242599666118622,
      "learning_rate": 4.140333333333333e-05,
      "loss": 0.0031,
      "step": 25790
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.13429369032382965,
      "learning_rate": 4.14e-05,
      "loss": 0.002,
      "step": 25800
    },
    {
      "epoch": 1.3765333333333334,
      "grad_norm": 0.5922022461891174,
      "learning_rate": 4.139666666666667e-05,
      "loss": 0.0021,
      "step": 25810
    },
    {
      "epoch": 1.3770666666666667,
      "grad_norm": 0.0506557933986187,
      "learning_rate": 4.1393333333333336e-05,
      "loss": 0.0024,
      "step": 25820
    },
    {
      "epoch": 1.3776,
      "grad_norm": 0.39344146847724915,
      "learning_rate": 4.139e-05,
      "loss": 0.0042,
      "step": 25830
    },
    {
      "epoch": 1.3781333333333334,
      "grad_norm": 0.3856249451637268,
      "learning_rate": 4.138666666666667e-05,
      "loss": 0.0025,
      "step": 25840
    },
    {
      "epoch": 1.3786666666666667,
      "grad_norm": 0.29950764775276184,
      "learning_rate": 4.1383333333333334e-05,
      "loss": 0.0026,
      "step": 25850
    },
    {
      "epoch": 1.3792,
      "grad_norm": 0.5106682777404785,
      "learning_rate": 4.138e-05,
      "loss": 0.0024,
      "step": 25860
    },
    {
      "epoch": 1.3797333333333333,
      "grad_norm": 0.27036064863204956,
      "learning_rate": 4.1376666666666666e-05,
      "loss": 0.0022,
      "step": 25870
    },
    {
      "epoch": 1.3802666666666665,
      "grad_norm": 0.6363639235496521,
      "learning_rate": 4.137333333333334e-05,
      "loss": 0.0037,
      "step": 25880
    },
    {
      "epoch": 1.3808,
      "grad_norm": 0.30133989453315735,
      "learning_rate": 4.1370000000000005e-05,
      "loss": 0.0022,
      "step": 25890
    },
    {
      "epoch": 1.3813333333333333,
      "grad_norm": 0.2731271982192993,
      "learning_rate": 4.136666666666667e-05,
      "loss": 0.0022,
      "step": 25900
    },
    {
      "epoch": 1.3818666666666668,
      "grad_norm": 0.04293614998459816,
      "learning_rate": 4.136333333333334e-05,
      "loss": 0.0029,
      "step": 25910
    },
    {
      "epoch": 1.3824,
      "grad_norm": 0.5946245789527893,
      "learning_rate": 4.1360000000000004e-05,
      "loss": 0.0028,
      "step": 25920
    },
    {
      "epoch": 1.3829333333333333,
      "grad_norm": 0.5442902445793152,
      "learning_rate": 4.135666666666667e-05,
      "loss": 0.0033,
      "step": 25930
    },
    {
      "epoch": 1.3834666666666666,
      "grad_norm": 0.11853240430355072,
      "learning_rate": 4.1353333333333336e-05,
      "loss": 0.0024,
      "step": 25940
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.22128191590309143,
      "learning_rate": 4.135e-05,
      "loss": 0.0026,
      "step": 25950
    },
    {
      "epoch": 1.3845333333333334,
      "grad_norm": 0.03657601773738861,
      "learning_rate": 4.134666666666667e-05,
      "loss": 0.002,
      "step": 25960
    },
    {
      "epoch": 1.3850666666666667,
      "grad_norm": 0.6563215851783752,
      "learning_rate": 4.1343333333333334e-05,
      "loss": 0.0026,
      "step": 25970
    },
    {
      "epoch": 1.3856,
      "grad_norm": 0.4795279800891876,
      "learning_rate": 4.134e-05,
      "loss": 0.0038,
      "step": 25980
    },
    {
      "epoch": 1.3861333333333334,
      "grad_norm": 0.6552959680557251,
      "learning_rate": 4.133666666666667e-05,
      "loss": 0.0024,
      "step": 25990
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.2857508361339569,
      "learning_rate": 4.133333333333333e-05,
      "loss": 0.0029,
      "step": 26000
    },
    {
      "epoch": 1.3872,
      "grad_norm": 0.12800182402133942,
      "learning_rate": 4.133e-05,
      "loss": 0.003,
      "step": 26010
    },
    {
      "epoch": 1.3877333333333333,
      "grad_norm": 0.2692733407020569,
      "learning_rate": 4.132666666666667e-05,
      "loss": 0.0023,
      "step": 26020
    },
    {
      "epoch": 1.3882666666666665,
      "grad_norm": 0.11945260316133499,
      "learning_rate": 4.132333333333334e-05,
      "loss": 0.003,
      "step": 26030
    },
    {
      "epoch": 1.3888,
      "grad_norm": 0.4448489546775818,
      "learning_rate": 4.1320000000000004e-05,
      "loss": 0.004,
      "step": 26040
    },
    {
      "epoch": 1.3893333333333333,
      "grad_norm": 0.3271220624446869,
      "learning_rate": 4.131666666666667e-05,
      "loss": 0.0034,
      "step": 26050
    },
    {
      "epoch": 1.3898666666666666,
      "grad_norm": 0.14782825112342834,
      "learning_rate": 4.1313333333333336e-05,
      "loss": 0.0019,
      "step": 26060
    },
    {
      "epoch": 1.3904,
      "grad_norm": 0.387580007314682,
      "learning_rate": 4.131e-05,
      "loss": 0.0029,
      "step": 26070
    },
    {
      "epoch": 1.3909333333333334,
      "grad_norm": 0.32561245560646057,
      "learning_rate": 4.130666666666667e-05,
      "loss": 0.0025,
      "step": 26080
    },
    {
      "epoch": 1.3914666666666666,
      "grad_norm": 0.20834845304489136,
      "learning_rate": 4.1303333333333335e-05,
      "loss": 0.0043,
      "step": 26090
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.29958415031433105,
      "learning_rate": 4.13e-05,
      "loss": 0.0035,
      "step": 26100
    },
    {
      "epoch": 1.3925333333333334,
      "grad_norm": 0.1880306601524353,
      "learning_rate": 4.129666666666667e-05,
      "loss": 0.0034,
      "step": 26110
    },
    {
      "epoch": 1.3930666666666667,
      "grad_norm": 0.35861945152282715,
      "learning_rate": 4.129333333333333e-05,
      "loss": 0.0032,
      "step": 26120
    },
    {
      "epoch": 1.3936,
      "grad_norm": 0.4720352590084076,
      "learning_rate": 4.129e-05,
      "loss": 0.0031,
      "step": 26130
    },
    {
      "epoch": 1.3941333333333334,
      "grad_norm": 0.09119619429111481,
      "learning_rate": 4.1286666666666666e-05,
      "loss": 0.0029,
      "step": 26140
    },
    {
      "epoch": 1.3946666666666667,
      "grad_norm": 0.3490086793899536,
      "learning_rate": 4.128333333333333e-05,
      "loss": 0.0043,
      "step": 26150
    },
    {
      "epoch": 1.3952,
      "grad_norm": 0.02442706562578678,
      "learning_rate": 4.1280000000000005e-05,
      "loss": 0.0029,
      "step": 26160
    },
    {
      "epoch": 1.3957333333333333,
      "grad_norm": 0.08990827947854996,
      "learning_rate": 4.127666666666667e-05,
      "loss": 0.0039,
      "step": 26170
    },
    {
      "epoch": 1.3962666666666665,
      "grad_norm": 0.8588124513626099,
      "learning_rate": 4.127333333333334e-05,
      "loss": 0.0023,
      "step": 26180
    },
    {
      "epoch": 1.3968,
      "grad_norm": 0.2408168464899063,
      "learning_rate": 4.127e-05,
      "loss": 0.0029,
      "step": 26190
    },
    {
      "epoch": 1.3973333333333333,
      "grad_norm": 0.181350439786911,
      "learning_rate": 4.126666666666667e-05,
      "loss": 0.004,
      "step": 26200
    },
    {
      "epoch": 1.3978666666666666,
      "grad_norm": 0.33456671237945557,
      "learning_rate": 4.1263333333333335e-05,
      "loss": 0.0034,
      "step": 26210
    },
    {
      "epoch": 1.3984,
      "grad_norm": 0.3695516884326935,
      "learning_rate": 4.126e-05,
      "loss": 0.003,
      "step": 26220
    },
    {
      "epoch": 1.3989333333333334,
      "grad_norm": 0.5449786186218262,
      "learning_rate": 4.1256666666666674e-05,
      "loss": 0.0037,
      "step": 26230
    },
    {
      "epoch": 1.3994666666666666,
      "grad_norm": 0.33592185378074646,
      "learning_rate": 4.1253333333333334e-05,
      "loss": 0.0033,
      "step": 26240
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.26999011635780334,
      "learning_rate": 4.125e-05,
      "loss": 0.0029,
      "step": 26250
    },
    {
      "epoch": 1.4005333333333334,
      "grad_norm": 0.8757401704788208,
      "learning_rate": 4.1246666666666666e-05,
      "loss": 0.0034,
      "step": 26260
    },
    {
      "epoch": 1.4010666666666667,
      "grad_norm": 0.22102782130241394,
      "learning_rate": 4.124333333333333e-05,
      "loss": 0.0023,
      "step": 26270
    },
    {
      "epoch": 1.4016,
      "grad_norm": 0.26720863580703735,
      "learning_rate": 4.124e-05,
      "loss": 0.0048,
      "step": 26280
    },
    {
      "epoch": 1.4021333333333335,
      "grad_norm": 0.1626342386007309,
      "learning_rate": 4.123666666666667e-05,
      "loss": 0.0029,
      "step": 26290
    },
    {
      "epoch": 1.4026666666666667,
      "grad_norm": 0.06736985594034195,
      "learning_rate": 4.123333333333334e-05,
      "loss": 0.004,
      "step": 26300
    },
    {
      "epoch": 1.4032,
      "grad_norm": 0.5709312558174133,
      "learning_rate": 4.123e-05,
      "loss": 0.003,
      "step": 26310
    },
    {
      "epoch": 1.4037333333333333,
      "grad_norm": 0.06488882750272751,
      "learning_rate": 4.122666666666667e-05,
      "loss": 0.0026,
      "step": 26320
    },
    {
      "epoch": 1.4042666666666666,
      "grad_norm": 0.24792565405368805,
      "learning_rate": 4.1223333333333336e-05,
      "loss": 0.0024,
      "step": 26330
    },
    {
      "epoch": 1.4048,
      "grad_norm": 0.26662522554397583,
      "learning_rate": 4.122e-05,
      "loss": 0.0027,
      "step": 26340
    },
    {
      "epoch": 1.4053333333333333,
      "grad_norm": 0.3055436909198761,
      "learning_rate": 4.121666666666667e-05,
      "loss": 0.0031,
      "step": 26350
    },
    {
      "epoch": 1.4058666666666666,
      "grad_norm": 0.26009640097618103,
      "learning_rate": 4.1213333333333334e-05,
      "loss": 0.0041,
      "step": 26360
    },
    {
      "epoch": 1.4064,
      "grad_norm": 0.7642415761947632,
      "learning_rate": 4.121000000000001e-05,
      "loss": 0.003,
      "step": 26370
    },
    {
      "epoch": 1.4069333333333334,
      "grad_norm": 0.5667176842689514,
      "learning_rate": 4.120666666666667e-05,
      "loss": 0.0034,
      "step": 26380
    },
    {
      "epoch": 1.4074666666666666,
      "grad_norm": 0.6905563473701477,
      "learning_rate": 4.120333333333333e-05,
      "loss": 0.0039,
      "step": 26390
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.4302689731121063,
      "learning_rate": 4.12e-05,
      "loss": 0.0032,
      "step": 26400
    },
    {
      "epoch": 1.4085333333333334,
      "grad_norm": 0.27333271503448486,
      "learning_rate": 4.1196666666666665e-05,
      "loss": 0.0035,
      "step": 26410
    },
    {
      "epoch": 1.4090666666666667,
      "grad_norm": 0.18622741103172302,
      "learning_rate": 4.119333333333333e-05,
      "loss": 0.0031,
      "step": 26420
    },
    {
      "epoch": 1.4096,
      "grad_norm": 0.4584521949291229,
      "learning_rate": 4.1190000000000004e-05,
      "loss": 0.0048,
      "step": 26430
    },
    {
      "epoch": 1.4101333333333335,
      "grad_norm": 0.06967031210660934,
      "learning_rate": 4.118666666666667e-05,
      "loss": 0.0038,
      "step": 26440
    },
    {
      "epoch": 1.4106666666666667,
      "grad_norm": 0.25589612126350403,
      "learning_rate": 4.1183333333333336e-05,
      "loss": 0.0036,
      "step": 26450
    },
    {
      "epoch": 1.4112,
      "grad_norm": 0.534721314907074,
      "learning_rate": 4.118e-05,
      "loss": 0.0023,
      "step": 26460
    },
    {
      "epoch": 1.4117333333333333,
      "grad_norm": 0.12966004014015198,
      "learning_rate": 4.117666666666667e-05,
      "loss": 0.0027,
      "step": 26470
    },
    {
      "epoch": 1.4122666666666666,
      "grad_norm": 0.09217024594545364,
      "learning_rate": 4.1173333333333334e-05,
      "loss": 0.0027,
      "step": 26480
    },
    {
      "epoch": 1.4128,
      "grad_norm": 0.15442296862602234,
      "learning_rate": 4.117e-05,
      "loss": 0.0033,
      "step": 26490
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 0.061233993619680405,
      "learning_rate": 4.116666666666667e-05,
      "loss": 0.0037,
      "step": 26500
    },
    {
      "epoch": 1.4138666666666666,
      "grad_norm": 0.2127116620540619,
      "learning_rate": 4.116333333333334e-05,
      "loss": 0.0029,
      "step": 26510
    },
    {
      "epoch": 1.4144,
      "grad_norm": 0.3573724627494812,
      "learning_rate": 4.1160000000000006e-05,
      "loss": 0.0033,
      "step": 26520
    },
    {
      "epoch": 1.4149333333333334,
      "grad_norm": 0.9189548492431641,
      "learning_rate": 4.115666666666667e-05,
      "loss": 0.0029,
      "step": 26530
    },
    {
      "epoch": 1.4154666666666667,
      "grad_norm": 0.7767900824546814,
      "learning_rate": 4.115333333333333e-05,
      "loss": 0.0021,
      "step": 26540
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.22735397517681122,
      "learning_rate": 4.115e-05,
      "loss": 0.0035,
      "step": 26550
    },
    {
      "epoch": 1.4165333333333332,
      "grad_norm": 0.6197498440742493,
      "learning_rate": 4.1146666666666663e-05,
      "loss": 0.004,
      "step": 26560
    },
    {
      "epoch": 1.4170666666666667,
      "grad_norm": 0.5385692715644836,
      "learning_rate": 4.1143333333333336e-05,
      "loss": 0.0026,
      "step": 26570
    },
    {
      "epoch": 1.4176,
      "grad_norm": 0.23969723284244537,
      "learning_rate": 4.114e-05,
      "loss": 0.002,
      "step": 26580
    },
    {
      "epoch": 1.4181333333333335,
      "grad_norm": 0.13822141289710999,
      "learning_rate": 4.113666666666667e-05,
      "loss": 0.0038,
      "step": 26590
    },
    {
      "epoch": 1.4186666666666667,
      "grad_norm": 0.3625059425830841,
      "learning_rate": 4.1133333333333335e-05,
      "loss": 0.0026,
      "step": 26600
    },
    {
      "epoch": 1.4192,
      "grad_norm": 0.16022716462612152,
      "learning_rate": 4.113e-05,
      "loss": 0.0039,
      "step": 26610
    },
    {
      "epoch": 1.4197333333333333,
      "grad_norm": 0.29427847266197205,
      "learning_rate": 4.112666666666667e-05,
      "loss": 0.0033,
      "step": 26620
    },
    {
      "epoch": 1.4202666666666666,
      "grad_norm": 0.3912203311920166,
      "learning_rate": 4.112333333333333e-05,
      "loss": 0.004,
      "step": 26630
    },
    {
      "epoch": 1.4208,
      "grad_norm": 0.2085072100162506,
      "learning_rate": 4.1120000000000006e-05,
      "loss": 0.0033,
      "step": 26640
    },
    {
      "epoch": 1.4213333333333333,
      "grad_norm": 0.18309065699577332,
      "learning_rate": 4.111666666666667e-05,
      "loss": 0.0018,
      "step": 26650
    },
    {
      "epoch": 1.4218666666666666,
      "grad_norm": 0.23853783309459686,
      "learning_rate": 4.111333333333334e-05,
      "loss": 0.0028,
      "step": 26660
    },
    {
      "epoch": 1.4224,
      "grad_norm": 0.15158717334270477,
      "learning_rate": 4.1110000000000005e-05,
      "loss": 0.004,
      "step": 26670
    },
    {
      "epoch": 1.4229333333333334,
      "grad_norm": 0.6218425631523132,
      "learning_rate": 4.110666666666667e-05,
      "loss": 0.0028,
      "step": 26680
    },
    {
      "epoch": 1.4234666666666667,
      "grad_norm": 0.5604226589202881,
      "learning_rate": 4.110333333333333e-05,
      "loss": 0.0031,
      "step": 26690
    },
    {
      "epoch": 1.424,
      "grad_norm": 0.10491082817316055,
      "learning_rate": 4.11e-05,
      "loss": 0.0035,
      "step": 26700
    },
    {
      "epoch": 1.4245333333333332,
      "grad_norm": 0.027591902762651443,
      "learning_rate": 4.109666666666667e-05,
      "loss": 0.0037,
      "step": 26710
    },
    {
      "epoch": 1.4250666666666667,
      "grad_norm": 0.14964628219604492,
      "learning_rate": 4.1093333333333335e-05,
      "loss": 0.0033,
      "step": 26720
    },
    {
      "epoch": 1.4256,
      "grad_norm": 0.4836715757846832,
      "learning_rate": 4.109e-05,
      "loss": 0.0038,
      "step": 26730
    },
    {
      "epoch": 1.4261333333333333,
      "grad_norm": 0.27908986806869507,
      "learning_rate": 4.108666666666667e-05,
      "loss": 0.0022,
      "step": 26740
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.04155195876955986,
      "learning_rate": 4.1083333333333334e-05,
      "loss": 0.0028,
      "step": 26750
    },
    {
      "epoch": 1.4272,
      "grad_norm": 0.21084770560264587,
      "learning_rate": 4.108e-05,
      "loss": 0.0038,
      "step": 26760
    },
    {
      "epoch": 1.4277333333333333,
      "grad_norm": 0.32997721433639526,
      "learning_rate": 4.1076666666666666e-05,
      "loss": 0.0031,
      "step": 26770
    },
    {
      "epoch": 1.4282666666666666,
      "grad_norm": 0.4778038263320923,
      "learning_rate": 4.107333333333334e-05,
      "loss": 0.0034,
      "step": 26780
    },
    {
      "epoch": 1.4288,
      "grad_norm": 0.08906165510416031,
      "learning_rate": 4.1070000000000005e-05,
      "loss": 0.0032,
      "step": 26790
    },
    {
      "epoch": 1.4293333333333333,
      "grad_norm": 0.4999288022518158,
      "learning_rate": 4.106666666666667e-05,
      "loss": 0.0027,
      "step": 26800
    },
    {
      "epoch": 1.4298666666666666,
      "grad_norm": 0.5309988856315613,
      "learning_rate": 4.106333333333334e-05,
      "loss": 0.0044,
      "step": 26810
    },
    {
      "epoch": 1.4304000000000001,
      "grad_norm": 0.34483563899993896,
      "learning_rate": 4.106e-05,
      "loss": 0.0032,
      "step": 26820
    },
    {
      "epoch": 1.4309333333333334,
      "grad_norm": 0.06260666251182556,
      "learning_rate": 4.105666666666667e-05,
      "loss": 0.0031,
      "step": 26830
    },
    {
      "epoch": 1.4314666666666667,
      "grad_norm": 0.5353162884712219,
      "learning_rate": 4.1053333333333336e-05,
      "loss": 0.0033,
      "step": 26840
    },
    {
      "epoch": 1.432,
      "grad_norm": 0.35284319519996643,
      "learning_rate": 4.105e-05,
      "loss": 0.0018,
      "step": 26850
    },
    {
      "epoch": 1.4325333333333332,
      "grad_norm": 0.18117858469486237,
      "learning_rate": 4.104666666666667e-05,
      "loss": 0.0022,
      "step": 26860
    },
    {
      "epoch": 1.4330666666666667,
      "grad_norm": 0.2649218738079071,
      "learning_rate": 4.1043333333333334e-05,
      "loss": 0.0021,
      "step": 26870
    },
    {
      "epoch": 1.4336,
      "grad_norm": 0.26718971133232117,
      "learning_rate": 4.104e-05,
      "loss": 0.0038,
      "step": 26880
    },
    {
      "epoch": 1.4341333333333333,
      "grad_norm": 0.3475293815135956,
      "learning_rate": 4.1036666666666666e-05,
      "loss": 0.0032,
      "step": 26890
    },
    {
      "epoch": 1.4346666666666668,
      "grad_norm": 0.21263502538204193,
      "learning_rate": 4.103333333333333e-05,
      "loss": 0.0027,
      "step": 26900
    },
    {
      "epoch": 1.4352,
      "grad_norm": 0.17914769053459167,
      "learning_rate": 4.103e-05,
      "loss": 0.0038,
      "step": 26910
    },
    {
      "epoch": 1.4357333333333333,
      "grad_norm": 0.4162382185459137,
      "learning_rate": 4.102666666666667e-05,
      "loss": 0.0025,
      "step": 26920
    },
    {
      "epoch": 1.4362666666666666,
      "grad_norm": 0.1943325698375702,
      "learning_rate": 4.102333333333334e-05,
      "loss": 0.0029,
      "step": 26930
    },
    {
      "epoch": 1.4368,
      "grad_norm": 0.10191332548856735,
      "learning_rate": 4.1020000000000004e-05,
      "loss": 0.002,
      "step": 26940
    },
    {
      "epoch": 1.4373333333333334,
      "grad_norm": 0.06475858390331268,
      "learning_rate": 4.101666666666667e-05,
      "loss": 0.0029,
      "step": 26950
    },
    {
      "epoch": 1.4378666666666666,
      "grad_norm": 0.2833916246891022,
      "learning_rate": 4.1013333333333336e-05,
      "loss": 0.0025,
      "step": 26960
    },
    {
      "epoch": 1.4384000000000001,
      "grad_norm": 0.07225839048624039,
      "learning_rate": 4.101e-05,
      "loss": 0.0024,
      "step": 26970
    },
    {
      "epoch": 1.4389333333333334,
      "grad_norm": 0.41309139132499695,
      "learning_rate": 4.100666666666667e-05,
      "loss": 0.0032,
      "step": 26980
    },
    {
      "epoch": 1.4394666666666667,
      "grad_norm": 0.7770481705665588,
      "learning_rate": 4.100333333333334e-05,
      "loss": 0.0024,
      "step": 26990
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.14841528236865997,
      "learning_rate": 4.1e-05,
      "loss": 0.0015,
      "step": 27000
    },
    {
      "epoch": 1.4405333333333332,
      "grad_norm": 0.4751741886138916,
      "learning_rate": 4.0996666666666667e-05,
      "loss": 0.0023,
      "step": 27010
    },
    {
      "epoch": 1.4410666666666667,
      "grad_norm": 0.18014337122440338,
      "learning_rate": 4.099333333333333e-05,
      "loss": 0.0038,
      "step": 27020
    },
    {
      "epoch": 1.4416,
      "grad_norm": 0.5340065956115723,
      "learning_rate": 4.099e-05,
      "loss": 0.0026,
      "step": 27030
    },
    {
      "epoch": 1.4421333333333333,
      "grad_norm": 0.16713730990886688,
      "learning_rate": 4.0986666666666665e-05,
      "loss": 0.0033,
      "step": 27040
    },
    {
      "epoch": 1.4426666666666668,
      "grad_norm": 0.5077298283576965,
      "learning_rate": 4.098333333333334e-05,
      "loss": 0.0029,
      "step": 27050
    },
    {
      "epoch": 1.4432,
      "grad_norm": 0.29919278621673584,
      "learning_rate": 4.0980000000000004e-05,
      "loss": 0.0032,
      "step": 27060
    },
    {
      "epoch": 1.4437333333333333,
      "grad_norm": 0.391655296087265,
      "learning_rate": 4.097666666666667e-05,
      "loss": 0.0029,
      "step": 27070
    },
    {
      "epoch": 1.4442666666666666,
      "grad_norm": 0.32594484090805054,
      "learning_rate": 4.0973333333333336e-05,
      "loss": 0.0027,
      "step": 27080
    },
    {
      "epoch": 1.4447999999999999,
      "grad_norm": 0.15060044825077057,
      "learning_rate": 4.097e-05,
      "loss": 0.0037,
      "step": 27090
    },
    {
      "epoch": 1.4453333333333334,
      "grad_norm": 0.4172748327255249,
      "learning_rate": 4.096666666666667e-05,
      "loss": 0.0027,
      "step": 27100
    },
    {
      "epoch": 1.4458666666666666,
      "grad_norm": 0.12190520018339157,
      "learning_rate": 4.0963333333333335e-05,
      "loss": 0.003,
      "step": 27110
    },
    {
      "epoch": 1.4464000000000001,
      "grad_norm": 0.3563386797904968,
      "learning_rate": 4.096e-05,
      "loss": 0.0031,
      "step": 27120
    },
    {
      "epoch": 1.4469333333333334,
      "grad_norm": 0.4242103397846222,
      "learning_rate": 4.0956666666666674e-05,
      "loss": 0.0029,
      "step": 27130
    },
    {
      "epoch": 1.4474666666666667,
      "grad_norm": 0.5365442633628845,
      "learning_rate": 4.095333333333334e-05,
      "loss": 0.0025,
      "step": 27140
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.5202220678329468,
      "learning_rate": 4.095e-05,
      "loss": 0.0023,
      "step": 27150
    },
    {
      "epoch": 1.4485333333333332,
      "grad_norm": 0.18477122485637665,
      "learning_rate": 4.0946666666666665e-05,
      "loss": 0.0041,
      "step": 27160
    },
    {
      "epoch": 1.4490666666666667,
      "grad_norm": 0.3614084720611572,
      "learning_rate": 4.094333333333333e-05,
      "loss": 0.0037,
      "step": 27170
    },
    {
      "epoch": 1.4496,
      "grad_norm": 0.1452142596244812,
      "learning_rate": 4.094e-05,
      "loss": 0.003,
      "step": 27180
    },
    {
      "epoch": 1.4501333333333333,
      "grad_norm": 0.59726881980896,
      "learning_rate": 4.093666666666667e-05,
      "loss": 0.0028,
      "step": 27190
    },
    {
      "epoch": 1.4506666666666668,
      "grad_norm": 0.5972196459770203,
      "learning_rate": 4.093333333333334e-05,
      "loss": 0.0047,
      "step": 27200
    },
    {
      "epoch": 1.4512,
      "grad_norm": 0.25781258940696716,
      "learning_rate": 4.093e-05,
      "loss": 0.0031,
      "step": 27210
    },
    {
      "epoch": 1.4517333333333333,
      "grad_norm": 0.1541811227798462,
      "learning_rate": 4.092666666666667e-05,
      "loss": 0.0027,
      "step": 27220
    },
    {
      "epoch": 1.4522666666666666,
      "grad_norm": 0.18064577877521515,
      "learning_rate": 4.0923333333333335e-05,
      "loss": 0.0029,
      "step": 27230
    },
    {
      "epoch": 1.4527999999999999,
      "grad_norm": 0.4817225933074951,
      "learning_rate": 4.092e-05,
      "loss": 0.0024,
      "step": 27240
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 0.35824474692344666,
      "learning_rate": 4.091666666666667e-05,
      "loss": 0.0035,
      "step": 27250
    },
    {
      "epoch": 1.4538666666666666,
      "grad_norm": 0.1761162281036377,
      "learning_rate": 4.0913333333333334e-05,
      "loss": 0.0025,
      "step": 27260
    },
    {
      "epoch": 1.4544000000000001,
      "grad_norm": 0.3900696337223053,
      "learning_rate": 4.0910000000000006e-05,
      "loss": 0.0026,
      "step": 27270
    },
    {
      "epoch": 1.4549333333333334,
      "grad_norm": 0.5697950720787048,
      "learning_rate": 4.090666666666667e-05,
      "loss": 0.0043,
      "step": 27280
    },
    {
      "epoch": 1.4554666666666667,
      "grad_norm": 0.23863384127616882,
      "learning_rate": 4.090333333333334e-05,
      "loss": 0.0026,
      "step": 27290
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.09763892740011215,
      "learning_rate": 4.09e-05,
      "loss": 0.0036,
      "step": 27300
    },
    {
      "epoch": 1.4565333333333332,
      "grad_norm": 0.3643035590648651,
      "learning_rate": 4.0896666666666664e-05,
      "loss": 0.0019,
      "step": 27310
    },
    {
      "epoch": 1.4570666666666667,
      "grad_norm": 0.2981084883213043,
      "learning_rate": 4.089333333333333e-05,
      "loss": 0.0021,
      "step": 27320
    },
    {
      "epoch": 1.4576,
      "grad_norm": 0.3806570768356323,
      "learning_rate": 4.089e-05,
      "loss": 0.0028,
      "step": 27330
    },
    {
      "epoch": 1.4581333333333333,
      "grad_norm": 0.5310488939285278,
      "learning_rate": 4.088666666666667e-05,
      "loss": 0.0027,
      "step": 27340
    },
    {
      "epoch": 1.4586666666666668,
      "grad_norm": 0.041558846831321716,
      "learning_rate": 4.0883333333333335e-05,
      "loss": 0.0028,
      "step": 27350
    },
    {
      "epoch": 1.4592,
      "grad_norm": 0.09515827894210815,
      "learning_rate": 4.088e-05,
      "loss": 0.003,
      "step": 27360
    },
    {
      "epoch": 1.4597333333333333,
      "grad_norm": 0.018719427287578583,
      "learning_rate": 4.087666666666667e-05,
      "loss": 0.0026,
      "step": 27370
    },
    {
      "epoch": 1.4602666666666666,
      "grad_norm": 0.16450433433055878,
      "learning_rate": 4.0873333333333334e-05,
      "loss": 0.0037,
      "step": 27380
    },
    {
      "epoch": 1.4607999999999999,
      "grad_norm": 0.17851963639259338,
      "learning_rate": 4.087e-05,
      "loss": 0.002,
      "step": 27390
    },
    {
      "epoch": 1.4613333333333334,
      "grad_norm": 0.44949227571487427,
      "learning_rate": 4.086666666666667e-05,
      "loss": 0.0033,
      "step": 27400
    },
    {
      "epoch": 1.4618666666666666,
      "grad_norm": 0.6577749252319336,
      "learning_rate": 4.086333333333334e-05,
      "loss": 0.0031,
      "step": 27410
    },
    {
      "epoch": 1.4624,
      "grad_norm": 0.0734187513589859,
      "learning_rate": 4.0860000000000005e-05,
      "loss": 0.0017,
      "step": 27420
    },
    {
      "epoch": 1.4629333333333334,
      "grad_norm": 0.35487592220306396,
      "learning_rate": 4.085666666666667e-05,
      "loss": 0.0035,
      "step": 27430
    },
    {
      "epoch": 1.4634666666666667,
      "grad_norm": 0.6304163336753845,
      "learning_rate": 4.085333333333334e-05,
      "loss": 0.0025,
      "step": 27440
    },
    {
      "epoch": 1.464,
      "grad_norm": 0.04741348326206207,
      "learning_rate": 4.085e-05,
      "loss": 0.0027,
      "step": 27450
    },
    {
      "epoch": 1.4645333333333332,
      "grad_norm": 0.21209685504436493,
      "learning_rate": 4.084666666666667e-05,
      "loss": 0.0026,
      "step": 27460
    },
    {
      "epoch": 1.4650666666666667,
      "grad_norm": 0.12568111717700958,
      "learning_rate": 4.0843333333333336e-05,
      "loss": 0.0032,
      "step": 27470
    },
    {
      "epoch": 1.4656,
      "grad_norm": 0.07206587493419647,
      "learning_rate": 4.084e-05,
      "loss": 0.0032,
      "step": 27480
    },
    {
      "epoch": 1.4661333333333333,
      "grad_norm": 0.48397788405418396,
      "learning_rate": 4.083666666666667e-05,
      "loss": 0.0027,
      "step": 27490
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.1167827844619751,
      "learning_rate": 4.0833333333333334e-05,
      "loss": 0.0035,
      "step": 27500
    },
    {
      "epoch": 1.4672,
      "grad_norm": 0.3275153338909149,
      "learning_rate": 4.083e-05,
      "loss": 0.0021,
      "step": 27510
    },
    {
      "epoch": 1.4677333333333333,
      "grad_norm": 0.08066432923078537,
      "learning_rate": 4.0826666666666667e-05,
      "loss": 0.0022,
      "step": 27520
    },
    {
      "epoch": 1.4682666666666666,
      "grad_norm": 0.21127353608608246,
      "learning_rate": 4.082333333333333e-05,
      "loss": 0.003,
      "step": 27530
    },
    {
      "epoch": 1.4687999999999999,
      "grad_norm": 0.15244749188423157,
      "learning_rate": 4.0820000000000006e-05,
      "loss": 0.0026,
      "step": 27540
    },
    {
      "epoch": 1.4693333333333334,
      "grad_norm": 0.4466332793235779,
      "learning_rate": 4.081666666666667e-05,
      "loss": 0.0029,
      "step": 27550
    },
    {
      "epoch": 1.4698666666666667,
      "grad_norm": 0.0893929973244667,
      "learning_rate": 4.081333333333334e-05,
      "loss": 0.0028,
      "step": 27560
    },
    {
      "epoch": 1.4704,
      "grad_norm": 0.09419438987970352,
      "learning_rate": 4.0810000000000004e-05,
      "loss": 0.0025,
      "step": 27570
    },
    {
      "epoch": 1.4709333333333334,
      "grad_norm": 0.18197759985923767,
      "learning_rate": 4.080666666666667e-05,
      "loss": 0.0035,
      "step": 27580
    },
    {
      "epoch": 1.4714666666666667,
      "grad_norm": 0.12181064486503601,
      "learning_rate": 4.0803333333333336e-05,
      "loss": 0.0031,
      "step": 27590
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.22756211459636688,
      "learning_rate": 4.08e-05,
      "loss": 0.0036,
      "step": 27600
    },
    {
      "epoch": 1.4725333333333332,
      "grad_norm": 0.4072641432285309,
      "learning_rate": 4.079666666666667e-05,
      "loss": 0.0026,
      "step": 27610
    },
    {
      "epoch": 1.4730666666666667,
      "grad_norm": 0.3017318546772003,
      "learning_rate": 4.0793333333333335e-05,
      "loss": 0.0021,
      "step": 27620
    },
    {
      "epoch": 1.4736,
      "grad_norm": 0.253420352935791,
      "learning_rate": 4.079e-05,
      "loss": 0.0035,
      "step": 27630
    },
    {
      "epoch": 1.4741333333333333,
      "grad_norm": 0.35868802666664124,
      "learning_rate": 4.078666666666667e-05,
      "loss": 0.0026,
      "step": 27640
    },
    {
      "epoch": 1.4746666666666668,
      "grad_norm": 0.1254073679447174,
      "learning_rate": 4.078333333333333e-05,
      "loss": 0.0022,
      "step": 27650
    },
    {
      "epoch": 1.4752,
      "grad_norm": 0.3940827548503876,
      "learning_rate": 4.078e-05,
      "loss": 0.0037,
      "step": 27660
    },
    {
      "epoch": 1.4757333333333333,
      "grad_norm": 0.4283972382545471,
      "learning_rate": 4.0776666666666665e-05,
      "loss": 0.0031,
      "step": 27670
    },
    {
      "epoch": 1.4762666666666666,
      "grad_norm": 0.3289376497268677,
      "learning_rate": 4.077333333333334e-05,
      "loss": 0.0034,
      "step": 27680
    },
    {
      "epoch": 1.4768,
      "grad_norm": 0.3379301428794861,
      "learning_rate": 4.0770000000000004e-05,
      "loss": 0.0023,
      "step": 27690
    },
    {
      "epoch": 1.4773333333333334,
      "grad_norm": 0.5059757232666016,
      "learning_rate": 4.076666666666667e-05,
      "loss": 0.002,
      "step": 27700
    },
    {
      "epoch": 1.4778666666666667,
      "grad_norm": 0.2700638771057129,
      "learning_rate": 4.076333333333334e-05,
      "loss": 0.0029,
      "step": 27710
    },
    {
      "epoch": 1.4784,
      "grad_norm": 0.5000584721565247,
      "learning_rate": 4.076e-05,
      "loss": 0.0041,
      "step": 27720
    },
    {
      "epoch": 1.4789333333333334,
      "grad_norm": 0.3162407875061035,
      "learning_rate": 4.075666666666667e-05,
      "loss": 0.0038,
      "step": 27730
    },
    {
      "epoch": 1.4794666666666667,
      "grad_norm": 0.18102864921092987,
      "learning_rate": 4.0753333333333335e-05,
      "loss": 0.0025,
      "step": 27740
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.20673097670078278,
      "learning_rate": 4.075e-05,
      "loss": 0.0028,
      "step": 27750
    },
    {
      "epoch": 1.4805333333333333,
      "grad_norm": 0.6594800353050232,
      "learning_rate": 4.074666666666667e-05,
      "loss": 0.0025,
      "step": 27760
    },
    {
      "epoch": 1.4810666666666665,
      "grad_norm": 0.020360058173537254,
      "learning_rate": 4.0743333333333333e-05,
      "loss": 0.0037,
      "step": 27770
    },
    {
      "epoch": 1.4816,
      "grad_norm": 0.049950554966926575,
      "learning_rate": 4.074e-05,
      "loss": 0.0033,
      "step": 27780
    },
    {
      "epoch": 1.4821333333333333,
      "grad_norm": 0.21326595544815063,
      "learning_rate": 4.0736666666666666e-05,
      "loss": 0.0029,
      "step": 27790
    },
    {
      "epoch": 1.4826666666666668,
      "grad_norm": 0.07093986123800278,
      "learning_rate": 4.073333333333333e-05,
      "loss": 0.0038,
      "step": 27800
    },
    {
      "epoch": 1.4832,
      "grad_norm": 0.26699575781822205,
      "learning_rate": 4.0730000000000005e-05,
      "loss": 0.003,
      "step": 27810
    },
    {
      "epoch": 1.4837333333333333,
      "grad_norm": 0.060410283505916595,
      "learning_rate": 4.072666666666667e-05,
      "loss": 0.003,
      "step": 27820
    },
    {
      "epoch": 1.4842666666666666,
      "grad_norm": 0.07382845133543015,
      "learning_rate": 4.072333333333334e-05,
      "loss": 0.0039,
      "step": 27830
    },
    {
      "epoch": 1.4848,
      "grad_norm": 0.3030562996864319,
      "learning_rate": 4.072e-05,
      "loss": 0.0036,
      "step": 27840
    },
    {
      "epoch": 1.4853333333333334,
      "grad_norm": 0.479644775390625,
      "learning_rate": 4.071666666666667e-05,
      "loss": 0.0029,
      "step": 27850
    },
    {
      "epoch": 1.4858666666666667,
      "grad_norm": 0.44599366188049316,
      "learning_rate": 4.0713333333333335e-05,
      "loss": 0.0021,
      "step": 27860
    },
    {
      "epoch": 1.4864,
      "grad_norm": 0.10259570181369781,
      "learning_rate": 4.071e-05,
      "loss": 0.0037,
      "step": 27870
    },
    {
      "epoch": 1.4869333333333334,
      "grad_norm": 0.41722381114959717,
      "learning_rate": 4.070666666666667e-05,
      "loss": 0.0026,
      "step": 27880
    },
    {
      "epoch": 1.4874666666666667,
      "grad_norm": 0.2952679991722107,
      "learning_rate": 4.070333333333334e-05,
      "loss": 0.0022,
      "step": 27890
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.12189210206270218,
      "learning_rate": 4.07e-05,
      "loss": 0.0022,
      "step": 27900
    },
    {
      "epoch": 1.4885333333333333,
      "grad_norm": 0.12747474014759064,
      "learning_rate": 4.0696666666666666e-05,
      "loss": 0.0031,
      "step": 27910
    },
    {
      "epoch": 1.4890666666666665,
      "grad_norm": 0.4790625274181366,
      "learning_rate": 4.069333333333333e-05,
      "loss": 0.0022,
      "step": 27920
    },
    {
      "epoch": 1.4896,
      "grad_norm": 0.17923180758953094,
      "learning_rate": 4.069e-05,
      "loss": 0.0033,
      "step": 27930
    },
    {
      "epoch": 1.4901333333333333,
      "grad_norm": 0.1217806339263916,
      "learning_rate": 4.0686666666666664e-05,
      "loss": 0.0024,
      "step": 27940
    },
    {
      "epoch": 1.4906666666666666,
      "grad_norm": 0.06817113608121872,
      "learning_rate": 4.068333333333334e-05,
      "loss": 0.0037,
      "step": 27950
    },
    {
      "epoch": 1.4912,
      "grad_norm": 0.23540158569812775,
      "learning_rate": 4.0680000000000004e-05,
      "loss": 0.0042,
      "step": 27960
    },
    {
      "epoch": 1.4917333333333334,
      "grad_norm": 0.3686971962451935,
      "learning_rate": 4.067666666666667e-05,
      "loss": 0.003,
      "step": 27970
    },
    {
      "epoch": 1.4922666666666666,
      "grad_norm": 0.07918215543031693,
      "learning_rate": 4.0673333333333336e-05,
      "loss": 0.0028,
      "step": 27980
    },
    {
      "epoch": 1.4928,
      "grad_norm": 0.037923868745565414,
      "learning_rate": 4.067e-05,
      "loss": 0.0026,
      "step": 27990
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 0.1313095986843109,
      "learning_rate": 4.066666666666667e-05,
      "loss": 0.0031,
      "step": 28000
    },
    {
      "epoch": 1.4938666666666667,
      "grad_norm": 0.5887858271598816,
      "learning_rate": 4.0663333333333334e-05,
      "loss": 0.0027,
      "step": 28010
    },
    {
      "epoch": 1.4944,
      "grad_norm": 0.2936288118362427,
      "learning_rate": 4.066e-05,
      "loss": 0.0018,
      "step": 28020
    },
    {
      "epoch": 1.4949333333333334,
      "grad_norm": 0.2156522125005722,
      "learning_rate": 4.065666666666667e-05,
      "loss": 0.0039,
      "step": 28030
    },
    {
      "epoch": 1.4954666666666667,
      "grad_norm": 0.23954088985919952,
      "learning_rate": 4.065333333333334e-05,
      "loss": 0.0027,
      "step": 28040
    },
    {
      "epoch": 1.496,
      "grad_norm": 0.08809901773929596,
      "learning_rate": 4.065e-05,
      "loss": 0.0029,
      "step": 28050
    },
    {
      "epoch": 1.4965333333333333,
      "grad_norm": 0.3893102705478668,
      "learning_rate": 4.0646666666666665e-05,
      "loss": 0.0033,
      "step": 28060
    },
    {
      "epoch": 1.4970666666666665,
      "grad_norm": 0.4154093861579895,
      "learning_rate": 4.064333333333333e-05,
      "loss": 0.0029,
      "step": 28070
    },
    {
      "epoch": 1.4976,
      "grad_norm": 0.7219887375831604,
      "learning_rate": 4.064e-05,
      "loss": 0.0035,
      "step": 28080
    },
    {
      "epoch": 1.4981333333333333,
      "grad_norm": 1.0545161962509155,
      "learning_rate": 4.063666666666667e-05,
      "loss": 0.0031,
      "step": 28090
    },
    {
      "epoch": 1.4986666666666666,
      "grad_norm": 0.21160626411437988,
      "learning_rate": 4.0633333333333336e-05,
      "loss": 0.0032,
      "step": 28100
    },
    {
      "epoch": 1.4992,
      "grad_norm": 0.32719531655311584,
      "learning_rate": 4.063e-05,
      "loss": 0.002,
      "step": 28110
    },
    {
      "epoch": 1.4997333333333334,
      "grad_norm": 0.15379968285560608,
      "learning_rate": 4.062666666666667e-05,
      "loss": 0.0026,
      "step": 28120
    },
    {
      "epoch": 1.5002666666666666,
      "grad_norm": 0.4157744348049164,
      "learning_rate": 4.0623333333333335e-05,
      "loss": 0.0033,
      "step": 28130
    },
    {
      "epoch": 1.5008,
      "grad_norm": 0.06149309501051903,
      "learning_rate": 4.062e-05,
      "loss": 0.0026,
      "step": 28140
    },
    {
      "epoch": 1.5013333333333332,
      "grad_norm": 0.0327029824256897,
      "learning_rate": 4.061666666666667e-05,
      "loss": 0.0033,
      "step": 28150
    },
    {
      "epoch": 1.5018666666666667,
      "grad_norm": 0.12152600288391113,
      "learning_rate": 4.061333333333334e-05,
      "loss": 0.0032,
      "step": 28160
    },
    {
      "epoch": 1.5024,
      "grad_norm": 0.4413701593875885,
      "learning_rate": 4.0610000000000006e-05,
      "loss": 0.004,
      "step": 28170
    },
    {
      "epoch": 1.5029333333333335,
      "grad_norm": 0.541563093662262,
      "learning_rate": 4.060666666666667e-05,
      "loss": 0.0031,
      "step": 28180
    },
    {
      "epoch": 1.5034666666666667,
      "grad_norm": 0.04308198019862175,
      "learning_rate": 4.060333333333334e-05,
      "loss": 0.0038,
      "step": 28190
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.03424125909805298,
      "learning_rate": 4.0600000000000004e-05,
      "loss": 0.0021,
      "step": 28200
    },
    {
      "epoch": 1.5045333333333333,
      "grad_norm": 0.18079647421836853,
      "learning_rate": 4.0596666666666664e-05,
      "loss": 0.0028,
      "step": 28210
    },
    {
      "epoch": 1.5050666666666666,
      "grad_norm": 0.39315977692604065,
      "learning_rate": 4.0593333333333337e-05,
      "loss": 0.0035,
      "step": 28220
    },
    {
      "epoch": 1.5056,
      "grad_norm": 0.12717467546463013,
      "learning_rate": 4.059e-05,
      "loss": 0.0031,
      "step": 28230
    },
    {
      "epoch": 1.5061333333333333,
      "grad_norm": 0.05553260073065758,
      "learning_rate": 4.058666666666667e-05,
      "loss": 0.0034,
      "step": 28240
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 0.18137341737747192,
      "learning_rate": 4.0583333333333335e-05,
      "loss": 0.0024,
      "step": 28250
    },
    {
      "epoch": 1.5072,
      "grad_norm": 0.06513094156980515,
      "learning_rate": 4.058e-05,
      "loss": 0.0016,
      "step": 28260
    },
    {
      "epoch": 1.5077333333333334,
      "grad_norm": 0.949973464012146,
      "learning_rate": 4.057666666666667e-05,
      "loss": 0.0034,
      "step": 28270
    },
    {
      "epoch": 1.5082666666666666,
      "grad_norm": 0.6798108220100403,
      "learning_rate": 4.057333333333333e-05,
      "loss": 0.0025,
      "step": 28280
    },
    {
      "epoch": 1.5088,
      "grad_norm": 0.09743096679449081,
      "learning_rate": 4.057e-05,
      "loss": 0.0038,
      "step": 28290
    },
    {
      "epoch": 1.5093333333333332,
      "grad_norm": 0.21167097985744476,
      "learning_rate": 4.056666666666667e-05,
      "loss": 0.0027,
      "step": 28300
    },
    {
      "epoch": 1.5098666666666667,
      "grad_norm": 0.012144104577600956,
      "learning_rate": 4.056333333333334e-05,
      "loss": 0.0032,
      "step": 28310
    },
    {
      "epoch": 1.5104,
      "grad_norm": 0.47752100229263306,
      "learning_rate": 4.0560000000000005e-05,
      "loss": 0.0036,
      "step": 28320
    },
    {
      "epoch": 1.5109333333333335,
      "grad_norm": 0.0961475521326065,
      "learning_rate": 4.055666666666667e-05,
      "loss": 0.0031,
      "step": 28330
    },
    {
      "epoch": 1.5114666666666667,
      "grad_norm": 0.3578885495662689,
      "learning_rate": 4.055333333333334e-05,
      "loss": 0.0036,
      "step": 28340
    },
    {
      "epoch": 1.512,
      "grad_norm": 0.5400587916374207,
      "learning_rate": 4.055e-05,
      "loss": 0.0035,
      "step": 28350
    },
    {
      "epoch": 1.5125333333333333,
      "grad_norm": 0.35674864053726196,
      "learning_rate": 4.054666666666667e-05,
      "loss": 0.003,
      "step": 28360
    },
    {
      "epoch": 1.5130666666666666,
      "grad_norm": 0.18249180912971497,
      "learning_rate": 4.0543333333333335e-05,
      "loss": 0.0033,
      "step": 28370
    },
    {
      "epoch": 1.5135999999999998,
      "grad_norm": 0.5354384183883667,
      "learning_rate": 4.054e-05,
      "loss": 0.0027,
      "step": 28380
    },
    {
      "epoch": 1.5141333333333333,
      "grad_norm": 0.17974084615707397,
      "learning_rate": 4.053666666666667e-05,
      "loss": 0.0037,
      "step": 28390
    },
    {
      "epoch": 1.5146666666666668,
      "grad_norm": 0.2961437404155731,
      "learning_rate": 4.0533333333333334e-05,
      "loss": 0.0049,
      "step": 28400
    },
    {
      "epoch": 1.5152,
      "grad_norm": 0.027057306841015816,
      "learning_rate": 4.053e-05,
      "loss": 0.0025,
      "step": 28410
    },
    {
      "epoch": 1.5157333333333334,
      "grad_norm": 0.4150908887386322,
      "learning_rate": 4.0526666666666666e-05,
      "loss": 0.0025,
      "step": 28420
    },
    {
      "epoch": 1.5162666666666667,
      "grad_norm": 0.12085743248462677,
      "learning_rate": 4.052333333333333e-05,
      "loss": 0.0032,
      "step": 28430
    },
    {
      "epoch": 1.5168,
      "grad_norm": 0.5616124868392944,
      "learning_rate": 4.0520000000000005e-05,
      "loss": 0.002,
      "step": 28440
    },
    {
      "epoch": 1.5173333333333332,
      "grad_norm": 0.18254421651363373,
      "learning_rate": 4.051666666666667e-05,
      "loss": 0.0031,
      "step": 28450
    },
    {
      "epoch": 1.5178666666666667,
      "grad_norm": 0.0914265587925911,
      "learning_rate": 4.051333333333334e-05,
      "loss": 0.0035,
      "step": 28460
    },
    {
      "epoch": 1.5184,
      "grad_norm": 0.5908617377281189,
      "learning_rate": 4.0510000000000003e-05,
      "loss": 0.0034,
      "step": 28470
    },
    {
      "epoch": 1.5189333333333335,
      "grad_norm": 0.6045750379562378,
      "learning_rate": 4.050666666666667e-05,
      "loss": 0.0034,
      "step": 28480
    },
    {
      "epoch": 1.5194666666666667,
      "grad_norm": 0.18172116577625275,
      "learning_rate": 4.0503333333333336e-05,
      "loss": 0.0028,
      "step": 28490
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.47805285453796387,
      "learning_rate": 4.05e-05,
      "loss": 0.0038,
      "step": 28500
    },
    {
      "epoch": 1.5205333333333333,
      "grad_norm": 0.7773345708847046,
      "learning_rate": 4.049666666666667e-05,
      "loss": 0.0028,
      "step": 28510
    },
    {
      "epoch": 1.5210666666666666,
      "grad_norm": 0.5683803558349609,
      "learning_rate": 4.0493333333333334e-05,
      "loss": 0.0025,
      "step": 28520
    },
    {
      "epoch": 1.5215999999999998,
      "grad_norm": 0.20919032394886017,
      "learning_rate": 4.049e-05,
      "loss": 0.0023,
      "step": 28530
    },
    {
      "epoch": 1.5221333333333333,
      "grad_norm": 0.06963007897138596,
      "learning_rate": 4.0486666666666666e-05,
      "loss": 0.0026,
      "step": 28540
    },
    {
      "epoch": 1.5226666666666666,
      "grad_norm": 0.5394226908683777,
      "learning_rate": 4.048333333333333e-05,
      "loss": 0.0029,
      "step": 28550
    },
    {
      "epoch": 1.5232,
      "grad_norm": 0.6312302350997925,
      "learning_rate": 4.048e-05,
      "loss": 0.0028,
      "step": 28560
    },
    {
      "epoch": 1.5237333333333334,
      "grad_norm": 0.19621476531028748,
      "learning_rate": 4.047666666666667e-05,
      "loss": 0.0021,
      "step": 28570
    },
    {
      "epoch": 1.5242666666666667,
      "grad_norm": 0.10409236699342728,
      "learning_rate": 4.047333333333334e-05,
      "loss": 0.0023,
      "step": 28580
    },
    {
      "epoch": 1.5248,
      "grad_norm": 0.07858693599700928,
      "learning_rate": 4.0470000000000004e-05,
      "loss": 0.0027,
      "step": 28590
    },
    {
      "epoch": 1.5253333333333332,
      "grad_norm": 0.1813843846321106,
      "learning_rate": 4.046666666666667e-05,
      "loss": 0.0026,
      "step": 28600
    },
    {
      "epoch": 1.5258666666666667,
      "grad_norm": 0.04388976842164993,
      "learning_rate": 4.0463333333333336e-05,
      "loss": 0.0028,
      "step": 28610
    },
    {
      "epoch": 1.5264,
      "grad_norm": 0.3605465292930603,
      "learning_rate": 4.046e-05,
      "loss": 0.0025,
      "step": 28620
    },
    {
      "epoch": 1.5269333333333335,
      "grad_norm": 0.30266857147216797,
      "learning_rate": 4.045666666666667e-05,
      "loss": 0.0025,
      "step": 28630
    },
    {
      "epoch": 1.5274666666666668,
      "grad_norm": 0.039004817605018616,
      "learning_rate": 4.0453333333333335e-05,
      "loss": 0.0025,
      "step": 28640
    },
    {
      "epoch": 1.528,
      "grad_norm": 0.20625221729278564,
      "learning_rate": 4.045000000000001e-05,
      "loss": 0.0026,
      "step": 28650
    },
    {
      "epoch": 1.5285333333333333,
      "grad_norm": 0.3273370563983917,
      "learning_rate": 4.044666666666667e-05,
      "loss": 0.0038,
      "step": 28660
    },
    {
      "epoch": 1.5290666666666666,
      "grad_norm": 0.2705763280391693,
      "learning_rate": 4.044333333333333e-05,
      "loss": 0.0037,
      "step": 28670
    },
    {
      "epoch": 1.5295999999999998,
      "grad_norm": 0.5715990662574768,
      "learning_rate": 4.044e-05,
      "loss": 0.0028,
      "step": 28680
    },
    {
      "epoch": 1.5301333333333333,
      "grad_norm": 0.08278504759073257,
      "learning_rate": 4.0436666666666665e-05,
      "loss": 0.0029,
      "step": 28690
    },
    {
      "epoch": 1.5306666666666666,
      "grad_norm": 0.2620190382003784,
      "learning_rate": 4.043333333333333e-05,
      "loss": 0.0025,
      "step": 28700
    },
    {
      "epoch": 1.5312000000000001,
      "grad_norm": 0.21395504474639893,
      "learning_rate": 4.0430000000000004e-05,
      "loss": 0.0024,
      "step": 28710
    },
    {
      "epoch": 1.5317333333333334,
      "grad_norm": 0.041657015681266785,
      "learning_rate": 4.042666666666667e-05,
      "loss": 0.0029,
      "step": 28720
    },
    {
      "epoch": 1.5322666666666667,
      "grad_norm": 0.09255994111299515,
      "learning_rate": 4.0423333333333337e-05,
      "loss": 0.0023,
      "step": 28730
    },
    {
      "epoch": 1.5328,
      "grad_norm": 0.36250415444374084,
      "learning_rate": 4.042e-05,
      "loss": 0.0032,
      "step": 28740
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.17624402046203613,
      "learning_rate": 4.041666666666667e-05,
      "loss": 0.0049,
      "step": 28750
    },
    {
      "epoch": 1.5338666666666667,
      "grad_norm": 0.05512692779302597,
      "learning_rate": 4.0413333333333335e-05,
      "loss": 0.0024,
      "step": 28760
    },
    {
      "epoch": 1.5344,
      "grad_norm": 0.1601833552122116,
      "learning_rate": 4.041e-05,
      "loss": 0.005,
      "step": 28770
    },
    {
      "epoch": 1.5349333333333335,
      "grad_norm": 0.47334450483322144,
      "learning_rate": 4.040666666666667e-05,
      "loss": 0.0036,
      "step": 28780
    },
    {
      "epoch": 1.5354666666666668,
      "grad_norm": 0.24863693118095398,
      "learning_rate": 4.040333333333334e-05,
      "loss": 0.002,
      "step": 28790
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.1567385494709015,
      "learning_rate": 4.0400000000000006e-05,
      "loss": 0.0026,
      "step": 28800
    },
    {
      "epoch": 1.5365333333333333,
      "grad_norm": 0.3909934461116791,
      "learning_rate": 4.0396666666666666e-05,
      "loss": 0.0023,
      "step": 28810
    },
    {
      "epoch": 1.5370666666666666,
      "grad_norm": 0.24244950711727142,
      "learning_rate": 4.039333333333333e-05,
      "loss": 0.002,
      "step": 28820
    },
    {
      "epoch": 1.5375999999999999,
      "grad_norm": 0.06616227328777313,
      "learning_rate": 4.039e-05,
      "loss": 0.0029,
      "step": 28830
    },
    {
      "epoch": 1.5381333333333334,
      "grad_norm": 0.06441424787044525,
      "learning_rate": 4.0386666666666664e-05,
      "loss": 0.0026,
      "step": 28840
    },
    {
      "epoch": 1.5386666666666666,
      "grad_norm": 0.06029121205210686,
      "learning_rate": 4.038333333333334e-05,
      "loss": 0.0043,
      "step": 28850
    },
    {
      "epoch": 1.5392000000000001,
      "grad_norm": 0.040003594011068344,
      "learning_rate": 4.038e-05,
      "loss": 0.0024,
      "step": 28860
    },
    {
      "epoch": 1.5397333333333334,
      "grad_norm": 0.3531058728694916,
      "learning_rate": 4.037666666666667e-05,
      "loss": 0.0025,
      "step": 28870
    },
    {
      "epoch": 1.5402666666666667,
      "grad_norm": 0.5104051232337952,
      "learning_rate": 4.0373333333333335e-05,
      "loss": 0.0024,
      "step": 28880
    },
    {
      "epoch": 1.5408,
      "grad_norm": 0.6909627914428711,
      "learning_rate": 4.037e-05,
      "loss": 0.0046,
      "step": 28890
    },
    {
      "epoch": 1.5413333333333332,
      "grad_norm": 0.2945325970649719,
      "learning_rate": 4.036666666666667e-05,
      "loss": 0.0028,
      "step": 28900
    },
    {
      "epoch": 1.5418666666666667,
      "grad_norm": 0.27793750166893005,
      "learning_rate": 4.0363333333333334e-05,
      "loss": 0.003,
      "step": 28910
    },
    {
      "epoch": 1.5424,
      "grad_norm": 0.32948070764541626,
      "learning_rate": 4.0360000000000007e-05,
      "loss": 0.0027,
      "step": 28920
    },
    {
      "epoch": 1.5429333333333335,
      "grad_norm": 0.04701744765043259,
      "learning_rate": 4.035666666666667e-05,
      "loss": 0.0026,
      "step": 28930
    },
    {
      "epoch": 1.5434666666666668,
      "grad_norm": 0.26419487595558167,
      "learning_rate": 4.035333333333334e-05,
      "loss": 0.0033,
      "step": 28940
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.034663643687963486,
      "learning_rate": 4.0350000000000005e-05,
      "loss": 0.0038,
      "step": 28950
    },
    {
      "epoch": 1.5445333333333333,
      "grad_norm": 0.385627418756485,
      "learning_rate": 4.0346666666666664e-05,
      "loss": 0.0031,
      "step": 28960
    },
    {
      "epoch": 1.5450666666666666,
      "grad_norm": 0.06894796341657639,
      "learning_rate": 4.034333333333333e-05,
      "loss": 0.0029,
      "step": 28970
    },
    {
      "epoch": 1.5455999999999999,
      "grad_norm": 0.10210119932889938,
      "learning_rate": 4.034e-05,
      "loss": 0.0044,
      "step": 28980
    },
    {
      "epoch": 1.5461333333333334,
      "grad_norm": 0.16142144799232483,
      "learning_rate": 4.033666666666667e-05,
      "loss": 0.0028,
      "step": 28990
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 0.04644494876265526,
      "learning_rate": 4.0333333333333336e-05,
      "loss": 0.0027,
      "step": 29000
    },
    {
      "epoch": 1.5472000000000001,
      "grad_norm": 0.8274332880973816,
      "learning_rate": 4.033e-05,
      "loss": 0.0032,
      "step": 29010
    },
    {
      "epoch": 1.5477333333333334,
      "grad_norm": 0.3849184811115265,
      "learning_rate": 4.032666666666667e-05,
      "loss": 0.0034,
      "step": 29020
    },
    {
      "epoch": 1.5482666666666667,
      "grad_norm": 0.38399839401245117,
      "learning_rate": 4.0323333333333334e-05,
      "loss": 0.0034,
      "step": 29030
    },
    {
      "epoch": 1.5488,
      "grad_norm": 0.29370176792144775,
      "learning_rate": 4.032e-05,
      "loss": 0.0027,
      "step": 29040
    },
    {
      "epoch": 1.5493333333333332,
      "grad_norm": 0.4757550358772278,
      "learning_rate": 4.0316666666666666e-05,
      "loss": 0.0028,
      "step": 29050
    },
    {
      "epoch": 1.5498666666666665,
      "grad_norm": 0.35556161403656006,
      "learning_rate": 4.031333333333334e-05,
      "loss": 0.002,
      "step": 29060
    },
    {
      "epoch": 1.5504,
      "grad_norm": 0.32942402362823486,
      "learning_rate": 4.0310000000000005e-05,
      "loss": 0.003,
      "step": 29070
    },
    {
      "epoch": 1.5509333333333335,
      "grad_norm": 0.3275034427642822,
      "learning_rate": 4.030666666666667e-05,
      "loss": 0.0038,
      "step": 29080
    },
    {
      "epoch": 1.5514666666666668,
      "grad_norm": 0.0897878035902977,
      "learning_rate": 4.030333333333334e-05,
      "loss": 0.0039,
      "step": 29090
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.26654669642448425,
      "learning_rate": 4.0300000000000004e-05,
      "loss": 0.003,
      "step": 29100
    },
    {
      "epoch": 1.5525333333333333,
      "grad_norm": 0.053139347583055496,
      "learning_rate": 4.029666666666666e-05,
      "loss": 0.0024,
      "step": 29110
    },
    {
      "epoch": 1.5530666666666666,
      "grad_norm": 0.15899911522865295,
      "learning_rate": 4.0293333333333336e-05,
      "loss": 0.0022,
      "step": 29120
    },
    {
      "epoch": 1.5535999999999999,
      "grad_norm": 0.5313884615898132,
      "learning_rate": 4.029e-05,
      "loss": 0.0026,
      "step": 29130
    },
    {
      "epoch": 1.5541333333333334,
      "grad_norm": 0.20846404135227203,
      "learning_rate": 4.028666666666667e-05,
      "loss": 0.0028,
      "step": 29140
    },
    {
      "epoch": 1.5546666666666666,
      "grad_norm": 0.29383325576782227,
      "learning_rate": 4.0283333333333334e-05,
      "loss": 0.0035,
      "step": 29150
    },
    {
      "epoch": 1.5552000000000001,
      "grad_norm": 0.47457441687583923,
      "learning_rate": 4.028e-05,
      "loss": 0.0037,
      "step": 29160
    },
    {
      "epoch": 1.5557333333333334,
      "grad_norm": 0.40420231223106384,
      "learning_rate": 4.027666666666667e-05,
      "loss": 0.0033,
      "step": 29170
    },
    {
      "epoch": 1.5562666666666667,
      "grad_norm": 0.48215559124946594,
      "learning_rate": 4.027333333333333e-05,
      "loss": 0.0038,
      "step": 29180
    },
    {
      "epoch": 1.5568,
      "grad_norm": 0.08835650235414505,
      "learning_rate": 4.027e-05,
      "loss": 0.0035,
      "step": 29190
    },
    {
      "epoch": 1.5573333333333332,
      "grad_norm": 0.06486193835735321,
      "learning_rate": 4.026666666666667e-05,
      "loss": 0.0029,
      "step": 29200
    },
    {
      "epoch": 1.5578666666666665,
      "grad_norm": 0.12555597722530365,
      "learning_rate": 4.026333333333334e-05,
      "loss": 0.0035,
      "step": 29210
    },
    {
      "epoch": 1.5584,
      "grad_norm": 0.3031443655490875,
      "learning_rate": 4.0260000000000004e-05,
      "loss": 0.0024,
      "step": 29220
    },
    {
      "epoch": 1.5589333333333333,
      "grad_norm": 0.8345364928245544,
      "learning_rate": 4.025666666666667e-05,
      "loss": 0.0039,
      "step": 29230
    },
    {
      "epoch": 1.5594666666666668,
      "grad_norm": 0.3010854721069336,
      "learning_rate": 4.0253333333333336e-05,
      "loss": 0.0033,
      "step": 29240
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.09207436442375183,
      "learning_rate": 4.025e-05,
      "loss": 0.0029,
      "step": 29250
    },
    {
      "epoch": 1.5605333333333333,
      "grad_norm": 0.1774938702583313,
      "learning_rate": 4.024666666666667e-05,
      "loss": 0.0028,
      "step": 29260
    },
    {
      "epoch": 1.5610666666666666,
      "grad_norm": 0.4314906597137451,
      "learning_rate": 4.0243333333333335e-05,
      "loss": 0.0028,
      "step": 29270
    },
    {
      "epoch": 1.5615999999999999,
      "grad_norm": 0.2425258755683899,
      "learning_rate": 4.024e-05,
      "loss": 0.0036,
      "step": 29280
    },
    {
      "epoch": 1.5621333333333334,
      "grad_norm": 0.183241605758667,
      "learning_rate": 4.023666666666667e-05,
      "loss": 0.0029,
      "step": 29290
    },
    {
      "epoch": 1.5626666666666666,
      "grad_norm": 0.035427339375019073,
      "learning_rate": 4.023333333333333e-05,
      "loss": 0.0029,
      "step": 29300
    },
    {
      "epoch": 1.5632000000000001,
      "grad_norm": 0.37801286578178406,
      "learning_rate": 4.023e-05,
      "loss": 0.0033,
      "step": 29310
    },
    {
      "epoch": 1.5637333333333334,
      "grad_norm": 0.2963390350341797,
      "learning_rate": 4.0226666666666666e-05,
      "loss": 0.0025,
      "step": 29320
    },
    {
      "epoch": 1.5642666666666667,
      "grad_norm": 0.1483776718378067,
      "learning_rate": 4.022333333333334e-05,
      "loss": 0.0034,
      "step": 29330
    },
    {
      "epoch": 1.5648,
      "grad_norm": 0.1257437765598297,
      "learning_rate": 4.0220000000000005e-05,
      "loss": 0.0032,
      "step": 29340
    },
    {
      "epoch": 1.5653333333333332,
      "grad_norm": 0.29768991470336914,
      "learning_rate": 4.021666666666667e-05,
      "loss": 0.0029,
      "step": 29350
    },
    {
      "epoch": 1.5658666666666665,
      "grad_norm": 0.12115854769945145,
      "learning_rate": 4.021333333333334e-05,
      "loss": 0.0024,
      "step": 29360
    },
    {
      "epoch": 1.5664,
      "grad_norm": 0.35158953070640564,
      "learning_rate": 4.021e-05,
      "loss": 0.0027,
      "step": 29370
    },
    {
      "epoch": 1.5669333333333333,
      "grad_norm": 0.2900330126285553,
      "learning_rate": 4.020666666666667e-05,
      "loss": 0.0038,
      "step": 29380
    },
    {
      "epoch": 1.5674666666666668,
      "grad_norm": 0.26947739720344543,
      "learning_rate": 4.0203333333333335e-05,
      "loss": 0.0038,
      "step": 29390
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.1296033412218094,
      "learning_rate": 4.02e-05,
      "loss": 0.0021,
      "step": 29400
    },
    {
      "epoch": 1.5685333333333333,
      "grad_norm": 0.2649758458137512,
      "learning_rate": 4.0196666666666674e-05,
      "loss": 0.0027,
      "step": 29410
    },
    {
      "epoch": 1.5690666666666666,
      "grad_norm": 0.3876422941684723,
      "learning_rate": 4.0193333333333334e-05,
      "loss": 0.0024,
      "step": 29420
    },
    {
      "epoch": 1.5695999999999999,
      "grad_norm": 0.30238252878189087,
      "learning_rate": 4.019e-05,
      "loss": 0.0033,
      "step": 29430
    },
    {
      "epoch": 1.5701333333333334,
      "grad_norm": 0.04692896455526352,
      "learning_rate": 4.0186666666666666e-05,
      "loss": 0.0024,
      "step": 29440
    },
    {
      "epoch": 1.5706666666666667,
      "grad_norm": 0.1537826806306839,
      "learning_rate": 4.018333333333333e-05,
      "loss": 0.003,
      "step": 29450
    },
    {
      "epoch": 1.5712000000000002,
      "grad_norm": 0.15279851853847504,
      "learning_rate": 4.018e-05,
      "loss": 0.0039,
      "step": 29460
    },
    {
      "epoch": 1.5717333333333334,
      "grad_norm": 0.18638762831687927,
      "learning_rate": 4.017666666666667e-05,
      "loss": 0.004,
      "step": 29470
    },
    {
      "epoch": 1.5722666666666667,
      "grad_norm": 0.4783432185649872,
      "learning_rate": 4.017333333333334e-05,
      "loss": 0.0027,
      "step": 29480
    },
    {
      "epoch": 1.5728,
      "grad_norm": 0.26643601059913635,
      "learning_rate": 4.017e-05,
      "loss": 0.0038,
      "step": 29490
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 0.7513262629508972,
      "learning_rate": 4.016666666666667e-05,
      "loss": 0.0024,
      "step": 29500
    },
    {
      "epoch": 1.5738666666666665,
      "grad_norm": 0.09740801155567169,
      "learning_rate": 4.0163333333333336e-05,
      "loss": 0.0028,
      "step": 29510
    },
    {
      "epoch": 1.5744,
      "grad_norm": 0.5390569567680359,
      "learning_rate": 4.016e-05,
      "loss": 0.0024,
      "step": 29520
    },
    {
      "epoch": 1.5749333333333333,
      "grad_norm": 0.5160923004150391,
      "learning_rate": 4.015666666666667e-05,
      "loss": 0.0031,
      "step": 29530
    },
    {
      "epoch": 1.5754666666666668,
      "grad_norm": 0.17733164131641388,
      "learning_rate": 4.0153333333333334e-05,
      "loss": 0.004,
      "step": 29540
    },
    {
      "epoch": 1.576,
      "grad_norm": 0.7145453095436096,
      "learning_rate": 4.015000000000001e-05,
      "loss": 0.0031,
      "step": 29550
    },
    {
      "epoch": 1.5765333333333333,
      "grad_norm": 0.09479143470525742,
      "learning_rate": 4.014666666666667e-05,
      "loss": 0.0018,
      "step": 29560
    },
    {
      "epoch": 1.5770666666666666,
      "grad_norm": 0.3939286470413208,
      "learning_rate": 4.014333333333333e-05,
      "loss": 0.0033,
      "step": 29570
    },
    {
      "epoch": 1.5776,
      "grad_norm": 0.03788433223962784,
      "learning_rate": 4.014e-05,
      "loss": 0.0028,
      "step": 29580
    },
    {
      "epoch": 1.5781333333333334,
      "grad_norm": 0.30587485432624817,
      "learning_rate": 4.0136666666666665e-05,
      "loss": 0.003,
      "step": 29590
    },
    {
      "epoch": 1.5786666666666667,
      "grad_norm": 0.5070223212242126,
      "learning_rate": 4.013333333333333e-05,
      "loss": 0.0029,
      "step": 29600
    },
    {
      "epoch": 1.5792000000000002,
      "grad_norm": 0.023400520905852318,
      "learning_rate": 4.0130000000000004e-05,
      "loss": 0.0025,
      "step": 29610
    },
    {
      "epoch": 1.5797333333333334,
      "grad_norm": 0.32483139634132385,
      "learning_rate": 4.012666666666667e-05,
      "loss": 0.0034,
      "step": 29620
    },
    {
      "epoch": 1.5802666666666667,
      "grad_norm": 0.1304709017276764,
      "learning_rate": 4.0123333333333336e-05,
      "loss": 0.0028,
      "step": 29630
    },
    {
      "epoch": 1.5808,
      "grad_norm": 0.2519257962703705,
      "learning_rate": 4.012e-05,
      "loss": 0.0029,
      "step": 29640
    },
    {
      "epoch": 1.5813333333333333,
      "grad_norm": 0.06496024131774902,
      "learning_rate": 4.011666666666667e-05,
      "loss": 0.0039,
      "step": 29650
    },
    {
      "epoch": 1.5818666666666665,
      "grad_norm": 0.09564544260501862,
      "learning_rate": 4.0113333333333334e-05,
      "loss": 0.0022,
      "step": 29660
    },
    {
      "epoch": 1.5824,
      "grad_norm": 0.2722039520740509,
      "learning_rate": 4.011e-05,
      "loss": 0.0024,
      "step": 29670
    },
    {
      "epoch": 1.5829333333333333,
      "grad_norm": 0.03607044741511345,
      "learning_rate": 4.0106666666666673e-05,
      "loss": 0.003,
      "step": 29680
    },
    {
      "epoch": 1.5834666666666668,
      "grad_norm": 0.2866762578487396,
      "learning_rate": 4.010333333333334e-05,
      "loss": 0.0021,
      "step": 29690
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.12324625998735428,
      "learning_rate": 4.0100000000000006e-05,
      "loss": 0.0026,
      "step": 29700
    },
    {
      "epoch": 1.5845333333333333,
      "grad_norm": 0.26961442828178406,
      "learning_rate": 4.009666666666667e-05,
      "loss": 0.0041,
      "step": 29710
    },
    {
      "epoch": 1.5850666666666666,
      "grad_norm": 0.5691055059432983,
      "learning_rate": 4.009333333333333e-05,
      "loss": 0.0024,
      "step": 29720
    },
    {
      "epoch": 1.5856,
      "grad_norm": 0.2068338841199875,
      "learning_rate": 4.009e-05,
      "loss": 0.003,
      "step": 29730
    },
    {
      "epoch": 1.5861333333333332,
      "grad_norm": 0.04369921237230301,
      "learning_rate": 4.0086666666666663e-05,
      "loss": 0.0033,
      "step": 29740
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 0.19538940489292145,
      "learning_rate": 4.0083333333333336e-05,
      "loss": 0.0023,
      "step": 29750
    },
    {
      "epoch": 1.5872000000000002,
      "grad_norm": 0.35885152220726013,
      "learning_rate": 4.008e-05,
      "loss": 0.0029,
      "step": 29760
    },
    {
      "epoch": 1.5877333333333334,
      "grad_norm": 0.2469303458929062,
      "learning_rate": 4.007666666666667e-05,
      "loss": 0.003,
      "step": 29770
    },
    {
      "epoch": 1.5882666666666667,
      "grad_norm": 0.300772100687027,
      "learning_rate": 4.0073333333333335e-05,
      "loss": 0.0028,
      "step": 29780
    },
    {
      "epoch": 1.5888,
      "grad_norm": 0.07918551564216614,
      "learning_rate": 4.007e-05,
      "loss": 0.0028,
      "step": 29790
    },
    {
      "epoch": 1.5893333333333333,
      "grad_norm": 0.20783576369285583,
      "learning_rate": 4.006666666666667e-05,
      "loss": 0.0034,
      "step": 29800
    },
    {
      "epoch": 1.5898666666666665,
      "grad_norm": 0.056457582861185074,
      "learning_rate": 4.006333333333333e-05,
      "loss": 0.0034,
      "step": 29810
    },
    {
      "epoch": 1.5904,
      "grad_norm": 0.0774737149477005,
      "learning_rate": 4.0060000000000006e-05,
      "loss": 0.003,
      "step": 29820
    },
    {
      "epoch": 1.5909333333333333,
      "grad_norm": 0.8305070996284485,
      "learning_rate": 4.005666666666667e-05,
      "loss": 0.0025,
      "step": 29830
    },
    {
      "epoch": 1.5914666666666668,
      "grad_norm": 0.5322769284248352,
      "learning_rate": 4.005333333333334e-05,
      "loss": 0.0035,
      "step": 29840
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.1487634927034378,
      "learning_rate": 4.0050000000000004e-05,
      "loss": 0.0023,
      "step": 29850
    },
    {
      "epoch": 1.5925333333333334,
      "grad_norm": 0.11924386024475098,
      "learning_rate": 4.004666666666667e-05,
      "loss": 0.0024,
      "step": 29860
    },
    {
      "epoch": 1.5930666666666666,
      "grad_norm": 0.6196784973144531,
      "learning_rate": 4.004333333333333e-05,
      "loss": 0.0045,
      "step": 29870
    },
    {
      "epoch": 1.5936,
      "grad_norm": 0.18428441882133484,
      "learning_rate": 4.004e-05,
      "loss": 0.0047,
      "step": 29880
    },
    {
      "epoch": 1.5941333333333332,
      "grad_norm": 0.3572588562965393,
      "learning_rate": 4.003666666666667e-05,
      "loss": 0.0036,
      "step": 29890
    },
    {
      "epoch": 1.5946666666666667,
      "grad_norm": 0.019473740831017494,
      "learning_rate": 4.0033333333333335e-05,
      "loss": 0.0024,
      "step": 29900
    },
    {
      "epoch": 1.5952,
      "grad_norm": 0.08883733302354813,
      "learning_rate": 4.003e-05,
      "loss": 0.0024,
      "step": 29910
    },
    {
      "epoch": 1.5957333333333334,
      "grad_norm": 0.16299878060817719,
      "learning_rate": 4.002666666666667e-05,
      "loss": 0.0027,
      "step": 29920
    },
    {
      "epoch": 1.5962666666666667,
      "grad_norm": 0.3533359169960022,
      "learning_rate": 4.0023333333333334e-05,
      "loss": 0.0038,
      "step": 29930
    },
    {
      "epoch": 1.5968,
      "grad_norm": 0.6471576690673828,
      "learning_rate": 4.002e-05,
      "loss": 0.003,
      "step": 29940
    },
    {
      "epoch": 1.5973333333333333,
      "grad_norm": 0.046083126217126846,
      "learning_rate": 4.0016666666666666e-05,
      "loss": 0.004,
      "step": 29950
    },
    {
      "epoch": 1.5978666666666665,
      "grad_norm": 0.25433510541915894,
      "learning_rate": 4.001333333333334e-05,
      "loss": 0.0024,
      "step": 29960
    },
    {
      "epoch": 1.5984,
      "grad_norm": 0.14842522144317627,
      "learning_rate": 4.0010000000000005e-05,
      "loss": 0.0046,
      "step": 29970
    },
    {
      "epoch": 1.5989333333333333,
      "grad_norm": 0.24777083098888397,
      "learning_rate": 4.000666666666667e-05,
      "loss": 0.0034,
      "step": 29980
    },
    {
      "epoch": 1.5994666666666668,
      "grad_norm": 0.04182332754135132,
      "learning_rate": 4.000333333333334e-05,
      "loss": 0.004,
      "step": 29990
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.23771308362483978,
      "learning_rate": 4e-05,
      "loss": 0.0039,
      "step": 30000
    },
    {
      "epoch": 1.6005333333333334,
      "grad_norm": 0.0978889912366867,
      "learning_rate": 3.999666666666667e-05,
      "loss": 0.0027,
      "step": 30010
    },
    {
      "epoch": 1.6010666666666666,
      "grad_norm": 0.3575943410396576,
      "learning_rate": 3.9993333333333336e-05,
      "loss": 0.0027,
      "step": 30020
    },
    {
      "epoch": 1.6016,
      "grad_norm": 0.020956560969352722,
      "learning_rate": 3.999e-05,
      "loss": 0.0034,
      "step": 30030
    },
    {
      "epoch": 1.6021333333333332,
      "grad_norm": 0.09292023628950119,
      "learning_rate": 3.998666666666667e-05,
      "loss": 0.0028,
      "step": 30040
    },
    {
      "epoch": 1.6026666666666667,
      "grad_norm": 0.2088645100593567,
      "learning_rate": 3.9983333333333334e-05,
      "loss": 0.0022,
      "step": 30050
    },
    {
      "epoch": 1.6032,
      "grad_norm": 0.25557929277420044,
      "learning_rate": 3.998e-05,
      "loss": 0.0031,
      "step": 30060
    },
    {
      "epoch": 1.6037333333333335,
      "grad_norm": 0.20599175989627838,
      "learning_rate": 3.9976666666666666e-05,
      "loss": 0.0032,
      "step": 30070
    },
    {
      "epoch": 1.6042666666666667,
      "grad_norm": 0.06803809851408005,
      "learning_rate": 3.997333333333333e-05,
      "loss": 0.0023,
      "step": 30080
    },
    {
      "epoch": 1.6048,
      "grad_norm": 0.2746132016181946,
      "learning_rate": 3.9970000000000005e-05,
      "loss": 0.0025,
      "step": 30090
    },
    {
      "epoch": 1.6053333333333333,
      "grad_norm": 0.2977266013622284,
      "learning_rate": 3.996666666666667e-05,
      "loss": 0.0029,
      "step": 30100
    },
    {
      "epoch": 1.6058666666666666,
      "grad_norm": 0.304395467042923,
      "learning_rate": 3.996333333333334e-05,
      "loss": 0.0024,
      "step": 30110
    },
    {
      "epoch": 1.6064,
      "grad_norm": 0.23734432458877563,
      "learning_rate": 3.9960000000000004e-05,
      "loss": 0.003,
      "step": 30120
    },
    {
      "epoch": 1.6069333333333333,
      "grad_norm": 0.0699085220694542,
      "learning_rate": 3.995666666666667e-05,
      "loss": 0.0022,
      "step": 30130
    },
    {
      "epoch": 1.6074666666666668,
      "grad_norm": 0.12210293114185333,
      "learning_rate": 3.9953333333333336e-05,
      "loss": 0.003,
      "step": 30140
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.6292217373847961,
      "learning_rate": 3.995e-05,
      "loss": 0.0024,
      "step": 30150
    },
    {
      "epoch": 1.6085333333333334,
      "grad_norm": 0.05697029456496239,
      "learning_rate": 3.994666666666667e-05,
      "loss": 0.005,
      "step": 30160
    },
    {
      "epoch": 1.6090666666666666,
      "grad_norm": 0.06379448622465134,
      "learning_rate": 3.9943333333333334e-05,
      "loss": 0.0026,
      "step": 30170
    },
    {
      "epoch": 1.6096,
      "grad_norm": 0.5080024003982544,
      "learning_rate": 3.994e-05,
      "loss": 0.003,
      "step": 30180
    },
    {
      "epoch": 1.6101333333333332,
      "grad_norm": 0.4528413712978363,
      "learning_rate": 3.9936666666666667e-05,
      "loss": 0.0024,
      "step": 30190
    },
    {
      "epoch": 1.6106666666666667,
      "grad_norm": 0.1171160340309143,
      "learning_rate": 3.993333333333333e-05,
      "loss": 0.0043,
      "step": 30200
    },
    {
      "epoch": 1.6112,
      "grad_norm": 0.03920339420437813,
      "learning_rate": 3.993e-05,
      "loss": 0.0029,
      "step": 30210
    },
    {
      "epoch": 1.6117333333333335,
      "grad_norm": 0.039840102195739746,
      "learning_rate": 3.9926666666666665e-05,
      "loss": 0.0024,
      "step": 30220
    },
    {
      "epoch": 1.6122666666666667,
      "grad_norm": 0.32486477494239807,
      "learning_rate": 3.992333333333334e-05,
      "loss": 0.0028,
      "step": 30230
    },
    {
      "epoch": 1.6128,
      "grad_norm": 0.2687489688396454,
      "learning_rate": 3.9920000000000004e-05,
      "loss": 0.0018,
      "step": 30240
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.17879314720630646,
      "learning_rate": 3.991666666666667e-05,
      "loss": 0.0033,
      "step": 30250
    },
    {
      "epoch": 1.6138666666666666,
      "grad_norm": 0.1206662580370903,
      "learning_rate": 3.9913333333333336e-05,
      "loss": 0.0027,
      "step": 30260
    },
    {
      "epoch": 1.6143999999999998,
      "grad_norm": 0.3830181956291199,
      "learning_rate": 3.991e-05,
      "loss": 0.0023,
      "step": 30270
    },
    {
      "epoch": 1.6149333333333333,
      "grad_norm": 0.3521701693534851,
      "learning_rate": 3.990666666666667e-05,
      "loss": 0.0025,
      "step": 30280
    },
    {
      "epoch": 1.6154666666666668,
      "grad_norm": 0.11713007837533951,
      "learning_rate": 3.9903333333333335e-05,
      "loss": 0.003,
      "step": 30290
    },
    {
      "epoch": 1.616,
      "grad_norm": 0.3078514635562897,
      "learning_rate": 3.99e-05,
      "loss": 0.0037,
      "step": 30300
    },
    {
      "epoch": 1.6165333333333334,
      "grad_norm": 0.6994331479072571,
      "learning_rate": 3.9896666666666674e-05,
      "loss": 0.0032,
      "step": 30310
    },
    {
      "epoch": 1.6170666666666667,
      "grad_norm": 0.7094230651855469,
      "learning_rate": 3.989333333333333e-05,
      "loss": 0.0019,
      "step": 30320
    },
    {
      "epoch": 1.6176,
      "grad_norm": 0.29736050963401794,
      "learning_rate": 3.989e-05,
      "loss": 0.0028,
      "step": 30330
    },
    {
      "epoch": 1.6181333333333332,
      "grad_norm": 0.2098805010318756,
      "learning_rate": 3.9886666666666665e-05,
      "loss": 0.0031,
      "step": 30340
    },
    {
      "epoch": 1.6186666666666667,
      "grad_norm": 0.05293061211705208,
      "learning_rate": 3.988333333333333e-05,
      "loss": 0.0027,
      "step": 30350
    },
    {
      "epoch": 1.6192,
      "grad_norm": 0.3826679289340973,
      "learning_rate": 3.988e-05,
      "loss": 0.0026,
      "step": 30360
    },
    {
      "epoch": 1.6197333333333335,
      "grad_norm": 0.10102035105228424,
      "learning_rate": 3.987666666666667e-05,
      "loss": 0.0031,
      "step": 30370
    },
    {
      "epoch": 1.6202666666666667,
      "grad_norm": 0.12228276580572128,
      "learning_rate": 3.987333333333334e-05,
      "loss": 0.0032,
      "step": 30380
    },
    {
      "epoch": 1.6208,
      "grad_norm": 0.35551971197128296,
      "learning_rate": 3.987e-05,
      "loss": 0.0028,
      "step": 30390
    },
    {
      "epoch": 1.6213333333333333,
      "grad_norm": 0.3563135266304016,
      "learning_rate": 3.986666666666667e-05,
      "loss": 0.0035,
      "step": 30400
    },
    {
      "epoch": 1.6218666666666666,
      "grad_norm": 0.743878960609436,
      "learning_rate": 3.9863333333333335e-05,
      "loss": 0.0029,
      "step": 30410
    },
    {
      "epoch": 1.6223999999999998,
      "grad_norm": 0.3013053238391876,
      "learning_rate": 3.986e-05,
      "loss": 0.0036,
      "step": 30420
    },
    {
      "epoch": 1.6229333333333333,
      "grad_norm": 0.3535199463367462,
      "learning_rate": 3.985666666666667e-05,
      "loss": 0.0023,
      "step": 30430
    },
    {
      "epoch": 1.6234666666666666,
      "grad_norm": 0.17954802513122559,
      "learning_rate": 3.985333333333334e-05,
      "loss": 0.0026,
      "step": 30440
    },
    {
      "epoch": 1.624,
      "grad_norm": 0.05783066898584366,
      "learning_rate": 3.9850000000000006e-05,
      "loss": 0.0028,
      "step": 30450
    },
    {
      "epoch": 1.6245333333333334,
      "grad_norm": 0.1782762110233307,
      "learning_rate": 3.984666666666667e-05,
      "loss": 0.0027,
      "step": 30460
    },
    {
      "epoch": 1.6250666666666667,
      "grad_norm": 0.2550007998943329,
      "learning_rate": 3.984333333333333e-05,
      "loss": 0.0022,
      "step": 30470
    },
    {
      "epoch": 1.6256,
      "grad_norm": 0.3536715805530548,
      "learning_rate": 3.984e-05,
      "loss": 0.0021,
      "step": 30480
    },
    {
      "epoch": 1.6261333333333332,
      "grad_norm": 0.38423481583595276,
      "learning_rate": 3.9836666666666664e-05,
      "loss": 0.0041,
      "step": 30490
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 0.24138610064983368,
      "learning_rate": 3.983333333333333e-05,
      "loss": 0.0029,
      "step": 30500
    },
    {
      "epoch": 1.6272,
      "grad_norm": 0.29125478863716125,
      "learning_rate": 3.983e-05,
      "loss": 0.0023,
      "step": 30510
    },
    {
      "epoch": 1.6277333333333335,
      "grad_norm": 0.17789411544799805,
      "learning_rate": 3.982666666666667e-05,
      "loss": 0.0026,
      "step": 30520
    },
    {
      "epoch": 1.6282666666666668,
      "grad_norm": 0.1994745284318924,
      "learning_rate": 3.9823333333333335e-05,
      "loss": 0.0037,
      "step": 30530
    },
    {
      "epoch": 1.6288,
      "grad_norm": 0.2327033430337906,
      "learning_rate": 3.982e-05,
      "loss": 0.0023,
      "step": 30540
    },
    {
      "epoch": 1.6293333333333333,
      "grad_norm": 0.3564892113208771,
      "learning_rate": 3.981666666666667e-05,
      "loss": 0.0029,
      "step": 30550
    },
    {
      "epoch": 1.6298666666666666,
      "grad_norm": 0.12411968410015106,
      "learning_rate": 3.9813333333333334e-05,
      "loss": 0.0037,
      "step": 30560
    },
    {
      "epoch": 1.6303999999999998,
      "grad_norm": 0.38205164670944214,
      "learning_rate": 3.981e-05,
      "loss": 0.0039,
      "step": 30570
    },
    {
      "epoch": 1.6309333333333333,
      "grad_norm": 0.4446002244949341,
      "learning_rate": 3.980666666666667e-05,
      "loss": 0.0045,
      "step": 30580
    },
    {
      "epoch": 1.6314666666666666,
      "grad_norm": 0.042183276265859604,
      "learning_rate": 3.980333333333334e-05,
      "loss": 0.0036,
      "step": 30590
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.26138728857040405,
      "learning_rate": 3.9800000000000005e-05,
      "loss": 0.0021,
      "step": 30600
    },
    {
      "epoch": 1.6325333333333334,
      "grad_norm": 0.21069110929965973,
      "learning_rate": 3.979666666666667e-05,
      "loss": 0.0029,
      "step": 30610
    },
    {
      "epoch": 1.6330666666666667,
      "grad_norm": 0.15852613747119904,
      "learning_rate": 3.979333333333333e-05,
      "loss": 0.0033,
      "step": 30620
    },
    {
      "epoch": 1.6336,
      "grad_norm": 0.502467930316925,
      "learning_rate": 3.979e-05,
      "loss": 0.0027,
      "step": 30630
    },
    {
      "epoch": 1.6341333333333332,
      "grad_norm": 0.14788921177387238,
      "learning_rate": 3.978666666666667e-05,
      "loss": 0.0032,
      "step": 30640
    },
    {
      "epoch": 1.6346666666666667,
      "grad_norm": 0.4715884029865265,
      "learning_rate": 3.9783333333333336e-05,
      "loss": 0.0016,
      "step": 30650
    },
    {
      "epoch": 1.6352,
      "grad_norm": 0.08985785394906998,
      "learning_rate": 3.978e-05,
      "loss": 0.0034,
      "step": 30660
    },
    {
      "epoch": 1.6357333333333335,
      "grad_norm": 0.4118274748325348,
      "learning_rate": 3.977666666666667e-05,
      "loss": 0.0032,
      "step": 30670
    },
    {
      "epoch": 1.6362666666666668,
      "grad_norm": 0.20820511877536774,
      "learning_rate": 3.9773333333333334e-05,
      "loss": 0.003,
      "step": 30680
    },
    {
      "epoch": 1.6368,
      "grad_norm": 0.8309633731842041,
      "learning_rate": 3.977e-05,
      "loss": 0.0038,
      "step": 30690
    },
    {
      "epoch": 1.6373333333333333,
      "grad_norm": 0.20941418409347534,
      "learning_rate": 3.9766666666666667e-05,
      "loss": 0.0023,
      "step": 30700
    },
    {
      "epoch": 1.6378666666666666,
      "grad_norm": 0.2669580280780792,
      "learning_rate": 3.976333333333333e-05,
      "loss": 0.0033,
      "step": 30710
    },
    {
      "epoch": 1.6383999999999999,
      "grad_norm": 0.2988234758377075,
      "learning_rate": 3.9760000000000006e-05,
      "loss": 0.0035,
      "step": 30720
    },
    {
      "epoch": 1.6389333333333334,
      "grad_norm": 0.47673332691192627,
      "learning_rate": 3.975666666666667e-05,
      "loss": 0.0037,
      "step": 30730
    },
    {
      "epoch": 1.6394666666666666,
      "grad_norm": 0.3238881826400757,
      "learning_rate": 3.975333333333334e-05,
      "loss": 0.0033,
      "step": 30740
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.09554766863584518,
      "learning_rate": 3.9750000000000004e-05,
      "loss": 0.0023,
      "step": 30750
    },
    {
      "epoch": 1.6405333333333334,
      "grad_norm": 0.32509738206863403,
      "learning_rate": 3.974666666666667e-05,
      "loss": 0.0038,
      "step": 30760
    },
    {
      "epoch": 1.6410666666666667,
      "grad_norm": 0.041271504014730453,
      "learning_rate": 3.9743333333333336e-05,
      "loss": 0.003,
      "step": 30770
    },
    {
      "epoch": 1.6416,
      "grad_norm": 0.031146708875894547,
      "learning_rate": 3.974e-05,
      "loss": 0.0026,
      "step": 30780
    },
    {
      "epoch": 1.6421333333333332,
      "grad_norm": 0.1573931872844696,
      "learning_rate": 3.973666666666667e-05,
      "loss": 0.0026,
      "step": 30790
    },
    {
      "epoch": 1.6426666666666667,
      "grad_norm": 0.20536085963249207,
      "learning_rate": 3.9733333333333335e-05,
      "loss": 0.0023,
      "step": 30800
    },
    {
      "epoch": 1.6432,
      "grad_norm": 0.09443093091249466,
      "learning_rate": 3.973e-05,
      "loss": 0.0044,
      "step": 30810
    },
    {
      "epoch": 1.6437333333333335,
      "grad_norm": 0.1800878494977951,
      "learning_rate": 3.972666666666667e-05,
      "loss": 0.0036,
      "step": 30820
    },
    {
      "epoch": 1.6442666666666668,
      "grad_norm": 0.053788378834724426,
      "learning_rate": 3.972333333333333e-05,
      "loss": 0.0031,
      "step": 30830
    },
    {
      "epoch": 1.6448,
      "grad_norm": 0.08973516523838043,
      "learning_rate": 3.972e-05,
      "loss": 0.0031,
      "step": 30840
    },
    {
      "epoch": 1.6453333333333333,
      "grad_norm": 0.5347641706466675,
      "learning_rate": 3.9716666666666665e-05,
      "loss": 0.0017,
      "step": 30850
    },
    {
      "epoch": 1.6458666666666666,
      "grad_norm": 0.2971186935901642,
      "learning_rate": 3.971333333333334e-05,
      "loss": 0.0033,
      "step": 30860
    },
    {
      "epoch": 1.6463999999999999,
      "grad_norm": 0.20523914694786072,
      "learning_rate": 3.9710000000000004e-05,
      "loss": 0.0036,
      "step": 30870
    },
    {
      "epoch": 1.6469333333333334,
      "grad_norm": 0.292925089597702,
      "learning_rate": 3.970666666666667e-05,
      "loss": 0.0027,
      "step": 30880
    },
    {
      "epoch": 1.6474666666666666,
      "grad_norm": 0.12016437947750092,
      "learning_rate": 3.970333333333334e-05,
      "loss": 0.0027,
      "step": 30890
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.12278445810079575,
      "learning_rate": 3.97e-05,
      "loss": 0.0029,
      "step": 30900
    },
    {
      "epoch": 1.6485333333333334,
      "grad_norm": 0.4097534418106079,
      "learning_rate": 3.969666666666667e-05,
      "loss": 0.0036,
      "step": 30910
    },
    {
      "epoch": 1.6490666666666667,
      "grad_norm": 0.039604704827070236,
      "learning_rate": 3.9693333333333335e-05,
      "loss": 0.0029,
      "step": 30920
    },
    {
      "epoch": 1.6496,
      "grad_norm": 0.4090244174003601,
      "learning_rate": 3.969e-05,
      "loss": 0.0027,
      "step": 30930
    },
    {
      "epoch": 1.6501333333333332,
      "grad_norm": 0.15192438662052155,
      "learning_rate": 3.968666666666667e-05,
      "loss": 0.0028,
      "step": 30940
    },
    {
      "epoch": 1.6506666666666665,
      "grad_norm": 0.06225777789950371,
      "learning_rate": 3.9683333333333333e-05,
      "loss": 0.0047,
      "step": 30950
    },
    {
      "epoch": 1.6512,
      "grad_norm": 0.18094223737716675,
      "learning_rate": 3.968e-05,
      "loss": 0.0041,
      "step": 30960
    },
    {
      "epoch": 1.6517333333333335,
      "grad_norm": 0.08458337187767029,
      "learning_rate": 3.9676666666666666e-05,
      "loss": 0.0025,
      "step": 30970
    },
    {
      "epoch": 1.6522666666666668,
      "grad_norm": 0.03691000118851662,
      "learning_rate": 3.967333333333333e-05,
      "loss": 0.0033,
      "step": 30980
    },
    {
      "epoch": 1.6528,
      "grad_norm": 0.27966269850730896,
      "learning_rate": 3.9670000000000005e-05,
      "loss": 0.0031,
      "step": 30990
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 0.29607513546943665,
      "learning_rate": 3.966666666666667e-05,
      "loss": 0.0025,
      "step": 31000
    },
    {
      "epoch": 1.6538666666666666,
      "grad_norm": 0.31408795714378357,
      "learning_rate": 3.966333333333334e-05,
      "loss": 0.0027,
      "step": 31010
    },
    {
      "epoch": 1.6543999999999999,
      "grad_norm": 0.03548763319849968,
      "learning_rate": 3.966e-05,
      "loss": 0.0023,
      "step": 31020
    },
    {
      "epoch": 1.6549333333333334,
      "grad_norm": 0.08965770155191422,
      "learning_rate": 3.965666666666667e-05,
      "loss": 0.0027,
      "step": 31030
    },
    {
      "epoch": 1.6554666666666666,
      "grad_norm": 0.0968221127986908,
      "learning_rate": 3.9653333333333335e-05,
      "loss": 0.0016,
      "step": 31040
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 0.478171706199646,
      "learning_rate": 3.965e-05,
      "loss": 0.003,
      "step": 31050
    },
    {
      "epoch": 1.6565333333333334,
      "grad_norm": 0.09197945892810822,
      "learning_rate": 3.964666666666667e-05,
      "loss": 0.0038,
      "step": 31060
    },
    {
      "epoch": 1.6570666666666667,
      "grad_norm": 0.4433369040489197,
      "learning_rate": 3.964333333333334e-05,
      "loss": 0.0023,
      "step": 31070
    },
    {
      "epoch": 1.6576,
      "grad_norm": 0.35436081886291504,
      "learning_rate": 3.964e-05,
      "loss": 0.0027,
      "step": 31080
    },
    {
      "epoch": 1.6581333333333332,
      "grad_norm": 0.5497037768363953,
      "learning_rate": 3.9636666666666666e-05,
      "loss": 0.0032,
      "step": 31090
    },
    {
      "epoch": 1.6586666666666665,
      "grad_norm": 0.36069706082344055,
      "learning_rate": 3.963333333333333e-05,
      "loss": 0.0024,
      "step": 31100
    },
    {
      "epoch": 1.6592,
      "grad_norm": 0.06318961083889008,
      "learning_rate": 3.963e-05,
      "loss": 0.004,
      "step": 31110
    },
    {
      "epoch": 1.6597333333333333,
      "grad_norm": 0.23752915859222412,
      "learning_rate": 3.9626666666666664e-05,
      "loss": 0.0024,
      "step": 31120
    },
    {
      "epoch": 1.6602666666666668,
      "grad_norm": 0.1487204134464264,
      "learning_rate": 3.962333333333334e-05,
      "loss": 0.004,
      "step": 31130
    },
    {
      "epoch": 1.6608,
      "grad_norm": 0.3264443874359131,
      "learning_rate": 3.9620000000000004e-05,
      "loss": 0.004,
      "step": 31140
    },
    {
      "epoch": 1.6613333333333333,
      "grad_norm": 0.24126562476158142,
      "learning_rate": 3.961666666666667e-05,
      "loss": 0.0019,
      "step": 31150
    },
    {
      "epoch": 1.6618666666666666,
      "grad_norm": 0.41838768124580383,
      "learning_rate": 3.9613333333333336e-05,
      "loss": 0.0029,
      "step": 31160
    },
    {
      "epoch": 1.6623999999999999,
      "grad_norm": 0.44519534707069397,
      "learning_rate": 3.961e-05,
      "loss": 0.0028,
      "step": 31170
    },
    {
      "epoch": 1.6629333333333334,
      "grad_norm": 0.22311483323574066,
      "learning_rate": 3.960666666666667e-05,
      "loss": 0.0045,
      "step": 31180
    },
    {
      "epoch": 1.6634666666666666,
      "grad_norm": 0.38563141226768494,
      "learning_rate": 3.9603333333333334e-05,
      "loss": 0.0025,
      "step": 31190
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.2980344891548157,
      "learning_rate": 3.960000000000001e-05,
      "loss": 0.0023,
      "step": 31200
    },
    {
      "epoch": 1.6645333333333334,
      "grad_norm": 0.4757142961025238,
      "learning_rate": 3.959666666666667e-05,
      "loss": 0.0033,
      "step": 31210
    },
    {
      "epoch": 1.6650666666666667,
      "grad_norm": 0.2679852247238159,
      "learning_rate": 3.959333333333334e-05,
      "loss": 0.0036,
      "step": 31220
    },
    {
      "epoch": 1.6656,
      "grad_norm": 0.1496986299753189,
      "learning_rate": 3.959e-05,
      "loss": 0.0036,
      "step": 31230
    },
    {
      "epoch": 1.6661333333333332,
      "grad_norm": 0.23444052040576935,
      "learning_rate": 3.9586666666666665e-05,
      "loss": 0.0028,
      "step": 31240
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.18285323679447174,
      "learning_rate": 3.958333333333333e-05,
      "loss": 0.0029,
      "step": 31250
    },
    {
      "epoch": 1.6672,
      "grad_norm": 0.4167605936527252,
      "learning_rate": 3.958e-05,
      "loss": 0.004,
      "step": 31260
    },
    {
      "epoch": 1.6677333333333333,
      "grad_norm": 0.09260016679763794,
      "learning_rate": 3.957666666666667e-05,
      "loss": 0.0027,
      "step": 31270
    },
    {
      "epoch": 1.6682666666666668,
      "grad_norm": 0.2335139513015747,
      "learning_rate": 3.9573333333333336e-05,
      "loss": 0.0028,
      "step": 31280
    },
    {
      "epoch": 1.6688,
      "grad_norm": 0.2108801156282425,
      "learning_rate": 3.957e-05,
      "loss": 0.0035,
      "step": 31290
    },
    {
      "epoch": 1.6693333333333333,
      "grad_norm": 0.37926921248435974,
      "learning_rate": 3.956666666666667e-05,
      "loss": 0.003,
      "step": 31300
    },
    {
      "epoch": 1.6698666666666666,
      "grad_norm": 0.504227876663208,
      "learning_rate": 3.9563333333333335e-05,
      "loss": 0.0032,
      "step": 31310
    },
    {
      "epoch": 1.6703999999999999,
      "grad_norm": 0.18475443124771118,
      "learning_rate": 3.956e-05,
      "loss": 0.0025,
      "step": 31320
    },
    {
      "epoch": 1.6709333333333334,
      "grad_norm": 0.18495677411556244,
      "learning_rate": 3.955666666666667e-05,
      "loss": 0.0029,
      "step": 31330
    },
    {
      "epoch": 1.6714666666666667,
      "grad_norm": 0.1460852473974228,
      "learning_rate": 3.955333333333334e-05,
      "loss": 0.0031,
      "step": 31340
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.24456430971622467,
      "learning_rate": 3.9550000000000006e-05,
      "loss": 0.0037,
      "step": 31350
    },
    {
      "epoch": 1.6725333333333334,
      "grad_norm": 0.06536387652158737,
      "learning_rate": 3.954666666666667e-05,
      "loss": 0.0025,
      "step": 31360
    },
    {
      "epoch": 1.6730666666666667,
      "grad_norm": 0.2670478820800781,
      "learning_rate": 3.954333333333334e-05,
      "loss": 0.004,
      "step": 31370
    },
    {
      "epoch": 1.6736,
      "grad_norm": 0.2540362775325775,
      "learning_rate": 3.954e-05,
      "loss": 0.0034,
      "step": 31380
    },
    {
      "epoch": 1.6741333333333333,
      "grad_norm": 0.3770168125629425,
      "learning_rate": 3.9536666666666664e-05,
      "loss": 0.003,
      "step": 31390
    },
    {
      "epoch": 1.6746666666666665,
      "grad_norm": 0.5314831733703613,
      "learning_rate": 3.9533333333333337e-05,
      "loss": 0.0023,
      "step": 31400
    },
    {
      "epoch": 1.6752,
      "grad_norm": 0.4395754635334015,
      "learning_rate": 3.953e-05,
      "loss": 0.0031,
      "step": 31410
    },
    {
      "epoch": 1.6757333333333333,
      "grad_norm": 0.48095086216926575,
      "learning_rate": 3.952666666666667e-05,
      "loss": 0.0032,
      "step": 31420
    },
    {
      "epoch": 1.6762666666666668,
      "grad_norm": 0.7343342900276184,
      "learning_rate": 3.9523333333333335e-05,
      "loss": 0.0038,
      "step": 31430
    },
    {
      "epoch": 1.6768,
      "grad_norm": 0.533135175704956,
      "learning_rate": 3.952e-05,
      "loss": 0.0029,
      "step": 31440
    },
    {
      "epoch": 1.6773333333333333,
      "grad_norm": 0.2623448967933655,
      "learning_rate": 3.951666666666667e-05,
      "loss": 0.0029,
      "step": 31450
    },
    {
      "epoch": 1.6778666666666666,
      "grad_norm": 0.18733789026737213,
      "learning_rate": 3.951333333333333e-05,
      "loss": 0.0034,
      "step": 31460
    },
    {
      "epoch": 1.6784,
      "grad_norm": 0.10367991030216217,
      "learning_rate": 3.951e-05,
      "loss": 0.003,
      "step": 31470
    },
    {
      "epoch": 1.6789333333333334,
      "grad_norm": 0.3139093220233917,
      "learning_rate": 3.950666666666667e-05,
      "loss": 0.0028,
      "step": 31480
    },
    {
      "epoch": 1.6794666666666667,
      "grad_norm": 0.47867128252983093,
      "learning_rate": 3.950333333333334e-05,
      "loss": 0.0038,
      "step": 31490
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.4067569375038147,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 0.0034,
      "step": 31500
    },
    {
      "epoch": 1.6805333333333334,
      "grad_norm": 0.7861634492874146,
      "learning_rate": 3.949666666666667e-05,
      "loss": 0.0028,
      "step": 31510
    },
    {
      "epoch": 1.6810666666666667,
      "grad_norm": 0.1800031214952469,
      "learning_rate": 3.949333333333334e-05,
      "loss": 0.0025,
      "step": 31520
    },
    {
      "epoch": 1.6816,
      "grad_norm": 0.08961073309183121,
      "learning_rate": 3.9489999999999996e-05,
      "loss": 0.0027,
      "step": 31530
    },
    {
      "epoch": 1.6821333333333333,
      "grad_norm": 0.14730165898799896,
      "learning_rate": 3.948666666666667e-05,
      "loss": 0.0033,
      "step": 31540
    },
    {
      "epoch": 1.6826666666666665,
      "grad_norm": 0.2987746000289917,
      "learning_rate": 3.9483333333333335e-05,
      "loss": 0.0029,
      "step": 31550
    },
    {
      "epoch": 1.6832,
      "grad_norm": 0.060983434319496155,
      "learning_rate": 3.948e-05,
      "loss": 0.0023,
      "step": 31560
    },
    {
      "epoch": 1.6837333333333333,
      "grad_norm": 0.11661256849765778,
      "learning_rate": 3.947666666666667e-05,
      "loss": 0.004,
      "step": 31570
    },
    {
      "epoch": 1.6842666666666668,
      "grad_norm": 0.2080278843641281,
      "learning_rate": 3.9473333333333334e-05,
      "loss": 0.003,
      "step": 31580
    },
    {
      "epoch": 1.6848,
      "grad_norm": 0.020511552691459656,
      "learning_rate": 3.947e-05,
      "loss": 0.003,
      "step": 31590
    },
    {
      "epoch": 1.6853333333333333,
      "grad_norm": 0.2937552034854889,
      "learning_rate": 3.9466666666666666e-05,
      "loss": 0.0032,
      "step": 31600
    },
    {
      "epoch": 1.6858666666666666,
      "grad_norm": 0.19058942794799805,
      "learning_rate": 3.946333333333333e-05,
      "loss": 0.0023,
      "step": 31610
    },
    {
      "epoch": 1.6864,
      "grad_norm": 0.6484217047691345,
      "learning_rate": 3.9460000000000005e-05,
      "loss": 0.0023,
      "step": 31620
    },
    {
      "epoch": 1.6869333333333332,
      "grad_norm": 0.42199164628982544,
      "learning_rate": 3.945666666666667e-05,
      "loss": 0.0031,
      "step": 31630
    },
    {
      "epoch": 1.6874666666666667,
      "grad_norm": 0.20235317945480347,
      "learning_rate": 3.945333333333334e-05,
      "loss": 0.0027,
      "step": 31640
    },
    {
      "epoch": 1.688,
      "grad_norm": 0.20443888008594513,
      "learning_rate": 3.9450000000000003e-05,
      "loss": 0.0036,
      "step": 31650
    },
    {
      "epoch": 1.6885333333333334,
      "grad_norm": 0.2898002564907074,
      "learning_rate": 3.944666666666667e-05,
      "loss": 0.0027,
      "step": 31660
    },
    {
      "epoch": 1.6890666666666667,
      "grad_norm": 0.5882129669189453,
      "learning_rate": 3.9443333333333336e-05,
      "loss": 0.004,
      "step": 31670
    },
    {
      "epoch": 1.6896,
      "grad_norm": 0.23582449555397034,
      "learning_rate": 3.944e-05,
      "loss": 0.0032,
      "step": 31680
    },
    {
      "epoch": 1.6901333333333333,
      "grad_norm": 0.3231436610221863,
      "learning_rate": 3.943666666666667e-05,
      "loss": 0.0034,
      "step": 31690
    },
    {
      "epoch": 1.6906666666666665,
      "grad_norm": 0.03982983157038689,
      "learning_rate": 3.9433333333333334e-05,
      "loss": 0.0022,
      "step": 31700
    },
    {
      "epoch": 1.6912,
      "grad_norm": 0.23906779289245605,
      "learning_rate": 3.943e-05,
      "loss": 0.0023,
      "step": 31710
    },
    {
      "epoch": 1.6917333333333333,
      "grad_norm": 0.059540558606386185,
      "learning_rate": 3.9426666666666666e-05,
      "loss": 0.003,
      "step": 31720
    },
    {
      "epoch": 1.6922666666666668,
      "grad_norm": 0.3829208016395569,
      "learning_rate": 3.942333333333333e-05,
      "loss": 0.0037,
      "step": 31730
    },
    {
      "epoch": 1.6928,
      "grad_norm": 0.0901217982172966,
      "learning_rate": 3.942e-05,
      "loss": 0.0029,
      "step": 31740
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.17835046350955963,
      "learning_rate": 3.941666666666667e-05,
      "loss": 0.0023,
      "step": 31750
    },
    {
      "epoch": 1.6938666666666666,
      "grad_norm": 0.1323683261871338,
      "learning_rate": 3.941333333333334e-05,
      "loss": 0.0025,
      "step": 31760
    },
    {
      "epoch": 1.6944,
      "grad_norm": 0.09595498442649841,
      "learning_rate": 3.9410000000000004e-05,
      "loss": 0.0037,
      "step": 31770
    },
    {
      "epoch": 1.6949333333333332,
      "grad_norm": 0.5352415442466736,
      "learning_rate": 3.940666666666667e-05,
      "loss": 0.0029,
      "step": 31780
    },
    {
      "epoch": 1.6954666666666667,
      "grad_norm": 0.1498449593782425,
      "learning_rate": 3.9403333333333336e-05,
      "loss": 0.0032,
      "step": 31790
    },
    {
      "epoch": 1.696,
      "grad_norm": 0.37674492597579956,
      "learning_rate": 3.94e-05,
      "loss": 0.0028,
      "step": 31800
    },
    {
      "epoch": 1.6965333333333334,
      "grad_norm": 0.13239669799804688,
      "learning_rate": 3.939666666666667e-05,
      "loss": 0.0026,
      "step": 31810
    },
    {
      "epoch": 1.6970666666666667,
      "grad_norm": 0.06928252428770065,
      "learning_rate": 3.9393333333333335e-05,
      "loss": 0.0021,
      "step": 31820
    },
    {
      "epoch": 1.6976,
      "grad_norm": 0.05504849553108215,
      "learning_rate": 3.939e-05,
      "loss": 0.0032,
      "step": 31830
    },
    {
      "epoch": 1.6981333333333333,
      "grad_norm": 0.32723864912986755,
      "learning_rate": 3.938666666666667e-05,
      "loss": 0.0025,
      "step": 31840
    },
    {
      "epoch": 1.6986666666666665,
      "grad_norm": 0.581577479839325,
      "learning_rate": 3.938333333333333e-05,
      "loss": 0.0032,
      "step": 31850
    },
    {
      "epoch": 1.6992,
      "grad_norm": 0.7315768599510193,
      "learning_rate": 3.938e-05,
      "loss": 0.004,
      "step": 31860
    },
    {
      "epoch": 1.6997333333333333,
      "grad_norm": 0.7691379189491272,
      "learning_rate": 3.9376666666666665e-05,
      "loss": 0.0042,
      "step": 31870
    },
    {
      "epoch": 1.7002666666666668,
      "grad_norm": 0.5287885665893555,
      "learning_rate": 3.937333333333333e-05,
      "loss": 0.0037,
      "step": 31880
    },
    {
      "epoch": 1.7008,
      "grad_norm": 0.2421943098306656,
      "learning_rate": 3.9370000000000004e-05,
      "loss": 0.0032,
      "step": 31890
    },
    {
      "epoch": 1.7013333333333334,
      "grad_norm": 0.35571998357772827,
      "learning_rate": 3.936666666666667e-05,
      "loss": 0.005,
      "step": 31900
    },
    {
      "epoch": 1.7018666666666666,
      "grad_norm": 0.09057597815990448,
      "learning_rate": 3.9363333333333336e-05,
      "loss": 0.003,
      "step": 31910
    },
    {
      "epoch": 1.7024,
      "grad_norm": 0.14863333106040955,
      "learning_rate": 3.936e-05,
      "loss": 0.0027,
      "step": 31920
    },
    {
      "epoch": 1.7029333333333332,
      "grad_norm": 0.4074522852897644,
      "learning_rate": 3.935666666666667e-05,
      "loss": 0.0017,
      "step": 31930
    },
    {
      "epoch": 1.7034666666666667,
      "grad_norm": 0.12013793736696243,
      "learning_rate": 3.9353333333333335e-05,
      "loss": 0.0023,
      "step": 31940
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.20441310107707977,
      "learning_rate": 3.935e-05,
      "loss": 0.0033,
      "step": 31950
    },
    {
      "epoch": 1.7045333333333335,
      "grad_norm": 0.04633313789963722,
      "learning_rate": 3.9346666666666674e-05,
      "loss": 0.0026,
      "step": 31960
    },
    {
      "epoch": 1.7050666666666667,
      "grad_norm": 0.11743596196174622,
      "learning_rate": 3.934333333333334e-05,
      "loss": 0.0024,
      "step": 31970
    },
    {
      "epoch": 1.7056,
      "grad_norm": 0.3801129460334778,
      "learning_rate": 3.9340000000000006e-05,
      "loss": 0.0028,
      "step": 31980
    },
    {
      "epoch": 1.7061333333333333,
      "grad_norm": 0.17656278610229492,
      "learning_rate": 3.9336666666666666e-05,
      "loss": 0.0035,
      "step": 31990
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.048134706914424896,
      "learning_rate": 3.933333333333333e-05,
      "loss": 0.0019,
      "step": 32000
    },
    {
      "epoch": 1.7072,
      "grad_norm": 0.4135684072971344,
      "learning_rate": 3.933e-05,
      "loss": 0.0044,
      "step": 32010
    },
    {
      "epoch": 1.7077333333333333,
      "grad_norm": 0.2728228271007538,
      "learning_rate": 3.9326666666666664e-05,
      "loss": 0.0022,
      "step": 32020
    },
    {
      "epoch": 1.7082666666666668,
      "grad_norm": 0.01993345096707344,
      "learning_rate": 3.932333333333334e-05,
      "loss": 0.0041,
      "step": 32030
    },
    {
      "epoch": 1.7088,
      "grad_norm": 0.04603530466556549,
      "learning_rate": 3.932e-05,
      "loss": 0.0027,
      "step": 32040
    },
    {
      "epoch": 1.7093333333333334,
      "grad_norm": 0.2445993423461914,
      "learning_rate": 3.931666666666667e-05,
      "loss": 0.0036,
      "step": 32050
    },
    {
      "epoch": 1.7098666666666666,
      "grad_norm": 0.127500981092453,
      "learning_rate": 3.9313333333333335e-05,
      "loss": 0.0029,
      "step": 32060
    },
    {
      "epoch": 1.7104,
      "grad_norm": 0.29202550649642944,
      "learning_rate": 3.931e-05,
      "loss": 0.0018,
      "step": 32070
    },
    {
      "epoch": 1.7109333333333332,
      "grad_norm": 0.08996617794036865,
      "learning_rate": 3.930666666666667e-05,
      "loss": 0.0045,
      "step": 32080
    },
    {
      "epoch": 1.7114666666666667,
      "grad_norm": 0.017569860443472862,
      "learning_rate": 3.9303333333333334e-05,
      "loss": 0.0029,
      "step": 32090
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.30109965801239014,
      "learning_rate": 3.9300000000000007e-05,
      "loss": 0.0033,
      "step": 32100
    },
    {
      "epoch": 1.7125333333333335,
      "grad_norm": 0.48589658737182617,
      "learning_rate": 3.929666666666667e-05,
      "loss": 0.0029,
      "step": 32110
    },
    {
      "epoch": 1.7130666666666667,
      "grad_norm": 0.5019603371620178,
      "learning_rate": 3.929333333333334e-05,
      "loss": 0.0027,
      "step": 32120
    },
    {
      "epoch": 1.7136,
      "grad_norm": 0.09045996516942978,
      "learning_rate": 3.9290000000000005e-05,
      "loss": 0.0024,
      "step": 32130
    },
    {
      "epoch": 1.7141333333333333,
      "grad_norm": 0.03200795128941536,
      "learning_rate": 3.9286666666666664e-05,
      "loss": 0.0032,
      "step": 32140
    },
    {
      "epoch": 1.7146666666666666,
      "grad_norm": 0.18068435788154602,
      "learning_rate": 3.928333333333333e-05,
      "loss": 0.0043,
      "step": 32150
    },
    {
      "epoch": 1.7151999999999998,
      "grad_norm": 0.2359945923089981,
      "learning_rate": 3.9280000000000003e-05,
      "loss": 0.0031,
      "step": 32160
    },
    {
      "epoch": 1.7157333333333333,
      "grad_norm": 0.4169887900352478,
      "learning_rate": 3.927666666666667e-05,
      "loss": 0.0044,
      "step": 32170
    },
    {
      "epoch": 1.7162666666666668,
      "grad_norm": 0.6188862323760986,
      "learning_rate": 3.9273333333333336e-05,
      "loss": 0.0033,
      "step": 32180
    },
    {
      "epoch": 1.7168,
      "grad_norm": 0.4429924786090851,
      "learning_rate": 3.927e-05,
      "loss": 0.003,
      "step": 32190
    },
    {
      "epoch": 1.7173333333333334,
      "grad_norm": 0.33202657103538513,
      "learning_rate": 3.926666666666667e-05,
      "loss": 0.0027,
      "step": 32200
    },
    {
      "epoch": 1.7178666666666667,
      "grad_norm": 0.0357472188770771,
      "learning_rate": 3.9263333333333334e-05,
      "loss": 0.0027,
      "step": 32210
    },
    {
      "epoch": 1.7184,
      "grad_norm": 0.18901561200618744,
      "learning_rate": 3.926e-05,
      "loss": 0.0028,
      "step": 32220
    },
    {
      "epoch": 1.7189333333333332,
      "grad_norm": 0.03827660530805588,
      "learning_rate": 3.9256666666666666e-05,
      "loss": 0.003,
      "step": 32230
    },
    {
      "epoch": 1.7194666666666667,
      "grad_norm": 0.26961272954940796,
      "learning_rate": 3.925333333333334e-05,
      "loss": 0.0023,
      "step": 32240
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.4499584436416626,
      "learning_rate": 3.9250000000000005e-05,
      "loss": 0.0041,
      "step": 32250
    },
    {
      "epoch": 1.7205333333333335,
      "grad_norm": 0.5032871961593628,
      "learning_rate": 3.924666666666667e-05,
      "loss": 0.0022,
      "step": 32260
    },
    {
      "epoch": 1.7210666666666667,
      "grad_norm": 0.16717146337032318,
      "learning_rate": 3.924333333333334e-05,
      "loss": 0.0053,
      "step": 32270
    },
    {
      "epoch": 1.7216,
      "grad_norm": 0.5010397434234619,
      "learning_rate": 3.9240000000000004e-05,
      "loss": 0.003,
      "step": 32280
    },
    {
      "epoch": 1.7221333333333333,
      "grad_norm": 0.26308074593544006,
      "learning_rate": 3.923666666666666e-05,
      "loss": 0.0021,
      "step": 32290
    },
    {
      "epoch": 1.7226666666666666,
      "grad_norm": 0.5284465551376343,
      "learning_rate": 3.9233333333333336e-05,
      "loss": 0.0036,
      "step": 32300
    },
    {
      "epoch": 1.7231999999999998,
      "grad_norm": 0.060650043189525604,
      "learning_rate": 3.923e-05,
      "loss": 0.0025,
      "step": 32310
    },
    {
      "epoch": 1.7237333333333333,
      "grad_norm": 0.06148955225944519,
      "learning_rate": 3.922666666666667e-05,
      "loss": 0.0024,
      "step": 32320
    },
    {
      "epoch": 1.7242666666666666,
      "grad_norm": 0.3485281467437744,
      "learning_rate": 3.9223333333333334e-05,
      "loss": 0.0029,
      "step": 32330
    },
    {
      "epoch": 1.7248,
      "grad_norm": 0.26950564980506897,
      "learning_rate": 3.922e-05,
      "loss": 0.0031,
      "step": 32340
    },
    {
      "epoch": 1.7253333333333334,
      "grad_norm": 0.1996179223060608,
      "learning_rate": 3.921666666666667e-05,
      "loss": 0.002,
      "step": 32350
    },
    {
      "epoch": 1.7258666666666667,
      "grad_norm": 0.40879517793655396,
      "learning_rate": 3.921333333333333e-05,
      "loss": 0.0029,
      "step": 32360
    },
    {
      "epoch": 1.7264,
      "grad_norm": 0.06678161025047302,
      "learning_rate": 3.921e-05,
      "loss": 0.0023,
      "step": 32370
    },
    {
      "epoch": 1.7269333333333332,
      "grad_norm": 0.30232346057891846,
      "learning_rate": 3.920666666666667e-05,
      "loss": 0.0024,
      "step": 32380
    },
    {
      "epoch": 1.7274666666666667,
      "grad_norm": 0.5902805924415588,
      "learning_rate": 3.920333333333334e-05,
      "loss": 0.003,
      "step": 32390
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.09144186973571777,
      "learning_rate": 3.9200000000000004e-05,
      "loss": 0.0028,
      "step": 32400
    },
    {
      "epoch": 1.7285333333333335,
      "grad_norm": 0.1749262511730194,
      "learning_rate": 3.919666666666667e-05,
      "loss": 0.0026,
      "step": 32410
    },
    {
      "epoch": 1.7290666666666668,
      "grad_norm": 0.14621451497077942,
      "learning_rate": 3.9193333333333336e-05,
      "loss": 0.0043,
      "step": 32420
    },
    {
      "epoch": 1.7296,
      "grad_norm": 0.554125189781189,
      "learning_rate": 3.919e-05,
      "loss": 0.0032,
      "step": 32430
    },
    {
      "epoch": 1.7301333333333333,
      "grad_norm": 0.3485422730445862,
      "learning_rate": 3.918666666666667e-05,
      "loss": 0.0029,
      "step": 32440
    },
    {
      "epoch": 1.7306666666666666,
      "grad_norm": 0.12889203429222107,
      "learning_rate": 3.9183333333333335e-05,
      "loss": 0.0036,
      "step": 32450
    },
    {
      "epoch": 1.7311999999999999,
      "grad_norm": 0.2938287854194641,
      "learning_rate": 3.918e-05,
      "loss": 0.0036,
      "step": 32460
    },
    {
      "epoch": 1.7317333333333333,
      "grad_norm": 0.0621764212846756,
      "learning_rate": 3.917666666666667e-05,
      "loss": 0.0022,
      "step": 32470
    },
    {
      "epoch": 1.7322666666666666,
      "grad_norm": 0.0635729655623436,
      "learning_rate": 3.917333333333333e-05,
      "loss": 0.0029,
      "step": 32480
    },
    {
      "epoch": 1.7328000000000001,
      "grad_norm": 0.5207479000091553,
      "learning_rate": 3.917e-05,
      "loss": 0.0041,
      "step": 32490
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.819058895111084,
      "learning_rate": 3.9166666666666665e-05,
      "loss": 0.0025,
      "step": 32500
    },
    {
      "epoch": 1.7338666666666667,
      "grad_norm": 0.18604087829589844,
      "learning_rate": 3.916333333333334e-05,
      "loss": 0.0029,
      "step": 32510
    },
    {
      "epoch": 1.7344,
      "grad_norm": 0.07406605780124664,
      "learning_rate": 3.9160000000000005e-05,
      "loss": 0.0028,
      "step": 32520
    },
    {
      "epoch": 1.7349333333333332,
      "grad_norm": 0.3845943212509155,
      "learning_rate": 3.915666666666667e-05,
      "loss": 0.0027,
      "step": 32530
    },
    {
      "epoch": 1.7354666666666667,
      "grad_norm": 0.2693963646888733,
      "learning_rate": 3.915333333333334e-05,
      "loss": 0.0027,
      "step": 32540
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.14896133542060852,
      "learning_rate": 3.915e-05,
      "loss": 0.0033,
      "step": 32550
    },
    {
      "epoch": 1.7365333333333335,
      "grad_norm": 0.3233923316001892,
      "learning_rate": 3.914666666666667e-05,
      "loss": 0.0029,
      "step": 32560
    },
    {
      "epoch": 1.7370666666666668,
      "grad_norm": 0.03735384717583656,
      "learning_rate": 3.9143333333333335e-05,
      "loss": 0.0025,
      "step": 32570
    },
    {
      "epoch": 1.7376,
      "grad_norm": 0.18394505977630615,
      "learning_rate": 3.914e-05,
      "loss": 0.0027,
      "step": 32580
    },
    {
      "epoch": 1.7381333333333333,
      "grad_norm": 0.14848874509334564,
      "learning_rate": 3.913666666666667e-05,
      "loss": 0.0031,
      "step": 32590
    },
    {
      "epoch": 1.7386666666666666,
      "grad_norm": 0.10615407675504684,
      "learning_rate": 3.9133333333333334e-05,
      "loss": 0.003,
      "step": 32600
    },
    {
      "epoch": 1.7391999999999999,
      "grad_norm": 0.09200184792280197,
      "learning_rate": 3.913e-05,
      "loss": 0.003,
      "step": 32610
    },
    {
      "epoch": 1.7397333333333334,
      "grad_norm": 0.25038450956344604,
      "learning_rate": 3.9126666666666666e-05,
      "loss": 0.0021,
      "step": 32620
    },
    {
      "epoch": 1.7402666666666666,
      "grad_norm": 0.20544631779193878,
      "learning_rate": 3.912333333333333e-05,
      "loss": 0.0027,
      "step": 32630
    },
    {
      "epoch": 1.7408000000000001,
      "grad_norm": 0.24144120514392853,
      "learning_rate": 3.912e-05,
      "loss": 0.0029,
      "step": 32640
    },
    {
      "epoch": 1.7413333333333334,
      "grad_norm": 0.034225787967443466,
      "learning_rate": 3.911666666666667e-05,
      "loss": 0.0039,
      "step": 32650
    },
    {
      "epoch": 1.7418666666666667,
      "grad_norm": 0.04136396944522858,
      "learning_rate": 3.911333333333334e-05,
      "loss": 0.002,
      "step": 32660
    },
    {
      "epoch": 1.7424,
      "grad_norm": 0.18262457847595215,
      "learning_rate": 3.911e-05,
      "loss": 0.0025,
      "step": 32670
    },
    {
      "epoch": 1.7429333333333332,
      "grad_norm": 0.6569504141807556,
      "learning_rate": 3.910666666666667e-05,
      "loss": 0.0024,
      "step": 32680
    },
    {
      "epoch": 1.7434666666666667,
      "grad_norm": 0.35552942752838135,
      "learning_rate": 3.9103333333333336e-05,
      "loss": 0.0026,
      "step": 32690
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.47264522314071655,
      "learning_rate": 3.91e-05,
      "loss": 0.003,
      "step": 32700
    },
    {
      "epoch": 1.7445333333333335,
      "grad_norm": 0.41150668263435364,
      "learning_rate": 3.909666666666667e-05,
      "loss": 0.003,
      "step": 32710
    },
    {
      "epoch": 1.7450666666666668,
      "grad_norm": 0.36097484827041626,
      "learning_rate": 3.9093333333333334e-05,
      "loss": 0.0029,
      "step": 32720
    },
    {
      "epoch": 1.7456,
      "grad_norm": 0.1818832904100418,
      "learning_rate": 3.909000000000001e-05,
      "loss": 0.0021,
      "step": 32730
    },
    {
      "epoch": 1.7461333333333333,
      "grad_norm": 0.3319016993045807,
      "learning_rate": 3.9086666666666666e-05,
      "loss": 0.0027,
      "step": 32740
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 0.7117671370506287,
      "learning_rate": 3.908333333333333e-05,
      "loss": 0.0022,
      "step": 32750
    },
    {
      "epoch": 1.7471999999999999,
      "grad_norm": 0.5659622550010681,
      "learning_rate": 3.908e-05,
      "loss": 0.0028,
      "step": 32760
    },
    {
      "epoch": 1.7477333333333334,
      "grad_norm": 0.059307318180799484,
      "learning_rate": 3.9076666666666665e-05,
      "loss": 0.0043,
      "step": 32770
    },
    {
      "epoch": 1.7482666666666666,
      "grad_norm": 0.4361051619052887,
      "learning_rate": 3.907333333333333e-05,
      "loss": 0.0036,
      "step": 32780
    },
    {
      "epoch": 1.7488000000000001,
      "grad_norm": 0.06288550049066544,
      "learning_rate": 3.9070000000000004e-05,
      "loss": 0.0046,
      "step": 32790
    },
    {
      "epoch": 1.7493333333333334,
      "grad_norm": 0.2550314962863922,
      "learning_rate": 3.906666666666667e-05,
      "loss": 0.003,
      "step": 32800
    },
    {
      "epoch": 1.7498666666666667,
      "grad_norm": 0.7912256717681885,
      "learning_rate": 3.9063333333333336e-05,
      "loss": 0.0041,
      "step": 32810
    },
    {
      "epoch": 1.7504,
      "grad_norm": 0.15132136642932892,
      "learning_rate": 3.906e-05,
      "loss": 0.0036,
      "step": 32820
    },
    {
      "epoch": 1.7509333333333332,
      "grad_norm": 0.23424138128757477,
      "learning_rate": 3.905666666666667e-05,
      "loss": 0.003,
      "step": 32830
    },
    {
      "epoch": 1.7514666666666665,
      "grad_norm": 0.07801690697669983,
      "learning_rate": 3.9053333333333334e-05,
      "loss": 0.0021,
      "step": 32840
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.4134838581085205,
      "learning_rate": 3.905e-05,
      "loss": 0.0031,
      "step": 32850
    },
    {
      "epoch": 1.7525333333333335,
      "grad_norm": 0.530122697353363,
      "learning_rate": 3.9046666666666673e-05,
      "loss": 0.0036,
      "step": 32860
    },
    {
      "epoch": 1.7530666666666668,
      "grad_norm": 0.6535094380378723,
      "learning_rate": 3.904333333333334e-05,
      "loss": 0.0022,
      "step": 32870
    },
    {
      "epoch": 1.7536,
      "grad_norm": 0.330003559589386,
      "learning_rate": 3.9040000000000006e-05,
      "loss": 0.0029,
      "step": 32880
    },
    {
      "epoch": 1.7541333333333333,
      "grad_norm": 0.33154594898223877,
      "learning_rate": 3.9036666666666665e-05,
      "loss": 0.0025,
      "step": 32890
    },
    {
      "epoch": 1.7546666666666666,
      "grad_norm": 0.34995174407958984,
      "learning_rate": 3.903333333333333e-05,
      "loss": 0.0025,
      "step": 32900
    },
    {
      "epoch": 1.7551999999999999,
      "grad_norm": 0.24142296612262726,
      "learning_rate": 3.903e-05,
      "loss": 0.0025,
      "step": 32910
    },
    {
      "epoch": 1.7557333333333334,
      "grad_norm": 0.41310426592826843,
      "learning_rate": 3.902666666666667e-05,
      "loss": 0.0021,
      "step": 32920
    },
    {
      "epoch": 1.7562666666666666,
      "grad_norm": 0.6633825898170471,
      "learning_rate": 3.9023333333333336e-05,
      "loss": 0.0024,
      "step": 32930
    },
    {
      "epoch": 1.7568000000000001,
      "grad_norm": 0.21207474172115326,
      "learning_rate": 3.902e-05,
      "loss": 0.0029,
      "step": 32940
    },
    {
      "epoch": 1.7573333333333334,
      "grad_norm": 0.18271391093730927,
      "learning_rate": 3.901666666666667e-05,
      "loss": 0.0033,
      "step": 32950
    },
    {
      "epoch": 1.7578666666666667,
      "grad_norm": 0.4023648798465729,
      "learning_rate": 3.9013333333333335e-05,
      "loss": 0.0031,
      "step": 32960
    },
    {
      "epoch": 1.7584,
      "grad_norm": 0.11672956496477127,
      "learning_rate": 3.901e-05,
      "loss": 0.0015,
      "step": 32970
    },
    {
      "epoch": 1.7589333333333332,
      "grad_norm": 0.09353076666593552,
      "learning_rate": 3.900666666666667e-05,
      "loss": 0.0025,
      "step": 32980
    },
    {
      "epoch": 1.7594666666666665,
      "grad_norm": 0.33423975110054016,
      "learning_rate": 3.900333333333333e-05,
      "loss": 0.0023,
      "step": 32990
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.40935400128364563,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.0027,
      "step": 33000
    },
    {
      "epoch": 1.7605333333333333,
      "grad_norm": 0.11689833551645279,
      "learning_rate": 3.899666666666667e-05,
      "loss": 0.0042,
      "step": 33010
    },
    {
      "epoch": 1.7610666666666668,
      "grad_norm": 0.3667214512825012,
      "learning_rate": 3.899333333333334e-05,
      "loss": 0.0025,
      "step": 33020
    },
    {
      "epoch": 1.7616,
      "grad_norm": 0.32676073908805847,
      "learning_rate": 3.8990000000000004e-05,
      "loss": 0.0032,
      "step": 33030
    },
    {
      "epoch": 1.7621333333333333,
      "grad_norm": 0.32430827617645264,
      "learning_rate": 3.8986666666666664e-05,
      "loss": 0.0034,
      "step": 33040
    },
    {
      "epoch": 1.7626666666666666,
      "grad_norm": 0.15452444553375244,
      "learning_rate": 3.898333333333333e-05,
      "loss": 0.0018,
      "step": 33050
    },
    {
      "epoch": 1.7631999999999999,
      "grad_norm": 0.2967698872089386,
      "learning_rate": 3.898e-05,
      "loss": 0.004,
      "step": 33060
    },
    {
      "epoch": 1.7637333333333334,
      "grad_norm": 0.042185086756944656,
      "learning_rate": 3.897666666666667e-05,
      "loss": 0.0039,
      "step": 33070
    },
    {
      "epoch": 1.7642666666666666,
      "grad_norm": 0.6295303106307983,
      "learning_rate": 3.8973333333333335e-05,
      "loss": 0.0028,
      "step": 33080
    },
    {
      "epoch": 1.7648000000000001,
      "grad_norm": 0.09348923712968826,
      "learning_rate": 3.897e-05,
      "loss": 0.0038,
      "step": 33090
    },
    {
      "epoch": 1.7653333333333334,
      "grad_norm": 0.3279606103897095,
      "learning_rate": 3.896666666666667e-05,
      "loss": 0.0026,
      "step": 33100
    },
    {
      "epoch": 1.7658666666666667,
      "grad_norm": 0.7625793814659119,
      "learning_rate": 3.8963333333333334e-05,
      "loss": 0.0037,
      "step": 33110
    },
    {
      "epoch": 1.7664,
      "grad_norm": 0.2438664585351944,
      "learning_rate": 3.896e-05,
      "loss": 0.0026,
      "step": 33120
    },
    {
      "epoch": 1.7669333333333332,
      "grad_norm": 0.5580410361289978,
      "learning_rate": 3.8956666666666666e-05,
      "loss": 0.0021,
      "step": 33130
    },
    {
      "epoch": 1.7674666666666665,
      "grad_norm": 0.03743521869182587,
      "learning_rate": 3.895333333333334e-05,
      "loss": 0.0028,
      "step": 33140
    },
    {
      "epoch": 1.768,
      "grad_norm": 0.14742986857891083,
      "learning_rate": 3.8950000000000005e-05,
      "loss": 0.0031,
      "step": 33150
    },
    {
      "epoch": 1.7685333333333333,
      "grad_norm": 0.0678887739777565,
      "learning_rate": 3.894666666666667e-05,
      "loss": 0.0026,
      "step": 33160
    },
    {
      "epoch": 1.7690666666666668,
      "grad_norm": 0.17619580030441284,
      "learning_rate": 3.894333333333334e-05,
      "loss": 0.003,
      "step": 33170
    },
    {
      "epoch": 1.7696,
      "grad_norm": 0.05954498425126076,
      "learning_rate": 3.894e-05,
      "loss": 0.0032,
      "step": 33180
    },
    {
      "epoch": 1.7701333333333333,
      "grad_norm": 0.15268932282924652,
      "learning_rate": 3.893666666666667e-05,
      "loss": 0.0023,
      "step": 33190
    },
    {
      "epoch": 1.7706666666666666,
      "grad_norm": 0.14942707121372223,
      "learning_rate": 3.8933333333333336e-05,
      "loss": 0.002,
      "step": 33200
    },
    {
      "epoch": 1.7711999999999999,
      "grad_norm": 0.14734448492527008,
      "learning_rate": 3.893e-05,
      "loss": 0.0023,
      "step": 33210
    },
    {
      "epoch": 1.7717333333333334,
      "grad_norm": 0.11906680464744568,
      "learning_rate": 3.892666666666667e-05,
      "loss": 0.0029,
      "step": 33220
    },
    {
      "epoch": 1.7722666666666667,
      "grad_norm": 0.0666891559958458,
      "learning_rate": 3.8923333333333334e-05,
      "loss": 0.0023,
      "step": 33230
    },
    {
      "epoch": 1.7728000000000002,
      "grad_norm": 0.6776286363601685,
      "learning_rate": 3.892e-05,
      "loss": 0.0024,
      "step": 33240
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 0.20345675945281982,
      "learning_rate": 3.8916666666666666e-05,
      "loss": 0.0035,
      "step": 33250
    },
    {
      "epoch": 1.7738666666666667,
      "grad_norm": 0.1194484606385231,
      "learning_rate": 3.891333333333333e-05,
      "loss": 0.0035,
      "step": 33260
    },
    {
      "epoch": 1.7744,
      "grad_norm": 0.4004688560962677,
      "learning_rate": 3.8910000000000005e-05,
      "loss": 0.0027,
      "step": 33270
    },
    {
      "epoch": 1.7749333333333333,
      "grad_norm": 0.3228995203971863,
      "learning_rate": 3.890666666666667e-05,
      "loss": 0.0037,
      "step": 33280
    },
    {
      "epoch": 1.7754666666666665,
      "grad_norm": 0.2617869973182678,
      "learning_rate": 3.890333333333334e-05,
      "loss": 0.0035,
      "step": 33290
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.3893297612667084,
      "learning_rate": 3.8900000000000004e-05,
      "loss": 0.0032,
      "step": 33300
    },
    {
      "epoch": 1.7765333333333333,
      "grad_norm": 0.276417076587677,
      "learning_rate": 3.889666666666667e-05,
      "loss": 0.0032,
      "step": 33310
    },
    {
      "epoch": 1.7770666666666668,
      "grad_norm": 0.3891517221927643,
      "learning_rate": 3.8893333333333336e-05,
      "loss": 0.0031,
      "step": 33320
    },
    {
      "epoch": 1.7776,
      "grad_norm": 0.15073639154434204,
      "learning_rate": 3.889e-05,
      "loss": 0.0024,
      "step": 33330
    },
    {
      "epoch": 1.7781333333333333,
      "grad_norm": 0.23442544043064117,
      "learning_rate": 3.888666666666667e-05,
      "loss": 0.0026,
      "step": 33340
    },
    {
      "epoch": 1.7786666666666666,
      "grad_norm": 0.059260305017232895,
      "learning_rate": 3.8883333333333334e-05,
      "loss": 0.0023,
      "step": 33350
    },
    {
      "epoch": 1.7792,
      "grad_norm": 0.20596012473106384,
      "learning_rate": 3.888e-05,
      "loss": 0.0018,
      "step": 33360
    },
    {
      "epoch": 1.7797333333333332,
      "grad_norm": 0.14760823547840118,
      "learning_rate": 3.8876666666666667e-05,
      "loss": 0.0033,
      "step": 33370
    },
    {
      "epoch": 1.7802666666666667,
      "grad_norm": 0.29472121596336365,
      "learning_rate": 3.887333333333333e-05,
      "loss": 0.0022,
      "step": 33380
    },
    {
      "epoch": 1.7808000000000002,
      "grad_norm": 0.2385168820619583,
      "learning_rate": 3.887e-05,
      "loss": 0.002,
      "step": 33390
    },
    {
      "epoch": 1.7813333333333334,
      "grad_norm": 0.2656722068786621,
      "learning_rate": 3.8866666666666665e-05,
      "loss": 0.0031,
      "step": 33400
    },
    {
      "epoch": 1.7818666666666667,
      "grad_norm": 0.15271438658237457,
      "learning_rate": 3.886333333333334e-05,
      "loss": 0.0029,
      "step": 33410
    },
    {
      "epoch": 1.7824,
      "grad_norm": 0.27310892939567566,
      "learning_rate": 3.8860000000000004e-05,
      "loss": 0.0026,
      "step": 33420
    },
    {
      "epoch": 1.7829333333333333,
      "grad_norm": 0.0878550112247467,
      "learning_rate": 3.885666666666667e-05,
      "loss": 0.0032,
      "step": 33430
    },
    {
      "epoch": 1.7834666666666665,
      "grad_norm": 0.1476992964744568,
      "learning_rate": 3.8853333333333336e-05,
      "loss": 0.0025,
      "step": 33440
    },
    {
      "epoch": 1.784,
      "grad_norm": 0.5214108228683472,
      "learning_rate": 3.885e-05,
      "loss": 0.0032,
      "step": 33450
    },
    {
      "epoch": 1.7845333333333333,
      "grad_norm": 0.5595709085464478,
      "learning_rate": 3.884666666666667e-05,
      "loss": 0.0033,
      "step": 33460
    },
    {
      "epoch": 1.7850666666666668,
      "grad_norm": 0.4923545718193054,
      "learning_rate": 3.8843333333333335e-05,
      "loss": 0.0029,
      "step": 33470
    },
    {
      "epoch": 1.7856,
      "grad_norm": 0.20304493606090546,
      "learning_rate": 3.884e-05,
      "loss": 0.002,
      "step": 33480
    },
    {
      "epoch": 1.7861333333333334,
      "grad_norm": 0.26138490438461304,
      "learning_rate": 3.8836666666666674e-05,
      "loss": 0.0029,
      "step": 33490
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 0.14748957753181458,
      "learning_rate": 3.883333333333333e-05,
      "loss": 0.0019,
      "step": 33500
    },
    {
      "epoch": 1.7872,
      "grad_norm": 0.27147969603538513,
      "learning_rate": 3.883e-05,
      "loss": 0.0037,
      "step": 33510
    },
    {
      "epoch": 1.7877333333333332,
      "grad_norm": 0.061213601380586624,
      "learning_rate": 3.8826666666666665e-05,
      "loss": 0.0017,
      "step": 33520
    },
    {
      "epoch": 1.7882666666666667,
      "grad_norm": 0.15447814762592316,
      "learning_rate": 3.882333333333333e-05,
      "loss": 0.0019,
      "step": 33530
    },
    {
      "epoch": 1.7888,
      "grad_norm": 0.065968818962574,
      "learning_rate": 3.882e-05,
      "loss": 0.003,
      "step": 33540
    },
    {
      "epoch": 1.7893333333333334,
      "grad_norm": 0.24050144851207733,
      "learning_rate": 3.881666666666667e-05,
      "loss": 0.0019,
      "step": 33550
    },
    {
      "epoch": 1.7898666666666667,
      "grad_norm": 0.46839165687561035,
      "learning_rate": 3.881333333333334e-05,
      "loss": 0.0029,
      "step": 33560
    },
    {
      "epoch": 1.7904,
      "grad_norm": 0.2039153128862381,
      "learning_rate": 3.881e-05,
      "loss": 0.0028,
      "step": 33570
    },
    {
      "epoch": 1.7909333333333333,
      "grad_norm": 0.2238190472126007,
      "learning_rate": 3.880666666666667e-05,
      "loss": 0.0045,
      "step": 33580
    },
    {
      "epoch": 1.7914666666666665,
      "grad_norm": 0.15886317193508148,
      "learning_rate": 3.8803333333333335e-05,
      "loss": 0.0035,
      "step": 33590
    },
    {
      "epoch": 1.792,
      "grad_norm": 0.7425244450569153,
      "learning_rate": 3.88e-05,
      "loss": 0.0031,
      "step": 33600
    },
    {
      "epoch": 1.7925333333333333,
      "grad_norm": 0.062242474406957626,
      "learning_rate": 3.879666666666667e-05,
      "loss": 0.0025,
      "step": 33610
    },
    {
      "epoch": 1.7930666666666668,
      "grad_norm": 0.14925546944141388,
      "learning_rate": 3.879333333333334e-05,
      "loss": 0.0021,
      "step": 33620
    },
    {
      "epoch": 1.7936,
      "grad_norm": 0.22995153069496155,
      "learning_rate": 3.8790000000000006e-05,
      "loss": 0.0026,
      "step": 33630
    },
    {
      "epoch": 1.7941333333333334,
      "grad_norm": 0.2833295464515686,
      "learning_rate": 3.878666666666667e-05,
      "loss": 0.0025,
      "step": 33640
    },
    {
      "epoch": 1.7946666666666666,
      "grad_norm": 0.2632463276386261,
      "learning_rate": 3.878333333333333e-05,
      "loss": 0.0024,
      "step": 33650
    },
    {
      "epoch": 1.7952,
      "grad_norm": 0.2983798086643219,
      "learning_rate": 3.878e-05,
      "loss": 0.0029,
      "step": 33660
    },
    {
      "epoch": 1.7957333333333332,
      "grad_norm": 0.09062748402357101,
      "learning_rate": 3.8776666666666664e-05,
      "loss": 0.0027,
      "step": 33670
    },
    {
      "epoch": 1.7962666666666667,
      "grad_norm": 0.17664113640785217,
      "learning_rate": 3.877333333333334e-05,
      "loss": 0.003,
      "step": 33680
    },
    {
      "epoch": 1.7968,
      "grad_norm": 0.674004852771759,
      "learning_rate": 3.877e-05,
      "loss": 0.0029,
      "step": 33690
    },
    {
      "epoch": 1.7973333333333334,
      "grad_norm": 0.26970699429512024,
      "learning_rate": 3.876666666666667e-05,
      "loss": 0.0025,
      "step": 33700
    },
    {
      "epoch": 1.7978666666666667,
      "grad_norm": 0.36130291223526,
      "learning_rate": 3.8763333333333335e-05,
      "loss": 0.0029,
      "step": 33710
    },
    {
      "epoch": 1.7984,
      "grad_norm": 0.09551893919706345,
      "learning_rate": 3.876e-05,
      "loss": 0.002,
      "step": 33720
    },
    {
      "epoch": 1.7989333333333333,
      "grad_norm": 0.17608042061328888,
      "learning_rate": 3.875666666666667e-05,
      "loss": 0.0023,
      "step": 33730
    },
    {
      "epoch": 1.7994666666666665,
      "grad_norm": 0.09815125167369843,
      "learning_rate": 3.8753333333333334e-05,
      "loss": 0.0028,
      "step": 33740
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.20899707078933716,
      "learning_rate": 3.875e-05,
      "loss": 0.002,
      "step": 33750
    },
    {
      "epoch": 1.8005333333333333,
      "grad_norm": 0.12023986876010895,
      "learning_rate": 3.874666666666667e-05,
      "loss": 0.0016,
      "step": 33760
    },
    {
      "epoch": 1.8010666666666668,
      "grad_norm": 0.43615230917930603,
      "learning_rate": 3.874333333333334e-05,
      "loss": 0.0034,
      "step": 33770
    },
    {
      "epoch": 1.8016,
      "grad_norm": 0.11695624887943268,
      "learning_rate": 3.8740000000000005e-05,
      "loss": 0.0046,
      "step": 33780
    },
    {
      "epoch": 1.8021333333333334,
      "grad_norm": 0.3860633969306946,
      "learning_rate": 3.873666666666667e-05,
      "loss": 0.0031,
      "step": 33790
    },
    {
      "epoch": 1.8026666666666666,
      "grad_norm": 0.270864874124527,
      "learning_rate": 3.873333333333333e-05,
      "loss": 0.003,
      "step": 33800
    },
    {
      "epoch": 1.8032,
      "grad_norm": 0.2393476814031601,
      "learning_rate": 3.873e-05,
      "loss": 0.0034,
      "step": 33810
    },
    {
      "epoch": 1.8037333333333332,
      "grad_norm": 0.35098427534103394,
      "learning_rate": 3.872666666666667e-05,
      "loss": 0.0038,
      "step": 33820
    },
    {
      "epoch": 1.8042666666666667,
      "grad_norm": 0.4458148777484894,
      "learning_rate": 3.8723333333333336e-05,
      "loss": 0.0024,
      "step": 33830
    },
    {
      "epoch": 1.8048,
      "grad_norm": 0.5604214668273926,
      "learning_rate": 3.872e-05,
      "loss": 0.0028,
      "step": 33840
    },
    {
      "epoch": 1.8053333333333335,
      "grad_norm": 0.14600376784801483,
      "learning_rate": 3.871666666666667e-05,
      "loss": 0.0033,
      "step": 33850
    },
    {
      "epoch": 1.8058666666666667,
      "grad_norm": 0.021705223247408867,
      "learning_rate": 3.8713333333333334e-05,
      "loss": 0.0028,
      "step": 33860
    },
    {
      "epoch": 1.8064,
      "grad_norm": 0.20440314710140228,
      "learning_rate": 3.871e-05,
      "loss": 0.0028,
      "step": 33870
    },
    {
      "epoch": 1.8069333333333333,
      "grad_norm": 0.028622319921851158,
      "learning_rate": 3.8706666666666667e-05,
      "loss": 0.0025,
      "step": 33880
    },
    {
      "epoch": 1.8074666666666666,
      "grad_norm": 0.14898130297660828,
      "learning_rate": 3.870333333333333e-05,
      "loss": 0.0019,
      "step": 33890
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.3520815372467041,
      "learning_rate": 3.8700000000000006e-05,
      "loss": 0.0033,
      "step": 33900
    },
    {
      "epoch": 1.8085333333333333,
      "grad_norm": 0.4955770969390869,
      "learning_rate": 3.869666666666667e-05,
      "loss": 0.0038,
      "step": 33910
    },
    {
      "epoch": 1.8090666666666668,
      "grad_norm": 0.23125813901424408,
      "learning_rate": 3.869333333333334e-05,
      "loss": 0.0024,
      "step": 33920
    },
    {
      "epoch": 1.8096,
      "grad_norm": 0.4476836323738098,
      "learning_rate": 3.8690000000000004e-05,
      "loss": 0.0026,
      "step": 33930
    },
    {
      "epoch": 1.8101333333333334,
      "grad_norm": 0.4122427701950073,
      "learning_rate": 3.868666666666667e-05,
      "loss": 0.0021,
      "step": 33940
    },
    {
      "epoch": 1.8106666666666666,
      "grad_norm": 0.12829160690307617,
      "learning_rate": 3.868333333333333e-05,
      "loss": 0.0027,
      "step": 33950
    },
    {
      "epoch": 1.8112,
      "grad_norm": 0.14773188531398773,
      "learning_rate": 3.868e-05,
      "loss": 0.0034,
      "step": 33960
    },
    {
      "epoch": 1.8117333333333332,
      "grad_norm": 0.04934338480234146,
      "learning_rate": 3.867666666666667e-05,
      "loss": 0.0027,
      "step": 33970
    },
    {
      "epoch": 1.8122666666666667,
      "grad_norm": 0.14792083203792572,
      "learning_rate": 3.8673333333333335e-05,
      "loss": 0.0024,
      "step": 33980
    },
    {
      "epoch": 1.8128,
      "grad_norm": 0.21168944239616394,
      "learning_rate": 3.867e-05,
      "loss": 0.0042,
      "step": 33990
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 0.4156664311885834,
      "learning_rate": 3.866666666666667e-05,
      "loss": 0.0029,
      "step": 34000
    },
    {
      "epoch": 1.8138666666666667,
      "grad_norm": 0.26318520307540894,
      "learning_rate": 3.866333333333333e-05,
      "loss": 0.0031,
      "step": 34010
    },
    {
      "epoch": 1.8144,
      "grad_norm": 0.016222011297941208,
      "learning_rate": 3.866e-05,
      "loss": 0.003,
      "step": 34020
    },
    {
      "epoch": 1.8149333333333333,
      "grad_norm": 0.32484713196754456,
      "learning_rate": 3.865666666666667e-05,
      "loss": 0.0016,
      "step": 34030
    },
    {
      "epoch": 1.8154666666666666,
      "grad_norm": 0.2338584065437317,
      "learning_rate": 3.865333333333334e-05,
      "loss": 0.0039,
      "step": 34040
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 0.03757035359740257,
      "learning_rate": 3.8650000000000004e-05,
      "loss": 0.0019,
      "step": 34050
    },
    {
      "epoch": 1.8165333333333333,
      "grad_norm": 0.05902944877743721,
      "learning_rate": 3.864666666666667e-05,
      "loss": 0.0038,
      "step": 34060
    },
    {
      "epoch": 1.8170666666666668,
      "grad_norm": 0.28956568241119385,
      "learning_rate": 3.8643333333333337e-05,
      "loss": 0.0019,
      "step": 34070
    },
    {
      "epoch": 1.8176,
      "grad_norm": 0.5301046371459961,
      "learning_rate": 3.864e-05,
      "loss": 0.0024,
      "step": 34080
    },
    {
      "epoch": 1.8181333333333334,
      "grad_norm": 0.581890881061554,
      "learning_rate": 3.863666666666667e-05,
      "loss": 0.0034,
      "step": 34090
    },
    {
      "epoch": 1.8186666666666667,
      "grad_norm": 0.04485737904906273,
      "learning_rate": 3.8633333333333335e-05,
      "loss": 0.0037,
      "step": 34100
    },
    {
      "epoch": 1.8192,
      "grad_norm": 0.17409475147724152,
      "learning_rate": 3.863e-05,
      "loss": 0.0019,
      "step": 34110
    },
    {
      "epoch": 1.8197333333333332,
      "grad_norm": 0.2432115525007248,
      "learning_rate": 3.862666666666667e-05,
      "loss": 0.0042,
      "step": 34120
    },
    {
      "epoch": 1.8202666666666667,
      "grad_norm": 0.036716803908348083,
      "learning_rate": 3.8623333333333333e-05,
      "loss": 0.0024,
      "step": 34130
    },
    {
      "epoch": 1.8208,
      "grad_norm": 0.15532730519771576,
      "learning_rate": 3.862e-05,
      "loss": 0.0037,
      "step": 34140
    },
    {
      "epoch": 1.8213333333333335,
      "grad_norm": 0.14782953262329102,
      "learning_rate": 3.8616666666666666e-05,
      "loss": 0.0035,
      "step": 34150
    },
    {
      "epoch": 1.8218666666666667,
      "grad_norm": 0.2965363562107086,
      "learning_rate": 3.861333333333333e-05,
      "loss": 0.0038,
      "step": 34160
    },
    {
      "epoch": 1.8224,
      "grad_norm": 0.22690074145793915,
      "learning_rate": 3.8610000000000005e-05,
      "loss": 0.0022,
      "step": 34170
    },
    {
      "epoch": 1.8229333333333333,
      "grad_norm": 0.7250703573226929,
      "learning_rate": 3.860666666666667e-05,
      "loss": 0.0024,
      "step": 34180
    },
    {
      "epoch": 1.8234666666666666,
      "grad_norm": 0.7634245753288269,
      "learning_rate": 3.860333333333334e-05,
      "loss": 0.0025,
      "step": 34190
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.5121908187866211,
      "learning_rate": 3.86e-05,
      "loss": 0.0024,
      "step": 34200
    },
    {
      "epoch": 1.8245333333333333,
      "grad_norm": 0.38648873567581177,
      "learning_rate": 3.859666666666667e-05,
      "loss": 0.0024,
      "step": 34210
    },
    {
      "epoch": 1.8250666666666666,
      "grad_norm": 0.385782927274704,
      "learning_rate": 3.8593333333333335e-05,
      "loss": 0.0038,
      "step": 34220
    },
    {
      "epoch": 1.8256000000000001,
      "grad_norm": 0.12929986417293549,
      "learning_rate": 3.859e-05,
      "loss": 0.003,
      "step": 34230
    },
    {
      "epoch": 1.8261333333333334,
      "grad_norm": 0.3290552496910095,
      "learning_rate": 3.858666666666667e-05,
      "loss": 0.0027,
      "step": 34240
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 0.14820992946624756,
      "learning_rate": 3.8583333333333334e-05,
      "loss": 0.0028,
      "step": 34250
    },
    {
      "epoch": 1.8272,
      "grad_norm": 0.31732651591300964,
      "learning_rate": 3.858e-05,
      "loss": 0.003,
      "step": 34260
    },
    {
      "epoch": 1.8277333333333332,
      "grad_norm": 0.33349642157554626,
      "learning_rate": 3.8576666666666666e-05,
      "loss": 0.0038,
      "step": 34270
    },
    {
      "epoch": 1.8282666666666667,
      "grad_norm": 0.5620240569114685,
      "learning_rate": 3.857333333333333e-05,
      "loss": 0.0045,
      "step": 34280
    },
    {
      "epoch": 1.8288,
      "grad_norm": 0.26993465423583984,
      "learning_rate": 3.857e-05,
      "loss": 0.0033,
      "step": 34290
    },
    {
      "epoch": 1.8293333333333335,
      "grad_norm": 0.1613599956035614,
      "learning_rate": 3.8566666666666664e-05,
      "loss": 0.0028,
      "step": 34300
    },
    {
      "epoch": 1.8298666666666668,
      "grad_norm": 0.3221013844013214,
      "learning_rate": 3.856333333333334e-05,
      "loss": 0.0033,
      "step": 34310
    },
    {
      "epoch": 1.8304,
      "grad_norm": 0.5229761600494385,
      "learning_rate": 3.8560000000000004e-05,
      "loss": 0.0023,
      "step": 34320
    },
    {
      "epoch": 1.8309333333333333,
      "grad_norm": 0.47750726342201233,
      "learning_rate": 3.855666666666667e-05,
      "loss": 0.0037,
      "step": 34330
    },
    {
      "epoch": 1.8314666666666666,
      "grad_norm": 0.2631513178348541,
      "learning_rate": 3.8553333333333336e-05,
      "loss": 0.0032,
      "step": 34340
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.09699872136116028,
      "learning_rate": 3.855e-05,
      "loss": 0.0032,
      "step": 34350
    },
    {
      "epoch": 1.8325333333333333,
      "grad_norm": 0.15233755111694336,
      "learning_rate": 3.854666666666667e-05,
      "loss": 0.0028,
      "step": 34360
    },
    {
      "epoch": 1.8330666666666666,
      "grad_norm": 0.03847586736083031,
      "learning_rate": 3.8543333333333334e-05,
      "loss": 0.0026,
      "step": 34370
    },
    {
      "epoch": 1.8336000000000001,
      "grad_norm": 0.1259099841117859,
      "learning_rate": 3.854000000000001e-05,
      "loss": 0.003,
      "step": 34380
    },
    {
      "epoch": 1.8341333333333334,
      "grad_norm": 0.5709381103515625,
      "learning_rate": 3.853666666666667e-05,
      "loss": 0.002,
      "step": 34390
    },
    {
      "epoch": 1.8346666666666667,
      "grad_norm": 0.6810462474822998,
      "learning_rate": 3.853333333333334e-05,
      "loss": 0.0026,
      "step": 34400
    },
    {
      "epoch": 1.8352,
      "grad_norm": 0.14823585748672485,
      "learning_rate": 3.853e-05,
      "loss": 0.0024,
      "step": 34410
    },
    {
      "epoch": 1.8357333333333332,
      "grad_norm": 0.23974649608135223,
      "learning_rate": 3.8526666666666665e-05,
      "loss": 0.0013,
      "step": 34420
    },
    {
      "epoch": 1.8362666666666667,
      "grad_norm": 0.26448187232017517,
      "learning_rate": 3.852333333333333e-05,
      "loss": 0.0032,
      "step": 34430
    },
    {
      "epoch": 1.8368,
      "grad_norm": 0.39134326577186584,
      "learning_rate": 3.8520000000000004e-05,
      "loss": 0.0033,
      "step": 34440
    },
    {
      "epoch": 1.8373333333333335,
      "grad_norm": 0.06918152421712875,
      "learning_rate": 3.851666666666667e-05,
      "loss": 0.0028,
      "step": 34450
    },
    {
      "epoch": 1.8378666666666668,
      "grad_norm": 0.10251613706350327,
      "learning_rate": 3.8513333333333336e-05,
      "loss": 0.0035,
      "step": 34460
    },
    {
      "epoch": 1.8384,
      "grad_norm": 0.40392768383026123,
      "learning_rate": 3.851e-05,
      "loss": 0.0024,
      "step": 34470
    },
    {
      "epoch": 1.8389333333333333,
      "grad_norm": 0.6421785950660706,
      "learning_rate": 3.850666666666667e-05,
      "loss": 0.0029,
      "step": 34480
    },
    {
      "epoch": 1.8394666666666666,
      "grad_norm": 0.3209201693534851,
      "learning_rate": 3.8503333333333335e-05,
      "loss": 0.0021,
      "step": 34490
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.03197203204035759,
      "learning_rate": 3.85e-05,
      "loss": 0.0027,
      "step": 34500
    },
    {
      "epoch": 1.8405333333333334,
      "grad_norm": 0.6109598278999329,
      "learning_rate": 3.849666666666667e-05,
      "loss": 0.0035,
      "step": 34510
    },
    {
      "epoch": 1.8410666666666666,
      "grad_norm": 0.5238968133926392,
      "learning_rate": 3.849333333333334e-05,
      "loss": 0.0031,
      "step": 34520
    },
    {
      "epoch": 1.8416000000000001,
      "grad_norm": 0.4632650315761566,
      "learning_rate": 3.8490000000000006e-05,
      "loss": 0.0026,
      "step": 34530
    },
    {
      "epoch": 1.8421333333333334,
      "grad_norm": 0.352984219789505,
      "learning_rate": 3.848666666666667e-05,
      "loss": 0.0037,
      "step": 34540
    },
    {
      "epoch": 1.8426666666666667,
      "grad_norm": 0.08872821182012558,
      "learning_rate": 3.848333333333334e-05,
      "loss": 0.002,
      "step": 34550
    },
    {
      "epoch": 1.8432,
      "grad_norm": 0.23664706945419312,
      "learning_rate": 3.848e-05,
      "loss": 0.0029,
      "step": 34560
    },
    {
      "epoch": 1.8437333333333332,
      "grad_norm": 0.4999908208847046,
      "learning_rate": 3.8476666666666664e-05,
      "loss": 0.0021,
      "step": 34570
    },
    {
      "epoch": 1.8442666666666667,
      "grad_norm": 0.20816732943058014,
      "learning_rate": 3.8473333333333337e-05,
      "loss": 0.0015,
      "step": 34580
    },
    {
      "epoch": 1.8448,
      "grad_norm": 0.24194374680519104,
      "learning_rate": 3.847e-05,
      "loss": 0.0033,
      "step": 34590
    },
    {
      "epoch": 1.8453333333333335,
      "grad_norm": 0.07090644538402557,
      "learning_rate": 3.846666666666667e-05,
      "loss": 0.0028,
      "step": 34600
    },
    {
      "epoch": 1.8458666666666668,
      "grad_norm": 0.3822060525417328,
      "learning_rate": 3.8463333333333335e-05,
      "loss": 0.0026,
      "step": 34610
    },
    {
      "epoch": 1.8464,
      "grad_norm": 0.08894234895706177,
      "learning_rate": 3.846e-05,
      "loss": 0.0028,
      "step": 34620
    },
    {
      "epoch": 1.8469333333333333,
      "grad_norm": 0.40076467394828796,
      "learning_rate": 3.845666666666667e-05,
      "loss": 0.0028,
      "step": 34630
    },
    {
      "epoch": 1.8474666666666666,
      "grad_norm": 0.06376515328884125,
      "learning_rate": 3.845333333333333e-05,
      "loss": 0.0032,
      "step": 34640
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 0.47219496965408325,
      "learning_rate": 3.845e-05,
      "loss": 0.0033,
      "step": 34650
    },
    {
      "epoch": 1.8485333333333334,
      "grad_norm": 0.7275968194007874,
      "learning_rate": 3.844666666666667e-05,
      "loss": 0.0035,
      "step": 34660
    },
    {
      "epoch": 1.8490666666666666,
      "grad_norm": 0.4286687672138214,
      "learning_rate": 3.844333333333334e-05,
      "loss": 0.0033,
      "step": 34670
    },
    {
      "epoch": 1.8496000000000001,
      "grad_norm": 0.03853379189968109,
      "learning_rate": 3.8440000000000005e-05,
      "loss": 0.0034,
      "step": 34680
    },
    {
      "epoch": 1.8501333333333334,
      "grad_norm": 0.01890520006418228,
      "learning_rate": 3.843666666666667e-05,
      "loss": 0.0019,
      "step": 34690
    },
    {
      "epoch": 1.8506666666666667,
      "grad_norm": 0.26709097623825073,
      "learning_rate": 3.843333333333334e-05,
      "loss": 0.0032,
      "step": 34700
    },
    {
      "epoch": 1.8512,
      "grad_norm": 0.08778265863656998,
      "learning_rate": 3.8429999999999996e-05,
      "loss": 0.0037,
      "step": 34710
    },
    {
      "epoch": 1.8517333333333332,
      "grad_norm": 0.1766386181116104,
      "learning_rate": 3.842666666666667e-05,
      "loss": 0.0031,
      "step": 34720
    },
    {
      "epoch": 1.8522666666666665,
      "grad_norm": 0.3222402334213257,
      "learning_rate": 3.8423333333333335e-05,
      "loss": 0.003,
      "step": 34730
    },
    {
      "epoch": 1.8528,
      "grad_norm": 0.08813861012458801,
      "learning_rate": 3.842e-05,
      "loss": 0.0031,
      "step": 34740
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 0.34862810373306274,
      "learning_rate": 3.841666666666667e-05,
      "loss": 0.0024,
      "step": 34750
    },
    {
      "epoch": 1.8538666666666668,
      "grad_norm": 0.20851774513721466,
      "learning_rate": 3.8413333333333334e-05,
      "loss": 0.0034,
      "step": 34760
    },
    {
      "epoch": 1.8544,
      "grad_norm": 0.2673965096473694,
      "learning_rate": 3.841e-05,
      "loss": 0.0027,
      "step": 34770
    },
    {
      "epoch": 1.8549333333333333,
      "grad_norm": 0.03431355953216553,
      "learning_rate": 3.8406666666666666e-05,
      "loss": 0.0026,
      "step": 34780
    },
    {
      "epoch": 1.8554666666666666,
      "grad_norm": 0.6184724569320679,
      "learning_rate": 3.840333333333334e-05,
      "loss": 0.0025,
      "step": 34790
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.14705903828144073,
      "learning_rate": 3.8400000000000005e-05,
      "loss": 0.002,
      "step": 34800
    },
    {
      "epoch": 1.8565333333333334,
      "grad_norm": 0.23514658212661743,
      "learning_rate": 3.839666666666667e-05,
      "loss": 0.0027,
      "step": 34810
    },
    {
      "epoch": 1.8570666666666666,
      "grad_norm": 0.3583308160305023,
      "learning_rate": 3.839333333333334e-05,
      "loss": 0.0032,
      "step": 34820
    },
    {
      "epoch": 1.8576000000000001,
      "grad_norm": 0.4413953721523285,
      "learning_rate": 3.8390000000000003e-05,
      "loss": 0.0023,
      "step": 34830
    },
    {
      "epoch": 1.8581333333333334,
      "grad_norm": 0.20516055822372437,
      "learning_rate": 3.838666666666667e-05,
      "loss": 0.0032,
      "step": 34840
    },
    {
      "epoch": 1.8586666666666667,
      "grad_norm": 0.09287981688976288,
      "learning_rate": 3.8383333333333336e-05,
      "loss": 0.0031,
      "step": 34850
    },
    {
      "epoch": 1.8592,
      "grad_norm": 0.26480400562286377,
      "learning_rate": 3.838e-05,
      "loss": 0.0025,
      "step": 34860
    },
    {
      "epoch": 1.8597333333333332,
      "grad_norm": 0.04461720958352089,
      "learning_rate": 3.837666666666667e-05,
      "loss": 0.0017,
      "step": 34870
    },
    {
      "epoch": 1.8602666666666665,
      "grad_norm": 0.05367608368396759,
      "learning_rate": 3.8373333333333334e-05,
      "loss": 0.0033,
      "step": 34880
    },
    {
      "epoch": 1.8608,
      "grad_norm": 0.1171969398856163,
      "learning_rate": 3.837e-05,
      "loss": 0.0022,
      "step": 34890
    },
    {
      "epoch": 1.8613333333333333,
      "grad_norm": 0.3343198299407959,
      "learning_rate": 3.8366666666666666e-05,
      "loss": 0.0033,
      "step": 34900
    },
    {
      "epoch": 1.8618666666666668,
      "grad_norm": 0.08755142241716385,
      "learning_rate": 3.836333333333333e-05,
      "loss": 0.0023,
      "step": 34910
    },
    {
      "epoch": 1.8624,
      "grad_norm": 0.20551085472106934,
      "learning_rate": 3.836e-05,
      "loss": 0.0024,
      "step": 34920
    },
    {
      "epoch": 1.8629333333333333,
      "grad_norm": 0.116000235080719,
      "learning_rate": 3.835666666666667e-05,
      "loss": 0.0026,
      "step": 34930
    },
    {
      "epoch": 1.8634666666666666,
      "grad_norm": 0.5268266201019287,
      "learning_rate": 3.835333333333334e-05,
      "loss": 0.0045,
      "step": 34940
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.0979446992278099,
      "learning_rate": 3.8350000000000004e-05,
      "loss": 0.0034,
      "step": 34950
    },
    {
      "epoch": 1.8645333333333334,
      "grad_norm": 0.1174880787730217,
      "learning_rate": 3.834666666666667e-05,
      "loss": 0.0024,
      "step": 34960
    },
    {
      "epoch": 1.8650666666666667,
      "grad_norm": 0.03962800279259682,
      "learning_rate": 3.8343333333333336e-05,
      "loss": 0.0025,
      "step": 34970
    },
    {
      "epoch": 1.8656000000000001,
      "grad_norm": 0.06348192691802979,
      "learning_rate": 3.834e-05,
      "loss": 0.0036,
      "step": 34980
    },
    {
      "epoch": 1.8661333333333334,
      "grad_norm": 0.26497742533683777,
      "learning_rate": 3.833666666666667e-05,
      "loss": 0.0025,
      "step": 34990
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.44005969166755676,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 0.0026,
      "step": 35000
    },
    {
      "epoch": 1.8672,
      "grad_norm": 0.14945338666439056,
      "learning_rate": 3.833e-05,
      "loss": 0.0028,
      "step": 35010
    },
    {
      "epoch": 1.8677333333333332,
      "grad_norm": 0.1468965858221054,
      "learning_rate": 3.832666666666667e-05,
      "loss": 0.0025,
      "step": 35020
    },
    {
      "epoch": 1.8682666666666665,
      "grad_norm": 0.17382699251174927,
      "learning_rate": 3.832333333333333e-05,
      "loss": 0.0033,
      "step": 35030
    },
    {
      "epoch": 1.8688,
      "grad_norm": 0.053285036236047745,
      "learning_rate": 3.832e-05,
      "loss": 0.0031,
      "step": 35040
    },
    {
      "epoch": 1.8693333333333333,
      "grad_norm": 0.29029688239097595,
      "learning_rate": 3.8316666666666665e-05,
      "loss": 0.0034,
      "step": 35050
    },
    {
      "epoch": 1.8698666666666668,
      "grad_norm": 0.1776147335767746,
      "learning_rate": 3.831333333333333e-05,
      "loss": 0.0033,
      "step": 35060
    },
    {
      "epoch": 1.8704,
      "grad_norm": 0.06559870392084122,
      "learning_rate": 3.8310000000000004e-05,
      "loss": 0.0022,
      "step": 35070
    },
    {
      "epoch": 1.8709333333333333,
      "grad_norm": 0.39703407883644104,
      "learning_rate": 3.830666666666667e-05,
      "loss": 0.0032,
      "step": 35080
    },
    {
      "epoch": 1.8714666666666666,
      "grad_norm": 0.35405808687210083,
      "learning_rate": 3.8303333333333336e-05,
      "loss": 0.0025,
      "step": 35090
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.4677484929561615,
      "learning_rate": 3.83e-05,
      "loss": 0.0022,
      "step": 35100
    },
    {
      "epoch": 1.8725333333333334,
      "grad_norm": 0.4078809916973114,
      "learning_rate": 3.829666666666667e-05,
      "loss": 0.0026,
      "step": 35110
    },
    {
      "epoch": 1.8730666666666667,
      "grad_norm": 0.39451122283935547,
      "learning_rate": 3.8293333333333335e-05,
      "loss": 0.0031,
      "step": 35120
    },
    {
      "epoch": 1.8736000000000002,
      "grad_norm": 0.2325151115655899,
      "learning_rate": 3.829e-05,
      "loss": 0.0036,
      "step": 35130
    },
    {
      "epoch": 1.8741333333333334,
      "grad_norm": 0.38253629207611084,
      "learning_rate": 3.8286666666666674e-05,
      "loss": 0.0022,
      "step": 35140
    },
    {
      "epoch": 1.8746666666666667,
      "grad_norm": 0.3475813567638397,
      "learning_rate": 3.828333333333334e-05,
      "loss": 0.0029,
      "step": 35150
    },
    {
      "epoch": 1.8752,
      "grad_norm": 0.13916811347007751,
      "learning_rate": 3.828e-05,
      "loss": 0.0022,
      "step": 35160
    },
    {
      "epoch": 1.8757333333333333,
      "grad_norm": 0.20806445181369781,
      "learning_rate": 3.8276666666666666e-05,
      "loss": 0.0034,
      "step": 35170
    },
    {
      "epoch": 1.8762666666666665,
      "grad_norm": 0.046793170273303986,
      "learning_rate": 3.827333333333333e-05,
      "loss": 0.0032,
      "step": 35180
    },
    {
      "epoch": 1.8768,
      "grad_norm": 0.07634932547807693,
      "learning_rate": 3.827e-05,
      "loss": 0.0024,
      "step": 35190
    },
    {
      "epoch": 1.8773333333333333,
      "grad_norm": 0.29187729954719543,
      "learning_rate": 3.8266666666666664e-05,
      "loss": 0.0018,
      "step": 35200
    },
    {
      "epoch": 1.8778666666666668,
      "grad_norm": 0.17364995181560516,
      "learning_rate": 3.826333333333334e-05,
      "loss": 0.0027,
      "step": 35210
    },
    {
      "epoch": 1.8784,
      "grad_norm": 0.25523701310157776,
      "learning_rate": 3.826e-05,
      "loss": 0.0022,
      "step": 35220
    },
    {
      "epoch": 1.8789333333333333,
      "grad_norm": 0.11748246848583221,
      "learning_rate": 3.825666666666667e-05,
      "loss": 0.0041,
      "step": 35230
    },
    {
      "epoch": 1.8794666666666666,
      "grad_norm": 0.17622074484825134,
      "learning_rate": 3.8253333333333335e-05,
      "loss": 0.003,
      "step": 35240
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.4907837212085724,
      "learning_rate": 3.825e-05,
      "loss": 0.0025,
      "step": 35250
    },
    {
      "epoch": 1.8805333333333332,
      "grad_norm": 0.127811461687088,
      "learning_rate": 3.824666666666667e-05,
      "loss": 0.0017,
      "step": 35260
    },
    {
      "epoch": 1.8810666666666667,
      "grad_norm": 0.0650898888707161,
      "learning_rate": 3.8243333333333334e-05,
      "loss": 0.0027,
      "step": 35270
    },
    {
      "epoch": 1.8816000000000002,
      "grad_norm": 0.09281796216964722,
      "learning_rate": 3.8240000000000007e-05,
      "loss": 0.002,
      "step": 35280
    },
    {
      "epoch": 1.8821333333333334,
      "grad_norm": 0.1198258176445961,
      "learning_rate": 3.823666666666667e-05,
      "loss": 0.0031,
      "step": 35290
    },
    {
      "epoch": 1.8826666666666667,
      "grad_norm": 0.26126715540885925,
      "learning_rate": 3.823333333333334e-05,
      "loss": 0.0027,
      "step": 35300
    },
    {
      "epoch": 1.8832,
      "grad_norm": 0.06121334061026573,
      "learning_rate": 3.823e-05,
      "loss": 0.0044,
      "step": 35310
    },
    {
      "epoch": 1.8837333333333333,
      "grad_norm": 0.2046719193458557,
      "learning_rate": 3.8226666666666664e-05,
      "loss": 0.003,
      "step": 35320
    },
    {
      "epoch": 1.8842666666666665,
      "grad_norm": 0.3199283480644226,
      "learning_rate": 3.822333333333333e-05,
      "loss": 0.0028,
      "step": 35330
    },
    {
      "epoch": 1.8848,
      "grad_norm": 0.17646361887454987,
      "learning_rate": 3.822e-05,
      "loss": 0.0033,
      "step": 35340
    },
    {
      "epoch": 1.8853333333333333,
      "grad_norm": 0.1178957000374794,
      "learning_rate": 3.821666666666667e-05,
      "loss": 0.003,
      "step": 35350
    },
    {
      "epoch": 1.8858666666666668,
      "grad_norm": 0.5694702863693237,
      "learning_rate": 3.8213333333333336e-05,
      "loss": 0.0028,
      "step": 35360
    },
    {
      "epoch": 1.8864,
      "grad_norm": 0.5331916213035583,
      "learning_rate": 3.821e-05,
      "loss": 0.003,
      "step": 35370
    },
    {
      "epoch": 1.8869333333333334,
      "grad_norm": 0.5424376726150513,
      "learning_rate": 3.820666666666667e-05,
      "loss": 0.0025,
      "step": 35380
    },
    {
      "epoch": 1.8874666666666666,
      "grad_norm": 0.34695670008659363,
      "learning_rate": 3.8203333333333334e-05,
      "loss": 0.0045,
      "step": 35390
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.17940129339694977,
      "learning_rate": 3.82e-05,
      "loss": 0.0038,
      "step": 35400
    },
    {
      "epoch": 1.8885333333333332,
      "grad_norm": 0.06319870054721832,
      "learning_rate": 3.8196666666666666e-05,
      "loss": 0.0019,
      "step": 35410
    },
    {
      "epoch": 1.8890666666666667,
      "grad_norm": 0.07407967746257782,
      "learning_rate": 3.819333333333334e-05,
      "loss": 0.0031,
      "step": 35420
    },
    {
      "epoch": 1.8896,
      "grad_norm": 0.027224164456129074,
      "learning_rate": 3.8190000000000005e-05,
      "loss": 0.0028,
      "step": 35430
    },
    {
      "epoch": 1.8901333333333334,
      "grad_norm": 0.6920811533927917,
      "learning_rate": 3.818666666666667e-05,
      "loss": 0.0039,
      "step": 35440
    },
    {
      "epoch": 1.8906666666666667,
      "grad_norm": 0.8397538661956787,
      "learning_rate": 3.818333333333334e-05,
      "loss": 0.0024,
      "step": 35450
    },
    {
      "epoch": 1.8912,
      "grad_norm": 0.25904619693756104,
      "learning_rate": 3.818e-05,
      "loss": 0.0021,
      "step": 35460
    },
    {
      "epoch": 1.8917333333333333,
      "grad_norm": 0.7624772191047668,
      "learning_rate": 3.817666666666666e-05,
      "loss": 0.0031,
      "step": 35470
    },
    {
      "epoch": 1.8922666666666665,
      "grad_norm": 0.4054458737373352,
      "learning_rate": 3.8173333333333336e-05,
      "loss": 0.0029,
      "step": 35480
    },
    {
      "epoch": 1.8928,
      "grad_norm": 0.11932402104139328,
      "learning_rate": 3.817e-05,
      "loss": 0.0022,
      "step": 35490
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 0.3936067223548889,
      "learning_rate": 3.816666666666667e-05,
      "loss": 0.003,
      "step": 35500
    },
    {
      "epoch": 1.8938666666666668,
      "grad_norm": 0.23210328817367554,
      "learning_rate": 3.8163333333333334e-05,
      "loss": 0.0025,
      "step": 35510
    },
    {
      "epoch": 1.8944,
      "grad_norm": 0.06787636876106262,
      "learning_rate": 3.816e-05,
      "loss": 0.0017,
      "step": 35520
    },
    {
      "epoch": 1.8949333333333334,
      "grad_norm": 0.4380350112915039,
      "learning_rate": 3.815666666666667e-05,
      "loss": 0.0021,
      "step": 35530
    },
    {
      "epoch": 1.8954666666666666,
      "grad_norm": 0.2318151593208313,
      "learning_rate": 3.815333333333333e-05,
      "loss": 0.0028,
      "step": 35540
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.41941893100738525,
      "learning_rate": 3.8150000000000006e-05,
      "loss": 0.0037,
      "step": 35550
    },
    {
      "epoch": 1.8965333333333332,
      "grad_norm": 0.5262964963912964,
      "learning_rate": 3.814666666666667e-05,
      "loss": 0.0021,
      "step": 35560
    },
    {
      "epoch": 1.8970666666666667,
      "grad_norm": 0.39880043268203735,
      "learning_rate": 3.814333333333334e-05,
      "loss": 0.0033,
      "step": 35570
    },
    {
      "epoch": 1.8976,
      "grad_norm": 0.23253868520259857,
      "learning_rate": 3.8140000000000004e-05,
      "loss": 0.003,
      "step": 35580
    },
    {
      "epoch": 1.8981333333333335,
      "grad_norm": 0.08934281021356583,
      "learning_rate": 3.813666666666667e-05,
      "loss": 0.0023,
      "step": 35590
    },
    {
      "epoch": 1.8986666666666667,
      "grad_norm": 0.3546605706214905,
      "learning_rate": 3.8133333333333336e-05,
      "loss": 0.0031,
      "step": 35600
    },
    {
      "epoch": 1.8992,
      "grad_norm": 0.5234724283218384,
      "learning_rate": 3.8129999999999996e-05,
      "loss": 0.0031,
      "step": 35610
    },
    {
      "epoch": 1.8997333333333333,
      "grad_norm": 0.20406417548656464,
      "learning_rate": 3.812666666666667e-05,
      "loss": 0.0026,
      "step": 35620
    },
    {
      "epoch": 1.9002666666666665,
      "grad_norm": 0.20741821825504303,
      "learning_rate": 3.8123333333333335e-05,
      "loss": 0.0036,
      "step": 35630
    },
    {
      "epoch": 1.9008,
      "grad_norm": 0.3176535665988922,
      "learning_rate": 3.812e-05,
      "loss": 0.0032,
      "step": 35640
    },
    {
      "epoch": 1.9013333333333333,
      "grad_norm": 0.5851535201072693,
      "learning_rate": 3.811666666666667e-05,
      "loss": 0.0025,
      "step": 35650
    },
    {
      "epoch": 1.9018666666666668,
      "grad_norm": 0.1208924949169159,
      "learning_rate": 3.811333333333333e-05,
      "loss": 0.0021,
      "step": 35660
    },
    {
      "epoch": 1.9024,
      "grad_norm": 0.3252790868282318,
      "learning_rate": 3.811e-05,
      "loss": 0.0017,
      "step": 35670
    },
    {
      "epoch": 1.9029333333333334,
      "grad_norm": 0.3881034255027771,
      "learning_rate": 3.8106666666666665e-05,
      "loss": 0.0042,
      "step": 35680
    },
    {
      "epoch": 1.9034666666666666,
      "grad_norm": 0.29056352376937866,
      "learning_rate": 3.810333333333334e-05,
      "loss": 0.0021,
      "step": 35690
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.06395655125379562,
      "learning_rate": 3.8100000000000005e-05,
      "loss": 0.0026,
      "step": 35700
    },
    {
      "epoch": 1.9045333333333332,
      "grad_norm": 0.42137694358825684,
      "learning_rate": 3.809666666666667e-05,
      "loss": 0.0026,
      "step": 35710
    },
    {
      "epoch": 1.9050666666666667,
      "grad_norm": 0.26541513204574585,
      "learning_rate": 3.809333333333334e-05,
      "loss": 0.0025,
      "step": 35720
    },
    {
      "epoch": 1.9056,
      "grad_norm": 0.26164519786834717,
      "learning_rate": 3.809e-05,
      "loss": 0.003,
      "step": 35730
    },
    {
      "epoch": 1.9061333333333335,
      "grad_norm": 0.027395352721214294,
      "learning_rate": 3.808666666666667e-05,
      "loss": 0.0013,
      "step": 35740
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.08996383845806122,
      "learning_rate": 3.8083333333333335e-05,
      "loss": 0.0024,
      "step": 35750
    },
    {
      "epoch": 1.9072,
      "grad_norm": 0.5334619879722595,
      "learning_rate": 3.808e-05,
      "loss": 0.0031,
      "step": 35760
    },
    {
      "epoch": 1.9077333333333333,
      "grad_norm": 0.11661048233509064,
      "learning_rate": 3.807666666666667e-05,
      "loss": 0.0024,
      "step": 35770
    },
    {
      "epoch": 1.9082666666666666,
      "grad_norm": 0.46902143955230713,
      "learning_rate": 3.8073333333333334e-05,
      "loss": 0.0039,
      "step": 35780
    },
    {
      "epoch": 1.9088,
      "grad_norm": 0.4616512656211853,
      "learning_rate": 3.807e-05,
      "loss": 0.0031,
      "step": 35790
    },
    {
      "epoch": 1.9093333333333333,
      "grad_norm": 0.319728285074234,
      "learning_rate": 3.8066666666666666e-05,
      "loss": 0.0039,
      "step": 35800
    },
    {
      "epoch": 1.9098666666666668,
      "grad_norm": 0.09336740523576736,
      "learning_rate": 3.806333333333333e-05,
      "loss": 0.0029,
      "step": 35810
    },
    {
      "epoch": 1.9104,
      "grad_norm": 0.02638218179345131,
      "learning_rate": 3.806e-05,
      "loss": 0.003,
      "step": 35820
    },
    {
      "epoch": 1.9109333333333334,
      "grad_norm": 0.03596942499279976,
      "learning_rate": 3.805666666666667e-05,
      "loss": 0.0041,
      "step": 35830
    },
    {
      "epoch": 1.9114666666666666,
      "grad_norm": 0.23466774821281433,
      "learning_rate": 3.805333333333334e-05,
      "loss": 0.0036,
      "step": 35840
    },
    {
      "epoch": 1.912,
      "grad_norm": 0.32440391182899475,
      "learning_rate": 3.805e-05,
      "loss": 0.0025,
      "step": 35850
    },
    {
      "epoch": 1.9125333333333332,
      "grad_norm": 0.2571428716182709,
      "learning_rate": 3.804666666666667e-05,
      "loss": 0.0023,
      "step": 35860
    },
    {
      "epoch": 1.9130666666666667,
      "grad_norm": 0.5568753480911255,
      "learning_rate": 3.8043333333333336e-05,
      "loss": 0.0028,
      "step": 35870
    },
    {
      "epoch": 1.9136,
      "grad_norm": 0.11710743606090546,
      "learning_rate": 3.804e-05,
      "loss": 0.0044,
      "step": 35880
    },
    {
      "epoch": 1.9141333333333335,
      "grad_norm": 0.06652457267045975,
      "learning_rate": 3.803666666666667e-05,
      "loss": 0.0028,
      "step": 35890
    },
    {
      "epoch": 1.9146666666666667,
      "grad_norm": 0.2353498786687851,
      "learning_rate": 3.803333333333334e-05,
      "loss": 0.0032,
      "step": 35900
    },
    {
      "epoch": 1.9152,
      "grad_norm": 0.29394564032554626,
      "learning_rate": 3.803000000000001e-05,
      "loss": 0.0032,
      "step": 35910
    },
    {
      "epoch": 1.9157333333333333,
      "grad_norm": 0.4655195474624634,
      "learning_rate": 3.8026666666666666e-05,
      "loss": 0.0017,
      "step": 35920
    },
    {
      "epoch": 1.9162666666666666,
      "grad_norm": 0.03372751548886299,
      "learning_rate": 3.802333333333333e-05,
      "loss": 0.0042,
      "step": 35930
    },
    {
      "epoch": 1.9167999999999998,
      "grad_norm": 0.0939670205116272,
      "learning_rate": 3.802e-05,
      "loss": 0.0026,
      "step": 35940
    },
    {
      "epoch": 1.9173333333333333,
      "grad_norm": 0.03781285881996155,
      "learning_rate": 3.8016666666666665e-05,
      "loss": 0.0036,
      "step": 35950
    },
    {
      "epoch": 1.9178666666666668,
      "grad_norm": 0.17493127286434174,
      "learning_rate": 3.801333333333333e-05,
      "loss": 0.0034,
      "step": 35960
    },
    {
      "epoch": 1.9184,
      "grad_norm": 0.4281140863895416,
      "learning_rate": 3.8010000000000004e-05,
      "loss": 0.0033,
      "step": 35970
    },
    {
      "epoch": 1.9189333333333334,
      "grad_norm": 0.4406837224960327,
      "learning_rate": 3.800666666666667e-05,
      "loss": 0.0027,
      "step": 35980
    },
    {
      "epoch": 1.9194666666666667,
      "grad_norm": 0.24417966604232788,
      "learning_rate": 3.8003333333333336e-05,
      "loss": 0.0031,
      "step": 35990
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.17659509181976318,
      "learning_rate": 3.8e-05,
      "loss": 0.0034,
      "step": 36000
    },
    {
      "epoch": 1.9205333333333332,
      "grad_norm": 0.26220831274986267,
      "learning_rate": 3.799666666666667e-05,
      "loss": 0.0027,
      "step": 36010
    },
    {
      "epoch": 1.9210666666666667,
      "grad_norm": 0.09285631030797958,
      "learning_rate": 3.7993333333333334e-05,
      "loss": 0.0024,
      "step": 36020
    },
    {
      "epoch": 1.9216,
      "grad_norm": 0.039158765226602554,
      "learning_rate": 3.799e-05,
      "loss": 0.0042,
      "step": 36030
    },
    {
      "epoch": 1.9221333333333335,
      "grad_norm": 0.15215300023555756,
      "learning_rate": 3.7986666666666673e-05,
      "loss": 0.0036,
      "step": 36040
    },
    {
      "epoch": 1.9226666666666667,
      "grad_norm": 0.04049384966492653,
      "learning_rate": 3.798333333333334e-05,
      "loss": 0.0031,
      "step": 36050
    },
    {
      "epoch": 1.9232,
      "grad_norm": 0.20830084383487701,
      "learning_rate": 3.7980000000000006e-05,
      "loss": 0.0028,
      "step": 36060
    },
    {
      "epoch": 1.9237333333333333,
      "grad_norm": 0.09488620609045029,
      "learning_rate": 3.7976666666666665e-05,
      "loss": 0.0043,
      "step": 36070
    },
    {
      "epoch": 1.9242666666666666,
      "grad_norm": 0.03769352659583092,
      "learning_rate": 3.797333333333333e-05,
      "loss": 0.0027,
      "step": 36080
    },
    {
      "epoch": 1.9247999999999998,
      "grad_norm": 0.3546122610569,
      "learning_rate": 3.797e-05,
      "loss": 0.0042,
      "step": 36090
    },
    {
      "epoch": 1.9253333333333333,
      "grad_norm": 0.22724980115890503,
      "learning_rate": 3.796666666666667e-05,
      "loss": 0.0036,
      "step": 36100
    },
    {
      "epoch": 1.9258666666666666,
      "grad_norm": 0.08992261439561844,
      "learning_rate": 3.7963333333333336e-05,
      "loss": 0.0026,
      "step": 36110
    },
    {
      "epoch": 1.9264000000000001,
      "grad_norm": 0.23393802344799042,
      "learning_rate": 3.796e-05,
      "loss": 0.003,
      "step": 36120
    },
    {
      "epoch": 1.9269333333333334,
      "grad_norm": 0.17636559903621674,
      "learning_rate": 3.795666666666667e-05,
      "loss": 0.0031,
      "step": 36130
    },
    {
      "epoch": 1.9274666666666667,
      "grad_norm": 0.4192228317260742,
      "learning_rate": 3.7953333333333335e-05,
      "loss": 0.0029,
      "step": 36140
    },
    {
      "epoch": 1.928,
      "grad_norm": 0.5323453545570374,
      "learning_rate": 3.795e-05,
      "loss": 0.0036,
      "step": 36150
    },
    {
      "epoch": 1.9285333333333332,
      "grad_norm": 0.203923299908638,
      "learning_rate": 3.794666666666667e-05,
      "loss": 0.0026,
      "step": 36160
    },
    {
      "epoch": 1.9290666666666667,
      "grad_norm": 0.03834535554051399,
      "learning_rate": 3.794333333333333e-05,
      "loss": 0.0029,
      "step": 36170
    },
    {
      "epoch": 1.9296,
      "grad_norm": 0.15230803191661835,
      "learning_rate": 3.7940000000000006e-05,
      "loss": 0.0024,
      "step": 36180
    },
    {
      "epoch": 1.9301333333333335,
      "grad_norm": 0.4856296181678772,
      "learning_rate": 3.793666666666667e-05,
      "loss": 0.0023,
      "step": 36190
    },
    {
      "epoch": 1.9306666666666668,
      "grad_norm": 0.2084241360425949,
      "learning_rate": 3.793333333333334e-05,
      "loss": 0.0025,
      "step": 36200
    },
    {
      "epoch": 1.9312,
      "grad_norm": 0.15797822177410126,
      "learning_rate": 3.7930000000000004e-05,
      "loss": 0.0032,
      "step": 36210
    },
    {
      "epoch": 1.9317333333333333,
      "grad_norm": 0.5368061065673828,
      "learning_rate": 3.7926666666666664e-05,
      "loss": 0.002,
      "step": 36220
    },
    {
      "epoch": 1.9322666666666666,
      "grad_norm": 0.23976001143455505,
      "learning_rate": 3.792333333333333e-05,
      "loss": 0.0031,
      "step": 36230
    },
    {
      "epoch": 1.9327999999999999,
      "grad_norm": 0.3214681148529053,
      "learning_rate": 3.792e-05,
      "loss": 0.0039,
      "step": 36240
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.03618885576725006,
      "learning_rate": 3.791666666666667e-05,
      "loss": 0.0032,
      "step": 36250
    },
    {
      "epoch": 1.9338666666666666,
      "grad_norm": 0.26147782802581787,
      "learning_rate": 3.7913333333333335e-05,
      "loss": 0.0028,
      "step": 36260
    },
    {
      "epoch": 1.9344000000000001,
      "grad_norm": 0.2085064947605133,
      "learning_rate": 3.791e-05,
      "loss": 0.002,
      "step": 36270
    },
    {
      "epoch": 1.9349333333333334,
      "grad_norm": 0.04860156029462814,
      "learning_rate": 3.790666666666667e-05,
      "loss": 0.0032,
      "step": 36280
    },
    {
      "epoch": 1.9354666666666667,
      "grad_norm": 0.15029385685920715,
      "learning_rate": 3.7903333333333334e-05,
      "loss": 0.003,
      "step": 36290
    },
    {
      "epoch": 1.936,
      "grad_norm": 0.04656166583299637,
      "learning_rate": 3.79e-05,
      "loss": 0.0024,
      "step": 36300
    },
    {
      "epoch": 1.9365333333333332,
      "grad_norm": 0.36123114824295044,
      "learning_rate": 3.789666666666667e-05,
      "loss": 0.0023,
      "step": 36310
    },
    {
      "epoch": 1.9370666666666667,
      "grad_norm": 0.11730748414993286,
      "learning_rate": 3.789333333333334e-05,
      "loss": 0.0029,
      "step": 36320
    },
    {
      "epoch": 1.9376,
      "grad_norm": 0.24000686407089233,
      "learning_rate": 3.7890000000000005e-05,
      "loss": 0.0023,
      "step": 36330
    },
    {
      "epoch": 1.9381333333333335,
      "grad_norm": 0.05867855250835419,
      "learning_rate": 3.788666666666667e-05,
      "loss": 0.0027,
      "step": 36340
    },
    {
      "epoch": 1.9386666666666668,
      "grad_norm": 0.3251630961894989,
      "learning_rate": 3.788333333333334e-05,
      "loss": 0.0035,
      "step": 36350
    },
    {
      "epoch": 1.9392,
      "grad_norm": 0.099782794713974,
      "learning_rate": 3.788e-05,
      "loss": 0.0042,
      "step": 36360
    },
    {
      "epoch": 1.9397333333333333,
      "grad_norm": 0.40462902188301086,
      "learning_rate": 3.787666666666666e-05,
      "loss": 0.003,
      "step": 36370
    },
    {
      "epoch": 1.9402666666666666,
      "grad_norm": 0.0642155110836029,
      "learning_rate": 3.7873333333333336e-05,
      "loss": 0.0022,
      "step": 36380
    },
    {
      "epoch": 1.9407999999999999,
      "grad_norm": 0.17430678009986877,
      "learning_rate": 3.787e-05,
      "loss": 0.0025,
      "step": 36390
    },
    {
      "epoch": 1.9413333333333334,
      "grad_norm": 0.14927777647972107,
      "learning_rate": 3.786666666666667e-05,
      "loss": 0.003,
      "step": 36400
    },
    {
      "epoch": 1.9418666666666666,
      "grad_norm": 0.284191757440567,
      "learning_rate": 3.7863333333333334e-05,
      "loss": 0.0026,
      "step": 36410
    },
    {
      "epoch": 1.9424000000000001,
      "grad_norm": 0.3428065776824951,
      "learning_rate": 3.786e-05,
      "loss": 0.0025,
      "step": 36420
    },
    {
      "epoch": 1.9429333333333334,
      "grad_norm": 0.3529756963253021,
      "learning_rate": 3.7856666666666666e-05,
      "loss": 0.0027,
      "step": 36430
    },
    {
      "epoch": 1.9434666666666667,
      "grad_norm": 0.4038991928100586,
      "learning_rate": 3.785333333333333e-05,
      "loss": 0.0033,
      "step": 36440
    },
    {
      "epoch": 1.944,
      "grad_norm": 0.1173725575208664,
      "learning_rate": 3.7850000000000005e-05,
      "loss": 0.0044,
      "step": 36450
    },
    {
      "epoch": 1.9445333333333332,
      "grad_norm": 0.8681578636169434,
      "learning_rate": 3.784666666666667e-05,
      "loss": 0.0026,
      "step": 36460
    },
    {
      "epoch": 1.9450666666666667,
      "grad_norm": 0.11759425699710846,
      "learning_rate": 3.784333333333334e-05,
      "loss": 0.0042,
      "step": 36470
    },
    {
      "epoch": 1.9456,
      "grad_norm": 0.14949767291545868,
      "learning_rate": 3.7840000000000004e-05,
      "loss": 0.0025,
      "step": 36480
    },
    {
      "epoch": 1.9461333333333335,
      "grad_norm": 0.1837722808122635,
      "learning_rate": 3.783666666666667e-05,
      "loss": 0.0036,
      "step": 36490
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 0.24422360956668854,
      "learning_rate": 3.7833333333333336e-05,
      "loss": 0.0026,
      "step": 36500
    },
    {
      "epoch": 1.9472,
      "grad_norm": 0.06396809965372086,
      "learning_rate": 3.783e-05,
      "loss": 0.0033,
      "step": 36510
    },
    {
      "epoch": 1.9477333333333333,
      "grad_norm": 0.3765823245048523,
      "learning_rate": 3.782666666666667e-05,
      "loss": 0.0025,
      "step": 36520
    },
    {
      "epoch": 1.9482666666666666,
      "grad_norm": 0.4110684096813202,
      "learning_rate": 3.7823333333333334e-05,
      "loss": 0.0023,
      "step": 36530
    },
    {
      "epoch": 1.9487999999999999,
      "grad_norm": 0.17502182722091675,
      "learning_rate": 3.782e-05,
      "loss": 0.0037,
      "step": 36540
    },
    {
      "epoch": 1.9493333333333334,
      "grad_norm": 0.2596655488014221,
      "learning_rate": 3.7816666666666667e-05,
      "loss": 0.0021,
      "step": 36550
    },
    {
      "epoch": 1.9498666666666666,
      "grad_norm": 0.180206298828125,
      "learning_rate": 3.781333333333333e-05,
      "loss": 0.0024,
      "step": 36560
    },
    {
      "epoch": 1.9504000000000001,
      "grad_norm": 0.2924579679965973,
      "learning_rate": 3.781e-05,
      "loss": 0.0031,
      "step": 36570
    },
    {
      "epoch": 1.9509333333333334,
      "grad_norm": 0.08834131807088852,
      "learning_rate": 3.7806666666666665e-05,
      "loss": 0.0028,
      "step": 36580
    },
    {
      "epoch": 1.9514666666666667,
      "grad_norm": 0.24517220258712769,
      "learning_rate": 3.780333333333334e-05,
      "loss": 0.0034,
      "step": 36590
    },
    {
      "epoch": 1.952,
      "grad_norm": 0.4495666027069092,
      "learning_rate": 3.7800000000000004e-05,
      "loss": 0.0031,
      "step": 36600
    },
    {
      "epoch": 1.9525333333333332,
      "grad_norm": 0.592402994632721,
      "learning_rate": 3.779666666666667e-05,
      "loss": 0.0031,
      "step": 36610
    },
    {
      "epoch": 1.9530666666666665,
      "grad_norm": 0.4988289475440979,
      "learning_rate": 3.7793333333333336e-05,
      "loss": 0.0035,
      "step": 36620
    },
    {
      "epoch": 1.9536,
      "grad_norm": 0.647569477558136,
      "learning_rate": 3.779e-05,
      "loss": 0.0018,
      "step": 36630
    },
    {
      "epoch": 1.9541333333333335,
      "grad_norm": 0.3540779948234558,
      "learning_rate": 3.778666666666667e-05,
      "loss": 0.0045,
      "step": 36640
    },
    {
      "epoch": 1.9546666666666668,
      "grad_norm": 0.3842085599899292,
      "learning_rate": 3.7783333333333335e-05,
      "loss": 0.0023,
      "step": 36650
    },
    {
      "epoch": 1.9552,
      "grad_norm": 0.036199722439050674,
      "learning_rate": 3.778000000000001e-05,
      "loss": 0.0028,
      "step": 36660
    },
    {
      "epoch": 1.9557333333333333,
      "grad_norm": 0.09396758675575256,
      "learning_rate": 3.777666666666667e-05,
      "loss": 0.0027,
      "step": 36670
    },
    {
      "epoch": 1.9562666666666666,
      "grad_norm": 0.23949165642261505,
      "learning_rate": 3.777333333333333e-05,
      "loss": 0.0024,
      "step": 36680
    },
    {
      "epoch": 1.9567999999999999,
      "grad_norm": 0.39545175433158875,
      "learning_rate": 3.777e-05,
      "loss": 0.0027,
      "step": 36690
    },
    {
      "epoch": 1.9573333333333334,
      "grad_norm": 0.15897539258003235,
      "learning_rate": 3.7766666666666665e-05,
      "loss": 0.004,
      "step": 36700
    },
    {
      "epoch": 1.9578666666666666,
      "grad_norm": 0.37562069296836853,
      "learning_rate": 3.776333333333333e-05,
      "loss": 0.004,
      "step": 36710
    },
    {
      "epoch": 1.9584000000000001,
      "grad_norm": 0.11663472652435303,
      "learning_rate": 3.776e-05,
      "loss": 0.0035,
      "step": 36720
    },
    {
      "epoch": 1.9589333333333334,
      "grad_norm": 0.06358218938112259,
      "learning_rate": 3.775666666666667e-05,
      "loss": 0.0042,
      "step": 36730
    },
    {
      "epoch": 1.9594666666666667,
      "grad_norm": 0.37540385127067566,
      "learning_rate": 3.775333333333334e-05,
      "loss": 0.0027,
      "step": 36740
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.527951180934906,
      "learning_rate": 3.775e-05,
      "loss": 0.0039,
      "step": 36750
    },
    {
      "epoch": 1.9605333333333332,
      "grad_norm": 0.267076313495636,
      "learning_rate": 3.774666666666667e-05,
      "loss": 0.0044,
      "step": 36760
    },
    {
      "epoch": 1.9610666666666665,
      "grad_norm": 0.04556121677160263,
      "learning_rate": 3.7743333333333335e-05,
      "loss": 0.0026,
      "step": 36770
    },
    {
      "epoch": 1.9616,
      "grad_norm": 0.14908024668693542,
      "learning_rate": 3.774e-05,
      "loss": 0.0032,
      "step": 36780
    },
    {
      "epoch": 1.9621333333333333,
      "grad_norm": 0.4830877482891083,
      "learning_rate": 3.773666666666667e-05,
      "loss": 0.0037,
      "step": 36790
    },
    {
      "epoch": 1.9626666666666668,
      "grad_norm": 0.34881719946861267,
      "learning_rate": 3.773333333333334e-05,
      "loss": 0.0035,
      "step": 36800
    },
    {
      "epoch": 1.9632,
      "grad_norm": 0.34969496726989746,
      "learning_rate": 3.7730000000000006e-05,
      "loss": 0.0032,
      "step": 36810
    },
    {
      "epoch": 1.9637333333333333,
      "grad_norm": 0.497016966342926,
      "learning_rate": 3.7726666666666666e-05,
      "loss": 0.0017,
      "step": 36820
    },
    {
      "epoch": 1.9642666666666666,
      "grad_norm": 0.1491055190563202,
      "learning_rate": 3.772333333333333e-05,
      "loss": 0.0023,
      "step": 36830
    },
    {
      "epoch": 1.9647999999999999,
      "grad_norm": 0.2661440670490265,
      "learning_rate": 3.772e-05,
      "loss": 0.002,
      "step": 36840
    },
    {
      "epoch": 1.9653333333333334,
      "grad_norm": 0.06893114000558853,
      "learning_rate": 3.7716666666666664e-05,
      "loss": 0.0023,
      "step": 36850
    },
    {
      "epoch": 1.9658666666666667,
      "grad_norm": 0.2038750797510147,
      "learning_rate": 3.771333333333334e-05,
      "loss": 0.0023,
      "step": 36860
    },
    {
      "epoch": 1.9664000000000001,
      "grad_norm": 0.09176059067249298,
      "learning_rate": 3.771e-05,
      "loss": 0.0021,
      "step": 36870
    },
    {
      "epoch": 1.9669333333333334,
      "grad_norm": 0.3531794250011444,
      "learning_rate": 3.770666666666667e-05,
      "loss": 0.0023,
      "step": 36880
    },
    {
      "epoch": 1.9674666666666667,
      "grad_norm": 0.6996546387672424,
      "learning_rate": 3.7703333333333335e-05,
      "loss": 0.0017,
      "step": 36890
    },
    {
      "epoch": 1.968,
      "grad_norm": 0.26515060663223267,
      "learning_rate": 3.77e-05,
      "loss": 0.0037,
      "step": 36900
    },
    {
      "epoch": 1.9685333333333332,
      "grad_norm": 0.468001127243042,
      "learning_rate": 3.769666666666667e-05,
      "loss": 0.0024,
      "step": 36910
    },
    {
      "epoch": 1.9690666666666665,
      "grad_norm": 0.026740940287709236,
      "learning_rate": 3.7693333333333334e-05,
      "loss": 0.0026,
      "step": 36920
    },
    {
      "epoch": 1.9696,
      "grad_norm": 0.11506606638431549,
      "learning_rate": 3.769e-05,
      "loss": 0.0016,
      "step": 36930
    },
    {
      "epoch": 1.9701333333333333,
      "grad_norm": 0.20511381328105927,
      "learning_rate": 3.768666666666667e-05,
      "loss": 0.0029,
      "step": 36940
    },
    {
      "epoch": 1.9706666666666668,
      "grad_norm": 0.05380672216415405,
      "learning_rate": 3.768333333333334e-05,
      "loss": 0.0024,
      "step": 36950
    },
    {
      "epoch": 1.9712,
      "grad_norm": 0.2904476523399353,
      "learning_rate": 3.7680000000000005e-05,
      "loss": 0.0036,
      "step": 36960
    },
    {
      "epoch": 1.9717333333333333,
      "grad_norm": 0.2063567340373993,
      "learning_rate": 3.767666666666667e-05,
      "loss": 0.0035,
      "step": 36970
    },
    {
      "epoch": 1.9722666666666666,
      "grad_norm": 0.2623629570007324,
      "learning_rate": 3.767333333333333e-05,
      "loss": 0.0035,
      "step": 36980
    },
    {
      "epoch": 1.9727999999999999,
      "grad_norm": 0.11886465549468994,
      "learning_rate": 3.767e-05,
      "loss": 0.003,
      "step": 36990
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 0.46943122148513794,
      "learning_rate": 3.766666666666667e-05,
      "loss": 0.004,
      "step": 37000
    },
    {
      "epoch": 1.9738666666666667,
      "grad_norm": 0.06266440451145172,
      "learning_rate": 3.7663333333333336e-05,
      "loss": 0.0021,
      "step": 37010
    },
    {
      "epoch": 1.9744000000000002,
      "grad_norm": 0.41039279103279114,
      "learning_rate": 3.766e-05,
      "loss": 0.0026,
      "step": 37020
    },
    {
      "epoch": 1.9749333333333334,
      "grad_norm": 0.2705881595611572,
      "learning_rate": 3.765666666666667e-05,
      "loss": 0.003,
      "step": 37030
    },
    {
      "epoch": 1.9754666666666667,
      "grad_norm": 0.43349412083625793,
      "learning_rate": 3.7653333333333334e-05,
      "loss": 0.0027,
      "step": 37040
    },
    {
      "epoch": 1.976,
      "grad_norm": 0.34802594780921936,
      "learning_rate": 3.765e-05,
      "loss": 0.0026,
      "step": 37050
    },
    {
      "epoch": 1.9765333333333333,
      "grad_norm": 0.29691368341445923,
      "learning_rate": 3.7646666666666666e-05,
      "loss": 0.002,
      "step": 37060
    },
    {
      "epoch": 1.9770666666666665,
      "grad_norm": 0.2305779904127121,
      "learning_rate": 3.764333333333333e-05,
      "loss": 0.004,
      "step": 37070
    },
    {
      "epoch": 1.9776,
      "grad_norm": 0.291595458984375,
      "learning_rate": 3.7640000000000006e-05,
      "loss": 0.0021,
      "step": 37080
    },
    {
      "epoch": 1.9781333333333333,
      "grad_norm": 0.37399041652679443,
      "learning_rate": 3.763666666666667e-05,
      "loss": 0.0028,
      "step": 37090
    },
    {
      "epoch": 1.9786666666666668,
      "grad_norm": 0.2936627268791199,
      "learning_rate": 3.763333333333334e-05,
      "loss": 0.0028,
      "step": 37100
    },
    {
      "epoch": 1.9792,
      "grad_norm": 0.04269609972834587,
      "learning_rate": 3.7630000000000004e-05,
      "loss": 0.0024,
      "step": 37110
    },
    {
      "epoch": 1.9797333333333333,
      "grad_norm": 0.2598036527633667,
      "learning_rate": 3.762666666666667e-05,
      "loss": 0.0024,
      "step": 37120
    },
    {
      "epoch": 1.9802666666666666,
      "grad_norm": 0.17641428112983704,
      "learning_rate": 3.762333333333333e-05,
      "loss": 0.0041,
      "step": 37130
    },
    {
      "epoch": 1.9808,
      "grad_norm": 0.6727248430252075,
      "learning_rate": 3.762e-05,
      "loss": 0.0042,
      "step": 37140
    },
    {
      "epoch": 1.9813333333333332,
      "grad_norm": 0.20900322496891022,
      "learning_rate": 3.761666666666667e-05,
      "loss": 0.0025,
      "step": 37150
    },
    {
      "epoch": 1.9818666666666667,
      "grad_norm": 0.4677784740924835,
      "learning_rate": 3.7613333333333335e-05,
      "loss": 0.0023,
      "step": 37160
    },
    {
      "epoch": 1.9824000000000002,
      "grad_norm": 0.175206258893013,
      "learning_rate": 3.761e-05,
      "loss": 0.0025,
      "step": 37170
    },
    {
      "epoch": 1.9829333333333334,
      "grad_norm": 0.2886362671852112,
      "learning_rate": 3.760666666666667e-05,
      "loss": 0.0019,
      "step": 37180
    },
    {
      "epoch": 1.9834666666666667,
      "grad_norm": 0.17636944353580475,
      "learning_rate": 3.760333333333333e-05,
      "loss": 0.003,
      "step": 37190
    },
    {
      "epoch": 1.984,
      "grad_norm": 0.148662269115448,
      "learning_rate": 3.76e-05,
      "loss": 0.0032,
      "step": 37200
    },
    {
      "epoch": 1.9845333333333333,
      "grad_norm": 0.11679384112358093,
      "learning_rate": 3.759666666666667e-05,
      "loss": 0.0024,
      "step": 37210
    },
    {
      "epoch": 1.9850666666666665,
      "grad_norm": 0.2152034342288971,
      "learning_rate": 3.759333333333334e-05,
      "loss": 0.0029,
      "step": 37220
    },
    {
      "epoch": 1.9856,
      "grad_norm": 0.06728367507457733,
      "learning_rate": 3.7590000000000004e-05,
      "loss": 0.0041,
      "step": 37230
    },
    {
      "epoch": 1.9861333333333333,
      "grad_norm": 0.03513283655047417,
      "learning_rate": 3.758666666666667e-05,
      "loss": 0.0035,
      "step": 37240
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 0.14961199462413788,
      "learning_rate": 3.7583333333333337e-05,
      "loss": 0.0026,
      "step": 37250
    },
    {
      "epoch": 1.9872,
      "grad_norm": 0.046388883143663406,
      "learning_rate": 3.758e-05,
      "loss": 0.0027,
      "step": 37260
    },
    {
      "epoch": 1.9877333333333334,
      "grad_norm": 0.1806676834821701,
      "learning_rate": 3.757666666666667e-05,
      "loss": 0.0032,
      "step": 37270
    },
    {
      "epoch": 1.9882666666666666,
      "grad_norm": 0.14940960705280304,
      "learning_rate": 3.7573333333333335e-05,
      "loss": 0.0019,
      "step": 37280
    },
    {
      "epoch": 1.9888,
      "grad_norm": 0.18122203648090363,
      "learning_rate": 3.757e-05,
      "loss": 0.0033,
      "step": 37290
    },
    {
      "epoch": 1.9893333333333332,
      "grad_norm": 0.3540058135986328,
      "learning_rate": 3.756666666666667e-05,
      "loss": 0.0046,
      "step": 37300
    },
    {
      "epoch": 1.9898666666666667,
      "grad_norm": 0.4136180877685547,
      "learning_rate": 3.7563333333333333e-05,
      "loss": 0.0026,
      "step": 37310
    },
    {
      "epoch": 1.9904,
      "grad_norm": 0.23466375470161438,
      "learning_rate": 3.756e-05,
      "loss": 0.003,
      "step": 37320
    },
    {
      "epoch": 1.9909333333333334,
      "grad_norm": 0.3165585398674011,
      "learning_rate": 3.7556666666666666e-05,
      "loss": 0.0039,
      "step": 37330
    },
    {
      "epoch": 1.9914666666666667,
      "grad_norm": 0.3498145341873169,
      "learning_rate": 3.755333333333333e-05,
      "loss": 0.0028,
      "step": 37340
    },
    {
      "epoch": 1.992,
      "grad_norm": 0.03629234433174133,
      "learning_rate": 3.7550000000000005e-05,
      "loss": 0.0018,
      "step": 37350
    },
    {
      "epoch": 1.9925333333333333,
      "grad_norm": 0.32262998819351196,
      "learning_rate": 3.754666666666667e-05,
      "loss": 0.0021,
      "step": 37360
    },
    {
      "epoch": 1.9930666666666665,
      "grad_norm": 0.18056702613830566,
      "learning_rate": 3.754333333333334e-05,
      "loss": 0.0021,
      "step": 37370
    },
    {
      "epoch": 1.9936,
      "grad_norm": 0.617242693901062,
      "learning_rate": 3.754e-05,
      "loss": 0.0028,
      "step": 37380
    },
    {
      "epoch": 1.9941333333333333,
      "grad_norm": 0.17416423559188843,
      "learning_rate": 3.753666666666667e-05,
      "loss": 0.0025,
      "step": 37390
    },
    {
      "epoch": 1.9946666666666668,
      "grad_norm": 0.21083064377307892,
      "learning_rate": 3.7533333333333335e-05,
      "loss": 0.0021,
      "step": 37400
    },
    {
      "epoch": 1.9952,
      "grad_norm": 0.2930321991443634,
      "learning_rate": 3.753e-05,
      "loss": 0.003,
      "step": 37410
    },
    {
      "epoch": 1.9957333333333334,
      "grad_norm": 0.41144105792045593,
      "learning_rate": 3.7526666666666674e-05,
      "loss": 0.0032,
      "step": 37420
    },
    {
      "epoch": 1.9962666666666666,
      "grad_norm": 0.06368891149759293,
      "learning_rate": 3.7523333333333334e-05,
      "loss": 0.0033,
      "step": 37430
    },
    {
      "epoch": 1.9968,
      "grad_norm": 0.2938230633735657,
      "learning_rate": 3.752e-05,
      "loss": 0.0028,
      "step": 37440
    },
    {
      "epoch": 1.9973333333333332,
      "grad_norm": 0.06002428010106087,
      "learning_rate": 3.7516666666666666e-05,
      "loss": 0.0029,
      "step": 37450
    },
    {
      "epoch": 1.9978666666666667,
      "grad_norm": 0.2639325261116028,
      "learning_rate": 3.751333333333333e-05,
      "loss": 0.0024,
      "step": 37460
    },
    {
      "epoch": 1.9984,
      "grad_norm": 0.08745185285806656,
      "learning_rate": 3.751e-05,
      "loss": 0.0032,
      "step": 37470
    },
    {
      "epoch": 1.9989333333333335,
      "grad_norm": 0.03734452277421951,
      "learning_rate": 3.7506666666666664e-05,
      "loss": 0.0027,
      "step": 37480
    },
    {
      "epoch": 1.9994666666666667,
      "grad_norm": 0.04489946365356445,
      "learning_rate": 3.750333333333334e-05,
      "loss": 0.0029,
      "step": 37490
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.18562540411949158,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.003,
      "step": 37500
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.0030731172300875187,
      "eval_runtime": 172.7367,
      "eval_samples_per_second": 1447.289,
      "eval_steps_per_second": 36.182,
      "step": 37500
    },
    {
      "epoch": 2.0005333333333333,
      "grad_norm": 0.27326643466949463,
      "learning_rate": 3.749666666666667e-05,
      "loss": 0.0026,
      "step": 37510
    },
    {
      "epoch": 2.0010666666666665,
      "grad_norm": 0.47038573026657104,
      "learning_rate": 3.7493333333333336e-05,
      "loss": 0.003,
      "step": 37520
    },
    {
      "epoch": 2.0016,
      "grad_norm": 0.32172852754592896,
      "learning_rate": 3.749e-05,
      "loss": 0.0035,
      "step": 37530
    },
    {
      "epoch": 2.0021333333333335,
      "grad_norm": 0.778455913066864,
      "learning_rate": 3.748666666666667e-05,
      "loss": 0.0039,
      "step": 37540
    },
    {
      "epoch": 2.002666666666667,
      "grad_norm": 0.548008918762207,
      "learning_rate": 3.7483333333333334e-05,
      "loss": 0.003,
      "step": 37550
    },
    {
      "epoch": 2.0032,
      "grad_norm": 0.180838480591774,
      "learning_rate": 3.748000000000001e-05,
      "loss": 0.004,
      "step": 37560
    },
    {
      "epoch": 2.0037333333333334,
      "grad_norm": 0.11936162412166595,
      "learning_rate": 3.747666666666667e-05,
      "loss": 0.0027,
      "step": 37570
    },
    {
      "epoch": 2.0042666666666666,
      "grad_norm": 0.14995139837265015,
      "learning_rate": 3.747333333333333e-05,
      "loss": 0.0027,
      "step": 37580
    },
    {
      "epoch": 2.0048,
      "grad_norm": 0.43130144476890564,
      "learning_rate": 3.747e-05,
      "loss": 0.0041,
      "step": 37590
    },
    {
      "epoch": 2.005333333333333,
      "grad_norm": 0.3788265585899353,
      "learning_rate": 3.7466666666666665e-05,
      "loss": 0.0019,
      "step": 37600
    },
    {
      "epoch": 2.0058666666666665,
      "grad_norm": 0.31849008798599243,
      "learning_rate": 3.746333333333333e-05,
      "loss": 0.0032,
      "step": 37610
    },
    {
      "epoch": 2.0064,
      "grad_norm": 0.6184820532798767,
      "learning_rate": 3.7460000000000004e-05,
      "loss": 0.0025,
      "step": 37620
    },
    {
      "epoch": 2.0069333333333335,
      "grad_norm": 0.11942006647586823,
      "learning_rate": 3.745666666666667e-05,
      "loss": 0.0023,
      "step": 37630
    },
    {
      "epoch": 2.0074666666666667,
      "grad_norm": 0.5832454562187195,
      "learning_rate": 3.7453333333333336e-05,
      "loss": 0.0025,
      "step": 37640
    },
    {
      "epoch": 2.008,
      "grad_norm": 0.5522409677505493,
      "learning_rate": 3.745e-05,
      "loss": 0.0026,
      "step": 37650
    },
    {
      "epoch": 2.0085333333333333,
      "grad_norm": 0.2958088517189026,
      "learning_rate": 3.744666666666667e-05,
      "loss": 0.0021,
      "step": 37660
    },
    {
      "epoch": 2.0090666666666666,
      "grad_norm": 0.19173423945903778,
      "learning_rate": 3.7443333333333335e-05,
      "loss": 0.0017,
      "step": 37670
    },
    {
      "epoch": 2.0096,
      "grad_norm": 0.527366578578949,
      "learning_rate": 3.744e-05,
      "loss": 0.0037,
      "step": 37680
    },
    {
      "epoch": 2.0101333333333335,
      "grad_norm": 0.21189290285110474,
      "learning_rate": 3.743666666666667e-05,
      "loss": 0.0026,
      "step": 37690
    },
    {
      "epoch": 2.010666666666667,
      "grad_norm": 0.41035032272338867,
      "learning_rate": 3.743333333333334e-05,
      "loss": 0.003,
      "step": 37700
    },
    {
      "epoch": 2.0112,
      "grad_norm": 0.14592428505420685,
      "learning_rate": 3.7430000000000006e-05,
      "loss": 0.0039,
      "step": 37710
    },
    {
      "epoch": 2.0117333333333334,
      "grad_norm": 0.1290712058544159,
      "learning_rate": 3.742666666666667e-05,
      "loss": 0.0027,
      "step": 37720
    },
    {
      "epoch": 2.0122666666666666,
      "grad_norm": 0.1461731493473053,
      "learning_rate": 3.742333333333333e-05,
      "loss": 0.0035,
      "step": 37730
    },
    {
      "epoch": 2.0128,
      "grad_norm": 0.0735013335943222,
      "learning_rate": 3.742e-05,
      "loss": 0.0021,
      "step": 37740
    },
    {
      "epoch": 2.013333333333333,
      "grad_norm": 0.43020451068878174,
      "learning_rate": 3.7416666666666664e-05,
      "loss": 0.0024,
      "step": 37750
    },
    {
      "epoch": 2.0138666666666665,
      "grad_norm": 0.2596158981323242,
      "learning_rate": 3.7413333333333337e-05,
      "loss": 0.0029,
      "step": 37760
    },
    {
      "epoch": 2.0144,
      "grad_norm": 0.03262457996606827,
      "learning_rate": 3.741e-05,
      "loss": 0.0023,
      "step": 37770
    },
    {
      "epoch": 2.0149333333333335,
      "grad_norm": 0.34790757298469543,
      "learning_rate": 3.740666666666667e-05,
      "loss": 0.0029,
      "step": 37780
    },
    {
      "epoch": 2.0154666666666667,
      "grad_norm": 0.20605258643627167,
      "learning_rate": 3.7403333333333335e-05,
      "loss": 0.0028,
      "step": 37790
    },
    {
      "epoch": 2.016,
      "grad_norm": 0.23133635520935059,
      "learning_rate": 3.74e-05,
      "loss": 0.0024,
      "step": 37800
    },
    {
      "epoch": 2.0165333333333333,
      "grad_norm": 0.2639589309692383,
      "learning_rate": 3.739666666666667e-05,
      "loss": 0.0026,
      "step": 37810
    },
    {
      "epoch": 2.0170666666666666,
      "grad_norm": 0.35087689757347107,
      "learning_rate": 3.739333333333333e-05,
      "loss": 0.0035,
      "step": 37820
    },
    {
      "epoch": 2.0176,
      "grad_norm": 0.17536288499832153,
      "learning_rate": 3.739e-05,
      "loss": 0.0037,
      "step": 37830
    },
    {
      "epoch": 2.018133333333333,
      "grad_norm": 0.4103878140449524,
      "learning_rate": 3.738666666666667e-05,
      "loss": 0.0019,
      "step": 37840
    },
    {
      "epoch": 2.018666666666667,
      "grad_norm": 0.3245941996574402,
      "learning_rate": 3.738333333333334e-05,
      "loss": 0.0041,
      "step": 37850
    },
    {
      "epoch": 2.0192,
      "grad_norm": 0.35768333077430725,
      "learning_rate": 3.7380000000000005e-05,
      "loss": 0.0048,
      "step": 37860
    },
    {
      "epoch": 2.0197333333333334,
      "grad_norm": 0.06398525089025497,
      "learning_rate": 3.737666666666667e-05,
      "loss": 0.0025,
      "step": 37870
    },
    {
      "epoch": 2.0202666666666667,
      "grad_norm": 0.1195610910654068,
      "learning_rate": 3.737333333333333e-05,
      "loss": 0.0035,
      "step": 37880
    },
    {
      "epoch": 2.0208,
      "grad_norm": 0.26770254969596863,
      "learning_rate": 3.7369999999999996e-05,
      "loss": 0.003,
      "step": 37890
    },
    {
      "epoch": 2.021333333333333,
      "grad_norm": 0.3010140359401703,
      "learning_rate": 3.736666666666667e-05,
      "loss": 0.002,
      "step": 37900
    },
    {
      "epoch": 2.0218666666666665,
      "grad_norm": 0.4054647386074066,
      "learning_rate": 3.7363333333333335e-05,
      "loss": 0.0033,
      "step": 37910
    },
    {
      "epoch": 2.0224,
      "grad_norm": 0.2898918688297272,
      "learning_rate": 3.736e-05,
      "loss": 0.0025,
      "step": 37920
    },
    {
      "epoch": 2.0229333333333335,
      "grad_norm": 0.0924767255783081,
      "learning_rate": 3.735666666666667e-05,
      "loss": 0.0035,
      "step": 37930
    },
    {
      "epoch": 2.0234666666666667,
      "grad_norm": 0.08664517849683762,
      "learning_rate": 3.7353333333333334e-05,
      "loss": 0.0023,
      "step": 37940
    },
    {
      "epoch": 2.024,
      "grad_norm": 0.17398561537265778,
      "learning_rate": 3.735e-05,
      "loss": 0.0026,
      "step": 37950
    },
    {
      "epoch": 2.0245333333333333,
      "grad_norm": 0.08979111909866333,
      "learning_rate": 3.7346666666666666e-05,
      "loss": 0.0026,
      "step": 37960
    },
    {
      "epoch": 2.0250666666666666,
      "grad_norm": 0.2705306112766266,
      "learning_rate": 3.734333333333334e-05,
      "loss": 0.002,
      "step": 37970
    },
    {
      "epoch": 2.0256,
      "grad_norm": 0.6118394136428833,
      "learning_rate": 3.7340000000000005e-05,
      "loss": 0.0028,
      "step": 37980
    },
    {
      "epoch": 2.026133333333333,
      "grad_norm": 0.2688387334346771,
      "learning_rate": 3.733666666666667e-05,
      "loss": 0.0031,
      "step": 37990
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 0.11536170542240143,
      "learning_rate": 3.733333333333334e-05,
      "loss": 0.0039,
      "step": 38000
    },
    {
      "epoch": 2.0272,
      "grad_norm": 0.20434099435806274,
      "learning_rate": 3.7330000000000003e-05,
      "loss": 0.0026,
      "step": 38010
    },
    {
      "epoch": 2.0277333333333334,
      "grad_norm": 0.34492629766464233,
      "learning_rate": 3.732666666666667e-05,
      "loss": 0.0045,
      "step": 38020
    },
    {
      "epoch": 2.0282666666666667,
      "grad_norm": 0.2071688324213028,
      "learning_rate": 3.7323333333333336e-05,
      "loss": 0.0031,
      "step": 38030
    },
    {
      "epoch": 2.0288,
      "grad_norm": 0.615651547908783,
      "learning_rate": 3.732e-05,
      "loss": 0.0026,
      "step": 38040
    },
    {
      "epoch": 2.029333333333333,
      "grad_norm": 0.20422813296318054,
      "learning_rate": 3.731666666666667e-05,
      "loss": 0.003,
      "step": 38050
    },
    {
      "epoch": 2.0298666666666665,
      "grad_norm": 0.06329304724931717,
      "learning_rate": 3.7313333333333334e-05,
      "loss": 0.0025,
      "step": 38060
    },
    {
      "epoch": 2.0304,
      "grad_norm": 0.2683523893356323,
      "learning_rate": 3.731e-05,
      "loss": 0.002,
      "step": 38070
    },
    {
      "epoch": 2.0309333333333335,
      "grad_norm": 0.1820608526468277,
      "learning_rate": 3.7306666666666666e-05,
      "loss": 0.003,
      "step": 38080
    },
    {
      "epoch": 2.0314666666666668,
      "grad_norm": 0.20601999759674072,
      "learning_rate": 3.730333333333333e-05,
      "loss": 0.0026,
      "step": 38090
    },
    {
      "epoch": 2.032,
      "grad_norm": 0.4111713469028473,
      "learning_rate": 3.73e-05,
      "loss": 0.0021,
      "step": 38100
    },
    {
      "epoch": 2.0325333333333333,
      "grad_norm": 0.037253063172101974,
      "learning_rate": 3.729666666666667e-05,
      "loss": 0.0037,
      "step": 38110
    },
    {
      "epoch": 2.0330666666666666,
      "grad_norm": 0.4043566584587097,
      "learning_rate": 3.729333333333334e-05,
      "loss": 0.0031,
      "step": 38120
    },
    {
      "epoch": 2.0336,
      "grad_norm": 0.2419271469116211,
      "learning_rate": 3.7290000000000004e-05,
      "loss": 0.0029,
      "step": 38130
    },
    {
      "epoch": 2.034133333333333,
      "grad_norm": 0.2971358001232147,
      "learning_rate": 3.728666666666667e-05,
      "loss": 0.0021,
      "step": 38140
    },
    {
      "epoch": 2.034666666666667,
      "grad_norm": 0.5669562816619873,
      "learning_rate": 3.7283333333333336e-05,
      "loss": 0.0032,
      "step": 38150
    },
    {
      "epoch": 2.0352,
      "grad_norm": 0.6476784348487854,
      "learning_rate": 3.728e-05,
      "loss": 0.0047,
      "step": 38160
    },
    {
      "epoch": 2.0357333333333334,
      "grad_norm": 0.03570381924510002,
      "learning_rate": 3.727666666666667e-05,
      "loss": 0.0028,
      "step": 38170
    },
    {
      "epoch": 2.0362666666666667,
      "grad_norm": 0.14563538134098053,
      "learning_rate": 3.727333333333334e-05,
      "loss": 0.0022,
      "step": 38180
    },
    {
      "epoch": 2.0368,
      "grad_norm": 0.3485339879989624,
      "learning_rate": 3.727e-05,
      "loss": 0.0029,
      "step": 38190
    },
    {
      "epoch": 2.037333333333333,
      "grad_norm": 0.28895020484924316,
      "learning_rate": 3.726666666666667e-05,
      "loss": 0.0027,
      "step": 38200
    },
    {
      "epoch": 2.0378666666666665,
      "grad_norm": 0.261736661195755,
      "learning_rate": 3.726333333333333e-05,
      "loss": 0.0037,
      "step": 38210
    },
    {
      "epoch": 2.0384,
      "grad_norm": 0.3799189329147339,
      "learning_rate": 3.726e-05,
      "loss": 0.0028,
      "step": 38220
    },
    {
      "epoch": 2.0389333333333335,
      "grad_norm": 0.15026643872261047,
      "learning_rate": 3.7256666666666665e-05,
      "loss": 0.0031,
      "step": 38230
    },
    {
      "epoch": 2.0394666666666668,
      "grad_norm": 0.5339528322219849,
      "learning_rate": 3.725333333333333e-05,
      "loss": 0.0024,
      "step": 38240
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.33207252621650696,
      "learning_rate": 3.7250000000000004e-05,
      "loss": 0.0034,
      "step": 38250
    },
    {
      "epoch": 2.0405333333333333,
      "grad_norm": 0.2085653841495514,
      "learning_rate": 3.724666666666667e-05,
      "loss": 0.0026,
      "step": 38260
    },
    {
      "epoch": 2.0410666666666666,
      "grad_norm": 0.14561550319194794,
      "learning_rate": 3.7243333333333336e-05,
      "loss": 0.0023,
      "step": 38270
    },
    {
      "epoch": 2.0416,
      "grad_norm": 0.04405828192830086,
      "learning_rate": 3.724e-05,
      "loss": 0.002,
      "step": 38280
    },
    {
      "epoch": 2.042133333333333,
      "grad_norm": 0.6638631224632263,
      "learning_rate": 3.723666666666667e-05,
      "loss": 0.0029,
      "step": 38290
    },
    {
      "epoch": 2.042666666666667,
      "grad_norm": 0.2916635572910309,
      "learning_rate": 3.7233333333333335e-05,
      "loss": 0.0026,
      "step": 38300
    },
    {
      "epoch": 2.0432,
      "grad_norm": 0.3168569803237915,
      "learning_rate": 3.723e-05,
      "loss": 0.0032,
      "step": 38310
    },
    {
      "epoch": 2.0437333333333334,
      "grad_norm": 0.20452779531478882,
      "learning_rate": 3.7226666666666674e-05,
      "loss": 0.0024,
      "step": 38320
    },
    {
      "epoch": 2.0442666666666667,
      "grad_norm": 0.4616124927997589,
      "learning_rate": 3.722333333333334e-05,
      "loss": 0.0023,
      "step": 38330
    },
    {
      "epoch": 2.0448,
      "grad_norm": 0.020535103976726532,
      "learning_rate": 3.722e-05,
      "loss": 0.0024,
      "step": 38340
    },
    {
      "epoch": 2.0453333333333332,
      "grad_norm": 0.666923463344574,
      "learning_rate": 3.7216666666666666e-05,
      "loss": 0.0037,
      "step": 38350
    },
    {
      "epoch": 2.0458666666666665,
      "grad_norm": 0.7040517926216125,
      "learning_rate": 3.721333333333333e-05,
      "loss": 0.0029,
      "step": 38360
    },
    {
      "epoch": 2.0464,
      "grad_norm": 0.911071240901947,
      "learning_rate": 3.721e-05,
      "loss": 0.0015,
      "step": 38370
    },
    {
      "epoch": 2.0469333333333335,
      "grad_norm": 0.48011794686317444,
      "learning_rate": 3.720666666666667e-05,
      "loss": 0.0028,
      "step": 38380
    },
    {
      "epoch": 2.0474666666666668,
      "grad_norm": 0.45082658529281616,
      "learning_rate": 3.720333333333334e-05,
      "loss": 0.0028,
      "step": 38390
    },
    {
      "epoch": 2.048,
      "grad_norm": 0.3531247675418854,
      "learning_rate": 3.72e-05,
      "loss": 0.0029,
      "step": 38400
    },
    {
      "epoch": 2.0485333333333333,
      "grad_norm": 0.5261313319206238,
      "learning_rate": 3.719666666666667e-05,
      "loss": 0.0025,
      "step": 38410
    },
    {
      "epoch": 2.0490666666666666,
      "grad_norm": 0.17540931701660156,
      "learning_rate": 3.7193333333333335e-05,
      "loss": 0.003,
      "step": 38420
    },
    {
      "epoch": 2.0496,
      "grad_norm": 0.04295342043042183,
      "learning_rate": 3.719e-05,
      "loss": 0.0029,
      "step": 38430
    },
    {
      "epoch": 2.050133333333333,
      "grad_norm": 0.386344850063324,
      "learning_rate": 3.718666666666667e-05,
      "loss": 0.0043,
      "step": 38440
    },
    {
      "epoch": 2.050666666666667,
      "grad_norm": 0.17726731300354004,
      "learning_rate": 3.7183333333333334e-05,
      "loss": 0.0044,
      "step": 38450
    },
    {
      "epoch": 2.0512,
      "grad_norm": 0.2342561036348343,
      "learning_rate": 3.7180000000000007e-05,
      "loss": 0.0027,
      "step": 38460
    },
    {
      "epoch": 2.0517333333333334,
      "grad_norm": 0.43756216764450073,
      "learning_rate": 3.717666666666667e-05,
      "loss": 0.003,
      "step": 38470
    },
    {
      "epoch": 2.0522666666666667,
      "grad_norm": 0.09430708736181259,
      "learning_rate": 3.717333333333334e-05,
      "loss": 0.0027,
      "step": 38480
    },
    {
      "epoch": 2.0528,
      "grad_norm": 0.23537501692771912,
      "learning_rate": 3.717e-05,
      "loss": 0.002,
      "step": 38490
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 0.03389372304081917,
      "learning_rate": 3.7166666666666664e-05,
      "loss": 0.0028,
      "step": 38500
    },
    {
      "epoch": 2.0538666666666665,
      "grad_norm": 0.3987843096256256,
      "learning_rate": 3.716333333333333e-05,
      "loss": 0.0025,
      "step": 38510
    },
    {
      "epoch": 2.0544,
      "grad_norm": 0.06132757291197777,
      "learning_rate": 3.716e-05,
      "loss": 0.0027,
      "step": 38520
    },
    {
      "epoch": 2.0549333333333335,
      "grad_norm": 0.6490512490272522,
      "learning_rate": 3.715666666666667e-05,
      "loss": 0.0029,
      "step": 38530
    },
    {
      "epoch": 2.0554666666666668,
      "grad_norm": 0.2040964961051941,
      "learning_rate": 3.7153333333333336e-05,
      "loss": 0.0024,
      "step": 38540
    },
    {
      "epoch": 2.056,
      "grad_norm": 0.3545452356338501,
      "learning_rate": 3.715e-05,
      "loss": 0.002,
      "step": 38550
    },
    {
      "epoch": 2.0565333333333333,
      "grad_norm": 0.38555410504341125,
      "learning_rate": 3.714666666666667e-05,
      "loss": 0.0046,
      "step": 38560
    },
    {
      "epoch": 2.0570666666666666,
      "grad_norm": 0.3794212341308594,
      "learning_rate": 3.7143333333333334e-05,
      "loss": 0.0031,
      "step": 38570
    },
    {
      "epoch": 2.0576,
      "grad_norm": 0.18151327967643738,
      "learning_rate": 3.714e-05,
      "loss": 0.0026,
      "step": 38580
    },
    {
      "epoch": 2.058133333333333,
      "grad_norm": 0.1850401908159256,
      "learning_rate": 3.7136666666666666e-05,
      "loss": 0.003,
      "step": 38590
    },
    {
      "epoch": 2.058666666666667,
      "grad_norm": 0.173880934715271,
      "learning_rate": 3.713333333333334e-05,
      "loss": 0.0021,
      "step": 38600
    },
    {
      "epoch": 2.0592,
      "grad_norm": 0.6346012353897095,
      "learning_rate": 3.7130000000000005e-05,
      "loss": 0.0034,
      "step": 38610
    },
    {
      "epoch": 2.0597333333333334,
      "grad_norm": 0.17418761551380157,
      "learning_rate": 3.712666666666667e-05,
      "loss": 0.0025,
      "step": 38620
    },
    {
      "epoch": 2.0602666666666667,
      "grad_norm": 0.43429625034332275,
      "learning_rate": 3.712333333333334e-05,
      "loss": 0.0019,
      "step": 38630
    },
    {
      "epoch": 2.0608,
      "grad_norm": 0.3835592567920685,
      "learning_rate": 3.712e-05,
      "loss": 0.0023,
      "step": 38640
    },
    {
      "epoch": 2.0613333333333332,
      "grad_norm": 0.3971126675605774,
      "learning_rate": 3.711666666666666e-05,
      "loss": 0.0028,
      "step": 38650
    },
    {
      "epoch": 2.0618666666666665,
      "grad_norm": 0.3740282356739044,
      "learning_rate": 3.7113333333333336e-05,
      "loss": 0.0034,
      "step": 38660
    },
    {
      "epoch": 2.0624,
      "grad_norm": 0.42094069719314575,
      "learning_rate": 3.711e-05,
      "loss": 0.0023,
      "step": 38670
    },
    {
      "epoch": 2.0629333333333335,
      "grad_norm": 0.03285513073205948,
      "learning_rate": 3.710666666666667e-05,
      "loss": 0.0037,
      "step": 38680
    },
    {
      "epoch": 2.063466666666667,
      "grad_norm": 0.18800349533557892,
      "learning_rate": 3.7103333333333334e-05,
      "loss": 0.0027,
      "step": 38690
    },
    {
      "epoch": 2.064,
      "grad_norm": 0.48631489276885986,
      "learning_rate": 3.71e-05,
      "loss": 0.0042,
      "step": 38700
    },
    {
      "epoch": 2.0645333333333333,
      "grad_norm": 0.4467025399208069,
      "learning_rate": 3.709666666666667e-05,
      "loss": 0.0031,
      "step": 38710
    },
    {
      "epoch": 2.0650666666666666,
      "grad_norm": 0.29622671008110046,
      "learning_rate": 3.709333333333333e-05,
      "loss": 0.0056,
      "step": 38720
    },
    {
      "epoch": 2.0656,
      "grad_norm": 0.333316445350647,
      "learning_rate": 3.7090000000000006e-05,
      "loss": 0.0046,
      "step": 38730
    },
    {
      "epoch": 2.066133333333333,
      "grad_norm": 0.5503057241439819,
      "learning_rate": 3.708666666666667e-05,
      "loss": 0.002,
      "step": 38740
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 0.18000543117523193,
      "learning_rate": 3.708333333333334e-05,
      "loss": 0.0025,
      "step": 38750
    },
    {
      "epoch": 2.0672,
      "grad_norm": 0.4949406087398529,
      "learning_rate": 3.7080000000000004e-05,
      "loss": 0.003,
      "step": 38760
    },
    {
      "epoch": 2.0677333333333334,
      "grad_norm": 0.17314669489860535,
      "learning_rate": 3.707666666666667e-05,
      "loss": 0.004,
      "step": 38770
    },
    {
      "epoch": 2.0682666666666667,
      "grad_norm": 0.05859595909714699,
      "learning_rate": 3.7073333333333336e-05,
      "loss": 0.0027,
      "step": 38780
    },
    {
      "epoch": 2.0688,
      "grad_norm": 0.2989360988140106,
      "learning_rate": 3.707e-05,
      "loss": 0.0021,
      "step": 38790
    },
    {
      "epoch": 2.0693333333333332,
      "grad_norm": 0.29042819142341614,
      "learning_rate": 3.706666666666667e-05,
      "loss": 0.0031,
      "step": 38800
    },
    {
      "epoch": 2.0698666666666665,
      "grad_norm": 0.24837076663970947,
      "learning_rate": 3.7063333333333335e-05,
      "loss": 0.003,
      "step": 38810
    },
    {
      "epoch": 2.0704,
      "grad_norm": 0.17727923393249512,
      "learning_rate": 3.706e-05,
      "loss": 0.0025,
      "step": 38820
    },
    {
      "epoch": 2.0709333333333335,
      "grad_norm": 0.03070071153342724,
      "learning_rate": 3.705666666666667e-05,
      "loss": 0.0038,
      "step": 38830
    },
    {
      "epoch": 2.071466666666667,
      "grad_norm": 0.5808513164520264,
      "learning_rate": 3.705333333333333e-05,
      "loss": 0.0025,
      "step": 38840
    },
    {
      "epoch": 2.072,
      "grad_norm": 0.519818902015686,
      "learning_rate": 3.705e-05,
      "loss": 0.0039,
      "step": 38850
    },
    {
      "epoch": 2.0725333333333333,
      "grad_norm": 0.2875245213508606,
      "learning_rate": 3.7046666666666665e-05,
      "loss": 0.0034,
      "step": 38860
    },
    {
      "epoch": 2.0730666666666666,
      "grad_norm": 0.2917912006378174,
      "learning_rate": 3.704333333333334e-05,
      "loss": 0.0019,
      "step": 38870
    },
    {
      "epoch": 2.0736,
      "grad_norm": 0.3447931706905365,
      "learning_rate": 3.7040000000000005e-05,
      "loss": 0.0021,
      "step": 38880
    },
    {
      "epoch": 2.074133333333333,
      "grad_norm": 0.11999393254518509,
      "learning_rate": 3.703666666666667e-05,
      "loss": 0.0032,
      "step": 38890
    },
    {
      "epoch": 2.074666666666667,
      "grad_norm": 0.04153253510594368,
      "learning_rate": 3.703333333333334e-05,
      "loss": 0.0036,
      "step": 38900
    },
    {
      "epoch": 2.0752,
      "grad_norm": 0.04926806315779686,
      "learning_rate": 3.703e-05,
      "loss": 0.0024,
      "step": 38910
    },
    {
      "epoch": 2.0757333333333334,
      "grad_norm": 0.3220406472682953,
      "learning_rate": 3.702666666666667e-05,
      "loss": 0.0028,
      "step": 38920
    },
    {
      "epoch": 2.0762666666666667,
      "grad_norm": 0.06013134866952896,
      "learning_rate": 3.7023333333333335e-05,
      "loss": 0.0014,
      "step": 38930
    },
    {
      "epoch": 2.0768,
      "grad_norm": 0.09110915660858154,
      "learning_rate": 3.702e-05,
      "loss": 0.0047,
      "step": 38940
    },
    {
      "epoch": 2.0773333333333333,
      "grad_norm": 0.20404861867427826,
      "learning_rate": 3.701666666666667e-05,
      "loss": 0.0022,
      "step": 38950
    },
    {
      "epoch": 2.0778666666666665,
      "grad_norm": 0.17795494198799133,
      "learning_rate": 3.7013333333333334e-05,
      "loss": 0.0022,
      "step": 38960
    },
    {
      "epoch": 2.0784,
      "grad_norm": 0.35078367590904236,
      "learning_rate": 3.701e-05,
      "loss": 0.0028,
      "step": 38970
    },
    {
      "epoch": 2.0789333333333335,
      "grad_norm": 0.20258527994155884,
      "learning_rate": 3.7006666666666666e-05,
      "loss": 0.0021,
      "step": 38980
    },
    {
      "epoch": 2.079466666666667,
      "grad_norm": 0.08945703506469727,
      "learning_rate": 3.700333333333333e-05,
      "loss": 0.003,
      "step": 38990
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.06809767335653305,
      "learning_rate": 3.7e-05,
      "loss": 0.0026,
      "step": 39000
    },
    {
      "epoch": 2.0805333333333333,
      "grad_norm": 0.3842756450176239,
      "learning_rate": 3.699666666666667e-05,
      "loss": 0.0016,
      "step": 39010
    },
    {
      "epoch": 2.0810666666666666,
      "grad_norm": 0.15449590981006622,
      "learning_rate": 3.699333333333334e-05,
      "loss": 0.0024,
      "step": 39020
    },
    {
      "epoch": 2.0816,
      "grad_norm": 0.46048176288604736,
      "learning_rate": 3.699e-05,
      "loss": 0.0029,
      "step": 39030
    },
    {
      "epoch": 2.082133333333333,
      "grad_norm": 0.0973411500453949,
      "learning_rate": 3.698666666666667e-05,
      "loss": 0.0027,
      "step": 39040
    },
    {
      "epoch": 2.0826666666666664,
      "grad_norm": 0.3229832649230957,
      "learning_rate": 3.6983333333333336e-05,
      "loss": 0.0037,
      "step": 39050
    },
    {
      "epoch": 2.0832,
      "grad_norm": 0.8356843590736389,
      "learning_rate": 3.698e-05,
      "loss": 0.0017,
      "step": 39060
    },
    {
      "epoch": 2.0837333333333334,
      "grad_norm": 0.5524141192436218,
      "learning_rate": 3.697666666666667e-05,
      "loss": 0.0028,
      "step": 39070
    },
    {
      "epoch": 2.0842666666666667,
      "grad_norm": 0.03825139254331589,
      "learning_rate": 3.697333333333334e-05,
      "loss": 0.0024,
      "step": 39080
    },
    {
      "epoch": 2.0848,
      "grad_norm": 0.27408289909362793,
      "learning_rate": 3.697e-05,
      "loss": 0.0024,
      "step": 39090
    },
    {
      "epoch": 2.0853333333333333,
      "grad_norm": 0.20484967529773712,
      "learning_rate": 3.6966666666666666e-05,
      "loss": 0.0036,
      "step": 39100
    },
    {
      "epoch": 2.0858666666666665,
      "grad_norm": 0.06084706261754036,
      "learning_rate": 3.696333333333333e-05,
      "loss": 0.0029,
      "step": 39110
    },
    {
      "epoch": 2.0864,
      "grad_norm": 0.06270331144332886,
      "learning_rate": 3.696e-05,
      "loss": 0.002,
      "step": 39120
    },
    {
      "epoch": 2.0869333333333335,
      "grad_norm": 0.1957385390996933,
      "learning_rate": 3.6956666666666665e-05,
      "loss": 0.0022,
      "step": 39130
    },
    {
      "epoch": 2.087466666666667,
      "grad_norm": 0.23076824843883514,
      "learning_rate": 3.695333333333334e-05,
      "loss": 0.0019,
      "step": 39140
    },
    {
      "epoch": 2.088,
      "grad_norm": 0.06345539540052414,
      "learning_rate": 3.6950000000000004e-05,
      "loss": 0.0033,
      "step": 39150
    },
    {
      "epoch": 2.0885333333333334,
      "grad_norm": 0.09361393004655838,
      "learning_rate": 3.694666666666667e-05,
      "loss": 0.0019,
      "step": 39160
    },
    {
      "epoch": 2.0890666666666666,
      "grad_norm": 0.29152563214302063,
      "learning_rate": 3.6943333333333336e-05,
      "loss": 0.0043,
      "step": 39170
    },
    {
      "epoch": 2.0896,
      "grad_norm": 0.609635591506958,
      "learning_rate": 3.694e-05,
      "loss": 0.003,
      "step": 39180
    },
    {
      "epoch": 2.090133333333333,
      "grad_norm": 0.020122235640883446,
      "learning_rate": 3.693666666666667e-05,
      "loss": 0.0031,
      "step": 39190
    },
    {
      "epoch": 2.0906666666666665,
      "grad_norm": 0.11595842242240906,
      "learning_rate": 3.6933333333333334e-05,
      "loss": 0.0048,
      "step": 39200
    },
    {
      "epoch": 2.0912,
      "grad_norm": 0.322547048330307,
      "learning_rate": 3.693e-05,
      "loss": 0.0027,
      "step": 39210
    },
    {
      "epoch": 2.0917333333333334,
      "grad_norm": 0.17471861839294434,
      "learning_rate": 3.6926666666666673e-05,
      "loss": 0.0032,
      "step": 39220
    },
    {
      "epoch": 2.0922666666666667,
      "grad_norm": 0.05893942713737488,
      "learning_rate": 3.692333333333334e-05,
      "loss": 0.0033,
      "step": 39230
    },
    {
      "epoch": 2.0928,
      "grad_norm": 0.06212615594267845,
      "learning_rate": 3.692e-05,
      "loss": 0.003,
      "step": 39240
    },
    {
      "epoch": 2.0933333333333333,
      "grad_norm": 0.32970571517944336,
      "learning_rate": 3.6916666666666665e-05,
      "loss": 0.003,
      "step": 39250
    },
    {
      "epoch": 2.0938666666666665,
      "grad_norm": 0.1448211669921875,
      "learning_rate": 3.691333333333333e-05,
      "loss": 0.0018,
      "step": 39260
    },
    {
      "epoch": 2.0944,
      "grad_norm": 0.2287001758813858,
      "learning_rate": 3.691e-05,
      "loss": 0.002,
      "step": 39270
    },
    {
      "epoch": 2.0949333333333335,
      "grad_norm": 0.28072112798690796,
      "learning_rate": 3.690666666666667e-05,
      "loss": 0.0029,
      "step": 39280
    },
    {
      "epoch": 2.095466666666667,
      "grad_norm": 0.09715405106544495,
      "learning_rate": 3.6903333333333336e-05,
      "loss": 0.0026,
      "step": 39290
    },
    {
      "epoch": 2.096,
      "grad_norm": 0.4377378821372986,
      "learning_rate": 3.69e-05,
      "loss": 0.0029,
      "step": 39300
    },
    {
      "epoch": 2.0965333333333334,
      "grad_norm": 0.3476907014846802,
      "learning_rate": 3.689666666666667e-05,
      "loss": 0.0031,
      "step": 39310
    },
    {
      "epoch": 2.0970666666666666,
      "grad_norm": 0.03736942261457443,
      "learning_rate": 3.6893333333333335e-05,
      "loss": 0.0028,
      "step": 39320
    },
    {
      "epoch": 2.0976,
      "grad_norm": 0.5936920046806335,
      "learning_rate": 3.689e-05,
      "loss": 0.0027,
      "step": 39330
    },
    {
      "epoch": 2.098133333333333,
      "grad_norm": 0.26314395666122437,
      "learning_rate": 3.688666666666667e-05,
      "loss": 0.0032,
      "step": 39340
    },
    {
      "epoch": 2.0986666666666665,
      "grad_norm": 0.23286771774291992,
      "learning_rate": 3.688333333333333e-05,
      "loss": 0.0035,
      "step": 39350
    },
    {
      "epoch": 2.0992,
      "grad_norm": 0.23558175563812256,
      "learning_rate": 3.6880000000000006e-05,
      "loss": 0.003,
      "step": 39360
    },
    {
      "epoch": 2.0997333333333335,
      "grad_norm": 0.2017299085855484,
      "learning_rate": 3.687666666666667e-05,
      "loss": 0.0021,
      "step": 39370
    },
    {
      "epoch": 2.1002666666666667,
      "grad_norm": 0.47539907693862915,
      "learning_rate": 3.687333333333334e-05,
      "loss": 0.0028,
      "step": 39380
    },
    {
      "epoch": 2.1008,
      "grad_norm": 0.20861801505088806,
      "learning_rate": 3.6870000000000004e-05,
      "loss": 0.0022,
      "step": 39390
    },
    {
      "epoch": 2.1013333333333333,
      "grad_norm": 0.06135611981153488,
      "learning_rate": 3.6866666666666664e-05,
      "loss": 0.0031,
      "step": 39400
    },
    {
      "epoch": 2.1018666666666665,
      "grad_norm": 0.061221301555633545,
      "learning_rate": 3.686333333333333e-05,
      "loss": 0.0031,
      "step": 39410
    },
    {
      "epoch": 2.1024,
      "grad_norm": 0.24341119825839996,
      "learning_rate": 3.686e-05,
      "loss": 0.0054,
      "step": 39420
    },
    {
      "epoch": 2.1029333333333335,
      "grad_norm": 0.2350379377603531,
      "learning_rate": 3.685666666666667e-05,
      "loss": 0.0033,
      "step": 39430
    },
    {
      "epoch": 2.103466666666667,
      "grad_norm": 0.17354875802993774,
      "learning_rate": 3.6853333333333335e-05,
      "loss": 0.0029,
      "step": 39440
    },
    {
      "epoch": 2.104,
      "grad_norm": 0.11732402443885803,
      "learning_rate": 3.685e-05,
      "loss": 0.0018,
      "step": 39450
    },
    {
      "epoch": 2.1045333333333334,
      "grad_norm": 0.12329216301441193,
      "learning_rate": 3.684666666666667e-05,
      "loss": 0.0023,
      "step": 39460
    },
    {
      "epoch": 2.1050666666666666,
      "grad_norm": 0.5539741516113281,
      "learning_rate": 3.6843333333333334e-05,
      "loss": 0.0026,
      "step": 39470
    },
    {
      "epoch": 2.1056,
      "grad_norm": 0.40643125772476196,
      "learning_rate": 3.684e-05,
      "loss": 0.0021,
      "step": 39480
    },
    {
      "epoch": 2.106133333333333,
      "grad_norm": 0.24415431916713715,
      "learning_rate": 3.683666666666667e-05,
      "loss": 0.003,
      "step": 39490
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 0.6120204925537109,
      "learning_rate": 3.683333333333334e-05,
      "loss": 0.0026,
      "step": 39500
    },
    {
      "epoch": 2.1072,
      "grad_norm": 0.01823519915342331,
      "learning_rate": 3.6830000000000005e-05,
      "loss": 0.0016,
      "step": 39510
    },
    {
      "epoch": 2.1077333333333335,
      "grad_norm": 0.5486427545547485,
      "learning_rate": 3.682666666666667e-05,
      "loss": 0.0024,
      "step": 39520
    },
    {
      "epoch": 2.1082666666666667,
      "grad_norm": 0.40216243267059326,
      "learning_rate": 3.682333333333334e-05,
      "loss": 0.0033,
      "step": 39530
    },
    {
      "epoch": 2.1088,
      "grad_norm": 0.17421458661556244,
      "learning_rate": 3.682e-05,
      "loss": 0.0022,
      "step": 39540
    },
    {
      "epoch": 2.1093333333333333,
      "grad_norm": 0.20617683231830597,
      "learning_rate": 3.681666666666667e-05,
      "loss": 0.0032,
      "step": 39550
    },
    {
      "epoch": 2.1098666666666666,
      "grad_norm": 0.09247113019227982,
      "learning_rate": 3.6813333333333335e-05,
      "loss": 0.0021,
      "step": 39560
    },
    {
      "epoch": 2.1104,
      "grad_norm": 0.03526140749454498,
      "learning_rate": 3.681e-05,
      "loss": 0.0019,
      "step": 39570
    },
    {
      "epoch": 2.1109333333333336,
      "grad_norm": 0.5339574813842773,
      "learning_rate": 3.680666666666667e-05,
      "loss": 0.0028,
      "step": 39580
    },
    {
      "epoch": 2.111466666666667,
      "grad_norm": 0.1754084974527359,
      "learning_rate": 3.6803333333333334e-05,
      "loss": 0.003,
      "step": 39590
    },
    {
      "epoch": 2.112,
      "grad_norm": 0.12041375041007996,
      "learning_rate": 3.68e-05,
      "loss": 0.0019,
      "step": 39600
    },
    {
      "epoch": 2.1125333333333334,
      "grad_norm": 0.11649557203054428,
      "learning_rate": 3.6796666666666666e-05,
      "loss": 0.0022,
      "step": 39610
    },
    {
      "epoch": 2.1130666666666666,
      "grad_norm": 0.37987738847732544,
      "learning_rate": 3.679333333333333e-05,
      "loss": 0.0029,
      "step": 39620
    },
    {
      "epoch": 2.1136,
      "grad_norm": 0.32973340153694153,
      "learning_rate": 3.6790000000000005e-05,
      "loss": 0.0027,
      "step": 39630
    },
    {
      "epoch": 2.114133333333333,
      "grad_norm": 0.14558683335781097,
      "learning_rate": 3.678666666666667e-05,
      "loss": 0.0029,
      "step": 39640
    },
    {
      "epoch": 2.1146666666666665,
      "grad_norm": 0.2892528474330902,
      "learning_rate": 3.678333333333334e-05,
      "loss": 0.0033,
      "step": 39650
    },
    {
      "epoch": 2.1152,
      "grad_norm": 0.7230698466300964,
      "learning_rate": 3.6780000000000004e-05,
      "loss": 0.0035,
      "step": 39660
    },
    {
      "epoch": 2.1157333333333335,
      "grad_norm": 0.154914990067482,
      "learning_rate": 3.677666666666667e-05,
      "loss": 0.0023,
      "step": 39670
    },
    {
      "epoch": 2.1162666666666667,
      "grad_norm": 0.04836485907435417,
      "learning_rate": 3.6773333333333336e-05,
      "loss": 0.0039,
      "step": 39680
    },
    {
      "epoch": 2.1168,
      "grad_norm": 0.5857862234115601,
      "learning_rate": 3.677e-05,
      "loss": 0.0033,
      "step": 39690
    },
    {
      "epoch": 2.1173333333333333,
      "grad_norm": 0.17705091834068298,
      "learning_rate": 3.676666666666667e-05,
      "loss": 0.0033,
      "step": 39700
    },
    {
      "epoch": 2.1178666666666666,
      "grad_norm": 0.3240372836589813,
      "learning_rate": 3.6763333333333334e-05,
      "loss": 0.0027,
      "step": 39710
    },
    {
      "epoch": 2.1184,
      "grad_norm": 0.20343804359436035,
      "learning_rate": 3.676e-05,
      "loss": 0.0024,
      "step": 39720
    },
    {
      "epoch": 2.1189333333333336,
      "grad_norm": 0.5563158392906189,
      "learning_rate": 3.6756666666666667e-05,
      "loss": 0.0024,
      "step": 39730
    },
    {
      "epoch": 2.119466666666667,
      "grad_norm": 0.20940238237380981,
      "learning_rate": 3.675333333333333e-05,
      "loss": 0.0021,
      "step": 39740
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.06108419969677925,
      "learning_rate": 3.675e-05,
      "loss": 0.0024,
      "step": 39750
    },
    {
      "epoch": 2.1205333333333334,
      "grad_norm": 0.3503900170326233,
      "learning_rate": 3.6746666666666665e-05,
      "loss": 0.0045,
      "step": 39760
    },
    {
      "epoch": 2.1210666666666667,
      "grad_norm": 0.11927208304405212,
      "learning_rate": 3.674333333333334e-05,
      "loss": 0.0022,
      "step": 39770
    },
    {
      "epoch": 2.1216,
      "grad_norm": 0.44274255633354187,
      "learning_rate": 3.6740000000000004e-05,
      "loss": 0.0027,
      "step": 39780
    },
    {
      "epoch": 2.122133333333333,
      "grad_norm": 0.6176459193229675,
      "learning_rate": 3.673666666666667e-05,
      "loss": 0.003,
      "step": 39790
    },
    {
      "epoch": 2.1226666666666665,
      "grad_norm": 0.5230373740196228,
      "learning_rate": 3.6733333333333336e-05,
      "loss": 0.0027,
      "step": 39800
    },
    {
      "epoch": 2.1232,
      "grad_norm": 0.44128715991973877,
      "learning_rate": 3.673e-05,
      "loss": 0.0027,
      "step": 39810
    },
    {
      "epoch": 2.1237333333333335,
      "grad_norm": 0.06241332367062569,
      "learning_rate": 3.672666666666667e-05,
      "loss": 0.0035,
      "step": 39820
    },
    {
      "epoch": 2.1242666666666667,
      "grad_norm": 0.09684695303440094,
      "learning_rate": 3.6723333333333335e-05,
      "loss": 0.0023,
      "step": 39830
    },
    {
      "epoch": 2.1248,
      "grad_norm": 0.31633567810058594,
      "learning_rate": 3.672000000000001e-05,
      "loss": 0.0037,
      "step": 39840
    },
    {
      "epoch": 2.1253333333333333,
      "grad_norm": 0.19075150787830353,
      "learning_rate": 3.671666666666667e-05,
      "loss": 0.0029,
      "step": 39850
    },
    {
      "epoch": 2.1258666666666666,
      "grad_norm": 0.036001648753881454,
      "learning_rate": 3.671333333333333e-05,
      "loss": 0.0023,
      "step": 39860
    },
    {
      "epoch": 2.1264,
      "grad_norm": 0.6217511892318726,
      "learning_rate": 3.671e-05,
      "loss": 0.0026,
      "step": 39870
    },
    {
      "epoch": 2.1269333333333336,
      "grad_norm": 0.09439778327941895,
      "learning_rate": 3.6706666666666665e-05,
      "loss": 0.0025,
      "step": 39880
    },
    {
      "epoch": 2.127466666666667,
      "grad_norm": 0.08890356123447418,
      "learning_rate": 3.670333333333333e-05,
      "loss": 0.0028,
      "step": 39890
    },
    {
      "epoch": 2.128,
      "grad_norm": 0.4650290310382843,
      "learning_rate": 3.6700000000000004e-05,
      "loss": 0.0032,
      "step": 39900
    },
    {
      "epoch": 2.1285333333333334,
      "grad_norm": 0.23928512632846832,
      "learning_rate": 3.669666666666667e-05,
      "loss": 0.0041,
      "step": 39910
    },
    {
      "epoch": 2.1290666666666667,
      "grad_norm": 0.4101826250553131,
      "learning_rate": 3.669333333333334e-05,
      "loss": 0.0038,
      "step": 39920
    },
    {
      "epoch": 2.1296,
      "grad_norm": 0.10120175778865814,
      "learning_rate": 3.669e-05,
      "loss": 0.0035,
      "step": 39930
    },
    {
      "epoch": 2.130133333333333,
      "grad_norm": 0.32617515325546265,
      "learning_rate": 3.668666666666667e-05,
      "loss": 0.0023,
      "step": 39940
    },
    {
      "epoch": 2.1306666666666665,
      "grad_norm": 0.26747041940689087,
      "learning_rate": 3.6683333333333335e-05,
      "loss": 0.0024,
      "step": 39950
    },
    {
      "epoch": 2.1312,
      "grad_norm": 0.14762674272060394,
      "learning_rate": 3.668e-05,
      "loss": 0.0025,
      "step": 39960
    },
    {
      "epoch": 2.1317333333333335,
      "grad_norm": 0.41974860429763794,
      "learning_rate": 3.667666666666667e-05,
      "loss": 0.0025,
      "step": 39970
    },
    {
      "epoch": 2.1322666666666668,
      "grad_norm": 0.26134905219078064,
      "learning_rate": 3.667333333333334e-05,
      "loss": 0.002,
      "step": 39980
    },
    {
      "epoch": 2.1328,
      "grad_norm": 0.4388953745365143,
      "learning_rate": 3.6670000000000006e-05,
      "loss": 0.0031,
      "step": 39990
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.20145133137702942,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 0.0024,
      "step": 40000
    },
    {
      "epoch": 2.1338666666666666,
      "grad_norm": 0.0880567878484726,
      "learning_rate": 3.666333333333333e-05,
      "loss": 0.002,
      "step": 40010
    },
    {
      "epoch": 2.1344,
      "grad_norm": 0.17407061159610748,
      "learning_rate": 3.666e-05,
      "loss": 0.0026,
      "step": 40020
    },
    {
      "epoch": 2.134933333333333,
      "grad_norm": 0.06149192526936531,
      "learning_rate": 3.6656666666666664e-05,
      "loss": 0.0035,
      "step": 40030
    },
    {
      "epoch": 2.135466666666667,
      "grad_norm": 0.5804765820503235,
      "learning_rate": 3.665333333333334e-05,
      "loss": 0.0039,
      "step": 40040
    },
    {
      "epoch": 2.136,
      "grad_norm": 0.1516495943069458,
      "learning_rate": 3.665e-05,
      "loss": 0.0033,
      "step": 40050
    },
    {
      "epoch": 2.1365333333333334,
      "grad_norm": 0.08729789406061172,
      "learning_rate": 3.664666666666667e-05,
      "loss": 0.0034,
      "step": 40060
    },
    {
      "epoch": 2.1370666666666667,
      "grad_norm": 0.2534540593624115,
      "learning_rate": 3.6643333333333335e-05,
      "loss": 0.0035,
      "step": 40070
    },
    {
      "epoch": 2.1376,
      "grad_norm": 0.4579559862613678,
      "learning_rate": 3.664e-05,
      "loss": 0.0035,
      "step": 40080
    },
    {
      "epoch": 2.138133333333333,
      "grad_norm": 0.06571638584136963,
      "learning_rate": 3.663666666666667e-05,
      "loss": 0.0034,
      "step": 40090
    },
    {
      "epoch": 2.1386666666666665,
      "grad_norm": 0.2615396976470947,
      "learning_rate": 3.6633333333333334e-05,
      "loss": 0.004,
      "step": 40100
    },
    {
      "epoch": 2.1391999999999998,
      "grad_norm": 0.5258577466011047,
      "learning_rate": 3.663e-05,
      "loss": 0.0025,
      "step": 40110
    },
    {
      "epoch": 2.1397333333333335,
      "grad_norm": 0.31882014870643616,
      "learning_rate": 3.662666666666667e-05,
      "loss": 0.0026,
      "step": 40120
    },
    {
      "epoch": 2.1402666666666668,
      "grad_norm": 0.15074415504932404,
      "learning_rate": 3.662333333333334e-05,
      "loss": 0.0028,
      "step": 40130
    },
    {
      "epoch": 2.1408,
      "grad_norm": 0.3775049149990082,
      "learning_rate": 3.6620000000000005e-05,
      "loss": 0.0029,
      "step": 40140
    },
    {
      "epoch": 2.1413333333333333,
      "grad_norm": 0.17232340574264526,
      "learning_rate": 3.6616666666666664e-05,
      "loss": 0.0021,
      "step": 40150
    },
    {
      "epoch": 2.1418666666666666,
      "grad_norm": 0.08974713832139969,
      "learning_rate": 3.661333333333333e-05,
      "loss": 0.0025,
      "step": 40160
    },
    {
      "epoch": 2.1424,
      "grad_norm": 0.20522230863571167,
      "learning_rate": 3.661e-05,
      "loss": 0.0015,
      "step": 40170
    },
    {
      "epoch": 2.142933333333333,
      "grad_norm": 0.1479308158159256,
      "learning_rate": 3.660666666666667e-05,
      "loss": 0.0031,
      "step": 40180
    },
    {
      "epoch": 2.143466666666667,
      "grad_norm": 0.17852620780467987,
      "learning_rate": 3.6603333333333336e-05,
      "loss": 0.0028,
      "step": 40190
    },
    {
      "epoch": 2.144,
      "grad_norm": 0.23175449669361115,
      "learning_rate": 3.66e-05,
      "loss": 0.0025,
      "step": 40200
    },
    {
      "epoch": 2.1445333333333334,
      "grad_norm": 0.12739905714988708,
      "learning_rate": 3.659666666666667e-05,
      "loss": 0.0036,
      "step": 40210
    },
    {
      "epoch": 2.1450666666666667,
      "grad_norm": 0.2950751483440399,
      "learning_rate": 3.6593333333333334e-05,
      "loss": 0.0027,
      "step": 40220
    },
    {
      "epoch": 2.1456,
      "grad_norm": 0.4040250778198242,
      "learning_rate": 3.659e-05,
      "loss": 0.0029,
      "step": 40230
    },
    {
      "epoch": 2.1461333333333332,
      "grad_norm": 0.5219107270240784,
      "learning_rate": 3.6586666666666666e-05,
      "loss": 0.003,
      "step": 40240
    },
    {
      "epoch": 2.1466666666666665,
      "grad_norm": 0.11772603541612625,
      "learning_rate": 3.658333333333334e-05,
      "loss": 0.0036,
      "step": 40250
    },
    {
      "epoch": 2.1471999999999998,
      "grad_norm": 0.6933615207672119,
      "learning_rate": 3.6580000000000006e-05,
      "loss": 0.0039,
      "step": 40260
    },
    {
      "epoch": 2.1477333333333335,
      "grad_norm": 0.09543266147375107,
      "learning_rate": 3.657666666666667e-05,
      "loss": 0.0032,
      "step": 40270
    },
    {
      "epoch": 2.1482666666666668,
      "grad_norm": 0.40269967913627625,
      "learning_rate": 3.657333333333334e-05,
      "loss": 0.0023,
      "step": 40280
    },
    {
      "epoch": 2.1488,
      "grad_norm": 0.20258080959320068,
      "learning_rate": 3.6570000000000004e-05,
      "loss": 0.0026,
      "step": 40290
    },
    {
      "epoch": 2.1493333333333333,
      "grad_norm": 0.4321551024913788,
      "learning_rate": 3.656666666666666e-05,
      "loss": 0.0036,
      "step": 40300
    },
    {
      "epoch": 2.1498666666666666,
      "grad_norm": 0.6342031359672546,
      "learning_rate": 3.656333333333333e-05,
      "loss": 0.0023,
      "step": 40310
    },
    {
      "epoch": 2.1504,
      "grad_norm": 0.03105437569320202,
      "learning_rate": 3.656e-05,
      "loss": 0.0022,
      "step": 40320
    },
    {
      "epoch": 2.150933333333333,
      "grad_norm": 0.3766205906867981,
      "learning_rate": 3.655666666666667e-05,
      "loss": 0.0037,
      "step": 40330
    },
    {
      "epoch": 2.151466666666667,
      "grad_norm": 0.14349383115768433,
      "learning_rate": 3.6553333333333335e-05,
      "loss": 0.0017,
      "step": 40340
    },
    {
      "epoch": 2.152,
      "grad_norm": 0.30453217029571533,
      "learning_rate": 3.655e-05,
      "loss": 0.0037,
      "step": 40350
    },
    {
      "epoch": 2.1525333333333334,
      "grad_norm": 0.23716497421264648,
      "learning_rate": 3.654666666666667e-05,
      "loss": 0.0021,
      "step": 40360
    },
    {
      "epoch": 2.1530666666666667,
      "grad_norm": 0.08721625059843063,
      "learning_rate": 3.654333333333333e-05,
      "loss": 0.0024,
      "step": 40370
    },
    {
      "epoch": 2.1536,
      "grad_norm": 0.4897410273551941,
      "learning_rate": 3.654e-05,
      "loss": 0.0023,
      "step": 40380
    },
    {
      "epoch": 2.1541333333333332,
      "grad_norm": 0.23866726458072662,
      "learning_rate": 3.653666666666667e-05,
      "loss": 0.0025,
      "step": 40390
    },
    {
      "epoch": 2.1546666666666665,
      "grad_norm": 0.025479188188910484,
      "learning_rate": 3.653333333333334e-05,
      "loss": 0.0019,
      "step": 40400
    },
    {
      "epoch": 2.1552,
      "grad_norm": 0.4908185303211212,
      "learning_rate": 3.6530000000000004e-05,
      "loss": 0.0018,
      "step": 40410
    },
    {
      "epoch": 2.1557333333333335,
      "grad_norm": 0.036795683205127716,
      "learning_rate": 3.652666666666667e-05,
      "loss": 0.0025,
      "step": 40420
    },
    {
      "epoch": 2.1562666666666668,
      "grad_norm": 0.37841087579727173,
      "learning_rate": 3.6523333333333337e-05,
      "loss": 0.0027,
      "step": 40430
    },
    {
      "epoch": 2.1568,
      "grad_norm": 0.03060866892337799,
      "learning_rate": 3.652e-05,
      "loss": 0.0023,
      "step": 40440
    },
    {
      "epoch": 2.1573333333333333,
      "grad_norm": 0.34423255920410156,
      "learning_rate": 3.651666666666667e-05,
      "loss": 0.0037,
      "step": 40450
    },
    {
      "epoch": 2.1578666666666666,
      "grad_norm": 0.17522355914115906,
      "learning_rate": 3.6513333333333335e-05,
      "loss": 0.0033,
      "step": 40460
    },
    {
      "epoch": 2.1584,
      "grad_norm": 0.12310729175806046,
      "learning_rate": 3.651e-05,
      "loss": 0.0021,
      "step": 40470
    },
    {
      "epoch": 2.158933333333333,
      "grad_norm": 0.14438459277153015,
      "learning_rate": 3.650666666666667e-05,
      "loss": 0.002,
      "step": 40480
    },
    {
      "epoch": 2.159466666666667,
      "grad_norm": 0.11690526455640793,
      "learning_rate": 3.650333333333333e-05,
      "loss": 0.0026,
      "step": 40490
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.09193311631679535,
      "learning_rate": 3.65e-05,
      "loss": 0.0029,
      "step": 40500
    },
    {
      "epoch": 2.1605333333333334,
      "grad_norm": 0.4426683187484741,
      "learning_rate": 3.6496666666666666e-05,
      "loss": 0.0026,
      "step": 40510
    },
    {
      "epoch": 2.1610666666666667,
      "grad_norm": 0.32418134808540344,
      "learning_rate": 3.649333333333333e-05,
      "loss": 0.002,
      "step": 40520
    },
    {
      "epoch": 2.1616,
      "grad_norm": 0.5171694755554199,
      "learning_rate": 3.6490000000000005e-05,
      "loss": 0.0026,
      "step": 40530
    },
    {
      "epoch": 2.1621333333333332,
      "grad_norm": 0.4043261408805847,
      "learning_rate": 3.648666666666667e-05,
      "loss": 0.0036,
      "step": 40540
    },
    {
      "epoch": 2.1626666666666665,
      "grad_norm": 0.23314714431762695,
      "learning_rate": 3.648333333333334e-05,
      "loss": 0.0023,
      "step": 40550
    },
    {
      "epoch": 2.1632,
      "grad_norm": 0.40498942136764526,
      "learning_rate": 3.648e-05,
      "loss": 0.0024,
      "step": 40560
    },
    {
      "epoch": 2.1637333333333335,
      "grad_norm": 0.06501170992851257,
      "learning_rate": 3.647666666666667e-05,
      "loss": 0.0037,
      "step": 40570
    },
    {
      "epoch": 2.164266666666667,
      "grad_norm": 0.28969115018844604,
      "learning_rate": 3.6473333333333335e-05,
      "loss": 0.0031,
      "step": 40580
    },
    {
      "epoch": 2.1648,
      "grad_norm": 0.4420015215873718,
      "learning_rate": 3.647e-05,
      "loss": 0.0028,
      "step": 40590
    },
    {
      "epoch": 2.1653333333333333,
      "grad_norm": 0.3481108248233795,
      "learning_rate": 3.646666666666667e-05,
      "loss": 0.0035,
      "step": 40600
    },
    {
      "epoch": 2.1658666666666666,
      "grad_norm": 0.26150885224342346,
      "learning_rate": 3.6463333333333334e-05,
      "loss": 0.0034,
      "step": 40610
    },
    {
      "epoch": 2.1664,
      "grad_norm": 0.2146463394165039,
      "learning_rate": 3.646e-05,
      "loss": 0.0029,
      "step": 40620
    },
    {
      "epoch": 2.166933333333333,
      "grad_norm": 0.12353310734033585,
      "learning_rate": 3.6456666666666666e-05,
      "loss": 0.0047,
      "step": 40630
    },
    {
      "epoch": 2.167466666666667,
      "grad_norm": 0.08918842673301697,
      "learning_rate": 3.645333333333333e-05,
      "loss": 0.0028,
      "step": 40640
    },
    {
      "epoch": 2.168,
      "grad_norm": 0.3250168561935425,
      "learning_rate": 3.645e-05,
      "loss": 0.0036,
      "step": 40650
    },
    {
      "epoch": 2.1685333333333334,
      "grad_norm": 0.03295111283659935,
      "learning_rate": 3.644666666666667e-05,
      "loss": 0.0017,
      "step": 40660
    },
    {
      "epoch": 2.1690666666666667,
      "grad_norm": 0.11672055721282959,
      "learning_rate": 3.644333333333334e-05,
      "loss": 0.0037,
      "step": 40670
    },
    {
      "epoch": 2.1696,
      "grad_norm": 0.02940063737332821,
      "learning_rate": 3.6440000000000003e-05,
      "loss": 0.0047,
      "step": 40680
    },
    {
      "epoch": 2.1701333333333332,
      "grad_norm": 0.12432624399662018,
      "learning_rate": 3.643666666666667e-05,
      "loss": 0.0034,
      "step": 40690
    },
    {
      "epoch": 2.1706666666666665,
      "grad_norm": 0.03522605076432228,
      "learning_rate": 3.6433333333333336e-05,
      "loss": 0.0029,
      "step": 40700
    },
    {
      "epoch": 2.1712,
      "grad_norm": 0.08734488487243652,
      "learning_rate": 3.643e-05,
      "loss": 0.0015,
      "step": 40710
    },
    {
      "epoch": 2.1717333333333335,
      "grad_norm": 0.06562496721744537,
      "learning_rate": 3.642666666666667e-05,
      "loss": 0.0042,
      "step": 40720
    },
    {
      "epoch": 2.172266666666667,
      "grad_norm": 0.4647669196128845,
      "learning_rate": 3.6423333333333334e-05,
      "loss": 0.0025,
      "step": 40730
    },
    {
      "epoch": 2.1728,
      "grad_norm": 0.17356111109256744,
      "learning_rate": 3.642000000000001e-05,
      "loss": 0.0035,
      "step": 40740
    },
    {
      "epoch": 2.1733333333333333,
      "grad_norm": 0.2897285223007202,
      "learning_rate": 3.641666666666667e-05,
      "loss": 0.0019,
      "step": 40750
    },
    {
      "epoch": 2.1738666666666666,
      "grad_norm": 0.059897758066654205,
      "learning_rate": 3.641333333333333e-05,
      "loss": 0.0027,
      "step": 40760
    },
    {
      "epoch": 2.1744,
      "grad_norm": 0.1448972076177597,
      "learning_rate": 3.641e-05,
      "loss": 0.0018,
      "step": 40770
    },
    {
      "epoch": 2.174933333333333,
      "grad_norm": 0.060225918889045715,
      "learning_rate": 3.6406666666666665e-05,
      "loss": 0.0023,
      "step": 40780
    },
    {
      "epoch": 2.175466666666667,
      "grad_norm": 0.06435441225767136,
      "learning_rate": 3.640333333333333e-05,
      "loss": 0.0032,
      "step": 40790
    },
    {
      "epoch": 2.176,
      "grad_norm": 0.1832188069820404,
      "learning_rate": 3.6400000000000004e-05,
      "loss": 0.0028,
      "step": 40800
    },
    {
      "epoch": 2.1765333333333334,
      "grad_norm": 0.2587648630142212,
      "learning_rate": 3.639666666666667e-05,
      "loss": 0.0024,
      "step": 40810
    },
    {
      "epoch": 2.1770666666666667,
      "grad_norm": 0.2742813229560852,
      "learning_rate": 3.6393333333333336e-05,
      "loss": 0.0032,
      "step": 40820
    },
    {
      "epoch": 2.1776,
      "grad_norm": 0.37970206141471863,
      "learning_rate": 3.639e-05,
      "loss": 0.0019,
      "step": 40830
    },
    {
      "epoch": 2.1781333333333333,
      "grad_norm": 0.7256721258163452,
      "learning_rate": 3.638666666666667e-05,
      "loss": 0.0028,
      "step": 40840
    },
    {
      "epoch": 2.1786666666666665,
      "grad_norm": 0.0672265887260437,
      "learning_rate": 3.6383333333333335e-05,
      "loss": 0.0034,
      "step": 40850
    },
    {
      "epoch": 2.1792,
      "grad_norm": 0.424221009016037,
      "learning_rate": 3.638e-05,
      "loss": 0.0029,
      "step": 40860
    },
    {
      "epoch": 2.1797333333333335,
      "grad_norm": 0.0909685268998146,
      "learning_rate": 3.637666666666667e-05,
      "loss": 0.0046,
      "step": 40870
    },
    {
      "epoch": 2.180266666666667,
      "grad_norm": 0.17423321306705475,
      "learning_rate": 3.637333333333334e-05,
      "loss": 0.0022,
      "step": 40880
    },
    {
      "epoch": 2.1808,
      "grad_norm": 0.3786351680755615,
      "learning_rate": 3.6370000000000006e-05,
      "loss": 0.0023,
      "step": 40890
    },
    {
      "epoch": 2.1813333333333333,
      "grad_norm": 0.2463466078042984,
      "learning_rate": 3.636666666666667e-05,
      "loss": 0.003,
      "step": 40900
    },
    {
      "epoch": 2.1818666666666666,
      "grad_norm": 0.06070777773857117,
      "learning_rate": 3.636333333333333e-05,
      "loss": 0.0036,
      "step": 40910
    },
    {
      "epoch": 2.1824,
      "grad_norm": 0.42643681168556213,
      "learning_rate": 3.636e-05,
      "loss": 0.0032,
      "step": 40920
    },
    {
      "epoch": 2.182933333333333,
      "grad_norm": 0.3985165059566498,
      "learning_rate": 3.6356666666666664e-05,
      "loss": 0.0037,
      "step": 40930
    },
    {
      "epoch": 2.183466666666667,
      "grad_norm": 0.06435143202543259,
      "learning_rate": 3.6353333333333337e-05,
      "loss": 0.0037,
      "step": 40940
    },
    {
      "epoch": 2.184,
      "grad_norm": 0.22962623834609985,
      "learning_rate": 3.635e-05,
      "loss": 0.0029,
      "step": 40950
    },
    {
      "epoch": 2.1845333333333334,
      "grad_norm": 0.25959447026252747,
      "learning_rate": 3.634666666666667e-05,
      "loss": 0.0029,
      "step": 40960
    },
    {
      "epoch": 2.1850666666666667,
      "grad_norm": 0.3444058895111084,
      "learning_rate": 3.6343333333333335e-05,
      "loss": 0.0035,
      "step": 40970
    },
    {
      "epoch": 2.1856,
      "grad_norm": 0.4196346700191498,
      "learning_rate": 3.634e-05,
      "loss": 0.0027,
      "step": 40980
    },
    {
      "epoch": 2.1861333333333333,
      "grad_norm": 0.23456676304340363,
      "learning_rate": 3.633666666666667e-05,
      "loss": 0.003,
      "step": 40990
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 0.08921114355325699,
      "learning_rate": 3.633333333333333e-05,
      "loss": 0.0024,
      "step": 41000
    },
    {
      "epoch": 2.1872,
      "grad_norm": 0.03807438164949417,
      "learning_rate": 3.6330000000000006e-05,
      "loss": 0.0034,
      "step": 41010
    },
    {
      "epoch": 2.1877333333333335,
      "grad_norm": 0.38054874539375305,
      "learning_rate": 3.632666666666667e-05,
      "loss": 0.0037,
      "step": 41020
    },
    {
      "epoch": 2.188266666666667,
      "grad_norm": 0.14458708465099335,
      "learning_rate": 3.632333333333334e-05,
      "loss": 0.0023,
      "step": 41030
    },
    {
      "epoch": 2.1888,
      "grad_norm": 0.019813477993011475,
      "learning_rate": 3.6320000000000005e-05,
      "loss": 0.0031,
      "step": 41040
    },
    {
      "epoch": 2.1893333333333334,
      "grad_norm": 0.23170913755893707,
      "learning_rate": 3.631666666666667e-05,
      "loss": 0.0027,
      "step": 41050
    },
    {
      "epoch": 2.1898666666666666,
      "grad_norm": 0.5534508228302002,
      "learning_rate": 3.631333333333333e-05,
      "loss": 0.003,
      "step": 41060
    },
    {
      "epoch": 2.1904,
      "grad_norm": 0.4154142141342163,
      "learning_rate": 3.6309999999999996e-05,
      "loss": 0.003,
      "step": 41070
    },
    {
      "epoch": 2.190933333333333,
      "grad_norm": 0.3223492503166199,
      "learning_rate": 3.630666666666667e-05,
      "loss": 0.0044,
      "step": 41080
    },
    {
      "epoch": 2.191466666666667,
      "grad_norm": 0.14432623982429504,
      "learning_rate": 3.6303333333333335e-05,
      "loss": 0.0034,
      "step": 41090
    },
    {
      "epoch": 2.192,
      "grad_norm": 0.17565442621707916,
      "learning_rate": 3.63e-05,
      "loss": 0.0025,
      "step": 41100
    },
    {
      "epoch": 2.1925333333333334,
      "grad_norm": 0.08312936127185822,
      "learning_rate": 3.629666666666667e-05,
      "loss": 0.0041,
      "step": 41110
    },
    {
      "epoch": 2.1930666666666667,
      "grad_norm": 0.3534790575504303,
      "learning_rate": 3.6293333333333334e-05,
      "loss": 0.0032,
      "step": 41120
    },
    {
      "epoch": 2.1936,
      "grad_norm": 0.12879158556461334,
      "learning_rate": 3.629e-05,
      "loss": 0.0025,
      "step": 41130
    },
    {
      "epoch": 2.1941333333333333,
      "grad_norm": 0.09468238800764084,
      "learning_rate": 3.6286666666666666e-05,
      "loss": 0.0025,
      "step": 41140
    },
    {
      "epoch": 2.1946666666666665,
      "grad_norm": 0.6325987577438354,
      "learning_rate": 3.628333333333334e-05,
      "loss": 0.0042,
      "step": 41150
    },
    {
      "epoch": 2.1952,
      "grad_norm": 0.4426577091217041,
      "learning_rate": 3.6280000000000005e-05,
      "loss": 0.0025,
      "step": 41160
    },
    {
      "epoch": 2.1957333333333335,
      "grad_norm": 0.09911568462848663,
      "learning_rate": 3.627666666666667e-05,
      "loss": 0.0034,
      "step": 41170
    },
    {
      "epoch": 2.196266666666667,
      "grad_norm": 0.15187588334083557,
      "learning_rate": 3.627333333333334e-05,
      "loss": 0.0023,
      "step": 41180
    },
    {
      "epoch": 2.1968,
      "grad_norm": 0.12530934810638428,
      "learning_rate": 3.6270000000000003e-05,
      "loss": 0.0024,
      "step": 41190
    },
    {
      "epoch": 2.1973333333333334,
      "grad_norm": 0.5490975975990295,
      "learning_rate": 3.626666666666667e-05,
      "loss": 0.0023,
      "step": 41200
    },
    {
      "epoch": 2.1978666666666666,
      "grad_norm": 0.720842719078064,
      "learning_rate": 3.6263333333333336e-05,
      "loss": 0.0035,
      "step": 41210
    },
    {
      "epoch": 2.1984,
      "grad_norm": 0.5236418843269348,
      "learning_rate": 3.626e-05,
      "loss": 0.0028,
      "step": 41220
    },
    {
      "epoch": 2.198933333333333,
      "grad_norm": 0.0926462709903717,
      "learning_rate": 3.625666666666667e-05,
      "loss": 0.0026,
      "step": 41230
    },
    {
      "epoch": 2.1994666666666665,
      "grad_norm": 0.34323835372924805,
      "learning_rate": 3.6253333333333334e-05,
      "loss": 0.0024,
      "step": 41240
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.20416703820228577,
      "learning_rate": 3.625e-05,
      "loss": 0.0029,
      "step": 41250
    },
    {
      "epoch": 2.2005333333333335,
      "grad_norm": 0.06114526465535164,
      "learning_rate": 3.6246666666666666e-05,
      "loss": 0.0033,
      "step": 41260
    },
    {
      "epoch": 2.2010666666666667,
      "grad_norm": 0.0869860053062439,
      "learning_rate": 3.624333333333333e-05,
      "loss": 0.0032,
      "step": 41270
    },
    {
      "epoch": 2.2016,
      "grad_norm": 0.08608553558588028,
      "learning_rate": 3.624e-05,
      "loss": 0.0031,
      "step": 41280
    },
    {
      "epoch": 2.2021333333333333,
      "grad_norm": 0.030482636764645576,
      "learning_rate": 3.623666666666667e-05,
      "loss": 0.0031,
      "step": 41290
    },
    {
      "epoch": 2.2026666666666666,
      "grad_norm": 0.06087479367852211,
      "learning_rate": 3.623333333333334e-05,
      "loss": 0.002,
      "step": 41300
    },
    {
      "epoch": 2.2032,
      "grad_norm": 0.317096084356308,
      "learning_rate": 3.6230000000000004e-05,
      "loss": 0.0023,
      "step": 41310
    },
    {
      "epoch": 2.203733333333333,
      "grad_norm": 0.036974839866161346,
      "learning_rate": 3.622666666666667e-05,
      "loss": 0.002,
      "step": 41320
    },
    {
      "epoch": 2.204266666666667,
      "grad_norm": 0.39724621176719666,
      "learning_rate": 3.6223333333333336e-05,
      "loss": 0.0024,
      "step": 41330
    },
    {
      "epoch": 2.2048,
      "grad_norm": 0.45020291209220886,
      "learning_rate": 3.622e-05,
      "loss": 0.0031,
      "step": 41340
    },
    {
      "epoch": 2.2053333333333334,
      "grad_norm": 0.3474973142147064,
      "learning_rate": 3.621666666666667e-05,
      "loss": 0.0025,
      "step": 41350
    },
    {
      "epoch": 2.2058666666666666,
      "grad_norm": 0.344382643699646,
      "learning_rate": 3.6213333333333334e-05,
      "loss": 0.0029,
      "step": 41360
    },
    {
      "epoch": 2.2064,
      "grad_norm": 0.23653662204742432,
      "learning_rate": 3.621e-05,
      "loss": 0.0036,
      "step": 41370
    },
    {
      "epoch": 2.206933333333333,
      "grad_norm": 0.5203233361244202,
      "learning_rate": 3.620666666666667e-05,
      "loss": 0.0031,
      "step": 41380
    },
    {
      "epoch": 2.2074666666666665,
      "grad_norm": 0.03295345604419708,
      "learning_rate": 3.620333333333333e-05,
      "loss": 0.0019,
      "step": 41390
    },
    {
      "epoch": 2.208,
      "grad_norm": 0.06485848128795624,
      "learning_rate": 3.62e-05,
      "loss": 0.0025,
      "step": 41400
    },
    {
      "epoch": 2.2085333333333335,
      "grad_norm": 0.17926563322544098,
      "learning_rate": 3.6196666666666665e-05,
      "loss": 0.0024,
      "step": 41410
    },
    {
      "epoch": 2.2090666666666667,
      "grad_norm": 0.21845971047878265,
      "learning_rate": 3.619333333333333e-05,
      "loss": 0.003,
      "step": 41420
    },
    {
      "epoch": 2.2096,
      "grad_norm": 0.11573901027441025,
      "learning_rate": 3.6190000000000004e-05,
      "loss": 0.0015,
      "step": 41430
    },
    {
      "epoch": 2.2101333333333333,
      "grad_norm": 0.03486000373959541,
      "learning_rate": 3.618666666666667e-05,
      "loss": 0.0028,
      "step": 41440
    },
    {
      "epoch": 2.2106666666666666,
      "grad_norm": 0.06254308670759201,
      "learning_rate": 3.6183333333333336e-05,
      "loss": 0.0027,
      "step": 41450
    },
    {
      "epoch": 2.2112,
      "grad_norm": 0.6806657910346985,
      "learning_rate": 3.618e-05,
      "loss": 0.0025,
      "step": 41460
    },
    {
      "epoch": 2.211733333333333,
      "grad_norm": 0.0927765965461731,
      "learning_rate": 3.617666666666667e-05,
      "loss": 0.0035,
      "step": 41470
    },
    {
      "epoch": 2.212266666666667,
      "grad_norm": 0.44351711869239807,
      "learning_rate": 3.6173333333333335e-05,
      "loss": 0.0028,
      "step": 41480
    },
    {
      "epoch": 2.2128,
      "grad_norm": 0.15272176265716553,
      "learning_rate": 3.617e-05,
      "loss": 0.0031,
      "step": 41490
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 0.20280003547668457,
      "learning_rate": 3.6166666666666674e-05,
      "loss": 0.0029,
      "step": 41500
    },
    {
      "epoch": 2.2138666666666666,
      "grad_norm": 0.03724374622106552,
      "learning_rate": 3.616333333333333e-05,
      "loss": 0.0016,
      "step": 41510
    },
    {
      "epoch": 2.2144,
      "grad_norm": 0.034619081765413284,
      "learning_rate": 3.616e-05,
      "loss": 0.0025,
      "step": 41520
    },
    {
      "epoch": 2.214933333333333,
      "grad_norm": 0.23195308446884155,
      "learning_rate": 3.6156666666666666e-05,
      "loss": 0.0028,
      "step": 41530
    },
    {
      "epoch": 2.2154666666666665,
      "grad_norm": 0.11795448511838913,
      "learning_rate": 3.615333333333333e-05,
      "loss": 0.003,
      "step": 41540
    },
    {
      "epoch": 2.216,
      "grad_norm": 0.29410919547080994,
      "learning_rate": 3.615e-05,
      "loss": 0.0018,
      "step": 41550
    },
    {
      "epoch": 2.2165333333333335,
      "grad_norm": 0.22861236333847046,
      "learning_rate": 3.614666666666667e-05,
      "loss": 0.0033,
      "step": 41560
    },
    {
      "epoch": 2.2170666666666667,
      "grad_norm": 0.21026107668876648,
      "learning_rate": 3.614333333333334e-05,
      "loss": 0.0032,
      "step": 41570
    },
    {
      "epoch": 2.2176,
      "grad_norm": 0.29293107986450195,
      "learning_rate": 3.614e-05,
      "loss": 0.004,
      "step": 41580
    },
    {
      "epoch": 2.2181333333333333,
      "grad_norm": 0.5443352460861206,
      "learning_rate": 3.613666666666667e-05,
      "loss": 0.0027,
      "step": 41590
    },
    {
      "epoch": 2.2186666666666666,
      "grad_norm": 0.29610204696655273,
      "learning_rate": 3.6133333333333335e-05,
      "loss": 0.0029,
      "step": 41600
    },
    {
      "epoch": 2.2192,
      "grad_norm": 0.03445000573992729,
      "learning_rate": 3.613e-05,
      "loss": 0.003,
      "step": 41610
    },
    {
      "epoch": 2.219733333333333,
      "grad_norm": 0.23140062391757965,
      "learning_rate": 3.612666666666667e-05,
      "loss": 0.0022,
      "step": 41620
    },
    {
      "epoch": 2.220266666666667,
      "grad_norm": 0.15148554742336273,
      "learning_rate": 3.6123333333333334e-05,
      "loss": 0.0037,
      "step": 41630
    },
    {
      "epoch": 2.2208,
      "grad_norm": 0.6344527006149292,
      "learning_rate": 3.6120000000000007e-05,
      "loss": 0.0023,
      "step": 41640
    },
    {
      "epoch": 2.2213333333333334,
      "grad_norm": 0.3467268645763397,
      "learning_rate": 3.611666666666667e-05,
      "loss": 0.003,
      "step": 41650
    },
    {
      "epoch": 2.2218666666666667,
      "grad_norm": 0.1594027429819107,
      "learning_rate": 3.611333333333333e-05,
      "loss": 0.0022,
      "step": 41660
    },
    {
      "epoch": 2.2224,
      "grad_norm": 0.6665337681770325,
      "learning_rate": 3.611e-05,
      "loss": 0.0023,
      "step": 41670
    },
    {
      "epoch": 2.222933333333333,
      "grad_norm": 0.1453942358493805,
      "learning_rate": 3.6106666666666664e-05,
      "loss": 0.0023,
      "step": 41680
    },
    {
      "epoch": 2.2234666666666665,
      "grad_norm": 0.09375040978193283,
      "learning_rate": 3.610333333333333e-05,
      "loss": 0.0032,
      "step": 41690
    },
    {
      "epoch": 2.224,
      "grad_norm": 0.09065696597099304,
      "learning_rate": 3.61e-05,
      "loss": 0.002,
      "step": 41700
    },
    {
      "epoch": 2.2245333333333335,
      "grad_norm": 0.09142880886793137,
      "learning_rate": 3.609666666666667e-05,
      "loss": 0.0035,
      "step": 41710
    },
    {
      "epoch": 2.2250666666666667,
      "grad_norm": 0.415447860956192,
      "learning_rate": 3.6093333333333336e-05,
      "loss": 0.0025,
      "step": 41720
    },
    {
      "epoch": 2.2256,
      "grad_norm": 0.172096848487854,
      "learning_rate": 3.609e-05,
      "loss": 0.0036,
      "step": 41730
    },
    {
      "epoch": 2.2261333333333333,
      "grad_norm": 0.42260122299194336,
      "learning_rate": 3.608666666666667e-05,
      "loss": 0.0028,
      "step": 41740
    },
    {
      "epoch": 2.2266666666666666,
      "grad_norm": 0.5498130917549133,
      "learning_rate": 3.6083333333333334e-05,
      "loss": 0.0027,
      "step": 41750
    },
    {
      "epoch": 2.2272,
      "grad_norm": 0.04523555934429169,
      "learning_rate": 3.608e-05,
      "loss": 0.0031,
      "step": 41760
    },
    {
      "epoch": 2.227733333333333,
      "grad_norm": 0.07555102556943893,
      "learning_rate": 3.607666666666667e-05,
      "loss": 0.0025,
      "step": 41770
    },
    {
      "epoch": 2.228266666666667,
      "grad_norm": 0.2875257432460785,
      "learning_rate": 3.607333333333334e-05,
      "loss": 0.0032,
      "step": 41780
    },
    {
      "epoch": 2.2288,
      "grad_norm": 0.08675532788038254,
      "learning_rate": 3.6070000000000005e-05,
      "loss": 0.0034,
      "step": 41790
    },
    {
      "epoch": 2.2293333333333334,
      "grad_norm": 0.2972099483013153,
      "learning_rate": 3.606666666666667e-05,
      "loss": 0.0026,
      "step": 41800
    },
    {
      "epoch": 2.2298666666666667,
      "grad_norm": 0.2713015079498291,
      "learning_rate": 3.606333333333333e-05,
      "loss": 0.0029,
      "step": 41810
    },
    {
      "epoch": 2.2304,
      "grad_norm": 0.3178791105747223,
      "learning_rate": 3.606e-05,
      "loss": 0.0019,
      "step": 41820
    },
    {
      "epoch": 2.230933333333333,
      "grad_norm": 0.24270431697368622,
      "learning_rate": 3.605666666666666e-05,
      "loss": 0.003,
      "step": 41830
    },
    {
      "epoch": 2.2314666666666665,
      "grad_norm": 0.032187145203351974,
      "learning_rate": 3.6053333333333336e-05,
      "loss": 0.0018,
      "step": 41840
    },
    {
      "epoch": 2.232,
      "grad_norm": 0.21652217209339142,
      "learning_rate": 3.605e-05,
      "loss": 0.0027,
      "step": 41850
    },
    {
      "epoch": 2.2325333333333335,
      "grad_norm": 0.06745452433824539,
      "learning_rate": 3.604666666666667e-05,
      "loss": 0.0033,
      "step": 41860
    },
    {
      "epoch": 2.2330666666666668,
      "grad_norm": 0.02107756957411766,
      "learning_rate": 3.6043333333333334e-05,
      "loss": 0.0023,
      "step": 41870
    },
    {
      "epoch": 2.2336,
      "grad_norm": 0.15410754084587097,
      "learning_rate": 3.604e-05,
      "loss": 0.0035,
      "step": 41880
    },
    {
      "epoch": 2.2341333333333333,
      "grad_norm": 0.40668052434921265,
      "learning_rate": 3.603666666666667e-05,
      "loss": 0.0028,
      "step": 41890
    },
    {
      "epoch": 2.2346666666666666,
      "grad_norm": 0.4642028212547302,
      "learning_rate": 3.603333333333333e-05,
      "loss": 0.0024,
      "step": 41900
    },
    {
      "epoch": 2.2352,
      "grad_norm": 0.35240283608436584,
      "learning_rate": 3.6030000000000006e-05,
      "loss": 0.0028,
      "step": 41910
    },
    {
      "epoch": 2.235733333333333,
      "grad_norm": 0.7294920086860657,
      "learning_rate": 3.602666666666667e-05,
      "loss": 0.0029,
      "step": 41920
    },
    {
      "epoch": 2.236266666666667,
      "grad_norm": 0.4956474006175995,
      "learning_rate": 3.602333333333334e-05,
      "loss": 0.0031,
      "step": 41930
    },
    {
      "epoch": 2.2368,
      "grad_norm": 0.20143043994903564,
      "learning_rate": 3.6020000000000004e-05,
      "loss": 0.0025,
      "step": 41940
    },
    {
      "epoch": 2.2373333333333334,
      "grad_norm": 0.17520079016685486,
      "learning_rate": 3.601666666666667e-05,
      "loss": 0.0034,
      "step": 41950
    },
    {
      "epoch": 2.2378666666666667,
      "grad_norm": 0.2914303243160248,
      "learning_rate": 3.6013333333333336e-05,
      "loss": 0.0023,
      "step": 41960
    },
    {
      "epoch": 2.2384,
      "grad_norm": 0.03596332296729088,
      "learning_rate": 3.601e-05,
      "loss": 0.0026,
      "step": 41970
    },
    {
      "epoch": 2.238933333333333,
      "grad_norm": 0.058539602905511856,
      "learning_rate": 3.600666666666667e-05,
      "loss": 0.0025,
      "step": 41980
    },
    {
      "epoch": 2.2394666666666665,
      "grad_norm": 0.14334037899971008,
      "learning_rate": 3.6003333333333335e-05,
      "loss": 0.0029,
      "step": 41990
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.3068696856498718,
      "learning_rate": 3.6e-05,
      "loss": 0.0026,
      "step": 42000
    },
    {
      "epoch": 2.2405333333333335,
      "grad_norm": 0.35565462708473206,
      "learning_rate": 3.599666666666667e-05,
      "loss": 0.0019,
      "step": 42010
    },
    {
      "epoch": 2.2410666666666668,
      "grad_norm": 0.2663795053958893,
      "learning_rate": 3.599333333333333e-05,
      "loss": 0.0034,
      "step": 42020
    },
    {
      "epoch": 2.2416,
      "grad_norm": 0.4057513177394867,
      "learning_rate": 3.599e-05,
      "loss": 0.0026,
      "step": 42030
    },
    {
      "epoch": 2.2421333333333333,
      "grad_norm": 0.09717167913913727,
      "learning_rate": 3.5986666666666665e-05,
      "loss": 0.0027,
      "step": 42040
    },
    {
      "epoch": 2.2426666666666666,
      "grad_norm": 0.08811609447002411,
      "learning_rate": 3.598333333333334e-05,
      "loss": 0.0033,
      "step": 42050
    },
    {
      "epoch": 2.2432,
      "grad_norm": 0.31831663846969604,
      "learning_rate": 3.5980000000000004e-05,
      "loss": 0.003,
      "step": 42060
    },
    {
      "epoch": 2.243733333333333,
      "grad_norm": 0.8365374803543091,
      "learning_rate": 3.597666666666667e-05,
      "loss": 0.0021,
      "step": 42070
    },
    {
      "epoch": 2.244266666666667,
      "grad_norm": 0.03928428515791893,
      "learning_rate": 3.597333333333334e-05,
      "loss": 0.0031,
      "step": 42080
    },
    {
      "epoch": 2.2448,
      "grad_norm": 0.08884867280721664,
      "learning_rate": 3.597e-05,
      "loss": 0.0035,
      "step": 42090
    },
    {
      "epoch": 2.2453333333333334,
      "grad_norm": 0.11704660952091217,
      "learning_rate": 3.596666666666667e-05,
      "loss": 0.0027,
      "step": 42100
    },
    {
      "epoch": 2.2458666666666667,
      "grad_norm": 0.09117738157510757,
      "learning_rate": 3.5963333333333335e-05,
      "loss": 0.003,
      "step": 42110
    },
    {
      "epoch": 2.2464,
      "grad_norm": 0.29399925470352173,
      "learning_rate": 3.596e-05,
      "loss": 0.0034,
      "step": 42120
    },
    {
      "epoch": 2.2469333333333332,
      "grad_norm": 0.5769919157028198,
      "learning_rate": 3.595666666666667e-05,
      "loss": 0.0033,
      "step": 42130
    },
    {
      "epoch": 2.2474666666666665,
      "grad_norm": 0.5179641246795654,
      "learning_rate": 3.5953333333333334e-05,
      "loss": 0.0037,
      "step": 42140
    },
    {
      "epoch": 2.248,
      "grad_norm": 0.11742845177650452,
      "learning_rate": 3.595e-05,
      "loss": 0.0029,
      "step": 42150
    },
    {
      "epoch": 2.2485333333333335,
      "grad_norm": 0.3085728883743286,
      "learning_rate": 3.5946666666666666e-05,
      "loss": 0.0042,
      "step": 42160
    },
    {
      "epoch": 2.2490666666666668,
      "grad_norm": 0.21024978160858154,
      "learning_rate": 3.594333333333333e-05,
      "loss": 0.0029,
      "step": 42170
    },
    {
      "epoch": 2.2496,
      "grad_norm": 0.5516055822372437,
      "learning_rate": 3.594e-05,
      "loss": 0.0026,
      "step": 42180
    },
    {
      "epoch": 2.2501333333333333,
      "grad_norm": 0.11993251740932465,
      "learning_rate": 3.593666666666667e-05,
      "loss": 0.0029,
      "step": 42190
    },
    {
      "epoch": 2.2506666666666666,
      "grad_norm": 0.2667628824710846,
      "learning_rate": 3.593333333333334e-05,
      "loss": 0.002,
      "step": 42200
    },
    {
      "epoch": 2.2512,
      "grad_norm": 0.03794088959693909,
      "learning_rate": 3.593e-05,
      "loss": 0.0026,
      "step": 42210
    },
    {
      "epoch": 2.251733333333333,
      "grad_norm": 0.17495760321617126,
      "learning_rate": 3.592666666666667e-05,
      "loss": 0.0026,
      "step": 42220
    },
    {
      "epoch": 2.2522666666666664,
      "grad_norm": 0.2657257914543152,
      "learning_rate": 3.5923333333333336e-05,
      "loss": 0.0028,
      "step": 42230
    },
    {
      "epoch": 2.2528,
      "grad_norm": 0.23654206097126007,
      "learning_rate": 3.592e-05,
      "loss": 0.0021,
      "step": 42240
    },
    {
      "epoch": 2.2533333333333334,
      "grad_norm": 0.146419495344162,
      "learning_rate": 3.591666666666667e-05,
      "loss": 0.0025,
      "step": 42250
    },
    {
      "epoch": 2.2538666666666667,
      "grad_norm": 0.3479030430316925,
      "learning_rate": 3.591333333333334e-05,
      "loss": 0.0023,
      "step": 42260
    },
    {
      "epoch": 2.2544,
      "grad_norm": 0.18071475625038147,
      "learning_rate": 3.591e-05,
      "loss": 0.0029,
      "step": 42270
    },
    {
      "epoch": 2.2549333333333332,
      "grad_norm": 0.2100003957748413,
      "learning_rate": 3.5906666666666666e-05,
      "loss": 0.003,
      "step": 42280
    },
    {
      "epoch": 2.2554666666666665,
      "grad_norm": 0.29076674580574036,
      "learning_rate": 3.590333333333333e-05,
      "loss": 0.0035,
      "step": 42290
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 0.04710763692855835,
      "learning_rate": 3.59e-05,
      "loss": 0.0026,
      "step": 42300
    },
    {
      "epoch": 2.2565333333333335,
      "grad_norm": 0.09323824197053909,
      "learning_rate": 3.5896666666666665e-05,
      "loss": 0.0035,
      "step": 42310
    },
    {
      "epoch": 2.2570666666666668,
      "grad_norm": 0.2935141623020172,
      "learning_rate": 3.589333333333334e-05,
      "loss": 0.0019,
      "step": 42320
    },
    {
      "epoch": 2.2576,
      "grad_norm": 0.23524151742458344,
      "learning_rate": 3.5890000000000004e-05,
      "loss": 0.0021,
      "step": 42330
    },
    {
      "epoch": 2.2581333333333333,
      "grad_norm": 0.20829111337661743,
      "learning_rate": 3.588666666666667e-05,
      "loss": 0.0026,
      "step": 42340
    },
    {
      "epoch": 2.2586666666666666,
      "grad_norm": 0.550088107585907,
      "learning_rate": 3.5883333333333336e-05,
      "loss": 0.0023,
      "step": 42350
    },
    {
      "epoch": 2.2592,
      "grad_norm": 0.02719290740787983,
      "learning_rate": 3.588e-05,
      "loss": 0.0035,
      "step": 42360
    },
    {
      "epoch": 2.259733333333333,
      "grad_norm": 0.11669253557920456,
      "learning_rate": 3.587666666666667e-05,
      "loss": 0.0033,
      "step": 42370
    },
    {
      "epoch": 2.2602666666666664,
      "grad_norm": 0.03548984229564667,
      "learning_rate": 3.5873333333333334e-05,
      "loss": 0.0025,
      "step": 42380
    },
    {
      "epoch": 2.2608,
      "grad_norm": 0.2024117261171341,
      "learning_rate": 3.587e-05,
      "loss": 0.0025,
      "step": 42390
    },
    {
      "epoch": 2.2613333333333334,
      "grad_norm": 0.09088491648435593,
      "learning_rate": 3.586666666666667e-05,
      "loss": 0.0041,
      "step": 42400
    },
    {
      "epoch": 2.2618666666666667,
      "grad_norm": 0.060564517974853516,
      "learning_rate": 3.586333333333334e-05,
      "loss": 0.0032,
      "step": 42410
    },
    {
      "epoch": 2.2624,
      "grad_norm": 0.4767695367336273,
      "learning_rate": 3.586e-05,
      "loss": 0.0024,
      "step": 42420
    },
    {
      "epoch": 2.2629333333333332,
      "grad_norm": 0.03328708931803703,
      "learning_rate": 3.5856666666666665e-05,
      "loss": 0.0026,
      "step": 42430
    },
    {
      "epoch": 2.2634666666666665,
      "grad_norm": 0.17658960819244385,
      "learning_rate": 3.585333333333333e-05,
      "loss": 0.0024,
      "step": 42440
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 0.1801270693540573,
      "learning_rate": 3.585e-05,
      "loss": 0.0025,
      "step": 42450
    },
    {
      "epoch": 2.2645333333333335,
      "grad_norm": 0.23356640338897705,
      "learning_rate": 3.584666666666667e-05,
      "loss": 0.0024,
      "step": 42460
    },
    {
      "epoch": 2.265066666666667,
      "grad_norm": 0.1756707727909088,
      "learning_rate": 3.5843333333333336e-05,
      "loss": 0.0031,
      "step": 42470
    },
    {
      "epoch": 2.2656,
      "grad_norm": 0.38227882981300354,
      "learning_rate": 3.584e-05,
      "loss": 0.0033,
      "step": 42480
    },
    {
      "epoch": 2.2661333333333333,
      "grad_norm": 0.23083557188510895,
      "learning_rate": 3.583666666666667e-05,
      "loss": 0.0038,
      "step": 42490
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.06593985110521317,
      "learning_rate": 3.5833333333333335e-05,
      "loss": 0.0033,
      "step": 42500
    },
    {
      "epoch": 2.2672,
      "grad_norm": 0.08633550256490707,
      "learning_rate": 3.583e-05,
      "loss": 0.0029,
      "step": 42510
    },
    {
      "epoch": 2.267733333333333,
      "grad_norm": 0.28704336285591125,
      "learning_rate": 3.582666666666667e-05,
      "loss": 0.0042,
      "step": 42520
    },
    {
      "epoch": 2.2682666666666664,
      "grad_norm": 0.11634417623281479,
      "learning_rate": 3.582333333333334e-05,
      "loss": 0.0032,
      "step": 42530
    },
    {
      "epoch": 2.2688,
      "grad_norm": 0.2694518268108368,
      "learning_rate": 3.5820000000000006e-05,
      "loss": 0.0031,
      "step": 42540
    },
    {
      "epoch": 2.2693333333333334,
      "grad_norm": 0.4351745545864105,
      "learning_rate": 3.581666666666667e-05,
      "loss": 0.0032,
      "step": 42550
    },
    {
      "epoch": 2.2698666666666667,
      "grad_norm": 0.25863638520240784,
      "learning_rate": 3.581333333333334e-05,
      "loss": 0.0029,
      "step": 42560
    },
    {
      "epoch": 2.2704,
      "grad_norm": 0.38007262349128723,
      "learning_rate": 3.581e-05,
      "loss": 0.0032,
      "step": 42570
    },
    {
      "epoch": 2.2709333333333332,
      "grad_norm": 0.2672889828681946,
      "learning_rate": 3.5806666666666664e-05,
      "loss": 0.0021,
      "step": 42580
    },
    {
      "epoch": 2.2714666666666665,
      "grad_norm": 0.037073127925395966,
      "learning_rate": 3.580333333333333e-05,
      "loss": 0.003,
      "step": 42590
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 0.4028870761394501,
      "learning_rate": 3.58e-05,
      "loss": 0.0023,
      "step": 42600
    },
    {
      "epoch": 2.2725333333333335,
      "grad_norm": 0.586209774017334,
      "learning_rate": 3.579666666666667e-05,
      "loss": 0.0018,
      "step": 42610
    },
    {
      "epoch": 2.273066666666667,
      "grad_norm": 0.14755500853061676,
      "learning_rate": 3.5793333333333335e-05,
      "loss": 0.0026,
      "step": 42620
    },
    {
      "epoch": 2.2736,
      "grad_norm": 0.31618162989616394,
      "learning_rate": 3.579e-05,
      "loss": 0.0019,
      "step": 42630
    },
    {
      "epoch": 2.2741333333333333,
      "grad_norm": 0.5192066431045532,
      "learning_rate": 3.578666666666667e-05,
      "loss": 0.0023,
      "step": 42640
    },
    {
      "epoch": 2.2746666666666666,
      "grad_norm": 0.3033585250377655,
      "learning_rate": 3.5783333333333333e-05,
      "loss": 0.0017,
      "step": 42650
    },
    {
      "epoch": 2.2752,
      "grad_norm": 0.22808974981307983,
      "learning_rate": 3.578e-05,
      "loss": 0.0032,
      "step": 42660
    },
    {
      "epoch": 2.275733333333333,
      "grad_norm": 0.1470436304807663,
      "learning_rate": 3.577666666666667e-05,
      "loss": 0.0022,
      "step": 42670
    },
    {
      "epoch": 2.2762666666666664,
      "grad_norm": 0.30623316764831543,
      "learning_rate": 3.577333333333334e-05,
      "loss": 0.0019,
      "step": 42680
    },
    {
      "epoch": 2.2768,
      "grad_norm": 0.48799094557762146,
      "learning_rate": 3.5770000000000005e-05,
      "loss": 0.0018,
      "step": 42690
    },
    {
      "epoch": 2.2773333333333334,
      "grad_norm": 0.48959264159202576,
      "learning_rate": 3.576666666666667e-05,
      "loss": 0.0025,
      "step": 42700
    },
    {
      "epoch": 2.2778666666666667,
      "grad_norm": 0.23189282417297363,
      "learning_rate": 3.576333333333334e-05,
      "loss": 0.0045,
      "step": 42710
    },
    {
      "epoch": 2.2784,
      "grad_norm": 0.43155592679977417,
      "learning_rate": 3.5759999999999996e-05,
      "loss": 0.0027,
      "step": 42720
    },
    {
      "epoch": 2.2789333333333333,
      "grad_norm": 0.20246073603630066,
      "learning_rate": 3.575666666666667e-05,
      "loss": 0.0024,
      "step": 42730
    },
    {
      "epoch": 2.2794666666666665,
      "grad_norm": 0.4158701002597809,
      "learning_rate": 3.5753333333333335e-05,
      "loss": 0.002,
      "step": 42740
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.04251302033662796,
      "learning_rate": 3.575e-05,
      "loss": 0.0029,
      "step": 42750
    },
    {
      "epoch": 2.2805333333333335,
      "grad_norm": 0.14494378864765167,
      "learning_rate": 3.574666666666667e-05,
      "loss": 0.0036,
      "step": 42760
    },
    {
      "epoch": 2.281066666666667,
      "grad_norm": 0.12265858054161072,
      "learning_rate": 3.5743333333333334e-05,
      "loss": 0.0041,
      "step": 42770
    },
    {
      "epoch": 2.2816,
      "grad_norm": 0.21859721839427948,
      "learning_rate": 3.574e-05,
      "loss": 0.0031,
      "step": 42780
    },
    {
      "epoch": 2.2821333333333333,
      "grad_norm": 0.20378853380680084,
      "learning_rate": 3.5736666666666666e-05,
      "loss": 0.0029,
      "step": 42790
    },
    {
      "epoch": 2.2826666666666666,
      "grad_norm": 0.14557352662086487,
      "learning_rate": 3.573333333333333e-05,
      "loss": 0.0041,
      "step": 42800
    },
    {
      "epoch": 2.2832,
      "grad_norm": 0.28855815529823303,
      "learning_rate": 3.5730000000000005e-05,
      "loss": 0.0037,
      "step": 42810
    },
    {
      "epoch": 2.283733333333333,
      "grad_norm": 0.581787109375,
      "learning_rate": 3.572666666666667e-05,
      "loss": 0.0026,
      "step": 42820
    },
    {
      "epoch": 2.2842666666666664,
      "grad_norm": 0.23174065351486206,
      "learning_rate": 3.572333333333334e-05,
      "loss": 0.0036,
      "step": 42830
    },
    {
      "epoch": 2.2848,
      "grad_norm": 0.5767956972122192,
      "learning_rate": 3.5720000000000004e-05,
      "loss": 0.0022,
      "step": 42840
    },
    {
      "epoch": 2.2853333333333334,
      "grad_norm": 0.3443681597709656,
      "learning_rate": 3.571666666666667e-05,
      "loss": 0.0036,
      "step": 42850
    },
    {
      "epoch": 2.2858666666666667,
      "grad_norm": 0.29468825459480286,
      "learning_rate": 3.5713333333333336e-05,
      "loss": 0.0026,
      "step": 42860
    },
    {
      "epoch": 2.2864,
      "grad_norm": 0.6996347308158875,
      "learning_rate": 3.571e-05,
      "loss": 0.0033,
      "step": 42870
    },
    {
      "epoch": 2.2869333333333333,
      "grad_norm": 0.23975631594657898,
      "learning_rate": 3.570666666666667e-05,
      "loss": 0.0039,
      "step": 42880
    },
    {
      "epoch": 2.2874666666666665,
      "grad_norm": 0.019107116386294365,
      "learning_rate": 3.5703333333333334e-05,
      "loss": 0.0018,
      "step": 42890
    },
    {
      "epoch": 2.288,
      "grad_norm": 0.4001378118991852,
      "learning_rate": 3.57e-05,
      "loss": 0.0032,
      "step": 42900
    },
    {
      "epoch": 2.2885333333333335,
      "grad_norm": 0.34670230746269226,
      "learning_rate": 3.5696666666666667e-05,
      "loss": 0.0017,
      "step": 42910
    },
    {
      "epoch": 2.289066666666667,
      "grad_norm": 0.11887946724891663,
      "learning_rate": 3.569333333333333e-05,
      "loss": 0.0029,
      "step": 42920
    },
    {
      "epoch": 2.2896,
      "grad_norm": 0.2292637676000595,
      "learning_rate": 3.569e-05,
      "loss": 0.0028,
      "step": 42930
    },
    {
      "epoch": 2.2901333333333334,
      "grad_norm": 0.4008985459804535,
      "learning_rate": 3.5686666666666665e-05,
      "loss": 0.0025,
      "step": 42940
    },
    {
      "epoch": 2.2906666666666666,
      "grad_norm": 0.6198341250419617,
      "learning_rate": 3.568333333333334e-05,
      "loss": 0.0033,
      "step": 42950
    },
    {
      "epoch": 2.2912,
      "grad_norm": 0.06223325431346893,
      "learning_rate": 3.5680000000000004e-05,
      "loss": 0.0023,
      "step": 42960
    },
    {
      "epoch": 2.291733333333333,
      "grad_norm": 0.47149965167045593,
      "learning_rate": 3.567666666666667e-05,
      "loss": 0.0024,
      "step": 42970
    },
    {
      "epoch": 2.2922666666666665,
      "grad_norm": 0.25688427686691284,
      "learning_rate": 3.5673333333333336e-05,
      "loss": 0.0021,
      "step": 42980
    },
    {
      "epoch": 2.2928,
      "grad_norm": 0.0361018031835556,
      "learning_rate": 3.567e-05,
      "loss": 0.0029,
      "step": 42990
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 0.31824544072151184,
      "learning_rate": 3.566666666666667e-05,
      "loss": 0.003,
      "step": 43000
    },
    {
      "epoch": 2.2938666666666667,
      "grad_norm": 0.2654273509979248,
      "learning_rate": 3.5663333333333335e-05,
      "loss": 0.0029,
      "step": 43010
    },
    {
      "epoch": 2.2944,
      "grad_norm": 0.035681355744600296,
      "learning_rate": 3.566e-05,
      "loss": 0.0023,
      "step": 43020
    },
    {
      "epoch": 2.2949333333333333,
      "grad_norm": 0.6035635471343994,
      "learning_rate": 3.565666666666667e-05,
      "loss": 0.0024,
      "step": 43030
    },
    {
      "epoch": 2.2954666666666665,
      "grad_norm": 0.7226535677909851,
      "learning_rate": 3.565333333333333e-05,
      "loss": 0.0038,
      "step": 43040
    },
    {
      "epoch": 2.296,
      "grad_norm": 0.2894602417945862,
      "learning_rate": 3.565e-05,
      "loss": 0.0035,
      "step": 43050
    },
    {
      "epoch": 2.2965333333333335,
      "grad_norm": 0.28930777311325073,
      "learning_rate": 3.5646666666666665e-05,
      "loss": 0.0023,
      "step": 43060
    },
    {
      "epoch": 2.297066666666667,
      "grad_norm": 0.033311884850263596,
      "learning_rate": 3.564333333333333e-05,
      "loss": 0.0033,
      "step": 43070
    },
    {
      "epoch": 2.2976,
      "grad_norm": 0.03796326741576195,
      "learning_rate": 3.5640000000000004e-05,
      "loss": 0.0024,
      "step": 43080
    },
    {
      "epoch": 2.2981333333333334,
      "grad_norm": 0.20185637474060059,
      "learning_rate": 3.563666666666667e-05,
      "loss": 0.0035,
      "step": 43090
    },
    {
      "epoch": 2.2986666666666666,
      "grad_norm": 0.17627592384815216,
      "learning_rate": 3.563333333333334e-05,
      "loss": 0.0025,
      "step": 43100
    },
    {
      "epoch": 2.2992,
      "grad_norm": 0.18100422620773315,
      "learning_rate": 3.563e-05,
      "loss": 0.0023,
      "step": 43110
    },
    {
      "epoch": 2.299733333333333,
      "grad_norm": 0.14944806694984436,
      "learning_rate": 3.562666666666667e-05,
      "loss": 0.0027,
      "step": 43120
    },
    {
      "epoch": 2.3002666666666665,
      "grad_norm": 0.06477317214012146,
      "learning_rate": 3.5623333333333335e-05,
      "loss": 0.0024,
      "step": 43130
    },
    {
      "epoch": 2.3008,
      "grad_norm": 0.4129667580127716,
      "learning_rate": 3.562e-05,
      "loss": 0.0026,
      "step": 43140
    },
    {
      "epoch": 2.3013333333333335,
      "grad_norm": 0.044695720076560974,
      "learning_rate": 3.561666666666667e-05,
      "loss": 0.0025,
      "step": 43150
    },
    {
      "epoch": 2.3018666666666667,
      "grad_norm": 0.42513352632522583,
      "learning_rate": 3.561333333333334e-05,
      "loss": 0.0035,
      "step": 43160
    },
    {
      "epoch": 2.3024,
      "grad_norm": 0.2315935641527176,
      "learning_rate": 3.5610000000000006e-05,
      "loss": 0.0037,
      "step": 43170
    },
    {
      "epoch": 2.3029333333333333,
      "grad_norm": 0.4947057068347931,
      "learning_rate": 3.5606666666666666e-05,
      "loss": 0.0016,
      "step": 43180
    },
    {
      "epoch": 2.3034666666666666,
      "grad_norm": 0.027743110433220863,
      "learning_rate": 3.560333333333333e-05,
      "loss": 0.0029,
      "step": 43190
    },
    {
      "epoch": 2.304,
      "grad_norm": 0.1437336802482605,
      "learning_rate": 3.56e-05,
      "loss": 0.0028,
      "step": 43200
    },
    {
      "epoch": 2.3045333333333335,
      "grad_norm": 0.5224885940551758,
      "learning_rate": 3.5596666666666664e-05,
      "loss": 0.0028,
      "step": 43210
    },
    {
      "epoch": 2.305066666666667,
      "grad_norm": 0.17802157998085022,
      "learning_rate": 3.559333333333334e-05,
      "loss": 0.0023,
      "step": 43220
    },
    {
      "epoch": 2.3056,
      "grad_norm": 0.23132146894931793,
      "learning_rate": 3.559e-05,
      "loss": 0.0031,
      "step": 43230
    },
    {
      "epoch": 2.3061333333333334,
      "grad_norm": 0.16941720247268677,
      "learning_rate": 3.558666666666667e-05,
      "loss": 0.0028,
      "step": 43240
    },
    {
      "epoch": 2.3066666666666666,
      "grad_norm": 0.23959219455718994,
      "learning_rate": 3.5583333333333335e-05,
      "loss": 0.0023,
      "step": 43250
    },
    {
      "epoch": 2.3072,
      "grad_norm": 0.6143646836280823,
      "learning_rate": 3.558e-05,
      "loss": 0.0037,
      "step": 43260
    },
    {
      "epoch": 2.307733333333333,
      "grad_norm": 0.2338392287492752,
      "learning_rate": 3.557666666666667e-05,
      "loss": 0.0025,
      "step": 43270
    },
    {
      "epoch": 2.3082666666666665,
      "grad_norm": 0.10276336222887039,
      "learning_rate": 3.5573333333333334e-05,
      "loss": 0.0031,
      "step": 43280
    },
    {
      "epoch": 2.3088,
      "grad_norm": 0.041986458003520966,
      "learning_rate": 3.557e-05,
      "loss": 0.0018,
      "step": 43290
    },
    {
      "epoch": 2.3093333333333335,
      "grad_norm": 0.12701311707496643,
      "learning_rate": 3.556666666666667e-05,
      "loss": 0.0041,
      "step": 43300
    },
    {
      "epoch": 2.3098666666666667,
      "grad_norm": 0.28992438316345215,
      "learning_rate": 3.556333333333334e-05,
      "loss": 0.0025,
      "step": 43310
    },
    {
      "epoch": 2.3104,
      "grad_norm": 0.5118385553359985,
      "learning_rate": 3.5560000000000005e-05,
      "loss": 0.0027,
      "step": 43320
    },
    {
      "epoch": 2.3109333333333333,
      "grad_norm": 0.26220545172691345,
      "learning_rate": 3.5556666666666664e-05,
      "loss": 0.002,
      "step": 43330
    },
    {
      "epoch": 2.3114666666666666,
      "grad_norm": 0.23408882319927216,
      "learning_rate": 3.555333333333333e-05,
      "loss": 0.0027,
      "step": 43340
    },
    {
      "epoch": 2.312,
      "grad_norm": 0.3259655833244324,
      "learning_rate": 3.555e-05,
      "loss": 0.0028,
      "step": 43350
    },
    {
      "epoch": 2.3125333333333336,
      "grad_norm": 0.14601360261440277,
      "learning_rate": 3.554666666666667e-05,
      "loss": 0.002,
      "step": 43360
    },
    {
      "epoch": 2.313066666666667,
      "grad_norm": 0.2588377892971039,
      "learning_rate": 3.5543333333333336e-05,
      "loss": 0.002,
      "step": 43370
    },
    {
      "epoch": 2.3136,
      "grad_norm": 0.20407910645008087,
      "learning_rate": 3.554e-05,
      "loss": 0.0017,
      "step": 43380
    },
    {
      "epoch": 2.3141333333333334,
      "grad_norm": 0.27302786707878113,
      "learning_rate": 3.553666666666667e-05,
      "loss": 0.0022,
      "step": 43390
    },
    {
      "epoch": 2.3146666666666667,
      "grad_norm": 0.2591831386089325,
      "learning_rate": 3.5533333333333334e-05,
      "loss": 0.0027,
      "step": 43400
    },
    {
      "epoch": 2.3152,
      "grad_norm": 0.042377032339572906,
      "learning_rate": 3.553e-05,
      "loss": 0.0028,
      "step": 43410
    },
    {
      "epoch": 2.315733333333333,
      "grad_norm": 0.10902777314186096,
      "learning_rate": 3.5526666666666666e-05,
      "loss": 0.0031,
      "step": 43420
    },
    {
      "epoch": 2.3162666666666665,
      "grad_norm": 0.020690223202109337,
      "learning_rate": 3.552333333333334e-05,
      "loss": 0.0028,
      "step": 43430
    },
    {
      "epoch": 2.3168,
      "grad_norm": 0.34685584902763367,
      "learning_rate": 3.5520000000000006e-05,
      "loss": 0.0036,
      "step": 43440
    },
    {
      "epoch": 2.3173333333333335,
      "grad_norm": 0.47185075283050537,
      "learning_rate": 3.551666666666667e-05,
      "loss": 0.002,
      "step": 43450
    },
    {
      "epoch": 2.3178666666666667,
      "grad_norm": 0.17702706158161163,
      "learning_rate": 3.551333333333334e-05,
      "loss": 0.0032,
      "step": 43460
    },
    {
      "epoch": 2.3184,
      "grad_norm": 0.08826155960559845,
      "learning_rate": 3.5510000000000004e-05,
      "loss": 0.0027,
      "step": 43470
    },
    {
      "epoch": 2.3189333333333333,
      "grad_norm": 0.17489659786224365,
      "learning_rate": 3.550666666666666e-05,
      "loss": 0.0036,
      "step": 43480
    },
    {
      "epoch": 2.3194666666666666,
      "grad_norm": 0.02630648948252201,
      "learning_rate": 3.5503333333333336e-05,
      "loss": 0.0033,
      "step": 43490
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.32028573751449585,
      "learning_rate": 3.55e-05,
      "loss": 0.0038,
      "step": 43500
    },
    {
      "epoch": 2.3205333333333336,
      "grad_norm": 0.4199873208999634,
      "learning_rate": 3.549666666666667e-05,
      "loss": 0.0021,
      "step": 43510
    },
    {
      "epoch": 2.321066666666667,
      "grad_norm": 0.4038388431072235,
      "learning_rate": 3.5493333333333335e-05,
      "loss": 0.003,
      "step": 43520
    },
    {
      "epoch": 2.3216,
      "grad_norm": 0.08946342766284943,
      "learning_rate": 3.549e-05,
      "loss": 0.0029,
      "step": 43530
    },
    {
      "epoch": 2.3221333333333334,
      "grad_norm": 0.4878518283367157,
      "learning_rate": 3.548666666666667e-05,
      "loss": 0.0029,
      "step": 43540
    },
    {
      "epoch": 2.3226666666666667,
      "grad_norm": 0.5857495665550232,
      "learning_rate": 3.548333333333333e-05,
      "loss": 0.003,
      "step": 43550
    },
    {
      "epoch": 2.3232,
      "grad_norm": 0.21414634585380554,
      "learning_rate": 3.548e-05,
      "loss": 0.0038,
      "step": 43560
    },
    {
      "epoch": 2.323733333333333,
      "grad_norm": 0.6188393235206604,
      "learning_rate": 3.547666666666667e-05,
      "loss": 0.0047,
      "step": 43570
    },
    {
      "epoch": 2.3242666666666665,
      "grad_norm": 0.23391330242156982,
      "learning_rate": 3.547333333333334e-05,
      "loss": 0.0028,
      "step": 43580
    },
    {
      "epoch": 2.3247999999999998,
      "grad_norm": 0.626581072807312,
      "learning_rate": 3.5470000000000004e-05,
      "loss": 0.0033,
      "step": 43590
    },
    {
      "epoch": 2.3253333333333335,
      "grad_norm": 0.03669384866952896,
      "learning_rate": 3.546666666666667e-05,
      "loss": 0.0023,
      "step": 43600
    },
    {
      "epoch": 2.3258666666666667,
      "grad_norm": 0.05070769786834717,
      "learning_rate": 3.5463333333333337e-05,
      "loss": 0.0032,
      "step": 43610
    },
    {
      "epoch": 2.3264,
      "grad_norm": 0.26193901896476746,
      "learning_rate": 3.546e-05,
      "loss": 0.0049,
      "step": 43620
    },
    {
      "epoch": 2.3269333333333333,
      "grad_norm": 0.3257101774215698,
      "learning_rate": 3.545666666666667e-05,
      "loss": 0.0025,
      "step": 43630
    },
    {
      "epoch": 2.3274666666666666,
      "grad_norm": 0.49313604831695557,
      "learning_rate": 3.5453333333333335e-05,
      "loss": 0.0029,
      "step": 43640
    },
    {
      "epoch": 2.328,
      "grad_norm": 0.6270261406898499,
      "learning_rate": 3.545e-05,
      "loss": 0.0023,
      "step": 43650
    },
    {
      "epoch": 2.3285333333333336,
      "grad_norm": 0.07009971141815186,
      "learning_rate": 3.544666666666667e-05,
      "loss": 0.0017,
      "step": 43660
    },
    {
      "epoch": 2.329066666666667,
      "grad_norm": 0.2915870249271393,
      "learning_rate": 3.544333333333333e-05,
      "loss": 0.0033,
      "step": 43670
    },
    {
      "epoch": 2.3296,
      "grad_norm": 0.3184235990047455,
      "learning_rate": 3.544e-05,
      "loss": 0.0023,
      "step": 43680
    },
    {
      "epoch": 2.3301333333333334,
      "grad_norm": 0.14482468366622925,
      "learning_rate": 3.5436666666666666e-05,
      "loss": 0.0029,
      "step": 43690
    },
    {
      "epoch": 2.3306666666666667,
      "grad_norm": 0.06566935032606125,
      "learning_rate": 3.543333333333333e-05,
      "loss": 0.003,
      "step": 43700
    },
    {
      "epoch": 2.3312,
      "grad_norm": 0.23338788747787476,
      "learning_rate": 3.5430000000000005e-05,
      "loss": 0.0041,
      "step": 43710
    },
    {
      "epoch": 2.331733333333333,
      "grad_norm": 0.15496014058589935,
      "learning_rate": 3.542666666666667e-05,
      "loss": 0.0019,
      "step": 43720
    },
    {
      "epoch": 2.3322666666666665,
      "grad_norm": 0.03248925879597664,
      "learning_rate": 3.542333333333334e-05,
      "loss": 0.003,
      "step": 43730
    },
    {
      "epoch": 2.3327999999999998,
      "grad_norm": 0.5073991417884827,
      "learning_rate": 3.542e-05,
      "loss": 0.0019,
      "step": 43740
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.4729548394680023,
      "learning_rate": 3.541666666666667e-05,
      "loss": 0.0026,
      "step": 43750
    },
    {
      "epoch": 2.3338666666666668,
      "grad_norm": 0.17973926663398743,
      "learning_rate": 3.5413333333333335e-05,
      "loss": 0.0032,
      "step": 43760
    },
    {
      "epoch": 2.3344,
      "grad_norm": 0.42310893535614014,
      "learning_rate": 3.541e-05,
      "loss": 0.0041,
      "step": 43770
    },
    {
      "epoch": 2.3349333333333333,
      "grad_norm": 0.030168352648615837,
      "learning_rate": 3.540666666666667e-05,
      "loss": 0.004,
      "step": 43780
    },
    {
      "epoch": 2.3354666666666666,
      "grad_norm": 0.03072305954992771,
      "learning_rate": 3.5403333333333334e-05,
      "loss": 0.0034,
      "step": 43790
    },
    {
      "epoch": 2.336,
      "grad_norm": 0.09310105443000793,
      "learning_rate": 3.54e-05,
      "loss": 0.0028,
      "step": 43800
    },
    {
      "epoch": 2.3365333333333336,
      "grad_norm": 0.2041437178850174,
      "learning_rate": 3.5396666666666666e-05,
      "loss": 0.002,
      "step": 43810
    },
    {
      "epoch": 2.337066666666667,
      "grad_norm": 0.17404654622077942,
      "learning_rate": 3.539333333333333e-05,
      "loss": 0.0025,
      "step": 43820
    },
    {
      "epoch": 2.3376,
      "grad_norm": 0.22924794256687164,
      "learning_rate": 3.539e-05,
      "loss": 0.0029,
      "step": 43830
    },
    {
      "epoch": 2.3381333333333334,
      "grad_norm": 0.6118143200874329,
      "learning_rate": 3.538666666666667e-05,
      "loss": 0.0031,
      "step": 43840
    },
    {
      "epoch": 2.3386666666666667,
      "grad_norm": 0.40554356575012207,
      "learning_rate": 3.538333333333334e-05,
      "loss": 0.0026,
      "step": 43850
    },
    {
      "epoch": 2.3392,
      "grad_norm": 0.15218102931976318,
      "learning_rate": 3.5380000000000003e-05,
      "loss": 0.0029,
      "step": 43860
    },
    {
      "epoch": 2.339733333333333,
      "grad_norm": 0.24531468749046326,
      "learning_rate": 3.537666666666667e-05,
      "loss": 0.0033,
      "step": 43870
    },
    {
      "epoch": 2.3402666666666665,
      "grad_norm": 0.4326016306877136,
      "learning_rate": 3.5373333333333336e-05,
      "loss": 0.0028,
      "step": 43880
    },
    {
      "epoch": 2.3407999999999998,
      "grad_norm": 0.11734932661056519,
      "learning_rate": 3.537e-05,
      "loss": 0.003,
      "step": 43890
    },
    {
      "epoch": 2.3413333333333335,
      "grad_norm": 0.25460541248321533,
      "learning_rate": 3.536666666666667e-05,
      "loss": 0.0028,
      "step": 43900
    },
    {
      "epoch": 2.3418666666666668,
      "grad_norm": 0.146353617310524,
      "learning_rate": 3.5363333333333334e-05,
      "loss": 0.0026,
      "step": 43910
    },
    {
      "epoch": 2.3424,
      "grad_norm": 0.3232306241989136,
      "learning_rate": 3.536000000000001e-05,
      "loss": 0.0038,
      "step": 43920
    },
    {
      "epoch": 2.3429333333333333,
      "grad_norm": 0.40795695781707764,
      "learning_rate": 3.5356666666666666e-05,
      "loss": 0.0029,
      "step": 43930
    },
    {
      "epoch": 2.3434666666666666,
      "grad_norm": 0.027406062930822372,
      "learning_rate": 3.535333333333333e-05,
      "loss": 0.0034,
      "step": 43940
    },
    {
      "epoch": 2.344,
      "grad_norm": 0.039566945284605026,
      "learning_rate": 3.535e-05,
      "loss": 0.0039,
      "step": 43950
    },
    {
      "epoch": 2.3445333333333336,
      "grad_norm": 0.7085254788398743,
      "learning_rate": 3.5346666666666665e-05,
      "loss": 0.0027,
      "step": 43960
    },
    {
      "epoch": 2.345066666666667,
      "grad_norm": 0.43785396218299866,
      "learning_rate": 3.534333333333333e-05,
      "loss": 0.0022,
      "step": 43970
    },
    {
      "epoch": 2.3456,
      "grad_norm": 0.0887492224574089,
      "learning_rate": 3.5340000000000004e-05,
      "loss": 0.0012,
      "step": 43980
    },
    {
      "epoch": 2.3461333333333334,
      "grad_norm": 0.050126805901527405,
      "learning_rate": 3.533666666666667e-05,
      "loss": 0.0029,
      "step": 43990
    },
    {
      "epoch": 2.3466666666666667,
      "grad_norm": 0.17256686091423035,
      "learning_rate": 3.5333333333333336e-05,
      "loss": 0.0022,
      "step": 44000
    },
    {
      "epoch": 2.3472,
      "grad_norm": 0.32297584414482117,
      "learning_rate": 3.533e-05,
      "loss": 0.0035,
      "step": 44010
    },
    {
      "epoch": 2.3477333333333332,
      "grad_norm": 0.24175290763378143,
      "learning_rate": 3.532666666666667e-05,
      "loss": 0.0026,
      "step": 44020
    },
    {
      "epoch": 2.3482666666666665,
      "grad_norm": 0.0985163003206253,
      "learning_rate": 3.5323333333333335e-05,
      "loss": 0.0031,
      "step": 44030
    },
    {
      "epoch": 2.3487999999999998,
      "grad_norm": 0.026087643578648567,
      "learning_rate": 3.532e-05,
      "loss": 0.0022,
      "step": 44040
    },
    {
      "epoch": 2.3493333333333335,
      "grad_norm": 0.30209705233573914,
      "learning_rate": 3.531666666666667e-05,
      "loss": 0.0039,
      "step": 44050
    },
    {
      "epoch": 2.3498666666666668,
      "grad_norm": 0.202161505818367,
      "learning_rate": 3.531333333333334e-05,
      "loss": 0.0036,
      "step": 44060
    },
    {
      "epoch": 2.3504,
      "grad_norm": 0.2296973168849945,
      "learning_rate": 3.5310000000000006e-05,
      "loss": 0.003,
      "step": 44070
    },
    {
      "epoch": 2.3509333333333333,
      "grad_norm": 0.4183215796947479,
      "learning_rate": 3.5306666666666665e-05,
      "loss": 0.0029,
      "step": 44080
    },
    {
      "epoch": 2.3514666666666666,
      "grad_norm": 0.14466258883476257,
      "learning_rate": 3.530333333333333e-05,
      "loss": 0.0027,
      "step": 44090
    },
    {
      "epoch": 2.352,
      "grad_norm": 0.04941657558083534,
      "learning_rate": 3.53e-05,
      "loss": 0.0026,
      "step": 44100
    },
    {
      "epoch": 2.352533333333333,
      "grad_norm": 0.6956992149353027,
      "learning_rate": 3.5296666666666664e-05,
      "loss": 0.0021,
      "step": 44110
    },
    {
      "epoch": 2.353066666666667,
      "grad_norm": 0.03587058559060097,
      "learning_rate": 3.5293333333333336e-05,
      "loss": 0.0043,
      "step": 44120
    },
    {
      "epoch": 2.3536,
      "grad_norm": 0.16109992563724518,
      "learning_rate": 3.529e-05,
      "loss": 0.0021,
      "step": 44130
    },
    {
      "epoch": 2.3541333333333334,
      "grad_norm": 0.11875078827142715,
      "learning_rate": 3.528666666666667e-05,
      "loss": 0.0019,
      "step": 44140
    },
    {
      "epoch": 2.3546666666666667,
      "grad_norm": 0.12366453558206558,
      "learning_rate": 3.5283333333333335e-05,
      "loss": 0.0023,
      "step": 44150
    },
    {
      "epoch": 2.3552,
      "grad_norm": 0.21239039301872253,
      "learning_rate": 3.528e-05,
      "loss": 0.002,
      "step": 44160
    },
    {
      "epoch": 2.3557333333333332,
      "grad_norm": 0.05686987191438675,
      "learning_rate": 3.527666666666667e-05,
      "loss": 0.0027,
      "step": 44170
    },
    {
      "epoch": 2.3562666666666665,
      "grad_norm": 0.5532912015914917,
      "learning_rate": 3.527333333333333e-05,
      "loss": 0.0028,
      "step": 44180
    },
    {
      "epoch": 2.3568,
      "grad_norm": 0.2906343638896942,
      "learning_rate": 3.5270000000000006e-05,
      "loss": 0.003,
      "step": 44190
    },
    {
      "epoch": 2.3573333333333335,
      "grad_norm": 0.4925050735473633,
      "learning_rate": 3.526666666666667e-05,
      "loss": 0.0022,
      "step": 44200
    },
    {
      "epoch": 2.3578666666666668,
      "grad_norm": 0.118240587413311,
      "learning_rate": 3.526333333333334e-05,
      "loss": 0.0029,
      "step": 44210
    },
    {
      "epoch": 2.3584,
      "grad_norm": 0.17444650828838348,
      "learning_rate": 3.5260000000000005e-05,
      "loss": 0.0027,
      "step": 44220
    },
    {
      "epoch": 2.3589333333333333,
      "grad_norm": 0.3424190878868103,
      "learning_rate": 3.5256666666666664e-05,
      "loss": 0.0028,
      "step": 44230
    },
    {
      "epoch": 2.3594666666666666,
      "grad_norm": 0.6226552128791809,
      "learning_rate": 3.525333333333333e-05,
      "loss": 0.002,
      "step": 44240
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.11986491829156876,
      "learning_rate": 3.525e-05,
      "loss": 0.0025,
      "step": 44250
    },
    {
      "epoch": 2.360533333333333,
      "grad_norm": 0.032279420644044876,
      "learning_rate": 3.524666666666667e-05,
      "loss": 0.0017,
      "step": 44260
    },
    {
      "epoch": 2.361066666666667,
      "grad_norm": 0.038357555866241455,
      "learning_rate": 3.5243333333333335e-05,
      "loss": 0.0038,
      "step": 44270
    },
    {
      "epoch": 2.3616,
      "grad_norm": 0.4308556616306305,
      "learning_rate": 3.524e-05,
      "loss": 0.0022,
      "step": 44280
    },
    {
      "epoch": 2.3621333333333334,
      "grad_norm": 0.28958767652511597,
      "learning_rate": 3.523666666666667e-05,
      "loss": 0.0033,
      "step": 44290
    },
    {
      "epoch": 2.3626666666666667,
      "grad_norm": 0.24086764454841614,
      "learning_rate": 3.5233333333333334e-05,
      "loss": 0.0024,
      "step": 44300
    },
    {
      "epoch": 2.3632,
      "grad_norm": 0.2895221710205078,
      "learning_rate": 3.523e-05,
      "loss": 0.0038,
      "step": 44310
    },
    {
      "epoch": 2.3637333333333332,
      "grad_norm": 0.5305967330932617,
      "learning_rate": 3.5226666666666666e-05,
      "loss": 0.0026,
      "step": 44320
    },
    {
      "epoch": 2.3642666666666665,
      "grad_norm": 0.37122148275375366,
      "learning_rate": 3.522333333333334e-05,
      "loss": 0.0028,
      "step": 44330
    },
    {
      "epoch": 2.3648,
      "grad_norm": 0.40514808893203735,
      "learning_rate": 3.5220000000000005e-05,
      "loss": 0.0029,
      "step": 44340
    },
    {
      "epoch": 2.3653333333333335,
      "grad_norm": 0.26241031289100647,
      "learning_rate": 3.521666666666667e-05,
      "loss": 0.0027,
      "step": 44350
    },
    {
      "epoch": 2.365866666666667,
      "grad_norm": 0.4120812714099884,
      "learning_rate": 3.521333333333334e-05,
      "loss": 0.0034,
      "step": 44360
    },
    {
      "epoch": 2.3664,
      "grad_norm": 0.238872230052948,
      "learning_rate": 3.5210000000000003e-05,
      "loss": 0.002,
      "step": 44370
    },
    {
      "epoch": 2.3669333333333333,
      "grad_norm": 0.5393229722976685,
      "learning_rate": 3.520666666666667e-05,
      "loss": 0.0034,
      "step": 44380
    },
    {
      "epoch": 2.3674666666666666,
      "grad_norm": 0.497950941324234,
      "learning_rate": 3.5203333333333336e-05,
      "loss": 0.0024,
      "step": 44390
    },
    {
      "epoch": 2.368,
      "grad_norm": 0.43412819504737854,
      "learning_rate": 3.52e-05,
      "loss": 0.0027,
      "step": 44400
    },
    {
      "epoch": 2.368533333333333,
      "grad_norm": 0.5750873684883118,
      "learning_rate": 3.519666666666667e-05,
      "loss": 0.0022,
      "step": 44410
    },
    {
      "epoch": 2.369066666666667,
      "grad_norm": 0.211721733212471,
      "learning_rate": 3.5193333333333334e-05,
      "loss": 0.0045,
      "step": 44420
    },
    {
      "epoch": 2.3696,
      "grad_norm": 0.0737801119685173,
      "learning_rate": 3.519e-05,
      "loss": 0.0029,
      "step": 44430
    },
    {
      "epoch": 2.3701333333333334,
      "grad_norm": 0.41897329688072205,
      "learning_rate": 3.5186666666666666e-05,
      "loss": 0.003,
      "step": 44440
    },
    {
      "epoch": 2.3706666666666667,
      "grad_norm": 0.1476135104894638,
      "learning_rate": 3.518333333333333e-05,
      "loss": 0.0033,
      "step": 44450
    },
    {
      "epoch": 2.3712,
      "grad_norm": 0.013189475052058697,
      "learning_rate": 3.518e-05,
      "loss": 0.0027,
      "step": 44460
    },
    {
      "epoch": 2.3717333333333332,
      "grad_norm": 0.0701114684343338,
      "learning_rate": 3.517666666666667e-05,
      "loss": 0.0029,
      "step": 44470
    },
    {
      "epoch": 2.3722666666666665,
      "grad_norm": 0.2697645425796509,
      "learning_rate": 3.517333333333334e-05,
      "loss": 0.0024,
      "step": 44480
    },
    {
      "epoch": 2.3728,
      "grad_norm": 0.2892051041126251,
      "learning_rate": 3.5170000000000004e-05,
      "loss": 0.0034,
      "step": 44490
    },
    {
      "epoch": 2.3733333333333335,
      "grad_norm": 0.035722896456718445,
      "learning_rate": 3.516666666666667e-05,
      "loss": 0.0025,
      "step": 44500
    },
    {
      "epoch": 2.373866666666667,
      "grad_norm": 0.01923009194433689,
      "learning_rate": 3.5163333333333336e-05,
      "loss": 0.0031,
      "step": 44510
    },
    {
      "epoch": 2.3744,
      "grad_norm": 0.124422587454319,
      "learning_rate": 3.516e-05,
      "loss": 0.0023,
      "step": 44520
    },
    {
      "epoch": 2.3749333333333333,
      "grad_norm": 0.019055908545851707,
      "learning_rate": 3.515666666666667e-05,
      "loss": 0.0023,
      "step": 44530
    },
    {
      "epoch": 2.3754666666666666,
      "grad_norm": 0.047013379633426666,
      "learning_rate": 3.5153333333333334e-05,
      "loss": 0.0025,
      "step": 44540
    },
    {
      "epoch": 2.376,
      "grad_norm": 0.23580652475357056,
      "learning_rate": 3.515e-05,
      "loss": 0.0037,
      "step": 44550
    },
    {
      "epoch": 2.376533333333333,
      "grad_norm": 0.5115320086479187,
      "learning_rate": 3.514666666666667e-05,
      "loss": 0.0032,
      "step": 44560
    },
    {
      "epoch": 2.377066666666667,
      "grad_norm": 0.6961555480957031,
      "learning_rate": 3.514333333333333e-05,
      "loss": 0.0025,
      "step": 44570
    },
    {
      "epoch": 2.3776,
      "grad_norm": 0.7223740220069885,
      "learning_rate": 3.514e-05,
      "loss": 0.0023,
      "step": 44580
    },
    {
      "epoch": 2.3781333333333334,
      "grad_norm": 0.23266512155532837,
      "learning_rate": 3.5136666666666665e-05,
      "loss": 0.003,
      "step": 44590
    },
    {
      "epoch": 2.3786666666666667,
      "grad_norm": 0.2098711133003235,
      "learning_rate": 3.513333333333334e-05,
      "loss": 0.0029,
      "step": 44600
    },
    {
      "epoch": 2.3792,
      "grad_norm": 0.03350793942809105,
      "learning_rate": 3.5130000000000004e-05,
      "loss": 0.0027,
      "step": 44610
    },
    {
      "epoch": 2.3797333333333333,
      "grad_norm": 0.2666216790676117,
      "learning_rate": 3.512666666666667e-05,
      "loss": 0.0027,
      "step": 44620
    },
    {
      "epoch": 2.3802666666666665,
      "grad_norm": 0.4024527072906494,
      "learning_rate": 3.5123333333333336e-05,
      "loss": 0.0026,
      "step": 44630
    },
    {
      "epoch": 2.3808,
      "grad_norm": 0.17364224791526794,
      "learning_rate": 3.512e-05,
      "loss": 0.0022,
      "step": 44640
    },
    {
      "epoch": 2.3813333333333335,
      "grad_norm": 0.022407291457057,
      "learning_rate": 3.511666666666667e-05,
      "loss": 0.0019,
      "step": 44650
    },
    {
      "epoch": 2.381866666666667,
      "grad_norm": 0.4946835935115814,
      "learning_rate": 3.5113333333333335e-05,
      "loss": 0.0025,
      "step": 44660
    },
    {
      "epoch": 2.3824,
      "grad_norm": 0.40171411633491516,
      "learning_rate": 3.511e-05,
      "loss": 0.0028,
      "step": 44670
    },
    {
      "epoch": 2.3829333333333333,
      "grad_norm": 0.03372221812605858,
      "learning_rate": 3.5106666666666674e-05,
      "loss": 0.0034,
      "step": 44680
    },
    {
      "epoch": 2.3834666666666666,
      "grad_norm": 0.40867993235588074,
      "learning_rate": 3.510333333333333e-05,
      "loss": 0.0029,
      "step": 44690
    },
    {
      "epoch": 2.384,
      "grad_norm": 0.29011279344558716,
      "learning_rate": 3.51e-05,
      "loss": 0.0019,
      "step": 44700
    },
    {
      "epoch": 2.384533333333333,
      "grad_norm": 0.018330732360482216,
      "learning_rate": 3.5096666666666665e-05,
      "loss": 0.0027,
      "step": 44710
    },
    {
      "epoch": 2.385066666666667,
      "grad_norm": 0.08957813680171967,
      "learning_rate": 3.509333333333333e-05,
      "loss": 0.0031,
      "step": 44720
    },
    {
      "epoch": 2.3856,
      "grad_norm": 0.11860644817352295,
      "learning_rate": 3.509e-05,
      "loss": 0.0014,
      "step": 44730
    },
    {
      "epoch": 2.3861333333333334,
      "grad_norm": 0.48925620317459106,
      "learning_rate": 3.508666666666667e-05,
      "loss": 0.0027,
      "step": 44740
    },
    {
      "epoch": 2.3866666666666667,
      "grad_norm": 0.31778210401535034,
      "learning_rate": 3.508333333333334e-05,
      "loss": 0.0013,
      "step": 44750
    },
    {
      "epoch": 2.3872,
      "grad_norm": 0.40593990683555603,
      "learning_rate": 3.508e-05,
      "loss": 0.0024,
      "step": 44760
    },
    {
      "epoch": 2.3877333333333333,
      "grad_norm": 0.19984377920627594,
      "learning_rate": 3.507666666666667e-05,
      "loss": 0.0031,
      "step": 44770
    },
    {
      "epoch": 2.3882666666666665,
      "grad_norm": 0.4003359079360962,
      "learning_rate": 3.5073333333333335e-05,
      "loss": 0.0027,
      "step": 44780
    },
    {
      "epoch": 2.3888,
      "grad_norm": 0.3457629680633545,
      "learning_rate": 3.507e-05,
      "loss": 0.0017,
      "step": 44790
    },
    {
      "epoch": 2.389333333333333,
      "grad_norm": 0.03934482857584953,
      "learning_rate": 3.506666666666667e-05,
      "loss": 0.0022,
      "step": 44800
    },
    {
      "epoch": 2.389866666666667,
      "grad_norm": 0.48805949091911316,
      "learning_rate": 3.5063333333333334e-05,
      "loss": 0.0024,
      "step": 44810
    },
    {
      "epoch": 2.3904,
      "grad_norm": 0.038507040590047836,
      "learning_rate": 3.5060000000000007e-05,
      "loss": 0.0025,
      "step": 44820
    },
    {
      "epoch": 2.3909333333333334,
      "grad_norm": 0.5763901472091675,
      "learning_rate": 3.505666666666667e-05,
      "loss": 0.002,
      "step": 44830
    },
    {
      "epoch": 2.3914666666666666,
      "grad_norm": 0.022088363766670227,
      "learning_rate": 3.505333333333333e-05,
      "loss": 0.0032,
      "step": 44840
    },
    {
      "epoch": 2.392,
      "grad_norm": 0.3795434534549713,
      "learning_rate": 3.505e-05,
      "loss": 0.0025,
      "step": 44850
    },
    {
      "epoch": 2.392533333333333,
      "grad_norm": 0.370433509349823,
      "learning_rate": 3.5046666666666664e-05,
      "loss": 0.0025,
      "step": 44860
    },
    {
      "epoch": 2.393066666666667,
      "grad_norm": 0.37622755765914917,
      "learning_rate": 3.504333333333333e-05,
      "loss": 0.0024,
      "step": 44870
    },
    {
      "epoch": 2.3936,
      "grad_norm": 0.2313188761472702,
      "learning_rate": 3.504e-05,
      "loss": 0.0033,
      "step": 44880
    },
    {
      "epoch": 2.3941333333333334,
      "grad_norm": 0.09355710446834564,
      "learning_rate": 3.503666666666667e-05,
      "loss": 0.0025,
      "step": 44890
    },
    {
      "epoch": 2.3946666666666667,
      "grad_norm": 0.2954540252685547,
      "learning_rate": 3.5033333333333336e-05,
      "loss": 0.0021,
      "step": 44900
    },
    {
      "epoch": 2.3952,
      "grad_norm": 0.05675790458917618,
      "learning_rate": 3.503e-05,
      "loss": 0.0021,
      "step": 44910
    },
    {
      "epoch": 2.3957333333333333,
      "grad_norm": 0.20028258860111237,
      "learning_rate": 3.502666666666667e-05,
      "loss": 0.0038,
      "step": 44920
    },
    {
      "epoch": 2.3962666666666665,
      "grad_norm": 0.15902240574359894,
      "learning_rate": 3.5023333333333334e-05,
      "loss": 0.004,
      "step": 44930
    },
    {
      "epoch": 2.3968,
      "grad_norm": 0.4318614900112152,
      "learning_rate": 3.502e-05,
      "loss": 0.0017,
      "step": 44940
    },
    {
      "epoch": 2.397333333333333,
      "grad_norm": 0.5588257312774658,
      "learning_rate": 3.501666666666667e-05,
      "loss": 0.0022,
      "step": 44950
    },
    {
      "epoch": 2.397866666666667,
      "grad_norm": 0.08811745792627335,
      "learning_rate": 3.501333333333334e-05,
      "loss": 0.0024,
      "step": 44960
    },
    {
      "epoch": 2.3984,
      "grad_norm": 0.12001196295022964,
      "learning_rate": 3.5010000000000005e-05,
      "loss": 0.0023,
      "step": 44970
    },
    {
      "epoch": 2.3989333333333334,
      "grad_norm": 0.20437012612819672,
      "learning_rate": 3.500666666666667e-05,
      "loss": 0.0023,
      "step": 44980
    },
    {
      "epoch": 2.3994666666666666,
      "grad_norm": 0.3739720284938812,
      "learning_rate": 3.500333333333333e-05,
      "loss": 0.0029,
      "step": 44990
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.29053184390068054,
      "learning_rate": 3.5e-05,
      "loss": 0.0029,
      "step": 45000
    },
    {
      "epoch": 2.400533333333333,
      "grad_norm": 0.2912614941596985,
      "learning_rate": 3.499666666666667e-05,
      "loss": 0.002,
      "step": 45010
    },
    {
      "epoch": 2.401066666666667,
      "grad_norm": 0.20416846871376038,
      "learning_rate": 3.4993333333333336e-05,
      "loss": 0.0031,
      "step": 45020
    },
    {
      "epoch": 2.4016,
      "grad_norm": 0.40178540349006653,
      "learning_rate": 3.499e-05,
      "loss": 0.0027,
      "step": 45030
    },
    {
      "epoch": 2.4021333333333335,
      "grad_norm": 0.4652605354785919,
      "learning_rate": 3.498666666666667e-05,
      "loss": 0.0021,
      "step": 45040
    },
    {
      "epoch": 2.4026666666666667,
      "grad_norm": 0.14536365866661072,
      "learning_rate": 3.4983333333333334e-05,
      "loss": 0.0029,
      "step": 45050
    },
    {
      "epoch": 2.4032,
      "grad_norm": 0.11864814162254333,
      "learning_rate": 3.498e-05,
      "loss": 0.0023,
      "step": 45060
    },
    {
      "epoch": 2.4037333333333333,
      "grad_norm": 0.08704333007335663,
      "learning_rate": 3.497666666666667e-05,
      "loss": 0.0028,
      "step": 45070
    },
    {
      "epoch": 2.4042666666666666,
      "grad_norm": 0.12072716653347015,
      "learning_rate": 3.497333333333333e-05,
      "loss": 0.0023,
      "step": 45080
    },
    {
      "epoch": 2.4048,
      "grad_norm": 0.17242680490016937,
      "learning_rate": 3.4970000000000006e-05,
      "loss": 0.0025,
      "step": 45090
    },
    {
      "epoch": 2.405333333333333,
      "grad_norm": 0.1342216283082962,
      "learning_rate": 3.496666666666667e-05,
      "loss": 0.0034,
      "step": 45100
    },
    {
      "epoch": 2.405866666666667,
      "grad_norm": 0.06227542459964752,
      "learning_rate": 3.496333333333334e-05,
      "loss": 0.0028,
      "step": 45110
    },
    {
      "epoch": 2.4064,
      "grad_norm": 0.20216438174247742,
      "learning_rate": 3.4960000000000004e-05,
      "loss": 0.0037,
      "step": 45120
    },
    {
      "epoch": 2.4069333333333334,
      "grad_norm": 0.17731055617332458,
      "learning_rate": 3.495666666666667e-05,
      "loss": 0.0042,
      "step": 45130
    },
    {
      "epoch": 2.4074666666666666,
      "grad_norm": 0.2321126013994217,
      "learning_rate": 3.495333333333333e-05,
      "loss": 0.0027,
      "step": 45140
    },
    {
      "epoch": 2.408,
      "grad_norm": 0.690055251121521,
      "learning_rate": 3.495e-05,
      "loss": 0.0024,
      "step": 45150
    },
    {
      "epoch": 2.408533333333333,
      "grad_norm": 0.11978361755609512,
      "learning_rate": 3.494666666666667e-05,
      "loss": 0.0023,
      "step": 45160
    },
    {
      "epoch": 2.409066666666667,
      "grad_norm": 0.4295193552970886,
      "learning_rate": 3.4943333333333335e-05,
      "loss": 0.002,
      "step": 45170
    },
    {
      "epoch": 2.4096,
      "grad_norm": 0.29144543409347534,
      "learning_rate": 3.494e-05,
      "loss": 0.0019,
      "step": 45180
    },
    {
      "epoch": 2.4101333333333335,
      "grad_norm": 0.4104739725589752,
      "learning_rate": 3.493666666666667e-05,
      "loss": 0.0031,
      "step": 45190
    },
    {
      "epoch": 2.4106666666666667,
      "grad_norm": 0.28833505511283875,
      "learning_rate": 3.493333333333333e-05,
      "loss": 0.0033,
      "step": 45200
    },
    {
      "epoch": 2.4112,
      "grad_norm": 0.14602598547935486,
      "learning_rate": 3.493e-05,
      "loss": 0.0022,
      "step": 45210
    },
    {
      "epoch": 2.4117333333333333,
      "grad_norm": 0.06210312992334366,
      "learning_rate": 3.4926666666666665e-05,
      "loss": 0.0021,
      "step": 45220
    },
    {
      "epoch": 2.4122666666666666,
      "grad_norm": 0.4599206745624542,
      "learning_rate": 3.492333333333334e-05,
      "loss": 0.002,
      "step": 45230
    },
    {
      "epoch": 2.4128,
      "grad_norm": 0.3714351952075958,
      "learning_rate": 3.4920000000000004e-05,
      "loss": 0.0029,
      "step": 45240
    },
    {
      "epoch": 2.413333333333333,
      "grad_norm": 0.7667582035064697,
      "learning_rate": 3.491666666666667e-05,
      "loss": 0.0043,
      "step": 45250
    },
    {
      "epoch": 2.413866666666667,
      "grad_norm": 0.2734239101409912,
      "learning_rate": 3.491333333333334e-05,
      "loss": 0.0032,
      "step": 45260
    },
    {
      "epoch": 2.4144,
      "grad_norm": 0.1169409528374672,
      "learning_rate": 3.491e-05,
      "loss": 0.0017,
      "step": 45270
    },
    {
      "epoch": 2.4149333333333334,
      "grad_norm": 0.5820493102073669,
      "learning_rate": 3.490666666666667e-05,
      "loss": 0.0024,
      "step": 45280
    },
    {
      "epoch": 2.4154666666666667,
      "grad_norm": 0.31037798523902893,
      "learning_rate": 3.4903333333333335e-05,
      "loss": 0.0029,
      "step": 45290
    },
    {
      "epoch": 2.416,
      "grad_norm": 0.37508928775787354,
      "learning_rate": 3.49e-05,
      "loss": 0.0032,
      "step": 45300
    },
    {
      "epoch": 2.416533333333333,
      "grad_norm": 0.35241642594337463,
      "learning_rate": 3.489666666666667e-05,
      "loss": 0.0025,
      "step": 45310
    },
    {
      "epoch": 2.4170666666666665,
      "grad_norm": 0.32730695605278015,
      "learning_rate": 3.4893333333333334e-05,
      "loss": 0.0028,
      "step": 45320
    },
    {
      "epoch": 2.4176,
      "grad_norm": 0.12428019940853119,
      "learning_rate": 3.489e-05,
      "loss": 0.0025,
      "step": 45330
    },
    {
      "epoch": 2.4181333333333335,
      "grad_norm": 0.2737734913825989,
      "learning_rate": 3.4886666666666666e-05,
      "loss": 0.0023,
      "step": 45340
    },
    {
      "epoch": 2.4186666666666667,
      "grad_norm": 0.5256346464157104,
      "learning_rate": 3.488333333333333e-05,
      "loss": 0.0032,
      "step": 45350
    },
    {
      "epoch": 2.4192,
      "grad_norm": 0.2929285764694214,
      "learning_rate": 3.4880000000000005e-05,
      "loss": 0.0043,
      "step": 45360
    },
    {
      "epoch": 2.4197333333333333,
      "grad_norm": 0.4883202910423279,
      "learning_rate": 3.487666666666667e-05,
      "loss": 0.0017,
      "step": 45370
    },
    {
      "epoch": 2.4202666666666666,
      "grad_norm": 0.29356086254119873,
      "learning_rate": 3.487333333333334e-05,
      "loss": 0.0036,
      "step": 45380
    },
    {
      "epoch": 2.4208,
      "grad_norm": 0.12184510380029678,
      "learning_rate": 3.487e-05,
      "loss": 0.0032,
      "step": 45390
    },
    {
      "epoch": 2.421333333333333,
      "grad_norm": 0.11646627634763718,
      "learning_rate": 3.486666666666667e-05,
      "loss": 0.0026,
      "step": 45400
    },
    {
      "epoch": 2.421866666666667,
      "grad_norm": 0.2887214124202728,
      "learning_rate": 3.4863333333333336e-05,
      "loss": 0.0028,
      "step": 45410
    },
    {
      "epoch": 2.4224,
      "grad_norm": 0.08725807070732117,
      "learning_rate": 3.486e-05,
      "loss": 0.0036,
      "step": 45420
    },
    {
      "epoch": 2.4229333333333334,
      "grad_norm": 0.17342229187488556,
      "learning_rate": 3.485666666666667e-05,
      "loss": 0.0026,
      "step": 45430
    },
    {
      "epoch": 2.4234666666666667,
      "grad_norm": 0.07308228313922882,
      "learning_rate": 3.4853333333333334e-05,
      "loss": 0.0026,
      "step": 45440
    },
    {
      "epoch": 2.424,
      "grad_norm": 0.28631800413131714,
      "learning_rate": 3.485e-05,
      "loss": 0.0027,
      "step": 45450
    },
    {
      "epoch": 2.424533333333333,
      "grad_norm": 0.14894473552703857,
      "learning_rate": 3.4846666666666666e-05,
      "loss": 0.0024,
      "step": 45460
    },
    {
      "epoch": 2.4250666666666665,
      "grad_norm": 0.20594549179077148,
      "learning_rate": 3.484333333333333e-05,
      "loss": 0.0029,
      "step": 45470
    },
    {
      "epoch": 2.4256,
      "grad_norm": 0.31684818863868713,
      "learning_rate": 3.484e-05,
      "loss": 0.0026,
      "step": 45480
    },
    {
      "epoch": 2.4261333333333335,
      "grad_norm": 0.5215088725090027,
      "learning_rate": 3.4836666666666665e-05,
      "loss": 0.0021,
      "step": 45490
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 0.26388105750083923,
      "learning_rate": 3.483333333333334e-05,
      "loss": 0.0028,
      "step": 45500
    },
    {
      "epoch": 2.4272,
      "grad_norm": 0.14539998769760132,
      "learning_rate": 3.4830000000000004e-05,
      "loss": 0.003,
      "step": 45510
    },
    {
      "epoch": 2.4277333333333333,
      "grad_norm": 0.20431415736675262,
      "learning_rate": 3.482666666666667e-05,
      "loss": 0.002,
      "step": 45520
    },
    {
      "epoch": 2.4282666666666666,
      "grad_norm": 0.14842545986175537,
      "learning_rate": 3.4823333333333336e-05,
      "loss": 0.0021,
      "step": 45530
    },
    {
      "epoch": 2.4288,
      "grad_norm": 0.12176687270402908,
      "learning_rate": 3.482e-05,
      "loss": 0.0024,
      "step": 45540
    },
    {
      "epoch": 2.429333333333333,
      "grad_norm": 0.14831051230430603,
      "learning_rate": 3.481666666666667e-05,
      "loss": 0.0021,
      "step": 45550
    },
    {
      "epoch": 2.429866666666667,
      "grad_norm": 0.40313011407852173,
      "learning_rate": 3.4813333333333334e-05,
      "loss": 0.0019,
      "step": 45560
    },
    {
      "epoch": 2.4304,
      "grad_norm": 0.02704634889960289,
      "learning_rate": 3.481e-05,
      "loss": 0.0026,
      "step": 45570
    },
    {
      "epoch": 2.4309333333333334,
      "grad_norm": 0.0620899423956871,
      "learning_rate": 3.480666666666667e-05,
      "loss": 0.0026,
      "step": 45580
    },
    {
      "epoch": 2.4314666666666667,
      "grad_norm": 0.11460868269205093,
      "learning_rate": 3.480333333333333e-05,
      "loss": 0.0027,
      "step": 45590
    },
    {
      "epoch": 2.432,
      "grad_norm": 0.4603889286518097,
      "learning_rate": 3.48e-05,
      "loss": 0.0032,
      "step": 45600
    },
    {
      "epoch": 2.432533333333333,
      "grad_norm": 0.038465142250061035,
      "learning_rate": 3.4796666666666665e-05,
      "loss": 0.0023,
      "step": 45610
    },
    {
      "epoch": 2.4330666666666665,
      "grad_norm": 0.25958916544914246,
      "learning_rate": 3.479333333333333e-05,
      "loss": 0.0024,
      "step": 45620
    },
    {
      "epoch": 2.4336,
      "grad_norm": 0.35693666338920593,
      "learning_rate": 3.479e-05,
      "loss": 0.0027,
      "step": 45630
    },
    {
      "epoch": 2.4341333333333335,
      "grad_norm": 0.20233480632305145,
      "learning_rate": 3.478666666666667e-05,
      "loss": 0.0024,
      "step": 45640
    },
    {
      "epoch": 2.4346666666666668,
      "grad_norm": 0.11712300032377243,
      "learning_rate": 3.4783333333333336e-05,
      "loss": 0.0028,
      "step": 45650
    },
    {
      "epoch": 2.4352,
      "grad_norm": 0.17495013773441315,
      "learning_rate": 3.478e-05,
      "loss": 0.0016,
      "step": 45660
    },
    {
      "epoch": 2.4357333333333333,
      "grad_norm": 0.06170595437288284,
      "learning_rate": 3.477666666666667e-05,
      "loss": 0.0024,
      "step": 45670
    },
    {
      "epoch": 2.4362666666666666,
      "grad_norm": 0.5460517406463623,
      "learning_rate": 3.4773333333333335e-05,
      "loss": 0.0034,
      "step": 45680
    },
    {
      "epoch": 2.4368,
      "grad_norm": 0.058191221207380295,
      "learning_rate": 3.477e-05,
      "loss": 0.0028,
      "step": 45690
    },
    {
      "epoch": 2.437333333333333,
      "grad_norm": 0.2272254079580307,
      "learning_rate": 3.476666666666667e-05,
      "loss": 0.0019,
      "step": 45700
    },
    {
      "epoch": 2.437866666666667,
      "grad_norm": 0.06475186347961426,
      "learning_rate": 3.476333333333334e-05,
      "loss": 0.0021,
      "step": 45710
    },
    {
      "epoch": 2.4384,
      "grad_norm": 0.0638299286365509,
      "learning_rate": 3.4760000000000006e-05,
      "loss": 0.0022,
      "step": 45720
    },
    {
      "epoch": 2.4389333333333334,
      "grad_norm": 0.06243373081088066,
      "learning_rate": 3.475666666666667e-05,
      "loss": 0.0031,
      "step": 45730
    },
    {
      "epoch": 2.4394666666666667,
      "grad_norm": 0.26258668303489685,
      "learning_rate": 3.475333333333334e-05,
      "loss": 0.0041,
      "step": 45740
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.12761256098747253,
      "learning_rate": 3.475e-05,
      "loss": 0.0026,
      "step": 45750
    },
    {
      "epoch": 2.440533333333333,
      "grad_norm": 0.033996064215898514,
      "learning_rate": 3.4746666666666664e-05,
      "loss": 0.0025,
      "step": 45760
    },
    {
      "epoch": 2.4410666666666665,
      "grad_norm": 0.06290590018033981,
      "learning_rate": 3.474333333333334e-05,
      "loss": 0.0031,
      "step": 45770
    },
    {
      "epoch": 2.4416,
      "grad_norm": 0.1509481519460678,
      "learning_rate": 3.474e-05,
      "loss": 0.002,
      "step": 45780
    },
    {
      "epoch": 2.4421333333333335,
      "grad_norm": 0.40540871024131775,
      "learning_rate": 3.473666666666667e-05,
      "loss": 0.0028,
      "step": 45790
    },
    {
      "epoch": 2.4426666666666668,
      "grad_norm": 0.1749403029680252,
      "learning_rate": 3.4733333333333335e-05,
      "loss": 0.0029,
      "step": 45800
    },
    {
      "epoch": 2.4432,
      "grad_norm": 0.03518728166818619,
      "learning_rate": 3.473e-05,
      "loss": 0.0022,
      "step": 45810
    },
    {
      "epoch": 2.4437333333333333,
      "grad_norm": 0.01604766584932804,
      "learning_rate": 3.472666666666667e-05,
      "loss": 0.003,
      "step": 45820
    },
    {
      "epoch": 2.4442666666666666,
      "grad_norm": 0.4070074260234833,
      "learning_rate": 3.4723333333333333e-05,
      "loss": 0.0034,
      "step": 45830
    },
    {
      "epoch": 2.4448,
      "grad_norm": 0.17206773161888123,
      "learning_rate": 3.472e-05,
      "loss": 0.0026,
      "step": 45840
    },
    {
      "epoch": 2.445333333333333,
      "grad_norm": 0.5185043215751648,
      "learning_rate": 3.471666666666667e-05,
      "loss": 0.0023,
      "step": 45850
    },
    {
      "epoch": 2.445866666666667,
      "grad_norm": 0.3520157039165497,
      "learning_rate": 3.471333333333334e-05,
      "loss": 0.0031,
      "step": 45860
    },
    {
      "epoch": 2.4464,
      "grad_norm": 0.24554364383220673,
      "learning_rate": 3.4710000000000005e-05,
      "loss": 0.0021,
      "step": 45870
    },
    {
      "epoch": 2.4469333333333334,
      "grad_norm": 0.4951857626438141,
      "learning_rate": 3.470666666666667e-05,
      "loss": 0.0029,
      "step": 45880
    },
    {
      "epoch": 2.4474666666666667,
      "grad_norm": 0.09186949580907822,
      "learning_rate": 3.470333333333334e-05,
      "loss": 0.0023,
      "step": 45890
    },
    {
      "epoch": 2.448,
      "grad_norm": 0.45781904458999634,
      "learning_rate": 3.4699999999999996e-05,
      "loss": 0.0025,
      "step": 45900
    },
    {
      "epoch": 2.4485333333333332,
      "grad_norm": 0.06965067237615585,
      "learning_rate": 3.469666666666667e-05,
      "loss": 0.0029,
      "step": 45910
    },
    {
      "epoch": 2.4490666666666665,
      "grad_norm": 0.2708204686641693,
      "learning_rate": 3.4693333333333335e-05,
      "loss": 0.0025,
      "step": 45920
    },
    {
      "epoch": 2.4496,
      "grad_norm": 0.08642281591892242,
      "learning_rate": 3.469e-05,
      "loss": 0.0028,
      "step": 45930
    },
    {
      "epoch": 2.4501333333333335,
      "grad_norm": 0.14574208855628967,
      "learning_rate": 3.468666666666667e-05,
      "loss": 0.0032,
      "step": 45940
    },
    {
      "epoch": 2.4506666666666668,
      "grad_norm": 0.319235235452652,
      "learning_rate": 3.4683333333333334e-05,
      "loss": 0.0019,
      "step": 45950
    },
    {
      "epoch": 2.4512,
      "grad_norm": 0.01993037573993206,
      "learning_rate": 3.468e-05,
      "loss": 0.0026,
      "step": 45960
    },
    {
      "epoch": 2.4517333333333333,
      "grad_norm": 0.6050875186920166,
      "learning_rate": 3.4676666666666666e-05,
      "loss": 0.0023,
      "step": 45970
    },
    {
      "epoch": 2.4522666666666666,
      "grad_norm": 0.4247182011604309,
      "learning_rate": 3.467333333333333e-05,
      "loss": 0.003,
      "step": 45980
    },
    {
      "epoch": 2.4528,
      "grad_norm": 0.2018597275018692,
      "learning_rate": 3.4670000000000005e-05,
      "loss": 0.0043,
      "step": 45990
    },
    {
      "epoch": 2.453333333333333,
      "grad_norm": 0.09528765827417374,
      "learning_rate": 3.466666666666667e-05,
      "loss": 0.0023,
      "step": 46000
    },
    {
      "epoch": 2.4538666666666664,
      "grad_norm": 0.3464195132255554,
      "learning_rate": 3.466333333333334e-05,
      "loss": 0.003,
      "step": 46010
    },
    {
      "epoch": 2.4544,
      "grad_norm": 0.4999893605709076,
      "learning_rate": 3.4660000000000004e-05,
      "loss": 0.0021,
      "step": 46020
    },
    {
      "epoch": 2.4549333333333334,
      "grad_norm": 0.5222993493080139,
      "learning_rate": 3.465666666666667e-05,
      "loss": 0.0026,
      "step": 46030
    },
    {
      "epoch": 2.4554666666666667,
      "grad_norm": 0.08824332058429718,
      "learning_rate": 3.4653333333333336e-05,
      "loss": 0.0021,
      "step": 46040
    },
    {
      "epoch": 2.456,
      "grad_norm": 0.5544834733009338,
      "learning_rate": 3.465e-05,
      "loss": 0.0022,
      "step": 46050
    },
    {
      "epoch": 2.4565333333333332,
      "grad_norm": 0.12575295567512512,
      "learning_rate": 3.464666666666667e-05,
      "loss": 0.003,
      "step": 46060
    },
    {
      "epoch": 2.4570666666666665,
      "grad_norm": 0.23522335290908813,
      "learning_rate": 3.4643333333333334e-05,
      "loss": 0.003,
      "step": 46070
    },
    {
      "epoch": 2.4576000000000002,
      "grad_norm": 0.5261939764022827,
      "learning_rate": 3.464e-05,
      "loss": 0.0026,
      "step": 46080
    },
    {
      "epoch": 2.4581333333333335,
      "grad_norm": 0.46065735816955566,
      "learning_rate": 3.4636666666666667e-05,
      "loss": 0.0023,
      "step": 46090
    },
    {
      "epoch": 2.458666666666667,
      "grad_norm": 0.29583582282066345,
      "learning_rate": 3.463333333333333e-05,
      "loss": 0.0024,
      "step": 46100
    },
    {
      "epoch": 2.4592,
      "grad_norm": 0.178313210606575,
      "learning_rate": 3.463e-05,
      "loss": 0.0032,
      "step": 46110
    },
    {
      "epoch": 2.4597333333333333,
      "grad_norm": 0.2708284556865692,
      "learning_rate": 3.462666666666667e-05,
      "loss": 0.0031,
      "step": 46120
    },
    {
      "epoch": 2.4602666666666666,
      "grad_norm": 0.4468599855899811,
      "learning_rate": 3.462333333333334e-05,
      "loss": 0.0028,
      "step": 46130
    },
    {
      "epoch": 2.4608,
      "grad_norm": 0.2607899308204651,
      "learning_rate": 3.4620000000000004e-05,
      "loss": 0.0055,
      "step": 46140
    },
    {
      "epoch": 2.461333333333333,
      "grad_norm": 0.33360034227371216,
      "learning_rate": 3.461666666666667e-05,
      "loss": 0.0022,
      "step": 46150
    },
    {
      "epoch": 2.4618666666666664,
      "grad_norm": 0.01951424404978752,
      "learning_rate": 3.4613333333333336e-05,
      "loss": 0.0026,
      "step": 46160
    },
    {
      "epoch": 2.4624,
      "grad_norm": 0.3520679473876953,
      "learning_rate": 3.461e-05,
      "loss": 0.0021,
      "step": 46170
    },
    {
      "epoch": 2.4629333333333334,
      "grad_norm": 0.17633120715618134,
      "learning_rate": 3.460666666666667e-05,
      "loss": 0.0028,
      "step": 46180
    },
    {
      "epoch": 2.4634666666666667,
      "grad_norm": 0.3173459470272064,
      "learning_rate": 3.4603333333333335e-05,
      "loss": 0.0032,
      "step": 46190
    },
    {
      "epoch": 2.464,
      "grad_norm": 0.2230457067489624,
      "learning_rate": 3.46e-05,
      "loss": 0.0039,
      "step": 46200
    },
    {
      "epoch": 2.4645333333333332,
      "grad_norm": 0.3782891035079956,
      "learning_rate": 3.459666666666667e-05,
      "loss": 0.0033,
      "step": 46210
    },
    {
      "epoch": 2.4650666666666665,
      "grad_norm": 0.20179101824760437,
      "learning_rate": 3.459333333333333e-05,
      "loss": 0.0023,
      "step": 46220
    },
    {
      "epoch": 2.4656000000000002,
      "grad_norm": 0.14533044397830963,
      "learning_rate": 3.459e-05,
      "loss": 0.002,
      "step": 46230
    },
    {
      "epoch": 2.4661333333333335,
      "grad_norm": 0.09352076798677444,
      "learning_rate": 3.4586666666666665e-05,
      "loss": 0.0021,
      "step": 46240
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 0.17974953353405,
      "learning_rate": 3.458333333333333e-05,
      "loss": 0.002,
      "step": 46250
    },
    {
      "epoch": 2.4672,
      "grad_norm": 0.035508010536432266,
      "learning_rate": 3.4580000000000004e-05,
      "loss": 0.0019,
      "step": 46260
    },
    {
      "epoch": 2.4677333333333333,
      "grad_norm": 0.2737555205821991,
      "learning_rate": 3.457666666666667e-05,
      "loss": 0.0023,
      "step": 46270
    },
    {
      "epoch": 2.4682666666666666,
      "grad_norm": 0.11627253144979477,
      "learning_rate": 3.4573333333333337e-05,
      "loss": 0.0023,
      "step": 46280
    },
    {
      "epoch": 2.4688,
      "grad_norm": 0.060380902141332626,
      "learning_rate": 3.457e-05,
      "loss": 0.0031,
      "step": 46290
    },
    {
      "epoch": 2.469333333333333,
      "grad_norm": 0.4029397964477539,
      "learning_rate": 3.456666666666667e-05,
      "loss": 0.0027,
      "step": 46300
    },
    {
      "epoch": 2.4698666666666664,
      "grad_norm": 0.2572544515132904,
      "learning_rate": 3.4563333333333335e-05,
      "loss": 0.0014,
      "step": 46310
    },
    {
      "epoch": 2.4704,
      "grad_norm": 0.4305693805217743,
      "learning_rate": 3.456e-05,
      "loss": 0.0024,
      "step": 46320
    },
    {
      "epoch": 2.4709333333333334,
      "grad_norm": 0.03904782608151436,
      "learning_rate": 3.455666666666667e-05,
      "loss": 0.0023,
      "step": 46330
    },
    {
      "epoch": 2.4714666666666667,
      "grad_norm": 0.06041131913661957,
      "learning_rate": 3.455333333333334e-05,
      "loss": 0.0019,
      "step": 46340
    },
    {
      "epoch": 2.472,
      "grad_norm": 0.06584607809782028,
      "learning_rate": 3.455e-05,
      "loss": 0.0031,
      "step": 46350
    },
    {
      "epoch": 2.4725333333333332,
      "grad_norm": 0.2028585523366928,
      "learning_rate": 3.4546666666666666e-05,
      "loss": 0.0022,
      "step": 46360
    },
    {
      "epoch": 2.4730666666666665,
      "grad_norm": 0.11558555066585541,
      "learning_rate": 3.454333333333333e-05,
      "loss": 0.0016,
      "step": 46370
    },
    {
      "epoch": 2.4736000000000002,
      "grad_norm": 0.1465606689453125,
      "learning_rate": 3.454e-05,
      "loss": 0.0021,
      "step": 46380
    },
    {
      "epoch": 2.4741333333333335,
      "grad_norm": 0.1202058270573616,
      "learning_rate": 3.4536666666666664e-05,
      "loss": 0.0017,
      "step": 46390
    },
    {
      "epoch": 2.474666666666667,
      "grad_norm": 0.09951402992010117,
      "learning_rate": 3.453333333333334e-05,
      "loss": 0.0024,
      "step": 46400
    },
    {
      "epoch": 2.4752,
      "grad_norm": 0.04099119454622269,
      "learning_rate": 3.453e-05,
      "loss": 0.0023,
      "step": 46410
    },
    {
      "epoch": 2.4757333333333333,
      "grad_norm": 0.25948792695999146,
      "learning_rate": 3.452666666666667e-05,
      "loss": 0.0024,
      "step": 46420
    },
    {
      "epoch": 2.4762666666666666,
      "grad_norm": 0.7180918455123901,
      "learning_rate": 3.4523333333333335e-05,
      "loss": 0.0021,
      "step": 46430
    },
    {
      "epoch": 2.4768,
      "grad_norm": 0.12053262442350388,
      "learning_rate": 3.452e-05,
      "loss": 0.0025,
      "step": 46440
    },
    {
      "epoch": 2.477333333333333,
      "grad_norm": 0.11900728195905685,
      "learning_rate": 3.451666666666667e-05,
      "loss": 0.0036,
      "step": 46450
    },
    {
      "epoch": 2.4778666666666664,
      "grad_norm": 0.2903793454170227,
      "learning_rate": 3.4513333333333334e-05,
      "loss": 0.0025,
      "step": 46460
    },
    {
      "epoch": 2.4784,
      "grad_norm": 0.3226167559623718,
      "learning_rate": 3.451000000000001e-05,
      "loss": 0.0031,
      "step": 46470
    },
    {
      "epoch": 2.4789333333333334,
      "grad_norm": 0.2617015838623047,
      "learning_rate": 3.450666666666667e-05,
      "loss": 0.0031,
      "step": 46480
    },
    {
      "epoch": 2.4794666666666667,
      "grad_norm": 0.061123140156269073,
      "learning_rate": 3.450333333333334e-05,
      "loss": 0.0025,
      "step": 46490
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.07490037381649017,
      "learning_rate": 3.45e-05,
      "loss": 0.0027,
      "step": 46500
    },
    {
      "epoch": 2.4805333333333333,
      "grad_norm": 0.04157504066824913,
      "learning_rate": 3.4496666666666664e-05,
      "loss": 0.0013,
      "step": 46510
    },
    {
      "epoch": 2.4810666666666665,
      "grad_norm": 0.23715488612651825,
      "learning_rate": 3.449333333333333e-05,
      "loss": 0.0023,
      "step": 46520
    },
    {
      "epoch": 2.4816,
      "grad_norm": 0.03602161630988121,
      "learning_rate": 3.449e-05,
      "loss": 0.0024,
      "step": 46530
    },
    {
      "epoch": 2.4821333333333335,
      "grad_norm": 0.08874056488275528,
      "learning_rate": 3.448666666666667e-05,
      "loss": 0.0018,
      "step": 46540
    },
    {
      "epoch": 2.482666666666667,
      "grad_norm": 0.11661034822463989,
      "learning_rate": 3.4483333333333336e-05,
      "loss": 0.0028,
      "step": 46550
    },
    {
      "epoch": 2.4832,
      "grad_norm": 0.4370293617248535,
      "learning_rate": 3.448e-05,
      "loss": 0.0022,
      "step": 46560
    },
    {
      "epoch": 2.4837333333333333,
      "grad_norm": 0.31630992889404297,
      "learning_rate": 3.447666666666667e-05,
      "loss": 0.0036,
      "step": 46570
    },
    {
      "epoch": 2.4842666666666666,
      "grad_norm": 0.10129063576459885,
      "learning_rate": 3.4473333333333334e-05,
      "loss": 0.0021,
      "step": 46580
    },
    {
      "epoch": 2.4848,
      "grad_norm": 0.17447757720947266,
      "learning_rate": 3.447e-05,
      "loss": 0.0032,
      "step": 46590
    },
    {
      "epoch": 2.485333333333333,
      "grad_norm": 0.03961949050426483,
      "learning_rate": 3.4466666666666666e-05,
      "loss": 0.0032,
      "step": 46600
    },
    {
      "epoch": 2.4858666666666664,
      "grad_norm": 0.1190929189324379,
      "learning_rate": 3.446333333333334e-05,
      "loss": 0.0024,
      "step": 46610
    },
    {
      "epoch": 2.4864,
      "grad_norm": 0.03871811553835869,
      "learning_rate": 3.4460000000000005e-05,
      "loss": 0.0025,
      "step": 46620
    },
    {
      "epoch": 2.4869333333333334,
      "grad_norm": 0.14740633964538574,
      "learning_rate": 3.445666666666667e-05,
      "loss": 0.0039,
      "step": 46630
    },
    {
      "epoch": 2.4874666666666667,
      "grad_norm": 0.021614642813801765,
      "learning_rate": 3.445333333333334e-05,
      "loss": 0.0029,
      "step": 46640
    },
    {
      "epoch": 2.488,
      "grad_norm": 0.0710734874010086,
      "learning_rate": 3.445e-05,
      "loss": 0.0025,
      "step": 46650
    },
    {
      "epoch": 2.4885333333333333,
      "grad_norm": 0.5775119066238403,
      "learning_rate": 3.444666666666666e-05,
      "loss": 0.0025,
      "step": 46660
    },
    {
      "epoch": 2.4890666666666665,
      "grad_norm": 0.06442740559577942,
      "learning_rate": 3.4443333333333336e-05,
      "loss": 0.002,
      "step": 46670
    },
    {
      "epoch": 2.4896,
      "grad_norm": 0.2871752083301544,
      "learning_rate": 3.444e-05,
      "loss": 0.0035,
      "step": 46680
    },
    {
      "epoch": 2.4901333333333335,
      "grad_norm": 0.42731425166130066,
      "learning_rate": 3.443666666666667e-05,
      "loss": 0.0023,
      "step": 46690
    },
    {
      "epoch": 2.490666666666667,
      "grad_norm": 0.06787292659282684,
      "learning_rate": 3.4433333333333335e-05,
      "loss": 0.0025,
      "step": 46700
    },
    {
      "epoch": 2.4912,
      "grad_norm": 0.3155862092971802,
      "learning_rate": 3.443e-05,
      "loss": 0.0026,
      "step": 46710
    },
    {
      "epoch": 2.4917333333333334,
      "grad_norm": 0.314786434173584,
      "learning_rate": 3.442666666666667e-05,
      "loss": 0.0034,
      "step": 46720
    },
    {
      "epoch": 2.4922666666666666,
      "grad_norm": 0.5741010308265686,
      "learning_rate": 3.442333333333333e-05,
      "loss": 0.0018,
      "step": 46730
    },
    {
      "epoch": 2.4928,
      "grad_norm": 0.3180951476097107,
      "learning_rate": 3.442e-05,
      "loss": 0.0022,
      "step": 46740
    },
    {
      "epoch": 2.493333333333333,
      "grad_norm": 0.4178397059440613,
      "learning_rate": 3.441666666666667e-05,
      "loss": 0.0035,
      "step": 46750
    },
    {
      "epoch": 2.4938666666666665,
      "grad_norm": 0.2912414073944092,
      "learning_rate": 3.441333333333334e-05,
      "loss": 0.0031,
      "step": 46760
    },
    {
      "epoch": 2.4944,
      "grad_norm": 0.05199488624930382,
      "learning_rate": 3.4410000000000004e-05,
      "loss": 0.0022,
      "step": 46770
    },
    {
      "epoch": 2.4949333333333334,
      "grad_norm": 0.1710655391216278,
      "learning_rate": 3.440666666666667e-05,
      "loss": 0.0031,
      "step": 46780
    },
    {
      "epoch": 2.4954666666666667,
      "grad_norm": 0.2320019155740738,
      "learning_rate": 3.4403333333333337e-05,
      "loss": 0.0028,
      "step": 46790
    },
    {
      "epoch": 2.496,
      "grad_norm": 0.3787505030632019,
      "learning_rate": 3.4399999999999996e-05,
      "loss": 0.0027,
      "step": 46800
    },
    {
      "epoch": 2.4965333333333333,
      "grad_norm": 0.5499335527420044,
      "learning_rate": 3.439666666666667e-05,
      "loss": 0.0036,
      "step": 46810
    },
    {
      "epoch": 2.4970666666666665,
      "grad_norm": 0.03732510656118393,
      "learning_rate": 3.4393333333333335e-05,
      "loss": 0.0028,
      "step": 46820
    },
    {
      "epoch": 2.4976,
      "grad_norm": 0.20132112503051758,
      "learning_rate": 3.439e-05,
      "loss": 0.0036,
      "step": 46830
    },
    {
      "epoch": 2.4981333333333335,
      "grad_norm": 0.1746165156364441,
      "learning_rate": 3.438666666666667e-05,
      "loss": 0.003,
      "step": 46840
    },
    {
      "epoch": 2.498666666666667,
      "grad_norm": 0.1470615565776825,
      "learning_rate": 3.438333333333333e-05,
      "loss": 0.0031,
      "step": 46850
    },
    {
      "epoch": 2.4992,
      "grad_norm": 0.37685471773147583,
      "learning_rate": 3.438e-05,
      "loss": 0.0039,
      "step": 46860
    },
    {
      "epoch": 2.4997333333333334,
      "grad_norm": 0.04264705255627632,
      "learning_rate": 3.4376666666666666e-05,
      "loss": 0.0025,
      "step": 46870
    },
    {
      "epoch": 2.5002666666666666,
      "grad_norm": 0.4043191075325012,
      "learning_rate": 3.437333333333334e-05,
      "loss": 0.0028,
      "step": 46880
    },
    {
      "epoch": 2.5008,
      "grad_norm": 0.060806311666965485,
      "learning_rate": 3.4370000000000005e-05,
      "loss": 0.0022,
      "step": 46890
    },
    {
      "epoch": 2.501333333333333,
      "grad_norm": 0.23426489531993866,
      "learning_rate": 3.436666666666667e-05,
      "loss": 0.0029,
      "step": 46900
    },
    {
      "epoch": 2.5018666666666665,
      "grad_norm": 0.04558775573968887,
      "learning_rate": 3.436333333333334e-05,
      "loss": 0.0026,
      "step": 46910
    },
    {
      "epoch": 2.5023999999999997,
      "grad_norm": 0.3161371946334839,
      "learning_rate": 3.436e-05,
      "loss": 0.0023,
      "step": 46920
    },
    {
      "epoch": 2.5029333333333335,
      "grad_norm": 0.43295931816101074,
      "learning_rate": 3.435666666666667e-05,
      "loss": 0.002,
      "step": 46930
    },
    {
      "epoch": 2.5034666666666667,
      "grad_norm": 0.5762192010879517,
      "learning_rate": 3.4353333333333335e-05,
      "loss": 0.0029,
      "step": 46940
    },
    {
      "epoch": 2.504,
      "grad_norm": 0.06082795187830925,
      "learning_rate": 3.435e-05,
      "loss": 0.0027,
      "step": 46950
    },
    {
      "epoch": 2.5045333333333333,
      "grad_norm": 0.270385205745697,
      "learning_rate": 3.434666666666667e-05,
      "loss": 0.0035,
      "step": 46960
    },
    {
      "epoch": 2.5050666666666666,
      "grad_norm": 0.12155358493328094,
      "learning_rate": 3.4343333333333334e-05,
      "loss": 0.0029,
      "step": 46970
    },
    {
      "epoch": 2.5056000000000003,
      "grad_norm": 0.4397387206554413,
      "learning_rate": 3.434e-05,
      "loss": 0.0029,
      "step": 46980
    },
    {
      "epoch": 2.5061333333333335,
      "grad_norm": 0.378371924161911,
      "learning_rate": 3.4336666666666666e-05,
      "loss": 0.0038,
      "step": 46990
    },
    {
      "epoch": 2.506666666666667,
      "grad_norm": 0.03353757783770561,
      "learning_rate": 3.433333333333333e-05,
      "loss": 0.0026,
      "step": 47000
    },
    {
      "epoch": 2.5072,
      "grad_norm": 0.19423462450504303,
      "learning_rate": 3.433e-05,
      "loss": 0.0034,
      "step": 47010
    },
    {
      "epoch": 2.5077333333333334,
      "grad_norm": 0.2074325680732727,
      "learning_rate": 3.432666666666667e-05,
      "loss": 0.0045,
      "step": 47020
    },
    {
      "epoch": 2.5082666666666666,
      "grad_norm": 0.11753968149423599,
      "learning_rate": 3.432333333333334e-05,
      "loss": 0.0039,
      "step": 47030
    },
    {
      "epoch": 2.5088,
      "grad_norm": 0.17278581857681274,
      "learning_rate": 3.4320000000000003e-05,
      "loss": 0.0024,
      "step": 47040
    },
    {
      "epoch": 2.509333333333333,
      "grad_norm": 0.11986812949180603,
      "learning_rate": 3.431666666666667e-05,
      "loss": 0.0024,
      "step": 47050
    },
    {
      "epoch": 2.5098666666666665,
      "grad_norm": 0.46941274404525757,
      "learning_rate": 3.4313333333333336e-05,
      "loss": 0.0023,
      "step": 47060
    },
    {
      "epoch": 2.5103999999999997,
      "grad_norm": 0.2876283824443817,
      "learning_rate": 3.431e-05,
      "loss": 0.0024,
      "step": 47070
    },
    {
      "epoch": 2.5109333333333335,
      "grad_norm": 0.061625588685274124,
      "learning_rate": 3.430666666666667e-05,
      "loss": 0.005,
      "step": 47080
    },
    {
      "epoch": 2.5114666666666667,
      "grad_norm": 0.11685441434383392,
      "learning_rate": 3.4303333333333334e-05,
      "loss": 0.0031,
      "step": 47090
    },
    {
      "epoch": 2.512,
      "grad_norm": 0.17539125680923462,
      "learning_rate": 3.430000000000001e-05,
      "loss": 0.0025,
      "step": 47100
    },
    {
      "epoch": 2.5125333333333333,
      "grad_norm": 0.5712732076644897,
      "learning_rate": 3.4296666666666666e-05,
      "loss": 0.0022,
      "step": 47110
    },
    {
      "epoch": 2.5130666666666666,
      "grad_norm": 0.542727530002594,
      "learning_rate": 3.429333333333333e-05,
      "loss": 0.0029,
      "step": 47120
    },
    {
      "epoch": 2.5136,
      "grad_norm": 0.20561622083187103,
      "learning_rate": 3.429e-05,
      "loss": 0.0044,
      "step": 47130
    },
    {
      "epoch": 2.5141333333333336,
      "grad_norm": 0.5193349123001099,
      "learning_rate": 3.4286666666666665e-05,
      "loss": 0.0022,
      "step": 47140
    },
    {
      "epoch": 2.514666666666667,
      "grad_norm": 0.08833811432123184,
      "learning_rate": 3.428333333333333e-05,
      "loss": 0.0021,
      "step": 47150
    },
    {
      "epoch": 2.5152,
      "grad_norm": 0.09034276753664017,
      "learning_rate": 3.4280000000000004e-05,
      "loss": 0.0028,
      "step": 47160
    },
    {
      "epoch": 2.5157333333333334,
      "grad_norm": 0.5835130214691162,
      "learning_rate": 3.427666666666667e-05,
      "loss": 0.0038,
      "step": 47170
    },
    {
      "epoch": 2.5162666666666667,
      "grad_norm": 0.20119024813175201,
      "learning_rate": 3.4273333333333336e-05,
      "loss": 0.0016,
      "step": 47180
    },
    {
      "epoch": 2.5168,
      "grad_norm": 0.08873853832483292,
      "learning_rate": 3.427e-05,
      "loss": 0.0035,
      "step": 47190
    },
    {
      "epoch": 2.517333333333333,
      "grad_norm": 0.2927713394165039,
      "learning_rate": 3.426666666666667e-05,
      "loss": 0.003,
      "step": 47200
    },
    {
      "epoch": 2.5178666666666665,
      "grad_norm": 0.3218088746070862,
      "learning_rate": 3.4263333333333334e-05,
      "loss": 0.0022,
      "step": 47210
    },
    {
      "epoch": 2.5183999999999997,
      "grad_norm": 0.32064133882522583,
      "learning_rate": 3.426e-05,
      "loss": 0.0029,
      "step": 47220
    },
    {
      "epoch": 2.5189333333333335,
      "grad_norm": 0.06652182340621948,
      "learning_rate": 3.4256666666666674e-05,
      "loss": 0.0021,
      "step": 47230
    },
    {
      "epoch": 2.5194666666666667,
      "grad_norm": 0.2946714758872986,
      "learning_rate": 3.425333333333334e-05,
      "loss": 0.004,
      "step": 47240
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.12013918161392212,
      "learning_rate": 3.4250000000000006e-05,
      "loss": 0.0034,
      "step": 47250
    },
    {
      "epoch": 2.5205333333333333,
      "grad_norm": 0.43460825085639954,
      "learning_rate": 3.4246666666666665e-05,
      "loss": 0.0048,
      "step": 47260
    },
    {
      "epoch": 2.5210666666666666,
      "grad_norm": 0.20151527225971222,
      "learning_rate": 3.424333333333333e-05,
      "loss": 0.0023,
      "step": 47270
    },
    {
      "epoch": 2.5216,
      "grad_norm": 0.2356797605752945,
      "learning_rate": 3.424e-05,
      "loss": 0.0039,
      "step": 47280
    },
    {
      "epoch": 2.5221333333333336,
      "grad_norm": 0.10217595845460892,
      "learning_rate": 3.4236666666666664e-05,
      "loss": 0.0027,
      "step": 47290
    },
    {
      "epoch": 2.522666666666667,
      "grad_norm": 0.11616306006908417,
      "learning_rate": 3.4233333333333336e-05,
      "loss": 0.0023,
      "step": 47300
    },
    {
      "epoch": 2.5232,
      "grad_norm": 0.20256133377552032,
      "learning_rate": 3.423e-05,
      "loss": 0.0026,
      "step": 47310
    },
    {
      "epoch": 2.5237333333333334,
      "grad_norm": 0.03671079874038696,
      "learning_rate": 3.422666666666667e-05,
      "loss": 0.003,
      "step": 47320
    },
    {
      "epoch": 2.5242666666666667,
      "grad_norm": 0.2025870829820633,
      "learning_rate": 3.4223333333333335e-05,
      "loss": 0.0023,
      "step": 47330
    },
    {
      "epoch": 2.5248,
      "grad_norm": 0.2611103355884552,
      "learning_rate": 3.422e-05,
      "loss": 0.0022,
      "step": 47340
    },
    {
      "epoch": 2.525333333333333,
      "grad_norm": 0.31858617067337036,
      "learning_rate": 3.421666666666667e-05,
      "loss": 0.0028,
      "step": 47350
    },
    {
      "epoch": 2.5258666666666665,
      "grad_norm": 0.02844851091504097,
      "learning_rate": 3.421333333333333e-05,
      "loss": 0.0025,
      "step": 47360
    },
    {
      "epoch": 2.5263999999999998,
      "grad_norm": 0.12627458572387695,
      "learning_rate": 3.4210000000000006e-05,
      "loss": 0.0022,
      "step": 47370
    },
    {
      "epoch": 2.5269333333333335,
      "grad_norm": 0.03932484984397888,
      "learning_rate": 3.420666666666667e-05,
      "loss": 0.0024,
      "step": 47380
    },
    {
      "epoch": 2.5274666666666668,
      "grad_norm": 0.20253585278987885,
      "learning_rate": 3.420333333333334e-05,
      "loss": 0.0035,
      "step": 47390
    },
    {
      "epoch": 2.528,
      "grad_norm": 0.08687882125377655,
      "learning_rate": 3.4200000000000005e-05,
      "loss": 0.003,
      "step": 47400
    },
    {
      "epoch": 2.5285333333333333,
      "grad_norm": 0.209371879696846,
      "learning_rate": 3.4196666666666664e-05,
      "loss": 0.0033,
      "step": 47410
    },
    {
      "epoch": 2.5290666666666666,
      "grad_norm": 0.24206207692623138,
      "learning_rate": 3.419333333333333e-05,
      "loss": 0.0022,
      "step": 47420
    },
    {
      "epoch": 2.5296,
      "grad_norm": 0.17559440433979034,
      "learning_rate": 3.419e-05,
      "loss": 0.0019,
      "step": 47430
    },
    {
      "epoch": 2.5301333333333336,
      "grad_norm": 0.14605070650577545,
      "learning_rate": 3.418666666666667e-05,
      "loss": 0.0032,
      "step": 47440
    },
    {
      "epoch": 2.530666666666667,
      "grad_norm": 0.21352194249629974,
      "learning_rate": 3.4183333333333335e-05,
      "loss": 0.0026,
      "step": 47450
    },
    {
      "epoch": 2.5312,
      "grad_norm": 0.2912711799144745,
      "learning_rate": 3.418e-05,
      "loss": 0.0024,
      "step": 47460
    },
    {
      "epoch": 2.5317333333333334,
      "grad_norm": 0.20434921979904175,
      "learning_rate": 3.417666666666667e-05,
      "loss": 0.0027,
      "step": 47470
    },
    {
      "epoch": 2.5322666666666667,
      "grad_norm": 0.23104363679885864,
      "learning_rate": 3.4173333333333334e-05,
      "loss": 0.0036,
      "step": 47480
    },
    {
      "epoch": 2.5328,
      "grad_norm": 0.10392150282859802,
      "learning_rate": 3.417e-05,
      "loss": 0.0036,
      "step": 47490
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 0.5984463095664978,
      "learning_rate": 3.4166666666666666e-05,
      "loss": 0.0033,
      "step": 47500
    },
    {
      "epoch": 2.5338666666666665,
      "grad_norm": 0.3510514497756958,
      "learning_rate": 3.416333333333334e-05,
      "loss": 0.0022,
      "step": 47510
    },
    {
      "epoch": 2.5343999999999998,
      "grad_norm": 0.5238723754882812,
      "learning_rate": 3.4160000000000005e-05,
      "loss": 0.0024,
      "step": 47520
    },
    {
      "epoch": 2.5349333333333335,
      "grad_norm": 0.1562042534351349,
      "learning_rate": 3.415666666666667e-05,
      "loss": 0.0031,
      "step": 47530
    },
    {
      "epoch": 2.5354666666666668,
      "grad_norm": 0.21096982061862946,
      "learning_rate": 3.415333333333334e-05,
      "loss": 0.0038,
      "step": 47540
    },
    {
      "epoch": 2.536,
      "grad_norm": 0.2087220698595047,
      "learning_rate": 3.415e-05,
      "loss": 0.0028,
      "step": 47550
    },
    {
      "epoch": 2.5365333333333333,
      "grad_norm": 0.12310405820608139,
      "learning_rate": 3.414666666666666e-05,
      "loss": 0.0032,
      "step": 47560
    },
    {
      "epoch": 2.5370666666666666,
      "grad_norm": 0.05032145231962204,
      "learning_rate": 3.4143333333333336e-05,
      "loss": 0.0028,
      "step": 47570
    },
    {
      "epoch": 2.5376,
      "grad_norm": 0.32030221819877625,
      "learning_rate": 3.414e-05,
      "loss": 0.0024,
      "step": 47580
    },
    {
      "epoch": 2.5381333333333336,
      "grad_norm": 0.0781104564666748,
      "learning_rate": 3.413666666666667e-05,
      "loss": 0.0032,
      "step": 47590
    },
    {
      "epoch": 2.538666666666667,
      "grad_norm": 0.060302622616291046,
      "learning_rate": 3.4133333333333334e-05,
      "loss": 0.0034,
      "step": 47600
    },
    {
      "epoch": 2.5392,
      "grad_norm": 0.2665744721889496,
      "learning_rate": 3.413e-05,
      "loss": 0.0027,
      "step": 47610
    },
    {
      "epoch": 2.5397333333333334,
      "grad_norm": 0.23266540467739105,
      "learning_rate": 3.4126666666666666e-05,
      "loss": 0.0023,
      "step": 47620
    },
    {
      "epoch": 2.5402666666666667,
      "grad_norm": 0.2583737373352051,
      "learning_rate": 3.412333333333333e-05,
      "loss": 0.0028,
      "step": 47630
    },
    {
      "epoch": 2.5408,
      "grad_norm": 0.4604591131210327,
      "learning_rate": 3.412e-05,
      "loss": 0.0031,
      "step": 47640
    },
    {
      "epoch": 2.541333333333333,
      "grad_norm": 0.04037816822528839,
      "learning_rate": 3.411666666666667e-05,
      "loss": 0.0019,
      "step": 47650
    },
    {
      "epoch": 2.5418666666666665,
      "grad_norm": 0.027412954717874527,
      "learning_rate": 3.411333333333334e-05,
      "loss": 0.0016,
      "step": 47660
    },
    {
      "epoch": 2.5423999999999998,
      "grad_norm": 0.1775810569524765,
      "learning_rate": 3.4110000000000004e-05,
      "loss": 0.0022,
      "step": 47670
    },
    {
      "epoch": 2.5429333333333335,
      "grad_norm": 0.3241022527217865,
      "learning_rate": 3.410666666666667e-05,
      "loss": 0.0026,
      "step": 47680
    },
    {
      "epoch": 2.5434666666666668,
      "grad_norm": 0.2104760855436325,
      "learning_rate": 3.4103333333333336e-05,
      "loss": 0.0033,
      "step": 47690
    },
    {
      "epoch": 2.544,
      "grad_norm": 0.41073018312454224,
      "learning_rate": 3.41e-05,
      "loss": 0.003,
      "step": 47700
    },
    {
      "epoch": 2.5445333333333333,
      "grad_norm": 0.11635445058345795,
      "learning_rate": 3.409666666666667e-05,
      "loss": 0.0018,
      "step": 47710
    },
    {
      "epoch": 2.5450666666666666,
      "grad_norm": 0.2001045197248459,
      "learning_rate": 3.4093333333333334e-05,
      "loss": 0.0032,
      "step": 47720
    },
    {
      "epoch": 2.5456,
      "grad_norm": 0.356448233127594,
      "learning_rate": 3.409e-05,
      "loss": 0.0029,
      "step": 47730
    },
    {
      "epoch": 2.5461333333333336,
      "grad_norm": 0.7838285565376282,
      "learning_rate": 3.408666666666667e-05,
      "loss": 0.0032,
      "step": 47740
    },
    {
      "epoch": 2.546666666666667,
      "grad_norm": 0.6111963391304016,
      "learning_rate": 3.408333333333333e-05,
      "loss": 0.002,
      "step": 47750
    },
    {
      "epoch": 2.5472,
      "grad_norm": 0.14917244017124176,
      "learning_rate": 3.408e-05,
      "loss": 0.0027,
      "step": 47760
    },
    {
      "epoch": 2.5477333333333334,
      "grad_norm": 0.40592506527900696,
      "learning_rate": 3.4076666666666665e-05,
      "loss": 0.0032,
      "step": 47770
    },
    {
      "epoch": 2.5482666666666667,
      "grad_norm": 0.5257264971733093,
      "learning_rate": 3.407333333333334e-05,
      "loss": 0.0032,
      "step": 47780
    },
    {
      "epoch": 2.5488,
      "grad_norm": 0.32125234603881836,
      "learning_rate": 3.4070000000000004e-05,
      "loss": 0.0027,
      "step": 47790
    },
    {
      "epoch": 2.5493333333333332,
      "grad_norm": 0.06430698931217194,
      "learning_rate": 3.406666666666667e-05,
      "loss": 0.0031,
      "step": 47800
    },
    {
      "epoch": 2.5498666666666665,
      "grad_norm": 0.36854270100593567,
      "learning_rate": 3.4063333333333336e-05,
      "loss": 0.0027,
      "step": 47810
    },
    {
      "epoch": 2.5504,
      "grad_norm": 0.4039354920387268,
      "learning_rate": 3.406e-05,
      "loss": 0.0021,
      "step": 47820
    },
    {
      "epoch": 2.5509333333333335,
      "grad_norm": 0.6781608462333679,
      "learning_rate": 3.405666666666667e-05,
      "loss": 0.0032,
      "step": 47830
    },
    {
      "epoch": 2.5514666666666668,
      "grad_norm": 0.35785534977912903,
      "learning_rate": 3.4053333333333335e-05,
      "loss": 0.0026,
      "step": 47840
    },
    {
      "epoch": 2.552,
      "grad_norm": 0.11896581947803497,
      "learning_rate": 3.405e-05,
      "loss": 0.0029,
      "step": 47850
    },
    {
      "epoch": 2.5525333333333333,
      "grad_norm": 0.37096691131591797,
      "learning_rate": 3.404666666666667e-05,
      "loss": 0.0025,
      "step": 47860
    },
    {
      "epoch": 2.5530666666666666,
      "grad_norm": 0.3245391845703125,
      "learning_rate": 3.404333333333333e-05,
      "loss": 0.0029,
      "step": 47870
    },
    {
      "epoch": 2.5536,
      "grad_norm": 0.3174603283405304,
      "learning_rate": 3.404e-05,
      "loss": 0.0029,
      "step": 47880
    },
    {
      "epoch": 2.5541333333333336,
      "grad_norm": 0.3433491289615631,
      "learning_rate": 3.4036666666666665e-05,
      "loss": 0.002,
      "step": 47890
    },
    {
      "epoch": 2.554666666666667,
      "grad_norm": 0.5486850142478943,
      "learning_rate": 3.403333333333333e-05,
      "loss": 0.0026,
      "step": 47900
    },
    {
      "epoch": 2.5552,
      "grad_norm": 0.4139614701271057,
      "learning_rate": 3.403e-05,
      "loss": 0.0032,
      "step": 47910
    },
    {
      "epoch": 2.5557333333333334,
      "grad_norm": 0.17248380184173584,
      "learning_rate": 3.402666666666667e-05,
      "loss": 0.003,
      "step": 47920
    },
    {
      "epoch": 2.5562666666666667,
      "grad_norm": 0.2928481101989746,
      "learning_rate": 3.402333333333334e-05,
      "loss": 0.0027,
      "step": 47930
    },
    {
      "epoch": 2.5568,
      "grad_norm": 0.35090166330337524,
      "learning_rate": 3.402e-05,
      "loss": 0.0035,
      "step": 47940
    },
    {
      "epoch": 2.5573333333333332,
      "grad_norm": 0.06372281908988953,
      "learning_rate": 3.401666666666667e-05,
      "loss": 0.0017,
      "step": 47950
    },
    {
      "epoch": 2.5578666666666665,
      "grad_norm": 0.17610928416252136,
      "learning_rate": 3.4013333333333335e-05,
      "loss": 0.0022,
      "step": 47960
    },
    {
      "epoch": 2.5584,
      "grad_norm": 0.2308773249387741,
      "learning_rate": 3.401e-05,
      "loss": 0.0025,
      "step": 47970
    },
    {
      "epoch": 2.558933333333333,
      "grad_norm": 0.4071617126464844,
      "learning_rate": 3.400666666666667e-05,
      "loss": 0.003,
      "step": 47980
    },
    {
      "epoch": 2.559466666666667,
      "grad_norm": 0.3436170220375061,
      "learning_rate": 3.400333333333334e-05,
      "loss": 0.0029,
      "step": 47990
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.09031970798969269,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.0027,
      "step": 48000
    },
    {
      "epoch": 2.5605333333333333,
      "grad_norm": 0.4317827522754669,
      "learning_rate": 3.3996666666666666e-05,
      "loss": 0.0026,
      "step": 48010
    },
    {
      "epoch": 2.5610666666666666,
      "grad_norm": 0.15038487315177917,
      "learning_rate": 3.399333333333333e-05,
      "loss": 0.0033,
      "step": 48020
    },
    {
      "epoch": 2.5616,
      "grad_norm": 0.06449581682682037,
      "learning_rate": 3.399e-05,
      "loss": 0.0022,
      "step": 48030
    },
    {
      "epoch": 2.5621333333333336,
      "grad_norm": 0.11756249517202377,
      "learning_rate": 3.3986666666666664e-05,
      "loss": 0.0026,
      "step": 48040
    },
    {
      "epoch": 2.562666666666667,
      "grad_norm": 0.3449835479259491,
      "learning_rate": 3.398333333333333e-05,
      "loss": 0.0021,
      "step": 48050
    },
    {
      "epoch": 2.5632,
      "grad_norm": 0.27552804350852966,
      "learning_rate": 3.398e-05,
      "loss": 0.0027,
      "step": 48060
    },
    {
      "epoch": 2.5637333333333334,
      "grad_norm": 0.6070005297660828,
      "learning_rate": 3.397666666666667e-05,
      "loss": 0.0028,
      "step": 48070
    },
    {
      "epoch": 2.5642666666666667,
      "grad_norm": 0.1258765161037445,
      "learning_rate": 3.3973333333333336e-05,
      "loss": 0.0031,
      "step": 48080
    },
    {
      "epoch": 2.5648,
      "grad_norm": 0.06431757658720016,
      "learning_rate": 3.397e-05,
      "loss": 0.0018,
      "step": 48090
    },
    {
      "epoch": 2.5653333333333332,
      "grad_norm": 0.31799188256263733,
      "learning_rate": 3.396666666666667e-05,
      "loss": 0.0032,
      "step": 48100
    },
    {
      "epoch": 2.5658666666666665,
      "grad_norm": 0.37658289074897766,
      "learning_rate": 3.3963333333333334e-05,
      "loss": 0.002,
      "step": 48110
    },
    {
      "epoch": 2.5664,
      "grad_norm": 0.28877928853034973,
      "learning_rate": 3.396e-05,
      "loss": 0.004,
      "step": 48120
    },
    {
      "epoch": 2.566933333333333,
      "grad_norm": 0.43339234590530396,
      "learning_rate": 3.395666666666667e-05,
      "loss": 0.0032,
      "step": 48130
    },
    {
      "epoch": 2.567466666666667,
      "grad_norm": 0.37584587931632996,
      "learning_rate": 3.395333333333334e-05,
      "loss": 0.0021,
      "step": 48140
    },
    {
      "epoch": 2.568,
      "grad_norm": 0.5403541922569275,
      "learning_rate": 3.3950000000000005e-05,
      "loss": 0.0022,
      "step": 48150
    },
    {
      "epoch": 2.5685333333333333,
      "grad_norm": 0.11893734335899353,
      "learning_rate": 3.394666666666667e-05,
      "loss": 0.0019,
      "step": 48160
    },
    {
      "epoch": 2.5690666666666666,
      "grad_norm": 0.17274147272109985,
      "learning_rate": 3.394333333333333e-05,
      "loss": 0.0028,
      "step": 48170
    },
    {
      "epoch": 2.5696,
      "grad_norm": 0.5230305790901184,
      "learning_rate": 3.394e-05,
      "loss": 0.0025,
      "step": 48180
    },
    {
      "epoch": 2.5701333333333336,
      "grad_norm": 0.3791196942329407,
      "learning_rate": 3.393666666666667e-05,
      "loss": 0.0036,
      "step": 48190
    },
    {
      "epoch": 2.570666666666667,
      "grad_norm": 0.5188397765159607,
      "learning_rate": 3.3933333333333336e-05,
      "loss": 0.0024,
      "step": 48200
    },
    {
      "epoch": 2.5712,
      "grad_norm": 0.4894947111606598,
      "learning_rate": 3.393e-05,
      "loss": 0.0023,
      "step": 48210
    },
    {
      "epoch": 2.5717333333333334,
      "grad_norm": 0.29315173625946045,
      "learning_rate": 3.392666666666667e-05,
      "loss": 0.0021,
      "step": 48220
    },
    {
      "epoch": 2.5722666666666667,
      "grad_norm": 0.03612343594431877,
      "learning_rate": 3.3923333333333334e-05,
      "loss": 0.0024,
      "step": 48230
    },
    {
      "epoch": 2.5728,
      "grad_norm": 0.08939901739358902,
      "learning_rate": 3.392e-05,
      "loss": 0.0023,
      "step": 48240
    },
    {
      "epoch": 2.5733333333333333,
      "grad_norm": 0.5809517502784729,
      "learning_rate": 3.391666666666667e-05,
      "loss": 0.0035,
      "step": 48250
    },
    {
      "epoch": 2.5738666666666665,
      "grad_norm": 0.671087384223938,
      "learning_rate": 3.391333333333333e-05,
      "loss": 0.0039,
      "step": 48260
    },
    {
      "epoch": 2.5744,
      "grad_norm": 0.026790091767907143,
      "learning_rate": 3.3910000000000006e-05,
      "loss": 0.0018,
      "step": 48270
    },
    {
      "epoch": 2.574933333333333,
      "grad_norm": 0.18514172732830048,
      "learning_rate": 3.390666666666667e-05,
      "loss": 0.0023,
      "step": 48280
    },
    {
      "epoch": 2.575466666666667,
      "grad_norm": 0.05250096693634987,
      "learning_rate": 3.390333333333334e-05,
      "loss": 0.0026,
      "step": 48290
    },
    {
      "epoch": 2.576,
      "grad_norm": 0.05124301090836525,
      "learning_rate": 3.3900000000000004e-05,
      "loss": 0.0023,
      "step": 48300
    },
    {
      "epoch": 2.5765333333333333,
      "grad_norm": 0.4435264468193054,
      "learning_rate": 3.389666666666667e-05,
      "loss": 0.0036,
      "step": 48310
    },
    {
      "epoch": 2.5770666666666666,
      "grad_norm": 0.4419414699077606,
      "learning_rate": 3.389333333333333e-05,
      "loss": 0.0018,
      "step": 48320
    },
    {
      "epoch": 2.5776,
      "grad_norm": 0.6361152529716492,
      "learning_rate": 3.389e-05,
      "loss": 0.0021,
      "step": 48330
    },
    {
      "epoch": 2.5781333333333336,
      "grad_norm": 0.2927681505680084,
      "learning_rate": 3.388666666666667e-05,
      "loss": 0.0019,
      "step": 48340
    },
    {
      "epoch": 2.578666666666667,
      "grad_norm": 0.016902104020118713,
      "learning_rate": 3.3883333333333335e-05,
      "loss": 0.0029,
      "step": 48350
    },
    {
      "epoch": 2.5792,
      "grad_norm": 0.1191772073507309,
      "learning_rate": 3.388e-05,
      "loss": 0.0034,
      "step": 48360
    },
    {
      "epoch": 2.5797333333333334,
      "grad_norm": 0.4132426083087921,
      "learning_rate": 3.387666666666667e-05,
      "loss": 0.0029,
      "step": 48370
    },
    {
      "epoch": 2.5802666666666667,
      "grad_norm": 0.2888014018535614,
      "learning_rate": 3.387333333333333e-05,
      "loss": 0.0018,
      "step": 48380
    },
    {
      "epoch": 2.5808,
      "grad_norm": 0.1729557365179062,
      "learning_rate": 3.387e-05,
      "loss": 0.0024,
      "step": 48390
    },
    {
      "epoch": 2.5813333333333333,
      "grad_norm": 0.13009673357009888,
      "learning_rate": 3.3866666666666665e-05,
      "loss": 0.0025,
      "step": 48400
    },
    {
      "epoch": 2.5818666666666665,
      "grad_norm": 0.25380396842956543,
      "learning_rate": 3.386333333333334e-05,
      "loss": 0.0025,
      "step": 48410
    },
    {
      "epoch": 2.5824,
      "grad_norm": 0.4374025762081146,
      "learning_rate": 3.3860000000000004e-05,
      "loss": 0.0028,
      "step": 48420
    },
    {
      "epoch": 2.582933333333333,
      "grad_norm": 0.26195773482322693,
      "learning_rate": 3.385666666666667e-05,
      "loss": 0.0024,
      "step": 48430
    },
    {
      "epoch": 2.583466666666667,
      "grad_norm": 0.14578166604042053,
      "learning_rate": 3.385333333333334e-05,
      "loss": 0.003,
      "step": 48440
    },
    {
      "epoch": 2.584,
      "grad_norm": 0.20104970037937164,
      "learning_rate": 3.385e-05,
      "loss": 0.0016,
      "step": 48450
    },
    {
      "epoch": 2.5845333333333333,
      "grad_norm": 0.14415399730205536,
      "learning_rate": 3.384666666666667e-05,
      "loss": 0.002,
      "step": 48460
    },
    {
      "epoch": 2.5850666666666666,
      "grad_norm": 0.5250360369682312,
      "learning_rate": 3.3843333333333335e-05,
      "loss": 0.0015,
      "step": 48470
    },
    {
      "epoch": 2.5856,
      "grad_norm": 0.06400979310274124,
      "learning_rate": 3.384e-05,
      "loss": 0.0035,
      "step": 48480
    },
    {
      "epoch": 2.586133333333333,
      "grad_norm": 0.20699651539325714,
      "learning_rate": 3.383666666666667e-05,
      "loss": 0.0035,
      "step": 48490
    },
    {
      "epoch": 2.586666666666667,
      "grad_norm": 0.12242650985717773,
      "learning_rate": 3.3833333333333334e-05,
      "loss": 0.0027,
      "step": 48500
    },
    {
      "epoch": 2.5872,
      "grad_norm": 0.058863308280706406,
      "learning_rate": 3.383e-05,
      "loss": 0.0029,
      "step": 48510
    },
    {
      "epoch": 2.5877333333333334,
      "grad_norm": 0.11689482629299164,
      "learning_rate": 3.3826666666666666e-05,
      "loss": 0.0033,
      "step": 48520
    },
    {
      "epoch": 2.5882666666666667,
      "grad_norm": 0.31597384810447693,
      "learning_rate": 3.382333333333333e-05,
      "loss": 0.0024,
      "step": 48530
    },
    {
      "epoch": 2.5888,
      "grad_norm": 0.20465226471424103,
      "learning_rate": 3.3820000000000005e-05,
      "loss": 0.0022,
      "step": 48540
    },
    {
      "epoch": 2.5893333333333333,
      "grad_norm": 0.23545847833156586,
      "learning_rate": 3.381666666666667e-05,
      "loss": 0.0034,
      "step": 48550
    },
    {
      "epoch": 2.5898666666666665,
      "grad_norm": 0.38706180453300476,
      "learning_rate": 3.381333333333334e-05,
      "loss": 0.0033,
      "step": 48560
    },
    {
      "epoch": 2.5904,
      "grad_norm": 0.23220577836036682,
      "learning_rate": 3.381e-05,
      "loss": 0.0028,
      "step": 48570
    },
    {
      "epoch": 2.590933333333333,
      "grad_norm": 0.031235795468091965,
      "learning_rate": 3.380666666666667e-05,
      "loss": 0.0029,
      "step": 48580
    },
    {
      "epoch": 2.591466666666667,
      "grad_norm": 0.1177578791975975,
      "learning_rate": 3.3803333333333336e-05,
      "loss": 0.0018,
      "step": 48590
    },
    {
      "epoch": 2.592,
      "grad_norm": 0.46519485116004944,
      "learning_rate": 3.38e-05,
      "loss": 0.0024,
      "step": 48600
    },
    {
      "epoch": 2.5925333333333334,
      "grad_norm": 0.1734129786491394,
      "learning_rate": 3.379666666666667e-05,
      "loss": 0.0043,
      "step": 48610
    },
    {
      "epoch": 2.5930666666666666,
      "grad_norm": 0.0636419951915741,
      "learning_rate": 3.3793333333333334e-05,
      "loss": 0.0027,
      "step": 48620
    },
    {
      "epoch": 2.5936,
      "grad_norm": 0.29147428274154663,
      "learning_rate": 3.379e-05,
      "loss": 0.0022,
      "step": 48630
    },
    {
      "epoch": 2.594133333333333,
      "grad_norm": 0.17639818787574768,
      "learning_rate": 3.3786666666666666e-05,
      "loss": 0.0022,
      "step": 48640
    },
    {
      "epoch": 2.594666666666667,
      "grad_norm": 0.14032866060733795,
      "learning_rate": 3.378333333333333e-05,
      "loss": 0.002,
      "step": 48650
    },
    {
      "epoch": 2.5952,
      "grad_norm": 0.32811176776885986,
      "learning_rate": 3.378e-05,
      "loss": 0.003,
      "step": 48660
    },
    {
      "epoch": 2.5957333333333334,
      "grad_norm": 0.17572809755802155,
      "learning_rate": 3.3776666666666665e-05,
      "loss": 0.0021,
      "step": 48670
    },
    {
      "epoch": 2.5962666666666667,
      "grad_norm": 0.1462288796901703,
      "learning_rate": 3.377333333333334e-05,
      "loss": 0.0035,
      "step": 48680
    },
    {
      "epoch": 2.5968,
      "grad_norm": 0.4062659442424774,
      "learning_rate": 3.3770000000000004e-05,
      "loss": 0.0032,
      "step": 48690
    },
    {
      "epoch": 2.5973333333333333,
      "grad_norm": 0.14696386456489563,
      "learning_rate": 3.376666666666667e-05,
      "loss": 0.0025,
      "step": 48700
    },
    {
      "epoch": 2.5978666666666665,
      "grad_norm": 0.047807708382606506,
      "learning_rate": 3.3763333333333336e-05,
      "loss": 0.0035,
      "step": 48710
    },
    {
      "epoch": 2.5984,
      "grad_norm": 0.11832359433174133,
      "learning_rate": 3.376e-05,
      "loss": 0.0034,
      "step": 48720
    },
    {
      "epoch": 2.598933333333333,
      "grad_norm": 0.2055743932723999,
      "learning_rate": 3.375666666666667e-05,
      "loss": 0.0028,
      "step": 48730
    },
    {
      "epoch": 2.599466666666667,
      "grad_norm": 0.5332445502281189,
      "learning_rate": 3.3753333333333334e-05,
      "loss": 0.0027,
      "step": 48740
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.2635537087917328,
      "learning_rate": 3.375000000000001e-05,
      "loss": 0.0025,
      "step": 48750
    },
    {
      "epoch": 2.6005333333333334,
      "grad_norm": 0.026854727417230606,
      "learning_rate": 3.374666666666667e-05,
      "loss": 0.0031,
      "step": 48760
    },
    {
      "epoch": 2.6010666666666666,
      "grad_norm": 0.11732438951730728,
      "learning_rate": 3.374333333333333e-05,
      "loss": 0.0019,
      "step": 48770
    },
    {
      "epoch": 2.6016,
      "grad_norm": 0.21064020693302155,
      "learning_rate": 3.374e-05,
      "loss": 0.0022,
      "step": 48780
    },
    {
      "epoch": 2.602133333333333,
      "grad_norm": 0.24182458221912384,
      "learning_rate": 3.3736666666666665e-05,
      "loss": 0.0028,
      "step": 48790
    },
    {
      "epoch": 2.602666666666667,
      "grad_norm": 0.061929866671562195,
      "learning_rate": 3.373333333333333e-05,
      "loss": 0.002,
      "step": 48800
    },
    {
      "epoch": 2.6032,
      "grad_norm": 0.3740317225456238,
      "learning_rate": 3.373e-05,
      "loss": 0.002,
      "step": 48810
    },
    {
      "epoch": 2.6037333333333335,
      "grad_norm": 0.09351678192615509,
      "learning_rate": 3.372666666666667e-05,
      "loss": 0.0029,
      "step": 48820
    },
    {
      "epoch": 2.6042666666666667,
      "grad_norm": 0.03185136988759041,
      "learning_rate": 3.3723333333333336e-05,
      "loss": 0.0018,
      "step": 48830
    },
    {
      "epoch": 2.6048,
      "grad_norm": 0.09609252214431763,
      "learning_rate": 3.372e-05,
      "loss": 0.0021,
      "step": 48840
    },
    {
      "epoch": 2.6053333333333333,
      "grad_norm": 0.38432836532592773,
      "learning_rate": 3.371666666666667e-05,
      "loss": 0.0027,
      "step": 48850
    },
    {
      "epoch": 2.6058666666666666,
      "grad_norm": 0.03765135258436203,
      "learning_rate": 3.3713333333333335e-05,
      "loss": 0.0025,
      "step": 48860
    },
    {
      "epoch": 2.6064,
      "grad_norm": 0.46072056889533997,
      "learning_rate": 3.371e-05,
      "loss": 0.003,
      "step": 48870
    },
    {
      "epoch": 2.606933333333333,
      "grad_norm": 0.13615749776363373,
      "learning_rate": 3.370666666666667e-05,
      "loss": 0.0022,
      "step": 48880
    },
    {
      "epoch": 2.607466666666667,
      "grad_norm": 0.6024575233459473,
      "learning_rate": 3.370333333333334e-05,
      "loss": 0.0034,
      "step": 48890
    },
    {
      "epoch": 2.608,
      "grad_norm": 0.1761562079191208,
      "learning_rate": 3.3700000000000006e-05,
      "loss": 0.0025,
      "step": 48900
    },
    {
      "epoch": 2.6085333333333334,
      "grad_norm": 0.3648286759853363,
      "learning_rate": 3.369666666666667e-05,
      "loss": 0.0027,
      "step": 48910
    },
    {
      "epoch": 2.6090666666666666,
      "grad_norm": 0.3169804811477661,
      "learning_rate": 3.369333333333333e-05,
      "loss": 0.002,
      "step": 48920
    },
    {
      "epoch": 2.6096,
      "grad_norm": 0.43433499336242676,
      "learning_rate": 3.369e-05,
      "loss": 0.002,
      "step": 48930
    },
    {
      "epoch": 2.610133333333333,
      "grad_norm": 0.3776407539844513,
      "learning_rate": 3.3686666666666664e-05,
      "loss": 0.0031,
      "step": 48940
    },
    {
      "epoch": 2.610666666666667,
      "grad_norm": 0.21271423995494843,
      "learning_rate": 3.368333333333334e-05,
      "loss": 0.0027,
      "step": 48950
    },
    {
      "epoch": 2.6112,
      "grad_norm": 0.46165335178375244,
      "learning_rate": 3.368e-05,
      "loss": 0.0036,
      "step": 48960
    },
    {
      "epoch": 2.6117333333333335,
      "grad_norm": 0.14470958709716797,
      "learning_rate": 3.367666666666667e-05,
      "loss": 0.0032,
      "step": 48970
    },
    {
      "epoch": 2.6122666666666667,
      "grad_norm": 0.06121375784277916,
      "learning_rate": 3.3673333333333335e-05,
      "loss": 0.0028,
      "step": 48980
    },
    {
      "epoch": 2.6128,
      "grad_norm": 0.38083821535110474,
      "learning_rate": 3.367e-05,
      "loss": 0.0022,
      "step": 48990
    },
    {
      "epoch": 2.6133333333333333,
      "grad_norm": 0.228743776679039,
      "learning_rate": 3.366666666666667e-05,
      "loss": 0.0025,
      "step": 49000
    },
    {
      "epoch": 2.6138666666666666,
      "grad_norm": 0.524520218372345,
      "learning_rate": 3.3663333333333333e-05,
      "loss": 0.0032,
      "step": 49010
    },
    {
      "epoch": 2.6144,
      "grad_norm": 0.1713811159133911,
      "learning_rate": 3.366e-05,
      "loss": 0.0017,
      "step": 49020
    },
    {
      "epoch": 2.614933333333333,
      "grad_norm": 0.693845272064209,
      "learning_rate": 3.365666666666667e-05,
      "loss": 0.0018,
      "step": 49030
    },
    {
      "epoch": 2.615466666666667,
      "grad_norm": 0.2041080743074417,
      "learning_rate": 3.365333333333334e-05,
      "loss": 0.0025,
      "step": 49040
    },
    {
      "epoch": 2.616,
      "grad_norm": 0.42728084325790405,
      "learning_rate": 3.3650000000000005e-05,
      "loss": 0.0016,
      "step": 49050
    },
    {
      "epoch": 2.6165333333333334,
      "grad_norm": 0.041099488735198975,
      "learning_rate": 3.364666666666667e-05,
      "loss": 0.0019,
      "step": 49060
    },
    {
      "epoch": 2.6170666666666667,
      "grad_norm": 0.17486421763896942,
      "learning_rate": 3.364333333333333e-05,
      "loss": 0.0035,
      "step": 49070
    },
    {
      "epoch": 2.6176,
      "grad_norm": 0.13294045627117157,
      "learning_rate": 3.3639999999999996e-05,
      "loss": 0.0024,
      "step": 49080
    },
    {
      "epoch": 2.618133333333333,
      "grad_norm": 0.4929907023906708,
      "learning_rate": 3.363666666666667e-05,
      "loss": 0.0039,
      "step": 49090
    },
    {
      "epoch": 2.618666666666667,
      "grad_norm": 0.2651959955692291,
      "learning_rate": 3.3633333333333335e-05,
      "loss": 0.0018,
      "step": 49100
    },
    {
      "epoch": 2.6192,
      "grad_norm": 0.2599872350692749,
      "learning_rate": 3.363e-05,
      "loss": 0.0032,
      "step": 49110
    },
    {
      "epoch": 2.6197333333333335,
      "grad_norm": 0.26104098558425903,
      "learning_rate": 3.362666666666667e-05,
      "loss": 0.0017,
      "step": 49120
    },
    {
      "epoch": 2.6202666666666667,
      "grad_norm": 0.042201559990644455,
      "learning_rate": 3.3623333333333334e-05,
      "loss": 0.0031,
      "step": 49130
    },
    {
      "epoch": 2.6208,
      "grad_norm": 0.09703046828508377,
      "learning_rate": 3.362e-05,
      "loss": 0.0024,
      "step": 49140
    },
    {
      "epoch": 2.6213333333333333,
      "grad_norm": 0.09143038839101791,
      "learning_rate": 3.3616666666666666e-05,
      "loss": 0.0037,
      "step": 49150
    },
    {
      "epoch": 2.6218666666666666,
      "grad_norm": 0.06554742902517319,
      "learning_rate": 3.361333333333333e-05,
      "loss": 0.0028,
      "step": 49160
    },
    {
      "epoch": 2.6224,
      "grad_norm": 0.14806672930717468,
      "learning_rate": 3.3610000000000005e-05,
      "loss": 0.0026,
      "step": 49170
    },
    {
      "epoch": 2.622933333333333,
      "grad_norm": 0.17666861414909363,
      "learning_rate": 3.360666666666667e-05,
      "loss": 0.0015,
      "step": 49180
    },
    {
      "epoch": 2.6234666666666664,
      "grad_norm": 0.039521895349025726,
      "learning_rate": 3.360333333333334e-05,
      "loss": 0.0036,
      "step": 49190
    },
    {
      "epoch": 2.624,
      "grad_norm": 0.02972133830189705,
      "learning_rate": 3.3600000000000004e-05,
      "loss": 0.0027,
      "step": 49200
    },
    {
      "epoch": 2.6245333333333334,
      "grad_norm": 0.12075736373662949,
      "learning_rate": 3.359666666666667e-05,
      "loss": 0.002,
      "step": 49210
    },
    {
      "epoch": 2.6250666666666667,
      "grad_norm": 0.2293042093515396,
      "learning_rate": 3.359333333333333e-05,
      "loss": 0.0028,
      "step": 49220
    },
    {
      "epoch": 2.6256,
      "grad_norm": 0.285057008266449,
      "learning_rate": 3.359e-05,
      "loss": 0.002,
      "step": 49230
    },
    {
      "epoch": 2.626133333333333,
      "grad_norm": 0.020922815427184105,
      "learning_rate": 3.358666666666667e-05,
      "loss": 0.0028,
      "step": 49240
    },
    {
      "epoch": 2.626666666666667,
      "grad_norm": 0.06059761345386505,
      "learning_rate": 3.3583333333333334e-05,
      "loss": 0.0017,
      "step": 49250
    },
    {
      "epoch": 2.6272,
      "grad_norm": 0.25881481170654297,
      "learning_rate": 3.358e-05,
      "loss": 0.0016,
      "step": 49260
    },
    {
      "epoch": 2.6277333333333335,
      "grad_norm": 0.1169290766119957,
      "learning_rate": 3.3576666666666666e-05,
      "loss": 0.0034,
      "step": 49270
    },
    {
      "epoch": 2.6282666666666668,
      "grad_norm": 0.4159693419933319,
      "learning_rate": 3.357333333333333e-05,
      "loss": 0.0023,
      "step": 49280
    },
    {
      "epoch": 2.6288,
      "grad_norm": 0.23647673428058624,
      "learning_rate": 3.357e-05,
      "loss": 0.0023,
      "step": 49290
    },
    {
      "epoch": 2.6293333333333333,
      "grad_norm": 0.09353934973478317,
      "learning_rate": 3.356666666666667e-05,
      "loss": 0.0019,
      "step": 49300
    },
    {
      "epoch": 2.6298666666666666,
      "grad_norm": 0.1817680299282074,
      "learning_rate": 3.356333333333334e-05,
      "loss": 0.0025,
      "step": 49310
    },
    {
      "epoch": 2.6304,
      "grad_norm": 0.6387761831283569,
      "learning_rate": 3.3560000000000004e-05,
      "loss": 0.0041,
      "step": 49320
    },
    {
      "epoch": 2.630933333333333,
      "grad_norm": 0.4078432619571686,
      "learning_rate": 3.355666666666667e-05,
      "loss": 0.0017,
      "step": 49330
    },
    {
      "epoch": 2.6314666666666664,
      "grad_norm": 0.5793545246124268,
      "learning_rate": 3.3553333333333336e-05,
      "loss": 0.0029,
      "step": 49340
    },
    {
      "epoch": 2.632,
      "grad_norm": 0.11868971586227417,
      "learning_rate": 3.355e-05,
      "loss": 0.0032,
      "step": 49350
    },
    {
      "epoch": 2.6325333333333334,
      "grad_norm": 0.14744152128696442,
      "learning_rate": 3.354666666666667e-05,
      "loss": 0.0028,
      "step": 49360
    },
    {
      "epoch": 2.6330666666666667,
      "grad_norm": 0.044654857367277145,
      "learning_rate": 3.3543333333333335e-05,
      "loss": 0.0024,
      "step": 49370
    },
    {
      "epoch": 2.6336,
      "grad_norm": 0.3542388379573822,
      "learning_rate": 3.354e-05,
      "loss": 0.0022,
      "step": 49380
    },
    {
      "epoch": 2.634133333333333,
      "grad_norm": 0.03758085146546364,
      "learning_rate": 3.353666666666667e-05,
      "loss": 0.0032,
      "step": 49390
    },
    {
      "epoch": 2.634666666666667,
      "grad_norm": 0.06145576387643814,
      "learning_rate": 3.353333333333333e-05,
      "loss": 0.0028,
      "step": 49400
    },
    {
      "epoch": 2.6352,
      "grad_norm": 0.17711414396762848,
      "learning_rate": 3.353e-05,
      "loss": 0.003,
      "step": 49410
    },
    {
      "epoch": 2.6357333333333335,
      "grad_norm": 0.5750236511230469,
      "learning_rate": 3.3526666666666665e-05,
      "loss": 0.0023,
      "step": 49420
    },
    {
      "epoch": 2.6362666666666668,
      "grad_norm": 0.024397442117333412,
      "learning_rate": 3.352333333333333e-05,
      "loss": 0.0022,
      "step": 49430
    },
    {
      "epoch": 2.6368,
      "grad_norm": 0.23043237626552582,
      "learning_rate": 3.3520000000000004e-05,
      "loss": 0.0027,
      "step": 49440
    },
    {
      "epoch": 2.6373333333333333,
      "grad_norm": 0.4615057706832886,
      "learning_rate": 3.351666666666667e-05,
      "loss": 0.0021,
      "step": 49450
    },
    {
      "epoch": 2.6378666666666666,
      "grad_norm": 0.29658275842666626,
      "learning_rate": 3.3513333333333337e-05,
      "loss": 0.0038,
      "step": 49460
    },
    {
      "epoch": 2.6384,
      "grad_norm": 0.09265685081481934,
      "learning_rate": 3.351e-05,
      "loss": 0.0039,
      "step": 49470
    },
    {
      "epoch": 2.638933333333333,
      "grad_norm": 0.033674564212560654,
      "learning_rate": 3.350666666666667e-05,
      "loss": 0.0017,
      "step": 49480
    },
    {
      "epoch": 2.6394666666666664,
      "grad_norm": 0.7239416241645813,
      "learning_rate": 3.3503333333333335e-05,
      "loss": 0.0028,
      "step": 49490
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.04352320358157158,
      "learning_rate": 3.35e-05,
      "loss": 0.002,
      "step": 49500
    },
    {
      "epoch": 2.6405333333333334,
      "grad_norm": 0.44272464513778687,
      "learning_rate": 3.349666666666667e-05,
      "loss": 0.0032,
      "step": 49510
    },
    {
      "epoch": 2.6410666666666667,
      "grad_norm": 0.06291591376066208,
      "learning_rate": 3.349333333333334e-05,
      "loss": 0.0031,
      "step": 49520
    },
    {
      "epoch": 2.6416,
      "grad_norm": 0.11538467556238174,
      "learning_rate": 3.349e-05,
      "loss": 0.0029,
      "step": 49530
    },
    {
      "epoch": 2.6421333333333332,
      "grad_norm": 0.03433763235807419,
      "learning_rate": 3.3486666666666666e-05,
      "loss": 0.0019,
      "step": 49540
    },
    {
      "epoch": 2.642666666666667,
      "grad_norm": 0.20156344771385193,
      "learning_rate": 3.348333333333333e-05,
      "loss": 0.0025,
      "step": 49550
    },
    {
      "epoch": 2.6432,
      "grad_norm": 0.7785102725028992,
      "learning_rate": 3.348e-05,
      "loss": 0.0024,
      "step": 49560
    },
    {
      "epoch": 2.6437333333333335,
      "grad_norm": 0.23131704330444336,
      "learning_rate": 3.3476666666666664e-05,
      "loss": 0.0021,
      "step": 49570
    },
    {
      "epoch": 2.6442666666666668,
      "grad_norm": 0.5764800310134888,
      "learning_rate": 3.347333333333334e-05,
      "loss": 0.002,
      "step": 49580
    },
    {
      "epoch": 2.6448,
      "grad_norm": 0.4676097333431244,
      "learning_rate": 3.347e-05,
      "loss": 0.0026,
      "step": 49590
    },
    {
      "epoch": 2.6453333333333333,
      "grad_norm": 0.375243216753006,
      "learning_rate": 3.346666666666667e-05,
      "loss": 0.0041,
      "step": 49600
    },
    {
      "epoch": 2.6458666666666666,
      "grad_norm": 0.2009032666683197,
      "learning_rate": 3.3463333333333335e-05,
      "loss": 0.0019,
      "step": 49610
    },
    {
      "epoch": 2.6464,
      "grad_norm": 0.20510591566562653,
      "learning_rate": 3.346e-05,
      "loss": 0.0021,
      "step": 49620
    },
    {
      "epoch": 2.646933333333333,
      "grad_norm": 0.28670015931129456,
      "learning_rate": 3.345666666666667e-05,
      "loss": 0.0028,
      "step": 49630
    },
    {
      "epoch": 2.6474666666666664,
      "grad_norm": 0.48559752106666565,
      "learning_rate": 3.3453333333333334e-05,
      "loss": 0.0023,
      "step": 49640
    },
    {
      "epoch": 2.648,
      "grad_norm": 0.051571935415267944,
      "learning_rate": 3.345000000000001e-05,
      "loss": 0.0022,
      "step": 49650
    },
    {
      "epoch": 2.6485333333333334,
      "grad_norm": 0.5042535662651062,
      "learning_rate": 3.344666666666667e-05,
      "loss": 0.0017,
      "step": 49660
    },
    {
      "epoch": 2.6490666666666667,
      "grad_norm": 0.6006346940994263,
      "learning_rate": 3.344333333333334e-05,
      "loss": 0.0024,
      "step": 49670
    },
    {
      "epoch": 2.6496,
      "grad_norm": 0.22928473353385925,
      "learning_rate": 3.344e-05,
      "loss": 0.003,
      "step": 49680
    },
    {
      "epoch": 2.6501333333333332,
      "grad_norm": 0.18273240327835083,
      "learning_rate": 3.3436666666666664e-05,
      "loss": 0.0022,
      "step": 49690
    },
    {
      "epoch": 2.6506666666666665,
      "grad_norm": 0.432887464761734,
      "learning_rate": 3.343333333333333e-05,
      "loss": 0.0025,
      "step": 49700
    },
    {
      "epoch": 2.6512000000000002,
      "grad_norm": 0.17220960557460785,
      "learning_rate": 3.3430000000000003e-05,
      "loss": 0.0032,
      "step": 49710
    },
    {
      "epoch": 2.6517333333333335,
      "grad_norm": 0.17632418870925903,
      "learning_rate": 3.342666666666667e-05,
      "loss": 0.0024,
      "step": 49720
    },
    {
      "epoch": 2.6522666666666668,
      "grad_norm": 0.5745993852615356,
      "learning_rate": 3.3423333333333336e-05,
      "loss": 0.002,
      "step": 49730
    },
    {
      "epoch": 2.6528,
      "grad_norm": 0.2733589708805084,
      "learning_rate": 3.342e-05,
      "loss": 0.0022,
      "step": 49740
    },
    {
      "epoch": 2.6533333333333333,
      "grad_norm": 0.4335630238056183,
      "learning_rate": 3.341666666666667e-05,
      "loss": 0.0023,
      "step": 49750
    },
    {
      "epoch": 2.6538666666666666,
      "grad_norm": 0.3208712637424469,
      "learning_rate": 3.3413333333333334e-05,
      "loss": 0.0015,
      "step": 49760
    },
    {
      "epoch": 2.6544,
      "grad_norm": 0.06289662420749664,
      "learning_rate": 3.341e-05,
      "loss": 0.0014,
      "step": 49770
    },
    {
      "epoch": 2.654933333333333,
      "grad_norm": 0.03835995867848396,
      "learning_rate": 3.3406666666666666e-05,
      "loss": 0.002,
      "step": 49780
    },
    {
      "epoch": 2.6554666666666664,
      "grad_norm": 0.1441783756017685,
      "learning_rate": 3.340333333333334e-05,
      "loss": 0.0027,
      "step": 49790
    },
    {
      "epoch": 2.656,
      "grad_norm": 0.4155634343624115,
      "learning_rate": 3.3400000000000005e-05,
      "loss": 0.0029,
      "step": 49800
    },
    {
      "epoch": 2.6565333333333334,
      "grad_norm": 0.2940615117549896,
      "learning_rate": 3.339666666666667e-05,
      "loss": 0.0028,
      "step": 49810
    },
    {
      "epoch": 2.6570666666666667,
      "grad_norm": 0.059281304478645325,
      "learning_rate": 3.339333333333334e-05,
      "loss": 0.0024,
      "step": 49820
    },
    {
      "epoch": 2.6576,
      "grad_norm": 0.03725306689739227,
      "learning_rate": 3.339e-05,
      "loss": 0.0029,
      "step": 49830
    },
    {
      "epoch": 2.6581333333333332,
      "grad_norm": 0.34785816073417664,
      "learning_rate": 3.338666666666666e-05,
      "loss": 0.003,
      "step": 49840
    },
    {
      "epoch": 2.6586666666666665,
      "grad_norm": 0.03239511325955391,
      "learning_rate": 3.3383333333333336e-05,
      "loss": 0.003,
      "step": 49850
    },
    {
      "epoch": 2.6592000000000002,
      "grad_norm": 0.18628324568271637,
      "learning_rate": 3.338e-05,
      "loss": 0.0037,
      "step": 49860
    },
    {
      "epoch": 2.6597333333333335,
      "grad_norm": 0.16495023667812347,
      "learning_rate": 3.337666666666667e-05,
      "loss": 0.0032,
      "step": 49870
    },
    {
      "epoch": 2.660266666666667,
      "grad_norm": 0.06230296567082405,
      "learning_rate": 3.3373333333333335e-05,
      "loss": 0.0021,
      "step": 49880
    },
    {
      "epoch": 2.6608,
      "grad_norm": 0.23707106709480286,
      "learning_rate": 3.337e-05,
      "loss": 0.0018,
      "step": 49890
    },
    {
      "epoch": 2.6613333333333333,
      "grad_norm": 0.26815059781074524,
      "learning_rate": 3.336666666666667e-05,
      "loss": 0.002,
      "step": 49900
    },
    {
      "epoch": 2.6618666666666666,
      "grad_norm": 0.23926806449890137,
      "learning_rate": 3.336333333333333e-05,
      "loss": 0.0024,
      "step": 49910
    },
    {
      "epoch": 2.6624,
      "grad_norm": 0.06303436309099197,
      "learning_rate": 3.336e-05,
      "loss": 0.0023,
      "step": 49920
    },
    {
      "epoch": 2.662933333333333,
      "grad_norm": 0.14798328280448914,
      "learning_rate": 3.335666666666667e-05,
      "loss": 0.0032,
      "step": 49930
    },
    {
      "epoch": 2.6634666666666664,
      "grad_norm": 0.2069234549999237,
      "learning_rate": 3.335333333333334e-05,
      "loss": 0.0027,
      "step": 49940
    },
    {
      "epoch": 2.664,
      "grad_norm": 0.09592164307832718,
      "learning_rate": 3.3350000000000004e-05,
      "loss": 0.0031,
      "step": 49950
    },
    {
      "epoch": 2.6645333333333334,
      "grad_norm": 0.516968309879303,
      "learning_rate": 3.334666666666667e-05,
      "loss": 0.003,
      "step": 49960
    },
    {
      "epoch": 2.6650666666666667,
      "grad_norm": 0.1754641979932785,
      "learning_rate": 3.3343333333333337e-05,
      "loss": 0.0021,
      "step": 49970
    },
    {
      "epoch": 2.6656,
      "grad_norm": 0.5523279309272766,
      "learning_rate": 3.3339999999999996e-05,
      "loss": 0.002,
      "step": 49980
    },
    {
      "epoch": 2.6661333333333332,
      "grad_norm": 0.2630314826965332,
      "learning_rate": 3.333666666666667e-05,
      "loss": 0.0019,
      "step": 49990
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.43724513053894043,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.0025,
      "step": 50000
    },
    {
      "epoch": 2.6672000000000002,
      "grad_norm": 0.06110669672489166,
      "learning_rate": 3.333e-05,
      "loss": 0.003,
      "step": 50010
    },
    {
      "epoch": 2.6677333333333335,
      "grad_norm": 0.06409953534603119,
      "learning_rate": 3.332666666666667e-05,
      "loss": 0.0031,
      "step": 50020
    },
    {
      "epoch": 2.668266666666667,
      "grad_norm": 0.09561712294816971,
      "learning_rate": 3.332333333333333e-05,
      "loss": 0.0025,
      "step": 50030
    },
    {
      "epoch": 2.6688,
      "grad_norm": 0.0906655415892601,
      "learning_rate": 3.332e-05,
      "loss": 0.0022,
      "step": 50040
    },
    {
      "epoch": 2.6693333333333333,
      "grad_norm": 0.7556108236312866,
      "learning_rate": 3.3316666666666666e-05,
      "loss": 0.0026,
      "step": 50050
    },
    {
      "epoch": 2.6698666666666666,
      "grad_norm": 0.4004947245121002,
      "learning_rate": 3.331333333333334e-05,
      "loss": 0.002,
      "step": 50060
    },
    {
      "epoch": 2.6704,
      "grad_norm": 0.07038294523954391,
      "learning_rate": 3.3310000000000005e-05,
      "loss": 0.0032,
      "step": 50070
    },
    {
      "epoch": 2.670933333333333,
      "grad_norm": 0.2434069663286209,
      "learning_rate": 3.330666666666667e-05,
      "loss": 0.0018,
      "step": 50080
    },
    {
      "epoch": 2.6714666666666664,
      "grad_norm": 0.4182564914226532,
      "learning_rate": 3.330333333333334e-05,
      "loss": 0.0031,
      "step": 50090
    },
    {
      "epoch": 2.672,
      "grad_norm": 0.3269050121307373,
      "learning_rate": 3.33e-05,
      "loss": 0.0024,
      "step": 50100
    },
    {
      "epoch": 2.6725333333333334,
      "grad_norm": 0.41259148716926575,
      "learning_rate": 3.329666666666667e-05,
      "loss": 0.0026,
      "step": 50110
    },
    {
      "epoch": 2.6730666666666667,
      "grad_norm": 0.08926844596862793,
      "learning_rate": 3.3293333333333335e-05,
      "loss": 0.0021,
      "step": 50120
    },
    {
      "epoch": 2.6736,
      "grad_norm": 0.3799581229686737,
      "learning_rate": 3.329e-05,
      "loss": 0.0029,
      "step": 50130
    },
    {
      "epoch": 2.6741333333333333,
      "grad_norm": 0.11629597842693329,
      "learning_rate": 3.328666666666667e-05,
      "loss": 0.0023,
      "step": 50140
    },
    {
      "epoch": 2.6746666666666665,
      "grad_norm": 0.5244466066360474,
      "learning_rate": 3.3283333333333334e-05,
      "loss": 0.0033,
      "step": 50150
    },
    {
      "epoch": 2.6752000000000002,
      "grad_norm": 0.08813191205263138,
      "learning_rate": 3.328e-05,
      "loss": 0.003,
      "step": 50160
    },
    {
      "epoch": 2.6757333333333335,
      "grad_norm": 0.1727418303489685,
      "learning_rate": 3.3276666666666666e-05,
      "loss": 0.0017,
      "step": 50170
    },
    {
      "epoch": 2.676266666666667,
      "grad_norm": 0.06963180750608444,
      "learning_rate": 3.327333333333333e-05,
      "loss": 0.0024,
      "step": 50180
    },
    {
      "epoch": 2.6768,
      "grad_norm": 0.017612021416425705,
      "learning_rate": 3.327e-05,
      "loss": 0.0019,
      "step": 50190
    },
    {
      "epoch": 2.6773333333333333,
      "grad_norm": 0.06450452655553818,
      "learning_rate": 3.326666666666667e-05,
      "loss": 0.0027,
      "step": 50200
    },
    {
      "epoch": 2.6778666666666666,
      "grad_norm": 0.23282404243946075,
      "learning_rate": 3.326333333333334e-05,
      "loss": 0.0025,
      "step": 50210
    },
    {
      "epoch": 2.6784,
      "grad_norm": 0.28818488121032715,
      "learning_rate": 3.3260000000000003e-05,
      "loss": 0.0034,
      "step": 50220
    },
    {
      "epoch": 2.678933333333333,
      "grad_norm": 0.056515466421842575,
      "learning_rate": 3.325666666666667e-05,
      "loss": 0.0021,
      "step": 50230
    },
    {
      "epoch": 2.6794666666666664,
      "grad_norm": 0.40315958857536316,
      "learning_rate": 3.3253333333333336e-05,
      "loss": 0.0021,
      "step": 50240
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.23936927318572998,
      "learning_rate": 3.325e-05,
      "loss": 0.0022,
      "step": 50250
    },
    {
      "epoch": 2.6805333333333334,
      "grad_norm": 0.11570165306329727,
      "learning_rate": 3.324666666666667e-05,
      "loss": 0.0034,
      "step": 50260
    },
    {
      "epoch": 2.6810666666666667,
      "grad_norm": 0.33951613306999207,
      "learning_rate": 3.3243333333333334e-05,
      "loss": 0.0025,
      "step": 50270
    },
    {
      "epoch": 2.6816,
      "grad_norm": 0.03621780499815941,
      "learning_rate": 3.324e-05,
      "loss": 0.0025,
      "step": 50280
    },
    {
      "epoch": 2.6821333333333333,
      "grad_norm": 0.04797552525997162,
      "learning_rate": 3.3236666666666666e-05,
      "loss": 0.0023,
      "step": 50290
    },
    {
      "epoch": 2.6826666666666665,
      "grad_norm": 0.4070054888725281,
      "learning_rate": 3.323333333333333e-05,
      "loss": 0.0021,
      "step": 50300
    },
    {
      "epoch": 2.6832000000000003,
      "grad_norm": 0.34465280175209045,
      "learning_rate": 3.323e-05,
      "loss": 0.0029,
      "step": 50310
    },
    {
      "epoch": 2.6837333333333335,
      "grad_norm": 0.12120555341243744,
      "learning_rate": 3.3226666666666665e-05,
      "loss": 0.0024,
      "step": 50320
    },
    {
      "epoch": 2.684266666666667,
      "grad_norm": 0.260975182056427,
      "learning_rate": 3.322333333333333e-05,
      "loss": 0.0024,
      "step": 50330
    },
    {
      "epoch": 2.6848,
      "grad_norm": 0.3147825002670288,
      "learning_rate": 3.3220000000000004e-05,
      "loss": 0.0028,
      "step": 50340
    },
    {
      "epoch": 2.6853333333333333,
      "grad_norm": 0.37699052691459656,
      "learning_rate": 3.321666666666667e-05,
      "loss": 0.0034,
      "step": 50350
    },
    {
      "epoch": 2.6858666666666666,
      "grad_norm": 0.2042660266160965,
      "learning_rate": 3.3213333333333336e-05,
      "loss": 0.002,
      "step": 50360
    },
    {
      "epoch": 2.6864,
      "grad_norm": 0.46411699056625366,
      "learning_rate": 3.321e-05,
      "loss": 0.0024,
      "step": 50370
    },
    {
      "epoch": 2.686933333333333,
      "grad_norm": 0.028864260762929916,
      "learning_rate": 3.320666666666667e-05,
      "loss": 0.004,
      "step": 50380
    },
    {
      "epoch": 2.6874666666666664,
      "grad_norm": 0.6682103872299194,
      "learning_rate": 3.3203333333333334e-05,
      "loss": 0.0029,
      "step": 50390
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 0.469464510679245,
      "learning_rate": 3.32e-05,
      "loss": 0.0022,
      "step": 50400
    },
    {
      "epoch": 2.6885333333333334,
      "grad_norm": 0.1485217809677124,
      "learning_rate": 3.3196666666666674e-05,
      "loss": 0.0023,
      "step": 50410
    },
    {
      "epoch": 2.6890666666666667,
      "grad_norm": 0.025975938886404037,
      "learning_rate": 3.319333333333334e-05,
      "loss": 0.0019,
      "step": 50420
    },
    {
      "epoch": 2.6896,
      "grad_norm": 0.2318502962589264,
      "learning_rate": 3.319e-05,
      "loss": 0.003,
      "step": 50430
    },
    {
      "epoch": 2.6901333333333333,
      "grad_norm": 0.1728777140378952,
      "learning_rate": 3.3186666666666665e-05,
      "loss": 0.002,
      "step": 50440
    },
    {
      "epoch": 2.6906666666666665,
      "grad_norm": 0.6029930710792542,
      "learning_rate": 3.318333333333333e-05,
      "loss": 0.0022,
      "step": 50450
    },
    {
      "epoch": 2.6912000000000003,
      "grad_norm": 0.03181896731257439,
      "learning_rate": 3.318e-05,
      "loss": 0.0029,
      "step": 50460
    },
    {
      "epoch": 2.6917333333333335,
      "grad_norm": 0.23518413305282593,
      "learning_rate": 3.317666666666667e-05,
      "loss": 0.0031,
      "step": 50470
    },
    {
      "epoch": 2.692266666666667,
      "grad_norm": 0.12565234303474426,
      "learning_rate": 3.3173333333333336e-05,
      "loss": 0.0025,
      "step": 50480
    },
    {
      "epoch": 2.6928,
      "grad_norm": 0.4907154142856598,
      "learning_rate": 3.317e-05,
      "loss": 0.0027,
      "step": 50490
    },
    {
      "epoch": 2.6933333333333334,
      "grad_norm": 0.3163086473941803,
      "learning_rate": 3.316666666666667e-05,
      "loss": 0.0032,
      "step": 50500
    },
    {
      "epoch": 2.6938666666666666,
      "grad_norm": 0.23308955132961273,
      "learning_rate": 3.3163333333333335e-05,
      "loss": 0.0011,
      "step": 50510
    },
    {
      "epoch": 2.6944,
      "grad_norm": 0.1536632925271988,
      "learning_rate": 3.316e-05,
      "loss": 0.0014,
      "step": 50520
    },
    {
      "epoch": 2.694933333333333,
      "grad_norm": 0.037946704775094986,
      "learning_rate": 3.315666666666667e-05,
      "loss": 0.0027,
      "step": 50530
    },
    {
      "epoch": 2.6954666666666665,
      "grad_norm": 0.11889298260211945,
      "learning_rate": 3.315333333333333e-05,
      "loss": 0.0022,
      "step": 50540
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 0.37434226274490356,
      "learning_rate": 3.3150000000000006e-05,
      "loss": 0.0025,
      "step": 50550
    },
    {
      "epoch": 2.6965333333333334,
      "grad_norm": 0.1145637109875679,
      "learning_rate": 3.314666666666667e-05,
      "loss": 0.0034,
      "step": 50560
    },
    {
      "epoch": 2.6970666666666667,
      "grad_norm": 0.12113498151302338,
      "learning_rate": 3.314333333333334e-05,
      "loss": 0.0025,
      "step": 50570
    },
    {
      "epoch": 2.6976,
      "grad_norm": 0.26251423358917236,
      "learning_rate": 3.314e-05,
      "loss": 0.0026,
      "step": 50580
    },
    {
      "epoch": 2.6981333333333333,
      "grad_norm": 0.5499683618545532,
      "learning_rate": 3.3136666666666664e-05,
      "loss": 0.0017,
      "step": 50590
    },
    {
      "epoch": 2.6986666666666665,
      "grad_norm": 0.3938156068325043,
      "learning_rate": 3.313333333333333e-05,
      "loss": 0.0027,
      "step": 50600
    },
    {
      "epoch": 2.6992000000000003,
      "grad_norm": 0.017274750396609306,
      "learning_rate": 3.313e-05,
      "loss": 0.0024,
      "step": 50610
    },
    {
      "epoch": 2.6997333333333335,
      "grad_norm": 0.14633215963840485,
      "learning_rate": 3.312666666666667e-05,
      "loss": 0.0027,
      "step": 50620
    },
    {
      "epoch": 2.700266666666667,
      "grad_norm": 0.5791402459144592,
      "learning_rate": 3.3123333333333335e-05,
      "loss": 0.002,
      "step": 50630
    },
    {
      "epoch": 2.7008,
      "grad_norm": 0.29017508029937744,
      "learning_rate": 3.312e-05,
      "loss": 0.0029,
      "step": 50640
    },
    {
      "epoch": 2.7013333333333334,
      "grad_norm": 0.28881171345710754,
      "learning_rate": 3.311666666666667e-05,
      "loss": 0.0029,
      "step": 50650
    },
    {
      "epoch": 2.7018666666666666,
      "grad_norm": 0.12078505754470825,
      "learning_rate": 3.3113333333333334e-05,
      "loss": 0.0021,
      "step": 50660
    },
    {
      "epoch": 2.7024,
      "grad_norm": 0.2467074692249298,
      "learning_rate": 3.311e-05,
      "loss": 0.0014,
      "step": 50670
    },
    {
      "epoch": 2.702933333333333,
      "grad_norm": 0.4001086950302124,
      "learning_rate": 3.3106666666666666e-05,
      "loss": 0.0038,
      "step": 50680
    },
    {
      "epoch": 2.7034666666666665,
      "grad_norm": 0.14290250837802887,
      "learning_rate": 3.310333333333334e-05,
      "loss": 0.0031,
      "step": 50690
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 0.2606634795665741,
      "learning_rate": 3.3100000000000005e-05,
      "loss": 0.0028,
      "step": 50700
    },
    {
      "epoch": 2.7045333333333335,
      "grad_norm": 0.06151701882481575,
      "learning_rate": 3.309666666666667e-05,
      "loss": 0.0024,
      "step": 50710
    },
    {
      "epoch": 2.7050666666666667,
      "grad_norm": 0.4105563461780548,
      "learning_rate": 3.309333333333334e-05,
      "loss": 0.0031,
      "step": 50720
    },
    {
      "epoch": 2.7056,
      "grad_norm": 0.20313039422035217,
      "learning_rate": 3.309e-05,
      "loss": 0.0033,
      "step": 50730
    },
    {
      "epoch": 2.7061333333333333,
      "grad_norm": 0.09758210927248001,
      "learning_rate": 3.308666666666666e-05,
      "loss": 0.0025,
      "step": 50740
    },
    {
      "epoch": 2.7066666666666666,
      "grad_norm": 0.37911486625671387,
      "learning_rate": 3.3083333333333336e-05,
      "loss": 0.0024,
      "step": 50750
    },
    {
      "epoch": 2.7072000000000003,
      "grad_norm": 0.5436590313911438,
      "learning_rate": 3.308e-05,
      "loss": 0.0035,
      "step": 50760
    },
    {
      "epoch": 2.7077333333333335,
      "grad_norm": 0.3830273449420929,
      "learning_rate": 3.307666666666667e-05,
      "loss": 0.0026,
      "step": 50770
    },
    {
      "epoch": 2.708266666666667,
      "grad_norm": 0.43249133229255676,
      "learning_rate": 3.3073333333333334e-05,
      "loss": 0.0037,
      "step": 50780
    },
    {
      "epoch": 2.7088,
      "grad_norm": 0.23091617226600647,
      "learning_rate": 3.307e-05,
      "loss": 0.0025,
      "step": 50790
    },
    {
      "epoch": 2.7093333333333334,
      "grad_norm": 0.03658773750066757,
      "learning_rate": 3.3066666666666666e-05,
      "loss": 0.0025,
      "step": 50800
    },
    {
      "epoch": 2.7098666666666666,
      "grad_norm": 0.025219207629561424,
      "learning_rate": 3.306333333333333e-05,
      "loss": 0.003,
      "step": 50810
    },
    {
      "epoch": 2.7104,
      "grad_norm": 0.11656545102596283,
      "learning_rate": 3.3060000000000005e-05,
      "loss": 0.0021,
      "step": 50820
    },
    {
      "epoch": 2.710933333333333,
      "grad_norm": 0.14372260868549347,
      "learning_rate": 3.305666666666667e-05,
      "loss": 0.0035,
      "step": 50830
    },
    {
      "epoch": 2.7114666666666665,
      "grad_norm": 0.1475537270307541,
      "learning_rate": 3.305333333333334e-05,
      "loss": 0.0026,
      "step": 50840
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 0.03358692675828934,
      "learning_rate": 3.3050000000000004e-05,
      "loss": 0.0022,
      "step": 50850
    },
    {
      "epoch": 2.7125333333333335,
      "grad_norm": 0.4611954987049103,
      "learning_rate": 3.304666666666667e-05,
      "loss": 0.0035,
      "step": 50860
    },
    {
      "epoch": 2.7130666666666667,
      "grad_norm": 0.15992221236228943,
      "learning_rate": 3.3043333333333336e-05,
      "loss": 0.0032,
      "step": 50870
    },
    {
      "epoch": 2.7136,
      "grad_norm": 0.2602047324180603,
      "learning_rate": 3.304e-05,
      "loss": 0.0031,
      "step": 50880
    },
    {
      "epoch": 2.7141333333333333,
      "grad_norm": 0.29472851753234863,
      "learning_rate": 3.303666666666667e-05,
      "loss": 0.0017,
      "step": 50890
    },
    {
      "epoch": 2.7146666666666666,
      "grad_norm": 0.09208682179450989,
      "learning_rate": 3.3033333333333334e-05,
      "loss": 0.0024,
      "step": 50900
    },
    {
      "epoch": 2.7152,
      "grad_norm": 0.03888072073459625,
      "learning_rate": 3.303e-05,
      "loss": 0.0024,
      "step": 50910
    },
    {
      "epoch": 2.7157333333333336,
      "grad_norm": 0.27816832065582275,
      "learning_rate": 3.302666666666667e-05,
      "loss": 0.002,
      "step": 50920
    },
    {
      "epoch": 2.716266666666667,
      "grad_norm": 0.1796434223651886,
      "learning_rate": 3.302333333333333e-05,
      "loss": 0.0021,
      "step": 50930
    },
    {
      "epoch": 2.7168,
      "grad_norm": 0.20505763590335846,
      "learning_rate": 3.302e-05,
      "loss": 0.0017,
      "step": 50940
    },
    {
      "epoch": 2.7173333333333334,
      "grad_norm": 0.40379592776298523,
      "learning_rate": 3.3016666666666665e-05,
      "loss": 0.0025,
      "step": 50950
    },
    {
      "epoch": 2.7178666666666667,
      "grad_norm": 0.03869501128792763,
      "learning_rate": 3.301333333333334e-05,
      "loss": 0.0023,
      "step": 50960
    },
    {
      "epoch": 2.7184,
      "grad_norm": 0.05080559104681015,
      "learning_rate": 3.3010000000000004e-05,
      "loss": 0.0026,
      "step": 50970
    },
    {
      "epoch": 2.718933333333333,
      "grad_norm": 0.20538164675235748,
      "learning_rate": 3.300666666666667e-05,
      "loss": 0.0037,
      "step": 50980
    },
    {
      "epoch": 2.7194666666666665,
      "grad_norm": 0.06065885350108147,
      "learning_rate": 3.3003333333333336e-05,
      "loss": 0.0028,
      "step": 50990
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.09406281262636185,
      "learning_rate": 3.3e-05,
      "loss": 0.0025,
      "step": 51000
    },
    {
      "epoch": 2.7205333333333335,
      "grad_norm": 0.5347983837127686,
      "learning_rate": 3.299666666666667e-05,
      "loss": 0.0025,
      "step": 51010
    },
    {
      "epoch": 2.7210666666666667,
      "grad_norm": 0.5768568515777588,
      "learning_rate": 3.2993333333333335e-05,
      "loss": 0.0031,
      "step": 51020
    },
    {
      "epoch": 2.7216,
      "grad_norm": 0.7642908692359924,
      "learning_rate": 3.299e-05,
      "loss": 0.0034,
      "step": 51030
    },
    {
      "epoch": 2.7221333333333333,
      "grad_norm": 0.3182314336299896,
      "learning_rate": 3.298666666666667e-05,
      "loss": 0.0019,
      "step": 51040
    },
    {
      "epoch": 2.7226666666666666,
      "grad_norm": 0.14957235753536224,
      "learning_rate": 3.298333333333333e-05,
      "loss": 0.0031,
      "step": 51050
    },
    {
      "epoch": 2.7232,
      "grad_norm": 0.1187223345041275,
      "learning_rate": 3.298e-05,
      "loss": 0.002,
      "step": 51060
    },
    {
      "epoch": 2.7237333333333336,
      "grad_norm": 0.482199102640152,
      "learning_rate": 3.2976666666666665e-05,
      "loss": 0.0024,
      "step": 51070
    },
    {
      "epoch": 2.724266666666667,
      "grad_norm": 0.4317771792411804,
      "learning_rate": 3.297333333333333e-05,
      "loss": 0.0018,
      "step": 51080
    },
    {
      "epoch": 2.7248,
      "grad_norm": 0.17521458864212036,
      "learning_rate": 3.297e-05,
      "loss": 0.0023,
      "step": 51090
    },
    {
      "epoch": 2.7253333333333334,
      "grad_norm": 0.27173441648483276,
      "learning_rate": 3.296666666666667e-05,
      "loss": 0.0028,
      "step": 51100
    },
    {
      "epoch": 2.7258666666666667,
      "grad_norm": 0.2584618628025055,
      "learning_rate": 3.296333333333334e-05,
      "loss": 0.0032,
      "step": 51110
    },
    {
      "epoch": 2.7264,
      "grad_norm": 0.2866174280643463,
      "learning_rate": 3.296e-05,
      "loss": 0.0041,
      "step": 51120
    },
    {
      "epoch": 2.726933333333333,
      "grad_norm": 0.1720215529203415,
      "learning_rate": 3.295666666666667e-05,
      "loss": 0.0036,
      "step": 51130
    },
    {
      "epoch": 2.7274666666666665,
      "grad_norm": 0.2923415005207062,
      "learning_rate": 3.2953333333333335e-05,
      "loss": 0.0033,
      "step": 51140
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 0.34863370656967163,
      "learning_rate": 3.295e-05,
      "loss": 0.0029,
      "step": 51150
    },
    {
      "epoch": 2.7285333333333335,
      "grad_norm": 0.15644215047359467,
      "learning_rate": 3.294666666666667e-05,
      "loss": 0.0033,
      "step": 51160
    },
    {
      "epoch": 2.7290666666666668,
      "grad_norm": 0.0325644314289093,
      "learning_rate": 3.294333333333334e-05,
      "loss": 0.0023,
      "step": 51170
    },
    {
      "epoch": 2.7296,
      "grad_norm": 0.11941264569759369,
      "learning_rate": 3.2940000000000006e-05,
      "loss": 0.0027,
      "step": 51180
    },
    {
      "epoch": 2.7301333333333333,
      "grad_norm": 0.09231223911046982,
      "learning_rate": 3.2936666666666666e-05,
      "loss": 0.0024,
      "step": 51190
    },
    {
      "epoch": 2.7306666666666666,
      "grad_norm": 0.1466168314218521,
      "learning_rate": 3.293333333333333e-05,
      "loss": 0.0024,
      "step": 51200
    },
    {
      "epoch": 2.7312,
      "grad_norm": 0.12892036139965057,
      "learning_rate": 3.293e-05,
      "loss": 0.0033,
      "step": 51210
    },
    {
      "epoch": 2.7317333333333336,
      "grad_norm": 0.34106120467185974,
      "learning_rate": 3.2926666666666664e-05,
      "loss": 0.0027,
      "step": 51220
    },
    {
      "epoch": 2.732266666666667,
      "grad_norm": 0.059692565351724625,
      "learning_rate": 3.292333333333334e-05,
      "loss": 0.0024,
      "step": 51230
    },
    {
      "epoch": 2.7328,
      "grad_norm": 0.09359397739171982,
      "learning_rate": 3.292e-05,
      "loss": 0.0028,
      "step": 51240
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 0.20249289274215698,
      "learning_rate": 3.291666666666667e-05,
      "loss": 0.0038,
      "step": 51250
    },
    {
      "epoch": 2.7338666666666667,
      "grad_norm": 0.12656833231449127,
      "learning_rate": 3.2913333333333336e-05,
      "loss": 0.0031,
      "step": 51260
    },
    {
      "epoch": 2.7344,
      "grad_norm": 0.14991562068462372,
      "learning_rate": 3.291e-05,
      "loss": 0.002,
      "step": 51270
    },
    {
      "epoch": 2.734933333333333,
      "grad_norm": 0.035508595407009125,
      "learning_rate": 3.290666666666667e-05,
      "loss": 0.0025,
      "step": 51280
    },
    {
      "epoch": 2.7354666666666665,
      "grad_norm": 0.612743616104126,
      "learning_rate": 3.2903333333333334e-05,
      "loss": 0.0028,
      "step": 51290
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 0.6025800108909607,
      "learning_rate": 3.29e-05,
      "loss": 0.0025,
      "step": 51300
    },
    {
      "epoch": 2.7365333333333335,
      "grad_norm": 0.11935222893953323,
      "learning_rate": 3.289666666666667e-05,
      "loss": 0.0026,
      "step": 51310
    },
    {
      "epoch": 2.7370666666666668,
      "grad_norm": 0.051310889422893524,
      "learning_rate": 3.289333333333334e-05,
      "loss": 0.0022,
      "step": 51320
    },
    {
      "epoch": 2.7376,
      "grad_norm": 0.08512204140424728,
      "learning_rate": 3.2890000000000005e-05,
      "loss": 0.0027,
      "step": 51330
    },
    {
      "epoch": 2.7381333333333333,
      "grad_norm": 0.17450731992721558,
      "learning_rate": 3.2886666666666665e-05,
      "loss": 0.0032,
      "step": 51340
    },
    {
      "epoch": 2.7386666666666666,
      "grad_norm": 0.11704465001821518,
      "learning_rate": 3.288333333333333e-05,
      "loss": 0.0021,
      "step": 51350
    },
    {
      "epoch": 2.7392,
      "grad_norm": 0.2033301442861557,
      "learning_rate": 3.288e-05,
      "loss": 0.0022,
      "step": 51360
    },
    {
      "epoch": 2.7397333333333336,
      "grad_norm": 0.17466704547405243,
      "learning_rate": 3.287666666666667e-05,
      "loss": 0.003,
      "step": 51370
    },
    {
      "epoch": 2.740266666666667,
      "grad_norm": 0.16007591784000397,
      "learning_rate": 3.2873333333333336e-05,
      "loss": 0.0024,
      "step": 51380
    },
    {
      "epoch": 2.7408,
      "grad_norm": 0.21791619062423706,
      "learning_rate": 3.287e-05,
      "loss": 0.0034,
      "step": 51390
    },
    {
      "epoch": 2.7413333333333334,
      "grad_norm": 0.3725515604019165,
      "learning_rate": 3.286666666666667e-05,
      "loss": 0.002,
      "step": 51400
    },
    {
      "epoch": 2.7418666666666667,
      "grad_norm": 0.4928743839263916,
      "learning_rate": 3.2863333333333334e-05,
      "loss": 0.0029,
      "step": 51410
    },
    {
      "epoch": 2.7424,
      "grad_norm": 0.3281053900718689,
      "learning_rate": 3.286e-05,
      "loss": 0.0014,
      "step": 51420
    },
    {
      "epoch": 2.7429333333333332,
      "grad_norm": 0.636489748954773,
      "learning_rate": 3.285666666666667e-05,
      "loss": 0.0032,
      "step": 51430
    },
    {
      "epoch": 2.7434666666666665,
      "grad_norm": 0.11918478459119797,
      "learning_rate": 3.285333333333333e-05,
      "loss": 0.0023,
      "step": 51440
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 0.044382043182849884,
      "learning_rate": 3.2850000000000006e-05,
      "loss": 0.0028,
      "step": 51450
    },
    {
      "epoch": 2.7445333333333335,
      "grad_norm": 0.11515660583972931,
      "learning_rate": 3.284666666666667e-05,
      "loss": 0.0023,
      "step": 51460
    },
    {
      "epoch": 2.7450666666666668,
      "grad_norm": 0.14799629151821136,
      "learning_rate": 3.284333333333334e-05,
      "loss": 0.0021,
      "step": 51470
    },
    {
      "epoch": 2.7456,
      "grad_norm": 0.6196194291114807,
      "learning_rate": 3.2840000000000004e-05,
      "loss": 0.0021,
      "step": 51480
    },
    {
      "epoch": 2.7461333333333333,
      "grad_norm": 0.14838483929634094,
      "learning_rate": 3.2836666666666663e-05,
      "loss": 0.0023,
      "step": 51490
    },
    {
      "epoch": 2.7466666666666666,
      "grad_norm": 0.06904632598161697,
      "learning_rate": 3.283333333333333e-05,
      "loss": 0.0031,
      "step": 51500
    },
    {
      "epoch": 2.7472,
      "grad_norm": 0.0873769149184227,
      "learning_rate": 3.283e-05,
      "loss": 0.002,
      "step": 51510
    },
    {
      "epoch": 2.7477333333333336,
      "grad_norm": 0.1452331393957138,
      "learning_rate": 3.282666666666667e-05,
      "loss": 0.0033,
      "step": 51520
    },
    {
      "epoch": 2.748266666666667,
      "grad_norm": 0.09669788926839828,
      "learning_rate": 3.2823333333333335e-05,
      "loss": 0.0031,
      "step": 51530
    },
    {
      "epoch": 2.7488,
      "grad_norm": 0.5437644720077515,
      "learning_rate": 3.282e-05,
      "loss": 0.004,
      "step": 51540
    },
    {
      "epoch": 2.7493333333333334,
      "grad_norm": 0.05706237256526947,
      "learning_rate": 3.281666666666667e-05,
      "loss": 0.0027,
      "step": 51550
    },
    {
      "epoch": 2.7498666666666667,
      "grad_norm": 0.054116252809762955,
      "learning_rate": 3.281333333333333e-05,
      "loss": 0.0035,
      "step": 51560
    },
    {
      "epoch": 2.7504,
      "grad_norm": 0.23153500258922577,
      "learning_rate": 3.281e-05,
      "loss": 0.0025,
      "step": 51570
    },
    {
      "epoch": 2.7509333333333332,
      "grad_norm": 0.47032758593559265,
      "learning_rate": 3.280666666666667e-05,
      "loss": 0.0031,
      "step": 51580
    },
    {
      "epoch": 2.7514666666666665,
      "grad_norm": 0.2892058491706848,
      "learning_rate": 3.280333333333334e-05,
      "loss": 0.0027,
      "step": 51590
    },
    {
      "epoch": 2.752,
      "grad_norm": 0.48264971375465393,
      "learning_rate": 3.2800000000000004e-05,
      "loss": 0.0013,
      "step": 51600
    },
    {
      "epoch": 2.7525333333333335,
      "grad_norm": 0.07113882154226303,
      "learning_rate": 3.279666666666667e-05,
      "loss": 0.0032,
      "step": 51610
    },
    {
      "epoch": 2.7530666666666668,
      "grad_norm": 0.2299264371395111,
      "learning_rate": 3.279333333333334e-05,
      "loss": 0.0027,
      "step": 51620
    },
    {
      "epoch": 2.7536,
      "grad_norm": 0.5834280848503113,
      "learning_rate": 3.279e-05,
      "loss": 0.0021,
      "step": 51630
    },
    {
      "epoch": 2.7541333333333333,
      "grad_norm": 0.061061181128025055,
      "learning_rate": 3.278666666666666e-05,
      "loss": 0.0026,
      "step": 51640
    },
    {
      "epoch": 2.7546666666666666,
      "grad_norm": 0.3582383096218109,
      "learning_rate": 3.2783333333333335e-05,
      "loss": 0.0032,
      "step": 51650
    },
    {
      "epoch": 2.7552,
      "grad_norm": 0.2658417522907257,
      "learning_rate": 3.278e-05,
      "loss": 0.0022,
      "step": 51660
    },
    {
      "epoch": 2.7557333333333336,
      "grad_norm": 0.2892861068248749,
      "learning_rate": 3.277666666666667e-05,
      "loss": 0.0024,
      "step": 51670
    },
    {
      "epoch": 2.756266666666667,
      "grad_norm": 0.2541034519672394,
      "learning_rate": 3.2773333333333334e-05,
      "loss": 0.0022,
      "step": 51680
    },
    {
      "epoch": 2.7568,
      "grad_norm": 0.4621885120868683,
      "learning_rate": 3.277e-05,
      "loss": 0.0021,
      "step": 51690
    },
    {
      "epoch": 2.7573333333333334,
      "grad_norm": 0.11916306614875793,
      "learning_rate": 3.2766666666666666e-05,
      "loss": 0.002,
      "step": 51700
    },
    {
      "epoch": 2.7578666666666667,
      "grad_norm": 0.2359468787908554,
      "learning_rate": 3.276333333333333e-05,
      "loss": 0.0028,
      "step": 51710
    },
    {
      "epoch": 2.7584,
      "grad_norm": 0.052348412573337555,
      "learning_rate": 3.2760000000000005e-05,
      "loss": 0.0044,
      "step": 51720
    },
    {
      "epoch": 2.7589333333333332,
      "grad_norm": 0.44898608326911926,
      "learning_rate": 3.275666666666667e-05,
      "loss": 0.0038,
      "step": 51730
    },
    {
      "epoch": 2.7594666666666665,
      "grad_norm": 0.25966617465019226,
      "learning_rate": 3.275333333333334e-05,
      "loss": 0.0022,
      "step": 51740
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.4355223774909973,
      "learning_rate": 3.275e-05,
      "loss": 0.0024,
      "step": 51750
    },
    {
      "epoch": 2.760533333333333,
      "grad_norm": 0.06040872260928154,
      "learning_rate": 3.274666666666667e-05,
      "loss": 0.0031,
      "step": 51760
    },
    {
      "epoch": 2.761066666666667,
      "grad_norm": 0.06837828457355499,
      "learning_rate": 3.2743333333333335e-05,
      "loss": 0.003,
      "step": 51770
    },
    {
      "epoch": 2.7616,
      "grad_norm": 0.431994765996933,
      "learning_rate": 3.274e-05,
      "loss": 0.0021,
      "step": 51780
    },
    {
      "epoch": 2.7621333333333333,
      "grad_norm": 0.4004156291484833,
      "learning_rate": 3.273666666666667e-05,
      "loss": 0.0041,
      "step": 51790
    },
    {
      "epoch": 2.7626666666666666,
      "grad_norm": 0.2584386169910431,
      "learning_rate": 3.2733333333333334e-05,
      "loss": 0.0026,
      "step": 51800
    },
    {
      "epoch": 2.7632,
      "grad_norm": 0.12858840823173523,
      "learning_rate": 3.273e-05,
      "loss": 0.0022,
      "step": 51810
    },
    {
      "epoch": 2.7637333333333336,
      "grad_norm": 0.233157679438591,
      "learning_rate": 3.2726666666666666e-05,
      "loss": 0.0018,
      "step": 51820
    },
    {
      "epoch": 2.764266666666667,
      "grad_norm": 0.08811219781637192,
      "learning_rate": 3.272333333333333e-05,
      "loss": 0.0019,
      "step": 51830
    },
    {
      "epoch": 2.7648,
      "grad_norm": 0.1369502693414688,
      "learning_rate": 3.272e-05,
      "loss": 0.0027,
      "step": 51840
    },
    {
      "epoch": 2.7653333333333334,
      "grad_norm": 0.06263358145952225,
      "learning_rate": 3.2716666666666665e-05,
      "loss": 0.0034,
      "step": 51850
    },
    {
      "epoch": 2.7658666666666667,
      "grad_norm": 0.11632466316223145,
      "learning_rate": 3.271333333333334e-05,
      "loss": 0.003,
      "step": 51860
    },
    {
      "epoch": 2.7664,
      "grad_norm": 0.040010370314121246,
      "learning_rate": 3.2710000000000004e-05,
      "loss": 0.0019,
      "step": 51870
    },
    {
      "epoch": 2.7669333333333332,
      "grad_norm": 0.46025148034095764,
      "learning_rate": 3.270666666666667e-05,
      "loss": 0.0025,
      "step": 51880
    },
    {
      "epoch": 2.7674666666666665,
      "grad_norm": 0.17139127850532532,
      "learning_rate": 3.2703333333333336e-05,
      "loss": 0.0028,
      "step": 51890
    },
    {
      "epoch": 2.768,
      "grad_norm": 0.08955762535333633,
      "learning_rate": 3.27e-05,
      "loss": 0.0047,
      "step": 51900
    },
    {
      "epoch": 2.768533333333333,
      "grad_norm": 0.1741189807653427,
      "learning_rate": 3.269666666666667e-05,
      "loss": 0.0023,
      "step": 51910
    },
    {
      "epoch": 2.769066666666667,
      "grad_norm": 0.6314664483070374,
      "learning_rate": 3.2693333333333334e-05,
      "loss": 0.0025,
      "step": 51920
    },
    {
      "epoch": 2.7696,
      "grad_norm": 0.0915394052863121,
      "learning_rate": 3.269000000000001e-05,
      "loss": 0.0018,
      "step": 51930
    },
    {
      "epoch": 2.7701333333333333,
      "grad_norm": 0.04216646030545235,
      "learning_rate": 3.268666666666667e-05,
      "loss": 0.002,
      "step": 51940
    },
    {
      "epoch": 2.7706666666666666,
      "grad_norm": 0.03323357552289963,
      "learning_rate": 3.268333333333333e-05,
      "loss": 0.0024,
      "step": 51950
    },
    {
      "epoch": 2.7712,
      "grad_norm": 0.26730063557624817,
      "learning_rate": 3.268e-05,
      "loss": 0.0028,
      "step": 51960
    },
    {
      "epoch": 2.7717333333333336,
      "grad_norm": 0.2853187620639801,
      "learning_rate": 3.2676666666666665e-05,
      "loss": 0.0029,
      "step": 51970
    },
    {
      "epoch": 2.772266666666667,
      "grad_norm": 0.06130427494645119,
      "learning_rate": 3.267333333333333e-05,
      "loss": 0.0024,
      "step": 51980
    },
    {
      "epoch": 2.7728,
      "grad_norm": 0.34777671098709106,
      "learning_rate": 3.267e-05,
      "loss": 0.0028,
      "step": 51990
    },
    {
      "epoch": 2.7733333333333334,
      "grad_norm": 0.20152489840984344,
      "learning_rate": 3.266666666666667e-05,
      "loss": 0.0024,
      "step": 52000
    },
    {
      "epoch": 2.7738666666666667,
      "grad_norm": 0.7000733613967896,
      "learning_rate": 3.2663333333333336e-05,
      "loss": 0.0024,
      "step": 52010
    },
    {
      "epoch": 2.7744,
      "grad_norm": 0.20503798127174377,
      "learning_rate": 3.266e-05,
      "loss": 0.0027,
      "step": 52020
    },
    {
      "epoch": 2.7749333333333333,
      "grad_norm": 0.2034079134464264,
      "learning_rate": 3.265666666666667e-05,
      "loss": 0.0038,
      "step": 52030
    },
    {
      "epoch": 2.7754666666666665,
      "grad_norm": 0.6627554893493652,
      "learning_rate": 3.2653333333333335e-05,
      "loss": 0.0026,
      "step": 52040
    },
    {
      "epoch": 2.776,
      "grad_norm": 0.2366773784160614,
      "learning_rate": 3.265e-05,
      "loss": 0.003,
      "step": 52050
    },
    {
      "epoch": 2.776533333333333,
      "grad_norm": 0.2899252772331238,
      "learning_rate": 3.264666666666667e-05,
      "loss": 0.0037,
      "step": 52060
    },
    {
      "epoch": 2.777066666666667,
      "grad_norm": 0.06257160753011703,
      "learning_rate": 3.264333333333334e-05,
      "loss": 0.0032,
      "step": 52070
    },
    {
      "epoch": 2.7776,
      "grad_norm": 0.3172249495983124,
      "learning_rate": 3.2640000000000006e-05,
      "loss": 0.003,
      "step": 52080
    },
    {
      "epoch": 2.7781333333333333,
      "grad_norm": 0.09177785366773605,
      "learning_rate": 3.263666666666667e-05,
      "loss": 0.0015,
      "step": 52090
    },
    {
      "epoch": 2.7786666666666666,
      "grad_norm": 0.29886242747306824,
      "learning_rate": 3.263333333333333e-05,
      "loss": 0.0026,
      "step": 52100
    },
    {
      "epoch": 2.7792,
      "grad_norm": 0.6131444573402405,
      "learning_rate": 3.263e-05,
      "loss": 0.0029,
      "step": 52110
    },
    {
      "epoch": 2.779733333333333,
      "grad_norm": 0.17191828787326813,
      "learning_rate": 3.2626666666666664e-05,
      "loss": 0.0019,
      "step": 52120
    },
    {
      "epoch": 2.780266666666667,
      "grad_norm": 0.040939077734947205,
      "learning_rate": 3.262333333333334e-05,
      "loss": 0.002,
      "step": 52130
    },
    {
      "epoch": 2.7808,
      "grad_norm": 0.290047824382782,
      "learning_rate": 3.262e-05,
      "loss": 0.0032,
      "step": 52140
    },
    {
      "epoch": 2.7813333333333334,
      "grad_norm": 0.1179364025592804,
      "learning_rate": 3.261666666666667e-05,
      "loss": 0.0025,
      "step": 52150
    },
    {
      "epoch": 2.7818666666666667,
      "grad_norm": 0.1808898150920868,
      "learning_rate": 3.2613333333333335e-05,
      "loss": 0.0021,
      "step": 52160
    },
    {
      "epoch": 2.7824,
      "grad_norm": 0.2054527997970581,
      "learning_rate": 3.261e-05,
      "loss": 0.003,
      "step": 52170
    },
    {
      "epoch": 2.7829333333333333,
      "grad_norm": 0.38241106271743774,
      "learning_rate": 3.260666666666667e-05,
      "loss": 0.0023,
      "step": 52180
    },
    {
      "epoch": 2.7834666666666665,
      "grad_norm": 0.4021344482898712,
      "learning_rate": 3.2603333333333333e-05,
      "loss": 0.0021,
      "step": 52190
    },
    {
      "epoch": 2.784,
      "grad_norm": 0.2392917424440384,
      "learning_rate": 3.26e-05,
      "loss": 0.0024,
      "step": 52200
    },
    {
      "epoch": 2.784533333333333,
      "grad_norm": 0.18259957432746887,
      "learning_rate": 3.259666666666667e-05,
      "loss": 0.0025,
      "step": 52210
    },
    {
      "epoch": 2.785066666666667,
      "grad_norm": 0.4599224328994751,
      "learning_rate": 3.259333333333334e-05,
      "loss": 0.0033,
      "step": 52220
    },
    {
      "epoch": 2.7856,
      "grad_norm": 0.23392115533351898,
      "learning_rate": 3.2590000000000005e-05,
      "loss": 0.0026,
      "step": 52230
    },
    {
      "epoch": 2.7861333333333334,
      "grad_norm": 0.2620680034160614,
      "learning_rate": 3.258666666666667e-05,
      "loss": 0.0016,
      "step": 52240
    },
    {
      "epoch": 2.7866666666666666,
      "grad_norm": 0.3270304203033447,
      "learning_rate": 3.258333333333333e-05,
      "loss": 0.0024,
      "step": 52250
    },
    {
      "epoch": 2.7872,
      "grad_norm": 0.06694532185792923,
      "learning_rate": 3.2579999999999996e-05,
      "loss": 0.0024,
      "step": 52260
    },
    {
      "epoch": 2.787733333333333,
      "grad_norm": 0.2404365986585617,
      "learning_rate": 3.257666666666667e-05,
      "loss": 0.0022,
      "step": 52270
    },
    {
      "epoch": 2.788266666666667,
      "grad_norm": 0.5747088193893433,
      "learning_rate": 3.2573333333333335e-05,
      "loss": 0.0021,
      "step": 52280
    },
    {
      "epoch": 2.7888,
      "grad_norm": 0.8026857972145081,
      "learning_rate": 3.257e-05,
      "loss": 0.003,
      "step": 52290
    },
    {
      "epoch": 2.7893333333333334,
      "grad_norm": 0.06767137348651886,
      "learning_rate": 3.256666666666667e-05,
      "loss": 0.002,
      "step": 52300
    },
    {
      "epoch": 2.7898666666666667,
      "grad_norm": 0.24855174124240875,
      "learning_rate": 3.2563333333333334e-05,
      "loss": 0.0023,
      "step": 52310
    },
    {
      "epoch": 2.7904,
      "grad_norm": 0.4318605065345764,
      "learning_rate": 3.256e-05,
      "loss": 0.002,
      "step": 52320
    },
    {
      "epoch": 2.7909333333333333,
      "grad_norm": 0.09429016709327698,
      "learning_rate": 3.2556666666666666e-05,
      "loss": 0.002,
      "step": 52330
    },
    {
      "epoch": 2.7914666666666665,
      "grad_norm": 0.09091705083847046,
      "learning_rate": 3.255333333333334e-05,
      "loss": 0.0039,
      "step": 52340
    },
    {
      "epoch": 2.792,
      "grad_norm": 0.025932911783456802,
      "learning_rate": 3.2550000000000005e-05,
      "loss": 0.0022,
      "step": 52350
    },
    {
      "epoch": 2.792533333333333,
      "grad_norm": 0.09510321170091629,
      "learning_rate": 3.254666666666667e-05,
      "loss": 0.0021,
      "step": 52360
    },
    {
      "epoch": 2.793066666666667,
      "grad_norm": 0.3473268747329712,
      "learning_rate": 3.254333333333334e-05,
      "loss": 0.002,
      "step": 52370
    },
    {
      "epoch": 2.7936,
      "grad_norm": 0.06286220252513885,
      "learning_rate": 3.2540000000000004e-05,
      "loss": 0.0021,
      "step": 52380
    },
    {
      "epoch": 2.7941333333333334,
      "grad_norm": 0.4025626480579376,
      "learning_rate": 3.253666666666667e-05,
      "loss": 0.0025,
      "step": 52390
    },
    {
      "epoch": 2.7946666666666666,
      "grad_norm": 0.20840060710906982,
      "learning_rate": 3.253333333333333e-05,
      "loss": 0.0028,
      "step": 52400
    },
    {
      "epoch": 2.7952,
      "grad_norm": 0.04005368798971176,
      "learning_rate": 3.253e-05,
      "loss": 0.002,
      "step": 52410
    },
    {
      "epoch": 2.795733333333333,
      "grad_norm": 0.4009060263633728,
      "learning_rate": 3.252666666666667e-05,
      "loss": 0.0045,
      "step": 52420
    },
    {
      "epoch": 2.796266666666667,
      "grad_norm": 0.432722270488739,
      "learning_rate": 3.2523333333333334e-05,
      "loss": 0.0024,
      "step": 52430
    },
    {
      "epoch": 2.7968,
      "grad_norm": 0.31976279616355896,
      "learning_rate": 3.252e-05,
      "loss": 0.003,
      "step": 52440
    },
    {
      "epoch": 2.7973333333333334,
      "grad_norm": 0.3195379078388214,
      "learning_rate": 3.2516666666666666e-05,
      "loss": 0.0035,
      "step": 52450
    },
    {
      "epoch": 2.7978666666666667,
      "grad_norm": 0.11089274287223816,
      "learning_rate": 3.251333333333333e-05,
      "loss": 0.0025,
      "step": 52460
    },
    {
      "epoch": 2.7984,
      "grad_norm": 0.20145094394683838,
      "learning_rate": 3.251e-05,
      "loss": 0.0032,
      "step": 52470
    },
    {
      "epoch": 2.7989333333333333,
      "grad_norm": 0.0351891927421093,
      "learning_rate": 3.250666666666667e-05,
      "loss": 0.0018,
      "step": 52480
    },
    {
      "epoch": 2.7994666666666665,
      "grad_norm": 0.2322463095188141,
      "learning_rate": 3.250333333333334e-05,
      "loss": 0.0018,
      "step": 52490
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.3921029567718506,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 0.0019,
      "step": 52500
    },
    {
      "epoch": 2.800533333333333,
      "grad_norm": 0.2869078814983368,
      "learning_rate": 3.249666666666667e-05,
      "loss": 0.0019,
      "step": 52510
    },
    {
      "epoch": 2.801066666666667,
      "grad_norm": 0.5845401883125305,
      "learning_rate": 3.2493333333333336e-05,
      "loss": 0.0023,
      "step": 52520
    },
    {
      "epoch": 2.8016,
      "grad_norm": 0.1533568650484085,
      "learning_rate": 3.249e-05,
      "loss": 0.0034,
      "step": 52530
    },
    {
      "epoch": 2.8021333333333334,
      "grad_norm": 0.5099045634269714,
      "learning_rate": 3.248666666666667e-05,
      "loss": 0.0032,
      "step": 52540
    },
    {
      "epoch": 2.8026666666666666,
      "grad_norm": 0.607373058795929,
      "learning_rate": 3.2483333333333335e-05,
      "loss": 0.002,
      "step": 52550
    },
    {
      "epoch": 2.8032,
      "grad_norm": 0.02684628590941429,
      "learning_rate": 3.248e-05,
      "loss": 0.0025,
      "step": 52560
    },
    {
      "epoch": 2.803733333333333,
      "grad_norm": 0.06267912685871124,
      "learning_rate": 3.247666666666667e-05,
      "loss": 0.0037,
      "step": 52570
    },
    {
      "epoch": 2.804266666666667,
      "grad_norm": 0.24148833751678467,
      "learning_rate": 3.247333333333333e-05,
      "loss": 0.0031,
      "step": 52580
    },
    {
      "epoch": 2.8048,
      "grad_norm": 0.07215913385152817,
      "learning_rate": 3.247e-05,
      "loss": 0.0026,
      "step": 52590
    },
    {
      "epoch": 2.8053333333333335,
      "grad_norm": 0.4958049952983856,
      "learning_rate": 3.2466666666666665e-05,
      "loss": 0.0027,
      "step": 52600
    },
    {
      "epoch": 2.8058666666666667,
      "grad_norm": 0.5251016020774841,
      "learning_rate": 3.246333333333333e-05,
      "loss": 0.0022,
      "step": 52610
    },
    {
      "epoch": 2.8064,
      "grad_norm": 0.14413689076900482,
      "learning_rate": 3.2460000000000004e-05,
      "loss": 0.0025,
      "step": 52620
    },
    {
      "epoch": 2.8069333333333333,
      "grad_norm": 0.16774071753025055,
      "learning_rate": 3.245666666666667e-05,
      "loss": 0.0022,
      "step": 52630
    },
    {
      "epoch": 2.8074666666666666,
      "grad_norm": 0.5468263626098633,
      "learning_rate": 3.2453333333333337e-05,
      "loss": 0.0024,
      "step": 52640
    },
    {
      "epoch": 2.808,
      "grad_norm": 0.4732159972190857,
      "learning_rate": 3.245e-05,
      "loss": 0.0031,
      "step": 52650
    },
    {
      "epoch": 2.808533333333333,
      "grad_norm": 0.5144257545471191,
      "learning_rate": 3.244666666666667e-05,
      "loss": 0.0038,
      "step": 52660
    },
    {
      "epoch": 2.809066666666667,
      "grad_norm": 0.10590904206037521,
      "learning_rate": 3.2443333333333335e-05,
      "loss": 0.0033,
      "step": 52670
    },
    {
      "epoch": 2.8096,
      "grad_norm": 0.35192546248435974,
      "learning_rate": 3.244e-05,
      "loss": 0.0028,
      "step": 52680
    },
    {
      "epoch": 2.8101333333333334,
      "grad_norm": 0.43812644481658936,
      "learning_rate": 3.2436666666666674e-05,
      "loss": 0.0024,
      "step": 52690
    },
    {
      "epoch": 2.8106666666666666,
      "grad_norm": 0.14478130638599396,
      "learning_rate": 3.243333333333333e-05,
      "loss": 0.0028,
      "step": 52700
    },
    {
      "epoch": 2.8112,
      "grad_norm": 0.46293017268180847,
      "learning_rate": 3.243e-05,
      "loss": 0.0029,
      "step": 52710
    },
    {
      "epoch": 2.811733333333333,
      "grad_norm": 0.5566547513008118,
      "learning_rate": 3.2426666666666666e-05,
      "loss": 0.003,
      "step": 52720
    },
    {
      "epoch": 2.812266666666667,
      "grad_norm": 0.2929694950580597,
      "learning_rate": 3.242333333333333e-05,
      "loss": 0.004,
      "step": 52730
    },
    {
      "epoch": 2.8128,
      "grad_norm": 0.5676983594894409,
      "learning_rate": 3.242e-05,
      "loss": 0.0024,
      "step": 52740
    },
    {
      "epoch": 2.8133333333333335,
      "grad_norm": 0.2934979796409607,
      "learning_rate": 3.2416666666666664e-05,
      "loss": 0.003,
      "step": 52750
    },
    {
      "epoch": 2.8138666666666667,
      "grad_norm": 0.10130380839109421,
      "learning_rate": 3.241333333333334e-05,
      "loss": 0.0031,
      "step": 52760
    },
    {
      "epoch": 2.8144,
      "grad_norm": 0.11598692089319229,
      "learning_rate": 3.241e-05,
      "loss": 0.0017,
      "step": 52770
    },
    {
      "epoch": 2.8149333333333333,
      "grad_norm": 0.12732112407684326,
      "learning_rate": 3.240666666666667e-05,
      "loss": 0.0026,
      "step": 52780
    },
    {
      "epoch": 2.8154666666666666,
      "grad_norm": 0.024741310626268387,
      "learning_rate": 3.2403333333333335e-05,
      "loss": 0.0029,
      "step": 52790
    },
    {
      "epoch": 2.816,
      "grad_norm": 0.11713533848524094,
      "learning_rate": 3.24e-05,
      "loss": 0.002,
      "step": 52800
    },
    {
      "epoch": 2.816533333333333,
      "grad_norm": 0.20301899313926697,
      "learning_rate": 3.239666666666667e-05,
      "loss": 0.0027,
      "step": 52810
    },
    {
      "epoch": 2.817066666666667,
      "grad_norm": 0.026545219123363495,
      "learning_rate": 3.2393333333333334e-05,
      "loss": 0.002,
      "step": 52820
    },
    {
      "epoch": 2.8176,
      "grad_norm": 0.3199557065963745,
      "learning_rate": 3.239000000000001e-05,
      "loss": 0.0038,
      "step": 52830
    },
    {
      "epoch": 2.8181333333333334,
      "grad_norm": 0.08233276754617691,
      "learning_rate": 3.238666666666667e-05,
      "loss": 0.0032,
      "step": 52840
    },
    {
      "epoch": 2.8186666666666667,
      "grad_norm": 0.03541962802410126,
      "learning_rate": 3.238333333333333e-05,
      "loss": 0.0031,
      "step": 52850
    },
    {
      "epoch": 2.8192,
      "grad_norm": 0.40760207176208496,
      "learning_rate": 3.238e-05,
      "loss": 0.002,
      "step": 52860
    },
    {
      "epoch": 2.819733333333333,
      "grad_norm": 0.11657781898975372,
      "learning_rate": 3.2376666666666664e-05,
      "loss": 0.003,
      "step": 52870
    },
    {
      "epoch": 2.820266666666667,
      "grad_norm": 0.47763770818710327,
      "learning_rate": 3.237333333333333e-05,
      "loss": 0.0028,
      "step": 52880
    },
    {
      "epoch": 2.8208,
      "grad_norm": 0.5016773343086243,
      "learning_rate": 3.2370000000000003e-05,
      "loss": 0.004,
      "step": 52890
    },
    {
      "epoch": 2.8213333333333335,
      "grad_norm": 0.17675137519836426,
      "learning_rate": 3.236666666666667e-05,
      "loss": 0.0033,
      "step": 52900
    },
    {
      "epoch": 2.8218666666666667,
      "grad_norm": 0.5590776801109314,
      "learning_rate": 3.2363333333333336e-05,
      "loss": 0.0024,
      "step": 52910
    },
    {
      "epoch": 2.8224,
      "grad_norm": 0.5793902277946472,
      "learning_rate": 3.236e-05,
      "loss": 0.0021,
      "step": 52920
    },
    {
      "epoch": 2.8229333333333333,
      "grad_norm": 0.2350836843252182,
      "learning_rate": 3.235666666666667e-05,
      "loss": 0.0034,
      "step": 52930
    },
    {
      "epoch": 2.8234666666666666,
      "grad_norm": 0.2648579478263855,
      "learning_rate": 3.2353333333333334e-05,
      "loss": 0.0021,
      "step": 52940
    },
    {
      "epoch": 2.824,
      "grad_norm": 0.3812391757965088,
      "learning_rate": 3.235e-05,
      "loss": 0.003,
      "step": 52950
    },
    {
      "epoch": 2.824533333333333,
      "grad_norm": 0.1483527570962906,
      "learning_rate": 3.2346666666666666e-05,
      "loss": 0.0023,
      "step": 52960
    },
    {
      "epoch": 2.8250666666666664,
      "grad_norm": 0.3102816641330719,
      "learning_rate": 3.234333333333334e-05,
      "loss": 0.0021,
      "step": 52970
    },
    {
      "epoch": 2.8256,
      "grad_norm": 0.5120153427124023,
      "learning_rate": 3.2340000000000005e-05,
      "loss": 0.0021,
      "step": 52980
    },
    {
      "epoch": 2.8261333333333334,
      "grad_norm": 0.3748782277107239,
      "learning_rate": 3.233666666666667e-05,
      "loss": 0.0025,
      "step": 52990
    },
    {
      "epoch": 2.8266666666666667,
      "grad_norm": 0.35572969913482666,
      "learning_rate": 3.233333333333333e-05,
      "loss": 0.0027,
      "step": 53000
    },
    {
      "epoch": 2.8272,
      "grad_norm": 0.4644351005554199,
      "learning_rate": 3.233e-05,
      "loss": 0.0018,
      "step": 53010
    },
    {
      "epoch": 2.827733333333333,
      "grad_norm": 0.17805571854114532,
      "learning_rate": 3.232666666666666e-05,
      "loss": 0.0032,
      "step": 53020
    },
    {
      "epoch": 2.828266666666667,
      "grad_norm": 0.26471537351608276,
      "learning_rate": 3.2323333333333336e-05,
      "loss": 0.0041,
      "step": 53030
    },
    {
      "epoch": 2.8288,
      "grad_norm": 0.2171667069196701,
      "learning_rate": 3.232e-05,
      "loss": 0.0022,
      "step": 53040
    },
    {
      "epoch": 2.8293333333333335,
      "grad_norm": 0.25091254711151123,
      "learning_rate": 3.231666666666667e-05,
      "loss": 0.002,
      "step": 53050
    },
    {
      "epoch": 2.8298666666666668,
      "grad_norm": 0.05830204114317894,
      "learning_rate": 3.2313333333333335e-05,
      "loss": 0.002,
      "step": 53060
    },
    {
      "epoch": 2.8304,
      "grad_norm": 0.11784243583679199,
      "learning_rate": 3.231e-05,
      "loss": 0.002,
      "step": 53070
    },
    {
      "epoch": 2.8309333333333333,
      "grad_norm": 0.1186130940914154,
      "learning_rate": 3.230666666666667e-05,
      "loss": 0.0031,
      "step": 53080
    },
    {
      "epoch": 2.8314666666666666,
      "grad_norm": 0.35054051876068115,
      "learning_rate": 3.230333333333333e-05,
      "loss": 0.0029,
      "step": 53090
    },
    {
      "epoch": 2.832,
      "grad_norm": 0.060875460505485535,
      "learning_rate": 3.2300000000000006e-05,
      "loss": 0.0021,
      "step": 53100
    },
    {
      "epoch": 2.832533333333333,
      "grad_norm": 0.48930037021636963,
      "learning_rate": 3.229666666666667e-05,
      "loss": 0.0022,
      "step": 53110
    },
    {
      "epoch": 2.8330666666666664,
      "grad_norm": 0.28739050030708313,
      "learning_rate": 3.229333333333334e-05,
      "loss": 0.0034,
      "step": 53120
    },
    {
      "epoch": 2.8336,
      "grad_norm": 0.4071322977542877,
      "learning_rate": 3.2290000000000004e-05,
      "loss": 0.0027,
      "step": 53130
    },
    {
      "epoch": 2.8341333333333334,
      "grad_norm": 0.15663953125476837,
      "learning_rate": 3.228666666666667e-05,
      "loss": 0.0032,
      "step": 53140
    },
    {
      "epoch": 2.8346666666666667,
      "grad_norm": 0.4387225806713104,
      "learning_rate": 3.2283333333333337e-05,
      "loss": 0.0025,
      "step": 53150
    },
    {
      "epoch": 2.8352,
      "grad_norm": 0.0661190003156662,
      "learning_rate": 3.2279999999999996e-05,
      "loss": 0.0026,
      "step": 53160
    },
    {
      "epoch": 2.835733333333333,
      "grad_norm": 0.3202480673789978,
      "learning_rate": 3.227666666666667e-05,
      "loss": 0.0024,
      "step": 53170
    },
    {
      "epoch": 2.836266666666667,
      "grad_norm": 0.021537180989980698,
      "learning_rate": 3.2273333333333335e-05,
      "loss": 0.0021,
      "step": 53180
    },
    {
      "epoch": 2.8368,
      "grad_norm": 0.165615051984787,
      "learning_rate": 3.227e-05,
      "loss": 0.0029,
      "step": 53190
    },
    {
      "epoch": 2.8373333333333335,
      "grad_norm": 0.4607642590999603,
      "learning_rate": 3.226666666666667e-05,
      "loss": 0.002,
      "step": 53200
    },
    {
      "epoch": 2.8378666666666668,
      "grad_norm": 0.153537780046463,
      "learning_rate": 3.226333333333333e-05,
      "loss": 0.0027,
      "step": 53210
    },
    {
      "epoch": 2.8384,
      "grad_norm": 0.2583884298801422,
      "learning_rate": 3.226e-05,
      "loss": 0.0033,
      "step": 53220
    },
    {
      "epoch": 2.8389333333333333,
      "grad_norm": 0.17802369594573975,
      "learning_rate": 3.2256666666666666e-05,
      "loss": 0.0033,
      "step": 53230
    },
    {
      "epoch": 2.8394666666666666,
      "grad_norm": 0.3167920410633087,
      "learning_rate": 3.225333333333334e-05,
      "loss": 0.0016,
      "step": 53240
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.20555032789707184,
      "learning_rate": 3.2250000000000005e-05,
      "loss": 0.0025,
      "step": 53250
    },
    {
      "epoch": 2.840533333333333,
      "grad_norm": 0.6071826219558716,
      "learning_rate": 3.224666666666667e-05,
      "loss": 0.0023,
      "step": 53260
    },
    {
      "epoch": 2.8410666666666664,
      "grad_norm": 0.3503292500972748,
      "learning_rate": 3.224333333333334e-05,
      "loss": 0.0031,
      "step": 53270
    },
    {
      "epoch": 2.8416,
      "grad_norm": 0.3509320020675659,
      "learning_rate": 3.224e-05,
      "loss": 0.0025,
      "step": 53280
    },
    {
      "epoch": 2.8421333333333334,
      "grad_norm": 0.09676456451416016,
      "learning_rate": 3.223666666666667e-05,
      "loss": 0.0032,
      "step": 53290
    },
    {
      "epoch": 2.8426666666666667,
      "grad_norm": 0.15248073637485504,
      "learning_rate": 3.2233333333333335e-05,
      "loss": 0.0014,
      "step": 53300
    },
    {
      "epoch": 2.8432,
      "grad_norm": 0.3744395077228546,
      "learning_rate": 3.223e-05,
      "loss": 0.0024,
      "step": 53310
    },
    {
      "epoch": 2.8437333333333332,
      "grad_norm": 0.14776401221752167,
      "learning_rate": 3.222666666666667e-05,
      "loss": 0.004,
      "step": 53320
    },
    {
      "epoch": 2.844266666666667,
      "grad_norm": 0.5740103125572205,
      "learning_rate": 3.2223333333333334e-05,
      "loss": 0.0041,
      "step": 53330
    },
    {
      "epoch": 2.8448,
      "grad_norm": 0.4351080060005188,
      "learning_rate": 3.222e-05,
      "loss": 0.0022,
      "step": 53340
    },
    {
      "epoch": 2.8453333333333335,
      "grad_norm": 0.09389486908912659,
      "learning_rate": 3.2216666666666666e-05,
      "loss": 0.0032,
      "step": 53350
    },
    {
      "epoch": 2.8458666666666668,
      "grad_norm": 0.07526673376560211,
      "learning_rate": 3.221333333333333e-05,
      "loss": 0.0032,
      "step": 53360
    },
    {
      "epoch": 2.8464,
      "grad_norm": 0.5115230083465576,
      "learning_rate": 3.221e-05,
      "loss": 0.003,
      "step": 53370
    },
    {
      "epoch": 2.8469333333333333,
      "grad_norm": 0.17300955951213837,
      "learning_rate": 3.220666666666667e-05,
      "loss": 0.0019,
      "step": 53380
    },
    {
      "epoch": 2.8474666666666666,
      "grad_norm": 0.2593348026275635,
      "learning_rate": 3.220333333333334e-05,
      "loss": 0.0031,
      "step": 53390
    },
    {
      "epoch": 2.848,
      "grad_norm": 0.037518080323934555,
      "learning_rate": 3.2200000000000003e-05,
      "loss": 0.0038,
      "step": 53400
    },
    {
      "epoch": 2.848533333333333,
      "grad_norm": 0.0328005887567997,
      "learning_rate": 3.219666666666667e-05,
      "loss": 0.0029,
      "step": 53410
    },
    {
      "epoch": 2.8490666666666664,
      "grad_norm": 0.05841423571109772,
      "learning_rate": 3.2193333333333336e-05,
      "loss": 0.004,
      "step": 53420
    },
    {
      "epoch": 2.8496,
      "grad_norm": 0.23701870441436768,
      "learning_rate": 3.219e-05,
      "loss": 0.0029,
      "step": 53430
    },
    {
      "epoch": 2.8501333333333334,
      "grad_norm": 0.14385968446731567,
      "learning_rate": 3.218666666666667e-05,
      "loss": 0.0024,
      "step": 53440
    },
    {
      "epoch": 2.8506666666666667,
      "grad_norm": 0.14143818616867065,
      "learning_rate": 3.218333333333334e-05,
      "loss": 0.0026,
      "step": 53450
    },
    {
      "epoch": 2.8512,
      "grad_norm": 0.1475154012441635,
      "learning_rate": 3.218e-05,
      "loss": 0.0019,
      "step": 53460
    },
    {
      "epoch": 2.8517333333333332,
      "grad_norm": 0.5250077247619629,
      "learning_rate": 3.2176666666666666e-05,
      "loss": 0.0025,
      "step": 53470
    },
    {
      "epoch": 2.8522666666666665,
      "grad_norm": 0.4456084966659546,
      "learning_rate": 3.217333333333333e-05,
      "loss": 0.0021,
      "step": 53480
    },
    {
      "epoch": 2.8528000000000002,
      "grad_norm": 0.017108576372265816,
      "learning_rate": 3.217e-05,
      "loss": 0.0017,
      "step": 53490
    },
    {
      "epoch": 2.8533333333333335,
      "grad_norm": 0.1521754413843155,
      "learning_rate": 3.2166666666666665e-05,
      "loss": 0.0019,
      "step": 53500
    },
    {
      "epoch": 2.8538666666666668,
      "grad_norm": 0.2589741051197052,
      "learning_rate": 3.216333333333333e-05,
      "loss": 0.0025,
      "step": 53510
    },
    {
      "epoch": 2.8544,
      "grad_norm": 0.20443879067897797,
      "learning_rate": 3.2160000000000004e-05,
      "loss": 0.0027,
      "step": 53520
    },
    {
      "epoch": 2.8549333333333333,
      "grad_norm": 0.06426358968019485,
      "learning_rate": 3.215666666666667e-05,
      "loss": 0.003,
      "step": 53530
    },
    {
      "epoch": 2.8554666666666666,
      "grad_norm": 0.2603349983692169,
      "learning_rate": 3.2153333333333336e-05,
      "loss": 0.0019,
      "step": 53540
    },
    {
      "epoch": 2.856,
      "grad_norm": 0.23541666567325592,
      "learning_rate": 3.215e-05,
      "loss": 0.0017,
      "step": 53550
    },
    {
      "epoch": 2.856533333333333,
      "grad_norm": 0.40698352456092834,
      "learning_rate": 3.214666666666667e-05,
      "loss": 0.0026,
      "step": 53560
    },
    {
      "epoch": 2.8570666666666664,
      "grad_norm": 0.5646939277648926,
      "learning_rate": 3.2143333333333334e-05,
      "loss": 0.0035,
      "step": 53570
    },
    {
      "epoch": 2.8576,
      "grad_norm": 0.1745316982269287,
      "learning_rate": 3.214e-05,
      "loss": 0.0031,
      "step": 53580
    },
    {
      "epoch": 2.8581333333333334,
      "grad_norm": 0.6752705574035645,
      "learning_rate": 3.2136666666666674e-05,
      "loss": 0.0029,
      "step": 53590
    },
    {
      "epoch": 2.8586666666666667,
      "grad_norm": 0.5030519962310791,
      "learning_rate": 3.213333333333334e-05,
      "loss": 0.0025,
      "step": 53600
    },
    {
      "epoch": 2.8592,
      "grad_norm": 0.2897228002548218,
      "learning_rate": 3.213e-05,
      "loss": 0.0027,
      "step": 53610
    },
    {
      "epoch": 2.8597333333333332,
      "grad_norm": 0.5369930863380432,
      "learning_rate": 3.2126666666666665e-05,
      "loss": 0.0029,
      "step": 53620
    },
    {
      "epoch": 2.8602666666666665,
      "grad_norm": 0.03123653121292591,
      "learning_rate": 3.212333333333333e-05,
      "loss": 0.0025,
      "step": 53630
    },
    {
      "epoch": 2.8608000000000002,
      "grad_norm": 0.06653312593698502,
      "learning_rate": 3.212e-05,
      "loss": 0.0027,
      "step": 53640
    },
    {
      "epoch": 2.8613333333333335,
      "grad_norm": 0.2414596825838089,
      "learning_rate": 3.211666666666667e-05,
      "loss": 0.002,
      "step": 53650
    },
    {
      "epoch": 2.861866666666667,
      "grad_norm": 0.4423794746398926,
      "learning_rate": 3.2113333333333336e-05,
      "loss": 0.0025,
      "step": 53660
    },
    {
      "epoch": 2.8624,
      "grad_norm": 0.06437399238348007,
      "learning_rate": 3.211e-05,
      "loss": 0.0032,
      "step": 53670
    },
    {
      "epoch": 2.8629333333333333,
      "grad_norm": 0.41778233647346497,
      "learning_rate": 3.210666666666667e-05,
      "loss": 0.0032,
      "step": 53680
    },
    {
      "epoch": 2.8634666666666666,
      "grad_norm": 0.050441939383745193,
      "learning_rate": 3.2103333333333335e-05,
      "loss": 0.003,
      "step": 53690
    },
    {
      "epoch": 2.864,
      "grad_norm": 0.38492637872695923,
      "learning_rate": 3.21e-05,
      "loss": 0.0029,
      "step": 53700
    },
    {
      "epoch": 2.864533333333333,
      "grad_norm": 0.32043129205703735,
      "learning_rate": 3.209666666666667e-05,
      "loss": 0.0024,
      "step": 53710
    },
    {
      "epoch": 2.8650666666666664,
      "grad_norm": 0.23569126427173615,
      "learning_rate": 3.209333333333333e-05,
      "loss": 0.0037,
      "step": 53720
    },
    {
      "epoch": 2.8656,
      "grad_norm": 0.05987130105495453,
      "learning_rate": 3.2090000000000006e-05,
      "loss": 0.0031,
      "step": 53730
    },
    {
      "epoch": 2.8661333333333334,
      "grad_norm": 0.11632704734802246,
      "learning_rate": 3.208666666666667e-05,
      "loss": 0.0037,
      "step": 53740
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 0.5496386289596558,
      "learning_rate": 3.208333333333334e-05,
      "loss": 0.0024,
      "step": 53750
    },
    {
      "epoch": 2.8672,
      "grad_norm": 0.3457570970058441,
      "learning_rate": 3.208e-05,
      "loss": 0.0034,
      "step": 53760
    },
    {
      "epoch": 2.8677333333333332,
      "grad_norm": 0.014960463158786297,
      "learning_rate": 3.2076666666666664e-05,
      "loss": 0.0029,
      "step": 53770
    },
    {
      "epoch": 2.8682666666666665,
      "grad_norm": 0.12334704399108887,
      "learning_rate": 3.207333333333333e-05,
      "loss": 0.0025,
      "step": 53780
    },
    {
      "epoch": 2.8688000000000002,
      "grad_norm": 0.17242607474327087,
      "learning_rate": 3.207e-05,
      "loss": 0.0033,
      "step": 53790
    },
    {
      "epoch": 2.8693333333333335,
      "grad_norm": 0.06334178149700165,
      "learning_rate": 3.206666666666667e-05,
      "loss": 0.0031,
      "step": 53800
    },
    {
      "epoch": 2.869866666666667,
      "grad_norm": 0.17464447021484375,
      "learning_rate": 3.2063333333333335e-05,
      "loss": 0.0023,
      "step": 53810
    },
    {
      "epoch": 2.8704,
      "grad_norm": 0.6700944304466248,
      "learning_rate": 3.206e-05,
      "loss": 0.0027,
      "step": 53820
    },
    {
      "epoch": 2.8709333333333333,
      "grad_norm": 0.1782275289297104,
      "learning_rate": 3.205666666666667e-05,
      "loss": 0.0016,
      "step": 53830
    },
    {
      "epoch": 2.8714666666666666,
      "grad_norm": 0.0926201120018959,
      "learning_rate": 3.2053333333333334e-05,
      "loss": 0.0029,
      "step": 53840
    },
    {
      "epoch": 2.872,
      "grad_norm": 0.027580218389630318,
      "learning_rate": 3.205e-05,
      "loss": 0.0033,
      "step": 53850
    },
    {
      "epoch": 2.872533333333333,
      "grad_norm": 0.1749035269021988,
      "learning_rate": 3.2046666666666666e-05,
      "loss": 0.0022,
      "step": 53860
    },
    {
      "epoch": 2.8730666666666664,
      "grad_norm": 0.08953982591629028,
      "learning_rate": 3.204333333333334e-05,
      "loss": 0.0037,
      "step": 53870
    },
    {
      "epoch": 2.8736,
      "grad_norm": 0.32731324434280396,
      "learning_rate": 3.2040000000000005e-05,
      "loss": 0.0024,
      "step": 53880
    },
    {
      "epoch": 2.8741333333333334,
      "grad_norm": 0.26404625177383423,
      "learning_rate": 3.203666666666667e-05,
      "loss": 0.0032,
      "step": 53890
    },
    {
      "epoch": 2.8746666666666667,
      "grad_norm": 0.6427637934684753,
      "learning_rate": 3.203333333333334e-05,
      "loss": 0.0024,
      "step": 53900
    },
    {
      "epoch": 2.8752,
      "grad_norm": 0.3882273733615875,
      "learning_rate": 3.2029999999999997e-05,
      "loss": 0.0032,
      "step": 53910
    },
    {
      "epoch": 2.8757333333333333,
      "grad_norm": 0.23330995440483093,
      "learning_rate": 3.202666666666666e-05,
      "loss": 0.0025,
      "step": 53920
    },
    {
      "epoch": 2.8762666666666665,
      "grad_norm": 0.20830784738063812,
      "learning_rate": 3.2023333333333336e-05,
      "loss": 0.0028,
      "step": 53930
    },
    {
      "epoch": 2.8768000000000002,
      "grad_norm": 0.43405282497406006,
      "learning_rate": 3.202e-05,
      "loss": 0.0026,
      "step": 53940
    },
    {
      "epoch": 2.8773333333333335,
      "grad_norm": 0.27405697107315063,
      "learning_rate": 3.201666666666667e-05,
      "loss": 0.0041,
      "step": 53950
    },
    {
      "epoch": 2.877866666666667,
      "grad_norm": 0.0365200936794281,
      "learning_rate": 3.2013333333333334e-05,
      "loss": 0.0028,
      "step": 53960
    },
    {
      "epoch": 2.8784,
      "grad_norm": 0.6112534999847412,
      "learning_rate": 3.201e-05,
      "loss": 0.0031,
      "step": 53970
    },
    {
      "epoch": 2.8789333333333333,
      "grad_norm": 0.08881436288356781,
      "learning_rate": 3.2006666666666666e-05,
      "loss": 0.0019,
      "step": 53980
    },
    {
      "epoch": 2.8794666666666666,
      "grad_norm": 0.39167723059654236,
      "learning_rate": 3.200333333333333e-05,
      "loss": 0.0029,
      "step": 53990
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.17625150084495544,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.0022,
      "step": 54000
    },
    {
      "epoch": 2.880533333333333,
      "grad_norm": 0.12106283009052277,
      "learning_rate": 3.199666666666667e-05,
      "loss": 0.0027,
      "step": 54010
    },
    {
      "epoch": 2.8810666666666664,
      "grad_norm": 0.06269372999668121,
      "learning_rate": 3.199333333333334e-05,
      "loss": 0.004,
      "step": 54020
    },
    {
      "epoch": 2.8816,
      "grad_norm": 0.716856062412262,
      "learning_rate": 3.1990000000000004e-05,
      "loss": 0.0024,
      "step": 54030
    },
    {
      "epoch": 2.8821333333333334,
      "grad_norm": 0.07515643537044525,
      "learning_rate": 3.198666666666667e-05,
      "loss": 0.0035,
      "step": 54040
    },
    {
      "epoch": 2.8826666666666667,
      "grad_norm": 0.1754855215549469,
      "learning_rate": 3.1983333333333336e-05,
      "loss": 0.0027,
      "step": 54050
    },
    {
      "epoch": 2.8832,
      "grad_norm": 0.15225166082382202,
      "learning_rate": 3.198e-05,
      "loss": 0.0056,
      "step": 54060
    },
    {
      "epoch": 2.8837333333333333,
      "grad_norm": 0.3488631546497345,
      "learning_rate": 3.197666666666667e-05,
      "loss": 0.002,
      "step": 54070
    },
    {
      "epoch": 2.8842666666666665,
      "grad_norm": 0.3569011092185974,
      "learning_rate": 3.1973333333333334e-05,
      "loss": 0.0021,
      "step": 54080
    },
    {
      "epoch": 2.8848000000000003,
      "grad_norm": 0.24019268155097961,
      "learning_rate": 3.197e-05,
      "loss": 0.003,
      "step": 54090
    },
    {
      "epoch": 2.8853333333333335,
      "grad_norm": 0.09659971296787262,
      "learning_rate": 3.196666666666667e-05,
      "loss": 0.0027,
      "step": 54100
    },
    {
      "epoch": 2.885866666666667,
      "grad_norm": 0.37918007373809814,
      "learning_rate": 3.196333333333333e-05,
      "loss": 0.0027,
      "step": 54110
    },
    {
      "epoch": 2.8864,
      "grad_norm": 0.29103216528892517,
      "learning_rate": 3.196e-05,
      "loss": 0.0019,
      "step": 54120
    },
    {
      "epoch": 2.8869333333333334,
      "grad_norm": 0.06679432839155197,
      "learning_rate": 3.1956666666666665e-05,
      "loss": 0.0017,
      "step": 54130
    },
    {
      "epoch": 2.8874666666666666,
      "grad_norm": 0.07060034573078156,
      "learning_rate": 3.195333333333334e-05,
      "loss": 0.0036,
      "step": 54140
    },
    {
      "epoch": 2.888,
      "grad_norm": 0.3481341600418091,
      "learning_rate": 3.1950000000000004e-05,
      "loss": 0.002,
      "step": 54150
    },
    {
      "epoch": 2.888533333333333,
      "grad_norm": 0.04770338162779808,
      "learning_rate": 3.194666666666667e-05,
      "loss": 0.0023,
      "step": 54160
    },
    {
      "epoch": 2.8890666666666664,
      "grad_norm": 0.14724202454090118,
      "learning_rate": 3.1943333333333336e-05,
      "loss": 0.0022,
      "step": 54170
    },
    {
      "epoch": 2.8895999999999997,
      "grad_norm": 0.18412326276302338,
      "learning_rate": 3.194e-05,
      "loss": 0.0014,
      "step": 54180
    },
    {
      "epoch": 2.8901333333333334,
      "grad_norm": 0.4043790102005005,
      "learning_rate": 3.193666666666667e-05,
      "loss": 0.0026,
      "step": 54190
    },
    {
      "epoch": 2.8906666666666667,
      "grad_norm": 0.2629602551460266,
      "learning_rate": 3.1933333333333335e-05,
      "loss": 0.003,
      "step": 54200
    },
    {
      "epoch": 2.8912,
      "grad_norm": 0.14918243885040283,
      "learning_rate": 3.193e-05,
      "loss": 0.0025,
      "step": 54210
    },
    {
      "epoch": 2.8917333333333333,
      "grad_norm": 0.15317891538143158,
      "learning_rate": 3.192666666666667e-05,
      "loss": 0.002,
      "step": 54220
    },
    {
      "epoch": 2.8922666666666665,
      "grad_norm": 0.0643264651298523,
      "learning_rate": 3.192333333333333e-05,
      "loss": 0.002,
      "step": 54230
    },
    {
      "epoch": 2.8928000000000003,
      "grad_norm": 0.12036378681659698,
      "learning_rate": 3.192e-05,
      "loss": 0.0036,
      "step": 54240
    },
    {
      "epoch": 2.8933333333333335,
      "grad_norm": 0.1756727248430252,
      "learning_rate": 3.1916666666666665e-05,
      "loss": 0.0017,
      "step": 54250
    },
    {
      "epoch": 2.893866666666667,
      "grad_norm": 0.20904025435447693,
      "learning_rate": 3.191333333333333e-05,
      "loss": 0.0028,
      "step": 54260
    },
    {
      "epoch": 2.8944,
      "grad_norm": 0.14911003410816193,
      "learning_rate": 3.191e-05,
      "loss": 0.0021,
      "step": 54270
    },
    {
      "epoch": 2.8949333333333334,
      "grad_norm": 0.46490025520324707,
      "learning_rate": 3.190666666666667e-05,
      "loss": 0.0021,
      "step": 54280
    },
    {
      "epoch": 2.8954666666666666,
      "grad_norm": 0.28262555599212646,
      "learning_rate": 3.190333333333334e-05,
      "loss": 0.0043,
      "step": 54290
    },
    {
      "epoch": 2.896,
      "grad_norm": 0.5841425061225891,
      "learning_rate": 3.19e-05,
      "loss": 0.0024,
      "step": 54300
    },
    {
      "epoch": 2.896533333333333,
      "grad_norm": 0.14442230761051178,
      "learning_rate": 3.189666666666667e-05,
      "loss": 0.0023,
      "step": 54310
    },
    {
      "epoch": 2.8970666666666665,
      "grad_norm": 0.0574880950152874,
      "learning_rate": 3.1893333333333335e-05,
      "loss": 0.002,
      "step": 54320
    },
    {
      "epoch": 2.8975999999999997,
      "grad_norm": 0.2873764932155609,
      "learning_rate": 3.189e-05,
      "loss": 0.0037,
      "step": 54330
    },
    {
      "epoch": 2.8981333333333335,
      "grad_norm": 0.40417996048927307,
      "learning_rate": 3.188666666666667e-05,
      "loss": 0.0029,
      "step": 54340
    },
    {
      "epoch": 2.8986666666666667,
      "grad_norm": 0.6746197938919067,
      "learning_rate": 3.188333333333334e-05,
      "loss": 0.0027,
      "step": 54350
    },
    {
      "epoch": 2.8992,
      "grad_norm": 0.4109024107456207,
      "learning_rate": 3.188e-05,
      "loss": 0.0023,
      "step": 54360
    },
    {
      "epoch": 2.8997333333333333,
      "grad_norm": 0.5174986124038696,
      "learning_rate": 3.1876666666666666e-05,
      "loss": 0.0026,
      "step": 54370
    },
    {
      "epoch": 2.9002666666666665,
      "grad_norm": 0.32055073976516724,
      "learning_rate": 3.187333333333333e-05,
      "loss": 0.0025,
      "step": 54380
    },
    {
      "epoch": 2.9008000000000003,
      "grad_norm": 0.09162314981222153,
      "learning_rate": 3.187e-05,
      "loss": 0.0023,
      "step": 54390
    },
    {
      "epoch": 2.9013333333333335,
      "grad_norm": 0.2622946798801422,
      "learning_rate": 3.1866666666666664e-05,
      "loss": 0.0033,
      "step": 54400
    },
    {
      "epoch": 2.901866666666667,
      "grad_norm": 0.22958046197891235,
      "learning_rate": 3.186333333333334e-05,
      "loss": 0.0035,
      "step": 54410
    },
    {
      "epoch": 2.9024,
      "grad_norm": 0.12703804671764374,
      "learning_rate": 3.186e-05,
      "loss": 0.0035,
      "step": 54420
    },
    {
      "epoch": 2.9029333333333334,
      "grad_norm": 0.05035321041941643,
      "learning_rate": 3.185666666666667e-05,
      "loss": 0.0027,
      "step": 54430
    },
    {
      "epoch": 2.9034666666666666,
      "grad_norm": 0.5161444544792175,
      "learning_rate": 3.1853333333333336e-05,
      "loss": 0.0024,
      "step": 54440
    },
    {
      "epoch": 2.904,
      "grad_norm": 0.11674781888723373,
      "learning_rate": 3.185e-05,
      "loss": 0.0024,
      "step": 54450
    },
    {
      "epoch": 2.904533333333333,
      "grad_norm": 0.25986894965171814,
      "learning_rate": 3.184666666666667e-05,
      "loss": 0.0028,
      "step": 54460
    },
    {
      "epoch": 2.9050666666666665,
      "grad_norm": 0.03866739571094513,
      "learning_rate": 3.1843333333333334e-05,
      "loss": 0.0023,
      "step": 54470
    },
    {
      "epoch": 2.9055999999999997,
      "grad_norm": 0.23588792979717255,
      "learning_rate": 3.184e-05,
      "loss": 0.0033,
      "step": 54480
    },
    {
      "epoch": 2.9061333333333335,
      "grad_norm": 0.3314912021160126,
      "learning_rate": 3.183666666666667e-05,
      "loss": 0.0029,
      "step": 54490
    },
    {
      "epoch": 2.9066666666666667,
      "grad_norm": 0.3787277936935425,
      "learning_rate": 3.183333333333334e-05,
      "loss": 0.0022,
      "step": 54500
    },
    {
      "epoch": 2.9072,
      "grad_norm": 0.40972891449928284,
      "learning_rate": 3.1830000000000005e-05,
      "loss": 0.0027,
      "step": 54510
    },
    {
      "epoch": 2.9077333333333333,
      "grad_norm": 0.2669330835342407,
      "learning_rate": 3.1826666666666665e-05,
      "loss": 0.0022,
      "step": 54520
    },
    {
      "epoch": 2.9082666666666666,
      "grad_norm": 1.0242929458618164,
      "learning_rate": 3.182333333333333e-05,
      "loss": 0.0034,
      "step": 54530
    },
    {
      "epoch": 2.9088000000000003,
      "grad_norm": 0.06552653014659882,
      "learning_rate": 3.182e-05,
      "loss": 0.0031,
      "step": 54540
    },
    {
      "epoch": 2.9093333333333335,
      "grad_norm": 0.03570760786533356,
      "learning_rate": 3.181666666666667e-05,
      "loss": 0.003,
      "step": 54550
    },
    {
      "epoch": 2.909866666666667,
      "grad_norm": 0.2898108959197998,
      "learning_rate": 3.1813333333333336e-05,
      "loss": 0.0017,
      "step": 54560
    },
    {
      "epoch": 2.9104,
      "grad_norm": 0.1481744796037674,
      "learning_rate": 3.181e-05,
      "loss": 0.0022,
      "step": 54570
    },
    {
      "epoch": 2.9109333333333334,
      "grad_norm": 0.2336658239364624,
      "learning_rate": 3.180666666666667e-05,
      "loss": 0.0018,
      "step": 54580
    },
    {
      "epoch": 2.9114666666666666,
      "grad_norm": 0.09237313270568848,
      "learning_rate": 3.1803333333333334e-05,
      "loss": 0.002,
      "step": 54590
    },
    {
      "epoch": 2.912,
      "grad_norm": 0.4918762445449829,
      "learning_rate": 3.18e-05,
      "loss": 0.0019,
      "step": 54600
    },
    {
      "epoch": 2.912533333333333,
      "grad_norm": 0.20612303912639618,
      "learning_rate": 3.1796666666666667e-05,
      "loss": 0.0024,
      "step": 54610
    },
    {
      "epoch": 2.9130666666666665,
      "grad_norm": 0.2929646968841553,
      "learning_rate": 3.179333333333333e-05,
      "loss": 0.0025,
      "step": 54620
    },
    {
      "epoch": 2.9135999999999997,
      "grad_norm": 0.05021435394883156,
      "learning_rate": 3.1790000000000006e-05,
      "loss": 0.0025,
      "step": 54630
    },
    {
      "epoch": 2.9141333333333335,
      "grad_norm": 0.11770350486040115,
      "learning_rate": 3.178666666666667e-05,
      "loss": 0.0028,
      "step": 54640
    },
    {
      "epoch": 2.9146666666666667,
      "grad_norm": 0.05970284342765808,
      "learning_rate": 3.178333333333334e-05,
      "loss": 0.0031,
      "step": 54650
    },
    {
      "epoch": 2.9152,
      "grad_norm": 0.583850085735321,
      "learning_rate": 3.1780000000000004e-05,
      "loss": 0.0027,
      "step": 54660
    },
    {
      "epoch": 2.9157333333333333,
      "grad_norm": 0.0493161603808403,
      "learning_rate": 3.1776666666666663e-05,
      "loss": 0.002,
      "step": 54670
    },
    {
      "epoch": 2.9162666666666666,
      "grad_norm": 0.7523771524429321,
      "learning_rate": 3.177333333333333e-05,
      "loss": 0.0028,
      "step": 54680
    },
    {
      "epoch": 2.9168,
      "grad_norm": 0.0828387439250946,
      "learning_rate": 3.177e-05,
      "loss": 0.002,
      "step": 54690
    },
    {
      "epoch": 2.9173333333333336,
      "grad_norm": 0.05827508494257927,
      "learning_rate": 3.176666666666667e-05,
      "loss": 0.0029,
      "step": 54700
    },
    {
      "epoch": 2.917866666666667,
      "grad_norm": 0.07150612771511078,
      "learning_rate": 3.1763333333333335e-05,
      "loss": 0.0032,
      "step": 54710
    },
    {
      "epoch": 2.9184,
      "grad_norm": 0.448280930519104,
      "learning_rate": 3.176e-05,
      "loss": 0.003,
      "step": 54720
    },
    {
      "epoch": 2.9189333333333334,
      "grad_norm": 0.20332270860671997,
      "learning_rate": 3.175666666666667e-05,
      "loss": 0.0025,
      "step": 54730
    },
    {
      "epoch": 2.9194666666666667,
      "grad_norm": 0.21913331747055054,
      "learning_rate": 3.175333333333333e-05,
      "loss": 0.0026,
      "step": 54740
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.49289554357528687,
      "learning_rate": 3.175e-05,
      "loss": 0.0028,
      "step": 54750
    },
    {
      "epoch": 2.920533333333333,
      "grad_norm": 0.11669479310512543,
      "learning_rate": 3.174666666666667e-05,
      "loss": 0.0024,
      "step": 54760
    },
    {
      "epoch": 2.9210666666666665,
      "grad_norm": 0.3214980363845825,
      "learning_rate": 3.174333333333334e-05,
      "loss": 0.0022,
      "step": 54770
    },
    {
      "epoch": 2.9215999999999998,
      "grad_norm": 0.03865297511219978,
      "learning_rate": 3.1740000000000004e-05,
      "loss": 0.0032,
      "step": 54780
    },
    {
      "epoch": 2.9221333333333335,
      "grad_norm": 0.2199995070695877,
      "learning_rate": 3.173666666666667e-05,
      "loss": 0.0026,
      "step": 54790
    },
    {
      "epoch": 2.9226666666666667,
      "grad_norm": 0.46539106965065,
      "learning_rate": 3.173333333333334e-05,
      "loss": 0.0029,
      "step": 54800
    },
    {
      "epoch": 2.9232,
      "grad_norm": 0.43242254853248596,
      "learning_rate": 3.173e-05,
      "loss": 0.0023,
      "step": 54810
    },
    {
      "epoch": 2.9237333333333333,
      "grad_norm": 0.08135928213596344,
      "learning_rate": 3.172666666666667e-05,
      "loss": 0.0022,
      "step": 54820
    },
    {
      "epoch": 2.9242666666666666,
      "grad_norm": 0.44674018025398254,
      "learning_rate": 3.1723333333333335e-05,
      "loss": 0.0021,
      "step": 54830
    },
    {
      "epoch": 2.9248,
      "grad_norm": 0.4626074433326721,
      "learning_rate": 3.172e-05,
      "loss": 0.0021,
      "step": 54840
    },
    {
      "epoch": 2.9253333333333336,
      "grad_norm": 0.09382066130638123,
      "learning_rate": 3.171666666666667e-05,
      "loss": 0.0025,
      "step": 54850
    },
    {
      "epoch": 2.925866666666667,
      "grad_norm": 0.4500894546508789,
      "learning_rate": 3.1713333333333334e-05,
      "loss": 0.0033,
      "step": 54860
    },
    {
      "epoch": 2.9264,
      "grad_norm": 0.2211771160364151,
      "learning_rate": 3.171e-05,
      "loss": 0.0022,
      "step": 54870
    },
    {
      "epoch": 2.9269333333333334,
      "grad_norm": 0.05008702725172043,
      "learning_rate": 3.1706666666666666e-05,
      "loss": 0.0022,
      "step": 54880
    },
    {
      "epoch": 2.9274666666666667,
      "grad_norm": 0.14798016846179962,
      "learning_rate": 3.170333333333333e-05,
      "loss": 0.0022,
      "step": 54890
    },
    {
      "epoch": 2.928,
      "grad_norm": 0.4189710021018982,
      "learning_rate": 3.1700000000000005e-05,
      "loss": 0.0019,
      "step": 54900
    },
    {
      "epoch": 2.928533333333333,
      "grad_norm": 0.08982421457767487,
      "learning_rate": 3.169666666666667e-05,
      "loss": 0.002,
      "step": 54910
    },
    {
      "epoch": 2.9290666666666665,
      "grad_norm": 0.11952425539493561,
      "learning_rate": 3.169333333333334e-05,
      "loss": 0.0023,
      "step": 54920
    },
    {
      "epoch": 2.9295999999999998,
      "grad_norm": 0.05903158336877823,
      "learning_rate": 3.169e-05,
      "loss": 0.0033,
      "step": 54930
    },
    {
      "epoch": 2.9301333333333335,
      "grad_norm": 0.21120814979076385,
      "learning_rate": 3.168666666666667e-05,
      "loss": 0.0027,
      "step": 54940
    },
    {
      "epoch": 2.9306666666666668,
      "grad_norm": 0.2595784664154053,
      "learning_rate": 3.1683333333333335e-05,
      "loss": 0.002,
      "step": 54950
    },
    {
      "epoch": 2.9312,
      "grad_norm": 0.1176212951540947,
      "learning_rate": 3.168e-05,
      "loss": 0.0025,
      "step": 54960
    },
    {
      "epoch": 2.9317333333333333,
      "grad_norm": 0.2388066053390503,
      "learning_rate": 3.167666666666667e-05,
      "loss": 0.0023,
      "step": 54970
    },
    {
      "epoch": 2.9322666666666666,
      "grad_norm": 0.30073869228363037,
      "learning_rate": 3.1673333333333334e-05,
      "loss": 0.0031,
      "step": 54980
    },
    {
      "epoch": 2.9328,
      "grad_norm": 0.5907193422317505,
      "learning_rate": 3.167e-05,
      "loss": 0.002,
      "step": 54990
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 0.34973007440567017,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 0.0029,
      "step": 55000
    },
    {
      "epoch": 2.933866666666667,
      "grad_norm": 0.2326231747865677,
      "learning_rate": 3.166333333333333e-05,
      "loss": 0.0017,
      "step": 55010
    },
    {
      "epoch": 2.9344,
      "grad_norm": 0.095952607691288,
      "learning_rate": 3.166e-05,
      "loss": 0.0021,
      "step": 55020
    },
    {
      "epoch": 2.9349333333333334,
      "grad_norm": 0.21351519227027893,
      "learning_rate": 3.1656666666666665e-05,
      "loss": 0.0023,
      "step": 55030
    },
    {
      "epoch": 2.9354666666666667,
      "grad_norm": 0.4442494511604309,
      "learning_rate": 3.165333333333334e-05,
      "loss": 0.0028,
      "step": 55040
    },
    {
      "epoch": 2.936,
      "grad_norm": 0.248252272605896,
      "learning_rate": 3.1650000000000004e-05,
      "loss": 0.0042,
      "step": 55050
    },
    {
      "epoch": 2.936533333333333,
      "grad_norm": 0.08727256953716278,
      "learning_rate": 3.164666666666667e-05,
      "loss": 0.0032,
      "step": 55060
    },
    {
      "epoch": 2.9370666666666665,
      "grad_norm": 0.45020371675491333,
      "learning_rate": 3.1643333333333336e-05,
      "loss": 0.0017,
      "step": 55070
    },
    {
      "epoch": 2.9375999999999998,
      "grad_norm": 0.531529426574707,
      "learning_rate": 3.164e-05,
      "loss": 0.0022,
      "step": 55080
    },
    {
      "epoch": 2.9381333333333335,
      "grad_norm": 0.4675728678703308,
      "learning_rate": 3.163666666666667e-05,
      "loss": 0.0025,
      "step": 55090
    },
    {
      "epoch": 2.9386666666666668,
      "grad_norm": 0.3845561146736145,
      "learning_rate": 3.1633333333333334e-05,
      "loss": 0.0021,
      "step": 55100
    },
    {
      "epoch": 2.9392,
      "grad_norm": 0.6397465467453003,
      "learning_rate": 3.163000000000001e-05,
      "loss": 0.0029,
      "step": 55110
    },
    {
      "epoch": 2.9397333333333333,
      "grad_norm": 0.12029726058244705,
      "learning_rate": 3.1626666666666667e-05,
      "loss": 0.003,
      "step": 55120
    },
    {
      "epoch": 2.9402666666666666,
      "grad_norm": 0.057881396263837814,
      "learning_rate": 3.162333333333333e-05,
      "loss": 0.0022,
      "step": 55130
    },
    {
      "epoch": 2.9408,
      "grad_norm": 0.1461944431066513,
      "learning_rate": 3.162e-05,
      "loss": 0.0019,
      "step": 55140
    },
    {
      "epoch": 2.9413333333333336,
      "grad_norm": 0.14672963321208954,
      "learning_rate": 3.1616666666666665e-05,
      "loss": 0.0031,
      "step": 55150
    },
    {
      "epoch": 2.941866666666667,
      "grad_norm": 0.26654571294784546,
      "learning_rate": 3.161333333333333e-05,
      "loss": 0.0017,
      "step": 55160
    },
    {
      "epoch": 2.9424,
      "grad_norm": 0.11965328454971313,
      "learning_rate": 3.1610000000000004e-05,
      "loss": 0.0026,
      "step": 55170
    },
    {
      "epoch": 2.9429333333333334,
      "grad_norm": 0.038886021822690964,
      "learning_rate": 3.160666666666667e-05,
      "loss": 0.0022,
      "step": 55180
    },
    {
      "epoch": 2.9434666666666667,
      "grad_norm": 0.40913212299346924,
      "learning_rate": 3.1603333333333336e-05,
      "loss": 0.0019,
      "step": 55190
    },
    {
      "epoch": 2.944,
      "grad_norm": 0.027209356427192688,
      "learning_rate": 3.16e-05,
      "loss": 0.0016,
      "step": 55200
    },
    {
      "epoch": 2.9445333333333332,
      "grad_norm": 0.7015138864517212,
      "learning_rate": 3.159666666666667e-05,
      "loss": 0.0023,
      "step": 55210
    },
    {
      "epoch": 2.9450666666666665,
      "grad_norm": 0.23744280636310577,
      "learning_rate": 3.1593333333333335e-05,
      "loss": 0.0027,
      "step": 55220
    },
    {
      "epoch": 2.9455999999999998,
      "grad_norm": 0.06079038605093956,
      "learning_rate": 3.159e-05,
      "loss": 0.0023,
      "step": 55230
    },
    {
      "epoch": 2.9461333333333335,
      "grad_norm": 0.2067207396030426,
      "learning_rate": 3.158666666666667e-05,
      "loss": 0.0044,
      "step": 55240
    },
    {
      "epoch": 2.9466666666666668,
      "grad_norm": 0.3979509472846985,
      "learning_rate": 3.158333333333334e-05,
      "loss": 0.0039,
      "step": 55250
    },
    {
      "epoch": 2.9472,
      "grad_norm": 0.37032607197761536,
      "learning_rate": 3.1580000000000006e-05,
      "loss": 0.0025,
      "step": 55260
    },
    {
      "epoch": 2.9477333333333333,
      "grad_norm": 0.2632761597633362,
      "learning_rate": 3.1576666666666665e-05,
      "loss": 0.0024,
      "step": 55270
    },
    {
      "epoch": 2.9482666666666666,
      "grad_norm": 0.3760862946510315,
      "learning_rate": 3.157333333333333e-05,
      "loss": 0.0019,
      "step": 55280
    },
    {
      "epoch": 2.9488,
      "grad_norm": 0.29162606596946716,
      "learning_rate": 3.157e-05,
      "loss": 0.0019,
      "step": 55290
    },
    {
      "epoch": 2.9493333333333336,
      "grad_norm": 0.17622435092926025,
      "learning_rate": 3.1566666666666664e-05,
      "loss": 0.0019,
      "step": 55300
    },
    {
      "epoch": 2.949866666666667,
      "grad_norm": 0.3017318546772003,
      "learning_rate": 3.156333333333334e-05,
      "loss": 0.002,
      "step": 55310
    },
    {
      "epoch": 2.9504,
      "grad_norm": 0.018539004027843475,
      "learning_rate": 3.156e-05,
      "loss": 0.0022,
      "step": 55320
    },
    {
      "epoch": 2.9509333333333334,
      "grad_norm": 0.09541892260313034,
      "learning_rate": 3.155666666666667e-05,
      "loss": 0.002,
      "step": 55330
    },
    {
      "epoch": 2.9514666666666667,
      "grad_norm": 0.5085245966911316,
      "learning_rate": 3.1553333333333335e-05,
      "loss": 0.0023,
      "step": 55340
    },
    {
      "epoch": 2.952,
      "grad_norm": 0.2634519338607788,
      "learning_rate": 3.155e-05,
      "loss": 0.003,
      "step": 55350
    },
    {
      "epoch": 2.9525333333333332,
      "grad_norm": 0.35355573892593384,
      "learning_rate": 3.154666666666667e-05,
      "loss": 0.0021,
      "step": 55360
    },
    {
      "epoch": 2.9530666666666665,
      "grad_norm": 0.04462627321481705,
      "learning_rate": 3.1543333333333333e-05,
      "loss": 0.0018,
      "step": 55370
    },
    {
      "epoch": 2.9536,
      "grad_norm": 0.01560194417834282,
      "learning_rate": 3.154e-05,
      "loss": 0.0024,
      "step": 55380
    },
    {
      "epoch": 2.9541333333333335,
      "grad_norm": 0.14561574161052704,
      "learning_rate": 3.153666666666667e-05,
      "loss": 0.0032,
      "step": 55390
    },
    {
      "epoch": 2.9546666666666668,
      "grad_norm": 0.508173942565918,
      "learning_rate": 3.153333333333334e-05,
      "loss": 0.0032,
      "step": 55400
    },
    {
      "epoch": 2.9552,
      "grad_norm": 0.06426803767681122,
      "learning_rate": 3.1530000000000005e-05,
      "loss": 0.0031,
      "step": 55410
    },
    {
      "epoch": 2.9557333333333333,
      "grad_norm": 0.29375794529914856,
      "learning_rate": 3.1526666666666664e-05,
      "loss": 0.0022,
      "step": 55420
    },
    {
      "epoch": 2.9562666666666666,
      "grad_norm": 0.11865419149398804,
      "learning_rate": 3.152333333333333e-05,
      "loss": 0.0022,
      "step": 55430
    },
    {
      "epoch": 2.9568,
      "grad_norm": 0.03841705992817879,
      "learning_rate": 3.1519999999999996e-05,
      "loss": 0.0016,
      "step": 55440
    },
    {
      "epoch": 2.9573333333333336,
      "grad_norm": 0.34787705540657043,
      "learning_rate": 3.151666666666667e-05,
      "loss": 0.0038,
      "step": 55450
    },
    {
      "epoch": 2.957866666666667,
      "grad_norm": 0.4703929126262665,
      "learning_rate": 3.1513333333333335e-05,
      "loss": 0.0029,
      "step": 55460
    },
    {
      "epoch": 2.9584,
      "grad_norm": 0.26299065351486206,
      "learning_rate": 3.151e-05,
      "loss": 0.0028,
      "step": 55470
    },
    {
      "epoch": 2.9589333333333334,
      "grad_norm": 0.07110589742660522,
      "learning_rate": 3.150666666666667e-05,
      "loss": 0.0016,
      "step": 55480
    },
    {
      "epoch": 2.9594666666666667,
      "grad_norm": 0.062441613525152206,
      "learning_rate": 3.1503333333333334e-05,
      "loss": 0.0028,
      "step": 55490
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.060102175921201706,
      "learning_rate": 3.15e-05,
      "loss": 0.0028,
      "step": 55500
    },
    {
      "epoch": 2.9605333333333332,
      "grad_norm": 0.2619991898536682,
      "learning_rate": 3.1496666666666666e-05,
      "loss": 0.0018,
      "step": 55510
    },
    {
      "epoch": 2.9610666666666665,
      "grad_norm": 0.5312532186508179,
      "learning_rate": 3.149333333333334e-05,
      "loss": 0.003,
      "step": 55520
    },
    {
      "epoch": 2.9616,
      "grad_norm": 0.04113967716693878,
      "learning_rate": 3.1490000000000005e-05,
      "loss": 0.0027,
      "step": 55530
    },
    {
      "epoch": 2.962133333333333,
      "grad_norm": 0.5288975238800049,
      "learning_rate": 3.148666666666667e-05,
      "loss": 0.0025,
      "step": 55540
    },
    {
      "epoch": 2.962666666666667,
      "grad_norm": 0.2885981500148773,
      "learning_rate": 3.148333333333334e-05,
      "loss": 0.0031,
      "step": 55550
    },
    {
      "epoch": 2.9632,
      "grad_norm": 0.5234553217887878,
      "learning_rate": 3.1480000000000004e-05,
      "loss": 0.0025,
      "step": 55560
    },
    {
      "epoch": 2.9637333333333333,
      "grad_norm": 0.0360015444457531,
      "learning_rate": 3.147666666666666e-05,
      "loss": 0.0022,
      "step": 55570
    },
    {
      "epoch": 2.9642666666666666,
      "grad_norm": 0.18823853135108948,
      "learning_rate": 3.1473333333333336e-05,
      "loss": 0.0017,
      "step": 55580
    },
    {
      "epoch": 2.9648,
      "grad_norm": 0.2918749749660492,
      "learning_rate": 3.147e-05,
      "loss": 0.0027,
      "step": 55590
    },
    {
      "epoch": 2.9653333333333336,
      "grad_norm": 0.2010766863822937,
      "learning_rate": 3.146666666666667e-05,
      "loss": 0.0024,
      "step": 55600
    },
    {
      "epoch": 2.965866666666667,
      "grad_norm": 0.056756943464279175,
      "learning_rate": 3.1463333333333334e-05,
      "loss": 0.0023,
      "step": 55610
    },
    {
      "epoch": 2.9664,
      "grad_norm": 0.2877420485019684,
      "learning_rate": 3.146e-05,
      "loss": 0.0043,
      "step": 55620
    },
    {
      "epoch": 2.9669333333333334,
      "grad_norm": 0.11776591092348099,
      "learning_rate": 3.1456666666666666e-05,
      "loss": 0.0028,
      "step": 55630
    },
    {
      "epoch": 2.9674666666666667,
      "grad_norm": 0.2465103268623352,
      "learning_rate": 3.145333333333333e-05,
      "loss": 0.0028,
      "step": 55640
    },
    {
      "epoch": 2.968,
      "grad_norm": 0.4690989851951599,
      "learning_rate": 3.145e-05,
      "loss": 0.003,
      "step": 55650
    },
    {
      "epoch": 2.9685333333333332,
      "grad_norm": 0.13094104826450348,
      "learning_rate": 3.144666666666667e-05,
      "loss": 0.003,
      "step": 55660
    },
    {
      "epoch": 2.9690666666666665,
      "grad_norm": 0.4661712646484375,
      "learning_rate": 3.144333333333334e-05,
      "loss": 0.0024,
      "step": 55670
    },
    {
      "epoch": 2.9696,
      "grad_norm": 0.18982408940792084,
      "learning_rate": 3.1440000000000004e-05,
      "loss": 0.0029,
      "step": 55680
    },
    {
      "epoch": 2.970133333333333,
      "grad_norm": 0.20892837643623352,
      "learning_rate": 3.143666666666667e-05,
      "loss": 0.0022,
      "step": 55690
    },
    {
      "epoch": 2.970666666666667,
      "grad_norm": 0.5832498669624329,
      "learning_rate": 3.1433333333333336e-05,
      "loss": 0.002,
      "step": 55700
    },
    {
      "epoch": 2.9712,
      "grad_norm": 0.529748260974884,
      "learning_rate": 3.143e-05,
      "loss": 0.0027,
      "step": 55710
    },
    {
      "epoch": 2.9717333333333333,
      "grad_norm": 0.10168029367923737,
      "learning_rate": 3.142666666666667e-05,
      "loss": 0.0028,
      "step": 55720
    },
    {
      "epoch": 2.9722666666666666,
      "grad_norm": 0.2603551745414734,
      "learning_rate": 3.1423333333333335e-05,
      "loss": 0.0016,
      "step": 55730
    },
    {
      "epoch": 2.9728,
      "grad_norm": 0.3794284164905548,
      "learning_rate": 3.142e-05,
      "loss": 0.0023,
      "step": 55740
    },
    {
      "epoch": 2.9733333333333336,
      "grad_norm": 0.03335197642445564,
      "learning_rate": 3.141666666666667e-05,
      "loss": 0.0025,
      "step": 55750
    },
    {
      "epoch": 2.973866666666667,
      "grad_norm": 0.12850786745548248,
      "learning_rate": 3.141333333333333e-05,
      "loss": 0.0025,
      "step": 55760
    },
    {
      "epoch": 2.9744,
      "grad_norm": 0.4108189046382904,
      "learning_rate": 3.141e-05,
      "loss": 0.0015,
      "step": 55770
    },
    {
      "epoch": 2.9749333333333334,
      "grad_norm": 0.08793874084949493,
      "learning_rate": 3.1406666666666665e-05,
      "loss": 0.0027,
      "step": 55780
    },
    {
      "epoch": 2.9754666666666667,
      "grad_norm": 0.088982954621315,
      "learning_rate": 3.140333333333333e-05,
      "loss": 0.0023,
      "step": 55790
    },
    {
      "epoch": 2.976,
      "grad_norm": 0.4103364646434784,
      "learning_rate": 3.1400000000000004e-05,
      "loss": 0.0026,
      "step": 55800
    },
    {
      "epoch": 2.9765333333333333,
      "grad_norm": 0.0942399799823761,
      "learning_rate": 3.139666666666667e-05,
      "loss": 0.0022,
      "step": 55810
    },
    {
      "epoch": 2.9770666666666665,
      "grad_norm": 0.2623586654663086,
      "learning_rate": 3.1393333333333337e-05,
      "loss": 0.0017,
      "step": 55820
    },
    {
      "epoch": 2.9776,
      "grad_norm": 0.28813451528549194,
      "learning_rate": 3.139e-05,
      "loss": 0.0016,
      "step": 55830
    },
    {
      "epoch": 2.978133333333333,
      "grad_norm": 0.0332438088953495,
      "learning_rate": 3.138666666666667e-05,
      "loss": 0.0042,
      "step": 55840
    },
    {
      "epoch": 2.978666666666667,
      "grad_norm": 0.37923651933670044,
      "learning_rate": 3.1383333333333335e-05,
      "loss": 0.0016,
      "step": 55850
    },
    {
      "epoch": 2.9792,
      "grad_norm": 0.11476770788431168,
      "learning_rate": 3.138e-05,
      "loss": 0.0015,
      "step": 55860
    },
    {
      "epoch": 2.9797333333333333,
      "grad_norm": 0.610308051109314,
      "learning_rate": 3.1376666666666674e-05,
      "loss": 0.002,
      "step": 55870
    },
    {
      "epoch": 2.9802666666666666,
      "grad_norm": 0.14714395999908447,
      "learning_rate": 3.137333333333333e-05,
      "loss": 0.0036,
      "step": 55880
    },
    {
      "epoch": 2.9808,
      "grad_norm": 0.4878573715686798,
      "learning_rate": 3.137e-05,
      "loss": 0.0027,
      "step": 55890
    },
    {
      "epoch": 2.981333333333333,
      "grad_norm": 0.5830621719360352,
      "learning_rate": 3.1366666666666666e-05,
      "loss": 0.0029,
      "step": 55900
    },
    {
      "epoch": 2.981866666666667,
      "grad_norm": 0.46035051345825195,
      "learning_rate": 3.136333333333333e-05,
      "loss": 0.0024,
      "step": 55910
    },
    {
      "epoch": 2.9824,
      "grad_norm": 0.15511484444141388,
      "learning_rate": 3.136e-05,
      "loss": 0.0031,
      "step": 55920
    },
    {
      "epoch": 2.9829333333333334,
      "grad_norm": 0.04974519833922386,
      "learning_rate": 3.135666666666667e-05,
      "loss": 0.0023,
      "step": 55930
    },
    {
      "epoch": 2.9834666666666667,
      "grad_norm": 0.08828409761190414,
      "learning_rate": 3.135333333333334e-05,
      "loss": 0.0028,
      "step": 55940
    },
    {
      "epoch": 2.984,
      "grad_norm": 0.03505389764904976,
      "learning_rate": 3.135e-05,
      "loss": 0.0031,
      "step": 55950
    },
    {
      "epoch": 2.9845333333333333,
      "grad_norm": 0.3529594838619232,
      "learning_rate": 3.134666666666667e-05,
      "loss": 0.0018,
      "step": 55960
    },
    {
      "epoch": 2.9850666666666665,
      "grad_norm": 0.23551014065742493,
      "learning_rate": 3.1343333333333335e-05,
      "loss": 0.0018,
      "step": 55970
    },
    {
      "epoch": 2.9856,
      "grad_norm": 0.46539002656936646,
      "learning_rate": 3.134e-05,
      "loss": 0.003,
      "step": 55980
    },
    {
      "epoch": 2.986133333333333,
      "grad_norm": 0.2651597559452057,
      "learning_rate": 3.133666666666667e-05,
      "loss": 0.0017,
      "step": 55990
    },
    {
      "epoch": 2.986666666666667,
      "grad_norm": 0.0356687493622303,
      "learning_rate": 3.1333333333333334e-05,
      "loss": 0.0016,
      "step": 56000
    },
    {
      "epoch": 2.9872,
      "grad_norm": 0.1265087127685547,
      "learning_rate": 3.133000000000001e-05,
      "loss": 0.0018,
      "step": 56010
    },
    {
      "epoch": 2.9877333333333334,
      "grad_norm": 0.3448655307292938,
      "learning_rate": 3.132666666666667e-05,
      "loss": 0.0024,
      "step": 56020
    },
    {
      "epoch": 2.9882666666666666,
      "grad_norm": 0.381701797246933,
      "learning_rate": 3.132333333333333e-05,
      "loss": 0.0027,
      "step": 56030
    },
    {
      "epoch": 2.9888,
      "grad_norm": 0.09068750590085983,
      "learning_rate": 3.132e-05,
      "loss": 0.0022,
      "step": 56040
    },
    {
      "epoch": 2.989333333333333,
      "grad_norm": 0.06088989973068237,
      "learning_rate": 3.1316666666666664e-05,
      "loss": 0.0022,
      "step": 56050
    },
    {
      "epoch": 2.989866666666667,
      "grad_norm": 0.06299629807472229,
      "learning_rate": 3.131333333333333e-05,
      "loss": 0.0022,
      "step": 56060
    },
    {
      "epoch": 2.9904,
      "grad_norm": 0.2892572283744812,
      "learning_rate": 3.1310000000000003e-05,
      "loss": 0.0036,
      "step": 56070
    },
    {
      "epoch": 2.9909333333333334,
      "grad_norm": 0.49107760190963745,
      "learning_rate": 3.130666666666667e-05,
      "loss": 0.002,
      "step": 56080
    },
    {
      "epoch": 2.9914666666666667,
      "grad_norm": 0.32041874527931213,
      "learning_rate": 3.1303333333333336e-05,
      "loss": 0.0038,
      "step": 56090
    },
    {
      "epoch": 2.992,
      "grad_norm": 0.1758899986743927,
      "learning_rate": 3.13e-05,
      "loss": 0.0025,
      "step": 56100
    },
    {
      "epoch": 2.9925333333333333,
      "grad_norm": 0.06338466703891754,
      "learning_rate": 3.129666666666667e-05,
      "loss": 0.0027,
      "step": 56110
    },
    {
      "epoch": 2.9930666666666665,
      "grad_norm": 0.12070383876562119,
      "learning_rate": 3.1293333333333334e-05,
      "loss": 0.0032,
      "step": 56120
    },
    {
      "epoch": 2.9936,
      "grad_norm": 0.19297653436660767,
      "learning_rate": 3.129e-05,
      "loss": 0.003,
      "step": 56130
    },
    {
      "epoch": 2.994133333333333,
      "grad_norm": 0.2870476245880127,
      "learning_rate": 3.1286666666666666e-05,
      "loss": 0.0021,
      "step": 56140
    },
    {
      "epoch": 2.994666666666667,
      "grad_norm": 0.2137383669614792,
      "learning_rate": 3.128333333333334e-05,
      "loss": 0.0031,
      "step": 56150
    },
    {
      "epoch": 2.9952,
      "grad_norm": 0.04715317487716675,
      "learning_rate": 3.1280000000000005e-05,
      "loss": 0.0019,
      "step": 56160
    },
    {
      "epoch": 2.9957333333333334,
      "grad_norm": 0.46536949276924133,
      "learning_rate": 3.127666666666667e-05,
      "loss": 0.0033,
      "step": 56170
    },
    {
      "epoch": 2.9962666666666666,
      "grad_norm": 0.4375421404838562,
      "learning_rate": 3.127333333333333e-05,
      "loss": 0.0021,
      "step": 56180
    },
    {
      "epoch": 2.9968,
      "grad_norm": 0.03532877564430237,
      "learning_rate": 3.127e-05,
      "loss": 0.0023,
      "step": 56190
    },
    {
      "epoch": 2.997333333333333,
      "grad_norm": 0.3817794620990753,
      "learning_rate": 3.126666666666666e-05,
      "loss": 0.0019,
      "step": 56200
    },
    {
      "epoch": 2.997866666666667,
      "grad_norm": 0.06902054697275162,
      "learning_rate": 3.1263333333333336e-05,
      "loss": 0.003,
      "step": 56210
    },
    {
      "epoch": 2.9984,
      "grad_norm": 0.10483886301517487,
      "learning_rate": 3.126e-05,
      "loss": 0.0031,
      "step": 56220
    },
    {
      "epoch": 2.9989333333333335,
      "grad_norm": 0.17893706262111664,
      "learning_rate": 3.125666666666667e-05,
      "loss": 0.002,
      "step": 56230
    },
    {
      "epoch": 2.9994666666666667,
      "grad_norm": 0.5639704465866089,
      "learning_rate": 3.1253333333333335e-05,
      "loss": 0.0019,
      "step": 56240
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.4057535231113434,
      "learning_rate": 3.125e-05,
      "loss": 0.0027,
      "step": 56250
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.0025172920431941748,
      "eval_runtime": 175.0007,
      "eval_samples_per_second": 1428.565,
      "eval_steps_per_second": 35.714,
      "step": 56250
    },
    {
      "epoch": 3.0005333333333333,
      "grad_norm": 0.2902264893054962,
      "learning_rate": 3.124666666666667e-05,
      "loss": 0.0024,
      "step": 56260
    },
    {
      "epoch": 3.0010666666666665,
      "grad_norm": 0.13454589247703552,
      "learning_rate": 3.124333333333333e-05,
      "loss": 0.0022,
      "step": 56270
    },
    {
      "epoch": 3.0016,
      "grad_norm": 0.07281481474637985,
      "learning_rate": 3.1240000000000006e-05,
      "loss": 0.0029,
      "step": 56280
    },
    {
      "epoch": 3.0021333333333335,
      "grad_norm": 0.5988867282867432,
      "learning_rate": 3.123666666666667e-05,
      "loss": 0.0024,
      "step": 56290
    },
    {
      "epoch": 3.002666666666667,
      "grad_norm": 0.35249099135398865,
      "learning_rate": 3.123333333333334e-05,
      "loss": 0.0018,
      "step": 56300
    },
    {
      "epoch": 3.0032,
      "grad_norm": 0.05890892446041107,
      "learning_rate": 3.1230000000000004e-05,
      "loss": 0.0025,
      "step": 56310
    },
    {
      "epoch": 3.0037333333333334,
      "grad_norm": 0.4678794741630554,
      "learning_rate": 3.122666666666667e-05,
      "loss": 0.0025,
      "step": 56320
    },
    {
      "epoch": 3.0042666666666666,
      "grad_norm": 0.04686685651540756,
      "learning_rate": 3.122333333333333e-05,
      "loss": 0.0024,
      "step": 56330
    },
    {
      "epoch": 3.0048,
      "grad_norm": 0.5019387006759644,
      "learning_rate": 3.122e-05,
      "loss": 0.0031,
      "step": 56340
    },
    {
      "epoch": 3.005333333333333,
      "grad_norm": 0.14988432824611664,
      "learning_rate": 3.121666666666667e-05,
      "loss": 0.0016,
      "step": 56350
    },
    {
      "epoch": 3.0058666666666665,
      "grad_norm": 0.06464575976133347,
      "learning_rate": 3.1213333333333335e-05,
      "loss": 0.0016,
      "step": 56360
    },
    {
      "epoch": 3.0064,
      "grad_norm": 0.03590260446071625,
      "learning_rate": 3.121e-05,
      "loss": 0.0015,
      "step": 56370
    },
    {
      "epoch": 3.0069333333333335,
      "grad_norm": 0.06301005184650421,
      "learning_rate": 3.120666666666667e-05,
      "loss": 0.002,
      "step": 56380
    },
    {
      "epoch": 3.0074666666666667,
      "grad_norm": 0.09972277283668518,
      "learning_rate": 3.120333333333333e-05,
      "loss": 0.003,
      "step": 56390
    },
    {
      "epoch": 3.008,
      "grad_norm": 0.11914201080799103,
      "learning_rate": 3.12e-05,
      "loss": 0.0019,
      "step": 56400
    },
    {
      "epoch": 3.0085333333333333,
      "grad_norm": 0.21861082315444946,
      "learning_rate": 3.1196666666666666e-05,
      "loss": 0.003,
      "step": 56410
    },
    {
      "epoch": 3.0090666666666666,
      "grad_norm": 0.12152267247438431,
      "learning_rate": 3.119333333333334e-05,
      "loss": 0.0018,
      "step": 56420
    },
    {
      "epoch": 3.0096,
      "grad_norm": 0.2908097803592682,
      "learning_rate": 3.1190000000000005e-05,
      "loss": 0.0032,
      "step": 56430
    },
    {
      "epoch": 3.0101333333333335,
      "grad_norm": 0.09079279750585556,
      "learning_rate": 3.118666666666667e-05,
      "loss": 0.0028,
      "step": 56440
    },
    {
      "epoch": 3.010666666666667,
      "grad_norm": 0.2016192227602005,
      "learning_rate": 3.118333333333334e-05,
      "loss": 0.0029,
      "step": 56450
    },
    {
      "epoch": 3.0112,
      "grad_norm": 0.26380953192710876,
      "learning_rate": 3.118e-05,
      "loss": 0.0022,
      "step": 56460
    },
    {
      "epoch": 3.0117333333333334,
      "grad_norm": 0.05030155926942825,
      "learning_rate": 3.117666666666667e-05,
      "loss": 0.0024,
      "step": 56470
    },
    {
      "epoch": 3.0122666666666666,
      "grad_norm": 0.1314237266778946,
      "learning_rate": 3.1173333333333335e-05,
      "loss": 0.0024,
      "step": 56480
    },
    {
      "epoch": 3.0128,
      "grad_norm": 0.23096825182437897,
      "learning_rate": 3.117e-05,
      "loss": 0.0018,
      "step": 56490
    },
    {
      "epoch": 3.013333333333333,
      "grad_norm": 0.05586879700422287,
      "learning_rate": 3.116666666666667e-05,
      "loss": 0.003,
      "step": 56500
    },
    {
      "epoch": 3.0138666666666665,
      "grad_norm": 0.15010708570480347,
      "learning_rate": 3.1163333333333334e-05,
      "loss": 0.0015,
      "step": 56510
    },
    {
      "epoch": 3.0144,
      "grad_norm": 0.5272864699363708,
      "learning_rate": 3.116e-05,
      "loss": 0.0026,
      "step": 56520
    },
    {
      "epoch": 3.0149333333333335,
      "grad_norm": 0.09373974055051804,
      "learning_rate": 3.1156666666666666e-05,
      "loss": 0.0012,
      "step": 56530
    },
    {
      "epoch": 3.0154666666666667,
      "grad_norm": 0.15615501999855042,
      "learning_rate": 3.115333333333333e-05,
      "loss": 0.0018,
      "step": 56540
    },
    {
      "epoch": 3.016,
      "grad_norm": 0.34932780265808105,
      "learning_rate": 3.115e-05,
      "loss": 0.0024,
      "step": 56550
    },
    {
      "epoch": 3.0165333333333333,
      "grad_norm": 0.15903133153915405,
      "learning_rate": 3.114666666666667e-05,
      "loss": 0.0022,
      "step": 56560
    },
    {
      "epoch": 3.0170666666666666,
      "grad_norm": 0.09341701120138168,
      "learning_rate": 3.114333333333334e-05,
      "loss": 0.0016,
      "step": 56570
    },
    {
      "epoch": 3.0176,
      "grad_norm": 0.14671950042247772,
      "learning_rate": 3.1140000000000003e-05,
      "loss": 0.0038,
      "step": 56580
    },
    {
      "epoch": 3.018133333333333,
      "grad_norm": 0.498110830783844,
      "learning_rate": 3.113666666666667e-05,
      "loss": 0.0015,
      "step": 56590
    },
    {
      "epoch": 3.018666666666667,
      "grad_norm": 0.17895503342151642,
      "learning_rate": 3.1133333333333336e-05,
      "loss": 0.0017,
      "step": 56600
    },
    {
      "epoch": 3.0192,
      "grad_norm": 0.20279590785503387,
      "learning_rate": 3.113e-05,
      "loss": 0.0027,
      "step": 56610
    },
    {
      "epoch": 3.0197333333333334,
      "grad_norm": 0.15791940689086914,
      "learning_rate": 3.112666666666667e-05,
      "loss": 0.0011,
      "step": 56620
    },
    {
      "epoch": 3.0202666666666667,
      "grad_norm": 0.29001161456108093,
      "learning_rate": 3.1123333333333334e-05,
      "loss": 0.0029,
      "step": 56630
    },
    {
      "epoch": 3.0208,
      "grad_norm": 0.4455130398273468,
      "learning_rate": 3.112e-05,
      "loss": 0.0027,
      "step": 56640
    },
    {
      "epoch": 3.021333333333333,
      "grad_norm": 0.3549562990665436,
      "learning_rate": 3.1116666666666666e-05,
      "loss": 0.0034,
      "step": 56650
    },
    {
      "epoch": 3.0218666666666665,
      "grad_norm": 0.09653864055871964,
      "learning_rate": 3.111333333333333e-05,
      "loss": 0.0029,
      "step": 56660
    },
    {
      "epoch": 3.0224,
      "grad_norm": 0.26201632618904114,
      "learning_rate": 3.111e-05,
      "loss": 0.0033,
      "step": 56670
    },
    {
      "epoch": 3.0229333333333335,
      "grad_norm": 0.20757585763931274,
      "learning_rate": 3.1106666666666665e-05,
      "loss": 0.0031,
      "step": 56680
    },
    {
      "epoch": 3.0234666666666667,
      "grad_norm": 0.18254485726356506,
      "learning_rate": 3.110333333333334e-05,
      "loss": 0.0029,
      "step": 56690
    },
    {
      "epoch": 3.024,
      "grad_norm": 0.7942720055580139,
      "learning_rate": 3.1100000000000004e-05,
      "loss": 0.003,
      "step": 56700
    },
    {
      "epoch": 3.0245333333333333,
      "grad_norm": 0.13270212709903717,
      "learning_rate": 3.109666666666667e-05,
      "loss": 0.0024,
      "step": 56710
    },
    {
      "epoch": 3.0250666666666666,
      "grad_norm": 0.4097418487071991,
      "learning_rate": 3.1093333333333336e-05,
      "loss": 0.0038,
      "step": 56720
    },
    {
      "epoch": 3.0256,
      "grad_norm": 0.20890958607196808,
      "learning_rate": 3.109e-05,
      "loss": 0.0014,
      "step": 56730
    },
    {
      "epoch": 3.026133333333333,
      "grad_norm": 0.20547832548618317,
      "learning_rate": 3.108666666666667e-05,
      "loss": 0.0024,
      "step": 56740
    },
    {
      "epoch": 3.026666666666667,
      "grad_norm": 0.23926304280757904,
      "learning_rate": 3.1083333333333334e-05,
      "loss": 0.0019,
      "step": 56750
    },
    {
      "epoch": 3.0272,
      "grad_norm": 0.2935885488986969,
      "learning_rate": 3.108e-05,
      "loss": 0.004,
      "step": 56760
    },
    {
      "epoch": 3.0277333333333334,
      "grad_norm": 0.4962809681892395,
      "learning_rate": 3.1076666666666673e-05,
      "loss": 0.0024,
      "step": 56770
    },
    {
      "epoch": 3.0282666666666667,
      "grad_norm": 0.11804356426000595,
      "learning_rate": 3.107333333333333e-05,
      "loss": 0.0022,
      "step": 56780
    },
    {
      "epoch": 3.0288,
      "grad_norm": 0.12835060060024261,
      "learning_rate": 3.107e-05,
      "loss": 0.0025,
      "step": 56790
    },
    {
      "epoch": 3.029333333333333,
      "grad_norm": 0.5233153104782104,
      "learning_rate": 3.1066666666666665e-05,
      "loss": 0.0028,
      "step": 56800
    },
    {
      "epoch": 3.0298666666666665,
      "grad_norm": 0.24801281094551086,
      "learning_rate": 3.106333333333333e-05,
      "loss": 0.0042,
      "step": 56810
    },
    {
      "epoch": 3.0304,
      "grad_norm": 0.1816631406545639,
      "learning_rate": 3.106e-05,
      "loss": 0.0025,
      "step": 56820
    },
    {
      "epoch": 3.0309333333333335,
      "grad_norm": 0.15461613237857819,
      "learning_rate": 3.105666666666667e-05,
      "loss": 0.002,
      "step": 56830
    },
    {
      "epoch": 3.0314666666666668,
      "grad_norm": 0.41058820486068726,
      "learning_rate": 3.1053333333333336e-05,
      "loss": 0.0025,
      "step": 56840
    },
    {
      "epoch": 3.032,
      "grad_norm": 0.18286091089248657,
      "learning_rate": 3.105e-05,
      "loss": 0.0017,
      "step": 56850
    },
    {
      "epoch": 3.0325333333333333,
      "grad_norm": 0.21474768221378326,
      "learning_rate": 3.104666666666667e-05,
      "loss": 0.0024,
      "step": 56860
    },
    {
      "epoch": 3.0330666666666666,
      "grad_norm": 0.08709558099508286,
      "learning_rate": 3.1043333333333335e-05,
      "loss": 0.0026,
      "step": 56870
    },
    {
      "epoch": 3.0336,
      "grad_norm": 0.06278001517057419,
      "learning_rate": 3.104e-05,
      "loss": 0.002,
      "step": 56880
    },
    {
      "epoch": 3.034133333333333,
      "grad_norm": 0.2594913840293884,
      "learning_rate": 3.103666666666667e-05,
      "loss": 0.0031,
      "step": 56890
    },
    {
      "epoch": 3.034666666666667,
      "grad_norm": 0.23017524182796478,
      "learning_rate": 3.103333333333333e-05,
      "loss": 0.0034,
      "step": 56900
    },
    {
      "epoch": 3.0352,
      "grad_norm": 0.1048860028386116,
      "learning_rate": 3.1030000000000006e-05,
      "loss": 0.0024,
      "step": 56910
    },
    {
      "epoch": 3.0357333333333334,
      "grad_norm": 0.0959886908531189,
      "learning_rate": 3.102666666666667e-05,
      "loss": 0.0021,
      "step": 56920
    },
    {
      "epoch": 3.0362666666666667,
      "grad_norm": 0.6099936962127686,
      "learning_rate": 3.102333333333334e-05,
      "loss": 0.0018,
      "step": 56930
    },
    {
      "epoch": 3.0368,
      "grad_norm": 0.43458259105682373,
      "learning_rate": 3.102e-05,
      "loss": 0.0016,
      "step": 56940
    },
    {
      "epoch": 3.037333333333333,
      "grad_norm": 0.4996245205402374,
      "learning_rate": 3.1016666666666664e-05,
      "loss": 0.0029,
      "step": 56950
    },
    {
      "epoch": 3.0378666666666665,
      "grad_norm": 0.4056466519832611,
      "learning_rate": 3.101333333333333e-05,
      "loss": 0.0025,
      "step": 56960
    },
    {
      "epoch": 3.0384,
      "grad_norm": 0.11743615567684174,
      "learning_rate": 3.101e-05,
      "loss": 0.0019,
      "step": 56970
    },
    {
      "epoch": 3.0389333333333335,
      "grad_norm": 0.2748289406299591,
      "learning_rate": 3.100666666666667e-05,
      "loss": 0.003,
      "step": 56980
    },
    {
      "epoch": 3.0394666666666668,
      "grad_norm": 0.3612300753593445,
      "learning_rate": 3.1003333333333335e-05,
      "loss": 0.0035,
      "step": 56990
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.5281111598014832,
      "learning_rate": 3.1e-05,
      "loss": 0.0025,
      "step": 57000
    },
    {
      "epoch": 3.0405333333333333,
      "grad_norm": 0.55283522605896,
      "learning_rate": 3.099666666666667e-05,
      "loss": 0.003,
      "step": 57010
    },
    {
      "epoch": 3.0410666666666666,
      "grad_norm": 0.04809379205107689,
      "learning_rate": 3.0993333333333334e-05,
      "loss": 0.0023,
      "step": 57020
    },
    {
      "epoch": 3.0416,
      "grad_norm": 0.03744451701641083,
      "learning_rate": 3.099e-05,
      "loss": 0.002,
      "step": 57030
    },
    {
      "epoch": 3.042133333333333,
      "grad_norm": 0.17347583174705505,
      "learning_rate": 3.098666666666667e-05,
      "loss": 0.0018,
      "step": 57040
    },
    {
      "epoch": 3.042666666666667,
      "grad_norm": 0.0702538937330246,
      "learning_rate": 3.098333333333334e-05,
      "loss": 0.0025,
      "step": 57050
    },
    {
      "epoch": 3.0432,
      "grad_norm": 0.689200758934021,
      "learning_rate": 3.0980000000000005e-05,
      "loss": 0.0022,
      "step": 57060
    },
    {
      "epoch": 3.0437333333333334,
      "grad_norm": 0.15141451358795166,
      "learning_rate": 3.097666666666667e-05,
      "loss": 0.0022,
      "step": 57070
    },
    {
      "epoch": 3.0442666666666667,
      "grad_norm": 0.09450092911720276,
      "learning_rate": 3.097333333333334e-05,
      "loss": 0.003,
      "step": 57080
    },
    {
      "epoch": 3.0448,
      "grad_norm": 0.3420073688030243,
      "learning_rate": 3.0969999999999997e-05,
      "loss": 0.0016,
      "step": 57090
    },
    {
      "epoch": 3.0453333333333332,
      "grad_norm": 0.4747021794319153,
      "learning_rate": 3.096666666666666e-05,
      "loss": 0.0021,
      "step": 57100
    },
    {
      "epoch": 3.0458666666666665,
      "grad_norm": 0.276986688375473,
      "learning_rate": 3.0963333333333336e-05,
      "loss": 0.0022,
      "step": 57110
    },
    {
      "epoch": 3.0464,
      "grad_norm": 0.2601624131202698,
      "learning_rate": 3.096e-05,
      "loss": 0.0018,
      "step": 57120
    },
    {
      "epoch": 3.0469333333333335,
      "grad_norm": 0.09528905153274536,
      "learning_rate": 3.095666666666667e-05,
      "loss": 0.0027,
      "step": 57130
    },
    {
      "epoch": 3.0474666666666668,
      "grad_norm": 0.09135151654481888,
      "learning_rate": 3.0953333333333334e-05,
      "loss": 0.0019,
      "step": 57140
    },
    {
      "epoch": 3.048,
      "grad_norm": 0.03539581596851349,
      "learning_rate": 3.095e-05,
      "loss": 0.0023,
      "step": 57150
    },
    {
      "epoch": 3.0485333333333333,
      "grad_norm": 0.17842023074626923,
      "learning_rate": 3.0946666666666666e-05,
      "loss": 0.0021,
      "step": 57160
    },
    {
      "epoch": 3.0490666666666666,
      "grad_norm": 0.3217698037624359,
      "learning_rate": 3.094333333333333e-05,
      "loss": 0.0032,
      "step": 57170
    },
    {
      "epoch": 3.0496,
      "grad_norm": 0.0945318341255188,
      "learning_rate": 3.0940000000000005e-05,
      "loss": 0.0026,
      "step": 57180
    },
    {
      "epoch": 3.050133333333333,
      "grad_norm": 0.11627746373414993,
      "learning_rate": 3.093666666666667e-05,
      "loss": 0.0016,
      "step": 57190
    },
    {
      "epoch": 3.050666666666667,
      "grad_norm": 0.23601554334163666,
      "learning_rate": 3.093333333333334e-05,
      "loss": 0.0022,
      "step": 57200
    },
    {
      "epoch": 3.0512,
      "grad_norm": 0.20410892367362976,
      "learning_rate": 3.0930000000000004e-05,
      "loss": 0.0028,
      "step": 57210
    },
    {
      "epoch": 3.0517333333333334,
      "grad_norm": 0.349399209022522,
      "learning_rate": 3.092666666666667e-05,
      "loss": 0.0025,
      "step": 57220
    },
    {
      "epoch": 3.0522666666666667,
      "grad_norm": 0.17752991616725922,
      "learning_rate": 3.0923333333333336e-05,
      "loss": 0.0022,
      "step": 57230
    },
    {
      "epoch": 3.0528,
      "grad_norm": 0.35214340686798096,
      "learning_rate": 3.092e-05,
      "loss": 0.0028,
      "step": 57240
    },
    {
      "epoch": 3.0533333333333332,
      "grad_norm": 0.46490275859832764,
      "learning_rate": 3.091666666666667e-05,
      "loss": 0.0021,
      "step": 57250
    },
    {
      "epoch": 3.0538666666666665,
      "grad_norm": 0.49708229303359985,
      "learning_rate": 3.0913333333333334e-05,
      "loss": 0.0017,
      "step": 57260
    },
    {
      "epoch": 3.0544,
      "grad_norm": 0.31777089834213257,
      "learning_rate": 3.091e-05,
      "loss": 0.0023,
      "step": 57270
    },
    {
      "epoch": 3.0549333333333335,
      "grad_norm": 0.2696637511253357,
      "learning_rate": 3.090666666666667e-05,
      "loss": 0.0019,
      "step": 57280
    },
    {
      "epoch": 3.0554666666666668,
      "grad_norm": 0.14987345039844513,
      "learning_rate": 3.090333333333333e-05,
      "loss": 0.0019,
      "step": 57290
    },
    {
      "epoch": 3.056,
      "grad_norm": 0.023015981540083885,
      "learning_rate": 3.09e-05,
      "loss": 0.0023,
      "step": 57300
    },
    {
      "epoch": 3.0565333333333333,
      "grad_norm": 0.14744409918785095,
      "learning_rate": 3.0896666666666665e-05,
      "loss": 0.0023,
      "step": 57310
    },
    {
      "epoch": 3.0570666666666666,
      "grad_norm": 0.2384200245141983,
      "learning_rate": 3.089333333333334e-05,
      "loss": 0.0017,
      "step": 57320
    },
    {
      "epoch": 3.0576,
      "grad_norm": 0.21223606169223785,
      "learning_rate": 3.0890000000000004e-05,
      "loss": 0.0025,
      "step": 57330
    },
    {
      "epoch": 3.058133333333333,
      "grad_norm": 0.5247013568878174,
      "learning_rate": 3.088666666666667e-05,
      "loss": 0.0026,
      "step": 57340
    },
    {
      "epoch": 3.058666666666667,
      "grad_norm": 0.07536358386278152,
      "learning_rate": 3.0883333333333336e-05,
      "loss": 0.0027,
      "step": 57350
    },
    {
      "epoch": 3.0592,
      "grad_norm": 0.3974716365337372,
      "learning_rate": 3.088e-05,
      "loss": 0.0024,
      "step": 57360
    },
    {
      "epoch": 3.0597333333333334,
      "grad_norm": 0.297219455242157,
      "learning_rate": 3.087666666666667e-05,
      "loss": 0.0028,
      "step": 57370
    },
    {
      "epoch": 3.0602666666666667,
      "grad_norm": 0.27279865741729736,
      "learning_rate": 3.0873333333333335e-05,
      "loss": 0.0022,
      "step": 57380
    },
    {
      "epoch": 3.0608,
      "grad_norm": 0.44122129678726196,
      "learning_rate": 3.087e-05,
      "loss": 0.0027,
      "step": 57390
    },
    {
      "epoch": 3.0613333333333332,
      "grad_norm": 0.12338565289974213,
      "learning_rate": 3.086666666666667e-05,
      "loss": 0.0026,
      "step": 57400
    },
    {
      "epoch": 3.0618666666666665,
      "grad_norm": 0.3399479389190674,
      "learning_rate": 3.086333333333333e-05,
      "loss": 0.0023,
      "step": 57410
    },
    {
      "epoch": 3.0624,
      "grad_norm": 0.1461401879787445,
      "learning_rate": 3.086e-05,
      "loss": 0.0019,
      "step": 57420
    },
    {
      "epoch": 3.0629333333333335,
      "grad_norm": 0.3523538410663605,
      "learning_rate": 3.0856666666666665e-05,
      "loss": 0.0021,
      "step": 57430
    },
    {
      "epoch": 3.063466666666667,
      "grad_norm": 0.17723174393177032,
      "learning_rate": 3.085333333333333e-05,
      "loss": 0.0018,
      "step": 57440
    },
    {
      "epoch": 3.064,
      "grad_norm": 0.3917713761329651,
      "learning_rate": 3.0850000000000004e-05,
      "loss": 0.0025,
      "step": 57450
    },
    {
      "epoch": 3.0645333333333333,
      "grad_norm": 0.08089418709278107,
      "learning_rate": 3.084666666666667e-05,
      "loss": 0.0029,
      "step": 57460
    },
    {
      "epoch": 3.0650666666666666,
      "grad_norm": 0.23762156069278717,
      "learning_rate": 3.084333333333334e-05,
      "loss": 0.0025,
      "step": 57470
    },
    {
      "epoch": 3.0656,
      "grad_norm": 0.1874007284641266,
      "learning_rate": 3.084e-05,
      "loss": 0.0022,
      "step": 57480
    },
    {
      "epoch": 3.066133333333333,
      "grad_norm": 0.3545103669166565,
      "learning_rate": 3.083666666666667e-05,
      "loss": 0.0018,
      "step": 57490
    },
    {
      "epoch": 3.066666666666667,
      "grad_norm": 0.044632215052843094,
      "learning_rate": 3.0833333333333335e-05,
      "loss": 0.0026,
      "step": 57500
    },
    {
      "epoch": 3.0672,
      "grad_norm": 0.33230704069137573,
      "learning_rate": 3.083e-05,
      "loss": 0.0018,
      "step": 57510
    },
    {
      "epoch": 3.0677333333333334,
      "grad_norm": 0.11656883358955383,
      "learning_rate": 3.082666666666667e-05,
      "loss": 0.0021,
      "step": 57520
    },
    {
      "epoch": 3.0682666666666667,
      "grad_norm": 0.04770690202713013,
      "learning_rate": 3.082333333333334e-05,
      "loss": 0.0024,
      "step": 57530
    },
    {
      "epoch": 3.0688,
      "grad_norm": 0.04676990211009979,
      "learning_rate": 3.082e-05,
      "loss": 0.0018,
      "step": 57540
    },
    {
      "epoch": 3.0693333333333332,
      "grad_norm": 0.05907898396253586,
      "learning_rate": 3.0816666666666666e-05,
      "loss": 0.0023,
      "step": 57550
    },
    {
      "epoch": 3.0698666666666665,
      "grad_norm": 0.23120243847370148,
      "learning_rate": 3.081333333333333e-05,
      "loss": 0.0023,
      "step": 57560
    },
    {
      "epoch": 3.0704,
      "grad_norm": 0.41075941920280457,
      "learning_rate": 3.081e-05,
      "loss": 0.0028,
      "step": 57570
    },
    {
      "epoch": 3.0709333333333335,
      "grad_norm": 0.32611626386642456,
      "learning_rate": 3.0806666666666664e-05,
      "loss": 0.0028,
      "step": 57580
    },
    {
      "epoch": 3.071466666666667,
      "grad_norm": 0.03450753912329674,
      "learning_rate": 3.080333333333334e-05,
      "loss": 0.0023,
      "step": 57590
    },
    {
      "epoch": 3.072,
      "grad_norm": 0.05964314937591553,
      "learning_rate": 3.08e-05,
      "loss": 0.0026,
      "step": 57600
    },
    {
      "epoch": 3.0725333333333333,
      "grad_norm": 0.2617070972919464,
      "learning_rate": 3.079666666666667e-05,
      "loss": 0.0024,
      "step": 57610
    },
    {
      "epoch": 3.0730666666666666,
      "grad_norm": 0.08152252435684204,
      "learning_rate": 3.0793333333333336e-05,
      "loss": 0.0026,
      "step": 57620
    },
    {
      "epoch": 3.0736,
      "grad_norm": 0.06765467673540115,
      "learning_rate": 3.079e-05,
      "loss": 0.002,
      "step": 57630
    },
    {
      "epoch": 3.074133333333333,
      "grad_norm": 0.26622363924980164,
      "learning_rate": 3.078666666666667e-05,
      "loss": 0.0022,
      "step": 57640
    },
    {
      "epoch": 3.074666666666667,
      "grad_norm": 0.23590554296970367,
      "learning_rate": 3.0783333333333334e-05,
      "loss": 0.002,
      "step": 57650
    },
    {
      "epoch": 3.0752,
      "grad_norm": 0.059493280947208405,
      "learning_rate": 3.078e-05,
      "loss": 0.0026,
      "step": 57660
    },
    {
      "epoch": 3.0757333333333334,
      "grad_norm": 0.12062986940145493,
      "learning_rate": 3.077666666666667e-05,
      "loss": 0.0019,
      "step": 57670
    },
    {
      "epoch": 3.0762666666666667,
      "grad_norm": 0.31709229946136475,
      "learning_rate": 3.077333333333334e-05,
      "loss": 0.0019,
      "step": 57680
    },
    {
      "epoch": 3.0768,
      "grad_norm": 0.4418690800666809,
      "learning_rate": 3.077e-05,
      "loss": 0.0029,
      "step": 57690
    },
    {
      "epoch": 3.0773333333333333,
      "grad_norm": 0.3491811752319336,
      "learning_rate": 3.0766666666666665e-05,
      "loss": 0.0016,
      "step": 57700
    },
    {
      "epoch": 3.0778666666666665,
      "grad_norm": 0.1456870138645172,
      "learning_rate": 3.076333333333333e-05,
      "loss": 0.0026,
      "step": 57710
    },
    {
      "epoch": 3.0784,
      "grad_norm": 0.1979730725288391,
      "learning_rate": 3.076e-05,
      "loss": 0.0022,
      "step": 57720
    },
    {
      "epoch": 3.0789333333333335,
      "grad_norm": 0.48653745651245117,
      "learning_rate": 3.075666666666667e-05,
      "loss": 0.0022,
      "step": 57730
    },
    {
      "epoch": 3.079466666666667,
      "grad_norm": 0.7871578931808472,
      "learning_rate": 3.0753333333333336e-05,
      "loss": 0.0028,
      "step": 57740
    },
    {
      "epoch": 3.08,
      "grad_norm": 0.18898896872997284,
      "learning_rate": 3.075e-05,
      "loss": 0.002,
      "step": 57750
    },
    {
      "epoch": 3.0805333333333333,
      "grad_norm": 0.13962240517139435,
      "learning_rate": 3.074666666666667e-05,
      "loss": 0.003,
      "step": 57760
    },
    {
      "epoch": 3.0810666666666666,
      "grad_norm": 0.29531028866767883,
      "learning_rate": 3.0743333333333334e-05,
      "loss": 0.0028,
      "step": 57770
    },
    {
      "epoch": 3.0816,
      "grad_norm": 0.4531940817832947,
      "learning_rate": 3.074e-05,
      "loss": 0.0023,
      "step": 57780
    },
    {
      "epoch": 3.082133333333333,
      "grad_norm": 0.20828019082546234,
      "learning_rate": 3.0736666666666667e-05,
      "loss": 0.0028,
      "step": 57790
    },
    {
      "epoch": 3.0826666666666664,
      "grad_norm": 0.38579249382019043,
      "learning_rate": 3.073333333333334e-05,
      "loss": 0.0029,
      "step": 57800
    },
    {
      "epoch": 3.0832,
      "grad_norm": 0.5450063943862915,
      "learning_rate": 3.0730000000000006e-05,
      "loss": 0.0024,
      "step": 57810
    },
    {
      "epoch": 3.0837333333333334,
      "grad_norm": 0.2358880490064621,
      "learning_rate": 3.072666666666667e-05,
      "loss": 0.0036,
      "step": 57820
    },
    {
      "epoch": 3.0842666666666667,
      "grad_norm": 0.05260153487324715,
      "learning_rate": 3.072333333333334e-05,
      "loss": 0.0025,
      "step": 57830
    },
    {
      "epoch": 3.0848,
      "grad_norm": 0.27777916193008423,
      "learning_rate": 3.072e-05,
      "loss": 0.0028,
      "step": 57840
    },
    {
      "epoch": 3.0853333333333333,
      "grad_norm": 0.08504556119441986,
      "learning_rate": 3.0716666666666663e-05,
      "loss": 0.0033,
      "step": 57850
    },
    {
      "epoch": 3.0858666666666665,
      "grad_norm": 0.15753400325775146,
      "learning_rate": 3.071333333333333e-05,
      "loss": 0.003,
      "step": 57860
    },
    {
      "epoch": 3.0864,
      "grad_norm": 0.36129727959632874,
      "learning_rate": 3.071e-05,
      "loss": 0.0026,
      "step": 57870
    },
    {
      "epoch": 3.0869333333333335,
      "grad_norm": 0.07485075294971466,
      "learning_rate": 3.070666666666667e-05,
      "loss": 0.0038,
      "step": 57880
    },
    {
      "epoch": 3.087466666666667,
      "grad_norm": 0.3367452025413513,
      "learning_rate": 3.0703333333333335e-05,
      "loss": 0.0026,
      "step": 57890
    },
    {
      "epoch": 3.088,
      "grad_norm": 0.5035355687141418,
      "learning_rate": 3.07e-05,
      "loss": 0.0026,
      "step": 57900
    },
    {
      "epoch": 3.0885333333333334,
      "grad_norm": 0.5012349486351013,
      "learning_rate": 3.069666666666667e-05,
      "loss": 0.0031,
      "step": 57910
    },
    {
      "epoch": 3.0890666666666666,
      "grad_norm": 0.11836186051368713,
      "learning_rate": 3.069333333333333e-05,
      "loss": 0.0029,
      "step": 57920
    },
    {
      "epoch": 3.0896,
      "grad_norm": 0.23577184975147247,
      "learning_rate": 3.069e-05,
      "loss": 0.003,
      "step": 57930
    },
    {
      "epoch": 3.090133333333333,
      "grad_norm": 0.145146906375885,
      "learning_rate": 3.068666666666667e-05,
      "loss": 0.0037,
      "step": 57940
    },
    {
      "epoch": 3.0906666666666665,
      "grad_norm": 0.2923583686351776,
      "learning_rate": 3.068333333333334e-05,
      "loss": 0.0019,
      "step": 57950
    },
    {
      "epoch": 3.0912,
      "grad_norm": 0.21021614968776703,
      "learning_rate": 3.0680000000000004e-05,
      "loss": 0.002,
      "step": 57960
    },
    {
      "epoch": 3.0917333333333334,
      "grad_norm": 0.12031148374080658,
      "learning_rate": 3.067666666666667e-05,
      "loss": 0.0019,
      "step": 57970
    },
    {
      "epoch": 3.0922666666666667,
      "grad_norm": 0.40287652611732483,
      "learning_rate": 3.067333333333334e-05,
      "loss": 0.0029,
      "step": 57980
    },
    {
      "epoch": 3.0928,
      "grad_norm": 0.03419771417975426,
      "learning_rate": 3.0669999999999996e-05,
      "loss": 0.0018,
      "step": 57990
    },
    {
      "epoch": 3.0933333333333333,
      "grad_norm": 0.293666273355484,
      "learning_rate": 3.066666666666667e-05,
      "loss": 0.0028,
      "step": 58000
    },
    {
      "epoch": 3.0938666666666665,
      "grad_norm": 0.24150148034095764,
      "learning_rate": 3.0663333333333335e-05,
      "loss": 0.0027,
      "step": 58010
    },
    {
      "epoch": 3.0944,
      "grad_norm": 0.3469748795032501,
      "learning_rate": 3.066e-05,
      "loss": 0.0026,
      "step": 58020
    },
    {
      "epoch": 3.0949333333333335,
      "grad_norm": 0.26258158683776855,
      "learning_rate": 3.065666666666667e-05,
      "loss": 0.0021,
      "step": 58030
    },
    {
      "epoch": 3.095466666666667,
      "grad_norm": 0.4139230251312256,
      "learning_rate": 3.0653333333333333e-05,
      "loss": 0.0023,
      "step": 58040
    },
    {
      "epoch": 3.096,
      "grad_norm": 0.25255605578422546,
      "learning_rate": 3.065e-05,
      "loss": 0.0033,
      "step": 58050
    },
    {
      "epoch": 3.0965333333333334,
      "grad_norm": 0.28876519203186035,
      "learning_rate": 3.0646666666666666e-05,
      "loss": 0.0033,
      "step": 58060
    },
    {
      "epoch": 3.0970666666666666,
      "grad_norm": 0.24277319014072418,
      "learning_rate": 3.064333333333333e-05,
      "loss": 0.0024,
      "step": 58070
    },
    {
      "epoch": 3.0976,
      "grad_norm": 0.1219322457909584,
      "learning_rate": 3.0640000000000005e-05,
      "loss": 0.0017,
      "step": 58080
    },
    {
      "epoch": 3.098133333333333,
      "grad_norm": 0.14712285995483398,
      "learning_rate": 3.063666666666667e-05,
      "loss": 0.0029,
      "step": 58090
    },
    {
      "epoch": 3.0986666666666665,
      "grad_norm": 0.06671226769685745,
      "learning_rate": 3.063333333333334e-05,
      "loss": 0.0012,
      "step": 58100
    },
    {
      "epoch": 3.0992,
      "grad_norm": 0.2917008101940155,
      "learning_rate": 3.063e-05,
      "loss": 0.0032,
      "step": 58110
    },
    {
      "epoch": 3.0997333333333335,
      "grad_norm": 0.6182473301887512,
      "learning_rate": 3.062666666666667e-05,
      "loss": 0.0023,
      "step": 58120
    },
    {
      "epoch": 3.1002666666666667,
      "grad_norm": 0.2773520052433014,
      "learning_rate": 3.0623333333333335e-05,
      "loss": 0.0023,
      "step": 58130
    },
    {
      "epoch": 3.1008,
      "grad_norm": 0.47828540205955505,
      "learning_rate": 3.062e-05,
      "loss": 0.0028,
      "step": 58140
    },
    {
      "epoch": 3.1013333333333333,
      "grad_norm": 0.04865147918462753,
      "learning_rate": 3.061666666666667e-05,
      "loss": 0.0028,
      "step": 58150
    },
    {
      "epoch": 3.1018666666666665,
      "grad_norm": 0.5649105906486511,
      "learning_rate": 3.0613333333333334e-05,
      "loss": 0.0033,
      "step": 58160
    },
    {
      "epoch": 3.1024,
      "grad_norm": 0.09268692135810852,
      "learning_rate": 3.061e-05,
      "loss": 0.0023,
      "step": 58170
    },
    {
      "epoch": 3.1029333333333335,
      "grad_norm": 0.26590538024902344,
      "learning_rate": 3.0606666666666666e-05,
      "loss": 0.0024,
      "step": 58180
    },
    {
      "epoch": 3.103466666666667,
      "grad_norm": 0.35510385036468506,
      "learning_rate": 3.060333333333333e-05,
      "loss": 0.0018,
      "step": 58190
    },
    {
      "epoch": 3.104,
      "grad_norm": 0.07697153091430664,
      "learning_rate": 3.06e-05,
      "loss": 0.0021,
      "step": 58200
    },
    {
      "epoch": 3.1045333333333334,
      "grad_norm": 0.20467501878738403,
      "learning_rate": 3.0596666666666665e-05,
      "loss": 0.0029,
      "step": 58210
    },
    {
      "epoch": 3.1050666666666666,
      "grad_norm": 0.06305226683616638,
      "learning_rate": 3.059333333333334e-05,
      "loss": 0.0025,
      "step": 58220
    },
    {
      "epoch": 3.1056,
      "grad_norm": 0.2032700628042221,
      "learning_rate": 3.0590000000000004e-05,
      "loss": 0.0025,
      "step": 58230
    },
    {
      "epoch": 3.106133333333333,
      "grad_norm": 0.1798459142446518,
      "learning_rate": 3.058666666666667e-05,
      "loss": 0.0019,
      "step": 58240
    },
    {
      "epoch": 3.1066666666666665,
      "grad_norm": 0.035417117178440094,
      "learning_rate": 3.0583333333333336e-05,
      "loss": 0.0026,
      "step": 58250
    },
    {
      "epoch": 3.1072,
      "grad_norm": 0.20502576231956482,
      "learning_rate": 3.058e-05,
      "loss": 0.0019,
      "step": 58260
    },
    {
      "epoch": 3.1077333333333335,
      "grad_norm": 0.5361071825027466,
      "learning_rate": 3.057666666666667e-05,
      "loss": 0.0022,
      "step": 58270
    },
    {
      "epoch": 3.1082666666666667,
      "grad_norm": 0.5348724126815796,
      "learning_rate": 3.0573333333333334e-05,
      "loss": 0.0023,
      "step": 58280
    },
    {
      "epoch": 3.1088,
      "grad_norm": 0.0670807734131813,
      "learning_rate": 3.057000000000001e-05,
      "loss": 0.0029,
      "step": 58290
    },
    {
      "epoch": 3.1093333333333333,
      "grad_norm": 0.12529928982257843,
      "learning_rate": 3.0566666666666667e-05,
      "loss": 0.0033,
      "step": 58300
    },
    {
      "epoch": 3.1098666666666666,
      "grad_norm": 0.4378810226917267,
      "learning_rate": 3.056333333333333e-05,
      "loss": 0.0028,
      "step": 58310
    },
    {
      "epoch": 3.1104,
      "grad_norm": 0.30263304710388184,
      "learning_rate": 3.056e-05,
      "loss": 0.0025,
      "step": 58320
    },
    {
      "epoch": 3.1109333333333336,
      "grad_norm": 0.3855086863040924,
      "learning_rate": 3.0556666666666665e-05,
      "loss": 0.0022,
      "step": 58330
    },
    {
      "epoch": 3.111466666666667,
      "grad_norm": 0.03359164297580719,
      "learning_rate": 3.055333333333333e-05,
      "loss": 0.0021,
      "step": 58340
    },
    {
      "epoch": 3.112,
      "grad_norm": 0.2746189534664154,
      "learning_rate": 3.0550000000000004e-05,
      "loss": 0.0036,
      "step": 58350
    },
    {
      "epoch": 3.1125333333333334,
      "grad_norm": 0.13288089632987976,
      "learning_rate": 3.054666666666667e-05,
      "loss": 0.002,
      "step": 58360
    },
    {
      "epoch": 3.1130666666666666,
      "grad_norm": 0.29779309034347534,
      "learning_rate": 3.0543333333333336e-05,
      "loss": 0.0017,
      "step": 58370
    },
    {
      "epoch": 3.1136,
      "grad_norm": 0.1777523010969162,
      "learning_rate": 3.054e-05,
      "loss": 0.0025,
      "step": 58380
    },
    {
      "epoch": 3.114133333333333,
      "grad_norm": 0.20884886384010315,
      "learning_rate": 3.053666666666667e-05,
      "loss": 0.0021,
      "step": 58390
    },
    {
      "epoch": 3.1146666666666665,
      "grad_norm": 0.07139462977647781,
      "learning_rate": 3.0533333333333335e-05,
      "loss": 0.002,
      "step": 58400
    },
    {
      "epoch": 3.1152,
      "grad_norm": 0.03794998303055763,
      "learning_rate": 3.053e-05,
      "loss": 0.0017,
      "step": 58410
    },
    {
      "epoch": 3.1157333333333335,
      "grad_norm": 0.2351573407649994,
      "learning_rate": 3.052666666666667e-05,
      "loss": 0.0021,
      "step": 58420
    },
    {
      "epoch": 3.1162666666666667,
      "grad_norm": 0.044926442205905914,
      "learning_rate": 3.052333333333334e-05,
      "loss": 0.0033,
      "step": 58430
    },
    {
      "epoch": 3.1168,
      "grad_norm": 0.1486118882894516,
      "learning_rate": 3.0520000000000006e-05,
      "loss": 0.003,
      "step": 58440
    },
    {
      "epoch": 3.1173333333333333,
      "grad_norm": 0.5860493779182434,
      "learning_rate": 3.0516666666666665e-05,
      "loss": 0.002,
      "step": 58450
    },
    {
      "epoch": 3.1178666666666666,
      "grad_norm": 0.40941545367240906,
      "learning_rate": 3.051333333333333e-05,
      "loss": 0.0027,
      "step": 58460
    },
    {
      "epoch": 3.1184,
      "grad_norm": 0.06203213334083557,
      "learning_rate": 3.051e-05,
      "loss": 0.0023,
      "step": 58470
    },
    {
      "epoch": 3.1189333333333336,
      "grad_norm": 0.17609959840774536,
      "learning_rate": 3.0506666666666667e-05,
      "loss": 0.0018,
      "step": 58480
    },
    {
      "epoch": 3.119466666666667,
      "grad_norm": 0.23755799233913422,
      "learning_rate": 3.0503333333333333e-05,
      "loss": 0.0016,
      "step": 58490
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.20743229985237122,
      "learning_rate": 3.05e-05,
      "loss": 0.0019,
      "step": 58500
    },
    {
      "epoch": 3.1205333333333334,
      "grad_norm": 0.5962680578231812,
      "learning_rate": 3.049666666666667e-05,
      "loss": 0.0028,
      "step": 58510
    },
    {
      "epoch": 3.1210666666666667,
      "grad_norm": 0.5299435257911682,
      "learning_rate": 3.0493333333333335e-05,
      "loss": 0.0014,
      "step": 58520
    },
    {
      "epoch": 3.1216,
      "grad_norm": 0.3797316253185272,
      "learning_rate": 3.049e-05,
      "loss": 0.0026,
      "step": 58530
    },
    {
      "epoch": 3.122133333333333,
      "grad_norm": 0.030065840110182762,
      "learning_rate": 3.0486666666666667e-05,
      "loss": 0.0029,
      "step": 58540
    },
    {
      "epoch": 3.1226666666666665,
      "grad_norm": 0.17875494062900543,
      "learning_rate": 3.0483333333333337e-05,
      "loss": 0.0026,
      "step": 58550
    },
    {
      "epoch": 3.1232,
      "grad_norm": 0.17573146522045135,
      "learning_rate": 3.0480000000000003e-05,
      "loss": 0.0019,
      "step": 58560
    },
    {
      "epoch": 3.1237333333333335,
      "grad_norm": 0.2938571870326996,
      "learning_rate": 3.047666666666667e-05,
      "loss": 0.002,
      "step": 58570
    },
    {
      "epoch": 3.1242666666666667,
      "grad_norm": 0.38799023628234863,
      "learning_rate": 3.047333333333334e-05,
      "loss": 0.0023,
      "step": 58580
    },
    {
      "epoch": 3.1248,
      "grad_norm": 0.3827734887599945,
      "learning_rate": 3.0470000000000005e-05,
      "loss": 0.0019,
      "step": 58590
    },
    {
      "epoch": 3.1253333333333333,
      "grad_norm": 0.20400705933570862,
      "learning_rate": 3.0466666666666664e-05,
      "loss": 0.0022,
      "step": 58600
    },
    {
      "epoch": 3.1258666666666666,
      "grad_norm": 0.20807816088199615,
      "learning_rate": 3.0463333333333334e-05,
      "loss": 0.0021,
      "step": 58610
    },
    {
      "epoch": 3.1264,
      "grad_norm": 0.08643793314695358,
      "learning_rate": 3.046e-05,
      "loss": 0.0016,
      "step": 58620
    },
    {
      "epoch": 3.1269333333333336,
      "grad_norm": 0.32546722888946533,
      "learning_rate": 3.0456666666666666e-05,
      "loss": 0.0024,
      "step": 58630
    },
    {
      "epoch": 3.127466666666667,
      "grad_norm": 0.14750082790851593,
      "learning_rate": 3.0453333333333335e-05,
      "loss": 0.0028,
      "step": 58640
    },
    {
      "epoch": 3.128,
      "grad_norm": 0.26152172684669495,
      "learning_rate": 3.045e-05,
      "loss": 0.0021,
      "step": 58650
    },
    {
      "epoch": 3.1285333333333334,
      "grad_norm": 0.42232421040534973,
      "learning_rate": 3.0446666666666668e-05,
      "loss": 0.0022,
      "step": 58660
    },
    {
      "epoch": 3.1290666666666667,
      "grad_norm": 0.15087375044822693,
      "learning_rate": 3.0443333333333334e-05,
      "loss": 0.0016,
      "step": 58670
    },
    {
      "epoch": 3.1296,
      "grad_norm": 0.38812321424484253,
      "learning_rate": 3.0440000000000003e-05,
      "loss": 0.0029,
      "step": 58680
    },
    {
      "epoch": 3.130133333333333,
      "grad_norm": 0.17777881026268005,
      "learning_rate": 3.043666666666667e-05,
      "loss": 0.0032,
      "step": 58690
    },
    {
      "epoch": 3.1306666666666665,
      "grad_norm": 0.1563161015510559,
      "learning_rate": 3.0433333333333336e-05,
      "loss": 0.0016,
      "step": 58700
    },
    {
      "epoch": 3.1312,
      "grad_norm": 0.14608441293239594,
      "learning_rate": 3.0430000000000002e-05,
      "loss": 0.0014,
      "step": 58710
    },
    {
      "epoch": 3.1317333333333335,
      "grad_norm": 0.15113398432731628,
      "learning_rate": 3.042666666666667e-05,
      "loss": 0.0022,
      "step": 58720
    },
    {
      "epoch": 3.1322666666666668,
      "grad_norm": 0.23125116527080536,
      "learning_rate": 3.0423333333333337e-05,
      "loss": 0.0025,
      "step": 58730
    },
    {
      "epoch": 3.1328,
      "grad_norm": 0.2575627565383911,
      "learning_rate": 3.0420000000000004e-05,
      "loss": 0.0021,
      "step": 58740
    },
    {
      "epoch": 3.1333333333333333,
      "grad_norm": 0.2613871395587921,
      "learning_rate": 3.0416666666666666e-05,
      "loss": 0.002,
      "step": 58750
    },
    {
      "epoch": 3.1338666666666666,
      "grad_norm": 0.3230760097503662,
      "learning_rate": 3.0413333333333332e-05,
      "loss": 0.0024,
      "step": 58760
    },
    {
      "epoch": 3.1344,
      "grad_norm": 0.17678570747375488,
      "learning_rate": 3.041e-05,
      "loss": 0.0022,
      "step": 58770
    },
    {
      "epoch": 3.134933333333333,
      "grad_norm": 0.4209446310997009,
      "learning_rate": 3.0406666666666668e-05,
      "loss": 0.0022,
      "step": 58780
    },
    {
      "epoch": 3.135466666666667,
      "grad_norm": 0.502955436706543,
      "learning_rate": 3.0403333333333334e-05,
      "loss": 0.002,
      "step": 58790
    },
    {
      "epoch": 3.136,
      "grad_norm": 0.6041339039802551,
      "learning_rate": 3.04e-05,
      "loss": 0.0019,
      "step": 58800
    },
    {
      "epoch": 3.1365333333333334,
      "grad_norm": 0.07299813628196716,
      "learning_rate": 3.0396666666666666e-05,
      "loss": 0.0017,
      "step": 58810
    },
    {
      "epoch": 3.1370666666666667,
      "grad_norm": 0.5014578104019165,
      "learning_rate": 3.0393333333333336e-05,
      "loss": 0.0024,
      "step": 58820
    },
    {
      "epoch": 3.1376,
      "grad_norm": 0.1842961311340332,
      "learning_rate": 3.0390000000000002e-05,
      "loss": 0.0025,
      "step": 58830
    },
    {
      "epoch": 3.138133333333333,
      "grad_norm": 0.046994004398584366,
      "learning_rate": 3.0386666666666668e-05,
      "loss": 0.0026,
      "step": 58840
    },
    {
      "epoch": 3.1386666666666665,
      "grad_norm": 0.08922123908996582,
      "learning_rate": 3.0383333333333334e-05,
      "loss": 0.0013,
      "step": 58850
    },
    {
      "epoch": 3.1391999999999998,
      "grad_norm": 0.18406912684440613,
      "learning_rate": 3.0380000000000004e-05,
      "loss": 0.002,
      "step": 58860
    },
    {
      "epoch": 3.1397333333333335,
      "grad_norm": 0.20378440618515015,
      "learning_rate": 3.037666666666667e-05,
      "loss": 0.003,
      "step": 58870
    },
    {
      "epoch": 3.1402666666666668,
      "grad_norm": 0.23378652334213257,
      "learning_rate": 3.0373333333333336e-05,
      "loss": 0.0036,
      "step": 58880
    },
    {
      "epoch": 3.1408,
      "grad_norm": 0.5564621686935425,
      "learning_rate": 3.0370000000000006e-05,
      "loss": 0.0021,
      "step": 58890
    },
    {
      "epoch": 3.1413333333333333,
      "grad_norm": 0.21158568561077118,
      "learning_rate": 3.0366666666666665e-05,
      "loss": 0.0041,
      "step": 58900
    },
    {
      "epoch": 3.1418666666666666,
      "grad_norm": 0.4345056116580963,
      "learning_rate": 3.036333333333333e-05,
      "loss": 0.0028,
      "step": 58910
    },
    {
      "epoch": 3.1424,
      "grad_norm": 0.21060805022716522,
      "learning_rate": 3.036e-05,
      "loss": 0.002,
      "step": 58920
    },
    {
      "epoch": 3.142933333333333,
      "grad_norm": 0.20347784459590912,
      "learning_rate": 3.0356666666666667e-05,
      "loss": 0.0018,
      "step": 58930
    },
    {
      "epoch": 3.143466666666667,
      "grad_norm": 0.27651122212409973,
      "learning_rate": 3.0353333333333333e-05,
      "loss": 0.0029,
      "step": 58940
    },
    {
      "epoch": 3.144,
      "grad_norm": 0.6234217882156372,
      "learning_rate": 3.035e-05,
      "loss": 0.0028,
      "step": 58950
    },
    {
      "epoch": 3.1445333333333334,
      "grad_norm": 0.4744563102722168,
      "learning_rate": 3.034666666666667e-05,
      "loss": 0.0025,
      "step": 58960
    },
    {
      "epoch": 3.1450666666666667,
      "grad_norm": 0.5636675357818604,
      "learning_rate": 3.0343333333333335e-05,
      "loss": 0.0026,
      "step": 58970
    },
    {
      "epoch": 3.1456,
      "grad_norm": 0.26545679569244385,
      "learning_rate": 3.034e-05,
      "loss": 0.0033,
      "step": 58980
    },
    {
      "epoch": 3.1461333333333332,
      "grad_norm": 0.6702167987823486,
      "learning_rate": 3.033666666666667e-05,
      "loss": 0.0022,
      "step": 58990
    },
    {
      "epoch": 3.1466666666666665,
      "grad_norm": 0.16360120475292206,
      "learning_rate": 3.0333333333333337e-05,
      "loss": 0.0031,
      "step": 59000
    },
    {
      "epoch": 3.1471999999999998,
      "grad_norm": 0.24973419308662415,
      "learning_rate": 3.0330000000000003e-05,
      "loss": 0.0023,
      "step": 59010
    },
    {
      "epoch": 3.1477333333333335,
      "grad_norm": 0.2967061400413513,
      "learning_rate": 3.032666666666667e-05,
      "loss": 0.002,
      "step": 59020
    },
    {
      "epoch": 3.1482666666666668,
      "grad_norm": 0.08597090095281601,
      "learning_rate": 3.032333333333334e-05,
      "loss": 0.0021,
      "step": 59030
    },
    {
      "epoch": 3.1488,
      "grad_norm": 0.4136054217815399,
      "learning_rate": 3.0320000000000004e-05,
      "loss": 0.0021,
      "step": 59040
    },
    {
      "epoch": 3.1493333333333333,
      "grad_norm": 0.030365869402885437,
      "learning_rate": 3.0316666666666664e-05,
      "loss": 0.0028,
      "step": 59050
    },
    {
      "epoch": 3.1498666666666666,
      "grad_norm": 0.040215302258729935,
      "learning_rate": 3.0313333333333333e-05,
      "loss": 0.0019,
      "step": 59060
    },
    {
      "epoch": 3.1504,
      "grad_norm": 0.5580825209617615,
      "learning_rate": 3.031e-05,
      "loss": 0.0022,
      "step": 59070
    },
    {
      "epoch": 3.150933333333333,
      "grad_norm": 0.1249781921505928,
      "learning_rate": 3.0306666666666666e-05,
      "loss": 0.0023,
      "step": 59080
    },
    {
      "epoch": 3.151466666666667,
      "grad_norm": 0.33401918411254883,
      "learning_rate": 3.0303333333333335e-05,
      "loss": 0.0024,
      "step": 59090
    },
    {
      "epoch": 3.152,
      "grad_norm": 0.6983386874198914,
      "learning_rate": 3.03e-05,
      "loss": 0.0016,
      "step": 59100
    },
    {
      "epoch": 3.1525333333333334,
      "grad_norm": 0.07047834992408752,
      "learning_rate": 3.0296666666666667e-05,
      "loss": 0.0024,
      "step": 59110
    },
    {
      "epoch": 3.1530666666666667,
      "grad_norm": 0.13902448117733002,
      "learning_rate": 3.0293333333333334e-05,
      "loss": 0.0028,
      "step": 59120
    },
    {
      "epoch": 3.1536,
      "grad_norm": 0.13258902728557587,
      "learning_rate": 3.0290000000000003e-05,
      "loss": 0.0024,
      "step": 59130
    },
    {
      "epoch": 3.1541333333333332,
      "grad_norm": 0.3019934296607971,
      "learning_rate": 3.028666666666667e-05,
      "loss": 0.0017,
      "step": 59140
    },
    {
      "epoch": 3.1546666666666665,
      "grad_norm": 0.2735050916671753,
      "learning_rate": 3.0283333333333335e-05,
      "loss": 0.0017,
      "step": 59150
    },
    {
      "epoch": 3.1552,
      "grad_norm": 0.04303888976573944,
      "learning_rate": 3.028e-05,
      "loss": 0.0017,
      "step": 59160
    },
    {
      "epoch": 3.1557333333333335,
      "grad_norm": 0.1920391023159027,
      "learning_rate": 3.027666666666667e-05,
      "loss": 0.0021,
      "step": 59170
    },
    {
      "epoch": 3.1562666666666668,
      "grad_norm": 0.3244170546531677,
      "learning_rate": 3.0273333333333337e-05,
      "loss": 0.002,
      "step": 59180
    },
    {
      "epoch": 3.1568,
      "grad_norm": 0.0943521112203598,
      "learning_rate": 3.0270000000000003e-05,
      "loss": 0.0023,
      "step": 59190
    },
    {
      "epoch": 3.1573333333333333,
      "grad_norm": 0.2988995611667633,
      "learning_rate": 3.0266666666666666e-05,
      "loss": 0.0024,
      "step": 59200
    },
    {
      "epoch": 3.1578666666666666,
      "grad_norm": 0.07394958287477493,
      "learning_rate": 3.0263333333333332e-05,
      "loss": 0.0019,
      "step": 59210
    },
    {
      "epoch": 3.1584,
      "grad_norm": 0.36663150787353516,
      "learning_rate": 3.0259999999999998e-05,
      "loss": 0.0022,
      "step": 59220
    },
    {
      "epoch": 3.158933333333333,
      "grad_norm": 0.13946539163589478,
      "learning_rate": 3.0256666666666668e-05,
      "loss": 0.0019,
      "step": 59230
    },
    {
      "epoch": 3.159466666666667,
      "grad_norm": 0.41013190150260925,
      "learning_rate": 3.0253333333333334e-05,
      "loss": 0.0023,
      "step": 59240
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.43960052728652954,
      "learning_rate": 3.025e-05,
      "loss": 0.0017,
      "step": 59250
    },
    {
      "epoch": 3.1605333333333334,
      "grad_norm": 0.29294803738594055,
      "learning_rate": 3.0246666666666666e-05,
      "loss": 0.0027,
      "step": 59260
    },
    {
      "epoch": 3.1610666666666667,
      "grad_norm": 0.2392631471157074,
      "learning_rate": 3.0243333333333336e-05,
      "loss": 0.0019,
      "step": 59270
    },
    {
      "epoch": 3.1616,
      "grad_norm": 0.09725087136030197,
      "learning_rate": 3.0240000000000002e-05,
      "loss": 0.0017,
      "step": 59280
    },
    {
      "epoch": 3.1621333333333332,
      "grad_norm": 0.2654401659965515,
      "learning_rate": 3.0236666666666668e-05,
      "loss": 0.002,
      "step": 59290
    },
    {
      "epoch": 3.1626666666666665,
      "grad_norm": 0.11542487889528275,
      "learning_rate": 3.0233333333333334e-05,
      "loss": 0.003,
      "step": 59300
    },
    {
      "epoch": 3.1632,
      "grad_norm": 0.09187322854995728,
      "learning_rate": 3.0230000000000004e-05,
      "loss": 0.0029,
      "step": 59310
    },
    {
      "epoch": 3.1637333333333335,
      "grad_norm": 0.1191382184624672,
      "learning_rate": 3.022666666666667e-05,
      "loss": 0.0028,
      "step": 59320
    },
    {
      "epoch": 3.164266666666667,
      "grad_norm": 0.12258370220661163,
      "learning_rate": 3.0223333333333336e-05,
      "loss": 0.0016,
      "step": 59330
    },
    {
      "epoch": 3.1648,
      "grad_norm": 0.271607905626297,
      "learning_rate": 3.0220000000000005e-05,
      "loss": 0.0028,
      "step": 59340
    },
    {
      "epoch": 3.1653333333333333,
      "grad_norm": 0.5624026656150818,
      "learning_rate": 3.0216666666666665e-05,
      "loss": 0.0023,
      "step": 59350
    },
    {
      "epoch": 3.1658666666666666,
      "grad_norm": 0.2770887315273285,
      "learning_rate": 3.021333333333333e-05,
      "loss": 0.002,
      "step": 59360
    },
    {
      "epoch": 3.1664,
      "grad_norm": 0.19043904542922974,
      "learning_rate": 3.021e-05,
      "loss": 0.0024,
      "step": 59370
    },
    {
      "epoch": 3.166933333333333,
      "grad_norm": 0.09071066230535507,
      "learning_rate": 3.0206666666666667e-05,
      "loss": 0.0021,
      "step": 59380
    },
    {
      "epoch": 3.167466666666667,
      "grad_norm": 0.20852665603160858,
      "learning_rate": 3.0203333333333333e-05,
      "loss": 0.0018,
      "step": 59390
    },
    {
      "epoch": 3.168,
      "grad_norm": 0.016259698197245598,
      "learning_rate": 3.02e-05,
      "loss": 0.0022,
      "step": 59400
    },
    {
      "epoch": 3.1685333333333334,
      "grad_norm": 0.49212098121643066,
      "learning_rate": 3.019666666666667e-05,
      "loss": 0.0023,
      "step": 59410
    },
    {
      "epoch": 3.1690666666666667,
      "grad_norm": 0.4007576107978821,
      "learning_rate": 3.0193333333333335e-05,
      "loss": 0.0026,
      "step": 59420
    },
    {
      "epoch": 3.1696,
      "grad_norm": 0.47029611468315125,
      "learning_rate": 3.019e-05,
      "loss": 0.0028,
      "step": 59430
    },
    {
      "epoch": 3.1701333333333332,
      "grad_norm": 0.5831279754638672,
      "learning_rate": 3.018666666666667e-05,
      "loss": 0.0016,
      "step": 59440
    },
    {
      "epoch": 3.1706666666666665,
      "grad_norm": 0.04112336412072182,
      "learning_rate": 3.0183333333333336e-05,
      "loss": 0.0017,
      "step": 59450
    },
    {
      "epoch": 3.1712,
      "grad_norm": 0.1798725575208664,
      "learning_rate": 3.0180000000000002e-05,
      "loss": 0.0026,
      "step": 59460
    },
    {
      "epoch": 3.1717333333333335,
      "grad_norm": 0.5034500360488892,
      "learning_rate": 3.017666666666667e-05,
      "loss": 0.0016,
      "step": 59470
    },
    {
      "epoch": 3.172266666666667,
      "grad_norm": 0.16719287633895874,
      "learning_rate": 3.0173333333333338e-05,
      "loss": 0.0026,
      "step": 59480
    },
    {
      "epoch": 3.1728,
      "grad_norm": 0.0844409242272377,
      "learning_rate": 3.0170000000000004e-05,
      "loss": 0.0022,
      "step": 59490
    },
    {
      "epoch": 3.1733333333333333,
      "grad_norm": 0.1571687012910843,
      "learning_rate": 3.016666666666667e-05,
      "loss": 0.0021,
      "step": 59500
    },
    {
      "epoch": 3.1738666666666666,
      "grad_norm": 0.14571689069271088,
      "learning_rate": 3.0163333333333333e-05,
      "loss": 0.0024,
      "step": 59510
    },
    {
      "epoch": 3.1744,
      "grad_norm": 0.15332184731960297,
      "learning_rate": 3.016e-05,
      "loss": 0.0024,
      "step": 59520
    },
    {
      "epoch": 3.174933333333333,
      "grad_norm": 0.04268285632133484,
      "learning_rate": 3.0156666666666665e-05,
      "loss": 0.0018,
      "step": 59530
    },
    {
      "epoch": 3.175466666666667,
      "grad_norm": 0.09076670557260513,
      "learning_rate": 3.0153333333333335e-05,
      "loss": 0.0025,
      "step": 59540
    },
    {
      "epoch": 3.176,
      "grad_norm": 0.09205705672502518,
      "learning_rate": 3.015e-05,
      "loss": 0.0024,
      "step": 59550
    },
    {
      "epoch": 3.1765333333333334,
      "grad_norm": 0.11914236098527908,
      "learning_rate": 3.0146666666666667e-05,
      "loss": 0.002,
      "step": 59560
    },
    {
      "epoch": 3.1770666666666667,
      "grad_norm": 0.24434426426887512,
      "learning_rate": 3.0143333333333333e-05,
      "loss": 0.0021,
      "step": 59570
    },
    {
      "epoch": 3.1776,
      "grad_norm": 0.14831258356571198,
      "learning_rate": 3.0140000000000003e-05,
      "loss": 0.0023,
      "step": 59580
    },
    {
      "epoch": 3.1781333333333333,
      "grad_norm": 0.07171782106161118,
      "learning_rate": 3.013666666666667e-05,
      "loss": 0.0019,
      "step": 59590
    },
    {
      "epoch": 3.1786666666666665,
      "grad_norm": 0.06422343105077744,
      "learning_rate": 3.0133333333333335e-05,
      "loss": 0.0027,
      "step": 59600
    },
    {
      "epoch": 3.1792,
      "grad_norm": 0.21194922924041748,
      "learning_rate": 3.013e-05,
      "loss": 0.0017,
      "step": 59610
    },
    {
      "epoch": 3.1797333333333335,
      "grad_norm": 0.3570462465286255,
      "learning_rate": 3.012666666666667e-05,
      "loss": 0.0016,
      "step": 59620
    },
    {
      "epoch": 3.180266666666667,
      "grad_norm": 0.1563943326473236,
      "learning_rate": 3.0123333333333337e-05,
      "loss": 0.0027,
      "step": 59630
    },
    {
      "epoch": 3.1808,
      "grad_norm": 0.046967972069978714,
      "learning_rate": 3.0120000000000003e-05,
      "loss": 0.0026,
      "step": 59640
    },
    {
      "epoch": 3.1813333333333333,
      "grad_norm": 0.1324947476387024,
      "learning_rate": 3.011666666666667e-05,
      "loss": 0.0017,
      "step": 59650
    },
    {
      "epoch": 3.1818666666666666,
      "grad_norm": 0.4142622649669647,
      "learning_rate": 3.0113333333333332e-05,
      "loss": 0.0016,
      "step": 59660
    },
    {
      "epoch": 3.1824,
      "grad_norm": 0.29202359914779663,
      "learning_rate": 3.0109999999999998e-05,
      "loss": 0.0013,
      "step": 59670
    },
    {
      "epoch": 3.182933333333333,
      "grad_norm": 0.0975232943892479,
      "learning_rate": 3.0106666666666668e-05,
      "loss": 0.0018,
      "step": 59680
    },
    {
      "epoch": 3.183466666666667,
      "grad_norm": 0.2649761438369751,
      "learning_rate": 3.0103333333333334e-05,
      "loss": 0.0029,
      "step": 59690
    },
    {
      "epoch": 3.184,
      "grad_norm": 0.27205440402030945,
      "learning_rate": 3.01e-05,
      "loss": 0.0017,
      "step": 59700
    },
    {
      "epoch": 3.1845333333333334,
      "grad_norm": 0.06511678546667099,
      "learning_rate": 3.0096666666666666e-05,
      "loss": 0.0024,
      "step": 59710
    },
    {
      "epoch": 3.1850666666666667,
      "grad_norm": 0.12139461189508438,
      "learning_rate": 3.0093333333333335e-05,
      "loss": 0.0029,
      "step": 59720
    },
    {
      "epoch": 3.1856,
      "grad_norm": 0.10932774841785431,
      "learning_rate": 3.009e-05,
      "loss": 0.0015,
      "step": 59730
    },
    {
      "epoch": 3.1861333333333333,
      "grad_norm": 0.07003965973854065,
      "learning_rate": 3.0086666666666668e-05,
      "loss": 0.0036,
      "step": 59740
    },
    {
      "epoch": 3.1866666666666665,
      "grad_norm": 0.10979728400707245,
      "learning_rate": 3.0083333333333337e-05,
      "loss": 0.003,
      "step": 59750
    },
    {
      "epoch": 3.1872,
      "grad_norm": 0.2887616753578186,
      "learning_rate": 3.0080000000000003e-05,
      "loss": 0.0017,
      "step": 59760
    },
    {
      "epoch": 3.1877333333333335,
      "grad_norm": 0.2065148651599884,
      "learning_rate": 3.007666666666667e-05,
      "loss": 0.0018,
      "step": 59770
    },
    {
      "epoch": 3.188266666666667,
      "grad_norm": 0.3855662941932678,
      "learning_rate": 3.0073333333333336e-05,
      "loss": 0.0019,
      "step": 59780
    },
    {
      "epoch": 3.1888,
      "grad_norm": 0.2063913494348526,
      "learning_rate": 3.0070000000000005e-05,
      "loss": 0.0024,
      "step": 59790
    },
    {
      "epoch": 3.1893333333333334,
      "grad_norm": 0.040696367621421814,
      "learning_rate": 3.006666666666667e-05,
      "loss": 0.0016,
      "step": 59800
    },
    {
      "epoch": 3.1898666666666666,
      "grad_norm": 0.5887483358383179,
      "learning_rate": 3.006333333333333e-05,
      "loss": 0.0026,
      "step": 59810
    },
    {
      "epoch": 3.1904,
      "grad_norm": 0.09055827558040619,
      "learning_rate": 3.006e-05,
      "loss": 0.0019,
      "step": 59820
    },
    {
      "epoch": 3.190933333333333,
      "grad_norm": 0.025615623220801353,
      "learning_rate": 3.0056666666666666e-05,
      "loss": 0.0017,
      "step": 59830
    },
    {
      "epoch": 3.191466666666667,
      "grad_norm": 0.6099224090576172,
      "learning_rate": 3.0053333333333332e-05,
      "loss": 0.0029,
      "step": 59840
    },
    {
      "epoch": 3.192,
      "grad_norm": 0.17827944457530975,
      "learning_rate": 3.0050000000000002e-05,
      "loss": 0.0019,
      "step": 59850
    },
    {
      "epoch": 3.1925333333333334,
      "grad_norm": 0.23089906573295593,
      "learning_rate": 3.0046666666666668e-05,
      "loss": 0.0028,
      "step": 59860
    },
    {
      "epoch": 3.1930666666666667,
      "grad_norm": 0.28893882036209106,
      "learning_rate": 3.0043333333333334e-05,
      "loss": 0.0023,
      "step": 59870
    },
    {
      "epoch": 3.1936,
      "grad_norm": 0.20599548518657684,
      "learning_rate": 3.004e-05,
      "loss": 0.0023,
      "step": 59880
    },
    {
      "epoch": 3.1941333333333333,
      "grad_norm": 0.05952746421098709,
      "learning_rate": 3.003666666666667e-05,
      "loss": 0.003,
      "step": 59890
    },
    {
      "epoch": 3.1946666666666665,
      "grad_norm": 0.6780773401260376,
      "learning_rate": 3.0033333333333336e-05,
      "loss": 0.0032,
      "step": 59900
    },
    {
      "epoch": 3.1952,
      "grad_norm": 0.11746608465909958,
      "learning_rate": 3.0030000000000002e-05,
      "loss": 0.0021,
      "step": 59910
    },
    {
      "epoch": 3.1957333333333335,
      "grad_norm": 0.6106060147285461,
      "learning_rate": 3.0026666666666668e-05,
      "loss": 0.0026,
      "step": 59920
    },
    {
      "epoch": 3.196266666666667,
      "grad_norm": 0.22255343198776245,
      "learning_rate": 3.0023333333333338e-05,
      "loss": 0.0028,
      "step": 59930
    },
    {
      "epoch": 3.1968,
      "grad_norm": 0.06378354132175446,
      "learning_rate": 3.0020000000000004e-05,
      "loss": 0.0033,
      "step": 59940
    },
    {
      "epoch": 3.1973333333333334,
      "grad_norm": 0.09541206806898117,
      "learning_rate": 3.001666666666667e-05,
      "loss": 0.0026,
      "step": 59950
    },
    {
      "epoch": 3.1978666666666666,
      "grad_norm": 0.18285737931728363,
      "learning_rate": 3.0013333333333333e-05,
      "loss": 0.002,
      "step": 59960
    },
    {
      "epoch": 3.1984,
      "grad_norm": 0.12011943757534027,
      "learning_rate": 3.001e-05,
      "loss": 0.0022,
      "step": 59970
    },
    {
      "epoch": 3.198933333333333,
      "grad_norm": 0.056919731199741364,
      "learning_rate": 3.0006666666666665e-05,
      "loss": 0.0025,
      "step": 59980
    },
    {
      "epoch": 3.1994666666666665,
      "grad_norm": 0.15023674070835114,
      "learning_rate": 3.0003333333333335e-05,
      "loss": 0.0016,
      "step": 59990
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.06276940554380417,
      "learning_rate": 3e-05,
      "loss": 0.0023,
      "step": 60000
    },
    {
      "epoch": 3.2005333333333335,
      "grad_norm": 0.17994311451911926,
      "learning_rate": 2.9996666666666667e-05,
      "loss": 0.0018,
      "step": 60010
    },
    {
      "epoch": 3.2010666666666667,
      "grad_norm": 0.18600647151470184,
      "learning_rate": 2.9993333333333333e-05,
      "loss": 0.0019,
      "step": 60020
    },
    {
      "epoch": 3.2016,
      "grad_norm": 0.18236060440540314,
      "learning_rate": 2.9990000000000003e-05,
      "loss": 0.0024,
      "step": 60030
    },
    {
      "epoch": 3.2021333333333333,
      "grad_norm": 0.11880394071340561,
      "learning_rate": 2.998666666666667e-05,
      "loss": 0.003,
      "step": 60040
    },
    {
      "epoch": 3.2026666666666666,
      "grad_norm": 0.1770125776529312,
      "learning_rate": 2.9983333333333335e-05,
      "loss": 0.0032,
      "step": 60050
    },
    {
      "epoch": 3.2032,
      "grad_norm": 0.2342771291732788,
      "learning_rate": 2.998e-05,
      "loss": 0.0021,
      "step": 60060
    },
    {
      "epoch": 3.203733333333333,
      "grad_norm": 0.14579705893993378,
      "learning_rate": 2.997666666666667e-05,
      "loss": 0.0028,
      "step": 60070
    },
    {
      "epoch": 3.204266666666667,
      "grad_norm": 0.3948114812374115,
      "learning_rate": 2.9973333333333337e-05,
      "loss": 0.0024,
      "step": 60080
    },
    {
      "epoch": 3.2048,
      "grad_norm": 0.5232143402099609,
      "learning_rate": 2.9970000000000003e-05,
      "loss": 0.0039,
      "step": 60090
    },
    {
      "epoch": 3.2053333333333334,
      "grad_norm": 0.36075690388679504,
      "learning_rate": 2.9966666666666672e-05,
      "loss": 0.0029,
      "step": 60100
    },
    {
      "epoch": 3.2058666666666666,
      "grad_norm": 0.15267720818519592,
      "learning_rate": 2.996333333333333e-05,
      "loss": 0.0026,
      "step": 60110
    },
    {
      "epoch": 3.2064,
      "grad_norm": 0.08795274794101715,
      "learning_rate": 2.9959999999999998e-05,
      "loss": 0.0019,
      "step": 60120
    },
    {
      "epoch": 3.206933333333333,
      "grad_norm": 0.34409472346305847,
      "learning_rate": 2.9956666666666667e-05,
      "loss": 0.0023,
      "step": 60130
    },
    {
      "epoch": 3.2074666666666665,
      "grad_norm": 0.2374712973833084,
      "learning_rate": 2.9953333333333333e-05,
      "loss": 0.0023,
      "step": 60140
    },
    {
      "epoch": 3.208,
      "grad_norm": 0.2656688392162323,
      "learning_rate": 2.995e-05,
      "loss": 0.0023,
      "step": 60150
    },
    {
      "epoch": 3.2085333333333335,
      "grad_norm": 0.29229670763015747,
      "learning_rate": 2.9946666666666666e-05,
      "loss": 0.0013,
      "step": 60160
    },
    {
      "epoch": 3.2090666666666667,
      "grad_norm": 0.3240807056427002,
      "learning_rate": 2.9943333333333335e-05,
      "loss": 0.0016,
      "step": 60170
    },
    {
      "epoch": 3.2096,
      "grad_norm": 0.031007764860987663,
      "learning_rate": 2.994e-05,
      "loss": 0.0027,
      "step": 60180
    },
    {
      "epoch": 3.2101333333333333,
      "grad_norm": 0.15633071959018707,
      "learning_rate": 2.9936666666666667e-05,
      "loss": 0.0022,
      "step": 60190
    },
    {
      "epoch": 3.2106666666666666,
      "grad_norm": 0.08322107791900635,
      "learning_rate": 2.9933333333333337e-05,
      "loss": 0.0027,
      "step": 60200
    },
    {
      "epoch": 3.2112,
      "grad_norm": 0.4765923023223877,
      "learning_rate": 2.9930000000000003e-05,
      "loss": 0.0018,
      "step": 60210
    },
    {
      "epoch": 3.211733333333333,
      "grad_norm": 0.29601985216140747,
      "learning_rate": 2.992666666666667e-05,
      "loss": 0.0025,
      "step": 60220
    },
    {
      "epoch": 3.212266666666667,
      "grad_norm": 0.0690268725156784,
      "learning_rate": 2.9923333333333335e-05,
      "loss": 0.0025,
      "step": 60230
    },
    {
      "epoch": 3.2128,
      "grad_norm": 0.07043580710887909,
      "learning_rate": 2.9920000000000005e-05,
      "loss": 0.0022,
      "step": 60240
    },
    {
      "epoch": 3.2133333333333334,
      "grad_norm": 0.04098080098628998,
      "learning_rate": 2.991666666666667e-05,
      "loss": 0.0024,
      "step": 60250
    },
    {
      "epoch": 3.2138666666666666,
      "grad_norm": 0.23745331168174744,
      "learning_rate": 2.991333333333333e-05,
      "loss": 0.003,
      "step": 60260
    },
    {
      "epoch": 3.2144,
      "grad_norm": 0.10249447822570801,
      "learning_rate": 2.991e-05,
      "loss": 0.0013,
      "step": 60270
    },
    {
      "epoch": 3.214933333333333,
      "grad_norm": 0.23557215929031372,
      "learning_rate": 2.9906666666666666e-05,
      "loss": 0.0017,
      "step": 60280
    },
    {
      "epoch": 3.2154666666666665,
      "grad_norm": 0.06152601167559624,
      "learning_rate": 2.9903333333333332e-05,
      "loss": 0.0027,
      "step": 60290
    },
    {
      "epoch": 3.216,
      "grad_norm": 0.09678594768047333,
      "learning_rate": 2.9900000000000002e-05,
      "loss": 0.0019,
      "step": 60300
    },
    {
      "epoch": 3.2165333333333335,
      "grad_norm": 0.5633927583694458,
      "learning_rate": 2.9896666666666668e-05,
      "loss": 0.0018,
      "step": 60310
    },
    {
      "epoch": 3.2170666666666667,
      "grad_norm": 0.4157630503177643,
      "learning_rate": 2.9893333333333334e-05,
      "loss": 0.0025,
      "step": 60320
    },
    {
      "epoch": 3.2176,
      "grad_norm": 0.17737403512001038,
      "learning_rate": 2.989e-05,
      "loss": 0.0014,
      "step": 60330
    },
    {
      "epoch": 3.2181333333333333,
      "grad_norm": 0.3874524235725403,
      "learning_rate": 2.988666666666667e-05,
      "loss": 0.0027,
      "step": 60340
    },
    {
      "epoch": 3.2186666666666666,
      "grad_norm": 0.267049103975296,
      "learning_rate": 2.9883333333333336e-05,
      "loss": 0.0037,
      "step": 60350
    },
    {
      "epoch": 3.2192,
      "grad_norm": 0.49623197317123413,
      "learning_rate": 2.9880000000000002e-05,
      "loss": 0.0026,
      "step": 60360
    },
    {
      "epoch": 3.219733333333333,
      "grad_norm": 0.28181302547454834,
      "learning_rate": 2.9876666666666668e-05,
      "loss": 0.0026,
      "step": 60370
    },
    {
      "epoch": 3.220266666666667,
      "grad_norm": 0.06905584782361984,
      "learning_rate": 2.9873333333333338e-05,
      "loss": 0.0026,
      "step": 60380
    },
    {
      "epoch": 3.2208,
      "grad_norm": 0.2715409994125366,
      "learning_rate": 2.9870000000000004e-05,
      "loss": 0.0025,
      "step": 60390
    },
    {
      "epoch": 3.2213333333333334,
      "grad_norm": 0.0662485733628273,
      "learning_rate": 2.986666666666667e-05,
      "loss": 0.0019,
      "step": 60400
    },
    {
      "epoch": 3.2218666666666667,
      "grad_norm": 0.1770012080669403,
      "learning_rate": 2.9863333333333333e-05,
      "loss": 0.0028,
      "step": 60410
    },
    {
      "epoch": 3.2224,
      "grad_norm": 0.2985866665840149,
      "learning_rate": 2.986e-05,
      "loss": 0.0017,
      "step": 60420
    },
    {
      "epoch": 3.222933333333333,
      "grad_norm": 0.5049930214881897,
      "learning_rate": 2.9856666666666665e-05,
      "loss": 0.0029,
      "step": 60430
    },
    {
      "epoch": 3.2234666666666665,
      "grad_norm": 0.08650855720043182,
      "learning_rate": 2.9853333333333334e-05,
      "loss": 0.0015,
      "step": 60440
    },
    {
      "epoch": 3.224,
      "grad_norm": 0.27260056138038635,
      "learning_rate": 2.985e-05,
      "loss": 0.0022,
      "step": 60450
    },
    {
      "epoch": 3.2245333333333335,
      "grad_norm": 0.2188994288444519,
      "learning_rate": 2.9846666666666667e-05,
      "loss": 0.0026,
      "step": 60460
    },
    {
      "epoch": 3.2250666666666667,
      "grad_norm": 0.3299969434738159,
      "learning_rate": 2.9843333333333333e-05,
      "loss": 0.0019,
      "step": 60470
    },
    {
      "epoch": 3.2256,
      "grad_norm": 0.20564211905002594,
      "learning_rate": 2.9840000000000002e-05,
      "loss": 0.0014,
      "step": 60480
    },
    {
      "epoch": 3.2261333333333333,
      "grad_norm": 0.12278703600168228,
      "learning_rate": 2.983666666666667e-05,
      "loss": 0.0028,
      "step": 60490
    },
    {
      "epoch": 3.2266666666666666,
      "grad_norm": 0.41913938522338867,
      "learning_rate": 2.9833333333333335e-05,
      "loss": 0.0025,
      "step": 60500
    },
    {
      "epoch": 3.2272,
      "grad_norm": 0.16952937841415405,
      "learning_rate": 2.9830000000000004e-05,
      "loss": 0.0038,
      "step": 60510
    },
    {
      "epoch": 3.227733333333333,
      "grad_norm": 0.1076722964644432,
      "learning_rate": 2.982666666666667e-05,
      "loss": 0.0021,
      "step": 60520
    },
    {
      "epoch": 3.228266666666667,
      "grad_norm": 0.26933422684669495,
      "learning_rate": 2.9823333333333336e-05,
      "loss": 0.0015,
      "step": 60530
    },
    {
      "epoch": 3.2288,
      "grad_norm": 0.3538562059402466,
      "learning_rate": 2.9820000000000002e-05,
      "loss": 0.0021,
      "step": 60540
    },
    {
      "epoch": 3.2293333333333334,
      "grad_norm": 0.5332382917404175,
      "learning_rate": 2.9816666666666672e-05,
      "loss": 0.0028,
      "step": 60550
    },
    {
      "epoch": 3.2298666666666667,
      "grad_norm": 0.4444914758205414,
      "learning_rate": 2.981333333333333e-05,
      "loss": 0.0014,
      "step": 60560
    },
    {
      "epoch": 3.2304,
      "grad_norm": 0.061300747096538544,
      "learning_rate": 2.9809999999999997e-05,
      "loss": 0.0026,
      "step": 60570
    },
    {
      "epoch": 3.230933333333333,
      "grad_norm": 0.29440227150917053,
      "learning_rate": 2.9806666666666667e-05,
      "loss": 0.0018,
      "step": 60580
    },
    {
      "epoch": 3.2314666666666665,
      "grad_norm": 0.14674437046051025,
      "learning_rate": 2.9803333333333333e-05,
      "loss": 0.0022,
      "step": 60590
    },
    {
      "epoch": 3.232,
      "grad_norm": 0.5546248555183411,
      "learning_rate": 2.98e-05,
      "loss": 0.002,
      "step": 60600
    },
    {
      "epoch": 3.2325333333333335,
      "grad_norm": 0.023432135581970215,
      "learning_rate": 2.979666666666667e-05,
      "loss": 0.0026,
      "step": 60610
    },
    {
      "epoch": 3.2330666666666668,
      "grad_norm": 0.022075841203331947,
      "learning_rate": 2.9793333333333335e-05,
      "loss": 0.0019,
      "step": 60620
    },
    {
      "epoch": 3.2336,
      "grad_norm": 0.2088979333639145,
      "learning_rate": 2.979e-05,
      "loss": 0.002,
      "step": 60630
    },
    {
      "epoch": 3.2341333333333333,
      "grad_norm": 0.11666569113731384,
      "learning_rate": 2.9786666666666667e-05,
      "loss": 0.0023,
      "step": 60640
    },
    {
      "epoch": 3.2346666666666666,
      "grad_norm": 0.2660694718360901,
      "learning_rate": 2.9783333333333337e-05,
      "loss": 0.0023,
      "step": 60650
    },
    {
      "epoch": 3.2352,
      "grad_norm": 0.4109099209308624,
      "learning_rate": 2.9780000000000003e-05,
      "loss": 0.0022,
      "step": 60660
    },
    {
      "epoch": 3.235733333333333,
      "grad_norm": 0.2243192046880722,
      "learning_rate": 2.977666666666667e-05,
      "loss": 0.002,
      "step": 60670
    },
    {
      "epoch": 3.236266666666667,
      "grad_norm": 0.23806259036064148,
      "learning_rate": 2.9773333333333335e-05,
      "loss": 0.0021,
      "step": 60680
    },
    {
      "epoch": 3.2368,
      "grad_norm": 0.302985280752182,
      "learning_rate": 2.9770000000000005e-05,
      "loss": 0.0018,
      "step": 60690
    },
    {
      "epoch": 3.2373333333333334,
      "grad_norm": 0.2065330147743225,
      "learning_rate": 2.976666666666667e-05,
      "loss": 0.0026,
      "step": 60700
    },
    {
      "epoch": 3.2378666666666667,
      "grad_norm": 0.23283828794956207,
      "learning_rate": 2.9763333333333337e-05,
      "loss": 0.0028,
      "step": 60710
    },
    {
      "epoch": 3.2384,
      "grad_norm": 0.18127180635929108,
      "learning_rate": 2.976e-05,
      "loss": 0.0019,
      "step": 60720
    },
    {
      "epoch": 3.238933333333333,
      "grad_norm": 0.6849583983421326,
      "learning_rate": 2.9756666666666666e-05,
      "loss": 0.0032,
      "step": 60730
    },
    {
      "epoch": 3.2394666666666665,
      "grad_norm": 0.28994375467300415,
      "learning_rate": 2.9753333333333332e-05,
      "loss": 0.0028,
      "step": 60740
    },
    {
      "epoch": 3.24,
      "grad_norm": 0.21102526783943176,
      "learning_rate": 2.975e-05,
      "loss": 0.0031,
      "step": 60750
    },
    {
      "epoch": 3.2405333333333335,
      "grad_norm": 0.03675499185919762,
      "learning_rate": 2.9746666666666668e-05,
      "loss": 0.0017,
      "step": 60760
    },
    {
      "epoch": 3.2410666666666668,
      "grad_norm": 0.2633020579814911,
      "learning_rate": 2.9743333333333334e-05,
      "loss": 0.0014,
      "step": 60770
    },
    {
      "epoch": 3.2416,
      "grad_norm": 0.15164805948734283,
      "learning_rate": 2.974e-05,
      "loss": 0.0028,
      "step": 60780
    },
    {
      "epoch": 3.2421333333333333,
      "grad_norm": 0.23580493032932281,
      "learning_rate": 2.973666666666667e-05,
      "loss": 0.0016,
      "step": 60790
    },
    {
      "epoch": 3.2426666666666666,
      "grad_norm": 0.26484429836273193,
      "learning_rate": 2.9733333333333336e-05,
      "loss": 0.0024,
      "step": 60800
    },
    {
      "epoch": 3.2432,
      "grad_norm": 0.2502991557121277,
      "learning_rate": 2.973e-05,
      "loss": 0.0021,
      "step": 60810
    },
    {
      "epoch": 3.243733333333333,
      "grad_norm": 0.20686160027980804,
      "learning_rate": 2.9726666666666668e-05,
      "loss": 0.0026,
      "step": 60820
    },
    {
      "epoch": 3.244266666666667,
      "grad_norm": 0.264902263879776,
      "learning_rate": 2.9723333333333337e-05,
      "loss": 0.0027,
      "step": 60830
    },
    {
      "epoch": 3.2448,
      "grad_norm": 0.08906880021095276,
      "learning_rate": 2.9720000000000003e-05,
      "loss": 0.0022,
      "step": 60840
    },
    {
      "epoch": 3.2453333333333334,
      "grad_norm": 0.2932358980178833,
      "learning_rate": 2.971666666666667e-05,
      "loss": 0.0018,
      "step": 60850
    },
    {
      "epoch": 3.2458666666666667,
      "grad_norm": 0.09179744124412537,
      "learning_rate": 2.971333333333334e-05,
      "loss": 0.0019,
      "step": 60860
    },
    {
      "epoch": 3.2464,
      "grad_norm": 0.11080443859100342,
      "learning_rate": 2.971e-05,
      "loss": 0.0019,
      "step": 60870
    },
    {
      "epoch": 3.2469333333333332,
      "grad_norm": 0.32178258895874023,
      "learning_rate": 2.9706666666666665e-05,
      "loss": 0.0014,
      "step": 60880
    },
    {
      "epoch": 3.2474666666666665,
      "grad_norm": 0.15794995427131653,
      "learning_rate": 2.9703333333333334e-05,
      "loss": 0.0027,
      "step": 60890
    },
    {
      "epoch": 3.248,
      "grad_norm": 0.0446404404938221,
      "learning_rate": 2.97e-05,
      "loss": 0.0019,
      "step": 60900
    },
    {
      "epoch": 3.2485333333333335,
      "grad_norm": 0.17016609013080597,
      "learning_rate": 2.9696666666666666e-05,
      "loss": 0.0026,
      "step": 60910
    },
    {
      "epoch": 3.2490666666666668,
      "grad_norm": 0.5413391590118408,
      "learning_rate": 2.9693333333333333e-05,
      "loss": 0.0018,
      "step": 60920
    },
    {
      "epoch": 3.2496,
      "grad_norm": 0.8841740489006042,
      "learning_rate": 2.9690000000000002e-05,
      "loss": 0.0018,
      "step": 60930
    },
    {
      "epoch": 3.2501333333333333,
      "grad_norm": 0.04882035404443741,
      "learning_rate": 2.9686666666666668e-05,
      "loss": 0.002,
      "step": 60940
    },
    {
      "epoch": 3.2506666666666666,
      "grad_norm": 0.3665836751461029,
      "learning_rate": 2.9683333333333334e-05,
      "loss": 0.0026,
      "step": 60950
    },
    {
      "epoch": 3.2512,
      "grad_norm": 0.09527803212404251,
      "learning_rate": 2.9680000000000004e-05,
      "loss": 0.0022,
      "step": 60960
    },
    {
      "epoch": 3.251733333333333,
      "grad_norm": 0.29374808073043823,
      "learning_rate": 2.967666666666667e-05,
      "loss": 0.0018,
      "step": 60970
    },
    {
      "epoch": 3.2522666666666664,
      "grad_norm": 0.14560334384441376,
      "learning_rate": 2.9673333333333336e-05,
      "loss": 0.0023,
      "step": 60980
    },
    {
      "epoch": 3.2528,
      "grad_norm": 0.2624529004096985,
      "learning_rate": 2.9670000000000002e-05,
      "loss": 0.0026,
      "step": 60990
    },
    {
      "epoch": 3.2533333333333334,
      "grad_norm": 0.31831666827201843,
      "learning_rate": 2.9666666666666672e-05,
      "loss": 0.0034,
      "step": 61000
    },
    {
      "epoch": 3.2538666666666667,
      "grad_norm": 0.19397315382957458,
      "learning_rate": 2.9663333333333338e-05,
      "loss": 0.0033,
      "step": 61010
    },
    {
      "epoch": 3.2544,
      "grad_norm": 0.33174410462379456,
      "learning_rate": 2.9659999999999997e-05,
      "loss": 0.0026,
      "step": 61020
    },
    {
      "epoch": 3.2549333333333332,
      "grad_norm": 0.22289715707302094,
      "learning_rate": 2.9656666666666667e-05,
      "loss": 0.0024,
      "step": 61030
    },
    {
      "epoch": 3.2554666666666665,
      "grad_norm": 0.3919958770275116,
      "learning_rate": 2.9653333333333333e-05,
      "loss": 0.0023,
      "step": 61040
    },
    {
      "epoch": 3.2560000000000002,
      "grad_norm": 0.3616895079612732,
      "learning_rate": 2.965e-05,
      "loss": 0.0037,
      "step": 61050
    },
    {
      "epoch": 3.2565333333333335,
      "grad_norm": 0.18973201513290405,
      "learning_rate": 2.964666666666667e-05,
      "loss": 0.0015,
      "step": 61060
    },
    {
      "epoch": 3.2570666666666668,
      "grad_norm": 0.318508118391037,
      "learning_rate": 2.9643333333333335e-05,
      "loss": 0.0026,
      "step": 61070
    },
    {
      "epoch": 3.2576,
      "grad_norm": 0.021290220320224762,
      "learning_rate": 2.964e-05,
      "loss": 0.0022,
      "step": 61080
    },
    {
      "epoch": 3.2581333333333333,
      "grad_norm": 0.1640177220106125,
      "learning_rate": 2.9636666666666667e-05,
      "loss": 0.0031,
      "step": 61090
    },
    {
      "epoch": 3.2586666666666666,
      "grad_norm": 0.09483986347913742,
      "learning_rate": 2.9633333333333336e-05,
      "loss": 0.0016,
      "step": 61100
    },
    {
      "epoch": 3.2592,
      "grad_norm": 0.31782016158103943,
      "learning_rate": 2.9630000000000003e-05,
      "loss": 0.0018,
      "step": 61110
    },
    {
      "epoch": 3.259733333333333,
      "grad_norm": 0.1188693568110466,
      "learning_rate": 2.962666666666667e-05,
      "loss": 0.0021,
      "step": 61120
    },
    {
      "epoch": 3.2602666666666664,
      "grad_norm": 0.22021900117397308,
      "learning_rate": 2.9623333333333335e-05,
      "loss": 0.0015,
      "step": 61130
    },
    {
      "epoch": 3.2608,
      "grad_norm": 0.3274771571159363,
      "learning_rate": 2.9620000000000004e-05,
      "loss": 0.0018,
      "step": 61140
    },
    {
      "epoch": 3.2613333333333334,
      "grad_norm": 0.06562116742134094,
      "learning_rate": 2.961666666666667e-05,
      "loss": 0.0021,
      "step": 61150
    },
    {
      "epoch": 3.2618666666666667,
      "grad_norm": 0.1309049427509308,
      "learning_rate": 2.9613333333333337e-05,
      "loss": 0.0034,
      "step": 61160
    },
    {
      "epoch": 3.2624,
      "grad_norm": 0.1253780573606491,
      "learning_rate": 2.961e-05,
      "loss": 0.0035,
      "step": 61170
    },
    {
      "epoch": 3.2629333333333332,
      "grad_norm": 0.1233956515789032,
      "learning_rate": 2.9606666666666666e-05,
      "loss": 0.0028,
      "step": 61180
    },
    {
      "epoch": 3.2634666666666665,
      "grad_norm": 0.2037055790424347,
      "learning_rate": 2.960333333333333e-05,
      "loss": 0.002,
      "step": 61190
    },
    {
      "epoch": 3.2640000000000002,
      "grad_norm": 0.23199757933616638,
      "learning_rate": 2.96e-05,
      "loss": 0.0017,
      "step": 61200
    },
    {
      "epoch": 3.2645333333333335,
      "grad_norm": 0.21009816229343414,
      "learning_rate": 2.9596666666666667e-05,
      "loss": 0.0036,
      "step": 61210
    },
    {
      "epoch": 3.265066666666667,
      "grad_norm": 0.13587534427642822,
      "learning_rate": 2.9593333333333333e-05,
      "loss": 0.0033,
      "step": 61220
    },
    {
      "epoch": 3.2656,
      "grad_norm": 0.09059393405914307,
      "learning_rate": 2.959e-05,
      "loss": 0.0026,
      "step": 61230
    },
    {
      "epoch": 3.2661333333333333,
      "grad_norm": 0.41255199909210205,
      "learning_rate": 2.958666666666667e-05,
      "loss": 0.0022,
      "step": 61240
    },
    {
      "epoch": 3.2666666666666666,
      "grad_norm": 0.45650678873062134,
      "learning_rate": 2.9583333333333335e-05,
      "loss": 0.0023,
      "step": 61250
    },
    {
      "epoch": 3.2672,
      "grad_norm": 0.04021812975406647,
      "learning_rate": 2.958e-05,
      "loss": 0.0023,
      "step": 61260
    },
    {
      "epoch": 3.267733333333333,
      "grad_norm": 0.06190330162644386,
      "learning_rate": 2.9576666666666668e-05,
      "loss": 0.0023,
      "step": 61270
    },
    {
      "epoch": 3.2682666666666664,
      "grad_norm": 0.3214639723300934,
      "learning_rate": 2.9573333333333337e-05,
      "loss": 0.0019,
      "step": 61280
    },
    {
      "epoch": 3.2688,
      "grad_norm": 0.209462508559227,
      "learning_rate": 2.9570000000000003e-05,
      "loss": 0.0026,
      "step": 61290
    },
    {
      "epoch": 3.2693333333333334,
      "grad_norm": 0.4354289770126343,
      "learning_rate": 2.956666666666667e-05,
      "loss": 0.0019,
      "step": 61300
    },
    {
      "epoch": 3.2698666666666667,
      "grad_norm": 0.32525163888931274,
      "learning_rate": 2.956333333333334e-05,
      "loss": 0.0019,
      "step": 61310
    },
    {
      "epoch": 3.2704,
      "grad_norm": 0.20481717586517334,
      "learning_rate": 2.9559999999999998e-05,
      "loss": 0.0018,
      "step": 61320
    },
    {
      "epoch": 3.2709333333333332,
      "grad_norm": 0.15375952422618866,
      "learning_rate": 2.9556666666666664e-05,
      "loss": 0.002,
      "step": 61330
    },
    {
      "epoch": 3.2714666666666665,
      "grad_norm": 0.29480311274528503,
      "learning_rate": 2.9553333333333334e-05,
      "loss": 0.0016,
      "step": 61340
    },
    {
      "epoch": 3.2720000000000002,
      "grad_norm": 0.09292055666446686,
      "learning_rate": 2.955e-05,
      "loss": 0.0022,
      "step": 61350
    },
    {
      "epoch": 3.2725333333333335,
      "grad_norm": 0.1544989049434662,
      "learning_rate": 2.9546666666666666e-05,
      "loss": 0.0023,
      "step": 61360
    },
    {
      "epoch": 3.273066666666667,
      "grad_norm": 0.12109078466892242,
      "learning_rate": 2.9543333333333336e-05,
      "loss": 0.0017,
      "step": 61370
    },
    {
      "epoch": 3.2736,
      "grad_norm": 0.30795225501060486,
      "learning_rate": 2.9540000000000002e-05,
      "loss": 0.0022,
      "step": 61380
    },
    {
      "epoch": 3.2741333333333333,
      "grad_norm": 0.20775090157985687,
      "learning_rate": 2.9536666666666668e-05,
      "loss": 0.0026,
      "step": 61390
    },
    {
      "epoch": 3.2746666666666666,
      "grad_norm": 0.44166696071624756,
      "learning_rate": 2.9533333333333334e-05,
      "loss": 0.0018,
      "step": 61400
    },
    {
      "epoch": 3.2752,
      "grad_norm": 0.26336175203323364,
      "learning_rate": 2.9530000000000004e-05,
      "loss": 0.0018,
      "step": 61410
    },
    {
      "epoch": 3.275733333333333,
      "grad_norm": 0.3541523814201355,
      "learning_rate": 2.952666666666667e-05,
      "loss": 0.0023,
      "step": 61420
    },
    {
      "epoch": 3.2762666666666664,
      "grad_norm": 0.18599161505699158,
      "learning_rate": 2.9523333333333336e-05,
      "loss": 0.0013,
      "step": 61430
    },
    {
      "epoch": 3.2768,
      "grad_norm": 0.23479725420475006,
      "learning_rate": 2.9520000000000002e-05,
      "loss": 0.0037,
      "step": 61440
    },
    {
      "epoch": 3.2773333333333334,
      "grad_norm": 0.29799121618270874,
      "learning_rate": 2.951666666666667e-05,
      "loss": 0.0024,
      "step": 61450
    },
    {
      "epoch": 3.2778666666666667,
      "grad_norm": 0.6824808716773987,
      "learning_rate": 2.9513333333333338e-05,
      "loss": 0.0021,
      "step": 61460
    },
    {
      "epoch": 3.2784,
      "grad_norm": 0.11761825531721115,
      "learning_rate": 2.951e-05,
      "loss": 0.0027,
      "step": 61470
    },
    {
      "epoch": 3.2789333333333333,
      "grad_norm": 0.24513468146324158,
      "learning_rate": 2.9506666666666667e-05,
      "loss": 0.0017,
      "step": 61480
    },
    {
      "epoch": 3.2794666666666665,
      "grad_norm": 0.6187862753868103,
      "learning_rate": 2.9503333333333333e-05,
      "loss": 0.0016,
      "step": 61490
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 0.14898714423179626,
      "learning_rate": 2.95e-05,
      "loss": 0.0036,
      "step": 61500
    },
    {
      "epoch": 3.2805333333333335,
      "grad_norm": 0.06258974969387054,
      "learning_rate": 2.9496666666666668e-05,
      "loss": 0.0018,
      "step": 61510
    },
    {
      "epoch": 3.281066666666667,
      "grad_norm": 0.19569769501686096,
      "learning_rate": 2.9493333333333334e-05,
      "loss": 0.0025,
      "step": 61520
    },
    {
      "epoch": 3.2816,
      "grad_norm": 0.18661311268806458,
      "learning_rate": 2.949e-05,
      "loss": 0.0033,
      "step": 61530
    },
    {
      "epoch": 3.2821333333333333,
      "grad_norm": 0.26053091883659363,
      "learning_rate": 2.9486666666666667e-05,
      "loss": 0.0033,
      "step": 61540
    },
    {
      "epoch": 3.2826666666666666,
      "grad_norm": 0.23360320925712585,
      "learning_rate": 2.9483333333333336e-05,
      "loss": 0.0025,
      "step": 61550
    },
    {
      "epoch": 3.2832,
      "grad_norm": 0.30109864473342896,
      "learning_rate": 2.9480000000000002e-05,
      "loss": 0.0026,
      "step": 61560
    },
    {
      "epoch": 3.283733333333333,
      "grad_norm": 0.298462450504303,
      "learning_rate": 2.947666666666667e-05,
      "loss": 0.0018,
      "step": 61570
    },
    {
      "epoch": 3.2842666666666664,
      "grad_norm": 0.14944517612457275,
      "learning_rate": 2.9473333333333335e-05,
      "loss": 0.0022,
      "step": 61580
    },
    {
      "epoch": 3.2848,
      "grad_norm": 0.32185912132263184,
      "learning_rate": 2.9470000000000004e-05,
      "loss": 0.0017,
      "step": 61590
    },
    {
      "epoch": 3.2853333333333334,
      "grad_norm": 0.5361363887786865,
      "learning_rate": 2.946666666666667e-05,
      "loss": 0.0027,
      "step": 61600
    },
    {
      "epoch": 3.2858666666666667,
      "grad_norm": 0.05338491126894951,
      "learning_rate": 2.9463333333333336e-05,
      "loss": 0.0019,
      "step": 61610
    },
    {
      "epoch": 3.2864,
      "grad_norm": 0.12839263677597046,
      "learning_rate": 2.946e-05,
      "loss": 0.0017,
      "step": 61620
    },
    {
      "epoch": 3.2869333333333333,
      "grad_norm": 0.6487234234809875,
      "learning_rate": 2.9456666666666665e-05,
      "loss": 0.0018,
      "step": 61630
    },
    {
      "epoch": 3.2874666666666665,
      "grad_norm": 0.41490107774734497,
      "learning_rate": 2.945333333333333e-05,
      "loss": 0.0023,
      "step": 61640
    },
    {
      "epoch": 3.288,
      "grad_norm": 0.060137491673231125,
      "learning_rate": 2.945e-05,
      "loss": 0.0029,
      "step": 61650
    },
    {
      "epoch": 3.2885333333333335,
      "grad_norm": 0.48028841614723206,
      "learning_rate": 2.9446666666666667e-05,
      "loss": 0.0025,
      "step": 61660
    },
    {
      "epoch": 3.289066666666667,
      "grad_norm": 0.28187403082847595,
      "learning_rate": 2.9443333333333333e-05,
      "loss": 0.0033,
      "step": 61670
    },
    {
      "epoch": 3.2896,
      "grad_norm": 0.5599066019058228,
      "learning_rate": 2.944e-05,
      "loss": 0.0022,
      "step": 61680
    },
    {
      "epoch": 3.2901333333333334,
      "grad_norm": 0.10246118903160095,
      "learning_rate": 2.943666666666667e-05,
      "loss": 0.0027,
      "step": 61690
    },
    {
      "epoch": 3.2906666666666666,
      "grad_norm": 0.06256198137998581,
      "learning_rate": 2.9433333333333335e-05,
      "loss": 0.0032,
      "step": 61700
    },
    {
      "epoch": 3.2912,
      "grad_norm": 0.15923306345939636,
      "learning_rate": 2.943e-05,
      "loss": 0.0017,
      "step": 61710
    },
    {
      "epoch": 3.291733333333333,
      "grad_norm": 0.26330995559692383,
      "learning_rate": 2.942666666666667e-05,
      "loss": 0.0021,
      "step": 61720
    },
    {
      "epoch": 3.2922666666666665,
      "grad_norm": 0.12171446532011032,
      "learning_rate": 2.9423333333333337e-05,
      "loss": 0.0027,
      "step": 61730
    },
    {
      "epoch": 3.2928,
      "grad_norm": 0.6492378115653992,
      "learning_rate": 2.9420000000000003e-05,
      "loss": 0.0027,
      "step": 61740
    },
    {
      "epoch": 3.2933333333333334,
      "grad_norm": 0.7207192778587341,
      "learning_rate": 2.941666666666667e-05,
      "loss": 0.0025,
      "step": 61750
    },
    {
      "epoch": 3.2938666666666667,
      "grad_norm": 0.1442677080631256,
      "learning_rate": 2.941333333333334e-05,
      "loss": 0.0019,
      "step": 61760
    },
    {
      "epoch": 3.2944,
      "grad_norm": 0.27401837706565857,
      "learning_rate": 2.9409999999999998e-05,
      "loss": 0.002,
      "step": 61770
    },
    {
      "epoch": 3.2949333333333333,
      "grad_norm": 0.3272104263305664,
      "learning_rate": 2.9406666666666664e-05,
      "loss": 0.0018,
      "step": 61780
    },
    {
      "epoch": 3.2954666666666665,
      "grad_norm": 0.3300767242908478,
      "learning_rate": 2.9403333333333334e-05,
      "loss": 0.0033,
      "step": 61790
    },
    {
      "epoch": 3.296,
      "grad_norm": 0.24115362763404846,
      "learning_rate": 2.94e-05,
      "loss": 0.0019,
      "step": 61800
    },
    {
      "epoch": 3.2965333333333335,
      "grad_norm": 0.44339677691459656,
      "learning_rate": 2.9396666666666666e-05,
      "loss": 0.0029,
      "step": 61810
    },
    {
      "epoch": 3.297066666666667,
      "grad_norm": 0.13701024651527405,
      "learning_rate": 2.9393333333333335e-05,
      "loss": 0.0028,
      "step": 61820
    },
    {
      "epoch": 3.2976,
      "grad_norm": 0.12077711522579193,
      "learning_rate": 2.939e-05,
      "loss": 0.0027,
      "step": 61830
    },
    {
      "epoch": 3.2981333333333334,
      "grad_norm": 0.3242637515068054,
      "learning_rate": 2.9386666666666668e-05,
      "loss": 0.0016,
      "step": 61840
    },
    {
      "epoch": 3.2986666666666666,
      "grad_norm": 0.17792271077632904,
      "learning_rate": 2.9383333333333334e-05,
      "loss": 0.0028,
      "step": 61850
    },
    {
      "epoch": 3.2992,
      "grad_norm": 0.12466877698898315,
      "learning_rate": 2.9380000000000003e-05,
      "loss": 0.0031,
      "step": 61860
    },
    {
      "epoch": 3.299733333333333,
      "grad_norm": 0.06443632394075394,
      "learning_rate": 2.937666666666667e-05,
      "loss": 0.0022,
      "step": 61870
    },
    {
      "epoch": 3.3002666666666665,
      "grad_norm": 0.11929570138454437,
      "learning_rate": 2.9373333333333336e-05,
      "loss": 0.0024,
      "step": 61880
    },
    {
      "epoch": 3.3008,
      "grad_norm": 0.12726418673992157,
      "learning_rate": 2.9370000000000002e-05,
      "loss": 0.0025,
      "step": 61890
    },
    {
      "epoch": 3.3013333333333335,
      "grad_norm": 0.0929364338517189,
      "learning_rate": 2.936666666666667e-05,
      "loss": 0.0019,
      "step": 61900
    },
    {
      "epoch": 3.3018666666666667,
      "grad_norm": 0.23982666432857513,
      "learning_rate": 2.9363333333333337e-05,
      "loss": 0.0028,
      "step": 61910
    },
    {
      "epoch": 3.3024,
      "grad_norm": 0.10947198420763016,
      "learning_rate": 2.9360000000000003e-05,
      "loss": 0.0029,
      "step": 61920
    },
    {
      "epoch": 3.3029333333333333,
      "grad_norm": 0.3005509674549103,
      "learning_rate": 2.9356666666666666e-05,
      "loss": 0.0032,
      "step": 61930
    },
    {
      "epoch": 3.3034666666666666,
      "grad_norm": 0.37081220746040344,
      "learning_rate": 2.9353333333333332e-05,
      "loss": 0.0027,
      "step": 61940
    },
    {
      "epoch": 3.304,
      "grad_norm": 0.5841063857078552,
      "learning_rate": 2.935e-05,
      "loss": 0.0017,
      "step": 61950
    },
    {
      "epoch": 3.3045333333333335,
      "grad_norm": 0.46920594573020935,
      "learning_rate": 2.9346666666666668e-05,
      "loss": 0.0033,
      "step": 61960
    },
    {
      "epoch": 3.305066666666667,
      "grad_norm": 0.06557033210992813,
      "learning_rate": 2.9343333333333334e-05,
      "loss": 0.0019,
      "step": 61970
    },
    {
      "epoch": 3.3056,
      "grad_norm": 0.17369428277015686,
      "learning_rate": 2.934e-05,
      "loss": 0.0022,
      "step": 61980
    },
    {
      "epoch": 3.3061333333333334,
      "grad_norm": 0.11786843091249466,
      "learning_rate": 2.9336666666666666e-05,
      "loss": 0.0025,
      "step": 61990
    },
    {
      "epoch": 3.3066666666666666,
      "grad_norm": 0.3231354057788849,
      "learning_rate": 2.9333333333333336e-05,
      "loss": 0.0018,
      "step": 62000
    },
    {
      "epoch": 3.3072,
      "grad_norm": 0.13257387280464172,
      "learning_rate": 2.9330000000000002e-05,
      "loss": 0.0037,
      "step": 62010
    },
    {
      "epoch": 3.307733333333333,
      "grad_norm": 0.4873799681663513,
      "learning_rate": 2.9326666666666668e-05,
      "loss": 0.0028,
      "step": 62020
    },
    {
      "epoch": 3.3082666666666665,
      "grad_norm": 0.30055657029151917,
      "learning_rate": 2.9323333333333334e-05,
      "loss": 0.0016,
      "step": 62030
    },
    {
      "epoch": 3.3088,
      "grad_norm": 0.4842911660671234,
      "learning_rate": 2.9320000000000004e-05,
      "loss": 0.0023,
      "step": 62040
    },
    {
      "epoch": 3.3093333333333335,
      "grad_norm": 0.5057777762413025,
      "learning_rate": 2.931666666666667e-05,
      "loss": 0.0016,
      "step": 62050
    },
    {
      "epoch": 3.3098666666666667,
      "grad_norm": 0.4421267509460449,
      "learning_rate": 2.9313333333333336e-05,
      "loss": 0.0026,
      "step": 62060
    },
    {
      "epoch": 3.3104,
      "grad_norm": 0.12499279528856277,
      "learning_rate": 2.9310000000000006e-05,
      "loss": 0.0023,
      "step": 62070
    },
    {
      "epoch": 3.3109333333333333,
      "grad_norm": 0.1435926854610443,
      "learning_rate": 2.9306666666666665e-05,
      "loss": 0.0027,
      "step": 62080
    },
    {
      "epoch": 3.3114666666666666,
      "grad_norm": 0.4419609308242798,
      "learning_rate": 2.930333333333333e-05,
      "loss": 0.0019,
      "step": 62090
    },
    {
      "epoch": 3.312,
      "grad_norm": 0.217813178896904,
      "learning_rate": 2.93e-05,
      "loss": 0.0031,
      "step": 62100
    },
    {
      "epoch": 3.3125333333333336,
      "grad_norm": 0.5342287421226501,
      "learning_rate": 2.9296666666666667e-05,
      "loss": 0.0021,
      "step": 62110
    },
    {
      "epoch": 3.313066666666667,
      "grad_norm": 0.09398599714040756,
      "learning_rate": 2.9293333333333333e-05,
      "loss": 0.0016,
      "step": 62120
    },
    {
      "epoch": 3.3136,
      "grad_norm": 0.275077223777771,
      "learning_rate": 2.929e-05,
      "loss": 0.0015,
      "step": 62130
    },
    {
      "epoch": 3.3141333333333334,
      "grad_norm": 0.5974372029304504,
      "learning_rate": 2.928666666666667e-05,
      "loss": 0.0026,
      "step": 62140
    },
    {
      "epoch": 3.3146666666666667,
      "grad_norm": 0.06650882214307785,
      "learning_rate": 2.9283333333333335e-05,
      "loss": 0.0025,
      "step": 62150
    },
    {
      "epoch": 3.3152,
      "grad_norm": 0.17878343164920807,
      "learning_rate": 2.928e-05,
      "loss": 0.002,
      "step": 62160
    },
    {
      "epoch": 3.315733333333333,
      "grad_norm": 1.160735845565796,
      "learning_rate": 2.927666666666667e-05,
      "loss": 0.0026,
      "step": 62170
    },
    {
      "epoch": 3.3162666666666665,
      "grad_norm": 0.538177490234375,
      "learning_rate": 2.9273333333333337e-05,
      "loss": 0.0021,
      "step": 62180
    },
    {
      "epoch": 3.3168,
      "grad_norm": 0.06547902524471283,
      "learning_rate": 2.9270000000000003e-05,
      "loss": 0.0025,
      "step": 62190
    },
    {
      "epoch": 3.3173333333333335,
      "grad_norm": 0.16337774693965912,
      "learning_rate": 2.926666666666667e-05,
      "loss": 0.0017,
      "step": 62200
    },
    {
      "epoch": 3.3178666666666667,
      "grad_norm": 0.4710521697998047,
      "learning_rate": 2.926333333333334e-05,
      "loss": 0.0036,
      "step": 62210
    },
    {
      "epoch": 3.3184,
      "grad_norm": 0.08586560189723969,
      "learning_rate": 2.9260000000000004e-05,
      "loss": 0.0019,
      "step": 62220
    },
    {
      "epoch": 3.3189333333333333,
      "grad_norm": 0.32541918754577637,
      "learning_rate": 2.9256666666666667e-05,
      "loss": 0.0026,
      "step": 62230
    },
    {
      "epoch": 3.3194666666666666,
      "grad_norm": 0.09509715437889099,
      "learning_rate": 2.9253333333333333e-05,
      "loss": 0.0017,
      "step": 62240
    },
    {
      "epoch": 3.32,
      "grad_norm": 0.20695807039737701,
      "learning_rate": 2.925e-05,
      "loss": 0.0025,
      "step": 62250
    },
    {
      "epoch": 3.3205333333333336,
      "grad_norm": 0.09464086592197418,
      "learning_rate": 2.9246666666666666e-05,
      "loss": 0.0017,
      "step": 62260
    },
    {
      "epoch": 3.321066666666667,
      "grad_norm": 0.3286162316799164,
      "learning_rate": 2.9243333333333335e-05,
      "loss": 0.0022,
      "step": 62270
    },
    {
      "epoch": 3.3216,
      "grad_norm": 0.3897654712200165,
      "learning_rate": 2.924e-05,
      "loss": 0.0025,
      "step": 62280
    },
    {
      "epoch": 3.3221333333333334,
      "grad_norm": 0.17661556601524353,
      "learning_rate": 2.9236666666666667e-05,
      "loss": 0.0018,
      "step": 62290
    },
    {
      "epoch": 3.3226666666666667,
      "grad_norm": 0.3903476297855377,
      "learning_rate": 2.9233333333333334e-05,
      "loss": 0.0026,
      "step": 62300
    },
    {
      "epoch": 3.3232,
      "grad_norm": 0.036650944501161575,
      "learning_rate": 2.9230000000000003e-05,
      "loss": 0.0024,
      "step": 62310
    },
    {
      "epoch": 3.323733333333333,
      "grad_norm": 0.5258021950721741,
      "learning_rate": 2.922666666666667e-05,
      "loss": 0.0026,
      "step": 62320
    },
    {
      "epoch": 3.3242666666666665,
      "grad_norm": 0.4648568332195282,
      "learning_rate": 2.9223333333333335e-05,
      "loss": 0.0028,
      "step": 62330
    },
    {
      "epoch": 3.3247999999999998,
      "grad_norm": 0.31410732865333557,
      "learning_rate": 2.922e-05,
      "loss": 0.002,
      "step": 62340
    },
    {
      "epoch": 3.3253333333333335,
      "grad_norm": 0.2698919475078583,
      "learning_rate": 2.921666666666667e-05,
      "loss": 0.0029,
      "step": 62350
    },
    {
      "epoch": 3.3258666666666667,
      "grad_norm": 0.287837952375412,
      "learning_rate": 2.9213333333333337e-05,
      "loss": 0.0024,
      "step": 62360
    },
    {
      "epoch": 3.3264,
      "grad_norm": 0.33291715383529663,
      "learning_rate": 2.9210000000000003e-05,
      "loss": 0.0025,
      "step": 62370
    },
    {
      "epoch": 3.3269333333333333,
      "grad_norm": 0.17531411349773407,
      "learning_rate": 2.9206666666666666e-05,
      "loss": 0.0029,
      "step": 62380
    },
    {
      "epoch": 3.3274666666666666,
      "grad_norm": 0.03292219713330269,
      "learning_rate": 2.9203333333333332e-05,
      "loss": 0.0019,
      "step": 62390
    },
    {
      "epoch": 3.328,
      "grad_norm": 0.09780226647853851,
      "learning_rate": 2.9199999999999998e-05,
      "loss": 0.0025,
      "step": 62400
    },
    {
      "epoch": 3.3285333333333336,
      "grad_norm": 0.18128237128257751,
      "learning_rate": 2.9196666666666668e-05,
      "loss": 0.0025,
      "step": 62410
    },
    {
      "epoch": 3.329066666666667,
      "grad_norm": 0.27729177474975586,
      "learning_rate": 2.9193333333333334e-05,
      "loss": 0.0017,
      "step": 62420
    },
    {
      "epoch": 3.3296,
      "grad_norm": 0.23817512392997742,
      "learning_rate": 2.919e-05,
      "loss": 0.0025,
      "step": 62430
    },
    {
      "epoch": 3.3301333333333334,
      "grad_norm": 0.10096710920333862,
      "learning_rate": 2.9186666666666666e-05,
      "loss": 0.0023,
      "step": 62440
    },
    {
      "epoch": 3.3306666666666667,
      "grad_norm": 0.29518646001815796,
      "learning_rate": 2.9183333333333336e-05,
      "loss": 0.0024,
      "step": 62450
    },
    {
      "epoch": 3.3312,
      "grad_norm": 0.3042360544204712,
      "learning_rate": 2.9180000000000002e-05,
      "loss": 0.0034,
      "step": 62460
    },
    {
      "epoch": 3.331733333333333,
      "grad_norm": 0.05668019875884056,
      "learning_rate": 2.9176666666666668e-05,
      "loss": 0.0027,
      "step": 62470
    },
    {
      "epoch": 3.3322666666666665,
      "grad_norm": 0.1773126870393753,
      "learning_rate": 2.9173333333333337e-05,
      "loss": 0.0014,
      "step": 62480
    },
    {
      "epoch": 3.3327999999999998,
      "grad_norm": 0.4129162132740021,
      "learning_rate": 2.9170000000000004e-05,
      "loss": 0.0021,
      "step": 62490
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.0681728944182396,
      "learning_rate": 2.916666666666667e-05,
      "loss": 0.0016,
      "step": 62500
    },
    {
      "epoch": 3.3338666666666668,
      "grad_norm": 0.07481948286294937,
      "learning_rate": 2.9163333333333336e-05,
      "loss": 0.0017,
      "step": 62510
    },
    {
      "epoch": 3.3344,
      "grad_norm": 0.12391620129346848,
      "learning_rate": 2.9160000000000005e-05,
      "loss": 0.002,
      "step": 62520
    },
    {
      "epoch": 3.3349333333333333,
      "grad_norm": 0.2319614738225937,
      "learning_rate": 2.9156666666666665e-05,
      "loss": 0.0023,
      "step": 62530
    },
    {
      "epoch": 3.3354666666666666,
      "grad_norm": 0.11007454991340637,
      "learning_rate": 2.915333333333333e-05,
      "loss": 0.0017,
      "step": 62540
    },
    {
      "epoch": 3.336,
      "grad_norm": 0.31739258766174316,
      "learning_rate": 2.915e-05,
      "loss": 0.0021,
      "step": 62550
    },
    {
      "epoch": 3.3365333333333336,
      "grad_norm": 0.4954533278942108,
      "learning_rate": 2.9146666666666667e-05,
      "loss": 0.0021,
      "step": 62560
    },
    {
      "epoch": 3.337066666666667,
      "grad_norm": 0.5089460015296936,
      "learning_rate": 2.9143333333333333e-05,
      "loss": 0.0027,
      "step": 62570
    },
    {
      "epoch": 3.3376,
      "grad_norm": 0.17264112830162048,
      "learning_rate": 2.9140000000000002e-05,
      "loss": 0.002,
      "step": 62580
    },
    {
      "epoch": 3.3381333333333334,
      "grad_norm": 0.14458461105823517,
      "learning_rate": 2.913666666666667e-05,
      "loss": 0.0028,
      "step": 62590
    },
    {
      "epoch": 3.3386666666666667,
      "grad_norm": 0.24138221144676208,
      "learning_rate": 2.9133333333333334e-05,
      "loss": 0.0022,
      "step": 62600
    },
    {
      "epoch": 3.3392,
      "grad_norm": 0.05437662452459335,
      "learning_rate": 2.913e-05,
      "loss": 0.0018,
      "step": 62610
    },
    {
      "epoch": 3.339733333333333,
      "grad_norm": 0.17512504756450653,
      "learning_rate": 2.912666666666667e-05,
      "loss": 0.0022,
      "step": 62620
    },
    {
      "epoch": 3.3402666666666665,
      "grad_norm": 0.5047348141670227,
      "learning_rate": 2.9123333333333336e-05,
      "loss": 0.0016,
      "step": 62630
    },
    {
      "epoch": 3.3407999999999998,
      "grad_norm": 0.36087337136268616,
      "learning_rate": 2.9120000000000002e-05,
      "loss": 0.0036,
      "step": 62640
    },
    {
      "epoch": 3.3413333333333335,
      "grad_norm": 0.08765332400798798,
      "learning_rate": 2.911666666666667e-05,
      "loss": 0.0027,
      "step": 62650
    },
    {
      "epoch": 3.3418666666666668,
      "grad_norm": 0.1815759688615799,
      "learning_rate": 2.9113333333333338e-05,
      "loss": 0.0033,
      "step": 62660
    },
    {
      "epoch": 3.3424,
      "grad_norm": 0.5225393176078796,
      "learning_rate": 2.9110000000000004e-05,
      "loss": 0.0018,
      "step": 62670
    },
    {
      "epoch": 3.3429333333333333,
      "grad_norm": 0.174141988158226,
      "learning_rate": 2.9106666666666667e-05,
      "loss": 0.0023,
      "step": 62680
    },
    {
      "epoch": 3.3434666666666666,
      "grad_norm": 0.267596036195755,
      "learning_rate": 2.9103333333333333e-05,
      "loss": 0.0025,
      "step": 62690
    },
    {
      "epoch": 3.344,
      "grad_norm": 0.11120346188545227,
      "learning_rate": 2.91e-05,
      "loss": 0.0033,
      "step": 62700
    },
    {
      "epoch": 3.3445333333333336,
      "grad_norm": 0.145529106259346,
      "learning_rate": 2.9096666666666665e-05,
      "loss": 0.0029,
      "step": 62710
    },
    {
      "epoch": 3.345066666666667,
      "grad_norm": 0.089591383934021,
      "learning_rate": 2.9093333333333335e-05,
      "loss": 0.0023,
      "step": 62720
    },
    {
      "epoch": 3.3456,
      "grad_norm": 0.066367007791996,
      "learning_rate": 2.909e-05,
      "loss": 0.0025,
      "step": 62730
    },
    {
      "epoch": 3.3461333333333334,
      "grad_norm": 0.5257940888404846,
      "learning_rate": 2.9086666666666667e-05,
      "loss": 0.0022,
      "step": 62740
    },
    {
      "epoch": 3.3466666666666667,
      "grad_norm": 0.5263479948043823,
      "learning_rate": 2.9083333333333333e-05,
      "loss": 0.0026,
      "step": 62750
    },
    {
      "epoch": 3.3472,
      "grad_norm": 0.20738838613033295,
      "learning_rate": 2.9080000000000003e-05,
      "loss": 0.0028,
      "step": 62760
    },
    {
      "epoch": 3.3477333333333332,
      "grad_norm": 0.1720721572637558,
      "learning_rate": 2.907666666666667e-05,
      "loss": 0.0013,
      "step": 62770
    },
    {
      "epoch": 3.3482666666666665,
      "grad_norm": 0.3040720224380493,
      "learning_rate": 2.9073333333333335e-05,
      "loss": 0.0027,
      "step": 62780
    },
    {
      "epoch": 3.3487999999999998,
      "grad_norm": 0.7776126861572266,
      "learning_rate": 2.907e-05,
      "loss": 0.0021,
      "step": 62790
    },
    {
      "epoch": 3.3493333333333335,
      "grad_norm": 0.0606420673429966,
      "learning_rate": 2.906666666666667e-05,
      "loss": 0.0026,
      "step": 62800
    },
    {
      "epoch": 3.3498666666666668,
      "grad_norm": 0.15948094427585602,
      "learning_rate": 2.9063333333333337e-05,
      "loss": 0.002,
      "step": 62810
    },
    {
      "epoch": 3.3504,
      "grad_norm": 0.6220660209655762,
      "learning_rate": 2.9060000000000003e-05,
      "loss": 0.0022,
      "step": 62820
    },
    {
      "epoch": 3.3509333333333333,
      "grad_norm": 0.07672370970249176,
      "learning_rate": 2.9056666666666666e-05,
      "loss": 0.0017,
      "step": 62830
    },
    {
      "epoch": 3.3514666666666666,
      "grad_norm": 0.09332312643527985,
      "learning_rate": 2.9053333333333332e-05,
      "loss": 0.0032,
      "step": 62840
    },
    {
      "epoch": 3.352,
      "grad_norm": 0.08947664499282837,
      "learning_rate": 2.9049999999999998e-05,
      "loss": 0.0024,
      "step": 62850
    },
    {
      "epoch": 3.352533333333333,
      "grad_norm": 0.24156080186367035,
      "learning_rate": 2.9046666666666668e-05,
      "loss": 0.0019,
      "step": 62860
    },
    {
      "epoch": 3.353066666666667,
      "grad_norm": 0.3362799882888794,
      "learning_rate": 2.9043333333333334e-05,
      "loss": 0.002,
      "step": 62870
    },
    {
      "epoch": 3.3536,
      "grad_norm": 0.17277398705482483,
      "learning_rate": 2.904e-05,
      "loss": 0.002,
      "step": 62880
    },
    {
      "epoch": 3.3541333333333334,
      "grad_norm": 0.2484211027622223,
      "learning_rate": 2.9036666666666666e-05,
      "loss": 0.0019,
      "step": 62890
    },
    {
      "epoch": 3.3546666666666667,
      "grad_norm": 0.21452979743480682,
      "learning_rate": 2.9033333333333335e-05,
      "loss": 0.0024,
      "step": 62900
    },
    {
      "epoch": 3.3552,
      "grad_norm": 0.22122184932231903,
      "learning_rate": 2.903e-05,
      "loss": 0.0029,
      "step": 62910
    },
    {
      "epoch": 3.3557333333333332,
      "grad_norm": 0.18928927183151245,
      "learning_rate": 2.9026666666666668e-05,
      "loss": 0.002,
      "step": 62920
    },
    {
      "epoch": 3.3562666666666665,
      "grad_norm": 0.09378918260335922,
      "learning_rate": 2.9023333333333337e-05,
      "loss": 0.0026,
      "step": 62930
    },
    {
      "epoch": 3.3568,
      "grad_norm": 0.15783873200416565,
      "learning_rate": 2.9020000000000003e-05,
      "loss": 0.0028,
      "step": 62940
    },
    {
      "epoch": 3.3573333333333335,
      "grad_norm": 0.03614174202084541,
      "learning_rate": 2.901666666666667e-05,
      "loss": 0.0019,
      "step": 62950
    },
    {
      "epoch": 3.3578666666666668,
      "grad_norm": 0.21118038892745972,
      "learning_rate": 2.9013333333333336e-05,
      "loss": 0.0019,
      "step": 62960
    },
    {
      "epoch": 3.3584,
      "grad_norm": 0.09660981595516205,
      "learning_rate": 2.9010000000000005e-05,
      "loss": 0.0018,
      "step": 62970
    },
    {
      "epoch": 3.3589333333333333,
      "grad_norm": 0.5075163841247559,
      "learning_rate": 2.9006666666666665e-05,
      "loss": 0.0025,
      "step": 62980
    },
    {
      "epoch": 3.3594666666666666,
      "grad_norm": 0.028294973075389862,
      "learning_rate": 2.9003333333333334e-05,
      "loss": 0.0019,
      "step": 62990
    },
    {
      "epoch": 3.36,
      "grad_norm": 0.21352599561214447,
      "learning_rate": 2.9e-05,
      "loss": 0.0017,
      "step": 63000
    },
    {
      "epoch": 3.360533333333333,
      "grad_norm": 0.7083415985107422,
      "learning_rate": 2.8996666666666666e-05,
      "loss": 0.0024,
      "step": 63010
    },
    {
      "epoch": 3.361066666666667,
      "grad_norm": 0.14963896572589874,
      "learning_rate": 2.8993333333333332e-05,
      "loss": 0.002,
      "step": 63020
    },
    {
      "epoch": 3.3616,
      "grad_norm": 0.06855539977550507,
      "learning_rate": 2.8990000000000002e-05,
      "loss": 0.0019,
      "step": 63030
    },
    {
      "epoch": 3.3621333333333334,
      "grad_norm": 0.33750519156455994,
      "learning_rate": 2.8986666666666668e-05,
      "loss": 0.0026,
      "step": 63040
    },
    {
      "epoch": 3.3626666666666667,
      "grad_norm": 0.30946677923202515,
      "learning_rate": 2.8983333333333334e-05,
      "loss": 0.0026,
      "step": 63050
    },
    {
      "epoch": 3.3632,
      "grad_norm": 0.2665165066719055,
      "learning_rate": 2.898e-05,
      "loss": 0.0029,
      "step": 63060
    },
    {
      "epoch": 3.3637333333333332,
      "grad_norm": 0.6178474426269531,
      "learning_rate": 2.897666666666667e-05,
      "loss": 0.0027,
      "step": 63070
    },
    {
      "epoch": 3.3642666666666665,
      "grad_norm": 0.238015815615654,
      "learning_rate": 2.8973333333333336e-05,
      "loss": 0.0024,
      "step": 63080
    },
    {
      "epoch": 3.3648,
      "grad_norm": 0.17671386897563934,
      "learning_rate": 2.8970000000000002e-05,
      "loss": 0.002,
      "step": 63090
    },
    {
      "epoch": 3.3653333333333335,
      "grad_norm": 0.46877726912498474,
      "learning_rate": 2.8966666666666668e-05,
      "loss": 0.0017,
      "step": 63100
    },
    {
      "epoch": 3.365866666666667,
      "grad_norm": 0.06238545477390289,
      "learning_rate": 2.8963333333333338e-05,
      "loss": 0.0024,
      "step": 63110
    },
    {
      "epoch": 3.3664,
      "grad_norm": 0.7585198283195496,
      "learning_rate": 2.8960000000000004e-05,
      "loss": 0.0023,
      "step": 63120
    },
    {
      "epoch": 3.3669333333333333,
      "grad_norm": 0.0929064229130745,
      "learning_rate": 2.895666666666667e-05,
      "loss": 0.0016,
      "step": 63130
    },
    {
      "epoch": 3.3674666666666666,
      "grad_norm": 0.44175106287002563,
      "learning_rate": 2.8953333333333333e-05,
      "loss": 0.0017,
      "step": 63140
    },
    {
      "epoch": 3.368,
      "grad_norm": 0.6161328554153442,
      "learning_rate": 2.895e-05,
      "loss": 0.0024,
      "step": 63150
    },
    {
      "epoch": 3.368533333333333,
      "grad_norm": 0.21615886688232422,
      "learning_rate": 2.8946666666666665e-05,
      "loss": 0.0023,
      "step": 63160
    },
    {
      "epoch": 3.369066666666667,
      "grad_norm": 0.2939625382423401,
      "learning_rate": 2.8943333333333335e-05,
      "loss": 0.0021,
      "step": 63170
    },
    {
      "epoch": 3.3696,
      "grad_norm": 0.07144313305616379,
      "learning_rate": 2.894e-05,
      "loss": 0.0028,
      "step": 63180
    },
    {
      "epoch": 3.3701333333333334,
      "grad_norm": 0.12634305655956268,
      "learning_rate": 2.8936666666666667e-05,
      "loss": 0.0017,
      "step": 63190
    },
    {
      "epoch": 3.3706666666666667,
      "grad_norm": 0.1804054230451584,
      "learning_rate": 2.8933333333333333e-05,
      "loss": 0.0018,
      "step": 63200
    },
    {
      "epoch": 3.3712,
      "grad_norm": 0.15663179755210876,
      "learning_rate": 2.8930000000000003e-05,
      "loss": 0.002,
      "step": 63210
    },
    {
      "epoch": 3.3717333333333332,
      "grad_norm": 0.1748473048210144,
      "learning_rate": 2.892666666666667e-05,
      "loss": 0.0026,
      "step": 63220
    },
    {
      "epoch": 3.3722666666666665,
      "grad_norm": 0.18988625705242157,
      "learning_rate": 2.8923333333333335e-05,
      "loss": 0.0025,
      "step": 63230
    },
    {
      "epoch": 3.3728,
      "grad_norm": 0.06712129712104797,
      "learning_rate": 2.8920000000000004e-05,
      "loss": 0.0025,
      "step": 63240
    },
    {
      "epoch": 3.3733333333333335,
      "grad_norm": 0.12883871793746948,
      "learning_rate": 2.891666666666667e-05,
      "loss": 0.0019,
      "step": 63250
    },
    {
      "epoch": 3.373866666666667,
      "grad_norm": 0.5028492212295532,
      "learning_rate": 2.8913333333333337e-05,
      "loss": 0.0029,
      "step": 63260
    },
    {
      "epoch": 3.3744,
      "grad_norm": 0.29642167687416077,
      "learning_rate": 2.8910000000000003e-05,
      "loss": 0.0022,
      "step": 63270
    },
    {
      "epoch": 3.3749333333333333,
      "grad_norm": 0.1812618523836136,
      "learning_rate": 2.8906666666666672e-05,
      "loss": 0.0021,
      "step": 63280
    },
    {
      "epoch": 3.3754666666666666,
      "grad_norm": 0.09025463461875916,
      "learning_rate": 2.890333333333333e-05,
      "loss": 0.0023,
      "step": 63290
    },
    {
      "epoch": 3.376,
      "grad_norm": 0.1533772498369217,
      "learning_rate": 2.8899999999999998e-05,
      "loss": 0.0017,
      "step": 63300
    },
    {
      "epoch": 3.376533333333333,
      "grad_norm": 0.09837622940540314,
      "learning_rate": 2.8896666666666667e-05,
      "loss": 0.0019,
      "step": 63310
    },
    {
      "epoch": 3.377066666666667,
      "grad_norm": 0.48037856817245483,
      "learning_rate": 2.8893333333333333e-05,
      "loss": 0.0021,
      "step": 63320
    },
    {
      "epoch": 3.3776,
      "grad_norm": 0.10538081079721451,
      "learning_rate": 2.889e-05,
      "loss": 0.0025,
      "step": 63330
    },
    {
      "epoch": 3.3781333333333334,
      "grad_norm": 0.21472255885601044,
      "learning_rate": 2.888666666666667e-05,
      "loss": 0.0022,
      "step": 63340
    },
    {
      "epoch": 3.3786666666666667,
      "grad_norm": 0.2286587357521057,
      "learning_rate": 2.8883333333333335e-05,
      "loss": 0.0029,
      "step": 63350
    },
    {
      "epoch": 3.3792,
      "grad_norm": 0.2069002389907837,
      "learning_rate": 2.888e-05,
      "loss": 0.0021,
      "step": 63360
    },
    {
      "epoch": 3.3797333333333333,
      "grad_norm": 0.36058273911476135,
      "learning_rate": 2.8876666666666667e-05,
      "loss": 0.0017,
      "step": 63370
    },
    {
      "epoch": 3.3802666666666665,
      "grad_norm": 0.20790810883045197,
      "learning_rate": 2.8873333333333337e-05,
      "loss": 0.0017,
      "step": 63380
    },
    {
      "epoch": 3.3808,
      "grad_norm": 0.146811842918396,
      "learning_rate": 2.8870000000000003e-05,
      "loss": 0.003,
      "step": 63390
    },
    {
      "epoch": 3.3813333333333335,
      "grad_norm": 0.14736545085906982,
      "learning_rate": 2.886666666666667e-05,
      "loss": 0.0018,
      "step": 63400
    },
    {
      "epoch": 3.381866666666667,
      "grad_norm": 0.294805109500885,
      "learning_rate": 2.8863333333333335e-05,
      "loss": 0.0025,
      "step": 63410
    },
    {
      "epoch": 3.3824,
      "grad_norm": 0.586003303527832,
      "learning_rate": 2.8860000000000005e-05,
      "loss": 0.0022,
      "step": 63420
    },
    {
      "epoch": 3.3829333333333333,
      "grad_norm": 0.15617765486240387,
      "learning_rate": 2.885666666666667e-05,
      "loss": 0.0014,
      "step": 63430
    },
    {
      "epoch": 3.3834666666666666,
      "grad_norm": 0.2366929054260254,
      "learning_rate": 2.8853333333333334e-05,
      "loss": 0.0017,
      "step": 63440
    },
    {
      "epoch": 3.384,
      "grad_norm": 0.47805526852607727,
      "learning_rate": 2.885e-05,
      "loss": 0.0022,
      "step": 63450
    },
    {
      "epoch": 3.384533333333333,
      "grad_norm": 0.18999572098255157,
      "learning_rate": 2.8846666666666666e-05,
      "loss": 0.0022,
      "step": 63460
    },
    {
      "epoch": 3.385066666666667,
      "grad_norm": 0.334635466337204,
      "learning_rate": 2.8843333333333332e-05,
      "loss": 0.0024,
      "step": 63470
    },
    {
      "epoch": 3.3856,
      "grad_norm": 0.03934018686413765,
      "learning_rate": 2.8840000000000002e-05,
      "loss": 0.003,
      "step": 63480
    },
    {
      "epoch": 3.3861333333333334,
      "grad_norm": 0.15121790766716003,
      "learning_rate": 2.8836666666666668e-05,
      "loss": 0.0025,
      "step": 63490
    },
    {
      "epoch": 3.3866666666666667,
      "grad_norm": 0.17809538543224335,
      "learning_rate": 2.8833333333333334e-05,
      "loss": 0.0025,
      "step": 63500
    },
    {
      "epoch": 3.3872,
      "grad_norm": 0.25386562943458557,
      "learning_rate": 2.883e-05,
      "loss": 0.0024,
      "step": 63510
    },
    {
      "epoch": 3.3877333333333333,
      "grad_norm": 0.04733679071068764,
      "learning_rate": 2.882666666666667e-05,
      "loss": 0.0024,
      "step": 63520
    },
    {
      "epoch": 3.3882666666666665,
      "grad_norm": 0.14583449065685272,
      "learning_rate": 2.8823333333333336e-05,
      "loss": 0.0026,
      "step": 63530
    },
    {
      "epoch": 3.3888,
      "grad_norm": 0.26615992188453674,
      "learning_rate": 2.8820000000000002e-05,
      "loss": 0.0027,
      "step": 63540
    },
    {
      "epoch": 3.389333333333333,
      "grad_norm": 0.5426516532897949,
      "learning_rate": 2.8816666666666668e-05,
      "loss": 0.0023,
      "step": 63550
    },
    {
      "epoch": 3.389866666666667,
      "grad_norm": 0.5013593435287476,
      "learning_rate": 2.8813333333333338e-05,
      "loss": 0.0021,
      "step": 63560
    },
    {
      "epoch": 3.3904,
      "grad_norm": 0.1747700721025467,
      "learning_rate": 2.8810000000000004e-05,
      "loss": 0.0027,
      "step": 63570
    },
    {
      "epoch": 3.3909333333333334,
      "grad_norm": 0.3308529853820801,
      "learning_rate": 2.880666666666667e-05,
      "loss": 0.0019,
      "step": 63580
    },
    {
      "epoch": 3.3914666666666666,
      "grad_norm": 0.24073243141174316,
      "learning_rate": 2.8803333333333333e-05,
      "loss": 0.0017,
      "step": 63590
    },
    {
      "epoch": 3.392,
      "grad_norm": 0.034547630697488785,
      "learning_rate": 2.88e-05,
      "loss": 0.0018,
      "step": 63600
    },
    {
      "epoch": 3.392533333333333,
      "grad_norm": 0.23397566378116608,
      "learning_rate": 2.8796666666666665e-05,
      "loss": 0.0024,
      "step": 63610
    },
    {
      "epoch": 3.393066666666667,
      "grad_norm": 0.30589163303375244,
      "learning_rate": 2.8793333333333334e-05,
      "loss": 0.0023,
      "step": 63620
    },
    {
      "epoch": 3.3936,
      "grad_norm": 0.24923110008239746,
      "learning_rate": 2.879e-05,
      "loss": 0.0018,
      "step": 63630
    },
    {
      "epoch": 3.3941333333333334,
      "grad_norm": 0.6152584552764893,
      "learning_rate": 2.8786666666666667e-05,
      "loss": 0.0024,
      "step": 63640
    },
    {
      "epoch": 3.3946666666666667,
      "grad_norm": 0.09173265099525452,
      "learning_rate": 2.8783333333333333e-05,
      "loss": 0.0027,
      "step": 63650
    },
    {
      "epoch": 3.3952,
      "grad_norm": 0.18244579434394836,
      "learning_rate": 2.8780000000000002e-05,
      "loss": 0.0016,
      "step": 63660
    },
    {
      "epoch": 3.3957333333333333,
      "grad_norm": 0.09335128217935562,
      "learning_rate": 2.877666666666667e-05,
      "loss": 0.0013,
      "step": 63670
    },
    {
      "epoch": 3.3962666666666665,
      "grad_norm": 0.5649939179420471,
      "learning_rate": 2.8773333333333335e-05,
      "loss": 0.0022,
      "step": 63680
    },
    {
      "epoch": 3.3968,
      "grad_norm": 0.10153881460428238,
      "learning_rate": 2.8770000000000004e-05,
      "loss": 0.0021,
      "step": 63690
    },
    {
      "epoch": 3.397333333333333,
      "grad_norm": 0.15401199460029602,
      "learning_rate": 2.876666666666667e-05,
      "loss": 0.0019,
      "step": 63700
    },
    {
      "epoch": 3.397866666666667,
      "grad_norm": 0.06326931715011597,
      "learning_rate": 2.8763333333333336e-05,
      "loss": 0.0013,
      "step": 63710
    },
    {
      "epoch": 3.3984,
      "grad_norm": 0.07739681750535965,
      "learning_rate": 2.8760000000000002e-05,
      "loss": 0.0029,
      "step": 63720
    },
    {
      "epoch": 3.3989333333333334,
      "grad_norm": 0.3882506489753723,
      "learning_rate": 2.8756666666666672e-05,
      "loss": 0.0023,
      "step": 63730
    },
    {
      "epoch": 3.3994666666666666,
      "grad_norm": 0.18155314028263092,
      "learning_rate": 2.875333333333333e-05,
      "loss": 0.0023,
      "step": 63740
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.21432799100875854,
      "learning_rate": 2.8749999999999997e-05,
      "loss": 0.0017,
      "step": 63750
    },
    {
      "epoch": 3.400533333333333,
      "grad_norm": 0.2937678098678589,
      "learning_rate": 2.8746666666666667e-05,
      "loss": 0.002,
      "step": 63760
    },
    {
      "epoch": 3.401066666666667,
      "grad_norm": 0.1844259798526764,
      "learning_rate": 2.8743333333333333e-05,
      "loss": 0.0025,
      "step": 63770
    },
    {
      "epoch": 3.4016,
      "grad_norm": 0.09114541858434677,
      "learning_rate": 2.874e-05,
      "loss": 0.002,
      "step": 63780
    },
    {
      "epoch": 3.4021333333333335,
      "grad_norm": 0.26742181181907654,
      "learning_rate": 2.873666666666667e-05,
      "loss": 0.003,
      "step": 63790
    },
    {
      "epoch": 3.4026666666666667,
      "grad_norm": 0.5651158094406128,
      "learning_rate": 2.8733333333333335e-05,
      "loss": 0.0027,
      "step": 63800
    },
    {
      "epoch": 3.4032,
      "grad_norm": 0.06427226215600967,
      "learning_rate": 2.873e-05,
      "loss": 0.0025,
      "step": 63810
    },
    {
      "epoch": 3.4037333333333333,
      "grad_norm": 0.21309275925159454,
      "learning_rate": 2.8726666666666667e-05,
      "loss": 0.0024,
      "step": 63820
    },
    {
      "epoch": 3.4042666666666666,
      "grad_norm": 0.24378876388072968,
      "learning_rate": 2.8723333333333337e-05,
      "loss": 0.0022,
      "step": 63830
    },
    {
      "epoch": 3.4048,
      "grad_norm": 0.5387155413627625,
      "learning_rate": 2.8720000000000003e-05,
      "loss": 0.0025,
      "step": 63840
    },
    {
      "epoch": 3.405333333333333,
      "grad_norm": 0.054786317050457,
      "learning_rate": 2.871666666666667e-05,
      "loss": 0.0033,
      "step": 63850
    },
    {
      "epoch": 3.405866666666667,
      "grad_norm": 0.34536391496658325,
      "learning_rate": 2.8713333333333335e-05,
      "loss": 0.0029,
      "step": 63860
    },
    {
      "epoch": 3.4064,
      "grad_norm": 0.5785667300224304,
      "learning_rate": 2.8710000000000005e-05,
      "loss": 0.0018,
      "step": 63870
    },
    {
      "epoch": 3.4069333333333334,
      "grad_norm": 0.5460735559463501,
      "learning_rate": 2.870666666666667e-05,
      "loss": 0.0016,
      "step": 63880
    },
    {
      "epoch": 3.4074666666666666,
      "grad_norm": 0.3011690676212311,
      "learning_rate": 2.8703333333333334e-05,
      "loss": 0.0038,
      "step": 63890
    },
    {
      "epoch": 3.408,
      "grad_norm": 0.037002578377723694,
      "learning_rate": 2.87e-05,
      "loss": 0.0025,
      "step": 63900
    },
    {
      "epoch": 3.408533333333333,
      "grad_norm": 0.02617598883807659,
      "learning_rate": 2.8696666666666666e-05,
      "loss": 0.0017,
      "step": 63910
    },
    {
      "epoch": 3.409066666666667,
      "grad_norm": 0.29896146059036255,
      "learning_rate": 2.8693333333333332e-05,
      "loss": 0.0018,
      "step": 63920
    },
    {
      "epoch": 3.4096,
      "grad_norm": 0.20528632402420044,
      "learning_rate": 2.869e-05,
      "loss": 0.0013,
      "step": 63930
    },
    {
      "epoch": 3.4101333333333335,
      "grad_norm": 0.09309215843677521,
      "learning_rate": 2.8686666666666668e-05,
      "loss": 0.0023,
      "step": 63940
    },
    {
      "epoch": 3.4106666666666667,
      "grad_norm": 0.38212326169013977,
      "learning_rate": 2.8683333333333334e-05,
      "loss": 0.0021,
      "step": 63950
    },
    {
      "epoch": 3.4112,
      "grad_norm": 0.4459969997406006,
      "learning_rate": 2.868e-05,
      "loss": 0.0031,
      "step": 63960
    },
    {
      "epoch": 3.4117333333333333,
      "grad_norm": 0.2782890498638153,
      "learning_rate": 2.867666666666667e-05,
      "loss": 0.0021,
      "step": 63970
    },
    {
      "epoch": 3.4122666666666666,
      "grad_norm": 0.02795378491282463,
      "learning_rate": 2.8673333333333336e-05,
      "loss": 0.0021,
      "step": 63980
    },
    {
      "epoch": 3.4128,
      "grad_norm": 0.24928760528564453,
      "learning_rate": 2.867e-05,
      "loss": 0.002,
      "step": 63990
    },
    {
      "epoch": 3.413333333333333,
      "grad_norm": 0.32723289728164673,
      "learning_rate": 2.8666666666666668e-05,
      "loss": 0.0021,
      "step": 64000
    },
    {
      "epoch": 3.413866666666667,
      "grad_norm": 0.12333190441131592,
      "learning_rate": 2.8663333333333337e-05,
      "loss": 0.002,
      "step": 64010
    },
    {
      "epoch": 3.4144,
      "grad_norm": 0.15128569304943085,
      "learning_rate": 2.8660000000000003e-05,
      "loss": 0.0018,
      "step": 64020
    },
    {
      "epoch": 3.4149333333333334,
      "grad_norm": 0.36011677980422974,
      "learning_rate": 2.865666666666667e-05,
      "loss": 0.0019,
      "step": 64030
    },
    {
      "epoch": 3.4154666666666667,
      "grad_norm": 0.32025113701820374,
      "learning_rate": 2.8653333333333332e-05,
      "loss": 0.0036,
      "step": 64040
    },
    {
      "epoch": 3.416,
      "grad_norm": 0.41173917055130005,
      "learning_rate": 2.865e-05,
      "loss": 0.0017,
      "step": 64050
    },
    {
      "epoch": 3.416533333333333,
      "grad_norm": 0.12389980256557465,
      "learning_rate": 2.8646666666666665e-05,
      "loss": 0.0021,
      "step": 64060
    },
    {
      "epoch": 3.4170666666666665,
      "grad_norm": 0.3314589262008667,
      "learning_rate": 2.8643333333333334e-05,
      "loss": 0.0021,
      "step": 64070
    },
    {
      "epoch": 3.4176,
      "grad_norm": 0.26873496174812317,
      "learning_rate": 2.864e-05,
      "loss": 0.0017,
      "step": 64080
    },
    {
      "epoch": 3.4181333333333335,
      "grad_norm": 0.3099813461303711,
      "learning_rate": 2.8636666666666666e-05,
      "loss": 0.0022,
      "step": 64090
    },
    {
      "epoch": 3.4186666666666667,
      "grad_norm": 0.10800468921661377,
      "learning_rate": 2.8633333333333336e-05,
      "loss": 0.0021,
      "step": 64100
    },
    {
      "epoch": 3.4192,
      "grad_norm": 0.07164698094129562,
      "learning_rate": 2.8630000000000002e-05,
      "loss": 0.0025,
      "step": 64110
    },
    {
      "epoch": 3.4197333333333333,
      "grad_norm": 0.3305133879184723,
      "learning_rate": 2.8626666666666668e-05,
      "loss": 0.0014,
      "step": 64120
    },
    {
      "epoch": 3.4202666666666666,
      "grad_norm": 0.06293848156929016,
      "learning_rate": 2.8623333333333334e-05,
      "loss": 0.002,
      "step": 64130
    },
    {
      "epoch": 3.4208,
      "grad_norm": 0.07683222740888596,
      "learning_rate": 2.8620000000000004e-05,
      "loss": 0.0019,
      "step": 64140
    },
    {
      "epoch": 3.421333333333333,
      "grad_norm": 0.08138186484575272,
      "learning_rate": 2.861666666666667e-05,
      "loss": 0.0025,
      "step": 64150
    },
    {
      "epoch": 3.421866666666667,
      "grad_norm": 0.5463815927505493,
      "learning_rate": 2.8613333333333336e-05,
      "loss": 0.0034,
      "step": 64160
    },
    {
      "epoch": 3.4224,
      "grad_norm": 0.052176594734191895,
      "learning_rate": 2.8610000000000002e-05,
      "loss": 0.0022,
      "step": 64170
    },
    {
      "epoch": 3.4229333333333334,
      "grad_norm": 0.33031898736953735,
      "learning_rate": 2.8606666666666672e-05,
      "loss": 0.0026,
      "step": 64180
    },
    {
      "epoch": 3.4234666666666667,
      "grad_norm": 0.12430982291698456,
      "learning_rate": 2.860333333333333e-05,
      "loss": 0.0019,
      "step": 64190
    },
    {
      "epoch": 3.424,
      "grad_norm": 0.12138969451189041,
      "learning_rate": 2.86e-05,
      "loss": 0.002,
      "step": 64200
    },
    {
      "epoch": 3.424533333333333,
      "grad_norm": 0.7827774286270142,
      "learning_rate": 2.8596666666666667e-05,
      "loss": 0.0026,
      "step": 64210
    },
    {
      "epoch": 3.4250666666666665,
      "grad_norm": 0.10717806965112686,
      "learning_rate": 2.8593333333333333e-05,
      "loss": 0.0023,
      "step": 64220
    },
    {
      "epoch": 3.4256,
      "grad_norm": 0.15368348360061646,
      "learning_rate": 2.859e-05,
      "loss": 0.0026,
      "step": 64230
    },
    {
      "epoch": 3.4261333333333335,
      "grad_norm": 0.1787007451057434,
      "learning_rate": 2.858666666666667e-05,
      "loss": 0.0016,
      "step": 64240
    },
    {
      "epoch": 3.4266666666666667,
      "grad_norm": 0.03785530477762222,
      "learning_rate": 2.8583333333333335e-05,
      "loss": 0.0024,
      "step": 64250
    },
    {
      "epoch": 3.4272,
      "grad_norm": 0.3331136703491211,
      "learning_rate": 2.858e-05,
      "loss": 0.0035,
      "step": 64260
    },
    {
      "epoch": 3.4277333333333333,
      "grad_norm": 0.2116173505783081,
      "learning_rate": 2.8576666666666667e-05,
      "loss": 0.0015,
      "step": 64270
    },
    {
      "epoch": 3.4282666666666666,
      "grad_norm": 0.11798088252544403,
      "learning_rate": 2.8573333333333336e-05,
      "loss": 0.0013,
      "step": 64280
    },
    {
      "epoch": 3.4288,
      "grad_norm": 0.5993998050689697,
      "learning_rate": 2.8570000000000003e-05,
      "loss": 0.0022,
      "step": 64290
    },
    {
      "epoch": 3.429333333333333,
      "grad_norm": 0.183106929063797,
      "learning_rate": 2.856666666666667e-05,
      "loss": 0.0033,
      "step": 64300
    },
    {
      "epoch": 3.429866666666667,
      "grad_norm": 0.12479721009731293,
      "learning_rate": 2.8563333333333335e-05,
      "loss": 0.0023,
      "step": 64310
    },
    {
      "epoch": 3.4304,
      "grad_norm": 0.2360532581806183,
      "learning_rate": 2.8560000000000004e-05,
      "loss": 0.0017,
      "step": 64320
    },
    {
      "epoch": 3.4309333333333334,
      "grad_norm": 0.29327574372291565,
      "learning_rate": 2.855666666666667e-05,
      "loss": 0.0026,
      "step": 64330
    },
    {
      "epoch": 3.4314666666666667,
      "grad_norm": 0.07831963151693344,
      "learning_rate": 2.8553333333333333e-05,
      "loss": 0.0023,
      "step": 64340
    },
    {
      "epoch": 3.432,
      "grad_norm": 0.39502763748168945,
      "learning_rate": 2.855e-05,
      "loss": 0.003,
      "step": 64350
    },
    {
      "epoch": 3.432533333333333,
      "grad_norm": 0.23708780109882355,
      "learning_rate": 2.8546666666666666e-05,
      "loss": 0.003,
      "step": 64360
    },
    {
      "epoch": 3.4330666666666665,
      "grad_norm": 0.38398703932762146,
      "learning_rate": 2.854333333333333e-05,
      "loss": 0.0021,
      "step": 64370
    },
    {
      "epoch": 3.4336,
      "grad_norm": 0.12740817666053772,
      "learning_rate": 2.854e-05,
      "loss": 0.0026,
      "step": 64380
    },
    {
      "epoch": 3.4341333333333335,
      "grad_norm": 0.32527047395706177,
      "learning_rate": 2.8536666666666667e-05,
      "loss": 0.0027,
      "step": 64390
    },
    {
      "epoch": 3.4346666666666668,
      "grad_norm": 0.08906946331262589,
      "learning_rate": 2.8533333333333333e-05,
      "loss": 0.0032,
      "step": 64400
    },
    {
      "epoch": 3.4352,
      "grad_norm": 0.23850855231285095,
      "learning_rate": 2.853e-05,
      "loss": 0.0023,
      "step": 64410
    },
    {
      "epoch": 3.4357333333333333,
      "grad_norm": 0.11436496675014496,
      "learning_rate": 2.852666666666667e-05,
      "loss": 0.0034,
      "step": 64420
    },
    {
      "epoch": 3.4362666666666666,
      "grad_norm": 0.1259526014328003,
      "learning_rate": 2.8523333333333335e-05,
      "loss": 0.0024,
      "step": 64430
    },
    {
      "epoch": 3.4368,
      "grad_norm": 0.4261679947376251,
      "learning_rate": 2.852e-05,
      "loss": 0.0033,
      "step": 64440
    },
    {
      "epoch": 3.437333333333333,
      "grad_norm": 0.27000993490219116,
      "learning_rate": 2.851666666666667e-05,
      "loss": 0.0023,
      "step": 64450
    },
    {
      "epoch": 3.437866666666667,
      "grad_norm": 0.3142019510269165,
      "learning_rate": 2.8513333333333337e-05,
      "loss": 0.0032,
      "step": 64460
    },
    {
      "epoch": 3.4384,
      "grad_norm": 0.573087751865387,
      "learning_rate": 2.8510000000000003e-05,
      "loss": 0.0029,
      "step": 64470
    },
    {
      "epoch": 3.4389333333333334,
      "grad_norm": 0.5617265105247498,
      "learning_rate": 2.850666666666667e-05,
      "loss": 0.0026,
      "step": 64480
    },
    {
      "epoch": 3.4394666666666667,
      "grad_norm": 0.06861226260662079,
      "learning_rate": 2.850333333333334e-05,
      "loss": 0.0014,
      "step": 64490
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.35745054483413696,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 0.0027,
      "step": 64500
    },
    {
      "epoch": 3.440533333333333,
      "grad_norm": 0.07412728667259216,
      "learning_rate": 2.8496666666666664e-05,
      "loss": 0.0018,
      "step": 64510
    },
    {
      "epoch": 3.4410666666666665,
      "grad_norm": 0.32612091302871704,
      "learning_rate": 2.8493333333333334e-05,
      "loss": 0.0029,
      "step": 64520
    },
    {
      "epoch": 3.4416,
      "grad_norm": 0.6499702334403992,
      "learning_rate": 2.849e-05,
      "loss": 0.0019,
      "step": 64530
    },
    {
      "epoch": 3.4421333333333335,
      "grad_norm": 0.08688250184059143,
      "learning_rate": 2.8486666666666666e-05,
      "loss": 0.0023,
      "step": 64540
    },
    {
      "epoch": 3.4426666666666668,
      "grad_norm": 0.397480309009552,
      "learning_rate": 2.8483333333333336e-05,
      "loss": 0.0019,
      "step": 64550
    },
    {
      "epoch": 3.4432,
      "grad_norm": 0.09073086082935333,
      "learning_rate": 2.8480000000000002e-05,
      "loss": 0.0022,
      "step": 64560
    },
    {
      "epoch": 3.4437333333333333,
      "grad_norm": 0.0388529971241951,
      "learning_rate": 2.8476666666666668e-05,
      "loss": 0.0021,
      "step": 64570
    },
    {
      "epoch": 3.4442666666666666,
      "grad_norm": 0.2919624447822571,
      "learning_rate": 2.8473333333333334e-05,
      "loss": 0.0017,
      "step": 64580
    },
    {
      "epoch": 3.4448,
      "grad_norm": 0.4381796419620514,
      "learning_rate": 2.8470000000000004e-05,
      "loss": 0.0026,
      "step": 64590
    },
    {
      "epoch": 3.445333333333333,
      "grad_norm": 0.15425965189933777,
      "learning_rate": 2.846666666666667e-05,
      "loss": 0.0031,
      "step": 64600
    },
    {
      "epoch": 3.445866666666667,
      "grad_norm": 0.10523893684148788,
      "learning_rate": 2.8463333333333336e-05,
      "loss": 0.0016,
      "step": 64610
    },
    {
      "epoch": 3.4464,
      "grad_norm": 0.2684495449066162,
      "learning_rate": 2.8460000000000002e-05,
      "loss": 0.0018,
      "step": 64620
    },
    {
      "epoch": 3.4469333333333334,
      "grad_norm": 0.12919265031814575,
      "learning_rate": 2.845666666666667e-05,
      "loss": 0.0015,
      "step": 64630
    },
    {
      "epoch": 3.4474666666666667,
      "grad_norm": 0.2160094827413559,
      "learning_rate": 2.8453333333333338e-05,
      "loss": 0.0026,
      "step": 64640
    },
    {
      "epoch": 3.448,
      "grad_norm": 0.46619346737861633,
      "learning_rate": 2.845e-05,
      "loss": 0.0018,
      "step": 64650
    },
    {
      "epoch": 3.4485333333333332,
      "grad_norm": 0.387504518032074,
      "learning_rate": 2.8446666666666666e-05,
      "loss": 0.0033,
      "step": 64660
    },
    {
      "epoch": 3.4490666666666665,
      "grad_norm": 0.48956823348999023,
      "learning_rate": 2.8443333333333333e-05,
      "loss": 0.0022,
      "step": 64670
    },
    {
      "epoch": 3.4496,
      "grad_norm": 0.4232933819293976,
      "learning_rate": 2.844e-05,
      "loss": 0.0035,
      "step": 64680
    },
    {
      "epoch": 3.4501333333333335,
      "grad_norm": 0.10000309348106384,
      "learning_rate": 2.8436666666666668e-05,
      "loss": 0.0027,
      "step": 64690
    },
    {
      "epoch": 3.4506666666666668,
      "grad_norm": 0.14922137558460236,
      "learning_rate": 2.8433333333333334e-05,
      "loss": 0.0023,
      "step": 64700
    },
    {
      "epoch": 3.4512,
      "grad_norm": 0.41785508394241333,
      "learning_rate": 2.843e-05,
      "loss": 0.0023,
      "step": 64710
    },
    {
      "epoch": 3.4517333333333333,
      "grad_norm": 0.44162431359291077,
      "learning_rate": 2.8426666666666667e-05,
      "loss": 0.0021,
      "step": 64720
    },
    {
      "epoch": 3.4522666666666666,
      "grad_norm": 0.14206089079380035,
      "learning_rate": 2.8423333333333336e-05,
      "loss": 0.0014,
      "step": 64730
    },
    {
      "epoch": 3.4528,
      "grad_norm": 0.12184920907020569,
      "learning_rate": 2.8420000000000002e-05,
      "loss": 0.0019,
      "step": 64740
    },
    {
      "epoch": 3.453333333333333,
      "grad_norm": 0.264551043510437,
      "learning_rate": 2.841666666666667e-05,
      "loss": 0.0022,
      "step": 64750
    },
    {
      "epoch": 3.4538666666666664,
      "grad_norm": 0.6173852682113647,
      "learning_rate": 2.8413333333333335e-05,
      "loss": 0.0022,
      "step": 64760
    },
    {
      "epoch": 3.4544,
      "grad_norm": 0.38366538286209106,
      "learning_rate": 2.8410000000000004e-05,
      "loss": 0.0034,
      "step": 64770
    },
    {
      "epoch": 3.4549333333333334,
      "grad_norm": 0.09609158337116241,
      "learning_rate": 2.840666666666667e-05,
      "loss": 0.0022,
      "step": 64780
    },
    {
      "epoch": 3.4554666666666667,
      "grad_norm": 0.14559942483901978,
      "learning_rate": 2.8403333333333336e-05,
      "loss": 0.0018,
      "step": 64790
    },
    {
      "epoch": 3.456,
      "grad_norm": 0.2053278535604477,
      "learning_rate": 2.84e-05,
      "loss": 0.0017,
      "step": 64800
    },
    {
      "epoch": 3.4565333333333332,
      "grad_norm": 0.29112696647644043,
      "learning_rate": 2.8396666666666665e-05,
      "loss": 0.0017,
      "step": 64810
    },
    {
      "epoch": 3.4570666666666665,
      "grad_norm": 0.5811687707901001,
      "learning_rate": 2.839333333333333e-05,
      "loss": 0.0021,
      "step": 64820
    },
    {
      "epoch": 3.4576000000000002,
      "grad_norm": 0.1162843108177185,
      "learning_rate": 2.839e-05,
      "loss": 0.0024,
      "step": 64830
    },
    {
      "epoch": 3.4581333333333335,
      "grad_norm": 0.09068658947944641,
      "learning_rate": 2.8386666666666667e-05,
      "loss": 0.0018,
      "step": 64840
    },
    {
      "epoch": 3.458666666666667,
      "grad_norm": 0.6137878894805908,
      "learning_rate": 2.8383333333333333e-05,
      "loss": 0.002,
      "step": 64850
    },
    {
      "epoch": 3.4592,
      "grad_norm": 0.41011419892311096,
      "learning_rate": 2.8380000000000003e-05,
      "loss": 0.0019,
      "step": 64860
    },
    {
      "epoch": 3.4597333333333333,
      "grad_norm": 0.6165392398834229,
      "learning_rate": 2.837666666666667e-05,
      "loss": 0.0029,
      "step": 64870
    },
    {
      "epoch": 3.4602666666666666,
      "grad_norm": 0.4702010750770569,
      "learning_rate": 2.8373333333333335e-05,
      "loss": 0.0022,
      "step": 64880
    },
    {
      "epoch": 3.4608,
      "grad_norm": 0.23304396867752075,
      "learning_rate": 2.837e-05,
      "loss": 0.0013,
      "step": 64890
    },
    {
      "epoch": 3.461333333333333,
      "grad_norm": 0.4235798418521881,
      "learning_rate": 2.836666666666667e-05,
      "loss": 0.0021,
      "step": 64900
    },
    {
      "epoch": 3.4618666666666664,
      "grad_norm": 0.06651569157838821,
      "learning_rate": 2.8363333333333337e-05,
      "loss": 0.0026,
      "step": 64910
    },
    {
      "epoch": 3.4624,
      "grad_norm": 0.16973340511322021,
      "learning_rate": 2.8360000000000003e-05,
      "loss": 0.0018,
      "step": 64920
    },
    {
      "epoch": 3.4629333333333334,
      "grad_norm": 0.24476701021194458,
      "learning_rate": 2.835666666666667e-05,
      "loss": 0.002,
      "step": 64930
    },
    {
      "epoch": 3.4634666666666667,
      "grad_norm": 0.5051459670066833,
      "learning_rate": 2.835333333333334e-05,
      "loss": 0.0023,
      "step": 64940
    },
    {
      "epoch": 3.464,
      "grad_norm": 0.12886103987693787,
      "learning_rate": 2.8349999999999998e-05,
      "loss": 0.0016,
      "step": 64950
    },
    {
      "epoch": 3.4645333333333332,
      "grad_norm": 0.23584924638271332,
      "learning_rate": 2.8346666666666667e-05,
      "loss": 0.0026,
      "step": 64960
    },
    {
      "epoch": 3.4650666666666665,
      "grad_norm": 0.1153874397277832,
      "learning_rate": 2.8343333333333334e-05,
      "loss": 0.002,
      "step": 64970
    },
    {
      "epoch": 3.4656000000000002,
      "grad_norm": 0.32219162583351135,
      "learning_rate": 2.834e-05,
      "loss": 0.0028,
      "step": 64980
    },
    {
      "epoch": 3.4661333333333335,
      "grad_norm": 0.1440177708864212,
      "learning_rate": 2.8336666666666666e-05,
      "loss": 0.0016,
      "step": 64990
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 0.047969892621040344,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 0.0014,
      "step": 65000
    },
    {
      "epoch": 3.4672,
      "grad_norm": 0.5052942037582397,
      "learning_rate": 2.833e-05,
      "loss": 0.002,
      "step": 65010
    },
    {
      "epoch": 3.4677333333333333,
      "grad_norm": 0.04014953598380089,
      "learning_rate": 2.8326666666666668e-05,
      "loss": 0.0014,
      "step": 65020
    },
    {
      "epoch": 3.4682666666666666,
      "grad_norm": 0.570553719997406,
      "learning_rate": 2.8323333333333334e-05,
      "loss": 0.0022,
      "step": 65030
    },
    {
      "epoch": 3.4688,
      "grad_norm": 0.1810867041349411,
      "learning_rate": 2.8320000000000003e-05,
      "loss": 0.0016,
      "step": 65040
    },
    {
      "epoch": 3.469333333333333,
      "grad_norm": 0.26459917426109314,
      "learning_rate": 2.831666666666667e-05,
      "loss": 0.0024,
      "step": 65050
    },
    {
      "epoch": 3.4698666666666664,
      "grad_norm": 0.2372332513332367,
      "learning_rate": 2.8313333333333336e-05,
      "loss": 0.0018,
      "step": 65060
    },
    {
      "epoch": 3.4704,
      "grad_norm": 0.18999725580215454,
      "learning_rate": 2.8310000000000002e-05,
      "loss": 0.0028,
      "step": 65070
    },
    {
      "epoch": 3.4709333333333334,
      "grad_norm": 0.46506890654563904,
      "learning_rate": 2.830666666666667e-05,
      "loss": 0.0025,
      "step": 65080
    },
    {
      "epoch": 3.4714666666666667,
      "grad_norm": 0.11841537058353424,
      "learning_rate": 2.8303333333333337e-05,
      "loss": 0.0016,
      "step": 65090
    },
    {
      "epoch": 3.472,
      "grad_norm": 0.21009472012519836,
      "learning_rate": 2.83e-05,
      "loss": 0.0025,
      "step": 65100
    },
    {
      "epoch": 3.4725333333333332,
      "grad_norm": 0.4718160331249237,
      "learning_rate": 2.8296666666666666e-05,
      "loss": 0.0019,
      "step": 65110
    },
    {
      "epoch": 3.4730666666666665,
      "grad_norm": 0.21429571509361267,
      "learning_rate": 2.8293333333333332e-05,
      "loss": 0.002,
      "step": 65120
    },
    {
      "epoch": 3.4736000000000002,
      "grad_norm": 0.37642255425453186,
      "learning_rate": 2.829e-05,
      "loss": 0.002,
      "step": 65130
    },
    {
      "epoch": 3.4741333333333335,
      "grad_norm": 0.2728324830532074,
      "learning_rate": 2.8286666666666668e-05,
      "loss": 0.0019,
      "step": 65140
    },
    {
      "epoch": 3.474666666666667,
      "grad_norm": 0.18528170883655548,
      "learning_rate": 2.8283333333333334e-05,
      "loss": 0.0033,
      "step": 65150
    },
    {
      "epoch": 3.4752,
      "grad_norm": 0.06871329247951508,
      "learning_rate": 2.828e-05,
      "loss": 0.0022,
      "step": 65160
    },
    {
      "epoch": 3.4757333333333333,
      "grad_norm": 0.15012496709823608,
      "learning_rate": 2.8276666666666666e-05,
      "loss": 0.0019,
      "step": 65170
    },
    {
      "epoch": 3.4762666666666666,
      "grad_norm": 0.17856138944625854,
      "learning_rate": 2.8273333333333336e-05,
      "loss": 0.0024,
      "step": 65180
    },
    {
      "epoch": 3.4768,
      "grad_norm": 0.3697901964187622,
      "learning_rate": 2.8270000000000002e-05,
      "loss": 0.0028,
      "step": 65190
    },
    {
      "epoch": 3.477333333333333,
      "grad_norm": 0.4502287209033966,
      "learning_rate": 2.8266666666666668e-05,
      "loss": 0.0024,
      "step": 65200
    },
    {
      "epoch": 3.4778666666666664,
      "grad_norm": 0.11800940334796906,
      "learning_rate": 2.8263333333333338e-05,
      "loss": 0.0026,
      "step": 65210
    },
    {
      "epoch": 3.4784,
      "grad_norm": 0.15695860981941223,
      "learning_rate": 2.8260000000000004e-05,
      "loss": 0.0019,
      "step": 65220
    },
    {
      "epoch": 3.4789333333333334,
      "grad_norm": 0.28707367181777954,
      "learning_rate": 2.825666666666667e-05,
      "loss": 0.0025,
      "step": 65230
    },
    {
      "epoch": 3.4794666666666667,
      "grad_norm": 0.04164980724453926,
      "learning_rate": 2.8253333333333336e-05,
      "loss": 0.0021,
      "step": 65240
    },
    {
      "epoch": 3.48,
      "grad_norm": 0.3041282594203949,
      "learning_rate": 2.825e-05,
      "loss": 0.0025,
      "step": 65250
    },
    {
      "epoch": 3.4805333333333333,
      "grad_norm": 0.2471480518579483,
      "learning_rate": 2.8246666666666665e-05,
      "loss": 0.0018,
      "step": 65260
    },
    {
      "epoch": 3.4810666666666665,
      "grad_norm": 0.4016083776950836,
      "learning_rate": 2.824333333333333e-05,
      "loss": 0.0021,
      "step": 65270
    },
    {
      "epoch": 3.4816,
      "grad_norm": 0.061050038784742355,
      "learning_rate": 2.824e-05,
      "loss": 0.0018,
      "step": 65280
    },
    {
      "epoch": 3.4821333333333335,
      "grad_norm": 0.0651506632566452,
      "learning_rate": 2.8236666666666667e-05,
      "loss": 0.002,
      "step": 65290
    },
    {
      "epoch": 3.482666666666667,
      "grad_norm": 0.1753082424402237,
      "learning_rate": 2.8233333333333333e-05,
      "loss": 0.0023,
      "step": 65300
    },
    {
      "epoch": 3.4832,
      "grad_norm": 0.26246824860572815,
      "learning_rate": 2.8230000000000002e-05,
      "loss": 0.0025,
      "step": 65310
    },
    {
      "epoch": 3.4837333333333333,
      "grad_norm": 0.18027149140834808,
      "learning_rate": 2.822666666666667e-05,
      "loss": 0.0016,
      "step": 65320
    },
    {
      "epoch": 3.4842666666666666,
      "grad_norm": 0.533291220664978,
      "learning_rate": 2.8223333333333335e-05,
      "loss": 0.0019,
      "step": 65330
    },
    {
      "epoch": 3.4848,
      "grad_norm": 0.6546264290809631,
      "learning_rate": 2.822e-05,
      "loss": 0.0023,
      "step": 65340
    },
    {
      "epoch": 3.485333333333333,
      "grad_norm": 0.5623686909675598,
      "learning_rate": 2.821666666666667e-05,
      "loss": 0.0018,
      "step": 65350
    },
    {
      "epoch": 3.4858666666666664,
      "grad_norm": 0.03987361118197441,
      "learning_rate": 2.8213333333333337e-05,
      "loss": 0.0022,
      "step": 65360
    },
    {
      "epoch": 3.4864,
      "grad_norm": 0.15009823441505432,
      "learning_rate": 2.8210000000000003e-05,
      "loss": 0.0038,
      "step": 65370
    },
    {
      "epoch": 3.4869333333333334,
      "grad_norm": 0.6943755149841309,
      "learning_rate": 2.820666666666667e-05,
      "loss": 0.0018,
      "step": 65380
    },
    {
      "epoch": 3.4874666666666667,
      "grad_norm": 0.44066622853279114,
      "learning_rate": 2.820333333333334e-05,
      "loss": 0.0023,
      "step": 65390
    },
    {
      "epoch": 3.488,
      "grad_norm": 0.35780251026153564,
      "learning_rate": 2.8199999999999998e-05,
      "loss": 0.0022,
      "step": 65400
    },
    {
      "epoch": 3.4885333333333333,
      "grad_norm": 0.8388335108757019,
      "learning_rate": 2.8196666666666667e-05,
      "loss": 0.0033,
      "step": 65410
    },
    {
      "epoch": 3.4890666666666665,
      "grad_norm": 0.22555695474147797,
      "learning_rate": 2.8193333333333333e-05,
      "loss": 0.0031,
      "step": 65420
    },
    {
      "epoch": 3.4896,
      "grad_norm": 0.478556752204895,
      "learning_rate": 2.819e-05,
      "loss": 0.0019,
      "step": 65430
    },
    {
      "epoch": 3.4901333333333335,
      "grad_norm": 0.21601702272891998,
      "learning_rate": 2.8186666666666666e-05,
      "loss": 0.0023,
      "step": 65440
    },
    {
      "epoch": 3.490666666666667,
      "grad_norm": 0.16180063784122467,
      "learning_rate": 2.8183333333333335e-05,
      "loss": 0.0027,
      "step": 65450
    },
    {
      "epoch": 3.4912,
      "grad_norm": 0.07321776449680328,
      "learning_rate": 2.818e-05,
      "loss": 0.003,
      "step": 65460
    },
    {
      "epoch": 3.4917333333333334,
      "grad_norm": 0.049846675246953964,
      "learning_rate": 2.8176666666666667e-05,
      "loss": 0.002,
      "step": 65470
    },
    {
      "epoch": 3.4922666666666666,
      "grad_norm": 0.526898980140686,
      "learning_rate": 2.8173333333333334e-05,
      "loss": 0.0023,
      "step": 65480
    },
    {
      "epoch": 3.4928,
      "grad_norm": 0.4347848892211914,
      "learning_rate": 2.8170000000000003e-05,
      "loss": 0.0016,
      "step": 65490
    },
    {
      "epoch": 3.493333333333333,
      "grad_norm": 0.09156803786754608,
      "learning_rate": 2.816666666666667e-05,
      "loss": 0.0024,
      "step": 65500
    },
    {
      "epoch": 3.4938666666666665,
      "grad_norm": 0.4302784502506256,
      "learning_rate": 2.8163333333333335e-05,
      "loss": 0.0019,
      "step": 65510
    },
    {
      "epoch": 3.4944,
      "grad_norm": 0.04652474820613861,
      "learning_rate": 2.816e-05,
      "loss": 0.0027,
      "step": 65520
    },
    {
      "epoch": 3.4949333333333334,
      "grad_norm": 0.38879483938217163,
      "learning_rate": 2.815666666666667e-05,
      "loss": 0.0018,
      "step": 65530
    },
    {
      "epoch": 3.4954666666666667,
      "grad_norm": 0.04050321504473686,
      "learning_rate": 2.8153333333333337e-05,
      "loss": 0.0015,
      "step": 65540
    },
    {
      "epoch": 3.496,
      "grad_norm": 0.17506621778011322,
      "learning_rate": 2.815e-05,
      "loss": 0.0017,
      "step": 65550
    },
    {
      "epoch": 3.4965333333333333,
      "grad_norm": 0.7419931292533875,
      "learning_rate": 2.8146666666666666e-05,
      "loss": 0.0017,
      "step": 65560
    },
    {
      "epoch": 3.4970666666666665,
      "grad_norm": 0.09054604917764664,
      "learning_rate": 2.8143333333333332e-05,
      "loss": 0.0022,
      "step": 65570
    },
    {
      "epoch": 3.4976,
      "grad_norm": 0.5271051526069641,
      "learning_rate": 2.8139999999999998e-05,
      "loss": 0.0021,
      "step": 65580
    },
    {
      "epoch": 3.4981333333333335,
      "grad_norm": 0.06179399415850639,
      "learning_rate": 2.8136666666666668e-05,
      "loss": 0.0015,
      "step": 65590
    },
    {
      "epoch": 3.498666666666667,
      "grad_norm": 0.1517617106437683,
      "learning_rate": 2.8133333333333334e-05,
      "loss": 0.0016,
      "step": 65600
    },
    {
      "epoch": 3.4992,
      "grad_norm": 0.18229809403419495,
      "learning_rate": 2.813e-05,
      "loss": 0.0025,
      "step": 65610
    },
    {
      "epoch": 3.4997333333333334,
      "grad_norm": 0.06787000596523285,
      "learning_rate": 2.8126666666666666e-05,
      "loss": 0.0014,
      "step": 65620
    },
    {
      "epoch": 3.5002666666666666,
      "grad_norm": 0.2047901600599289,
      "learning_rate": 2.8123333333333336e-05,
      "loss": 0.0032,
      "step": 65630
    },
    {
      "epoch": 3.5008,
      "grad_norm": 0.24717606604099274,
      "learning_rate": 2.8120000000000002e-05,
      "loss": 0.0024,
      "step": 65640
    },
    {
      "epoch": 3.501333333333333,
      "grad_norm": 0.12549877166748047,
      "learning_rate": 2.8116666666666668e-05,
      "loss": 0.0019,
      "step": 65650
    },
    {
      "epoch": 3.5018666666666665,
      "grad_norm": 0.24022914469242096,
      "learning_rate": 2.8113333333333337e-05,
      "loss": 0.0014,
      "step": 65660
    },
    {
      "epoch": 3.5023999999999997,
      "grad_norm": 0.14985686540603638,
      "learning_rate": 2.8110000000000004e-05,
      "loss": 0.0018,
      "step": 65670
    },
    {
      "epoch": 3.5029333333333335,
      "grad_norm": 0.11978556215763092,
      "learning_rate": 2.810666666666667e-05,
      "loss": 0.0017,
      "step": 65680
    },
    {
      "epoch": 3.5034666666666667,
      "grad_norm": 0.10218516737222672,
      "learning_rate": 2.8103333333333336e-05,
      "loss": 0.0017,
      "step": 65690
    },
    {
      "epoch": 3.504,
      "grad_norm": 0.49325063824653625,
      "learning_rate": 2.8100000000000005e-05,
      "loss": 0.0028,
      "step": 65700
    },
    {
      "epoch": 3.5045333333333333,
      "grad_norm": 0.09526519477367401,
      "learning_rate": 2.8096666666666665e-05,
      "loss": 0.0024,
      "step": 65710
    },
    {
      "epoch": 3.5050666666666666,
      "grad_norm": 0.12741726636886597,
      "learning_rate": 2.8093333333333334e-05,
      "loss": 0.0031,
      "step": 65720
    },
    {
      "epoch": 3.5056000000000003,
      "grad_norm": 0.14013148844242096,
      "learning_rate": 2.809e-05,
      "loss": 0.0021,
      "step": 65730
    },
    {
      "epoch": 3.5061333333333335,
      "grad_norm": 0.2339748740196228,
      "learning_rate": 2.8086666666666667e-05,
      "loss": 0.0019,
      "step": 65740
    },
    {
      "epoch": 3.506666666666667,
      "grad_norm": 0.029224861413240433,
      "learning_rate": 2.8083333333333333e-05,
      "loss": 0.002,
      "step": 65750
    },
    {
      "epoch": 3.5072,
      "grad_norm": 0.330745667219162,
      "learning_rate": 2.8080000000000002e-05,
      "loss": 0.0022,
      "step": 65760
    },
    {
      "epoch": 3.5077333333333334,
      "grad_norm": 0.3464537262916565,
      "learning_rate": 2.807666666666667e-05,
      "loss": 0.0013,
      "step": 65770
    },
    {
      "epoch": 3.5082666666666666,
      "grad_norm": 0.29160815477371216,
      "learning_rate": 2.8073333333333334e-05,
      "loss": 0.0023,
      "step": 65780
    },
    {
      "epoch": 3.5088,
      "grad_norm": 0.44957077503204346,
      "learning_rate": 2.807e-05,
      "loss": 0.0022,
      "step": 65790
    },
    {
      "epoch": 3.509333333333333,
      "grad_norm": 0.48056522011756897,
      "learning_rate": 2.806666666666667e-05,
      "loss": 0.0017,
      "step": 65800
    },
    {
      "epoch": 3.5098666666666665,
      "grad_norm": 0.6759597659111023,
      "learning_rate": 2.8063333333333336e-05,
      "loss": 0.0021,
      "step": 65810
    },
    {
      "epoch": 3.5103999999999997,
      "grad_norm": 0.10319272428750992,
      "learning_rate": 2.8060000000000002e-05,
      "loss": 0.0028,
      "step": 65820
    },
    {
      "epoch": 3.5109333333333335,
      "grad_norm": 0.20725497603416443,
      "learning_rate": 2.805666666666667e-05,
      "loss": 0.0024,
      "step": 65830
    },
    {
      "epoch": 3.5114666666666667,
      "grad_norm": 0.4256536066532135,
      "learning_rate": 2.8053333333333338e-05,
      "loss": 0.0022,
      "step": 65840
    },
    {
      "epoch": 3.512,
      "grad_norm": 0.062179259955883026,
      "learning_rate": 2.8050000000000004e-05,
      "loss": 0.0023,
      "step": 65850
    },
    {
      "epoch": 3.5125333333333333,
      "grad_norm": 0.20576974749565125,
      "learning_rate": 2.8046666666666667e-05,
      "loss": 0.0012,
      "step": 65860
    },
    {
      "epoch": 3.5130666666666666,
      "grad_norm": 0.3296079635620117,
      "learning_rate": 2.8043333333333333e-05,
      "loss": 0.0022,
      "step": 65870
    },
    {
      "epoch": 3.5136,
      "grad_norm": 0.13277822732925415,
      "learning_rate": 2.804e-05,
      "loss": 0.0022,
      "step": 65880
    },
    {
      "epoch": 3.5141333333333336,
      "grad_norm": 0.037259556353092194,
      "learning_rate": 2.8036666666666665e-05,
      "loss": 0.0019,
      "step": 65890
    },
    {
      "epoch": 3.514666666666667,
      "grad_norm": 0.36405420303344727,
      "learning_rate": 2.8033333333333335e-05,
      "loss": 0.0016,
      "step": 65900
    },
    {
      "epoch": 3.5152,
      "grad_norm": 0.3807026445865631,
      "learning_rate": 2.803e-05,
      "loss": 0.0017,
      "step": 65910
    },
    {
      "epoch": 3.5157333333333334,
      "grad_norm": 0.06248387694358826,
      "learning_rate": 2.8026666666666667e-05,
      "loss": 0.0025,
      "step": 65920
    },
    {
      "epoch": 3.5162666666666667,
      "grad_norm": 0.8264051675796509,
      "learning_rate": 2.8023333333333333e-05,
      "loss": 0.002,
      "step": 65930
    },
    {
      "epoch": 3.5168,
      "grad_norm": 0.18839231133460999,
      "learning_rate": 2.8020000000000003e-05,
      "loss": 0.0028,
      "step": 65940
    },
    {
      "epoch": 3.517333333333333,
      "grad_norm": 0.05369376018643379,
      "learning_rate": 2.801666666666667e-05,
      "loss": 0.0018,
      "step": 65950
    },
    {
      "epoch": 3.5178666666666665,
      "grad_norm": 0.850604236125946,
      "learning_rate": 2.8013333333333335e-05,
      "loss": 0.003,
      "step": 65960
    },
    {
      "epoch": 3.5183999999999997,
      "grad_norm": 0.14943429827690125,
      "learning_rate": 2.8010000000000005e-05,
      "loss": 0.0018,
      "step": 65970
    },
    {
      "epoch": 3.5189333333333335,
      "grad_norm": 0.1096198707818985,
      "learning_rate": 2.800666666666667e-05,
      "loss": 0.0019,
      "step": 65980
    },
    {
      "epoch": 3.5194666666666667,
      "grad_norm": 0.30507969856262207,
      "learning_rate": 2.8003333333333337e-05,
      "loss": 0.0029,
      "step": 65990
    },
    {
      "epoch": 3.52,
      "grad_norm": 0.12271124124526978,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.0024,
      "step": 66000
    },
    {
      "epoch": 3.5205333333333333,
      "grad_norm": 0.3790234923362732,
      "learning_rate": 2.7996666666666666e-05,
      "loss": 0.0027,
      "step": 66010
    },
    {
      "epoch": 3.5210666666666666,
      "grad_norm": 0.3747250437736511,
      "learning_rate": 2.7993333333333332e-05,
      "loss": 0.0021,
      "step": 66020
    },
    {
      "epoch": 3.5216,
      "grad_norm": 0.15514932572841644,
      "learning_rate": 2.7989999999999998e-05,
      "loss": 0.0015,
      "step": 66030
    },
    {
      "epoch": 3.5221333333333336,
      "grad_norm": 0.16494271159172058,
      "learning_rate": 2.7986666666666668e-05,
      "loss": 0.0015,
      "step": 66040
    },
    {
      "epoch": 3.522666666666667,
      "grad_norm": 0.4482788145542145,
      "learning_rate": 2.7983333333333334e-05,
      "loss": 0.0021,
      "step": 66050
    },
    {
      "epoch": 3.5232,
      "grad_norm": 0.5427213311195374,
      "learning_rate": 2.798e-05,
      "loss": 0.0018,
      "step": 66060
    },
    {
      "epoch": 3.5237333333333334,
      "grad_norm": 0.12423627078533173,
      "learning_rate": 2.797666666666667e-05,
      "loss": 0.0033,
      "step": 66070
    },
    {
      "epoch": 3.5242666666666667,
      "grad_norm": 0.33467745780944824,
      "learning_rate": 2.7973333333333335e-05,
      "loss": 0.0025,
      "step": 66080
    },
    {
      "epoch": 3.5248,
      "grad_norm": 0.04069146886467934,
      "learning_rate": 2.797e-05,
      "loss": 0.0027,
      "step": 66090
    },
    {
      "epoch": 3.525333333333333,
      "grad_norm": 0.2669816017150879,
      "learning_rate": 2.7966666666666668e-05,
      "loss": 0.0021,
      "step": 66100
    },
    {
      "epoch": 3.5258666666666665,
      "grad_norm": 0.4125800132751465,
      "learning_rate": 2.7963333333333337e-05,
      "loss": 0.0021,
      "step": 66110
    },
    {
      "epoch": 3.5263999999999998,
      "grad_norm": 0.36063480377197266,
      "learning_rate": 2.7960000000000003e-05,
      "loss": 0.0024,
      "step": 66120
    },
    {
      "epoch": 3.5269333333333335,
      "grad_norm": 0.6254234910011292,
      "learning_rate": 2.795666666666667e-05,
      "loss": 0.0016,
      "step": 66130
    },
    {
      "epoch": 3.5274666666666668,
      "grad_norm": 0.23318210244178772,
      "learning_rate": 2.7953333333333336e-05,
      "loss": 0.0026,
      "step": 66140
    },
    {
      "epoch": 3.528,
      "grad_norm": 0.17727045714855194,
      "learning_rate": 2.7950000000000005e-05,
      "loss": 0.0023,
      "step": 66150
    },
    {
      "epoch": 3.5285333333333333,
      "grad_norm": 0.5179623365402222,
      "learning_rate": 2.7946666666666664e-05,
      "loss": 0.0021,
      "step": 66160
    },
    {
      "epoch": 3.5290666666666666,
      "grad_norm": 0.5912282466888428,
      "learning_rate": 2.7943333333333334e-05,
      "loss": 0.0026,
      "step": 66170
    },
    {
      "epoch": 3.5296,
      "grad_norm": 0.35713741183280945,
      "learning_rate": 2.794e-05,
      "loss": 0.0017,
      "step": 66180
    },
    {
      "epoch": 3.5301333333333336,
      "grad_norm": 0.20591583847999573,
      "learning_rate": 2.7936666666666666e-05,
      "loss": 0.0021,
      "step": 66190
    },
    {
      "epoch": 3.530666666666667,
      "grad_norm": 0.12731026113033295,
      "learning_rate": 2.7933333333333332e-05,
      "loss": 0.0027,
      "step": 66200
    },
    {
      "epoch": 3.5312,
      "grad_norm": 0.08849013596773148,
      "learning_rate": 2.7930000000000002e-05,
      "loss": 0.0021,
      "step": 66210
    },
    {
      "epoch": 3.5317333333333334,
      "grad_norm": 0.24013271927833557,
      "learning_rate": 2.7926666666666668e-05,
      "loss": 0.0015,
      "step": 66220
    },
    {
      "epoch": 3.5322666666666667,
      "grad_norm": 0.06563667207956314,
      "learning_rate": 2.7923333333333334e-05,
      "loss": 0.0017,
      "step": 66230
    },
    {
      "epoch": 3.5328,
      "grad_norm": 0.15291251242160797,
      "learning_rate": 2.792e-05,
      "loss": 0.0018,
      "step": 66240
    },
    {
      "epoch": 3.533333333333333,
      "grad_norm": 0.07061649858951569,
      "learning_rate": 2.791666666666667e-05,
      "loss": 0.0029,
      "step": 66250
    },
    {
      "epoch": 3.5338666666666665,
      "grad_norm": 0.17859598994255066,
      "learning_rate": 2.7913333333333336e-05,
      "loss": 0.0022,
      "step": 66260
    },
    {
      "epoch": 3.5343999999999998,
      "grad_norm": 0.43154144287109375,
      "learning_rate": 2.7910000000000002e-05,
      "loss": 0.0019,
      "step": 66270
    },
    {
      "epoch": 3.5349333333333335,
      "grad_norm": 0.2707791328430176,
      "learning_rate": 2.7906666666666668e-05,
      "loss": 0.002,
      "step": 66280
    },
    {
      "epoch": 3.5354666666666668,
      "grad_norm": 0.08765881508588791,
      "learning_rate": 2.7903333333333338e-05,
      "loss": 0.0022,
      "step": 66290
    },
    {
      "epoch": 3.536,
      "grad_norm": 0.11975394934415817,
      "learning_rate": 2.7900000000000004e-05,
      "loss": 0.0017,
      "step": 66300
    },
    {
      "epoch": 3.5365333333333333,
      "grad_norm": 0.03625333681702614,
      "learning_rate": 2.7896666666666667e-05,
      "loss": 0.0029,
      "step": 66310
    },
    {
      "epoch": 3.5370666666666666,
      "grad_norm": 0.21117088198661804,
      "learning_rate": 2.7893333333333333e-05,
      "loss": 0.0017,
      "step": 66320
    },
    {
      "epoch": 3.5376,
      "grad_norm": 0.07337653636932373,
      "learning_rate": 2.789e-05,
      "loss": 0.0025,
      "step": 66330
    },
    {
      "epoch": 3.5381333333333336,
      "grad_norm": 0.6245595812797546,
      "learning_rate": 2.7886666666666665e-05,
      "loss": 0.0022,
      "step": 66340
    },
    {
      "epoch": 3.538666666666667,
      "grad_norm": 0.15264834463596344,
      "learning_rate": 2.7883333333333335e-05,
      "loss": 0.0018,
      "step": 66350
    },
    {
      "epoch": 3.5392,
      "grad_norm": 0.14909470081329346,
      "learning_rate": 2.788e-05,
      "loss": 0.0013,
      "step": 66360
    },
    {
      "epoch": 3.5397333333333334,
      "grad_norm": 0.12459676712751389,
      "learning_rate": 2.7876666666666667e-05,
      "loss": 0.0018,
      "step": 66370
    },
    {
      "epoch": 3.5402666666666667,
      "grad_norm": 0.03877847641706467,
      "learning_rate": 2.7873333333333333e-05,
      "loss": 0.0017,
      "step": 66380
    },
    {
      "epoch": 3.5408,
      "grad_norm": 0.3783038556575775,
      "learning_rate": 2.7870000000000003e-05,
      "loss": 0.0021,
      "step": 66390
    },
    {
      "epoch": 3.541333333333333,
      "grad_norm": 0.14972832798957825,
      "learning_rate": 2.786666666666667e-05,
      "loss": 0.0014,
      "step": 66400
    },
    {
      "epoch": 3.5418666666666665,
      "grad_norm": 0.1855369359254837,
      "learning_rate": 2.7863333333333335e-05,
      "loss": 0.0024,
      "step": 66410
    },
    {
      "epoch": 3.5423999999999998,
      "grad_norm": 0.3530077636241913,
      "learning_rate": 2.7860000000000004e-05,
      "loss": 0.0027,
      "step": 66420
    },
    {
      "epoch": 3.5429333333333335,
      "grad_norm": 0.07914505153894424,
      "learning_rate": 2.785666666666667e-05,
      "loss": 0.0015,
      "step": 66430
    },
    {
      "epoch": 3.5434666666666668,
      "grad_norm": 0.3883451223373413,
      "learning_rate": 2.7853333333333337e-05,
      "loss": 0.0022,
      "step": 66440
    },
    {
      "epoch": 3.544,
      "grad_norm": 0.14585161209106445,
      "learning_rate": 2.7850000000000003e-05,
      "loss": 0.0024,
      "step": 66450
    },
    {
      "epoch": 3.5445333333333333,
      "grad_norm": 0.35857921838760376,
      "learning_rate": 2.7846666666666665e-05,
      "loss": 0.0018,
      "step": 66460
    },
    {
      "epoch": 3.5450666666666666,
      "grad_norm": 0.2306927889585495,
      "learning_rate": 2.784333333333333e-05,
      "loss": 0.0018,
      "step": 66470
    },
    {
      "epoch": 3.5456,
      "grad_norm": 0.06888160109519958,
      "learning_rate": 2.7839999999999998e-05,
      "loss": 0.0014,
      "step": 66480
    },
    {
      "epoch": 3.5461333333333336,
      "grad_norm": 0.23623088002204895,
      "learning_rate": 2.7836666666666667e-05,
      "loss": 0.0014,
      "step": 66490
    },
    {
      "epoch": 3.546666666666667,
      "grad_norm": 0.04254566505551338,
      "learning_rate": 2.7833333333333333e-05,
      "loss": 0.0018,
      "step": 66500
    },
    {
      "epoch": 3.5472,
      "grad_norm": 0.5001452565193176,
      "learning_rate": 2.783e-05,
      "loss": 0.0023,
      "step": 66510
    },
    {
      "epoch": 3.5477333333333334,
      "grad_norm": 0.2907366156578064,
      "learning_rate": 2.782666666666667e-05,
      "loss": 0.002,
      "step": 66520
    },
    {
      "epoch": 3.5482666666666667,
      "grad_norm": 0.436440110206604,
      "learning_rate": 2.7823333333333335e-05,
      "loss": 0.0027,
      "step": 66530
    },
    {
      "epoch": 3.5488,
      "grad_norm": 0.2221214771270752,
      "learning_rate": 2.782e-05,
      "loss": 0.0017,
      "step": 66540
    },
    {
      "epoch": 3.5493333333333332,
      "grad_norm": 0.3868761956691742,
      "learning_rate": 2.7816666666666667e-05,
      "loss": 0.0014,
      "step": 66550
    },
    {
      "epoch": 3.5498666666666665,
      "grad_norm": 0.1021171435713768,
      "learning_rate": 2.7813333333333337e-05,
      "loss": 0.0021,
      "step": 66560
    },
    {
      "epoch": 3.5504,
      "grad_norm": 0.4559002220630646,
      "learning_rate": 2.7810000000000003e-05,
      "loss": 0.0018,
      "step": 66570
    },
    {
      "epoch": 3.5509333333333335,
      "grad_norm": 0.3987073004245758,
      "learning_rate": 2.780666666666667e-05,
      "loss": 0.003,
      "step": 66580
    },
    {
      "epoch": 3.5514666666666668,
      "grad_norm": 0.9924675226211548,
      "learning_rate": 2.7803333333333335e-05,
      "loss": 0.0036,
      "step": 66590
    },
    {
      "epoch": 3.552,
      "grad_norm": 0.20889584720134735,
      "learning_rate": 2.7800000000000005e-05,
      "loss": 0.0021,
      "step": 66600
    },
    {
      "epoch": 3.5525333333333333,
      "grad_norm": 0.45354488492012024,
      "learning_rate": 2.7796666666666664e-05,
      "loss": 0.003,
      "step": 66610
    },
    {
      "epoch": 3.5530666666666666,
      "grad_norm": 0.208305224776268,
      "learning_rate": 2.7793333333333334e-05,
      "loss": 0.0022,
      "step": 66620
    },
    {
      "epoch": 3.5536,
      "grad_norm": 0.042937226593494415,
      "learning_rate": 2.779e-05,
      "loss": 0.0022,
      "step": 66630
    },
    {
      "epoch": 3.5541333333333336,
      "grad_norm": 0.10103294998407364,
      "learning_rate": 2.7786666666666666e-05,
      "loss": 0.0018,
      "step": 66640
    },
    {
      "epoch": 3.554666666666667,
      "grad_norm": 0.4239422082901001,
      "learning_rate": 2.7783333333333332e-05,
      "loss": 0.003,
      "step": 66650
    },
    {
      "epoch": 3.5552,
      "grad_norm": 0.09641431272029877,
      "learning_rate": 2.778e-05,
      "loss": 0.0019,
      "step": 66660
    },
    {
      "epoch": 3.5557333333333334,
      "grad_norm": 0.3534272313117981,
      "learning_rate": 2.7776666666666668e-05,
      "loss": 0.0021,
      "step": 66670
    },
    {
      "epoch": 3.5562666666666667,
      "grad_norm": 0.06385248899459839,
      "learning_rate": 2.7773333333333334e-05,
      "loss": 0.0031,
      "step": 66680
    },
    {
      "epoch": 3.5568,
      "grad_norm": 0.20386017858982086,
      "learning_rate": 2.777e-05,
      "loss": 0.0022,
      "step": 66690
    },
    {
      "epoch": 3.5573333333333332,
      "grad_norm": 0.047331031411886215,
      "learning_rate": 2.776666666666667e-05,
      "loss": 0.0021,
      "step": 66700
    },
    {
      "epoch": 3.5578666666666665,
      "grad_norm": 0.17912927269935608,
      "learning_rate": 2.7763333333333336e-05,
      "loss": 0.0023,
      "step": 66710
    },
    {
      "epoch": 3.5584,
      "grad_norm": 0.47056740522384644,
      "learning_rate": 2.7760000000000002e-05,
      "loss": 0.0027,
      "step": 66720
    },
    {
      "epoch": 3.558933333333333,
      "grad_norm": 0.23150624334812164,
      "learning_rate": 2.7756666666666668e-05,
      "loss": 0.0032,
      "step": 66730
    },
    {
      "epoch": 3.559466666666667,
      "grad_norm": 0.41706398129463196,
      "learning_rate": 2.7753333333333338e-05,
      "loss": 0.0032,
      "step": 66740
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.28087499737739563,
      "learning_rate": 2.7750000000000004e-05,
      "loss": 0.0022,
      "step": 66750
    },
    {
      "epoch": 3.5605333333333333,
      "grad_norm": 0.2744385302066803,
      "learning_rate": 2.7746666666666666e-05,
      "loss": 0.0025,
      "step": 66760
    },
    {
      "epoch": 3.5610666666666666,
      "grad_norm": 0.4741164445877075,
      "learning_rate": 2.7743333333333333e-05,
      "loss": 0.0027,
      "step": 66770
    },
    {
      "epoch": 3.5616,
      "grad_norm": 0.0620153471827507,
      "learning_rate": 2.774e-05,
      "loss": 0.0029,
      "step": 66780
    },
    {
      "epoch": 3.5621333333333336,
      "grad_norm": 0.2357436567544937,
      "learning_rate": 2.7736666666666665e-05,
      "loss": 0.002,
      "step": 66790
    },
    {
      "epoch": 3.562666666666667,
      "grad_norm": 0.1519509106874466,
      "learning_rate": 2.7733333333333334e-05,
      "loss": 0.0025,
      "step": 66800
    },
    {
      "epoch": 3.5632,
      "grad_norm": 0.2346789389848709,
      "learning_rate": 2.773e-05,
      "loss": 0.0036,
      "step": 66810
    },
    {
      "epoch": 3.5637333333333334,
      "grad_norm": 0.5447061657905579,
      "learning_rate": 2.7726666666666667e-05,
      "loss": 0.0028,
      "step": 66820
    },
    {
      "epoch": 3.5642666666666667,
      "grad_norm": 0.08784475922584534,
      "learning_rate": 2.7723333333333336e-05,
      "loss": 0.0025,
      "step": 66830
    },
    {
      "epoch": 3.5648,
      "grad_norm": 0.23559512197971344,
      "learning_rate": 2.7720000000000002e-05,
      "loss": 0.002,
      "step": 66840
    },
    {
      "epoch": 3.5653333333333332,
      "grad_norm": 0.2625305950641632,
      "learning_rate": 2.771666666666667e-05,
      "loss": 0.0027,
      "step": 66850
    },
    {
      "epoch": 3.5658666666666665,
      "grad_norm": 0.08816713094711304,
      "learning_rate": 2.7713333333333335e-05,
      "loss": 0.0021,
      "step": 66860
    },
    {
      "epoch": 3.5664,
      "grad_norm": 0.29927778244018555,
      "learning_rate": 2.7710000000000004e-05,
      "loss": 0.0017,
      "step": 66870
    },
    {
      "epoch": 3.566933333333333,
      "grad_norm": 0.3216492831707001,
      "learning_rate": 2.770666666666667e-05,
      "loss": 0.0027,
      "step": 66880
    },
    {
      "epoch": 3.567466666666667,
      "grad_norm": 0.26836633682250977,
      "learning_rate": 2.7703333333333336e-05,
      "loss": 0.0021,
      "step": 66890
    },
    {
      "epoch": 3.568,
      "grad_norm": 0.2426554411649704,
      "learning_rate": 2.7700000000000002e-05,
      "loss": 0.0021,
      "step": 66900
    },
    {
      "epoch": 3.5685333333333333,
      "grad_norm": 0.5218393206596375,
      "learning_rate": 2.7696666666666672e-05,
      "loss": 0.0018,
      "step": 66910
    },
    {
      "epoch": 3.5690666666666666,
      "grad_norm": 0.32656756043434143,
      "learning_rate": 2.769333333333333e-05,
      "loss": 0.0014,
      "step": 66920
    },
    {
      "epoch": 3.5696,
      "grad_norm": 0.3165740966796875,
      "learning_rate": 2.769e-05,
      "loss": 0.0029,
      "step": 66930
    },
    {
      "epoch": 3.5701333333333336,
      "grad_norm": 0.2067287713289261,
      "learning_rate": 2.7686666666666667e-05,
      "loss": 0.0019,
      "step": 66940
    },
    {
      "epoch": 3.570666666666667,
      "grad_norm": 0.1314374804496765,
      "learning_rate": 2.7683333333333333e-05,
      "loss": 0.0022,
      "step": 66950
    },
    {
      "epoch": 3.5712,
      "grad_norm": 0.5962277054786682,
      "learning_rate": 2.768e-05,
      "loss": 0.002,
      "step": 66960
    },
    {
      "epoch": 3.5717333333333334,
      "grad_norm": 0.20059892535209656,
      "learning_rate": 2.767666666666667e-05,
      "loss": 0.0016,
      "step": 66970
    },
    {
      "epoch": 3.5722666666666667,
      "grad_norm": 0.08826594054698944,
      "learning_rate": 2.7673333333333335e-05,
      "loss": 0.0013,
      "step": 66980
    },
    {
      "epoch": 3.5728,
      "grad_norm": 0.4950949251651764,
      "learning_rate": 2.767e-05,
      "loss": 0.0023,
      "step": 66990
    },
    {
      "epoch": 3.5733333333333333,
      "grad_norm": 0.3519008755683899,
      "learning_rate": 2.7666666666666667e-05,
      "loss": 0.0025,
      "step": 67000
    },
    {
      "epoch": 3.5738666666666665,
      "grad_norm": 0.14374539256095886,
      "learning_rate": 2.7663333333333337e-05,
      "loss": 0.0025,
      "step": 67010
    },
    {
      "epoch": 3.5744,
      "grad_norm": 0.06799948960542679,
      "learning_rate": 2.7660000000000003e-05,
      "loss": 0.0019,
      "step": 67020
    },
    {
      "epoch": 3.574933333333333,
      "grad_norm": 0.24024292826652527,
      "learning_rate": 2.765666666666667e-05,
      "loss": 0.0017,
      "step": 67030
    },
    {
      "epoch": 3.575466666666667,
      "grad_norm": 0.36893612146377563,
      "learning_rate": 2.7653333333333335e-05,
      "loss": 0.0022,
      "step": 67040
    },
    {
      "epoch": 3.576,
      "grad_norm": 0.38594141602516174,
      "learning_rate": 2.7650000000000005e-05,
      "loss": 0.0022,
      "step": 67050
    },
    {
      "epoch": 3.5765333333333333,
      "grad_norm": 0.10323148965835571,
      "learning_rate": 2.764666666666667e-05,
      "loss": 0.0018,
      "step": 67060
    },
    {
      "epoch": 3.5770666666666666,
      "grad_norm": 0.18314871191978455,
      "learning_rate": 2.7643333333333334e-05,
      "loss": 0.002,
      "step": 67070
    },
    {
      "epoch": 3.5776,
      "grad_norm": 0.2665415406227112,
      "learning_rate": 2.764e-05,
      "loss": 0.0029,
      "step": 67080
    },
    {
      "epoch": 3.5781333333333336,
      "grad_norm": 0.042791444808244705,
      "learning_rate": 2.7636666666666666e-05,
      "loss": 0.0015,
      "step": 67090
    },
    {
      "epoch": 3.578666666666667,
      "grad_norm": 0.5940776467323303,
      "learning_rate": 2.7633333333333332e-05,
      "loss": 0.0019,
      "step": 67100
    },
    {
      "epoch": 3.5792,
      "grad_norm": 0.4660980999469757,
      "learning_rate": 2.763e-05,
      "loss": 0.0024,
      "step": 67110
    },
    {
      "epoch": 3.5797333333333334,
      "grad_norm": 0.3212391138076782,
      "learning_rate": 2.7626666666666668e-05,
      "loss": 0.0023,
      "step": 67120
    },
    {
      "epoch": 3.5802666666666667,
      "grad_norm": 0.41505691409111023,
      "learning_rate": 2.7623333333333334e-05,
      "loss": 0.0016,
      "step": 67130
    },
    {
      "epoch": 3.5808,
      "grad_norm": 0.05907239764928818,
      "learning_rate": 2.762e-05,
      "loss": 0.0018,
      "step": 67140
    },
    {
      "epoch": 3.5813333333333333,
      "grad_norm": 0.17157304286956787,
      "learning_rate": 2.761666666666667e-05,
      "loss": 0.0016,
      "step": 67150
    },
    {
      "epoch": 3.5818666666666665,
      "grad_norm": 0.3589225709438324,
      "learning_rate": 2.7613333333333335e-05,
      "loss": 0.0018,
      "step": 67160
    },
    {
      "epoch": 3.5824,
      "grad_norm": 0.31991928815841675,
      "learning_rate": 2.761e-05,
      "loss": 0.0022,
      "step": 67170
    },
    {
      "epoch": 3.582933333333333,
      "grad_norm": 0.32139846682548523,
      "learning_rate": 2.760666666666667e-05,
      "loss": 0.0014,
      "step": 67180
    },
    {
      "epoch": 3.583466666666667,
      "grad_norm": 0.21324875950813293,
      "learning_rate": 2.7603333333333337e-05,
      "loss": 0.0031,
      "step": 67190
    },
    {
      "epoch": 3.584,
      "grad_norm": 0.12453251332044601,
      "learning_rate": 2.7600000000000003e-05,
      "loss": 0.0019,
      "step": 67200
    },
    {
      "epoch": 3.5845333333333333,
      "grad_norm": 0.2755226790904999,
      "learning_rate": 2.759666666666667e-05,
      "loss": 0.0012,
      "step": 67210
    },
    {
      "epoch": 3.5850666666666666,
      "grad_norm": 0.38036179542541504,
      "learning_rate": 2.7593333333333332e-05,
      "loss": 0.0027,
      "step": 67220
    },
    {
      "epoch": 3.5856,
      "grad_norm": 0.06892179697751999,
      "learning_rate": 2.759e-05,
      "loss": 0.0018,
      "step": 67230
    },
    {
      "epoch": 3.586133333333333,
      "grad_norm": 0.12234675884246826,
      "learning_rate": 2.7586666666666665e-05,
      "loss": 0.0018,
      "step": 67240
    },
    {
      "epoch": 3.586666666666667,
      "grad_norm": 0.15138643980026245,
      "learning_rate": 2.7583333333333334e-05,
      "loss": 0.0014,
      "step": 67250
    },
    {
      "epoch": 3.5872,
      "grad_norm": 0.09287841618061066,
      "learning_rate": 2.758e-05,
      "loss": 0.0024,
      "step": 67260
    },
    {
      "epoch": 3.5877333333333334,
      "grad_norm": 0.6737437844276428,
      "learning_rate": 2.7576666666666666e-05,
      "loss": 0.002,
      "step": 67270
    },
    {
      "epoch": 3.5882666666666667,
      "grad_norm": 0.6180534362792969,
      "learning_rate": 2.7573333333333336e-05,
      "loss": 0.0016,
      "step": 67280
    },
    {
      "epoch": 3.5888,
      "grad_norm": 0.49719148874282837,
      "learning_rate": 2.7570000000000002e-05,
      "loss": 0.0014,
      "step": 67290
    },
    {
      "epoch": 3.5893333333333333,
      "grad_norm": 0.26169848442077637,
      "learning_rate": 2.7566666666666668e-05,
      "loss": 0.0018,
      "step": 67300
    },
    {
      "epoch": 3.5898666666666665,
      "grad_norm": 0.09461046010255814,
      "learning_rate": 2.7563333333333334e-05,
      "loss": 0.0017,
      "step": 67310
    },
    {
      "epoch": 3.5904,
      "grad_norm": 0.0353974774479866,
      "learning_rate": 2.7560000000000004e-05,
      "loss": 0.0015,
      "step": 67320
    },
    {
      "epoch": 3.590933333333333,
      "grad_norm": 0.5634683966636658,
      "learning_rate": 2.755666666666667e-05,
      "loss": 0.0023,
      "step": 67330
    },
    {
      "epoch": 3.591466666666667,
      "grad_norm": 0.06554453074932098,
      "learning_rate": 2.7553333333333336e-05,
      "loss": 0.0016,
      "step": 67340
    },
    {
      "epoch": 3.592,
      "grad_norm": 0.35182318091392517,
      "learning_rate": 2.7550000000000002e-05,
      "loss": 0.0019,
      "step": 67350
    },
    {
      "epoch": 3.5925333333333334,
      "grad_norm": 0.1377989500761032,
      "learning_rate": 2.7546666666666672e-05,
      "loss": 0.0018,
      "step": 67360
    },
    {
      "epoch": 3.5930666666666666,
      "grad_norm": 0.17544366419315338,
      "learning_rate": 2.754333333333333e-05,
      "loss": 0.0018,
      "step": 67370
    },
    {
      "epoch": 3.5936,
      "grad_norm": 0.055104952305555344,
      "learning_rate": 2.754e-05,
      "loss": 0.0022,
      "step": 67380
    },
    {
      "epoch": 3.594133333333333,
      "grad_norm": 0.17794272303581238,
      "learning_rate": 2.7536666666666667e-05,
      "loss": 0.0023,
      "step": 67390
    },
    {
      "epoch": 3.594666666666667,
      "grad_norm": 0.49200430512428284,
      "learning_rate": 2.7533333333333333e-05,
      "loss": 0.0019,
      "step": 67400
    },
    {
      "epoch": 3.5952,
      "grad_norm": 0.2451346218585968,
      "learning_rate": 2.753e-05,
      "loss": 0.002,
      "step": 67410
    },
    {
      "epoch": 3.5957333333333334,
      "grad_norm": 0.46912646293640137,
      "learning_rate": 2.752666666666667e-05,
      "loss": 0.0019,
      "step": 67420
    },
    {
      "epoch": 3.5962666666666667,
      "grad_norm": 0.17884501814842224,
      "learning_rate": 2.7523333333333335e-05,
      "loss": 0.0021,
      "step": 67430
    },
    {
      "epoch": 3.5968,
      "grad_norm": 0.1845499724149704,
      "learning_rate": 2.752e-05,
      "loss": 0.0029,
      "step": 67440
    },
    {
      "epoch": 3.5973333333333333,
      "grad_norm": 0.11972961574792862,
      "learning_rate": 2.7516666666666667e-05,
      "loss": 0.0015,
      "step": 67450
    },
    {
      "epoch": 3.5978666666666665,
      "grad_norm": 0.24326546490192413,
      "learning_rate": 2.7513333333333336e-05,
      "loss": 0.0015,
      "step": 67460
    },
    {
      "epoch": 3.5984,
      "grad_norm": 0.06146993115544319,
      "learning_rate": 2.7510000000000003e-05,
      "loss": 0.0016,
      "step": 67470
    },
    {
      "epoch": 3.598933333333333,
      "grad_norm": 0.4139545261859894,
      "learning_rate": 2.750666666666667e-05,
      "loss": 0.0021,
      "step": 67480
    },
    {
      "epoch": 3.599466666666667,
      "grad_norm": 0.06207366660237312,
      "learning_rate": 2.7503333333333335e-05,
      "loss": 0.0022,
      "step": 67490
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.09012456983327866,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 0.0016,
      "step": 67500
    },
    {
      "epoch": 3.6005333333333334,
      "grad_norm": 0.05801424756646156,
      "learning_rate": 2.749666666666667e-05,
      "loss": 0.0024,
      "step": 67510
    },
    {
      "epoch": 3.6010666666666666,
      "grad_norm": 0.5021725296974182,
      "learning_rate": 2.7493333333333333e-05,
      "loss": 0.0037,
      "step": 67520
    },
    {
      "epoch": 3.6016,
      "grad_norm": 0.2346918135881424,
      "learning_rate": 2.749e-05,
      "loss": 0.0018,
      "step": 67530
    },
    {
      "epoch": 3.602133333333333,
      "grad_norm": 0.29885315895080566,
      "learning_rate": 2.7486666666666666e-05,
      "loss": 0.0016,
      "step": 67540
    },
    {
      "epoch": 3.602666666666667,
      "grad_norm": 0.21692736446857452,
      "learning_rate": 2.748333333333333e-05,
      "loss": 0.0015,
      "step": 67550
    },
    {
      "epoch": 3.6032,
      "grad_norm": 0.3023301064968109,
      "learning_rate": 2.748e-05,
      "loss": 0.0022,
      "step": 67560
    },
    {
      "epoch": 3.6037333333333335,
      "grad_norm": 0.06933528184890747,
      "learning_rate": 2.7476666666666667e-05,
      "loss": 0.0015,
      "step": 67570
    },
    {
      "epoch": 3.6042666666666667,
      "grad_norm": 0.17897073924541473,
      "learning_rate": 2.7473333333333333e-05,
      "loss": 0.0018,
      "step": 67580
    },
    {
      "epoch": 3.6048,
      "grad_norm": 0.12185712903738022,
      "learning_rate": 2.7470000000000003e-05,
      "loss": 0.0032,
      "step": 67590
    },
    {
      "epoch": 3.6053333333333333,
      "grad_norm": 0.23372092843055725,
      "learning_rate": 2.746666666666667e-05,
      "loss": 0.002,
      "step": 67600
    },
    {
      "epoch": 3.6058666666666666,
      "grad_norm": 0.45500174164772034,
      "learning_rate": 2.7463333333333335e-05,
      "loss": 0.0023,
      "step": 67610
    },
    {
      "epoch": 3.6064,
      "grad_norm": 0.302903950214386,
      "learning_rate": 2.746e-05,
      "loss": 0.0021,
      "step": 67620
    },
    {
      "epoch": 3.606933333333333,
      "grad_norm": 0.16100721061229706,
      "learning_rate": 2.745666666666667e-05,
      "loss": 0.0019,
      "step": 67630
    },
    {
      "epoch": 3.607466666666667,
      "grad_norm": 0.23550185561180115,
      "learning_rate": 2.7453333333333337e-05,
      "loss": 0.0018,
      "step": 67640
    },
    {
      "epoch": 3.608,
      "grad_norm": 0.43506771326065063,
      "learning_rate": 2.7450000000000003e-05,
      "loss": 0.003,
      "step": 67650
    },
    {
      "epoch": 3.6085333333333334,
      "grad_norm": 0.6211220622062683,
      "learning_rate": 2.744666666666667e-05,
      "loss": 0.0023,
      "step": 67660
    },
    {
      "epoch": 3.6090666666666666,
      "grad_norm": 0.26086166501045227,
      "learning_rate": 2.7443333333333332e-05,
      "loss": 0.0032,
      "step": 67670
    },
    {
      "epoch": 3.6096,
      "grad_norm": 0.06842269003391266,
      "learning_rate": 2.7439999999999998e-05,
      "loss": 0.0022,
      "step": 67680
    },
    {
      "epoch": 3.610133333333333,
      "grad_norm": 0.20596063137054443,
      "learning_rate": 2.7436666666666668e-05,
      "loss": 0.0018,
      "step": 67690
    },
    {
      "epoch": 3.610666666666667,
      "grad_norm": 0.06290434300899506,
      "learning_rate": 2.7433333333333334e-05,
      "loss": 0.0016,
      "step": 67700
    },
    {
      "epoch": 3.6112,
      "grad_norm": 0.1211153194308281,
      "learning_rate": 2.743e-05,
      "loss": 0.0022,
      "step": 67710
    },
    {
      "epoch": 3.6117333333333335,
      "grad_norm": 0.5620173215866089,
      "learning_rate": 2.7426666666666666e-05,
      "loss": 0.0017,
      "step": 67720
    },
    {
      "epoch": 3.6122666666666667,
      "grad_norm": 0.6745204925537109,
      "learning_rate": 2.7423333333333336e-05,
      "loss": 0.0028,
      "step": 67730
    },
    {
      "epoch": 3.6128,
      "grad_norm": 0.2543684244155884,
      "learning_rate": 2.7420000000000002e-05,
      "loss": 0.0022,
      "step": 67740
    },
    {
      "epoch": 3.6133333333333333,
      "grad_norm": 0.3185308873653412,
      "learning_rate": 2.7416666666666668e-05,
      "loss": 0.0019,
      "step": 67750
    },
    {
      "epoch": 3.6138666666666666,
      "grad_norm": 0.17683172225952148,
      "learning_rate": 2.7413333333333334e-05,
      "loss": 0.002,
      "step": 67760
    },
    {
      "epoch": 3.6144,
      "grad_norm": 0.04040497541427612,
      "learning_rate": 2.7410000000000004e-05,
      "loss": 0.0019,
      "step": 67770
    },
    {
      "epoch": 3.614933333333333,
      "grad_norm": 0.032084591686725616,
      "learning_rate": 2.740666666666667e-05,
      "loss": 0.0027,
      "step": 67780
    },
    {
      "epoch": 3.615466666666667,
      "grad_norm": 0.288133829832077,
      "learning_rate": 2.7403333333333336e-05,
      "loss": 0.0022,
      "step": 67790
    },
    {
      "epoch": 3.616,
      "grad_norm": 0.293260782957077,
      "learning_rate": 2.7400000000000002e-05,
      "loss": 0.0026,
      "step": 67800
    },
    {
      "epoch": 3.6165333333333334,
      "grad_norm": 0.5456571578979492,
      "learning_rate": 2.739666666666667e-05,
      "loss": 0.0022,
      "step": 67810
    },
    {
      "epoch": 3.6170666666666667,
      "grad_norm": 0.18475423753261566,
      "learning_rate": 2.739333333333333e-05,
      "loss": 0.002,
      "step": 67820
    },
    {
      "epoch": 3.6176,
      "grad_norm": 0.1160411536693573,
      "learning_rate": 2.739e-05,
      "loss": 0.0033,
      "step": 67830
    },
    {
      "epoch": 3.618133333333333,
      "grad_norm": 0.1828909069299698,
      "learning_rate": 2.7386666666666666e-05,
      "loss": 0.002,
      "step": 67840
    },
    {
      "epoch": 3.618666666666667,
      "grad_norm": 0.06545491516590118,
      "learning_rate": 2.7383333333333333e-05,
      "loss": 0.0021,
      "step": 67850
    },
    {
      "epoch": 3.6192,
      "grad_norm": 0.23106171190738678,
      "learning_rate": 2.738e-05,
      "loss": 0.0019,
      "step": 67860
    },
    {
      "epoch": 3.6197333333333335,
      "grad_norm": 0.021772395819425583,
      "learning_rate": 2.7376666666666668e-05,
      "loss": 0.0021,
      "step": 67870
    },
    {
      "epoch": 3.6202666666666667,
      "grad_norm": 0.2683495581150055,
      "learning_rate": 2.7373333333333334e-05,
      "loss": 0.0037,
      "step": 67880
    },
    {
      "epoch": 3.6208,
      "grad_norm": 0.2018345296382904,
      "learning_rate": 2.737e-05,
      "loss": 0.0023,
      "step": 67890
    },
    {
      "epoch": 3.6213333333333333,
      "grad_norm": 0.12363743782043457,
      "learning_rate": 2.7366666666666667e-05,
      "loss": 0.0015,
      "step": 67900
    },
    {
      "epoch": 3.6218666666666666,
      "grad_norm": 0.12600015103816986,
      "learning_rate": 2.7363333333333336e-05,
      "loss": 0.0016,
      "step": 67910
    },
    {
      "epoch": 3.6224,
      "grad_norm": 0.04733893275260925,
      "learning_rate": 2.7360000000000002e-05,
      "loss": 0.0016,
      "step": 67920
    },
    {
      "epoch": 3.622933333333333,
      "grad_norm": 0.6992011070251465,
      "learning_rate": 2.735666666666667e-05,
      "loss": 0.0022,
      "step": 67930
    },
    {
      "epoch": 3.6234666666666664,
      "grad_norm": 0.17979662120342255,
      "learning_rate": 2.7353333333333338e-05,
      "loss": 0.0021,
      "step": 67940
    },
    {
      "epoch": 3.624,
      "grad_norm": 0.3541186451911926,
      "learning_rate": 2.7350000000000004e-05,
      "loss": 0.0016,
      "step": 67950
    },
    {
      "epoch": 3.6245333333333334,
      "grad_norm": 0.20913653075695038,
      "learning_rate": 2.734666666666667e-05,
      "loss": 0.0021,
      "step": 67960
    },
    {
      "epoch": 3.6250666666666667,
      "grad_norm": 0.4282605051994324,
      "learning_rate": 2.7343333333333333e-05,
      "loss": 0.0016,
      "step": 67970
    },
    {
      "epoch": 3.6256,
      "grad_norm": 0.06055323779582977,
      "learning_rate": 2.734e-05,
      "loss": 0.0015,
      "step": 67980
    },
    {
      "epoch": 3.626133333333333,
      "grad_norm": 0.5935889482498169,
      "learning_rate": 2.7336666666666665e-05,
      "loss": 0.0025,
      "step": 67990
    },
    {
      "epoch": 3.626666666666667,
      "grad_norm": 0.1484958380460739,
      "learning_rate": 2.733333333333333e-05,
      "loss": 0.0019,
      "step": 68000
    },
    {
      "epoch": 3.6272,
      "grad_norm": 0.2214052677154541,
      "learning_rate": 2.733e-05,
      "loss": 0.0019,
      "step": 68010
    },
    {
      "epoch": 3.6277333333333335,
      "grad_norm": 0.24549166858196259,
      "learning_rate": 2.7326666666666667e-05,
      "loss": 0.0029,
      "step": 68020
    },
    {
      "epoch": 3.6282666666666668,
      "grad_norm": 0.0748070776462555,
      "learning_rate": 2.7323333333333333e-05,
      "loss": 0.0018,
      "step": 68030
    },
    {
      "epoch": 3.6288,
      "grad_norm": 0.48007506132125854,
      "learning_rate": 2.7320000000000003e-05,
      "loss": 0.0015,
      "step": 68040
    },
    {
      "epoch": 3.6293333333333333,
      "grad_norm": 0.04417651146650314,
      "learning_rate": 2.731666666666667e-05,
      "loss": 0.0019,
      "step": 68050
    },
    {
      "epoch": 3.6298666666666666,
      "grad_norm": 0.5647380352020264,
      "learning_rate": 2.7313333333333335e-05,
      "loss": 0.0017,
      "step": 68060
    },
    {
      "epoch": 3.6304,
      "grad_norm": 0.3806614875793457,
      "learning_rate": 2.731e-05,
      "loss": 0.0036,
      "step": 68070
    },
    {
      "epoch": 3.630933333333333,
      "grad_norm": 0.4790000915527344,
      "learning_rate": 2.730666666666667e-05,
      "loss": 0.0027,
      "step": 68080
    },
    {
      "epoch": 3.6314666666666664,
      "grad_norm": 0.5364407300949097,
      "learning_rate": 2.7303333333333337e-05,
      "loss": 0.0026,
      "step": 68090
    },
    {
      "epoch": 3.632,
      "grad_norm": 0.4387972950935364,
      "learning_rate": 2.7300000000000003e-05,
      "loss": 0.0014,
      "step": 68100
    },
    {
      "epoch": 3.6325333333333334,
      "grad_norm": 0.396858274936676,
      "learning_rate": 2.729666666666667e-05,
      "loss": 0.0027,
      "step": 68110
    },
    {
      "epoch": 3.6330666666666667,
      "grad_norm": 0.2773593068122864,
      "learning_rate": 2.7293333333333332e-05,
      "loss": 0.0021,
      "step": 68120
    },
    {
      "epoch": 3.6336,
      "grad_norm": 0.44494539499282837,
      "learning_rate": 2.7289999999999998e-05,
      "loss": 0.0015,
      "step": 68130
    },
    {
      "epoch": 3.634133333333333,
      "grad_norm": 0.11992163956165314,
      "learning_rate": 2.7286666666666667e-05,
      "loss": 0.0031,
      "step": 68140
    },
    {
      "epoch": 3.634666666666667,
      "grad_norm": 0.4161938428878784,
      "learning_rate": 2.7283333333333334e-05,
      "loss": 0.0015,
      "step": 68150
    },
    {
      "epoch": 3.6352,
      "grad_norm": 0.2825954556465149,
      "learning_rate": 2.728e-05,
      "loss": 0.0018,
      "step": 68160
    },
    {
      "epoch": 3.6357333333333335,
      "grad_norm": 0.06193096563220024,
      "learning_rate": 2.7276666666666666e-05,
      "loss": 0.0018,
      "step": 68170
    },
    {
      "epoch": 3.6362666666666668,
      "grad_norm": 0.1839025467634201,
      "learning_rate": 2.7273333333333335e-05,
      "loss": 0.0024,
      "step": 68180
    },
    {
      "epoch": 3.6368,
      "grad_norm": 0.598118245601654,
      "learning_rate": 2.727e-05,
      "loss": 0.0023,
      "step": 68190
    },
    {
      "epoch": 3.6373333333333333,
      "grad_norm": 0.3004496991634369,
      "learning_rate": 2.7266666666666668e-05,
      "loss": 0.0022,
      "step": 68200
    },
    {
      "epoch": 3.6378666666666666,
      "grad_norm": 0.24718743562698364,
      "learning_rate": 2.7263333333333334e-05,
      "loss": 0.0024,
      "step": 68210
    },
    {
      "epoch": 3.6384,
      "grad_norm": 0.27751848101615906,
      "learning_rate": 2.7260000000000003e-05,
      "loss": 0.0028,
      "step": 68220
    },
    {
      "epoch": 3.638933333333333,
      "grad_norm": 0.17282134294509888,
      "learning_rate": 2.725666666666667e-05,
      "loss": 0.002,
      "step": 68230
    },
    {
      "epoch": 3.6394666666666664,
      "grad_norm": 0.10558594018220901,
      "learning_rate": 2.7253333333333336e-05,
      "loss": 0.0014,
      "step": 68240
    },
    {
      "epoch": 3.64,
      "grad_norm": 0.1232379898428917,
      "learning_rate": 2.725e-05,
      "loss": 0.0018,
      "step": 68250
    },
    {
      "epoch": 3.6405333333333334,
      "grad_norm": 0.07301781326532364,
      "learning_rate": 2.724666666666667e-05,
      "loss": 0.0021,
      "step": 68260
    },
    {
      "epoch": 3.6410666666666667,
      "grad_norm": 0.030825437977910042,
      "learning_rate": 2.7243333333333337e-05,
      "loss": 0.0018,
      "step": 68270
    },
    {
      "epoch": 3.6416,
      "grad_norm": 0.06304864585399628,
      "learning_rate": 2.724e-05,
      "loss": 0.0018,
      "step": 68280
    },
    {
      "epoch": 3.6421333333333332,
      "grad_norm": 0.5550140142440796,
      "learning_rate": 2.7236666666666666e-05,
      "loss": 0.0019,
      "step": 68290
    },
    {
      "epoch": 3.642666666666667,
      "grad_norm": 0.17975999414920807,
      "learning_rate": 2.7233333333333332e-05,
      "loss": 0.0022,
      "step": 68300
    },
    {
      "epoch": 3.6432,
      "grad_norm": 0.13446320593357086,
      "learning_rate": 2.723e-05,
      "loss": 0.0027,
      "step": 68310
    },
    {
      "epoch": 3.6437333333333335,
      "grad_norm": 0.06803733110427856,
      "learning_rate": 2.7226666666666668e-05,
      "loss": 0.0019,
      "step": 68320
    },
    {
      "epoch": 3.6442666666666668,
      "grad_norm": 0.1250479370355606,
      "learning_rate": 2.7223333333333334e-05,
      "loss": 0.0018,
      "step": 68330
    },
    {
      "epoch": 3.6448,
      "grad_norm": 0.38716843724250793,
      "learning_rate": 2.722e-05,
      "loss": 0.002,
      "step": 68340
    },
    {
      "epoch": 3.6453333333333333,
      "grad_norm": 0.14635775983333588,
      "learning_rate": 2.7216666666666666e-05,
      "loss": 0.0018,
      "step": 68350
    },
    {
      "epoch": 3.6458666666666666,
      "grad_norm": 0.39404675364494324,
      "learning_rate": 2.7213333333333336e-05,
      "loss": 0.0015,
      "step": 68360
    },
    {
      "epoch": 3.6464,
      "grad_norm": 0.0444260835647583,
      "learning_rate": 2.7210000000000002e-05,
      "loss": 0.0015,
      "step": 68370
    },
    {
      "epoch": 3.646933333333333,
      "grad_norm": 0.23999232053756714,
      "learning_rate": 2.7206666666666668e-05,
      "loss": 0.0017,
      "step": 68380
    },
    {
      "epoch": 3.6474666666666664,
      "grad_norm": 0.6503468155860901,
      "learning_rate": 2.7203333333333338e-05,
      "loss": 0.0027,
      "step": 68390
    },
    {
      "epoch": 3.648,
      "grad_norm": 0.2345210611820221,
      "learning_rate": 2.7200000000000004e-05,
      "loss": 0.0016,
      "step": 68400
    },
    {
      "epoch": 3.6485333333333334,
      "grad_norm": 0.4788617193698883,
      "learning_rate": 2.719666666666667e-05,
      "loss": 0.0027,
      "step": 68410
    },
    {
      "epoch": 3.6490666666666667,
      "grad_norm": 0.06470055133104324,
      "learning_rate": 2.7193333333333336e-05,
      "loss": 0.0025,
      "step": 68420
    },
    {
      "epoch": 3.6496,
      "grad_norm": 0.318989634513855,
      "learning_rate": 2.719e-05,
      "loss": 0.0022,
      "step": 68430
    },
    {
      "epoch": 3.6501333333333332,
      "grad_norm": 0.06292819231748581,
      "learning_rate": 2.7186666666666665e-05,
      "loss": 0.0033,
      "step": 68440
    },
    {
      "epoch": 3.6506666666666665,
      "grad_norm": 0.03503190726041794,
      "learning_rate": 2.7183333333333335e-05,
      "loss": 0.0029,
      "step": 68450
    },
    {
      "epoch": 3.6512000000000002,
      "grad_norm": 0.17345313727855682,
      "learning_rate": 2.718e-05,
      "loss": 0.0022,
      "step": 68460
    },
    {
      "epoch": 3.6517333333333335,
      "grad_norm": 0.21705195307731628,
      "learning_rate": 2.7176666666666667e-05,
      "loss": 0.0019,
      "step": 68470
    },
    {
      "epoch": 3.6522666666666668,
      "grad_norm": 0.042762354016304016,
      "learning_rate": 2.7173333333333333e-05,
      "loss": 0.0025,
      "step": 68480
    },
    {
      "epoch": 3.6528,
      "grad_norm": 0.4614112675189972,
      "learning_rate": 2.7170000000000002e-05,
      "loss": 0.002,
      "step": 68490
    },
    {
      "epoch": 3.6533333333333333,
      "grad_norm": 0.5580258965492249,
      "learning_rate": 2.716666666666667e-05,
      "loss": 0.0029,
      "step": 68500
    },
    {
      "epoch": 3.6538666666666666,
      "grad_norm": 0.26480159163475037,
      "learning_rate": 2.7163333333333335e-05,
      "loss": 0.0023,
      "step": 68510
    },
    {
      "epoch": 3.6544,
      "grad_norm": 0.09360965341329575,
      "learning_rate": 2.716e-05,
      "loss": 0.0021,
      "step": 68520
    },
    {
      "epoch": 3.654933333333333,
      "grad_norm": 0.2443186193704605,
      "learning_rate": 2.715666666666667e-05,
      "loss": 0.0037,
      "step": 68530
    },
    {
      "epoch": 3.6554666666666664,
      "grad_norm": 0.2643635869026184,
      "learning_rate": 2.7153333333333337e-05,
      "loss": 0.0023,
      "step": 68540
    },
    {
      "epoch": 3.656,
      "grad_norm": 0.44977661967277527,
      "learning_rate": 2.7150000000000003e-05,
      "loss": 0.0029,
      "step": 68550
    },
    {
      "epoch": 3.6565333333333334,
      "grad_norm": 0.08196178078651428,
      "learning_rate": 2.714666666666667e-05,
      "loss": 0.0022,
      "step": 68560
    },
    {
      "epoch": 3.6570666666666667,
      "grad_norm": 0.23310473561286926,
      "learning_rate": 2.7143333333333338e-05,
      "loss": 0.0036,
      "step": 68570
    },
    {
      "epoch": 3.6576,
      "grad_norm": 0.069504514336586,
      "learning_rate": 2.7139999999999998e-05,
      "loss": 0.0017,
      "step": 68580
    },
    {
      "epoch": 3.6581333333333332,
      "grad_norm": 0.14907753467559814,
      "learning_rate": 2.7136666666666667e-05,
      "loss": 0.0022,
      "step": 68590
    },
    {
      "epoch": 3.6586666666666665,
      "grad_norm": 0.17297185957431793,
      "learning_rate": 2.7133333333333333e-05,
      "loss": 0.002,
      "step": 68600
    },
    {
      "epoch": 3.6592000000000002,
      "grad_norm": 0.08885197341442108,
      "learning_rate": 2.713e-05,
      "loss": 0.0017,
      "step": 68610
    },
    {
      "epoch": 3.6597333333333335,
      "grad_norm": 0.3020898997783661,
      "learning_rate": 2.7126666666666666e-05,
      "loss": 0.0022,
      "step": 68620
    },
    {
      "epoch": 3.660266666666667,
      "grad_norm": 0.2073066085577011,
      "learning_rate": 2.7123333333333335e-05,
      "loss": 0.0019,
      "step": 68630
    },
    {
      "epoch": 3.6608,
      "grad_norm": 0.26907819509506226,
      "learning_rate": 2.712e-05,
      "loss": 0.0023,
      "step": 68640
    },
    {
      "epoch": 3.6613333333333333,
      "grad_norm": 0.6731514930725098,
      "learning_rate": 2.7116666666666667e-05,
      "loss": 0.0033,
      "step": 68650
    },
    {
      "epoch": 3.6618666666666666,
      "grad_norm": 0.4877142012119293,
      "learning_rate": 2.7113333333333333e-05,
      "loss": 0.0021,
      "step": 68660
    },
    {
      "epoch": 3.6624,
      "grad_norm": 0.38644224405288696,
      "learning_rate": 2.7110000000000003e-05,
      "loss": 0.0024,
      "step": 68670
    },
    {
      "epoch": 3.662933333333333,
      "grad_norm": 0.050187449902296066,
      "learning_rate": 2.710666666666667e-05,
      "loss": 0.0017,
      "step": 68680
    },
    {
      "epoch": 3.6634666666666664,
      "grad_norm": 0.15643799304962158,
      "learning_rate": 2.7103333333333335e-05,
      "loss": 0.0015,
      "step": 68690
    },
    {
      "epoch": 3.664,
      "grad_norm": 0.04214111343026161,
      "learning_rate": 2.7100000000000005e-05,
      "loss": 0.0019,
      "step": 68700
    },
    {
      "epoch": 3.6645333333333334,
      "grad_norm": 0.4858095645904541,
      "learning_rate": 2.709666666666667e-05,
      "loss": 0.0027,
      "step": 68710
    },
    {
      "epoch": 3.6650666666666667,
      "grad_norm": 0.06104390695691109,
      "learning_rate": 2.7093333333333337e-05,
      "loss": 0.0014,
      "step": 68720
    },
    {
      "epoch": 3.6656,
      "grad_norm": 0.11400597542524338,
      "learning_rate": 2.709e-05,
      "loss": 0.0011,
      "step": 68730
    },
    {
      "epoch": 3.6661333333333332,
      "grad_norm": 0.21312664449214935,
      "learning_rate": 2.7086666666666666e-05,
      "loss": 0.002,
      "step": 68740
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 0.3317364752292633,
      "learning_rate": 2.7083333333333332e-05,
      "loss": 0.0024,
      "step": 68750
    },
    {
      "epoch": 3.6672000000000002,
      "grad_norm": 0.4498123228549957,
      "learning_rate": 2.7079999999999998e-05,
      "loss": 0.0017,
      "step": 68760
    },
    {
      "epoch": 3.6677333333333335,
      "grad_norm": 0.48895397782325745,
      "learning_rate": 2.7076666666666668e-05,
      "loss": 0.0021,
      "step": 68770
    },
    {
      "epoch": 3.668266666666667,
      "grad_norm": 0.08135776966810226,
      "learning_rate": 2.7073333333333334e-05,
      "loss": 0.0027,
      "step": 68780
    },
    {
      "epoch": 3.6688,
      "grad_norm": 0.032596345990896225,
      "learning_rate": 2.707e-05,
      "loss": 0.0024,
      "step": 68790
    },
    {
      "epoch": 3.6693333333333333,
      "grad_norm": 0.1570962369441986,
      "learning_rate": 2.706666666666667e-05,
      "loss": 0.0018,
      "step": 68800
    },
    {
      "epoch": 3.6698666666666666,
      "grad_norm": 0.2425878643989563,
      "learning_rate": 2.7063333333333336e-05,
      "loss": 0.002,
      "step": 68810
    },
    {
      "epoch": 3.6704,
      "grad_norm": 0.06846363097429276,
      "learning_rate": 2.7060000000000002e-05,
      "loss": 0.0014,
      "step": 68820
    },
    {
      "epoch": 3.670933333333333,
      "grad_norm": 0.2434060275554657,
      "learning_rate": 2.7056666666666668e-05,
      "loss": 0.0017,
      "step": 68830
    },
    {
      "epoch": 3.6714666666666664,
      "grad_norm": 0.4706309735774994,
      "learning_rate": 2.7053333333333337e-05,
      "loss": 0.0018,
      "step": 68840
    },
    {
      "epoch": 3.672,
      "grad_norm": 0.156929150223732,
      "learning_rate": 2.7050000000000004e-05,
      "loss": 0.0018,
      "step": 68850
    },
    {
      "epoch": 3.6725333333333334,
      "grad_norm": 0.471250981092453,
      "learning_rate": 2.704666666666667e-05,
      "loss": 0.0027,
      "step": 68860
    },
    {
      "epoch": 3.6730666666666667,
      "grad_norm": 0.19192105531692505,
      "learning_rate": 2.7043333333333336e-05,
      "loss": 0.002,
      "step": 68870
    },
    {
      "epoch": 3.6736,
      "grad_norm": 0.10832981020212173,
      "learning_rate": 2.704e-05,
      "loss": 0.0021,
      "step": 68880
    },
    {
      "epoch": 3.6741333333333333,
      "grad_norm": 0.21024274826049805,
      "learning_rate": 2.7036666666666665e-05,
      "loss": 0.002,
      "step": 68890
    },
    {
      "epoch": 3.6746666666666665,
      "grad_norm": 0.3873439431190491,
      "learning_rate": 2.7033333333333334e-05,
      "loss": 0.0018,
      "step": 68900
    },
    {
      "epoch": 3.6752000000000002,
      "grad_norm": 0.12052126973867416,
      "learning_rate": 2.703e-05,
      "loss": 0.0025,
      "step": 68910
    },
    {
      "epoch": 3.6757333333333335,
      "grad_norm": 0.14121156930923462,
      "learning_rate": 2.7026666666666667e-05,
      "loss": 0.0022,
      "step": 68920
    },
    {
      "epoch": 3.676266666666667,
      "grad_norm": 0.14496859908103943,
      "learning_rate": 2.7023333333333333e-05,
      "loss": 0.0014,
      "step": 68930
    },
    {
      "epoch": 3.6768,
      "grad_norm": 0.1299653947353363,
      "learning_rate": 2.7020000000000002e-05,
      "loss": 0.0031,
      "step": 68940
    },
    {
      "epoch": 3.6773333333333333,
      "grad_norm": 0.051867809146642685,
      "learning_rate": 2.701666666666667e-05,
      "loss": 0.0017,
      "step": 68950
    },
    {
      "epoch": 3.6778666666666666,
      "grad_norm": 0.4437715709209442,
      "learning_rate": 2.7013333333333334e-05,
      "loss": 0.0026,
      "step": 68960
    },
    {
      "epoch": 3.6784,
      "grad_norm": 0.1480788141489029,
      "learning_rate": 2.701e-05,
      "loss": 0.0015,
      "step": 68970
    },
    {
      "epoch": 3.678933333333333,
      "grad_norm": 0.026056131348013878,
      "learning_rate": 2.700666666666667e-05,
      "loss": 0.002,
      "step": 68980
    },
    {
      "epoch": 3.6794666666666664,
      "grad_norm": 0.23462364077568054,
      "learning_rate": 2.7003333333333336e-05,
      "loss": 0.0018,
      "step": 68990
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.16498377919197083,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.002,
      "step": 69000
    },
    {
      "epoch": 3.6805333333333334,
      "grad_norm": 0.4709773063659668,
      "learning_rate": 2.699666666666667e-05,
      "loss": 0.002,
      "step": 69010
    },
    {
      "epoch": 3.6810666666666667,
      "grad_norm": 0.2861286401748657,
      "learning_rate": 2.6993333333333338e-05,
      "loss": 0.0028,
      "step": 69020
    },
    {
      "epoch": 3.6816,
      "grad_norm": 0.15465599298477173,
      "learning_rate": 2.6989999999999997e-05,
      "loss": 0.002,
      "step": 69030
    },
    {
      "epoch": 3.6821333333333333,
      "grad_norm": 0.2644617259502411,
      "learning_rate": 2.6986666666666667e-05,
      "loss": 0.0016,
      "step": 69040
    },
    {
      "epoch": 3.6826666666666665,
      "grad_norm": 0.1268668919801712,
      "learning_rate": 2.6983333333333333e-05,
      "loss": 0.0023,
      "step": 69050
    },
    {
      "epoch": 3.6832000000000003,
      "grad_norm": 0.2991195321083069,
      "learning_rate": 2.698e-05,
      "loss": 0.0025,
      "step": 69060
    },
    {
      "epoch": 3.6837333333333335,
      "grad_norm": 0.05879656970500946,
      "learning_rate": 2.6976666666666665e-05,
      "loss": 0.0024,
      "step": 69070
    },
    {
      "epoch": 3.684266666666667,
      "grad_norm": 0.44406041502952576,
      "learning_rate": 2.6973333333333335e-05,
      "loss": 0.0023,
      "step": 69080
    },
    {
      "epoch": 3.6848,
      "grad_norm": 0.15263943374156952,
      "learning_rate": 2.697e-05,
      "loss": 0.0017,
      "step": 69090
    },
    {
      "epoch": 3.6853333333333333,
      "grad_norm": 0.1809050738811493,
      "learning_rate": 2.6966666666666667e-05,
      "loss": 0.0029,
      "step": 69100
    },
    {
      "epoch": 3.6858666666666666,
      "grad_norm": 0.3278129994869232,
      "learning_rate": 2.6963333333333333e-05,
      "loss": 0.0017,
      "step": 69110
    },
    {
      "epoch": 3.6864,
      "grad_norm": 0.4109390377998352,
      "learning_rate": 2.6960000000000003e-05,
      "loss": 0.0027,
      "step": 69120
    },
    {
      "epoch": 3.686933333333333,
      "grad_norm": 0.1201169565320015,
      "learning_rate": 2.695666666666667e-05,
      "loss": 0.004,
      "step": 69130
    },
    {
      "epoch": 3.6874666666666664,
      "grad_norm": 0.16630293428897858,
      "learning_rate": 2.6953333333333335e-05,
      "loss": 0.0023,
      "step": 69140
    },
    {
      "epoch": 3.6879999999999997,
      "grad_norm": 0.07387055456638336,
      "learning_rate": 2.6950000000000005e-05,
      "loss": 0.0013,
      "step": 69150
    },
    {
      "epoch": 3.6885333333333334,
      "grad_norm": 0.10120376199483871,
      "learning_rate": 2.694666666666667e-05,
      "loss": 0.0021,
      "step": 69160
    },
    {
      "epoch": 3.6890666666666667,
      "grad_norm": 0.38142725825309753,
      "learning_rate": 2.6943333333333337e-05,
      "loss": 0.0028,
      "step": 69170
    },
    {
      "epoch": 3.6896,
      "grad_norm": 0.15803058445453644,
      "learning_rate": 2.694e-05,
      "loss": 0.0019,
      "step": 69180
    },
    {
      "epoch": 3.6901333333333333,
      "grad_norm": 0.40147432684898376,
      "learning_rate": 2.6936666666666666e-05,
      "loss": 0.0024,
      "step": 69190
    },
    {
      "epoch": 3.6906666666666665,
      "grad_norm": 0.09712227433919907,
      "learning_rate": 2.6933333333333332e-05,
      "loss": 0.0025,
      "step": 69200
    },
    {
      "epoch": 3.6912000000000003,
      "grad_norm": 0.15699869394302368,
      "learning_rate": 2.693e-05,
      "loss": 0.0024,
      "step": 69210
    },
    {
      "epoch": 3.6917333333333335,
      "grad_norm": 0.17668119072914124,
      "learning_rate": 2.6926666666666667e-05,
      "loss": 0.0024,
      "step": 69220
    },
    {
      "epoch": 3.692266666666667,
      "grad_norm": 0.2450651079416275,
      "learning_rate": 2.6923333333333334e-05,
      "loss": 0.0024,
      "step": 69230
    },
    {
      "epoch": 3.6928,
      "grad_norm": 0.20734930038452148,
      "learning_rate": 2.692e-05,
      "loss": 0.0026,
      "step": 69240
    },
    {
      "epoch": 3.6933333333333334,
      "grad_norm": 0.1062813326716423,
      "learning_rate": 2.691666666666667e-05,
      "loss": 0.0015,
      "step": 69250
    },
    {
      "epoch": 3.6938666666666666,
      "grad_norm": 0.18260416388511658,
      "learning_rate": 2.6913333333333335e-05,
      "loss": 0.0017,
      "step": 69260
    },
    {
      "epoch": 3.6944,
      "grad_norm": 0.20816387236118317,
      "learning_rate": 2.691e-05,
      "loss": 0.0024,
      "step": 69270
    },
    {
      "epoch": 3.694933333333333,
      "grad_norm": 0.36211681365966797,
      "learning_rate": 2.6906666666666668e-05,
      "loss": 0.0025,
      "step": 69280
    },
    {
      "epoch": 3.6954666666666665,
      "grad_norm": 0.17524972558021545,
      "learning_rate": 2.6903333333333337e-05,
      "loss": 0.0024,
      "step": 69290
    },
    {
      "epoch": 3.6959999999999997,
      "grad_norm": 0.1509769707918167,
      "learning_rate": 2.6900000000000003e-05,
      "loss": 0.0025,
      "step": 69300
    },
    {
      "epoch": 3.6965333333333334,
      "grad_norm": 0.12279434502124786,
      "learning_rate": 2.689666666666667e-05,
      "loss": 0.0017,
      "step": 69310
    },
    {
      "epoch": 3.6970666666666667,
      "grad_norm": 0.395831823348999,
      "learning_rate": 2.6893333333333336e-05,
      "loss": 0.0029,
      "step": 69320
    },
    {
      "epoch": 3.6976,
      "grad_norm": 0.26449012756347656,
      "learning_rate": 2.689e-05,
      "loss": 0.0023,
      "step": 69330
    },
    {
      "epoch": 3.6981333333333333,
      "grad_norm": 0.32999876141548157,
      "learning_rate": 2.6886666666666664e-05,
      "loss": 0.002,
      "step": 69340
    },
    {
      "epoch": 3.6986666666666665,
      "grad_norm": 0.4103989601135254,
      "learning_rate": 2.6883333333333334e-05,
      "loss": 0.0039,
      "step": 69350
    },
    {
      "epoch": 3.6992000000000003,
      "grad_norm": 0.5368407964706421,
      "learning_rate": 2.688e-05,
      "loss": 0.002,
      "step": 69360
    },
    {
      "epoch": 3.6997333333333335,
      "grad_norm": 0.18409128487110138,
      "learning_rate": 2.6876666666666666e-05,
      "loss": 0.0014,
      "step": 69370
    },
    {
      "epoch": 3.700266666666667,
      "grad_norm": 0.5017463564872742,
      "learning_rate": 2.6873333333333332e-05,
      "loss": 0.0023,
      "step": 69380
    },
    {
      "epoch": 3.7008,
      "grad_norm": 0.4808897376060486,
      "learning_rate": 2.6870000000000002e-05,
      "loss": 0.004,
      "step": 69390
    },
    {
      "epoch": 3.7013333333333334,
      "grad_norm": 0.2425219863653183,
      "learning_rate": 2.6866666666666668e-05,
      "loss": 0.0041,
      "step": 69400
    },
    {
      "epoch": 3.7018666666666666,
      "grad_norm": 0.11840716004371643,
      "learning_rate": 2.6863333333333334e-05,
      "loss": 0.0029,
      "step": 69410
    },
    {
      "epoch": 3.7024,
      "grad_norm": 0.18002112209796906,
      "learning_rate": 2.686e-05,
      "loss": 0.0012,
      "step": 69420
    },
    {
      "epoch": 3.702933333333333,
      "grad_norm": 0.4021027386188507,
      "learning_rate": 2.685666666666667e-05,
      "loss": 0.0025,
      "step": 69430
    },
    {
      "epoch": 3.7034666666666665,
      "grad_norm": 0.22490577399730682,
      "learning_rate": 2.6853333333333336e-05,
      "loss": 0.0015,
      "step": 69440
    },
    {
      "epoch": 3.7039999999999997,
      "grad_norm": 0.05902385339140892,
      "learning_rate": 2.6850000000000002e-05,
      "loss": 0.003,
      "step": 69450
    },
    {
      "epoch": 3.7045333333333335,
      "grad_norm": 0.18573608994483948,
      "learning_rate": 2.684666666666667e-05,
      "loss": 0.0014,
      "step": 69460
    },
    {
      "epoch": 3.7050666666666667,
      "grad_norm": 0.21627987921237946,
      "learning_rate": 2.6843333333333338e-05,
      "loss": 0.0026,
      "step": 69470
    },
    {
      "epoch": 3.7056,
      "grad_norm": 0.21063101291656494,
      "learning_rate": 2.6840000000000004e-05,
      "loss": 0.002,
      "step": 69480
    },
    {
      "epoch": 3.7061333333333333,
      "grad_norm": 0.12203327566385269,
      "learning_rate": 2.6836666666666667e-05,
      "loss": 0.0021,
      "step": 69490
    },
    {
      "epoch": 3.7066666666666666,
      "grad_norm": 0.12347140163183212,
      "learning_rate": 2.6833333333333333e-05,
      "loss": 0.0023,
      "step": 69500
    },
    {
      "epoch": 3.7072000000000003,
      "grad_norm": 0.12117178738117218,
      "learning_rate": 2.683e-05,
      "loss": 0.0018,
      "step": 69510
    },
    {
      "epoch": 3.7077333333333335,
      "grad_norm": 0.1805894523859024,
      "learning_rate": 2.6826666666666665e-05,
      "loss": 0.0028,
      "step": 69520
    },
    {
      "epoch": 3.708266666666667,
      "grad_norm": 0.14761671423912048,
      "learning_rate": 2.6823333333333335e-05,
      "loss": 0.0029,
      "step": 69530
    },
    {
      "epoch": 3.7088,
      "grad_norm": 0.24081644415855408,
      "learning_rate": 2.682e-05,
      "loss": 0.002,
      "step": 69540
    },
    {
      "epoch": 3.7093333333333334,
      "grad_norm": 0.040003035217523575,
      "learning_rate": 2.6816666666666667e-05,
      "loss": 0.0025,
      "step": 69550
    },
    {
      "epoch": 3.7098666666666666,
      "grad_norm": 0.03869343549013138,
      "learning_rate": 2.6813333333333336e-05,
      "loss": 0.0019,
      "step": 69560
    },
    {
      "epoch": 3.7104,
      "grad_norm": 0.668475329875946,
      "learning_rate": 2.6810000000000003e-05,
      "loss": 0.0027,
      "step": 69570
    },
    {
      "epoch": 3.710933333333333,
      "grad_norm": 0.11962532252073288,
      "learning_rate": 2.680666666666667e-05,
      "loss": 0.003,
      "step": 69580
    },
    {
      "epoch": 3.7114666666666665,
      "grad_norm": 0.23446035385131836,
      "learning_rate": 2.6803333333333335e-05,
      "loss": 0.0029,
      "step": 69590
    },
    {
      "epoch": 3.7119999999999997,
      "grad_norm": 0.25537580251693726,
      "learning_rate": 2.6800000000000004e-05,
      "loss": 0.0029,
      "step": 69600
    },
    {
      "epoch": 3.7125333333333335,
      "grad_norm": 0.2692018151283264,
      "learning_rate": 2.679666666666667e-05,
      "loss": 0.0019,
      "step": 69610
    },
    {
      "epoch": 3.7130666666666667,
      "grad_norm": 0.06377393007278442,
      "learning_rate": 2.6793333333333337e-05,
      "loss": 0.0023,
      "step": 69620
    },
    {
      "epoch": 3.7136,
      "grad_norm": 0.31697261333465576,
      "learning_rate": 2.6790000000000003e-05,
      "loss": 0.002,
      "step": 69630
    },
    {
      "epoch": 3.7141333333333333,
      "grad_norm": 0.6552823185920715,
      "learning_rate": 2.6786666666666665e-05,
      "loss": 0.0026,
      "step": 69640
    },
    {
      "epoch": 3.7146666666666666,
      "grad_norm": 0.024161430075764656,
      "learning_rate": 2.678333333333333e-05,
      "loss": 0.0027,
      "step": 69650
    },
    {
      "epoch": 3.7152,
      "grad_norm": 0.25921767950057983,
      "learning_rate": 2.678e-05,
      "loss": 0.0028,
      "step": 69660
    },
    {
      "epoch": 3.7157333333333336,
      "grad_norm": 0.18265950679779053,
      "learning_rate": 2.6776666666666667e-05,
      "loss": 0.0016,
      "step": 69670
    },
    {
      "epoch": 3.716266666666667,
      "grad_norm": 0.061792612075805664,
      "learning_rate": 2.6773333333333333e-05,
      "loss": 0.0025,
      "step": 69680
    },
    {
      "epoch": 3.7168,
      "grad_norm": 0.27843621373176575,
      "learning_rate": 2.677e-05,
      "loss": 0.0018,
      "step": 69690
    },
    {
      "epoch": 3.7173333333333334,
      "grad_norm": 0.29851779341697693,
      "learning_rate": 2.676666666666667e-05,
      "loss": 0.003,
      "step": 69700
    },
    {
      "epoch": 3.7178666666666667,
      "grad_norm": 0.22907203435897827,
      "learning_rate": 2.6763333333333335e-05,
      "loss": 0.0015,
      "step": 69710
    },
    {
      "epoch": 3.7184,
      "grad_norm": 0.41274574398994446,
      "learning_rate": 2.676e-05,
      "loss": 0.0017,
      "step": 69720
    },
    {
      "epoch": 3.718933333333333,
      "grad_norm": 0.6046984791755676,
      "learning_rate": 2.6756666666666667e-05,
      "loss": 0.0022,
      "step": 69730
    },
    {
      "epoch": 3.7194666666666665,
      "grad_norm": 0.6725672483444214,
      "learning_rate": 2.6753333333333337e-05,
      "loss": 0.002,
      "step": 69740
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 0.23073606193065643,
      "learning_rate": 2.6750000000000003e-05,
      "loss": 0.0034,
      "step": 69750
    },
    {
      "epoch": 3.7205333333333335,
      "grad_norm": 0.06948260962963104,
      "learning_rate": 2.674666666666667e-05,
      "loss": 0.0026,
      "step": 69760
    },
    {
      "epoch": 3.7210666666666667,
      "grad_norm": 0.3077142834663391,
      "learning_rate": 2.6743333333333335e-05,
      "loss": 0.0025,
      "step": 69770
    },
    {
      "epoch": 3.7216,
      "grad_norm": 0.17726172506809235,
      "learning_rate": 2.6740000000000005e-05,
      "loss": 0.0021,
      "step": 69780
    },
    {
      "epoch": 3.7221333333333333,
      "grad_norm": 0.11160846799612045,
      "learning_rate": 2.6736666666666664e-05,
      "loss": 0.0019,
      "step": 69790
    },
    {
      "epoch": 3.7226666666666666,
      "grad_norm": 0.5629833936691284,
      "learning_rate": 2.6733333333333334e-05,
      "loss": 0.0018,
      "step": 69800
    },
    {
      "epoch": 3.7232,
      "grad_norm": 0.058074112981557846,
      "learning_rate": 2.673e-05,
      "loss": 0.0023,
      "step": 69810
    },
    {
      "epoch": 3.7237333333333336,
      "grad_norm": 0.5200987458229065,
      "learning_rate": 2.6726666666666666e-05,
      "loss": 0.0033,
      "step": 69820
    },
    {
      "epoch": 3.724266666666667,
      "grad_norm": 0.09405600279569626,
      "learning_rate": 2.6723333333333332e-05,
      "loss": 0.0031,
      "step": 69830
    },
    {
      "epoch": 3.7248,
      "grad_norm": 0.12026344239711761,
      "learning_rate": 2.672e-05,
      "loss": 0.003,
      "step": 69840
    },
    {
      "epoch": 3.7253333333333334,
      "grad_norm": 0.6256605386734009,
      "learning_rate": 2.6716666666666668e-05,
      "loss": 0.0029,
      "step": 69850
    },
    {
      "epoch": 3.7258666666666667,
      "grad_norm": 0.3324888348579407,
      "learning_rate": 2.6713333333333334e-05,
      "loss": 0.0023,
      "step": 69860
    },
    {
      "epoch": 3.7264,
      "grad_norm": 0.24003826081752777,
      "learning_rate": 2.671e-05,
      "loss": 0.0027,
      "step": 69870
    },
    {
      "epoch": 3.726933333333333,
      "grad_norm": 0.12042325735092163,
      "learning_rate": 2.670666666666667e-05,
      "loss": 0.0026,
      "step": 69880
    },
    {
      "epoch": 3.7274666666666665,
      "grad_norm": 0.05689924582839012,
      "learning_rate": 2.6703333333333336e-05,
      "loss": 0.0028,
      "step": 69890
    },
    {
      "epoch": 3.7279999999999998,
      "grad_norm": 0.24776652455329895,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 0.0015,
      "step": 69900
    },
    {
      "epoch": 3.7285333333333335,
      "grad_norm": 0.3554600179195404,
      "learning_rate": 2.669666666666667e-05,
      "loss": 0.0025,
      "step": 69910
    },
    {
      "epoch": 3.7290666666666668,
      "grad_norm": 0.06683317571878433,
      "learning_rate": 2.6693333333333338e-05,
      "loss": 0.0023,
      "step": 69920
    },
    {
      "epoch": 3.7296,
      "grad_norm": 0.2656598389148712,
      "learning_rate": 2.6690000000000004e-05,
      "loss": 0.0024,
      "step": 69930
    },
    {
      "epoch": 3.7301333333333333,
      "grad_norm": 0.10058224201202393,
      "learning_rate": 2.6686666666666666e-05,
      "loss": 0.0019,
      "step": 69940
    },
    {
      "epoch": 3.7306666666666666,
      "grad_norm": 0.2296900749206543,
      "learning_rate": 2.6683333333333333e-05,
      "loss": 0.0012,
      "step": 69950
    },
    {
      "epoch": 3.7312,
      "grad_norm": 0.11767176538705826,
      "learning_rate": 2.668e-05,
      "loss": 0.0018,
      "step": 69960
    },
    {
      "epoch": 3.7317333333333336,
      "grad_norm": 0.2871870994567871,
      "learning_rate": 2.6676666666666665e-05,
      "loss": 0.0017,
      "step": 69970
    },
    {
      "epoch": 3.732266666666667,
      "grad_norm": 0.3649817407131195,
      "learning_rate": 2.6673333333333334e-05,
      "loss": 0.002,
      "step": 69980
    },
    {
      "epoch": 3.7328,
      "grad_norm": 0.14932961761951447,
      "learning_rate": 2.667e-05,
      "loss": 0.0018,
      "step": 69990
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 0.1279749870300293,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.0024,
      "step": 70000
    },
    {
      "epoch": 3.7338666666666667,
      "grad_norm": 0.18588003516197205,
      "learning_rate": 2.6663333333333336e-05,
      "loss": 0.0018,
      "step": 70010
    },
    {
      "epoch": 3.7344,
      "grad_norm": 0.1151648610830307,
      "learning_rate": 2.6660000000000002e-05,
      "loss": 0.0032,
      "step": 70020
    },
    {
      "epoch": 3.734933333333333,
      "grad_norm": 0.3764466941356659,
      "learning_rate": 2.665666666666667e-05,
      "loss": 0.0022,
      "step": 70030
    },
    {
      "epoch": 3.7354666666666665,
      "grad_norm": 0.39421525597572327,
      "learning_rate": 2.6653333333333335e-05,
      "loss": 0.0018,
      "step": 70040
    },
    {
      "epoch": 3.7359999999999998,
      "grad_norm": 0.03164244443178177,
      "learning_rate": 2.6650000000000004e-05,
      "loss": 0.0026,
      "step": 70050
    },
    {
      "epoch": 3.7365333333333335,
      "grad_norm": 0.09557253122329712,
      "learning_rate": 2.664666666666667e-05,
      "loss": 0.0026,
      "step": 70060
    },
    {
      "epoch": 3.7370666666666668,
      "grad_norm": 0.10086102038621902,
      "learning_rate": 2.6643333333333336e-05,
      "loss": 0.0021,
      "step": 70070
    },
    {
      "epoch": 3.7376,
      "grad_norm": 0.0755549818277359,
      "learning_rate": 2.6640000000000002e-05,
      "loss": 0.0031,
      "step": 70080
    },
    {
      "epoch": 3.7381333333333333,
      "grad_norm": 0.24090808629989624,
      "learning_rate": 2.6636666666666665e-05,
      "loss": 0.0029,
      "step": 70090
    },
    {
      "epoch": 3.7386666666666666,
      "grad_norm": 0.27645477652549744,
      "learning_rate": 2.663333333333333e-05,
      "loss": 0.0024,
      "step": 70100
    },
    {
      "epoch": 3.7392,
      "grad_norm": 0.2811110317707062,
      "learning_rate": 2.663e-05,
      "loss": 0.0025,
      "step": 70110
    },
    {
      "epoch": 3.7397333333333336,
      "grad_norm": 0.29942914843559265,
      "learning_rate": 2.6626666666666667e-05,
      "loss": 0.0016,
      "step": 70120
    },
    {
      "epoch": 3.740266666666667,
      "grad_norm": 0.0633523017168045,
      "learning_rate": 2.6623333333333333e-05,
      "loss": 0.0014,
      "step": 70130
    },
    {
      "epoch": 3.7408,
      "grad_norm": 0.32772162556648254,
      "learning_rate": 2.662e-05,
      "loss": 0.0016,
      "step": 70140
    },
    {
      "epoch": 3.7413333333333334,
      "grad_norm": 0.35655200481414795,
      "learning_rate": 2.661666666666667e-05,
      "loss": 0.0017,
      "step": 70150
    },
    {
      "epoch": 3.7418666666666667,
      "grad_norm": 0.2126108705997467,
      "learning_rate": 2.6613333333333335e-05,
      "loss": 0.002,
      "step": 70160
    },
    {
      "epoch": 3.7424,
      "grad_norm": 0.10973844677209854,
      "learning_rate": 2.661e-05,
      "loss": 0.0019,
      "step": 70170
    },
    {
      "epoch": 3.7429333333333332,
      "grad_norm": 0.3171823024749756,
      "learning_rate": 2.6606666666666667e-05,
      "loss": 0.002,
      "step": 70180
    },
    {
      "epoch": 3.7434666666666665,
      "grad_norm": 0.052739884704351425,
      "learning_rate": 2.6603333333333337e-05,
      "loss": 0.0015,
      "step": 70190
    },
    {
      "epoch": 3.7439999999999998,
      "grad_norm": 0.15345633029937744,
      "learning_rate": 2.6600000000000003e-05,
      "loss": 0.0021,
      "step": 70200
    },
    {
      "epoch": 3.7445333333333335,
      "grad_norm": 0.35401007533073425,
      "learning_rate": 2.659666666666667e-05,
      "loss": 0.0017,
      "step": 70210
    },
    {
      "epoch": 3.7450666666666668,
      "grad_norm": 0.1543569564819336,
      "learning_rate": 2.6593333333333335e-05,
      "loss": 0.0021,
      "step": 70220
    },
    {
      "epoch": 3.7456,
      "grad_norm": 0.429874449968338,
      "learning_rate": 2.6590000000000005e-05,
      "loss": 0.0036,
      "step": 70230
    },
    {
      "epoch": 3.7461333333333333,
      "grad_norm": 0.21100743114948273,
      "learning_rate": 2.6586666666666664e-05,
      "loss": 0.0019,
      "step": 70240
    },
    {
      "epoch": 3.7466666666666666,
      "grad_norm": 0.250800758600235,
      "learning_rate": 2.6583333333333333e-05,
      "loss": 0.0015,
      "step": 70250
    },
    {
      "epoch": 3.7472,
      "grad_norm": 0.2975768446922302,
      "learning_rate": 2.658e-05,
      "loss": 0.0019,
      "step": 70260
    },
    {
      "epoch": 3.7477333333333336,
      "grad_norm": 0.0665850043296814,
      "learning_rate": 2.6576666666666666e-05,
      "loss": 0.0033,
      "step": 70270
    },
    {
      "epoch": 3.748266666666667,
      "grad_norm": 0.04454701021313667,
      "learning_rate": 2.6573333333333332e-05,
      "loss": 0.0017,
      "step": 70280
    },
    {
      "epoch": 3.7488,
      "grad_norm": 0.20765694975852966,
      "learning_rate": 2.657e-05,
      "loss": 0.0026,
      "step": 70290
    },
    {
      "epoch": 3.7493333333333334,
      "grad_norm": 0.442699134349823,
      "learning_rate": 2.6566666666666668e-05,
      "loss": 0.002,
      "step": 70300
    },
    {
      "epoch": 3.7498666666666667,
      "grad_norm": 0.4090743958950043,
      "learning_rate": 2.6563333333333334e-05,
      "loss": 0.0021,
      "step": 70310
    },
    {
      "epoch": 3.7504,
      "grad_norm": 0.4522983431816101,
      "learning_rate": 2.6560000000000003e-05,
      "loss": 0.0014,
      "step": 70320
    },
    {
      "epoch": 3.7509333333333332,
      "grad_norm": 0.32907775044441223,
      "learning_rate": 2.655666666666667e-05,
      "loss": 0.0017,
      "step": 70330
    },
    {
      "epoch": 3.7514666666666665,
      "grad_norm": 0.18125459551811218,
      "learning_rate": 2.6553333333333335e-05,
      "loss": 0.0017,
      "step": 70340
    },
    {
      "epoch": 3.752,
      "grad_norm": 0.20698782801628113,
      "learning_rate": 2.655e-05,
      "loss": 0.0019,
      "step": 70350
    },
    {
      "epoch": 3.7525333333333335,
      "grad_norm": 0.02671745792031288,
      "learning_rate": 2.654666666666667e-05,
      "loss": 0.0017,
      "step": 70360
    },
    {
      "epoch": 3.7530666666666668,
      "grad_norm": 0.4667156934738159,
      "learning_rate": 2.6543333333333337e-05,
      "loss": 0.002,
      "step": 70370
    },
    {
      "epoch": 3.7536,
      "grad_norm": 0.3270872235298157,
      "learning_rate": 2.6540000000000003e-05,
      "loss": 0.0023,
      "step": 70380
    },
    {
      "epoch": 3.7541333333333333,
      "grad_norm": 0.07629310339689255,
      "learning_rate": 2.6536666666666666e-05,
      "loss": 0.0025,
      "step": 70390
    },
    {
      "epoch": 3.7546666666666666,
      "grad_norm": 0.3568788170814514,
      "learning_rate": 2.6533333333333332e-05,
      "loss": 0.0024,
      "step": 70400
    },
    {
      "epoch": 3.7552,
      "grad_norm": 0.40586695075035095,
      "learning_rate": 2.653e-05,
      "loss": 0.0022,
      "step": 70410
    },
    {
      "epoch": 3.7557333333333336,
      "grad_norm": 0.13180847465991974,
      "learning_rate": 2.6526666666666668e-05,
      "loss": 0.0016,
      "step": 70420
    },
    {
      "epoch": 3.756266666666667,
      "grad_norm": 0.06513673067092896,
      "learning_rate": 2.6523333333333334e-05,
      "loss": 0.0017,
      "step": 70430
    },
    {
      "epoch": 3.7568,
      "grad_norm": 0.2698637843132019,
      "learning_rate": 2.652e-05,
      "loss": 0.0022,
      "step": 70440
    },
    {
      "epoch": 3.7573333333333334,
      "grad_norm": 0.026827959343791008,
      "learning_rate": 2.6516666666666666e-05,
      "loss": 0.002,
      "step": 70450
    },
    {
      "epoch": 3.7578666666666667,
      "grad_norm": 0.06609039008617401,
      "learning_rate": 2.6513333333333336e-05,
      "loss": 0.0021,
      "step": 70460
    },
    {
      "epoch": 3.7584,
      "grad_norm": 0.4994015693664551,
      "learning_rate": 2.6510000000000002e-05,
      "loss": 0.0017,
      "step": 70470
    },
    {
      "epoch": 3.7589333333333332,
      "grad_norm": 0.7132265567779541,
      "learning_rate": 2.6506666666666668e-05,
      "loss": 0.0027,
      "step": 70480
    },
    {
      "epoch": 3.7594666666666665,
      "grad_norm": 0.030165286734700203,
      "learning_rate": 2.6503333333333334e-05,
      "loss": 0.0017,
      "step": 70490
    },
    {
      "epoch": 3.76,
      "grad_norm": 0.062107790261507034,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 0.0023,
      "step": 70500
    },
    {
      "epoch": 3.760533333333333,
      "grad_norm": 0.06491778790950775,
      "learning_rate": 2.649666666666667e-05,
      "loss": 0.0015,
      "step": 70510
    },
    {
      "epoch": 3.761066666666667,
      "grad_norm": 0.17737635970115662,
      "learning_rate": 2.6493333333333336e-05,
      "loss": 0.0023,
      "step": 70520
    },
    {
      "epoch": 3.7616,
      "grad_norm": 0.06138569861650467,
      "learning_rate": 2.6490000000000002e-05,
      "loss": 0.002,
      "step": 70530
    },
    {
      "epoch": 3.7621333333333333,
      "grad_norm": 0.23912248015403748,
      "learning_rate": 2.6486666666666665e-05,
      "loss": 0.0019,
      "step": 70540
    },
    {
      "epoch": 3.7626666666666666,
      "grad_norm": 0.18177378177642822,
      "learning_rate": 2.648333333333333e-05,
      "loss": 0.0013,
      "step": 70550
    },
    {
      "epoch": 3.7632,
      "grad_norm": 0.10154997557401657,
      "learning_rate": 2.648e-05,
      "loss": 0.0022,
      "step": 70560
    },
    {
      "epoch": 3.7637333333333336,
      "grad_norm": 0.4955174922943115,
      "learning_rate": 2.6476666666666667e-05,
      "loss": 0.0026,
      "step": 70570
    },
    {
      "epoch": 3.764266666666667,
      "grad_norm": 0.4052612781524658,
      "learning_rate": 2.6473333333333333e-05,
      "loss": 0.0019,
      "step": 70580
    },
    {
      "epoch": 3.7648,
      "grad_norm": 0.15671223402023315,
      "learning_rate": 2.647e-05,
      "loss": 0.0019,
      "step": 70590
    },
    {
      "epoch": 3.7653333333333334,
      "grad_norm": 0.17704224586486816,
      "learning_rate": 2.646666666666667e-05,
      "loss": 0.0015,
      "step": 70600
    },
    {
      "epoch": 3.7658666666666667,
      "grad_norm": 0.5442714095115662,
      "learning_rate": 2.6463333333333335e-05,
      "loss": 0.002,
      "step": 70610
    },
    {
      "epoch": 3.7664,
      "grad_norm": 0.1615690439939499,
      "learning_rate": 2.646e-05,
      "loss": 0.0018,
      "step": 70620
    },
    {
      "epoch": 3.7669333333333332,
      "grad_norm": 0.4097250998020172,
      "learning_rate": 2.6456666666666667e-05,
      "loss": 0.0023,
      "step": 70630
    },
    {
      "epoch": 3.7674666666666665,
      "grad_norm": 0.3858989477157593,
      "learning_rate": 2.6453333333333336e-05,
      "loss": 0.0024,
      "step": 70640
    },
    {
      "epoch": 3.768,
      "grad_norm": 0.4421870708465576,
      "learning_rate": 2.6450000000000003e-05,
      "loss": 0.0027,
      "step": 70650
    },
    {
      "epoch": 3.768533333333333,
      "grad_norm": 0.5897351503372192,
      "learning_rate": 2.644666666666667e-05,
      "loss": 0.0017,
      "step": 70660
    },
    {
      "epoch": 3.769066666666667,
      "grad_norm": 0.47629567980766296,
      "learning_rate": 2.6443333333333338e-05,
      "loss": 0.0024,
      "step": 70670
    },
    {
      "epoch": 3.7696,
      "grad_norm": 0.06320364028215408,
      "learning_rate": 2.6440000000000004e-05,
      "loss": 0.0027,
      "step": 70680
    },
    {
      "epoch": 3.7701333333333333,
      "grad_norm": 0.32844167947769165,
      "learning_rate": 2.643666666666667e-05,
      "loss": 0.0014,
      "step": 70690
    },
    {
      "epoch": 3.7706666666666666,
      "grad_norm": 0.1173613891005516,
      "learning_rate": 2.6433333333333333e-05,
      "loss": 0.0035,
      "step": 70700
    },
    {
      "epoch": 3.7712,
      "grad_norm": 0.3763619661331177,
      "learning_rate": 2.643e-05,
      "loss": 0.0026,
      "step": 70710
    },
    {
      "epoch": 3.7717333333333336,
      "grad_norm": 0.3543115258216858,
      "learning_rate": 2.6426666666666665e-05,
      "loss": 0.0022,
      "step": 70720
    },
    {
      "epoch": 3.772266666666667,
      "grad_norm": 0.17601357400417328,
      "learning_rate": 2.642333333333333e-05,
      "loss": 0.0027,
      "step": 70730
    },
    {
      "epoch": 3.7728,
      "grad_norm": 0.1451548933982849,
      "learning_rate": 2.642e-05,
      "loss": 0.0021,
      "step": 70740
    },
    {
      "epoch": 3.7733333333333334,
      "grad_norm": 0.20412211120128632,
      "learning_rate": 2.6416666666666667e-05,
      "loss": 0.0017,
      "step": 70750
    },
    {
      "epoch": 3.7738666666666667,
      "grad_norm": 0.5749146938323975,
      "learning_rate": 2.6413333333333333e-05,
      "loss": 0.0026,
      "step": 70760
    },
    {
      "epoch": 3.7744,
      "grad_norm": 0.04269328713417053,
      "learning_rate": 2.6410000000000003e-05,
      "loss": 0.0023,
      "step": 70770
    },
    {
      "epoch": 3.7749333333333333,
      "grad_norm": 0.3192133903503418,
      "learning_rate": 2.640666666666667e-05,
      "loss": 0.002,
      "step": 70780
    },
    {
      "epoch": 3.7754666666666665,
      "grad_norm": 0.265606552362442,
      "learning_rate": 2.6403333333333335e-05,
      "loss": 0.0025,
      "step": 70790
    },
    {
      "epoch": 3.776,
      "grad_norm": 0.27984529733657837,
      "learning_rate": 2.64e-05,
      "loss": 0.0032,
      "step": 70800
    },
    {
      "epoch": 3.776533333333333,
      "grad_norm": 0.7045873999595642,
      "learning_rate": 2.639666666666667e-05,
      "loss": 0.0018,
      "step": 70810
    },
    {
      "epoch": 3.777066666666667,
      "grad_norm": 0.17305788397789001,
      "learning_rate": 2.6393333333333337e-05,
      "loss": 0.0021,
      "step": 70820
    },
    {
      "epoch": 3.7776,
      "grad_norm": 0.5311952829360962,
      "learning_rate": 2.6390000000000003e-05,
      "loss": 0.002,
      "step": 70830
    },
    {
      "epoch": 3.7781333333333333,
      "grad_norm": 0.049336981028318405,
      "learning_rate": 2.638666666666667e-05,
      "loss": 0.0021,
      "step": 70840
    },
    {
      "epoch": 3.7786666666666666,
      "grad_norm": 0.3402288556098938,
      "learning_rate": 2.6383333333333332e-05,
      "loss": 0.0018,
      "step": 70850
    },
    {
      "epoch": 3.7792,
      "grad_norm": 0.4595920741558075,
      "learning_rate": 2.6379999999999998e-05,
      "loss": 0.003,
      "step": 70860
    },
    {
      "epoch": 3.779733333333333,
      "grad_norm": 0.47000133991241455,
      "learning_rate": 2.6376666666666668e-05,
      "loss": 0.0029,
      "step": 70870
    },
    {
      "epoch": 3.780266666666667,
      "grad_norm": 0.2107251137495041,
      "learning_rate": 2.6373333333333334e-05,
      "loss": 0.0017,
      "step": 70880
    },
    {
      "epoch": 3.7808,
      "grad_norm": 0.30307120084762573,
      "learning_rate": 2.637e-05,
      "loss": 0.0022,
      "step": 70890
    },
    {
      "epoch": 3.7813333333333334,
      "grad_norm": 0.07200097292661667,
      "learning_rate": 2.6366666666666666e-05,
      "loss": 0.0029,
      "step": 70900
    },
    {
      "epoch": 3.7818666666666667,
      "grad_norm": 0.29666537046432495,
      "learning_rate": 2.6363333333333336e-05,
      "loss": 0.0024,
      "step": 70910
    },
    {
      "epoch": 3.7824,
      "grad_norm": 0.18464446067810059,
      "learning_rate": 2.6360000000000002e-05,
      "loss": 0.0023,
      "step": 70920
    },
    {
      "epoch": 3.7829333333333333,
      "grad_norm": 0.1954103410243988,
      "learning_rate": 2.6356666666666668e-05,
      "loss": 0.0021,
      "step": 70930
    },
    {
      "epoch": 3.7834666666666665,
      "grad_norm": 0.27426302433013916,
      "learning_rate": 2.6353333333333334e-05,
      "loss": 0.0017,
      "step": 70940
    },
    {
      "epoch": 3.784,
      "grad_norm": 0.1309238076210022,
      "learning_rate": 2.6350000000000004e-05,
      "loss": 0.0019,
      "step": 70950
    },
    {
      "epoch": 3.784533333333333,
      "grad_norm": 0.1044744923710823,
      "learning_rate": 2.634666666666667e-05,
      "loss": 0.002,
      "step": 70960
    },
    {
      "epoch": 3.785066666666667,
      "grad_norm": 0.4398063123226166,
      "learning_rate": 2.6343333333333336e-05,
      "loss": 0.0024,
      "step": 70970
    },
    {
      "epoch": 3.7856,
      "grad_norm": 0.11838003247976303,
      "learning_rate": 2.6340000000000002e-05,
      "loss": 0.0015,
      "step": 70980
    },
    {
      "epoch": 3.7861333333333334,
      "grad_norm": 0.5018616914749146,
      "learning_rate": 2.633666666666667e-05,
      "loss": 0.0012,
      "step": 70990
    },
    {
      "epoch": 3.7866666666666666,
      "grad_norm": 0.04377969726920128,
      "learning_rate": 2.633333333333333e-05,
      "loss": 0.002,
      "step": 71000
    },
    {
      "epoch": 3.7872,
      "grad_norm": 0.25174954533576965,
      "learning_rate": 2.633e-05,
      "loss": 0.0028,
      "step": 71010
    },
    {
      "epoch": 3.787733333333333,
      "grad_norm": 0.37441155314445496,
      "learning_rate": 2.6326666666666666e-05,
      "loss": 0.0021,
      "step": 71020
    },
    {
      "epoch": 3.788266666666667,
      "grad_norm": 0.08272901922464371,
      "learning_rate": 2.6323333333333333e-05,
      "loss": 0.002,
      "step": 71030
    },
    {
      "epoch": 3.7888,
      "grad_norm": 0.25063782930374146,
      "learning_rate": 2.632e-05,
      "loss": 0.0015,
      "step": 71040
    },
    {
      "epoch": 3.7893333333333334,
      "grad_norm": 0.24009884893894196,
      "learning_rate": 2.6316666666666668e-05,
      "loss": 0.0031,
      "step": 71050
    },
    {
      "epoch": 3.7898666666666667,
      "grad_norm": 0.258139431476593,
      "learning_rate": 2.6313333333333334e-05,
      "loss": 0.0023,
      "step": 71060
    },
    {
      "epoch": 3.7904,
      "grad_norm": 0.12314992398023605,
      "learning_rate": 2.631e-05,
      "loss": 0.0027,
      "step": 71070
    },
    {
      "epoch": 3.7909333333333333,
      "grad_norm": 0.5878754258155823,
      "learning_rate": 2.630666666666667e-05,
      "loss": 0.0026,
      "step": 71080
    },
    {
      "epoch": 3.7914666666666665,
      "grad_norm": 0.06186842918395996,
      "learning_rate": 2.6303333333333336e-05,
      "loss": 0.0017,
      "step": 71090
    },
    {
      "epoch": 3.792,
      "grad_norm": 0.18074752390384674,
      "learning_rate": 2.6300000000000002e-05,
      "loss": 0.0015,
      "step": 71100
    },
    {
      "epoch": 3.792533333333333,
      "grad_norm": 0.131952702999115,
      "learning_rate": 2.629666666666667e-05,
      "loss": 0.002,
      "step": 71110
    },
    {
      "epoch": 3.793066666666667,
      "grad_norm": 0.09153109788894653,
      "learning_rate": 2.6293333333333338e-05,
      "loss": 0.0018,
      "step": 71120
    },
    {
      "epoch": 3.7936,
      "grad_norm": 0.06350700557231903,
      "learning_rate": 2.6290000000000004e-05,
      "loss": 0.0036,
      "step": 71130
    },
    {
      "epoch": 3.7941333333333334,
      "grad_norm": 0.14797340333461761,
      "learning_rate": 2.628666666666667e-05,
      "loss": 0.0023,
      "step": 71140
    },
    {
      "epoch": 3.7946666666666666,
      "grad_norm": 0.23450900614261627,
      "learning_rate": 2.6283333333333333e-05,
      "loss": 0.0015,
      "step": 71150
    },
    {
      "epoch": 3.7952,
      "grad_norm": 0.38191697001457214,
      "learning_rate": 2.628e-05,
      "loss": 0.0024,
      "step": 71160
    },
    {
      "epoch": 3.795733333333333,
      "grad_norm": 0.299761027097702,
      "learning_rate": 2.6276666666666665e-05,
      "loss": 0.0026,
      "step": 71170
    },
    {
      "epoch": 3.796266666666667,
      "grad_norm": 0.10067625343799591,
      "learning_rate": 2.6273333333333335e-05,
      "loss": 0.0013,
      "step": 71180
    },
    {
      "epoch": 3.7968,
      "grad_norm": 0.3844492435455322,
      "learning_rate": 2.627e-05,
      "loss": 0.0029,
      "step": 71190
    },
    {
      "epoch": 3.7973333333333334,
      "grad_norm": 0.407162070274353,
      "learning_rate": 2.6266666666666667e-05,
      "loss": 0.0027,
      "step": 71200
    },
    {
      "epoch": 3.7978666666666667,
      "grad_norm": 0.26242753863334656,
      "learning_rate": 2.6263333333333333e-05,
      "loss": 0.0028,
      "step": 71210
    },
    {
      "epoch": 3.7984,
      "grad_norm": 0.4988689124584198,
      "learning_rate": 2.6260000000000003e-05,
      "loss": 0.0026,
      "step": 71220
    },
    {
      "epoch": 3.7989333333333333,
      "grad_norm": 0.6804323196411133,
      "learning_rate": 2.625666666666667e-05,
      "loss": 0.0035,
      "step": 71230
    },
    {
      "epoch": 3.7994666666666665,
      "grad_norm": 0.7035918831825256,
      "learning_rate": 2.6253333333333335e-05,
      "loss": 0.0023,
      "step": 71240
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.4040292203426361,
      "learning_rate": 2.625e-05,
      "loss": 0.0015,
      "step": 71250
    },
    {
      "epoch": 3.800533333333333,
      "grad_norm": 0.17763899266719818,
      "learning_rate": 2.624666666666667e-05,
      "loss": 0.0015,
      "step": 71260
    },
    {
      "epoch": 3.801066666666667,
      "grad_norm": 0.2984088957309723,
      "learning_rate": 2.6243333333333337e-05,
      "loss": 0.002,
      "step": 71270
    },
    {
      "epoch": 3.8016,
      "grad_norm": 0.29642197489738464,
      "learning_rate": 2.6240000000000003e-05,
      "loss": 0.0015,
      "step": 71280
    },
    {
      "epoch": 3.8021333333333334,
      "grad_norm": 0.15988034009933472,
      "learning_rate": 2.623666666666667e-05,
      "loss": 0.0022,
      "step": 71290
    },
    {
      "epoch": 3.8026666666666666,
      "grad_norm": 0.4882388710975647,
      "learning_rate": 2.6233333333333332e-05,
      "loss": 0.0023,
      "step": 71300
    },
    {
      "epoch": 3.8032,
      "grad_norm": 0.07068900763988495,
      "learning_rate": 2.6229999999999998e-05,
      "loss": 0.0022,
      "step": 71310
    },
    {
      "epoch": 3.803733333333333,
      "grad_norm": 0.21880172193050385,
      "learning_rate": 2.6226666666666667e-05,
      "loss": 0.0015,
      "step": 71320
    },
    {
      "epoch": 3.804266666666667,
      "grad_norm": 0.26450321078300476,
      "learning_rate": 2.6223333333333334e-05,
      "loss": 0.0019,
      "step": 71330
    },
    {
      "epoch": 3.8048,
      "grad_norm": 0.2166605442762375,
      "learning_rate": 2.622e-05,
      "loss": 0.0024,
      "step": 71340
    },
    {
      "epoch": 3.8053333333333335,
      "grad_norm": 0.11929817497730255,
      "learning_rate": 2.6216666666666666e-05,
      "loss": 0.0028,
      "step": 71350
    },
    {
      "epoch": 3.8058666666666667,
      "grad_norm": 0.6327905058860779,
      "learning_rate": 2.6213333333333335e-05,
      "loss": 0.002,
      "step": 71360
    },
    {
      "epoch": 3.8064,
      "grad_norm": 0.31895264983177185,
      "learning_rate": 2.621e-05,
      "loss": 0.0023,
      "step": 71370
    },
    {
      "epoch": 3.8069333333333333,
      "grad_norm": 0.1260589212179184,
      "learning_rate": 2.6206666666666668e-05,
      "loss": 0.003,
      "step": 71380
    },
    {
      "epoch": 3.8074666666666666,
      "grad_norm": 0.2948777377605438,
      "learning_rate": 2.6203333333333334e-05,
      "loss": 0.0017,
      "step": 71390
    },
    {
      "epoch": 3.808,
      "grad_norm": 0.10564298182725906,
      "learning_rate": 2.6200000000000003e-05,
      "loss": 0.0022,
      "step": 71400
    },
    {
      "epoch": 3.808533333333333,
      "grad_norm": 0.24972601234912872,
      "learning_rate": 2.619666666666667e-05,
      "loss": 0.002,
      "step": 71410
    },
    {
      "epoch": 3.809066666666667,
      "grad_norm": 0.15994226932525635,
      "learning_rate": 2.6193333333333336e-05,
      "loss": 0.0016,
      "step": 71420
    },
    {
      "epoch": 3.8096,
      "grad_norm": 0.12109965831041336,
      "learning_rate": 2.6190000000000005e-05,
      "loss": 0.0017,
      "step": 71430
    },
    {
      "epoch": 3.8101333333333334,
      "grad_norm": 0.05910320207476616,
      "learning_rate": 2.618666666666667e-05,
      "loss": 0.0016,
      "step": 71440
    },
    {
      "epoch": 3.8106666666666666,
      "grad_norm": 0.09457465261220932,
      "learning_rate": 2.618333333333333e-05,
      "loss": 0.0021,
      "step": 71450
    },
    {
      "epoch": 3.8112,
      "grad_norm": 0.03724496811628342,
      "learning_rate": 2.618e-05,
      "loss": 0.0026,
      "step": 71460
    },
    {
      "epoch": 3.811733333333333,
      "grad_norm": 0.3354170024394989,
      "learning_rate": 2.6176666666666666e-05,
      "loss": 0.002,
      "step": 71470
    },
    {
      "epoch": 3.812266666666667,
      "grad_norm": 0.1455051451921463,
      "learning_rate": 2.6173333333333332e-05,
      "loss": 0.0011,
      "step": 71480
    },
    {
      "epoch": 3.8128,
      "grad_norm": 0.1914549171924591,
      "learning_rate": 2.617e-05,
      "loss": 0.0012,
      "step": 71490
    },
    {
      "epoch": 3.8133333333333335,
      "grad_norm": 0.1595258265733719,
      "learning_rate": 2.6166666666666668e-05,
      "loss": 0.0025,
      "step": 71500
    },
    {
      "epoch": 3.8138666666666667,
      "grad_norm": 0.38286659121513367,
      "learning_rate": 2.6163333333333334e-05,
      "loss": 0.0019,
      "step": 71510
    },
    {
      "epoch": 3.8144,
      "grad_norm": 0.2076047956943512,
      "learning_rate": 2.616e-05,
      "loss": 0.0024,
      "step": 71520
    },
    {
      "epoch": 3.8149333333333333,
      "grad_norm": 0.1522512286901474,
      "learning_rate": 2.615666666666667e-05,
      "loss": 0.0023,
      "step": 71530
    },
    {
      "epoch": 3.8154666666666666,
      "grad_norm": 0.08873553574085236,
      "learning_rate": 2.6153333333333336e-05,
      "loss": 0.0017,
      "step": 71540
    },
    {
      "epoch": 3.816,
      "grad_norm": 0.20650207996368408,
      "learning_rate": 2.6150000000000002e-05,
      "loss": 0.002,
      "step": 71550
    },
    {
      "epoch": 3.816533333333333,
      "grad_norm": 0.22969910502433777,
      "learning_rate": 2.6146666666666668e-05,
      "loss": 0.0017,
      "step": 71560
    },
    {
      "epoch": 3.817066666666667,
      "grad_norm": 0.29732227325439453,
      "learning_rate": 2.6143333333333338e-05,
      "loss": 0.0019,
      "step": 71570
    },
    {
      "epoch": 3.8176,
      "grad_norm": 0.040389593690633774,
      "learning_rate": 2.6140000000000004e-05,
      "loss": 0.0022,
      "step": 71580
    },
    {
      "epoch": 3.8181333333333334,
      "grad_norm": 0.4286465048789978,
      "learning_rate": 2.613666666666667e-05,
      "loss": 0.0017,
      "step": 71590
    },
    {
      "epoch": 3.8186666666666667,
      "grad_norm": 0.04153504595160484,
      "learning_rate": 2.6133333333333333e-05,
      "loss": 0.0025,
      "step": 71600
    },
    {
      "epoch": 3.8192,
      "grad_norm": 0.4795280992984772,
      "learning_rate": 2.613e-05,
      "loss": 0.0031,
      "step": 71610
    },
    {
      "epoch": 3.819733333333333,
      "grad_norm": 0.22395530343055725,
      "learning_rate": 2.6126666666666665e-05,
      "loss": 0.0029,
      "step": 71620
    },
    {
      "epoch": 3.820266666666667,
      "grad_norm": 0.12187471240758896,
      "learning_rate": 2.6123333333333335e-05,
      "loss": 0.0018,
      "step": 71630
    },
    {
      "epoch": 3.8208,
      "grad_norm": 0.4955393970012665,
      "learning_rate": 2.612e-05,
      "loss": 0.0018,
      "step": 71640
    },
    {
      "epoch": 3.8213333333333335,
      "grad_norm": 0.29854586720466614,
      "learning_rate": 2.6116666666666667e-05,
      "loss": 0.0027,
      "step": 71650
    },
    {
      "epoch": 3.8218666666666667,
      "grad_norm": 0.06775889545679092,
      "learning_rate": 2.6113333333333333e-05,
      "loss": 0.0029,
      "step": 71660
    },
    {
      "epoch": 3.8224,
      "grad_norm": 0.470074862241745,
      "learning_rate": 2.6110000000000002e-05,
      "loss": 0.0017,
      "step": 71670
    },
    {
      "epoch": 3.8229333333333333,
      "grad_norm": 0.1820284128189087,
      "learning_rate": 2.610666666666667e-05,
      "loss": 0.002,
      "step": 71680
    },
    {
      "epoch": 3.8234666666666666,
      "grad_norm": 0.14486749470233917,
      "learning_rate": 2.6103333333333335e-05,
      "loss": 0.0014,
      "step": 71690
    },
    {
      "epoch": 3.824,
      "grad_norm": 0.22146038711071014,
      "learning_rate": 2.61e-05,
      "loss": 0.0028,
      "step": 71700
    },
    {
      "epoch": 3.824533333333333,
      "grad_norm": 0.2701645493507385,
      "learning_rate": 2.609666666666667e-05,
      "loss": 0.0018,
      "step": 71710
    },
    {
      "epoch": 3.8250666666666664,
      "grad_norm": 0.23471102118492126,
      "learning_rate": 2.6093333333333336e-05,
      "loss": 0.0025,
      "step": 71720
    },
    {
      "epoch": 3.8256,
      "grad_norm": 0.15670254826545715,
      "learning_rate": 2.6090000000000003e-05,
      "loss": 0.0025,
      "step": 71730
    },
    {
      "epoch": 3.8261333333333334,
      "grad_norm": 0.38201582431793213,
      "learning_rate": 2.608666666666667e-05,
      "loss": 0.0025,
      "step": 71740
    },
    {
      "epoch": 3.8266666666666667,
      "grad_norm": 0.6638766527175903,
      "learning_rate": 2.608333333333333e-05,
      "loss": 0.0026,
      "step": 71750
    },
    {
      "epoch": 3.8272,
      "grad_norm": 0.27360662817955017,
      "learning_rate": 2.6079999999999998e-05,
      "loss": 0.0014,
      "step": 71760
    },
    {
      "epoch": 3.827733333333333,
      "grad_norm": 0.24034801125526428,
      "learning_rate": 2.6076666666666667e-05,
      "loss": 0.0024,
      "step": 71770
    },
    {
      "epoch": 3.828266666666667,
      "grad_norm": 0.189537912607193,
      "learning_rate": 2.6073333333333333e-05,
      "loss": 0.0023,
      "step": 71780
    },
    {
      "epoch": 3.8288,
      "grad_norm": 0.33379432559013367,
      "learning_rate": 2.607e-05,
      "loss": 0.0015,
      "step": 71790
    },
    {
      "epoch": 3.8293333333333335,
      "grad_norm": 0.30765512585639954,
      "learning_rate": 2.6066666666666666e-05,
      "loss": 0.0016,
      "step": 71800
    },
    {
      "epoch": 3.8298666666666668,
      "grad_norm": 0.30751413106918335,
      "learning_rate": 2.6063333333333335e-05,
      "loss": 0.003,
      "step": 71810
    },
    {
      "epoch": 3.8304,
      "grad_norm": 0.250288188457489,
      "learning_rate": 2.606e-05,
      "loss": 0.002,
      "step": 71820
    },
    {
      "epoch": 3.8309333333333333,
      "grad_norm": 0.2981508672237396,
      "learning_rate": 2.6056666666666667e-05,
      "loss": 0.0022,
      "step": 71830
    },
    {
      "epoch": 3.8314666666666666,
      "grad_norm": 0.34229347109794617,
      "learning_rate": 2.6053333333333333e-05,
      "loss": 0.0025,
      "step": 71840
    },
    {
      "epoch": 3.832,
      "grad_norm": 0.12139426171779633,
      "learning_rate": 2.6050000000000003e-05,
      "loss": 0.0018,
      "step": 71850
    },
    {
      "epoch": 3.832533333333333,
      "grad_norm": 0.2540932297706604,
      "learning_rate": 2.604666666666667e-05,
      "loss": 0.0022,
      "step": 71860
    },
    {
      "epoch": 3.8330666666666664,
      "grad_norm": 0.17680582404136658,
      "learning_rate": 2.6043333333333335e-05,
      "loss": 0.0023,
      "step": 71870
    },
    {
      "epoch": 3.8336,
      "grad_norm": 0.10486243665218353,
      "learning_rate": 2.6040000000000005e-05,
      "loss": 0.002,
      "step": 71880
    },
    {
      "epoch": 3.8341333333333334,
      "grad_norm": 0.06033025681972504,
      "learning_rate": 2.603666666666667e-05,
      "loss": 0.003,
      "step": 71890
    },
    {
      "epoch": 3.8346666666666667,
      "grad_norm": 0.03814372420310974,
      "learning_rate": 2.6033333333333337e-05,
      "loss": 0.0022,
      "step": 71900
    },
    {
      "epoch": 3.8352,
      "grad_norm": 0.2942322790622711,
      "learning_rate": 2.603e-05,
      "loss": 0.0024,
      "step": 71910
    },
    {
      "epoch": 3.835733333333333,
      "grad_norm": 0.5614199638366699,
      "learning_rate": 2.6026666666666666e-05,
      "loss": 0.0025,
      "step": 71920
    },
    {
      "epoch": 3.836266666666667,
      "grad_norm": 0.4899389445781708,
      "learning_rate": 2.6023333333333332e-05,
      "loss": 0.0024,
      "step": 71930
    },
    {
      "epoch": 3.8368,
      "grad_norm": 0.04870486259460449,
      "learning_rate": 2.602e-05,
      "loss": 0.0034,
      "step": 71940
    },
    {
      "epoch": 3.8373333333333335,
      "grad_norm": 0.02741626836359501,
      "learning_rate": 2.6016666666666668e-05,
      "loss": 0.0013,
      "step": 71950
    },
    {
      "epoch": 3.8378666666666668,
      "grad_norm": 0.49814802408218384,
      "learning_rate": 2.6013333333333334e-05,
      "loss": 0.0016,
      "step": 71960
    },
    {
      "epoch": 3.8384,
      "grad_norm": 0.1778692603111267,
      "learning_rate": 2.601e-05,
      "loss": 0.0027,
      "step": 71970
    },
    {
      "epoch": 3.8389333333333333,
      "grad_norm": 0.06400961428880692,
      "learning_rate": 2.600666666666667e-05,
      "loss": 0.002,
      "step": 71980
    },
    {
      "epoch": 3.8394666666666666,
      "grad_norm": 0.5401701331138611,
      "learning_rate": 2.6003333333333336e-05,
      "loss": 0.0022,
      "step": 71990
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.40872707962989807,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.0015,
      "step": 72000
    },
    {
      "epoch": 3.840533333333333,
      "grad_norm": 0.35328739881515503,
      "learning_rate": 2.5996666666666668e-05,
      "loss": 0.0027,
      "step": 72010
    },
    {
      "epoch": 3.8410666666666664,
      "grad_norm": 0.1970747709274292,
      "learning_rate": 2.5993333333333337e-05,
      "loss": 0.0023,
      "step": 72020
    },
    {
      "epoch": 3.8416,
      "grad_norm": 0.2646554708480835,
      "learning_rate": 2.5990000000000004e-05,
      "loss": 0.0028,
      "step": 72030
    },
    {
      "epoch": 3.8421333333333334,
      "grad_norm": 0.35228028893470764,
      "learning_rate": 2.598666666666667e-05,
      "loss": 0.0025,
      "step": 72040
    },
    {
      "epoch": 3.8426666666666667,
      "grad_norm": 0.17401129007339478,
      "learning_rate": 2.5983333333333336e-05,
      "loss": 0.0025,
      "step": 72050
    },
    {
      "epoch": 3.8432,
      "grad_norm": 0.11798645555973053,
      "learning_rate": 2.598e-05,
      "loss": 0.0021,
      "step": 72060
    },
    {
      "epoch": 3.8437333333333332,
      "grad_norm": 0.29652613401412964,
      "learning_rate": 2.5976666666666665e-05,
      "loss": 0.0025,
      "step": 72070
    },
    {
      "epoch": 3.844266666666667,
      "grad_norm": 0.41133883595466614,
      "learning_rate": 2.5973333333333334e-05,
      "loss": 0.0021,
      "step": 72080
    },
    {
      "epoch": 3.8448,
      "grad_norm": 0.26797157526016235,
      "learning_rate": 2.597e-05,
      "loss": 0.0016,
      "step": 72090
    },
    {
      "epoch": 3.8453333333333335,
      "grad_norm": 0.621396541595459,
      "learning_rate": 2.5966666666666667e-05,
      "loss": 0.0017,
      "step": 72100
    },
    {
      "epoch": 3.8458666666666668,
      "grad_norm": 0.2104244977235794,
      "learning_rate": 2.5963333333333333e-05,
      "loss": 0.0019,
      "step": 72110
    },
    {
      "epoch": 3.8464,
      "grad_norm": 0.5043248534202576,
      "learning_rate": 2.5960000000000002e-05,
      "loss": 0.0023,
      "step": 72120
    },
    {
      "epoch": 3.8469333333333333,
      "grad_norm": 0.09423407912254333,
      "learning_rate": 2.5956666666666668e-05,
      "loss": 0.0018,
      "step": 72130
    },
    {
      "epoch": 3.8474666666666666,
      "grad_norm": 0.24697546660900116,
      "learning_rate": 2.5953333333333334e-05,
      "loss": 0.0018,
      "step": 72140
    },
    {
      "epoch": 3.848,
      "grad_norm": 0.19971027970314026,
      "learning_rate": 2.595e-05,
      "loss": 0.0022,
      "step": 72150
    },
    {
      "epoch": 3.848533333333333,
      "grad_norm": 0.18709594011306763,
      "learning_rate": 2.594666666666667e-05,
      "loss": 0.0018,
      "step": 72160
    },
    {
      "epoch": 3.8490666666666664,
      "grad_norm": 0.03742572292685509,
      "learning_rate": 2.5943333333333336e-05,
      "loss": 0.0016,
      "step": 72170
    },
    {
      "epoch": 3.8496,
      "grad_norm": 0.636103630065918,
      "learning_rate": 2.5940000000000002e-05,
      "loss": 0.0028,
      "step": 72180
    },
    {
      "epoch": 3.8501333333333334,
      "grad_norm": 0.6490121483802795,
      "learning_rate": 2.5936666666666672e-05,
      "loss": 0.0022,
      "step": 72190
    },
    {
      "epoch": 3.8506666666666667,
      "grad_norm": 0.41261187195777893,
      "learning_rate": 2.5933333333333338e-05,
      "loss": 0.0021,
      "step": 72200
    },
    {
      "epoch": 3.8512,
      "grad_norm": 0.3282764256000519,
      "learning_rate": 2.5929999999999997e-05,
      "loss": 0.002,
      "step": 72210
    },
    {
      "epoch": 3.8517333333333332,
      "grad_norm": 0.35516712069511414,
      "learning_rate": 2.5926666666666667e-05,
      "loss": 0.0021,
      "step": 72220
    },
    {
      "epoch": 3.8522666666666665,
      "grad_norm": 0.35521072149276733,
      "learning_rate": 2.5923333333333333e-05,
      "loss": 0.0017,
      "step": 72230
    },
    {
      "epoch": 3.8528000000000002,
      "grad_norm": 0.30842745304107666,
      "learning_rate": 2.592e-05,
      "loss": 0.0032,
      "step": 72240
    },
    {
      "epoch": 3.8533333333333335,
      "grad_norm": 0.060005027800798416,
      "learning_rate": 2.5916666666666665e-05,
      "loss": 0.0018,
      "step": 72250
    },
    {
      "epoch": 3.8538666666666668,
      "grad_norm": 0.034200821071863174,
      "learning_rate": 2.5913333333333335e-05,
      "loss": 0.0013,
      "step": 72260
    },
    {
      "epoch": 3.8544,
      "grad_norm": 0.11452213674783707,
      "learning_rate": 2.591e-05,
      "loss": 0.002,
      "step": 72270
    },
    {
      "epoch": 3.8549333333333333,
      "grad_norm": 0.18304036557674408,
      "learning_rate": 2.5906666666666667e-05,
      "loss": 0.0019,
      "step": 72280
    },
    {
      "epoch": 3.8554666666666666,
      "grad_norm": 0.0348910354077816,
      "learning_rate": 2.5903333333333337e-05,
      "loss": 0.0021,
      "step": 72290
    },
    {
      "epoch": 3.856,
      "grad_norm": 0.17685425281524658,
      "learning_rate": 2.5900000000000003e-05,
      "loss": 0.0015,
      "step": 72300
    },
    {
      "epoch": 3.856533333333333,
      "grad_norm": 0.07026971131563187,
      "learning_rate": 2.589666666666667e-05,
      "loss": 0.0022,
      "step": 72310
    },
    {
      "epoch": 3.8570666666666664,
      "grad_norm": 0.17496716976165771,
      "learning_rate": 2.5893333333333335e-05,
      "loss": 0.0021,
      "step": 72320
    },
    {
      "epoch": 3.8576,
      "grad_norm": 0.3327767550945282,
      "learning_rate": 2.5890000000000005e-05,
      "loss": 0.0022,
      "step": 72330
    },
    {
      "epoch": 3.8581333333333334,
      "grad_norm": 0.20461498200893402,
      "learning_rate": 2.588666666666667e-05,
      "loss": 0.0021,
      "step": 72340
    },
    {
      "epoch": 3.8586666666666667,
      "grad_norm": 0.1558259129524231,
      "learning_rate": 2.5883333333333337e-05,
      "loss": 0.0022,
      "step": 72350
    },
    {
      "epoch": 3.8592,
      "grad_norm": 0.13081425428390503,
      "learning_rate": 2.588e-05,
      "loss": 0.0023,
      "step": 72360
    },
    {
      "epoch": 3.8597333333333332,
      "grad_norm": 0.1785670816898346,
      "learning_rate": 2.5876666666666666e-05,
      "loss": 0.002,
      "step": 72370
    },
    {
      "epoch": 3.8602666666666665,
      "grad_norm": 0.4136854410171509,
      "learning_rate": 2.5873333333333332e-05,
      "loss": 0.0019,
      "step": 72380
    },
    {
      "epoch": 3.8608000000000002,
      "grad_norm": 0.09885216504335403,
      "learning_rate": 2.587e-05,
      "loss": 0.0015,
      "step": 72390
    },
    {
      "epoch": 3.8613333333333335,
      "grad_norm": 0.050230544060468674,
      "learning_rate": 2.5866666666666667e-05,
      "loss": 0.0012,
      "step": 72400
    },
    {
      "epoch": 3.861866666666667,
      "grad_norm": 0.2958638668060303,
      "learning_rate": 2.5863333333333334e-05,
      "loss": 0.0014,
      "step": 72410
    },
    {
      "epoch": 3.8624,
      "grad_norm": 0.2098625898361206,
      "learning_rate": 2.586e-05,
      "loss": 0.0015,
      "step": 72420
    },
    {
      "epoch": 3.8629333333333333,
      "grad_norm": 0.27256813645362854,
      "learning_rate": 2.585666666666667e-05,
      "loss": 0.0018,
      "step": 72430
    },
    {
      "epoch": 3.8634666666666666,
      "grad_norm": 0.2490180879831314,
      "learning_rate": 2.5853333333333335e-05,
      "loss": 0.0022,
      "step": 72440
    },
    {
      "epoch": 3.864,
      "grad_norm": 0.0344870425760746,
      "learning_rate": 2.585e-05,
      "loss": 0.0025,
      "step": 72450
    },
    {
      "epoch": 3.864533333333333,
      "grad_norm": 0.2515961229801178,
      "learning_rate": 2.5846666666666668e-05,
      "loss": 0.0019,
      "step": 72460
    },
    {
      "epoch": 3.8650666666666664,
      "grad_norm": 0.0946890339255333,
      "learning_rate": 2.5843333333333337e-05,
      "loss": 0.0014,
      "step": 72470
    },
    {
      "epoch": 3.8656,
      "grad_norm": 0.0871918797492981,
      "learning_rate": 2.5840000000000003e-05,
      "loss": 0.0025,
      "step": 72480
    },
    {
      "epoch": 3.8661333333333334,
      "grad_norm": 0.03830825537443161,
      "learning_rate": 2.583666666666667e-05,
      "loss": 0.003,
      "step": 72490
    },
    {
      "epoch": 3.8666666666666667,
      "grad_norm": 0.11720452457666397,
      "learning_rate": 2.5833333333333336e-05,
      "loss": 0.0021,
      "step": 72500
    },
    {
      "epoch": 3.8672,
      "grad_norm": 0.04431416466832161,
      "learning_rate": 2.583e-05,
      "loss": 0.0033,
      "step": 72510
    },
    {
      "epoch": 3.8677333333333332,
      "grad_norm": 0.058992866426706314,
      "learning_rate": 2.5826666666666664e-05,
      "loss": 0.0022,
      "step": 72520
    },
    {
      "epoch": 3.8682666666666665,
      "grad_norm": 0.2151995748281479,
      "learning_rate": 2.5823333333333334e-05,
      "loss": 0.002,
      "step": 72530
    },
    {
      "epoch": 3.8688000000000002,
      "grad_norm": 0.18522511422634125,
      "learning_rate": 2.582e-05,
      "loss": 0.0022,
      "step": 72540
    },
    {
      "epoch": 3.8693333333333335,
      "grad_norm": 0.12462330609560013,
      "learning_rate": 2.5816666666666666e-05,
      "loss": 0.0019,
      "step": 72550
    },
    {
      "epoch": 3.869866666666667,
      "grad_norm": 0.05383887141942978,
      "learning_rate": 2.5813333333333332e-05,
      "loss": 0.0016,
      "step": 72560
    },
    {
      "epoch": 3.8704,
      "grad_norm": 0.09182688593864441,
      "learning_rate": 2.5810000000000002e-05,
      "loss": 0.0024,
      "step": 72570
    },
    {
      "epoch": 3.8709333333333333,
      "grad_norm": 0.21134616434574127,
      "learning_rate": 2.5806666666666668e-05,
      "loss": 0.0032,
      "step": 72580
    },
    {
      "epoch": 3.8714666666666666,
      "grad_norm": 0.186128631234169,
      "learning_rate": 2.5803333333333334e-05,
      "loss": 0.0025,
      "step": 72590
    },
    {
      "epoch": 3.872,
      "grad_norm": 0.03448471054434776,
      "learning_rate": 2.58e-05,
      "loss": 0.0016,
      "step": 72600
    },
    {
      "epoch": 3.872533333333333,
      "grad_norm": 0.06577286124229431,
      "learning_rate": 2.579666666666667e-05,
      "loss": 0.0029,
      "step": 72610
    },
    {
      "epoch": 3.8730666666666664,
      "grad_norm": 0.20797204971313477,
      "learning_rate": 2.5793333333333336e-05,
      "loss": 0.0024,
      "step": 72620
    },
    {
      "epoch": 3.8736,
      "grad_norm": 0.12573467195034027,
      "learning_rate": 2.5790000000000002e-05,
      "loss": 0.0029,
      "step": 72630
    },
    {
      "epoch": 3.8741333333333334,
      "grad_norm": 0.5641093254089355,
      "learning_rate": 2.578666666666667e-05,
      "loss": 0.0026,
      "step": 72640
    },
    {
      "epoch": 3.8746666666666667,
      "grad_norm": 0.1284697949886322,
      "learning_rate": 2.5783333333333338e-05,
      "loss": 0.0026,
      "step": 72650
    },
    {
      "epoch": 3.8752,
      "grad_norm": 0.12012545764446259,
      "learning_rate": 2.5779999999999997e-05,
      "loss": 0.0021,
      "step": 72660
    },
    {
      "epoch": 3.8757333333333333,
      "grad_norm": 0.39389464259147644,
      "learning_rate": 2.5776666666666667e-05,
      "loss": 0.002,
      "step": 72670
    },
    {
      "epoch": 3.8762666666666665,
      "grad_norm": 0.15513618290424347,
      "learning_rate": 2.5773333333333333e-05,
      "loss": 0.0021,
      "step": 72680
    },
    {
      "epoch": 3.8768000000000002,
      "grad_norm": 0.08669451624155045,
      "learning_rate": 2.577e-05,
      "loss": 0.0019,
      "step": 72690
    },
    {
      "epoch": 3.8773333333333335,
      "grad_norm": 0.32485127449035645,
      "learning_rate": 2.5766666666666665e-05,
      "loss": 0.002,
      "step": 72700
    },
    {
      "epoch": 3.877866666666667,
      "grad_norm": 0.2608798146247864,
      "learning_rate": 2.5763333333333335e-05,
      "loss": 0.0028,
      "step": 72710
    },
    {
      "epoch": 3.8784,
      "grad_norm": 0.14442332088947296,
      "learning_rate": 2.576e-05,
      "loss": 0.0017,
      "step": 72720
    },
    {
      "epoch": 3.8789333333333333,
      "grad_norm": 0.47297123074531555,
      "learning_rate": 2.5756666666666667e-05,
      "loss": 0.0018,
      "step": 72730
    },
    {
      "epoch": 3.8794666666666666,
      "grad_norm": 0.2301768809556961,
      "learning_rate": 2.5753333333333336e-05,
      "loss": 0.0016,
      "step": 72740
    },
    {
      "epoch": 3.88,
      "grad_norm": 0.15859495103359222,
      "learning_rate": 2.5750000000000002e-05,
      "loss": 0.003,
      "step": 72750
    },
    {
      "epoch": 3.880533333333333,
      "grad_norm": 0.1542641818523407,
      "learning_rate": 2.574666666666667e-05,
      "loss": 0.0017,
      "step": 72760
    },
    {
      "epoch": 3.8810666666666664,
      "grad_norm": 0.3253379762172699,
      "learning_rate": 2.5743333333333335e-05,
      "loss": 0.0025,
      "step": 72770
    },
    {
      "epoch": 3.8816,
      "grad_norm": 0.18810877203941345,
      "learning_rate": 2.5740000000000004e-05,
      "loss": 0.0016,
      "step": 72780
    },
    {
      "epoch": 3.8821333333333334,
      "grad_norm": 0.44398683309555054,
      "learning_rate": 2.573666666666667e-05,
      "loss": 0.0021,
      "step": 72790
    },
    {
      "epoch": 3.8826666666666667,
      "grad_norm": 0.09083020687103271,
      "learning_rate": 2.5733333333333337e-05,
      "loss": 0.003,
      "step": 72800
    },
    {
      "epoch": 3.8832,
      "grad_norm": 0.06916642189025879,
      "learning_rate": 2.573e-05,
      "loss": 0.0023,
      "step": 72810
    },
    {
      "epoch": 3.8837333333333333,
      "grad_norm": 0.1222790852189064,
      "learning_rate": 2.5726666666666665e-05,
      "loss": 0.0019,
      "step": 72820
    },
    {
      "epoch": 3.8842666666666665,
      "grad_norm": 0.23650498688220978,
      "learning_rate": 2.572333333333333e-05,
      "loss": 0.0019,
      "step": 72830
    },
    {
      "epoch": 3.8848000000000003,
      "grad_norm": 0.3333739638328552,
      "learning_rate": 2.572e-05,
      "loss": 0.0021,
      "step": 72840
    },
    {
      "epoch": 3.8853333333333335,
      "grad_norm": 0.12765832245349884,
      "learning_rate": 2.5716666666666667e-05,
      "loss": 0.0024,
      "step": 72850
    },
    {
      "epoch": 3.885866666666667,
      "grad_norm": 0.3012233078479767,
      "learning_rate": 2.5713333333333333e-05,
      "loss": 0.0019,
      "step": 72860
    },
    {
      "epoch": 3.8864,
      "grad_norm": 0.3067159354686737,
      "learning_rate": 2.571e-05,
      "loss": 0.0018,
      "step": 72870
    },
    {
      "epoch": 3.8869333333333334,
      "grad_norm": 0.18540561199188232,
      "learning_rate": 2.570666666666667e-05,
      "loss": 0.0025,
      "step": 72880
    },
    {
      "epoch": 3.8874666666666666,
      "grad_norm": 0.11462854593992233,
      "learning_rate": 2.5703333333333335e-05,
      "loss": 0.0023,
      "step": 72890
    },
    {
      "epoch": 3.888,
      "grad_norm": 0.36161303520202637,
      "learning_rate": 2.57e-05,
      "loss": 0.0025,
      "step": 72900
    },
    {
      "epoch": 3.888533333333333,
      "grad_norm": 0.06108545511960983,
      "learning_rate": 2.5696666666666667e-05,
      "loss": 0.0016,
      "step": 72910
    },
    {
      "epoch": 3.8890666666666664,
      "grad_norm": 0.29797059297561646,
      "learning_rate": 2.5693333333333337e-05,
      "loss": 0.0029,
      "step": 72920
    },
    {
      "epoch": 3.8895999999999997,
      "grad_norm": 0.08769087493419647,
      "learning_rate": 2.5690000000000003e-05,
      "loss": 0.0017,
      "step": 72930
    },
    {
      "epoch": 3.8901333333333334,
      "grad_norm": 0.6491195559501648,
      "learning_rate": 2.568666666666667e-05,
      "loss": 0.0019,
      "step": 72940
    },
    {
      "epoch": 3.8906666666666667,
      "grad_norm": 0.04680410772562027,
      "learning_rate": 2.5683333333333335e-05,
      "loss": 0.0026,
      "step": 72950
    },
    {
      "epoch": 3.8912,
      "grad_norm": 0.1443767547607422,
      "learning_rate": 2.5679999999999998e-05,
      "loss": 0.0028,
      "step": 72960
    },
    {
      "epoch": 3.8917333333333333,
      "grad_norm": 0.21944521367549896,
      "learning_rate": 2.5676666666666664e-05,
      "loss": 0.0015,
      "step": 72970
    },
    {
      "epoch": 3.8922666666666665,
      "grad_norm": 0.048558421432971954,
      "learning_rate": 2.5673333333333334e-05,
      "loss": 0.0015,
      "step": 72980
    },
    {
      "epoch": 3.8928000000000003,
      "grad_norm": 0.2106129676103592,
      "learning_rate": 2.567e-05,
      "loss": 0.0016,
      "step": 72990
    },
    {
      "epoch": 3.8933333333333335,
      "grad_norm": 0.09683046489953995,
      "learning_rate": 2.5666666666666666e-05,
      "loss": 0.0014,
      "step": 73000
    },
    {
      "epoch": 3.893866666666667,
      "grad_norm": 0.36909469962120056,
      "learning_rate": 2.5663333333333332e-05,
      "loss": 0.002,
      "step": 73010
    },
    {
      "epoch": 3.8944,
      "grad_norm": 0.06022440642118454,
      "learning_rate": 2.566e-05,
      "loss": 0.0028,
      "step": 73020
    },
    {
      "epoch": 3.8949333333333334,
      "grad_norm": 0.09665841609239578,
      "learning_rate": 2.5656666666666668e-05,
      "loss": 0.002,
      "step": 73030
    },
    {
      "epoch": 3.8954666666666666,
      "grad_norm": 0.09194303303956985,
      "learning_rate": 2.5653333333333334e-05,
      "loss": 0.0016,
      "step": 73040
    },
    {
      "epoch": 3.896,
      "grad_norm": 0.24693244695663452,
      "learning_rate": 2.5650000000000003e-05,
      "loss": 0.002,
      "step": 73050
    },
    {
      "epoch": 3.896533333333333,
      "grad_norm": 0.2466984987258911,
      "learning_rate": 2.564666666666667e-05,
      "loss": 0.0019,
      "step": 73060
    },
    {
      "epoch": 3.8970666666666665,
      "grad_norm": 0.17568565905094147,
      "learning_rate": 2.5643333333333336e-05,
      "loss": 0.0021,
      "step": 73070
    },
    {
      "epoch": 3.8975999999999997,
      "grad_norm": 0.29805439710617065,
      "learning_rate": 2.5640000000000002e-05,
      "loss": 0.0017,
      "step": 73080
    },
    {
      "epoch": 3.8981333333333335,
      "grad_norm": 0.16439516842365265,
      "learning_rate": 2.563666666666667e-05,
      "loss": 0.0025,
      "step": 73090
    },
    {
      "epoch": 3.8986666666666667,
      "grad_norm": 0.32992619276046753,
      "learning_rate": 2.5633333333333338e-05,
      "loss": 0.0017,
      "step": 73100
    },
    {
      "epoch": 3.8992,
      "grad_norm": 0.01717783324420452,
      "learning_rate": 2.5629999999999997e-05,
      "loss": 0.0019,
      "step": 73110
    },
    {
      "epoch": 3.8997333333333333,
      "grad_norm": 0.2394627034664154,
      "learning_rate": 2.5626666666666666e-05,
      "loss": 0.0034,
      "step": 73120
    },
    {
      "epoch": 3.9002666666666665,
      "grad_norm": 0.3907047212123871,
      "learning_rate": 2.5623333333333333e-05,
      "loss": 0.0015,
      "step": 73130
    },
    {
      "epoch": 3.9008000000000003,
      "grad_norm": 0.11406131833791733,
      "learning_rate": 2.562e-05,
      "loss": 0.0019,
      "step": 73140
    },
    {
      "epoch": 3.9013333333333335,
      "grad_norm": 0.357072651386261,
      "learning_rate": 2.5616666666666668e-05,
      "loss": 0.0025,
      "step": 73150
    },
    {
      "epoch": 3.901866666666667,
      "grad_norm": 0.4109147787094116,
      "learning_rate": 2.5613333333333334e-05,
      "loss": 0.0017,
      "step": 73160
    },
    {
      "epoch": 3.9024,
      "grad_norm": 0.26285701990127563,
      "learning_rate": 2.561e-05,
      "loss": 0.002,
      "step": 73170
    },
    {
      "epoch": 3.9029333333333334,
      "grad_norm": 0.09800101816654205,
      "learning_rate": 2.5606666666666667e-05,
      "loss": 0.0023,
      "step": 73180
    },
    {
      "epoch": 3.9034666666666666,
      "grad_norm": 0.0663120374083519,
      "learning_rate": 2.5603333333333336e-05,
      "loss": 0.0019,
      "step": 73190
    },
    {
      "epoch": 3.904,
      "grad_norm": 0.5025410652160645,
      "learning_rate": 2.5600000000000002e-05,
      "loss": 0.0024,
      "step": 73200
    },
    {
      "epoch": 3.904533333333333,
      "grad_norm": 0.542543888092041,
      "learning_rate": 2.559666666666667e-05,
      "loss": 0.0027,
      "step": 73210
    },
    {
      "epoch": 3.9050666666666665,
      "grad_norm": 0.2374931275844574,
      "learning_rate": 2.5593333333333334e-05,
      "loss": 0.0019,
      "step": 73220
    },
    {
      "epoch": 3.9055999999999997,
      "grad_norm": 0.30111974477767944,
      "learning_rate": 2.5590000000000004e-05,
      "loss": 0.0024,
      "step": 73230
    },
    {
      "epoch": 3.9061333333333335,
      "grad_norm": 0.32502615451812744,
      "learning_rate": 2.558666666666667e-05,
      "loss": 0.0029,
      "step": 73240
    },
    {
      "epoch": 3.9066666666666667,
      "grad_norm": 0.09434852004051208,
      "learning_rate": 2.5583333333333336e-05,
      "loss": 0.0021,
      "step": 73250
    },
    {
      "epoch": 3.9072,
      "grad_norm": 0.2110918015241623,
      "learning_rate": 2.5580000000000002e-05,
      "loss": 0.0038,
      "step": 73260
    },
    {
      "epoch": 3.9077333333333333,
      "grad_norm": 0.3604217767715454,
      "learning_rate": 2.5576666666666665e-05,
      "loss": 0.0019,
      "step": 73270
    },
    {
      "epoch": 3.9082666666666666,
      "grad_norm": 0.2124921828508377,
      "learning_rate": 2.557333333333333e-05,
      "loss": 0.0019,
      "step": 73280
    },
    {
      "epoch": 3.9088000000000003,
      "grad_norm": 0.1760682463645935,
      "learning_rate": 2.557e-05,
      "loss": 0.002,
      "step": 73290
    },
    {
      "epoch": 3.9093333333333335,
      "grad_norm": 0.09067727625370026,
      "learning_rate": 2.5566666666666667e-05,
      "loss": 0.0029,
      "step": 73300
    },
    {
      "epoch": 3.909866666666667,
      "grad_norm": 0.41915541887283325,
      "learning_rate": 2.5563333333333333e-05,
      "loss": 0.002,
      "step": 73310
    },
    {
      "epoch": 3.9104,
      "grad_norm": 0.4712788462638855,
      "learning_rate": 2.556e-05,
      "loss": 0.0018,
      "step": 73320
    },
    {
      "epoch": 3.9109333333333334,
      "grad_norm": 0.042160384356975555,
      "learning_rate": 2.555666666666667e-05,
      "loss": 0.0018,
      "step": 73330
    },
    {
      "epoch": 3.9114666666666666,
      "grad_norm": 0.21478328108787537,
      "learning_rate": 2.5553333333333335e-05,
      "loss": 0.0017,
      "step": 73340
    },
    {
      "epoch": 3.912,
      "grad_norm": 0.23792631924152374,
      "learning_rate": 2.555e-05,
      "loss": 0.0019,
      "step": 73350
    },
    {
      "epoch": 3.912533333333333,
      "grad_norm": 0.06294286996126175,
      "learning_rate": 2.5546666666666667e-05,
      "loss": 0.0026,
      "step": 73360
    },
    {
      "epoch": 3.9130666666666665,
      "grad_norm": 0.12354705482721329,
      "learning_rate": 2.5543333333333337e-05,
      "loss": 0.0017,
      "step": 73370
    },
    {
      "epoch": 3.9135999999999997,
      "grad_norm": 0.3269764184951782,
      "learning_rate": 2.5540000000000003e-05,
      "loss": 0.0025,
      "step": 73380
    },
    {
      "epoch": 3.9141333333333335,
      "grad_norm": 0.522118091583252,
      "learning_rate": 2.553666666666667e-05,
      "loss": 0.0033,
      "step": 73390
    },
    {
      "epoch": 3.9146666666666667,
      "grad_norm": 0.09947168081998825,
      "learning_rate": 2.553333333333334e-05,
      "loss": 0.0023,
      "step": 73400
    },
    {
      "epoch": 3.9152,
      "grad_norm": 0.2966581881046295,
      "learning_rate": 2.5530000000000005e-05,
      "loss": 0.0018,
      "step": 73410
    },
    {
      "epoch": 3.9157333333333333,
      "grad_norm": 0.09468977153301239,
      "learning_rate": 2.5526666666666664e-05,
      "loss": 0.0019,
      "step": 73420
    },
    {
      "epoch": 3.9162666666666666,
      "grad_norm": 0.03746352717280388,
      "learning_rate": 2.5523333333333333e-05,
      "loss": 0.0014,
      "step": 73430
    },
    {
      "epoch": 3.9168,
      "grad_norm": 0.09231465309858322,
      "learning_rate": 2.552e-05,
      "loss": 0.0019,
      "step": 73440
    },
    {
      "epoch": 3.9173333333333336,
      "grad_norm": 0.4182089865207672,
      "learning_rate": 2.5516666666666666e-05,
      "loss": 0.0029,
      "step": 73450
    },
    {
      "epoch": 3.917866666666667,
      "grad_norm": 0.5063170194625854,
      "learning_rate": 2.5513333333333332e-05,
      "loss": 0.0018,
      "step": 73460
    },
    {
      "epoch": 3.9184,
      "grad_norm": 0.24847820401191711,
      "learning_rate": 2.551e-05,
      "loss": 0.0029,
      "step": 73470
    },
    {
      "epoch": 3.9189333333333334,
      "grad_norm": 0.27095669507980347,
      "learning_rate": 2.5506666666666668e-05,
      "loss": 0.0018,
      "step": 73480
    },
    {
      "epoch": 3.9194666666666667,
      "grad_norm": 0.07551947981119156,
      "learning_rate": 2.5503333333333334e-05,
      "loss": 0.0016,
      "step": 73490
    },
    {
      "epoch": 3.92,
      "grad_norm": 0.2178175449371338,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 0.0025,
      "step": 73500
    },
    {
      "epoch": 3.920533333333333,
      "grad_norm": 0.08984828740358353,
      "learning_rate": 2.549666666666667e-05,
      "loss": 0.0029,
      "step": 73510
    },
    {
      "epoch": 3.9210666666666665,
      "grad_norm": 0.26091140508651733,
      "learning_rate": 2.5493333333333335e-05,
      "loss": 0.0018,
      "step": 73520
    },
    {
      "epoch": 3.9215999999999998,
      "grad_norm": 0.29451876878738403,
      "learning_rate": 2.549e-05,
      "loss": 0.0022,
      "step": 73530
    },
    {
      "epoch": 3.9221333333333335,
      "grad_norm": 0.23884806036949158,
      "learning_rate": 2.548666666666667e-05,
      "loss": 0.0024,
      "step": 73540
    },
    {
      "epoch": 3.9226666666666667,
      "grad_norm": 0.467155784368515,
      "learning_rate": 2.5483333333333337e-05,
      "loss": 0.0019,
      "step": 73550
    },
    {
      "epoch": 3.9232,
      "grad_norm": 0.48573634028434753,
      "learning_rate": 2.5480000000000003e-05,
      "loss": 0.0025,
      "step": 73560
    },
    {
      "epoch": 3.9237333333333333,
      "grad_norm": 0.35112521052360535,
      "learning_rate": 2.5476666666666666e-05,
      "loss": 0.003,
      "step": 73570
    },
    {
      "epoch": 3.9242666666666666,
      "grad_norm": 0.3282128572463989,
      "learning_rate": 2.5473333333333332e-05,
      "loss": 0.0027,
      "step": 73580
    },
    {
      "epoch": 3.9248,
      "grad_norm": 0.15213370323181152,
      "learning_rate": 2.547e-05,
      "loss": 0.0017,
      "step": 73590
    },
    {
      "epoch": 3.9253333333333336,
      "grad_norm": 0.09610415250062943,
      "learning_rate": 2.5466666666666668e-05,
      "loss": 0.0015,
      "step": 73600
    },
    {
      "epoch": 3.925866666666667,
      "grad_norm": 0.046109482645988464,
      "learning_rate": 2.5463333333333334e-05,
      "loss": 0.0019,
      "step": 73610
    },
    {
      "epoch": 3.9264,
      "grad_norm": 0.4166140556335449,
      "learning_rate": 2.546e-05,
      "loss": 0.0014,
      "step": 73620
    },
    {
      "epoch": 3.9269333333333334,
      "grad_norm": 0.3502556383609772,
      "learning_rate": 2.5456666666666666e-05,
      "loss": 0.0019,
      "step": 73630
    },
    {
      "epoch": 3.9274666666666667,
      "grad_norm": 0.1813795566558838,
      "learning_rate": 2.5453333333333336e-05,
      "loss": 0.0012,
      "step": 73640
    },
    {
      "epoch": 3.928,
      "grad_norm": 0.20702600479125977,
      "learning_rate": 2.5450000000000002e-05,
      "loss": 0.0018,
      "step": 73650
    },
    {
      "epoch": 3.928533333333333,
      "grad_norm": 0.3762199580669403,
      "learning_rate": 2.5446666666666668e-05,
      "loss": 0.0018,
      "step": 73660
    },
    {
      "epoch": 3.9290666666666665,
      "grad_norm": 0.18000730872154236,
      "learning_rate": 2.5443333333333334e-05,
      "loss": 0.0025,
      "step": 73670
    },
    {
      "epoch": 3.9295999999999998,
      "grad_norm": 0.3049629032611847,
      "learning_rate": 2.5440000000000004e-05,
      "loss": 0.002,
      "step": 73680
    },
    {
      "epoch": 3.9301333333333335,
      "grad_norm": 0.6734629273414612,
      "learning_rate": 2.543666666666667e-05,
      "loss": 0.0017,
      "step": 73690
    },
    {
      "epoch": 3.9306666666666668,
      "grad_norm": 0.05735282227396965,
      "learning_rate": 2.5433333333333336e-05,
      "loss": 0.0034,
      "step": 73700
    },
    {
      "epoch": 3.9312,
      "grad_norm": 0.07293608039617538,
      "learning_rate": 2.5430000000000002e-05,
      "loss": 0.0022,
      "step": 73710
    },
    {
      "epoch": 3.9317333333333333,
      "grad_norm": 0.0633571445941925,
      "learning_rate": 2.5426666666666665e-05,
      "loss": 0.0021,
      "step": 73720
    },
    {
      "epoch": 3.9322666666666666,
      "grad_norm": 0.4697876274585724,
      "learning_rate": 2.542333333333333e-05,
      "loss": 0.0021,
      "step": 73730
    },
    {
      "epoch": 3.9328,
      "grad_norm": 0.32468703389167786,
      "learning_rate": 2.542e-05,
      "loss": 0.0014,
      "step": 73740
    },
    {
      "epoch": 3.9333333333333336,
      "grad_norm": 0.07515094429254532,
      "learning_rate": 2.5416666666666667e-05,
      "loss": 0.0015,
      "step": 73750
    },
    {
      "epoch": 3.933866666666667,
      "grad_norm": 0.1232721209526062,
      "learning_rate": 2.5413333333333333e-05,
      "loss": 0.0018,
      "step": 73760
    },
    {
      "epoch": 3.9344,
      "grad_norm": 0.07904073596000671,
      "learning_rate": 2.541e-05,
      "loss": 0.0016,
      "step": 73770
    },
    {
      "epoch": 3.9349333333333334,
      "grad_norm": 0.1396516114473343,
      "learning_rate": 2.540666666666667e-05,
      "loss": 0.0026,
      "step": 73780
    },
    {
      "epoch": 3.9354666666666667,
      "grad_norm": 0.15976746380329132,
      "learning_rate": 2.5403333333333335e-05,
      "loss": 0.0025,
      "step": 73790
    },
    {
      "epoch": 3.936,
      "grad_norm": 0.14505821466445923,
      "learning_rate": 2.54e-05,
      "loss": 0.0019,
      "step": 73800
    },
    {
      "epoch": 3.936533333333333,
      "grad_norm": 0.6471471190452576,
      "learning_rate": 2.539666666666667e-05,
      "loss": 0.0021,
      "step": 73810
    },
    {
      "epoch": 3.9370666666666665,
      "grad_norm": 0.349401593208313,
      "learning_rate": 2.5393333333333336e-05,
      "loss": 0.0017,
      "step": 73820
    },
    {
      "epoch": 3.9375999999999998,
      "grad_norm": 0.20837098360061646,
      "learning_rate": 2.5390000000000003e-05,
      "loss": 0.0021,
      "step": 73830
    },
    {
      "epoch": 3.9381333333333335,
      "grad_norm": 0.0660574734210968,
      "learning_rate": 2.538666666666667e-05,
      "loss": 0.0018,
      "step": 73840
    },
    {
      "epoch": 3.9386666666666668,
      "grad_norm": 0.17611871659755707,
      "learning_rate": 2.5383333333333338e-05,
      "loss": 0.0013,
      "step": 73850
    },
    {
      "epoch": 3.9392,
      "grad_norm": 0.2353748381137848,
      "learning_rate": 2.5380000000000004e-05,
      "loss": 0.0024,
      "step": 73860
    },
    {
      "epoch": 3.9397333333333333,
      "grad_norm": 0.15346252918243408,
      "learning_rate": 2.5376666666666664e-05,
      "loss": 0.0021,
      "step": 73870
    },
    {
      "epoch": 3.9402666666666666,
      "grad_norm": 0.08998505771160126,
      "learning_rate": 2.5373333333333333e-05,
      "loss": 0.0024,
      "step": 73880
    },
    {
      "epoch": 3.9408,
      "grad_norm": 0.32249316573143005,
      "learning_rate": 2.537e-05,
      "loss": 0.0021,
      "step": 73890
    },
    {
      "epoch": 3.9413333333333336,
      "grad_norm": 0.17738324403762817,
      "learning_rate": 2.5366666666666665e-05,
      "loss": 0.0021,
      "step": 73900
    },
    {
      "epoch": 3.941866666666667,
      "grad_norm": 0.2172284871339798,
      "learning_rate": 2.5363333333333335e-05,
      "loss": 0.0022,
      "step": 73910
    },
    {
      "epoch": 3.9424,
      "grad_norm": 0.11684191226959229,
      "learning_rate": 2.536e-05,
      "loss": 0.0022,
      "step": 73920
    },
    {
      "epoch": 3.9429333333333334,
      "grad_norm": 0.22428663074970245,
      "learning_rate": 2.5356666666666667e-05,
      "loss": 0.0023,
      "step": 73930
    },
    {
      "epoch": 3.9434666666666667,
      "grad_norm": 0.4644566476345062,
      "learning_rate": 2.5353333333333333e-05,
      "loss": 0.0015,
      "step": 73940
    },
    {
      "epoch": 3.944,
      "grad_norm": 0.14365069568157196,
      "learning_rate": 2.5350000000000003e-05,
      "loss": 0.0024,
      "step": 73950
    },
    {
      "epoch": 3.9445333333333332,
      "grad_norm": 0.19339331984519958,
      "learning_rate": 2.534666666666667e-05,
      "loss": 0.0015,
      "step": 73960
    },
    {
      "epoch": 3.9450666666666665,
      "grad_norm": 0.5368578433990479,
      "learning_rate": 2.5343333333333335e-05,
      "loss": 0.0014,
      "step": 73970
    },
    {
      "epoch": 3.9455999999999998,
      "grad_norm": 0.04506867378950119,
      "learning_rate": 2.534e-05,
      "loss": 0.0022,
      "step": 73980
    },
    {
      "epoch": 3.9461333333333335,
      "grad_norm": 0.03820762038230896,
      "learning_rate": 2.533666666666667e-05,
      "loss": 0.0018,
      "step": 73990
    },
    {
      "epoch": 3.9466666666666668,
      "grad_norm": 0.556706428527832,
      "learning_rate": 2.5333333333333337e-05,
      "loss": 0.0024,
      "step": 74000
    },
    {
      "epoch": 3.9472,
      "grad_norm": 0.6519089937210083,
      "learning_rate": 2.5330000000000003e-05,
      "loss": 0.002,
      "step": 74010
    },
    {
      "epoch": 3.9477333333333333,
      "grad_norm": 0.15621395409107208,
      "learning_rate": 2.5326666666666666e-05,
      "loss": 0.0018,
      "step": 74020
    },
    {
      "epoch": 3.9482666666666666,
      "grad_norm": 0.024648405611515045,
      "learning_rate": 2.5323333333333332e-05,
      "loss": 0.0027,
      "step": 74030
    },
    {
      "epoch": 3.9488,
      "grad_norm": 0.8244454264640808,
      "learning_rate": 2.5319999999999998e-05,
      "loss": 0.0023,
      "step": 74040
    },
    {
      "epoch": 3.9493333333333336,
      "grad_norm": 0.36119428277015686,
      "learning_rate": 2.5316666666666668e-05,
      "loss": 0.0025,
      "step": 74050
    },
    {
      "epoch": 3.949866666666667,
      "grad_norm": 0.4055657982826233,
      "learning_rate": 2.5313333333333334e-05,
      "loss": 0.0023,
      "step": 74060
    },
    {
      "epoch": 3.9504,
      "grad_norm": 0.2484079748392105,
      "learning_rate": 2.531e-05,
      "loss": 0.0023,
      "step": 74070
    },
    {
      "epoch": 3.9509333333333334,
      "grad_norm": 0.052934180945158005,
      "learning_rate": 2.5306666666666666e-05,
      "loss": 0.0016,
      "step": 74080
    },
    {
      "epoch": 3.9514666666666667,
      "grad_norm": 0.4755083918571472,
      "learning_rate": 2.5303333333333336e-05,
      "loss": 0.0025,
      "step": 74090
    },
    {
      "epoch": 3.952,
      "grad_norm": 0.28770193457603455,
      "learning_rate": 2.5300000000000002e-05,
      "loss": 0.0028,
      "step": 74100
    },
    {
      "epoch": 3.9525333333333332,
      "grad_norm": 0.3588065803050995,
      "learning_rate": 2.5296666666666668e-05,
      "loss": 0.0021,
      "step": 74110
    },
    {
      "epoch": 3.9530666666666665,
      "grad_norm": 0.35391807556152344,
      "learning_rate": 2.5293333333333334e-05,
      "loss": 0.0026,
      "step": 74120
    },
    {
      "epoch": 3.9536,
      "grad_norm": 0.2319210320711136,
      "learning_rate": 2.5290000000000004e-05,
      "loss": 0.0018,
      "step": 74130
    },
    {
      "epoch": 3.9541333333333335,
      "grad_norm": 0.20610181987285614,
      "learning_rate": 2.528666666666667e-05,
      "loss": 0.0019,
      "step": 74140
    },
    {
      "epoch": 3.9546666666666668,
      "grad_norm": 0.3016353249549866,
      "learning_rate": 2.5283333333333336e-05,
      "loss": 0.0016,
      "step": 74150
    },
    {
      "epoch": 3.9552,
      "grad_norm": 0.061825599521398544,
      "learning_rate": 2.5280000000000005e-05,
      "loss": 0.0019,
      "step": 74160
    },
    {
      "epoch": 3.9557333333333333,
      "grad_norm": 0.13531212508678436,
      "learning_rate": 2.5276666666666665e-05,
      "loss": 0.0018,
      "step": 74170
    },
    {
      "epoch": 3.9562666666666666,
      "grad_norm": 0.37872758507728577,
      "learning_rate": 2.527333333333333e-05,
      "loss": 0.0024,
      "step": 74180
    },
    {
      "epoch": 3.9568,
      "grad_norm": 0.09900620579719543,
      "learning_rate": 2.527e-05,
      "loss": 0.0015,
      "step": 74190
    },
    {
      "epoch": 3.9573333333333336,
      "grad_norm": 0.32522377371788025,
      "learning_rate": 2.5266666666666666e-05,
      "loss": 0.0016,
      "step": 74200
    },
    {
      "epoch": 3.957866666666667,
      "grad_norm": 0.31842371821403503,
      "learning_rate": 2.5263333333333333e-05,
      "loss": 0.0023,
      "step": 74210
    },
    {
      "epoch": 3.9584,
      "grad_norm": 0.11888888478279114,
      "learning_rate": 2.526e-05,
      "loss": 0.0024,
      "step": 74220
    },
    {
      "epoch": 3.9589333333333334,
      "grad_norm": 0.2573847770690918,
      "learning_rate": 2.5256666666666668e-05,
      "loss": 0.0032,
      "step": 74230
    },
    {
      "epoch": 3.9594666666666667,
      "grad_norm": 0.17276068031787872,
      "learning_rate": 2.5253333333333334e-05,
      "loss": 0.002,
      "step": 74240
    },
    {
      "epoch": 3.96,
      "grad_norm": 0.06501968950033188,
      "learning_rate": 2.525e-05,
      "loss": 0.0025,
      "step": 74250
    },
    {
      "epoch": 3.9605333333333332,
      "grad_norm": 0.288505494594574,
      "learning_rate": 2.524666666666667e-05,
      "loss": 0.0022,
      "step": 74260
    },
    {
      "epoch": 3.9610666666666665,
      "grad_norm": 0.07662059366703033,
      "learning_rate": 2.5243333333333336e-05,
      "loss": 0.0014,
      "step": 74270
    },
    {
      "epoch": 3.9616,
      "grad_norm": 0.1004389151930809,
      "learning_rate": 2.5240000000000002e-05,
      "loss": 0.0013,
      "step": 74280
    },
    {
      "epoch": 3.962133333333333,
      "grad_norm": 0.08559024333953857,
      "learning_rate": 2.523666666666667e-05,
      "loss": 0.0023,
      "step": 74290
    },
    {
      "epoch": 3.962666666666667,
      "grad_norm": 0.2643814980983734,
      "learning_rate": 2.5233333333333338e-05,
      "loss": 0.0027,
      "step": 74300
    },
    {
      "epoch": 3.9632,
      "grad_norm": 0.43827202916145325,
      "learning_rate": 2.5230000000000004e-05,
      "loss": 0.0016,
      "step": 74310
    },
    {
      "epoch": 3.9637333333333333,
      "grad_norm": 0.11910233646631241,
      "learning_rate": 2.5226666666666663e-05,
      "loss": 0.0013,
      "step": 74320
    },
    {
      "epoch": 3.9642666666666666,
      "grad_norm": 0.0983976349234581,
      "learning_rate": 2.5223333333333333e-05,
      "loss": 0.0027,
      "step": 74330
    },
    {
      "epoch": 3.9648,
      "grad_norm": 0.546416699886322,
      "learning_rate": 2.522e-05,
      "loss": 0.0035,
      "step": 74340
    },
    {
      "epoch": 3.9653333333333336,
      "grad_norm": 0.23258116841316223,
      "learning_rate": 2.5216666666666665e-05,
      "loss": 0.0018,
      "step": 74350
    },
    {
      "epoch": 3.965866666666667,
      "grad_norm": 0.4727388024330139,
      "learning_rate": 2.5213333333333335e-05,
      "loss": 0.0023,
      "step": 74360
    },
    {
      "epoch": 3.9664,
      "grad_norm": 0.24445225298404694,
      "learning_rate": 2.521e-05,
      "loss": 0.0017,
      "step": 74370
    },
    {
      "epoch": 3.9669333333333334,
      "grad_norm": 0.40516170859336853,
      "learning_rate": 2.5206666666666667e-05,
      "loss": 0.002,
      "step": 74380
    },
    {
      "epoch": 3.9674666666666667,
      "grad_norm": 0.14606420695781708,
      "learning_rate": 2.5203333333333333e-05,
      "loss": 0.0016,
      "step": 74390
    },
    {
      "epoch": 3.968,
      "grad_norm": 0.12742182612419128,
      "learning_rate": 2.5200000000000003e-05,
      "loss": 0.002,
      "step": 74400
    },
    {
      "epoch": 3.9685333333333332,
      "grad_norm": 0.09451364725828171,
      "learning_rate": 2.519666666666667e-05,
      "loss": 0.0017,
      "step": 74410
    },
    {
      "epoch": 3.9690666666666665,
      "grad_norm": 0.14334194362163544,
      "learning_rate": 2.5193333333333335e-05,
      "loss": 0.0023,
      "step": 74420
    },
    {
      "epoch": 3.9696,
      "grad_norm": 0.23689360916614532,
      "learning_rate": 2.519e-05,
      "loss": 0.0016,
      "step": 74430
    },
    {
      "epoch": 3.970133333333333,
      "grad_norm": 0.41615185141563416,
      "learning_rate": 2.518666666666667e-05,
      "loss": 0.0024,
      "step": 74440
    },
    {
      "epoch": 3.970666666666667,
      "grad_norm": 0.1448916345834732,
      "learning_rate": 2.5183333333333337e-05,
      "loss": 0.0027,
      "step": 74450
    },
    {
      "epoch": 3.9712,
      "grad_norm": 0.18666815757751465,
      "learning_rate": 2.5180000000000003e-05,
      "loss": 0.0016,
      "step": 74460
    },
    {
      "epoch": 3.9717333333333333,
      "grad_norm": 0.20320600271224976,
      "learning_rate": 2.517666666666667e-05,
      "loss": 0.0017,
      "step": 74470
    },
    {
      "epoch": 3.9722666666666666,
      "grad_norm": 0.09914745390415192,
      "learning_rate": 2.5173333333333332e-05,
      "loss": 0.002,
      "step": 74480
    },
    {
      "epoch": 3.9728,
      "grad_norm": 0.2285703867673874,
      "learning_rate": 2.5169999999999998e-05,
      "loss": 0.0015,
      "step": 74490
    },
    {
      "epoch": 3.9733333333333336,
      "grad_norm": 0.3336467742919922,
      "learning_rate": 2.5166666666666667e-05,
      "loss": 0.0017,
      "step": 74500
    },
    {
      "epoch": 3.973866666666667,
      "grad_norm": 0.2124120146036148,
      "learning_rate": 2.5163333333333334e-05,
      "loss": 0.0017,
      "step": 74510
    },
    {
      "epoch": 3.9744,
      "grad_norm": 0.029828064143657684,
      "learning_rate": 2.516e-05,
      "loss": 0.0018,
      "step": 74520
    },
    {
      "epoch": 3.9749333333333334,
      "grad_norm": 0.2670190632343292,
      "learning_rate": 2.5156666666666666e-05,
      "loss": 0.002,
      "step": 74530
    },
    {
      "epoch": 3.9754666666666667,
      "grad_norm": 0.4941445291042328,
      "learning_rate": 2.5153333333333335e-05,
      "loss": 0.0018,
      "step": 74540
    },
    {
      "epoch": 3.976,
      "grad_norm": 0.14478252828121185,
      "learning_rate": 2.515e-05,
      "loss": 0.0026,
      "step": 74550
    },
    {
      "epoch": 3.9765333333333333,
      "grad_norm": 0.06165821850299835,
      "learning_rate": 2.5146666666666668e-05,
      "loss": 0.0013,
      "step": 74560
    },
    {
      "epoch": 3.9770666666666665,
      "grad_norm": 0.26880142092704773,
      "learning_rate": 2.5143333333333334e-05,
      "loss": 0.0015,
      "step": 74570
    },
    {
      "epoch": 3.9776,
      "grad_norm": 0.32590773701667786,
      "learning_rate": 2.5140000000000003e-05,
      "loss": 0.0023,
      "step": 74580
    },
    {
      "epoch": 3.978133333333333,
      "grad_norm": 0.20883548259735107,
      "learning_rate": 2.513666666666667e-05,
      "loss": 0.0023,
      "step": 74590
    },
    {
      "epoch": 3.978666666666667,
      "grad_norm": 0.2660076320171356,
      "learning_rate": 2.5133333333333336e-05,
      "loss": 0.0016,
      "step": 74600
    },
    {
      "epoch": 3.9792,
      "grad_norm": 0.3006064295768738,
      "learning_rate": 2.5130000000000005e-05,
      "loss": 0.0021,
      "step": 74610
    },
    {
      "epoch": 3.9797333333333333,
      "grad_norm": 0.14976853132247925,
      "learning_rate": 2.512666666666667e-05,
      "loss": 0.0014,
      "step": 74620
    },
    {
      "epoch": 3.9802666666666666,
      "grad_norm": 0.16449086368083954,
      "learning_rate": 2.512333333333333e-05,
      "loss": 0.0028,
      "step": 74630
    },
    {
      "epoch": 3.9808,
      "grad_norm": 0.2074439376592636,
      "learning_rate": 2.512e-05,
      "loss": 0.0021,
      "step": 74640
    },
    {
      "epoch": 3.981333333333333,
      "grad_norm": 0.33894509077072144,
      "learning_rate": 2.5116666666666666e-05,
      "loss": 0.0028,
      "step": 74650
    },
    {
      "epoch": 3.981866666666667,
      "grad_norm": 0.6122666597366333,
      "learning_rate": 2.5113333333333332e-05,
      "loss": 0.0024,
      "step": 74660
    },
    {
      "epoch": 3.9824,
      "grad_norm": 0.09051793813705444,
      "learning_rate": 2.5110000000000002e-05,
      "loss": 0.0019,
      "step": 74670
    },
    {
      "epoch": 3.9829333333333334,
      "grad_norm": 0.646756649017334,
      "learning_rate": 2.5106666666666668e-05,
      "loss": 0.0017,
      "step": 74680
    },
    {
      "epoch": 3.9834666666666667,
      "grad_norm": 0.21909500658512115,
      "learning_rate": 2.5103333333333334e-05,
      "loss": 0.0018,
      "step": 74690
    },
    {
      "epoch": 3.984,
      "grad_norm": 0.17646101117134094,
      "learning_rate": 2.51e-05,
      "loss": 0.0016,
      "step": 74700
    },
    {
      "epoch": 3.9845333333333333,
      "grad_norm": 0.43546828627586365,
      "learning_rate": 2.509666666666667e-05,
      "loss": 0.0012,
      "step": 74710
    },
    {
      "epoch": 3.9850666666666665,
      "grad_norm": 0.28168314695358276,
      "learning_rate": 2.5093333333333336e-05,
      "loss": 0.0017,
      "step": 74720
    },
    {
      "epoch": 3.9856,
      "grad_norm": 0.2962421774864197,
      "learning_rate": 2.5090000000000002e-05,
      "loss": 0.0027,
      "step": 74730
    },
    {
      "epoch": 3.986133333333333,
      "grad_norm": 0.23719555139541626,
      "learning_rate": 2.5086666666666668e-05,
      "loss": 0.0016,
      "step": 74740
    },
    {
      "epoch": 3.986666666666667,
      "grad_norm": 0.3921816051006317,
      "learning_rate": 2.5083333333333338e-05,
      "loss": 0.0019,
      "step": 74750
    },
    {
      "epoch": 3.9872,
      "grad_norm": 0.14728379249572754,
      "learning_rate": 2.5080000000000004e-05,
      "loss": 0.0014,
      "step": 74760
    },
    {
      "epoch": 3.9877333333333334,
      "grad_norm": 0.29374197125434875,
      "learning_rate": 2.507666666666667e-05,
      "loss": 0.0019,
      "step": 74770
    },
    {
      "epoch": 3.9882666666666666,
      "grad_norm": 0.09391795098781586,
      "learning_rate": 2.5073333333333333e-05,
      "loss": 0.0015,
      "step": 74780
    },
    {
      "epoch": 3.9888,
      "grad_norm": 0.210152268409729,
      "learning_rate": 2.507e-05,
      "loss": 0.0017,
      "step": 74790
    },
    {
      "epoch": 3.989333333333333,
      "grad_norm": 0.23013924062252045,
      "learning_rate": 2.5066666666666665e-05,
      "loss": 0.0027,
      "step": 74800
    },
    {
      "epoch": 3.989866666666667,
      "grad_norm": 0.47531211376190186,
      "learning_rate": 2.5063333333333334e-05,
      "loss": 0.0021,
      "step": 74810
    },
    {
      "epoch": 3.9904,
      "grad_norm": 0.6558499336242676,
      "learning_rate": 2.506e-05,
      "loss": 0.002,
      "step": 74820
    },
    {
      "epoch": 3.9909333333333334,
      "grad_norm": 0.10383075475692749,
      "learning_rate": 2.5056666666666667e-05,
      "loss": 0.0028,
      "step": 74830
    },
    {
      "epoch": 3.9914666666666667,
      "grad_norm": 0.2688378095626831,
      "learning_rate": 2.5053333333333333e-05,
      "loss": 0.0019,
      "step": 74840
    },
    {
      "epoch": 3.992,
      "grad_norm": 0.09598193317651749,
      "learning_rate": 2.5050000000000002e-05,
      "loss": 0.0028,
      "step": 74850
    },
    {
      "epoch": 3.9925333333333333,
      "grad_norm": 0.21734224259853363,
      "learning_rate": 2.504666666666667e-05,
      "loss": 0.0021,
      "step": 74860
    },
    {
      "epoch": 3.9930666666666665,
      "grad_norm": 0.240132674574852,
      "learning_rate": 2.5043333333333335e-05,
      "loss": 0.0015,
      "step": 74870
    },
    {
      "epoch": 3.9936,
      "grad_norm": 0.07216431945562363,
      "learning_rate": 2.504e-05,
      "loss": 0.0025,
      "step": 74880
    },
    {
      "epoch": 3.994133333333333,
      "grad_norm": 0.06278251856565475,
      "learning_rate": 2.503666666666667e-05,
      "loss": 0.0018,
      "step": 74890
    },
    {
      "epoch": 3.994666666666667,
      "grad_norm": 0.2729644477367401,
      "learning_rate": 2.5033333333333336e-05,
      "loss": 0.002,
      "step": 74900
    },
    {
      "epoch": 3.9952,
      "grad_norm": 0.20075280964374542,
      "learning_rate": 2.5030000000000003e-05,
      "loss": 0.0015,
      "step": 74910
    },
    {
      "epoch": 3.9957333333333334,
      "grad_norm": 0.07746696472167969,
      "learning_rate": 2.5026666666666672e-05,
      "loss": 0.0017,
      "step": 74920
    },
    {
      "epoch": 3.9962666666666666,
      "grad_norm": 0.10673972219228745,
      "learning_rate": 2.502333333333333e-05,
      "loss": 0.0027,
      "step": 74930
    },
    {
      "epoch": 3.9968,
      "grad_norm": 0.05613207444548607,
      "learning_rate": 2.5019999999999998e-05,
      "loss": 0.0022,
      "step": 74940
    },
    {
      "epoch": 3.997333333333333,
      "grad_norm": 0.2013341635465622,
      "learning_rate": 2.5016666666666667e-05,
      "loss": 0.0016,
      "step": 74950
    },
    {
      "epoch": 3.997866666666667,
      "grad_norm": 0.041070740669965744,
      "learning_rate": 2.5013333333333333e-05,
      "loss": 0.0025,
      "step": 74960
    },
    {
      "epoch": 3.9984,
      "grad_norm": 0.3216799199581146,
      "learning_rate": 2.501e-05,
      "loss": 0.0022,
      "step": 74970
    },
    {
      "epoch": 3.9989333333333335,
      "grad_norm": 0.2904794216156006,
      "learning_rate": 2.5006666666666666e-05,
      "loss": 0.0022,
      "step": 74980
    },
    {
      "epoch": 3.9994666666666667,
      "grad_norm": 0.046876128762960434,
      "learning_rate": 2.5003333333333335e-05,
      "loss": 0.0019,
      "step": 74990
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.36204496026039124,
      "learning_rate": 2.5e-05,
      "loss": 0.0014,
      "step": 75000
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.0020735145080834627,
      "eval_runtime": 188.8158,
      "eval_samples_per_second": 1324.042,
      "eval_steps_per_second": 33.101,
      "step": 75000
    },
    {
      "epoch": 4.000533333333333,
      "grad_norm": 0.06284787505865097,
      "learning_rate": 2.4996666666666667e-05,
      "loss": 0.0016,
      "step": 75010
    },
    {
      "epoch": 4.0010666666666665,
      "grad_norm": 0.12196715176105499,
      "learning_rate": 2.4993333333333337e-05,
      "loss": 0.0023,
      "step": 75020
    },
    {
      "epoch": 4.0016,
      "grad_norm": 0.11966599524021149,
      "learning_rate": 2.4990000000000003e-05,
      "loss": 0.0015,
      "step": 75030
    },
    {
      "epoch": 4.002133333333333,
      "grad_norm": 0.09460132569074631,
      "learning_rate": 2.4986666666666666e-05,
      "loss": 0.0015,
      "step": 75040
    },
    {
      "epoch": 4.002666666666666,
      "grad_norm": 0.3768683075904846,
      "learning_rate": 2.4983333333333335e-05,
      "loss": 0.0016,
      "step": 75050
    },
    {
      "epoch": 4.0032,
      "grad_norm": 0.01994473673403263,
      "learning_rate": 2.498e-05,
      "loss": 0.0023,
      "step": 75060
    },
    {
      "epoch": 4.003733333333333,
      "grad_norm": 0.17328166961669922,
      "learning_rate": 2.4976666666666668e-05,
      "loss": 0.003,
      "step": 75070
    },
    {
      "epoch": 4.004266666666667,
      "grad_norm": 0.38495099544525146,
      "learning_rate": 2.4973333333333334e-05,
      "loss": 0.0025,
      "step": 75080
    },
    {
      "epoch": 4.0048,
      "grad_norm": 0.11704551428556442,
      "learning_rate": 2.4970000000000003e-05,
      "loss": 0.002,
      "step": 75090
    },
    {
      "epoch": 4.005333333333334,
      "grad_norm": 0.2090016007423401,
      "learning_rate": 2.496666666666667e-05,
      "loss": 0.0028,
      "step": 75100
    },
    {
      "epoch": 4.005866666666667,
      "grad_norm": 0.08938439935445786,
      "learning_rate": 2.4963333333333335e-05,
      "loss": 0.0025,
      "step": 75110
    },
    {
      "epoch": 4.0064,
      "grad_norm": 0.264085590839386,
      "learning_rate": 2.496e-05,
      "loss": 0.0023,
      "step": 75120
    },
    {
      "epoch": 4.0069333333333335,
      "grad_norm": 0.41250890493392944,
      "learning_rate": 2.4956666666666668e-05,
      "loss": 0.0022,
      "step": 75130
    },
    {
      "epoch": 4.007466666666667,
      "grad_norm": 0.08887890726327896,
      "learning_rate": 2.4953333333333334e-05,
      "loss": 0.0022,
      "step": 75140
    },
    {
      "epoch": 4.008,
      "grad_norm": 0.21058523654937744,
      "learning_rate": 2.495e-05,
      "loss": 0.0019,
      "step": 75150
    },
    {
      "epoch": 4.008533333333333,
      "grad_norm": 0.4992685317993164,
      "learning_rate": 2.494666666666667e-05,
      "loss": 0.0022,
      "step": 75160
    },
    {
      "epoch": 4.009066666666667,
      "grad_norm": 0.04733071103692055,
      "learning_rate": 2.4943333333333336e-05,
      "loss": 0.0023,
      "step": 75170
    },
    {
      "epoch": 4.0096,
      "grad_norm": 0.23518289625644684,
      "learning_rate": 2.4940000000000002e-05,
      "loss": 0.0018,
      "step": 75180
    },
    {
      "epoch": 4.010133333333333,
      "grad_norm": 0.24619793891906738,
      "learning_rate": 2.4936666666666668e-05,
      "loss": 0.0018,
      "step": 75190
    },
    {
      "epoch": 4.010666666666666,
      "grad_norm": 0.292202353477478,
      "learning_rate": 2.4933333333333334e-05,
      "loss": 0.0023,
      "step": 75200
    },
    {
      "epoch": 4.0112,
      "grad_norm": 0.23601707816123962,
      "learning_rate": 2.493e-05,
      "loss": 0.002,
      "step": 75210
    },
    {
      "epoch": 4.011733333333333,
      "grad_norm": 0.08840242028236389,
      "learning_rate": 2.4926666666666666e-05,
      "loss": 0.0024,
      "step": 75220
    },
    {
      "epoch": 4.012266666666667,
      "grad_norm": 0.26883411407470703,
      "learning_rate": 2.4923333333333336e-05,
      "loss": 0.0023,
      "step": 75230
    },
    {
      "epoch": 4.0128,
      "grad_norm": 0.5540454983711243,
      "learning_rate": 2.4920000000000002e-05,
      "loss": 0.0018,
      "step": 75240
    },
    {
      "epoch": 4.013333333333334,
      "grad_norm": 0.3270900547504425,
      "learning_rate": 2.4916666666666668e-05,
      "loss": 0.0024,
      "step": 75250
    },
    {
      "epoch": 4.013866666666667,
      "grad_norm": 0.09691960364580154,
      "learning_rate": 2.4913333333333334e-05,
      "loss": 0.0019,
      "step": 75260
    },
    {
      "epoch": 4.0144,
      "grad_norm": 0.034103795886039734,
      "learning_rate": 2.491e-05,
      "loss": 0.0014,
      "step": 75270
    },
    {
      "epoch": 4.0149333333333335,
      "grad_norm": 0.13570000231266022,
      "learning_rate": 2.4906666666666666e-05,
      "loss": 0.0024,
      "step": 75280
    },
    {
      "epoch": 4.015466666666667,
      "grad_norm": 0.2636396884918213,
      "learning_rate": 2.4903333333333333e-05,
      "loss": 0.0013,
      "step": 75290
    },
    {
      "epoch": 4.016,
      "grad_norm": 0.3929413855075836,
      "learning_rate": 2.4900000000000002e-05,
      "loss": 0.0025,
      "step": 75300
    },
    {
      "epoch": 4.016533333333333,
      "grad_norm": 0.26661932468414307,
      "learning_rate": 2.4896666666666668e-05,
      "loss": 0.0022,
      "step": 75310
    },
    {
      "epoch": 4.017066666666667,
      "grad_norm": 0.04352045804262161,
      "learning_rate": 2.4893333333333334e-05,
      "loss": 0.0018,
      "step": 75320
    },
    {
      "epoch": 4.0176,
      "grad_norm": 0.29299476742744446,
      "learning_rate": 2.489e-05,
      "loss": 0.0019,
      "step": 75330
    },
    {
      "epoch": 4.018133333333333,
      "grad_norm": 0.29656893014907837,
      "learning_rate": 2.488666666666667e-05,
      "loss": 0.0018,
      "step": 75340
    },
    {
      "epoch": 4.018666666666666,
      "grad_norm": 0.09243535250425339,
      "learning_rate": 2.4883333333333333e-05,
      "loss": 0.0015,
      "step": 75350
    },
    {
      "epoch": 4.0192,
      "grad_norm": 0.5008482336997986,
      "learning_rate": 2.488e-05,
      "loss": 0.0017,
      "step": 75360
    },
    {
      "epoch": 4.019733333333333,
      "grad_norm": 0.3564748764038086,
      "learning_rate": 2.487666666666667e-05,
      "loss": 0.0022,
      "step": 75370
    },
    {
      "epoch": 4.020266666666667,
      "grad_norm": 0.40777042508125305,
      "learning_rate": 2.4873333333333335e-05,
      "loss": 0.0015,
      "step": 75380
    },
    {
      "epoch": 4.0208,
      "grad_norm": 0.2921237349510193,
      "learning_rate": 2.487e-05,
      "loss": 0.0016,
      "step": 75390
    },
    {
      "epoch": 4.021333333333334,
      "grad_norm": 0.3595379889011383,
      "learning_rate": 2.486666666666667e-05,
      "loss": 0.0019,
      "step": 75400
    },
    {
      "epoch": 4.021866666666667,
      "grad_norm": 0.20475700497627258,
      "learning_rate": 2.4863333333333336e-05,
      "loss": 0.0016,
      "step": 75410
    },
    {
      "epoch": 4.0224,
      "grad_norm": 0.04523061215877533,
      "learning_rate": 2.486e-05,
      "loss": 0.0025,
      "step": 75420
    },
    {
      "epoch": 4.0229333333333335,
      "grad_norm": 0.04901372268795967,
      "learning_rate": 2.485666666666667e-05,
      "loss": 0.0035,
      "step": 75430
    },
    {
      "epoch": 4.023466666666667,
      "grad_norm": 0.2159261852502823,
      "learning_rate": 2.4853333333333335e-05,
      "loss": 0.0021,
      "step": 75440
    },
    {
      "epoch": 4.024,
      "grad_norm": 0.2440629005432129,
      "learning_rate": 2.485e-05,
      "loss": 0.002,
      "step": 75450
    },
    {
      "epoch": 4.024533333333333,
      "grad_norm": 0.3888937830924988,
      "learning_rate": 2.4846666666666667e-05,
      "loss": 0.0024,
      "step": 75460
    },
    {
      "epoch": 4.025066666666667,
      "grad_norm": 0.035968340933322906,
      "learning_rate": 2.4843333333333337e-05,
      "loss": 0.0016,
      "step": 75470
    },
    {
      "epoch": 4.0256,
      "grad_norm": 0.0367862693965435,
      "learning_rate": 2.4840000000000003e-05,
      "loss": 0.0019,
      "step": 75480
    },
    {
      "epoch": 4.026133333333333,
      "grad_norm": 0.20903874933719635,
      "learning_rate": 2.483666666666667e-05,
      "loss": 0.0024,
      "step": 75490
    },
    {
      "epoch": 4.026666666666666,
      "grad_norm": 0.3510160744190216,
      "learning_rate": 2.4833333333333335e-05,
      "loss": 0.0022,
      "step": 75500
    },
    {
      "epoch": 4.0272,
      "grad_norm": 0.3658888339996338,
      "learning_rate": 2.483e-05,
      "loss": 0.0021,
      "step": 75510
    },
    {
      "epoch": 4.027733333333333,
      "grad_norm": 0.354734867811203,
      "learning_rate": 2.4826666666666667e-05,
      "loss": 0.0016,
      "step": 75520
    },
    {
      "epoch": 4.028266666666667,
      "grad_norm": 0.04621979221701622,
      "learning_rate": 2.4823333333333333e-05,
      "loss": 0.0026,
      "step": 75530
    },
    {
      "epoch": 4.0288,
      "grad_norm": 0.17758849263191223,
      "learning_rate": 2.4820000000000003e-05,
      "loss": 0.0024,
      "step": 75540
    },
    {
      "epoch": 4.029333333333334,
      "grad_norm": 0.17649368941783905,
      "learning_rate": 2.481666666666667e-05,
      "loss": 0.0017,
      "step": 75550
    },
    {
      "epoch": 4.029866666666667,
      "grad_norm": 0.06686490774154663,
      "learning_rate": 2.4813333333333335e-05,
      "loss": 0.002,
      "step": 75560
    },
    {
      "epoch": 4.0304,
      "grad_norm": 0.3336789309978485,
      "learning_rate": 2.481e-05,
      "loss": 0.0012,
      "step": 75570
    },
    {
      "epoch": 4.0309333333333335,
      "grad_norm": 0.3726807236671448,
      "learning_rate": 2.4806666666666667e-05,
      "loss": 0.0028,
      "step": 75580
    },
    {
      "epoch": 4.031466666666667,
      "grad_norm": 0.1646723747253418,
      "learning_rate": 2.4803333333333334e-05,
      "loss": 0.0021,
      "step": 75590
    },
    {
      "epoch": 4.032,
      "grad_norm": 0.262306809425354,
      "learning_rate": 2.48e-05,
      "loss": 0.0017,
      "step": 75600
    },
    {
      "epoch": 4.032533333333333,
      "grad_norm": 0.41594424843788147,
      "learning_rate": 2.479666666666667e-05,
      "loss": 0.0019,
      "step": 75610
    },
    {
      "epoch": 4.033066666666667,
      "grad_norm": 0.15664951503276825,
      "learning_rate": 2.4793333333333335e-05,
      "loss": 0.002,
      "step": 75620
    },
    {
      "epoch": 4.0336,
      "grad_norm": 0.47693637013435364,
      "learning_rate": 2.479e-05,
      "loss": 0.0029,
      "step": 75630
    },
    {
      "epoch": 4.034133333333333,
      "grad_norm": 0.3249390125274658,
      "learning_rate": 2.4786666666666668e-05,
      "loss": 0.0024,
      "step": 75640
    },
    {
      "epoch": 4.034666666666666,
      "grad_norm": 0.34908610582351685,
      "learning_rate": 2.4783333333333334e-05,
      "loss": 0.0018,
      "step": 75650
    },
    {
      "epoch": 4.0352,
      "grad_norm": 0.05723230168223381,
      "learning_rate": 2.478e-05,
      "loss": 0.002,
      "step": 75660
    },
    {
      "epoch": 4.035733333333333,
      "grad_norm": 0.4082096517086029,
      "learning_rate": 2.4776666666666666e-05,
      "loss": 0.0021,
      "step": 75670
    },
    {
      "epoch": 4.036266666666666,
      "grad_norm": 0.3207431733608246,
      "learning_rate": 2.4773333333333336e-05,
      "loss": 0.0015,
      "step": 75680
    },
    {
      "epoch": 4.0368,
      "grad_norm": 0.23600119352340698,
      "learning_rate": 2.4770000000000002e-05,
      "loss": 0.0023,
      "step": 75690
    },
    {
      "epoch": 4.037333333333334,
      "grad_norm": 0.1499830186367035,
      "learning_rate": 2.4766666666666668e-05,
      "loss": 0.0016,
      "step": 75700
    },
    {
      "epoch": 4.037866666666667,
      "grad_norm": 0.2346080243587494,
      "learning_rate": 2.4763333333333334e-05,
      "loss": 0.0017,
      "step": 75710
    },
    {
      "epoch": 4.0384,
      "grad_norm": 0.18848957121372223,
      "learning_rate": 2.476e-05,
      "loss": 0.002,
      "step": 75720
    },
    {
      "epoch": 4.0389333333333335,
      "grad_norm": 0.06252404302358627,
      "learning_rate": 2.4756666666666666e-05,
      "loss": 0.0014,
      "step": 75730
    },
    {
      "epoch": 4.039466666666667,
      "grad_norm": 0.20432846248149872,
      "learning_rate": 2.4753333333333332e-05,
      "loss": 0.0022,
      "step": 75740
    },
    {
      "epoch": 4.04,
      "grad_norm": 0.14180704951286316,
      "learning_rate": 2.4750000000000002e-05,
      "loss": 0.0019,
      "step": 75750
    },
    {
      "epoch": 4.040533333333333,
      "grad_norm": 0.046585146337747574,
      "learning_rate": 2.4746666666666668e-05,
      "loss": 0.0022,
      "step": 75760
    },
    {
      "epoch": 4.041066666666667,
      "grad_norm": 0.5214362740516663,
      "learning_rate": 2.4743333333333334e-05,
      "loss": 0.0018,
      "step": 75770
    },
    {
      "epoch": 4.0416,
      "grad_norm": 0.4819670617580414,
      "learning_rate": 2.4740000000000004e-05,
      "loss": 0.0027,
      "step": 75780
    },
    {
      "epoch": 4.042133333333333,
      "grad_norm": 0.261687308549881,
      "learning_rate": 2.473666666666667e-05,
      "loss": 0.0024,
      "step": 75790
    },
    {
      "epoch": 4.042666666666666,
      "grad_norm": 0.43968522548675537,
      "learning_rate": 2.4733333333333333e-05,
      "loss": 0.0015,
      "step": 75800
    },
    {
      "epoch": 4.0432,
      "grad_norm": 0.1145065501332283,
      "learning_rate": 2.473e-05,
      "loss": 0.0019,
      "step": 75810
    },
    {
      "epoch": 4.043733333333333,
      "grad_norm": 0.26678362488746643,
      "learning_rate": 2.4726666666666668e-05,
      "loss": 0.0022,
      "step": 75820
    },
    {
      "epoch": 4.044266666666666,
      "grad_norm": 0.019773108884692192,
      "learning_rate": 2.4723333333333334e-05,
      "loss": 0.0018,
      "step": 75830
    },
    {
      "epoch": 4.0448,
      "grad_norm": 0.38052695989608765,
      "learning_rate": 2.472e-05,
      "loss": 0.0019,
      "step": 75840
    },
    {
      "epoch": 4.045333333333334,
      "grad_norm": 0.29996538162231445,
      "learning_rate": 2.471666666666667e-05,
      "loss": 0.002,
      "step": 75850
    },
    {
      "epoch": 4.045866666666667,
      "grad_norm": 0.2979598939418793,
      "learning_rate": 2.4713333333333336e-05,
      "loss": 0.0013,
      "step": 75860
    },
    {
      "epoch": 4.0464,
      "grad_norm": 0.18001538515090942,
      "learning_rate": 2.471e-05,
      "loss": 0.002,
      "step": 75870
    },
    {
      "epoch": 4.0469333333333335,
      "grad_norm": 0.35577428340911865,
      "learning_rate": 2.470666666666667e-05,
      "loss": 0.0025,
      "step": 75880
    },
    {
      "epoch": 4.047466666666667,
      "grad_norm": 0.19633162021636963,
      "learning_rate": 2.4703333333333335e-05,
      "loss": 0.0024,
      "step": 75890
    },
    {
      "epoch": 4.048,
      "grad_norm": 0.12203136086463928,
      "learning_rate": 2.47e-05,
      "loss": 0.0013,
      "step": 75900
    },
    {
      "epoch": 4.048533333333333,
      "grad_norm": 0.44617128372192383,
      "learning_rate": 2.4696666666666667e-05,
      "loss": 0.0016,
      "step": 75910
    },
    {
      "epoch": 4.049066666666667,
      "grad_norm": 0.2208249270915985,
      "learning_rate": 2.4693333333333336e-05,
      "loss": 0.0019,
      "step": 75920
    },
    {
      "epoch": 4.0496,
      "grad_norm": 0.29512590169906616,
      "learning_rate": 2.4690000000000002e-05,
      "loss": 0.0021,
      "step": 75930
    },
    {
      "epoch": 4.050133333333333,
      "grad_norm": 0.2690694332122803,
      "learning_rate": 2.468666666666667e-05,
      "loss": 0.0016,
      "step": 75940
    },
    {
      "epoch": 4.050666666666666,
      "grad_norm": 0.36617591977119446,
      "learning_rate": 2.4683333333333335e-05,
      "loss": 0.0023,
      "step": 75950
    },
    {
      "epoch": 4.0512,
      "grad_norm": 0.06670384109020233,
      "learning_rate": 2.468e-05,
      "loss": 0.0027,
      "step": 75960
    },
    {
      "epoch": 4.051733333333333,
      "grad_norm": 0.17795296013355255,
      "learning_rate": 2.4676666666666667e-05,
      "loss": 0.0019,
      "step": 75970
    },
    {
      "epoch": 4.052266666666666,
      "grad_norm": 0.3499678075313568,
      "learning_rate": 2.4673333333333333e-05,
      "loss": 0.0032,
      "step": 75980
    },
    {
      "epoch": 4.0528,
      "grad_norm": 0.07461769878864288,
      "learning_rate": 2.4670000000000003e-05,
      "loss": 0.0018,
      "step": 75990
    },
    {
      "epoch": 4.053333333333334,
      "grad_norm": 0.05499355494976044,
      "learning_rate": 2.466666666666667e-05,
      "loss": 0.0016,
      "step": 76000
    },
    {
      "epoch": 4.053866666666667,
      "grad_norm": 0.07295658439397812,
      "learning_rate": 2.4663333333333335e-05,
      "loss": 0.0017,
      "step": 76010
    },
    {
      "epoch": 4.0544,
      "grad_norm": 0.06830695271492004,
      "learning_rate": 2.466e-05,
      "loss": 0.0019,
      "step": 76020
    },
    {
      "epoch": 4.0549333333333335,
      "grad_norm": 0.4112723171710968,
      "learning_rate": 2.4656666666666667e-05,
      "loss": 0.002,
      "step": 76030
    },
    {
      "epoch": 4.055466666666667,
      "grad_norm": 0.5321478247642517,
      "learning_rate": 2.4653333333333333e-05,
      "loss": 0.0024,
      "step": 76040
    },
    {
      "epoch": 4.056,
      "grad_norm": 0.18451575934886932,
      "learning_rate": 2.465e-05,
      "loss": 0.0022,
      "step": 76050
    },
    {
      "epoch": 4.056533333333333,
      "grad_norm": 0.1987619400024414,
      "learning_rate": 2.464666666666667e-05,
      "loss": 0.0016,
      "step": 76060
    },
    {
      "epoch": 4.057066666666667,
      "grad_norm": 0.172923281788826,
      "learning_rate": 2.4643333333333335e-05,
      "loss": 0.0018,
      "step": 76070
    },
    {
      "epoch": 4.0576,
      "grad_norm": 0.05014088377356529,
      "learning_rate": 2.464e-05,
      "loss": 0.0022,
      "step": 76080
    },
    {
      "epoch": 4.058133333333333,
      "grad_norm": 0.40911340713500977,
      "learning_rate": 2.4636666666666667e-05,
      "loss": 0.0021,
      "step": 76090
    },
    {
      "epoch": 4.058666666666666,
      "grad_norm": 0.1844429075717926,
      "learning_rate": 2.4633333333333334e-05,
      "loss": 0.0024,
      "step": 76100
    },
    {
      "epoch": 4.0592,
      "grad_norm": 0.10073185712099075,
      "learning_rate": 2.463e-05,
      "loss": 0.0016,
      "step": 76110
    },
    {
      "epoch": 4.059733333333333,
      "grad_norm": 0.5061472654342651,
      "learning_rate": 2.4626666666666666e-05,
      "loss": 0.0027,
      "step": 76120
    },
    {
      "epoch": 4.060266666666666,
      "grad_norm": 0.26293015480041504,
      "learning_rate": 2.4623333333333335e-05,
      "loss": 0.0018,
      "step": 76130
    },
    {
      "epoch": 4.0608,
      "grad_norm": 0.29112571477890015,
      "learning_rate": 2.462e-05,
      "loss": 0.0024,
      "step": 76140
    },
    {
      "epoch": 4.061333333333334,
      "grad_norm": 0.3192853033542633,
      "learning_rate": 2.4616666666666668e-05,
      "loss": 0.0015,
      "step": 76150
    },
    {
      "epoch": 4.061866666666667,
      "grad_norm": 0.660018265247345,
      "learning_rate": 2.4613333333333337e-05,
      "loss": 0.0021,
      "step": 76160
    },
    {
      "epoch": 4.0624,
      "grad_norm": 0.15497276186943054,
      "learning_rate": 2.4610000000000003e-05,
      "loss": 0.0016,
      "step": 76170
    },
    {
      "epoch": 4.0629333333333335,
      "grad_norm": 0.30273622274398804,
      "learning_rate": 2.4606666666666666e-05,
      "loss": 0.0016,
      "step": 76180
    },
    {
      "epoch": 4.063466666666667,
      "grad_norm": 0.37808969616889954,
      "learning_rate": 2.4603333333333332e-05,
      "loss": 0.0019,
      "step": 76190
    },
    {
      "epoch": 4.064,
      "grad_norm": 0.7295058369636536,
      "learning_rate": 2.46e-05,
      "loss": 0.0018,
      "step": 76200
    },
    {
      "epoch": 4.064533333333333,
      "grad_norm": 0.159381702542305,
      "learning_rate": 2.4596666666666668e-05,
      "loss": 0.0017,
      "step": 76210
    },
    {
      "epoch": 4.065066666666667,
      "grad_norm": 0.05974658951163292,
      "learning_rate": 2.4593333333333334e-05,
      "loss": 0.0016,
      "step": 76220
    },
    {
      "epoch": 4.0656,
      "grad_norm": 0.03787516430020332,
      "learning_rate": 2.4590000000000003e-05,
      "loss": 0.0029,
      "step": 76230
    },
    {
      "epoch": 4.066133333333333,
      "grad_norm": 0.26517972350120544,
      "learning_rate": 2.458666666666667e-05,
      "loss": 0.0021,
      "step": 76240
    },
    {
      "epoch": 4.066666666666666,
      "grad_norm": 0.08575348556041718,
      "learning_rate": 2.4583333333333332e-05,
      "loss": 0.0018,
      "step": 76250
    },
    {
      "epoch": 4.0672,
      "grad_norm": 0.09275121241807938,
      "learning_rate": 2.4580000000000002e-05,
      "loss": 0.0023,
      "step": 76260
    },
    {
      "epoch": 4.067733333333333,
      "grad_norm": 0.5601367354393005,
      "learning_rate": 2.4576666666666668e-05,
      "loss": 0.0019,
      "step": 76270
    },
    {
      "epoch": 4.068266666666666,
      "grad_norm": 0.14549338817596436,
      "learning_rate": 2.4573333333333334e-05,
      "loss": 0.0018,
      "step": 76280
    },
    {
      "epoch": 4.0688,
      "grad_norm": 0.5769734978675842,
      "learning_rate": 2.457e-05,
      "loss": 0.0019,
      "step": 76290
    },
    {
      "epoch": 4.069333333333334,
      "grad_norm": 0.3851478397846222,
      "learning_rate": 2.456666666666667e-05,
      "loss": 0.0023,
      "step": 76300
    },
    {
      "epoch": 4.069866666666667,
      "grad_norm": 0.4714025855064392,
      "learning_rate": 2.4563333333333336e-05,
      "loss": 0.0019,
      "step": 76310
    },
    {
      "epoch": 4.0704,
      "grad_norm": 0.438113808631897,
      "learning_rate": 2.4560000000000002e-05,
      "loss": 0.0022,
      "step": 76320
    },
    {
      "epoch": 4.0709333333333335,
      "grad_norm": 0.4410409927368164,
      "learning_rate": 2.4556666666666668e-05,
      "loss": 0.0025,
      "step": 76330
    },
    {
      "epoch": 4.071466666666667,
      "grad_norm": 0.5596534609794617,
      "learning_rate": 2.4553333333333334e-05,
      "loss": 0.0031,
      "step": 76340
    },
    {
      "epoch": 4.072,
      "grad_norm": 0.3229239583015442,
      "learning_rate": 2.455e-05,
      "loss": 0.0023,
      "step": 76350
    },
    {
      "epoch": 4.072533333333333,
      "grad_norm": 0.4692606031894684,
      "learning_rate": 2.4546666666666667e-05,
      "loss": 0.0019,
      "step": 76360
    },
    {
      "epoch": 4.073066666666667,
      "grad_norm": 0.5333595275878906,
      "learning_rate": 2.4543333333333336e-05,
      "loss": 0.0017,
      "step": 76370
    },
    {
      "epoch": 4.0736,
      "grad_norm": 0.2634185254573822,
      "learning_rate": 2.4540000000000002e-05,
      "loss": 0.0025,
      "step": 76380
    },
    {
      "epoch": 4.074133333333333,
      "grad_norm": 0.04772591218352318,
      "learning_rate": 2.453666666666667e-05,
      "loss": 0.0018,
      "step": 76390
    },
    {
      "epoch": 4.074666666666666,
      "grad_norm": 0.6287301778793335,
      "learning_rate": 2.4533333333333334e-05,
      "loss": 0.002,
      "step": 76400
    },
    {
      "epoch": 4.0752,
      "grad_norm": 0.6501840353012085,
      "learning_rate": 2.453e-05,
      "loss": 0.0022,
      "step": 76410
    },
    {
      "epoch": 4.075733333333333,
      "grad_norm": 0.18346333503723145,
      "learning_rate": 2.4526666666666667e-05,
      "loss": 0.0025,
      "step": 76420
    },
    {
      "epoch": 4.076266666666666,
      "grad_norm": 0.21057219803333282,
      "learning_rate": 2.4523333333333333e-05,
      "loss": 0.0022,
      "step": 76430
    },
    {
      "epoch": 4.0768,
      "grad_norm": 0.24118803441524506,
      "learning_rate": 2.4520000000000002e-05,
      "loss": 0.0026,
      "step": 76440
    },
    {
      "epoch": 4.077333333333334,
      "grad_norm": 0.20264099538326263,
      "learning_rate": 2.451666666666667e-05,
      "loss": 0.0027,
      "step": 76450
    },
    {
      "epoch": 4.077866666666667,
      "grad_norm": 0.2322409451007843,
      "learning_rate": 2.4513333333333335e-05,
      "loss": 0.0018,
      "step": 76460
    },
    {
      "epoch": 4.0784,
      "grad_norm": 0.23348228633403778,
      "learning_rate": 2.451e-05,
      "loss": 0.0014,
      "step": 76470
    },
    {
      "epoch": 4.0789333333333335,
      "grad_norm": 0.12937399744987488,
      "learning_rate": 2.4506666666666667e-05,
      "loss": 0.002,
      "step": 76480
    },
    {
      "epoch": 4.079466666666667,
      "grad_norm": 0.06413234770298004,
      "learning_rate": 2.4503333333333333e-05,
      "loss": 0.0019,
      "step": 76490
    },
    {
      "epoch": 4.08,
      "grad_norm": 0.3291781544685364,
      "learning_rate": 2.45e-05,
      "loss": 0.0018,
      "step": 76500
    },
    {
      "epoch": 4.080533333333333,
      "grad_norm": 0.1720750778913498,
      "learning_rate": 2.449666666666667e-05,
      "loss": 0.002,
      "step": 76510
    },
    {
      "epoch": 4.081066666666667,
      "grad_norm": 0.25858813524246216,
      "learning_rate": 2.4493333333333335e-05,
      "loss": 0.0017,
      "step": 76520
    },
    {
      "epoch": 4.0816,
      "grad_norm": 0.23118694126605988,
      "learning_rate": 2.449e-05,
      "loss": 0.002,
      "step": 76530
    },
    {
      "epoch": 4.082133333333333,
      "grad_norm": 0.3517901599407196,
      "learning_rate": 2.448666666666667e-05,
      "loss": 0.0022,
      "step": 76540
    },
    {
      "epoch": 4.082666666666666,
      "grad_norm": 0.11826223880052567,
      "learning_rate": 2.4483333333333333e-05,
      "loss": 0.0017,
      "step": 76550
    },
    {
      "epoch": 4.0832,
      "grad_norm": 0.3516797721385956,
      "learning_rate": 2.448e-05,
      "loss": 0.0014,
      "step": 76560
    },
    {
      "epoch": 4.083733333333333,
      "grad_norm": 0.3601236641407013,
      "learning_rate": 2.4476666666666666e-05,
      "loss": 0.0023,
      "step": 76570
    },
    {
      "epoch": 4.084266666666666,
      "grad_norm": 0.17687778174877167,
      "learning_rate": 2.4473333333333335e-05,
      "loss": 0.0028,
      "step": 76580
    },
    {
      "epoch": 4.0848,
      "grad_norm": 0.12201913446187973,
      "learning_rate": 2.447e-05,
      "loss": 0.0018,
      "step": 76590
    },
    {
      "epoch": 4.085333333333334,
      "grad_norm": 0.2610623240470886,
      "learning_rate": 2.4466666666666667e-05,
      "loss": 0.002,
      "step": 76600
    },
    {
      "epoch": 4.085866666666667,
      "grad_norm": 0.27432310581207275,
      "learning_rate": 2.4463333333333337e-05,
      "loss": 0.0021,
      "step": 76610
    },
    {
      "epoch": 4.0864,
      "grad_norm": 0.03965189680457115,
      "learning_rate": 2.4460000000000003e-05,
      "loss": 0.0022,
      "step": 76620
    },
    {
      "epoch": 4.0869333333333335,
      "grad_norm": 0.03175061196088791,
      "learning_rate": 2.4456666666666666e-05,
      "loss": 0.0017,
      "step": 76630
    },
    {
      "epoch": 4.087466666666667,
      "grad_norm": 0.38989824056625366,
      "learning_rate": 2.4453333333333335e-05,
      "loss": 0.0015,
      "step": 76640
    },
    {
      "epoch": 4.088,
      "grad_norm": 0.11955329775810242,
      "learning_rate": 2.445e-05,
      "loss": 0.0018,
      "step": 76650
    },
    {
      "epoch": 4.088533333333333,
      "grad_norm": 0.26685699820518494,
      "learning_rate": 2.4446666666666668e-05,
      "loss": 0.0018,
      "step": 76660
    },
    {
      "epoch": 4.089066666666667,
      "grad_norm": 0.24585656821727753,
      "learning_rate": 2.4443333333333334e-05,
      "loss": 0.0021,
      "step": 76670
    },
    {
      "epoch": 4.0896,
      "grad_norm": 0.12446144223213196,
      "learning_rate": 2.4440000000000003e-05,
      "loss": 0.0024,
      "step": 76680
    },
    {
      "epoch": 4.090133333333333,
      "grad_norm": 0.5235876441001892,
      "learning_rate": 2.443666666666667e-05,
      "loss": 0.0023,
      "step": 76690
    },
    {
      "epoch": 4.0906666666666665,
      "grad_norm": 0.12107688188552856,
      "learning_rate": 2.4433333333333335e-05,
      "loss": 0.0028,
      "step": 76700
    },
    {
      "epoch": 4.0912,
      "grad_norm": 0.2642315626144409,
      "learning_rate": 2.443e-05,
      "loss": 0.0015,
      "step": 76710
    },
    {
      "epoch": 4.091733333333333,
      "grad_norm": 0.07382149249315262,
      "learning_rate": 2.4426666666666668e-05,
      "loss": 0.0013,
      "step": 76720
    },
    {
      "epoch": 4.092266666666666,
      "grad_norm": 0.11996778100728989,
      "learning_rate": 2.4423333333333334e-05,
      "loss": 0.0022,
      "step": 76730
    },
    {
      "epoch": 4.0928,
      "grad_norm": 0.1710054725408554,
      "learning_rate": 2.442e-05,
      "loss": 0.0025,
      "step": 76740
    },
    {
      "epoch": 4.093333333333334,
      "grad_norm": 0.17856574058532715,
      "learning_rate": 2.441666666666667e-05,
      "loss": 0.0022,
      "step": 76750
    },
    {
      "epoch": 4.093866666666667,
      "grad_norm": 0.271812379360199,
      "learning_rate": 2.4413333333333336e-05,
      "loss": 0.0022,
      "step": 76760
    },
    {
      "epoch": 4.0944,
      "grad_norm": 0.2881689965724945,
      "learning_rate": 2.4410000000000002e-05,
      "loss": 0.0011,
      "step": 76770
    },
    {
      "epoch": 4.0949333333333335,
      "grad_norm": 0.2959139347076416,
      "learning_rate": 2.4406666666666668e-05,
      "loss": 0.0021,
      "step": 76780
    },
    {
      "epoch": 4.095466666666667,
      "grad_norm": 0.47118663787841797,
      "learning_rate": 2.4403333333333334e-05,
      "loss": 0.002,
      "step": 76790
    },
    {
      "epoch": 4.096,
      "grad_norm": 0.1780785322189331,
      "learning_rate": 2.44e-05,
      "loss": 0.0019,
      "step": 76800
    },
    {
      "epoch": 4.096533333333333,
      "grad_norm": 0.49886196851730347,
      "learning_rate": 2.4396666666666666e-05,
      "loss": 0.0017,
      "step": 76810
    },
    {
      "epoch": 4.097066666666667,
      "grad_norm": 0.3284021317958832,
      "learning_rate": 2.4393333333333336e-05,
      "loss": 0.0017,
      "step": 76820
    },
    {
      "epoch": 4.0976,
      "grad_norm": 0.39609432220458984,
      "learning_rate": 2.4390000000000002e-05,
      "loss": 0.0023,
      "step": 76830
    },
    {
      "epoch": 4.098133333333333,
      "grad_norm": 0.3070943355560303,
      "learning_rate": 2.4386666666666668e-05,
      "loss": 0.002,
      "step": 76840
    },
    {
      "epoch": 4.0986666666666665,
      "grad_norm": 0.04790955409407616,
      "learning_rate": 2.4383333333333334e-05,
      "loss": 0.0016,
      "step": 76850
    },
    {
      "epoch": 4.0992,
      "grad_norm": 0.22979328036308289,
      "learning_rate": 2.438e-05,
      "loss": 0.0016,
      "step": 76860
    },
    {
      "epoch": 4.099733333333333,
      "grad_norm": 0.32611575722694397,
      "learning_rate": 2.4376666666666666e-05,
      "loss": 0.0031,
      "step": 76870
    },
    {
      "epoch": 4.100266666666666,
      "grad_norm": 0.12867896258831024,
      "learning_rate": 2.4373333333333333e-05,
      "loss": 0.0021,
      "step": 76880
    },
    {
      "epoch": 4.1008,
      "grad_norm": 0.20903664827346802,
      "learning_rate": 2.4370000000000002e-05,
      "loss": 0.002,
      "step": 76890
    },
    {
      "epoch": 4.101333333333334,
      "grad_norm": 0.20538057386875153,
      "learning_rate": 2.4366666666666668e-05,
      "loss": 0.002,
      "step": 76900
    },
    {
      "epoch": 4.101866666666667,
      "grad_norm": 0.21234776079654694,
      "learning_rate": 2.4363333333333334e-05,
      "loss": 0.0019,
      "step": 76910
    },
    {
      "epoch": 4.1024,
      "grad_norm": 0.06787475943565369,
      "learning_rate": 2.4360000000000004e-05,
      "loss": 0.003,
      "step": 76920
    },
    {
      "epoch": 4.1029333333333335,
      "grad_norm": 0.09343095123767853,
      "learning_rate": 2.4356666666666667e-05,
      "loss": 0.0023,
      "step": 76930
    },
    {
      "epoch": 4.103466666666667,
      "grad_norm": 0.09200412780046463,
      "learning_rate": 2.4353333333333333e-05,
      "loss": 0.0027,
      "step": 76940
    },
    {
      "epoch": 4.104,
      "grad_norm": 0.12736569344997406,
      "learning_rate": 2.435e-05,
      "loss": 0.0027,
      "step": 76950
    },
    {
      "epoch": 4.104533333333333,
      "grad_norm": 0.04735281318426132,
      "learning_rate": 2.434666666666667e-05,
      "loss": 0.0023,
      "step": 76960
    },
    {
      "epoch": 4.105066666666667,
      "grad_norm": 0.08063226193189621,
      "learning_rate": 2.4343333333333335e-05,
      "loss": 0.0016,
      "step": 76970
    },
    {
      "epoch": 4.1056,
      "grad_norm": 0.17602357268333435,
      "learning_rate": 2.434e-05,
      "loss": 0.0026,
      "step": 76980
    },
    {
      "epoch": 4.106133333333333,
      "grad_norm": 0.15557676553726196,
      "learning_rate": 2.433666666666667e-05,
      "loss": 0.0024,
      "step": 76990
    },
    {
      "epoch": 4.1066666666666665,
      "grad_norm": 0.7669005990028381,
      "learning_rate": 2.4333333333333336e-05,
      "loss": 0.0018,
      "step": 77000
    },
    {
      "epoch": 4.1072,
      "grad_norm": 0.15563485026359558,
      "learning_rate": 2.433e-05,
      "loss": 0.0018,
      "step": 77010
    },
    {
      "epoch": 4.107733333333333,
      "grad_norm": 0.12002959102392197,
      "learning_rate": 2.432666666666667e-05,
      "loss": 0.0018,
      "step": 77020
    },
    {
      "epoch": 4.108266666666666,
      "grad_norm": 0.21193726360797882,
      "learning_rate": 2.4323333333333335e-05,
      "loss": 0.0023,
      "step": 77030
    },
    {
      "epoch": 4.1088,
      "grad_norm": 0.08960552513599396,
      "learning_rate": 2.432e-05,
      "loss": 0.0016,
      "step": 77040
    },
    {
      "epoch": 4.109333333333334,
      "grad_norm": 0.1523464322090149,
      "learning_rate": 2.4316666666666667e-05,
      "loss": 0.0026,
      "step": 77050
    },
    {
      "epoch": 4.109866666666667,
      "grad_norm": 0.4045228064060211,
      "learning_rate": 2.4313333333333337e-05,
      "loss": 0.0021,
      "step": 77060
    },
    {
      "epoch": 4.1104,
      "grad_norm": 0.13231520354747772,
      "learning_rate": 2.4310000000000003e-05,
      "loss": 0.0021,
      "step": 77070
    },
    {
      "epoch": 4.1109333333333336,
      "grad_norm": 0.21100161969661713,
      "learning_rate": 2.4306666666666665e-05,
      "loss": 0.0017,
      "step": 77080
    },
    {
      "epoch": 4.111466666666667,
      "grad_norm": 0.05394129455089569,
      "learning_rate": 2.4303333333333335e-05,
      "loss": 0.0019,
      "step": 77090
    },
    {
      "epoch": 4.112,
      "grad_norm": 0.49145761132240295,
      "learning_rate": 2.43e-05,
      "loss": 0.0016,
      "step": 77100
    },
    {
      "epoch": 4.112533333333333,
      "grad_norm": 0.155469611287117,
      "learning_rate": 2.4296666666666667e-05,
      "loss": 0.0023,
      "step": 77110
    },
    {
      "epoch": 4.113066666666667,
      "grad_norm": 0.4479922652244568,
      "learning_rate": 2.4293333333333333e-05,
      "loss": 0.0022,
      "step": 77120
    },
    {
      "epoch": 4.1136,
      "grad_norm": 0.2347741574048996,
      "learning_rate": 2.4290000000000003e-05,
      "loss": 0.0023,
      "step": 77130
    },
    {
      "epoch": 4.114133333333333,
      "grad_norm": 0.6808809638023376,
      "learning_rate": 2.428666666666667e-05,
      "loss": 0.0024,
      "step": 77140
    },
    {
      "epoch": 4.1146666666666665,
      "grad_norm": 0.499053955078125,
      "learning_rate": 2.4283333333333335e-05,
      "loss": 0.0014,
      "step": 77150
    },
    {
      "epoch": 4.1152,
      "grad_norm": 0.33417069911956787,
      "learning_rate": 2.428e-05,
      "loss": 0.002,
      "step": 77160
    },
    {
      "epoch": 4.115733333333333,
      "grad_norm": 0.10033953189849854,
      "learning_rate": 2.4276666666666667e-05,
      "loss": 0.0029,
      "step": 77170
    },
    {
      "epoch": 4.116266666666666,
      "grad_norm": 0.04393990710377693,
      "learning_rate": 2.4273333333333334e-05,
      "loss": 0.002,
      "step": 77180
    },
    {
      "epoch": 4.1168,
      "grad_norm": 0.08308524638414383,
      "learning_rate": 2.427e-05,
      "loss": 0.0021,
      "step": 77190
    },
    {
      "epoch": 4.117333333333334,
      "grad_norm": 0.29487451910972595,
      "learning_rate": 2.426666666666667e-05,
      "loss": 0.0021,
      "step": 77200
    },
    {
      "epoch": 4.117866666666667,
      "grad_norm": 0.3873724043369293,
      "learning_rate": 2.4263333333333335e-05,
      "loss": 0.0012,
      "step": 77210
    },
    {
      "epoch": 4.1184,
      "grad_norm": 0.2986029088497162,
      "learning_rate": 2.426e-05,
      "loss": 0.0014,
      "step": 77220
    },
    {
      "epoch": 4.118933333333334,
      "grad_norm": 0.41990429162979126,
      "learning_rate": 2.4256666666666668e-05,
      "loss": 0.0022,
      "step": 77230
    },
    {
      "epoch": 4.119466666666667,
      "grad_norm": 0.2571464478969574,
      "learning_rate": 2.4253333333333334e-05,
      "loss": 0.0026,
      "step": 77240
    },
    {
      "epoch": 4.12,
      "grad_norm": 0.8705249428749084,
      "learning_rate": 2.425e-05,
      "loss": 0.0039,
      "step": 77250
    },
    {
      "epoch": 4.120533333333333,
      "grad_norm": 0.18319879472255707,
      "learning_rate": 2.4246666666666666e-05,
      "loss": 0.0028,
      "step": 77260
    },
    {
      "epoch": 4.121066666666667,
      "grad_norm": 0.4789142310619354,
      "learning_rate": 2.4243333333333336e-05,
      "loss": 0.0016,
      "step": 77270
    },
    {
      "epoch": 4.1216,
      "grad_norm": 0.5225195288658142,
      "learning_rate": 2.4240000000000002e-05,
      "loss": 0.0015,
      "step": 77280
    },
    {
      "epoch": 4.122133333333333,
      "grad_norm": 0.23575043678283691,
      "learning_rate": 2.4236666666666668e-05,
      "loss": 0.0016,
      "step": 77290
    },
    {
      "epoch": 4.1226666666666665,
      "grad_norm": 0.4518255591392517,
      "learning_rate": 2.4233333333333337e-05,
      "loss": 0.0019,
      "step": 77300
    },
    {
      "epoch": 4.1232,
      "grad_norm": 0.17570894956588745,
      "learning_rate": 2.423e-05,
      "loss": 0.0017,
      "step": 77310
    },
    {
      "epoch": 4.123733333333333,
      "grad_norm": 0.06814248859882355,
      "learning_rate": 2.4226666666666666e-05,
      "loss": 0.002,
      "step": 77320
    },
    {
      "epoch": 4.124266666666666,
      "grad_norm": 0.2921201288700104,
      "learning_rate": 2.4223333333333332e-05,
      "loss": 0.0019,
      "step": 77330
    },
    {
      "epoch": 4.1248,
      "grad_norm": 0.1780652552843094,
      "learning_rate": 2.4220000000000002e-05,
      "loss": 0.0031,
      "step": 77340
    },
    {
      "epoch": 4.125333333333334,
      "grad_norm": 0.5689050555229187,
      "learning_rate": 2.4216666666666668e-05,
      "loss": 0.0019,
      "step": 77350
    },
    {
      "epoch": 4.125866666666667,
      "grad_norm": 0.4943324327468872,
      "learning_rate": 2.4213333333333334e-05,
      "loss": 0.0023,
      "step": 77360
    },
    {
      "epoch": 4.1264,
      "grad_norm": 0.26259228587150574,
      "learning_rate": 2.4210000000000004e-05,
      "loss": 0.0017,
      "step": 77370
    },
    {
      "epoch": 4.126933333333334,
      "grad_norm": 0.2611025869846344,
      "learning_rate": 2.420666666666667e-05,
      "loss": 0.0017,
      "step": 77380
    },
    {
      "epoch": 4.127466666666667,
      "grad_norm": 0.03239966183900833,
      "learning_rate": 2.4203333333333333e-05,
      "loss": 0.0019,
      "step": 77390
    },
    {
      "epoch": 4.128,
      "grad_norm": 0.6641525030136108,
      "learning_rate": 2.4200000000000002e-05,
      "loss": 0.0021,
      "step": 77400
    },
    {
      "epoch": 4.128533333333333,
      "grad_norm": 0.26604515314102173,
      "learning_rate": 2.4196666666666668e-05,
      "loss": 0.0036,
      "step": 77410
    },
    {
      "epoch": 4.129066666666667,
      "grad_norm": 0.3148815929889679,
      "learning_rate": 2.4193333333333334e-05,
      "loss": 0.0021,
      "step": 77420
    },
    {
      "epoch": 4.1296,
      "grad_norm": 0.06776074320077896,
      "learning_rate": 2.419e-05,
      "loss": 0.002,
      "step": 77430
    },
    {
      "epoch": 4.130133333333333,
      "grad_norm": 0.12402857840061188,
      "learning_rate": 2.418666666666667e-05,
      "loss": 0.0024,
      "step": 77440
    },
    {
      "epoch": 4.1306666666666665,
      "grad_norm": 0.24659408628940582,
      "learning_rate": 2.4183333333333336e-05,
      "loss": 0.002,
      "step": 77450
    },
    {
      "epoch": 4.1312,
      "grad_norm": 0.10308251529932022,
      "learning_rate": 2.418e-05,
      "loss": 0.0017,
      "step": 77460
    },
    {
      "epoch": 4.131733333333333,
      "grad_norm": 0.12816943228244781,
      "learning_rate": 2.417666666666667e-05,
      "loss": 0.0017,
      "step": 77470
    },
    {
      "epoch": 4.132266666666666,
      "grad_norm": 0.16576287150382996,
      "learning_rate": 2.4173333333333335e-05,
      "loss": 0.0024,
      "step": 77480
    },
    {
      "epoch": 4.1328,
      "grad_norm": 0.3326144516468048,
      "learning_rate": 2.417e-05,
      "loss": 0.0021,
      "step": 77490
    },
    {
      "epoch": 4.133333333333334,
      "grad_norm": 0.07235059142112732,
      "learning_rate": 2.4166666666666667e-05,
      "loss": 0.0027,
      "step": 77500
    },
    {
      "epoch": 4.133866666666667,
      "grad_norm": 0.24037723243236542,
      "learning_rate": 2.4163333333333336e-05,
      "loss": 0.0027,
      "step": 77510
    },
    {
      "epoch": 4.1344,
      "grad_norm": 0.3809696137905121,
      "learning_rate": 2.4160000000000002e-05,
      "loss": 0.0014,
      "step": 77520
    },
    {
      "epoch": 4.134933333333334,
      "grad_norm": 0.2654542326927185,
      "learning_rate": 2.415666666666667e-05,
      "loss": 0.0019,
      "step": 77530
    },
    {
      "epoch": 4.135466666666667,
      "grad_norm": 0.1506342589855194,
      "learning_rate": 2.4153333333333335e-05,
      "loss": 0.0026,
      "step": 77540
    },
    {
      "epoch": 4.136,
      "grad_norm": 0.12584255635738373,
      "learning_rate": 2.415e-05,
      "loss": 0.002,
      "step": 77550
    },
    {
      "epoch": 4.136533333333333,
      "grad_norm": 0.19616085290908813,
      "learning_rate": 2.4146666666666667e-05,
      "loss": 0.002,
      "step": 77560
    },
    {
      "epoch": 4.137066666666667,
      "grad_norm": 0.0657755583524704,
      "learning_rate": 2.4143333333333333e-05,
      "loss": 0.0021,
      "step": 77570
    },
    {
      "epoch": 4.1376,
      "grad_norm": 0.06515849381685257,
      "learning_rate": 2.4140000000000003e-05,
      "loss": 0.0018,
      "step": 77580
    },
    {
      "epoch": 4.138133333333333,
      "grad_norm": 0.17283518612384796,
      "learning_rate": 2.413666666666667e-05,
      "loss": 0.0029,
      "step": 77590
    },
    {
      "epoch": 4.1386666666666665,
      "grad_norm": 0.6456667184829712,
      "learning_rate": 2.4133333333333335e-05,
      "loss": 0.002,
      "step": 77600
    },
    {
      "epoch": 4.1392,
      "grad_norm": 0.27953270077705383,
      "learning_rate": 2.413e-05,
      "loss": 0.0017,
      "step": 77610
    },
    {
      "epoch": 4.139733333333333,
      "grad_norm": 0.10042000561952591,
      "learning_rate": 2.4126666666666667e-05,
      "loss": 0.0022,
      "step": 77620
    },
    {
      "epoch": 4.140266666666666,
      "grad_norm": 0.23792321979999542,
      "learning_rate": 2.4123333333333333e-05,
      "loss": 0.0014,
      "step": 77630
    },
    {
      "epoch": 4.1408,
      "grad_norm": 0.3121374249458313,
      "learning_rate": 2.412e-05,
      "loss": 0.0024,
      "step": 77640
    },
    {
      "epoch": 4.141333333333334,
      "grad_norm": 0.17900535464286804,
      "learning_rate": 2.411666666666667e-05,
      "loss": 0.0015,
      "step": 77650
    },
    {
      "epoch": 4.141866666666667,
      "grad_norm": 0.5394569635391235,
      "learning_rate": 2.4113333333333335e-05,
      "loss": 0.0026,
      "step": 77660
    },
    {
      "epoch": 4.1424,
      "grad_norm": 0.17958615720272064,
      "learning_rate": 2.411e-05,
      "loss": 0.0015,
      "step": 77670
    },
    {
      "epoch": 4.142933333333334,
      "grad_norm": 0.10836520791053772,
      "learning_rate": 2.4106666666666667e-05,
      "loss": 0.0016,
      "step": 77680
    },
    {
      "epoch": 4.143466666666667,
      "grad_norm": 0.2986367344856262,
      "learning_rate": 2.4103333333333334e-05,
      "loss": 0.0015,
      "step": 77690
    },
    {
      "epoch": 4.144,
      "grad_norm": 0.14537830650806427,
      "learning_rate": 2.41e-05,
      "loss": 0.0043,
      "step": 77700
    },
    {
      "epoch": 4.144533333333333,
      "grad_norm": 0.26890453696250916,
      "learning_rate": 2.4096666666666666e-05,
      "loss": 0.0018,
      "step": 77710
    },
    {
      "epoch": 4.145066666666667,
      "grad_norm": 0.06317371129989624,
      "learning_rate": 2.4093333333333335e-05,
      "loss": 0.0028,
      "step": 77720
    },
    {
      "epoch": 4.1456,
      "grad_norm": 0.6840349435806274,
      "learning_rate": 2.409e-05,
      "loss": 0.0022,
      "step": 77730
    },
    {
      "epoch": 4.146133333333333,
      "grad_norm": 0.0531589575111866,
      "learning_rate": 2.4086666666666668e-05,
      "loss": 0.0019,
      "step": 77740
    },
    {
      "epoch": 4.1466666666666665,
      "grad_norm": 0.3028501868247986,
      "learning_rate": 2.4083333333333337e-05,
      "loss": 0.0017,
      "step": 77750
    },
    {
      "epoch": 4.1472,
      "grad_norm": 0.19883307814598083,
      "learning_rate": 2.408e-05,
      "loss": 0.0021,
      "step": 77760
    },
    {
      "epoch": 4.147733333333333,
      "grad_norm": 0.043463513255119324,
      "learning_rate": 2.4076666666666666e-05,
      "loss": 0.0023,
      "step": 77770
    },
    {
      "epoch": 4.148266666666666,
      "grad_norm": 0.051738787442445755,
      "learning_rate": 2.4073333333333335e-05,
      "loss": 0.0017,
      "step": 77780
    },
    {
      "epoch": 4.1488,
      "grad_norm": 0.29923775792121887,
      "learning_rate": 2.407e-05,
      "loss": 0.0026,
      "step": 77790
    },
    {
      "epoch": 4.149333333333334,
      "grad_norm": 0.47490212321281433,
      "learning_rate": 2.4066666666666668e-05,
      "loss": 0.003,
      "step": 77800
    },
    {
      "epoch": 4.149866666666667,
      "grad_norm": 0.42115235328674316,
      "learning_rate": 2.4063333333333334e-05,
      "loss": 0.0021,
      "step": 77810
    },
    {
      "epoch": 4.1504,
      "grad_norm": 0.5665213465690613,
      "learning_rate": 2.4060000000000003e-05,
      "loss": 0.0021,
      "step": 77820
    },
    {
      "epoch": 4.150933333333334,
      "grad_norm": 0.12014113366603851,
      "learning_rate": 2.405666666666667e-05,
      "loss": 0.0017,
      "step": 77830
    },
    {
      "epoch": 4.151466666666667,
      "grad_norm": 0.3509504795074463,
      "learning_rate": 2.4053333333333332e-05,
      "loss": 0.0014,
      "step": 77840
    },
    {
      "epoch": 4.152,
      "grad_norm": 0.15616042912006378,
      "learning_rate": 2.4050000000000002e-05,
      "loss": 0.0026,
      "step": 77850
    },
    {
      "epoch": 4.152533333333333,
      "grad_norm": 0.3352115750312805,
      "learning_rate": 2.4046666666666668e-05,
      "loss": 0.0016,
      "step": 77860
    },
    {
      "epoch": 4.153066666666667,
      "grad_norm": 0.11588721722364426,
      "learning_rate": 2.4043333333333334e-05,
      "loss": 0.0031,
      "step": 77870
    },
    {
      "epoch": 4.1536,
      "grad_norm": 0.1394585222005844,
      "learning_rate": 2.404e-05,
      "loss": 0.0019,
      "step": 77880
    },
    {
      "epoch": 4.154133333333333,
      "grad_norm": 0.2838563919067383,
      "learning_rate": 2.403666666666667e-05,
      "loss": 0.0033,
      "step": 77890
    },
    {
      "epoch": 4.1546666666666665,
      "grad_norm": 0.12173798680305481,
      "learning_rate": 2.4033333333333336e-05,
      "loss": 0.0022,
      "step": 77900
    },
    {
      "epoch": 4.1552,
      "grad_norm": 0.3069553077220917,
      "learning_rate": 2.4030000000000002e-05,
      "loss": 0.0031,
      "step": 77910
    },
    {
      "epoch": 4.155733333333333,
      "grad_norm": 0.1518593728542328,
      "learning_rate": 2.4026666666666668e-05,
      "loss": 0.0024,
      "step": 77920
    },
    {
      "epoch": 4.156266666666666,
      "grad_norm": 0.34876298904418945,
      "learning_rate": 2.4023333333333334e-05,
      "loss": 0.002,
      "step": 77930
    },
    {
      "epoch": 4.1568,
      "grad_norm": 0.1260233372449875,
      "learning_rate": 2.402e-05,
      "loss": 0.0014,
      "step": 77940
    },
    {
      "epoch": 4.157333333333334,
      "grad_norm": 0.25233134627342224,
      "learning_rate": 2.4016666666666667e-05,
      "loss": 0.0023,
      "step": 77950
    },
    {
      "epoch": 4.157866666666667,
      "grad_norm": 0.32952794432640076,
      "learning_rate": 2.4013333333333336e-05,
      "loss": 0.0027,
      "step": 77960
    },
    {
      "epoch": 4.1584,
      "grad_norm": 0.3361940383911133,
      "learning_rate": 2.4010000000000002e-05,
      "loss": 0.0027,
      "step": 77970
    },
    {
      "epoch": 4.158933333333334,
      "grad_norm": 0.23734182119369507,
      "learning_rate": 2.400666666666667e-05,
      "loss": 0.0019,
      "step": 77980
    },
    {
      "epoch": 4.159466666666667,
      "grad_norm": 0.17461706697940826,
      "learning_rate": 2.4003333333333334e-05,
      "loss": 0.0028,
      "step": 77990
    },
    {
      "epoch": 4.16,
      "grad_norm": 0.0393223911523819,
      "learning_rate": 2.4e-05,
      "loss": 0.0015,
      "step": 78000
    },
    {
      "epoch": 4.160533333333333,
      "grad_norm": 0.19153711199760437,
      "learning_rate": 2.3996666666666667e-05,
      "loss": 0.0026,
      "step": 78010
    },
    {
      "epoch": 4.161066666666667,
      "grad_norm": 0.18341204524040222,
      "learning_rate": 2.3993333333333333e-05,
      "loss": 0.0013,
      "step": 78020
    },
    {
      "epoch": 4.1616,
      "grad_norm": 0.18931850790977478,
      "learning_rate": 2.3990000000000002e-05,
      "loss": 0.0016,
      "step": 78030
    },
    {
      "epoch": 4.162133333333333,
      "grad_norm": 0.35236725211143494,
      "learning_rate": 2.398666666666667e-05,
      "loss": 0.0019,
      "step": 78040
    },
    {
      "epoch": 4.1626666666666665,
      "grad_norm": 0.06997096538543701,
      "learning_rate": 2.3983333333333335e-05,
      "loss": 0.0025,
      "step": 78050
    },
    {
      "epoch": 4.1632,
      "grad_norm": 0.44695040583610535,
      "learning_rate": 2.398e-05,
      "loss": 0.0028,
      "step": 78060
    },
    {
      "epoch": 4.163733333333333,
      "grad_norm": 0.38714683055877686,
      "learning_rate": 2.3976666666666667e-05,
      "loss": 0.0024,
      "step": 78070
    },
    {
      "epoch": 4.164266666666666,
      "grad_norm": 0.6680795550346375,
      "learning_rate": 2.3973333333333333e-05,
      "loss": 0.0024,
      "step": 78080
    },
    {
      "epoch": 4.1648,
      "grad_norm": 0.18545889854431152,
      "learning_rate": 2.397e-05,
      "loss": 0.002,
      "step": 78090
    },
    {
      "epoch": 4.165333333333333,
      "grad_norm": 0.18089987337589264,
      "learning_rate": 2.396666666666667e-05,
      "loss": 0.0018,
      "step": 78100
    },
    {
      "epoch": 4.165866666666667,
      "grad_norm": 0.12257242202758789,
      "learning_rate": 2.3963333333333335e-05,
      "loss": 0.003,
      "step": 78110
    },
    {
      "epoch": 4.1664,
      "grad_norm": 0.4048078954219818,
      "learning_rate": 2.396e-05,
      "loss": 0.002,
      "step": 78120
    },
    {
      "epoch": 4.166933333333334,
      "grad_norm": 0.035989660769701004,
      "learning_rate": 2.395666666666667e-05,
      "loss": 0.0017,
      "step": 78130
    },
    {
      "epoch": 4.167466666666667,
      "grad_norm": 0.06883681565523148,
      "learning_rate": 2.3953333333333333e-05,
      "loss": 0.0018,
      "step": 78140
    },
    {
      "epoch": 4.168,
      "grad_norm": 0.049932338297367096,
      "learning_rate": 2.395e-05,
      "loss": 0.0019,
      "step": 78150
    },
    {
      "epoch": 4.168533333333333,
      "grad_norm": 0.12724338471889496,
      "learning_rate": 2.394666666666667e-05,
      "loss": 0.0013,
      "step": 78160
    },
    {
      "epoch": 4.169066666666667,
      "grad_norm": 0.09539447724819183,
      "learning_rate": 2.3943333333333335e-05,
      "loss": 0.0021,
      "step": 78170
    },
    {
      "epoch": 4.1696,
      "grad_norm": 0.526370644569397,
      "learning_rate": 2.394e-05,
      "loss": 0.0033,
      "step": 78180
    },
    {
      "epoch": 4.170133333333333,
      "grad_norm": 0.1573403775691986,
      "learning_rate": 2.3936666666666667e-05,
      "loss": 0.0023,
      "step": 78190
    },
    {
      "epoch": 4.1706666666666665,
      "grad_norm": 0.6351380944252014,
      "learning_rate": 2.3933333333333337e-05,
      "loss": 0.0023,
      "step": 78200
    },
    {
      "epoch": 4.1712,
      "grad_norm": 0.07605867087841034,
      "learning_rate": 2.3930000000000003e-05,
      "loss": 0.0031,
      "step": 78210
    },
    {
      "epoch": 4.171733333333333,
      "grad_norm": 0.2668669521808624,
      "learning_rate": 2.3926666666666666e-05,
      "loss": 0.0017,
      "step": 78220
    },
    {
      "epoch": 4.172266666666666,
      "grad_norm": 0.20115099847316742,
      "learning_rate": 2.3923333333333335e-05,
      "loss": 0.0021,
      "step": 78230
    },
    {
      "epoch": 4.1728,
      "grad_norm": 0.09446848928928375,
      "learning_rate": 2.392e-05,
      "loss": 0.0017,
      "step": 78240
    },
    {
      "epoch": 4.173333333333334,
      "grad_norm": 0.28794220089912415,
      "learning_rate": 2.3916666666666668e-05,
      "loss": 0.0015,
      "step": 78250
    },
    {
      "epoch": 4.173866666666667,
      "grad_norm": 0.14297202229499817,
      "learning_rate": 2.3913333333333334e-05,
      "loss": 0.0021,
      "step": 78260
    },
    {
      "epoch": 4.1744,
      "grad_norm": 0.03947822377085686,
      "learning_rate": 2.3910000000000003e-05,
      "loss": 0.0019,
      "step": 78270
    },
    {
      "epoch": 4.174933333333334,
      "grad_norm": 0.2737979292869568,
      "learning_rate": 2.390666666666667e-05,
      "loss": 0.0028,
      "step": 78280
    },
    {
      "epoch": 4.175466666666667,
      "grad_norm": 0.07154778391122818,
      "learning_rate": 2.3903333333333332e-05,
      "loss": 0.0013,
      "step": 78290
    },
    {
      "epoch": 4.176,
      "grad_norm": 0.2790931165218353,
      "learning_rate": 2.39e-05,
      "loss": 0.0019,
      "step": 78300
    },
    {
      "epoch": 4.176533333333333,
      "grad_norm": 0.41236284375190735,
      "learning_rate": 2.3896666666666668e-05,
      "loss": 0.0017,
      "step": 78310
    },
    {
      "epoch": 4.177066666666667,
      "grad_norm": 0.12167096138000488,
      "learning_rate": 2.3893333333333334e-05,
      "loss": 0.0023,
      "step": 78320
    },
    {
      "epoch": 4.1776,
      "grad_norm": 0.07487821578979492,
      "learning_rate": 2.389e-05,
      "loss": 0.0024,
      "step": 78330
    },
    {
      "epoch": 4.178133333333333,
      "grad_norm": 0.07318509370088577,
      "learning_rate": 2.388666666666667e-05,
      "loss": 0.002,
      "step": 78340
    },
    {
      "epoch": 4.1786666666666665,
      "grad_norm": 0.32112598419189453,
      "learning_rate": 2.3883333333333336e-05,
      "loss": 0.0021,
      "step": 78350
    },
    {
      "epoch": 4.1792,
      "grad_norm": 0.06835319101810455,
      "learning_rate": 2.3880000000000002e-05,
      "loss": 0.0017,
      "step": 78360
    },
    {
      "epoch": 4.179733333333333,
      "grad_norm": 0.3194959759712219,
      "learning_rate": 2.3876666666666668e-05,
      "loss": 0.0025,
      "step": 78370
    },
    {
      "epoch": 4.180266666666666,
      "grad_norm": 0.05105074495077133,
      "learning_rate": 2.3873333333333334e-05,
      "loss": 0.0019,
      "step": 78380
    },
    {
      "epoch": 4.1808,
      "grad_norm": 0.5282912254333496,
      "learning_rate": 2.387e-05,
      "loss": 0.0017,
      "step": 78390
    },
    {
      "epoch": 4.181333333333333,
      "grad_norm": 0.5321649312973022,
      "learning_rate": 2.3866666666666666e-05,
      "loss": 0.0023,
      "step": 78400
    },
    {
      "epoch": 4.181866666666667,
      "grad_norm": 0.15687020123004913,
      "learning_rate": 2.3863333333333336e-05,
      "loss": 0.0018,
      "step": 78410
    },
    {
      "epoch": 4.1824,
      "grad_norm": 0.3219533860683441,
      "learning_rate": 2.3860000000000002e-05,
      "loss": 0.0015,
      "step": 78420
    },
    {
      "epoch": 4.182933333333334,
      "grad_norm": 0.21695271134376526,
      "learning_rate": 2.3856666666666668e-05,
      "loss": 0.0022,
      "step": 78430
    },
    {
      "epoch": 4.183466666666667,
      "grad_norm": 0.06390851736068726,
      "learning_rate": 2.3853333333333334e-05,
      "loss": 0.0021,
      "step": 78440
    },
    {
      "epoch": 4.184,
      "grad_norm": 0.03694145381450653,
      "learning_rate": 2.385e-05,
      "loss": 0.0022,
      "step": 78450
    },
    {
      "epoch": 4.184533333333333,
      "grad_norm": 0.49677926301956177,
      "learning_rate": 2.3846666666666666e-05,
      "loss": 0.0025,
      "step": 78460
    },
    {
      "epoch": 4.185066666666667,
      "grad_norm": 0.09304133802652359,
      "learning_rate": 2.3843333333333333e-05,
      "loss": 0.0029,
      "step": 78470
    },
    {
      "epoch": 4.1856,
      "grad_norm": 0.7284113168716431,
      "learning_rate": 2.3840000000000002e-05,
      "loss": 0.0026,
      "step": 78480
    },
    {
      "epoch": 4.186133333333333,
      "grad_norm": 0.15703585743904114,
      "learning_rate": 2.3836666666666668e-05,
      "loss": 0.0017,
      "step": 78490
    },
    {
      "epoch": 4.1866666666666665,
      "grad_norm": 0.2981809973716736,
      "learning_rate": 2.3833333333333334e-05,
      "loss": 0.0028,
      "step": 78500
    },
    {
      "epoch": 4.1872,
      "grad_norm": 0.31956517696380615,
      "learning_rate": 2.3830000000000004e-05,
      "loss": 0.0026,
      "step": 78510
    },
    {
      "epoch": 4.187733333333333,
      "grad_norm": 0.5659536123275757,
      "learning_rate": 2.3826666666666667e-05,
      "loss": 0.0017,
      "step": 78520
    },
    {
      "epoch": 4.188266666666666,
      "grad_norm": 0.19680345058441162,
      "learning_rate": 2.3823333333333333e-05,
      "loss": 0.0024,
      "step": 78530
    },
    {
      "epoch": 4.1888,
      "grad_norm": 0.3286755681037903,
      "learning_rate": 2.3820000000000002e-05,
      "loss": 0.0021,
      "step": 78540
    },
    {
      "epoch": 4.189333333333333,
      "grad_norm": 0.3896792232990265,
      "learning_rate": 2.381666666666667e-05,
      "loss": 0.0026,
      "step": 78550
    },
    {
      "epoch": 4.189866666666667,
      "grad_norm": 0.35824254155158997,
      "learning_rate": 2.3813333333333335e-05,
      "loss": 0.0022,
      "step": 78560
    },
    {
      "epoch": 4.1904,
      "grad_norm": 0.15015843510627747,
      "learning_rate": 2.381e-05,
      "loss": 0.0017,
      "step": 78570
    },
    {
      "epoch": 4.190933333333334,
      "grad_norm": 0.26315468549728394,
      "learning_rate": 2.380666666666667e-05,
      "loss": 0.0025,
      "step": 78580
    },
    {
      "epoch": 4.191466666666667,
      "grad_norm": 0.4367814362049103,
      "learning_rate": 2.3803333333333336e-05,
      "loss": 0.0024,
      "step": 78590
    },
    {
      "epoch": 4.192,
      "grad_norm": 0.07416549324989319,
      "learning_rate": 2.38e-05,
      "loss": 0.002,
      "step": 78600
    },
    {
      "epoch": 4.1925333333333334,
      "grad_norm": 0.4332890212535858,
      "learning_rate": 2.379666666666667e-05,
      "loss": 0.002,
      "step": 78610
    },
    {
      "epoch": 4.193066666666667,
      "grad_norm": 0.13865245878696442,
      "learning_rate": 2.3793333333333335e-05,
      "loss": 0.0022,
      "step": 78620
    },
    {
      "epoch": 4.1936,
      "grad_norm": 0.15041062235832214,
      "learning_rate": 2.379e-05,
      "loss": 0.0023,
      "step": 78630
    },
    {
      "epoch": 4.194133333333333,
      "grad_norm": 0.16284650564193726,
      "learning_rate": 2.3786666666666667e-05,
      "loss": 0.0013,
      "step": 78640
    },
    {
      "epoch": 4.1946666666666665,
      "grad_norm": 0.4662008285522461,
      "learning_rate": 2.3783333333333337e-05,
      "loss": 0.0018,
      "step": 78650
    },
    {
      "epoch": 4.1952,
      "grad_norm": 0.050817977637052536,
      "learning_rate": 2.3780000000000003e-05,
      "loss": 0.0015,
      "step": 78660
    },
    {
      "epoch": 4.195733333333333,
      "grad_norm": 0.4688919484615326,
      "learning_rate": 2.3776666666666665e-05,
      "loss": 0.0029,
      "step": 78670
    },
    {
      "epoch": 4.196266666666666,
      "grad_norm": 0.2091922014951706,
      "learning_rate": 2.3773333333333335e-05,
      "loss": 0.0019,
      "step": 78680
    },
    {
      "epoch": 4.1968,
      "grad_norm": 0.11684351414442062,
      "learning_rate": 2.377e-05,
      "loss": 0.0028,
      "step": 78690
    },
    {
      "epoch": 4.197333333333333,
      "grad_norm": 0.3346226215362549,
      "learning_rate": 2.3766666666666667e-05,
      "loss": 0.0014,
      "step": 78700
    },
    {
      "epoch": 4.197866666666667,
      "grad_norm": 0.13519635796546936,
      "learning_rate": 2.3763333333333333e-05,
      "loss": 0.0018,
      "step": 78710
    },
    {
      "epoch": 4.1984,
      "grad_norm": 0.4109971225261688,
      "learning_rate": 2.3760000000000003e-05,
      "loss": 0.0021,
      "step": 78720
    },
    {
      "epoch": 4.198933333333334,
      "grad_norm": 0.5031331777572632,
      "learning_rate": 2.375666666666667e-05,
      "loss": 0.0026,
      "step": 78730
    },
    {
      "epoch": 4.199466666666667,
      "grad_norm": 0.16233107447624207,
      "learning_rate": 2.3753333333333335e-05,
      "loss": 0.0024,
      "step": 78740
    },
    {
      "epoch": 4.2,
      "grad_norm": 0.03717406466603279,
      "learning_rate": 2.375e-05,
      "loss": 0.0021,
      "step": 78750
    },
    {
      "epoch": 4.2005333333333335,
      "grad_norm": 0.26439064741134644,
      "learning_rate": 2.3746666666666667e-05,
      "loss": 0.002,
      "step": 78760
    },
    {
      "epoch": 4.201066666666667,
      "grad_norm": 0.12201590090990067,
      "learning_rate": 2.3743333333333334e-05,
      "loss": 0.0019,
      "step": 78770
    },
    {
      "epoch": 4.2016,
      "grad_norm": 0.5268270969390869,
      "learning_rate": 2.374e-05,
      "loss": 0.0022,
      "step": 78780
    },
    {
      "epoch": 4.202133333333333,
      "grad_norm": 0.12066859751939774,
      "learning_rate": 2.373666666666667e-05,
      "loss": 0.0026,
      "step": 78790
    },
    {
      "epoch": 4.2026666666666666,
      "grad_norm": 0.06468003243207932,
      "learning_rate": 2.3733333333333335e-05,
      "loss": 0.0018,
      "step": 78800
    },
    {
      "epoch": 4.2032,
      "grad_norm": 0.5422827005386353,
      "learning_rate": 2.373e-05,
      "loss": 0.0017,
      "step": 78810
    },
    {
      "epoch": 4.203733333333333,
      "grad_norm": 0.29142412543296814,
      "learning_rate": 2.3726666666666668e-05,
      "loss": 0.0023,
      "step": 78820
    },
    {
      "epoch": 4.204266666666666,
      "grad_norm": 0.03688947856426239,
      "learning_rate": 2.3723333333333334e-05,
      "loss": 0.0024,
      "step": 78830
    },
    {
      "epoch": 4.2048,
      "grad_norm": 0.22869931161403656,
      "learning_rate": 2.372e-05,
      "loss": 0.0023,
      "step": 78840
    },
    {
      "epoch": 4.205333333333333,
      "grad_norm": 0.2047138512134552,
      "learning_rate": 2.3716666666666666e-05,
      "loss": 0.0014,
      "step": 78850
    },
    {
      "epoch": 4.205866666666667,
      "grad_norm": 0.146189883351326,
      "learning_rate": 2.3713333333333336e-05,
      "loss": 0.0029,
      "step": 78860
    },
    {
      "epoch": 4.2064,
      "grad_norm": 0.12439269572496414,
      "learning_rate": 2.371e-05,
      "loss": 0.0021,
      "step": 78870
    },
    {
      "epoch": 4.206933333333334,
      "grad_norm": 0.06649001687765121,
      "learning_rate": 2.3706666666666668e-05,
      "loss": 0.0015,
      "step": 78880
    },
    {
      "epoch": 4.207466666666667,
      "grad_norm": 0.2048167586326599,
      "learning_rate": 2.3703333333333337e-05,
      "loss": 0.0023,
      "step": 78890
    },
    {
      "epoch": 4.208,
      "grad_norm": 0.16649939119815826,
      "learning_rate": 2.37e-05,
      "loss": 0.0017,
      "step": 78900
    },
    {
      "epoch": 4.2085333333333335,
      "grad_norm": 0.0461910106241703,
      "learning_rate": 2.3696666666666666e-05,
      "loss": 0.0019,
      "step": 78910
    },
    {
      "epoch": 4.209066666666667,
      "grad_norm": 0.04165703058242798,
      "learning_rate": 2.3693333333333332e-05,
      "loss": 0.0021,
      "step": 78920
    },
    {
      "epoch": 4.2096,
      "grad_norm": 0.36644548177719116,
      "learning_rate": 2.3690000000000002e-05,
      "loss": 0.002,
      "step": 78930
    },
    {
      "epoch": 4.210133333333333,
      "grad_norm": 0.10898741334676743,
      "learning_rate": 2.3686666666666668e-05,
      "loss": 0.0029,
      "step": 78940
    },
    {
      "epoch": 4.210666666666667,
      "grad_norm": 0.6316946744918823,
      "learning_rate": 2.3683333333333334e-05,
      "loss": 0.0027,
      "step": 78950
    },
    {
      "epoch": 4.2112,
      "grad_norm": 0.059506118297576904,
      "learning_rate": 2.3680000000000004e-05,
      "loss": 0.0018,
      "step": 78960
    },
    {
      "epoch": 4.211733333333333,
      "grad_norm": 0.4414975941181183,
      "learning_rate": 2.3676666666666666e-05,
      "loss": 0.0028,
      "step": 78970
    },
    {
      "epoch": 4.212266666666666,
      "grad_norm": 0.23958609998226166,
      "learning_rate": 2.3673333333333333e-05,
      "loss": 0.0024,
      "step": 78980
    },
    {
      "epoch": 4.2128,
      "grad_norm": 0.2520763576030731,
      "learning_rate": 2.3670000000000002e-05,
      "loss": 0.0022,
      "step": 78990
    },
    {
      "epoch": 4.213333333333333,
      "grad_norm": 0.29394933581352234,
      "learning_rate": 2.3666666666666668e-05,
      "loss": 0.0033,
      "step": 79000
    },
    {
      "epoch": 4.213866666666667,
      "grad_norm": 0.4903865456581116,
      "learning_rate": 2.3663333333333334e-05,
      "loss": 0.0027,
      "step": 79010
    },
    {
      "epoch": 4.2144,
      "grad_norm": 0.07183846086263657,
      "learning_rate": 2.366e-05,
      "loss": 0.0022,
      "step": 79020
    },
    {
      "epoch": 4.214933333333334,
      "grad_norm": 0.7441282868385315,
      "learning_rate": 2.365666666666667e-05,
      "loss": 0.0022,
      "step": 79030
    },
    {
      "epoch": 4.215466666666667,
      "grad_norm": 0.07134120911359787,
      "learning_rate": 2.3653333333333336e-05,
      "loss": 0.0027,
      "step": 79040
    },
    {
      "epoch": 4.216,
      "grad_norm": 0.6761007905006409,
      "learning_rate": 2.365e-05,
      "loss": 0.0026,
      "step": 79050
    },
    {
      "epoch": 4.2165333333333335,
      "grad_norm": 0.36090269684791565,
      "learning_rate": 2.364666666666667e-05,
      "loss": 0.002,
      "step": 79060
    },
    {
      "epoch": 4.217066666666667,
      "grad_norm": 0.36397677659988403,
      "learning_rate": 2.3643333333333335e-05,
      "loss": 0.0023,
      "step": 79070
    },
    {
      "epoch": 4.2176,
      "grad_norm": 0.3551095128059387,
      "learning_rate": 2.364e-05,
      "loss": 0.0019,
      "step": 79080
    },
    {
      "epoch": 4.218133333333333,
      "grad_norm": 0.05198173224925995,
      "learning_rate": 2.3636666666666667e-05,
      "loss": 0.0033,
      "step": 79090
    },
    {
      "epoch": 4.218666666666667,
      "grad_norm": 0.18129782378673553,
      "learning_rate": 2.3633333333333336e-05,
      "loss": 0.0028,
      "step": 79100
    },
    {
      "epoch": 4.2192,
      "grad_norm": 0.20973186194896698,
      "learning_rate": 2.3630000000000002e-05,
      "loss": 0.0016,
      "step": 79110
    },
    {
      "epoch": 4.219733333333333,
      "grad_norm": 0.0973740816116333,
      "learning_rate": 2.362666666666667e-05,
      "loss": 0.0025,
      "step": 79120
    },
    {
      "epoch": 4.220266666666666,
      "grad_norm": 0.23935706913471222,
      "learning_rate": 2.3623333333333335e-05,
      "loss": 0.0017,
      "step": 79130
    },
    {
      "epoch": 4.2208,
      "grad_norm": 0.5775501728057861,
      "learning_rate": 2.362e-05,
      "loss": 0.0023,
      "step": 79140
    },
    {
      "epoch": 4.221333333333333,
      "grad_norm": 0.15044663846492767,
      "learning_rate": 2.3616666666666667e-05,
      "loss": 0.0014,
      "step": 79150
    },
    {
      "epoch": 4.221866666666667,
      "grad_norm": 0.1824881136417389,
      "learning_rate": 2.3613333333333333e-05,
      "loss": 0.0019,
      "step": 79160
    },
    {
      "epoch": 4.2224,
      "grad_norm": 0.17269104719161987,
      "learning_rate": 2.3610000000000003e-05,
      "loss": 0.0036,
      "step": 79170
    },
    {
      "epoch": 4.222933333333334,
      "grad_norm": 0.29203662276268005,
      "learning_rate": 2.360666666666667e-05,
      "loss": 0.0018,
      "step": 79180
    },
    {
      "epoch": 4.223466666666667,
      "grad_norm": 0.17918668687343597,
      "learning_rate": 2.3603333333333335e-05,
      "loss": 0.0017,
      "step": 79190
    },
    {
      "epoch": 4.224,
      "grad_norm": 0.2392025589942932,
      "learning_rate": 2.36e-05,
      "loss": 0.0016,
      "step": 79200
    },
    {
      "epoch": 4.2245333333333335,
      "grad_norm": 0.12412837892770767,
      "learning_rate": 2.3596666666666667e-05,
      "loss": 0.0016,
      "step": 79210
    },
    {
      "epoch": 4.225066666666667,
      "grad_norm": 0.2052215337753296,
      "learning_rate": 2.3593333333333333e-05,
      "loss": 0.0027,
      "step": 79220
    },
    {
      "epoch": 4.2256,
      "grad_norm": 0.2529243230819702,
      "learning_rate": 2.359e-05,
      "loss": 0.0014,
      "step": 79230
    },
    {
      "epoch": 4.226133333333333,
      "grad_norm": 0.14993594586849213,
      "learning_rate": 2.358666666666667e-05,
      "loss": 0.0023,
      "step": 79240
    },
    {
      "epoch": 4.226666666666667,
      "grad_norm": 0.33059293031692505,
      "learning_rate": 2.3583333333333335e-05,
      "loss": 0.0021,
      "step": 79250
    },
    {
      "epoch": 4.2272,
      "grad_norm": 0.324727863073349,
      "learning_rate": 2.358e-05,
      "loss": 0.0032,
      "step": 79260
    },
    {
      "epoch": 4.227733333333333,
      "grad_norm": 0.04684429243206978,
      "learning_rate": 2.357666666666667e-05,
      "loss": 0.0026,
      "step": 79270
    },
    {
      "epoch": 4.228266666666666,
      "grad_norm": 0.5321499109268188,
      "learning_rate": 2.3573333333333334e-05,
      "loss": 0.002,
      "step": 79280
    },
    {
      "epoch": 4.2288,
      "grad_norm": 0.24601514637470245,
      "learning_rate": 2.357e-05,
      "loss": 0.0016,
      "step": 79290
    },
    {
      "epoch": 4.229333333333333,
      "grad_norm": 0.1765201985836029,
      "learning_rate": 2.3566666666666666e-05,
      "loss": 0.0018,
      "step": 79300
    },
    {
      "epoch": 4.229866666666666,
      "grad_norm": 0.46999314427375793,
      "learning_rate": 2.3563333333333335e-05,
      "loss": 0.0014,
      "step": 79310
    },
    {
      "epoch": 4.2304,
      "grad_norm": 0.5551860332489014,
      "learning_rate": 2.356e-05,
      "loss": 0.0022,
      "step": 79320
    },
    {
      "epoch": 4.230933333333334,
      "grad_norm": 0.1665681004524231,
      "learning_rate": 2.3556666666666668e-05,
      "loss": 0.0014,
      "step": 79330
    },
    {
      "epoch": 4.231466666666667,
      "grad_norm": 0.1327570229768753,
      "learning_rate": 2.3553333333333337e-05,
      "loss": 0.0019,
      "step": 79340
    },
    {
      "epoch": 4.232,
      "grad_norm": 0.35554492473602295,
      "learning_rate": 2.355e-05,
      "loss": 0.0026,
      "step": 79350
    },
    {
      "epoch": 4.2325333333333335,
      "grad_norm": 0.4501076638698578,
      "learning_rate": 2.3546666666666666e-05,
      "loss": 0.0024,
      "step": 79360
    },
    {
      "epoch": 4.233066666666667,
      "grad_norm": 0.352468878030777,
      "learning_rate": 2.3543333333333335e-05,
      "loss": 0.0025,
      "step": 79370
    },
    {
      "epoch": 4.2336,
      "grad_norm": 0.0954992026090622,
      "learning_rate": 2.354e-05,
      "loss": 0.0017,
      "step": 79380
    },
    {
      "epoch": 4.234133333333333,
      "grad_norm": 0.08133811503648758,
      "learning_rate": 2.3536666666666668e-05,
      "loss": 0.002,
      "step": 79390
    },
    {
      "epoch": 4.234666666666667,
      "grad_norm": 0.09171441942453384,
      "learning_rate": 2.3533333333333334e-05,
      "loss": 0.0023,
      "step": 79400
    },
    {
      "epoch": 4.2352,
      "grad_norm": 0.24316324293613434,
      "learning_rate": 2.3530000000000003e-05,
      "loss": 0.0015,
      "step": 79410
    },
    {
      "epoch": 4.235733333333333,
      "grad_norm": 0.3241628408432007,
      "learning_rate": 2.352666666666667e-05,
      "loss": 0.0028,
      "step": 79420
    },
    {
      "epoch": 4.236266666666666,
      "grad_norm": 0.04902375116944313,
      "learning_rate": 2.3523333333333332e-05,
      "loss": 0.0022,
      "step": 79430
    },
    {
      "epoch": 4.2368,
      "grad_norm": 0.18238167464733124,
      "learning_rate": 2.3520000000000002e-05,
      "loss": 0.0026,
      "step": 79440
    },
    {
      "epoch": 4.237333333333333,
      "grad_norm": 0.2398611456155777,
      "learning_rate": 2.3516666666666668e-05,
      "loss": 0.0039,
      "step": 79450
    },
    {
      "epoch": 4.237866666666667,
      "grad_norm": 0.2943198084831238,
      "learning_rate": 2.3513333333333334e-05,
      "loss": 0.0028,
      "step": 79460
    },
    {
      "epoch": 4.2384,
      "grad_norm": 0.7251513600349426,
      "learning_rate": 2.351e-05,
      "loss": 0.0029,
      "step": 79470
    },
    {
      "epoch": 4.238933333333334,
      "grad_norm": 0.6213998794555664,
      "learning_rate": 2.350666666666667e-05,
      "loss": 0.0023,
      "step": 79480
    },
    {
      "epoch": 4.239466666666667,
      "grad_norm": 0.5378230810165405,
      "learning_rate": 2.3503333333333336e-05,
      "loss": 0.0019,
      "step": 79490
    },
    {
      "epoch": 4.24,
      "grad_norm": 0.22838512063026428,
      "learning_rate": 2.35e-05,
      "loss": 0.002,
      "step": 79500
    },
    {
      "epoch": 4.2405333333333335,
      "grad_norm": 0.36113786697387695,
      "learning_rate": 2.3496666666666668e-05,
      "loss": 0.0021,
      "step": 79510
    },
    {
      "epoch": 4.241066666666667,
      "grad_norm": 0.26918676495552063,
      "learning_rate": 2.3493333333333334e-05,
      "loss": 0.0019,
      "step": 79520
    },
    {
      "epoch": 4.2416,
      "grad_norm": 0.6123541593551636,
      "learning_rate": 2.349e-05,
      "loss": 0.0012,
      "step": 79530
    },
    {
      "epoch": 4.242133333333333,
      "grad_norm": 0.20697587728500366,
      "learning_rate": 2.3486666666666667e-05,
      "loss": 0.002,
      "step": 79540
    },
    {
      "epoch": 4.242666666666667,
      "grad_norm": 0.0911974236369133,
      "learning_rate": 2.3483333333333336e-05,
      "loss": 0.0034,
      "step": 79550
    },
    {
      "epoch": 4.2432,
      "grad_norm": 0.6100252866744995,
      "learning_rate": 2.3480000000000002e-05,
      "loss": 0.0017,
      "step": 79560
    },
    {
      "epoch": 4.243733333333333,
      "grad_norm": 0.357646644115448,
      "learning_rate": 2.347666666666667e-05,
      "loss": 0.0017,
      "step": 79570
    },
    {
      "epoch": 4.244266666666666,
      "grad_norm": 0.5264233350753784,
      "learning_rate": 2.3473333333333334e-05,
      "loss": 0.002,
      "step": 79580
    },
    {
      "epoch": 4.2448,
      "grad_norm": 0.26409903168678284,
      "learning_rate": 2.347e-05,
      "loss": 0.0017,
      "step": 79590
    },
    {
      "epoch": 4.245333333333333,
      "grad_norm": 0.18270529806613922,
      "learning_rate": 2.3466666666666667e-05,
      "loss": 0.0022,
      "step": 79600
    },
    {
      "epoch": 4.245866666666666,
      "grad_norm": 0.3213675320148468,
      "learning_rate": 2.3463333333333333e-05,
      "loss": 0.0025,
      "step": 79610
    },
    {
      "epoch": 4.2464,
      "grad_norm": 0.3763735592365265,
      "learning_rate": 2.3460000000000002e-05,
      "loss": 0.0031,
      "step": 79620
    },
    {
      "epoch": 4.246933333333334,
      "grad_norm": 0.2587127089500427,
      "learning_rate": 2.345666666666667e-05,
      "loss": 0.0019,
      "step": 79630
    },
    {
      "epoch": 4.247466666666667,
      "grad_norm": 0.05579664930701256,
      "learning_rate": 2.3453333333333335e-05,
      "loss": 0.0021,
      "step": 79640
    },
    {
      "epoch": 4.248,
      "grad_norm": 0.37100306153297424,
      "learning_rate": 2.345e-05,
      "loss": 0.0017,
      "step": 79650
    },
    {
      "epoch": 4.2485333333333335,
      "grad_norm": 0.19685006141662598,
      "learning_rate": 2.3446666666666667e-05,
      "loss": 0.002,
      "step": 79660
    },
    {
      "epoch": 4.249066666666667,
      "grad_norm": 0.23655912280082703,
      "learning_rate": 2.3443333333333333e-05,
      "loss": 0.002,
      "step": 79670
    },
    {
      "epoch": 4.2496,
      "grad_norm": 0.21138091385364532,
      "learning_rate": 2.344e-05,
      "loss": 0.0016,
      "step": 79680
    },
    {
      "epoch": 4.250133333333333,
      "grad_norm": 0.35150837898254395,
      "learning_rate": 2.343666666666667e-05,
      "loss": 0.0019,
      "step": 79690
    },
    {
      "epoch": 4.250666666666667,
      "grad_norm": 0.3260785639286041,
      "learning_rate": 2.3433333333333335e-05,
      "loss": 0.0019,
      "step": 79700
    },
    {
      "epoch": 4.2512,
      "grad_norm": 0.09326471388339996,
      "learning_rate": 2.343e-05,
      "loss": 0.0013,
      "step": 79710
    },
    {
      "epoch": 4.251733333333333,
      "grad_norm": 0.47247767448425293,
      "learning_rate": 2.342666666666667e-05,
      "loss": 0.002,
      "step": 79720
    },
    {
      "epoch": 4.252266666666666,
      "grad_norm": 0.2423914670944214,
      "learning_rate": 2.3423333333333333e-05,
      "loss": 0.0022,
      "step": 79730
    },
    {
      "epoch": 4.2528,
      "grad_norm": 0.07660268992185593,
      "learning_rate": 2.342e-05,
      "loss": 0.0023,
      "step": 79740
    },
    {
      "epoch": 4.253333333333333,
      "grad_norm": 0.22655507922172546,
      "learning_rate": 2.341666666666667e-05,
      "loss": 0.0016,
      "step": 79750
    },
    {
      "epoch": 4.253866666666667,
      "grad_norm": 0.12323054671287537,
      "learning_rate": 2.3413333333333335e-05,
      "loss": 0.0017,
      "step": 79760
    },
    {
      "epoch": 4.2544,
      "grad_norm": 0.21264930069446564,
      "learning_rate": 2.341e-05,
      "loss": 0.0021,
      "step": 79770
    },
    {
      "epoch": 4.254933333333334,
      "grad_norm": 0.2998387813568115,
      "learning_rate": 2.3406666666666667e-05,
      "loss": 0.0017,
      "step": 79780
    },
    {
      "epoch": 4.255466666666667,
      "grad_norm": 0.2921946346759796,
      "learning_rate": 2.3403333333333337e-05,
      "loss": 0.0021,
      "step": 79790
    },
    {
      "epoch": 4.256,
      "grad_norm": 0.18478815257549286,
      "learning_rate": 2.3400000000000003e-05,
      "loss": 0.0016,
      "step": 79800
    },
    {
      "epoch": 4.2565333333333335,
      "grad_norm": 0.04455331340432167,
      "learning_rate": 2.3396666666666666e-05,
      "loss": 0.0022,
      "step": 79810
    },
    {
      "epoch": 4.257066666666667,
      "grad_norm": 0.4260792136192322,
      "learning_rate": 2.3393333333333335e-05,
      "loss": 0.0025,
      "step": 79820
    },
    {
      "epoch": 4.2576,
      "grad_norm": 0.2017637938261032,
      "learning_rate": 2.339e-05,
      "loss": 0.0026,
      "step": 79830
    },
    {
      "epoch": 4.258133333333333,
      "grad_norm": 0.23075060546398163,
      "learning_rate": 2.3386666666666668e-05,
      "loss": 0.0022,
      "step": 79840
    },
    {
      "epoch": 4.258666666666667,
      "grad_norm": 0.3412427604198456,
      "learning_rate": 2.3383333333333334e-05,
      "loss": 0.0018,
      "step": 79850
    },
    {
      "epoch": 4.2592,
      "grad_norm": 0.06347287446260452,
      "learning_rate": 2.3380000000000003e-05,
      "loss": 0.0026,
      "step": 79860
    },
    {
      "epoch": 4.259733333333333,
      "grad_norm": 0.14670193195343018,
      "learning_rate": 2.337666666666667e-05,
      "loss": 0.0024,
      "step": 79870
    },
    {
      "epoch": 4.260266666666666,
      "grad_norm": 0.06401725858449936,
      "learning_rate": 2.3373333333333332e-05,
      "loss": 0.0019,
      "step": 79880
    },
    {
      "epoch": 4.2608,
      "grad_norm": 0.12566617131233215,
      "learning_rate": 2.337e-05,
      "loss": 0.0017,
      "step": 79890
    },
    {
      "epoch": 4.261333333333333,
      "grad_norm": 0.23790927231311798,
      "learning_rate": 2.3366666666666668e-05,
      "loss": 0.0015,
      "step": 79900
    },
    {
      "epoch": 4.261866666666666,
      "grad_norm": 0.29463112354278564,
      "learning_rate": 2.3363333333333334e-05,
      "loss": 0.002,
      "step": 79910
    },
    {
      "epoch": 4.2624,
      "grad_norm": 0.17714780569076538,
      "learning_rate": 2.336e-05,
      "loss": 0.0019,
      "step": 79920
    },
    {
      "epoch": 4.262933333333334,
      "grad_norm": 0.18199485540390015,
      "learning_rate": 2.335666666666667e-05,
      "loss": 0.0019,
      "step": 79930
    },
    {
      "epoch": 4.263466666666667,
      "grad_norm": 0.5340723395347595,
      "learning_rate": 2.3353333333333336e-05,
      "loss": 0.0018,
      "step": 79940
    },
    {
      "epoch": 4.264,
      "grad_norm": 0.22170650959014893,
      "learning_rate": 2.3350000000000002e-05,
      "loss": 0.0015,
      "step": 79950
    },
    {
      "epoch": 4.2645333333333335,
      "grad_norm": 0.15180569887161255,
      "learning_rate": 2.3346666666666668e-05,
      "loss": 0.0021,
      "step": 79960
    },
    {
      "epoch": 4.265066666666667,
      "grad_norm": 0.6604517102241516,
      "learning_rate": 2.3343333333333334e-05,
      "loss": 0.0016,
      "step": 79970
    },
    {
      "epoch": 4.2656,
      "grad_norm": 0.0458935983479023,
      "learning_rate": 2.334e-05,
      "loss": 0.0015,
      "step": 79980
    },
    {
      "epoch": 4.266133333333333,
      "grad_norm": 0.03759002313017845,
      "learning_rate": 2.3336666666666666e-05,
      "loss": 0.0021,
      "step": 79990
    },
    {
      "epoch": 4.266666666666667,
      "grad_norm": 0.24224433302879333,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 0.0025,
      "step": 80000
    },
    {
      "epoch": 4.2672,
      "grad_norm": 0.09118857234716415,
      "learning_rate": 2.3330000000000002e-05,
      "loss": 0.0021,
      "step": 80010
    },
    {
      "epoch": 4.267733333333333,
      "grad_norm": 0.49336129426956177,
      "learning_rate": 2.3326666666666668e-05,
      "loss": 0.0018,
      "step": 80020
    },
    {
      "epoch": 4.268266666666666,
      "grad_norm": 0.11875177919864655,
      "learning_rate": 2.3323333333333334e-05,
      "loss": 0.0021,
      "step": 80030
    },
    {
      "epoch": 4.2688,
      "grad_norm": 0.14449867606163025,
      "learning_rate": 2.332e-05,
      "loss": 0.0025,
      "step": 80040
    },
    {
      "epoch": 4.269333333333333,
      "grad_norm": 0.06923911720514297,
      "learning_rate": 2.3316666666666666e-05,
      "loss": 0.0021,
      "step": 80050
    },
    {
      "epoch": 4.269866666666666,
      "grad_norm": 0.34363505244255066,
      "learning_rate": 2.3313333333333333e-05,
      "loss": 0.0027,
      "step": 80060
    },
    {
      "epoch": 4.2704,
      "grad_norm": 0.18411490321159363,
      "learning_rate": 2.3310000000000002e-05,
      "loss": 0.0024,
      "step": 80070
    },
    {
      "epoch": 4.270933333333334,
      "grad_norm": 0.12038969993591309,
      "learning_rate": 2.3306666666666668e-05,
      "loss": 0.0019,
      "step": 80080
    },
    {
      "epoch": 4.271466666666667,
      "grad_norm": 0.161943718791008,
      "learning_rate": 2.3303333333333334e-05,
      "loss": 0.0016,
      "step": 80090
    },
    {
      "epoch": 4.272,
      "grad_norm": 0.06634895503520966,
      "learning_rate": 2.3300000000000004e-05,
      "loss": 0.0022,
      "step": 80100
    },
    {
      "epoch": 4.2725333333333335,
      "grad_norm": 0.26634180545806885,
      "learning_rate": 2.3296666666666667e-05,
      "loss": 0.0016,
      "step": 80110
    },
    {
      "epoch": 4.273066666666667,
      "grad_norm": 0.18391326069831848,
      "learning_rate": 2.3293333333333333e-05,
      "loss": 0.002,
      "step": 80120
    },
    {
      "epoch": 4.2736,
      "grad_norm": 0.3866090476512909,
      "learning_rate": 2.3290000000000002e-05,
      "loss": 0.0016,
      "step": 80130
    },
    {
      "epoch": 4.274133333333333,
      "grad_norm": 0.06368820369243622,
      "learning_rate": 2.328666666666667e-05,
      "loss": 0.0023,
      "step": 80140
    },
    {
      "epoch": 4.274666666666667,
      "grad_norm": 0.04698929190635681,
      "learning_rate": 2.3283333333333335e-05,
      "loss": 0.0019,
      "step": 80150
    },
    {
      "epoch": 4.2752,
      "grad_norm": 0.420663058757782,
      "learning_rate": 2.328e-05,
      "loss": 0.0016,
      "step": 80160
    },
    {
      "epoch": 4.275733333333333,
      "grad_norm": 0.12849882245063782,
      "learning_rate": 2.327666666666667e-05,
      "loss": 0.0013,
      "step": 80170
    },
    {
      "epoch": 4.276266666666666,
      "grad_norm": 0.4357967972755432,
      "learning_rate": 2.3273333333333333e-05,
      "loss": 0.0023,
      "step": 80180
    },
    {
      "epoch": 4.2768,
      "grad_norm": 0.07312611490488052,
      "learning_rate": 2.327e-05,
      "loss": 0.0022,
      "step": 80190
    },
    {
      "epoch": 4.277333333333333,
      "grad_norm": 0.14600789546966553,
      "learning_rate": 2.326666666666667e-05,
      "loss": 0.0018,
      "step": 80200
    },
    {
      "epoch": 4.277866666666666,
      "grad_norm": 0.08276861160993576,
      "learning_rate": 2.3263333333333335e-05,
      "loss": 0.002,
      "step": 80210
    },
    {
      "epoch": 4.2783999999999995,
      "grad_norm": 0.35651805996894836,
      "learning_rate": 2.326e-05,
      "loss": 0.0013,
      "step": 80220
    },
    {
      "epoch": 4.278933333333334,
      "grad_norm": 0.12858766317367554,
      "learning_rate": 2.3256666666666667e-05,
      "loss": 0.0021,
      "step": 80230
    },
    {
      "epoch": 4.279466666666667,
      "grad_norm": 0.1512717306613922,
      "learning_rate": 2.3253333333333337e-05,
      "loss": 0.0029,
      "step": 80240
    },
    {
      "epoch": 4.28,
      "grad_norm": 0.47666308283805847,
      "learning_rate": 2.3250000000000003e-05,
      "loss": 0.0021,
      "step": 80250
    },
    {
      "epoch": 4.2805333333333335,
      "grad_norm": 0.5259593725204468,
      "learning_rate": 2.3246666666666665e-05,
      "loss": 0.0019,
      "step": 80260
    },
    {
      "epoch": 4.281066666666667,
      "grad_norm": 0.09012772887945175,
      "learning_rate": 2.3243333333333335e-05,
      "loss": 0.002,
      "step": 80270
    },
    {
      "epoch": 4.2816,
      "grad_norm": 0.11469322443008423,
      "learning_rate": 2.324e-05,
      "loss": 0.0024,
      "step": 80280
    },
    {
      "epoch": 4.282133333333333,
      "grad_norm": 0.11881481856107712,
      "learning_rate": 2.3236666666666667e-05,
      "loss": 0.0014,
      "step": 80290
    },
    {
      "epoch": 4.282666666666667,
      "grad_norm": 0.1796228289604187,
      "learning_rate": 2.3233333333333333e-05,
      "loss": 0.0018,
      "step": 80300
    },
    {
      "epoch": 4.2832,
      "grad_norm": 0.046269480139017105,
      "learning_rate": 2.3230000000000003e-05,
      "loss": 0.0019,
      "step": 80310
    },
    {
      "epoch": 4.283733333333333,
      "grad_norm": 0.4080811142921448,
      "learning_rate": 2.322666666666667e-05,
      "loss": 0.0026,
      "step": 80320
    },
    {
      "epoch": 4.2842666666666664,
      "grad_norm": 0.11703797429800034,
      "learning_rate": 2.3223333333333335e-05,
      "loss": 0.0023,
      "step": 80330
    },
    {
      "epoch": 4.2848,
      "grad_norm": 0.1077691987156868,
      "learning_rate": 2.322e-05,
      "loss": 0.0021,
      "step": 80340
    },
    {
      "epoch": 4.285333333333333,
      "grad_norm": 0.2491508424282074,
      "learning_rate": 2.3216666666666667e-05,
      "loss": 0.0015,
      "step": 80350
    },
    {
      "epoch": 4.285866666666666,
      "grad_norm": 0.6086069345474243,
      "learning_rate": 2.3213333333333334e-05,
      "loss": 0.0018,
      "step": 80360
    },
    {
      "epoch": 4.2864,
      "grad_norm": 0.029248179867863655,
      "learning_rate": 2.321e-05,
      "loss": 0.0028,
      "step": 80370
    },
    {
      "epoch": 4.286933333333334,
      "grad_norm": 0.2090827375650406,
      "learning_rate": 2.320666666666667e-05,
      "loss": 0.0025,
      "step": 80380
    },
    {
      "epoch": 4.287466666666667,
      "grad_norm": 0.29094716906547546,
      "learning_rate": 2.3203333333333335e-05,
      "loss": 0.0014,
      "step": 80390
    },
    {
      "epoch": 4.288,
      "grad_norm": 0.1386253386735916,
      "learning_rate": 2.32e-05,
      "loss": 0.0021,
      "step": 80400
    },
    {
      "epoch": 4.2885333333333335,
      "grad_norm": 0.16880981624126434,
      "learning_rate": 2.3196666666666668e-05,
      "loss": 0.0017,
      "step": 80410
    },
    {
      "epoch": 4.289066666666667,
      "grad_norm": 0.0967322438955307,
      "learning_rate": 2.3193333333333334e-05,
      "loss": 0.0024,
      "step": 80420
    },
    {
      "epoch": 4.2896,
      "grad_norm": 0.26773831248283386,
      "learning_rate": 2.319e-05,
      "loss": 0.0022,
      "step": 80430
    },
    {
      "epoch": 4.290133333333333,
      "grad_norm": 0.05870502069592476,
      "learning_rate": 2.3186666666666666e-05,
      "loss": 0.0023,
      "step": 80440
    },
    {
      "epoch": 4.290666666666667,
      "grad_norm": 0.15106038749217987,
      "learning_rate": 2.3183333333333336e-05,
      "loss": 0.0024,
      "step": 80450
    },
    {
      "epoch": 4.2912,
      "grad_norm": 0.12924210727214813,
      "learning_rate": 2.318e-05,
      "loss": 0.0023,
      "step": 80460
    },
    {
      "epoch": 4.291733333333333,
      "grad_norm": 0.14255140721797943,
      "learning_rate": 2.3176666666666668e-05,
      "loss": 0.002,
      "step": 80470
    },
    {
      "epoch": 4.2922666666666665,
      "grad_norm": 0.20624291896820068,
      "learning_rate": 2.3173333333333337e-05,
      "loss": 0.0019,
      "step": 80480
    },
    {
      "epoch": 4.2928,
      "grad_norm": 0.11088240146636963,
      "learning_rate": 2.317e-05,
      "loss": 0.0023,
      "step": 80490
    },
    {
      "epoch": 4.293333333333333,
      "grad_norm": 0.04771627485752106,
      "learning_rate": 2.3166666666666666e-05,
      "loss": 0.0028,
      "step": 80500
    },
    {
      "epoch": 4.293866666666666,
      "grad_norm": 0.4757481515407562,
      "learning_rate": 2.3163333333333336e-05,
      "loss": 0.0019,
      "step": 80510
    },
    {
      "epoch": 4.2943999999999996,
      "grad_norm": 0.08355431258678436,
      "learning_rate": 2.3160000000000002e-05,
      "loss": 0.0025,
      "step": 80520
    },
    {
      "epoch": 4.294933333333334,
      "grad_norm": 0.06852507591247559,
      "learning_rate": 2.3156666666666668e-05,
      "loss": 0.0026,
      "step": 80530
    },
    {
      "epoch": 4.295466666666667,
      "grad_norm": 0.47446534037590027,
      "learning_rate": 2.3153333333333334e-05,
      "loss": 0.0017,
      "step": 80540
    },
    {
      "epoch": 4.296,
      "grad_norm": 0.41748854517936707,
      "learning_rate": 2.3150000000000004e-05,
      "loss": 0.0021,
      "step": 80550
    },
    {
      "epoch": 4.2965333333333335,
      "grad_norm": 0.11970437318086624,
      "learning_rate": 2.3146666666666666e-05,
      "loss": 0.0024,
      "step": 80560
    },
    {
      "epoch": 4.297066666666667,
      "grad_norm": 0.5367529988288879,
      "learning_rate": 2.3143333333333333e-05,
      "loss": 0.0024,
      "step": 80570
    },
    {
      "epoch": 4.2976,
      "grad_norm": 0.09472554922103882,
      "learning_rate": 2.3140000000000002e-05,
      "loss": 0.0017,
      "step": 80580
    },
    {
      "epoch": 4.298133333333333,
      "grad_norm": 0.14654237031936646,
      "learning_rate": 2.3136666666666668e-05,
      "loss": 0.0015,
      "step": 80590
    },
    {
      "epoch": 4.298666666666667,
      "grad_norm": 0.30089643597602844,
      "learning_rate": 2.3133333333333334e-05,
      "loss": 0.0019,
      "step": 80600
    },
    {
      "epoch": 4.2992,
      "grad_norm": 0.22730012238025665,
      "learning_rate": 2.313e-05,
      "loss": 0.0018,
      "step": 80610
    },
    {
      "epoch": 4.299733333333333,
      "grad_norm": 0.1600651741027832,
      "learning_rate": 2.312666666666667e-05,
      "loss": 0.0025,
      "step": 80620
    },
    {
      "epoch": 4.3002666666666665,
      "grad_norm": 0.09392542392015457,
      "learning_rate": 2.3123333333333336e-05,
      "loss": 0.0022,
      "step": 80630
    },
    {
      "epoch": 4.3008,
      "grad_norm": 0.18042801320552826,
      "learning_rate": 2.312e-05,
      "loss": 0.0018,
      "step": 80640
    },
    {
      "epoch": 4.301333333333333,
      "grad_norm": 0.11473444849252701,
      "learning_rate": 2.311666666666667e-05,
      "loss": 0.0028,
      "step": 80650
    },
    {
      "epoch": 4.301866666666666,
      "grad_norm": 0.13185706734657288,
      "learning_rate": 2.3113333333333335e-05,
      "loss": 0.0018,
      "step": 80660
    },
    {
      "epoch": 4.3024000000000004,
      "grad_norm": 0.17713403701782227,
      "learning_rate": 2.311e-05,
      "loss": 0.002,
      "step": 80670
    },
    {
      "epoch": 4.302933333333334,
      "grad_norm": 0.0927082896232605,
      "learning_rate": 2.3106666666666667e-05,
      "loss": 0.0018,
      "step": 80680
    },
    {
      "epoch": 4.303466666666667,
      "grad_norm": 0.3936830461025238,
      "learning_rate": 2.3103333333333336e-05,
      "loss": 0.0016,
      "step": 80690
    },
    {
      "epoch": 4.304,
      "grad_norm": 0.09446493536233902,
      "learning_rate": 2.3100000000000002e-05,
      "loss": 0.0028,
      "step": 80700
    },
    {
      "epoch": 4.3045333333333335,
      "grad_norm": 0.1766427755355835,
      "learning_rate": 2.3096666666666665e-05,
      "loss": 0.0014,
      "step": 80710
    },
    {
      "epoch": 4.305066666666667,
      "grad_norm": 0.26566457748413086,
      "learning_rate": 2.3093333333333335e-05,
      "loss": 0.0017,
      "step": 80720
    },
    {
      "epoch": 4.3056,
      "grad_norm": 0.05867493152618408,
      "learning_rate": 2.309e-05,
      "loss": 0.0021,
      "step": 80730
    },
    {
      "epoch": 4.306133333333333,
      "grad_norm": 0.0330270417034626,
      "learning_rate": 2.3086666666666667e-05,
      "loss": 0.002,
      "step": 80740
    },
    {
      "epoch": 4.306666666666667,
      "grad_norm": 0.0957210585474968,
      "learning_rate": 2.3083333333333333e-05,
      "loss": 0.0028,
      "step": 80750
    },
    {
      "epoch": 4.3072,
      "grad_norm": 0.03479751572012901,
      "learning_rate": 2.3080000000000003e-05,
      "loss": 0.0025,
      "step": 80760
    },
    {
      "epoch": 4.307733333333333,
      "grad_norm": 0.102728933095932,
      "learning_rate": 2.307666666666667e-05,
      "loss": 0.0016,
      "step": 80770
    },
    {
      "epoch": 4.3082666666666665,
      "grad_norm": 0.08854162693023682,
      "learning_rate": 2.3073333333333335e-05,
      "loss": 0.0021,
      "step": 80780
    },
    {
      "epoch": 4.3088,
      "grad_norm": 0.13513004779815674,
      "learning_rate": 2.307e-05,
      "loss": 0.002,
      "step": 80790
    },
    {
      "epoch": 4.309333333333333,
      "grad_norm": 0.4257184565067291,
      "learning_rate": 2.3066666666666667e-05,
      "loss": 0.0023,
      "step": 80800
    },
    {
      "epoch": 4.309866666666666,
      "grad_norm": 0.3118763267993927,
      "learning_rate": 2.3063333333333333e-05,
      "loss": 0.0035,
      "step": 80810
    },
    {
      "epoch": 4.3104,
      "grad_norm": 0.15526621043682098,
      "learning_rate": 2.306e-05,
      "loss": 0.0022,
      "step": 80820
    },
    {
      "epoch": 4.310933333333334,
      "grad_norm": 0.279243528842926,
      "learning_rate": 2.305666666666667e-05,
      "loss": 0.0018,
      "step": 80830
    },
    {
      "epoch": 4.311466666666667,
      "grad_norm": 0.41079089045524597,
      "learning_rate": 2.3053333333333335e-05,
      "loss": 0.002,
      "step": 80840
    },
    {
      "epoch": 4.312,
      "grad_norm": 0.18646857142448425,
      "learning_rate": 2.305e-05,
      "loss": 0.0017,
      "step": 80850
    },
    {
      "epoch": 4.3125333333333336,
      "grad_norm": 0.32200878858566284,
      "learning_rate": 2.3046666666666667e-05,
      "loss": 0.0017,
      "step": 80860
    },
    {
      "epoch": 4.313066666666667,
      "grad_norm": 0.05431484058499336,
      "learning_rate": 2.3043333333333334e-05,
      "loss": 0.0023,
      "step": 80870
    },
    {
      "epoch": 4.3136,
      "grad_norm": 0.22109201550483704,
      "learning_rate": 2.304e-05,
      "loss": 0.0018,
      "step": 80880
    },
    {
      "epoch": 4.314133333333333,
      "grad_norm": 0.26557227969169617,
      "learning_rate": 2.303666666666667e-05,
      "loss": 0.0025,
      "step": 80890
    },
    {
      "epoch": 4.314666666666667,
      "grad_norm": 0.17744813859462738,
      "learning_rate": 2.3033333333333335e-05,
      "loss": 0.0021,
      "step": 80900
    },
    {
      "epoch": 4.3152,
      "grad_norm": 0.6100654602050781,
      "learning_rate": 2.303e-05,
      "loss": 0.0015,
      "step": 80910
    },
    {
      "epoch": 4.315733333333333,
      "grad_norm": 0.20383094251155853,
      "learning_rate": 2.3026666666666668e-05,
      "loss": 0.0019,
      "step": 80920
    },
    {
      "epoch": 4.3162666666666665,
      "grad_norm": 0.09175466001033783,
      "learning_rate": 2.3023333333333337e-05,
      "loss": 0.002,
      "step": 80930
    },
    {
      "epoch": 4.3168,
      "grad_norm": 0.2021089345216751,
      "learning_rate": 2.302e-05,
      "loss": 0.0025,
      "step": 80940
    },
    {
      "epoch": 4.317333333333333,
      "grad_norm": 0.06845632195472717,
      "learning_rate": 2.3016666666666666e-05,
      "loss": 0.0023,
      "step": 80950
    },
    {
      "epoch": 4.317866666666666,
      "grad_norm": 0.43697822093963623,
      "learning_rate": 2.3013333333333335e-05,
      "loss": 0.0023,
      "step": 80960
    },
    {
      "epoch": 4.3184000000000005,
      "grad_norm": 0.020663395524024963,
      "learning_rate": 2.301e-05,
      "loss": 0.0027,
      "step": 80970
    },
    {
      "epoch": 4.318933333333334,
      "grad_norm": 0.24002914130687714,
      "learning_rate": 2.3006666666666668e-05,
      "loss": 0.0023,
      "step": 80980
    },
    {
      "epoch": 4.319466666666667,
      "grad_norm": 0.2345534861087799,
      "learning_rate": 2.3003333333333334e-05,
      "loss": 0.0021,
      "step": 80990
    },
    {
      "epoch": 4.32,
      "grad_norm": 0.14985117316246033,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.0026,
      "step": 81000
    },
    {
      "epoch": 4.320533333333334,
      "grad_norm": 0.46316272020339966,
      "learning_rate": 2.299666666666667e-05,
      "loss": 0.0022,
      "step": 81010
    },
    {
      "epoch": 4.321066666666667,
      "grad_norm": 0.1803233027458191,
      "learning_rate": 2.2993333333333332e-05,
      "loss": 0.0016,
      "step": 81020
    },
    {
      "epoch": 4.3216,
      "grad_norm": 0.32839974761009216,
      "learning_rate": 2.2990000000000002e-05,
      "loss": 0.0024,
      "step": 81030
    },
    {
      "epoch": 4.322133333333333,
      "grad_norm": 0.14954079687595367,
      "learning_rate": 2.2986666666666668e-05,
      "loss": 0.0017,
      "step": 81040
    },
    {
      "epoch": 4.322666666666667,
      "grad_norm": 0.31788933277130127,
      "learning_rate": 2.2983333333333334e-05,
      "loss": 0.0016,
      "step": 81050
    },
    {
      "epoch": 4.3232,
      "grad_norm": 0.20241843163967133,
      "learning_rate": 2.298e-05,
      "loss": 0.0022,
      "step": 81060
    },
    {
      "epoch": 4.323733333333333,
      "grad_norm": 0.24232344329357147,
      "learning_rate": 2.297666666666667e-05,
      "loss": 0.0016,
      "step": 81070
    },
    {
      "epoch": 4.3242666666666665,
      "grad_norm": 0.33437323570251465,
      "learning_rate": 2.2973333333333336e-05,
      "loss": 0.0023,
      "step": 81080
    },
    {
      "epoch": 4.3248,
      "grad_norm": 0.0661320760846138,
      "learning_rate": 2.297e-05,
      "loss": 0.002,
      "step": 81090
    },
    {
      "epoch": 4.325333333333333,
      "grad_norm": 0.2721472680568695,
      "learning_rate": 2.2966666666666668e-05,
      "loss": 0.0017,
      "step": 81100
    },
    {
      "epoch": 4.325866666666666,
      "grad_norm": 0.37758171558380127,
      "learning_rate": 2.2963333333333334e-05,
      "loss": 0.0014,
      "step": 81110
    },
    {
      "epoch": 4.3264,
      "grad_norm": 0.2668118476867676,
      "learning_rate": 2.296e-05,
      "loss": 0.0027,
      "step": 81120
    },
    {
      "epoch": 4.326933333333334,
      "grad_norm": 0.24032925069332123,
      "learning_rate": 2.2956666666666667e-05,
      "loss": 0.002,
      "step": 81130
    },
    {
      "epoch": 4.327466666666667,
      "grad_norm": 0.2170737385749817,
      "learning_rate": 2.2953333333333336e-05,
      "loss": 0.0021,
      "step": 81140
    },
    {
      "epoch": 4.328,
      "grad_norm": 0.527969479560852,
      "learning_rate": 2.2950000000000002e-05,
      "loss": 0.0026,
      "step": 81150
    },
    {
      "epoch": 4.328533333333334,
      "grad_norm": 0.2589016556739807,
      "learning_rate": 2.294666666666667e-05,
      "loss": 0.0022,
      "step": 81160
    },
    {
      "epoch": 4.329066666666667,
      "grad_norm": 0.524558424949646,
      "learning_rate": 2.2943333333333334e-05,
      "loss": 0.0025,
      "step": 81170
    },
    {
      "epoch": 4.3296,
      "grad_norm": 0.133741557598114,
      "learning_rate": 2.294e-05,
      "loss": 0.003,
      "step": 81180
    },
    {
      "epoch": 4.330133333333333,
      "grad_norm": 0.1683642864227295,
      "learning_rate": 2.2936666666666667e-05,
      "loss": 0.0018,
      "step": 81190
    },
    {
      "epoch": 4.330666666666667,
      "grad_norm": 0.04543514922261238,
      "learning_rate": 2.2933333333333333e-05,
      "loss": 0.0021,
      "step": 81200
    },
    {
      "epoch": 4.3312,
      "grad_norm": 0.35088050365448,
      "learning_rate": 2.2930000000000002e-05,
      "loss": 0.0022,
      "step": 81210
    },
    {
      "epoch": 4.331733333333333,
      "grad_norm": 0.4066227078437805,
      "learning_rate": 2.292666666666667e-05,
      "loss": 0.0019,
      "step": 81220
    },
    {
      "epoch": 4.3322666666666665,
      "grad_norm": 0.35426363348960876,
      "learning_rate": 2.2923333333333335e-05,
      "loss": 0.0021,
      "step": 81230
    },
    {
      "epoch": 4.3328,
      "grad_norm": 0.12245231121778488,
      "learning_rate": 2.292e-05,
      "loss": 0.0017,
      "step": 81240
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 0.29165413975715637,
      "learning_rate": 2.2916666666666667e-05,
      "loss": 0.0018,
      "step": 81250
    },
    {
      "epoch": 4.333866666666666,
      "grad_norm": 0.20669123530387878,
      "learning_rate": 2.2913333333333333e-05,
      "loss": 0.0029,
      "step": 81260
    },
    {
      "epoch": 4.3344,
      "grad_norm": 0.12121625244617462,
      "learning_rate": 2.2910000000000003e-05,
      "loss": 0.0017,
      "step": 81270
    },
    {
      "epoch": 4.334933333333334,
      "grad_norm": 0.09063941985368729,
      "learning_rate": 2.290666666666667e-05,
      "loss": 0.0015,
      "step": 81280
    },
    {
      "epoch": 4.335466666666667,
      "grad_norm": 0.16696301102638245,
      "learning_rate": 2.2903333333333335e-05,
      "loss": 0.0019,
      "step": 81290
    },
    {
      "epoch": 4.336,
      "grad_norm": 0.06577204912900925,
      "learning_rate": 2.29e-05,
      "loss": 0.0018,
      "step": 81300
    },
    {
      "epoch": 4.336533333333334,
      "grad_norm": 0.39915746450424194,
      "learning_rate": 2.289666666666667e-05,
      "loss": 0.0024,
      "step": 81310
    },
    {
      "epoch": 4.337066666666667,
      "grad_norm": 0.2638721466064453,
      "learning_rate": 2.2893333333333333e-05,
      "loss": 0.0022,
      "step": 81320
    },
    {
      "epoch": 4.3376,
      "grad_norm": 0.20969939231872559,
      "learning_rate": 2.289e-05,
      "loss": 0.0016,
      "step": 81330
    },
    {
      "epoch": 4.338133333333333,
      "grad_norm": 0.08682607114315033,
      "learning_rate": 2.288666666666667e-05,
      "loss": 0.0022,
      "step": 81340
    },
    {
      "epoch": 4.338666666666667,
      "grad_norm": 0.12122353166341782,
      "learning_rate": 2.2883333333333335e-05,
      "loss": 0.002,
      "step": 81350
    },
    {
      "epoch": 4.3392,
      "grad_norm": 0.04044962674379349,
      "learning_rate": 2.288e-05,
      "loss": 0.002,
      "step": 81360
    },
    {
      "epoch": 4.339733333333333,
      "grad_norm": 0.38791027665138245,
      "learning_rate": 2.2876666666666667e-05,
      "loss": 0.0031,
      "step": 81370
    },
    {
      "epoch": 4.3402666666666665,
      "grad_norm": 0.7096483707427979,
      "learning_rate": 2.2873333333333337e-05,
      "loss": 0.0041,
      "step": 81380
    },
    {
      "epoch": 4.3408,
      "grad_norm": 0.17119064927101135,
      "learning_rate": 2.287e-05,
      "loss": 0.0021,
      "step": 81390
    },
    {
      "epoch": 4.341333333333333,
      "grad_norm": 0.352485328912735,
      "learning_rate": 2.2866666666666666e-05,
      "loss": 0.0018,
      "step": 81400
    },
    {
      "epoch": 4.341866666666666,
      "grad_norm": 0.35089972615242004,
      "learning_rate": 2.2863333333333335e-05,
      "loss": 0.0016,
      "step": 81410
    },
    {
      "epoch": 4.3424,
      "grad_norm": 0.1810964047908783,
      "learning_rate": 2.286e-05,
      "loss": 0.0012,
      "step": 81420
    },
    {
      "epoch": 4.342933333333333,
      "grad_norm": 0.17148904502391815,
      "learning_rate": 2.2856666666666667e-05,
      "loss": 0.0021,
      "step": 81430
    },
    {
      "epoch": 4.343466666666667,
      "grad_norm": 0.2009858787059784,
      "learning_rate": 2.2853333333333334e-05,
      "loss": 0.0018,
      "step": 81440
    },
    {
      "epoch": 4.344,
      "grad_norm": 0.26630207896232605,
      "learning_rate": 2.2850000000000003e-05,
      "loss": 0.0013,
      "step": 81450
    },
    {
      "epoch": 4.344533333333334,
      "grad_norm": 0.20406243205070496,
      "learning_rate": 2.284666666666667e-05,
      "loss": 0.0019,
      "step": 81460
    },
    {
      "epoch": 4.345066666666667,
      "grad_norm": 0.2520257830619812,
      "learning_rate": 2.2843333333333332e-05,
      "loss": 0.0017,
      "step": 81470
    },
    {
      "epoch": 4.3456,
      "grad_norm": 0.5246717929840088,
      "learning_rate": 2.284e-05,
      "loss": 0.0018,
      "step": 81480
    },
    {
      "epoch": 4.346133333333333,
      "grad_norm": 0.09539523720741272,
      "learning_rate": 2.2836666666666668e-05,
      "loss": 0.0015,
      "step": 81490
    },
    {
      "epoch": 4.346666666666667,
      "grad_norm": 0.3615807592868805,
      "learning_rate": 2.2833333333333334e-05,
      "loss": 0.0023,
      "step": 81500
    },
    {
      "epoch": 4.3472,
      "grad_norm": 0.11746329069137573,
      "learning_rate": 2.283e-05,
      "loss": 0.0021,
      "step": 81510
    },
    {
      "epoch": 4.347733333333333,
      "grad_norm": 0.3216174840927124,
      "learning_rate": 2.282666666666667e-05,
      "loss": 0.0017,
      "step": 81520
    },
    {
      "epoch": 4.3482666666666665,
      "grad_norm": 0.25238609313964844,
      "learning_rate": 2.2823333333333336e-05,
      "loss": 0.0017,
      "step": 81530
    },
    {
      "epoch": 4.3488,
      "grad_norm": 0.5117779970169067,
      "learning_rate": 2.282e-05,
      "loss": 0.0028,
      "step": 81540
    },
    {
      "epoch": 4.349333333333333,
      "grad_norm": 0.16027195751667023,
      "learning_rate": 2.2816666666666668e-05,
      "loss": 0.002,
      "step": 81550
    },
    {
      "epoch": 4.349866666666666,
      "grad_norm": 0.35464948415756226,
      "learning_rate": 2.2813333333333334e-05,
      "loss": 0.002,
      "step": 81560
    },
    {
      "epoch": 4.3504,
      "grad_norm": 0.4093942642211914,
      "learning_rate": 2.281e-05,
      "loss": 0.0024,
      "step": 81570
    },
    {
      "epoch": 4.350933333333334,
      "grad_norm": 0.37843289971351624,
      "learning_rate": 2.2806666666666666e-05,
      "loss": 0.0018,
      "step": 81580
    },
    {
      "epoch": 4.351466666666667,
      "grad_norm": 0.1480245590209961,
      "learning_rate": 2.2803333333333336e-05,
      "loss": 0.0022,
      "step": 81590
    },
    {
      "epoch": 4.352,
      "grad_norm": 0.8088931441307068,
      "learning_rate": 2.2800000000000002e-05,
      "loss": 0.0018,
      "step": 81600
    },
    {
      "epoch": 4.352533333333334,
      "grad_norm": 0.356588751077652,
      "learning_rate": 2.2796666666666668e-05,
      "loss": 0.0029,
      "step": 81610
    },
    {
      "epoch": 4.353066666666667,
      "grad_norm": 0.7111648321151733,
      "learning_rate": 2.2793333333333334e-05,
      "loss": 0.0026,
      "step": 81620
    },
    {
      "epoch": 4.3536,
      "grad_norm": 0.21175427734851837,
      "learning_rate": 2.279e-05,
      "loss": 0.0022,
      "step": 81630
    },
    {
      "epoch": 4.354133333333333,
      "grad_norm": 0.9227904677391052,
      "learning_rate": 2.2786666666666666e-05,
      "loss": 0.0022,
      "step": 81640
    },
    {
      "epoch": 4.354666666666667,
      "grad_norm": 0.058193378150463104,
      "learning_rate": 2.2783333333333336e-05,
      "loss": 0.0018,
      "step": 81650
    },
    {
      "epoch": 4.3552,
      "grad_norm": 0.2899066209793091,
      "learning_rate": 2.2780000000000002e-05,
      "loss": 0.0025,
      "step": 81660
    },
    {
      "epoch": 4.355733333333333,
      "grad_norm": 0.09436739236116409,
      "learning_rate": 2.2776666666666668e-05,
      "loss": 0.002,
      "step": 81670
    },
    {
      "epoch": 4.3562666666666665,
      "grad_norm": 0.12180521339178085,
      "learning_rate": 2.2773333333333334e-05,
      "loss": 0.0017,
      "step": 81680
    },
    {
      "epoch": 4.3568,
      "grad_norm": 0.2090100198984146,
      "learning_rate": 2.2770000000000004e-05,
      "loss": 0.002,
      "step": 81690
    },
    {
      "epoch": 4.357333333333333,
      "grad_norm": 0.13420304656028748,
      "learning_rate": 2.2766666666666667e-05,
      "loss": 0.0022,
      "step": 81700
    },
    {
      "epoch": 4.357866666666666,
      "grad_norm": 0.06391818076372147,
      "learning_rate": 2.2763333333333333e-05,
      "loss": 0.0025,
      "step": 81710
    },
    {
      "epoch": 4.3584,
      "grad_norm": 0.2467169463634491,
      "learning_rate": 2.2760000000000002e-05,
      "loss": 0.0015,
      "step": 81720
    },
    {
      "epoch": 4.358933333333333,
      "grad_norm": 0.468839555978775,
      "learning_rate": 2.275666666666667e-05,
      "loss": 0.0019,
      "step": 81730
    },
    {
      "epoch": 4.359466666666667,
      "grad_norm": 0.2608460485935211,
      "learning_rate": 2.2753333333333335e-05,
      "loss": 0.002,
      "step": 81740
    },
    {
      "epoch": 4.36,
      "grad_norm": 0.4992147386074066,
      "learning_rate": 2.275e-05,
      "loss": 0.0022,
      "step": 81750
    },
    {
      "epoch": 4.360533333333334,
      "grad_norm": 0.18121173977851868,
      "learning_rate": 2.274666666666667e-05,
      "loss": 0.0025,
      "step": 81760
    },
    {
      "epoch": 4.361066666666667,
      "grad_norm": 0.06384008377790451,
      "learning_rate": 2.2743333333333333e-05,
      "loss": 0.0021,
      "step": 81770
    },
    {
      "epoch": 4.3616,
      "grad_norm": 0.3172147274017334,
      "learning_rate": 2.274e-05,
      "loss": 0.0028,
      "step": 81780
    },
    {
      "epoch": 4.362133333333333,
      "grad_norm": 0.21687842905521393,
      "learning_rate": 2.273666666666667e-05,
      "loss": 0.0017,
      "step": 81790
    },
    {
      "epoch": 4.362666666666667,
      "grad_norm": 0.08699368685483932,
      "learning_rate": 2.2733333333333335e-05,
      "loss": 0.0023,
      "step": 81800
    },
    {
      "epoch": 4.3632,
      "grad_norm": 0.18959811329841614,
      "learning_rate": 2.273e-05,
      "loss": 0.0026,
      "step": 81810
    },
    {
      "epoch": 4.363733333333333,
      "grad_norm": 0.0987606942653656,
      "learning_rate": 2.2726666666666667e-05,
      "loss": 0.0015,
      "step": 81820
    },
    {
      "epoch": 4.3642666666666665,
      "grad_norm": 0.3542877733707428,
      "learning_rate": 2.2723333333333337e-05,
      "loss": 0.002,
      "step": 81830
    },
    {
      "epoch": 4.3648,
      "grad_norm": 0.049602486193180084,
      "learning_rate": 2.2720000000000003e-05,
      "loss": 0.0021,
      "step": 81840
    },
    {
      "epoch": 4.365333333333333,
      "grad_norm": 0.11641959846019745,
      "learning_rate": 2.2716666666666665e-05,
      "loss": 0.0013,
      "step": 81850
    },
    {
      "epoch": 4.365866666666666,
      "grad_norm": 0.06310710310935974,
      "learning_rate": 2.2713333333333335e-05,
      "loss": 0.0021,
      "step": 81860
    },
    {
      "epoch": 4.3664,
      "grad_norm": 0.3535390794277191,
      "learning_rate": 2.271e-05,
      "loss": 0.0025,
      "step": 81870
    },
    {
      "epoch": 4.366933333333334,
      "grad_norm": 0.09594687074422836,
      "learning_rate": 2.2706666666666667e-05,
      "loss": 0.0026,
      "step": 81880
    },
    {
      "epoch": 4.367466666666667,
      "grad_norm": 0.1433134227991104,
      "learning_rate": 2.2703333333333333e-05,
      "loss": 0.0018,
      "step": 81890
    },
    {
      "epoch": 4.368,
      "grad_norm": 0.2130698561668396,
      "learning_rate": 2.2700000000000003e-05,
      "loss": 0.0015,
      "step": 81900
    },
    {
      "epoch": 4.368533333333334,
      "grad_norm": 0.2906108796596527,
      "learning_rate": 2.269666666666667e-05,
      "loss": 0.0016,
      "step": 81910
    },
    {
      "epoch": 4.369066666666667,
      "grad_norm": 0.14801062643527985,
      "learning_rate": 2.2693333333333332e-05,
      "loss": 0.0014,
      "step": 81920
    },
    {
      "epoch": 4.3696,
      "grad_norm": 0.1726193130016327,
      "learning_rate": 2.269e-05,
      "loss": 0.0014,
      "step": 81930
    },
    {
      "epoch": 4.370133333333333,
      "grad_norm": 0.03920157998800278,
      "learning_rate": 2.2686666666666667e-05,
      "loss": 0.0016,
      "step": 81940
    },
    {
      "epoch": 4.370666666666667,
      "grad_norm": 0.10635240375995636,
      "learning_rate": 2.2683333333333334e-05,
      "loss": 0.0024,
      "step": 81950
    },
    {
      "epoch": 4.3712,
      "grad_norm": 0.20170418918132782,
      "learning_rate": 2.268e-05,
      "loss": 0.0018,
      "step": 81960
    },
    {
      "epoch": 4.371733333333333,
      "grad_norm": 0.2880231738090515,
      "learning_rate": 2.267666666666667e-05,
      "loss": 0.0022,
      "step": 81970
    },
    {
      "epoch": 4.3722666666666665,
      "grad_norm": 0.08800909668207169,
      "learning_rate": 2.2673333333333335e-05,
      "loss": 0.0024,
      "step": 81980
    },
    {
      "epoch": 4.3728,
      "grad_norm": 0.04879961535334587,
      "learning_rate": 2.267e-05,
      "loss": 0.0014,
      "step": 81990
    },
    {
      "epoch": 4.373333333333333,
      "grad_norm": 0.022946607321500778,
      "learning_rate": 2.2666666666666668e-05,
      "loss": 0.002,
      "step": 82000
    },
    {
      "epoch": 4.373866666666666,
      "grad_norm": 0.20712481439113617,
      "learning_rate": 2.2663333333333334e-05,
      "loss": 0.0017,
      "step": 82010
    },
    {
      "epoch": 4.3744,
      "grad_norm": 0.5387987494468689,
      "learning_rate": 2.266e-05,
      "loss": 0.0017,
      "step": 82020
    },
    {
      "epoch": 4.374933333333333,
      "grad_norm": 0.19923463463783264,
      "learning_rate": 2.2656666666666666e-05,
      "loss": 0.0016,
      "step": 82030
    },
    {
      "epoch": 4.375466666666667,
      "grad_norm": 0.469218373298645,
      "learning_rate": 2.2653333333333336e-05,
      "loss": 0.0018,
      "step": 82040
    },
    {
      "epoch": 4.376,
      "grad_norm": 0.06554541736841202,
      "learning_rate": 2.265e-05,
      "loss": 0.0019,
      "step": 82050
    },
    {
      "epoch": 4.376533333333334,
      "grad_norm": 0.4836428761482239,
      "learning_rate": 2.2646666666666668e-05,
      "loss": 0.0023,
      "step": 82060
    },
    {
      "epoch": 4.377066666666667,
      "grad_norm": 0.3507406711578369,
      "learning_rate": 2.2643333333333334e-05,
      "loss": 0.0025,
      "step": 82070
    },
    {
      "epoch": 4.3776,
      "grad_norm": 0.09178377687931061,
      "learning_rate": 2.264e-05,
      "loss": 0.0022,
      "step": 82080
    },
    {
      "epoch": 4.378133333333333,
      "grad_norm": 0.41784000396728516,
      "learning_rate": 2.2636666666666666e-05,
      "loss": 0.002,
      "step": 82090
    },
    {
      "epoch": 4.378666666666667,
      "grad_norm": 0.2423795759677887,
      "learning_rate": 2.2633333333333336e-05,
      "loss": 0.0018,
      "step": 82100
    },
    {
      "epoch": 4.3792,
      "grad_norm": 0.175639346241951,
      "learning_rate": 2.2630000000000002e-05,
      "loss": 0.002,
      "step": 82110
    },
    {
      "epoch": 4.379733333333333,
      "grad_norm": 0.12662406265735626,
      "learning_rate": 2.2626666666666668e-05,
      "loss": 0.0017,
      "step": 82120
    },
    {
      "epoch": 4.3802666666666665,
      "grad_norm": 0.11462723463773727,
      "learning_rate": 2.2623333333333334e-05,
      "loss": 0.0018,
      "step": 82130
    },
    {
      "epoch": 4.3808,
      "grad_norm": 0.05676582455635071,
      "learning_rate": 2.2620000000000004e-05,
      "loss": 0.0021,
      "step": 82140
    },
    {
      "epoch": 4.381333333333333,
      "grad_norm": 0.3196224570274353,
      "learning_rate": 2.2616666666666666e-05,
      "loss": 0.0019,
      "step": 82150
    },
    {
      "epoch": 4.381866666666666,
      "grad_norm": 0.8158295750617981,
      "learning_rate": 2.2613333333333333e-05,
      "loss": 0.0023,
      "step": 82160
    },
    {
      "epoch": 4.3824,
      "grad_norm": 0.07501330971717834,
      "learning_rate": 2.2610000000000002e-05,
      "loss": 0.0018,
      "step": 82170
    },
    {
      "epoch": 4.382933333333334,
      "grad_norm": 0.06916967779397964,
      "learning_rate": 2.2606666666666668e-05,
      "loss": 0.0027,
      "step": 82180
    },
    {
      "epoch": 4.383466666666667,
      "grad_norm": 0.07446127384901047,
      "learning_rate": 2.2603333333333334e-05,
      "loss": 0.0016,
      "step": 82190
    },
    {
      "epoch": 4.384,
      "grad_norm": 0.1507369726896286,
      "learning_rate": 2.26e-05,
      "loss": 0.0018,
      "step": 82200
    },
    {
      "epoch": 4.384533333333334,
      "grad_norm": 0.32311320304870605,
      "learning_rate": 2.259666666666667e-05,
      "loss": 0.0022,
      "step": 82210
    },
    {
      "epoch": 4.385066666666667,
      "grad_norm": 0.1544497311115265,
      "learning_rate": 2.2593333333333336e-05,
      "loss": 0.0017,
      "step": 82220
    },
    {
      "epoch": 4.3856,
      "grad_norm": 0.17351731657981873,
      "learning_rate": 2.259e-05,
      "loss": 0.0029,
      "step": 82230
    },
    {
      "epoch": 4.386133333333333,
      "grad_norm": 0.08723416924476624,
      "learning_rate": 2.258666666666667e-05,
      "loss": 0.0014,
      "step": 82240
    },
    {
      "epoch": 4.386666666666667,
      "grad_norm": 0.12152116745710373,
      "learning_rate": 2.2583333333333335e-05,
      "loss": 0.0022,
      "step": 82250
    },
    {
      "epoch": 4.3872,
      "grad_norm": 0.2633724808692932,
      "learning_rate": 2.258e-05,
      "loss": 0.0025,
      "step": 82260
    },
    {
      "epoch": 4.387733333333333,
      "grad_norm": 0.6265567541122437,
      "learning_rate": 2.2576666666666667e-05,
      "loss": 0.0023,
      "step": 82270
    },
    {
      "epoch": 4.3882666666666665,
      "grad_norm": 0.08985470235347748,
      "learning_rate": 2.2573333333333336e-05,
      "loss": 0.0021,
      "step": 82280
    },
    {
      "epoch": 4.3888,
      "grad_norm": 0.18519063293933868,
      "learning_rate": 2.2570000000000002e-05,
      "loss": 0.0022,
      "step": 82290
    },
    {
      "epoch": 4.389333333333333,
      "grad_norm": 0.26729628443717957,
      "learning_rate": 2.2566666666666665e-05,
      "loss": 0.0023,
      "step": 82300
    },
    {
      "epoch": 4.389866666666666,
      "grad_norm": 0.40762072801589966,
      "learning_rate": 2.2563333333333335e-05,
      "loss": 0.0014,
      "step": 82310
    },
    {
      "epoch": 4.3904,
      "grad_norm": 0.049323730170726776,
      "learning_rate": 2.256e-05,
      "loss": 0.0021,
      "step": 82320
    },
    {
      "epoch": 4.390933333333333,
      "grad_norm": 0.32174044847488403,
      "learning_rate": 2.2556666666666667e-05,
      "loss": 0.0024,
      "step": 82330
    },
    {
      "epoch": 4.391466666666667,
      "grad_norm": 0.4632948338985443,
      "learning_rate": 2.2553333333333333e-05,
      "loss": 0.0019,
      "step": 82340
    },
    {
      "epoch": 4.392,
      "grad_norm": 0.3564262092113495,
      "learning_rate": 2.2550000000000003e-05,
      "loss": 0.0028,
      "step": 82350
    },
    {
      "epoch": 4.392533333333334,
      "grad_norm": 0.3385714590549469,
      "learning_rate": 2.254666666666667e-05,
      "loss": 0.0017,
      "step": 82360
    },
    {
      "epoch": 4.393066666666667,
      "grad_norm": 0.07339561730623245,
      "learning_rate": 2.2543333333333335e-05,
      "loss": 0.0021,
      "step": 82370
    },
    {
      "epoch": 4.3936,
      "grad_norm": 0.35315921902656555,
      "learning_rate": 2.254e-05,
      "loss": 0.0018,
      "step": 82380
    },
    {
      "epoch": 4.3941333333333334,
      "grad_norm": 0.5771529674530029,
      "learning_rate": 2.2536666666666667e-05,
      "loss": 0.0019,
      "step": 82390
    },
    {
      "epoch": 4.394666666666667,
      "grad_norm": 0.3340645730495453,
      "learning_rate": 2.2533333333333333e-05,
      "loss": 0.0015,
      "step": 82400
    },
    {
      "epoch": 4.3952,
      "grad_norm": 0.2647443413734436,
      "learning_rate": 2.253e-05,
      "loss": 0.0029,
      "step": 82410
    },
    {
      "epoch": 4.395733333333333,
      "grad_norm": 0.4658411741256714,
      "learning_rate": 2.252666666666667e-05,
      "loss": 0.0021,
      "step": 82420
    },
    {
      "epoch": 4.3962666666666665,
      "grad_norm": 0.28767961263656616,
      "learning_rate": 2.2523333333333335e-05,
      "loss": 0.0014,
      "step": 82430
    },
    {
      "epoch": 4.3968,
      "grad_norm": 0.14631396532058716,
      "learning_rate": 2.252e-05,
      "loss": 0.0015,
      "step": 82440
    },
    {
      "epoch": 4.397333333333333,
      "grad_norm": 0.30099669098854065,
      "learning_rate": 2.2516666666666667e-05,
      "loss": 0.0026,
      "step": 82450
    },
    {
      "epoch": 4.397866666666666,
      "grad_norm": 0.5377726554870605,
      "learning_rate": 2.2513333333333333e-05,
      "loss": 0.0016,
      "step": 82460
    },
    {
      "epoch": 4.3984,
      "grad_norm": 0.13341164588928223,
      "learning_rate": 2.251e-05,
      "loss": 0.0028,
      "step": 82470
    },
    {
      "epoch": 4.398933333333333,
      "grad_norm": 0.16330772638320923,
      "learning_rate": 2.250666666666667e-05,
      "loss": 0.0021,
      "step": 82480
    },
    {
      "epoch": 4.399466666666667,
      "grad_norm": 0.3044709861278534,
      "learning_rate": 2.2503333333333335e-05,
      "loss": 0.0031,
      "step": 82490
    },
    {
      "epoch": 4.4,
      "grad_norm": 0.29322490096092224,
      "learning_rate": 2.25e-05,
      "loss": 0.0027,
      "step": 82500
    },
    {
      "epoch": 4.400533333333334,
      "grad_norm": 0.08363114297389984,
      "learning_rate": 2.2496666666666668e-05,
      "loss": 0.0028,
      "step": 82510
    },
    {
      "epoch": 4.401066666666667,
      "grad_norm": 0.40995725989341736,
      "learning_rate": 2.2493333333333337e-05,
      "loss": 0.0016,
      "step": 82520
    },
    {
      "epoch": 4.4016,
      "grad_norm": 0.2346620112657547,
      "learning_rate": 2.249e-05,
      "loss": 0.0023,
      "step": 82530
    },
    {
      "epoch": 4.4021333333333335,
      "grad_norm": 0.05861359089612961,
      "learning_rate": 2.2486666666666666e-05,
      "loss": 0.0017,
      "step": 82540
    },
    {
      "epoch": 4.402666666666667,
      "grad_norm": 0.06830117851495743,
      "learning_rate": 2.2483333333333335e-05,
      "loss": 0.0015,
      "step": 82550
    },
    {
      "epoch": 4.4032,
      "grad_norm": 0.28886544704437256,
      "learning_rate": 2.248e-05,
      "loss": 0.0034,
      "step": 82560
    },
    {
      "epoch": 4.403733333333333,
      "grad_norm": 0.40955838561058044,
      "learning_rate": 2.2476666666666668e-05,
      "loss": 0.0018,
      "step": 82570
    },
    {
      "epoch": 4.4042666666666666,
      "grad_norm": 0.0657818391919136,
      "learning_rate": 2.2473333333333334e-05,
      "loss": 0.0017,
      "step": 82580
    },
    {
      "epoch": 4.4048,
      "grad_norm": 0.09684143960475922,
      "learning_rate": 2.2470000000000003e-05,
      "loss": 0.0015,
      "step": 82590
    },
    {
      "epoch": 4.405333333333333,
      "grad_norm": 0.1485840529203415,
      "learning_rate": 2.2466666666666666e-05,
      "loss": 0.0017,
      "step": 82600
    },
    {
      "epoch": 4.405866666666666,
      "grad_norm": 0.05804324895143509,
      "learning_rate": 2.2463333333333332e-05,
      "loss": 0.0024,
      "step": 82610
    },
    {
      "epoch": 4.4064,
      "grad_norm": 0.034390561282634735,
      "learning_rate": 2.2460000000000002e-05,
      "loss": 0.002,
      "step": 82620
    },
    {
      "epoch": 4.406933333333333,
      "grad_norm": 0.14661279320716858,
      "learning_rate": 2.2456666666666668e-05,
      "loss": 0.0018,
      "step": 82630
    },
    {
      "epoch": 4.407466666666666,
      "grad_norm": 0.2412412315607071,
      "learning_rate": 2.2453333333333334e-05,
      "loss": 0.0017,
      "step": 82640
    },
    {
      "epoch": 4.408,
      "grad_norm": 0.5367045998573303,
      "learning_rate": 2.245e-05,
      "loss": 0.0016,
      "step": 82650
    },
    {
      "epoch": 4.408533333333334,
      "grad_norm": 0.2679588794708252,
      "learning_rate": 2.244666666666667e-05,
      "loss": 0.0022,
      "step": 82660
    },
    {
      "epoch": 4.409066666666667,
      "grad_norm": 0.17681781947612762,
      "learning_rate": 2.2443333333333336e-05,
      "loss": 0.0017,
      "step": 82670
    },
    {
      "epoch": 4.4096,
      "grad_norm": 0.3556777536869049,
      "learning_rate": 2.244e-05,
      "loss": 0.0026,
      "step": 82680
    },
    {
      "epoch": 4.4101333333333335,
      "grad_norm": 0.4994809925556183,
      "learning_rate": 2.2436666666666668e-05,
      "loss": 0.0017,
      "step": 82690
    },
    {
      "epoch": 4.410666666666667,
      "grad_norm": 0.127041295170784,
      "learning_rate": 2.2433333333333334e-05,
      "loss": 0.0017,
      "step": 82700
    },
    {
      "epoch": 4.4112,
      "grad_norm": 0.18343006074428558,
      "learning_rate": 2.243e-05,
      "loss": 0.0017,
      "step": 82710
    },
    {
      "epoch": 4.411733333333333,
      "grad_norm": 0.22823375463485718,
      "learning_rate": 2.2426666666666667e-05,
      "loss": 0.0013,
      "step": 82720
    },
    {
      "epoch": 4.412266666666667,
      "grad_norm": 0.04084766283631325,
      "learning_rate": 2.2423333333333336e-05,
      "loss": 0.0014,
      "step": 82730
    },
    {
      "epoch": 4.4128,
      "grad_norm": 0.0945638120174408,
      "learning_rate": 2.2420000000000002e-05,
      "loss": 0.0015,
      "step": 82740
    },
    {
      "epoch": 4.413333333333333,
      "grad_norm": 0.2712377607822418,
      "learning_rate": 2.2416666666666665e-05,
      "loss": 0.0013,
      "step": 82750
    },
    {
      "epoch": 4.413866666666666,
      "grad_norm": 0.20719151198863983,
      "learning_rate": 2.2413333333333334e-05,
      "loss": 0.0018,
      "step": 82760
    },
    {
      "epoch": 4.4144,
      "grad_norm": 0.034327223896980286,
      "learning_rate": 2.241e-05,
      "loss": 0.002,
      "step": 82770
    },
    {
      "epoch": 4.414933333333333,
      "grad_norm": 0.27678656578063965,
      "learning_rate": 2.2406666666666667e-05,
      "loss": 0.002,
      "step": 82780
    },
    {
      "epoch": 4.415466666666667,
      "grad_norm": 0.07171083241701126,
      "learning_rate": 2.2403333333333333e-05,
      "loss": 0.0012,
      "step": 82790
    },
    {
      "epoch": 4.416,
      "grad_norm": 0.21791338920593262,
      "learning_rate": 2.2400000000000002e-05,
      "loss": 0.0027,
      "step": 82800
    },
    {
      "epoch": 4.416533333333334,
      "grad_norm": 0.09292569756507874,
      "learning_rate": 2.239666666666667e-05,
      "loss": 0.0016,
      "step": 82810
    },
    {
      "epoch": 4.417066666666667,
      "grad_norm": 0.1760409027338028,
      "learning_rate": 2.2393333333333335e-05,
      "loss": 0.0028,
      "step": 82820
    },
    {
      "epoch": 4.4176,
      "grad_norm": 0.23084968328475952,
      "learning_rate": 2.239e-05,
      "loss": 0.0017,
      "step": 82830
    },
    {
      "epoch": 4.4181333333333335,
      "grad_norm": 0.3222561776638031,
      "learning_rate": 2.2386666666666667e-05,
      "loss": 0.0027,
      "step": 82840
    },
    {
      "epoch": 4.418666666666667,
      "grad_norm": 0.32574743032455444,
      "learning_rate": 2.2383333333333333e-05,
      "loss": 0.0017,
      "step": 82850
    },
    {
      "epoch": 4.4192,
      "grad_norm": 0.11731018126010895,
      "learning_rate": 2.2380000000000003e-05,
      "loss": 0.002,
      "step": 82860
    },
    {
      "epoch": 4.419733333333333,
      "grad_norm": 0.4931844174861908,
      "learning_rate": 2.237666666666667e-05,
      "loss": 0.0026,
      "step": 82870
    },
    {
      "epoch": 4.420266666666667,
      "grad_norm": 0.22022688388824463,
      "learning_rate": 2.2373333333333335e-05,
      "loss": 0.0018,
      "step": 82880
    },
    {
      "epoch": 4.4208,
      "grad_norm": 0.3296296298503876,
      "learning_rate": 2.237e-05,
      "loss": 0.0034,
      "step": 82890
    },
    {
      "epoch": 4.421333333333333,
      "grad_norm": 0.46480995416641235,
      "learning_rate": 2.236666666666667e-05,
      "loss": 0.0033,
      "step": 82900
    },
    {
      "epoch": 4.421866666666666,
      "grad_norm": 0.061220064759254456,
      "learning_rate": 2.2363333333333333e-05,
      "loss": 0.0015,
      "step": 82910
    },
    {
      "epoch": 4.4224,
      "grad_norm": 0.06496721506118774,
      "learning_rate": 2.236e-05,
      "loss": 0.0012,
      "step": 82920
    },
    {
      "epoch": 4.422933333333333,
      "grad_norm": 0.14794495701789856,
      "learning_rate": 2.235666666666667e-05,
      "loss": 0.0018,
      "step": 82930
    },
    {
      "epoch": 4.423466666666666,
      "grad_norm": 0.5010172724723816,
      "learning_rate": 2.2353333333333335e-05,
      "loss": 0.0019,
      "step": 82940
    },
    {
      "epoch": 4.424,
      "grad_norm": 0.10493440181016922,
      "learning_rate": 2.235e-05,
      "loss": 0.0017,
      "step": 82950
    },
    {
      "epoch": 4.424533333333334,
      "grad_norm": 0.30289870500564575,
      "learning_rate": 2.2346666666666667e-05,
      "loss": 0.0017,
      "step": 82960
    },
    {
      "epoch": 4.425066666666667,
      "grad_norm": 0.23682013154029846,
      "learning_rate": 2.2343333333333337e-05,
      "loss": 0.0014,
      "step": 82970
    },
    {
      "epoch": 4.4256,
      "grad_norm": 0.1726815402507782,
      "learning_rate": 2.234e-05,
      "loss": 0.002,
      "step": 82980
    },
    {
      "epoch": 4.4261333333333335,
      "grad_norm": 0.11766257882118225,
      "learning_rate": 2.2336666666666666e-05,
      "loss": 0.0027,
      "step": 82990
    },
    {
      "epoch": 4.426666666666667,
      "grad_norm": 0.0490528903901577,
      "learning_rate": 2.2333333333333335e-05,
      "loss": 0.0016,
      "step": 83000
    },
    {
      "epoch": 4.4272,
      "grad_norm": 0.21873249113559723,
      "learning_rate": 2.233e-05,
      "loss": 0.0024,
      "step": 83010
    },
    {
      "epoch": 4.427733333333333,
      "grad_norm": 0.15039530396461487,
      "learning_rate": 2.2326666666666667e-05,
      "loss": 0.0018,
      "step": 83020
    },
    {
      "epoch": 4.428266666666667,
      "grad_norm": 0.6445428133010864,
      "learning_rate": 2.2323333333333334e-05,
      "loss": 0.002,
      "step": 83030
    },
    {
      "epoch": 4.4288,
      "grad_norm": 0.1147427037358284,
      "learning_rate": 2.2320000000000003e-05,
      "loss": 0.0015,
      "step": 83040
    },
    {
      "epoch": 4.429333333333333,
      "grad_norm": 0.1255132108926773,
      "learning_rate": 2.231666666666667e-05,
      "loss": 0.0021,
      "step": 83050
    },
    {
      "epoch": 4.429866666666666,
      "grad_norm": 0.1844109743833542,
      "learning_rate": 2.2313333333333332e-05,
      "loss": 0.0017,
      "step": 83060
    },
    {
      "epoch": 4.4304,
      "grad_norm": 0.3356146514415741,
      "learning_rate": 2.231e-05,
      "loss": 0.0025,
      "step": 83070
    },
    {
      "epoch": 4.430933333333333,
      "grad_norm": 0.26159191131591797,
      "learning_rate": 2.2306666666666668e-05,
      "loss": 0.0017,
      "step": 83080
    },
    {
      "epoch": 4.431466666666667,
      "grad_norm": 0.406088650226593,
      "learning_rate": 2.2303333333333334e-05,
      "loss": 0.0013,
      "step": 83090
    },
    {
      "epoch": 4.432,
      "grad_norm": 0.18044887483119965,
      "learning_rate": 2.23e-05,
      "loss": 0.0014,
      "step": 83100
    },
    {
      "epoch": 4.432533333333334,
      "grad_norm": 0.032267771661281586,
      "learning_rate": 2.229666666666667e-05,
      "loss": 0.0018,
      "step": 83110
    },
    {
      "epoch": 4.433066666666667,
      "grad_norm": 0.23639585077762604,
      "learning_rate": 2.2293333333333336e-05,
      "loss": 0.0033,
      "step": 83120
    },
    {
      "epoch": 4.4336,
      "grad_norm": 0.13044831156730652,
      "learning_rate": 2.229e-05,
      "loss": 0.0013,
      "step": 83130
    },
    {
      "epoch": 4.4341333333333335,
      "grad_norm": 0.08770625293254852,
      "learning_rate": 2.2286666666666668e-05,
      "loss": 0.0022,
      "step": 83140
    },
    {
      "epoch": 4.434666666666667,
      "grad_norm": 0.044235896319150925,
      "learning_rate": 2.2283333333333334e-05,
      "loss": 0.0019,
      "step": 83150
    },
    {
      "epoch": 4.4352,
      "grad_norm": 0.0391106903553009,
      "learning_rate": 2.228e-05,
      "loss": 0.002,
      "step": 83160
    },
    {
      "epoch": 4.435733333333333,
      "grad_norm": 0.3902309834957123,
      "learning_rate": 2.2276666666666666e-05,
      "loss": 0.0017,
      "step": 83170
    },
    {
      "epoch": 4.436266666666667,
      "grad_norm": 0.08607824891805649,
      "learning_rate": 2.2273333333333336e-05,
      "loss": 0.002,
      "step": 83180
    },
    {
      "epoch": 4.4368,
      "grad_norm": 0.41255679726600647,
      "learning_rate": 2.2270000000000002e-05,
      "loss": 0.0017,
      "step": 83190
    },
    {
      "epoch": 4.437333333333333,
      "grad_norm": 0.3501529395580292,
      "learning_rate": 2.2266666666666668e-05,
      "loss": 0.0032,
      "step": 83200
    },
    {
      "epoch": 4.437866666666666,
      "grad_norm": 0.11978241056203842,
      "learning_rate": 2.2263333333333334e-05,
      "loss": 0.0033,
      "step": 83210
    },
    {
      "epoch": 4.4384,
      "grad_norm": 0.12083345651626587,
      "learning_rate": 2.226e-05,
      "loss": 0.0016,
      "step": 83220
    },
    {
      "epoch": 4.438933333333333,
      "grad_norm": 0.0934712216258049,
      "learning_rate": 2.2256666666666666e-05,
      "loss": 0.0026,
      "step": 83230
    },
    {
      "epoch": 4.439466666666666,
      "grad_norm": 0.3245084285736084,
      "learning_rate": 2.2253333333333336e-05,
      "loss": 0.0015,
      "step": 83240
    },
    {
      "epoch": 4.44,
      "grad_norm": 0.20371319353580475,
      "learning_rate": 2.2250000000000002e-05,
      "loss": 0.0018,
      "step": 83250
    },
    {
      "epoch": 4.440533333333334,
      "grad_norm": 0.4630168378353119,
      "learning_rate": 2.2246666666666668e-05,
      "loss": 0.0017,
      "step": 83260
    },
    {
      "epoch": 4.441066666666667,
      "grad_norm": 0.09655122458934784,
      "learning_rate": 2.2243333333333334e-05,
      "loss": 0.002,
      "step": 83270
    },
    {
      "epoch": 4.4416,
      "grad_norm": 0.06168581172823906,
      "learning_rate": 2.224e-05,
      "loss": 0.0023,
      "step": 83280
    },
    {
      "epoch": 4.4421333333333335,
      "grad_norm": 0.72641521692276,
      "learning_rate": 2.2236666666666667e-05,
      "loss": 0.0018,
      "step": 83290
    },
    {
      "epoch": 4.442666666666667,
      "grad_norm": 0.3600772023200989,
      "learning_rate": 2.2233333333333333e-05,
      "loss": 0.0018,
      "step": 83300
    },
    {
      "epoch": 4.4432,
      "grad_norm": 0.17975777387619019,
      "learning_rate": 2.2230000000000002e-05,
      "loss": 0.0019,
      "step": 83310
    },
    {
      "epoch": 4.443733333333333,
      "grad_norm": 0.4968402087688446,
      "learning_rate": 2.222666666666667e-05,
      "loss": 0.0016,
      "step": 83320
    },
    {
      "epoch": 4.444266666666667,
      "grad_norm": 0.3543725907802582,
      "learning_rate": 2.2223333333333335e-05,
      "loss": 0.0018,
      "step": 83330
    },
    {
      "epoch": 4.4448,
      "grad_norm": 0.3089187443256378,
      "learning_rate": 2.222e-05,
      "loss": 0.0015,
      "step": 83340
    },
    {
      "epoch": 4.445333333333333,
      "grad_norm": 0.5840127468109131,
      "learning_rate": 2.221666666666667e-05,
      "loss": 0.0022,
      "step": 83350
    },
    {
      "epoch": 4.445866666666666,
      "grad_norm": 0.10758593678474426,
      "learning_rate": 2.2213333333333333e-05,
      "loss": 0.0023,
      "step": 83360
    },
    {
      "epoch": 4.4464,
      "grad_norm": 0.17139634490013123,
      "learning_rate": 2.221e-05,
      "loss": 0.0024,
      "step": 83370
    },
    {
      "epoch": 4.446933333333333,
      "grad_norm": 0.5202420353889465,
      "learning_rate": 2.220666666666667e-05,
      "loss": 0.0018,
      "step": 83380
    },
    {
      "epoch": 4.447466666666667,
      "grad_norm": 0.2442229837179184,
      "learning_rate": 2.2203333333333335e-05,
      "loss": 0.0014,
      "step": 83390
    },
    {
      "epoch": 4.448,
      "grad_norm": 0.15672627091407776,
      "learning_rate": 2.22e-05,
      "loss": 0.0016,
      "step": 83400
    },
    {
      "epoch": 4.448533333333334,
      "grad_norm": 0.3228292465209961,
      "learning_rate": 2.2196666666666667e-05,
      "loss": 0.0021,
      "step": 83410
    },
    {
      "epoch": 4.449066666666667,
      "grad_norm": 0.35353055596351624,
      "learning_rate": 2.2193333333333337e-05,
      "loss": 0.0018,
      "step": 83420
    },
    {
      "epoch": 4.4496,
      "grad_norm": 0.32315829396247864,
      "learning_rate": 2.219e-05,
      "loss": 0.0023,
      "step": 83430
    },
    {
      "epoch": 4.4501333333333335,
      "grad_norm": 0.20626331865787506,
      "learning_rate": 2.2186666666666665e-05,
      "loss": 0.002,
      "step": 83440
    },
    {
      "epoch": 4.450666666666667,
      "grad_norm": 0.5735146999359131,
      "learning_rate": 2.2183333333333335e-05,
      "loss": 0.0023,
      "step": 83450
    },
    {
      "epoch": 4.4512,
      "grad_norm": 0.38648974895477295,
      "learning_rate": 2.218e-05,
      "loss": 0.0015,
      "step": 83460
    },
    {
      "epoch": 4.451733333333333,
      "grad_norm": 0.17391835153102875,
      "learning_rate": 2.2176666666666667e-05,
      "loss": 0.0024,
      "step": 83470
    },
    {
      "epoch": 4.452266666666667,
      "grad_norm": 0.17338943481445312,
      "learning_rate": 2.2173333333333333e-05,
      "loss": 0.0019,
      "step": 83480
    },
    {
      "epoch": 4.4528,
      "grad_norm": 0.18187157809734344,
      "learning_rate": 2.2170000000000003e-05,
      "loss": 0.0019,
      "step": 83490
    },
    {
      "epoch": 4.453333333333333,
      "grad_norm": 0.06627954542636871,
      "learning_rate": 2.216666666666667e-05,
      "loss": 0.0027,
      "step": 83500
    },
    {
      "epoch": 4.453866666666666,
      "grad_norm": 0.554263710975647,
      "learning_rate": 2.2163333333333332e-05,
      "loss": 0.0023,
      "step": 83510
    },
    {
      "epoch": 4.4544,
      "grad_norm": 0.1367688924074173,
      "learning_rate": 2.216e-05,
      "loss": 0.0016,
      "step": 83520
    },
    {
      "epoch": 4.454933333333333,
      "grad_norm": 0.4031946659088135,
      "learning_rate": 2.2156666666666667e-05,
      "loss": 0.0021,
      "step": 83530
    },
    {
      "epoch": 4.455466666666666,
      "grad_norm": 0.06240212172269821,
      "learning_rate": 2.2153333333333334e-05,
      "loss": 0.0026,
      "step": 83540
    },
    {
      "epoch": 4.456,
      "grad_norm": 0.4956858158111572,
      "learning_rate": 2.215e-05,
      "loss": 0.0014,
      "step": 83550
    },
    {
      "epoch": 4.456533333333334,
      "grad_norm": 0.3216925859451294,
      "learning_rate": 2.214666666666667e-05,
      "loss": 0.003,
      "step": 83560
    },
    {
      "epoch": 4.457066666666667,
      "grad_norm": 0.23659847676753998,
      "learning_rate": 2.2143333333333335e-05,
      "loss": 0.003,
      "step": 83570
    },
    {
      "epoch": 4.4576,
      "grad_norm": 0.04302321746945381,
      "learning_rate": 2.214e-05,
      "loss": 0.002,
      "step": 83580
    },
    {
      "epoch": 4.4581333333333335,
      "grad_norm": 0.150243878364563,
      "learning_rate": 2.2136666666666668e-05,
      "loss": 0.0022,
      "step": 83590
    },
    {
      "epoch": 4.458666666666667,
      "grad_norm": 0.20950523018836975,
      "learning_rate": 2.2133333333333334e-05,
      "loss": 0.0018,
      "step": 83600
    },
    {
      "epoch": 4.4592,
      "grad_norm": 0.048204775899648666,
      "learning_rate": 2.213e-05,
      "loss": 0.0017,
      "step": 83610
    },
    {
      "epoch": 4.459733333333333,
      "grad_norm": 0.3571181297302246,
      "learning_rate": 2.212666666666667e-05,
      "loss": 0.0021,
      "step": 83620
    },
    {
      "epoch": 4.460266666666667,
      "grad_norm": 0.037149783223867416,
      "learning_rate": 2.2123333333333336e-05,
      "loss": 0.0029,
      "step": 83630
    },
    {
      "epoch": 4.4608,
      "grad_norm": 0.11980737000703812,
      "learning_rate": 2.212e-05,
      "loss": 0.0014,
      "step": 83640
    },
    {
      "epoch": 4.461333333333333,
      "grad_norm": 0.26505178213119507,
      "learning_rate": 2.2116666666666668e-05,
      "loss": 0.002,
      "step": 83650
    },
    {
      "epoch": 4.461866666666666,
      "grad_norm": 0.28826257586479187,
      "learning_rate": 2.2113333333333334e-05,
      "loss": 0.0019,
      "step": 83660
    },
    {
      "epoch": 4.4624,
      "grad_norm": 0.32562750577926636,
      "learning_rate": 2.211e-05,
      "loss": 0.002,
      "step": 83670
    },
    {
      "epoch": 4.462933333333333,
      "grad_norm": 0.4446343779563904,
      "learning_rate": 2.2106666666666666e-05,
      "loss": 0.0032,
      "step": 83680
    },
    {
      "epoch": 4.463466666666667,
      "grad_norm": 0.21701541543006897,
      "learning_rate": 2.2103333333333336e-05,
      "loss": 0.0019,
      "step": 83690
    },
    {
      "epoch": 4.464,
      "grad_norm": 0.23882298171520233,
      "learning_rate": 2.2100000000000002e-05,
      "loss": 0.0031,
      "step": 83700
    },
    {
      "epoch": 4.464533333333334,
      "grad_norm": 0.17116542160511017,
      "learning_rate": 2.2096666666666668e-05,
      "loss": 0.0018,
      "step": 83710
    },
    {
      "epoch": 4.465066666666667,
      "grad_norm": 0.4913314878940582,
      "learning_rate": 2.2093333333333334e-05,
      "loss": 0.0016,
      "step": 83720
    },
    {
      "epoch": 4.4656,
      "grad_norm": 0.5547146201133728,
      "learning_rate": 2.2090000000000004e-05,
      "loss": 0.0018,
      "step": 83730
    },
    {
      "epoch": 4.4661333333333335,
      "grad_norm": 0.18915046751499176,
      "learning_rate": 2.2086666666666666e-05,
      "loss": 0.0015,
      "step": 83740
    },
    {
      "epoch": 4.466666666666667,
      "grad_norm": 0.021954523399472237,
      "learning_rate": 2.2083333333333333e-05,
      "loss": 0.0021,
      "step": 83750
    },
    {
      "epoch": 4.4672,
      "grad_norm": 0.4044090509414673,
      "learning_rate": 2.2080000000000002e-05,
      "loss": 0.003,
      "step": 83760
    },
    {
      "epoch": 4.467733333333333,
      "grad_norm": 0.23500563204288483,
      "learning_rate": 2.2076666666666668e-05,
      "loss": 0.0023,
      "step": 83770
    },
    {
      "epoch": 4.468266666666667,
      "grad_norm": 0.15892557799816132,
      "learning_rate": 2.2073333333333334e-05,
      "loss": 0.0016,
      "step": 83780
    },
    {
      "epoch": 4.4688,
      "grad_norm": 0.1572907269001007,
      "learning_rate": 2.207e-05,
      "loss": 0.0015,
      "step": 83790
    },
    {
      "epoch": 4.469333333333333,
      "grad_norm": 0.06397481262683868,
      "learning_rate": 2.206666666666667e-05,
      "loss": 0.0022,
      "step": 83800
    },
    {
      "epoch": 4.469866666666666,
      "grad_norm": 0.2656990587711334,
      "learning_rate": 2.2063333333333333e-05,
      "loss": 0.0019,
      "step": 83810
    },
    {
      "epoch": 4.4704,
      "grad_norm": 0.40850573778152466,
      "learning_rate": 2.206e-05,
      "loss": 0.0022,
      "step": 83820
    },
    {
      "epoch": 4.470933333333333,
      "grad_norm": 0.11485189199447632,
      "learning_rate": 2.205666666666667e-05,
      "loss": 0.0023,
      "step": 83830
    },
    {
      "epoch": 4.471466666666666,
      "grad_norm": 0.5257798433303833,
      "learning_rate": 2.2053333333333335e-05,
      "loss": 0.002,
      "step": 83840
    },
    {
      "epoch": 4.4719999999999995,
      "grad_norm": 0.3883098363876343,
      "learning_rate": 2.205e-05,
      "loss": 0.0026,
      "step": 83850
    },
    {
      "epoch": 4.472533333333334,
      "grad_norm": 0.05882050096988678,
      "learning_rate": 2.2046666666666667e-05,
      "loss": 0.0014,
      "step": 83860
    },
    {
      "epoch": 4.473066666666667,
      "grad_norm": 0.2311888188123703,
      "learning_rate": 2.2043333333333336e-05,
      "loss": 0.0017,
      "step": 83870
    },
    {
      "epoch": 4.4736,
      "grad_norm": 0.09779061377048492,
      "learning_rate": 2.2040000000000002e-05,
      "loss": 0.0016,
      "step": 83880
    },
    {
      "epoch": 4.4741333333333335,
      "grad_norm": 0.263336718082428,
      "learning_rate": 2.2036666666666665e-05,
      "loss": 0.0027,
      "step": 83890
    },
    {
      "epoch": 4.474666666666667,
      "grad_norm": 0.1873951107263565,
      "learning_rate": 2.2033333333333335e-05,
      "loss": 0.0023,
      "step": 83900
    },
    {
      "epoch": 4.4752,
      "grad_norm": 0.12117169052362442,
      "learning_rate": 2.203e-05,
      "loss": 0.0017,
      "step": 83910
    },
    {
      "epoch": 4.475733333333333,
      "grad_norm": 0.5028082728385925,
      "learning_rate": 2.2026666666666667e-05,
      "loss": 0.002,
      "step": 83920
    },
    {
      "epoch": 4.476266666666667,
      "grad_norm": 0.18804976344108582,
      "learning_rate": 2.2023333333333333e-05,
      "loss": 0.0015,
      "step": 83930
    },
    {
      "epoch": 4.4768,
      "grad_norm": 0.04047654941678047,
      "learning_rate": 2.2020000000000003e-05,
      "loss": 0.0023,
      "step": 83940
    },
    {
      "epoch": 4.477333333333333,
      "grad_norm": 0.28438547253608704,
      "learning_rate": 2.201666666666667e-05,
      "loss": 0.0014,
      "step": 83950
    },
    {
      "epoch": 4.477866666666666,
      "grad_norm": 0.09299512952566147,
      "learning_rate": 2.201333333333333e-05,
      "loss": 0.0028,
      "step": 83960
    },
    {
      "epoch": 4.4784,
      "grad_norm": 0.26400721073150635,
      "learning_rate": 2.201e-05,
      "loss": 0.0015,
      "step": 83970
    },
    {
      "epoch": 4.478933333333333,
      "grad_norm": 0.1242389976978302,
      "learning_rate": 2.2006666666666667e-05,
      "loss": 0.0023,
      "step": 83980
    },
    {
      "epoch": 4.479466666666666,
      "grad_norm": 0.11801422387361526,
      "learning_rate": 2.2003333333333333e-05,
      "loss": 0.0022,
      "step": 83990
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.4286264181137085,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.0024,
      "step": 84000
    },
    {
      "epoch": 4.480533333333334,
      "grad_norm": 0.04378749802708626,
      "learning_rate": 2.199666666666667e-05,
      "loss": 0.0019,
      "step": 84010
    },
    {
      "epoch": 4.481066666666667,
      "grad_norm": 0.4097045063972473,
      "learning_rate": 2.1993333333333335e-05,
      "loss": 0.0014,
      "step": 84020
    },
    {
      "epoch": 4.4816,
      "grad_norm": 0.14598649740219116,
      "learning_rate": 2.199e-05,
      "loss": 0.0023,
      "step": 84030
    },
    {
      "epoch": 4.4821333333333335,
      "grad_norm": 0.1178811714053154,
      "learning_rate": 2.1986666666666667e-05,
      "loss": 0.0017,
      "step": 84040
    },
    {
      "epoch": 4.482666666666667,
      "grad_norm": 0.32812777161598206,
      "learning_rate": 2.1983333333333333e-05,
      "loss": 0.0016,
      "step": 84050
    },
    {
      "epoch": 4.4832,
      "grad_norm": 0.23547245562076569,
      "learning_rate": 2.198e-05,
      "loss": 0.0016,
      "step": 84060
    },
    {
      "epoch": 4.483733333333333,
      "grad_norm": 0.3199474811553955,
      "learning_rate": 2.197666666666667e-05,
      "loss": 0.0011,
      "step": 84070
    },
    {
      "epoch": 4.484266666666667,
      "grad_norm": 0.23583176732063293,
      "learning_rate": 2.1973333333333335e-05,
      "loss": 0.0027,
      "step": 84080
    },
    {
      "epoch": 4.4848,
      "grad_norm": 0.21687467396259308,
      "learning_rate": 2.197e-05,
      "loss": 0.002,
      "step": 84090
    },
    {
      "epoch": 4.485333333333333,
      "grad_norm": 0.1882251799106598,
      "learning_rate": 2.1966666666666668e-05,
      "loss": 0.0021,
      "step": 84100
    },
    {
      "epoch": 4.4858666666666664,
      "grad_norm": 0.1506578028202057,
      "learning_rate": 2.1963333333333337e-05,
      "loss": 0.0018,
      "step": 84110
    },
    {
      "epoch": 4.4864,
      "grad_norm": 0.09868160635232925,
      "learning_rate": 2.196e-05,
      "loss": 0.0029,
      "step": 84120
    },
    {
      "epoch": 4.486933333333333,
      "grad_norm": 0.04098040610551834,
      "learning_rate": 2.1956666666666666e-05,
      "loss": 0.0017,
      "step": 84130
    },
    {
      "epoch": 4.487466666666666,
      "grad_norm": 0.6291400790214539,
      "learning_rate": 2.1953333333333335e-05,
      "loss": 0.0022,
      "step": 84140
    },
    {
      "epoch": 4.4879999999999995,
      "grad_norm": 0.1905064880847931,
      "learning_rate": 2.195e-05,
      "loss": 0.0024,
      "step": 84150
    },
    {
      "epoch": 4.488533333333334,
      "grad_norm": 0.3224402368068695,
      "learning_rate": 2.1946666666666668e-05,
      "loss": 0.0015,
      "step": 84160
    },
    {
      "epoch": 4.489066666666667,
      "grad_norm": 0.04236992076039314,
      "learning_rate": 2.1943333333333334e-05,
      "loss": 0.0023,
      "step": 84170
    },
    {
      "epoch": 4.4896,
      "grad_norm": 0.027432052418589592,
      "learning_rate": 2.1940000000000003e-05,
      "loss": 0.0012,
      "step": 84180
    },
    {
      "epoch": 4.4901333333333335,
      "grad_norm": 0.09611959010362625,
      "learning_rate": 2.1936666666666666e-05,
      "loss": 0.0033,
      "step": 84190
    },
    {
      "epoch": 4.490666666666667,
      "grad_norm": 0.13398224115371704,
      "learning_rate": 2.1933333333333332e-05,
      "loss": 0.002,
      "step": 84200
    },
    {
      "epoch": 4.4912,
      "grad_norm": 0.46889281272888184,
      "learning_rate": 2.1930000000000002e-05,
      "loss": 0.0015,
      "step": 84210
    },
    {
      "epoch": 4.491733333333333,
      "grad_norm": 0.07880692183971405,
      "learning_rate": 2.1926666666666668e-05,
      "loss": 0.0014,
      "step": 84220
    },
    {
      "epoch": 4.492266666666667,
      "grad_norm": 0.12736856937408447,
      "learning_rate": 2.1923333333333334e-05,
      "loss": 0.0021,
      "step": 84230
    },
    {
      "epoch": 4.4928,
      "grad_norm": 0.4905046820640564,
      "learning_rate": 2.192e-05,
      "loss": 0.0019,
      "step": 84240
    },
    {
      "epoch": 4.493333333333333,
      "grad_norm": 0.15357571840286255,
      "learning_rate": 2.191666666666667e-05,
      "loss": 0.0016,
      "step": 84250
    },
    {
      "epoch": 4.4938666666666665,
      "grad_norm": 0.11101128160953522,
      "learning_rate": 2.1913333333333336e-05,
      "loss": 0.0019,
      "step": 84260
    },
    {
      "epoch": 4.4944,
      "grad_norm": 0.23507264256477356,
      "learning_rate": 2.191e-05,
      "loss": 0.0016,
      "step": 84270
    },
    {
      "epoch": 4.494933333333333,
      "grad_norm": 0.1229439452290535,
      "learning_rate": 2.1906666666666668e-05,
      "loss": 0.0018,
      "step": 84280
    },
    {
      "epoch": 4.495466666666666,
      "grad_norm": 0.02009817399084568,
      "learning_rate": 2.1903333333333334e-05,
      "loss": 0.0017,
      "step": 84290
    },
    {
      "epoch": 4.496,
      "grad_norm": 0.2104908525943756,
      "learning_rate": 2.19e-05,
      "loss": 0.0022,
      "step": 84300
    },
    {
      "epoch": 4.496533333333334,
      "grad_norm": 0.039281461387872696,
      "learning_rate": 2.1896666666666667e-05,
      "loss": 0.0016,
      "step": 84310
    },
    {
      "epoch": 4.497066666666667,
      "grad_norm": 0.07604358345270157,
      "learning_rate": 2.1893333333333336e-05,
      "loss": 0.0016,
      "step": 84320
    },
    {
      "epoch": 4.4976,
      "grad_norm": 0.3927459418773651,
      "learning_rate": 2.1890000000000002e-05,
      "loss": 0.002,
      "step": 84330
    },
    {
      "epoch": 4.4981333333333335,
      "grad_norm": 0.24176877737045288,
      "learning_rate": 2.1886666666666665e-05,
      "loss": 0.0019,
      "step": 84340
    },
    {
      "epoch": 4.498666666666667,
      "grad_norm": 0.05829392001032829,
      "learning_rate": 2.1883333333333334e-05,
      "loss": 0.0019,
      "step": 84350
    },
    {
      "epoch": 4.4992,
      "grad_norm": 0.08820188045501709,
      "learning_rate": 2.188e-05,
      "loss": 0.0012,
      "step": 84360
    },
    {
      "epoch": 4.499733333333333,
      "grad_norm": 0.5383484959602356,
      "learning_rate": 2.1876666666666667e-05,
      "loss": 0.0021,
      "step": 84370
    },
    {
      "epoch": 4.500266666666667,
      "grad_norm": 0.05630515515804291,
      "learning_rate": 2.1873333333333336e-05,
      "loss": 0.0014,
      "step": 84380
    },
    {
      "epoch": 4.5008,
      "grad_norm": 0.12691573798656464,
      "learning_rate": 2.1870000000000002e-05,
      "loss": 0.0032,
      "step": 84390
    },
    {
      "epoch": 4.501333333333333,
      "grad_norm": 0.0943768247961998,
      "learning_rate": 2.186666666666667e-05,
      "loss": 0.0025,
      "step": 84400
    },
    {
      "epoch": 4.5018666666666665,
      "grad_norm": 0.5809179544448853,
      "learning_rate": 2.1863333333333335e-05,
      "loss": 0.0016,
      "step": 84410
    },
    {
      "epoch": 4.5024,
      "grad_norm": 0.2887118458747864,
      "learning_rate": 2.186e-05,
      "loss": 0.0028,
      "step": 84420
    },
    {
      "epoch": 4.502933333333333,
      "grad_norm": 0.23444505035877228,
      "learning_rate": 2.1856666666666667e-05,
      "loss": 0.002,
      "step": 84430
    },
    {
      "epoch": 4.503466666666666,
      "grad_norm": 0.5641295909881592,
      "learning_rate": 2.1853333333333333e-05,
      "loss": 0.0021,
      "step": 84440
    },
    {
      "epoch": 4.504,
      "grad_norm": 0.18138620257377625,
      "learning_rate": 2.1850000000000003e-05,
      "loss": 0.0025,
      "step": 84450
    },
    {
      "epoch": 4.504533333333333,
      "grad_norm": 0.2088477611541748,
      "learning_rate": 2.184666666666667e-05,
      "loss": 0.002,
      "step": 84460
    },
    {
      "epoch": 4.505066666666667,
      "grad_norm": 0.12151825428009033,
      "learning_rate": 2.1843333333333335e-05,
      "loss": 0.0026,
      "step": 84470
    },
    {
      "epoch": 4.5056,
      "grad_norm": 0.11823919415473938,
      "learning_rate": 2.184e-05,
      "loss": 0.0014,
      "step": 84480
    },
    {
      "epoch": 4.5061333333333335,
      "grad_norm": 0.21091654896736145,
      "learning_rate": 2.1836666666666667e-05,
      "loss": 0.003,
      "step": 84490
    },
    {
      "epoch": 4.506666666666667,
      "grad_norm": 0.07779006659984589,
      "learning_rate": 2.1833333333333333e-05,
      "loss": 0.0017,
      "step": 84500
    },
    {
      "epoch": 4.5072,
      "grad_norm": 0.477752685546875,
      "learning_rate": 2.183e-05,
      "loss": 0.0016,
      "step": 84510
    },
    {
      "epoch": 4.507733333333333,
      "grad_norm": 0.1984575241804123,
      "learning_rate": 2.182666666666667e-05,
      "loss": 0.0018,
      "step": 84520
    },
    {
      "epoch": 4.508266666666667,
      "grad_norm": 0.15327098965644836,
      "learning_rate": 2.1823333333333335e-05,
      "loss": 0.0018,
      "step": 84530
    },
    {
      "epoch": 4.5088,
      "grad_norm": 0.450160413980484,
      "learning_rate": 2.182e-05,
      "loss": 0.0026,
      "step": 84540
    },
    {
      "epoch": 4.509333333333333,
      "grad_norm": 0.060705479234457016,
      "learning_rate": 2.1816666666666667e-05,
      "loss": 0.0021,
      "step": 84550
    },
    {
      "epoch": 4.5098666666666665,
      "grad_norm": 0.21041056513786316,
      "learning_rate": 2.1813333333333337e-05,
      "loss": 0.0021,
      "step": 84560
    },
    {
      "epoch": 4.5104,
      "grad_norm": 0.13196799159049988,
      "learning_rate": 2.181e-05,
      "loss": 0.0022,
      "step": 84570
    },
    {
      "epoch": 4.510933333333333,
      "grad_norm": 0.47585976123809814,
      "learning_rate": 2.1806666666666666e-05,
      "loss": 0.0023,
      "step": 84580
    },
    {
      "epoch": 4.511466666666666,
      "grad_norm": 0.12299938499927521,
      "learning_rate": 2.1803333333333335e-05,
      "loss": 0.0012,
      "step": 84590
    },
    {
      "epoch": 4.5120000000000005,
      "grad_norm": 0.2601392865180969,
      "learning_rate": 2.18e-05,
      "loss": 0.0015,
      "step": 84600
    },
    {
      "epoch": 4.512533333333334,
      "grad_norm": 0.1155722513794899,
      "learning_rate": 2.1796666666666667e-05,
      "loss": 0.0026,
      "step": 84610
    },
    {
      "epoch": 4.513066666666667,
      "grad_norm": 0.26536399126052856,
      "learning_rate": 2.1793333333333334e-05,
      "loss": 0.0019,
      "step": 84620
    },
    {
      "epoch": 4.5136,
      "grad_norm": 0.24712935090065002,
      "learning_rate": 2.1790000000000003e-05,
      "loss": 0.0026,
      "step": 84630
    },
    {
      "epoch": 4.5141333333333336,
      "grad_norm": 0.11756938695907593,
      "learning_rate": 2.1786666666666666e-05,
      "loss": 0.0014,
      "step": 84640
    },
    {
      "epoch": 4.514666666666667,
      "grad_norm": 0.09743864834308624,
      "learning_rate": 2.1783333333333332e-05,
      "loss": 0.0023,
      "step": 84650
    },
    {
      "epoch": 4.5152,
      "grad_norm": 0.1555623710155487,
      "learning_rate": 2.178e-05,
      "loss": 0.002,
      "step": 84660
    },
    {
      "epoch": 4.515733333333333,
      "grad_norm": 0.13384954631328583,
      "learning_rate": 2.1776666666666668e-05,
      "loss": 0.0021,
      "step": 84670
    },
    {
      "epoch": 4.516266666666667,
      "grad_norm": 0.29373225569725037,
      "learning_rate": 2.1773333333333334e-05,
      "loss": 0.0016,
      "step": 84680
    },
    {
      "epoch": 4.5168,
      "grad_norm": 0.037699103355407715,
      "learning_rate": 2.177e-05,
      "loss": 0.0015,
      "step": 84690
    },
    {
      "epoch": 4.517333333333333,
      "grad_norm": 0.40471330285072327,
      "learning_rate": 2.176666666666667e-05,
      "loss": 0.0027,
      "step": 84700
    },
    {
      "epoch": 4.5178666666666665,
      "grad_norm": 0.3008936047554016,
      "learning_rate": 2.1763333333333336e-05,
      "loss": 0.0025,
      "step": 84710
    },
    {
      "epoch": 4.5184,
      "grad_norm": 0.24708396196365356,
      "learning_rate": 2.176e-05,
      "loss": 0.0016,
      "step": 84720
    },
    {
      "epoch": 4.518933333333333,
      "grad_norm": 0.06436939537525177,
      "learning_rate": 2.1756666666666668e-05,
      "loss": 0.002,
      "step": 84730
    },
    {
      "epoch": 4.519466666666666,
      "grad_norm": 0.020722830668091774,
      "learning_rate": 2.1753333333333334e-05,
      "loss": 0.0019,
      "step": 84740
    },
    {
      "epoch": 4.52,
      "grad_norm": 0.03957732394337654,
      "learning_rate": 2.175e-05,
      "loss": 0.0018,
      "step": 84750
    },
    {
      "epoch": 4.520533333333333,
      "grad_norm": 0.2506521940231323,
      "learning_rate": 2.174666666666667e-05,
      "loss": 0.0017,
      "step": 84760
    },
    {
      "epoch": 4.521066666666667,
      "grad_norm": 0.2685203552246094,
      "learning_rate": 2.1743333333333336e-05,
      "loss": 0.0015,
      "step": 84770
    },
    {
      "epoch": 4.5216,
      "grad_norm": 0.30279821157455444,
      "learning_rate": 2.1740000000000002e-05,
      "loss": 0.0015,
      "step": 84780
    },
    {
      "epoch": 4.522133333333334,
      "grad_norm": 0.21103966236114502,
      "learning_rate": 2.1736666666666668e-05,
      "loss": 0.0019,
      "step": 84790
    },
    {
      "epoch": 4.522666666666667,
      "grad_norm": 0.2695440351963043,
      "learning_rate": 2.1733333333333334e-05,
      "loss": 0.0029,
      "step": 84800
    },
    {
      "epoch": 4.5232,
      "grad_norm": 0.31935131549835205,
      "learning_rate": 2.173e-05,
      "loss": 0.0028,
      "step": 84810
    },
    {
      "epoch": 4.523733333333333,
      "grad_norm": 0.04259240999817848,
      "learning_rate": 2.1726666666666666e-05,
      "loss": 0.0015,
      "step": 84820
    },
    {
      "epoch": 4.524266666666667,
      "grad_norm": 0.23242871463298798,
      "learning_rate": 2.1723333333333336e-05,
      "loss": 0.0031,
      "step": 84830
    },
    {
      "epoch": 4.5248,
      "grad_norm": 0.28990697860717773,
      "learning_rate": 2.1720000000000002e-05,
      "loss": 0.0017,
      "step": 84840
    },
    {
      "epoch": 4.525333333333333,
      "grad_norm": 0.14971375465393066,
      "learning_rate": 2.1716666666666668e-05,
      "loss": 0.0019,
      "step": 84850
    },
    {
      "epoch": 4.5258666666666665,
      "grad_norm": 0.14852572977542877,
      "learning_rate": 2.1713333333333334e-05,
      "loss": 0.0022,
      "step": 84860
    },
    {
      "epoch": 4.5264,
      "grad_norm": 0.3956364393234253,
      "learning_rate": 2.171e-05,
      "loss": 0.0026,
      "step": 84870
    },
    {
      "epoch": 4.526933333333333,
      "grad_norm": 0.04569784924387932,
      "learning_rate": 2.1706666666666667e-05,
      "loss": 0.0019,
      "step": 84880
    },
    {
      "epoch": 4.527466666666666,
      "grad_norm": 0.8452513813972473,
      "learning_rate": 2.1703333333333333e-05,
      "loss": 0.0015,
      "step": 84890
    },
    {
      "epoch": 4.5280000000000005,
      "grad_norm": 0.06152360513806343,
      "learning_rate": 2.1700000000000002e-05,
      "loss": 0.0018,
      "step": 84900
    },
    {
      "epoch": 4.528533333333334,
      "grad_norm": 0.09239111095666885,
      "learning_rate": 2.169666666666667e-05,
      "loss": 0.0024,
      "step": 84910
    },
    {
      "epoch": 4.529066666666667,
      "grad_norm": 0.11418037116527557,
      "learning_rate": 2.1693333333333335e-05,
      "loss": 0.0021,
      "step": 84920
    },
    {
      "epoch": 4.5296,
      "grad_norm": 0.09373244643211365,
      "learning_rate": 2.169e-05,
      "loss": 0.0019,
      "step": 84930
    },
    {
      "epoch": 4.530133333333334,
      "grad_norm": 0.2039436250925064,
      "learning_rate": 2.168666666666667e-05,
      "loss": 0.0024,
      "step": 84940
    },
    {
      "epoch": 4.530666666666667,
      "grad_norm": 0.6653873920440674,
      "learning_rate": 2.1683333333333333e-05,
      "loss": 0.0019,
      "step": 84950
    },
    {
      "epoch": 4.5312,
      "grad_norm": 0.4929209351539612,
      "learning_rate": 2.168e-05,
      "loss": 0.0021,
      "step": 84960
    },
    {
      "epoch": 4.531733333333333,
      "grad_norm": 0.3509577512741089,
      "learning_rate": 2.167666666666667e-05,
      "loss": 0.0025,
      "step": 84970
    },
    {
      "epoch": 4.532266666666667,
      "grad_norm": 0.5966648459434509,
      "learning_rate": 2.1673333333333335e-05,
      "loss": 0.0015,
      "step": 84980
    },
    {
      "epoch": 4.5328,
      "grad_norm": 0.496216356754303,
      "learning_rate": 2.167e-05,
      "loss": 0.0019,
      "step": 84990
    },
    {
      "epoch": 4.533333333333333,
      "grad_norm": 0.15078896284103394,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 0.0024,
      "step": 85000
    },
    {
      "epoch": 4.5338666666666665,
      "grad_norm": 0.5203667283058167,
      "learning_rate": 2.1663333333333337e-05,
      "loss": 0.0025,
      "step": 85010
    },
    {
      "epoch": 4.5344,
      "grad_norm": 0.290237158536911,
      "learning_rate": 2.166e-05,
      "loss": 0.0015,
      "step": 85020
    },
    {
      "epoch": 4.534933333333333,
      "grad_norm": 0.28734856843948364,
      "learning_rate": 2.1656666666666665e-05,
      "loss": 0.0018,
      "step": 85030
    },
    {
      "epoch": 4.535466666666666,
      "grad_norm": 0.6002858281135559,
      "learning_rate": 2.1653333333333335e-05,
      "loss": 0.0028,
      "step": 85040
    },
    {
      "epoch": 4.536,
      "grad_norm": 0.61338210105896,
      "learning_rate": 2.165e-05,
      "loss": 0.0016,
      "step": 85050
    },
    {
      "epoch": 4.536533333333333,
      "grad_norm": 0.3558642864227295,
      "learning_rate": 2.1646666666666667e-05,
      "loss": 0.0015,
      "step": 85060
    },
    {
      "epoch": 4.537066666666667,
      "grad_norm": 0.40675702691078186,
      "learning_rate": 2.1643333333333333e-05,
      "loss": 0.0017,
      "step": 85070
    },
    {
      "epoch": 4.5376,
      "grad_norm": 0.03440709784626961,
      "learning_rate": 2.1640000000000003e-05,
      "loss": 0.0017,
      "step": 85080
    },
    {
      "epoch": 4.538133333333334,
      "grad_norm": 0.09438850730657578,
      "learning_rate": 2.163666666666667e-05,
      "loss": 0.0013,
      "step": 85090
    },
    {
      "epoch": 4.538666666666667,
      "grad_norm": 0.23394767940044403,
      "learning_rate": 2.1633333333333332e-05,
      "loss": 0.0026,
      "step": 85100
    },
    {
      "epoch": 4.5392,
      "grad_norm": 0.13871237635612488,
      "learning_rate": 2.163e-05,
      "loss": 0.0013,
      "step": 85110
    },
    {
      "epoch": 4.539733333333333,
      "grad_norm": 0.4502347707748413,
      "learning_rate": 2.1626666666666667e-05,
      "loss": 0.0015,
      "step": 85120
    },
    {
      "epoch": 4.540266666666667,
      "grad_norm": 0.09158827364444733,
      "learning_rate": 2.1623333333333334e-05,
      "loss": 0.0019,
      "step": 85130
    },
    {
      "epoch": 4.5408,
      "grad_norm": 0.23944680392742157,
      "learning_rate": 2.162e-05,
      "loss": 0.0015,
      "step": 85140
    },
    {
      "epoch": 4.541333333333333,
      "grad_norm": 0.09523467719554901,
      "learning_rate": 2.161666666666667e-05,
      "loss": 0.0019,
      "step": 85150
    },
    {
      "epoch": 4.5418666666666665,
      "grad_norm": 0.49270501732826233,
      "learning_rate": 2.1613333333333335e-05,
      "loss": 0.0022,
      "step": 85160
    },
    {
      "epoch": 4.5424,
      "grad_norm": 0.23463070392608643,
      "learning_rate": 2.1609999999999998e-05,
      "loss": 0.0017,
      "step": 85170
    },
    {
      "epoch": 4.542933333333333,
      "grad_norm": 0.494006484746933,
      "learning_rate": 2.1606666666666668e-05,
      "loss": 0.0029,
      "step": 85180
    },
    {
      "epoch": 4.543466666666666,
      "grad_norm": 0.20866785943508148,
      "learning_rate": 2.1603333333333334e-05,
      "loss": 0.0024,
      "step": 85190
    },
    {
      "epoch": 4.5440000000000005,
      "grad_norm": 0.2613701820373535,
      "learning_rate": 2.16e-05,
      "loss": 0.0024,
      "step": 85200
    },
    {
      "epoch": 4.544533333333334,
      "grad_norm": 0.0673295333981514,
      "learning_rate": 2.159666666666667e-05,
      "loss": 0.0014,
      "step": 85210
    },
    {
      "epoch": 4.545066666666667,
      "grad_norm": 0.12241264432668686,
      "learning_rate": 2.1593333333333336e-05,
      "loss": 0.0014,
      "step": 85220
    },
    {
      "epoch": 4.5456,
      "grad_norm": 0.37545546889305115,
      "learning_rate": 2.159e-05,
      "loss": 0.0021,
      "step": 85230
    },
    {
      "epoch": 4.546133333333334,
      "grad_norm": 0.3279639184474945,
      "learning_rate": 2.1586666666666668e-05,
      "loss": 0.0019,
      "step": 85240
    },
    {
      "epoch": 4.546666666666667,
      "grad_norm": 0.28882184624671936,
      "learning_rate": 2.1583333333333334e-05,
      "loss": 0.0019,
      "step": 85250
    },
    {
      "epoch": 4.5472,
      "grad_norm": 0.2985316812992096,
      "learning_rate": 2.158e-05,
      "loss": 0.0022,
      "step": 85260
    },
    {
      "epoch": 4.547733333333333,
      "grad_norm": 0.0397048145532608,
      "learning_rate": 2.1576666666666666e-05,
      "loss": 0.0021,
      "step": 85270
    },
    {
      "epoch": 4.548266666666667,
      "grad_norm": 0.061495259404182434,
      "learning_rate": 2.1573333333333336e-05,
      "loss": 0.0018,
      "step": 85280
    },
    {
      "epoch": 4.5488,
      "grad_norm": 0.0811770111322403,
      "learning_rate": 2.1570000000000002e-05,
      "loss": 0.0025,
      "step": 85290
    },
    {
      "epoch": 4.549333333333333,
      "grad_norm": 0.1847187876701355,
      "learning_rate": 2.1566666666666668e-05,
      "loss": 0.002,
      "step": 85300
    },
    {
      "epoch": 4.5498666666666665,
      "grad_norm": 0.14687076210975647,
      "learning_rate": 2.1563333333333334e-05,
      "loss": 0.0017,
      "step": 85310
    },
    {
      "epoch": 4.5504,
      "grad_norm": 0.0225000511854887,
      "learning_rate": 2.1560000000000004e-05,
      "loss": 0.0021,
      "step": 85320
    },
    {
      "epoch": 4.550933333333333,
      "grad_norm": 0.14953063428401947,
      "learning_rate": 2.1556666666666666e-05,
      "loss": 0.0015,
      "step": 85330
    },
    {
      "epoch": 4.551466666666666,
      "grad_norm": 0.15364408493041992,
      "learning_rate": 2.1553333333333333e-05,
      "loss": 0.0022,
      "step": 85340
    },
    {
      "epoch": 4.552,
      "grad_norm": 0.31788063049316406,
      "learning_rate": 2.1550000000000002e-05,
      "loss": 0.0024,
      "step": 85350
    },
    {
      "epoch": 4.552533333333333,
      "grad_norm": 0.26532596349716187,
      "learning_rate": 2.1546666666666668e-05,
      "loss": 0.0016,
      "step": 85360
    },
    {
      "epoch": 4.553066666666667,
      "grad_norm": 0.34618812799453735,
      "learning_rate": 2.1543333333333334e-05,
      "loss": 0.0032,
      "step": 85370
    },
    {
      "epoch": 4.5536,
      "grad_norm": 0.19780178368091583,
      "learning_rate": 2.154e-05,
      "loss": 0.0025,
      "step": 85380
    },
    {
      "epoch": 4.554133333333334,
      "grad_norm": 0.39304307103157043,
      "learning_rate": 2.153666666666667e-05,
      "loss": 0.0015,
      "step": 85390
    },
    {
      "epoch": 4.554666666666667,
      "grad_norm": 0.48335185647010803,
      "learning_rate": 2.1533333333333333e-05,
      "loss": 0.0019,
      "step": 85400
    },
    {
      "epoch": 4.5552,
      "grad_norm": 0.0958087295293808,
      "learning_rate": 2.153e-05,
      "loss": 0.0018,
      "step": 85410
    },
    {
      "epoch": 4.555733333333333,
      "grad_norm": 0.3624449372291565,
      "learning_rate": 2.152666666666667e-05,
      "loss": 0.002,
      "step": 85420
    },
    {
      "epoch": 4.556266666666667,
      "grad_norm": 0.18832501769065857,
      "learning_rate": 2.1523333333333335e-05,
      "loss": 0.0019,
      "step": 85430
    },
    {
      "epoch": 4.5568,
      "grad_norm": 0.3561543822288513,
      "learning_rate": 2.152e-05,
      "loss": 0.0019,
      "step": 85440
    },
    {
      "epoch": 4.557333333333333,
      "grad_norm": 0.20854079723358154,
      "learning_rate": 2.1516666666666667e-05,
      "loss": 0.0028,
      "step": 85450
    },
    {
      "epoch": 4.5578666666666665,
      "grad_norm": 0.25676652789115906,
      "learning_rate": 2.1513333333333336e-05,
      "loss": 0.0019,
      "step": 85460
    },
    {
      "epoch": 4.5584,
      "grad_norm": 0.32153964042663574,
      "learning_rate": 2.1510000000000002e-05,
      "loss": 0.0018,
      "step": 85470
    },
    {
      "epoch": 4.558933333333333,
      "grad_norm": 0.4628286063671112,
      "learning_rate": 2.1506666666666665e-05,
      "loss": 0.0018,
      "step": 85480
    },
    {
      "epoch": 4.559466666666666,
      "grad_norm": 0.2624078392982483,
      "learning_rate": 2.1503333333333335e-05,
      "loss": 0.0022,
      "step": 85490
    },
    {
      "epoch": 4.5600000000000005,
      "grad_norm": 0.14364981651306152,
      "learning_rate": 2.15e-05,
      "loss": 0.0021,
      "step": 85500
    },
    {
      "epoch": 4.560533333333334,
      "grad_norm": 0.11770322918891907,
      "learning_rate": 2.1496666666666667e-05,
      "loss": 0.0017,
      "step": 85510
    },
    {
      "epoch": 4.561066666666667,
      "grad_norm": 0.5472152829170227,
      "learning_rate": 2.1493333333333333e-05,
      "loss": 0.0022,
      "step": 85520
    },
    {
      "epoch": 4.5616,
      "grad_norm": 0.32828229665756226,
      "learning_rate": 2.1490000000000003e-05,
      "loss": 0.0016,
      "step": 85530
    },
    {
      "epoch": 4.562133333333334,
      "grad_norm": 0.05227316915988922,
      "learning_rate": 2.148666666666667e-05,
      "loss": 0.002,
      "step": 85540
    },
    {
      "epoch": 4.562666666666667,
      "grad_norm": 0.08887723833322525,
      "learning_rate": 2.148333333333333e-05,
      "loss": 0.0019,
      "step": 85550
    },
    {
      "epoch": 4.5632,
      "grad_norm": 0.23816438019275665,
      "learning_rate": 2.148e-05,
      "loss": 0.0025,
      "step": 85560
    },
    {
      "epoch": 4.563733333333333,
      "grad_norm": 0.2110656201839447,
      "learning_rate": 2.1476666666666667e-05,
      "loss": 0.0015,
      "step": 85570
    },
    {
      "epoch": 4.564266666666667,
      "grad_norm": 0.18346332013607025,
      "learning_rate": 2.1473333333333333e-05,
      "loss": 0.0021,
      "step": 85580
    },
    {
      "epoch": 4.5648,
      "grad_norm": 0.2940158545970917,
      "learning_rate": 2.1470000000000003e-05,
      "loss": 0.0018,
      "step": 85590
    },
    {
      "epoch": 4.565333333333333,
      "grad_norm": 0.22911983728408813,
      "learning_rate": 2.146666666666667e-05,
      "loss": 0.0028,
      "step": 85600
    },
    {
      "epoch": 4.5658666666666665,
      "grad_norm": 0.22886504232883453,
      "learning_rate": 2.1463333333333335e-05,
      "loss": 0.002,
      "step": 85610
    },
    {
      "epoch": 4.5664,
      "grad_norm": 0.2427031546831131,
      "learning_rate": 2.146e-05,
      "loss": 0.0023,
      "step": 85620
    },
    {
      "epoch": 4.566933333333333,
      "grad_norm": 0.08937052637338638,
      "learning_rate": 2.1456666666666667e-05,
      "loss": 0.0019,
      "step": 85630
    },
    {
      "epoch": 4.567466666666666,
      "grad_norm": 0.07040416449308395,
      "learning_rate": 2.1453333333333333e-05,
      "loss": 0.0019,
      "step": 85640
    },
    {
      "epoch": 4.568,
      "grad_norm": 0.38905370235443115,
      "learning_rate": 2.145e-05,
      "loss": 0.0028,
      "step": 85650
    },
    {
      "epoch": 4.568533333333333,
      "grad_norm": 0.6744205951690674,
      "learning_rate": 2.144666666666667e-05,
      "loss": 0.0018,
      "step": 85660
    },
    {
      "epoch": 4.569066666666667,
      "grad_norm": 0.06576728075742722,
      "learning_rate": 2.1443333333333335e-05,
      "loss": 0.0023,
      "step": 85670
    },
    {
      "epoch": 4.5696,
      "grad_norm": 0.4383224546909332,
      "learning_rate": 2.144e-05,
      "loss": 0.0017,
      "step": 85680
    },
    {
      "epoch": 4.570133333333334,
      "grad_norm": 0.3870444595813751,
      "learning_rate": 2.1436666666666668e-05,
      "loss": 0.0024,
      "step": 85690
    },
    {
      "epoch": 4.570666666666667,
      "grad_norm": 0.43811270594596863,
      "learning_rate": 2.1433333333333334e-05,
      "loss": 0.0019,
      "step": 85700
    },
    {
      "epoch": 4.5712,
      "grad_norm": 0.23582378029823303,
      "learning_rate": 2.143e-05,
      "loss": 0.002,
      "step": 85710
    },
    {
      "epoch": 4.571733333333333,
      "grad_norm": 0.04099426791071892,
      "learning_rate": 2.1426666666666666e-05,
      "loss": 0.0034,
      "step": 85720
    },
    {
      "epoch": 4.572266666666667,
      "grad_norm": 0.17068763077259064,
      "learning_rate": 2.1423333333333335e-05,
      "loss": 0.0015,
      "step": 85730
    },
    {
      "epoch": 4.5728,
      "grad_norm": 0.10515965521335602,
      "learning_rate": 2.142e-05,
      "loss": 0.0021,
      "step": 85740
    },
    {
      "epoch": 4.573333333333333,
      "grad_norm": 0.21053043007850647,
      "learning_rate": 2.1416666666666668e-05,
      "loss": 0.0019,
      "step": 85750
    },
    {
      "epoch": 4.5738666666666665,
      "grad_norm": 0.4267427623271942,
      "learning_rate": 2.1413333333333334e-05,
      "loss": 0.0021,
      "step": 85760
    },
    {
      "epoch": 4.5744,
      "grad_norm": 0.2888985276222229,
      "learning_rate": 2.1410000000000003e-05,
      "loss": 0.0028,
      "step": 85770
    },
    {
      "epoch": 4.574933333333333,
      "grad_norm": 0.12445082515478134,
      "learning_rate": 2.1406666666666666e-05,
      "loss": 0.0016,
      "step": 85780
    },
    {
      "epoch": 4.575466666666666,
      "grad_norm": 0.3196905255317688,
      "learning_rate": 2.1403333333333332e-05,
      "loss": 0.0019,
      "step": 85790
    },
    {
      "epoch": 4.576,
      "grad_norm": 0.04078606888651848,
      "learning_rate": 2.1400000000000002e-05,
      "loss": 0.0017,
      "step": 85800
    },
    {
      "epoch": 4.576533333333334,
      "grad_norm": 0.23269738256931305,
      "learning_rate": 2.1396666666666668e-05,
      "loss": 0.0015,
      "step": 85810
    },
    {
      "epoch": 4.577066666666667,
      "grad_norm": 0.1688307225704193,
      "learning_rate": 2.1393333333333334e-05,
      "loss": 0.0019,
      "step": 85820
    },
    {
      "epoch": 4.5776,
      "grad_norm": 0.09182479977607727,
      "learning_rate": 2.139e-05,
      "loss": 0.0016,
      "step": 85830
    },
    {
      "epoch": 4.578133333333334,
      "grad_norm": 0.19625362753868103,
      "learning_rate": 2.138666666666667e-05,
      "loss": 0.0022,
      "step": 85840
    },
    {
      "epoch": 4.578666666666667,
      "grad_norm": 0.18840904533863068,
      "learning_rate": 2.1383333333333332e-05,
      "loss": 0.0026,
      "step": 85850
    },
    {
      "epoch": 4.5792,
      "grad_norm": 0.2635575830936432,
      "learning_rate": 2.138e-05,
      "loss": 0.0018,
      "step": 85860
    },
    {
      "epoch": 4.579733333333333,
      "grad_norm": 0.10222432017326355,
      "learning_rate": 2.1376666666666668e-05,
      "loss": 0.0015,
      "step": 85870
    },
    {
      "epoch": 4.580266666666667,
      "grad_norm": 0.4051937162876129,
      "learning_rate": 2.1373333333333334e-05,
      "loss": 0.0019,
      "step": 85880
    },
    {
      "epoch": 4.5808,
      "grad_norm": 0.11552782356739044,
      "learning_rate": 2.137e-05,
      "loss": 0.0021,
      "step": 85890
    },
    {
      "epoch": 4.581333333333333,
      "grad_norm": 0.552476704120636,
      "learning_rate": 2.1366666666666667e-05,
      "loss": 0.0019,
      "step": 85900
    },
    {
      "epoch": 4.5818666666666665,
      "grad_norm": 0.13317449390888214,
      "learning_rate": 2.1363333333333336e-05,
      "loss": 0.0023,
      "step": 85910
    },
    {
      "epoch": 4.5824,
      "grad_norm": 0.29102835059165955,
      "learning_rate": 2.1360000000000002e-05,
      "loss": 0.0024,
      "step": 85920
    },
    {
      "epoch": 4.582933333333333,
      "grad_norm": 0.43171682953834534,
      "learning_rate": 2.1356666666666665e-05,
      "loss": 0.0026,
      "step": 85930
    },
    {
      "epoch": 4.583466666666666,
      "grad_norm": 0.17903056740760803,
      "learning_rate": 2.1353333333333334e-05,
      "loss": 0.0024,
      "step": 85940
    },
    {
      "epoch": 4.584,
      "grad_norm": 0.5531483888626099,
      "learning_rate": 2.135e-05,
      "loss": 0.0016,
      "step": 85950
    },
    {
      "epoch": 4.584533333333333,
      "grad_norm": 0.10765203088521957,
      "learning_rate": 2.1346666666666667e-05,
      "loss": 0.0017,
      "step": 85960
    },
    {
      "epoch": 4.585066666666666,
      "grad_norm": 0.06682184338569641,
      "learning_rate": 2.1343333333333336e-05,
      "loss": 0.0018,
      "step": 85970
    },
    {
      "epoch": 4.5856,
      "grad_norm": 0.4660714566707611,
      "learning_rate": 2.1340000000000002e-05,
      "loss": 0.002,
      "step": 85980
    },
    {
      "epoch": 4.586133333333334,
      "grad_norm": 0.086711086332798,
      "learning_rate": 2.133666666666667e-05,
      "loss": 0.0018,
      "step": 85990
    },
    {
      "epoch": 4.586666666666667,
      "grad_norm": 0.44410592317581177,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 0.0019,
      "step": 86000
    },
    {
      "epoch": 4.5872,
      "grad_norm": 0.2596845030784607,
      "learning_rate": 2.133e-05,
      "loss": 0.0013,
      "step": 86010
    },
    {
      "epoch": 4.587733333333333,
      "grad_norm": 0.11550268530845642,
      "learning_rate": 2.1326666666666667e-05,
      "loss": 0.0023,
      "step": 86020
    },
    {
      "epoch": 4.588266666666667,
      "grad_norm": 0.12310844659805298,
      "learning_rate": 2.1323333333333333e-05,
      "loss": 0.0025,
      "step": 86030
    },
    {
      "epoch": 4.5888,
      "grad_norm": 0.0879690870642662,
      "learning_rate": 2.1320000000000003e-05,
      "loss": 0.002,
      "step": 86040
    },
    {
      "epoch": 4.589333333333333,
      "grad_norm": 0.41228631138801575,
      "learning_rate": 2.131666666666667e-05,
      "loss": 0.0025,
      "step": 86050
    },
    {
      "epoch": 4.5898666666666665,
      "grad_norm": 0.06587686389684677,
      "learning_rate": 2.1313333333333335e-05,
      "loss": 0.0016,
      "step": 86060
    },
    {
      "epoch": 4.5904,
      "grad_norm": 0.05716388300061226,
      "learning_rate": 2.131e-05,
      "loss": 0.0018,
      "step": 86070
    },
    {
      "epoch": 4.590933333333333,
      "grad_norm": 0.06951595842838287,
      "learning_rate": 2.1306666666666667e-05,
      "loss": 0.0012,
      "step": 86080
    },
    {
      "epoch": 4.591466666666666,
      "grad_norm": 0.05460497364401817,
      "learning_rate": 2.1303333333333333e-05,
      "loss": 0.0015,
      "step": 86090
    },
    {
      "epoch": 4.592,
      "grad_norm": 0.17720481753349304,
      "learning_rate": 2.13e-05,
      "loss": 0.0019,
      "step": 86100
    },
    {
      "epoch": 4.592533333333334,
      "grad_norm": 0.28933072090148926,
      "learning_rate": 2.129666666666667e-05,
      "loss": 0.002,
      "step": 86110
    },
    {
      "epoch": 4.593066666666667,
      "grad_norm": 0.22766369581222534,
      "learning_rate": 2.1293333333333335e-05,
      "loss": 0.0022,
      "step": 86120
    },
    {
      "epoch": 4.5936,
      "grad_norm": 0.05741013586521149,
      "learning_rate": 2.129e-05,
      "loss": 0.0025,
      "step": 86130
    },
    {
      "epoch": 4.594133333333334,
      "grad_norm": 0.17115764319896698,
      "learning_rate": 2.1286666666666667e-05,
      "loss": 0.0023,
      "step": 86140
    },
    {
      "epoch": 4.594666666666667,
      "grad_norm": 0.2691993713378906,
      "learning_rate": 2.1283333333333337e-05,
      "loss": 0.0018,
      "step": 86150
    },
    {
      "epoch": 4.5952,
      "grad_norm": 0.3197491466999054,
      "learning_rate": 2.128e-05,
      "loss": 0.0024,
      "step": 86160
    },
    {
      "epoch": 4.5957333333333334,
      "grad_norm": 0.05869930982589722,
      "learning_rate": 2.1276666666666666e-05,
      "loss": 0.0021,
      "step": 86170
    },
    {
      "epoch": 4.596266666666667,
      "grad_norm": 0.1500616818666458,
      "learning_rate": 2.1273333333333335e-05,
      "loss": 0.002,
      "step": 86180
    },
    {
      "epoch": 4.5968,
      "grad_norm": 0.21255739033222198,
      "learning_rate": 2.127e-05,
      "loss": 0.002,
      "step": 86190
    },
    {
      "epoch": 4.597333333333333,
      "grad_norm": 0.4356720745563507,
      "learning_rate": 2.1266666666666667e-05,
      "loss": 0.0017,
      "step": 86200
    },
    {
      "epoch": 4.5978666666666665,
      "grad_norm": 0.2398555725812912,
      "learning_rate": 2.1263333333333334e-05,
      "loss": 0.002,
      "step": 86210
    },
    {
      "epoch": 4.5984,
      "grad_norm": 0.6980100870132446,
      "learning_rate": 2.1260000000000003e-05,
      "loss": 0.0028,
      "step": 86220
    },
    {
      "epoch": 4.598933333333333,
      "grad_norm": 0.8439520597457886,
      "learning_rate": 2.1256666666666666e-05,
      "loss": 0.0023,
      "step": 86230
    },
    {
      "epoch": 4.599466666666666,
      "grad_norm": 0.2700371742248535,
      "learning_rate": 2.1253333333333332e-05,
      "loss": 0.0024,
      "step": 86240
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.29333236813545227,
      "learning_rate": 2.125e-05,
      "loss": 0.0021,
      "step": 86250
    },
    {
      "epoch": 4.600533333333333,
      "grad_norm": 0.32424110174179077,
      "learning_rate": 2.1246666666666668e-05,
      "loss": 0.0014,
      "step": 86260
    },
    {
      "epoch": 4.601066666666666,
      "grad_norm": 0.22914834320545197,
      "learning_rate": 2.1243333333333334e-05,
      "loss": 0.0019,
      "step": 86270
    },
    {
      "epoch": 4.6016,
      "grad_norm": 0.38042011857032776,
      "learning_rate": 2.124e-05,
      "loss": 0.002,
      "step": 86280
    },
    {
      "epoch": 4.602133333333334,
      "grad_norm": 0.35443633794784546,
      "learning_rate": 2.123666666666667e-05,
      "loss": 0.0024,
      "step": 86290
    },
    {
      "epoch": 4.602666666666667,
      "grad_norm": 0.6225230693817139,
      "learning_rate": 2.1233333333333336e-05,
      "loss": 0.0031,
      "step": 86300
    },
    {
      "epoch": 4.6032,
      "grad_norm": 0.3787846267223358,
      "learning_rate": 2.123e-05,
      "loss": 0.0021,
      "step": 86310
    },
    {
      "epoch": 4.6037333333333335,
      "grad_norm": 0.20714758336544037,
      "learning_rate": 2.1226666666666668e-05,
      "loss": 0.002,
      "step": 86320
    },
    {
      "epoch": 4.604266666666667,
      "grad_norm": 0.21714052557945251,
      "learning_rate": 2.1223333333333334e-05,
      "loss": 0.0014,
      "step": 86330
    },
    {
      "epoch": 4.6048,
      "grad_norm": 0.08842609077692032,
      "learning_rate": 2.122e-05,
      "loss": 0.0017,
      "step": 86340
    },
    {
      "epoch": 4.605333333333333,
      "grad_norm": 0.09486071020364761,
      "learning_rate": 2.121666666666667e-05,
      "loss": 0.0017,
      "step": 86350
    },
    {
      "epoch": 4.6058666666666666,
      "grad_norm": 0.12204831093549728,
      "learning_rate": 2.1213333333333336e-05,
      "loss": 0.0019,
      "step": 86360
    },
    {
      "epoch": 4.6064,
      "grad_norm": 0.08861042559146881,
      "learning_rate": 2.1210000000000002e-05,
      "loss": 0.0015,
      "step": 86370
    },
    {
      "epoch": 4.606933333333333,
      "grad_norm": 0.09596827626228333,
      "learning_rate": 2.1206666666666665e-05,
      "loss": 0.0021,
      "step": 86380
    },
    {
      "epoch": 4.607466666666666,
      "grad_norm": 0.15837819874286652,
      "learning_rate": 2.1203333333333334e-05,
      "loss": 0.0017,
      "step": 86390
    },
    {
      "epoch": 4.608,
      "grad_norm": 0.17870557308197021,
      "learning_rate": 2.12e-05,
      "loss": 0.0018,
      "step": 86400
    },
    {
      "epoch": 4.608533333333334,
      "grad_norm": 0.14211557805538177,
      "learning_rate": 2.1196666666666666e-05,
      "loss": 0.0016,
      "step": 86410
    },
    {
      "epoch": 4.609066666666667,
      "grad_norm": 0.1496778279542923,
      "learning_rate": 2.1193333333333336e-05,
      "loss": 0.0018,
      "step": 86420
    },
    {
      "epoch": 4.6096,
      "grad_norm": 0.519845187664032,
      "learning_rate": 2.1190000000000002e-05,
      "loss": 0.0017,
      "step": 86430
    },
    {
      "epoch": 4.610133333333334,
      "grad_norm": 0.1981060653924942,
      "learning_rate": 2.1186666666666668e-05,
      "loss": 0.0019,
      "step": 86440
    },
    {
      "epoch": 4.610666666666667,
      "grad_norm": 0.32025837898254395,
      "learning_rate": 2.1183333333333334e-05,
      "loss": 0.0014,
      "step": 86450
    },
    {
      "epoch": 4.6112,
      "grad_norm": 0.35946640372276306,
      "learning_rate": 2.118e-05,
      "loss": 0.0014,
      "step": 86460
    },
    {
      "epoch": 4.6117333333333335,
      "grad_norm": 0.11854670941829681,
      "learning_rate": 2.1176666666666667e-05,
      "loss": 0.002,
      "step": 86470
    },
    {
      "epoch": 4.612266666666667,
      "grad_norm": 0.11879267543554306,
      "learning_rate": 2.1173333333333333e-05,
      "loss": 0.0013,
      "step": 86480
    },
    {
      "epoch": 4.6128,
      "grad_norm": 0.04318132996559143,
      "learning_rate": 2.1170000000000002e-05,
      "loss": 0.0014,
      "step": 86490
    },
    {
      "epoch": 4.613333333333333,
      "grad_norm": 0.05095573514699936,
      "learning_rate": 2.116666666666667e-05,
      "loss": 0.0024,
      "step": 86500
    },
    {
      "epoch": 4.613866666666667,
      "grad_norm": 0.11374563723802567,
      "learning_rate": 2.1163333333333335e-05,
      "loss": 0.0015,
      "step": 86510
    },
    {
      "epoch": 4.6144,
      "grad_norm": 0.03794762119650841,
      "learning_rate": 2.116e-05,
      "loss": 0.0016,
      "step": 86520
    },
    {
      "epoch": 4.614933333333333,
      "grad_norm": 0.40526217222213745,
      "learning_rate": 2.1156666666666667e-05,
      "loss": 0.0014,
      "step": 86530
    },
    {
      "epoch": 4.615466666666666,
      "grad_norm": 0.31932470202445984,
      "learning_rate": 2.1153333333333333e-05,
      "loss": 0.0019,
      "step": 86540
    },
    {
      "epoch": 4.616,
      "grad_norm": 0.3011733591556549,
      "learning_rate": 2.115e-05,
      "loss": 0.0028,
      "step": 86550
    },
    {
      "epoch": 4.616533333333333,
      "grad_norm": 0.11808919161558151,
      "learning_rate": 2.114666666666667e-05,
      "loss": 0.0019,
      "step": 86560
    },
    {
      "epoch": 4.617066666666666,
      "grad_norm": 0.2282089740037918,
      "learning_rate": 2.1143333333333335e-05,
      "loss": 0.0021,
      "step": 86570
    },
    {
      "epoch": 4.6176,
      "grad_norm": 0.18349893391132355,
      "learning_rate": 2.114e-05,
      "loss": 0.0015,
      "step": 86580
    },
    {
      "epoch": 4.618133333333334,
      "grad_norm": 0.27042633295059204,
      "learning_rate": 2.1136666666666667e-05,
      "loss": 0.0017,
      "step": 86590
    },
    {
      "epoch": 4.618666666666667,
      "grad_norm": 0.14400796592235565,
      "learning_rate": 2.1133333333333337e-05,
      "loss": 0.0022,
      "step": 86600
    },
    {
      "epoch": 4.6192,
      "grad_norm": 0.04581904411315918,
      "learning_rate": 2.113e-05,
      "loss": 0.0019,
      "step": 86610
    },
    {
      "epoch": 4.6197333333333335,
      "grad_norm": 0.222390815615654,
      "learning_rate": 2.1126666666666665e-05,
      "loss": 0.002,
      "step": 86620
    },
    {
      "epoch": 4.620266666666667,
      "grad_norm": 0.23852945864200592,
      "learning_rate": 2.1123333333333335e-05,
      "loss": 0.0036,
      "step": 86630
    },
    {
      "epoch": 4.6208,
      "grad_norm": 0.05285484716296196,
      "learning_rate": 2.112e-05,
      "loss": 0.0015,
      "step": 86640
    },
    {
      "epoch": 4.621333333333333,
      "grad_norm": 0.43397200107574463,
      "learning_rate": 2.1116666666666667e-05,
      "loss": 0.002,
      "step": 86650
    },
    {
      "epoch": 4.621866666666667,
      "grad_norm": 0.20781177282333374,
      "learning_rate": 2.1113333333333333e-05,
      "loss": 0.0018,
      "step": 86660
    },
    {
      "epoch": 4.6224,
      "grad_norm": 0.31975847482681274,
      "learning_rate": 2.1110000000000003e-05,
      "loss": 0.0015,
      "step": 86670
    },
    {
      "epoch": 4.622933333333333,
      "grad_norm": 0.027595696970820427,
      "learning_rate": 2.110666666666667e-05,
      "loss": 0.0026,
      "step": 86680
    },
    {
      "epoch": 4.623466666666666,
      "grad_norm": 0.14972227811813354,
      "learning_rate": 2.1103333333333332e-05,
      "loss": 0.0029,
      "step": 86690
    },
    {
      "epoch": 4.624,
      "grad_norm": 0.5230127573013306,
      "learning_rate": 2.11e-05,
      "loss": 0.0022,
      "step": 86700
    },
    {
      "epoch": 4.624533333333334,
      "grad_norm": 0.2985396683216095,
      "learning_rate": 2.1096666666666667e-05,
      "loss": 0.0028,
      "step": 86710
    },
    {
      "epoch": 4.625066666666667,
      "grad_norm": 0.0951775386929512,
      "learning_rate": 2.1093333333333334e-05,
      "loss": 0.0019,
      "step": 86720
    },
    {
      "epoch": 4.6256,
      "grad_norm": 0.0721789300441742,
      "learning_rate": 2.1090000000000003e-05,
      "loss": 0.0019,
      "step": 86730
    },
    {
      "epoch": 4.626133333333334,
      "grad_norm": 0.059490885585546494,
      "learning_rate": 2.108666666666667e-05,
      "loss": 0.0028,
      "step": 86740
    },
    {
      "epoch": 4.626666666666667,
      "grad_norm": 0.31745150685310364,
      "learning_rate": 2.1083333333333335e-05,
      "loss": 0.0025,
      "step": 86750
    },
    {
      "epoch": 4.6272,
      "grad_norm": 0.22200405597686768,
      "learning_rate": 2.1079999999999998e-05,
      "loss": 0.0022,
      "step": 86760
    },
    {
      "epoch": 4.6277333333333335,
      "grad_norm": 0.3844617009162903,
      "learning_rate": 2.1076666666666668e-05,
      "loss": 0.0017,
      "step": 86770
    },
    {
      "epoch": 4.628266666666667,
      "grad_norm": 0.23394332826137543,
      "learning_rate": 2.1073333333333334e-05,
      "loss": 0.0022,
      "step": 86780
    },
    {
      "epoch": 4.6288,
      "grad_norm": 0.1925124228000641,
      "learning_rate": 2.107e-05,
      "loss": 0.0017,
      "step": 86790
    },
    {
      "epoch": 4.629333333333333,
      "grad_norm": 0.3049893081188202,
      "learning_rate": 2.106666666666667e-05,
      "loss": 0.0021,
      "step": 86800
    },
    {
      "epoch": 4.629866666666667,
      "grad_norm": 0.2421119660139084,
      "learning_rate": 2.1063333333333336e-05,
      "loss": 0.0018,
      "step": 86810
    },
    {
      "epoch": 4.6304,
      "grad_norm": 0.19199763238430023,
      "learning_rate": 2.106e-05,
      "loss": 0.0028,
      "step": 86820
    },
    {
      "epoch": 4.630933333333333,
      "grad_norm": 0.24643319845199585,
      "learning_rate": 2.1056666666666668e-05,
      "loss": 0.0024,
      "step": 86830
    },
    {
      "epoch": 4.631466666666666,
      "grad_norm": 0.18876750767230988,
      "learning_rate": 2.1053333333333334e-05,
      "loss": 0.0032,
      "step": 86840
    },
    {
      "epoch": 4.632,
      "grad_norm": 0.2335127592086792,
      "learning_rate": 2.105e-05,
      "loss": 0.0027,
      "step": 86850
    },
    {
      "epoch": 4.632533333333333,
      "grad_norm": 0.15765275061130524,
      "learning_rate": 2.1046666666666666e-05,
      "loss": 0.0023,
      "step": 86860
    },
    {
      "epoch": 4.633066666666666,
      "grad_norm": 0.25060203671455383,
      "learning_rate": 2.1043333333333336e-05,
      "loss": 0.0024,
      "step": 86870
    },
    {
      "epoch": 4.6336,
      "grad_norm": 0.2177019864320755,
      "learning_rate": 2.1040000000000002e-05,
      "loss": 0.0021,
      "step": 86880
    },
    {
      "epoch": 4.634133333333334,
      "grad_norm": 0.4639284312725067,
      "learning_rate": 2.1036666666666668e-05,
      "loss": 0.0015,
      "step": 86890
    },
    {
      "epoch": 4.634666666666667,
      "grad_norm": 0.04752909764647484,
      "learning_rate": 2.1033333333333334e-05,
      "loss": 0.0019,
      "step": 86900
    },
    {
      "epoch": 4.6352,
      "grad_norm": 0.5033320784568787,
      "learning_rate": 2.103e-05,
      "loss": 0.0016,
      "step": 86910
    },
    {
      "epoch": 4.6357333333333335,
      "grad_norm": 0.2988740801811218,
      "learning_rate": 2.1026666666666666e-05,
      "loss": 0.002,
      "step": 86920
    },
    {
      "epoch": 4.636266666666667,
      "grad_norm": 0.2402220517396927,
      "learning_rate": 2.1023333333333333e-05,
      "loss": 0.003,
      "step": 86930
    },
    {
      "epoch": 4.6368,
      "grad_norm": 0.17856787145137787,
      "learning_rate": 2.1020000000000002e-05,
      "loss": 0.0019,
      "step": 86940
    },
    {
      "epoch": 4.637333333333333,
      "grad_norm": 0.16147290170192719,
      "learning_rate": 2.1016666666666668e-05,
      "loss": 0.0017,
      "step": 86950
    },
    {
      "epoch": 4.637866666666667,
      "grad_norm": 0.1512778252363205,
      "learning_rate": 2.1013333333333334e-05,
      "loss": 0.002,
      "step": 86960
    },
    {
      "epoch": 4.6384,
      "grad_norm": 0.5325549840927124,
      "learning_rate": 2.101e-05,
      "loss": 0.0018,
      "step": 86970
    },
    {
      "epoch": 4.638933333333333,
      "grad_norm": 0.17671091854572296,
      "learning_rate": 2.100666666666667e-05,
      "loss": 0.0018,
      "step": 86980
    },
    {
      "epoch": 4.639466666666666,
      "grad_norm": 0.10345859825611115,
      "learning_rate": 2.1003333333333333e-05,
      "loss": 0.003,
      "step": 86990
    },
    {
      "epoch": 4.64,
      "grad_norm": 0.04138679802417755,
      "learning_rate": 2.1e-05,
      "loss": 0.0018,
      "step": 87000
    },
    {
      "epoch": 4.640533333333333,
      "grad_norm": 0.09863121807575226,
      "learning_rate": 2.099666666666667e-05,
      "loss": 0.0018,
      "step": 87010
    },
    {
      "epoch": 4.641066666666667,
      "grad_norm": 0.33764028549194336,
      "learning_rate": 2.0993333333333334e-05,
      "loss": 0.0018,
      "step": 87020
    },
    {
      "epoch": 4.6416,
      "grad_norm": 0.04510733112692833,
      "learning_rate": 2.099e-05,
      "loss": 0.0023,
      "step": 87030
    },
    {
      "epoch": 4.642133333333334,
      "grad_norm": 0.23714275658130646,
      "learning_rate": 2.0986666666666667e-05,
      "loss": 0.002,
      "step": 87040
    },
    {
      "epoch": 4.642666666666667,
      "grad_norm": 0.23637546598911285,
      "learning_rate": 2.0983333333333336e-05,
      "loss": 0.0013,
      "step": 87050
    },
    {
      "epoch": 4.6432,
      "grad_norm": 0.041643813252449036,
      "learning_rate": 2.098e-05,
      "loss": 0.0022,
      "step": 87060
    },
    {
      "epoch": 4.6437333333333335,
      "grad_norm": 0.24530678987503052,
      "learning_rate": 2.0976666666666665e-05,
      "loss": 0.0023,
      "step": 87070
    },
    {
      "epoch": 4.644266666666667,
      "grad_norm": 0.10476425290107727,
      "learning_rate": 2.0973333333333335e-05,
      "loss": 0.002,
      "step": 87080
    },
    {
      "epoch": 4.6448,
      "grad_norm": 0.15655449032783508,
      "learning_rate": 2.097e-05,
      "loss": 0.0021,
      "step": 87090
    },
    {
      "epoch": 4.645333333333333,
      "grad_norm": 0.04491870477795601,
      "learning_rate": 2.0966666666666667e-05,
      "loss": 0.0013,
      "step": 87100
    },
    {
      "epoch": 4.645866666666667,
      "grad_norm": 0.325697660446167,
      "learning_rate": 2.0963333333333336e-05,
      "loss": 0.0021,
      "step": 87110
    },
    {
      "epoch": 4.6464,
      "grad_norm": 0.44531166553497314,
      "learning_rate": 2.0960000000000003e-05,
      "loss": 0.002,
      "step": 87120
    },
    {
      "epoch": 4.646933333333333,
      "grad_norm": 0.1286393105983734,
      "learning_rate": 2.095666666666667e-05,
      "loss": 0.0029,
      "step": 87130
    },
    {
      "epoch": 4.647466666666666,
      "grad_norm": 0.2036619335412979,
      "learning_rate": 2.095333333333333e-05,
      "loss": 0.0021,
      "step": 87140
    },
    {
      "epoch": 4.648,
      "grad_norm": 0.06423556804656982,
      "learning_rate": 2.095e-05,
      "loss": 0.0019,
      "step": 87150
    },
    {
      "epoch": 4.648533333333333,
      "grad_norm": 0.27868035435676575,
      "learning_rate": 2.0946666666666667e-05,
      "loss": 0.0019,
      "step": 87160
    },
    {
      "epoch": 4.649066666666666,
      "grad_norm": 0.44061943888664246,
      "learning_rate": 2.0943333333333333e-05,
      "loss": 0.0019,
      "step": 87170
    },
    {
      "epoch": 4.6495999999999995,
      "grad_norm": 0.5794004201889038,
      "learning_rate": 2.0940000000000003e-05,
      "loss": 0.0013,
      "step": 87180
    },
    {
      "epoch": 4.650133333333334,
      "grad_norm": 0.405538409948349,
      "learning_rate": 2.093666666666667e-05,
      "loss": 0.0017,
      "step": 87190
    },
    {
      "epoch": 4.650666666666667,
      "grad_norm": 0.12262288480997086,
      "learning_rate": 2.0933333333333335e-05,
      "loss": 0.0013,
      "step": 87200
    },
    {
      "epoch": 4.6512,
      "grad_norm": 0.17683394253253937,
      "learning_rate": 2.093e-05,
      "loss": 0.0021,
      "step": 87210
    },
    {
      "epoch": 4.6517333333333335,
      "grad_norm": 0.3797655701637268,
      "learning_rate": 2.0926666666666667e-05,
      "loss": 0.0021,
      "step": 87220
    },
    {
      "epoch": 4.652266666666667,
      "grad_norm": 0.03450731933116913,
      "learning_rate": 2.0923333333333333e-05,
      "loss": 0.0021,
      "step": 87230
    },
    {
      "epoch": 4.6528,
      "grad_norm": 0.09345627576112747,
      "learning_rate": 2.092e-05,
      "loss": 0.0024,
      "step": 87240
    },
    {
      "epoch": 4.653333333333333,
      "grad_norm": 0.6080296039581299,
      "learning_rate": 2.091666666666667e-05,
      "loss": 0.0021,
      "step": 87250
    },
    {
      "epoch": 4.653866666666667,
      "grad_norm": 0.08767205476760864,
      "learning_rate": 2.0913333333333335e-05,
      "loss": 0.0017,
      "step": 87260
    },
    {
      "epoch": 4.6544,
      "grad_norm": 0.5517778992652893,
      "learning_rate": 2.091e-05,
      "loss": 0.0018,
      "step": 87270
    },
    {
      "epoch": 4.654933333333333,
      "grad_norm": 0.05320131406188011,
      "learning_rate": 2.0906666666666668e-05,
      "loss": 0.0016,
      "step": 87280
    },
    {
      "epoch": 4.655466666666666,
      "grad_norm": 0.5566680431365967,
      "learning_rate": 2.0903333333333334e-05,
      "loss": 0.0017,
      "step": 87290
    },
    {
      "epoch": 4.656,
      "grad_norm": 0.15300168097019196,
      "learning_rate": 2.09e-05,
      "loss": 0.0019,
      "step": 87300
    },
    {
      "epoch": 4.656533333333333,
      "grad_norm": 0.021265285089612007,
      "learning_rate": 2.0896666666666666e-05,
      "loss": 0.0022,
      "step": 87310
    },
    {
      "epoch": 4.657066666666667,
      "grad_norm": 0.23122212290763855,
      "learning_rate": 2.0893333333333335e-05,
      "loss": 0.0016,
      "step": 87320
    },
    {
      "epoch": 4.6576,
      "grad_norm": 0.17852185666561127,
      "learning_rate": 2.089e-05,
      "loss": 0.0017,
      "step": 87330
    },
    {
      "epoch": 4.658133333333334,
      "grad_norm": 0.044254664331674576,
      "learning_rate": 2.0886666666666668e-05,
      "loss": 0.0015,
      "step": 87340
    },
    {
      "epoch": 4.658666666666667,
      "grad_norm": 0.05305998772382736,
      "learning_rate": 2.0883333333333334e-05,
      "loss": 0.0019,
      "step": 87350
    },
    {
      "epoch": 4.6592,
      "grad_norm": 0.07454244047403336,
      "learning_rate": 2.0880000000000003e-05,
      "loss": 0.0022,
      "step": 87360
    },
    {
      "epoch": 4.6597333333333335,
      "grad_norm": 0.26679080724716187,
      "learning_rate": 2.0876666666666666e-05,
      "loss": 0.0022,
      "step": 87370
    },
    {
      "epoch": 4.660266666666667,
      "grad_norm": 0.06576499342918396,
      "learning_rate": 2.0873333333333332e-05,
      "loss": 0.0014,
      "step": 87380
    },
    {
      "epoch": 4.6608,
      "grad_norm": 0.3783963620662689,
      "learning_rate": 2.0870000000000002e-05,
      "loss": 0.0026,
      "step": 87390
    },
    {
      "epoch": 4.661333333333333,
      "grad_norm": 0.25930261611938477,
      "learning_rate": 2.0866666666666668e-05,
      "loss": 0.0028,
      "step": 87400
    },
    {
      "epoch": 4.661866666666667,
      "grad_norm": 0.1308528184890747,
      "learning_rate": 2.0863333333333334e-05,
      "loss": 0.0018,
      "step": 87410
    },
    {
      "epoch": 4.6624,
      "grad_norm": 0.29111993312835693,
      "learning_rate": 2.086e-05,
      "loss": 0.002,
      "step": 87420
    },
    {
      "epoch": 4.662933333333333,
      "grad_norm": 0.2115524709224701,
      "learning_rate": 2.085666666666667e-05,
      "loss": 0.0021,
      "step": 87430
    },
    {
      "epoch": 4.663466666666666,
      "grad_norm": 0.2626618444919586,
      "learning_rate": 2.0853333333333332e-05,
      "loss": 0.0016,
      "step": 87440
    },
    {
      "epoch": 4.664,
      "grad_norm": 0.23989593982696533,
      "learning_rate": 2.085e-05,
      "loss": 0.0024,
      "step": 87450
    },
    {
      "epoch": 4.664533333333333,
      "grad_norm": 0.03666944429278374,
      "learning_rate": 2.0846666666666668e-05,
      "loss": 0.0024,
      "step": 87460
    },
    {
      "epoch": 4.665066666666666,
      "grad_norm": 0.15339745581150055,
      "learning_rate": 2.0843333333333334e-05,
      "loss": 0.0013,
      "step": 87470
    },
    {
      "epoch": 4.6655999999999995,
      "grad_norm": 0.20208126306533813,
      "learning_rate": 2.084e-05,
      "loss": 0.0022,
      "step": 87480
    },
    {
      "epoch": 4.666133333333334,
      "grad_norm": 0.2870338261127472,
      "learning_rate": 2.083666666666667e-05,
      "loss": 0.0023,
      "step": 87490
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 0.11786609888076782,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 0.0018,
      "step": 87500
    },
    {
      "epoch": 4.6672,
      "grad_norm": 0.49929162859916687,
      "learning_rate": 2.0830000000000002e-05,
      "loss": 0.0024,
      "step": 87510
    },
    {
      "epoch": 4.6677333333333335,
      "grad_norm": 0.051051825284957886,
      "learning_rate": 2.0826666666666665e-05,
      "loss": 0.0021,
      "step": 87520
    },
    {
      "epoch": 4.668266666666667,
      "grad_norm": 0.19986531138420105,
      "learning_rate": 2.0823333333333334e-05,
      "loss": 0.0019,
      "step": 87530
    },
    {
      "epoch": 4.6688,
      "grad_norm": 0.5260725617408752,
      "learning_rate": 2.082e-05,
      "loss": 0.0015,
      "step": 87540
    },
    {
      "epoch": 4.669333333333333,
      "grad_norm": 0.42641544342041016,
      "learning_rate": 2.0816666666666667e-05,
      "loss": 0.0025,
      "step": 87550
    },
    {
      "epoch": 4.669866666666667,
      "grad_norm": 0.37904372811317444,
      "learning_rate": 2.0813333333333336e-05,
      "loss": 0.0014,
      "step": 87560
    },
    {
      "epoch": 4.6704,
      "grad_norm": 0.2091914415359497,
      "learning_rate": 2.0810000000000002e-05,
      "loss": 0.0017,
      "step": 87570
    },
    {
      "epoch": 4.670933333333333,
      "grad_norm": 0.6352498531341553,
      "learning_rate": 2.080666666666667e-05,
      "loss": 0.0013,
      "step": 87580
    },
    {
      "epoch": 4.671466666666666,
      "grad_norm": 0.29277321696281433,
      "learning_rate": 2.0803333333333335e-05,
      "loss": 0.0017,
      "step": 87590
    },
    {
      "epoch": 4.672,
      "grad_norm": 0.405754029750824,
      "learning_rate": 2.08e-05,
      "loss": 0.0018,
      "step": 87600
    },
    {
      "epoch": 4.672533333333333,
      "grad_norm": 0.14743931591510773,
      "learning_rate": 2.0796666666666667e-05,
      "loss": 0.0023,
      "step": 87610
    },
    {
      "epoch": 4.673066666666667,
      "grad_norm": 0.2677769958972931,
      "learning_rate": 2.0793333333333333e-05,
      "loss": 0.0018,
      "step": 87620
    },
    {
      "epoch": 4.6736,
      "grad_norm": 0.23353233933448792,
      "learning_rate": 2.0790000000000003e-05,
      "loss": 0.0021,
      "step": 87630
    },
    {
      "epoch": 4.674133333333334,
      "grad_norm": 0.23102319240570068,
      "learning_rate": 2.078666666666667e-05,
      "loss": 0.0019,
      "step": 87640
    },
    {
      "epoch": 4.674666666666667,
      "grad_norm": 0.07242803275585175,
      "learning_rate": 2.0783333333333335e-05,
      "loss": 0.0021,
      "step": 87650
    },
    {
      "epoch": 4.6752,
      "grad_norm": 0.2126663625240326,
      "learning_rate": 2.078e-05,
      "loss": 0.0015,
      "step": 87660
    },
    {
      "epoch": 4.6757333333333335,
      "grad_norm": 0.21257798373699188,
      "learning_rate": 2.0776666666666667e-05,
      "loss": 0.0018,
      "step": 87670
    },
    {
      "epoch": 4.676266666666667,
      "grad_norm": 0.32736262679100037,
      "learning_rate": 2.0773333333333333e-05,
      "loss": 0.0029,
      "step": 87680
    },
    {
      "epoch": 4.6768,
      "grad_norm": 0.18079723417758942,
      "learning_rate": 2.077e-05,
      "loss": 0.0022,
      "step": 87690
    },
    {
      "epoch": 4.677333333333333,
      "grad_norm": 0.2959703207015991,
      "learning_rate": 2.076666666666667e-05,
      "loss": 0.0027,
      "step": 87700
    },
    {
      "epoch": 4.677866666666667,
      "grad_norm": 0.06224282458424568,
      "learning_rate": 2.0763333333333335e-05,
      "loss": 0.0019,
      "step": 87710
    },
    {
      "epoch": 4.6784,
      "grad_norm": 0.5707569718360901,
      "learning_rate": 2.076e-05,
      "loss": 0.002,
      "step": 87720
    },
    {
      "epoch": 4.678933333333333,
      "grad_norm": 0.25874650478363037,
      "learning_rate": 2.0756666666666667e-05,
      "loss": 0.0019,
      "step": 87730
    },
    {
      "epoch": 4.679466666666666,
      "grad_norm": 0.17659753561019897,
      "learning_rate": 2.0753333333333333e-05,
      "loss": 0.0016,
      "step": 87740
    },
    {
      "epoch": 4.68,
      "grad_norm": 0.2915070950984955,
      "learning_rate": 2.075e-05,
      "loss": 0.0021,
      "step": 87750
    },
    {
      "epoch": 4.680533333333333,
      "grad_norm": 0.26519638299942017,
      "learning_rate": 2.0746666666666666e-05,
      "loss": 0.0018,
      "step": 87760
    },
    {
      "epoch": 4.681066666666666,
      "grad_norm": 0.09662766009569168,
      "learning_rate": 2.0743333333333335e-05,
      "loss": 0.0023,
      "step": 87770
    },
    {
      "epoch": 4.6815999999999995,
      "grad_norm": 0.5110881924629211,
      "learning_rate": 2.074e-05,
      "loss": 0.0021,
      "step": 87780
    },
    {
      "epoch": 4.682133333333334,
      "grad_norm": 0.22592437267303467,
      "learning_rate": 2.0736666666666667e-05,
      "loss": 0.0018,
      "step": 87790
    },
    {
      "epoch": 4.682666666666667,
      "grad_norm": 0.14395074546337128,
      "learning_rate": 2.0733333333333334e-05,
      "loss": 0.0017,
      "step": 87800
    },
    {
      "epoch": 4.6832,
      "grad_norm": 0.1621047556400299,
      "learning_rate": 2.0730000000000003e-05,
      "loss": 0.0014,
      "step": 87810
    },
    {
      "epoch": 4.6837333333333335,
      "grad_norm": 0.1261845976114273,
      "learning_rate": 2.0726666666666666e-05,
      "loss": 0.0025,
      "step": 87820
    },
    {
      "epoch": 4.684266666666667,
      "grad_norm": 0.23039637506008148,
      "learning_rate": 2.0723333333333332e-05,
      "loss": 0.0014,
      "step": 87830
    },
    {
      "epoch": 4.6848,
      "grad_norm": 0.14346256852149963,
      "learning_rate": 2.072e-05,
      "loss": 0.0014,
      "step": 87840
    },
    {
      "epoch": 4.685333333333333,
      "grad_norm": 0.14449305832386017,
      "learning_rate": 2.0716666666666668e-05,
      "loss": 0.0032,
      "step": 87850
    },
    {
      "epoch": 4.685866666666667,
      "grad_norm": 0.17381855845451355,
      "learning_rate": 2.0713333333333334e-05,
      "loss": 0.0026,
      "step": 87860
    },
    {
      "epoch": 4.6864,
      "grad_norm": 0.08287667483091354,
      "learning_rate": 2.0710000000000003e-05,
      "loss": 0.0014,
      "step": 87870
    },
    {
      "epoch": 4.686933333333333,
      "grad_norm": 0.6405478715896606,
      "learning_rate": 2.070666666666667e-05,
      "loss": 0.0015,
      "step": 87880
    },
    {
      "epoch": 4.6874666666666664,
      "grad_norm": 0.29725897312164307,
      "learning_rate": 2.0703333333333336e-05,
      "loss": 0.0016,
      "step": 87890
    },
    {
      "epoch": 4.688,
      "grad_norm": 0.23410503566265106,
      "learning_rate": 2.07e-05,
      "loss": 0.0027,
      "step": 87900
    },
    {
      "epoch": 4.688533333333333,
      "grad_norm": 0.04122494161128998,
      "learning_rate": 2.0696666666666668e-05,
      "loss": 0.002,
      "step": 87910
    },
    {
      "epoch": 4.689066666666667,
      "grad_norm": 0.23101381957530975,
      "learning_rate": 2.0693333333333334e-05,
      "loss": 0.0015,
      "step": 87920
    },
    {
      "epoch": 4.6896,
      "grad_norm": 0.15431098639965057,
      "learning_rate": 2.069e-05,
      "loss": 0.0012,
      "step": 87930
    },
    {
      "epoch": 4.690133333333334,
      "grad_norm": 0.1288413405418396,
      "learning_rate": 2.068666666666667e-05,
      "loss": 0.0019,
      "step": 87940
    },
    {
      "epoch": 4.690666666666667,
      "grad_norm": 0.14435413479804993,
      "learning_rate": 2.0683333333333336e-05,
      "loss": 0.0017,
      "step": 87950
    },
    {
      "epoch": 4.6912,
      "grad_norm": 0.1425510048866272,
      "learning_rate": 2.0680000000000002e-05,
      "loss": 0.003,
      "step": 87960
    },
    {
      "epoch": 4.6917333333333335,
      "grad_norm": 0.23222017288208008,
      "learning_rate": 2.0676666666666668e-05,
      "loss": 0.0019,
      "step": 87970
    },
    {
      "epoch": 4.692266666666667,
      "grad_norm": 0.5261459946632385,
      "learning_rate": 2.0673333333333334e-05,
      "loss": 0.0018,
      "step": 87980
    },
    {
      "epoch": 4.6928,
      "grad_norm": 0.2014561891555786,
      "learning_rate": 2.067e-05,
      "loss": 0.0021,
      "step": 87990
    },
    {
      "epoch": 4.693333333333333,
      "grad_norm": 0.17767201364040375,
      "learning_rate": 2.0666666666666666e-05,
      "loss": 0.0022,
      "step": 88000
    },
    {
      "epoch": 4.693866666666667,
      "grad_norm": 0.584353506565094,
      "learning_rate": 2.0663333333333336e-05,
      "loss": 0.0024,
      "step": 88010
    },
    {
      "epoch": 4.6944,
      "grad_norm": 0.027899347245693207,
      "learning_rate": 2.0660000000000002e-05,
      "loss": 0.0022,
      "step": 88020
    },
    {
      "epoch": 4.694933333333333,
      "grad_norm": 0.3577500581741333,
      "learning_rate": 2.0656666666666668e-05,
      "loss": 0.0023,
      "step": 88030
    },
    {
      "epoch": 4.6954666666666665,
      "grad_norm": 0.11989255994558334,
      "learning_rate": 2.0653333333333334e-05,
      "loss": 0.002,
      "step": 88040
    },
    {
      "epoch": 4.696,
      "grad_norm": 0.08834575116634369,
      "learning_rate": 2.065e-05,
      "loss": 0.0022,
      "step": 88050
    },
    {
      "epoch": 4.696533333333333,
      "grad_norm": 0.23602743446826935,
      "learning_rate": 2.0646666666666667e-05,
      "loss": 0.0016,
      "step": 88060
    },
    {
      "epoch": 4.697066666666666,
      "grad_norm": 0.0522407628595829,
      "learning_rate": 2.0643333333333333e-05,
      "loss": 0.002,
      "step": 88070
    },
    {
      "epoch": 4.6975999999999996,
      "grad_norm": 0.212493434548378,
      "learning_rate": 2.0640000000000002e-05,
      "loss": 0.0022,
      "step": 88080
    },
    {
      "epoch": 4.698133333333334,
      "grad_norm": 0.5170575380325317,
      "learning_rate": 2.063666666666667e-05,
      "loss": 0.0022,
      "step": 88090
    },
    {
      "epoch": 4.698666666666667,
      "grad_norm": 0.35795336961746216,
      "learning_rate": 2.0633333333333335e-05,
      "loss": 0.0021,
      "step": 88100
    },
    {
      "epoch": 4.6992,
      "grad_norm": 0.32163503766059875,
      "learning_rate": 2.063e-05,
      "loss": 0.0028,
      "step": 88110
    },
    {
      "epoch": 4.6997333333333335,
      "grad_norm": 0.4071754515171051,
      "learning_rate": 2.0626666666666667e-05,
      "loss": 0.0023,
      "step": 88120
    },
    {
      "epoch": 4.700266666666667,
      "grad_norm": 0.09811846911907196,
      "learning_rate": 2.0623333333333333e-05,
      "loss": 0.0022,
      "step": 88130
    },
    {
      "epoch": 4.7008,
      "grad_norm": 0.11735978722572327,
      "learning_rate": 2.062e-05,
      "loss": 0.0013,
      "step": 88140
    },
    {
      "epoch": 4.701333333333333,
      "grad_norm": 0.43613186478614807,
      "learning_rate": 2.061666666666667e-05,
      "loss": 0.0017,
      "step": 88150
    },
    {
      "epoch": 4.701866666666667,
      "grad_norm": 0.09059988707304001,
      "learning_rate": 2.0613333333333335e-05,
      "loss": 0.002,
      "step": 88160
    },
    {
      "epoch": 4.7024,
      "grad_norm": 0.26775258779525757,
      "learning_rate": 2.061e-05,
      "loss": 0.0016,
      "step": 88170
    },
    {
      "epoch": 4.702933333333333,
      "grad_norm": 0.04415520280599594,
      "learning_rate": 2.0606666666666667e-05,
      "loss": 0.0016,
      "step": 88180
    },
    {
      "epoch": 4.7034666666666665,
      "grad_norm": 0.22964046895503998,
      "learning_rate": 2.0603333333333337e-05,
      "loss": 0.0028,
      "step": 88190
    },
    {
      "epoch": 4.704,
      "grad_norm": 0.03828740492463112,
      "learning_rate": 2.06e-05,
      "loss": 0.0021,
      "step": 88200
    },
    {
      "epoch": 4.704533333333333,
      "grad_norm": 0.1548304408788681,
      "learning_rate": 2.0596666666666665e-05,
      "loss": 0.0016,
      "step": 88210
    },
    {
      "epoch": 4.705066666666666,
      "grad_norm": 0.3387839198112488,
      "learning_rate": 2.0593333333333335e-05,
      "loss": 0.0021,
      "step": 88220
    },
    {
      "epoch": 4.7056000000000004,
      "grad_norm": 0.6151472926139832,
      "learning_rate": 2.059e-05,
      "loss": 0.0025,
      "step": 88230
    },
    {
      "epoch": 4.706133333333334,
      "grad_norm": 0.2662718594074249,
      "learning_rate": 2.0586666666666667e-05,
      "loss": 0.0014,
      "step": 88240
    },
    {
      "epoch": 4.706666666666667,
      "grad_norm": 0.18141914904117584,
      "learning_rate": 2.0583333333333333e-05,
      "loss": 0.0023,
      "step": 88250
    },
    {
      "epoch": 4.7072,
      "grad_norm": 0.2023184597492218,
      "learning_rate": 2.0580000000000003e-05,
      "loss": 0.0017,
      "step": 88260
    },
    {
      "epoch": 4.7077333333333335,
      "grad_norm": 0.15199856460094452,
      "learning_rate": 2.0576666666666666e-05,
      "loss": 0.0024,
      "step": 88270
    },
    {
      "epoch": 4.708266666666667,
      "grad_norm": 0.060494329780340195,
      "learning_rate": 2.0573333333333332e-05,
      "loss": 0.0019,
      "step": 88280
    },
    {
      "epoch": 4.7088,
      "grad_norm": 0.4655115008354187,
      "learning_rate": 2.057e-05,
      "loss": 0.0017,
      "step": 88290
    },
    {
      "epoch": 4.709333333333333,
      "grad_norm": 0.18417321145534515,
      "learning_rate": 2.0566666666666667e-05,
      "loss": 0.0022,
      "step": 88300
    },
    {
      "epoch": 4.709866666666667,
      "grad_norm": 0.39121732115745544,
      "learning_rate": 2.0563333333333334e-05,
      "loss": 0.0022,
      "step": 88310
    },
    {
      "epoch": 4.7104,
      "grad_norm": 0.08575671911239624,
      "learning_rate": 2.0560000000000003e-05,
      "loss": 0.0019,
      "step": 88320
    },
    {
      "epoch": 4.710933333333333,
      "grad_norm": 0.19785626232624054,
      "learning_rate": 2.055666666666667e-05,
      "loss": 0.0019,
      "step": 88330
    },
    {
      "epoch": 4.7114666666666665,
      "grad_norm": 0.4315151274204254,
      "learning_rate": 2.0553333333333335e-05,
      "loss": 0.0016,
      "step": 88340
    },
    {
      "epoch": 4.712,
      "grad_norm": 0.07057204842567444,
      "learning_rate": 2.055e-05,
      "loss": 0.0017,
      "step": 88350
    },
    {
      "epoch": 4.712533333333333,
      "grad_norm": 0.2311098724603653,
      "learning_rate": 2.0546666666666668e-05,
      "loss": 0.0023,
      "step": 88360
    },
    {
      "epoch": 4.713066666666666,
      "grad_norm": 0.06123460456728935,
      "learning_rate": 2.0543333333333334e-05,
      "loss": 0.0018,
      "step": 88370
    },
    {
      "epoch": 4.7136,
      "grad_norm": 0.17532114684581757,
      "learning_rate": 2.054e-05,
      "loss": 0.0021,
      "step": 88380
    },
    {
      "epoch": 4.714133333333333,
      "grad_norm": 0.4978429675102234,
      "learning_rate": 2.053666666666667e-05,
      "loss": 0.0022,
      "step": 88390
    },
    {
      "epoch": 4.714666666666667,
      "grad_norm": 0.03637145459651947,
      "learning_rate": 2.0533333333333336e-05,
      "loss": 0.0018,
      "step": 88400
    },
    {
      "epoch": 4.7152,
      "grad_norm": 0.06992265582084656,
      "learning_rate": 2.053e-05,
      "loss": 0.0012,
      "step": 88410
    },
    {
      "epoch": 4.7157333333333336,
      "grad_norm": 0.14631813764572144,
      "learning_rate": 2.0526666666666668e-05,
      "loss": 0.0019,
      "step": 88420
    },
    {
      "epoch": 4.716266666666667,
      "grad_norm": 0.08948057144880295,
      "learning_rate": 2.0523333333333334e-05,
      "loss": 0.002,
      "step": 88430
    },
    {
      "epoch": 4.7168,
      "grad_norm": 0.041534073650836945,
      "learning_rate": 2.052e-05,
      "loss": 0.0021,
      "step": 88440
    },
    {
      "epoch": 4.717333333333333,
      "grad_norm": 0.429399698972702,
      "learning_rate": 2.0516666666666666e-05,
      "loss": 0.0022,
      "step": 88450
    },
    {
      "epoch": 4.717866666666667,
      "grad_norm": 0.16556872427463531,
      "learning_rate": 2.0513333333333336e-05,
      "loss": 0.002,
      "step": 88460
    },
    {
      "epoch": 4.7184,
      "grad_norm": 0.20218409597873688,
      "learning_rate": 2.0510000000000002e-05,
      "loss": 0.0014,
      "step": 88470
    },
    {
      "epoch": 4.718933333333333,
      "grad_norm": 0.18218596279621124,
      "learning_rate": 2.0506666666666668e-05,
      "loss": 0.0022,
      "step": 88480
    },
    {
      "epoch": 4.7194666666666665,
      "grad_norm": 0.3255857825279236,
      "learning_rate": 2.0503333333333334e-05,
      "loss": 0.0019,
      "step": 88490
    },
    {
      "epoch": 4.72,
      "grad_norm": 0.09197176247835159,
      "learning_rate": 2.05e-05,
      "loss": 0.003,
      "step": 88500
    },
    {
      "epoch": 4.720533333333333,
      "grad_norm": 0.4029744863510132,
      "learning_rate": 2.0496666666666666e-05,
      "loss": 0.0018,
      "step": 88510
    },
    {
      "epoch": 4.721066666666666,
      "grad_norm": 0.23360182344913483,
      "learning_rate": 2.0493333333333333e-05,
      "loss": 0.0019,
      "step": 88520
    },
    {
      "epoch": 4.7216000000000005,
      "grad_norm": 0.5254340767860413,
      "learning_rate": 2.0490000000000002e-05,
      "loss": 0.0018,
      "step": 88530
    },
    {
      "epoch": 4.722133333333334,
      "grad_norm": 0.26382502913475037,
      "learning_rate": 2.0486666666666668e-05,
      "loss": 0.002,
      "step": 88540
    },
    {
      "epoch": 4.722666666666667,
      "grad_norm": 0.13203926384449005,
      "learning_rate": 2.0483333333333334e-05,
      "loss": 0.0027,
      "step": 88550
    },
    {
      "epoch": 4.7232,
      "grad_norm": 0.6934802532196045,
      "learning_rate": 2.048e-05,
      "loss": 0.0021,
      "step": 88560
    },
    {
      "epoch": 4.723733333333334,
      "grad_norm": 0.26796260476112366,
      "learning_rate": 2.047666666666667e-05,
      "loss": 0.0019,
      "step": 88570
    },
    {
      "epoch": 4.724266666666667,
      "grad_norm": 0.33892181515693665,
      "learning_rate": 2.0473333333333333e-05,
      "loss": 0.0017,
      "step": 88580
    },
    {
      "epoch": 4.7248,
      "grad_norm": 0.054171591997146606,
      "learning_rate": 2.047e-05,
      "loss": 0.0023,
      "step": 88590
    },
    {
      "epoch": 4.725333333333333,
      "grad_norm": 0.1734188348054886,
      "learning_rate": 2.046666666666667e-05,
      "loss": 0.0023,
      "step": 88600
    },
    {
      "epoch": 4.725866666666667,
      "grad_norm": 0.27010342478752136,
      "learning_rate": 2.0463333333333334e-05,
      "loss": 0.0014,
      "step": 88610
    },
    {
      "epoch": 4.7264,
      "grad_norm": 0.47305893898010254,
      "learning_rate": 2.046e-05,
      "loss": 0.0017,
      "step": 88620
    },
    {
      "epoch": 4.726933333333333,
      "grad_norm": 0.03191527724266052,
      "learning_rate": 2.0456666666666667e-05,
      "loss": 0.0017,
      "step": 88630
    },
    {
      "epoch": 4.7274666666666665,
      "grad_norm": 0.07797186076641083,
      "learning_rate": 2.0453333333333336e-05,
      "loss": 0.0016,
      "step": 88640
    },
    {
      "epoch": 4.728,
      "grad_norm": 0.2000044286251068,
      "learning_rate": 2.045e-05,
      "loss": 0.0022,
      "step": 88650
    },
    {
      "epoch": 4.728533333333333,
      "grad_norm": 0.3348388969898224,
      "learning_rate": 2.0446666666666665e-05,
      "loss": 0.0023,
      "step": 88660
    },
    {
      "epoch": 4.729066666666666,
      "grad_norm": 0.28856977820396423,
      "learning_rate": 2.0443333333333335e-05,
      "loss": 0.0021,
      "step": 88670
    },
    {
      "epoch": 4.7296,
      "grad_norm": 0.2877821922302246,
      "learning_rate": 2.044e-05,
      "loss": 0.0021,
      "step": 88680
    },
    {
      "epoch": 4.730133333333333,
      "grad_norm": 0.0873115211725235,
      "learning_rate": 2.0436666666666667e-05,
      "loss": 0.0029,
      "step": 88690
    },
    {
      "epoch": 4.730666666666667,
      "grad_norm": 0.4701589345932007,
      "learning_rate": 2.0433333333333336e-05,
      "loss": 0.002,
      "step": 88700
    },
    {
      "epoch": 4.7312,
      "grad_norm": 0.3856210708618164,
      "learning_rate": 2.0430000000000003e-05,
      "loss": 0.0019,
      "step": 88710
    },
    {
      "epoch": 4.731733333333334,
      "grad_norm": 0.3478515148162842,
      "learning_rate": 2.042666666666667e-05,
      "loss": 0.0025,
      "step": 88720
    },
    {
      "epoch": 4.732266666666667,
      "grad_norm": 0.05713099613785744,
      "learning_rate": 2.0423333333333335e-05,
      "loss": 0.0025,
      "step": 88730
    },
    {
      "epoch": 4.7328,
      "grad_norm": 0.18158333003520966,
      "learning_rate": 2.042e-05,
      "loss": 0.0015,
      "step": 88740
    },
    {
      "epoch": 4.733333333333333,
      "grad_norm": 0.04688300937414169,
      "learning_rate": 2.0416666666666667e-05,
      "loss": 0.0016,
      "step": 88750
    },
    {
      "epoch": 4.733866666666667,
      "grad_norm": 0.09244391322135925,
      "learning_rate": 2.0413333333333333e-05,
      "loss": 0.0025,
      "step": 88760
    },
    {
      "epoch": 4.7344,
      "grad_norm": 0.01906893029808998,
      "learning_rate": 2.0410000000000003e-05,
      "loss": 0.0014,
      "step": 88770
    },
    {
      "epoch": 4.734933333333333,
      "grad_norm": 0.23796531558036804,
      "learning_rate": 2.040666666666667e-05,
      "loss": 0.0022,
      "step": 88780
    },
    {
      "epoch": 4.7354666666666665,
      "grad_norm": 0.25810113549232483,
      "learning_rate": 2.0403333333333335e-05,
      "loss": 0.0017,
      "step": 88790
    },
    {
      "epoch": 4.736,
      "grad_norm": 0.3778032660484314,
      "learning_rate": 2.04e-05,
      "loss": 0.002,
      "step": 88800
    },
    {
      "epoch": 4.736533333333333,
      "grad_norm": 0.4621601998806,
      "learning_rate": 2.0396666666666667e-05,
      "loss": 0.0015,
      "step": 88810
    },
    {
      "epoch": 4.737066666666666,
      "grad_norm": 0.09399145841598511,
      "learning_rate": 2.0393333333333333e-05,
      "loss": 0.0018,
      "step": 88820
    },
    {
      "epoch": 4.7376000000000005,
      "grad_norm": 0.11666274815797806,
      "learning_rate": 2.039e-05,
      "loss": 0.002,
      "step": 88830
    },
    {
      "epoch": 4.738133333333334,
      "grad_norm": 0.3352326452732086,
      "learning_rate": 2.038666666666667e-05,
      "loss": 0.0023,
      "step": 88840
    },
    {
      "epoch": 4.738666666666667,
      "grad_norm": 0.6791976094245911,
      "learning_rate": 2.0383333333333335e-05,
      "loss": 0.0017,
      "step": 88850
    },
    {
      "epoch": 4.7392,
      "grad_norm": 0.5595937967300415,
      "learning_rate": 2.038e-05,
      "loss": 0.0019,
      "step": 88860
    },
    {
      "epoch": 4.739733333333334,
      "grad_norm": 0.159868061542511,
      "learning_rate": 2.0376666666666668e-05,
      "loss": 0.0019,
      "step": 88870
    },
    {
      "epoch": 4.740266666666667,
      "grad_norm": 0.49431005120277405,
      "learning_rate": 2.0373333333333334e-05,
      "loss": 0.0018,
      "step": 88880
    },
    {
      "epoch": 4.7408,
      "grad_norm": 0.1999272108078003,
      "learning_rate": 2.037e-05,
      "loss": 0.0016,
      "step": 88890
    },
    {
      "epoch": 4.741333333333333,
      "grad_norm": 0.0864628404378891,
      "learning_rate": 2.0366666666666666e-05,
      "loss": 0.0017,
      "step": 88900
    },
    {
      "epoch": 4.741866666666667,
      "grad_norm": 0.06204367056488991,
      "learning_rate": 2.0363333333333335e-05,
      "loss": 0.002,
      "step": 88910
    },
    {
      "epoch": 4.7424,
      "grad_norm": 0.0849282518029213,
      "learning_rate": 2.036e-05,
      "loss": 0.0019,
      "step": 88920
    },
    {
      "epoch": 4.742933333333333,
      "grad_norm": 0.2491295039653778,
      "learning_rate": 2.0356666666666668e-05,
      "loss": 0.0021,
      "step": 88930
    },
    {
      "epoch": 4.7434666666666665,
      "grad_norm": 0.09036828577518463,
      "learning_rate": 2.0353333333333334e-05,
      "loss": 0.0028,
      "step": 88940
    },
    {
      "epoch": 4.744,
      "grad_norm": 0.5695330500602722,
      "learning_rate": 2.035e-05,
      "loss": 0.0016,
      "step": 88950
    },
    {
      "epoch": 4.744533333333333,
      "grad_norm": 0.0961867943406105,
      "learning_rate": 2.0346666666666666e-05,
      "loss": 0.0027,
      "step": 88960
    },
    {
      "epoch": 4.745066666666666,
      "grad_norm": 0.11592287570238113,
      "learning_rate": 2.0343333333333332e-05,
      "loss": 0.0021,
      "step": 88970
    },
    {
      "epoch": 4.7456,
      "grad_norm": 0.04129675775766373,
      "learning_rate": 2.0340000000000002e-05,
      "loss": 0.0022,
      "step": 88980
    },
    {
      "epoch": 4.746133333333333,
      "grad_norm": 0.49031463265419006,
      "learning_rate": 2.0336666666666668e-05,
      "loss": 0.0015,
      "step": 88990
    },
    {
      "epoch": 4.746666666666667,
      "grad_norm": 0.14365600049495697,
      "learning_rate": 2.0333333333333334e-05,
      "loss": 0.0019,
      "step": 89000
    },
    {
      "epoch": 4.7472,
      "grad_norm": 0.047901056706905365,
      "learning_rate": 2.033e-05,
      "loss": 0.0021,
      "step": 89010
    },
    {
      "epoch": 4.747733333333334,
      "grad_norm": 0.17828084528446198,
      "learning_rate": 2.032666666666667e-05,
      "loss": 0.0017,
      "step": 89020
    },
    {
      "epoch": 4.748266666666667,
      "grad_norm": 0.2907247543334961,
      "learning_rate": 2.0323333333333332e-05,
      "loss": 0.0019,
      "step": 89030
    },
    {
      "epoch": 4.7488,
      "grad_norm": 0.06063273549079895,
      "learning_rate": 2.032e-05,
      "loss": 0.0013,
      "step": 89040
    },
    {
      "epoch": 4.749333333333333,
      "grad_norm": 0.09235338121652603,
      "learning_rate": 2.0316666666666668e-05,
      "loss": 0.002,
      "step": 89050
    },
    {
      "epoch": 4.749866666666667,
      "grad_norm": 0.18183079361915588,
      "learning_rate": 2.0313333333333334e-05,
      "loss": 0.0018,
      "step": 89060
    },
    {
      "epoch": 4.7504,
      "grad_norm": 0.17365586757659912,
      "learning_rate": 2.031e-05,
      "loss": 0.0029,
      "step": 89070
    },
    {
      "epoch": 4.750933333333333,
      "grad_norm": 0.09280133247375488,
      "learning_rate": 2.030666666666667e-05,
      "loss": 0.0017,
      "step": 89080
    },
    {
      "epoch": 4.7514666666666665,
      "grad_norm": 0.03220387548208237,
      "learning_rate": 2.0303333333333336e-05,
      "loss": 0.002,
      "step": 89090
    },
    {
      "epoch": 4.752,
      "grad_norm": 0.146086186170578,
      "learning_rate": 2.0300000000000002e-05,
      "loss": 0.0021,
      "step": 89100
    },
    {
      "epoch": 4.752533333333333,
      "grad_norm": 0.24230264127254486,
      "learning_rate": 2.0296666666666668e-05,
      "loss": 0.0017,
      "step": 89110
    },
    {
      "epoch": 4.753066666666666,
      "grad_norm": 0.3176150918006897,
      "learning_rate": 2.0293333333333334e-05,
      "loss": 0.0019,
      "step": 89120
    },
    {
      "epoch": 4.7536000000000005,
      "grad_norm": 0.2878745496273041,
      "learning_rate": 2.029e-05,
      "loss": 0.0015,
      "step": 89130
    },
    {
      "epoch": 4.754133333333334,
      "grad_norm": 0.03327302634716034,
      "learning_rate": 2.0286666666666667e-05,
      "loss": 0.002,
      "step": 89140
    },
    {
      "epoch": 4.754666666666667,
      "grad_norm": 0.3219710886478424,
      "learning_rate": 2.0283333333333336e-05,
      "loss": 0.0015,
      "step": 89150
    },
    {
      "epoch": 4.7552,
      "grad_norm": 0.26631981134414673,
      "learning_rate": 2.0280000000000002e-05,
      "loss": 0.0021,
      "step": 89160
    },
    {
      "epoch": 4.755733333333334,
      "grad_norm": 0.149482861161232,
      "learning_rate": 2.027666666666667e-05,
      "loss": 0.0019,
      "step": 89170
    },
    {
      "epoch": 4.756266666666667,
      "grad_norm": 0.14955654740333557,
      "learning_rate": 2.0273333333333335e-05,
      "loss": 0.0019,
      "step": 89180
    },
    {
      "epoch": 4.7568,
      "grad_norm": 0.15018466114997864,
      "learning_rate": 2.027e-05,
      "loss": 0.002,
      "step": 89190
    },
    {
      "epoch": 4.757333333333333,
      "grad_norm": 0.0733824148774147,
      "learning_rate": 2.0266666666666667e-05,
      "loss": 0.0017,
      "step": 89200
    },
    {
      "epoch": 4.757866666666667,
      "grad_norm": 0.21148082613945007,
      "learning_rate": 2.0263333333333333e-05,
      "loss": 0.002,
      "step": 89210
    },
    {
      "epoch": 4.7584,
      "grad_norm": 0.1431782841682434,
      "learning_rate": 2.0260000000000003e-05,
      "loss": 0.0019,
      "step": 89220
    },
    {
      "epoch": 4.758933333333333,
      "grad_norm": 0.35532698035240173,
      "learning_rate": 2.025666666666667e-05,
      "loss": 0.0019,
      "step": 89230
    },
    {
      "epoch": 4.7594666666666665,
      "grad_norm": 0.5209648013114929,
      "learning_rate": 2.0253333333333335e-05,
      "loss": 0.0036,
      "step": 89240
    },
    {
      "epoch": 4.76,
      "grad_norm": 0.3481249511241913,
      "learning_rate": 2.025e-05,
      "loss": 0.0019,
      "step": 89250
    },
    {
      "epoch": 4.760533333333333,
      "grad_norm": 0.17642953991889954,
      "learning_rate": 2.0246666666666667e-05,
      "loss": 0.0021,
      "step": 89260
    },
    {
      "epoch": 4.761066666666666,
      "grad_norm": 0.1764032244682312,
      "learning_rate": 2.0243333333333333e-05,
      "loss": 0.0028,
      "step": 89270
    },
    {
      "epoch": 4.7616,
      "grad_norm": 0.46205636858940125,
      "learning_rate": 2.024e-05,
      "loss": 0.0017,
      "step": 89280
    },
    {
      "epoch": 4.762133333333333,
      "grad_norm": 0.29799333214759827,
      "learning_rate": 2.023666666666667e-05,
      "loss": 0.0015,
      "step": 89290
    },
    {
      "epoch": 4.762666666666667,
      "grad_norm": 0.259027898311615,
      "learning_rate": 2.0233333333333335e-05,
      "loss": 0.0014,
      "step": 89300
    },
    {
      "epoch": 4.7632,
      "grad_norm": 0.04574744403362274,
      "learning_rate": 2.023e-05,
      "loss": 0.0026,
      "step": 89310
    },
    {
      "epoch": 4.763733333333334,
      "grad_norm": 0.14566078782081604,
      "learning_rate": 2.0226666666666667e-05,
      "loss": 0.0013,
      "step": 89320
    },
    {
      "epoch": 4.764266666666667,
      "grad_norm": 0.26926150918006897,
      "learning_rate": 2.0223333333333333e-05,
      "loss": 0.0021,
      "step": 89330
    },
    {
      "epoch": 4.7648,
      "grad_norm": 0.40589940547943115,
      "learning_rate": 2.022e-05,
      "loss": 0.0023,
      "step": 89340
    },
    {
      "epoch": 4.765333333333333,
      "grad_norm": 0.26475098729133606,
      "learning_rate": 2.0216666666666666e-05,
      "loss": 0.0015,
      "step": 89350
    },
    {
      "epoch": 4.765866666666667,
      "grad_norm": 0.3578982353210449,
      "learning_rate": 2.0213333333333335e-05,
      "loss": 0.0021,
      "step": 89360
    },
    {
      "epoch": 4.7664,
      "grad_norm": 0.3193630576133728,
      "learning_rate": 2.021e-05,
      "loss": 0.0022,
      "step": 89370
    },
    {
      "epoch": 4.766933333333333,
      "grad_norm": 0.14911672472953796,
      "learning_rate": 2.0206666666666667e-05,
      "loss": 0.0022,
      "step": 89380
    },
    {
      "epoch": 4.7674666666666665,
      "grad_norm": 0.14912576973438263,
      "learning_rate": 2.0203333333333334e-05,
      "loss": 0.002,
      "step": 89390
    },
    {
      "epoch": 4.768,
      "grad_norm": 0.6597595810890198,
      "learning_rate": 2.0200000000000003e-05,
      "loss": 0.0021,
      "step": 89400
    },
    {
      "epoch": 4.768533333333333,
      "grad_norm": 0.0468318797647953,
      "learning_rate": 2.0196666666666666e-05,
      "loss": 0.0021,
      "step": 89410
    },
    {
      "epoch": 4.769066666666666,
      "grad_norm": 0.29040926694869995,
      "learning_rate": 2.0193333333333332e-05,
      "loss": 0.0018,
      "step": 89420
    },
    {
      "epoch": 4.7696,
      "grad_norm": 0.4310232102870941,
      "learning_rate": 2.019e-05,
      "loss": 0.0019,
      "step": 89430
    },
    {
      "epoch": 4.770133333333334,
      "grad_norm": 0.20708495378494263,
      "learning_rate": 2.0186666666666668e-05,
      "loss": 0.0015,
      "step": 89440
    },
    {
      "epoch": 4.770666666666667,
      "grad_norm": 0.6731299757957458,
      "learning_rate": 2.0183333333333334e-05,
      "loss": 0.0018,
      "step": 89450
    },
    {
      "epoch": 4.7712,
      "grad_norm": 0.0910966545343399,
      "learning_rate": 2.0180000000000003e-05,
      "loss": 0.0015,
      "step": 89460
    },
    {
      "epoch": 4.771733333333334,
      "grad_norm": 0.38644087314605713,
      "learning_rate": 2.017666666666667e-05,
      "loss": 0.0016,
      "step": 89470
    },
    {
      "epoch": 4.772266666666667,
      "grad_norm": 0.1412084549665451,
      "learning_rate": 2.0173333333333332e-05,
      "loss": 0.0014,
      "step": 89480
    },
    {
      "epoch": 4.7728,
      "grad_norm": 0.5404205918312073,
      "learning_rate": 2.017e-05,
      "loss": 0.0027,
      "step": 89490
    },
    {
      "epoch": 4.773333333333333,
      "grad_norm": 0.2003019005060196,
      "learning_rate": 2.0166666666666668e-05,
      "loss": 0.0016,
      "step": 89500
    },
    {
      "epoch": 4.773866666666667,
      "grad_norm": 0.3545561730861664,
      "learning_rate": 2.0163333333333334e-05,
      "loss": 0.0025,
      "step": 89510
    },
    {
      "epoch": 4.7744,
      "grad_norm": 0.10405416786670685,
      "learning_rate": 2.016e-05,
      "loss": 0.0025,
      "step": 89520
    },
    {
      "epoch": 4.774933333333333,
      "grad_norm": 0.20674313604831696,
      "learning_rate": 2.015666666666667e-05,
      "loss": 0.0018,
      "step": 89530
    },
    {
      "epoch": 4.7754666666666665,
      "grad_norm": 0.5901496410369873,
      "learning_rate": 2.0153333333333336e-05,
      "loss": 0.0023,
      "step": 89540
    },
    {
      "epoch": 4.776,
      "grad_norm": 0.292226642370224,
      "learning_rate": 2.0150000000000002e-05,
      "loss": 0.0017,
      "step": 89550
    },
    {
      "epoch": 4.776533333333333,
      "grad_norm": 0.24020473659038544,
      "learning_rate": 2.0146666666666668e-05,
      "loss": 0.002,
      "step": 89560
    },
    {
      "epoch": 4.777066666666666,
      "grad_norm": 0.43135279417037964,
      "learning_rate": 2.0143333333333334e-05,
      "loss": 0.0026,
      "step": 89570
    },
    {
      "epoch": 4.7776,
      "grad_norm": 0.5711296796798706,
      "learning_rate": 2.014e-05,
      "loss": 0.0017,
      "step": 89580
    },
    {
      "epoch": 4.778133333333333,
      "grad_norm": 0.4089914858341217,
      "learning_rate": 2.0136666666666666e-05,
      "loss": 0.0016,
      "step": 89590
    },
    {
      "epoch": 4.778666666666666,
      "grad_norm": 0.12206544727087021,
      "learning_rate": 2.0133333333333336e-05,
      "loss": 0.0016,
      "step": 89600
    },
    {
      "epoch": 4.7792,
      "grad_norm": 0.22764448821544647,
      "learning_rate": 2.0130000000000002e-05,
      "loss": 0.0023,
      "step": 89610
    },
    {
      "epoch": 4.779733333333334,
      "grad_norm": 0.025822553783655167,
      "learning_rate": 2.0126666666666668e-05,
      "loss": 0.0026,
      "step": 89620
    },
    {
      "epoch": 4.780266666666667,
      "grad_norm": 0.17803063988685608,
      "learning_rate": 2.0123333333333334e-05,
      "loss": 0.0025,
      "step": 89630
    },
    {
      "epoch": 4.7808,
      "grad_norm": 0.18584538996219635,
      "learning_rate": 2.012e-05,
      "loss": 0.0023,
      "step": 89640
    },
    {
      "epoch": 4.781333333333333,
      "grad_norm": 0.03985893726348877,
      "learning_rate": 2.0116666666666667e-05,
      "loss": 0.0031,
      "step": 89650
    },
    {
      "epoch": 4.781866666666667,
      "grad_norm": 0.4041820168495178,
      "learning_rate": 2.0113333333333333e-05,
      "loss": 0.0022,
      "step": 89660
    },
    {
      "epoch": 4.7824,
      "grad_norm": 0.07193892449140549,
      "learning_rate": 2.0110000000000002e-05,
      "loss": 0.0014,
      "step": 89670
    },
    {
      "epoch": 4.782933333333333,
      "grad_norm": 0.14943264424800873,
      "learning_rate": 2.010666666666667e-05,
      "loss": 0.0016,
      "step": 89680
    },
    {
      "epoch": 4.7834666666666665,
      "grad_norm": 0.23347309231758118,
      "learning_rate": 2.0103333333333335e-05,
      "loss": 0.0014,
      "step": 89690
    },
    {
      "epoch": 4.784,
      "grad_norm": 0.1769038885831833,
      "learning_rate": 2.01e-05,
      "loss": 0.0017,
      "step": 89700
    },
    {
      "epoch": 4.784533333333333,
      "grad_norm": 0.31300118565559387,
      "learning_rate": 2.0096666666666667e-05,
      "loss": 0.0015,
      "step": 89710
    },
    {
      "epoch": 4.785066666666666,
      "grad_norm": 0.23553012311458588,
      "learning_rate": 2.0093333333333333e-05,
      "loss": 0.0023,
      "step": 89720
    },
    {
      "epoch": 4.7856,
      "grad_norm": 0.37779930233955383,
      "learning_rate": 2.009e-05,
      "loss": 0.0016,
      "step": 89730
    },
    {
      "epoch": 4.786133333333334,
      "grad_norm": 0.1306765079498291,
      "learning_rate": 2.008666666666667e-05,
      "loss": 0.0016,
      "step": 89740
    },
    {
      "epoch": 4.786666666666667,
      "grad_norm": 0.17746546864509583,
      "learning_rate": 2.0083333333333335e-05,
      "loss": 0.0018,
      "step": 89750
    },
    {
      "epoch": 4.7872,
      "grad_norm": 0.19828841090202332,
      "learning_rate": 2.008e-05,
      "loss": 0.0013,
      "step": 89760
    },
    {
      "epoch": 4.787733333333334,
      "grad_norm": 0.04068732634186745,
      "learning_rate": 2.0076666666666667e-05,
      "loss": 0.0021,
      "step": 89770
    },
    {
      "epoch": 4.788266666666667,
      "grad_norm": 0.25856876373291016,
      "learning_rate": 2.0073333333333337e-05,
      "loss": 0.0018,
      "step": 89780
    },
    {
      "epoch": 4.7888,
      "grad_norm": 0.3664017915725708,
      "learning_rate": 2.007e-05,
      "loss": 0.0022,
      "step": 89790
    },
    {
      "epoch": 4.789333333333333,
      "grad_norm": 0.02980606071650982,
      "learning_rate": 2.0066666666666665e-05,
      "loss": 0.0018,
      "step": 89800
    },
    {
      "epoch": 4.789866666666667,
      "grad_norm": 0.18001116812229156,
      "learning_rate": 2.0063333333333335e-05,
      "loss": 0.0011,
      "step": 89810
    },
    {
      "epoch": 4.7904,
      "grad_norm": 0.37223556637763977,
      "learning_rate": 2.006e-05,
      "loss": 0.0023,
      "step": 89820
    },
    {
      "epoch": 4.790933333333333,
      "grad_norm": 0.48977750539779663,
      "learning_rate": 2.0056666666666667e-05,
      "loss": 0.0015,
      "step": 89830
    },
    {
      "epoch": 4.7914666666666665,
      "grad_norm": 0.42237386107444763,
      "learning_rate": 2.0053333333333337e-05,
      "loss": 0.002,
      "step": 89840
    },
    {
      "epoch": 4.792,
      "grad_norm": 0.3084939122200012,
      "learning_rate": 2.0050000000000003e-05,
      "loss": 0.0019,
      "step": 89850
    },
    {
      "epoch": 4.792533333333333,
      "grad_norm": 0.12907397747039795,
      "learning_rate": 2.0046666666666666e-05,
      "loss": 0.0017,
      "step": 89860
    },
    {
      "epoch": 4.793066666666666,
      "grad_norm": 0.11869807541370392,
      "learning_rate": 2.0043333333333332e-05,
      "loss": 0.002,
      "step": 89870
    },
    {
      "epoch": 4.7936,
      "grad_norm": 0.21285554766654968,
      "learning_rate": 2.004e-05,
      "loss": 0.0014,
      "step": 89880
    },
    {
      "epoch": 4.794133333333333,
      "grad_norm": 0.5046353340148926,
      "learning_rate": 2.0036666666666667e-05,
      "loss": 0.0023,
      "step": 89890
    },
    {
      "epoch": 4.794666666666666,
      "grad_norm": 0.15201731026172638,
      "learning_rate": 2.0033333333333334e-05,
      "loss": 0.0016,
      "step": 89900
    },
    {
      "epoch": 4.7952,
      "grad_norm": 0.15127506852149963,
      "learning_rate": 2.0030000000000003e-05,
      "loss": 0.0016,
      "step": 89910
    },
    {
      "epoch": 4.795733333333334,
      "grad_norm": 0.3495786190032959,
      "learning_rate": 2.002666666666667e-05,
      "loss": 0.002,
      "step": 89920
    },
    {
      "epoch": 4.796266666666667,
      "grad_norm": 0.045280348509550095,
      "learning_rate": 2.0023333333333335e-05,
      "loss": 0.0013,
      "step": 89930
    },
    {
      "epoch": 4.7968,
      "grad_norm": 0.1275130808353424,
      "learning_rate": 2.002e-05,
      "loss": 0.0021,
      "step": 89940
    },
    {
      "epoch": 4.7973333333333334,
      "grad_norm": 0.08756324648857117,
      "learning_rate": 2.0016666666666668e-05,
      "loss": 0.0021,
      "step": 89950
    },
    {
      "epoch": 4.797866666666667,
      "grad_norm": 0.17260725796222687,
      "learning_rate": 2.0013333333333334e-05,
      "loss": 0.0019,
      "step": 89960
    },
    {
      "epoch": 4.7984,
      "grad_norm": 0.34476909041404724,
      "learning_rate": 2.001e-05,
      "loss": 0.0018,
      "step": 89970
    },
    {
      "epoch": 4.798933333333333,
      "grad_norm": 0.1454058140516281,
      "learning_rate": 2.000666666666667e-05,
      "loss": 0.0017,
      "step": 89980
    },
    {
      "epoch": 4.7994666666666665,
      "grad_norm": 0.34400925040245056,
      "learning_rate": 2.0003333333333336e-05,
      "loss": 0.0026,
      "step": 89990
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.03749994561076164,
      "learning_rate": 2e-05,
      "loss": 0.0025,
      "step": 90000
    },
    {
      "epoch": 4.800533333333333,
      "grad_norm": 0.32977914810180664,
      "learning_rate": 1.9996666666666668e-05,
      "loss": 0.0023,
      "step": 90010
    },
    {
      "epoch": 4.801066666666666,
      "grad_norm": 0.18843825161457062,
      "learning_rate": 1.9993333333333334e-05,
      "loss": 0.0017,
      "step": 90020
    },
    {
      "epoch": 4.8016,
      "grad_norm": 0.10433673113584518,
      "learning_rate": 1.999e-05,
      "loss": 0.0021,
      "step": 90030
    },
    {
      "epoch": 4.802133333333334,
      "grad_norm": 0.23723560571670532,
      "learning_rate": 1.9986666666666666e-05,
      "loss": 0.0017,
      "step": 90040
    },
    {
      "epoch": 4.802666666666667,
      "grad_norm": 0.28380998969078064,
      "learning_rate": 1.9983333333333336e-05,
      "loss": 0.003,
      "step": 90050
    },
    {
      "epoch": 4.8032,
      "grad_norm": 0.2701343894004822,
      "learning_rate": 1.9980000000000002e-05,
      "loss": 0.0015,
      "step": 90060
    },
    {
      "epoch": 4.803733333333334,
      "grad_norm": 0.3474465310573578,
      "learning_rate": 1.9976666666666668e-05,
      "loss": 0.0014,
      "step": 90070
    },
    {
      "epoch": 4.804266666666667,
      "grad_norm": 0.07851525396108627,
      "learning_rate": 1.9973333333333334e-05,
      "loss": 0.0016,
      "step": 90080
    },
    {
      "epoch": 4.8048,
      "grad_norm": 0.4309263527393341,
      "learning_rate": 1.997e-05,
      "loss": 0.0023,
      "step": 90090
    },
    {
      "epoch": 4.8053333333333335,
      "grad_norm": 0.22762826085090637,
      "learning_rate": 1.9966666666666666e-05,
      "loss": 0.0013,
      "step": 90100
    },
    {
      "epoch": 4.805866666666667,
      "grad_norm": 0.17513762414455414,
      "learning_rate": 1.9963333333333332e-05,
      "loss": 0.0023,
      "step": 90110
    },
    {
      "epoch": 4.8064,
      "grad_norm": 0.4142163395881653,
      "learning_rate": 1.9960000000000002e-05,
      "loss": 0.002,
      "step": 90120
    },
    {
      "epoch": 4.806933333333333,
      "grad_norm": 0.05251309275627136,
      "learning_rate": 1.9956666666666668e-05,
      "loss": 0.0016,
      "step": 90130
    },
    {
      "epoch": 4.8074666666666666,
      "grad_norm": 0.14895334839820862,
      "learning_rate": 1.9953333333333334e-05,
      "loss": 0.0021,
      "step": 90140
    },
    {
      "epoch": 4.808,
      "grad_norm": 0.14368411898612976,
      "learning_rate": 1.995e-05,
      "loss": 0.002,
      "step": 90150
    },
    {
      "epoch": 4.808533333333333,
      "grad_norm": 0.08845348656177521,
      "learning_rate": 1.9946666666666667e-05,
      "loss": 0.0022,
      "step": 90160
    },
    {
      "epoch": 4.809066666666666,
      "grad_norm": 0.23452122509479523,
      "learning_rate": 1.9943333333333333e-05,
      "loss": 0.0026,
      "step": 90170
    },
    {
      "epoch": 4.8096,
      "grad_norm": 0.15894564986228943,
      "learning_rate": 1.994e-05,
      "loss": 0.0018,
      "step": 90180
    },
    {
      "epoch": 4.810133333333333,
      "grad_norm": 0.07512614876031876,
      "learning_rate": 1.993666666666667e-05,
      "loss": 0.0016,
      "step": 90190
    },
    {
      "epoch": 4.810666666666666,
      "grad_norm": 0.06420598179101944,
      "learning_rate": 1.9933333333333334e-05,
      "loss": 0.0014,
      "step": 90200
    },
    {
      "epoch": 4.8112,
      "grad_norm": 0.1118253841996193,
      "learning_rate": 1.993e-05,
      "loss": 0.0028,
      "step": 90210
    },
    {
      "epoch": 4.811733333333334,
      "grad_norm": 0.18044273555278778,
      "learning_rate": 1.992666666666667e-05,
      "loss": 0.0022,
      "step": 90220
    },
    {
      "epoch": 4.812266666666667,
      "grad_norm": 0.048519209027290344,
      "learning_rate": 1.9923333333333336e-05,
      "loss": 0.0024,
      "step": 90230
    },
    {
      "epoch": 4.8128,
      "grad_norm": 0.04403690621256828,
      "learning_rate": 1.992e-05,
      "loss": 0.0029,
      "step": 90240
    },
    {
      "epoch": 4.8133333333333335,
      "grad_norm": 0.08879952132701874,
      "learning_rate": 1.9916666666666665e-05,
      "loss": 0.0029,
      "step": 90250
    },
    {
      "epoch": 4.813866666666667,
      "grad_norm": 0.49223676323890686,
      "learning_rate": 1.9913333333333335e-05,
      "loss": 0.002,
      "step": 90260
    },
    {
      "epoch": 4.8144,
      "grad_norm": 0.1456708014011383,
      "learning_rate": 1.991e-05,
      "loss": 0.0021,
      "step": 90270
    },
    {
      "epoch": 4.814933333333333,
      "grad_norm": 0.35024961829185486,
      "learning_rate": 1.9906666666666667e-05,
      "loss": 0.0017,
      "step": 90280
    },
    {
      "epoch": 4.815466666666667,
      "grad_norm": 0.09024899452924728,
      "learning_rate": 1.9903333333333336e-05,
      "loss": 0.0029,
      "step": 90290
    },
    {
      "epoch": 4.816,
      "grad_norm": 0.043729305267333984,
      "learning_rate": 1.9900000000000003e-05,
      "loss": 0.0012,
      "step": 90300
    },
    {
      "epoch": 4.816533333333333,
      "grad_norm": 0.08916302025318146,
      "learning_rate": 1.9896666666666665e-05,
      "loss": 0.0015,
      "step": 90310
    },
    {
      "epoch": 4.817066666666666,
      "grad_norm": 0.046369947493076324,
      "learning_rate": 1.9893333333333335e-05,
      "loss": 0.0021,
      "step": 90320
    },
    {
      "epoch": 4.8176,
      "grad_norm": 0.17930153012275696,
      "learning_rate": 1.989e-05,
      "loss": 0.0011,
      "step": 90330
    },
    {
      "epoch": 4.818133333333334,
      "grad_norm": 0.04219832271337509,
      "learning_rate": 1.9886666666666667e-05,
      "loss": 0.002,
      "step": 90340
    },
    {
      "epoch": 4.818666666666667,
      "grad_norm": 0.4452666938304901,
      "learning_rate": 1.9883333333333333e-05,
      "loss": 0.0015,
      "step": 90350
    },
    {
      "epoch": 4.8192,
      "grad_norm": 0.25892874598503113,
      "learning_rate": 1.9880000000000003e-05,
      "loss": 0.0022,
      "step": 90360
    },
    {
      "epoch": 4.819733333333334,
      "grad_norm": 0.07809510827064514,
      "learning_rate": 1.987666666666667e-05,
      "loss": 0.0016,
      "step": 90370
    },
    {
      "epoch": 4.820266666666667,
      "grad_norm": 0.06067997217178345,
      "learning_rate": 1.9873333333333335e-05,
      "loss": 0.0017,
      "step": 90380
    },
    {
      "epoch": 4.8208,
      "grad_norm": 0.23695722222328186,
      "learning_rate": 1.987e-05,
      "loss": 0.0019,
      "step": 90390
    },
    {
      "epoch": 4.8213333333333335,
      "grad_norm": 0.060976292937994,
      "learning_rate": 1.9866666666666667e-05,
      "loss": 0.0015,
      "step": 90400
    },
    {
      "epoch": 4.821866666666667,
      "grad_norm": 0.24119383096694946,
      "learning_rate": 1.9863333333333333e-05,
      "loss": 0.0016,
      "step": 90410
    },
    {
      "epoch": 4.8224,
      "grad_norm": 0.3848956823348999,
      "learning_rate": 1.986e-05,
      "loss": 0.0016,
      "step": 90420
    },
    {
      "epoch": 4.822933333333333,
      "grad_norm": 0.5490919947624207,
      "learning_rate": 1.985666666666667e-05,
      "loss": 0.0021,
      "step": 90430
    },
    {
      "epoch": 4.823466666666667,
      "grad_norm": 0.217213436961174,
      "learning_rate": 1.9853333333333335e-05,
      "loss": 0.0029,
      "step": 90440
    },
    {
      "epoch": 4.824,
      "grad_norm": 0.22589623928070068,
      "learning_rate": 1.985e-05,
      "loss": 0.0017,
      "step": 90450
    },
    {
      "epoch": 4.824533333333333,
      "grad_norm": 0.17563365399837494,
      "learning_rate": 1.9846666666666668e-05,
      "loss": 0.0024,
      "step": 90460
    },
    {
      "epoch": 4.825066666666666,
      "grad_norm": 0.299407422542572,
      "learning_rate": 1.9843333333333334e-05,
      "loss": 0.0021,
      "step": 90470
    },
    {
      "epoch": 4.8256,
      "grad_norm": 0.022317400202155113,
      "learning_rate": 1.984e-05,
      "loss": 0.0026,
      "step": 90480
    },
    {
      "epoch": 4.826133333333333,
      "grad_norm": 0.10461735725402832,
      "learning_rate": 1.9836666666666666e-05,
      "loss": 0.0021,
      "step": 90490
    },
    {
      "epoch": 4.826666666666666,
      "grad_norm": 0.2030341923236847,
      "learning_rate": 1.9833333333333335e-05,
      "loss": 0.0019,
      "step": 90500
    },
    {
      "epoch": 4.8272,
      "grad_norm": 0.09979820996522903,
      "learning_rate": 1.983e-05,
      "loss": 0.0015,
      "step": 90510
    },
    {
      "epoch": 4.827733333333334,
      "grad_norm": 0.0685688778758049,
      "learning_rate": 1.9826666666666668e-05,
      "loss": 0.0016,
      "step": 90520
    },
    {
      "epoch": 4.828266666666667,
      "grad_norm": 0.11753584444522858,
      "learning_rate": 1.9823333333333334e-05,
      "loss": 0.0026,
      "step": 90530
    },
    {
      "epoch": 4.8288,
      "grad_norm": 0.1039012148976326,
      "learning_rate": 1.982e-05,
      "loss": 0.0024,
      "step": 90540
    },
    {
      "epoch": 4.8293333333333335,
      "grad_norm": 0.17945851385593414,
      "learning_rate": 1.9816666666666666e-05,
      "loss": 0.0014,
      "step": 90550
    },
    {
      "epoch": 4.829866666666667,
      "grad_norm": 0.035587821155786514,
      "learning_rate": 1.9813333333333332e-05,
      "loss": 0.0028,
      "step": 90560
    },
    {
      "epoch": 4.8304,
      "grad_norm": 0.11941298097372055,
      "learning_rate": 1.9810000000000002e-05,
      "loss": 0.0018,
      "step": 90570
    },
    {
      "epoch": 4.830933333333333,
      "grad_norm": 0.05063172057271004,
      "learning_rate": 1.9806666666666668e-05,
      "loss": 0.0015,
      "step": 90580
    },
    {
      "epoch": 4.831466666666667,
      "grad_norm": 0.4364877939224243,
      "learning_rate": 1.9803333333333334e-05,
      "loss": 0.0017,
      "step": 90590
    },
    {
      "epoch": 4.832,
      "grad_norm": 0.14940190315246582,
      "learning_rate": 1.9800000000000004e-05,
      "loss": 0.0014,
      "step": 90600
    },
    {
      "epoch": 4.832533333333333,
      "grad_norm": 0.02928558923304081,
      "learning_rate": 1.979666666666667e-05,
      "loss": 0.0017,
      "step": 90610
    },
    {
      "epoch": 4.833066666666666,
      "grad_norm": 0.09214072674512863,
      "learning_rate": 1.9793333333333332e-05,
      "loss": 0.0024,
      "step": 90620
    },
    {
      "epoch": 4.8336,
      "grad_norm": 0.26381489634513855,
      "learning_rate": 1.979e-05,
      "loss": 0.0016,
      "step": 90630
    },
    {
      "epoch": 4.834133333333333,
      "grad_norm": 0.07218791544437408,
      "learning_rate": 1.9786666666666668e-05,
      "loss": 0.0022,
      "step": 90640
    },
    {
      "epoch": 4.834666666666667,
      "grad_norm": 0.1456824541091919,
      "learning_rate": 1.9783333333333334e-05,
      "loss": 0.0023,
      "step": 90650
    },
    {
      "epoch": 4.8352,
      "grad_norm": 0.187355175614357,
      "learning_rate": 1.978e-05,
      "loss": 0.003,
      "step": 90660
    },
    {
      "epoch": 4.835733333333334,
      "grad_norm": 0.21676108241081238,
      "learning_rate": 1.977666666666667e-05,
      "loss": 0.0018,
      "step": 90670
    },
    {
      "epoch": 4.836266666666667,
      "grad_norm": 0.14616358280181885,
      "learning_rate": 1.9773333333333336e-05,
      "loss": 0.0021,
      "step": 90680
    },
    {
      "epoch": 4.8368,
      "grad_norm": 0.11598823219537735,
      "learning_rate": 1.977e-05,
      "loss": 0.0017,
      "step": 90690
    },
    {
      "epoch": 4.8373333333333335,
      "grad_norm": 0.17343075573444366,
      "learning_rate": 1.9766666666666668e-05,
      "loss": 0.0018,
      "step": 90700
    },
    {
      "epoch": 4.837866666666667,
      "grad_norm": 0.11964643001556396,
      "learning_rate": 1.9763333333333334e-05,
      "loss": 0.0021,
      "step": 90710
    },
    {
      "epoch": 4.8384,
      "grad_norm": 0.2155057191848755,
      "learning_rate": 1.976e-05,
      "loss": 0.0018,
      "step": 90720
    },
    {
      "epoch": 4.838933333333333,
      "grad_norm": 0.20280064642429352,
      "learning_rate": 1.9756666666666667e-05,
      "loss": 0.0015,
      "step": 90730
    },
    {
      "epoch": 4.839466666666667,
      "grad_norm": 0.29077187180519104,
      "learning_rate": 1.9753333333333336e-05,
      "loss": 0.0021,
      "step": 90740
    },
    {
      "epoch": 4.84,
      "grad_norm": 0.07073856145143509,
      "learning_rate": 1.9750000000000002e-05,
      "loss": 0.002,
      "step": 90750
    },
    {
      "epoch": 4.840533333333333,
      "grad_norm": 0.09181743860244751,
      "learning_rate": 1.974666666666667e-05,
      "loss": 0.0018,
      "step": 90760
    },
    {
      "epoch": 4.841066666666666,
      "grad_norm": 0.09375636279582977,
      "learning_rate": 1.9743333333333335e-05,
      "loss": 0.0022,
      "step": 90770
    },
    {
      "epoch": 4.8416,
      "grad_norm": 0.18829038739204407,
      "learning_rate": 1.974e-05,
      "loss": 0.0016,
      "step": 90780
    },
    {
      "epoch": 4.842133333333333,
      "grad_norm": 0.03437359258532524,
      "learning_rate": 1.9736666666666667e-05,
      "loss": 0.0019,
      "step": 90790
    },
    {
      "epoch": 4.842666666666666,
      "grad_norm": 0.10275980830192566,
      "learning_rate": 1.9733333333333333e-05,
      "loss": 0.0022,
      "step": 90800
    },
    {
      "epoch": 4.8431999999999995,
      "grad_norm": 0.08864807337522507,
      "learning_rate": 1.9730000000000003e-05,
      "loss": 0.0024,
      "step": 90810
    },
    {
      "epoch": 4.843733333333334,
      "grad_norm": 0.04098803177475929,
      "learning_rate": 1.972666666666667e-05,
      "loss": 0.0023,
      "step": 90820
    },
    {
      "epoch": 4.844266666666667,
      "grad_norm": 0.4316360652446747,
      "learning_rate": 1.9723333333333335e-05,
      "loss": 0.0017,
      "step": 90830
    },
    {
      "epoch": 4.8448,
      "grad_norm": 0.33589085936546326,
      "learning_rate": 1.972e-05,
      "loss": 0.0015,
      "step": 90840
    },
    {
      "epoch": 4.8453333333333335,
      "grad_norm": 0.2164788693189621,
      "learning_rate": 1.9716666666666667e-05,
      "loss": 0.0014,
      "step": 90850
    },
    {
      "epoch": 4.845866666666667,
      "grad_norm": 0.1294027864933014,
      "learning_rate": 1.9713333333333333e-05,
      "loss": 0.0016,
      "step": 90860
    },
    {
      "epoch": 4.8464,
      "grad_norm": 0.7130624055862427,
      "learning_rate": 1.971e-05,
      "loss": 0.0019,
      "step": 90870
    },
    {
      "epoch": 4.846933333333333,
      "grad_norm": 0.23293282091617584,
      "learning_rate": 1.970666666666667e-05,
      "loss": 0.0018,
      "step": 90880
    },
    {
      "epoch": 4.847466666666667,
      "grad_norm": 0.287557989358902,
      "learning_rate": 1.9703333333333335e-05,
      "loss": 0.0015,
      "step": 90890
    },
    {
      "epoch": 4.848,
      "grad_norm": 0.26498308777809143,
      "learning_rate": 1.97e-05,
      "loss": 0.0024,
      "step": 90900
    },
    {
      "epoch": 4.848533333333333,
      "grad_norm": 0.1886993944644928,
      "learning_rate": 1.9696666666666667e-05,
      "loss": 0.0017,
      "step": 90910
    },
    {
      "epoch": 4.849066666666666,
      "grad_norm": 0.4370689392089844,
      "learning_rate": 1.9693333333333333e-05,
      "loss": 0.0022,
      "step": 90920
    },
    {
      "epoch": 4.8496,
      "grad_norm": 0.2009637951850891,
      "learning_rate": 1.969e-05,
      "loss": 0.0016,
      "step": 90930
    },
    {
      "epoch": 4.850133333333333,
      "grad_norm": 0.35288771986961365,
      "learning_rate": 1.9686666666666666e-05,
      "loss": 0.0025,
      "step": 90940
    },
    {
      "epoch": 4.850666666666667,
      "grad_norm": 0.2641436457633972,
      "learning_rate": 1.9683333333333335e-05,
      "loss": 0.0018,
      "step": 90950
    },
    {
      "epoch": 4.8512,
      "grad_norm": 0.2616764008998871,
      "learning_rate": 1.968e-05,
      "loss": 0.0019,
      "step": 90960
    },
    {
      "epoch": 4.851733333333334,
      "grad_norm": 0.42937490344047546,
      "learning_rate": 1.9676666666666667e-05,
      "loss": 0.0023,
      "step": 90970
    },
    {
      "epoch": 4.852266666666667,
      "grad_norm": 0.20101141929626465,
      "learning_rate": 1.9673333333333337e-05,
      "loss": 0.0022,
      "step": 90980
    },
    {
      "epoch": 4.8528,
      "grad_norm": 0.4636378586292267,
      "learning_rate": 1.9670000000000003e-05,
      "loss": 0.0017,
      "step": 90990
    },
    {
      "epoch": 4.8533333333333335,
      "grad_norm": 0.2651551365852356,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 0.0015,
      "step": 91000
    },
    {
      "epoch": 4.853866666666667,
      "grad_norm": 0.36867567896842957,
      "learning_rate": 1.9663333333333332e-05,
      "loss": 0.0028,
      "step": 91010
    },
    {
      "epoch": 4.8544,
      "grad_norm": 0.26620134711265564,
      "learning_rate": 1.966e-05,
      "loss": 0.0026,
      "step": 91020
    },
    {
      "epoch": 4.854933333333333,
      "grad_norm": 0.09954196959733963,
      "learning_rate": 1.9656666666666668e-05,
      "loss": 0.0016,
      "step": 91030
    },
    {
      "epoch": 4.855466666666667,
      "grad_norm": 0.29184675216674805,
      "learning_rate": 1.9653333333333334e-05,
      "loss": 0.0018,
      "step": 91040
    },
    {
      "epoch": 4.856,
      "grad_norm": 0.3386128842830658,
      "learning_rate": 1.9650000000000003e-05,
      "loss": 0.0018,
      "step": 91050
    },
    {
      "epoch": 4.856533333333333,
      "grad_norm": 0.4366612136363983,
      "learning_rate": 1.964666666666667e-05,
      "loss": 0.002,
      "step": 91060
    },
    {
      "epoch": 4.857066666666666,
      "grad_norm": 0.20778267085552216,
      "learning_rate": 1.9643333333333332e-05,
      "loss": 0.0023,
      "step": 91070
    },
    {
      "epoch": 4.8576,
      "grad_norm": 0.2054540067911148,
      "learning_rate": 1.9640000000000002e-05,
      "loss": 0.002,
      "step": 91080
    },
    {
      "epoch": 4.858133333333333,
      "grad_norm": 0.14287400245666504,
      "learning_rate": 1.9636666666666668e-05,
      "loss": 0.0024,
      "step": 91090
    },
    {
      "epoch": 4.858666666666666,
      "grad_norm": 0.29157960414886475,
      "learning_rate": 1.9633333333333334e-05,
      "loss": 0.0016,
      "step": 91100
    },
    {
      "epoch": 4.8591999999999995,
      "grad_norm": 0.0956408828496933,
      "learning_rate": 1.963e-05,
      "loss": 0.0015,
      "step": 91110
    },
    {
      "epoch": 4.859733333333334,
      "grad_norm": 0.08020532876253128,
      "learning_rate": 1.962666666666667e-05,
      "loss": 0.0015,
      "step": 91120
    },
    {
      "epoch": 4.860266666666667,
      "grad_norm": 0.06739158183336258,
      "learning_rate": 1.9623333333333336e-05,
      "loss": 0.0025,
      "step": 91130
    },
    {
      "epoch": 4.8608,
      "grad_norm": 0.2613028287887573,
      "learning_rate": 1.9620000000000002e-05,
      "loss": 0.0015,
      "step": 91140
    },
    {
      "epoch": 4.8613333333333335,
      "grad_norm": 0.3473288118839264,
      "learning_rate": 1.9616666666666668e-05,
      "loss": 0.0016,
      "step": 91150
    },
    {
      "epoch": 4.861866666666667,
      "grad_norm": 0.12105824798345566,
      "learning_rate": 1.9613333333333334e-05,
      "loss": 0.0015,
      "step": 91160
    },
    {
      "epoch": 4.8624,
      "grad_norm": 0.4709400534629822,
      "learning_rate": 1.961e-05,
      "loss": 0.0017,
      "step": 91170
    },
    {
      "epoch": 4.862933333333333,
      "grad_norm": 0.036202263087034225,
      "learning_rate": 1.9606666666666666e-05,
      "loss": 0.0014,
      "step": 91180
    },
    {
      "epoch": 4.863466666666667,
      "grad_norm": 0.150949165225029,
      "learning_rate": 1.9603333333333336e-05,
      "loss": 0.002,
      "step": 91190
    },
    {
      "epoch": 4.864,
      "grad_norm": 0.350023090839386,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 0.002,
      "step": 91200
    },
    {
      "epoch": 4.864533333333333,
      "grad_norm": 0.1485186368227005,
      "learning_rate": 1.9596666666666668e-05,
      "loss": 0.0018,
      "step": 91210
    },
    {
      "epoch": 4.865066666666666,
      "grad_norm": 0.12774842977523804,
      "learning_rate": 1.9593333333333334e-05,
      "loss": 0.0017,
      "step": 91220
    },
    {
      "epoch": 4.8656,
      "grad_norm": 0.46102941036224365,
      "learning_rate": 1.959e-05,
      "loss": 0.0011,
      "step": 91230
    },
    {
      "epoch": 4.866133333333333,
      "grad_norm": 0.3444182574748993,
      "learning_rate": 1.9586666666666667e-05,
      "loss": 0.0013,
      "step": 91240
    },
    {
      "epoch": 4.866666666666667,
      "grad_norm": 0.29042860865592957,
      "learning_rate": 1.9583333333333333e-05,
      "loss": 0.0019,
      "step": 91250
    },
    {
      "epoch": 4.8672,
      "grad_norm": 0.11558878421783447,
      "learning_rate": 1.9580000000000002e-05,
      "loss": 0.0021,
      "step": 91260
    },
    {
      "epoch": 4.867733333333334,
      "grad_norm": 0.05312136933207512,
      "learning_rate": 1.957666666666667e-05,
      "loss": 0.0017,
      "step": 91270
    },
    {
      "epoch": 4.868266666666667,
      "grad_norm": 0.09677398949861526,
      "learning_rate": 1.9573333333333335e-05,
      "loss": 0.0022,
      "step": 91280
    },
    {
      "epoch": 4.8688,
      "grad_norm": 0.32720521092414856,
      "learning_rate": 1.957e-05,
      "loss": 0.0015,
      "step": 91290
    },
    {
      "epoch": 4.8693333333333335,
      "grad_norm": 0.09731268882751465,
      "learning_rate": 1.9566666666666667e-05,
      "loss": 0.002,
      "step": 91300
    },
    {
      "epoch": 4.869866666666667,
      "grad_norm": 0.15420424938201904,
      "learning_rate": 1.9563333333333333e-05,
      "loss": 0.0022,
      "step": 91310
    },
    {
      "epoch": 4.8704,
      "grad_norm": 0.17774920165538788,
      "learning_rate": 1.956e-05,
      "loss": 0.0016,
      "step": 91320
    },
    {
      "epoch": 4.870933333333333,
      "grad_norm": 0.20701996982097626,
      "learning_rate": 1.955666666666667e-05,
      "loss": 0.0022,
      "step": 91330
    },
    {
      "epoch": 4.871466666666667,
      "grad_norm": 0.1517261117696762,
      "learning_rate": 1.9553333333333335e-05,
      "loss": 0.0017,
      "step": 91340
    },
    {
      "epoch": 4.872,
      "grad_norm": 0.05831186845898628,
      "learning_rate": 1.955e-05,
      "loss": 0.0022,
      "step": 91350
    },
    {
      "epoch": 4.872533333333333,
      "grad_norm": 0.1781603842973709,
      "learning_rate": 1.9546666666666667e-05,
      "loss": 0.0014,
      "step": 91360
    },
    {
      "epoch": 4.873066666666666,
      "grad_norm": 0.565575897693634,
      "learning_rate": 1.9543333333333333e-05,
      "loss": 0.0016,
      "step": 91370
    },
    {
      "epoch": 4.8736,
      "grad_norm": 0.10280733555555344,
      "learning_rate": 1.954e-05,
      "loss": 0.0026,
      "step": 91380
    },
    {
      "epoch": 4.874133333333333,
      "grad_norm": 0.4382534921169281,
      "learning_rate": 1.9536666666666665e-05,
      "loss": 0.0017,
      "step": 91390
    },
    {
      "epoch": 4.874666666666666,
      "grad_norm": 0.4757610559463501,
      "learning_rate": 1.9533333333333335e-05,
      "loss": 0.0025,
      "step": 91400
    },
    {
      "epoch": 4.8751999999999995,
      "grad_norm": 0.12345530837774277,
      "learning_rate": 1.953e-05,
      "loss": 0.0013,
      "step": 91410
    },
    {
      "epoch": 4.875733333333334,
      "grad_norm": 0.4909200072288513,
      "learning_rate": 1.9526666666666667e-05,
      "loss": 0.0021,
      "step": 91420
    },
    {
      "epoch": 4.876266666666667,
      "grad_norm": 0.11516425758600235,
      "learning_rate": 1.9523333333333337e-05,
      "loss": 0.002,
      "step": 91430
    },
    {
      "epoch": 4.8768,
      "grad_norm": 0.2316550612449646,
      "learning_rate": 1.9520000000000003e-05,
      "loss": 0.003,
      "step": 91440
    },
    {
      "epoch": 4.8773333333333335,
      "grad_norm": 0.6993330121040344,
      "learning_rate": 1.9516666666666666e-05,
      "loss": 0.0018,
      "step": 91450
    },
    {
      "epoch": 4.877866666666667,
      "grad_norm": 0.06695927679538727,
      "learning_rate": 1.9513333333333335e-05,
      "loss": 0.002,
      "step": 91460
    },
    {
      "epoch": 4.8784,
      "grad_norm": 0.32117974758148193,
      "learning_rate": 1.951e-05,
      "loss": 0.0022,
      "step": 91470
    },
    {
      "epoch": 4.878933333333333,
      "grad_norm": 0.23650683462619781,
      "learning_rate": 1.9506666666666667e-05,
      "loss": 0.0027,
      "step": 91480
    },
    {
      "epoch": 4.879466666666667,
      "grad_norm": 0.1865665763616562,
      "learning_rate": 1.9503333333333334e-05,
      "loss": 0.0027,
      "step": 91490
    },
    {
      "epoch": 4.88,
      "grad_norm": 0.26618409156799316,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 0.0021,
      "step": 91500
    },
    {
      "epoch": 4.880533333333333,
      "grad_norm": 0.2671384811401367,
      "learning_rate": 1.949666666666667e-05,
      "loss": 0.0015,
      "step": 91510
    },
    {
      "epoch": 4.881066666666666,
      "grad_norm": 0.11989302933216095,
      "learning_rate": 1.9493333333333332e-05,
      "loss": 0.0021,
      "step": 91520
    },
    {
      "epoch": 4.8816,
      "grad_norm": 0.04256115108728409,
      "learning_rate": 1.949e-05,
      "loss": 0.0025,
      "step": 91530
    },
    {
      "epoch": 4.882133333333333,
      "grad_norm": 0.12426964938640594,
      "learning_rate": 1.9486666666666668e-05,
      "loss": 0.0019,
      "step": 91540
    },
    {
      "epoch": 4.882666666666667,
      "grad_norm": 0.29817819595336914,
      "learning_rate": 1.9483333333333334e-05,
      "loss": 0.0012,
      "step": 91550
    },
    {
      "epoch": 4.8832,
      "grad_norm": 0.3264329731464386,
      "learning_rate": 1.948e-05,
      "loss": 0.0014,
      "step": 91560
    },
    {
      "epoch": 4.883733333333334,
      "grad_norm": 0.12153791636228561,
      "learning_rate": 1.947666666666667e-05,
      "loss": 0.0024,
      "step": 91570
    },
    {
      "epoch": 4.884266666666667,
      "grad_norm": 0.24072201550006866,
      "learning_rate": 1.9473333333333335e-05,
      "loss": 0.0019,
      "step": 91580
    },
    {
      "epoch": 4.8848,
      "grad_norm": 0.09746105968952179,
      "learning_rate": 1.947e-05,
      "loss": 0.0015,
      "step": 91590
    },
    {
      "epoch": 4.8853333333333335,
      "grad_norm": 0.2930152714252472,
      "learning_rate": 1.9466666666666668e-05,
      "loss": 0.0022,
      "step": 91600
    },
    {
      "epoch": 4.885866666666667,
      "grad_norm": 0.23208805918693542,
      "learning_rate": 1.9463333333333334e-05,
      "loss": 0.0026,
      "step": 91610
    },
    {
      "epoch": 4.8864,
      "grad_norm": 0.17901986837387085,
      "learning_rate": 1.946e-05,
      "loss": 0.0027,
      "step": 91620
    },
    {
      "epoch": 4.886933333333333,
      "grad_norm": 0.12176983058452606,
      "learning_rate": 1.9456666666666666e-05,
      "loss": 0.0013,
      "step": 91630
    },
    {
      "epoch": 4.887466666666667,
      "grad_norm": 0.4995153546333313,
      "learning_rate": 1.9453333333333336e-05,
      "loss": 0.0027,
      "step": 91640
    },
    {
      "epoch": 4.888,
      "grad_norm": 0.32223209738731384,
      "learning_rate": 1.9450000000000002e-05,
      "loss": 0.0014,
      "step": 91650
    },
    {
      "epoch": 4.888533333333333,
      "grad_norm": 0.29865574836730957,
      "learning_rate": 1.9446666666666668e-05,
      "loss": 0.0015,
      "step": 91660
    },
    {
      "epoch": 4.8890666666666664,
      "grad_norm": 0.0689961239695549,
      "learning_rate": 1.9443333333333334e-05,
      "loss": 0.0019,
      "step": 91670
    },
    {
      "epoch": 4.8896,
      "grad_norm": 0.065536268055439,
      "learning_rate": 1.944e-05,
      "loss": 0.0018,
      "step": 91680
    },
    {
      "epoch": 4.890133333333333,
      "grad_norm": 0.03881034627556801,
      "learning_rate": 1.9436666666666666e-05,
      "loss": 0.0022,
      "step": 91690
    },
    {
      "epoch": 4.890666666666666,
      "grad_norm": 0.04091612622141838,
      "learning_rate": 1.9433333333333332e-05,
      "loss": 0.0015,
      "step": 91700
    },
    {
      "epoch": 4.8911999999999995,
      "grad_norm": 0.12323267757892609,
      "learning_rate": 1.9430000000000002e-05,
      "loss": 0.0033,
      "step": 91710
    },
    {
      "epoch": 4.891733333333334,
      "grad_norm": 0.05091162025928497,
      "learning_rate": 1.9426666666666668e-05,
      "loss": 0.0015,
      "step": 91720
    },
    {
      "epoch": 4.892266666666667,
      "grad_norm": 0.12799279391765594,
      "learning_rate": 1.9423333333333334e-05,
      "loss": 0.002,
      "step": 91730
    },
    {
      "epoch": 4.8928,
      "grad_norm": 0.44606050848960876,
      "learning_rate": 1.942e-05,
      "loss": 0.0025,
      "step": 91740
    },
    {
      "epoch": 4.8933333333333335,
      "grad_norm": 0.26535481214523315,
      "learning_rate": 1.9416666666666667e-05,
      "loss": 0.0022,
      "step": 91750
    },
    {
      "epoch": 4.893866666666667,
      "grad_norm": 0.03774811327457428,
      "learning_rate": 1.9413333333333333e-05,
      "loss": 0.0014,
      "step": 91760
    },
    {
      "epoch": 4.8944,
      "grad_norm": 0.26187756657600403,
      "learning_rate": 1.941e-05,
      "loss": 0.0022,
      "step": 91770
    },
    {
      "epoch": 4.894933333333333,
      "grad_norm": 0.09336861968040466,
      "learning_rate": 1.940666666666667e-05,
      "loss": 0.0021,
      "step": 91780
    },
    {
      "epoch": 4.895466666666667,
      "grad_norm": 0.1442287415266037,
      "learning_rate": 1.9403333333333334e-05,
      "loss": 0.0019,
      "step": 91790
    },
    {
      "epoch": 4.896,
      "grad_norm": 0.29164355993270874,
      "learning_rate": 1.94e-05,
      "loss": 0.0028,
      "step": 91800
    },
    {
      "epoch": 4.896533333333333,
      "grad_norm": 0.1466914266347885,
      "learning_rate": 1.939666666666667e-05,
      "loss": 0.0012,
      "step": 91810
    },
    {
      "epoch": 4.8970666666666665,
      "grad_norm": 0.06765056401491165,
      "learning_rate": 1.9393333333333336e-05,
      "loss": 0.0022,
      "step": 91820
    },
    {
      "epoch": 4.8976,
      "grad_norm": 0.26649436354637146,
      "learning_rate": 1.939e-05,
      "loss": 0.0022,
      "step": 91830
    },
    {
      "epoch": 4.898133333333333,
      "grad_norm": 0.3553371727466583,
      "learning_rate": 1.938666666666667e-05,
      "loss": 0.0016,
      "step": 91840
    },
    {
      "epoch": 4.898666666666666,
      "grad_norm": 0.09106682240962982,
      "learning_rate": 1.9383333333333335e-05,
      "loss": 0.0025,
      "step": 91850
    },
    {
      "epoch": 4.8992,
      "grad_norm": 0.14679235219955444,
      "learning_rate": 1.938e-05,
      "loss": 0.0022,
      "step": 91860
    },
    {
      "epoch": 4.899733333333334,
      "grad_norm": 0.20821429789066315,
      "learning_rate": 1.9376666666666667e-05,
      "loss": 0.0018,
      "step": 91870
    },
    {
      "epoch": 4.900266666666667,
      "grad_norm": 0.08871489763259888,
      "learning_rate": 1.9373333333333336e-05,
      "loss": 0.0023,
      "step": 91880
    },
    {
      "epoch": 4.9008,
      "grad_norm": 0.07496022433042526,
      "learning_rate": 1.9370000000000003e-05,
      "loss": 0.0014,
      "step": 91890
    },
    {
      "epoch": 4.9013333333333335,
      "grad_norm": 0.11864403635263443,
      "learning_rate": 1.9366666666666665e-05,
      "loss": 0.0022,
      "step": 91900
    },
    {
      "epoch": 4.901866666666667,
      "grad_norm": 0.35365530848503113,
      "learning_rate": 1.9363333333333335e-05,
      "loss": 0.0029,
      "step": 91910
    },
    {
      "epoch": 4.9024,
      "grad_norm": 0.06479525566101074,
      "learning_rate": 1.936e-05,
      "loss": 0.0015,
      "step": 91920
    },
    {
      "epoch": 4.902933333333333,
      "grad_norm": 0.09837693721055984,
      "learning_rate": 1.9356666666666667e-05,
      "loss": 0.0019,
      "step": 91930
    },
    {
      "epoch": 4.903466666666667,
      "grad_norm": 0.2583940923213959,
      "learning_rate": 1.9353333333333333e-05,
      "loss": 0.002,
      "step": 91940
    },
    {
      "epoch": 4.904,
      "grad_norm": 0.04819932579994202,
      "learning_rate": 1.9350000000000003e-05,
      "loss": 0.0022,
      "step": 91950
    },
    {
      "epoch": 4.904533333333333,
      "grad_norm": 0.528115451335907,
      "learning_rate": 1.934666666666667e-05,
      "loss": 0.0027,
      "step": 91960
    },
    {
      "epoch": 4.9050666666666665,
      "grad_norm": 0.041334476321935654,
      "learning_rate": 1.9343333333333335e-05,
      "loss": 0.0026,
      "step": 91970
    },
    {
      "epoch": 4.9056,
      "grad_norm": 0.3460223972797394,
      "learning_rate": 1.934e-05,
      "loss": 0.0014,
      "step": 91980
    },
    {
      "epoch": 4.906133333333333,
      "grad_norm": 0.14329297840595245,
      "learning_rate": 1.9336666666666667e-05,
      "loss": 0.0019,
      "step": 91990
    },
    {
      "epoch": 4.906666666666666,
      "grad_norm": 0.45482897758483887,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 0.0018,
      "step": 92000
    },
    {
      "epoch": 4.9072,
      "grad_norm": 0.5320364236831665,
      "learning_rate": 1.933e-05,
      "loss": 0.0014,
      "step": 92010
    },
    {
      "epoch": 4.907733333333333,
      "grad_norm": 0.20428384840488434,
      "learning_rate": 1.932666666666667e-05,
      "loss": 0.0017,
      "step": 92020
    },
    {
      "epoch": 4.908266666666667,
      "grad_norm": 0.1914835423231125,
      "learning_rate": 1.9323333333333335e-05,
      "loss": 0.0015,
      "step": 92030
    },
    {
      "epoch": 4.9088,
      "grad_norm": 0.41189247369766235,
      "learning_rate": 1.932e-05,
      "loss": 0.0019,
      "step": 92040
    },
    {
      "epoch": 4.9093333333333335,
      "grad_norm": 0.3058784604072571,
      "learning_rate": 1.9316666666666668e-05,
      "loss": 0.0022,
      "step": 92050
    },
    {
      "epoch": 4.909866666666667,
      "grad_norm": 0.263235479593277,
      "learning_rate": 1.9313333333333334e-05,
      "loss": 0.0018,
      "step": 92060
    },
    {
      "epoch": 4.9104,
      "grad_norm": 0.24561060965061188,
      "learning_rate": 1.931e-05,
      "loss": 0.0019,
      "step": 92070
    },
    {
      "epoch": 4.910933333333333,
      "grad_norm": 0.06138762831687927,
      "learning_rate": 1.9306666666666666e-05,
      "loss": 0.0017,
      "step": 92080
    },
    {
      "epoch": 4.911466666666667,
      "grad_norm": 0.27323201298713684,
      "learning_rate": 1.9303333333333335e-05,
      "loss": 0.0022,
      "step": 92090
    },
    {
      "epoch": 4.912,
      "grad_norm": 0.14824894070625305,
      "learning_rate": 1.93e-05,
      "loss": 0.0028,
      "step": 92100
    },
    {
      "epoch": 4.912533333333333,
      "grad_norm": 0.10578525066375732,
      "learning_rate": 1.9296666666666668e-05,
      "loss": 0.0017,
      "step": 92110
    },
    {
      "epoch": 4.9130666666666665,
      "grad_norm": 0.3622768819332123,
      "learning_rate": 1.9293333333333334e-05,
      "loss": 0.0016,
      "step": 92120
    },
    {
      "epoch": 4.9136,
      "grad_norm": 0.37438926100730896,
      "learning_rate": 1.929e-05,
      "loss": 0.0017,
      "step": 92130
    },
    {
      "epoch": 4.914133333333333,
      "grad_norm": 0.15836398303508759,
      "learning_rate": 1.9286666666666666e-05,
      "loss": 0.0017,
      "step": 92140
    },
    {
      "epoch": 4.914666666666666,
      "grad_norm": 0.2536063492298126,
      "learning_rate": 1.9283333333333332e-05,
      "loss": 0.0026,
      "step": 92150
    },
    {
      "epoch": 4.9152000000000005,
      "grad_norm": 0.08566393703222275,
      "learning_rate": 1.9280000000000002e-05,
      "loss": 0.0022,
      "step": 92160
    },
    {
      "epoch": 4.915733333333334,
      "grad_norm": 0.06310036033391953,
      "learning_rate": 1.9276666666666668e-05,
      "loss": 0.0033,
      "step": 92170
    },
    {
      "epoch": 4.916266666666667,
      "grad_norm": 0.0323987640440464,
      "learning_rate": 1.9273333333333334e-05,
      "loss": 0.0022,
      "step": 92180
    },
    {
      "epoch": 4.9168,
      "grad_norm": 0.2843487560749054,
      "learning_rate": 1.9270000000000004e-05,
      "loss": 0.002,
      "step": 92190
    },
    {
      "epoch": 4.917333333333334,
      "grad_norm": 0.07459397614002228,
      "learning_rate": 1.926666666666667e-05,
      "loss": 0.0011,
      "step": 92200
    },
    {
      "epoch": 4.917866666666667,
      "grad_norm": 0.2893264889717102,
      "learning_rate": 1.9263333333333332e-05,
      "loss": 0.0026,
      "step": 92210
    },
    {
      "epoch": 4.9184,
      "grad_norm": 0.16450566053390503,
      "learning_rate": 1.9260000000000002e-05,
      "loss": 0.0017,
      "step": 92220
    },
    {
      "epoch": 4.918933333333333,
      "grad_norm": 0.13206924498081207,
      "learning_rate": 1.9256666666666668e-05,
      "loss": 0.0021,
      "step": 92230
    },
    {
      "epoch": 4.919466666666667,
      "grad_norm": 0.09202384203672409,
      "learning_rate": 1.9253333333333334e-05,
      "loss": 0.0029,
      "step": 92240
    },
    {
      "epoch": 4.92,
      "grad_norm": 0.15291796624660492,
      "learning_rate": 1.925e-05,
      "loss": 0.0022,
      "step": 92250
    },
    {
      "epoch": 4.920533333333333,
      "grad_norm": 0.06378977745771408,
      "learning_rate": 1.924666666666667e-05,
      "loss": 0.0025,
      "step": 92260
    },
    {
      "epoch": 4.9210666666666665,
      "grad_norm": 0.3743194341659546,
      "learning_rate": 1.9243333333333336e-05,
      "loss": 0.0018,
      "step": 92270
    },
    {
      "epoch": 4.9216,
      "grad_norm": 0.41254130005836487,
      "learning_rate": 1.924e-05,
      "loss": 0.0026,
      "step": 92280
    },
    {
      "epoch": 4.922133333333333,
      "grad_norm": 0.041142333298921585,
      "learning_rate": 1.9236666666666668e-05,
      "loss": 0.0015,
      "step": 92290
    },
    {
      "epoch": 4.922666666666666,
      "grad_norm": 0.1449165940284729,
      "learning_rate": 1.9233333333333334e-05,
      "loss": 0.004,
      "step": 92300
    },
    {
      "epoch": 4.9232,
      "grad_norm": 0.17061497271060944,
      "learning_rate": 1.923e-05,
      "loss": 0.002,
      "step": 92310
    },
    {
      "epoch": 4.923733333333333,
      "grad_norm": 0.08904142677783966,
      "learning_rate": 1.9226666666666667e-05,
      "loss": 0.0014,
      "step": 92320
    },
    {
      "epoch": 4.924266666666667,
      "grad_norm": 0.21310091018676758,
      "learning_rate": 1.9223333333333336e-05,
      "loss": 0.0018,
      "step": 92330
    },
    {
      "epoch": 4.9248,
      "grad_norm": 0.640893816947937,
      "learning_rate": 1.9220000000000002e-05,
      "loss": 0.0019,
      "step": 92340
    },
    {
      "epoch": 4.925333333333334,
      "grad_norm": 0.35482609272003174,
      "learning_rate": 1.921666666666667e-05,
      "loss": 0.0018,
      "step": 92350
    },
    {
      "epoch": 4.925866666666667,
      "grad_norm": 0.3951556980609894,
      "learning_rate": 1.9213333333333335e-05,
      "loss": 0.0023,
      "step": 92360
    },
    {
      "epoch": 4.9264,
      "grad_norm": 0.14781416952610016,
      "learning_rate": 1.921e-05,
      "loss": 0.002,
      "step": 92370
    },
    {
      "epoch": 4.926933333333333,
      "grad_norm": 0.3297606110572815,
      "learning_rate": 1.9206666666666667e-05,
      "loss": 0.0018,
      "step": 92380
    },
    {
      "epoch": 4.927466666666667,
      "grad_norm": 0.09262609481811523,
      "learning_rate": 1.9203333333333333e-05,
      "loss": 0.0017,
      "step": 92390
    },
    {
      "epoch": 4.928,
      "grad_norm": 0.34851908683776855,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.0031,
      "step": 92400
    },
    {
      "epoch": 4.928533333333333,
      "grad_norm": 0.20437633991241455,
      "learning_rate": 1.919666666666667e-05,
      "loss": 0.0026,
      "step": 92410
    },
    {
      "epoch": 4.9290666666666665,
      "grad_norm": 0.19510632753372192,
      "learning_rate": 1.9193333333333335e-05,
      "loss": 0.0019,
      "step": 92420
    },
    {
      "epoch": 4.9296,
      "grad_norm": 0.6033992767333984,
      "learning_rate": 1.919e-05,
      "loss": 0.0015,
      "step": 92430
    },
    {
      "epoch": 4.930133333333333,
      "grad_norm": 0.03576911240816116,
      "learning_rate": 1.9186666666666667e-05,
      "loss": 0.0017,
      "step": 92440
    },
    {
      "epoch": 4.930666666666666,
      "grad_norm": 0.23749902844429016,
      "learning_rate": 1.9183333333333333e-05,
      "loss": 0.0022,
      "step": 92450
    },
    {
      "epoch": 4.9312000000000005,
      "grad_norm": 0.04686491936445236,
      "learning_rate": 1.918e-05,
      "loss": 0.0014,
      "step": 92460
    },
    {
      "epoch": 4.931733333333334,
      "grad_norm": 0.0652768686413765,
      "learning_rate": 1.917666666666667e-05,
      "loss": 0.0021,
      "step": 92470
    },
    {
      "epoch": 4.932266666666667,
      "grad_norm": 0.3593849837779999,
      "learning_rate": 1.9173333333333335e-05,
      "loss": 0.0021,
      "step": 92480
    },
    {
      "epoch": 4.9328,
      "grad_norm": 0.1255764365196228,
      "learning_rate": 1.917e-05,
      "loss": 0.0018,
      "step": 92490
    },
    {
      "epoch": 4.933333333333334,
      "grad_norm": 0.2635096311569214,
      "learning_rate": 1.9166666666666667e-05,
      "loss": 0.0014,
      "step": 92500
    },
    {
      "epoch": 4.933866666666667,
      "grad_norm": 0.33459964394569397,
      "learning_rate": 1.9163333333333333e-05,
      "loss": 0.0025,
      "step": 92510
    },
    {
      "epoch": 4.9344,
      "grad_norm": 0.46424728631973267,
      "learning_rate": 1.916e-05,
      "loss": 0.0017,
      "step": 92520
    },
    {
      "epoch": 4.934933333333333,
      "grad_norm": 0.07561957091093063,
      "learning_rate": 1.9156666666666666e-05,
      "loss": 0.0021,
      "step": 92530
    },
    {
      "epoch": 4.935466666666667,
      "grad_norm": 0.5563423037528992,
      "learning_rate": 1.9153333333333335e-05,
      "loss": 0.0018,
      "step": 92540
    },
    {
      "epoch": 4.936,
      "grad_norm": 0.06619223952293396,
      "learning_rate": 1.915e-05,
      "loss": 0.0019,
      "step": 92550
    },
    {
      "epoch": 4.936533333333333,
      "grad_norm": 0.204963818192482,
      "learning_rate": 1.9146666666666667e-05,
      "loss": 0.0015,
      "step": 92560
    },
    {
      "epoch": 4.9370666666666665,
      "grad_norm": 0.17832224071025848,
      "learning_rate": 1.9143333333333337e-05,
      "loss": 0.0017,
      "step": 92570
    },
    {
      "epoch": 4.9376,
      "grad_norm": 0.23742565512657166,
      "learning_rate": 1.914e-05,
      "loss": 0.0018,
      "step": 92580
    },
    {
      "epoch": 4.938133333333333,
      "grad_norm": 0.5693432688713074,
      "learning_rate": 1.9136666666666666e-05,
      "loss": 0.0016,
      "step": 92590
    },
    {
      "epoch": 4.938666666666666,
      "grad_norm": 0.3750377893447876,
      "learning_rate": 1.9133333333333332e-05,
      "loss": 0.0027,
      "step": 92600
    },
    {
      "epoch": 4.9392,
      "grad_norm": 0.29327547550201416,
      "learning_rate": 1.913e-05,
      "loss": 0.0017,
      "step": 92610
    },
    {
      "epoch": 4.939733333333333,
      "grad_norm": 0.5201287269592285,
      "learning_rate": 1.9126666666666668e-05,
      "loss": 0.0027,
      "step": 92620
    },
    {
      "epoch": 4.940266666666667,
      "grad_norm": 0.2086993306875229,
      "learning_rate": 1.9123333333333334e-05,
      "loss": 0.0027,
      "step": 92630
    },
    {
      "epoch": 4.9408,
      "grad_norm": 0.18723486363887787,
      "learning_rate": 1.9120000000000003e-05,
      "loss": 0.0022,
      "step": 92640
    },
    {
      "epoch": 4.941333333333334,
      "grad_norm": 0.06970886141061783,
      "learning_rate": 1.911666666666667e-05,
      "loss": 0.0018,
      "step": 92650
    },
    {
      "epoch": 4.941866666666667,
      "grad_norm": 0.16603730618953705,
      "learning_rate": 1.9113333333333332e-05,
      "loss": 0.0015,
      "step": 92660
    },
    {
      "epoch": 4.9424,
      "grad_norm": 0.4417494237422943,
      "learning_rate": 1.911e-05,
      "loss": 0.002,
      "step": 92670
    },
    {
      "epoch": 4.942933333333333,
      "grad_norm": 0.09394503384828568,
      "learning_rate": 1.9106666666666668e-05,
      "loss": 0.0013,
      "step": 92680
    },
    {
      "epoch": 4.943466666666667,
      "grad_norm": 0.3917614221572876,
      "learning_rate": 1.9103333333333334e-05,
      "loss": 0.0017,
      "step": 92690
    },
    {
      "epoch": 4.944,
      "grad_norm": 0.0942297950387001,
      "learning_rate": 1.91e-05,
      "loss": 0.0016,
      "step": 92700
    },
    {
      "epoch": 4.944533333333333,
      "grad_norm": 0.09961633384227753,
      "learning_rate": 1.909666666666667e-05,
      "loss": 0.002,
      "step": 92710
    },
    {
      "epoch": 4.9450666666666665,
      "grad_norm": 0.2660563886165619,
      "learning_rate": 1.9093333333333336e-05,
      "loss": 0.0023,
      "step": 92720
    },
    {
      "epoch": 4.9456,
      "grad_norm": 0.16052000224590302,
      "learning_rate": 1.909e-05,
      "loss": 0.0031,
      "step": 92730
    },
    {
      "epoch": 4.946133333333333,
      "grad_norm": 0.20800909399986267,
      "learning_rate": 1.9086666666666668e-05,
      "loss": 0.0016,
      "step": 92740
    },
    {
      "epoch": 4.946666666666666,
      "grad_norm": 0.07011190056800842,
      "learning_rate": 1.9083333333333334e-05,
      "loss": 0.0025,
      "step": 92750
    },
    {
      "epoch": 4.9472000000000005,
      "grad_norm": 0.3248468041419983,
      "learning_rate": 1.908e-05,
      "loss": 0.002,
      "step": 92760
    },
    {
      "epoch": 4.947733333333334,
      "grad_norm": 0.3070663511753082,
      "learning_rate": 1.9076666666666666e-05,
      "loss": 0.0019,
      "step": 92770
    },
    {
      "epoch": 4.948266666666667,
      "grad_norm": 0.11439864337444305,
      "learning_rate": 1.9073333333333336e-05,
      "loss": 0.0019,
      "step": 92780
    },
    {
      "epoch": 4.9488,
      "grad_norm": 0.43078264594078064,
      "learning_rate": 1.9070000000000002e-05,
      "loss": 0.0016,
      "step": 92790
    },
    {
      "epoch": 4.949333333333334,
      "grad_norm": 0.15932393074035645,
      "learning_rate": 1.9066666666666668e-05,
      "loss": 0.0017,
      "step": 92800
    },
    {
      "epoch": 4.949866666666667,
      "grad_norm": 0.5213236808776855,
      "learning_rate": 1.9063333333333334e-05,
      "loss": 0.0018,
      "step": 92810
    },
    {
      "epoch": 4.9504,
      "grad_norm": 0.09181973338127136,
      "learning_rate": 1.906e-05,
      "loss": 0.0018,
      "step": 92820
    },
    {
      "epoch": 4.950933333333333,
      "grad_norm": 0.20066510140895844,
      "learning_rate": 1.9056666666666667e-05,
      "loss": 0.0016,
      "step": 92830
    },
    {
      "epoch": 4.951466666666667,
      "grad_norm": 0.2112341821193695,
      "learning_rate": 1.9053333333333333e-05,
      "loss": 0.0018,
      "step": 92840
    },
    {
      "epoch": 4.952,
      "grad_norm": 0.0939100980758667,
      "learning_rate": 1.9050000000000002e-05,
      "loss": 0.0015,
      "step": 92850
    },
    {
      "epoch": 4.952533333333333,
      "grad_norm": 0.048430439084768295,
      "learning_rate": 1.904666666666667e-05,
      "loss": 0.0023,
      "step": 92860
    },
    {
      "epoch": 4.9530666666666665,
      "grad_norm": 0.38647282123565674,
      "learning_rate": 1.9043333333333335e-05,
      "loss": 0.0016,
      "step": 92870
    },
    {
      "epoch": 4.9536,
      "grad_norm": 0.0984020009636879,
      "learning_rate": 1.904e-05,
      "loss": 0.0017,
      "step": 92880
    },
    {
      "epoch": 4.954133333333333,
      "grad_norm": 0.09136000275611877,
      "learning_rate": 1.9036666666666667e-05,
      "loss": 0.0016,
      "step": 92890
    },
    {
      "epoch": 4.954666666666666,
      "grad_norm": 0.11859029531478882,
      "learning_rate": 1.9033333333333333e-05,
      "loss": 0.0028,
      "step": 92900
    },
    {
      "epoch": 4.9552,
      "grad_norm": 0.17981645464897156,
      "learning_rate": 1.903e-05,
      "loss": 0.0015,
      "step": 92910
    },
    {
      "epoch": 4.955733333333333,
      "grad_norm": 0.18245723843574524,
      "learning_rate": 1.902666666666667e-05,
      "loss": 0.0016,
      "step": 92920
    },
    {
      "epoch": 4.956266666666667,
      "grad_norm": 0.2953982651233673,
      "learning_rate": 1.9023333333333335e-05,
      "loss": 0.0019,
      "step": 92930
    },
    {
      "epoch": 4.9568,
      "grad_norm": 0.14737877249717712,
      "learning_rate": 1.902e-05,
      "loss": 0.0023,
      "step": 92940
    },
    {
      "epoch": 4.957333333333334,
      "grad_norm": 0.35408586263656616,
      "learning_rate": 1.901666666666667e-05,
      "loss": 0.0027,
      "step": 92950
    },
    {
      "epoch": 4.957866666666667,
      "grad_norm": 0.20236967504024506,
      "learning_rate": 1.9013333333333333e-05,
      "loss": 0.0024,
      "step": 92960
    },
    {
      "epoch": 4.9584,
      "grad_norm": 0.20892062783241272,
      "learning_rate": 1.901e-05,
      "loss": 0.002,
      "step": 92970
    },
    {
      "epoch": 4.958933333333333,
      "grad_norm": 0.2896615266799927,
      "learning_rate": 1.9006666666666665e-05,
      "loss": 0.0025,
      "step": 92980
    },
    {
      "epoch": 4.959466666666667,
      "grad_norm": 0.4071079194545746,
      "learning_rate": 1.9003333333333335e-05,
      "loss": 0.0025,
      "step": 92990
    },
    {
      "epoch": 4.96,
      "grad_norm": 0.09119010716676712,
      "learning_rate": 1.9e-05,
      "loss": 0.0022,
      "step": 93000
    },
    {
      "epoch": 4.960533333333333,
      "grad_norm": 0.17891161143779755,
      "learning_rate": 1.8996666666666667e-05,
      "loss": 0.0022,
      "step": 93010
    },
    {
      "epoch": 4.9610666666666665,
      "grad_norm": 0.20991778373718262,
      "learning_rate": 1.8993333333333337e-05,
      "loss": 0.0026,
      "step": 93020
    },
    {
      "epoch": 4.9616,
      "grad_norm": 0.4080788791179657,
      "learning_rate": 1.8990000000000003e-05,
      "loss": 0.0016,
      "step": 93030
    },
    {
      "epoch": 4.962133333333333,
      "grad_norm": 0.391131192445755,
      "learning_rate": 1.8986666666666666e-05,
      "loss": 0.0015,
      "step": 93040
    },
    {
      "epoch": 4.962666666666666,
      "grad_norm": 0.3831014037132263,
      "learning_rate": 1.8983333333333335e-05,
      "loss": 0.0022,
      "step": 93050
    },
    {
      "epoch": 4.9632,
      "grad_norm": 0.330964058637619,
      "learning_rate": 1.898e-05,
      "loss": 0.0021,
      "step": 93060
    },
    {
      "epoch": 4.963733333333334,
      "grad_norm": 0.149909108877182,
      "learning_rate": 1.8976666666666667e-05,
      "loss": 0.0024,
      "step": 93070
    },
    {
      "epoch": 4.964266666666667,
      "grad_norm": 0.052924174815416336,
      "learning_rate": 1.8973333333333334e-05,
      "loss": 0.0016,
      "step": 93080
    },
    {
      "epoch": 4.9648,
      "grad_norm": 0.4700036346912384,
      "learning_rate": 1.8970000000000003e-05,
      "loss": 0.0025,
      "step": 93090
    },
    {
      "epoch": 4.965333333333334,
      "grad_norm": 0.04052557423710823,
      "learning_rate": 1.896666666666667e-05,
      "loss": 0.0015,
      "step": 93100
    },
    {
      "epoch": 4.965866666666667,
      "grad_norm": 0.4036386013031006,
      "learning_rate": 1.8963333333333332e-05,
      "loss": 0.0019,
      "step": 93110
    },
    {
      "epoch": 4.9664,
      "grad_norm": 0.047081731259822845,
      "learning_rate": 1.896e-05,
      "loss": 0.0015,
      "step": 93120
    },
    {
      "epoch": 4.966933333333333,
      "grad_norm": 0.17494533956050873,
      "learning_rate": 1.8956666666666668e-05,
      "loss": 0.0017,
      "step": 93130
    },
    {
      "epoch": 4.967466666666667,
      "grad_norm": 0.06714582443237305,
      "learning_rate": 1.8953333333333334e-05,
      "loss": 0.0026,
      "step": 93140
    },
    {
      "epoch": 4.968,
      "grad_norm": 0.021488267928361893,
      "learning_rate": 1.895e-05,
      "loss": 0.0014,
      "step": 93150
    },
    {
      "epoch": 4.968533333333333,
      "grad_norm": 0.3485875129699707,
      "learning_rate": 1.894666666666667e-05,
      "loss": 0.0016,
      "step": 93160
    },
    {
      "epoch": 4.9690666666666665,
      "grad_norm": 0.3147764205932617,
      "learning_rate": 1.8943333333333335e-05,
      "loss": 0.002,
      "step": 93170
    },
    {
      "epoch": 4.9696,
      "grad_norm": 0.2685299217700958,
      "learning_rate": 1.894e-05,
      "loss": 0.0014,
      "step": 93180
    },
    {
      "epoch": 4.970133333333333,
      "grad_norm": 0.24247901141643524,
      "learning_rate": 1.8936666666666668e-05,
      "loss": 0.0021,
      "step": 93190
    },
    {
      "epoch": 4.970666666666666,
      "grad_norm": 0.4395543038845062,
      "learning_rate": 1.8933333333333334e-05,
      "loss": 0.0019,
      "step": 93200
    },
    {
      "epoch": 4.9712,
      "grad_norm": 0.23607274889945984,
      "learning_rate": 1.893e-05,
      "loss": 0.0014,
      "step": 93210
    },
    {
      "epoch": 4.971733333333333,
      "grad_norm": 0.48784714937210083,
      "learning_rate": 1.8926666666666666e-05,
      "loss": 0.0017,
      "step": 93220
    },
    {
      "epoch": 4.972266666666666,
      "grad_norm": 0.35861924290657043,
      "learning_rate": 1.8923333333333336e-05,
      "loss": 0.0023,
      "step": 93230
    },
    {
      "epoch": 4.9728,
      "grad_norm": 0.29977917671203613,
      "learning_rate": 1.8920000000000002e-05,
      "loss": 0.0022,
      "step": 93240
    },
    {
      "epoch": 4.973333333333334,
      "grad_norm": 0.047345615923404694,
      "learning_rate": 1.8916666666666668e-05,
      "loss": 0.0017,
      "step": 93250
    },
    {
      "epoch": 4.973866666666667,
      "grad_norm": 0.0719168409705162,
      "learning_rate": 1.8913333333333334e-05,
      "loss": 0.0018,
      "step": 93260
    },
    {
      "epoch": 4.9744,
      "grad_norm": 0.4403080940246582,
      "learning_rate": 1.891e-05,
      "loss": 0.002,
      "step": 93270
    },
    {
      "epoch": 4.974933333333333,
      "grad_norm": 0.2146277278661728,
      "learning_rate": 1.8906666666666666e-05,
      "loss": 0.0013,
      "step": 93280
    },
    {
      "epoch": 4.975466666666667,
      "grad_norm": 0.1250942349433899,
      "learning_rate": 1.8903333333333332e-05,
      "loss": 0.0016,
      "step": 93290
    },
    {
      "epoch": 4.976,
      "grad_norm": 0.09791639447212219,
      "learning_rate": 1.8900000000000002e-05,
      "loss": 0.002,
      "step": 93300
    },
    {
      "epoch": 4.976533333333333,
      "grad_norm": 0.14504341781139374,
      "learning_rate": 1.8896666666666668e-05,
      "loss": 0.002,
      "step": 93310
    },
    {
      "epoch": 4.9770666666666665,
      "grad_norm": 0.17346541583538055,
      "learning_rate": 1.8893333333333334e-05,
      "loss": 0.0025,
      "step": 93320
    },
    {
      "epoch": 4.9776,
      "grad_norm": 0.23174305260181427,
      "learning_rate": 1.8890000000000004e-05,
      "loss": 0.0017,
      "step": 93330
    },
    {
      "epoch": 4.978133333333333,
      "grad_norm": 0.18436305224895477,
      "learning_rate": 1.8886666666666667e-05,
      "loss": 0.0024,
      "step": 93340
    },
    {
      "epoch": 4.978666666666666,
      "grad_norm": 0.0948190689086914,
      "learning_rate": 1.8883333333333333e-05,
      "loss": 0.0025,
      "step": 93350
    },
    {
      "epoch": 4.9792,
      "grad_norm": 0.03862234577536583,
      "learning_rate": 1.888e-05,
      "loss": 0.0013,
      "step": 93360
    },
    {
      "epoch": 4.979733333333334,
      "grad_norm": 0.3362976014614105,
      "learning_rate": 1.887666666666667e-05,
      "loss": 0.0015,
      "step": 93370
    },
    {
      "epoch": 4.980266666666667,
      "grad_norm": 0.05992508679628372,
      "learning_rate": 1.8873333333333334e-05,
      "loss": 0.0015,
      "step": 93380
    },
    {
      "epoch": 4.9808,
      "grad_norm": 0.3302007019519806,
      "learning_rate": 1.887e-05,
      "loss": 0.0018,
      "step": 93390
    },
    {
      "epoch": 4.981333333333334,
      "grad_norm": 0.26844480633735657,
      "learning_rate": 1.886666666666667e-05,
      "loss": 0.0019,
      "step": 93400
    },
    {
      "epoch": 4.981866666666667,
      "grad_norm": 0.36182865500450134,
      "learning_rate": 1.8863333333333333e-05,
      "loss": 0.0019,
      "step": 93410
    },
    {
      "epoch": 4.9824,
      "grad_norm": 0.15532273054122925,
      "learning_rate": 1.886e-05,
      "loss": 0.0022,
      "step": 93420
    },
    {
      "epoch": 4.982933333333333,
      "grad_norm": 0.06412461400032043,
      "learning_rate": 1.885666666666667e-05,
      "loss": 0.0016,
      "step": 93430
    },
    {
      "epoch": 4.983466666666667,
      "grad_norm": 0.3260495960712433,
      "learning_rate": 1.8853333333333335e-05,
      "loss": 0.0018,
      "step": 93440
    },
    {
      "epoch": 4.984,
      "grad_norm": 0.4692317843437195,
      "learning_rate": 1.885e-05,
      "loss": 0.0022,
      "step": 93450
    },
    {
      "epoch": 4.984533333333333,
      "grad_norm": 0.16128522157669067,
      "learning_rate": 1.8846666666666667e-05,
      "loss": 0.0025,
      "step": 93460
    },
    {
      "epoch": 4.9850666666666665,
      "grad_norm": 0.20467087626457214,
      "learning_rate": 1.8843333333333336e-05,
      "loss": 0.0015,
      "step": 93470
    },
    {
      "epoch": 4.9856,
      "grad_norm": 0.2382010966539383,
      "learning_rate": 1.8840000000000003e-05,
      "loss": 0.0023,
      "step": 93480
    },
    {
      "epoch": 4.986133333333333,
      "grad_norm": 0.19312602281570435,
      "learning_rate": 1.8836666666666665e-05,
      "loss": 0.0016,
      "step": 93490
    },
    {
      "epoch": 4.986666666666666,
      "grad_norm": 0.14082194864749908,
      "learning_rate": 1.8833333333333335e-05,
      "loss": 0.0023,
      "step": 93500
    },
    {
      "epoch": 4.9872,
      "grad_norm": 0.6456874012947083,
      "learning_rate": 1.883e-05,
      "loss": 0.0023,
      "step": 93510
    },
    {
      "epoch": 4.987733333333333,
      "grad_norm": 0.376212477684021,
      "learning_rate": 1.8826666666666667e-05,
      "loss": 0.0024,
      "step": 93520
    },
    {
      "epoch": 4.988266666666666,
      "grad_norm": 0.3589510917663574,
      "learning_rate": 1.8823333333333333e-05,
      "loss": 0.0023,
      "step": 93530
    },
    {
      "epoch": 4.9888,
      "grad_norm": 0.14625214040279388,
      "learning_rate": 1.8820000000000003e-05,
      "loss": 0.0023,
      "step": 93540
    },
    {
      "epoch": 4.989333333333334,
      "grad_norm": 0.06580916792154312,
      "learning_rate": 1.881666666666667e-05,
      "loss": 0.0024,
      "step": 93550
    },
    {
      "epoch": 4.989866666666667,
      "grad_norm": 0.3179735839366913,
      "learning_rate": 1.8813333333333335e-05,
      "loss": 0.0022,
      "step": 93560
    },
    {
      "epoch": 4.9904,
      "grad_norm": 0.07603128254413605,
      "learning_rate": 1.881e-05,
      "loss": 0.0016,
      "step": 93570
    },
    {
      "epoch": 4.990933333333333,
      "grad_norm": 0.1821572631597519,
      "learning_rate": 1.8806666666666667e-05,
      "loss": 0.0022,
      "step": 93580
    },
    {
      "epoch": 4.991466666666667,
      "grad_norm": 0.3938811123371124,
      "learning_rate": 1.8803333333333333e-05,
      "loss": 0.0021,
      "step": 93590
    },
    {
      "epoch": 4.992,
      "grad_norm": 0.17642664909362793,
      "learning_rate": 1.88e-05,
      "loss": 0.0023,
      "step": 93600
    },
    {
      "epoch": 4.992533333333333,
      "grad_norm": 0.41326889395713806,
      "learning_rate": 1.879666666666667e-05,
      "loss": 0.0016,
      "step": 93610
    },
    {
      "epoch": 4.9930666666666665,
      "grad_norm": 0.17814284563064575,
      "learning_rate": 1.8793333333333335e-05,
      "loss": 0.0015,
      "step": 93620
    },
    {
      "epoch": 4.9936,
      "grad_norm": 0.09211661666631699,
      "learning_rate": 1.879e-05,
      "loss": 0.0016,
      "step": 93630
    },
    {
      "epoch": 4.994133333333333,
      "grad_norm": 0.044943783432245255,
      "learning_rate": 1.8786666666666667e-05,
      "loss": 0.0019,
      "step": 93640
    },
    {
      "epoch": 4.994666666666666,
      "grad_norm": 0.0851963609457016,
      "learning_rate": 1.8783333333333334e-05,
      "loss": 0.0012,
      "step": 93650
    },
    {
      "epoch": 4.9952,
      "grad_norm": 0.06268001347780228,
      "learning_rate": 1.878e-05,
      "loss": 0.0018,
      "step": 93660
    },
    {
      "epoch": 4.995733333333334,
      "grad_norm": 0.6106104254722595,
      "learning_rate": 1.8776666666666666e-05,
      "loss": 0.002,
      "step": 93670
    },
    {
      "epoch": 4.996266666666667,
      "grad_norm": 0.1213085800409317,
      "learning_rate": 1.8773333333333335e-05,
      "loss": 0.0016,
      "step": 93680
    },
    {
      "epoch": 4.9968,
      "grad_norm": 0.18176722526550293,
      "learning_rate": 1.877e-05,
      "loss": 0.0022,
      "step": 93690
    },
    {
      "epoch": 4.997333333333334,
      "grad_norm": 0.3705015182495117,
      "learning_rate": 1.8766666666666668e-05,
      "loss": 0.003,
      "step": 93700
    },
    {
      "epoch": 4.997866666666667,
      "grad_norm": 0.3563685417175293,
      "learning_rate": 1.8763333333333337e-05,
      "loss": 0.002,
      "step": 93710
    },
    {
      "epoch": 4.9984,
      "grad_norm": 0.521722137928009,
      "learning_rate": 1.876e-05,
      "loss": 0.0026,
      "step": 93720
    },
    {
      "epoch": 4.9989333333333335,
      "grad_norm": 0.1873900443315506,
      "learning_rate": 1.8756666666666666e-05,
      "loss": 0.0029,
      "step": 93730
    },
    {
      "epoch": 4.999466666666667,
      "grad_norm": 0.2876928448677063,
      "learning_rate": 1.8753333333333332e-05,
      "loss": 0.0026,
      "step": 93740
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.1496671885251999,
      "learning_rate": 1.8750000000000002e-05,
      "loss": 0.0023,
      "step": 93750
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.0021553821861743927,
      "eval_runtime": 174.0496,
      "eval_samples_per_second": 1436.372,
      "eval_steps_per_second": 35.909,
      "step": 93750
    },
    {
      "epoch": 5.000533333333333,
      "grad_norm": 0.29125913977622986,
      "learning_rate": 1.8746666666666668e-05,
      "loss": 0.002,
      "step": 93760
    },
    {
      "epoch": 5.0010666666666665,
      "grad_norm": 0.14768418669700623,
      "learning_rate": 1.8743333333333334e-05,
      "loss": 0.0018,
      "step": 93770
    },
    {
      "epoch": 5.0016,
      "grad_norm": 0.5022985935211182,
      "learning_rate": 1.8740000000000004e-05,
      "loss": 0.0026,
      "step": 93780
    },
    {
      "epoch": 5.002133333333333,
      "grad_norm": 0.050005629658699036,
      "learning_rate": 1.8736666666666666e-05,
      "loss": 0.0018,
      "step": 93790
    },
    {
      "epoch": 5.002666666666666,
      "grad_norm": 0.2434847354888916,
      "learning_rate": 1.8733333333333332e-05,
      "loss": 0.002,
      "step": 93800
    },
    {
      "epoch": 5.0032,
      "grad_norm": 0.265166699886322,
      "learning_rate": 1.8730000000000002e-05,
      "loss": 0.0023,
      "step": 93810
    },
    {
      "epoch": 5.003733333333333,
      "grad_norm": 0.0661255419254303,
      "learning_rate": 1.8726666666666668e-05,
      "loss": 0.0021,
      "step": 93820
    },
    {
      "epoch": 5.004266666666667,
      "grad_norm": 0.10233713686466217,
      "learning_rate": 1.8723333333333334e-05,
      "loss": 0.002,
      "step": 93830
    },
    {
      "epoch": 5.0048,
      "grad_norm": 0.20313367247581482,
      "learning_rate": 1.872e-05,
      "loss": 0.0023,
      "step": 93840
    },
    {
      "epoch": 5.005333333333334,
      "grad_norm": 0.33465149998664856,
      "learning_rate": 1.871666666666667e-05,
      "loss": 0.002,
      "step": 93850
    },
    {
      "epoch": 5.005866666666667,
      "grad_norm": 0.06171627342700958,
      "learning_rate": 1.8713333333333336e-05,
      "loss": 0.0022,
      "step": 93860
    },
    {
      "epoch": 5.0064,
      "grad_norm": 0.09729159623384476,
      "learning_rate": 1.871e-05,
      "loss": 0.0019,
      "step": 93870
    },
    {
      "epoch": 5.0069333333333335,
      "grad_norm": 0.18659840524196625,
      "learning_rate": 1.8706666666666668e-05,
      "loss": 0.0022,
      "step": 93880
    },
    {
      "epoch": 5.007466666666667,
      "grad_norm": 0.0600418783724308,
      "learning_rate": 1.8703333333333334e-05,
      "loss": 0.0014,
      "step": 93890
    },
    {
      "epoch": 5.008,
      "grad_norm": 0.06392256170511246,
      "learning_rate": 1.87e-05,
      "loss": 0.0016,
      "step": 93900
    },
    {
      "epoch": 5.008533333333333,
      "grad_norm": 0.3274135887622833,
      "learning_rate": 1.8696666666666667e-05,
      "loss": 0.0026,
      "step": 93910
    },
    {
      "epoch": 5.009066666666667,
      "grad_norm": 0.38037118315696716,
      "learning_rate": 1.8693333333333336e-05,
      "loss": 0.0021,
      "step": 93920
    },
    {
      "epoch": 5.0096,
      "grad_norm": 0.08183933049440384,
      "learning_rate": 1.8690000000000002e-05,
      "loss": 0.0012,
      "step": 93930
    },
    {
      "epoch": 5.010133333333333,
      "grad_norm": 0.05059552565217018,
      "learning_rate": 1.8686666666666665e-05,
      "loss": 0.002,
      "step": 93940
    },
    {
      "epoch": 5.010666666666666,
      "grad_norm": 0.26174986362457275,
      "learning_rate": 1.8683333333333335e-05,
      "loss": 0.003,
      "step": 93950
    },
    {
      "epoch": 5.0112,
      "grad_norm": 0.06628049165010452,
      "learning_rate": 1.868e-05,
      "loss": 0.0023,
      "step": 93960
    },
    {
      "epoch": 5.011733333333333,
      "grad_norm": 0.3790990710258484,
      "learning_rate": 1.8676666666666667e-05,
      "loss": 0.0032,
      "step": 93970
    },
    {
      "epoch": 5.012266666666667,
      "grad_norm": 0.07262992858886719,
      "learning_rate": 1.8673333333333333e-05,
      "loss": 0.0014,
      "step": 93980
    },
    {
      "epoch": 5.0128,
      "grad_norm": 0.23691661655902863,
      "learning_rate": 1.8670000000000003e-05,
      "loss": 0.0014,
      "step": 93990
    },
    {
      "epoch": 5.013333333333334,
      "grad_norm": 0.11866661161184311,
      "learning_rate": 1.866666666666667e-05,
      "loss": 0.0014,
      "step": 94000
    },
    {
      "epoch": 5.013866666666667,
      "grad_norm": 0.2957533001899719,
      "learning_rate": 1.8663333333333335e-05,
      "loss": 0.0016,
      "step": 94010
    },
    {
      "epoch": 5.0144,
      "grad_norm": 0.5847240686416626,
      "learning_rate": 1.866e-05,
      "loss": 0.0021,
      "step": 94020
    },
    {
      "epoch": 5.0149333333333335,
      "grad_norm": 0.6936547160148621,
      "learning_rate": 1.8656666666666667e-05,
      "loss": 0.003,
      "step": 94030
    },
    {
      "epoch": 5.015466666666667,
      "grad_norm": 0.06251953542232513,
      "learning_rate": 1.8653333333333333e-05,
      "loss": 0.0015,
      "step": 94040
    },
    {
      "epoch": 5.016,
      "grad_norm": 0.04777584224939346,
      "learning_rate": 1.865e-05,
      "loss": 0.0017,
      "step": 94050
    },
    {
      "epoch": 5.016533333333333,
      "grad_norm": 0.1482585221529007,
      "learning_rate": 1.864666666666667e-05,
      "loss": 0.0017,
      "step": 94060
    },
    {
      "epoch": 5.017066666666667,
      "grad_norm": 0.5465273261070251,
      "learning_rate": 1.8643333333333335e-05,
      "loss": 0.0015,
      "step": 94070
    },
    {
      "epoch": 5.0176,
      "grad_norm": 0.0617559552192688,
      "learning_rate": 1.864e-05,
      "loss": 0.0015,
      "step": 94080
    },
    {
      "epoch": 5.018133333333333,
      "grad_norm": 0.0284428633749485,
      "learning_rate": 1.863666666666667e-05,
      "loss": 0.0017,
      "step": 94090
    },
    {
      "epoch": 5.018666666666666,
      "grad_norm": 0.11081300675868988,
      "learning_rate": 1.8633333333333333e-05,
      "loss": 0.0017,
      "step": 94100
    },
    {
      "epoch": 5.0192,
      "grad_norm": 0.13008639216423035,
      "learning_rate": 1.863e-05,
      "loss": 0.0023,
      "step": 94110
    },
    {
      "epoch": 5.019733333333333,
      "grad_norm": 0.3621746599674225,
      "learning_rate": 1.8626666666666666e-05,
      "loss": 0.0014,
      "step": 94120
    },
    {
      "epoch": 5.020266666666667,
      "grad_norm": 0.17957626283168793,
      "learning_rate": 1.8623333333333335e-05,
      "loss": 0.0017,
      "step": 94130
    },
    {
      "epoch": 5.0208,
      "grad_norm": 0.5714367628097534,
      "learning_rate": 1.862e-05,
      "loss": 0.0023,
      "step": 94140
    },
    {
      "epoch": 5.021333333333334,
      "grad_norm": 0.12162264436483383,
      "learning_rate": 1.8616666666666667e-05,
      "loss": 0.0016,
      "step": 94150
    },
    {
      "epoch": 5.021866666666667,
      "grad_norm": 0.11636000871658325,
      "learning_rate": 1.8613333333333337e-05,
      "loss": 0.0014,
      "step": 94160
    },
    {
      "epoch": 5.0224,
      "grad_norm": 0.47061121463775635,
      "learning_rate": 1.861e-05,
      "loss": 0.0032,
      "step": 94170
    },
    {
      "epoch": 5.0229333333333335,
      "grad_norm": 0.12799537181854248,
      "learning_rate": 1.8606666666666666e-05,
      "loss": 0.0016,
      "step": 94180
    },
    {
      "epoch": 5.023466666666667,
      "grad_norm": 0.17770449817180634,
      "learning_rate": 1.8603333333333335e-05,
      "loss": 0.0029,
      "step": 94190
    },
    {
      "epoch": 5.024,
      "grad_norm": 0.05948730930685997,
      "learning_rate": 1.86e-05,
      "loss": 0.0022,
      "step": 94200
    },
    {
      "epoch": 5.024533333333333,
      "grad_norm": 0.056800734251737595,
      "learning_rate": 1.8596666666666668e-05,
      "loss": 0.0014,
      "step": 94210
    },
    {
      "epoch": 5.025066666666667,
      "grad_norm": 0.09479718655347824,
      "learning_rate": 1.8593333333333334e-05,
      "loss": 0.002,
      "step": 94220
    },
    {
      "epoch": 5.0256,
      "grad_norm": 0.3489034175872803,
      "learning_rate": 1.8590000000000003e-05,
      "loss": 0.0013,
      "step": 94230
    },
    {
      "epoch": 5.026133333333333,
      "grad_norm": 0.241263285279274,
      "learning_rate": 1.858666666666667e-05,
      "loss": 0.001,
      "step": 94240
    },
    {
      "epoch": 5.026666666666666,
      "grad_norm": 0.049002304673194885,
      "learning_rate": 1.8583333333333332e-05,
      "loss": 0.0027,
      "step": 94250
    },
    {
      "epoch": 5.0272,
      "grad_norm": 0.02039479836821556,
      "learning_rate": 1.858e-05,
      "loss": 0.0015,
      "step": 94260
    },
    {
      "epoch": 5.027733333333333,
      "grad_norm": 0.18030232191085815,
      "learning_rate": 1.8576666666666668e-05,
      "loss": 0.0019,
      "step": 94270
    },
    {
      "epoch": 5.028266666666667,
      "grad_norm": 0.1438346952199936,
      "learning_rate": 1.8573333333333334e-05,
      "loss": 0.0017,
      "step": 94280
    },
    {
      "epoch": 5.0288,
      "grad_norm": 0.4919252395629883,
      "learning_rate": 1.857e-05,
      "loss": 0.0016,
      "step": 94290
    },
    {
      "epoch": 5.029333333333334,
      "grad_norm": 0.05262589454650879,
      "learning_rate": 1.856666666666667e-05,
      "loss": 0.0019,
      "step": 94300
    },
    {
      "epoch": 5.029866666666667,
      "grad_norm": 0.07771144062280655,
      "learning_rate": 1.8563333333333336e-05,
      "loss": 0.0022,
      "step": 94310
    },
    {
      "epoch": 5.0304,
      "grad_norm": 0.2575942575931549,
      "learning_rate": 1.856e-05,
      "loss": 0.0015,
      "step": 94320
    },
    {
      "epoch": 5.0309333333333335,
      "grad_norm": 0.09463249891996384,
      "learning_rate": 1.8556666666666668e-05,
      "loss": 0.0017,
      "step": 94330
    },
    {
      "epoch": 5.031466666666667,
      "grad_norm": 0.03728184103965759,
      "learning_rate": 1.8553333333333334e-05,
      "loss": 0.0014,
      "step": 94340
    },
    {
      "epoch": 5.032,
      "grad_norm": 0.2879573106765747,
      "learning_rate": 1.855e-05,
      "loss": 0.0016,
      "step": 94350
    },
    {
      "epoch": 5.032533333333333,
      "grad_norm": 0.4736040532588959,
      "learning_rate": 1.8546666666666666e-05,
      "loss": 0.0022,
      "step": 94360
    },
    {
      "epoch": 5.033066666666667,
      "grad_norm": 0.4053831994533539,
      "learning_rate": 1.8543333333333336e-05,
      "loss": 0.0012,
      "step": 94370
    },
    {
      "epoch": 5.0336,
      "grad_norm": 0.5201572179794312,
      "learning_rate": 1.8540000000000002e-05,
      "loss": 0.002,
      "step": 94380
    },
    {
      "epoch": 5.034133333333333,
      "grad_norm": 0.11309271305799484,
      "learning_rate": 1.8536666666666668e-05,
      "loss": 0.0019,
      "step": 94390
    },
    {
      "epoch": 5.034666666666666,
      "grad_norm": 0.24340537190437317,
      "learning_rate": 1.8533333333333334e-05,
      "loss": 0.0019,
      "step": 94400
    },
    {
      "epoch": 5.0352,
      "grad_norm": 0.2320038080215454,
      "learning_rate": 1.853e-05,
      "loss": 0.0014,
      "step": 94410
    },
    {
      "epoch": 5.035733333333333,
      "grad_norm": 0.4682672321796417,
      "learning_rate": 1.8526666666666667e-05,
      "loss": 0.0028,
      "step": 94420
    },
    {
      "epoch": 5.036266666666666,
      "grad_norm": 0.4664015471935272,
      "learning_rate": 1.8523333333333333e-05,
      "loss": 0.0015,
      "step": 94430
    },
    {
      "epoch": 5.0368,
      "grad_norm": 0.2064511775970459,
      "learning_rate": 1.8520000000000002e-05,
      "loss": 0.0021,
      "step": 94440
    },
    {
      "epoch": 5.037333333333334,
      "grad_norm": 0.09123773127794266,
      "learning_rate": 1.851666666666667e-05,
      "loss": 0.0018,
      "step": 94450
    },
    {
      "epoch": 5.037866666666667,
      "grad_norm": 0.2032599300146103,
      "learning_rate": 1.8513333333333335e-05,
      "loss": 0.0018,
      "step": 94460
    },
    {
      "epoch": 5.0384,
      "grad_norm": 0.10227467119693756,
      "learning_rate": 1.851e-05,
      "loss": 0.0018,
      "step": 94470
    },
    {
      "epoch": 5.0389333333333335,
      "grad_norm": 0.15019530057907104,
      "learning_rate": 1.8506666666666667e-05,
      "loss": 0.0023,
      "step": 94480
    },
    {
      "epoch": 5.039466666666667,
      "grad_norm": 0.23308196663856506,
      "learning_rate": 1.8503333333333333e-05,
      "loss": 0.0015,
      "step": 94490
    },
    {
      "epoch": 5.04,
      "grad_norm": 0.08975724130868912,
      "learning_rate": 1.85e-05,
      "loss": 0.0026,
      "step": 94500
    },
    {
      "epoch": 5.040533333333333,
      "grad_norm": 0.46478450298309326,
      "learning_rate": 1.849666666666667e-05,
      "loss": 0.0016,
      "step": 94510
    },
    {
      "epoch": 5.041066666666667,
      "grad_norm": 0.26559844613075256,
      "learning_rate": 1.8493333333333335e-05,
      "loss": 0.0016,
      "step": 94520
    },
    {
      "epoch": 5.0416,
      "grad_norm": 0.233468696475029,
      "learning_rate": 1.849e-05,
      "loss": 0.0022,
      "step": 94530
    },
    {
      "epoch": 5.042133333333333,
      "grad_norm": 0.2625584900379181,
      "learning_rate": 1.848666666666667e-05,
      "loss": 0.0021,
      "step": 94540
    },
    {
      "epoch": 5.042666666666666,
      "grad_norm": 0.06190764531493187,
      "learning_rate": 1.8483333333333333e-05,
      "loss": 0.0015,
      "step": 94550
    },
    {
      "epoch": 5.0432,
      "grad_norm": 0.17836014926433563,
      "learning_rate": 1.848e-05,
      "loss": 0.0019,
      "step": 94560
    },
    {
      "epoch": 5.043733333333333,
      "grad_norm": 0.2670629024505615,
      "learning_rate": 1.847666666666667e-05,
      "loss": 0.0032,
      "step": 94570
    },
    {
      "epoch": 5.044266666666666,
      "grad_norm": 0.1722966432571411,
      "learning_rate": 1.8473333333333335e-05,
      "loss": 0.0016,
      "step": 94580
    },
    {
      "epoch": 5.0448,
      "grad_norm": 0.0919010266661644,
      "learning_rate": 1.847e-05,
      "loss": 0.0017,
      "step": 94590
    },
    {
      "epoch": 5.045333333333334,
      "grad_norm": 0.1294175237417221,
      "learning_rate": 1.8466666666666667e-05,
      "loss": 0.0019,
      "step": 94600
    },
    {
      "epoch": 5.045866666666667,
      "grad_norm": 0.0626896321773529,
      "learning_rate": 1.8463333333333337e-05,
      "loss": 0.0018,
      "step": 94610
    },
    {
      "epoch": 5.0464,
      "grad_norm": 0.12778548896312714,
      "learning_rate": 1.846e-05,
      "loss": 0.004,
      "step": 94620
    },
    {
      "epoch": 5.0469333333333335,
      "grad_norm": 0.08966202288866043,
      "learning_rate": 1.8456666666666666e-05,
      "loss": 0.0017,
      "step": 94630
    },
    {
      "epoch": 5.047466666666667,
      "grad_norm": 0.11645638197660446,
      "learning_rate": 1.8453333333333335e-05,
      "loss": 0.0011,
      "step": 94640
    },
    {
      "epoch": 5.048,
      "grad_norm": 0.08293074369430542,
      "learning_rate": 1.845e-05,
      "loss": 0.0015,
      "step": 94650
    },
    {
      "epoch": 5.048533333333333,
      "grad_norm": 0.0362318679690361,
      "learning_rate": 1.8446666666666667e-05,
      "loss": 0.0016,
      "step": 94660
    },
    {
      "epoch": 5.049066666666667,
      "grad_norm": 0.19886267185211182,
      "learning_rate": 1.8443333333333333e-05,
      "loss": 0.0016,
      "step": 94670
    },
    {
      "epoch": 5.0496,
      "grad_norm": 0.12665709853172302,
      "learning_rate": 1.8440000000000003e-05,
      "loss": 0.0017,
      "step": 94680
    },
    {
      "epoch": 5.050133333333333,
      "grad_norm": 0.17751029133796692,
      "learning_rate": 1.843666666666667e-05,
      "loss": 0.0019,
      "step": 94690
    },
    {
      "epoch": 5.050666666666666,
      "grad_norm": 0.5498068332672119,
      "learning_rate": 1.8433333333333332e-05,
      "loss": 0.002,
      "step": 94700
    },
    {
      "epoch": 5.0512,
      "grad_norm": 0.12955883145332336,
      "learning_rate": 1.843e-05,
      "loss": 0.0015,
      "step": 94710
    },
    {
      "epoch": 5.051733333333333,
      "grad_norm": 0.03762281686067581,
      "learning_rate": 1.8426666666666668e-05,
      "loss": 0.0018,
      "step": 94720
    },
    {
      "epoch": 5.052266666666666,
      "grad_norm": 0.1722267121076584,
      "learning_rate": 1.8423333333333334e-05,
      "loss": 0.0018,
      "step": 94730
    },
    {
      "epoch": 5.0528,
      "grad_norm": 0.20576873421669006,
      "learning_rate": 1.842e-05,
      "loss": 0.0022,
      "step": 94740
    },
    {
      "epoch": 5.053333333333334,
      "grad_norm": 0.1508389562368393,
      "learning_rate": 1.841666666666667e-05,
      "loss": 0.0021,
      "step": 94750
    },
    {
      "epoch": 5.053866666666667,
      "grad_norm": 0.21292737126350403,
      "learning_rate": 1.8413333333333335e-05,
      "loss": 0.0015,
      "step": 94760
    },
    {
      "epoch": 5.0544,
      "grad_norm": 0.06964936852455139,
      "learning_rate": 1.841e-05,
      "loss": 0.0022,
      "step": 94770
    },
    {
      "epoch": 5.0549333333333335,
      "grad_norm": 0.12034744024276733,
      "learning_rate": 1.8406666666666668e-05,
      "loss": 0.0028,
      "step": 94780
    },
    {
      "epoch": 5.055466666666667,
      "grad_norm": 0.17655439674854279,
      "learning_rate": 1.8403333333333334e-05,
      "loss": 0.0029,
      "step": 94790
    },
    {
      "epoch": 5.056,
      "grad_norm": 0.021621327847242355,
      "learning_rate": 1.84e-05,
      "loss": 0.0018,
      "step": 94800
    },
    {
      "epoch": 5.056533333333333,
      "grad_norm": 0.3250439763069153,
      "learning_rate": 1.8396666666666666e-05,
      "loss": 0.0016,
      "step": 94810
    },
    {
      "epoch": 5.057066666666667,
      "grad_norm": 0.08882492035627365,
      "learning_rate": 1.8393333333333336e-05,
      "loss": 0.0025,
      "step": 94820
    },
    {
      "epoch": 5.0576,
      "grad_norm": 0.2356722205877304,
      "learning_rate": 1.8390000000000002e-05,
      "loss": 0.0014,
      "step": 94830
    },
    {
      "epoch": 5.058133333333333,
      "grad_norm": 0.117372527718544,
      "learning_rate": 1.8386666666666668e-05,
      "loss": 0.0023,
      "step": 94840
    },
    {
      "epoch": 5.058666666666666,
      "grad_norm": 0.030099447816610336,
      "learning_rate": 1.8383333333333334e-05,
      "loss": 0.0024,
      "step": 94850
    },
    {
      "epoch": 5.0592,
      "grad_norm": 0.44449877738952637,
      "learning_rate": 1.838e-05,
      "loss": 0.0017,
      "step": 94860
    },
    {
      "epoch": 5.059733333333333,
      "grad_norm": 0.22852201759815216,
      "learning_rate": 1.8376666666666666e-05,
      "loss": 0.0015,
      "step": 94870
    },
    {
      "epoch": 5.060266666666666,
      "grad_norm": 0.16146601736545563,
      "learning_rate": 1.8373333333333332e-05,
      "loss": 0.0014,
      "step": 94880
    },
    {
      "epoch": 5.0608,
      "grad_norm": 0.03369824215769768,
      "learning_rate": 1.8370000000000002e-05,
      "loss": 0.002,
      "step": 94890
    },
    {
      "epoch": 5.061333333333334,
      "grad_norm": 0.41157132387161255,
      "learning_rate": 1.8366666666666668e-05,
      "loss": 0.0027,
      "step": 94900
    },
    {
      "epoch": 5.061866666666667,
      "grad_norm": 0.5780165195465088,
      "learning_rate": 1.8363333333333334e-05,
      "loss": 0.003,
      "step": 94910
    },
    {
      "epoch": 5.0624,
      "grad_norm": 0.26095202565193176,
      "learning_rate": 1.8360000000000004e-05,
      "loss": 0.0022,
      "step": 94920
    },
    {
      "epoch": 5.0629333333333335,
      "grad_norm": 0.02877030335366726,
      "learning_rate": 1.8356666666666667e-05,
      "loss": 0.0024,
      "step": 94930
    },
    {
      "epoch": 5.063466666666667,
      "grad_norm": 0.17032964527606964,
      "learning_rate": 1.8353333333333333e-05,
      "loss": 0.0019,
      "step": 94940
    },
    {
      "epoch": 5.064,
      "grad_norm": 0.1796150803565979,
      "learning_rate": 1.8350000000000002e-05,
      "loss": 0.0016,
      "step": 94950
    },
    {
      "epoch": 5.064533333333333,
      "grad_norm": 0.341325581073761,
      "learning_rate": 1.834666666666667e-05,
      "loss": 0.0024,
      "step": 94960
    },
    {
      "epoch": 5.065066666666667,
      "grad_norm": 0.12003609538078308,
      "learning_rate": 1.8343333333333334e-05,
      "loss": 0.0029,
      "step": 94970
    },
    {
      "epoch": 5.0656,
      "grad_norm": 0.2932916581630707,
      "learning_rate": 1.834e-05,
      "loss": 0.0015,
      "step": 94980
    },
    {
      "epoch": 5.066133333333333,
      "grad_norm": 0.07971489429473877,
      "learning_rate": 1.833666666666667e-05,
      "loss": 0.0017,
      "step": 94990
    },
    {
      "epoch": 5.066666666666666,
      "grad_norm": 0.3593660891056061,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 0.0018,
      "step": 95000
    },
    {
      "epoch": 5.0672,
      "grad_norm": 0.3241071403026581,
      "learning_rate": 1.833e-05,
      "loss": 0.0021,
      "step": 95010
    },
    {
      "epoch": 5.067733333333333,
      "grad_norm": 0.03986993059515953,
      "learning_rate": 1.832666666666667e-05,
      "loss": 0.0017,
      "step": 95020
    },
    {
      "epoch": 5.068266666666666,
      "grad_norm": 0.0797894075512886,
      "learning_rate": 1.8323333333333335e-05,
      "loss": 0.0018,
      "step": 95030
    },
    {
      "epoch": 5.0688,
      "grad_norm": 0.17733033001422882,
      "learning_rate": 1.832e-05,
      "loss": 0.0014,
      "step": 95040
    },
    {
      "epoch": 5.069333333333334,
      "grad_norm": 0.03934017941355705,
      "learning_rate": 1.8316666666666667e-05,
      "loss": 0.0015,
      "step": 95050
    },
    {
      "epoch": 5.069866666666667,
      "grad_norm": 0.028336983174085617,
      "learning_rate": 1.8313333333333336e-05,
      "loss": 0.002,
      "step": 95060
    },
    {
      "epoch": 5.0704,
      "grad_norm": 0.18011145293712616,
      "learning_rate": 1.8310000000000003e-05,
      "loss": 0.0031,
      "step": 95070
    },
    {
      "epoch": 5.0709333333333335,
      "grad_norm": 0.5832024216651917,
      "learning_rate": 1.8306666666666665e-05,
      "loss": 0.0021,
      "step": 95080
    },
    {
      "epoch": 5.071466666666667,
      "grad_norm": 0.41298428177833557,
      "learning_rate": 1.8303333333333335e-05,
      "loss": 0.0026,
      "step": 95090
    },
    {
      "epoch": 5.072,
      "grad_norm": 0.03221404179930687,
      "learning_rate": 1.83e-05,
      "loss": 0.0024,
      "step": 95100
    },
    {
      "epoch": 5.072533333333333,
      "grad_norm": 0.2898525595664978,
      "learning_rate": 1.8296666666666667e-05,
      "loss": 0.0019,
      "step": 95110
    },
    {
      "epoch": 5.073066666666667,
      "grad_norm": 0.14486698806285858,
      "learning_rate": 1.8293333333333333e-05,
      "loss": 0.0019,
      "step": 95120
    },
    {
      "epoch": 5.0736,
      "grad_norm": 0.20437099039554596,
      "learning_rate": 1.8290000000000003e-05,
      "loss": 0.0026,
      "step": 95130
    },
    {
      "epoch": 5.074133333333333,
      "grad_norm": 0.09003834426403046,
      "learning_rate": 1.828666666666667e-05,
      "loss": 0.0029,
      "step": 95140
    },
    {
      "epoch": 5.074666666666666,
      "grad_norm": 0.4726051092147827,
      "learning_rate": 1.828333333333333e-05,
      "loss": 0.0017,
      "step": 95150
    },
    {
      "epoch": 5.0752,
      "grad_norm": 0.2577662765979767,
      "learning_rate": 1.828e-05,
      "loss": 0.0021,
      "step": 95160
    },
    {
      "epoch": 5.075733333333333,
      "grad_norm": 0.09177999943494797,
      "learning_rate": 1.8276666666666667e-05,
      "loss": 0.0012,
      "step": 95170
    },
    {
      "epoch": 5.076266666666666,
      "grad_norm": 0.08946604281663895,
      "learning_rate": 1.8273333333333333e-05,
      "loss": 0.0022,
      "step": 95180
    },
    {
      "epoch": 5.0768,
      "grad_norm": 0.06522547453641891,
      "learning_rate": 1.827e-05,
      "loss": 0.0018,
      "step": 95190
    },
    {
      "epoch": 5.077333333333334,
      "grad_norm": 0.11707484722137451,
      "learning_rate": 1.826666666666667e-05,
      "loss": 0.0023,
      "step": 95200
    },
    {
      "epoch": 5.077866666666667,
      "grad_norm": 0.07682043313980103,
      "learning_rate": 1.8263333333333335e-05,
      "loss": 0.0018,
      "step": 95210
    },
    {
      "epoch": 5.0784,
      "grad_norm": 0.03727266192436218,
      "learning_rate": 1.826e-05,
      "loss": 0.0022,
      "step": 95220
    },
    {
      "epoch": 5.0789333333333335,
      "grad_norm": 0.29262274503707886,
      "learning_rate": 1.8256666666666667e-05,
      "loss": 0.0022,
      "step": 95230
    },
    {
      "epoch": 5.079466666666667,
      "grad_norm": 0.20421409606933594,
      "learning_rate": 1.8253333333333334e-05,
      "loss": 0.0019,
      "step": 95240
    },
    {
      "epoch": 5.08,
      "grad_norm": 0.06334923207759857,
      "learning_rate": 1.825e-05,
      "loss": 0.0023,
      "step": 95250
    },
    {
      "epoch": 5.080533333333333,
      "grad_norm": 0.31136754155158997,
      "learning_rate": 1.8246666666666666e-05,
      "loss": 0.0022,
      "step": 95260
    },
    {
      "epoch": 5.081066666666667,
      "grad_norm": 0.1675562560558319,
      "learning_rate": 1.8243333333333335e-05,
      "loss": 0.0021,
      "step": 95270
    },
    {
      "epoch": 5.0816,
      "grad_norm": 0.025006193667650223,
      "learning_rate": 1.824e-05,
      "loss": 0.0034,
      "step": 95280
    },
    {
      "epoch": 5.082133333333333,
      "grad_norm": 0.22854256629943848,
      "learning_rate": 1.8236666666666668e-05,
      "loss": 0.0018,
      "step": 95290
    },
    {
      "epoch": 5.082666666666666,
      "grad_norm": 0.41189464926719666,
      "learning_rate": 1.8233333333333334e-05,
      "loss": 0.0025,
      "step": 95300
    },
    {
      "epoch": 5.0832,
      "grad_norm": 0.6079100966453552,
      "learning_rate": 1.823e-05,
      "loss": 0.0016,
      "step": 95310
    },
    {
      "epoch": 5.083733333333333,
      "grad_norm": 0.07177209854125977,
      "learning_rate": 1.8226666666666666e-05,
      "loss": 0.002,
      "step": 95320
    },
    {
      "epoch": 5.084266666666666,
      "grad_norm": 0.037988729774951935,
      "learning_rate": 1.8223333333333336e-05,
      "loss": 0.0021,
      "step": 95330
    },
    {
      "epoch": 5.0848,
      "grad_norm": 0.2022000253200531,
      "learning_rate": 1.8220000000000002e-05,
      "loss": 0.0012,
      "step": 95340
    },
    {
      "epoch": 5.085333333333334,
      "grad_norm": 0.04780345782637596,
      "learning_rate": 1.8216666666666668e-05,
      "loss": 0.0018,
      "step": 95350
    },
    {
      "epoch": 5.085866666666667,
      "grad_norm": 0.19673100113868713,
      "learning_rate": 1.8213333333333334e-05,
      "loss": 0.0021,
      "step": 95360
    },
    {
      "epoch": 5.0864,
      "grad_norm": 0.260833203792572,
      "learning_rate": 1.8210000000000004e-05,
      "loss": 0.0023,
      "step": 95370
    },
    {
      "epoch": 5.0869333333333335,
      "grad_norm": 0.24051308631896973,
      "learning_rate": 1.8206666666666666e-05,
      "loss": 0.0019,
      "step": 95380
    },
    {
      "epoch": 5.087466666666667,
      "grad_norm": 0.1546698361635208,
      "learning_rate": 1.8203333333333332e-05,
      "loss": 0.0013,
      "step": 95390
    },
    {
      "epoch": 5.088,
      "grad_norm": 0.02428811974823475,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 0.0017,
      "step": 95400
    },
    {
      "epoch": 5.088533333333333,
      "grad_norm": 0.2935711741447449,
      "learning_rate": 1.8196666666666668e-05,
      "loss": 0.0016,
      "step": 95410
    },
    {
      "epoch": 5.089066666666667,
      "grad_norm": 0.14572134613990784,
      "learning_rate": 1.8193333333333334e-05,
      "loss": 0.0024,
      "step": 95420
    },
    {
      "epoch": 5.0896,
      "grad_norm": 0.11559909582138062,
      "learning_rate": 1.819e-05,
      "loss": 0.0022,
      "step": 95430
    },
    {
      "epoch": 5.090133333333333,
      "grad_norm": 0.0328044556081295,
      "learning_rate": 1.818666666666667e-05,
      "loss": 0.0018,
      "step": 95440
    },
    {
      "epoch": 5.0906666666666665,
      "grad_norm": 0.04269925504922867,
      "learning_rate": 1.8183333333333336e-05,
      "loss": 0.0018,
      "step": 95450
    },
    {
      "epoch": 5.0912,
      "grad_norm": 0.08507790416479111,
      "learning_rate": 1.818e-05,
      "loss": 0.0029,
      "step": 95460
    },
    {
      "epoch": 5.091733333333333,
      "grad_norm": 0.21327713131904602,
      "learning_rate": 1.8176666666666668e-05,
      "loss": 0.0023,
      "step": 95470
    },
    {
      "epoch": 5.092266666666666,
      "grad_norm": 0.1519804447889328,
      "learning_rate": 1.8173333333333334e-05,
      "loss": 0.0027,
      "step": 95480
    },
    {
      "epoch": 5.0928,
      "grad_norm": 0.4461396336555481,
      "learning_rate": 1.817e-05,
      "loss": 0.0017,
      "step": 95490
    },
    {
      "epoch": 5.093333333333334,
      "grad_norm": 0.08304090052843094,
      "learning_rate": 1.8166666666666667e-05,
      "loss": 0.0035,
      "step": 95500
    },
    {
      "epoch": 5.093866666666667,
      "grad_norm": 0.6081627607345581,
      "learning_rate": 1.8163333333333336e-05,
      "loss": 0.0013,
      "step": 95510
    },
    {
      "epoch": 5.0944,
      "grad_norm": 0.1709282398223877,
      "learning_rate": 1.8160000000000002e-05,
      "loss": 0.0015,
      "step": 95520
    },
    {
      "epoch": 5.0949333333333335,
      "grad_norm": 0.26679977774620056,
      "learning_rate": 1.8156666666666665e-05,
      "loss": 0.0019,
      "step": 95530
    },
    {
      "epoch": 5.095466666666667,
      "grad_norm": 0.03597699850797653,
      "learning_rate": 1.8153333333333335e-05,
      "loss": 0.0032,
      "step": 95540
    },
    {
      "epoch": 5.096,
      "grad_norm": 0.14574365317821503,
      "learning_rate": 1.815e-05,
      "loss": 0.0019,
      "step": 95550
    },
    {
      "epoch": 5.096533333333333,
      "grad_norm": 0.9097516536712646,
      "learning_rate": 1.8146666666666667e-05,
      "loss": 0.0019,
      "step": 95560
    },
    {
      "epoch": 5.097066666666667,
      "grad_norm": 0.5955884456634521,
      "learning_rate": 1.8143333333333333e-05,
      "loss": 0.0021,
      "step": 95570
    },
    {
      "epoch": 5.0976,
      "grad_norm": 0.15462011098861694,
      "learning_rate": 1.8140000000000003e-05,
      "loss": 0.0017,
      "step": 95580
    },
    {
      "epoch": 5.098133333333333,
      "grad_norm": 0.12596876919269562,
      "learning_rate": 1.813666666666667e-05,
      "loss": 0.0016,
      "step": 95590
    },
    {
      "epoch": 5.0986666666666665,
      "grad_norm": 0.12320888042449951,
      "learning_rate": 1.8133333333333335e-05,
      "loss": 0.0023,
      "step": 95600
    },
    {
      "epoch": 5.0992,
      "grad_norm": 0.0707160085439682,
      "learning_rate": 1.813e-05,
      "loss": 0.0012,
      "step": 95610
    },
    {
      "epoch": 5.099733333333333,
      "grad_norm": 0.10725270211696625,
      "learning_rate": 1.8126666666666667e-05,
      "loss": 0.0024,
      "step": 95620
    },
    {
      "epoch": 5.100266666666666,
      "grad_norm": 0.35314226150512695,
      "learning_rate": 1.8123333333333333e-05,
      "loss": 0.002,
      "step": 95630
    },
    {
      "epoch": 5.1008,
      "grad_norm": 0.20169827342033386,
      "learning_rate": 1.812e-05,
      "loss": 0.0017,
      "step": 95640
    },
    {
      "epoch": 5.101333333333334,
      "grad_norm": 0.29136231541633606,
      "learning_rate": 1.811666666666667e-05,
      "loss": 0.0013,
      "step": 95650
    },
    {
      "epoch": 5.101866666666667,
      "grad_norm": 0.13066501915454865,
      "learning_rate": 1.8113333333333335e-05,
      "loss": 0.0017,
      "step": 95660
    },
    {
      "epoch": 5.1024,
      "grad_norm": 0.06239406019449234,
      "learning_rate": 1.811e-05,
      "loss": 0.0021,
      "step": 95670
    },
    {
      "epoch": 5.1029333333333335,
      "grad_norm": 0.2072327435016632,
      "learning_rate": 1.8106666666666667e-05,
      "loss": 0.0022,
      "step": 95680
    },
    {
      "epoch": 5.103466666666667,
      "grad_norm": 0.07803118973970413,
      "learning_rate": 1.8103333333333333e-05,
      "loss": 0.002,
      "step": 95690
    },
    {
      "epoch": 5.104,
      "grad_norm": 0.16007541120052338,
      "learning_rate": 1.81e-05,
      "loss": 0.0017,
      "step": 95700
    },
    {
      "epoch": 5.104533333333333,
      "grad_norm": 0.6565239429473877,
      "learning_rate": 1.8096666666666666e-05,
      "loss": 0.0015,
      "step": 95710
    },
    {
      "epoch": 5.105066666666667,
      "grad_norm": 0.362001895904541,
      "learning_rate": 1.8093333333333335e-05,
      "loss": 0.0031,
      "step": 95720
    },
    {
      "epoch": 5.1056,
      "grad_norm": 0.25262749195098877,
      "learning_rate": 1.809e-05,
      "loss": 0.0021,
      "step": 95730
    },
    {
      "epoch": 5.106133333333333,
      "grad_norm": 0.15631338953971863,
      "learning_rate": 1.8086666666666667e-05,
      "loss": 0.0023,
      "step": 95740
    },
    {
      "epoch": 5.1066666666666665,
      "grad_norm": 0.04759296402335167,
      "learning_rate": 1.8083333333333337e-05,
      "loss": 0.0028,
      "step": 95750
    },
    {
      "epoch": 5.1072,
      "grad_norm": 0.5260597467422485,
      "learning_rate": 1.808e-05,
      "loss": 0.0023,
      "step": 95760
    },
    {
      "epoch": 5.107733333333333,
      "grad_norm": 0.29586827754974365,
      "learning_rate": 1.8076666666666666e-05,
      "loss": 0.0024,
      "step": 95770
    },
    {
      "epoch": 5.108266666666666,
      "grad_norm": 0.18004469573497772,
      "learning_rate": 1.8073333333333335e-05,
      "loss": 0.0017,
      "step": 95780
    },
    {
      "epoch": 5.1088,
      "grad_norm": 0.23031888902187347,
      "learning_rate": 1.807e-05,
      "loss": 0.002,
      "step": 95790
    },
    {
      "epoch": 5.109333333333334,
      "grad_norm": 0.03473082557320595,
      "learning_rate": 1.8066666666666668e-05,
      "loss": 0.0018,
      "step": 95800
    },
    {
      "epoch": 5.109866666666667,
      "grad_norm": 0.39948245882987976,
      "learning_rate": 1.8063333333333334e-05,
      "loss": 0.0017,
      "step": 95810
    },
    {
      "epoch": 5.1104,
      "grad_norm": 0.026853298768401146,
      "learning_rate": 1.8060000000000003e-05,
      "loss": 0.0016,
      "step": 95820
    },
    {
      "epoch": 5.1109333333333336,
      "grad_norm": 0.4090105891227722,
      "learning_rate": 1.8056666666666666e-05,
      "loss": 0.002,
      "step": 95830
    },
    {
      "epoch": 5.111466666666667,
      "grad_norm": 0.04531285911798477,
      "learning_rate": 1.8053333333333332e-05,
      "loss": 0.0019,
      "step": 95840
    },
    {
      "epoch": 5.112,
      "grad_norm": 0.19777902960777283,
      "learning_rate": 1.805e-05,
      "loss": 0.0013,
      "step": 95850
    },
    {
      "epoch": 5.112533333333333,
      "grad_norm": 0.021620268002152443,
      "learning_rate": 1.8046666666666668e-05,
      "loss": 0.0018,
      "step": 95860
    },
    {
      "epoch": 5.113066666666667,
      "grad_norm": 0.14420601725578308,
      "learning_rate": 1.8043333333333334e-05,
      "loss": 0.0023,
      "step": 95870
    },
    {
      "epoch": 5.1136,
      "grad_norm": 0.17793801426887512,
      "learning_rate": 1.804e-05,
      "loss": 0.0017,
      "step": 95880
    },
    {
      "epoch": 5.114133333333333,
      "grad_norm": 0.44621387124061584,
      "learning_rate": 1.803666666666667e-05,
      "loss": 0.0016,
      "step": 95890
    },
    {
      "epoch": 5.1146666666666665,
      "grad_norm": 0.25667670369148254,
      "learning_rate": 1.8033333333333336e-05,
      "loss": 0.0016,
      "step": 95900
    },
    {
      "epoch": 5.1152,
      "grad_norm": 0.09490106254816055,
      "learning_rate": 1.803e-05,
      "loss": 0.0021,
      "step": 95910
    },
    {
      "epoch": 5.115733333333333,
      "grad_norm": 0.12155859917402267,
      "learning_rate": 1.8026666666666668e-05,
      "loss": 0.0014,
      "step": 95920
    },
    {
      "epoch": 5.116266666666666,
      "grad_norm": 0.09337130188941956,
      "learning_rate": 1.8023333333333334e-05,
      "loss": 0.0021,
      "step": 95930
    },
    {
      "epoch": 5.1168,
      "grad_norm": 0.17714135348796844,
      "learning_rate": 1.802e-05,
      "loss": 0.0025,
      "step": 95940
    },
    {
      "epoch": 5.117333333333334,
      "grad_norm": 0.1289629191160202,
      "learning_rate": 1.8016666666666666e-05,
      "loss": 0.0021,
      "step": 95950
    },
    {
      "epoch": 5.117866666666667,
      "grad_norm": 0.14192114770412445,
      "learning_rate": 1.8013333333333336e-05,
      "loss": 0.0017,
      "step": 95960
    },
    {
      "epoch": 5.1184,
      "grad_norm": 0.12510690093040466,
      "learning_rate": 1.8010000000000002e-05,
      "loss": 0.0024,
      "step": 95970
    },
    {
      "epoch": 5.118933333333334,
      "grad_norm": 0.3772093653678894,
      "learning_rate": 1.8006666666666668e-05,
      "loss": 0.0013,
      "step": 95980
    },
    {
      "epoch": 5.119466666666667,
      "grad_norm": 0.16930581629276276,
      "learning_rate": 1.8003333333333334e-05,
      "loss": 0.002,
      "step": 95990
    },
    {
      "epoch": 5.12,
      "grad_norm": 0.5553969144821167,
      "learning_rate": 1.8e-05,
      "loss": 0.0019,
      "step": 96000
    },
    {
      "epoch": 5.120533333333333,
      "grad_norm": 0.037404220551252365,
      "learning_rate": 1.7996666666666667e-05,
      "loss": 0.0019,
      "step": 96010
    },
    {
      "epoch": 5.121066666666667,
      "grad_norm": 0.14702574908733368,
      "learning_rate": 1.7993333333333333e-05,
      "loss": 0.0027,
      "step": 96020
    },
    {
      "epoch": 5.1216,
      "grad_norm": 0.31829866766929626,
      "learning_rate": 1.7990000000000002e-05,
      "loss": 0.0029,
      "step": 96030
    },
    {
      "epoch": 5.122133333333333,
      "grad_norm": 0.3495548665523529,
      "learning_rate": 1.798666666666667e-05,
      "loss": 0.0022,
      "step": 96040
    },
    {
      "epoch": 5.1226666666666665,
      "grad_norm": 0.03542506322264671,
      "learning_rate": 1.7983333333333335e-05,
      "loss": 0.0023,
      "step": 96050
    },
    {
      "epoch": 5.1232,
      "grad_norm": 0.3796501159667969,
      "learning_rate": 1.798e-05,
      "loss": 0.0015,
      "step": 96060
    },
    {
      "epoch": 5.123733333333333,
      "grad_norm": 0.28320181369781494,
      "learning_rate": 1.7976666666666667e-05,
      "loss": 0.0015,
      "step": 96070
    },
    {
      "epoch": 5.124266666666666,
      "grad_norm": 0.09120971709489822,
      "learning_rate": 1.7973333333333333e-05,
      "loss": 0.0013,
      "step": 96080
    },
    {
      "epoch": 5.1248,
      "grad_norm": 0.6086666584014893,
      "learning_rate": 1.797e-05,
      "loss": 0.0025,
      "step": 96090
    },
    {
      "epoch": 5.125333333333334,
      "grad_norm": 0.06948862224817276,
      "learning_rate": 1.796666666666667e-05,
      "loss": 0.002,
      "step": 96100
    },
    {
      "epoch": 5.125866666666667,
      "grad_norm": 0.26211997866630554,
      "learning_rate": 1.7963333333333335e-05,
      "loss": 0.0018,
      "step": 96110
    },
    {
      "epoch": 5.1264,
      "grad_norm": 0.28604426980018616,
      "learning_rate": 1.796e-05,
      "loss": 0.0018,
      "step": 96120
    },
    {
      "epoch": 5.126933333333334,
      "grad_norm": 0.6977871656417847,
      "learning_rate": 1.795666666666667e-05,
      "loss": 0.002,
      "step": 96130
    },
    {
      "epoch": 5.127466666666667,
      "grad_norm": 0.46983763575553894,
      "learning_rate": 1.7953333333333333e-05,
      "loss": 0.0019,
      "step": 96140
    },
    {
      "epoch": 5.128,
      "grad_norm": 0.12232974916696548,
      "learning_rate": 1.795e-05,
      "loss": 0.0018,
      "step": 96150
    },
    {
      "epoch": 5.128533333333333,
      "grad_norm": 0.1923089474439621,
      "learning_rate": 1.794666666666667e-05,
      "loss": 0.0014,
      "step": 96160
    },
    {
      "epoch": 5.129066666666667,
      "grad_norm": 0.06176948547363281,
      "learning_rate": 1.7943333333333335e-05,
      "loss": 0.0017,
      "step": 96170
    },
    {
      "epoch": 5.1296,
      "grad_norm": 0.10269346088171005,
      "learning_rate": 1.794e-05,
      "loss": 0.0014,
      "step": 96180
    },
    {
      "epoch": 5.130133333333333,
      "grad_norm": 0.5207094550132751,
      "learning_rate": 1.7936666666666667e-05,
      "loss": 0.0013,
      "step": 96190
    },
    {
      "epoch": 5.1306666666666665,
      "grad_norm": 0.12161467224359512,
      "learning_rate": 1.7933333333333337e-05,
      "loss": 0.0016,
      "step": 96200
    },
    {
      "epoch": 5.1312,
      "grad_norm": 0.11104605346918106,
      "learning_rate": 1.793e-05,
      "loss": 0.0016,
      "step": 96210
    },
    {
      "epoch": 5.131733333333333,
      "grad_norm": 0.4357980489730835,
      "learning_rate": 1.7926666666666666e-05,
      "loss": 0.0015,
      "step": 96220
    },
    {
      "epoch": 5.132266666666666,
      "grad_norm": 0.08588255941867828,
      "learning_rate": 1.7923333333333335e-05,
      "loss": 0.0014,
      "step": 96230
    },
    {
      "epoch": 5.1328,
      "grad_norm": 0.19946810603141785,
      "learning_rate": 1.792e-05,
      "loss": 0.0016,
      "step": 96240
    },
    {
      "epoch": 5.133333333333334,
      "grad_norm": 0.02952655777335167,
      "learning_rate": 1.7916666666666667e-05,
      "loss": 0.0019,
      "step": 96250
    },
    {
      "epoch": 5.133866666666667,
      "grad_norm": 0.08225548267364502,
      "learning_rate": 1.7913333333333333e-05,
      "loss": 0.0017,
      "step": 96260
    },
    {
      "epoch": 5.1344,
      "grad_norm": 0.36259329319000244,
      "learning_rate": 1.7910000000000003e-05,
      "loss": 0.0015,
      "step": 96270
    },
    {
      "epoch": 5.134933333333334,
      "grad_norm": 0.04666811227798462,
      "learning_rate": 1.790666666666667e-05,
      "loss": 0.0017,
      "step": 96280
    },
    {
      "epoch": 5.135466666666667,
      "grad_norm": 0.5547428727149963,
      "learning_rate": 1.7903333333333332e-05,
      "loss": 0.0023,
      "step": 96290
    },
    {
      "epoch": 5.136,
      "grad_norm": 0.12476935982704163,
      "learning_rate": 1.79e-05,
      "loss": 0.0015,
      "step": 96300
    },
    {
      "epoch": 5.136533333333333,
      "grad_norm": 0.4156666398048401,
      "learning_rate": 1.7896666666666668e-05,
      "loss": 0.0021,
      "step": 96310
    },
    {
      "epoch": 5.137066666666667,
      "grad_norm": 0.2998444437980652,
      "learning_rate": 1.7893333333333334e-05,
      "loss": 0.0017,
      "step": 96320
    },
    {
      "epoch": 5.1376,
      "grad_norm": 0.5801117420196533,
      "learning_rate": 1.789e-05,
      "loss": 0.002,
      "step": 96330
    },
    {
      "epoch": 5.138133333333333,
      "grad_norm": 0.34060347080230713,
      "learning_rate": 1.788666666666667e-05,
      "loss": 0.0017,
      "step": 96340
    },
    {
      "epoch": 5.1386666666666665,
      "grad_norm": 0.6360801458358765,
      "learning_rate": 1.7883333333333335e-05,
      "loss": 0.002,
      "step": 96350
    },
    {
      "epoch": 5.1392,
      "grad_norm": 0.4004460573196411,
      "learning_rate": 1.7879999999999998e-05,
      "loss": 0.0018,
      "step": 96360
    },
    {
      "epoch": 5.139733333333333,
      "grad_norm": 0.07037493586540222,
      "learning_rate": 1.7876666666666668e-05,
      "loss": 0.0019,
      "step": 96370
    },
    {
      "epoch": 5.140266666666666,
      "grad_norm": 0.3301584720611572,
      "learning_rate": 1.7873333333333334e-05,
      "loss": 0.0014,
      "step": 96380
    },
    {
      "epoch": 5.1408,
      "grad_norm": 0.5622278451919556,
      "learning_rate": 1.787e-05,
      "loss": 0.0029,
      "step": 96390
    },
    {
      "epoch": 5.141333333333334,
      "grad_norm": 0.3666544258594513,
      "learning_rate": 1.7866666666666666e-05,
      "loss": 0.0017,
      "step": 96400
    },
    {
      "epoch": 5.141866666666667,
      "grad_norm": 0.2885812222957611,
      "learning_rate": 1.7863333333333336e-05,
      "loss": 0.0018,
      "step": 96410
    },
    {
      "epoch": 5.1424,
      "grad_norm": 0.5170178413391113,
      "learning_rate": 1.7860000000000002e-05,
      "loss": 0.0018,
      "step": 96420
    },
    {
      "epoch": 5.142933333333334,
      "grad_norm": 0.1284075230360031,
      "learning_rate": 1.7856666666666668e-05,
      "loss": 0.0031,
      "step": 96430
    },
    {
      "epoch": 5.143466666666667,
      "grad_norm": 0.20887932181358337,
      "learning_rate": 1.7853333333333334e-05,
      "loss": 0.0024,
      "step": 96440
    },
    {
      "epoch": 5.144,
      "grad_norm": 0.14645832777023315,
      "learning_rate": 1.785e-05,
      "loss": 0.002,
      "step": 96450
    },
    {
      "epoch": 5.144533333333333,
      "grad_norm": 0.13436958193778992,
      "learning_rate": 1.7846666666666666e-05,
      "loss": 0.0018,
      "step": 96460
    },
    {
      "epoch": 5.145066666666667,
      "grad_norm": 0.23275591433048248,
      "learning_rate": 1.7843333333333332e-05,
      "loss": 0.002,
      "step": 96470
    },
    {
      "epoch": 5.1456,
      "grad_norm": 0.5394279360771179,
      "learning_rate": 1.7840000000000002e-05,
      "loss": 0.0016,
      "step": 96480
    },
    {
      "epoch": 5.146133333333333,
      "grad_norm": 0.2601754367351532,
      "learning_rate": 1.7836666666666668e-05,
      "loss": 0.0019,
      "step": 96490
    },
    {
      "epoch": 5.1466666666666665,
      "grad_norm": 0.16873261332511902,
      "learning_rate": 1.7833333333333334e-05,
      "loss": 0.0018,
      "step": 96500
    },
    {
      "epoch": 5.1472,
      "grad_norm": 0.31189700961112976,
      "learning_rate": 1.783e-05,
      "loss": 0.0018,
      "step": 96510
    },
    {
      "epoch": 5.147733333333333,
      "grad_norm": 0.17338496446609497,
      "learning_rate": 1.7826666666666667e-05,
      "loss": 0.0019,
      "step": 96520
    },
    {
      "epoch": 5.148266666666666,
      "grad_norm": 0.170536071062088,
      "learning_rate": 1.7823333333333333e-05,
      "loss": 0.0015,
      "step": 96530
    },
    {
      "epoch": 5.1488,
      "grad_norm": 0.31510767340660095,
      "learning_rate": 1.7820000000000002e-05,
      "loss": 0.0013,
      "step": 96540
    },
    {
      "epoch": 5.149333333333334,
      "grad_norm": 0.3704472780227661,
      "learning_rate": 1.781666666666667e-05,
      "loss": 0.0033,
      "step": 96550
    },
    {
      "epoch": 5.149866666666667,
      "grad_norm": 0.5338865518569946,
      "learning_rate": 1.7813333333333334e-05,
      "loss": 0.0015,
      "step": 96560
    },
    {
      "epoch": 5.1504,
      "grad_norm": 0.12004578858613968,
      "learning_rate": 1.781e-05,
      "loss": 0.002,
      "step": 96570
    },
    {
      "epoch": 5.150933333333334,
      "grad_norm": 0.43827125430107117,
      "learning_rate": 1.780666666666667e-05,
      "loss": 0.0025,
      "step": 96580
    },
    {
      "epoch": 5.151466666666667,
      "grad_norm": 0.09705673903226852,
      "learning_rate": 1.7803333333333333e-05,
      "loss": 0.0014,
      "step": 96590
    },
    {
      "epoch": 5.152,
      "grad_norm": 0.02497769333422184,
      "learning_rate": 1.78e-05,
      "loss": 0.0014,
      "step": 96600
    },
    {
      "epoch": 5.152533333333333,
      "grad_norm": 0.5330586433410645,
      "learning_rate": 1.779666666666667e-05,
      "loss": 0.0027,
      "step": 96610
    },
    {
      "epoch": 5.153066666666667,
      "grad_norm": 0.2571626305580139,
      "learning_rate": 1.7793333333333335e-05,
      "loss": 0.0014,
      "step": 96620
    },
    {
      "epoch": 5.1536,
      "grad_norm": 0.2045077085494995,
      "learning_rate": 1.779e-05,
      "loss": 0.0024,
      "step": 96630
    },
    {
      "epoch": 5.154133333333333,
      "grad_norm": 0.2408694475889206,
      "learning_rate": 1.7786666666666667e-05,
      "loss": 0.0014,
      "step": 96640
    },
    {
      "epoch": 5.1546666666666665,
      "grad_norm": 0.3641761243343353,
      "learning_rate": 1.7783333333333336e-05,
      "loss": 0.0019,
      "step": 96650
    },
    {
      "epoch": 5.1552,
      "grad_norm": 0.3167808949947357,
      "learning_rate": 1.7780000000000003e-05,
      "loss": 0.0012,
      "step": 96660
    },
    {
      "epoch": 5.155733333333333,
      "grad_norm": 0.43395718932151794,
      "learning_rate": 1.7776666666666665e-05,
      "loss": 0.0024,
      "step": 96670
    },
    {
      "epoch": 5.156266666666666,
      "grad_norm": 0.25295838713645935,
      "learning_rate": 1.7773333333333335e-05,
      "loss": 0.0019,
      "step": 96680
    },
    {
      "epoch": 5.1568,
      "grad_norm": 0.04131104052066803,
      "learning_rate": 1.777e-05,
      "loss": 0.0022,
      "step": 96690
    },
    {
      "epoch": 5.157333333333334,
      "grad_norm": 0.05348798260092735,
      "learning_rate": 1.7766666666666667e-05,
      "loss": 0.0024,
      "step": 96700
    },
    {
      "epoch": 5.157866666666667,
      "grad_norm": 0.1829751431941986,
      "learning_rate": 1.7763333333333333e-05,
      "loss": 0.0017,
      "step": 96710
    },
    {
      "epoch": 5.1584,
      "grad_norm": 0.15100911259651184,
      "learning_rate": 1.7760000000000003e-05,
      "loss": 0.0027,
      "step": 96720
    },
    {
      "epoch": 5.158933333333334,
      "grad_norm": 0.18449345231056213,
      "learning_rate": 1.775666666666667e-05,
      "loss": 0.0018,
      "step": 96730
    },
    {
      "epoch": 5.159466666666667,
      "grad_norm": 0.2301848828792572,
      "learning_rate": 1.775333333333333e-05,
      "loss": 0.0019,
      "step": 96740
    },
    {
      "epoch": 5.16,
      "grad_norm": 0.045936357229948044,
      "learning_rate": 1.775e-05,
      "loss": 0.002,
      "step": 96750
    },
    {
      "epoch": 5.160533333333333,
      "grad_norm": 0.17582745850086212,
      "learning_rate": 1.7746666666666667e-05,
      "loss": 0.0017,
      "step": 96760
    },
    {
      "epoch": 5.161066666666667,
      "grad_norm": 0.4371306598186493,
      "learning_rate": 1.7743333333333333e-05,
      "loss": 0.0018,
      "step": 96770
    },
    {
      "epoch": 5.1616,
      "grad_norm": 0.28672143816947937,
      "learning_rate": 1.774e-05,
      "loss": 0.0014,
      "step": 96780
    },
    {
      "epoch": 5.162133333333333,
      "grad_norm": 0.05013330653309822,
      "learning_rate": 1.773666666666667e-05,
      "loss": 0.0021,
      "step": 96790
    },
    {
      "epoch": 5.1626666666666665,
      "grad_norm": 0.2689807116985321,
      "learning_rate": 1.7733333333333335e-05,
      "loss": 0.002,
      "step": 96800
    },
    {
      "epoch": 5.1632,
      "grad_norm": 0.11988826096057892,
      "learning_rate": 1.773e-05,
      "loss": 0.0017,
      "step": 96810
    },
    {
      "epoch": 5.163733333333333,
      "grad_norm": 0.1999940723180771,
      "learning_rate": 1.7726666666666667e-05,
      "loss": 0.0017,
      "step": 96820
    },
    {
      "epoch": 5.164266666666666,
      "grad_norm": 0.06375575065612793,
      "learning_rate": 1.7723333333333334e-05,
      "loss": 0.0018,
      "step": 96830
    },
    {
      "epoch": 5.1648,
      "grad_norm": 0.22819693386554718,
      "learning_rate": 1.772e-05,
      "loss": 0.0022,
      "step": 96840
    },
    {
      "epoch": 5.165333333333333,
      "grad_norm": 0.26125532388687134,
      "learning_rate": 1.7716666666666666e-05,
      "loss": 0.0018,
      "step": 96850
    },
    {
      "epoch": 5.165866666666667,
      "grad_norm": 0.06377588212490082,
      "learning_rate": 1.7713333333333335e-05,
      "loss": 0.0016,
      "step": 96860
    },
    {
      "epoch": 5.1664,
      "grad_norm": 0.16070763766765594,
      "learning_rate": 1.771e-05,
      "loss": 0.0013,
      "step": 96870
    },
    {
      "epoch": 5.166933333333334,
      "grad_norm": 0.32162174582481384,
      "learning_rate": 1.7706666666666668e-05,
      "loss": 0.0016,
      "step": 96880
    },
    {
      "epoch": 5.167466666666667,
      "grad_norm": 0.1797507256269455,
      "learning_rate": 1.7703333333333334e-05,
      "loss": 0.0018,
      "step": 96890
    },
    {
      "epoch": 5.168,
      "grad_norm": 0.43701234459877014,
      "learning_rate": 1.77e-05,
      "loss": 0.0017,
      "step": 96900
    },
    {
      "epoch": 5.168533333333333,
      "grad_norm": 0.15536698698997498,
      "learning_rate": 1.7696666666666666e-05,
      "loss": 0.0017,
      "step": 96910
    },
    {
      "epoch": 5.169066666666667,
      "grad_norm": 0.11595189571380615,
      "learning_rate": 1.7693333333333336e-05,
      "loss": 0.0014,
      "step": 96920
    },
    {
      "epoch": 5.1696,
      "grad_norm": 0.057748403400182724,
      "learning_rate": 1.7690000000000002e-05,
      "loss": 0.0022,
      "step": 96930
    },
    {
      "epoch": 5.170133333333333,
      "grad_norm": 0.20357540249824524,
      "learning_rate": 1.7686666666666668e-05,
      "loss": 0.0015,
      "step": 96940
    },
    {
      "epoch": 5.1706666666666665,
      "grad_norm": 0.11968090385198593,
      "learning_rate": 1.7683333333333334e-05,
      "loss": 0.0014,
      "step": 96950
    },
    {
      "epoch": 5.1712,
      "grad_norm": 0.2609194219112396,
      "learning_rate": 1.7680000000000004e-05,
      "loss": 0.0025,
      "step": 96960
    },
    {
      "epoch": 5.171733333333333,
      "grad_norm": 0.03810754045844078,
      "learning_rate": 1.7676666666666666e-05,
      "loss": 0.0023,
      "step": 96970
    },
    {
      "epoch": 5.172266666666666,
      "grad_norm": 0.15182168781757355,
      "learning_rate": 1.7673333333333332e-05,
      "loss": 0.0021,
      "step": 96980
    },
    {
      "epoch": 5.1728,
      "grad_norm": 0.04448046162724495,
      "learning_rate": 1.7670000000000002e-05,
      "loss": 0.0016,
      "step": 96990
    },
    {
      "epoch": 5.173333333333334,
      "grad_norm": 0.09204798936843872,
      "learning_rate": 1.7666666666666668e-05,
      "loss": 0.0024,
      "step": 97000
    },
    {
      "epoch": 5.173866666666667,
      "grad_norm": 0.38711467385292053,
      "learning_rate": 1.7663333333333334e-05,
      "loss": 0.0026,
      "step": 97010
    },
    {
      "epoch": 5.1744,
      "grad_norm": 0.06371121108531952,
      "learning_rate": 1.766e-05,
      "loss": 0.0021,
      "step": 97020
    },
    {
      "epoch": 5.174933333333334,
      "grad_norm": 0.20282694697380066,
      "learning_rate": 1.765666666666667e-05,
      "loss": 0.0034,
      "step": 97030
    },
    {
      "epoch": 5.175466666666667,
      "grad_norm": 0.6435310244560242,
      "learning_rate": 1.7653333333333333e-05,
      "loss": 0.0026,
      "step": 97040
    },
    {
      "epoch": 5.176,
      "grad_norm": 0.32041314244270325,
      "learning_rate": 1.765e-05,
      "loss": 0.0029,
      "step": 97050
    },
    {
      "epoch": 5.176533333333333,
      "grad_norm": 0.2925828695297241,
      "learning_rate": 1.7646666666666668e-05,
      "loss": 0.0018,
      "step": 97060
    },
    {
      "epoch": 5.177066666666667,
      "grad_norm": 0.4310528039932251,
      "learning_rate": 1.7643333333333334e-05,
      "loss": 0.0016,
      "step": 97070
    },
    {
      "epoch": 5.1776,
      "grad_norm": 0.4966282844543457,
      "learning_rate": 1.764e-05,
      "loss": 0.0014,
      "step": 97080
    },
    {
      "epoch": 5.178133333333333,
      "grad_norm": 0.17867088317871094,
      "learning_rate": 1.7636666666666667e-05,
      "loss": 0.0021,
      "step": 97090
    },
    {
      "epoch": 5.1786666666666665,
      "grad_norm": 0.06517209112644196,
      "learning_rate": 1.7633333333333336e-05,
      "loss": 0.0017,
      "step": 97100
    },
    {
      "epoch": 5.1792,
      "grad_norm": 0.2971659302711487,
      "learning_rate": 1.7630000000000002e-05,
      "loss": 0.0015,
      "step": 97110
    },
    {
      "epoch": 5.179733333333333,
      "grad_norm": 0.17639237642288208,
      "learning_rate": 1.7626666666666665e-05,
      "loss": 0.0015,
      "step": 97120
    },
    {
      "epoch": 5.180266666666666,
      "grad_norm": 0.20682740211486816,
      "learning_rate": 1.7623333333333335e-05,
      "loss": 0.0019,
      "step": 97130
    },
    {
      "epoch": 5.1808,
      "grad_norm": 0.18577446043491364,
      "learning_rate": 1.762e-05,
      "loss": 0.0014,
      "step": 97140
    },
    {
      "epoch": 5.181333333333333,
      "grad_norm": 0.30550527572631836,
      "learning_rate": 1.7616666666666667e-05,
      "loss": 0.0014,
      "step": 97150
    },
    {
      "epoch": 5.181866666666667,
      "grad_norm": 0.11344973742961884,
      "learning_rate": 1.7613333333333333e-05,
      "loss": 0.0019,
      "step": 97160
    },
    {
      "epoch": 5.1824,
      "grad_norm": 0.15806271135807037,
      "learning_rate": 1.7610000000000002e-05,
      "loss": 0.0026,
      "step": 97170
    },
    {
      "epoch": 5.182933333333334,
      "grad_norm": 0.06314446777105331,
      "learning_rate": 1.760666666666667e-05,
      "loss": 0.0028,
      "step": 97180
    },
    {
      "epoch": 5.183466666666667,
      "grad_norm": 0.06601326167583466,
      "learning_rate": 1.7603333333333335e-05,
      "loss": 0.0018,
      "step": 97190
    },
    {
      "epoch": 5.184,
      "grad_norm": 0.14730790257453918,
      "learning_rate": 1.76e-05,
      "loss": 0.0019,
      "step": 97200
    },
    {
      "epoch": 5.184533333333333,
      "grad_norm": 0.1194862499833107,
      "learning_rate": 1.7596666666666667e-05,
      "loss": 0.0017,
      "step": 97210
    },
    {
      "epoch": 5.185066666666667,
      "grad_norm": 0.32353630661964417,
      "learning_rate": 1.7593333333333333e-05,
      "loss": 0.0033,
      "step": 97220
    },
    {
      "epoch": 5.1856,
      "grad_norm": 0.029148118570446968,
      "learning_rate": 1.759e-05,
      "loss": 0.0021,
      "step": 97230
    },
    {
      "epoch": 5.186133333333333,
      "grad_norm": 0.43606898188591003,
      "learning_rate": 1.758666666666667e-05,
      "loss": 0.0021,
      "step": 97240
    },
    {
      "epoch": 5.1866666666666665,
      "grad_norm": 0.5257153511047363,
      "learning_rate": 1.7583333333333335e-05,
      "loss": 0.0015,
      "step": 97250
    },
    {
      "epoch": 5.1872,
      "grad_norm": 0.16062608361244202,
      "learning_rate": 1.758e-05,
      "loss": 0.003,
      "step": 97260
    },
    {
      "epoch": 5.187733333333333,
      "grad_norm": 0.3472042977809906,
      "learning_rate": 1.7576666666666667e-05,
      "loss": 0.0027,
      "step": 97270
    },
    {
      "epoch": 5.188266666666666,
      "grad_norm": 0.32647427916526794,
      "learning_rate": 1.7573333333333333e-05,
      "loss": 0.0024,
      "step": 97280
    },
    {
      "epoch": 5.1888,
      "grad_norm": 0.044917382299900055,
      "learning_rate": 1.757e-05,
      "loss": 0.002,
      "step": 97290
    },
    {
      "epoch": 5.189333333333333,
      "grad_norm": 0.19091914594173431,
      "learning_rate": 1.756666666666667e-05,
      "loss": 0.0026,
      "step": 97300
    },
    {
      "epoch": 5.189866666666667,
      "grad_norm": 0.2760079503059387,
      "learning_rate": 1.7563333333333335e-05,
      "loss": 0.0019,
      "step": 97310
    },
    {
      "epoch": 5.1904,
      "grad_norm": 0.21606037020683289,
      "learning_rate": 1.756e-05,
      "loss": 0.002,
      "step": 97320
    },
    {
      "epoch": 5.190933333333334,
      "grad_norm": 0.5050573348999023,
      "learning_rate": 1.7556666666666667e-05,
      "loss": 0.0018,
      "step": 97330
    },
    {
      "epoch": 5.191466666666667,
      "grad_norm": 0.23655551671981812,
      "learning_rate": 1.7553333333333337e-05,
      "loss": 0.0015,
      "step": 97340
    },
    {
      "epoch": 5.192,
      "grad_norm": 0.21072480082511902,
      "learning_rate": 1.755e-05,
      "loss": 0.0017,
      "step": 97350
    },
    {
      "epoch": 5.1925333333333334,
      "grad_norm": 0.03859446570277214,
      "learning_rate": 1.7546666666666666e-05,
      "loss": 0.0018,
      "step": 97360
    },
    {
      "epoch": 5.193066666666667,
      "grad_norm": 0.14353851974010468,
      "learning_rate": 1.7543333333333335e-05,
      "loss": 0.0021,
      "step": 97370
    },
    {
      "epoch": 5.1936,
      "grad_norm": 0.20654571056365967,
      "learning_rate": 1.754e-05,
      "loss": 0.0016,
      "step": 97380
    },
    {
      "epoch": 5.194133333333333,
      "grad_norm": 0.3180275559425354,
      "learning_rate": 1.7536666666666668e-05,
      "loss": 0.0023,
      "step": 97390
    },
    {
      "epoch": 5.1946666666666665,
      "grad_norm": 0.7584888935089111,
      "learning_rate": 1.7533333333333334e-05,
      "loss": 0.0033,
      "step": 97400
    },
    {
      "epoch": 5.1952,
      "grad_norm": 0.2347370833158493,
      "learning_rate": 1.7530000000000003e-05,
      "loss": 0.0018,
      "step": 97410
    },
    {
      "epoch": 5.195733333333333,
      "grad_norm": 0.40301188826560974,
      "learning_rate": 1.7526666666666666e-05,
      "loss": 0.0013,
      "step": 97420
    },
    {
      "epoch": 5.196266666666666,
      "grad_norm": 0.5469838976860046,
      "learning_rate": 1.7523333333333332e-05,
      "loss": 0.0027,
      "step": 97430
    },
    {
      "epoch": 5.1968,
      "grad_norm": 0.2030014544725418,
      "learning_rate": 1.752e-05,
      "loss": 0.0022,
      "step": 97440
    },
    {
      "epoch": 5.197333333333333,
      "grad_norm": 0.1469263732433319,
      "learning_rate": 1.7516666666666668e-05,
      "loss": 0.0021,
      "step": 97450
    },
    {
      "epoch": 5.197866666666667,
      "grad_norm": 0.040760353207588196,
      "learning_rate": 1.7513333333333334e-05,
      "loss": 0.002,
      "step": 97460
    },
    {
      "epoch": 5.1984,
      "grad_norm": 0.5401182770729065,
      "learning_rate": 1.751e-05,
      "loss": 0.0018,
      "step": 97470
    },
    {
      "epoch": 5.198933333333334,
      "grad_norm": 0.4004930257797241,
      "learning_rate": 1.750666666666667e-05,
      "loss": 0.0016,
      "step": 97480
    },
    {
      "epoch": 5.199466666666667,
      "grad_norm": 0.06349323689937592,
      "learning_rate": 1.7503333333333336e-05,
      "loss": 0.0013,
      "step": 97490
    },
    {
      "epoch": 5.2,
      "grad_norm": 0.26975807547569275,
      "learning_rate": 1.75e-05,
      "loss": 0.0023,
      "step": 97500
    },
    {
      "epoch": 5.2005333333333335,
      "grad_norm": 0.2686978280544281,
      "learning_rate": 1.7496666666666668e-05,
      "loss": 0.0018,
      "step": 97510
    },
    {
      "epoch": 5.201066666666667,
      "grad_norm": 0.4350425601005554,
      "learning_rate": 1.7493333333333334e-05,
      "loss": 0.0015,
      "step": 97520
    },
    {
      "epoch": 5.2016,
      "grad_norm": 0.2676027715206146,
      "learning_rate": 1.749e-05,
      "loss": 0.0022,
      "step": 97530
    },
    {
      "epoch": 5.202133333333333,
      "grad_norm": 0.40282559394836426,
      "learning_rate": 1.7486666666666666e-05,
      "loss": 0.0027,
      "step": 97540
    },
    {
      "epoch": 5.2026666666666666,
      "grad_norm": 0.34647101163864136,
      "learning_rate": 1.7483333333333336e-05,
      "loss": 0.0021,
      "step": 97550
    },
    {
      "epoch": 5.2032,
      "grad_norm": 0.09103544801473618,
      "learning_rate": 1.7480000000000002e-05,
      "loss": 0.0018,
      "step": 97560
    },
    {
      "epoch": 5.203733333333333,
      "grad_norm": 0.20542177557945251,
      "learning_rate": 1.7476666666666665e-05,
      "loss": 0.0019,
      "step": 97570
    },
    {
      "epoch": 5.204266666666666,
      "grad_norm": 0.2134750932455063,
      "learning_rate": 1.7473333333333334e-05,
      "loss": 0.0023,
      "step": 97580
    },
    {
      "epoch": 5.2048,
      "grad_norm": 0.38231360912323,
      "learning_rate": 1.747e-05,
      "loss": 0.0018,
      "step": 97590
    },
    {
      "epoch": 5.205333333333333,
      "grad_norm": 0.5395064949989319,
      "learning_rate": 1.7466666666666667e-05,
      "loss": 0.0018,
      "step": 97600
    },
    {
      "epoch": 5.205866666666667,
      "grad_norm": 0.05988067388534546,
      "learning_rate": 1.7463333333333333e-05,
      "loss": 0.0019,
      "step": 97610
    },
    {
      "epoch": 5.2064,
      "grad_norm": 0.2717762887477875,
      "learning_rate": 1.7460000000000002e-05,
      "loss": 0.0023,
      "step": 97620
    },
    {
      "epoch": 5.206933333333334,
      "grad_norm": 0.41460803151130676,
      "learning_rate": 1.745666666666667e-05,
      "loss": 0.0016,
      "step": 97630
    },
    {
      "epoch": 5.207466666666667,
      "grad_norm": 0.2051193118095398,
      "learning_rate": 1.7453333333333335e-05,
      "loss": 0.0016,
      "step": 97640
    },
    {
      "epoch": 5.208,
      "grad_norm": 0.2322646826505661,
      "learning_rate": 1.745e-05,
      "loss": 0.0018,
      "step": 97650
    },
    {
      "epoch": 5.2085333333333335,
      "grad_norm": 0.14370544254779816,
      "learning_rate": 1.7446666666666667e-05,
      "loss": 0.0016,
      "step": 97660
    },
    {
      "epoch": 5.209066666666667,
      "grad_norm": 0.23244959115982056,
      "learning_rate": 1.7443333333333333e-05,
      "loss": 0.0022,
      "step": 97670
    },
    {
      "epoch": 5.2096,
      "grad_norm": 0.060684241354465485,
      "learning_rate": 1.7440000000000002e-05,
      "loss": 0.0022,
      "step": 97680
    },
    {
      "epoch": 5.210133333333333,
      "grad_norm": 0.1261770874261856,
      "learning_rate": 1.743666666666667e-05,
      "loss": 0.0012,
      "step": 97690
    },
    {
      "epoch": 5.210666666666667,
      "grad_norm": 0.08095148950815201,
      "learning_rate": 1.7433333333333335e-05,
      "loss": 0.0024,
      "step": 97700
    },
    {
      "epoch": 5.2112,
      "grad_norm": 0.28869956731796265,
      "learning_rate": 1.743e-05,
      "loss": 0.0026,
      "step": 97710
    },
    {
      "epoch": 5.211733333333333,
      "grad_norm": 0.12145573645830154,
      "learning_rate": 1.7426666666666667e-05,
      "loss": 0.0023,
      "step": 97720
    },
    {
      "epoch": 5.212266666666666,
      "grad_norm": 0.1985689252614975,
      "learning_rate": 1.7423333333333333e-05,
      "loss": 0.002,
      "step": 97730
    },
    {
      "epoch": 5.2128,
      "grad_norm": 0.07935827225446701,
      "learning_rate": 1.742e-05,
      "loss": 0.003,
      "step": 97740
    },
    {
      "epoch": 5.213333333333333,
      "grad_norm": 0.2698056697845459,
      "learning_rate": 1.741666666666667e-05,
      "loss": 0.0013,
      "step": 97750
    },
    {
      "epoch": 5.213866666666667,
      "grad_norm": 0.14366614818572998,
      "learning_rate": 1.7413333333333335e-05,
      "loss": 0.0014,
      "step": 97760
    },
    {
      "epoch": 5.2144,
      "grad_norm": 0.20291469991207123,
      "learning_rate": 1.741e-05,
      "loss": 0.0018,
      "step": 97770
    },
    {
      "epoch": 5.214933333333334,
      "grad_norm": 0.2356894165277481,
      "learning_rate": 1.7406666666666667e-05,
      "loss": 0.0022,
      "step": 97780
    },
    {
      "epoch": 5.215466666666667,
      "grad_norm": 0.15122321248054504,
      "learning_rate": 1.7403333333333337e-05,
      "loss": 0.0024,
      "step": 97790
    },
    {
      "epoch": 5.216,
      "grad_norm": 0.20343030989170074,
      "learning_rate": 1.74e-05,
      "loss": 0.0015,
      "step": 97800
    },
    {
      "epoch": 5.2165333333333335,
      "grad_norm": 0.21303124725818634,
      "learning_rate": 1.7396666666666666e-05,
      "loss": 0.0022,
      "step": 97810
    },
    {
      "epoch": 5.217066666666667,
      "grad_norm": 0.2645636200904846,
      "learning_rate": 1.7393333333333335e-05,
      "loss": 0.0018,
      "step": 97820
    },
    {
      "epoch": 5.2176,
      "grad_norm": 0.2914566695690155,
      "learning_rate": 1.739e-05,
      "loss": 0.0025,
      "step": 97830
    },
    {
      "epoch": 5.218133333333333,
      "grad_norm": 0.11618044227361679,
      "learning_rate": 1.7386666666666667e-05,
      "loss": 0.0017,
      "step": 97840
    },
    {
      "epoch": 5.218666666666667,
      "grad_norm": 0.3504254221916199,
      "learning_rate": 1.7383333333333333e-05,
      "loss": 0.002,
      "step": 97850
    },
    {
      "epoch": 5.2192,
      "grad_norm": 0.21796657145023346,
      "learning_rate": 1.7380000000000003e-05,
      "loss": 0.0022,
      "step": 97860
    },
    {
      "epoch": 5.219733333333333,
      "grad_norm": 0.3458113670349121,
      "learning_rate": 1.737666666666667e-05,
      "loss": 0.0021,
      "step": 97870
    },
    {
      "epoch": 5.220266666666666,
      "grad_norm": 0.39025238156318665,
      "learning_rate": 1.7373333333333332e-05,
      "loss": 0.0014,
      "step": 97880
    },
    {
      "epoch": 5.2208,
      "grad_norm": 0.14322081208229065,
      "learning_rate": 1.737e-05,
      "loss": 0.0016,
      "step": 97890
    },
    {
      "epoch": 5.221333333333333,
      "grad_norm": 0.29154470562934875,
      "learning_rate": 1.7366666666666668e-05,
      "loss": 0.0022,
      "step": 97900
    },
    {
      "epoch": 5.221866666666667,
      "grad_norm": 0.31883522868156433,
      "learning_rate": 1.7363333333333334e-05,
      "loss": 0.0015,
      "step": 97910
    },
    {
      "epoch": 5.2224,
      "grad_norm": 0.14316798746585846,
      "learning_rate": 1.736e-05,
      "loss": 0.0015,
      "step": 97920
    },
    {
      "epoch": 5.222933333333334,
      "grad_norm": 0.22452141344547272,
      "learning_rate": 1.735666666666667e-05,
      "loss": 0.0014,
      "step": 97930
    },
    {
      "epoch": 5.223466666666667,
      "grad_norm": 0.2736423909664154,
      "learning_rate": 1.7353333333333335e-05,
      "loss": 0.0018,
      "step": 97940
    },
    {
      "epoch": 5.224,
      "grad_norm": 0.2854631543159485,
      "learning_rate": 1.7349999999999998e-05,
      "loss": 0.0024,
      "step": 97950
    },
    {
      "epoch": 5.2245333333333335,
      "grad_norm": 0.12466201931238174,
      "learning_rate": 1.7346666666666668e-05,
      "loss": 0.0019,
      "step": 97960
    },
    {
      "epoch": 5.225066666666667,
      "grad_norm": 0.2112201601266861,
      "learning_rate": 1.7343333333333334e-05,
      "loss": 0.0017,
      "step": 97970
    },
    {
      "epoch": 5.2256,
      "grad_norm": 0.18224801123142242,
      "learning_rate": 1.734e-05,
      "loss": 0.0014,
      "step": 97980
    },
    {
      "epoch": 5.226133333333333,
      "grad_norm": 0.18994393944740295,
      "learning_rate": 1.7336666666666666e-05,
      "loss": 0.0014,
      "step": 97990
    },
    {
      "epoch": 5.226666666666667,
      "grad_norm": 0.3709832727909088,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 0.0021,
      "step": 98000
    },
    {
      "epoch": 5.2272,
      "grad_norm": 0.17358514666557312,
      "learning_rate": 1.7330000000000002e-05,
      "loss": 0.0034,
      "step": 98010
    },
    {
      "epoch": 5.227733333333333,
      "grad_norm": 0.046175818890333176,
      "learning_rate": 1.7326666666666668e-05,
      "loss": 0.0018,
      "step": 98020
    },
    {
      "epoch": 5.228266666666666,
      "grad_norm": 0.08151906728744507,
      "learning_rate": 1.7323333333333334e-05,
      "loss": 0.0021,
      "step": 98030
    },
    {
      "epoch": 5.2288,
      "grad_norm": 0.3999989926815033,
      "learning_rate": 1.732e-05,
      "loss": 0.0017,
      "step": 98040
    },
    {
      "epoch": 5.229333333333333,
      "grad_norm": 0.3112231492996216,
      "learning_rate": 1.7316666666666666e-05,
      "loss": 0.0017,
      "step": 98050
    },
    {
      "epoch": 5.229866666666666,
      "grad_norm": 0.1314675360918045,
      "learning_rate": 1.7313333333333336e-05,
      "loss": 0.0017,
      "step": 98060
    },
    {
      "epoch": 5.2304,
      "grad_norm": 0.28731876611709595,
      "learning_rate": 1.7310000000000002e-05,
      "loss": 0.0015,
      "step": 98070
    },
    {
      "epoch": 5.230933333333334,
      "grad_norm": 0.15605291724205017,
      "learning_rate": 1.7306666666666668e-05,
      "loss": 0.0016,
      "step": 98080
    },
    {
      "epoch": 5.231466666666667,
      "grad_norm": 0.15288197994232178,
      "learning_rate": 1.7303333333333334e-05,
      "loss": 0.0018,
      "step": 98090
    },
    {
      "epoch": 5.232,
      "grad_norm": 0.17757658660411835,
      "learning_rate": 1.73e-05,
      "loss": 0.0014,
      "step": 98100
    },
    {
      "epoch": 5.2325333333333335,
      "grad_norm": 0.3230740427970886,
      "learning_rate": 1.7296666666666667e-05,
      "loss": 0.0019,
      "step": 98110
    },
    {
      "epoch": 5.233066666666667,
      "grad_norm": 0.4890252947807312,
      "learning_rate": 1.7293333333333333e-05,
      "loss": 0.0021,
      "step": 98120
    },
    {
      "epoch": 5.2336,
      "grad_norm": 0.07867547869682312,
      "learning_rate": 1.7290000000000002e-05,
      "loss": 0.0015,
      "step": 98130
    },
    {
      "epoch": 5.234133333333333,
      "grad_norm": 0.4427005648612976,
      "learning_rate": 1.7286666666666668e-05,
      "loss": 0.0027,
      "step": 98140
    },
    {
      "epoch": 5.234666666666667,
      "grad_norm": 0.31199613213539124,
      "learning_rate": 1.7283333333333334e-05,
      "loss": 0.0022,
      "step": 98150
    },
    {
      "epoch": 5.2352,
      "grad_norm": 0.24048814177513123,
      "learning_rate": 1.728e-05,
      "loss": 0.0016,
      "step": 98160
    },
    {
      "epoch": 5.235733333333333,
      "grad_norm": 0.11878909170627594,
      "learning_rate": 1.727666666666667e-05,
      "loss": 0.0021,
      "step": 98170
    },
    {
      "epoch": 5.236266666666666,
      "grad_norm": 0.398337185382843,
      "learning_rate": 1.7273333333333333e-05,
      "loss": 0.0016,
      "step": 98180
    },
    {
      "epoch": 5.2368,
      "grad_norm": 0.4143572747707367,
      "learning_rate": 1.727e-05,
      "loss": 0.0016,
      "step": 98190
    },
    {
      "epoch": 5.237333333333333,
      "grad_norm": 0.17364594340324402,
      "learning_rate": 1.726666666666667e-05,
      "loss": 0.0018,
      "step": 98200
    },
    {
      "epoch": 5.237866666666667,
      "grad_norm": 0.06605694442987442,
      "learning_rate": 1.7263333333333335e-05,
      "loss": 0.0024,
      "step": 98210
    },
    {
      "epoch": 5.2384,
      "grad_norm": 0.06266235560178757,
      "learning_rate": 1.726e-05,
      "loss": 0.0017,
      "step": 98220
    },
    {
      "epoch": 5.238933333333334,
      "grad_norm": 0.20600178837776184,
      "learning_rate": 1.7256666666666667e-05,
      "loss": 0.0024,
      "step": 98230
    },
    {
      "epoch": 5.239466666666667,
      "grad_norm": 0.40597596764564514,
      "learning_rate": 1.7253333333333336e-05,
      "loss": 0.0018,
      "step": 98240
    },
    {
      "epoch": 5.24,
      "grad_norm": 0.5144214034080505,
      "learning_rate": 1.725e-05,
      "loss": 0.0016,
      "step": 98250
    },
    {
      "epoch": 5.2405333333333335,
      "grad_norm": 0.24432067573070526,
      "learning_rate": 1.7246666666666665e-05,
      "loss": 0.0023,
      "step": 98260
    },
    {
      "epoch": 5.241066666666667,
      "grad_norm": 0.09168941527605057,
      "learning_rate": 1.7243333333333335e-05,
      "loss": 0.0015,
      "step": 98270
    },
    {
      "epoch": 5.2416,
      "grad_norm": 0.24977032840251923,
      "learning_rate": 1.724e-05,
      "loss": 0.0015,
      "step": 98280
    },
    {
      "epoch": 5.242133333333333,
      "grad_norm": 0.17461644113063812,
      "learning_rate": 1.7236666666666667e-05,
      "loss": 0.0022,
      "step": 98290
    },
    {
      "epoch": 5.242666666666667,
      "grad_norm": 0.15562666952610016,
      "learning_rate": 1.7233333333333333e-05,
      "loss": 0.0024,
      "step": 98300
    },
    {
      "epoch": 5.2432,
      "grad_norm": 0.0792606770992279,
      "learning_rate": 1.7230000000000003e-05,
      "loss": 0.0024,
      "step": 98310
    },
    {
      "epoch": 5.243733333333333,
      "grad_norm": 0.09956784546375275,
      "learning_rate": 1.722666666666667e-05,
      "loss": 0.0021,
      "step": 98320
    },
    {
      "epoch": 5.244266666666666,
      "grad_norm": 0.5245110988616943,
      "learning_rate": 1.722333333333333e-05,
      "loss": 0.0018,
      "step": 98330
    },
    {
      "epoch": 5.2448,
      "grad_norm": 0.0349467433989048,
      "learning_rate": 1.722e-05,
      "loss": 0.0024,
      "step": 98340
    },
    {
      "epoch": 5.245333333333333,
      "grad_norm": 0.09283976256847382,
      "learning_rate": 1.7216666666666667e-05,
      "loss": 0.0015,
      "step": 98350
    },
    {
      "epoch": 5.245866666666666,
      "grad_norm": 0.21455428004264832,
      "learning_rate": 1.7213333333333333e-05,
      "loss": 0.0016,
      "step": 98360
    },
    {
      "epoch": 5.2464,
      "grad_norm": 0.1874098777770996,
      "learning_rate": 1.721e-05,
      "loss": 0.0026,
      "step": 98370
    },
    {
      "epoch": 5.246933333333334,
      "grad_norm": 0.30414825677871704,
      "learning_rate": 1.720666666666667e-05,
      "loss": 0.0027,
      "step": 98380
    },
    {
      "epoch": 5.247466666666667,
      "grad_norm": 0.11963264644145966,
      "learning_rate": 1.7203333333333335e-05,
      "loss": 0.0023,
      "step": 98390
    },
    {
      "epoch": 5.248,
      "grad_norm": 0.4048224687576294,
      "learning_rate": 1.7199999999999998e-05,
      "loss": 0.0029,
      "step": 98400
    },
    {
      "epoch": 5.2485333333333335,
      "grad_norm": 0.5222446322441101,
      "learning_rate": 1.7196666666666667e-05,
      "loss": 0.0025,
      "step": 98410
    },
    {
      "epoch": 5.249066666666667,
      "grad_norm": 0.3236723244190216,
      "learning_rate": 1.7193333333333334e-05,
      "loss": 0.0014,
      "step": 98420
    },
    {
      "epoch": 5.2496,
      "grad_norm": 0.14905081689357758,
      "learning_rate": 1.719e-05,
      "loss": 0.0021,
      "step": 98430
    },
    {
      "epoch": 5.250133333333333,
      "grad_norm": 0.06568963080644608,
      "learning_rate": 1.718666666666667e-05,
      "loss": 0.0021,
      "step": 98440
    },
    {
      "epoch": 5.250666666666667,
      "grad_norm": 0.13049104809761047,
      "learning_rate": 1.7183333333333335e-05,
      "loss": 0.0015,
      "step": 98450
    },
    {
      "epoch": 5.2512,
      "grad_norm": 0.22578708827495575,
      "learning_rate": 1.718e-05,
      "loss": 0.002,
      "step": 98460
    },
    {
      "epoch": 5.251733333333333,
      "grad_norm": 0.19861054420471191,
      "learning_rate": 1.7176666666666668e-05,
      "loss": 0.0015,
      "step": 98470
    },
    {
      "epoch": 5.252266666666666,
      "grad_norm": 0.11475870013237,
      "learning_rate": 1.7173333333333334e-05,
      "loss": 0.0017,
      "step": 98480
    },
    {
      "epoch": 5.2528,
      "grad_norm": 0.5099782347679138,
      "learning_rate": 1.717e-05,
      "loss": 0.0019,
      "step": 98490
    },
    {
      "epoch": 5.253333333333333,
      "grad_norm": 0.3301762640476227,
      "learning_rate": 1.7166666666666666e-05,
      "loss": 0.0028,
      "step": 98500
    },
    {
      "epoch": 5.253866666666667,
      "grad_norm": 0.5998961925506592,
      "learning_rate": 1.7163333333333336e-05,
      "loss": 0.0016,
      "step": 98510
    },
    {
      "epoch": 5.2544,
      "grad_norm": 0.08311739563941956,
      "learning_rate": 1.7160000000000002e-05,
      "loss": 0.0017,
      "step": 98520
    },
    {
      "epoch": 5.254933333333334,
      "grad_norm": 0.2519899606704712,
      "learning_rate": 1.7156666666666668e-05,
      "loss": 0.0016,
      "step": 98530
    },
    {
      "epoch": 5.255466666666667,
      "grad_norm": 0.0636090636253357,
      "learning_rate": 1.7153333333333334e-05,
      "loss": 0.0015,
      "step": 98540
    },
    {
      "epoch": 5.256,
      "grad_norm": 0.1246911957859993,
      "learning_rate": 1.7150000000000004e-05,
      "loss": 0.0024,
      "step": 98550
    },
    {
      "epoch": 5.2565333333333335,
      "grad_norm": 0.2341429442167282,
      "learning_rate": 1.7146666666666666e-05,
      "loss": 0.0016,
      "step": 98560
    },
    {
      "epoch": 5.257066666666667,
      "grad_norm": 0.2647469639778137,
      "learning_rate": 1.7143333333333332e-05,
      "loss": 0.0016,
      "step": 98570
    },
    {
      "epoch": 5.2576,
      "grad_norm": 0.32020220160484314,
      "learning_rate": 1.7140000000000002e-05,
      "loss": 0.0018,
      "step": 98580
    },
    {
      "epoch": 5.258133333333333,
      "grad_norm": 0.03989953175187111,
      "learning_rate": 1.7136666666666668e-05,
      "loss": 0.0022,
      "step": 98590
    },
    {
      "epoch": 5.258666666666667,
      "grad_norm": 0.18148471415042877,
      "learning_rate": 1.7133333333333334e-05,
      "loss": 0.0014,
      "step": 98600
    },
    {
      "epoch": 5.2592,
      "grad_norm": 0.4069231152534485,
      "learning_rate": 1.713e-05,
      "loss": 0.0017,
      "step": 98610
    },
    {
      "epoch": 5.259733333333333,
      "grad_norm": 0.35757341980934143,
      "learning_rate": 1.712666666666667e-05,
      "loss": 0.0029,
      "step": 98620
    },
    {
      "epoch": 5.260266666666666,
      "grad_norm": 0.27939367294311523,
      "learning_rate": 1.7123333333333333e-05,
      "loss": 0.0018,
      "step": 98630
    },
    {
      "epoch": 5.2608,
      "grad_norm": 0.17511816322803497,
      "learning_rate": 1.712e-05,
      "loss": 0.0018,
      "step": 98640
    },
    {
      "epoch": 5.261333333333333,
      "grad_norm": 0.14730429649353027,
      "learning_rate": 1.7116666666666668e-05,
      "loss": 0.0025,
      "step": 98650
    },
    {
      "epoch": 5.261866666666666,
      "grad_norm": 0.1492186188697815,
      "learning_rate": 1.7113333333333334e-05,
      "loss": 0.0017,
      "step": 98660
    },
    {
      "epoch": 5.2624,
      "grad_norm": 0.07348096370697021,
      "learning_rate": 1.711e-05,
      "loss": 0.0018,
      "step": 98670
    },
    {
      "epoch": 5.262933333333334,
      "grad_norm": 0.06356777995824814,
      "learning_rate": 1.7106666666666667e-05,
      "loss": 0.0012,
      "step": 98680
    },
    {
      "epoch": 5.263466666666667,
      "grad_norm": 0.14749708771705627,
      "learning_rate": 1.7103333333333336e-05,
      "loss": 0.002,
      "step": 98690
    },
    {
      "epoch": 5.264,
      "grad_norm": 0.25566864013671875,
      "learning_rate": 1.7100000000000002e-05,
      "loss": 0.0019,
      "step": 98700
    },
    {
      "epoch": 5.2645333333333335,
      "grad_norm": 0.4061015546321869,
      "learning_rate": 1.7096666666666665e-05,
      "loss": 0.0017,
      "step": 98710
    },
    {
      "epoch": 5.265066666666667,
      "grad_norm": 0.3410624861717224,
      "learning_rate": 1.7093333333333335e-05,
      "loss": 0.0019,
      "step": 98720
    },
    {
      "epoch": 5.2656,
      "grad_norm": 0.2685664892196655,
      "learning_rate": 1.709e-05,
      "loss": 0.0018,
      "step": 98730
    },
    {
      "epoch": 5.266133333333333,
      "grad_norm": 0.20937208831310272,
      "learning_rate": 1.7086666666666667e-05,
      "loss": 0.0019,
      "step": 98740
    },
    {
      "epoch": 5.266666666666667,
      "grad_norm": 0.3116509020328522,
      "learning_rate": 1.7083333333333333e-05,
      "loss": 0.002,
      "step": 98750
    },
    {
      "epoch": 5.2672,
      "grad_norm": 0.20765675604343414,
      "learning_rate": 1.7080000000000002e-05,
      "loss": 0.0021,
      "step": 98760
    },
    {
      "epoch": 5.267733333333333,
      "grad_norm": 0.07163602113723755,
      "learning_rate": 1.707666666666667e-05,
      "loss": 0.0019,
      "step": 98770
    },
    {
      "epoch": 5.268266666666666,
      "grad_norm": 0.18078108131885529,
      "learning_rate": 1.707333333333333e-05,
      "loss": 0.0015,
      "step": 98780
    },
    {
      "epoch": 5.2688,
      "grad_norm": 0.12322117388248444,
      "learning_rate": 1.707e-05,
      "loss": 0.0015,
      "step": 98790
    },
    {
      "epoch": 5.269333333333333,
      "grad_norm": 0.3278946876525879,
      "learning_rate": 1.7066666666666667e-05,
      "loss": 0.002,
      "step": 98800
    },
    {
      "epoch": 5.269866666666666,
      "grad_norm": 0.09080173075199127,
      "learning_rate": 1.7063333333333333e-05,
      "loss": 0.0012,
      "step": 98810
    },
    {
      "epoch": 5.2704,
      "grad_norm": 0.10482864826917648,
      "learning_rate": 1.706e-05,
      "loss": 0.0013,
      "step": 98820
    },
    {
      "epoch": 5.270933333333334,
      "grad_norm": 0.5022546648979187,
      "learning_rate": 1.705666666666667e-05,
      "loss": 0.0027,
      "step": 98830
    },
    {
      "epoch": 5.271466666666667,
      "grad_norm": 0.2579018175601959,
      "learning_rate": 1.7053333333333335e-05,
      "loss": 0.0015,
      "step": 98840
    },
    {
      "epoch": 5.272,
      "grad_norm": 0.11534623056650162,
      "learning_rate": 1.705e-05,
      "loss": 0.0022,
      "step": 98850
    },
    {
      "epoch": 5.2725333333333335,
      "grad_norm": 0.17034782469272614,
      "learning_rate": 1.7046666666666667e-05,
      "loss": 0.0019,
      "step": 98860
    },
    {
      "epoch": 5.273066666666667,
      "grad_norm": 0.2668066918849945,
      "learning_rate": 1.7043333333333333e-05,
      "loss": 0.0022,
      "step": 98870
    },
    {
      "epoch": 5.2736,
      "grad_norm": 0.2611689269542694,
      "learning_rate": 1.704e-05,
      "loss": 0.0021,
      "step": 98880
    },
    {
      "epoch": 5.274133333333333,
      "grad_norm": 0.32225948572158813,
      "learning_rate": 1.703666666666667e-05,
      "loss": 0.0022,
      "step": 98890
    },
    {
      "epoch": 5.274666666666667,
      "grad_norm": 0.1239376962184906,
      "learning_rate": 1.7033333333333335e-05,
      "loss": 0.0017,
      "step": 98900
    },
    {
      "epoch": 5.2752,
      "grad_norm": 0.2657410502433777,
      "learning_rate": 1.703e-05,
      "loss": 0.0014,
      "step": 98910
    },
    {
      "epoch": 5.275733333333333,
      "grad_norm": 0.15689848363399506,
      "learning_rate": 1.7026666666666667e-05,
      "loss": 0.0019,
      "step": 98920
    },
    {
      "epoch": 5.276266666666666,
      "grad_norm": 0.5219792723655701,
      "learning_rate": 1.7023333333333334e-05,
      "loss": 0.0021,
      "step": 98930
    },
    {
      "epoch": 5.2768,
      "grad_norm": 0.42631667852401733,
      "learning_rate": 1.702e-05,
      "loss": 0.0023,
      "step": 98940
    },
    {
      "epoch": 5.277333333333333,
      "grad_norm": 0.060625769197940826,
      "learning_rate": 1.7016666666666666e-05,
      "loss": 0.0013,
      "step": 98950
    },
    {
      "epoch": 5.277866666666666,
      "grad_norm": 0.17821797728538513,
      "learning_rate": 1.7013333333333335e-05,
      "loss": 0.0021,
      "step": 98960
    },
    {
      "epoch": 5.2783999999999995,
      "grad_norm": 0.34873077273368835,
      "learning_rate": 1.701e-05,
      "loss": 0.0017,
      "step": 98970
    },
    {
      "epoch": 5.278933333333334,
      "grad_norm": 0.40750935673713684,
      "learning_rate": 1.7006666666666668e-05,
      "loss": 0.0021,
      "step": 98980
    },
    {
      "epoch": 5.279466666666667,
      "grad_norm": 0.2387005090713501,
      "learning_rate": 1.7003333333333334e-05,
      "loss": 0.0028,
      "step": 98990
    },
    {
      "epoch": 5.28,
      "grad_norm": 0.20809295773506165,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.0022,
      "step": 99000
    },
    {
      "epoch": 5.2805333333333335,
      "grad_norm": 0.04931066185235977,
      "learning_rate": 1.6996666666666666e-05,
      "loss": 0.0017,
      "step": 99010
    },
    {
      "epoch": 5.281066666666667,
      "grad_norm": 0.6077107191085815,
      "learning_rate": 1.6993333333333332e-05,
      "loss": 0.002,
      "step": 99020
    },
    {
      "epoch": 5.2816,
      "grad_norm": 0.08802300691604614,
      "learning_rate": 1.699e-05,
      "loss": 0.0016,
      "step": 99030
    },
    {
      "epoch": 5.282133333333333,
      "grad_norm": 0.1827733963727951,
      "learning_rate": 1.6986666666666668e-05,
      "loss": 0.0023,
      "step": 99040
    },
    {
      "epoch": 5.282666666666667,
      "grad_norm": 0.24340520799160004,
      "learning_rate": 1.6983333333333334e-05,
      "loss": 0.0018,
      "step": 99050
    },
    {
      "epoch": 5.2832,
      "grad_norm": 0.18762435019016266,
      "learning_rate": 1.698e-05,
      "loss": 0.0016,
      "step": 99060
    },
    {
      "epoch": 5.283733333333333,
      "grad_norm": 0.06362828612327576,
      "learning_rate": 1.697666666666667e-05,
      "loss": 0.0019,
      "step": 99070
    },
    {
      "epoch": 5.2842666666666664,
      "grad_norm": 0.12424656003713608,
      "learning_rate": 1.6973333333333336e-05,
      "loss": 0.002,
      "step": 99080
    },
    {
      "epoch": 5.2848,
      "grad_norm": 0.08728944510221481,
      "learning_rate": 1.697e-05,
      "loss": 0.0013,
      "step": 99090
    },
    {
      "epoch": 5.285333333333333,
      "grad_norm": 0.36802196502685547,
      "learning_rate": 1.6966666666666668e-05,
      "loss": 0.0014,
      "step": 99100
    },
    {
      "epoch": 5.285866666666666,
      "grad_norm": 0.22770245373249054,
      "learning_rate": 1.6963333333333334e-05,
      "loss": 0.0023,
      "step": 99110
    },
    {
      "epoch": 5.2864,
      "grad_norm": 0.3761458396911621,
      "learning_rate": 1.696e-05,
      "loss": 0.0034,
      "step": 99120
    },
    {
      "epoch": 5.286933333333334,
      "grad_norm": 0.14871536195278168,
      "learning_rate": 1.6956666666666666e-05,
      "loss": 0.0018,
      "step": 99130
    },
    {
      "epoch": 5.287466666666667,
      "grad_norm": 0.1794586479663849,
      "learning_rate": 1.6953333333333336e-05,
      "loss": 0.0017,
      "step": 99140
    },
    {
      "epoch": 5.288,
      "grad_norm": 0.42815378308296204,
      "learning_rate": 1.6950000000000002e-05,
      "loss": 0.0022,
      "step": 99150
    },
    {
      "epoch": 5.2885333333333335,
      "grad_norm": 0.33908939361572266,
      "learning_rate": 1.6946666666666665e-05,
      "loss": 0.0015,
      "step": 99160
    },
    {
      "epoch": 5.289066666666667,
      "grad_norm": 0.3701322376728058,
      "learning_rate": 1.6943333333333334e-05,
      "loss": 0.0028,
      "step": 99170
    },
    {
      "epoch": 5.2896,
      "grad_norm": 0.07635906338691711,
      "learning_rate": 1.694e-05,
      "loss": 0.0013,
      "step": 99180
    },
    {
      "epoch": 5.290133333333333,
      "grad_norm": 0.3791331350803375,
      "learning_rate": 1.6936666666666667e-05,
      "loss": 0.0021,
      "step": 99190
    },
    {
      "epoch": 5.290666666666667,
      "grad_norm": 0.23002439737319946,
      "learning_rate": 1.6933333333333333e-05,
      "loss": 0.0028,
      "step": 99200
    },
    {
      "epoch": 5.2912,
      "grad_norm": 0.20528608560562134,
      "learning_rate": 1.6930000000000002e-05,
      "loss": 0.0025,
      "step": 99210
    },
    {
      "epoch": 5.291733333333333,
      "grad_norm": 0.6972744464874268,
      "learning_rate": 1.692666666666667e-05,
      "loss": 0.002,
      "step": 99220
    },
    {
      "epoch": 5.2922666666666665,
      "grad_norm": 0.15648405253887177,
      "learning_rate": 1.6923333333333334e-05,
      "loss": 0.0013,
      "step": 99230
    },
    {
      "epoch": 5.2928,
      "grad_norm": 0.20245812833309174,
      "learning_rate": 1.692e-05,
      "loss": 0.0021,
      "step": 99240
    },
    {
      "epoch": 5.293333333333333,
      "grad_norm": 0.29035016894340515,
      "learning_rate": 1.6916666666666667e-05,
      "loss": 0.0016,
      "step": 99250
    },
    {
      "epoch": 5.293866666666666,
      "grad_norm": 0.28637516498565674,
      "learning_rate": 1.6913333333333333e-05,
      "loss": 0.0022,
      "step": 99260
    },
    {
      "epoch": 5.2943999999999996,
      "grad_norm": 0.14825566112995148,
      "learning_rate": 1.6910000000000002e-05,
      "loss": 0.0014,
      "step": 99270
    },
    {
      "epoch": 5.294933333333334,
      "grad_norm": 0.19666914641857147,
      "learning_rate": 1.690666666666667e-05,
      "loss": 0.0017,
      "step": 99280
    },
    {
      "epoch": 5.295466666666667,
      "grad_norm": 0.09101979434490204,
      "learning_rate": 1.6903333333333335e-05,
      "loss": 0.0019,
      "step": 99290
    },
    {
      "epoch": 5.296,
      "grad_norm": 0.18299010396003723,
      "learning_rate": 1.69e-05,
      "loss": 0.0019,
      "step": 99300
    },
    {
      "epoch": 5.2965333333333335,
      "grad_norm": 0.26844722032546997,
      "learning_rate": 1.6896666666666667e-05,
      "loss": 0.0023,
      "step": 99310
    },
    {
      "epoch": 5.297066666666667,
      "grad_norm": 0.11757112294435501,
      "learning_rate": 1.6893333333333333e-05,
      "loss": 0.0021,
      "step": 99320
    },
    {
      "epoch": 5.2976,
      "grad_norm": 0.31994569301605225,
      "learning_rate": 1.689e-05,
      "loss": 0.0022,
      "step": 99330
    },
    {
      "epoch": 5.298133333333333,
      "grad_norm": 0.2399420589208603,
      "learning_rate": 1.688666666666667e-05,
      "loss": 0.0017,
      "step": 99340
    },
    {
      "epoch": 5.298666666666667,
      "grad_norm": 0.5801031589508057,
      "learning_rate": 1.6883333333333335e-05,
      "loss": 0.002,
      "step": 99350
    },
    {
      "epoch": 5.2992,
      "grad_norm": 0.17577305436134338,
      "learning_rate": 1.688e-05,
      "loss": 0.0023,
      "step": 99360
    },
    {
      "epoch": 5.299733333333333,
      "grad_norm": 0.08926622569561005,
      "learning_rate": 1.6876666666666667e-05,
      "loss": 0.002,
      "step": 99370
    },
    {
      "epoch": 5.3002666666666665,
      "grad_norm": 0.24138686060905457,
      "learning_rate": 1.6873333333333337e-05,
      "loss": 0.0022,
      "step": 99380
    },
    {
      "epoch": 5.3008,
      "grad_norm": 0.5608317255973816,
      "learning_rate": 1.687e-05,
      "loss": 0.0016,
      "step": 99390
    },
    {
      "epoch": 5.301333333333333,
      "grad_norm": 0.2272251844406128,
      "learning_rate": 1.6866666666666666e-05,
      "loss": 0.0021,
      "step": 99400
    },
    {
      "epoch": 5.301866666666666,
      "grad_norm": 0.14397549629211426,
      "learning_rate": 1.6863333333333335e-05,
      "loss": 0.0019,
      "step": 99410
    },
    {
      "epoch": 5.3024000000000004,
      "grad_norm": 0.40188315510749817,
      "learning_rate": 1.686e-05,
      "loss": 0.0026,
      "step": 99420
    },
    {
      "epoch": 5.302933333333334,
      "grad_norm": 0.060671087354421616,
      "learning_rate": 1.6856666666666667e-05,
      "loss": 0.0014,
      "step": 99430
    },
    {
      "epoch": 5.303466666666667,
      "grad_norm": 0.20217466354370117,
      "learning_rate": 1.6853333333333333e-05,
      "loss": 0.0021,
      "step": 99440
    },
    {
      "epoch": 5.304,
      "grad_norm": 0.1875145584344864,
      "learning_rate": 1.6850000000000003e-05,
      "loss": 0.0023,
      "step": 99450
    },
    {
      "epoch": 5.3045333333333335,
      "grad_norm": 0.05724136531352997,
      "learning_rate": 1.6846666666666666e-05,
      "loss": 0.0014,
      "step": 99460
    },
    {
      "epoch": 5.305066666666667,
      "grad_norm": 0.3468797206878662,
      "learning_rate": 1.6843333333333332e-05,
      "loss": 0.0015,
      "step": 99470
    },
    {
      "epoch": 5.3056,
      "grad_norm": 0.31795957684516907,
      "learning_rate": 1.684e-05,
      "loss": 0.0027,
      "step": 99480
    },
    {
      "epoch": 5.306133333333333,
      "grad_norm": 0.29127851128578186,
      "learning_rate": 1.6836666666666668e-05,
      "loss": 0.0015,
      "step": 99490
    },
    {
      "epoch": 5.306666666666667,
      "grad_norm": 0.3258523941040039,
      "learning_rate": 1.6833333333333334e-05,
      "loss": 0.0025,
      "step": 99500
    },
    {
      "epoch": 5.3072,
      "grad_norm": 0.15187706053256989,
      "learning_rate": 1.683e-05,
      "loss": 0.0014,
      "step": 99510
    },
    {
      "epoch": 5.307733333333333,
      "grad_norm": 0.2627739906311035,
      "learning_rate": 1.682666666666667e-05,
      "loss": 0.0018,
      "step": 99520
    },
    {
      "epoch": 5.3082666666666665,
      "grad_norm": 0.4345340132713318,
      "learning_rate": 1.6823333333333335e-05,
      "loss": 0.0013,
      "step": 99530
    },
    {
      "epoch": 5.3088,
      "grad_norm": 0.2693560719490051,
      "learning_rate": 1.6819999999999998e-05,
      "loss": 0.002,
      "step": 99540
    },
    {
      "epoch": 5.309333333333333,
      "grad_norm": 0.27587416768074036,
      "learning_rate": 1.6816666666666668e-05,
      "loss": 0.002,
      "step": 99550
    },
    {
      "epoch": 5.309866666666666,
      "grad_norm": 0.14530658721923828,
      "learning_rate": 1.6813333333333334e-05,
      "loss": 0.0019,
      "step": 99560
    },
    {
      "epoch": 5.3104,
      "grad_norm": 0.09613773226737976,
      "learning_rate": 1.681e-05,
      "loss": 0.0021,
      "step": 99570
    },
    {
      "epoch": 5.310933333333334,
      "grad_norm": 0.40174442529678345,
      "learning_rate": 1.6806666666666666e-05,
      "loss": 0.0013,
      "step": 99580
    },
    {
      "epoch": 5.311466666666667,
      "grad_norm": 0.5142178535461426,
      "learning_rate": 1.6803333333333336e-05,
      "loss": 0.0016,
      "step": 99590
    },
    {
      "epoch": 5.312,
      "grad_norm": 0.4995282292366028,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 0.0022,
      "step": 99600
    },
    {
      "epoch": 5.3125333333333336,
      "grad_norm": 0.3111860454082489,
      "learning_rate": 1.6796666666666665e-05,
      "loss": 0.0037,
      "step": 99610
    },
    {
      "epoch": 5.313066666666667,
      "grad_norm": 0.37733638286590576,
      "learning_rate": 1.6793333333333334e-05,
      "loss": 0.0018,
      "step": 99620
    },
    {
      "epoch": 5.3136,
      "grad_norm": 0.212149515748024,
      "learning_rate": 1.679e-05,
      "loss": 0.0019,
      "step": 99630
    },
    {
      "epoch": 5.314133333333333,
      "grad_norm": 0.20300327241420746,
      "learning_rate": 1.6786666666666666e-05,
      "loss": 0.0016,
      "step": 99640
    },
    {
      "epoch": 5.314666666666667,
      "grad_norm": 0.064951591193676,
      "learning_rate": 1.6783333333333336e-05,
      "loss": 0.0018,
      "step": 99650
    },
    {
      "epoch": 5.3152,
      "grad_norm": 0.0646781474351883,
      "learning_rate": 1.6780000000000002e-05,
      "loss": 0.0013,
      "step": 99660
    },
    {
      "epoch": 5.315733333333333,
      "grad_norm": 0.23951824009418488,
      "learning_rate": 1.6776666666666668e-05,
      "loss": 0.0021,
      "step": 99670
    },
    {
      "epoch": 5.3162666666666665,
      "grad_norm": 0.11550423502922058,
      "learning_rate": 1.6773333333333334e-05,
      "loss": 0.0017,
      "step": 99680
    },
    {
      "epoch": 5.3168,
      "grad_norm": 0.09334789216518402,
      "learning_rate": 1.677e-05,
      "loss": 0.002,
      "step": 99690
    },
    {
      "epoch": 5.317333333333333,
      "grad_norm": 0.1446392834186554,
      "learning_rate": 1.6766666666666667e-05,
      "loss": 0.0024,
      "step": 99700
    },
    {
      "epoch": 5.317866666666666,
      "grad_norm": 0.2707132399082184,
      "learning_rate": 1.6763333333333333e-05,
      "loss": 0.002,
      "step": 99710
    },
    {
      "epoch": 5.3184000000000005,
      "grad_norm": 0.14841295778751373,
      "learning_rate": 1.6760000000000002e-05,
      "loss": 0.0021,
      "step": 99720
    },
    {
      "epoch": 5.318933333333334,
      "grad_norm": 0.23683147132396698,
      "learning_rate": 1.6756666666666668e-05,
      "loss": 0.0015,
      "step": 99730
    },
    {
      "epoch": 5.319466666666667,
      "grad_norm": 0.03375401347875595,
      "learning_rate": 1.6753333333333334e-05,
      "loss": 0.0014,
      "step": 99740
    },
    {
      "epoch": 5.32,
      "grad_norm": 0.23219084739685059,
      "learning_rate": 1.675e-05,
      "loss": 0.0018,
      "step": 99750
    },
    {
      "epoch": 5.320533333333334,
      "grad_norm": 0.14316751062870026,
      "learning_rate": 1.674666666666667e-05,
      "loss": 0.002,
      "step": 99760
    },
    {
      "epoch": 5.321066666666667,
      "grad_norm": 0.09138084948062897,
      "learning_rate": 1.6743333333333333e-05,
      "loss": 0.0023,
      "step": 99770
    },
    {
      "epoch": 5.3216,
      "grad_norm": 0.673902153968811,
      "learning_rate": 1.674e-05,
      "loss": 0.0024,
      "step": 99780
    },
    {
      "epoch": 5.322133333333333,
      "grad_norm": 0.25735408067703247,
      "learning_rate": 1.673666666666667e-05,
      "loss": 0.0021,
      "step": 99790
    },
    {
      "epoch": 5.322666666666667,
      "grad_norm": 0.5634797215461731,
      "learning_rate": 1.6733333333333335e-05,
      "loss": 0.0025,
      "step": 99800
    },
    {
      "epoch": 5.3232,
      "grad_norm": 0.3636174499988556,
      "learning_rate": 1.673e-05,
      "loss": 0.0015,
      "step": 99810
    },
    {
      "epoch": 5.323733333333333,
      "grad_norm": 0.0490029901266098,
      "learning_rate": 1.6726666666666667e-05,
      "loss": 0.0024,
      "step": 99820
    },
    {
      "epoch": 5.3242666666666665,
      "grad_norm": 0.1811194121837616,
      "learning_rate": 1.6723333333333336e-05,
      "loss": 0.0017,
      "step": 99830
    },
    {
      "epoch": 5.3248,
      "grad_norm": 0.5728114247322083,
      "learning_rate": 1.672e-05,
      "loss": 0.0024,
      "step": 99840
    },
    {
      "epoch": 5.325333333333333,
      "grad_norm": 0.05290110409259796,
      "learning_rate": 1.6716666666666665e-05,
      "loss": 0.0028,
      "step": 99850
    },
    {
      "epoch": 5.325866666666666,
      "grad_norm": 0.2598567605018616,
      "learning_rate": 1.6713333333333335e-05,
      "loss": 0.0023,
      "step": 99860
    },
    {
      "epoch": 5.3264,
      "grad_norm": 0.2151811420917511,
      "learning_rate": 1.671e-05,
      "loss": 0.0018,
      "step": 99870
    },
    {
      "epoch": 5.326933333333334,
      "grad_norm": 0.24189957976341248,
      "learning_rate": 1.6706666666666667e-05,
      "loss": 0.0016,
      "step": 99880
    },
    {
      "epoch": 5.327466666666667,
      "grad_norm": 0.1285746991634369,
      "learning_rate": 1.6703333333333333e-05,
      "loss": 0.002,
      "step": 99890
    },
    {
      "epoch": 5.328,
      "grad_norm": 0.3221995532512665,
      "learning_rate": 1.6700000000000003e-05,
      "loss": 0.0018,
      "step": 99900
    },
    {
      "epoch": 5.328533333333334,
      "grad_norm": 0.4494531750679016,
      "learning_rate": 1.669666666666667e-05,
      "loss": 0.0018,
      "step": 99910
    },
    {
      "epoch": 5.329066666666667,
      "grad_norm": 0.4442857801914215,
      "learning_rate": 1.669333333333333e-05,
      "loss": 0.0014,
      "step": 99920
    },
    {
      "epoch": 5.3296,
      "grad_norm": 0.08431434631347656,
      "learning_rate": 1.669e-05,
      "loss": 0.0025,
      "step": 99930
    },
    {
      "epoch": 5.330133333333333,
      "grad_norm": 0.3580053746700287,
      "learning_rate": 1.6686666666666667e-05,
      "loss": 0.0017,
      "step": 99940
    },
    {
      "epoch": 5.330666666666667,
      "grad_norm": 0.3903340995311737,
      "learning_rate": 1.6683333333333333e-05,
      "loss": 0.0028,
      "step": 99950
    },
    {
      "epoch": 5.3312,
      "grad_norm": 0.062350817024707794,
      "learning_rate": 1.668e-05,
      "loss": 0.0013,
      "step": 99960
    },
    {
      "epoch": 5.331733333333333,
      "grad_norm": 0.2938912808895111,
      "learning_rate": 1.667666666666667e-05,
      "loss": 0.0016,
      "step": 99970
    },
    {
      "epoch": 5.3322666666666665,
      "grad_norm": 0.22673094272613525,
      "learning_rate": 1.6673333333333335e-05,
      "loss": 0.0019,
      "step": 99980
    },
    {
      "epoch": 5.3328,
      "grad_norm": 0.2902644872665405,
      "learning_rate": 1.6669999999999998e-05,
      "loss": 0.0024,
      "step": 99990
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 0.4956590235233307,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.0022,
      "step": 100000
    },
    {
      "epoch": 5.333866666666666,
      "grad_norm": 0.07837530225515366,
      "learning_rate": 1.6663333333333334e-05,
      "loss": 0.0015,
      "step": 100010
    },
    {
      "epoch": 5.3344,
      "grad_norm": 0.04720491170883179,
      "learning_rate": 1.666e-05,
      "loss": 0.0011,
      "step": 100020
    },
    {
      "epoch": 5.334933333333334,
      "grad_norm": 0.2627626061439514,
      "learning_rate": 1.665666666666667e-05,
      "loss": 0.0022,
      "step": 100030
    },
    {
      "epoch": 5.335466666666667,
      "grad_norm": 0.04032221809029579,
      "learning_rate": 1.6653333333333335e-05,
      "loss": 0.0016,
      "step": 100040
    },
    {
      "epoch": 5.336,
      "grad_norm": 0.3064439594745636,
      "learning_rate": 1.665e-05,
      "loss": 0.0021,
      "step": 100050
    },
    {
      "epoch": 5.336533333333334,
      "grad_norm": 0.6481435298919678,
      "learning_rate": 1.6646666666666668e-05,
      "loss": 0.0026,
      "step": 100060
    },
    {
      "epoch": 5.337066666666667,
      "grad_norm": 0.14728201925754547,
      "learning_rate": 1.6643333333333334e-05,
      "loss": 0.0015,
      "step": 100070
    },
    {
      "epoch": 5.3376,
      "grad_norm": 0.12264223396778107,
      "learning_rate": 1.664e-05,
      "loss": 0.0017,
      "step": 100080
    },
    {
      "epoch": 5.338133333333333,
      "grad_norm": 0.03758671134710312,
      "learning_rate": 1.6636666666666666e-05,
      "loss": 0.0016,
      "step": 100090
    },
    {
      "epoch": 5.338666666666667,
      "grad_norm": 0.20216231048107147,
      "learning_rate": 1.6633333333333336e-05,
      "loss": 0.0022,
      "step": 100100
    },
    {
      "epoch": 5.3392,
      "grad_norm": 0.14829997718334198,
      "learning_rate": 1.6630000000000002e-05,
      "loss": 0.0014,
      "step": 100110
    },
    {
      "epoch": 5.339733333333333,
      "grad_norm": 0.14561867713928223,
      "learning_rate": 1.6626666666666668e-05,
      "loss": 0.0021,
      "step": 100120
    },
    {
      "epoch": 5.3402666666666665,
      "grad_norm": 0.0926436111330986,
      "learning_rate": 1.6623333333333334e-05,
      "loss": 0.0016,
      "step": 100130
    },
    {
      "epoch": 5.3408,
      "grad_norm": 0.2919783294200897,
      "learning_rate": 1.662e-05,
      "loss": 0.0019,
      "step": 100140
    },
    {
      "epoch": 5.341333333333333,
      "grad_norm": 0.2810983657836914,
      "learning_rate": 1.6616666666666666e-05,
      "loss": 0.0017,
      "step": 100150
    },
    {
      "epoch": 5.341866666666666,
      "grad_norm": 0.2660731077194214,
      "learning_rate": 1.6613333333333332e-05,
      "loss": 0.0016,
      "step": 100160
    },
    {
      "epoch": 5.3424,
      "grad_norm": 0.15576738119125366,
      "learning_rate": 1.6610000000000002e-05,
      "loss": 0.002,
      "step": 100170
    },
    {
      "epoch": 5.342933333333333,
      "grad_norm": 0.27704545855522156,
      "learning_rate": 1.6606666666666668e-05,
      "loss": 0.0022,
      "step": 100180
    },
    {
      "epoch": 5.343466666666667,
      "grad_norm": 0.31099531054496765,
      "learning_rate": 1.6603333333333334e-05,
      "loss": 0.0019,
      "step": 100190
    },
    {
      "epoch": 5.344,
      "grad_norm": 0.054061222821474075,
      "learning_rate": 1.66e-05,
      "loss": 0.0018,
      "step": 100200
    },
    {
      "epoch": 5.344533333333334,
      "grad_norm": 0.5727304816246033,
      "learning_rate": 1.659666666666667e-05,
      "loss": 0.0015,
      "step": 100210
    },
    {
      "epoch": 5.345066666666667,
      "grad_norm": 0.2940339148044586,
      "learning_rate": 1.6593333333333333e-05,
      "loss": 0.002,
      "step": 100220
    },
    {
      "epoch": 5.3456,
      "grad_norm": 0.1814013570547104,
      "learning_rate": 1.659e-05,
      "loss": 0.0017,
      "step": 100230
    },
    {
      "epoch": 5.346133333333333,
      "grad_norm": 0.26722079515457153,
      "learning_rate": 1.6586666666666668e-05,
      "loss": 0.0025,
      "step": 100240
    },
    {
      "epoch": 5.346666666666667,
      "grad_norm": 0.48478397727012634,
      "learning_rate": 1.6583333333333334e-05,
      "loss": 0.0025,
      "step": 100250
    },
    {
      "epoch": 5.3472,
      "grad_norm": 0.4583224356174469,
      "learning_rate": 1.658e-05,
      "loss": 0.0021,
      "step": 100260
    },
    {
      "epoch": 5.347733333333333,
      "grad_norm": 0.20316903293132782,
      "learning_rate": 1.6576666666666667e-05,
      "loss": 0.0017,
      "step": 100270
    },
    {
      "epoch": 5.3482666666666665,
      "grad_norm": 0.2602902948856354,
      "learning_rate": 1.6573333333333336e-05,
      "loss": 0.0027,
      "step": 100280
    },
    {
      "epoch": 5.3488,
      "grad_norm": 0.23349887132644653,
      "learning_rate": 1.657e-05,
      "loss": 0.0023,
      "step": 100290
    },
    {
      "epoch": 5.349333333333333,
      "grad_norm": 0.11579225212335587,
      "learning_rate": 1.6566666666666665e-05,
      "loss": 0.0015,
      "step": 100300
    },
    {
      "epoch": 5.349866666666666,
      "grad_norm": 0.25356853008270264,
      "learning_rate": 1.6563333333333335e-05,
      "loss": 0.0018,
      "step": 100310
    },
    {
      "epoch": 5.3504,
      "grad_norm": 0.24470387399196625,
      "learning_rate": 1.656e-05,
      "loss": 0.0022,
      "step": 100320
    },
    {
      "epoch": 5.350933333333334,
      "grad_norm": 0.17977072298526764,
      "learning_rate": 1.6556666666666667e-05,
      "loss": 0.0018,
      "step": 100330
    },
    {
      "epoch": 5.351466666666667,
      "grad_norm": 0.1442895382642746,
      "learning_rate": 1.6553333333333333e-05,
      "loss": 0.0014,
      "step": 100340
    },
    {
      "epoch": 5.352,
      "grad_norm": 0.04157090187072754,
      "learning_rate": 1.6550000000000002e-05,
      "loss": 0.0024,
      "step": 100350
    },
    {
      "epoch": 5.352533333333334,
      "grad_norm": 0.2588740885257721,
      "learning_rate": 1.654666666666667e-05,
      "loss": 0.0017,
      "step": 100360
    },
    {
      "epoch": 5.353066666666667,
      "grad_norm": 0.23007336258888245,
      "learning_rate": 1.654333333333333e-05,
      "loss": 0.002,
      "step": 100370
    },
    {
      "epoch": 5.3536,
      "grad_norm": 0.13009458780288696,
      "learning_rate": 1.654e-05,
      "loss": 0.0015,
      "step": 100380
    },
    {
      "epoch": 5.354133333333333,
      "grad_norm": 0.41617339849472046,
      "learning_rate": 1.6536666666666667e-05,
      "loss": 0.0017,
      "step": 100390
    },
    {
      "epoch": 5.354666666666667,
      "grad_norm": 0.2719518542289734,
      "learning_rate": 1.6533333333333333e-05,
      "loss": 0.0018,
      "step": 100400
    },
    {
      "epoch": 5.3552,
      "grad_norm": 0.3571174442768097,
      "learning_rate": 1.6530000000000003e-05,
      "loss": 0.0018,
      "step": 100410
    },
    {
      "epoch": 5.355733333333333,
      "grad_norm": 0.3134736716747284,
      "learning_rate": 1.652666666666667e-05,
      "loss": 0.0019,
      "step": 100420
    },
    {
      "epoch": 5.3562666666666665,
      "grad_norm": 0.29244983196258545,
      "learning_rate": 1.6523333333333335e-05,
      "loss": 0.0028,
      "step": 100430
    },
    {
      "epoch": 5.3568,
      "grad_norm": 0.25737902522087097,
      "learning_rate": 1.652e-05,
      "loss": 0.0013,
      "step": 100440
    },
    {
      "epoch": 5.357333333333333,
      "grad_norm": 0.0441107451915741,
      "learning_rate": 1.6516666666666667e-05,
      "loss": 0.002,
      "step": 100450
    },
    {
      "epoch": 5.357866666666666,
      "grad_norm": 0.3759590685367584,
      "learning_rate": 1.6513333333333333e-05,
      "loss": 0.0026,
      "step": 100460
    },
    {
      "epoch": 5.3584,
      "grad_norm": 0.039022695273160934,
      "learning_rate": 1.651e-05,
      "loss": 0.0021,
      "step": 100470
    },
    {
      "epoch": 5.358933333333333,
      "grad_norm": 0.11477421969175339,
      "learning_rate": 1.650666666666667e-05,
      "loss": 0.0016,
      "step": 100480
    },
    {
      "epoch": 5.359466666666667,
      "grad_norm": 0.4005658030509949,
      "learning_rate": 1.6503333333333335e-05,
      "loss": 0.0018,
      "step": 100490
    },
    {
      "epoch": 5.36,
      "grad_norm": 0.3443447947502136,
      "learning_rate": 1.65e-05,
      "loss": 0.0015,
      "step": 100500
    },
    {
      "epoch": 5.360533333333334,
      "grad_norm": 0.28770124912261963,
      "learning_rate": 1.6496666666666667e-05,
      "loss": 0.0018,
      "step": 100510
    },
    {
      "epoch": 5.361066666666667,
      "grad_norm": 0.38191622495651245,
      "learning_rate": 1.6493333333333334e-05,
      "loss": 0.0022,
      "step": 100520
    },
    {
      "epoch": 5.3616,
      "grad_norm": 0.14571312069892883,
      "learning_rate": 1.649e-05,
      "loss": 0.0018,
      "step": 100530
    },
    {
      "epoch": 5.362133333333333,
      "grad_norm": 0.2903609275817871,
      "learning_rate": 1.6486666666666666e-05,
      "loss": 0.0022,
      "step": 100540
    },
    {
      "epoch": 5.362666666666667,
      "grad_norm": 0.02241225354373455,
      "learning_rate": 1.6483333333333335e-05,
      "loss": 0.0017,
      "step": 100550
    },
    {
      "epoch": 5.3632,
      "grad_norm": 0.3454478085041046,
      "learning_rate": 1.648e-05,
      "loss": 0.0024,
      "step": 100560
    },
    {
      "epoch": 5.363733333333333,
      "grad_norm": 0.23314709961414337,
      "learning_rate": 1.6476666666666668e-05,
      "loss": 0.0018,
      "step": 100570
    },
    {
      "epoch": 5.3642666666666665,
      "grad_norm": 0.44323673844337463,
      "learning_rate": 1.6473333333333334e-05,
      "loss": 0.0024,
      "step": 100580
    },
    {
      "epoch": 5.3648,
      "grad_norm": 0.30427980422973633,
      "learning_rate": 1.6470000000000003e-05,
      "loss": 0.0016,
      "step": 100590
    },
    {
      "epoch": 5.365333333333333,
      "grad_norm": 0.2874521315097809,
      "learning_rate": 1.6466666666666666e-05,
      "loss": 0.0013,
      "step": 100600
    },
    {
      "epoch": 5.365866666666666,
      "grad_norm": 0.038364119827747345,
      "learning_rate": 1.6463333333333332e-05,
      "loss": 0.0021,
      "step": 100610
    },
    {
      "epoch": 5.3664,
      "grad_norm": 0.2866193950176239,
      "learning_rate": 1.646e-05,
      "loss": 0.0015,
      "step": 100620
    },
    {
      "epoch": 5.366933333333334,
      "grad_norm": 0.23320554196834564,
      "learning_rate": 1.6456666666666668e-05,
      "loss": 0.002,
      "step": 100630
    },
    {
      "epoch": 5.367466666666667,
      "grad_norm": 0.026374051347374916,
      "learning_rate": 1.6453333333333334e-05,
      "loss": 0.0015,
      "step": 100640
    },
    {
      "epoch": 5.368,
      "grad_norm": 0.08661621809005737,
      "learning_rate": 1.645e-05,
      "loss": 0.0022,
      "step": 100650
    },
    {
      "epoch": 5.368533333333334,
      "grad_norm": 0.11978037655353546,
      "learning_rate": 1.644666666666667e-05,
      "loss": 0.0021,
      "step": 100660
    },
    {
      "epoch": 5.369066666666667,
      "grad_norm": 0.5178903937339783,
      "learning_rate": 1.6443333333333332e-05,
      "loss": 0.0022,
      "step": 100670
    },
    {
      "epoch": 5.3696,
      "grad_norm": 0.03144482523202896,
      "learning_rate": 1.644e-05,
      "loss": 0.0026,
      "step": 100680
    },
    {
      "epoch": 5.370133333333333,
      "grad_norm": 0.07567302882671356,
      "learning_rate": 1.6436666666666668e-05,
      "loss": 0.0015,
      "step": 100690
    },
    {
      "epoch": 5.370666666666667,
      "grad_norm": 0.21322108805179596,
      "learning_rate": 1.6433333333333334e-05,
      "loss": 0.0029,
      "step": 100700
    },
    {
      "epoch": 5.3712,
      "grad_norm": 0.4073970317840576,
      "learning_rate": 1.643e-05,
      "loss": 0.0023,
      "step": 100710
    },
    {
      "epoch": 5.371733333333333,
      "grad_norm": 0.1451256275177002,
      "learning_rate": 1.6426666666666666e-05,
      "loss": 0.0013,
      "step": 100720
    },
    {
      "epoch": 5.3722666666666665,
      "grad_norm": 0.32191675901412964,
      "learning_rate": 1.6423333333333336e-05,
      "loss": 0.0019,
      "step": 100730
    },
    {
      "epoch": 5.3728,
      "grad_norm": 0.04366900026798248,
      "learning_rate": 1.6420000000000002e-05,
      "loss": 0.0017,
      "step": 100740
    },
    {
      "epoch": 5.373333333333333,
      "grad_norm": 0.5712481737136841,
      "learning_rate": 1.6416666666666665e-05,
      "loss": 0.0026,
      "step": 100750
    },
    {
      "epoch": 5.373866666666666,
      "grad_norm": 0.21023325622081757,
      "learning_rate": 1.6413333333333334e-05,
      "loss": 0.0018,
      "step": 100760
    },
    {
      "epoch": 5.3744,
      "grad_norm": 0.2880933880805969,
      "learning_rate": 1.641e-05,
      "loss": 0.0022,
      "step": 100770
    },
    {
      "epoch": 5.374933333333333,
      "grad_norm": 0.17399320006370544,
      "learning_rate": 1.6406666666666667e-05,
      "loss": 0.0019,
      "step": 100780
    },
    {
      "epoch": 5.375466666666667,
      "grad_norm": 0.23681659996509552,
      "learning_rate": 1.6403333333333336e-05,
      "loss": 0.0017,
      "step": 100790
    },
    {
      "epoch": 5.376,
      "grad_norm": 0.4795461893081665,
      "learning_rate": 1.6400000000000002e-05,
      "loss": 0.0031,
      "step": 100800
    },
    {
      "epoch": 5.376533333333334,
      "grad_norm": 0.346529096364975,
      "learning_rate": 1.639666666666667e-05,
      "loss": 0.0023,
      "step": 100810
    },
    {
      "epoch": 5.377066666666667,
      "grad_norm": 0.1018655002117157,
      "learning_rate": 1.639333333333333e-05,
      "loss": 0.0017,
      "step": 100820
    },
    {
      "epoch": 5.3776,
      "grad_norm": 0.14301058650016785,
      "learning_rate": 1.639e-05,
      "loss": 0.0015,
      "step": 100830
    },
    {
      "epoch": 5.378133333333333,
      "grad_norm": 0.37184804677963257,
      "learning_rate": 1.6386666666666667e-05,
      "loss": 0.0018,
      "step": 100840
    },
    {
      "epoch": 5.378666666666667,
      "grad_norm": 0.05560247227549553,
      "learning_rate": 1.6383333333333333e-05,
      "loss": 0.0017,
      "step": 100850
    },
    {
      "epoch": 5.3792,
      "grad_norm": 0.5278548002243042,
      "learning_rate": 1.6380000000000002e-05,
      "loss": 0.0016,
      "step": 100860
    },
    {
      "epoch": 5.379733333333333,
      "grad_norm": 0.08738154917955399,
      "learning_rate": 1.637666666666667e-05,
      "loss": 0.0017,
      "step": 100870
    },
    {
      "epoch": 5.3802666666666665,
      "grad_norm": 0.2980955243110657,
      "learning_rate": 1.6373333333333335e-05,
      "loss": 0.0015,
      "step": 100880
    },
    {
      "epoch": 5.3808,
      "grad_norm": 0.2478117197751999,
      "learning_rate": 1.637e-05,
      "loss": 0.0032,
      "step": 100890
    },
    {
      "epoch": 5.381333333333333,
      "grad_norm": 0.07875949144363403,
      "learning_rate": 1.6366666666666667e-05,
      "loss": 0.0025,
      "step": 100900
    },
    {
      "epoch": 5.381866666666666,
      "grad_norm": 0.3770151436328888,
      "learning_rate": 1.6363333333333333e-05,
      "loss": 0.0018,
      "step": 100910
    },
    {
      "epoch": 5.3824,
      "grad_norm": 0.3336624801158905,
      "learning_rate": 1.636e-05,
      "loss": 0.0018,
      "step": 100920
    },
    {
      "epoch": 5.382933333333334,
      "grad_norm": 0.6078287959098816,
      "learning_rate": 1.635666666666667e-05,
      "loss": 0.0014,
      "step": 100930
    },
    {
      "epoch": 5.383466666666667,
      "grad_norm": 0.3181115686893463,
      "learning_rate": 1.6353333333333335e-05,
      "loss": 0.0029,
      "step": 100940
    },
    {
      "epoch": 5.384,
      "grad_norm": 0.21264371275901794,
      "learning_rate": 1.635e-05,
      "loss": 0.0018,
      "step": 100950
    },
    {
      "epoch": 5.384533333333334,
      "grad_norm": 0.21028012037277222,
      "learning_rate": 1.6346666666666667e-05,
      "loss": 0.0026,
      "step": 100960
    },
    {
      "epoch": 5.385066666666667,
      "grad_norm": 0.057305872440338135,
      "learning_rate": 1.6343333333333337e-05,
      "loss": 0.0017,
      "step": 100970
    },
    {
      "epoch": 5.3856,
      "grad_norm": 0.057793889194726944,
      "learning_rate": 1.634e-05,
      "loss": 0.0019,
      "step": 100980
    },
    {
      "epoch": 5.386133333333333,
      "grad_norm": 0.3297795057296753,
      "learning_rate": 1.6336666666666666e-05,
      "loss": 0.0015,
      "step": 100990
    },
    {
      "epoch": 5.386666666666667,
      "grad_norm": 0.30167916417121887,
      "learning_rate": 1.6333333333333335e-05,
      "loss": 0.0023,
      "step": 101000
    },
    {
      "epoch": 5.3872,
      "grad_norm": 0.18116450309753418,
      "learning_rate": 1.633e-05,
      "loss": 0.0018,
      "step": 101010
    },
    {
      "epoch": 5.387733333333333,
      "grad_norm": 0.08919505774974823,
      "learning_rate": 1.6326666666666667e-05,
      "loss": 0.0011,
      "step": 101020
    },
    {
      "epoch": 5.3882666666666665,
      "grad_norm": 0.15710417926311493,
      "learning_rate": 1.6323333333333333e-05,
      "loss": 0.002,
      "step": 101030
    },
    {
      "epoch": 5.3888,
      "grad_norm": 0.06505449116230011,
      "learning_rate": 1.6320000000000003e-05,
      "loss": 0.002,
      "step": 101040
    },
    {
      "epoch": 5.389333333333333,
      "grad_norm": 0.21586734056472778,
      "learning_rate": 1.6316666666666666e-05,
      "loss": 0.0029,
      "step": 101050
    },
    {
      "epoch": 5.389866666666666,
      "grad_norm": 0.1196158304810524,
      "learning_rate": 1.6313333333333332e-05,
      "loss": 0.0026,
      "step": 101060
    },
    {
      "epoch": 5.3904,
      "grad_norm": 0.20759767293930054,
      "learning_rate": 1.631e-05,
      "loss": 0.0015,
      "step": 101070
    },
    {
      "epoch": 5.390933333333333,
      "grad_norm": 0.3166811168193817,
      "learning_rate": 1.6306666666666668e-05,
      "loss": 0.0016,
      "step": 101080
    },
    {
      "epoch": 5.391466666666667,
      "grad_norm": 0.09665671736001968,
      "learning_rate": 1.6303333333333334e-05,
      "loss": 0.002,
      "step": 101090
    },
    {
      "epoch": 5.392,
      "grad_norm": 0.04209195822477341,
      "learning_rate": 1.63e-05,
      "loss": 0.0021,
      "step": 101100
    },
    {
      "epoch": 5.392533333333334,
      "grad_norm": 0.20589293539524078,
      "learning_rate": 1.629666666666667e-05,
      "loss": 0.0017,
      "step": 101110
    },
    {
      "epoch": 5.393066666666667,
      "grad_norm": 0.18665504455566406,
      "learning_rate": 1.6293333333333335e-05,
      "loss": 0.0013,
      "step": 101120
    },
    {
      "epoch": 5.3936,
      "grad_norm": 0.11737030744552612,
      "learning_rate": 1.6289999999999998e-05,
      "loss": 0.0022,
      "step": 101130
    },
    {
      "epoch": 5.3941333333333334,
      "grad_norm": 0.043850403279066086,
      "learning_rate": 1.6286666666666668e-05,
      "loss": 0.0015,
      "step": 101140
    },
    {
      "epoch": 5.394666666666667,
      "grad_norm": 0.4009898602962494,
      "learning_rate": 1.6283333333333334e-05,
      "loss": 0.0027,
      "step": 101150
    },
    {
      "epoch": 5.3952,
      "grad_norm": 0.05871346965432167,
      "learning_rate": 1.628e-05,
      "loss": 0.0018,
      "step": 101160
    },
    {
      "epoch": 5.395733333333333,
      "grad_norm": 0.1697527915239334,
      "learning_rate": 1.627666666666667e-05,
      "loss": 0.0019,
      "step": 101170
    },
    {
      "epoch": 5.3962666666666665,
      "grad_norm": 0.044751740992069244,
      "learning_rate": 1.6273333333333336e-05,
      "loss": 0.002,
      "step": 101180
    },
    {
      "epoch": 5.3968,
      "grad_norm": 0.22998452186584473,
      "learning_rate": 1.6270000000000002e-05,
      "loss": 0.0018,
      "step": 101190
    },
    {
      "epoch": 5.397333333333333,
      "grad_norm": 0.26622137427330017,
      "learning_rate": 1.6266666666666665e-05,
      "loss": 0.0023,
      "step": 101200
    },
    {
      "epoch": 5.397866666666666,
      "grad_norm": 0.20176181197166443,
      "learning_rate": 1.6263333333333334e-05,
      "loss": 0.0015,
      "step": 101210
    },
    {
      "epoch": 5.3984,
      "grad_norm": 0.12065728008747101,
      "learning_rate": 1.626e-05,
      "loss": 0.0022,
      "step": 101220
    },
    {
      "epoch": 5.398933333333333,
      "grad_norm": 0.34703701734542847,
      "learning_rate": 1.6256666666666666e-05,
      "loss": 0.0017,
      "step": 101230
    },
    {
      "epoch": 5.399466666666667,
      "grad_norm": 0.28551146388053894,
      "learning_rate": 1.6253333333333336e-05,
      "loss": 0.0024,
      "step": 101240
    },
    {
      "epoch": 5.4,
      "grad_norm": 0.042815081775188446,
      "learning_rate": 1.6250000000000002e-05,
      "loss": 0.0027,
      "step": 101250
    },
    {
      "epoch": 5.400533333333334,
      "grad_norm": 0.0628809705376625,
      "learning_rate": 1.6246666666666668e-05,
      "loss": 0.0027,
      "step": 101260
    },
    {
      "epoch": 5.401066666666667,
      "grad_norm": 0.1289130598306656,
      "learning_rate": 1.6243333333333334e-05,
      "loss": 0.0022,
      "step": 101270
    },
    {
      "epoch": 5.4016,
      "grad_norm": 0.2319585680961609,
      "learning_rate": 1.624e-05,
      "loss": 0.0012,
      "step": 101280
    },
    {
      "epoch": 5.4021333333333335,
      "grad_norm": 0.18381690979003906,
      "learning_rate": 1.6236666666666667e-05,
      "loss": 0.0016,
      "step": 101290
    },
    {
      "epoch": 5.402666666666667,
      "grad_norm": 0.29796016216278076,
      "learning_rate": 1.6233333333333333e-05,
      "loss": 0.0032,
      "step": 101300
    },
    {
      "epoch": 5.4032,
      "grad_norm": 0.12162747979164124,
      "learning_rate": 1.6230000000000002e-05,
      "loss": 0.0016,
      "step": 101310
    },
    {
      "epoch": 5.403733333333333,
      "grad_norm": 0.04101631045341492,
      "learning_rate": 1.6226666666666668e-05,
      "loss": 0.0024,
      "step": 101320
    },
    {
      "epoch": 5.4042666666666666,
      "grad_norm": 0.35929226875305176,
      "learning_rate": 1.6223333333333334e-05,
      "loss": 0.0026,
      "step": 101330
    },
    {
      "epoch": 5.4048,
      "grad_norm": 0.17320024967193604,
      "learning_rate": 1.622e-05,
      "loss": 0.0017,
      "step": 101340
    },
    {
      "epoch": 5.405333333333333,
      "grad_norm": 0.06328397989273071,
      "learning_rate": 1.6216666666666667e-05,
      "loss": 0.0021,
      "step": 101350
    },
    {
      "epoch": 5.405866666666666,
      "grad_norm": 0.23016299307346344,
      "learning_rate": 1.6213333333333333e-05,
      "loss": 0.002,
      "step": 101360
    },
    {
      "epoch": 5.4064,
      "grad_norm": 0.22128145396709442,
      "learning_rate": 1.621e-05,
      "loss": 0.0019,
      "step": 101370
    },
    {
      "epoch": 5.406933333333333,
      "grad_norm": 0.3442227244377136,
      "learning_rate": 1.620666666666667e-05,
      "loss": 0.0022,
      "step": 101380
    },
    {
      "epoch": 5.407466666666666,
      "grad_norm": 0.4322083592414856,
      "learning_rate": 1.6203333333333335e-05,
      "loss": 0.0022,
      "step": 101390
    },
    {
      "epoch": 5.408,
      "grad_norm": 0.3716839849948883,
      "learning_rate": 1.62e-05,
      "loss": 0.0019,
      "step": 101400
    },
    {
      "epoch": 5.408533333333334,
      "grad_norm": 0.06713911145925522,
      "learning_rate": 1.6196666666666667e-05,
      "loss": 0.0018,
      "step": 101410
    },
    {
      "epoch": 5.409066666666667,
      "grad_norm": 0.2076730728149414,
      "learning_rate": 1.6193333333333336e-05,
      "loss": 0.0018,
      "step": 101420
    },
    {
      "epoch": 5.4096,
      "grad_norm": 0.14570482075214386,
      "learning_rate": 1.619e-05,
      "loss": 0.0016,
      "step": 101430
    },
    {
      "epoch": 5.4101333333333335,
      "grad_norm": 0.042632732540369034,
      "learning_rate": 1.6186666666666665e-05,
      "loss": 0.0017,
      "step": 101440
    },
    {
      "epoch": 5.410666666666667,
      "grad_norm": 0.0640280470252037,
      "learning_rate": 1.6183333333333335e-05,
      "loss": 0.0011,
      "step": 101450
    },
    {
      "epoch": 5.4112,
      "grad_norm": 0.3109123408794403,
      "learning_rate": 1.618e-05,
      "loss": 0.002,
      "step": 101460
    },
    {
      "epoch": 5.411733333333333,
      "grad_norm": 0.4744130074977875,
      "learning_rate": 1.6176666666666667e-05,
      "loss": 0.0021,
      "step": 101470
    },
    {
      "epoch": 5.412266666666667,
      "grad_norm": 0.043144043534994125,
      "learning_rate": 1.6173333333333333e-05,
      "loss": 0.002,
      "step": 101480
    },
    {
      "epoch": 5.4128,
      "grad_norm": 0.06766330450773239,
      "learning_rate": 1.6170000000000003e-05,
      "loss": 0.0028,
      "step": 101490
    },
    {
      "epoch": 5.413333333333333,
      "grad_norm": 0.17000454664230347,
      "learning_rate": 1.6166666666666665e-05,
      "loss": 0.0025,
      "step": 101500
    },
    {
      "epoch": 5.413866666666666,
      "grad_norm": 0.23061352968215942,
      "learning_rate": 1.616333333333333e-05,
      "loss": 0.0018,
      "step": 101510
    },
    {
      "epoch": 5.4144,
      "grad_norm": 0.28621211647987366,
      "learning_rate": 1.616e-05,
      "loss": 0.0017,
      "step": 101520
    },
    {
      "epoch": 5.414933333333333,
      "grad_norm": 0.2588839828968048,
      "learning_rate": 1.6156666666666667e-05,
      "loss": 0.0029,
      "step": 101530
    },
    {
      "epoch": 5.415466666666667,
      "grad_norm": 0.2580057680606842,
      "learning_rate": 1.6153333333333333e-05,
      "loss": 0.0028,
      "step": 101540
    },
    {
      "epoch": 5.416,
      "grad_norm": 0.048648733645677567,
      "learning_rate": 1.6150000000000003e-05,
      "loss": 0.0018,
      "step": 101550
    },
    {
      "epoch": 5.416533333333334,
      "grad_norm": 0.1977590173482895,
      "learning_rate": 1.614666666666667e-05,
      "loss": 0.0012,
      "step": 101560
    },
    {
      "epoch": 5.417066666666667,
      "grad_norm": 0.3457048535346985,
      "learning_rate": 1.6143333333333335e-05,
      "loss": 0.0019,
      "step": 101570
    },
    {
      "epoch": 5.4176,
      "grad_norm": 0.2077339142560959,
      "learning_rate": 1.6139999999999998e-05,
      "loss": 0.0016,
      "step": 101580
    },
    {
      "epoch": 5.4181333333333335,
      "grad_norm": 0.5310207009315491,
      "learning_rate": 1.6136666666666667e-05,
      "loss": 0.002,
      "step": 101590
    },
    {
      "epoch": 5.418666666666667,
      "grad_norm": 0.43440577387809753,
      "learning_rate": 1.6133333333333334e-05,
      "loss": 0.0017,
      "step": 101600
    },
    {
      "epoch": 5.4192,
      "grad_norm": 0.3133060932159424,
      "learning_rate": 1.613e-05,
      "loss": 0.0032,
      "step": 101610
    },
    {
      "epoch": 5.419733333333333,
      "grad_norm": 0.43079686164855957,
      "learning_rate": 1.612666666666667e-05,
      "loss": 0.0017,
      "step": 101620
    },
    {
      "epoch": 5.420266666666667,
      "grad_norm": 0.20584383606910706,
      "learning_rate": 1.6123333333333335e-05,
      "loss": 0.0017,
      "step": 101630
    },
    {
      "epoch": 5.4208,
      "grad_norm": 0.2560090124607086,
      "learning_rate": 1.612e-05,
      "loss": 0.0022,
      "step": 101640
    },
    {
      "epoch": 5.421333333333333,
      "grad_norm": 0.09127502143383026,
      "learning_rate": 1.6116666666666668e-05,
      "loss": 0.0016,
      "step": 101650
    },
    {
      "epoch": 5.421866666666666,
      "grad_norm": 0.15119683742523193,
      "learning_rate": 1.6113333333333334e-05,
      "loss": 0.0016,
      "step": 101660
    },
    {
      "epoch": 5.4224,
      "grad_norm": 0.14704345166683197,
      "learning_rate": 1.611e-05,
      "loss": 0.0017,
      "step": 101670
    },
    {
      "epoch": 5.422933333333333,
      "grad_norm": 0.2340797632932663,
      "learning_rate": 1.6106666666666666e-05,
      "loss": 0.0022,
      "step": 101680
    },
    {
      "epoch": 5.423466666666666,
      "grad_norm": 0.03035600110888481,
      "learning_rate": 1.6103333333333336e-05,
      "loss": 0.0016,
      "step": 101690
    },
    {
      "epoch": 5.424,
      "grad_norm": 0.20710675418376923,
      "learning_rate": 1.6100000000000002e-05,
      "loss": 0.0013,
      "step": 101700
    },
    {
      "epoch": 5.424533333333334,
      "grad_norm": 0.3042047917842865,
      "learning_rate": 1.6096666666666668e-05,
      "loss": 0.0022,
      "step": 101710
    },
    {
      "epoch": 5.425066666666667,
      "grad_norm": 0.21818338334560394,
      "learning_rate": 1.6093333333333334e-05,
      "loss": 0.002,
      "step": 101720
    },
    {
      "epoch": 5.4256,
      "grad_norm": 0.28446486592292786,
      "learning_rate": 1.609e-05,
      "loss": 0.0024,
      "step": 101730
    },
    {
      "epoch": 5.4261333333333335,
      "grad_norm": 0.0679122656583786,
      "learning_rate": 1.6086666666666666e-05,
      "loss": 0.0019,
      "step": 101740
    },
    {
      "epoch": 5.426666666666667,
      "grad_norm": 0.07116533070802689,
      "learning_rate": 1.6083333333333332e-05,
      "loss": 0.0021,
      "step": 101750
    },
    {
      "epoch": 5.4272,
      "grad_norm": 0.1856892853975296,
      "learning_rate": 1.6080000000000002e-05,
      "loss": 0.0015,
      "step": 101760
    },
    {
      "epoch": 5.427733333333333,
      "grad_norm": 0.4644467532634735,
      "learning_rate": 1.6076666666666668e-05,
      "loss": 0.0016,
      "step": 101770
    },
    {
      "epoch": 5.428266666666667,
      "grad_norm": 0.18437084555625916,
      "learning_rate": 1.6073333333333334e-05,
      "loss": 0.0016,
      "step": 101780
    },
    {
      "epoch": 5.4288,
      "grad_norm": 0.04392041638493538,
      "learning_rate": 1.607e-05,
      "loss": 0.002,
      "step": 101790
    },
    {
      "epoch": 5.429333333333333,
      "grad_norm": 0.453787624835968,
      "learning_rate": 1.606666666666667e-05,
      "loss": 0.002,
      "step": 101800
    },
    {
      "epoch": 5.429866666666666,
      "grad_norm": 0.47962188720703125,
      "learning_rate": 1.6063333333333333e-05,
      "loss": 0.0018,
      "step": 101810
    },
    {
      "epoch": 5.4304,
      "grad_norm": 0.36959001421928406,
      "learning_rate": 1.606e-05,
      "loss": 0.0024,
      "step": 101820
    },
    {
      "epoch": 5.430933333333333,
      "grad_norm": 0.1766713261604309,
      "learning_rate": 1.6056666666666668e-05,
      "loss": 0.0015,
      "step": 101830
    },
    {
      "epoch": 5.431466666666667,
      "grad_norm": 0.14911401271820068,
      "learning_rate": 1.6053333333333334e-05,
      "loss": 0.0032,
      "step": 101840
    },
    {
      "epoch": 5.432,
      "grad_norm": 0.2587285041809082,
      "learning_rate": 1.605e-05,
      "loss": 0.0016,
      "step": 101850
    },
    {
      "epoch": 5.432533333333334,
      "grad_norm": 0.2632865309715271,
      "learning_rate": 1.6046666666666667e-05,
      "loss": 0.0022,
      "step": 101860
    },
    {
      "epoch": 5.433066666666667,
      "grad_norm": 0.29348984360694885,
      "learning_rate": 1.6043333333333336e-05,
      "loss": 0.002,
      "step": 101870
    },
    {
      "epoch": 5.4336,
      "grad_norm": 0.2612200975418091,
      "learning_rate": 1.604e-05,
      "loss": 0.002,
      "step": 101880
    },
    {
      "epoch": 5.4341333333333335,
      "grad_norm": 0.14621806144714355,
      "learning_rate": 1.6036666666666665e-05,
      "loss": 0.0027,
      "step": 101890
    },
    {
      "epoch": 5.434666666666667,
      "grad_norm": 0.23845787346363068,
      "learning_rate": 1.6033333333333335e-05,
      "loss": 0.0018,
      "step": 101900
    },
    {
      "epoch": 5.4352,
      "grad_norm": 0.06315301358699799,
      "learning_rate": 1.603e-05,
      "loss": 0.0018,
      "step": 101910
    },
    {
      "epoch": 5.435733333333333,
      "grad_norm": 0.0912824347615242,
      "learning_rate": 1.6026666666666667e-05,
      "loss": 0.0016,
      "step": 101920
    },
    {
      "epoch": 5.436266666666667,
      "grad_norm": 0.38555821776390076,
      "learning_rate": 1.6023333333333333e-05,
      "loss": 0.0027,
      "step": 101930
    },
    {
      "epoch": 5.4368,
      "grad_norm": 0.3788800835609436,
      "learning_rate": 1.6020000000000002e-05,
      "loss": 0.0014,
      "step": 101940
    },
    {
      "epoch": 5.437333333333333,
      "grad_norm": 0.5621898174285889,
      "learning_rate": 1.601666666666667e-05,
      "loss": 0.0024,
      "step": 101950
    },
    {
      "epoch": 5.437866666666666,
      "grad_norm": 0.15418048202991486,
      "learning_rate": 1.601333333333333e-05,
      "loss": 0.0017,
      "step": 101960
    },
    {
      "epoch": 5.4384,
      "grad_norm": 0.2929726541042328,
      "learning_rate": 1.601e-05,
      "loss": 0.0014,
      "step": 101970
    },
    {
      "epoch": 5.438933333333333,
      "grad_norm": 0.5501943230628967,
      "learning_rate": 1.6006666666666667e-05,
      "loss": 0.0015,
      "step": 101980
    },
    {
      "epoch": 5.439466666666666,
      "grad_norm": 0.040728859603405,
      "learning_rate": 1.6003333333333333e-05,
      "loss": 0.0017,
      "step": 101990
    },
    {
      "epoch": 5.44,
      "grad_norm": 0.19784921407699585,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.0013,
      "step": 102000
    },
    {
      "epoch": 5.440533333333334,
      "grad_norm": 0.5483999252319336,
      "learning_rate": 1.599666666666667e-05,
      "loss": 0.0019,
      "step": 102010
    },
    {
      "epoch": 5.441066666666667,
      "grad_norm": 0.06297624111175537,
      "learning_rate": 1.5993333333333335e-05,
      "loss": 0.0018,
      "step": 102020
    },
    {
      "epoch": 5.4416,
      "grad_norm": 0.1073533296585083,
      "learning_rate": 1.599e-05,
      "loss": 0.0029,
      "step": 102030
    },
    {
      "epoch": 5.4421333333333335,
      "grad_norm": 0.4606291949748993,
      "learning_rate": 1.5986666666666667e-05,
      "loss": 0.0016,
      "step": 102040
    },
    {
      "epoch": 5.442666666666667,
      "grad_norm": 0.04591307416558266,
      "learning_rate": 1.5983333333333333e-05,
      "loss": 0.0021,
      "step": 102050
    },
    {
      "epoch": 5.4432,
      "grad_norm": 0.48439615964889526,
      "learning_rate": 1.598e-05,
      "loss": 0.0025,
      "step": 102060
    },
    {
      "epoch": 5.443733333333333,
      "grad_norm": 0.1476127654314041,
      "learning_rate": 1.597666666666667e-05,
      "loss": 0.0026,
      "step": 102070
    },
    {
      "epoch": 5.444266666666667,
      "grad_norm": 0.08968330919742584,
      "learning_rate": 1.5973333333333335e-05,
      "loss": 0.0017,
      "step": 102080
    },
    {
      "epoch": 5.4448,
      "grad_norm": 0.4901513159275055,
      "learning_rate": 1.597e-05,
      "loss": 0.0021,
      "step": 102090
    },
    {
      "epoch": 5.445333333333333,
      "grad_norm": 0.17646931111812592,
      "learning_rate": 1.5966666666666667e-05,
      "loss": 0.0015,
      "step": 102100
    },
    {
      "epoch": 5.445866666666666,
      "grad_norm": 0.18039625883102417,
      "learning_rate": 1.5963333333333334e-05,
      "loss": 0.002,
      "step": 102110
    },
    {
      "epoch": 5.4464,
      "grad_norm": 0.25866633653640747,
      "learning_rate": 1.596e-05,
      "loss": 0.0015,
      "step": 102120
    },
    {
      "epoch": 5.446933333333333,
      "grad_norm": 0.20738305151462555,
      "learning_rate": 1.5956666666666666e-05,
      "loss": 0.0025,
      "step": 102130
    },
    {
      "epoch": 5.447466666666667,
      "grad_norm": 0.3254982531070709,
      "learning_rate": 1.5953333333333335e-05,
      "loss": 0.0021,
      "step": 102140
    },
    {
      "epoch": 5.448,
      "grad_norm": 0.6029219031333923,
      "learning_rate": 1.595e-05,
      "loss": 0.0029,
      "step": 102150
    },
    {
      "epoch": 5.448533333333334,
      "grad_norm": 0.3986179232597351,
      "learning_rate": 1.5946666666666668e-05,
      "loss": 0.0023,
      "step": 102160
    },
    {
      "epoch": 5.449066666666667,
      "grad_norm": 0.1573166698217392,
      "learning_rate": 1.5943333333333334e-05,
      "loss": 0.0026,
      "step": 102170
    },
    {
      "epoch": 5.4496,
      "grad_norm": 0.04977450519800186,
      "learning_rate": 1.594e-05,
      "loss": 0.0026,
      "step": 102180
    },
    {
      "epoch": 5.4501333333333335,
      "grad_norm": 0.07211948931217194,
      "learning_rate": 1.5936666666666666e-05,
      "loss": 0.0013,
      "step": 102190
    },
    {
      "epoch": 5.450666666666667,
      "grad_norm": 0.04353727027773857,
      "learning_rate": 1.5933333333333332e-05,
      "loss": 0.0019,
      "step": 102200
    },
    {
      "epoch": 5.4512,
      "grad_norm": 0.06641213595867157,
      "learning_rate": 1.593e-05,
      "loss": 0.0019,
      "step": 102210
    },
    {
      "epoch": 5.451733333333333,
      "grad_norm": 0.06640074402093887,
      "learning_rate": 1.5926666666666668e-05,
      "loss": 0.0015,
      "step": 102220
    },
    {
      "epoch": 5.452266666666667,
      "grad_norm": 0.20189011096954346,
      "learning_rate": 1.5923333333333334e-05,
      "loss": 0.0014,
      "step": 102230
    },
    {
      "epoch": 5.4528,
      "grad_norm": 0.12451127171516418,
      "learning_rate": 1.592e-05,
      "loss": 0.0025,
      "step": 102240
    },
    {
      "epoch": 5.453333333333333,
      "grad_norm": 0.04922183230519295,
      "learning_rate": 1.591666666666667e-05,
      "loss": 0.0023,
      "step": 102250
    },
    {
      "epoch": 5.453866666666666,
      "grad_norm": 0.09171903133392334,
      "learning_rate": 1.5913333333333332e-05,
      "loss": 0.0029,
      "step": 102260
    },
    {
      "epoch": 5.4544,
      "grad_norm": 0.3204260468482971,
      "learning_rate": 1.591e-05,
      "loss": 0.0019,
      "step": 102270
    },
    {
      "epoch": 5.454933333333333,
      "grad_norm": 0.028546778485178947,
      "learning_rate": 1.5906666666666668e-05,
      "loss": 0.002,
      "step": 102280
    },
    {
      "epoch": 5.455466666666666,
      "grad_norm": 0.19917309284210205,
      "learning_rate": 1.5903333333333334e-05,
      "loss": 0.0013,
      "step": 102290
    },
    {
      "epoch": 5.456,
      "grad_norm": 0.09672456979751587,
      "learning_rate": 1.59e-05,
      "loss": 0.0013,
      "step": 102300
    },
    {
      "epoch": 5.456533333333334,
      "grad_norm": 0.04402396082878113,
      "learning_rate": 1.5896666666666666e-05,
      "loss": 0.0011,
      "step": 102310
    },
    {
      "epoch": 5.457066666666667,
      "grad_norm": 0.20700325071811676,
      "learning_rate": 1.5893333333333336e-05,
      "loss": 0.0022,
      "step": 102320
    },
    {
      "epoch": 5.4576,
      "grad_norm": 0.20111533999443054,
      "learning_rate": 1.5890000000000002e-05,
      "loss": 0.002,
      "step": 102330
    },
    {
      "epoch": 5.4581333333333335,
      "grad_norm": 0.278380811214447,
      "learning_rate": 1.5886666666666665e-05,
      "loss": 0.0013,
      "step": 102340
    },
    {
      "epoch": 5.458666666666667,
      "grad_norm": 0.0736548900604248,
      "learning_rate": 1.5883333333333334e-05,
      "loss": 0.0016,
      "step": 102350
    },
    {
      "epoch": 5.4592,
      "grad_norm": 0.27105727791786194,
      "learning_rate": 1.588e-05,
      "loss": 0.0012,
      "step": 102360
    },
    {
      "epoch": 5.459733333333333,
      "grad_norm": 0.1299276351928711,
      "learning_rate": 1.5876666666666667e-05,
      "loss": 0.0014,
      "step": 102370
    },
    {
      "epoch": 5.460266666666667,
      "grad_norm": 0.32369667291641235,
      "learning_rate": 1.5873333333333336e-05,
      "loss": 0.0017,
      "step": 102380
    },
    {
      "epoch": 5.4608,
      "grad_norm": 0.1509569138288498,
      "learning_rate": 1.5870000000000002e-05,
      "loss": 0.002,
      "step": 102390
    },
    {
      "epoch": 5.461333333333333,
      "grad_norm": 0.10086718946695328,
      "learning_rate": 1.586666666666667e-05,
      "loss": 0.0015,
      "step": 102400
    },
    {
      "epoch": 5.461866666666666,
      "grad_norm": 0.17188405990600586,
      "learning_rate": 1.5863333333333334e-05,
      "loss": 0.0013,
      "step": 102410
    },
    {
      "epoch": 5.4624,
      "grad_norm": 0.12180966138839722,
      "learning_rate": 1.586e-05,
      "loss": 0.0017,
      "step": 102420
    },
    {
      "epoch": 5.462933333333333,
      "grad_norm": 0.2299923598766327,
      "learning_rate": 1.5856666666666667e-05,
      "loss": 0.0021,
      "step": 102430
    },
    {
      "epoch": 5.463466666666667,
      "grad_norm": 0.17044122517108917,
      "learning_rate": 1.5853333333333333e-05,
      "loss": 0.0018,
      "step": 102440
    },
    {
      "epoch": 5.464,
      "grad_norm": 0.03793082386255264,
      "learning_rate": 1.5850000000000002e-05,
      "loss": 0.0016,
      "step": 102450
    },
    {
      "epoch": 5.464533333333334,
      "grad_norm": 0.023336974903941154,
      "learning_rate": 1.584666666666667e-05,
      "loss": 0.0012,
      "step": 102460
    },
    {
      "epoch": 5.465066666666667,
      "grad_norm": 0.46463578939437866,
      "learning_rate": 1.5843333333333335e-05,
      "loss": 0.0016,
      "step": 102470
    },
    {
      "epoch": 5.4656,
      "grad_norm": 0.4150809347629547,
      "learning_rate": 1.584e-05,
      "loss": 0.0017,
      "step": 102480
    },
    {
      "epoch": 5.4661333333333335,
      "grad_norm": 0.3163145184516907,
      "learning_rate": 1.5836666666666667e-05,
      "loss": 0.0023,
      "step": 102490
    },
    {
      "epoch": 5.466666666666667,
      "grad_norm": 0.05029282346367836,
      "learning_rate": 1.5833333333333333e-05,
      "loss": 0.0016,
      "step": 102500
    },
    {
      "epoch": 5.4672,
      "grad_norm": 0.22460617125034332,
      "learning_rate": 1.583e-05,
      "loss": 0.0029,
      "step": 102510
    },
    {
      "epoch": 5.467733333333333,
      "grad_norm": 0.059929490089416504,
      "learning_rate": 1.582666666666667e-05,
      "loss": 0.0021,
      "step": 102520
    },
    {
      "epoch": 5.468266666666667,
      "grad_norm": 0.10768494009971619,
      "learning_rate": 1.5823333333333335e-05,
      "loss": 0.0023,
      "step": 102530
    },
    {
      "epoch": 5.4688,
      "grad_norm": 0.06762682646512985,
      "learning_rate": 1.582e-05,
      "loss": 0.0023,
      "step": 102540
    },
    {
      "epoch": 5.469333333333333,
      "grad_norm": 0.6806836724281311,
      "learning_rate": 1.5816666666666667e-05,
      "loss": 0.0021,
      "step": 102550
    },
    {
      "epoch": 5.469866666666666,
      "grad_norm": 0.26465409994125366,
      "learning_rate": 1.5813333333333333e-05,
      "loss": 0.0021,
      "step": 102560
    },
    {
      "epoch": 5.4704,
      "grad_norm": 0.07093387097120285,
      "learning_rate": 1.581e-05,
      "loss": 0.0018,
      "step": 102570
    },
    {
      "epoch": 5.470933333333333,
      "grad_norm": 0.20711539685726166,
      "learning_rate": 1.5806666666666666e-05,
      "loss": 0.0023,
      "step": 102580
    },
    {
      "epoch": 5.471466666666666,
      "grad_norm": 0.28483593463897705,
      "learning_rate": 1.5803333333333335e-05,
      "loss": 0.0029,
      "step": 102590
    },
    {
      "epoch": 5.4719999999999995,
      "grad_norm": 0.5475324392318726,
      "learning_rate": 1.58e-05,
      "loss": 0.0017,
      "step": 102600
    },
    {
      "epoch": 5.472533333333334,
      "grad_norm": 0.06695426255464554,
      "learning_rate": 1.5796666666666667e-05,
      "loss": 0.0022,
      "step": 102610
    },
    {
      "epoch": 5.473066666666667,
      "grad_norm": 0.20322297513484955,
      "learning_rate": 1.5793333333333333e-05,
      "loss": 0.0021,
      "step": 102620
    },
    {
      "epoch": 5.4736,
      "grad_norm": 0.17205040156841278,
      "learning_rate": 1.5790000000000003e-05,
      "loss": 0.0015,
      "step": 102630
    },
    {
      "epoch": 5.4741333333333335,
      "grad_norm": 0.1728697419166565,
      "learning_rate": 1.5786666666666666e-05,
      "loss": 0.002,
      "step": 102640
    },
    {
      "epoch": 5.474666666666667,
      "grad_norm": 0.0628829374909401,
      "learning_rate": 1.5783333333333332e-05,
      "loss": 0.0023,
      "step": 102650
    },
    {
      "epoch": 5.4752,
      "grad_norm": 0.06714800000190735,
      "learning_rate": 1.578e-05,
      "loss": 0.0022,
      "step": 102660
    },
    {
      "epoch": 5.475733333333333,
      "grad_norm": 0.04117788374423981,
      "learning_rate": 1.5776666666666668e-05,
      "loss": 0.0019,
      "step": 102670
    },
    {
      "epoch": 5.476266666666667,
      "grad_norm": 0.051824867725372314,
      "learning_rate": 1.5773333333333334e-05,
      "loss": 0.0018,
      "step": 102680
    },
    {
      "epoch": 5.4768,
      "grad_norm": 0.044871170073747635,
      "learning_rate": 1.577e-05,
      "loss": 0.0016,
      "step": 102690
    },
    {
      "epoch": 5.477333333333333,
      "grad_norm": 0.28806236386299133,
      "learning_rate": 1.576666666666667e-05,
      "loss": 0.0025,
      "step": 102700
    },
    {
      "epoch": 5.477866666666666,
      "grad_norm": 0.03685345500707626,
      "learning_rate": 1.5763333333333332e-05,
      "loss": 0.0018,
      "step": 102710
    },
    {
      "epoch": 5.4784,
      "grad_norm": 0.11604846268892288,
      "learning_rate": 1.5759999999999998e-05,
      "loss": 0.0029,
      "step": 102720
    },
    {
      "epoch": 5.478933333333333,
      "grad_norm": 0.46656304597854614,
      "learning_rate": 1.5756666666666668e-05,
      "loss": 0.0017,
      "step": 102730
    },
    {
      "epoch": 5.479466666666666,
      "grad_norm": 0.17272023856639862,
      "learning_rate": 1.5753333333333334e-05,
      "loss": 0.0026,
      "step": 102740
    },
    {
      "epoch": 5.48,
      "grad_norm": 0.2836054265499115,
      "learning_rate": 1.575e-05,
      "loss": 0.0017,
      "step": 102750
    },
    {
      "epoch": 5.480533333333334,
      "grad_norm": 0.1734294593334198,
      "learning_rate": 1.574666666666667e-05,
      "loss": 0.0021,
      "step": 102760
    },
    {
      "epoch": 5.481066666666667,
      "grad_norm": 0.12438333034515381,
      "learning_rate": 1.5743333333333336e-05,
      "loss": 0.0021,
      "step": 102770
    },
    {
      "epoch": 5.4816,
      "grad_norm": 0.17299313843250275,
      "learning_rate": 1.5740000000000002e-05,
      "loss": 0.0025,
      "step": 102780
    },
    {
      "epoch": 5.4821333333333335,
      "grad_norm": 0.20792843401432037,
      "learning_rate": 1.5736666666666668e-05,
      "loss": 0.002,
      "step": 102790
    },
    {
      "epoch": 5.482666666666667,
      "grad_norm": 0.09184995293617249,
      "learning_rate": 1.5733333333333334e-05,
      "loss": 0.0019,
      "step": 102800
    },
    {
      "epoch": 5.4832,
      "grad_norm": 0.06878744065761566,
      "learning_rate": 1.573e-05,
      "loss": 0.0014,
      "step": 102810
    },
    {
      "epoch": 5.483733333333333,
      "grad_norm": 0.11814668774604797,
      "learning_rate": 1.5726666666666666e-05,
      "loss": 0.0019,
      "step": 102820
    },
    {
      "epoch": 5.484266666666667,
      "grad_norm": 0.11959752440452576,
      "learning_rate": 1.5723333333333336e-05,
      "loss": 0.0024,
      "step": 102830
    },
    {
      "epoch": 5.4848,
      "grad_norm": 0.038831889629364014,
      "learning_rate": 1.5720000000000002e-05,
      "loss": 0.0014,
      "step": 102840
    },
    {
      "epoch": 5.485333333333333,
      "grad_norm": 0.06910037249326706,
      "learning_rate": 1.5716666666666668e-05,
      "loss": 0.002,
      "step": 102850
    },
    {
      "epoch": 5.4858666666666664,
      "grad_norm": 0.0804089829325676,
      "learning_rate": 1.5713333333333334e-05,
      "loss": 0.0014,
      "step": 102860
    },
    {
      "epoch": 5.4864,
      "grad_norm": 0.3895220458507538,
      "learning_rate": 1.571e-05,
      "loss": 0.0016,
      "step": 102870
    },
    {
      "epoch": 5.486933333333333,
      "grad_norm": 0.1492140144109726,
      "learning_rate": 1.5706666666666666e-05,
      "loss": 0.0017,
      "step": 102880
    },
    {
      "epoch": 5.487466666666666,
      "grad_norm": 0.04707591235637665,
      "learning_rate": 1.5703333333333333e-05,
      "loss": 0.0015,
      "step": 102890
    },
    {
      "epoch": 5.4879999999999995,
      "grad_norm": 0.32339736819267273,
      "learning_rate": 1.5700000000000002e-05,
      "loss": 0.0022,
      "step": 102900
    },
    {
      "epoch": 5.488533333333334,
      "grad_norm": 0.07276801764965057,
      "learning_rate": 1.5696666666666668e-05,
      "loss": 0.0015,
      "step": 102910
    },
    {
      "epoch": 5.489066666666667,
      "grad_norm": 0.042742762714624405,
      "learning_rate": 1.5693333333333334e-05,
      "loss": 0.0017,
      "step": 102920
    },
    {
      "epoch": 5.4896,
      "grad_norm": 0.027817996218800545,
      "learning_rate": 1.569e-05,
      "loss": 0.0017,
      "step": 102930
    },
    {
      "epoch": 5.4901333333333335,
      "grad_norm": 0.27896085381507874,
      "learning_rate": 1.5686666666666667e-05,
      "loss": 0.002,
      "step": 102940
    },
    {
      "epoch": 5.490666666666667,
      "grad_norm": 0.1954898089170456,
      "learning_rate": 1.5683333333333333e-05,
      "loss": 0.0016,
      "step": 102950
    },
    {
      "epoch": 5.4912,
      "grad_norm": 0.31593215465545654,
      "learning_rate": 1.568e-05,
      "loss": 0.0021,
      "step": 102960
    },
    {
      "epoch": 5.491733333333333,
      "grad_norm": 0.050776250660419464,
      "learning_rate": 1.567666666666667e-05,
      "loss": 0.0032,
      "step": 102970
    },
    {
      "epoch": 5.492266666666667,
      "grad_norm": 0.20075708627700806,
      "learning_rate": 1.5673333333333335e-05,
      "loss": 0.0023,
      "step": 102980
    },
    {
      "epoch": 5.4928,
      "grad_norm": 0.6595177054405212,
      "learning_rate": 1.567e-05,
      "loss": 0.0028,
      "step": 102990
    },
    {
      "epoch": 5.493333333333333,
      "grad_norm": 0.14431466162204742,
      "learning_rate": 1.5666666666666667e-05,
      "loss": 0.0019,
      "step": 103000
    },
    {
      "epoch": 5.4938666666666665,
      "grad_norm": 0.022163696587085724,
      "learning_rate": 1.5663333333333336e-05,
      "loss": 0.0022,
      "step": 103010
    },
    {
      "epoch": 5.4944,
      "grad_norm": 0.45235636830329895,
      "learning_rate": 1.566e-05,
      "loss": 0.0021,
      "step": 103020
    },
    {
      "epoch": 5.494933333333333,
      "grad_norm": 0.46211743354797363,
      "learning_rate": 1.5656666666666665e-05,
      "loss": 0.0013,
      "step": 103030
    },
    {
      "epoch": 5.495466666666666,
      "grad_norm": 0.07156099379062653,
      "learning_rate": 1.5653333333333335e-05,
      "loss": 0.0022,
      "step": 103040
    },
    {
      "epoch": 5.496,
      "grad_norm": 0.4584156274795532,
      "learning_rate": 1.565e-05,
      "loss": 0.0021,
      "step": 103050
    },
    {
      "epoch": 5.496533333333334,
      "grad_norm": 0.19914555549621582,
      "learning_rate": 1.5646666666666667e-05,
      "loss": 0.0031,
      "step": 103060
    },
    {
      "epoch": 5.497066666666667,
      "grad_norm": 0.18752314150333405,
      "learning_rate": 1.5643333333333333e-05,
      "loss": 0.0025,
      "step": 103070
    },
    {
      "epoch": 5.4976,
      "grad_norm": 0.20161184668540955,
      "learning_rate": 1.5640000000000003e-05,
      "loss": 0.0016,
      "step": 103080
    },
    {
      "epoch": 5.4981333333333335,
      "grad_norm": 0.41327589750289917,
      "learning_rate": 1.5636666666666665e-05,
      "loss": 0.0023,
      "step": 103090
    },
    {
      "epoch": 5.498666666666667,
      "grad_norm": 0.4176785945892334,
      "learning_rate": 1.563333333333333e-05,
      "loss": 0.0024,
      "step": 103100
    },
    {
      "epoch": 5.4992,
      "grad_norm": 0.09033399820327759,
      "learning_rate": 1.563e-05,
      "loss": 0.0018,
      "step": 103110
    },
    {
      "epoch": 5.499733333333333,
      "grad_norm": 0.27965283393859863,
      "learning_rate": 1.5626666666666667e-05,
      "loss": 0.0021,
      "step": 103120
    },
    {
      "epoch": 5.500266666666667,
      "grad_norm": 0.33347567915916443,
      "learning_rate": 1.5623333333333333e-05,
      "loss": 0.0023,
      "step": 103130
    },
    {
      "epoch": 5.5008,
      "grad_norm": 0.25948336720466614,
      "learning_rate": 1.5620000000000003e-05,
      "loss": 0.0016,
      "step": 103140
    },
    {
      "epoch": 5.501333333333333,
      "grad_norm": 0.17139458656311035,
      "learning_rate": 1.561666666666667e-05,
      "loss": 0.0019,
      "step": 103150
    },
    {
      "epoch": 5.5018666666666665,
      "grad_norm": 0.09696990251541138,
      "learning_rate": 1.5613333333333335e-05,
      "loss": 0.0013,
      "step": 103160
    },
    {
      "epoch": 5.5024,
      "grad_norm": 0.48559126257896423,
      "learning_rate": 1.561e-05,
      "loss": 0.0017,
      "step": 103170
    },
    {
      "epoch": 5.502933333333333,
      "grad_norm": 0.06188863143324852,
      "learning_rate": 1.5606666666666667e-05,
      "loss": 0.0015,
      "step": 103180
    },
    {
      "epoch": 5.503466666666666,
      "grad_norm": 0.17175568640232086,
      "learning_rate": 1.5603333333333334e-05,
      "loss": 0.0037,
      "step": 103190
    },
    {
      "epoch": 5.504,
      "grad_norm": 0.15085169672966003,
      "learning_rate": 1.56e-05,
      "loss": 0.0014,
      "step": 103200
    },
    {
      "epoch": 5.504533333333333,
      "grad_norm": 0.049037426710128784,
      "learning_rate": 1.559666666666667e-05,
      "loss": 0.0021,
      "step": 103210
    },
    {
      "epoch": 5.505066666666667,
      "grad_norm": 0.0364728718996048,
      "learning_rate": 1.5593333333333335e-05,
      "loss": 0.0026,
      "step": 103220
    },
    {
      "epoch": 5.5056,
      "grad_norm": 0.35228341817855835,
      "learning_rate": 1.559e-05,
      "loss": 0.0018,
      "step": 103230
    },
    {
      "epoch": 5.5061333333333335,
      "grad_norm": 0.29080528020858765,
      "learning_rate": 1.5586666666666668e-05,
      "loss": 0.0016,
      "step": 103240
    },
    {
      "epoch": 5.506666666666667,
      "grad_norm": 0.17228196561336517,
      "learning_rate": 1.5583333333333334e-05,
      "loss": 0.0018,
      "step": 103250
    },
    {
      "epoch": 5.5072,
      "grad_norm": 0.04821754992008209,
      "learning_rate": 1.558e-05,
      "loss": 0.0021,
      "step": 103260
    },
    {
      "epoch": 5.507733333333333,
      "grad_norm": 0.21664582192897797,
      "learning_rate": 1.5576666666666666e-05,
      "loss": 0.0017,
      "step": 103270
    },
    {
      "epoch": 5.508266666666667,
      "grad_norm": 0.10352347791194916,
      "learning_rate": 1.5573333333333336e-05,
      "loss": 0.0018,
      "step": 103280
    },
    {
      "epoch": 5.5088,
      "grad_norm": 0.41265395283699036,
      "learning_rate": 1.5570000000000002e-05,
      "loss": 0.0028,
      "step": 103290
    },
    {
      "epoch": 5.509333333333333,
      "grad_norm": 0.1751602441072464,
      "learning_rate": 1.5566666666666668e-05,
      "loss": 0.0014,
      "step": 103300
    },
    {
      "epoch": 5.5098666666666665,
      "grad_norm": 0.02678600698709488,
      "learning_rate": 1.5563333333333334e-05,
      "loss": 0.0014,
      "step": 103310
    },
    {
      "epoch": 5.5104,
      "grad_norm": 0.13207077980041504,
      "learning_rate": 1.556e-05,
      "loss": 0.0019,
      "step": 103320
    },
    {
      "epoch": 5.510933333333333,
      "grad_norm": 0.1755322813987732,
      "learning_rate": 1.5556666666666666e-05,
      "loss": 0.002,
      "step": 103330
    },
    {
      "epoch": 5.511466666666666,
      "grad_norm": 0.26637211441993713,
      "learning_rate": 1.5553333333333332e-05,
      "loss": 0.0025,
      "step": 103340
    },
    {
      "epoch": 5.5120000000000005,
      "grad_norm": 0.05558711662888527,
      "learning_rate": 1.5550000000000002e-05,
      "loss": 0.0019,
      "step": 103350
    },
    {
      "epoch": 5.512533333333334,
      "grad_norm": 0.23811781406402588,
      "learning_rate": 1.5546666666666668e-05,
      "loss": 0.0018,
      "step": 103360
    },
    {
      "epoch": 5.513066666666667,
      "grad_norm": 0.18188734352588654,
      "learning_rate": 1.5543333333333334e-05,
      "loss": 0.0015,
      "step": 103370
    },
    {
      "epoch": 5.5136,
      "grad_norm": 0.20138992369174957,
      "learning_rate": 1.554e-05,
      "loss": 0.0021,
      "step": 103380
    },
    {
      "epoch": 5.5141333333333336,
      "grad_norm": 0.3216771185398102,
      "learning_rate": 1.5536666666666666e-05,
      "loss": 0.0016,
      "step": 103390
    },
    {
      "epoch": 5.514666666666667,
      "grad_norm": 0.2564801573753357,
      "learning_rate": 1.5533333333333333e-05,
      "loss": 0.002,
      "step": 103400
    },
    {
      "epoch": 5.5152,
      "grad_norm": 0.3805857300758362,
      "learning_rate": 1.553e-05,
      "loss": 0.0014,
      "step": 103410
    },
    {
      "epoch": 5.515733333333333,
      "grad_norm": 0.35774731636047363,
      "learning_rate": 1.5526666666666668e-05,
      "loss": 0.0017,
      "step": 103420
    },
    {
      "epoch": 5.516266666666667,
      "grad_norm": 0.12379124760627747,
      "learning_rate": 1.5523333333333334e-05,
      "loss": 0.0015,
      "step": 103430
    },
    {
      "epoch": 5.5168,
      "grad_norm": 0.03597619757056236,
      "learning_rate": 1.552e-05,
      "loss": 0.0015,
      "step": 103440
    },
    {
      "epoch": 5.517333333333333,
      "grad_norm": 0.16803385317325592,
      "learning_rate": 1.5516666666666667e-05,
      "loss": 0.003,
      "step": 103450
    },
    {
      "epoch": 5.5178666666666665,
      "grad_norm": 0.07599785178899765,
      "learning_rate": 1.5513333333333336e-05,
      "loss": 0.0012,
      "step": 103460
    },
    {
      "epoch": 5.5184,
      "grad_norm": 0.11902428418397903,
      "learning_rate": 1.551e-05,
      "loss": 0.0023,
      "step": 103470
    },
    {
      "epoch": 5.518933333333333,
      "grad_norm": 0.5122249126434326,
      "learning_rate": 1.5506666666666665e-05,
      "loss": 0.0019,
      "step": 103480
    },
    {
      "epoch": 5.519466666666666,
      "grad_norm": 0.329809308052063,
      "learning_rate": 1.5503333333333335e-05,
      "loss": 0.0013,
      "step": 103490
    },
    {
      "epoch": 5.52,
      "grad_norm": 0.34093743562698364,
      "learning_rate": 1.55e-05,
      "loss": 0.0019,
      "step": 103500
    },
    {
      "epoch": 5.520533333333333,
      "grad_norm": 0.1738535612821579,
      "learning_rate": 1.5496666666666667e-05,
      "loss": 0.0013,
      "step": 103510
    },
    {
      "epoch": 5.521066666666667,
      "grad_norm": 0.06791448593139648,
      "learning_rate": 1.5493333333333336e-05,
      "loss": 0.0017,
      "step": 103520
    },
    {
      "epoch": 5.5216,
      "grad_norm": 0.4308071434497833,
      "learning_rate": 1.5490000000000002e-05,
      "loss": 0.0016,
      "step": 103530
    },
    {
      "epoch": 5.522133333333334,
      "grad_norm": 0.20062139630317688,
      "learning_rate": 1.548666666666667e-05,
      "loss": 0.0016,
      "step": 103540
    },
    {
      "epoch": 5.522666666666667,
      "grad_norm": 0.1628616452217102,
      "learning_rate": 1.548333333333333e-05,
      "loss": 0.0019,
      "step": 103550
    },
    {
      "epoch": 5.5232,
      "grad_norm": 0.22604523599147797,
      "learning_rate": 1.548e-05,
      "loss": 0.0017,
      "step": 103560
    },
    {
      "epoch": 5.523733333333333,
      "grad_norm": 0.07732919603586197,
      "learning_rate": 1.5476666666666667e-05,
      "loss": 0.0019,
      "step": 103570
    },
    {
      "epoch": 5.524266666666667,
      "grad_norm": 0.32192856073379517,
      "learning_rate": 1.5473333333333333e-05,
      "loss": 0.0021,
      "step": 103580
    },
    {
      "epoch": 5.5248,
      "grad_norm": 0.03563439100980759,
      "learning_rate": 1.5470000000000003e-05,
      "loss": 0.0017,
      "step": 103590
    },
    {
      "epoch": 5.525333333333333,
      "grad_norm": 0.07168560475111008,
      "learning_rate": 1.546666666666667e-05,
      "loss": 0.0017,
      "step": 103600
    },
    {
      "epoch": 5.5258666666666665,
      "grad_norm": 0.04775163531303406,
      "learning_rate": 1.5463333333333335e-05,
      "loss": 0.0018,
      "step": 103610
    },
    {
      "epoch": 5.5264,
      "grad_norm": 0.07316426187753677,
      "learning_rate": 1.546e-05,
      "loss": 0.0021,
      "step": 103620
    },
    {
      "epoch": 5.526933333333333,
      "grad_norm": 0.2546475827693939,
      "learning_rate": 1.5456666666666667e-05,
      "loss": 0.0023,
      "step": 103630
    },
    {
      "epoch": 5.527466666666666,
      "grad_norm": 0.2109488844871521,
      "learning_rate": 1.5453333333333333e-05,
      "loss": 0.0011,
      "step": 103640
    },
    {
      "epoch": 5.5280000000000005,
      "grad_norm": 0.19826681911945343,
      "learning_rate": 1.545e-05,
      "loss": 0.0013,
      "step": 103650
    },
    {
      "epoch": 5.528533333333334,
      "grad_norm": 0.12325470894575119,
      "learning_rate": 1.544666666666667e-05,
      "loss": 0.002,
      "step": 103660
    },
    {
      "epoch": 5.529066666666667,
      "grad_norm": 0.09266561269760132,
      "learning_rate": 1.5443333333333335e-05,
      "loss": 0.0014,
      "step": 103670
    },
    {
      "epoch": 5.5296,
      "grad_norm": 0.20397798717021942,
      "learning_rate": 1.544e-05,
      "loss": 0.0014,
      "step": 103680
    },
    {
      "epoch": 5.530133333333334,
      "grad_norm": 0.38257738947868347,
      "learning_rate": 1.5436666666666667e-05,
      "loss": 0.0021,
      "step": 103690
    },
    {
      "epoch": 5.530666666666667,
      "grad_norm": 0.11980722844600677,
      "learning_rate": 1.5433333333333334e-05,
      "loss": 0.0014,
      "step": 103700
    },
    {
      "epoch": 5.5312,
      "grad_norm": 0.3525207042694092,
      "learning_rate": 1.543e-05,
      "loss": 0.002,
      "step": 103710
    },
    {
      "epoch": 5.531733333333333,
      "grad_norm": 0.2667052447795868,
      "learning_rate": 1.5426666666666666e-05,
      "loss": 0.0018,
      "step": 103720
    },
    {
      "epoch": 5.532266666666667,
      "grad_norm": 0.2883255183696747,
      "learning_rate": 1.5423333333333335e-05,
      "loss": 0.003,
      "step": 103730
    },
    {
      "epoch": 5.5328,
      "grad_norm": 0.49029460549354553,
      "learning_rate": 1.542e-05,
      "loss": 0.002,
      "step": 103740
    },
    {
      "epoch": 5.533333333333333,
      "grad_norm": 0.2893250584602356,
      "learning_rate": 1.5416666666666668e-05,
      "loss": 0.0016,
      "step": 103750
    },
    {
      "epoch": 5.5338666666666665,
      "grad_norm": 0.23132675886154175,
      "learning_rate": 1.5413333333333334e-05,
      "loss": 0.002,
      "step": 103760
    },
    {
      "epoch": 5.5344,
      "grad_norm": 0.6713091135025024,
      "learning_rate": 1.541e-05,
      "loss": 0.0018,
      "step": 103770
    },
    {
      "epoch": 5.534933333333333,
      "grad_norm": 0.48751628398895264,
      "learning_rate": 1.5406666666666666e-05,
      "loss": 0.0011,
      "step": 103780
    },
    {
      "epoch": 5.535466666666666,
      "grad_norm": 0.07173512876033783,
      "learning_rate": 1.5403333333333332e-05,
      "loss": 0.002,
      "step": 103790
    },
    {
      "epoch": 5.536,
      "grad_norm": 0.2926497459411621,
      "learning_rate": 1.54e-05,
      "loss": 0.0019,
      "step": 103800
    },
    {
      "epoch": 5.536533333333333,
      "grad_norm": 0.17359307408332825,
      "learning_rate": 1.5396666666666668e-05,
      "loss": 0.0024,
      "step": 103810
    },
    {
      "epoch": 5.537066666666667,
      "grad_norm": 0.07462673634290695,
      "learning_rate": 1.5393333333333334e-05,
      "loss": 0.0017,
      "step": 103820
    },
    {
      "epoch": 5.5376,
      "grad_norm": 0.20112363994121552,
      "learning_rate": 1.539e-05,
      "loss": 0.0028,
      "step": 103830
    },
    {
      "epoch": 5.538133333333334,
      "grad_norm": 0.08927231281995773,
      "learning_rate": 1.538666666666667e-05,
      "loss": 0.0015,
      "step": 103840
    },
    {
      "epoch": 5.538666666666667,
      "grad_norm": 0.0875094011425972,
      "learning_rate": 1.5383333333333332e-05,
      "loss": 0.0012,
      "step": 103850
    },
    {
      "epoch": 5.5392,
      "grad_norm": 0.46903857588768005,
      "learning_rate": 1.538e-05,
      "loss": 0.0018,
      "step": 103860
    },
    {
      "epoch": 5.539733333333333,
      "grad_norm": 0.0672469511628151,
      "learning_rate": 1.5376666666666668e-05,
      "loss": 0.0016,
      "step": 103870
    },
    {
      "epoch": 5.540266666666667,
      "grad_norm": 0.5540043115615845,
      "learning_rate": 1.5373333333333334e-05,
      "loss": 0.0014,
      "step": 103880
    },
    {
      "epoch": 5.5408,
      "grad_norm": 0.17270125448703766,
      "learning_rate": 1.537e-05,
      "loss": 0.0016,
      "step": 103890
    },
    {
      "epoch": 5.541333333333333,
      "grad_norm": 0.12547679245471954,
      "learning_rate": 1.536666666666667e-05,
      "loss": 0.0017,
      "step": 103900
    },
    {
      "epoch": 5.5418666666666665,
      "grad_norm": 0.15957266092300415,
      "learning_rate": 1.5363333333333336e-05,
      "loss": 0.002,
      "step": 103910
    },
    {
      "epoch": 5.5424,
      "grad_norm": 0.12413487583398819,
      "learning_rate": 1.536e-05,
      "loss": 0.0016,
      "step": 103920
    },
    {
      "epoch": 5.542933333333333,
      "grad_norm": 0.2522312104701996,
      "learning_rate": 1.5356666666666665e-05,
      "loss": 0.0016,
      "step": 103930
    },
    {
      "epoch": 5.543466666666666,
      "grad_norm": 0.20166842639446259,
      "learning_rate": 1.5353333333333334e-05,
      "loss": 0.0019,
      "step": 103940
    },
    {
      "epoch": 5.5440000000000005,
      "grad_norm": 0.28065821528434753,
      "learning_rate": 1.535e-05,
      "loss": 0.0027,
      "step": 103950
    },
    {
      "epoch": 5.544533333333334,
      "grad_norm": 0.09339733421802521,
      "learning_rate": 1.5346666666666667e-05,
      "loss": 0.0019,
      "step": 103960
    },
    {
      "epoch": 5.545066666666667,
      "grad_norm": 0.20406681299209595,
      "learning_rate": 1.5343333333333336e-05,
      "loss": 0.0017,
      "step": 103970
    },
    {
      "epoch": 5.5456,
      "grad_norm": 0.1519075632095337,
      "learning_rate": 1.5340000000000002e-05,
      "loss": 0.0021,
      "step": 103980
    },
    {
      "epoch": 5.546133333333334,
      "grad_norm": 0.29350587725639343,
      "learning_rate": 1.533666666666667e-05,
      "loss": 0.0031,
      "step": 103990
    },
    {
      "epoch": 5.546666666666667,
      "grad_norm": 0.13446706533432007,
      "learning_rate": 1.5333333333333334e-05,
      "loss": 0.0024,
      "step": 104000
    },
    {
      "epoch": 5.5472,
      "grad_norm": 0.1378474235534668,
      "learning_rate": 1.533e-05,
      "loss": 0.0015,
      "step": 104010
    },
    {
      "epoch": 5.547733333333333,
      "grad_norm": 0.12179315835237503,
      "learning_rate": 1.5326666666666667e-05,
      "loss": 0.0014,
      "step": 104020
    },
    {
      "epoch": 5.548266666666667,
      "grad_norm": 0.23757119476795197,
      "learning_rate": 1.5323333333333333e-05,
      "loss": 0.0016,
      "step": 104030
    },
    {
      "epoch": 5.5488,
      "grad_norm": 0.06280241906642914,
      "learning_rate": 1.5320000000000002e-05,
      "loss": 0.0014,
      "step": 104040
    },
    {
      "epoch": 5.549333333333333,
      "grad_norm": 0.11487524956464767,
      "learning_rate": 1.531666666666667e-05,
      "loss": 0.0025,
      "step": 104050
    },
    {
      "epoch": 5.5498666666666665,
      "grad_norm": 0.21106941998004913,
      "learning_rate": 1.5313333333333335e-05,
      "loss": 0.0017,
      "step": 104060
    },
    {
      "epoch": 5.5504,
      "grad_norm": 0.06910575181245804,
      "learning_rate": 1.531e-05,
      "loss": 0.0015,
      "step": 104070
    },
    {
      "epoch": 5.550933333333333,
      "grad_norm": 0.23400159180164337,
      "learning_rate": 1.5306666666666667e-05,
      "loss": 0.0022,
      "step": 104080
    },
    {
      "epoch": 5.551466666666666,
      "grad_norm": 0.10316800326108932,
      "learning_rate": 1.5303333333333333e-05,
      "loss": 0.0015,
      "step": 104090
    },
    {
      "epoch": 5.552,
      "grad_norm": 0.2928686738014221,
      "learning_rate": 1.53e-05,
      "loss": 0.0014,
      "step": 104100
    },
    {
      "epoch": 5.552533333333333,
      "grad_norm": 0.6252573132514954,
      "learning_rate": 1.529666666666667e-05,
      "loss": 0.0023,
      "step": 104110
    },
    {
      "epoch": 5.553066666666667,
      "grad_norm": 0.27662980556488037,
      "learning_rate": 1.5293333333333335e-05,
      "loss": 0.0014,
      "step": 104120
    },
    {
      "epoch": 5.5536,
      "grad_norm": 0.13958702981472015,
      "learning_rate": 1.529e-05,
      "loss": 0.0028,
      "step": 104130
    },
    {
      "epoch": 5.554133333333334,
      "grad_norm": 0.06941494345664978,
      "learning_rate": 1.5286666666666667e-05,
      "loss": 0.0014,
      "step": 104140
    },
    {
      "epoch": 5.554666666666667,
      "grad_norm": 0.14622288942337036,
      "learning_rate": 1.5283333333333333e-05,
      "loss": 0.0018,
      "step": 104150
    },
    {
      "epoch": 5.5552,
      "grad_norm": 0.12444140017032623,
      "learning_rate": 1.528e-05,
      "loss": 0.0021,
      "step": 104160
    },
    {
      "epoch": 5.555733333333333,
      "grad_norm": 0.31259357929229736,
      "learning_rate": 1.5276666666666666e-05,
      "loss": 0.0017,
      "step": 104170
    },
    {
      "epoch": 5.556266666666667,
      "grad_norm": 0.31860414147377014,
      "learning_rate": 1.5273333333333335e-05,
      "loss": 0.0015,
      "step": 104180
    },
    {
      "epoch": 5.5568,
      "grad_norm": 0.0602109432220459,
      "learning_rate": 1.527e-05,
      "loss": 0.0029,
      "step": 104190
    },
    {
      "epoch": 5.557333333333333,
      "grad_norm": 0.077138751745224,
      "learning_rate": 1.5266666666666667e-05,
      "loss": 0.0012,
      "step": 104200
    },
    {
      "epoch": 5.5578666666666665,
      "grad_norm": 0.2882671654224396,
      "learning_rate": 1.5263333333333333e-05,
      "loss": 0.0018,
      "step": 104210
    },
    {
      "epoch": 5.5584,
      "grad_norm": 0.6018536686897278,
      "learning_rate": 1.5260000000000003e-05,
      "loss": 0.0027,
      "step": 104220
    },
    {
      "epoch": 5.558933333333333,
      "grad_norm": 0.15251424908638,
      "learning_rate": 1.5256666666666666e-05,
      "loss": 0.0024,
      "step": 104230
    },
    {
      "epoch": 5.559466666666666,
      "grad_norm": 0.2728387415409088,
      "learning_rate": 1.5253333333333334e-05,
      "loss": 0.0017,
      "step": 104240
    },
    {
      "epoch": 5.5600000000000005,
      "grad_norm": 0.1757287234067917,
      "learning_rate": 1.525e-05,
      "loss": 0.0012,
      "step": 104250
    },
    {
      "epoch": 5.560533333333334,
      "grad_norm": 0.14885151386260986,
      "learning_rate": 1.5246666666666668e-05,
      "loss": 0.0024,
      "step": 104260
    },
    {
      "epoch": 5.561066666666667,
      "grad_norm": 0.093747578561306,
      "learning_rate": 1.5243333333333334e-05,
      "loss": 0.0022,
      "step": 104270
    },
    {
      "epoch": 5.5616,
      "grad_norm": 0.11814039200544357,
      "learning_rate": 1.5240000000000001e-05,
      "loss": 0.0025,
      "step": 104280
    },
    {
      "epoch": 5.562133333333334,
      "grad_norm": 0.17736616730690002,
      "learning_rate": 1.523666666666667e-05,
      "loss": 0.0014,
      "step": 104290
    },
    {
      "epoch": 5.562666666666667,
      "grad_norm": 0.38159453868865967,
      "learning_rate": 1.5233333333333332e-05,
      "loss": 0.0018,
      "step": 104300
    },
    {
      "epoch": 5.5632,
      "grad_norm": 0.34469789266586304,
      "learning_rate": 1.523e-05,
      "loss": 0.002,
      "step": 104310
    },
    {
      "epoch": 5.563733333333333,
      "grad_norm": 0.03773285076022148,
      "learning_rate": 1.5226666666666668e-05,
      "loss": 0.0029,
      "step": 104320
    },
    {
      "epoch": 5.564266666666667,
      "grad_norm": 0.36665526032447815,
      "learning_rate": 1.5223333333333334e-05,
      "loss": 0.0018,
      "step": 104330
    },
    {
      "epoch": 5.5648,
      "grad_norm": 0.09204676002264023,
      "learning_rate": 1.5220000000000002e-05,
      "loss": 0.0017,
      "step": 104340
    },
    {
      "epoch": 5.565333333333333,
      "grad_norm": 0.24360615015029907,
      "learning_rate": 1.5216666666666668e-05,
      "loss": 0.0016,
      "step": 104350
    },
    {
      "epoch": 5.5658666666666665,
      "grad_norm": 0.34730640053749084,
      "learning_rate": 1.5213333333333336e-05,
      "loss": 0.0021,
      "step": 104360
    },
    {
      "epoch": 5.5664,
      "grad_norm": 0.23198309540748596,
      "learning_rate": 1.5210000000000002e-05,
      "loss": 0.0017,
      "step": 104370
    },
    {
      "epoch": 5.566933333333333,
      "grad_norm": 0.1580544412136078,
      "learning_rate": 1.5206666666666666e-05,
      "loss": 0.0015,
      "step": 104380
    },
    {
      "epoch": 5.567466666666666,
      "grad_norm": 0.02107757329940796,
      "learning_rate": 1.5203333333333334e-05,
      "loss": 0.0024,
      "step": 104390
    },
    {
      "epoch": 5.568,
      "grad_norm": 0.34755250811576843,
      "learning_rate": 1.52e-05,
      "loss": 0.002,
      "step": 104400
    },
    {
      "epoch": 5.568533333333333,
      "grad_norm": 0.15022777020931244,
      "learning_rate": 1.5196666666666668e-05,
      "loss": 0.0023,
      "step": 104410
    },
    {
      "epoch": 5.569066666666667,
      "grad_norm": 0.09432684630155563,
      "learning_rate": 1.5193333333333334e-05,
      "loss": 0.0019,
      "step": 104420
    },
    {
      "epoch": 5.5696,
      "grad_norm": 0.09146923571825027,
      "learning_rate": 1.5190000000000002e-05,
      "loss": 0.0018,
      "step": 104430
    },
    {
      "epoch": 5.570133333333334,
      "grad_norm": 0.05872628092765808,
      "learning_rate": 1.5186666666666668e-05,
      "loss": 0.0025,
      "step": 104440
    },
    {
      "epoch": 5.570666666666667,
      "grad_norm": 0.4337209165096283,
      "learning_rate": 1.5183333333333333e-05,
      "loss": 0.0017,
      "step": 104450
    },
    {
      "epoch": 5.5712,
      "grad_norm": 0.5451633930206299,
      "learning_rate": 1.518e-05,
      "loss": 0.0017,
      "step": 104460
    },
    {
      "epoch": 5.571733333333333,
      "grad_norm": 0.06175880506634712,
      "learning_rate": 1.5176666666666666e-05,
      "loss": 0.0021,
      "step": 104470
    },
    {
      "epoch": 5.572266666666667,
      "grad_norm": 0.08772359788417816,
      "learning_rate": 1.5173333333333334e-05,
      "loss": 0.0019,
      "step": 104480
    },
    {
      "epoch": 5.5728,
      "grad_norm": 0.2603019177913666,
      "learning_rate": 1.517e-05,
      "loss": 0.0015,
      "step": 104490
    },
    {
      "epoch": 5.573333333333333,
      "grad_norm": 0.49158239364624023,
      "learning_rate": 1.5166666666666668e-05,
      "loss": 0.0012,
      "step": 104500
    },
    {
      "epoch": 5.5738666666666665,
      "grad_norm": 0.18249432742595673,
      "learning_rate": 1.5163333333333334e-05,
      "loss": 0.002,
      "step": 104510
    },
    {
      "epoch": 5.5744,
      "grad_norm": 0.2367440015077591,
      "learning_rate": 1.5160000000000002e-05,
      "loss": 0.0015,
      "step": 104520
    },
    {
      "epoch": 5.574933333333333,
      "grad_norm": 0.2350747138261795,
      "learning_rate": 1.5156666666666667e-05,
      "loss": 0.0023,
      "step": 104530
    },
    {
      "epoch": 5.575466666666666,
      "grad_norm": 0.09210851043462753,
      "learning_rate": 1.5153333333333333e-05,
      "loss": 0.0028,
      "step": 104540
    },
    {
      "epoch": 5.576,
      "grad_norm": 0.0618009977042675,
      "learning_rate": 1.515e-05,
      "loss": 0.0028,
      "step": 104550
    },
    {
      "epoch": 5.576533333333334,
      "grad_norm": 0.4996502697467804,
      "learning_rate": 1.5146666666666667e-05,
      "loss": 0.0022,
      "step": 104560
    },
    {
      "epoch": 5.577066666666667,
      "grad_norm": 0.2039135843515396,
      "learning_rate": 1.5143333333333335e-05,
      "loss": 0.0013,
      "step": 104570
    },
    {
      "epoch": 5.5776,
      "grad_norm": 0.48339375853538513,
      "learning_rate": 1.514e-05,
      "loss": 0.002,
      "step": 104580
    },
    {
      "epoch": 5.578133333333334,
      "grad_norm": 0.34784120321273804,
      "learning_rate": 1.5136666666666669e-05,
      "loss": 0.0018,
      "step": 104590
    },
    {
      "epoch": 5.578666666666667,
      "grad_norm": 0.09355450421571732,
      "learning_rate": 1.5133333333333333e-05,
      "loss": 0.0015,
      "step": 104600
    },
    {
      "epoch": 5.5792,
      "grad_norm": 0.21002806723117828,
      "learning_rate": 1.5129999999999999e-05,
      "loss": 0.0017,
      "step": 104610
    },
    {
      "epoch": 5.579733333333333,
      "grad_norm": 0.23703214526176453,
      "learning_rate": 1.5126666666666667e-05,
      "loss": 0.0018,
      "step": 104620
    },
    {
      "epoch": 5.580266666666667,
      "grad_norm": 0.02628515288233757,
      "learning_rate": 1.5123333333333333e-05,
      "loss": 0.0028,
      "step": 104630
    },
    {
      "epoch": 5.5808,
      "grad_norm": 0.664488673210144,
      "learning_rate": 1.5120000000000001e-05,
      "loss": 0.0021,
      "step": 104640
    },
    {
      "epoch": 5.581333333333333,
      "grad_norm": 0.3468000292778015,
      "learning_rate": 1.5116666666666667e-05,
      "loss": 0.0015,
      "step": 104650
    },
    {
      "epoch": 5.5818666666666665,
      "grad_norm": 0.09128088504076004,
      "learning_rate": 1.5113333333333335e-05,
      "loss": 0.0026,
      "step": 104660
    },
    {
      "epoch": 5.5824,
      "grad_norm": 0.29512476921081543,
      "learning_rate": 1.5110000000000003e-05,
      "loss": 0.0019,
      "step": 104670
    },
    {
      "epoch": 5.582933333333333,
      "grad_norm": 0.5802326202392578,
      "learning_rate": 1.5106666666666665e-05,
      "loss": 0.0019,
      "step": 104680
    },
    {
      "epoch": 5.583466666666666,
      "grad_norm": 0.07070744037628174,
      "learning_rate": 1.5103333333333333e-05,
      "loss": 0.0019,
      "step": 104690
    },
    {
      "epoch": 5.584,
      "grad_norm": 0.14473584294319153,
      "learning_rate": 1.51e-05,
      "loss": 0.0016,
      "step": 104700
    },
    {
      "epoch": 5.584533333333333,
      "grad_norm": 0.08717897534370422,
      "learning_rate": 1.5096666666666667e-05,
      "loss": 0.0024,
      "step": 104710
    },
    {
      "epoch": 5.585066666666666,
      "grad_norm": 0.14297586679458618,
      "learning_rate": 1.5093333333333335e-05,
      "loss": 0.0031,
      "step": 104720
    },
    {
      "epoch": 5.5856,
      "grad_norm": 0.11872614175081253,
      "learning_rate": 1.5090000000000001e-05,
      "loss": 0.0022,
      "step": 104730
    },
    {
      "epoch": 5.586133333333334,
      "grad_norm": 0.0667402446269989,
      "learning_rate": 1.5086666666666669e-05,
      "loss": 0.0015,
      "step": 104740
    },
    {
      "epoch": 5.586666666666667,
      "grad_norm": 0.3209743797779083,
      "learning_rate": 1.5083333333333335e-05,
      "loss": 0.0024,
      "step": 104750
    },
    {
      "epoch": 5.5872,
      "grad_norm": 0.04127169027924538,
      "learning_rate": 1.508e-05,
      "loss": 0.0016,
      "step": 104760
    },
    {
      "epoch": 5.587733333333333,
      "grad_norm": 0.3162662088871002,
      "learning_rate": 1.5076666666666667e-05,
      "loss": 0.0018,
      "step": 104770
    },
    {
      "epoch": 5.588266666666667,
      "grad_norm": 0.14620405435562134,
      "learning_rate": 1.5073333333333334e-05,
      "loss": 0.0027,
      "step": 104780
    },
    {
      "epoch": 5.5888,
      "grad_norm": 0.2932939827442169,
      "learning_rate": 1.5070000000000001e-05,
      "loss": 0.0012,
      "step": 104790
    },
    {
      "epoch": 5.589333333333333,
      "grad_norm": 0.27097585797309875,
      "learning_rate": 1.5066666666666668e-05,
      "loss": 0.0015,
      "step": 104800
    },
    {
      "epoch": 5.5898666666666665,
      "grad_norm": 0.08201886713504791,
      "learning_rate": 1.5063333333333335e-05,
      "loss": 0.0013,
      "step": 104810
    },
    {
      "epoch": 5.5904,
      "grad_norm": 0.294857919216156,
      "learning_rate": 1.5060000000000001e-05,
      "loss": 0.0016,
      "step": 104820
    },
    {
      "epoch": 5.590933333333333,
      "grad_norm": 0.3437277376651764,
      "learning_rate": 1.5056666666666666e-05,
      "loss": 0.0016,
      "step": 104830
    },
    {
      "epoch": 5.591466666666666,
      "grad_norm": 0.07095132768154144,
      "learning_rate": 1.5053333333333334e-05,
      "loss": 0.0025,
      "step": 104840
    },
    {
      "epoch": 5.592,
      "grad_norm": 0.10958662629127502,
      "learning_rate": 1.505e-05,
      "loss": 0.0015,
      "step": 104850
    },
    {
      "epoch": 5.592533333333334,
      "grad_norm": 0.2547146677970886,
      "learning_rate": 1.5046666666666668e-05,
      "loss": 0.0026,
      "step": 104860
    },
    {
      "epoch": 5.593066666666667,
      "grad_norm": 0.11646455526351929,
      "learning_rate": 1.5043333333333334e-05,
      "loss": 0.0016,
      "step": 104870
    },
    {
      "epoch": 5.5936,
      "grad_norm": 0.16324599087238312,
      "learning_rate": 1.5040000000000002e-05,
      "loss": 0.0023,
      "step": 104880
    },
    {
      "epoch": 5.594133333333334,
      "grad_norm": 0.37081611156463623,
      "learning_rate": 1.5036666666666668e-05,
      "loss": 0.0025,
      "step": 104890
    },
    {
      "epoch": 5.594666666666667,
      "grad_norm": 0.022662052884697914,
      "learning_rate": 1.5033333333333336e-05,
      "loss": 0.0016,
      "step": 104900
    },
    {
      "epoch": 5.5952,
      "grad_norm": 0.050001055002212524,
      "learning_rate": 1.503e-05,
      "loss": 0.0015,
      "step": 104910
    },
    {
      "epoch": 5.5957333333333334,
      "grad_norm": 0.28785115480422974,
      "learning_rate": 1.5026666666666666e-05,
      "loss": 0.0014,
      "step": 104920
    },
    {
      "epoch": 5.596266666666667,
      "grad_norm": 0.423202782869339,
      "learning_rate": 1.5023333333333334e-05,
      "loss": 0.0017,
      "step": 104930
    },
    {
      "epoch": 5.5968,
      "grad_norm": 0.2836470603942871,
      "learning_rate": 1.502e-05,
      "loss": 0.0016,
      "step": 104940
    },
    {
      "epoch": 5.597333333333333,
      "grad_norm": 0.2859586477279663,
      "learning_rate": 1.5016666666666668e-05,
      "loss": 0.002,
      "step": 104950
    },
    {
      "epoch": 5.5978666666666665,
      "grad_norm": 0.2526831030845642,
      "learning_rate": 1.5013333333333334e-05,
      "loss": 0.0016,
      "step": 104960
    },
    {
      "epoch": 5.5984,
      "grad_norm": 0.01939670741558075,
      "learning_rate": 1.5010000000000002e-05,
      "loss": 0.0015,
      "step": 104970
    },
    {
      "epoch": 5.598933333333333,
      "grad_norm": 0.12566471099853516,
      "learning_rate": 1.5006666666666666e-05,
      "loss": 0.0021,
      "step": 104980
    },
    {
      "epoch": 5.599466666666666,
      "grad_norm": 0.11634127050638199,
      "learning_rate": 1.5003333333333333e-05,
      "loss": 0.0018,
      "step": 104990
    },
    {
      "epoch": 5.6,
      "grad_norm": 0.14559893310070038,
      "learning_rate": 1.5e-05,
      "loss": 0.0016,
      "step": 105000
    },
    {
      "epoch": 5.600533333333333,
      "grad_norm": 0.0725947767496109,
      "learning_rate": 1.4996666666666667e-05,
      "loss": 0.0015,
      "step": 105010
    },
    {
      "epoch": 5.601066666666666,
      "grad_norm": 0.09992916882038116,
      "learning_rate": 1.4993333333333334e-05,
      "loss": 0.0014,
      "step": 105020
    },
    {
      "epoch": 5.6016,
      "grad_norm": 0.26207777857780457,
      "learning_rate": 1.499e-05,
      "loss": 0.002,
      "step": 105030
    },
    {
      "epoch": 5.602133333333334,
      "grad_norm": 0.2595047950744629,
      "learning_rate": 1.4986666666666668e-05,
      "loss": 0.0016,
      "step": 105040
    },
    {
      "epoch": 5.602666666666667,
      "grad_norm": 0.2592621445655823,
      "learning_rate": 1.4983333333333336e-05,
      "loss": 0.0013,
      "step": 105050
    },
    {
      "epoch": 5.6032,
      "grad_norm": 0.32356980443000793,
      "learning_rate": 1.4979999999999999e-05,
      "loss": 0.0029,
      "step": 105060
    },
    {
      "epoch": 5.6037333333333335,
      "grad_norm": 0.230189248919487,
      "learning_rate": 1.4976666666666667e-05,
      "loss": 0.0023,
      "step": 105070
    },
    {
      "epoch": 5.604266666666667,
      "grad_norm": 0.20298023521900177,
      "learning_rate": 1.4973333333333333e-05,
      "loss": 0.0022,
      "step": 105080
    },
    {
      "epoch": 5.6048,
      "grad_norm": 0.25871098041534424,
      "learning_rate": 1.497e-05,
      "loss": 0.0016,
      "step": 105090
    },
    {
      "epoch": 5.605333333333333,
      "grad_norm": 0.3121427297592163,
      "learning_rate": 1.4966666666666668e-05,
      "loss": 0.0018,
      "step": 105100
    },
    {
      "epoch": 5.6058666666666666,
      "grad_norm": 0.3195032477378845,
      "learning_rate": 1.4963333333333335e-05,
      "loss": 0.0017,
      "step": 105110
    },
    {
      "epoch": 5.6064,
      "grad_norm": 0.03504395857453346,
      "learning_rate": 1.4960000000000002e-05,
      "loss": 0.0022,
      "step": 105120
    },
    {
      "epoch": 5.606933333333333,
      "grad_norm": 0.034816280007362366,
      "learning_rate": 1.4956666666666665e-05,
      "loss": 0.0019,
      "step": 105130
    },
    {
      "epoch": 5.607466666666666,
      "grad_norm": 0.23560567200183868,
      "learning_rate": 1.4953333333333333e-05,
      "loss": 0.0014,
      "step": 105140
    },
    {
      "epoch": 5.608,
      "grad_norm": 0.12411181628704071,
      "learning_rate": 1.4950000000000001e-05,
      "loss": 0.002,
      "step": 105150
    },
    {
      "epoch": 5.608533333333334,
      "grad_norm": 0.23270925879478455,
      "learning_rate": 1.4946666666666667e-05,
      "loss": 0.0015,
      "step": 105160
    },
    {
      "epoch": 5.609066666666667,
      "grad_norm": 0.5220279097557068,
      "learning_rate": 1.4943333333333335e-05,
      "loss": 0.0015,
      "step": 105170
    },
    {
      "epoch": 5.6096,
      "grad_norm": 0.14911691844463348,
      "learning_rate": 1.4940000000000001e-05,
      "loss": 0.0016,
      "step": 105180
    },
    {
      "epoch": 5.610133333333334,
      "grad_norm": 0.5686836838722229,
      "learning_rate": 1.4936666666666669e-05,
      "loss": 0.0023,
      "step": 105190
    },
    {
      "epoch": 5.610666666666667,
      "grad_norm": 0.4633074104785919,
      "learning_rate": 1.4933333333333335e-05,
      "loss": 0.0014,
      "step": 105200
    },
    {
      "epoch": 5.6112,
      "grad_norm": 0.45984068512916565,
      "learning_rate": 1.493e-05,
      "loss": 0.0022,
      "step": 105210
    },
    {
      "epoch": 5.6117333333333335,
      "grad_norm": 0.4333983063697815,
      "learning_rate": 1.4926666666666667e-05,
      "loss": 0.0019,
      "step": 105220
    },
    {
      "epoch": 5.612266666666667,
      "grad_norm": 0.179597869515419,
      "learning_rate": 1.4923333333333333e-05,
      "loss": 0.0014,
      "step": 105230
    },
    {
      "epoch": 5.6128,
      "grad_norm": 0.2322816699743271,
      "learning_rate": 1.4920000000000001e-05,
      "loss": 0.0028,
      "step": 105240
    },
    {
      "epoch": 5.613333333333333,
      "grad_norm": 0.35544517636299133,
      "learning_rate": 1.4916666666666667e-05,
      "loss": 0.0016,
      "step": 105250
    },
    {
      "epoch": 5.613866666666667,
      "grad_norm": 0.4588041305541992,
      "learning_rate": 1.4913333333333335e-05,
      "loss": 0.0028,
      "step": 105260
    },
    {
      "epoch": 5.6144,
      "grad_norm": 0.32033470273017883,
      "learning_rate": 1.4910000000000001e-05,
      "loss": 0.002,
      "step": 105270
    },
    {
      "epoch": 5.614933333333333,
      "grad_norm": 0.09874019771814346,
      "learning_rate": 1.4906666666666666e-05,
      "loss": 0.0019,
      "step": 105280
    },
    {
      "epoch": 5.615466666666666,
      "grad_norm": 0.39505165815353394,
      "learning_rate": 1.4903333333333334e-05,
      "loss": 0.002,
      "step": 105290
    },
    {
      "epoch": 5.616,
      "grad_norm": 0.15367887914180756,
      "learning_rate": 1.49e-05,
      "loss": 0.0017,
      "step": 105300
    },
    {
      "epoch": 5.616533333333333,
      "grad_norm": 0.06393977254629135,
      "learning_rate": 1.4896666666666667e-05,
      "loss": 0.0029,
      "step": 105310
    },
    {
      "epoch": 5.617066666666666,
      "grad_norm": 0.5448335409164429,
      "learning_rate": 1.4893333333333334e-05,
      "loss": 0.0018,
      "step": 105320
    },
    {
      "epoch": 5.6176,
      "grad_norm": 0.30326735973358154,
      "learning_rate": 1.4890000000000001e-05,
      "loss": 0.0022,
      "step": 105330
    },
    {
      "epoch": 5.618133333333334,
      "grad_norm": 0.46221840381622314,
      "learning_rate": 1.4886666666666668e-05,
      "loss": 0.0019,
      "step": 105340
    },
    {
      "epoch": 5.618666666666667,
      "grad_norm": 0.8206980228424072,
      "learning_rate": 1.4883333333333335e-05,
      "loss": 0.0022,
      "step": 105350
    },
    {
      "epoch": 5.6192,
      "grad_norm": 0.3058459162712097,
      "learning_rate": 1.488e-05,
      "loss": 0.0015,
      "step": 105360
    },
    {
      "epoch": 5.6197333333333335,
      "grad_norm": 0.4956096112728119,
      "learning_rate": 1.4876666666666666e-05,
      "loss": 0.0029,
      "step": 105370
    },
    {
      "epoch": 5.620266666666667,
      "grad_norm": 0.1722584217786789,
      "learning_rate": 1.4873333333333334e-05,
      "loss": 0.0016,
      "step": 105380
    },
    {
      "epoch": 5.6208,
      "grad_norm": 0.3481700122356415,
      "learning_rate": 1.487e-05,
      "loss": 0.0013,
      "step": 105390
    },
    {
      "epoch": 5.621333333333333,
      "grad_norm": 0.06417333334684372,
      "learning_rate": 1.4866666666666668e-05,
      "loss": 0.0018,
      "step": 105400
    },
    {
      "epoch": 5.621866666666667,
      "grad_norm": 0.14064133167266846,
      "learning_rate": 1.4863333333333334e-05,
      "loss": 0.0024,
      "step": 105410
    },
    {
      "epoch": 5.6224,
      "grad_norm": 0.19904443621635437,
      "learning_rate": 1.4860000000000002e-05,
      "loss": 0.0023,
      "step": 105420
    },
    {
      "epoch": 5.622933333333333,
      "grad_norm": 0.10300015658140182,
      "learning_rate": 1.485666666666667e-05,
      "loss": 0.0012,
      "step": 105430
    },
    {
      "epoch": 5.623466666666666,
      "grad_norm": 0.11777665466070175,
      "learning_rate": 1.4853333333333332e-05,
      "loss": 0.0018,
      "step": 105440
    },
    {
      "epoch": 5.624,
      "grad_norm": 0.5058838725090027,
      "learning_rate": 1.485e-05,
      "loss": 0.0013,
      "step": 105450
    },
    {
      "epoch": 5.624533333333334,
      "grad_norm": 0.1733190268278122,
      "learning_rate": 1.4846666666666666e-05,
      "loss": 0.0019,
      "step": 105460
    },
    {
      "epoch": 5.625066666666667,
      "grad_norm": 0.07057875394821167,
      "learning_rate": 1.4843333333333334e-05,
      "loss": 0.0019,
      "step": 105470
    },
    {
      "epoch": 5.6256,
      "grad_norm": 0.20785905420780182,
      "learning_rate": 1.4840000000000002e-05,
      "loss": 0.0017,
      "step": 105480
    },
    {
      "epoch": 5.626133333333334,
      "grad_norm": 0.2647186815738678,
      "learning_rate": 1.4836666666666668e-05,
      "loss": 0.0019,
      "step": 105490
    },
    {
      "epoch": 5.626666666666667,
      "grad_norm": 0.40370413661003113,
      "learning_rate": 1.4833333333333336e-05,
      "loss": 0.0026,
      "step": 105500
    },
    {
      "epoch": 5.6272,
      "grad_norm": 0.2375630885362625,
      "learning_rate": 1.4829999999999999e-05,
      "loss": 0.0018,
      "step": 105510
    },
    {
      "epoch": 5.6277333333333335,
      "grad_norm": 0.06889630109071732,
      "learning_rate": 1.4826666666666666e-05,
      "loss": 0.0036,
      "step": 105520
    },
    {
      "epoch": 5.628266666666667,
      "grad_norm": 0.049238890409469604,
      "learning_rate": 1.4823333333333334e-05,
      "loss": 0.0017,
      "step": 105530
    },
    {
      "epoch": 5.6288,
      "grad_norm": 0.23312446475028992,
      "learning_rate": 1.482e-05,
      "loss": 0.0018,
      "step": 105540
    },
    {
      "epoch": 5.629333333333333,
      "grad_norm": 0.0851270854473114,
      "learning_rate": 1.4816666666666668e-05,
      "loss": 0.0014,
      "step": 105550
    },
    {
      "epoch": 5.629866666666667,
      "grad_norm": 0.229003444314003,
      "learning_rate": 1.4813333333333334e-05,
      "loss": 0.0017,
      "step": 105560
    },
    {
      "epoch": 5.6304,
      "grad_norm": 0.09867443144321442,
      "learning_rate": 1.4810000000000002e-05,
      "loss": 0.0015,
      "step": 105570
    },
    {
      "epoch": 5.630933333333333,
      "grad_norm": 0.022050876170396805,
      "learning_rate": 1.4806666666666668e-05,
      "loss": 0.0018,
      "step": 105580
    },
    {
      "epoch": 5.631466666666666,
      "grad_norm": 0.2332530915737152,
      "learning_rate": 1.4803333333333333e-05,
      "loss": 0.0025,
      "step": 105590
    },
    {
      "epoch": 5.632,
      "grad_norm": 0.04592714086174965,
      "learning_rate": 1.48e-05,
      "loss": 0.0018,
      "step": 105600
    },
    {
      "epoch": 5.632533333333333,
      "grad_norm": 0.08727331459522247,
      "learning_rate": 1.4796666666666667e-05,
      "loss": 0.0015,
      "step": 105610
    },
    {
      "epoch": 5.633066666666666,
      "grad_norm": 0.06112756207585335,
      "learning_rate": 1.4793333333333335e-05,
      "loss": 0.0018,
      "step": 105620
    },
    {
      "epoch": 5.6336,
      "grad_norm": 0.20544417202472687,
      "learning_rate": 1.479e-05,
      "loss": 0.0012,
      "step": 105630
    },
    {
      "epoch": 5.634133333333334,
      "grad_norm": 0.1729380339384079,
      "learning_rate": 1.4786666666666669e-05,
      "loss": 0.0021,
      "step": 105640
    },
    {
      "epoch": 5.634666666666667,
      "grad_norm": 0.09330467134714127,
      "learning_rate": 1.4783333333333335e-05,
      "loss": 0.0017,
      "step": 105650
    },
    {
      "epoch": 5.6352,
      "grad_norm": 0.11483260244131088,
      "learning_rate": 1.4779999999999999e-05,
      "loss": 0.0019,
      "step": 105660
    },
    {
      "epoch": 5.6357333333333335,
      "grad_norm": 0.14973443746566772,
      "learning_rate": 1.4776666666666667e-05,
      "loss": 0.0019,
      "step": 105670
    },
    {
      "epoch": 5.636266666666667,
      "grad_norm": 0.04536953195929527,
      "learning_rate": 1.4773333333333333e-05,
      "loss": 0.0013,
      "step": 105680
    },
    {
      "epoch": 5.6368,
      "grad_norm": 0.25865086913108826,
      "learning_rate": 1.4770000000000001e-05,
      "loss": 0.0026,
      "step": 105690
    },
    {
      "epoch": 5.637333333333333,
      "grad_norm": 0.2519723176956177,
      "learning_rate": 1.4766666666666667e-05,
      "loss": 0.0025,
      "step": 105700
    },
    {
      "epoch": 5.637866666666667,
      "grad_norm": 0.40613916516304016,
      "learning_rate": 1.4763333333333335e-05,
      "loss": 0.0022,
      "step": 105710
    },
    {
      "epoch": 5.6384,
      "grad_norm": 0.09457029402256012,
      "learning_rate": 1.4760000000000001e-05,
      "loss": 0.0016,
      "step": 105720
    },
    {
      "epoch": 5.638933333333333,
      "grad_norm": 0.2921355962753296,
      "learning_rate": 1.4756666666666669e-05,
      "loss": 0.0021,
      "step": 105730
    },
    {
      "epoch": 5.639466666666666,
      "grad_norm": 0.06674563884735107,
      "learning_rate": 1.4753333333333333e-05,
      "loss": 0.0021,
      "step": 105740
    },
    {
      "epoch": 5.64,
      "grad_norm": 0.13837604224681854,
      "learning_rate": 1.475e-05,
      "loss": 0.0017,
      "step": 105750
    },
    {
      "epoch": 5.640533333333333,
      "grad_norm": 0.34964755177497864,
      "learning_rate": 1.4746666666666667e-05,
      "loss": 0.0015,
      "step": 105760
    },
    {
      "epoch": 5.641066666666667,
      "grad_norm": 0.2547142505645752,
      "learning_rate": 1.4743333333333333e-05,
      "loss": 0.0025,
      "step": 105770
    },
    {
      "epoch": 5.6416,
      "grad_norm": 0.11675061285495758,
      "learning_rate": 1.4740000000000001e-05,
      "loss": 0.0018,
      "step": 105780
    },
    {
      "epoch": 5.642133333333334,
      "grad_norm": 0.1529727280139923,
      "learning_rate": 1.4736666666666667e-05,
      "loss": 0.0018,
      "step": 105790
    },
    {
      "epoch": 5.642666666666667,
      "grad_norm": 0.37202250957489014,
      "learning_rate": 1.4733333333333335e-05,
      "loss": 0.0019,
      "step": 105800
    },
    {
      "epoch": 5.6432,
      "grad_norm": 0.21038265526294708,
      "learning_rate": 1.473e-05,
      "loss": 0.0013,
      "step": 105810
    },
    {
      "epoch": 5.6437333333333335,
      "grad_norm": 0.06829018145799637,
      "learning_rate": 1.4726666666666666e-05,
      "loss": 0.0016,
      "step": 105820
    },
    {
      "epoch": 5.644266666666667,
      "grad_norm": 0.09669958800077438,
      "learning_rate": 1.4723333333333334e-05,
      "loss": 0.0019,
      "step": 105830
    },
    {
      "epoch": 5.6448,
      "grad_norm": 0.4671803116798401,
      "learning_rate": 1.472e-05,
      "loss": 0.0016,
      "step": 105840
    },
    {
      "epoch": 5.645333333333333,
      "grad_norm": 0.06898526102304459,
      "learning_rate": 1.4716666666666668e-05,
      "loss": 0.0021,
      "step": 105850
    },
    {
      "epoch": 5.645866666666667,
      "grad_norm": 0.3744778335094452,
      "learning_rate": 1.4713333333333335e-05,
      "loss": 0.0021,
      "step": 105860
    },
    {
      "epoch": 5.6464,
      "grad_norm": 0.24956262111663818,
      "learning_rate": 1.4710000000000001e-05,
      "loss": 0.0015,
      "step": 105870
    },
    {
      "epoch": 5.646933333333333,
      "grad_norm": 0.2793545424938202,
      "learning_rate": 1.470666666666667e-05,
      "loss": 0.0015,
      "step": 105880
    },
    {
      "epoch": 5.647466666666666,
      "grad_norm": 0.5733060836791992,
      "learning_rate": 1.4703333333333332e-05,
      "loss": 0.0022,
      "step": 105890
    },
    {
      "epoch": 5.648,
      "grad_norm": 0.18101975321769714,
      "learning_rate": 1.47e-05,
      "loss": 0.0017,
      "step": 105900
    },
    {
      "epoch": 5.648533333333333,
      "grad_norm": 0.03500651940703392,
      "learning_rate": 1.4696666666666668e-05,
      "loss": 0.0013,
      "step": 105910
    },
    {
      "epoch": 5.649066666666666,
      "grad_norm": 0.14343033730983734,
      "learning_rate": 1.4693333333333334e-05,
      "loss": 0.0024,
      "step": 105920
    },
    {
      "epoch": 5.6495999999999995,
      "grad_norm": 0.029427772387862206,
      "learning_rate": 1.4690000000000002e-05,
      "loss": 0.0013,
      "step": 105930
    },
    {
      "epoch": 5.650133333333334,
      "grad_norm": 0.23113678395748138,
      "learning_rate": 1.4686666666666668e-05,
      "loss": 0.0018,
      "step": 105940
    },
    {
      "epoch": 5.650666666666667,
      "grad_norm": 0.3442203104496002,
      "learning_rate": 1.4683333333333336e-05,
      "loss": 0.0018,
      "step": 105950
    },
    {
      "epoch": 5.6512,
      "grad_norm": 0.23000335693359375,
      "learning_rate": 1.4680000000000002e-05,
      "loss": 0.0027,
      "step": 105960
    },
    {
      "epoch": 5.6517333333333335,
      "grad_norm": 0.2366277128458023,
      "learning_rate": 1.4676666666666666e-05,
      "loss": 0.0022,
      "step": 105970
    },
    {
      "epoch": 5.652266666666667,
      "grad_norm": 0.25947821140289307,
      "learning_rate": 1.4673333333333334e-05,
      "loss": 0.0019,
      "step": 105980
    },
    {
      "epoch": 5.6528,
      "grad_norm": 0.14145244657993317,
      "learning_rate": 1.467e-05,
      "loss": 0.0017,
      "step": 105990
    },
    {
      "epoch": 5.653333333333333,
      "grad_norm": 0.3800800144672394,
      "learning_rate": 1.4666666666666668e-05,
      "loss": 0.0019,
      "step": 106000
    },
    {
      "epoch": 5.653866666666667,
      "grad_norm": 0.34266236424446106,
      "learning_rate": 1.4663333333333334e-05,
      "loss": 0.0022,
      "step": 106010
    },
    {
      "epoch": 5.6544,
      "grad_norm": 0.46237248182296753,
      "learning_rate": 1.4660000000000002e-05,
      "loss": 0.0019,
      "step": 106020
    },
    {
      "epoch": 5.654933333333333,
      "grad_norm": 0.1278630644083023,
      "learning_rate": 1.4656666666666668e-05,
      "loss": 0.0019,
      "step": 106030
    },
    {
      "epoch": 5.655466666666666,
      "grad_norm": 0.3123473525047302,
      "learning_rate": 1.4653333333333333e-05,
      "loss": 0.0023,
      "step": 106040
    },
    {
      "epoch": 5.656,
      "grad_norm": 0.25941768288612366,
      "learning_rate": 1.465e-05,
      "loss": 0.0015,
      "step": 106050
    },
    {
      "epoch": 5.656533333333333,
      "grad_norm": 0.3504842519760132,
      "learning_rate": 1.4646666666666666e-05,
      "loss": 0.0021,
      "step": 106060
    },
    {
      "epoch": 5.657066666666667,
      "grad_norm": 0.22875632345676422,
      "learning_rate": 1.4643333333333334e-05,
      "loss": 0.0019,
      "step": 106070
    },
    {
      "epoch": 5.6576,
      "grad_norm": 0.29163721203804016,
      "learning_rate": 1.464e-05,
      "loss": 0.0013,
      "step": 106080
    },
    {
      "epoch": 5.658133333333334,
      "grad_norm": 0.20736578106880188,
      "learning_rate": 1.4636666666666668e-05,
      "loss": 0.0023,
      "step": 106090
    },
    {
      "epoch": 5.658666666666667,
      "grad_norm": 0.5436135530471802,
      "learning_rate": 1.4633333333333334e-05,
      "loss": 0.0021,
      "step": 106100
    },
    {
      "epoch": 5.6592,
      "grad_norm": 0.2899686098098755,
      "learning_rate": 1.4630000000000002e-05,
      "loss": 0.0032,
      "step": 106110
    },
    {
      "epoch": 5.6597333333333335,
      "grad_norm": 0.09263758361339569,
      "learning_rate": 1.4626666666666667e-05,
      "loss": 0.0014,
      "step": 106120
    },
    {
      "epoch": 5.660266666666667,
      "grad_norm": 0.3722057342529297,
      "learning_rate": 1.4623333333333333e-05,
      "loss": 0.002,
      "step": 106130
    },
    {
      "epoch": 5.6608,
      "grad_norm": 0.07878074049949646,
      "learning_rate": 1.462e-05,
      "loss": 0.0016,
      "step": 106140
    },
    {
      "epoch": 5.661333333333333,
      "grad_norm": 0.0929999053478241,
      "learning_rate": 1.4616666666666667e-05,
      "loss": 0.0012,
      "step": 106150
    },
    {
      "epoch": 5.661866666666667,
      "grad_norm": 0.18105562031269073,
      "learning_rate": 1.4613333333333335e-05,
      "loss": 0.0023,
      "step": 106160
    },
    {
      "epoch": 5.6624,
      "grad_norm": 0.11722950637340546,
      "learning_rate": 1.461e-05,
      "loss": 0.0016,
      "step": 106170
    },
    {
      "epoch": 5.662933333333333,
      "grad_norm": 0.34625253081321716,
      "learning_rate": 1.4606666666666669e-05,
      "loss": 0.0018,
      "step": 106180
    },
    {
      "epoch": 5.663466666666666,
      "grad_norm": 0.09285998344421387,
      "learning_rate": 1.4603333333333333e-05,
      "loss": 0.0016,
      "step": 106190
    },
    {
      "epoch": 5.664,
      "grad_norm": 0.36855947971343994,
      "learning_rate": 1.4599999999999999e-05,
      "loss": 0.0023,
      "step": 106200
    },
    {
      "epoch": 5.664533333333333,
      "grad_norm": 0.5876937508583069,
      "learning_rate": 1.4596666666666667e-05,
      "loss": 0.0023,
      "step": 106210
    },
    {
      "epoch": 5.665066666666666,
      "grad_norm": 0.11813078075647354,
      "learning_rate": 1.4593333333333333e-05,
      "loss": 0.0015,
      "step": 106220
    },
    {
      "epoch": 5.6655999999999995,
      "grad_norm": 0.06643702834844589,
      "learning_rate": 1.4590000000000001e-05,
      "loss": 0.0016,
      "step": 106230
    },
    {
      "epoch": 5.666133333333334,
      "grad_norm": 0.26995620131492615,
      "learning_rate": 1.4586666666666669e-05,
      "loss": 0.0011,
      "step": 106240
    },
    {
      "epoch": 5.666666666666667,
      "grad_norm": 0.11520436406135559,
      "learning_rate": 1.4583333333333335e-05,
      "loss": 0.0021,
      "step": 106250
    },
    {
      "epoch": 5.6672,
      "grad_norm": 0.3998572826385498,
      "learning_rate": 1.4580000000000003e-05,
      "loss": 0.0016,
      "step": 106260
    },
    {
      "epoch": 5.6677333333333335,
      "grad_norm": 0.3643752336502075,
      "learning_rate": 1.4576666666666665e-05,
      "loss": 0.0021,
      "step": 106270
    },
    {
      "epoch": 5.668266666666667,
      "grad_norm": 0.28549724817276,
      "learning_rate": 1.4573333333333333e-05,
      "loss": 0.003,
      "step": 106280
    },
    {
      "epoch": 5.6688,
      "grad_norm": 0.3053540289402008,
      "learning_rate": 1.4570000000000001e-05,
      "loss": 0.0016,
      "step": 106290
    },
    {
      "epoch": 5.669333333333333,
      "grad_norm": 0.07140860706567764,
      "learning_rate": 1.4566666666666667e-05,
      "loss": 0.0025,
      "step": 106300
    },
    {
      "epoch": 5.669866666666667,
      "grad_norm": 0.20324355363845825,
      "learning_rate": 1.4563333333333335e-05,
      "loss": 0.0017,
      "step": 106310
    },
    {
      "epoch": 5.6704,
      "grad_norm": 0.24229270219802856,
      "learning_rate": 1.4560000000000001e-05,
      "loss": 0.0026,
      "step": 106320
    },
    {
      "epoch": 5.670933333333333,
      "grad_norm": 0.20145562291145325,
      "learning_rate": 1.4556666666666669e-05,
      "loss": 0.0023,
      "step": 106330
    },
    {
      "epoch": 5.671466666666666,
      "grad_norm": 0.1987743377685547,
      "learning_rate": 1.4553333333333333e-05,
      "loss": 0.0012,
      "step": 106340
    },
    {
      "epoch": 5.672,
      "grad_norm": 0.12643416225910187,
      "learning_rate": 1.455e-05,
      "loss": 0.0016,
      "step": 106350
    },
    {
      "epoch": 5.672533333333333,
      "grad_norm": 0.261962890625,
      "learning_rate": 1.4546666666666667e-05,
      "loss": 0.0023,
      "step": 106360
    },
    {
      "epoch": 5.673066666666667,
      "grad_norm": 0.3182682991027832,
      "learning_rate": 1.4543333333333334e-05,
      "loss": 0.0013,
      "step": 106370
    },
    {
      "epoch": 5.6736,
      "grad_norm": 0.07483166456222534,
      "learning_rate": 1.4540000000000001e-05,
      "loss": 0.0014,
      "step": 106380
    },
    {
      "epoch": 5.674133333333334,
      "grad_norm": 0.11820371448993683,
      "learning_rate": 1.4536666666666668e-05,
      "loss": 0.0015,
      "step": 106390
    },
    {
      "epoch": 5.674666666666667,
      "grad_norm": 0.48952651023864746,
      "learning_rate": 1.4533333333333335e-05,
      "loss": 0.0019,
      "step": 106400
    },
    {
      "epoch": 5.6752,
      "grad_norm": 0.12959812581539154,
      "learning_rate": 1.4530000000000001e-05,
      "loss": 0.0025,
      "step": 106410
    },
    {
      "epoch": 5.6757333333333335,
      "grad_norm": 0.12173236161470413,
      "learning_rate": 1.4526666666666666e-05,
      "loss": 0.0016,
      "step": 106420
    },
    {
      "epoch": 5.676266666666667,
      "grad_norm": 0.2617429196834564,
      "learning_rate": 1.4523333333333334e-05,
      "loss": 0.0026,
      "step": 106430
    },
    {
      "epoch": 5.6768,
      "grad_norm": 0.3765379786491394,
      "learning_rate": 1.452e-05,
      "loss": 0.0021,
      "step": 106440
    },
    {
      "epoch": 5.677333333333333,
      "grad_norm": 0.07444079965353012,
      "learning_rate": 1.4516666666666668e-05,
      "loss": 0.0019,
      "step": 106450
    },
    {
      "epoch": 5.677866666666667,
      "grad_norm": 0.09911031275987625,
      "learning_rate": 1.4513333333333334e-05,
      "loss": 0.0017,
      "step": 106460
    },
    {
      "epoch": 5.6784,
      "grad_norm": 0.08863179385662079,
      "learning_rate": 1.4510000000000002e-05,
      "loss": 0.0017,
      "step": 106470
    },
    {
      "epoch": 5.678933333333333,
      "grad_norm": 0.3723061978816986,
      "learning_rate": 1.4506666666666668e-05,
      "loss": 0.002,
      "step": 106480
    },
    {
      "epoch": 5.679466666666666,
      "grad_norm": 0.08003344386816025,
      "learning_rate": 1.4503333333333332e-05,
      "loss": 0.0022,
      "step": 106490
    },
    {
      "epoch": 5.68,
      "grad_norm": 0.1201167181134224,
      "learning_rate": 1.45e-05,
      "loss": 0.0021,
      "step": 106500
    },
    {
      "epoch": 5.680533333333333,
      "grad_norm": 0.1826217621564865,
      "learning_rate": 1.4496666666666666e-05,
      "loss": 0.0014,
      "step": 106510
    },
    {
      "epoch": 5.681066666666666,
      "grad_norm": 0.2120068073272705,
      "learning_rate": 1.4493333333333334e-05,
      "loss": 0.0023,
      "step": 106520
    },
    {
      "epoch": 5.6815999999999995,
      "grad_norm": 0.12056448310613632,
      "learning_rate": 1.449e-05,
      "loss": 0.0022,
      "step": 106530
    },
    {
      "epoch": 5.682133333333334,
      "grad_norm": 0.22855857014656067,
      "learning_rate": 1.4486666666666668e-05,
      "loss": 0.0019,
      "step": 106540
    },
    {
      "epoch": 5.682666666666667,
      "grad_norm": 0.05915002524852753,
      "learning_rate": 1.4483333333333334e-05,
      "loss": 0.0013,
      "step": 106550
    },
    {
      "epoch": 5.6832,
      "grad_norm": 0.14724379777908325,
      "learning_rate": 1.4480000000000002e-05,
      "loss": 0.003,
      "step": 106560
    },
    {
      "epoch": 5.6837333333333335,
      "grad_norm": 0.2638401389122009,
      "learning_rate": 1.4476666666666666e-05,
      "loss": 0.0017,
      "step": 106570
    },
    {
      "epoch": 5.684266666666667,
      "grad_norm": 0.18625885248184204,
      "learning_rate": 1.4473333333333333e-05,
      "loss": 0.0014,
      "step": 106580
    },
    {
      "epoch": 5.6848,
      "grad_norm": 0.24191845953464508,
      "learning_rate": 1.447e-05,
      "loss": 0.0019,
      "step": 106590
    },
    {
      "epoch": 5.685333333333333,
      "grad_norm": 0.14774172008037567,
      "learning_rate": 1.4466666666666667e-05,
      "loss": 0.0018,
      "step": 106600
    },
    {
      "epoch": 5.685866666666667,
      "grad_norm": 0.08068614453077316,
      "learning_rate": 1.4463333333333334e-05,
      "loss": 0.0026,
      "step": 106610
    },
    {
      "epoch": 5.6864,
      "grad_norm": 0.3750884234905243,
      "learning_rate": 1.4460000000000002e-05,
      "loss": 0.0017,
      "step": 106620
    },
    {
      "epoch": 5.686933333333333,
      "grad_norm": 0.3463812470436096,
      "learning_rate": 1.4456666666666668e-05,
      "loss": 0.0022,
      "step": 106630
    },
    {
      "epoch": 5.6874666666666664,
      "grad_norm": 0.06932639330625534,
      "learning_rate": 1.4453333333333336e-05,
      "loss": 0.0017,
      "step": 106640
    },
    {
      "epoch": 5.688,
      "grad_norm": 0.04891838878393173,
      "learning_rate": 1.4449999999999999e-05,
      "loss": 0.0024,
      "step": 106650
    },
    {
      "epoch": 5.688533333333333,
      "grad_norm": 0.2605655789375305,
      "learning_rate": 1.4446666666666667e-05,
      "loss": 0.0014,
      "step": 106660
    },
    {
      "epoch": 5.689066666666667,
      "grad_norm": 0.2889201045036316,
      "learning_rate": 1.4443333333333335e-05,
      "loss": 0.0014,
      "step": 106670
    },
    {
      "epoch": 5.6896,
      "grad_norm": 0.1184193566441536,
      "learning_rate": 1.444e-05,
      "loss": 0.0017,
      "step": 106680
    },
    {
      "epoch": 5.690133333333334,
      "grad_norm": 0.2920854091644287,
      "learning_rate": 1.4436666666666668e-05,
      "loss": 0.0019,
      "step": 106690
    },
    {
      "epoch": 5.690666666666667,
      "grad_norm": 0.08855673670768738,
      "learning_rate": 1.4433333333333335e-05,
      "loss": 0.0018,
      "step": 106700
    },
    {
      "epoch": 5.6912,
      "grad_norm": 0.06914764642715454,
      "learning_rate": 1.4430000000000002e-05,
      "loss": 0.0016,
      "step": 106710
    },
    {
      "epoch": 5.6917333333333335,
      "grad_norm": 0.20515966415405273,
      "learning_rate": 1.4426666666666667e-05,
      "loss": 0.0027,
      "step": 106720
    },
    {
      "epoch": 5.692266666666667,
      "grad_norm": 0.24802324175834656,
      "learning_rate": 1.4423333333333333e-05,
      "loss": 0.0019,
      "step": 106730
    },
    {
      "epoch": 5.6928,
      "grad_norm": 0.23764359951019287,
      "learning_rate": 1.4420000000000001e-05,
      "loss": 0.0022,
      "step": 106740
    },
    {
      "epoch": 5.693333333333333,
      "grad_norm": 0.3142205476760864,
      "learning_rate": 1.4416666666666667e-05,
      "loss": 0.0019,
      "step": 106750
    },
    {
      "epoch": 5.693866666666667,
      "grad_norm": 0.4701414108276367,
      "learning_rate": 1.4413333333333335e-05,
      "loss": 0.0026,
      "step": 106760
    },
    {
      "epoch": 5.6944,
      "grad_norm": 0.20038792490959167,
      "learning_rate": 1.4410000000000001e-05,
      "loss": 0.0022,
      "step": 106770
    },
    {
      "epoch": 5.694933333333333,
      "grad_norm": 0.17332647740840912,
      "learning_rate": 1.4406666666666669e-05,
      "loss": 0.0015,
      "step": 106780
    },
    {
      "epoch": 5.6954666666666665,
      "grad_norm": 0.18392594158649445,
      "learning_rate": 1.4403333333333335e-05,
      "loss": 0.0014,
      "step": 106790
    },
    {
      "epoch": 5.696,
      "grad_norm": 0.545005738735199,
      "learning_rate": 1.44e-05,
      "loss": 0.0018,
      "step": 106800
    },
    {
      "epoch": 5.696533333333333,
      "grad_norm": 0.033745087683200836,
      "learning_rate": 1.4396666666666667e-05,
      "loss": 0.0019,
      "step": 106810
    },
    {
      "epoch": 5.697066666666666,
      "grad_norm": 0.2547217905521393,
      "learning_rate": 1.4393333333333333e-05,
      "loss": 0.0016,
      "step": 106820
    },
    {
      "epoch": 5.6975999999999996,
      "grad_norm": 0.14895638823509216,
      "learning_rate": 1.4390000000000001e-05,
      "loss": 0.0016,
      "step": 106830
    },
    {
      "epoch": 5.698133333333334,
      "grad_norm": 0.31574535369873047,
      "learning_rate": 1.4386666666666667e-05,
      "loss": 0.0022,
      "step": 106840
    },
    {
      "epoch": 5.698666666666667,
      "grad_norm": 0.15275780856609344,
      "learning_rate": 1.4383333333333335e-05,
      "loss": 0.0034,
      "step": 106850
    },
    {
      "epoch": 5.6992,
      "grad_norm": 0.12057574838399887,
      "learning_rate": 1.4380000000000001e-05,
      "loss": 0.0015,
      "step": 106860
    },
    {
      "epoch": 5.6997333333333335,
      "grad_norm": 0.6778622269630432,
      "learning_rate": 1.4376666666666666e-05,
      "loss": 0.0021,
      "step": 106870
    },
    {
      "epoch": 5.700266666666667,
      "grad_norm": 0.3494274318218231,
      "learning_rate": 1.4373333333333334e-05,
      "loss": 0.0018,
      "step": 106880
    },
    {
      "epoch": 5.7008,
      "grad_norm": 0.05665678530931473,
      "learning_rate": 1.437e-05,
      "loss": 0.0014,
      "step": 106890
    },
    {
      "epoch": 5.701333333333333,
      "grad_norm": 0.04451113939285278,
      "learning_rate": 1.4366666666666667e-05,
      "loss": 0.0017,
      "step": 106900
    },
    {
      "epoch": 5.701866666666667,
      "grad_norm": 0.1753043830394745,
      "learning_rate": 1.4363333333333334e-05,
      "loss": 0.0015,
      "step": 106910
    },
    {
      "epoch": 5.7024,
      "grad_norm": 0.0578298382461071,
      "learning_rate": 1.4360000000000001e-05,
      "loss": 0.0018,
      "step": 106920
    },
    {
      "epoch": 5.702933333333333,
      "grad_norm": 0.04071476683020592,
      "learning_rate": 1.4356666666666668e-05,
      "loss": 0.0027,
      "step": 106930
    },
    {
      "epoch": 5.7034666666666665,
      "grad_norm": 0.1762523204088211,
      "learning_rate": 1.4353333333333335e-05,
      "loss": 0.0017,
      "step": 106940
    },
    {
      "epoch": 5.704,
      "grad_norm": 0.4033576250076294,
      "learning_rate": 1.435e-05,
      "loss": 0.0015,
      "step": 106950
    },
    {
      "epoch": 5.704533333333333,
      "grad_norm": 0.1468190997838974,
      "learning_rate": 1.4346666666666666e-05,
      "loss": 0.0019,
      "step": 106960
    },
    {
      "epoch": 5.705066666666666,
      "grad_norm": 0.15833431482315063,
      "learning_rate": 1.4343333333333334e-05,
      "loss": 0.0022,
      "step": 106970
    },
    {
      "epoch": 5.7056000000000004,
      "grad_norm": 0.17800001800060272,
      "learning_rate": 1.434e-05,
      "loss": 0.003,
      "step": 106980
    },
    {
      "epoch": 5.706133333333334,
      "grad_norm": 0.13386981189250946,
      "learning_rate": 1.4336666666666668e-05,
      "loss": 0.0014,
      "step": 106990
    },
    {
      "epoch": 5.706666666666667,
      "grad_norm": 0.09362467378377914,
      "learning_rate": 1.4333333333333334e-05,
      "loss": 0.0018,
      "step": 107000
    },
    {
      "epoch": 5.7072,
      "grad_norm": 0.06561797112226486,
      "learning_rate": 1.4330000000000002e-05,
      "loss": 0.0023,
      "step": 107010
    },
    {
      "epoch": 5.7077333333333335,
      "grad_norm": 0.5541424751281738,
      "learning_rate": 1.4326666666666666e-05,
      "loss": 0.002,
      "step": 107020
    },
    {
      "epoch": 5.708266666666667,
      "grad_norm": 0.2646538019180298,
      "learning_rate": 1.4323333333333332e-05,
      "loss": 0.0014,
      "step": 107030
    },
    {
      "epoch": 5.7088,
      "grad_norm": 0.04243098199367523,
      "learning_rate": 1.432e-05,
      "loss": 0.0018,
      "step": 107040
    },
    {
      "epoch": 5.709333333333333,
      "grad_norm": 0.1168041080236435,
      "learning_rate": 1.4316666666666668e-05,
      "loss": 0.0015,
      "step": 107050
    },
    {
      "epoch": 5.709866666666667,
      "grad_norm": 0.43784797191619873,
      "learning_rate": 1.4313333333333334e-05,
      "loss": 0.0021,
      "step": 107060
    },
    {
      "epoch": 5.7104,
      "grad_norm": 0.06466332077980042,
      "learning_rate": 1.4310000000000002e-05,
      "loss": 0.0013,
      "step": 107070
    },
    {
      "epoch": 5.710933333333333,
      "grad_norm": 0.10132905095815659,
      "learning_rate": 1.4306666666666668e-05,
      "loss": 0.0019,
      "step": 107080
    },
    {
      "epoch": 5.7114666666666665,
      "grad_norm": 0.3742046654224396,
      "learning_rate": 1.4303333333333336e-05,
      "loss": 0.0022,
      "step": 107090
    },
    {
      "epoch": 5.712,
      "grad_norm": 0.1549997329711914,
      "learning_rate": 1.43e-05,
      "loss": 0.002,
      "step": 107100
    },
    {
      "epoch": 5.712533333333333,
      "grad_norm": 0.16940294206142426,
      "learning_rate": 1.4296666666666666e-05,
      "loss": 0.0012,
      "step": 107110
    },
    {
      "epoch": 5.713066666666666,
      "grad_norm": 0.2589097321033478,
      "learning_rate": 1.4293333333333334e-05,
      "loss": 0.0016,
      "step": 107120
    },
    {
      "epoch": 5.7136,
      "grad_norm": 0.6828481554985046,
      "learning_rate": 1.429e-05,
      "loss": 0.0026,
      "step": 107130
    },
    {
      "epoch": 5.714133333333333,
      "grad_norm": 0.3191642165184021,
      "learning_rate": 1.4286666666666668e-05,
      "loss": 0.0021,
      "step": 107140
    },
    {
      "epoch": 5.714666666666667,
      "grad_norm": 0.1258559674024582,
      "learning_rate": 1.4283333333333334e-05,
      "loss": 0.0017,
      "step": 107150
    },
    {
      "epoch": 5.7152,
      "grad_norm": 0.2843988537788391,
      "learning_rate": 1.4280000000000002e-05,
      "loss": 0.0022,
      "step": 107160
    },
    {
      "epoch": 5.7157333333333336,
      "grad_norm": 0.25792938470840454,
      "learning_rate": 1.4276666666666667e-05,
      "loss": 0.0021,
      "step": 107170
    },
    {
      "epoch": 5.716266666666667,
      "grad_norm": 0.29061052203178406,
      "learning_rate": 1.4273333333333333e-05,
      "loss": 0.0023,
      "step": 107180
    },
    {
      "epoch": 5.7168,
      "grad_norm": 0.525749146938324,
      "learning_rate": 1.427e-05,
      "loss": 0.0023,
      "step": 107190
    },
    {
      "epoch": 5.717333333333333,
      "grad_norm": 0.5913668274879456,
      "learning_rate": 1.4266666666666667e-05,
      "loss": 0.0019,
      "step": 107200
    },
    {
      "epoch": 5.717866666666667,
      "grad_norm": 0.2107585221529007,
      "learning_rate": 1.4263333333333335e-05,
      "loss": 0.0018,
      "step": 107210
    },
    {
      "epoch": 5.7184,
      "grad_norm": 0.17479626834392548,
      "learning_rate": 1.426e-05,
      "loss": 0.0014,
      "step": 107220
    },
    {
      "epoch": 5.718933333333333,
      "grad_norm": 0.527436375617981,
      "learning_rate": 1.4256666666666669e-05,
      "loss": 0.0019,
      "step": 107230
    },
    {
      "epoch": 5.7194666666666665,
      "grad_norm": 0.29632481932640076,
      "learning_rate": 1.4253333333333335e-05,
      "loss": 0.0024,
      "step": 107240
    },
    {
      "epoch": 5.72,
      "grad_norm": 0.03642193600535393,
      "learning_rate": 1.4249999999999999e-05,
      "loss": 0.0021,
      "step": 107250
    },
    {
      "epoch": 5.720533333333333,
      "grad_norm": 0.2642366588115692,
      "learning_rate": 1.4246666666666667e-05,
      "loss": 0.002,
      "step": 107260
    },
    {
      "epoch": 5.721066666666666,
      "grad_norm": 0.5252451300621033,
      "learning_rate": 1.4243333333333333e-05,
      "loss": 0.0019,
      "step": 107270
    },
    {
      "epoch": 5.7216000000000005,
      "grad_norm": 0.27099284529685974,
      "learning_rate": 1.4240000000000001e-05,
      "loss": 0.0017,
      "step": 107280
    },
    {
      "epoch": 5.722133333333334,
      "grad_norm": 0.40693455934524536,
      "learning_rate": 1.4236666666666667e-05,
      "loss": 0.0024,
      "step": 107290
    },
    {
      "epoch": 5.722666666666667,
      "grad_norm": 0.20262016355991364,
      "learning_rate": 1.4233333333333335e-05,
      "loss": 0.0018,
      "step": 107300
    },
    {
      "epoch": 5.7232,
      "grad_norm": 0.2589157223701477,
      "learning_rate": 1.4230000000000001e-05,
      "loss": 0.0015,
      "step": 107310
    },
    {
      "epoch": 5.723733333333334,
      "grad_norm": 0.4880436360836029,
      "learning_rate": 1.4226666666666669e-05,
      "loss": 0.0019,
      "step": 107320
    },
    {
      "epoch": 5.724266666666667,
      "grad_norm": 0.11009281128644943,
      "learning_rate": 1.4223333333333333e-05,
      "loss": 0.0014,
      "step": 107330
    },
    {
      "epoch": 5.7248,
      "grad_norm": 0.1715114861726761,
      "learning_rate": 1.422e-05,
      "loss": 0.0024,
      "step": 107340
    },
    {
      "epoch": 5.725333333333333,
      "grad_norm": 0.3232336938381195,
      "learning_rate": 1.4216666666666667e-05,
      "loss": 0.0019,
      "step": 107350
    },
    {
      "epoch": 5.725866666666667,
      "grad_norm": 0.09340637922286987,
      "learning_rate": 1.4213333333333333e-05,
      "loss": 0.0018,
      "step": 107360
    },
    {
      "epoch": 5.7264,
      "grad_norm": 0.343627005815506,
      "learning_rate": 1.4210000000000001e-05,
      "loss": 0.0016,
      "step": 107370
    },
    {
      "epoch": 5.726933333333333,
      "grad_norm": 0.20358985662460327,
      "learning_rate": 1.4206666666666667e-05,
      "loss": 0.0018,
      "step": 107380
    },
    {
      "epoch": 5.7274666666666665,
      "grad_norm": 0.43311935663223267,
      "learning_rate": 1.4203333333333335e-05,
      "loss": 0.0013,
      "step": 107390
    },
    {
      "epoch": 5.728,
      "grad_norm": 0.11805519461631775,
      "learning_rate": 1.42e-05,
      "loss": 0.0014,
      "step": 107400
    },
    {
      "epoch": 5.728533333333333,
      "grad_norm": 0.26191216707229614,
      "learning_rate": 1.4196666666666666e-05,
      "loss": 0.0022,
      "step": 107410
    },
    {
      "epoch": 5.729066666666666,
      "grad_norm": 0.056746747344732285,
      "learning_rate": 1.4193333333333334e-05,
      "loss": 0.0016,
      "step": 107420
    },
    {
      "epoch": 5.7296,
      "grad_norm": 0.15001724660396576,
      "learning_rate": 1.4190000000000001e-05,
      "loss": 0.0016,
      "step": 107430
    },
    {
      "epoch": 5.730133333333333,
      "grad_norm": 0.2731032371520996,
      "learning_rate": 1.4186666666666667e-05,
      "loss": 0.0014,
      "step": 107440
    },
    {
      "epoch": 5.730666666666667,
      "grad_norm": 0.26226022839546204,
      "learning_rate": 1.4183333333333335e-05,
      "loss": 0.0019,
      "step": 107450
    },
    {
      "epoch": 5.7312,
      "grad_norm": 0.46312811970710754,
      "learning_rate": 1.4180000000000001e-05,
      "loss": 0.0018,
      "step": 107460
    },
    {
      "epoch": 5.731733333333334,
      "grad_norm": 0.3839196562767029,
      "learning_rate": 1.417666666666667e-05,
      "loss": 0.0021,
      "step": 107470
    },
    {
      "epoch": 5.732266666666667,
      "grad_norm": 0.20670539140701294,
      "learning_rate": 1.4173333333333334e-05,
      "loss": 0.0028,
      "step": 107480
    },
    {
      "epoch": 5.7328,
      "grad_norm": 0.11442091315984726,
      "learning_rate": 1.417e-05,
      "loss": 0.0016,
      "step": 107490
    },
    {
      "epoch": 5.733333333333333,
      "grad_norm": 0.20448173582553864,
      "learning_rate": 1.4166666666666668e-05,
      "loss": 0.002,
      "step": 107500
    },
    {
      "epoch": 5.733866666666667,
      "grad_norm": 0.25756368041038513,
      "learning_rate": 1.4163333333333334e-05,
      "loss": 0.0014,
      "step": 107510
    },
    {
      "epoch": 5.7344,
      "grad_norm": 0.09917620569467545,
      "learning_rate": 1.4160000000000002e-05,
      "loss": 0.002,
      "step": 107520
    },
    {
      "epoch": 5.734933333333333,
      "grad_norm": 0.11768635362386703,
      "learning_rate": 1.4156666666666668e-05,
      "loss": 0.0017,
      "step": 107530
    },
    {
      "epoch": 5.7354666666666665,
      "grad_norm": 0.22967292368412018,
      "learning_rate": 1.4153333333333336e-05,
      "loss": 0.0025,
      "step": 107540
    },
    {
      "epoch": 5.736,
      "grad_norm": 0.09242425858974457,
      "learning_rate": 1.415e-05,
      "loss": 0.0022,
      "step": 107550
    },
    {
      "epoch": 5.736533333333333,
      "grad_norm": 0.3185660243034363,
      "learning_rate": 1.4146666666666666e-05,
      "loss": 0.0013,
      "step": 107560
    },
    {
      "epoch": 5.737066666666666,
      "grad_norm": 0.2293233871459961,
      "learning_rate": 1.4143333333333334e-05,
      "loss": 0.0014,
      "step": 107570
    },
    {
      "epoch": 5.7376000000000005,
      "grad_norm": 0.37137725949287415,
      "learning_rate": 1.414e-05,
      "loss": 0.0021,
      "step": 107580
    },
    {
      "epoch": 5.738133333333334,
      "grad_norm": 0.375227153301239,
      "learning_rate": 1.4136666666666668e-05,
      "loss": 0.0015,
      "step": 107590
    },
    {
      "epoch": 5.738666666666667,
      "grad_norm": 0.1525295078754425,
      "learning_rate": 1.4133333333333334e-05,
      "loss": 0.002,
      "step": 107600
    },
    {
      "epoch": 5.7392,
      "grad_norm": 0.48161444067955017,
      "learning_rate": 1.4130000000000002e-05,
      "loss": 0.0022,
      "step": 107610
    },
    {
      "epoch": 5.739733333333334,
      "grad_norm": 0.1722538322210312,
      "learning_rate": 1.4126666666666668e-05,
      "loss": 0.0013,
      "step": 107620
    },
    {
      "epoch": 5.740266666666667,
      "grad_norm": 0.3311946392059326,
      "learning_rate": 1.4123333333333333e-05,
      "loss": 0.0019,
      "step": 107630
    },
    {
      "epoch": 5.7408,
      "grad_norm": 0.0423777773976326,
      "learning_rate": 1.412e-05,
      "loss": 0.0015,
      "step": 107640
    },
    {
      "epoch": 5.741333333333333,
      "grad_norm": 0.06253451108932495,
      "learning_rate": 1.4116666666666666e-05,
      "loss": 0.0021,
      "step": 107650
    },
    {
      "epoch": 5.741866666666667,
      "grad_norm": 0.06499423831701279,
      "learning_rate": 1.4113333333333334e-05,
      "loss": 0.0014,
      "step": 107660
    },
    {
      "epoch": 5.7424,
      "grad_norm": 0.2095409482717514,
      "learning_rate": 1.411e-05,
      "loss": 0.0026,
      "step": 107670
    },
    {
      "epoch": 5.742933333333333,
      "grad_norm": 0.06501447409391403,
      "learning_rate": 1.4106666666666668e-05,
      "loss": 0.0015,
      "step": 107680
    },
    {
      "epoch": 5.7434666666666665,
      "grad_norm": 0.06404373794794083,
      "learning_rate": 1.4103333333333334e-05,
      "loss": 0.0021,
      "step": 107690
    },
    {
      "epoch": 5.744,
      "grad_norm": 0.12248631566762924,
      "learning_rate": 1.4099999999999999e-05,
      "loss": 0.0019,
      "step": 107700
    },
    {
      "epoch": 5.744533333333333,
      "grad_norm": 0.29297950863838196,
      "learning_rate": 1.4096666666666667e-05,
      "loss": 0.0013,
      "step": 107710
    },
    {
      "epoch": 5.745066666666666,
      "grad_norm": 0.3156701624393463,
      "learning_rate": 1.4093333333333333e-05,
      "loss": 0.0017,
      "step": 107720
    },
    {
      "epoch": 5.7456,
      "grad_norm": 0.042964544147253036,
      "learning_rate": 1.409e-05,
      "loss": 0.0022,
      "step": 107730
    },
    {
      "epoch": 5.746133333333333,
      "grad_norm": 0.17378492653369904,
      "learning_rate": 1.4086666666666667e-05,
      "loss": 0.002,
      "step": 107740
    },
    {
      "epoch": 5.746666666666667,
      "grad_norm": 0.04638136178255081,
      "learning_rate": 1.4083333333333335e-05,
      "loss": 0.0019,
      "step": 107750
    },
    {
      "epoch": 5.7472,
      "grad_norm": 0.2670424282550812,
      "learning_rate": 1.408e-05,
      "loss": 0.0027,
      "step": 107760
    },
    {
      "epoch": 5.747733333333334,
      "grad_norm": 0.24825462698936462,
      "learning_rate": 1.4076666666666669e-05,
      "loss": 0.0015,
      "step": 107770
    },
    {
      "epoch": 5.748266666666667,
      "grad_norm": 0.1460627317428589,
      "learning_rate": 1.4073333333333333e-05,
      "loss": 0.0017,
      "step": 107780
    },
    {
      "epoch": 5.7488,
      "grad_norm": 0.19705887138843536,
      "learning_rate": 1.4069999999999999e-05,
      "loss": 0.0016,
      "step": 107790
    },
    {
      "epoch": 5.749333333333333,
      "grad_norm": 0.3735980987548828,
      "learning_rate": 1.4066666666666667e-05,
      "loss": 0.0021,
      "step": 107800
    },
    {
      "epoch": 5.749866666666667,
      "grad_norm": 0.12820686399936676,
      "learning_rate": 1.4063333333333333e-05,
      "loss": 0.0018,
      "step": 107810
    },
    {
      "epoch": 5.7504,
      "grad_norm": 0.03616030514240265,
      "learning_rate": 1.4060000000000001e-05,
      "loss": 0.0017,
      "step": 107820
    },
    {
      "epoch": 5.750933333333333,
      "grad_norm": 0.0443291962146759,
      "learning_rate": 1.4056666666666669e-05,
      "loss": 0.0014,
      "step": 107830
    },
    {
      "epoch": 5.7514666666666665,
      "grad_norm": 0.32620495557785034,
      "learning_rate": 1.4053333333333335e-05,
      "loss": 0.0021,
      "step": 107840
    },
    {
      "epoch": 5.752,
      "grad_norm": 0.24037449061870575,
      "learning_rate": 1.4050000000000003e-05,
      "loss": 0.0013,
      "step": 107850
    },
    {
      "epoch": 5.752533333333333,
      "grad_norm": 0.048890817910432816,
      "learning_rate": 1.4046666666666667e-05,
      "loss": 0.0037,
      "step": 107860
    },
    {
      "epoch": 5.753066666666666,
      "grad_norm": 0.1527981162071228,
      "learning_rate": 1.4043333333333333e-05,
      "loss": 0.0013,
      "step": 107870
    },
    {
      "epoch": 5.7536000000000005,
      "grad_norm": 0.17411741614341736,
      "learning_rate": 1.4040000000000001e-05,
      "loss": 0.0021,
      "step": 107880
    },
    {
      "epoch": 5.754133333333334,
      "grad_norm": 0.04713287204504013,
      "learning_rate": 1.4036666666666667e-05,
      "loss": 0.0023,
      "step": 107890
    },
    {
      "epoch": 5.754666666666667,
      "grad_norm": 0.22725626826286316,
      "learning_rate": 1.4033333333333335e-05,
      "loss": 0.0019,
      "step": 107900
    },
    {
      "epoch": 5.7552,
      "grad_norm": 0.28331828117370605,
      "learning_rate": 1.4030000000000001e-05,
      "loss": 0.0017,
      "step": 107910
    },
    {
      "epoch": 5.755733333333334,
      "grad_norm": 0.3189648687839508,
      "learning_rate": 1.4026666666666669e-05,
      "loss": 0.0015,
      "step": 107920
    },
    {
      "epoch": 5.756266666666667,
      "grad_norm": 0.1427866369485855,
      "learning_rate": 1.4023333333333333e-05,
      "loss": 0.0018,
      "step": 107930
    },
    {
      "epoch": 5.7568,
      "grad_norm": 0.17710770666599274,
      "learning_rate": 1.402e-05,
      "loss": 0.0016,
      "step": 107940
    },
    {
      "epoch": 5.757333333333333,
      "grad_norm": 0.04927453026175499,
      "learning_rate": 1.4016666666666667e-05,
      "loss": 0.0031,
      "step": 107950
    },
    {
      "epoch": 5.757866666666667,
      "grad_norm": 0.25749123096466064,
      "learning_rate": 1.4013333333333334e-05,
      "loss": 0.0024,
      "step": 107960
    },
    {
      "epoch": 5.7584,
      "grad_norm": 0.25878915190696716,
      "learning_rate": 1.4010000000000001e-05,
      "loss": 0.0015,
      "step": 107970
    },
    {
      "epoch": 5.758933333333333,
      "grad_norm": 0.2608741521835327,
      "learning_rate": 1.4006666666666668e-05,
      "loss": 0.0015,
      "step": 107980
    },
    {
      "epoch": 5.7594666666666665,
      "grad_norm": 0.04821780323982239,
      "learning_rate": 1.4003333333333335e-05,
      "loss": 0.0013,
      "step": 107990
    },
    {
      "epoch": 5.76,
      "grad_norm": 0.046485017985105515,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.0024,
      "step": 108000
    },
    {
      "epoch": 5.760533333333333,
      "grad_norm": 0.23723752796649933,
      "learning_rate": 1.3996666666666666e-05,
      "loss": 0.0021,
      "step": 108010
    },
    {
      "epoch": 5.761066666666666,
      "grad_norm": 0.06561443954706192,
      "learning_rate": 1.3993333333333334e-05,
      "loss": 0.0018,
      "step": 108020
    },
    {
      "epoch": 5.7616,
      "grad_norm": 0.401043564081192,
      "learning_rate": 1.399e-05,
      "loss": 0.0022,
      "step": 108030
    },
    {
      "epoch": 5.762133333333333,
      "grad_norm": 0.20271845161914825,
      "learning_rate": 1.3986666666666668e-05,
      "loss": 0.0026,
      "step": 108040
    },
    {
      "epoch": 5.762666666666667,
      "grad_norm": 0.20208142697811127,
      "learning_rate": 1.3983333333333334e-05,
      "loss": 0.0026,
      "step": 108050
    },
    {
      "epoch": 5.7632,
      "grad_norm": 0.17319247126579285,
      "learning_rate": 1.3980000000000002e-05,
      "loss": 0.0026,
      "step": 108060
    },
    {
      "epoch": 5.763733333333334,
      "grad_norm": 0.17323872447013855,
      "learning_rate": 1.3976666666666668e-05,
      "loss": 0.0021,
      "step": 108070
    },
    {
      "epoch": 5.764266666666667,
      "grad_norm": 0.26530858874320984,
      "learning_rate": 1.3973333333333332e-05,
      "loss": 0.0015,
      "step": 108080
    },
    {
      "epoch": 5.7648,
      "grad_norm": 0.0383228063583374,
      "learning_rate": 1.397e-05,
      "loss": 0.0019,
      "step": 108090
    },
    {
      "epoch": 5.765333333333333,
      "grad_norm": 0.2904939651489258,
      "learning_rate": 1.3966666666666666e-05,
      "loss": 0.0015,
      "step": 108100
    },
    {
      "epoch": 5.765866666666667,
      "grad_norm": 0.21171055734157562,
      "learning_rate": 1.3963333333333334e-05,
      "loss": 0.0021,
      "step": 108110
    },
    {
      "epoch": 5.7664,
      "grad_norm": 0.08951424062252045,
      "learning_rate": 1.396e-05,
      "loss": 0.0016,
      "step": 108120
    },
    {
      "epoch": 5.766933333333333,
      "grad_norm": 0.10586871951818466,
      "learning_rate": 1.3956666666666668e-05,
      "loss": 0.002,
      "step": 108130
    },
    {
      "epoch": 5.7674666666666665,
      "grad_norm": 0.09017311036586761,
      "learning_rate": 1.3953333333333334e-05,
      "loss": 0.0014,
      "step": 108140
    },
    {
      "epoch": 5.768,
      "grad_norm": 0.20696014165878296,
      "learning_rate": 1.3950000000000002e-05,
      "loss": 0.0023,
      "step": 108150
    },
    {
      "epoch": 5.768533333333333,
      "grad_norm": 0.11411291360855103,
      "learning_rate": 1.3946666666666666e-05,
      "loss": 0.0014,
      "step": 108160
    },
    {
      "epoch": 5.769066666666666,
      "grad_norm": 0.46034207940101624,
      "learning_rate": 1.3943333333333333e-05,
      "loss": 0.0023,
      "step": 108170
    },
    {
      "epoch": 5.7696,
      "grad_norm": 0.17379502952098846,
      "learning_rate": 1.394e-05,
      "loss": 0.0018,
      "step": 108180
    },
    {
      "epoch": 5.770133333333334,
      "grad_norm": 0.23800742626190186,
      "learning_rate": 1.3936666666666666e-05,
      "loss": 0.002,
      "step": 108190
    },
    {
      "epoch": 5.770666666666667,
      "grad_norm": 0.05239522084593773,
      "learning_rate": 1.3933333333333334e-05,
      "loss": 0.002,
      "step": 108200
    },
    {
      "epoch": 5.7712,
      "grad_norm": 0.07308020442724228,
      "learning_rate": 1.3930000000000002e-05,
      "loss": 0.0017,
      "step": 108210
    },
    {
      "epoch": 5.771733333333334,
      "grad_norm": 0.01921083778142929,
      "learning_rate": 1.3926666666666668e-05,
      "loss": 0.0022,
      "step": 108220
    },
    {
      "epoch": 5.772266666666667,
      "grad_norm": 0.3965286910533905,
      "learning_rate": 1.3923333333333333e-05,
      "loss": 0.0025,
      "step": 108230
    },
    {
      "epoch": 5.7728,
      "grad_norm": 0.17386119067668915,
      "learning_rate": 1.3919999999999999e-05,
      "loss": 0.0026,
      "step": 108240
    },
    {
      "epoch": 5.773333333333333,
      "grad_norm": 0.08768059313297272,
      "learning_rate": 1.3916666666666667e-05,
      "loss": 0.0016,
      "step": 108250
    },
    {
      "epoch": 5.773866666666667,
      "grad_norm": 0.14724589884281158,
      "learning_rate": 1.3913333333333335e-05,
      "loss": 0.0017,
      "step": 108260
    },
    {
      "epoch": 5.7744,
      "grad_norm": 0.3561936914920807,
      "learning_rate": 1.391e-05,
      "loss": 0.0027,
      "step": 108270
    },
    {
      "epoch": 5.774933333333333,
      "grad_norm": 0.34782281517982483,
      "learning_rate": 1.3906666666666668e-05,
      "loss": 0.0014,
      "step": 108280
    },
    {
      "epoch": 5.7754666666666665,
      "grad_norm": 0.31263023614883423,
      "learning_rate": 1.3903333333333335e-05,
      "loss": 0.0016,
      "step": 108290
    },
    {
      "epoch": 5.776,
      "grad_norm": 0.3306165635585785,
      "learning_rate": 1.3900000000000002e-05,
      "loss": 0.0023,
      "step": 108300
    },
    {
      "epoch": 5.776533333333333,
      "grad_norm": 0.14709816873073578,
      "learning_rate": 1.3896666666666667e-05,
      "loss": 0.0022,
      "step": 108310
    },
    {
      "epoch": 5.777066666666666,
      "grad_norm": 0.7277097702026367,
      "learning_rate": 1.3893333333333333e-05,
      "loss": 0.0021,
      "step": 108320
    },
    {
      "epoch": 5.7776,
      "grad_norm": 0.09161324799060822,
      "learning_rate": 1.389e-05,
      "loss": 0.0024,
      "step": 108330
    },
    {
      "epoch": 5.778133333333333,
      "grad_norm": 0.4897734224796295,
      "learning_rate": 1.3886666666666667e-05,
      "loss": 0.0014,
      "step": 108340
    },
    {
      "epoch": 5.778666666666666,
      "grad_norm": 0.21075616776943207,
      "learning_rate": 1.3883333333333335e-05,
      "loss": 0.002,
      "step": 108350
    },
    {
      "epoch": 5.7792,
      "grad_norm": 0.32087191939353943,
      "learning_rate": 1.3880000000000001e-05,
      "loss": 0.0025,
      "step": 108360
    },
    {
      "epoch": 5.779733333333334,
      "grad_norm": 0.06296876817941666,
      "learning_rate": 1.3876666666666669e-05,
      "loss": 0.0021,
      "step": 108370
    },
    {
      "epoch": 5.780266666666667,
      "grad_norm": 0.09432166814804077,
      "learning_rate": 1.3873333333333333e-05,
      "loss": 0.0013,
      "step": 108380
    },
    {
      "epoch": 5.7808,
      "grad_norm": 0.1275026947259903,
      "learning_rate": 1.387e-05,
      "loss": 0.002,
      "step": 108390
    },
    {
      "epoch": 5.781333333333333,
      "grad_norm": 0.09416169673204422,
      "learning_rate": 1.3866666666666667e-05,
      "loss": 0.0018,
      "step": 108400
    },
    {
      "epoch": 5.781866666666667,
      "grad_norm": 0.11543557047843933,
      "learning_rate": 1.3863333333333333e-05,
      "loss": 0.0015,
      "step": 108410
    },
    {
      "epoch": 5.7824,
      "grad_norm": 0.2081679403781891,
      "learning_rate": 1.3860000000000001e-05,
      "loss": 0.0016,
      "step": 108420
    },
    {
      "epoch": 5.782933333333333,
      "grad_norm": 0.257119745016098,
      "learning_rate": 1.3856666666666667e-05,
      "loss": 0.0022,
      "step": 108430
    },
    {
      "epoch": 5.7834666666666665,
      "grad_norm": 0.22830455005168915,
      "learning_rate": 1.3853333333333335e-05,
      "loss": 0.0017,
      "step": 108440
    },
    {
      "epoch": 5.784,
      "grad_norm": 0.08876863121986389,
      "learning_rate": 1.3850000000000001e-05,
      "loss": 0.0018,
      "step": 108450
    },
    {
      "epoch": 5.784533333333333,
      "grad_norm": 0.07357942312955856,
      "learning_rate": 1.3846666666666666e-05,
      "loss": 0.0015,
      "step": 108460
    },
    {
      "epoch": 5.785066666666666,
      "grad_norm": 0.20760691165924072,
      "learning_rate": 1.3843333333333333e-05,
      "loss": 0.0014,
      "step": 108470
    },
    {
      "epoch": 5.7856,
      "grad_norm": 0.5470499396324158,
      "learning_rate": 1.384e-05,
      "loss": 0.002,
      "step": 108480
    },
    {
      "epoch": 5.786133333333334,
      "grad_norm": 0.315189927816391,
      "learning_rate": 1.3836666666666667e-05,
      "loss": 0.0015,
      "step": 108490
    },
    {
      "epoch": 5.786666666666667,
      "grad_norm": 0.11932744085788727,
      "learning_rate": 1.3833333333333334e-05,
      "loss": 0.002,
      "step": 108500
    },
    {
      "epoch": 5.7872,
      "grad_norm": 0.03965619578957558,
      "learning_rate": 1.3830000000000001e-05,
      "loss": 0.0017,
      "step": 108510
    },
    {
      "epoch": 5.787733333333334,
      "grad_norm": 0.09296443313360214,
      "learning_rate": 1.3826666666666668e-05,
      "loss": 0.0023,
      "step": 108520
    },
    {
      "epoch": 5.788266666666667,
      "grad_norm": 0.0486583337187767,
      "learning_rate": 1.3823333333333335e-05,
      "loss": 0.0014,
      "step": 108530
    },
    {
      "epoch": 5.7888,
      "grad_norm": 0.09307964146137238,
      "learning_rate": 1.382e-05,
      "loss": 0.0019,
      "step": 108540
    },
    {
      "epoch": 5.789333333333333,
      "grad_norm": 0.4903290867805481,
      "learning_rate": 1.3816666666666666e-05,
      "loss": 0.0021,
      "step": 108550
    },
    {
      "epoch": 5.789866666666667,
      "grad_norm": 0.43380290269851685,
      "learning_rate": 1.3813333333333334e-05,
      "loss": 0.0015,
      "step": 108560
    },
    {
      "epoch": 5.7904,
      "grad_norm": 0.23552197217941284,
      "learning_rate": 1.381e-05,
      "loss": 0.002,
      "step": 108570
    },
    {
      "epoch": 5.790933333333333,
      "grad_norm": 0.5098559260368347,
      "learning_rate": 1.3806666666666668e-05,
      "loss": 0.0013,
      "step": 108580
    },
    {
      "epoch": 5.7914666666666665,
      "grad_norm": 0.46319711208343506,
      "learning_rate": 1.3803333333333336e-05,
      "loss": 0.002,
      "step": 108590
    },
    {
      "epoch": 5.792,
      "grad_norm": 0.346002459526062,
      "learning_rate": 1.3800000000000002e-05,
      "loss": 0.0024,
      "step": 108600
    },
    {
      "epoch": 5.792533333333333,
      "grad_norm": 0.18014757335186005,
      "learning_rate": 1.3796666666666666e-05,
      "loss": 0.0026,
      "step": 108610
    },
    {
      "epoch": 5.793066666666666,
      "grad_norm": 0.31973105669021606,
      "learning_rate": 1.3793333333333332e-05,
      "loss": 0.0019,
      "step": 108620
    },
    {
      "epoch": 5.7936,
      "grad_norm": 0.34280094504356384,
      "learning_rate": 1.379e-05,
      "loss": 0.0018,
      "step": 108630
    },
    {
      "epoch": 5.794133333333333,
      "grad_norm": 0.3203008770942688,
      "learning_rate": 1.3786666666666668e-05,
      "loss": 0.0021,
      "step": 108640
    },
    {
      "epoch": 5.794666666666666,
      "grad_norm": 0.032618436962366104,
      "learning_rate": 1.3783333333333334e-05,
      "loss": 0.002,
      "step": 108650
    },
    {
      "epoch": 5.7952,
      "grad_norm": 0.08676483482122421,
      "learning_rate": 1.3780000000000002e-05,
      "loss": 0.0023,
      "step": 108660
    },
    {
      "epoch": 5.795733333333334,
      "grad_norm": 0.04114324226975441,
      "learning_rate": 1.3776666666666668e-05,
      "loss": 0.0021,
      "step": 108670
    },
    {
      "epoch": 5.796266666666667,
      "grad_norm": 0.4031805992126465,
      "learning_rate": 1.3773333333333336e-05,
      "loss": 0.0014,
      "step": 108680
    },
    {
      "epoch": 5.7968,
      "grad_norm": 0.27993085980415344,
      "learning_rate": 1.377e-05,
      "loss": 0.0019,
      "step": 108690
    },
    {
      "epoch": 5.7973333333333334,
      "grad_norm": 0.08702515065670013,
      "learning_rate": 1.3766666666666666e-05,
      "loss": 0.0018,
      "step": 108700
    },
    {
      "epoch": 5.797866666666667,
      "grad_norm": 0.2865684926509857,
      "learning_rate": 1.3763333333333334e-05,
      "loss": 0.0023,
      "step": 108710
    },
    {
      "epoch": 5.7984,
      "grad_norm": 0.2838205397129059,
      "learning_rate": 1.376e-05,
      "loss": 0.0017,
      "step": 108720
    },
    {
      "epoch": 5.798933333333333,
      "grad_norm": 0.20954382419586182,
      "learning_rate": 1.3756666666666668e-05,
      "loss": 0.0016,
      "step": 108730
    },
    {
      "epoch": 5.7994666666666665,
      "grad_norm": 0.03650208190083504,
      "learning_rate": 1.3753333333333334e-05,
      "loss": 0.0018,
      "step": 108740
    },
    {
      "epoch": 5.8,
      "grad_norm": 0.0922313705086708,
      "learning_rate": 1.3750000000000002e-05,
      "loss": 0.0014,
      "step": 108750
    },
    {
      "epoch": 5.800533333333333,
      "grad_norm": 0.25507068634033203,
      "learning_rate": 1.3746666666666667e-05,
      "loss": 0.0016,
      "step": 108760
    },
    {
      "epoch": 5.801066666666666,
      "grad_norm": 0.2008090317249298,
      "learning_rate": 1.3743333333333333e-05,
      "loss": 0.0026,
      "step": 108770
    },
    {
      "epoch": 5.8016,
      "grad_norm": 0.024573132395744324,
      "learning_rate": 1.374e-05,
      "loss": 0.0024,
      "step": 108780
    },
    {
      "epoch": 5.802133333333334,
      "grad_norm": 0.2040368616580963,
      "learning_rate": 1.3736666666666667e-05,
      "loss": 0.0014,
      "step": 108790
    },
    {
      "epoch": 5.802666666666667,
      "grad_norm": 0.11941932886838913,
      "learning_rate": 1.3733333333333335e-05,
      "loss": 0.0027,
      "step": 108800
    },
    {
      "epoch": 5.8032,
      "grad_norm": 0.04415404424071312,
      "learning_rate": 1.373e-05,
      "loss": 0.0027,
      "step": 108810
    },
    {
      "epoch": 5.803733333333334,
      "grad_norm": 0.5157672166824341,
      "learning_rate": 1.3726666666666669e-05,
      "loss": 0.0014,
      "step": 108820
    },
    {
      "epoch": 5.804266666666667,
      "grad_norm": 0.6272581815719604,
      "learning_rate": 1.3723333333333335e-05,
      "loss": 0.0014,
      "step": 108830
    },
    {
      "epoch": 5.8048,
      "grad_norm": 0.04491284862160683,
      "learning_rate": 1.3719999999999999e-05,
      "loss": 0.0019,
      "step": 108840
    },
    {
      "epoch": 5.8053333333333335,
      "grad_norm": 0.22043479979038239,
      "learning_rate": 1.3716666666666667e-05,
      "loss": 0.0019,
      "step": 108850
    },
    {
      "epoch": 5.805866666666667,
      "grad_norm": 0.09368917346000671,
      "learning_rate": 1.3713333333333333e-05,
      "loss": 0.0017,
      "step": 108860
    },
    {
      "epoch": 5.8064,
      "grad_norm": 0.17308858036994934,
      "learning_rate": 1.3710000000000001e-05,
      "loss": 0.002,
      "step": 108870
    },
    {
      "epoch": 5.806933333333333,
      "grad_norm": 0.2317972034215927,
      "learning_rate": 1.3706666666666667e-05,
      "loss": 0.0019,
      "step": 108880
    },
    {
      "epoch": 5.8074666666666666,
      "grad_norm": 0.09829311817884445,
      "learning_rate": 1.3703333333333335e-05,
      "loss": 0.0018,
      "step": 108890
    },
    {
      "epoch": 5.808,
      "grad_norm": 0.027149276807904243,
      "learning_rate": 1.3700000000000001e-05,
      "loss": 0.0015,
      "step": 108900
    },
    {
      "epoch": 5.808533333333333,
      "grad_norm": 0.28702086210250854,
      "learning_rate": 1.3696666666666665e-05,
      "loss": 0.0017,
      "step": 108910
    },
    {
      "epoch": 5.809066666666666,
      "grad_norm": 0.044820692390203476,
      "learning_rate": 1.3693333333333333e-05,
      "loss": 0.0023,
      "step": 108920
    },
    {
      "epoch": 5.8096,
      "grad_norm": 0.14800909161567688,
      "learning_rate": 1.369e-05,
      "loss": 0.0013,
      "step": 108930
    },
    {
      "epoch": 5.810133333333333,
      "grad_norm": 0.3622662127017975,
      "learning_rate": 1.3686666666666667e-05,
      "loss": 0.0028,
      "step": 108940
    },
    {
      "epoch": 5.810666666666666,
      "grad_norm": 0.3176989257335663,
      "learning_rate": 1.3683333333333333e-05,
      "loss": 0.0022,
      "step": 108950
    },
    {
      "epoch": 5.8112,
      "grad_norm": 0.26427075266838074,
      "learning_rate": 1.3680000000000001e-05,
      "loss": 0.0018,
      "step": 108960
    },
    {
      "epoch": 5.811733333333334,
      "grad_norm": 0.20252051949501038,
      "learning_rate": 1.3676666666666669e-05,
      "loss": 0.0026,
      "step": 108970
    },
    {
      "epoch": 5.812266666666667,
      "grad_norm": 0.06807034462690353,
      "learning_rate": 1.3673333333333335e-05,
      "loss": 0.0019,
      "step": 108980
    },
    {
      "epoch": 5.8128,
      "grad_norm": 0.7524133324623108,
      "learning_rate": 1.367e-05,
      "loss": 0.0024,
      "step": 108990
    },
    {
      "epoch": 5.8133333333333335,
      "grad_norm": 0.4575675427913666,
      "learning_rate": 1.3666666666666666e-05,
      "loss": 0.002,
      "step": 109000
    },
    {
      "epoch": 5.813866666666667,
      "grad_norm": 0.12050461769104004,
      "learning_rate": 1.3663333333333334e-05,
      "loss": 0.0021,
      "step": 109010
    },
    {
      "epoch": 5.8144,
      "grad_norm": 0.0862477645277977,
      "learning_rate": 1.3660000000000001e-05,
      "loss": 0.0026,
      "step": 109020
    },
    {
      "epoch": 5.814933333333333,
      "grad_norm": 0.1876169741153717,
      "learning_rate": 1.3656666666666667e-05,
      "loss": 0.0012,
      "step": 109030
    },
    {
      "epoch": 5.815466666666667,
      "grad_norm": 0.3444592356681824,
      "learning_rate": 1.3653333333333335e-05,
      "loss": 0.0012,
      "step": 109040
    },
    {
      "epoch": 5.816,
      "grad_norm": 0.18539831042289734,
      "learning_rate": 1.3650000000000001e-05,
      "loss": 0.0017,
      "step": 109050
    },
    {
      "epoch": 5.816533333333333,
      "grad_norm": 0.06820680946111679,
      "learning_rate": 1.3646666666666666e-05,
      "loss": 0.0018,
      "step": 109060
    },
    {
      "epoch": 5.817066666666666,
      "grad_norm": 0.047294605523347855,
      "learning_rate": 1.3643333333333334e-05,
      "loss": 0.0023,
      "step": 109070
    },
    {
      "epoch": 5.8176,
      "grad_norm": 0.14708726108074188,
      "learning_rate": 1.364e-05,
      "loss": 0.0019,
      "step": 109080
    },
    {
      "epoch": 5.818133333333334,
      "grad_norm": 0.367316335439682,
      "learning_rate": 1.3636666666666668e-05,
      "loss": 0.0016,
      "step": 109090
    },
    {
      "epoch": 5.818666666666667,
      "grad_norm": 0.48311686515808105,
      "learning_rate": 1.3633333333333334e-05,
      "loss": 0.0012,
      "step": 109100
    },
    {
      "epoch": 5.8192,
      "grad_norm": 0.3783627450466156,
      "learning_rate": 1.3630000000000002e-05,
      "loss": 0.002,
      "step": 109110
    },
    {
      "epoch": 5.819733333333334,
      "grad_norm": 0.1772744208574295,
      "learning_rate": 1.3626666666666668e-05,
      "loss": 0.0015,
      "step": 109120
    },
    {
      "epoch": 5.820266666666667,
      "grad_norm": 0.43013936281204224,
      "learning_rate": 1.3623333333333336e-05,
      "loss": 0.0019,
      "step": 109130
    },
    {
      "epoch": 5.8208,
      "grad_norm": 0.027107276022434235,
      "learning_rate": 1.362e-05,
      "loss": 0.0019,
      "step": 109140
    },
    {
      "epoch": 5.8213333333333335,
      "grad_norm": 0.2320283204317093,
      "learning_rate": 1.3616666666666666e-05,
      "loss": 0.0019,
      "step": 109150
    },
    {
      "epoch": 5.821866666666667,
      "grad_norm": 0.04090236499905586,
      "learning_rate": 1.3613333333333334e-05,
      "loss": 0.002,
      "step": 109160
    },
    {
      "epoch": 5.8224,
      "grad_norm": 0.11435572057962418,
      "learning_rate": 1.361e-05,
      "loss": 0.002,
      "step": 109170
    },
    {
      "epoch": 5.822933333333333,
      "grad_norm": 0.09669166803359985,
      "learning_rate": 1.3606666666666668e-05,
      "loss": 0.002,
      "step": 109180
    },
    {
      "epoch": 5.823466666666667,
      "grad_norm": 0.10103010386228561,
      "learning_rate": 1.3603333333333334e-05,
      "loss": 0.0017,
      "step": 109190
    },
    {
      "epoch": 5.824,
      "grad_norm": 0.40574967861175537,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 0.0017,
      "step": 109200
    },
    {
      "epoch": 5.824533333333333,
      "grad_norm": 0.17689314484596252,
      "learning_rate": 1.3596666666666668e-05,
      "loss": 0.0029,
      "step": 109210
    },
    {
      "epoch": 5.825066666666666,
      "grad_norm": 0.44822704792022705,
      "learning_rate": 1.3593333333333332e-05,
      "loss": 0.0014,
      "step": 109220
    },
    {
      "epoch": 5.8256,
      "grad_norm": 0.019301410764455795,
      "learning_rate": 1.359e-05,
      "loss": 0.0018,
      "step": 109230
    },
    {
      "epoch": 5.826133333333333,
      "grad_norm": 0.11546148359775543,
      "learning_rate": 1.3586666666666666e-05,
      "loss": 0.0021,
      "step": 109240
    },
    {
      "epoch": 5.826666666666666,
      "grad_norm": 0.4025653600692749,
      "learning_rate": 1.3583333333333334e-05,
      "loss": 0.002,
      "step": 109250
    },
    {
      "epoch": 5.8272,
      "grad_norm": 0.5169142484664917,
      "learning_rate": 1.358e-05,
      "loss": 0.0026,
      "step": 109260
    },
    {
      "epoch": 5.827733333333334,
      "grad_norm": 0.13354180753231049,
      "learning_rate": 1.3576666666666668e-05,
      "loss": 0.0024,
      "step": 109270
    },
    {
      "epoch": 5.828266666666667,
      "grad_norm": 0.14358487725257874,
      "learning_rate": 1.3573333333333334e-05,
      "loss": 0.0027,
      "step": 109280
    },
    {
      "epoch": 5.8288,
      "grad_norm": 0.1514899879693985,
      "learning_rate": 1.3569999999999999e-05,
      "loss": 0.0024,
      "step": 109290
    },
    {
      "epoch": 5.8293333333333335,
      "grad_norm": 0.08838000893592834,
      "learning_rate": 1.3566666666666667e-05,
      "loss": 0.0017,
      "step": 109300
    },
    {
      "epoch": 5.829866666666667,
      "grad_norm": 0.2672417461872101,
      "learning_rate": 1.3563333333333333e-05,
      "loss": 0.0017,
      "step": 109310
    },
    {
      "epoch": 5.8304,
      "grad_norm": 0.09184354543685913,
      "learning_rate": 1.356e-05,
      "loss": 0.0015,
      "step": 109320
    },
    {
      "epoch": 5.830933333333333,
      "grad_norm": 0.19146734476089478,
      "learning_rate": 1.3556666666666667e-05,
      "loss": 0.002,
      "step": 109330
    },
    {
      "epoch": 5.831466666666667,
      "grad_norm": 0.4984975755214691,
      "learning_rate": 1.3553333333333335e-05,
      "loss": 0.0027,
      "step": 109340
    },
    {
      "epoch": 5.832,
      "grad_norm": 0.3670042157173157,
      "learning_rate": 1.3550000000000002e-05,
      "loss": 0.0016,
      "step": 109350
    },
    {
      "epoch": 5.832533333333333,
      "grad_norm": 0.20167919993400574,
      "learning_rate": 1.3546666666666669e-05,
      "loss": 0.0016,
      "step": 109360
    },
    {
      "epoch": 5.833066666666666,
      "grad_norm": 0.4083081781864166,
      "learning_rate": 1.3543333333333333e-05,
      "loss": 0.0016,
      "step": 109370
    },
    {
      "epoch": 5.8336,
      "grad_norm": 0.12337836623191833,
      "learning_rate": 1.3539999999999999e-05,
      "loss": 0.0024,
      "step": 109380
    },
    {
      "epoch": 5.834133333333333,
      "grad_norm": 0.43645086884498596,
      "learning_rate": 1.3536666666666667e-05,
      "loss": 0.0017,
      "step": 109390
    },
    {
      "epoch": 5.834666666666667,
      "grad_norm": 0.3013593256473541,
      "learning_rate": 1.3533333333333335e-05,
      "loss": 0.0021,
      "step": 109400
    },
    {
      "epoch": 5.8352,
      "grad_norm": 0.038046494126319885,
      "learning_rate": 1.3530000000000001e-05,
      "loss": 0.0015,
      "step": 109410
    },
    {
      "epoch": 5.835733333333334,
      "grad_norm": 0.037248823791742325,
      "learning_rate": 1.3526666666666669e-05,
      "loss": 0.0017,
      "step": 109420
    },
    {
      "epoch": 5.836266666666667,
      "grad_norm": 0.0755973756313324,
      "learning_rate": 1.3523333333333335e-05,
      "loss": 0.0025,
      "step": 109430
    },
    {
      "epoch": 5.8368,
      "grad_norm": 0.37045395374298096,
      "learning_rate": 1.352e-05,
      "loss": 0.0021,
      "step": 109440
    },
    {
      "epoch": 5.8373333333333335,
      "grad_norm": 0.32093545794487,
      "learning_rate": 1.3516666666666667e-05,
      "loss": 0.0017,
      "step": 109450
    },
    {
      "epoch": 5.837866666666667,
      "grad_norm": 0.34677478671073914,
      "learning_rate": 1.3513333333333333e-05,
      "loss": 0.0013,
      "step": 109460
    },
    {
      "epoch": 5.8384,
      "grad_norm": 0.21176651120185852,
      "learning_rate": 1.3510000000000001e-05,
      "loss": 0.0017,
      "step": 109470
    },
    {
      "epoch": 5.838933333333333,
      "grad_norm": 0.18631474673748016,
      "learning_rate": 1.3506666666666667e-05,
      "loss": 0.0018,
      "step": 109480
    },
    {
      "epoch": 5.839466666666667,
      "grad_norm": 0.2018337994813919,
      "learning_rate": 1.3503333333333335e-05,
      "loss": 0.0018,
      "step": 109490
    },
    {
      "epoch": 5.84,
      "grad_norm": 0.07173798978328705,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 0.0025,
      "step": 109500
    },
    {
      "epoch": 5.840533333333333,
      "grad_norm": 0.2566811442375183,
      "learning_rate": 1.3496666666666669e-05,
      "loss": 0.0019,
      "step": 109510
    },
    {
      "epoch": 5.841066666666666,
      "grad_norm": 0.08788274973630905,
      "learning_rate": 1.3493333333333333e-05,
      "loss": 0.0019,
      "step": 109520
    },
    {
      "epoch": 5.8416,
      "grad_norm": 0.26203134655952454,
      "learning_rate": 1.349e-05,
      "loss": 0.0017,
      "step": 109530
    },
    {
      "epoch": 5.842133333333333,
      "grad_norm": 0.23883242905139923,
      "learning_rate": 1.3486666666666667e-05,
      "loss": 0.0019,
      "step": 109540
    },
    {
      "epoch": 5.842666666666666,
      "grad_norm": 0.20316176116466522,
      "learning_rate": 1.3483333333333334e-05,
      "loss": 0.0031,
      "step": 109550
    },
    {
      "epoch": 5.8431999999999995,
      "grad_norm": 0.17467935383319855,
      "learning_rate": 1.3480000000000001e-05,
      "loss": 0.0016,
      "step": 109560
    },
    {
      "epoch": 5.843733333333334,
      "grad_norm": 0.05567144602537155,
      "learning_rate": 1.3476666666666668e-05,
      "loss": 0.0033,
      "step": 109570
    },
    {
      "epoch": 5.844266666666667,
      "grad_norm": 0.37259575724601746,
      "learning_rate": 1.3473333333333335e-05,
      "loss": 0.0016,
      "step": 109580
    },
    {
      "epoch": 5.8448,
      "grad_norm": 0.23291435837745667,
      "learning_rate": 1.347e-05,
      "loss": 0.003,
      "step": 109590
    },
    {
      "epoch": 5.8453333333333335,
      "grad_norm": 0.1142185851931572,
      "learning_rate": 1.3466666666666666e-05,
      "loss": 0.0014,
      "step": 109600
    },
    {
      "epoch": 5.845866666666667,
      "grad_norm": 0.5477026700973511,
      "learning_rate": 1.3463333333333334e-05,
      "loss": 0.0022,
      "step": 109610
    },
    {
      "epoch": 5.8464,
      "grad_norm": 0.12060556560754776,
      "learning_rate": 1.346e-05,
      "loss": 0.002,
      "step": 109620
    },
    {
      "epoch": 5.846933333333333,
      "grad_norm": 0.2069588303565979,
      "learning_rate": 1.3456666666666668e-05,
      "loss": 0.0024,
      "step": 109630
    },
    {
      "epoch": 5.847466666666667,
      "grad_norm": 0.09595458209514618,
      "learning_rate": 1.3453333333333334e-05,
      "loss": 0.0015,
      "step": 109640
    },
    {
      "epoch": 5.848,
      "grad_norm": 0.25603294372558594,
      "learning_rate": 1.3450000000000002e-05,
      "loss": 0.0017,
      "step": 109650
    },
    {
      "epoch": 5.848533333333333,
      "grad_norm": 0.2001175880432129,
      "learning_rate": 1.3446666666666668e-05,
      "loss": 0.0015,
      "step": 109660
    },
    {
      "epoch": 5.849066666666666,
      "grad_norm": 0.26192837953567505,
      "learning_rate": 1.3443333333333332e-05,
      "loss": 0.0021,
      "step": 109670
    },
    {
      "epoch": 5.8496,
      "grad_norm": 0.36583173274993896,
      "learning_rate": 1.344e-05,
      "loss": 0.002,
      "step": 109680
    },
    {
      "epoch": 5.850133333333333,
      "grad_norm": 0.296728253364563,
      "learning_rate": 1.3436666666666666e-05,
      "loss": 0.0016,
      "step": 109690
    },
    {
      "epoch": 5.850666666666667,
      "grad_norm": 0.0365082323551178,
      "learning_rate": 1.3433333333333334e-05,
      "loss": 0.0022,
      "step": 109700
    },
    {
      "epoch": 5.8512,
      "grad_norm": 0.2952592074871063,
      "learning_rate": 1.343e-05,
      "loss": 0.0025,
      "step": 109710
    },
    {
      "epoch": 5.851733333333334,
      "grad_norm": 0.1453842669725418,
      "learning_rate": 1.3426666666666668e-05,
      "loss": 0.002,
      "step": 109720
    },
    {
      "epoch": 5.852266666666667,
      "grad_norm": 0.2649392783641815,
      "learning_rate": 1.3423333333333336e-05,
      "loss": 0.0013,
      "step": 109730
    },
    {
      "epoch": 5.8528,
      "grad_norm": 0.12542594969272614,
      "learning_rate": 1.3420000000000002e-05,
      "loss": 0.0021,
      "step": 109740
    },
    {
      "epoch": 5.8533333333333335,
      "grad_norm": 0.5039718747138977,
      "learning_rate": 1.3416666666666666e-05,
      "loss": 0.002,
      "step": 109750
    },
    {
      "epoch": 5.853866666666667,
      "grad_norm": 0.1624073088169098,
      "learning_rate": 1.3413333333333333e-05,
      "loss": 0.0015,
      "step": 109760
    },
    {
      "epoch": 5.8544,
      "grad_norm": 0.25881749391555786,
      "learning_rate": 1.341e-05,
      "loss": 0.0019,
      "step": 109770
    },
    {
      "epoch": 5.854933333333333,
      "grad_norm": 0.2412966638803482,
      "learning_rate": 1.3406666666666668e-05,
      "loss": 0.0034,
      "step": 109780
    },
    {
      "epoch": 5.855466666666667,
      "grad_norm": 0.11862895637750626,
      "learning_rate": 1.3403333333333334e-05,
      "loss": 0.0022,
      "step": 109790
    },
    {
      "epoch": 5.856,
      "grad_norm": 0.14283956587314606,
      "learning_rate": 1.3400000000000002e-05,
      "loss": 0.0019,
      "step": 109800
    },
    {
      "epoch": 5.856533333333333,
      "grad_norm": 0.21363064646720886,
      "learning_rate": 1.3396666666666668e-05,
      "loss": 0.002,
      "step": 109810
    },
    {
      "epoch": 5.857066666666666,
      "grad_norm": 0.20110593736171722,
      "learning_rate": 1.3393333333333333e-05,
      "loss": 0.0016,
      "step": 109820
    },
    {
      "epoch": 5.8576,
      "grad_norm": 0.29565536975860596,
      "learning_rate": 1.339e-05,
      "loss": 0.0016,
      "step": 109830
    },
    {
      "epoch": 5.858133333333333,
      "grad_norm": 0.19266365468502045,
      "learning_rate": 1.3386666666666667e-05,
      "loss": 0.0017,
      "step": 109840
    },
    {
      "epoch": 5.858666666666666,
      "grad_norm": 0.5151097178459167,
      "learning_rate": 1.3383333333333335e-05,
      "loss": 0.0024,
      "step": 109850
    },
    {
      "epoch": 5.8591999999999995,
      "grad_norm": 0.2573795020580292,
      "learning_rate": 1.338e-05,
      "loss": 0.0015,
      "step": 109860
    },
    {
      "epoch": 5.859733333333334,
      "grad_norm": 0.3087630569934845,
      "learning_rate": 1.3376666666666668e-05,
      "loss": 0.002,
      "step": 109870
    },
    {
      "epoch": 5.860266666666667,
      "grad_norm": 0.6708810925483704,
      "learning_rate": 1.3373333333333335e-05,
      "loss": 0.0021,
      "step": 109880
    },
    {
      "epoch": 5.8608,
      "grad_norm": 0.12262848019599915,
      "learning_rate": 1.3370000000000002e-05,
      "loss": 0.0027,
      "step": 109890
    },
    {
      "epoch": 5.8613333333333335,
      "grad_norm": 0.2620756924152374,
      "learning_rate": 1.3366666666666667e-05,
      "loss": 0.002,
      "step": 109900
    },
    {
      "epoch": 5.861866666666667,
      "grad_norm": 0.17359454929828644,
      "learning_rate": 1.3363333333333333e-05,
      "loss": 0.002,
      "step": 109910
    },
    {
      "epoch": 5.8624,
      "grad_norm": 0.08552189916372299,
      "learning_rate": 1.336e-05,
      "loss": 0.0016,
      "step": 109920
    },
    {
      "epoch": 5.862933333333333,
      "grad_norm": 0.42815306782722473,
      "learning_rate": 1.3356666666666667e-05,
      "loss": 0.002,
      "step": 109930
    },
    {
      "epoch": 5.863466666666667,
      "grad_norm": 0.06691069900989532,
      "learning_rate": 1.3353333333333335e-05,
      "loss": 0.0015,
      "step": 109940
    },
    {
      "epoch": 5.864,
      "grad_norm": 0.29165300726890564,
      "learning_rate": 1.3350000000000001e-05,
      "loss": 0.0026,
      "step": 109950
    },
    {
      "epoch": 5.864533333333333,
      "grad_norm": 0.1184844896197319,
      "learning_rate": 1.3346666666666669e-05,
      "loss": 0.004,
      "step": 109960
    },
    {
      "epoch": 5.865066666666666,
      "grad_norm": 0.22887498140335083,
      "learning_rate": 1.3343333333333333e-05,
      "loss": 0.0016,
      "step": 109970
    },
    {
      "epoch": 5.8656,
      "grad_norm": 0.088419109582901,
      "learning_rate": 1.334e-05,
      "loss": 0.0012,
      "step": 109980
    },
    {
      "epoch": 5.866133333333333,
      "grad_norm": 0.19689282774925232,
      "learning_rate": 1.3336666666666667e-05,
      "loss": 0.0018,
      "step": 109990
    },
    {
      "epoch": 5.866666666666667,
      "grad_norm": 0.37834224104881287,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.0016,
      "step": 110000
    },
    {
      "epoch": 5.8672,
      "grad_norm": 0.06128065288066864,
      "learning_rate": 1.3330000000000001e-05,
      "loss": 0.002,
      "step": 110010
    },
    {
      "epoch": 5.867733333333334,
      "grad_norm": 0.17013901472091675,
      "learning_rate": 1.3326666666666667e-05,
      "loss": 0.0018,
      "step": 110020
    },
    {
      "epoch": 5.868266666666667,
      "grad_norm": 0.18478138744831085,
      "learning_rate": 1.3323333333333335e-05,
      "loss": 0.0021,
      "step": 110030
    },
    {
      "epoch": 5.8688,
      "grad_norm": 0.08006065338850021,
      "learning_rate": 1.3320000000000001e-05,
      "loss": 0.0019,
      "step": 110040
    },
    {
      "epoch": 5.8693333333333335,
      "grad_norm": 0.5916934609413147,
      "learning_rate": 1.3316666666666666e-05,
      "loss": 0.0024,
      "step": 110050
    },
    {
      "epoch": 5.869866666666667,
      "grad_norm": 0.37912026047706604,
      "learning_rate": 1.3313333333333333e-05,
      "loss": 0.0018,
      "step": 110060
    },
    {
      "epoch": 5.8704,
      "grad_norm": 0.4390266239643097,
      "learning_rate": 1.331e-05,
      "loss": 0.0018,
      "step": 110070
    },
    {
      "epoch": 5.870933333333333,
      "grad_norm": 0.3715883195400238,
      "learning_rate": 1.3306666666666667e-05,
      "loss": 0.0017,
      "step": 110080
    },
    {
      "epoch": 5.871466666666667,
      "grad_norm": 0.3528991937637329,
      "learning_rate": 1.3303333333333334e-05,
      "loss": 0.0015,
      "step": 110090
    },
    {
      "epoch": 5.872,
      "grad_norm": 0.32676759362220764,
      "learning_rate": 1.3300000000000001e-05,
      "loss": 0.0014,
      "step": 110100
    },
    {
      "epoch": 5.872533333333333,
      "grad_norm": 0.09307586401700974,
      "learning_rate": 1.3296666666666668e-05,
      "loss": 0.0025,
      "step": 110110
    },
    {
      "epoch": 5.873066666666666,
      "grad_norm": 0.08809610456228256,
      "learning_rate": 1.3293333333333332e-05,
      "loss": 0.0017,
      "step": 110120
    },
    {
      "epoch": 5.8736,
      "grad_norm": 0.12694744765758514,
      "learning_rate": 1.329e-05,
      "loss": 0.002,
      "step": 110130
    },
    {
      "epoch": 5.874133333333333,
      "grad_norm": 0.07233772426843643,
      "learning_rate": 1.3286666666666666e-05,
      "loss": 0.0018,
      "step": 110140
    },
    {
      "epoch": 5.874666666666666,
      "grad_norm": 0.35338693857192993,
      "learning_rate": 1.3283333333333334e-05,
      "loss": 0.0021,
      "step": 110150
    },
    {
      "epoch": 5.8751999999999995,
      "grad_norm": 0.26596489548683167,
      "learning_rate": 1.3280000000000002e-05,
      "loss": 0.0016,
      "step": 110160
    },
    {
      "epoch": 5.875733333333334,
      "grad_norm": 0.3485907316207886,
      "learning_rate": 1.3276666666666668e-05,
      "loss": 0.0023,
      "step": 110170
    },
    {
      "epoch": 5.876266666666667,
      "grad_norm": 0.2901798486709595,
      "learning_rate": 1.3273333333333336e-05,
      "loss": 0.0025,
      "step": 110180
    },
    {
      "epoch": 5.8768,
      "grad_norm": 0.07580303400754929,
      "learning_rate": 1.3270000000000002e-05,
      "loss": 0.002,
      "step": 110190
    },
    {
      "epoch": 5.8773333333333335,
      "grad_norm": 0.2360415905714035,
      "learning_rate": 1.3266666666666666e-05,
      "loss": 0.0015,
      "step": 110200
    },
    {
      "epoch": 5.877866666666667,
      "grad_norm": 0.07392223179340363,
      "learning_rate": 1.3263333333333334e-05,
      "loss": 0.0022,
      "step": 110210
    },
    {
      "epoch": 5.8784,
      "grad_norm": 0.5433358550071716,
      "learning_rate": 1.326e-05,
      "loss": 0.0026,
      "step": 110220
    },
    {
      "epoch": 5.878933333333333,
      "grad_norm": 0.3117593824863434,
      "learning_rate": 1.3256666666666668e-05,
      "loss": 0.002,
      "step": 110230
    },
    {
      "epoch": 5.879466666666667,
      "grad_norm": 0.2076106071472168,
      "learning_rate": 1.3253333333333334e-05,
      "loss": 0.0021,
      "step": 110240
    },
    {
      "epoch": 5.88,
      "grad_norm": 0.18834258615970612,
      "learning_rate": 1.3250000000000002e-05,
      "loss": 0.002,
      "step": 110250
    },
    {
      "epoch": 5.880533333333333,
      "grad_norm": 0.22624057531356812,
      "learning_rate": 1.3246666666666668e-05,
      "loss": 0.0014,
      "step": 110260
    },
    {
      "epoch": 5.881066666666666,
      "grad_norm": 0.2880542278289795,
      "learning_rate": 1.3243333333333332e-05,
      "loss": 0.0018,
      "step": 110270
    },
    {
      "epoch": 5.8816,
      "grad_norm": 0.7464252710342407,
      "learning_rate": 1.324e-05,
      "loss": 0.0026,
      "step": 110280
    },
    {
      "epoch": 5.882133333333333,
      "grad_norm": 0.19322599470615387,
      "learning_rate": 1.3236666666666666e-05,
      "loss": 0.0023,
      "step": 110290
    },
    {
      "epoch": 5.882666666666667,
      "grad_norm": 0.1699439138174057,
      "learning_rate": 1.3233333333333334e-05,
      "loss": 0.0031,
      "step": 110300
    },
    {
      "epoch": 5.8832,
      "grad_norm": 0.6255612969398499,
      "learning_rate": 1.323e-05,
      "loss": 0.0014,
      "step": 110310
    },
    {
      "epoch": 5.883733333333334,
      "grad_norm": 0.06617693603038788,
      "learning_rate": 1.3226666666666668e-05,
      "loss": 0.0019,
      "step": 110320
    },
    {
      "epoch": 5.884266666666667,
      "grad_norm": 0.07659977674484253,
      "learning_rate": 1.3223333333333334e-05,
      "loss": 0.0015,
      "step": 110330
    },
    {
      "epoch": 5.8848,
      "grad_norm": 0.2617459297180176,
      "learning_rate": 1.3220000000000002e-05,
      "loss": 0.0021,
      "step": 110340
    },
    {
      "epoch": 5.8853333333333335,
      "grad_norm": 0.25658851861953735,
      "learning_rate": 1.3216666666666667e-05,
      "loss": 0.0015,
      "step": 110350
    },
    {
      "epoch": 5.885866666666667,
      "grad_norm": 0.37953826785087585,
      "learning_rate": 1.3213333333333333e-05,
      "loss": 0.0024,
      "step": 110360
    },
    {
      "epoch": 5.8864,
      "grad_norm": 0.05815662816166878,
      "learning_rate": 1.321e-05,
      "loss": 0.0018,
      "step": 110370
    },
    {
      "epoch": 5.886933333333333,
      "grad_norm": 0.05435558408498764,
      "learning_rate": 1.3206666666666667e-05,
      "loss": 0.0028,
      "step": 110380
    },
    {
      "epoch": 5.887466666666667,
      "grad_norm": 0.17250306904315948,
      "learning_rate": 1.3203333333333335e-05,
      "loss": 0.0018,
      "step": 110390
    },
    {
      "epoch": 5.888,
      "grad_norm": 0.150843545794487,
      "learning_rate": 1.32e-05,
      "loss": 0.0025,
      "step": 110400
    },
    {
      "epoch": 5.888533333333333,
      "grad_norm": 0.1691901683807373,
      "learning_rate": 1.3196666666666669e-05,
      "loss": 0.0019,
      "step": 110410
    },
    {
      "epoch": 5.8890666666666664,
      "grad_norm": 0.1494675874710083,
      "learning_rate": 1.3193333333333335e-05,
      "loss": 0.0023,
      "step": 110420
    },
    {
      "epoch": 5.8896,
      "grad_norm": 0.17949774861335754,
      "learning_rate": 1.3189999999999999e-05,
      "loss": 0.0016,
      "step": 110430
    },
    {
      "epoch": 5.890133333333333,
      "grad_norm": 0.22988010942935944,
      "learning_rate": 1.3186666666666667e-05,
      "loss": 0.0017,
      "step": 110440
    },
    {
      "epoch": 5.890666666666666,
      "grad_norm": 0.20039096474647522,
      "learning_rate": 1.3183333333333333e-05,
      "loss": 0.0022,
      "step": 110450
    },
    {
      "epoch": 5.8911999999999995,
      "grad_norm": 0.18108880519866943,
      "learning_rate": 1.3180000000000001e-05,
      "loss": 0.0017,
      "step": 110460
    },
    {
      "epoch": 5.891733333333334,
      "grad_norm": 0.09379085898399353,
      "learning_rate": 1.3176666666666667e-05,
      "loss": 0.0017,
      "step": 110470
    },
    {
      "epoch": 5.892266666666667,
      "grad_norm": 0.17532005906105042,
      "learning_rate": 1.3173333333333335e-05,
      "loss": 0.0017,
      "step": 110480
    },
    {
      "epoch": 5.8928,
      "grad_norm": 0.343071848154068,
      "learning_rate": 1.3170000000000001e-05,
      "loss": 0.0017,
      "step": 110490
    },
    {
      "epoch": 5.8933333333333335,
      "grad_norm": 0.23828791081905365,
      "learning_rate": 1.3166666666666665e-05,
      "loss": 0.0015,
      "step": 110500
    },
    {
      "epoch": 5.893866666666667,
      "grad_norm": 0.07293965667486191,
      "learning_rate": 1.3163333333333333e-05,
      "loss": 0.0021,
      "step": 110510
    },
    {
      "epoch": 5.8944,
      "grad_norm": 0.35578346252441406,
      "learning_rate": 1.316e-05,
      "loss": 0.0014,
      "step": 110520
    },
    {
      "epoch": 5.894933333333333,
      "grad_norm": 0.06083753705024719,
      "learning_rate": 1.3156666666666667e-05,
      "loss": 0.0018,
      "step": 110530
    },
    {
      "epoch": 5.895466666666667,
      "grad_norm": 0.28459134697914124,
      "learning_rate": 1.3153333333333335e-05,
      "loss": 0.0013,
      "step": 110540
    },
    {
      "epoch": 5.896,
      "grad_norm": 0.15536245703697205,
      "learning_rate": 1.3150000000000001e-05,
      "loss": 0.0016,
      "step": 110550
    },
    {
      "epoch": 5.896533333333333,
      "grad_norm": 0.08661242574453354,
      "learning_rate": 1.3146666666666669e-05,
      "loss": 0.0017,
      "step": 110560
    },
    {
      "epoch": 5.8970666666666665,
      "grad_norm": 0.11602833867073059,
      "learning_rate": 1.3143333333333335e-05,
      "loss": 0.0023,
      "step": 110570
    },
    {
      "epoch": 5.8976,
      "grad_norm": 0.23174771666526794,
      "learning_rate": 1.314e-05,
      "loss": 0.0014,
      "step": 110580
    },
    {
      "epoch": 5.898133333333333,
      "grad_norm": 0.06912592798471451,
      "learning_rate": 1.3136666666666667e-05,
      "loss": 0.0016,
      "step": 110590
    },
    {
      "epoch": 5.898666666666666,
      "grad_norm": 0.07279238104820251,
      "learning_rate": 1.3133333333333334e-05,
      "loss": 0.0016,
      "step": 110600
    },
    {
      "epoch": 5.8992,
      "grad_norm": 0.148972749710083,
      "learning_rate": 1.3130000000000001e-05,
      "loss": 0.0015,
      "step": 110610
    },
    {
      "epoch": 5.899733333333334,
      "grad_norm": 0.23116645216941833,
      "learning_rate": 1.3126666666666667e-05,
      "loss": 0.0022,
      "step": 110620
    },
    {
      "epoch": 5.900266666666667,
      "grad_norm": 0.15080666542053223,
      "learning_rate": 1.3123333333333335e-05,
      "loss": 0.0015,
      "step": 110630
    },
    {
      "epoch": 5.9008,
      "grad_norm": 0.09651265293359756,
      "learning_rate": 1.3120000000000001e-05,
      "loss": 0.0022,
      "step": 110640
    },
    {
      "epoch": 5.9013333333333335,
      "grad_norm": 0.06626519560813904,
      "learning_rate": 1.3116666666666666e-05,
      "loss": 0.0031,
      "step": 110650
    },
    {
      "epoch": 5.901866666666667,
      "grad_norm": 0.2701781690120697,
      "learning_rate": 1.3113333333333334e-05,
      "loss": 0.0023,
      "step": 110660
    },
    {
      "epoch": 5.9024,
      "grad_norm": 0.3143913447856903,
      "learning_rate": 1.311e-05,
      "loss": 0.0024,
      "step": 110670
    },
    {
      "epoch": 5.902933333333333,
      "grad_norm": 0.20725923776626587,
      "learning_rate": 1.3106666666666668e-05,
      "loss": 0.0013,
      "step": 110680
    },
    {
      "epoch": 5.903466666666667,
      "grad_norm": 0.2601378262042999,
      "learning_rate": 1.3103333333333334e-05,
      "loss": 0.0016,
      "step": 110690
    },
    {
      "epoch": 5.904,
      "grad_norm": 0.542619526386261,
      "learning_rate": 1.3100000000000002e-05,
      "loss": 0.0024,
      "step": 110700
    },
    {
      "epoch": 5.904533333333333,
      "grad_norm": 0.31223541498184204,
      "learning_rate": 1.3096666666666668e-05,
      "loss": 0.0023,
      "step": 110710
    },
    {
      "epoch": 5.9050666666666665,
      "grad_norm": 0.26558780670166016,
      "learning_rate": 1.3093333333333336e-05,
      "loss": 0.0021,
      "step": 110720
    },
    {
      "epoch": 5.9056,
      "grad_norm": 0.5923792719841003,
      "learning_rate": 1.309e-05,
      "loss": 0.0035,
      "step": 110730
    },
    {
      "epoch": 5.906133333333333,
      "grad_norm": 0.5480349659919739,
      "learning_rate": 1.3086666666666666e-05,
      "loss": 0.0015,
      "step": 110740
    },
    {
      "epoch": 5.906666666666666,
      "grad_norm": 0.1758335381746292,
      "learning_rate": 1.3083333333333334e-05,
      "loss": 0.0014,
      "step": 110750
    },
    {
      "epoch": 5.9072,
      "grad_norm": 0.061832379549741745,
      "learning_rate": 1.308e-05,
      "loss": 0.0017,
      "step": 110760
    },
    {
      "epoch": 5.907733333333333,
      "grad_norm": 0.14342984557151794,
      "learning_rate": 1.3076666666666668e-05,
      "loss": 0.0027,
      "step": 110770
    },
    {
      "epoch": 5.908266666666667,
      "grad_norm": 0.2009555846452713,
      "learning_rate": 1.3073333333333334e-05,
      "loss": 0.0023,
      "step": 110780
    },
    {
      "epoch": 5.9088,
      "grad_norm": 0.14395403861999512,
      "learning_rate": 1.3070000000000002e-05,
      "loss": 0.0013,
      "step": 110790
    },
    {
      "epoch": 5.9093333333333335,
      "grad_norm": 0.25772103667259216,
      "learning_rate": 1.3066666666666666e-05,
      "loss": 0.0024,
      "step": 110800
    },
    {
      "epoch": 5.909866666666667,
      "grad_norm": 0.04656211659312248,
      "learning_rate": 1.3063333333333332e-05,
      "loss": 0.0015,
      "step": 110810
    },
    {
      "epoch": 5.9104,
      "grad_norm": 0.08014728873968124,
      "learning_rate": 1.306e-05,
      "loss": 0.0019,
      "step": 110820
    },
    {
      "epoch": 5.910933333333333,
      "grad_norm": 0.14473511278629303,
      "learning_rate": 1.3056666666666666e-05,
      "loss": 0.0024,
      "step": 110830
    },
    {
      "epoch": 5.911466666666667,
      "grad_norm": 0.19945082068443298,
      "learning_rate": 1.3053333333333334e-05,
      "loss": 0.0015,
      "step": 110840
    },
    {
      "epoch": 5.912,
      "grad_norm": 0.12757346034049988,
      "learning_rate": 1.305e-05,
      "loss": 0.0024,
      "step": 110850
    },
    {
      "epoch": 5.912533333333333,
      "grad_norm": 0.1425333470106125,
      "learning_rate": 1.3046666666666668e-05,
      "loss": 0.0025,
      "step": 110860
    },
    {
      "epoch": 5.9130666666666665,
      "grad_norm": 0.49271267652511597,
      "learning_rate": 1.3043333333333334e-05,
      "loss": 0.0019,
      "step": 110870
    },
    {
      "epoch": 5.9136,
      "grad_norm": 0.07791826874017715,
      "learning_rate": 1.3039999999999999e-05,
      "loss": 0.0017,
      "step": 110880
    },
    {
      "epoch": 5.914133333333333,
      "grad_norm": 0.03673413023352623,
      "learning_rate": 1.3036666666666667e-05,
      "loss": 0.002,
      "step": 110890
    },
    {
      "epoch": 5.914666666666666,
      "grad_norm": 0.03962766006588936,
      "learning_rate": 1.3033333333333333e-05,
      "loss": 0.0025,
      "step": 110900
    },
    {
      "epoch": 5.9152000000000005,
      "grad_norm": 0.1470629721879959,
      "learning_rate": 1.303e-05,
      "loss": 0.0018,
      "step": 110910
    },
    {
      "epoch": 5.915733333333334,
      "grad_norm": 0.23776066303253174,
      "learning_rate": 1.3026666666666667e-05,
      "loss": 0.0015,
      "step": 110920
    },
    {
      "epoch": 5.916266666666667,
      "grad_norm": 0.03608062118291855,
      "learning_rate": 1.3023333333333335e-05,
      "loss": 0.0021,
      "step": 110930
    },
    {
      "epoch": 5.9168,
      "grad_norm": 0.4088743329048157,
      "learning_rate": 1.3020000000000002e-05,
      "loss": 0.0018,
      "step": 110940
    },
    {
      "epoch": 5.917333333333334,
      "grad_norm": 0.19947879016399384,
      "learning_rate": 1.3016666666666669e-05,
      "loss": 0.0017,
      "step": 110950
    },
    {
      "epoch": 5.917866666666667,
      "grad_norm": 0.09845900535583496,
      "learning_rate": 1.3013333333333333e-05,
      "loss": 0.0017,
      "step": 110960
    },
    {
      "epoch": 5.9184,
      "grad_norm": 0.28438276052474976,
      "learning_rate": 1.301e-05,
      "loss": 0.0016,
      "step": 110970
    },
    {
      "epoch": 5.918933333333333,
      "grad_norm": 0.029537130147218704,
      "learning_rate": 1.3006666666666667e-05,
      "loss": 0.0024,
      "step": 110980
    },
    {
      "epoch": 5.919466666666667,
      "grad_norm": 0.11826713383197784,
      "learning_rate": 1.3003333333333335e-05,
      "loss": 0.0025,
      "step": 110990
    },
    {
      "epoch": 5.92,
      "grad_norm": 0.0515616238117218,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.0015,
      "step": 111000
    },
    {
      "epoch": 5.920533333333333,
      "grad_norm": 0.06762252748012543,
      "learning_rate": 1.2996666666666669e-05,
      "loss": 0.0018,
      "step": 111010
    },
    {
      "epoch": 5.9210666666666665,
      "grad_norm": 0.059193018823862076,
      "learning_rate": 1.2993333333333335e-05,
      "loss": 0.0026,
      "step": 111020
    },
    {
      "epoch": 5.9216,
      "grad_norm": 0.0910315290093422,
      "learning_rate": 1.299e-05,
      "loss": 0.0027,
      "step": 111030
    },
    {
      "epoch": 5.922133333333333,
      "grad_norm": 0.5141233205795288,
      "learning_rate": 1.2986666666666667e-05,
      "loss": 0.0018,
      "step": 111040
    },
    {
      "epoch": 5.922666666666666,
      "grad_norm": 0.5486846566200256,
      "learning_rate": 1.2983333333333333e-05,
      "loss": 0.0012,
      "step": 111050
    },
    {
      "epoch": 5.9232,
      "grad_norm": 0.04182925820350647,
      "learning_rate": 1.2980000000000001e-05,
      "loss": 0.0011,
      "step": 111060
    },
    {
      "epoch": 5.923733333333333,
      "grad_norm": 0.06687616556882858,
      "learning_rate": 1.2976666666666667e-05,
      "loss": 0.0022,
      "step": 111070
    },
    {
      "epoch": 5.924266666666667,
      "grad_norm": 0.31687578558921814,
      "learning_rate": 1.2973333333333335e-05,
      "loss": 0.0014,
      "step": 111080
    },
    {
      "epoch": 5.9248,
      "grad_norm": 0.04751766845583916,
      "learning_rate": 1.2970000000000001e-05,
      "loss": 0.0017,
      "step": 111090
    },
    {
      "epoch": 5.925333333333334,
      "grad_norm": 0.5434191823005676,
      "learning_rate": 1.2966666666666669e-05,
      "loss": 0.0026,
      "step": 111100
    },
    {
      "epoch": 5.925866666666667,
      "grad_norm": 0.27999594807624817,
      "learning_rate": 1.2963333333333333e-05,
      "loss": 0.0023,
      "step": 111110
    },
    {
      "epoch": 5.9264,
      "grad_norm": 0.06467592716217041,
      "learning_rate": 1.296e-05,
      "loss": 0.0029,
      "step": 111120
    },
    {
      "epoch": 5.926933333333333,
      "grad_norm": 0.1735830008983612,
      "learning_rate": 1.2956666666666667e-05,
      "loss": 0.0021,
      "step": 111130
    },
    {
      "epoch": 5.927466666666667,
      "grad_norm": 0.1966163069009781,
      "learning_rate": 1.2953333333333334e-05,
      "loss": 0.002,
      "step": 111140
    },
    {
      "epoch": 5.928,
      "grad_norm": 0.04006986320018768,
      "learning_rate": 1.2950000000000001e-05,
      "loss": 0.002,
      "step": 111150
    },
    {
      "epoch": 5.928533333333333,
      "grad_norm": 0.05778105556964874,
      "learning_rate": 1.2946666666666668e-05,
      "loss": 0.0023,
      "step": 111160
    },
    {
      "epoch": 5.9290666666666665,
      "grad_norm": 0.5424926280975342,
      "learning_rate": 1.2943333333333335e-05,
      "loss": 0.0014,
      "step": 111170
    },
    {
      "epoch": 5.9296,
      "grad_norm": 0.11230811476707458,
      "learning_rate": 1.294e-05,
      "loss": 0.0019,
      "step": 111180
    },
    {
      "epoch": 5.930133333333333,
      "grad_norm": 0.332590788602829,
      "learning_rate": 1.2936666666666666e-05,
      "loss": 0.0014,
      "step": 111190
    },
    {
      "epoch": 5.930666666666666,
      "grad_norm": 0.11857227236032486,
      "learning_rate": 1.2933333333333334e-05,
      "loss": 0.0021,
      "step": 111200
    },
    {
      "epoch": 5.9312000000000005,
      "grad_norm": 0.06979446113109589,
      "learning_rate": 1.293e-05,
      "loss": 0.002,
      "step": 111210
    },
    {
      "epoch": 5.931733333333334,
      "grad_norm": 0.38272595405578613,
      "learning_rate": 1.2926666666666668e-05,
      "loss": 0.0014,
      "step": 111220
    },
    {
      "epoch": 5.932266666666667,
      "grad_norm": 0.3235826790332794,
      "learning_rate": 1.2923333333333334e-05,
      "loss": 0.0011,
      "step": 111230
    },
    {
      "epoch": 5.9328,
      "grad_norm": 0.047938600182533264,
      "learning_rate": 1.2920000000000002e-05,
      "loss": 0.0011,
      "step": 111240
    },
    {
      "epoch": 5.933333333333334,
      "grad_norm": 0.14474225044250488,
      "learning_rate": 1.2916666666666668e-05,
      "loss": 0.0017,
      "step": 111250
    },
    {
      "epoch": 5.933866666666667,
      "grad_norm": 0.4137330949306488,
      "learning_rate": 1.2913333333333332e-05,
      "loss": 0.0025,
      "step": 111260
    },
    {
      "epoch": 5.9344,
      "grad_norm": 0.6612282991409302,
      "learning_rate": 1.291e-05,
      "loss": 0.0015,
      "step": 111270
    },
    {
      "epoch": 5.934933333333333,
      "grad_norm": 0.4342457950115204,
      "learning_rate": 1.2906666666666666e-05,
      "loss": 0.0019,
      "step": 111280
    },
    {
      "epoch": 5.935466666666667,
      "grad_norm": 0.06037791073322296,
      "learning_rate": 1.2903333333333334e-05,
      "loss": 0.0015,
      "step": 111290
    },
    {
      "epoch": 5.936,
      "grad_norm": 0.12129299342632294,
      "learning_rate": 1.29e-05,
      "loss": 0.002,
      "step": 111300
    },
    {
      "epoch": 5.936533333333333,
      "grad_norm": 0.1483173966407776,
      "learning_rate": 1.2896666666666668e-05,
      "loss": 0.0015,
      "step": 111310
    },
    {
      "epoch": 5.9370666666666665,
      "grad_norm": 0.039819974452257156,
      "learning_rate": 1.2893333333333336e-05,
      "loss": 0.0015,
      "step": 111320
    },
    {
      "epoch": 5.9376,
      "grad_norm": 0.09799829870462418,
      "learning_rate": 1.2889999999999999e-05,
      "loss": 0.0018,
      "step": 111330
    },
    {
      "epoch": 5.938133333333333,
      "grad_norm": 0.7619961500167847,
      "learning_rate": 1.2886666666666666e-05,
      "loss": 0.0028,
      "step": 111340
    },
    {
      "epoch": 5.938666666666666,
      "grad_norm": 0.15113957226276398,
      "learning_rate": 1.2883333333333333e-05,
      "loss": 0.0017,
      "step": 111350
    },
    {
      "epoch": 5.9392,
      "grad_norm": 0.319132536649704,
      "learning_rate": 1.288e-05,
      "loss": 0.0021,
      "step": 111360
    },
    {
      "epoch": 5.939733333333333,
      "grad_norm": 0.11813583225011826,
      "learning_rate": 1.2876666666666668e-05,
      "loss": 0.0017,
      "step": 111370
    },
    {
      "epoch": 5.940266666666667,
      "grad_norm": 0.03506730496883392,
      "learning_rate": 1.2873333333333334e-05,
      "loss": 0.0016,
      "step": 111380
    },
    {
      "epoch": 5.9408,
      "grad_norm": 0.42997169494628906,
      "learning_rate": 1.2870000000000002e-05,
      "loss": 0.0016,
      "step": 111390
    },
    {
      "epoch": 5.941333333333334,
      "grad_norm": 0.29174429178237915,
      "learning_rate": 1.2866666666666668e-05,
      "loss": 0.0024,
      "step": 111400
    },
    {
      "epoch": 5.941866666666667,
      "grad_norm": 0.14554938673973083,
      "learning_rate": 1.2863333333333333e-05,
      "loss": 0.002,
      "step": 111410
    },
    {
      "epoch": 5.9424,
      "grad_norm": 0.09952332079410553,
      "learning_rate": 1.286e-05,
      "loss": 0.0014,
      "step": 111420
    },
    {
      "epoch": 5.942933333333333,
      "grad_norm": 0.04601258039474487,
      "learning_rate": 1.2856666666666667e-05,
      "loss": 0.0023,
      "step": 111430
    },
    {
      "epoch": 5.943466666666667,
      "grad_norm": 0.23153190314769745,
      "learning_rate": 1.2853333333333335e-05,
      "loss": 0.0013,
      "step": 111440
    },
    {
      "epoch": 5.944,
      "grad_norm": 0.5140849947929382,
      "learning_rate": 1.285e-05,
      "loss": 0.0021,
      "step": 111450
    },
    {
      "epoch": 5.944533333333333,
      "grad_norm": 0.09183692187070847,
      "learning_rate": 1.2846666666666668e-05,
      "loss": 0.0013,
      "step": 111460
    },
    {
      "epoch": 5.9450666666666665,
      "grad_norm": 0.08863364160060883,
      "learning_rate": 1.2843333333333335e-05,
      "loss": 0.0021,
      "step": 111470
    },
    {
      "epoch": 5.9456,
      "grad_norm": 0.09699037671089172,
      "learning_rate": 1.2839999999999999e-05,
      "loss": 0.0015,
      "step": 111480
    },
    {
      "epoch": 5.946133333333333,
      "grad_norm": 0.09404657781124115,
      "learning_rate": 1.2836666666666667e-05,
      "loss": 0.0017,
      "step": 111490
    },
    {
      "epoch": 5.946666666666666,
      "grad_norm": 0.1449652910232544,
      "learning_rate": 1.2833333333333333e-05,
      "loss": 0.0014,
      "step": 111500
    },
    {
      "epoch": 5.9472000000000005,
      "grad_norm": 0.051156800240278244,
      "learning_rate": 1.283e-05,
      "loss": 0.002,
      "step": 111510
    },
    {
      "epoch": 5.947733333333334,
      "grad_norm": 0.20260611176490784,
      "learning_rate": 1.2826666666666667e-05,
      "loss": 0.0026,
      "step": 111520
    },
    {
      "epoch": 5.948266666666667,
      "grad_norm": 0.11885294318199158,
      "learning_rate": 1.2823333333333335e-05,
      "loss": 0.0021,
      "step": 111530
    },
    {
      "epoch": 5.9488,
      "grad_norm": 0.31465500593185425,
      "learning_rate": 1.2820000000000001e-05,
      "loss": 0.0014,
      "step": 111540
    },
    {
      "epoch": 5.949333333333334,
      "grad_norm": 0.02199866995215416,
      "learning_rate": 1.2816666666666669e-05,
      "loss": 0.0013,
      "step": 111550
    },
    {
      "epoch": 5.949866666666667,
      "grad_norm": 0.519085168838501,
      "learning_rate": 1.2813333333333333e-05,
      "loss": 0.0017,
      "step": 111560
    },
    {
      "epoch": 5.9504,
      "grad_norm": 0.18966102600097656,
      "learning_rate": 1.281e-05,
      "loss": 0.0014,
      "step": 111570
    },
    {
      "epoch": 5.950933333333333,
      "grad_norm": 0.6049424409866333,
      "learning_rate": 1.2806666666666667e-05,
      "loss": 0.0015,
      "step": 111580
    },
    {
      "epoch": 5.951466666666667,
      "grad_norm": 0.10563450306653976,
      "learning_rate": 1.2803333333333333e-05,
      "loss": 0.0016,
      "step": 111590
    },
    {
      "epoch": 5.952,
      "grad_norm": 0.17697729170322418,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 0.0015,
      "step": 111600
    },
    {
      "epoch": 5.952533333333333,
      "grad_norm": 0.34842580556869507,
      "learning_rate": 1.2796666666666667e-05,
      "loss": 0.0016,
      "step": 111610
    },
    {
      "epoch": 5.9530666666666665,
      "grad_norm": 0.14415058493614197,
      "learning_rate": 1.2793333333333335e-05,
      "loss": 0.0022,
      "step": 111620
    },
    {
      "epoch": 5.9536,
      "grad_norm": 0.050669487565755844,
      "learning_rate": 1.2790000000000001e-05,
      "loss": 0.0023,
      "step": 111630
    },
    {
      "epoch": 5.954133333333333,
      "grad_norm": 0.23050230741500854,
      "learning_rate": 1.2786666666666666e-05,
      "loss": 0.0024,
      "step": 111640
    },
    {
      "epoch": 5.954666666666666,
      "grad_norm": 0.3459051251411438,
      "learning_rate": 1.2783333333333333e-05,
      "loss": 0.0032,
      "step": 111650
    },
    {
      "epoch": 5.9552,
      "grad_norm": 0.06699024140834808,
      "learning_rate": 1.278e-05,
      "loss": 0.002,
      "step": 111660
    },
    {
      "epoch": 5.955733333333333,
      "grad_norm": 0.11665760725736618,
      "learning_rate": 1.2776666666666667e-05,
      "loss": 0.0015,
      "step": 111670
    },
    {
      "epoch": 5.956266666666667,
      "grad_norm": 0.3127220869064331,
      "learning_rate": 1.2773333333333334e-05,
      "loss": 0.0021,
      "step": 111680
    },
    {
      "epoch": 5.9568,
      "grad_norm": 0.3324640393257141,
      "learning_rate": 1.2770000000000001e-05,
      "loss": 0.0023,
      "step": 111690
    },
    {
      "epoch": 5.957333333333334,
      "grad_norm": 0.4878811240196228,
      "learning_rate": 1.276666666666667e-05,
      "loss": 0.0012,
      "step": 111700
    },
    {
      "epoch": 5.957866666666667,
      "grad_norm": 0.3115760385990143,
      "learning_rate": 1.2763333333333332e-05,
      "loss": 0.0015,
      "step": 111710
    },
    {
      "epoch": 5.9584,
      "grad_norm": 0.2314663529396057,
      "learning_rate": 1.276e-05,
      "loss": 0.0018,
      "step": 111720
    },
    {
      "epoch": 5.958933333333333,
      "grad_norm": 0.380831778049469,
      "learning_rate": 1.2756666666666666e-05,
      "loss": 0.0015,
      "step": 111730
    },
    {
      "epoch": 5.959466666666667,
      "grad_norm": 0.342075377702713,
      "learning_rate": 1.2753333333333334e-05,
      "loss": 0.0018,
      "step": 111740
    },
    {
      "epoch": 5.96,
      "grad_norm": 0.4029901325702667,
      "learning_rate": 1.2750000000000002e-05,
      "loss": 0.0025,
      "step": 111750
    },
    {
      "epoch": 5.960533333333333,
      "grad_norm": 0.14430201053619385,
      "learning_rate": 1.2746666666666668e-05,
      "loss": 0.0018,
      "step": 111760
    },
    {
      "epoch": 5.9610666666666665,
      "grad_norm": 0.5689626932144165,
      "learning_rate": 1.2743333333333336e-05,
      "loss": 0.0017,
      "step": 111770
    },
    {
      "epoch": 5.9616,
      "grad_norm": 0.06552376598119736,
      "learning_rate": 1.2740000000000002e-05,
      "loss": 0.0019,
      "step": 111780
    },
    {
      "epoch": 5.962133333333333,
      "grad_norm": 0.07806150615215302,
      "learning_rate": 1.2736666666666666e-05,
      "loss": 0.0026,
      "step": 111790
    },
    {
      "epoch": 5.962666666666666,
      "grad_norm": 0.1720450520515442,
      "learning_rate": 1.2733333333333334e-05,
      "loss": 0.0014,
      "step": 111800
    },
    {
      "epoch": 5.9632,
      "grad_norm": 0.18035052716732025,
      "learning_rate": 1.273e-05,
      "loss": 0.0021,
      "step": 111810
    },
    {
      "epoch": 5.963733333333334,
      "grad_norm": 0.06265369802713394,
      "learning_rate": 1.2726666666666668e-05,
      "loss": 0.0017,
      "step": 111820
    },
    {
      "epoch": 5.964266666666667,
      "grad_norm": 0.041587453335523605,
      "learning_rate": 1.2723333333333334e-05,
      "loss": 0.0022,
      "step": 111830
    },
    {
      "epoch": 5.9648,
      "grad_norm": 0.06752340495586395,
      "learning_rate": 1.2720000000000002e-05,
      "loss": 0.0015,
      "step": 111840
    },
    {
      "epoch": 5.965333333333334,
      "grad_norm": 0.2416895478963852,
      "learning_rate": 1.2716666666666668e-05,
      "loss": 0.0023,
      "step": 111850
    },
    {
      "epoch": 5.965866666666667,
      "grad_norm": 0.03672187775373459,
      "learning_rate": 1.2713333333333332e-05,
      "loss": 0.0018,
      "step": 111860
    },
    {
      "epoch": 5.9664,
      "grad_norm": 0.1458553522825241,
      "learning_rate": 1.271e-05,
      "loss": 0.0026,
      "step": 111870
    },
    {
      "epoch": 5.966933333333333,
      "grad_norm": 0.37999796867370605,
      "learning_rate": 1.2706666666666666e-05,
      "loss": 0.0019,
      "step": 111880
    },
    {
      "epoch": 5.967466666666667,
      "grad_norm": 0.23343782126903534,
      "learning_rate": 1.2703333333333334e-05,
      "loss": 0.002,
      "step": 111890
    },
    {
      "epoch": 5.968,
      "grad_norm": 0.1710091084241867,
      "learning_rate": 1.27e-05,
      "loss": 0.0023,
      "step": 111900
    },
    {
      "epoch": 5.968533333333333,
      "grad_norm": 0.2361879050731659,
      "learning_rate": 1.2696666666666668e-05,
      "loss": 0.0016,
      "step": 111910
    },
    {
      "epoch": 5.9690666666666665,
      "grad_norm": 0.07140905410051346,
      "learning_rate": 1.2693333333333334e-05,
      "loss": 0.0021,
      "step": 111920
    },
    {
      "epoch": 5.9696,
      "grad_norm": 0.038312651216983795,
      "learning_rate": 1.2690000000000002e-05,
      "loss": 0.0016,
      "step": 111930
    },
    {
      "epoch": 5.970133333333333,
      "grad_norm": 0.2011868953704834,
      "learning_rate": 1.2686666666666667e-05,
      "loss": 0.0022,
      "step": 111940
    },
    {
      "epoch": 5.970666666666666,
      "grad_norm": 0.17876343429088593,
      "learning_rate": 1.2683333333333333e-05,
      "loss": 0.0016,
      "step": 111950
    },
    {
      "epoch": 5.9712,
      "grad_norm": 0.42189931869506836,
      "learning_rate": 1.268e-05,
      "loss": 0.0017,
      "step": 111960
    },
    {
      "epoch": 5.971733333333333,
      "grad_norm": 0.22661234438419342,
      "learning_rate": 1.2676666666666667e-05,
      "loss": 0.0013,
      "step": 111970
    },
    {
      "epoch": 5.972266666666666,
      "grad_norm": 0.17311297357082367,
      "learning_rate": 1.2673333333333335e-05,
      "loss": 0.0023,
      "step": 111980
    },
    {
      "epoch": 5.9728,
      "grad_norm": 0.12973733246326447,
      "learning_rate": 1.267e-05,
      "loss": 0.0018,
      "step": 111990
    },
    {
      "epoch": 5.973333333333334,
      "grad_norm": 0.31282663345336914,
      "learning_rate": 1.2666666666666668e-05,
      "loss": 0.0013,
      "step": 112000
    },
    {
      "epoch": 5.973866666666667,
      "grad_norm": 0.34539031982421875,
      "learning_rate": 1.2663333333333333e-05,
      "loss": 0.0017,
      "step": 112010
    },
    {
      "epoch": 5.9744,
      "grad_norm": 0.024643074721097946,
      "learning_rate": 1.2659999999999999e-05,
      "loss": 0.0021,
      "step": 112020
    },
    {
      "epoch": 5.974933333333333,
      "grad_norm": 0.12510092556476593,
      "learning_rate": 1.2656666666666667e-05,
      "loss": 0.0016,
      "step": 112030
    },
    {
      "epoch": 5.975466666666667,
      "grad_norm": 0.20048899948596954,
      "learning_rate": 1.2653333333333333e-05,
      "loss": 0.0022,
      "step": 112040
    },
    {
      "epoch": 5.976,
      "grad_norm": 0.14290127158164978,
      "learning_rate": 1.2650000000000001e-05,
      "loss": 0.0015,
      "step": 112050
    },
    {
      "epoch": 5.976533333333333,
      "grad_norm": 0.3123812675476074,
      "learning_rate": 1.2646666666666667e-05,
      "loss": 0.0014,
      "step": 112060
    },
    {
      "epoch": 5.9770666666666665,
      "grad_norm": 0.22252099215984344,
      "learning_rate": 1.2643333333333335e-05,
      "loss": 0.0015,
      "step": 112070
    },
    {
      "epoch": 5.9776,
      "grad_norm": 0.22566941380500793,
      "learning_rate": 1.2640000000000003e-05,
      "loss": 0.0025,
      "step": 112080
    },
    {
      "epoch": 5.978133333333333,
      "grad_norm": 0.5481630563735962,
      "learning_rate": 1.2636666666666665e-05,
      "loss": 0.0014,
      "step": 112090
    },
    {
      "epoch": 5.978666666666666,
      "grad_norm": 0.06484992802143097,
      "learning_rate": 1.2633333333333333e-05,
      "loss": 0.0015,
      "step": 112100
    },
    {
      "epoch": 5.9792,
      "grad_norm": 0.03979537636041641,
      "learning_rate": 1.263e-05,
      "loss": 0.0033,
      "step": 112110
    },
    {
      "epoch": 5.979733333333334,
      "grad_norm": 0.26812994480133057,
      "learning_rate": 1.2626666666666667e-05,
      "loss": 0.0016,
      "step": 112120
    },
    {
      "epoch": 5.980266666666667,
      "grad_norm": 0.36161163449287415,
      "learning_rate": 1.2623333333333335e-05,
      "loss": 0.0022,
      "step": 112130
    },
    {
      "epoch": 5.9808,
      "grad_norm": 0.40362200140953064,
      "learning_rate": 1.2620000000000001e-05,
      "loss": 0.0022,
      "step": 112140
    },
    {
      "epoch": 5.981333333333334,
      "grad_norm": 0.15638574957847595,
      "learning_rate": 1.2616666666666669e-05,
      "loss": 0.0029,
      "step": 112150
    },
    {
      "epoch": 5.981866666666667,
      "grad_norm": 0.15586307644844055,
      "learning_rate": 1.2613333333333332e-05,
      "loss": 0.0014,
      "step": 112160
    },
    {
      "epoch": 5.9824,
      "grad_norm": 0.09580638259649277,
      "learning_rate": 1.261e-05,
      "loss": 0.0015,
      "step": 112170
    },
    {
      "epoch": 5.982933333333333,
      "grad_norm": 0.2608036696910858,
      "learning_rate": 1.2606666666666667e-05,
      "loss": 0.0013,
      "step": 112180
    },
    {
      "epoch": 5.983466666666667,
      "grad_norm": 0.17233775556087494,
      "learning_rate": 1.2603333333333334e-05,
      "loss": 0.0027,
      "step": 112190
    },
    {
      "epoch": 5.984,
      "grad_norm": 0.23199646174907684,
      "learning_rate": 1.2600000000000001e-05,
      "loss": 0.002,
      "step": 112200
    },
    {
      "epoch": 5.984533333333333,
      "grad_norm": 0.1720803678035736,
      "learning_rate": 1.2596666666666667e-05,
      "loss": 0.0023,
      "step": 112210
    },
    {
      "epoch": 5.9850666666666665,
      "grad_norm": 0.31515470147132874,
      "learning_rate": 1.2593333333333335e-05,
      "loss": 0.0019,
      "step": 112220
    },
    {
      "epoch": 5.9856,
      "grad_norm": 0.3163990080356598,
      "learning_rate": 1.2590000000000001e-05,
      "loss": 0.0017,
      "step": 112230
    },
    {
      "epoch": 5.986133333333333,
      "grad_norm": 0.12280769646167755,
      "learning_rate": 1.2586666666666666e-05,
      "loss": 0.0019,
      "step": 112240
    },
    {
      "epoch": 5.986666666666666,
      "grad_norm": 0.08742688596248627,
      "learning_rate": 1.2583333333333334e-05,
      "loss": 0.0019,
      "step": 112250
    },
    {
      "epoch": 5.9872,
      "grad_norm": 0.23589572310447693,
      "learning_rate": 1.258e-05,
      "loss": 0.0017,
      "step": 112260
    },
    {
      "epoch": 5.987733333333333,
      "grad_norm": 0.08618971705436707,
      "learning_rate": 1.2576666666666668e-05,
      "loss": 0.0018,
      "step": 112270
    },
    {
      "epoch": 5.988266666666666,
      "grad_norm": 0.04929513856768608,
      "learning_rate": 1.2573333333333334e-05,
      "loss": 0.0014,
      "step": 112280
    },
    {
      "epoch": 5.9888,
      "grad_norm": 0.23478367924690247,
      "learning_rate": 1.2570000000000002e-05,
      "loss": 0.0016,
      "step": 112290
    },
    {
      "epoch": 5.989333333333334,
      "grad_norm": 0.32027164101600647,
      "learning_rate": 1.2566666666666668e-05,
      "loss": 0.0025,
      "step": 112300
    },
    {
      "epoch": 5.989866666666667,
      "grad_norm": 0.2907676696777344,
      "learning_rate": 1.2563333333333336e-05,
      "loss": 0.0021,
      "step": 112310
    },
    {
      "epoch": 5.9904,
      "grad_norm": 0.39746659994125366,
      "learning_rate": 1.256e-05,
      "loss": 0.0015,
      "step": 112320
    },
    {
      "epoch": 5.990933333333333,
      "grad_norm": 0.26037800312042236,
      "learning_rate": 1.2556666666666666e-05,
      "loss": 0.002,
      "step": 112330
    },
    {
      "epoch": 5.991466666666667,
      "grad_norm": 0.2299010306596756,
      "learning_rate": 1.2553333333333334e-05,
      "loss": 0.0016,
      "step": 112340
    },
    {
      "epoch": 5.992,
      "grad_norm": 0.1432470828294754,
      "learning_rate": 1.255e-05,
      "loss": 0.0016,
      "step": 112350
    },
    {
      "epoch": 5.992533333333333,
      "grad_norm": 0.17532815039157867,
      "learning_rate": 1.2546666666666668e-05,
      "loss": 0.0032,
      "step": 112360
    },
    {
      "epoch": 5.9930666666666665,
      "grad_norm": 0.14878414571285248,
      "learning_rate": 1.2543333333333334e-05,
      "loss": 0.0017,
      "step": 112370
    },
    {
      "epoch": 5.9936,
      "grad_norm": 0.07282595336437225,
      "learning_rate": 1.2540000000000002e-05,
      "loss": 0.0018,
      "step": 112380
    },
    {
      "epoch": 5.994133333333333,
      "grad_norm": 0.11382465809583664,
      "learning_rate": 1.2536666666666666e-05,
      "loss": 0.0013,
      "step": 112390
    },
    {
      "epoch": 5.994666666666666,
      "grad_norm": 0.4278526306152344,
      "learning_rate": 1.2533333333333332e-05,
      "loss": 0.0018,
      "step": 112400
    },
    {
      "epoch": 5.9952,
      "grad_norm": 0.218572735786438,
      "learning_rate": 1.253e-05,
      "loss": 0.0017,
      "step": 112410
    },
    {
      "epoch": 5.995733333333334,
      "grad_norm": 0.3156968951225281,
      "learning_rate": 1.2526666666666666e-05,
      "loss": 0.0019,
      "step": 112420
    },
    {
      "epoch": 5.996266666666667,
      "grad_norm": 0.2911494970321655,
      "learning_rate": 1.2523333333333334e-05,
      "loss": 0.0018,
      "step": 112430
    },
    {
      "epoch": 5.9968,
      "grad_norm": 0.28961166739463806,
      "learning_rate": 1.252e-05,
      "loss": 0.0016,
      "step": 112440
    },
    {
      "epoch": 5.997333333333334,
      "grad_norm": 0.3767559230327606,
      "learning_rate": 1.2516666666666668e-05,
      "loss": 0.0015,
      "step": 112450
    },
    {
      "epoch": 5.997866666666667,
      "grad_norm": 0.5952388048171997,
      "learning_rate": 1.2513333333333336e-05,
      "loss": 0.0015,
      "step": 112460
    },
    {
      "epoch": 5.9984,
      "grad_norm": 0.21376648545265198,
      "learning_rate": 1.2509999999999999e-05,
      "loss": 0.0024,
      "step": 112470
    },
    {
      "epoch": 5.9989333333333335,
      "grad_norm": 0.34821322560310364,
      "learning_rate": 1.2506666666666667e-05,
      "loss": 0.0019,
      "step": 112480
    },
    {
      "epoch": 5.999466666666667,
      "grad_norm": 0.3125050663948059,
      "learning_rate": 1.2503333333333333e-05,
      "loss": 0.0013,
      "step": 112490
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.18478669226169586,
      "learning_rate": 1.25e-05,
      "loss": 0.0017,
      "step": 112500
    },
    {
      "epoch": 6.0,
      "eval_loss": 0.0019178555812686682,
      "eval_runtime": 170.8258,
      "eval_samples_per_second": 1463.479,
      "eval_steps_per_second": 36.587,
      "step": 112500
    },
    {
      "epoch": 6.000533333333333,
      "grad_norm": 0.1767408847808838,
      "learning_rate": 1.2496666666666668e-05,
      "loss": 0.0022,
      "step": 112510
    },
    {
      "epoch": 6.0010666666666665,
      "grad_norm": 0.08807647973299026,
      "learning_rate": 1.2493333333333333e-05,
      "loss": 0.0019,
      "step": 112520
    },
    {
      "epoch": 6.0016,
      "grad_norm": 0.12183082848787308,
      "learning_rate": 1.249e-05,
      "loss": 0.0021,
      "step": 112530
    },
    {
      "epoch": 6.002133333333333,
      "grad_norm": 0.17944379150867462,
      "learning_rate": 1.2486666666666667e-05,
      "loss": 0.0019,
      "step": 112540
    },
    {
      "epoch": 6.002666666666666,
      "grad_norm": 0.06674836575984955,
      "learning_rate": 1.2483333333333335e-05,
      "loss": 0.002,
      "step": 112550
    },
    {
      "epoch": 6.0032,
      "grad_norm": 0.20720092952251434,
      "learning_rate": 1.248e-05,
      "loss": 0.0018,
      "step": 112560
    },
    {
      "epoch": 6.003733333333333,
      "grad_norm": 0.3672357499599457,
      "learning_rate": 1.2476666666666667e-05,
      "loss": 0.0032,
      "step": 112570
    },
    {
      "epoch": 6.004266666666667,
      "grad_norm": 0.0566425547003746,
      "learning_rate": 1.2473333333333335e-05,
      "loss": 0.0015,
      "step": 112580
    },
    {
      "epoch": 6.0048,
      "grad_norm": 0.2115178406238556,
      "learning_rate": 1.2470000000000001e-05,
      "loss": 0.0025,
      "step": 112590
    },
    {
      "epoch": 6.005333333333334,
      "grad_norm": 0.1181795746088028,
      "learning_rate": 1.2466666666666667e-05,
      "loss": 0.0023,
      "step": 112600
    },
    {
      "epoch": 6.005866666666667,
      "grad_norm": 0.2606728672981262,
      "learning_rate": 1.2463333333333333e-05,
      "loss": 0.0015,
      "step": 112610
    },
    {
      "epoch": 6.0064,
      "grad_norm": 0.4343564510345459,
      "learning_rate": 1.2460000000000001e-05,
      "loss": 0.0019,
      "step": 112620
    },
    {
      "epoch": 6.0069333333333335,
      "grad_norm": 0.27044087648391724,
      "learning_rate": 1.2456666666666667e-05,
      "loss": 0.0021,
      "step": 112630
    },
    {
      "epoch": 6.007466666666667,
      "grad_norm": 0.11630892008543015,
      "learning_rate": 1.2453333333333333e-05,
      "loss": 0.0015,
      "step": 112640
    },
    {
      "epoch": 6.008,
      "grad_norm": 0.059272244572639465,
      "learning_rate": 1.2450000000000001e-05,
      "loss": 0.0022,
      "step": 112650
    },
    {
      "epoch": 6.008533333333333,
      "grad_norm": 0.12290965765714645,
      "learning_rate": 1.2446666666666667e-05,
      "loss": 0.0022,
      "step": 112660
    },
    {
      "epoch": 6.009066666666667,
      "grad_norm": 0.1998719871044159,
      "learning_rate": 1.2443333333333335e-05,
      "loss": 0.0027,
      "step": 112670
    },
    {
      "epoch": 6.0096,
      "grad_norm": 0.12514889240264893,
      "learning_rate": 1.244e-05,
      "loss": 0.0015,
      "step": 112680
    },
    {
      "epoch": 6.010133333333333,
      "grad_norm": 0.08254338055849075,
      "learning_rate": 1.2436666666666667e-05,
      "loss": 0.0012,
      "step": 112690
    },
    {
      "epoch": 6.010666666666666,
      "grad_norm": 0.04784173518419266,
      "learning_rate": 1.2433333333333335e-05,
      "loss": 0.0022,
      "step": 112700
    },
    {
      "epoch": 6.0112,
      "grad_norm": 0.034369826316833496,
      "learning_rate": 1.243e-05,
      "loss": 0.0021,
      "step": 112710
    },
    {
      "epoch": 6.011733333333333,
      "grad_norm": 0.29102057218551636,
      "learning_rate": 1.2426666666666667e-05,
      "loss": 0.0019,
      "step": 112720
    },
    {
      "epoch": 6.012266666666667,
      "grad_norm": 0.06783989071846008,
      "learning_rate": 1.2423333333333334e-05,
      "loss": 0.0021,
      "step": 112730
    },
    {
      "epoch": 6.0128,
      "grad_norm": 0.1449768990278244,
      "learning_rate": 1.2420000000000001e-05,
      "loss": 0.0023,
      "step": 112740
    },
    {
      "epoch": 6.013333333333334,
      "grad_norm": 0.5503545999526978,
      "learning_rate": 1.2416666666666667e-05,
      "loss": 0.0019,
      "step": 112750
    },
    {
      "epoch": 6.013866666666667,
      "grad_norm": 0.046991366893053055,
      "learning_rate": 1.2413333333333334e-05,
      "loss": 0.0022,
      "step": 112760
    },
    {
      "epoch": 6.0144,
      "grad_norm": 0.12820951640605927,
      "learning_rate": 1.2410000000000001e-05,
      "loss": 0.0028,
      "step": 112770
    },
    {
      "epoch": 6.0149333333333335,
      "grad_norm": 0.2946048080921173,
      "learning_rate": 1.2406666666666668e-05,
      "loss": 0.0021,
      "step": 112780
    },
    {
      "epoch": 6.015466666666667,
      "grad_norm": 0.09470614045858383,
      "learning_rate": 1.2403333333333334e-05,
      "loss": 0.0016,
      "step": 112790
    },
    {
      "epoch": 6.016,
      "grad_norm": 0.20773129165172577,
      "learning_rate": 1.24e-05,
      "loss": 0.0024,
      "step": 112800
    },
    {
      "epoch": 6.016533333333333,
      "grad_norm": 0.5165945887565613,
      "learning_rate": 1.2396666666666668e-05,
      "loss": 0.0018,
      "step": 112810
    },
    {
      "epoch": 6.017066666666667,
      "grad_norm": 0.14345203340053558,
      "learning_rate": 1.2393333333333334e-05,
      "loss": 0.0022,
      "step": 112820
    },
    {
      "epoch": 6.0176,
      "grad_norm": 0.18167699873447418,
      "learning_rate": 1.239e-05,
      "loss": 0.0026,
      "step": 112830
    },
    {
      "epoch": 6.018133333333333,
      "grad_norm": 0.14069724082946777,
      "learning_rate": 1.2386666666666668e-05,
      "loss": 0.0013,
      "step": 112840
    },
    {
      "epoch": 6.018666666666666,
      "grad_norm": 0.28171753883361816,
      "learning_rate": 1.2383333333333334e-05,
      "loss": 0.0017,
      "step": 112850
    },
    {
      "epoch": 6.0192,
      "grad_norm": 0.0671122744679451,
      "learning_rate": 1.238e-05,
      "loss": 0.0024,
      "step": 112860
    },
    {
      "epoch": 6.019733333333333,
      "grad_norm": 0.06362049281597137,
      "learning_rate": 1.2376666666666666e-05,
      "loss": 0.0017,
      "step": 112870
    },
    {
      "epoch": 6.020266666666667,
      "grad_norm": 0.37353217601776123,
      "learning_rate": 1.2373333333333334e-05,
      "loss": 0.0029,
      "step": 112880
    },
    {
      "epoch": 6.0208,
      "grad_norm": 0.522864580154419,
      "learning_rate": 1.2370000000000002e-05,
      "loss": 0.0019,
      "step": 112890
    },
    {
      "epoch": 6.021333333333334,
      "grad_norm": 0.24339234828948975,
      "learning_rate": 1.2366666666666666e-05,
      "loss": 0.0015,
      "step": 112900
    },
    {
      "epoch": 6.021866666666667,
      "grad_norm": 0.019197147339582443,
      "learning_rate": 1.2363333333333334e-05,
      "loss": 0.0022,
      "step": 112910
    },
    {
      "epoch": 6.0224,
      "grad_norm": 0.2877742052078247,
      "learning_rate": 1.236e-05,
      "loss": 0.0016,
      "step": 112920
    },
    {
      "epoch": 6.0229333333333335,
      "grad_norm": 0.17133350670337677,
      "learning_rate": 1.2356666666666668e-05,
      "loss": 0.0014,
      "step": 112930
    },
    {
      "epoch": 6.023466666666667,
      "grad_norm": 0.2622721791267395,
      "learning_rate": 1.2353333333333334e-05,
      "loss": 0.002,
      "step": 112940
    },
    {
      "epoch": 6.024,
      "grad_norm": 0.18118663132190704,
      "learning_rate": 1.235e-05,
      "loss": 0.0014,
      "step": 112950
    },
    {
      "epoch": 6.024533333333333,
      "grad_norm": 0.6758849620819092,
      "learning_rate": 1.2346666666666668e-05,
      "loss": 0.0016,
      "step": 112960
    },
    {
      "epoch": 6.025066666666667,
      "grad_norm": 0.40531429648399353,
      "learning_rate": 1.2343333333333334e-05,
      "loss": 0.0019,
      "step": 112970
    },
    {
      "epoch": 6.0256,
      "grad_norm": 0.19498717784881592,
      "learning_rate": 1.234e-05,
      "loss": 0.0019,
      "step": 112980
    },
    {
      "epoch": 6.026133333333333,
      "grad_norm": 0.11637445539236069,
      "learning_rate": 1.2336666666666667e-05,
      "loss": 0.0027,
      "step": 112990
    },
    {
      "epoch": 6.026666666666666,
      "grad_norm": 0.37419942021369934,
      "learning_rate": 1.2333333333333334e-05,
      "loss": 0.0022,
      "step": 113000
    },
    {
      "epoch": 6.0272,
      "grad_norm": 0.23549823462963104,
      "learning_rate": 1.233e-05,
      "loss": 0.002,
      "step": 113010
    },
    {
      "epoch": 6.027733333333333,
      "grad_norm": 0.2688143253326416,
      "learning_rate": 1.2326666666666667e-05,
      "loss": 0.0015,
      "step": 113020
    },
    {
      "epoch": 6.028266666666667,
      "grad_norm": 0.1220121905207634,
      "learning_rate": 1.2323333333333334e-05,
      "loss": 0.0018,
      "step": 113030
    },
    {
      "epoch": 6.0288,
      "grad_norm": 0.346191942691803,
      "learning_rate": 1.232e-05,
      "loss": 0.0021,
      "step": 113040
    },
    {
      "epoch": 6.029333333333334,
      "grad_norm": 0.42293837666511536,
      "learning_rate": 1.2316666666666667e-05,
      "loss": 0.0015,
      "step": 113050
    },
    {
      "epoch": 6.029866666666667,
      "grad_norm": 0.2747581899166107,
      "learning_rate": 1.2313333333333333e-05,
      "loss": 0.0021,
      "step": 113060
    },
    {
      "epoch": 6.0304,
      "grad_norm": 0.152994766831398,
      "learning_rate": 1.231e-05,
      "loss": 0.0017,
      "step": 113070
    },
    {
      "epoch": 6.0309333333333335,
      "grad_norm": 0.0901346504688263,
      "learning_rate": 1.2306666666666669e-05,
      "loss": 0.0014,
      "step": 113080
    },
    {
      "epoch": 6.031466666666667,
      "grad_norm": 0.25565105676651,
      "learning_rate": 1.2303333333333333e-05,
      "loss": 0.002,
      "step": 113090
    },
    {
      "epoch": 6.032,
      "grad_norm": 0.38554298877716064,
      "learning_rate": 1.23e-05,
      "loss": 0.0022,
      "step": 113100
    },
    {
      "epoch": 6.032533333333333,
      "grad_norm": 0.1408625990152359,
      "learning_rate": 1.2296666666666667e-05,
      "loss": 0.0017,
      "step": 113110
    },
    {
      "epoch": 6.033066666666667,
      "grad_norm": 0.04221852868795395,
      "learning_rate": 1.2293333333333335e-05,
      "loss": 0.0015,
      "step": 113120
    },
    {
      "epoch": 6.0336,
      "grad_norm": 0.34582868218421936,
      "learning_rate": 1.2290000000000001e-05,
      "loss": 0.0022,
      "step": 113130
    },
    {
      "epoch": 6.034133333333333,
      "grad_norm": 0.14418552815914154,
      "learning_rate": 1.2286666666666667e-05,
      "loss": 0.0012,
      "step": 113140
    },
    {
      "epoch": 6.034666666666666,
      "grad_norm": 0.03846538066864014,
      "learning_rate": 1.2283333333333335e-05,
      "loss": 0.0014,
      "step": 113150
    },
    {
      "epoch": 6.0352,
      "grad_norm": 0.31568071246147156,
      "learning_rate": 1.2280000000000001e-05,
      "loss": 0.0015,
      "step": 113160
    },
    {
      "epoch": 6.035733333333333,
      "grad_norm": 0.3939181864261627,
      "learning_rate": 1.2276666666666667e-05,
      "loss": 0.0014,
      "step": 113170
    },
    {
      "epoch": 6.036266666666666,
      "grad_norm": 0.13633938133716583,
      "learning_rate": 1.2273333333333333e-05,
      "loss": 0.002,
      "step": 113180
    },
    {
      "epoch": 6.0368,
      "grad_norm": 0.06304576247930527,
      "learning_rate": 1.2270000000000001e-05,
      "loss": 0.0019,
      "step": 113190
    },
    {
      "epoch": 6.037333333333334,
      "grad_norm": 0.5024134516716003,
      "learning_rate": 1.2266666666666667e-05,
      "loss": 0.0026,
      "step": 113200
    },
    {
      "epoch": 6.037866666666667,
      "grad_norm": 0.430719256401062,
      "learning_rate": 1.2263333333333333e-05,
      "loss": 0.0022,
      "step": 113210
    },
    {
      "epoch": 6.0384,
      "grad_norm": 0.09712937474250793,
      "learning_rate": 1.2260000000000001e-05,
      "loss": 0.0023,
      "step": 113220
    },
    {
      "epoch": 6.0389333333333335,
      "grad_norm": 0.12855805456638336,
      "learning_rate": 1.2256666666666667e-05,
      "loss": 0.0021,
      "step": 113230
    },
    {
      "epoch": 6.039466666666667,
      "grad_norm": 0.0647812932729721,
      "learning_rate": 1.2253333333333333e-05,
      "loss": 0.0023,
      "step": 113240
    },
    {
      "epoch": 6.04,
      "grad_norm": 0.0727536603808403,
      "learning_rate": 1.225e-05,
      "loss": 0.0017,
      "step": 113250
    },
    {
      "epoch": 6.040533333333333,
      "grad_norm": 0.03580647334456444,
      "learning_rate": 1.2246666666666667e-05,
      "loss": 0.0015,
      "step": 113260
    },
    {
      "epoch": 6.041066666666667,
      "grad_norm": 0.37638059258461,
      "learning_rate": 1.2243333333333335e-05,
      "loss": 0.0026,
      "step": 113270
    },
    {
      "epoch": 6.0416,
      "grad_norm": 0.22206035256385803,
      "learning_rate": 1.224e-05,
      "loss": 0.0021,
      "step": 113280
    },
    {
      "epoch": 6.042133333333333,
      "grad_norm": 0.2567715644836426,
      "learning_rate": 1.2236666666666668e-05,
      "loss": 0.0033,
      "step": 113290
    },
    {
      "epoch": 6.042666666666666,
      "grad_norm": 0.10309770703315735,
      "learning_rate": 1.2233333333333334e-05,
      "loss": 0.0015,
      "step": 113300
    },
    {
      "epoch": 6.0432,
      "grad_norm": 0.2674969732761383,
      "learning_rate": 1.2230000000000001e-05,
      "loss": 0.0013,
      "step": 113310
    },
    {
      "epoch": 6.043733333333333,
      "grad_norm": 0.11017294228076935,
      "learning_rate": 1.2226666666666668e-05,
      "loss": 0.0019,
      "step": 113320
    },
    {
      "epoch": 6.044266666666666,
      "grad_norm": 0.2947582006454468,
      "learning_rate": 1.2223333333333334e-05,
      "loss": 0.0021,
      "step": 113330
    },
    {
      "epoch": 6.0448,
      "grad_norm": 0.45392554998397827,
      "learning_rate": 1.2220000000000002e-05,
      "loss": 0.0022,
      "step": 113340
    },
    {
      "epoch": 6.045333333333334,
      "grad_norm": 0.4425222873687744,
      "learning_rate": 1.2216666666666668e-05,
      "loss": 0.0027,
      "step": 113350
    },
    {
      "epoch": 6.045866666666667,
      "grad_norm": 0.040324147790670395,
      "learning_rate": 1.2213333333333334e-05,
      "loss": 0.0014,
      "step": 113360
    },
    {
      "epoch": 6.0464,
      "grad_norm": 0.2317950278520584,
      "learning_rate": 1.221e-05,
      "loss": 0.0014,
      "step": 113370
    },
    {
      "epoch": 6.0469333333333335,
      "grad_norm": 0.22397363185882568,
      "learning_rate": 1.2206666666666668e-05,
      "loss": 0.0018,
      "step": 113380
    },
    {
      "epoch": 6.047466666666667,
      "grad_norm": 0.0898633524775505,
      "learning_rate": 1.2203333333333334e-05,
      "loss": 0.0019,
      "step": 113390
    },
    {
      "epoch": 6.048,
      "grad_norm": 0.13335490226745605,
      "learning_rate": 1.22e-05,
      "loss": 0.0027,
      "step": 113400
    },
    {
      "epoch": 6.048533333333333,
      "grad_norm": 0.26800552010536194,
      "learning_rate": 1.2196666666666668e-05,
      "loss": 0.002,
      "step": 113410
    },
    {
      "epoch": 6.049066666666667,
      "grad_norm": 0.2630801796913147,
      "learning_rate": 1.2193333333333334e-05,
      "loss": 0.0018,
      "step": 113420
    },
    {
      "epoch": 6.0496,
      "grad_norm": 0.06519898027181625,
      "learning_rate": 1.219e-05,
      "loss": 0.0023,
      "step": 113430
    },
    {
      "epoch": 6.050133333333333,
      "grad_norm": 0.08655284345149994,
      "learning_rate": 1.2186666666666666e-05,
      "loss": 0.0019,
      "step": 113440
    },
    {
      "epoch": 6.050666666666666,
      "grad_norm": 0.1372482031583786,
      "learning_rate": 1.2183333333333334e-05,
      "loss": 0.002,
      "step": 113450
    },
    {
      "epoch": 6.0512,
      "grad_norm": 0.06628212332725525,
      "learning_rate": 1.2180000000000002e-05,
      "loss": 0.0019,
      "step": 113460
    },
    {
      "epoch": 6.051733333333333,
      "grad_norm": 0.40146541595458984,
      "learning_rate": 1.2176666666666666e-05,
      "loss": 0.0013,
      "step": 113470
    },
    {
      "epoch": 6.052266666666666,
      "grad_norm": 0.1506498008966446,
      "learning_rate": 1.2173333333333334e-05,
      "loss": 0.0021,
      "step": 113480
    },
    {
      "epoch": 6.0528,
      "grad_norm": 0.20286720991134644,
      "learning_rate": 1.217e-05,
      "loss": 0.0014,
      "step": 113490
    },
    {
      "epoch": 6.053333333333334,
      "grad_norm": 0.09789817780256271,
      "learning_rate": 1.2166666666666668e-05,
      "loss": 0.0016,
      "step": 113500
    },
    {
      "epoch": 6.053866666666667,
      "grad_norm": 0.22960467636585236,
      "learning_rate": 1.2163333333333334e-05,
      "loss": 0.0026,
      "step": 113510
    },
    {
      "epoch": 6.0544,
      "grad_norm": 0.3469637334346771,
      "learning_rate": 1.216e-05,
      "loss": 0.002,
      "step": 113520
    },
    {
      "epoch": 6.0549333333333335,
      "grad_norm": 0.05100369453430176,
      "learning_rate": 1.2156666666666668e-05,
      "loss": 0.0025,
      "step": 113530
    },
    {
      "epoch": 6.055466666666667,
      "grad_norm": 0.17672987282276154,
      "learning_rate": 1.2153333333333333e-05,
      "loss": 0.002,
      "step": 113540
    },
    {
      "epoch": 6.056,
      "grad_norm": 0.5432087779045105,
      "learning_rate": 1.215e-05,
      "loss": 0.0015,
      "step": 113550
    },
    {
      "epoch": 6.056533333333333,
      "grad_norm": 0.061754148453474045,
      "learning_rate": 1.2146666666666667e-05,
      "loss": 0.0021,
      "step": 113560
    },
    {
      "epoch": 6.057066666666667,
      "grad_norm": 0.2574842870235443,
      "learning_rate": 1.2143333333333335e-05,
      "loss": 0.0023,
      "step": 113570
    },
    {
      "epoch": 6.0576,
      "grad_norm": 0.07760176807641983,
      "learning_rate": 1.214e-05,
      "loss": 0.0014,
      "step": 113580
    },
    {
      "epoch": 6.058133333333333,
      "grad_norm": 0.12378980219364166,
      "learning_rate": 1.2136666666666667e-05,
      "loss": 0.0018,
      "step": 113590
    },
    {
      "epoch": 6.058666666666666,
      "grad_norm": 0.09751386195421219,
      "learning_rate": 1.2133333333333335e-05,
      "loss": 0.0019,
      "step": 113600
    },
    {
      "epoch": 6.0592,
      "grad_norm": 0.12975963950157166,
      "learning_rate": 1.213e-05,
      "loss": 0.0016,
      "step": 113610
    },
    {
      "epoch": 6.059733333333333,
      "grad_norm": 0.1550142914056778,
      "learning_rate": 1.2126666666666667e-05,
      "loss": 0.0014,
      "step": 113620
    },
    {
      "epoch": 6.060266666666666,
      "grad_norm": 0.06100093200802803,
      "learning_rate": 1.2123333333333333e-05,
      "loss": 0.0022,
      "step": 113630
    },
    {
      "epoch": 6.0608,
      "grad_norm": 0.1800955981016159,
      "learning_rate": 1.2120000000000001e-05,
      "loss": 0.0025,
      "step": 113640
    },
    {
      "epoch": 6.061333333333334,
      "grad_norm": 0.06228667125105858,
      "learning_rate": 1.2116666666666669e-05,
      "loss": 0.0026,
      "step": 113650
    },
    {
      "epoch": 6.061866666666667,
      "grad_norm": 0.12139604985713959,
      "learning_rate": 1.2113333333333333e-05,
      "loss": 0.0013,
      "step": 113660
    },
    {
      "epoch": 6.0624,
      "grad_norm": 0.4291960895061493,
      "learning_rate": 1.2110000000000001e-05,
      "loss": 0.0019,
      "step": 113670
    },
    {
      "epoch": 6.0629333333333335,
      "grad_norm": 0.2864826023578644,
      "learning_rate": 1.2106666666666667e-05,
      "loss": 0.0016,
      "step": 113680
    },
    {
      "epoch": 6.063466666666667,
      "grad_norm": 0.05176840350031853,
      "learning_rate": 1.2103333333333335e-05,
      "loss": 0.0015,
      "step": 113690
    },
    {
      "epoch": 6.064,
      "grad_norm": 0.285942405462265,
      "learning_rate": 1.2100000000000001e-05,
      "loss": 0.0024,
      "step": 113700
    },
    {
      "epoch": 6.064533333333333,
      "grad_norm": 0.023875944316387177,
      "learning_rate": 1.2096666666666667e-05,
      "loss": 0.0019,
      "step": 113710
    },
    {
      "epoch": 6.065066666666667,
      "grad_norm": 0.1713726967573166,
      "learning_rate": 1.2093333333333335e-05,
      "loss": 0.0023,
      "step": 113720
    },
    {
      "epoch": 6.0656,
      "grad_norm": 0.3141128718852997,
      "learning_rate": 1.209e-05,
      "loss": 0.0022,
      "step": 113730
    },
    {
      "epoch": 6.066133333333333,
      "grad_norm": 0.09239368885755539,
      "learning_rate": 1.2086666666666667e-05,
      "loss": 0.002,
      "step": 113740
    },
    {
      "epoch": 6.066666666666666,
      "grad_norm": 0.05886108800768852,
      "learning_rate": 1.2083333333333333e-05,
      "loss": 0.0017,
      "step": 113750
    },
    {
      "epoch": 6.0672,
      "grad_norm": 0.14387287199497223,
      "learning_rate": 1.2080000000000001e-05,
      "loss": 0.0012,
      "step": 113760
    },
    {
      "epoch": 6.067733333333333,
      "grad_norm": 0.06689994782209396,
      "learning_rate": 1.2076666666666667e-05,
      "loss": 0.0012,
      "step": 113770
    },
    {
      "epoch": 6.068266666666666,
      "grad_norm": 0.2074361890554428,
      "learning_rate": 1.2073333333333333e-05,
      "loss": 0.0024,
      "step": 113780
    },
    {
      "epoch": 6.0688,
      "grad_norm": 0.26691964268684387,
      "learning_rate": 1.2070000000000001e-05,
      "loss": 0.0013,
      "step": 113790
    },
    {
      "epoch": 6.069333333333334,
      "grad_norm": 0.22180332243442535,
      "learning_rate": 1.2066666666666667e-05,
      "loss": 0.0019,
      "step": 113800
    },
    {
      "epoch": 6.069866666666667,
      "grad_norm": 0.1933744102716446,
      "learning_rate": 1.2063333333333334e-05,
      "loss": 0.0029,
      "step": 113810
    },
    {
      "epoch": 6.0704,
      "grad_norm": 0.23804299533367157,
      "learning_rate": 1.206e-05,
      "loss": 0.0015,
      "step": 113820
    },
    {
      "epoch": 6.0709333333333335,
      "grad_norm": 0.0616653710603714,
      "learning_rate": 1.2056666666666668e-05,
      "loss": 0.002,
      "step": 113830
    },
    {
      "epoch": 6.071466666666667,
      "grad_norm": 0.10462166368961334,
      "learning_rate": 1.2053333333333334e-05,
      "loss": 0.003,
      "step": 113840
    },
    {
      "epoch": 6.072,
      "grad_norm": 0.12303195893764496,
      "learning_rate": 1.205e-05,
      "loss": 0.0017,
      "step": 113850
    },
    {
      "epoch": 6.072533333333333,
      "grad_norm": 0.3394089341163635,
      "learning_rate": 1.2046666666666668e-05,
      "loss": 0.0023,
      "step": 113860
    },
    {
      "epoch": 6.073066666666667,
      "grad_norm": 0.15200091898441315,
      "learning_rate": 1.2043333333333334e-05,
      "loss": 0.0012,
      "step": 113870
    },
    {
      "epoch": 6.0736,
      "grad_norm": 0.022059474140405655,
      "learning_rate": 1.204e-05,
      "loss": 0.0018,
      "step": 113880
    },
    {
      "epoch": 6.074133333333333,
      "grad_norm": 0.06388287246227264,
      "learning_rate": 1.2036666666666668e-05,
      "loss": 0.002,
      "step": 113890
    },
    {
      "epoch": 6.074666666666666,
      "grad_norm": 0.10319516062736511,
      "learning_rate": 1.2033333333333334e-05,
      "loss": 0.0022,
      "step": 113900
    },
    {
      "epoch": 6.0752,
      "grad_norm": 0.06597364693880081,
      "learning_rate": 1.2030000000000002e-05,
      "loss": 0.0016,
      "step": 113910
    },
    {
      "epoch": 6.075733333333333,
      "grad_norm": 0.11937032639980316,
      "learning_rate": 1.2026666666666666e-05,
      "loss": 0.002,
      "step": 113920
    },
    {
      "epoch": 6.076266666666666,
      "grad_norm": 0.14553041756153107,
      "learning_rate": 1.2023333333333334e-05,
      "loss": 0.0019,
      "step": 113930
    },
    {
      "epoch": 6.0768,
      "grad_norm": 0.15087749063968658,
      "learning_rate": 1.202e-05,
      "loss": 0.0019,
      "step": 113940
    },
    {
      "epoch": 6.077333333333334,
      "grad_norm": 0.06062466278672218,
      "learning_rate": 1.2016666666666668e-05,
      "loss": 0.0023,
      "step": 113950
    },
    {
      "epoch": 6.077866666666667,
      "grad_norm": 0.04041639342904091,
      "learning_rate": 1.2013333333333334e-05,
      "loss": 0.0018,
      "step": 113960
    },
    {
      "epoch": 6.0784,
      "grad_norm": 0.28864553570747375,
      "learning_rate": 1.201e-05,
      "loss": 0.0017,
      "step": 113970
    },
    {
      "epoch": 6.0789333333333335,
      "grad_norm": 0.250311940908432,
      "learning_rate": 1.2006666666666668e-05,
      "loss": 0.0021,
      "step": 113980
    },
    {
      "epoch": 6.079466666666667,
      "grad_norm": 0.10075043886899948,
      "learning_rate": 1.2003333333333334e-05,
      "loss": 0.0023,
      "step": 113990
    },
    {
      "epoch": 6.08,
      "grad_norm": 0.09284994751214981,
      "learning_rate": 1.2e-05,
      "loss": 0.002,
      "step": 114000
    },
    {
      "epoch": 6.080533333333333,
      "grad_norm": 0.17731934785842896,
      "learning_rate": 1.1996666666666666e-05,
      "loss": 0.0015,
      "step": 114010
    },
    {
      "epoch": 6.081066666666667,
      "grad_norm": 0.22840942442417145,
      "learning_rate": 1.1993333333333334e-05,
      "loss": 0.002,
      "step": 114020
    },
    {
      "epoch": 6.0816,
      "grad_norm": 0.09674766659736633,
      "learning_rate": 1.199e-05,
      "loss": 0.0016,
      "step": 114030
    },
    {
      "epoch": 6.082133333333333,
      "grad_norm": 0.26190799474716187,
      "learning_rate": 1.1986666666666667e-05,
      "loss": 0.002,
      "step": 114040
    },
    {
      "epoch": 6.082666666666666,
      "grad_norm": 0.07370229065418243,
      "learning_rate": 1.1983333333333334e-05,
      "loss": 0.002,
      "step": 114050
    },
    {
      "epoch": 6.0832,
      "grad_norm": 0.07680999487638474,
      "learning_rate": 1.198e-05,
      "loss": 0.0022,
      "step": 114060
    },
    {
      "epoch": 6.083733333333333,
      "grad_norm": 0.11924132704734802,
      "learning_rate": 1.1976666666666667e-05,
      "loss": 0.0016,
      "step": 114070
    },
    {
      "epoch": 6.084266666666666,
      "grad_norm": 0.06300659477710724,
      "learning_rate": 1.1973333333333334e-05,
      "loss": 0.0013,
      "step": 114080
    },
    {
      "epoch": 6.0848,
      "grad_norm": 0.3511331081390381,
      "learning_rate": 1.197e-05,
      "loss": 0.0021,
      "step": 114090
    },
    {
      "epoch": 6.085333333333334,
      "grad_norm": 0.49186933040618896,
      "learning_rate": 1.1966666666666668e-05,
      "loss": 0.0015,
      "step": 114100
    },
    {
      "epoch": 6.085866666666667,
      "grad_norm": 0.34073957800865173,
      "learning_rate": 1.1963333333333333e-05,
      "loss": 0.002,
      "step": 114110
    },
    {
      "epoch": 6.0864,
      "grad_norm": 0.20015591382980347,
      "learning_rate": 1.196e-05,
      "loss": 0.0024,
      "step": 114120
    },
    {
      "epoch": 6.0869333333333335,
      "grad_norm": 0.43493959307670593,
      "learning_rate": 1.1956666666666667e-05,
      "loss": 0.0019,
      "step": 114130
    },
    {
      "epoch": 6.087466666666667,
      "grad_norm": 0.1411880999803543,
      "learning_rate": 1.1953333333333335e-05,
      "loss": 0.0023,
      "step": 114140
    },
    {
      "epoch": 6.088,
      "grad_norm": 0.03832152113318443,
      "learning_rate": 1.195e-05,
      "loss": 0.0017,
      "step": 114150
    },
    {
      "epoch": 6.088533333333333,
      "grad_norm": 0.11031047254800797,
      "learning_rate": 1.1946666666666667e-05,
      "loss": 0.0018,
      "step": 114160
    },
    {
      "epoch": 6.089066666666667,
      "grad_norm": 0.05024469271302223,
      "learning_rate": 1.1943333333333335e-05,
      "loss": 0.0021,
      "step": 114170
    },
    {
      "epoch": 6.0896,
      "grad_norm": 0.2614321708679199,
      "learning_rate": 1.1940000000000001e-05,
      "loss": 0.0017,
      "step": 114180
    },
    {
      "epoch": 6.090133333333333,
      "grad_norm": 0.2007720172405243,
      "learning_rate": 1.1936666666666667e-05,
      "loss": 0.0022,
      "step": 114190
    },
    {
      "epoch": 6.0906666666666665,
      "grad_norm": 0.061183102428913116,
      "learning_rate": 1.1933333333333333e-05,
      "loss": 0.0022,
      "step": 114200
    },
    {
      "epoch": 6.0912,
      "grad_norm": 0.5404908061027527,
      "learning_rate": 1.1930000000000001e-05,
      "loss": 0.0016,
      "step": 114210
    },
    {
      "epoch": 6.091733333333333,
      "grad_norm": 0.219368115067482,
      "learning_rate": 1.1926666666666667e-05,
      "loss": 0.0023,
      "step": 114220
    },
    {
      "epoch": 6.092266666666666,
      "grad_norm": 0.0990830510854721,
      "learning_rate": 1.1923333333333333e-05,
      "loss": 0.002,
      "step": 114230
    },
    {
      "epoch": 6.0928,
      "grad_norm": 0.11918050795793533,
      "learning_rate": 1.1920000000000001e-05,
      "loss": 0.0015,
      "step": 114240
    },
    {
      "epoch": 6.093333333333334,
      "grad_norm": 0.3751097023487091,
      "learning_rate": 1.1916666666666667e-05,
      "loss": 0.0017,
      "step": 114250
    },
    {
      "epoch": 6.093866666666667,
      "grad_norm": 0.3062025010585785,
      "learning_rate": 1.1913333333333333e-05,
      "loss": 0.0017,
      "step": 114260
    },
    {
      "epoch": 6.0944,
      "grad_norm": 0.18374936282634735,
      "learning_rate": 1.1910000000000001e-05,
      "loss": 0.0019,
      "step": 114270
    },
    {
      "epoch": 6.0949333333333335,
      "grad_norm": 0.1982036828994751,
      "learning_rate": 1.1906666666666667e-05,
      "loss": 0.002,
      "step": 114280
    },
    {
      "epoch": 6.095466666666667,
      "grad_norm": 0.255862832069397,
      "learning_rate": 1.1903333333333335e-05,
      "loss": 0.002,
      "step": 114290
    },
    {
      "epoch": 6.096,
      "grad_norm": 0.25992316007614136,
      "learning_rate": 1.19e-05,
      "loss": 0.0018,
      "step": 114300
    },
    {
      "epoch": 6.096533333333333,
      "grad_norm": 0.23009124398231506,
      "learning_rate": 1.1896666666666667e-05,
      "loss": 0.0019,
      "step": 114310
    },
    {
      "epoch": 6.097066666666667,
      "grad_norm": 0.1569395512342453,
      "learning_rate": 1.1893333333333334e-05,
      "loss": 0.0015,
      "step": 114320
    },
    {
      "epoch": 6.0976,
      "grad_norm": 0.23184937238693237,
      "learning_rate": 1.1890000000000001e-05,
      "loss": 0.0015,
      "step": 114330
    },
    {
      "epoch": 6.098133333333333,
      "grad_norm": 0.13657048344612122,
      "learning_rate": 1.1886666666666667e-05,
      "loss": 0.0013,
      "step": 114340
    },
    {
      "epoch": 6.0986666666666665,
      "grad_norm": 0.06139511615037918,
      "learning_rate": 1.1883333333333334e-05,
      "loss": 0.0021,
      "step": 114350
    },
    {
      "epoch": 6.0992,
      "grad_norm": 0.11924508959054947,
      "learning_rate": 1.1880000000000001e-05,
      "loss": 0.0017,
      "step": 114360
    },
    {
      "epoch": 6.099733333333333,
      "grad_norm": 0.2924904525279999,
      "learning_rate": 1.1876666666666668e-05,
      "loss": 0.0016,
      "step": 114370
    },
    {
      "epoch": 6.100266666666666,
      "grad_norm": 0.061268772929906845,
      "learning_rate": 1.1873333333333334e-05,
      "loss": 0.0021,
      "step": 114380
    },
    {
      "epoch": 6.1008,
      "grad_norm": 0.3168628513813019,
      "learning_rate": 1.187e-05,
      "loss": 0.0021,
      "step": 114390
    },
    {
      "epoch": 6.101333333333334,
      "grad_norm": 0.39789897203445435,
      "learning_rate": 1.1866666666666668e-05,
      "loss": 0.0015,
      "step": 114400
    },
    {
      "epoch": 6.101866666666667,
      "grad_norm": 0.4300088584423065,
      "learning_rate": 1.1863333333333334e-05,
      "loss": 0.0016,
      "step": 114410
    },
    {
      "epoch": 6.1024,
      "grad_norm": 0.05293544754385948,
      "learning_rate": 1.186e-05,
      "loss": 0.0016,
      "step": 114420
    },
    {
      "epoch": 6.1029333333333335,
      "grad_norm": 0.19878563284873962,
      "learning_rate": 1.1856666666666668e-05,
      "loss": 0.0016,
      "step": 114430
    },
    {
      "epoch": 6.103466666666667,
      "grad_norm": 0.1702905148267746,
      "learning_rate": 1.1853333333333334e-05,
      "loss": 0.0023,
      "step": 114440
    },
    {
      "epoch": 6.104,
      "grad_norm": 0.5275899171829224,
      "learning_rate": 1.185e-05,
      "loss": 0.0023,
      "step": 114450
    },
    {
      "epoch": 6.104533333333333,
      "grad_norm": 0.18394392728805542,
      "learning_rate": 1.1846666666666666e-05,
      "loss": 0.0018,
      "step": 114460
    },
    {
      "epoch": 6.105066666666667,
      "grad_norm": 0.07321459800004959,
      "learning_rate": 1.1843333333333334e-05,
      "loss": 0.0017,
      "step": 114470
    },
    {
      "epoch": 6.1056,
      "grad_norm": 0.04183069244027138,
      "learning_rate": 1.1840000000000002e-05,
      "loss": 0.0014,
      "step": 114480
    },
    {
      "epoch": 6.106133333333333,
      "grad_norm": 0.026165729388594627,
      "learning_rate": 1.1836666666666666e-05,
      "loss": 0.0017,
      "step": 114490
    },
    {
      "epoch": 6.1066666666666665,
      "grad_norm": 0.07071471214294434,
      "learning_rate": 1.1833333333333334e-05,
      "loss": 0.0018,
      "step": 114500
    },
    {
      "epoch": 6.1072,
      "grad_norm": 0.03698917105793953,
      "learning_rate": 1.183e-05,
      "loss": 0.0021,
      "step": 114510
    },
    {
      "epoch": 6.107733333333333,
      "grad_norm": 0.20794342458248138,
      "learning_rate": 1.1826666666666668e-05,
      "loss": 0.0016,
      "step": 114520
    },
    {
      "epoch": 6.108266666666666,
      "grad_norm": 0.16999132931232452,
      "learning_rate": 1.1823333333333334e-05,
      "loss": 0.0027,
      "step": 114530
    },
    {
      "epoch": 6.1088,
      "grad_norm": 0.39825817942619324,
      "learning_rate": 1.182e-05,
      "loss": 0.0017,
      "step": 114540
    },
    {
      "epoch": 6.109333333333334,
      "grad_norm": 0.15623335540294647,
      "learning_rate": 1.1816666666666668e-05,
      "loss": 0.0011,
      "step": 114550
    },
    {
      "epoch": 6.109866666666667,
      "grad_norm": 0.20094020664691925,
      "learning_rate": 1.1813333333333334e-05,
      "loss": 0.0015,
      "step": 114560
    },
    {
      "epoch": 6.1104,
      "grad_norm": 0.11989422887563705,
      "learning_rate": 1.181e-05,
      "loss": 0.0021,
      "step": 114570
    },
    {
      "epoch": 6.1109333333333336,
      "grad_norm": 0.09022635966539383,
      "learning_rate": 1.1806666666666667e-05,
      "loss": 0.0019,
      "step": 114580
    },
    {
      "epoch": 6.111466666666667,
      "grad_norm": 0.19974961876869202,
      "learning_rate": 1.1803333333333334e-05,
      "loss": 0.0017,
      "step": 114590
    },
    {
      "epoch": 6.112,
      "grad_norm": 0.061722949147224426,
      "learning_rate": 1.18e-05,
      "loss": 0.0021,
      "step": 114600
    },
    {
      "epoch": 6.112533333333333,
      "grad_norm": 0.060823433101177216,
      "learning_rate": 1.1796666666666667e-05,
      "loss": 0.002,
      "step": 114610
    },
    {
      "epoch": 6.113066666666667,
      "grad_norm": 0.04208265244960785,
      "learning_rate": 1.1793333333333334e-05,
      "loss": 0.0035,
      "step": 114620
    },
    {
      "epoch": 6.1136,
      "grad_norm": 0.08832863718271255,
      "learning_rate": 1.179e-05,
      "loss": 0.0022,
      "step": 114630
    },
    {
      "epoch": 6.114133333333333,
      "grad_norm": 0.18579255044460297,
      "learning_rate": 1.1786666666666667e-05,
      "loss": 0.0023,
      "step": 114640
    },
    {
      "epoch": 6.1146666666666665,
      "grad_norm": 0.22904787957668304,
      "learning_rate": 1.1783333333333333e-05,
      "loss": 0.0017,
      "step": 114650
    },
    {
      "epoch": 6.1152,
      "grad_norm": 0.052241016179323196,
      "learning_rate": 1.178e-05,
      "loss": 0.0025,
      "step": 114660
    },
    {
      "epoch": 6.115733333333333,
      "grad_norm": 0.14574764668941498,
      "learning_rate": 1.1776666666666669e-05,
      "loss": 0.0018,
      "step": 114670
    },
    {
      "epoch": 6.116266666666666,
      "grad_norm": 0.13618765771389008,
      "learning_rate": 1.1773333333333333e-05,
      "loss": 0.0011,
      "step": 114680
    },
    {
      "epoch": 6.1168,
      "grad_norm": 0.11953920125961304,
      "learning_rate": 1.177e-05,
      "loss": 0.0023,
      "step": 114690
    },
    {
      "epoch": 6.117333333333334,
      "grad_norm": 0.12344769388437271,
      "learning_rate": 1.1766666666666667e-05,
      "loss": 0.0013,
      "step": 114700
    },
    {
      "epoch": 6.117866666666667,
      "grad_norm": 0.2926572561264038,
      "learning_rate": 1.1763333333333335e-05,
      "loss": 0.0018,
      "step": 114710
    },
    {
      "epoch": 6.1184,
      "grad_norm": 0.2883950471878052,
      "learning_rate": 1.1760000000000001e-05,
      "loss": 0.0027,
      "step": 114720
    },
    {
      "epoch": 6.118933333333334,
      "grad_norm": 0.40032508969306946,
      "learning_rate": 1.1756666666666667e-05,
      "loss": 0.0012,
      "step": 114730
    },
    {
      "epoch": 6.119466666666667,
      "grad_norm": 0.0359828919172287,
      "learning_rate": 1.1753333333333335e-05,
      "loss": 0.0016,
      "step": 114740
    },
    {
      "epoch": 6.12,
      "grad_norm": 0.1252780556678772,
      "learning_rate": 1.175e-05,
      "loss": 0.0013,
      "step": 114750
    },
    {
      "epoch": 6.120533333333333,
      "grad_norm": 0.22683344781398773,
      "learning_rate": 1.1746666666666667e-05,
      "loss": 0.0015,
      "step": 114760
    },
    {
      "epoch": 6.121066666666667,
      "grad_norm": 0.23187752068042755,
      "learning_rate": 1.1743333333333333e-05,
      "loss": 0.0026,
      "step": 114770
    },
    {
      "epoch": 6.1216,
      "grad_norm": 0.04416775703430176,
      "learning_rate": 1.1740000000000001e-05,
      "loss": 0.0016,
      "step": 114780
    },
    {
      "epoch": 6.122133333333333,
      "grad_norm": 0.15481850504875183,
      "learning_rate": 1.1736666666666667e-05,
      "loss": 0.0018,
      "step": 114790
    },
    {
      "epoch": 6.1226666666666665,
      "grad_norm": 0.1809166818857193,
      "learning_rate": 1.1733333333333333e-05,
      "loss": 0.0016,
      "step": 114800
    },
    {
      "epoch": 6.1232,
      "grad_norm": 0.06604737043380737,
      "learning_rate": 1.1730000000000001e-05,
      "loss": 0.002,
      "step": 114810
    },
    {
      "epoch": 6.123733333333333,
      "grad_norm": 0.07972688227891922,
      "learning_rate": 1.1726666666666667e-05,
      "loss": 0.0018,
      "step": 114820
    },
    {
      "epoch": 6.124266666666666,
      "grad_norm": 0.0345122367143631,
      "learning_rate": 1.1723333333333333e-05,
      "loss": 0.0018,
      "step": 114830
    },
    {
      "epoch": 6.1248,
      "grad_norm": 0.20457686483860016,
      "learning_rate": 1.172e-05,
      "loss": 0.0017,
      "step": 114840
    },
    {
      "epoch": 6.125333333333334,
      "grad_norm": 0.15013721585273743,
      "learning_rate": 1.1716666666666667e-05,
      "loss": 0.002,
      "step": 114850
    },
    {
      "epoch": 6.125866666666667,
      "grad_norm": 0.08639229834079742,
      "learning_rate": 1.1713333333333335e-05,
      "loss": 0.0015,
      "step": 114860
    },
    {
      "epoch": 6.1264,
      "grad_norm": 0.06524085998535156,
      "learning_rate": 1.171e-05,
      "loss": 0.0017,
      "step": 114870
    },
    {
      "epoch": 6.126933333333334,
      "grad_norm": 0.30666860938072205,
      "learning_rate": 1.1706666666666668e-05,
      "loss": 0.0014,
      "step": 114880
    },
    {
      "epoch": 6.127466666666667,
      "grad_norm": 0.14514470100402832,
      "learning_rate": 1.1703333333333334e-05,
      "loss": 0.0014,
      "step": 114890
    },
    {
      "epoch": 6.128,
      "grad_norm": 0.054550427943468094,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 0.0022,
      "step": 114900
    },
    {
      "epoch": 6.128533333333333,
      "grad_norm": 0.2082507461309433,
      "learning_rate": 1.1696666666666668e-05,
      "loss": 0.0014,
      "step": 114910
    },
    {
      "epoch": 6.129066666666667,
      "grad_norm": 0.38488733768463135,
      "learning_rate": 1.1693333333333334e-05,
      "loss": 0.0017,
      "step": 114920
    },
    {
      "epoch": 6.1296,
      "grad_norm": 0.186004638671875,
      "learning_rate": 1.1690000000000002e-05,
      "loss": 0.0013,
      "step": 114930
    },
    {
      "epoch": 6.130133333333333,
      "grad_norm": 0.11528448015451431,
      "learning_rate": 1.1686666666666666e-05,
      "loss": 0.0021,
      "step": 114940
    },
    {
      "epoch": 6.1306666666666665,
      "grad_norm": 0.29237475991249084,
      "learning_rate": 1.1683333333333334e-05,
      "loss": 0.0013,
      "step": 114950
    },
    {
      "epoch": 6.1312,
      "grad_norm": 0.12444397807121277,
      "learning_rate": 1.168e-05,
      "loss": 0.0016,
      "step": 114960
    },
    {
      "epoch": 6.131733333333333,
      "grad_norm": 0.347696453332901,
      "learning_rate": 1.1676666666666668e-05,
      "loss": 0.0024,
      "step": 114970
    },
    {
      "epoch": 6.132266666666666,
      "grad_norm": 0.30478349328041077,
      "learning_rate": 1.1673333333333334e-05,
      "loss": 0.0019,
      "step": 114980
    },
    {
      "epoch": 6.1328,
      "grad_norm": 0.2153608649969101,
      "learning_rate": 1.167e-05,
      "loss": 0.0015,
      "step": 114990
    },
    {
      "epoch": 6.133333333333334,
      "grad_norm": 0.7223231196403503,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 0.0019,
      "step": 115000
    },
    {
      "epoch": 6.133866666666667,
      "grad_norm": 0.20480988919734955,
      "learning_rate": 1.1663333333333334e-05,
      "loss": 0.0019,
      "step": 115010
    },
    {
      "epoch": 6.1344,
      "grad_norm": 0.11590304225683212,
      "learning_rate": 1.166e-05,
      "loss": 0.0012,
      "step": 115020
    },
    {
      "epoch": 6.134933333333334,
      "grad_norm": 0.1747264266014099,
      "learning_rate": 1.1656666666666666e-05,
      "loss": 0.0026,
      "step": 115030
    },
    {
      "epoch": 6.135466666666667,
      "grad_norm": 0.05007602274417877,
      "learning_rate": 1.1653333333333334e-05,
      "loss": 0.0015,
      "step": 115040
    },
    {
      "epoch": 6.136,
      "grad_norm": 0.2357400804758072,
      "learning_rate": 1.1650000000000002e-05,
      "loss": 0.0013,
      "step": 115050
    },
    {
      "epoch": 6.136533333333333,
      "grad_norm": 0.23146039247512817,
      "learning_rate": 1.1646666666666666e-05,
      "loss": 0.0018,
      "step": 115060
    },
    {
      "epoch": 6.137066666666667,
      "grad_norm": 0.3710678219795227,
      "learning_rate": 1.1643333333333334e-05,
      "loss": 0.0015,
      "step": 115070
    },
    {
      "epoch": 6.1376,
      "grad_norm": 0.4575784504413605,
      "learning_rate": 1.164e-05,
      "loss": 0.0014,
      "step": 115080
    },
    {
      "epoch": 6.138133333333333,
      "grad_norm": 0.06502442061901093,
      "learning_rate": 1.1636666666666666e-05,
      "loss": 0.0017,
      "step": 115090
    },
    {
      "epoch": 6.1386666666666665,
      "grad_norm": 0.07498685270547867,
      "learning_rate": 1.1633333333333334e-05,
      "loss": 0.0012,
      "step": 115100
    },
    {
      "epoch": 6.1392,
      "grad_norm": 0.5804852247238159,
      "learning_rate": 1.163e-05,
      "loss": 0.0014,
      "step": 115110
    },
    {
      "epoch": 6.139733333333333,
      "grad_norm": 0.11571455001831055,
      "learning_rate": 1.1626666666666668e-05,
      "loss": 0.0018,
      "step": 115120
    },
    {
      "epoch": 6.140266666666666,
      "grad_norm": 0.3318318724632263,
      "learning_rate": 1.1623333333333333e-05,
      "loss": 0.0027,
      "step": 115130
    },
    {
      "epoch": 6.1408,
      "grad_norm": 0.04985947161912918,
      "learning_rate": 1.162e-05,
      "loss": 0.0018,
      "step": 115140
    },
    {
      "epoch": 6.141333333333334,
      "grad_norm": 0.08645365387201309,
      "learning_rate": 1.1616666666666667e-05,
      "loss": 0.0015,
      "step": 115150
    },
    {
      "epoch": 6.141866666666667,
      "grad_norm": 0.3190605938434601,
      "learning_rate": 1.1613333333333335e-05,
      "loss": 0.0018,
      "step": 115160
    },
    {
      "epoch": 6.1424,
      "grad_norm": 0.25729069113731384,
      "learning_rate": 1.161e-05,
      "loss": 0.0014,
      "step": 115170
    },
    {
      "epoch": 6.142933333333334,
      "grad_norm": 0.12408998608589172,
      "learning_rate": 1.1606666666666667e-05,
      "loss": 0.002,
      "step": 115180
    },
    {
      "epoch": 6.143466666666667,
      "grad_norm": 0.2265605479478836,
      "learning_rate": 1.1603333333333335e-05,
      "loss": 0.0021,
      "step": 115190
    },
    {
      "epoch": 6.144,
      "grad_norm": 0.04188506677746773,
      "learning_rate": 1.16e-05,
      "loss": 0.003,
      "step": 115200
    },
    {
      "epoch": 6.144533333333333,
      "grad_norm": 0.0495496541261673,
      "learning_rate": 1.1596666666666667e-05,
      "loss": 0.0016,
      "step": 115210
    },
    {
      "epoch": 6.145066666666667,
      "grad_norm": 0.05435248464345932,
      "learning_rate": 1.1593333333333333e-05,
      "loss": 0.0014,
      "step": 115220
    },
    {
      "epoch": 6.1456,
      "grad_norm": 0.14969921112060547,
      "learning_rate": 1.159e-05,
      "loss": 0.0013,
      "step": 115230
    },
    {
      "epoch": 6.146133333333333,
      "grad_norm": 0.17254552245140076,
      "learning_rate": 1.1586666666666669e-05,
      "loss": 0.0024,
      "step": 115240
    },
    {
      "epoch": 6.1466666666666665,
      "grad_norm": 0.2606382966041565,
      "learning_rate": 1.1583333333333333e-05,
      "loss": 0.0017,
      "step": 115250
    },
    {
      "epoch": 6.1472,
      "grad_norm": 0.032609350979328156,
      "learning_rate": 1.1580000000000001e-05,
      "loss": 0.0018,
      "step": 115260
    },
    {
      "epoch": 6.147733333333333,
      "grad_norm": 0.22620031237602234,
      "learning_rate": 1.1576666666666667e-05,
      "loss": 0.0019,
      "step": 115270
    },
    {
      "epoch": 6.148266666666666,
      "grad_norm": 0.05007403716444969,
      "learning_rate": 1.1573333333333333e-05,
      "loss": 0.0016,
      "step": 115280
    },
    {
      "epoch": 6.1488,
      "grad_norm": 0.06210820749402046,
      "learning_rate": 1.1570000000000001e-05,
      "loss": 0.0019,
      "step": 115290
    },
    {
      "epoch": 6.149333333333334,
      "grad_norm": 0.2552427351474762,
      "learning_rate": 1.1566666666666667e-05,
      "loss": 0.0029,
      "step": 115300
    },
    {
      "epoch": 6.149866666666667,
      "grad_norm": 0.42471402883529663,
      "learning_rate": 1.1563333333333335e-05,
      "loss": 0.003,
      "step": 115310
    },
    {
      "epoch": 6.1504,
      "grad_norm": 0.0603652149438858,
      "learning_rate": 1.156e-05,
      "loss": 0.0024,
      "step": 115320
    },
    {
      "epoch": 6.150933333333334,
      "grad_norm": 0.2582607567310333,
      "learning_rate": 1.1556666666666667e-05,
      "loss": 0.0026,
      "step": 115330
    },
    {
      "epoch": 6.151466666666667,
      "grad_norm": 0.17492333054542542,
      "learning_rate": 1.1553333333333333e-05,
      "loss": 0.0018,
      "step": 115340
    },
    {
      "epoch": 6.152,
      "grad_norm": 0.035026248544454575,
      "learning_rate": 1.1550000000000001e-05,
      "loss": 0.0018,
      "step": 115350
    },
    {
      "epoch": 6.152533333333333,
      "grad_norm": 0.17341603338718414,
      "learning_rate": 1.1546666666666667e-05,
      "loss": 0.0017,
      "step": 115360
    },
    {
      "epoch": 6.153066666666667,
      "grad_norm": 0.35003870725631714,
      "learning_rate": 1.1543333333333333e-05,
      "loss": 0.002,
      "step": 115370
    },
    {
      "epoch": 6.1536,
      "grad_norm": 0.1754811406135559,
      "learning_rate": 1.1540000000000001e-05,
      "loss": 0.0014,
      "step": 115380
    },
    {
      "epoch": 6.154133333333333,
      "grad_norm": 0.2078421413898468,
      "learning_rate": 1.1536666666666667e-05,
      "loss": 0.0015,
      "step": 115390
    },
    {
      "epoch": 6.1546666666666665,
      "grad_norm": 0.06072484701871872,
      "learning_rate": 1.1533333333333334e-05,
      "loss": 0.0012,
      "step": 115400
    },
    {
      "epoch": 6.1552,
      "grad_norm": 0.23785711824893951,
      "learning_rate": 1.153e-05,
      "loss": 0.0025,
      "step": 115410
    },
    {
      "epoch": 6.155733333333333,
      "grad_norm": 0.1732126623392105,
      "learning_rate": 1.1526666666666668e-05,
      "loss": 0.0015,
      "step": 115420
    },
    {
      "epoch": 6.156266666666666,
      "grad_norm": 0.20727454125881195,
      "learning_rate": 1.1523333333333334e-05,
      "loss": 0.0019,
      "step": 115430
    },
    {
      "epoch": 6.1568,
      "grad_norm": 0.4279167652130127,
      "learning_rate": 1.152e-05,
      "loss": 0.002,
      "step": 115440
    },
    {
      "epoch": 6.157333333333334,
      "grad_norm": 0.12075653672218323,
      "learning_rate": 1.1516666666666668e-05,
      "loss": 0.0017,
      "step": 115450
    },
    {
      "epoch": 6.157866666666667,
      "grad_norm": 0.27039065957069397,
      "learning_rate": 1.1513333333333334e-05,
      "loss": 0.0023,
      "step": 115460
    },
    {
      "epoch": 6.1584,
      "grad_norm": 0.40572282671928406,
      "learning_rate": 1.151e-05,
      "loss": 0.0018,
      "step": 115470
    },
    {
      "epoch": 6.158933333333334,
      "grad_norm": 0.2017931342124939,
      "learning_rate": 1.1506666666666668e-05,
      "loss": 0.0012,
      "step": 115480
    },
    {
      "epoch": 6.159466666666667,
      "grad_norm": 0.2317861169576645,
      "learning_rate": 1.1503333333333334e-05,
      "loss": 0.0014,
      "step": 115490
    },
    {
      "epoch": 6.16,
      "grad_norm": 0.46070343255996704,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 0.0013,
      "step": 115500
    },
    {
      "epoch": 6.160533333333333,
      "grad_norm": 0.04580814763903618,
      "learning_rate": 1.1496666666666666e-05,
      "loss": 0.0016,
      "step": 115510
    },
    {
      "epoch": 6.161066666666667,
      "grad_norm": 0.23570756614208221,
      "learning_rate": 1.1493333333333334e-05,
      "loss": 0.0011,
      "step": 115520
    },
    {
      "epoch": 6.1616,
      "grad_norm": 0.23112381994724274,
      "learning_rate": 1.149e-05,
      "loss": 0.0018,
      "step": 115530
    },
    {
      "epoch": 6.162133333333333,
      "grad_norm": 0.13079652190208435,
      "learning_rate": 1.1486666666666668e-05,
      "loss": 0.0014,
      "step": 115540
    },
    {
      "epoch": 6.1626666666666665,
      "grad_norm": 0.1446269303560257,
      "learning_rate": 1.1483333333333334e-05,
      "loss": 0.0018,
      "step": 115550
    },
    {
      "epoch": 6.1632,
      "grad_norm": 0.24099870026111603,
      "learning_rate": 1.148e-05,
      "loss": 0.0017,
      "step": 115560
    },
    {
      "epoch": 6.163733333333333,
      "grad_norm": 0.573728084564209,
      "learning_rate": 1.1476666666666668e-05,
      "loss": 0.0016,
      "step": 115570
    },
    {
      "epoch": 6.164266666666666,
      "grad_norm": 0.12672901153564453,
      "learning_rate": 1.1473333333333334e-05,
      "loss": 0.0014,
      "step": 115580
    },
    {
      "epoch": 6.1648,
      "grad_norm": 0.23507709801197052,
      "learning_rate": 1.147e-05,
      "loss": 0.0018,
      "step": 115590
    },
    {
      "epoch": 6.165333333333333,
      "grad_norm": 0.058446794748306274,
      "learning_rate": 1.1466666666666666e-05,
      "loss": 0.0021,
      "step": 115600
    },
    {
      "epoch": 6.165866666666667,
      "grad_norm": 0.2582590878009796,
      "learning_rate": 1.1463333333333334e-05,
      "loss": 0.0023,
      "step": 115610
    },
    {
      "epoch": 6.1664,
      "grad_norm": 0.11448060721158981,
      "learning_rate": 1.146e-05,
      "loss": 0.0018,
      "step": 115620
    },
    {
      "epoch": 6.166933333333334,
      "grad_norm": 0.11973077058792114,
      "learning_rate": 1.1456666666666667e-05,
      "loss": 0.0019,
      "step": 115630
    },
    {
      "epoch": 6.167466666666667,
      "grad_norm": 0.044249314814805984,
      "learning_rate": 1.1453333333333334e-05,
      "loss": 0.0026,
      "step": 115640
    },
    {
      "epoch": 6.168,
      "grad_norm": 0.21022218465805054,
      "learning_rate": 1.145e-05,
      "loss": 0.0018,
      "step": 115650
    },
    {
      "epoch": 6.168533333333333,
      "grad_norm": 0.1717803031206131,
      "learning_rate": 1.1446666666666667e-05,
      "loss": 0.002,
      "step": 115660
    },
    {
      "epoch": 6.169066666666667,
      "grad_norm": 0.1502322554588318,
      "learning_rate": 1.1443333333333334e-05,
      "loss": 0.0016,
      "step": 115670
    },
    {
      "epoch": 6.1696,
      "grad_norm": 0.022166132926940918,
      "learning_rate": 1.144e-05,
      "loss": 0.0013,
      "step": 115680
    },
    {
      "epoch": 6.170133333333333,
      "grad_norm": 0.17572061717510223,
      "learning_rate": 1.1436666666666668e-05,
      "loss": 0.0025,
      "step": 115690
    },
    {
      "epoch": 6.1706666666666665,
      "grad_norm": 0.12684807181358337,
      "learning_rate": 1.1433333333333333e-05,
      "loss": 0.0012,
      "step": 115700
    },
    {
      "epoch": 6.1712,
      "grad_norm": 0.063475102186203,
      "learning_rate": 1.143e-05,
      "loss": 0.0023,
      "step": 115710
    },
    {
      "epoch": 6.171733333333333,
      "grad_norm": 0.05799030140042305,
      "learning_rate": 1.1426666666666667e-05,
      "loss": 0.0013,
      "step": 115720
    },
    {
      "epoch": 6.172266666666666,
      "grad_norm": 0.14385411143302917,
      "learning_rate": 1.1423333333333335e-05,
      "loss": 0.0016,
      "step": 115730
    },
    {
      "epoch": 6.1728,
      "grad_norm": 0.0904928669333458,
      "learning_rate": 1.142e-05,
      "loss": 0.0018,
      "step": 115740
    },
    {
      "epoch": 6.173333333333334,
      "grad_norm": 0.08954720944166183,
      "learning_rate": 1.1416666666666667e-05,
      "loss": 0.0028,
      "step": 115750
    },
    {
      "epoch": 6.173866666666667,
      "grad_norm": 0.1506759375333786,
      "learning_rate": 1.1413333333333335e-05,
      "loss": 0.0018,
      "step": 115760
    },
    {
      "epoch": 6.1744,
      "grad_norm": 0.24330900609493256,
      "learning_rate": 1.141e-05,
      "loss": 0.0018,
      "step": 115770
    },
    {
      "epoch": 6.174933333333334,
      "grad_norm": 0.14306864142417908,
      "learning_rate": 1.1406666666666667e-05,
      "loss": 0.0016,
      "step": 115780
    },
    {
      "epoch": 6.175466666666667,
      "grad_norm": 0.09072994440793991,
      "learning_rate": 1.1403333333333333e-05,
      "loss": 0.0015,
      "step": 115790
    },
    {
      "epoch": 6.176,
      "grad_norm": 0.07225930690765381,
      "learning_rate": 1.1400000000000001e-05,
      "loss": 0.0022,
      "step": 115800
    },
    {
      "epoch": 6.176533333333333,
      "grad_norm": 0.29152706265449524,
      "learning_rate": 1.1396666666666667e-05,
      "loss": 0.0014,
      "step": 115810
    },
    {
      "epoch": 6.177066666666667,
      "grad_norm": 0.12171436101198196,
      "learning_rate": 1.1393333333333333e-05,
      "loss": 0.0013,
      "step": 115820
    },
    {
      "epoch": 6.1776,
      "grad_norm": 0.11304252594709396,
      "learning_rate": 1.1390000000000001e-05,
      "loss": 0.0026,
      "step": 115830
    },
    {
      "epoch": 6.178133333333333,
      "grad_norm": 0.5994919538497925,
      "learning_rate": 1.1386666666666667e-05,
      "loss": 0.0018,
      "step": 115840
    },
    {
      "epoch": 6.1786666666666665,
      "grad_norm": 0.17861337959766388,
      "learning_rate": 1.1383333333333333e-05,
      "loss": 0.0019,
      "step": 115850
    },
    {
      "epoch": 6.1792,
      "grad_norm": 0.06719207018613815,
      "learning_rate": 1.1380000000000001e-05,
      "loss": 0.002,
      "step": 115860
    },
    {
      "epoch": 6.179733333333333,
      "grad_norm": 0.17034421861171722,
      "learning_rate": 1.1376666666666667e-05,
      "loss": 0.0017,
      "step": 115870
    },
    {
      "epoch": 6.180266666666666,
      "grad_norm": 0.06473463028669357,
      "learning_rate": 1.1373333333333335e-05,
      "loss": 0.0032,
      "step": 115880
    },
    {
      "epoch": 6.1808,
      "grad_norm": 0.5497205853462219,
      "learning_rate": 1.137e-05,
      "loss": 0.002,
      "step": 115890
    },
    {
      "epoch": 6.181333333333333,
      "grad_norm": 0.3488985300064087,
      "learning_rate": 1.1366666666666667e-05,
      "loss": 0.0021,
      "step": 115900
    },
    {
      "epoch": 6.181866666666667,
      "grad_norm": 0.12463890016078949,
      "learning_rate": 1.1363333333333334e-05,
      "loss": 0.0016,
      "step": 115910
    },
    {
      "epoch": 6.1824,
      "grad_norm": 0.06129244342446327,
      "learning_rate": 1.1360000000000001e-05,
      "loss": 0.0021,
      "step": 115920
    },
    {
      "epoch": 6.182933333333334,
      "grad_norm": 0.2680884003639221,
      "learning_rate": 1.1356666666666667e-05,
      "loss": 0.0022,
      "step": 115930
    },
    {
      "epoch": 6.183466666666667,
      "grad_norm": 0.15025189518928528,
      "learning_rate": 1.1353333333333334e-05,
      "loss": 0.0013,
      "step": 115940
    },
    {
      "epoch": 6.184,
      "grad_norm": 0.0751878023147583,
      "learning_rate": 1.1350000000000001e-05,
      "loss": 0.0013,
      "step": 115950
    },
    {
      "epoch": 6.184533333333333,
      "grad_norm": 0.12357430160045624,
      "learning_rate": 1.1346666666666666e-05,
      "loss": 0.0015,
      "step": 115960
    },
    {
      "epoch": 6.185066666666667,
      "grad_norm": 0.11704522371292114,
      "learning_rate": 1.1343333333333334e-05,
      "loss": 0.0012,
      "step": 115970
    },
    {
      "epoch": 6.1856,
      "grad_norm": 0.08782623708248138,
      "learning_rate": 1.134e-05,
      "loss": 0.0012,
      "step": 115980
    },
    {
      "epoch": 6.186133333333333,
      "grad_norm": 0.29260823130607605,
      "learning_rate": 1.1336666666666668e-05,
      "loss": 0.0014,
      "step": 115990
    },
    {
      "epoch": 6.1866666666666665,
      "grad_norm": 0.40184998512268066,
      "learning_rate": 1.1333333333333334e-05,
      "loss": 0.0022,
      "step": 116000
    },
    {
      "epoch": 6.1872,
      "grad_norm": 0.2843320965766907,
      "learning_rate": 1.133e-05,
      "loss": 0.0023,
      "step": 116010
    },
    {
      "epoch": 6.187733333333333,
      "grad_norm": 0.25674787163734436,
      "learning_rate": 1.1326666666666668e-05,
      "loss": 0.0022,
      "step": 116020
    },
    {
      "epoch": 6.188266666666666,
      "grad_norm": 0.20274539291858673,
      "learning_rate": 1.1323333333333334e-05,
      "loss": 0.0019,
      "step": 116030
    },
    {
      "epoch": 6.1888,
      "grad_norm": 0.21478591859340668,
      "learning_rate": 1.132e-05,
      "loss": 0.0018,
      "step": 116040
    },
    {
      "epoch": 6.189333333333333,
      "grad_norm": 0.34664639830589294,
      "learning_rate": 1.1316666666666668e-05,
      "loss": 0.0013,
      "step": 116050
    },
    {
      "epoch": 6.189866666666667,
      "grad_norm": 0.2320156693458557,
      "learning_rate": 1.1313333333333334e-05,
      "loss": 0.0014,
      "step": 116060
    },
    {
      "epoch": 6.1904,
      "grad_norm": 0.11466668546199799,
      "learning_rate": 1.1310000000000002e-05,
      "loss": 0.002,
      "step": 116070
    },
    {
      "epoch": 6.190933333333334,
      "grad_norm": 0.17585402727127075,
      "learning_rate": 1.1306666666666666e-05,
      "loss": 0.0021,
      "step": 116080
    },
    {
      "epoch": 6.191466666666667,
      "grad_norm": 0.20184478163719177,
      "learning_rate": 1.1303333333333334e-05,
      "loss": 0.0025,
      "step": 116090
    },
    {
      "epoch": 6.192,
      "grad_norm": 0.0949869379401207,
      "learning_rate": 1.13e-05,
      "loss": 0.0023,
      "step": 116100
    },
    {
      "epoch": 6.1925333333333334,
      "grad_norm": 0.20493480563163757,
      "learning_rate": 1.1296666666666668e-05,
      "loss": 0.0018,
      "step": 116110
    },
    {
      "epoch": 6.193066666666667,
      "grad_norm": 0.06658477336168289,
      "learning_rate": 1.1293333333333334e-05,
      "loss": 0.0016,
      "step": 116120
    },
    {
      "epoch": 6.1936,
      "grad_norm": 0.15358230471611023,
      "learning_rate": 1.129e-05,
      "loss": 0.0026,
      "step": 116130
    },
    {
      "epoch": 6.194133333333333,
      "grad_norm": 0.2612016797065735,
      "learning_rate": 1.1286666666666668e-05,
      "loss": 0.0015,
      "step": 116140
    },
    {
      "epoch": 6.1946666666666665,
      "grad_norm": 0.4314199686050415,
      "learning_rate": 1.1283333333333333e-05,
      "loss": 0.0014,
      "step": 116150
    },
    {
      "epoch": 6.1952,
      "grad_norm": 0.06310045719146729,
      "learning_rate": 1.128e-05,
      "loss": 0.0029,
      "step": 116160
    },
    {
      "epoch": 6.195733333333333,
      "grad_norm": 0.3171497881412506,
      "learning_rate": 1.1276666666666667e-05,
      "loss": 0.0017,
      "step": 116170
    },
    {
      "epoch": 6.196266666666666,
      "grad_norm": 0.0670543983578682,
      "learning_rate": 1.1273333333333334e-05,
      "loss": 0.0017,
      "step": 116180
    },
    {
      "epoch": 6.1968,
      "grad_norm": 0.20197291672229767,
      "learning_rate": 1.127e-05,
      "loss": 0.0019,
      "step": 116190
    },
    {
      "epoch": 6.197333333333333,
      "grad_norm": 0.2938387989997864,
      "learning_rate": 1.1266666666666667e-05,
      "loss": 0.0017,
      "step": 116200
    },
    {
      "epoch": 6.197866666666667,
      "grad_norm": 0.5961129069328308,
      "learning_rate": 1.1263333333333334e-05,
      "loss": 0.0021,
      "step": 116210
    },
    {
      "epoch": 6.1984,
      "grad_norm": 0.11991128325462341,
      "learning_rate": 1.126e-05,
      "loss": 0.002,
      "step": 116220
    },
    {
      "epoch": 6.198933333333334,
      "grad_norm": 0.2623817026615143,
      "learning_rate": 1.1256666666666667e-05,
      "loss": 0.002,
      "step": 116230
    },
    {
      "epoch": 6.199466666666667,
      "grad_norm": 0.25661131739616394,
      "learning_rate": 1.1253333333333335e-05,
      "loss": 0.0016,
      "step": 116240
    },
    {
      "epoch": 6.2,
      "grad_norm": 0.15208370983600616,
      "learning_rate": 1.125e-05,
      "loss": 0.0014,
      "step": 116250
    },
    {
      "epoch": 6.2005333333333335,
      "grad_norm": 0.20961615443229675,
      "learning_rate": 1.1246666666666669e-05,
      "loss": 0.0021,
      "step": 116260
    },
    {
      "epoch": 6.201066666666667,
      "grad_norm": 0.343853235244751,
      "learning_rate": 1.1243333333333333e-05,
      "loss": 0.0012,
      "step": 116270
    },
    {
      "epoch": 6.2016,
      "grad_norm": 0.517033576965332,
      "learning_rate": 1.124e-05,
      "loss": 0.0022,
      "step": 116280
    },
    {
      "epoch": 6.202133333333333,
      "grad_norm": 0.0317918062210083,
      "learning_rate": 1.1236666666666667e-05,
      "loss": 0.0022,
      "step": 116290
    },
    {
      "epoch": 6.2026666666666666,
      "grad_norm": 0.2893930673599243,
      "learning_rate": 1.1233333333333333e-05,
      "loss": 0.0017,
      "step": 116300
    },
    {
      "epoch": 6.2032,
      "grad_norm": 0.2266547679901123,
      "learning_rate": 1.1230000000000001e-05,
      "loss": 0.0018,
      "step": 116310
    },
    {
      "epoch": 6.203733333333333,
      "grad_norm": 0.14165028929710388,
      "learning_rate": 1.1226666666666667e-05,
      "loss": 0.003,
      "step": 116320
    },
    {
      "epoch": 6.204266666666666,
      "grad_norm": 0.03802059218287468,
      "learning_rate": 1.1223333333333335e-05,
      "loss": 0.0023,
      "step": 116330
    },
    {
      "epoch": 6.2048,
      "grad_norm": 0.11847992986440659,
      "learning_rate": 1.122e-05,
      "loss": 0.0023,
      "step": 116340
    },
    {
      "epoch": 6.205333333333333,
      "grad_norm": 0.15753498673439026,
      "learning_rate": 1.1216666666666667e-05,
      "loss": 0.0017,
      "step": 116350
    },
    {
      "epoch": 6.205866666666667,
      "grad_norm": 0.17375829815864563,
      "learning_rate": 1.1213333333333333e-05,
      "loss": 0.0022,
      "step": 116360
    },
    {
      "epoch": 6.2064,
      "grad_norm": 0.061976827681064606,
      "learning_rate": 1.1210000000000001e-05,
      "loss": 0.0014,
      "step": 116370
    },
    {
      "epoch": 6.206933333333334,
      "grad_norm": 0.1234649196267128,
      "learning_rate": 1.1206666666666667e-05,
      "loss": 0.0019,
      "step": 116380
    },
    {
      "epoch": 6.207466666666667,
      "grad_norm": 0.21270272135734558,
      "learning_rate": 1.1203333333333333e-05,
      "loss": 0.0017,
      "step": 116390
    },
    {
      "epoch": 6.208,
      "grad_norm": 0.16180716454982758,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 0.0018,
      "step": 116400
    },
    {
      "epoch": 6.2085333333333335,
      "grad_norm": 0.09980922937393188,
      "learning_rate": 1.1196666666666667e-05,
      "loss": 0.0016,
      "step": 116410
    },
    {
      "epoch": 6.209066666666667,
      "grad_norm": 0.1751522570848465,
      "learning_rate": 1.1193333333333333e-05,
      "loss": 0.0014,
      "step": 116420
    },
    {
      "epoch": 6.2096,
      "grad_norm": 0.06618887186050415,
      "learning_rate": 1.1190000000000001e-05,
      "loss": 0.0016,
      "step": 116430
    },
    {
      "epoch": 6.210133333333333,
      "grad_norm": 0.2287265807390213,
      "learning_rate": 1.1186666666666667e-05,
      "loss": 0.0018,
      "step": 116440
    },
    {
      "epoch": 6.210666666666667,
      "grad_norm": 0.2543780207633972,
      "learning_rate": 1.1183333333333335e-05,
      "loss": 0.0018,
      "step": 116450
    },
    {
      "epoch": 6.2112,
      "grad_norm": 0.2027948498725891,
      "learning_rate": 1.118e-05,
      "loss": 0.0016,
      "step": 116460
    },
    {
      "epoch": 6.211733333333333,
      "grad_norm": 0.09824518114328384,
      "learning_rate": 1.1176666666666668e-05,
      "loss": 0.0015,
      "step": 116470
    },
    {
      "epoch": 6.212266666666666,
      "grad_norm": 0.06512635946273804,
      "learning_rate": 1.1173333333333334e-05,
      "loss": 0.0025,
      "step": 116480
    },
    {
      "epoch": 6.2128,
      "grad_norm": 0.056663937866687775,
      "learning_rate": 1.117e-05,
      "loss": 0.0024,
      "step": 116490
    },
    {
      "epoch": 6.213333333333333,
      "grad_norm": 0.21097607910633087,
      "learning_rate": 1.1166666666666668e-05,
      "loss": 0.0027,
      "step": 116500
    },
    {
      "epoch": 6.213866666666667,
      "grad_norm": 0.17694510519504547,
      "learning_rate": 1.1163333333333334e-05,
      "loss": 0.0014,
      "step": 116510
    },
    {
      "epoch": 6.2144,
      "grad_norm": 0.05374079570174217,
      "learning_rate": 1.1160000000000002e-05,
      "loss": 0.0024,
      "step": 116520
    },
    {
      "epoch": 6.214933333333334,
      "grad_norm": 0.21179425716400146,
      "learning_rate": 1.1156666666666666e-05,
      "loss": 0.0016,
      "step": 116530
    },
    {
      "epoch": 6.215466666666667,
      "grad_norm": 0.04117986187338829,
      "learning_rate": 1.1153333333333334e-05,
      "loss": 0.0018,
      "step": 116540
    },
    {
      "epoch": 6.216,
      "grad_norm": 0.7128406167030334,
      "learning_rate": 1.115e-05,
      "loss": 0.0026,
      "step": 116550
    },
    {
      "epoch": 6.2165333333333335,
      "grad_norm": 0.08744905889034271,
      "learning_rate": 1.1146666666666668e-05,
      "loss": 0.0017,
      "step": 116560
    },
    {
      "epoch": 6.217066666666667,
      "grad_norm": 0.03652955964207649,
      "learning_rate": 1.1143333333333334e-05,
      "loss": 0.002,
      "step": 116570
    },
    {
      "epoch": 6.2176,
      "grad_norm": 0.09144268929958344,
      "learning_rate": 1.114e-05,
      "loss": 0.0013,
      "step": 116580
    },
    {
      "epoch": 6.218133333333333,
      "grad_norm": 0.3256010115146637,
      "learning_rate": 1.1136666666666668e-05,
      "loss": 0.0015,
      "step": 116590
    },
    {
      "epoch": 6.218666666666667,
      "grad_norm": 0.5217430591583252,
      "learning_rate": 1.1133333333333334e-05,
      "loss": 0.0015,
      "step": 116600
    },
    {
      "epoch": 6.2192,
      "grad_norm": 0.0477379634976387,
      "learning_rate": 1.113e-05,
      "loss": 0.0012,
      "step": 116610
    },
    {
      "epoch": 6.219733333333333,
      "grad_norm": 0.032606177031993866,
      "learning_rate": 1.1126666666666668e-05,
      "loss": 0.0018,
      "step": 116620
    },
    {
      "epoch": 6.220266666666666,
      "grad_norm": 0.07774936407804489,
      "learning_rate": 1.1123333333333334e-05,
      "loss": 0.0016,
      "step": 116630
    },
    {
      "epoch": 6.2208,
      "grad_norm": 0.1712931990623474,
      "learning_rate": 1.112e-05,
      "loss": 0.0023,
      "step": 116640
    },
    {
      "epoch": 6.221333333333333,
      "grad_norm": 0.15307262539863586,
      "learning_rate": 1.1116666666666666e-05,
      "loss": 0.0017,
      "step": 116650
    },
    {
      "epoch": 6.221866666666667,
      "grad_norm": 0.26297077536582947,
      "learning_rate": 1.1113333333333334e-05,
      "loss": 0.0023,
      "step": 116660
    },
    {
      "epoch": 6.2224,
      "grad_norm": 0.14368723332881927,
      "learning_rate": 1.111e-05,
      "loss": 0.0022,
      "step": 116670
    },
    {
      "epoch": 6.222933333333334,
      "grad_norm": 0.03899122774600983,
      "learning_rate": 1.1106666666666666e-05,
      "loss": 0.0024,
      "step": 116680
    },
    {
      "epoch": 6.223466666666667,
      "grad_norm": 0.3189952075481415,
      "learning_rate": 1.1103333333333334e-05,
      "loss": 0.0013,
      "step": 116690
    },
    {
      "epoch": 6.224,
      "grad_norm": 0.1751575917005539,
      "learning_rate": 1.11e-05,
      "loss": 0.0014,
      "step": 116700
    },
    {
      "epoch": 6.2245333333333335,
      "grad_norm": 0.3305690586566925,
      "learning_rate": 1.1096666666666668e-05,
      "loss": 0.002,
      "step": 116710
    },
    {
      "epoch": 6.225066666666667,
      "grad_norm": 0.08594553917646408,
      "learning_rate": 1.1093333333333333e-05,
      "loss": 0.0031,
      "step": 116720
    },
    {
      "epoch": 6.2256,
      "grad_norm": 0.04442337527871132,
      "learning_rate": 1.109e-05,
      "loss": 0.0027,
      "step": 116730
    },
    {
      "epoch": 6.226133333333333,
      "grad_norm": 0.39855945110321045,
      "learning_rate": 1.1086666666666667e-05,
      "loss": 0.0026,
      "step": 116740
    },
    {
      "epoch": 6.226666666666667,
      "grad_norm": 0.11792883276939392,
      "learning_rate": 1.1083333333333335e-05,
      "loss": 0.0022,
      "step": 116750
    },
    {
      "epoch": 6.2272,
      "grad_norm": 0.14286188781261444,
      "learning_rate": 1.108e-05,
      "loss": 0.0026,
      "step": 116760
    },
    {
      "epoch": 6.227733333333333,
      "grad_norm": 0.4368179738521576,
      "learning_rate": 1.1076666666666667e-05,
      "loss": 0.0017,
      "step": 116770
    },
    {
      "epoch": 6.228266666666666,
      "grad_norm": 0.24246378242969513,
      "learning_rate": 1.1073333333333335e-05,
      "loss": 0.0019,
      "step": 116780
    },
    {
      "epoch": 6.2288,
      "grad_norm": 0.37391021847724915,
      "learning_rate": 1.107e-05,
      "loss": 0.0016,
      "step": 116790
    },
    {
      "epoch": 6.229333333333333,
      "grad_norm": 0.08655130863189697,
      "learning_rate": 1.1066666666666667e-05,
      "loss": 0.0017,
      "step": 116800
    },
    {
      "epoch": 6.229866666666666,
      "grad_norm": 0.39990848302841187,
      "learning_rate": 1.1063333333333335e-05,
      "loss": 0.0013,
      "step": 116810
    },
    {
      "epoch": 6.2304,
      "grad_norm": 0.2032080590724945,
      "learning_rate": 1.106e-05,
      "loss": 0.0017,
      "step": 116820
    },
    {
      "epoch": 6.230933333333334,
      "grad_norm": 0.16987860202789307,
      "learning_rate": 1.1056666666666667e-05,
      "loss": 0.0018,
      "step": 116830
    },
    {
      "epoch": 6.231466666666667,
      "grad_norm": 0.15044328570365906,
      "learning_rate": 1.1053333333333333e-05,
      "loss": 0.0032,
      "step": 116840
    },
    {
      "epoch": 6.232,
      "grad_norm": 0.2435625195503235,
      "learning_rate": 1.1050000000000001e-05,
      "loss": 0.0015,
      "step": 116850
    },
    {
      "epoch": 6.2325333333333335,
      "grad_norm": 0.11666350066661835,
      "learning_rate": 1.1046666666666667e-05,
      "loss": 0.0016,
      "step": 116860
    },
    {
      "epoch": 6.233066666666667,
      "grad_norm": 0.28402042388916016,
      "learning_rate": 1.1043333333333333e-05,
      "loss": 0.0019,
      "step": 116870
    },
    {
      "epoch": 6.2336,
      "grad_norm": 0.15561382472515106,
      "learning_rate": 1.1040000000000001e-05,
      "loss": 0.0023,
      "step": 116880
    },
    {
      "epoch": 6.234133333333333,
      "grad_norm": 0.04297768324613571,
      "learning_rate": 1.1036666666666667e-05,
      "loss": 0.0031,
      "step": 116890
    },
    {
      "epoch": 6.234666666666667,
      "grad_norm": 0.03658141940832138,
      "learning_rate": 1.1033333333333335e-05,
      "loss": 0.002,
      "step": 116900
    },
    {
      "epoch": 6.2352,
      "grad_norm": 0.06841418147087097,
      "learning_rate": 1.103e-05,
      "loss": 0.0018,
      "step": 116910
    },
    {
      "epoch": 6.235733333333333,
      "grad_norm": 0.14311283826828003,
      "learning_rate": 1.1026666666666667e-05,
      "loss": 0.0015,
      "step": 116920
    },
    {
      "epoch": 6.236266666666666,
      "grad_norm": 0.37776607275009155,
      "learning_rate": 1.1023333333333333e-05,
      "loss": 0.0018,
      "step": 116930
    },
    {
      "epoch": 6.2368,
      "grad_norm": 0.2564496397972107,
      "learning_rate": 1.1020000000000001e-05,
      "loss": 0.0029,
      "step": 116940
    },
    {
      "epoch": 6.237333333333333,
      "grad_norm": 0.14415273070335388,
      "learning_rate": 1.1016666666666667e-05,
      "loss": 0.0014,
      "step": 116950
    },
    {
      "epoch": 6.237866666666667,
      "grad_norm": 0.09542187303304672,
      "learning_rate": 1.1013333333333333e-05,
      "loss": 0.0016,
      "step": 116960
    },
    {
      "epoch": 6.2384,
      "grad_norm": 0.29099422693252563,
      "learning_rate": 1.1010000000000001e-05,
      "loss": 0.0014,
      "step": 116970
    },
    {
      "epoch": 6.238933333333334,
      "grad_norm": 0.3994532525539398,
      "learning_rate": 1.1006666666666666e-05,
      "loss": 0.0015,
      "step": 116980
    },
    {
      "epoch": 6.239466666666667,
      "grad_norm": 0.04838183522224426,
      "learning_rate": 1.1003333333333334e-05,
      "loss": 0.0019,
      "step": 116990
    },
    {
      "epoch": 6.24,
      "grad_norm": 0.3902249038219452,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.0018,
      "step": 117000
    },
    {
      "epoch": 6.2405333333333335,
      "grad_norm": 0.28362059593200684,
      "learning_rate": 1.0996666666666668e-05,
      "loss": 0.0027,
      "step": 117010
    },
    {
      "epoch": 6.241066666666667,
      "grad_norm": 0.0940488874912262,
      "learning_rate": 1.0993333333333334e-05,
      "loss": 0.0012,
      "step": 117020
    },
    {
      "epoch": 6.2416,
      "grad_norm": 0.10694538801908493,
      "learning_rate": 1.099e-05,
      "loss": 0.0015,
      "step": 117030
    },
    {
      "epoch": 6.242133333333333,
      "grad_norm": 0.3372518718242645,
      "learning_rate": 1.0986666666666668e-05,
      "loss": 0.0015,
      "step": 117040
    },
    {
      "epoch": 6.242666666666667,
      "grad_norm": 0.1278267800807953,
      "learning_rate": 1.0983333333333334e-05,
      "loss": 0.0026,
      "step": 117050
    },
    {
      "epoch": 6.2432,
      "grad_norm": 0.0492674894630909,
      "learning_rate": 1.098e-05,
      "loss": 0.0013,
      "step": 117060
    },
    {
      "epoch": 6.243733333333333,
      "grad_norm": 0.04049406945705414,
      "learning_rate": 1.0976666666666668e-05,
      "loss": 0.0016,
      "step": 117070
    },
    {
      "epoch": 6.244266666666666,
      "grad_norm": 0.32058870792388916,
      "learning_rate": 1.0973333333333334e-05,
      "loss": 0.0018,
      "step": 117080
    },
    {
      "epoch": 6.2448,
      "grad_norm": 0.26519501209259033,
      "learning_rate": 1.0970000000000002e-05,
      "loss": 0.0019,
      "step": 117090
    },
    {
      "epoch": 6.245333333333333,
      "grad_norm": 0.5145277380943298,
      "learning_rate": 1.0966666666666666e-05,
      "loss": 0.0014,
      "step": 117100
    },
    {
      "epoch": 6.245866666666666,
      "grad_norm": 0.060529038310050964,
      "learning_rate": 1.0963333333333334e-05,
      "loss": 0.0015,
      "step": 117110
    },
    {
      "epoch": 6.2464,
      "grad_norm": 0.06415998190641403,
      "learning_rate": 1.096e-05,
      "loss": 0.0024,
      "step": 117120
    },
    {
      "epoch": 6.246933333333334,
      "grad_norm": 0.06611443310976028,
      "learning_rate": 1.0956666666666668e-05,
      "loss": 0.002,
      "step": 117130
    },
    {
      "epoch": 6.247466666666667,
      "grad_norm": 0.028631040826439857,
      "learning_rate": 1.0953333333333334e-05,
      "loss": 0.0014,
      "step": 117140
    },
    {
      "epoch": 6.248,
      "grad_norm": 0.1749502271413803,
      "learning_rate": 1.095e-05,
      "loss": 0.0014,
      "step": 117150
    },
    {
      "epoch": 6.2485333333333335,
      "grad_norm": 0.2755254805088043,
      "learning_rate": 1.0946666666666668e-05,
      "loss": 0.0015,
      "step": 117160
    },
    {
      "epoch": 6.249066666666667,
      "grad_norm": 0.09340693056583405,
      "learning_rate": 1.0943333333333332e-05,
      "loss": 0.0021,
      "step": 117170
    },
    {
      "epoch": 6.2496,
      "grad_norm": 0.22848263382911682,
      "learning_rate": 1.094e-05,
      "loss": 0.0016,
      "step": 117180
    },
    {
      "epoch": 6.250133333333333,
      "grad_norm": 0.09560888260602951,
      "learning_rate": 1.0936666666666668e-05,
      "loss": 0.0016,
      "step": 117190
    },
    {
      "epoch": 6.250666666666667,
      "grad_norm": 0.18693719804286957,
      "learning_rate": 1.0933333333333334e-05,
      "loss": 0.0017,
      "step": 117200
    },
    {
      "epoch": 6.2512,
      "grad_norm": 0.02202233299612999,
      "learning_rate": 1.093e-05,
      "loss": 0.0018,
      "step": 117210
    },
    {
      "epoch": 6.251733333333333,
      "grad_norm": 0.10122202336788177,
      "learning_rate": 1.0926666666666667e-05,
      "loss": 0.0014,
      "step": 117220
    },
    {
      "epoch": 6.252266666666666,
      "grad_norm": 0.07097209244966507,
      "learning_rate": 1.0923333333333334e-05,
      "loss": 0.0013,
      "step": 117230
    },
    {
      "epoch": 6.2528,
      "grad_norm": 0.09079999476671219,
      "learning_rate": 1.092e-05,
      "loss": 0.0013,
      "step": 117240
    },
    {
      "epoch": 6.253333333333333,
      "grad_norm": 0.14507098495960236,
      "learning_rate": 1.0916666666666667e-05,
      "loss": 0.0016,
      "step": 117250
    },
    {
      "epoch": 6.253866666666667,
      "grad_norm": 0.11461393535137177,
      "learning_rate": 1.0913333333333334e-05,
      "loss": 0.0014,
      "step": 117260
    },
    {
      "epoch": 6.2544,
      "grad_norm": 0.13434892892837524,
      "learning_rate": 1.091e-05,
      "loss": 0.0016,
      "step": 117270
    },
    {
      "epoch": 6.254933333333334,
      "grad_norm": 0.07273143529891968,
      "learning_rate": 1.0906666666666668e-05,
      "loss": 0.0015,
      "step": 117280
    },
    {
      "epoch": 6.255466666666667,
      "grad_norm": 0.06515183299779892,
      "learning_rate": 1.0903333333333333e-05,
      "loss": 0.0015,
      "step": 117290
    },
    {
      "epoch": 6.256,
      "grad_norm": 0.23637507855892181,
      "learning_rate": 1.09e-05,
      "loss": 0.0021,
      "step": 117300
    },
    {
      "epoch": 6.2565333333333335,
      "grad_norm": 0.08663623780012131,
      "learning_rate": 1.0896666666666667e-05,
      "loss": 0.0018,
      "step": 117310
    },
    {
      "epoch": 6.257066666666667,
      "grad_norm": 0.09212008118629456,
      "learning_rate": 1.0893333333333333e-05,
      "loss": 0.0024,
      "step": 117320
    },
    {
      "epoch": 6.2576,
      "grad_norm": 0.20035533607006073,
      "learning_rate": 1.089e-05,
      "loss": 0.0021,
      "step": 117330
    },
    {
      "epoch": 6.258133333333333,
      "grad_norm": 0.03972136601805687,
      "learning_rate": 1.0886666666666667e-05,
      "loss": 0.0015,
      "step": 117340
    },
    {
      "epoch": 6.258666666666667,
      "grad_norm": 0.26110637187957764,
      "learning_rate": 1.0883333333333335e-05,
      "loss": 0.0022,
      "step": 117350
    },
    {
      "epoch": 6.2592,
      "grad_norm": 0.19733792543411255,
      "learning_rate": 1.088e-05,
      "loss": 0.0014,
      "step": 117360
    },
    {
      "epoch": 6.259733333333333,
      "grad_norm": 0.1423126757144928,
      "learning_rate": 1.0876666666666667e-05,
      "loss": 0.0015,
      "step": 117370
    },
    {
      "epoch": 6.260266666666666,
      "grad_norm": 0.14678622782230377,
      "learning_rate": 1.0873333333333335e-05,
      "loss": 0.0014,
      "step": 117380
    },
    {
      "epoch": 6.2608,
      "grad_norm": 0.039975784718990326,
      "learning_rate": 1.0870000000000001e-05,
      "loss": 0.0016,
      "step": 117390
    },
    {
      "epoch": 6.261333333333333,
      "grad_norm": 0.29078418016433716,
      "learning_rate": 1.0866666666666667e-05,
      "loss": 0.002,
      "step": 117400
    },
    {
      "epoch": 6.261866666666666,
      "grad_norm": 0.3470323085784912,
      "learning_rate": 1.0863333333333333e-05,
      "loss": 0.0015,
      "step": 117410
    },
    {
      "epoch": 6.2624,
      "grad_norm": 0.036004748195409775,
      "learning_rate": 1.0860000000000001e-05,
      "loss": 0.0013,
      "step": 117420
    },
    {
      "epoch": 6.262933333333334,
      "grad_norm": 0.04023423790931702,
      "learning_rate": 1.0856666666666667e-05,
      "loss": 0.0018,
      "step": 117430
    },
    {
      "epoch": 6.263466666666667,
      "grad_norm": 0.11686502397060394,
      "learning_rate": 1.0853333333333333e-05,
      "loss": 0.002,
      "step": 117440
    },
    {
      "epoch": 6.264,
      "grad_norm": 0.3971055746078491,
      "learning_rate": 1.0850000000000001e-05,
      "loss": 0.0015,
      "step": 117450
    },
    {
      "epoch": 6.2645333333333335,
      "grad_norm": 0.23021423816680908,
      "learning_rate": 1.0846666666666667e-05,
      "loss": 0.002,
      "step": 117460
    },
    {
      "epoch": 6.265066666666667,
      "grad_norm": 0.04187507554888725,
      "learning_rate": 1.0843333333333335e-05,
      "loss": 0.0014,
      "step": 117470
    },
    {
      "epoch": 6.2656,
      "grad_norm": 0.1735798716545105,
      "learning_rate": 1.084e-05,
      "loss": 0.0015,
      "step": 117480
    },
    {
      "epoch": 6.266133333333333,
      "grad_norm": 0.088164322078228,
      "learning_rate": 1.0836666666666667e-05,
      "loss": 0.0029,
      "step": 117490
    },
    {
      "epoch": 6.266666666666667,
      "grad_norm": 0.4001467823982239,
      "learning_rate": 1.0833333333333334e-05,
      "loss": 0.0015,
      "step": 117500
    },
    {
      "epoch": 6.2672,
      "grad_norm": 0.06998582929372787,
      "learning_rate": 1.083e-05,
      "loss": 0.002,
      "step": 117510
    },
    {
      "epoch": 6.267733333333333,
      "grad_norm": 0.22687920928001404,
      "learning_rate": 1.0826666666666667e-05,
      "loss": 0.0021,
      "step": 117520
    },
    {
      "epoch": 6.268266666666666,
      "grad_norm": 0.14991094172000885,
      "learning_rate": 1.0823333333333334e-05,
      "loss": 0.0015,
      "step": 117530
    },
    {
      "epoch": 6.2688,
      "grad_norm": 0.2558785676956177,
      "learning_rate": 1.0820000000000001e-05,
      "loss": 0.0015,
      "step": 117540
    },
    {
      "epoch": 6.269333333333333,
      "grad_norm": 0.23372316360473633,
      "learning_rate": 1.0816666666666666e-05,
      "loss": 0.0016,
      "step": 117550
    },
    {
      "epoch": 6.269866666666666,
      "grad_norm": 0.03744062781333923,
      "learning_rate": 1.0813333333333334e-05,
      "loss": 0.0035,
      "step": 117560
    },
    {
      "epoch": 6.2704,
      "grad_norm": 0.2886066734790802,
      "learning_rate": 1.081e-05,
      "loss": 0.0018,
      "step": 117570
    },
    {
      "epoch": 6.270933333333334,
      "grad_norm": 0.1630248725414276,
      "learning_rate": 1.0806666666666668e-05,
      "loss": 0.0016,
      "step": 117580
    },
    {
      "epoch": 6.271466666666667,
      "grad_norm": 0.28154876828193665,
      "learning_rate": 1.0803333333333334e-05,
      "loss": 0.0016,
      "step": 117590
    },
    {
      "epoch": 6.272,
      "grad_norm": 0.05942215025424957,
      "learning_rate": 1.08e-05,
      "loss": 0.0016,
      "step": 117600
    },
    {
      "epoch": 6.2725333333333335,
      "grad_norm": 0.21151281893253326,
      "learning_rate": 1.0796666666666668e-05,
      "loss": 0.0012,
      "step": 117610
    },
    {
      "epoch": 6.273066666666667,
      "grad_norm": 0.23024140298366547,
      "learning_rate": 1.0793333333333334e-05,
      "loss": 0.0012,
      "step": 117620
    },
    {
      "epoch": 6.2736,
      "grad_norm": 0.2769118547439575,
      "learning_rate": 1.079e-05,
      "loss": 0.0027,
      "step": 117630
    },
    {
      "epoch": 6.274133333333333,
      "grad_norm": 0.13997626304626465,
      "learning_rate": 1.0786666666666668e-05,
      "loss": 0.0026,
      "step": 117640
    },
    {
      "epoch": 6.274666666666667,
      "grad_norm": 0.12087913602590561,
      "learning_rate": 1.0783333333333334e-05,
      "loss": 0.0024,
      "step": 117650
    },
    {
      "epoch": 6.2752,
      "grad_norm": 0.2300451248884201,
      "learning_rate": 1.0780000000000002e-05,
      "loss": 0.0017,
      "step": 117660
    },
    {
      "epoch": 6.275733333333333,
      "grad_norm": 0.18023401498794556,
      "learning_rate": 1.0776666666666666e-05,
      "loss": 0.0026,
      "step": 117670
    },
    {
      "epoch": 6.276266666666666,
      "grad_norm": 0.35122743248939514,
      "learning_rate": 1.0773333333333334e-05,
      "loss": 0.002,
      "step": 117680
    },
    {
      "epoch": 6.2768,
      "grad_norm": 0.1055641770362854,
      "learning_rate": 1.077e-05,
      "loss": 0.0018,
      "step": 117690
    },
    {
      "epoch": 6.277333333333333,
      "grad_norm": 0.09353107213973999,
      "learning_rate": 1.0766666666666666e-05,
      "loss": 0.0019,
      "step": 117700
    },
    {
      "epoch": 6.277866666666666,
      "grad_norm": 0.515518844127655,
      "learning_rate": 1.0763333333333334e-05,
      "loss": 0.0014,
      "step": 117710
    },
    {
      "epoch": 6.2783999999999995,
      "grad_norm": 0.052823249250650406,
      "learning_rate": 1.076e-05,
      "loss": 0.0019,
      "step": 117720
    },
    {
      "epoch": 6.278933333333334,
      "grad_norm": 0.29552677273750305,
      "learning_rate": 1.0756666666666668e-05,
      "loss": 0.0025,
      "step": 117730
    },
    {
      "epoch": 6.279466666666667,
      "grad_norm": 0.14401446282863617,
      "learning_rate": 1.0753333333333333e-05,
      "loss": 0.0015,
      "step": 117740
    },
    {
      "epoch": 6.28,
      "grad_norm": 0.2852049469947815,
      "learning_rate": 1.075e-05,
      "loss": 0.002,
      "step": 117750
    },
    {
      "epoch": 6.2805333333333335,
      "grad_norm": 0.47901463508605957,
      "learning_rate": 1.0746666666666667e-05,
      "loss": 0.0021,
      "step": 117760
    },
    {
      "epoch": 6.281066666666667,
      "grad_norm": 0.21163880825042725,
      "learning_rate": 1.0743333333333334e-05,
      "loss": 0.0024,
      "step": 117770
    },
    {
      "epoch": 6.2816,
      "grad_norm": 0.5030938982963562,
      "learning_rate": 1.074e-05,
      "loss": 0.0019,
      "step": 117780
    },
    {
      "epoch": 6.282133333333333,
      "grad_norm": 0.41492128372192383,
      "learning_rate": 1.0736666666666667e-05,
      "loss": 0.0025,
      "step": 117790
    },
    {
      "epoch": 6.282666666666667,
      "grad_norm": 0.22763706743717194,
      "learning_rate": 1.0733333333333334e-05,
      "loss": 0.0023,
      "step": 117800
    },
    {
      "epoch": 6.2832,
      "grad_norm": 0.1435902714729309,
      "learning_rate": 1.073e-05,
      "loss": 0.0014,
      "step": 117810
    },
    {
      "epoch": 6.283733333333333,
      "grad_norm": 0.37530767917633057,
      "learning_rate": 1.0726666666666667e-05,
      "loss": 0.0021,
      "step": 117820
    },
    {
      "epoch": 6.2842666666666664,
      "grad_norm": 0.09597749263048172,
      "learning_rate": 1.0723333333333335e-05,
      "loss": 0.0025,
      "step": 117830
    },
    {
      "epoch": 6.2848,
      "grad_norm": 0.2298891693353653,
      "learning_rate": 1.072e-05,
      "loss": 0.0019,
      "step": 117840
    },
    {
      "epoch": 6.285333333333333,
      "grad_norm": 0.05947992205619812,
      "learning_rate": 1.0716666666666667e-05,
      "loss": 0.0017,
      "step": 117850
    },
    {
      "epoch": 6.285866666666666,
      "grad_norm": 0.1144118458032608,
      "learning_rate": 1.0713333333333333e-05,
      "loss": 0.0013,
      "step": 117860
    },
    {
      "epoch": 6.2864,
      "grad_norm": 0.2336888313293457,
      "learning_rate": 1.071e-05,
      "loss": 0.0015,
      "step": 117870
    },
    {
      "epoch": 6.286933333333334,
      "grad_norm": 0.049469999969005585,
      "learning_rate": 1.0706666666666667e-05,
      "loss": 0.0018,
      "step": 117880
    },
    {
      "epoch": 6.287466666666667,
      "grad_norm": 0.25818848609924316,
      "learning_rate": 1.0703333333333333e-05,
      "loss": 0.0025,
      "step": 117890
    },
    {
      "epoch": 6.288,
      "grad_norm": 0.08825388550758362,
      "learning_rate": 1.0700000000000001e-05,
      "loss": 0.002,
      "step": 117900
    },
    {
      "epoch": 6.2885333333333335,
      "grad_norm": 0.18531657755374908,
      "learning_rate": 1.0696666666666667e-05,
      "loss": 0.0022,
      "step": 117910
    },
    {
      "epoch": 6.289066666666667,
      "grad_norm": 0.3715888559818268,
      "learning_rate": 1.0693333333333335e-05,
      "loss": 0.002,
      "step": 117920
    },
    {
      "epoch": 6.2896,
      "grad_norm": 0.20917649567127228,
      "learning_rate": 1.069e-05,
      "loss": 0.0013,
      "step": 117930
    },
    {
      "epoch": 6.290133333333333,
      "grad_norm": 0.17486600577831268,
      "learning_rate": 1.0686666666666667e-05,
      "loss": 0.0034,
      "step": 117940
    },
    {
      "epoch": 6.290666666666667,
      "grad_norm": 0.20530889928340912,
      "learning_rate": 1.0683333333333333e-05,
      "loss": 0.0021,
      "step": 117950
    },
    {
      "epoch": 6.2912,
      "grad_norm": 0.05290009453892708,
      "learning_rate": 1.0680000000000001e-05,
      "loss": 0.003,
      "step": 117960
    },
    {
      "epoch": 6.291733333333333,
      "grad_norm": 0.26366668939590454,
      "learning_rate": 1.0676666666666667e-05,
      "loss": 0.0016,
      "step": 117970
    },
    {
      "epoch": 6.2922666666666665,
      "grad_norm": 0.1738107055425644,
      "learning_rate": 1.0673333333333333e-05,
      "loss": 0.0018,
      "step": 117980
    },
    {
      "epoch": 6.2928,
      "grad_norm": 0.4072471559047699,
      "learning_rate": 1.0670000000000001e-05,
      "loss": 0.0019,
      "step": 117990
    },
    {
      "epoch": 6.293333333333333,
      "grad_norm": 0.11880122125148773,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 0.0026,
      "step": 118000
    },
    {
      "epoch": 6.293866666666666,
      "grad_norm": 0.09804808348417282,
      "learning_rate": 1.0663333333333333e-05,
      "loss": 0.0018,
      "step": 118010
    },
    {
      "epoch": 6.2943999999999996,
      "grad_norm": 0.17037105560302734,
      "learning_rate": 1.0660000000000001e-05,
      "loss": 0.001,
      "step": 118020
    },
    {
      "epoch": 6.294933333333334,
      "grad_norm": 0.11207660287618637,
      "learning_rate": 1.0656666666666667e-05,
      "loss": 0.0022,
      "step": 118030
    },
    {
      "epoch": 6.295466666666667,
      "grad_norm": 0.14455264806747437,
      "learning_rate": 1.0653333333333334e-05,
      "loss": 0.0012,
      "step": 118040
    },
    {
      "epoch": 6.296,
      "grad_norm": 0.1168108880519867,
      "learning_rate": 1.065e-05,
      "loss": 0.0013,
      "step": 118050
    },
    {
      "epoch": 6.2965333333333335,
      "grad_norm": 0.09696608036756516,
      "learning_rate": 1.0646666666666668e-05,
      "loss": 0.0014,
      "step": 118060
    },
    {
      "epoch": 6.297066666666667,
      "grad_norm": 0.18031160533428192,
      "learning_rate": 1.0643333333333334e-05,
      "loss": 0.0016,
      "step": 118070
    },
    {
      "epoch": 6.2976,
      "grad_norm": 0.3911406397819519,
      "learning_rate": 1.064e-05,
      "loss": 0.0015,
      "step": 118080
    },
    {
      "epoch": 6.298133333333333,
      "grad_norm": 0.06250234693288803,
      "learning_rate": 1.0636666666666668e-05,
      "loss": 0.0022,
      "step": 118090
    },
    {
      "epoch": 6.298666666666667,
      "grad_norm": 0.09240543097257614,
      "learning_rate": 1.0633333333333334e-05,
      "loss": 0.0016,
      "step": 118100
    },
    {
      "epoch": 6.2992,
      "grad_norm": 0.14847344160079956,
      "learning_rate": 1.0630000000000002e-05,
      "loss": 0.0015,
      "step": 118110
    },
    {
      "epoch": 6.299733333333333,
      "grad_norm": 0.06766650080680847,
      "learning_rate": 1.0626666666666666e-05,
      "loss": 0.0023,
      "step": 118120
    },
    {
      "epoch": 6.3002666666666665,
      "grad_norm": 0.12016766518354416,
      "learning_rate": 1.0623333333333334e-05,
      "loss": 0.0013,
      "step": 118130
    },
    {
      "epoch": 6.3008,
      "grad_norm": 0.20516881346702576,
      "learning_rate": 1.062e-05,
      "loss": 0.003,
      "step": 118140
    },
    {
      "epoch": 6.301333333333333,
      "grad_norm": 0.19423359632492065,
      "learning_rate": 1.0616666666666668e-05,
      "loss": 0.0025,
      "step": 118150
    },
    {
      "epoch": 6.301866666666666,
      "grad_norm": 0.08138149231672287,
      "learning_rate": 1.0613333333333334e-05,
      "loss": 0.0016,
      "step": 118160
    },
    {
      "epoch": 6.3024000000000004,
      "grad_norm": 0.5413854122161865,
      "learning_rate": 1.061e-05,
      "loss": 0.0014,
      "step": 118170
    },
    {
      "epoch": 6.302933333333334,
      "grad_norm": 0.08373292535543442,
      "learning_rate": 1.0606666666666668e-05,
      "loss": 0.0022,
      "step": 118180
    },
    {
      "epoch": 6.303466666666667,
      "grad_norm": 0.3165455758571625,
      "learning_rate": 1.0603333333333332e-05,
      "loss": 0.0012,
      "step": 118190
    },
    {
      "epoch": 6.304,
      "grad_norm": 0.3174819350242615,
      "learning_rate": 1.06e-05,
      "loss": 0.0022,
      "step": 118200
    },
    {
      "epoch": 6.3045333333333335,
      "grad_norm": 0.0247043389827013,
      "learning_rate": 1.0596666666666668e-05,
      "loss": 0.0018,
      "step": 118210
    },
    {
      "epoch": 6.305066666666667,
      "grad_norm": 0.06282144784927368,
      "learning_rate": 1.0593333333333334e-05,
      "loss": 0.0015,
      "step": 118220
    },
    {
      "epoch": 6.3056,
      "grad_norm": 0.054065167903900146,
      "learning_rate": 1.059e-05,
      "loss": 0.0017,
      "step": 118230
    },
    {
      "epoch": 6.306133333333333,
      "grad_norm": 0.4330117404460907,
      "learning_rate": 1.0586666666666666e-05,
      "loss": 0.0018,
      "step": 118240
    },
    {
      "epoch": 6.306666666666667,
      "grad_norm": 0.15247352421283722,
      "learning_rate": 1.0583333333333334e-05,
      "loss": 0.0019,
      "step": 118250
    },
    {
      "epoch": 6.3072,
      "grad_norm": 0.1009816974401474,
      "learning_rate": 1.058e-05,
      "loss": 0.0016,
      "step": 118260
    },
    {
      "epoch": 6.307733333333333,
      "grad_norm": 0.043974027037620544,
      "learning_rate": 1.0576666666666666e-05,
      "loss": 0.0024,
      "step": 118270
    },
    {
      "epoch": 6.3082666666666665,
      "grad_norm": 0.040613893419504166,
      "learning_rate": 1.0573333333333334e-05,
      "loss": 0.0032,
      "step": 118280
    },
    {
      "epoch": 6.3088,
      "grad_norm": 0.06263382732868195,
      "learning_rate": 1.057e-05,
      "loss": 0.0023,
      "step": 118290
    },
    {
      "epoch": 6.309333333333333,
      "grad_norm": 0.07097544521093369,
      "learning_rate": 1.0566666666666668e-05,
      "loss": 0.0012,
      "step": 118300
    },
    {
      "epoch": 6.309866666666666,
      "grad_norm": 0.18223576247692108,
      "learning_rate": 1.0563333333333333e-05,
      "loss": 0.0014,
      "step": 118310
    },
    {
      "epoch": 6.3104,
      "grad_norm": 0.06941419094800949,
      "learning_rate": 1.056e-05,
      "loss": 0.0022,
      "step": 118320
    },
    {
      "epoch": 6.310933333333334,
      "grad_norm": 0.053878020495176315,
      "learning_rate": 1.0556666666666667e-05,
      "loss": 0.0014,
      "step": 118330
    },
    {
      "epoch": 6.311466666666667,
      "grad_norm": 0.09836886823177338,
      "learning_rate": 1.0553333333333335e-05,
      "loss": 0.0019,
      "step": 118340
    },
    {
      "epoch": 6.312,
      "grad_norm": 0.05112890899181366,
      "learning_rate": 1.055e-05,
      "loss": 0.0037,
      "step": 118350
    },
    {
      "epoch": 6.3125333333333336,
      "grad_norm": 0.06565797328948975,
      "learning_rate": 1.0546666666666667e-05,
      "loss": 0.0022,
      "step": 118360
    },
    {
      "epoch": 6.313066666666667,
      "grad_norm": 0.03860608488321304,
      "learning_rate": 1.0543333333333335e-05,
      "loss": 0.0012,
      "step": 118370
    },
    {
      "epoch": 6.3136,
      "grad_norm": 0.5183618068695068,
      "learning_rate": 1.0539999999999999e-05,
      "loss": 0.0018,
      "step": 118380
    },
    {
      "epoch": 6.314133333333333,
      "grad_norm": 0.2541859745979309,
      "learning_rate": 1.0536666666666667e-05,
      "loss": 0.0016,
      "step": 118390
    },
    {
      "epoch": 6.314666666666667,
      "grad_norm": 0.23129376769065857,
      "learning_rate": 1.0533333333333335e-05,
      "loss": 0.002,
      "step": 118400
    },
    {
      "epoch": 6.3152,
      "grad_norm": 0.17422324419021606,
      "learning_rate": 1.053e-05,
      "loss": 0.0014,
      "step": 118410
    },
    {
      "epoch": 6.315733333333333,
      "grad_norm": 0.3420461416244507,
      "learning_rate": 1.0526666666666667e-05,
      "loss": 0.0019,
      "step": 118420
    },
    {
      "epoch": 6.3162666666666665,
      "grad_norm": 0.2917653024196625,
      "learning_rate": 1.0523333333333333e-05,
      "loss": 0.0012,
      "step": 118430
    },
    {
      "epoch": 6.3168,
      "grad_norm": 0.031316276639699936,
      "learning_rate": 1.0520000000000001e-05,
      "loss": 0.0016,
      "step": 118440
    },
    {
      "epoch": 6.317333333333333,
      "grad_norm": 0.2914888560771942,
      "learning_rate": 1.0516666666666667e-05,
      "loss": 0.0016,
      "step": 118450
    },
    {
      "epoch": 6.317866666666666,
      "grad_norm": 0.10996247082948685,
      "learning_rate": 1.0513333333333333e-05,
      "loss": 0.0015,
      "step": 118460
    },
    {
      "epoch": 6.3184000000000005,
      "grad_norm": 0.052066922187805176,
      "learning_rate": 1.0510000000000001e-05,
      "loss": 0.0019,
      "step": 118470
    },
    {
      "epoch": 6.318933333333334,
      "grad_norm": 0.14850443601608276,
      "learning_rate": 1.0506666666666667e-05,
      "loss": 0.0025,
      "step": 118480
    },
    {
      "epoch": 6.319466666666667,
      "grad_norm": 0.05598371848464012,
      "learning_rate": 1.0503333333333335e-05,
      "loss": 0.0019,
      "step": 118490
    },
    {
      "epoch": 6.32,
      "grad_norm": 0.31284257769584656,
      "learning_rate": 1.05e-05,
      "loss": 0.0022,
      "step": 118500
    },
    {
      "epoch": 6.320533333333334,
      "grad_norm": 0.08938425779342651,
      "learning_rate": 1.0496666666666667e-05,
      "loss": 0.0013,
      "step": 118510
    },
    {
      "epoch": 6.321066666666667,
      "grad_norm": 0.1738627552986145,
      "learning_rate": 1.0493333333333333e-05,
      "loss": 0.0022,
      "step": 118520
    },
    {
      "epoch": 6.3216,
      "grad_norm": 0.09690544754266739,
      "learning_rate": 1.049e-05,
      "loss": 0.0011,
      "step": 118530
    },
    {
      "epoch": 6.322133333333333,
      "grad_norm": 0.1451011747121811,
      "learning_rate": 1.0486666666666667e-05,
      "loss": 0.0017,
      "step": 118540
    },
    {
      "epoch": 6.322666666666667,
      "grad_norm": 0.07781580090522766,
      "learning_rate": 1.0483333333333333e-05,
      "loss": 0.0015,
      "step": 118550
    },
    {
      "epoch": 6.3232,
      "grad_norm": 0.10208291560411453,
      "learning_rate": 1.0480000000000001e-05,
      "loss": 0.0027,
      "step": 118560
    },
    {
      "epoch": 6.323733333333333,
      "grad_norm": 0.058823272585868835,
      "learning_rate": 1.0476666666666666e-05,
      "loss": 0.0018,
      "step": 118570
    },
    {
      "epoch": 6.3242666666666665,
      "grad_norm": 0.31381675601005554,
      "learning_rate": 1.0473333333333334e-05,
      "loss": 0.0023,
      "step": 118580
    },
    {
      "epoch": 6.3248,
      "grad_norm": 0.6007740497589111,
      "learning_rate": 1.0470000000000001e-05,
      "loss": 0.0017,
      "step": 118590
    },
    {
      "epoch": 6.325333333333333,
      "grad_norm": 0.20297133922576904,
      "learning_rate": 1.0466666666666668e-05,
      "loss": 0.0027,
      "step": 118600
    },
    {
      "epoch": 6.325866666666666,
      "grad_norm": 0.22979505360126495,
      "learning_rate": 1.0463333333333334e-05,
      "loss": 0.0027,
      "step": 118610
    },
    {
      "epoch": 6.3264,
      "grad_norm": 0.09268128871917725,
      "learning_rate": 1.046e-05,
      "loss": 0.0016,
      "step": 118620
    },
    {
      "epoch": 6.326933333333334,
      "grad_norm": 0.06985600292682648,
      "learning_rate": 1.0456666666666668e-05,
      "loss": 0.0026,
      "step": 118630
    },
    {
      "epoch": 6.327466666666667,
      "grad_norm": 0.19721731543540955,
      "learning_rate": 1.0453333333333334e-05,
      "loss": 0.0014,
      "step": 118640
    },
    {
      "epoch": 6.328,
      "grad_norm": 0.17818143963813782,
      "learning_rate": 1.045e-05,
      "loss": 0.0022,
      "step": 118650
    },
    {
      "epoch": 6.328533333333334,
      "grad_norm": 0.26906901597976685,
      "learning_rate": 1.0446666666666668e-05,
      "loss": 0.0014,
      "step": 118660
    },
    {
      "epoch": 6.329066666666667,
      "grad_norm": 0.45872873067855835,
      "learning_rate": 1.0443333333333334e-05,
      "loss": 0.003,
      "step": 118670
    },
    {
      "epoch": 6.3296,
      "grad_norm": 0.43829911947250366,
      "learning_rate": 1.0440000000000002e-05,
      "loss": 0.0021,
      "step": 118680
    },
    {
      "epoch": 6.330133333333333,
      "grad_norm": 0.20084723830223083,
      "learning_rate": 1.0436666666666666e-05,
      "loss": 0.0024,
      "step": 118690
    },
    {
      "epoch": 6.330666666666667,
      "grad_norm": 0.12018350511789322,
      "learning_rate": 1.0433333333333334e-05,
      "loss": 0.003,
      "step": 118700
    },
    {
      "epoch": 6.3312,
      "grad_norm": 0.04212045297026634,
      "learning_rate": 1.043e-05,
      "loss": 0.0017,
      "step": 118710
    },
    {
      "epoch": 6.331733333333333,
      "grad_norm": 0.15119971334934235,
      "learning_rate": 1.0426666666666666e-05,
      "loss": 0.0013,
      "step": 118720
    },
    {
      "epoch": 6.3322666666666665,
      "grad_norm": 0.14716647565364838,
      "learning_rate": 1.0423333333333334e-05,
      "loss": 0.0016,
      "step": 118730
    },
    {
      "epoch": 6.3328,
      "grad_norm": 0.0683099552989006,
      "learning_rate": 1.042e-05,
      "loss": 0.0015,
      "step": 118740
    },
    {
      "epoch": 6.333333333333333,
      "grad_norm": 0.48268213868141174,
      "learning_rate": 1.0416666666666668e-05,
      "loss": 0.0016,
      "step": 118750
    },
    {
      "epoch": 6.333866666666666,
      "grad_norm": 0.15582725405693054,
      "learning_rate": 1.0413333333333332e-05,
      "loss": 0.002,
      "step": 118760
    },
    {
      "epoch": 6.3344,
      "grad_norm": 0.1791328340768814,
      "learning_rate": 1.041e-05,
      "loss": 0.0015,
      "step": 118770
    },
    {
      "epoch": 6.334933333333334,
      "grad_norm": 0.06290454417467117,
      "learning_rate": 1.0406666666666668e-05,
      "loss": 0.0021,
      "step": 118780
    },
    {
      "epoch": 6.335466666666667,
      "grad_norm": 0.2854684591293335,
      "learning_rate": 1.0403333333333334e-05,
      "loss": 0.0026,
      "step": 118790
    },
    {
      "epoch": 6.336,
      "grad_norm": 0.39151084423065186,
      "learning_rate": 1.04e-05,
      "loss": 0.0015,
      "step": 118800
    },
    {
      "epoch": 6.336533333333334,
      "grad_norm": 0.25222906470298767,
      "learning_rate": 1.0396666666666667e-05,
      "loss": 0.002,
      "step": 118810
    },
    {
      "epoch": 6.337066666666667,
      "grad_norm": 0.09880851954221725,
      "learning_rate": 1.0393333333333334e-05,
      "loss": 0.0014,
      "step": 118820
    },
    {
      "epoch": 6.3376,
      "grad_norm": 0.14010393619537354,
      "learning_rate": 1.039e-05,
      "loss": 0.0023,
      "step": 118830
    },
    {
      "epoch": 6.338133333333333,
      "grad_norm": 0.2326529175043106,
      "learning_rate": 1.0386666666666667e-05,
      "loss": 0.002,
      "step": 118840
    },
    {
      "epoch": 6.338666666666667,
      "grad_norm": 0.18761074542999268,
      "learning_rate": 1.0383333333333334e-05,
      "loss": 0.0014,
      "step": 118850
    },
    {
      "epoch": 6.3392,
      "grad_norm": 0.045174408704042435,
      "learning_rate": 1.038e-05,
      "loss": 0.0019,
      "step": 118860
    },
    {
      "epoch": 6.339733333333333,
      "grad_norm": 0.5139142870903015,
      "learning_rate": 1.0376666666666667e-05,
      "loss": 0.0016,
      "step": 118870
    },
    {
      "epoch": 6.3402666666666665,
      "grad_norm": 0.06080666556954384,
      "learning_rate": 1.0373333333333333e-05,
      "loss": 0.0014,
      "step": 118880
    },
    {
      "epoch": 6.3408,
      "grad_norm": 0.03910648077726364,
      "learning_rate": 1.037e-05,
      "loss": 0.0017,
      "step": 118890
    },
    {
      "epoch": 6.341333333333333,
      "grad_norm": 0.317426472902298,
      "learning_rate": 1.0366666666666667e-05,
      "loss": 0.0019,
      "step": 118900
    },
    {
      "epoch": 6.341866666666666,
      "grad_norm": 0.206323504447937,
      "learning_rate": 1.0363333333333333e-05,
      "loss": 0.0019,
      "step": 118910
    },
    {
      "epoch": 6.3424,
      "grad_norm": 0.026056669652462006,
      "learning_rate": 1.036e-05,
      "loss": 0.0021,
      "step": 118920
    },
    {
      "epoch": 6.342933333333333,
      "grad_norm": 0.23147429525852203,
      "learning_rate": 1.0356666666666667e-05,
      "loss": 0.0024,
      "step": 118930
    },
    {
      "epoch": 6.343466666666667,
      "grad_norm": 0.26034727692604065,
      "learning_rate": 1.0353333333333335e-05,
      "loss": 0.0017,
      "step": 118940
    },
    {
      "epoch": 6.344,
      "grad_norm": 0.3432844281196594,
      "learning_rate": 1.035e-05,
      "loss": 0.0027,
      "step": 118950
    },
    {
      "epoch": 6.344533333333334,
      "grad_norm": 0.058875925838947296,
      "learning_rate": 1.0346666666666667e-05,
      "loss": 0.0014,
      "step": 118960
    },
    {
      "epoch": 6.345066666666667,
      "grad_norm": 0.06604969501495361,
      "learning_rate": 1.0343333333333335e-05,
      "loss": 0.0016,
      "step": 118970
    },
    {
      "epoch": 6.3456,
      "grad_norm": 0.4963395297527313,
      "learning_rate": 1.0340000000000001e-05,
      "loss": 0.0017,
      "step": 118980
    },
    {
      "epoch": 6.346133333333333,
      "grad_norm": 0.11400888860225677,
      "learning_rate": 1.0336666666666667e-05,
      "loss": 0.0016,
      "step": 118990
    },
    {
      "epoch": 6.346666666666667,
      "grad_norm": 0.08839599788188934,
      "learning_rate": 1.0333333333333333e-05,
      "loss": 0.0019,
      "step": 119000
    },
    {
      "epoch": 6.3472,
      "grad_norm": 0.08309199661016464,
      "learning_rate": 1.0330000000000001e-05,
      "loss": 0.0022,
      "step": 119010
    },
    {
      "epoch": 6.347733333333333,
      "grad_norm": 0.23251885175704956,
      "learning_rate": 1.0326666666666667e-05,
      "loss": 0.0012,
      "step": 119020
    },
    {
      "epoch": 6.3482666666666665,
      "grad_norm": 0.2037450522184372,
      "learning_rate": 1.0323333333333333e-05,
      "loss": 0.002,
      "step": 119030
    },
    {
      "epoch": 6.3488,
      "grad_norm": 0.21888549625873566,
      "learning_rate": 1.0320000000000001e-05,
      "loss": 0.0022,
      "step": 119040
    },
    {
      "epoch": 6.349333333333333,
      "grad_norm": 0.2951662838459015,
      "learning_rate": 1.0316666666666667e-05,
      "loss": 0.0016,
      "step": 119050
    },
    {
      "epoch": 6.349866666666666,
      "grad_norm": 0.3742887079715729,
      "learning_rate": 1.0313333333333333e-05,
      "loss": 0.002,
      "step": 119060
    },
    {
      "epoch": 6.3504,
      "grad_norm": 0.4435988664627075,
      "learning_rate": 1.031e-05,
      "loss": 0.0015,
      "step": 119070
    },
    {
      "epoch": 6.350933333333334,
      "grad_norm": 0.23396684229373932,
      "learning_rate": 1.0306666666666667e-05,
      "loss": 0.0015,
      "step": 119080
    },
    {
      "epoch": 6.351466666666667,
      "grad_norm": 0.2569122910499573,
      "learning_rate": 1.0303333333333334e-05,
      "loss": 0.0016,
      "step": 119090
    },
    {
      "epoch": 6.352,
      "grad_norm": 0.230562224984169,
      "learning_rate": 1.03e-05,
      "loss": 0.0015,
      "step": 119100
    },
    {
      "epoch": 6.352533333333334,
      "grad_norm": 0.2323017716407776,
      "learning_rate": 1.0296666666666667e-05,
      "loss": 0.0015,
      "step": 119110
    },
    {
      "epoch": 6.353066666666667,
      "grad_norm": 0.04711991548538208,
      "learning_rate": 1.0293333333333334e-05,
      "loss": 0.0014,
      "step": 119120
    },
    {
      "epoch": 6.3536,
      "grad_norm": 0.20163121819496155,
      "learning_rate": 1.0290000000000001e-05,
      "loss": 0.0019,
      "step": 119130
    },
    {
      "epoch": 6.354133333333333,
      "grad_norm": 0.3466254472732544,
      "learning_rate": 1.0286666666666666e-05,
      "loss": 0.002,
      "step": 119140
    },
    {
      "epoch": 6.354666666666667,
      "grad_norm": 0.05119982734322548,
      "learning_rate": 1.0283333333333334e-05,
      "loss": 0.0026,
      "step": 119150
    },
    {
      "epoch": 6.3552,
      "grad_norm": 0.12680689990520477,
      "learning_rate": 1.0280000000000002e-05,
      "loss": 0.0017,
      "step": 119160
    },
    {
      "epoch": 6.355733333333333,
      "grad_norm": 0.22857558727264404,
      "learning_rate": 1.0276666666666668e-05,
      "loss": 0.002,
      "step": 119170
    },
    {
      "epoch": 6.3562666666666665,
      "grad_norm": 0.3769339323043823,
      "learning_rate": 1.0273333333333334e-05,
      "loss": 0.0018,
      "step": 119180
    },
    {
      "epoch": 6.3568,
      "grad_norm": 0.25717365741729736,
      "learning_rate": 1.027e-05,
      "loss": 0.0017,
      "step": 119190
    },
    {
      "epoch": 6.357333333333333,
      "grad_norm": 0.6801219582557678,
      "learning_rate": 1.0266666666666668e-05,
      "loss": 0.0016,
      "step": 119200
    },
    {
      "epoch": 6.357866666666666,
      "grad_norm": 0.3433774411678314,
      "learning_rate": 1.0263333333333334e-05,
      "loss": 0.0019,
      "step": 119210
    },
    {
      "epoch": 6.3584,
      "grad_norm": 0.044656842947006226,
      "learning_rate": 1.026e-05,
      "loss": 0.0017,
      "step": 119220
    },
    {
      "epoch": 6.358933333333333,
      "grad_norm": 0.040664222091436386,
      "learning_rate": 1.0256666666666668e-05,
      "loss": 0.0012,
      "step": 119230
    },
    {
      "epoch": 6.359466666666667,
      "grad_norm": 0.044588979333639145,
      "learning_rate": 1.0253333333333334e-05,
      "loss": 0.0018,
      "step": 119240
    },
    {
      "epoch": 6.36,
      "grad_norm": 0.17608439922332764,
      "learning_rate": 1.025e-05,
      "loss": 0.0014,
      "step": 119250
    },
    {
      "epoch": 6.360533333333334,
      "grad_norm": 0.11997795104980469,
      "learning_rate": 1.0246666666666666e-05,
      "loss": 0.0013,
      "step": 119260
    },
    {
      "epoch": 6.361066666666667,
      "grad_norm": 0.3284200429916382,
      "learning_rate": 1.0243333333333334e-05,
      "loss": 0.0021,
      "step": 119270
    },
    {
      "epoch": 6.3616,
      "grad_norm": 0.20458397269248962,
      "learning_rate": 1.024e-05,
      "loss": 0.0016,
      "step": 119280
    },
    {
      "epoch": 6.362133333333333,
      "grad_norm": 0.11596263200044632,
      "learning_rate": 1.0236666666666666e-05,
      "loss": 0.0018,
      "step": 119290
    },
    {
      "epoch": 6.362666666666667,
      "grad_norm": 0.12258066236972809,
      "learning_rate": 1.0233333333333334e-05,
      "loss": 0.0035,
      "step": 119300
    },
    {
      "epoch": 6.3632,
      "grad_norm": 0.10497596859931946,
      "learning_rate": 1.023e-05,
      "loss": 0.0013,
      "step": 119310
    },
    {
      "epoch": 6.363733333333333,
      "grad_norm": 0.12300868332386017,
      "learning_rate": 1.0226666666666668e-05,
      "loss": 0.0014,
      "step": 119320
    },
    {
      "epoch": 6.3642666666666665,
      "grad_norm": 0.20644119381904602,
      "learning_rate": 1.0223333333333333e-05,
      "loss": 0.0015,
      "step": 119330
    },
    {
      "epoch": 6.3648,
      "grad_norm": 0.31833696365356445,
      "learning_rate": 1.022e-05,
      "loss": 0.002,
      "step": 119340
    },
    {
      "epoch": 6.365333333333333,
      "grad_norm": 0.16827403008937836,
      "learning_rate": 1.0216666666666668e-05,
      "loss": 0.0022,
      "step": 119350
    },
    {
      "epoch": 6.365866666666666,
      "grad_norm": 0.30282342433929443,
      "learning_rate": 1.0213333333333334e-05,
      "loss": 0.0024,
      "step": 119360
    },
    {
      "epoch": 6.3664,
      "grad_norm": 0.35990995168685913,
      "learning_rate": 1.021e-05,
      "loss": 0.0019,
      "step": 119370
    },
    {
      "epoch": 6.366933333333334,
      "grad_norm": 0.33453285694122314,
      "learning_rate": 1.0206666666666667e-05,
      "loss": 0.0021,
      "step": 119380
    },
    {
      "epoch": 6.367466666666667,
      "grad_norm": 0.3104933202266693,
      "learning_rate": 1.0203333333333334e-05,
      "loss": 0.0017,
      "step": 119390
    },
    {
      "epoch": 6.368,
      "grad_norm": 0.08995041251182556,
      "learning_rate": 1.02e-05,
      "loss": 0.0018,
      "step": 119400
    },
    {
      "epoch": 6.368533333333334,
      "grad_norm": 0.42300403118133545,
      "learning_rate": 1.0196666666666667e-05,
      "loss": 0.0016,
      "step": 119410
    },
    {
      "epoch": 6.369066666666667,
      "grad_norm": 0.11928629875183105,
      "learning_rate": 1.0193333333333335e-05,
      "loss": 0.0015,
      "step": 119420
    },
    {
      "epoch": 6.3696,
      "grad_norm": 0.19966043531894684,
      "learning_rate": 1.019e-05,
      "loss": 0.0018,
      "step": 119430
    },
    {
      "epoch": 6.370133333333333,
      "grad_norm": 0.06877012550830841,
      "learning_rate": 1.0186666666666667e-05,
      "loss": 0.0015,
      "step": 119440
    },
    {
      "epoch": 6.370666666666667,
      "grad_norm": 0.31510838866233826,
      "learning_rate": 1.0183333333333333e-05,
      "loss": 0.0014,
      "step": 119450
    },
    {
      "epoch": 6.3712,
      "grad_norm": 0.06679739058017731,
      "learning_rate": 1.018e-05,
      "loss": 0.0017,
      "step": 119460
    },
    {
      "epoch": 6.371733333333333,
      "grad_norm": 0.14178624749183655,
      "learning_rate": 1.0176666666666667e-05,
      "loss": 0.0021,
      "step": 119470
    },
    {
      "epoch": 6.3722666666666665,
      "grad_norm": 0.5121265053749084,
      "learning_rate": 1.0173333333333333e-05,
      "loss": 0.0016,
      "step": 119480
    },
    {
      "epoch": 6.3728,
      "grad_norm": 0.14825744926929474,
      "learning_rate": 1.0170000000000001e-05,
      "loss": 0.0018,
      "step": 119490
    },
    {
      "epoch": 6.373333333333333,
      "grad_norm": 0.06284476071596146,
      "learning_rate": 1.0166666666666667e-05,
      "loss": 0.002,
      "step": 119500
    },
    {
      "epoch": 6.373866666666666,
      "grad_norm": 0.11581075191497803,
      "learning_rate": 1.0163333333333335e-05,
      "loss": 0.0019,
      "step": 119510
    },
    {
      "epoch": 6.3744,
      "grad_norm": 0.1709720939397812,
      "learning_rate": 1.016e-05,
      "loss": 0.0023,
      "step": 119520
    },
    {
      "epoch": 6.374933333333333,
      "grad_norm": 0.17322567105293274,
      "learning_rate": 1.0156666666666667e-05,
      "loss": 0.0016,
      "step": 119530
    },
    {
      "epoch": 6.375466666666667,
      "grad_norm": 0.1466885805130005,
      "learning_rate": 1.0153333333333335e-05,
      "loss": 0.0017,
      "step": 119540
    },
    {
      "epoch": 6.376,
      "grad_norm": 0.3832407593727112,
      "learning_rate": 1.0150000000000001e-05,
      "loss": 0.0025,
      "step": 119550
    },
    {
      "epoch": 6.376533333333334,
      "grad_norm": 0.14734122157096863,
      "learning_rate": 1.0146666666666667e-05,
      "loss": 0.0012,
      "step": 119560
    },
    {
      "epoch": 6.377066666666667,
      "grad_norm": 0.43025171756744385,
      "learning_rate": 1.0143333333333333e-05,
      "loss": 0.0021,
      "step": 119570
    },
    {
      "epoch": 6.3776,
      "grad_norm": 0.20766769349575043,
      "learning_rate": 1.0140000000000001e-05,
      "loss": 0.0023,
      "step": 119580
    },
    {
      "epoch": 6.378133333333333,
      "grad_norm": 0.046601150184869766,
      "learning_rate": 1.0136666666666667e-05,
      "loss": 0.0015,
      "step": 119590
    },
    {
      "epoch": 6.378666666666667,
      "grad_norm": 0.09075010567903519,
      "learning_rate": 1.0133333333333333e-05,
      "loss": 0.0024,
      "step": 119600
    },
    {
      "epoch": 6.3792,
      "grad_norm": 0.11560656130313873,
      "learning_rate": 1.0130000000000001e-05,
      "loss": 0.0019,
      "step": 119610
    },
    {
      "epoch": 6.379733333333333,
      "grad_norm": 0.3121136724948883,
      "learning_rate": 1.0126666666666667e-05,
      "loss": 0.0019,
      "step": 119620
    },
    {
      "epoch": 6.3802666666666665,
      "grad_norm": 0.144894078373909,
      "learning_rate": 1.0123333333333334e-05,
      "loss": 0.0021,
      "step": 119630
    },
    {
      "epoch": 6.3808,
      "grad_norm": 0.22820048034191132,
      "learning_rate": 1.012e-05,
      "loss": 0.0014,
      "step": 119640
    },
    {
      "epoch": 6.381333333333333,
      "grad_norm": 0.12356524169445038,
      "learning_rate": 1.0116666666666667e-05,
      "loss": 0.0019,
      "step": 119650
    },
    {
      "epoch": 6.381866666666666,
      "grad_norm": 0.4272688031196594,
      "learning_rate": 1.0113333333333334e-05,
      "loss": 0.0016,
      "step": 119660
    },
    {
      "epoch": 6.3824,
      "grad_norm": 0.34291285276412964,
      "learning_rate": 1.011e-05,
      "loss": 0.0019,
      "step": 119670
    },
    {
      "epoch": 6.382933333333334,
      "grad_norm": 0.17218001186847687,
      "learning_rate": 1.0106666666666668e-05,
      "loss": 0.0018,
      "step": 119680
    },
    {
      "epoch": 6.383466666666667,
      "grad_norm": 0.0891413763165474,
      "learning_rate": 1.0103333333333334e-05,
      "loss": 0.003,
      "step": 119690
    },
    {
      "epoch": 6.384,
      "grad_norm": 0.06366327404975891,
      "learning_rate": 1.0100000000000002e-05,
      "loss": 0.0013,
      "step": 119700
    },
    {
      "epoch": 6.384533333333334,
      "grad_norm": 0.0680646151304245,
      "learning_rate": 1.0096666666666666e-05,
      "loss": 0.0022,
      "step": 119710
    },
    {
      "epoch": 6.385066666666667,
      "grad_norm": 0.066897913813591,
      "learning_rate": 1.0093333333333334e-05,
      "loss": 0.0017,
      "step": 119720
    },
    {
      "epoch": 6.3856,
      "grad_norm": 0.10914412885904312,
      "learning_rate": 1.0090000000000002e-05,
      "loss": 0.0016,
      "step": 119730
    },
    {
      "epoch": 6.386133333333333,
      "grad_norm": 0.03310083597898483,
      "learning_rate": 1.0086666666666666e-05,
      "loss": 0.0019,
      "step": 119740
    },
    {
      "epoch": 6.386666666666667,
      "grad_norm": 0.055587008595466614,
      "learning_rate": 1.0083333333333334e-05,
      "loss": 0.0018,
      "step": 119750
    },
    {
      "epoch": 6.3872,
      "grad_norm": 0.3470400273799896,
      "learning_rate": 1.008e-05,
      "loss": 0.0014,
      "step": 119760
    },
    {
      "epoch": 6.387733333333333,
      "grad_norm": 0.3252483606338501,
      "learning_rate": 1.0076666666666668e-05,
      "loss": 0.0017,
      "step": 119770
    },
    {
      "epoch": 6.3882666666666665,
      "grad_norm": 0.11626581102609634,
      "learning_rate": 1.0073333333333334e-05,
      "loss": 0.0019,
      "step": 119780
    },
    {
      "epoch": 6.3888,
      "grad_norm": 0.34207916259765625,
      "learning_rate": 1.007e-05,
      "loss": 0.0015,
      "step": 119790
    },
    {
      "epoch": 6.389333333333333,
      "grad_norm": 0.03162028640508652,
      "learning_rate": 1.0066666666666668e-05,
      "loss": 0.002,
      "step": 119800
    },
    {
      "epoch": 6.389866666666666,
      "grad_norm": 0.11820819228887558,
      "learning_rate": 1.0063333333333334e-05,
      "loss": 0.0017,
      "step": 119810
    },
    {
      "epoch": 6.3904,
      "grad_norm": 0.05183188617229462,
      "learning_rate": 1.006e-05,
      "loss": 0.0023,
      "step": 119820
    },
    {
      "epoch": 6.390933333333333,
      "grad_norm": 0.04658123850822449,
      "learning_rate": 1.0056666666666666e-05,
      "loss": 0.0027,
      "step": 119830
    },
    {
      "epoch": 6.391466666666667,
      "grad_norm": 0.054389502853155136,
      "learning_rate": 1.0053333333333334e-05,
      "loss": 0.0014,
      "step": 119840
    },
    {
      "epoch": 6.392,
      "grad_norm": 0.0708557590842247,
      "learning_rate": 1.005e-05,
      "loss": 0.0019,
      "step": 119850
    },
    {
      "epoch": 6.392533333333334,
      "grad_norm": 0.3448950946331024,
      "learning_rate": 1.0046666666666666e-05,
      "loss": 0.0017,
      "step": 119860
    },
    {
      "epoch": 6.393066666666667,
      "grad_norm": 0.09619869291782379,
      "learning_rate": 1.0043333333333334e-05,
      "loss": 0.0012,
      "step": 119870
    },
    {
      "epoch": 6.3936,
      "grad_norm": 0.040065910667181015,
      "learning_rate": 1.004e-05,
      "loss": 0.0013,
      "step": 119880
    },
    {
      "epoch": 6.3941333333333334,
      "grad_norm": 0.45737481117248535,
      "learning_rate": 1.0036666666666668e-05,
      "loss": 0.002,
      "step": 119890
    },
    {
      "epoch": 6.394666666666667,
      "grad_norm": 0.451132595539093,
      "learning_rate": 1.0033333333333333e-05,
      "loss": 0.0016,
      "step": 119900
    },
    {
      "epoch": 6.3952,
      "grad_norm": 0.3171728551387787,
      "learning_rate": 1.003e-05,
      "loss": 0.0018,
      "step": 119910
    },
    {
      "epoch": 6.395733333333333,
      "grad_norm": 0.09081719070672989,
      "learning_rate": 1.0026666666666668e-05,
      "loss": 0.0017,
      "step": 119920
    },
    {
      "epoch": 6.3962666666666665,
      "grad_norm": 0.18964883685112,
      "learning_rate": 1.0023333333333333e-05,
      "loss": 0.0026,
      "step": 119930
    },
    {
      "epoch": 6.3968,
      "grad_norm": 0.2063749134540558,
      "learning_rate": 1.002e-05,
      "loss": 0.0027,
      "step": 119940
    },
    {
      "epoch": 6.397333333333333,
      "grad_norm": 0.12391926348209381,
      "learning_rate": 1.0016666666666667e-05,
      "loss": 0.0014,
      "step": 119950
    },
    {
      "epoch": 6.397866666666666,
      "grad_norm": 0.29494795203208923,
      "learning_rate": 1.0013333333333335e-05,
      "loss": 0.0015,
      "step": 119960
    },
    {
      "epoch": 6.3984,
      "grad_norm": 0.08809671550989151,
      "learning_rate": 1.001e-05,
      "loss": 0.0015,
      "step": 119970
    },
    {
      "epoch": 6.398933333333333,
      "grad_norm": 0.26567620038986206,
      "learning_rate": 1.0006666666666667e-05,
      "loss": 0.0026,
      "step": 119980
    },
    {
      "epoch": 6.399466666666667,
      "grad_norm": 0.13252460956573486,
      "learning_rate": 1.0003333333333335e-05,
      "loss": 0.0016,
      "step": 119990
    },
    {
      "epoch": 6.4,
      "grad_norm": 0.11544158309698105,
      "learning_rate": 1e-05,
      "loss": 0.0018,
      "step": 120000
    },
    {
      "epoch": 6.400533333333334,
      "grad_norm": 0.3093600571155548,
      "learning_rate": 9.996666666666667e-06,
      "loss": 0.0017,
      "step": 120010
    },
    {
      "epoch": 6.401066666666667,
      "grad_norm": 0.06829661875963211,
      "learning_rate": 9.993333333333333e-06,
      "loss": 0.0019,
      "step": 120020
    },
    {
      "epoch": 6.4016,
      "grad_norm": 0.37969645857810974,
      "learning_rate": 9.990000000000001e-06,
      "loss": 0.002,
      "step": 120030
    },
    {
      "epoch": 6.4021333333333335,
      "grad_norm": 0.26196369528770447,
      "learning_rate": 9.986666666666667e-06,
      "loss": 0.0019,
      "step": 120040
    },
    {
      "epoch": 6.402666666666667,
      "grad_norm": 0.3456065356731415,
      "learning_rate": 9.983333333333333e-06,
      "loss": 0.0014,
      "step": 120050
    },
    {
      "epoch": 6.4032,
      "grad_norm": 0.05136650428175926,
      "learning_rate": 9.980000000000001e-06,
      "loss": 0.0019,
      "step": 120060
    },
    {
      "epoch": 6.403733333333333,
      "grad_norm": 0.171393021941185,
      "learning_rate": 9.976666666666667e-06,
      "loss": 0.0017,
      "step": 120070
    },
    {
      "epoch": 6.4042666666666666,
      "grad_norm": 0.06732334196567535,
      "learning_rate": 9.973333333333333e-06,
      "loss": 0.002,
      "step": 120080
    },
    {
      "epoch": 6.4048,
      "grad_norm": 0.31238460540771484,
      "learning_rate": 9.97e-06,
      "loss": 0.0015,
      "step": 120090
    },
    {
      "epoch": 6.405333333333333,
      "grad_norm": 0.07701950520277023,
      "learning_rate": 9.966666666666667e-06,
      "loss": 0.0013,
      "step": 120100
    },
    {
      "epoch": 6.405866666666666,
      "grad_norm": 0.11650131642818451,
      "learning_rate": 9.963333333333335e-06,
      "loss": 0.0023,
      "step": 120110
    },
    {
      "epoch": 6.4064,
      "grad_norm": 0.30988943576812744,
      "learning_rate": 9.96e-06,
      "loss": 0.0012,
      "step": 120120
    },
    {
      "epoch": 6.406933333333333,
      "grad_norm": 0.0759085863828659,
      "learning_rate": 9.956666666666667e-06,
      "loss": 0.0016,
      "step": 120130
    },
    {
      "epoch": 6.407466666666666,
      "grad_norm": 0.14825493097305298,
      "learning_rate": 9.953333333333333e-06,
      "loss": 0.0019,
      "step": 120140
    },
    {
      "epoch": 6.408,
      "grad_norm": 0.08715375512838364,
      "learning_rate": 9.950000000000001e-06,
      "loss": 0.0021,
      "step": 120150
    },
    {
      "epoch": 6.408533333333334,
      "grad_norm": 0.20153391361236572,
      "learning_rate": 9.946666666666667e-06,
      "loss": 0.0013,
      "step": 120160
    },
    {
      "epoch": 6.409066666666667,
      "grad_norm": 0.03321656584739685,
      "learning_rate": 9.943333333333334e-06,
      "loss": 0.0015,
      "step": 120170
    },
    {
      "epoch": 6.4096,
      "grad_norm": 0.08635752648115158,
      "learning_rate": 9.940000000000001e-06,
      "loss": 0.0015,
      "step": 120180
    },
    {
      "epoch": 6.4101333333333335,
      "grad_norm": 0.3737550675868988,
      "learning_rate": 9.936666666666668e-06,
      "loss": 0.0014,
      "step": 120190
    },
    {
      "epoch": 6.410666666666667,
      "grad_norm": 0.06360075622797012,
      "learning_rate": 9.933333333333334e-06,
      "loss": 0.0018,
      "step": 120200
    },
    {
      "epoch": 6.4112,
      "grad_norm": 0.2905844748020172,
      "learning_rate": 9.93e-06,
      "loss": 0.0017,
      "step": 120210
    },
    {
      "epoch": 6.411733333333333,
      "grad_norm": 0.07224977761507034,
      "learning_rate": 9.926666666666668e-06,
      "loss": 0.0019,
      "step": 120220
    },
    {
      "epoch": 6.412266666666667,
      "grad_norm": 0.18008749186992645,
      "learning_rate": 9.923333333333334e-06,
      "loss": 0.0016,
      "step": 120230
    },
    {
      "epoch": 6.4128,
      "grad_norm": 0.23207125067710876,
      "learning_rate": 9.92e-06,
      "loss": 0.0016,
      "step": 120240
    },
    {
      "epoch": 6.413333333333333,
      "grad_norm": 0.4888674318790436,
      "learning_rate": 9.916666666666668e-06,
      "loss": 0.003,
      "step": 120250
    },
    {
      "epoch": 6.413866666666666,
      "grad_norm": 0.11728042364120483,
      "learning_rate": 9.913333333333334e-06,
      "loss": 0.0015,
      "step": 120260
    },
    {
      "epoch": 6.4144,
      "grad_norm": 0.357492595911026,
      "learning_rate": 9.91e-06,
      "loss": 0.002,
      "step": 120270
    },
    {
      "epoch": 6.414933333333333,
      "grad_norm": 0.35682782530784607,
      "learning_rate": 9.906666666666666e-06,
      "loss": 0.002,
      "step": 120280
    },
    {
      "epoch": 6.415466666666667,
      "grad_norm": 0.23879654705524445,
      "learning_rate": 9.903333333333334e-06,
      "loss": 0.0016,
      "step": 120290
    },
    {
      "epoch": 6.416,
      "grad_norm": 0.17217394709587097,
      "learning_rate": 9.900000000000002e-06,
      "loss": 0.0028,
      "step": 120300
    },
    {
      "epoch": 6.416533333333334,
      "grad_norm": 0.20097564160823822,
      "learning_rate": 9.896666666666666e-06,
      "loss": 0.002,
      "step": 120310
    },
    {
      "epoch": 6.417066666666667,
      "grad_norm": 0.08970841020345688,
      "learning_rate": 9.893333333333334e-06,
      "loss": 0.002,
      "step": 120320
    },
    {
      "epoch": 6.4176,
      "grad_norm": 0.2536683976650238,
      "learning_rate": 9.89e-06,
      "loss": 0.0017,
      "step": 120330
    },
    {
      "epoch": 6.4181333333333335,
      "grad_norm": 0.17623448371887207,
      "learning_rate": 9.886666666666668e-06,
      "loss": 0.0017,
      "step": 120340
    },
    {
      "epoch": 6.418666666666667,
      "grad_norm": 0.3127162456512451,
      "learning_rate": 9.883333333333334e-06,
      "loss": 0.0014,
      "step": 120350
    },
    {
      "epoch": 6.4192,
      "grad_norm": 0.08928786218166351,
      "learning_rate": 9.88e-06,
      "loss": 0.0014,
      "step": 120360
    },
    {
      "epoch": 6.419733333333333,
      "grad_norm": 0.05167831853032112,
      "learning_rate": 9.876666666666668e-06,
      "loss": 0.0013,
      "step": 120370
    },
    {
      "epoch": 6.420266666666667,
      "grad_norm": 0.14637690782546997,
      "learning_rate": 9.873333333333334e-06,
      "loss": 0.0018,
      "step": 120380
    },
    {
      "epoch": 6.4208,
      "grad_norm": 0.1870621144771576,
      "learning_rate": 9.87e-06,
      "loss": 0.0019,
      "step": 120390
    },
    {
      "epoch": 6.421333333333333,
      "grad_norm": 0.25877436995506287,
      "learning_rate": 9.866666666666667e-06,
      "loss": 0.0017,
      "step": 120400
    },
    {
      "epoch": 6.421866666666666,
      "grad_norm": 0.17053593695163727,
      "learning_rate": 9.863333333333334e-06,
      "loss": 0.0013,
      "step": 120410
    },
    {
      "epoch": 6.4224,
      "grad_norm": 0.0665835365653038,
      "learning_rate": 9.86e-06,
      "loss": 0.0025,
      "step": 120420
    },
    {
      "epoch": 6.422933333333333,
      "grad_norm": 0.03061809577047825,
      "learning_rate": 9.856666666666667e-06,
      "loss": 0.0015,
      "step": 120430
    },
    {
      "epoch": 6.423466666666666,
      "grad_norm": 0.40131664276123047,
      "learning_rate": 9.853333333333334e-06,
      "loss": 0.0029,
      "step": 120440
    },
    {
      "epoch": 6.424,
      "grad_norm": 0.28840330243110657,
      "learning_rate": 9.85e-06,
      "loss": 0.0013,
      "step": 120450
    },
    {
      "epoch": 6.424533333333334,
      "grad_norm": 0.08694478124380112,
      "learning_rate": 9.846666666666667e-06,
      "loss": 0.0023,
      "step": 120460
    },
    {
      "epoch": 6.425066666666667,
      "grad_norm": 0.1722230166196823,
      "learning_rate": 9.843333333333333e-06,
      "loss": 0.0014,
      "step": 120470
    },
    {
      "epoch": 6.4256,
      "grad_norm": 0.11809328943490982,
      "learning_rate": 9.84e-06,
      "loss": 0.0014,
      "step": 120480
    },
    {
      "epoch": 6.4261333333333335,
      "grad_norm": 0.45447850227355957,
      "learning_rate": 9.836666666666668e-06,
      "loss": 0.0015,
      "step": 120490
    },
    {
      "epoch": 6.426666666666667,
      "grad_norm": 0.13972418010234833,
      "learning_rate": 9.833333333333333e-06,
      "loss": 0.0022,
      "step": 120500
    },
    {
      "epoch": 6.4272,
      "grad_norm": 0.26067617535591125,
      "learning_rate": 9.83e-06,
      "loss": 0.0015,
      "step": 120510
    },
    {
      "epoch": 6.427733333333333,
      "grad_norm": 0.12480618059635162,
      "learning_rate": 9.826666666666667e-06,
      "loss": 0.0017,
      "step": 120520
    },
    {
      "epoch": 6.428266666666667,
      "grad_norm": 0.0994793251156807,
      "learning_rate": 9.823333333333335e-06,
      "loss": 0.0013,
      "step": 120530
    },
    {
      "epoch": 6.4288,
      "grad_norm": 0.11711861193180084,
      "learning_rate": 9.820000000000001e-06,
      "loss": 0.0013,
      "step": 120540
    },
    {
      "epoch": 6.429333333333333,
      "grad_norm": 0.29300427436828613,
      "learning_rate": 9.816666666666667e-06,
      "loss": 0.0016,
      "step": 120550
    },
    {
      "epoch": 6.429866666666666,
      "grad_norm": 0.26954564452171326,
      "learning_rate": 9.813333333333335e-06,
      "loss": 0.0014,
      "step": 120560
    },
    {
      "epoch": 6.4304,
      "grad_norm": 0.20050960779190063,
      "learning_rate": 9.810000000000001e-06,
      "loss": 0.002,
      "step": 120570
    },
    {
      "epoch": 6.430933333333333,
      "grad_norm": 0.3094378411769867,
      "learning_rate": 9.806666666666667e-06,
      "loss": 0.0013,
      "step": 120580
    },
    {
      "epoch": 6.431466666666667,
      "grad_norm": 0.12385524809360504,
      "learning_rate": 9.803333333333333e-06,
      "loss": 0.0014,
      "step": 120590
    },
    {
      "epoch": 6.432,
      "grad_norm": 0.06668882817029953,
      "learning_rate": 9.800000000000001e-06,
      "loss": 0.0017,
      "step": 120600
    },
    {
      "epoch": 6.432533333333334,
      "grad_norm": 0.17569100856781006,
      "learning_rate": 9.796666666666667e-06,
      "loss": 0.0013,
      "step": 120610
    },
    {
      "epoch": 6.433066666666667,
      "grad_norm": 0.14166823029518127,
      "learning_rate": 9.793333333333333e-06,
      "loss": 0.0021,
      "step": 120620
    },
    {
      "epoch": 6.4336,
      "grad_norm": 0.23033520579338074,
      "learning_rate": 9.790000000000001e-06,
      "loss": 0.0016,
      "step": 120630
    },
    {
      "epoch": 6.4341333333333335,
      "grad_norm": 0.20206832885742188,
      "learning_rate": 9.786666666666667e-06,
      "loss": 0.0018,
      "step": 120640
    },
    {
      "epoch": 6.434666666666667,
      "grad_norm": 0.20013770461082458,
      "learning_rate": 9.783333333333333e-06,
      "loss": 0.0015,
      "step": 120650
    },
    {
      "epoch": 6.4352,
      "grad_norm": 0.03983977809548378,
      "learning_rate": 9.78e-06,
      "loss": 0.0019,
      "step": 120660
    },
    {
      "epoch": 6.435733333333333,
      "grad_norm": 0.19945335388183594,
      "learning_rate": 9.776666666666667e-06,
      "loss": 0.0017,
      "step": 120670
    },
    {
      "epoch": 6.436266666666667,
      "grad_norm": 0.17332229018211365,
      "learning_rate": 9.773333333333333e-06,
      "loss": 0.0016,
      "step": 120680
    },
    {
      "epoch": 6.4368,
      "grad_norm": 0.09471068531274796,
      "learning_rate": 9.77e-06,
      "loss": 0.0014,
      "step": 120690
    },
    {
      "epoch": 6.437333333333333,
      "grad_norm": 0.2810785472393036,
      "learning_rate": 9.766666666666667e-06,
      "loss": 0.002,
      "step": 120700
    },
    {
      "epoch": 6.437866666666666,
      "grad_norm": 0.12636201083660126,
      "learning_rate": 9.763333333333334e-06,
      "loss": 0.0019,
      "step": 120710
    },
    {
      "epoch": 6.4384,
      "grad_norm": 0.2166636735200882,
      "learning_rate": 9.760000000000001e-06,
      "loss": 0.0017,
      "step": 120720
    },
    {
      "epoch": 6.438933333333333,
      "grad_norm": 0.10838541388511658,
      "learning_rate": 9.756666666666668e-06,
      "loss": 0.0022,
      "step": 120730
    },
    {
      "epoch": 6.439466666666666,
      "grad_norm": 0.09596272557973862,
      "learning_rate": 9.753333333333334e-06,
      "loss": 0.0019,
      "step": 120740
    },
    {
      "epoch": 6.44,
      "grad_norm": 0.1526837795972824,
      "learning_rate": 9.750000000000002e-06,
      "loss": 0.0014,
      "step": 120750
    },
    {
      "epoch": 6.440533333333334,
      "grad_norm": 0.15836918354034424,
      "learning_rate": 9.746666666666666e-06,
      "loss": 0.0023,
      "step": 120760
    },
    {
      "epoch": 6.441066666666667,
      "grad_norm": 0.03911226987838745,
      "learning_rate": 9.743333333333334e-06,
      "loss": 0.003,
      "step": 120770
    },
    {
      "epoch": 6.4416,
      "grad_norm": 0.22930508852005005,
      "learning_rate": 9.74e-06,
      "loss": 0.0017,
      "step": 120780
    },
    {
      "epoch": 6.4421333333333335,
      "grad_norm": 0.2790933847427368,
      "learning_rate": 9.736666666666668e-06,
      "loss": 0.0018,
      "step": 120790
    },
    {
      "epoch": 6.442666666666667,
      "grad_norm": 0.2502003312110901,
      "learning_rate": 9.733333333333334e-06,
      "loss": 0.0032,
      "step": 120800
    },
    {
      "epoch": 6.4432,
      "grad_norm": 0.5720834732055664,
      "learning_rate": 9.73e-06,
      "loss": 0.0026,
      "step": 120810
    },
    {
      "epoch": 6.443733333333333,
      "grad_norm": 0.19844412803649902,
      "learning_rate": 9.726666666666668e-06,
      "loss": 0.0027,
      "step": 120820
    },
    {
      "epoch": 6.444266666666667,
      "grad_norm": 0.10558655112981796,
      "learning_rate": 9.723333333333334e-06,
      "loss": 0.002,
      "step": 120830
    },
    {
      "epoch": 6.4448,
      "grad_norm": 0.17130836844444275,
      "learning_rate": 9.72e-06,
      "loss": 0.0017,
      "step": 120840
    },
    {
      "epoch": 6.445333333333333,
      "grad_norm": 0.2571445107460022,
      "learning_rate": 9.716666666666666e-06,
      "loss": 0.0014,
      "step": 120850
    },
    {
      "epoch": 6.445866666666666,
      "grad_norm": 0.15161702036857605,
      "learning_rate": 9.713333333333334e-06,
      "loss": 0.0019,
      "step": 120860
    },
    {
      "epoch": 6.4464,
      "grad_norm": 0.19819259643554688,
      "learning_rate": 9.71e-06,
      "loss": 0.0017,
      "step": 120870
    },
    {
      "epoch": 6.446933333333333,
      "grad_norm": 0.1422351598739624,
      "learning_rate": 9.706666666666666e-06,
      "loss": 0.0024,
      "step": 120880
    },
    {
      "epoch": 6.447466666666667,
      "grad_norm": 0.2834499478340149,
      "learning_rate": 9.703333333333334e-06,
      "loss": 0.0016,
      "step": 120890
    },
    {
      "epoch": 6.448,
      "grad_norm": 0.23328055441379547,
      "learning_rate": 9.7e-06,
      "loss": 0.0016,
      "step": 120900
    },
    {
      "epoch": 6.448533333333334,
      "grad_norm": 0.14650890231132507,
      "learning_rate": 9.696666666666668e-06,
      "loss": 0.0021,
      "step": 120910
    },
    {
      "epoch": 6.449066666666667,
      "grad_norm": 0.07337085902690887,
      "learning_rate": 9.693333333333334e-06,
      "loss": 0.0018,
      "step": 120920
    },
    {
      "epoch": 6.4496,
      "grad_norm": 0.05604560673236847,
      "learning_rate": 9.69e-06,
      "loss": 0.0025,
      "step": 120930
    },
    {
      "epoch": 6.4501333333333335,
      "grad_norm": 0.31076839566230774,
      "learning_rate": 9.686666666666668e-06,
      "loss": 0.0028,
      "step": 120940
    },
    {
      "epoch": 6.450666666666667,
      "grad_norm": 0.039451517164707184,
      "learning_rate": 9.683333333333333e-06,
      "loss": 0.0016,
      "step": 120950
    },
    {
      "epoch": 6.4512,
      "grad_norm": 0.34256526827812195,
      "learning_rate": 9.68e-06,
      "loss": 0.0013,
      "step": 120960
    },
    {
      "epoch": 6.451733333333333,
      "grad_norm": 0.14879821240901947,
      "learning_rate": 9.676666666666667e-06,
      "loss": 0.0018,
      "step": 120970
    },
    {
      "epoch": 6.452266666666667,
      "grad_norm": 0.11294015496969223,
      "learning_rate": 9.673333333333334e-06,
      "loss": 0.0022,
      "step": 120980
    },
    {
      "epoch": 6.4528,
      "grad_norm": 0.2917855978012085,
      "learning_rate": 9.67e-06,
      "loss": 0.0014,
      "step": 120990
    },
    {
      "epoch": 6.453333333333333,
      "grad_norm": 0.3107428550720215,
      "learning_rate": 9.666666666666667e-06,
      "loss": 0.0014,
      "step": 121000
    },
    {
      "epoch": 6.453866666666666,
      "grad_norm": 0.11894075572490692,
      "learning_rate": 9.663333333333335e-06,
      "loss": 0.002,
      "step": 121010
    },
    {
      "epoch": 6.4544,
      "grad_norm": 0.06402762234210968,
      "learning_rate": 9.66e-06,
      "loss": 0.002,
      "step": 121020
    },
    {
      "epoch": 6.454933333333333,
      "grad_norm": 0.049746256321668625,
      "learning_rate": 9.656666666666667e-06,
      "loss": 0.0022,
      "step": 121030
    },
    {
      "epoch": 6.455466666666666,
      "grad_norm": 0.2892148792743683,
      "learning_rate": 9.653333333333333e-06,
      "loss": 0.0021,
      "step": 121040
    },
    {
      "epoch": 6.456,
      "grad_norm": 0.1741863340139389,
      "learning_rate": 9.65e-06,
      "loss": 0.0023,
      "step": 121050
    },
    {
      "epoch": 6.456533333333334,
      "grad_norm": 0.09189480543136597,
      "learning_rate": 9.646666666666667e-06,
      "loss": 0.0013,
      "step": 121060
    },
    {
      "epoch": 6.457066666666667,
      "grad_norm": 0.08406726270914078,
      "learning_rate": 9.643333333333333e-06,
      "loss": 0.0022,
      "step": 121070
    },
    {
      "epoch": 6.4576,
      "grad_norm": 0.31032440066337585,
      "learning_rate": 9.640000000000001e-06,
      "loss": 0.0013,
      "step": 121080
    },
    {
      "epoch": 6.4581333333333335,
      "grad_norm": 0.06523003429174423,
      "learning_rate": 9.636666666666667e-06,
      "loss": 0.0019,
      "step": 121090
    },
    {
      "epoch": 6.458666666666667,
      "grad_norm": 0.09806084632873535,
      "learning_rate": 9.633333333333335e-06,
      "loss": 0.0018,
      "step": 121100
    },
    {
      "epoch": 6.4592,
      "grad_norm": 0.17225399613380432,
      "learning_rate": 9.630000000000001e-06,
      "loss": 0.0019,
      "step": 121110
    },
    {
      "epoch": 6.459733333333333,
      "grad_norm": 0.035165246576070786,
      "learning_rate": 9.626666666666667e-06,
      "loss": 0.0018,
      "step": 121120
    },
    {
      "epoch": 6.460266666666667,
      "grad_norm": 0.30548614263534546,
      "learning_rate": 9.623333333333335e-06,
      "loss": 0.0018,
      "step": 121130
    },
    {
      "epoch": 6.4608,
      "grad_norm": 0.10656312853097916,
      "learning_rate": 9.62e-06,
      "loss": 0.0035,
      "step": 121140
    },
    {
      "epoch": 6.461333333333333,
      "grad_norm": 0.09635470062494278,
      "learning_rate": 9.616666666666667e-06,
      "loss": 0.0011,
      "step": 121150
    },
    {
      "epoch": 6.461866666666666,
      "grad_norm": 0.38461071252822876,
      "learning_rate": 9.613333333333333e-06,
      "loss": 0.0018,
      "step": 121160
    },
    {
      "epoch": 6.4624,
      "grad_norm": 0.1457943320274353,
      "learning_rate": 9.610000000000001e-06,
      "loss": 0.0022,
      "step": 121170
    },
    {
      "epoch": 6.462933333333333,
      "grad_norm": 0.04286422207951546,
      "learning_rate": 9.606666666666667e-06,
      "loss": 0.0021,
      "step": 121180
    },
    {
      "epoch": 6.463466666666667,
      "grad_norm": 0.1528681367635727,
      "learning_rate": 9.603333333333333e-06,
      "loss": 0.0018,
      "step": 121190
    },
    {
      "epoch": 6.464,
      "grad_norm": 0.16945227980613708,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.0015,
      "step": 121200
    },
    {
      "epoch": 6.464533333333334,
      "grad_norm": 0.07582347095012665,
      "learning_rate": 9.596666666666667e-06,
      "loss": 0.0019,
      "step": 121210
    },
    {
      "epoch": 6.465066666666667,
      "grad_norm": 0.6035155057907104,
      "learning_rate": 9.593333333333334e-06,
      "loss": 0.0023,
      "step": 121220
    },
    {
      "epoch": 6.4656,
      "grad_norm": 0.11582454293966293,
      "learning_rate": 9.59e-06,
      "loss": 0.0018,
      "step": 121230
    },
    {
      "epoch": 6.4661333333333335,
      "grad_norm": 0.0363357812166214,
      "learning_rate": 9.586666666666667e-06,
      "loss": 0.0021,
      "step": 121240
    },
    {
      "epoch": 6.466666666666667,
      "grad_norm": 0.3780372142791748,
      "learning_rate": 9.583333333333334e-06,
      "loss": 0.0018,
      "step": 121250
    },
    {
      "epoch": 6.4672,
      "grad_norm": 0.25409647822380066,
      "learning_rate": 9.58e-06,
      "loss": 0.0018,
      "step": 121260
    },
    {
      "epoch": 6.467733333333333,
      "grad_norm": 0.17589624226093292,
      "learning_rate": 9.576666666666668e-06,
      "loss": 0.0017,
      "step": 121270
    },
    {
      "epoch": 6.468266666666667,
      "grad_norm": 0.25799840688705444,
      "learning_rate": 9.573333333333334e-06,
      "loss": 0.0021,
      "step": 121280
    },
    {
      "epoch": 6.4688,
      "grad_norm": 0.04518924281001091,
      "learning_rate": 9.57e-06,
      "loss": 0.0024,
      "step": 121290
    },
    {
      "epoch": 6.469333333333333,
      "grad_norm": 0.2635906934738159,
      "learning_rate": 9.566666666666666e-06,
      "loss": 0.0016,
      "step": 121300
    },
    {
      "epoch": 6.469866666666666,
      "grad_norm": 0.20491409301757812,
      "learning_rate": 9.563333333333334e-06,
      "loss": 0.0018,
      "step": 121310
    },
    {
      "epoch": 6.4704,
      "grad_norm": 0.21160326898097992,
      "learning_rate": 9.560000000000002e-06,
      "loss": 0.0012,
      "step": 121320
    },
    {
      "epoch": 6.470933333333333,
      "grad_norm": 0.10874626040458679,
      "learning_rate": 9.556666666666666e-06,
      "loss": 0.0024,
      "step": 121330
    },
    {
      "epoch": 6.471466666666666,
      "grad_norm": 0.17516058683395386,
      "learning_rate": 9.553333333333334e-06,
      "loss": 0.0023,
      "step": 121340
    },
    {
      "epoch": 6.4719999999999995,
      "grad_norm": 0.31262433528900146,
      "learning_rate": 9.55e-06,
      "loss": 0.0013,
      "step": 121350
    },
    {
      "epoch": 6.472533333333334,
      "grad_norm": 0.2788827419281006,
      "learning_rate": 9.546666666666668e-06,
      "loss": 0.0016,
      "step": 121360
    },
    {
      "epoch": 6.473066666666667,
      "grad_norm": 0.17512480914592743,
      "learning_rate": 9.543333333333334e-06,
      "loss": 0.0015,
      "step": 121370
    },
    {
      "epoch": 6.4736,
      "grad_norm": 0.29058837890625,
      "learning_rate": 9.54e-06,
      "loss": 0.0019,
      "step": 121380
    },
    {
      "epoch": 6.4741333333333335,
      "grad_norm": 0.09580409526824951,
      "learning_rate": 9.536666666666668e-06,
      "loss": 0.0017,
      "step": 121390
    },
    {
      "epoch": 6.474666666666667,
      "grad_norm": 0.5111367106437683,
      "learning_rate": 9.533333333333334e-06,
      "loss": 0.0019,
      "step": 121400
    },
    {
      "epoch": 6.4752,
      "grad_norm": 0.08975798636674881,
      "learning_rate": 9.53e-06,
      "loss": 0.0021,
      "step": 121410
    },
    {
      "epoch": 6.475733333333333,
      "grad_norm": 0.1200801432132721,
      "learning_rate": 9.526666666666666e-06,
      "loss": 0.0023,
      "step": 121420
    },
    {
      "epoch": 6.476266666666667,
      "grad_norm": 0.04892124980688095,
      "learning_rate": 9.523333333333334e-06,
      "loss": 0.0012,
      "step": 121430
    },
    {
      "epoch": 6.4768,
      "grad_norm": 0.11468426883220673,
      "learning_rate": 9.52e-06,
      "loss": 0.0016,
      "step": 121440
    },
    {
      "epoch": 6.477333333333333,
      "grad_norm": 0.0388534851372242,
      "learning_rate": 9.516666666666666e-06,
      "loss": 0.0018,
      "step": 121450
    },
    {
      "epoch": 6.477866666666666,
      "grad_norm": 0.2597654461860657,
      "learning_rate": 9.513333333333334e-06,
      "loss": 0.0017,
      "step": 121460
    },
    {
      "epoch": 6.4784,
      "grad_norm": 0.2847834825515747,
      "learning_rate": 9.51e-06,
      "loss": 0.0014,
      "step": 121470
    },
    {
      "epoch": 6.478933333333333,
      "grad_norm": 0.5969974398612976,
      "learning_rate": 9.506666666666667e-06,
      "loss": 0.0018,
      "step": 121480
    },
    {
      "epoch": 6.479466666666666,
      "grad_norm": 0.10345613211393356,
      "learning_rate": 9.503333333333333e-06,
      "loss": 0.0017,
      "step": 121490
    },
    {
      "epoch": 6.48,
      "grad_norm": 0.02838466875255108,
      "learning_rate": 9.5e-06,
      "loss": 0.0021,
      "step": 121500
    },
    {
      "epoch": 6.480533333333334,
      "grad_norm": 0.052176669239997864,
      "learning_rate": 9.496666666666668e-06,
      "loss": 0.0013,
      "step": 121510
    },
    {
      "epoch": 6.481066666666667,
      "grad_norm": 0.08308210968971252,
      "learning_rate": 9.493333333333333e-06,
      "loss": 0.0017,
      "step": 121520
    },
    {
      "epoch": 6.4816,
      "grad_norm": 0.4218285381793976,
      "learning_rate": 9.49e-06,
      "loss": 0.002,
      "step": 121530
    },
    {
      "epoch": 6.4821333333333335,
      "grad_norm": 0.04766855016350746,
      "learning_rate": 9.486666666666667e-06,
      "loss": 0.0025,
      "step": 121540
    },
    {
      "epoch": 6.482666666666667,
      "grad_norm": 0.34084174036979675,
      "learning_rate": 9.483333333333335e-06,
      "loss": 0.0015,
      "step": 121550
    },
    {
      "epoch": 6.4832,
      "grad_norm": 0.14924469590187073,
      "learning_rate": 9.48e-06,
      "loss": 0.0022,
      "step": 121560
    },
    {
      "epoch": 6.483733333333333,
      "grad_norm": 0.2819063365459442,
      "learning_rate": 9.476666666666667e-06,
      "loss": 0.0025,
      "step": 121570
    },
    {
      "epoch": 6.484266666666667,
      "grad_norm": 0.04056665673851967,
      "learning_rate": 9.473333333333335e-06,
      "loss": 0.0014,
      "step": 121580
    },
    {
      "epoch": 6.4848,
      "grad_norm": 0.13621295988559723,
      "learning_rate": 9.47e-06,
      "loss": 0.0014,
      "step": 121590
    },
    {
      "epoch": 6.485333333333333,
      "grad_norm": 0.2276044338941574,
      "learning_rate": 9.466666666666667e-06,
      "loss": 0.0015,
      "step": 121600
    },
    {
      "epoch": 6.4858666666666664,
      "grad_norm": 0.15695320069789886,
      "learning_rate": 9.463333333333333e-06,
      "loss": 0.0018,
      "step": 121610
    },
    {
      "epoch": 6.4864,
      "grad_norm": 0.17440249025821686,
      "learning_rate": 9.460000000000001e-06,
      "loss": 0.0021,
      "step": 121620
    },
    {
      "epoch": 6.486933333333333,
      "grad_norm": 0.5721449255943298,
      "learning_rate": 9.456666666666667e-06,
      "loss": 0.0017,
      "step": 121630
    },
    {
      "epoch": 6.487466666666666,
      "grad_norm": 0.14331600069999695,
      "learning_rate": 9.453333333333333e-06,
      "loss": 0.0021,
      "step": 121640
    },
    {
      "epoch": 6.4879999999999995,
      "grad_norm": 0.48532938957214355,
      "learning_rate": 9.450000000000001e-06,
      "loss": 0.0015,
      "step": 121650
    },
    {
      "epoch": 6.488533333333334,
      "grad_norm": 0.34517398476600647,
      "learning_rate": 9.446666666666667e-06,
      "loss": 0.0026,
      "step": 121660
    },
    {
      "epoch": 6.489066666666667,
      "grad_norm": 0.14963984489440918,
      "learning_rate": 9.443333333333333e-06,
      "loss": 0.0018,
      "step": 121670
    },
    {
      "epoch": 6.4896,
      "grad_norm": 0.09431394934654236,
      "learning_rate": 9.44e-06,
      "loss": 0.0024,
      "step": 121680
    },
    {
      "epoch": 6.4901333333333335,
      "grad_norm": 0.26303932070732117,
      "learning_rate": 9.436666666666667e-06,
      "loss": 0.0013,
      "step": 121690
    },
    {
      "epoch": 6.490666666666667,
      "grad_norm": 0.32590195536613464,
      "learning_rate": 9.433333333333335e-06,
      "loss": 0.0017,
      "step": 121700
    },
    {
      "epoch": 6.4912,
      "grad_norm": 0.10025116056203842,
      "learning_rate": 9.43e-06,
      "loss": 0.0025,
      "step": 121710
    },
    {
      "epoch": 6.491733333333333,
      "grad_norm": 0.08873727172613144,
      "learning_rate": 9.426666666666667e-06,
      "loss": 0.0019,
      "step": 121720
    },
    {
      "epoch": 6.492266666666667,
      "grad_norm": 0.3396661579608917,
      "learning_rate": 9.423333333333333e-06,
      "loss": 0.0015,
      "step": 121730
    },
    {
      "epoch": 6.4928,
      "grad_norm": 0.2014036476612091,
      "learning_rate": 9.420000000000001e-06,
      "loss": 0.0016,
      "step": 121740
    },
    {
      "epoch": 6.493333333333333,
      "grad_norm": 0.23344027996063232,
      "learning_rate": 9.416666666666667e-06,
      "loss": 0.0021,
      "step": 121750
    },
    {
      "epoch": 6.4938666666666665,
      "grad_norm": 0.17181813716888428,
      "learning_rate": 9.413333333333334e-06,
      "loss": 0.0027,
      "step": 121760
    },
    {
      "epoch": 6.4944,
      "grad_norm": 0.1779770702123642,
      "learning_rate": 9.410000000000001e-06,
      "loss": 0.0026,
      "step": 121770
    },
    {
      "epoch": 6.494933333333333,
      "grad_norm": 0.3238676190376282,
      "learning_rate": 9.406666666666668e-06,
      "loss": 0.0017,
      "step": 121780
    },
    {
      "epoch": 6.495466666666666,
      "grad_norm": 0.2025233805179596,
      "learning_rate": 9.403333333333334e-06,
      "loss": 0.0017,
      "step": 121790
    },
    {
      "epoch": 6.496,
      "grad_norm": 0.08949774503707886,
      "learning_rate": 9.4e-06,
      "loss": 0.0015,
      "step": 121800
    },
    {
      "epoch": 6.496533333333334,
      "grad_norm": 0.31662026047706604,
      "learning_rate": 9.396666666666668e-06,
      "loss": 0.0014,
      "step": 121810
    },
    {
      "epoch": 6.497066666666667,
      "grad_norm": 0.039825424551963806,
      "learning_rate": 9.393333333333334e-06,
      "loss": 0.0023,
      "step": 121820
    },
    {
      "epoch": 6.4976,
      "grad_norm": 0.2265000343322754,
      "learning_rate": 9.39e-06,
      "loss": 0.0015,
      "step": 121830
    },
    {
      "epoch": 6.4981333333333335,
      "grad_norm": 0.12076133489608765,
      "learning_rate": 9.386666666666668e-06,
      "loss": 0.0015,
      "step": 121840
    },
    {
      "epoch": 6.498666666666667,
      "grad_norm": 0.37257394194602966,
      "learning_rate": 9.383333333333334e-06,
      "loss": 0.002,
      "step": 121850
    },
    {
      "epoch": 6.4992,
      "grad_norm": 0.11855524778366089,
      "learning_rate": 9.38e-06,
      "loss": 0.0013,
      "step": 121860
    },
    {
      "epoch": 6.499733333333333,
      "grad_norm": 0.40333986282348633,
      "learning_rate": 9.376666666666666e-06,
      "loss": 0.0011,
      "step": 121870
    },
    {
      "epoch": 6.500266666666667,
      "grad_norm": 0.029341965913772583,
      "learning_rate": 9.373333333333334e-06,
      "loss": 0.0022,
      "step": 121880
    },
    {
      "epoch": 6.5008,
      "grad_norm": 0.06990885734558105,
      "learning_rate": 9.370000000000002e-06,
      "loss": 0.0017,
      "step": 121890
    },
    {
      "epoch": 6.501333333333333,
      "grad_norm": 0.11822254210710526,
      "learning_rate": 9.366666666666666e-06,
      "loss": 0.0014,
      "step": 121900
    },
    {
      "epoch": 6.5018666666666665,
      "grad_norm": 0.09004554897546768,
      "learning_rate": 9.363333333333334e-06,
      "loss": 0.0023,
      "step": 121910
    },
    {
      "epoch": 6.5024,
      "grad_norm": 0.09633366763591766,
      "learning_rate": 9.36e-06,
      "loss": 0.0019,
      "step": 121920
    },
    {
      "epoch": 6.502933333333333,
      "grad_norm": 0.11276900768280029,
      "learning_rate": 9.356666666666668e-06,
      "loss": 0.0022,
      "step": 121930
    },
    {
      "epoch": 6.503466666666666,
      "grad_norm": 0.43851080536842346,
      "learning_rate": 9.353333333333334e-06,
      "loss": 0.0027,
      "step": 121940
    },
    {
      "epoch": 6.504,
      "grad_norm": 0.07380491495132446,
      "learning_rate": 9.35e-06,
      "loss": 0.0015,
      "step": 121950
    },
    {
      "epoch": 6.504533333333333,
      "grad_norm": 0.3167504072189331,
      "learning_rate": 9.346666666666668e-06,
      "loss": 0.0019,
      "step": 121960
    },
    {
      "epoch": 6.505066666666667,
      "grad_norm": 0.3299393951892853,
      "learning_rate": 9.343333333333333e-06,
      "loss": 0.0017,
      "step": 121970
    },
    {
      "epoch": 6.5056,
      "grad_norm": 0.1407758593559265,
      "learning_rate": 9.34e-06,
      "loss": 0.0023,
      "step": 121980
    },
    {
      "epoch": 6.5061333333333335,
      "grad_norm": 0.34613555669784546,
      "learning_rate": 9.336666666666666e-06,
      "loss": 0.0016,
      "step": 121990
    },
    {
      "epoch": 6.506666666666667,
      "grad_norm": 0.4016750752925873,
      "learning_rate": 9.333333333333334e-06,
      "loss": 0.002,
      "step": 122000
    },
    {
      "epoch": 6.5072,
      "grad_norm": 0.10172217339277267,
      "learning_rate": 9.33e-06,
      "loss": 0.0012,
      "step": 122010
    },
    {
      "epoch": 6.507733333333333,
      "grad_norm": 0.046606697142124176,
      "learning_rate": 9.326666666666667e-06,
      "loss": 0.0017,
      "step": 122020
    },
    {
      "epoch": 6.508266666666667,
      "grad_norm": 0.04064776003360748,
      "learning_rate": 9.323333333333334e-06,
      "loss": 0.0017,
      "step": 122030
    },
    {
      "epoch": 6.5088,
      "grad_norm": 0.37939122319221497,
      "learning_rate": 9.32e-06,
      "loss": 0.0016,
      "step": 122040
    },
    {
      "epoch": 6.509333333333333,
      "grad_norm": 0.11285068094730377,
      "learning_rate": 9.316666666666667e-06,
      "loss": 0.0018,
      "step": 122050
    },
    {
      "epoch": 6.5098666666666665,
      "grad_norm": 0.14638976752758026,
      "learning_rate": 9.313333333333333e-06,
      "loss": 0.0013,
      "step": 122060
    },
    {
      "epoch": 6.5104,
      "grad_norm": 0.33613690733909607,
      "learning_rate": 9.31e-06,
      "loss": 0.0018,
      "step": 122070
    },
    {
      "epoch": 6.510933333333333,
      "grad_norm": 0.6040692329406738,
      "learning_rate": 9.306666666666668e-06,
      "loss": 0.0011,
      "step": 122080
    },
    {
      "epoch": 6.511466666666666,
      "grad_norm": 0.12786754965782166,
      "learning_rate": 9.303333333333333e-06,
      "loss": 0.0016,
      "step": 122090
    },
    {
      "epoch": 6.5120000000000005,
      "grad_norm": 0.144635409116745,
      "learning_rate": 9.3e-06,
      "loss": 0.0016,
      "step": 122100
    },
    {
      "epoch": 6.512533333333334,
      "grad_norm": 0.25397518277168274,
      "learning_rate": 9.296666666666667e-06,
      "loss": 0.0028,
      "step": 122110
    },
    {
      "epoch": 6.513066666666667,
      "grad_norm": 0.1729498654603958,
      "learning_rate": 9.293333333333335e-06,
      "loss": 0.0018,
      "step": 122120
    },
    {
      "epoch": 6.5136,
      "grad_norm": 0.054973259568214417,
      "learning_rate": 9.29e-06,
      "loss": 0.0015,
      "step": 122130
    },
    {
      "epoch": 6.5141333333333336,
      "grad_norm": 0.032119475305080414,
      "learning_rate": 9.286666666666667e-06,
      "loss": 0.0026,
      "step": 122140
    },
    {
      "epoch": 6.514666666666667,
      "grad_norm": 0.3404337465763092,
      "learning_rate": 9.283333333333335e-06,
      "loss": 0.0016,
      "step": 122150
    },
    {
      "epoch": 6.5152,
      "grad_norm": 0.49173328280448914,
      "learning_rate": 9.28e-06,
      "loss": 0.0024,
      "step": 122160
    },
    {
      "epoch": 6.515733333333333,
      "grad_norm": 0.060894161462783813,
      "learning_rate": 9.276666666666667e-06,
      "loss": 0.0021,
      "step": 122170
    },
    {
      "epoch": 6.516266666666667,
      "grad_norm": 0.16890965402126312,
      "learning_rate": 9.273333333333333e-06,
      "loss": 0.0016,
      "step": 122180
    },
    {
      "epoch": 6.5168,
      "grad_norm": 0.11994677782058716,
      "learning_rate": 9.270000000000001e-06,
      "loss": 0.0011,
      "step": 122190
    },
    {
      "epoch": 6.517333333333333,
      "grad_norm": 0.06660191714763641,
      "learning_rate": 9.266666666666667e-06,
      "loss": 0.0017,
      "step": 122200
    },
    {
      "epoch": 6.5178666666666665,
      "grad_norm": 0.4554348289966583,
      "learning_rate": 9.263333333333333e-06,
      "loss": 0.0018,
      "step": 122210
    },
    {
      "epoch": 6.5184,
      "grad_norm": 0.04058219492435455,
      "learning_rate": 9.260000000000001e-06,
      "loss": 0.0021,
      "step": 122220
    },
    {
      "epoch": 6.518933333333333,
      "grad_norm": 0.07734093815088272,
      "learning_rate": 9.256666666666667e-06,
      "loss": 0.0021,
      "step": 122230
    },
    {
      "epoch": 6.519466666666666,
      "grad_norm": 0.12066283822059631,
      "learning_rate": 9.253333333333333e-06,
      "loss": 0.0015,
      "step": 122240
    },
    {
      "epoch": 6.52,
      "grad_norm": 0.039946965873241425,
      "learning_rate": 9.25e-06,
      "loss": 0.0025,
      "step": 122250
    },
    {
      "epoch": 6.520533333333333,
      "grad_norm": 0.1151934415102005,
      "learning_rate": 9.246666666666667e-06,
      "loss": 0.0023,
      "step": 122260
    },
    {
      "epoch": 6.521066666666667,
      "grad_norm": 0.6198344230651855,
      "learning_rate": 9.243333333333335e-06,
      "loss": 0.0021,
      "step": 122270
    },
    {
      "epoch": 6.5216,
      "grad_norm": 0.4996507465839386,
      "learning_rate": 9.24e-06,
      "loss": 0.0013,
      "step": 122280
    },
    {
      "epoch": 6.522133333333334,
      "grad_norm": 0.24847261607646942,
      "learning_rate": 9.236666666666667e-06,
      "loss": 0.0014,
      "step": 122290
    },
    {
      "epoch": 6.522666666666667,
      "grad_norm": 0.08945553004741669,
      "learning_rate": 9.233333333333334e-06,
      "loss": 0.0015,
      "step": 122300
    },
    {
      "epoch": 6.5232,
      "grad_norm": 0.0477764867246151,
      "learning_rate": 9.23e-06,
      "loss": 0.0025,
      "step": 122310
    },
    {
      "epoch": 6.523733333333333,
      "grad_norm": 0.3373309075832367,
      "learning_rate": 9.226666666666668e-06,
      "loss": 0.0012,
      "step": 122320
    },
    {
      "epoch": 6.524266666666667,
      "grad_norm": 0.09226939082145691,
      "learning_rate": 9.223333333333334e-06,
      "loss": 0.0018,
      "step": 122330
    },
    {
      "epoch": 6.5248,
      "grad_norm": 0.2002844214439392,
      "learning_rate": 9.220000000000002e-06,
      "loss": 0.0017,
      "step": 122340
    },
    {
      "epoch": 6.525333333333333,
      "grad_norm": 0.2202010154724121,
      "learning_rate": 9.216666666666666e-06,
      "loss": 0.0019,
      "step": 122350
    },
    {
      "epoch": 6.5258666666666665,
      "grad_norm": 0.2635120749473572,
      "learning_rate": 9.213333333333334e-06,
      "loss": 0.0015,
      "step": 122360
    },
    {
      "epoch": 6.5264,
      "grad_norm": 0.04653258994221687,
      "learning_rate": 9.21e-06,
      "loss": 0.0024,
      "step": 122370
    },
    {
      "epoch": 6.526933333333333,
      "grad_norm": 0.3403683006763458,
      "learning_rate": 9.206666666666668e-06,
      "loss": 0.0019,
      "step": 122380
    },
    {
      "epoch": 6.527466666666666,
      "grad_norm": 0.06368318200111389,
      "learning_rate": 9.203333333333334e-06,
      "loss": 0.0015,
      "step": 122390
    },
    {
      "epoch": 6.5280000000000005,
      "grad_norm": 0.2092418521642685,
      "learning_rate": 9.2e-06,
      "loss": 0.0017,
      "step": 122400
    },
    {
      "epoch": 6.528533333333334,
      "grad_norm": 0.4887784421443939,
      "learning_rate": 9.196666666666668e-06,
      "loss": 0.0021,
      "step": 122410
    },
    {
      "epoch": 6.529066666666667,
      "grad_norm": 0.4008581042289734,
      "learning_rate": 9.193333333333334e-06,
      "loss": 0.0018,
      "step": 122420
    },
    {
      "epoch": 6.5296,
      "grad_norm": 0.17151272296905518,
      "learning_rate": 9.19e-06,
      "loss": 0.0014,
      "step": 122430
    },
    {
      "epoch": 6.530133333333334,
      "grad_norm": 0.17782382667064667,
      "learning_rate": 9.186666666666666e-06,
      "loss": 0.0017,
      "step": 122440
    },
    {
      "epoch": 6.530666666666667,
      "grad_norm": 0.147883802652359,
      "learning_rate": 9.183333333333334e-06,
      "loss": 0.002,
      "step": 122450
    },
    {
      "epoch": 6.5312,
      "grad_norm": 0.06371542811393738,
      "learning_rate": 9.180000000000002e-06,
      "loss": 0.0016,
      "step": 122460
    },
    {
      "epoch": 6.531733333333333,
      "grad_norm": 0.07543548941612244,
      "learning_rate": 9.176666666666666e-06,
      "loss": 0.0011,
      "step": 122470
    },
    {
      "epoch": 6.532266666666667,
      "grad_norm": 0.08919108659029007,
      "learning_rate": 9.173333333333334e-06,
      "loss": 0.0012,
      "step": 122480
    },
    {
      "epoch": 6.5328,
      "grad_norm": 0.28455767035484314,
      "learning_rate": 9.17e-06,
      "loss": 0.0012,
      "step": 122490
    },
    {
      "epoch": 6.533333333333333,
      "grad_norm": 0.32118409872055054,
      "learning_rate": 9.166666666666666e-06,
      "loss": 0.0022,
      "step": 122500
    },
    {
      "epoch": 6.5338666666666665,
      "grad_norm": 0.11648232489824295,
      "learning_rate": 9.163333333333334e-06,
      "loss": 0.0016,
      "step": 122510
    },
    {
      "epoch": 6.5344,
      "grad_norm": 0.283750057220459,
      "learning_rate": 9.16e-06,
      "loss": 0.0018,
      "step": 122520
    },
    {
      "epoch": 6.534933333333333,
      "grad_norm": 0.04629475623369217,
      "learning_rate": 9.156666666666668e-06,
      "loss": 0.0015,
      "step": 122530
    },
    {
      "epoch": 6.535466666666666,
      "grad_norm": 0.06350604444742203,
      "learning_rate": 9.153333333333333e-06,
      "loss": 0.0017,
      "step": 122540
    },
    {
      "epoch": 6.536,
      "grad_norm": 0.06763705611228943,
      "learning_rate": 9.15e-06,
      "loss": 0.002,
      "step": 122550
    },
    {
      "epoch": 6.536533333333333,
      "grad_norm": 0.2288241684436798,
      "learning_rate": 9.146666666666667e-06,
      "loss": 0.0021,
      "step": 122560
    },
    {
      "epoch": 6.537066666666667,
      "grad_norm": 0.0411655455827713,
      "learning_rate": 9.143333333333334e-06,
      "loss": 0.0016,
      "step": 122570
    },
    {
      "epoch": 6.5376,
      "grad_norm": 0.39863288402557373,
      "learning_rate": 9.14e-06,
      "loss": 0.0031,
      "step": 122580
    },
    {
      "epoch": 6.538133333333334,
      "grad_norm": 0.038286712020635605,
      "learning_rate": 9.136666666666667e-06,
      "loss": 0.0026,
      "step": 122590
    },
    {
      "epoch": 6.538666666666667,
      "grad_norm": 0.06636416167020798,
      "learning_rate": 9.133333333333335e-06,
      "loss": 0.0012,
      "step": 122600
    },
    {
      "epoch": 6.5392,
      "grad_norm": 0.3160146474838257,
      "learning_rate": 9.13e-06,
      "loss": 0.0017,
      "step": 122610
    },
    {
      "epoch": 6.539733333333333,
      "grad_norm": 0.17359447479248047,
      "learning_rate": 9.126666666666667e-06,
      "loss": 0.0015,
      "step": 122620
    },
    {
      "epoch": 6.540266666666667,
      "grad_norm": 0.20503242313861847,
      "learning_rate": 9.123333333333333e-06,
      "loss": 0.0023,
      "step": 122630
    },
    {
      "epoch": 6.5408,
      "grad_norm": 0.14905846118927002,
      "learning_rate": 9.12e-06,
      "loss": 0.0024,
      "step": 122640
    },
    {
      "epoch": 6.541333333333333,
      "grad_norm": 0.14335159957408905,
      "learning_rate": 9.116666666666667e-06,
      "loss": 0.0014,
      "step": 122650
    },
    {
      "epoch": 6.5418666666666665,
      "grad_norm": 0.03616617992520332,
      "learning_rate": 9.113333333333333e-06,
      "loss": 0.0017,
      "step": 122660
    },
    {
      "epoch": 6.5424,
      "grad_norm": 0.2817229926586151,
      "learning_rate": 9.110000000000001e-06,
      "loss": 0.002,
      "step": 122670
    },
    {
      "epoch": 6.542933333333333,
      "grad_norm": 0.04512997716665268,
      "learning_rate": 9.106666666666667e-06,
      "loss": 0.0021,
      "step": 122680
    },
    {
      "epoch": 6.543466666666666,
      "grad_norm": 0.14345568418502808,
      "learning_rate": 9.103333333333333e-06,
      "loss": 0.0014,
      "step": 122690
    },
    {
      "epoch": 6.5440000000000005,
      "grad_norm": 0.17885522544384003,
      "learning_rate": 9.100000000000001e-06,
      "loss": 0.0023,
      "step": 122700
    },
    {
      "epoch": 6.544533333333334,
      "grad_norm": 0.36807698011398315,
      "learning_rate": 9.096666666666667e-06,
      "loss": 0.0016,
      "step": 122710
    },
    {
      "epoch": 6.545066666666667,
      "grad_norm": 0.25964027643203735,
      "learning_rate": 9.093333333333335e-06,
      "loss": 0.0015,
      "step": 122720
    },
    {
      "epoch": 6.5456,
      "grad_norm": 0.0656406432390213,
      "learning_rate": 9.09e-06,
      "loss": 0.002,
      "step": 122730
    },
    {
      "epoch": 6.546133333333334,
      "grad_norm": 0.43083852529525757,
      "learning_rate": 9.086666666666667e-06,
      "loss": 0.002,
      "step": 122740
    },
    {
      "epoch": 6.546666666666667,
      "grad_norm": 0.25996047258377075,
      "learning_rate": 9.083333333333333e-06,
      "loss": 0.0014,
      "step": 122750
    },
    {
      "epoch": 6.5472,
      "grad_norm": 0.12103985995054245,
      "learning_rate": 9.080000000000001e-06,
      "loss": 0.0013,
      "step": 122760
    },
    {
      "epoch": 6.547733333333333,
      "grad_norm": 0.17589369416236877,
      "learning_rate": 9.076666666666667e-06,
      "loss": 0.0015,
      "step": 122770
    },
    {
      "epoch": 6.548266666666667,
      "grad_norm": 0.0931396409869194,
      "learning_rate": 9.073333333333333e-06,
      "loss": 0.0014,
      "step": 122780
    },
    {
      "epoch": 6.5488,
      "grad_norm": 0.051957808434963226,
      "learning_rate": 9.070000000000001e-06,
      "loss": 0.002,
      "step": 122790
    },
    {
      "epoch": 6.549333333333333,
      "grad_norm": 0.17188438773155212,
      "learning_rate": 9.066666666666667e-06,
      "loss": 0.0018,
      "step": 122800
    },
    {
      "epoch": 6.5498666666666665,
      "grad_norm": 0.25597408413887024,
      "learning_rate": 9.063333333333334e-06,
      "loss": 0.0016,
      "step": 122810
    },
    {
      "epoch": 6.5504,
      "grad_norm": 0.16377907991409302,
      "learning_rate": 9.06e-06,
      "loss": 0.0019,
      "step": 122820
    },
    {
      "epoch": 6.550933333333333,
      "grad_norm": 0.06825094670057297,
      "learning_rate": 9.056666666666667e-06,
      "loss": 0.0024,
      "step": 122830
    },
    {
      "epoch": 6.551466666666666,
      "grad_norm": 0.1784265786409378,
      "learning_rate": 9.053333333333334e-06,
      "loss": 0.0015,
      "step": 122840
    },
    {
      "epoch": 6.552,
      "grad_norm": 0.19851499795913696,
      "learning_rate": 9.05e-06,
      "loss": 0.0016,
      "step": 122850
    },
    {
      "epoch": 6.552533333333333,
      "grad_norm": 0.2532966136932373,
      "learning_rate": 9.046666666666668e-06,
      "loss": 0.002,
      "step": 122860
    },
    {
      "epoch": 6.553066666666667,
      "grad_norm": 0.13304835557937622,
      "learning_rate": 9.043333333333334e-06,
      "loss": 0.002,
      "step": 122870
    },
    {
      "epoch": 6.5536,
      "grad_norm": 0.052744947373867035,
      "learning_rate": 9.04e-06,
      "loss": 0.0018,
      "step": 122880
    },
    {
      "epoch": 6.554133333333334,
      "grad_norm": 0.0883614793419838,
      "learning_rate": 9.036666666666668e-06,
      "loss": 0.0025,
      "step": 122890
    },
    {
      "epoch": 6.554666666666667,
      "grad_norm": 0.12174416333436966,
      "learning_rate": 9.033333333333334e-06,
      "loss": 0.0021,
      "step": 122900
    },
    {
      "epoch": 6.5552,
      "grad_norm": 0.23436732590198517,
      "learning_rate": 9.030000000000002e-06,
      "loss": 0.002,
      "step": 122910
    },
    {
      "epoch": 6.555733333333333,
      "grad_norm": 0.17309732735157013,
      "learning_rate": 9.026666666666666e-06,
      "loss": 0.0014,
      "step": 122920
    },
    {
      "epoch": 6.556266666666667,
      "grad_norm": 0.03777783736586571,
      "learning_rate": 9.023333333333334e-06,
      "loss": 0.0019,
      "step": 122930
    },
    {
      "epoch": 6.5568,
      "grad_norm": 0.11740922927856445,
      "learning_rate": 9.02e-06,
      "loss": 0.0013,
      "step": 122940
    },
    {
      "epoch": 6.557333333333333,
      "grad_norm": 0.20549918711185455,
      "learning_rate": 9.016666666666668e-06,
      "loss": 0.0017,
      "step": 122950
    },
    {
      "epoch": 6.5578666666666665,
      "grad_norm": 0.06790488958358765,
      "learning_rate": 9.013333333333334e-06,
      "loss": 0.0013,
      "step": 122960
    },
    {
      "epoch": 6.5584,
      "grad_norm": 0.23215965926647186,
      "learning_rate": 9.01e-06,
      "loss": 0.0017,
      "step": 122970
    },
    {
      "epoch": 6.558933333333333,
      "grad_norm": 0.0246815774589777,
      "learning_rate": 9.006666666666668e-06,
      "loss": 0.0017,
      "step": 122980
    },
    {
      "epoch": 6.559466666666666,
      "grad_norm": 0.01966709829866886,
      "learning_rate": 9.003333333333334e-06,
      "loss": 0.0014,
      "step": 122990
    },
    {
      "epoch": 6.5600000000000005,
      "grad_norm": 0.20595118403434753,
      "learning_rate": 9e-06,
      "loss": 0.0014,
      "step": 123000
    },
    {
      "epoch": 6.560533333333334,
      "grad_norm": 0.28409162163734436,
      "learning_rate": 8.996666666666666e-06,
      "loss": 0.0017,
      "step": 123010
    },
    {
      "epoch": 6.561066666666667,
      "grad_norm": 0.24673280119895935,
      "learning_rate": 8.993333333333334e-06,
      "loss": 0.002,
      "step": 123020
    },
    {
      "epoch": 6.5616,
      "grad_norm": 0.12026095390319824,
      "learning_rate": 8.99e-06,
      "loss": 0.0013,
      "step": 123030
    },
    {
      "epoch": 6.562133333333334,
      "grad_norm": 0.11325220763683319,
      "learning_rate": 8.986666666666666e-06,
      "loss": 0.0015,
      "step": 123040
    },
    {
      "epoch": 6.562666666666667,
      "grad_norm": 0.11772188544273376,
      "learning_rate": 8.983333333333334e-06,
      "loss": 0.0022,
      "step": 123050
    },
    {
      "epoch": 6.5632,
      "grad_norm": 0.13948853313922882,
      "learning_rate": 8.98e-06,
      "loss": 0.003,
      "step": 123060
    },
    {
      "epoch": 6.563733333333333,
      "grad_norm": 0.6316363215446472,
      "learning_rate": 8.976666666666667e-06,
      "loss": 0.0024,
      "step": 123070
    },
    {
      "epoch": 6.564266666666667,
      "grad_norm": 0.025133296847343445,
      "learning_rate": 8.973333333333334e-06,
      "loss": 0.0019,
      "step": 123080
    },
    {
      "epoch": 6.5648,
      "grad_norm": 0.04571075364947319,
      "learning_rate": 8.97e-06,
      "loss": 0.002,
      "step": 123090
    },
    {
      "epoch": 6.565333333333333,
      "grad_norm": 0.17063820362091064,
      "learning_rate": 8.966666666666668e-06,
      "loss": 0.0013,
      "step": 123100
    },
    {
      "epoch": 6.5658666666666665,
      "grad_norm": 0.07118935137987137,
      "learning_rate": 8.963333333333333e-06,
      "loss": 0.0023,
      "step": 123110
    },
    {
      "epoch": 6.5664,
      "grad_norm": 0.060301098972558975,
      "learning_rate": 8.96e-06,
      "loss": 0.0019,
      "step": 123120
    },
    {
      "epoch": 6.566933333333333,
      "grad_norm": 0.25543370842933655,
      "learning_rate": 8.956666666666667e-06,
      "loss": 0.0014,
      "step": 123130
    },
    {
      "epoch": 6.567466666666666,
      "grad_norm": 0.11966363340616226,
      "learning_rate": 8.953333333333335e-06,
      "loss": 0.002,
      "step": 123140
    },
    {
      "epoch": 6.568,
      "grad_norm": 0.28619733452796936,
      "learning_rate": 8.95e-06,
      "loss": 0.0014,
      "step": 123150
    },
    {
      "epoch": 6.568533333333333,
      "grad_norm": 0.09072183072566986,
      "learning_rate": 8.946666666666667e-06,
      "loss": 0.0014,
      "step": 123160
    },
    {
      "epoch": 6.569066666666667,
      "grad_norm": 0.06584788113832474,
      "learning_rate": 8.943333333333335e-06,
      "loss": 0.0018,
      "step": 123170
    },
    {
      "epoch": 6.5696,
      "grad_norm": 0.2875851094722748,
      "learning_rate": 8.939999999999999e-06,
      "loss": 0.0015,
      "step": 123180
    },
    {
      "epoch": 6.570133333333334,
      "grad_norm": 0.35961806774139404,
      "learning_rate": 8.936666666666667e-06,
      "loss": 0.0012,
      "step": 123190
    },
    {
      "epoch": 6.570666666666667,
      "grad_norm": 0.25577643513679504,
      "learning_rate": 8.933333333333333e-06,
      "loss": 0.0026,
      "step": 123200
    },
    {
      "epoch": 6.5712,
      "grad_norm": 0.3036746382713318,
      "learning_rate": 8.930000000000001e-06,
      "loss": 0.002,
      "step": 123210
    },
    {
      "epoch": 6.571733333333333,
      "grad_norm": 0.11976441740989685,
      "learning_rate": 8.926666666666667e-06,
      "loss": 0.0017,
      "step": 123220
    },
    {
      "epoch": 6.572266666666667,
      "grad_norm": 0.11567216366529465,
      "learning_rate": 8.923333333333333e-06,
      "loss": 0.0023,
      "step": 123230
    },
    {
      "epoch": 6.5728,
      "grad_norm": 0.06430377066135406,
      "learning_rate": 8.920000000000001e-06,
      "loss": 0.0018,
      "step": 123240
    },
    {
      "epoch": 6.573333333333333,
      "grad_norm": 0.09320930391550064,
      "learning_rate": 8.916666666666667e-06,
      "loss": 0.0023,
      "step": 123250
    },
    {
      "epoch": 6.5738666666666665,
      "grad_norm": 0.14255113899707794,
      "learning_rate": 8.913333333333333e-06,
      "loss": 0.0022,
      "step": 123260
    },
    {
      "epoch": 6.5744,
      "grad_norm": 0.14857468008995056,
      "learning_rate": 8.910000000000001e-06,
      "loss": 0.0021,
      "step": 123270
    },
    {
      "epoch": 6.574933333333333,
      "grad_norm": 0.11944720894098282,
      "learning_rate": 8.906666666666667e-06,
      "loss": 0.002,
      "step": 123280
    },
    {
      "epoch": 6.575466666666666,
      "grad_norm": 0.09614484012126923,
      "learning_rate": 8.903333333333335e-06,
      "loss": 0.0015,
      "step": 123290
    },
    {
      "epoch": 6.576,
      "grad_norm": 0.5282536149024963,
      "learning_rate": 8.9e-06,
      "loss": 0.0022,
      "step": 123300
    },
    {
      "epoch": 6.576533333333334,
      "grad_norm": 0.29907462000846863,
      "learning_rate": 8.896666666666667e-06,
      "loss": 0.002,
      "step": 123310
    },
    {
      "epoch": 6.577066666666667,
      "grad_norm": 0.17450416088104248,
      "learning_rate": 8.893333333333333e-06,
      "loss": 0.0015,
      "step": 123320
    },
    {
      "epoch": 6.5776,
      "grad_norm": 0.039305947721004486,
      "learning_rate": 8.890000000000001e-06,
      "loss": 0.0025,
      "step": 123330
    },
    {
      "epoch": 6.578133333333334,
      "grad_norm": 0.14654654264450073,
      "learning_rate": 8.886666666666667e-06,
      "loss": 0.0022,
      "step": 123340
    },
    {
      "epoch": 6.578666666666667,
      "grad_norm": 0.07871027290821075,
      "learning_rate": 8.883333333333334e-06,
      "loss": 0.0012,
      "step": 123350
    },
    {
      "epoch": 6.5792,
      "grad_norm": 0.204549640417099,
      "learning_rate": 8.880000000000001e-06,
      "loss": 0.0014,
      "step": 123360
    },
    {
      "epoch": 6.579733333333333,
      "grad_norm": 0.06278754025697708,
      "learning_rate": 8.876666666666666e-06,
      "loss": 0.0025,
      "step": 123370
    },
    {
      "epoch": 6.580266666666667,
      "grad_norm": 0.0407668799161911,
      "learning_rate": 8.873333333333334e-06,
      "loss": 0.0018,
      "step": 123380
    },
    {
      "epoch": 6.5808,
      "grad_norm": 0.4045787751674652,
      "learning_rate": 8.87e-06,
      "loss": 0.0029,
      "step": 123390
    },
    {
      "epoch": 6.581333333333333,
      "grad_norm": 0.11602293699979782,
      "learning_rate": 8.866666666666668e-06,
      "loss": 0.0014,
      "step": 123400
    },
    {
      "epoch": 6.5818666666666665,
      "grad_norm": 0.08705974370241165,
      "learning_rate": 8.863333333333334e-06,
      "loss": 0.0015,
      "step": 123410
    },
    {
      "epoch": 6.5824,
      "grad_norm": 0.22003495693206787,
      "learning_rate": 8.86e-06,
      "loss": 0.0015,
      "step": 123420
    },
    {
      "epoch": 6.582933333333333,
      "grad_norm": 0.23600023984909058,
      "learning_rate": 8.856666666666668e-06,
      "loss": 0.0029,
      "step": 123430
    },
    {
      "epoch": 6.583466666666666,
      "grad_norm": 0.09566958248615265,
      "learning_rate": 8.853333333333334e-06,
      "loss": 0.0015,
      "step": 123440
    },
    {
      "epoch": 6.584,
      "grad_norm": 0.42622873187065125,
      "learning_rate": 8.85e-06,
      "loss": 0.0013,
      "step": 123450
    },
    {
      "epoch": 6.584533333333333,
      "grad_norm": 0.1721487194299698,
      "learning_rate": 8.846666666666668e-06,
      "loss": 0.0013,
      "step": 123460
    },
    {
      "epoch": 6.585066666666666,
      "grad_norm": 0.05843232944607735,
      "learning_rate": 8.843333333333334e-06,
      "loss": 0.0023,
      "step": 123470
    },
    {
      "epoch": 6.5856,
      "grad_norm": 0.12343384325504303,
      "learning_rate": 8.840000000000002e-06,
      "loss": 0.0016,
      "step": 123480
    },
    {
      "epoch": 6.586133333333334,
      "grad_norm": 0.20254497230052948,
      "learning_rate": 8.836666666666666e-06,
      "loss": 0.0031,
      "step": 123490
    },
    {
      "epoch": 6.586666666666667,
      "grad_norm": 0.11886576563119888,
      "learning_rate": 8.833333333333334e-06,
      "loss": 0.0018,
      "step": 123500
    },
    {
      "epoch": 6.5872,
      "grad_norm": 0.04254554957151413,
      "learning_rate": 8.83e-06,
      "loss": 0.0021,
      "step": 123510
    },
    {
      "epoch": 6.587733333333333,
      "grad_norm": 0.0613982118666172,
      "learning_rate": 8.826666666666666e-06,
      "loss": 0.0016,
      "step": 123520
    },
    {
      "epoch": 6.588266666666667,
      "grad_norm": 0.26278120279312134,
      "learning_rate": 8.823333333333334e-06,
      "loss": 0.0016,
      "step": 123530
    },
    {
      "epoch": 6.5888,
      "grad_norm": 0.22017501294612885,
      "learning_rate": 8.82e-06,
      "loss": 0.0015,
      "step": 123540
    },
    {
      "epoch": 6.589333333333333,
      "grad_norm": 0.0444817952811718,
      "learning_rate": 8.816666666666668e-06,
      "loss": 0.0014,
      "step": 123550
    },
    {
      "epoch": 6.5898666666666665,
      "grad_norm": 0.12382658571004868,
      "learning_rate": 8.813333333333333e-06,
      "loss": 0.0017,
      "step": 123560
    },
    {
      "epoch": 6.5904,
      "grad_norm": 0.2824085056781769,
      "learning_rate": 8.81e-06,
      "loss": 0.002,
      "step": 123570
    },
    {
      "epoch": 6.590933333333333,
      "grad_norm": 0.10947966575622559,
      "learning_rate": 8.806666666666666e-06,
      "loss": 0.0016,
      "step": 123580
    },
    {
      "epoch": 6.591466666666666,
      "grad_norm": 0.28633913397789,
      "learning_rate": 8.803333333333334e-06,
      "loss": 0.0018,
      "step": 123590
    },
    {
      "epoch": 6.592,
      "grad_norm": 0.23090657591819763,
      "learning_rate": 8.8e-06,
      "loss": 0.0016,
      "step": 123600
    },
    {
      "epoch": 6.592533333333334,
      "grad_norm": 0.2649860084056854,
      "learning_rate": 8.796666666666667e-06,
      "loss": 0.0012,
      "step": 123610
    },
    {
      "epoch": 6.593066666666667,
      "grad_norm": 0.20294615626335144,
      "learning_rate": 8.793333333333334e-06,
      "loss": 0.0015,
      "step": 123620
    },
    {
      "epoch": 6.5936,
      "grad_norm": 0.058841634541749954,
      "learning_rate": 8.79e-06,
      "loss": 0.0019,
      "step": 123630
    },
    {
      "epoch": 6.594133333333334,
      "grad_norm": 0.054881565272808075,
      "learning_rate": 8.786666666666667e-06,
      "loss": 0.0025,
      "step": 123640
    },
    {
      "epoch": 6.594666666666667,
      "grad_norm": 0.3169358968734741,
      "learning_rate": 8.783333333333335e-06,
      "loss": 0.0024,
      "step": 123650
    },
    {
      "epoch": 6.5952,
      "grad_norm": 0.11553271859884262,
      "learning_rate": 8.78e-06,
      "loss": 0.0023,
      "step": 123660
    },
    {
      "epoch": 6.5957333333333334,
      "grad_norm": 0.10291419178247452,
      "learning_rate": 8.776666666666668e-06,
      "loss": 0.0016,
      "step": 123670
    },
    {
      "epoch": 6.596266666666667,
      "grad_norm": 0.17291933298110962,
      "learning_rate": 8.773333333333333e-06,
      "loss": 0.0019,
      "step": 123680
    },
    {
      "epoch": 6.5968,
      "grad_norm": 0.2046261578798294,
      "learning_rate": 8.77e-06,
      "loss": 0.0013,
      "step": 123690
    },
    {
      "epoch": 6.597333333333333,
      "grad_norm": 0.15136471390724182,
      "learning_rate": 8.766666666666667e-06,
      "loss": 0.002,
      "step": 123700
    },
    {
      "epoch": 6.5978666666666665,
      "grad_norm": 0.5040483474731445,
      "learning_rate": 8.763333333333333e-06,
      "loss": 0.0028,
      "step": 123710
    },
    {
      "epoch": 6.5984,
      "grad_norm": 0.14297008514404297,
      "learning_rate": 8.76e-06,
      "loss": 0.0014,
      "step": 123720
    },
    {
      "epoch": 6.598933333333333,
      "grad_norm": 0.28660768270492554,
      "learning_rate": 8.756666666666667e-06,
      "loss": 0.0022,
      "step": 123730
    },
    {
      "epoch": 6.599466666666666,
      "grad_norm": 0.2617175579071045,
      "learning_rate": 8.753333333333335e-06,
      "loss": 0.0025,
      "step": 123740
    },
    {
      "epoch": 6.6,
      "grad_norm": 0.27254605293273926,
      "learning_rate": 8.75e-06,
      "loss": 0.0019,
      "step": 123750
    },
    {
      "epoch": 6.600533333333333,
      "grad_norm": 0.12512223422527313,
      "learning_rate": 8.746666666666667e-06,
      "loss": 0.0022,
      "step": 123760
    },
    {
      "epoch": 6.601066666666666,
      "grad_norm": 0.23157858848571777,
      "learning_rate": 8.743333333333333e-06,
      "loss": 0.0026,
      "step": 123770
    },
    {
      "epoch": 6.6016,
      "grad_norm": 0.15962789952754974,
      "learning_rate": 8.740000000000001e-06,
      "loss": 0.0012,
      "step": 123780
    },
    {
      "epoch": 6.602133333333334,
      "grad_norm": 0.06683684140443802,
      "learning_rate": 8.736666666666667e-06,
      "loss": 0.0015,
      "step": 123790
    },
    {
      "epoch": 6.602666666666667,
      "grad_norm": 0.03948434069752693,
      "learning_rate": 8.733333333333333e-06,
      "loss": 0.0014,
      "step": 123800
    },
    {
      "epoch": 6.6032,
      "grad_norm": 0.059481021016836166,
      "learning_rate": 8.730000000000001e-06,
      "loss": 0.0023,
      "step": 123810
    },
    {
      "epoch": 6.6037333333333335,
      "grad_norm": 0.0800958201289177,
      "learning_rate": 8.726666666666667e-06,
      "loss": 0.0019,
      "step": 123820
    },
    {
      "epoch": 6.604266666666667,
      "grad_norm": 0.09650041908025742,
      "learning_rate": 8.723333333333333e-06,
      "loss": 0.0015,
      "step": 123830
    },
    {
      "epoch": 6.6048,
      "grad_norm": 0.06432078033685684,
      "learning_rate": 8.720000000000001e-06,
      "loss": 0.0024,
      "step": 123840
    },
    {
      "epoch": 6.605333333333333,
      "grad_norm": 0.0733601450920105,
      "learning_rate": 8.716666666666667e-06,
      "loss": 0.002,
      "step": 123850
    },
    {
      "epoch": 6.6058666666666666,
      "grad_norm": 0.19973407685756683,
      "learning_rate": 8.713333333333333e-06,
      "loss": 0.0024,
      "step": 123860
    },
    {
      "epoch": 6.6064,
      "grad_norm": 0.038556549698114395,
      "learning_rate": 8.71e-06,
      "loss": 0.0016,
      "step": 123870
    },
    {
      "epoch": 6.606933333333333,
      "grad_norm": 0.26424577832221985,
      "learning_rate": 8.706666666666667e-06,
      "loss": 0.0015,
      "step": 123880
    },
    {
      "epoch": 6.607466666666666,
      "grad_norm": 0.5985536575317383,
      "learning_rate": 8.703333333333334e-06,
      "loss": 0.0013,
      "step": 123890
    },
    {
      "epoch": 6.608,
      "grad_norm": 0.08921774476766586,
      "learning_rate": 8.7e-06,
      "loss": 0.0027,
      "step": 123900
    },
    {
      "epoch": 6.608533333333334,
      "grad_norm": 0.33525681495666504,
      "learning_rate": 8.696666666666668e-06,
      "loss": 0.003,
      "step": 123910
    },
    {
      "epoch": 6.609066666666667,
      "grad_norm": 0.25747445225715637,
      "learning_rate": 8.693333333333334e-06,
      "loss": 0.0014,
      "step": 123920
    },
    {
      "epoch": 6.6096,
      "grad_norm": 0.2894211709499359,
      "learning_rate": 8.690000000000002e-06,
      "loss": 0.0014,
      "step": 123930
    },
    {
      "epoch": 6.610133333333334,
      "grad_norm": 0.5258745551109314,
      "learning_rate": 8.686666666666666e-06,
      "loss": 0.0019,
      "step": 123940
    },
    {
      "epoch": 6.610666666666667,
      "grad_norm": 0.016307733952999115,
      "learning_rate": 8.683333333333334e-06,
      "loss": 0.0011,
      "step": 123950
    },
    {
      "epoch": 6.6112,
      "grad_norm": 0.31010115146636963,
      "learning_rate": 8.68e-06,
      "loss": 0.0017,
      "step": 123960
    },
    {
      "epoch": 6.6117333333333335,
      "grad_norm": 0.278974324464798,
      "learning_rate": 8.676666666666668e-06,
      "loss": 0.0023,
      "step": 123970
    },
    {
      "epoch": 6.612266666666667,
      "grad_norm": 0.3415051996707916,
      "learning_rate": 8.673333333333334e-06,
      "loss": 0.0015,
      "step": 123980
    },
    {
      "epoch": 6.6128,
      "grad_norm": 0.0969659835100174,
      "learning_rate": 8.67e-06,
      "loss": 0.0017,
      "step": 123990
    },
    {
      "epoch": 6.613333333333333,
      "grad_norm": 0.08711455762386322,
      "learning_rate": 8.666666666666668e-06,
      "loss": 0.0017,
      "step": 124000
    },
    {
      "epoch": 6.613866666666667,
      "grad_norm": 0.2889420688152313,
      "learning_rate": 8.663333333333334e-06,
      "loss": 0.0022,
      "step": 124010
    },
    {
      "epoch": 6.6144,
      "grad_norm": 0.07376641035079956,
      "learning_rate": 8.66e-06,
      "loss": 0.0012,
      "step": 124020
    },
    {
      "epoch": 6.614933333333333,
      "grad_norm": 0.0344155989587307,
      "learning_rate": 8.656666666666668e-06,
      "loss": 0.0015,
      "step": 124030
    },
    {
      "epoch": 6.615466666666666,
      "grad_norm": 0.33996137976646423,
      "learning_rate": 8.653333333333334e-06,
      "loss": 0.0015,
      "step": 124040
    },
    {
      "epoch": 6.616,
      "grad_norm": 0.048173610121011734,
      "learning_rate": 8.65e-06,
      "loss": 0.0019,
      "step": 124050
    },
    {
      "epoch": 6.616533333333333,
      "grad_norm": 0.3184734582901001,
      "learning_rate": 8.646666666666666e-06,
      "loss": 0.0016,
      "step": 124060
    },
    {
      "epoch": 6.617066666666666,
      "grad_norm": 0.11924190819263458,
      "learning_rate": 8.643333333333334e-06,
      "loss": 0.0015,
      "step": 124070
    },
    {
      "epoch": 6.6176,
      "grad_norm": 0.08593922108411789,
      "learning_rate": 8.64e-06,
      "loss": 0.0029,
      "step": 124080
    },
    {
      "epoch": 6.618133333333334,
      "grad_norm": 0.09821956604719162,
      "learning_rate": 8.636666666666666e-06,
      "loss": 0.0021,
      "step": 124090
    },
    {
      "epoch": 6.618666666666667,
      "grad_norm": 0.05817553028464317,
      "learning_rate": 8.633333333333334e-06,
      "loss": 0.0016,
      "step": 124100
    },
    {
      "epoch": 6.6192,
      "grad_norm": 0.09180646389722824,
      "learning_rate": 8.63e-06,
      "loss": 0.0013,
      "step": 124110
    },
    {
      "epoch": 6.6197333333333335,
      "grad_norm": 0.40213507413864136,
      "learning_rate": 8.626666666666668e-06,
      "loss": 0.0014,
      "step": 124120
    },
    {
      "epoch": 6.620266666666667,
      "grad_norm": 0.1455833911895752,
      "learning_rate": 8.623333333333333e-06,
      "loss": 0.0013,
      "step": 124130
    },
    {
      "epoch": 6.6208,
      "grad_norm": 0.08849874138832092,
      "learning_rate": 8.62e-06,
      "loss": 0.0013,
      "step": 124140
    },
    {
      "epoch": 6.621333333333333,
      "grad_norm": 0.1807367503643036,
      "learning_rate": 8.616666666666667e-06,
      "loss": 0.0014,
      "step": 124150
    },
    {
      "epoch": 6.621866666666667,
      "grad_norm": 0.17759846150875092,
      "learning_rate": 8.613333333333334e-06,
      "loss": 0.0018,
      "step": 124160
    },
    {
      "epoch": 6.6224,
      "grad_norm": 0.11718357354402542,
      "learning_rate": 8.61e-06,
      "loss": 0.0018,
      "step": 124170
    },
    {
      "epoch": 6.622933333333333,
      "grad_norm": 0.23114652931690216,
      "learning_rate": 8.606666666666667e-06,
      "loss": 0.002,
      "step": 124180
    },
    {
      "epoch": 6.623466666666666,
      "grad_norm": 0.03428923338651657,
      "learning_rate": 8.603333333333335e-06,
      "loss": 0.0028,
      "step": 124190
    },
    {
      "epoch": 6.624,
      "grad_norm": 0.20335176587104797,
      "learning_rate": 8.599999999999999e-06,
      "loss": 0.0012,
      "step": 124200
    },
    {
      "epoch": 6.624533333333334,
      "grad_norm": 0.128614604473114,
      "learning_rate": 8.596666666666667e-06,
      "loss": 0.002,
      "step": 124210
    },
    {
      "epoch": 6.625066666666667,
      "grad_norm": 0.17152459919452667,
      "learning_rate": 8.593333333333335e-06,
      "loss": 0.0014,
      "step": 124220
    },
    {
      "epoch": 6.6256,
      "grad_norm": 0.16419921815395355,
      "learning_rate": 8.59e-06,
      "loss": 0.002,
      "step": 124230
    },
    {
      "epoch": 6.626133333333334,
      "grad_norm": 0.10418695956468582,
      "learning_rate": 8.586666666666667e-06,
      "loss": 0.0019,
      "step": 124240
    },
    {
      "epoch": 6.626666666666667,
      "grad_norm": 0.15692420303821564,
      "learning_rate": 8.583333333333333e-06,
      "loss": 0.0018,
      "step": 124250
    },
    {
      "epoch": 6.6272,
      "grad_norm": 0.14538775384426117,
      "learning_rate": 8.580000000000001e-06,
      "loss": 0.0018,
      "step": 124260
    },
    {
      "epoch": 6.6277333333333335,
      "grad_norm": 0.17693853378295898,
      "learning_rate": 8.576666666666667e-06,
      "loss": 0.0014,
      "step": 124270
    },
    {
      "epoch": 6.628266666666667,
      "grad_norm": 0.20423980057239532,
      "learning_rate": 8.573333333333333e-06,
      "loss": 0.0022,
      "step": 124280
    },
    {
      "epoch": 6.6288,
      "grad_norm": 0.17790058255195618,
      "learning_rate": 8.570000000000001e-06,
      "loss": 0.0016,
      "step": 124290
    },
    {
      "epoch": 6.629333333333333,
      "grad_norm": 0.14342547953128815,
      "learning_rate": 8.566666666666667e-06,
      "loss": 0.0015,
      "step": 124300
    },
    {
      "epoch": 6.629866666666667,
      "grad_norm": 0.18371127545833588,
      "learning_rate": 8.563333333333335e-06,
      "loss": 0.0014,
      "step": 124310
    },
    {
      "epoch": 6.6304,
      "grad_norm": 0.22791525721549988,
      "learning_rate": 8.56e-06,
      "loss": 0.002,
      "step": 124320
    },
    {
      "epoch": 6.630933333333333,
      "grad_norm": 0.3160361349582672,
      "learning_rate": 8.556666666666667e-06,
      "loss": 0.0016,
      "step": 124330
    },
    {
      "epoch": 6.631466666666666,
      "grad_norm": 0.1496134251356125,
      "learning_rate": 8.553333333333333e-06,
      "loss": 0.0026,
      "step": 124340
    },
    {
      "epoch": 6.632,
      "grad_norm": 0.29100045561790466,
      "learning_rate": 8.550000000000001e-06,
      "loss": 0.0024,
      "step": 124350
    },
    {
      "epoch": 6.632533333333333,
      "grad_norm": 0.05347485840320587,
      "learning_rate": 8.546666666666667e-06,
      "loss": 0.0018,
      "step": 124360
    },
    {
      "epoch": 6.633066666666666,
      "grad_norm": 0.12191461771726608,
      "learning_rate": 8.543333333333333e-06,
      "loss": 0.0021,
      "step": 124370
    },
    {
      "epoch": 6.6336,
      "grad_norm": 0.07095243781805038,
      "learning_rate": 8.540000000000001e-06,
      "loss": 0.0023,
      "step": 124380
    },
    {
      "epoch": 6.634133333333334,
      "grad_norm": 0.11371371895074844,
      "learning_rate": 8.536666666666666e-06,
      "loss": 0.0017,
      "step": 124390
    },
    {
      "epoch": 6.634666666666667,
      "grad_norm": 0.09223401546478271,
      "learning_rate": 8.533333333333334e-06,
      "loss": 0.0018,
      "step": 124400
    },
    {
      "epoch": 6.6352,
      "grad_norm": 0.12450926750898361,
      "learning_rate": 8.53e-06,
      "loss": 0.0015,
      "step": 124410
    },
    {
      "epoch": 6.6357333333333335,
      "grad_norm": 0.08829325437545776,
      "learning_rate": 8.526666666666667e-06,
      "loss": 0.0013,
      "step": 124420
    },
    {
      "epoch": 6.636266666666667,
      "grad_norm": 0.05821053683757782,
      "learning_rate": 8.523333333333334e-06,
      "loss": 0.0018,
      "step": 124430
    },
    {
      "epoch": 6.6368,
      "grad_norm": 0.051391519606113434,
      "learning_rate": 8.52e-06,
      "loss": 0.0023,
      "step": 124440
    },
    {
      "epoch": 6.637333333333333,
      "grad_norm": 0.09774114936590195,
      "learning_rate": 8.516666666666668e-06,
      "loss": 0.0013,
      "step": 124450
    },
    {
      "epoch": 6.637866666666667,
      "grad_norm": 0.16591325402259827,
      "learning_rate": 8.513333333333334e-06,
      "loss": 0.0027,
      "step": 124460
    },
    {
      "epoch": 6.6384,
      "grad_norm": 0.29395803809165955,
      "learning_rate": 8.51e-06,
      "loss": 0.0023,
      "step": 124470
    },
    {
      "epoch": 6.638933333333333,
      "grad_norm": 0.1723962277173996,
      "learning_rate": 8.506666666666668e-06,
      "loss": 0.0022,
      "step": 124480
    },
    {
      "epoch": 6.639466666666666,
      "grad_norm": 0.2500905692577362,
      "learning_rate": 8.503333333333334e-06,
      "loss": 0.0016,
      "step": 124490
    },
    {
      "epoch": 6.64,
      "grad_norm": 0.14360401034355164,
      "learning_rate": 8.500000000000002e-06,
      "loss": 0.0016,
      "step": 124500
    },
    {
      "epoch": 6.640533333333333,
      "grad_norm": 0.4622589945793152,
      "learning_rate": 8.496666666666666e-06,
      "loss": 0.0022,
      "step": 124510
    },
    {
      "epoch": 6.641066666666667,
      "grad_norm": 0.26713889837265015,
      "learning_rate": 8.493333333333334e-06,
      "loss": 0.0018,
      "step": 124520
    },
    {
      "epoch": 6.6416,
      "grad_norm": 0.17857547104358673,
      "learning_rate": 8.49e-06,
      "loss": 0.0015,
      "step": 124530
    },
    {
      "epoch": 6.642133333333334,
      "grad_norm": 0.2907172441482544,
      "learning_rate": 8.486666666666668e-06,
      "loss": 0.0027,
      "step": 124540
    },
    {
      "epoch": 6.642666666666667,
      "grad_norm": 0.04104407876729965,
      "learning_rate": 8.483333333333334e-06,
      "loss": 0.0012,
      "step": 124550
    },
    {
      "epoch": 6.6432,
      "grad_norm": 0.38751474022865295,
      "learning_rate": 8.48e-06,
      "loss": 0.0017,
      "step": 124560
    },
    {
      "epoch": 6.6437333333333335,
      "grad_norm": 0.3768593966960907,
      "learning_rate": 8.476666666666668e-06,
      "loss": 0.0019,
      "step": 124570
    },
    {
      "epoch": 6.644266666666667,
      "grad_norm": 0.5124556422233582,
      "learning_rate": 8.473333333333332e-06,
      "loss": 0.002,
      "step": 124580
    },
    {
      "epoch": 6.6448,
      "grad_norm": 0.046687621623277664,
      "learning_rate": 8.47e-06,
      "loss": 0.0015,
      "step": 124590
    },
    {
      "epoch": 6.645333333333333,
      "grad_norm": 0.43022575974464417,
      "learning_rate": 8.466666666666666e-06,
      "loss": 0.0013,
      "step": 124600
    },
    {
      "epoch": 6.645866666666667,
      "grad_norm": 0.4338594377040863,
      "learning_rate": 8.463333333333334e-06,
      "loss": 0.0017,
      "step": 124610
    },
    {
      "epoch": 6.6464,
      "grad_norm": 0.12699735164642334,
      "learning_rate": 8.46e-06,
      "loss": 0.002,
      "step": 124620
    },
    {
      "epoch": 6.646933333333333,
      "grad_norm": 0.16229425370693207,
      "learning_rate": 8.456666666666666e-06,
      "loss": 0.0025,
      "step": 124630
    },
    {
      "epoch": 6.647466666666666,
      "grad_norm": 0.2587074339389801,
      "learning_rate": 8.453333333333334e-06,
      "loss": 0.0017,
      "step": 124640
    },
    {
      "epoch": 6.648,
      "grad_norm": 0.054639652371406555,
      "learning_rate": 8.45e-06,
      "loss": 0.0021,
      "step": 124650
    },
    {
      "epoch": 6.648533333333333,
      "grad_norm": 0.08937274664640427,
      "learning_rate": 8.446666666666667e-06,
      "loss": 0.0027,
      "step": 124660
    },
    {
      "epoch": 6.649066666666666,
      "grad_norm": 0.11495096981525421,
      "learning_rate": 8.443333333333334e-06,
      "loss": 0.0016,
      "step": 124670
    },
    {
      "epoch": 6.6495999999999995,
      "grad_norm": 0.05762862786650658,
      "learning_rate": 8.44e-06,
      "loss": 0.0025,
      "step": 124680
    },
    {
      "epoch": 6.650133333333334,
      "grad_norm": 0.11788833141326904,
      "learning_rate": 8.436666666666668e-06,
      "loss": 0.0018,
      "step": 124690
    },
    {
      "epoch": 6.650666666666667,
      "grad_norm": 0.26228436827659607,
      "learning_rate": 8.433333333333333e-06,
      "loss": 0.0014,
      "step": 124700
    },
    {
      "epoch": 6.6512,
      "grad_norm": 0.22626511752605438,
      "learning_rate": 8.43e-06,
      "loss": 0.0022,
      "step": 124710
    },
    {
      "epoch": 6.6517333333333335,
      "grad_norm": 0.025683941319584846,
      "learning_rate": 8.426666666666667e-06,
      "loss": 0.0015,
      "step": 124720
    },
    {
      "epoch": 6.652266666666667,
      "grad_norm": 0.0555206798017025,
      "learning_rate": 8.423333333333333e-06,
      "loss": 0.0026,
      "step": 124730
    },
    {
      "epoch": 6.6528,
      "grad_norm": 0.11392468214035034,
      "learning_rate": 8.42e-06,
      "loss": 0.0019,
      "step": 124740
    },
    {
      "epoch": 6.653333333333333,
      "grad_norm": 0.3282710015773773,
      "learning_rate": 8.416666666666667e-06,
      "loss": 0.0024,
      "step": 124750
    },
    {
      "epoch": 6.653866666666667,
      "grad_norm": 0.36749663949012756,
      "learning_rate": 8.413333333333335e-06,
      "loss": 0.0016,
      "step": 124760
    },
    {
      "epoch": 6.6544,
      "grad_norm": 0.17590166628360748,
      "learning_rate": 8.409999999999999e-06,
      "loss": 0.0017,
      "step": 124770
    },
    {
      "epoch": 6.654933333333333,
      "grad_norm": 0.4613681137561798,
      "learning_rate": 8.406666666666667e-06,
      "loss": 0.0014,
      "step": 124780
    },
    {
      "epoch": 6.655466666666666,
      "grad_norm": 0.44879987835884094,
      "learning_rate": 8.403333333333333e-06,
      "loss": 0.0015,
      "step": 124790
    },
    {
      "epoch": 6.656,
      "grad_norm": 0.058314062654972076,
      "learning_rate": 8.400000000000001e-06,
      "loss": 0.0014,
      "step": 124800
    },
    {
      "epoch": 6.656533333333333,
      "grad_norm": 0.24120575189590454,
      "learning_rate": 8.396666666666667e-06,
      "loss": 0.0019,
      "step": 124810
    },
    {
      "epoch": 6.657066666666667,
      "grad_norm": 0.1714859902858734,
      "learning_rate": 8.393333333333333e-06,
      "loss": 0.0024,
      "step": 124820
    },
    {
      "epoch": 6.6576,
      "grad_norm": 0.08727967739105225,
      "learning_rate": 8.390000000000001e-06,
      "loss": 0.0017,
      "step": 124830
    },
    {
      "epoch": 6.658133333333334,
      "grad_norm": 0.17661623656749725,
      "learning_rate": 8.386666666666667e-06,
      "loss": 0.0028,
      "step": 124840
    },
    {
      "epoch": 6.658666666666667,
      "grad_norm": 0.11591984331607819,
      "learning_rate": 8.383333333333333e-06,
      "loss": 0.0024,
      "step": 124850
    },
    {
      "epoch": 6.6592,
      "grad_norm": 0.15868723392486572,
      "learning_rate": 8.380000000000001e-06,
      "loss": 0.0015,
      "step": 124860
    },
    {
      "epoch": 6.6597333333333335,
      "grad_norm": 0.08780228346586227,
      "learning_rate": 8.376666666666667e-06,
      "loss": 0.0018,
      "step": 124870
    },
    {
      "epoch": 6.660266666666667,
      "grad_norm": 0.12189753353595734,
      "learning_rate": 8.373333333333335e-06,
      "loss": 0.0023,
      "step": 124880
    },
    {
      "epoch": 6.6608,
      "grad_norm": 0.28808706998825073,
      "learning_rate": 8.37e-06,
      "loss": 0.0021,
      "step": 124890
    },
    {
      "epoch": 6.661333333333333,
      "grad_norm": 0.24135299026966095,
      "learning_rate": 8.366666666666667e-06,
      "loss": 0.0015,
      "step": 124900
    },
    {
      "epoch": 6.661866666666667,
      "grad_norm": 0.18865768611431122,
      "learning_rate": 8.363333333333333e-06,
      "loss": 0.0015,
      "step": 124910
    },
    {
      "epoch": 6.6624,
      "grad_norm": 0.542565643787384,
      "learning_rate": 8.36e-06,
      "loss": 0.0018,
      "step": 124920
    },
    {
      "epoch": 6.662933333333333,
      "grad_norm": 0.36937621235847473,
      "learning_rate": 8.356666666666667e-06,
      "loss": 0.0016,
      "step": 124930
    },
    {
      "epoch": 6.663466666666666,
      "grad_norm": 0.14503169059753418,
      "learning_rate": 8.353333333333334e-06,
      "loss": 0.0014,
      "step": 124940
    },
    {
      "epoch": 6.664,
      "grad_norm": 0.4831438660621643,
      "learning_rate": 8.350000000000001e-06,
      "loss": 0.0019,
      "step": 124950
    },
    {
      "epoch": 6.664533333333333,
      "grad_norm": 0.46966737508773804,
      "learning_rate": 8.346666666666666e-06,
      "loss": 0.0019,
      "step": 124960
    },
    {
      "epoch": 6.665066666666666,
      "grad_norm": 0.17777012288570404,
      "learning_rate": 8.343333333333334e-06,
      "loss": 0.0014,
      "step": 124970
    },
    {
      "epoch": 6.6655999999999995,
      "grad_norm": 0.2566705346107483,
      "learning_rate": 8.34e-06,
      "loss": 0.0021,
      "step": 124980
    },
    {
      "epoch": 6.666133333333334,
      "grad_norm": 0.18285612761974335,
      "learning_rate": 8.336666666666668e-06,
      "loss": 0.0019,
      "step": 124990
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 0.5094466209411621,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.0014,
      "step": 125000
    },
    {
      "epoch": 6.6672,
      "grad_norm": 0.23018309473991394,
      "learning_rate": 8.33e-06,
      "loss": 0.0016,
      "step": 125010
    },
    {
      "epoch": 6.6677333333333335,
      "grad_norm": 0.07686595618724823,
      "learning_rate": 8.326666666666668e-06,
      "loss": 0.0024,
      "step": 125020
    },
    {
      "epoch": 6.668266666666667,
      "grad_norm": 0.40120306611061096,
      "learning_rate": 8.323333333333334e-06,
      "loss": 0.0027,
      "step": 125030
    },
    {
      "epoch": 6.6688,
      "grad_norm": 0.1022585928440094,
      "learning_rate": 8.32e-06,
      "loss": 0.0017,
      "step": 125040
    },
    {
      "epoch": 6.669333333333333,
      "grad_norm": 0.1436186283826828,
      "learning_rate": 8.316666666666668e-06,
      "loss": 0.0028,
      "step": 125050
    },
    {
      "epoch": 6.669866666666667,
      "grad_norm": 0.19679169356822968,
      "learning_rate": 8.313333333333334e-06,
      "loss": 0.0021,
      "step": 125060
    },
    {
      "epoch": 6.6704,
      "grad_norm": 0.26292091608047485,
      "learning_rate": 8.31e-06,
      "loss": 0.0025,
      "step": 125070
    },
    {
      "epoch": 6.670933333333333,
      "grad_norm": 0.06610558182001114,
      "learning_rate": 8.306666666666666e-06,
      "loss": 0.0015,
      "step": 125080
    },
    {
      "epoch": 6.671466666666666,
      "grad_norm": 0.1694359928369522,
      "learning_rate": 8.303333333333334e-06,
      "loss": 0.0017,
      "step": 125090
    },
    {
      "epoch": 6.672,
      "grad_norm": 0.0606054849922657,
      "learning_rate": 8.3e-06,
      "loss": 0.0016,
      "step": 125100
    },
    {
      "epoch": 6.672533333333333,
      "grad_norm": 0.11334460973739624,
      "learning_rate": 8.296666666666666e-06,
      "loss": 0.0018,
      "step": 125110
    },
    {
      "epoch": 6.673066666666667,
      "grad_norm": 0.06032750755548477,
      "learning_rate": 8.293333333333334e-06,
      "loss": 0.0016,
      "step": 125120
    },
    {
      "epoch": 6.6736,
      "grad_norm": 0.3432528078556061,
      "learning_rate": 8.29e-06,
      "loss": 0.0013,
      "step": 125130
    },
    {
      "epoch": 6.674133333333334,
      "grad_norm": 0.085527203977108,
      "learning_rate": 8.286666666666668e-06,
      "loss": 0.0024,
      "step": 125140
    },
    {
      "epoch": 6.674666666666667,
      "grad_norm": 0.0688159167766571,
      "learning_rate": 8.283333333333333e-06,
      "loss": 0.0021,
      "step": 125150
    },
    {
      "epoch": 6.6752,
      "grad_norm": 0.7527291178703308,
      "learning_rate": 8.28e-06,
      "loss": 0.0018,
      "step": 125160
    },
    {
      "epoch": 6.6757333333333335,
      "grad_norm": 0.2944929301738739,
      "learning_rate": 8.276666666666666e-06,
      "loss": 0.0024,
      "step": 125170
    },
    {
      "epoch": 6.676266666666667,
      "grad_norm": 0.09219866991043091,
      "learning_rate": 8.273333333333334e-06,
      "loss": 0.002,
      "step": 125180
    },
    {
      "epoch": 6.6768,
      "grad_norm": 0.13625451922416687,
      "learning_rate": 8.27e-06,
      "loss": 0.0016,
      "step": 125190
    },
    {
      "epoch": 6.677333333333333,
      "grad_norm": 0.20280589163303375,
      "learning_rate": 8.266666666666667e-06,
      "loss": 0.002,
      "step": 125200
    },
    {
      "epoch": 6.677866666666667,
      "grad_norm": 0.09399963170289993,
      "learning_rate": 8.263333333333334e-06,
      "loss": 0.0018,
      "step": 125210
    },
    {
      "epoch": 6.6784,
      "grad_norm": 0.16242057085037231,
      "learning_rate": 8.26e-06,
      "loss": 0.0016,
      "step": 125220
    },
    {
      "epoch": 6.678933333333333,
      "grad_norm": 0.23029407858848572,
      "learning_rate": 8.256666666666667e-06,
      "loss": 0.0017,
      "step": 125230
    },
    {
      "epoch": 6.679466666666666,
      "grad_norm": 0.20122438669204712,
      "learning_rate": 8.253333333333334e-06,
      "loss": 0.0016,
      "step": 125240
    },
    {
      "epoch": 6.68,
      "grad_norm": 0.06874163448810577,
      "learning_rate": 8.25e-06,
      "loss": 0.0022,
      "step": 125250
    },
    {
      "epoch": 6.680533333333333,
      "grad_norm": 0.2879301905632019,
      "learning_rate": 8.246666666666667e-06,
      "loss": 0.0017,
      "step": 125260
    },
    {
      "epoch": 6.681066666666666,
      "grad_norm": 0.05468948557972908,
      "learning_rate": 8.243333333333333e-06,
      "loss": 0.002,
      "step": 125270
    },
    {
      "epoch": 6.6815999999999995,
      "grad_norm": 0.3952837586402893,
      "learning_rate": 8.24e-06,
      "loss": 0.002,
      "step": 125280
    },
    {
      "epoch": 6.682133333333334,
      "grad_norm": 0.09684169292449951,
      "learning_rate": 8.236666666666667e-06,
      "loss": 0.0016,
      "step": 125290
    },
    {
      "epoch": 6.682666666666667,
      "grad_norm": 0.41501814126968384,
      "learning_rate": 8.233333333333333e-06,
      "loss": 0.0014,
      "step": 125300
    },
    {
      "epoch": 6.6832,
      "grad_norm": 0.14677663147449493,
      "learning_rate": 8.23e-06,
      "loss": 0.002,
      "step": 125310
    },
    {
      "epoch": 6.6837333333333335,
      "grad_norm": 0.5947482585906982,
      "learning_rate": 8.226666666666667e-06,
      "loss": 0.0015,
      "step": 125320
    },
    {
      "epoch": 6.684266666666667,
      "grad_norm": 0.14609071612358093,
      "learning_rate": 8.223333333333335e-06,
      "loss": 0.0019,
      "step": 125330
    },
    {
      "epoch": 6.6848,
      "grad_norm": 0.061248522251844406,
      "learning_rate": 8.22e-06,
      "loss": 0.0017,
      "step": 125340
    },
    {
      "epoch": 6.685333333333333,
      "grad_norm": 0.09164541214704514,
      "learning_rate": 8.216666666666667e-06,
      "loss": 0.0017,
      "step": 125350
    },
    {
      "epoch": 6.685866666666667,
      "grad_norm": 0.2849491834640503,
      "learning_rate": 8.213333333333333e-06,
      "loss": 0.0033,
      "step": 125360
    },
    {
      "epoch": 6.6864,
      "grad_norm": 0.12315157800912857,
      "learning_rate": 8.210000000000001e-06,
      "loss": 0.0017,
      "step": 125370
    },
    {
      "epoch": 6.686933333333333,
      "grad_norm": 0.22982735931873322,
      "learning_rate": 8.206666666666667e-06,
      "loss": 0.002,
      "step": 125380
    },
    {
      "epoch": 6.6874666666666664,
      "grad_norm": 0.14909254014492035,
      "learning_rate": 8.203333333333333e-06,
      "loss": 0.0017,
      "step": 125390
    },
    {
      "epoch": 6.688,
      "grad_norm": 0.21311548352241516,
      "learning_rate": 8.200000000000001e-06,
      "loss": 0.0015,
      "step": 125400
    },
    {
      "epoch": 6.688533333333333,
      "grad_norm": 0.07164286077022552,
      "learning_rate": 8.196666666666666e-06,
      "loss": 0.0017,
      "step": 125410
    },
    {
      "epoch": 6.689066666666667,
      "grad_norm": 0.5118064880371094,
      "learning_rate": 8.193333333333333e-06,
      "loss": 0.0024,
      "step": 125420
    },
    {
      "epoch": 6.6896,
      "grad_norm": 0.3396194279193878,
      "learning_rate": 8.190000000000001e-06,
      "loss": 0.0019,
      "step": 125430
    },
    {
      "epoch": 6.690133333333334,
      "grad_norm": 0.06644841283559799,
      "learning_rate": 8.186666666666667e-06,
      "loss": 0.0027,
      "step": 125440
    },
    {
      "epoch": 6.690666666666667,
      "grad_norm": 0.09310712665319443,
      "learning_rate": 8.183333333333333e-06,
      "loss": 0.0024,
      "step": 125450
    },
    {
      "epoch": 6.6912,
      "grad_norm": 0.30991387367248535,
      "learning_rate": 8.18e-06,
      "loss": 0.0027,
      "step": 125460
    },
    {
      "epoch": 6.6917333333333335,
      "grad_norm": 0.07112818211317062,
      "learning_rate": 8.176666666666667e-06,
      "loss": 0.0025,
      "step": 125470
    },
    {
      "epoch": 6.692266666666667,
      "grad_norm": 0.27175286412239075,
      "learning_rate": 8.173333333333334e-06,
      "loss": 0.0022,
      "step": 125480
    },
    {
      "epoch": 6.6928,
      "grad_norm": 0.04590899124741554,
      "learning_rate": 8.17e-06,
      "loss": 0.002,
      "step": 125490
    },
    {
      "epoch": 6.693333333333333,
      "grad_norm": 0.07236561924219131,
      "learning_rate": 8.166666666666668e-06,
      "loss": 0.003,
      "step": 125500
    },
    {
      "epoch": 6.693866666666667,
      "grad_norm": 0.29668155312538147,
      "learning_rate": 8.163333333333334e-06,
      "loss": 0.0022,
      "step": 125510
    },
    {
      "epoch": 6.6944,
      "grad_norm": 0.0685945600271225,
      "learning_rate": 8.160000000000001e-06,
      "loss": 0.0022,
      "step": 125520
    },
    {
      "epoch": 6.694933333333333,
      "grad_norm": 0.05718660727143288,
      "learning_rate": 8.156666666666666e-06,
      "loss": 0.0017,
      "step": 125530
    },
    {
      "epoch": 6.6954666666666665,
      "grad_norm": 0.04707895964384079,
      "learning_rate": 8.153333333333334e-06,
      "loss": 0.0019,
      "step": 125540
    },
    {
      "epoch": 6.696,
      "grad_norm": 0.22244775295257568,
      "learning_rate": 8.15e-06,
      "loss": 0.0014,
      "step": 125550
    },
    {
      "epoch": 6.696533333333333,
      "grad_norm": 0.14878614246845245,
      "learning_rate": 8.146666666666668e-06,
      "loss": 0.0019,
      "step": 125560
    },
    {
      "epoch": 6.697066666666666,
      "grad_norm": 0.1500891149044037,
      "learning_rate": 8.143333333333334e-06,
      "loss": 0.0015,
      "step": 125570
    },
    {
      "epoch": 6.6975999999999996,
      "grad_norm": 0.16073913872241974,
      "learning_rate": 8.14e-06,
      "loss": 0.0017,
      "step": 125580
    },
    {
      "epoch": 6.698133333333334,
      "grad_norm": 0.04437100142240524,
      "learning_rate": 8.136666666666668e-06,
      "loss": 0.0017,
      "step": 125590
    },
    {
      "epoch": 6.698666666666667,
      "grad_norm": 0.12080305814743042,
      "learning_rate": 8.133333333333332e-06,
      "loss": 0.0015,
      "step": 125600
    },
    {
      "epoch": 6.6992,
      "grad_norm": 0.170686274766922,
      "learning_rate": 8.13e-06,
      "loss": 0.0023,
      "step": 125610
    },
    {
      "epoch": 6.6997333333333335,
      "grad_norm": 0.2848505973815918,
      "learning_rate": 8.126666666666668e-06,
      "loss": 0.0024,
      "step": 125620
    },
    {
      "epoch": 6.700266666666667,
      "grad_norm": 0.20072351396083832,
      "learning_rate": 8.123333333333334e-06,
      "loss": 0.0027,
      "step": 125630
    },
    {
      "epoch": 6.7008,
      "grad_norm": 0.03585818037390709,
      "learning_rate": 8.12e-06,
      "loss": 0.0017,
      "step": 125640
    },
    {
      "epoch": 6.701333333333333,
      "grad_norm": 0.14180651307106018,
      "learning_rate": 8.116666666666666e-06,
      "loss": 0.0013,
      "step": 125650
    },
    {
      "epoch": 6.701866666666667,
      "grad_norm": 0.048999130725860596,
      "learning_rate": 8.113333333333334e-06,
      "loss": 0.003,
      "step": 125660
    },
    {
      "epoch": 6.7024,
      "grad_norm": 0.1540335863828659,
      "learning_rate": 8.11e-06,
      "loss": 0.0017,
      "step": 125670
    },
    {
      "epoch": 6.702933333333333,
      "grad_norm": 0.4644748568534851,
      "learning_rate": 8.106666666666666e-06,
      "loss": 0.0016,
      "step": 125680
    },
    {
      "epoch": 6.7034666666666665,
      "grad_norm": 0.04119057208299637,
      "learning_rate": 8.103333333333334e-06,
      "loss": 0.0015,
      "step": 125690
    },
    {
      "epoch": 6.704,
      "grad_norm": 0.12183216959238052,
      "learning_rate": 8.1e-06,
      "loss": 0.0017,
      "step": 125700
    },
    {
      "epoch": 6.704533333333333,
      "grad_norm": 0.19559238851070404,
      "learning_rate": 8.096666666666668e-06,
      "loss": 0.0028,
      "step": 125710
    },
    {
      "epoch": 6.705066666666666,
      "grad_norm": 0.08052045851945877,
      "learning_rate": 8.093333333333333e-06,
      "loss": 0.0022,
      "step": 125720
    },
    {
      "epoch": 6.7056000000000004,
      "grad_norm": 0.03159833326935768,
      "learning_rate": 8.09e-06,
      "loss": 0.0013,
      "step": 125730
    },
    {
      "epoch": 6.706133333333334,
      "grad_norm": 0.3880070149898529,
      "learning_rate": 8.086666666666667e-06,
      "loss": 0.0019,
      "step": 125740
    },
    {
      "epoch": 6.706666666666667,
      "grad_norm": 0.17980575561523438,
      "learning_rate": 8.083333333333333e-06,
      "loss": 0.0017,
      "step": 125750
    },
    {
      "epoch": 6.7072,
      "grad_norm": 0.21586088836193085,
      "learning_rate": 8.08e-06,
      "loss": 0.0017,
      "step": 125760
    },
    {
      "epoch": 6.7077333333333335,
      "grad_norm": 0.18786774575710297,
      "learning_rate": 8.076666666666667e-06,
      "loss": 0.0015,
      "step": 125770
    },
    {
      "epoch": 6.708266666666667,
      "grad_norm": 0.03451026231050491,
      "learning_rate": 8.073333333333335e-06,
      "loss": 0.002,
      "step": 125780
    },
    {
      "epoch": 6.7088,
      "grad_norm": 0.02726092003285885,
      "learning_rate": 8.069999999999999e-06,
      "loss": 0.0019,
      "step": 125790
    },
    {
      "epoch": 6.709333333333333,
      "grad_norm": 0.07400219887495041,
      "learning_rate": 8.066666666666667e-06,
      "loss": 0.0031,
      "step": 125800
    },
    {
      "epoch": 6.709866666666667,
      "grad_norm": 0.140471950173378,
      "learning_rate": 8.063333333333335e-06,
      "loss": 0.0032,
      "step": 125810
    },
    {
      "epoch": 6.7104,
      "grad_norm": 0.3783756494522095,
      "learning_rate": 8.06e-06,
      "loss": 0.0023,
      "step": 125820
    },
    {
      "epoch": 6.710933333333333,
      "grad_norm": 0.06469758599996567,
      "learning_rate": 8.056666666666667e-06,
      "loss": 0.0016,
      "step": 125830
    },
    {
      "epoch": 6.7114666666666665,
      "grad_norm": 0.06527616828680038,
      "learning_rate": 8.053333333333333e-06,
      "loss": 0.0029,
      "step": 125840
    },
    {
      "epoch": 6.712,
      "grad_norm": 0.33964788913726807,
      "learning_rate": 8.050000000000001e-06,
      "loss": 0.002,
      "step": 125850
    },
    {
      "epoch": 6.712533333333333,
      "grad_norm": 0.17642523348331451,
      "learning_rate": 8.046666666666667e-06,
      "loss": 0.0019,
      "step": 125860
    },
    {
      "epoch": 6.713066666666666,
      "grad_norm": 0.12832525372505188,
      "learning_rate": 8.043333333333333e-06,
      "loss": 0.0017,
      "step": 125870
    },
    {
      "epoch": 6.7136,
      "grad_norm": 0.026767810806632042,
      "learning_rate": 8.040000000000001e-06,
      "loss": 0.0013,
      "step": 125880
    },
    {
      "epoch": 6.714133333333333,
      "grad_norm": 0.09843277186155319,
      "learning_rate": 8.036666666666667e-06,
      "loss": 0.0019,
      "step": 125890
    },
    {
      "epoch": 6.714666666666667,
      "grad_norm": 0.044717755168676376,
      "learning_rate": 8.033333333333335e-06,
      "loss": 0.0023,
      "step": 125900
    },
    {
      "epoch": 6.7152,
      "grad_norm": 0.04564034193754196,
      "learning_rate": 8.03e-06,
      "loss": 0.0019,
      "step": 125910
    },
    {
      "epoch": 6.7157333333333336,
      "grad_norm": 0.07234310358762741,
      "learning_rate": 8.026666666666667e-06,
      "loss": 0.0016,
      "step": 125920
    },
    {
      "epoch": 6.716266666666667,
      "grad_norm": 0.14445385336875916,
      "learning_rate": 8.023333333333333e-06,
      "loss": 0.0018,
      "step": 125930
    },
    {
      "epoch": 6.7168,
      "grad_norm": 0.22070951759815216,
      "learning_rate": 8.02e-06,
      "loss": 0.0018,
      "step": 125940
    },
    {
      "epoch": 6.717333333333333,
      "grad_norm": 0.047481078654527664,
      "learning_rate": 8.016666666666667e-06,
      "loss": 0.0019,
      "step": 125950
    },
    {
      "epoch": 6.717866666666667,
      "grad_norm": 0.2652958035469055,
      "learning_rate": 8.013333333333333e-06,
      "loss": 0.0012,
      "step": 125960
    },
    {
      "epoch": 6.7184,
      "grad_norm": 0.5552675127983093,
      "learning_rate": 8.010000000000001e-06,
      "loss": 0.0023,
      "step": 125970
    },
    {
      "epoch": 6.718933333333333,
      "grad_norm": 0.3870673179626465,
      "learning_rate": 8.006666666666666e-06,
      "loss": 0.0023,
      "step": 125980
    },
    {
      "epoch": 6.7194666666666665,
      "grad_norm": 0.11468330770730972,
      "learning_rate": 8.003333333333334e-06,
      "loss": 0.0014,
      "step": 125990
    },
    {
      "epoch": 6.72,
      "grad_norm": 0.3676741123199463,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.002,
      "step": 126000
    },
    {
      "epoch": 6.720533333333333,
      "grad_norm": 0.3451930284500122,
      "learning_rate": 7.996666666666667e-06,
      "loss": 0.0014,
      "step": 126010
    },
    {
      "epoch": 6.721066666666666,
      "grad_norm": 0.17108318209648132,
      "learning_rate": 7.993333333333334e-06,
      "loss": 0.0014,
      "step": 126020
    },
    {
      "epoch": 6.7216000000000005,
      "grad_norm": 0.14206074178218842,
      "learning_rate": 7.99e-06,
      "loss": 0.0013,
      "step": 126030
    },
    {
      "epoch": 6.722133333333334,
      "grad_norm": 0.589730441570282,
      "learning_rate": 7.986666666666668e-06,
      "loss": 0.0016,
      "step": 126040
    },
    {
      "epoch": 6.722666666666667,
      "grad_norm": 0.035666391253471375,
      "learning_rate": 7.983333333333334e-06,
      "loss": 0.0014,
      "step": 126050
    },
    {
      "epoch": 6.7232,
      "grad_norm": 0.3435118794441223,
      "learning_rate": 7.98e-06,
      "loss": 0.002,
      "step": 126060
    },
    {
      "epoch": 6.723733333333334,
      "grad_norm": 0.2003948986530304,
      "learning_rate": 7.976666666666668e-06,
      "loss": 0.0015,
      "step": 126070
    },
    {
      "epoch": 6.724266666666667,
      "grad_norm": 0.48734819889068604,
      "learning_rate": 7.973333333333334e-06,
      "loss": 0.0017,
      "step": 126080
    },
    {
      "epoch": 6.7248,
      "grad_norm": 0.2572104334831238,
      "learning_rate": 7.97e-06,
      "loss": 0.0019,
      "step": 126090
    },
    {
      "epoch": 6.725333333333333,
      "grad_norm": 0.07669021934270859,
      "learning_rate": 7.966666666666666e-06,
      "loss": 0.0026,
      "step": 126100
    },
    {
      "epoch": 6.725866666666667,
      "grad_norm": 0.2220069169998169,
      "learning_rate": 7.963333333333334e-06,
      "loss": 0.0014,
      "step": 126110
    },
    {
      "epoch": 6.7264,
      "grad_norm": 0.22234536707401276,
      "learning_rate": 7.96e-06,
      "loss": 0.0023,
      "step": 126120
    },
    {
      "epoch": 6.726933333333333,
      "grad_norm": 0.10208749026060104,
      "learning_rate": 7.956666666666666e-06,
      "loss": 0.0022,
      "step": 126130
    },
    {
      "epoch": 6.7274666666666665,
      "grad_norm": 0.06576274335384369,
      "learning_rate": 7.953333333333334e-06,
      "loss": 0.0014,
      "step": 126140
    },
    {
      "epoch": 6.728,
      "grad_norm": 0.12567780911922455,
      "learning_rate": 7.95e-06,
      "loss": 0.0016,
      "step": 126150
    },
    {
      "epoch": 6.728533333333333,
      "grad_norm": 0.284675657749176,
      "learning_rate": 7.946666666666668e-06,
      "loss": 0.0014,
      "step": 126160
    },
    {
      "epoch": 6.729066666666666,
      "grad_norm": 0.14068825542926788,
      "learning_rate": 7.943333333333332e-06,
      "loss": 0.0013,
      "step": 126170
    },
    {
      "epoch": 6.7296,
      "grad_norm": 0.06964369118213654,
      "learning_rate": 7.94e-06,
      "loss": 0.0022,
      "step": 126180
    },
    {
      "epoch": 6.730133333333333,
      "grad_norm": 0.04801264777779579,
      "learning_rate": 7.936666666666668e-06,
      "loss": 0.0021,
      "step": 126190
    },
    {
      "epoch": 6.730666666666667,
      "grad_norm": 0.1881408989429474,
      "learning_rate": 7.933333333333334e-06,
      "loss": 0.0017,
      "step": 126200
    },
    {
      "epoch": 6.7312,
      "grad_norm": 0.2575220763683319,
      "learning_rate": 7.93e-06,
      "loss": 0.0015,
      "step": 126210
    },
    {
      "epoch": 6.731733333333334,
      "grad_norm": 0.06405430287122726,
      "learning_rate": 7.926666666666666e-06,
      "loss": 0.0018,
      "step": 126220
    },
    {
      "epoch": 6.732266666666667,
      "grad_norm": 0.20137536525726318,
      "learning_rate": 7.923333333333334e-06,
      "loss": 0.0016,
      "step": 126230
    },
    {
      "epoch": 6.7328,
      "grad_norm": 0.14509224891662598,
      "learning_rate": 7.92e-06,
      "loss": 0.0021,
      "step": 126240
    },
    {
      "epoch": 6.733333333333333,
      "grad_norm": 0.08768817037343979,
      "learning_rate": 7.916666666666667e-06,
      "loss": 0.0015,
      "step": 126250
    },
    {
      "epoch": 6.733866666666667,
      "grad_norm": 0.073061004281044,
      "learning_rate": 7.913333333333334e-06,
      "loss": 0.0016,
      "step": 126260
    },
    {
      "epoch": 6.7344,
      "grad_norm": 0.12445123493671417,
      "learning_rate": 7.91e-06,
      "loss": 0.0021,
      "step": 126270
    },
    {
      "epoch": 6.734933333333333,
      "grad_norm": 0.15614205598831177,
      "learning_rate": 7.906666666666667e-06,
      "loss": 0.0016,
      "step": 126280
    },
    {
      "epoch": 6.7354666666666665,
      "grad_norm": 0.20420226454734802,
      "learning_rate": 7.903333333333333e-06,
      "loss": 0.0017,
      "step": 126290
    },
    {
      "epoch": 6.736,
      "grad_norm": 0.11663739383220673,
      "learning_rate": 7.9e-06,
      "loss": 0.0017,
      "step": 126300
    },
    {
      "epoch": 6.736533333333333,
      "grad_norm": 0.30081668496131897,
      "learning_rate": 7.896666666666667e-06,
      "loss": 0.0022,
      "step": 126310
    },
    {
      "epoch": 6.737066666666666,
      "grad_norm": 0.10224152356386185,
      "learning_rate": 7.893333333333333e-06,
      "loss": 0.0018,
      "step": 126320
    },
    {
      "epoch": 6.7376000000000005,
      "grad_norm": 0.2781907021999359,
      "learning_rate": 7.89e-06,
      "loss": 0.003,
      "step": 126330
    },
    {
      "epoch": 6.738133333333334,
      "grad_norm": 0.07507514953613281,
      "learning_rate": 7.886666666666667e-06,
      "loss": 0.0023,
      "step": 126340
    },
    {
      "epoch": 6.738666666666667,
      "grad_norm": 0.07352308183908463,
      "learning_rate": 7.883333333333335e-06,
      "loss": 0.0029,
      "step": 126350
    },
    {
      "epoch": 6.7392,
      "grad_norm": 0.3178849220275879,
      "learning_rate": 7.879999999999999e-06,
      "loss": 0.0023,
      "step": 126360
    },
    {
      "epoch": 6.739733333333334,
      "grad_norm": 0.34420618414878845,
      "learning_rate": 7.876666666666667e-06,
      "loss": 0.0017,
      "step": 126370
    },
    {
      "epoch": 6.740266666666667,
      "grad_norm": 0.06372376531362534,
      "learning_rate": 7.873333333333335e-06,
      "loss": 0.0012,
      "step": 126380
    },
    {
      "epoch": 6.7408,
      "grad_norm": 0.18072061240673065,
      "learning_rate": 7.870000000000001e-06,
      "loss": 0.0016,
      "step": 126390
    },
    {
      "epoch": 6.741333333333333,
      "grad_norm": 0.14137151837348938,
      "learning_rate": 7.866666666666667e-06,
      "loss": 0.0027,
      "step": 126400
    },
    {
      "epoch": 6.741866666666667,
      "grad_norm": 0.26076391339302063,
      "learning_rate": 7.863333333333333e-06,
      "loss": 0.0019,
      "step": 126410
    },
    {
      "epoch": 6.7424,
      "grad_norm": 0.17393794655799866,
      "learning_rate": 7.860000000000001e-06,
      "loss": 0.0019,
      "step": 126420
    },
    {
      "epoch": 6.742933333333333,
      "grad_norm": 0.2528676390647888,
      "learning_rate": 7.856666666666667e-06,
      "loss": 0.0013,
      "step": 126430
    },
    {
      "epoch": 6.7434666666666665,
      "grad_norm": 0.28327783942222595,
      "learning_rate": 7.853333333333333e-06,
      "loss": 0.0018,
      "step": 126440
    },
    {
      "epoch": 6.744,
      "grad_norm": 0.08362974971532822,
      "learning_rate": 7.850000000000001e-06,
      "loss": 0.002,
      "step": 126450
    },
    {
      "epoch": 6.744533333333333,
      "grad_norm": 0.06201866269111633,
      "learning_rate": 7.846666666666667e-06,
      "loss": 0.002,
      "step": 126460
    },
    {
      "epoch": 6.745066666666666,
      "grad_norm": 0.05701775848865509,
      "learning_rate": 7.843333333333333e-06,
      "loss": 0.0026,
      "step": 126470
    },
    {
      "epoch": 6.7456,
      "grad_norm": 0.3712519109249115,
      "learning_rate": 7.84e-06,
      "loss": 0.0014,
      "step": 126480
    },
    {
      "epoch": 6.746133333333333,
      "grad_norm": 0.20200088620185852,
      "learning_rate": 7.836666666666667e-06,
      "loss": 0.0022,
      "step": 126490
    },
    {
      "epoch": 6.746666666666667,
      "grad_norm": 0.05001847445964813,
      "learning_rate": 7.833333333333333e-06,
      "loss": 0.0019,
      "step": 126500
    },
    {
      "epoch": 6.7472,
      "grad_norm": 0.17713062465190887,
      "learning_rate": 7.83e-06,
      "loss": 0.0017,
      "step": 126510
    },
    {
      "epoch": 6.747733333333334,
      "grad_norm": 0.153787761926651,
      "learning_rate": 7.826666666666667e-06,
      "loss": 0.002,
      "step": 126520
    },
    {
      "epoch": 6.748266666666667,
      "grad_norm": 0.11944770812988281,
      "learning_rate": 7.823333333333334e-06,
      "loss": 0.0015,
      "step": 126530
    },
    {
      "epoch": 6.7488,
      "grad_norm": 0.2327445149421692,
      "learning_rate": 7.820000000000001e-06,
      "loss": 0.0012,
      "step": 126540
    },
    {
      "epoch": 6.749333333333333,
      "grad_norm": 0.01638510264456272,
      "learning_rate": 7.816666666666666e-06,
      "loss": 0.002,
      "step": 126550
    },
    {
      "epoch": 6.749866666666667,
      "grad_norm": 0.19658714532852173,
      "learning_rate": 7.813333333333334e-06,
      "loss": 0.0022,
      "step": 126560
    },
    {
      "epoch": 6.7504,
      "grad_norm": 0.12008364498615265,
      "learning_rate": 7.810000000000001e-06,
      "loss": 0.0013,
      "step": 126570
    },
    {
      "epoch": 6.750933333333333,
      "grad_norm": 0.02112162671983242,
      "learning_rate": 7.806666666666668e-06,
      "loss": 0.0016,
      "step": 126580
    },
    {
      "epoch": 6.7514666666666665,
      "grad_norm": 0.39976227283477783,
      "learning_rate": 7.803333333333334e-06,
      "loss": 0.0013,
      "step": 126590
    },
    {
      "epoch": 6.752,
      "grad_norm": 0.03194136545062065,
      "learning_rate": 7.8e-06,
      "loss": 0.0019,
      "step": 126600
    },
    {
      "epoch": 6.752533333333333,
      "grad_norm": 0.17123371362686157,
      "learning_rate": 7.796666666666668e-06,
      "loss": 0.002,
      "step": 126610
    },
    {
      "epoch": 6.753066666666666,
      "grad_norm": 0.349300354719162,
      "learning_rate": 7.793333333333334e-06,
      "loss": 0.0019,
      "step": 126620
    },
    {
      "epoch": 6.7536000000000005,
      "grad_norm": 0.23697908222675323,
      "learning_rate": 7.79e-06,
      "loss": 0.002,
      "step": 126630
    },
    {
      "epoch": 6.754133333333334,
      "grad_norm": 0.20289234817028046,
      "learning_rate": 7.786666666666668e-06,
      "loss": 0.0015,
      "step": 126640
    },
    {
      "epoch": 6.754666666666667,
      "grad_norm": 0.2830530107021332,
      "learning_rate": 7.783333333333334e-06,
      "loss": 0.0018,
      "step": 126650
    },
    {
      "epoch": 6.7552,
      "grad_norm": 0.14581194519996643,
      "learning_rate": 7.78e-06,
      "loss": 0.0015,
      "step": 126660
    },
    {
      "epoch": 6.755733333333334,
      "grad_norm": 0.1212920993566513,
      "learning_rate": 7.776666666666666e-06,
      "loss": 0.0018,
      "step": 126670
    },
    {
      "epoch": 6.756266666666667,
      "grad_norm": 0.04309888556599617,
      "learning_rate": 7.773333333333334e-06,
      "loss": 0.0018,
      "step": 126680
    },
    {
      "epoch": 6.7568,
      "grad_norm": 0.23505528271198273,
      "learning_rate": 7.77e-06,
      "loss": 0.0018,
      "step": 126690
    },
    {
      "epoch": 6.757333333333333,
      "grad_norm": 0.27282851934432983,
      "learning_rate": 7.766666666666666e-06,
      "loss": 0.0015,
      "step": 126700
    },
    {
      "epoch": 6.757866666666667,
      "grad_norm": 0.1553623527288437,
      "learning_rate": 7.763333333333334e-06,
      "loss": 0.0018,
      "step": 126710
    },
    {
      "epoch": 6.7584,
      "grad_norm": 0.23186244070529938,
      "learning_rate": 7.76e-06,
      "loss": 0.0036,
      "step": 126720
    },
    {
      "epoch": 6.758933333333333,
      "grad_norm": 0.15177761018276215,
      "learning_rate": 7.756666666666668e-06,
      "loss": 0.0014,
      "step": 126730
    },
    {
      "epoch": 6.7594666666666665,
      "grad_norm": 0.1505720615386963,
      "learning_rate": 7.753333333333333e-06,
      "loss": 0.0016,
      "step": 126740
    },
    {
      "epoch": 6.76,
      "grad_norm": 0.14751386642456055,
      "learning_rate": 7.75e-06,
      "loss": 0.0017,
      "step": 126750
    },
    {
      "epoch": 6.760533333333333,
      "grad_norm": 0.23785296082496643,
      "learning_rate": 7.746666666666668e-06,
      "loss": 0.002,
      "step": 126760
    },
    {
      "epoch": 6.761066666666666,
      "grad_norm": 0.48619765043258667,
      "learning_rate": 7.743333333333334e-06,
      "loss": 0.0024,
      "step": 126770
    },
    {
      "epoch": 6.7616,
      "grad_norm": 0.2263115495443344,
      "learning_rate": 7.74e-06,
      "loss": 0.0017,
      "step": 126780
    },
    {
      "epoch": 6.762133333333333,
      "grad_norm": 0.1417904794216156,
      "learning_rate": 7.736666666666667e-06,
      "loss": 0.0013,
      "step": 126790
    },
    {
      "epoch": 6.762666666666667,
      "grad_norm": 0.0661432296037674,
      "learning_rate": 7.733333333333334e-06,
      "loss": 0.0019,
      "step": 126800
    },
    {
      "epoch": 6.7632,
      "grad_norm": 0.3398941457271576,
      "learning_rate": 7.73e-06,
      "loss": 0.0021,
      "step": 126810
    },
    {
      "epoch": 6.763733333333334,
      "grad_norm": 0.04814526066184044,
      "learning_rate": 7.726666666666667e-06,
      "loss": 0.0016,
      "step": 126820
    },
    {
      "epoch": 6.764266666666667,
      "grad_norm": 0.4005855321884155,
      "learning_rate": 7.723333333333334e-06,
      "loss": 0.0016,
      "step": 126830
    },
    {
      "epoch": 6.7648,
      "grad_norm": 0.20467731356620789,
      "learning_rate": 7.72e-06,
      "loss": 0.0015,
      "step": 126840
    },
    {
      "epoch": 6.765333333333333,
      "grad_norm": 0.40511268377304077,
      "learning_rate": 7.716666666666667e-06,
      "loss": 0.0022,
      "step": 126850
    },
    {
      "epoch": 6.765866666666667,
      "grad_norm": 0.40545549988746643,
      "learning_rate": 7.713333333333333e-06,
      "loss": 0.0022,
      "step": 126860
    },
    {
      "epoch": 6.7664,
      "grad_norm": 0.19862399995326996,
      "learning_rate": 7.71e-06,
      "loss": 0.0021,
      "step": 126870
    },
    {
      "epoch": 6.766933333333333,
      "grad_norm": 0.04164886847138405,
      "learning_rate": 7.706666666666667e-06,
      "loss": 0.0014,
      "step": 126880
    },
    {
      "epoch": 6.7674666666666665,
      "grad_norm": 0.14799222350120544,
      "learning_rate": 7.703333333333333e-06,
      "loss": 0.0014,
      "step": 126890
    },
    {
      "epoch": 6.768,
      "grad_norm": 0.09517441689968109,
      "learning_rate": 7.7e-06,
      "loss": 0.0021,
      "step": 126900
    },
    {
      "epoch": 6.768533333333333,
      "grad_norm": 0.0827617421746254,
      "learning_rate": 7.696666666666667e-06,
      "loss": 0.0019,
      "step": 126910
    },
    {
      "epoch": 6.769066666666666,
      "grad_norm": 0.0960073173046112,
      "learning_rate": 7.693333333333335e-06,
      "loss": 0.0014,
      "step": 126920
    },
    {
      "epoch": 6.7696,
      "grad_norm": 0.1443280577659607,
      "learning_rate": 7.69e-06,
      "loss": 0.003,
      "step": 126930
    },
    {
      "epoch": 6.770133333333334,
      "grad_norm": 0.09303490072488785,
      "learning_rate": 7.686666666666667e-06,
      "loss": 0.0021,
      "step": 126940
    },
    {
      "epoch": 6.770666666666667,
      "grad_norm": 0.14426599442958832,
      "learning_rate": 7.683333333333335e-06,
      "loss": 0.0014,
      "step": 126950
    },
    {
      "epoch": 6.7712,
      "grad_norm": 0.25965338945388794,
      "learning_rate": 7.68e-06,
      "loss": 0.0017,
      "step": 126960
    },
    {
      "epoch": 6.771733333333334,
      "grad_norm": 0.2573048174381256,
      "learning_rate": 7.676666666666667e-06,
      "loss": 0.0018,
      "step": 126970
    },
    {
      "epoch": 6.772266666666667,
      "grad_norm": 0.3149625062942505,
      "learning_rate": 7.673333333333333e-06,
      "loss": 0.0018,
      "step": 126980
    },
    {
      "epoch": 6.7728,
      "grad_norm": 0.3963875472545624,
      "learning_rate": 7.670000000000001e-06,
      "loss": 0.0021,
      "step": 126990
    },
    {
      "epoch": 6.773333333333333,
      "grad_norm": 0.09252353012561798,
      "learning_rate": 7.666666666666667e-06,
      "loss": 0.0026,
      "step": 127000
    },
    {
      "epoch": 6.773866666666667,
      "grad_norm": 0.06744634360074997,
      "learning_rate": 7.663333333333333e-06,
      "loss": 0.0017,
      "step": 127010
    },
    {
      "epoch": 6.7744,
      "grad_norm": 0.20096443593502045,
      "learning_rate": 7.660000000000001e-06,
      "loss": 0.0021,
      "step": 127020
    },
    {
      "epoch": 6.774933333333333,
      "grad_norm": 0.4420945346355438,
      "learning_rate": 7.656666666666667e-06,
      "loss": 0.0016,
      "step": 127030
    },
    {
      "epoch": 6.7754666666666665,
      "grad_norm": 0.10260482877492905,
      "learning_rate": 7.653333333333333e-06,
      "loss": 0.0018,
      "step": 127040
    },
    {
      "epoch": 6.776,
      "grad_norm": 0.4306130111217499,
      "learning_rate": 7.65e-06,
      "loss": 0.0032,
      "step": 127050
    },
    {
      "epoch": 6.776533333333333,
      "grad_norm": 0.45275789499282837,
      "learning_rate": 7.646666666666667e-06,
      "loss": 0.0028,
      "step": 127060
    },
    {
      "epoch": 6.777066666666666,
      "grad_norm": 0.21950002014636993,
      "learning_rate": 7.643333333333334e-06,
      "loss": 0.0014,
      "step": 127070
    },
    {
      "epoch": 6.7776,
      "grad_norm": 0.4002968668937683,
      "learning_rate": 7.64e-06,
      "loss": 0.0022,
      "step": 127080
    },
    {
      "epoch": 6.778133333333333,
      "grad_norm": 0.038668688386678696,
      "learning_rate": 7.636666666666668e-06,
      "loss": 0.0017,
      "step": 127090
    },
    {
      "epoch": 6.778666666666666,
      "grad_norm": 0.5493491291999817,
      "learning_rate": 7.633333333333334e-06,
      "loss": 0.0022,
      "step": 127100
    },
    {
      "epoch": 6.7792,
      "grad_norm": 0.20864830911159515,
      "learning_rate": 7.630000000000001e-06,
      "loss": 0.0017,
      "step": 127110
    },
    {
      "epoch": 6.779733333333334,
      "grad_norm": 0.3199557065963745,
      "learning_rate": 7.626666666666667e-06,
      "loss": 0.0016,
      "step": 127120
    },
    {
      "epoch": 6.780266666666667,
      "grad_norm": 0.03285803645849228,
      "learning_rate": 7.623333333333334e-06,
      "loss": 0.0016,
      "step": 127130
    },
    {
      "epoch": 6.7808,
      "grad_norm": 0.17161935567855835,
      "learning_rate": 7.620000000000001e-06,
      "loss": 0.0021,
      "step": 127140
    },
    {
      "epoch": 6.781333333333333,
      "grad_norm": 0.3702795207500458,
      "learning_rate": 7.616666666666666e-06,
      "loss": 0.002,
      "step": 127150
    },
    {
      "epoch": 6.781866666666667,
      "grad_norm": 0.059414174407720566,
      "learning_rate": 7.613333333333334e-06,
      "loss": 0.0021,
      "step": 127160
    },
    {
      "epoch": 6.7824,
      "grad_norm": 0.09434390813112259,
      "learning_rate": 7.610000000000001e-06,
      "loss": 0.0016,
      "step": 127170
    },
    {
      "epoch": 6.782933333333333,
      "grad_norm": 0.31609612703323364,
      "learning_rate": 7.606666666666668e-06,
      "loss": 0.0025,
      "step": 127180
    },
    {
      "epoch": 6.7834666666666665,
      "grad_norm": 0.33532676100730896,
      "learning_rate": 7.603333333333333e-06,
      "loss": 0.0014,
      "step": 127190
    },
    {
      "epoch": 6.784,
      "grad_norm": 0.09182478487491608,
      "learning_rate": 7.6e-06,
      "loss": 0.0013,
      "step": 127200
    },
    {
      "epoch": 6.784533333333333,
      "grad_norm": 0.1992442011833191,
      "learning_rate": 7.596666666666667e-06,
      "loss": 0.0021,
      "step": 127210
    },
    {
      "epoch": 6.785066666666666,
      "grad_norm": 0.08946720510721207,
      "learning_rate": 7.593333333333334e-06,
      "loss": 0.0019,
      "step": 127220
    },
    {
      "epoch": 6.7856,
      "grad_norm": 0.04434766247868538,
      "learning_rate": 7.59e-06,
      "loss": 0.0014,
      "step": 127230
    },
    {
      "epoch": 6.786133333333334,
      "grad_norm": 0.4892790913581848,
      "learning_rate": 7.586666666666667e-06,
      "loss": 0.0017,
      "step": 127240
    },
    {
      "epoch": 6.786666666666667,
      "grad_norm": 0.1705106943845749,
      "learning_rate": 7.583333333333334e-06,
      "loss": 0.0016,
      "step": 127250
    },
    {
      "epoch": 6.7872,
      "grad_norm": 0.29483431577682495,
      "learning_rate": 7.580000000000001e-06,
      "loss": 0.0014,
      "step": 127260
    },
    {
      "epoch": 6.787733333333334,
      "grad_norm": 0.048065878450870514,
      "learning_rate": 7.576666666666666e-06,
      "loss": 0.0023,
      "step": 127270
    },
    {
      "epoch": 6.788266666666667,
      "grad_norm": 0.1531347632408142,
      "learning_rate": 7.573333333333333e-06,
      "loss": 0.0015,
      "step": 127280
    },
    {
      "epoch": 6.7888,
      "grad_norm": 0.06146185100078583,
      "learning_rate": 7.57e-06,
      "loss": 0.0018,
      "step": 127290
    },
    {
      "epoch": 6.789333333333333,
      "grad_norm": 0.2796522378921509,
      "learning_rate": 7.5666666666666665e-06,
      "loss": 0.0025,
      "step": 127300
    },
    {
      "epoch": 6.789866666666667,
      "grad_norm": 0.1696508526802063,
      "learning_rate": 7.5633333333333335e-06,
      "loss": 0.0011,
      "step": 127310
    },
    {
      "epoch": 6.7904,
      "grad_norm": 0.06366953998804092,
      "learning_rate": 7.5600000000000005e-06,
      "loss": 0.0016,
      "step": 127320
    },
    {
      "epoch": 6.790933333333333,
      "grad_norm": 0.25986286997795105,
      "learning_rate": 7.5566666666666674e-06,
      "loss": 0.0018,
      "step": 127330
    },
    {
      "epoch": 6.7914666666666665,
      "grad_norm": 0.051884084939956665,
      "learning_rate": 7.553333333333333e-06,
      "loss": 0.0013,
      "step": 127340
    },
    {
      "epoch": 6.792,
      "grad_norm": 0.23274146020412445,
      "learning_rate": 7.55e-06,
      "loss": 0.0019,
      "step": 127350
    },
    {
      "epoch": 6.792533333333333,
      "grad_norm": 0.09136345982551575,
      "learning_rate": 7.5466666666666675e-06,
      "loss": 0.0019,
      "step": 127360
    },
    {
      "epoch": 6.793066666666666,
      "grad_norm": 0.4259548485279083,
      "learning_rate": 7.5433333333333345e-06,
      "loss": 0.0016,
      "step": 127370
    },
    {
      "epoch": 6.7936,
      "grad_norm": 0.2874898314476013,
      "learning_rate": 7.54e-06,
      "loss": 0.0014,
      "step": 127380
    },
    {
      "epoch": 6.794133333333333,
      "grad_norm": 0.2325882613658905,
      "learning_rate": 7.536666666666667e-06,
      "loss": 0.0023,
      "step": 127390
    },
    {
      "epoch": 6.794666666666666,
      "grad_norm": 0.2570095956325531,
      "learning_rate": 7.533333333333334e-06,
      "loss": 0.0015,
      "step": 127400
    },
    {
      "epoch": 6.7952,
      "grad_norm": 0.48269182443618774,
      "learning_rate": 7.530000000000001e-06,
      "loss": 0.0021,
      "step": 127410
    },
    {
      "epoch": 6.795733333333334,
      "grad_norm": 0.22698713839054108,
      "learning_rate": 7.526666666666667e-06,
      "loss": 0.002,
      "step": 127420
    },
    {
      "epoch": 6.796266666666667,
      "grad_norm": 0.3350287973880768,
      "learning_rate": 7.523333333333334e-06,
      "loss": 0.0027,
      "step": 127430
    },
    {
      "epoch": 6.7968,
      "grad_norm": 0.026268187910318375,
      "learning_rate": 7.520000000000001e-06,
      "loss": 0.0013,
      "step": 127440
    },
    {
      "epoch": 6.7973333333333334,
      "grad_norm": 0.17592772841453552,
      "learning_rate": 7.516666666666668e-06,
      "loss": 0.0017,
      "step": 127450
    },
    {
      "epoch": 6.797866666666667,
      "grad_norm": 0.05874936282634735,
      "learning_rate": 7.513333333333333e-06,
      "loss": 0.0024,
      "step": 127460
    },
    {
      "epoch": 6.7984,
      "grad_norm": 0.3072424530982971,
      "learning_rate": 7.51e-06,
      "loss": 0.0021,
      "step": 127470
    },
    {
      "epoch": 6.798933333333333,
      "grad_norm": 0.20025798678398132,
      "learning_rate": 7.506666666666667e-06,
      "loss": 0.0013,
      "step": 127480
    },
    {
      "epoch": 6.7994666666666665,
      "grad_norm": 0.17112289369106293,
      "learning_rate": 7.503333333333333e-06,
      "loss": 0.002,
      "step": 127490
    },
    {
      "epoch": 6.8,
      "grad_norm": 0.02112731896340847,
      "learning_rate": 7.5e-06,
      "loss": 0.002,
      "step": 127500
    },
    {
      "epoch": 6.800533333333333,
      "grad_norm": 0.045170921832323074,
      "learning_rate": 7.496666666666667e-06,
      "loss": 0.0021,
      "step": 127510
    },
    {
      "epoch": 6.801066666666666,
      "grad_norm": 0.0672667995095253,
      "learning_rate": 7.493333333333334e-06,
      "loss": 0.0018,
      "step": 127520
    },
    {
      "epoch": 6.8016,
      "grad_norm": 0.2920912206172943,
      "learning_rate": 7.4899999999999994e-06,
      "loss": 0.0021,
      "step": 127530
    },
    {
      "epoch": 6.802133333333334,
      "grad_norm": 0.21252118051052094,
      "learning_rate": 7.486666666666666e-06,
      "loss": 0.0021,
      "step": 127540
    },
    {
      "epoch": 6.802666666666667,
      "grad_norm": 0.17147000133991241,
      "learning_rate": 7.483333333333334e-06,
      "loss": 0.002,
      "step": 127550
    },
    {
      "epoch": 6.8032,
      "grad_norm": 0.14781081676483154,
      "learning_rate": 7.480000000000001e-06,
      "loss": 0.0019,
      "step": 127560
    },
    {
      "epoch": 6.803733333333334,
      "grad_norm": 0.03768187388777733,
      "learning_rate": 7.4766666666666665e-06,
      "loss": 0.0016,
      "step": 127570
    },
    {
      "epoch": 6.804266666666667,
      "grad_norm": 0.2941560447216034,
      "learning_rate": 7.4733333333333335e-06,
      "loss": 0.0015,
      "step": 127580
    },
    {
      "epoch": 6.8048,
      "grad_norm": 0.22791899740695953,
      "learning_rate": 7.4700000000000005e-06,
      "loss": 0.0023,
      "step": 127590
    },
    {
      "epoch": 6.8053333333333335,
      "grad_norm": 0.28040504455566406,
      "learning_rate": 7.4666666666666675e-06,
      "loss": 0.0017,
      "step": 127600
    },
    {
      "epoch": 6.805866666666667,
      "grad_norm": 0.10065512359142303,
      "learning_rate": 7.463333333333334e-06,
      "loss": 0.0015,
      "step": 127610
    },
    {
      "epoch": 6.8064,
      "grad_norm": 0.06499668955802917,
      "learning_rate": 7.4600000000000006e-06,
      "loss": 0.002,
      "step": 127620
    },
    {
      "epoch": 6.806933333333333,
      "grad_norm": 0.09277410060167313,
      "learning_rate": 7.4566666666666676e-06,
      "loss": 0.0013,
      "step": 127630
    },
    {
      "epoch": 6.8074666666666666,
      "grad_norm": 0.1479853242635727,
      "learning_rate": 7.453333333333333e-06,
      "loss": 0.0019,
      "step": 127640
    },
    {
      "epoch": 6.808,
      "grad_norm": 0.06792104989290237,
      "learning_rate": 7.45e-06,
      "loss": 0.0026,
      "step": 127650
    },
    {
      "epoch": 6.808533333333333,
      "grad_norm": 0.20918451249599457,
      "learning_rate": 7.446666666666667e-06,
      "loss": 0.0017,
      "step": 127660
    },
    {
      "epoch": 6.809066666666666,
      "grad_norm": 0.31365418434143066,
      "learning_rate": 7.443333333333334e-06,
      "loss": 0.0015,
      "step": 127670
    },
    {
      "epoch": 6.8096,
      "grad_norm": 0.1189744770526886,
      "learning_rate": 7.44e-06,
      "loss": 0.0024,
      "step": 127680
    },
    {
      "epoch": 6.810133333333333,
      "grad_norm": 0.5524734854698181,
      "learning_rate": 7.436666666666667e-06,
      "loss": 0.0017,
      "step": 127690
    },
    {
      "epoch": 6.810666666666666,
      "grad_norm": 0.21146149933338165,
      "learning_rate": 7.433333333333334e-06,
      "loss": 0.0016,
      "step": 127700
    },
    {
      "epoch": 6.8112,
      "grad_norm": 0.34864872694015503,
      "learning_rate": 7.430000000000001e-06,
      "loss": 0.0016,
      "step": 127710
    },
    {
      "epoch": 6.811733333333334,
      "grad_norm": 0.12390683591365814,
      "learning_rate": 7.426666666666666e-06,
      "loss": 0.0026,
      "step": 127720
    },
    {
      "epoch": 6.812266666666667,
      "grad_norm": 0.02102106623351574,
      "learning_rate": 7.423333333333333e-06,
      "loss": 0.0013,
      "step": 127730
    },
    {
      "epoch": 6.8128,
      "grad_norm": 0.041210975497961044,
      "learning_rate": 7.420000000000001e-06,
      "loss": 0.0013,
      "step": 127740
    },
    {
      "epoch": 6.8133333333333335,
      "grad_norm": 0.1432259976863861,
      "learning_rate": 7.416666666666668e-06,
      "loss": 0.0026,
      "step": 127750
    },
    {
      "epoch": 6.813866666666667,
      "grad_norm": 0.25668570399284363,
      "learning_rate": 7.413333333333333e-06,
      "loss": 0.0014,
      "step": 127760
    },
    {
      "epoch": 6.8144,
      "grad_norm": 0.09070221334695816,
      "learning_rate": 7.41e-06,
      "loss": 0.002,
      "step": 127770
    },
    {
      "epoch": 6.814933333333333,
      "grad_norm": 0.2854558527469635,
      "learning_rate": 7.406666666666667e-06,
      "loss": 0.0016,
      "step": 127780
    },
    {
      "epoch": 6.815466666666667,
      "grad_norm": 0.19647017121315002,
      "learning_rate": 7.403333333333334e-06,
      "loss": 0.0018,
      "step": 127790
    },
    {
      "epoch": 6.816,
      "grad_norm": 0.03900426626205444,
      "learning_rate": 7.4e-06,
      "loss": 0.0025,
      "step": 127800
    },
    {
      "epoch": 6.816533333333333,
      "grad_norm": 0.2814956605434418,
      "learning_rate": 7.396666666666667e-06,
      "loss": 0.0014,
      "step": 127810
    },
    {
      "epoch": 6.817066666666666,
      "grad_norm": 0.025984803214669228,
      "learning_rate": 7.393333333333334e-06,
      "loss": 0.0027,
      "step": 127820
    },
    {
      "epoch": 6.8176,
      "grad_norm": 0.20280176401138306,
      "learning_rate": 7.3899999999999995e-06,
      "loss": 0.0021,
      "step": 127830
    },
    {
      "epoch": 6.818133333333334,
      "grad_norm": 0.044664666056632996,
      "learning_rate": 7.3866666666666665e-06,
      "loss": 0.0026,
      "step": 127840
    },
    {
      "epoch": 6.818666666666667,
      "grad_norm": 0.22822962701320648,
      "learning_rate": 7.3833333333333335e-06,
      "loss": 0.0027,
      "step": 127850
    },
    {
      "epoch": 6.8192,
      "grad_norm": 0.3189210593700409,
      "learning_rate": 7.3800000000000005e-06,
      "loss": 0.0022,
      "step": 127860
    },
    {
      "epoch": 6.819733333333334,
      "grad_norm": 0.29122933745384216,
      "learning_rate": 7.376666666666667e-06,
      "loss": 0.0029,
      "step": 127870
    },
    {
      "epoch": 6.820266666666667,
      "grad_norm": 0.20751893520355225,
      "learning_rate": 7.373333333333334e-06,
      "loss": 0.0015,
      "step": 127880
    },
    {
      "epoch": 6.8208,
      "grad_norm": 0.09520354866981506,
      "learning_rate": 7.370000000000001e-06,
      "loss": 0.0018,
      "step": 127890
    },
    {
      "epoch": 6.8213333333333335,
      "grad_norm": 0.2786651849746704,
      "learning_rate": 7.3666666666666676e-06,
      "loss": 0.0025,
      "step": 127900
    },
    {
      "epoch": 6.821866666666667,
      "grad_norm": 0.1469690203666687,
      "learning_rate": 7.363333333333333e-06,
      "loss": 0.0028,
      "step": 127910
    },
    {
      "epoch": 6.8224,
      "grad_norm": 0.14790306985378265,
      "learning_rate": 7.36e-06,
      "loss": 0.0022,
      "step": 127920
    },
    {
      "epoch": 6.822933333333333,
      "grad_norm": 0.04081731662154198,
      "learning_rate": 7.356666666666668e-06,
      "loss": 0.0013,
      "step": 127930
    },
    {
      "epoch": 6.823466666666667,
      "grad_norm": 0.03309467062354088,
      "learning_rate": 7.353333333333335e-06,
      "loss": 0.0017,
      "step": 127940
    },
    {
      "epoch": 6.824,
      "grad_norm": 0.23282396793365479,
      "learning_rate": 7.35e-06,
      "loss": 0.0026,
      "step": 127950
    },
    {
      "epoch": 6.824533333333333,
      "grad_norm": 0.3074468970298767,
      "learning_rate": 7.346666666666667e-06,
      "loss": 0.0024,
      "step": 127960
    },
    {
      "epoch": 6.825066666666666,
      "grad_norm": 0.038880325853824615,
      "learning_rate": 7.343333333333334e-06,
      "loss": 0.0019,
      "step": 127970
    },
    {
      "epoch": 6.8256,
      "grad_norm": 0.0979810431599617,
      "learning_rate": 7.340000000000001e-06,
      "loss": 0.0018,
      "step": 127980
    },
    {
      "epoch": 6.826133333333333,
      "grad_norm": 0.1474103182554245,
      "learning_rate": 7.336666666666667e-06,
      "loss": 0.0013,
      "step": 127990
    },
    {
      "epoch": 6.826666666666666,
      "grad_norm": 0.17406541109085083,
      "learning_rate": 7.333333333333334e-06,
      "loss": 0.0017,
      "step": 128000
    },
    {
      "epoch": 6.8272,
      "grad_norm": 0.06526439636945724,
      "learning_rate": 7.330000000000001e-06,
      "loss": 0.0021,
      "step": 128010
    },
    {
      "epoch": 6.827733333333334,
      "grad_norm": 0.11697172373533249,
      "learning_rate": 7.326666666666666e-06,
      "loss": 0.0013,
      "step": 128020
    },
    {
      "epoch": 6.828266666666667,
      "grad_norm": 0.12563547492027283,
      "learning_rate": 7.323333333333333e-06,
      "loss": 0.0018,
      "step": 128030
    },
    {
      "epoch": 6.8288,
      "grad_norm": 0.06600601971149445,
      "learning_rate": 7.32e-06,
      "loss": 0.0027,
      "step": 128040
    },
    {
      "epoch": 6.8293333333333335,
      "grad_norm": 0.12356105446815491,
      "learning_rate": 7.316666666666667e-06,
      "loss": 0.0014,
      "step": 128050
    },
    {
      "epoch": 6.829866666666667,
      "grad_norm": 0.03462846577167511,
      "learning_rate": 7.313333333333333e-06,
      "loss": 0.0016,
      "step": 128060
    },
    {
      "epoch": 6.8304,
      "grad_norm": 0.11534678936004639,
      "learning_rate": 7.31e-06,
      "loss": 0.0016,
      "step": 128070
    },
    {
      "epoch": 6.830933333333333,
      "grad_norm": 0.06501752883195877,
      "learning_rate": 7.306666666666667e-06,
      "loss": 0.002,
      "step": 128080
    },
    {
      "epoch": 6.831466666666667,
      "grad_norm": 0.19885221123695374,
      "learning_rate": 7.303333333333334e-06,
      "loss": 0.002,
      "step": 128090
    },
    {
      "epoch": 6.832,
      "grad_norm": 0.3468407690525055,
      "learning_rate": 7.2999999999999996e-06,
      "loss": 0.0019,
      "step": 128100
    },
    {
      "epoch": 6.832533333333333,
      "grad_norm": 0.3108764886856079,
      "learning_rate": 7.2966666666666665e-06,
      "loss": 0.0014,
      "step": 128110
    },
    {
      "epoch": 6.833066666666666,
      "grad_norm": 0.1989111751317978,
      "learning_rate": 7.293333333333334e-06,
      "loss": 0.0021,
      "step": 128120
    },
    {
      "epoch": 6.8336,
      "grad_norm": 0.17105518281459808,
      "learning_rate": 7.290000000000001e-06,
      "loss": 0.0017,
      "step": 128130
    },
    {
      "epoch": 6.834133333333333,
      "grad_norm": 0.2527899444103241,
      "learning_rate": 7.286666666666667e-06,
      "loss": 0.0029,
      "step": 128140
    },
    {
      "epoch": 6.834666666666667,
      "grad_norm": 0.040035318583250046,
      "learning_rate": 7.283333333333334e-06,
      "loss": 0.0014,
      "step": 128150
    },
    {
      "epoch": 6.8352,
      "grad_norm": 0.09033262729644775,
      "learning_rate": 7.280000000000001e-06,
      "loss": 0.0024,
      "step": 128160
    },
    {
      "epoch": 6.835733333333334,
      "grad_norm": 0.2597337067127228,
      "learning_rate": 7.276666666666667e-06,
      "loss": 0.0016,
      "step": 128170
    },
    {
      "epoch": 6.836266666666667,
      "grad_norm": 0.05042540654540062,
      "learning_rate": 7.273333333333334e-06,
      "loss": 0.002,
      "step": 128180
    },
    {
      "epoch": 6.8368,
      "grad_norm": 0.26004546880722046,
      "learning_rate": 7.270000000000001e-06,
      "loss": 0.0016,
      "step": 128190
    },
    {
      "epoch": 6.8373333333333335,
      "grad_norm": 0.09596274048089981,
      "learning_rate": 7.266666666666668e-06,
      "loss": 0.002,
      "step": 128200
    },
    {
      "epoch": 6.837866666666667,
      "grad_norm": 0.3160839080810547,
      "learning_rate": 7.263333333333333e-06,
      "loss": 0.0015,
      "step": 128210
    },
    {
      "epoch": 6.8384,
      "grad_norm": 0.22498469054698944,
      "learning_rate": 7.26e-06,
      "loss": 0.0019,
      "step": 128220
    },
    {
      "epoch": 6.838933333333333,
      "grad_norm": 0.1957465410232544,
      "learning_rate": 7.256666666666667e-06,
      "loss": 0.0013,
      "step": 128230
    },
    {
      "epoch": 6.839466666666667,
      "grad_norm": 0.27632781863212585,
      "learning_rate": 7.253333333333334e-06,
      "loss": 0.0019,
      "step": 128240
    },
    {
      "epoch": 6.84,
      "grad_norm": 0.11643296480178833,
      "learning_rate": 7.25e-06,
      "loss": 0.0013,
      "step": 128250
    },
    {
      "epoch": 6.840533333333333,
      "grad_norm": 0.19930100440979004,
      "learning_rate": 7.246666666666667e-06,
      "loss": 0.0033,
      "step": 128260
    },
    {
      "epoch": 6.841066666666666,
      "grad_norm": 0.37005582451820374,
      "learning_rate": 7.243333333333334e-06,
      "loss": 0.0018,
      "step": 128270
    },
    {
      "epoch": 6.8416,
      "grad_norm": 0.14874204993247986,
      "learning_rate": 7.240000000000001e-06,
      "loss": 0.0016,
      "step": 128280
    },
    {
      "epoch": 6.842133333333333,
      "grad_norm": 0.06141113117337227,
      "learning_rate": 7.236666666666666e-06,
      "loss": 0.0023,
      "step": 128290
    },
    {
      "epoch": 6.842666666666666,
      "grad_norm": 0.19635945558547974,
      "learning_rate": 7.233333333333333e-06,
      "loss": 0.0017,
      "step": 128300
    },
    {
      "epoch": 6.8431999999999995,
      "grad_norm": 0.39655476808547974,
      "learning_rate": 7.230000000000001e-06,
      "loss": 0.0014,
      "step": 128310
    },
    {
      "epoch": 6.843733333333334,
      "grad_norm": 0.5367332696914673,
      "learning_rate": 7.226666666666668e-06,
      "loss": 0.0018,
      "step": 128320
    },
    {
      "epoch": 6.844266666666667,
      "grad_norm": 0.17563918232917786,
      "learning_rate": 7.223333333333333e-06,
      "loss": 0.0024,
      "step": 128330
    },
    {
      "epoch": 6.8448,
      "grad_norm": 0.06831539422273636,
      "learning_rate": 7.22e-06,
      "loss": 0.0014,
      "step": 128340
    },
    {
      "epoch": 6.8453333333333335,
      "grad_norm": 0.25698789954185486,
      "learning_rate": 7.216666666666667e-06,
      "loss": 0.0017,
      "step": 128350
    },
    {
      "epoch": 6.845866666666667,
      "grad_norm": 0.17421703040599823,
      "learning_rate": 7.2133333333333334e-06,
      "loss": 0.0015,
      "step": 128360
    },
    {
      "epoch": 6.8464,
      "grad_norm": 0.08245781809091568,
      "learning_rate": 7.2100000000000004e-06,
      "loss": 0.0018,
      "step": 128370
    },
    {
      "epoch": 6.846933333333333,
      "grad_norm": 0.06754513829946518,
      "learning_rate": 7.206666666666667e-06,
      "loss": 0.0028,
      "step": 128380
    },
    {
      "epoch": 6.847466666666667,
      "grad_norm": 0.43916308879852295,
      "learning_rate": 7.203333333333334e-06,
      "loss": 0.0019,
      "step": 128390
    },
    {
      "epoch": 6.848,
      "grad_norm": 0.6839068531990051,
      "learning_rate": 7.2e-06,
      "loss": 0.0025,
      "step": 128400
    },
    {
      "epoch": 6.848533333333333,
      "grad_norm": 0.4023784101009369,
      "learning_rate": 7.196666666666667e-06,
      "loss": 0.0024,
      "step": 128410
    },
    {
      "epoch": 6.849066666666666,
      "grad_norm": 0.14706583321094513,
      "learning_rate": 7.193333333333334e-06,
      "loss": 0.0014,
      "step": 128420
    },
    {
      "epoch": 6.8496,
      "grad_norm": 0.11349481344223022,
      "learning_rate": 7.190000000000001e-06,
      "loss": 0.0015,
      "step": 128430
    },
    {
      "epoch": 6.850133333333333,
      "grad_norm": 0.2566883862018585,
      "learning_rate": 7.186666666666667e-06,
      "loss": 0.0015,
      "step": 128440
    },
    {
      "epoch": 6.850666666666667,
      "grad_norm": 0.09200452268123627,
      "learning_rate": 7.183333333333334e-06,
      "loss": 0.0016,
      "step": 128450
    },
    {
      "epoch": 6.8512,
      "grad_norm": 0.07186965644359589,
      "learning_rate": 7.180000000000001e-06,
      "loss": 0.0021,
      "step": 128460
    },
    {
      "epoch": 6.851733333333334,
      "grad_norm": 0.25746431946754456,
      "learning_rate": 7.176666666666668e-06,
      "loss": 0.0024,
      "step": 128470
    },
    {
      "epoch": 6.852266666666667,
      "grad_norm": 0.17579060792922974,
      "learning_rate": 7.173333333333333e-06,
      "loss": 0.0021,
      "step": 128480
    },
    {
      "epoch": 6.8528,
      "grad_norm": 0.211269348859787,
      "learning_rate": 7.17e-06,
      "loss": 0.0027,
      "step": 128490
    },
    {
      "epoch": 6.8533333333333335,
      "grad_norm": 0.06802813708782196,
      "learning_rate": 7.166666666666667e-06,
      "loss": 0.0026,
      "step": 128500
    },
    {
      "epoch": 6.853866666666667,
      "grad_norm": 0.14234989881515503,
      "learning_rate": 7.163333333333333e-06,
      "loss": 0.0019,
      "step": 128510
    },
    {
      "epoch": 6.8544,
      "grad_norm": 0.5249717831611633,
      "learning_rate": 7.16e-06,
      "loss": 0.0015,
      "step": 128520
    },
    {
      "epoch": 6.854933333333333,
      "grad_norm": 0.04891221597790718,
      "learning_rate": 7.156666666666667e-06,
      "loss": 0.0026,
      "step": 128530
    },
    {
      "epoch": 6.855466666666667,
      "grad_norm": 0.11890754848718643,
      "learning_rate": 7.153333333333334e-06,
      "loss": 0.0015,
      "step": 128540
    },
    {
      "epoch": 6.856,
      "grad_norm": 0.14784026145935059,
      "learning_rate": 7.15e-06,
      "loss": 0.0015,
      "step": 128550
    },
    {
      "epoch": 6.856533333333333,
      "grad_norm": 0.1325654238462448,
      "learning_rate": 7.146666666666667e-06,
      "loss": 0.0012,
      "step": 128560
    },
    {
      "epoch": 6.857066666666666,
      "grad_norm": 0.07275436073541641,
      "learning_rate": 7.143333333333334e-06,
      "loss": 0.0013,
      "step": 128570
    },
    {
      "epoch": 6.8576,
      "grad_norm": 0.12609025835990906,
      "learning_rate": 7.140000000000001e-06,
      "loss": 0.0014,
      "step": 128580
    },
    {
      "epoch": 6.858133333333333,
      "grad_norm": 0.18525008857250214,
      "learning_rate": 7.136666666666666e-06,
      "loss": 0.0013,
      "step": 128590
    },
    {
      "epoch": 6.858666666666666,
      "grad_norm": 0.24057836830615997,
      "learning_rate": 7.133333333333333e-06,
      "loss": 0.0013,
      "step": 128600
    },
    {
      "epoch": 6.8591999999999995,
      "grad_norm": 0.23627427220344543,
      "learning_rate": 7.13e-06,
      "loss": 0.0011,
      "step": 128610
    },
    {
      "epoch": 6.859733333333334,
      "grad_norm": 0.22575150430202484,
      "learning_rate": 7.126666666666667e-06,
      "loss": 0.0023,
      "step": 128620
    },
    {
      "epoch": 6.860266666666667,
      "grad_norm": 0.3779817521572113,
      "learning_rate": 7.1233333333333335e-06,
      "loss": 0.002,
      "step": 128630
    },
    {
      "epoch": 6.8608,
      "grad_norm": 0.3931112587451935,
      "learning_rate": 7.1200000000000004e-06,
      "loss": 0.003,
      "step": 128640
    },
    {
      "epoch": 6.8613333333333335,
      "grad_norm": 0.34701740741729736,
      "learning_rate": 7.116666666666667e-06,
      "loss": 0.002,
      "step": 128650
    },
    {
      "epoch": 6.861866666666667,
      "grad_norm": 0.04674054682254791,
      "learning_rate": 7.113333333333334e-06,
      "loss": 0.0014,
      "step": 128660
    },
    {
      "epoch": 6.8624,
      "grad_norm": 0.20028056204319,
      "learning_rate": 7.11e-06,
      "loss": 0.0012,
      "step": 128670
    },
    {
      "epoch": 6.862933333333333,
      "grad_norm": 0.03913649171590805,
      "learning_rate": 7.106666666666667e-06,
      "loss": 0.0021,
      "step": 128680
    },
    {
      "epoch": 6.863466666666667,
      "grad_norm": 0.28351908922195435,
      "learning_rate": 7.103333333333334e-06,
      "loss": 0.0018,
      "step": 128690
    },
    {
      "epoch": 6.864,
      "grad_norm": 0.26498278975486755,
      "learning_rate": 7.1e-06,
      "loss": 0.0018,
      "step": 128700
    },
    {
      "epoch": 6.864533333333333,
      "grad_norm": 0.12207718938589096,
      "learning_rate": 7.096666666666667e-06,
      "loss": 0.0019,
      "step": 128710
    },
    {
      "epoch": 6.865066666666666,
      "grad_norm": 0.04734373092651367,
      "learning_rate": 7.093333333333334e-06,
      "loss": 0.0016,
      "step": 128720
    },
    {
      "epoch": 6.8656,
      "grad_norm": 0.03712700679898262,
      "learning_rate": 7.090000000000001e-06,
      "loss": 0.0018,
      "step": 128730
    },
    {
      "epoch": 6.866133333333333,
      "grad_norm": 0.07517246901988983,
      "learning_rate": 7.086666666666667e-06,
      "loss": 0.0021,
      "step": 128740
    },
    {
      "epoch": 6.866666666666667,
      "grad_norm": 0.3161422312259674,
      "learning_rate": 7.083333333333334e-06,
      "loss": 0.0017,
      "step": 128750
    },
    {
      "epoch": 6.8672,
      "grad_norm": 0.3681526780128479,
      "learning_rate": 7.080000000000001e-06,
      "loss": 0.0026,
      "step": 128760
    },
    {
      "epoch": 6.867733333333334,
      "grad_norm": 0.06669263541698456,
      "learning_rate": 7.076666666666668e-06,
      "loss": 0.002,
      "step": 128770
    },
    {
      "epoch": 6.868266666666667,
      "grad_norm": 0.14950434863567352,
      "learning_rate": 7.073333333333333e-06,
      "loss": 0.0012,
      "step": 128780
    },
    {
      "epoch": 6.8688,
      "grad_norm": 0.11617089807987213,
      "learning_rate": 7.07e-06,
      "loss": 0.002,
      "step": 128790
    },
    {
      "epoch": 6.8693333333333335,
      "grad_norm": 0.10345689207315445,
      "learning_rate": 7.066666666666667e-06,
      "loss": 0.0023,
      "step": 128800
    },
    {
      "epoch": 6.869866666666667,
      "grad_norm": 0.34249913692474365,
      "learning_rate": 7.063333333333334e-06,
      "loss": 0.0022,
      "step": 128810
    },
    {
      "epoch": 6.8704,
      "grad_norm": 0.31254395842552185,
      "learning_rate": 7.06e-06,
      "loss": 0.0012,
      "step": 128820
    },
    {
      "epoch": 6.870933333333333,
      "grad_norm": 0.37156930565834045,
      "learning_rate": 7.056666666666667e-06,
      "loss": 0.0017,
      "step": 128830
    },
    {
      "epoch": 6.871466666666667,
      "grad_norm": 0.0931004211306572,
      "learning_rate": 7.053333333333334e-06,
      "loss": 0.0017,
      "step": 128840
    },
    {
      "epoch": 6.872,
      "grad_norm": 0.06336048990488052,
      "learning_rate": 7.049999999999999e-06,
      "loss": 0.0023,
      "step": 128850
    },
    {
      "epoch": 6.872533333333333,
      "grad_norm": 0.060720883309841156,
      "learning_rate": 7.046666666666666e-06,
      "loss": 0.0013,
      "step": 128860
    },
    {
      "epoch": 6.873066666666666,
      "grad_norm": 0.5301843285560608,
      "learning_rate": 7.043333333333333e-06,
      "loss": 0.0022,
      "step": 128870
    },
    {
      "epoch": 6.8736,
      "grad_norm": 0.3228309154510498,
      "learning_rate": 7.04e-06,
      "loss": 0.0028,
      "step": 128880
    },
    {
      "epoch": 6.874133333333333,
      "grad_norm": 0.05206408351659775,
      "learning_rate": 7.0366666666666665e-06,
      "loss": 0.0018,
      "step": 128890
    },
    {
      "epoch": 6.874666666666666,
      "grad_norm": 0.26595523953437805,
      "learning_rate": 7.0333333333333335e-06,
      "loss": 0.0017,
      "step": 128900
    },
    {
      "epoch": 6.8751999999999995,
      "grad_norm": 0.17619942128658295,
      "learning_rate": 7.0300000000000005e-06,
      "loss": 0.0018,
      "step": 128910
    },
    {
      "epoch": 6.875733333333334,
      "grad_norm": 0.21906441450119019,
      "learning_rate": 7.0266666666666674e-06,
      "loss": 0.0016,
      "step": 128920
    },
    {
      "epoch": 6.876266666666667,
      "grad_norm": 0.20016148686408997,
      "learning_rate": 7.0233333333333336e-06,
      "loss": 0.0018,
      "step": 128930
    },
    {
      "epoch": 6.8768,
      "grad_norm": 0.17927785217761993,
      "learning_rate": 7.0200000000000006e-06,
      "loss": 0.0018,
      "step": 128940
    },
    {
      "epoch": 6.8773333333333335,
      "grad_norm": 0.08847931027412415,
      "learning_rate": 7.0166666666666675e-06,
      "loss": 0.0015,
      "step": 128950
    },
    {
      "epoch": 6.877866666666667,
      "grad_norm": 0.10840851068496704,
      "learning_rate": 7.0133333333333345e-06,
      "loss": 0.0019,
      "step": 128960
    },
    {
      "epoch": 6.8784,
      "grad_norm": 0.3135475814342499,
      "learning_rate": 7.01e-06,
      "loss": 0.0015,
      "step": 128970
    },
    {
      "epoch": 6.878933333333333,
      "grad_norm": 0.061699654906988144,
      "learning_rate": 7.006666666666667e-06,
      "loss": 0.0019,
      "step": 128980
    },
    {
      "epoch": 6.879466666666667,
      "grad_norm": 0.039090994745492935,
      "learning_rate": 7.003333333333334e-06,
      "loss": 0.002,
      "step": 128990
    },
    {
      "epoch": 6.88,
      "grad_norm": 0.1174345389008522,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.0017,
      "step": 129000
    },
    {
      "epoch": 6.880533333333333,
      "grad_norm": 0.03772621229290962,
      "learning_rate": 6.996666666666667e-06,
      "loss": 0.0015,
      "step": 129010
    },
    {
      "epoch": 6.881066666666666,
      "grad_norm": 0.34400323033332825,
      "learning_rate": 6.993333333333334e-06,
      "loss": 0.0017,
      "step": 129020
    },
    {
      "epoch": 6.8816,
      "grad_norm": 0.1753503829240799,
      "learning_rate": 6.990000000000001e-06,
      "loss": 0.0017,
      "step": 129030
    },
    {
      "epoch": 6.882133333333333,
      "grad_norm": 0.2328735888004303,
      "learning_rate": 6.986666666666666e-06,
      "loss": 0.0013,
      "step": 129040
    },
    {
      "epoch": 6.882666666666667,
      "grad_norm": 0.05789027363061905,
      "learning_rate": 6.983333333333333e-06,
      "loss": 0.0019,
      "step": 129050
    },
    {
      "epoch": 6.8832,
      "grad_norm": 0.3683689534664154,
      "learning_rate": 6.98e-06,
      "loss": 0.0029,
      "step": 129060
    },
    {
      "epoch": 6.883733333333334,
      "grad_norm": 0.36649975180625916,
      "learning_rate": 6.976666666666667e-06,
      "loss": 0.0025,
      "step": 129070
    },
    {
      "epoch": 6.884266666666667,
      "grad_norm": 0.045290570706129074,
      "learning_rate": 6.973333333333333e-06,
      "loss": 0.0015,
      "step": 129080
    },
    {
      "epoch": 6.8848,
      "grad_norm": 0.4284834861755371,
      "learning_rate": 6.97e-06,
      "loss": 0.0019,
      "step": 129090
    },
    {
      "epoch": 6.8853333333333335,
      "grad_norm": 0.14847242832183838,
      "learning_rate": 6.966666666666667e-06,
      "loss": 0.0023,
      "step": 129100
    },
    {
      "epoch": 6.885866666666667,
      "grad_norm": 0.4534631371498108,
      "learning_rate": 6.963333333333334e-06,
      "loss": 0.0013,
      "step": 129110
    },
    {
      "epoch": 6.8864,
      "grad_norm": 0.27502962946891785,
      "learning_rate": 6.9599999999999994e-06,
      "loss": 0.0015,
      "step": 129120
    },
    {
      "epoch": 6.886933333333333,
      "grad_norm": 0.176569402217865,
      "learning_rate": 6.956666666666667e-06,
      "loss": 0.0014,
      "step": 129130
    },
    {
      "epoch": 6.887466666666667,
      "grad_norm": 0.2421206384897232,
      "learning_rate": 6.953333333333334e-06,
      "loss": 0.0022,
      "step": 129140
    },
    {
      "epoch": 6.888,
      "grad_norm": 0.07121340930461884,
      "learning_rate": 6.950000000000001e-06,
      "loss": 0.0018,
      "step": 129150
    },
    {
      "epoch": 6.888533333333333,
      "grad_norm": 0.7496456503868103,
      "learning_rate": 6.9466666666666665e-06,
      "loss": 0.0024,
      "step": 129160
    },
    {
      "epoch": 6.8890666666666664,
      "grad_norm": 0.16139958798885345,
      "learning_rate": 6.9433333333333335e-06,
      "loss": 0.002,
      "step": 129170
    },
    {
      "epoch": 6.8896,
      "grad_norm": 0.06717604398727417,
      "learning_rate": 6.9400000000000005e-06,
      "loss": 0.0019,
      "step": 129180
    },
    {
      "epoch": 6.890133333333333,
      "grad_norm": 0.3389135003089905,
      "learning_rate": 6.936666666666667e-06,
      "loss": 0.0017,
      "step": 129190
    },
    {
      "epoch": 6.890666666666666,
      "grad_norm": 0.23149673640727997,
      "learning_rate": 6.933333333333334e-06,
      "loss": 0.0012,
      "step": 129200
    },
    {
      "epoch": 6.8911999999999995,
      "grad_norm": 0.22444956004619598,
      "learning_rate": 6.9300000000000006e-06,
      "loss": 0.0015,
      "step": 129210
    },
    {
      "epoch": 6.891733333333334,
      "grad_norm": 0.08070063591003418,
      "learning_rate": 6.9266666666666675e-06,
      "loss": 0.0017,
      "step": 129220
    },
    {
      "epoch": 6.892266666666667,
      "grad_norm": 0.17351296544075012,
      "learning_rate": 6.923333333333333e-06,
      "loss": 0.002,
      "step": 129230
    },
    {
      "epoch": 6.8928,
      "grad_norm": 0.15045572817325592,
      "learning_rate": 6.92e-06,
      "loss": 0.0033,
      "step": 129240
    },
    {
      "epoch": 6.8933333333333335,
      "grad_norm": 0.17582160234451294,
      "learning_rate": 6.916666666666667e-06,
      "loss": 0.002,
      "step": 129250
    },
    {
      "epoch": 6.893866666666667,
      "grad_norm": 0.0646289736032486,
      "learning_rate": 6.913333333333334e-06,
      "loss": 0.0016,
      "step": 129260
    },
    {
      "epoch": 6.8944,
      "grad_norm": 0.13501940667629242,
      "learning_rate": 6.91e-06,
      "loss": 0.0017,
      "step": 129270
    },
    {
      "epoch": 6.894933333333333,
      "grad_norm": 0.22744841873645782,
      "learning_rate": 6.906666666666667e-06,
      "loss": 0.0023,
      "step": 129280
    },
    {
      "epoch": 6.895466666666667,
      "grad_norm": 0.28868165612220764,
      "learning_rate": 6.903333333333334e-06,
      "loss": 0.0016,
      "step": 129290
    },
    {
      "epoch": 6.896,
      "grad_norm": 0.1732081174850464,
      "learning_rate": 6.900000000000001e-06,
      "loss": 0.0017,
      "step": 129300
    },
    {
      "epoch": 6.896533333333333,
      "grad_norm": 0.09949866682291031,
      "learning_rate": 6.896666666666666e-06,
      "loss": 0.0021,
      "step": 129310
    },
    {
      "epoch": 6.8970666666666665,
      "grad_norm": 0.330871045589447,
      "learning_rate": 6.893333333333334e-06,
      "loss": 0.0019,
      "step": 129320
    },
    {
      "epoch": 6.8976,
      "grad_norm": 0.2076673060655594,
      "learning_rate": 6.890000000000001e-06,
      "loss": 0.0014,
      "step": 129330
    },
    {
      "epoch": 6.898133333333333,
      "grad_norm": 0.26349836587905884,
      "learning_rate": 6.886666666666668e-06,
      "loss": 0.0015,
      "step": 129340
    },
    {
      "epoch": 6.898666666666666,
      "grad_norm": 0.10463032871484756,
      "learning_rate": 6.883333333333333e-06,
      "loss": 0.0023,
      "step": 129350
    },
    {
      "epoch": 6.8992,
      "grad_norm": 0.2990607023239136,
      "learning_rate": 6.88e-06,
      "loss": 0.002,
      "step": 129360
    },
    {
      "epoch": 6.899733333333334,
      "grad_norm": 0.06218424811959267,
      "learning_rate": 6.876666666666667e-06,
      "loss": 0.0017,
      "step": 129370
    },
    {
      "epoch": 6.900266666666667,
      "grad_norm": 0.18257685005664825,
      "learning_rate": 6.873333333333333e-06,
      "loss": 0.0019,
      "step": 129380
    },
    {
      "epoch": 6.9008,
      "grad_norm": 0.05957677587866783,
      "learning_rate": 6.87e-06,
      "loss": 0.0021,
      "step": 129390
    },
    {
      "epoch": 6.9013333333333335,
      "grad_norm": 0.3963480293750763,
      "learning_rate": 6.866666666666667e-06,
      "loss": 0.0025,
      "step": 129400
    },
    {
      "epoch": 6.901866666666667,
      "grad_norm": 0.04482325166463852,
      "learning_rate": 6.863333333333334e-06,
      "loss": 0.0019,
      "step": 129410
    },
    {
      "epoch": 6.9024,
      "grad_norm": 0.1421845704317093,
      "learning_rate": 6.8599999999999995e-06,
      "loss": 0.0016,
      "step": 129420
    },
    {
      "epoch": 6.902933333333333,
      "grad_norm": 0.253733366727829,
      "learning_rate": 6.8566666666666665e-06,
      "loss": 0.0012,
      "step": 129430
    },
    {
      "epoch": 6.903466666666667,
      "grad_norm": 0.39835429191589355,
      "learning_rate": 6.8533333333333335e-06,
      "loss": 0.0014,
      "step": 129440
    },
    {
      "epoch": 6.904,
      "grad_norm": 0.11565408855676651,
      "learning_rate": 6.8500000000000005e-06,
      "loss": 0.0013,
      "step": 129450
    },
    {
      "epoch": 6.904533333333333,
      "grad_norm": 0.11028562486171722,
      "learning_rate": 6.846666666666667e-06,
      "loss": 0.0016,
      "step": 129460
    },
    {
      "epoch": 6.9050666666666665,
      "grad_norm": 0.14953167736530304,
      "learning_rate": 6.843333333333334e-06,
      "loss": 0.0013,
      "step": 129470
    },
    {
      "epoch": 6.9056,
      "grad_norm": 0.06561160832643509,
      "learning_rate": 6.840000000000001e-06,
      "loss": 0.0014,
      "step": 129480
    },
    {
      "epoch": 6.906133333333333,
      "grad_norm": 0.5335858464241028,
      "learning_rate": 6.8366666666666676e-06,
      "loss": 0.0019,
      "step": 129490
    },
    {
      "epoch": 6.906666666666666,
      "grad_norm": 0.20708702504634857,
      "learning_rate": 6.833333333333333e-06,
      "loss": 0.0019,
      "step": 129500
    },
    {
      "epoch": 6.9072,
      "grad_norm": 0.15266990661621094,
      "learning_rate": 6.830000000000001e-06,
      "loss": 0.0029,
      "step": 129510
    },
    {
      "epoch": 6.907733333333333,
      "grad_norm": 0.22783248126506805,
      "learning_rate": 6.826666666666668e-06,
      "loss": 0.0014,
      "step": 129520
    },
    {
      "epoch": 6.908266666666667,
      "grad_norm": 0.08757543563842773,
      "learning_rate": 6.823333333333333e-06,
      "loss": 0.002,
      "step": 129530
    },
    {
      "epoch": 6.9088,
      "grad_norm": 0.5375115275382996,
      "learning_rate": 6.82e-06,
      "loss": 0.0015,
      "step": 129540
    },
    {
      "epoch": 6.9093333333333335,
      "grad_norm": 0.40971142053604126,
      "learning_rate": 6.816666666666667e-06,
      "loss": 0.0016,
      "step": 129550
    },
    {
      "epoch": 6.909866666666667,
      "grad_norm": 0.1274741142988205,
      "learning_rate": 6.813333333333334e-06,
      "loss": 0.0017,
      "step": 129560
    },
    {
      "epoch": 6.9104,
      "grad_norm": 0.05149054527282715,
      "learning_rate": 6.81e-06,
      "loss": 0.0014,
      "step": 129570
    },
    {
      "epoch": 6.910933333333333,
      "grad_norm": 0.037384238094091415,
      "learning_rate": 6.806666666666667e-06,
      "loss": 0.0019,
      "step": 129580
    },
    {
      "epoch": 6.911466666666667,
      "grad_norm": 0.5203817486763,
      "learning_rate": 6.803333333333334e-06,
      "loss": 0.0013,
      "step": 129590
    },
    {
      "epoch": 6.912,
      "grad_norm": 0.22972865402698517,
      "learning_rate": 6.800000000000001e-06,
      "loss": 0.0018,
      "step": 129600
    },
    {
      "epoch": 6.912533333333333,
      "grad_norm": 0.25683343410491943,
      "learning_rate": 6.796666666666666e-06,
      "loss": 0.0024,
      "step": 129610
    },
    {
      "epoch": 6.9130666666666665,
      "grad_norm": 0.25525912642478943,
      "learning_rate": 6.793333333333333e-06,
      "loss": 0.002,
      "step": 129620
    },
    {
      "epoch": 6.9136,
      "grad_norm": 0.23282615840435028,
      "learning_rate": 6.79e-06,
      "loss": 0.0012,
      "step": 129630
    },
    {
      "epoch": 6.914133333333333,
      "grad_norm": 0.11476877331733704,
      "learning_rate": 6.786666666666667e-06,
      "loss": 0.0014,
      "step": 129640
    },
    {
      "epoch": 6.914666666666666,
      "grad_norm": 0.20567575097084045,
      "learning_rate": 6.783333333333333e-06,
      "loss": 0.0015,
      "step": 129650
    },
    {
      "epoch": 6.9152000000000005,
      "grad_norm": 0.11454946547746658,
      "learning_rate": 6.78e-06,
      "loss": 0.0018,
      "step": 129660
    },
    {
      "epoch": 6.915733333333334,
      "grad_norm": 0.5136584639549255,
      "learning_rate": 6.776666666666667e-06,
      "loss": 0.0022,
      "step": 129670
    },
    {
      "epoch": 6.916266666666667,
      "grad_norm": 0.17143458127975464,
      "learning_rate": 6.773333333333334e-06,
      "loss": 0.0025,
      "step": 129680
    },
    {
      "epoch": 6.9168,
      "grad_norm": 0.3505949378013611,
      "learning_rate": 6.7699999999999996e-06,
      "loss": 0.002,
      "step": 129690
    },
    {
      "epoch": 6.917333333333334,
      "grad_norm": 0.31264039874076843,
      "learning_rate": 6.766666666666667e-06,
      "loss": 0.0015,
      "step": 129700
    },
    {
      "epoch": 6.917866666666667,
      "grad_norm": 0.038417600095272064,
      "learning_rate": 6.763333333333334e-06,
      "loss": 0.0016,
      "step": 129710
    },
    {
      "epoch": 6.9184,
      "grad_norm": 0.14328040182590485,
      "learning_rate": 6.76e-06,
      "loss": 0.0021,
      "step": 129720
    },
    {
      "epoch": 6.918933333333333,
      "grad_norm": 0.3104974627494812,
      "learning_rate": 6.756666666666667e-06,
      "loss": 0.002,
      "step": 129730
    },
    {
      "epoch": 6.919466666666667,
      "grad_norm": 0.07719336450099945,
      "learning_rate": 6.753333333333334e-06,
      "loss": 0.0016,
      "step": 129740
    },
    {
      "epoch": 6.92,
      "grad_norm": 0.08708042651414871,
      "learning_rate": 6.750000000000001e-06,
      "loss": 0.0017,
      "step": 129750
    },
    {
      "epoch": 6.920533333333333,
      "grad_norm": 0.13002072274684906,
      "learning_rate": 6.746666666666667e-06,
      "loss": 0.0022,
      "step": 129760
    },
    {
      "epoch": 6.9210666666666665,
      "grad_norm": 0.03507542982697487,
      "learning_rate": 6.743333333333334e-06,
      "loss": 0.0026,
      "step": 129770
    },
    {
      "epoch": 6.9216,
      "grad_norm": 0.1732359677553177,
      "learning_rate": 6.740000000000001e-06,
      "loss": 0.0017,
      "step": 129780
    },
    {
      "epoch": 6.922133333333333,
      "grad_norm": 0.1416042447090149,
      "learning_rate": 6.736666666666668e-06,
      "loss": 0.0019,
      "step": 129790
    },
    {
      "epoch": 6.922666666666666,
      "grad_norm": 0.17361833155155182,
      "learning_rate": 6.733333333333333e-06,
      "loss": 0.0015,
      "step": 129800
    },
    {
      "epoch": 6.9232,
      "grad_norm": 0.17589101195335388,
      "learning_rate": 6.73e-06,
      "loss": 0.0015,
      "step": 129810
    },
    {
      "epoch": 6.923733333333333,
      "grad_norm": 0.19772741198539734,
      "learning_rate": 6.726666666666667e-06,
      "loss": 0.0025,
      "step": 129820
    },
    {
      "epoch": 6.924266666666667,
      "grad_norm": 0.14512112736701965,
      "learning_rate": 6.723333333333334e-06,
      "loss": 0.0019,
      "step": 129830
    },
    {
      "epoch": 6.9248,
      "grad_norm": 0.05818881839513779,
      "learning_rate": 6.72e-06,
      "loss": 0.0017,
      "step": 129840
    },
    {
      "epoch": 6.925333333333334,
      "grad_norm": 0.2729298174381256,
      "learning_rate": 6.716666666666667e-06,
      "loss": 0.0027,
      "step": 129850
    },
    {
      "epoch": 6.925866666666667,
      "grad_norm": 0.2861590087413788,
      "learning_rate": 6.713333333333334e-06,
      "loss": 0.0012,
      "step": 129860
    },
    {
      "epoch": 6.9264,
      "grad_norm": 0.2857778072357178,
      "learning_rate": 6.710000000000001e-06,
      "loss": 0.0021,
      "step": 129870
    },
    {
      "epoch": 6.926933333333333,
      "grad_norm": 0.121016725897789,
      "learning_rate": 6.706666666666666e-06,
      "loss": 0.0021,
      "step": 129880
    },
    {
      "epoch": 6.927466666666667,
      "grad_norm": 0.055765777826309204,
      "learning_rate": 6.703333333333334e-06,
      "loss": 0.0027,
      "step": 129890
    },
    {
      "epoch": 6.928,
      "grad_norm": 0.06582643836736679,
      "learning_rate": 6.700000000000001e-06,
      "loss": 0.0019,
      "step": 129900
    },
    {
      "epoch": 6.928533333333333,
      "grad_norm": 0.041068777441978455,
      "learning_rate": 6.696666666666666e-06,
      "loss": 0.0018,
      "step": 129910
    },
    {
      "epoch": 6.9290666666666665,
      "grad_norm": 0.22475284337997437,
      "learning_rate": 6.693333333333333e-06,
      "loss": 0.0018,
      "step": 129920
    },
    {
      "epoch": 6.9296,
      "grad_norm": 0.20793722569942474,
      "learning_rate": 6.69e-06,
      "loss": 0.0015,
      "step": 129930
    },
    {
      "epoch": 6.930133333333333,
      "grad_norm": 0.06631381064653397,
      "learning_rate": 6.686666666666667e-06,
      "loss": 0.0013,
      "step": 129940
    },
    {
      "epoch": 6.930666666666666,
      "grad_norm": 0.1051628515124321,
      "learning_rate": 6.6833333333333334e-06,
      "loss": 0.0012,
      "step": 129950
    },
    {
      "epoch": 6.9312000000000005,
      "grad_norm": 0.11853926628828049,
      "learning_rate": 6.68e-06,
      "loss": 0.0021,
      "step": 129960
    },
    {
      "epoch": 6.931733333333334,
      "grad_norm": 0.15490928292274475,
      "learning_rate": 6.676666666666667e-06,
      "loss": 0.0017,
      "step": 129970
    },
    {
      "epoch": 6.932266666666667,
      "grad_norm": 0.28130412101745605,
      "learning_rate": 6.673333333333334e-06,
      "loss": 0.0026,
      "step": 129980
    },
    {
      "epoch": 6.9328,
      "grad_norm": 0.0812014639377594,
      "learning_rate": 6.67e-06,
      "loss": 0.0019,
      "step": 129990
    },
    {
      "epoch": 6.933333333333334,
      "grad_norm": 0.16921794414520264,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.0014,
      "step": 130000
    },
    {
      "epoch": 6.933866666666667,
      "grad_norm": 0.18705764412879944,
      "learning_rate": 6.663333333333334e-06,
      "loss": 0.003,
      "step": 130010
    },
    {
      "epoch": 6.9344,
      "grad_norm": 0.17583416402339935,
      "learning_rate": 6.660000000000001e-06,
      "loss": 0.0014,
      "step": 130020
    },
    {
      "epoch": 6.934933333333333,
      "grad_norm": 0.08633807301521301,
      "learning_rate": 6.656666666666667e-06,
      "loss": 0.0014,
      "step": 130030
    },
    {
      "epoch": 6.935466666666667,
      "grad_norm": 0.05239484831690788,
      "learning_rate": 6.653333333333334e-06,
      "loss": 0.0015,
      "step": 130040
    },
    {
      "epoch": 6.936,
      "grad_norm": 0.3753698170185089,
      "learning_rate": 6.650000000000001e-06,
      "loss": 0.0013,
      "step": 130050
    },
    {
      "epoch": 6.936533333333333,
      "grad_norm": 0.2898663282394409,
      "learning_rate": 6.646666666666666e-06,
      "loss": 0.0019,
      "step": 130060
    },
    {
      "epoch": 6.9370666666666665,
      "grad_norm": 0.2251104861497879,
      "learning_rate": 6.643333333333333e-06,
      "loss": 0.0023,
      "step": 130070
    },
    {
      "epoch": 6.9376,
      "grad_norm": 0.09277074784040451,
      "learning_rate": 6.640000000000001e-06,
      "loss": 0.0016,
      "step": 130080
    },
    {
      "epoch": 6.938133333333333,
      "grad_norm": 0.22617436945438385,
      "learning_rate": 6.636666666666668e-06,
      "loss": 0.0013,
      "step": 130090
    },
    {
      "epoch": 6.938666666666666,
      "grad_norm": 0.15468095242977142,
      "learning_rate": 6.633333333333333e-06,
      "loss": 0.0016,
      "step": 130100
    },
    {
      "epoch": 6.9392,
      "grad_norm": 0.2516440451145172,
      "learning_rate": 6.63e-06,
      "loss": 0.0021,
      "step": 130110
    },
    {
      "epoch": 6.939733333333333,
      "grad_norm": 0.3209458589553833,
      "learning_rate": 6.626666666666667e-06,
      "loss": 0.0014,
      "step": 130120
    },
    {
      "epoch": 6.940266666666667,
      "grad_norm": 0.43234124779701233,
      "learning_rate": 6.623333333333334e-06,
      "loss": 0.0022,
      "step": 130130
    },
    {
      "epoch": 6.9408,
      "grad_norm": 0.28861722350120544,
      "learning_rate": 6.62e-06,
      "loss": 0.0014,
      "step": 130140
    },
    {
      "epoch": 6.941333333333334,
      "grad_norm": 0.03786445036530495,
      "learning_rate": 6.616666666666667e-06,
      "loss": 0.0018,
      "step": 130150
    },
    {
      "epoch": 6.941866666666667,
      "grad_norm": 0.24450969696044922,
      "learning_rate": 6.613333333333334e-06,
      "loss": 0.002,
      "step": 130160
    },
    {
      "epoch": 6.9424,
      "grad_norm": 0.0658024325966835,
      "learning_rate": 6.610000000000001e-06,
      "loss": 0.0018,
      "step": 130170
    },
    {
      "epoch": 6.942933333333333,
      "grad_norm": 0.20443418622016907,
      "learning_rate": 6.606666666666666e-06,
      "loss": 0.0023,
      "step": 130180
    },
    {
      "epoch": 6.943466666666667,
      "grad_norm": 0.06791259348392487,
      "learning_rate": 6.603333333333333e-06,
      "loss": 0.0016,
      "step": 130190
    },
    {
      "epoch": 6.944,
      "grad_norm": 0.14558228850364685,
      "learning_rate": 6.6e-06,
      "loss": 0.002,
      "step": 130200
    },
    {
      "epoch": 6.944533333333333,
      "grad_norm": 0.11676953732967377,
      "learning_rate": 6.596666666666667e-06,
      "loss": 0.0028,
      "step": 130210
    },
    {
      "epoch": 6.9450666666666665,
      "grad_norm": 0.18409954011440277,
      "learning_rate": 6.5933333333333335e-06,
      "loss": 0.0028,
      "step": 130220
    },
    {
      "epoch": 6.9456,
      "grad_norm": 0.1192549467086792,
      "learning_rate": 6.5900000000000004e-06,
      "loss": 0.0014,
      "step": 130230
    },
    {
      "epoch": 6.946133333333333,
      "grad_norm": 0.0888274684548378,
      "learning_rate": 6.586666666666667e-06,
      "loss": 0.0016,
      "step": 130240
    },
    {
      "epoch": 6.946666666666666,
      "grad_norm": 0.16937053203582764,
      "learning_rate": 6.583333333333333e-06,
      "loss": 0.002,
      "step": 130250
    },
    {
      "epoch": 6.9472000000000005,
      "grad_norm": 0.09006102383136749,
      "learning_rate": 6.58e-06,
      "loss": 0.0018,
      "step": 130260
    },
    {
      "epoch": 6.947733333333334,
      "grad_norm": 0.2625982463359833,
      "learning_rate": 6.5766666666666675e-06,
      "loss": 0.0015,
      "step": 130270
    },
    {
      "epoch": 6.948266666666667,
      "grad_norm": 0.19039466977119446,
      "learning_rate": 6.5733333333333345e-06,
      "loss": 0.0016,
      "step": 130280
    },
    {
      "epoch": 6.9488,
      "grad_norm": 0.37001147866249084,
      "learning_rate": 6.57e-06,
      "loss": 0.0016,
      "step": 130290
    },
    {
      "epoch": 6.949333333333334,
      "grad_norm": 0.45503807067871094,
      "learning_rate": 6.566666666666667e-06,
      "loss": 0.0025,
      "step": 130300
    },
    {
      "epoch": 6.949866666666667,
      "grad_norm": 0.04278722405433655,
      "learning_rate": 6.563333333333334e-06,
      "loss": 0.002,
      "step": 130310
    },
    {
      "epoch": 6.9504,
      "grad_norm": 0.03164222836494446,
      "learning_rate": 6.560000000000001e-06,
      "loss": 0.0022,
      "step": 130320
    },
    {
      "epoch": 6.950933333333333,
      "grad_norm": 0.23912930488586426,
      "learning_rate": 6.556666666666667e-06,
      "loss": 0.0017,
      "step": 130330
    },
    {
      "epoch": 6.951466666666667,
      "grad_norm": 0.28562435507774353,
      "learning_rate": 6.553333333333334e-06,
      "loss": 0.0018,
      "step": 130340
    },
    {
      "epoch": 6.952,
      "grad_norm": 0.17149700224399567,
      "learning_rate": 6.550000000000001e-06,
      "loss": 0.0022,
      "step": 130350
    },
    {
      "epoch": 6.952533333333333,
      "grad_norm": 0.37889909744262695,
      "learning_rate": 6.546666666666668e-06,
      "loss": 0.0014,
      "step": 130360
    },
    {
      "epoch": 6.9530666666666665,
      "grad_norm": 0.19882605969905853,
      "learning_rate": 6.543333333333333e-06,
      "loss": 0.0026,
      "step": 130370
    },
    {
      "epoch": 6.9536,
      "grad_norm": 0.056568220257759094,
      "learning_rate": 6.54e-06,
      "loss": 0.0012,
      "step": 130380
    },
    {
      "epoch": 6.954133333333333,
      "grad_norm": 0.11017885059118271,
      "learning_rate": 6.536666666666667e-06,
      "loss": 0.0029,
      "step": 130390
    },
    {
      "epoch": 6.954666666666666,
      "grad_norm": 0.09492547810077667,
      "learning_rate": 6.533333333333333e-06,
      "loss": 0.0013,
      "step": 130400
    },
    {
      "epoch": 6.9552,
      "grad_norm": 0.5373156070709229,
      "learning_rate": 6.53e-06,
      "loss": 0.0024,
      "step": 130410
    },
    {
      "epoch": 6.955733333333333,
      "grad_norm": 0.06126917898654938,
      "learning_rate": 6.526666666666667e-06,
      "loss": 0.0018,
      "step": 130420
    },
    {
      "epoch": 6.956266666666667,
      "grad_norm": 0.22662542760372162,
      "learning_rate": 6.523333333333334e-06,
      "loss": 0.0025,
      "step": 130430
    },
    {
      "epoch": 6.9568,
      "grad_norm": 0.2332942634820938,
      "learning_rate": 6.519999999999999e-06,
      "loss": 0.002,
      "step": 130440
    },
    {
      "epoch": 6.957333333333334,
      "grad_norm": 0.6124964952468872,
      "learning_rate": 6.516666666666666e-06,
      "loss": 0.0014,
      "step": 130450
    },
    {
      "epoch": 6.957866666666667,
      "grad_norm": 0.2043052464723587,
      "learning_rate": 6.513333333333333e-06,
      "loss": 0.0021,
      "step": 130460
    },
    {
      "epoch": 6.9584,
      "grad_norm": 0.22744062542915344,
      "learning_rate": 6.510000000000001e-06,
      "loss": 0.0017,
      "step": 130470
    },
    {
      "epoch": 6.958933333333333,
      "grad_norm": 0.039158910512924194,
      "learning_rate": 6.5066666666666665e-06,
      "loss": 0.002,
      "step": 130480
    },
    {
      "epoch": 6.959466666666667,
      "grad_norm": 0.11444176733493805,
      "learning_rate": 6.5033333333333335e-06,
      "loss": 0.002,
      "step": 130490
    },
    {
      "epoch": 6.96,
      "grad_norm": 0.03954511508345604,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 0.0022,
      "step": 130500
    },
    {
      "epoch": 6.960533333333333,
      "grad_norm": 0.22242173552513123,
      "learning_rate": 6.4966666666666674e-06,
      "loss": 0.0014,
      "step": 130510
    },
    {
      "epoch": 6.9610666666666665,
      "grad_norm": 0.07967077940702438,
      "learning_rate": 6.4933333333333336e-06,
      "loss": 0.0018,
      "step": 130520
    },
    {
      "epoch": 6.9616,
      "grad_norm": 0.04514819756150246,
      "learning_rate": 6.4900000000000005e-06,
      "loss": 0.0021,
      "step": 130530
    },
    {
      "epoch": 6.962133333333333,
      "grad_norm": 0.3713768720626831,
      "learning_rate": 6.4866666666666675e-06,
      "loss": 0.0014,
      "step": 130540
    },
    {
      "epoch": 6.962666666666666,
      "grad_norm": 0.11677078157663345,
      "learning_rate": 6.4833333333333345e-06,
      "loss": 0.0015,
      "step": 130550
    },
    {
      "epoch": 6.9632,
      "grad_norm": 0.1712290346622467,
      "learning_rate": 6.48e-06,
      "loss": 0.0012,
      "step": 130560
    },
    {
      "epoch": 6.963733333333334,
      "grad_norm": 0.06145031750202179,
      "learning_rate": 6.476666666666667e-06,
      "loss": 0.0012,
      "step": 130570
    },
    {
      "epoch": 6.964266666666667,
      "grad_norm": 0.3123774826526642,
      "learning_rate": 6.473333333333334e-06,
      "loss": 0.0023,
      "step": 130580
    },
    {
      "epoch": 6.9648,
      "grad_norm": 0.06078682094812393,
      "learning_rate": 6.47e-06,
      "loss": 0.0018,
      "step": 130590
    },
    {
      "epoch": 6.965333333333334,
      "grad_norm": 0.14934013783931732,
      "learning_rate": 6.466666666666667e-06,
      "loss": 0.002,
      "step": 130600
    },
    {
      "epoch": 6.965866666666667,
      "grad_norm": 0.2287990301847458,
      "learning_rate": 6.463333333333334e-06,
      "loss": 0.0024,
      "step": 130610
    },
    {
      "epoch": 6.9664,
      "grad_norm": 0.3880300223827362,
      "learning_rate": 6.460000000000001e-06,
      "loss": 0.0023,
      "step": 130620
    },
    {
      "epoch": 6.966933333333333,
      "grad_norm": 0.12254463881254196,
      "learning_rate": 6.456666666666666e-06,
      "loss": 0.0019,
      "step": 130630
    },
    {
      "epoch": 6.967466666666667,
      "grad_norm": 0.060866184532642365,
      "learning_rate": 6.453333333333333e-06,
      "loss": 0.0012,
      "step": 130640
    },
    {
      "epoch": 6.968,
      "grad_norm": 0.04527735710144043,
      "learning_rate": 6.45e-06,
      "loss": 0.0018,
      "step": 130650
    },
    {
      "epoch": 6.968533333333333,
      "grad_norm": 0.20365005731582642,
      "learning_rate": 6.446666666666668e-06,
      "loss": 0.0023,
      "step": 130660
    },
    {
      "epoch": 6.9690666666666665,
      "grad_norm": 0.09075263887643814,
      "learning_rate": 6.443333333333333e-06,
      "loss": 0.0014,
      "step": 130670
    },
    {
      "epoch": 6.9696,
      "grad_norm": 0.06342419236898422,
      "learning_rate": 6.44e-06,
      "loss": 0.0013,
      "step": 130680
    },
    {
      "epoch": 6.970133333333333,
      "grad_norm": 0.06530051678419113,
      "learning_rate": 6.436666666666667e-06,
      "loss": 0.0015,
      "step": 130690
    },
    {
      "epoch": 6.970666666666666,
      "grad_norm": 0.09958769381046295,
      "learning_rate": 6.433333333333334e-06,
      "loss": 0.0018,
      "step": 130700
    },
    {
      "epoch": 6.9712,
      "grad_norm": 0.2580774128437042,
      "learning_rate": 6.43e-06,
      "loss": 0.0021,
      "step": 130710
    },
    {
      "epoch": 6.971733333333333,
      "grad_norm": 0.4270816743373871,
      "learning_rate": 6.426666666666667e-06,
      "loss": 0.0013,
      "step": 130720
    },
    {
      "epoch": 6.972266666666666,
      "grad_norm": 0.11559578776359558,
      "learning_rate": 6.423333333333334e-06,
      "loss": 0.001,
      "step": 130730
    },
    {
      "epoch": 6.9728,
      "grad_norm": 0.08961658924818039,
      "learning_rate": 6.4199999999999995e-06,
      "loss": 0.0018,
      "step": 130740
    },
    {
      "epoch": 6.973333333333334,
      "grad_norm": 0.06482125073671341,
      "learning_rate": 6.4166666666666665e-06,
      "loss": 0.002,
      "step": 130750
    },
    {
      "epoch": 6.973866666666667,
      "grad_norm": 0.3459733724594116,
      "learning_rate": 6.4133333333333335e-06,
      "loss": 0.0018,
      "step": 130760
    },
    {
      "epoch": 6.9744,
      "grad_norm": 0.03666675090789795,
      "learning_rate": 6.4100000000000005e-06,
      "loss": 0.0013,
      "step": 130770
    },
    {
      "epoch": 6.974933333333333,
      "grad_norm": 0.1211756095290184,
      "learning_rate": 6.406666666666667e-06,
      "loss": 0.0015,
      "step": 130780
    },
    {
      "epoch": 6.975466666666667,
      "grad_norm": 0.2125256359577179,
      "learning_rate": 6.403333333333334e-06,
      "loss": 0.0018,
      "step": 130790
    },
    {
      "epoch": 6.976,
      "grad_norm": 0.0611223429441452,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 0.0017,
      "step": 130800
    },
    {
      "epoch": 6.976533333333333,
      "grad_norm": 0.17431128025054932,
      "learning_rate": 6.3966666666666675e-06,
      "loss": 0.0024,
      "step": 130810
    },
    {
      "epoch": 6.9770666666666665,
      "grad_norm": 0.17514222860336304,
      "learning_rate": 6.393333333333333e-06,
      "loss": 0.0018,
      "step": 130820
    },
    {
      "epoch": 6.9776,
      "grad_norm": 0.12192127108573914,
      "learning_rate": 6.39e-06,
      "loss": 0.0017,
      "step": 130830
    },
    {
      "epoch": 6.978133333333333,
      "grad_norm": 0.1985570192337036,
      "learning_rate": 6.386666666666667e-06,
      "loss": 0.0017,
      "step": 130840
    },
    {
      "epoch": 6.978666666666666,
      "grad_norm": 0.369296133518219,
      "learning_rate": 6.383333333333335e-06,
      "loss": 0.0022,
      "step": 130850
    },
    {
      "epoch": 6.9792,
      "grad_norm": 0.07527865469455719,
      "learning_rate": 6.38e-06,
      "loss": 0.0014,
      "step": 130860
    },
    {
      "epoch": 6.979733333333334,
      "grad_norm": 0.1975541114807129,
      "learning_rate": 6.376666666666667e-06,
      "loss": 0.0014,
      "step": 130870
    },
    {
      "epoch": 6.980266666666667,
      "grad_norm": 0.21149826049804688,
      "learning_rate": 6.373333333333334e-06,
      "loss": 0.0014,
      "step": 130880
    },
    {
      "epoch": 6.9808,
      "grad_norm": 0.19668439030647278,
      "learning_rate": 6.370000000000001e-06,
      "loss": 0.0012,
      "step": 130890
    },
    {
      "epoch": 6.981333333333334,
      "grad_norm": 0.170010045170784,
      "learning_rate": 6.366666666666667e-06,
      "loss": 0.0018,
      "step": 130900
    },
    {
      "epoch": 6.981866666666667,
      "grad_norm": 0.24867205321788788,
      "learning_rate": 6.363333333333334e-06,
      "loss": 0.0016,
      "step": 130910
    },
    {
      "epoch": 6.9824,
      "grad_norm": 0.19904381036758423,
      "learning_rate": 6.360000000000001e-06,
      "loss": 0.0016,
      "step": 130920
    },
    {
      "epoch": 6.982933333333333,
      "grad_norm": 0.060991350561380386,
      "learning_rate": 6.356666666666666e-06,
      "loss": 0.0013,
      "step": 130930
    },
    {
      "epoch": 6.983466666666667,
      "grad_norm": 0.24044997990131378,
      "learning_rate": 6.353333333333333e-06,
      "loss": 0.0022,
      "step": 130940
    },
    {
      "epoch": 6.984,
      "grad_norm": 0.34068921208381653,
      "learning_rate": 6.35e-06,
      "loss": 0.0014,
      "step": 130950
    },
    {
      "epoch": 6.984533333333333,
      "grad_norm": 0.28875982761383057,
      "learning_rate": 6.346666666666667e-06,
      "loss": 0.0012,
      "step": 130960
    },
    {
      "epoch": 6.9850666666666665,
      "grad_norm": 0.1515178382396698,
      "learning_rate": 6.343333333333333e-06,
      "loss": 0.0027,
      "step": 130970
    },
    {
      "epoch": 6.9856,
      "grad_norm": 0.17011018097400665,
      "learning_rate": 6.34e-06,
      "loss": 0.0021,
      "step": 130980
    },
    {
      "epoch": 6.986133333333333,
      "grad_norm": 0.14905212819576263,
      "learning_rate": 6.336666666666667e-06,
      "loss": 0.0015,
      "step": 130990
    },
    {
      "epoch": 6.986666666666666,
      "grad_norm": 0.04678990691900253,
      "learning_rate": 6.333333333333334e-06,
      "loss": 0.0019,
      "step": 131000
    },
    {
      "epoch": 6.9872,
      "grad_norm": 0.3099385201931,
      "learning_rate": 6.3299999999999995e-06,
      "loss": 0.0019,
      "step": 131010
    },
    {
      "epoch": 6.987733333333333,
      "grad_norm": 0.3945735692977905,
      "learning_rate": 6.3266666666666665e-06,
      "loss": 0.002,
      "step": 131020
    },
    {
      "epoch": 6.988266666666666,
      "grad_norm": 0.1722429096698761,
      "learning_rate": 6.3233333333333335e-06,
      "loss": 0.0016,
      "step": 131030
    },
    {
      "epoch": 6.9888,
      "grad_norm": 0.3977147042751312,
      "learning_rate": 6.320000000000001e-06,
      "loss": 0.0019,
      "step": 131040
    },
    {
      "epoch": 6.989333333333334,
      "grad_norm": 0.5466371774673462,
      "learning_rate": 6.316666666666667e-06,
      "loss": 0.0023,
      "step": 131050
    },
    {
      "epoch": 6.989866666666667,
      "grad_norm": 0.4094564914703369,
      "learning_rate": 6.313333333333334e-06,
      "loss": 0.0016,
      "step": 131060
    },
    {
      "epoch": 6.9904,
      "grad_norm": 0.4365309178829193,
      "learning_rate": 6.3100000000000006e-06,
      "loss": 0.0016,
      "step": 131070
    },
    {
      "epoch": 6.990933333333333,
      "grad_norm": 0.1419849395751953,
      "learning_rate": 6.306666666666666e-06,
      "loss": 0.0021,
      "step": 131080
    },
    {
      "epoch": 6.991466666666667,
      "grad_norm": 0.04221505671739578,
      "learning_rate": 6.303333333333334e-06,
      "loss": 0.0013,
      "step": 131090
    },
    {
      "epoch": 6.992,
      "grad_norm": 0.11697831749916077,
      "learning_rate": 6.300000000000001e-06,
      "loss": 0.0026,
      "step": 131100
    },
    {
      "epoch": 6.992533333333333,
      "grad_norm": 0.08334282785654068,
      "learning_rate": 6.296666666666668e-06,
      "loss": 0.0019,
      "step": 131110
    },
    {
      "epoch": 6.9930666666666665,
      "grad_norm": 0.05205991491675377,
      "learning_rate": 6.293333333333333e-06,
      "loss": 0.0013,
      "step": 131120
    },
    {
      "epoch": 6.9936,
      "grad_norm": 0.06542777270078659,
      "learning_rate": 6.29e-06,
      "loss": 0.0014,
      "step": 131130
    },
    {
      "epoch": 6.994133333333333,
      "grad_norm": 0.09479083865880966,
      "learning_rate": 6.286666666666667e-06,
      "loss": 0.0012,
      "step": 131140
    },
    {
      "epoch": 6.994666666666666,
      "grad_norm": 0.12046602368354797,
      "learning_rate": 6.283333333333334e-06,
      "loss": 0.0016,
      "step": 131150
    },
    {
      "epoch": 6.9952,
      "grad_norm": 0.32004213333129883,
      "learning_rate": 6.28e-06,
      "loss": 0.0013,
      "step": 131160
    },
    {
      "epoch": 6.995733333333334,
      "grad_norm": 0.15910565853118896,
      "learning_rate": 6.276666666666667e-06,
      "loss": 0.0026,
      "step": 131170
    },
    {
      "epoch": 6.996266666666667,
      "grad_norm": 0.5719983577728271,
      "learning_rate": 6.273333333333334e-06,
      "loss": 0.0025,
      "step": 131180
    },
    {
      "epoch": 6.9968,
      "grad_norm": 0.1826508492231369,
      "learning_rate": 6.270000000000001e-06,
      "loss": 0.0024,
      "step": 131190
    },
    {
      "epoch": 6.997333333333334,
      "grad_norm": 0.06566668301820755,
      "learning_rate": 6.266666666666666e-06,
      "loss": 0.0013,
      "step": 131200
    },
    {
      "epoch": 6.997866666666667,
      "grad_norm": 0.18960271775722504,
      "learning_rate": 6.263333333333333e-06,
      "loss": 0.0019,
      "step": 131210
    },
    {
      "epoch": 6.9984,
      "grad_norm": 0.41642406582832336,
      "learning_rate": 6.26e-06,
      "loss": 0.0014,
      "step": 131220
    },
    {
      "epoch": 6.9989333333333335,
      "grad_norm": 0.2384047508239746,
      "learning_rate": 6.256666666666668e-06,
      "loss": 0.0028,
      "step": 131230
    },
    {
      "epoch": 6.999466666666667,
      "grad_norm": 0.5528540015220642,
      "learning_rate": 6.253333333333333e-06,
      "loss": 0.0022,
      "step": 131240
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.11677194386720657,
      "learning_rate": 6.25e-06,
      "loss": 0.0025,
      "step": 131250
    },
    {
      "epoch": 7.0,
      "eval_loss": 0.001825681421905756,
      "eval_runtime": 176.2106,
      "eval_samples_per_second": 1418.757,
      "eval_steps_per_second": 35.469,
      "step": 131250
    },
    {
      "epoch": 7.000533333333333,
      "grad_norm": 0.03253093734383583,
      "learning_rate": 6.2466666666666664e-06,
      "loss": 0.0014,
      "step": 131260
    },
    {
      "epoch": 7.0010666666666665,
      "grad_norm": 0.2373724728822708,
      "learning_rate": 6.243333333333333e-06,
      "loss": 0.0031,
      "step": 131270
    },
    {
      "epoch": 7.0016,
      "grad_norm": 0.48140525817871094,
      "learning_rate": 6.24e-06,
      "loss": 0.0015,
      "step": 131280
    },
    {
      "epoch": 7.002133333333333,
      "grad_norm": 0.1128675565123558,
      "learning_rate": 6.236666666666667e-06,
      "loss": 0.0018,
      "step": 131290
    },
    {
      "epoch": 7.002666666666666,
      "grad_norm": 0.13398928940296173,
      "learning_rate": 6.2333333333333335e-06,
      "loss": 0.0016,
      "step": 131300
    },
    {
      "epoch": 7.0032,
      "grad_norm": 0.2772919237613678,
      "learning_rate": 6.2300000000000005e-06,
      "loss": 0.0013,
      "step": 131310
    },
    {
      "epoch": 7.003733333333333,
      "grad_norm": 0.08761367201805115,
      "learning_rate": 6.226666666666667e-06,
      "loss": 0.0013,
      "step": 131320
    },
    {
      "epoch": 7.004266666666667,
      "grad_norm": 0.1212082952260971,
      "learning_rate": 6.223333333333334e-06,
      "loss": 0.0019,
      "step": 131330
    },
    {
      "epoch": 7.0048,
      "grad_norm": 0.18807968497276306,
      "learning_rate": 6.22e-06,
      "loss": 0.0014,
      "step": 131340
    },
    {
      "epoch": 7.005333333333334,
      "grad_norm": 0.25671255588531494,
      "learning_rate": 6.2166666666666676e-06,
      "loss": 0.0023,
      "step": 131350
    },
    {
      "epoch": 7.005866666666667,
      "grad_norm": 0.3977322280406952,
      "learning_rate": 6.213333333333334e-06,
      "loss": 0.0017,
      "step": 131360
    },
    {
      "epoch": 7.0064,
      "grad_norm": 0.14452217519283295,
      "learning_rate": 6.210000000000001e-06,
      "loss": 0.003,
      "step": 131370
    },
    {
      "epoch": 7.0069333333333335,
      "grad_norm": 0.3274724781513214,
      "learning_rate": 6.206666666666667e-06,
      "loss": 0.0022,
      "step": 131380
    },
    {
      "epoch": 7.007466666666667,
      "grad_norm": 0.39720168709754944,
      "learning_rate": 6.203333333333334e-06,
      "loss": 0.002,
      "step": 131390
    },
    {
      "epoch": 7.008,
      "grad_norm": 0.2815243601799011,
      "learning_rate": 6.2e-06,
      "loss": 0.0017,
      "step": 131400
    },
    {
      "epoch": 7.008533333333333,
      "grad_norm": 0.22678405046463013,
      "learning_rate": 6.196666666666667e-06,
      "loss": 0.0014,
      "step": 131410
    },
    {
      "epoch": 7.009066666666667,
      "grad_norm": 0.07043879479169846,
      "learning_rate": 6.193333333333334e-06,
      "loss": 0.0015,
      "step": 131420
    },
    {
      "epoch": 7.0096,
      "grad_norm": 0.08942513912916183,
      "learning_rate": 6.19e-06,
      "loss": 0.0015,
      "step": 131430
    },
    {
      "epoch": 7.010133333333333,
      "grad_norm": 0.2873721122741699,
      "learning_rate": 6.186666666666667e-06,
      "loss": 0.0013,
      "step": 131440
    },
    {
      "epoch": 7.010666666666666,
      "grad_norm": 0.07024269551038742,
      "learning_rate": 6.183333333333333e-06,
      "loss": 0.0017,
      "step": 131450
    },
    {
      "epoch": 7.0112,
      "grad_norm": 0.18087053298950195,
      "learning_rate": 6.18e-06,
      "loss": 0.0029,
      "step": 131460
    },
    {
      "epoch": 7.011733333333333,
      "grad_norm": 0.06920260190963745,
      "learning_rate": 6.176666666666667e-06,
      "loss": 0.0021,
      "step": 131470
    },
    {
      "epoch": 7.012266666666667,
      "grad_norm": 0.34201866388320923,
      "learning_rate": 6.173333333333334e-06,
      "loss": 0.0018,
      "step": 131480
    },
    {
      "epoch": 7.0128,
      "grad_norm": 0.33853524923324585,
      "learning_rate": 6.17e-06,
      "loss": 0.0017,
      "step": 131490
    },
    {
      "epoch": 7.013333333333334,
      "grad_norm": 0.11527851223945618,
      "learning_rate": 6.166666666666667e-06,
      "loss": 0.0018,
      "step": 131500
    },
    {
      "epoch": 7.013866666666667,
      "grad_norm": 0.02884656935930252,
      "learning_rate": 6.163333333333333e-06,
      "loss": 0.0018,
      "step": 131510
    },
    {
      "epoch": 7.0144,
      "grad_norm": 0.027349496260285378,
      "learning_rate": 6.16e-06,
      "loss": 0.0016,
      "step": 131520
    },
    {
      "epoch": 7.0149333333333335,
      "grad_norm": 0.06559538841247559,
      "learning_rate": 6.1566666666666664e-06,
      "loss": 0.0019,
      "step": 131530
    },
    {
      "epoch": 7.015466666666667,
      "grad_norm": 0.04520183056592941,
      "learning_rate": 6.153333333333334e-06,
      "loss": 0.0014,
      "step": 131540
    },
    {
      "epoch": 7.016,
      "grad_norm": 0.15262368321418762,
      "learning_rate": 6.15e-06,
      "loss": 0.002,
      "step": 131550
    },
    {
      "epoch": 7.016533333333333,
      "grad_norm": 0.11761845648288727,
      "learning_rate": 6.146666666666667e-06,
      "loss": 0.0019,
      "step": 131560
    },
    {
      "epoch": 7.017066666666667,
      "grad_norm": 0.1156216710805893,
      "learning_rate": 6.1433333333333335e-06,
      "loss": 0.0044,
      "step": 131570
    },
    {
      "epoch": 7.0176,
      "grad_norm": 0.03930230438709259,
      "learning_rate": 6.1400000000000005e-06,
      "loss": 0.0015,
      "step": 131580
    },
    {
      "epoch": 7.018133333333333,
      "grad_norm": 0.38201504945755005,
      "learning_rate": 6.136666666666667e-06,
      "loss": 0.0023,
      "step": 131590
    },
    {
      "epoch": 7.018666666666666,
      "grad_norm": 0.08028244972229004,
      "learning_rate": 6.133333333333334e-06,
      "loss": 0.0023,
      "step": 131600
    },
    {
      "epoch": 7.0192,
      "grad_norm": 0.34804651141166687,
      "learning_rate": 6.130000000000001e-06,
      "loss": 0.0013,
      "step": 131610
    },
    {
      "epoch": 7.019733333333333,
      "grad_norm": 0.17837625741958618,
      "learning_rate": 6.126666666666667e-06,
      "loss": 0.0026,
      "step": 131620
    },
    {
      "epoch": 7.020266666666667,
      "grad_norm": 0.09267439693212509,
      "learning_rate": 6.123333333333334e-06,
      "loss": 0.0021,
      "step": 131630
    },
    {
      "epoch": 7.0208,
      "grad_norm": 0.09511449187994003,
      "learning_rate": 6.12e-06,
      "loss": 0.0021,
      "step": 131640
    },
    {
      "epoch": 7.021333333333334,
      "grad_norm": 0.22413703799247742,
      "learning_rate": 6.116666666666667e-06,
      "loss": 0.0027,
      "step": 131650
    },
    {
      "epoch": 7.021866666666667,
      "grad_norm": 0.17504319548606873,
      "learning_rate": 6.113333333333334e-06,
      "loss": 0.0022,
      "step": 131660
    },
    {
      "epoch": 7.0224,
      "grad_norm": 0.0462287999689579,
      "learning_rate": 6.110000000000001e-06,
      "loss": 0.0021,
      "step": 131670
    },
    {
      "epoch": 7.0229333333333335,
      "grad_norm": 0.23355208337306976,
      "learning_rate": 6.106666666666667e-06,
      "loss": 0.0022,
      "step": 131680
    },
    {
      "epoch": 7.023466666666667,
      "grad_norm": 0.2853638529777527,
      "learning_rate": 6.103333333333334e-06,
      "loss": 0.0032,
      "step": 131690
    },
    {
      "epoch": 7.024,
      "grad_norm": 0.08824721723794937,
      "learning_rate": 6.1e-06,
      "loss": 0.0023,
      "step": 131700
    },
    {
      "epoch": 7.024533333333333,
      "grad_norm": 0.07054276764392853,
      "learning_rate": 6.096666666666667e-06,
      "loss": 0.0015,
      "step": 131710
    },
    {
      "epoch": 7.025066666666667,
      "grad_norm": 0.17018070816993713,
      "learning_rate": 6.093333333333333e-06,
      "loss": 0.0015,
      "step": 131720
    },
    {
      "epoch": 7.0256,
      "grad_norm": 0.08181551098823547,
      "learning_rate": 6.090000000000001e-06,
      "loss": 0.0024,
      "step": 131730
    },
    {
      "epoch": 7.026133333333333,
      "grad_norm": 0.23348253965377808,
      "learning_rate": 6.086666666666667e-06,
      "loss": 0.0016,
      "step": 131740
    },
    {
      "epoch": 7.026666666666666,
      "grad_norm": 0.11803094297647476,
      "learning_rate": 6.083333333333334e-06,
      "loss": 0.0013,
      "step": 131750
    },
    {
      "epoch": 7.0272,
      "grad_norm": 0.07678306102752686,
      "learning_rate": 6.08e-06,
      "loss": 0.002,
      "step": 131760
    },
    {
      "epoch": 7.027733333333333,
      "grad_norm": 0.09248114377260208,
      "learning_rate": 6.076666666666666e-06,
      "loss": 0.0019,
      "step": 131770
    },
    {
      "epoch": 7.028266666666667,
      "grad_norm": 0.4509437680244446,
      "learning_rate": 6.073333333333333e-06,
      "loss": 0.0012,
      "step": 131780
    },
    {
      "epoch": 7.0288,
      "grad_norm": 0.11494454741477966,
      "learning_rate": 6.07e-06,
      "loss": 0.0016,
      "step": 131790
    },
    {
      "epoch": 7.029333333333334,
      "grad_norm": 0.11845498532056808,
      "learning_rate": 6.066666666666667e-06,
      "loss": 0.0017,
      "step": 131800
    },
    {
      "epoch": 7.029866666666667,
      "grad_norm": 0.2054315209388733,
      "learning_rate": 6.0633333333333334e-06,
      "loss": 0.0013,
      "step": 131810
    },
    {
      "epoch": 7.0304,
      "grad_norm": 0.2050514966249466,
      "learning_rate": 6.0600000000000004e-06,
      "loss": 0.0017,
      "step": 131820
    },
    {
      "epoch": 7.0309333333333335,
      "grad_norm": 0.22824236750602722,
      "learning_rate": 6.0566666666666666e-06,
      "loss": 0.0016,
      "step": 131830
    },
    {
      "epoch": 7.031466666666667,
      "grad_norm": 0.798828661441803,
      "learning_rate": 6.0533333333333335e-06,
      "loss": 0.0017,
      "step": 131840
    },
    {
      "epoch": 7.032,
      "grad_norm": 0.19788259267807007,
      "learning_rate": 6.0500000000000005e-06,
      "loss": 0.0018,
      "step": 131850
    },
    {
      "epoch": 7.032533333333333,
      "grad_norm": 0.19608482718467712,
      "learning_rate": 6.0466666666666675e-06,
      "loss": 0.0013,
      "step": 131860
    },
    {
      "epoch": 7.033066666666667,
      "grad_norm": 0.34615471959114075,
      "learning_rate": 6.043333333333334e-06,
      "loss": 0.0021,
      "step": 131870
    },
    {
      "epoch": 7.0336,
      "grad_norm": 0.09844823181629181,
      "learning_rate": 6.040000000000001e-06,
      "loss": 0.0018,
      "step": 131880
    },
    {
      "epoch": 7.034133333333333,
      "grad_norm": 0.15980181097984314,
      "learning_rate": 6.036666666666667e-06,
      "loss": 0.0017,
      "step": 131890
    },
    {
      "epoch": 7.034666666666666,
      "grad_norm": 0.057875100523233414,
      "learning_rate": 6.033333333333334e-06,
      "loss": 0.002,
      "step": 131900
    },
    {
      "epoch": 7.0352,
      "grad_norm": 0.20094017684459686,
      "learning_rate": 6.03e-06,
      "loss": 0.0013,
      "step": 131910
    },
    {
      "epoch": 7.035733333333333,
      "grad_norm": 0.026961201801896095,
      "learning_rate": 6.026666666666667e-06,
      "loss": 0.0021,
      "step": 131920
    },
    {
      "epoch": 7.036266666666666,
      "grad_norm": 0.14703097939491272,
      "learning_rate": 6.023333333333334e-06,
      "loss": 0.003,
      "step": 131930
    },
    {
      "epoch": 7.0368,
      "grad_norm": 0.2347719669342041,
      "learning_rate": 6.02e-06,
      "loss": 0.0021,
      "step": 131940
    },
    {
      "epoch": 7.037333333333334,
      "grad_norm": 0.08812417089939117,
      "learning_rate": 6.016666666666667e-06,
      "loss": 0.0018,
      "step": 131950
    },
    {
      "epoch": 7.037866666666667,
      "grad_norm": 0.06521376222372055,
      "learning_rate": 6.013333333333333e-06,
      "loss": 0.0015,
      "step": 131960
    },
    {
      "epoch": 7.0384,
      "grad_norm": 0.08861501514911652,
      "learning_rate": 6.01e-06,
      "loss": 0.0021,
      "step": 131970
    },
    {
      "epoch": 7.0389333333333335,
      "grad_norm": 0.12999609112739563,
      "learning_rate": 6.006666666666667e-06,
      "loss": 0.0018,
      "step": 131980
    },
    {
      "epoch": 7.039466666666667,
      "grad_norm": 0.4004041850566864,
      "learning_rate": 6.003333333333334e-06,
      "loss": 0.0014,
      "step": 131990
    },
    {
      "epoch": 7.04,
      "grad_norm": 0.08692260086536407,
      "learning_rate": 6e-06,
      "loss": 0.0016,
      "step": 132000
    },
    {
      "epoch": 7.040533333333333,
      "grad_norm": 0.3123622238636017,
      "learning_rate": 5.996666666666667e-06,
      "loss": 0.0019,
      "step": 132010
    },
    {
      "epoch": 7.041066666666667,
      "grad_norm": 0.17448481917381287,
      "learning_rate": 5.993333333333333e-06,
      "loss": 0.0015,
      "step": 132020
    },
    {
      "epoch": 7.0416,
      "grad_norm": 0.15867699682712555,
      "learning_rate": 5.99e-06,
      "loss": 0.0017,
      "step": 132030
    },
    {
      "epoch": 7.042133333333333,
      "grad_norm": 0.07629269361495972,
      "learning_rate": 5.986666666666667e-06,
      "loss": 0.0015,
      "step": 132040
    },
    {
      "epoch": 7.042666666666666,
      "grad_norm": 0.17155903577804565,
      "learning_rate": 5.983333333333334e-06,
      "loss": 0.0021,
      "step": 132050
    },
    {
      "epoch": 7.0432,
      "grad_norm": 0.12068004906177521,
      "learning_rate": 5.98e-06,
      "loss": 0.0019,
      "step": 132060
    },
    {
      "epoch": 7.043733333333333,
      "grad_norm": 0.17771568894386292,
      "learning_rate": 5.976666666666667e-06,
      "loss": 0.0021,
      "step": 132070
    },
    {
      "epoch": 7.044266666666666,
      "grad_norm": 0.456460565328598,
      "learning_rate": 5.9733333333333335e-06,
      "loss": 0.0014,
      "step": 132080
    },
    {
      "epoch": 7.0448,
      "grad_norm": 0.4037739634513855,
      "learning_rate": 5.9700000000000004e-06,
      "loss": 0.0021,
      "step": 132090
    },
    {
      "epoch": 7.045333333333334,
      "grad_norm": 0.22511468827724457,
      "learning_rate": 5.9666666666666666e-06,
      "loss": 0.0019,
      "step": 132100
    },
    {
      "epoch": 7.045866666666667,
      "grad_norm": 0.25776809453964233,
      "learning_rate": 5.9633333333333336e-06,
      "loss": 0.0017,
      "step": 132110
    },
    {
      "epoch": 7.0464,
      "grad_norm": 0.2043004035949707,
      "learning_rate": 5.9600000000000005e-06,
      "loss": 0.0015,
      "step": 132120
    },
    {
      "epoch": 7.0469333333333335,
      "grad_norm": 0.0897609069943428,
      "learning_rate": 5.956666666666667e-06,
      "loss": 0.0012,
      "step": 132130
    },
    {
      "epoch": 7.047466666666667,
      "grad_norm": 0.054598625749349594,
      "learning_rate": 5.953333333333334e-06,
      "loss": 0.0015,
      "step": 132140
    },
    {
      "epoch": 7.048,
      "grad_norm": 0.29162347316741943,
      "learning_rate": 5.95e-06,
      "loss": 0.0028,
      "step": 132150
    },
    {
      "epoch": 7.048533333333333,
      "grad_norm": 0.2425801008939743,
      "learning_rate": 5.946666666666667e-06,
      "loss": 0.0017,
      "step": 132160
    },
    {
      "epoch": 7.049066666666667,
      "grad_norm": 0.17508620023727417,
      "learning_rate": 5.943333333333334e-06,
      "loss": 0.0014,
      "step": 132170
    },
    {
      "epoch": 7.0496,
      "grad_norm": 0.049059730023145676,
      "learning_rate": 5.940000000000001e-06,
      "loss": 0.0017,
      "step": 132180
    },
    {
      "epoch": 7.050133333333333,
      "grad_norm": 0.11765418946743011,
      "learning_rate": 5.936666666666667e-06,
      "loss": 0.0021,
      "step": 132190
    },
    {
      "epoch": 7.050666666666666,
      "grad_norm": 0.08676805347204208,
      "learning_rate": 5.933333333333334e-06,
      "loss": 0.0022,
      "step": 132200
    },
    {
      "epoch": 7.0512,
      "grad_norm": 0.3826068937778473,
      "learning_rate": 5.93e-06,
      "loss": 0.0014,
      "step": 132210
    },
    {
      "epoch": 7.051733333333333,
      "grad_norm": 0.4001372456550598,
      "learning_rate": 5.926666666666667e-06,
      "loss": 0.002,
      "step": 132220
    },
    {
      "epoch": 7.052266666666666,
      "grad_norm": 0.11774255335330963,
      "learning_rate": 5.923333333333333e-06,
      "loss": 0.0015,
      "step": 132230
    },
    {
      "epoch": 7.0528,
      "grad_norm": 0.06227082386612892,
      "learning_rate": 5.920000000000001e-06,
      "loss": 0.0015,
      "step": 132240
    },
    {
      "epoch": 7.053333333333334,
      "grad_norm": 0.2753187417984009,
      "learning_rate": 5.916666666666667e-06,
      "loss": 0.0026,
      "step": 132250
    },
    {
      "epoch": 7.053866666666667,
      "grad_norm": 0.12150470912456512,
      "learning_rate": 5.913333333333334e-06,
      "loss": 0.0023,
      "step": 132260
    },
    {
      "epoch": 7.0544,
      "grad_norm": 0.31061309576034546,
      "learning_rate": 5.91e-06,
      "loss": 0.0017,
      "step": 132270
    },
    {
      "epoch": 7.0549333333333335,
      "grad_norm": 0.37397536635398865,
      "learning_rate": 5.906666666666667e-06,
      "loss": 0.0014,
      "step": 132280
    },
    {
      "epoch": 7.055466666666667,
      "grad_norm": 0.11885897815227509,
      "learning_rate": 5.903333333333333e-06,
      "loss": 0.0021,
      "step": 132290
    },
    {
      "epoch": 7.056,
      "grad_norm": 0.07565809786319733,
      "learning_rate": 5.9e-06,
      "loss": 0.0019,
      "step": 132300
    },
    {
      "epoch": 7.056533333333333,
      "grad_norm": 0.17373816668987274,
      "learning_rate": 5.896666666666667e-06,
      "loss": 0.0013,
      "step": 132310
    },
    {
      "epoch": 7.057066666666667,
      "grad_norm": 0.041656892746686935,
      "learning_rate": 5.893333333333333e-06,
      "loss": 0.0014,
      "step": 132320
    },
    {
      "epoch": 7.0576,
      "grad_norm": 0.200810045003891,
      "learning_rate": 5.89e-06,
      "loss": 0.0024,
      "step": 132330
    },
    {
      "epoch": 7.058133333333333,
      "grad_norm": 0.1698712408542633,
      "learning_rate": 5.8866666666666665e-06,
      "loss": 0.0022,
      "step": 132340
    },
    {
      "epoch": 7.058666666666666,
      "grad_norm": 0.04126063361763954,
      "learning_rate": 5.8833333333333335e-06,
      "loss": 0.0019,
      "step": 132350
    },
    {
      "epoch": 7.0592,
      "grad_norm": 0.20201556384563446,
      "learning_rate": 5.8800000000000005e-06,
      "loss": 0.0016,
      "step": 132360
    },
    {
      "epoch": 7.059733333333333,
      "grad_norm": 0.21301330626010895,
      "learning_rate": 5.8766666666666674e-06,
      "loss": 0.0014,
      "step": 132370
    },
    {
      "epoch": 7.060266666666666,
      "grad_norm": 0.10755577683448792,
      "learning_rate": 5.8733333333333336e-06,
      "loss": 0.002,
      "step": 132380
    },
    {
      "epoch": 7.0608,
      "grad_norm": 0.23766404390335083,
      "learning_rate": 5.8700000000000005e-06,
      "loss": 0.0029,
      "step": 132390
    },
    {
      "epoch": 7.061333333333334,
      "grad_norm": 0.1340605914592743,
      "learning_rate": 5.866666666666667e-06,
      "loss": 0.002,
      "step": 132400
    },
    {
      "epoch": 7.061866666666667,
      "grad_norm": 0.11454091221094131,
      "learning_rate": 5.863333333333334e-06,
      "loss": 0.0016,
      "step": 132410
    },
    {
      "epoch": 7.0624,
      "grad_norm": 0.1988711804151535,
      "learning_rate": 5.86e-06,
      "loss": 0.0019,
      "step": 132420
    },
    {
      "epoch": 7.0629333333333335,
      "grad_norm": 0.20145085453987122,
      "learning_rate": 5.856666666666668e-06,
      "loss": 0.0011,
      "step": 132430
    },
    {
      "epoch": 7.063466666666667,
      "grad_norm": 0.03677446022629738,
      "learning_rate": 5.853333333333334e-06,
      "loss": 0.0015,
      "step": 132440
    },
    {
      "epoch": 7.064,
      "grad_norm": 0.08940966427326202,
      "learning_rate": 5.850000000000001e-06,
      "loss": 0.0014,
      "step": 132450
    },
    {
      "epoch": 7.064533333333333,
      "grad_norm": 0.09698992222547531,
      "learning_rate": 5.846666666666667e-06,
      "loss": 0.0019,
      "step": 132460
    },
    {
      "epoch": 7.065066666666667,
      "grad_norm": 0.09618092328310013,
      "learning_rate": 5.843333333333333e-06,
      "loss": 0.0018,
      "step": 132470
    },
    {
      "epoch": 7.0656,
      "grad_norm": 0.4358260929584503,
      "learning_rate": 5.84e-06,
      "loss": 0.0024,
      "step": 132480
    },
    {
      "epoch": 7.066133333333333,
      "grad_norm": 0.17955656349658966,
      "learning_rate": 5.836666666666667e-06,
      "loss": 0.0017,
      "step": 132490
    },
    {
      "epoch": 7.066666666666666,
      "grad_norm": 0.14483307301998138,
      "learning_rate": 5.833333333333334e-06,
      "loss": 0.0011,
      "step": 132500
    },
    {
      "epoch": 7.0672,
      "grad_norm": 0.19733218848705292,
      "learning_rate": 5.83e-06,
      "loss": 0.0022,
      "step": 132510
    },
    {
      "epoch": 7.067733333333333,
      "grad_norm": 0.3676305115222931,
      "learning_rate": 5.826666666666667e-06,
      "loss": 0.0014,
      "step": 132520
    },
    {
      "epoch": 7.068266666666666,
      "grad_norm": 0.11857719719409943,
      "learning_rate": 5.823333333333333e-06,
      "loss": 0.0021,
      "step": 132530
    },
    {
      "epoch": 7.0688,
      "grad_norm": 0.10622957348823547,
      "learning_rate": 5.82e-06,
      "loss": 0.0023,
      "step": 132540
    },
    {
      "epoch": 7.069333333333334,
      "grad_norm": 0.19742071628570557,
      "learning_rate": 5.816666666666667e-06,
      "loss": 0.0035,
      "step": 132550
    },
    {
      "epoch": 7.069866666666667,
      "grad_norm": 0.1458713859319687,
      "learning_rate": 5.813333333333334e-06,
      "loss": 0.0016,
      "step": 132560
    },
    {
      "epoch": 7.0704,
      "grad_norm": 0.20335142314434052,
      "learning_rate": 5.81e-06,
      "loss": 0.0016,
      "step": 132570
    },
    {
      "epoch": 7.0709333333333335,
      "grad_norm": 0.22421397268772125,
      "learning_rate": 5.806666666666667e-06,
      "loss": 0.002,
      "step": 132580
    },
    {
      "epoch": 7.071466666666667,
      "grad_norm": 0.15221542119979858,
      "learning_rate": 5.803333333333333e-06,
      "loss": 0.0014,
      "step": 132590
    },
    {
      "epoch": 7.072,
      "grad_norm": 0.12022027373313904,
      "learning_rate": 5.8e-06,
      "loss": 0.0019,
      "step": 132600
    },
    {
      "epoch": 7.072533333333333,
      "grad_norm": 0.08657271414995193,
      "learning_rate": 5.7966666666666665e-06,
      "loss": 0.0016,
      "step": 132610
    },
    {
      "epoch": 7.073066666666667,
      "grad_norm": 0.17077699303627014,
      "learning_rate": 5.793333333333334e-06,
      "loss": 0.0013,
      "step": 132620
    },
    {
      "epoch": 7.0736,
      "grad_norm": 0.3364611268043518,
      "learning_rate": 5.7900000000000005e-06,
      "loss": 0.0013,
      "step": 132630
    },
    {
      "epoch": 7.074133333333333,
      "grad_norm": 0.3768058121204376,
      "learning_rate": 5.786666666666667e-06,
      "loss": 0.0019,
      "step": 132640
    },
    {
      "epoch": 7.074666666666666,
      "grad_norm": 0.149399071931839,
      "learning_rate": 5.783333333333334e-06,
      "loss": 0.0021,
      "step": 132650
    },
    {
      "epoch": 7.0752,
      "grad_norm": 0.22742518782615662,
      "learning_rate": 5.78e-06,
      "loss": 0.0017,
      "step": 132660
    },
    {
      "epoch": 7.075733333333333,
      "grad_norm": 0.15303705632686615,
      "learning_rate": 5.776666666666667e-06,
      "loss": 0.0013,
      "step": 132670
    },
    {
      "epoch": 7.076266666666666,
      "grad_norm": 0.03396730497479439,
      "learning_rate": 5.773333333333334e-06,
      "loss": 0.0015,
      "step": 132680
    },
    {
      "epoch": 7.0768,
      "grad_norm": 0.20079000294208527,
      "learning_rate": 5.770000000000001e-06,
      "loss": 0.0013,
      "step": 132690
    },
    {
      "epoch": 7.077333333333334,
      "grad_norm": 0.05370166525244713,
      "learning_rate": 5.766666666666667e-06,
      "loss": 0.0021,
      "step": 132700
    },
    {
      "epoch": 7.077866666666667,
      "grad_norm": 0.08726388216018677,
      "learning_rate": 5.763333333333334e-06,
      "loss": 0.0031,
      "step": 132710
    },
    {
      "epoch": 7.0784,
      "grad_norm": 0.39570966362953186,
      "learning_rate": 5.76e-06,
      "loss": 0.0019,
      "step": 132720
    },
    {
      "epoch": 7.0789333333333335,
      "grad_norm": 0.06515796482563019,
      "learning_rate": 5.756666666666667e-06,
      "loss": 0.0018,
      "step": 132730
    },
    {
      "epoch": 7.079466666666667,
      "grad_norm": 0.17863570153713226,
      "learning_rate": 5.753333333333334e-06,
      "loss": 0.0019,
      "step": 132740
    },
    {
      "epoch": 7.08,
      "grad_norm": 0.14228114485740662,
      "learning_rate": 5.750000000000001e-06,
      "loss": 0.0018,
      "step": 132750
    },
    {
      "epoch": 7.080533333333333,
      "grad_norm": 0.08293835073709488,
      "learning_rate": 5.746666666666667e-06,
      "loss": 0.002,
      "step": 132760
    },
    {
      "epoch": 7.081066666666667,
      "grad_norm": 0.5409301519393921,
      "learning_rate": 5.743333333333334e-06,
      "loss": 0.0015,
      "step": 132770
    },
    {
      "epoch": 7.0816,
      "grad_norm": 0.03644753620028496,
      "learning_rate": 5.74e-06,
      "loss": 0.0015,
      "step": 132780
    },
    {
      "epoch": 7.082133333333333,
      "grad_norm": 0.254573792219162,
      "learning_rate": 5.736666666666667e-06,
      "loss": 0.0019,
      "step": 132790
    },
    {
      "epoch": 7.082666666666666,
      "grad_norm": 0.22003228962421417,
      "learning_rate": 5.733333333333333e-06,
      "loss": 0.0015,
      "step": 132800
    },
    {
      "epoch": 7.0832,
      "grad_norm": 0.11744105070829391,
      "learning_rate": 5.73e-06,
      "loss": 0.0014,
      "step": 132810
    },
    {
      "epoch": 7.083733333333333,
      "grad_norm": 0.03687553107738495,
      "learning_rate": 5.726666666666667e-06,
      "loss": 0.0019,
      "step": 132820
    },
    {
      "epoch": 7.084266666666666,
      "grad_norm": 0.2808689475059509,
      "learning_rate": 5.723333333333333e-06,
      "loss": 0.0017,
      "step": 132830
    },
    {
      "epoch": 7.0848,
      "grad_norm": 0.08951988816261292,
      "learning_rate": 5.72e-06,
      "loss": 0.0015,
      "step": 132840
    },
    {
      "epoch": 7.085333333333334,
      "grad_norm": 0.12067802250385284,
      "learning_rate": 5.7166666666666664e-06,
      "loss": 0.0018,
      "step": 132850
    },
    {
      "epoch": 7.085866666666667,
      "grad_norm": 0.42662400007247925,
      "learning_rate": 5.713333333333333e-06,
      "loss": 0.0025,
      "step": 132860
    },
    {
      "epoch": 7.0864,
      "grad_norm": 0.1701500415802002,
      "learning_rate": 5.71e-06,
      "loss": 0.0015,
      "step": 132870
    },
    {
      "epoch": 7.0869333333333335,
      "grad_norm": 0.03415212780237198,
      "learning_rate": 5.706666666666667e-06,
      "loss": 0.0015,
      "step": 132880
    },
    {
      "epoch": 7.087466666666667,
      "grad_norm": 0.062257468700408936,
      "learning_rate": 5.7033333333333335e-06,
      "loss": 0.0021,
      "step": 132890
    },
    {
      "epoch": 7.088,
      "grad_norm": 0.04515102878212929,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 0.0012,
      "step": 132900
    },
    {
      "epoch": 7.088533333333333,
      "grad_norm": 0.21913598477840424,
      "learning_rate": 5.696666666666667e-06,
      "loss": 0.0028,
      "step": 132910
    },
    {
      "epoch": 7.089066666666667,
      "grad_norm": 0.11759296804666519,
      "learning_rate": 5.693333333333334e-06,
      "loss": 0.0019,
      "step": 132920
    },
    {
      "epoch": 7.0896,
      "grad_norm": 0.3840600252151489,
      "learning_rate": 5.690000000000001e-06,
      "loss": 0.0017,
      "step": 132930
    },
    {
      "epoch": 7.090133333333333,
      "grad_norm": 0.14682802557945251,
      "learning_rate": 5.6866666666666676e-06,
      "loss": 0.002,
      "step": 132940
    },
    {
      "epoch": 7.0906666666666665,
      "grad_norm": 0.3096920847892761,
      "learning_rate": 5.683333333333334e-06,
      "loss": 0.0022,
      "step": 132950
    },
    {
      "epoch": 7.0912,
      "grad_norm": 0.3732238709926605,
      "learning_rate": 5.680000000000001e-06,
      "loss": 0.0022,
      "step": 132960
    },
    {
      "epoch": 7.091733333333333,
      "grad_norm": 0.08794059604406357,
      "learning_rate": 5.676666666666667e-06,
      "loss": 0.0016,
      "step": 132970
    },
    {
      "epoch": 7.092266666666666,
      "grad_norm": 0.039350662380456924,
      "learning_rate": 5.673333333333333e-06,
      "loss": 0.0024,
      "step": 132980
    },
    {
      "epoch": 7.0928,
      "grad_norm": 0.16998976469039917,
      "learning_rate": 5.67e-06,
      "loss": 0.0013,
      "step": 132990
    },
    {
      "epoch": 7.093333333333334,
      "grad_norm": 0.1435663253068924,
      "learning_rate": 5.666666666666667e-06,
      "loss": 0.0018,
      "step": 133000
    },
    {
      "epoch": 7.093866666666667,
      "grad_norm": 0.29643499851226807,
      "learning_rate": 5.663333333333334e-06,
      "loss": 0.0022,
      "step": 133010
    },
    {
      "epoch": 7.0944,
      "grad_norm": 0.26067495346069336,
      "learning_rate": 5.66e-06,
      "loss": 0.0017,
      "step": 133020
    },
    {
      "epoch": 7.0949333333333335,
      "grad_norm": 0.5014386773109436,
      "learning_rate": 5.656666666666667e-06,
      "loss": 0.0015,
      "step": 133030
    },
    {
      "epoch": 7.095466666666667,
      "grad_norm": 0.1756887435913086,
      "learning_rate": 5.653333333333333e-06,
      "loss": 0.002,
      "step": 133040
    },
    {
      "epoch": 7.096,
      "grad_norm": 0.147445946931839,
      "learning_rate": 5.65e-06,
      "loss": 0.0017,
      "step": 133050
    },
    {
      "epoch": 7.096533333333333,
      "grad_norm": 0.28314393758773804,
      "learning_rate": 5.646666666666667e-06,
      "loss": 0.0022,
      "step": 133060
    },
    {
      "epoch": 7.097066666666667,
      "grad_norm": 0.09812476485967636,
      "learning_rate": 5.643333333333334e-06,
      "loss": 0.002,
      "step": 133070
    },
    {
      "epoch": 7.0976,
      "grad_norm": 0.3409382402896881,
      "learning_rate": 5.64e-06,
      "loss": 0.0018,
      "step": 133080
    },
    {
      "epoch": 7.098133333333333,
      "grad_norm": 0.24138769507408142,
      "learning_rate": 5.636666666666667e-06,
      "loss": 0.0018,
      "step": 133090
    },
    {
      "epoch": 7.0986666666666665,
      "grad_norm": 0.2842479348182678,
      "learning_rate": 5.633333333333333e-06,
      "loss": 0.0011,
      "step": 133100
    },
    {
      "epoch": 7.0992,
      "grad_norm": 0.319650262594223,
      "learning_rate": 5.63e-06,
      "loss": 0.0016,
      "step": 133110
    },
    {
      "epoch": 7.099733333333333,
      "grad_norm": 0.44578817486763,
      "learning_rate": 5.626666666666667e-06,
      "loss": 0.0013,
      "step": 133120
    },
    {
      "epoch": 7.100266666666666,
      "grad_norm": 0.4279361963272095,
      "learning_rate": 5.623333333333334e-06,
      "loss": 0.0029,
      "step": 133130
    },
    {
      "epoch": 7.1008,
      "grad_norm": 0.08672678470611572,
      "learning_rate": 5.62e-06,
      "loss": 0.0013,
      "step": 133140
    },
    {
      "epoch": 7.101333333333334,
      "grad_norm": 0.0868024155497551,
      "learning_rate": 5.6166666666666665e-06,
      "loss": 0.0017,
      "step": 133150
    },
    {
      "epoch": 7.101866666666667,
      "grad_norm": 0.5743778347969055,
      "learning_rate": 5.6133333333333335e-06,
      "loss": 0.0016,
      "step": 133160
    },
    {
      "epoch": 7.1024,
      "grad_norm": 0.05496387183666229,
      "learning_rate": 5.61e-06,
      "loss": 0.0015,
      "step": 133170
    },
    {
      "epoch": 7.1029333333333335,
      "grad_norm": 0.024095647037029266,
      "learning_rate": 5.606666666666667e-06,
      "loss": 0.0014,
      "step": 133180
    },
    {
      "epoch": 7.103466666666667,
      "grad_norm": 0.42981642484664917,
      "learning_rate": 5.603333333333334e-06,
      "loss": 0.0015,
      "step": 133190
    },
    {
      "epoch": 7.104,
      "grad_norm": 0.34414002299308777,
      "learning_rate": 5.600000000000001e-06,
      "loss": 0.0017,
      "step": 133200
    },
    {
      "epoch": 7.104533333333333,
      "grad_norm": 0.20053419470787048,
      "learning_rate": 5.596666666666667e-06,
      "loss": 0.0025,
      "step": 133210
    },
    {
      "epoch": 7.105066666666667,
      "grad_norm": 0.1495521515607834,
      "learning_rate": 5.593333333333334e-06,
      "loss": 0.0021,
      "step": 133220
    },
    {
      "epoch": 7.1056,
      "grad_norm": 0.22747795283794403,
      "learning_rate": 5.59e-06,
      "loss": 0.0015,
      "step": 133230
    },
    {
      "epoch": 7.106133333333333,
      "grad_norm": 0.33969467878341675,
      "learning_rate": 5.586666666666667e-06,
      "loss": 0.0028,
      "step": 133240
    },
    {
      "epoch": 7.1066666666666665,
      "grad_norm": 0.1832173764705658,
      "learning_rate": 5.583333333333334e-06,
      "loss": 0.0014,
      "step": 133250
    },
    {
      "epoch": 7.1072,
      "grad_norm": 0.09663211554288864,
      "learning_rate": 5.580000000000001e-06,
      "loss": 0.0016,
      "step": 133260
    },
    {
      "epoch": 7.107733333333333,
      "grad_norm": 0.31277111172676086,
      "learning_rate": 5.576666666666667e-06,
      "loss": 0.0026,
      "step": 133270
    },
    {
      "epoch": 7.108266666666666,
      "grad_norm": 0.14780424535274506,
      "learning_rate": 5.573333333333334e-06,
      "loss": 0.0014,
      "step": 133280
    },
    {
      "epoch": 7.1088,
      "grad_norm": 0.24885302782058716,
      "learning_rate": 5.57e-06,
      "loss": 0.002,
      "step": 133290
    },
    {
      "epoch": 7.109333333333334,
      "grad_norm": 0.1495712697505951,
      "learning_rate": 5.566666666666667e-06,
      "loss": 0.0023,
      "step": 133300
    },
    {
      "epoch": 7.109866666666667,
      "grad_norm": 0.14428536593914032,
      "learning_rate": 5.563333333333334e-06,
      "loss": 0.0032,
      "step": 133310
    },
    {
      "epoch": 7.1104,
      "grad_norm": 0.14390891790390015,
      "learning_rate": 5.56e-06,
      "loss": 0.0018,
      "step": 133320
    },
    {
      "epoch": 7.1109333333333336,
      "grad_norm": 0.06742870807647705,
      "learning_rate": 5.556666666666667e-06,
      "loss": 0.0013,
      "step": 133330
    },
    {
      "epoch": 7.111466666666667,
      "grad_norm": 0.214178666472435,
      "learning_rate": 5.553333333333333e-06,
      "loss": 0.0014,
      "step": 133340
    },
    {
      "epoch": 7.112,
      "grad_norm": 0.12344059348106384,
      "learning_rate": 5.55e-06,
      "loss": 0.0018,
      "step": 133350
    },
    {
      "epoch": 7.112533333333333,
      "grad_norm": 0.17261728644371033,
      "learning_rate": 5.546666666666666e-06,
      "loss": 0.0015,
      "step": 133360
    },
    {
      "epoch": 7.113066666666667,
      "grad_norm": 0.30827733874320984,
      "learning_rate": 5.543333333333333e-06,
      "loss": 0.0017,
      "step": 133370
    },
    {
      "epoch": 7.1136,
      "grad_norm": 0.1969473659992218,
      "learning_rate": 5.54e-06,
      "loss": 0.0018,
      "step": 133380
    },
    {
      "epoch": 7.114133333333333,
      "grad_norm": 0.3384077548980713,
      "learning_rate": 5.536666666666667e-06,
      "loss": 0.0014,
      "step": 133390
    },
    {
      "epoch": 7.1146666666666665,
      "grad_norm": 0.14483624696731567,
      "learning_rate": 5.5333333333333334e-06,
      "loss": 0.0018,
      "step": 133400
    },
    {
      "epoch": 7.1152,
      "grad_norm": 0.2278967946767807,
      "learning_rate": 5.53e-06,
      "loss": 0.0017,
      "step": 133410
    },
    {
      "epoch": 7.115733333333333,
      "grad_norm": 0.14222785830497742,
      "learning_rate": 5.5266666666666666e-06,
      "loss": 0.0016,
      "step": 133420
    },
    {
      "epoch": 7.116266666666666,
      "grad_norm": 0.5545156002044678,
      "learning_rate": 5.5233333333333335e-06,
      "loss": 0.0015,
      "step": 133430
    },
    {
      "epoch": 7.1168,
      "grad_norm": 0.09627945721149445,
      "learning_rate": 5.5200000000000005e-06,
      "loss": 0.0017,
      "step": 133440
    },
    {
      "epoch": 7.117333333333334,
      "grad_norm": 0.17587320506572723,
      "learning_rate": 5.5166666666666675e-06,
      "loss": 0.0014,
      "step": 133450
    },
    {
      "epoch": 7.117866666666667,
      "grad_norm": 0.10261764377355576,
      "learning_rate": 5.513333333333334e-06,
      "loss": 0.0016,
      "step": 133460
    },
    {
      "epoch": 7.1184,
      "grad_norm": 0.23329192399978638,
      "learning_rate": 5.510000000000001e-06,
      "loss": 0.0024,
      "step": 133470
    },
    {
      "epoch": 7.118933333333334,
      "grad_norm": 0.14678210020065308,
      "learning_rate": 5.506666666666667e-06,
      "loss": 0.0017,
      "step": 133480
    },
    {
      "epoch": 7.119466666666667,
      "grad_norm": 0.1516791731119156,
      "learning_rate": 5.503333333333333e-06,
      "loss": 0.0015,
      "step": 133490
    },
    {
      "epoch": 7.12,
      "grad_norm": 0.2518012523651123,
      "learning_rate": 5.500000000000001e-06,
      "loss": 0.0017,
      "step": 133500
    },
    {
      "epoch": 7.120533333333333,
      "grad_norm": 0.07132285088300705,
      "learning_rate": 5.496666666666667e-06,
      "loss": 0.0019,
      "step": 133510
    },
    {
      "epoch": 7.121066666666667,
      "grad_norm": 0.2462054193019867,
      "learning_rate": 5.493333333333334e-06,
      "loss": 0.0023,
      "step": 133520
    },
    {
      "epoch": 7.1216,
      "grad_norm": 0.20015616714954376,
      "learning_rate": 5.49e-06,
      "loss": 0.0018,
      "step": 133530
    },
    {
      "epoch": 7.122133333333333,
      "grad_norm": 0.36920422315597534,
      "learning_rate": 5.486666666666667e-06,
      "loss": 0.0025,
      "step": 133540
    },
    {
      "epoch": 7.1226666666666665,
      "grad_norm": 0.014751340262591839,
      "learning_rate": 5.483333333333333e-06,
      "loss": 0.0014,
      "step": 133550
    },
    {
      "epoch": 7.1232,
      "grad_norm": 0.07316497713327408,
      "learning_rate": 5.48e-06,
      "loss": 0.0016,
      "step": 133560
    },
    {
      "epoch": 7.123733333333333,
      "grad_norm": 0.11985552310943604,
      "learning_rate": 5.476666666666667e-06,
      "loss": 0.0018,
      "step": 133570
    },
    {
      "epoch": 7.124266666666666,
      "grad_norm": 0.30863717198371887,
      "learning_rate": 5.473333333333334e-06,
      "loss": 0.0012,
      "step": 133580
    },
    {
      "epoch": 7.1248,
      "grad_norm": 0.3716158866882324,
      "learning_rate": 5.47e-06,
      "loss": 0.0015,
      "step": 133590
    },
    {
      "epoch": 7.125333333333334,
      "grad_norm": 0.19941052794456482,
      "learning_rate": 5.466666666666667e-06,
      "loss": 0.0017,
      "step": 133600
    },
    {
      "epoch": 7.125866666666667,
      "grad_norm": 0.09093005210161209,
      "learning_rate": 5.463333333333333e-06,
      "loss": 0.0019,
      "step": 133610
    },
    {
      "epoch": 7.1264,
      "grad_norm": 0.06443629413843155,
      "learning_rate": 5.46e-06,
      "loss": 0.0021,
      "step": 133620
    },
    {
      "epoch": 7.126933333333334,
      "grad_norm": 0.16039684414863586,
      "learning_rate": 5.456666666666667e-06,
      "loss": 0.0014,
      "step": 133630
    },
    {
      "epoch": 7.127466666666667,
      "grad_norm": 0.11466042697429657,
      "learning_rate": 5.453333333333334e-06,
      "loss": 0.0019,
      "step": 133640
    },
    {
      "epoch": 7.128,
      "grad_norm": 0.08623050153255463,
      "learning_rate": 5.45e-06,
      "loss": 0.0016,
      "step": 133650
    },
    {
      "epoch": 7.128533333333333,
      "grad_norm": 0.11438950151205063,
      "learning_rate": 5.4466666666666665e-06,
      "loss": 0.0023,
      "step": 133660
    },
    {
      "epoch": 7.129066666666667,
      "grad_norm": 0.053996384143829346,
      "learning_rate": 5.4433333333333335e-06,
      "loss": 0.002,
      "step": 133670
    },
    {
      "epoch": 7.1296,
      "grad_norm": 0.046007558703422546,
      "learning_rate": 5.44e-06,
      "loss": 0.0014,
      "step": 133680
    },
    {
      "epoch": 7.130133333333333,
      "grad_norm": 0.2247789204120636,
      "learning_rate": 5.436666666666667e-06,
      "loss": 0.0015,
      "step": 133690
    },
    {
      "epoch": 7.1306666666666665,
      "grad_norm": 0.1406569629907608,
      "learning_rate": 5.4333333333333335e-06,
      "loss": 0.0017,
      "step": 133700
    },
    {
      "epoch": 7.1312,
      "grad_norm": 0.04767676815390587,
      "learning_rate": 5.4300000000000005e-06,
      "loss": 0.0019,
      "step": 133710
    },
    {
      "epoch": 7.131733333333333,
      "grad_norm": 0.15418553352355957,
      "learning_rate": 5.426666666666667e-06,
      "loss": 0.0014,
      "step": 133720
    },
    {
      "epoch": 7.132266666666666,
      "grad_norm": 0.3303796350955963,
      "learning_rate": 5.423333333333334e-06,
      "loss": 0.0013,
      "step": 133730
    },
    {
      "epoch": 7.1328,
      "grad_norm": 0.285052090883255,
      "learning_rate": 5.42e-06,
      "loss": 0.0013,
      "step": 133740
    },
    {
      "epoch": 7.133333333333334,
      "grad_norm": 0.06441529840230942,
      "learning_rate": 5.416666666666667e-06,
      "loss": 0.0023,
      "step": 133750
    },
    {
      "epoch": 7.133866666666667,
      "grad_norm": 0.17416460812091827,
      "learning_rate": 5.413333333333334e-06,
      "loss": 0.0015,
      "step": 133760
    },
    {
      "epoch": 7.1344,
      "grad_norm": 0.09804549813270569,
      "learning_rate": 5.410000000000001e-06,
      "loss": 0.0021,
      "step": 133770
    },
    {
      "epoch": 7.134933333333334,
      "grad_norm": 0.3960948884487152,
      "learning_rate": 5.406666666666667e-06,
      "loss": 0.0019,
      "step": 133780
    },
    {
      "epoch": 7.135466666666667,
      "grad_norm": 0.09301545470952988,
      "learning_rate": 5.403333333333334e-06,
      "loss": 0.0016,
      "step": 133790
    },
    {
      "epoch": 7.136,
      "grad_norm": 0.08734902739524841,
      "learning_rate": 5.4e-06,
      "loss": 0.0014,
      "step": 133800
    },
    {
      "epoch": 7.136533333333333,
      "grad_norm": 0.2617436647415161,
      "learning_rate": 5.396666666666667e-06,
      "loss": 0.0018,
      "step": 133810
    },
    {
      "epoch": 7.137066666666667,
      "grad_norm": 0.08917465806007385,
      "learning_rate": 5.393333333333334e-06,
      "loss": 0.002,
      "step": 133820
    },
    {
      "epoch": 7.1376,
      "grad_norm": 0.15630172193050385,
      "learning_rate": 5.390000000000001e-06,
      "loss": 0.0023,
      "step": 133830
    },
    {
      "epoch": 7.138133333333333,
      "grad_norm": 0.14024196565151215,
      "learning_rate": 5.386666666666667e-06,
      "loss": 0.0015,
      "step": 133840
    },
    {
      "epoch": 7.1386666666666665,
      "grad_norm": 0.17190168797969818,
      "learning_rate": 5.383333333333333e-06,
      "loss": 0.0023,
      "step": 133850
    },
    {
      "epoch": 7.1392,
      "grad_norm": 0.24684853851795197,
      "learning_rate": 5.38e-06,
      "loss": 0.0015,
      "step": 133860
    },
    {
      "epoch": 7.139733333333333,
      "grad_norm": 0.6232500672340393,
      "learning_rate": 5.376666666666666e-06,
      "loss": 0.0014,
      "step": 133870
    },
    {
      "epoch": 7.140266666666666,
      "grad_norm": 0.12209714949131012,
      "learning_rate": 5.373333333333333e-06,
      "loss": 0.0011,
      "step": 133880
    },
    {
      "epoch": 7.1408,
      "grad_norm": 0.28616711497306824,
      "learning_rate": 5.37e-06,
      "loss": 0.002,
      "step": 133890
    },
    {
      "epoch": 7.141333333333334,
      "grad_norm": 0.17692895233631134,
      "learning_rate": 5.366666666666667e-06,
      "loss": 0.0021,
      "step": 133900
    },
    {
      "epoch": 7.141866666666667,
      "grad_norm": 0.19811704754829407,
      "learning_rate": 5.363333333333333e-06,
      "loss": 0.0016,
      "step": 133910
    },
    {
      "epoch": 7.1424,
      "grad_norm": 0.3248492479324341,
      "learning_rate": 5.36e-06,
      "loss": 0.0026,
      "step": 133920
    },
    {
      "epoch": 7.142933333333334,
      "grad_norm": 0.3722039461135864,
      "learning_rate": 5.3566666666666665e-06,
      "loss": 0.0015,
      "step": 133930
    },
    {
      "epoch": 7.143466666666667,
      "grad_norm": 0.13232176005840302,
      "learning_rate": 5.3533333333333335e-06,
      "loss": 0.0012,
      "step": 133940
    },
    {
      "epoch": 7.144,
      "grad_norm": 0.15051214396953583,
      "learning_rate": 5.3500000000000004e-06,
      "loss": 0.0013,
      "step": 133950
    },
    {
      "epoch": 7.144533333333333,
      "grad_norm": 0.30980566143989563,
      "learning_rate": 5.3466666666666674e-06,
      "loss": 0.0021,
      "step": 133960
    },
    {
      "epoch": 7.145066666666667,
      "grad_norm": 0.04468610882759094,
      "learning_rate": 5.3433333333333336e-06,
      "loss": 0.0015,
      "step": 133970
    },
    {
      "epoch": 7.1456,
      "grad_norm": 0.1144857108592987,
      "learning_rate": 5.3400000000000005e-06,
      "loss": 0.0015,
      "step": 133980
    },
    {
      "epoch": 7.146133333333333,
      "grad_norm": 0.14948175847530365,
      "learning_rate": 5.336666666666667e-06,
      "loss": 0.002,
      "step": 133990
    },
    {
      "epoch": 7.1466666666666665,
      "grad_norm": 0.16872282326221466,
      "learning_rate": 5.333333333333334e-06,
      "loss": 0.0014,
      "step": 134000
    },
    {
      "epoch": 7.1472,
      "grad_norm": 0.0876365453004837,
      "learning_rate": 5.330000000000001e-06,
      "loss": 0.0017,
      "step": 134010
    },
    {
      "epoch": 7.147733333333333,
      "grad_norm": 0.12113960087299347,
      "learning_rate": 5.326666666666667e-06,
      "loss": 0.0017,
      "step": 134020
    },
    {
      "epoch": 7.148266666666666,
      "grad_norm": 0.201352059841156,
      "learning_rate": 5.323333333333334e-06,
      "loss": 0.002,
      "step": 134030
    },
    {
      "epoch": 7.1488,
      "grad_norm": 0.23550350964069366,
      "learning_rate": 5.32e-06,
      "loss": 0.0016,
      "step": 134040
    },
    {
      "epoch": 7.149333333333334,
      "grad_norm": 0.3663710057735443,
      "learning_rate": 5.316666666666667e-06,
      "loss": 0.0014,
      "step": 134050
    },
    {
      "epoch": 7.149866666666667,
      "grad_norm": 0.08205460757017136,
      "learning_rate": 5.313333333333333e-06,
      "loss": 0.0025,
      "step": 134060
    },
    {
      "epoch": 7.1504,
      "grad_norm": 0.0905277281999588,
      "learning_rate": 5.31e-06,
      "loss": 0.0017,
      "step": 134070
    },
    {
      "epoch": 7.150933333333334,
      "grad_norm": 0.028735024854540825,
      "learning_rate": 5.306666666666667e-06,
      "loss": 0.0016,
      "step": 134080
    },
    {
      "epoch": 7.151466666666667,
      "grad_norm": 0.12412316352128983,
      "learning_rate": 5.303333333333334e-06,
      "loss": 0.0019,
      "step": 134090
    },
    {
      "epoch": 7.152,
      "grad_norm": 0.09563092142343521,
      "learning_rate": 5.3e-06,
      "loss": 0.0011,
      "step": 134100
    },
    {
      "epoch": 7.152533333333333,
      "grad_norm": 0.16975057125091553,
      "learning_rate": 5.296666666666667e-06,
      "loss": 0.0016,
      "step": 134110
    },
    {
      "epoch": 7.153066666666667,
      "grad_norm": 0.09149815887212753,
      "learning_rate": 5.293333333333333e-06,
      "loss": 0.0021,
      "step": 134120
    },
    {
      "epoch": 7.1536,
      "grad_norm": 0.09549811482429504,
      "learning_rate": 5.29e-06,
      "loss": 0.0025,
      "step": 134130
    },
    {
      "epoch": 7.154133333333333,
      "grad_norm": 0.3497345447540283,
      "learning_rate": 5.286666666666667e-06,
      "loss": 0.0016,
      "step": 134140
    },
    {
      "epoch": 7.1546666666666665,
      "grad_norm": 0.16156208515167236,
      "learning_rate": 5.283333333333334e-06,
      "loss": 0.0013,
      "step": 134150
    },
    {
      "epoch": 7.1552,
      "grad_norm": 0.11596128344535828,
      "learning_rate": 5.28e-06,
      "loss": 0.0014,
      "step": 134160
    },
    {
      "epoch": 7.155733333333333,
      "grad_norm": 0.09230872988700867,
      "learning_rate": 5.276666666666667e-06,
      "loss": 0.002,
      "step": 134170
    },
    {
      "epoch": 7.156266666666666,
      "grad_norm": 0.3402197062969208,
      "learning_rate": 5.273333333333333e-06,
      "loss": 0.0011,
      "step": 134180
    },
    {
      "epoch": 7.1568,
      "grad_norm": 0.03950788080692291,
      "learning_rate": 5.2699999999999995e-06,
      "loss": 0.0018,
      "step": 134190
    },
    {
      "epoch": 7.157333333333334,
      "grad_norm": 0.2882224917411804,
      "learning_rate": 5.266666666666667e-06,
      "loss": 0.0014,
      "step": 134200
    },
    {
      "epoch": 7.157866666666667,
      "grad_norm": 0.14465804398059845,
      "learning_rate": 5.2633333333333335e-06,
      "loss": 0.0015,
      "step": 134210
    },
    {
      "epoch": 7.1584,
      "grad_norm": 0.25467365980148315,
      "learning_rate": 5.2600000000000005e-06,
      "loss": 0.0016,
      "step": 134220
    },
    {
      "epoch": 7.158933333333334,
      "grad_norm": 0.39762452244758606,
      "learning_rate": 5.256666666666667e-06,
      "loss": 0.0026,
      "step": 134230
    },
    {
      "epoch": 7.159466666666667,
      "grad_norm": 0.14239396154880524,
      "learning_rate": 5.2533333333333336e-06,
      "loss": 0.0017,
      "step": 134240
    },
    {
      "epoch": 7.16,
      "grad_norm": 0.06327932327985764,
      "learning_rate": 5.25e-06,
      "loss": 0.0012,
      "step": 134250
    },
    {
      "epoch": 7.160533333333333,
      "grad_norm": 0.20884521305561066,
      "learning_rate": 5.246666666666667e-06,
      "loss": 0.0021,
      "step": 134260
    },
    {
      "epoch": 7.161066666666667,
      "grad_norm": 0.0613471083343029,
      "learning_rate": 5.243333333333334e-06,
      "loss": 0.0019,
      "step": 134270
    },
    {
      "epoch": 7.1616,
      "grad_norm": 0.20945176482200623,
      "learning_rate": 5.240000000000001e-06,
      "loss": 0.0015,
      "step": 134280
    },
    {
      "epoch": 7.162133333333333,
      "grad_norm": 0.05742362514138222,
      "learning_rate": 5.236666666666667e-06,
      "loss": 0.0016,
      "step": 134290
    },
    {
      "epoch": 7.1626666666666665,
      "grad_norm": 0.07553179562091827,
      "learning_rate": 5.233333333333334e-06,
      "loss": 0.002,
      "step": 134300
    },
    {
      "epoch": 7.1632,
      "grad_norm": 0.12273037433624268,
      "learning_rate": 5.23e-06,
      "loss": 0.0014,
      "step": 134310
    },
    {
      "epoch": 7.163733333333333,
      "grad_norm": 0.09647197276353836,
      "learning_rate": 5.226666666666667e-06,
      "loss": 0.0016,
      "step": 134320
    },
    {
      "epoch": 7.164266666666666,
      "grad_norm": 0.04179397225379944,
      "learning_rate": 5.223333333333334e-06,
      "loss": 0.0021,
      "step": 134330
    },
    {
      "epoch": 7.1648,
      "grad_norm": 0.061282679438591,
      "learning_rate": 5.220000000000001e-06,
      "loss": 0.0017,
      "step": 134340
    },
    {
      "epoch": 7.165333333333333,
      "grad_norm": 0.2900126874446869,
      "learning_rate": 5.216666666666667e-06,
      "loss": 0.0015,
      "step": 134350
    },
    {
      "epoch": 7.165866666666667,
      "grad_norm": 0.06231614947319031,
      "learning_rate": 5.213333333333333e-06,
      "loss": 0.0014,
      "step": 134360
    },
    {
      "epoch": 7.1664,
      "grad_norm": 0.2010394036769867,
      "learning_rate": 5.21e-06,
      "loss": 0.0012,
      "step": 134370
    },
    {
      "epoch": 7.166933333333334,
      "grad_norm": 0.05857358127832413,
      "learning_rate": 5.206666666666666e-06,
      "loss": 0.0017,
      "step": 134380
    },
    {
      "epoch": 7.167466666666667,
      "grad_norm": 0.22847653925418854,
      "learning_rate": 5.203333333333334e-06,
      "loss": 0.0018,
      "step": 134390
    },
    {
      "epoch": 7.168,
      "grad_norm": 0.23287510871887207,
      "learning_rate": 5.2e-06,
      "loss": 0.0038,
      "step": 134400
    },
    {
      "epoch": 7.168533333333333,
      "grad_norm": 0.0967094749212265,
      "learning_rate": 5.196666666666667e-06,
      "loss": 0.0014,
      "step": 134410
    },
    {
      "epoch": 7.169066666666667,
      "grad_norm": 0.043973661959171295,
      "learning_rate": 5.193333333333333e-06,
      "loss": 0.0024,
      "step": 134420
    },
    {
      "epoch": 7.1696,
      "grad_norm": 0.14603428542613983,
      "learning_rate": 5.19e-06,
      "loss": 0.002,
      "step": 134430
    },
    {
      "epoch": 7.170133333333333,
      "grad_norm": 0.36653512716293335,
      "learning_rate": 5.186666666666666e-06,
      "loss": 0.0014,
      "step": 134440
    },
    {
      "epoch": 7.1706666666666665,
      "grad_norm": 0.3658077120780945,
      "learning_rate": 5.183333333333333e-06,
      "loss": 0.0015,
      "step": 134450
    },
    {
      "epoch": 7.1712,
      "grad_norm": 0.053051214665174484,
      "learning_rate": 5.18e-06,
      "loss": 0.0014,
      "step": 134460
    },
    {
      "epoch": 7.171733333333333,
      "grad_norm": 0.023219408467411995,
      "learning_rate": 5.176666666666667e-06,
      "loss": 0.0019,
      "step": 134470
    },
    {
      "epoch": 7.172266666666666,
      "grad_norm": 0.45303452014923096,
      "learning_rate": 5.1733333333333335e-06,
      "loss": 0.0012,
      "step": 134480
    },
    {
      "epoch": 7.1728,
      "grad_norm": 0.33080047369003296,
      "learning_rate": 5.1700000000000005e-06,
      "loss": 0.0014,
      "step": 134490
    },
    {
      "epoch": 7.173333333333334,
      "grad_norm": 0.1451011449098587,
      "learning_rate": 5.166666666666667e-06,
      "loss": 0.002,
      "step": 134500
    },
    {
      "epoch": 7.173866666666667,
      "grad_norm": 0.3402731716632843,
      "learning_rate": 5.163333333333334e-06,
      "loss": 0.0032,
      "step": 134510
    },
    {
      "epoch": 7.1744,
      "grad_norm": 0.2143094837665558,
      "learning_rate": 5.1600000000000006e-06,
      "loss": 0.0026,
      "step": 134520
    },
    {
      "epoch": 7.174933333333334,
      "grad_norm": 0.26428937911987305,
      "learning_rate": 5.156666666666667e-06,
      "loss": 0.0018,
      "step": 134530
    },
    {
      "epoch": 7.175466666666667,
      "grad_norm": 0.04004688933491707,
      "learning_rate": 5.153333333333334e-06,
      "loss": 0.0024,
      "step": 134540
    },
    {
      "epoch": 7.176,
      "grad_norm": 0.12434752285480499,
      "learning_rate": 5.15e-06,
      "loss": 0.0013,
      "step": 134550
    },
    {
      "epoch": 7.176533333333333,
      "grad_norm": 0.07071195542812347,
      "learning_rate": 5.146666666666667e-06,
      "loss": 0.003,
      "step": 134560
    },
    {
      "epoch": 7.177066666666667,
      "grad_norm": 0.026216667145490646,
      "learning_rate": 5.143333333333333e-06,
      "loss": 0.0015,
      "step": 134570
    },
    {
      "epoch": 7.1776,
      "grad_norm": 0.2593093514442444,
      "learning_rate": 5.140000000000001e-06,
      "loss": 0.002,
      "step": 134580
    },
    {
      "epoch": 7.178133333333333,
      "grad_norm": 0.10053494572639465,
      "learning_rate": 5.136666666666667e-06,
      "loss": 0.0021,
      "step": 134590
    },
    {
      "epoch": 7.1786666666666665,
      "grad_norm": 0.0366639569401741,
      "learning_rate": 5.133333333333334e-06,
      "loss": 0.0014,
      "step": 134600
    },
    {
      "epoch": 7.1792,
      "grad_norm": 0.42439842224121094,
      "learning_rate": 5.13e-06,
      "loss": 0.0022,
      "step": 134610
    },
    {
      "epoch": 7.179733333333333,
      "grad_norm": 0.19066894054412842,
      "learning_rate": 5.126666666666667e-06,
      "loss": 0.0018,
      "step": 134620
    },
    {
      "epoch": 7.180266666666666,
      "grad_norm": 0.23300056159496307,
      "learning_rate": 5.123333333333333e-06,
      "loss": 0.002,
      "step": 134630
    },
    {
      "epoch": 7.1808,
      "grad_norm": 0.041503626853227615,
      "learning_rate": 5.12e-06,
      "loss": 0.0013,
      "step": 134640
    },
    {
      "epoch": 7.181333333333333,
      "grad_norm": 0.09302262961864471,
      "learning_rate": 5.116666666666667e-06,
      "loss": 0.0023,
      "step": 134650
    },
    {
      "epoch": 7.181866666666667,
      "grad_norm": 0.25449952483177185,
      "learning_rate": 5.113333333333334e-06,
      "loss": 0.0025,
      "step": 134660
    },
    {
      "epoch": 7.1824,
      "grad_norm": 0.22409527003765106,
      "learning_rate": 5.11e-06,
      "loss": 0.0025,
      "step": 134670
    },
    {
      "epoch": 7.182933333333334,
      "grad_norm": 0.37024131417274475,
      "learning_rate": 5.106666666666667e-06,
      "loss": 0.0015,
      "step": 134680
    },
    {
      "epoch": 7.183466666666667,
      "grad_norm": 0.15004798769950867,
      "learning_rate": 5.103333333333333e-06,
      "loss": 0.0018,
      "step": 134690
    },
    {
      "epoch": 7.184,
      "grad_norm": 0.15676645934581757,
      "learning_rate": 5.1e-06,
      "loss": 0.0017,
      "step": 134700
    },
    {
      "epoch": 7.184533333333333,
      "grad_norm": 0.3011477589607239,
      "learning_rate": 5.096666666666667e-06,
      "loss": 0.0015,
      "step": 134710
    },
    {
      "epoch": 7.185066666666667,
      "grad_norm": 0.1356339156627655,
      "learning_rate": 5.093333333333333e-06,
      "loss": 0.0013,
      "step": 134720
    },
    {
      "epoch": 7.1856,
      "grad_norm": 0.05940454825758934,
      "learning_rate": 5.09e-06,
      "loss": 0.0014,
      "step": 134730
    },
    {
      "epoch": 7.186133333333333,
      "grad_norm": 0.11577390879392624,
      "learning_rate": 5.0866666666666665e-06,
      "loss": 0.0018,
      "step": 134740
    },
    {
      "epoch": 7.1866666666666665,
      "grad_norm": 0.6314998269081116,
      "learning_rate": 5.0833333333333335e-06,
      "loss": 0.0017,
      "step": 134750
    },
    {
      "epoch": 7.1872,
      "grad_norm": 0.06996007263660431,
      "learning_rate": 5.08e-06,
      "loss": 0.0019,
      "step": 134760
    },
    {
      "epoch": 7.187733333333333,
      "grad_norm": 0.2588755786418915,
      "learning_rate": 5.0766666666666675e-06,
      "loss": 0.0019,
      "step": 134770
    },
    {
      "epoch": 7.188266666666666,
      "grad_norm": 0.42448583245277405,
      "learning_rate": 5.073333333333334e-06,
      "loss": 0.0015,
      "step": 134780
    },
    {
      "epoch": 7.1888,
      "grad_norm": 0.6212085485458374,
      "learning_rate": 5.070000000000001e-06,
      "loss": 0.0019,
      "step": 134790
    },
    {
      "epoch": 7.189333333333333,
      "grad_norm": 0.3224390745162964,
      "learning_rate": 5.066666666666667e-06,
      "loss": 0.0017,
      "step": 134800
    },
    {
      "epoch": 7.189866666666667,
      "grad_norm": 0.1454877406358719,
      "learning_rate": 5.063333333333334e-06,
      "loss": 0.0024,
      "step": 134810
    },
    {
      "epoch": 7.1904,
      "grad_norm": 0.04379361867904663,
      "learning_rate": 5.06e-06,
      "loss": 0.0017,
      "step": 134820
    },
    {
      "epoch": 7.190933333333334,
      "grad_norm": 0.14400126039981842,
      "learning_rate": 5.056666666666667e-06,
      "loss": 0.0013,
      "step": 134830
    },
    {
      "epoch": 7.191466666666667,
      "grad_norm": 0.04580102488398552,
      "learning_rate": 5.053333333333334e-06,
      "loss": 0.0013,
      "step": 134840
    },
    {
      "epoch": 7.192,
      "grad_norm": 0.30692383646965027,
      "learning_rate": 5.050000000000001e-06,
      "loss": 0.002,
      "step": 134850
    },
    {
      "epoch": 7.1925333333333334,
      "grad_norm": 0.12082984298467636,
      "learning_rate": 5.046666666666667e-06,
      "loss": 0.0015,
      "step": 134860
    },
    {
      "epoch": 7.193066666666667,
      "grad_norm": 0.021057788282632828,
      "learning_rate": 5.043333333333333e-06,
      "loss": 0.0015,
      "step": 134870
    },
    {
      "epoch": 7.1936,
      "grad_norm": 0.061964988708496094,
      "learning_rate": 5.04e-06,
      "loss": 0.0026,
      "step": 134880
    },
    {
      "epoch": 7.194133333333333,
      "grad_norm": 0.24671275913715363,
      "learning_rate": 5.036666666666667e-06,
      "loss": 0.0013,
      "step": 134890
    },
    {
      "epoch": 7.1946666666666665,
      "grad_norm": 0.1959936022758484,
      "learning_rate": 5.033333333333334e-06,
      "loss": 0.0017,
      "step": 134900
    },
    {
      "epoch": 7.1952,
      "grad_norm": 0.14745837450027466,
      "learning_rate": 5.03e-06,
      "loss": 0.0015,
      "step": 134910
    },
    {
      "epoch": 7.195733333333333,
      "grad_norm": 0.3395821452140808,
      "learning_rate": 5.026666666666667e-06,
      "loss": 0.0016,
      "step": 134920
    },
    {
      "epoch": 7.196266666666666,
      "grad_norm": 0.09561508893966675,
      "learning_rate": 5.023333333333333e-06,
      "loss": 0.0018,
      "step": 134930
    },
    {
      "epoch": 7.1968,
      "grad_norm": 0.18523000180721283,
      "learning_rate": 5.02e-06,
      "loss": 0.0016,
      "step": 134940
    },
    {
      "epoch": 7.197333333333333,
      "grad_norm": 0.255323201417923,
      "learning_rate": 5.016666666666666e-06,
      "loss": 0.0017,
      "step": 134950
    },
    {
      "epoch": 7.197866666666667,
      "grad_norm": 0.06274482607841492,
      "learning_rate": 5.013333333333334e-06,
      "loss": 0.0017,
      "step": 134960
    },
    {
      "epoch": 7.1984,
      "grad_norm": 0.05843768268823624,
      "learning_rate": 5.01e-06,
      "loss": 0.002,
      "step": 134970
    },
    {
      "epoch": 7.198933333333334,
      "grad_norm": 0.2265360951423645,
      "learning_rate": 5.006666666666667e-06,
      "loss": 0.002,
      "step": 134980
    },
    {
      "epoch": 7.199466666666667,
      "grad_norm": 0.17328746616840363,
      "learning_rate": 5.0033333333333334e-06,
      "loss": 0.0028,
      "step": 134990
    },
    {
      "epoch": 7.2,
      "grad_norm": 0.17270657420158386,
      "learning_rate": 5e-06,
      "loss": 0.0019,
      "step": 135000
    },
    {
      "epoch": 7.2005333333333335,
      "grad_norm": 0.04835119843482971,
      "learning_rate": 4.9966666666666665e-06,
      "loss": 0.0023,
      "step": 135010
    },
    {
      "epoch": 7.201066666666667,
      "grad_norm": 0.16990065574645996,
      "learning_rate": 4.9933333333333335e-06,
      "loss": 0.0015,
      "step": 135020
    },
    {
      "epoch": 7.2016,
      "grad_norm": 0.017250150442123413,
      "learning_rate": 4.9900000000000005e-06,
      "loss": 0.0013,
      "step": 135030
    },
    {
      "epoch": 7.202133333333333,
      "grad_norm": 0.20689909160137177,
      "learning_rate": 4.986666666666667e-06,
      "loss": 0.0017,
      "step": 135040
    },
    {
      "epoch": 7.2026666666666666,
      "grad_norm": 0.09818621724843979,
      "learning_rate": 4.983333333333334e-06,
      "loss": 0.0013,
      "step": 135050
    },
    {
      "epoch": 7.2032,
      "grad_norm": 0.12729094922542572,
      "learning_rate": 4.98e-06,
      "loss": 0.0017,
      "step": 135060
    },
    {
      "epoch": 7.203733333333333,
      "grad_norm": 0.05487624928355217,
      "learning_rate": 4.976666666666667e-06,
      "loss": 0.0015,
      "step": 135070
    },
    {
      "epoch": 7.204266666666666,
      "grad_norm": 0.037299394607543945,
      "learning_rate": 4.973333333333334e-06,
      "loss": 0.0013,
      "step": 135080
    },
    {
      "epoch": 7.2048,
      "grad_norm": 0.1767250895500183,
      "learning_rate": 4.970000000000001e-06,
      "loss": 0.0012,
      "step": 135090
    },
    {
      "epoch": 7.205333333333333,
      "grad_norm": 0.1751192808151245,
      "learning_rate": 4.966666666666667e-06,
      "loss": 0.0016,
      "step": 135100
    },
    {
      "epoch": 7.205866666666667,
      "grad_norm": 0.20684103667736053,
      "learning_rate": 4.963333333333334e-06,
      "loss": 0.0013,
      "step": 135110
    },
    {
      "epoch": 7.2064,
      "grad_norm": 0.09432156383991241,
      "learning_rate": 4.96e-06,
      "loss": 0.0016,
      "step": 135120
    },
    {
      "epoch": 7.206933333333334,
      "grad_norm": 0.09986566752195358,
      "learning_rate": 4.956666666666667e-06,
      "loss": 0.0015,
      "step": 135130
    },
    {
      "epoch": 7.207466666666667,
      "grad_norm": 0.0643073245882988,
      "learning_rate": 4.953333333333333e-06,
      "loss": 0.0022,
      "step": 135140
    },
    {
      "epoch": 7.208,
      "grad_norm": 0.3132166266441345,
      "learning_rate": 4.950000000000001e-06,
      "loss": 0.0019,
      "step": 135150
    },
    {
      "epoch": 7.2085333333333335,
      "grad_norm": 0.391952246427536,
      "learning_rate": 4.946666666666667e-06,
      "loss": 0.0016,
      "step": 135160
    },
    {
      "epoch": 7.209066666666667,
      "grad_norm": 0.07045641541481018,
      "learning_rate": 4.943333333333334e-06,
      "loss": 0.002,
      "step": 135170
    },
    {
      "epoch": 7.2096,
      "grad_norm": 0.11654753983020782,
      "learning_rate": 4.94e-06,
      "loss": 0.0017,
      "step": 135180
    },
    {
      "epoch": 7.210133333333333,
      "grad_norm": 0.36922261118888855,
      "learning_rate": 4.936666666666667e-06,
      "loss": 0.002,
      "step": 135190
    },
    {
      "epoch": 7.210666666666667,
      "grad_norm": 0.05940403789281845,
      "learning_rate": 4.933333333333333e-06,
      "loss": 0.0016,
      "step": 135200
    },
    {
      "epoch": 7.2112,
      "grad_norm": 0.09132297337055206,
      "learning_rate": 4.93e-06,
      "loss": 0.0018,
      "step": 135210
    },
    {
      "epoch": 7.211733333333333,
      "grad_norm": 0.11589931696653366,
      "learning_rate": 4.926666666666667e-06,
      "loss": 0.0022,
      "step": 135220
    },
    {
      "epoch": 7.212266666666666,
      "grad_norm": 0.42157986760139465,
      "learning_rate": 4.923333333333333e-06,
      "loss": 0.0014,
      "step": 135230
    },
    {
      "epoch": 7.2128,
      "grad_norm": 0.08774560689926147,
      "learning_rate": 4.92e-06,
      "loss": 0.0013,
      "step": 135240
    },
    {
      "epoch": 7.213333333333333,
      "grad_norm": 0.4567728638648987,
      "learning_rate": 4.9166666666666665e-06,
      "loss": 0.0024,
      "step": 135250
    },
    {
      "epoch": 7.213866666666667,
      "grad_norm": 0.1726548820734024,
      "learning_rate": 4.9133333333333334e-06,
      "loss": 0.0013,
      "step": 135260
    },
    {
      "epoch": 7.2144,
      "grad_norm": 0.06453116983175278,
      "learning_rate": 4.9100000000000004e-06,
      "loss": 0.0022,
      "step": 135270
    },
    {
      "epoch": 7.214933333333334,
      "grad_norm": 0.4006393849849701,
      "learning_rate": 4.906666666666667e-06,
      "loss": 0.0023,
      "step": 135280
    },
    {
      "epoch": 7.215466666666667,
      "grad_norm": 0.2543938159942627,
      "learning_rate": 4.9033333333333335e-06,
      "loss": 0.0026,
      "step": 135290
    },
    {
      "epoch": 7.216,
      "grad_norm": 0.19968050718307495,
      "learning_rate": 4.9000000000000005e-06,
      "loss": 0.0012,
      "step": 135300
    },
    {
      "epoch": 7.2165333333333335,
      "grad_norm": 0.28264951705932617,
      "learning_rate": 4.896666666666667e-06,
      "loss": 0.0015,
      "step": 135310
    },
    {
      "epoch": 7.217066666666667,
      "grad_norm": 0.08915683627128601,
      "learning_rate": 4.893333333333334e-06,
      "loss": 0.0017,
      "step": 135320
    },
    {
      "epoch": 7.2176,
      "grad_norm": 0.20582488179206848,
      "learning_rate": 4.89e-06,
      "loss": 0.0017,
      "step": 135330
    },
    {
      "epoch": 7.218133333333333,
      "grad_norm": 0.04948559030890465,
      "learning_rate": 4.886666666666667e-06,
      "loss": 0.0025,
      "step": 135340
    },
    {
      "epoch": 7.218666666666667,
      "grad_norm": 0.2098369598388672,
      "learning_rate": 4.883333333333334e-06,
      "loss": 0.0014,
      "step": 135350
    },
    {
      "epoch": 7.2192,
      "grad_norm": 0.3713008463382721,
      "learning_rate": 4.880000000000001e-06,
      "loss": 0.0018,
      "step": 135360
    },
    {
      "epoch": 7.219733333333333,
      "grad_norm": 0.21345241367816925,
      "learning_rate": 4.876666666666667e-06,
      "loss": 0.0019,
      "step": 135370
    },
    {
      "epoch": 7.220266666666666,
      "grad_norm": 0.28857535123825073,
      "learning_rate": 4.873333333333333e-06,
      "loss": 0.0016,
      "step": 135380
    },
    {
      "epoch": 7.2208,
      "grad_norm": 0.4586593508720398,
      "learning_rate": 4.87e-06,
      "loss": 0.002,
      "step": 135390
    },
    {
      "epoch": 7.221333333333333,
      "grad_norm": 0.03861832991242409,
      "learning_rate": 4.866666666666667e-06,
      "loss": 0.0026,
      "step": 135400
    },
    {
      "epoch": 7.221866666666667,
      "grad_norm": 0.09041503071784973,
      "learning_rate": 4.863333333333334e-06,
      "loss": 0.0014,
      "step": 135410
    },
    {
      "epoch": 7.2224,
      "grad_norm": 0.061679136008024216,
      "learning_rate": 4.86e-06,
      "loss": 0.002,
      "step": 135420
    },
    {
      "epoch": 7.222933333333334,
      "grad_norm": 0.3097379803657532,
      "learning_rate": 4.856666666666667e-06,
      "loss": 0.0021,
      "step": 135430
    },
    {
      "epoch": 7.223466666666667,
      "grad_norm": 0.4049064517021179,
      "learning_rate": 4.853333333333333e-06,
      "loss": 0.0018,
      "step": 135440
    },
    {
      "epoch": 7.224,
      "grad_norm": 0.1778695434331894,
      "learning_rate": 4.85e-06,
      "loss": 0.0014,
      "step": 135450
    },
    {
      "epoch": 7.2245333333333335,
      "grad_norm": 0.145601287484169,
      "learning_rate": 4.846666666666667e-06,
      "loss": 0.0013,
      "step": 135460
    },
    {
      "epoch": 7.225066666666667,
      "grad_norm": 0.09833250194787979,
      "learning_rate": 4.843333333333334e-06,
      "loss": 0.0014,
      "step": 135470
    },
    {
      "epoch": 7.2256,
      "grad_norm": 0.53886878490448,
      "learning_rate": 4.84e-06,
      "loss": 0.0037,
      "step": 135480
    },
    {
      "epoch": 7.226133333333333,
      "grad_norm": 0.060015879571437836,
      "learning_rate": 4.836666666666667e-06,
      "loss": 0.0013,
      "step": 135490
    },
    {
      "epoch": 7.226666666666667,
      "grad_norm": 0.14301906526088715,
      "learning_rate": 4.833333333333333e-06,
      "loss": 0.0013,
      "step": 135500
    },
    {
      "epoch": 7.2272,
      "grad_norm": 0.21806380152702332,
      "learning_rate": 4.83e-06,
      "loss": 0.0016,
      "step": 135510
    },
    {
      "epoch": 7.227733333333333,
      "grad_norm": 0.33713117241859436,
      "learning_rate": 4.8266666666666665e-06,
      "loss": 0.0014,
      "step": 135520
    },
    {
      "epoch": 7.228266666666666,
      "grad_norm": 0.14326618611812592,
      "learning_rate": 4.8233333333333335e-06,
      "loss": 0.0014,
      "step": 135530
    },
    {
      "epoch": 7.2288,
      "grad_norm": 0.22781766951084137,
      "learning_rate": 4.8200000000000004e-06,
      "loss": 0.0018,
      "step": 135540
    },
    {
      "epoch": 7.229333333333333,
      "grad_norm": 0.12268619239330292,
      "learning_rate": 4.816666666666667e-06,
      "loss": 0.0015,
      "step": 135550
    },
    {
      "epoch": 7.229866666666666,
      "grad_norm": 0.28227096796035767,
      "learning_rate": 4.8133333333333336e-06,
      "loss": 0.0016,
      "step": 135560
    },
    {
      "epoch": 7.2304,
      "grad_norm": 0.14452533423900604,
      "learning_rate": 4.81e-06,
      "loss": 0.0015,
      "step": 135570
    },
    {
      "epoch": 7.230933333333334,
      "grad_norm": 0.1734135001897812,
      "learning_rate": 4.806666666666667e-06,
      "loss": 0.002,
      "step": 135580
    },
    {
      "epoch": 7.231466666666667,
      "grad_norm": 0.2556374967098236,
      "learning_rate": 4.803333333333334e-06,
      "loss": 0.0016,
      "step": 135590
    },
    {
      "epoch": 7.232,
      "grad_norm": 0.16290293633937836,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.0024,
      "step": 135600
    },
    {
      "epoch": 7.2325333333333335,
      "grad_norm": 0.19868941605091095,
      "learning_rate": 4.796666666666667e-06,
      "loss": 0.0022,
      "step": 135610
    },
    {
      "epoch": 7.233066666666667,
      "grad_norm": 0.08134639263153076,
      "learning_rate": 4.793333333333334e-06,
      "loss": 0.0027,
      "step": 135620
    },
    {
      "epoch": 7.2336,
      "grad_norm": 0.08744923770427704,
      "learning_rate": 4.79e-06,
      "loss": 0.0017,
      "step": 135630
    },
    {
      "epoch": 7.234133333333333,
      "grad_norm": 0.11681868135929108,
      "learning_rate": 4.786666666666667e-06,
      "loss": 0.0026,
      "step": 135640
    },
    {
      "epoch": 7.234666666666667,
      "grad_norm": 0.04605427756905556,
      "learning_rate": 4.783333333333333e-06,
      "loss": 0.0012,
      "step": 135650
    },
    {
      "epoch": 7.2352,
      "grad_norm": 0.13481822609901428,
      "learning_rate": 4.780000000000001e-06,
      "loss": 0.0017,
      "step": 135660
    },
    {
      "epoch": 7.235733333333333,
      "grad_norm": 0.38407668471336365,
      "learning_rate": 4.776666666666667e-06,
      "loss": 0.0015,
      "step": 135670
    },
    {
      "epoch": 7.236266666666666,
      "grad_norm": 0.22806808352470398,
      "learning_rate": 4.773333333333334e-06,
      "loss": 0.0014,
      "step": 135680
    },
    {
      "epoch": 7.2368,
      "grad_norm": 0.09669173508882523,
      "learning_rate": 4.77e-06,
      "loss": 0.002,
      "step": 135690
    },
    {
      "epoch": 7.237333333333333,
      "grad_norm": 0.1182522103190422,
      "learning_rate": 4.766666666666667e-06,
      "loss": 0.0012,
      "step": 135700
    },
    {
      "epoch": 7.237866666666667,
      "grad_norm": 0.14851880073547363,
      "learning_rate": 4.763333333333333e-06,
      "loss": 0.0018,
      "step": 135710
    },
    {
      "epoch": 7.2384,
      "grad_norm": 0.24856393039226532,
      "learning_rate": 4.76e-06,
      "loss": 0.0012,
      "step": 135720
    },
    {
      "epoch": 7.238933333333334,
      "grad_norm": 0.12236126512289047,
      "learning_rate": 4.756666666666667e-06,
      "loss": 0.0014,
      "step": 135730
    },
    {
      "epoch": 7.239466666666667,
      "grad_norm": 0.31061649322509766,
      "learning_rate": 4.753333333333333e-06,
      "loss": 0.0013,
      "step": 135740
    },
    {
      "epoch": 7.24,
      "grad_norm": 0.1174306720495224,
      "learning_rate": 4.75e-06,
      "loss": 0.0019,
      "step": 135750
    },
    {
      "epoch": 7.2405333333333335,
      "grad_norm": 0.060688428580760956,
      "learning_rate": 4.746666666666666e-06,
      "loss": 0.0017,
      "step": 135760
    },
    {
      "epoch": 7.241066666666667,
      "grad_norm": 0.19892321527004242,
      "learning_rate": 4.743333333333333e-06,
      "loss": 0.0014,
      "step": 135770
    },
    {
      "epoch": 7.2416,
      "grad_norm": 0.06543343514204025,
      "learning_rate": 4.74e-06,
      "loss": 0.0016,
      "step": 135780
    },
    {
      "epoch": 7.242133333333333,
      "grad_norm": 0.3810794949531555,
      "learning_rate": 4.736666666666667e-06,
      "loss": 0.0023,
      "step": 135790
    },
    {
      "epoch": 7.242666666666667,
      "grad_norm": 0.33955928683280945,
      "learning_rate": 4.7333333333333335e-06,
      "loss": 0.0017,
      "step": 135800
    },
    {
      "epoch": 7.2432,
      "grad_norm": 0.31858715415000916,
      "learning_rate": 4.7300000000000005e-06,
      "loss": 0.002,
      "step": 135810
    },
    {
      "epoch": 7.243733333333333,
      "grad_norm": 0.17106620967388153,
      "learning_rate": 4.726666666666667e-06,
      "loss": 0.0014,
      "step": 135820
    },
    {
      "epoch": 7.244266666666666,
      "grad_norm": 0.20745764672756195,
      "learning_rate": 4.7233333333333336e-06,
      "loss": 0.0014,
      "step": 135830
    },
    {
      "epoch": 7.2448,
      "grad_norm": 0.13602286577224731,
      "learning_rate": 4.72e-06,
      "loss": 0.0017,
      "step": 135840
    },
    {
      "epoch": 7.245333333333333,
      "grad_norm": 0.2596675157546997,
      "learning_rate": 4.7166666666666675e-06,
      "loss": 0.002,
      "step": 135850
    },
    {
      "epoch": 7.245866666666666,
      "grad_norm": 0.04513094201683998,
      "learning_rate": 4.713333333333334e-06,
      "loss": 0.0014,
      "step": 135860
    },
    {
      "epoch": 7.2464,
      "grad_norm": 0.12221184372901917,
      "learning_rate": 4.710000000000001e-06,
      "loss": 0.0012,
      "step": 135870
    },
    {
      "epoch": 7.246933333333334,
      "grad_norm": 0.057158149778842926,
      "learning_rate": 4.706666666666667e-06,
      "loss": 0.0018,
      "step": 135880
    },
    {
      "epoch": 7.247466666666667,
      "grad_norm": 0.14365500211715698,
      "learning_rate": 4.703333333333334e-06,
      "loss": 0.0022,
      "step": 135890
    },
    {
      "epoch": 7.248,
      "grad_norm": 0.1732248216867447,
      "learning_rate": 4.7e-06,
      "loss": 0.0013,
      "step": 135900
    },
    {
      "epoch": 7.2485333333333335,
      "grad_norm": 0.09472069144248962,
      "learning_rate": 4.696666666666667e-06,
      "loss": 0.0018,
      "step": 135910
    },
    {
      "epoch": 7.249066666666667,
      "grad_norm": 0.11572372168302536,
      "learning_rate": 4.693333333333334e-06,
      "loss": 0.0023,
      "step": 135920
    },
    {
      "epoch": 7.2496,
      "grad_norm": 0.2278597503900528,
      "learning_rate": 4.69e-06,
      "loss": 0.0012,
      "step": 135930
    },
    {
      "epoch": 7.250133333333333,
      "grad_norm": 0.2288447618484497,
      "learning_rate": 4.686666666666667e-06,
      "loss": 0.0017,
      "step": 135940
    },
    {
      "epoch": 7.250666666666667,
      "grad_norm": 0.25624898076057434,
      "learning_rate": 4.683333333333333e-06,
      "loss": 0.0021,
      "step": 135950
    },
    {
      "epoch": 7.2512,
      "grad_norm": 0.11924602836370468,
      "learning_rate": 4.68e-06,
      "loss": 0.002,
      "step": 135960
    },
    {
      "epoch": 7.251733333333333,
      "grad_norm": 0.23113547265529633,
      "learning_rate": 4.676666666666667e-06,
      "loss": 0.0019,
      "step": 135970
    },
    {
      "epoch": 7.252266666666666,
      "grad_norm": 0.11309788376092911,
      "learning_rate": 4.673333333333334e-06,
      "loss": 0.0024,
      "step": 135980
    },
    {
      "epoch": 7.2528,
      "grad_norm": 0.04311751946806908,
      "learning_rate": 4.67e-06,
      "loss": 0.0019,
      "step": 135990
    },
    {
      "epoch": 7.253333333333333,
      "grad_norm": 0.11874191462993622,
      "learning_rate": 4.666666666666667e-06,
      "loss": 0.0019,
      "step": 136000
    },
    {
      "epoch": 7.253866666666667,
      "grad_norm": 0.21906377375125885,
      "learning_rate": 4.663333333333333e-06,
      "loss": 0.0016,
      "step": 136010
    },
    {
      "epoch": 7.2544,
      "grad_norm": 0.12586092948913574,
      "learning_rate": 4.66e-06,
      "loss": 0.0016,
      "step": 136020
    },
    {
      "epoch": 7.254933333333334,
      "grad_norm": 0.1692652702331543,
      "learning_rate": 4.656666666666666e-06,
      "loss": 0.0016,
      "step": 136030
    },
    {
      "epoch": 7.255466666666667,
      "grad_norm": 0.22972950339317322,
      "learning_rate": 4.653333333333334e-06,
      "loss": 0.0014,
      "step": 136040
    },
    {
      "epoch": 7.256,
      "grad_norm": 0.1452624350786209,
      "learning_rate": 4.65e-06,
      "loss": 0.0021,
      "step": 136050
    },
    {
      "epoch": 7.2565333333333335,
      "grad_norm": 0.314698189496994,
      "learning_rate": 4.646666666666667e-06,
      "loss": 0.0019,
      "step": 136060
    },
    {
      "epoch": 7.257066666666667,
      "grad_norm": 0.021363861858844757,
      "learning_rate": 4.6433333333333335e-06,
      "loss": 0.0018,
      "step": 136070
    },
    {
      "epoch": 7.2576,
      "grad_norm": 0.15018849074840546,
      "learning_rate": 4.64e-06,
      "loss": 0.0017,
      "step": 136080
    },
    {
      "epoch": 7.258133333333333,
      "grad_norm": 0.39377859234809875,
      "learning_rate": 4.636666666666667e-06,
      "loss": 0.0017,
      "step": 136090
    },
    {
      "epoch": 7.258666666666667,
      "grad_norm": 0.2081887274980545,
      "learning_rate": 4.633333333333334e-06,
      "loss": 0.0017,
      "step": 136100
    },
    {
      "epoch": 7.2592,
      "grad_norm": 0.14874550700187683,
      "learning_rate": 4.6300000000000006e-06,
      "loss": 0.0013,
      "step": 136110
    },
    {
      "epoch": 7.259733333333333,
      "grad_norm": 0.12363488227128983,
      "learning_rate": 4.626666666666667e-06,
      "loss": 0.0015,
      "step": 136120
    },
    {
      "epoch": 7.260266666666666,
      "grad_norm": 0.5959334373474121,
      "learning_rate": 4.623333333333334e-06,
      "loss": 0.0018,
      "step": 136130
    },
    {
      "epoch": 7.2608,
      "grad_norm": 0.1980089694261551,
      "learning_rate": 4.62e-06,
      "loss": 0.0022,
      "step": 136140
    },
    {
      "epoch": 7.261333333333333,
      "grad_norm": 0.03451618179678917,
      "learning_rate": 4.616666666666667e-06,
      "loss": 0.0017,
      "step": 136150
    },
    {
      "epoch": 7.261866666666666,
      "grad_norm": 0.033725082874298096,
      "learning_rate": 4.613333333333334e-06,
      "loss": 0.0025,
      "step": 136160
    },
    {
      "epoch": 7.2624,
      "grad_norm": 0.06261343508958817,
      "learning_rate": 4.610000000000001e-06,
      "loss": 0.0015,
      "step": 136170
    },
    {
      "epoch": 7.262933333333334,
      "grad_norm": 0.17711497843265533,
      "learning_rate": 4.606666666666667e-06,
      "loss": 0.0018,
      "step": 136180
    },
    {
      "epoch": 7.263466666666667,
      "grad_norm": 0.14049296081066132,
      "learning_rate": 4.603333333333334e-06,
      "loss": 0.0018,
      "step": 136190
    },
    {
      "epoch": 7.264,
      "grad_norm": 0.28482893109321594,
      "learning_rate": 4.6e-06,
      "loss": 0.0018,
      "step": 136200
    },
    {
      "epoch": 7.2645333333333335,
      "grad_norm": 0.21912208199501038,
      "learning_rate": 4.596666666666667e-06,
      "loss": 0.0017,
      "step": 136210
    },
    {
      "epoch": 7.265066666666667,
      "grad_norm": 0.09611570090055466,
      "learning_rate": 4.593333333333333e-06,
      "loss": 0.0016,
      "step": 136220
    },
    {
      "epoch": 7.2656,
      "grad_norm": 0.26649656891822815,
      "learning_rate": 4.590000000000001e-06,
      "loss": 0.0018,
      "step": 136230
    },
    {
      "epoch": 7.266133333333333,
      "grad_norm": 0.12842561304569244,
      "learning_rate": 4.586666666666667e-06,
      "loss": 0.0016,
      "step": 136240
    },
    {
      "epoch": 7.266666666666667,
      "grad_norm": 0.12694665789604187,
      "learning_rate": 4.583333333333333e-06,
      "loss": 0.0021,
      "step": 136250
    },
    {
      "epoch": 7.2672,
      "grad_norm": 0.37694185972213745,
      "learning_rate": 4.58e-06,
      "loss": 0.0016,
      "step": 136260
    },
    {
      "epoch": 7.267733333333333,
      "grad_norm": 0.25907033681869507,
      "learning_rate": 4.576666666666666e-06,
      "loss": 0.0015,
      "step": 136270
    },
    {
      "epoch": 7.268266666666666,
      "grad_norm": 0.042930688709020615,
      "learning_rate": 4.573333333333333e-06,
      "loss": 0.0017,
      "step": 136280
    },
    {
      "epoch": 7.2688,
      "grad_norm": 0.21105526387691498,
      "learning_rate": 4.57e-06,
      "loss": 0.0015,
      "step": 136290
    },
    {
      "epoch": 7.269333333333333,
      "grad_norm": 0.033218659460544586,
      "learning_rate": 4.566666666666667e-06,
      "loss": 0.0013,
      "step": 136300
    },
    {
      "epoch": 7.269866666666666,
      "grad_norm": 0.06751391291618347,
      "learning_rate": 4.563333333333333e-06,
      "loss": 0.0021,
      "step": 136310
    },
    {
      "epoch": 7.2704,
      "grad_norm": 0.6337417960166931,
      "learning_rate": 4.56e-06,
      "loss": 0.0024,
      "step": 136320
    },
    {
      "epoch": 7.270933333333334,
      "grad_norm": 0.2601322829723358,
      "learning_rate": 4.5566666666666665e-06,
      "loss": 0.0016,
      "step": 136330
    },
    {
      "epoch": 7.271466666666667,
      "grad_norm": 0.5210580825805664,
      "learning_rate": 4.5533333333333335e-06,
      "loss": 0.0016,
      "step": 136340
    },
    {
      "epoch": 7.272,
      "grad_norm": 0.3082944452762604,
      "learning_rate": 4.5500000000000005e-06,
      "loss": 0.0017,
      "step": 136350
    },
    {
      "epoch": 7.2725333333333335,
      "grad_norm": 0.06241542473435402,
      "learning_rate": 4.5466666666666675e-06,
      "loss": 0.0019,
      "step": 136360
    },
    {
      "epoch": 7.273066666666667,
      "grad_norm": 0.0947568267583847,
      "learning_rate": 4.543333333333334e-06,
      "loss": 0.002,
      "step": 136370
    },
    {
      "epoch": 7.2736,
      "grad_norm": 0.1629370003938675,
      "learning_rate": 4.540000000000001e-06,
      "loss": 0.0024,
      "step": 136380
    },
    {
      "epoch": 7.274133333333333,
      "grad_norm": 0.05326893925666809,
      "learning_rate": 4.536666666666667e-06,
      "loss": 0.0021,
      "step": 136390
    },
    {
      "epoch": 7.274666666666667,
      "grad_norm": 0.25439342856407166,
      "learning_rate": 4.533333333333334e-06,
      "loss": 0.0013,
      "step": 136400
    },
    {
      "epoch": 7.2752,
      "grad_norm": 0.19990353286266327,
      "learning_rate": 4.53e-06,
      "loss": 0.0022,
      "step": 136410
    },
    {
      "epoch": 7.275733333333333,
      "grad_norm": 0.1268172562122345,
      "learning_rate": 4.526666666666667e-06,
      "loss": 0.0021,
      "step": 136420
    },
    {
      "epoch": 7.276266666666666,
      "grad_norm": 0.12288343161344528,
      "learning_rate": 4.523333333333334e-06,
      "loss": 0.0016,
      "step": 136430
    },
    {
      "epoch": 7.2768,
      "grad_norm": 0.04691261053085327,
      "learning_rate": 4.52e-06,
      "loss": 0.0014,
      "step": 136440
    },
    {
      "epoch": 7.277333333333333,
      "grad_norm": 0.14434586465358734,
      "learning_rate": 4.516666666666667e-06,
      "loss": 0.0018,
      "step": 136450
    },
    {
      "epoch": 7.277866666666666,
      "grad_norm": 0.20984214544296265,
      "learning_rate": 4.513333333333333e-06,
      "loss": 0.0018,
      "step": 136460
    },
    {
      "epoch": 7.2783999999999995,
      "grad_norm": 0.0338137187063694,
      "learning_rate": 4.51e-06,
      "loss": 0.0025,
      "step": 136470
    },
    {
      "epoch": 7.278933333333334,
      "grad_norm": 0.21230289340019226,
      "learning_rate": 4.506666666666667e-06,
      "loss": 0.0015,
      "step": 136480
    },
    {
      "epoch": 7.279466666666667,
      "grad_norm": 0.06607942283153534,
      "learning_rate": 4.503333333333334e-06,
      "loss": 0.0012,
      "step": 136490
    },
    {
      "epoch": 7.28,
      "grad_norm": 0.25652214884757996,
      "learning_rate": 4.5e-06,
      "loss": 0.0019,
      "step": 136500
    },
    {
      "epoch": 7.2805333333333335,
      "grad_norm": 0.04912484064698219,
      "learning_rate": 4.496666666666667e-06,
      "loss": 0.0019,
      "step": 136510
    },
    {
      "epoch": 7.281066666666667,
      "grad_norm": 0.23069675266742706,
      "learning_rate": 4.493333333333333e-06,
      "loss": 0.0013,
      "step": 136520
    },
    {
      "epoch": 7.2816,
      "grad_norm": 0.040886424481868744,
      "learning_rate": 4.49e-06,
      "loss": 0.0019,
      "step": 136530
    },
    {
      "epoch": 7.282133333333333,
      "grad_norm": 0.1355457305908203,
      "learning_rate": 4.486666666666667e-06,
      "loss": 0.0018,
      "step": 136540
    },
    {
      "epoch": 7.282666666666667,
      "grad_norm": 0.12118791043758392,
      "learning_rate": 4.483333333333334e-06,
      "loss": 0.0014,
      "step": 136550
    },
    {
      "epoch": 7.2832,
      "grad_norm": 0.22649624943733215,
      "learning_rate": 4.48e-06,
      "loss": 0.0025,
      "step": 136560
    },
    {
      "epoch": 7.283733333333333,
      "grad_norm": 0.05336833745241165,
      "learning_rate": 4.476666666666667e-06,
      "loss": 0.0012,
      "step": 136570
    },
    {
      "epoch": 7.2842666666666664,
      "grad_norm": 0.28183513879776,
      "learning_rate": 4.473333333333333e-06,
      "loss": 0.0013,
      "step": 136580
    },
    {
      "epoch": 7.2848,
      "grad_norm": 0.2567494213581085,
      "learning_rate": 4.4699999999999996e-06,
      "loss": 0.0014,
      "step": 136590
    },
    {
      "epoch": 7.285333333333333,
      "grad_norm": 0.09609412401914597,
      "learning_rate": 4.4666666666666665e-06,
      "loss": 0.0015,
      "step": 136600
    },
    {
      "epoch": 7.285866666666666,
      "grad_norm": 0.4242761433124542,
      "learning_rate": 4.4633333333333335e-06,
      "loss": 0.0019,
      "step": 136610
    },
    {
      "epoch": 7.2864,
      "grad_norm": 0.06261444091796875,
      "learning_rate": 4.4600000000000005e-06,
      "loss": 0.0022,
      "step": 136620
    },
    {
      "epoch": 7.286933333333334,
      "grad_norm": 0.17636995017528534,
      "learning_rate": 4.456666666666667e-06,
      "loss": 0.0015,
      "step": 136630
    },
    {
      "epoch": 7.287466666666667,
      "grad_norm": 0.2850683033466339,
      "learning_rate": 4.453333333333334e-06,
      "loss": 0.0013,
      "step": 136640
    },
    {
      "epoch": 7.288,
      "grad_norm": 0.14088045060634613,
      "learning_rate": 4.45e-06,
      "loss": 0.0027,
      "step": 136650
    },
    {
      "epoch": 7.2885333333333335,
      "grad_norm": 0.05607936531305313,
      "learning_rate": 4.446666666666667e-06,
      "loss": 0.0018,
      "step": 136660
    },
    {
      "epoch": 7.289066666666667,
      "grad_norm": 0.24005141854286194,
      "learning_rate": 4.443333333333334e-06,
      "loss": 0.0016,
      "step": 136670
    },
    {
      "epoch": 7.2896,
      "grad_norm": 0.1315193772315979,
      "learning_rate": 4.440000000000001e-06,
      "loss": 0.0019,
      "step": 136680
    },
    {
      "epoch": 7.290133333333333,
      "grad_norm": 0.19847749173641205,
      "learning_rate": 4.436666666666667e-06,
      "loss": 0.0019,
      "step": 136690
    },
    {
      "epoch": 7.290666666666667,
      "grad_norm": 0.2820192277431488,
      "learning_rate": 4.433333333333334e-06,
      "loss": 0.0021,
      "step": 136700
    },
    {
      "epoch": 7.2912,
      "grad_norm": 0.0421619638800621,
      "learning_rate": 4.43e-06,
      "loss": 0.0014,
      "step": 136710
    },
    {
      "epoch": 7.291733333333333,
      "grad_norm": 0.0971224382519722,
      "learning_rate": 4.426666666666667e-06,
      "loss": 0.0025,
      "step": 136720
    },
    {
      "epoch": 7.2922666666666665,
      "grad_norm": 0.05164689943194389,
      "learning_rate": 4.423333333333334e-06,
      "loss": 0.0015,
      "step": 136730
    },
    {
      "epoch": 7.2928,
      "grad_norm": 0.11218617111444473,
      "learning_rate": 4.420000000000001e-06,
      "loss": 0.0024,
      "step": 136740
    },
    {
      "epoch": 7.293333333333333,
      "grad_norm": 0.10367637127637863,
      "learning_rate": 4.416666666666667e-06,
      "loss": 0.0011,
      "step": 136750
    },
    {
      "epoch": 7.293866666666666,
      "grad_norm": 0.22855731844902039,
      "learning_rate": 4.413333333333333e-06,
      "loss": 0.0017,
      "step": 136760
    },
    {
      "epoch": 7.2943999999999996,
      "grad_norm": 0.42224574089050293,
      "learning_rate": 4.41e-06,
      "loss": 0.0012,
      "step": 136770
    },
    {
      "epoch": 7.294933333333334,
      "grad_norm": 0.038030482828617096,
      "learning_rate": 4.406666666666666e-06,
      "loss": 0.0017,
      "step": 136780
    },
    {
      "epoch": 7.295466666666667,
      "grad_norm": 0.15051284432411194,
      "learning_rate": 4.403333333333333e-06,
      "loss": 0.0016,
      "step": 136790
    },
    {
      "epoch": 7.296,
      "grad_norm": 0.36408376693725586,
      "learning_rate": 4.4e-06,
      "loss": 0.0015,
      "step": 136800
    },
    {
      "epoch": 7.2965333333333335,
      "grad_norm": 0.29493582248687744,
      "learning_rate": 4.396666666666667e-06,
      "loss": 0.0018,
      "step": 136810
    },
    {
      "epoch": 7.297066666666667,
      "grad_norm": 0.1984168142080307,
      "learning_rate": 4.393333333333333e-06,
      "loss": 0.0017,
      "step": 136820
    },
    {
      "epoch": 7.2976,
      "grad_norm": 0.11570236831903458,
      "learning_rate": 4.39e-06,
      "loss": 0.0017,
      "step": 136830
    },
    {
      "epoch": 7.298133333333333,
      "grad_norm": 0.20266470313072205,
      "learning_rate": 4.3866666666666665e-06,
      "loss": 0.0016,
      "step": 136840
    },
    {
      "epoch": 7.298666666666667,
      "grad_norm": 0.016810255125164986,
      "learning_rate": 4.3833333333333334e-06,
      "loss": 0.0019,
      "step": 136850
    },
    {
      "epoch": 7.2992,
      "grad_norm": 0.23792868852615356,
      "learning_rate": 4.38e-06,
      "loss": 0.0015,
      "step": 136860
    },
    {
      "epoch": 7.299733333333333,
      "grad_norm": 0.14434058964252472,
      "learning_rate": 4.376666666666667e-06,
      "loss": 0.0022,
      "step": 136870
    },
    {
      "epoch": 7.3002666666666665,
      "grad_norm": 0.2910490930080414,
      "learning_rate": 4.3733333333333335e-06,
      "loss": 0.0015,
      "step": 136880
    },
    {
      "epoch": 7.3008,
      "grad_norm": 0.11668946593999863,
      "learning_rate": 4.3700000000000005e-06,
      "loss": 0.0019,
      "step": 136890
    },
    {
      "epoch": 7.301333333333333,
      "grad_norm": 0.1227179765701294,
      "learning_rate": 4.366666666666667e-06,
      "loss": 0.0014,
      "step": 136900
    },
    {
      "epoch": 7.301866666666666,
      "grad_norm": 0.3095271587371826,
      "learning_rate": 4.363333333333334e-06,
      "loss": 0.0017,
      "step": 136910
    },
    {
      "epoch": 7.3024000000000004,
      "grad_norm": 0.23162174224853516,
      "learning_rate": 4.360000000000001e-06,
      "loss": 0.0024,
      "step": 136920
    },
    {
      "epoch": 7.302933333333334,
      "grad_norm": 0.047407086938619614,
      "learning_rate": 4.356666666666667e-06,
      "loss": 0.0014,
      "step": 136930
    },
    {
      "epoch": 7.303466666666667,
      "grad_norm": 0.3997634947299957,
      "learning_rate": 4.353333333333334e-06,
      "loss": 0.002,
      "step": 136940
    },
    {
      "epoch": 7.304,
      "grad_norm": 0.11922440677881241,
      "learning_rate": 4.35e-06,
      "loss": 0.0013,
      "step": 136950
    },
    {
      "epoch": 7.3045333333333335,
      "grad_norm": 0.053282517939805984,
      "learning_rate": 4.346666666666667e-06,
      "loss": 0.0018,
      "step": 136960
    },
    {
      "epoch": 7.305066666666667,
      "grad_norm": 0.32227376103401184,
      "learning_rate": 4.343333333333333e-06,
      "loss": 0.0025,
      "step": 136970
    },
    {
      "epoch": 7.3056,
      "grad_norm": 0.2260020226240158,
      "learning_rate": 4.34e-06,
      "loss": 0.0015,
      "step": 136980
    },
    {
      "epoch": 7.306133333333333,
      "grad_norm": 0.15154466032981873,
      "learning_rate": 4.336666666666667e-06,
      "loss": 0.0027,
      "step": 136990
    },
    {
      "epoch": 7.306666666666667,
      "grad_norm": 0.14180147647857666,
      "learning_rate": 4.333333333333334e-06,
      "loss": 0.0028,
      "step": 137000
    },
    {
      "epoch": 7.3072,
      "grad_norm": 0.11612594872713089,
      "learning_rate": 4.33e-06,
      "loss": 0.0023,
      "step": 137010
    },
    {
      "epoch": 7.307733333333333,
      "grad_norm": 0.2241453379392624,
      "learning_rate": 4.326666666666667e-06,
      "loss": 0.0019,
      "step": 137020
    },
    {
      "epoch": 7.3082666666666665,
      "grad_norm": 0.03955879807472229,
      "learning_rate": 4.323333333333333e-06,
      "loss": 0.0023,
      "step": 137030
    },
    {
      "epoch": 7.3088,
      "grad_norm": 0.09376540780067444,
      "learning_rate": 4.32e-06,
      "loss": 0.0012,
      "step": 137040
    },
    {
      "epoch": 7.309333333333333,
      "grad_norm": 0.4292963445186615,
      "learning_rate": 4.316666666666667e-06,
      "loss": 0.002,
      "step": 137050
    },
    {
      "epoch": 7.309866666666666,
      "grad_norm": 0.09046969562768936,
      "learning_rate": 4.313333333333334e-06,
      "loss": 0.0018,
      "step": 137060
    },
    {
      "epoch": 7.3104,
      "grad_norm": 0.16570283472537994,
      "learning_rate": 4.31e-06,
      "loss": 0.0016,
      "step": 137070
    },
    {
      "epoch": 7.310933333333334,
      "grad_norm": 0.3661978840827942,
      "learning_rate": 4.306666666666667e-06,
      "loss": 0.0013,
      "step": 137080
    },
    {
      "epoch": 7.311466666666667,
      "grad_norm": 0.18550115823745728,
      "learning_rate": 4.303333333333333e-06,
      "loss": 0.0016,
      "step": 137090
    },
    {
      "epoch": 7.312,
      "grad_norm": 0.07001518458127975,
      "learning_rate": 4.2999999999999995e-06,
      "loss": 0.0019,
      "step": 137100
    },
    {
      "epoch": 7.3125333333333336,
      "grad_norm": 0.11746058613061905,
      "learning_rate": 4.296666666666667e-06,
      "loss": 0.0018,
      "step": 137110
    },
    {
      "epoch": 7.313066666666667,
      "grad_norm": 0.27137619256973267,
      "learning_rate": 4.2933333333333334e-06,
      "loss": 0.0017,
      "step": 137120
    },
    {
      "epoch": 7.3136,
      "grad_norm": 0.1831607073545456,
      "learning_rate": 4.2900000000000004e-06,
      "loss": 0.0016,
      "step": 137130
    },
    {
      "epoch": 7.314133333333333,
      "grad_norm": 0.2272908091545105,
      "learning_rate": 4.2866666666666666e-06,
      "loss": 0.002,
      "step": 137140
    },
    {
      "epoch": 7.314666666666667,
      "grad_norm": 0.225444033741951,
      "learning_rate": 4.2833333333333335e-06,
      "loss": 0.0015,
      "step": 137150
    },
    {
      "epoch": 7.3152,
      "grad_norm": 0.11906125396490097,
      "learning_rate": 4.28e-06,
      "loss": 0.0024,
      "step": 137160
    },
    {
      "epoch": 7.315733333333333,
      "grad_norm": 0.05895101651549339,
      "learning_rate": 4.276666666666667e-06,
      "loss": 0.0014,
      "step": 137170
    },
    {
      "epoch": 7.3162666666666665,
      "grad_norm": 0.067190021276474,
      "learning_rate": 4.273333333333334e-06,
      "loss": 0.0022,
      "step": 137180
    },
    {
      "epoch": 7.3168,
      "grad_norm": 0.17067624628543854,
      "learning_rate": 4.270000000000001e-06,
      "loss": 0.002,
      "step": 137190
    },
    {
      "epoch": 7.317333333333333,
      "grad_norm": 0.15372292697429657,
      "learning_rate": 4.266666666666667e-06,
      "loss": 0.0017,
      "step": 137200
    },
    {
      "epoch": 7.317866666666666,
      "grad_norm": 0.3104642629623413,
      "learning_rate": 4.263333333333334e-06,
      "loss": 0.0018,
      "step": 137210
    },
    {
      "epoch": 7.3184000000000005,
      "grad_norm": 0.31648480892181396,
      "learning_rate": 4.26e-06,
      "loss": 0.0024,
      "step": 137220
    },
    {
      "epoch": 7.318933333333334,
      "grad_norm": 0.3121398687362671,
      "learning_rate": 4.256666666666667e-06,
      "loss": 0.0021,
      "step": 137230
    },
    {
      "epoch": 7.319466666666667,
      "grad_norm": 0.037656113505363464,
      "learning_rate": 4.253333333333334e-06,
      "loss": 0.0021,
      "step": 137240
    },
    {
      "epoch": 7.32,
      "grad_norm": 0.12236406654119492,
      "learning_rate": 4.250000000000001e-06,
      "loss": 0.0024,
      "step": 137250
    },
    {
      "epoch": 7.320533333333334,
      "grad_norm": 0.17183904349803925,
      "learning_rate": 4.246666666666667e-06,
      "loss": 0.0014,
      "step": 137260
    },
    {
      "epoch": 7.321066666666667,
      "grad_norm": 0.3196805715560913,
      "learning_rate": 4.243333333333334e-06,
      "loss": 0.0013,
      "step": 137270
    },
    {
      "epoch": 7.3216,
      "grad_norm": 0.4026777446269989,
      "learning_rate": 4.24e-06,
      "loss": 0.0017,
      "step": 137280
    },
    {
      "epoch": 7.322133333333333,
      "grad_norm": 0.0425446592271328,
      "learning_rate": 4.236666666666666e-06,
      "loss": 0.0017,
      "step": 137290
    },
    {
      "epoch": 7.322666666666667,
      "grad_norm": 0.045243263244628906,
      "learning_rate": 4.233333333333333e-06,
      "loss": 0.0019,
      "step": 137300
    },
    {
      "epoch": 7.3232,
      "grad_norm": 0.08647256344556808,
      "learning_rate": 4.23e-06,
      "loss": 0.0017,
      "step": 137310
    },
    {
      "epoch": 7.323733333333333,
      "grad_norm": 0.06252802163362503,
      "learning_rate": 4.226666666666667e-06,
      "loss": 0.0022,
      "step": 137320
    },
    {
      "epoch": 7.3242666666666665,
      "grad_norm": 0.22666309773921967,
      "learning_rate": 4.223333333333333e-06,
      "loss": 0.0021,
      "step": 137330
    },
    {
      "epoch": 7.3248,
      "grad_norm": 0.14993971586227417,
      "learning_rate": 4.22e-06,
      "loss": 0.0016,
      "step": 137340
    },
    {
      "epoch": 7.325333333333333,
      "grad_norm": 0.369734525680542,
      "learning_rate": 4.216666666666666e-06,
      "loss": 0.0014,
      "step": 137350
    },
    {
      "epoch": 7.325866666666666,
      "grad_norm": 0.08925266563892365,
      "learning_rate": 4.213333333333333e-06,
      "loss": 0.0021,
      "step": 137360
    },
    {
      "epoch": 7.3264,
      "grad_norm": 0.21165701746940613,
      "learning_rate": 4.21e-06,
      "loss": 0.0017,
      "step": 137370
    },
    {
      "epoch": 7.326933333333334,
      "grad_norm": 0.5170998573303223,
      "learning_rate": 4.206666666666667e-06,
      "loss": 0.0013,
      "step": 137380
    },
    {
      "epoch": 7.327466666666667,
      "grad_norm": 0.28217649459838867,
      "learning_rate": 4.2033333333333335e-06,
      "loss": 0.0013,
      "step": 137390
    },
    {
      "epoch": 7.328,
      "grad_norm": 0.03290411829948425,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 0.0013,
      "step": 137400
    },
    {
      "epoch": 7.328533333333334,
      "grad_norm": 0.25695595145225525,
      "learning_rate": 4.196666666666667e-06,
      "loss": 0.0016,
      "step": 137410
    },
    {
      "epoch": 7.329066666666667,
      "grad_norm": 0.09768660366535187,
      "learning_rate": 4.1933333333333336e-06,
      "loss": 0.0019,
      "step": 137420
    },
    {
      "epoch": 7.3296,
      "grad_norm": 0.11469654738903046,
      "learning_rate": 4.1900000000000005e-06,
      "loss": 0.0019,
      "step": 137430
    },
    {
      "epoch": 7.330133333333333,
      "grad_norm": 0.2923544943332672,
      "learning_rate": 4.1866666666666675e-06,
      "loss": 0.0014,
      "step": 137440
    },
    {
      "epoch": 7.330666666666667,
      "grad_norm": 0.17741435766220093,
      "learning_rate": 4.183333333333334e-06,
      "loss": 0.0015,
      "step": 137450
    },
    {
      "epoch": 7.3312,
      "grad_norm": 0.03861990571022034,
      "learning_rate": 4.18e-06,
      "loss": 0.0019,
      "step": 137460
    },
    {
      "epoch": 7.331733333333333,
      "grad_norm": 0.2527158558368683,
      "learning_rate": 4.176666666666667e-06,
      "loss": 0.0014,
      "step": 137470
    },
    {
      "epoch": 7.3322666666666665,
      "grad_norm": 0.23387928307056427,
      "learning_rate": 4.173333333333333e-06,
      "loss": 0.0023,
      "step": 137480
    },
    {
      "epoch": 7.3328,
      "grad_norm": 0.11889606714248657,
      "learning_rate": 4.17e-06,
      "loss": 0.0016,
      "step": 137490
    },
    {
      "epoch": 7.333333333333333,
      "grad_norm": 0.04010439291596413,
      "learning_rate": 4.166666666666667e-06,
      "loss": 0.0017,
      "step": 137500
    },
    {
      "epoch": 7.333866666666666,
      "grad_norm": 0.1155017614364624,
      "learning_rate": 4.163333333333334e-06,
      "loss": 0.0022,
      "step": 137510
    },
    {
      "epoch": 7.3344,
      "grad_norm": 0.09892657399177551,
      "learning_rate": 4.16e-06,
      "loss": 0.0016,
      "step": 137520
    },
    {
      "epoch": 7.334933333333334,
      "grad_norm": 0.03463969752192497,
      "learning_rate": 4.156666666666667e-06,
      "loss": 0.0013,
      "step": 137530
    },
    {
      "epoch": 7.335466666666667,
      "grad_norm": 0.20327691733837128,
      "learning_rate": 4.153333333333333e-06,
      "loss": 0.0018,
      "step": 137540
    },
    {
      "epoch": 7.336,
      "grad_norm": 0.20035922527313232,
      "learning_rate": 4.15e-06,
      "loss": 0.0023,
      "step": 137550
    },
    {
      "epoch": 7.336533333333334,
      "grad_norm": 0.05284356698393822,
      "learning_rate": 4.146666666666667e-06,
      "loss": 0.0014,
      "step": 137560
    },
    {
      "epoch": 7.337066666666667,
      "grad_norm": 0.11432990431785583,
      "learning_rate": 4.143333333333334e-06,
      "loss": 0.0015,
      "step": 137570
    },
    {
      "epoch": 7.3376,
      "grad_norm": 0.1723502278327942,
      "learning_rate": 4.14e-06,
      "loss": 0.0028,
      "step": 137580
    },
    {
      "epoch": 7.338133333333333,
      "grad_norm": 0.09639639407396317,
      "learning_rate": 4.136666666666667e-06,
      "loss": 0.0016,
      "step": 137590
    },
    {
      "epoch": 7.338666666666667,
      "grad_norm": 0.25548091530799866,
      "learning_rate": 4.133333333333333e-06,
      "loss": 0.0014,
      "step": 137600
    },
    {
      "epoch": 7.3392,
      "grad_norm": 0.24026967585086823,
      "learning_rate": 4.13e-06,
      "loss": 0.003,
      "step": 137610
    },
    {
      "epoch": 7.339733333333333,
      "grad_norm": 0.09024898707866669,
      "learning_rate": 4.126666666666667e-06,
      "loss": 0.0012,
      "step": 137620
    },
    {
      "epoch": 7.3402666666666665,
      "grad_norm": 0.04839642345905304,
      "learning_rate": 4.123333333333333e-06,
      "loss": 0.0016,
      "step": 137630
    },
    {
      "epoch": 7.3408,
      "grad_norm": 0.31304731965065,
      "learning_rate": 4.12e-06,
      "loss": 0.0022,
      "step": 137640
    },
    {
      "epoch": 7.341333333333333,
      "grad_norm": 0.16970033943653107,
      "learning_rate": 4.1166666666666665e-06,
      "loss": 0.0014,
      "step": 137650
    },
    {
      "epoch": 7.341866666666666,
      "grad_norm": 0.3108283281326294,
      "learning_rate": 4.1133333333333335e-06,
      "loss": 0.0013,
      "step": 137660
    },
    {
      "epoch": 7.3424,
      "grad_norm": 0.2832258343696594,
      "learning_rate": 4.11e-06,
      "loss": 0.002,
      "step": 137670
    },
    {
      "epoch": 7.342933333333333,
      "grad_norm": 0.17756865918636322,
      "learning_rate": 4.106666666666667e-06,
      "loss": 0.0027,
      "step": 137680
    },
    {
      "epoch": 7.343466666666667,
      "grad_norm": 0.3143141567707062,
      "learning_rate": 4.1033333333333336e-06,
      "loss": 0.0027,
      "step": 137690
    },
    {
      "epoch": 7.344,
      "grad_norm": 0.03978727385401726,
      "learning_rate": 4.1000000000000006e-06,
      "loss": 0.0021,
      "step": 137700
    },
    {
      "epoch": 7.344533333333334,
      "grad_norm": 0.25811201333999634,
      "learning_rate": 4.096666666666667e-06,
      "loss": 0.0022,
      "step": 137710
    },
    {
      "epoch": 7.345066666666667,
      "grad_norm": 0.32596251368522644,
      "learning_rate": 4.093333333333334e-06,
      "loss": 0.0014,
      "step": 137720
    },
    {
      "epoch": 7.3456,
      "grad_norm": 0.043422989547252655,
      "learning_rate": 4.09e-06,
      "loss": 0.0016,
      "step": 137730
    },
    {
      "epoch": 7.346133333333333,
      "grad_norm": 0.151383638381958,
      "learning_rate": 4.086666666666667e-06,
      "loss": 0.0025,
      "step": 137740
    },
    {
      "epoch": 7.346666666666667,
      "grad_norm": 0.12328407913446426,
      "learning_rate": 4.083333333333334e-06,
      "loss": 0.0019,
      "step": 137750
    },
    {
      "epoch": 7.3472,
      "grad_norm": 0.10922127217054367,
      "learning_rate": 4.080000000000001e-06,
      "loss": 0.002,
      "step": 137760
    },
    {
      "epoch": 7.347733333333333,
      "grad_norm": 0.1138279065489769,
      "learning_rate": 4.076666666666667e-06,
      "loss": 0.0018,
      "step": 137770
    },
    {
      "epoch": 7.3482666666666665,
      "grad_norm": 0.08658632636070251,
      "learning_rate": 4.073333333333334e-06,
      "loss": 0.0018,
      "step": 137780
    },
    {
      "epoch": 7.3488,
      "grad_norm": 0.0697871744632721,
      "learning_rate": 4.07e-06,
      "loss": 0.0025,
      "step": 137790
    },
    {
      "epoch": 7.349333333333333,
      "grad_norm": 0.043266817927360535,
      "learning_rate": 4.066666666666666e-06,
      "loss": 0.0017,
      "step": 137800
    },
    {
      "epoch": 7.349866666666666,
      "grad_norm": 0.17152899503707886,
      "learning_rate": 4.063333333333334e-06,
      "loss": 0.0018,
      "step": 137810
    },
    {
      "epoch": 7.3504,
      "grad_norm": 0.054854415357112885,
      "learning_rate": 4.06e-06,
      "loss": 0.0018,
      "step": 137820
    },
    {
      "epoch": 7.350933333333334,
      "grad_norm": 0.1950126737356186,
      "learning_rate": 4.056666666666667e-06,
      "loss": 0.0018,
      "step": 137830
    },
    {
      "epoch": 7.351466666666667,
      "grad_norm": 0.048780445009469986,
      "learning_rate": 4.053333333333333e-06,
      "loss": 0.0014,
      "step": 137840
    },
    {
      "epoch": 7.352,
      "grad_norm": 0.05953172594308853,
      "learning_rate": 4.05e-06,
      "loss": 0.0019,
      "step": 137850
    },
    {
      "epoch": 7.352533333333334,
      "grad_norm": 0.06998039782047272,
      "learning_rate": 4.046666666666666e-06,
      "loss": 0.0015,
      "step": 137860
    },
    {
      "epoch": 7.353066666666667,
      "grad_norm": 0.24214014410972595,
      "learning_rate": 4.043333333333333e-06,
      "loss": 0.0017,
      "step": 137870
    },
    {
      "epoch": 7.3536,
      "grad_norm": 0.42509210109710693,
      "learning_rate": 4.04e-06,
      "loss": 0.0017,
      "step": 137880
    },
    {
      "epoch": 7.354133333333333,
      "grad_norm": 0.17723916471004486,
      "learning_rate": 4.036666666666667e-06,
      "loss": 0.003,
      "step": 137890
    },
    {
      "epoch": 7.354666666666667,
      "grad_norm": 0.30965328216552734,
      "learning_rate": 4.033333333333333e-06,
      "loss": 0.0018,
      "step": 137900
    },
    {
      "epoch": 7.3552,
      "grad_norm": 0.31043535470962524,
      "learning_rate": 4.03e-06,
      "loss": 0.002,
      "step": 137910
    },
    {
      "epoch": 7.355733333333333,
      "grad_norm": 0.23532888293266296,
      "learning_rate": 4.0266666666666665e-06,
      "loss": 0.0025,
      "step": 137920
    },
    {
      "epoch": 7.3562666666666665,
      "grad_norm": 0.036339856684207916,
      "learning_rate": 4.0233333333333335e-06,
      "loss": 0.0015,
      "step": 137930
    },
    {
      "epoch": 7.3568,
      "grad_norm": 0.2099883258342743,
      "learning_rate": 4.0200000000000005e-06,
      "loss": 0.0019,
      "step": 137940
    },
    {
      "epoch": 7.357333333333333,
      "grad_norm": 0.4261288642883301,
      "learning_rate": 4.0166666666666675e-06,
      "loss": 0.0026,
      "step": 137950
    },
    {
      "epoch": 7.357866666666666,
      "grad_norm": 0.09226391464471817,
      "learning_rate": 4.013333333333334e-06,
      "loss": 0.0015,
      "step": 137960
    },
    {
      "epoch": 7.3584,
      "grad_norm": 0.2029609978199005,
      "learning_rate": 4.01e-06,
      "loss": 0.002,
      "step": 137970
    },
    {
      "epoch": 7.358933333333333,
      "grad_norm": 0.1799260973930359,
      "learning_rate": 4.006666666666667e-06,
      "loss": 0.0019,
      "step": 137980
    },
    {
      "epoch": 7.359466666666667,
      "grad_norm": 0.11566026508808136,
      "learning_rate": 4.003333333333333e-06,
      "loss": 0.0013,
      "step": 137990
    },
    {
      "epoch": 7.36,
      "grad_norm": 0.22752071917057037,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.0024,
      "step": 138000
    },
    {
      "epoch": 7.360533333333334,
      "grad_norm": 0.2398456484079361,
      "learning_rate": 3.996666666666667e-06,
      "loss": 0.0022,
      "step": 138010
    },
    {
      "epoch": 7.361066666666667,
      "grad_norm": 0.36787843704223633,
      "learning_rate": 3.993333333333334e-06,
      "loss": 0.0018,
      "step": 138020
    },
    {
      "epoch": 7.3616,
      "grad_norm": 0.24383419752120972,
      "learning_rate": 3.99e-06,
      "loss": 0.0018,
      "step": 138030
    },
    {
      "epoch": 7.362133333333333,
      "grad_norm": 0.22649268805980682,
      "learning_rate": 3.986666666666667e-06,
      "loss": 0.0025,
      "step": 138040
    },
    {
      "epoch": 7.362666666666667,
      "grad_norm": 0.06289779394865036,
      "learning_rate": 3.983333333333333e-06,
      "loss": 0.0015,
      "step": 138050
    },
    {
      "epoch": 7.3632,
      "grad_norm": 0.3610422611236572,
      "learning_rate": 3.98e-06,
      "loss": 0.0018,
      "step": 138060
    },
    {
      "epoch": 7.363733333333333,
      "grad_norm": 0.16122199594974518,
      "learning_rate": 3.976666666666667e-06,
      "loss": 0.0013,
      "step": 138070
    },
    {
      "epoch": 7.3642666666666665,
      "grad_norm": 0.14185167849063873,
      "learning_rate": 3.973333333333334e-06,
      "loss": 0.0025,
      "step": 138080
    },
    {
      "epoch": 7.3648,
      "grad_norm": 0.06049633398652077,
      "learning_rate": 3.97e-06,
      "loss": 0.0013,
      "step": 138090
    },
    {
      "epoch": 7.365333333333333,
      "grad_norm": 0.04120738059282303,
      "learning_rate": 3.966666666666667e-06,
      "loss": 0.0032,
      "step": 138100
    },
    {
      "epoch": 7.365866666666666,
      "grad_norm": 0.07698686420917511,
      "learning_rate": 3.963333333333333e-06,
      "loss": 0.0016,
      "step": 138110
    },
    {
      "epoch": 7.3664,
      "grad_norm": 0.0718933716416359,
      "learning_rate": 3.96e-06,
      "loss": 0.0013,
      "step": 138120
    },
    {
      "epoch": 7.366933333333334,
      "grad_norm": 0.2874516248703003,
      "learning_rate": 3.956666666666667e-06,
      "loss": 0.0021,
      "step": 138130
    },
    {
      "epoch": 7.367466666666667,
      "grad_norm": 0.0649096816778183,
      "learning_rate": 3.953333333333333e-06,
      "loss": 0.0012,
      "step": 138140
    },
    {
      "epoch": 7.368,
      "grad_norm": 0.2545955181121826,
      "learning_rate": 3.95e-06,
      "loss": 0.0014,
      "step": 138150
    },
    {
      "epoch": 7.368533333333334,
      "grad_norm": 0.23029382526874542,
      "learning_rate": 3.9466666666666664e-06,
      "loss": 0.0019,
      "step": 138160
    },
    {
      "epoch": 7.369066666666667,
      "grad_norm": 0.22590677440166473,
      "learning_rate": 3.943333333333333e-06,
      "loss": 0.0015,
      "step": 138170
    },
    {
      "epoch": 7.3696,
      "grad_norm": 0.2799568176269531,
      "learning_rate": 3.9399999999999995e-06,
      "loss": 0.0014,
      "step": 138180
    },
    {
      "epoch": 7.370133333333333,
      "grad_norm": 0.1980096399784088,
      "learning_rate": 3.936666666666667e-06,
      "loss": 0.0015,
      "step": 138190
    },
    {
      "epoch": 7.370666666666667,
      "grad_norm": 0.4830654263496399,
      "learning_rate": 3.9333333333333335e-06,
      "loss": 0.0019,
      "step": 138200
    },
    {
      "epoch": 7.3712,
      "grad_norm": 0.14536228775978088,
      "learning_rate": 3.9300000000000005e-06,
      "loss": 0.0021,
      "step": 138210
    },
    {
      "epoch": 7.371733333333333,
      "grad_norm": 0.28877004981040955,
      "learning_rate": 3.926666666666667e-06,
      "loss": 0.0019,
      "step": 138220
    },
    {
      "epoch": 7.3722666666666665,
      "grad_norm": 0.059031594544649124,
      "learning_rate": 3.923333333333334e-06,
      "loss": 0.0021,
      "step": 138230
    },
    {
      "epoch": 7.3728,
      "grad_norm": 0.050868041813373566,
      "learning_rate": 3.92e-06,
      "loss": 0.002,
      "step": 138240
    },
    {
      "epoch": 7.373333333333333,
      "grad_norm": 0.0685211792588234,
      "learning_rate": 3.916666666666667e-06,
      "loss": 0.0023,
      "step": 138250
    },
    {
      "epoch": 7.373866666666666,
      "grad_norm": 0.14511381089687347,
      "learning_rate": 3.913333333333334e-06,
      "loss": 0.0013,
      "step": 138260
    },
    {
      "epoch": 7.3744,
      "grad_norm": 0.04471702501177788,
      "learning_rate": 3.910000000000001e-06,
      "loss": 0.0018,
      "step": 138270
    },
    {
      "epoch": 7.374933333333333,
      "grad_norm": 0.22476355731487274,
      "learning_rate": 3.906666666666667e-06,
      "loss": 0.0026,
      "step": 138280
    },
    {
      "epoch": 7.375466666666667,
      "grad_norm": 0.24008411169052124,
      "learning_rate": 3.903333333333334e-06,
      "loss": 0.0014,
      "step": 138290
    },
    {
      "epoch": 7.376,
      "grad_norm": 0.195425346493721,
      "learning_rate": 3.9e-06,
      "loss": 0.0026,
      "step": 138300
    },
    {
      "epoch": 7.376533333333334,
      "grad_norm": 0.1897663176059723,
      "learning_rate": 3.896666666666667e-06,
      "loss": 0.0019,
      "step": 138310
    },
    {
      "epoch": 7.377066666666667,
      "grad_norm": 0.22878871858119965,
      "learning_rate": 3.893333333333334e-06,
      "loss": 0.0016,
      "step": 138320
    },
    {
      "epoch": 7.3776,
      "grad_norm": 0.39705824851989746,
      "learning_rate": 3.89e-06,
      "loss": 0.0021,
      "step": 138330
    },
    {
      "epoch": 7.378133333333333,
      "grad_norm": 0.22771689295768738,
      "learning_rate": 3.886666666666667e-06,
      "loss": 0.0015,
      "step": 138340
    },
    {
      "epoch": 7.378666666666667,
      "grad_norm": 0.2684570848941803,
      "learning_rate": 3.883333333333333e-06,
      "loss": 0.002,
      "step": 138350
    },
    {
      "epoch": 7.3792,
      "grad_norm": 0.11931585520505905,
      "learning_rate": 3.88e-06,
      "loss": 0.0021,
      "step": 138360
    },
    {
      "epoch": 7.379733333333333,
      "grad_norm": 0.14938746392726898,
      "learning_rate": 3.876666666666666e-06,
      "loss": 0.0013,
      "step": 138370
    },
    {
      "epoch": 7.3802666666666665,
      "grad_norm": 0.4414355158805847,
      "learning_rate": 3.873333333333334e-06,
      "loss": 0.0013,
      "step": 138380
    },
    {
      "epoch": 7.3808,
      "grad_norm": 0.07941260188817978,
      "learning_rate": 3.87e-06,
      "loss": 0.0015,
      "step": 138390
    },
    {
      "epoch": 7.381333333333333,
      "grad_norm": 0.0498092882335186,
      "learning_rate": 3.866666666666667e-06,
      "loss": 0.0013,
      "step": 138400
    },
    {
      "epoch": 7.381866666666666,
      "grad_norm": 0.44650471210479736,
      "learning_rate": 3.863333333333333e-06,
      "loss": 0.0014,
      "step": 138410
    },
    {
      "epoch": 7.3824,
      "grad_norm": 0.20448081195354462,
      "learning_rate": 3.86e-06,
      "loss": 0.0013,
      "step": 138420
    },
    {
      "epoch": 7.382933333333334,
      "grad_norm": 0.2441645860671997,
      "learning_rate": 3.8566666666666664e-06,
      "loss": 0.0014,
      "step": 138430
    },
    {
      "epoch": 7.383466666666667,
      "grad_norm": 0.3158087432384491,
      "learning_rate": 3.8533333333333334e-06,
      "loss": 0.0016,
      "step": 138440
    },
    {
      "epoch": 7.384,
      "grad_norm": 0.04226497933268547,
      "learning_rate": 3.85e-06,
      "loss": 0.0015,
      "step": 138450
    },
    {
      "epoch": 7.384533333333334,
      "grad_norm": 0.1269802302122116,
      "learning_rate": 3.846666666666667e-06,
      "loss": 0.0018,
      "step": 138460
    },
    {
      "epoch": 7.385066666666667,
      "grad_norm": 0.04719441011548042,
      "learning_rate": 3.8433333333333335e-06,
      "loss": 0.0019,
      "step": 138470
    },
    {
      "epoch": 7.3856,
      "grad_norm": 0.07144992053508759,
      "learning_rate": 3.84e-06,
      "loss": 0.0019,
      "step": 138480
    },
    {
      "epoch": 7.386133333333333,
      "grad_norm": 0.22952771186828613,
      "learning_rate": 3.836666666666667e-06,
      "loss": 0.0025,
      "step": 138490
    },
    {
      "epoch": 7.386666666666667,
      "grad_norm": 0.11823493242263794,
      "learning_rate": 3.833333333333334e-06,
      "loss": 0.002,
      "step": 138500
    },
    {
      "epoch": 7.3872,
      "grad_norm": 0.11732709407806396,
      "learning_rate": 3.830000000000001e-06,
      "loss": 0.0015,
      "step": 138510
    },
    {
      "epoch": 7.387733333333333,
      "grad_norm": 0.5284388661384583,
      "learning_rate": 3.826666666666667e-06,
      "loss": 0.0027,
      "step": 138520
    },
    {
      "epoch": 7.3882666666666665,
      "grad_norm": 0.259987473487854,
      "learning_rate": 3.823333333333334e-06,
      "loss": 0.002,
      "step": 138530
    },
    {
      "epoch": 7.3888,
      "grad_norm": 0.42188820242881775,
      "learning_rate": 3.82e-06,
      "loss": 0.0019,
      "step": 138540
    },
    {
      "epoch": 7.389333333333333,
      "grad_norm": 0.08929194509983063,
      "learning_rate": 3.816666666666667e-06,
      "loss": 0.0017,
      "step": 138550
    },
    {
      "epoch": 7.389866666666666,
      "grad_norm": 0.2611883878707886,
      "learning_rate": 3.8133333333333334e-06,
      "loss": 0.0016,
      "step": 138560
    },
    {
      "epoch": 7.3904,
      "grad_norm": 0.3362094759941101,
      "learning_rate": 3.8100000000000004e-06,
      "loss": 0.0018,
      "step": 138570
    },
    {
      "epoch": 7.390933333333333,
      "grad_norm": 0.12010976672172546,
      "learning_rate": 3.806666666666667e-06,
      "loss": 0.0017,
      "step": 138580
    },
    {
      "epoch": 7.391466666666667,
      "grad_norm": 0.2897246181964874,
      "learning_rate": 3.803333333333334e-06,
      "loss": 0.0023,
      "step": 138590
    },
    {
      "epoch": 7.392,
      "grad_norm": 0.14648598432540894,
      "learning_rate": 3.8e-06,
      "loss": 0.0024,
      "step": 138600
    },
    {
      "epoch": 7.392533333333334,
      "grad_norm": 0.2326153814792633,
      "learning_rate": 3.796666666666667e-06,
      "loss": 0.0016,
      "step": 138610
    },
    {
      "epoch": 7.393066666666667,
      "grad_norm": 0.09238748252391815,
      "learning_rate": 3.7933333333333336e-06,
      "loss": 0.0024,
      "step": 138620
    },
    {
      "epoch": 7.3936,
      "grad_norm": 0.17615722119808197,
      "learning_rate": 3.7900000000000006e-06,
      "loss": 0.0014,
      "step": 138630
    },
    {
      "epoch": 7.3941333333333334,
      "grad_norm": 0.312963604927063,
      "learning_rate": 3.7866666666666667e-06,
      "loss": 0.0013,
      "step": 138640
    },
    {
      "epoch": 7.394666666666667,
      "grad_norm": 0.06654506921768188,
      "learning_rate": 3.7833333333333333e-06,
      "loss": 0.0022,
      "step": 138650
    },
    {
      "epoch": 7.3952,
      "grad_norm": 0.17315080761909485,
      "learning_rate": 3.7800000000000002e-06,
      "loss": 0.0028,
      "step": 138660
    },
    {
      "epoch": 7.395733333333333,
      "grad_norm": 0.25352856516838074,
      "learning_rate": 3.7766666666666664e-06,
      "loss": 0.0015,
      "step": 138670
    },
    {
      "epoch": 7.3962666666666665,
      "grad_norm": 0.11670685559511185,
      "learning_rate": 3.7733333333333338e-06,
      "loss": 0.0021,
      "step": 138680
    },
    {
      "epoch": 7.3968,
      "grad_norm": 0.22818917036056519,
      "learning_rate": 3.77e-06,
      "loss": 0.0019,
      "step": 138690
    },
    {
      "epoch": 7.397333333333333,
      "grad_norm": 0.1697375774383545,
      "learning_rate": 3.766666666666667e-06,
      "loss": 0.0017,
      "step": 138700
    },
    {
      "epoch": 7.397866666666666,
      "grad_norm": 0.28496018052101135,
      "learning_rate": 3.7633333333333334e-06,
      "loss": 0.0016,
      "step": 138710
    },
    {
      "epoch": 7.3984,
      "grad_norm": 0.17904049158096313,
      "learning_rate": 3.7600000000000004e-06,
      "loss": 0.0016,
      "step": 138720
    },
    {
      "epoch": 7.398933333333333,
      "grad_norm": 0.1495533585548401,
      "learning_rate": 3.7566666666666666e-06,
      "loss": 0.003,
      "step": 138730
    },
    {
      "epoch": 7.399466666666667,
      "grad_norm": 0.14533816277980804,
      "learning_rate": 3.7533333333333335e-06,
      "loss": 0.0018,
      "step": 138740
    },
    {
      "epoch": 7.4,
      "grad_norm": 0.22594623267650604,
      "learning_rate": 3.75e-06,
      "loss": 0.0017,
      "step": 138750
    },
    {
      "epoch": 7.400533333333334,
      "grad_norm": 0.06476365774869919,
      "learning_rate": 3.746666666666667e-06,
      "loss": 0.0022,
      "step": 138760
    },
    {
      "epoch": 7.401066666666667,
      "grad_norm": 0.19319523870944977,
      "learning_rate": 3.743333333333333e-06,
      "loss": 0.0015,
      "step": 138770
    },
    {
      "epoch": 7.4016,
      "grad_norm": 0.2254907190799713,
      "learning_rate": 3.7400000000000006e-06,
      "loss": 0.0017,
      "step": 138780
    },
    {
      "epoch": 7.4021333333333335,
      "grad_norm": 0.06847268342971802,
      "learning_rate": 3.7366666666666667e-06,
      "loss": 0.0022,
      "step": 138790
    },
    {
      "epoch": 7.402666666666667,
      "grad_norm": 0.22727210819721222,
      "learning_rate": 3.7333333333333337e-06,
      "loss": 0.0014,
      "step": 138800
    },
    {
      "epoch": 7.4032,
      "grad_norm": 0.25290054082870483,
      "learning_rate": 3.7300000000000003e-06,
      "loss": 0.002,
      "step": 138810
    },
    {
      "epoch": 7.403733333333333,
      "grad_norm": 0.2930172085762024,
      "learning_rate": 3.7266666666666664e-06,
      "loss": 0.0021,
      "step": 138820
    },
    {
      "epoch": 7.4042666666666666,
      "grad_norm": 0.26108723878860474,
      "learning_rate": 3.7233333333333334e-06,
      "loss": 0.0021,
      "step": 138830
    },
    {
      "epoch": 7.4048,
      "grad_norm": 0.08810942620038986,
      "learning_rate": 3.72e-06,
      "loss": 0.0017,
      "step": 138840
    },
    {
      "epoch": 7.405333333333333,
      "grad_norm": 0.09347791224718094,
      "learning_rate": 3.716666666666667e-06,
      "loss": 0.0015,
      "step": 138850
    },
    {
      "epoch": 7.405866666666666,
      "grad_norm": 0.4290972650051117,
      "learning_rate": 3.713333333333333e-06,
      "loss": 0.0023,
      "step": 138860
    },
    {
      "epoch": 7.4064,
      "grad_norm": 0.34357547760009766,
      "learning_rate": 3.7100000000000005e-06,
      "loss": 0.0018,
      "step": 138870
    },
    {
      "epoch": 7.406933333333333,
      "grad_norm": 0.1746862232685089,
      "learning_rate": 3.7066666666666666e-06,
      "loss": 0.0015,
      "step": 138880
    },
    {
      "epoch": 7.407466666666666,
      "grad_norm": 0.24376115202903748,
      "learning_rate": 3.7033333333333336e-06,
      "loss": 0.0029,
      "step": 138890
    },
    {
      "epoch": 7.408,
      "grad_norm": 0.2602646052837372,
      "learning_rate": 3.7e-06,
      "loss": 0.0014,
      "step": 138900
    },
    {
      "epoch": 7.408533333333334,
      "grad_norm": 0.05766601115465164,
      "learning_rate": 3.696666666666667e-06,
      "loss": 0.0015,
      "step": 138910
    },
    {
      "epoch": 7.409066666666667,
      "grad_norm": 0.04264486953616142,
      "learning_rate": 3.6933333333333333e-06,
      "loss": 0.0019,
      "step": 138920
    },
    {
      "epoch": 7.4096,
      "grad_norm": 0.5114187598228455,
      "learning_rate": 3.6900000000000002e-06,
      "loss": 0.0017,
      "step": 138930
    },
    {
      "epoch": 7.4101333333333335,
      "grad_norm": 0.08728231489658356,
      "learning_rate": 3.686666666666667e-06,
      "loss": 0.0017,
      "step": 138940
    },
    {
      "epoch": 7.410666666666667,
      "grad_norm": 0.22047416865825653,
      "learning_rate": 3.6833333333333338e-06,
      "loss": 0.0027,
      "step": 138950
    },
    {
      "epoch": 7.4112,
      "grad_norm": 0.14517098665237427,
      "learning_rate": 3.68e-06,
      "loss": 0.0015,
      "step": 138960
    },
    {
      "epoch": 7.411733333333333,
      "grad_norm": 0.20126470923423767,
      "learning_rate": 3.6766666666666673e-06,
      "loss": 0.0027,
      "step": 138970
    },
    {
      "epoch": 7.412266666666667,
      "grad_norm": 0.09338609874248505,
      "learning_rate": 3.6733333333333335e-06,
      "loss": 0.0021,
      "step": 138980
    },
    {
      "epoch": 7.4128,
      "grad_norm": 0.08834894001483917,
      "learning_rate": 3.6700000000000004e-06,
      "loss": 0.0022,
      "step": 138990
    },
    {
      "epoch": 7.413333333333333,
      "grad_norm": 0.06730858981609344,
      "learning_rate": 3.666666666666667e-06,
      "loss": 0.0019,
      "step": 139000
    },
    {
      "epoch": 7.413866666666666,
      "grad_norm": 0.0730699747800827,
      "learning_rate": 3.663333333333333e-06,
      "loss": 0.0017,
      "step": 139010
    },
    {
      "epoch": 7.4144,
      "grad_norm": 0.14284512400627136,
      "learning_rate": 3.66e-06,
      "loss": 0.0022,
      "step": 139020
    },
    {
      "epoch": 7.414933333333333,
      "grad_norm": 0.036925870925188065,
      "learning_rate": 3.6566666666666667e-06,
      "loss": 0.0013,
      "step": 139030
    },
    {
      "epoch": 7.415466666666667,
      "grad_norm": 0.15001942217350006,
      "learning_rate": 3.6533333333333336e-06,
      "loss": 0.0015,
      "step": 139040
    },
    {
      "epoch": 7.416,
      "grad_norm": 0.2961452603340149,
      "learning_rate": 3.6499999999999998e-06,
      "loss": 0.0015,
      "step": 139050
    },
    {
      "epoch": 7.416533333333334,
      "grad_norm": 0.23386798799037933,
      "learning_rate": 3.646666666666667e-06,
      "loss": 0.0013,
      "step": 139060
    },
    {
      "epoch": 7.417066666666667,
      "grad_norm": 0.08777563273906708,
      "learning_rate": 3.6433333333333333e-06,
      "loss": 0.002,
      "step": 139070
    },
    {
      "epoch": 7.4176,
      "grad_norm": 0.28204262256622314,
      "learning_rate": 3.6400000000000003e-06,
      "loss": 0.0018,
      "step": 139080
    },
    {
      "epoch": 7.4181333333333335,
      "grad_norm": 0.2605137228965759,
      "learning_rate": 3.636666666666667e-06,
      "loss": 0.0024,
      "step": 139090
    },
    {
      "epoch": 7.418666666666667,
      "grad_norm": 0.25256478786468506,
      "learning_rate": 3.633333333333334e-06,
      "loss": 0.0017,
      "step": 139100
    },
    {
      "epoch": 7.4192,
      "grad_norm": 0.3227020800113678,
      "learning_rate": 3.63e-06,
      "loss": 0.0023,
      "step": 139110
    },
    {
      "epoch": 7.419733333333333,
      "grad_norm": 0.04560329392552376,
      "learning_rate": 3.626666666666667e-06,
      "loss": 0.0014,
      "step": 139120
    },
    {
      "epoch": 7.420266666666667,
      "grad_norm": 0.1716415137052536,
      "learning_rate": 3.6233333333333335e-06,
      "loss": 0.0016,
      "step": 139130
    },
    {
      "epoch": 7.4208,
      "grad_norm": 0.22580760717391968,
      "learning_rate": 3.6200000000000005e-06,
      "loss": 0.0014,
      "step": 139140
    },
    {
      "epoch": 7.421333333333333,
      "grad_norm": 0.33017802238464355,
      "learning_rate": 3.6166666666666666e-06,
      "loss": 0.0015,
      "step": 139150
    },
    {
      "epoch": 7.421866666666666,
      "grad_norm": 0.3744763731956482,
      "learning_rate": 3.613333333333334e-06,
      "loss": 0.0014,
      "step": 139160
    },
    {
      "epoch": 7.4224,
      "grad_norm": 0.41074955463409424,
      "learning_rate": 3.61e-06,
      "loss": 0.0019,
      "step": 139170
    },
    {
      "epoch": 7.422933333333333,
      "grad_norm": 0.28802961111068726,
      "learning_rate": 3.6066666666666667e-06,
      "loss": 0.0014,
      "step": 139180
    },
    {
      "epoch": 7.423466666666666,
      "grad_norm": 0.1538793444633484,
      "learning_rate": 3.6033333333333337e-06,
      "loss": 0.0015,
      "step": 139190
    },
    {
      "epoch": 7.424,
      "grad_norm": 0.27400001883506775,
      "learning_rate": 3.6e-06,
      "loss": 0.0013,
      "step": 139200
    },
    {
      "epoch": 7.424533333333334,
      "grad_norm": 0.04470910504460335,
      "learning_rate": 3.596666666666667e-06,
      "loss": 0.0021,
      "step": 139210
    },
    {
      "epoch": 7.425066666666667,
      "grad_norm": 0.04374315217137337,
      "learning_rate": 3.5933333333333334e-06,
      "loss": 0.0019,
      "step": 139220
    },
    {
      "epoch": 7.4256,
      "grad_norm": 0.14422239363193512,
      "learning_rate": 3.5900000000000004e-06,
      "loss": 0.0016,
      "step": 139230
    },
    {
      "epoch": 7.4261333333333335,
      "grad_norm": 0.051526207476854324,
      "learning_rate": 3.5866666666666665e-06,
      "loss": 0.0033,
      "step": 139240
    },
    {
      "epoch": 7.426666666666667,
      "grad_norm": 0.11372464150190353,
      "learning_rate": 3.5833333333333335e-06,
      "loss": 0.0034,
      "step": 139250
    },
    {
      "epoch": 7.4272,
      "grad_norm": 0.45137664675712585,
      "learning_rate": 3.58e-06,
      "loss": 0.0012,
      "step": 139260
    },
    {
      "epoch": 7.427733333333333,
      "grad_norm": 0.14457252621650696,
      "learning_rate": 3.576666666666667e-06,
      "loss": 0.0016,
      "step": 139270
    },
    {
      "epoch": 7.428266666666667,
      "grad_norm": 0.7321865558624268,
      "learning_rate": 3.5733333333333336e-06,
      "loss": 0.002,
      "step": 139280
    },
    {
      "epoch": 7.4288,
      "grad_norm": 0.0862031802535057,
      "learning_rate": 3.5700000000000005e-06,
      "loss": 0.0015,
      "step": 139290
    },
    {
      "epoch": 7.429333333333333,
      "grad_norm": 0.17456093430519104,
      "learning_rate": 3.5666666666666667e-06,
      "loss": 0.0018,
      "step": 139300
    },
    {
      "epoch": 7.429866666666666,
      "grad_norm": 0.38060030341148376,
      "learning_rate": 3.5633333333333337e-06,
      "loss": 0.0013,
      "step": 139310
    },
    {
      "epoch": 7.4304,
      "grad_norm": 0.07365158200263977,
      "learning_rate": 3.5600000000000002e-06,
      "loss": 0.0017,
      "step": 139320
    },
    {
      "epoch": 7.430933333333333,
      "grad_norm": 0.23558343946933746,
      "learning_rate": 3.556666666666667e-06,
      "loss": 0.0019,
      "step": 139330
    },
    {
      "epoch": 7.431466666666667,
      "grad_norm": 0.0948440358042717,
      "learning_rate": 3.5533333333333333e-06,
      "loss": 0.0023,
      "step": 139340
    },
    {
      "epoch": 7.432,
      "grad_norm": 0.27255675196647644,
      "learning_rate": 3.55e-06,
      "loss": 0.0019,
      "step": 139350
    },
    {
      "epoch": 7.432533333333334,
      "grad_norm": 0.31394538283348083,
      "learning_rate": 3.546666666666667e-06,
      "loss": 0.0016,
      "step": 139360
    },
    {
      "epoch": 7.433066666666667,
      "grad_norm": 0.12021196633577347,
      "learning_rate": 3.5433333333333334e-06,
      "loss": 0.0017,
      "step": 139370
    },
    {
      "epoch": 7.4336,
      "grad_norm": 0.10251422226428986,
      "learning_rate": 3.5400000000000004e-06,
      "loss": 0.0012,
      "step": 139380
    },
    {
      "epoch": 7.4341333333333335,
      "grad_norm": 0.20316241681575775,
      "learning_rate": 3.5366666666666665e-06,
      "loss": 0.0021,
      "step": 139390
    },
    {
      "epoch": 7.434666666666667,
      "grad_norm": 0.023245161399245262,
      "learning_rate": 3.5333333333333335e-06,
      "loss": 0.0021,
      "step": 139400
    },
    {
      "epoch": 7.4352,
      "grad_norm": 0.16519157588481903,
      "learning_rate": 3.53e-06,
      "loss": 0.0018,
      "step": 139410
    },
    {
      "epoch": 7.435733333333333,
      "grad_norm": 0.11391855776309967,
      "learning_rate": 3.526666666666667e-06,
      "loss": 0.0015,
      "step": 139420
    },
    {
      "epoch": 7.436266666666667,
      "grad_norm": 0.17231322824954987,
      "learning_rate": 3.523333333333333e-06,
      "loss": 0.0016,
      "step": 139430
    },
    {
      "epoch": 7.4368,
      "grad_norm": 0.37275993824005127,
      "learning_rate": 3.52e-06,
      "loss": 0.0015,
      "step": 139440
    },
    {
      "epoch": 7.437333333333333,
      "grad_norm": 0.2327660173177719,
      "learning_rate": 3.5166666666666667e-06,
      "loss": 0.0019,
      "step": 139450
    },
    {
      "epoch": 7.437866666666666,
      "grad_norm": 0.1724717617034912,
      "learning_rate": 3.5133333333333337e-06,
      "loss": 0.0026,
      "step": 139460
    },
    {
      "epoch": 7.4384,
      "grad_norm": 0.06236505135893822,
      "learning_rate": 3.5100000000000003e-06,
      "loss": 0.003,
      "step": 139470
    },
    {
      "epoch": 7.438933333333333,
      "grad_norm": 0.1243676096200943,
      "learning_rate": 3.5066666666666673e-06,
      "loss": 0.0015,
      "step": 139480
    },
    {
      "epoch": 7.439466666666666,
      "grad_norm": 0.12219401448965073,
      "learning_rate": 3.5033333333333334e-06,
      "loss": 0.0013,
      "step": 139490
    },
    {
      "epoch": 7.44,
      "grad_norm": 0.2628919780254364,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 0.0017,
      "step": 139500
    },
    {
      "epoch": 7.440533333333334,
      "grad_norm": 0.4646259844303131,
      "learning_rate": 3.496666666666667e-06,
      "loss": 0.0018,
      "step": 139510
    },
    {
      "epoch": 7.441066666666667,
      "grad_norm": 0.05736709386110306,
      "learning_rate": 3.493333333333333e-06,
      "loss": 0.002,
      "step": 139520
    },
    {
      "epoch": 7.4416,
      "grad_norm": 0.15551573038101196,
      "learning_rate": 3.49e-06,
      "loss": 0.0016,
      "step": 139530
    },
    {
      "epoch": 7.4421333333333335,
      "grad_norm": 0.06621916592121124,
      "learning_rate": 3.4866666666666666e-06,
      "loss": 0.0017,
      "step": 139540
    },
    {
      "epoch": 7.442666666666667,
      "grad_norm": 0.06987199932336807,
      "learning_rate": 3.4833333333333336e-06,
      "loss": 0.0024,
      "step": 139550
    },
    {
      "epoch": 7.4432,
      "grad_norm": 0.06620616465806961,
      "learning_rate": 3.4799999999999997e-06,
      "loss": 0.0011,
      "step": 139560
    },
    {
      "epoch": 7.443733333333333,
      "grad_norm": 0.48192209005355835,
      "learning_rate": 3.476666666666667e-06,
      "loss": 0.0016,
      "step": 139570
    },
    {
      "epoch": 7.444266666666667,
      "grad_norm": 0.04365997761487961,
      "learning_rate": 3.4733333333333333e-06,
      "loss": 0.0013,
      "step": 139580
    },
    {
      "epoch": 7.4448,
      "grad_norm": 0.028706904500722885,
      "learning_rate": 3.4700000000000002e-06,
      "loss": 0.0014,
      "step": 139590
    },
    {
      "epoch": 7.445333333333333,
      "grad_norm": 0.3613365590572357,
      "learning_rate": 3.466666666666667e-06,
      "loss": 0.0031,
      "step": 139600
    },
    {
      "epoch": 7.445866666666666,
      "grad_norm": 0.11648343503475189,
      "learning_rate": 3.4633333333333338e-06,
      "loss": 0.0019,
      "step": 139610
    },
    {
      "epoch": 7.4464,
      "grad_norm": 0.23097825050354004,
      "learning_rate": 3.46e-06,
      "loss": 0.002,
      "step": 139620
    },
    {
      "epoch": 7.446933333333333,
      "grad_norm": 0.09309595823287964,
      "learning_rate": 3.456666666666667e-06,
      "loss": 0.0019,
      "step": 139630
    },
    {
      "epoch": 7.447466666666667,
      "grad_norm": 0.08707047253847122,
      "learning_rate": 3.4533333333333334e-06,
      "loss": 0.0016,
      "step": 139640
    },
    {
      "epoch": 7.448,
      "grad_norm": 0.22924762964248657,
      "learning_rate": 3.4500000000000004e-06,
      "loss": 0.0015,
      "step": 139650
    },
    {
      "epoch": 7.448533333333334,
      "grad_norm": 0.4269554316997528,
      "learning_rate": 3.446666666666667e-06,
      "loss": 0.0017,
      "step": 139660
    },
    {
      "epoch": 7.449066666666667,
      "grad_norm": 0.1400993913412094,
      "learning_rate": 3.443333333333334e-06,
      "loss": 0.0018,
      "step": 139670
    },
    {
      "epoch": 7.4496,
      "grad_norm": 0.14525440335273743,
      "learning_rate": 3.44e-06,
      "loss": 0.0021,
      "step": 139680
    },
    {
      "epoch": 7.4501333333333335,
      "grad_norm": 0.1713370680809021,
      "learning_rate": 3.4366666666666667e-06,
      "loss": 0.0023,
      "step": 139690
    },
    {
      "epoch": 7.450666666666667,
      "grad_norm": 0.38595050573349,
      "learning_rate": 3.4333333333333336e-06,
      "loss": 0.0016,
      "step": 139700
    },
    {
      "epoch": 7.4512,
      "grad_norm": 0.16958090662956238,
      "learning_rate": 3.4299999999999998e-06,
      "loss": 0.0017,
      "step": 139710
    },
    {
      "epoch": 7.451733333333333,
      "grad_norm": 0.04407665878534317,
      "learning_rate": 3.4266666666666668e-06,
      "loss": 0.0028,
      "step": 139720
    },
    {
      "epoch": 7.452266666666667,
      "grad_norm": 0.1760493367910385,
      "learning_rate": 3.4233333333333333e-06,
      "loss": 0.0014,
      "step": 139730
    },
    {
      "epoch": 7.4528,
      "grad_norm": 0.12120227515697479,
      "learning_rate": 3.4200000000000003e-06,
      "loss": 0.0018,
      "step": 139740
    },
    {
      "epoch": 7.453333333333333,
      "grad_norm": 0.27999770641326904,
      "learning_rate": 3.4166666666666664e-06,
      "loss": 0.0017,
      "step": 139750
    },
    {
      "epoch": 7.453866666666666,
      "grad_norm": 0.09438728541135788,
      "learning_rate": 3.413333333333334e-06,
      "loss": 0.0017,
      "step": 139760
    },
    {
      "epoch": 7.4544,
      "grad_norm": 0.19673120975494385,
      "learning_rate": 3.41e-06,
      "loss": 0.0017,
      "step": 139770
    },
    {
      "epoch": 7.454933333333333,
      "grad_norm": 0.12516893446445465,
      "learning_rate": 3.406666666666667e-06,
      "loss": 0.0016,
      "step": 139780
    },
    {
      "epoch": 7.455466666666666,
      "grad_norm": 0.31937339901924133,
      "learning_rate": 3.4033333333333335e-06,
      "loss": 0.0018,
      "step": 139790
    },
    {
      "epoch": 7.456,
      "grad_norm": 0.04128960892558098,
      "learning_rate": 3.4000000000000005e-06,
      "loss": 0.0033,
      "step": 139800
    },
    {
      "epoch": 7.456533333333334,
      "grad_norm": 0.14738300442695618,
      "learning_rate": 3.3966666666666666e-06,
      "loss": 0.0014,
      "step": 139810
    },
    {
      "epoch": 7.457066666666667,
      "grad_norm": 0.14288075268268585,
      "learning_rate": 3.3933333333333336e-06,
      "loss": 0.0022,
      "step": 139820
    },
    {
      "epoch": 7.4576,
      "grad_norm": 0.04284907877445221,
      "learning_rate": 3.39e-06,
      "loss": 0.0017,
      "step": 139830
    },
    {
      "epoch": 7.4581333333333335,
      "grad_norm": 0.22648301720619202,
      "learning_rate": 3.386666666666667e-06,
      "loss": 0.0014,
      "step": 139840
    },
    {
      "epoch": 7.458666666666667,
      "grad_norm": 0.4838523268699646,
      "learning_rate": 3.3833333333333337e-06,
      "loss": 0.002,
      "step": 139850
    },
    {
      "epoch": 7.4592,
      "grad_norm": 0.23449674248695374,
      "learning_rate": 3.38e-06,
      "loss": 0.0046,
      "step": 139860
    },
    {
      "epoch": 7.459733333333333,
      "grad_norm": 0.18509377539157867,
      "learning_rate": 3.376666666666667e-06,
      "loss": 0.0014,
      "step": 139870
    },
    {
      "epoch": 7.460266666666667,
      "grad_norm": 0.12386447936296463,
      "learning_rate": 3.3733333333333334e-06,
      "loss": 0.0015,
      "step": 139880
    },
    {
      "epoch": 7.4608,
      "grad_norm": 0.1193661019206047,
      "learning_rate": 3.3700000000000003e-06,
      "loss": 0.0021,
      "step": 139890
    },
    {
      "epoch": 7.461333333333333,
      "grad_norm": 0.22752265632152557,
      "learning_rate": 3.3666666666666665e-06,
      "loss": 0.002,
      "step": 139900
    },
    {
      "epoch": 7.461866666666666,
      "grad_norm": 0.2850356698036194,
      "learning_rate": 3.3633333333333335e-06,
      "loss": 0.0018,
      "step": 139910
    },
    {
      "epoch": 7.4624,
      "grad_norm": 0.19750116765499115,
      "learning_rate": 3.36e-06,
      "loss": 0.0018,
      "step": 139920
    },
    {
      "epoch": 7.462933333333333,
      "grad_norm": 0.14509086310863495,
      "learning_rate": 3.356666666666667e-06,
      "loss": 0.0022,
      "step": 139930
    },
    {
      "epoch": 7.463466666666667,
      "grad_norm": 0.25771746039390564,
      "learning_rate": 3.353333333333333e-06,
      "loss": 0.0022,
      "step": 139940
    },
    {
      "epoch": 7.464,
      "grad_norm": 0.040694404393434525,
      "learning_rate": 3.3500000000000005e-06,
      "loss": 0.0017,
      "step": 139950
    },
    {
      "epoch": 7.464533333333334,
      "grad_norm": 0.059016093611717224,
      "learning_rate": 3.3466666666666667e-06,
      "loss": 0.0021,
      "step": 139960
    },
    {
      "epoch": 7.465066666666667,
      "grad_norm": 0.09265871345996857,
      "learning_rate": 3.3433333333333337e-06,
      "loss": 0.001,
      "step": 139970
    },
    {
      "epoch": 7.4656,
      "grad_norm": 0.03805956989526749,
      "learning_rate": 3.34e-06,
      "loss": 0.0015,
      "step": 139980
    },
    {
      "epoch": 7.4661333333333335,
      "grad_norm": 0.28571105003356934,
      "learning_rate": 3.336666666666667e-06,
      "loss": 0.0023,
      "step": 139990
    },
    {
      "epoch": 7.466666666666667,
      "grad_norm": 0.08156192302703857,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.0014,
      "step": 140000
    },
    {
      "epoch": 7.4672,
      "grad_norm": 0.040466200560331345,
      "learning_rate": 3.3300000000000003e-06,
      "loss": 0.0016,
      "step": 140010
    },
    {
      "epoch": 7.467733333333333,
      "grad_norm": 0.11124860495328903,
      "learning_rate": 3.326666666666667e-06,
      "loss": 0.0013,
      "step": 140020
    },
    {
      "epoch": 7.468266666666667,
      "grad_norm": 0.07512550055980682,
      "learning_rate": 3.323333333333333e-06,
      "loss": 0.0016,
      "step": 140030
    },
    {
      "epoch": 7.4688,
      "grad_norm": 0.09300878643989563,
      "learning_rate": 3.3200000000000004e-06,
      "loss": 0.0016,
      "step": 140040
    },
    {
      "epoch": 7.469333333333333,
      "grad_norm": 0.09863953292369843,
      "learning_rate": 3.3166666666666665e-06,
      "loss": 0.0018,
      "step": 140050
    },
    {
      "epoch": 7.469866666666666,
      "grad_norm": 0.2846621870994568,
      "learning_rate": 3.3133333333333335e-06,
      "loss": 0.0011,
      "step": 140060
    },
    {
      "epoch": 7.4704,
      "grad_norm": 0.04675587639212608,
      "learning_rate": 3.31e-06,
      "loss": 0.0015,
      "step": 140070
    },
    {
      "epoch": 7.470933333333333,
      "grad_norm": 0.08599592000246048,
      "learning_rate": 3.306666666666667e-06,
      "loss": 0.0026,
      "step": 140080
    },
    {
      "epoch": 7.471466666666666,
      "grad_norm": 0.2407660037279129,
      "learning_rate": 3.303333333333333e-06,
      "loss": 0.0016,
      "step": 140090
    },
    {
      "epoch": 7.4719999999999995,
      "grad_norm": 0.09024759382009506,
      "learning_rate": 3.3e-06,
      "loss": 0.002,
      "step": 140100
    },
    {
      "epoch": 7.472533333333334,
      "grad_norm": 0.059680260717868805,
      "learning_rate": 3.2966666666666667e-06,
      "loss": 0.0019,
      "step": 140110
    },
    {
      "epoch": 7.473066666666667,
      "grad_norm": 0.2508498728275299,
      "learning_rate": 3.2933333333333337e-06,
      "loss": 0.0012,
      "step": 140120
    },
    {
      "epoch": 7.4736,
      "grad_norm": 0.17520086467266083,
      "learning_rate": 3.29e-06,
      "loss": 0.0019,
      "step": 140130
    },
    {
      "epoch": 7.4741333333333335,
      "grad_norm": 0.45501387119293213,
      "learning_rate": 3.2866666666666672e-06,
      "loss": 0.0016,
      "step": 140140
    },
    {
      "epoch": 7.474666666666667,
      "grad_norm": 0.5098894834518433,
      "learning_rate": 3.2833333333333334e-06,
      "loss": 0.0022,
      "step": 140150
    },
    {
      "epoch": 7.4752,
      "grad_norm": 0.4304542541503906,
      "learning_rate": 3.2800000000000004e-06,
      "loss": 0.0018,
      "step": 140160
    },
    {
      "epoch": 7.475733333333333,
      "grad_norm": 0.2121904492378235,
      "learning_rate": 3.276666666666667e-06,
      "loss": 0.0013,
      "step": 140170
    },
    {
      "epoch": 7.476266666666667,
      "grad_norm": 0.23887522518634796,
      "learning_rate": 3.273333333333334e-06,
      "loss": 0.0017,
      "step": 140180
    },
    {
      "epoch": 7.4768,
      "grad_norm": 0.6292243003845215,
      "learning_rate": 3.27e-06,
      "loss": 0.0018,
      "step": 140190
    },
    {
      "epoch": 7.477333333333333,
      "grad_norm": 0.09894416481256485,
      "learning_rate": 3.2666666666666666e-06,
      "loss": 0.0022,
      "step": 140200
    },
    {
      "epoch": 7.477866666666666,
      "grad_norm": 0.05425423011183739,
      "learning_rate": 3.2633333333333336e-06,
      "loss": 0.0017,
      "step": 140210
    },
    {
      "epoch": 7.4784,
      "grad_norm": 0.18408453464508057,
      "learning_rate": 3.2599999999999997e-06,
      "loss": 0.0016,
      "step": 140220
    },
    {
      "epoch": 7.478933333333333,
      "grad_norm": 0.14464980363845825,
      "learning_rate": 3.2566666666666667e-06,
      "loss": 0.0013,
      "step": 140230
    },
    {
      "epoch": 7.479466666666666,
      "grad_norm": 0.18066607415676117,
      "learning_rate": 3.2533333333333332e-06,
      "loss": 0.0026,
      "step": 140240
    },
    {
      "epoch": 7.48,
      "grad_norm": 0.2550187408924103,
      "learning_rate": 3.2500000000000002e-06,
      "loss": 0.0014,
      "step": 140250
    },
    {
      "epoch": 7.480533333333334,
      "grad_norm": 0.4549543261528015,
      "learning_rate": 3.2466666666666668e-06,
      "loss": 0.0019,
      "step": 140260
    },
    {
      "epoch": 7.481066666666667,
      "grad_norm": 0.1565728485584259,
      "learning_rate": 3.2433333333333338e-06,
      "loss": 0.002,
      "step": 140270
    },
    {
      "epoch": 7.4816,
      "grad_norm": 0.5908628106117249,
      "learning_rate": 3.24e-06,
      "loss": 0.0014,
      "step": 140280
    },
    {
      "epoch": 7.4821333333333335,
      "grad_norm": 0.1033535897731781,
      "learning_rate": 3.236666666666667e-06,
      "loss": 0.003,
      "step": 140290
    },
    {
      "epoch": 7.482666666666667,
      "grad_norm": 0.08571704477071762,
      "learning_rate": 3.2333333333333334e-06,
      "loss": 0.0015,
      "step": 140300
    },
    {
      "epoch": 7.4832,
      "grad_norm": 0.07090489566326141,
      "learning_rate": 3.2300000000000004e-06,
      "loss": 0.0014,
      "step": 140310
    },
    {
      "epoch": 7.483733333333333,
      "grad_norm": 0.14998050034046173,
      "learning_rate": 3.2266666666666665e-06,
      "loss": 0.0022,
      "step": 140320
    },
    {
      "epoch": 7.484266666666667,
      "grad_norm": 0.23029081523418427,
      "learning_rate": 3.223333333333334e-06,
      "loss": 0.0013,
      "step": 140330
    },
    {
      "epoch": 7.4848,
      "grad_norm": 0.11969099938869476,
      "learning_rate": 3.22e-06,
      "loss": 0.0017,
      "step": 140340
    },
    {
      "epoch": 7.485333333333333,
      "grad_norm": 0.09826215356588364,
      "learning_rate": 3.216666666666667e-06,
      "loss": 0.0021,
      "step": 140350
    },
    {
      "epoch": 7.4858666666666664,
      "grad_norm": 0.2586812376976013,
      "learning_rate": 3.2133333333333336e-06,
      "loss": 0.0026,
      "step": 140360
    },
    {
      "epoch": 7.4864,
      "grad_norm": 0.11272439360618591,
      "learning_rate": 3.2099999999999998e-06,
      "loss": 0.0013,
      "step": 140370
    },
    {
      "epoch": 7.486933333333333,
      "grad_norm": 0.35313835740089417,
      "learning_rate": 3.2066666666666667e-06,
      "loss": 0.0017,
      "step": 140380
    },
    {
      "epoch": 7.487466666666666,
      "grad_norm": 0.16364087164402008,
      "learning_rate": 3.2033333333333333e-06,
      "loss": 0.0017,
      "step": 140390
    },
    {
      "epoch": 7.4879999999999995,
      "grad_norm": 0.4583451747894287,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 0.0016,
      "step": 140400
    },
    {
      "epoch": 7.488533333333334,
      "grad_norm": 0.0505441352725029,
      "learning_rate": 3.1966666666666664e-06,
      "loss": 0.0015,
      "step": 140410
    },
    {
      "epoch": 7.489066666666667,
      "grad_norm": 0.23738397657871246,
      "learning_rate": 3.1933333333333334e-06,
      "loss": 0.0012,
      "step": 140420
    },
    {
      "epoch": 7.4896,
      "grad_norm": 0.36594077944755554,
      "learning_rate": 3.19e-06,
      "loss": 0.0015,
      "step": 140430
    },
    {
      "epoch": 7.4901333333333335,
      "grad_norm": 0.1316514015197754,
      "learning_rate": 3.186666666666667e-06,
      "loss": 0.0018,
      "step": 140440
    },
    {
      "epoch": 7.490666666666667,
      "grad_norm": 0.1477171778678894,
      "learning_rate": 3.1833333333333335e-06,
      "loss": 0.0014,
      "step": 140450
    },
    {
      "epoch": 7.4912,
      "grad_norm": 0.43567028641700745,
      "learning_rate": 3.1800000000000005e-06,
      "loss": 0.0026,
      "step": 140460
    },
    {
      "epoch": 7.491733333333333,
      "grad_norm": 0.2628994286060333,
      "learning_rate": 3.1766666666666666e-06,
      "loss": 0.0026,
      "step": 140470
    },
    {
      "epoch": 7.492266666666667,
      "grad_norm": 0.19923745095729828,
      "learning_rate": 3.1733333333333336e-06,
      "loss": 0.0013,
      "step": 140480
    },
    {
      "epoch": 7.4928,
      "grad_norm": 0.3080425262451172,
      "learning_rate": 3.17e-06,
      "loss": 0.0015,
      "step": 140490
    },
    {
      "epoch": 7.493333333333333,
      "grad_norm": 0.19764558970928192,
      "learning_rate": 3.166666666666667e-06,
      "loss": 0.0015,
      "step": 140500
    },
    {
      "epoch": 7.4938666666666665,
      "grad_norm": 0.05121771618723869,
      "learning_rate": 3.1633333333333333e-06,
      "loss": 0.002,
      "step": 140510
    },
    {
      "epoch": 7.4944,
      "grad_norm": 0.3717442452907562,
      "learning_rate": 3.1600000000000007e-06,
      "loss": 0.0018,
      "step": 140520
    },
    {
      "epoch": 7.494933333333333,
      "grad_norm": 0.17071489989757538,
      "learning_rate": 3.156666666666667e-06,
      "loss": 0.0023,
      "step": 140530
    },
    {
      "epoch": 7.495466666666666,
      "grad_norm": 0.40114718675613403,
      "learning_rate": 3.153333333333333e-06,
      "loss": 0.0018,
      "step": 140540
    },
    {
      "epoch": 7.496,
      "grad_norm": 0.14110015332698822,
      "learning_rate": 3.1500000000000003e-06,
      "loss": 0.0013,
      "step": 140550
    },
    {
      "epoch": 7.496533333333334,
      "grad_norm": 0.39554905891418457,
      "learning_rate": 3.1466666666666665e-06,
      "loss": 0.0019,
      "step": 140560
    },
    {
      "epoch": 7.497066666666667,
      "grad_norm": 0.2558678090572357,
      "learning_rate": 3.1433333333333334e-06,
      "loss": 0.0021,
      "step": 140570
    },
    {
      "epoch": 7.4976,
      "grad_norm": 0.31334465742111206,
      "learning_rate": 3.14e-06,
      "loss": 0.0019,
      "step": 140580
    },
    {
      "epoch": 7.4981333333333335,
      "grad_norm": 0.04619850963354111,
      "learning_rate": 3.136666666666667e-06,
      "loss": 0.0021,
      "step": 140590
    },
    {
      "epoch": 7.498666666666667,
      "grad_norm": 0.3517517149448395,
      "learning_rate": 3.133333333333333e-06,
      "loss": 0.0013,
      "step": 140600
    },
    {
      "epoch": 7.4992,
      "grad_norm": 0.14192259311676025,
      "learning_rate": 3.13e-06,
      "loss": 0.0032,
      "step": 140610
    },
    {
      "epoch": 7.499733333333333,
      "grad_norm": 0.5614942312240601,
      "learning_rate": 3.1266666666666667e-06,
      "loss": 0.0016,
      "step": 140620
    },
    {
      "epoch": 7.500266666666667,
      "grad_norm": 0.15527750551700592,
      "learning_rate": 3.1233333333333332e-06,
      "loss": 0.002,
      "step": 140630
    },
    {
      "epoch": 7.5008,
      "grad_norm": 0.29938238859176636,
      "learning_rate": 3.12e-06,
      "loss": 0.0012,
      "step": 140640
    },
    {
      "epoch": 7.501333333333333,
      "grad_norm": 0.14525145292282104,
      "learning_rate": 3.1166666666666668e-06,
      "loss": 0.0026,
      "step": 140650
    },
    {
      "epoch": 7.5018666666666665,
      "grad_norm": 0.05977218225598335,
      "learning_rate": 3.1133333333333333e-06,
      "loss": 0.0014,
      "step": 140660
    },
    {
      "epoch": 7.5024,
      "grad_norm": 0.1427498161792755,
      "learning_rate": 3.11e-06,
      "loss": 0.002,
      "step": 140670
    },
    {
      "epoch": 7.502933333333333,
      "grad_norm": 0.11382179707288742,
      "learning_rate": 3.106666666666667e-06,
      "loss": 0.002,
      "step": 140680
    },
    {
      "epoch": 7.503466666666666,
      "grad_norm": 0.06622032076120377,
      "learning_rate": 3.1033333333333334e-06,
      "loss": 0.002,
      "step": 140690
    },
    {
      "epoch": 7.504,
      "grad_norm": 0.3991996943950653,
      "learning_rate": 3.1e-06,
      "loss": 0.0016,
      "step": 140700
    },
    {
      "epoch": 7.504533333333333,
      "grad_norm": 0.37678202986717224,
      "learning_rate": 3.096666666666667e-06,
      "loss": 0.0017,
      "step": 140710
    },
    {
      "epoch": 7.505066666666667,
      "grad_norm": 0.33704087138175964,
      "learning_rate": 3.0933333333333335e-06,
      "loss": 0.0015,
      "step": 140720
    },
    {
      "epoch": 7.5056,
      "grad_norm": 0.2842612564563751,
      "learning_rate": 3.09e-06,
      "loss": 0.002,
      "step": 140730
    },
    {
      "epoch": 7.5061333333333335,
      "grad_norm": 0.19953083992004395,
      "learning_rate": 3.086666666666667e-06,
      "loss": 0.0014,
      "step": 140740
    },
    {
      "epoch": 7.506666666666667,
      "grad_norm": 0.051586367189884186,
      "learning_rate": 3.0833333333333336e-06,
      "loss": 0.0019,
      "step": 140750
    },
    {
      "epoch": 7.5072,
      "grad_norm": 0.0918714627623558,
      "learning_rate": 3.08e-06,
      "loss": 0.0023,
      "step": 140760
    },
    {
      "epoch": 7.507733333333333,
      "grad_norm": 0.4836065471172333,
      "learning_rate": 3.076666666666667e-06,
      "loss": 0.0019,
      "step": 140770
    },
    {
      "epoch": 7.508266666666667,
      "grad_norm": 0.1426745504140854,
      "learning_rate": 3.0733333333333337e-06,
      "loss": 0.0014,
      "step": 140780
    },
    {
      "epoch": 7.5088,
      "grad_norm": 0.1585274338722229,
      "learning_rate": 3.0700000000000003e-06,
      "loss": 0.0019,
      "step": 140790
    },
    {
      "epoch": 7.509333333333333,
      "grad_norm": 0.06251963973045349,
      "learning_rate": 3.066666666666667e-06,
      "loss": 0.0018,
      "step": 140800
    },
    {
      "epoch": 7.5098666666666665,
      "grad_norm": 0.196000337600708,
      "learning_rate": 3.0633333333333334e-06,
      "loss": 0.0017,
      "step": 140810
    },
    {
      "epoch": 7.5104,
      "grad_norm": 0.06249719113111496,
      "learning_rate": 3.06e-06,
      "loss": 0.0017,
      "step": 140820
    },
    {
      "epoch": 7.510933333333333,
      "grad_norm": 0.08572868257761002,
      "learning_rate": 3.056666666666667e-06,
      "loss": 0.001,
      "step": 140830
    },
    {
      "epoch": 7.511466666666666,
      "grad_norm": 0.12617842853069305,
      "learning_rate": 3.0533333333333335e-06,
      "loss": 0.0013,
      "step": 140840
    },
    {
      "epoch": 7.5120000000000005,
      "grad_norm": 0.16899865865707397,
      "learning_rate": 3.05e-06,
      "loss": 0.0021,
      "step": 140850
    },
    {
      "epoch": 7.512533333333334,
      "grad_norm": 0.4236844480037689,
      "learning_rate": 3.0466666666666666e-06,
      "loss": 0.0016,
      "step": 140860
    },
    {
      "epoch": 7.513066666666667,
      "grad_norm": 0.3875519633293152,
      "learning_rate": 3.0433333333333336e-06,
      "loss": 0.0023,
      "step": 140870
    },
    {
      "epoch": 7.5136,
      "grad_norm": 0.06783634424209595,
      "learning_rate": 3.04e-06,
      "loss": 0.0017,
      "step": 140880
    },
    {
      "epoch": 7.5141333333333336,
      "grad_norm": 0.37280938029289246,
      "learning_rate": 3.0366666666666667e-06,
      "loss": 0.0015,
      "step": 140890
    },
    {
      "epoch": 7.514666666666667,
      "grad_norm": 0.19798794388771057,
      "learning_rate": 3.0333333333333337e-06,
      "loss": 0.002,
      "step": 140900
    },
    {
      "epoch": 7.5152,
      "grad_norm": 0.06428901106119156,
      "learning_rate": 3.0300000000000002e-06,
      "loss": 0.0015,
      "step": 140910
    },
    {
      "epoch": 7.515733333333333,
      "grad_norm": 0.31391263008117676,
      "learning_rate": 3.0266666666666668e-06,
      "loss": 0.0011,
      "step": 140920
    },
    {
      "epoch": 7.516266666666667,
      "grad_norm": 0.0647759810090065,
      "learning_rate": 3.0233333333333338e-06,
      "loss": 0.0015,
      "step": 140930
    },
    {
      "epoch": 7.5168,
      "grad_norm": 0.23426905274391174,
      "learning_rate": 3.0200000000000003e-06,
      "loss": 0.0017,
      "step": 140940
    },
    {
      "epoch": 7.517333333333333,
      "grad_norm": 0.36685505509376526,
      "learning_rate": 3.016666666666667e-06,
      "loss": 0.0016,
      "step": 140950
    },
    {
      "epoch": 7.5178666666666665,
      "grad_norm": 0.03938903659582138,
      "learning_rate": 3.0133333333333334e-06,
      "loss": 0.0013,
      "step": 140960
    },
    {
      "epoch": 7.5184,
      "grad_norm": 0.12069012969732285,
      "learning_rate": 3.01e-06,
      "loss": 0.0019,
      "step": 140970
    },
    {
      "epoch": 7.518933333333333,
      "grad_norm": 0.14319996535778046,
      "learning_rate": 3.0066666666666665e-06,
      "loss": 0.0027,
      "step": 140980
    },
    {
      "epoch": 7.519466666666666,
      "grad_norm": 0.480357825756073,
      "learning_rate": 3.0033333333333335e-06,
      "loss": 0.0014,
      "step": 140990
    },
    {
      "epoch": 7.52,
      "grad_norm": 0.056293681263923645,
      "learning_rate": 3e-06,
      "loss": 0.0012,
      "step": 141000
    },
    {
      "epoch": 7.520533333333333,
      "grad_norm": 0.23814651370048523,
      "learning_rate": 2.9966666666666666e-06,
      "loss": 0.0019,
      "step": 141010
    },
    {
      "epoch": 7.521066666666667,
      "grad_norm": 0.04461711272597313,
      "learning_rate": 2.9933333333333336e-06,
      "loss": 0.0012,
      "step": 141020
    },
    {
      "epoch": 7.5216,
      "grad_norm": 0.19817328453063965,
      "learning_rate": 2.99e-06,
      "loss": 0.0021,
      "step": 141030
    },
    {
      "epoch": 7.522133333333334,
      "grad_norm": 0.036720722913742065,
      "learning_rate": 2.9866666666666667e-06,
      "loss": 0.0017,
      "step": 141040
    },
    {
      "epoch": 7.522666666666667,
      "grad_norm": 0.2883158326148987,
      "learning_rate": 2.9833333333333333e-06,
      "loss": 0.0017,
      "step": 141050
    },
    {
      "epoch": 7.5232,
      "grad_norm": 0.06689828634262085,
      "learning_rate": 2.9800000000000003e-06,
      "loss": 0.0019,
      "step": 141060
    },
    {
      "epoch": 7.523733333333333,
      "grad_norm": 0.1418180614709854,
      "learning_rate": 2.976666666666667e-06,
      "loss": 0.0019,
      "step": 141070
    },
    {
      "epoch": 7.524266666666667,
      "grad_norm": 0.25303223729133606,
      "learning_rate": 2.9733333333333334e-06,
      "loss": 0.0021,
      "step": 141080
    },
    {
      "epoch": 7.5248,
      "grad_norm": 0.2577472925186157,
      "learning_rate": 2.9700000000000004e-06,
      "loss": 0.0017,
      "step": 141090
    },
    {
      "epoch": 7.525333333333333,
      "grad_norm": 0.08566678315401077,
      "learning_rate": 2.966666666666667e-06,
      "loss": 0.0018,
      "step": 141100
    },
    {
      "epoch": 7.5258666666666665,
      "grad_norm": 0.28499072790145874,
      "learning_rate": 2.9633333333333335e-06,
      "loss": 0.0019,
      "step": 141110
    },
    {
      "epoch": 7.5264,
      "grad_norm": 0.06552417576313019,
      "learning_rate": 2.9600000000000005e-06,
      "loss": 0.0015,
      "step": 141120
    },
    {
      "epoch": 7.526933333333333,
      "grad_norm": 0.12063699960708618,
      "learning_rate": 2.956666666666667e-06,
      "loss": 0.0023,
      "step": 141130
    },
    {
      "epoch": 7.527466666666666,
      "grad_norm": 0.061527837067842484,
      "learning_rate": 2.9533333333333336e-06,
      "loss": 0.0022,
      "step": 141140
    },
    {
      "epoch": 7.5280000000000005,
      "grad_norm": 0.07046902924776077,
      "learning_rate": 2.95e-06,
      "loss": 0.0015,
      "step": 141150
    },
    {
      "epoch": 7.528533333333334,
      "grad_norm": 0.34034278988838196,
      "learning_rate": 2.9466666666666667e-06,
      "loss": 0.0017,
      "step": 141160
    },
    {
      "epoch": 7.529066666666667,
      "grad_norm": 0.14426440000534058,
      "learning_rate": 2.9433333333333332e-06,
      "loss": 0.0017,
      "step": 141170
    },
    {
      "epoch": 7.5296,
      "grad_norm": 0.2310270518064499,
      "learning_rate": 2.9400000000000002e-06,
      "loss": 0.0013,
      "step": 141180
    },
    {
      "epoch": 7.530133333333334,
      "grad_norm": 0.2201625108718872,
      "learning_rate": 2.9366666666666668e-06,
      "loss": 0.0014,
      "step": 141190
    },
    {
      "epoch": 7.530666666666667,
      "grad_norm": 0.06560874730348587,
      "learning_rate": 2.9333333333333333e-06,
      "loss": 0.0016,
      "step": 141200
    },
    {
      "epoch": 7.5312,
      "grad_norm": 0.144642636179924,
      "learning_rate": 2.93e-06,
      "loss": 0.0023,
      "step": 141210
    },
    {
      "epoch": 7.531733333333333,
      "grad_norm": 0.1786584109067917,
      "learning_rate": 2.926666666666667e-06,
      "loss": 0.0014,
      "step": 141220
    },
    {
      "epoch": 7.532266666666667,
      "grad_norm": 0.04220438748598099,
      "learning_rate": 2.9233333333333334e-06,
      "loss": 0.003,
      "step": 141230
    },
    {
      "epoch": 7.5328,
      "grad_norm": 0.09162899851799011,
      "learning_rate": 2.92e-06,
      "loss": 0.0017,
      "step": 141240
    },
    {
      "epoch": 7.533333333333333,
      "grad_norm": 0.17391446232795715,
      "learning_rate": 2.916666666666667e-06,
      "loss": 0.0017,
      "step": 141250
    },
    {
      "epoch": 7.5338666666666665,
      "grad_norm": 0.05494588986039162,
      "learning_rate": 2.9133333333333335e-06,
      "loss": 0.0021,
      "step": 141260
    },
    {
      "epoch": 7.5344,
      "grad_norm": 0.20059850811958313,
      "learning_rate": 2.91e-06,
      "loss": 0.0017,
      "step": 141270
    },
    {
      "epoch": 7.534933333333333,
      "grad_norm": 0.14989998936653137,
      "learning_rate": 2.906666666666667e-06,
      "loss": 0.0018,
      "step": 141280
    },
    {
      "epoch": 7.535466666666666,
      "grad_norm": 0.05494397506117821,
      "learning_rate": 2.9033333333333336e-06,
      "loss": 0.0019,
      "step": 141290
    },
    {
      "epoch": 7.536,
      "grad_norm": 0.47502532601356506,
      "learning_rate": 2.9e-06,
      "loss": 0.0015,
      "step": 141300
    },
    {
      "epoch": 7.536533333333333,
      "grad_norm": 0.030276592820882797,
      "learning_rate": 2.896666666666667e-06,
      "loss": 0.0016,
      "step": 141310
    },
    {
      "epoch": 7.537066666666667,
      "grad_norm": 0.1425047516822815,
      "learning_rate": 2.8933333333333333e-06,
      "loss": 0.0013,
      "step": 141320
    },
    {
      "epoch": 7.5376,
      "grad_norm": 0.17270158231258392,
      "learning_rate": 2.89e-06,
      "loss": 0.0017,
      "step": 141330
    },
    {
      "epoch": 7.538133333333334,
      "grad_norm": 0.15629903972148895,
      "learning_rate": 2.886666666666667e-06,
      "loss": 0.0024,
      "step": 141340
    },
    {
      "epoch": 7.538666666666667,
      "grad_norm": 0.18803353607654572,
      "learning_rate": 2.8833333333333334e-06,
      "loss": 0.0016,
      "step": 141350
    },
    {
      "epoch": 7.5392,
      "grad_norm": 0.04123051464557648,
      "learning_rate": 2.88e-06,
      "loss": 0.0017,
      "step": 141360
    },
    {
      "epoch": 7.539733333333333,
      "grad_norm": 0.20636560022830963,
      "learning_rate": 2.876666666666667e-06,
      "loss": 0.0015,
      "step": 141370
    },
    {
      "epoch": 7.540266666666667,
      "grad_norm": 0.07092271000146866,
      "learning_rate": 2.8733333333333335e-06,
      "loss": 0.0017,
      "step": 141380
    },
    {
      "epoch": 7.5408,
      "grad_norm": 0.30355551838874817,
      "learning_rate": 2.87e-06,
      "loss": 0.0019,
      "step": 141390
    },
    {
      "epoch": 7.541333333333333,
      "grad_norm": 0.18129299581050873,
      "learning_rate": 2.8666666666666666e-06,
      "loss": 0.002,
      "step": 141400
    },
    {
      "epoch": 7.5418666666666665,
      "grad_norm": 0.1588752269744873,
      "learning_rate": 2.8633333333333336e-06,
      "loss": 0.0014,
      "step": 141410
    },
    {
      "epoch": 7.5424,
      "grad_norm": 0.31377580761909485,
      "learning_rate": 2.86e-06,
      "loss": 0.0013,
      "step": 141420
    },
    {
      "epoch": 7.542933333333333,
      "grad_norm": 0.2568660378456116,
      "learning_rate": 2.8566666666666667e-06,
      "loss": 0.0022,
      "step": 141430
    },
    {
      "epoch": 7.543466666666666,
      "grad_norm": 0.3676927387714386,
      "learning_rate": 2.8533333333333337e-06,
      "loss": 0.002,
      "step": 141440
    },
    {
      "epoch": 7.5440000000000005,
      "grad_norm": 0.1151718869805336,
      "learning_rate": 2.8500000000000002e-06,
      "loss": 0.0014,
      "step": 141450
    },
    {
      "epoch": 7.544533333333334,
      "grad_norm": 0.06048660725355148,
      "learning_rate": 2.846666666666667e-06,
      "loss": 0.0017,
      "step": 141460
    },
    {
      "epoch": 7.545066666666667,
      "grad_norm": 0.22675462067127228,
      "learning_rate": 2.8433333333333338e-06,
      "loss": 0.0015,
      "step": 141470
    },
    {
      "epoch": 7.5456,
      "grad_norm": 0.06736325472593307,
      "learning_rate": 2.8400000000000003e-06,
      "loss": 0.0014,
      "step": 141480
    },
    {
      "epoch": 7.546133333333334,
      "grad_norm": 0.22815437614917755,
      "learning_rate": 2.8366666666666665e-06,
      "loss": 0.0024,
      "step": 141490
    },
    {
      "epoch": 7.546666666666667,
      "grad_norm": 0.23364709317684174,
      "learning_rate": 2.8333333333333335e-06,
      "loss": 0.0013,
      "step": 141500
    },
    {
      "epoch": 7.5472,
      "grad_norm": 0.11964240670204163,
      "learning_rate": 2.83e-06,
      "loss": 0.0021,
      "step": 141510
    },
    {
      "epoch": 7.547733333333333,
      "grad_norm": 0.1695897877216339,
      "learning_rate": 2.8266666666666666e-06,
      "loss": 0.0023,
      "step": 141520
    },
    {
      "epoch": 7.548266666666667,
      "grad_norm": 0.1730388104915619,
      "learning_rate": 2.8233333333333335e-06,
      "loss": 0.0016,
      "step": 141530
    },
    {
      "epoch": 7.5488,
      "grad_norm": 0.22756503522396088,
      "learning_rate": 2.82e-06,
      "loss": 0.0015,
      "step": 141540
    },
    {
      "epoch": 7.549333333333333,
      "grad_norm": 0.06550926715135574,
      "learning_rate": 2.8166666666666667e-06,
      "loss": 0.0027,
      "step": 141550
    },
    {
      "epoch": 7.5498666666666665,
      "grad_norm": 0.0887051597237587,
      "learning_rate": 2.8133333333333336e-06,
      "loss": 0.0018,
      "step": 141560
    },
    {
      "epoch": 7.5504,
      "grad_norm": 0.09016577899456024,
      "learning_rate": 2.81e-06,
      "loss": 0.0015,
      "step": 141570
    },
    {
      "epoch": 7.550933333333333,
      "grad_norm": 0.3861876130104065,
      "learning_rate": 2.8066666666666668e-06,
      "loss": 0.0021,
      "step": 141580
    },
    {
      "epoch": 7.551466666666666,
      "grad_norm": 0.2197844237089157,
      "learning_rate": 2.8033333333333333e-06,
      "loss": 0.0022,
      "step": 141590
    },
    {
      "epoch": 7.552,
      "grad_norm": 0.178256094455719,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 0.0018,
      "step": 141600
    },
    {
      "epoch": 7.552533333333333,
      "grad_norm": 0.08845961838960648,
      "learning_rate": 2.796666666666667e-06,
      "loss": 0.002,
      "step": 141610
    },
    {
      "epoch": 7.553066666666667,
      "grad_norm": 0.06737878173589706,
      "learning_rate": 2.7933333333333334e-06,
      "loss": 0.0016,
      "step": 141620
    },
    {
      "epoch": 7.5536,
      "grad_norm": 0.33907514810562134,
      "learning_rate": 2.7900000000000004e-06,
      "loss": 0.0017,
      "step": 141630
    },
    {
      "epoch": 7.554133333333334,
      "grad_norm": 0.14615635573863983,
      "learning_rate": 2.786666666666667e-06,
      "loss": 0.0012,
      "step": 141640
    },
    {
      "epoch": 7.554666666666667,
      "grad_norm": 0.020664988085627556,
      "learning_rate": 2.7833333333333335e-06,
      "loss": 0.0017,
      "step": 141650
    },
    {
      "epoch": 7.5552,
      "grad_norm": 0.2148725390434265,
      "learning_rate": 2.78e-06,
      "loss": 0.0025,
      "step": 141660
    },
    {
      "epoch": 7.555733333333333,
      "grad_norm": 0.03922067582607269,
      "learning_rate": 2.7766666666666666e-06,
      "loss": 0.0028,
      "step": 141670
    },
    {
      "epoch": 7.556266666666667,
      "grad_norm": 0.0496775321662426,
      "learning_rate": 2.773333333333333e-06,
      "loss": 0.002,
      "step": 141680
    },
    {
      "epoch": 7.5568,
      "grad_norm": 0.3968733251094818,
      "learning_rate": 2.77e-06,
      "loss": 0.0021,
      "step": 141690
    },
    {
      "epoch": 7.557333333333333,
      "grad_norm": 0.04141395539045334,
      "learning_rate": 2.7666666666666667e-06,
      "loss": 0.0021,
      "step": 141700
    },
    {
      "epoch": 7.5578666666666665,
      "grad_norm": 0.22605785727500916,
      "learning_rate": 2.7633333333333333e-06,
      "loss": 0.0016,
      "step": 141710
    },
    {
      "epoch": 7.5584,
      "grad_norm": 0.19587993621826172,
      "learning_rate": 2.7600000000000003e-06,
      "loss": 0.0025,
      "step": 141720
    },
    {
      "epoch": 7.558933333333333,
      "grad_norm": 0.20472972095012665,
      "learning_rate": 2.756666666666667e-06,
      "loss": 0.0021,
      "step": 141730
    },
    {
      "epoch": 7.559466666666666,
      "grad_norm": 0.2027536928653717,
      "learning_rate": 2.7533333333333334e-06,
      "loss": 0.0016,
      "step": 141740
    },
    {
      "epoch": 7.5600000000000005,
      "grad_norm": 0.28101667761802673,
      "learning_rate": 2.7500000000000004e-06,
      "loss": 0.0014,
      "step": 141750
    },
    {
      "epoch": 7.560533333333334,
      "grad_norm": 0.11565754562616348,
      "learning_rate": 2.746666666666667e-06,
      "loss": 0.0018,
      "step": 141760
    },
    {
      "epoch": 7.561066666666667,
      "grad_norm": 0.08660146594047546,
      "learning_rate": 2.7433333333333335e-06,
      "loss": 0.0024,
      "step": 141770
    },
    {
      "epoch": 7.5616,
      "grad_norm": 0.25708186626434326,
      "learning_rate": 2.74e-06,
      "loss": 0.0015,
      "step": 141780
    },
    {
      "epoch": 7.562133333333334,
      "grad_norm": 0.10428174585103989,
      "learning_rate": 2.736666666666667e-06,
      "loss": 0.0018,
      "step": 141790
    },
    {
      "epoch": 7.562666666666667,
      "grad_norm": 0.11473879218101501,
      "learning_rate": 2.7333333333333336e-06,
      "loss": 0.0024,
      "step": 141800
    },
    {
      "epoch": 7.5632,
      "grad_norm": 0.2934269309043884,
      "learning_rate": 2.73e-06,
      "loss": 0.0014,
      "step": 141810
    },
    {
      "epoch": 7.563733333333333,
      "grad_norm": 0.1760302484035492,
      "learning_rate": 2.726666666666667e-06,
      "loss": 0.0019,
      "step": 141820
    },
    {
      "epoch": 7.564266666666667,
      "grad_norm": 0.048583511263132095,
      "learning_rate": 2.7233333333333332e-06,
      "loss": 0.0022,
      "step": 141830
    },
    {
      "epoch": 7.5648,
      "grad_norm": 0.3182997405529022,
      "learning_rate": 2.72e-06,
      "loss": 0.0021,
      "step": 141840
    },
    {
      "epoch": 7.565333333333333,
      "grad_norm": 0.07437589764595032,
      "learning_rate": 2.7166666666666668e-06,
      "loss": 0.0012,
      "step": 141850
    },
    {
      "epoch": 7.5658666666666665,
      "grad_norm": 0.23251551389694214,
      "learning_rate": 2.7133333333333333e-06,
      "loss": 0.0018,
      "step": 141860
    },
    {
      "epoch": 7.5664,
      "grad_norm": 0.4371855854988098,
      "learning_rate": 2.71e-06,
      "loss": 0.0012,
      "step": 141870
    },
    {
      "epoch": 7.566933333333333,
      "grad_norm": 0.06165022403001785,
      "learning_rate": 2.706666666666667e-06,
      "loss": 0.0013,
      "step": 141880
    },
    {
      "epoch": 7.567466666666666,
      "grad_norm": 0.6593685150146484,
      "learning_rate": 2.7033333333333334e-06,
      "loss": 0.0015,
      "step": 141890
    },
    {
      "epoch": 7.568,
      "grad_norm": 0.057178858667612076,
      "learning_rate": 2.7e-06,
      "loss": 0.0018,
      "step": 141900
    },
    {
      "epoch": 7.568533333333333,
      "grad_norm": 0.14305539429187775,
      "learning_rate": 2.696666666666667e-06,
      "loss": 0.0013,
      "step": 141910
    },
    {
      "epoch": 7.569066666666667,
      "grad_norm": 0.4756772518157959,
      "learning_rate": 2.6933333333333335e-06,
      "loss": 0.0017,
      "step": 141920
    },
    {
      "epoch": 7.5696,
      "grad_norm": 0.024603789672255516,
      "learning_rate": 2.69e-06,
      "loss": 0.0018,
      "step": 141930
    },
    {
      "epoch": 7.570133333333334,
      "grad_norm": 0.5633550882339478,
      "learning_rate": 2.6866666666666666e-06,
      "loss": 0.0029,
      "step": 141940
    },
    {
      "epoch": 7.570666666666667,
      "grad_norm": 0.048293422907590866,
      "learning_rate": 2.6833333333333336e-06,
      "loss": 0.0017,
      "step": 141950
    },
    {
      "epoch": 7.5712,
      "grad_norm": 0.1738276183605194,
      "learning_rate": 2.68e-06,
      "loss": 0.0021,
      "step": 141960
    },
    {
      "epoch": 7.571733333333333,
      "grad_norm": 0.4270210266113281,
      "learning_rate": 2.6766666666666667e-06,
      "loss": 0.0019,
      "step": 141970
    },
    {
      "epoch": 7.572266666666667,
      "grad_norm": 0.0627310499548912,
      "learning_rate": 2.6733333333333337e-06,
      "loss": 0.0016,
      "step": 141980
    },
    {
      "epoch": 7.5728,
      "grad_norm": 0.09535421431064606,
      "learning_rate": 2.6700000000000003e-06,
      "loss": 0.0013,
      "step": 141990
    },
    {
      "epoch": 7.573333333333333,
      "grad_norm": 0.23264780640602112,
      "learning_rate": 2.666666666666667e-06,
      "loss": 0.0019,
      "step": 142000
    },
    {
      "epoch": 7.5738666666666665,
      "grad_norm": 0.1902916580438614,
      "learning_rate": 2.6633333333333334e-06,
      "loss": 0.0022,
      "step": 142010
    },
    {
      "epoch": 7.5744,
      "grad_norm": 0.0737195611000061,
      "learning_rate": 2.66e-06,
      "loss": 0.0021,
      "step": 142020
    },
    {
      "epoch": 7.574933333333333,
      "grad_norm": 0.12358707189559937,
      "learning_rate": 2.6566666666666665e-06,
      "loss": 0.0015,
      "step": 142030
    },
    {
      "epoch": 7.575466666666666,
      "grad_norm": 0.08503683656454086,
      "learning_rate": 2.6533333333333335e-06,
      "loss": 0.0014,
      "step": 142040
    },
    {
      "epoch": 7.576,
      "grad_norm": 0.45745858550071716,
      "learning_rate": 2.65e-06,
      "loss": 0.0016,
      "step": 142050
    },
    {
      "epoch": 7.576533333333334,
      "grad_norm": 0.533976137638092,
      "learning_rate": 2.6466666666666666e-06,
      "loss": 0.003,
      "step": 142060
    },
    {
      "epoch": 7.577066666666667,
      "grad_norm": 0.23031818866729736,
      "learning_rate": 2.6433333333333336e-06,
      "loss": 0.0012,
      "step": 142070
    },
    {
      "epoch": 7.5776,
      "grad_norm": 0.07442426681518555,
      "learning_rate": 2.64e-06,
      "loss": 0.0013,
      "step": 142080
    },
    {
      "epoch": 7.578133333333334,
      "grad_norm": 0.25446319580078125,
      "learning_rate": 2.6366666666666667e-06,
      "loss": 0.0016,
      "step": 142090
    },
    {
      "epoch": 7.578666666666667,
      "grad_norm": 0.06397134065628052,
      "learning_rate": 2.6333333333333337e-06,
      "loss": 0.0015,
      "step": 142100
    },
    {
      "epoch": 7.5792,
      "grad_norm": 0.476297527551651,
      "learning_rate": 2.6300000000000002e-06,
      "loss": 0.0015,
      "step": 142110
    },
    {
      "epoch": 7.579733333333333,
      "grad_norm": 0.16486401855945587,
      "learning_rate": 2.6266666666666668e-06,
      "loss": 0.0022,
      "step": 142120
    },
    {
      "epoch": 7.580266666666667,
      "grad_norm": 0.0887354165315628,
      "learning_rate": 2.6233333333333333e-06,
      "loss": 0.0019,
      "step": 142130
    },
    {
      "epoch": 7.5808,
      "grad_norm": 0.225493386387825,
      "learning_rate": 2.6200000000000003e-06,
      "loss": 0.0015,
      "step": 142140
    },
    {
      "epoch": 7.581333333333333,
      "grad_norm": 0.4007313549518585,
      "learning_rate": 2.616666666666667e-06,
      "loss": 0.0015,
      "step": 142150
    },
    {
      "epoch": 7.5818666666666665,
      "grad_norm": 0.42792654037475586,
      "learning_rate": 2.6133333333333334e-06,
      "loss": 0.0022,
      "step": 142160
    },
    {
      "epoch": 7.5824,
      "grad_norm": 0.37186235189437866,
      "learning_rate": 2.6100000000000004e-06,
      "loss": 0.0029,
      "step": 142170
    },
    {
      "epoch": 7.582933333333333,
      "grad_norm": 0.28635305166244507,
      "learning_rate": 2.6066666666666666e-06,
      "loss": 0.0015,
      "step": 142180
    },
    {
      "epoch": 7.583466666666666,
      "grad_norm": 0.5831036567687988,
      "learning_rate": 2.603333333333333e-06,
      "loss": 0.0022,
      "step": 142190
    },
    {
      "epoch": 7.584,
      "grad_norm": 0.03726257383823395,
      "learning_rate": 2.6e-06,
      "loss": 0.0026,
      "step": 142200
    },
    {
      "epoch": 7.584533333333333,
      "grad_norm": 0.34198448061943054,
      "learning_rate": 2.5966666666666667e-06,
      "loss": 0.0019,
      "step": 142210
    },
    {
      "epoch": 7.585066666666666,
      "grad_norm": 0.11516191810369492,
      "learning_rate": 2.593333333333333e-06,
      "loss": 0.0022,
      "step": 142220
    },
    {
      "epoch": 7.5856,
      "grad_norm": 0.06652531027793884,
      "learning_rate": 2.59e-06,
      "loss": 0.0024,
      "step": 142230
    },
    {
      "epoch": 7.586133333333334,
      "grad_norm": 0.23060855269432068,
      "learning_rate": 2.5866666666666667e-06,
      "loss": 0.002,
      "step": 142240
    },
    {
      "epoch": 7.586666666666667,
      "grad_norm": 0.28766390681266785,
      "learning_rate": 2.5833333333333333e-06,
      "loss": 0.0013,
      "step": 142250
    },
    {
      "epoch": 7.5872,
      "grad_norm": 0.11418479681015015,
      "learning_rate": 2.5800000000000003e-06,
      "loss": 0.0015,
      "step": 142260
    },
    {
      "epoch": 7.587733333333333,
      "grad_norm": 0.17728671431541443,
      "learning_rate": 2.576666666666667e-06,
      "loss": 0.0018,
      "step": 142270
    },
    {
      "epoch": 7.588266666666667,
      "grad_norm": 0.11460933089256287,
      "learning_rate": 2.5733333333333334e-06,
      "loss": 0.0015,
      "step": 142280
    },
    {
      "epoch": 7.5888,
      "grad_norm": 0.028471071273088455,
      "learning_rate": 2.5700000000000004e-06,
      "loss": 0.0016,
      "step": 142290
    },
    {
      "epoch": 7.589333333333333,
      "grad_norm": 0.27498993277549744,
      "learning_rate": 2.566666666666667e-06,
      "loss": 0.0013,
      "step": 142300
    },
    {
      "epoch": 7.5898666666666665,
      "grad_norm": 0.19734445214271545,
      "learning_rate": 2.5633333333333335e-06,
      "loss": 0.0015,
      "step": 142310
    },
    {
      "epoch": 7.5904,
      "grad_norm": 0.039955779910087585,
      "learning_rate": 2.56e-06,
      "loss": 0.0014,
      "step": 142320
    },
    {
      "epoch": 7.590933333333333,
      "grad_norm": 0.25770580768585205,
      "learning_rate": 2.556666666666667e-06,
      "loss": 0.0014,
      "step": 142330
    },
    {
      "epoch": 7.591466666666666,
      "grad_norm": 0.08244547247886658,
      "learning_rate": 2.5533333333333336e-06,
      "loss": 0.0019,
      "step": 142340
    },
    {
      "epoch": 7.592,
      "grad_norm": 0.045311566442251205,
      "learning_rate": 2.55e-06,
      "loss": 0.0022,
      "step": 142350
    },
    {
      "epoch": 7.592533333333334,
      "grad_norm": 0.06425420194864273,
      "learning_rate": 2.5466666666666667e-06,
      "loss": 0.0019,
      "step": 142360
    },
    {
      "epoch": 7.593066666666667,
      "grad_norm": 0.28639718890190125,
      "learning_rate": 2.5433333333333333e-06,
      "loss": 0.0014,
      "step": 142370
    },
    {
      "epoch": 7.5936,
      "grad_norm": 0.09032974392175674,
      "learning_rate": 2.54e-06,
      "loss": 0.0014,
      "step": 142380
    },
    {
      "epoch": 7.594133333333334,
      "grad_norm": 0.14677312970161438,
      "learning_rate": 2.536666666666667e-06,
      "loss": 0.0014,
      "step": 142390
    },
    {
      "epoch": 7.594666666666667,
      "grad_norm": 0.2354981154203415,
      "learning_rate": 2.5333333333333334e-06,
      "loss": 0.0014,
      "step": 142400
    },
    {
      "epoch": 7.5952,
      "grad_norm": 0.13899104297161102,
      "learning_rate": 2.53e-06,
      "loss": 0.0013,
      "step": 142410
    },
    {
      "epoch": 7.5957333333333334,
      "grad_norm": 0.31248998641967773,
      "learning_rate": 2.526666666666667e-06,
      "loss": 0.0015,
      "step": 142420
    },
    {
      "epoch": 7.596266666666667,
      "grad_norm": 0.22758707404136658,
      "learning_rate": 2.5233333333333335e-06,
      "loss": 0.0013,
      "step": 142430
    },
    {
      "epoch": 7.5968,
      "grad_norm": 0.17234602570533752,
      "learning_rate": 2.52e-06,
      "loss": 0.0012,
      "step": 142440
    },
    {
      "epoch": 7.597333333333333,
      "grad_norm": 0.0394921749830246,
      "learning_rate": 2.516666666666667e-06,
      "loss": 0.0016,
      "step": 142450
    },
    {
      "epoch": 7.5978666666666665,
      "grad_norm": 0.14544759690761566,
      "learning_rate": 2.5133333333333336e-06,
      "loss": 0.0015,
      "step": 142460
    },
    {
      "epoch": 7.5984,
      "grad_norm": 0.2611548602581024,
      "learning_rate": 2.51e-06,
      "loss": 0.0013,
      "step": 142470
    },
    {
      "epoch": 7.598933333333333,
      "grad_norm": 0.04642890393733978,
      "learning_rate": 2.506666666666667e-06,
      "loss": 0.0023,
      "step": 142480
    },
    {
      "epoch": 7.599466666666666,
      "grad_norm": 0.2131623476743698,
      "learning_rate": 2.5033333333333336e-06,
      "loss": 0.0015,
      "step": 142490
    },
    {
      "epoch": 7.6,
      "grad_norm": 0.31996333599090576,
      "learning_rate": 2.5e-06,
      "loss": 0.0022,
      "step": 142500
    },
    {
      "epoch": 7.600533333333333,
      "grad_norm": 0.049309052526950836,
      "learning_rate": 2.4966666666666668e-06,
      "loss": 0.0015,
      "step": 142510
    },
    {
      "epoch": 7.601066666666666,
      "grad_norm": 0.06765652447938919,
      "learning_rate": 2.4933333333333333e-06,
      "loss": 0.0025,
      "step": 142520
    },
    {
      "epoch": 7.6016,
      "grad_norm": 0.14547446370124817,
      "learning_rate": 2.49e-06,
      "loss": 0.0015,
      "step": 142530
    },
    {
      "epoch": 7.602133333333334,
      "grad_norm": 0.1800042986869812,
      "learning_rate": 2.486666666666667e-06,
      "loss": 0.0015,
      "step": 142540
    },
    {
      "epoch": 7.602666666666667,
      "grad_norm": 0.03680095449090004,
      "learning_rate": 2.4833333333333334e-06,
      "loss": 0.0016,
      "step": 142550
    },
    {
      "epoch": 7.6032,
      "grad_norm": 0.14311425387859344,
      "learning_rate": 2.48e-06,
      "loss": 0.0013,
      "step": 142560
    },
    {
      "epoch": 7.6037333333333335,
      "grad_norm": 0.10066716372966766,
      "learning_rate": 2.4766666666666665e-06,
      "loss": 0.0014,
      "step": 142570
    },
    {
      "epoch": 7.604266666666667,
      "grad_norm": 0.12035829573869705,
      "learning_rate": 2.4733333333333335e-06,
      "loss": 0.0018,
      "step": 142580
    },
    {
      "epoch": 7.6048,
      "grad_norm": 0.271634966135025,
      "learning_rate": 2.47e-06,
      "loss": 0.0019,
      "step": 142590
    },
    {
      "epoch": 7.605333333333333,
      "grad_norm": 0.07203936576843262,
      "learning_rate": 2.4666666666666666e-06,
      "loss": 0.0017,
      "step": 142600
    },
    {
      "epoch": 7.6058666666666666,
      "grad_norm": 0.04193209111690521,
      "learning_rate": 2.4633333333333336e-06,
      "loss": 0.0013,
      "step": 142610
    },
    {
      "epoch": 7.6064,
      "grad_norm": 0.14319153130054474,
      "learning_rate": 2.46e-06,
      "loss": 0.0013,
      "step": 142620
    },
    {
      "epoch": 7.606933333333333,
      "grad_norm": 0.47707757353782654,
      "learning_rate": 2.4566666666666667e-06,
      "loss": 0.0016,
      "step": 142630
    },
    {
      "epoch": 7.607466666666666,
      "grad_norm": 0.08943873643875122,
      "learning_rate": 2.4533333333333337e-06,
      "loss": 0.0013,
      "step": 142640
    },
    {
      "epoch": 7.608,
      "grad_norm": 0.16834035515785217,
      "learning_rate": 2.4500000000000003e-06,
      "loss": 0.0017,
      "step": 142650
    },
    {
      "epoch": 7.608533333333334,
      "grad_norm": 0.3127967119216919,
      "learning_rate": 2.446666666666667e-06,
      "loss": 0.0025,
      "step": 142660
    },
    {
      "epoch": 7.609066666666667,
      "grad_norm": 0.06709755957126617,
      "learning_rate": 2.4433333333333334e-06,
      "loss": 0.0022,
      "step": 142670
    },
    {
      "epoch": 7.6096,
      "grad_norm": 0.08889642357826233,
      "learning_rate": 2.4400000000000004e-06,
      "loss": 0.0013,
      "step": 142680
    },
    {
      "epoch": 7.610133333333334,
      "grad_norm": 0.20694933831691742,
      "learning_rate": 2.4366666666666665e-06,
      "loss": 0.0014,
      "step": 142690
    },
    {
      "epoch": 7.610666666666667,
      "grad_norm": 0.14266502857208252,
      "learning_rate": 2.4333333333333335e-06,
      "loss": 0.0028,
      "step": 142700
    },
    {
      "epoch": 7.6112,
      "grad_norm": 0.03619130328297615,
      "learning_rate": 2.43e-06,
      "loss": 0.0019,
      "step": 142710
    },
    {
      "epoch": 7.6117333333333335,
      "grad_norm": 0.11549639701843262,
      "learning_rate": 2.4266666666666666e-06,
      "loss": 0.0016,
      "step": 142720
    },
    {
      "epoch": 7.612266666666667,
      "grad_norm": 0.2795295715332031,
      "learning_rate": 2.4233333333333336e-06,
      "loss": 0.0017,
      "step": 142730
    },
    {
      "epoch": 7.6128,
      "grad_norm": 0.11394638568162918,
      "learning_rate": 2.42e-06,
      "loss": 0.0018,
      "step": 142740
    },
    {
      "epoch": 7.613333333333333,
      "grad_norm": 0.2202034443616867,
      "learning_rate": 2.4166666666666667e-06,
      "loss": 0.0017,
      "step": 142750
    },
    {
      "epoch": 7.613866666666667,
      "grad_norm": 0.08579806238412857,
      "learning_rate": 2.4133333333333332e-06,
      "loss": 0.0017,
      "step": 142760
    },
    {
      "epoch": 7.6144,
      "grad_norm": 0.36589425802230835,
      "learning_rate": 2.4100000000000002e-06,
      "loss": 0.0023,
      "step": 142770
    },
    {
      "epoch": 7.614933333333333,
      "grad_norm": 0.19973009824752808,
      "learning_rate": 2.4066666666666668e-06,
      "loss": 0.0015,
      "step": 142780
    },
    {
      "epoch": 7.615466666666666,
      "grad_norm": 0.1265777200460434,
      "learning_rate": 2.4033333333333333e-06,
      "loss": 0.0021,
      "step": 142790
    },
    {
      "epoch": 7.616,
      "grad_norm": 0.1784275323152542,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 0.002,
      "step": 142800
    },
    {
      "epoch": 7.616533333333333,
      "grad_norm": 0.18030166625976562,
      "learning_rate": 2.396666666666667e-06,
      "loss": 0.0028,
      "step": 142810
    },
    {
      "epoch": 7.617066666666666,
      "grad_norm": 0.44228416681289673,
      "learning_rate": 2.3933333333333334e-06,
      "loss": 0.0027,
      "step": 142820
    },
    {
      "epoch": 7.6176,
      "grad_norm": 0.06294631212949753,
      "learning_rate": 2.3900000000000004e-06,
      "loss": 0.0011,
      "step": 142830
    },
    {
      "epoch": 7.618133333333334,
      "grad_norm": 0.285715252161026,
      "learning_rate": 2.386666666666667e-06,
      "loss": 0.0021,
      "step": 142840
    },
    {
      "epoch": 7.618666666666667,
      "grad_norm": 0.5117249488830566,
      "learning_rate": 2.3833333333333335e-06,
      "loss": 0.0024,
      "step": 142850
    },
    {
      "epoch": 7.6192,
      "grad_norm": 0.06976775825023651,
      "learning_rate": 2.38e-06,
      "loss": 0.0012,
      "step": 142860
    },
    {
      "epoch": 7.6197333333333335,
      "grad_norm": 0.0804387554526329,
      "learning_rate": 2.3766666666666666e-06,
      "loss": 0.0013,
      "step": 142870
    },
    {
      "epoch": 7.620266666666667,
      "grad_norm": 0.06966640055179596,
      "learning_rate": 2.373333333333333e-06,
      "loss": 0.0016,
      "step": 142880
    },
    {
      "epoch": 7.6208,
      "grad_norm": 0.34298694133758545,
      "learning_rate": 2.37e-06,
      "loss": 0.0012,
      "step": 142890
    },
    {
      "epoch": 7.621333333333333,
      "grad_norm": 0.23725223541259766,
      "learning_rate": 2.3666666666666667e-06,
      "loss": 0.0019,
      "step": 142900
    },
    {
      "epoch": 7.621866666666667,
      "grad_norm": 0.24721543490886688,
      "learning_rate": 2.3633333333333333e-06,
      "loss": 0.0026,
      "step": 142910
    },
    {
      "epoch": 7.6224,
      "grad_norm": 0.285503089427948,
      "learning_rate": 2.36e-06,
      "loss": 0.0027,
      "step": 142920
    },
    {
      "epoch": 7.622933333333333,
      "grad_norm": 0.4079558551311493,
      "learning_rate": 2.356666666666667e-06,
      "loss": 0.0021,
      "step": 142930
    },
    {
      "epoch": 7.623466666666666,
      "grad_norm": 0.19386273622512817,
      "learning_rate": 2.3533333333333334e-06,
      "loss": 0.0022,
      "step": 142940
    },
    {
      "epoch": 7.624,
      "grad_norm": 0.11771325767040253,
      "learning_rate": 2.35e-06,
      "loss": 0.0025,
      "step": 142950
    },
    {
      "epoch": 7.624533333333334,
      "grad_norm": 0.06133006885647774,
      "learning_rate": 2.346666666666667e-06,
      "loss": 0.0013,
      "step": 142960
    },
    {
      "epoch": 7.625066666666667,
      "grad_norm": 0.08608250319957733,
      "learning_rate": 2.3433333333333335e-06,
      "loss": 0.002,
      "step": 142970
    },
    {
      "epoch": 7.6256,
      "grad_norm": 0.0799722746014595,
      "learning_rate": 2.34e-06,
      "loss": 0.0023,
      "step": 142980
    },
    {
      "epoch": 7.626133333333334,
      "grad_norm": 0.04960714280605316,
      "learning_rate": 2.336666666666667e-06,
      "loss": 0.0021,
      "step": 142990
    },
    {
      "epoch": 7.626666666666667,
      "grad_norm": 0.12069332599639893,
      "learning_rate": 2.3333333333333336e-06,
      "loss": 0.0013,
      "step": 143000
    },
    {
      "epoch": 7.6272,
      "grad_norm": 0.09656082838773727,
      "learning_rate": 2.33e-06,
      "loss": 0.0017,
      "step": 143010
    },
    {
      "epoch": 7.6277333333333335,
      "grad_norm": 0.47807881236076355,
      "learning_rate": 2.326666666666667e-06,
      "loss": 0.0013,
      "step": 143020
    },
    {
      "epoch": 7.628266666666667,
      "grad_norm": 0.2284577637910843,
      "learning_rate": 2.3233333333333337e-06,
      "loss": 0.0019,
      "step": 143030
    },
    {
      "epoch": 7.6288,
      "grad_norm": 0.28968068957328796,
      "learning_rate": 2.32e-06,
      "loss": 0.0014,
      "step": 143040
    },
    {
      "epoch": 7.629333333333333,
      "grad_norm": 0.31474387645721436,
      "learning_rate": 2.316666666666667e-06,
      "loss": 0.0018,
      "step": 143050
    },
    {
      "epoch": 7.629866666666667,
      "grad_norm": 0.1717609316110611,
      "learning_rate": 2.3133333333333333e-06,
      "loss": 0.0015,
      "step": 143060
    },
    {
      "epoch": 7.6304,
      "grad_norm": 0.24054601788520813,
      "learning_rate": 2.31e-06,
      "loss": 0.0017,
      "step": 143070
    },
    {
      "epoch": 7.630933333333333,
      "grad_norm": 0.0976743996143341,
      "learning_rate": 2.306666666666667e-06,
      "loss": 0.002,
      "step": 143080
    },
    {
      "epoch": 7.631466666666666,
      "grad_norm": 0.2671963572502136,
      "learning_rate": 2.3033333333333334e-06,
      "loss": 0.0017,
      "step": 143090
    },
    {
      "epoch": 7.632,
      "grad_norm": 0.03104284591972828,
      "learning_rate": 2.3e-06,
      "loss": 0.0015,
      "step": 143100
    },
    {
      "epoch": 7.632533333333333,
      "grad_norm": 0.2268785834312439,
      "learning_rate": 2.2966666666666666e-06,
      "loss": 0.0018,
      "step": 143110
    },
    {
      "epoch": 7.633066666666666,
      "grad_norm": 0.32504332065582275,
      "learning_rate": 2.2933333333333335e-06,
      "loss": 0.0016,
      "step": 143120
    },
    {
      "epoch": 7.6336,
      "grad_norm": 0.3630244731903076,
      "learning_rate": 2.29e-06,
      "loss": 0.002,
      "step": 143130
    },
    {
      "epoch": 7.634133333333334,
      "grad_norm": 0.14174982905387878,
      "learning_rate": 2.2866666666666667e-06,
      "loss": 0.002,
      "step": 143140
    },
    {
      "epoch": 7.634666666666667,
      "grad_norm": 0.4176516532897949,
      "learning_rate": 2.2833333333333336e-06,
      "loss": 0.0015,
      "step": 143150
    },
    {
      "epoch": 7.6352,
      "grad_norm": 0.22847358882427216,
      "learning_rate": 2.28e-06,
      "loss": 0.0012,
      "step": 143160
    },
    {
      "epoch": 7.6357333333333335,
      "grad_norm": 0.1389385163784027,
      "learning_rate": 2.2766666666666668e-06,
      "loss": 0.0017,
      "step": 143170
    },
    {
      "epoch": 7.636266666666667,
      "grad_norm": 0.3387455642223358,
      "learning_rate": 2.2733333333333337e-06,
      "loss": 0.0024,
      "step": 143180
    },
    {
      "epoch": 7.6368,
      "grad_norm": 0.16021916270256042,
      "learning_rate": 2.2700000000000003e-06,
      "loss": 0.0024,
      "step": 143190
    },
    {
      "epoch": 7.637333333333333,
      "grad_norm": 0.1720750331878662,
      "learning_rate": 2.266666666666667e-06,
      "loss": 0.0016,
      "step": 143200
    },
    {
      "epoch": 7.637866666666667,
      "grad_norm": 0.0655011385679245,
      "learning_rate": 2.2633333333333334e-06,
      "loss": 0.0021,
      "step": 143210
    },
    {
      "epoch": 7.6384,
      "grad_norm": 0.5161683559417725,
      "learning_rate": 2.26e-06,
      "loss": 0.0024,
      "step": 143220
    },
    {
      "epoch": 7.638933333333333,
      "grad_norm": 0.24665763974189758,
      "learning_rate": 2.2566666666666665e-06,
      "loss": 0.0019,
      "step": 143230
    },
    {
      "epoch": 7.639466666666666,
      "grad_norm": 0.20217184722423553,
      "learning_rate": 2.2533333333333335e-06,
      "loss": 0.0015,
      "step": 143240
    },
    {
      "epoch": 7.64,
      "grad_norm": 0.046367347240448,
      "learning_rate": 2.25e-06,
      "loss": 0.0022,
      "step": 143250
    },
    {
      "epoch": 7.640533333333333,
      "grad_norm": 0.2351783812046051,
      "learning_rate": 2.2466666666666666e-06,
      "loss": 0.0017,
      "step": 143260
    },
    {
      "epoch": 7.641066666666667,
      "grad_norm": 0.5125253200531006,
      "learning_rate": 2.2433333333333336e-06,
      "loss": 0.0021,
      "step": 143270
    },
    {
      "epoch": 7.6416,
      "grad_norm": 0.2036123424768448,
      "learning_rate": 2.24e-06,
      "loss": 0.0015,
      "step": 143280
    },
    {
      "epoch": 7.642133333333334,
      "grad_norm": 0.22492246329784393,
      "learning_rate": 2.2366666666666667e-06,
      "loss": 0.0017,
      "step": 143290
    },
    {
      "epoch": 7.642666666666667,
      "grad_norm": 0.05860668793320656,
      "learning_rate": 2.2333333333333333e-06,
      "loss": 0.0018,
      "step": 143300
    },
    {
      "epoch": 7.6432,
      "grad_norm": 0.09592683613300323,
      "learning_rate": 2.2300000000000002e-06,
      "loss": 0.0012,
      "step": 143310
    },
    {
      "epoch": 7.6437333333333335,
      "grad_norm": 0.09253338724374771,
      "learning_rate": 2.226666666666667e-06,
      "loss": 0.0011,
      "step": 143320
    },
    {
      "epoch": 7.644266666666667,
      "grad_norm": 0.17053619027137756,
      "learning_rate": 2.2233333333333334e-06,
      "loss": 0.0014,
      "step": 143330
    },
    {
      "epoch": 7.6448,
      "grad_norm": 0.4513291120529175,
      "learning_rate": 2.2200000000000003e-06,
      "loss": 0.0015,
      "step": 143340
    },
    {
      "epoch": 7.645333333333333,
      "grad_norm": 0.08160296827554703,
      "learning_rate": 2.216666666666667e-06,
      "loss": 0.0019,
      "step": 143350
    },
    {
      "epoch": 7.645866666666667,
      "grad_norm": 0.06586652249097824,
      "learning_rate": 2.2133333333333335e-06,
      "loss": 0.0014,
      "step": 143360
    },
    {
      "epoch": 7.6464,
      "grad_norm": 0.1437481790781021,
      "learning_rate": 2.2100000000000004e-06,
      "loss": 0.0014,
      "step": 143370
    },
    {
      "epoch": 7.646933333333333,
      "grad_norm": 0.39726272225379944,
      "learning_rate": 2.2066666666666666e-06,
      "loss": 0.0024,
      "step": 143380
    },
    {
      "epoch": 7.647466666666666,
      "grad_norm": 0.2867262065410614,
      "learning_rate": 2.203333333333333e-06,
      "loss": 0.0012,
      "step": 143390
    },
    {
      "epoch": 7.648,
      "grad_norm": 0.08698763698339462,
      "learning_rate": 2.2e-06,
      "loss": 0.0017,
      "step": 143400
    },
    {
      "epoch": 7.648533333333333,
      "grad_norm": 0.29128074645996094,
      "learning_rate": 2.1966666666666667e-06,
      "loss": 0.0016,
      "step": 143410
    },
    {
      "epoch": 7.649066666666666,
      "grad_norm": 0.028618985787034035,
      "learning_rate": 2.1933333333333332e-06,
      "loss": 0.0027,
      "step": 143420
    },
    {
      "epoch": 7.6495999999999995,
      "grad_norm": 0.1772230714559555,
      "learning_rate": 2.19e-06,
      "loss": 0.0018,
      "step": 143430
    },
    {
      "epoch": 7.650133333333334,
      "grad_norm": 0.1732906699180603,
      "learning_rate": 2.1866666666666668e-06,
      "loss": 0.0013,
      "step": 143440
    },
    {
      "epoch": 7.650666666666667,
      "grad_norm": 0.06804511696100235,
      "learning_rate": 2.1833333333333333e-06,
      "loss": 0.0022,
      "step": 143450
    },
    {
      "epoch": 7.6512,
      "grad_norm": 0.2294234335422516,
      "learning_rate": 2.1800000000000003e-06,
      "loss": 0.0017,
      "step": 143460
    },
    {
      "epoch": 7.6517333333333335,
      "grad_norm": 0.16840484738349915,
      "learning_rate": 2.176666666666667e-06,
      "loss": 0.0012,
      "step": 143470
    },
    {
      "epoch": 7.652266666666667,
      "grad_norm": 0.0937967598438263,
      "learning_rate": 2.1733333333333334e-06,
      "loss": 0.0014,
      "step": 143480
    },
    {
      "epoch": 7.6528,
      "grad_norm": 0.11643306910991669,
      "learning_rate": 2.17e-06,
      "loss": 0.0015,
      "step": 143490
    },
    {
      "epoch": 7.653333333333333,
      "grad_norm": 0.2856544256210327,
      "learning_rate": 2.166666666666667e-06,
      "loss": 0.0015,
      "step": 143500
    },
    {
      "epoch": 7.653866666666667,
      "grad_norm": 0.04155905544757843,
      "learning_rate": 2.1633333333333335e-06,
      "loss": 0.0024,
      "step": 143510
    },
    {
      "epoch": 7.6544,
      "grad_norm": 0.13260632753372192,
      "learning_rate": 2.16e-06,
      "loss": 0.002,
      "step": 143520
    },
    {
      "epoch": 7.654933333333333,
      "grad_norm": 0.2777310013771057,
      "learning_rate": 2.156666666666667e-06,
      "loss": 0.0014,
      "step": 143530
    },
    {
      "epoch": 7.655466666666666,
      "grad_norm": 0.17548157274723053,
      "learning_rate": 2.1533333333333336e-06,
      "loss": 0.0023,
      "step": 143540
    },
    {
      "epoch": 7.656,
      "grad_norm": 0.28552109003067017,
      "learning_rate": 2.1499999999999997e-06,
      "loss": 0.0017,
      "step": 143550
    },
    {
      "epoch": 7.656533333333333,
      "grad_norm": 0.2848582863807678,
      "learning_rate": 2.1466666666666667e-06,
      "loss": 0.0016,
      "step": 143560
    },
    {
      "epoch": 7.657066666666667,
      "grad_norm": 0.5268044471740723,
      "learning_rate": 2.1433333333333333e-06,
      "loss": 0.0024,
      "step": 143570
    },
    {
      "epoch": 7.6576,
      "grad_norm": 0.17700016498565674,
      "learning_rate": 2.14e-06,
      "loss": 0.0014,
      "step": 143580
    },
    {
      "epoch": 7.658133333333334,
      "grad_norm": 0.04038684815168381,
      "learning_rate": 2.136666666666667e-06,
      "loss": 0.0019,
      "step": 143590
    },
    {
      "epoch": 7.658666666666667,
      "grad_norm": 0.07494540512561798,
      "learning_rate": 2.1333333333333334e-06,
      "loss": 0.0025,
      "step": 143600
    },
    {
      "epoch": 7.6592,
      "grad_norm": 0.14584749937057495,
      "learning_rate": 2.13e-06,
      "loss": 0.0011,
      "step": 143610
    },
    {
      "epoch": 7.6597333333333335,
      "grad_norm": 0.03735138475894928,
      "learning_rate": 2.126666666666667e-06,
      "loss": 0.0019,
      "step": 143620
    },
    {
      "epoch": 7.660266666666667,
      "grad_norm": 0.08992130309343338,
      "learning_rate": 2.1233333333333335e-06,
      "loss": 0.0017,
      "step": 143630
    },
    {
      "epoch": 7.6608,
      "grad_norm": 0.1428966224193573,
      "learning_rate": 2.12e-06,
      "loss": 0.0014,
      "step": 143640
    },
    {
      "epoch": 7.661333333333333,
      "grad_norm": 0.053253479301929474,
      "learning_rate": 2.1166666666666666e-06,
      "loss": 0.0019,
      "step": 143650
    },
    {
      "epoch": 7.661866666666667,
      "grad_norm": 0.22699585556983948,
      "learning_rate": 2.1133333333333336e-06,
      "loss": 0.0018,
      "step": 143660
    },
    {
      "epoch": 7.6624,
      "grad_norm": 0.028741290792822838,
      "learning_rate": 2.11e-06,
      "loss": 0.0018,
      "step": 143670
    },
    {
      "epoch": 7.662933333333333,
      "grad_norm": 0.10668160021305084,
      "learning_rate": 2.1066666666666667e-06,
      "loss": 0.0021,
      "step": 143680
    },
    {
      "epoch": 7.663466666666666,
      "grad_norm": 0.08056136965751648,
      "learning_rate": 2.1033333333333337e-06,
      "loss": 0.0012,
      "step": 143690
    },
    {
      "epoch": 7.664,
      "grad_norm": 0.041671838611364365,
      "learning_rate": 2.1000000000000002e-06,
      "loss": 0.0019,
      "step": 143700
    },
    {
      "epoch": 7.664533333333333,
      "grad_norm": 0.04479381442070007,
      "learning_rate": 2.0966666666666668e-06,
      "loss": 0.002,
      "step": 143710
    },
    {
      "epoch": 7.665066666666666,
      "grad_norm": 0.04487219825387001,
      "learning_rate": 2.0933333333333338e-06,
      "loss": 0.0017,
      "step": 143720
    },
    {
      "epoch": 7.6655999999999995,
      "grad_norm": 0.25196418166160583,
      "learning_rate": 2.09e-06,
      "loss": 0.0022,
      "step": 143730
    },
    {
      "epoch": 7.666133333333334,
      "grad_norm": 0.28783589601516724,
      "learning_rate": 2.0866666666666665e-06,
      "loss": 0.0013,
      "step": 143740
    },
    {
      "epoch": 7.666666666666667,
      "grad_norm": 0.05474796146154404,
      "learning_rate": 2.0833333333333334e-06,
      "loss": 0.0023,
      "step": 143750
    },
    {
      "epoch": 7.6672,
      "grad_norm": 0.12721361219882965,
      "learning_rate": 2.08e-06,
      "loss": 0.0013,
      "step": 143760
    },
    {
      "epoch": 7.6677333333333335,
      "grad_norm": 0.20321594178676605,
      "learning_rate": 2.0766666666666665e-06,
      "loss": 0.0021,
      "step": 143770
    },
    {
      "epoch": 7.668266666666667,
      "grad_norm": 0.049147285521030426,
      "learning_rate": 2.0733333333333335e-06,
      "loss": 0.0021,
      "step": 143780
    },
    {
      "epoch": 7.6688,
      "grad_norm": 0.05054556205868721,
      "learning_rate": 2.07e-06,
      "loss": 0.0016,
      "step": 143790
    },
    {
      "epoch": 7.669333333333333,
      "grad_norm": 0.17937235534191132,
      "learning_rate": 2.0666666666666666e-06,
      "loss": 0.0019,
      "step": 143800
    },
    {
      "epoch": 7.669866666666667,
      "grad_norm": 0.16566680371761322,
      "learning_rate": 2.0633333333333336e-06,
      "loss": 0.0016,
      "step": 143810
    },
    {
      "epoch": 7.6704,
      "grad_norm": 0.3450734615325928,
      "learning_rate": 2.06e-06,
      "loss": 0.0014,
      "step": 143820
    },
    {
      "epoch": 7.670933333333333,
      "grad_norm": 0.2872856855392456,
      "learning_rate": 2.0566666666666667e-06,
      "loss": 0.0021,
      "step": 143830
    },
    {
      "epoch": 7.671466666666666,
      "grad_norm": 0.2280447632074356,
      "learning_rate": 2.0533333333333333e-06,
      "loss": 0.0023,
      "step": 143840
    },
    {
      "epoch": 7.672,
      "grad_norm": 0.14309075474739075,
      "learning_rate": 2.0500000000000003e-06,
      "loss": 0.0017,
      "step": 143850
    },
    {
      "epoch": 7.672533333333333,
      "grad_norm": 0.17804671823978424,
      "learning_rate": 2.046666666666667e-06,
      "loss": 0.0014,
      "step": 143860
    },
    {
      "epoch": 7.673066666666667,
      "grad_norm": 0.23553863167762756,
      "learning_rate": 2.0433333333333334e-06,
      "loss": 0.0014,
      "step": 143870
    },
    {
      "epoch": 7.6736,
      "grad_norm": 0.0427735298871994,
      "learning_rate": 2.0400000000000004e-06,
      "loss": 0.002,
      "step": 143880
    },
    {
      "epoch": 7.674133333333334,
      "grad_norm": 0.06424976140260696,
      "learning_rate": 2.036666666666667e-06,
      "loss": 0.002,
      "step": 143890
    },
    {
      "epoch": 7.674666666666667,
      "grad_norm": 0.11960268765687943,
      "learning_rate": 2.033333333333333e-06,
      "loss": 0.0012,
      "step": 143900
    },
    {
      "epoch": 7.6752,
      "grad_norm": 0.22666971385478973,
      "learning_rate": 2.03e-06,
      "loss": 0.0023,
      "step": 143910
    },
    {
      "epoch": 7.6757333333333335,
      "grad_norm": 0.25460126996040344,
      "learning_rate": 2.0266666666666666e-06,
      "loss": 0.0012,
      "step": 143920
    },
    {
      "epoch": 7.676266666666667,
      "grad_norm": 0.10272962599992752,
      "learning_rate": 2.023333333333333e-06,
      "loss": 0.0028,
      "step": 143930
    },
    {
      "epoch": 7.6768,
      "grad_norm": 0.5259761810302734,
      "learning_rate": 2.02e-06,
      "loss": 0.0019,
      "step": 143940
    },
    {
      "epoch": 7.677333333333333,
      "grad_norm": 0.19986149668693542,
      "learning_rate": 2.0166666666666667e-06,
      "loss": 0.002,
      "step": 143950
    },
    {
      "epoch": 7.677866666666667,
      "grad_norm": 0.14730362594127655,
      "learning_rate": 2.0133333333333333e-06,
      "loss": 0.0015,
      "step": 143960
    },
    {
      "epoch": 7.6784,
      "grad_norm": 0.18202723562717438,
      "learning_rate": 2.0100000000000002e-06,
      "loss": 0.0022,
      "step": 143970
    },
    {
      "epoch": 7.678933333333333,
      "grad_norm": 0.044163621962070465,
      "learning_rate": 2.006666666666667e-06,
      "loss": 0.0016,
      "step": 143980
    },
    {
      "epoch": 7.679466666666666,
      "grad_norm": 0.1569776087999344,
      "learning_rate": 2.0033333333333334e-06,
      "loss": 0.0013,
      "step": 143990
    },
    {
      "epoch": 7.68,
      "grad_norm": 0.0611455962061882,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.0012,
      "step": 144000
    },
    {
      "epoch": 7.680533333333333,
      "grad_norm": 0.31329265236854553,
      "learning_rate": 1.996666666666667e-06,
      "loss": 0.0016,
      "step": 144010
    },
    {
      "epoch": 7.681066666666666,
      "grad_norm": 0.29085806012153625,
      "learning_rate": 1.9933333333333334e-06,
      "loss": 0.0015,
      "step": 144020
    },
    {
      "epoch": 7.6815999999999995,
      "grad_norm": 0.07077126204967499,
      "learning_rate": 1.99e-06,
      "loss": 0.0025,
      "step": 144030
    },
    {
      "epoch": 7.682133333333334,
      "grad_norm": 0.12187983095645905,
      "learning_rate": 1.986666666666667e-06,
      "loss": 0.0021,
      "step": 144040
    },
    {
      "epoch": 7.682666666666667,
      "grad_norm": 0.10486139357089996,
      "learning_rate": 1.9833333333333335e-06,
      "loss": 0.0017,
      "step": 144050
    },
    {
      "epoch": 7.6832,
      "grad_norm": 0.04766714200377464,
      "learning_rate": 1.98e-06,
      "loss": 0.0013,
      "step": 144060
    },
    {
      "epoch": 7.6837333333333335,
      "grad_norm": 0.39296385645866394,
      "learning_rate": 1.9766666666666667e-06,
      "loss": 0.0024,
      "step": 144070
    },
    {
      "epoch": 7.684266666666667,
      "grad_norm": 0.11886656284332275,
      "learning_rate": 1.9733333333333332e-06,
      "loss": 0.0027,
      "step": 144080
    },
    {
      "epoch": 7.6848,
      "grad_norm": 0.04680055007338524,
      "learning_rate": 1.9699999999999998e-06,
      "loss": 0.0023,
      "step": 144090
    },
    {
      "epoch": 7.685333333333333,
      "grad_norm": 0.14128383994102478,
      "learning_rate": 1.9666666666666668e-06,
      "loss": 0.0014,
      "step": 144100
    },
    {
      "epoch": 7.685866666666667,
      "grad_norm": 0.05879482626914978,
      "learning_rate": 1.9633333333333333e-06,
      "loss": 0.0019,
      "step": 144110
    },
    {
      "epoch": 7.6864,
      "grad_norm": 0.24881626665592194,
      "learning_rate": 1.96e-06,
      "loss": 0.0018,
      "step": 144120
    },
    {
      "epoch": 7.686933333333333,
      "grad_norm": 0.20131221413612366,
      "learning_rate": 1.956666666666667e-06,
      "loss": 0.0016,
      "step": 144130
    },
    {
      "epoch": 7.6874666666666664,
      "grad_norm": 0.45472100377082825,
      "learning_rate": 1.9533333333333334e-06,
      "loss": 0.0013,
      "step": 144140
    },
    {
      "epoch": 7.688,
      "grad_norm": 0.14306405186653137,
      "learning_rate": 1.95e-06,
      "loss": 0.002,
      "step": 144150
    },
    {
      "epoch": 7.688533333333333,
      "grad_norm": 0.34509730339050293,
      "learning_rate": 1.946666666666667e-06,
      "loss": 0.0015,
      "step": 144160
    },
    {
      "epoch": 7.689066666666667,
      "grad_norm": 0.1690094769001007,
      "learning_rate": 1.9433333333333335e-06,
      "loss": 0.0021,
      "step": 144170
    },
    {
      "epoch": 7.6896,
      "grad_norm": 0.18501324951648712,
      "learning_rate": 1.94e-06,
      "loss": 0.002,
      "step": 144180
    },
    {
      "epoch": 7.690133333333334,
      "grad_norm": 0.1692625880241394,
      "learning_rate": 1.936666666666667e-06,
      "loss": 0.0023,
      "step": 144190
    },
    {
      "epoch": 7.690666666666667,
      "grad_norm": 0.2306821346282959,
      "learning_rate": 1.9333333333333336e-06,
      "loss": 0.0024,
      "step": 144200
    },
    {
      "epoch": 7.6912,
      "grad_norm": 0.22675460577011108,
      "learning_rate": 1.93e-06,
      "loss": 0.0013,
      "step": 144210
    },
    {
      "epoch": 7.6917333333333335,
      "grad_norm": 0.16933247447013855,
      "learning_rate": 1.9266666666666667e-06,
      "loss": 0.0012,
      "step": 144220
    },
    {
      "epoch": 7.692266666666667,
      "grad_norm": 0.20135733485221863,
      "learning_rate": 1.9233333333333337e-06,
      "loss": 0.0014,
      "step": 144230
    },
    {
      "epoch": 7.6928,
      "grad_norm": 0.4490850269794464,
      "learning_rate": 1.92e-06,
      "loss": 0.0022,
      "step": 144240
    },
    {
      "epoch": 7.693333333333333,
      "grad_norm": 0.06329582631587982,
      "learning_rate": 1.916666666666667e-06,
      "loss": 0.0021,
      "step": 144250
    },
    {
      "epoch": 7.693866666666667,
      "grad_norm": 0.16841745376586914,
      "learning_rate": 1.9133333333333334e-06,
      "loss": 0.0015,
      "step": 144260
    },
    {
      "epoch": 7.6944,
      "grad_norm": 0.23624640703201294,
      "learning_rate": 1.91e-06,
      "loss": 0.0012,
      "step": 144270
    },
    {
      "epoch": 7.694933333333333,
      "grad_norm": 0.22804541885852814,
      "learning_rate": 1.9066666666666667e-06,
      "loss": 0.0013,
      "step": 144280
    },
    {
      "epoch": 7.6954666666666665,
      "grad_norm": 0.3357252776622772,
      "learning_rate": 1.9033333333333335e-06,
      "loss": 0.002,
      "step": 144290
    },
    {
      "epoch": 7.696,
      "grad_norm": 0.0712977722287178,
      "learning_rate": 1.9e-06,
      "loss": 0.002,
      "step": 144300
    },
    {
      "epoch": 7.696533333333333,
      "grad_norm": 0.11596311628818512,
      "learning_rate": 1.8966666666666668e-06,
      "loss": 0.0015,
      "step": 144310
    },
    {
      "epoch": 7.697066666666666,
      "grad_norm": 0.14637324213981628,
      "learning_rate": 1.8933333333333333e-06,
      "loss": 0.0016,
      "step": 144320
    },
    {
      "epoch": 7.6975999999999996,
      "grad_norm": 0.2534942626953125,
      "learning_rate": 1.8900000000000001e-06,
      "loss": 0.0017,
      "step": 144330
    },
    {
      "epoch": 7.698133333333334,
      "grad_norm": 0.06914522498846054,
      "learning_rate": 1.8866666666666669e-06,
      "loss": 0.0018,
      "step": 144340
    },
    {
      "epoch": 7.698666666666667,
      "grad_norm": 0.06259476393461227,
      "learning_rate": 1.8833333333333334e-06,
      "loss": 0.0019,
      "step": 144350
    },
    {
      "epoch": 7.6992,
      "grad_norm": 0.06774432212114334,
      "learning_rate": 1.8800000000000002e-06,
      "loss": 0.0012,
      "step": 144360
    },
    {
      "epoch": 7.6997333333333335,
      "grad_norm": 0.08949316293001175,
      "learning_rate": 1.8766666666666668e-06,
      "loss": 0.0014,
      "step": 144370
    },
    {
      "epoch": 7.700266666666667,
      "grad_norm": 0.09701023995876312,
      "learning_rate": 1.8733333333333335e-06,
      "loss": 0.0023,
      "step": 144380
    },
    {
      "epoch": 7.7008,
      "grad_norm": 0.07555165886878967,
      "learning_rate": 1.8700000000000003e-06,
      "loss": 0.002,
      "step": 144390
    },
    {
      "epoch": 7.701333333333333,
      "grad_norm": 0.22746968269348145,
      "learning_rate": 1.8666666666666669e-06,
      "loss": 0.0016,
      "step": 144400
    },
    {
      "epoch": 7.701866666666667,
      "grad_norm": 0.30999675393104553,
      "learning_rate": 1.8633333333333332e-06,
      "loss": 0.0024,
      "step": 144410
    },
    {
      "epoch": 7.7024,
      "grad_norm": 0.14300361275672913,
      "learning_rate": 1.86e-06,
      "loss": 0.0012,
      "step": 144420
    },
    {
      "epoch": 7.702933333333333,
      "grad_norm": 0.2260974943637848,
      "learning_rate": 1.8566666666666665e-06,
      "loss": 0.0014,
      "step": 144430
    },
    {
      "epoch": 7.7034666666666665,
      "grad_norm": 0.06760275363922119,
      "learning_rate": 1.8533333333333333e-06,
      "loss": 0.0027,
      "step": 144440
    },
    {
      "epoch": 7.704,
      "grad_norm": 0.48863670229911804,
      "learning_rate": 1.85e-06,
      "loss": 0.0012,
      "step": 144450
    },
    {
      "epoch": 7.704533333333333,
      "grad_norm": 0.19944415986537933,
      "learning_rate": 1.8466666666666666e-06,
      "loss": 0.0016,
      "step": 144460
    },
    {
      "epoch": 7.705066666666666,
      "grad_norm": 0.09169246256351471,
      "learning_rate": 1.8433333333333334e-06,
      "loss": 0.0015,
      "step": 144470
    },
    {
      "epoch": 7.7056000000000004,
      "grad_norm": 0.47971901297569275,
      "learning_rate": 1.84e-06,
      "loss": 0.0016,
      "step": 144480
    },
    {
      "epoch": 7.706133333333334,
      "grad_norm": 0.061187997460365295,
      "learning_rate": 1.8366666666666667e-06,
      "loss": 0.0015,
      "step": 144490
    },
    {
      "epoch": 7.706666666666667,
      "grad_norm": 0.14537711441516876,
      "learning_rate": 1.8333333333333335e-06,
      "loss": 0.0017,
      "step": 144500
    },
    {
      "epoch": 7.7072,
      "grad_norm": 0.02823876403272152,
      "learning_rate": 1.83e-06,
      "loss": 0.0016,
      "step": 144510
    },
    {
      "epoch": 7.7077333333333335,
      "grad_norm": 0.11308430135250092,
      "learning_rate": 1.8266666666666668e-06,
      "loss": 0.0025,
      "step": 144520
    },
    {
      "epoch": 7.708266666666667,
      "grad_norm": 0.258162260055542,
      "learning_rate": 1.8233333333333336e-06,
      "loss": 0.0015,
      "step": 144530
    },
    {
      "epoch": 7.7088,
      "grad_norm": 0.12426627427339554,
      "learning_rate": 1.8200000000000002e-06,
      "loss": 0.0014,
      "step": 144540
    },
    {
      "epoch": 7.709333333333333,
      "grad_norm": 0.12994053959846497,
      "learning_rate": 1.816666666666667e-06,
      "loss": 0.0017,
      "step": 144550
    },
    {
      "epoch": 7.709866666666667,
      "grad_norm": 0.09377812594175339,
      "learning_rate": 1.8133333333333335e-06,
      "loss": 0.0018,
      "step": 144560
    },
    {
      "epoch": 7.7104,
      "grad_norm": 0.09651175141334534,
      "learning_rate": 1.8100000000000002e-06,
      "loss": 0.0021,
      "step": 144570
    },
    {
      "epoch": 7.710933333333333,
      "grad_norm": 0.03364238888025284,
      "learning_rate": 1.806666666666667e-06,
      "loss": 0.0026,
      "step": 144580
    },
    {
      "epoch": 7.7114666666666665,
      "grad_norm": 0.22466351091861725,
      "learning_rate": 1.8033333333333334e-06,
      "loss": 0.0017,
      "step": 144590
    },
    {
      "epoch": 7.712,
      "grad_norm": 0.09794865548610687,
      "learning_rate": 1.8e-06,
      "loss": 0.0024,
      "step": 144600
    },
    {
      "epoch": 7.712533333333333,
      "grad_norm": 0.03700258210301399,
      "learning_rate": 1.7966666666666667e-06,
      "loss": 0.0015,
      "step": 144610
    },
    {
      "epoch": 7.713066666666666,
      "grad_norm": 0.19655419886112213,
      "learning_rate": 1.7933333333333332e-06,
      "loss": 0.0027,
      "step": 144620
    },
    {
      "epoch": 7.7136,
      "grad_norm": 0.1918891966342926,
      "learning_rate": 1.79e-06,
      "loss": 0.0018,
      "step": 144630
    },
    {
      "epoch": 7.714133333333333,
      "grad_norm": 0.06046732887625694,
      "learning_rate": 1.7866666666666668e-06,
      "loss": 0.0015,
      "step": 144640
    },
    {
      "epoch": 7.714666666666667,
      "grad_norm": 0.05948960408568382,
      "learning_rate": 1.7833333333333333e-06,
      "loss": 0.0019,
      "step": 144650
    },
    {
      "epoch": 7.7152,
      "grad_norm": 0.02350575104355812,
      "learning_rate": 1.7800000000000001e-06,
      "loss": 0.0025,
      "step": 144660
    },
    {
      "epoch": 7.7157333333333336,
      "grad_norm": 0.3388877213001251,
      "learning_rate": 1.7766666666666667e-06,
      "loss": 0.0021,
      "step": 144670
    },
    {
      "epoch": 7.716266666666667,
      "grad_norm": 0.0879790410399437,
      "learning_rate": 1.7733333333333334e-06,
      "loss": 0.0012,
      "step": 144680
    },
    {
      "epoch": 7.7168,
      "grad_norm": 0.17931945621967316,
      "learning_rate": 1.7700000000000002e-06,
      "loss": 0.0018,
      "step": 144690
    },
    {
      "epoch": 7.717333333333333,
      "grad_norm": 0.09790347516536713,
      "learning_rate": 1.7666666666666668e-06,
      "loss": 0.0016,
      "step": 144700
    },
    {
      "epoch": 7.717866666666667,
      "grad_norm": 0.04064380005002022,
      "learning_rate": 1.7633333333333335e-06,
      "loss": 0.0025,
      "step": 144710
    },
    {
      "epoch": 7.7184,
      "grad_norm": 0.20470237731933594,
      "learning_rate": 1.76e-06,
      "loss": 0.0013,
      "step": 144720
    },
    {
      "epoch": 7.718933333333333,
      "grad_norm": 0.09044011682271957,
      "learning_rate": 1.7566666666666669e-06,
      "loss": 0.0021,
      "step": 144730
    },
    {
      "epoch": 7.7194666666666665,
      "grad_norm": 0.45272597670555115,
      "learning_rate": 1.7533333333333336e-06,
      "loss": 0.0022,
      "step": 144740
    },
    {
      "epoch": 7.72,
      "grad_norm": 0.18184708058834076,
      "learning_rate": 1.7500000000000002e-06,
      "loss": 0.0012,
      "step": 144750
    },
    {
      "epoch": 7.720533333333333,
      "grad_norm": 0.11607280373573303,
      "learning_rate": 1.7466666666666665e-06,
      "loss": 0.0015,
      "step": 144760
    },
    {
      "epoch": 7.721066666666666,
      "grad_norm": 0.22889457643032074,
      "learning_rate": 1.7433333333333333e-06,
      "loss": 0.0015,
      "step": 144770
    },
    {
      "epoch": 7.7216000000000005,
      "grad_norm": 0.11418215185403824,
      "learning_rate": 1.7399999999999999e-06,
      "loss": 0.0023,
      "step": 144780
    },
    {
      "epoch": 7.722133333333334,
      "grad_norm": 0.10343907028436661,
      "learning_rate": 1.7366666666666666e-06,
      "loss": 0.0025,
      "step": 144790
    },
    {
      "epoch": 7.722666666666667,
      "grad_norm": 0.03444484993815422,
      "learning_rate": 1.7333333333333334e-06,
      "loss": 0.0016,
      "step": 144800
    },
    {
      "epoch": 7.7232,
      "grad_norm": 0.04040246456861496,
      "learning_rate": 1.73e-06,
      "loss": 0.0024,
      "step": 144810
    },
    {
      "epoch": 7.723733333333334,
      "grad_norm": 0.07204648852348328,
      "learning_rate": 1.7266666666666667e-06,
      "loss": 0.0015,
      "step": 144820
    },
    {
      "epoch": 7.724266666666667,
      "grad_norm": 0.21343830227851868,
      "learning_rate": 1.7233333333333335e-06,
      "loss": 0.0016,
      "step": 144830
    },
    {
      "epoch": 7.7248,
      "grad_norm": 0.06367933005094528,
      "learning_rate": 1.72e-06,
      "loss": 0.0015,
      "step": 144840
    },
    {
      "epoch": 7.725333333333333,
      "grad_norm": 0.17361539602279663,
      "learning_rate": 1.7166666666666668e-06,
      "loss": 0.0015,
      "step": 144850
    },
    {
      "epoch": 7.725866666666667,
      "grad_norm": 0.0701216384768486,
      "learning_rate": 1.7133333333333334e-06,
      "loss": 0.0014,
      "step": 144860
    },
    {
      "epoch": 7.7264,
      "grad_norm": 0.14670194685459137,
      "learning_rate": 1.7100000000000001e-06,
      "loss": 0.0018,
      "step": 144870
    },
    {
      "epoch": 7.726933333333333,
      "grad_norm": 0.4502563178539276,
      "learning_rate": 1.706666666666667e-06,
      "loss": 0.0018,
      "step": 144880
    },
    {
      "epoch": 7.7274666666666665,
      "grad_norm": 0.10608755797147751,
      "learning_rate": 1.7033333333333335e-06,
      "loss": 0.0015,
      "step": 144890
    },
    {
      "epoch": 7.728,
      "grad_norm": 0.10538050532341003,
      "learning_rate": 1.7000000000000002e-06,
      "loss": 0.0012,
      "step": 144900
    },
    {
      "epoch": 7.728533333333333,
      "grad_norm": 0.11900278925895691,
      "learning_rate": 1.6966666666666668e-06,
      "loss": 0.0018,
      "step": 144910
    },
    {
      "epoch": 7.729066666666666,
      "grad_norm": 0.2600610852241516,
      "learning_rate": 1.6933333333333336e-06,
      "loss": 0.0027,
      "step": 144920
    },
    {
      "epoch": 7.7296,
      "grad_norm": 0.0767107829451561,
      "learning_rate": 1.69e-06,
      "loss": 0.0018,
      "step": 144930
    },
    {
      "epoch": 7.730133333333333,
      "grad_norm": 0.32551759481430054,
      "learning_rate": 1.6866666666666667e-06,
      "loss": 0.0012,
      "step": 144940
    },
    {
      "epoch": 7.730666666666667,
      "grad_norm": 0.13313204050064087,
      "learning_rate": 1.6833333333333332e-06,
      "loss": 0.0015,
      "step": 144950
    },
    {
      "epoch": 7.7312,
      "grad_norm": 0.19784745573997498,
      "learning_rate": 1.68e-06,
      "loss": 0.0016,
      "step": 144960
    },
    {
      "epoch": 7.731733333333334,
      "grad_norm": 0.11466272175312042,
      "learning_rate": 1.6766666666666666e-06,
      "loss": 0.0018,
      "step": 144970
    },
    {
      "epoch": 7.732266666666667,
      "grad_norm": 0.08737781643867493,
      "learning_rate": 1.6733333333333333e-06,
      "loss": 0.0017,
      "step": 144980
    },
    {
      "epoch": 7.7328,
      "grad_norm": 0.2865108251571655,
      "learning_rate": 1.67e-06,
      "loss": 0.0019,
      "step": 144990
    },
    {
      "epoch": 7.733333333333333,
      "grad_norm": 0.24481764435768127,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 0.0019,
      "step": 145000
    },
    {
      "epoch": 7.733866666666667,
      "grad_norm": 0.05214938521385193,
      "learning_rate": 1.6633333333333334e-06,
      "loss": 0.0016,
      "step": 145010
    },
    {
      "epoch": 7.7344,
      "grad_norm": 0.2426183521747589,
      "learning_rate": 1.6600000000000002e-06,
      "loss": 0.0017,
      "step": 145020
    },
    {
      "epoch": 7.734933333333333,
      "grad_norm": 0.15302446484565735,
      "learning_rate": 1.6566666666666668e-06,
      "loss": 0.0019,
      "step": 145030
    },
    {
      "epoch": 7.7354666666666665,
      "grad_norm": 0.25517210364341736,
      "learning_rate": 1.6533333333333335e-06,
      "loss": 0.0015,
      "step": 145040
    },
    {
      "epoch": 7.736,
      "grad_norm": 0.09059470146894455,
      "learning_rate": 1.65e-06,
      "loss": 0.0013,
      "step": 145050
    },
    {
      "epoch": 7.736533333333333,
      "grad_norm": 0.1467524617910385,
      "learning_rate": 1.6466666666666669e-06,
      "loss": 0.0017,
      "step": 145060
    },
    {
      "epoch": 7.737066666666666,
      "grad_norm": 0.1436973214149475,
      "learning_rate": 1.6433333333333336e-06,
      "loss": 0.0017,
      "step": 145070
    },
    {
      "epoch": 7.7376000000000005,
      "grad_norm": 0.14296023547649384,
      "learning_rate": 1.6400000000000002e-06,
      "loss": 0.0011,
      "step": 145080
    },
    {
      "epoch": 7.738133333333334,
      "grad_norm": 0.2299119532108307,
      "learning_rate": 1.636666666666667e-06,
      "loss": 0.0017,
      "step": 145090
    },
    {
      "epoch": 7.738666666666667,
      "grad_norm": 0.03064548410475254,
      "learning_rate": 1.6333333333333333e-06,
      "loss": 0.0027,
      "step": 145100
    },
    {
      "epoch": 7.7392,
      "grad_norm": 0.08789048343896866,
      "learning_rate": 1.6299999999999999e-06,
      "loss": 0.0016,
      "step": 145110
    },
    {
      "epoch": 7.739733333333334,
      "grad_norm": 0.05925135686993599,
      "learning_rate": 1.6266666666666666e-06,
      "loss": 0.0015,
      "step": 145120
    },
    {
      "epoch": 7.740266666666667,
      "grad_norm": 0.11993537098169327,
      "learning_rate": 1.6233333333333334e-06,
      "loss": 0.0013,
      "step": 145130
    },
    {
      "epoch": 7.7408,
      "grad_norm": 0.2891324460506439,
      "learning_rate": 1.62e-06,
      "loss": 0.0018,
      "step": 145140
    },
    {
      "epoch": 7.741333333333333,
      "grad_norm": 0.014630730263888836,
      "learning_rate": 1.6166666666666667e-06,
      "loss": 0.0014,
      "step": 145150
    },
    {
      "epoch": 7.741866666666667,
      "grad_norm": 0.4000150263309479,
      "learning_rate": 1.6133333333333333e-06,
      "loss": 0.0017,
      "step": 145160
    },
    {
      "epoch": 7.7424,
      "grad_norm": 0.03541940078139305,
      "learning_rate": 1.61e-06,
      "loss": 0.0016,
      "step": 145170
    },
    {
      "epoch": 7.742933333333333,
      "grad_norm": 0.16965633630752563,
      "learning_rate": 1.6066666666666668e-06,
      "loss": 0.0023,
      "step": 145180
    },
    {
      "epoch": 7.7434666666666665,
      "grad_norm": 0.26035332679748535,
      "learning_rate": 1.6033333333333334e-06,
      "loss": 0.0026,
      "step": 145190
    },
    {
      "epoch": 7.744,
      "grad_norm": 0.15186449885368347,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 0.0017,
      "step": 145200
    },
    {
      "epoch": 7.744533333333333,
      "grad_norm": 0.22594119608402252,
      "learning_rate": 1.5966666666666667e-06,
      "loss": 0.0021,
      "step": 145210
    },
    {
      "epoch": 7.745066666666666,
      "grad_norm": 0.17775462567806244,
      "learning_rate": 1.5933333333333335e-06,
      "loss": 0.0022,
      "step": 145220
    },
    {
      "epoch": 7.7456,
      "grad_norm": 0.22753171622753143,
      "learning_rate": 1.5900000000000002e-06,
      "loss": 0.0017,
      "step": 145230
    },
    {
      "epoch": 7.746133333333333,
      "grad_norm": 0.2878965437412262,
      "learning_rate": 1.5866666666666668e-06,
      "loss": 0.0016,
      "step": 145240
    },
    {
      "epoch": 7.746666666666667,
      "grad_norm": 0.28133147954940796,
      "learning_rate": 1.5833333333333336e-06,
      "loss": 0.0031,
      "step": 145250
    },
    {
      "epoch": 7.7472,
      "grad_norm": 0.3078334629535675,
      "learning_rate": 1.5800000000000003e-06,
      "loss": 0.0016,
      "step": 145260
    },
    {
      "epoch": 7.747733333333334,
      "grad_norm": 0.08586803823709488,
      "learning_rate": 1.5766666666666665e-06,
      "loss": 0.0017,
      "step": 145270
    },
    {
      "epoch": 7.748266666666667,
      "grad_norm": 0.3317472040653229,
      "learning_rate": 1.5733333333333332e-06,
      "loss": 0.0021,
      "step": 145280
    },
    {
      "epoch": 7.7488,
      "grad_norm": 0.1838534027338028,
      "learning_rate": 1.57e-06,
      "loss": 0.0024,
      "step": 145290
    },
    {
      "epoch": 7.749333333333333,
      "grad_norm": 0.061207134276628494,
      "learning_rate": 1.5666666666666666e-06,
      "loss": 0.0018,
      "step": 145300
    },
    {
      "epoch": 7.749866666666667,
      "grad_norm": 0.19645486772060394,
      "learning_rate": 1.5633333333333333e-06,
      "loss": 0.0014,
      "step": 145310
    },
    {
      "epoch": 7.7504,
      "grad_norm": 0.6799017786979675,
      "learning_rate": 1.56e-06,
      "loss": 0.0019,
      "step": 145320
    },
    {
      "epoch": 7.750933333333333,
      "grad_norm": 0.1103527620434761,
      "learning_rate": 1.5566666666666667e-06,
      "loss": 0.0011,
      "step": 145330
    },
    {
      "epoch": 7.7514666666666665,
      "grad_norm": 0.3639340400695801,
      "learning_rate": 1.5533333333333334e-06,
      "loss": 0.0023,
      "step": 145340
    },
    {
      "epoch": 7.752,
      "grad_norm": 0.28560924530029297,
      "learning_rate": 1.55e-06,
      "loss": 0.0028,
      "step": 145350
    },
    {
      "epoch": 7.752533333333333,
      "grad_norm": 0.28856727480888367,
      "learning_rate": 1.5466666666666668e-06,
      "loss": 0.0019,
      "step": 145360
    },
    {
      "epoch": 7.753066666666666,
      "grad_norm": 0.1406862437725067,
      "learning_rate": 1.5433333333333335e-06,
      "loss": 0.002,
      "step": 145370
    },
    {
      "epoch": 7.7536000000000005,
      "grad_norm": 0.11365966498851776,
      "learning_rate": 1.54e-06,
      "loss": 0.0024,
      "step": 145380
    },
    {
      "epoch": 7.754133333333334,
      "grad_norm": 0.03254171460866928,
      "learning_rate": 1.5366666666666668e-06,
      "loss": 0.0014,
      "step": 145390
    },
    {
      "epoch": 7.754666666666667,
      "grad_norm": 0.14536228775978088,
      "learning_rate": 1.5333333333333334e-06,
      "loss": 0.0012,
      "step": 145400
    },
    {
      "epoch": 7.7552,
      "grad_norm": 0.22625263035297394,
      "learning_rate": 1.53e-06,
      "loss": 0.0025,
      "step": 145410
    },
    {
      "epoch": 7.755733333333334,
      "grad_norm": 0.28671154379844666,
      "learning_rate": 1.5266666666666667e-06,
      "loss": 0.0017,
      "step": 145420
    },
    {
      "epoch": 7.756266666666667,
      "grad_norm": 0.2180813103914261,
      "learning_rate": 1.5233333333333333e-06,
      "loss": 0.0021,
      "step": 145430
    },
    {
      "epoch": 7.7568,
      "grad_norm": 0.1158730685710907,
      "learning_rate": 1.52e-06,
      "loss": 0.0016,
      "step": 145440
    },
    {
      "epoch": 7.757333333333333,
      "grad_norm": 0.40212133526802063,
      "learning_rate": 1.5166666666666668e-06,
      "loss": 0.0027,
      "step": 145450
    },
    {
      "epoch": 7.757866666666667,
      "grad_norm": 0.20093484222888947,
      "learning_rate": 1.5133333333333334e-06,
      "loss": 0.002,
      "step": 145460
    },
    {
      "epoch": 7.7584,
      "grad_norm": 0.043221816420555115,
      "learning_rate": 1.5100000000000002e-06,
      "loss": 0.0012,
      "step": 145470
    },
    {
      "epoch": 7.758933333333333,
      "grad_norm": 0.16996555030345917,
      "learning_rate": 1.5066666666666667e-06,
      "loss": 0.002,
      "step": 145480
    },
    {
      "epoch": 7.7594666666666665,
      "grad_norm": 0.053994905203580856,
      "learning_rate": 1.5033333333333333e-06,
      "loss": 0.0022,
      "step": 145490
    },
    {
      "epoch": 7.76,
      "grad_norm": 0.08859937638044357,
      "learning_rate": 1.5e-06,
      "loss": 0.0012,
      "step": 145500
    },
    {
      "epoch": 7.760533333333333,
      "grad_norm": 0.09611795097589493,
      "learning_rate": 1.4966666666666668e-06,
      "loss": 0.0017,
      "step": 145510
    },
    {
      "epoch": 7.761066666666666,
      "grad_norm": 0.19751395285129547,
      "learning_rate": 1.4933333333333334e-06,
      "loss": 0.002,
      "step": 145520
    },
    {
      "epoch": 7.7616,
      "grad_norm": 0.0838128924369812,
      "learning_rate": 1.4900000000000001e-06,
      "loss": 0.0025,
      "step": 145530
    },
    {
      "epoch": 7.762133333333333,
      "grad_norm": 0.40371760725975037,
      "learning_rate": 1.4866666666666667e-06,
      "loss": 0.0023,
      "step": 145540
    },
    {
      "epoch": 7.762666666666667,
      "grad_norm": 0.06160896643996239,
      "learning_rate": 1.4833333333333335e-06,
      "loss": 0.0023,
      "step": 145550
    },
    {
      "epoch": 7.7632,
      "grad_norm": 0.061546552926301956,
      "learning_rate": 1.4800000000000002e-06,
      "loss": 0.0025,
      "step": 145560
    },
    {
      "epoch": 7.763733333333334,
      "grad_norm": 0.2299511730670929,
      "learning_rate": 1.4766666666666668e-06,
      "loss": 0.0017,
      "step": 145570
    },
    {
      "epoch": 7.764266666666667,
      "grad_norm": 0.3649693429470062,
      "learning_rate": 1.4733333333333333e-06,
      "loss": 0.0027,
      "step": 145580
    },
    {
      "epoch": 7.7648,
      "grad_norm": 0.06132512539625168,
      "learning_rate": 1.4700000000000001e-06,
      "loss": 0.0014,
      "step": 145590
    },
    {
      "epoch": 7.765333333333333,
      "grad_norm": 0.2416115552186966,
      "learning_rate": 1.4666666666666667e-06,
      "loss": 0.0016,
      "step": 145600
    },
    {
      "epoch": 7.765866666666667,
      "grad_norm": 0.3449126183986664,
      "learning_rate": 1.4633333333333334e-06,
      "loss": 0.0018,
      "step": 145610
    },
    {
      "epoch": 7.7664,
      "grad_norm": 0.28460267186164856,
      "learning_rate": 1.46e-06,
      "loss": 0.0017,
      "step": 145620
    },
    {
      "epoch": 7.766933333333333,
      "grad_norm": 0.06805671006441116,
      "learning_rate": 1.4566666666666668e-06,
      "loss": 0.0019,
      "step": 145630
    },
    {
      "epoch": 7.7674666666666665,
      "grad_norm": 0.23135176301002502,
      "learning_rate": 1.4533333333333335e-06,
      "loss": 0.0019,
      "step": 145640
    },
    {
      "epoch": 7.768,
      "grad_norm": 0.23580467700958252,
      "learning_rate": 1.45e-06,
      "loss": 0.0017,
      "step": 145650
    },
    {
      "epoch": 7.768533333333333,
      "grad_norm": 0.37218546867370605,
      "learning_rate": 1.4466666666666667e-06,
      "loss": 0.0015,
      "step": 145660
    },
    {
      "epoch": 7.769066666666666,
      "grad_norm": 0.033564191311597824,
      "learning_rate": 1.4433333333333334e-06,
      "loss": 0.0015,
      "step": 145670
    },
    {
      "epoch": 7.7696,
      "grad_norm": 0.49960002303123474,
      "learning_rate": 1.44e-06,
      "loss": 0.0013,
      "step": 145680
    },
    {
      "epoch": 7.770133333333334,
      "grad_norm": 0.08731145411729813,
      "learning_rate": 1.4366666666666667e-06,
      "loss": 0.0024,
      "step": 145690
    },
    {
      "epoch": 7.770666666666667,
      "grad_norm": 0.14046989381313324,
      "learning_rate": 1.4333333333333333e-06,
      "loss": 0.002,
      "step": 145700
    },
    {
      "epoch": 7.7712,
      "grad_norm": 0.08629590272903442,
      "learning_rate": 1.43e-06,
      "loss": 0.0024,
      "step": 145710
    },
    {
      "epoch": 7.771733333333334,
      "grad_norm": 0.06316009163856506,
      "learning_rate": 1.4266666666666668e-06,
      "loss": 0.0013,
      "step": 145720
    },
    {
      "epoch": 7.772266666666667,
      "grad_norm": 0.14087308943271637,
      "learning_rate": 1.4233333333333334e-06,
      "loss": 0.0016,
      "step": 145730
    },
    {
      "epoch": 7.7728,
      "grad_norm": 0.23397286236286163,
      "learning_rate": 1.4200000000000002e-06,
      "loss": 0.0018,
      "step": 145740
    },
    {
      "epoch": 7.773333333333333,
      "grad_norm": 0.2608531415462494,
      "learning_rate": 1.4166666666666667e-06,
      "loss": 0.0015,
      "step": 145750
    },
    {
      "epoch": 7.773866666666667,
      "grad_norm": 0.05403799191117287,
      "learning_rate": 1.4133333333333333e-06,
      "loss": 0.0016,
      "step": 145760
    },
    {
      "epoch": 7.7744,
      "grad_norm": 0.09535355865955353,
      "learning_rate": 1.41e-06,
      "loss": 0.002,
      "step": 145770
    },
    {
      "epoch": 7.774933333333333,
      "grad_norm": 0.052286382764577866,
      "learning_rate": 1.4066666666666668e-06,
      "loss": 0.0018,
      "step": 145780
    },
    {
      "epoch": 7.7754666666666665,
      "grad_norm": 0.15075017511844635,
      "learning_rate": 1.4033333333333334e-06,
      "loss": 0.0018,
      "step": 145790
    },
    {
      "epoch": 7.776,
      "grad_norm": 0.11718444526195526,
      "learning_rate": 1.4000000000000001e-06,
      "loss": 0.0021,
      "step": 145800
    },
    {
      "epoch": 7.776533333333333,
      "grad_norm": 0.2543427646160126,
      "learning_rate": 1.3966666666666667e-06,
      "loss": 0.0023,
      "step": 145810
    },
    {
      "epoch": 7.777066666666666,
      "grad_norm": 0.03312437981367111,
      "learning_rate": 1.3933333333333335e-06,
      "loss": 0.0013,
      "step": 145820
    },
    {
      "epoch": 7.7776,
      "grad_norm": 0.19677692651748657,
      "learning_rate": 1.39e-06,
      "loss": 0.0018,
      "step": 145830
    },
    {
      "epoch": 7.778133333333333,
      "grad_norm": 0.30065131187438965,
      "learning_rate": 1.3866666666666666e-06,
      "loss": 0.002,
      "step": 145840
    },
    {
      "epoch": 7.778666666666666,
      "grad_norm": 0.2550029754638672,
      "learning_rate": 1.3833333333333334e-06,
      "loss": 0.0019,
      "step": 145850
    },
    {
      "epoch": 7.7792,
      "grad_norm": 0.4845908582210541,
      "learning_rate": 1.3800000000000001e-06,
      "loss": 0.0013,
      "step": 145860
    },
    {
      "epoch": 7.779733333333334,
      "grad_norm": 0.19720210134983063,
      "learning_rate": 1.3766666666666667e-06,
      "loss": 0.0024,
      "step": 145870
    },
    {
      "epoch": 7.780266666666667,
      "grad_norm": 0.14493805170059204,
      "learning_rate": 1.3733333333333335e-06,
      "loss": 0.0017,
      "step": 145880
    },
    {
      "epoch": 7.7808,
      "grad_norm": 0.3156241476535797,
      "learning_rate": 1.37e-06,
      "loss": 0.0017,
      "step": 145890
    },
    {
      "epoch": 7.781333333333333,
      "grad_norm": 0.5130714774131775,
      "learning_rate": 1.3666666666666668e-06,
      "loss": 0.0015,
      "step": 145900
    },
    {
      "epoch": 7.781866666666667,
      "grad_norm": 0.40434348583221436,
      "learning_rate": 1.3633333333333336e-06,
      "loss": 0.0018,
      "step": 145910
    },
    {
      "epoch": 7.7824,
      "grad_norm": 0.06806895136833191,
      "learning_rate": 1.36e-06,
      "loss": 0.0028,
      "step": 145920
    },
    {
      "epoch": 7.782933333333333,
      "grad_norm": 0.048715539276599884,
      "learning_rate": 1.3566666666666667e-06,
      "loss": 0.0018,
      "step": 145930
    },
    {
      "epoch": 7.7834666666666665,
      "grad_norm": 0.1144210696220398,
      "learning_rate": 1.3533333333333334e-06,
      "loss": 0.0016,
      "step": 145940
    },
    {
      "epoch": 7.784,
      "grad_norm": 0.11613300442695618,
      "learning_rate": 1.35e-06,
      "loss": 0.0014,
      "step": 145950
    },
    {
      "epoch": 7.784533333333333,
      "grad_norm": 0.284303218126297,
      "learning_rate": 1.3466666666666668e-06,
      "loss": 0.0013,
      "step": 145960
    },
    {
      "epoch": 7.785066666666666,
      "grad_norm": 0.20756873488426208,
      "learning_rate": 1.3433333333333333e-06,
      "loss": 0.0019,
      "step": 145970
    },
    {
      "epoch": 7.7856,
      "grad_norm": 0.12421441823244095,
      "learning_rate": 1.34e-06,
      "loss": 0.0014,
      "step": 145980
    },
    {
      "epoch": 7.786133333333334,
      "grad_norm": 0.19355255365371704,
      "learning_rate": 1.3366666666666669e-06,
      "loss": 0.0029,
      "step": 145990
    },
    {
      "epoch": 7.786666666666667,
      "grad_norm": 0.42138636112213135,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 0.0014,
      "step": 146000
    },
    {
      "epoch": 7.7872,
      "grad_norm": 0.20027567446231842,
      "learning_rate": 1.33e-06,
      "loss": 0.0018,
      "step": 146010
    },
    {
      "epoch": 7.787733333333334,
      "grad_norm": 0.23301933705806732,
      "learning_rate": 1.3266666666666667e-06,
      "loss": 0.0014,
      "step": 146020
    },
    {
      "epoch": 7.788266666666667,
      "grad_norm": 0.04428144171833992,
      "learning_rate": 1.3233333333333333e-06,
      "loss": 0.0017,
      "step": 146030
    },
    {
      "epoch": 7.7888,
      "grad_norm": 0.1472107321023941,
      "learning_rate": 1.32e-06,
      "loss": 0.0016,
      "step": 146040
    },
    {
      "epoch": 7.789333333333333,
      "grad_norm": 0.22539731860160828,
      "learning_rate": 1.3166666666666668e-06,
      "loss": 0.0022,
      "step": 146050
    },
    {
      "epoch": 7.789866666666667,
      "grad_norm": 0.534983217716217,
      "learning_rate": 1.3133333333333334e-06,
      "loss": 0.0017,
      "step": 146060
    },
    {
      "epoch": 7.7904,
      "grad_norm": 0.16433799266815186,
      "learning_rate": 1.3100000000000002e-06,
      "loss": 0.0023,
      "step": 146070
    },
    {
      "epoch": 7.790933333333333,
      "grad_norm": 0.059220507740974426,
      "learning_rate": 1.3066666666666667e-06,
      "loss": 0.0018,
      "step": 146080
    },
    {
      "epoch": 7.7914666666666665,
      "grad_norm": 0.09300991892814636,
      "learning_rate": 1.3033333333333333e-06,
      "loss": 0.0016,
      "step": 146090
    },
    {
      "epoch": 7.792,
      "grad_norm": 0.022652974352240562,
      "learning_rate": 1.3e-06,
      "loss": 0.0017,
      "step": 146100
    },
    {
      "epoch": 7.792533333333333,
      "grad_norm": 0.11660353094339371,
      "learning_rate": 1.2966666666666666e-06,
      "loss": 0.0012,
      "step": 146110
    },
    {
      "epoch": 7.793066666666666,
      "grad_norm": 0.3129352629184723,
      "learning_rate": 1.2933333333333334e-06,
      "loss": 0.0012,
      "step": 146120
    },
    {
      "epoch": 7.7936,
      "grad_norm": 0.3691267669200897,
      "learning_rate": 1.2900000000000001e-06,
      "loss": 0.0012,
      "step": 146130
    },
    {
      "epoch": 7.794133333333333,
      "grad_norm": 0.5057677626609802,
      "learning_rate": 1.2866666666666667e-06,
      "loss": 0.0024,
      "step": 146140
    },
    {
      "epoch": 7.794666666666666,
      "grad_norm": 0.1738053858280182,
      "learning_rate": 1.2833333333333335e-06,
      "loss": 0.0016,
      "step": 146150
    },
    {
      "epoch": 7.7952,
      "grad_norm": 0.23681318759918213,
      "learning_rate": 1.28e-06,
      "loss": 0.0017,
      "step": 146160
    },
    {
      "epoch": 7.795733333333334,
      "grad_norm": 0.446580708026886,
      "learning_rate": 1.2766666666666668e-06,
      "loss": 0.0014,
      "step": 146170
    },
    {
      "epoch": 7.796266666666667,
      "grad_norm": 0.16974671185016632,
      "learning_rate": 1.2733333333333334e-06,
      "loss": 0.0017,
      "step": 146180
    },
    {
      "epoch": 7.7968,
      "grad_norm": 0.2000546008348465,
      "learning_rate": 1.27e-06,
      "loss": 0.0017,
      "step": 146190
    },
    {
      "epoch": 7.7973333333333334,
      "grad_norm": 0.06172583997249603,
      "learning_rate": 1.2666666666666667e-06,
      "loss": 0.0016,
      "step": 146200
    },
    {
      "epoch": 7.797866666666667,
      "grad_norm": 0.200500026345253,
      "learning_rate": 1.2633333333333334e-06,
      "loss": 0.0018,
      "step": 146210
    },
    {
      "epoch": 7.7984,
      "grad_norm": 0.21047519147396088,
      "learning_rate": 1.26e-06,
      "loss": 0.0021,
      "step": 146220
    },
    {
      "epoch": 7.798933333333333,
      "grad_norm": 0.22967348992824554,
      "learning_rate": 1.2566666666666668e-06,
      "loss": 0.0017,
      "step": 146230
    },
    {
      "epoch": 7.7994666666666665,
      "grad_norm": 0.11941800266504288,
      "learning_rate": 1.2533333333333335e-06,
      "loss": 0.0014,
      "step": 146240
    },
    {
      "epoch": 7.8,
      "grad_norm": 0.060994092375040054,
      "learning_rate": 1.25e-06,
      "loss": 0.0012,
      "step": 146250
    },
    {
      "epoch": 7.800533333333333,
      "grad_norm": 0.06808201223611832,
      "learning_rate": 1.2466666666666667e-06,
      "loss": 0.0014,
      "step": 146260
    },
    {
      "epoch": 7.801066666666666,
      "grad_norm": 0.1400730013847351,
      "learning_rate": 1.2433333333333334e-06,
      "loss": 0.0013,
      "step": 146270
    },
    {
      "epoch": 7.8016,
      "grad_norm": 0.14438636600971222,
      "learning_rate": 1.24e-06,
      "loss": 0.0021,
      "step": 146280
    },
    {
      "epoch": 7.802133333333334,
      "grad_norm": 0.10228345543146133,
      "learning_rate": 1.2366666666666668e-06,
      "loss": 0.0016,
      "step": 146290
    },
    {
      "epoch": 7.802666666666667,
      "grad_norm": 0.1049218401312828,
      "learning_rate": 1.2333333333333333e-06,
      "loss": 0.0016,
      "step": 146300
    },
    {
      "epoch": 7.8032,
      "grad_norm": 0.04470524564385414,
      "learning_rate": 1.23e-06,
      "loss": 0.0013,
      "step": 146310
    },
    {
      "epoch": 7.803733333333334,
      "grad_norm": 0.09700502455234528,
      "learning_rate": 1.2266666666666669e-06,
      "loss": 0.0013,
      "step": 146320
    },
    {
      "epoch": 7.804266666666667,
      "grad_norm": 0.05997622385621071,
      "learning_rate": 1.2233333333333334e-06,
      "loss": 0.0014,
      "step": 146330
    },
    {
      "epoch": 7.8048,
      "grad_norm": 0.14387403428554535,
      "learning_rate": 1.2200000000000002e-06,
      "loss": 0.002,
      "step": 146340
    },
    {
      "epoch": 7.8053333333333335,
      "grad_norm": 0.05869390442967415,
      "learning_rate": 1.2166666666666667e-06,
      "loss": 0.0021,
      "step": 146350
    },
    {
      "epoch": 7.805866666666667,
      "grad_norm": 0.13114875555038452,
      "learning_rate": 1.2133333333333333e-06,
      "loss": 0.0013,
      "step": 146360
    },
    {
      "epoch": 7.8064,
      "grad_norm": 0.29748958349227905,
      "learning_rate": 1.21e-06,
      "loss": 0.0015,
      "step": 146370
    },
    {
      "epoch": 7.806933333333333,
      "grad_norm": 0.2972925305366516,
      "learning_rate": 1.2066666666666666e-06,
      "loss": 0.0016,
      "step": 146380
    },
    {
      "epoch": 7.8074666666666666,
      "grad_norm": 0.14664611220359802,
      "learning_rate": 1.2033333333333334e-06,
      "loss": 0.0018,
      "step": 146390
    },
    {
      "epoch": 7.808,
      "grad_norm": 0.09508363157510757,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 0.0012,
      "step": 146400
    },
    {
      "epoch": 7.808533333333333,
      "grad_norm": 0.038126278668642044,
      "learning_rate": 1.1966666666666667e-06,
      "loss": 0.0029,
      "step": 146410
    },
    {
      "epoch": 7.809066666666666,
      "grad_norm": 0.19356651604175568,
      "learning_rate": 1.1933333333333335e-06,
      "loss": 0.0016,
      "step": 146420
    },
    {
      "epoch": 7.8096,
      "grad_norm": 0.17134124040603638,
      "learning_rate": 1.19e-06,
      "loss": 0.0014,
      "step": 146430
    },
    {
      "epoch": 7.810133333333333,
      "grad_norm": 0.09463140368461609,
      "learning_rate": 1.1866666666666666e-06,
      "loss": 0.0024,
      "step": 146440
    },
    {
      "epoch": 7.810666666666666,
      "grad_norm": 0.1284082978963852,
      "learning_rate": 1.1833333333333334e-06,
      "loss": 0.0014,
      "step": 146450
    },
    {
      "epoch": 7.8112,
      "grad_norm": 0.3685537576675415,
      "learning_rate": 1.18e-06,
      "loss": 0.002,
      "step": 146460
    },
    {
      "epoch": 7.811733333333334,
      "grad_norm": 0.11319160461425781,
      "learning_rate": 1.1766666666666667e-06,
      "loss": 0.0019,
      "step": 146470
    },
    {
      "epoch": 7.812266666666667,
      "grad_norm": 0.36995553970336914,
      "learning_rate": 1.1733333333333335e-06,
      "loss": 0.0021,
      "step": 146480
    },
    {
      "epoch": 7.8128,
      "grad_norm": 0.19809214770793915,
      "learning_rate": 1.17e-06,
      "loss": 0.0016,
      "step": 146490
    },
    {
      "epoch": 7.8133333333333335,
      "grad_norm": 0.3698834776878357,
      "learning_rate": 1.1666666666666668e-06,
      "loss": 0.0015,
      "step": 146500
    },
    {
      "epoch": 7.813866666666667,
      "grad_norm": 0.1512739062309265,
      "learning_rate": 1.1633333333333336e-06,
      "loss": 0.0021,
      "step": 146510
    },
    {
      "epoch": 7.8144,
      "grad_norm": 0.1683109700679779,
      "learning_rate": 1.16e-06,
      "loss": 0.0021,
      "step": 146520
    },
    {
      "epoch": 7.814933333333333,
      "grad_norm": 0.045493174344301224,
      "learning_rate": 1.1566666666666667e-06,
      "loss": 0.0016,
      "step": 146530
    },
    {
      "epoch": 7.815466666666667,
      "grad_norm": 0.06294488906860352,
      "learning_rate": 1.1533333333333334e-06,
      "loss": 0.0017,
      "step": 146540
    },
    {
      "epoch": 7.816,
      "grad_norm": 0.34818384051322937,
      "learning_rate": 1.15e-06,
      "loss": 0.0018,
      "step": 146550
    },
    {
      "epoch": 7.816533333333333,
      "grad_norm": 0.03333652392029762,
      "learning_rate": 1.1466666666666668e-06,
      "loss": 0.0012,
      "step": 146560
    },
    {
      "epoch": 7.817066666666666,
      "grad_norm": 0.40157756209373474,
      "learning_rate": 1.1433333333333333e-06,
      "loss": 0.0022,
      "step": 146570
    },
    {
      "epoch": 7.8176,
      "grad_norm": 0.03802845627069473,
      "learning_rate": 1.14e-06,
      "loss": 0.0018,
      "step": 146580
    },
    {
      "epoch": 7.818133333333334,
      "grad_norm": 0.08725041896104813,
      "learning_rate": 1.1366666666666669e-06,
      "loss": 0.0017,
      "step": 146590
    },
    {
      "epoch": 7.818666666666667,
      "grad_norm": 0.044561151415109634,
      "learning_rate": 1.1333333333333334e-06,
      "loss": 0.0021,
      "step": 146600
    },
    {
      "epoch": 7.8192,
      "grad_norm": 0.20150898396968842,
      "learning_rate": 1.13e-06,
      "loss": 0.0027,
      "step": 146610
    },
    {
      "epoch": 7.819733333333334,
      "grad_norm": 0.19039903581142426,
      "learning_rate": 1.1266666666666667e-06,
      "loss": 0.0015,
      "step": 146620
    },
    {
      "epoch": 7.820266666666667,
      "grad_norm": 0.09016247093677521,
      "learning_rate": 1.1233333333333333e-06,
      "loss": 0.0017,
      "step": 146630
    },
    {
      "epoch": 7.8208,
      "grad_norm": 0.026840228587388992,
      "learning_rate": 1.12e-06,
      "loss": 0.0018,
      "step": 146640
    },
    {
      "epoch": 7.8213333333333335,
      "grad_norm": 0.12128742784261703,
      "learning_rate": 1.1166666666666666e-06,
      "loss": 0.0013,
      "step": 146650
    },
    {
      "epoch": 7.821866666666667,
      "grad_norm": 0.4516333341598511,
      "learning_rate": 1.1133333333333334e-06,
      "loss": 0.0018,
      "step": 146660
    },
    {
      "epoch": 7.8224,
      "grad_norm": 0.06491328030824661,
      "learning_rate": 1.1100000000000002e-06,
      "loss": 0.0019,
      "step": 146670
    },
    {
      "epoch": 7.822933333333333,
      "grad_norm": 0.15557904541492462,
      "learning_rate": 1.1066666666666667e-06,
      "loss": 0.0012,
      "step": 146680
    },
    {
      "epoch": 7.823466666666667,
      "grad_norm": 0.22857137024402618,
      "learning_rate": 1.1033333333333333e-06,
      "loss": 0.001,
      "step": 146690
    },
    {
      "epoch": 7.824,
      "grad_norm": 0.2291690558195114,
      "learning_rate": 1.1e-06,
      "loss": 0.002,
      "step": 146700
    },
    {
      "epoch": 7.824533333333333,
      "grad_norm": 0.09212367981672287,
      "learning_rate": 1.0966666666666666e-06,
      "loss": 0.0019,
      "step": 146710
    },
    {
      "epoch": 7.825066666666666,
      "grad_norm": 0.06642594188451767,
      "learning_rate": 1.0933333333333334e-06,
      "loss": 0.0021,
      "step": 146720
    },
    {
      "epoch": 7.8256,
      "grad_norm": 0.2288971096277237,
      "learning_rate": 1.0900000000000002e-06,
      "loss": 0.0018,
      "step": 146730
    },
    {
      "epoch": 7.826133333333333,
      "grad_norm": 0.16890323162078857,
      "learning_rate": 1.0866666666666667e-06,
      "loss": 0.0017,
      "step": 146740
    },
    {
      "epoch": 7.826666666666666,
      "grad_norm": 0.16918890178203583,
      "learning_rate": 1.0833333333333335e-06,
      "loss": 0.0011,
      "step": 146750
    },
    {
      "epoch": 7.8272,
      "grad_norm": 0.02974097803235054,
      "learning_rate": 1.08e-06,
      "loss": 0.0022,
      "step": 146760
    },
    {
      "epoch": 7.827733333333334,
      "grad_norm": 0.3706284463405609,
      "learning_rate": 1.0766666666666668e-06,
      "loss": 0.0017,
      "step": 146770
    },
    {
      "epoch": 7.828266666666667,
      "grad_norm": 0.22803400456905365,
      "learning_rate": 1.0733333333333334e-06,
      "loss": 0.0025,
      "step": 146780
    },
    {
      "epoch": 7.8288,
      "grad_norm": 0.0912833884358406,
      "learning_rate": 1.07e-06,
      "loss": 0.0017,
      "step": 146790
    },
    {
      "epoch": 7.8293333333333335,
      "grad_norm": 0.2808132469654083,
      "learning_rate": 1.0666666666666667e-06,
      "loss": 0.0016,
      "step": 146800
    },
    {
      "epoch": 7.829866666666667,
      "grad_norm": 0.16922537982463837,
      "learning_rate": 1.0633333333333335e-06,
      "loss": 0.0019,
      "step": 146810
    },
    {
      "epoch": 7.8304,
      "grad_norm": 0.037231504917144775,
      "learning_rate": 1.06e-06,
      "loss": 0.0021,
      "step": 146820
    },
    {
      "epoch": 7.830933333333333,
      "grad_norm": 0.04420313611626625,
      "learning_rate": 1.0566666666666668e-06,
      "loss": 0.0018,
      "step": 146830
    },
    {
      "epoch": 7.831466666666667,
      "grad_norm": 0.11613806337118149,
      "learning_rate": 1.0533333333333333e-06,
      "loss": 0.0015,
      "step": 146840
    },
    {
      "epoch": 7.832,
      "grad_norm": 0.228827103972435,
      "learning_rate": 1.0500000000000001e-06,
      "loss": 0.0013,
      "step": 146850
    },
    {
      "epoch": 7.832533333333333,
      "grad_norm": 0.20311787724494934,
      "learning_rate": 1.0466666666666669e-06,
      "loss": 0.0012,
      "step": 146860
    },
    {
      "epoch": 7.833066666666666,
      "grad_norm": 0.14570395648479462,
      "learning_rate": 1.0433333333333332e-06,
      "loss": 0.0025,
      "step": 146870
    },
    {
      "epoch": 7.8336,
      "grad_norm": 0.1755552440881729,
      "learning_rate": 1.04e-06,
      "loss": 0.0014,
      "step": 146880
    },
    {
      "epoch": 7.834133333333333,
      "grad_norm": 0.4376419186592102,
      "learning_rate": 1.0366666666666668e-06,
      "loss": 0.0013,
      "step": 146890
    },
    {
      "epoch": 7.834666666666667,
      "grad_norm": 0.018430400639772415,
      "learning_rate": 1.0333333333333333e-06,
      "loss": 0.0017,
      "step": 146900
    },
    {
      "epoch": 7.8352,
      "grad_norm": 0.11409103125333786,
      "learning_rate": 1.03e-06,
      "loss": 0.0013,
      "step": 146910
    },
    {
      "epoch": 7.835733333333334,
      "grad_norm": 0.08256398141384125,
      "learning_rate": 1.0266666666666666e-06,
      "loss": 0.002,
      "step": 146920
    },
    {
      "epoch": 7.836266666666667,
      "grad_norm": 0.035585738718509674,
      "learning_rate": 1.0233333333333334e-06,
      "loss": 0.0017,
      "step": 146930
    },
    {
      "epoch": 7.8368,
      "grad_norm": 0.25741663575172424,
      "learning_rate": 1.0200000000000002e-06,
      "loss": 0.0015,
      "step": 146940
    },
    {
      "epoch": 7.8373333333333335,
      "grad_norm": 0.2813330888748169,
      "learning_rate": 1.0166666666666665e-06,
      "loss": 0.0014,
      "step": 146950
    },
    {
      "epoch": 7.837866666666667,
      "grad_norm": 0.09156673401594162,
      "learning_rate": 1.0133333333333333e-06,
      "loss": 0.0023,
      "step": 146960
    },
    {
      "epoch": 7.8384,
      "grad_norm": 0.36657896637916565,
      "learning_rate": 1.01e-06,
      "loss": 0.0016,
      "step": 146970
    },
    {
      "epoch": 7.838933333333333,
      "grad_norm": 0.1208401694893837,
      "learning_rate": 1.0066666666666666e-06,
      "loss": 0.0025,
      "step": 146980
    },
    {
      "epoch": 7.839466666666667,
      "grad_norm": 0.22640442848205566,
      "learning_rate": 1.0033333333333334e-06,
      "loss": 0.0021,
      "step": 146990
    },
    {
      "epoch": 7.84,
      "grad_norm": 0.5639950633049011,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.0022,
      "step": 147000
    },
    {
      "epoch": 7.840533333333333,
      "grad_norm": 0.42973119020462036,
      "learning_rate": 9.966666666666667e-07,
      "loss": 0.0023,
      "step": 147010
    },
    {
      "epoch": 7.841066666666666,
      "grad_norm": 0.22944404184818268,
      "learning_rate": 9.933333333333335e-07,
      "loss": 0.0027,
      "step": 147020
    },
    {
      "epoch": 7.8416,
      "grad_norm": 0.05274275317788124,
      "learning_rate": 9.9e-07,
      "loss": 0.0023,
      "step": 147030
    },
    {
      "epoch": 7.842133333333333,
      "grad_norm": 0.335553377866745,
      "learning_rate": 9.866666666666666e-07,
      "loss": 0.002,
      "step": 147040
    },
    {
      "epoch": 7.842666666666666,
      "grad_norm": 0.03659939393401146,
      "learning_rate": 9.833333333333334e-07,
      "loss": 0.0015,
      "step": 147050
    },
    {
      "epoch": 7.8431999999999995,
      "grad_norm": 0.25975504517555237,
      "learning_rate": 9.8e-07,
      "loss": 0.0023,
      "step": 147060
    },
    {
      "epoch": 7.843733333333334,
      "grad_norm": 0.12226387113332748,
      "learning_rate": 9.766666666666667e-07,
      "loss": 0.0012,
      "step": 147070
    },
    {
      "epoch": 7.844266666666667,
      "grad_norm": 0.07079175859689713,
      "learning_rate": 9.733333333333335e-07,
      "loss": 0.002,
      "step": 147080
    },
    {
      "epoch": 7.8448,
      "grad_norm": 0.0430445522069931,
      "learning_rate": 9.7e-07,
      "loss": 0.0019,
      "step": 147090
    },
    {
      "epoch": 7.8453333333333335,
      "grad_norm": 0.1827678382396698,
      "learning_rate": 9.666666666666668e-07,
      "loss": 0.0015,
      "step": 147100
    },
    {
      "epoch": 7.845866666666667,
      "grad_norm": 0.06766054034233093,
      "learning_rate": 9.633333333333334e-07,
      "loss": 0.0017,
      "step": 147110
    },
    {
      "epoch": 7.8464,
      "grad_norm": 0.352514386177063,
      "learning_rate": 9.6e-07,
      "loss": 0.0015,
      "step": 147120
    },
    {
      "epoch": 7.846933333333333,
      "grad_norm": 0.12057941406965256,
      "learning_rate": 9.566666666666667e-07,
      "loss": 0.0024,
      "step": 147130
    },
    {
      "epoch": 7.847466666666667,
      "grad_norm": 0.20286925137043,
      "learning_rate": 9.533333333333333e-07,
      "loss": 0.0012,
      "step": 147140
    },
    {
      "epoch": 7.848,
      "grad_norm": 0.20662198960781097,
      "learning_rate": 9.5e-07,
      "loss": 0.0013,
      "step": 147150
    },
    {
      "epoch": 7.848533333333333,
      "grad_norm": 0.11474539339542389,
      "learning_rate": 9.466666666666667e-07,
      "loss": 0.0023,
      "step": 147160
    },
    {
      "epoch": 7.849066666666666,
      "grad_norm": 0.052431728690862656,
      "learning_rate": 9.433333333333334e-07,
      "loss": 0.0022,
      "step": 147170
    },
    {
      "epoch": 7.8496,
      "grad_norm": 0.057663023471832275,
      "learning_rate": 9.400000000000001e-07,
      "loss": 0.0028,
      "step": 147180
    },
    {
      "epoch": 7.850133333333333,
      "grad_norm": 0.08424115926027298,
      "learning_rate": 9.366666666666668e-07,
      "loss": 0.002,
      "step": 147190
    },
    {
      "epoch": 7.850666666666667,
      "grad_norm": 0.1996781975030899,
      "learning_rate": 9.333333333333334e-07,
      "loss": 0.0026,
      "step": 147200
    },
    {
      "epoch": 7.8512,
      "grad_norm": 0.06488703191280365,
      "learning_rate": 9.3e-07,
      "loss": 0.0013,
      "step": 147210
    },
    {
      "epoch": 7.851733333333334,
      "grad_norm": 0.23443256318569183,
      "learning_rate": 9.266666666666667e-07,
      "loss": 0.0016,
      "step": 147220
    },
    {
      "epoch": 7.852266666666667,
      "grad_norm": 0.04038944095373154,
      "learning_rate": 9.233333333333333e-07,
      "loss": 0.0019,
      "step": 147230
    },
    {
      "epoch": 7.8528,
      "grad_norm": 0.20308803021907806,
      "learning_rate": 9.2e-07,
      "loss": 0.0016,
      "step": 147240
    },
    {
      "epoch": 7.8533333333333335,
      "grad_norm": 0.11665268242359161,
      "learning_rate": 9.166666666666667e-07,
      "loss": 0.0014,
      "step": 147250
    },
    {
      "epoch": 7.853866666666667,
      "grad_norm": 0.3390648663043976,
      "learning_rate": 9.133333333333334e-07,
      "loss": 0.0018,
      "step": 147260
    },
    {
      "epoch": 7.8544,
      "grad_norm": 0.22624321281909943,
      "learning_rate": 9.100000000000001e-07,
      "loss": 0.0022,
      "step": 147270
    },
    {
      "epoch": 7.854933333333333,
      "grad_norm": 0.12131747603416443,
      "learning_rate": 9.066666666666667e-07,
      "loss": 0.0018,
      "step": 147280
    },
    {
      "epoch": 7.855466666666667,
      "grad_norm": 0.2628142237663269,
      "learning_rate": 9.033333333333335e-07,
      "loss": 0.0013,
      "step": 147290
    },
    {
      "epoch": 7.856,
      "grad_norm": 0.03230881318449974,
      "learning_rate": 9e-07,
      "loss": 0.0024,
      "step": 147300
    },
    {
      "epoch": 7.856533333333333,
      "grad_norm": 0.34218573570251465,
      "learning_rate": 8.966666666666666e-07,
      "loss": 0.0012,
      "step": 147310
    },
    {
      "epoch": 7.857066666666666,
      "grad_norm": 0.4256841242313385,
      "learning_rate": 8.933333333333334e-07,
      "loss": 0.0016,
      "step": 147320
    },
    {
      "epoch": 7.8576,
      "grad_norm": 0.11753383278846741,
      "learning_rate": 8.900000000000001e-07,
      "loss": 0.0016,
      "step": 147330
    },
    {
      "epoch": 7.858133333333333,
      "grad_norm": 0.07157570868730545,
      "learning_rate": 8.866666666666667e-07,
      "loss": 0.0016,
      "step": 147340
    },
    {
      "epoch": 7.858666666666666,
      "grad_norm": 0.30992549657821655,
      "learning_rate": 8.833333333333334e-07,
      "loss": 0.0017,
      "step": 147350
    },
    {
      "epoch": 7.8591999999999995,
      "grad_norm": 0.08486849814653397,
      "learning_rate": 8.8e-07,
      "loss": 0.0024,
      "step": 147360
    },
    {
      "epoch": 7.859733333333334,
      "grad_norm": 0.4521626830101013,
      "learning_rate": 8.766666666666668e-07,
      "loss": 0.0022,
      "step": 147370
    },
    {
      "epoch": 7.860266666666667,
      "grad_norm": 0.0901646763086319,
      "learning_rate": 8.733333333333333e-07,
      "loss": 0.002,
      "step": 147380
    },
    {
      "epoch": 7.8608,
      "grad_norm": 0.0670359879732132,
      "learning_rate": 8.699999999999999e-07,
      "loss": 0.0018,
      "step": 147390
    },
    {
      "epoch": 7.8613333333333335,
      "grad_norm": 0.363083153963089,
      "learning_rate": 8.666666666666667e-07,
      "loss": 0.0017,
      "step": 147400
    },
    {
      "epoch": 7.861866666666667,
      "grad_norm": 0.10132806748151779,
      "learning_rate": 8.633333333333334e-07,
      "loss": 0.0018,
      "step": 147410
    },
    {
      "epoch": 7.8624,
      "grad_norm": 0.2294759750366211,
      "learning_rate": 8.6e-07,
      "loss": 0.002,
      "step": 147420
    },
    {
      "epoch": 7.862933333333333,
      "grad_norm": 0.11710192263126373,
      "learning_rate": 8.566666666666667e-07,
      "loss": 0.002,
      "step": 147430
    },
    {
      "epoch": 7.863466666666667,
      "grad_norm": 0.0347047820687294,
      "learning_rate": 8.533333333333335e-07,
      "loss": 0.0015,
      "step": 147440
    },
    {
      "epoch": 7.864,
      "grad_norm": 0.3318674862384796,
      "learning_rate": 8.500000000000001e-07,
      "loss": 0.0014,
      "step": 147450
    },
    {
      "epoch": 7.864533333333333,
      "grad_norm": 0.05861292779445648,
      "learning_rate": 8.466666666666668e-07,
      "loss": 0.0016,
      "step": 147460
    },
    {
      "epoch": 7.865066666666666,
      "grad_norm": 0.5411278009414673,
      "learning_rate": 8.433333333333333e-07,
      "loss": 0.0013,
      "step": 147470
    },
    {
      "epoch": 7.8656,
      "grad_norm": 0.030408773571252823,
      "learning_rate": 8.4e-07,
      "loss": 0.0018,
      "step": 147480
    },
    {
      "epoch": 7.866133333333333,
      "grad_norm": 0.39768004417419434,
      "learning_rate": 8.366666666666667e-07,
      "loss": 0.0025,
      "step": 147490
    },
    {
      "epoch": 7.866666666666667,
      "grad_norm": 0.11827240884304047,
      "learning_rate": 8.333333333333333e-07,
      "loss": 0.0014,
      "step": 147500
    },
    {
      "epoch": 7.8672,
      "grad_norm": 0.12028846144676208,
      "learning_rate": 8.300000000000001e-07,
      "loss": 0.0022,
      "step": 147510
    },
    {
      "epoch": 7.867733333333334,
      "grad_norm": 0.07227972894906998,
      "learning_rate": 8.266666666666668e-07,
      "loss": 0.0015,
      "step": 147520
    },
    {
      "epoch": 7.868266666666667,
      "grad_norm": 0.22960688173770905,
      "learning_rate": 8.233333333333334e-07,
      "loss": 0.0016,
      "step": 147530
    },
    {
      "epoch": 7.8688,
      "grad_norm": 0.08892130851745605,
      "learning_rate": 8.200000000000001e-07,
      "loss": 0.002,
      "step": 147540
    },
    {
      "epoch": 7.8693333333333335,
      "grad_norm": 0.0393863208591938,
      "learning_rate": 8.166666666666666e-07,
      "loss": 0.0015,
      "step": 147550
    },
    {
      "epoch": 7.869866666666667,
      "grad_norm": 0.12422008067369461,
      "learning_rate": 8.133333333333333e-07,
      "loss": 0.0023,
      "step": 147560
    },
    {
      "epoch": 7.8704,
      "grad_norm": 0.20072242617607117,
      "learning_rate": 8.1e-07,
      "loss": 0.0014,
      "step": 147570
    },
    {
      "epoch": 7.870933333333333,
      "grad_norm": 0.17854157090187073,
      "learning_rate": 8.066666666666666e-07,
      "loss": 0.0012,
      "step": 147580
    },
    {
      "epoch": 7.871466666666667,
      "grad_norm": 0.1532277762889862,
      "learning_rate": 8.033333333333334e-07,
      "loss": 0.0016,
      "step": 147590
    },
    {
      "epoch": 7.872,
      "grad_norm": 0.284252792596817,
      "learning_rate": 8.000000000000001e-07,
      "loss": 0.0013,
      "step": 147600
    },
    {
      "epoch": 7.872533333333333,
      "grad_norm": 0.04963906854391098,
      "learning_rate": 7.966666666666667e-07,
      "loss": 0.0017,
      "step": 147610
    },
    {
      "epoch": 7.873066666666666,
      "grad_norm": 0.08727937936782837,
      "learning_rate": 7.933333333333334e-07,
      "loss": 0.0019,
      "step": 147620
    },
    {
      "epoch": 7.8736,
      "grad_norm": 0.06513305753469467,
      "learning_rate": 7.900000000000002e-07,
      "loss": 0.0017,
      "step": 147630
    },
    {
      "epoch": 7.874133333333333,
      "grad_norm": 0.14308618009090424,
      "learning_rate": 7.866666666666666e-07,
      "loss": 0.0019,
      "step": 147640
    },
    {
      "epoch": 7.874666666666666,
      "grad_norm": 0.03224606439471245,
      "learning_rate": 7.833333333333333e-07,
      "loss": 0.0018,
      "step": 147650
    },
    {
      "epoch": 7.8751999999999995,
      "grad_norm": 0.1413087546825409,
      "learning_rate": 7.8e-07,
      "loss": 0.0016,
      "step": 147660
    },
    {
      "epoch": 7.875733333333334,
      "grad_norm": 0.1994585543870926,
      "learning_rate": 7.766666666666667e-07,
      "loss": 0.0018,
      "step": 147670
    },
    {
      "epoch": 7.876266666666667,
      "grad_norm": 0.16876590251922607,
      "learning_rate": 7.733333333333334e-07,
      "loss": 0.0019,
      "step": 147680
    },
    {
      "epoch": 7.8768,
      "grad_norm": 0.052184972912073135,
      "learning_rate": 7.7e-07,
      "loss": 0.0015,
      "step": 147690
    },
    {
      "epoch": 7.8773333333333335,
      "grad_norm": 0.16994914412498474,
      "learning_rate": 7.666666666666667e-07,
      "loss": 0.0015,
      "step": 147700
    },
    {
      "epoch": 7.877866666666667,
      "grad_norm": 0.1740427017211914,
      "learning_rate": 7.633333333333334e-07,
      "loss": 0.0017,
      "step": 147710
    },
    {
      "epoch": 7.8784,
      "grad_norm": 0.1415146291255951,
      "learning_rate": 7.6e-07,
      "loss": 0.0013,
      "step": 147720
    },
    {
      "epoch": 7.878933333333333,
      "grad_norm": 0.19204601645469666,
      "learning_rate": 7.566666666666667e-07,
      "loss": 0.0013,
      "step": 147730
    },
    {
      "epoch": 7.879466666666667,
      "grad_norm": 0.28306034207344055,
      "learning_rate": 7.533333333333334e-07,
      "loss": 0.0014,
      "step": 147740
    },
    {
      "epoch": 7.88,
      "grad_norm": 0.09023508429527283,
      "learning_rate": 7.5e-07,
      "loss": 0.0022,
      "step": 147750
    },
    {
      "epoch": 7.880533333333333,
      "grad_norm": 0.25608763098716736,
      "learning_rate": 7.466666666666667e-07,
      "loss": 0.0013,
      "step": 147760
    },
    {
      "epoch": 7.881066666666666,
      "grad_norm": 0.0509992353618145,
      "learning_rate": 7.433333333333333e-07,
      "loss": 0.002,
      "step": 147770
    },
    {
      "epoch": 7.8816,
      "grad_norm": 0.2581119239330292,
      "learning_rate": 7.400000000000001e-07,
      "loss": 0.0015,
      "step": 147780
    },
    {
      "epoch": 7.882133333333333,
      "grad_norm": 0.256457656621933,
      "learning_rate": 7.366666666666667e-07,
      "loss": 0.0021,
      "step": 147790
    },
    {
      "epoch": 7.882666666666667,
      "grad_norm": 0.11482059210538864,
      "learning_rate": 7.333333333333333e-07,
      "loss": 0.0015,
      "step": 147800
    },
    {
      "epoch": 7.8832,
      "grad_norm": 0.11891230195760727,
      "learning_rate": 7.3e-07,
      "loss": 0.0013,
      "step": 147810
    },
    {
      "epoch": 7.883733333333334,
      "grad_norm": 0.14167913794517517,
      "learning_rate": 7.266666666666668e-07,
      "loss": 0.0024,
      "step": 147820
    },
    {
      "epoch": 7.884266666666667,
      "grad_norm": 0.1420174241065979,
      "learning_rate": 7.233333333333333e-07,
      "loss": 0.0021,
      "step": 147830
    },
    {
      "epoch": 7.8848,
      "grad_norm": 0.19877246022224426,
      "learning_rate": 7.2e-07,
      "loss": 0.0015,
      "step": 147840
    },
    {
      "epoch": 7.8853333333333335,
      "grad_norm": 0.2268415242433548,
      "learning_rate": 7.166666666666667e-07,
      "loss": 0.0012,
      "step": 147850
    },
    {
      "epoch": 7.885866666666667,
      "grad_norm": 0.09658685326576233,
      "learning_rate": 7.133333333333334e-07,
      "loss": 0.002,
      "step": 147860
    },
    {
      "epoch": 7.8864,
      "grad_norm": 0.20272284746170044,
      "learning_rate": 7.100000000000001e-07,
      "loss": 0.002,
      "step": 147870
    },
    {
      "epoch": 7.886933333333333,
      "grad_norm": 0.2235182821750641,
      "learning_rate": 7.066666666666666e-07,
      "loss": 0.0017,
      "step": 147880
    },
    {
      "epoch": 7.887466666666667,
      "grad_norm": 0.4561792314052582,
      "learning_rate": 7.033333333333334e-07,
      "loss": 0.0012,
      "step": 147890
    },
    {
      "epoch": 7.888,
      "grad_norm": 0.16079112887382507,
      "learning_rate": 7.000000000000001e-07,
      "loss": 0.0015,
      "step": 147900
    },
    {
      "epoch": 7.888533333333333,
      "grad_norm": 0.2013619989156723,
      "learning_rate": 6.966666666666667e-07,
      "loss": 0.0019,
      "step": 147910
    },
    {
      "epoch": 7.8890666666666664,
      "grad_norm": 0.23217469453811646,
      "learning_rate": 6.933333333333333e-07,
      "loss": 0.002,
      "step": 147920
    },
    {
      "epoch": 7.8896,
      "grad_norm": 0.19603712856769562,
      "learning_rate": 6.900000000000001e-07,
      "loss": 0.0023,
      "step": 147930
    },
    {
      "epoch": 7.890133333333333,
      "grad_norm": 0.1539643406867981,
      "learning_rate": 6.866666666666667e-07,
      "loss": 0.0018,
      "step": 147940
    },
    {
      "epoch": 7.890666666666666,
      "grad_norm": 0.14468076825141907,
      "learning_rate": 6.833333333333334e-07,
      "loss": 0.0024,
      "step": 147950
    },
    {
      "epoch": 7.8911999999999995,
      "grad_norm": 0.14702430367469788,
      "learning_rate": 6.8e-07,
      "loss": 0.0016,
      "step": 147960
    },
    {
      "epoch": 7.891733333333334,
      "grad_norm": 0.20080214738845825,
      "learning_rate": 6.766666666666667e-07,
      "loss": 0.0014,
      "step": 147970
    },
    {
      "epoch": 7.892266666666667,
      "grad_norm": 0.08868853002786636,
      "learning_rate": 6.733333333333334e-07,
      "loss": 0.0021,
      "step": 147980
    },
    {
      "epoch": 7.8928,
      "grad_norm": 0.3923524022102356,
      "learning_rate": 6.7e-07,
      "loss": 0.0015,
      "step": 147990
    },
    {
      "epoch": 7.8933333333333335,
      "grad_norm": 0.34077197313308716,
      "learning_rate": 6.666666666666667e-07,
      "loss": 0.0013,
      "step": 148000
    },
    {
      "epoch": 7.893866666666667,
      "grad_norm": 0.06760185211896896,
      "learning_rate": 6.633333333333334e-07,
      "loss": 0.0019,
      "step": 148010
    },
    {
      "epoch": 7.8944,
      "grad_norm": 0.14240285754203796,
      "learning_rate": 6.6e-07,
      "loss": 0.0022,
      "step": 148020
    },
    {
      "epoch": 7.894933333333333,
      "grad_norm": 0.18068242073059082,
      "learning_rate": 6.566666666666667e-07,
      "loss": 0.0014,
      "step": 148030
    },
    {
      "epoch": 7.895466666666667,
      "grad_norm": 0.0890105739235878,
      "learning_rate": 6.533333333333334e-07,
      "loss": 0.0015,
      "step": 148040
    },
    {
      "epoch": 7.896,
      "grad_norm": 0.1722337156534195,
      "learning_rate": 6.5e-07,
      "loss": 0.0022,
      "step": 148050
    },
    {
      "epoch": 7.896533333333333,
      "grad_norm": 0.06881948560476303,
      "learning_rate": 6.466666666666667e-07,
      "loss": 0.0021,
      "step": 148060
    },
    {
      "epoch": 7.8970666666666665,
      "grad_norm": 0.5455797910690308,
      "learning_rate": 6.433333333333334e-07,
      "loss": 0.0021,
      "step": 148070
    },
    {
      "epoch": 7.8976,
      "grad_norm": 0.0349106527864933,
      "learning_rate": 6.4e-07,
      "loss": 0.0022,
      "step": 148080
    },
    {
      "epoch": 7.898133333333333,
      "grad_norm": 0.3746225833892822,
      "learning_rate": 6.366666666666667e-07,
      "loss": 0.0028,
      "step": 148090
    },
    {
      "epoch": 7.898666666666666,
      "grad_norm": 0.08859989047050476,
      "learning_rate": 6.333333333333333e-07,
      "loss": 0.0016,
      "step": 148100
    },
    {
      "epoch": 7.8992,
      "grad_norm": 0.17092400789260864,
      "learning_rate": 6.3e-07,
      "loss": 0.0015,
      "step": 148110
    },
    {
      "epoch": 7.899733333333334,
      "grad_norm": 0.307394802570343,
      "learning_rate": 6.266666666666668e-07,
      "loss": 0.0011,
      "step": 148120
    },
    {
      "epoch": 7.900266666666667,
      "grad_norm": 0.5832656025886536,
      "learning_rate": 6.233333333333333e-07,
      "loss": 0.0016,
      "step": 148130
    },
    {
      "epoch": 7.9008,
      "grad_norm": 0.08797237277030945,
      "learning_rate": 6.2e-07,
      "loss": 0.0022,
      "step": 148140
    },
    {
      "epoch": 7.9013333333333335,
      "grad_norm": 0.06111929938197136,
      "learning_rate": 6.166666666666667e-07,
      "loss": 0.0011,
      "step": 148150
    },
    {
      "epoch": 7.901866666666667,
      "grad_norm": 0.3654634356498718,
      "learning_rate": 6.133333333333334e-07,
      "loss": 0.0019,
      "step": 148160
    },
    {
      "epoch": 7.9024,
      "grad_norm": 0.261874794960022,
      "learning_rate": 6.100000000000001e-07,
      "loss": 0.0023,
      "step": 148170
    },
    {
      "epoch": 7.902933333333333,
      "grad_norm": 0.31137317419052124,
      "learning_rate": 6.066666666666666e-07,
      "loss": 0.0018,
      "step": 148180
    },
    {
      "epoch": 7.903466666666667,
      "grad_norm": 0.22469323873519897,
      "learning_rate": 6.033333333333333e-07,
      "loss": 0.0012,
      "step": 148190
    },
    {
      "epoch": 7.904,
      "grad_norm": 0.39984455704689026,
      "learning_rate": 6.000000000000001e-07,
      "loss": 0.0013,
      "step": 148200
    },
    {
      "epoch": 7.904533333333333,
      "grad_norm": 0.3393220603466034,
      "learning_rate": 5.966666666666667e-07,
      "loss": 0.0015,
      "step": 148210
    },
    {
      "epoch": 7.9050666666666665,
      "grad_norm": 0.32907190918922424,
      "learning_rate": 5.933333333333333e-07,
      "loss": 0.0018,
      "step": 148220
    },
    {
      "epoch": 7.9056,
      "grad_norm": 0.20246948301792145,
      "learning_rate": 5.9e-07,
      "loss": 0.0021,
      "step": 148230
    },
    {
      "epoch": 7.906133333333333,
      "grad_norm": 0.0789075642824173,
      "learning_rate": 5.866666666666667e-07,
      "loss": 0.0023,
      "step": 148240
    },
    {
      "epoch": 7.906666666666666,
      "grad_norm": 0.1516360342502594,
      "learning_rate": 5.833333333333334e-07,
      "loss": 0.0018,
      "step": 148250
    },
    {
      "epoch": 7.9072,
      "grad_norm": 0.47264280915260315,
      "learning_rate": 5.8e-07,
      "loss": 0.0018,
      "step": 148260
    },
    {
      "epoch": 7.907733333333333,
      "grad_norm": 0.3362131714820862,
      "learning_rate": 5.766666666666667e-07,
      "loss": 0.0017,
      "step": 148270
    },
    {
      "epoch": 7.908266666666667,
      "grad_norm": 0.033769190311431885,
      "learning_rate": 5.733333333333334e-07,
      "loss": 0.002,
      "step": 148280
    },
    {
      "epoch": 7.9088,
      "grad_norm": 0.14975613355636597,
      "learning_rate": 5.7e-07,
      "loss": 0.0021,
      "step": 148290
    },
    {
      "epoch": 7.9093333333333335,
      "grad_norm": 0.17157942056655884,
      "learning_rate": 5.666666666666667e-07,
      "loss": 0.0018,
      "step": 148300
    },
    {
      "epoch": 7.909866666666667,
      "grad_norm": 0.29153794050216675,
      "learning_rate": 5.633333333333334e-07,
      "loss": 0.002,
      "step": 148310
    },
    {
      "epoch": 7.9104,
      "grad_norm": 0.08591657876968384,
      "learning_rate": 5.6e-07,
      "loss": 0.0021,
      "step": 148320
    },
    {
      "epoch": 7.910933333333333,
      "grad_norm": 0.11900067329406738,
      "learning_rate": 5.566666666666667e-07,
      "loss": 0.0027,
      "step": 148330
    },
    {
      "epoch": 7.911466666666667,
      "grad_norm": 0.15139715373516083,
      "learning_rate": 5.533333333333334e-07,
      "loss": 0.0016,
      "step": 148340
    },
    {
      "epoch": 7.912,
      "grad_norm": 0.1170843243598938,
      "learning_rate": 5.5e-07,
      "loss": 0.0024,
      "step": 148350
    },
    {
      "epoch": 7.912533333333333,
      "grad_norm": 0.12024683505296707,
      "learning_rate": 5.466666666666667e-07,
      "loss": 0.0017,
      "step": 148360
    },
    {
      "epoch": 7.9130666666666665,
      "grad_norm": 0.07416392117738724,
      "learning_rate": 5.433333333333334e-07,
      "loss": 0.0022,
      "step": 148370
    },
    {
      "epoch": 7.9136,
      "grad_norm": 0.04546629264950752,
      "learning_rate": 5.4e-07,
      "loss": 0.0014,
      "step": 148380
    },
    {
      "epoch": 7.914133333333333,
      "grad_norm": 0.3957737982273102,
      "learning_rate": 5.366666666666667e-07,
      "loss": 0.0013,
      "step": 148390
    },
    {
      "epoch": 7.914666666666666,
      "grad_norm": 0.030280498787760735,
      "learning_rate": 5.333333333333333e-07,
      "loss": 0.0014,
      "step": 148400
    },
    {
      "epoch": 7.9152000000000005,
      "grad_norm": 0.05330728366971016,
      "learning_rate": 5.3e-07,
      "loss": 0.0011,
      "step": 148410
    },
    {
      "epoch": 7.915733333333334,
      "grad_norm": 0.1994766891002655,
      "learning_rate": 5.266666666666667e-07,
      "loss": 0.0012,
      "step": 148420
    },
    {
      "epoch": 7.916266666666667,
      "grad_norm": 0.3668763339519501,
      "learning_rate": 5.233333333333334e-07,
      "loss": 0.002,
      "step": 148430
    },
    {
      "epoch": 7.9168,
      "grad_norm": 0.12152896821498871,
      "learning_rate": 5.2e-07,
      "loss": 0.0022,
      "step": 148440
    },
    {
      "epoch": 7.917333333333334,
      "grad_norm": 0.1430022269487381,
      "learning_rate": 5.166666666666667e-07,
      "loss": 0.0026,
      "step": 148450
    },
    {
      "epoch": 7.917866666666667,
      "grad_norm": 0.057333942502737045,
      "learning_rate": 5.133333333333333e-07,
      "loss": 0.0017,
      "step": 148460
    },
    {
      "epoch": 7.9184,
      "grad_norm": 0.17720884084701538,
      "learning_rate": 5.100000000000001e-07,
      "loss": 0.0018,
      "step": 148470
    },
    {
      "epoch": 7.918933333333333,
      "grad_norm": 0.2525789141654968,
      "learning_rate": 5.066666666666667e-07,
      "loss": 0.0018,
      "step": 148480
    },
    {
      "epoch": 7.919466666666667,
      "grad_norm": 0.20820175111293793,
      "learning_rate": 5.033333333333333e-07,
      "loss": 0.0015,
      "step": 148490
    },
    {
      "epoch": 7.92,
      "grad_norm": 0.043714992702007294,
      "learning_rate": 5.000000000000001e-07,
      "loss": 0.0015,
      "step": 148500
    },
    {
      "epoch": 7.920533333333333,
      "grad_norm": 0.3210049867630005,
      "learning_rate": 4.966666666666667e-07,
      "loss": 0.0013,
      "step": 148510
    },
    {
      "epoch": 7.9210666666666665,
      "grad_norm": 0.08030126243829727,
      "learning_rate": 4.933333333333333e-07,
      "loss": 0.0011,
      "step": 148520
    },
    {
      "epoch": 7.9216,
      "grad_norm": 0.1833675652742386,
      "learning_rate": 4.9e-07,
      "loss": 0.0013,
      "step": 148530
    },
    {
      "epoch": 7.922133333333333,
      "grad_norm": 0.11728494614362717,
      "learning_rate": 4.866666666666667e-07,
      "loss": 0.0013,
      "step": 148540
    },
    {
      "epoch": 7.922666666666666,
      "grad_norm": 0.16131918132305145,
      "learning_rate": 4.833333333333334e-07,
      "loss": 0.002,
      "step": 148550
    },
    {
      "epoch": 7.9232,
      "grad_norm": 0.1724245697259903,
      "learning_rate": 4.8e-07,
      "loss": 0.002,
      "step": 148560
    },
    {
      "epoch": 7.923733333333333,
      "grad_norm": 0.3306589722633362,
      "learning_rate": 4.7666666666666667e-07,
      "loss": 0.0019,
      "step": 148570
    },
    {
      "epoch": 7.924266666666667,
      "grad_norm": 0.22011341154575348,
      "learning_rate": 4.7333333333333334e-07,
      "loss": 0.0013,
      "step": 148580
    },
    {
      "epoch": 7.9248,
      "grad_norm": 0.19991379976272583,
      "learning_rate": 4.7000000000000005e-07,
      "loss": 0.0014,
      "step": 148590
    },
    {
      "epoch": 7.925333333333334,
      "grad_norm": 0.15142586827278137,
      "learning_rate": 4.666666666666667e-07,
      "loss": 0.0019,
      "step": 148600
    },
    {
      "epoch": 7.925866666666667,
      "grad_norm": 0.1187024861574173,
      "learning_rate": 4.6333333333333333e-07,
      "loss": 0.0016,
      "step": 148610
    },
    {
      "epoch": 7.9264,
      "grad_norm": 0.19937504827976227,
      "learning_rate": 4.6e-07,
      "loss": 0.0017,
      "step": 148620
    },
    {
      "epoch": 7.926933333333333,
      "grad_norm": 0.4231519401073456,
      "learning_rate": 4.566666666666667e-07,
      "loss": 0.0015,
      "step": 148630
    },
    {
      "epoch": 7.927466666666667,
      "grad_norm": 0.2185603231191635,
      "learning_rate": 4.5333333333333337e-07,
      "loss": 0.002,
      "step": 148640
    },
    {
      "epoch": 7.928,
      "grad_norm": 0.20214566588401794,
      "learning_rate": 4.5e-07,
      "loss": 0.0016,
      "step": 148650
    },
    {
      "epoch": 7.928533333333333,
      "grad_norm": 0.05997743085026741,
      "learning_rate": 4.466666666666667e-07,
      "loss": 0.0018,
      "step": 148660
    },
    {
      "epoch": 7.9290666666666665,
      "grad_norm": 0.043374404311180115,
      "learning_rate": 4.4333333333333336e-07,
      "loss": 0.0015,
      "step": 148670
    },
    {
      "epoch": 7.9296,
      "grad_norm": 0.14831453561782837,
      "learning_rate": 4.4e-07,
      "loss": 0.0017,
      "step": 148680
    },
    {
      "epoch": 7.930133333333333,
      "grad_norm": 0.20001345872879028,
      "learning_rate": 4.3666666666666663e-07,
      "loss": 0.0013,
      "step": 148690
    },
    {
      "epoch": 7.930666666666666,
      "grad_norm": 0.20240207016468048,
      "learning_rate": 4.3333333333333335e-07,
      "loss": 0.0025,
      "step": 148700
    },
    {
      "epoch": 7.9312000000000005,
      "grad_norm": 0.032116085290908813,
      "learning_rate": 4.3e-07,
      "loss": 0.0023,
      "step": 148710
    },
    {
      "epoch": 7.931733333333334,
      "grad_norm": 0.17843839526176453,
      "learning_rate": 4.2666666666666673e-07,
      "loss": 0.0016,
      "step": 148720
    },
    {
      "epoch": 7.932266666666667,
      "grad_norm": 0.11287254840135574,
      "learning_rate": 4.233333333333334e-07,
      "loss": 0.0018,
      "step": 148730
    },
    {
      "epoch": 7.9328,
      "grad_norm": 0.04061659798026085,
      "learning_rate": 4.2e-07,
      "loss": 0.0019,
      "step": 148740
    },
    {
      "epoch": 7.933333333333334,
      "grad_norm": 0.11427003890275955,
      "learning_rate": 4.1666666666666667e-07,
      "loss": 0.0016,
      "step": 148750
    },
    {
      "epoch": 7.933866666666667,
      "grad_norm": 0.5313931703567505,
      "learning_rate": 4.133333333333334e-07,
      "loss": 0.0012,
      "step": 148760
    },
    {
      "epoch": 7.9344,
      "grad_norm": 0.11911015957593918,
      "learning_rate": 4.1000000000000004e-07,
      "loss": 0.0023,
      "step": 148770
    },
    {
      "epoch": 7.934933333333333,
      "grad_norm": 0.4269571006298065,
      "learning_rate": 4.0666666666666666e-07,
      "loss": 0.0015,
      "step": 148780
    },
    {
      "epoch": 7.935466666666667,
      "grad_norm": 0.14257915318012238,
      "learning_rate": 4.033333333333333e-07,
      "loss": 0.0018,
      "step": 148790
    },
    {
      "epoch": 7.936,
      "grad_norm": 0.06911013275384903,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 0.0018,
      "step": 148800
    },
    {
      "epoch": 7.936533333333333,
      "grad_norm": 0.06905800849199295,
      "learning_rate": 3.966666666666667e-07,
      "loss": 0.0023,
      "step": 148810
    },
    {
      "epoch": 7.9370666666666665,
      "grad_norm": 0.30907151103019714,
      "learning_rate": 3.933333333333333e-07,
      "loss": 0.0023,
      "step": 148820
    },
    {
      "epoch": 7.9376,
      "grad_norm": 0.1827092319726944,
      "learning_rate": 3.9e-07,
      "loss": 0.0014,
      "step": 148830
    },
    {
      "epoch": 7.938133333333333,
      "grad_norm": 0.1447889804840088,
      "learning_rate": 3.866666666666667e-07,
      "loss": 0.0019,
      "step": 148840
    },
    {
      "epoch": 7.938666666666666,
      "grad_norm": 0.08724680542945862,
      "learning_rate": 3.8333333333333335e-07,
      "loss": 0.0021,
      "step": 148850
    },
    {
      "epoch": 7.9392,
      "grad_norm": 0.09063874185085297,
      "learning_rate": 3.8e-07,
      "loss": 0.0021,
      "step": 148860
    },
    {
      "epoch": 7.939733333333333,
      "grad_norm": 0.1011742576956749,
      "learning_rate": 3.766666666666667e-07,
      "loss": 0.0013,
      "step": 148870
    },
    {
      "epoch": 7.940266666666667,
      "grad_norm": 0.27251240611076355,
      "learning_rate": 3.7333333333333334e-07,
      "loss": 0.0022,
      "step": 148880
    },
    {
      "epoch": 7.9408,
      "grad_norm": 0.4300595223903656,
      "learning_rate": 3.7000000000000006e-07,
      "loss": 0.0017,
      "step": 148890
    },
    {
      "epoch": 7.941333333333334,
      "grad_norm": 0.17927142977714539,
      "learning_rate": 3.6666666666666667e-07,
      "loss": 0.0018,
      "step": 148900
    },
    {
      "epoch": 7.941866666666667,
      "grad_norm": 0.12041433900594711,
      "learning_rate": 3.633333333333334e-07,
      "loss": 0.0014,
      "step": 148910
    },
    {
      "epoch": 7.9424,
      "grad_norm": 0.3113449215888977,
      "learning_rate": 3.6e-07,
      "loss": 0.0016,
      "step": 148920
    },
    {
      "epoch": 7.942933333333333,
      "grad_norm": 0.2611578702926636,
      "learning_rate": 3.566666666666667e-07,
      "loss": 0.0018,
      "step": 148930
    },
    {
      "epoch": 7.943466666666667,
      "grad_norm": 0.09408377856016159,
      "learning_rate": 3.533333333333333e-07,
      "loss": 0.0015,
      "step": 148940
    },
    {
      "epoch": 7.944,
      "grad_norm": 0.1488657146692276,
      "learning_rate": 3.5000000000000004e-07,
      "loss": 0.0013,
      "step": 148950
    },
    {
      "epoch": 7.944533333333333,
      "grad_norm": 0.1500629335641861,
      "learning_rate": 3.4666666666666665e-07,
      "loss": 0.002,
      "step": 148960
    },
    {
      "epoch": 7.9450666666666665,
      "grad_norm": 0.17389190196990967,
      "learning_rate": 3.4333333333333336e-07,
      "loss": 0.002,
      "step": 148970
    },
    {
      "epoch": 7.9456,
      "grad_norm": 0.06018953025341034,
      "learning_rate": 3.4e-07,
      "loss": 0.0016,
      "step": 148980
    },
    {
      "epoch": 7.946133333333333,
      "grad_norm": 0.08584966510534286,
      "learning_rate": 3.366666666666667e-07,
      "loss": 0.0018,
      "step": 148990
    },
    {
      "epoch": 7.946666666666666,
      "grad_norm": 0.06521007418632507,
      "learning_rate": 3.3333333333333335e-07,
      "loss": 0.0023,
      "step": 149000
    },
    {
      "epoch": 7.9472000000000005,
      "grad_norm": 0.1150653138756752,
      "learning_rate": 3.3e-07,
      "loss": 0.0019,
      "step": 149010
    },
    {
      "epoch": 7.947733333333334,
      "grad_norm": 0.037471871823072433,
      "learning_rate": 3.266666666666667e-07,
      "loss": 0.0014,
      "step": 149020
    },
    {
      "epoch": 7.948266666666667,
      "grad_norm": 0.132036954164505,
      "learning_rate": 3.2333333333333334e-07,
      "loss": 0.0011,
      "step": 149030
    },
    {
      "epoch": 7.9488,
      "grad_norm": 0.13355879485607147,
      "learning_rate": 3.2e-07,
      "loss": 0.0023,
      "step": 149040
    },
    {
      "epoch": 7.949333333333334,
      "grad_norm": 0.08750001341104507,
      "learning_rate": 3.1666666666666667e-07,
      "loss": 0.0011,
      "step": 149050
    },
    {
      "epoch": 7.949866666666667,
      "grad_norm": 0.21873977780342102,
      "learning_rate": 3.133333333333334e-07,
      "loss": 0.0017,
      "step": 149060
    },
    {
      "epoch": 7.9504,
      "grad_norm": 0.2514578402042389,
      "learning_rate": 3.1e-07,
      "loss": 0.0018,
      "step": 149070
    },
    {
      "epoch": 7.950933333333333,
      "grad_norm": 0.074568010866642,
      "learning_rate": 3.066666666666667e-07,
      "loss": 0.0025,
      "step": 149080
    },
    {
      "epoch": 7.951466666666667,
      "grad_norm": 0.19870807230472565,
      "learning_rate": 3.033333333333333e-07,
      "loss": 0.0015,
      "step": 149090
    },
    {
      "epoch": 7.952,
      "grad_norm": 0.0651707872748375,
      "learning_rate": 3.0000000000000004e-07,
      "loss": 0.0019,
      "step": 149100
    },
    {
      "epoch": 7.952533333333333,
      "grad_norm": 0.29502013325691223,
      "learning_rate": 2.9666666666666665e-07,
      "loss": 0.0023,
      "step": 149110
    },
    {
      "epoch": 7.9530666666666665,
      "grad_norm": 0.06138892099261284,
      "learning_rate": 2.9333333333333337e-07,
      "loss": 0.0012,
      "step": 149120
    },
    {
      "epoch": 7.9536,
      "grad_norm": 0.03878627344965935,
      "learning_rate": 2.9e-07,
      "loss": 0.0014,
      "step": 149130
    },
    {
      "epoch": 7.954133333333333,
      "grad_norm": 0.19805097579956055,
      "learning_rate": 2.866666666666667e-07,
      "loss": 0.0017,
      "step": 149140
    },
    {
      "epoch": 7.954666666666666,
      "grad_norm": 0.07027256488800049,
      "learning_rate": 2.8333333333333336e-07,
      "loss": 0.002,
      "step": 149150
    },
    {
      "epoch": 7.9552,
      "grad_norm": 0.13291914761066437,
      "learning_rate": 2.8e-07,
      "loss": 0.0015,
      "step": 149160
    },
    {
      "epoch": 7.955733333333333,
      "grad_norm": 0.09484916925430298,
      "learning_rate": 2.766666666666667e-07,
      "loss": 0.0026,
      "step": 149170
    },
    {
      "epoch": 7.956266666666667,
      "grad_norm": 0.06589870899915695,
      "learning_rate": 2.7333333333333335e-07,
      "loss": 0.0015,
      "step": 149180
    },
    {
      "epoch": 7.9568,
      "grad_norm": 0.4632030129432678,
      "learning_rate": 2.7e-07,
      "loss": 0.003,
      "step": 149190
    },
    {
      "epoch": 7.957333333333334,
      "grad_norm": 0.19994567334651947,
      "learning_rate": 2.6666666666666667e-07,
      "loss": 0.0013,
      "step": 149200
    },
    {
      "epoch": 7.957866666666667,
      "grad_norm": 0.28611263632774353,
      "learning_rate": 2.6333333333333334e-07,
      "loss": 0.0018,
      "step": 149210
    },
    {
      "epoch": 7.9584,
      "grad_norm": 0.31260302662849426,
      "learning_rate": 2.6e-07,
      "loss": 0.003,
      "step": 149220
    },
    {
      "epoch": 7.958933333333333,
      "grad_norm": 0.11946531385183334,
      "learning_rate": 2.5666666666666666e-07,
      "loss": 0.0032,
      "step": 149230
    },
    {
      "epoch": 7.959466666666667,
      "grad_norm": 0.2880447804927826,
      "learning_rate": 2.533333333333333e-07,
      "loss": 0.0021,
      "step": 149240
    },
    {
      "epoch": 7.96,
      "grad_norm": 0.3973303437232971,
      "learning_rate": 2.5000000000000004e-07,
      "loss": 0.0027,
      "step": 149250
    },
    {
      "epoch": 7.960533333333333,
      "grad_norm": 0.31556588411331177,
      "learning_rate": 2.4666666666666665e-07,
      "loss": 0.0017,
      "step": 149260
    },
    {
      "epoch": 7.9610666666666665,
      "grad_norm": 0.11495746672153473,
      "learning_rate": 2.4333333333333337e-07,
      "loss": 0.0021,
      "step": 149270
    },
    {
      "epoch": 7.9616,
      "grad_norm": 0.0471082404255867,
      "learning_rate": 2.4e-07,
      "loss": 0.0021,
      "step": 149280
    },
    {
      "epoch": 7.962133333333333,
      "grad_norm": 0.10141465812921524,
      "learning_rate": 2.3666666666666667e-07,
      "loss": 0.0023,
      "step": 149290
    },
    {
      "epoch": 7.962666666666666,
      "grad_norm": 0.27929192781448364,
      "learning_rate": 2.3333333333333336e-07,
      "loss": 0.0014,
      "step": 149300
    },
    {
      "epoch": 7.9632,
      "grad_norm": 0.060391344130039215,
      "learning_rate": 2.3e-07,
      "loss": 0.0014,
      "step": 149310
    },
    {
      "epoch": 7.963733333333334,
      "grad_norm": 0.028221584856510162,
      "learning_rate": 2.2666666666666668e-07,
      "loss": 0.0019,
      "step": 149320
    },
    {
      "epoch": 7.964266666666667,
      "grad_norm": 0.22803443670272827,
      "learning_rate": 2.2333333333333335e-07,
      "loss": 0.0034,
      "step": 149330
    },
    {
      "epoch": 7.9648,
      "grad_norm": 0.11731608957052231,
      "learning_rate": 2.2e-07,
      "loss": 0.0015,
      "step": 149340
    },
    {
      "epoch": 7.965333333333334,
      "grad_norm": 0.5528143644332886,
      "learning_rate": 2.1666666666666667e-07,
      "loss": 0.0019,
      "step": 149350
    },
    {
      "epoch": 7.965866666666667,
      "grad_norm": 0.06069355458021164,
      "learning_rate": 2.1333333333333336e-07,
      "loss": 0.0014,
      "step": 149360
    },
    {
      "epoch": 7.9664,
      "grad_norm": 0.03617629036307335,
      "learning_rate": 2.1e-07,
      "loss": 0.0012,
      "step": 149370
    },
    {
      "epoch": 7.966933333333333,
      "grad_norm": 0.08580201864242554,
      "learning_rate": 2.066666666666667e-07,
      "loss": 0.0023,
      "step": 149380
    },
    {
      "epoch": 7.967466666666667,
      "grad_norm": 0.14342111349105835,
      "learning_rate": 2.0333333333333333e-07,
      "loss": 0.0019,
      "step": 149390
    },
    {
      "epoch": 7.968,
      "grad_norm": 0.28413501381874084,
      "learning_rate": 2.0000000000000002e-07,
      "loss": 0.0014,
      "step": 149400
    },
    {
      "epoch": 7.968533333333333,
      "grad_norm": 0.17356330156326294,
      "learning_rate": 1.9666666666666665e-07,
      "loss": 0.0014,
      "step": 149410
    },
    {
      "epoch": 7.9690666666666665,
      "grad_norm": 0.1277569979429245,
      "learning_rate": 1.9333333333333334e-07,
      "loss": 0.002,
      "step": 149420
    },
    {
      "epoch": 7.9696,
      "grad_norm": 0.02897264063358307,
      "learning_rate": 1.9e-07,
      "loss": 0.0013,
      "step": 149430
    },
    {
      "epoch": 7.970133333333333,
      "grad_norm": 0.22949616611003876,
      "learning_rate": 1.8666666666666667e-07,
      "loss": 0.0018,
      "step": 149440
    },
    {
      "epoch": 7.970666666666666,
      "grad_norm": 0.09241646528244019,
      "learning_rate": 1.8333333333333333e-07,
      "loss": 0.0015,
      "step": 149450
    },
    {
      "epoch": 7.9712,
      "grad_norm": 0.3374110460281372,
      "learning_rate": 1.8e-07,
      "loss": 0.0021,
      "step": 149460
    },
    {
      "epoch": 7.971733333333333,
      "grad_norm": 0.08926139771938324,
      "learning_rate": 1.7666666666666666e-07,
      "loss": 0.0023,
      "step": 149470
    },
    {
      "epoch": 7.972266666666666,
      "grad_norm": 0.2579849362373352,
      "learning_rate": 1.7333333333333332e-07,
      "loss": 0.0019,
      "step": 149480
    },
    {
      "epoch": 7.9728,
      "grad_norm": 0.2570963203907013,
      "learning_rate": 1.7e-07,
      "loss": 0.0016,
      "step": 149490
    },
    {
      "epoch": 7.973333333333334,
      "grad_norm": 0.1988542228937149,
      "learning_rate": 1.6666666666666668e-07,
      "loss": 0.0012,
      "step": 149500
    }
  ],
  "logging_steps": 10,
  "max_steps": 150000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 8,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 40,
  "trial_name": null,
  "trial_params": null
}

{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.9733333333333334,
  "eval_steps": 500,
  "global_step": 37000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0005333333333333334,
      "grad_norm": 0.35102126002311707,
      "learning_rate": 4.9986666666666674e-05,
      "loss": 0.0025,
      "step": 10
    },
    {
      "epoch": 0.0010666666666666667,
      "grad_norm": 0.6368792057037354,
      "learning_rate": 4.997333333333333e-05,
      "loss": 0.0029,
      "step": 20
    },
    {
      "epoch": 0.0016,
      "grad_norm": 0.26681193709373474,
      "learning_rate": 4.996e-05,
      "loss": 0.0031,
      "step": 30
    },
    {
      "epoch": 0.0021333333333333334,
      "grad_norm": 0.9121682643890381,
      "learning_rate": 4.994666666666667e-05,
      "loss": 0.0027,
      "step": 40
    },
    {
      "epoch": 0.0026666666666666666,
      "grad_norm": 0.2002459466457367,
      "learning_rate": 4.993333333333334e-05,
      "loss": 0.0033,
      "step": 50
    },
    {
      "epoch": 0.0032,
      "grad_norm": 0.18833447992801666,
      "learning_rate": 4.992e-05,
      "loss": 0.004,
      "step": 60
    },
    {
      "epoch": 0.0037333333333333333,
      "grad_norm": 0.6092230677604675,
      "learning_rate": 4.990666666666667e-05,
      "loss": 0.0034,
      "step": 70
    },
    {
      "epoch": 0.004266666666666667,
      "grad_norm": 0.31568393111228943,
      "learning_rate": 4.989333333333334e-05,
      "loss": 0.0034,
      "step": 80
    },
    {
      "epoch": 0.0048,
      "grad_norm": 0.164932519197464,
      "learning_rate": 4.9880000000000004e-05,
      "loss": 0.0032,
      "step": 90
    },
    {
      "epoch": 0.005333333333333333,
      "grad_norm": 0.6411898136138916,
      "learning_rate": 4.986666666666667e-05,
      "loss": 0.0027,
      "step": 100
    },
    {
      "epoch": 0.005866666666666667,
      "grad_norm": 0.5267878174781799,
      "learning_rate": 4.985333333333333e-05,
      "loss": 0.0034,
      "step": 110
    },
    {
      "epoch": 0.0064,
      "grad_norm": 0.7750455737113953,
      "learning_rate": 4.9840000000000004e-05,
      "loss": 0.0034,
      "step": 120
    },
    {
      "epoch": 0.006933333333333333,
      "grad_norm": 0.16783475875854492,
      "learning_rate": 4.982666666666667e-05,
      "loss": 0.0035,
      "step": 130
    },
    {
      "epoch": 0.007466666666666667,
      "grad_norm": 0.5967932939529419,
      "learning_rate": 4.981333333333333e-05,
      "loss": 0.0027,
      "step": 140
    },
    {
      "epoch": 0.008,
      "grad_norm": 0.5098660588264465,
      "learning_rate": 4.9800000000000004e-05,
      "loss": 0.0033,
      "step": 150
    },
    {
      "epoch": 0.008533333333333334,
      "grad_norm": 0.8241793513298035,
      "learning_rate": 4.978666666666667e-05,
      "loss": 0.0046,
      "step": 160
    },
    {
      "epoch": 0.009066666666666667,
      "grad_norm": 0.11747591197490692,
      "learning_rate": 4.977333333333334e-05,
      "loss": 0.003,
      "step": 170
    },
    {
      "epoch": 0.0096,
      "grad_norm": 0.20918971300125122,
      "learning_rate": 4.976e-05,
      "loss": 0.004,
      "step": 180
    },
    {
      "epoch": 0.010133333333333333,
      "grad_norm": 0.1245092824101448,
      "learning_rate": 4.974666666666667e-05,
      "loss": 0.0025,
      "step": 190
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 0.1701459437608719,
      "learning_rate": 4.973333333333334e-05,
      "loss": 0.0027,
      "step": 200
    },
    {
      "epoch": 0.0112,
      "grad_norm": 0.5278815627098083,
      "learning_rate": 4.972e-05,
      "loss": 0.0023,
      "step": 210
    },
    {
      "epoch": 0.011733333333333333,
      "grad_norm": 0.3402888774871826,
      "learning_rate": 4.970666666666667e-05,
      "loss": 0.0026,
      "step": 220
    },
    {
      "epoch": 0.012266666666666667,
      "grad_norm": 0.16577070951461792,
      "learning_rate": 4.9693333333333334e-05,
      "loss": 0.0043,
      "step": 230
    },
    {
      "epoch": 0.0128,
      "grad_norm": 0.7600204348564148,
      "learning_rate": 4.9680000000000005e-05,
      "loss": 0.0031,
      "step": 240
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 0.6250991225242615,
      "learning_rate": 4.966666666666667e-05,
      "loss": 0.0031,
      "step": 250
    },
    {
      "epoch": 0.013866666666666666,
      "grad_norm": 0.8759021759033203,
      "learning_rate": 4.9653333333333335e-05,
      "loss": 0.0033,
      "step": 260
    },
    {
      "epoch": 0.0144,
      "grad_norm": 0.4478793144226074,
      "learning_rate": 4.9640000000000006e-05,
      "loss": 0.0031,
      "step": 270
    },
    {
      "epoch": 0.014933333333333333,
      "grad_norm": 0.5027718544006348,
      "learning_rate": 4.962666666666667e-05,
      "loss": 0.0028,
      "step": 280
    },
    {
      "epoch": 0.015466666666666667,
      "grad_norm": 0.40363597869873047,
      "learning_rate": 4.9613333333333335e-05,
      "loss": 0.0032,
      "step": 290
    },
    {
      "epoch": 0.016,
      "grad_norm": 0.4016950726509094,
      "learning_rate": 4.96e-05,
      "loss": 0.0026,
      "step": 300
    },
    {
      "epoch": 0.016533333333333334,
      "grad_norm": 0.5756269693374634,
      "learning_rate": 4.958666666666667e-05,
      "loss": 0.0026,
      "step": 310
    },
    {
      "epoch": 0.017066666666666667,
      "grad_norm": 0.20247362554073334,
      "learning_rate": 4.9573333333333335e-05,
      "loss": 0.0029,
      "step": 320
    },
    {
      "epoch": 0.0176,
      "grad_norm": 0.12811970710754395,
      "learning_rate": 4.956e-05,
      "loss": 0.0028,
      "step": 330
    },
    {
      "epoch": 0.018133333333333335,
      "grad_norm": 0.7110955715179443,
      "learning_rate": 4.954666666666667e-05,
      "loss": 0.0038,
      "step": 340
    },
    {
      "epoch": 0.018666666666666668,
      "grad_norm": 0.329321950674057,
      "learning_rate": 4.9533333333333336e-05,
      "loss": 0.0031,
      "step": 350
    },
    {
      "epoch": 0.0192,
      "grad_norm": 0.5826025009155273,
      "learning_rate": 4.952e-05,
      "loss": 0.0043,
      "step": 360
    },
    {
      "epoch": 0.019733333333333332,
      "grad_norm": 0.6107167601585388,
      "learning_rate": 4.9506666666666665e-05,
      "loss": 0.0035,
      "step": 370
    },
    {
      "epoch": 0.020266666666666665,
      "grad_norm": 0.39365407824516296,
      "learning_rate": 4.9493333333333336e-05,
      "loss": 0.0029,
      "step": 380
    },
    {
      "epoch": 0.0208,
      "grad_norm": 0.3224782347679138,
      "learning_rate": 4.948000000000001e-05,
      "loss": 0.0033,
      "step": 390
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 0.44380488991737366,
      "learning_rate": 4.9466666666666665e-05,
      "loss": 0.0028,
      "step": 400
    },
    {
      "epoch": 0.021866666666666666,
      "grad_norm": 0.6779375672340393,
      "learning_rate": 4.9453333333333336e-05,
      "loss": 0.0027,
      "step": 410
    },
    {
      "epoch": 0.0224,
      "grad_norm": 0.3145882785320282,
      "learning_rate": 4.944e-05,
      "loss": 0.003,
      "step": 420
    },
    {
      "epoch": 0.022933333333333333,
      "grad_norm": 0.1617577075958252,
      "learning_rate": 4.942666666666667e-05,
      "loss": 0.0032,
      "step": 430
    },
    {
      "epoch": 0.023466666666666667,
      "grad_norm": 0.872071385383606,
      "learning_rate": 4.941333333333334e-05,
      "loss": 0.0029,
      "step": 440
    },
    {
      "epoch": 0.024,
      "grad_norm": 0.20128431916236877,
      "learning_rate": 4.94e-05,
      "loss": 0.0023,
      "step": 450
    },
    {
      "epoch": 0.024533333333333334,
      "grad_norm": 0.20044489204883575,
      "learning_rate": 4.938666666666667e-05,
      "loss": 0.003,
      "step": 460
    },
    {
      "epoch": 0.025066666666666668,
      "grad_norm": 0.5593298673629761,
      "learning_rate": 4.937333333333334e-05,
      "loss": 0.0027,
      "step": 470
    },
    {
      "epoch": 0.0256,
      "grad_norm": 0.21433302760124207,
      "learning_rate": 4.936e-05,
      "loss": 0.0027,
      "step": 480
    },
    {
      "epoch": 0.026133333333333335,
      "grad_norm": 0.32679283618927,
      "learning_rate": 4.9346666666666666e-05,
      "loss": 0.0025,
      "step": 490
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.11039227992296219,
      "learning_rate": 4.933333333333334e-05,
      "loss": 0.0024,
      "step": 500
    },
    {
      "epoch": 0.0272,
      "grad_norm": 0.12717591226100922,
      "learning_rate": 4.932e-05,
      "loss": 0.004,
      "step": 510
    },
    {
      "epoch": 0.027733333333333332,
      "grad_norm": 0.25886768102645874,
      "learning_rate": 4.930666666666667e-05,
      "loss": 0.0023,
      "step": 520
    },
    {
      "epoch": 0.028266666666666666,
      "grad_norm": 0.4744412302970886,
      "learning_rate": 4.929333333333334e-05,
      "loss": 0.0026,
      "step": 530
    },
    {
      "epoch": 0.0288,
      "grad_norm": 0.32676225900650024,
      "learning_rate": 4.928e-05,
      "loss": 0.0043,
      "step": 540
    },
    {
      "epoch": 0.029333333333333333,
      "grad_norm": 0.27454954385757446,
      "learning_rate": 4.926666666666667e-05,
      "loss": 0.0036,
      "step": 550
    },
    {
      "epoch": 0.029866666666666666,
      "grad_norm": 0.34194713830947876,
      "learning_rate": 4.925333333333333e-05,
      "loss": 0.0023,
      "step": 560
    },
    {
      "epoch": 0.0304,
      "grad_norm": 0.8625809550285339,
      "learning_rate": 4.924e-05,
      "loss": 0.0041,
      "step": 570
    },
    {
      "epoch": 0.030933333333333334,
      "grad_norm": 0.6128092408180237,
      "learning_rate": 4.9226666666666674e-05,
      "loss": 0.003,
      "step": 580
    },
    {
      "epoch": 0.031466666666666664,
      "grad_norm": 0.4288201630115509,
      "learning_rate": 4.921333333333333e-05,
      "loss": 0.003,
      "step": 590
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.2785400450229645,
      "learning_rate": 4.92e-05,
      "loss": 0.003,
      "step": 600
    },
    {
      "epoch": 0.03253333333333333,
      "grad_norm": 0.7481486797332764,
      "learning_rate": 4.918666666666667e-05,
      "loss": 0.0036,
      "step": 610
    },
    {
      "epoch": 0.03306666666666667,
      "grad_norm": 0.3632248044013977,
      "learning_rate": 4.917333333333334e-05,
      "loss": 0.0033,
      "step": 620
    },
    {
      "epoch": 0.0336,
      "grad_norm": 0.2682854235172272,
      "learning_rate": 4.9160000000000004e-05,
      "loss": 0.0034,
      "step": 630
    },
    {
      "epoch": 0.034133333333333335,
      "grad_norm": 0.5313720703125,
      "learning_rate": 4.914666666666667e-05,
      "loss": 0.0034,
      "step": 640
    },
    {
      "epoch": 0.034666666666666665,
      "grad_norm": 0.5627630949020386,
      "learning_rate": 4.913333333333334e-05,
      "loss": 0.0046,
      "step": 650
    },
    {
      "epoch": 0.0352,
      "grad_norm": 0.44339877367019653,
      "learning_rate": 4.9120000000000004e-05,
      "loss": 0.0035,
      "step": 660
    },
    {
      "epoch": 0.03573333333333333,
      "grad_norm": 0.6869767308235168,
      "learning_rate": 4.910666666666667e-05,
      "loss": 0.0026,
      "step": 670
    },
    {
      "epoch": 0.03626666666666667,
      "grad_norm": 0.2480492889881134,
      "learning_rate": 4.909333333333333e-05,
      "loss": 0.0026,
      "step": 680
    },
    {
      "epoch": 0.0368,
      "grad_norm": 0.49138638377189636,
      "learning_rate": 4.9080000000000004e-05,
      "loss": 0.0058,
      "step": 690
    },
    {
      "epoch": 0.037333333333333336,
      "grad_norm": 0.1165957972407341,
      "learning_rate": 4.906666666666667e-05,
      "loss": 0.0041,
      "step": 700
    },
    {
      "epoch": 0.037866666666666667,
      "grad_norm": 0.7316282987594604,
      "learning_rate": 4.9053333333333333e-05,
      "loss": 0.0036,
      "step": 710
    },
    {
      "epoch": 0.0384,
      "grad_norm": 0.49947404861450195,
      "learning_rate": 4.9040000000000005e-05,
      "loss": 0.0031,
      "step": 720
    },
    {
      "epoch": 0.038933333333333334,
      "grad_norm": 0.2564740478992462,
      "learning_rate": 4.902666666666667e-05,
      "loss": 0.0028,
      "step": 730
    },
    {
      "epoch": 0.039466666666666664,
      "grad_norm": 0.15163619816303253,
      "learning_rate": 4.9013333333333334e-05,
      "loss": 0.0034,
      "step": 740
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.22490975260734558,
      "learning_rate": 4.9e-05,
      "loss": 0.0026,
      "step": 750
    },
    {
      "epoch": 0.04053333333333333,
      "grad_norm": 0.6309601068496704,
      "learning_rate": 4.898666666666667e-05,
      "loss": 0.0025,
      "step": 760
    },
    {
      "epoch": 0.04106666666666667,
      "grad_norm": 0.674313485622406,
      "learning_rate": 4.897333333333334e-05,
      "loss": 0.0022,
      "step": 770
    },
    {
      "epoch": 0.0416,
      "grad_norm": 0.2385675609111786,
      "learning_rate": 4.896e-05,
      "loss": 0.0032,
      "step": 780
    },
    {
      "epoch": 0.042133333333333335,
      "grad_norm": 0.25024184584617615,
      "learning_rate": 4.894666666666667e-05,
      "loss": 0.0025,
      "step": 790
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 0.3588910698890686,
      "learning_rate": 4.8933333333333335e-05,
      "loss": 0.0023,
      "step": 800
    },
    {
      "epoch": 0.0432,
      "grad_norm": 0.6970180869102478,
      "learning_rate": 4.8920000000000006e-05,
      "loss": 0.0036,
      "step": 810
    },
    {
      "epoch": 0.04373333333333333,
      "grad_norm": 0.8640341758728027,
      "learning_rate": 4.890666666666667e-05,
      "loss": 0.0046,
      "step": 820
    },
    {
      "epoch": 0.04426666666666667,
      "grad_norm": 0.18829785287380219,
      "learning_rate": 4.8893333333333335e-05,
      "loss": 0.0026,
      "step": 830
    },
    {
      "epoch": 0.0448,
      "grad_norm": 0.20672838389873505,
      "learning_rate": 4.8880000000000006e-05,
      "loss": 0.0031,
      "step": 840
    },
    {
      "epoch": 0.04533333333333334,
      "grad_norm": 0.4458788335323334,
      "learning_rate": 4.886666666666667e-05,
      "loss": 0.0031,
      "step": 850
    },
    {
      "epoch": 0.04586666666666667,
      "grad_norm": 0.1532212346792221,
      "learning_rate": 4.8853333333333335e-05,
      "loss": 0.0027,
      "step": 860
    },
    {
      "epoch": 0.0464,
      "grad_norm": 0.6347904801368713,
      "learning_rate": 4.884e-05,
      "loss": 0.0027,
      "step": 870
    },
    {
      "epoch": 0.046933333333333334,
      "grad_norm": 0.16024625301361084,
      "learning_rate": 4.882666666666667e-05,
      "loss": 0.002,
      "step": 880
    },
    {
      "epoch": 0.047466666666666664,
      "grad_norm": 0.23008763790130615,
      "learning_rate": 4.8813333333333336e-05,
      "loss": 0.0046,
      "step": 890
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.2253226786851883,
      "learning_rate": 4.88e-05,
      "loss": 0.0036,
      "step": 900
    },
    {
      "epoch": 0.04853333333333333,
      "grad_norm": 0.5394231081008911,
      "learning_rate": 4.878666666666667e-05,
      "loss": 0.003,
      "step": 910
    },
    {
      "epoch": 0.04906666666666667,
      "grad_norm": 0.4471856951713562,
      "learning_rate": 4.8773333333333336e-05,
      "loss": 0.0034,
      "step": 920
    },
    {
      "epoch": 0.0496,
      "grad_norm": 0.3664996325969696,
      "learning_rate": 4.876e-05,
      "loss": 0.0027,
      "step": 930
    },
    {
      "epoch": 0.050133333333333335,
      "grad_norm": 0.20338734984397888,
      "learning_rate": 4.8746666666666665e-05,
      "loss": 0.0031,
      "step": 940
    },
    {
      "epoch": 0.050666666666666665,
      "grad_norm": 0.12230583280324936,
      "learning_rate": 4.8733333333333337e-05,
      "loss": 0.0021,
      "step": 950
    },
    {
      "epoch": 0.0512,
      "grad_norm": 0.4946095943450928,
      "learning_rate": 4.872000000000001e-05,
      "loss": 0.0043,
      "step": 960
    },
    {
      "epoch": 0.05173333333333333,
      "grad_norm": 0.3767809569835663,
      "learning_rate": 4.8706666666666666e-05,
      "loss": 0.0033,
      "step": 970
    },
    {
      "epoch": 0.05226666666666667,
      "grad_norm": 0.14080177247524261,
      "learning_rate": 4.869333333333334e-05,
      "loss": 0.0039,
      "step": 980
    },
    {
      "epoch": 0.0528,
      "grad_norm": 0.28514912724494934,
      "learning_rate": 4.868e-05,
      "loss": 0.003,
      "step": 990
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.21990171074867249,
      "learning_rate": 4.866666666666667e-05,
      "loss": 0.0031,
      "step": 1000
    },
    {
      "epoch": 0.05386666666666667,
      "grad_norm": 0.7177760601043701,
      "learning_rate": 4.865333333333334e-05,
      "loss": 0.0031,
      "step": 1010
    },
    {
      "epoch": 0.0544,
      "grad_norm": 0.36917516589164734,
      "learning_rate": 4.864e-05,
      "loss": 0.0041,
      "step": 1020
    },
    {
      "epoch": 0.054933333333333334,
      "grad_norm": 0.2093883752822876,
      "learning_rate": 4.862666666666667e-05,
      "loss": 0.0029,
      "step": 1030
    },
    {
      "epoch": 0.055466666666666664,
      "grad_norm": 0.39416345953941345,
      "learning_rate": 4.861333333333333e-05,
      "loss": 0.0033,
      "step": 1040
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.40653058886528015,
      "learning_rate": 4.86e-05,
      "loss": 0.0022,
      "step": 1050
    },
    {
      "epoch": 0.05653333333333333,
      "grad_norm": 0.2670009732246399,
      "learning_rate": 4.858666666666667e-05,
      "loss": 0.0026,
      "step": 1060
    },
    {
      "epoch": 0.05706666666666667,
      "grad_norm": 0.4875708222389221,
      "learning_rate": 4.857333333333334e-05,
      "loss": 0.0021,
      "step": 1070
    },
    {
      "epoch": 0.0576,
      "grad_norm": 0.2673298120498657,
      "learning_rate": 4.856e-05,
      "loss": 0.0031,
      "step": 1080
    },
    {
      "epoch": 0.058133333333333335,
      "grad_norm": 0.16945655643939972,
      "learning_rate": 4.854666666666667e-05,
      "loss": 0.0029,
      "step": 1090
    },
    {
      "epoch": 0.058666666666666666,
      "grad_norm": 0.30835530161857605,
      "learning_rate": 4.853333333333334e-05,
      "loss": 0.0036,
      "step": 1100
    },
    {
      "epoch": 0.0592,
      "grad_norm": 0.1825968474149704,
      "learning_rate": 4.852e-05,
      "loss": 0.0032,
      "step": 1110
    },
    {
      "epoch": 0.05973333333333333,
      "grad_norm": 0.3410610258579254,
      "learning_rate": 4.850666666666667e-05,
      "loss": 0.0025,
      "step": 1120
    },
    {
      "epoch": 0.06026666666666667,
      "grad_norm": 0.19906353950500488,
      "learning_rate": 4.849333333333333e-05,
      "loss": 0.0025,
      "step": 1130
    },
    {
      "epoch": 0.0608,
      "grad_norm": 0.5654935240745544,
      "learning_rate": 4.8480000000000003e-05,
      "loss": 0.0021,
      "step": 1140
    },
    {
      "epoch": 0.06133333333333333,
      "grad_norm": 0.9276710748672485,
      "learning_rate": 4.8466666666666675e-05,
      "loss": 0.0022,
      "step": 1150
    },
    {
      "epoch": 0.06186666666666667,
      "grad_norm": 0.24468916654586792,
      "learning_rate": 4.845333333333333e-05,
      "loss": 0.0026,
      "step": 1160
    },
    {
      "epoch": 0.0624,
      "grad_norm": 0.18734495341777802,
      "learning_rate": 4.8440000000000004e-05,
      "loss": 0.0028,
      "step": 1170
    },
    {
      "epoch": 0.06293333333333333,
      "grad_norm": 0.35375261306762695,
      "learning_rate": 4.842666666666667e-05,
      "loss": 0.0024,
      "step": 1180
    },
    {
      "epoch": 0.06346666666666667,
      "grad_norm": 0.6006233096122742,
      "learning_rate": 4.841333333333334e-05,
      "loss": 0.0027,
      "step": 1190
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.40528422594070435,
      "learning_rate": 4.8400000000000004e-05,
      "loss": 0.0039,
      "step": 1200
    },
    {
      "epoch": 0.06453333333333333,
      "grad_norm": 0.41478365659713745,
      "learning_rate": 4.838666666666667e-05,
      "loss": 0.0023,
      "step": 1210
    },
    {
      "epoch": 0.06506666666666666,
      "grad_norm": 0.8658657670021057,
      "learning_rate": 4.837333333333334e-05,
      "loss": 0.0037,
      "step": 1220
    },
    {
      "epoch": 0.0656,
      "grad_norm": 0.17362907528877258,
      "learning_rate": 4.836e-05,
      "loss": 0.0039,
      "step": 1230
    },
    {
      "epoch": 0.06613333333333334,
      "grad_norm": 0.6508603096008301,
      "learning_rate": 4.834666666666667e-05,
      "loss": 0.003,
      "step": 1240
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.29149049520492554,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 0.0031,
      "step": 1250
    },
    {
      "epoch": 0.0672,
      "grad_norm": 0.15798184275627136,
      "learning_rate": 4.8320000000000005e-05,
      "loss": 0.003,
      "step": 1260
    },
    {
      "epoch": 0.06773333333333334,
      "grad_norm": 0.12670905888080597,
      "learning_rate": 4.830666666666667e-05,
      "loss": 0.0046,
      "step": 1270
    },
    {
      "epoch": 0.06826666666666667,
      "grad_norm": 0.47331178188323975,
      "learning_rate": 4.8293333333333334e-05,
      "loss": 0.0042,
      "step": 1280
    },
    {
      "epoch": 0.0688,
      "grad_norm": 0.35660699009895325,
      "learning_rate": 4.8280000000000005e-05,
      "loss": 0.0037,
      "step": 1290
    },
    {
      "epoch": 0.06933333333333333,
      "grad_norm": 0.33218997716903687,
      "learning_rate": 4.826666666666667e-05,
      "loss": 0.002,
      "step": 1300
    },
    {
      "epoch": 0.06986666666666666,
      "grad_norm": 0.5227598547935486,
      "learning_rate": 4.8253333333333334e-05,
      "loss": 0.0029,
      "step": 1310
    },
    {
      "epoch": 0.0704,
      "grad_norm": 0.12401149421930313,
      "learning_rate": 4.824e-05,
      "loss": 0.0035,
      "step": 1320
    },
    {
      "epoch": 0.07093333333333333,
      "grad_norm": 0.46093466877937317,
      "learning_rate": 4.822666666666667e-05,
      "loss": 0.0027,
      "step": 1330
    },
    {
      "epoch": 0.07146666666666666,
      "grad_norm": 0.6515517830848694,
      "learning_rate": 4.8213333333333335e-05,
      "loss": 0.0031,
      "step": 1340
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.5068068504333496,
      "learning_rate": 4.82e-05,
      "loss": 0.0021,
      "step": 1350
    },
    {
      "epoch": 0.07253333333333334,
      "grad_norm": 0.3233390152454376,
      "learning_rate": 4.818666666666667e-05,
      "loss": 0.003,
      "step": 1360
    },
    {
      "epoch": 0.07306666666666667,
      "grad_norm": 0.14934781193733215,
      "learning_rate": 4.8173333333333335e-05,
      "loss": 0.0029,
      "step": 1370
    },
    {
      "epoch": 0.0736,
      "grad_norm": 0.6411246061325073,
      "learning_rate": 4.816e-05,
      "loss": 0.0021,
      "step": 1380
    },
    {
      "epoch": 0.07413333333333333,
      "grad_norm": 0.1143428310751915,
      "learning_rate": 4.814666666666667e-05,
      "loss": 0.0022,
      "step": 1390
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 0.40579938888549805,
      "learning_rate": 4.8133333333333336e-05,
      "loss": 0.0041,
      "step": 1400
    },
    {
      "epoch": 0.0752,
      "grad_norm": 0.23179730772972107,
      "learning_rate": 4.812000000000001e-05,
      "loss": 0.0025,
      "step": 1410
    },
    {
      "epoch": 0.07573333333333333,
      "grad_norm": 0.5925196409225464,
      "learning_rate": 4.8106666666666665e-05,
      "loss": 0.003,
      "step": 1420
    },
    {
      "epoch": 0.07626666666666666,
      "grad_norm": 0.25222188234329224,
      "learning_rate": 4.8093333333333336e-05,
      "loss": 0.0037,
      "step": 1430
    },
    {
      "epoch": 0.0768,
      "grad_norm": 0.4061864912509918,
      "learning_rate": 4.808e-05,
      "loss": 0.0031,
      "step": 1440
    },
    {
      "epoch": 0.07733333333333334,
      "grad_norm": 0.16048450767993927,
      "learning_rate": 4.806666666666667e-05,
      "loss": 0.0027,
      "step": 1450
    },
    {
      "epoch": 0.07786666666666667,
      "grad_norm": 0.35902076959609985,
      "learning_rate": 4.8053333333333336e-05,
      "loss": 0.0026,
      "step": 1460
    },
    {
      "epoch": 0.0784,
      "grad_norm": 0.32324859499931335,
      "learning_rate": 4.804e-05,
      "loss": 0.0028,
      "step": 1470
    },
    {
      "epoch": 0.07893333333333333,
      "grad_norm": 0.13375329971313477,
      "learning_rate": 4.802666666666667e-05,
      "loss": 0.0037,
      "step": 1480
    },
    {
      "epoch": 0.07946666666666667,
      "grad_norm": 0.16824203729629517,
      "learning_rate": 4.801333333333334e-05,
      "loss": 0.0032,
      "step": 1490
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.2815898656845093,
      "learning_rate": 4.8e-05,
      "loss": 0.0037,
      "step": 1500
    },
    {
      "epoch": 0.08053333333333333,
      "grad_norm": 0.34779149293899536,
      "learning_rate": 4.7986666666666666e-05,
      "loss": 0.0026,
      "step": 1510
    },
    {
      "epoch": 0.08106666666666666,
      "grad_norm": 0.5440766215324402,
      "learning_rate": 4.797333333333334e-05,
      "loss": 0.0028,
      "step": 1520
    },
    {
      "epoch": 0.0816,
      "grad_norm": 0.4959019124507904,
      "learning_rate": 4.796e-05,
      "loss": 0.0031,
      "step": 1530
    },
    {
      "epoch": 0.08213333333333334,
      "grad_norm": 0.24848397076129913,
      "learning_rate": 4.7946666666666666e-05,
      "loss": 0.0027,
      "step": 1540
    },
    {
      "epoch": 0.08266666666666667,
      "grad_norm": 0.23700210452079773,
      "learning_rate": 4.793333333333334e-05,
      "loss": 0.0029,
      "step": 1550
    },
    {
      "epoch": 0.0832,
      "grad_norm": 0.4890199303627014,
      "learning_rate": 4.792e-05,
      "loss": 0.0035,
      "step": 1560
    },
    {
      "epoch": 0.08373333333333334,
      "grad_norm": 1.1271319389343262,
      "learning_rate": 4.7906666666666667e-05,
      "loss": 0.0038,
      "step": 1570
    },
    {
      "epoch": 0.08426666666666667,
      "grad_norm": 1.0423388481140137,
      "learning_rate": 4.789333333333334e-05,
      "loss": 0.0026,
      "step": 1580
    },
    {
      "epoch": 0.0848,
      "grad_norm": 0.2737019658088684,
      "learning_rate": 4.788e-05,
      "loss": 0.0031,
      "step": 1590
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 0.4362019896507263,
      "learning_rate": 4.7866666666666674e-05,
      "loss": 0.0026,
      "step": 1600
    },
    {
      "epoch": 0.08586666666666666,
      "grad_norm": 0.22444602847099304,
      "learning_rate": 4.785333333333333e-05,
      "loss": 0.0021,
      "step": 1610
    },
    {
      "epoch": 0.0864,
      "grad_norm": 0.3446061909198761,
      "learning_rate": 4.784e-05,
      "loss": 0.0031,
      "step": 1620
    },
    {
      "epoch": 0.08693333333333333,
      "grad_norm": 0.31353744864463806,
      "learning_rate": 4.782666666666667e-05,
      "loss": 0.0031,
      "step": 1630
    },
    {
      "epoch": 0.08746666666666666,
      "grad_norm": 0.0946463793516159,
      "learning_rate": 4.781333333333334e-05,
      "loss": 0.0021,
      "step": 1640
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.4146580398082733,
      "learning_rate": 4.78e-05,
      "loss": 0.0032,
      "step": 1650
    },
    {
      "epoch": 0.08853333333333334,
      "grad_norm": 0.13805197179317474,
      "learning_rate": 4.778666666666667e-05,
      "loss": 0.0024,
      "step": 1660
    },
    {
      "epoch": 0.08906666666666667,
      "grad_norm": 0.9977276921272278,
      "learning_rate": 4.777333333333334e-05,
      "loss": 0.0031,
      "step": 1670
    },
    {
      "epoch": 0.0896,
      "grad_norm": 0.1906127780675888,
      "learning_rate": 4.7760000000000004e-05,
      "loss": 0.0026,
      "step": 1680
    },
    {
      "epoch": 0.09013333333333333,
      "grad_norm": 0.12623311579227448,
      "learning_rate": 4.774666666666667e-05,
      "loss": 0.0027,
      "step": 1690
    },
    {
      "epoch": 0.09066666666666667,
      "grad_norm": 0.2424117773771286,
      "learning_rate": 4.773333333333333e-05,
      "loss": 0.0029,
      "step": 1700
    },
    {
      "epoch": 0.0912,
      "grad_norm": 0.7238699197769165,
      "learning_rate": 4.7720000000000004e-05,
      "loss": 0.0044,
      "step": 1710
    },
    {
      "epoch": 0.09173333333333333,
      "grad_norm": 0.6482234001159668,
      "learning_rate": 4.770666666666667e-05,
      "loss": 0.0028,
      "step": 1720
    },
    {
      "epoch": 0.09226666666666666,
      "grad_norm": 0.28918835520744324,
      "learning_rate": 4.769333333333333e-05,
      "loss": 0.0021,
      "step": 1730
    },
    {
      "epoch": 0.0928,
      "grad_norm": 0.4063946306705475,
      "learning_rate": 4.7680000000000004e-05,
      "loss": 0.0026,
      "step": 1740
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.20211069285869598,
      "learning_rate": 4.766666666666667e-05,
      "loss": 0.0032,
      "step": 1750
    },
    {
      "epoch": 0.09386666666666667,
      "grad_norm": 0.11445748805999756,
      "learning_rate": 4.765333333333333e-05,
      "loss": 0.0032,
      "step": 1760
    },
    {
      "epoch": 0.0944,
      "grad_norm": 0.1726835072040558,
      "learning_rate": 4.7640000000000005e-05,
      "loss": 0.0028,
      "step": 1770
    },
    {
      "epoch": 0.09493333333333333,
      "grad_norm": 0.09696818888187408,
      "learning_rate": 4.762666666666667e-05,
      "loss": 0.0024,
      "step": 1780
    },
    {
      "epoch": 0.09546666666666667,
      "grad_norm": 0.08952897042036057,
      "learning_rate": 4.761333333333334e-05,
      "loss": 0.0029,
      "step": 1790
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.43702155351638794,
      "learning_rate": 4.76e-05,
      "loss": 0.0032,
      "step": 1800
    },
    {
      "epoch": 0.09653333333333333,
      "grad_norm": 0.30550047755241394,
      "learning_rate": 4.758666666666667e-05,
      "loss": 0.0024,
      "step": 1810
    },
    {
      "epoch": 0.09706666666666666,
      "grad_norm": 0.3077337145805359,
      "learning_rate": 4.7573333333333334e-05,
      "loss": 0.0024,
      "step": 1820
    },
    {
      "epoch": 0.0976,
      "grad_norm": 0.3025337755680084,
      "learning_rate": 4.7560000000000005e-05,
      "loss": 0.0032,
      "step": 1830
    },
    {
      "epoch": 0.09813333333333334,
      "grad_norm": 0.6804813742637634,
      "learning_rate": 4.754666666666667e-05,
      "loss": 0.0025,
      "step": 1840
    },
    {
      "epoch": 0.09866666666666667,
      "grad_norm": 0.11143399775028229,
      "learning_rate": 4.7533333333333334e-05,
      "loss": 0.002,
      "step": 1850
    },
    {
      "epoch": 0.0992,
      "grad_norm": 0.31815826892852783,
      "learning_rate": 4.7520000000000006e-05,
      "loss": 0.0022,
      "step": 1860
    },
    {
      "epoch": 0.09973333333333333,
      "grad_norm": 0.2528816759586334,
      "learning_rate": 4.750666666666667e-05,
      "loss": 0.0031,
      "step": 1870
    },
    {
      "epoch": 0.10026666666666667,
      "grad_norm": 0.5066144466400146,
      "learning_rate": 4.7493333333333335e-05,
      "loss": 0.0023,
      "step": 1880
    },
    {
      "epoch": 0.1008,
      "grad_norm": 0.10812564939260483,
      "learning_rate": 4.748e-05,
      "loss": 0.0022,
      "step": 1890
    },
    {
      "epoch": 0.10133333333333333,
      "grad_norm": 0.531188428401947,
      "learning_rate": 4.746666666666667e-05,
      "loss": 0.003,
      "step": 1900
    },
    {
      "epoch": 0.10186666666666666,
      "grad_norm": 0.10557719320058823,
      "learning_rate": 4.7453333333333335e-05,
      "loss": 0.003,
      "step": 1910
    },
    {
      "epoch": 0.1024,
      "grad_norm": 0.43504080176353455,
      "learning_rate": 4.744e-05,
      "loss": 0.0041,
      "step": 1920
    },
    {
      "epoch": 0.10293333333333334,
      "grad_norm": 0.33201974630355835,
      "learning_rate": 4.742666666666667e-05,
      "loss": 0.0033,
      "step": 1930
    },
    {
      "epoch": 0.10346666666666667,
      "grad_norm": 0.6772449612617493,
      "learning_rate": 4.7413333333333336e-05,
      "loss": 0.0028,
      "step": 1940
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.3476818799972534,
      "learning_rate": 4.74e-05,
      "loss": 0.0022,
      "step": 1950
    },
    {
      "epoch": 0.10453333333333334,
      "grad_norm": 0.4066120386123657,
      "learning_rate": 4.7386666666666665e-05,
      "loss": 0.002,
      "step": 1960
    },
    {
      "epoch": 0.10506666666666667,
      "grad_norm": 0.16357329487800598,
      "learning_rate": 4.7373333333333336e-05,
      "loss": 0.0025,
      "step": 1970
    },
    {
      "epoch": 0.1056,
      "grad_norm": 0.24747329950332642,
      "learning_rate": 4.736000000000001e-05,
      "loss": 0.002,
      "step": 1980
    },
    {
      "epoch": 0.10613333333333333,
      "grad_norm": 0.8479353785514832,
      "learning_rate": 4.7346666666666665e-05,
      "loss": 0.0034,
      "step": 1990
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.7662545442581177,
      "learning_rate": 4.7333333333333336e-05,
      "loss": 0.0035,
      "step": 2000
    },
    {
      "epoch": 0.1072,
      "grad_norm": 0.4431486427783966,
      "learning_rate": 4.732e-05,
      "loss": 0.0032,
      "step": 2010
    },
    {
      "epoch": 0.10773333333333333,
      "grad_norm": 0.17606861889362335,
      "learning_rate": 4.730666666666667e-05,
      "loss": 0.0031,
      "step": 2020
    },
    {
      "epoch": 0.10826666666666666,
      "grad_norm": 0.5527159571647644,
      "learning_rate": 4.729333333333334e-05,
      "loss": 0.0029,
      "step": 2030
    },
    {
      "epoch": 0.1088,
      "grad_norm": 0.5662087798118591,
      "learning_rate": 4.728e-05,
      "loss": 0.0039,
      "step": 2040
    },
    {
      "epoch": 0.10933333333333334,
      "grad_norm": 0.6045582890510559,
      "learning_rate": 4.726666666666667e-05,
      "loss": 0.0033,
      "step": 2050
    },
    {
      "epoch": 0.10986666666666667,
      "grad_norm": 0.5537855625152588,
      "learning_rate": 4.725333333333334e-05,
      "loss": 0.0027,
      "step": 2060
    },
    {
      "epoch": 0.1104,
      "grad_norm": 0.5577764511108398,
      "learning_rate": 4.724e-05,
      "loss": 0.0031,
      "step": 2070
    },
    {
      "epoch": 0.11093333333333333,
      "grad_norm": 0.3598276376724243,
      "learning_rate": 4.7226666666666666e-05,
      "loss": 0.0021,
      "step": 2080
    },
    {
      "epoch": 0.11146666666666667,
      "grad_norm": 0.4938769042491913,
      "learning_rate": 4.721333333333334e-05,
      "loss": 0.0021,
      "step": 2090
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.23636874556541443,
      "learning_rate": 4.72e-05,
      "loss": 0.0032,
      "step": 2100
    },
    {
      "epoch": 0.11253333333333333,
      "grad_norm": 0.2690882384777069,
      "learning_rate": 4.718666666666667e-05,
      "loss": 0.0028,
      "step": 2110
    },
    {
      "epoch": 0.11306666666666666,
      "grad_norm": 0.532355546951294,
      "learning_rate": 4.717333333333334e-05,
      "loss": 0.0032,
      "step": 2120
    },
    {
      "epoch": 0.1136,
      "grad_norm": 0.6319028735160828,
      "learning_rate": 4.716e-05,
      "loss": 0.0032,
      "step": 2130
    },
    {
      "epoch": 0.11413333333333334,
      "grad_norm": 0.40493887662887573,
      "learning_rate": 4.714666666666667e-05,
      "loss": 0.0032,
      "step": 2140
    },
    {
      "epoch": 0.11466666666666667,
      "grad_norm": 0.3087421953678131,
      "learning_rate": 4.713333333333333e-05,
      "loss": 0.0027,
      "step": 2150
    },
    {
      "epoch": 0.1152,
      "grad_norm": 0.43631795048713684,
      "learning_rate": 4.712e-05,
      "loss": 0.0028,
      "step": 2160
    },
    {
      "epoch": 0.11573333333333333,
      "grad_norm": 0.4686944782733917,
      "learning_rate": 4.7106666666666674e-05,
      "loss": 0.0034,
      "step": 2170
    },
    {
      "epoch": 0.11626666666666667,
      "grad_norm": 0.5269723534584045,
      "learning_rate": 4.709333333333333e-05,
      "loss": 0.0038,
      "step": 2180
    },
    {
      "epoch": 0.1168,
      "grad_norm": 0.2053011655807495,
      "learning_rate": 4.708e-05,
      "loss": 0.0028,
      "step": 2190
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 0.346012681722641,
      "learning_rate": 4.706666666666667e-05,
      "loss": 0.0027,
      "step": 2200
    },
    {
      "epoch": 0.11786666666666666,
      "grad_norm": 0.20237144827842712,
      "learning_rate": 4.705333333333334e-05,
      "loss": 0.0028,
      "step": 2210
    },
    {
      "epoch": 0.1184,
      "grad_norm": 0.23484449088573456,
      "learning_rate": 4.7040000000000004e-05,
      "loss": 0.0028,
      "step": 2220
    },
    {
      "epoch": 0.11893333333333334,
      "grad_norm": 0.7247641086578369,
      "learning_rate": 4.702666666666667e-05,
      "loss": 0.0038,
      "step": 2230
    },
    {
      "epoch": 0.11946666666666667,
      "grad_norm": 0.2818537950515747,
      "learning_rate": 4.701333333333334e-05,
      "loss": 0.0026,
      "step": 2240
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.39571669697761536,
      "learning_rate": 4.7e-05,
      "loss": 0.002,
      "step": 2250
    },
    {
      "epoch": 0.12053333333333334,
      "grad_norm": 0.4249441623687744,
      "learning_rate": 4.698666666666667e-05,
      "loss": 0.0021,
      "step": 2260
    },
    {
      "epoch": 0.12106666666666667,
      "grad_norm": 0.5595032572746277,
      "learning_rate": 4.697333333333333e-05,
      "loss": 0.0031,
      "step": 2270
    },
    {
      "epoch": 0.1216,
      "grad_norm": 0.10491117089986801,
      "learning_rate": 4.6960000000000004e-05,
      "loss": 0.0021,
      "step": 2280
    },
    {
      "epoch": 0.12213333333333333,
      "grad_norm": 0.6540265083312988,
      "learning_rate": 4.694666666666667e-05,
      "loss": 0.004,
      "step": 2290
    },
    {
      "epoch": 0.12266666666666666,
      "grad_norm": 0.47898659110069275,
      "learning_rate": 4.6933333333333333e-05,
      "loss": 0.0026,
      "step": 2300
    },
    {
      "epoch": 0.1232,
      "grad_norm": 0.21494483947753906,
      "learning_rate": 4.6920000000000005e-05,
      "loss": 0.0026,
      "step": 2310
    },
    {
      "epoch": 0.12373333333333333,
      "grad_norm": 0.29741957783699036,
      "learning_rate": 4.690666666666667e-05,
      "loss": 0.0036,
      "step": 2320
    },
    {
      "epoch": 0.12426666666666666,
      "grad_norm": 0.30045580863952637,
      "learning_rate": 4.6893333333333334e-05,
      "loss": 0.0027,
      "step": 2330
    },
    {
      "epoch": 0.1248,
      "grad_norm": 0.26373401284217834,
      "learning_rate": 4.688e-05,
      "loss": 0.0033,
      "step": 2340
    },
    {
      "epoch": 0.12533333333333332,
      "grad_norm": 0.35770198702812195,
      "learning_rate": 4.686666666666667e-05,
      "loss": 0.0024,
      "step": 2350
    },
    {
      "epoch": 0.12586666666666665,
      "grad_norm": 0.42220714688301086,
      "learning_rate": 4.685333333333334e-05,
      "loss": 0.0025,
      "step": 2360
    },
    {
      "epoch": 0.1264,
      "grad_norm": 0.3891259729862213,
      "learning_rate": 4.684e-05,
      "loss": 0.0038,
      "step": 2370
    },
    {
      "epoch": 0.12693333333333334,
      "grad_norm": 0.15742579102516174,
      "learning_rate": 4.682666666666667e-05,
      "loss": 0.0037,
      "step": 2380
    },
    {
      "epoch": 0.12746666666666667,
      "grad_norm": 0.371584951877594,
      "learning_rate": 4.6813333333333335e-05,
      "loss": 0.0023,
      "step": 2390
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.1584940105676651,
      "learning_rate": 4.6800000000000006e-05,
      "loss": 0.0026,
      "step": 2400
    },
    {
      "epoch": 0.12853333333333333,
      "grad_norm": 0.5985338687896729,
      "learning_rate": 4.678666666666667e-05,
      "loss": 0.0025,
      "step": 2410
    },
    {
      "epoch": 0.12906666666666666,
      "grad_norm": 0.4390926659107208,
      "learning_rate": 4.6773333333333335e-05,
      "loss": 0.0024,
      "step": 2420
    },
    {
      "epoch": 0.1296,
      "grad_norm": 0.3362313508987427,
      "learning_rate": 4.6760000000000006e-05,
      "loss": 0.0032,
      "step": 2430
    },
    {
      "epoch": 0.13013333333333332,
      "grad_norm": 0.3520546853542328,
      "learning_rate": 4.6746666666666664e-05,
      "loss": 0.0022,
      "step": 2440
    },
    {
      "epoch": 0.13066666666666665,
      "grad_norm": 0.4589189291000366,
      "learning_rate": 4.6733333333333335e-05,
      "loss": 0.0028,
      "step": 2450
    },
    {
      "epoch": 0.1312,
      "grad_norm": 0.7596955895423889,
      "learning_rate": 4.672e-05,
      "loss": 0.0032,
      "step": 2460
    },
    {
      "epoch": 0.13173333333333334,
      "grad_norm": 0.251949280500412,
      "learning_rate": 4.670666666666667e-05,
      "loss": 0.0033,
      "step": 2470
    },
    {
      "epoch": 0.13226666666666667,
      "grad_norm": 0.1823362410068512,
      "learning_rate": 4.6693333333333336e-05,
      "loss": 0.0024,
      "step": 2480
    },
    {
      "epoch": 0.1328,
      "grad_norm": 0.3016801178455353,
      "learning_rate": 4.668e-05,
      "loss": 0.0028,
      "step": 2490
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.5506646633148193,
      "learning_rate": 4.666666666666667e-05,
      "loss": 0.003,
      "step": 2500
    },
    {
      "epoch": 0.13386666666666666,
      "grad_norm": 0.13887512683868408,
      "learning_rate": 4.6653333333333336e-05,
      "loss": 0.0035,
      "step": 2510
    },
    {
      "epoch": 0.1344,
      "grad_norm": 0.16227735579013824,
      "learning_rate": 4.664e-05,
      "loss": 0.0035,
      "step": 2520
    },
    {
      "epoch": 0.13493333333333332,
      "grad_norm": 0.2466437965631485,
      "learning_rate": 4.6626666666666665e-05,
      "loss": 0.004,
      "step": 2530
    },
    {
      "epoch": 0.13546666666666668,
      "grad_norm": 0.45333877205848694,
      "learning_rate": 4.6613333333333337e-05,
      "loss": 0.0024,
      "step": 2540
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.5066860914230347,
      "learning_rate": 4.660000000000001e-05,
      "loss": 0.0029,
      "step": 2550
    },
    {
      "epoch": 0.13653333333333334,
      "grad_norm": 0.44788774847984314,
      "learning_rate": 4.6586666666666666e-05,
      "loss": 0.0024,
      "step": 2560
    },
    {
      "epoch": 0.13706666666666667,
      "grad_norm": 0.2739434540271759,
      "learning_rate": 4.657333333333334e-05,
      "loss": 0.0035,
      "step": 2570
    },
    {
      "epoch": 0.1376,
      "grad_norm": 0.19060005247592926,
      "learning_rate": 4.656e-05,
      "loss": 0.0024,
      "step": 2580
    },
    {
      "epoch": 0.13813333333333333,
      "grad_norm": 0.2016344964504242,
      "learning_rate": 4.6546666666666666e-05,
      "loss": 0.0025,
      "step": 2590
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 0.4303017258644104,
      "learning_rate": 4.653333333333334e-05,
      "loss": 0.0022,
      "step": 2600
    },
    {
      "epoch": 0.1392,
      "grad_norm": 0.34557467699050903,
      "learning_rate": 4.652e-05,
      "loss": 0.0027,
      "step": 2610
    },
    {
      "epoch": 0.13973333333333332,
      "grad_norm": 0.6381624937057495,
      "learning_rate": 4.650666666666667e-05,
      "loss": 0.0023,
      "step": 2620
    },
    {
      "epoch": 0.14026666666666668,
      "grad_norm": 0.5460704565048218,
      "learning_rate": 4.649333333333333e-05,
      "loss": 0.0033,
      "step": 2630
    },
    {
      "epoch": 0.1408,
      "grad_norm": 0.6875086426734924,
      "learning_rate": 4.648e-05,
      "loss": 0.0027,
      "step": 2640
    },
    {
      "epoch": 0.14133333333333334,
      "grad_norm": 0.14929485321044922,
      "learning_rate": 4.646666666666667e-05,
      "loss": 0.0035,
      "step": 2650
    },
    {
      "epoch": 0.14186666666666667,
      "grad_norm": 0.4995640814304352,
      "learning_rate": 4.645333333333334e-05,
      "loss": 0.0041,
      "step": 2660
    },
    {
      "epoch": 0.1424,
      "grad_norm": 0.5380394458770752,
      "learning_rate": 4.644e-05,
      "loss": 0.0033,
      "step": 2670
    },
    {
      "epoch": 0.14293333333333333,
      "grad_norm": 0.5898723602294922,
      "learning_rate": 4.642666666666667e-05,
      "loss": 0.0023,
      "step": 2680
    },
    {
      "epoch": 0.14346666666666666,
      "grad_norm": 0.29844799637794495,
      "learning_rate": 4.641333333333334e-05,
      "loss": 0.0046,
      "step": 2690
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.6805363297462463,
      "learning_rate": 4.64e-05,
      "loss": 0.0024,
      "step": 2700
    },
    {
      "epoch": 0.14453333333333335,
      "grad_norm": 0.4496375620365143,
      "learning_rate": 4.638666666666667e-05,
      "loss": 0.0027,
      "step": 2710
    },
    {
      "epoch": 0.14506666666666668,
      "grad_norm": 0.2375802993774414,
      "learning_rate": 4.637333333333333e-05,
      "loss": 0.0022,
      "step": 2720
    },
    {
      "epoch": 0.1456,
      "grad_norm": 0.44561338424682617,
      "learning_rate": 4.636e-05,
      "loss": 0.0043,
      "step": 2730
    },
    {
      "epoch": 0.14613333333333334,
      "grad_norm": 0.17977041006088257,
      "learning_rate": 4.6346666666666675e-05,
      "loss": 0.0032,
      "step": 2740
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.46999117732048035,
      "learning_rate": 4.633333333333333e-05,
      "loss": 0.0027,
      "step": 2750
    },
    {
      "epoch": 0.1472,
      "grad_norm": 0.4435325562953949,
      "learning_rate": 4.6320000000000004e-05,
      "loss": 0.0029,
      "step": 2760
    },
    {
      "epoch": 0.14773333333333333,
      "grad_norm": 0.44063130021095276,
      "learning_rate": 4.630666666666667e-05,
      "loss": 0.004,
      "step": 2770
    },
    {
      "epoch": 0.14826666666666666,
      "grad_norm": 0.247315913438797,
      "learning_rate": 4.629333333333333e-05,
      "loss": 0.0036,
      "step": 2780
    },
    {
      "epoch": 0.1488,
      "grad_norm": 0.4997331500053406,
      "learning_rate": 4.6280000000000004e-05,
      "loss": 0.0026,
      "step": 2790
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 0.6423954367637634,
      "learning_rate": 4.626666666666667e-05,
      "loss": 0.0031,
      "step": 2800
    },
    {
      "epoch": 0.14986666666666668,
      "grad_norm": 0.13713473081588745,
      "learning_rate": 4.625333333333334e-05,
      "loss": 0.0043,
      "step": 2810
    },
    {
      "epoch": 0.1504,
      "grad_norm": 0.2859831154346466,
      "learning_rate": 4.624e-05,
      "loss": 0.0026,
      "step": 2820
    },
    {
      "epoch": 0.15093333333333334,
      "grad_norm": 0.4503333568572998,
      "learning_rate": 4.622666666666667e-05,
      "loss": 0.003,
      "step": 2830
    },
    {
      "epoch": 0.15146666666666667,
      "grad_norm": 0.33346351981163025,
      "learning_rate": 4.6213333333333334e-05,
      "loss": 0.0029,
      "step": 2840
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.2184377759695053,
      "learning_rate": 4.6200000000000005e-05,
      "loss": 0.0033,
      "step": 2850
    },
    {
      "epoch": 0.15253333333333333,
      "grad_norm": 0.3588951826095581,
      "learning_rate": 4.618666666666667e-05,
      "loss": 0.0053,
      "step": 2860
    },
    {
      "epoch": 0.15306666666666666,
      "grad_norm": 0.14965631067752838,
      "learning_rate": 4.6173333333333334e-05,
      "loss": 0.0028,
      "step": 2870
    },
    {
      "epoch": 0.1536,
      "grad_norm": 0.2773520052433014,
      "learning_rate": 4.6160000000000005e-05,
      "loss": 0.0031,
      "step": 2880
    },
    {
      "epoch": 0.15413333333333334,
      "grad_norm": 0.4027148485183716,
      "learning_rate": 4.614666666666667e-05,
      "loss": 0.0035,
      "step": 2890
    },
    {
      "epoch": 0.15466666666666667,
      "grad_norm": 0.3835445046424866,
      "learning_rate": 4.6133333333333334e-05,
      "loss": 0.0033,
      "step": 2900
    },
    {
      "epoch": 0.1552,
      "grad_norm": 0.32258695363998413,
      "learning_rate": 4.612e-05,
      "loss": 0.003,
      "step": 2910
    },
    {
      "epoch": 0.15573333333333333,
      "grad_norm": 0.5419690012931824,
      "learning_rate": 4.610666666666667e-05,
      "loss": 0.0041,
      "step": 2920
    },
    {
      "epoch": 0.15626666666666666,
      "grad_norm": 0.1362726092338562,
      "learning_rate": 4.6093333333333335e-05,
      "loss": 0.0022,
      "step": 2930
    },
    {
      "epoch": 0.1568,
      "grad_norm": 0.7629122138023376,
      "learning_rate": 4.608e-05,
      "loss": 0.0024,
      "step": 2940
    },
    {
      "epoch": 0.15733333333333333,
      "grad_norm": 0.3202524185180664,
      "learning_rate": 4.606666666666667e-05,
      "loss": 0.0036,
      "step": 2950
    },
    {
      "epoch": 0.15786666666666666,
      "grad_norm": 0.47347354888916016,
      "learning_rate": 4.6053333333333335e-05,
      "loss": 0.0026,
      "step": 2960
    },
    {
      "epoch": 0.1584,
      "grad_norm": 0.3129037618637085,
      "learning_rate": 4.604e-05,
      "loss": 0.0031,
      "step": 2970
    },
    {
      "epoch": 0.15893333333333334,
      "grad_norm": 0.3630346357822418,
      "learning_rate": 4.602666666666667e-05,
      "loss": 0.0029,
      "step": 2980
    },
    {
      "epoch": 0.15946666666666667,
      "grad_norm": 0.5097295641899109,
      "learning_rate": 4.6013333333333336e-05,
      "loss": 0.0024,
      "step": 2990
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.31553083658218384,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.004,
      "step": 3000
    },
    {
      "epoch": 0.16053333333333333,
      "grad_norm": 0.17913320660591125,
      "learning_rate": 4.5986666666666665e-05,
      "loss": 0.0025,
      "step": 3010
    },
    {
      "epoch": 0.16106666666666666,
      "grad_norm": 0.6090285778045654,
      "learning_rate": 4.5973333333333336e-05,
      "loss": 0.003,
      "step": 3020
    },
    {
      "epoch": 0.1616,
      "grad_norm": 0.14389288425445557,
      "learning_rate": 4.596e-05,
      "loss": 0.0022,
      "step": 3030
    },
    {
      "epoch": 0.16213333333333332,
      "grad_norm": 0.17407841980457306,
      "learning_rate": 4.594666666666667e-05,
      "loss": 0.0035,
      "step": 3040
    },
    {
      "epoch": 0.16266666666666665,
      "grad_norm": 0.08268850296735764,
      "learning_rate": 4.5933333333333336e-05,
      "loss": 0.0025,
      "step": 3050
    },
    {
      "epoch": 0.1632,
      "grad_norm": 0.8468242883682251,
      "learning_rate": 4.592e-05,
      "loss": 0.0039,
      "step": 3060
    },
    {
      "epoch": 0.16373333333333334,
      "grad_norm": 0.15253426134586334,
      "learning_rate": 4.590666666666667e-05,
      "loss": 0.0024,
      "step": 3070
    },
    {
      "epoch": 0.16426666666666667,
      "grad_norm": 0.6127568483352661,
      "learning_rate": 4.589333333333334e-05,
      "loss": 0.0031,
      "step": 3080
    },
    {
      "epoch": 0.1648,
      "grad_norm": 0.43856561183929443,
      "learning_rate": 4.588e-05,
      "loss": 0.0022,
      "step": 3090
    },
    {
      "epoch": 0.16533333333333333,
      "grad_norm": 0.560053288936615,
      "learning_rate": 4.5866666666666666e-05,
      "loss": 0.0028,
      "step": 3100
    },
    {
      "epoch": 0.16586666666666666,
      "grad_norm": 0.21476121246814728,
      "learning_rate": 4.585333333333334e-05,
      "loss": 0.0043,
      "step": 3110
    },
    {
      "epoch": 0.1664,
      "grad_norm": 0.22295208275318146,
      "learning_rate": 4.584e-05,
      "loss": 0.003,
      "step": 3120
    },
    {
      "epoch": 0.16693333333333332,
      "grad_norm": 0.26401376724243164,
      "learning_rate": 4.5826666666666666e-05,
      "loss": 0.002,
      "step": 3130
    },
    {
      "epoch": 0.16746666666666668,
      "grad_norm": 0.2168777585029602,
      "learning_rate": 4.581333333333334e-05,
      "loss": 0.0042,
      "step": 3140
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.12060488760471344,
      "learning_rate": 4.58e-05,
      "loss": 0.0022,
      "step": 3150
    },
    {
      "epoch": 0.16853333333333334,
      "grad_norm": 0.3945830166339874,
      "learning_rate": 4.5786666666666666e-05,
      "loss": 0.0027,
      "step": 3160
    },
    {
      "epoch": 0.16906666666666667,
      "grad_norm": 0.46541985869407654,
      "learning_rate": 4.577333333333334e-05,
      "loss": 0.0032,
      "step": 3170
    },
    {
      "epoch": 0.1696,
      "grad_norm": 0.15628699958324432,
      "learning_rate": 4.576e-05,
      "loss": 0.0033,
      "step": 3180
    },
    {
      "epoch": 0.17013333333333333,
      "grad_norm": 0.20510780811309814,
      "learning_rate": 4.5746666666666674e-05,
      "loss": 0.003,
      "step": 3190
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 0.5649088621139526,
      "learning_rate": 4.573333333333333e-05,
      "loss": 0.0024,
      "step": 3200
    },
    {
      "epoch": 0.1712,
      "grad_norm": 0.15343239903450012,
      "learning_rate": 4.572e-05,
      "loss": 0.0032,
      "step": 3210
    },
    {
      "epoch": 0.17173333333333332,
      "grad_norm": 0.18580377101898193,
      "learning_rate": 4.570666666666667e-05,
      "loss": 0.0042,
      "step": 3220
    },
    {
      "epoch": 0.17226666666666668,
      "grad_norm": 0.3439041078090668,
      "learning_rate": 4.569333333333334e-05,
      "loss": 0.003,
      "step": 3230
    },
    {
      "epoch": 0.1728,
      "grad_norm": 0.2881488800048828,
      "learning_rate": 4.568e-05,
      "loss": 0.0024,
      "step": 3240
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.28975245356559753,
      "learning_rate": 4.566666666666667e-05,
      "loss": 0.0019,
      "step": 3250
    },
    {
      "epoch": 0.17386666666666667,
      "grad_norm": 0.29911020398139954,
      "learning_rate": 4.565333333333334e-05,
      "loss": 0.002,
      "step": 3260
    },
    {
      "epoch": 0.1744,
      "grad_norm": 0.5895172953605652,
      "learning_rate": 4.564e-05,
      "loss": 0.0029,
      "step": 3270
    },
    {
      "epoch": 0.17493333333333333,
      "grad_norm": 0.5799048542976379,
      "learning_rate": 4.562666666666667e-05,
      "loss": 0.0032,
      "step": 3280
    },
    {
      "epoch": 0.17546666666666666,
      "grad_norm": 0.2656268775463104,
      "learning_rate": 4.561333333333333e-05,
      "loss": 0.0046,
      "step": 3290
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.25294041633605957,
      "learning_rate": 4.5600000000000004e-05,
      "loss": 0.0025,
      "step": 3300
    },
    {
      "epoch": 0.17653333333333332,
      "grad_norm": 0.36830222606658936,
      "learning_rate": 4.558666666666667e-05,
      "loss": 0.0036,
      "step": 3310
    },
    {
      "epoch": 0.17706666666666668,
      "grad_norm": 0.16265122592449188,
      "learning_rate": 4.557333333333333e-05,
      "loss": 0.0021,
      "step": 3320
    },
    {
      "epoch": 0.1776,
      "grad_norm": 0.2562033236026764,
      "learning_rate": 4.5560000000000004e-05,
      "loss": 0.0026,
      "step": 3330
    },
    {
      "epoch": 0.17813333333333334,
      "grad_norm": 0.2344093769788742,
      "learning_rate": 4.554666666666667e-05,
      "loss": 0.0037,
      "step": 3340
    },
    {
      "epoch": 0.17866666666666667,
      "grad_norm": 0.16022881865501404,
      "learning_rate": 4.553333333333333e-05,
      "loss": 0.0026,
      "step": 3350
    },
    {
      "epoch": 0.1792,
      "grad_norm": 0.6253729462623596,
      "learning_rate": 4.5520000000000005e-05,
      "loss": 0.0029,
      "step": 3360
    },
    {
      "epoch": 0.17973333333333333,
      "grad_norm": 0.16245532035827637,
      "learning_rate": 4.550666666666667e-05,
      "loss": 0.0026,
      "step": 3370
    },
    {
      "epoch": 0.18026666666666666,
      "grad_norm": 0.1734151691198349,
      "learning_rate": 4.549333333333334e-05,
      "loss": 0.0022,
      "step": 3380
    },
    {
      "epoch": 0.1808,
      "grad_norm": 0.18465590476989746,
      "learning_rate": 4.548e-05,
      "loss": 0.0022,
      "step": 3390
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 0.34925714135169983,
      "learning_rate": 4.546666666666667e-05,
      "loss": 0.0019,
      "step": 3400
    },
    {
      "epoch": 0.18186666666666668,
      "grad_norm": 0.20273755490779877,
      "learning_rate": 4.5453333333333334e-05,
      "loss": 0.0024,
      "step": 3410
    },
    {
      "epoch": 0.1824,
      "grad_norm": 0.7429243922233582,
      "learning_rate": 4.5440000000000005e-05,
      "loss": 0.0035,
      "step": 3420
    },
    {
      "epoch": 0.18293333333333334,
      "grad_norm": 0.7634297609329224,
      "learning_rate": 4.542666666666667e-05,
      "loss": 0.0033,
      "step": 3430
    },
    {
      "epoch": 0.18346666666666667,
      "grad_norm": 0.7292332053184509,
      "learning_rate": 4.5413333333333334e-05,
      "loss": 0.002,
      "step": 3440
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.11107651144266129,
      "learning_rate": 4.5400000000000006e-05,
      "loss": 0.0034,
      "step": 3450
    },
    {
      "epoch": 0.18453333333333333,
      "grad_norm": 0.12344391644001007,
      "learning_rate": 4.5386666666666664e-05,
      "loss": 0.0022,
      "step": 3460
    },
    {
      "epoch": 0.18506666666666666,
      "grad_norm": 0.48032239079475403,
      "learning_rate": 4.5373333333333335e-05,
      "loss": 0.0028,
      "step": 3470
    },
    {
      "epoch": 0.1856,
      "grad_norm": 0.11521339416503906,
      "learning_rate": 4.536e-05,
      "loss": 0.0027,
      "step": 3480
    },
    {
      "epoch": 0.18613333333333335,
      "grad_norm": 0.27567732334136963,
      "learning_rate": 4.534666666666667e-05,
      "loss": 0.0023,
      "step": 3490
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.5153452157974243,
      "learning_rate": 4.5333333333333335e-05,
      "loss": 0.0023,
      "step": 3500
    },
    {
      "epoch": 0.1872,
      "grad_norm": 0.32921302318573,
      "learning_rate": 4.532e-05,
      "loss": 0.0033,
      "step": 3510
    },
    {
      "epoch": 0.18773333333333334,
      "grad_norm": 0.7841393351554871,
      "learning_rate": 4.530666666666667e-05,
      "loss": 0.0043,
      "step": 3520
    },
    {
      "epoch": 0.18826666666666667,
      "grad_norm": 0.5619852542877197,
      "learning_rate": 4.5293333333333336e-05,
      "loss": 0.0036,
      "step": 3530
    },
    {
      "epoch": 0.1888,
      "grad_norm": 0.19414673745632172,
      "learning_rate": 4.528e-05,
      "loss": 0.0035,
      "step": 3540
    },
    {
      "epoch": 0.18933333333333333,
      "grad_norm": 0.23260177671909332,
      "learning_rate": 4.526666666666667e-05,
      "loss": 0.0045,
      "step": 3550
    },
    {
      "epoch": 0.18986666666666666,
      "grad_norm": 0.32672569155693054,
      "learning_rate": 4.5253333333333336e-05,
      "loss": 0.0027,
      "step": 3560
    },
    {
      "epoch": 0.1904,
      "grad_norm": 0.16518069803714752,
      "learning_rate": 4.524000000000001e-05,
      "loss": 0.0022,
      "step": 3570
    },
    {
      "epoch": 0.19093333333333334,
      "grad_norm": 0.20212963223457336,
      "learning_rate": 4.5226666666666665e-05,
      "loss": 0.003,
      "step": 3580
    },
    {
      "epoch": 0.19146666666666667,
      "grad_norm": 0.11096583306789398,
      "learning_rate": 4.5213333333333336e-05,
      "loss": 0.0035,
      "step": 3590
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.29721033573150635,
      "learning_rate": 4.52e-05,
      "loss": 0.0044,
      "step": 3600
    },
    {
      "epoch": 0.19253333333333333,
      "grad_norm": 0.31837591528892517,
      "learning_rate": 4.518666666666667e-05,
      "loss": 0.0026,
      "step": 3610
    },
    {
      "epoch": 0.19306666666666666,
      "grad_norm": 0.37966588139533997,
      "learning_rate": 4.517333333333334e-05,
      "loss": 0.0026,
      "step": 3620
    },
    {
      "epoch": 0.1936,
      "grad_norm": 0.351077675819397,
      "learning_rate": 4.516e-05,
      "loss": 0.0034,
      "step": 3630
    },
    {
      "epoch": 0.19413333333333332,
      "grad_norm": 0.20059676468372345,
      "learning_rate": 4.514666666666667e-05,
      "loss": 0.0036,
      "step": 3640
    },
    {
      "epoch": 0.19466666666666665,
      "grad_norm": 0.36218297481536865,
      "learning_rate": 4.513333333333333e-05,
      "loss": 0.0028,
      "step": 3650
    },
    {
      "epoch": 0.1952,
      "grad_norm": 0.18471764028072357,
      "learning_rate": 4.512e-05,
      "loss": 0.0036,
      "step": 3660
    },
    {
      "epoch": 0.19573333333333334,
      "grad_norm": 0.2568168342113495,
      "learning_rate": 4.5106666666666666e-05,
      "loss": 0.0035,
      "step": 3670
    },
    {
      "epoch": 0.19626666666666667,
      "grad_norm": 0.6775362491607666,
      "learning_rate": 4.509333333333334e-05,
      "loss": 0.002,
      "step": 3680
    },
    {
      "epoch": 0.1968,
      "grad_norm": 0.5737816095352173,
      "learning_rate": 4.508e-05,
      "loss": 0.0039,
      "step": 3690
    },
    {
      "epoch": 0.19733333333333333,
      "grad_norm": 0.32724711298942566,
      "learning_rate": 4.5066666666666667e-05,
      "loss": 0.0027,
      "step": 3700
    },
    {
      "epoch": 0.19786666666666666,
      "grad_norm": 0.2901994585990906,
      "learning_rate": 4.505333333333334e-05,
      "loss": 0.0037,
      "step": 3710
    },
    {
      "epoch": 0.1984,
      "grad_norm": 0.12664493918418884,
      "learning_rate": 4.504e-05,
      "loss": 0.0026,
      "step": 3720
    },
    {
      "epoch": 0.19893333333333332,
      "grad_norm": 0.38848575949668884,
      "learning_rate": 4.502666666666667e-05,
      "loss": 0.0024,
      "step": 3730
    },
    {
      "epoch": 0.19946666666666665,
      "grad_norm": 0.6692451238632202,
      "learning_rate": 4.501333333333334e-05,
      "loss": 0.0025,
      "step": 3740
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.18476076424121857,
      "learning_rate": 4.5e-05,
      "loss": 0.0024,
      "step": 3750
    },
    {
      "epoch": 0.20053333333333334,
      "grad_norm": 0.4009059965610504,
      "learning_rate": 4.4986666666666674e-05,
      "loss": 0.0028,
      "step": 3760
    },
    {
      "epoch": 0.20106666666666667,
      "grad_norm": 0.3190488815307617,
      "learning_rate": 4.497333333333333e-05,
      "loss": 0.0027,
      "step": 3770
    },
    {
      "epoch": 0.2016,
      "grad_norm": 0.48819807171821594,
      "learning_rate": 4.496e-05,
      "loss": 0.0021,
      "step": 3780
    },
    {
      "epoch": 0.20213333333333333,
      "grad_norm": 0.4416932761669159,
      "learning_rate": 4.494666666666667e-05,
      "loss": 0.0035,
      "step": 3790
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 0.8497806191444397,
      "learning_rate": 4.493333333333333e-05,
      "loss": 0.0029,
      "step": 3800
    },
    {
      "epoch": 0.2032,
      "grad_norm": 0.8194786906242371,
      "learning_rate": 4.4920000000000004e-05,
      "loss": 0.0039,
      "step": 3810
    },
    {
      "epoch": 0.20373333333333332,
      "grad_norm": 0.7144873142242432,
      "learning_rate": 4.490666666666667e-05,
      "loss": 0.0038,
      "step": 3820
    },
    {
      "epoch": 0.20426666666666668,
      "grad_norm": 0.44622546434402466,
      "learning_rate": 4.489333333333334e-05,
      "loss": 0.0032,
      "step": 3830
    },
    {
      "epoch": 0.2048,
      "grad_norm": 0.6468726396560669,
      "learning_rate": 4.488e-05,
      "loss": 0.003,
      "step": 3840
    },
    {
      "epoch": 0.20533333333333334,
      "grad_norm": 0.6380339860916138,
      "learning_rate": 4.486666666666667e-05,
      "loss": 0.0035,
      "step": 3850
    },
    {
      "epoch": 0.20586666666666667,
      "grad_norm": 0.3705078959465027,
      "learning_rate": 4.485333333333333e-05,
      "loss": 0.0029,
      "step": 3860
    },
    {
      "epoch": 0.2064,
      "grad_norm": 0.11657777428627014,
      "learning_rate": 4.4840000000000004e-05,
      "loss": 0.0025,
      "step": 3870
    },
    {
      "epoch": 0.20693333333333333,
      "grad_norm": 0.4844856262207031,
      "learning_rate": 4.482666666666667e-05,
      "loss": 0.0029,
      "step": 3880
    },
    {
      "epoch": 0.20746666666666666,
      "grad_norm": 0.291507363319397,
      "learning_rate": 4.4813333333333333e-05,
      "loss": 0.0032,
      "step": 3890
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.20481422543525696,
      "learning_rate": 4.4800000000000005e-05,
      "loss": 0.0025,
      "step": 3900
    },
    {
      "epoch": 0.20853333333333332,
      "grad_norm": 0.3395234942436218,
      "learning_rate": 4.478666666666667e-05,
      "loss": 0.0021,
      "step": 3910
    },
    {
      "epoch": 0.20906666666666668,
      "grad_norm": 0.1901383101940155,
      "learning_rate": 4.4773333333333334e-05,
      "loss": 0.0029,
      "step": 3920
    },
    {
      "epoch": 0.2096,
      "grad_norm": 0.33407849073410034,
      "learning_rate": 4.4760000000000005e-05,
      "loss": 0.0026,
      "step": 3930
    },
    {
      "epoch": 0.21013333333333334,
      "grad_norm": 0.10851243138313293,
      "learning_rate": 4.474666666666667e-05,
      "loss": 0.0031,
      "step": 3940
    },
    {
      "epoch": 0.21066666666666667,
      "grad_norm": 0.29805195331573486,
      "learning_rate": 4.473333333333334e-05,
      "loss": 0.0037,
      "step": 3950
    },
    {
      "epoch": 0.2112,
      "grad_norm": 0.4333183765411377,
      "learning_rate": 4.472e-05,
      "loss": 0.0036,
      "step": 3960
    },
    {
      "epoch": 0.21173333333333333,
      "grad_norm": 0.1526816487312317,
      "learning_rate": 4.470666666666667e-05,
      "loss": 0.0027,
      "step": 3970
    },
    {
      "epoch": 0.21226666666666666,
      "grad_norm": 0.2831099033355713,
      "learning_rate": 4.4693333333333335e-05,
      "loss": 0.0023,
      "step": 3980
    },
    {
      "epoch": 0.2128,
      "grad_norm": 0.5099756121635437,
      "learning_rate": 4.468e-05,
      "loss": 0.0046,
      "step": 3990
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.4777369797229767,
      "learning_rate": 4.466666666666667e-05,
      "loss": 0.0029,
      "step": 4000
    },
    {
      "epoch": 0.21386666666666668,
      "grad_norm": 0.39911648631095886,
      "learning_rate": 4.4653333333333335e-05,
      "loss": 0.0029,
      "step": 4010
    },
    {
      "epoch": 0.2144,
      "grad_norm": 0.19382750988006592,
      "learning_rate": 4.4640000000000006e-05,
      "loss": 0.0032,
      "step": 4020
    },
    {
      "epoch": 0.21493333333333334,
      "grad_norm": 0.4500639736652374,
      "learning_rate": 4.4626666666666664e-05,
      "loss": 0.0024,
      "step": 4030
    },
    {
      "epoch": 0.21546666666666667,
      "grad_norm": 0.30089256167411804,
      "learning_rate": 4.4613333333333335e-05,
      "loss": 0.0026,
      "step": 4040
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.23230545222759247,
      "learning_rate": 4.46e-05,
      "loss": 0.0036,
      "step": 4050
    },
    {
      "epoch": 0.21653333333333333,
      "grad_norm": 0.5135208368301392,
      "learning_rate": 4.458666666666667e-05,
      "loss": 0.0034,
      "step": 4060
    },
    {
      "epoch": 0.21706666666666666,
      "grad_norm": 0.3116420805454254,
      "learning_rate": 4.4573333333333336e-05,
      "loss": 0.0035,
      "step": 4070
    },
    {
      "epoch": 0.2176,
      "grad_norm": 0.7826194763183594,
      "learning_rate": 4.456e-05,
      "loss": 0.004,
      "step": 4080
    },
    {
      "epoch": 0.21813333333333335,
      "grad_norm": 0.5169838666915894,
      "learning_rate": 4.454666666666667e-05,
      "loss": 0.0023,
      "step": 4090
    },
    {
      "epoch": 0.21866666666666668,
      "grad_norm": 0.5954519510269165,
      "learning_rate": 4.4533333333333336e-05,
      "loss": 0.0031,
      "step": 4100
    },
    {
      "epoch": 0.2192,
      "grad_norm": 0.5431162118911743,
      "learning_rate": 4.452e-05,
      "loss": 0.0035,
      "step": 4110
    },
    {
      "epoch": 0.21973333333333334,
      "grad_norm": 0.39586275815963745,
      "learning_rate": 4.450666666666667e-05,
      "loss": 0.0025,
      "step": 4120
    },
    {
      "epoch": 0.22026666666666667,
      "grad_norm": 0.161690816283226,
      "learning_rate": 4.4493333333333337e-05,
      "loss": 0.0029,
      "step": 4130
    },
    {
      "epoch": 0.2208,
      "grad_norm": 0.23306098580360413,
      "learning_rate": 4.448e-05,
      "loss": 0.0033,
      "step": 4140
    },
    {
      "epoch": 0.22133333333333333,
      "grad_norm": 0.4568333327770233,
      "learning_rate": 4.4466666666666666e-05,
      "loss": 0.0026,
      "step": 4150
    },
    {
      "epoch": 0.22186666666666666,
      "grad_norm": 0.2901155352592468,
      "learning_rate": 4.445333333333334e-05,
      "loss": 0.0032,
      "step": 4160
    },
    {
      "epoch": 0.2224,
      "grad_norm": 0.21487052738666534,
      "learning_rate": 4.444e-05,
      "loss": 0.0025,
      "step": 4170
    },
    {
      "epoch": 0.22293333333333334,
      "grad_norm": 0.25082927942276,
      "learning_rate": 4.4426666666666666e-05,
      "loss": 0.002,
      "step": 4180
    },
    {
      "epoch": 0.22346666666666667,
      "grad_norm": 0.22620320320129395,
      "learning_rate": 4.441333333333334e-05,
      "loss": 0.0032,
      "step": 4190
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.5419252514839172,
      "learning_rate": 4.44e-05,
      "loss": 0.0028,
      "step": 4200
    },
    {
      "epoch": 0.22453333333333333,
      "grad_norm": 0.19896194338798523,
      "learning_rate": 4.438666666666667e-05,
      "loss": 0.003,
      "step": 4210
    },
    {
      "epoch": 0.22506666666666666,
      "grad_norm": 0.1809750199317932,
      "learning_rate": 4.437333333333333e-05,
      "loss": 0.0021,
      "step": 4220
    },
    {
      "epoch": 0.2256,
      "grad_norm": 0.2715068757534027,
      "learning_rate": 4.436e-05,
      "loss": 0.0026,
      "step": 4230
    },
    {
      "epoch": 0.22613333333333333,
      "grad_norm": 1.110140085220337,
      "learning_rate": 4.434666666666667e-05,
      "loss": 0.0027,
      "step": 4240
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.3953380882740021,
      "learning_rate": 4.433333333333334e-05,
      "loss": 0.003,
      "step": 4250
    },
    {
      "epoch": 0.2272,
      "grad_norm": 0.7368340492248535,
      "learning_rate": 4.432e-05,
      "loss": 0.0033,
      "step": 4260
    },
    {
      "epoch": 0.22773333333333334,
      "grad_norm": 0.1557479351758957,
      "learning_rate": 4.430666666666667e-05,
      "loss": 0.0029,
      "step": 4270
    },
    {
      "epoch": 0.22826666666666667,
      "grad_norm": 0.7437606453895569,
      "learning_rate": 4.429333333333334e-05,
      "loss": 0.0032,
      "step": 4280
    },
    {
      "epoch": 0.2288,
      "grad_norm": 0.6025123596191406,
      "learning_rate": 4.428e-05,
      "loss": 0.0023,
      "step": 4290
    },
    {
      "epoch": 0.22933333333333333,
      "grad_norm": 0.1611061990261078,
      "learning_rate": 4.426666666666667e-05,
      "loss": 0.002,
      "step": 4300
    },
    {
      "epoch": 0.22986666666666666,
      "grad_norm": 0.13625596463680267,
      "learning_rate": 4.425333333333334e-05,
      "loss": 0.0024,
      "step": 4310
    },
    {
      "epoch": 0.2304,
      "grad_norm": 0.24558039009571075,
      "learning_rate": 4.424e-05,
      "loss": 0.0023,
      "step": 4320
    },
    {
      "epoch": 0.23093333333333332,
      "grad_norm": 0.5183761119842529,
      "learning_rate": 4.422666666666667e-05,
      "loss": 0.0038,
      "step": 4330
    },
    {
      "epoch": 0.23146666666666665,
      "grad_norm": 0.14110571146011353,
      "learning_rate": 4.421333333333333e-05,
      "loss": 0.0034,
      "step": 4340
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.2232886552810669,
      "learning_rate": 4.4200000000000004e-05,
      "loss": 0.0018,
      "step": 4350
    },
    {
      "epoch": 0.23253333333333334,
      "grad_norm": 0.3463354706764221,
      "learning_rate": 4.418666666666667e-05,
      "loss": 0.004,
      "step": 4360
    },
    {
      "epoch": 0.23306666666666667,
      "grad_norm": 0.34680482745170593,
      "learning_rate": 4.417333333333333e-05,
      "loss": 0.0026,
      "step": 4370
    },
    {
      "epoch": 0.2336,
      "grad_norm": 0.4479166865348816,
      "learning_rate": 4.4160000000000004e-05,
      "loss": 0.0018,
      "step": 4380
    },
    {
      "epoch": 0.23413333333333333,
      "grad_norm": 0.2070322334766388,
      "learning_rate": 4.414666666666667e-05,
      "loss": 0.0026,
      "step": 4390
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 0.3605336546897888,
      "learning_rate": 4.413333333333334e-05,
      "loss": 0.0023,
      "step": 4400
    },
    {
      "epoch": 0.2352,
      "grad_norm": 0.1489660143852234,
      "learning_rate": 4.412e-05,
      "loss": 0.0031,
      "step": 4410
    },
    {
      "epoch": 0.23573333333333332,
      "grad_norm": 0.14370352029800415,
      "learning_rate": 4.410666666666667e-05,
      "loss": 0.0022,
      "step": 4420
    },
    {
      "epoch": 0.23626666666666668,
      "grad_norm": 0.33418285846710205,
      "learning_rate": 4.4093333333333334e-05,
      "loss": 0.0021,
      "step": 4430
    },
    {
      "epoch": 0.2368,
      "grad_norm": 0.13678434491157532,
      "learning_rate": 4.4080000000000005e-05,
      "loss": 0.0023,
      "step": 4440
    },
    {
      "epoch": 0.23733333333333334,
      "grad_norm": 0.4983605146408081,
      "learning_rate": 4.406666666666667e-05,
      "loss": 0.0022,
      "step": 4450
    },
    {
      "epoch": 0.23786666666666667,
      "grad_norm": 0.37553921341896057,
      "learning_rate": 4.4053333333333334e-05,
      "loss": 0.0037,
      "step": 4460
    },
    {
      "epoch": 0.2384,
      "grad_norm": 0.28656136989593506,
      "learning_rate": 4.4040000000000005e-05,
      "loss": 0.0024,
      "step": 4470
    },
    {
      "epoch": 0.23893333333333333,
      "grad_norm": 0.24918143451213837,
      "learning_rate": 4.402666666666666e-05,
      "loss": 0.0032,
      "step": 4480
    },
    {
      "epoch": 0.23946666666666666,
      "grad_norm": 0.07557390630245209,
      "learning_rate": 4.4013333333333334e-05,
      "loss": 0.0021,
      "step": 4490
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.22989478707313538,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.0026,
      "step": 4500
    },
    {
      "epoch": 0.24053333333333332,
      "grad_norm": 0.13759423792362213,
      "learning_rate": 4.398666666666667e-05,
      "loss": 0.0029,
      "step": 4510
    },
    {
      "epoch": 0.24106666666666668,
      "grad_norm": 0.3390274941921234,
      "learning_rate": 4.3973333333333335e-05,
      "loss": 0.0032,
      "step": 4520
    },
    {
      "epoch": 0.2416,
      "grad_norm": 0.37175285816192627,
      "learning_rate": 4.396e-05,
      "loss": 0.0022,
      "step": 4530
    },
    {
      "epoch": 0.24213333333333334,
      "grad_norm": 0.38739821314811707,
      "learning_rate": 4.394666666666667e-05,
      "loss": 0.0022,
      "step": 4540
    },
    {
      "epoch": 0.24266666666666667,
      "grad_norm": 0.33325842022895813,
      "learning_rate": 4.3933333333333335e-05,
      "loss": 0.0029,
      "step": 4550
    },
    {
      "epoch": 0.2432,
      "grad_norm": 0.5220767855644226,
      "learning_rate": 4.392e-05,
      "loss": 0.0021,
      "step": 4560
    },
    {
      "epoch": 0.24373333333333333,
      "grad_norm": 0.8174947500228882,
      "learning_rate": 4.390666666666667e-05,
      "loss": 0.0025,
      "step": 4570
    },
    {
      "epoch": 0.24426666666666666,
      "grad_norm": 0.3662944734096527,
      "learning_rate": 4.3893333333333335e-05,
      "loss": 0.0026,
      "step": 4580
    },
    {
      "epoch": 0.2448,
      "grad_norm": 0.6148641109466553,
      "learning_rate": 4.388000000000001e-05,
      "loss": 0.0019,
      "step": 4590
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 0.10963530838489532,
      "learning_rate": 4.3866666666666665e-05,
      "loss": 0.0023,
      "step": 4600
    },
    {
      "epoch": 0.24586666666666668,
      "grad_norm": 0.5495123267173767,
      "learning_rate": 4.3853333333333336e-05,
      "loss": 0.0041,
      "step": 4610
    },
    {
      "epoch": 0.2464,
      "grad_norm": 0.135764941573143,
      "learning_rate": 4.384e-05,
      "loss": 0.0021,
      "step": 4620
    },
    {
      "epoch": 0.24693333333333334,
      "grad_norm": 0.6801205277442932,
      "learning_rate": 4.382666666666667e-05,
      "loss": 0.0022,
      "step": 4630
    },
    {
      "epoch": 0.24746666666666667,
      "grad_norm": 0.1836833953857422,
      "learning_rate": 4.3813333333333336e-05,
      "loss": 0.0027,
      "step": 4640
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.24583931267261505,
      "learning_rate": 4.38e-05,
      "loss": 0.0031,
      "step": 4650
    },
    {
      "epoch": 0.24853333333333333,
      "grad_norm": 0.5515477061271667,
      "learning_rate": 4.378666666666667e-05,
      "loss": 0.0022,
      "step": 4660
    },
    {
      "epoch": 0.24906666666666666,
      "grad_norm": 0.5749999284744263,
      "learning_rate": 4.377333333333333e-05,
      "loss": 0.0024,
      "step": 4670
    },
    {
      "epoch": 0.2496,
      "grad_norm": 0.734032928943634,
      "learning_rate": 4.376e-05,
      "loss": 0.0031,
      "step": 4680
    },
    {
      "epoch": 0.2501333333333333,
      "grad_norm": 0.09479519724845886,
      "learning_rate": 4.374666666666667e-05,
      "loss": 0.0029,
      "step": 4690
    },
    {
      "epoch": 0.25066666666666665,
      "grad_norm": 0.5958948731422424,
      "learning_rate": 4.373333333333334e-05,
      "loss": 0.0029,
      "step": 4700
    },
    {
      "epoch": 0.2512,
      "grad_norm": 0.5172159671783447,
      "learning_rate": 4.372e-05,
      "loss": 0.0027,
      "step": 4710
    },
    {
      "epoch": 0.2517333333333333,
      "grad_norm": 0.07415895164012909,
      "learning_rate": 4.3706666666666666e-05,
      "loss": 0.0019,
      "step": 4720
    },
    {
      "epoch": 0.25226666666666664,
      "grad_norm": 0.5531887412071228,
      "learning_rate": 4.369333333333334e-05,
      "loss": 0.0025,
      "step": 4730
    },
    {
      "epoch": 0.2528,
      "grad_norm": 0.33331945538520813,
      "learning_rate": 4.368e-05,
      "loss": 0.0031,
      "step": 4740
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.14178083837032318,
      "learning_rate": 4.3666666666666666e-05,
      "loss": 0.0038,
      "step": 4750
    },
    {
      "epoch": 0.2538666666666667,
      "grad_norm": 0.2503119707107544,
      "learning_rate": 4.365333333333334e-05,
      "loss": 0.0024,
      "step": 4760
    },
    {
      "epoch": 0.2544,
      "grad_norm": 0.3236677944660187,
      "learning_rate": 4.364e-05,
      "loss": 0.0026,
      "step": 4770
    },
    {
      "epoch": 0.25493333333333335,
      "grad_norm": 0.2799382209777832,
      "learning_rate": 4.3626666666666674e-05,
      "loss": 0.0026,
      "step": 4780
    },
    {
      "epoch": 0.2554666666666667,
      "grad_norm": 0.41555431485176086,
      "learning_rate": 4.361333333333333e-05,
      "loss": 0.0028,
      "step": 4790
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.6171472072601318,
      "learning_rate": 4.36e-05,
      "loss": 0.0032,
      "step": 4800
    },
    {
      "epoch": 0.25653333333333334,
      "grad_norm": 0.2081703543663025,
      "learning_rate": 4.358666666666667e-05,
      "loss": 0.0033,
      "step": 4810
    },
    {
      "epoch": 0.25706666666666667,
      "grad_norm": 0.268789142370224,
      "learning_rate": 4.357333333333333e-05,
      "loss": 0.0032,
      "step": 4820
    },
    {
      "epoch": 0.2576,
      "grad_norm": 0.5086947083473206,
      "learning_rate": 4.356e-05,
      "loss": 0.0034,
      "step": 4830
    },
    {
      "epoch": 0.2581333333333333,
      "grad_norm": 0.4171963036060333,
      "learning_rate": 4.354666666666667e-05,
      "loss": 0.0023,
      "step": 4840
    },
    {
      "epoch": 0.25866666666666666,
      "grad_norm": 0.6196460127830505,
      "learning_rate": 4.353333333333334e-05,
      "loss": 0.003,
      "step": 4850
    },
    {
      "epoch": 0.2592,
      "grad_norm": 0.3110122084617615,
      "learning_rate": 4.352e-05,
      "loss": 0.0035,
      "step": 4860
    },
    {
      "epoch": 0.2597333333333333,
      "grad_norm": 0.318761944770813,
      "learning_rate": 4.350666666666667e-05,
      "loss": 0.0022,
      "step": 4870
    },
    {
      "epoch": 0.26026666666666665,
      "grad_norm": 0.2317630648612976,
      "learning_rate": 4.349333333333334e-05,
      "loss": 0.0035,
      "step": 4880
    },
    {
      "epoch": 0.2608,
      "grad_norm": 0.7872533202171326,
      "learning_rate": 4.3480000000000004e-05,
      "loss": 0.0033,
      "step": 4890
    },
    {
      "epoch": 0.2613333333333333,
      "grad_norm": 0.5599833726882935,
      "learning_rate": 4.346666666666667e-05,
      "loss": 0.002,
      "step": 4900
    },
    {
      "epoch": 0.2618666666666667,
      "grad_norm": 0.49520331621170044,
      "learning_rate": 4.345333333333333e-05,
      "loss": 0.0035,
      "step": 4910
    },
    {
      "epoch": 0.2624,
      "grad_norm": 1.0181077718734741,
      "learning_rate": 4.3440000000000004e-05,
      "loss": 0.0026,
      "step": 4920
    },
    {
      "epoch": 0.26293333333333335,
      "grad_norm": 0.47037628293037415,
      "learning_rate": 4.342666666666667e-05,
      "loss": 0.0019,
      "step": 4930
    },
    {
      "epoch": 0.2634666666666667,
      "grad_norm": 0.2676118314266205,
      "learning_rate": 4.341333333333333e-05,
      "loss": 0.002,
      "step": 4940
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.25453847646713257,
      "learning_rate": 4.3400000000000005e-05,
      "loss": 0.0038,
      "step": 4950
    },
    {
      "epoch": 0.26453333333333334,
      "grad_norm": 0.34917953610420227,
      "learning_rate": 4.338666666666667e-05,
      "loss": 0.0028,
      "step": 4960
    },
    {
      "epoch": 0.2650666666666667,
      "grad_norm": 0.18718105554580688,
      "learning_rate": 4.337333333333334e-05,
      "loss": 0.003,
      "step": 4970
    },
    {
      "epoch": 0.2656,
      "grad_norm": 0.2956927716732025,
      "learning_rate": 4.336e-05,
      "loss": 0.0023,
      "step": 4980
    },
    {
      "epoch": 0.26613333333333333,
      "grad_norm": 0.7070742845535278,
      "learning_rate": 4.334666666666667e-05,
      "loss": 0.0019,
      "step": 4990
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.5081766247749329,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 0.0027,
      "step": 5000
    },
    {
      "epoch": 0.2672,
      "grad_norm": 0.1222720518708229,
      "learning_rate": 4.332e-05,
      "loss": 0.0036,
      "step": 5010
    },
    {
      "epoch": 0.2677333333333333,
      "grad_norm": 0.42174994945526123,
      "learning_rate": 4.330666666666667e-05,
      "loss": 0.0039,
      "step": 5020
    },
    {
      "epoch": 0.26826666666666665,
      "grad_norm": 0.38949549198150635,
      "learning_rate": 4.3293333333333334e-05,
      "loss": 0.0024,
      "step": 5030
    },
    {
      "epoch": 0.2688,
      "grad_norm": 0.0967123880982399,
      "learning_rate": 4.3280000000000006e-05,
      "loss": 0.0021,
      "step": 5040
    },
    {
      "epoch": 0.2693333333333333,
      "grad_norm": 0.13828633725643158,
      "learning_rate": 4.3266666666666664e-05,
      "loss": 0.0022,
      "step": 5050
    },
    {
      "epoch": 0.26986666666666664,
      "grad_norm": 0.14943361282348633,
      "learning_rate": 4.3253333333333335e-05,
      "loss": 0.0028,
      "step": 5060
    },
    {
      "epoch": 0.2704,
      "grad_norm": 0.32893747091293335,
      "learning_rate": 4.324e-05,
      "loss": 0.002,
      "step": 5070
    },
    {
      "epoch": 0.27093333333333336,
      "grad_norm": 0.24629759788513184,
      "learning_rate": 4.322666666666667e-05,
      "loss": 0.0022,
      "step": 5080
    },
    {
      "epoch": 0.2714666666666667,
      "grad_norm": 0.144692063331604,
      "learning_rate": 4.3213333333333335e-05,
      "loss": 0.0034,
      "step": 5090
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.1461353898048401,
      "learning_rate": 4.32e-05,
      "loss": 0.0021,
      "step": 5100
    },
    {
      "epoch": 0.27253333333333335,
      "grad_norm": 0.3213692307472229,
      "learning_rate": 4.318666666666667e-05,
      "loss": 0.0023,
      "step": 5110
    },
    {
      "epoch": 0.2730666666666667,
      "grad_norm": 0.4100509583950043,
      "learning_rate": 4.3173333333333336e-05,
      "loss": 0.0027,
      "step": 5120
    },
    {
      "epoch": 0.2736,
      "grad_norm": 0.5177026391029358,
      "learning_rate": 4.316e-05,
      "loss": 0.0026,
      "step": 5130
    },
    {
      "epoch": 0.27413333333333334,
      "grad_norm": 0.37659570574760437,
      "learning_rate": 4.314666666666667e-05,
      "loss": 0.0028,
      "step": 5140
    },
    {
      "epoch": 0.27466666666666667,
      "grad_norm": 0.2397693246603012,
      "learning_rate": 4.3133333333333336e-05,
      "loss": 0.0027,
      "step": 5150
    },
    {
      "epoch": 0.2752,
      "grad_norm": 0.21952959895133972,
      "learning_rate": 4.312000000000001e-05,
      "loss": 0.0021,
      "step": 5160
    },
    {
      "epoch": 0.27573333333333333,
      "grad_norm": 0.5903810858726501,
      "learning_rate": 4.3106666666666665e-05,
      "loss": 0.0023,
      "step": 5170
    },
    {
      "epoch": 0.27626666666666666,
      "grad_norm": 0.2359536588191986,
      "learning_rate": 4.3093333333333336e-05,
      "loss": 0.0022,
      "step": 5180
    },
    {
      "epoch": 0.2768,
      "grad_norm": 0.20292101800441742,
      "learning_rate": 4.308e-05,
      "loss": 0.0027,
      "step": 5190
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 0.3146935701370239,
      "learning_rate": 4.3066666666666665e-05,
      "loss": 0.0025,
      "step": 5200
    },
    {
      "epoch": 0.27786666666666665,
      "grad_norm": 0.1471342146396637,
      "learning_rate": 4.305333333333334e-05,
      "loss": 0.0019,
      "step": 5210
    },
    {
      "epoch": 0.2784,
      "grad_norm": 0.13118833303451538,
      "learning_rate": 4.304e-05,
      "loss": 0.0031,
      "step": 5220
    },
    {
      "epoch": 0.2789333333333333,
      "grad_norm": 0.1711268275976181,
      "learning_rate": 4.302666666666667e-05,
      "loss": 0.0021,
      "step": 5230
    },
    {
      "epoch": 0.27946666666666664,
      "grad_norm": 0.3162470757961273,
      "learning_rate": 4.301333333333333e-05,
      "loss": 0.0041,
      "step": 5240
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.20474739372730255,
      "learning_rate": 4.3e-05,
      "loss": 0.0019,
      "step": 5250
    },
    {
      "epoch": 0.28053333333333336,
      "grad_norm": 0.7159528732299805,
      "learning_rate": 4.2986666666666666e-05,
      "loss": 0.003,
      "step": 5260
    },
    {
      "epoch": 0.2810666666666667,
      "grad_norm": 0.6784454584121704,
      "learning_rate": 4.297333333333334e-05,
      "loss": 0.0026,
      "step": 5270
    },
    {
      "epoch": 0.2816,
      "grad_norm": 0.07956156879663467,
      "learning_rate": 4.296e-05,
      "loss": 0.0031,
      "step": 5280
    },
    {
      "epoch": 0.28213333333333335,
      "grad_norm": 0.5685825943946838,
      "learning_rate": 4.2946666666666667e-05,
      "loss": 0.0024,
      "step": 5290
    },
    {
      "epoch": 0.2826666666666667,
      "grad_norm": 0.6917399764060974,
      "learning_rate": 4.293333333333334e-05,
      "loss": 0.003,
      "step": 5300
    },
    {
      "epoch": 0.2832,
      "grad_norm": 0.2706318497657776,
      "learning_rate": 4.292e-05,
      "loss": 0.0038,
      "step": 5310
    },
    {
      "epoch": 0.28373333333333334,
      "grad_norm": 0.9804276823997498,
      "learning_rate": 4.290666666666667e-05,
      "loss": 0.0025,
      "step": 5320
    },
    {
      "epoch": 0.28426666666666667,
      "grad_norm": 0.36337241530418396,
      "learning_rate": 4.289333333333334e-05,
      "loss": 0.0023,
      "step": 5330
    },
    {
      "epoch": 0.2848,
      "grad_norm": 0.7343758940696716,
      "learning_rate": 4.288e-05,
      "loss": 0.003,
      "step": 5340
    },
    {
      "epoch": 0.2853333333333333,
      "grad_norm": 0.18811479210853577,
      "learning_rate": 4.286666666666667e-05,
      "loss": 0.0023,
      "step": 5350
    },
    {
      "epoch": 0.28586666666666666,
      "grad_norm": 0.3413133919239044,
      "learning_rate": 4.285333333333333e-05,
      "loss": 0.0031,
      "step": 5360
    },
    {
      "epoch": 0.2864,
      "grad_norm": 0.34276115894317627,
      "learning_rate": 4.284e-05,
      "loss": 0.0029,
      "step": 5370
    },
    {
      "epoch": 0.2869333333333333,
      "grad_norm": 0.16713687777519226,
      "learning_rate": 4.282666666666667e-05,
      "loss": 0.0026,
      "step": 5380
    },
    {
      "epoch": 0.28746666666666665,
      "grad_norm": 0.6883746981620789,
      "learning_rate": 4.281333333333333e-05,
      "loss": 0.0021,
      "step": 5390
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.3556794822216034,
      "learning_rate": 4.2800000000000004e-05,
      "loss": 0.0028,
      "step": 5400
    },
    {
      "epoch": 0.2885333333333333,
      "grad_norm": 0.8427671194076538,
      "learning_rate": 4.278666666666667e-05,
      "loss": 0.0018,
      "step": 5410
    },
    {
      "epoch": 0.2890666666666667,
      "grad_norm": 0.20143428444862366,
      "learning_rate": 4.277333333333334e-05,
      "loss": 0.0044,
      "step": 5420
    },
    {
      "epoch": 0.2896,
      "grad_norm": 0.5458788871765137,
      "learning_rate": 4.276e-05,
      "loss": 0.0022,
      "step": 5430
    },
    {
      "epoch": 0.29013333333333335,
      "grad_norm": 0.4056607484817505,
      "learning_rate": 4.274666666666667e-05,
      "loss": 0.0027,
      "step": 5440
    },
    {
      "epoch": 0.2906666666666667,
      "grad_norm": 0.37484991550445557,
      "learning_rate": 4.273333333333333e-05,
      "loss": 0.0025,
      "step": 5450
    },
    {
      "epoch": 0.2912,
      "grad_norm": 0.5117227435112,
      "learning_rate": 4.2720000000000004e-05,
      "loss": 0.0022,
      "step": 5460
    },
    {
      "epoch": 0.29173333333333334,
      "grad_norm": 0.38367760181427,
      "learning_rate": 4.270666666666667e-05,
      "loss": 0.0024,
      "step": 5470
    },
    {
      "epoch": 0.2922666666666667,
      "grad_norm": 0.2291407436132431,
      "learning_rate": 4.2693333333333333e-05,
      "loss": 0.0026,
      "step": 5480
    },
    {
      "epoch": 0.2928,
      "grad_norm": 0.7362182140350342,
      "learning_rate": 4.2680000000000005e-05,
      "loss": 0.0033,
      "step": 5490
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.26807770133018494,
      "learning_rate": 4.266666666666667e-05,
      "loss": 0.003,
      "step": 5500
    },
    {
      "epoch": 0.29386666666666666,
      "grad_norm": 0.1518259346485138,
      "learning_rate": 4.2653333333333334e-05,
      "loss": 0.0031,
      "step": 5510
    },
    {
      "epoch": 0.2944,
      "grad_norm": 0.32449713349342346,
      "learning_rate": 4.2640000000000005e-05,
      "loss": 0.003,
      "step": 5520
    },
    {
      "epoch": 0.2949333333333333,
      "grad_norm": 0.4187544584274292,
      "learning_rate": 4.262666666666667e-05,
      "loss": 0.003,
      "step": 5530
    },
    {
      "epoch": 0.29546666666666666,
      "grad_norm": 0.21573451161384583,
      "learning_rate": 4.2613333333333334e-05,
      "loss": 0.0022,
      "step": 5540
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.4663230776786804,
      "learning_rate": 4.26e-05,
      "loss": 0.0031,
      "step": 5550
    },
    {
      "epoch": 0.2965333333333333,
      "grad_norm": 0.2611558139324188,
      "learning_rate": 4.258666666666667e-05,
      "loss": 0.0022,
      "step": 5560
    },
    {
      "epoch": 0.29706666666666665,
      "grad_norm": 0.5538924336433411,
      "learning_rate": 4.2573333333333335e-05,
      "loss": 0.0038,
      "step": 5570
    },
    {
      "epoch": 0.2976,
      "grad_norm": 0.7032426595687866,
      "learning_rate": 4.256e-05,
      "loss": 0.0049,
      "step": 5580
    },
    {
      "epoch": 0.2981333333333333,
      "grad_norm": 0.13062837719917297,
      "learning_rate": 4.254666666666667e-05,
      "loss": 0.0039,
      "step": 5590
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 0.2097868025302887,
      "learning_rate": 4.2533333333333335e-05,
      "loss": 0.003,
      "step": 5600
    },
    {
      "epoch": 0.2992,
      "grad_norm": 0.4626479148864746,
      "learning_rate": 4.2520000000000006e-05,
      "loss": 0.0048,
      "step": 5610
    },
    {
      "epoch": 0.29973333333333335,
      "grad_norm": 0.432107537984848,
      "learning_rate": 4.2506666666666664e-05,
      "loss": 0.0034,
      "step": 5620
    },
    {
      "epoch": 0.3002666666666667,
      "grad_norm": 0.2938764691352844,
      "learning_rate": 4.2493333333333335e-05,
      "loss": 0.0028,
      "step": 5630
    },
    {
      "epoch": 0.3008,
      "grad_norm": 0.3367592394351959,
      "learning_rate": 4.248e-05,
      "loss": 0.003,
      "step": 5640
    },
    {
      "epoch": 0.30133333333333334,
      "grad_norm": 0.5777459144592285,
      "learning_rate": 4.246666666666667e-05,
      "loss": 0.0031,
      "step": 5650
    },
    {
      "epoch": 0.30186666666666667,
      "grad_norm": 0.20414255559444427,
      "learning_rate": 4.2453333333333336e-05,
      "loss": 0.004,
      "step": 5660
    },
    {
      "epoch": 0.3024,
      "grad_norm": 0.7597344517707825,
      "learning_rate": 4.244e-05,
      "loss": 0.0036,
      "step": 5670
    },
    {
      "epoch": 0.30293333333333333,
      "grad_norm": 0.16608473658561707,
      "learning_rate": 4.242666666666667e-05,
      "loss": 0.0025,
      "step": 5680
    },
    {
      "epoch": 0.30346666666666666,
      "grad_norm": 0.5842737555503845,
      "learning_rate": 4.241333333333333e-05,
      "loss": 0.0034,
      "step": 5690
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.6391652822494507,
      "learning_rate": 4.24e-05,
      "loss": 0.0037,
      "step": 5700
    },
    {
      "epoch": 0.3045333333333333,
      "grad_norm": 0.6048666834831238,
      "learning_rate": 4.238666666666667e-05,
      "loss": 0.0017,
      "step": 5710
    },
    {
      "epoch": 0.30506666666666665,
      "grad_norm": 0.9259210824966431,
      "learning_rate": 4.2373333333333336e-05,
      "loss": 0.0019,
      "step": 5720
    },
    {
      "epoch": 0.3056,
      "grad_norm": 0.6476956605911255,
      "learning_rate": 4.236e-05,
      "loss": 0.0019,
      "step": 5730
    },
    {
      "epoch": 0.3061333333333333,
      "grad_norm": 0.6925224661827087,
      "learning_rate": 4.2346666666666666e-05,
      "loss": 0.0031,
      "step": 5740
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.13372504711151123,
      "learning_rate": 4.233333333333334e-05,
      "loss": 0.0024,
      "step": 5750
    },
    {
      "epoch": 0.3072,
      "grad_norm": 0.3230297863483429,
      "learning_rate": 4.232e-05,
      "loss": 0.0036,
      "step": 5760
    },
    {
      "epoch": 0.30773333333333336,
      "grad_norm": 0.10254846513271332,
      "learning_rate": 4.2306666666666666e-05,
      "loss": 0.0022,
      "step": 5770
    },
    {
      "epoch": 0.3082666666666667,
      "grad_norm": 0.4498192369937897,
      "learning_rate": 4.229333333333334e-05,
      "loss": 0.0026,
      "step": 5780
    },
    {
      "epoch": 0.3088,
      "grad_norm": 0.490670770406723,
      "learning_rate": 4.228e-05,
      "loss": 0.0031,
      "step": 5790
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 0.15816088020801544,
      "learning_rate": 4.226666666666667e-05,
      "loss": 0.002,
      "step": 5800
    },
    {
      "epoch": 0.3098666666666667,
      "grad_norm": 0.11851593852043152,
      "learning_rate": 4.225333333333333e-05,
      "loss": 0.0025,
      "step": 5810
    },
    {
      "epoch": 0.3104,
      "grad_norm": 0.48202475905418396,
      "learning_rate": 4.224e-05,
      "loss": 0.0023,
      "step": 5820
    },
    {
      "epoch": 0.31093333333333334,
      "grad_norm": 0.20594289898872375,
      "learning_rate": 4.222666666666667e-05,
      "loss": 0.0026,
      "step": 5830
    },
    {
      "epoch": 0.31146666666666667,
      "grad_norm": 0.42036131024360657,
      "learning_rate": 4.221333333333334e-05,
      "loss": 0.0026,
      "step": 5840
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.7316758632659912,
      "learning_rate": 4.22e-05,
      "loss": 0.0031,
      "step": 5850
    },
    {
      "epoch": 0.31253333333333333,
      "grad_norm": 0.49852126836776733,
      "learning_rate": 4.218666666666667e-05,
      "loss": 0.0019,
      "step": 5860
    },
    {
      "epoch": 0.31306666666666666,
      "grad_norm": 0.32787537574768066,
      "learning_rate": 4.217333333333334e-05,
      "loss": 0.0022,
      "step": 5870
    },
    {
      "epoch": 0.3136,
      "grad_norm": 0.19544997811317444,
      "learning_rate": 4.2159999999999996e-05,
      "loss": 0.0022,
      "step": 5880
    },
    {
      "epoch": 0.3141333333333333,
      "grad_norm": 0.26248255372047424,
      "learning_rate": 4.214666666666667e-05,
      "loss": 0.0022,
      "step": 5890
    },
    {
      "epoch": 0.31466666666666665,
      "grad_norm": 0.40023306012153625,
      "learning_rate": 4.213333333333334e-05,
      "loss": 0.0038,
      "step": 5900
    },
    {
      "epoch": 0.3152,
      "grad_norm": 0.847815990447998,
      "learning_rate": 4.212e-05,
      "loss": 0.0021,
      "step": 5910
    },
    {
      "epoch": 0.3157333333333333,
      "grad_norm": 0.22152648866176605,
      "learning_rate": 4.210666666666667e-05,
      "loss": 0.0022,
      "step": 5920
    },
    {
      "epoch": 0.31626666666666664,
      "grad_norm": 0.21892450749874115,
      "learning_rate": 4.209333333333333e-05,
      "loss": 0.0035,
      "step": 5930
    },
    {
      "epoch": 0.3168,
      "grad_norm": 0.42469680309295654,
      "learning_rate": 4.2080000000000004e-05,
      "loss": 0.0043,
      "step": 5940
    },
    {
      "epoch": 0.31733333333333336,
      "grad_norm": 0.4461604356765747,
      "learning_rate": 4.206666666666667e-05,
      "loss": 0.0022,
      "step": 5950
    },
    {
      "epoch": 0.3178666666666667,
      "grad_norm": 0.22479379177093506,
      "learning_rate": 4.205333333333333e-05,
      "loss": 0.0023,
      "step": 5960
    },
    {
      "epoch": 0.3184,
      "grad_norm": 0.26580414175987244,
      "learning_rate": 4.2040000000000004e-05,
      "loss": 0.0032,
      "step": 5970
    },
    {
      "epoch": 0.31893333333333335,
      "grad_norm": 0.11859721690416336,
      "learning_rate": 4.202666666666667e-05,
      "loss": 0.003,
      "step": 5980
    },
    {
      "epoch": 0.3194666666666667,
      "grad_norm": 0.6220294237136841,
      "learning_rate": 4.201333333333334e-05,
      "loss": 0.0024,
      "step": 5990
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.47416236996650696,
      "learning_rate": 4.2e-05,
      "loss": 0.003,
      "step": 6000
    },
    {
      "epoch": 0.32053333333333334,
      "grad_norm": 0.3619784414768219,
      "learning_rate": 4.198666666666667e-05,
      "loss": 0.0022,
      "step": 6010
    },
    {
      "epoch": 0.32106666666666667,
      "grad_norm": 0.5230055451393127,
      "learning_rate": 4.1973333333333334e-05,
      "loss": 0.0021,
      "step": 6020
    },
    {
      "epoch": 0.3216,
      "grad_norm": 0.42356255650520325,
      "learning_rate": 4.196e-05,
      "loss": 0.0022,
      "step": 6030
    },
    {
      "epoch": 0.3221333333333333,
      "grad_norm": 0.48824113607406616,
      "learning_rate": 4.194666666666667e-05,
      "loss": 0.003,
      "step": 6040
    },
    {
      "epoch": 0.32266666666666666,
      "grad_norm": 0.20257754623889923,
      "learning_rate": 4.1933333333333334e-05,
      "loss": 0.0037,
      "step": 6050
    },
    {
      "epoch": 0.3232,
      "grad_norm": 0.6659343838691711,
      "learning_rate": 4.1920000000000005e-05,
      "loss": 0.0027,
      "step": 6060
    },
    {
      "epoch": 0.3237333333333333,
      "grad_norm": 0.5025250315666199,
      "learning_rate": 4.190666666666666e-05,
      "loss": 0.0022,
      "step": 6070
    },
    {
      "epoch": 0.32426666666666665,
      "grad_norm": 0.1116553246974945,
      "learning_rate": 4.1893333333333334e-05,
      "loss": 0.0027,
      "step": 6080
    },
    {
      "epoch": 0.3248,
      "grad_norm": 0.09686045348644257,
      "learning_rate": 4.1880000000000006e-05,
      "loss": 0.0021,
      "step": 6090
    },
    {
      "epoch": 0.3253333333333333,
      "grad_norm": 0.10074801743030548,
      "learning_rate": 4.186666666666667e-05,
      "loss": 0.004,
      "step": 6100
    },
    {
      "epoch": 0.3258666666666667,
      "grad_norm": 0.3020952641963959,
      "learning_rate": 4.1853333333333335e-05,
      "loss": 0.0029,
      "step": 6110
    },
    {
      "epoch": 0.3264,
      "grad_norm": 0.490940123796463,
      "learning_rate": 4.184e-05,
      "loss": 0.0034,
      "step": 6120
    },
    {
      "epoch": 0.32693333333333335,
      "grad_norm": 0.22826999425888062,
      "learning_rate": 4.182666666666667e-05,
      "loss": 0.0036,
      "step": 6130
    },
    {
      "epoch": 0.3274666666666667,
      "grad_norm": 0.06367398798465729,
      "learning_rate": 4.1813333333333335e-05,
      "loss": 0.0039,
      "step": 6140
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.3900696039199829,
      "learning_rate": 4.18e-05,
      "loss": 0.0025,
      "step": 6150
    },
    {
      "epoch": 0.32853333333333334,
      "grad_norm": 0.23528346419334412,
      "learning_rate": 4.178666666666667e-05,
      "loss": 0.0033,
      "step": 6160
    },
    {
      "epoch": 0.3290666666666667,
      "grad_norm": 0.3524126708507538,
      "learning_rate": 4.1773333333333335e-05,
      "loss": 0.0023,
      "step": 6170
    },
    {
      "epoch": 0.3296,
      "grad_norm": 0.09260393679141998,
      "learning_rate": 4.176000000000001e-05,
      "loss": 0.0027,
      "step": 6180
    },
    {
      "epoch": 0.33013333333333333,
      "grad_norm": 0.20413953065872192,
      "learning_rate": 4.1746666666666665e-05,
      "loss": 0.0021,
      "step": 6190
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 0.21580243110656738,
      "learning_rate": 4.1733333333333336e-05,
      "loss": 0.0032,
      "step": 6200
    },
    {
      "epoch": 0.3312,
      "grad_norm": 0.1405300348997116,
      "learning_rate": 4.172e-05,
      "loss": 0.0026,
      "step": 6210
    },
    {
      "epoch": 0.3317333333333333,
      "grad_norm": 0.3305480182170868,
      "learning_rate": 4.1706666666666665e-05,
      "loss": 0.0037,
      "step": 6220
    },
    {
      "epoch": 0.33226666666666665,
      "grad_norm": 0.25629526376724243,
      "learning_rate": 4.1693333333333336e-05,
      "loss": 0.0019,
      "step": 6230
    },
    {
      "epoch": 0.3328,
      "grad_norm": 0.5910685062408447,
      "learning_rate": 4.168e-05,
      "loss": 0.0023,
      "step": 6240
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.256146639585495,
      "learning_rate": 4.166666666666667e-05,
      "loss": 0.0022,
      "step": 6250
    },
    {
      "epoch": 0.33386666666666664,
      "grad_norm": 0.10161667317152023,
      "learning_rate": 4.165333333333333e-05,
      "loss": 0.0032,
      "step": 6260
    },
    {
      "epoch": 0.3344,
      "grad_norm": 0.1858624368906021,
      "learning_rate": 4.164e-05,
      "loss": 0.0038,
      "step": 6270
    },
    {
      "epoch": 0.33493333333333336,
      "grad_norm": 0.19609081745147705,
      "learning_rate": 4.162666666666667e-05,
      "loss": 0.0027,
      "step": 6280
    },
    {
      "epoch": 0.3354666666666667,
      "grad_norm": 0.7037987112998962,
      "learning_rate": 4.161333333333334e-05,
      "loss": 0.0033,
      "step": 6290
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.2928731143474579,
      "learning_rate": 4.16e-05,
      "loss": 0.0021,
      "step": 6300
    },
    {
      "epoch": 0.33653333333333335,
      "grad_norm": 0.46818339824676514,
      "learning_rate": 4.1586666666666666e-05,
      "loss": 0.003,
      "step": 6310
    },
    {
      "epoch": 0.3370666666666667,
      "grad_norm": 0.08420223742723465,
      "learning_rate": 4.157333333333334e-05,
      "loss": 0.0023,
      "step": 6320
    },
    {
      "epoch": 0.3376,
      "grad_norm": 0.23453326523303986,
      "learning_rate": 4.156e-05,
      "loss": 0.0041,
      "step": 6330
    },
    {
      "epoch": 0.33813333333333334,
      "grad_norm": 0.18632753193378448,
      "learning_rate": 4.1546666666666666e-05,
      "loss": 0.0035,
      "step": 6340
    },
    {
      "epoch": 0.33866666666666667,
      "grad_norm": 0.07420630753040314,
      "learning_rate": 4.153333333333334e-05,
      "loss": 0.0026,
      "step": 6350
    },
    {
      "epoch": 0.3392,
      "grad_norm": 0.7942066192626953,
      "learning_rate": 4.152e-05,
      "loss": 0.0024,
      "step": 6360
    },
    {
      "epoch": 0.33973333333333333,
      "grad_norm": 0.910084068775177,
      "learning_rate": 4.150666666666667e-05,
      "loss": 0.0023,
      "step": 6370
    },
    {
      "epoch": 0.34026666666666666,
      "grad_norm": 0.6018054485321045,
      "learning_rate": 4.149333333333333e-05,
      "loss": 0.0031,
      "step": 6380
    },
    {
      "epoch": 0.3408,
      "grad_norm": 0.35417839884757996,
      "learning_rate": 4.148e-05,
      "loss": 0.0031,
      "step": 6390
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 0.31752774119377136,
      "learning_rate": 4.146666666666667e-05,
      "loss": 0.0038,
      "step": 6400
    },
    {
      "epoch": 0.34186666666666665,
      "grad_norm": 0.5110024809837341,
      "learning_rate": 4.145333333333333e-05,
      "loss": 0.0036,
      "step": 6410
    },
    {
      "epoch": 0.3424,
      "grad_norm": 0.2013237625360489,
      "learning_rate": 4.144e-05,
      "loss": 0.0032,
      "step": 6420
    },
    {
      "epoch": 0.3429333333333333,
      "grad_norm": 0.596092700958252,
      "learning_rate": 4.142666666666667e-05,
      "loss": 0.0037,
      "step": 6430
    },
    {
      "epoch": 0.34346666666666664,
      "grad_norm": 0.3746866285800934,
      "learning_rate": 4.141333333333334e-05,
      "loss": 0.003,
      "step": 6440
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.4200841784477234,
      "learning_rate": 4.14e-05,
      "loss": 0.0038,
      "step": 6450
    },
    {
      "epoch": 0.34453333333333336,
      "grad_norm": 0.07203634828329086,
      "learning_rate": 4.138666666666667e-05,
      "loss": 0.0027,
      "step": 6460
    },
    {
      "epoch": 0.3450666666666667,
      "grad_norm": 0.21512289345264435,
      "learning_rate": 4.137333333333334e-05,
      "loss": 0.0023,
      "step": 6470
    },
    {
      "epoch": 0.3456,
      "grad_norm": 0.41516318917274475,
      "learning_rate": 4.1360000000000004e-05,
      "loss": 0.0022,
      "step": 6480
    },
    {
      "epoch": 0.34613333333333335,
      "grad_norm": 0.14549264311790466,
      "learning_rate": 4.134666666666667e-05,
      "loss": 0.002,
      "step": 6490
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.40641289949417114,
      "learning_rate": 4.133333333333333e-05,
      "loss": 0.0022,
      "step": 6500
    },
    {
      "epoch": 0.3472,
      "grad_norm": 0.5264748334884644,
      "learning_rate": 4.1320000000000004e-05,
      "loss": 0.0031,
      "step": 6510
    },
    {
      "epoch": 0.34773333333333334,
      "grad_norm": 0.2960855960845947,
      "learning_rate": 4.130666666666667e-05,
      "loss": 0.0027,
      "step": 6520
    },
    {
      "epoch": 0.34826666666666667,
      "grad_norm": 0.3991789221763611,
      "learning_rate": 4.129333333333333e-05,
      "loss": 0.003,
      "step": 6530
    },
    {
      "epoch": 0.3488,
      "grad_norm": 0.1029893308877945,
      "learning_rate": 4.1280000000000005e-05,
      "loss": 0.0035,
      "step": 6540
    },
    {
      "epoch": 0.34933333333333333,
      "grad_norm": 0.8712034821510315,
      "learning_rate": 4.126666666666667e-05,
      "loss": 0.003,
      "step": 6550
    },
    {
      "epoch": 0.34986666666666666,
      "grad_norm": 0.09698367118835449,
      "learning_rate": 4.1253333333333334e-05,
      "loss": 0.0031,
      "step": 6560
    },
    {
      "epoch": 0.3504,
      "grad_norm": 0.43224334716796875,
      "learning_rate": 4.124e-05,
      "loss": 0.0025,
      "step": 6570
    },
    {
      "epoch": 0.3509333333333333,
      "grad_norm": 0.13232141733169556,
      "learning_rate": 4.122666666666667e-05,
      "loss": 0.0018,
      "step": 6580
    },
    {
      "epoch": 0.35146666666666665,
      "grad_norm": 0.2684444785118103,
      "learning_rate": 4.1213333333333334e-05,
      "loss": 0.002,
      "step": 6590
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.35162466764450073,
      "learning_rate": 4.12e-05,
      "loss": 0.0022,
      "step": 6600
    },
    {
      "epoch": 0.3525333333333333,
      "grad_norm": 0.12749119102954865,
      "learning_rate": 4.118666666666667e-05,
      "loss": 0.002,
      "step": 6610
    },
    {
      "epoch": 0.35306666666666664,
      "grad_norm": 0.2020660936832428,
      "learning_rate": 4.1173333333333334e-05,
      "loss": 0.002,
      "step": 6620
    },
    {
      "epoch": 0.3536,
      "grad_norm": 0.2228143960237503,
      "learning_rate": 4.1160000000000006e-05,
      "loss": 0.0045,
      "step": 6630
    },
    {
      "epoch": 0.35413333333333336,
      "grad_norm": 0.28274041414260864,
      "learning_rate": 4.1146666666666663e-05,
      "loss": 0.003,
      "step": 6640
    },
    {
      "epoch": 0.3546666666666667,
      "grad_norm": 0.36337438225746155,
      "learning_rate": 4.1133333333333335e-05,
      "loss": 0.0047,
      "step": 6650
    },
    {
      "epoch": 0.3552,
      "grad_norm": 0.3865000605583191,
      "learning_rate": 4.1120000000000006e-05,
      "loss": 0.0029,
      "step": 6660
    },
    {
      "epoch": 0.35573333333333335,
      "grad_norm": 0.4603765308856964,
      "learning_rate": 4.110666666666667e-05,
      "loss": 0.0021,
      "step": 6670
    },
    {
      "epoch": 0.3562666666666667,
      "grad_norm": 0.17348894476890564,
      "learning_rate": 4.1093333333333335e-05,
      "loss": 0.0019,
      "step": 6680
    },
    {
      "epoch": 0.3568,
      "grad_norm": 0.3922625780105591,
      "learning_rate": 4.108e-05,
      "loss": 0.003,
      "step": 6690
    },
    {
      "epoch": 0.35733333333333334,
      "grad_norm": 0.37691277265548706,
      "learning_rate": 4.106666666666667e-05,
      "loss": 0.0027,
      "step": 6700
    },
    {
      "epoch": 0.35786666666666667,
      "grad_norm": 0.1150948777794838,
      "learning_rate": 4.1053333333333336e-05,
      "loss": 0.0046,
      "step": 6710
    },
    {
      "epoch": 0.3584,
      "grad_norm": 0.23803077638149261,
      "learning_rate": 4.104e-05,
      "loss": 0.0027,
      "step": 6720
    },
    {
      "epoch": 0.3589333333333333,
      "grad_norm": 0.6022666692733765,
      "learning_rate": 4.102666666666667e-05,
      "loss": 0.0031,
      "step": 6730
    },
    {
      "epoch": 0.35946666666666666,
      "grad_norm": 0.3497900664806366,
      "learning_rate": 4.1013333333333336e-05,
      "loss": 0.0034,
      "step": 6740
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.2128411829471588,
      "learning_rate": 4.1e-05,
      "loss": 0.0025,
      "step": 6750
    },
    {
      "epoch": 0.3605333333333333,
      "grad_norm": 0.24754495918750763,
      "learning_rate": 4.0986666666666665e-05,
      "loss": 0.0045,
      "step": 6760
    },
    {
      "epoch": 0.36106666666666665,
      "grad_norm": 0.21736232936382294,
      "learning_rate": 4.0973333333333336e-05,
      "loss": 0.0032,
      "step": 6770
    },
    {
      "epoch": 0.3616,
      "grad_norm": 0.1793053150177002,
      "learning_rate": 4.096e-05,
      "loss": 0.0021,
      "step": 6780
    },
    {
      "epoch": 0.3621333333333333,
      "grad_norm": 0.5848383903503418,
      "learning_rate": 4.0946666666666665e-05,
      "loss": 0.0026,
      "step": 6790
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 0.4407918155193329,
      "learning_rate": 4.093333333333334e-05,
      "loss": 0.0026,
      "step": 6800
    },
    {
      "epoch": 0.3632,
      "grad_norm": 0.6734309196472168,
      "learning_rate": 4.092e-05,
      "loss": 0.0029,
      "step": 6810
    },
    {
      "epoch": 0.36373333333333335,
      "grad_norm": 0.47275158762931824,
      "learning_rate": 4.090666666666667e-05,
      "loss": 0.0029,
      "step": 6820
    },
    {
      "epoch": 0.3642666666666667,
      "grad_norm": 0.27931421995162964,
      "learning_rate": 4.089333333333333e-05,
      "loss": 0.0022,
      "step": 6830
    },
    {
      "epoch": 0.3648,
      "grad_norm": 0.09732045233249664,
      "learning_rate": 4.088e-05,
      "loss": 0.0019,
      "step": 6840
    },
    {
      "epoch": 0.36533333333333334,
      "grad_norm": 0.7709556818008423,
      "learning_rate": 4.086666666666667e-05,
      "loss": 0.0031,
      "step": 6850
    },
    {
      "epoch": 0.3658666666666667,
      "grad_norm": 0.38304418325424194,
      "learning_rate": 4.085333333333334e-05,
      "loss": 0.0024,
      "step": 6860
    },
    {
      "epoch": 0.3664,
      "grad_norm": 0.3067394495010376,
      "learning_rate": 4.084e-05,
      "loss": 0.0027,
      "step": 6870
    },
    {
      "epoch": 0.36693333333333333,
      "grad_norm": 0.3915926516056061,
      "learning_rate": 4.0826666666666667e-05,
      "loss": 0.0023,
      "step": 6880
    },
    {
      "epoch": 0.36746666666666666,
      "grad_norm": 0.37181511521339417,
      "learning_rate": 4.081333333333334e-05,
      "loss": 0.0025,
      "step": 6890
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.2563537061214447,
      "learning_rate": 4.08e-05,
      "loss": 0.0025,
      "step": 6900
    },
    {
      "epoch": 0.3685333333333333,
      "grad_norm": 0.5878335237503052,
      "learning_rate": 4.078666666666667e-05,
      "loss": 0.0023,
      "step": 6910
    },
    {
      "epoch": 0.36906666666666665,
      "grad_norm": 0.26316866278648376,
      "learning_rate": 4.077333333333334e-05,
      "loss": 0.0023,
      "step": 6920
    },
    {
      "epoch": 0.3696,
      "grad_norm": 0.46273842453956604,
      "learning_rate": 4.076e-05,
      "loss": 0.0023,
      "step": 6930
    },
    {
      "epoch": 0.3701333333333333,
      "grad_norm": 0.371180921792984,
      "learning_rate": 4.074666666666667e-05,
      "loss": 0.003,
      "step": 6940
    },
    {
      "epoch": 0.37066666666666664,
      "grad_norm": 0.4611414074897766,
      "learning_rate": 4.073333333333333e-05,
      "loss": 0.0023,
      "step": 6950
    },
    {
      "epoch": 0.3712,
      "grad_norm": 0.19029587507247925,
      "learning_rate": 4.072e-05,
      "loss": 0.0029,
      "step": 6960
    },
    {
      "epoch": 0.37173333333333336,
      "grad_norm": 0.30948638916015625,
      "learning_rate": 4.070666666666667e-05,
      "loss": 0.0027,
      "step": 6970
    },
    {
      "epoch": 0.3722666666666667,
      "grad_norm": 0.3877545893192291,
      "learning_rate": 4.069333333333333e-05,
      "loss": 0.003,
      "step": 6980
    },
    {
      "epoch": 0.3728,
      "grad_norm": 0.19688716530799866,
      "learning_rate": 4.0680000000000004e-05,
      "loss": 0.0038,
      "step": 6990
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.5593760013580322,
      "learning_rate": 4.066666666666667e-05,
      "loss": 0.003,
      "step": 7000
    },
    {
      "epoch": 0.3738666666666667,
      "grad_norm": 0.5824311971664429,
      "learning_rate": 4.065333333333334e-05,
      "loss": 0.0034,
      "step": 7010
    },
    {
      "epoch": 0.3744,
      "grad_norm": 0.19080840051174164,
      "learning_rate": 4.064e-05,
      "loss": 0.0023,
      "step": 7020
    },
    {
      "epoch": 0.37493333333333334,
      "grad_norm": 0.2460756152868271,
      "learning_rate": 4.062666666666667e-05,
      "loss": 0.002,
      "step": 7030
    },
    {
      "epoch": 0.37546666666666667,
      "grad_norm": 0.4138795733451843,
      "learning_rate": 4.061333333333334e-05,
      "loss": 0.0025,
      "step": 7040
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.5446323156356812,
      "learning_rate": 4.0600000000000004e-05,
      "loss": 0.0028,
      "step": 7050
    },
    {
      "epoch": 0.37653333333333333,
      "grad_norm": 0.9874396324157715,
      "learning_rate": 4.058666666666667e-05,
      "loss": 0.0021,
      "step": 7060
    },
    {
      "epoch": 0.37706666666666666,
      "grad_norm": 0.42980673909187317,
      "learning_rate": 4.057333333333333e-05,
      "loss": 0.0024,
      "step": 7070
    },
    {
      "epoch": 0.3776,
      "grad_norm": 0.24859663844108582,
      "learning_rate": 4.0560000000000005e-05,
      "loss": 0.002,
      "step": 7080
    },
    {
      "epoch": 0.3781333333333333,
      "grad_norm": 0.43788942694664,
      "learning_rate": 4.054666666666667e-05,
      "loss": 0.0035,
      "step": 7090
    },
    {
      "epoch": 0.37866666666666665,
      "grad_norm": 0.864018976688385,
      "learning_rate": 4.0533333333333334e-05,
      "loss": 0.004,
      "step": 7100
    },
    {
      "epoch": 0.3792,
      "grad_norm": 0.12275173515081406,
      "learning_rate": 4.0520000000000005e-05,
      "loss": 0.0023,
      "step": 7110
    },
    {
      "epoch": 0.3797333333333333,
      "grad_norm": 0.5498377084732056,
      "learning_rate": 4.050666666666667e-05,
      "loss": 0.0032,
      "step": 7120
    },
    {
      "epoch": 0.38026666666666664,
      "grad_norm": 0.4631511867046356,
      "learning_rate": 4.0493333333333334e-05,
      "loss": 0.0031,
      "step": 7130
    },
    {
      "epoch": 0.3808,
      "grad_norm": 0.5679969191551208,
      "learning_rate": 4.048e-05,
      "loss": 0.0033,
      "step": 7140
    },
    {
      "epoch": 0.38133333333333336,
      "grad_norm": 0.4491972029209137,
      "learning_rate": 4.046666666666667e-05,
      "loss": 0.0033,
      "step": 7150
    },
    {
      "epoch": 0.3818666666666667,
      "grad_norm": 0.34097760915756226,
      "learning_rate": 4.0453333333333335e-05,
      "loss": 0.0023,
      "step": 7160
    },
    {
      "epoch": 0.3824,
      "grad_norm": 0.5602749586105347,
      "learning_rate": 4.044e-05,
      "loss": 0.0032,
      "step": 7170
    },
    {
      "epoch": 0.38293333333333335,
      "grad_norm": 0.7894740104675293,
      "learning_rate": 4.042666666666667e-05,
      "loss": 0.0028,
      "step": 7180
    },
    {
      "epoch": 0.3834666666666667,
      "grad_norm": 0.5442187190055847,
      "learning_rate": 4.0413333333333335e-05,
      "loss": 0.0021,
      "step": 7190
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.169875368475914,
      "learning_rate": 4.0400000000000006e-05,
      "loss": 0.0022,
      "step": 7200
    },
    {
      "epoch": 0.38453333333333334,
      "grad_norm": 0.0992698147892952,
      "learning_rate": 4.0386666666666664e-05,
      "loss": 0.0022,
      "step": 7210
    },
    {
      "epoch": 0.38506666666666667,
      "grad_norm": 0.600662887096405,
      "learning_rate": 4.0373333333333335e-05,
      "loss": 0.0028,
      "step": 7220
    },
    {
      "epoch": 0.3856,
      "grad_norm": 0.13450419902801514,
      "learning_rate": 4.0360000000000007e-05,
      "loss": 0.0022,
      "step": 7230
    },
    {
      "epoch": 0.38613333333333333,
      "grad_norm": 0.2948133051395416,
      "learning_rate": 4.0346666666666664e-05,
      "loss": 0.0028,
      "step": 7240
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.08304567635059357,
      "learning_rate": 4.0333333333333336e-05,
      "loss": 0.0033,
      "step": 7250
    },
    {
      "epoch": 0.3872,
      "grad_norm": 0.544532835483551,
      "learning_rate": 4.032e-05,
      "loss": 0.0019,
      "step": 7260
    },
    {
      "epoch": 0.3877333333333333,
      "grad_norm": 0.1118975579738617,
      "learning_rate": 4.030666666666667e-05,
      "loss": 0.0023,
      "step": 7270
    },
    {
      "epoch": 0.38826666666666665,
      "grad_norm": 0.4510889947414398,
      "learning_rate": 4.0293333333333336e-05,
      "loss": 0.0026,
      "step": 7280
    },
    {
      "epoch": 0.3888,
      "grad_norm": 0.4718609154224396,
      "learning_rate": 4.028e-05,
      "loss": 0.0024,
      "step": 7290
    },
    {
      "epoch": 0.3893333333333333,
      "grad_norm": 0.2988685071468353,
      "learning_rate": 4.026666666666667e-05,
      "loss": 0.0024,
      "step": 7300
    },
    {
      "epoch": 0.38986666666666664,
      "grad_norm": 0.44554442167282104,
      "learning_rate": 4.0253333333333336e-05,
      "loss": 0.0024,
      "step": 7310
    },
    {
      "epoch": 0.3904,
      "grad_norm": 0.3387209475040436,
      "learning_rate": 4.024e-05,
      "loss": 0.0024,
      "step": 7320
    },
    {
      "epoch": 0.39093333333333335,
      "grad_norm": 0.5881633162498474,
      "learning_rate": 4.0226666666666666e-05,
      "loss": 0.0021,
      "step": 7330
    },
    {
      "epoch": 0.3914666666666667,
      "grad_norm": 0.28553807735443115,
      "learning_rate": 4.021333333333334e-05,
      "loss": 0.0025,
      "step": 7340
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.48097866773605347,
      "learning_rate": 4.02e-05,
      "loss": 0.0023,
      "step": 7350
    },
    {
      "epoch": 0.39253333333333335,
      "grad_norm": 0.5431700944900513,
      "learning_rate": 4.0186666666666666e-05,
      "loss": 0.0038,
      "step": 7360
    },
    {
      "epoch": 0.3930666666666667,
      "grad_norm": 0.11701083928346634,
      "learning_rate": 4.017333333333334e-05,
      "loss": 0.0031,
      "step": 7370
    },
    {
      "epoch": 0.3936,
      "grad_norm": 0.2857929468154907,
      "learning_rate": 4.016e-05,
      "loss": 0.003,
      "step": 7380
    },
    {
      "epoch": 0.39413333333333334,
      "grad_norm": 0.25352784991264343,
      "learning_rate": 4.014666666666667e-05,
      "loss": 0.0029,
      "step": 7390
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 0.14567430317401886,
      "learning_rate": 4.013333333333333e-05,
      "loss": 0.002,
      "step": 7400
    },
    {
      "epoch": 0.3952,
      "grad_norm": 0.35067516565322876,
      "learning_rate": 4.012e-05,
      "loss": 0.0043,
      "step": 7410
    },
    {
      "epoch": 0.3957333333333333,
      "grad_norm": 0.12321152538061142,
      "learning_rate": 4.0106666666666673e-05,
      "loss": 0.002,
      "step": 7420
    },
    {
      "epoch": 0.39626666666666666,
      "grad_norm": 0.15117143094539642,
      "learning_rate": 4.009333333333333e-05,
      "loss": 0.003,
      "step": 7430
    },
    {
      "epoch": 0.3968,
      "grad_norm": 0.19310560822486877,
      "learning_rate": 4.008e-05,
      "loss": 0.003,
      "step": 7440
    },
    {
      "epoch": 0.3973333333333333,
      "grad_norm": 0.5053331851959229,
      "learning_rate": 4.006666666666667e-05,
      "loss": 0.0029,
      "step": 7450
    },
    {
      "epoch": 0.39786666666666665,
      "grad_norm": 0.4187072515487671,
      "learning_rate": 4.005333333333334e-05,
      "loss": 0.0024,
      "step": 7460
    },
    {
      "epoch": 0.3984,
      "grad_norm": 0.09871521592140198,
      "learning_rate": 4.004e-05,
      "loss": 0.0024,
      "step": 7470
    },
    {
      "epoch": 0.3989333333333333,
      "grad_norm": 0.1365608423948288,
      "learning_rate": 4.002666666666667e-05,
      "loss": 0.0038,
      "step": 7480
    },
    {
      "epoch": 0.3994666666666667,
      "grad_norm": 0.16500231623649597,
      "learning_rate": 4.001333333333334e-05,
      "loss": 0.0019,
      "step": 7490
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.3783380687236786,
      "learning_rate": 4e-05,
      "loss": 0.0019,
      "step": 7500
    },
    {
      "epoch": 0.40053333333333335,
      "grad_norm": 0.4489130973815918,
      "learning_rate": 3.998666666666667e-05,
      "loss": 0.0027,
      "step": 7510
    },
    {
      "epoch": 0.4010666666666667,
      "grad_norm": 0.1021585762500763,
      "learning_rate": 3.997333333333333e-05,
      "loss": 0.003,
      "step": 7520
    },
    {
      "epoch": 0.4016,
      "grad_norm": 0.6106492877006531,
      "learning_rate": 3.9960000000000004e-05,
      "loss": 0.0038,
      "step": 7530
    },
    {
      "epoch": 0.40213333333333334,
      "grad_norm": 0.06825008243322372,
      "learning_rate": 3.994666666666667e-05,
      "loss": 0.0026,
      "step": 7540
    },
    {
      "epoch": 0.4026666666666667,
      "grad_norm": 0.6439187526702881,
      "learning_rate": 3.993333333333333e-05,
      "loss": 0.0025,
      "step": 7550
    },
    {
      "epoch": 0.4032,
      "grad_norm": 0.10851660370826721,
      "learning_rate": 3.9920000000000004e-05,
      "loss": 0.0028,
      "step": 7560
    },
    {
      "epoch": 0.40373333333333333,
      "grad_norm": 0.24537630379199982,
      "learning_rate": 3.990666666666667e-05,
      "loss": 0.0019,
      "step": 7570
    },
    {
      "epoch": 0.40426666666666666,
      "grad_norm": 0.4087104797363281,
      "learning_rate": 3.989333333333333e-05,
      "loss": 0.0022,
      "step": 7580
    },
    {
      "epoch": 0.4048,
      "grad_norm": 0.2925339341163635,
      "learning_rate": 3.988e-05,
      "loss": 0.0025,
      "step": 7590
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 0.17201896011829376,
      "learning_rate": 3.986666666666667e-05,
      "loss": 0.0023,
      "step": 7600
    },
    {
      "epoch": 0.40586666666666665,
      "grad_norm": 0.24004235863685608,
      "learning_rate": 3.985333333333334e-05,
      "loss": 0.0024,
      "step": 7610
    },
    {
      "epoch": 0.4064,
      "grad_norm": 0.28451740741729736,
      "learning_rate": 3.984e-05,
      "loss": 0.0029,
      "step": 7620
    },
    {
      "epoch": 0.4069333333333333,
      "grad_norm": 0.17168159782886505,
      "learning_rate": 3.982666666666667e-05,
      "loss": 0.0028,
      "step": 7630
    },
    {
      "epoch": 0.40746666666666664,
      "grad_norm": 0.09616396576166153,
      "learning_rate": 3.9813333333333334e-05,
      "loss": 0.0027,
      "step": 7640
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.3264288306236267,
      "learning_rate": 3.9800000000000005e-05,
      "loss": 0.0026,
      "step": 7650
    },
    {
      "epoch": 0.40853333333333336,
      "grad_norm": 0.5889774560928345,
      "learning_rate": 3.978666666666667e-05,
      "loss": 0.0023,
      "step": 7660
    },
    {
      "epoch": 0.4090666666666667,
      "grad_norm": 0.5009453296661377,
      "learning_rate": 3.9773333333333334e-05,
      "loss": 0.003,
      "step": 7670
    },
    {
      "epoch": 0.4096,
      "grad_norm": 0.14148232340812683,
      "learning_rate": 3.9760000000000006e-05,
      "loss": 0.0028,
      "step": 7680
    },
    {
      "epoch": 0.41013333333333335,
      "grad_norm": 0.3058108985424042,
      "learning_rate": 3.974666666666667e-05,
      "loss": 0.0034,
      "step": 7690
    },
    {
      "epoch": 0.4106666666666667,
      "grad_norm": 0.14564339816570282,
      "learning_rate": 3.9733333333333335e-05,
      "loss": 0.0022,
      "step": 7700
    },
    {
      "epoch": 0.4112,
      "grad_norm": 0.46489986777305603,
      "learning_rate": 3.972e-05,
      "loss": 0.0029,
      "step": 7710
    },
    {
      "epoch": 0.41173333333333334,
      "grad_norm": 0.44824638962745667,
      "learning_rate": 3.970666666666667e-05,
      "loss": 0.0027,
      "step": 7720
    },
    {
      "epoch": 0.41226666666666667,
      "grad_norm": 0.24716691672801971,
      "learning_rate": 3.9693333333333335e-05,
      "loss": 0.0025,
      "step": 7730
    },
    {
      "epoch": 0.4128,
      "grad_norm": 0.0912712961435318,
      "learning_rate": 3.968e-05,
      "loss": 0.0031,
      "step": 7740
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.7398284077644348,
      "learning_rate": 3.966666666666667e-05,
      "loss": 0.0029,
      "step": 7750
    },
    {
      "epoch": 0.41386666666666666,
      "grad_norm": 0.15564432740211487,
      "learning_rate": 3.9653333333333335e-05,
      "loss": 0.0028,
      "step": 7760
    },
    {
      "epoch": 0.4144,
      "grad_norm": 0.1745794713497162,
      "learning_rate": 3.964e-05,
      "loss": 0.0028,
      "step": 7770
    },
    {
      "epoch": 0.4149333333333333,
      "grad_norm": 0.1805032640695572,
      "learning_rate": 3.9626666666666664e-05,
      "loss": 0.0041,
      "step": 7780
    },
    {
      "epoch": 0.41546666666666665,
      "grad_norm": 0.2257077395915985,
      "learning_rate": 3.9613333333333336e-05,
      "loss": 0.0032,
      "step": 7790
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.25813743472099304,
      "learning_rate": 3.960000000000001e-05,
      "loss": 0.0028,
      "step": 7800
    },
    {
      "epoch": 0.4165333333333333,
      "grad_norm": 0.2524232864379883,
      "learning_rate": 3.9586666666666665e-05,
      "loss": 0.0038,
      "step": 7810
    },
    {
      "epoch": 0.41706666666666664,
      "grad_norm": 0.37910139560699463,
      "learning_rate": 3.9573333333333336e-05,
      "loss": 0.004,
      "step": 7820
    },
    {
      "epoch": 0.4176,
      "grad_norm": 0.19796840846538544,
      "learning_rate": 3.956e-05,
      "loss": 0.0024,
      "step": 7830
    },
    {
      "epoch": 0.41813333333333336,
      "grad_norm": 0.44560977816581726,
      "learning_rate": 3.954666666666667e-05,
      "loss": 0.0024,
      "step": 7840
    },
    {
      "epoch": 0.4186666666666667,
      "grad_norm": 0.18904966115951538,
      "learning_rate": 3.9533333333333337e-05,
      "loss": 0.0028,
      "step": 7850
    },
    {
      "epoch": 0.4192,
      "grad_norm": 0.26404550671577454,
      "learning_rate": 3.952e-05,
      "loss": 0.0024,
      "step": 7860
    },
    {
      "epoch": 0.41973333333333335,
      "grad_norm": 0.2856944501399994,
      "learning_rate": 3.950666666666667e-05,
      "loss": 0.0029,
      "step": 7870
    },
    {
      "epoch": 0.4202666666666667,
      "grad_norm": 0.20744489133358002,
      "learning_rate": 3.949333333333334e-05,
      "loss": 0.0022,
      "step": 7880
    },
    {
      "epoch": 0.4208,
      "grad_norm": 0.3981354236602783,
      "learning_rate": 3.948e-05,
      "loss": 0.0023,
      "step": 7890
    },
    {
      "epoch": 0.42133333333333334,
      "grad_norm": 0.1790446937084198,
      "learning_rate": 3.9466666666666666e-05,
      "loss": 0.0027,
      "step": 7900
    },
    {
      "epoch": 0.42186666666666667,
      "grad_norm": 0.3066331148147583,
      "learning_rate": 3.945333333333334e-05,
      "loss": 0.0034,
      "step": 7910
    },
    {
      "epoch": 0.4224,
      "grad_norm": 0.5099966526031494,
      "learning_rate": 3.944e-05,
      "loss": 0.0033,
      "step": 7920
    },
    {
      "epoch": 0.42293333333333333,
      "grad_norm": 0.2421167939901352,
      "learning_rate": 3.9426666666666666e-05,
      "loss": 0.0018,
      "step": 7930
    },
    {
      "epoch": 0.42346666666666666,
      "grad_norm": 0.28531908988952637,
      "learning_rate": 3.941333333333334e-05,
      "loss": 0.0023,
      "step": 7940
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.3225744962692261,
      "learning_rate": 3.94e-05,
      "loss": 0.0027,
      "step": 7950
    },
    {
      "epoch": 0.4245333333333333,
      "grad_norm": 0.1720322221517563,
      "learning_rate": 3.938666666666667e-05,
      "loss": 0.0024,
      "step": 7960
    },
    {
      "epoch": 0.42506666666666665,
      "grad_norm": 0.20486649870872498,
      "learning_rate": 3.937333333333333e-05,
      "loss": 0.0021,
      "step": 7970
    },
    {
      "epoch": 0.4256,
      "grad_norm": 0.6390509605407715,
      "learning_rate": 3.936e-05,
      "loss": 0.0029,
      "step": 7980
    },
    {
      "epoch": 0.4261333333333333,
      "grad_norm": 0.4864300489425659,
      "learning_rate": 3.9346666666666674e-05,
      "loss": 0.0024,
      "step": 7990
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.38214945793151855,
      "learning_rate": 3.933333333333333e-05,
      "loss": 0.0025,
      "step": 8000
    },
    {
      "epoch": 0.4272,
      "grad_norm": 0.23981863260269165,
      "learning_rate": 3.932e-05,
      "loss": 0.0021,
      "step": 8010
    },
    {
      "epoch": 0.42773333333333335,
      "grad_norm": 0.62296462059021,
      "learning_rate": 3.930666666666667e-05,
      "loss": 0.003,
      "step": 8020
    },
    {
      "epoch": 0.4282666666666667,
      "grad_norm": 0.30414286255836487,
      "learning_rate": 3.929333333333334e-05,
      "loss": 0.0036,
      "step": 8030
    },
    {
      "epoch": 0.4288,
      "grad_norm": 0.21600157022476196,
      "learning_rate": 3.9280000000000003e-05,
      "loss": 0.0023,
      "step": 8040
    },
    {
      "epoch": 0.42933333333333334,
      "grad_norm": 0.5486615300178528,
      "learning_rate": 3.926666666666667e-05,
      "loss": 0.0023,
      "step": 8050
    },
    {
      "epoch": 0.4298666666666667,
      "grad_norm": 0.08110027015209198,
      "learning_rate": 3.925333333333334e-05,
      "loss": 0.0026,
      "step": 8060
    },
    {
      "epoch": 0.4304,
      "grad_norm": 0.47558286786079407,
      "learning_rate": 3.9240000000000004e-05,
      "loss": 0.0033,
      "step": 8070
    },
    {
      "epoch": 0.43093333333333333,
      "grad_norm": 0.7949070334434509,
      "learning_rate": 3.922666666666667e-05,
      "loss": 0.0023,
      "step": 8080
    },
    {
      "epoch": 0.43146666666666667,
      "grad_norm": 0.41689035296440125,
      "learning_rate": 3.921333333333333e-05,
      "loss": 0.002,
      "step": 8090
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.12023691833019257,
      "learning_rate": 3.9200000000000004e-05,
      "loss": 0.0026,
      "step": 8100
    },
    {
      "epoch": 0.4325333333333333,
      "grad_norm": 0.4468745291233063,
      "learning_rate": 3.918666666666667e-05,
      "loss": 0.0041,
      "step": 8110
    },
    {
      "epoch": 0.43306666666666666,
      "grad_norm": 0.23679788410663605,
      "learning_rate": 3.917333333333333e-05,
      "loss": 0.0037,
      "step": 8120
    },
    {
      "epoch": 0.4336,
      "grad_norm": 0.13488461077213287,
      "learning_rate": 3.9160000000000005e-05,
      "loss": 0.0039,
      "step": 8130
    },
    {
      "epoch": 0.4341333333333333,
      "grad_norm": 0.5173206925392151,
      "learning_rate": 3.914666666666667e-05,
      "loss": 0.0026,
      "step": 8140
    },
    {
      "epoch": 0.43466666666666665,
      "grad_norm": 0.11558976769447327,
      "learning_rate": 3.9133333333333334e-05,
      "loss": 0.0029,
      "step": 8150
    },
    {
      "epoch": 0.4352,
      "grad_norm": 0.36144155263900757,
      "learning_rate": 3.912e-05,
      "loss": 0.0024,
      "step": 8160
    },
    {
      "epoch": 0.4357333333333333,
      "grad_norm": 0.15759794414043427,
      "learning_rate": 3.910666666666667e-05,
      "loss": 0.0033,
      "step": 8170
    },
    {
      "epoch": 0.4362666666666667,
      "grad_norm": 0.1306220442056656,
      "learning_rate": 3.9093333333333334e-05,
      "loss": 0.003,
      "step": 8180
    },
    {
      "epoch": 0.4368,
      "grad_norm": 0.5487171411514282,
      "learning_rate": 3.908e-05,
      "loss": 0.003,
      "step": 8190
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 0.1589869111776352,
      "learning_rate": 3.906666666666667e-05,
      "loss": 0.0027,
      "step": 8200
    },
    {
      "epoch": 0.4378666666666667,
      "grad_norm": 0.6763954758644104,
      "learning_rate": 3.9053333333333334e-05,
      "loss": 0.0024,
      "step": 8210
    },
    {
      "epoch": 0.4384,
      "grad_norm": 0.28235697746276855,
      "learning_rate": 3.9040000000000006e-05,
      "loss": 0.0029,
      "step": 8220
    },
    {
      "epoch": 0.43893333333333334,
      "grad_norm": 0.23613859713077545,
      "learning_rate": 3.902666666666667e-05,
      "loss": 0.0029,
      "step": 8230
    },
    {
      "epoch": 0.43946666666666667,
      "grad_norm": 0.6728312969207764,
      "learning_rate": 3.9013333333333335e-05,
      "loss": 0.0031,
      "step": 8240
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.30048274993896484,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.0026,
      "step": 8250
    },
    {
      "epoch": 0.44053333333333333,
      "grad_norm": 0.5834197402000427,
      "learning_rate": 3.8986666666666664e-05,
      "loss": 0.002,
      "step": 8260
    },
    {
      "epoch": 0.44106666666666666,
      "grad_norm": 0.633976936340332,
      "learning_rate": 3.8973333333333335e-05,
      "loss": 0.0024,
      "step": 8270
    },
    {
      "epoch": 0.4416,
      "grad_norm": 0.3923409581184387,
      "learning_rate": 3.896e-05,
      "loss": 0.0034,
      "step": 8280
    },
    {
      "epoch": 0.4421333333333333,
      "grad_norm": 0.1354176253080368,
      "learning_rate": 3.894666666666667e-05,
      "loss": 0.0034,
      "step": 8290
    },
    {
      "epoch": 0.44266666666666665,
      "grad_norm": 0.4796885848045349,
      "learning_rate": 3.8933333333333336e-05,
      "loss": 0.0035,
      "step": 8300
    },
    {
      "epoch": 0.4432,
      "grad_norm": 0.16094031929969788,
      "learning_rate": 3.892e-05,
      "loss": 0.0022,
      "step": 8310
    },
    {
      "epoch": 0.4437333333333333,
      "grad_norm": 0.19609782099723816,
      "learning_rate": 3.890666666666667e-05,
      "loss": 0.0024,
      "step": 8320
    },
    {
      "epoch": 0.44426666666666664,
      "grad_norm": 0.23842571675777435,
      "learning_rate": 3.8893333333333336e-05,
      "loss": 0.0032,
      "step": 8330
    },
    {
      "epoch": 0.4448,
      "grad_norm": 0.4233369529247284,
      "learning_rate": 3.888e-05,
      "loss": 0.0024,
      "step": 8340
    },
    {
      "epoch": 0.44533333333333336,
      "grad_norm": 0.23070022463798523,
      "learning_rate": 3.8866666666666665e-05,
      "loss": 0.0026,
      "step": 8350
    },
    {
      "epoch": 0.4458666666666667,
      "grad_norm": 0.36755621433258057,
      "learning_rate": 3.8853333333333336e-05,
      "loss": 0.0029,
      "step": 8360
    },
    {
      "epoch": 0.4464,
      "grad_norm": 0.21189996600151062,
      "learning_rate": 3.884e-05,
      "loss": 0.0035,
      "step": 8370
    },
    {
      "epoch": 0.44693333333333335,
      "grad_norm": 0.4511590898036957,
      "learning_rate": 3.8826666666666665e-05,
      "loss": 0.0022,
      "step": 8380
    },
    {
      "epoch": 0.4474666666666667,
      "grad_norm": 0.5516368746757507,
      "learning_rate": 3.881333333333334e-05,
      "loss": 0.0021,
      "step": 8390
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.12238024175167084,
      "learning_rate": 3.88e-05,
      "loss": 0.0034,
      "step": 8400
    },
    {
      "epoch": 0.44853333333333334,
      "grad_norm": 0.24884341657161713,
      "learning_rate": 3.878666666666667e-05,
      "loss": 0.002,
      "step": 8410
    },
    {
      "epoch": 0.44906666666666667,
      "grad_norm": 0.21987099945545197,
      "learning_rate": 3.877333333333334e-05,
      "loss": 0.0024,
      "step": 8420
    },
    {
      "epoch": 0.4496,
      "grad_norm": 0.4067856967449188,
      "learning_rate": 3.876e-05,
      "loss": 0.0028,
      "step": 8430
    },
    {
      "epoch": 0.45013333333333333,
      "grad_norm": 0.1205342561006546,
      "learning_rate": 3.874666666666667e-05,
      "loss": 0.0025,
      "step": 8440
    },
    {
      "epoch": 0.45066666666666666,
      "grad_norm": 0.8725754618644714,
      "learning_rate": 3.873333333333333e-05,
      "loss": 0.0023,
      "step": 8450
    },
    {
      "epoch": 0.4512,
      "grad_norm": 0.8766317367553711,
      "learning_rate": 3.872e-05,
      "loss": 0.003,
      "step": 8460
    },
    {
      "epoch": 0.4517333333333333,
      "grad_norm": 0.306622713804245,
      "learning_rate": 3.8706666666666667e-05,
      "loss": 0.0024,
      "step": 8470
    },
    {
      "epoch": 0.45226666666666665,
      "grad_norm": 0.16547049582004547,
      "learning_rate": 3.869333333333334e-05,
      "loss": 0.003,
      "step": 8480
    },
    {
      "epoch": 0.4528,
      "grad_norm": 0.23204633593559265,
      "learning_rate": 3.868e-05,
      "loss": 0.0035,
      "step": 8490
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.3426249027252197,
      "learning_rate": 3.866666666666667e-05,
      "loss": 0.0023,
      "step": 8500
    },
    {
      "epoch": 0.45386666666666664,
      "grad_norm": 0.2919008433818817,
      "learning_rate": 3.865333333333334e-05,
      "loss": 0.0036,
      "step": 8510
    },
    {
      "epoch": 0.4544,
      "grad_norm": 0.4962993562221527,
      "learning_rate": 3.864e-05,
      "loss": 0.0021,
      "step": 8520
    },
    {
      "epoch": 0.45493333333333336,
      "grad_norm": 0.34833982586860657,
      "learning_rate": 3.862666666666667e-05,
      "loss": 0.0041,
      "step": 8530
    },
    {
      "epoch": 0.4554666666666667,
      "grad_norm": 0.16710089147090912,
      "learning_rate": 3.861333333333333e-05,
      "loss": 0.0024,
      "step": 8540
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.10542384535074234,
      "learning_rate": 3.86e-05,
      "loss": 0.002,
      "step": 8550
    },
    {
      "epoch": 0.45653333333333335,
      "grad_norm": 0.380084365606308,
      "learning_rate": 3.858666666666667e-05,
      "loss": 0.003,
      "step": 8560
    },
    {
      "epoch": 0.4570666666666667,
      "grad_norm": 0.13647472858428955,
      "learning_rate": 3.857333333333333e-05,
      "loss": 0.0023,
      "step": 8570
    },
    {
      "epoch": 0.4576,
      "grad_norm": 0.4034552276134491,
      "learning_rate": 3.8560000000000004e-05,
      "loss": 0.0022,
      "step": 8580
    },
    {
      "epoch": 0.45813333333333334,
      "grad_norm": 0.4178909659385681,
      "learning_rate": 3.854666666666667e-05,
      "loss": 0.0023,
      "step": 8590
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 0.4552702307701111,
      "learning_rate": 3.853333333333334e-05,
      "loss": 0.0028,
      "step": 8600
    },
    {
      "epoch": 0.4592,
      "grad_norm": 0.40324369072914124,
      "learning_rate": 3.8520000000000004e-05,
      "loss": 0.0025,
      "step": 8610
    },
    {
      "epoch": 0.4597333333333333,
      "grad_norm": 0.3297029435634613,
      "learning_rate": 3.850666666666667e-05,
      "loss": 0.0033,
      "step": 8620
    },
    {
      "epoch": 0.46026666666666666,
      "grad_norm": 0.7648253440856934,
      "learning_rate": 3.849333333333334e-05,
      "loss": 0.0024,
      "step": 8630
    },
    {
      "epoch": 0.4608,
      "grad_norm": 0.2769050598144531,
      "learning_rate": 3.848e-05,
      "loss": 0.003,
      "step": 8640
    },
    {
      "epoch": 0.4613333333333333,
      "grad_norm": 0.18597611784934998,
      "learning_rate": 3.846666666666667e-05,
      "loss": 0.0027,
      "step": 8650
    },
    {
      "epoch": 0.46186666666666665,
      "grad_norm": 0.1571858823299408,
      "learning_rate": 3.845333333333333e-05,
      "loss": 0.0029,
      "step": 8660
    },
    {
      "epoch": 0.4624,
      "grad_norm": 0.13862395286560059,
      "learning_rate": 3.8440000000000005e-05,
      "loss": 0.0029,
      "step": 8670
    },
    {
      "epoch": 0.4629333333333333,
      "grad_norm": 0.3056521713733673,
      "learning_rate": 3.842666666666667e-05,
      "loss": 0.0035,
      "step": 8680
    },
    {
      "epoch": 0.4634666666666667,
      "grad_norm": 0.16888591647148132,
      "learning_rate": 3.8413333333333334e-05,
      "loss": 0.0021,
      "step": 8690
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.6411114931106567,
      "learning_rate": 3.8400000000000005e-05,
      "loss": 0.0041,
      "step": 8700
    },
    {
      "epoch": 0.46453333333333335,
      "grad_norm": 0.18665479123592377,
      "learning_rate": 3.838666666666667e-05,
      "loss": 0.0033,
      "step": 8710
    },
    {
      "epoch": 0.4650666666666667,
      "grad_norm": 0.15088631212711334,
      "learning_rate": 3.8373333333333334e-05,
      "loss": 0.0023,
      "step": 8720
    },
    {
      "epoch": 0.4656,
      "grad_norm": 0.4871250092983246,
      "learning_rate": 3.836e-05,
      "loss": 0.0023,
      "step": 8730
    },
    {
      "epoch": 0.46613333333333334,
      "grad_norm": 0.2545575797557831,
      "learning_rate": 3.834666666666667e-05,
      "loss": 0.0027,
      "step": 8740
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.11326391994953156,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 0.0023,
      "step": 8750
    },
    {
      "epoch": 0.4672,
      "grad_norm": 0.341232031583786,
      "learning_rate": 3.832e-05,
      "loss": 0.0023,
      "step": 8760
    },
    {
      "epoch": 0.46773333333333333,
      "grad_norm": 0.23834587633609772,
      "learning_rate": 3.830666666666667e-05,
      "loss": 0.0022,
      "step": 8770
    },
    {
      "epoch": 0.46826666666666666,
      "grad_norm": 0.45193788409233093,
      "learning_rate": 3.8293333333333335e-05,
      "loss": 0.0036,
      "step": 8780
    },
    {
      "epoch": 0.4688,
      "grad_norm": 0.28011447191238403,
      "learning_rate": 3.828e-05,
      "loss": 0.0034,
      "step": 8790
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 0.2542118430137634,
      "learning_rate": 3.8266666666666664e-05,
      "loss": 0.0021,
      "step": 8800
    },
    {
      "epoch": 0.46986666666666665,
      "grad_norm": 0.4026658236980438,
      "learning_rate": 3.8253333333333335e-05,
      "loss": 0.0032,
      "step": 8810
    },
    {
      "epoch": 0.4704,
      "grad_norm": 0.6226420998573303,
      "learning_rate": 3.8240000000000007e-05,
      "loss": 0.002,
      "step": 8820
    },
    {
      "epoch": 0.4709333333333333,
      "grad_norm": 0.1951167732477188,
      "learning_rate": 3.8226666666666664e-05,
      "loss": 0.0029,
      "step": 8830
    },
    {
      "epoch": 0.47146666666666665,
      "grad_norm": 0.7235798835754395,
      "learning_rate": 3.8213333333333336e-05,
      "loss": 0.0029,
      "step": 8840
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.6103388667106628,
      "learning_rate": 3.82e-05,
      "loss": 0.0021,
      "step": 8850
    },
    {
      "epoch": 0.47253333333333336,
      "grad_norm": 0.23375900089740753,
      "learning_rate": 3.818666666666667e-05,
      "loss": 0.0031,
      "step": 8860
    },
    {
      "epoch": 0.4730666666666667,
      "grad_norm": 0.24765728414058685,
      "learning_rate": 3.8173333333333336e-05,
      "loss": 0.0029,
      "step": 8870
    },
    {
      "epoch": 0.4736,
      "grad_norm": 0.10058863461017609,
      "learning_rate": 3.816e-05,
      "loss": 0.0019,
      "step": 8880
    },
    {
      "epoch": 0.47413333333333335,
      "grad_norm": 0.22468949854373932,
      "learning_rate": 3.814666666666667e-05,
      "loss": 0.0023,
      "step": 8890
    },
    {
      "epoch": 0.4746666666666667,
      "grad_norm": 0.5616377592086792,
      "learning_rate": 3.8133333333333336e-05,
      "loss": 0.0023,
      "step": 8900
    },
    {
      "epoch": 0.4752,
      "grad_norm": 0.16778865456581116,
      "learning_rate": 3.812e-05,
      "loss": 0.0027,
      "step": 8910
    },
    {
      "epoch": 0.47573333333333334,
      "grad_norm": 0.2892124354839325,
      "learning_rate": 3.8106666666666665e-05,
      "loss": 0.0032,
      "step": 8920
    },
    {
      "epoch": 0.47626666666666667,
      "grad_norm": 0.4003118872642517,
      "learning_rate": 3.809333333333334e-05,
      "loss": 0.0033,
      "step": 8930
    },
    {
      "epoch": 0.4768,
      "grad_norm": 0.3688106834888458,
      "learning_rate": 3.808e-05,
      "loss": 0.004,
      "step": 8940
    },
    {
      "epoch": 0.47733333333333333,
      "grad_norm": 0.37555187940597534,
      "learning_rate": 3.8066666666666666e-05,
      "loss": 0.0038,
      "step": 8950
    },
    {
      "epoch": 0.47786666666666666,
      "grad_norm": 0.22453032433986664,
      "learning_rate": 3.805333333333334e-05,
      "loss": 0.0018,
      "step": 8960
    },
    {
      "epoch": 0.4784,
      "grad_norm": 0.8232836723327637,
      "learning_rate": 3.804e-05,
      "loss": 0.0024,
      "step": 8970
    },
    {
      "epoch": 0.4789333333333333,
      "grad_norm": 0.16531600058078766,
      "learning_rate": 3.8026666666666666e-05,
      "loss": 0.0027,
      "step": 8980
    },
    {
      "epoch": 0.47946666666666665,
      "grad_norm": 0.1357254534959793,
      "learning_rate": 3.801333333333333e-05,
      "loss": 0.0034,
      "step": 8990
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.15500423312187195,
      "learning_rate": 3.8e-05,
      "loss": 0.0026,
      "step": 9000
    },
    {
      "epoch": 0.4805333333333333,
      "grad_norm": 0.8284280300140381,
      "learning_rate": 3.7986666666666673e-05,
      "loss": 0.002,
      "step": 9010
    },
    {
      "epoch": 0.48106666666666664,
      "grad_norm": 0.21487371623516083,
      "learning_rate": 3.797333333333333e-05,
      "loss": 0.0029,
      "step": 9020
    },
    {
      "epoch": 0.4816,
      "grad_norm": 0.11220600455999374,
      "learning_rate": 3.796e-05,
      "loss": 0.0027,
      "step": 9030
    },
    {
      "epoch": 0.48213333333333336,
      "grad_norm": 0.2575434744358063,
      "learning_rate": 3.794666666666667e-05,
      "loss": 0.0021,
      "step": 9040
    },
    {
      "epoch": 0.4826666666666667,
      "grad_norm": 0.44764044880867004,
      "learning_rate": 3.793333333333334e-05,
      "loss": 0.0029,
      "step": 9050
    },
    {
      "epoch": 0.4832,
      "grad_norm": 0.3148850202560425,
      "learning_rate": 3.792e-05,
      "loss": 0.0028,
      "step": 9060
    },
    {
      "epoch": 0.48373333333333335,
      "grad_norm": 0.5183342099189758,
      "learning_rate": 3.790666666666667e-05,
      "loss": 0.0033,
      "step": 9070
    },
    {
      "epoch": 0.4842666666666667,
      "grad_norm": 0.4141624867916107,
      "learning_rate": 3.789333333333334e-05,
      "loss": 0.004,
      "step": 9080
    },
    {
      "epoch": 0.4848,
      "grad_norm": 0.35589683055877686,
      "learning_rate": 3.788e-05,
      "loss": 0.0024,
      "step": 9090
    },
    {
      "epoch": 0.48533333333333334,
      "grad_norm": 0.7087244391441345,
      "learning_rate": 3.786666666666667e-05,
      "loss": 0.0033,
      "step": 9100
    },
    {
      "epoch": 0.48586666666666667,
      "grad_norm": 0.4908168911933899,
      "learning_rate": 3.785333333333333e-05,
      "loss": 0.0021,
      "step": 9110
    },
    {
      "epoch": 0.4864,
      "grad_norm": 0.4951014816761017,
      "learning_rate": 3.7840000000000004e-05,
      "loss": 0.0032,
      "step": 9120
    },
    {
      "epoch": 0.48693333333333333,
      "grad_norm": 0.15145917236804962,
      "learning_rate": 3.782666666666667e-05,
      "loss": 0.0045,
      "step": 9130
    },
    {
      "epoch": 0.48746666666666666,
      "grad_norm": 0.14598748087882996,
      "learning_rate": 3.781333333333333e-05,
      "loss": 0.0026,
      "step": 9140
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.28571218252182007,
      "learning_rate": 3.7800000000000004e-05,
      "loss": 0.0022,
      "step": 9150
    },
    {
      "epoch": 0.4885333333333333,
      "grad_norm": 0.5320067405700684,
      "learning_rate": 3.778666666666667e-05,
      "loss": 0.0022,
      "step": 9160
    },
    {
      "epoch": 0.48906666666666665,
      "grad_norm": 0.19818930327892303,
      "learning_rate": 3.777333333333333e-05,
      "loss": 0.0025,
      "step": 9170
    },
    {
      "epoch": 0.4896,
      "grad_norm": 0.3753620982170105,
      "learning_rate": 3.776e-05,
      "loss": 0.0022,
      "step": 9180
    },
    {
      "epoch": 0.4901333333333333,
      "grad_norm": 0.09696920961141586,
      "learning_rate": 3.774666666666667e-05,
      "loss": 0.0023,
      "step": 9190
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 0.25863882899284363,
      "learning_rate": 3.773333333333334e-05,
      "loss": 0.0033,
      "step": 9200
    },
    {
      "epoch": 0.4912,
      "grad_norm": 0.22268138825893402,
      "learning_rate": 3.772e-05,
      "loss": 0.0023,
      "step": 9210
    },
    {
      "epoch": 0.49173333333333336,
      "grad_norm": 0.26530325412750244,
      "learning_rate": 3.770666666666667e-05,
      "loss": 0.0025,
      "step": 9220
    },
    {
      "epoch": 0.4922666666666667,
      "grad_norm": 0.2552550733089447,
      "learning_rate": 3.7693333333333334e-05,
      "loss": 0.0035,
      "step": 9230
    },
    {
      "epoch": 0.4928,
      "grad_norm": 0.2126893550157547,
      "learning_rate": 3.7680000000000005e-05,
      "loss": 0.0029,
      "step": 9240
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.29403573274612427,
      "learning_rate": 3.766666666666667e-05,
      "loss": 0.0022,
      "step": 9250
    },
    {
      "epoch": 0.4938666666666667,
      "grad_norm": 0.9949982166290283,
      "learning_rate": 3.7653333333333334e-05,
      "loss": 0.0024,
      "step": 9260
    },
    {
      "epoch": 0.4944,
      "grad_norm": 0.309640109539032,
      "learning_rate": 3.7640000000000006e-05,
      "loss": 0.0021,
      "step": 9270
    },
    {
      "epoch": 0.49493333333333334,
      "grad_norm": 0.3545122444629669,
      "learning_rate": 3.762666666666667e-05,
      "loss": 0.0023,
      "step": 9280
    },
    {
      "epoch": 0.49546666666666667,
      "grad_norm": 0.30994677543640137,
      "learning_rate": 3.7613333333333335e-05,
      "loss": 0.0022,
      "step": 9290
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.5376607179641724,
      "learning_rate": 3.76e-05,
      "loss": 0.0031,
      "step": 9300
    },
    {
      "epoch": 0.4965333333333333,
      "grad_norm": 0.43421080708503723,
      "learning_rate": 3.758666666666667e-05,
      "loss": 0.0025,
      "step": 9310
    },
    {
      "epoch": 0.49706666666666666,
      "grad_norm": 0.4071543514728546,
      "learning_rate": 3.7573333333333335e-05,
      "loss": 0.0048,
      "step": 9320
    },
    {
      "epoch": 0.4976,
      "grad_norm": 0.600843071937561,
      "learning_rate": 3.756e-05,
      "loss": 0.0025,
      "step": 9330
    },
    {
      "epoch": 0.4981333333333333,
      "grad_norm": 0.3318006694316864,
      "learning_rate": 3.754666666666667e-05,
      "loss": 0.003,
      "step": 9340
    },
    {
      "epoch": 0.49866666666666665,
      "grad_norm": 0.33178770542144775,
      "learning_rate": 3.7533333333333335e-05,
      "loss": 0.0021,
      "step": 9350
    },
    {
      "epoch": 0.4992,
      "grad_norm": 0.1510465294122696,
      "learning_rate": 3.752e-05,
      "loss": 0.0024,
      "step": 9360
    },
    {
      "epoch": 0.4997333333333333,
      "grad_norm": 0.5821176171302795,
      "learning_rate": 3.7506666666666664e-05,
      "loss": 0.0035,
      "step": 9370
    },
    {
      "epoch": 0.5002666666666666,
      "grad_norm": 0.663971483707428,
      "learning_rate": 3.7493333333333336e-05,
      "loss": 0.003,
      "step": 9380
    },
    {
      "epoch": 0.5008,
      "grad_norm": 0.7372192144393921,
      "learning_rate": 3.748000000000001e-05,
      "loss": 0.0027,
      "step": 9390
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 0.39201924204826355,
      "learning_rate": 3.7466666666666665e-05,
      "loss": 0.0026,
      "step": 9400
    },
    {
      "epoch": 0.5018666666666667,
      "grad_norm": 0.3878710865974426,
      "learning_rate": 3.7453333333333336e-05,
      "loss": 0.0024,
      "step": 9410
    },
    {
      "epoch": 0.5024,
      "grad_norm": 0.5199595093727112,
      "learning_rate": 3.744e-05,
      "loss": 0.0029,
      "step": 9420
    },
    {
      "epoch": 0.5029333333333333,
      "grad_norm": 0.45371150970458984,
      "learning_rate": 3.742666666666667e-05,
      "loss": 0.0023,
      "step": 9430
    },
    {
      "epoch": 0.5034666666666666,
      "grad_norm": 0.2464635968208313,
      "learning_rate": 3.7413333333333337e-05,
      "loss": 0.0024,
      "step": 9440
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.6509961485862732,
      "learning_rate": 3.74e-05,
      "loss": 0.0021,
      "step": 9450
    },
    {
      "epoch": 0.5045333333333333,
      "grad_norm": 0.3109350800514221,
      "learning_rate": 3.738666666666667e-05,
      "loss": 0.0022,
      "step": 9460
    },
    {
      "epoch": 0.5050666666666667,
      "grad_norm": 0.27759572863578796,
      "learning_rate": 3.737333333333333e-05,
      "loss": 0.0032,
      "step": 9470
    },
    {
      "epoch": 0.5056,
      "grad_norm": 0.20020028948783875,
      "learning_rate": 3.736e-05,
      "loss": 0.002,
      "step": 9480
    },
    {
      "epoch": 0.5061333333333333,
      "grad_norm": 0.13656409084796906,
      "learning_rate": 3.7346666666666666e-05,
      "loss": 0.0018,
      "step": 9490
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.4304544925689697,
      "learning_rate": 3.733333333333334e-05,
      "loss": 0.002,
      "step": 9500
    },
    {
      "epoch": 0.5072,
      "grad_norm": 0.3682151138782501,
      "learning_rate": 3.732e-05,
      "loss": 0.0027,
      "step": 9510
    },
    {
      "epoch": 0.5077333333333334,
      "grad_norm": 0.1104239821434021,
      "learning_rate": 3.7306666666666666e-05,
      "loss": 0.0022,
      "step": 9520
    },
    {
      "epoch": 0.5082666666666666,
      "grad_norm": 0.3169640302658081,
      "learning_rate": 3.729333333333334e-05,
      "loss": 0.0032,
      "step": 9530
    },
    {
      "epoch": 0.5088,
      "grad_norm": 0.6282261610031128,
      "learning_rate": 3.728e-05,
      "loss": 0.0025,
      "step": 9540
    },
    {
      "epoch": 0.5093333333333333,
      "grad_norm": 0.4023672640323639,
      "learning_rate": 3.726666666666667e-05,
      "loss": 0.0028,
      "step": 9550
    },
    {
      "epoch": 0.5098666666666667,
      "grad_norm": 0.10112731903791428,
      "learning_rate": 3.725333333333333e-05,
      "loss": 0.0021,
      "step": 9560
    },
    {
      "epoch": 0.5104,
      "grad_norm": 0.21414080262184143,
      "learning_rate": 3.724e-05,
      "loss": 0.0026,
      "step": 9570
    },
    {
      "epoch": 0.5109333333333334,
      "grad_norm": 0.6386736035346985,
      "learning_rate": 3.7226666666666674e-05,
      "loss": 0.0026,
      "step": 9580
    },
    {
      "epoch": 0.5114666666666666,
      "grad_norm": 0.4913827180862427,
      "learning_rate": 3.721333333333333e-05,
      "loss": 0.0028,
      "step": 9590
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.32758426666259766,
      "learning_rate": 3.72e-05,
      "loss": 0.0029,
      "step": 9600
    },
    {
      "epoch": 0.5125333333333333,
      "grad_norm": 0.5349113345146179,
      "learning_rate": 3.718666666666667e-05,
      "loss": 0.0024,
      "step": 9610
    },
    {
      "epoch": 0.5130666666666667,
      "grad_norm": 0.11365430802106857,
      "learning_rate": 3.717333333333334e-05,
      "loss": 0.0026,
      "step": 9620
    },
    {
      "epoch": 0.5136,
      "grad_norm": 0.1401064544916153,
      "learning_rate": 3.716e-05,
      "loss": 0.0028,
      "step": 9630
    },
    {
      "epoch": 0.5141333333333333,
      "grad_norm": 0.3290141522884369,
      "learning_rate": 3.714666666666667e-05,
      "loss": 0.0037,
      "step": 9640
    },
    {
      "epoch": 0.5146666666666667,
      "grad_norm": 0.4069453179836273,
      "learning_rate": 3.713333333333334e-05,
      "loss": 0.0031,
      "step": 9650
    },
    {
      "epoch": 0.5152,
      "grad_norm": 0.30268242955207825,
      "learning_rate": 3.712e-05,
      "loss": 0.0032,
      "step": 9660
    },
    {
      "epoch": 0.5157333333333334,
      "grad_norm": 0.17510777711868286,
      "learning_rate": 3.710666666666667e-05,
      "loss": 0.0024,
      "step": 9670
    },
    {
      "epoch": 0.5162666666666667,
      "grad_norm": 0.762666642665863,
      "learning_rate": 3.709333333333333e-05,
      "loss": 0.0022,
      "step": 9680
    },
    {
      "epoch": 0.5168,
      "grad_norm": 0.4306856393814087,
      "learning_rate": 3.7080000000000004e-05,
      "loss": 0.0024,
      "step": 9690
    },
    {
      "epoch": 0.5173333333333333,
      "grad_norm": 0.1642092764377594,
      "learning_rate": 3.706666666666667e-05,
      "loss": 0.0027,
      "step": 9700
    },
    {
      "epoch": 0.5178666666666667,
      "grad_norm": 0.4854696989059448,
      "learning_rate": 3.705333333333333e-05,
      "loss": 0.003,
      "step": 9710
    },
    {
      "epoch": 0.5184,
      "grad_norm": 0.3513175845146179,
      "learning_rate": 3.7040000000000005e-05,
      "loss": 0.0045,
      "step": 9720
    },
    {
      "epoch": 0.5189333333333334,
      "grad_norm": 0.30844998359680176,
      "learning_rate": 3.702666666666667e-05,
      "loss": 0.0022,
      "step": 9730
    },
    {
      "epoch": 0.5194666666666666,
      "grad_norm": 0.23303458094596863,
      "learning_rate": 3.7013333333333334e-05,
      "loss": 0.0034,
      "step": 9740
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.13632938265800476,
      "learning_rate": 3.7e-05,
      "loss": 0.0022,
      "step": 9750
    },
    {
      "epoch": 0.5205333333333333,
      "grad_norm": 0.3386618494987488,
      "learning_rate": 3.698666666666667e-05,
      "loss": 0.0021,
      "step": 9760
    },
    {
      "epoch": 0.5210666666666667,
      "grad_norm": 0.7344027757644653,
      "learning_rate": 3.697333333333334e-05,
      "loss": 0.0025,
      "step": 9770
    },
    {
      "epoch": 0.5216,
      "grad_norm": 0.18577419221401215,
      "learning_rate": 3.696e-05,
      "loss": 0.0036,
      "step": 9780
    },
    {
      "epoch": 0.5221333333333333,
      "grad_norm": 0.2830795645713806,
      "learning_rate": 3.694666666666667e-05,
      "loss": 0.0026,
      "step": 9790
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 0.7830349802970886,
      "learning_rate": 3.6933333333333334e-05,
      "loss": 0.0027,
      "step": 9800
    },
    {
      "epoch": 0.5232,
      "grad_norm": 0.41779008507728577,
      "learning_rate": 3.692e-05,
      "loss": 0.0033,
      "step": 9810
    },
    {
      "epoch": 0.5237333333333334,
      "grad_norm": 0.24969686567783356,
      "learning_rate": 3.690666666666667e-05,
      "loss": 0.0023,
      "step": 9820
    },
    {
      "epoch": 0.5242666666666667,
      "grad_norm": 0.08613678067922592,
      "learning_rate": 3.6893333333333335e-05,
      "loss": 0.0027,
      "step": 9830
    },
    {
      "epoch": 0.5248,
      "grad_norm": 0.4266345202922821,
      "learning_rate": 3.6880000000000006e-05,
      "loss": 0.0019,
      "step": 9840
    },
    {
      "epoch": 0.5253333333333333,
      "grad_norm": 0.4742033779621124,
      "learning_rate": 3.6866666666666664e-05,
      "loss": 0.0033,
      "step": 9850
    },
    {
      "epoch": 0.5258666666666667,
      "grad_norm": 0.2057751566171646,
      "learning_rate": 3.6853333333333335e-05,
      "loss": 0.0019,
      "step": 9860
    },
    {
      "epoch": 0.5264,
      "grad_norm": 0.2790682017803192,
      "learning_rate": 3.684e-05,
      "loss": 0.0025,
      "step": 9870
    },
    {
      "epoch": 0.5269333333333334,
      "grad_norm": 0.3267989754676819,
      "learning_rate": 3.682666666666667e-05,
      "loss": 0.0026,
      "step": 9880
    },
    {
      "epoch": 0.5274666666666666,
      "grad_norm": 0.17606359720230103,
      "learning_rate": 3.6813333333333335e-05,
      "loss": 0.0027,
      "step": 9890
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.30858319997787476,
      "learning_rate": 3.68e-05,
      "loss": 0.0032,
      "step": 9900
    },
    {
      "epoch": 0.5285333333333333,
      "grad_norm": 0.30611947178840637,
      "learning_rate": 3.678666666666667e-05,
      "loss": 0.0034,
      "step": 9910
    },
    {
      "epoch": 0.5290666666666667,
      "grad_norm": 0.7825669646263123,
      "learning_rate": 3.6773333333333336e-05,
      "loss": 0.0026,
      "step": 9920
    },
    {
      "epoch": 0.5296,
      "grad_norm": 0.18705683946609497,
      "learning_rate": 3.676e-05,
      "loss": 0.0036,
      "step": 9930
    },
    {
      "epoch": 0.5301333333333333,
      "grad_norm": 0.32744619250297546,
      "learning_rate": 3.6746666666666665e-05,
      "loss": 0.0024,
      "step": 9940
    },
    {
      "epoch": 0.5306666666666666,
      "grad_norm": 0.4715123176574707,
      "learning_rate": 3.6733333333333336e-05,
      "loss": 0.003,
      "step": 9950
    },
    {
      "epoch": 0.5312,
      "grad_norm": 0.18082976341247559,
      "learning_rate": 3.672000000000001e-05,
      "loss": 0.0042,
      "step": 9960
    },
    {
      "epoch": 0.5317333333333333,
      "grad_norm": 0.10088009387254715,
      "learning_rate": 3.6706666666666665e-05,
      "loss": 0.0023,
      "step": 9970
    },
    {
      "epoch": 0.5322666666666667,
      "grad_norm": 0.6462412476539612,
      "learning_rate": 3.669333333333334e-05,
      "loss": 0.0024,
      "step": 9980
    },
    {
      "epoch": 0.5328,
      "grad_norm": 0.20782622694969177,
      "learning_rate": 3.668e-05,
      "loss": 0.0034,
      "step": 9990
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.19986726343631744,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 0.0022,
      "step": 10000
    },
    {
      "epoch": 0.5338666666666667,
      "grad_norm": 0.285451740026474,
      "learning_rate": 3.665333333333334e-05,
      "loss": 0.0031,
      "step": 10010
    },
    {
      "epoch": 0.5344,
      "grad_norm": 0.5332759022712708,
      "learning_rate": 3.664e-05,
      "loss": 0.0025,
      "step": 10020
    },
    {
      "epoch": 0.5349333333333334,
      "grad_norm": 0.3297802209854126,
      "learning_rate": 3.662666666666667e-05,
      "loss": 0.0027,
      "step": 10030
    },
    {
      "epoch": 0.5354666666666666,
      "grad_norm": 0.24446889758110046,
      "learning_rate": 3.661333333333333e-05,
      "loss": 0.0028,
      "step": 10040
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.2451477348804474,
      "learning_rate": 3.66e-05,
      "loss": 0.0039,
      "step": 10050
    },
    {
      "epoch": 0.5365333333333333,
      "grad_norm": 0.4609241187572479,
      "learning_rate": 3.6586666666666666e-05,
      "loss": 0.005,
      "step": 10060
    },
    {
      "epoch": 0.5370666666666667,
      "grad_norm": 0.339350163936615,
      "learning_rate": 3.657333333333334e-05,
      "loss": 0.0024,
      "step": 10070
    },
    {
      "epoch": 0.5376,
      "grad_norm": 0.5680829286575317,
      "learning_rate": 3.656e-05,
      "loss": 0.0028,
      "step": 10080
    },
    {
      "epoch": 0.5381333333333334,
      "grad_norm": 0.3130047619342804,
      "learning_rate": 3.654666666666667e-05,
      "loss": 0.0027,
      "step": 10090
    },
    {
      "epoch": 0.5386666666666666,
      "grad_norm": 0.11264708638191223,
      "learning_rate": 3.653333333333334e-05,
      "loss": 0.0024,
      "step": 10100
    },
    {
      "epoch": 0.5392,
      "grad_norm": 0.3550484776496887,
      "learning_rate": 3.652e-05,
      "loss": 0.0023,
      "step": 10110
    },
    {
      "epoch": 0.5397333333333333,
      "grad_norm": 0.11375359445810318,
      "learning_rate": 3.650666666666667e-05,
      "loss": 0.0032,
      "step": 10120
    },
    {
      "epoch": 0.5402666666666667,
      "grad_norm": 0.26548007130622864,
      "learning_rate": 3.649333333333333e-05,
      "loss": 0.0023,
      "step": 10130
    },
    {
      "epoch": 0.5408,
      "grad_norm": 0.48575738072395325,
      "learning_rate": 3.648e-05,
      "loss": 0.0028,
      "step": 10140
    },
    {
      "epoch": 0.5413333333333333,
      "grad_norm": 0.1816212385892868,
      "learning_rate": 3.646666666666667e-05,
      "loss": 0.0025,
      "step": 10150
    },
    {
      "epoch": 0.5418666666666667,
      "grad_norm": 0.41632044315338135,
      "learning_rate": 3.645333333333333e-05,
      "loss": 0.0032,
      "step": 10160
    },
    {
      "epoch": 0.5424,
      "grad_norm": 0.7623260617256165,
      "learning_rate": 3.6440000000000003e-05,
      "loss": 0.0021,
      "step": 10170
    },
    {
      "epoch": 0.5429333333333334,
      "grad_norm": 0.31844717264175415,
      "learning_rate": 3.642666666666667e-05,
      "loss": 0.002,
      "step": 10180
    },
    {
      "epoch": 0.5434666666666667,
      "grad_norm": 0.2522770166397095,
      "learning_rate": 3.641333333333333e-05,
      "loss": 0.0029,
      "step": 10190
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.2536928355693817,
      "learning_rate": 3.6400000000000004e-05,
      "loss": 0.0034,
      "step": 10200
    },
    {
      "epoch": 0.5445333333333333,
      "grad_norm": 0.41000688076019287,
      "learning_rate": 3.638666666666667e-05,
      "loss": 0.0023,
      "step": 10210
    },
    {
      "epoch": 0.5450666666666667,
      "grad_norm": 0.5855167508125305,
      "learning_rate": 3.637333333333334e-05,
      "loss": 0.0035,
      "step": 10220
    },
    {
      "epoch": 0.5456,
      "grad_norm": 0.15636524558067322,
      "learning_rate": 3.636e-05,
      "loss": 0.0032,
      "step": 10230
    },
    {
      "epoch": 0.5461333333333334,
      "grad_norm": 0.531887412071228,
      "learning_rate": 3.634666666666667e-05,
      "loss": 0.0024,
      "step": 10240
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.15162473917007446,
      "learning_rate": 3.633333333333333e-05,
      "loss": 0.0028,
      "step": 10250
    },
    {
      "epoch": 0.5472,
      "grad_norm": 0.5654122829437256,
      "learning_rate": 3.6320000000000005e-05,
      "loss": 0.002,
      "step": 10260
    },
    {
      "epoch": 0.5477333333333333,
      "grad_norm": 0.26671335101127625,
      "learning_rate": 3.630666666666667e-05,
      "loss": 0.0033,
      "step": 10270
    },
    {
      "epoch": 0.5482666666666667,
      "grad_norm": 0.30582016706466675,
      "learning_rate": 3.6293333333333334e-05,
      "loss": 0.0029,
      "step": 10280
    },
    {
      "epoch": 0.5488,
      "grad_norm": 0.3343370258808136,
      "learning_rate": 3.6280000000000005e-05,
      "loss": 0.0035,
      "step": 10290
    },
    {
      "epoch": 0.5493333333333333,
      "grad_norm": 0.250057190656662,
      "learning_rate": 3.626666666666667e-05,
      "loss": 0.0041,
      "step": 10300
    },
    {
      "epoch": 0.5498666666666666,
      "grad_norm": 0.17518314719200134,
      "learning_rate": 3.6253333333333334e-05,
      "loss": 0.0021,
      "step": 10310
    },
    {
      "epoch": 0.5504,
      "grad_norm": 0.19420453906059265,
      "learning_rate": 3.624e-05,
      "loss": 0.0029,
      "step": 10320
    },
    {
      "epoch": 0.5509333333333334,
      "grad_norm": 0.4628108739852905,
      "learning_rate": 3.622666666666667e-05,
      "loss": 0.0024,
      "step": 10330
    },
    {
      "epoch": 0.5514666666666667,
      "grad_norm": 0.2630927562713623,
      "learning_rate": 3.6213333333333334e-05,
      "loss": 0.0023,
      "step": 10340
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.20620521903038025,
      "learning_rate": 3.62e-05,
      "loss": 0.002,
      "step": 10350
    },
    {
      "epoch": 0.5525333333333333,
      "grad_norm": 0.1459129899740219,
      "learning_rate": 3.618666666666667e-05,
      "loss": 0.0023,
      "step": 10360
    },
    {
      "epoch": 0.5530666666666667,
      "grad_norm": 0.6106155514717102,
      "learning_rate": 3.6173333333333335e-05,
      "loss": 0.0025,
      "step": 10370
    },
    {
      "epoch": 0.5536,
      "grad_norm": 0.1484912931919098,
      "learning_rate": 3.616e-05,
      "loss": 0.0035,
      "step": 10380
    },
    {
      "epoch": 0.5541333333333334,
      "grad_norm": 0.13109521567821503,
      "learning_rate": 3.614666666666667e-05,
      "loss": 0.004,
      "step": 10390
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 0.2553408443927765,
      "learning_rate": 3.6133333333333335e-05,
      "loss": 0.0026,
      "step": 10400
    },
    {
      "epoch": 0.5552,
      "grad_norm": 0.3033696711063385,
      "learning_rate": 3.6120000000000007e-05,
      "loss": 0.0024,
      "step": 10410
    },
    {
      "epoch": 0.5557333333333333,
      "grad_norm": 0.1728900521993637,
      "learning_rate": 3.6106666666666664e-05,
      "loss": 0.0035,
      "step": 10420
    },
    {
      "epoch": 0.5562666666666667,
      "grad_norm": 0.25187912583351135,
      "learning_rate": 3.6093333333333336e-05,
      "loss": 0.0027,
      "step": 10430
    },
    {
      "epoch": 0.5568,
      "grad_norm": 0.37683340907096863,
      "learning_rate": 3.608e-05,
      "loss": 0.0029,
      "step": 10440
    },
    {
      "epoch": 0.5573333333333333,
      "grad_norm": 0.38111573457717896,
      "learning_rate": 3.606666666666667e-05,
      "loss": 0.0047,
      "step": 10450
    },
    {
      "epoch": 0.5578666666666666,
      "grad_norm": 0.24908168613910675,
      "learning_rate": 3.6053333333333336e-05,
      "loss": 0.0029,
      "step": 10460
    },
    {
      "epoch": 0.5584,
      "grad_norm": 0.38118788599967957,
      "learning_rate": 3.604e-05,
      "loss": 0.0043,
      "step": 10470
    },
    {
      "epoch": 0.5589333333333333,
      "grad_norm": 0.19729658961296082,
      "learning_rate": 3.602666666666667e-05,
      "loss": 0.0028,
      "step": 10480
    },
    {
      "epoch": 0.5594666666666667,
      "grad_norm": 0.13425055146217346,
      "learning_rate": 3.6013333333333336e-05,
      "loss": 0.0028,
      "step": 10490
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.17519059777259827,
      "learning_rate": 3.6e-05,
      "loss": 0.0033,
      "step": 10500
    },
    {
      "epoch": 0.5605333333333333,
      "grad_norm": 0.369880735874176,
      "learning_rate": 3.5986666666666665e-05,
      "loss": 0.0019,
      "step": 10510
    },
    {
      "epoch": 0.5610666666666667,
      "grad_norm": 0.1487332135438919,
      "learning_rate": 3.597333333333334e-05,
      "loss": 0.002,
      "step": 10520
    },
    {
      "epoch": 0.5616,
      "grad_norm": 0.14611175656318665,
      "learning_rate": 3.596e-05,
      "loss": 0.0033,
      "step": 10530
    },
    {
      "epoch": 0.5621333333333334,
      "grad_norm": 0.4107118844985962,
      "learning_rate": 3.5946666666666666e-05,
      "loss": 0.0021,
      "step": 10540
    },
    {
      "epoch": 0.5626666666666666,
      "grad_norm": 0.3200034499168396,
      "learning_rate": 3.593333333333334e-05,
      "loss": 0.0021,
      "step": 10550
    },
    {
      "epoch": 0.5632,
      "grad_norm": 0.31577715277671814,
      "learning_rate": 3.592e-05,
      "loss": 0.0026,
      "step": 10560
    },
    {
      "epoch": 0.5637333333333333,
      "grad_norm": 0.1514601856470108,
      "learning_rate": 3.5906666666666666e-05,
      "loss": 0.0027,
      "step": 10570
    },
    {
      "epoch": 0.5642666666666667,
      "grad_norm": 0.39779022336006165,
      "learning_rate": 3.589333333333334e-05,
      "loss": 0.0017,
      "step": 10580
    },
    {
      "epoch": 0.5648,
      "grad_norm": 0.3717576563358307,
      "learning_rate": 3.588e-05,
      "loss": 0.0023,
      "step": 10590
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 0.6215888261795044,
      "learning_rate": 3.586666666666667e-05,
      "loss": 0.0033,
      "step": 10600
    },
    {
      "epoch": 0.5658666666666666,
      "grad_norm": 0.18962538242340088,
      "learning_rate": 3.585333333333333e-05,
      "loss": 0.003,
      "step": 10610
    },
    {
      "epoch": 0.5664,
      "grad_norm": 0.33798789978027344,
      "learning_rate": 3.584e-05,
      "loss": 0.002,
      "step": 10620
    },
    {
      "epoch": 0.5669333333333333,
      "grad_norm": 0.23206643760204315,
      "learning_rate": 3.582666666666667e-05,
      "loss": 0.003,
      "step": 10630
    },
    {
      "epoch": 0.5674666666666667,
      "grad_norm": 0.6697986125946045,
      "learning_rate": 3.581333333333334e-05,
      "loss": 0.0027,
      "step": 10640
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.3843478858470917,
      "learning_rate": 3.58e-05,
      "loss": 0.0019,
      "step": 10650
    },
    {
      "epoch": 0.5685333333333333,
      "grad_norm": 0.14175064861774445,
      "learning_rate": 3.578666666666667e-05,
      "loss": 0.0026,
      "step": 10660
    },
    {
      "epoch": 0.5690666666666667,
      "grad_norm": 0.08512884378433228,
      "learning_rate": 3.577333333333334e-05,
      "loss": 0.003,
      "step": 10670
    },
    {
      "epoch": 0.5696,
      "grad_norm": 0.08155906200408936,
      "learning_rate": 3.5759999999999996e-05,
      "loss": 0.0033,
      "step": 10680
    },
    {
      "epoch": 0.5701333333333334,
      "grad_norm": 0.3158408999443054,
      "learning_rate": 3.574666666666667e-05,
      "loss": 0.0021,
      "step": 10690
    },
    {
      "epoch": 0.5706666666666667,
      "grad_norm": 0.27048999071121216,
      "learning_rate": 3.573333333333333e-05,
      "loss": 0.0034,
      "step": 10700
    },
    {
      "epoch": 0.5712,
      "grad_norm": 0.48863130807876587,
      "learning_rate": 3.5720000000000004e-05,
      "loss": 0.0022,
      "step": 10710
    },
    {
      "epoch": 0.5717333333333333,
      "grad_norm": 0.13569536805152893,
      "learning_rate": 3.570666666666667e-05,
      "loss": 0.0028,
      "step": 10720
    },
    {
      "epoch": 0.5722666666666667,
      "grad_norm": 0.7061583399772644,
      "learning_rate": 3.569333333333333e-05,
      "loss": 0.0029,
      "step": 10730
    },
    {
      "epoch": 0.5728,
      "grad_norm": 0.18233278393745422,
      "learning_rate": 3.5680000000000004e-05,
      "loss": 0.0026,
      "step": 10740
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.6496468782424927,
      "learning_rate": 3.566666666666667e-05,
      "loss": 0.003,
      "step": 10750
    },
    {
      "epoch": 0.5738666666666666,
      "grad_norm": 0.6719040274620056,
      "learning_rate": 3.565333333333333e-05,
      "loss": 0.0034,
      "step": 10760
    },
    {
      "epoch": 0.5744,
      "grad_norm": 0.13489073514938354,
      "learning_rate": 3.5640000000000004e-05,
      "loss": 0.0035,
      "step": 10770
    },
    {
      "epoch": 0.5749333333333333,
      "grad_norm": 0.3121294677257538,
      "learning_rate": 3.562666666666667e-05,
      "loss": 0.0023,
      "step": 10780
    },
    {
      "epoch": 0.5754666666666667,
      "grad_norm": 0.1555224359035492,
      "learning_rate": 3.561333333333334e-05,
      "loss": 0.0043,
      "step": 10790
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.14162708818912506,
      "learning_rate": 3.56e-05,
      "loss": 0.0048,
      "step": 10800
    },
    {
      "epoch": 0.5765333333333333,
      "grad_norm": 0.3629661202430725,
      "learning_rate": 3.558666666666667e-05,
      "loss": 0.0024,
      "step": 10810
    },
    {
      "epoch": 0.5770666666666666,
      "grad_norm": 0.5028064250946045,
      "learning_rate": 3.5573333333333334e-05,
      "loss": 0.0034,
      "step": 10820
    },
    {
      "epoch": 0.5776,
      "grad_norm": 0.13071832060813904,
      "learning_rate": 3.5560000000000005e-05,
      "loss": 0.003,
      "step": 10830
    },
    {
      "epoch": 0.5781333333333334,
      "grad_norm": 0.5733637809753418,
      "learning_rate": 3.554666666666667e-05,
      "loss": 0.0027,
      "step": 10840
    },
    {
      "epoch": 0.5786666666666667,
      "grad_norm": 0.19651839137077332,
      "learning_rate": 3.5533333333333334e-05,
      "loss": 0.0024,
      "step": 10850
    },
    {
      "epoch": 0.5792,
      "grad_norm": 0.12732969224452972,
      "learning_rate": 3.5520000000000006e-05,
      "loss": 0.0035,
      "step": 10860
    },
    {
      "epoch": 0.5797333333333333,
      "grad_norm": 0.5254197716712952,
      "learning_rate": 3.550666666666666e-05,
      "loss": 0.002,
      "step": 10870
    },
    {
      "epoch": 0.5802666666666667,
      "grad_norm": 0.2608445882797241,
      "learning_rate": 3.5493333333333335e-05,
      "loss": 0.0024,
      "step": 10880
    },
    {
      "epoch": 0.5808,
      "grad_norm": 0.3086394667625427,
      "learning_rate": 3.548e-05,
      "loss": 0.0024,
      "step": 10890
    },
    {
      "epoch": 0.5813333333333334,
      "grad_norm": 0.31599223613739014,
      "learning_rate": 3.546666666666667e-05,
      "loss": 0.002,
      "step": 10900
    },
    {
      "epoch": 0.5818666666666666,
      "grad_norm": 0.13265253603458405,
      "learning_rate": 3.5453333333333335e-05,
      "loss": 0.002,
      "step": 10910
    },
    {
      "epoch": 0.5824,
      "grad_norm": 0.1891379952430725,
      "learning_rate": 3.544e-05,
      "loss": 0.0031,
      "step": 10920
    },
    {
      "epoch": 0.5829333333333333,
      "grad_norm": 0.2962843179702759,
      "learning_rate": 3.542666666666667e-05,
      "loss": 0.0027,
      "step": 10930
    },
    {
      "epoch": 0.5834666666666667,
      "grad_norm": 0.19606715440750122,
      "learning_rate": 3.5413333333333335e-05,
      "loss": 0.0026,
      "step": 10940
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.7057841420173645,
      "learning_rate": 3.54e-05,
      "loss": 0.0036,
      "step": 10950
    },
    {
      "epoch": 0.5845333333333333,
      "grad_norm": 0.14070934057235718,
      "learning_rate": 3.538666666666667e-05,
      "loss": 0.002,
      "step": 10960
    },
    {
      "epoch": 0.5850666666666666,
      "grad_norm": 0.2430075705051422,
      "learning_rate": 3.5373333333333336e-05,
      "loss": 0.0033,
      "step": 10970
    },
    {
      "epoch": 0.5856,
      "grad_norm": 0.593095064163208,
      "learning_rate": 3.536000000000001e-05,
      "loss": 0.0024,
      "step": 10980
    },
    {
      "epoch": 0.5861333333333333,
      "grad_norm": 0.3929542601108551,
      "learning_rate": 3.5346666666666665e-05,
      "loss": 0.0027,
      "step": 10990
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.4007217586040497,
      "learning_rate": 3.5333333333333336e-05,
      "loss": 0.0028,
      "step": 11000
    },
    {
      "epoch": 0.5872,
      "grad_norm": 0.26093146204948425,
      "learning_rate": 3.532e-05,
      "loss": 0.002,
      "step": 11010
    },
    {
      "epoch": 0.5877333333333333,
      "grad_norm": 0.2392835170030594,
      "learning_rate": 3.5306666666666665e-05,
      "loss": 0.0036,
      "step": 11020
    },
    {
      "epoch": 0.5882666666666667,
      "grad_norm": 0.2556505799293518,
      "learning_rate": 3.5293333333333336e-05,
      "loss": 0.0028,
      "step": 11030
    },
    {
      "epoch": 0.5888,
      "grad_norm": 0.19973574578762054,
      "learning_rate": 3.528e-05,
      "loss": 0.0025,
      "step": 11040
    },
    {
      "epoch": 0.5893333333333334,
      "grad_norm": 0.471622496843338,
      "learning_rate": 3.526666666666667e-05,
      "loss": 0.0022,
      "step": 11050
    },
    {
      "epoch": 0.5898666666666667,
      "grad_norm": 0.6428389549255371,
      "learning_rate": 3.525333333333333e-05,
      "loss": 0.0022,
      "step": 11060
    },
    {
      "epoch": 0.5904,
      "grad_norm": 0.16148976981639862,
      "learning_rate": 3.524e-05,
      "loss": 0.0019,
      "step": 11070
    },
    {
      "epoch": 0.5909333333333333,
      "grad_norm": 0.17168128490447998,
      "learning_rate": 3.5226666666666666e-05,
      "loss": 0.0022,
      "step": 11080
    },
    {
      "epoch": 0.5914666666666667,
      "grad_norm": 0.37008991837501526,
      "learning_rate": 3.521333333333334e-05,
      "loss": 0.0025,
      "step": 11090
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.20687828958034515,
      "learning_rate": 3.52e-05,
      "loss": 0.0022,
      "step": 11100
    },
    {
      "epoch": 0.5925333333333334,
      "grad_norm": 0.4730660915374756,
      "learning_rate": 3.5186666666666666e-05,
      "loss": 0.0023,
      "step": 11110
    },
    {
      "epoch": 0.5930666666666666,
      "grad_norm": 0.2928377687931061,
      "learning_rate": 3.517333333333334e-05,
      "loss": 0.0025,
      "step": 11120
    },
    {
      "epoch": 0.5936,
      "grad_norm": 0.6913327574729919,
      "learning_rate": 3.516e-05,
      "loss": 0.0038,
      "step": 11130
    },
    {
      "epoch": 0.5941333333333333,
      "grad_norm": 0.7028910517692566,
      "learning_rate": 3.514666666666667e-05,
      "loss": 0.0029,
      "step": 11140
    },
    {
      "epoch": 0.5946666666666667,
      "grad_norm": 0.12041888386011124,
      "learning_rate": 3.513333333333334e-05,
      "loss": 0.0021,
      "step": 11150
    },
    {
      "epoch": 0.5952,
      "grad_norm": 0.16322322189807892,
      "learning_rate": 3.512e-05,
      "loss": 0.0023,
      "step": 11160
    },
    {
      "epoch": 0.5957333333333333,
      "grad_norm": 0.5091988444328308,
      "learning_rate": 3.5106666666666674e-05,
      "loss": 0.0023,
      "step": 11170
    },
    {
      "epoch": 0.5962666666666666,
      "grad_norm": 0.3874669075012207,
      "learning_rate": 3.509333333333333e-05,
      "loss": 0.0024,
      "step": 11180
    },
    {
      "epoch": 0.5968,
      "grad_norm": 0.20959874987602234,
      "learning_rate": 3.508e-05,
      "loss": 0.0029,
      "step": 11190
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 0.4125182032585144,
      "learning_rate": 3.506666666666667e-05,
      "loss": 0.0024,
      "step": 11200
    },
    {
      "epoch": 0.5978666666666667,
      "grad_norm": 0.20295196771621704,
      "learning_rate": 3.505333333333333e-05,
      "loss": 0.0024,
      "step": 11210
    },
    {
      "epoch": 0.5984,
      "grad_norm": 0.38766780495643616,
      "learning_rate": 3.504e-05,
      "loss": 0.0019,
      "step": 11220
    },
    {
      "epoch": 0.5989333333333333,
      "grad_norm": 0.1173681765794754,
      "learning_rate": 3.502666666666667e-05,
      "loss": 0.0027,
      "step": 11230
    },
    {
      "epoch": 0.5994666666666667,
      "grad_norm": 0.26921147108078003,
      "learning_rate": 3.501333333333334e-05,
      "loss": 0.0022,
      "step": 11240
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.23824937641620636,
      "learning_rate": 3.5e-05,
      "loss": 0.003,
      "step": 11250
    },
    {
      "epoch": 0.6005333333333334,
      "grad_norm": 0.09436362981796265,
      "learning_rate": 3.498666666666667e-05,
      "loss": 0.0026,
      "step": 11260
    },
    {
      "epoch": 0.6010666666666666,
      "grad_norm": 0.2809942960739136,
      "learning_rate": 3.497333333333333e-05,
      "loss": 0.0023,
      "step": 11270
    },
    {
      "epoch": 0.6016,
      "grad_norm": 0.4024326801300049,
      "learning_rate": 3.4960000000000004e-05,
      "loss": 0.0027,
      "step": 11280
    },
    {
      "epoch": 0.6021333333333333,
      "grad_norm": 0.4273344576358795,
      "learning_rate": 3.494666666666667e-05,
      "loss": 0.0045,
      "step": 11290
    },
    {
      "epoch": 0.6026666666666667,
      "grad_norm": 0.42231833934783936,
      "learning_rate": 3.493333333333333e-05,
      "loss": 0.0027,
      "step": 11300
    },
    {
      "epoch": 0.6032,
      "grad_norm": 0.13122324645519257,
      "learning_rate": 3.4920000000000004e-05,
      "loss": 0.002,
      "step": 11310
    },
    {
      "epoch": 0.6037333333333333,
      "grad_norm": 0.2939842641353607,
      "learning_rate": 3.490666666666667e-05,
      "loss": 0.0029,
      "step": 11320
    },
    {
      "epoch": 0.6042666666666666,
      "grad_norm": 0.34914958477020264,
      "learning_rate": 3.4893333333333334e-05,
      "loss": 0.0043,
      "step": 11330
    },
    {
      "epoch": 0.6048,
      "grad_norm": 0.6573684215545654,
      "learning_rate": 3.4880000000000005e-05,
      "loss": 0.0021,
      "step": 11340
    },
    {
      "epoch": 0.6053333333333333,
      "grad_norm": 0.12533347308635712,
      "learning_rate": 3.486666666666667e-05,
      "loss": 0.0024,
      "step": 11350
    },
    {
      "epoch": 0.6058666666666667,
      "grad_norm": 0.24198831617832184,
      "learning_rate": 3.4853333333333334e-05,
      "loss": 0.0025,
      "step": 11360
    },
    {
      "epoch": 0.6064,
      "grad_norm": 0.09025029093027115,
      "learning_rate": 3.484e-05,
      "loss": 0.0016,
      "step": 11370
    },
    {
      "epoch": 0.6069333333333333,
      "grad_norm": 0.40283095836639404,
      "learning_rate": 3.482666666666667e-05,
      "loss": 0.0023,
      "step": 11380
    },
    {
      "epoch": 0.6074666666666667,
      "grad_norm": 0.2393474578857422,
      "learning_rate": 3.4813333333333334e-05,
      "loss": 0.0027,
      "step": 11390
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.10600518435239792,
      "learning_rate": 3.48e-05,
      "loss": 0.0021,
      "step": 11400
    },
    {
      "epoch": 0.6085333333333334,
      "grad_norm": 0.14104299247264862,
      "learning_rate": 3.478666666666667e-05,
      "loss": 0.0022,
      "step": 11410
    },
    {
      "epoch": 0.6090666666666666,
      "grad_norm": 0.22993025183677673,
      "learning_rate": 3.4773333333333335e-05,
      "loss": 0.0024,
      "step": 11420
    },
    {
      "epoch": 0.6096,
      "grad_norm": 0.2897113859653473,
      "learning_rate": 3.4760000000000006e-05,
      "loss": 0.002,
      "step": 11430
    },
    {
      "epoch": 0.6101333333333333,
      "grad_norm": 0.5786426663398743,
      "learning_rate": 3.4746666666666664e-05,
      "loss": 0.0023,
      "step": 11440
    },
    {
      "epoch": 0.6106666666666667,
      "grad_norm": 0.30847635865211487,
      "learning_rate": 3.4733333333333335e-05,
      "loss": 0.0031,
      "step": 11450
    },
    {
      "epoch": 0.6112,
      "grad_norm": 0.5930156707763672,
      "learning_rate": 3.472e-05,
      "loss": 0.0022,
      "step": 11460
    },
    {
      "epoch": 0.6117333333333334,
      "grad_norm": 0.47425711154937744,
      "learning_rate": 3.470666666666667e-05,
      "loss": 0.0035,
      "step": 11470
    },
    {
      "epoch": 0.6122666666666666,
      "grad_norm": 0.28475692868232727,
      "learning_rate": 3.4693333333333335e-05,
      "loss": 0.0033,
      "step": 11480
    },
    {
      "epoch": 0.6128,
      "grad_norm": 0.2595024108886719,
      "learning_rate": 3.468e-05,
      "loss": 0.002,
      "step": 11490
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.3276716470718384,
      "learning_rate": 3.466666666666667e-05,
      "loss": 0.0033,
      "step": 11500
    },
    {
      "epoch": 0.6138666666666667,
      "grad_norm": 0.25548359751701355,
      "learning_rate": 3.4653333333333336e-05,
      "loss": 0.0049,
      "step": 11510
    },
    {
      "epoch": 0.6144,
      "grad_norm": 0.6715651750564575,
      "learning_rate": 3.464e-05,
      "loss": 0.0031,
      "step": 11520
    },
    {
      "epoch": 0.6149333333333333,
      "grad_norm": 0.09762563556432724,
      "learning_rate": 3.462666666666667e-05,
      "loss": 0.0023,
      "step": 11530
    },
    {
      "epoch": 0.6154666666666667,
      "grad_norm": 0.29660508036613464,
      "learning_rate": 3.4613333333333336e-05,
      "loss": 0.0035,
      "step": 11540
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.2873978614807129,
      "learning_rate": 3.46e-05,
      "loss": 0.0027,
      "step": 11550
    },
    {
      "epoch": 0.6165333333333334,
      "grad_norm": 0.13235469162464142,
      "learning_rate": 3.4586666666666665e-05,
      "loss": 0.0028,
      "step": 11560
    },
    {
      "epoch": 0.6170666666666667,
      "grad_norm": 0.13314592838287354,
      "learning_rate": 3.4573333333333337e-05,
      "loss": 0.0042,
      "step": 11570
    },
    {
      "epoch": 0.6176,
      "grad_norm": 0.18790501356124878,
      "learning_rate": 3.456e-05,
      "loss": 0.0038,
      "step": 11580
    },
    {
      "epoch": 0.6181333333333333,
      "grad_norm": 0.3767845630645752,
      "learning_rate": 3.4546666666666666e-05,
      "loss": 0.0033,
      "step": 11590
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 0.7015889286994934,
      "learning_rate": 3.453333333333334e-05,
      "loss": 0.0024,
      "step": 11600
    },
    {
      "epoch": 0.6192,
      "grad_norm": 0.2978195548057556,
      "learning_rate": 3.452e-05,
      "loss": 0.003,
      "step": 11610
    },
    {
      "epoch": 0.6197333333333334,
      "grad_norm": 0.5308526754379272,
      "learning_rate": 3.450666666666667e-05,
      "loss": 0.003,
      "step": 11620
    },
    {
      "epoch": 0.6202666666666666,
      "grad_norm": 0.6622627377510071,
      "learning_rate": 3.449333333333333e-05,
      "loss": 0.0034,
      "step": 11630
    },
    {
      "epoch": 0.6208,
      "grad_norm": 0.31814467906951904,
      "learning_rate": 3.448e-05,
      "loss": 0.0031,
      "step": 11640
    },
    {
      "epoch": 0.6213333333333333,
      "grad_norm": 0.27807122468948364,
      "learning_rate": 3.4466666666666666e-05,
      "loss": 0.0025,
      "step": 11650
    },
    {
      "epoch": 0.6218666666666667,
      "grad_norm": 0.6677339673042297,
      "learning_rate": 3.445333333333334e-05,
      "loss": 0.0032,
      "step": 11660
    },
    {
      "epoch": 0.6224,
      "grad_norm": 0.8470672369003296,
      "learning_rate": 3.444e-05,
      "loss": 0.0031,
      "step": 11670
    },
    {
      "epoch": 0.6229333333333333,
      "grad_norm": 0.11849864572286606,
      "learning_rate": 3.442666666666667e-05,
      "loss": 0.0028,
      "step": 11680
    },
    {
      "epoch": 0.6234666666666666,
      "grad_norm": 0.14807452261447906,
      "learning_rate": 3.441333333333334e-05,
      "loss": 0.0026,
      "step": 11690
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.26936718821525574,
      "learning_rate": 3.4399999999999996e-05,
      "loss": 0.0022,
      "step": 11700
    },
    {
      "epoch": 0.6245333333333334,
      "grad_norm": 0.2390473634004593,
      "learning_rate": 3.438666666666667e-05,
      "loss": 0.0036,
      "step": 11710
    },
    {
      "epoch": 0.6250666666666667,
      "grad_norm": 0.25427311658859253,
      "learning_rate": 3.437333333333334e-05,
      "loss": 0.0031,
      "step": 11720
    },
    {
      "epoch": 0.6256,
      "grad_norm": 0.3035062849521637,
      "learning_rate": 3.436e-05,
      "loss": 0.0025,
      "step": 11730
    },
    {
      "epoch": 0.6261333333333333,
      "grad_norm": 0.5009767413139343,
      "learning_rate": 3.434666666666667e-05,
      "loss": 0.0028,
      "step": 11740
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.574881374835968,
      "learning_rate": 3.433333333333333e-05,
      "loss": 0.005,
      "step": 11750
    },
    {
      "epoch": 0.6272,
      "grad_norm": 0.9844026565551758,
      "learning_rate": 3.4320000000000003e-05,
      "loss": 0.0033,
      "step": 11760
    },
    {
      "epoch": 0.6277333333333334,
      "grad_norm": 0.24387970566749573,
      "learning_rate": 3.430666666666667e-05,
      "loss": 0.0032,
      "step": 11770
    },
    {
      "epoch": 0.6282666666666666,
      "grad_norm": 0.2520950734615326,
      "learning_rate": 3.429333333333333e-05,
      "loss": 0.0027,
      "step": 11780
    },
    {
      "epoch": 0.6288,
      "grad_norm": 0.19529955089092255,
      "learning_rate": 3.4280000000000004e-05,
      "loss": 0.0031,
      "step": 11790
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 0.6783424019813538,
      "learning_rate": 3.426666666666667e-05,
      "loss": 0.0019,
      "step": 11800
    },
    {
      "epoch": 0.6298666666666667,
      "grad_norm": 0.12479132413864136,
      "learning_rate": 3.425333333333334e-05,
      "loss": 0.0033,
      "step": 11810
    },
    {
      "epoch": 0.6304,
      "grad_norm": 0.3286791145801544,
      "learning_rate": 3.424e-05,
      "loss": 0.0022,
      "step": 11820
    },
    {
      "epoch": 0.6309333333333333,
      "grad_norm": 0.5745911002159119,
      "learning_rate": 3.422666666666667e-05,
      "loss": 0.002,
      "step": 11830
    },
    {
      "epoch": 0.6314666666666666,
      "grad_norm": 0.10965781658887863,
      "learning_rate": 3.421333333333333e-05,
      "loss": 0.002,
      "step": 11840
    },
    {
      "epoch": 0.632,
      "grad_norm": 0.48099270462989807,
      "learning_rate": 3.4200000000000005e-05,
      "loss": 0.0021,
      "step": 11850
    },
    {
      "epoch": 0.6325333333333333,
      "grad_norm": 0.35198190808296204,
      "learning_rate": 3.418666666666667e-05,
      "loss": 0.0024,
      "step": 11860
    },
    {
      "epoch": 0.6330666666666667,
      "grad_norm": 0.38038113713264465,
      "learning_rate": 3.4173333333333334e-05,
      "loss": 0.004,
      "step": 11870
    },
    {
      "epoch": 0.6336,
      "grad_norm": 0.5578117370605469,
      "learning_rate": 3.4160000000000005e-05,
      "loss": 0.0026,
      "step": 11880
    },
    {
      "epoch": 0.6341333333333333,
      "grad_norm": 0.14145620167255402,
      "learning_rate": 3.414666666666666e-05,
      "loss": 0.0031,
      "step": 11890
    },
    {
      "epoch": 0.6346666666666667,
      "grad_norm": 0.7961474657058716,
      "learning_rate": 3.4133333333333334e-05,
      "loss": 0.0029,
      "step": 11900
    },
    {
      "epoch": 0.6352,
      "grad_norm": 0.28115081787109375,
      "learning_rate": 3.412e-05,
      "loss": 0.003,
      "step": 11910
    },
    {
      "epoch": 0.6357333333333334,
      "grad_norm": 0.28798580169677734,
      "learning_rate": 3.410666666666667e-05,
      "loss": 0.003,
      "step": 11920
    },
    {
      "epoch": 0.6362666666666666,
      "grad_norm": 0.25976911187171936,
      "learning_rate": 3.4093333333333334e-05,
      "loss": 0.0023,
      "step": 11930
    },
    {
      "epoch": 0.6368,
      "grad_norm": 0.22618654370307922,
      "learning_rate": 3.408e-05,
      "loss": 0.002,
      "step": 11940
    },
    {
      "epoch": 0.6373333333333333,
      "grad_norm": 0.5385867953300476,
      "learning_rate": 3.406666666666667e-05,
      "loss": 0.0025,
      "step": 11950
    },
    {
      "epoch": 0.6378666666666667,
      "grad_norm": 0.14699135720729828,
      "learning_rate": 3.4053333333333335e-05,
      "loss": 0.0023,
      "step": 11960
    },
    {
      "epoch": 0.6384,
      "grad_norm": 0.5154585838317871,
      "learning_rate": 3.404e-05,
      "loss": 0.0027,
      "step": 11970
    },
    {
      "epoch": 0.6389333333333334,
      "grad_norm": 0.6578357815742493,
      "learning_rate": 3.402666666666667e-05,
      "loss": 0.003,
      "step": 11980
    },
    {
      "epoch": 0.6394666666666666,
      "grad_norm": 0.1042734906077385,
      "learning_rate": 3.4013333333333335e-05,
      "loss": 0.0029,
      "step": 11990
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.702877938747406,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.0021,
      "step": 12000
    },
    {
      "epoch": 0.6405333333333333,
      "grad_norm": 0.3790093660354614,
      "learning_rate": 3.3986666666666664e-05,
      "loss": 0.0028,
      "step": 12010
    },
    {
      "epoch": 0.6410666666666667,
      "grad_norm": 0.6226942539215088,
      "learning_rate": 3.3973333333333336e-05,
      "loss": 0.0019,
      "step": 12020
    },
    {
      "epoch": 0.6416,
      "grad_norm": 0.37456586956977844,
      "learning_rate": 3.396e-05,
      "loss": 0.0029,
      "step": 12030
    },
    {
      "epoch": 0.6421333333333333,
      "grad_norm": 0.6256780624389648,
      "learning_rate": 3.394666666666667e-05,
      "loss": 0.0023,
      "step": 12040
    },
    {
      "epoch": 0.6426666666666667,
      "grad_norm": 0.29959285259246826,
      "learning_rate": 3.3933333333333336e-05,
      "loss": 0.0041,
      "step": 12050
    },
    {
      "epoch": 0.6432,
      "grad_norm": 0.5350919961929321,
      "learning_rate": 3.392e-05,
      "loss": 0.0026,
      "step": 12060
    },
    {
      "epoch": 0.6437333333333334,
      "grad_norm": 0.3273274302482605,
      "learning_rate": 3.390666666666667e-05,
      "loss": 0.0037,
      "step": 12070
    },
    {
      "epoch": 0.6442666666666667,
      "grad_norm": 0.572755753993988,
      "learning_rate": 3.389333333333333e-05,
      "loss": 0.0022,
      "step": 12080
    },
    {
      "epoch": 0.6448,
      "grad_norm": 0.39691177010536194,
      "learning_rate": 3.388e-05,
      "loss": 0.0024,
      "step": 12090
    },
    {
      "epoch": 0.6453333333333333,
      "grad_norm": 0.5285108685493469,
      "learning_rate": 3.3866666666666665e-05,
      "loss": 0.0032,
      "step": 12100
    },
    {
      "epoch": 0.6458666666666667,
      "grad_norm": 0.7776978611946106,
      "learning_rate": 3.385333333333334e-05,
      "loss": 0.0032,
      "step": 12110
    },
    {
      "epoch": 0.6464,
      "grad_norm": 0.09563636034727097,
      "learning_rate": 3.384e-05,
      "loss": 0.0026,
      "step": 12120
    },
    {
      "epoch": 0.6469333333333334,
      "grad_norm": 0.5798451900482178,
      "learning_rate": 3.3826666666666666e-05,
      "loss": 0.0029,
      "step": 12130
    },
    {
      "epoch": 0.6474666666666666,
      "grad_norm": 0.2418348789215088,
      "learning_rate": 3.381333333333334e-05,
      "loss": 0.0024,
      "step": 12140
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.696005642414093,
      "learning_rate": 3.38e-05,
      "loss": 0.0034,
      "step": 12150
    },
    {
      "epoch": 0.6485333333333333,
      "grad_norm": 0.38793277740478516,
      "learning_rate": 3.3786666666666666e-05,
      "loss": 0.0031,
      "step": 12160
    },
    {
      "epoch": 0.6490666666666667,
      "grad_norm": 0.1500277817249298,
      "learning_rate": 3.377333333333334e-05,
      "loss": 0.0019,
      "step": 12170
    },
    {
      "epoch": 0.6496,
      "grad_norm": 0.1374797224998474,
      "learning_rate": 3.376e-05,
      "loss": 0.0023,
      "step": 12180
    },
    {
      "epoch": 0.6501333333333333,
      "grad_norm": 0.4478122591972351,
      "learning_rate": 3.374666666666667e-05,
      "loss": 0.0033,
      "step": 12190
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 0.3322265148162842,
      "learning_rate": 3.373333333333333e-05,
      "loss": 0.0028,
      "step": 12200
    },
    {
      "epoch": 0.6512,
      "grad_norm": 0.17021192610263824,
      "learning_rate": 3.372e-05,
      "loss": 0.0021,
      "step": 12210
    },
    {
      "epoch": 0.6517333333333334,
      "grad_norm": 0.4059809148311615,
      "learning_rate": 3.370666666666667e-05,
      "loss": 0.0031,
      "step": 12220
    },
    {
      "epoch": 0.6522666666666667,
      "grad_norm": 0.2403980791568756,
      "learning_rate": 3.369333333333333e-05,
      "loss": 0.003,
      "step": 12230
    },
    {
      "epoch": 0.6528,
      "grad_norm": 0.40779727697372437,
      "learning_rate": 3.368e-05,
      "loss": 0.0029,
      "step": 12240
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.34234490990638733,
      "learning_rate": 3.366666666666667e-05,
      "loss": 0.0028,
      "step": 12250
    },
    {
      "epoch": 0.6538666666666667,
      "grad_norm": 0.1166863664984703,
      "learning_rate": 3.365333333333334e-05,
      "loss": 0.0024,
      "step": 12260
    },
    {
      "epoch": 0.6544,
      "grad_norm": 0.26484912633895874,
      "learning_rate": 3.3639999999999996e-05,
      "loss": 0.0034,
      "step": 12270
    },
    {
      "epoch": 0.6549333333333334,
      "grad_norm": 0.12321417033672333,
      "learning_rate": 3.362666666666667e-05,
      "loss": 0.0021,
      "step": 12280
    },
    {
      "epoch": 0.6554666666666666,
      "grad_norm": 0.5367462038993835,
      "learning_rate": 3.361333333333333e-05,
      "loss": 0.0021,
      "step": 12290
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.30454614758491516,
      "learning_rate": 3.3600000000000004e-05,
      "loss": 0.003,
      "step": 12300
    },
    {
      "epoch": 0.6565333333333333,
      "grad_norm": 0.4088422954082489,
      "learning_rate": 3.358666666666667e-05,
      "loss": 0.0024,
      "step": 12310
    },
    {
      "epoch": 0.6570666666666667,
      "grad_norm": 0.19337551295757294,
      "learning_rate": 3.357333333333333e-05,
      "loss": 0.003,
      "step": 12320
    },
    {
      "epoch": 0.6576,
      "grad_norm": 0.3052220344543457,
      "learning_rate": 3.3560000000000004e-05,
      "loss": 0.003,
      "step": 12330
    },
    {
      "epoch": 0.6581333333333333,
      "grad_norm": 0.19273865222930908,
      "learning_rate": 3.354666666666667e-05,
      "loss": 0.002,
      "step": 12340
    },
    {
      "epoch": 0.6586666666666666,
      "grad_norm": 0.40377548336982727,
      "learning_rate": 3.353333333333333e-05,
      "loss": 0.0035,
      "step": 12350
    },
    {
      "epoch": 0.6592,
      "grad_norm": 0.15552109479904175,
      "learning_rate": 3.3520000000000004e-05,
      "loss": 0.0025,
      "step": 12360
    },
    {
      "epoch": 0.6597333333333333,
      "grad_norm": 0.3557984530925751,
      "learning_rate": 3.350666666666667e-05,
      "loss": 0.0026,
      "step": 12370
    },
    {
      "epoch": 0.6602666666666667,
      "grad_norm": 0.4432849884033203,
      "learning_rate": 3.349333333333334e-05,
      "loss": 0.0031,
      "step": 12380
    },
    {
      "epoch": 0.6608,
      "grad_norm": 0.16118918359279633,
      "learning_rate": 3.348e-05,
      "loss": 0.0022,
      "step": 12390
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 0.16704826056957245,
      "learning_rate": 3.346666666666667e-05,
      "loss": 0.0022,
      "step": 12400
    },
    {
      "epoch": 0.6618666666666667,
      "grad_norm": 0.12575200200080872,
      "learning_rate": 3.3453333333333334e-05,
      "loss": 0.0028,
      "step": 12410
    },
    {
      "epoch": 0.6624,
      "grad_norm": 0.1160484030842781,
      "learning_rate": 3.344e-05,
      "loss": 0.0019,
      "step": 12420
    },
    {
      "epoch": 0.6629333333333334,
      "grad_norm": 0.0746796652674675,
      "learning_rate": 3.342666666666667e-05,
      "loss": 0.0021,
      "step": 12430
    },
    {
      "epoch": 0.6634666666666666,
      "grad_norm": 0.547532856464386,
      "learning_rate": 3.3413333333333334e-05,
      "loss": 0.0024,
      "step": 12440
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.37914690375328064,
      "learning_rate": 3.3400000000000005e-05,
      "loss": 0.0039,
      "step": 12450
    },
    {
      "epoch": 0.6645333333333333,
      "grad_norm": 0.12654952704906464,
      "learning_rate": 3.338666666666666e-05,
      "loss": 0.0025,
      "step": 12460
    },
    {
      "epoch": 0.6650666666666667,
      "grad_norm": 0.3014950454235077,
      "learning_rate": 3.3373333333333335e-05,
      "loss": 0.0028,
      "step": 12470
    },
    {
      "epoch": 0.6656,
      "grad_norm": 0.3706735074520111,
      "learning_rate": 3.336e-05,
      "loss": 0.0031,
      "step": 12480
    },
    {
      "epoch": 0.6661333333333334,
      "grad_norm": 0.2577023208141327,
      "learning_rate": 3.334666666666667e-05,
      "loss": 0.0028,
      "step": 12490
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.13489221036434174,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.0028,
      "step": 12500
    },
    {
      "epoch": 0.6672,
      "grad_norm": 0.21713773906230927,
      "learning_rate": 3.332e-05,
      "loss": 0.0023,
      "step": 12510
    },
    {
      "epoch": 0.6677333333333333,
      "grad_norm": 0.3582010865211487,
      "learning_rate": 3.330666666666667e-05,
      "loss": 0.0035,
      "step": 12520
    },
    {
      "epoch": 0.6682666666666667,
      "grad_norm": 0.2815622389316559,
      "learning_rate": 3.3293333333333335e-05,
      "loss": 0.003,
      "step": 12530
    },
    {
      "epoch": 0.6688,
      "grad_norm": 0.44857925176620483,
      "learning_rate": 3.328e-05,
      "loss": 0.0038,
      "step": 12540
    },
    {
      "epoch": 0.6693333333333333,
      "grad_norm": 0.31401193141937256,
      "learning_rate": 3.326666666666667e-05,
      "loss": 0.0028,
      "step": 12550
    },
    {
      "epoch": 0.6698666666666667,
      "grad_norm": 0.37996798753738403,
      "learning_rate": 3.3253333333333336e-05,
      "loss": 0.0032,
      "step": 12560
    },
    {
      "epoch": 0.6704,
      "grad_norm": 0.41705426573753357,
      "learning_rate": 3.324e-05,
      "loss": 0.0022,
      "step": 12570
    },
    {
      "epoch": 0.6709333333333334,
      "grad_norm": 0.3633202612400055,
      "learning_rate": 3.3226666666666665e-05,
      "loss": 0.0046,
      "step": 12580
    },
    {
      "epoch": 0.6714666666666667,
      "grad_norm": 0.20509609580039978,
      "learning_rate": 3.3213333333333336e-05,
      "loss": 0.0035,
      "step": 12590
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.4572484493255615,
      "learning_rate": 3.32e-05,
      "loss": 0.0026,
      "step": 12600
    },
    {
      "epoch": 0.6725333333333333,
      "grad_norm": 0.5993439555168152,
      "learning_rate": 3.3186666666666665e-05,
      "loss": 0.0035,
      "step": 12610
    },
    {
      "epoch": 0.6730666666666667,
      "grad_norm": 0.17458079755306244,
      "learning_rate": 3.3173333333333336e-05,
      "loss": 0.0022,
      "step": 12620
    },
    {
      "epoch": 0.6736,
      "grad_norm": 0.44091030955314636,
      "learning_rate": 3.316e-05,
      "loss": 0.0019,
      "step": 12630
    },
    {
      "epoch": 0.6741333333333334,
      "grad_norm": 0.7188588380813599,
      "learning_rate": 3.314666666666667e-05,
      "loss": 0.0016,
      "step": 12640
    },
    {
      "epoch": 0.6746666666666666,
      "grad_norm": 0.3787354528903961,
      "learning_rate": 3.313333333333333e-05,
      "loss": 0.0032,
      "step": 12650
    },
    {
      "epoch": 0.6752,
      "grad_norm": 0.31015515327453613,
      "learning_rate": 3.312e-05,
      "loss": 0.0025,
      "step": 12660
    },
    {
      "epoch": 0.6757333333333333,
      "grad_norm": 0.1235489472746849,
      "learning_rate": 3.3106666666666666e-05,
      "loss": 0.0021,
      "step": 12670
    },
    {
      "epoch": 0.6762666666666667,
      "grad_norm": 0.13477076590061188,
      "learning_rate": 3.309333333333334e-05,
      "loss": 0.0021,
      "step": 12680
    },
    {
      "epoch": 0.6768,
      "grad_norm": 0.291976660490036,
      "learning_rate": 3.308e-05,
      "loss": 0.002,
      "step": 12690
    },
    {
      "epoch": 0.6773333333333333,
      "grad_norm": 0.210173562169075,
      "learning_rate": 3.3066666666666666e-05,
      "loss": 0.0021,
      "step": 12700
    },
    {
      "epoch": 0.6778666666666666,
      "grad_norm": 0.40362390875816345,
      "learning_rate": 3.305333333333334e-05,
      "loss": 0.0029,
      "step": 12710
    },
    {
      "epoch": 0.6784,
      "grad_norm": 0.17227518558502197,
      "learning_rate": 3.304e-05,
      "loss": 0.0021,
      "step": 12720
    },
    {
      "epoch": 0.6789333333333334,
      "grad_norm": 0.18717463314533234,
      "learning_rate": 3.302666666666667e-05,
      "loss": 0.0035,
      "step": 12730
    },
    {
      "epoch": 0.6794666666666667,
      "grad_norm": 0.1591874063014984,
      "learning_rate": 3.301333333333334e-05,
      "loss": 0.0028,
      "step": 12740
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.5092875957489014,
      "learning_rate": 3.3e-05,
      "loss": 0.0026,
      "step": 12750
    },
    {
      "epoch": 0.6805333333333333,
      "grad_norm": 0.23848651349544525,
      "learning_rate": 3.298666666666667e-05,
      "loss": 0.0023,
      "step": 12760
    },
    {
      "epoch": 0.6810666666666667,
      "grad_norm": 0.21665583550930023,
      "learning_rate": 3.297333333333333e-05,
      "loss": 0.002,
      "step": 12770
    },
    {
      "epoch": 0.6816,
      "grad_norm": 0.22339703142642975,
      "learning_rate": 3.296e-05,
      "loss": 0.0043,
      "step": 12780
    },
    {
      "epoch": 0.6821333333333334,
      "grad_norm": 0.505957841873169,
      "learning_rate": 3.294666666666667e-05,
      "loss": 0.0042,
      "step": 12790
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 0.5288156867027283,
      "learning_rate": 3.293333333333333e-05,
      "loss": 0.0024,
      "step": 12800
    },
    {
      "epoch": 0.6832,
      "grad_norm": 0.28450801968574524,
      "learning_rate": 3.292e-05,
      "loss": 0.0025,
      "step": 12810
    },
    {
      "epoch": 0.6837333333333333,
      "grad_norm": 0.5445160269737244,
      "learning_rate": 3.290666666666667e-05,
      "loss": 0.0032,
      "step": 12820
    },
    {
      "epoch": 0.6842666666666667,
      "grad_norm": 0.1961830109357834,
      "learning_rate": 3.289333333333334e-05,
      "loss": 0.0032,
      "step": 12830
    },
    {
      "epoch": 0.6848,
      "grad_norm": 0.25923994183540344,
      "learning_rate": 3.288e-05,
      "loss": 0.002,
      "step": 12840
    },
    {
      "epoch": 0.6853333333333333,
      "grad_norm": 0.5839048624038696,
      "learning_rate": 3.286666666666667e-05,
      "loss": 0.0036,
      "step": 12850
    },
    {
      "epoch": 0.6858666666666666,
      "grad_norm": 0.1487579643726349,
      "learning_rate": 3.285333333333333e-05,
      "loss": 0.0033,
      "step": 12860
    },
    {
      "epoch": 0.6864,
      "grad_norm": 0.612231433391571,
      "learning_rate": 3.2840000000000004e-05,
      "loss": 0.0024,
      "step": 12870
    },
    {
      "epoch": 0.6869333333333333,
      "grad_norm": 0.42380115389823914,
      "learning_rate": 3.282666666666667e-05,
      "loss": 0.0029,
      "step": 12880
    },
    {
      "epoch": 0.6874666666666667,
      "grad_norm": 0.35553762316703796,
      "learning_rate": 3.281333333333333e-05,
      "loss": 0.0036,
      "step": 12890
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.42189332842826843,
      "learning_rate": 3.2800000000000004e-05,
      "loss": 0.002,
      "step": 12900
    },
    {
      "epoch": 0.6885333333333333,
      "grad_norm": 0.26593607664108276,
      "learning_rate": 3.278666666666666e-05,
      "loss": 0.0037,
      "step": 12910
    },
    {
      "epoch": 0.6890666666666667,
      "grad_norm": 0.1801590472459793,
      "learning_rate": 3.2773333333333334e-05,
      "loss": 0.0022,
      "step": 12920
    },
    {
      "epoch": 0.6896,
      "grad_norm": 0.35217851400375366,
      "learning_rate": 3.2760000000000005e-05,
      "loss": 0.0018,
      "step": 12930
    },
    {
      "epoch": 0.6901333333333334,
      "grad_norm": 0.12074960023164749,
      "learning_rate": 3.274666666666667e-05,
      "loss": 0.0029,
      "step": 12940
    },
    {
      "epoch": 0.6906666666666667,
      "grad_norm": 0.22341980040073395,
      "learning_rate": 3.2733333333333334e-05,
      "loss": 0.0018,
      "step": 12950
    },
    {
      "epoch": 0.6912,
      "grad_norm": 0.4214315116405487,
      "learning_rate": 3.272e-05,
      "loss": 0.0027,
      "step": 12960
    },
    {
      "epoch": 0.6917333333333333,
      "grad_norm": 0.16264966130256653,
      "learning_rate": 3.270666666666667e-05,
      "loss": 0.0022,
      "step": 12970
    },
    {
      "epoch": 0.6922666666666667,
      "grad_norm": 0.30128809809684753,
      "learning_rate": 3.2693333333333334e-05,
      "loss": 0.0024,
      "step": 12980
    },
    {
      "epoch": 0.6928,
      "grad_norm": 0.27272120118141174,
      "learning_rate": 3.268e-05,
      "loss": 0.0025,
      "step": 12990
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.39177051186561584,
      "learning_rate": 3.266666666666667e-05,
      "loss": 0.0031,
      "step": 13000
    },
    {
      "epoch": 0.6938666666666666,
      "grad_norm": 0.22788360714912415,
      "learning_rate": 3.2653333333333335e-05,
      "loss": 0.0032,
      "step": 13010
    },
    {
      "epoch": 0.6944,
      "grad_norm": 0.250436007976532,
      "learning_rate": 3.2640000000000006e-05,
      "loss": 0.0023,
      "step": 13020
    },
    {
      "epoch": 0.6949333333333333,
      "grad_norm": 0.20431970059871674,
      "learning_rate": 3.2626666666666664e-05,
      "loss": 0.003,
      "step": 13030
    },
    {
      "epoch": 0.6954666666666667,
      "grad_norm": 0.45549681782722473,
      "learning_rate": 3.2613333333333335e-05,
      "loss": 0.0024,
      "step": 13040
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.4449187219142914,
      "learning_rate": 3.26e-05,
      "loss": 0.0019,
      "step": 13050
    },
    {
      "epoch": 0.6965333333333333,
      "grad_norm": 0.3195625841617584,
      "learning_rate": 3.258666666666667e-05,
      "loss": 0.0028,
      "step": 13060
    },
    {
      "epoch": 0.6970666666666666,
      "grad_norm": 0.2757554352283478,
      "learning_rate": 3.2573333333333335e-05,
      "loss": 0.0029,
      "step": 13070
    },
    {
      "epoch": 0.6976,
      "grad_norm": 0.4567379355430603,
      "learning_rate": 3.256e-05,
      "loss": 0.0033,
      "step": 13080
    },
    {
      "epoch": 0.6981333333333334,
      "grad_norm": 0.3498157560825348,
      "learning_rate": 3.254666666666667e-05,
      "loss": 0.0034,
      "step": 13090
    },
    {
      "epoch": 0.6986666666666667,
      "grad_norm": 0.5846946239471436,
      "learning_rate": 3.253333333333333e-05,
      "loss": 0.0022,
      "step": 13100
    },
    {
      "epoch": 0.6992,
      "grad_norm": 0.11729605495929718,
      "learning_rate": 3.252e-05,
      "loss": 0.0023,
      "step": 13110
    },
    {
      "epoch": 0.6997333333333333,
      "grad_norm": 0.11389942467212677,
      "learning_rate": 3.250666666666667e-05,
      "loss": 0.0023,
      "step": 13120
    },
    {
      "epoch": 0.7002666666666667,
      "grad_norm": 0.1529749184846878,
      "learning_rate": 3.2493333333333336e-05,
      "loss": 0.0018,
      "step": 13130
    },
    {
      "epoch": 0.7008,
      "grad_norm": 0.43749910593032837,
      "learning_rate": 3.248e-05,
      "loss": 0.0028,
      "step": 13140
    },
    {
      "epoch": 0.7013333333333334,
      "grad_norm": 0.24416662752628326,
      "learning_rate": 3.2466666666666665e-05,
      "loss": 0.0025,
      "step": 13150
    },
    {
      "epoch": 0.7018666666666666,
      "grad_norm": 0.09144103527069092,
      "learning_rate": 3.2453333333333337e-05,
      "loss": 0.0021,
      "step": 13160
    },
    {
      "epoch": 0.7024,
      "grad_norm": 0.10984686017036438,
      "learning_rate": 3.244e-05,
      "loss": 0.003,
      "step": 13170
    },
    {
      "epoch": 0.7029333333333333,
      "grad_norm": 0.24616919457912445,
      "learning_rate": 3.2426666666666666e-05,
      "loss": 0.0032,
      "step": 13180
    },
    {
      "epoch": 0.7034666666666667,
      "grad_norm": 0.27387672662734985,
      "learning_rate": 3.241333333333334e-05,
      "loss": 0.0024,
      "step": 13190
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.2482432723045349,
      "learning_rate": 3.24e-05,
      "loss": 0.0027,
      "step": 13200
    },
    {
      "epoch": 0.7045333333333333,
      "grad_norm": 0.09626336395740509,
      "learning_rate": 3.238666666666667e-05,
      "loss": 0.0021,
      "step": 13210
    },
    {
      "epoch": 0.7050666666666666,
      "grad_norm": 0.13809064030647278,
      "learning_rate": 3.237333333333333e-05,
      "loss": 0.0024,
      "step": 13220
    },
    {
      "epoch": 0.7056,
      "grad_norm": 0.37710925936698914,
      "learning_rate": 3.236e-05,
      "loss": 0.0025,
      "step": 13230
    },
    {
      "epoch": 0.7061333333333333,
      "grad_norm": 0.5070495009422302,
      "learning_rate": 3.2346666666666666e-05,
      "loss": 0.002,
      "step": 13240
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.14802339673042297,
      "learning_rate": 3.233333333333333e-05,
      "loss": 0.0022,
      "step": 13250
    },
    {
      "epoch": 0.7072,
      "grad_norm": 0.45048171281814575,
      "learning_rate": 3.232e-05,
      "loss": 0.0021,
      "step": 13260
    },
    {
      "epoch": 0.7077333333333333,
      "grad_norm": 0.12310396134853363,
      "learning_rate": 3.230666666666667e-05,
      "loss": 0.0023,
      "step": 13270
    },
    {
      "epoch": 0.7082666666666667,
      "grad_norm": 0.3708621561527252,
      "learning_rate": 3.229333333333334e-05,
      "loss": 0.0028,
      "step": 13280
    },
    {
      "epoch": 0.7088,
      "grad_norm": 0.33235570788383484,
      "learning_rate": 3.2279999999999996e-05,
      "loss": 0.0029,
      "step": 13290
    },
    {
      "epoch": 0.7093333333333334,
      "grad_norm": 0.362153559923172,
      "learning_rate": 3.226666666666667e-05,
      "loss": 0.0023,
      "step": 13300
    },
    {
      "epoch": 0.7098666666666666,
      "grad_norm": 0.16173261404037476,
      "learning_rate": 3.225333333333334e-05,
      "loss": 0.0027,
      "step": 13310
    },
    {
      "epoch": 0.7104,
      "grad_norm": 0.2803870141506195,
      "learning_rate": 3.224e-05,
      "loss": 0.0027,
      "step": 13320
    },
    {
      "epoch": 0.7109333333333333,
      "grad_norm": 0.20100779831409454,
      "learning_rate": 3.222666666666667e-05,
      "loss": 0.0025,
      "step": 13330
    },
    {
      "epoch": 0.7114666666666667,
      "grad_norm": 0.11403660476207733,
      "learning_rate": 3.221333333333333e-05,
      "loss": 0.0022,
      "step": 13340
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.13344161212444305,
      "learning_rate": 3.2200000000000003e-05,
      "loss": 0.003,
      "step": 13350
    },
    {
      "epoch": 0.7125333333333334,
      "grad_norm": 0.8074027895927429,
      "learning_rate": 3.218666666666667e-05,
      "loss": 0.0029,
      "step": 13360
    },
    {
      "epoch": 0.7130666666666666,
      "grad_norm": 0.09843521565198898,
      "learning_rate": 3.217333333333333e-05,
      "loss": 0.0039,
      "step": 13370
    },
    {
      "epoch": 0.7136,
      "grad_norm": 0.1594793200492859,
      "learning_rate": 3.2160000000000004e-05,
      "loss": 0.0036,
      "step": 13380
    },
    {
      "epoch": 0.7141333333333333,
      "grad_norm": 0.09323176741600037,
      "learning_rate": 3.214666666666667e-05,
      "loss": 0.0027,
      "step": 13390
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 0.3264860510826111,
      "learning_rate": 3.213333333333334e-05,
      "loss": 0.0018,
      "step": 13400
    },
    {
      "epoch": 0.7152,
      "grad_norm": 0.4601706266403198,
      "learning_rate": 3.212e-05,
      "loss": 0.004,
      "step": 13410
    },
    {
      "epoch": 0.7157333333333333,
      "grad_norm": 0.13251851499080658,
      "learning_rate": 3.210666666666667e-05,
      "loss": 0.003,
      "step": 13420
    },
    {
      "epoch": 0.7162666666666667,
      "grad_norm": 0.34234681725502014,
      "learning_rate": 3.209333333333333e-05,
      "loss": 0.0027,
      "step": 13430
    },
    {
      "epoch": 0.7168,
      "grad_norm": 0.436381071805954,
      "learning_rate": 3.208e-05,
      "loss": 0.0022,
      "step": 13440
    },
    {
      "epoch": 0.7173333333333334,
      "grad_norm": 0.3793080449104309,
      "learning_rate": 3.206666666666667e-05,
      "loss": 0.0027,
      "step": 13450
    },
    {
      "epoch": 0.7178666666666667,
      "grad_norm": 0.09391012787818909,
      "learning_rate": 3.2053333333333334e-05,
      "loss": 0.0018,
      "step": 13460
    },
    {
      "epoch": 0.7184,
      "grad_norm": 0.4054338037967682,
      "learning_rate": 3.2040000000000005e-05,
      "loss": 0.0018,
      "step": 13470
    },
    {
      "epoch": 0.7189333333333333,
      "grad_norm": 0.6118068099021912,
      "learning_rate": 3.202666666666666e-05,
      "loss": 0.0033,
      "step": 13480
    },
    {
      "epoch": 0.7194666666666667,
      "grad_norm": 0.4143858253955841,
      "learning_rate": 3.2013333333333334e-05,
      "loss": 0.0024,
      "step": 13490
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.2779375910758972,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.0021,
      "step": 13500
    },
    {
      "epoch": 0.7205333333333334,
      "grad_norm": 0.15580534934997559,
      "learning_rate": 3.198666666666667e-05,
      "loss": 0.0021,
      "step": 13510
    },
    {
      "epoch": 0.7210666666666666,
      "grad_norm": 0.11645115911960602,
      "learning_rate": 3.1973333333333334e-05,
      "loss": 0.0041,
      "step": 13520
    },
    {
      "epoch": 0.7216,
      "grad_norm": 0.33022773265838623,
      "learning_rate": 3.196e-05,
      "loss": 0.0017,
      "step": 13530
    },
    {
      "epoch": 0.7221333333333333,
      "grad_norm": 0.1985504925251007,
      "learning_rate": 3.194666666666667e-05,
      "loss": 0.002,
      "step": 13540
    },
    {
      "epoch": 0.7226666666666667,
      "grad_norm": 0.24966439604759216,
      "learning_rate": 3.1933333333333335e-05,
      "loss": 0.0023,
      "step": 13550
    },
    {
      "epoch": 0.7232,
      "grad_norm": 0.7568218111991882,
      "learning_rate": 3.192e-05,
      "loss": 0.0021,
      "step": 13560
    },
    {
      "epoch": 0.7237333333333333,
      "grad_norm": 0.27401718497276306,
      "learning_rate": 3.190666666666667e-05,
      "loss": 0.0029,
      "step": 13570
    },
    {
      "epoch": 0.7242666666666666,
      "grad_norm": 0.5251982808113098,
      "learning_rate": 3.1893333333333335e-05,
      "loss": 0.0022,
      "step": 13580
    },
    {
      "epoch": 0.7248,
      "grad_norm": 0.2877601087093353,
      "learning_rate": 3.188e-05,
      "loss": 0.0036,
      "step": 13590
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 0.18446895480155945,
      "learning_rate": 3.1866666666666664e-05,
      "loss": 0.002,
      "step": 13600
    },
    {
      "epoch": 0.7258666666666667,
      "grad_norm": 0.1469956636428833,
      "learning_rate": 3.1853333333333336e-05,
      "loss": 0.0038,
      "step": 13610
    },
    {
      "epoch": 0.7264,
      "grad_norm": 0.4420537054538727,
      "learning_rate": 3.184e-05,
      "loss": 0.0028,
      "step": 13620
    },
    {
      "epoch": 0.7269333333333333,
      "grad_norm": 0.7554474472999573,
      "learning_rate": 3.1826666666666665e-05,
      "loss": 0.0018,
      "step": 13630
    },
    {
      "epoch": 0.7274666666666667,
      "grad_norm": 0.7566989660263062,
      "learning_rate": 3.1813333333333336e-05,
      "loss": 0.0021,
      "step": 13640
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.5343483090400696,
      "learning_rate": 3.18e-05,
      "loss": 0.0023,
      "step": 13650
    },
    {
      "epoch": 0.7285333333333334,
      "grad_norm": 0.6649266481399536,
      "learning_rate": 3.178666666666667e-05,
      "loss": 0.0021,
      "step": 13660
    },
    {
      "epoch": 0.7290666666666666,
      "grad_norm": 0.7469481229782104,
      "learning_rate": 3.177333333333333e-05,
      "loss": 0.0023,
      "step": 13670
    },
    {
      "epoch": 0.7296,
      "grad_norm": 0.254258930683136,
      "learning_rate": 3.176e-05,
      "loss": 0.0043,
      "step": 13680
    },
    {
      "epoch": 0.7301333333333333,
      "grad_norm": 0.7932577133178711,
      "learning_rate": 3.174666666666667e-05,
      "loss": 0.0028,
      "step": 13690
    },
    {
      "epoch": 0.7306666666666667,
      "grad_norm": 0.3442600667476654,
      "learning_rate": 3.173333333333334e-05,
      "loss": 0.0024,
      "step": 13700
    },
    {
      "epoch": 0.7312,
      "grad_norm": 0.4021165668964386,
      "learning_rate": 3.172e-05,
      "loss": 0.0023,
      "step": 13710
    },
    {
      "epoch": 0.7317333333333333,
      "grad_norm": 0.09527447819709778,
      "learning_rate": 3.1706666666666666e-05,
      "loss": 0.0025,
      "step": 13720
    },
    {
      "epoch": 0.7322666666666666,
      "grad_norm": 0.40334364771842957,
      "learning_rate": 3.169333333333334e-05,
      "loss": 0.0018,
      "step": 13730
    },
    {
      "epoch": 0.7328,
      "grad_norm": 0.5881790518760681,
      "learning_rate": 3.168e-05,
      "loss": 0.0024,
      "step": 13740
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.5436340570449829,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 0.0022,
      "step": 13750
    },
    {
      "epoch": 0.7338666666666667,
      "grad_norm": 0.172232985496521,
      "learning_rate": 3.165333333333334e-05,
      "loss": 0.0028,
      "step": 13760
    },
    {
      "epoch": 0.7344,
      "grad_norm": 0.10529445111751556,
      "learning_rate": 3.164e-05,
      "loss": 0.0028,
      "step": 13770
    },
    {
      "epoch": 0.7349333333333333,
      "grad_norm": 0.3697205185890198,
      "learning_rate": 3.1626666666666667e-05,
      "loss": 0.002,
      "step": 13780
    },
    {
      "epoch": 0.7354666666666667,
      "grad_norm": 0.6318446397781372,
      "learning_rate": 3.161333333333333e-05,
      "loss": 0.0028,
      "step": 13790
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.13089659810066223,
      "learning_rate": 3.16e-05,
      "loss": 0.0021,
      "step": 13800
    },
    {
      "epoch": 0.7365333333333334,
      "grad_norm": 0.17337797582149506,
      "learning_rate": 3.158666666666667e-05,
      "loss": 0.0022,
      "step": 13810
    },
    {
      "epoch": 0.7370666666666666,
      "grad_norm": 0.2014673352241516,
      "learning_rate": 3.157333333333333e-05,
      "loss": 0.003,
      "step": 13820
    },
    {
      "epoch": 0.7376,
      "grad_norm": 0.3164186477661133,
      "learning_rate": 3.156e-05,
      "loss": 0.0022,
      "step": 13830
    },
    {
      "epoch": 0.7381333333333333,
      "grad_norm": 0.17720568180084229,
      "learning_rate": 3.154666666666667e-05,
      "loss": 0.0025,
      "step": 13840
    },
    {
      "epoch": 0.7386666666666667,
      "grad_norm": 0.4668257534503937,
      "learning_rate": 3.153333333333334e-05,
      "loss": 0.0026,
      "step": 13850
    },
    {
      "epoch": 0.7392,
      "grad_norm": 0.12286993116140366,
      "learning_rate": 3.1519999999999996e-05,
      "loss": 0.0029,
      "step": 13860
    },
    {
      "epoch": 0.7397333333333334,
      "grad_norm": 0.34792962670326233,
      "learning_rate": 3.150666666666667e-05,
      "loss": 0.0023,
      "step": 13870
    },
    {
      "epoch": 0.7402666666666666,
      "grad_norm": 0.3602064251899719,
      "learning_rate": 3.149333333333334e-05,
      "loss": 0.0021,
      "step": 13880
    },
    {
      "epoch": 0.7408,
      "grad_norm": 0.1472027450799942,
      "learning_rate": 3.1480000000000004e-05,
      "loss": 0.0019,
      "step": 13890
    },
    {
      "epoch": 0.7413333333333333,
      "grad_norm": 0.23843343555927277,
      "learning_rate": 3.146666666666667e-05,
      "loss": 0.0037,
      "step": 13900
    },
    {
      "epoch": 0.7418666666666667,
      "grad_norm": 0.17331592738628387,
      "learning_rate": 3.145333333333333e-05,
      "loss": 0.0024,
      "step": 13910
    },
    {
      "epoch": 0.7424,
      "grad_norm": 0.5385261178016663,
      "learning_rate": 3.1440000000000004e-05,
      "loss": 0.003,
      "step": 13920
    },
    {
      "epoch": 0.7429333333333333,
      "grad_norm": 0.33348074555397034,
      "learning_rate": 3.142666666666667e-05,
      "loss": 0.0029,
      "step": 13930
    },
    {
      "epoch": 0.7434666666666667,
      "grad_norm": 0.12724797427654266,
      "learning_rate": 3.141333333333333e-05,
      "loss": 0.0025,
      "step": 13940
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.28162941336631775,
      "learning_rate": 3.1400000000000004e-05,
      "loss": 0.002,
      "step": 13950
    },
    {
      "epoch": 0.7445333333333334,
      "grad_norm": 0.4901973605155945,
      "learning_rate": 3.138666666666667e-05,
      "loss": 0.0027,
      "step": 13960
    },
    {
      "epoch": 0.7450666666666667,
      "grad_norm": 0.9143580198287964,
      "learning_rate": 3.137333333333333e-05,
      "loss": 0.0023,
      "step": 13970
    },
    {
      "epoch": 0.7456,
      "grad_norm": 0.4211803674697876,
      "learning_rate": 3.136e-05,
      "loss": 0.0042,
      "step": 13980
    },
    {
      "epoch": 0.7461333333333333,
      "grad_norm": 0.31798791885375977,
      "learning_rate": 3.134666666666667e-05,
      "loss": 0.0026,
      "step": 13990
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.38309338688850403,
      "learning_rate": 3.1333333333333334e-05,
      "loss": 0.0039,
      "step": 14000
    },
    {
      "epoch": 0.7472,
      "grad_norm": 0.5125096440315247,
      "learning_rate": 3.132e-05,
      "loss": 0.0033,
      "step": 14010
    },
    {
      "epoch": 0.7477333333333334,
      "grad_norm": 0.17886807024478912,
      "learning_rate": 3.130666666666667e-05,
      "loss": 0.0031,
      "step": 14020
    },
    {
      "epoch": 0.7482666666666666,
      "grad_norm": 0.3673454225063324,
      "learning_rate": 3.1293333333333334e-05,
      "loss": 0.0026,
      "step": 14030
    },
    {
      "epoch": 0.7488,
      "grad_norm": 0.14866606891155243,
      "learning_rate": 3.1280000000000005e-05,
      "loss": 0.0024,
      "step": 14040
    },
    {
      "epoch": 0.7493333333333333,
      "grad_norm": 0.19918306171894073,
      "learning_rate": 3.126666666666666e-05,
      "loss": 0.0023,
      "step": 14050
    },
    {
      "epoch": 0.7498666666666667,
      "grad_norm": 0.7024479508399963,
      "learning_rate": 3.1253333333333335e-05,
      "loss": 0.0033,
      "step": 14060
    },
    {
      "epoch": 0.7504,
      "grad_norm": 0.15028610825538635,
      "learning_rate": 3.1240000000000006e-05,
      "loss": 0.002,
      "step": 14070
    },
    {
      "epoch": 0.7509333333333333,
      "grad_norm": 0.19475658237934113,
      "learning_rate": 3.122666666666667e-05,
      "loss": 0.0021,
      "step": 14080
    },
    {
      "epoch": 0.7514666666666666,
      "grad_norm": 0.42205074429512024,
      "learning_rate": 3.1213333333333335e-05,
      "loss": 0.0028,
      "step": 14090
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.2446720004081726,
      "learning_rate": 3.12e-05,
      "loss": 0.0028,
      "step": 14100
    },
    {
      "epoch": 0.7525333333333334,
      "grad_norm": 0.22764116525650024,
      "learning_rate": 3.118666666666667e-05,
      "loss": 0.004,
      "step": 14110
    },
    {
      "epoch": 0.7530666666666667,
      "grad_norm": 0.45529717206954956,
      "learning_rate": 3.1173333333333335e-05,
      "loss": 0.0032,
      "step": 14120
    },
    {
      "epoch": 0.7536,
      "grad_norm": 0.2519284188747406,
      "learning_rate": 3.116e-05,
      "loss": 0.0033,
      "step": 14130
    },
    {
      "epoch": 0.7541333333333333,
      "grad_norm": 0.38535839319229126,
      "learning_rate": 3.114666666666667e-05,
      "loss": 0.0021,
      "step": 14140
    },
    {
      "epoch": 0.7546666666666667,
      "grad_norm": 0.12335693091154099,
      "learning_rate": 3.1133333333333336e-05,
      "loss": 0.003,
      "step": 14150
    },
    {
      "epoch": 0.7552,
      "grad_norm": 0.08887220919132233,
      "learning_rate": 3.112e-05,
      "loss": 0.002,
      "step": 14160
    },
    {
      "epoch": 0.7557333333333334,
      "grad_norm": 0.49841442704200745,
      "learning_rate": 3.1106666666666665e-05,
      "loss": 0.0026,
      "step": 14170
    },
    {
      "epoch": 0.7562666666666666,
      "grad_norm": 0.6762878894805908,
      "learning_rate": 3.1093333333333336e-05,
      "loss": 0.0022,
      "step": 14180
    },
    {
      "epoch": 0.7568,
      "grad_norm": 0.6890597939491272,
      "learning_rate": 3.108e-05,
      "loss": 0.002,
      "step": 14190
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 0.4048044681549072,
      "learning_rate": 3.1066666666666665e-05,
      "loss": 0.0046,
      "step": 14200
    },
    {
      "epoch": 0.7578666666666667,
      "grad_norm": 0.6085559129714966,
      "learning_rate": 3.1053333333333336e-05,
      "loss": 0.003,
      "step": 14210
    },
    {
      "epoch": 0.7584,
      "grad_norm": 0.1519288867712021,
      "learning_rate": 3.104e-05,
      "loss": 0.002,
      "step": 14220
    },
    {
      "epoch": 0.7589333333333333,
      "grad_norm": 0.6143193244934082,
      "learning_rate": 3.102666666666667e-05,
      "loss": 0.0022,
      "step": 14230
    },
    {
      "epoch": 0.7594666666666666,
      "grad_norm": 0.3585605025291443,
      "learning_rate": 3.101333333333333e-05,
      "loss": 0.0025,
      "step": 14240
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.14013521373271942,
      "learning_rate": 3.1e-05,
      "loss": 0.0033,
      "step": 14250
    },
    {
      "epoch": 0.7605333333333333,
      "grad_norm": 0.4417181611061096,
      "learning_rate": 3.098666666666667e-05,
      "loss": 0.0023,
      "step": 14260
    },
    {
      "epoch": 0.7610666666666667,
      "grad_norm": 0.40212228894233704,
      "learning_rate": 3.097333333333334e-05,
      "loss": 0.0018,
      "step": 14270
    },
    {
      "epoch": 0.7616,
      "grad_norm": 0.15213905274868011,
      "learning_rate": 3.096e-05,
      "loss": 0.0023,
      "step": 14280
    },
    {
      "epoch": 0.7621333333333333,
      "grad_norm": 0.6067153215408325,
      "learning_rate": 3.0946666666666666e-05,
      "loss": 0.002,
      "step": 14290
    },
    {
      "epoch": 0.7626666666666667,
      "grad_norm": 0.21396085619926453,
      "learning_rate": 3.093333333333334e-05,
      "loss": 0.0035,
      "step": 14300
    },
    {
      "epoch": 0.7632,
      "grad_norm": 0.5435894131660461,
      "learning_rate": 3.092e-05,
      "loss": 0.0023,
      "step": 14310
    },
    {
      "epoch": 0.7637333333333334,
      "grad_norm": 0.39204323291778564,
      "learning_rate": 3.090666666666667e-05,
      "loss": 0.0027,
      "step": 14320
    },
    {
      "epoch": 0.7642666666666666,
      "grad_norm": 0.3256594240665436,
      "learning_rate": 3.089333333333334e-05,
      "loss": 0.0018,
      "step": 14330
    },
    {
      "epoch": 0.7648,
      "grad_norm": 0.16690593957901,
      "learning_rate": 3.088e-05,
      "loss": 0.0031,
      "step": 14340
    },
    {
      "epoch": 0.7653333333333333,
      "grad_norm": 0.22210440039634705,
      "learning_rate": 3.086666666666667e-05,
      "loss": 0.003,
      "step": 14350
    },
    {
      "epoch": 0.7658666666666667,
      "grad_norm": 0.18183839321136475,
      "learning_rate": 3.085333333333333e-05,
      "loss": 0.0031,
      "step": 14360
    },
    {
      "epoch": 0.7664,
      "grad_norm": 0.32512831687927246,
      "learning_rate": 3.084e-05,
      "loss": 0.0036,
      "step": 14370
    },
    {
      "epoch": 0.7669333333333334,
      "grad_norm": 0.603514552116394,
      "learning_rate": 3.082666666666667e-05,
      "loss": 0.0026,
      "step": 14380
    },
    {
      "epoch": 0.7674666666666666,
      "grad_norm": 0.48895731568336487,
      "learning_rate": 3.081333333333333e-05,
      "loss": 0.0022,
      "step": 14390
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.49126338958740234,
      "learning_rate": 3.08e-05,
      "loss": 0.0025,
      "step": 14400
    },
    {
      "epoch": 0.7685333333333333,
      "grad_norm": 0.5155563354492188,
      "learning_rate": 3.078666666666667e-05,
      "loss": 0.0031,
      "step": 14410
    },
    {
      "epoch": 0.7690666666666667,
      "grad_norm": 0.31728917360305786,
      "learning_rate": 3.077333333333334e-05,
      "loss": 0.0025,
      "step": 14420
    },
    {
      "epoch": 0.7696,
      "grad_norm": 0.8091334104537964,
      "learning_rate": 3.076e-05,
      "loss": 0.0038,
      "step": 14430
    },
    {
      "epoch": 0.7701333333333333,
      "grad_norm": 0.14142346382141113,
      "learning_rate": 3.074666666666667e-05,
      "loss": 0.002,
      "step": 14440
    },
    {
      "epoch": 0.7706666666666667,
      "grad_norm": 0.8427842855453491,
      "learning_rate": 3.073333333333334e-05,
      "loss": 0.0024,
      "step": 14450
    },
    {
      "epoch": 0.7712,
      "grad_norm": 0.4561708867549896,
      "learning_rate": 3.072e-05,
      "loss": 0.0023,
      "step": 14460
    },
    {
      "epoch": 0.7717333333333334,
      "grad_norm": 0.36672645807266235,
      "learning_rate": 3.070666666666667e-05,
      "loss": 0.0032,
      "step": 14470
    },
    {
      "epoch": 0.7722666666666667,
      "grad_norm": 0.2306678146123886,
      "learning_rate": 3.069333333333333e-05,
      "loss": 0.0017,
      "step": 14480
    },
    {
      "epoch": 0.7728,
      "grad_norm": 0.17705416679382324,
      "learning_rate": 3.0680000000000004e-05,
      "loss": 0.0025,
      "step": 14490
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.25500044226646423,
      "learning_rate": 3.066666666666667e-05,
      "loss": 0.0027,
      "step": 14500
    },
    {
      "epoch": 0.7738666666666667,
      "grad_norm": 1.0800855159759521,
      "learning_rate": 3.0653333333333333e-05,
      "loss": 0.0034,
      "step": 14510
    },
    {
      "epoch": 0.7744,
      "grad_norm": 0.14676114916801453,
      "learning_rate": 3.0640000000000005e-05,
      "loss": 0.0023,
      "step": 14520
    },
    {
      "epoch": 0.7749333333333334,
      "grad_norm": 0.16069617867469788,
      "learning_rate": 3.062666666666667e-05,
      "loss": 0.0034,
      "step": 14530
    },
    {
      "epoch": 0.7754666666666666,
      "grad_norm": 0.5522734522819519,
      "learning_rate": 3.0613333333333334e-05,
      "loss": 0.0031,
      "step": 14540
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.12130341678857803,
      "learning_rate": 3.06e-05,
      "loss": 0.0021,
      "step": 14550
    },
    {
      "epoch": 0.7765333333333333,
      "grad_norm": 0.2903538942337036,
      "learning_rate": 3.058666666666667e-05,
      "loss": 0.003,
      "step": 14560
    },
    {
      "epoch": 0.7770666666666667,
      "grad_norm": 0.2433653473854065,
      "learning_rate": 3.0573333333333334e-05,
      "loss": 0.0027,
      "step": 14570
    },
    {
      "epoch": 0.7776,
      "grad_norm": 0.28988921642303467,
      "learning_rate": 3.056e-05,
      "loss": 0.0023,
      "step": 14580
    },
    {
      "epoch": 0.7781333333333333,
      "grad_norm": 0.3488461375236511,
      "learning_rate": 3.054666666666667e-05,
      "loss": 0.0035,
      "step": 14590
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 0.3760712146759033,
      "learning_rate": 3.0533333333333335e-05,
      "loss": 0.0017,
      "step": 14600
    },
    {
      "epoch": 0.7792,
      "grad_norm": 0.6806578040122986,
      "learning_rate": 3.0520000000000006e-05,
      "loss": 0.0024,
      "step": 14610
    },
    {
      "epoch": 0.7797333333333333,
      "grad_norm": 0.5646665096282959,
      "learning_rate": 3.0506666666666667e-05,
      "loss": 0.0022,
      "step": 14620
    },
    {
      "epoch": 0.7802666666666667,
      "grad_norm": 0.4402253329753876,
      "learning_rate": 3.0493333333333335e-05,
      "loss": 0.0047,
      "step": 14630
    },
    {
      "epoch": 0.7808,
      "grad_norm": 0.26300424337387085,
      "learning_rate": 3.0480000000000003e-05,
      "loss": 0.0021,
      "step": 14640
    },
    {
      "epoch": 0.7813333333333333,
      "grad_norm": 0.08704197406768799,
      "learning_rate": 3.0466666666666664e-05,
      "loss": 0.0021,
      "step": 14650
    },
    {
      "epoch": 0.7818666666666667,
      "grad_norm": 0.4744654595851898,
      "learning_rate": 3.0453333333333335e-05,
      "loss": 0.0022,
      "step": 14660
    },
    {
      "epoch": 0.7824,
      "grad_norm": 0.42124319076538086,
      "learning_rate": 3.0440000000000003e-05,
      "loss": 0.002,
      "step": 14670
    },
    {
      "epoch": 0.7829333333333334,
      "grad_norm": 0.1429063379764557,
      "learning_rate": 3.042666666666667e-05,
      "loss": 0.0036,
      "step": 14680
    },
    {
      "epoch": 0.7834666666666666,
      "grad_norm": 0.42197930812835693,
      "learning_rate": 3.0413333333333332e-05,
      "loss": 0.0026,
      "step": 14690
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.8777794241905212,
      "learning_rate": 3.04e-05,
      "loss": 0.0021,
      "step": 14700
    },
    {
      "epoch": 0.7845333333333333,
      "grad_norm": 0.06648759543895721,
      "learning_rate": 3.0386666666666668e-05,
      "loss": 0.0018,
      "step": 14710
    },
    {
      "epoch": 0.7850666666666667,
      "grad_norm": 0.402182400226593,
      "learning_rate": 3.0373333333333336e-05,
      "loss": 0.003,
      "step": 14720
    },
    {
      "epoch": 0.7856,
      "grad_norm": 0.44238120317459106,
      "learning_rate": 3.036e-05,
      "loss": 0.0018,
      "step": 14730
    },
    {
      "epoch": 0.7861333333333334,
      "grad_norm": 0.07505571842193604,
      "learning_rate": 3.034666666666667e-05,
      "loss": 0.0026,
      "step": 14740
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.29205626249313354,
      "learning_rate": 3.0333333333333337e-05,
      "loss": 0.0031,
      "step": 14750
    },
    {
      "epoch": 0.7872,
      "grad_norm": 0.4412517249584198,
      "learning_rate": 3.0320000000000004e-05,
      "loss": 0.0032,
      "step": 14760
    },
    {
      "epoch": 0.7877333333333333,
      "grad_norm": 0.16595099866390228,
      "learning_rate": 3.0306666666666666e-05,
      "loss": 0.0029,
      "step": 14770
    },
    {
      "epoch": 0.7882666666666667,
      "grad_norm": 0.23640693724155426,
      "learning_rate": 3.0293333333333334e-05,
      "loss": 0.0035,
      "step": 14780
    },
    {
      "epoch": 0.7888,
      "grad_norm": 0.212410569190979,
      "learning_rate": 3.028e-05,
      "loss": 0.0028,
      "step": 14790
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 0.3629639744758606,
      "learning_rate": 3.0266666666666666e-05,
      "loss": 0.0022,
      "step": 14800
    },
    {
      "epoch": 0.7898666666666667,
      "grad_norm": 0.719330370426178,
      "learning_rate": 3.0253333333333334e-05,
      "loss": 0.0022,
      "step": 14810
    },
    {
      "epoch": 0.7904,
      "grad_norm": 0.1258971095085144,
      "learning_rate": 3.0240000000000002e-05,
      "loss": 0.0028,
      "step": 14820
    },
    {
      "epoch": 0.7909333333333334,
      "grad_norm": 0.9258955717086792,
      "learning_rate": 3.022666666666667e-05,
      "loss": 0.0035,
      "step": 14830
    },
    {
      "epoch": 0.7914666666666667,
      "grad_norm": 0.3841967284679413,
      "learning_rate": 3.021333333333333e-05,
      "loss": 0.0036,
      "step": 14840
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.14956212043762207,
      "learning_rate": 3.02e-05,
      "loss": 0.0023,
      "step": 14850
    },
    {
      "epoch": 0.7925333333333333,
      "grad_norm": 0.10777687281370163,
      "learning_rate": 3.018666666666667e-05,
      "loss": 0.0026,
      "step": 14860
    },
    {
      "epoch": 0.7930666666666667,
      "grad_norm": 0.26887047290802,
      "learning_rate": 3.0173333333333338e-05,
      "loss": 0.003,
      "step": 14870
    },
    {
      "epoch": 0.7936,
      "grad_norm": 0.5253481864929199,
      "learning_rate": 3.016e-05,
      "loss": 0.002,
      "step": 14880
    },
    {
      "epoch": 0.7941333333333334,
      "grad_norm": 0.7274584174156189,
      "learning_rate": 3.0146666666666667e-05,
      "loss": 0.003,
      "step": 14890
    },
    {
      "epoch": 0.7946666666666666,
      "grad_norm": 0.10373905301094055,
      "learning_rate": 3.0133333333333335e-05,
      "loss": 0.0031,
      "step": 14900
    },
    {
      "epoch": 0.7952,
      "grad_norm": 0.3762049674987793,
      "learning_rate": 3.0120000000000003e-05,
      "loss": 0.0031,
      "step": 14910
    },
    {
      "epoch": 0.7957333333333333,
      "grad_norm": 0.3365006744861603,
      "learning_rate": 3.0106666666666668e-05,
      "loss": 0.003,
      "step": 14920
    },
    {
      "epoch": 0.7962666666666667,
      "grad_norm": 0.41080713272094727,
      "learning_rate": 3.0093333333333335e-05,
      "loss": 0.0021,
      "step": 14930
    },
    {
      "epoch": 0.7968,
      "grad_norm": 0.22805799543857574,
      "learning_rate": 3.0080000000000003e-05,
      "loss": 0.0028,
      "step": 14940
    },
    {
      "epoch": 0.7973333333333333,
      "grad_norm": 1.0219727754592896,
      "learning_rate": 3.006666666666667e-05,
      "loss": 0.003,
      "step": 14950
    },
    {
      "epoch": 0.7978666666666666,
      "grad_norm": 0.30361929535865784,
      "learning_rate": 3.0053333333333332e-05,
      "loss": 0.0029,
      "step": 14960
    },
    {
      "epoch": 0.7984,
      "grad_norm": 0.31158581376075745,
      "learning_rate": 3.004e-05,
      "loss": 0.0023,
      "step": 14970
    },
    {
      "epoch": 0.7989333333333334,
      "grad_norm": 0.31712624430656433,
      "learning_rate": 3.0026666666666668e-05,
      "loss": 0.0024,
      "step": 14980
    },
    {
      "epoch": 0.7994666666666667,
      "grad_norm": 0.42609089612960815,
      "learning_rate": 3.0013333333333333e-05,
      "loss": 0.002,
      "step": 14990
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.20117421448230743,
      "learning_rate": 3e-05,
      "loss": 0.003,
      "step": 15000
    },
    {
      "epoch": 0.8005333333333333,
      "grad_norm": 0.08819118142127991,
      "learning_rate": 2.998666666666667e-05,
      "loss": 0.0029,
      "step": 15010
    },
    {
      "epoch": 0.8010666666666667,
      "grad_norm": 0.3927434980869293,
      "learning_rate": 2.9973333333333337e-05,
      "loss": 0.0018,
      "step": 15020
    },
    {
      "epoch": 0.8016,
      "grad_norm": 0.23495252430438995,
      "learning_rate": 2.9959999999999998e-05,
      "loss": 0.0026,
      "step": 15030
    },
    {
      "epoch": 0.8021333333333334,
      "grad_norm": 0.38946107029914856,
      "learning_rate": 2.9946666666666666e-05,
      "loss": 0.003,
      "step": 15040
    },
    {
      "epoch": 0.8026666666666666,
      "grad_norm": 0.36962878704071045,
      "learning_rate": 2.9933333333333337e-05,
      "loss": 0.0039,
      "step": 15050
    },
    {
      "epoch": 0.8032,
      "grad_norm": 0.5545797944068909,
      "learning_rate": 2.9920000000000005e-05,
      "loss": 0.0023,
      "step": 15060
    },
    {
      "epoch": 0.8037333333333333,
      "grad_norm": 0.3136880099773407,
      "learning_rate": 2.9906666666666666e-05,
      "loss": 0.0026,
      "step": 15070
    },
    {
      "epoch": 0.8042666666666667,
      "grad_norm": 0.4155014753341675,
      "learning_rate": 2.9893333333333334e-05,
      "loss": 0.0022,
      "step": 15080
    },
    {
      "epoch": 0.8048,
      "grad_norm": 0.43906474113464355,
      "learning_rate": 2.9880000000000002e-05,
      "loss": 0.0027,
      "step": 15090
    },
    {
      "epoch": 0.8053333333333333,
      "grad_norm": 0.5116719007492065,
      "learning_rate": 2.986666666666667e-05,
      "loss": 0.0028,
      "step": 15100
    },
    {
      "epoch": 0.8058666666666666,
      "grad_norm": 0.6699109077453613,
      "learning_rate": 2.9853333333333334e-05,
      "loss": 0.0041,
      "step": 15110
    },
    {
      "epoch": 0.8064,
      "grad_norm": 0.2268301248550415,
      "learning_rate": 2.9840000000000002e-05,
      "loss": 0.0025,
      "step": 15120
    },
    {
      "epoch": 0.8069333333333333,
      "grad_norm": 0.18319863080978394,
      "learning_rate": 2.982666666666667e-05,
      "loss": 0.0023,
      "step": 15130
    },
    {
      "epoch": 0.8074666666666667,
      "grad_norm": 0.35359451174736023,
      "learning_rate": 2.981333333333333e-05,
      "loss": 0.0031,
      "step": 15140
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.07621165364980698,
      "learning_rate": 2.98e-05,
      "loss": 0.0024,
      "step": 15150
    },
    {
      "epoch": 0.8085333333333333,
      "grad_norm": 0.5258293747901917,
      "learning_rate": 2.9786666666666667e-05,
      "loss": 0.0023,
      "step": 15160
    },
    {
      "epoch": 0.8090666666666667,
      "grad_norm": 0.5308166146278381,
      "learning_rate": 2.9773333333333335e-05,
      "loss": 0.0026,
      "step": 15170
    },
    {
      "epoch": 0.8096,
      "grad_norm": 0.17103081941604614,
      "learning_rate": 2.976e-05,
      "loss": 0.0023,
      "step": 15180
    },
    {
      "epoch": 0.8101333333333334,
      "grad_norm": 0.12268592417240143,
      "learning_rate": 2.9746666666666668e-05,
      "loss": 0.0027,
      "step": 15190
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 0.3311447203159332,
      "learning_rate": 2.9733333333333336e-05,
      "loss": 0.0027,
      "step": 15200
    },
    {
      "epoch": 0.8112,
      "grad_norm": 0.2701486349105835,
      "learning_rate": 2.9720000000000003e-05,
      "loss": 0.0018,
      "step": 15210
    },
    {
      "epoch": 0.8117333333333333,
      "grad_norm": 0.4896974563598633,
      "learning_rate": 2.9706666666666665e-05,
      "loss": 0.003,
      "step": 15220
    },
    {
      "epoch": 0.8122666666666667,
      "grad_norm": 0.2015000581741333,
      "learning_rate": 2.9693333333333333e-05,
      "loss": 0.0026,
      "step": 15230
    },
    {
      "epoch": 0.8128,
      "grad_norm": 0.591305673122406,
      "learning_rate": 2.9680000000000004e-05,
      "loss": 0.0029,
      "step": 15240
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.4641059637069702,
      "learning_rate": 2.9666666666666672e-05,
      "loss": 0.0032,
      "step": 15250
    },
    {
      "epoch": 0.8138666666666666,
      "grad_norm": 0.3965276777744293,
      "learning_rate": 2.9653333333333333e-05,
      "loss": 0.0028,
      "step": 15260
    },
    {
      "epoch": 0.8144,
      "grad_norm": 0.4366981089115143,
      "learning_rate": 2.964e-05,
      "loss": 0.0026,
      "step": 15270
    },
    {
      "epoch": 0.8149333333333333,
      "grad_norm": 0.22269776463508606,
      "learning_rate": 2.962666666666667e-05,
      "loss": 0.0019,
      "step": 15280
    },
    {
      "epoch": 0.8154666666666667,
      "grad_norm": 0.4424121379852295,
      "learning_rate": 2.9613333333333337e-05,
      "loss": 0.0017,
      "step": 15290
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.10444624722003937,
      "learning_rate": 2.96e-05,
      "loss": 0.0024,
      "step": 15300
    },
    {
      "epoch": 0.8165333333333333,
      "grad_norm": 0.2810938060283661,
      "learning_rate": 2.958666666666667e-05,
      "loss": 0.003,
      "step": 15310
    },
    {
      "epoch": 0.8170666666666667,
      "grad_norm": 0.5123994946479797,
      "learning_rate": 2.9573333333333337e-05,
      "loss": 0.0026,
      "step": 15320
    },
    {
      "epoch": 0.8176,
      "grad_norm": 0.0676622986793518,
      "learning_rate": 2.9559999999999998e-05,
      "loss": 0.0026,
      "step": 15330
    },
    {
      "epoch": 0.8181333333333334,
      "grad_norm": 0.5624225735664368,
      "learning_rate": 2.9546666666666666e-05,
      "loss": 0.003,
      "step": 15340
    },
    {
      "epoch": 0.8186666666666667,
      "grad_norm": 0.13582031428813934,
      "learning_rate": 2.9533333333333334e-05,
      "loss": 0.0025,
      "step": 15350
    },
    {
      "epoch": 0.8192,
      "grad_norm": 0.6638662815093994,
      "learning_rate": 2.9520000000000002e-05,
      "loss": 0.003,
      "step": 15360
    },
    {
      "epoch": 0.8197333333333333,
      "grad_norm": 0.12983369827270508,
      "learning_rate": 2.9506666666666667e-05,
      "loss": 0.0025,
      "step": 15370
    },
    {
      "epoch": 0.8202666666666667,
      "grad_norm": 0.19359466433525085,
      "learning_rate": 2.9493333333333334e-05,
      "loss": 0.0023,
      "step": 15380
    },
    {
      "epoch": 0.8208,
      "grad_norm": 0.22403749823570251,
      "learning_rate": 2.9480000000000002e-05,
      "loss": 0.0024,
      "step": 15390
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 0.26395300030708313,
      "learning_rate": 2.946666666666667e-05,
      "loss": 0.0027,
      "step": 15400
    },
    {
      "epoch": 0.8218666666666666,
      "grad_norm": 0.4443848431110382,
      "learning_rate": 2.945333333333333e-05,
      "loss": 0.003,
      "step": 15410
    },
    {
      "epoch": 0.8224,
      "grad_norm": 0.17674624919891357,
      "learning_rate": 2.944e-05,
      "loss": 0.0023,
      "step": 15420
    },
    {
      "epoch": 0.8229333333333333,
      "grad_norm": 0.4506576657295227,
      "learning_rate": 2.942666666666667e-05,
      "loss": 0.0021,
      "step": 15430
    },
    {
      "epoch": 0.8234666666666667,
      "grad_norm": 0.31884562969207764,
      "learning_rate": 2.941333333333334e-05,
      "loss": 0.0031,
      "step": 15440
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.18123967945575714,
      "learning_rate": 2.94e-05,
      "loss": 0.0021,
      "step": 15450
    },
    {
      "epoch": 0.8245333333333333,
      "grad_norm": 0.16181664168834686,
      "learning_rate": 2.9386666666666668e-05,
      "loss": 0.0021,
      "step": 15460
    },
    {
      "epoch": 0.8250666666666666,
      "grad_norm": 0.43016982078552246,
      "learning_rate": 2.9373333333333336e-05,
      "loss": 0.0033,
      "step": 15470
    },
    {
      "epoch": 0.8256,
      "grad_norm": 0.16795714199543,
      "learning_rate": 2.9360000000000003e-05,
      "loss": 0.0042,
      "step": 15480
    },
    {
      "epoch": 0.8261333333333334,
      "grad_norm": 0.3541356027126312,
      "learning_rate": 2.9346666666666668e-05,
      "loss": 0.004,
      "step": 15490
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.3735051453113556,
      "learning_rate": 2.9333333333333336e-05,
      "loss": 0.0031,
      "step": 15500
    },
    {
      "epoch": 0.8272,
      "grad_norm": 0.28368622064590454,
      "learning_rate": 2.9320000000000004e-05,
      "loss": 0.0021,
      "step": 15510
    },
    {
      "epoch": 0.8277333333333333,
      "grad_norm": 0.14251019060611725,
      "learning_rate": 2.9306666666666665e-05,
      "loss": 0.0034,
      "step": 15520
    },
    {
      "epoch": 0.8282666666666667,
      "grad_norm": 0.33125585317611694,
      "learning_rate": 2.9293333333333333e-05,
      "loss": 0.0021,
      "step": 15530
    },
    {
      "epoch": 0.8288,
      "grad_norm": 0.09937480837106705,
      "learning_rate": 2.928e-05,
      "loss": 0.0017,
      "step": 15540
    },
    {
      "epoch": 0.8293333333333334,
      "grad_norm": 0.9510829448699951,
      "learning_rate": 2.926666666666667e-05,
      "loss": 0.0021,
      "step": 15550
    },
    {
      "epoch": 0.8298666666666666,
      "grad_norm": 0.1207689493894577,
      "learning_rate": 2.9253333333333333e-05,
      "loss": 0.0034,
      "step": 15560
    },
    {
      "epoch": 0.8304,
      "grad_norm": 0.1707841455936432,
      "learning_rate": 2.924e-05,
      "loss": 0.0021,
      "step": 15570
    },
    {
      "epoch": 0.8309333333333333,
      "grad_norm": 0.07097677141427994,
      "learning_rate": 2.922666666666667e-05,
      "loss": 0.0031,
      "step": 15580
    },
    {
      "epoch": 0.8314666666666667,
      "grad_norm": 0.4633997976779938,
      "learning_rate": 2.9213333333333337e-05,
      "loss": 0.0019,
      "step": 15590
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.6070473790168762,
      "learning_rate": 2.9199999999999998e-05,
      "loss": 0.0028,
      "step": 15600
    },
    {
      "epoch": 0.8325333333333333,
      "grad_norm": 0.19097961485385895,
      "learning_rate": 2.9186666666666666e-05,
      "loss": 0.0028,
      "step": 15610
    },
    {
      "epoch": 0.8330666666666666,
      "grad_norm": 0.12223631888628006,
      "learning_rate": 2.9173333333333337e-05,
      "loss": 0.0024,
      "step": 15620
    },
    {
      "epoch": 0.8336,
      "grad_norm": 0.36076509952545166,
      "learning_rate": 2.9160000000000005e-05,
      "loss": 0.0019,
      "step": 15630
    },
    {
      "epoch": 0.8341333333333333,
      "grad_norm": 0.38827836513519287,
      "learning_rate": 2.9146666666666667e-05,
      "loss": 0.0026,
      "step": 15640
    },
    {
      "epoch": 0.8346666666666667,
      "grad_norm": 0.23328490555286407,
      "learning_rate": 2.9133333333333334e-05,
      "loss": 0.0029,
      "step": 15650
    },
    {
      "epoch": 0.8352,
      "grad_norm": 0.1928786188364029,
      "learning_rate": 2.9120000000000002e-05,
      "loss": 0.0017,
      "step": 15660
    },
    {
      "epoch": 0.8357333333333333,
      "grad_norm": 0.319163978099823,
      "learning_rate": 2.9106666666666667e-05,
      "loss": 0.0026,
      "step": 15670
    },
    {
      "epoch": 0.8362666666666667,
      "grad_norm": 0.11617495864629745,
      "learning_rate": 2.9093333333333335e-05,
      "loss": 0.002,
      "step": 15680
    },
    {
      "epoch": 0.8368,
      "grad_norm": 0.10200496017932892,
      "learning_rate": 2.9080000000000003e-05,
      "loss": 0.0017,
      "step": 15690
    },
    {
      "epoch": 0.8373333333333334,
      "grad_norm": 0.4553424119949341,
      "learning_rate": 2.906666666666667e-05,
      "loss": 0.0021,
      "step": 15700
    },
    {
      "epoch": 0.8378666666666666,
      "grad_norm": 0.871925413608551,
      "learning_rate": 2.9053333333333332e-05,
      "loss": 0.0034,
      "step": 15710
    },
    {
      "epoch": 0.8384,
      "grad_norm": 0.5682183504104614,
      "learning_rate": 2.904e-05,
      "loss": 0.0025,
      "step": 15720
    },
    {
      "epoch": 0.8389333333333333,
      "grad_norm": 0.8230119347572327,
      "learning_rate": 2.9026666666666668e-05,
      "loss": 0.0031,
      "step": 15730
    },
    {
      "epoch": 0.8394666666666667,
      "grad_norm": 0.43905770778656006,
      "learning_rate": 2.9013333333333336e-05,
      "loss": 0.0021,
      "step": 15740
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.6429658532142639,
      "learning_rate": 2.9e-05,
      "loss": 0.0023,
      "step": 15750
    },
    {
      "epoch": 0.8405333333333334,
      "grad_norm": 0.24437974393367767,
      "learning_rate": 2.8986666666666668e-05,
      "loss": 0.0033,
      "step": 15760
    },
    {
      "epoch": 0.8410666666666666,
      "grad_norm": 0.28436869382858276,
      "learning_rate": 2.8973333333333336e-05,
      "loss": 0.0029,
      "step": 15770
    },
    {
      "epoch": 0.8416,
      "grad_norm": 0.5696926712989807,
      "learning_rate": 2.8960000000000004e-05,
      "loss": 0.0031,
      "step": 15780
    },
    {
      "epoch": 0.8421333333333333,
      "grad_norm": 0.062474098056554794,
      "learning_rate": 2.8946666666666665e-05,
      "loss": 0.0032,
      "step": 15790
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 0.5514722466468811,
      "learning_rate": 2.8933333333333333e-05,
      "loss": 0.0027,
      "step": 15800
    },
    {
      "epoch": 0.8432,
      "grad_norm": 0.09457898139953613,
      "learning_rate": 2.8920000000000004e-05,
      "loss": 0.0023,
      "step": 15810
    },
    {
      "epoch": 0.8437333333333333,
      "grad_norm": 0.30168336629867554,
      "learning_rate": 2.8906666666666672e-05,
      "loss": 0.0018,
      "step": 15820
    },
    {
      "epoch": 0.8442666666666667,
      "grad_norm": 0.1292354166507721,
      "learning_rate": 2.8893333333333333e-05,
      "loss": 0.0022,
      "step": 15830
    },
    {
      "epoch": 0.8448,
      "grad_norm": 0.3786303400993347,
      "learning_rate": 2.888e-05,
      "loss": 0.0018,
      "step": 15840
    },
    {
      "epoch": 0.8453333333333334,
      "grad_norm": 0.29867470264434814,
      "learning_rate": 2.886666666666667e-05,
      "loss": 0.0025,
      "step": 15850
    },
    {
      "epoch": 0.8458666666666667,
      "grad_norm": 0.4795100688934326,
      "learning_rate": 2.8853333333333334e-05,
      "loss": 0.002,
      "step": 15860
    },
    {
      "epoch": 0.8464,
      "grad_norm": 0.30850908160209656,
      "learning_rate": 2.8840000000000002e-05,
      "loss": 0.0034,
      "step": 15870
    },
    {
      "epoch": 0.8469333333333333,
      "grad_norm": 0.3507644534111023,
      "learning_rate": 2.882666666666667e-05,
      "loss": 0.0034,
      "step": 15880
    },
    {
      "epoch": 0.8474666666666667,
      "grad_norm": 0.2796562612056732,
      "learning_rate": 2.8813333333333338e-05,
      "loss": 0.002,
      "step": 15890
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.3107719421386719,
      "learning_rate": 2.88e-05,
      "loss": 0.0021,
      "step": 15900
    },
    {
      "epoch": 0.8485333333333334,
      "grad_norm": 0.18758539855480194,
      "learning_rate": 2.8786666666666667e-05,
      "loss": 0.0027,
      "step": 15910
    },
    {
      "epoch": 0.8490666666666666,
      "grad_norm": 0.7195987701416016,
      "learning_rate": 2.8773333333333335e-05,
      "loss": 0.0039,
      "step": 15920
    },
    {
      "epoch": 0.8496,
      "grad_norm": 0.2310030162334442,
      "learning_rate": 2.8760000000000002e-05,
      "loss": 0.0031,
      "step": 15930
    },
    {
      "epoch": 0.8501333333333333,
      "grad_norm": 0.1663188338279724,
      "learning_rate": 2.8746666666666667e-05,
      "loss": 0.0022,
      "step": 15940
    },
    {
      "epoch": 0.8506666666666667,
      "grad_norm": 0.2963348627090454,
      "learning_rate": 2.8733333333333335e-05,
      "loss": 0.0028,
      "step": 15950
    },
    {
      "epoch": 0.8512,
      "grad_norm": 0.427996963262558,
      "learning_rate": 2.8720000000000003e-05,
      "loss": 0.0024,
      "step": 15960
    },
    {
      "epoch": 0.8517333333333333,
      "grad_norm": 0.6208342909812927,
      "learning_rate": 2.870666666666667e-05,
      "loss": 0.0022,
      "step": 15970
    },
    {
      "epoch": 0.8522666666666666,
      "grad_norm": 0.11434612423181534,
      "learning_rate": 2.8693333333333332e-05,
      "loss": 0.0021,
      "step": 15980
    },
    {
      "epoch": 0.8528,
      "grad_norm": 0.4788167476654053,
      "learning_rate": 2.868e-05,
      "loss": 0.0025,
      "step": 15990
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.40932324528694153,
      "learning_rate": 2.8666666666666668e-05,
      "loss": 0.0025,
      "step": 16000
    },
    {
      "epoch": 0.8538666666666667,
      "grad_norm": 0.5686463713645935,
      "learning_rate": 2.8653333333333332e-05,
      "loss": 0.0024,
      "step": 16010
    },
    {
      "epoch": 0.8544,
      "grad_norm": 0.983853816986084,
      "learning_rate": 2.864e-05,
      "loss": 0.0032,
      "step": 16020
    },
    {
      "epoch": 0.8549333333333333,
      "grad_norm": 0.21136371791362762,
      "learning_rate": 2.8626666666666668e-05,
      "loss": 0.002,
      "step": 16030
    },
    {
      "epoch": 0.8554666666666667,
      "grad_norm": 0.0996336042881012,
      "learning_rate": 2.8613333333333336e-05,
      "loss": 0.0021,
      "step": 16040
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.4616157114505768,
      "learning_rate": 2.86e-05,
      "loss": 0.0041,
      "step": 16050
    },
    {
      "epoch": 0.8565333333333334,
      "grad_norm": 0.3461459279060364,
      "learning_rate": 2.858666666666667e-05,
      "loss": 0.0025,
      "step": 16060
    },
    {
      "epoch": 0.8570666666666666,
      "grad_norm": 0.34866246581077576,
      "learning_rate": 2.8573333333333336e-05,
      "loss": 0.0021,
      "step": 16070
    },
    {
      "epoch": 0.8576,
      "grad_norm": 0.25270751118659973,
      "learning_rate": 2.8560000000000004e-05,
      "loss": 0.0022,
      "step": 16080
    },
    {
      "epoch": 0.8581333333333333,
      "grad_norm": 0.583020031452179,
      "learning_rate": 2.8546666666666666e-05,
      "loss": 0.0027,
      "step": 16090
    },
    {
      "epoch": 0.8586666666666667,
      "grad_norm": 0.5327824950218201,
      "learning_rate": 2.8533333333333333e-05,
      "loss": 0.0034,
      "step": 16100
    },
    {
      "epoch": 0.8592,
      "grad_norm": 0.8859305381774902,
      "learning_rate": 2.852e-05,
      "loss": 0.002,
      "step": 16110
    },
    {
      "epoch": 0.8597333333333333,
      "grad_norm": 0.3746681213378906,
      "learning_rate": 2.850666666666667e-05,
      "loss": 0.0021,
      "step": 16120
    },
    {
      "epoch": 0.8602666666666666,
      "grad_norm": 0.143516406416893,
      "learning_rate": 2.8493333333333334e-05,
      "loss": 0.003,
      "step": 16130
    },
    {
      "epoch": 0.8608,
      "grad_norm": 0.44482114911079407,
      "learning_rate": 2.8480000000000002e-05,
      "loss": 0.0019,
      "step": 16140
    },
    {
      "epoch": 0.8613333333333333,
      "grad_norm": 0.5508993268013,
      "learning_rate": 2.846666666666667e-05,
      "loss": 0.0031,
      "step": 16150
    },
    {
      "epoch": 0.8618666666666667,
      "grad_norm": 0.13657145202159882,
      "learning_rate": 2.8453333333333338e-05,
      "loss": 0.0024,
      "step": 16160
    },
    {
      "epoch": 0.8624,
      "grad_norm": 0.5972384214401245,
      "learning_rate": 2.844e-05,
      "loss": 0.0025,
      "step": 16170
    },
    {
      "epoch": 0.8629333333333333,
      "grad_norm": 0.25973811745643616,
      "learning_rate": 2.8426666666666667e-05,
      "loss": 0.0029,
      "step": 16180
    },
    {
      "epoch": 0.8634666666666667,
      "grad_norm": 0.35015517473220825,
      "learning_rate": 2.8413333333333335e-05,
      "loss": 0.0039,
      "step": 16190
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.2199770212173462,
      "learning_rate": 2.84e-05,
      "loss": 0.0022,
      "step": 16200
    },
    {
      "epoch": 0.8645333333333334,
      "grad_norm": 0.16060398519039154,
      "learning_rate": 2.8386666666666667e-05,
      "loss": 0.0029,
      "step": 16210
    },
    {
      "epoch": 0.8650666666666667,
      "grad_norm": 0.12725190818309784,
      "learning_rate": 2.8373333333333335e-05,
      "loss": 0.0024,
      "step": 16220
    },
    {
      "epoch": 0.8656,
      "grad_norm": 0.15442661941051483,
      "learning_rate": 2.8360000000000003e-05,
      "loss": 0.002,
      "step": 16230
    },
    {
      "epoch": 0.8661333333333333,
      "grad_norm": 0.0864759013056755,
      "learning_rate": 2.8346666666666667e-05,
      "loss": 0.0028,
      "step": 16240
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.6046755313873291,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 0.0021,
      "step": 16250
    },
    {
      "epoch": 0.8672,
      "grad_norm": 0.32465699315071106,
      "learning_rate": 2.8320000000000003e-05,
      "loss": 0.0028,
      "step": 16260
    },
    {
      "epoch": 0.8677333333333334,
      "grad_norm": 0.17570821940898895,
      "learning_rate": 2.830666666666667e-05,
      "loss": 0.0029,
      "step": 16270
    },
    {
      "epoch": 0.8682666666666666,
      "grad_norm": 0.12133527547121048,
      "learning_rate": 2.8293333333333332e-05,
      "loss": 0.0025,
      "step": 16280
    },
    {
      "epoch": 0.8688,
      "grad_norm": 0.37227365374565125,
      "learning_rate": 2.828e-05,
      "loss": 0.002,
      "step": 16290
    },
    {
      "epoch": 0.8693333333333333,
      "grad_norm": 0.28952717781066895,
      "learning_rate": 2.8266666666666668e-05,
      "loss": 0.0031,
      "step": 16300
    },
    {
      "epoch": 0.8698666666666667,
      "grad_norm": 0.1008419468998909,
      "learning_rate": 2.8253333333333336e-05,
      "loss": 0.0034,
      "step": 16310
    },
    {
      "epoch": 0.8704,
      "grad_norm": 0.3660769462585449,
      "learning_rate": 2.824e-05,
      "loss": 0.0029,
      "step": 16320
    },
    {
      "epoch": 0.8709333333333333,
      "grad_norm": 0.5513639450073242,
      "learning_rate": 2.822666666666667e-05,
      "loss": 0.0037,
      "step": 16330
    },
    {
      "epoch": 0.8714666666666666,
      "grad_norm": 0.12099193781614304,
      "learning_rate": 2.8213333333333337e-05,
      "loss": 0.0032,
      "step": 16340
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.4494534432888031,
      "learning_rate": 2.8199999999999998e-05,
      "loss": 0.0021,
      "step": 16350
    },
    {
      "epoch": 0.8725333333333334,
      "grad_norm": 0.4674254357814789,
      "learning_rate": 2.8186666666666666e-05,
      "loss": 0.0031,
      "step": 16360
    },
    {
      "epoch": 0.8730666666666667,
      "grad_norm": 0.07686234265565872,
      "learning_rate": 2.8173333333333334e-05,
      "loss": 0.0022,
      "step": 16370
    },
    {
      "epoch": 0.8736,
      "grad_norm": 0.2595922648906708,
      "learning_rate": 2.816e-05,
      "loss": 0.0022,
      "step": 16380
    },
    {
      "epoch": 0.8741333333333333,
      "grad_norm": 0.10730595886707306,
      "learning_rate": 2.8146666666666666e-05,
      "loss": 0.0024,
      "step": 16390
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 0.6962698698043823,
      "learning_rate": 2.8133333333333334e-05,
      "loss": 0.0022,
      "step": 16400
    },
    {
      "epoch": 0.8752,
      "grad_norm": 0.14427348971366882,
      "learning_rate": 2.8120000000000002e-05,
      "loss": 0.0021,
      "step": 16410
    },
    {
      "epoch": 0.8757333333333334,
      "grad_norm": 0.6338837742805481,
      "learning_rate": 2.810666666666667e-05,
      "loss": 0.0039,
      "step": 16420
    },
    {
      "epoch": 0.8762666666666666,
      "grad_norm": 0.38898319005966187,
      "learning_rate": 2.8093333333333334e-05,
      "loss": 0.0028,
      "step": 16430
    },
    {
      "epoch": 0.8768,
      "grad_norm": 0.3444250226020813,
      "learning_rate": 2.8080000000000002e-05,
      "loss": 0.0024,
      "step": 16440
    },
    {
      "epoch": 0.8773333333333333,
      "grad_norm": 0.39143216609954834,
      "learning_rate": 2.806666666666667e-05,
      "loss": 0.0031,
      "step": 16450
    },
    {
      "epoch": 0.8778666666666667,
      "grad_norm": 1.0653375387191772,
      "learning_rate": 2.8053333333333338e-05,
      "loss": 0.0031,
      "step": 16460
    },
    {
      "epoch": 0.8784,
      "grad_norm": 0.3964204490184784,
      "learning_rate": 2.804e-05,
      "loss": 0.0027,
      "step": 16470
    },
    {
      "epoch": 0.8789333333333333,
      "grad_norm": 0.1663716584444046,
      "learning_rate": 2.8026666666666667e-05,
      "loss": 0.0032,
      "step": 16480
    },
    {
      "epoch": 0.8794666666666666,
      "grad_norm": 0.5312122106552124,
      "learning_rate": 2.8013333333333335e-05,
      "loss": 0.0022,
      "step": 16490
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.3915719985961914,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.0023,
      "step": 16500
    },
    {
      "epoch": 0.8805333333333333,
      "grad_norm": 0.34072795510292053,
      "learning_rate": 2.7986666666666668e-05,
      "loss": 0.0025,
      "step": 16510
    },
    {
      "epoch": 0.8810666666666667,
      "grad_norm": 0.35375744104385376,
      "learning_rate": 2.7973333333333335e-05,
      "loss": 0.0033,
      "step": 16520
    },
    {
      "epoch": 0.8816,
      "grad_norm": 0.12177970260381699,
      "learning_rate": 2.7960000000000003e-05,
      "loss": 0.0021,
      "step": 16530
    },
    {
      "epoch": 0.8821333333333333,
      "grad_norm": 0.07768206298351288,
      "learning_rate": 2.7946666666666664e-05,
      "loss": 0.0035,
      "step": 16540
    },
    {
      "epoch": 0.8826666666666667,
      "grad_norm": 0.09659958630800247,
      "learning_rate": 2.7933333333333332e-05,
      "loss": 0.0029,
      "step": 16550
    },
    {
      "epoch": 0.8832,
      "grad_norm": 0.26585066318511963,
      "learning_rate": 2.792e-05,
      "loss": 0.0027,
      "step": 16560
    },
    {
      "epoch": 0.8837333333333334,
      "grad_norm": 0.08407234400510788,
      "learning_rate": 2.7906666666666668e-05,
      "loss": 0.0024,
      "step": 16570
    },
    {
      "epoch": 0.8842666666666666,
      "grad_norm": 0.3870835602283478,
      "learning_rate": 2.7893333333333333e-05,
      "loss": 0.0021,
      "step": 16580
    },
    {
      "epoch": 0.8848,
      "grad_norm": 0.3057210445404053,
      "learning_rate": 2.788e-05,
      "loss": 0.0027,
      "step": 16590
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 0.14989645779132843,
      "learning_rate": 2.786666666666667e-05,
      "loss": 0.0018,
      "step": 16600
    },
    {
      "epoch": 0.8858666666666667,
      "grad_norm": 0.1113782450556755,
      "learning_rate": 2.7853333333333337e-05,
      "loss": 0.0028,
      "step": 16610
    },
    {
      "epoch": 0.8864,
      "grad_norm": 0.45767858624458313,
      "learning_rate": 2.7839999999999998e-05,
      "loss": 0.0021,
      "step": 16620
    },
    {
      "epoch": 0.8869333333333334,
      "grad_norm": 0.6763116121292114,
      "learning_rate": 2.782666666666667e-05,
      "loss": 0.0034,
      "step": 16630
    },
    {
      "epoch": 0.8874666666666666,
      "grad_norm": 0.46276581287384033,
      "learning_rate": 2.7813333333333337e-05,
      "loss": 0.0023,
      "step": 16640
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.24953971803188324,
      "learning_rate": 2.7800000000000005e-05,
      "loss": 0.0031,
      "step": 16650
    },
    {
      "epoch": 0.8885333333333333,
      "grad_norm": 0.29214462637901306,
      "learning_rate": 2.7786666666666666e-05,
      "loss": 0.0023,
      "step": 16660
    },
    {
      "epoch": 0.8890666666666667,
      "grad_norm": 0.09012912958860397,
      "learning_rate": 2.7773333333333334e-05,
      "loss": 0.0019,
      "step": 16670
    },
    {
      "epoch": 0.8896,
      "grad_norm": 0.15453843772411346,
      "learning_rate": 2.7760000000000002e-05,
      "loss": 0.002,
      "step": 16680
    },
    {
      "epoch": 0.8901333333333333,
      "grad_norm": 0.42703956365585327,
      "learning_rate": 2.7746666666666666e-05,
      "loss": 0.002,
      "step": 16690
    },
    {
      "epoch": 0.8906666666666667,
      "grad_norm": 0.20250646770000458,
      "learning_rate": 2.7733333333333334e-05,
      "loss": 0.0026,
      "step": 16700
    },
    {
      "epoch": 0.8912,
      "grad_norm": 0.5522605776786804,
      "learning_rate": 2.7720000000000002e-05,
      "loss": 0.002,
      "step": 16710
    },
    {
      "epoch": 0.8917333333333334,
      "grad_norm": 0.6549431681632996,
      "learning_rate": 2.770666666666667e-05,
      "loss": 0.0026,
      "step": 16720
    },
    {
      "epoch": 0.8922666666666667,
      "grad_norm": 0.25743475556373596,
      "learning_rate": 2.769333333333333e-05,
      "loss": 0.0018,
      "step": 16730
    },
    {
      "epoch": 0.8928,
      "grad_norm": 0.35221973061561584,
      "learning_rate": 2.768e-05,
      "loss": 0.0037,
      "step": 16740
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.2628249526023865,
      "learning_rate": 2.7666666666666667e-05,
      "loss": 0.0029,
      "step": 16750
    },
    {
      "epoch": 0.8938666666666667,
      "grad_norm": 0.31101328134536743,
      "learning_rate": 2.7653333333333335e-05,
      "loss": 0.0034,
      "step": 16760
    },
    {
      "epoch": 0.8944,
      "grad_norm": 0.13907171785831451,
      "learning_rate": 2.764e-05,
      "loss": 0.0022,
      "step": 16770
    },
    {
      "epoch": 0.8949333333333334,
      "grad_norm": 0.2715611457824707,
      "learning_rate": 2.7626666666666668e-05,
      "loss": 0.0019,
      "step": 16780
    },
    {
      "epoch": 0.8954666666666666,
      "grad_norm": 0.30715543031692505,
      "learning_rate": 2.7613333333333335e-05,
      "loss": 0.0021,
      "step": 16790
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.108685702085495,
      "learning_rate": 2.7600000000000003e-05,
      "loss": 0.0019,
      "step": 16800
    },
    {
      "epoch": 0.8965333333333333,
      "grad_norm": 0.46103915572166443,
      "learning_rate": 2.7586666666666665e-05,
      "loss": 0.0028,
      "step": 16810
    },
    {
      "epoch": 0.8970666666666667,
      "grad_norm": 0.6772036552429199,
      "learning_rate": 2.7573333333333336e-05,
      "loss": 0.003,
      "step": 16820
    },
    {
      "epoch": 0.8976,
      "grad_norm": 0.21108120679855347,
      "learning_rate": 2.7560000000000004e-05,
      "loss": 0.0027,
      "step": 16830
    },
    {
      "epoch": 0.8981333333333333,
      "grad_norm": 0.07593825459480286,
      "learning_rate": 2.7546666666666672e-05,
      "loss": 0.0027,
      "step": 16840
    },
    {
      "epoch": 0.8986666666666666,
      "grad_norm": 0.13957908749580383,
      "learning_rate": 2.7533333333333333e-05,
      "loss": 0.0029,
      "step": 16850
    },
    {
      "epoch": 0.8992,
      "grad_norm": 0.37069451808929443,
      "learning_rate": 2.752e-05,
      "loss": 0.0032,
      "step": 16860
    },
    {
      "epoch": 0.8997333333333334,
      "grad_norm": 0.30301183462142944,
      "learning_rate": 2.750666666666667e-05,
      "loss": 0.0026,
      "step": 16870
    },
    {
      "epoch": 0.9002666666666667,
      "grad_norm": 0.16509559750556946,
      "learning_rate": 2.7493333333333333e-05,
      "loss": 0.0023,
      "step": 16880
    },
    {
      "epoch": 0.9008,
      "grad_norm": 0.3878653645515442,
      "learning_rate": 2.748e-05,
      "loss": 0.003,
      "step": 16890
    },
    {
      "epoch": 0.9013333333333333,
      "grad_norm": 0.3390452563762665,
      "learning_rate": 2.746666666666667e-05,
      "loss": 0.0027,
      "step": 16900
    },
    {
      "epoch": 0.9018666666666667,
      "grad_norm": 0.4355064332485199,
      "learning_rate": 2.7453333333333337e-05,
      "loss": 0.0029,
      "step": 16910
    },
    {
      "epoch": 0.9024,
      "grad_norm": 0.22619779407978058,
      "learning_rate": 2.7439999999999998e-05,
      "loss": 0.0033,
      "step": 16920
    },
    {
      "epoch": 0.9029333333333334,
      "grad_norm": 0.6026586294174194,
      "learning_rate": 2.7426666666666666e-05,
      "loss": 0.0028,
      "step": 16930
    },
    {
      "epoch": 0.9034666666666666,
      "grad_norm": 0.23036448657512665,
      "learning_rate": 2.7413333333333334e-05,
      "loss": 0.0028,
      "step": 16940
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.10695702582597733,
      "learning_rate": 2.7400000000000002e-05,
      "loss": 0.0031,
      "step": 16950
    },
    {
      "epoch": 0.9045333333333333,
      "grad_norm": 0.24132482707500458,
      "learning_rate": 2.7386666666666666e-05,
      "loss": 0.002,
      "step": 16960
    },
    {
      "epoch": 0.9050666666666667,
      "grad_norm": 0.17519940435886383,
      "learning_rate": 2.7373333333333334e-05,
      "loss": 0.003,
      "step": 16970
    },
    {
      "epoch": 0.9056,
      "grad_norm": 0.5510225892066956,
      "learning_rate": 2.7360000000000002e-05,
      "loss": 0.0024,
      "step": 16980
    },
    {
      "epoch": 0.9061333333333333,
      "grad_norm": 0.4911171793937683,
      "learning_rate": 2.734666666666667e-05,
      "loss": 0.0025,
      "step": 16990
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.13089527189731598,
      "learning_rate": 2.733333333333333e-05,
      "loss": 0.002,
      "step": 17000
    },
    {
      "epoch": 0.9072,
      "grad_norm": 0.16545769572257996,
      "learning_rate": 2.7320000000000003e-05,
      "loss": 0.0026,
      "step": 17010
    },
    {
      "epoch": 0.9077333333333333,
      "grad_norm": 0.4187171459197998,
      "learning_rate": 2.730666666666667e-05,
      "loss": 0.0024,
      "step": 17020
    },
    {
      "epoch": 0.9082666666666667,
      "grad_norm": 0.5242153406143188,
      "learning_rate": 2.7293333333333332e-05,
      "loss": 0.0019,
      "step": 17030
    },
    {
      "epoch": 0.9088,
      "grad_norm": 0.10656517744064331,
      "learning_rate": 2.728e-05,
      "loss": 0.003,
      "step": 17040
    },
    {
      "epoch": 0.9093333333333333,
      "grad_norm": 0.592522382736206,
      "learning_rate": 2.7266666666666668e-05,
      "loss": 0.0022,
      "step": 17050
    },
    {
      "epoch": 0.9098666666666667,
      "grad_norm": 0.3962232172489166,
      "learning_rate": 2.7253333333333336e-05,
      "loss": 0.0024,
      "step": 17060
    },
    {
      "epoch": 0.9104,
      "grad_norm": 0.1316252499818802,
      "learning_rate": 2.724e-05,
      "loss": 0.0027,
      "step": 17070
    },
    {
      "epoch": 0.9109333333333334,
      "grad_norm": 0.21587736904621124,
      "learning_rate": 2.7226666666666668e-05,
      "loss": 0.0025,
      "step": 17080
    },
    {
      "epoch": 0.9114666666666666,
      "grad_norm": 0.2009129673242569,
      "learning_rate": 2.7213333333333336e-05,
      "loss": 0.0041,
      "step": 17090
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.5128138661384583,
      "learning_rate": 2.7200000000000004e-05,
      "loss": 0.0041,
      "step": 17100
    },
    {
      "epoch": 0.9125333333333333,
      "grad_norm": 0.2973678708076477,
      "learning_rate": 2.7186666666666665e-05,
      "loss": 0.0036,
      "step": 17110
    },
    {
      "epoch": 0.9130666666666667,
      "grad_norm": 0.19061726331710815,
      "learning_rate": 2.7173333333333333e-05,
      "loss": 0.004,
      "step": 17120
    },
    {
      "epoch": 0.9136,
      "grad_norm": 0.4112711548805237,
      "learning_rate": 2.716e-05,
      "loss": 0.0027,
      "step": 17130
    },
    {
      "epoch": 0.9141333333333334,
      "grad_norm": 0.7749583125114441,
      "learning_rate": 2.714666666666667e-05,
      "loss": 0.002,
      "step": 17140
    },
    {
      "epoch": 0.9146666666666666,
      "grad_norm": 0.21630637347698212,
      "learning_rate": 2.7133333333333333e-05,
      "loss": 0.002,
      "step": 17150
    },
    {
      "epoch": 0.9152,
      "grad_norm": 0.5299139618873596,
      "learning_rate": 2.712e-05,
      "loss": 0.0037,
      "step": 17160
    },
    {
      "epoch": 0.9157333333333333,
      "grad_norm": 0.08572883158922195,
      "learning_rate": 2.710666666666667e-05,
      "loss": 0.0027,
      "step": 17170
    },
    {
      "epoch": 0.9162666666666667,
      "grad_norm": 0.14706973731517792,
      "learning_rate": 2.7093333333333337e-05,
      "loss": 0.0023,
      "step": 17180
    },
    {
      "epoch": 0.9168,
      "grad_norm": 0.3491811454296112,
      "learning_rate": 2.7079999999999998e-05,
      "loss": 0.003,
      "step": 17190
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 0.6246240735054016,
      "learning_rate": 2.706666666666667e-05,
      "loss": 0.0029,
      "step": 17200
    },
    {
      "epoch": 0.9178666666666667,
      "grad_norm": 0.1863308995962143,
      "learning_rate": 2.7053333333333337e-05,
      "loss": 0.0019,
      "step": 17210
    },
    {
      "epoch": 0.9184,
      "grad_norm": 0.13189835846424103,
      "learning_rate": 2.704e-05,
      "loss": 0.0016,
      "step": 17220
    },
    {
      "epoch": 0.9189333333333334,
      "grad_norm": 0.29922112822532654,
      "learning_rate": 2.7026666666666667e-05,
      "loss": 0.0045,
      "step": 17230
    },
    {
      "epoch": 0.9194666666666667,
      "grad_norm": 0.2833445966243744,
      "learning_rate": 2.7013333333333334e-05,
      "loss": 0.0025,
      "step": 17240
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.2184927761554718,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.0029,
      "step": 17250
    },
    {
      "epoch": 0.9205333333333333,
      "grad_norm": 0.224413201212883,
      "learning_rate": 2.6986666666666667e-05,
      "loss": 0.0049,
      "step": 17260
    },
    {
      "epoch": 0.9210666666666667,
      "grad_norm": 0.2606956660747528,
      "learning_rate": 2.6973333333333335e-05,
      "loss": 0.0019,
      "step": 17270
    },
    {
      "epoch": 0.9216,
      "grad_norm": 0.41524821519851685,
      "learning_rate": 2.6960000000000003e-05,
      "loss": 0.0031,
      "step": 17280
    },
    {
      "epoch": 0.9221333333333334,
      "grad_norm": 0.1784447431564331,
      "learning_rate": 2.694666666666667e-05,
      "loss": 0.0037,
      "step": 17290
    },
    {
      "epoch": 0.9226666666666666,
      "grad_norm": 0.1551346331834793,
      "learning_rate": 2.6933333333333332e-05,
      "loss": 0.0021,
      "step": 17300
    },
    {
      "epoch": 0.9232,
      "grad_norm": 0.3175542950630188,
      "learning_rate": 2.692e-05,
      "loss": 0.0043,
      "step": 17310
    },
    {
      "epoch": 0.9237333333333333,
      "grad_norm": 0.19221554696559906,
      "learning_rate": 2.6906666666666668e-05,
      "loss": 0.0019,
      "step": 17320
    },
    {
      "epoch": 0.9242666666666667,
      "grad_norm": 0.6201901435852051,
      "learning_rate": 2.6893333333333336e-05,
      "loss": 0.0024,
      "step": 17330
    },
    {
      "epoch": 0.9248,
      "grad_norm": 0.08313383162021637,
      "learning_rate": 2.688e-05,
      "loss": 0.0019,
      "step": 17340
    },
    {
      "epoch": 0.9253333333333333,
      "grad_norm": 0.2575698494911194,
      "learning_rate": 2.6866666666666668e-05,
      "loss": 0.0025,
      "step": 17350
    },
    {
      "epoch": 0.9258666666666666,
      "grad_norm": 0.10048550367355347,
      "learning_rate": 2.6853333333333336e-05,
      "loss": 0.0026,
      "step": 17360
    },
    {
      "epoch": 0.9264,
      "grad_norm": 0.5601838231086731,
      "learning_rate": 2.6840000000000004e-05,
      "loss": 0.002,
      "step": 17370
    },
    {
      "epoch": 0.9269333333333334,
      "grad_norm": 0.5632030963897705,
      "learning_rate": 2.6826666666666665e-05,
      "loss": 0.0029,
      "step": 17380
    },
    {
      "epoch": 0.9274666666666667,
      "grad_norm": 0.170609712600708,
      "learning_rate": 2.6813333333333336e-05,
      "loss": 0.0024,
      "step": 17390
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.14314593374729156,
      "learning_rate": 2.6800000000000004e-05,
      "loss": 0.0035,
      "step": 17400
    },
    {
      "epoch": 0.9285333333333333,
      "grad_norm": 0.5541142225265503,
      "learning_rate": 2.6786666666666665e-05,
      "loss": 0.0024,
      "step": 17410
    },
    {
      "epoch": 0.9290666666666667,
      "grad_norm": 0.2600109577178955,
      "learning_rate": 2.6773333333333333e-05,
      "loss": 0.0022,
      "step": 17420
    },
    {
      "epoch": 0.9296,
      "grad_norm": 0.13901996612548828,
      "learning_rate": 2.676e-05,
      "loss": 0.002,
      "step": 17430
    },
    {
      "epoch": 0.9301333333333334,
      "grad_norm": 0.24708232283592224,
      "learning_rate": 2.674666666666667e-05,
      "loss": 0.002,
      "step": 17440
    },
    {
      "epoch": 0.9306666666666666,
      "grad_norm": 0.44185203313827515,
      "learning_rate": 2.6733333333333334e-05,
      "loss": 0.0018,
      "step": 17450
    },
    {
      "epoch": 0.9312,
      "grad_norm": 0.3195093274116516,
      "learning_rate": 2.672e-05,
      "loss": 0.0019,
      "step": 17460
    },
    {
      "epoch": 0.9317333333333333,
      "grad_norm": 0.522453784942627,
      "learning_rate": 2.670666666666667e-05,
      "loss": 0.0039,
      "step": 17470
    },
    {
      "epoch": 0.9322666666666667,
      "grad_norm": 0.20543785393238068,
      "learning_rate": 2.6693333333333338e-05,
      "loss": 0.0028,
      "step": 17480
    },
    {
      "epoch": 0.9328,
      "grad_norm": 0.2623993754386902,
      "learning_rate": 2.668e-05,
      "loss": 0.0021,
      "step": 17490
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.12594066560268402,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.0021,
      "step": 17500
    },
    {
      "epoch": 0.9338666666666666,
      "grad_norm": 0.48624375462532043,
      "learning_rate": 2.6653333333333335e-05,
      "loss": 0.0028,
      "step": 17510
    },
    {
      "epoch": 0.9344,
      "grad_norm": 0.7174227237701416,
      "learning_rate": 2.6640000000000002e-05,
      "loss": 0.0027,
      "step": 17520
    },
    {
      "epoch": 0.9349333333333333,
      "grad_norm": 0.06238117814064026,
      "learning_rate": 2.6626666666666667e-05,
      "loss": 0.0028,
      "step": 17530
    },
    {
      "epoch": 0.9354666666666667,
      "grad_norm": 0.11174976825714111,
      "learning_rate": 2.6613333333333335e-05,
      "loss": 0.003,
      "step": 17540
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.7821887135505676,
      "learning_rate": 2.6600000000000003e-05,
      "loss": 0.0022,
      "step": 17550
    },
    {
      "epoch": 0.9365333333333333,
      "grad_norm": 0.44011855125427246,
      "learning_rate": 2.6586666666666664e-05,
      "loss": 0.0024,
      "step": 17560
    },
    {
      "epoch": 0.9370666666666667,
      "grad_norm": 0.6699081063270569,
      "learning_rate": 2.6573333333333332e-05,
      "loss": 0.003,
      "step": 17570
    },
    {
      "epoch": 0.9376,
      "grad_norm": 0.19364279508590698,
      "learning_rate": 2.6560000000000003e-05,
      "loss": 0.0022,
      "step": 17580
    },
    {
      "epoch": 0.9381333333333334,
      "grad_norm": 0.23737233877182007,
      "learning_rate": 2.654666666666667e-05,
      "loss": 0.0021,
      "step": 17590
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 0.1263892501592636,
      "learning_rate": 2.6533333333333332e-05,
      "loss": 0.0036,
      "step": 17600
    },
    {
      "epoch": 0.9392,
      "grad_norm": 0.3736753463745117,
      "learning_rate": 2.652e-05,
      "loss": 0.0037,
      "step": 17610
    },
    {
      "epoch": 0.9397333333333333,
      "grad_norm": 0.10622234642505646,
      "learning_rate": 2.6506666666666668e-05,
      "loss": 0.002,
      "step": 17620
    },
    {
      "epoch": 0.9402666666666667,
      "grad_norm": 0.34806811809539795,
      "learning_rate": 2.6493333333333336e-05,
      "loss": 0.0051,
      "step": 17630
    },
    {
      "epoch": 0.9408,
      "grad_norm": 0.11885664612054825,
      "learning_rate": 2.648e-05,
      "loss": 0.0021,
      "step": 17640
    },
    {
      "epoch": 0.9413333333333334,
      "grad_norm": 0.4596719443798065,
      "learning_rate": 2.646666666666667e-05,
      "loss": 0.0023,
      "step": 17650
    },
    {
      "epoch": 0.9418666666666666,
      "grad_norm": 0.09153961390256882,
      "learning_rate": 2.6453333333333336e-05,
      "loss": 0.0019,
      "step": 17660
    },
    {
      "epoch": 0.9424,
      "grad_norm": 0.20513856410980225,
      "learning_rate": 2.6440000000000004e-05,
      "loss": 0.003,
      "step": 17670
    },
    {
      "epoch": 0.9429333333333333,
      "grad_norm": 0.09796607494354248,
      "learning_rate": 2.6426666666666665e-05,
      "loss": 0.0022,
      "step": 17680
    },
    {
      "epoch": 0.9434666666666667,
      "grad_norm": 0.44780632853507996,
      "learning_rate": 2.6413333333333333e-05,
      "loss": 0.0034,
      "step": 17690
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.21227027475833893,
      "learning_rate": 2.64e-05,
      "loss": 0.0023,
      "step": 17700
    },
    {
      "epoch": 0.9445333333333333,
      "grad_norm": 0.18557146191596985,
      "learning_rate": 2.638666666666667e-05,
      "loss": 0.0018,
      "step": 17710
    },
    {
      "epoch": 0.9450666666666667,
      "grad_norm": 0.31685593724250793,
      "learning_rate": 2.6373333333333334e-05,
      "loss": 0.0026,
      "step": 17720
    },
    {
      "epoch": 0.9456,
      "grad_norm": 0.07350759208202362,
      "learning_rate": 2.6360000000000002e-05,
      "loss": 0.0023,
      "step": 17730
    },
    {
      "epoch": 0.9461333333333334,
      "grad_norm": 0.42154642939567566,
      "learning_rate": 2.634666666666667e-05,
      "loss": 0.0027,
      "step": 17740
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.39741313457489014,
      "learning_rate": 2.633333333333333e-05,
      "loss": 0.0028,
      "step": 17750
    },
    {
      "epoch": 0.9472,
      "grad_norm": 0.16953310370445251,
      "learning_rate": 2.632e-05,
      "loss": 0.0022,
      "step": 17760
    },
    {
      "epoch": 0.9477333333333333,
      "grad_norm": 0.45851340889930725,
      "learning_rate": 2.630666666666667e-05,
      "loss": 0.0028,
      "step": 17770
    },
    {
      "epoch": 0.9482666666666667,
      "grad_norm": 0.3906904458999634,
      "learning_rate": 2.6293333333333338e-05,
      "loss": 0.0029,
      "step": 17780
    },
    {
      "epoch": 0.9488,
      "grad_norm": 0.4395320415496826,
      "learning_rate": 2.628e-05,
      "loss": 0.0025,
      "step": 17790
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 0.42047742009162903,
      "learning_rate": 2.6266666666666667e-05,
      "loss": 0.0021,
      "step": 17800
    },
    {
      "epoch": 0.9498666666666666,
      "grad_norm": 0.4795228838920593,
      "learning_rate": 2.6253333333333335e-05,
      "loss": 0.0031,
      "step": 17810
    },
    {
      "epoch": 0.9504,
      "grad_norm": 0.13413284718990326,
      "learning_rate": 2.6240000000000003e-05,
      "loss": 0.0027,
      "step": 17820
    },
    {
      "epoch": 0.9509333333333333,
      "grad_norm": 0.24339300394058228,
      "learning_rate": 2.6226666666666667e-05,
      "loss": 0.0019,
      "step": 17830
    },
    {
      "epoch": 0.9514666666666667,
      "grad_norm": 0.3686203956604004,
      "learning_rate": 2.6213333333333335e-05,
      "loss": 0.0026,
      "step": 17840
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.35544630885124207,
      "learning_rate": 2.6200000000000003e-05,
      "loss": 0.0027,
      "step": 17850
    },
    {
      "epoch": 0.9525333333333333,
      "grad_norm": 0.13606463372707367,
      "learning_rate": 2.618666666666667e-05,
      "loss": 0.0022,
      "step": 17860
    },
    {
      "epoch": 0.9530666666666666,
      "grad_norm": 0.5161614418029785,
      "learning_rate": 2.6173333333333332e-05,
      "loss": 0.0028,
      "step": 17870
    },
    {
      "epoch": 0.9536,
      "grad_norm": 0.10680174827575684,
      "learning_rate": 2.616e-05,
      "loss": 0.0023,
      "step": 17880
    },
    {
      "epoch": 0.9541333333333334,
      "grad_norm": 0.6293246746063232,
      "learning_rate": 2.6146666666666668e-05,
      "loss": 0.0041,
      "step": 17890
    },
    {
      "epoch": 0.9546666666666667,
      "grad_norm": 0.6847255825996399,
      "learning_rate": 2.6133333333333333e-05,
      "loss": 0.0031,
      "step": 17900
    },
    {
      "epoch": 0.9552,
      "grad_norm": 0.3488951623439789,
      "learning_rate": 2.612e-05,
      "loss": 0.0027,
      "step": 17910
    },
    {
      "epoch": 0.9557333333333333,
      "grad_norm": 0.21333296597003937,
      "learning_rate": 2.610666666666667e-05,
      "loss": 0.0023,
      "step": 17920
    },
    {
      "epoch": 0.9562666666666667,
      "grad_norm": 0.45259466767311096,
      "learning_rate": 2.6093333333333336e-05,
      "loss": 0.0041,
      "step": 17930
    },
    {
      "epoch": 0.9568,
      "grad_norm": 0.6185155510902405,
      "learning_rate": 2.6079999999999998e-05,
      "loss": 0.0037,
      "step": 17940
    },
    {
      "epoch": 0.9573333333333334,
      "grad_norm": 0.10139810293912888,
      "learning_rate": 2.6066666666666666e-05,
      "loss": 0.0023,
      "step": 17950
    },
    {
      "epoch": 0.9578666666666666,
      "grad_norm": 0.624439537525177,
      "learning_rate": 2.6053333333333333e-05,
      "loss": 0.0027,
      "step": 17960
    },
    {
      "epoch": 0.9584,
      "grad_norm": 0.28849905729293823,
      "learning_rate": 2.6040000000000005e-05,
      "loss": 0.0021,
      "step": 17970
    },
    {
      "epoch": 0.9589333333333333,
      "grad_norm": 0.1580423265695572,
      "learning_rate": 2.6026666666666666e-05,
      "loss": 0.0025,
      "step": 17980
    },
    {
      "epoch": 0.9594666666666667,
      "grad_norm": 0.33598655462265015,
      "learning_rate": 2.6013333333333334e-05,
      "loss": 0.0033,
      "step": 17990
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.2749057412147522,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.0025,
      "step": 18000
    },
    {
      "epoch": 0.9605333333333334,
      "grad_norm": 0.08740000426769257,
      "learning_rate": 2.598666666666667e-05,
      "loss": 0.0022,
      "step": 18010
    },
    {
      "epoch": 0.9610666666666666,
      "grad_norm": 0.20322063565254211,
      "learning_rate": 2.5973333333333334e-05,
      "loss": 0.0019,
      "step": 18020
    },
    {
      "epoch": 0.9616,
      "grad_norm": 0.11347699910402298,
      "learning_rate": 2.5960000000000002e-05,
      "loss": 0.0039,
      "step": 18030
    },
    {
      "epoch": 0.9621333333333333,
      "grad_norm": 0.15190812945365906,
      "learning_rate": 2.594666666666667e-05,
      "loss": 0.0021,
      "step": 18040
    },
    {
      "epoch": 0.9626666666666667,
      "grad_norm": 0.13501860201358795,
      "learning_rate": 2.5933333333333338e-05,
      "loss": 0.002,
      "step": 18050
    },
    {
      "epoch": 0.9632,
      "grad_norm": 0.3104318082332611,
      "learning_rate": 2.592e-05,
      "loss": 0.0034,
      "step": 18060
    },
    {
      "epoch": 0.9637333333333333,
      "grad_norm": 0.10125813633203506,
      "learning_rate": 2.5906666666666667e-05,
      "loss": 0.0031,
      "step": 18070
    },
    {
      "epoch": 0.9642666666666667,
      "grad_norm": 0.16581383347511292,
      "learning_rate": 2.5893333333333335e-05,
      "loss": 0.0043,
      "step": 18080
    },
    {
      "epoch": 0.9648,
      "grad_norm": 0.13592851161956787,
      "learning_rate": 2.588e-05,
      "loss": 0.0019,
      "step": 18090
    },
    {
      "epoch": 0.9653333333333334,
      "grad_norm": 0.23093102872371674,
      "learning_rate": 2.5866666666666667e-05,
      "loss": 0.0019,
      "step": 18100
    },
    {
      "epoch": 0.9658666666666667,
      "grad_norm": 0.3960772454738617,
      "learning_rate": 2.5853333333333335e-05,
      "loss": 0.0024,
      "step": 18110
    },
    {
      "epoch": 0.9664,
      "grad_norm": 0.2507420480251312,
      "learning_rate": 2.5840000000000003e-05,
      "loss": 0.0023,
      "step": 18120
    },
    {
      "epoch": 0.9669333333333333,
      "grad_norm": 0.5170394778251648,
      "learning_rate": 2.5826666666666664e-05,
      "loss": 0.0039,
      "step": 18130
    },
    {
      "epoch": 0.9674666666666667,
      "grad_norm": 0.857642650604248,
      "learning_rate": 2.5813333333333332e-05,
      "loss": 0.0031,
      "step": 18140
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.6035982370376587,
      "learning_rate": 2.58e-05,
      "loss": 0.0027,
      "step": 18150
    },
    {
      "epoch": 0.9685333333333334,
      "grad_norm": 0.5455616116523743,
      "learning_rate": 2.578666666666667e-05,
      "loss": 0.0019,
      "step": 18160
    },
    {
      "epoch": 0.9690666666666666,
      "grad_norm": 0.34179094433784485,
      "learning_rate": 2.5773333333333333e-05,
      "loss": 0.0022,
      "step": 18170
    },
    {
      "epoch": 0.9696,
      "grad_norm": 0.12296595424413681,
      "learning_rate": 2.576e-05,
      "loss": 0.0039,
      "step": 18180
    },
    {
      "epoch": 0.9701333333333333,
      "grad_norm": 0.2271839827299118,
      "learning_rate": 2.574666666666667e-05,
      "loss": 0.0022,
      "step": 18190
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 0.10376407206058502,
      "learning_rate": 2.5733333333333337e-05,
      "loss": 0.0028,
      "step": 18200
    },
    {
      "epoch": 0.9712,
      "grad_norm": 0.30060848593711853,
      "learning_rate": 2.572e-05,
      "loss": 0.0027,
      "step": 18210
    },
    {
      "epoch": 0.9717333333333333,
      "grad_norm": 0.32262784242630005,
      "learning_rate": 2.570666666666667e-05,
      "loss": 0.0031,
      "step": 18220
    },
    {
      "epoch": 0.9722666666666666,
      "grad_norm": 0.2653757929801941,
      "learning_rate": 2.5693333333333337e-05,
      "loss": 0.0024,
      "step": 18230
    },
    {
      "epoch": 0.9728,
      "grad_norm": 0.15518799424171448,
      "learning_rate": 2.5679999999999998e-05,
      "loss": 0.0028,
      "step": 18240
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 0.30124562978744507,
      "learning_rate": 2.5666666666666666e-05,
      "loss": 0.0018,
      "step": 18250
    },
    {
      "epoch": 0.9738666666666667,
      "grad_norm": 0.07570771127939224,
      "learning_rate": 2.5653333333333334e-05,
      "loss": 0.002,
      "step": 18260
    },
    {
      "epoch": 0.9744,
      "grad_norm": 0.25578683614730835,
      "learning_rate": 2.5640000000000002e-05,
      "loss": 0.0018,
      "step": 18270
    },
    {
      "epoch": 0.9749333333333333,
      "grad_norm": 0.4654620289802551,
      "learning_rate": 2.5626666666666666e-05,
      "loss": 0.0017,
      "step": 18280
    },
    {
      "epoch": 0.9754666666666667,
      "grad_norm": 0.5854537487030029,
      "learning_rate": 2.5613333333333334e-05,
      "loss": 0.0022,
      "step": 18290
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.46144795417785645,
      "learning_rate": 2.5600000000000002e-05,
      "loss": 0.0027,
      "step": 18300
    },
    {
      "epoch": 0.9765333333333334,
      "grad_norm": 0.13007564842700958,
      "learning_rate": 2.558666666666667e-05,
      "loss": 0.0028,
      "step": 18310
    },
    {
      "epoch": 0.9770666666666666,
      "grad_norm": 0.19638510048389435,
      "learning_rate": 2.557333333333333e-05,
      "loss": 0.003,
      "step": 18320
    },
    {
      "epoch": 0.9776,
      "grad_norm": 0.35764414072036743,
      "learning_rate": 2.556e-05,
      "loss": 0.0031,
      "step": 18330
    },
    {
      "epoch": 0.9781333333333333,
      "grad_norm": 0.12435032427310944,
      "learning_rate": 2.5546666666666667e-05,
      "loss": 0.0019,
      "step": 18340
    },
    {
      "epoch": 0.9786666666666667,
      "grad_norm": 0.48191866278648376,
      "learning_rate": 2.553333333333334e-05,
      "loss": 0.0028,
      "step": 18350
    },
    {
      "epoch": 0.9792,
      "grad_norm": 0.25717130303382874,
      "learning_rate": 2.552e-05,
      "loss": 0.002,
      "step": 18360
    },
    {
      "epoch": 0.9797333333333333,
      "grad_norm": 0.40039435029029846,
      "learning_rate": 2.5506666666666668e-05,
      "loss": 0.0022,
      "step": 18370
    },
    {
      "epoch": 0.9802666666666666,
      "grad_norm": 0.5719379782676697,
      "learning_rate": 2.5493333333333335e-05,
      "loss": 0.0019,
      "step": 18380
    },
    {
      "epoch": 0.9808,
      "grad_norm": 0.25404447317123413,
      "learning_rate": 2.5480000000000003e-05,
      "loss": 0.0025,
      "step": 18390
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 0.3467619717121124,
      "learning_rate": 2.5466666666666668e-05,
      "loss": 0.003,
      "step": 18400
    },
    {
      "epoch": 0.9818666666666667,
      "grad_norm": 0.4563906192779541,
      "learning_rate": 2.5453333333333336e-05,
      "loss": 0.0021,
      "step": 18410
    },
    {
      "epoch": 0.9824,
      "grad_norm": 0.21777023375034332,
      "learning_rate": 2.5440000000000004e-05,
      "loss": 0.0028,
      "step": 18420
    },
    {
      "epoch": 0.9829333333333333,
      "grad_norm": 0.26049697399139404,
      "learning_rate": 2.5426666666666665e-05,
      "loss": 0.0022,
      "step": 18430
    },
    {
      "epoch": 0.9834666666666667,
      "grad_norm": 0.2427327036857605,
      "learning_rate": 2.5413333333333333e-05,
      "loss": 0.0019,
      "step": 18440
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.377716988325119,
      "learning_rate": 2.54e-05,
      "loss": 0.002,
      "step": 18450
    },
    {
      "epoch": 0.9845333333333334,
      "grad_norm": 0.1489226073026657,
      "learning_rate": 2.538666666666667e-05,
      "loss": 0.0018,
      "step": 18460
    },
    {
      "epoch": 0.9850666666666666,
      "grad_norm": 0.3686516582965851,
      "learning_rate": 2.5373333333333333e-05,
      "loss": 0.0022,
      "step": 18470
    },
    {
      "epoch": 0.9856,
      "grad_norm": 0.8052111864089966,
      "learning_rate": 2.536e-05,
      "loss": 0.0024,
      "step": 18480
    },
    {
      "epoch": 0.9861333333333333,
      "grad_norm": 0.1656355857849121,
      "learning_rate": 2.534666666666667e-05,
      "loss": 0.0023,
      "step": 18490
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.2734018862247467,
      "learning_rate": 2.5333333333333337e-05,
      "loss": 0.0023,
      "step": 18500
    },
    {
      "epoch": 0.9872,
      "grad_norm": 0.49823951721191406,
      "learning_rate": 2.5319999999999998e-05,
      "loss": 0.0038,
      "step": 18510
    },
    {
      "epoch": 0.9877333333333334,
      "grad_norm": 0.4628291428089142,
      "learning_rate": 2.5306666666666666e-05,
      "loss": 0.003,
      "step": 18520
    },
    {
      "epoch": 0.9882666666666666,
      "grad_norm": 0.09681028127670288,
      "learning_rate": 2.5293333333333334e-05,
      "loss": 0.0027,
      "step": 18530
    },
    {
      "epoch": 0.9888,
      "grad_norm": 0.43362870812416077,
      "learning_rate": 2.5280000000000005e-05,
      "loss": 0.0022,
      "step": 18540
    },
    {
      "epoch": 0.9893333333333333,
      "grad_norm": 0.14785702526569366,
      "learning_rate": 2.5266666666666666e-05,
      "loss": 0.0023,
      "step": 18550
    },
    {
      "epoch": 0.9898666666666667,
      "grad_norm": 0.31376415491104126,
      "learning_rate": 2.5253333333333334e-05,
      "loss": 0.002,
      "step": 18560
    },
    {
      "epoch": 0.9904,
      "grad_norm": 0.11898722499608994,
      "learning_rate": 2.5240000000000002e-05,
      "loss": 0.0027,
      "step": 18570
    },
    {
      "epoch": 0.9909333333333333,
      "grad_norm": 0.9975359439849854,
      "learning_rate": 2.5226666666666663e-05,
      "loss": 0.0029,
      "step": 18580
    },
    {
      "epoch": 0.9914666666666667,
      "grad_norm": 0.45077064633369446,
      "learning_rate": 2.5213333333333335e-05,
      "loss": 0.0024,
      "step": 18590
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.5445218086242676,
      "learning_rate": 2.5200000000000003e-05,
      "loss": 0.002,
      "step": 18600
    },
    {
      "epoch": 0.9925333333333334,
      "grad_norm": 0.11782016605138779,
      "learning_rate": 2.518666666666667e-05,
      "loss": 0.0016,
      "step": 18610
    },
    {
      "epoch": 0.9930666666666667,
      "grad_norm": 0.3183146119117737,
      "learning_rate": 2.5173333333333332e-05,
      "loss": 0.0031,
      "step": 18620
    },
    {
      "epoch": 0.9936,
      "grad_norm": 0.10511370003223419,
      "learning_rate": 2.516e-05,
      "loss": 0.002,
      "step": 18630
    },
    {
      "epoch": 0.9941333333333333,
      "grad_norm": 0.20836445689201355,
      "learning_rate": 2.5146666666666668e-05,
      "loss": 0.002,
      "step": 18640
    },
    {
      "epoch": 0.9946666666666667,
      "grad_norm": 0.23895163834095,
      "learning_rate": 2.5133333333333336e-05,
      "loss": 0.0019,
      "step": 18650
    },
    {
      "epoch": 0.9952,
      "grad_norm": 0.250479131937027,
      "learning_rate": 2.512e-05,
      "loss": 0.0031,
      "step": 18660
    },
    {
      "epoch": 0.9957333333333334,
      "grad_norm": 0.44280555844306946,
      "learning_rate": 2.5106666666666668e-05,
      "loss": 0.0021,
      "step": 18670
    },
    {
      "epoch": 0.9962666666666666,
      "grad_norm": 0.33348318934440613,
      "learning_rate": 2.5093333333333336e-05,
      "loss": 0.0031,
      "step": 18680
    },
    {
      "epoch": 0.9968,
      "grad_norm": 0.7529667615890503,
      "learning_rate": 2.5080000000000004e-05,
      "loss": 0.0028,
      "step": 18690
    },
    {
      "epoch": 0.9973333333333333,
      "grad_norm": 0.05199206992983818,
      "learning_rate": 2.5066666666666665e-05,
      "loss": 0.0016,
      "step": 18700
    },
    {
      "epoch": 0.9978666666666667,
      "grad_norm": 0.32452064752578735,
      "learning_rate": 2.5053333333333333e-05,
      "loss": 0.0027,
      "step": 18710
    },
    {
      "epoch": 0.9984,
      "grad_norm": 0.2708536684513092,
      "learning_rate": 2.504e-05,
      "loss": 0.0016,
      "step": 18720
    },
    {
      "epoch": 0.9989333333333333,
      "grad_norm": 0.22692044079303741,
      "learning_rate": 2.5026666666666672e-05,
      "loss": 0.0026,
      "step": 18730
    },
    {
      "epoch": 0.9994666666666666,
      "grad_norm": 0.13084889948368073,
      "learning_rate": 2.5013333333333333e-05,
      "loss": 0.0025,
      "step": 18740
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.1627376526594162,
      "learning_rate": 2.5e-05,
      "loss": 0.0026,
      "step": 18750
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.002584029221907258,
      "eval_runtime": 170.9884,
      "eval_samples_per_second": 1462.087,
      "eval_steps_per_second": 36.552,
      "step": 18750
    },
    {
      "epoch": 1.0005333333333333,
      "grad_norm": 0.611420750617981,
      "learning_rate": 2.4986666666666666e-05,
      "loss": 0.0043,
      "step": 18760
    },
    {
      "epoch": 1.0010666666666668,
      "grad_norm": 0.3689095973968506,
      "learning_rate": 2.4973333333333334e-05,
      "loss": 0.0029,
      "step": 18770
    },
    {
      "epoch": 1.0016,
      "grad_norm": 0.2887875735759735,
      "learning_rate": 2.496e-05,
      "loss": 0.0019,
      "step": 18780
    },
    {
      "epoch": 1.0021333333333333,
      "grad_norm": 0.08904699981212616,
      "learning_rate": 2.494666666666667e-05,
      "loss": 0.0023,
      "step": 18790
    },
    {
      "epoch": 1.0026666666666666,
      "grad_norm": 0.09707938879728317,
      "learning_rate": 2.4933333333333334e-05,
      "loss": 0.0019,
      "step": 18800
    },
    {
      "epoch": 1.0032,
      "grad_norm": 0.17896607518196106,
      "learning_rate": 2.4920000000000002e-05,
      "loss": 0.0033,
      "step": 18810
    },
    {
      "epoch": 1.0037333333333334,
      "grad_norm": 0.2003653198480606,
      "learning_rate": 2.4906666666666666e-05,
      "loss": 0.0021,
      "step": 18820
    },
    {
      "epoch": 1.0042666666666666,
      "grad_norm": 0.16093918681144714,
      "learning_rate": 2.4893333333333334e-05,
      "loss": 0.003,
      "step": 18830
    },
    {
      "epoch": 1.0048,
      "grad_norm": 0.46697524189949036,
      "learning_rate": 2.488e-05,
      "loss": 0.0023,
      "step": 18840
    },
    {
      "epoch": 1.0053333333333334,
      "grad_norm": 0.13235855102539062,
      "learning_rate": 2.486666666666667e-05,
      "loss": 0.0021,
      "step": 18850
    },
    {
      "epoch": 1.0058666666666667,
      "grad_norm": 0.6661743521690369,
      "learning_rate": 2.4853333333333335e-05,
      "loss": 0.0029,
      "step": 18860
    },
    {
      "epoch": 1.0064,
      "grad_norm": 0.5058395266532898,
      "learning_rate": 2.4840000000000003e-05,
      "loss": 0.002,
      "step": 18870
    },
    {
      "epoch": 1.0069333333333332,
      "grad_norm": 0.12520864605903625,
      "learning_rate": 2.4826666666666667e-05,
      "loss": 0.0036,
      "step": 18880
    },
    {
      "epoch": 1.0074666666666667,
      "grad_norm": 0.1503651887178421,
      "learning_rate": 2.4813333333333335e-05,
      "loss": 0.0026,
      "step": 18890
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.7527775168418884,
      "learning_rate": 2.48e-05,
      "loss": 0.0016,
      "step": 18900
    },
    {
      "epoch": 1.0085333333333333,
      "grad_norm": 0.07634014636278152,
      "learning_rate": 2.4786666666666668e-05,
      "loss": 0.0019,
      "step": 18910
    },
    {
      "epoch": 1.0090666666666666,
      "grad_norm": 0.08250341564416885,
      "learning_rate": 2.4773333333333336e-05,
      "loss": 0.0028,
      "step": 18920
    },
    {
      "epoch": 1.0096,
      "grad_norm": 0.08488026261329651,
      "learning_rate": 2.476e-05,
      "loss": 0.0024,
      "step": 18930
    },
    {
      "epoch": 1.0101333333333333,
      "grad_norm": 0.4992140531539917,
      "learning_rate": 2.4746666666666668e-05,
      "loss": 0.0019,
      "step": 18940
    },
    {
      "epoch": 1.0106666666666666,
      "grad_norm": 0.3940385580062866,
      "learning_rate": 2.4733333333333333e-05,
      "loss": 0.0037,
      "step": 18950
    },
    {
      "epoch": 1.0112,
      "grad_norm": 0.2586360275745392,
      "learning_rate": 2.472e-05,
      "loss": 0.0029,
      "step": 18960
    },
    {
      "epoch": 1.0117333333333334,
      "grad_norm": 0.3026076555252075,
      "learning_rate": 2.470666666666667e-05,
      "loss": 0.002,
      "step": 18970
    },
    {
      "epoch": 1.0122666666666666,
      "grad_norm": 0.3701019585132599,
      "learning_rate": 2.4693333333333336e-05,
      "loss": 0.004,
      "step": 18980
    },
    {
      "epoch": 1.0128,
      "grad_norm": 0.14203262329101562,
      "learning_rate": 2.468e-05,
      "loss": 0.002,
      "step": 18990
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.46161583065986633,
      "learning_rate": 2.466666666666667e-05,
      "loss": 0.0024,
      "step": 19000
    },
    {
      "epoch": 1.0138666666666667,
      "grad_norm": 0.1049327552318573,
      "learning_rate": 2.4653333333333333e-05,
      "loss": 0.002,
      "step": 19010
    },
    {
      "epoch": 1.0144,
      "grad_norm": 0.2993354797363281,
      "learning_rate": 2.464e-05,
      "loss": 0.0023,
      "step": 19020
    },
    {
      "epoch": 1.0149333333333332,
      "grad_norm": 0.26585817337036133,
      "learning_rate": 2.4626666666666666e-05,
      "loss": 0.0024,
      "step": 19030
    },
    {
      "epoch": 1.0154666666666667,
      "grad_norm": 0.1365787833929062,
      "learning_rate": 2.4613333333333337e-05,
      "loss": 0.0028,
      "step": 19040
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.23339644074440002,
      "learning_rate": 2.46e-05,
      "loss": 0.0022,
      "step": 19050
    },
    {
      "epoch": 1.0165333333333333,
      "grad_norm": 0.6167640089988708,
      "learning_rate": 2.458666666666667e-05,
      "loss": 0.0024,
      "step": 19060
    },
    {
      "epoch": 1.0170666666666666,
      "grad_norm": 0.09398830682039261,
      "learning_rate": 2.4573333333333334e-05,
      "loss": 0.0019,
      "step": 19070
    },
    {
      "epoch": 1.0176,
      "grad_norm": 0.1506492644548416,
      "learning_rate": 2.4560000000000002e-05,
      "loss": 0.0021,
      "step": 19080
    },
    {
      "epoch": 1.0181333333333333,
      "grad_norm": 0.24970127642154694,
      "learning_rate": 2.4546666666666667e-05,
      "loss": 0.0024,
      "step": 19090
    },
    {
      "epoch": 1.0186666666666666,
      "grad_norm": 0.25821343064308167,
      "learning_rate": 2.4533333333333334e-05,
      "loss": 0.0025,
      "step": 19100
    },
    {
      "epoch": 1.0192,
      "grad_norm": 0.1113172322511673,
      "learning_rate": 2.4520000000000002e-05,
      "loss": 0.0043,
      "step": 19110
    },
    {
      "epoch": 1.0197333333333334,
      "grad_norm": 0.40049630403518677,
      "learning_rate": 2.4506666666666667e-05,
      "loss": 0.002,
      "step": 19120
    },
    {
      "epoch": 1.0202666666666667,
      "grad_norm": 0.5291111469268799,
      "learning_rate": 2.4493333333333335e-05,
      "loss": 0.0037,
      "step": 19130
    },
    {
      "epoch": 1.0208,
      "grad_norm": 0.16017453372478485,
      "learning_rate": 2.448e-05,
      "loss": 0.0021,
      "step": 19140
    },
    {
      "epoch": 1.0213333333333334,
      "grad_norm": 0.33877792954444885,
      "learning_rate": 2.4466666666666667e-05,
      "loss": 0.0032,
      "step": 19150
    },
    {
      "epoch": 1.0218666666666667,
      "grad_norm": 0.15669630467891693,
      "learning_rate": 2.4453333333333335e-05,
      "loss": 0.0019,
      "step": 19160
    },
    {
      "epoch": 1.0224,
      "grad_norm": 0.21640287339687347,
      "learning_rate": 2.4440000000000003e-05,
      "loss": 0.002,
      "step": 19170
    },
    {
      "epoch": 1.0229333333333333,
      "grad_norm": 0.09942387044429779,
      "learning_rate": 2.4426666666666668e-05,
      "loss": 0.0021,
      "step": 19180
    },
    {
      "epoch": 1.0234666666666667,
      "grad_norm": 0.2612783908843994,
      "learning_rate": 2.4413333333333336e-05,
      "loss": 0.0029,
      "step": 19190
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.42063358426094055,
      "learning_rate": 2.44e-05,
      "loss": 0.0029,
      "step": 19200
    },
    {
      "epoch": 1.0245333333333333,
      "grad_norm": 0.41895467042922974,
      "learning_rate": 2.4386666666666668e-05,
      "loss": 0.0018,
      "step": 19210
    },
    {
      "epoch": 1.0250666666666666,
      "grad_norm": 0.8649448156356812,
      "learning_rate": 2.4373333333333333e-05,
      "loss": 0.0037,
      "step": 19220
    },
    {
      "epoch": 1.0256,
      "grad_norm": 0.6261858344078064,
      "learning_rate": 2.4360000000000004e-05,
      "loss": 0.0022,
      "step": 19230
    },
    {
      "epoch": 1.0261333333333333,
      "grad_norm": 0.7324044704437256,
      "learning_rate": 2.434666666666667e-05,
      "loss": 0.003,
      "step": 19240
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 0.1563396155834198,
      "learning_rate": 2.4333333333333336e-05,
      "loss": 0.0026,
      "step": 19250
    },
    {
      "epoch": 1.0272,
      "grad_norm": 0.20533399283885956,
      "learning_rate": 2.432e-05,
      "loss": 0.0026,
      "step": 19260
    },
    {
      "epoch": 1.0277333333333334,
      "grad_norm": 0.16826947033405304,
      "learning_rate": 2.4306666666666665e-05,
      "loss": 0.0032,
      "step": 19270
    },
    {
      "epoch": 1.0282666666666667,
      "grad_norm": 0.1276090145111084,
      "learning_rate": 2.4293333333333333e-05,
      "loss": 0.0024,
      "step": 19280
    },
    {
      "epoch": 1.0288,
      "grad_norm": 0.4279809892177582,
      "learning_rate": 2.428e-05,
      "loss": 0.0029,
      "step": 19290
    },
    {
      "epoch": 1.0293333333333334,
      "grad_norm": 0.3314538896083832,
      "learning_rate": 2.426666666666667e-05,
      "loss": 0.002,
      "step": 19300
    },
    {
      "epoch": 1.0298666666666667,
      "grad_norm": 0.2447202503681183,
      "learning_rate": 2.4253333333333334e-05,
      "loss": 0.0028,
      "step": 19310
    },
    {
      "epoch": 1.0304,
      "grad_norm": 0.16729846596717834,
      "learning_rate": 2.4240000000000002e-05,
      "loss": 0.002,
      "step": 19320
    },
    {
      "epoch": 1.0309333333333333,
      "grad_norm": 0.1871950775384903,
      "learning_rate": 2.4226666666666666e-05,
      "loss": 0.002,
      "step": 19330
    },
    {
      "epoch": 1.0314666666666668,
      "grad_norm": 0.2607346475124359,
      "learning_rate": 2.4213333333333334e-05,
      "loss": 0.0029,
      "step": 19340
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.365995854139328,
      "learning_rate": 2.4200000000000002e-05,
      "loss": 0.0038,
      "step": 19350
    },
    {
      "epoch": 1.0325333333333333,
      "grad_norm": 0.1422634720802307,
      "learning_rate": 2.418666666666667e-05,
      "loss": 0.003,
      "step": 19360
    },
    {
      "epoch": 1.0330666666666666,
      "grad_norm": 0.5235889554023743,
      "learning_rate": 2.4173333333333335e-05,
      "loss": 0.002,
      "step": 19370
    },
    {
      "epoch": 1.0336,
      "grad_norm": 0.127383291721344,
      "learning_rate": 2.4160000000000002e-05,
      "loss": 0.0038,
      "step": 19380
    },
    {
      "epoch": 1.0341333333333333,
      "grad_norm": 0.3118751347064972,
      "learning_rate": 2.4146666666666667e-05,
      "loss": 0.0021,
      "step": 19390
    },
    {
      "epoch": 1.0346666666666666,
      "grad_norm": 0.23462772369384766,
      "learning_rate": 2.4133333333333335e-05,
      "loss": 0.003,
      "step": 19400
    },
    {
      "epoch": 1.0352,
      "grad_norm": 0.15317581593990326,
      "learning_rate": 2.412e-05,
      "loss": 0.0026,
      "step": 19410
    },
    {
      "epoch": 1.0357333333333334,
      "grad_norm": 0.16456818580627441,
      "learning_rate": 2.4106666666666667e-05,
      "loss": 0.0028,
      "step": 19420
    },
    {
      "epoch": 1.0362666666666667,
      "grad_norm": 0.44161760807037354,
      "learning_rate": 2.4093333333333335e-05,
      "loss": 0.0021,
      "step": 19430
    },
    {
      "epoch": 1.0368,
      "grad_norm": 0.13811099529266357,
      "learning_rate": 2.408e-05,
      "loss": 0.0018,
      "step": 19440
    },
    {
      "epoch": 1.0373333333333334,
      "grad_norm": 0.2257276475429535,
      "learning_rate": 2.4066666666666668e-05,
      "loss": 0.0022,
      "step": 19450
    },
    {
      "epoch": 1.0378666666666667,
      "grad_norm": 0.3876054286956787,
      "learning_rate": 2.4053333333333332e-05,
      "loss": 0.0028,
      "step": 19460
    },
    {
      "epoch": 1.0384,
      "grad_norm": 0.4702407121658325,
      "learning_rate": 2.404e-05,
      "loss": 0.002,
      "step": 19470
    },
    {
      "epoch": 1.0389333333333333,
      "grad_norm": 0.18610833585262299,
      "learning_rate": 2.4026666666666668e-05,
      "loss": 0.0028,
      "step": 19480
    },
    {
      "epoch": 1.0394666666666668,
      "grad_norm": 0.14935393631458282,
      "learning_rate": 2.4013333333333336e-05,
      "loss": 0.003,
      "step": 19490
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.24324631690979004,
      "learning_rate": 2.4e-05,
      "loss": 0.0022,
      "step": 19500
    },
    {
      "epoch": 1.0405333333333333,
      "grad_norm": 0.11551710963249207,
      "learning_rate": 2.398666666666667e-05,
      "loss": 0.0035,
      "step": 19510
    },
    {
      "epoch": 1.0410666666666666,
      "grad_norm": 0.10610804706811905,
      "learning_rate": 2.3973333333333333e-05,
      "loss": 0.0018,
      "step": 19520
    },
    {
      "epoch": 1.0416,
      "grad_norm": 0.1592310667037964,
      "learning_rate": 2.396e-05,
      "loss": 0.0026,
      "step": 19530
    },
    {
      "epoch": 1.0421333333333334,
      "grad_norm": 0.569182813167572,
      "learning_rate": 2.394666666666667e-05,
      "loss": 0.0036,
      "step": 19540
    },
    {
      "epoch": 1.0426666666666666,
      "grad_norm": 0.47500520944595337,
      "learning_rate": 2.3933333333333337e-05,
      "loss": 0.0025,
      "step": 19550
    },
    {
      "epoch": 1.0432,
      "grad_norm": 0.11054045706987381,
      "learning_rate": 2.392e-05,
      "loss": 0.002,
      "step": 19560
    },
    {
      "epoch": 1.0437333333333334,
      "grad_norm": 0.43064484000205994,
      "learning_rate": 2.390666666666667e-05,
      "loss": 0.0033,
      "step": 19570
    },
    {
      "epoch": 1.0442666666666667,
      "grad_norm": 0.09232546389102936,
      "learning_rate": 2.3893333333333334e-05,
      "loss": 0.002,
      "step": 19580
    },
    {
      "epoch": 1.0448,
      "grad_norm": 0.10817326605319977,
      "learning_rate": 2.3880000000000002e-05,
      "loss": 0.0033,
      "step": 19590
    },
    {
      "epoch": 1.0453333333333332,
      "grad_norm": 0.2743453085422516,
      "learning_rate": 2.3866666666666666e-05,
      "loss": 0.0021,
      "step": 19600
    },
    {
      "epoch": 1.0458666666666667,
      "grad_norm": 0.14700160920619965,
      "learning_rate": 2.3853333333333334e-05,
      "loss": 0.0025,
      "step": 19610
    },
    {
      "epoch": 1.0464,
      "grad_norm": 0.5809934735298157,
      "learning_rate": 2.3840000000000002e-05,
      "loss": 0.002,
      "step": 19620
    },
    {
      "epoch": 1.0469333333333333,
      "grad_norm": 0.4149308502674103,
      "learning_rate": 2.3826666666666667e-05,
      "loss": 0.0018,
      "step": 19630
    },
    {
      "epoch": 1.0474666666666668,
      "grad_norm": 0.9158692955970764,
      "learning_rate": 2.3813333333333335e-05,
      "loss": 0.0024,
      "step": 19640
    },
    {
      "epoch": 1.048,
      "grad_norm": 0.23007987439632416,
      "learning_rate": 2.38e-05,
      "loss": 0.0036,
      "step": 19650
    },
    {
      "epoch": 1.0485333333333333,
      "grad_norm": 0.48970943689346313,
      "learning_rate": 2.3786666666666667e-05,
      "loss": 0.0026,
      "step": 19660
    },
    {
      "epoch": 1.0490666666666666,
      "grad_norm": 0.9237986207008362,
      "learning_rate": 2.3773333333333335e-05,
      "loss": 0.0023,
      "step": 19670
    },
    {
      "epoch": 1.0496,
      "grad_norm": 0.47366535663604736,
      "learning_rate": 2.3760000000000003e-05,
      "loss": 0.003,
      "step": 19680
    },
    {
      "epoch": 1.0501333333333334,
      "grad_norm": 0.28294408321380615,
      "learning_rate": 2.3746666666666667e-05,
      "loss": 0.0024,
      "step": 19690
    },
    {
      "epoch": 1.0506666666666666,
      "grad_norm": 0.23387013375759125,
      "learning_rate": 2.3733333333333335e-05,
      "loss": 0.0024,
      "step": 19700
    },
    {
      "epoch": 1.0512,
      "grad_norm": 0.22564327716827393,
      "learning_rate": 2.372e-05,
      "loss": 0.0022,
      "step": 19710
    },
    {
      "epoch": 1.0517333333333334,
      "grad_norm": 0.14648422598838806,
      "learning_rate": 2.3706666666666668e-05,
      "loss": 0.003,
      "step": 19720
    },
    {
      "epoch": 1.0522666666666667,
      "grad_norm": 0.18681465089321136,
      "learning_rate": 2.3693333333333332e-05,
      "loss": 0.0029,
      "step": 19730
    },
    {
      "epoch": 1.0528,
      "grad_norm": 0.2389623522758484,
      "learning_rate": 2.3680000000000004e-05,
      "loss": 0.0026,
      "step": 19740
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.1965867280960083,
      "learning_rate": 2.3666666666666668e-05,
      "loss": 0.002,
      "step": 19750
    },
    {
      "epoch": 1.0538666666666667,
      "grad_norm": 0.2659386098384857,
      "learning_rate": 2.3653333333333336e-05,
      "loss": 0.0019,
      "step": 19760
    },
    {
      "epoch": 1.0544,
      "grad_norm": 0.09794643521308899,
      "learning_rate": 2.364e-05,
      "loss": 0.0035,
      "step": 19770
    },
    {
      "epoch": 1.0549333333333333,
      "grad_norm": 0.06557746231555939,
      "learning_rate": 2.362666666666667e-05,
      "loss": 0.002,
      "step": 19780
    },
    {
      "epoch": 1.0554666666666668,
      "grad_norm": 0.37534821033477783,
      "learning_rate": 2.3613333333333333e-05,
      "loss": 0.0035,
      "step": 19790
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.27808961272239685,
      "learning_rate": 2.36e-05,
      "loss": 0.0029,
      "step": 19800
    },
    {
      "epoch": 1.0565333333333333,
      "grad_norm": 0.10979751497507095,
      "learning_rate": 2.358666666666667e-05,
      "loss": 0.0023,
      "step": 19810
    },
    {
      "epoch": 1.0570666666666666,
      "grad_norm": 0.47838109731674194,
      "learning_rate": 2.3573333333333334e-05,
      "loss": 0.0024,
      "step": 19820
    },
    {
      "epoch": 1.0576,
      "grad_norm": 0.2614479959011078,
      "learning_rate": 2.356e-05,
      "loss": 0.0029,
      "step": 19830
    },
    {
      "epoch": 1.0581333333333334,
      "grad_norm": 0.4177408516407013,
      "learning_rate": 2.3546666666666666e-05,
      "loss": 0.0041,
      "step": 19840
    },
    {
      "epoch": 1.0586666666666666,
      "grad_norm": 0.20976075530052185,
      "learning_rate": 2.3533333333333334e-05,
      "loss": 0.0021,
      "step": 19850
    },
    {
      "epoch": 1.0592,
      "grad_norm": 0.36200258135795593,
      "learning_rate": 2.3520000000000002e-05,
      "loss": 0.0026,
      "step": 19860
    },
    {
      "epoch": 1.0597333333333334,
      "grad_norm": 0.2840701937675476,
      "learning_rate": 2.350666666666667e-05,
      "loss": 0.0034,
      "step": 19870
    },
    {
      "epoch": 1.0602666666666667,
      "grad_norm": 0.06982791423797607,
      "learning_rate": 2.3493333333333334e-05,
      "loss": 0.0029,
      "step": 19880
    },
    {
      "epoch": 1.0608,
      "grad_norm": 0.12223983556032181,
      "learning_rate": 2.3480000000000002e-05,
      "loss": 0.0031,
      "step": 19890
    },
    {
      "epoch": 1.0613333333333332,
      "grad_norm": 0.7956839799880981,
      "learning_rate": 2.3466666666666667e-05,
      "loss": 0.0024,
      "step": 19900
    },
    {
      "epoch": 1.0618666666666667,
      "grad_norm": 0.20468449592590332,
      "learning_rate": 2.3453333333333335e-05,
      "loss": 0.0028,
      "step": 19910
    },
    {
      "epoch": 1.0624,
      "grad_norm": 0.08588528633117676,
      "learning_rate": 2.344e-05,
      "loss": 0.0025,
      "step": 19920
    },
    {
      "epoch": 1.0629333333333333,
      "grad_norm": 0.1424434781074524,
      "learning_rate": 2.342666666666667e-05,
      "loss": 0.002,
      "step": 19930
    },
    {
      "epoch": 1.0634666666666668,
      "grad_norm": 0.6018003821372986,
      "learning_rate": 2.3413333333333335e-05,
      "loss": 0.0025,
      "step": 19940
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.2695985734462738,
      "learning_rate": 2.3400000000000003e-05,
      "loss": 0.0029,
      "step": 19950
    },
    {
      "epoch": 1.0645333333333333,
      "grad_norm": 0.11269556730985641,
      "learning_rate": 2.3386666666666668e-05,
      "loss": 0.0036,
      "step": 19960
    },
    {
      "epoch": 1.0650666666666666,
      "grad_norm": 0.10972196608781815,
      "learning_rate": 2.3373333333333332e-05,
      "loss": 0.002,
      "step": 19970
    },
    {
      "epoch": 1.0656,
      "grad_norm": 0.516020655632019,
      "learning_rate": 2.336e-05,
      "loss": 0.0028,
      "step": 19980
    },
    {
      "epoch": 1.0661333333333334,
      "grad_norm": 0.2992303967475891,
      "learning_rate": 2.3346666666666668e-05,
      "loss": 0.0025,
      "step": 19990
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.14518587291240692,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 0.0022,
      "step": 20000
    },
    {
      "epoch": 1.0672,
      "grad_norm": 0.16866321861743927,
      "learning_rate": 2.332e-05,
      "loss": 0.0031,
      "step": 20010
    },
    {
      "epoch": 1.0677333333333334,
      "grad_norm": 0.1937987357378006,
      "learning_rate": 2.3306666666666668e-05,
      "loss": 0.0024,
      "step": 20020
    },
    {
      "epoch": 1.0682666666666667,
      "grad_norm": 0.2247774749994278,
      "learning_rate": 2.3293333333333333e-05,
      "loss": 0.0022,
      "step": 20030
    },
    {
      "epoch": 1.0688,
      "grad_norm": 0.2844795286655426,
      "learning_rate": 2.328e-05,
      "loss": 0.0022,
      "step": 20040
    },
    {
      "epoch": 1.0693333333333332,
      "grad_norm": 0.21150967478752136,
      "learning_rate": 2.326666666666667e-05,
      "loss": 0.0021,
      "step": 20050
    },
    {
      "epoch": 1.0698666666666667,
      "grad_norm": 0.20357148349285126,
      "learning_rate": 2.3253333333333337e-05,
      "loss": 0.0023,
      "step": 20060
    },
    {
      "epoch": 1.0704,
      "grad_norm": 0.32868143916130066,
      "learning_rate": 2.324e-05,
      "loss": 0.0019,
      "step": 20070
    },
    {
      "epoch": 1.0709333333333333,
      "grad_norm": 0.19934684038162231,
      "learning_rate": 2.322666666666667e-05,
      "loss": 0.0019,
      "step": 20080
    },
    {
      "epoch": 1.0714666666666666,
      "grad_norm": 0.3329254984855652,
      "learning_rate": 2.3213333333333334e-05,
      "loss": 0.0034,
      "step": 20090
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.14071664214134216,
      "learning_rate": 2.32e-05,
      "loss": 0.0022,
      "step": 20100
    },
    {
      "epoch": 1.0725333333333333,
      "grad_norm": 0.5675889849662781,
      "learning_rate": 2.3186666666666666e-05,
      "loss": 0.0034,
      "step": 20110
    },
    {
      "epoch": 1.0730666666666666,
      "grad_norm": 0.21899893879890442,
      "learning_rate": 2.3173333333333337e-05,
      "loss": 0.0022,
      "step": 20120
    },
    {
      "epoch": 1.0735999999999999,
      "grad_norm": 0.45221656560897827,
      "learning_rate": 2.3160000000000002e-05,
      "loss": 0.0018,
      "step": 20130
    },
    {
      "epoch": 1.0741333333333334,
      "grad_norm": 0.09593038260936737,
      "learning_rate": 2.3146666666666666e-05,
      "loss": 0.0035,
      "step": 20140
    },
    {
      "epoch": 1.0746666666666667,
      "grad_norm": 0.1509801298379898,
      "learning_rate": 2.3133333333333334e-05,
      "loss": 0.0022,
      "step": 20150
    },
    {
      "epoch": 1.0752,
      "grad_norm": 0.1581575721502304,
      "learning_rate": 2.312e-05,
      "loss": 0.0021,
      "step": 20160
    },
    {
      "epoch": 1.0757333333333334,
      "grad_norm": 0.38625264167785645,
      "learning_rate": 2.3106666666666667e-05,
      "loss": 0.0021,
      "step": 20170
    },
    {
      "epoch": 1.0762666666666667,
      "grad_norm": 0.5430892705917358,
      "learning_rate": 2.3093333333333335e-05,
      "loss": 0.0029,
      "step": 20180
    },
    {
      "epoch": 1.0768,
      "grad_norm": 0.26220810413360596,
      "learning_rate": 2.3080000000000003e-05,
      "loss": 0.002,
      "step": 20190
    },
    {
      "epoch": 1.0773333333333333,
      "grad_norm": 0.19466105103492737,
      "learning_rate": 2.3066666666666667e-05,
      "loss": 0.0022,
      "step": 20200
    },
    {
      "epoch": 1.0778666666666668,
      "grad_norm": 0.25427064299583435,
      "learning_rate": 2.3053333333333335e-05,
      "loss": 0.0022,
      "step": 20210
    },
    {
      "epoch": 1.0784,
      "grad_norm": 0.5512209534645081,
      "learning_rate": 2.304e-05,
      "loss": 0.0024,
      "step": 20220
    },
    {
      "epoch": 1.0789333333333333,
      "grad_norm": 0.12581118941307068,
      "learning_rate": 2.3026666666666668e-05,
      "loss": 0.0029,
      "step": 20230
    },
    {
      "epoch": 1.0794666666666666,
      "grad_norm": 0.26955392956733704,
      "learning_rate": 2.3013333333333335e-05,
      "loss": 0.0023,
      "step": 20240
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.1545913815498352,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.0025,
      "step": 20250
    },
    {
      "epoch": 1.0805333333333333,
      "grad_norm": 0.490784227848053,
      "learning_rate": 2.2986666666666668e-05,
      "loss": 0.0021,
      "step": 20260
    },
    {
      "epoch": 1.0810666666666666,
      "grad_norm": 0.2987881600856781,
      "learning_rate": 2.2973333333333336e-05,
      "loss": 0.0022,
      "step": 20270
    },
    {
      "epoch": 1.0816,
      "grad_norm": 0.7659452557563782,
      "learning_rate": 2.296e-05,
      "loss": 0.0021,
      "step": 20280
    },
    {
      "epoch": 1.0821333333333334,
      "grad_norm": 0.36603137850761414,
      "learning_rate": 2.294666666666667e-05,
      "loss": 0.0026,
      "step": 20290
    },
    {
      "epoch": 1.0826666666666667,
      "grad_norm": 0.18895916640758514,
      "learning_rate": 2.2933333333333333e-05,
      "loss": 0.0032,
      "step": 20300
    },
    {
      "epoch": 1.0832,
      "grad_norm": 0.1034151241183281,
      "learning_rate": 2.292e-05,
      "loss": 0.0022,
      "step": 20310
    },
    {
      "epoch": 1.0837333333333334,
      "grad_norm": 0.1879846453666687,
      "learning_rate": 2.290666666666667e-05,
      "loss": 0.0026,
      "step": 20320
    },
    {
      "epoch": 1.0842666666666667,
      "grad_norm": 0.31513649225234985,
      "learning_rate": 2.2893333333333333e-05,
      "loss": 0.0046,
      "step": 20330
    },
    {
      "epoch": 1.0848,
      "grad_norm": 0.5418205857276917,
      "learning_rate": 2.288e-05,
      "loss": 0.0029,
      "step": 20340
    },
    {
      "epoch": 1.0853333333333333,
      "grad_norm": 0.17213331162929535,
      "learning_rate": 2.2866666666666666e-05,
      "loss": 0.0019,
      "step": 20350
    },
    {
      "epoch": 1.0858666666666668,
      "grad_norm": 0.4217202365398407,
      "learning_rate": 2.2853333333333334e-05,
      "loss": 0.0018,
      "step": 20360
    },
    {
      "epoch": 1.0864,
      "grad_norm": 0.46755555272102356,
      "learning_rate": 2.284e-05,
      "loss": 0.002,
      "step": 20370
    },
    {
      "epoch": 1.0869333333333333,
      "grad_norm": 0.13330362737178802,
      "learning_rate": 2.282666666666667e-05,
      "loss": 0.0032,
      "step": 20380
    },
    {
      "epoch": 1.0874666666666666,
      "grad_norm": 0.6184502840042114,
      "learning_rate": 2.2813333333333334e-05,
      "loss": 0.0026,
      "step": 20390
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.16537822782993317,
      "learning_rate": 2.2800000000000002e-05,
      "loss": 0.0039,
      "step": 20400
    },
    {
      "epoch": 1.0885333333333334,
      "grad_norm": 0.19660256803035736,
      "learning_rate": 2.2786666666666666e-05,
      "loss": 0.003,
      "step": 20410
    },
    {
      "epoch": 1.0890666666666666,
      "grad_norm": 0.4165649116039276,
      "learning_rate": 2.2773333333333334e-05,
      "loss": 0.004,
      "step": 20420
    },
    {
      "epoch": 1.0896,
      "grad_norm": 0.24928629398345947,
      "learning_rate": 2.2760000000000002e-05,
      "loss": 0.002,
      "step": 20430
    },
    {
      "epoch": 1.0901333333333334,
      "grad_norm": 0.22657914459705353,
      "learning_rate": 2.274666666666667e-05,
      "loss": 0.0024,
      "step": 20440
    },
    {
      "epoch": 1.0906666666666667,
      "grad_norm": 0.16650305688381195,
      "learning_rate": 2.2733333333333335e-05,
      "loss": 0.0021,
      "step": 20450
    },
    {
      "epoch": 1.0912,
      "grad_norm": 0.25853002071380615,
      "learning_rate": 2.2720000000000003e-05,
      "loss": 0.0029,
      "step": 20460
    },
    {
      "epoch": 1.0917333333333334,
      "grad_norm": 0.5779824256896973,
      "learning_rate": 2.2706666666666667e-05,
      "loss": 0.003,
      "step": 20470
    },
    {
      "epoch": 1.0922666666666667,
      "grad_norm": 0.5480002760887146,
      "learning_rate": 2.2693333333333332e-05,
      "loss": 0.0022,
      "step": 20480
    },
    {
      "epoch": 1.0928,
      "grad_norm": 0.7527221441268921,
      "learning_rate": 2.268e-05,
      "loss": 0.0023,
      "step": 20490
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 0.7640887498855591,
      "learning_rate": 2.2666666666666668e-05,
      "loss": 0.0021,
      "step": 20500
    },
    {
      "epoch": 1.0938666666666668,
      "grad_norm": 0.3003440499305725,
      "learning_rate": 2.2653333333333336e-05,
      "loss": 0.0021,
      "step": 20510
    },
    {
      "epoch": 1.0944,
      "grad_norm": 0.13009195029735565,
      "learning_rate": 2.264e-05,
      "loss": 0.0033,
      "step": 20520
    },
    {
      "epoch": 1.0949333333333333,
      "grad_norm": 0.37344664335250854,
      "learning_rate": 2.2626666666666668e-05,
      "loss": 0.0042,
      "step": 20530
    },
    {
      "epoch": 1.0954666666666666,
      "grad_norm": 0.9941703081130981,
      "learning_rate": 2.2613333333333333e-05,
      "loss": 0.0021,
      "step": 20540
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.08855482190847397,
      "learning_rate": 2.26e-05,
      "loss": 0.0022,
      "step": 20550
    },
    {
      "epoch": 1.0965333333333334,
      "grad_norm": 0.36387526988983154,
      "learning_rate": 2.258666666666667e-05,
      "loss": 0.0028,
      "step": 20560
    },
    {
      "epoch": 1.0970666666666666,
      "grad_norm": 0.13692685961723328,
      "learning_rate": 2.2573333333333336e-05,
      "loss": 0.0024,
      "step": 20570
    },
    {
      "epoch": 1.0976,
      "grad_norm": 0.2245437502861023,
      "learning_rate": 2.256e-05,
      "loss": 0.0025,
      "step": 20580
    },
    {
      "epoch": 1.0981333333333334,
      "grad_norm": 0.3620651066303253,
      "learning_rate": 2.254666666666667e-05,
      "loss": 0.0042,
      "step": 20590
    },
    {
      "epoch": 1.0986666666666667,
      "grad_norm": 0.08363473415374756,
      "learning_rate": 2.2533333333333333e-05,
      "loss": 0.0019,
      "step": 20600
    },
    {
      "epoch": 1.0992,
      "grad_norm": 0.35274824500083923,
      "learning_rate": 2.252e-05,
      "loss": 0.0023,
      "step": 20610
    },
    {
      "epoch": 1.0997333333333332,
      "grad_norm": 0.5009206533432007,
      "learning_rate": 2.250666666666667e-05,
      "loss": 0.0018,
      "step": 20620
    },
    {
      "epoch": 1.1002666666666667,
      "grad_norm": 0.7675592303276062,
      "learning_rate": 2.2493333333333337e-05,
      "loss": 0.0021,
      "step": 20630
    },
    {
      "epoch": 1.1008,
      "grad_norm": 0.31227076053619385,
      "learning_rate": 2.248e-05,
      "loss": 0.0029,
      "step": 20640
    },
    {
      "epoch": 1.1013333333333333,
      "grad_norm": 0.28410348296165466,
      "learning_rate": 2.2466666666666666e-05,
      "loss": 0.0023,
      "step": 20650
    },
    {
      "epoch": 1.1018666666666665,
      "grad_norm": 0.15870362520217896,
      "learning_rate": 2.2453333333333334e-05,
      "loss": 0.0019,
      "step": 20660
    },
    {
      "epoch": 1.1024,
      "grad_norm": 0.7265769839286804,
      "learning_rate": 2.244e-05,
      "loss": 0.0029,
      "step": 20670
    },
    {
      "epoch": 1.1029333333333333,
      "grad_norm": 0.5709123611450195,
      "learning_rate": 2.2426666666666667e-05,
      "loss": 0.0032,
      "step": 20680
    },
    {
      "epoch": 1.1034666666666666,
      "grad_norm": 0.3169867992401123,
      "learning_rate": 2.2413333333333334e-05,
      "loss": 0.0024,
      "step": 20690
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.25661206245422363,
      "learning_rate": 2.2400000000000002e-05,
      "loss": 0.0027,
      "step": 20700
    },
    {
      "epoch": 1.1045333333333334,
      "grad_norm": 0.1449252963066101,
      "learning_rate": 2.2386666666666667e-05,
      "loss": 0.0021,
      "step": 20710
    },
    {
      "epoch": 1.1050666666666666,
      "grad_norm": 0.09831250458955765,
      "learning_rate": 2.2373333333333335e-05,
      "loss": 0.0021,
      "step": 20720
    },
    {
      "epoch": 1.1056,
      "grad_norm": 0.14157818257808685,
      "learning_rate": 2.236e-05,
      "loss": 0.0037,
      "step": 20730
    },
    {
      "epoch": 1.1061333333333334,
      "grad_norm": 0.09526275098323822,
      "learning_rate": 2.2346666666666667e-05,
      "loss": 0.002,
      "step": 20740
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 0.36737141013145447,
      "learning_rate": 2.2333333333333335e-05,
      "loss": 0.0029,
      "step": 20750
    },
    {
      "epoch": 1.1072,
      "grad_norm": 0.40658652782440186,
      "learning_rate": 2.2320000000000003e-05,
      "loss": 0.0021,
      "step": 20760
    },
    {
      "epoch": 1.1077333333333332,
      "grad_norm": 0.33515942096710205,
      "learning_rate": 2.2306666666666668e-05,
      "loss": 0.0026,
      "step": 20770
    },
    {
      "epoch": 1.1082666666666667,
      "grad_norm": 0.24666205048561096,
      "learning_rate": 2.2293333333333336e-05,
      "loss": 0.0019,
      "step": 20780
    },
    {
      "epoch": 1.1088,
      "grad_norm": 0.12871164083480835,
      "learning_rate": 2.228e-05,
      "loss": 0.0026,
      "step": 20790
    },
    {
      "epoch": 1.1093333333333333,
      "grad_norm": 0.18938855826854706,
      "learning_rate": 2.2266666666666668e-05,
      "loss": 0.0024,
      "step": 20800
    },
    {
      "epoch": 1.1098666666666666,
      "grad_norm": 0.13012170791625977,
      "learning_rate": 2.2253333333333336e-05,
      "loss": 0.0021,
      "step": 20810
    },
    {
      "epoch": 1.1104,
      "grad_norm": 0.46036386489868164,
      "learning_rate": 2.224e-05,
      "loss": 0.0021,
      "step": 20820
    },
    {
      "epoch": 1.1109333333333333,
      "grad_norm": 0.1288830190896988,
      "learning_rate": 2.222666666666667e-05,
      "loss": 0.0027,
      "step": 20830
    },
    {
      "epoch": 1.1114666666666666,
      "grad_norm": 0.3945753276348114,
      "learning_rate": 2.2213333333333333e-05,
      "loss": 0.0017,
      "step": 20840
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.5232192277908325,
      "learning_rate": 2.22e-05,
      "loss": 0.0019,
      "step": 20850
    },
    {
      "epoch": 1.1125333333333334,
      "grad_norm": 0.4307473301887512,
      "learning_rate": 2.2186666666666665e-05,
      "loss": 0.003,
      "step": 20860
    },
    {
      "epoch": 1.1130666666666666,
      "grad_norm": 0.12681879103183746,
      "learning_rate": 2.2173333333333333e-05,
      "loss": 0.0023,
      "step": 20870
    },
    {
      "epoch": 1.1136,
      "grad_norm": 0.2675158679485321,
      "learning_rate": 2.216e-05,
      "loss": 0.002,
      "step": 20880
    },
    {
      "epoch": 1.1141333333333334,
      "grad_norm": 0.09151292592287064,
      "learning_rate": 2.214666666666667e-05,
      "loss": 0.0026,
      "step": 20890
    },
    {
      "epoch": 1.1146666666666667,
      "grad_norm": 0.6162362098693848,
      "learning_rate": 2.2133333333333334e-05,
      "loss": 0.002,
      "step": 20900
    },
    {
      "epoch": 1.1152,
      "grad_norm": 0.11379243433475494,
      "learning_rate": 2.212e-05,
      "loss": 0.0024,
      "step": 20910
    },
    {
      "epoch": 1.1157333333333332,
      "grad_norm": 0.42080190777778625,
      "learning_rate": 2.2106666666666666e-05,
      "loss": 0.0028,
      "step": 20920
    },
    {
      "epoch": 1.1162666666666667,
      "grad_norm": 0.07984548062086105,
      "learning_rate": 2.2093333333333334e-05,
      "loss": 0.003,
      "step": 20930
    },
    {
      "epoch": 1.1168,
      "grad_norm": 0.05530925840139389,
      "learning_rate": 2.2080000000000002e-05,
      "loss": 0.0024,
      "step": 20940
    },
    {
      "epoch": 1.1173333333333333,
      "grad_norm": 0.09405539184808731,
      "learning_rate": 2.206666666666667e-05,
      "loss": 0.0025,
      "step": 20950
    },
    {
      "epoch": 1.1178666666666666,
      "grad_norm": 0.29058778285980225,
      "learning_rate": 2.2053333333333335e-05,
      "loss": 0.0026,
      "step": 20960
    },
    {
      "epoch": 1.1184,
      "grad_norm": 0.4947455823421478,
      "learning_rate": 2.2040000000000002e-05,
      "loss": 0.003,
      "step": 20970
    },
    {
      "epoch": 1.1189333333333333,
      "grad_norm": 0.06461625546216965,
      "learning_rate": 2.2026666666666667e-05,
      "loss": 0.0029,
      "step": 20980
    },
    {
      "epoch": 1.1194666666666666,
      "grad_norm": 0.3822447657585144,
      "learning_rate": 2.201333333333333e-05,
      "loss": 0.0032,
      "step": 20990
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.6252740025520325,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.003,
      "step": 21000
    },
    {
      "epoch": 1.1205333333333334,
      "grad_norm": 0.5659090876579285,
      "learning_rate": 2.1986666666666667e-05,
      "loss": 0.0028,
      "step": 21010
    },
    {
      "epoch": 1.1210666666666667,
      "grad_norm": 0.31251683831214905,
      "learning_rate": 2.1973333333333335e-05,
      "loss": 0.0038,
      "step": 21020
    },
    {
      "epoch": 1.1216,
      "grad_norm": 0.21237266063690186,
      "learning_rate": 2.196e-05,
      "loss": 0.0043,
      "step": 21030
    },
    {
      "epoch": 1.1221333333333334,
      "grad_norm": 0.25379008054733276,
      "learning_rate": 2.1946666666666668e-05,
      "loss": 0.0021,
      "step": 21040
    },
    {
      "epoch": 1.1226666666666667,
      "grad_norm": 0.21367862820625305,
      "learning_rate": 2.1933333333333332e-05,
      "loss": 0.0031,
      "step": 21050
    },
    {
      "epoch": 1.1232,
      "grad_norm": 0.21799534559249878,
      "learning_rate": 2.192e-05,
      "loss": 0.0039,
      "step": 21060
    },
    {
      "epoch": 1.1237333333333333,
      "grad_norm": 0.4756263196468353,
      "learning_rate": 2.1906666666666668e-05,
      "loss": 0.0025,
      "step": 21070
    },
    {
      "epoch": 1.1242666666666667,
      "grad_norm": 0.3412059545516968,
      "learning_rate": 2.1893333333333336e-05,
      "loss": 0.0023,
      "step": 21080
    },
    {
      "epoch": 1.1248,
      "grad_norm": 0.24595005810260773,
      "learning_rate": 2.188e-05,
      "loss": 0.0037,
      "step": 21090
    },
    {
      "epoch": 1.1253333333333333,
      "grad_norm": 0.14743772149085999,
      "learning_rate": 2.186666666666667e-05,
      "loss": 0.0031,
      "step": 21100
    },
    {
      "epoch": 1.1258666666666666,
      "grad_norm": 0.22576095163822174,
      "learning_rate": 2.1853333333333333e-05,
      "loss": 0.0021,
      "step": 21110
    },
    {
      "epoch": 1.1264,
      "grad_norm": 0.3266550600528717,
      "learning_rate": 2.184e-05,
      "loss": 0.0025,
      "step": 21120
    },
    {
      "epoch": 1.1269333333333333,
      "grad_norm": 0.5790925025939941,
      "learning_rate": 2.182666666666667e-05,
      "loss": 0.0022,
      "step": 21130
    },
    {
      "epoch": 1.1274666666666666,
      "grad_norm": 0.24628892540931702,
      "learning_rate": 2.1813333333333337e-05,
      "loss": 0.0033,
      "step": 21140
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.1697748601436615,
      "learning_rate": 2.18e-05,
      "loss": 0.0023,
      "step": 21150
    },
    {
      "epoch": 1.1285333333333334,
      "grad_norm": 0.3564634621143341,
      "learning_rate": 2.1786666666666666e-05,
      "loss": 0.0027,
      "step": 21160
    },
    {
      "epoch": 1.1290666666666667,
      "grad_norm": 0.3191743493080139,
      "learning_rate": 2.1773333333333334e-05,
      "loss": 0.0028,
      "step": 21170
    },
    {
      "epoch": 1.1296,
      "grad_norm": 0.19157783687114716,
      "learning_rate": 2.176e-05,
      "loss": 0.0019,
      "step": 21180
    },
    {
      "epoch": 1.1301333333333332,
      "grad_norm": 0.592086672782898,
      "learning_rate": 2.174666666666667e-05,
      "loss": 0.002,
      "step": 21190
    },
    {
      "epoch": 1.1306666666666667,
      "grad_norm": 0.308465838432312,
      "learning_rate": 2.1733333333333334e-05,
      "loss": 0.0024,
      "step": 21200
    },
    {
      "epoch": 1.1312,
      "grad_norm": 0.09986228495836258,
      "learning_rate": 2.1720000000000002e-05,
      "loss": 0.0022,
      "step": 21210
    },
    {
      "epoch": 1.1317333333333333,
      "grad_norm": 0.3382315933704376,
      "learning_rate": 2.1706666666666667e-05,
      "loss": 0.003,
      "step": 21220
    },
    {
      "epoch": 1.1322666666666668,
      "grad_norm": 0.48616793751716614,
      "learning_rate": 2.1693333333333335e-05,
      "loss": 0.0029,
      "step": 21230
    },
    {
      "epoch": 1.1328,
      "grad_norm": 0.15846003592014313,
      "learning_rate": 2.168e-05,
      "loss": 0.0028,
      "step": 21240
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.598196268081665,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 0.0024,
      "step": 21250
    },
    {
      "epoch": 1.1338666666666666,
      "grad_norm": 0.3935965895652771,
      "learning_rate": 2.1653333333333335e-05,
      "loss": 0.0017,
      "step": 21260
    },
    {
      "epoch": 1.1344,
      "grad_norm": 0.17815496027469635,
      "learning_rate": 2.1640000000000003e-05,
      "loss": 0.0022,
      "step": 21270
    },
    {
      "epoch": 1.1349333333333333,
      "grad_norm": 0.33050674200057983,
      "learning_rate": 2.1626666666666667e-05,
      "loss": 0.0021,
      "step": 21280
    },
    {
      "epoch": 1.1354666666666666,
      "grad_norm": 0.40576767921447754,
      "learning_rate": 2.1613333333333335e-05,
      "loss": 0.0027,
      "step": 21290
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 0.12426339834928513,
      "learning_rate": 2.16e-05,
      "loss": 0.0022,
      "step": 21300
    },
    {
      "epoch": 1.1365333333333334,
      "grad_norm": 0.1372765600681305,
      "learning_rate": 2.1586666666666668e-05,
      "loss": 0.0025,
      "step": 21310
    },
    {
      "epoch": 1.1370666666666667,
      "grad_norm": 0.5750936269760132,
      "learning_rate": 2.1573333333333336e-05,
      "loss": 0.0028,
      "step": 21320
    },
    {
      "epoch": 1.1376,
      "grad_norm": 0.1865643709897995,
      "learning_rate": 2.1560000000000004e-05,
      "loss": 0.0034,
      "step": 21330
    },
    {
      "epoch": 1.1381333333333332,
      "grad_norm": 0.48936790227890015,
      "learning_rate": 2.1546666666666668e-05,
      "loss": 0.0034,
      "step": 21340
    },
    {
      "epoch": 1.1386666666666667,
      "grad_norm": 0.41098955273628235,
      "learning_rate": 2.1533333333333333e-05,
      "loss": 0.003,
      "step": 21350
    },
    {
      "epoch": 1.1392,
      "grad_norm": 0.18102474510669708,
      "learning_rate": 2.152e-05,
      "loss": 0.0037,
      "step": 21360
    },
    {
      "epoch": 1.1397333333333333,
      "grad_norm": 0.15852847695350647,
      "learning_rate": 2.1506666666666665e-05,
      "loss": 0.0029,
      "step": 21370
    },
    {
      "epoch": 1.1402666666666668,
      "grad_norm": 0.28608328104019165,
      "learning_rate": 2.1493333333333333e-05,
      "loss": 0.0027,
      "step": 21380
    },
    {
      "epoch": 1.1408,
      "grad_norm": 0.10717877000570297,
      "learning_rate": 2.148e-05,
      "loss": 0.0018,
      "step": 21390
    },
    {
      "epoch": 1.1413333333333333,
      "grad_norm": 0.13112714886665344,
      "learning_rate": 2.146666666666667e-05,
      "loss": 0.002,
      "step": 21400
    },
    {
      "epoch": 1.1418666666666666,
      "grad_norm": 0.13949179649353027,
      "learning_rate": 2.1453333333333333e-05,
      "loss": 0.0017,
      "step": 21410
    },
    {
      "epoch": 1.1424,
      "grad_norm": 0.7995368242263794,
      "learning_rate": 2.144e-05,
      "loss": 0.0022,
      "step": 21420
    },
    {
      "epoch": 1.1429333333333334,
      "grad_norm": 0.7644590735435486,
      "learning_rate": 2.1426666666666666e-05,
      "loss": 0.0033,
      "step": 21430
    },
    {
      "epoch": 1.1434666666666666,
      "grad_norm": 0.7317183613777161,
      "learning_rate": 2.1413333333333334e-05,
      "loss": 0.0038,
      "step": 21440
    },
    {
      "epoch": 1.144,
      "grad_norm": 0.3632063567638397,
      "learning_rate": 2.1400000000000002e-05,
      "loss": 0.0028,
      "step": 21450
    },
    {
      "epoch": 1.1445333333333334,
      "grad_norm": 0.24451857805252075,
      "learning_rate": 2.138666666666667e-05,
      "loss": 0.0027,
      "step": 21460
    },
    {
      "epoch": 1.1450666666666667,
      "grad_norm": 0.3516870141029358,
      "learning_rate": 2.1373333333333334e-05,
      "loss": 0.002,
      "step": 21470
    },
    {
      "epoch": 1.1456,
      "grad_norm": 0.30926981568336487,
      "learning_rate": 2.1360000000000002e-05,
      "loss": 0.0025,
      "step": 21480
    },
    {
      "epoch": 1.1461333333333332,
      "grad_norm": 0.3144340217113495,
      "learning_rate": 2.1346666666666667e-05,
      "loss": 0.002,
      "step": 21490
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 0.21409562230110168,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 0.0022,
      "step": 21500
    },
    {
      "epoch": 1.1472,
      "grad_norm": 0.35450348258018494,
      "learning_rate": 2.1320000000000003e-05,
      "loss": 0.0027,
      "step": 21510
    },
    {
      "epoch": 1.1477333333333333,
      "grad_norm": 0.12206409126520157,
      "learning_rate": 2.1306666666666667e-05,
      "loss": 0.0026,
      "step": 21520
    },
    {
      "epoch": 1.1482666666666668,
      "grad_norm": 0.2075732946395874,
      "learning_rate": 2.1293333333333335e-05,
      "loss": 0.002,
      "step": 21530
    },
    {
      "epoch": 1.1488,
      "grad_norm": 0.10359915345907211,
      "learning_rate": 2.128e-05,
      "loss": 0.0026,
      "step": 21540
    },
    {
      "epoch": 1.1493333333333333,
      "grad_norm": 0.4489179253578186,
      "learning_rate": 2.1266666666666667e-05,
      "loss": 0.0022,
      "step": 21550
    },
    {
      "epoch": 1.1498666666666666,
      "grad_norm": 0.08472251147031784,
      "learning_rate": 2.1253333333333332e-05,
      "loss": 0.002,
      "step": 21560
    },
    {
      "epoch": 1.1504,
      "grad_norm": 0.18428297340869904,
      "learning_rate": 2.124e-05,
      "loss": 0.0027,
      "step": 21570
    },
    {
      "epoch": 1.1509333333333334,
      "grad_norm": 0.46436020731925964,
      "learning_rate": 2.1226666666666668e-05,
      "loss": 0.0037,
      "step": 21580
    },
    {
      "epoch": 1.1514666666666666,
      "grad_norm": 0.16643431782722473,
      "learning_rate": 2.1213333333333336e-05,
      "loss": 0.0022,
      "step": 21590
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.47762495279312134,
      "learning_rate": 2.12e-05,
      "loss": 0.002,
      "step": 21600
    },
    {
      "epoch": 1.1525333333333334,
      "grad_norm": 0.5488871932029724,
      "learning_rate": 2.1186666666666668e-05,
      "loss": 0.0021,
      "step": 21610
    },
    {
      "epoch": 1.1530666666666667,
      "grad_norm": 0.14015142619609833,
      "learning_rate": 2.1173333333333333e-05,
      "loss": 0.0033,
      "step": 21620
    },
    {
      "epoch": 1.1536,
      "grad_norm": 0.8955427408218384,
      "learning_rate": 2.116e-05,
      "loss": 0.0035,
      "step": 21630
    },
    {
      "epoch": 1.1541333333333332,
      "grad_norm": 0.08144449442625046,
      "learning_rate": 2.114666666666667e-05,
      "loss": 0.0018,
      "step": 21640
    },
    {
      "epoch": 1.1546666666666667,
      "grad_norm": 0.3386244773864746,
      "learning_rate": 2.1133333333333337e-05,
      "loss": 0.0019,
      "step": 21650
    },
    {
      "epoch": 1.1552,
      "grad_norm": 0.27277421951293945,
      "learning_rate": 2.112e-05,
      "loss": 0.0024,
      "step": 21660
    },
    {
      "epoch": 1.1557333333333333,
      "grad_norm": 0.33061063289642334,
      "learning_rate": 2.110666666666667e-05,
      "loss": 0.0019,
      "step": 21670
    },
    {
      "epoch": 1.1562666666666668,
      "grad_norm": 0.27150604128837585,
      "learning_rate": 2.1093333333333334e-05,
      "loss": 0.0016,
      "step": 21680
    },
    {
      "epoch": 1.1568,
      "grad_norm": 0.21748720109462738,
      "learning_rate": 2.1079999999999998e-05,
      "loss": 0.0026,
      "step": 21690
    },
    {
      "epoch": 1.1573333333333333,
      "grad_norm": 0.15953843295574188,
      "learning_rate": 2.106666666666667e-05,
      "loss": 0.0026,
      "step": 21700
    },
    {
      "epoch": 1.1578666666666666,
      "grad_norm": 0.29813358187675476,
      "learning_rate": 2.1053333333333334e-05,
      "loss": 0.0045,
      "step": 21710
    },
    {
      "epoch": 1.1584,
      "grad_norm": 0.12585467100143433,
      "learning_rate": 2.1040000000000002e-05,
      "loss": 0.0023,
      "step": 21720
    },
    {
      "epoch": 1.1589333333333334,
      "grad_norm": 0.20746932923793793,
      "learning_rate": 2.1026666666666666e-05,
      "loss": 0.0029,
      "step": 21730
    },
    {
      "epoch": 1.1594666666666666,
      "grad_norm": 0.5210974812507629,
      "learning_rate": 2.1013333333333334e-05,
      "loss": 0.002,
      "step": 21740
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.18501806259155273,
      "learning_rate": 2.1e-05,
      "loss": 0.0024,
      "step": 21750
    },
    {
      "epoch": 1.1605333333333334,
      "grad_norm": 0.25938284397125244,
      "learning_rate": 2.0986666666666667e-05,
      "loss": 0.002,
      "step": 21760
    },
    {
      "epoch": 1.1610666666666667,
      "grad_norm": 0.11512605845928192,
      "learning_rate": 2.0973333333333335e-05,
      "loss": 0.0026,
      "step": 21770
    },
    {
      "epoch": 1.1616,
      "grad_norm": 0.435576468706131,
      "learning_rate": 2.0960000000000003e-05,
      "loss": 0.0026,
      "step": 21780
    },
    {
      "epoch": 1.1621333333333332,
      "grad_norm": 0.2331327199935913,
      "learning_rate": 2.0946666666666667e-05,
      "loss": 0.0022,
      "step": 21790
    },
    {
      "epoch": 1.1626666666666667,
      "grad_norm": 0.2450316995382309,
      "learning_rate": 2.0933333333333335e-05,
      "loss": 0.0018,
      "step": 21800
    },
    {
      "epoch": 1.1632,
      "grad_norm": 0.5349915623664856,
      "learning_rate": 2.092e-05,
      "loss": 0.0028,
      "step": 21810
    },
    {
      "epoch": 1.1637333333333333,
      "grad_norm": 0.1408281922340393,
      "learning_rate": 2.0906666666666668e-05,
      "loss": 0.0025,
      "step": 21820
    },
    {
      "epoch": 1.1642666666666668,
      "grad_norm": 0.5110712051391602,
      "learning_rate": 2.0893333333333335e-05,
      "loss": 0.0024,
      "step": 21830
    },
    {
      "epoch": 1.1648,
      "grad_norm": 0.11277075856924057,
      "learning_rate": 2.0880000000000003e-05,
      "loss": 0.0027,
      "step": 21840
    },
    {
      "epoch": 1.1653333333333333,
      "grad_norm": 0.14834876358509064,
      "learning_rate": 2.0866666666666668e-05,
      "loss": 0.0028,
      "step": 21850
    },
    {
      "epoch": 1.1658666666666666,
      "grad_norm": 0.22594960033893585,
      "learning_rate": 2.0853333333333332e-05,
      "loss": 0.002,
      "step": 21860
    },
    {
      "epoch": 1.1663999999999999,
      "grad_norm": 0.3316183090209961,
      "learning_rate": 2.084e-05,
      "loss": 0.0025,
      "step": 21870
    },
    {
      "epoch": 1.1669333333333334,
      "grad_norm": 0.5835808515548706,
      "learning_rate": 2.0826666666666665e-05,
      "loss": 0.0031,
      "step": 21880
    },
    {
      "epoch": 1.1674666666666667,
      "grad_norm": 0.34670647978782654,
      "learning_rate": 2.0813333333333336e-05,
      "loss": 0.0022,
      "step": 21890
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.30990996956825256,
      "learning_rate": 2.08e-05,
      "loss": 0.0035,
      "step": 21900
    },
    {
      "epoch": 1.1685333333333334,
      "grad_norm": 0.18655312061309814,
      "learning_rate": 2.078666666666667e-05,
      "loss": 0.0034,
      "step": 21910
    },
    {
      "epoch": 1.1690666666666667,
      "grad_norm": 0.31157058477401733,
      "learning_rate": 2.0773333333333333e-05,
      "loss": 0.0037,
      "step": 21920
    },
    {
      "epoch": 1.1696,
      "grad_norm": 0.4765470325946808,
      "learning_rate": 2.076e-05,
      "loss": 0.0022,
      "step": 21930
    },
    {
      "epoch": 1.1701333333333332,
      "grad_norm": 0.5724990367889404,
      "learning_rate": 2.0746666666666666e-05,
      "loss": 0.0022,
      "step": 21940
    },
    {
      "epoch": 1.1706666666666667,
      "grad_norm": 0.13641756772994995,
      "learning_rate": 2.0733333333333334e-05,
      "loss": 0.0033,
      "step": 21950
    },
    {
      "epoch": 1.1712,
      "grad_norm": 0.3254522681236267,
      "learning_rate": 2.072e-05,
      "loss": 0.002,
      "step": 21960
    },
    {
      "epoch": 1.1717333333333333,
      "grad_norm": 0.4731062948703766,
      "learning_rate": 2.070666666666667e-05,
      "loss": 0.0018,
      "step": 21970
    },
    {
      "epoch": 1.1722666666666668,
      "grad_norm": 0.2746517062187195,
      "learning_rate": 2.0693333333333334e-05,
      "loss": 0.0032,
      "step": 21980
    },
    {
      "epoch": 1.1728,
      "grad_norm": 0.12427932769060135,
      "learning_rate": 2.0680000000000002e-05,
      "loss": 0.0019,
      "step": 21990
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 0.23876923322677612,
      "learning_rate": 2.0666666666666666e-05,
      "loss": 0.002,
      "step": 22000
    },
    {
      "epoch": 1.1738666666666666,
      "grad_norm": 0.14888787269592285,
      "learning_rate": 2.0653333333333334e-05,
      "loss": 0.002,
      "step": 22010
    },
    {
      "epoch": 1.1743999999999999,
      "grad_norm": 0.4023638069629669,
      "learning_rate": 2.0640000000000002e-05,
      "loss": 0.0031,
      "step": 22020
    },
    {
      "epoch": 1.1749333333333334,
      "grad_norm": 0.424402117729187,
      "learning_rate": 2.0626666666666667e-05,
      "loss": 0.0026,
      "step": 22030
    },
    {
      "epoch": 1.1754666666666667,
      "grad_norm": 0.16770626604557037,
      "learning_rate": 2.0613333333333335e-05,
      "loss": 0.0041,
      "step": 22040
    },
    {
      "epoch": 1.176,
      "grad_norm": 0.49827539920806885,
      "learning_rate": 2.06e-05,
      "loss": 0.0031,
      "step": 22050
    },
    {
      "epoch": 1.1765333333333334,
      "grad_norm": 0.38696298003196716,
      "learning_rate": 2.0586666666666667e-05,
      "loss": 0.0035,
      "step": 22060
    },
    {
      "epoch": 1.1770666666666667,
      "grad_norm": 0.05315762758255005,
      "learning_rate": 2.0573333333333332e-05,
      "loss": 0.0027,
      "step": 22070
    },
    {
      "epoch": 1.1776,
      "grad_norm": 0.2511129081249237,
      "learning_rate": 2.0560000000000003e-05,
      "loss": 0.002,
      "step": 22080
    },
    {
      "epoch": 1.1781333333333333,
      "grad_norm": 0.4309869110584259,
      "learning_rate": 2.0546666666666668e-05,
      "loss": 0.0023,
      "step": 22090
    },
    {
      "epoch": 1.1786666666666668,
      "grad_norm": 0.40625378489494324,
      "learning_rate": 2.0533333333333336e-05,
      "loss": 0.0022,
      "step": 22100
    },
    {
      "epoch": 1.1792,
      "grad_norm": 0.09444224834442139,
      "learning_rate": 2.052e-05,
      "loss": 0.0024,
      "step": 22110
    },
    {
      "epoch": 1.1797333333333333,
      "grad_norm": 0.4437507390975952,
      "learning_rate": 2.0506666666666668e-05,
      "loss": 0.0027,
      "step": 22120
    },
    {
      "epoch": 1.1802666666666666,
      "grad_norm": 0.5345660448074341,
      "learning_rate": 2.0493333333333333e-05,
      "loss": 0.0032,
      "step": 22130
    },
    {
      "epoch": 1.1808,
      "grad_norm": 0.20127545297145844,
      "learning_rate": 2.048e-05,
      "loss": 0.0027,
      "step": 22140
    },
    {
      "epoch": 1.1813333333333333,
      "grad_norm": 0.13773232698440552,
      "learning_rate": 2.046666666666667e-05,
      "loss": 0.0031,
      "step": 22150
    },
    {
      "epoch": 1.1818666666666666,
      "grad_norm": 0.5839223265647888,
      "learning_rate": 2.0453333333333336e-05,
      "loss": 0.0029,
      "step": 22160
    },
    {
      "epoch": 1.1824,
      "grad_norm": 0.4447649419307709,
      "learning_rate": 2.044e-05,
      "loss": 0.002,
      "step": 22170
    },
    {
      "epoch": 1.1829333333333334,
      "grad_norm": 0.33491960167884827,
      "learning_rate": 2.042666666666667e-05,
      "loss": 0.0018,
      "step": 22180
    },
    {
      "epoch": 1.1834666666666667,
      "grad_norm": 0.345416396856308,
      "learning_rate": 2.0413333333333333e-05,
      "loss": 0.0019,
      "step": 22190
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.408581405878067,
      "learning_rate": 2.04e-05,
      "loss": 0.0028,
      "step": 22200
    },
    {
      "epoch": 1.1845333333333334,
      "grad_norm": 0.20327676832675934,
      "learning_rate": 2.038666666666667e-05,
      "loss": 0.0026,
      "step": 22210
    },
    {
      "epoch": 1.1850666666666667,
      "grad_norm": 0.2796953320503235,
      "learning_rate": 2.0373333333333334e-05,
      "loss": 0.0022,
      "step": 22220
    },
    {
      "epoch": 1.1856,
      "grad_norm": 0.11853133141994476,
      "learning_rate": 2.036e-05,
      "loss": 0.0026,
      "step": 22230
    },
    {
      "epoch": 1.1861333333333333,
      "grad_norm": 0.8922498226165771,
      "learning_rate": 2.0346666666666666e-05,
      "loss": 0.0026,
      "step": 22240
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 0.3443891704082489,
      "learning_rate": 2.0333333333333334e-05,
      "loss": 0.0019,
      "step": 22250
    },
    {
      "epoch": 1.1872,
      "grad_norm": 0.17435403168201447,
      "learning_rate": 2.032e-05,
      "loss": 0.0031,
      "step": 22260
    },
    {
      "epoch": 1.1877333333333333,
      "grad_norm": 0.554809033870697,
      "learning_rate": 2.030666666666667e-05,
      "loss": 0.0029,
      "step": 22270
    },
    {
      "epoch": 1.1882666666666666,
      "grad_norm": 0.10988172143697739,
      "learning_rate": 2.0293333333333334e-05,
      "loss": 0.0024,
      "step": 22280
    },
    {
      "epoch": 1.1888,
      "grad_norm": 0.29948949813842773,
      "learning_rate": 2.0280000000000002e-05,
      "loss": 0.0029,
      "step": 22290
    },
    {
      "epoch": 1.1893333333333334,
      "grad_norm": 0.1160835325717926,
      "learning_rate": 2.0266666666666667e-05,
      "loss": 0.002,
      "step": 22300
    },
    {
      "epoch": 1.1898666666666666,
      "grad_norm": 0.1060357391834259,
      "learning_rate": 2.0253333333333335e-05,
      "loss": 0.0018,
      "step": 22310
    },
    {
      "epoch": 1.1904,
      "grad_norm": 0.38331013917922974,
      "learning_rate": 2.024e-05,
      "loss": 0.0025,
      "step": 22320
    },
    {
      "epoch": 1.1909333333333334,
      "grad_norm": 0.16114988923072815,
      "learning_rate": 2.0226666666666667e-05,
      "loss": 0.0021,
      "step": 22330
    },
    {
      "epoch": 1.1914666666666667,
      "grad_norm": 0.33206498622894287,
      "learning_rate": 2.0213333333333335e-05,
      "loss": 0.0031,
      "step": 22340
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.43359383940696716,
      "learning_rate": 2.0200000000000003e-05,
      "loss": 0.0019,
      "step": 22350
    },
    {
      "epoch": 1.1925333333333334,
      "grad_norm": 0.41109126806259155,
      "learning_rate": 2.0186666666666668e-05,
      "loss": 0.0023,
      "step": 22360
    },
    {
      "epoch": 1.1930666666666667,
      "grad_norm": 0.3829886019229889,
      "learning_rate": 2.0173333333333332e-05,
      "loss": 0.0029,
      "step": 22370
    },
    {
      "epoch": 1.1936,
      "grad_norm": 0.15309038758277893,
      "learning_rate": 2.016e-05,
      "loss": 0.0021,
      "step": 22380
    },
    {
      "epoch": 1.1941333333333333,
      "grad_norm": 0.13231460750102997,
      "learning_rate": 2.0146666666666668e-05,
      "loss": 0.0035,
      "step": 22390
    },
    {
      "epoch": 1.1946666666666665,
      "grad_norm": 0.2330116629600525,
      "learning_rate": 2.0133333333333336e-05,
      "loss": 0.0028,
      "step": 22400
    },
    {
      "epoch": 1.1952,
      "grad_norm": 0.25009685754776,
      "learning_rate": 2.012e-05,
      "loss": 0.0034,
      "step": 22410
    },
    {
      "epoch": 1.1957333333333333,
      "grad_norm": 0.657315731048584,
      "learning_rate": 2.010666666666667e-05,
      "loss": 0.0032,
      "step": 22420
    },
    {
      "epoch": 1.1962666666666666,
      "grad_norm": 0.1794092208147049,
      "learning_rate": 2.0093333333333333e-05,
      "loss": 0.0035,
      "step": 22430
    },
    {
      "epoch": 1.1968,
      "grad_norm": 0.10587120056152344,
      "learning_rate": 2.008e-05,
      "loss": 0.0022,
      "step": 22440
    },
    {
      "epoch": 1.1973333333333334,
      "grad_norm": 0.41211244463920593,
      "learning_rate": 2.0066666666666665e-05,
      "loss": 0.0022,
      "step": 22450
    },
    {
      "epoch": 1.1978666666666666,
      "grad_norm": 0.33313342928886414,
      "learning_rate": 2.0053333333333337e-05,
      "loss": 0.003,
      "step": 22460
    },
    {
      "epoch": 1.1984,
      "grad_norm": 0.7619598507881165,
      "learning_rate": 2.004e-05,
      "loss": 0.0022,
      "step": 22470
    },
    {
      "epoch": 1.1989333333333334,
      "grad_norm": 0.44899967312812805,
      "learning_rate": 2.002666666666667e-05,
      "loss": 0.0028,
      "step": 22480
    },
    {
      "epoch": 1.1994666666666667,
      "grad_norm": 0.22363145649433136,
      "learning_rate": 2.0013333333333334e-05,
      "loss": 0.0025,
      "step": 22490
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.3945148289203644,
      "learning_rate": 2e-05,
      "loss": 0.0017,
      "step": 22500
    },
    {
      "epoch": 1.2005333333333335,
      "grad_norm": 0.44416019320487976,
      "learning_rate": 1.9986666666666666e-05,
      "loss": 0.0019,
      "step": 22510
    },
    {
      "epoch": 1.2010666666666667,
      "grad_norm": 0.24816875159740448,
      "learning_rate": 1.9973333333333334e-05,
      "loss": 0.0029,
      "step": 22520
    },
    {
      "epoch": 1.2016,
      "grad_norm": 0.24402466416358948,
      "learning_rate": 1.9960000000000002e-05,
      "loss": 0.0031,
      "step": 22530
    },
    {
      "epoch": 1.2021333333333333,
      "grad_norm": 0.25936973094940186,
      "learning_rate": 1.9946666666666667e-05,
      "loss": 0.0031,
      "step": 22540
    },
    {
      "epoch": 1.2026666666666666,
      "grad_norm": 0.39012762904167175,
      "learning_rate": 1.9933333333333334e-05,
      "loss": 0.0033,
      "step": 22550
    },
    {
      "epoch": 1.2032,
      "grad_norm": 0.39804384112358093,
      "learning_rate": 1.992e-05,
      "loss": 0.0028,
      "step": 22560
    },
    {
      "epoch": 1.2037333333333333,
      "grad_norm": 0.30627599358558655,
      "learning_rate": 1.9906666666666667e-05,
      "loss": 0.0028,
      "step": 22570
    },
    {
      "epoch": 1.2042666666666666,
      "grad_norm": 0.10406012088060379,
      "learning_rate": 1.9893333333333335e-05,
      "loss": 0.0028,
      "step": 22580
    },
    {
      "epoch": 1.2048,
      "grad_norm": 0.14323265850543976,
      "learning_rate": 1.9880000000000003e-05,
      "loss": 0.002,
      "step": 22590
    },
    {
      "epoch": 1.2053333333333334,
      "grad_norm": 0.2170352041721344,
      "learning_rate": 1.9866666666666667e-05,
      "loss": 0.0019,
      "step": 22600
    },
    {
      "epoch": 1.2058666666666666,
      "grad_norm": 0.5083110332489014,
      "learning_rate": 1.9853333333333335e-05,
      "loss": 0.0021,
      "step": 22610
    },
    {
      "epoch": 1.2064,
      "grad_norm": 0.1438864767551422,
      "learning_rate": 1.984e-05,
      "loss": 0.0023,
      "step": 22620
    },
    {
      "epoch": 1.2069333333333334,
      "grad_norm": 0.3392138183116913,
      "learning_rate": 1.9826666666666668e-05,
      "loss": 0.0036,
      "step": 22630
    },
    {
      "epoch": 1.2074666666666667,
      "grad_norm": 0.24245621263980865,
      "learning_rate": 1.9813333333333332e-05,
      "loss": 0.0026,
      "step": 22640
    },
    {
      "epoch": 1.208,
      "grad_norm": 0.3537661135196686,
      "learning_rate": 1.9800000000000004e-05,
      "loss": 0.0031,
      "step": 22650
    },
    {
      "epoch": 1.2085333333333332,
      "grad_norm": 0.351386159658432,
      "learning_rate": 1.9786666666666668e-05,
      "loss": 0.0018,
      "step": 22660
    },
    {
      "epoch": 1.2090666666666667,
      "grad_norm": 0.16369429230690002,
      "learning_rate": 1.9773333333333336e-05,
      "loss": 0.0039,
      "step": 22670
    },
    {
      "epoch": 1.2096,
      "grad_norm": 0.4136638045310974,
      "learning_rate": 1.976e-05,
      "loss": 0.0027,
      "step": 22680
    },
    {
      "epoch": 1.2101333333333333,
      "grad_norm": 0.376577764749527,
      "learning_rate": 1.974666666666667e-05,
      "loss": 0.0021,
      "step": 22690
    },
    {
      "epoch": 1.2106666666666666,
      "grad_norm": 0.5337672233581543,
      "learning_rate": 1.9733333333333333e-05,
      "loss": 0.0028,
      "step": 22700
    },
    {
      "epoch": 1.2112,
      "grad_norm": 0.26162225008010864,
      "learning_rate": 1.972e-05,
      "loss": 0.0018,
      "step": 22710
    },
    {
      "epoch": 1.2117333333333333,
      "grad_norm": 0.5268184542655945,
      "learning_rate": 1.970666666666667e-05,
      "loss": 0.0029,
      "step": 22720
    },
    {
      "epoch": 1.2122666666666666,
      "grad_norm": 0.26143068075180054,
      "learning_rate": 1.9693333333333333e-05,
      "loss": 0.0046,
      "step": 22730
    },
    {
      "epoch": 1.2128,
      "grad_norm": 0.3259853720664978,
      "learning_rate": 1.968e-05,
      "loss": 0.0037,
      "step": 22740
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.14238229393959045,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 0.0019,
      "step": 22750
    },
    {
      "epoch": 1.2138666666666666,
      "grad_norm": 0.18453557789325714,
      "learning_rate": 1.9653333333333334e-05,
      "loss": 0.0023,
      "step": 22760
    },
    {
      "epoch": 1.2144,
      "grad_norm": 0.2875826060771942,
      "learning_rate": 1.9640000000000002e-05,
      "loss": 0.0039,
      "step": 22770
    },
    {
      "epoch": 1.2149333333333334,
      "grad_norm": 0.5189701318740845,
      "learning_rate": 1.962666666666667e-05,
      "loss": 0.0031,
      "step": 22780
    },
    {
      "epoch": 1.2154666666666667,
      "grad_norm": 0.13126222789287567,
      "learning_rate": 1.9613333333333334e-05,
      "loss": 0.0019,
      "step": 22790
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.18678860366344452,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 0.0023,
      "step": 22800
    },
    {
      "epoch": 1.2165333333333332,
      "grad_norm": 0.14571605622768402,
      "learning_rate": 1.9586666666666667e-05,
      "loss": 0.0023,
      "step": 22810
    },
    {
      "epoch": 1.2170666666666667,
      "grad_norm": 0.30557146668434143,
      "learning_rate": 1.9573333333333335e-05,
      "loss": 0.0025,
      "step": 22820
    },
    {
      "epoch": 1.2176,
      "grad_norm": 0.2223532497882843,
      "learning_rate": 1.956e-05,
      "loss": 0.0026,
      "step": 22830
    },
    {
      "epoch": 1.2181333333333333,
      "grad_norm": 0.36930012702941895,
      "learning_rate": 1.9546666666666667e-05,
      "loss": 0.002,
      "step": 22840
    },
    {
      "epoch": 1.2186666666666666,
      "grad_norm": 0.4137233793735504,
      "learning_rate": 1.9533333333333335e-05,
      "loss": 0.003,
      "step": 22850
    },
    {
      "epoch": 1.2192,
      "grad_norm": 0.16974258422851562,
      "learning_rate": 1.9520000000000003e-05,
      "loss": 0.002,
      "step": 22860
    },
    {
      "epoch": 1.2197333333333333,
      "grad_norm": 0.391877144575119,
      "learning_rate": 1.9506666666666667e-05,
      "loss": 0.0034,
      "step": 22870
    },
    {
      "epoch": 1.2202666666666666,
      "grad_norm": 0.3633309602737427,
      "learning_rate": 1.9493333333333332e-05,
      "loss": 0.0024,
      "step": 22880
    },
    {
      "epoch": 1.2208,
      "grad_norm": 0.15788070857524872,
      "learning_rate": 1.948e-05,
      "loss": 0.0041,
      "step": 22890
    },
    {
      "epoch": 1.2213333333333334,
      "grad_norm": 0.4077000916004181,
      "learning_rate": 1.9466666666666668e-05,
      "loss": 0.0028,
      "step": 22900
    },
    {
      "epoch": 1.2218666666666667,
      "grad_norm": 0.22044630348682404,
      "learning_rate": 1.9453333333333336e-05,
      "loss": 0.0046,
      "step": 22910
    },
    {
      "epoch": 1.2224,
      "grad_norm": 0.45232319831848145,
      "learning_rate": 1.944e-05,
      "loss": 0.0023,
      "step": 22920
    },
    {
      "epoch": 1.2229333333333334,
      "grad_norm": 0.12657174468040466,
      "learning_rate": 1.9426666666666668e-05,
      "loss": 0.0018,
      "step": 22930
    },
    {
      "epoch": 1.2234666666666667,
      "grad_norm": 0.4015997350215912,
      "learning_rate": 1.9413333333333333e-05,
      "loss": 0.0021,
      "step": 22940
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.15767642855644226,
      "learning_rate": 1.94e-05,
      "loss": 0.0024,
      "step": 22950
    },
    {
      "epoch": 1.2245333333333333,
      "grad_norm": 0.06742588430643082,
      "learning_rate": 1.938666666666667e-05,
      "loss": 0.0025,
      "step": 22960
    },
    {
      "epoch": 1.2250666666666667,
      "grad_norm": 0.15685634315013885,
      "learning_rate": 1.9373333333333336e-05,
      "loss": 0.002,
      "step": 22970
    },
    {
      "epoch": 1.2256,
      "grad_norm": 0.39471104741096497,
      "learning_rate": 1.936e-05,
      "loss": 0.003,
      "step": 22980
    },
    {
      "epoch": 1.2261333333333333,
      "grad_norm": 0.10357042402029037,
      "learning_rate": 1.934666666666667e-05,
      "loss": 0.0034,
      "step": 22990
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 0.1404862105846405,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 0.002,
      "step": 23000
    },
    {
      "epoch": 1.2272,
      "grad_norm": 0.6109011769294739,
      "learning_rate": 1.932e-05,
      "loss": 0.0018,
      "step": 23010
    },
    {
      "epoch": 1.2277333333333333,
      "grad_norm": 0.3105010390281677,
      "learning_rate": 1.9306666666666666e-05,
      "loss": 0.0029,
      "step": 23020
    },
    {
      "epoch": 1.2282666666666666,
      "grad_norm": 0.10891968011856079,
      "learning_rate": 1.9293333333333334e-05,
      "loss": 0.003,
      "step": 23030
    },
    {
      "epoch": 1.2288000000000001,
      "grad_norm": 0.5535606741905212,
      "learning_rate": 1.9280000000000002e-05,
      "loss": 0.0021,
      "step": 23040
    },
    {
      "epoch": 1.2293333333333334,
      "grad_norm": 0.4035896360874176,
      "learning_rate": 1.926666666666667e-05,
      "loss": 0.0022,
      "step": 23050
    },
    {
      "epoch": 1.2298666666666667,
      "grad_norm": 0.24144773185253143,
      "learning_rate": 1.9253333333333334e-05,
      "loss": 0.002,
      "step": 23060
    },
    {
      "epoch": 1.2304,
      "grad_norm": 0.3331015408039093,
      "learning_rate": 1.924e-05,
      "loss": 0.0022,
      "step": 23070
    },
    {
      "epoch": 1.2309333333333332,
      "grad_norm": 0.1668826788663864,
      "learning_rate": 1.9226666666666667e-05,
      "loss": 0.0019,
      "step": 23080
    },
    {
      "epoch": 1.2314666666666667,
      "grad_norm": 0.453692227602005,
      "learning_rate": 1.9213333333333335e-05,
      "loss": 0.0028,
      "step": 23090
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.2618761956691742,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.0018,
      "step": 23100
    },
    {
      "epoch": 1.2325333333333333,
      "grad_norm": 0.12098751962184906,
      "learning_rate": 1.9186666666666667e-05,
      "loss": 0.003,
      "step": 23110
    },
    {
      "epoch": 1.2330666666666668,
      "grad_norm": 0.38091111183166504,
      "learning_rate": 1.9173333333333335e-05,
      "loss": 0.0027,
      "step": 23120
    },
    {
      "epoch": 1.2336,
      "grad_norm": 0.29165056347846985,
      "learning_rate": 1.916e-05,
      "loss": 0.0019,
      "step": 23130
    },
    {
      "epoch": 1.2341333333333333,
      "grad_norm": 0.14411164820194244,
      "learning_rate": 1.9146666666666667e-05,
      "loss": 0.0023,
      "step": 23140
    },
    {
      "epoch": 1.2346666666666666,
      "grad_norm": 0.21138304471969604,
      "learning_rate": 1.9133333333333332e-05,
      "loss": 0.0022,
      "step": 23150
    },
    {
      "epoch": 1.2352,
      "grad_norm": 0.1319189965724945,
      "learning_rate": 1.9120000000000003e-05,
      "loss": 0.0023,
      "step": 23160
    },
    {
      "epoch": 1.2357333333333334,
      "grad_norm": 0.14737068116664886,
      "learning_rate": 1.9106666666666668e-05,
      "loss": 0.0025,
      "step": 23170
    },
    {
      "epoch": 1.2362666666666666,
      "grad_norm": 0.08522946387529373,
      "learning_rate": 1.9093333333333336e-05,
      "loss": 0.0017,
      "step": 23180
    },
    {
      "epoch": 1.2368000000000001,
      "grad_norm": 0.46074578166007996,
      "learning_rate": 1.908e-05,
      "loss": 0.0023,
      "step": 23190
    },
    {
      "epoch": 1.2373333333333334,
      "grad_norm": 0.11533533781766891,
      "learning_rate": 1.9066666666666668e-05,
      "loss": 0.0039,
      "step": 23200
    },
    {
      "epoch": 1.2378666666666667,
      "grad_norm": 0.338261216878891,
      "learning_rate": 1.9053333333333333e-05,
      "loss": 0.0018,
      "step": 23210
    },
    {
      "epoch": 1.2384,
      "grad_norm": 0.1733384132385254,
      "learning_rate": 1.904e-05,
      "loss": 0.0031,
      "step": 23220
    },
    {
      "epoch": 1.2389333333333332,
      "grad_norm": 0.26238715648651123,
      "learning_rate": 1.902666666666667e-05,
      "loss": 0.0016,
      "step": 23230
    },
    {
      "epoch": 1.2394666666666667,
      "grad_norm": 0.3444216549396515,
      "learning_rate": 1.9013333333333333e-05,
      "loss": 0.0023,
      "step": 23240
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.49748724699020386,
      "learning_rate": 1.9e-05,
      "loss": 0.0026,
      "step": 23250
    },
    {
      "epoch": 1.2405333333333333,
      "grad_norm": 0.21203704178333282,
      "learning_rate": 1.8986666666666666e-05,
      "loss": 0.0021,
      "step": 23260
    },
    {
      "epoch": 1.2410666666666668,
      "grad_norm": 0.12889045476913452,
      "learning_rate": 1.8973333333333334e-05,
      "loss": 0.002,
      "step": 23270
    },
    {
      "epoch": 1.2416,
      "grad_norm": 0.2913210391998291,
      "learning_rate": 1.896e-05,
      "loss": 0.002,
      "step": 23280
    },
    {
      "epoch": 1.2421333333333333,
      "grad_norm": 0.18788418173789978,
      "learning_rate": 1.894666666666667e-05,
      "loss": 0.0043,
      "step": 23290
    },
    {
      "epoch": 1.2426666666666666,
      "grad_norm": 0.1910393089056015,
      "learning_rate": 1.8933333333333334e-05,
      "loss": 0.0033,
      "step": 23300
    },
    {
      "epoch": 1.2432,
      "grad_norm": 0.22508683800697327,
      "learning_rate": 1.8920000000000002e-05,
      "loss": 0.003,
      "step": 23310
    },
    {
      "epoch": 1.2437333333333334,
      "grad_norm": 0.10396702587604523,
      "learning_rate": 1.8906666666666666e-05,
      "loss": 0.0027,
      "step": 23320
    },
    {
      "epoch": 1.2442666666666666,
      "grad_norm": 0.15124720335006714,
      "learning_rate": 1.8893333333333334e-05,
      "loss": 0.0026,
      "step": 23330
    },
    {
      "epoch": 1.2448,
      "grad_norm": 0.4608128070831299,
      "learning_rate": 1.888e-05,
      "loss": 0.0032,
      "step": 23340
    },
    {
      "epoch": 1.2453333333333334,
      "grad_norm": 0.3030379116535187,
      "learning_rate": 1.886666666666667e-05,
      "loss": 0.0021,
      "step": 23350
    },
    {
      "epoch": 1.2458666666666667,
      "grad_norm": 0.1660359650850296,
      "learning_rate": 1.8853333333333335e-05,
      "loss": 0.0022,
      "step": 23360
    },
    {
      "epoch": 1.2464,
      "grad_norm": 0.11636259406805038,
      "learning_rate": 1.8840000000000003e-05,
      "loss": 0.0026,
      "step": 23370
    },
    {
      "epoch": 1.2469333333333332,
      "grad_norm": 0.2639956474304199,
      "learning_rate": 1.8826666666666667e-05,
      "loss": 0.0027,
      "step": 23380
    },
    {
      "epoch": 1.2474666666666667,
      "grad_norm": 0.46339869499206543,
      "learning_rate": 1.8813333333333335e-05,
      "loss": 0.002,
      "step": 23390
    },
    {
      "epoch": 1.248,
      "grad_norm": 0.5565953850746155,
      "learning_rate": 1.88e-05,
      "loss": 0.0027,
      "step": 23400
    },
    {
      "epoch": 1.2485333333333333,
      "grad_norm": 0.4754660427570343,
      "learning_rate": 1.8786666666666667e-05,
      "loss": 0.0032,
      "step": 23410
    },
    {
      "epoch": 1.2490666666666668,
      "grad_norm": 0.18251308798789978,
      "learning_rate": 1.8773333333333335e-05,
      "loss": 0.0023,
      "step": 23420
    },
    {
      "epoch": 1.2496,
      "grad_norm": 0.49763867259025574,
      "learning_rate": 1.876e-05,
      "loss": 0.0018,
      "step": 23430
    },
    {
      "epoch": 1.2501333333333333,
      "grad_norm": 0.21746791899204254,
      "learning_rate": 1.8746666666666668e-05,
      "loss": 0.0022,
      "step": 23440
    },
    {
      "epoch": 1.2506666666666666,
      "grad_norm": 0.13309648633003235,
      "learning_rate": 1.8733333333333332e-05,
      "loss": 0.0033,
      "step": 23450
    },
    {
      "epoch": 1.2511999999999999,
      "grad_norm": 0.19005027413368225,
      "learning_rate": 1.872e-05,
      "loss": 0.0029,
      "step": 23460
    },
    {
      "epoch": 1.2517333333333334,
      "grad_norm": 0.35968416929244995,
      "learning_rate": 1.8706666666666668e-05,
      "loss": 0.0039,
      "step": 23470
    },
    {
      "epoch": 1.2522666666666666,
      "grad_norm": 0.6243705153465271,
      "learning_rate": 1.8693333333333336e-05,
      "loss": 0.0024,
      "step": 23480
    },
    {
      "epoch": 1.2528000000000001,
      "grad_norm": 0.1645982563495636,
      "learning_rate": 1.868e-05,
      "loss": 0.0024,
      "step": 23490
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 0.18672409653663635,
      "learning_rate": 1.866666666666667e-05,
      "loss": 0.0018,
      "step": 23500
    },
    {
      "epoch": 1.2538666666666667,
      "grad_norm": 0.3309403359889984,
      "learning_rate": 1.8653333333333333e-05,
      "loss": 0.0028,
      "step": 23510
    },
    {
      "epoch": 1.2544,
      "grad_norm": 0.6177411079406738,
      "learning_rate": 1.864e-05,
      "loss": 0.0021,
      "step": 23520
    },
    {
      "epoch": 1.2549333333333332,
      "grad_norm": 0.09477447718381882,
      "learning_rate": 1.8626666666666666e-05,
      "loss": 0.0028,
      "step": 23530
    },
    {
      "epoch": 1.2554666666666667,
      "grad_norm": 0.10252895206212997,
      "learning_rate": 1.8613333333333337e-05,
      "loss": 0.0026,
      "step": 23540
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.13689790666103363,
      "learning_rate": 1.86e-05,
      "loss": 0.0019,
      "step": 23550
    },
    {
      "epoch": 1.2565333333333333,
      "grad_norm": 0.20601725578308105,
      "learning_rate": 1.858666666666667e-05,
      "loss": 0.0024,
      "step": 23560
    },
    {
      "epoch": 1.2570666666666668,
      "grad_norm": 0.3684772253036499,
      "learning_rate": 1.8573333333333334e-05,
      "loss": 0.0038,
      "step": 23570
    },
    {
      "epoch": 1.2576,
      "grad_norm": 0.35761550068855286,
      "learning_rate": 1.856e-05,
      "loss": 0.0032,
      "step": 23580
    },
    {
      "epoch": 1.2581333333333333,
      "grad_norm": 0.5424566864967346,
      "learning_rate": 1.8546666666666666e-05,
      "loss": 0.0033,
      "step": 23590
    },
    {
      "epoch": 1.2586666666666666,
      "grad_norm": 0.6667088866233826,
      "learning_rate": 1.8533333333333334e-05,
      "loss": 0.0032,
      "step": 23600
    },
    {
      "epoch": 1.2591999999999999,
      "grad_norm": 0.1256456822156906,
      "learning_rate": 1.8520000000000002e-05,
      "loss": 0.0024,
      "step": 23610
    },
    {
      "epoch": 1.2597333333333334,
      "grad_norm": 0.30753061175346375,
      "learning_rate": 1.8506666666666667e-05,
      "loss": 0.002,
      "step": 23620
    },
    {
      "epoch": 1.2602666666666666,
      "grad_norm": 0.4264768362045288,
      "learning_rate": 1.8493333333333335e-05,
      "loss": 0.0027,
      "step": 23630
    },
    {
      "epoch": 1.2608,
      "grad_norm": 0.3606911599636078,
      "learning_rate": 1.848e-05,
      "loss": 0.0026,
      "step": 23640
    },
    {
      "epoch": 1.2613333333333334,
      "grad_norm": 0.13331305980682373,
      "learning_rate": 1.8466666666666667e-05,
      "loss": 0.002,
      "step": 23650
    },
    {
      "epoch": 1.2618666666666667,
      "grad_norm": 0.32555830478668213,
      "learning_rate": 1.8453333333333335e-05,
      "loss": 0.0029,
      "step": 23660
    },
    {
      "epoch": 1.2624,
      "grad_norm": 0.3174351751804352,
      "learning_rate": 1.8440000000000003e-05,
      "loss": 0.0026,
      "step": 23670
    },
    {
      "epoch": 1.2629333333333332,
      "grad_norm": 0.2685149013996124,
      "learning_rate": 1.8426666666666668e-05,
      "loss": 0.002,
      "step": 23680
    },
    {
      "epoch": 1.2634666666666667,
      "grad_norm": 0.19553236663341522,
      "learning_rate": 1.8413333333333335e-05,
      "loss": 0.0035,
      "step": 23690
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.3140157163143158,
      "learning_rate": 1.84e-05,
      "loss": 0.0018,
      "step": 23700
    },
    {
      "epoch": 1.2645333333333333,
      "grad_norm": 0.08990833163261414,
      "learning_rate": 1.8386666666666668e-05,
      "loss": 0.0027,
      "step": 23710
    },
    {
      "epoch": 1.2650666666666668,
      "grad_norm": 0.06844345480203629,
      "learning_rate": 1.8373333333333332e-05,
      "loss": 0.0034,
      "step": 23720
    },
    {
      "epoch": 1.2656,
      "grad_norm": 0.46813008189201355,
      "learning_rate": 1.8360000000000004e-05,
      "loss": 0.0036,
      "step": 23730
    },
    {
      "epoch": 1.2661333333333333,
      "grad_norm": 0.16179493069648743,
      "learning_rate": 1.834666666666667e-05,
      "loss": 0.0019,
      "step": 23740
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.4650498330593109,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 0.0034,
      "step": 23750
    },
    {
      "epoch": 1.2671999999999999,
      "grad_norm": 0.5593751668930054,
      "learning_rate": 1.832e-05,
      "loss": 0.0018,
      "step": 23760
    },
    {
      "epoch": 1.2677333333333334,
      "grad_norm": 0.1134815514087677,
      "learning_rate": 1.8306666666666665e-05,
      "loss": 0.0018,
      "step": 23770
    },
    {
      "epoch": 1.2682666666666667,
      "grad_norm": 0.28857341408729553,
      "learning_rate": 1.8293333333333333e-05,
      "loss": 0.0029,
      "step": 23780
    },
    {
      "epoch": 1.2688,
      "grad_norm": 0.6165235042572021,
      "learning_rate": 1.828e-05,
      "loss": 0.0029,
      "step": 23790
    },
    {
      "epoch": 1.2693333333333334,
      "grad_norm": 0.300621896982193,
      "learning_rate": 1.826666666666667e-05,
      "loss": 0.0019,
      "step": 23800
    },
    {
      "epoch": 1.2698666666666667,
      "grad_norm": 0.40042635798454285,
      "learning_rate": 1.8253333333333334e-05,
      "loss": 0.002,
      "step": 23810
    },
    {
      "epoch": 1.2704,
      "grad_norm": 0.11696886271238327,
      "learning_rate": 1.824e-05,
      "loss": 0.0019,
      "step": 23820
    },
    {
      "epoch": 1.2709333333333332,
      "grad_norm": 0.09612907469272614,
      "learning_rate": 1.8226666666666666e-05,
      "loss": 0.0028,
      "step": 23830
    },
    {
      "epoch": 1.2714666666666667,
      "grad_norm": 0.10394616425037384,
      "learning_rate": 1.8213333333333334e-05,
      "loss": 0.0026,
      "step": 23840
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.16372331976890564,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 0.0028,
      "step": 23850
    },
    {
      "epoch": 1.2725333333333333,
      "grad_norm": 0.08776495605707169,
      "learning_rate": 1.818666666666667e-05,
      "loss": 0.0026,
      "step": 23860
    },
    {
      "epoch": 1.2730666666666668,
      "grad_norm": 0.18173672258853912,
      "learning_rate": 1.8173333333333334e-05,
      "loss": 0.0021,
      "step": 23870
    },
    {
      "epoch": 1.2736,
      "grad_norm": 0.19144995510578156,
      "learning_rate": 1.8160000000000002e-05,
      "loss": 0.0017,
      "step": 23880
    },
    {
      "epoch": 1.2741333333333333,
      "grad_norm": 0.630523681640625,
      "learning_rate": 1.8146666666666667e-05,
      "loss": 0.0022,
      "step": 23890
    },
    {
      "epoch": 1.2746666666666666,
      "grad_norm": 0.5960223078727722,
      "learning_rate": 1.8133333333333335e-05,
      "loss": 0.0025,
      "step": 23900
    },
    {
      "epoch": 1.2752,
      "grad_norm": 0.5435652136802673,
      "learning_rate": 1.812e-05,
      "loss": 0.0024,
      "step": 23910
    },
    {
      "epoch": 1.2757333333333334,
      "grad_norm": 0.6103776097297668,
      "learning_rate": 1.8106666666666667e-05,
      "loss": 0.002,
      "step": 23920
    },
    {
      "epoch": 1.2762666666666667,
      "grad_norm": 0.12294302880764008,
      "learning_rate": 1.8093333333333335e-05,
      "loss": 0.0036,
      "step": 23930
    },
    {
      "epoch": 1.2768,
      "grad_norm": 0.2531452178955078,
      "learning_rate": 1.808e-05,
      "loss": 0.0021,
      "step": 23940
    },
    {
      "epoch": 1.2773333333333334,
      "grad_norm": 0.17644904553890228,
      "learning_rate": 1.8066666666666668e-05,
      "loss": 0.0028,
      "step": 23950
    },
    {
      "epoch": 1.2778666666666667,
      "grad_norm": 0.5513533353805542,
      "learning_rate": 1.8053333333333332e-05,
      "loss": 0.0033,
      "step": 23960
    },
    {
      "epoch": 1.2784,
      "grad_norm": 0.05388524383306503,
      "learning_rate": 1.804e-05,
      "loss": 0.0029,
      "step": 23970
    },
    {
      "epoch": 1.2789333333333333,
      "grad_norm": 0.1570982038974762,
      "learning_rate": 1.8026666666666668e-05,
      "loss": 0.0026,
      "step": 23980
    },
    {
      "epoch": 1.2794666666666665,
      "grad_norm": 0.06928537786006927,
      "learning_rate": 1.8013333333333336e-05,
      "loss": 0.0024,
      "step": 23990
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.2348065823316574,
      "learning_rate": 1.8e-05,
      "loss": 0.0034,
      "step": 24000
    },
    {
      "epoch": 1.2805333333333333,
      "grad_norm": 0.5544645190238953,
      "learning_rate": 1.798666666666667e-05,
      "loss": 0.002,
      "step": 24010
    },
    {
      "epoch": 1.2810666666666668,
      "grad_norm": 0.12888723611831665,
      "learning_rate": 1.7973333333333333e-05,
      "loss": 0.0035,
      "step": 24020
    },
    {
      "epoch": 1.2816,
      "grad_norm": 0.11304221302270889,
      "learning_rate": 1.796e-05,
      "loss": 0.0026,
      "step": 24030
    },
    {
      "epoch": 1.2821333333333333,
      "grad_norm": 0.3647758662700653,
      "learning_rate": 1.794666666666667e-05,
      "loss": 0.0022,
      "step": 24040
    },
    {
      "epoch": 1.2826666666666666,
      "grad_norm": 0.3990134000778198,
      "learning_rate": 1.7933333333333337e-05,
      "loss": 0.0019,
      "step": 24050
    },
    {
      "epoch": 1.2832,
      "grad_norm": 0.4382633864879608,
      "learning_rate": 1.792e-05,
      "loss": 0.0018,
      "step": 24060
    },
    {
      "epoch": 1.2837333333333334,
      "grad_norm": 0.4870215952396393,
      "learning_rate": 1.790666666666667e-05,
      "loss": 0.0035,
      "step": 24070
    },
    {
      "epoch": 1.2842666666666667,
      "grad_norm": 0.5377172827720642,
      "learning_rate": 1.7893333333333334e-05,
      "loss": 0.0022,
      "step": 24080
    },
    {
      "epoch": 1.2848,
      "grad_norm": 0.2604619860649109,
      "learning_rate": 1.7879999999999998e-05,
      "loss": 0.002,
      "step": 24090
    },
    {
      "epoch": 1.2853333333333334,
      "grad_norm": 0.14914971590042114,
      "learning_rate": 1.7866666666666666e-05,
      "loss": 0.0019,
      "step": 24100
    },
    {
      "epoch": 1.2858666666666667,
      "grad_norm": 0.2833344042301178,
      "learning_rate": 1.7853333333333334e-05,
      "loss": 0.0023,
      "step": 24110
    },
    {
      "epoch": 1.2864,
      "grad_norm": 0.4396990239620209,
      "learning_rate": 1.7840000000000002e-05,
      "loss": 0.0022,
      "step": 24120
    },
    {
      "epoch": 1.2869333333333333,
      "grad_norm": 0.10328599810600281,
      "learning_rate": 1.7826666666666667e-05,
      "loss": 0.0027,
      "step": 24130
    },
    {
      "epoch": 1.2874666666666665,
      "grad_norm": 0.8862357139587402,
      "learning_rate": 1.7813333333333334e-05,
      "loss": 0.002,
      "step": 24140
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.22562016546726227,
      "learning_rate": 1.78e-05,
      "loss": 0.0035,
      "step": 24150
    },
    {
      "epoch": 1.2885333333333333,
      "grad_norm": 0.1384677290916443,
      "learning_rate": 1.7786666666666667e-05,
      "loss": 0.0019,
      "step": 24160
    },
    {
      "epoch": 1.2890666666666668,
      "grad_norm": 0.7077102661132812,
      "learning_rate": 1.7773333333333335e-05,
      "loss": 0.0024,
      "step": 24170
    },
    {
      "epoch": 1.2896,
      "grad_norm": 0.22514581680297852,
      "learning_rate": 1.7760000000000003e-05,
      "loss": 0.0023,
      "step": 24180
    },
    {
      "epoch": 1.2901333333333334,
      "grad_norm": 0.08590157330036163,
      "learning_rate": 1.7746666666666667e-05,
      "loss": 0.0032,
      "step": 24190
    },
    {
      "epoch": 1.2906666666666666,
      "grad_norm": 0.37612271308898926,
      "learning_rate": 1.7733333333333335e-05,
      "loss": 0.0027,
      "step": 24200
    },
    {
      "epoch": 1.2912,
      "grad_norm": 0.1551225334405899,
      "learning_rate": 1.772e-05,
      "loss": 0.0018,
      "step": 24210
    },
    {
      "epoch": 1.2917333333333334,
      "grad_norm": 0.1404857486486435,
      "learning_rate": 1.7706666666666668e-05,
      "loss": 0.0018,
      "step": 24220
    },
    {
      "epoch": 1.2922666666666667,
      "grad_norm": 0.23802241683006287,
      "learning_rate": 1.7693333333333336e-05,
      "loss": 0.0024,
      "step": 24230
    },
    {
      "epoch": 1.2928,
      "grad_norm": 0.552068293094635,
      "learning_rate": 1.7680000000000004e-05,
      "loss": 0.0033,
      "step": 24240
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 0.3741737902164459,
      "learning_rate": 1.7666666666666668e-05,
      "loss": 0.004,
      "step": 24250
    },
    {
      "epoch": 1.2938666666666667,
      "grad_norm": 0.3250432312488556,
      "learning_rate": 1.7653333333333333e-05,
      "loss": 0.0031,
      "step": 24260
    },
    {
      "epoch": 1.2944,
      "grad_norm": 0.5340532064437866,
      "learning_rate": 1.764e-05,
      "loss": 0.0029,
      "step": 24270
    },
    {
      "epoch": 1.2949333333333333,
      "grad_norm": 0.2530789375305176,
      "learning_rate": 1.7626666666666665e-05,
      "loss": 0.0032,
      "step": 24280
    },
    {
      "epoch": 1.2954666666666665,
      "grad_norm": 0.3502427935600281,
      "learning_rate": 1.7613333333333333e-05,
      "loss": 0.0031,
      "step": 24290
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.23551861941814423,
      "learning_rate": 1.76e-05,
      "loss": 0.0025,
      "step": 24300
    },
    {
      "epoch": 1.2965333333333333,
      "grad_norm": 0.23896729946136475,
      "learning_rate": 1.758666666666667e-05,
      "loss": 0.0022,
      "step": 24310
    },
    {
      "epoch": 1.2970666666666666,
      "grad_norm": 0.43233707547187805,
      "learning_rate": 1.7573333333333333e-05,
      "loss": 0.003,
      "step": 24320
    },
    {
      "epoch": 1.2976,
      "grad_norm": 0.5180403590202332,
      "learning_rate": 1.756e-05,
      "loss": 0.0028,
      "step": 24330
    },
    {
      "epoch": 1.2981333333333334,
      "grad_norm": 0.11956733465194702,
      "learning_rate": 1.7546666666666666e-05,
      "loss": 0.0022,
      "step": 24340
    },
    {
      "epoch": 1.2986666666666666,
      "grad_norm": 0.38331693410873413,
      "learning_rate": 1.7533333333333334e-05,
      "loss": 0.0024,
      "step": 24350
    },
    {
      "epoch": 1.2992,
      "grad_norm": 0.4236001670360565,
      "learning_rate": 1.752e-05,
      "loss": 0.0031,
      "step": 24360
    },
    {
      "epoch": 1.2997333333333334,
      "grad_norm": 0.44091400504112244,
      "learning_rate": 1.750666666666667e-05,
      "loss": 0.0029,
      "step": 24370
    },
    {
      "epoch": 1.3002666666666667,
      "grad_norm": 0.1467505693435669,
      "learning_rate": 1.7493333333333334e-05,
      "loss": 0.0031,
      "step": 24380
    },
    {
      "epoch": 1.3008,
      "grad_norm": 0.49155235290527344,
      "learning_rate": 1.7480000000000002e-05,
      "loss": 0.0041,
      "step": 24390
    },
    {
      "epoch": 1.3013333333333335,
      "grad_norm": 0.27295053005218506,
      "learning_rate": 1.7466666666666667e-05,
      "loss": 0.0022,
      "step": 24400
    },
    {
      "epoch": 1.3018666666666667,
      "grad_norm": 0.0891600176692009,
      "learning_rate": 1.7453333333333335e-05,
      "loss": 0.0028,
      "step": 24410
    },
    {
      "epoch": 1.3024,
      "grad_norm": 0.4141347110271454,
      "learning_rate": 1.7440000000000002e-05,
      "loss": 0.0026,
      "step": 24420
    },
    {
      "epoch": 1.3029333333333333,
      "grad_norm": 0.5543745160102844,
      "learning_rate": 1.7426666666666667e-05,
      "loss": 0.0023,
      "step": 24430
    },
    {
      "epoch": 1.3034666666666666,
      "grad_norm": 0.09172928333282471,
      "learning_rate": 1.7413333333333335e-05,
      "loss": 0.0037,
      "step": 24440
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.30524593591690063,
      "learning_rate": 1.74e-05,
      "loss": 0.0031,
      "step": 24450
    },
    {
      "epoch": 1.3045333333333333,
      "grad_norm": 0.11321097612380981,
      "learning_rate": 1.7386666666666667e-05,
      "loss": 0.002,
      "step": 24460
    },
    {
      "epoch": 1.3050666666666666,
      "grad_norm": 0.2520567774772644,
      "learning_rate": 1.7373333333333332e-05,
      "loss": 0.0021,
      "step": 24470
    },
    {
      "epoch": 1.3056,
      "grad_norm": 0.0954497754573822,
      "learning_rate": 1.736e-05,
      "loss": 0.0018,
      "step": 24480
    },
    {
      "epoch": 1.3061333333333334,
      "grad_norm": 0.09926118701696396,
      "learning_rate": 1.7346666666666668e-05,
      "loss": 0.0023,
      "step": 24490
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 0.24252445995807648,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 0.0022,
      "step": 24500
    },
    {
      "epoch": 1.3072,
      "grad_norm": 0.8273232579231262,
      "learning_rate": 1.732e-05,
      "loss": 0.0024,
      "step": 24510
    },
    {
      "epoch": 1.3077333333333334,
      "grad_norm": 0.0708179771900177,
      "learning_rate": 1.7306666666666668e-05,
      "loss": 0.0016,
      "step": 24520
    },
    {
      "epoch": 1.3082666666666667,
      "grad_norm": 0.26214972138404846,
      "learning_rate": 1.7293333333333333e-05,
      "loss": 0.0039,
      "step": 24530
    },
    {
      "epoch": 1.3088,
      "grad_norm": 0.3614501655101776,
      "learning_rate": 1.728e-05,
      "loss": 0.0024,
      "step": 24540
    },
    {
      "epoch": 1.3093333333333335,
      "grad_norm": 0.15613146126270294,
      "learning_rate": 1.726666666666667e-05,
      "loss": 0.0026,
      "step": 24550
    },
    {
      "epoch": 1.3098666666666667,
      "grad_norm": 0.28019729256629944,
      "learning_rate": 1.7253333333333336e-05,
      "loss": 0.0022,
      "step": 24560
    },
    {
      "epoch": 1.3104,
      "grad_norm": 0.10830344259738922,
      "learning_rate": 1.724e-05,
      "loss": 0.0032,
      "step": 24570
    },
    {
      "epoch": 1.3109333333333333,
      "grad_norm": 0.20300886034965515,
      "learning_rate": 1.722666666666667e-05,
      "loss": 0.0017,
      "step": 24580
    },
    {
      "epoch": 1.3114666666666666,
      "grad_norm": 0.14034594595432281,
      "learning_rate": 1.7213333333333333e-05,
      "loss": 0.0039,
      "step": 24590
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.17801117897033691,
      "learning_rate": 1.7199999999999998e-05,
      "loss": 0.0019,
      "step": 24600
    },
    {
      "epoch": 1.3125333333333333,
      "grad_norm": 0.2964555025100708,
      "learning_rate": 1.718666666666667e-05,
      "loss": 0.0022,
      "step": 24610
    },
    {
      "epoch": 1.3130666666666666,
      "grad_norm": 0.19784757494926453,
      "learning_rate": 1.7173333333333334e-05,
      "loss": 0.0019,
      "step": 24620
    },
    {
      "epoch": 1.3136,
      "grad_norm": 0.3184483051300049,
      "learning_rate": 1.7160000000000002e-05,
      "loss": 0.0026,
      "step": 24630
    },
    {
      "epoch": 1.3141333333333334,
      "grad_norm": 0.15285950899124146,
      "learning_rate": 1.7146666666666666e-05,
      "loss": 0.0022,
      "step": 24640
    },
    {
      "epoch": 1.3146666666666667,
      "grad_norm": 0.18357843160629272,
      "learning_rate": 1.7133333333333334e-05,
      "loss": 0.002,
      "step": 24650
    },
    {
      "epoch": 1.3152,
      "grad_norm": 0.21413511037826538,
      "learning_rate": 1.712e-05,
      "loss": 0.0021,
      "step": 24660
    },
    {
      "epoch": 1.3157333333333332,
      "grad_norm": 0.13527852296829224,
      "learning_rate": 1.7106666666666667e-05,
      "loss": 0.0027,
      "step": 24670
    },
    {
      "epoch": 1.3162666666666667,
      "grad_norm": 0.15185484290122986,
      "learning_rate": 1.7093333333333335e-05,
      "loss": 0.0025,
      "step": 24680
    },
    {
      "epoch": 1.3168,
      "grad_norm": 0.4441893994808197,
      "learning_rate": 1.7080000000000002e-05,
      "loss": 0.0021,
      "step": 24690
    },
    {
      "epoch": 1.3173333333333335,
      "grad_norm": 0.12500706315040588,
      "learning_rate": 1.7066666666666667e-05,
      "loss": 0.0019,
      "step": 24700
    },
    {
      "epoch": 1.3178666666666667,
      "grad_norm": 0.3910733461380005,
      "learning_rate": 1.7053333333333335e-05,
      "loss": 0.003,
      "step": 24710
    },
    {
      "epoch": 1.3184,
      "grad_norm": 0.120568186044693,
      "learning_rate": 1.704e-05,
      "loss": 0.003,
      "step": 24720
    },
    {
      "epoch": 1.3189333333333333,
      "grad_norm": 0.33244866132736206,
      "learning_rate": 1.7026666666666667e-05,
      "loss": 0.0026,
      "step": 24730
    },
    {
      "epoch": 1.3194666666666666,
      "grad_norm": 0.519724428653717,
      "learning_rate": 1.7013333333333335e-05,
      "loss": 0.0021,
      "step": 24740
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.338207483291626,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.0017,
      "step": 24750
    },
    {
      "epoch": 1.3205333333333333,
      "grad_norm": 0.4662485122680664,
      "learning_rate": 1.6986666666666668e-05,
      "loss": 0.002,
      "step": 24760
    },
    {
      "epoch": 1.3210666666666666,
      "grad_norm": 0.1832399070262909,
      "learning_rate": 1.6973333333333336e-05,
      "loss": 0.003,
      "step": 24770
    },
    {
      "epoch": 1.3216,
      "grad_norm": 0.1042749434709549,
      "learning_rate": 1.696e-05,
      "loss": 0.0021,
      "step": 24780
    },
    {
      "epoch": 1.3221333333333334,
      "grad_norm": 0.09796963632106781,
      "learning_rate": 1.6946666666666665e-05,
      "loss": 0.002,
      "step": 24790
    },
    {
      "epoch": 1.3226666666666667,
      "grad_norm": 0.5406732559204102,
      "learning_rate": 1.6933333333333333e-05,
      "loss": 0.0021,
      "step": 24800
    },
    {
      "epoch": 1.3232,
      "grad_norm": 0.459169864654541,
      "learning_rate": 1.692e-05,
      "loss": 0.002,
      "step": 24810
    },
    {
      "epoch": 1.3237333333333332,
      "grad_norm": 0.487358033657074,
      "learning_rate": 1.690666666666667e-05,
      "loss": 0.0041,
      "step": 24820
    },
    {
      "epoch": 1.3242666666666667,
      "grad_norm": 0.20371310412883759,
      "learning_rate": 1.6893333333333333e-05,
      "loss": 0.0026,
      "step": 24830
    },
    {
      "epoch": 1.3248,
      "grad_norm": 0.30602598190307617,
      "learning_rate": 1.688e-05,
      "loss": 0.0027,
      "step": 24840
    },
    {
      "epoch": 1.3253333333333333,
      "grad_norm": 0.09095501154661179,
      "learning_rate": 1.6866666666666666e-05,
      "loss": 0.0026,
      "step": 24850
    },
    {
      "epoch": 1.3258666666666667,
      "grad_norm": 0.1152312159538269,
      "learning_rate": 1.6853333333333333e-05,
      "loss": 0.0024,
      "step": 24860
    },
    {
      "epoch": 1.3264,
      "grad_norm": 0.22297242283821106,
      "learning_rate": 1.684e-05,
      "loss": 0.0023,
      "step": 24870
    },
    {
      "epoch": 1.3269333333333333,
      "grad_norm": 0.43240949511528015,
      "learning_rate": 1.682666666666667e-05,
      "loss": 0.0025,
      "step": 24880
    },
    {
      "epoch": 1.3274666666666666,
      "grad_norm": 0.2352456897497177,
      "learning_rate": 1.6813333333333334e-05,
      "loss": 0.0021,
      "step": 24890
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.1919284164905548,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 0.0027,
      "step": 24900
    },
    {
      "epoch": 1.3285333333333333,
      "grad_norm": 0.40977948904037476,
      "learning_rate": 1.6786666666666666e-05,
      "loss": 0.0021,
      "step": 24910
    },
    {
      "epoch": 1.3290666666666666,
      "grad_norm": 0.1344844400882721,
      "learning_rate": 1.6773333333333334e-05,
      "loss": 0.003,
      "step": 24920
    },
    {
      "epoch": 1.3296000000000001,
      "grad_norm": 0.30267438292503357,
      "learning_rate": 1.6760000000000002e-05,
      "loss": 0.0029,
      "step": 24930
    },
    {
      "epoch": 1.3301333333333334,
      "grad_norm": 0.11016524583101273,
      "learning_rate": 1.674666666666667e-05,
      "loss": 0.0031,
      "step": 24940
    },
    {
      "epoch": 1.3306666666666667,
      "grad_norm": 0.0904170572757721,
      "learning_rate": 1.6733333333333335e-05,
      "loss": 0.002,
      "step": 24950
    },
    {
      "epoch": 1.3312,
      "grad_norm": 0.18241111934185028,
      "learning_rate": 1.672e-05,
      "loss": 0.002,
      "step": 24960
    },
    {
      "epoch": 1.3317333333333332,
      "grad_norm": 0.33722129464149475,
      "learning_rate": 1.6706666666666667e-05,
      "loss": 0.0022,
      "step": 24970
    },
    {
      "epoch": 1.3322666666666667,
      "grad_norm": 0.1703951060771942,
      "learning_rate": 1.669333333333333e-05,
      "loss": 0.0021,
      "step": 24980
    },
    {
      "epoch": 1.3328,
      "grad_norm": 0.2918054461479187,
      "learning_rate": 1.668e-05,
      "loss": 0.0027,
      "step": 24990
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.5887743830680847,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.0024,
      "step": 25000
    },
    {
      "epoch": 1.3338666666666668,
      "grad_norm": 0.7686341404914856,
      "learning_rate": 1.6653333333333335e-05,
      "loss": 0.0027,
      "step": 25010
    },
    {
      "epoch": 1.3344,
      "grad_norm": 0.2082618921995163,
      "learning_rate": 1.664e-05,
      "loss": 0.0033,
      "step": 25020
    },
    {
      "epoch": 1.3349333333333333,
      "grad_norm": 0.6628938913345337,
      "learning_rate": 1.6626666666666668e-05,
      "loss": 0.0023,
      "step": 25030
    },
    {
      "epoch": 1.3354666666666666,
      "grad_norm": 0.35731858015060425,
      "learning_rate": 1.6613333333333332e-05,
      "loss": 0.0048,
      "step": 25040
    },
    {
      "epoch": 1.336,
      "grad_norm": 0.23310944437980652,
      "learning_rate": 1.66e-05,
      "loss": 0.0022,
      "step": 25050
    },
    {
      "epoch": 1.3365333333333334,
      "grad_norm": 0.21733328700065613,
      "learning_rate": 1.6586666666666668e-05,
      "loss": 0.0018,
      "step": 25060
    },
    {
      "epoch": 1.3370666666666666,
      "grad_norm": 0.33249062299728394,
      "learning_rate": 1.6573333333333336e-05,
      "loss": 0.002,
      "step": 25070
    },
    {
      "epoch": 1.3376000000000001,
      "grad_norm": 0.1825627237558365,
      "learning_rate": 1.656e-05,
      "loss": 0.0019,
      "step": 25080
    },
    {
      "epoch": 1.3381333333333334,
      "grad_norm": 0.395801305770874,
      "learning_rate": 1.654666666666667e-05,
      "loss": 0.0028,
      "step": 25090
    },
    {
      "epoch": 1.3386666666666667,
      "grad_norm": 0.1279449164867401,
      "learning_rate": 1.6533333333333333e-05,
      "loss": 0.0024,
      "step": 25100
    },
    {
      "epoch": 1.3392,
      "grad_norm": 0.16054946184158325,
      "learning_rate": 1.652e-05,
      "loss": 0.0026,
      "step": 25110
    },
    {
      "epoch": 1.3397333333333332,
      "grad_norm": 0.20433621108531952,
      "learning_rate": 1.650666666666667e-05,
      "loss": 0.0027,
      "step": 25120
    },
    {
      "epoch": 1.3402666666666667,
      "grad_norm": 0.26084011793136597,
      "learning_rate": 1.6493333333333334e-05,
      "loss": 0.0027,
      "step": 25130
    },
    {
      "epoch": 1.3408,
      "grad_norm": 0.3386155068874359,
      "learning_rate": 1.648e-05,
      "loss": 0.0019,
      "step": 25140
    },
    {
      "epoch": 1.3413333333333333,
      "grad_norm": 0.12577621638774872,
      "learning_rate": 1.6466666666666666e-05,
      "loss": 0.0021,
      "step": 25150
    },
    {
      "epoch": 1.3418666666666668,
      "grad_norm": 0.1565476506948471,
      "learning_rate": 1.6453333333333334e-05,
      "loss": 0.0031,
      "step": 25160
    },
    {
      "epoch": 1.3424,
      "grad_norm": 0.4680471420288086,
      "learning_rate": 1.644e-05,
      "loss": 0.0025,
      "step": 25170
    },
    {
      "epoch": 1.3429333333333333,
      "grad_norm": 0.6633978486061096,
      "learning_rate": 1.6426666666666666e-05,
      "loss": 0.003,
      "step": 25180
    },
    {
      "epoch": 1.3434666666666666,
      "grad_norm": 0.15576127171516418,
      "learning_rate": 1.6413333333333334e-05,
      "loss": 0.0024,
      "step": 25190
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.13463033735752106,
      "learning_rate": 1.6400000000000002e-05,
      "loss": 0.0031,
      "step": 25200
    },
    {
      "epoch": 1.3445333333333334,
      "grad_norm": 0.09540960937738419,
      "learning_rate": 1.6386666666666667e-05,
      "loss": 0.0018,
      "step": 25210
    },
    {
      "epoch": 1.3450666666666666,
      "grad_norm": 0.5109608173370361,
      "learning_rate": 1.6373333333333335e-05,
      "loss": 0.0023,
      "step": 25220
    },
    {
      "epoch": 1.3456000000000001,
      "grad_norm": 0.12123817205429077,
      "learning_rate": 1.636e-05,
      "loss": 0.0018,
      "step": 25230
    },
    {
      "epoch": 1.3461333333333334,
      "grad_norm": 0.33093541860580444,
      "learning_rate": 1.6346666666666667e-05,
      "loss": 0.0025,
      "step": 25240
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.2398078888654709,
      "learning_rate": 1.6333333333333335e-05,
      "loss": 0.0034,
      "step": 25250
    },
    {
      "epoch": 1.3472,
      "grad_norm": 0.23686885833740234,
      "learning_rate": 1.6320000000000003e-05,
      "loss": 0.0031,
      "step": 25260
    },
    {
      "epoch": 1.3477333333333332,
      "grad_norm": 0.22598865628242493,
      "learning_rate": 1.6306666666666668e-05,
      "loss": 0.0029,
      "step": 25270
    },
    {
      "epoch": 1.3482666666666667,
      "grad_norm": 0.3038088381290436,
      "learning_rate": 1.6293333333333335e-05,
      "loss": 0.0032,
      "step": 25280
    },
    {
      "epoch": 1.3488,
      "grad_norm": 0.10083051025867462,
      "learning_rate": 1.628e-05,
      "loss": 0.0023,
      "step": 25290
    },
    {
      "epoch": 1.3493333333333333,
      "grad_norm": 0.3079114854335785,
      "learning_rate": 1.6266666666666665e-05,
      "loss": 0.0023,
      "step": 25300
    },
    {
      "epoch": 1.3498666666666668,
      "grad_norm": 0.18750911951065063,
      "learning_rate": 1.6253333333333336e-05,
      "loss": 0.0023,
      "step": 25310
    },
    {
      "epoch": 1.3504,
      "grad_norm": 0.2911813259124756,
      "learning_rate": 1.624e-05,
      "loss": 0.003,
      "step": 25320
    },
    {
      "epoch": 1.3509333333333333,
      "grad_norm": 0.19650985300540924,
      "learning_rate": 1.6226666666666668e-05,
      "loss": 0.0015,
      "step": 25330
    },
    {
      "epoch": 1.3514666666666666,
      "grad_norm": 0.41555875539779663,
      "learning_rate": 1.6213333333333333e-05,
      "loss": 0.0022,
      "step": 25340
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.31433436274528503,
      "learning_rate": 1.62e-05,
      "loss": 0.0019,
      "step": 25350
    },
    {
      "epoch": 1.3525333333333334,
      "grad_norm": 0.45558130741119385,
      "learning_rate": 1.6186666666666665e-05,
      "loss": 0.0017,
      "step": 25360
    },
    {
      "epoch": 1.3530666666666666,
      "grad_norm": 0.28184187412261963,
      "learning_rate": 1.6173333333333333e-05,
      "loss": 0.0032,
      "step": 25370
    },
    {
      "epoch": 1.3536000000000001,
      "grad_norm": 0.20396380126476288,
      "learning_rate": 1.616e-05,
      "loss": 0.0021,
      "step": 25380
    },
    {
      "epoch": 1.3541333333333334,
      "grad_norm": 0.3941955268383026,
      "learning_rate": 1.614666666666667e-05,
      "loss": 0.0019,
      "step": 25390
    },
    {
      "epoch": 1.3546666666666667,
      "grad_norm": 0.36959800124168396,
      "learning_rate": 1.6133333333333334e-05,
      "loss": 0.0022,
      "step": 25400
    },
    {
      "epoch": 1.3552,
      "grad_norm": 0.34376052021980286,
      "learning_rate": 1.612e-05,
      "loss": 0.0032,
      "step": 25410
    },
    {
      "epoch": 1.3557333333333332,
      "grad_norm": 0.1425188183784485,
      "learning_rate": 1.6106666666666666e-05,
      "loss": 0.0018,
      "step": 25420
    },
    {
      "epoch": 1.3562666666666667,
      "grad_norm": 0.17420050501823425,
      "learning_rate": 1.6093333333333334e-05,
      "loss": 0.0018,
      "step": 25430
    },
    {
      "epoch": 1.3568,
      "grad_norm": 0.3254833221435547,
      "learning_rate": 1.6080000000000002e-05,
      "loss": 0.0019,
      "step": 25440
    },
    {
      "epoch": 1.3573333333333333,
      "grad_norm": 0.3682910203933716,
      "learning_rate": 1.606666666666667e-05,
      "loss": 0.0018,
      "step": 25450
    },
    {
      "epoch": 1.3578666666666668,
      "grad_norm": 0.4435211718082428,
      "learning_rate": 1.6053333333333334e-05,
      "loss": 0.0049,
      "step": 25460
    },
    {
      "epoch": 1.3584,
      "grad_norm": 0.5194787979125977,
      "learning_rate": 1.604e-05,
      "loss": 0.0019,
      "step": 25470
    },
    {
      "epoch": 1.3589333333333333,
      "grad_norm": 0.2819445729255676,
      "learning_rate": 1.6026666666666667e-05,
      "loss": 0.0029,
      "step": 25480
    },
    {
      "epoch": 1.3594666666666666,
      "grad_norm": 0.5243962407112122,
      "learning_rate": 1.601333333333333e-05,
      "loss": 0.0019,
      "step": 25490
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.39164435863494873,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.0021,
      "step": 25500
    },
    {
      "epoch": 1.3605333333333334,
      "grad_norm": 0.1311173141002655,
      "learning_rate": 1.5986666666666667e-05,
      "loss": 0.0018,
      "step": 25510
    },
    {
      "epoch": 1.3610666666666666,
      "grad_norm": 0.16906097531318665,
      "learning_rate": 1.5973333333333335e-05,
      "loss": 0.0023,
      "step": 25520
    },
    {
      "epoch": 1.3616,
      "grad_norm": 0.46361133456230164,
      "learning_rate": 1.596e-05,
      "loss": 0.0024,
      "step": 25530
    },
    {
      "epoch": 1.3621333333333334,
      "grad_norm": 0.3474052846431732,
      "learning_rate": 1.5946666666666668e-05,
      "loss": 0.002,
      "step": 25540
    },
    {
      "epoch": 1.3626666666666667,
      "grad_norm": 0.2401641309261322,
      "learning_rate": 1.5933333333333332e-05,
      "loss": 0.0022,
      "step": 25550
    },
    {
      "epoch": 1.3632,
      "grad_norm": 0.25182661414146423,
      "learning_rate": 1.592e-05,
      "loss": 0.0023,
      "step": 25560
    },
    {
      "epoch": 1.3637333333333332,
      "grad_norm": 0.27761954069137573,
      "learning_rate": 1.5906666666666668e-05,
      "loss": 0.0024,
      "step": 25570
    },
    {
      "epoch": 1.3642666666666667,
      "grad_norm": 0.14332997798919678,
      "learning_rate": 1.5893333333333336e-05,
      "loss": 0.0024,
      "step": 25580
    },
    {
      "epoch": 1.3648,
      "grad_norm": 0.09745950251817703,
      "learning_rate": 1.588e-05,
      "loss": 0.0021,
      "step": 25590
    },
    {
      "epoch": 1.3653333333333333,
      "grad_norm": 0.2302316129207611,
      "learning_rate": 1.586666666666667e-05,
      "loss": 0.0035,
      "step": 25600
    },
    {
      "epoch": 1.3658666666666668,
      "grad_norm": 0.24487578868865967,
      "learning_rate": 1.5853333333333333e-05,
      "loss": 0.0028,
      "step": 25610
    },
    {
      "epoch": 1.3664,
      "grad_norm": 0.29799744486808777,
      "learning_rate": 1.584e-05,
      "loss": 0.0034,
      "step": 25620
    },
    {
      "epoch": 1.3669333333333333,
      "grad_norm": 0.25794216990470886,
      "learning_rate": 1.582666666666667e-05,
      "loss": 0.0028,
      "step": 25630
    },
    {
      "epoch": 1.3674666666666666,
      "grad_norm": 0.2779976427555084,
      "learning_rate": 1.5813333333333333e-05,
      "loss": 0.0026,
      "step": 25640
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 0.4697932004928589,
      "learning_rate": 1.58e-05,
      "loss": 0.0018,
      "step": 25650
    },
    {
      "epoch": 1.3685333333333334,
      "grad_norm": 0.1835179328918457,
      "learning_rate": 1.5786666666666666e-05,
      "loss": 0.0027,
      "step": 25660
    },
    {
      "epoch": 1.3690666666666667,
      "grad_norm": 0.2392476350069046,
      "learning_rate": 1.5773333333333334e-05,
      "loss": 0.0028,
      "step": 25670
    },
    {
      "epoch": 1.3696,
      "grad_norm": 0.1448633223772049,
      "learning_rate": 1.5759999999999998e-05,
      "loss": 0.0023,
      "step": 25680
    },
    {
      "epoch": 1.3701333333333334,
      "grad_norm": 0.7939630150794983,
      "learning_rate": 1.574666666666667e-05,
      "loss": 0.002,
      "step": 25690
    },
    {
      "epoch": 1.3706666666666667,
      "grad_norm": 0.5686816573143005,
      "learning_rate": 1.5733333333333334e-05,
      "loss": 0.0033,
      "step": 25700
    },
    {
      "epoch": 1.3712,
      "grad_norm": 0.07876554131507874,
      "learning_rate": 1.5720000000000002e-05,
      "loss": 0.0022,
      "step": 25710
    },
    {
      "epoch": 1.3717333333333332,
      "grad_norm": 0.26666852831840515,
      "learning_rate": 1.5706666666666666e-05,
      "loss": 0.0019,
      "step": 25720
    },
    {
      "epoch": 1.3722666666666667,
      "grad_norm": 0.1500963717699051,
      "learning_rate": 1.5693333333333334e-05,
      "loss": 0.0042,
      "step": 25730
    },
    {
      "epoch": 1.3728,
      "grad_norm": 0.5303235054016113,
      "learning_rate": 1.568e-05,
      "loss": 0.0027,
      "step": 25740
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.1727582961320877,
      "learning_rate": 1.5666666666666667e-05,
      "loss": 0.0025,
      "step": 25750
    },
    {
      "epoch": 1.3738666666666668,
      "grad_norm": 0.40535080432891846,
      "learning_rate": 1.5653333333333335e-05,
      "loss": 0.0019,
      "step": 25760
    },
    {
      "epoch": 1.3744,
      "grad_norm": 0.3022165596485138,
      "learning_rate": 1.5640000000000003e-05,
      "loss": 0.0027,
      "step": 25770
    },
    {
      "epoch": 1.3749333333333333,
      "grad_norm": 0.4022662937641144,
      "learning_rate": 1.5626666666666667e-05,
      "loss": 0.0021,
      "step": 25780
    },
    {
      "epoch": 1.3754666666666666,
      "grad_norm": 0.2702683210372925,
      "learning_rate": 1.5613333333333335e-05,
      "loss": 0.0021,
      "step": 25790
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.19349750876426697,
      "learning_rate": 1.56e-05,
      "loss": 0.002,
      "step": 25800
    },
    {
      "epoch": 1.3765333333333334,
      "grad_norm": 0.3533499240875244,
      "learning_rate": 1.5586666666666668e-05,
      "loss": 0.0019,
      "step": 25810
    },
    {
      "epoch": 1.3770666666666667,
      "grad_norm": 0.465636283159256,
      "learning_rate": 1.5573333333333336e-05,
      "loss": 0.0019,
      "step": 25820
    },
    {
      "epoch": 1.3776,
      "grad_norm": 0.41871628165245056,
      "learning_rate": 1.556e-05,
      "loss": 0.0039,
      "step": 25830
    },
    {
      "epoch": 1.3781333333333334,
      "grad_norm": 0.15057966113090515,
      "learning_rate": 1.5546666666666668e-05,
      "loss": 0.0022,
      "step": 25840
    },
    {
      "epoch": 1.3786666666666667,
      "grad_norm": 0.4549427926540375,
      "learning_rate": 1.5533333333333333e-05,
      "loss": 0.0027,
      "step": 25850
    },
    {
      "epoch": 1.3792,
      "grad_norm": 0.6211471557617188,
      "learning_rate": 1.552e-05,
      "loss": 0.0027,
      "step": 25860
    },
    {
      "epoch": 1.3797333333333333,
      "grad_norm": 0.2047591358423233,
      "learning_rate": 1.5506666666666665e-05,
      "loss": 0.0025,
      "step": 25870
    },
    {
      "epoch": 1.3802666666666665,
      "grad_norm": 0.677952229976654,
      "learning_rate": 1.5493333333333336e-05,
      "loss": 0.003,
      "step": 25880
    },
    {
      "epoch": 1.3808,
      "grad_norm": 0.22003613412380219,
      "learning_rate": 1.548e-05,
      "loss": 0.0018,
      "step": 25890
    },
    {
      "epoch": 1.3813333333333333,
      "grad_norm": 0.25553029775619507,
      "learning_rate": 1.546666666666667e-05,
      "loss": 0.002,
      "step": 25900
    },
    {
      "epoch": 1.3818666666666668,
      "grad_norm": 0.11019890010356903,
      "learning_rate": 1.5453333333333333e-05,
      "loss": 0.0027,
      "step": 25910
    },
    {
      "epoch": 1.3824,
      "grad_norm": 0.08003265410661697,
      "learning_rate": 1.544e-05,
      "loss": 0.0022,
      "step": 25920
    },
    {
      "epoch": 1.3829333333333333,
      "grad_norm": 0.3117566406726837,
      "learning_rate": 1.5426666666666666e-05,
      "loss": 0.0021,
      "step": 25930
    },
    {
      "epoch": 1.3834666666666666,
      "grad_norm": 0.32453712821006775,
      "learning_rate": 1.5413333333333334e-05,
      "loss": 0.0029,
      "step": 25940
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.26311227679252625,
      "learning_rate": 1.54e-05,
      "loss": 0.0024,
      "step": 25950
    },
    {
      "epoch": 1.3845333333333334,
      "grad_norm": 0.6821592450141907,
      "learning_rate": 1.538666666666667e-05,
      "loss": 0.0019,
      "step": 25960
    },
    {
      "epoch": 1.3850666666666667,
      "grad_norm": 0.23572063446044922,
      "learning_rate": 1.5373333333333334e-05,
      "loss": 0.0018,
      "step": 25970
    },
    {
      "epoch": 1.3856,
      "grad_norm": 0.22935420274734497,
      "learning_rate": 1.536e-05,
      "loss": 0.0033,
      "step": 25980
    },
    {
      "epoch": 1.3861333333333334,
      "grad_norm": 0.11370466649532318,
      "learning_rate": 1.5346666666666667e-05,
      "loss": 0.0025,
      "step": 25990
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.22516800463199615,
      "learning_rate": 1.5333333333333334e-05,
      "loss": 0.0028,
      "step": 26000
    },
    {
      "epoch": 1.3872,
      "grad_norm": 0.380144327878952,
      "learning_rate": 1.5320000000000002e-05,
      "loss": 0.0035,
      "step": 26010
    },
    {
      "epoch": 1.3877333333333333,
      "grad_norm": 0.5402164459228516,
      "learning_rate": 1.5306666666666667e-05,
      "loss": 0.0019,
      "step": 26020
    },
    {
      "epoch": 1.3882666666666665,
      "grad_norm": 0.2524600923061371,
      "learning_rate": 1.5293333333333335e-05,
      "loss": 0.002,
      "step": 26030
    },
    {
      "epoch": 1.3888,
      "grad_norm": 0.8526264429092407,
      "learning_rate": 1.528e-05,
      "loss": 0.0031,
      "step": 26040
    },
    {
      "epoch": 1.3893333333333333,
      "grad_norm": 0.20121894776821136,
      "learning_rate": 1.5266666666666667e-05,
      "loss": 0.0027,
      "step": 26050
    },
    {
      "epoch": 1.3898666666666666,
      "grad_norm": 0.11935620754957199,
      "learning_rate": 1.5253333333333334e-05,
      "loss": 0.0018,
      "step": 26060
    },
    {
      "epoch": 1.3904,
      "grad_norm": 0.15136834979057312,
      "learning_rate": 1.5240000000000001e-05,
      "loss": 0.0019,
      "step": 26070
    },
    {
      "epoch": 1.3909333333333334,
      "grad_norm": 0.43130746483802795,
      "learning_rate": 1.5226666666666668e-05,
      "loss": 0.0022,
      "step": 26080
    },
    {
      "epoch": 1.3914666666666666,
      "grad_norm": 0.21635384857654572,
      "learning_rate": 1.5213333333333336e-05,
      "loss": 0.0025,
      "step": 26090
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.33097684383392334,
      "learning_rate": 1.52e-05,
      "loss": 0.0025,
      "step": 26100
    },
    {
      "epoch": 1.3925333333333334,
      "grad_norm": 0.12122806161642075,
      "learning_rate": 1.5186666666666668e-05,
      "loss": 0.0034,
      "step": 26110
    },
    {
      "epoch": 1.3930666666666667,
      "grad_norm": 0.08206655830144882,
      "learning_rate": 1.5173333333333334e-05,
      "loss": 0.0037,
      "step": 26120
    },
    {
      "epoch": 1.3936,
      "grad_norm": 0.145682692527771,
      "learning_rate": 1.5160000000000002e-05,
      "loss": 0.0029,
      "step": 26130
    },
    {
      "epoch": 1.3941333333333334,
      "grad_norm": 0.2577114701271057,
      "learning_rate": 1.5146666666666667e-05,
      "loss": 0.0029,
      "step": 26140
    },
    {
      "epoch": 1.3946666666666667,
      "grad_norm": 0.1959548145532608,
      "learning_rate": 1.5133333333333333e-05,
      "loss": 0.003,
      "step": 26150
    },
    {
      "epoch": 1.3952,
      "grad_norm": 0.11969254165887833,
      "learning_rate": 1.5120000000000001e-05,
      "loss": 0.0021,
      "step": 26160
    },
    {
      "epoch": 1.3957333333333333,
      "grad_norm": 0.44649049639701843,
      "learning_rate": 1.5106666666666665e-05,
      "loss": 0.0037,
      "step": 26170
    },
    {
      "epoch": 1.3962666666666665,
      "grad_norm": 0.08974159508943558,
      "learning_rate": 1.5093333333333335e-05,
      "loss": 0.0021,
      "step": 26180
    },
    {
      "epoch": 1.3968,
      "grad_norm": 0.17130343616008759,
      "learning_rate": 1.508e-05,
      "loss": 0.0027,
      "step": 26190
    },
    {
      "epoch": 1.3973333333333333,
      "grad_norm": 0.09393920004367828,
      "learning_rate": 1.5066666666666668e-05,
      "loss": 0.0021,
      "step": 26200
    },
    {
      "epoch": 1.3978666666666666,
      "grad_norm": 0.6244897842407227,
      "learning_rate": 1.5053333333333334e-05,
      "loss": 0.0027,
      "step": 26210
    },
    {
      "epoch": 1.3984,
      "grad_norm": 0.1368611752986908,
      "learning_rate": 1.5040000000000002e-05,
      "loss": 0.0026,
      "step": 26220
    },
    {
      "epoch": 1.3989333333333334,
      "grad_norm": 0.2787604033946991,
      "learning_rate": 1.5026666666666666e-05,
      "loss": 0.0021,
      "step": 26230
    },
    {
      "epoch": 1.3994666666666666,
      "grad_norm": 0.5309564471244812,
      "learning_rate": 1.5013333333333334e-05,
      "loss": 0.0022,
      "step": 26240
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.1308870017528534,
      "learning_rate": 1.5e-05,
      "loss": 0.0018,
      "step": 26250
    },
    {
      "epoch": 1.4005333333333334,
      "grad_norm": 0.7886762022972107,
      "learning_rate": 1.4986666666666668e-05,
      "loss": 0.0023,
      "step": 26260
    },
    {
      "epoch": 1.4010666666666667,
      "grad_norm": 0.20313896238803864,
      "learning_rate": 1.4973333333333333e-05,
      "loss": 0.0022,
      "step": 26270
    },
    {
      "epoch": 1.4016,
      "grad_norm": 0.16208505630493164,
      "learning_rate": 1.4960000000000002e-05,
      "loss": 0.0035,
      "step": 26280
    },
    {
      "epoch": 1.4021333333333335,
      "grad_norm": 0.22245551645755768,
      "learning_rate": 1.4946666666666667e-05,
      "loss": 0.0028,
      "step": 26290
    },
    {
      "epoch": 1.4026666666666667,
      "grad_norm": 0.14637809991836548,
      "learning_rate": 1.4933333333333335e-05,
      "loss": 0.0033,
      "step": 26300
    },
    {
      "epoch": 1.4032,
      "grad_norm": 0.2266346961259842,
      "learning_rate": 1.4920000000000001e-05,
      "loss": 0.003,
      "step": 26310
    },
    {
      "epoch": 1.4037333333333333,
      "grad_norm": 0.19497354328632355,
      "learning_rate": 1.4906666666666666e-05,
      "loss": 0.0018,
      "step": 26320
    },
    {
      "epoch": 1.4042666666666666,
      "grad_norm": 0.2858600914478302,
      "learning_rate": 1.4893333333333334e-05,
      "loss": 0.0022,
      "step": 26330
    },
    {
      "epoch": 1.4048,
      "grad_norm": 0.15337517857551575,
      "learning_rate": 1.488e-05,
      "loss": 0.0018,
      "step": 26340
    },
    {
      "epoch": 1.4053333333333333,
      "grad_norm": 0.6033202409744263,
      "learning_rate": 1.4866666666666668e-05,
      "loss": 0.0021,
      "step": 26350
    },
    {
      "epoch": 1.4058666666666666,
      "grad_norm": 0.28256282210350037,
      "learning_rate": 1.4853333333333332e-05,
      "loss": 0.0025,
      "step": 26360
    },
    {
      "epoch": 1.4064,
      "grad_norm": 0.22169604897499084,
      "learning_rate": 1.4840000000000002e-05,
      "loss": 0.0027,
      "step": 26370
    },
    {
      "epoch": 1.4069333333333334,
      "grad_norm": 0.16217057406902313,
      "learning_rate": 1.4826666666666666e-05,
      "loss": 0.0017,
      "step": 26380
    },
    {
      "epoch": 1.4074666666666666,
      "grad_norm": 0.2609459161758423,
      "learning_rate": 1.4813333333333334e-05,
      "loss": 0.0027,
      "step": 26390
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.2520003318786621,
      "learning_rate": 1.48e-05,
      "loss": 0.0036,
      "step": 26400
    },
    {
      "epoch": 1.4085333333333334,
      "grad_norm": 0.364055335521698,
      "learning_rate": 1.4786666666666669e-05,
      "loss": 0.0024,
      "step": 26410
    },
    {
      "epoch": 1.4090666666666667,
      "grad_norm": 0.1572299748659134,
      "learning_rate": 1.4773333333333333e-05,
      "loss": 0.0018,
      "step": 26420
    },
    {
      "epoch": 1.4096,
      "grad_norm": 0.41460853815078735,
      "learning_rate": 1.4760000000000001e-05,
      "loss": 0.0038,
      "step": 26430
    },
    {
      "epoch": 1.4101333333333335,
      "grad_norm": 0.44202789664268494,
      "learning_rate": 1.4746666666666667e-05,
      "loss": 0.0031,
      "step": 26440
    },
    {
      "epoch": 1.4106666666666667,
      "grad_norm": 0.1275441199541092,
      "learning_rate": 1.4733333333333335e-05,
      "loss": 0.0025,
      "step": 26450
    },
    {
      "epoch": 1.4112,
      "grad_norm": 0.5282764434814453,
      "learning_rate": 1.472e-05,
      "loss": 0.0019,
      "step": 26460
    },
    {
      "epoch": 1.4117333333333333,
      "grad_norm": 0.27440381050109863,
      "learning_rate": 1.470666666666667e-05,
      "loss": 0.0018,
      "step": 26470
    },
    {
      "epoch": 1.4122666666666666,
      "grad_norm": 0.1116841733455658,
      "learning_rate": 1.4693333333333334e-05,
      "loss": 0.0026,
      "step": 26480
    },
    {
      "epoch": 1.4128,
      "grad_norm": 0.19250409305095673,
      "learning_rate": 1.4680000000000002e-05,
      "loss": 0.0019,
      "step": 26490
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 0.0803331509232521,
      "learning_rate": 1.4666666666666668e-05,
      "loss": 0.002,
      "step": 26500
    },
    {
      "epoch": 1.4138666666666666,
      "grad_norm": 0.38462039828300476,
      "learning_rate": 1.4653333333333333e-05,
      "loss": 0.0028,
      "step": 26510
    },
    {
      "epoch": 1.4144,
      "grad_norm": 0.2148044854402542,
      "learning_rate": 1.464e-05,
      "loss": 0.0022,
      "step": 26520
    },
    {
      "epoch": 1.4149333333333334,
      "grad_norm": 0.2056524008512497,
      "learning_rate": 1.4626666666666667e-05,
      "loss": 0.0026,
      "step": 26530
    },
    {
      "epoch": 1.4154666666666667,
      "grad_norm": 0.18344996869564056,
      "learning_rate": 1.4613333333333335e-05,
      "loss": 0.002,
      "step": 26540
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.3787257969379425,
      "learning_rate": 1.4599999999999999e-05,
      "loss": 0.0028,
      "step": 26550
    },
    {
      "epoch": 1.4165333333333332,
      "grad_norm": 0.5416972041130066,
      "learning_rate": 1.4586666666666669e-05,
      "loss": 0.0038,
      "step": 26560
    },
    {
      "epoch": 1.4170666666666667,
      "grad_norm": 0.1536264270544052,
      "learning_rate": 1.4573333333333333e-05,
      "loss": 0.0022,
      "step": 26570
    },
    {
      "epoch": 1.4176,
      "grad_norm": 0.5142081379890442,
      "learning_rate": 1.4560000000000001e-05,
      "loss": 0.0023,
      "step": 26580
    },
    {
      "epoch": 1.4181333333333335,
      "grad_norm": 0.1510745733976364,
      "learning_rate": 1.4546666666666667e-05,
      "loss": 0.002,
      "step": 26590
    },
    {
      "epoch": 1.4186666666666667,
      "grad_norm": 0.2615585923194885,
      "learning_rate": 1.4533333333333335e-05,
      "loss": 0.0021,
      "step": 26600
    },
    {
      "epoch": 1.4192,
      "grad_norm": 0.14227226376533508,
      "learning_rate": 1.452e-05,
      "loss": 0.0021,
      "step": 26610
    },
    {
      "epoch": 1.4197333333333333,
      "grad_norm": 0.12017020583152771,
      "learning_rate": 1.4506666666666668e-05,
      "loss": 0.0019,
      "step": 26620
    },
    {
      "epoch": 1.4202666666666666,
      "grad_norm": 0.2946554124355316,
      "learning_rate": 1.4493333333333334e-05,
      "loss": 0.003,
      "step": 26630
    },
    {
      "epoch": 1.4208,
      "grad_norm": 0.13826104998588562,
      "learning_rate": 1.4480000000000002e-05,
      "loss": 0.0018,
      "step": 26640
    },
    {
      "epoch": 1.4213333333333333,
      "grad_norm": 0.35792315006256104,
      "learning_rate": 1.4466666666666667e-05,
      "loss": 0.0022,
      "step": 26650
    },
    {
      "epoch": 1.4218666666666666,
      "grad_norm": 0.5082550048828125,
      "learning_rate": 1.4453333333333336e-05,
      "loss": 0.0031,
      "step": 26660
    },
    {
      "epoch": 1.4224,
      "grad_norm": 0.33249416947364807,
      "learning_rate": 1.444e-05,
      "loss": 0.0026,
      "step": 26670
    },
    {
      "epoch": 1.4229333333333334,
      "grad_norm": 0.2038097083568573,
      "learning_rate": 1.4426666666666667e-05,
      "loss": 0.0026,
      "step": 26680
    },
    {
      "epoch": 1.4234666666666667,
      "grad_norm": 0.36433371901512146,
      "learning_rate": 1.4413333333333335e-05,
      "loss": 0.0018,
      "step": 26690
    },
    {
      "epoch": 1.424,
      "grad_norm": 0.5280887484550476,
      "learning_rate": 1.44e-05,
      "loss": 0.0018,
      "step": 26700
    },
    {
      "epoch": 1.4245333333333332,
      "grad_norm": 0.08839023113250732,
      "learning_rate": 1.4386666666666667e-05,
      "loss": 0.0022,
      "step": 26710
    },
    {
      "epoch": 1.4250666666666667,
      "grad_norm": 0.33227866888046265,
      "learning_rate": 1.4373333333333334e-05,
      "loss": 0.0019,
      "step": 26720
    },
    {
      "epoch": 1.4256,
      "grad_norm": 0.7412234544754028,
      "learning_rate": 1.4360000000000001e-05,
      "loss": 0.0038,
      "step": 26730
    },
    {
      "epoch": 1.4261333333333333,
      "grad_norm": 0.11710678040981293,
      "learning_rate": 1.4346666666666666e-05,
      "loss": 0.0019,
      "step": 26740
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.24586504697799683,
      "learning_rate": 1.4333333333333334e-05,
      "loss": 0.0022,
      "step": 26750
    },
    {
      "epoch": 1.4272,
      "grad_norm": 0.21915437281131744,
      "learning_rate": 1.432e-05,
      "loss": 0.0021,
      "step": 26760
    },
    {
      "epoch": 1.4277333333333333,
      "grad_norm": 0.10945083945989609,
      "learning_rate": 1.4306666666666668e-05,
      "loss": 0.0031,
      "step": 26770
    },
    {
      "epoch": 1.4282666666666666,
      "grad_norm": 0.8427965044975281,
      "learning_rate": 1.4293333333333334e-05,
      "loss": 0.0037,
      "step": 26780
    },
    {
      "epoch": 1.4288,
      "grad_norm": 0.11703617870807648,
      "learning_rate": 1.4280000000000002e-05,
      "loss": 0.0029,
      "step": 26790
    },
    {
      "epoch": 1.4293333333333333,
      "grad_norm": 0.33667898178100586,
      "learning_rate": 1.4266666666666667e-05,
      "loss": 0.002,
      "step": 26800
    },
    {
      "epoch": 1.4298666666666666,
      "grad_norm": 0.476089209318161,
      "learning_rate": 1.4253333333333335e-05,
      "loss": 0.0036,
      "step": 26810
    },
    {
      "epoch": 1.4304000000000001,
      "grad_norm": 0.425371915102005,
      "learning_rate": 1.4240000000000001e-05,
      "loss": 0.0022,
      "step": 26820
    },
    {
      "epoch": 1.4309333333333334,
      "grad_norm": 0.3048432469367981,
      "learning_rate": 1.4226666666666669e-05,
      "loss": 0.0023,
      "step": 26830
    },
    {
      "epoch": 1.4314666666666667,
      "grad_norm": 0.12706033885478973,
      "learning_rate": 1.4213333333333333e-05,
      "loss": 0.0031,
      "step": 26840
    },
    {
      "epoch": 1.432,
      "grad_norm": 0.3749505579471588,
      "learning_rate": 1.42e-05,
      "loss": 0.0018,
      "step": 26850
    },
    {
      "epoch": 1.4325333333333332,
      "grad_norm": 0.2964233458042145,
      "learning_rate": 1.4186666666666667e-05,
      "loss": 0.0026,
      "step": 26860
    },
    {
      "epoch": 1.4330666666666667,
      "grad_norm": 0.2495265007019043,
      "learning_rate": 1.4173333333333334e-05,
      "loss": 0.0018,
      "step": 26870
    },
    {
      "epoch": 1.4336,
      "grad_norm": 0.2432154417037964,
      "learning_rate": 1.4160000000000002e-05,
      "loss": 0.0032,
      "step": 26880
    },
    {
      "epoch": 1.4341333333333333,
      "grad_norm": 0.09539308398962021,
      "learning_rate": 1.4146666666666666e-05,
      "loss": 0.0019,
      "step": 26890
    },
    {
      "epoch": 1.4346666666666668,
      "grad_norm": 0.4018087089061737,
      "learning_rate": 1.4133333333333334e-05,
      "loss": 0.0019,
      "step": 26900
    },
    {
      "epoch": 1.4352,
      "grad_norm": 0.17421136796474457,
      "learning_rate": 1.412e-05,
      "loss": 0.0034,
      "step": 26910
    },
    {
      "epoch": 1.4357333333333333,
      "grad_norm": 0.09822875261306763,
      "learning_rate": 1.4106666666666668e-05,
      "loss": 0.0022,
      "step": 26920
    },
    {
      "epoch": 1.4362666666666666,
      "grad_norm": 0.2585791349411011,
      "learning_rate": 1.4093333333333333e-05,
      "loss": 0.002,
      "step": 26930
    },
    {
      "epoch": 1.4368,
      "grad_norm": 0.1259932667016983,
      "learning_rate": 1.408e-05,
      "loss": 0.0018,
      "step": 26940
    },
    {
      "epoch": 1.4373333333333334,
      "grad_norm": 0.32905101776123047,
      "learning_rate": 1.4066666666666667e-05,
      "loss": 0.0021,
      "step": 26950
    },
    {
      "epoch": 1.4378666666666666,
      "grad_norm": 0.08759196847677231,
      "learning_rate": 1.4053333333333335e-05,
      "loss": 0.0025,
      "step": 26960
    },
    {
      "epoch": 1.4384000000000001,
      "grad_norm": 0.6512735486030579,
      "learning_rate": 1.4040000000000001e-05,
      "loss": 0.0017,
      "step": 26970
    },
    {
      "epoch": 1.4389333333333334,
      "grad_norm": 0.19332408905029297,
      "learning_rate": 1.4026666666666669e-05,
      "loss": 0.0025,
      "step": 26980
    },
    {
      "epoch": 1.4394666666666667,
      "grad_norm": 0.6393033266067505,
      "learning_rate": 1.4013333333333334e-05,
      "loss": 0.0018,
      "step": 26990
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.13018597662448883,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.0017,
      "step": 27000
    },
    {
      "epoch": 1.4405333333333332,
      "grad_norm": 0.8177289962768555,
      "learning_rate": 1.3986666666666668e-05,
      "loss": 0.0021,
      "step": 27010
    },
    {
      "epoch": 1.4410666666666667,
      "grad_norm": 0.5892351269721985,
      "learning_rate": 1.3973333333333332e-05,
      "loss": 0.0022,
      "step": 27020
    },
    {
      "epoch": 1.4416,
      "grad_norm": 0.3025500178337097,
      "learning_rate": 1.396e-05,
      "loss": 0.0018,
      "step": 27030
    },
    {
      "epoch": 1.4421333333333333,
      "grad_norm": 0.4149174392223358,
      "learning_rate": 1.3946666666666666e-05,
      "loss": 0.002,
      "step": 27040
    },
    {
      "epoch": 1.4426666666666668,
      "grad_norm": 0.19941166043281555,
      "learning_rate": 1.3933333333333334e-05,
      "loss": 0.0022,
      "step": 27050
    },
    {
      "epoch": 1.4432,
      "grad_norm": 0.4551226496696472,
      "learning_rate": 1.3919999999999999e-05,
      "loss": 0.0026,
      "step": 27060
    },
    {
      "epoch": 1.4437333333333333,
      "grad_norm": 0.3194306194782257,
      "learning_rate": 1.3906666666666668e-05,
      "loss": 0.0026,
      "step": 27070
    },
    {
      "epoch": 1.4442666666666666,
      "grad_norm": 0.10199693590402603,
      "learning_rate": 1.3893333333333333e-05,
      "loss": 0.0027,
      "step": 27080
    },
    {
      "epoch": 1.4447999999999999,
      "grad_norm": 0.18658243119716644,
      "learning_rate": 1.3880000000000001e-05,
      "loss": 0.0027,
      "step": 27090
    },
    {
      "epoch": 1.4453333333333334,
      "grad_norm": 0.2544919550418854,
      "learning_rate": 1.3866666666666667e-05,
      "loss": 0.0019,
      "step": 27100
    },
    {
      "epoch": 1.4458666666666666,
      "grad_norm": 0.17824670672416687,
      "learning_rate": 1.3853333333333335e-05,
      "loss": 0.0019,
      "step": 27110
    },
    {
      "epoch": 1.4464000000000001,
      "grad_norm": 0.2965768277645111,
      "learning_rate": 1.384e-05,
      "loss": 0.0029,
      "step": 27120
    },
    {
      "epoch": 1.4469333333333334,
      "grad_norm": 0.4269103407859802,
      "learning_rate": 1.3826666666666668e-05,
      "loss": 0.0024,
      "step": 27130
    },
    {
      "epoch": 1.4474666666666667,
      "grad_norm": 0.23094817996025085,
      "learning_rate": 1.3813333333333334e-05,
      "loss": 0.0019,
      "step": 27140
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.266478568315506,
      "learning_rate": 1.3800000000000002e-05,
      "loss": 0.002,
      "step": 27150
    },
    {
      "epoch": 1.4485333333333332,
      "grad_norm": 0.24628402292728424,
      "learning_rate": 1.3786666666666668e-05,
      "loss": 0.0029,
      "step": 27160
    },
    {
      "epoch": 1.4490666666666667,
      "grad_norm": 0.12246695905923843,
      "learning_rate": 1.3773333333333336e-05,
      "loss": 0.0023,
      "step": 27170
    },
    {
      "epoch": 1.4496,
      "grad_norm": 0.14263787865638733,
      "learning_rate": 1.376e-05,
      "loss": 0.0021,
      "step": 27180
    },
    {
      "epoch": 1.4501333333333333,
      "grad_norm": 0.28118830919265747,
      "learning_rate": 1.3746666666666667e-05,
      "loss": 0.0026,
      "step": 27190
    },
    {
      "epoch": 1.4506666666666668,
      "grad_norm": 0.15655359625816345,
      "learning_rate": 1.3733333333333335e-05,
      "loss": 0.0041,
      "step": 27200
    },
    {
      "epoch": 1.4512,
      "grad_norm": 0.6406981945037842,
      "learning_rate": 1.3719999999999999e-05,
      "loss": 0.003,
      "step": 27210
    },
    {
      "epoch": 1.4517333333333333,
      "grad_norm": 0.525642991065979,
      "learning_rate": 1.3706666666666667e-05,
      "loss": 0.0022,
      "step": 27220
    },
    {
      "epoch": 1.4522666666666666,
      "grad_norm": 0.2957810163497925,
      "learning_rate": 1.3693333333333333e-05,
      "loss": 0.0032,
      "step": 27230
    },
    {
      "epoch": 1.4527999999999999,
      "grad_norm": 0.0735495537519455,
      "learning_rate": 1.3680000000000001e-05,
      "loss": 0.002,
      "step": 27240
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 0.12144108861684799,
      "learning_rate": 1.3666666666666666e-05,
      "loss": 0.0033,
      "step": 27250
    },
    {
      "epoch": 1.4538666666666666,
      "grad_norm": 0.26085981726646423,
      "learning_rate": 1.3653333333333335e-05,
      "loss": 0.0026,
      "step": 27260
    },
    {
      "epoch": 1.4544000000000001,
      "grad_norm": 0.18614745140075684,
      "learning_rate": 1.364e-05,
      "loss": 0.0016,
      "step": 27270
    },
    {
      "epoch": 1.4549333333333334,
      "grad_norm": 0.797963559627533,
      "learning_rate": 1.3626666666666668e-05,
      "loss": 0.0043,
      "step": 27280
    },
    {
      "epoch": 1.4554666666666667,
      "grad_norm": 0.3158913850784302,
      "learning_rate": 1.3613333333333334e-05,
      "loss": 0.0021,
      "step": 27290
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.2586280107498169,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 0.0022,
      "step": 27300
    },
    {
      "epoch": 1.4565333333333332,
      "grad_norm": 0.15592576563358307,
      "learning_rate": 1.3586666666666666e-05,
      "loss": 0.0019,
      "step": 27310
    },
    {
      "epoch": 1.4570666666666667,
      "grad_norm": 0.36724913120269775,
      "learning_rate": 1.3573333333333334e-05,
      "loss": 0.0017,
      "step": 27320
    },
    {
      "epoch": 1.4576,
      "grad_norm": 0.07092055678367615,
      "learning_rate": 1.356e-05,
      "loss": 0.0024,
      "step": 27330
    },
    {
      "epoch": 1.4581333333333333,
      "grad_norm": 0.32680264115333557,
      "learning_rate": 1.3546666666666669e-05,
      "loss": 0.003,
      "step": 27340
    },
    {
      "epoch": 1.4586666666666668,
      "grad_norm": 0.1404963880777359,
      "learning_rate": 1.3533333333333335e-05,
      "loss": 0.0035,
      "step": 27350
    },
    {
      "epoch": 1.4592,
      "grad_norm": 0.1591881364583969,
      "learning_rate": 1.352e-05,
      "loss": 0.0021,
      "step": 27360
    },
    {
      "epoch": 1.4597333333333333,
      "grad_norm": 0.31258097290992737,
      "learning_rate": 1.3506666666666667e-05,
      "loss": 0.0025,
      "step": 27370
    },
    {
      "epoch": 1.4602666666666666,
      "grad_norm": 0.31448397040367126,
      "learning_rate": 1.3493333333333333e-05,
      "loss": 0.0025,
      "step": 27380
    },
    {
      "epoch": 1.4607999999999999,
      "grad_norm": 0.11649911105632782,
      "learning_rate": 1.3480000000000001e-05,
      "loss": 0.0023,
      "step": 27390
    },
    {
      "epoch": 1.4613333333333334,
      "grad_norm": 0.0997888445854187,
      "learning_rate": 1.3466666666666666e-05,
      "loss": 0.0021,
      "step": 27400
    },
    {
      "epoch": 1.4618666666666666,
      "grad_norm": 0.35712116956710815,
      "learning_rate": 1.3453333333333334e-05,
      "loss": 0.002,
      "step": 27410
    },
    {
      "epoch": 1.4624,
      "grad_norm": 0.29531463980674744,
      "learning_rate": 1.344e-05,
      "loss": 0.0019,
      "step": 27420
    },
    {
      "epoch": 1.4629333333333334,
      "grad_norm": 0.5673670172691345,
      "learning_rate": 1.3426666666666668e-05,
      "loss": 0.0024,
      "step": 27430
    },
    {
      "epoch": 1.4634666666666667,
      "grad_norm": 0.3138994574546814,
      "learning_rate": 1.3413333333333333e-05,
      "loss": 0.0021,
      "step": 27440
    },
    {
      "epoch": 1.464,
      "grad_norm": 0.289244145154953,
      "learning_rate": 1.3400000000000002e-05,
      "loss": 0.0027,
      "step": 27450
    },
    {
      "epoch": 1.4645333333333332,
      "grad_norm": 0.17265965044498444,
      "learning_rate": 1.3386666666666667e-05,
      "loss": 0.0024,
      "step": 27460
    },
    {
      "epoch": 1.4650666666666667,
      "grad_norm": 0.5694338083267212,
      "learning_rate": 1.3373333333333335e-05,
      "loss": 0.0031,
      "step": 27470
    },
    {
      "epoch": 1.4656,
      "grad_norm": 0.15278443694114685,
      "learning_rate": 1.336e-05,
      "loss": 0.0027,
      "step": 27480
    },
    {
      "epoch": 1.4661333333333333,
      "grad_norm": 0.3457241952419281,
      "learning_rate": 1.3346666666666669e-05,
      "loss": 0.0017,
      "step": 27490
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.08192487061023712,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.0025,
      "step": 27500
    },
    {
      "epoch": 1.4672,
      "grad_norm": 0.3857920169830322,
      "learning_rate": 1.3320000000000001e-05,
      "loss": 0.0022,
      "step": 27510
    },
    {
      "epoch": 1.4677333333333333,
      "grad_norm": 0.21799980103969574,
      "learning_rate": 1.3306666666666667e-05,
      "loss": 0.0025,
      "step": 27520
    },
    {
      "epoch": 1.4682666666666666,
      "grad_norm": 0.20288996398448944,
      "learning_rate": 1.3293333333333332e-05,
      "loss": 0.003,
      "step": 27530
    },
    {
      "epoch": 1.4687999999999999,
      "grad_norm": 0.19181716442108154,
      "learning_rate": 1.3280000000000002e-05,
      "loss": 0.002,
      "step": 27540
    },
    {
      "epoch": 1.4693333333333334,
      "grad_norm": 0.21646854281425476,
      "learning_rate": 1.3266666666666666e-05,
      "loss": 0.0034,
      "step": 27550
    },
    {
      "epoch": 1.4698666666666667,
      "grad_norm": 0.2622131109237671,
      "learning_rate": 1.3253333333333334e-05,
      "loss": 0.0018,
      "step": 27560
    },
    {
      "epoch": 1.4704,
      "grad_norm": 0.16863979399204254,
      "learning_rate": 1.324e-05,
      "loss": 0.0018,
      "step": 27570
    },
    {
      "epoch": 1.4709333333333334,
      "grad_norm": 0.10217199474573135,
      "learning_rate": 1.3226666666666668e-05,
      "loss": 0.0026,
      "step": 27580
    },
    {
      "epoch": 1.4714666666666667,
      "grad_norm": 0.324886679649353,
      "learning_rate": 1.3213333333333333e-05,
      "loss": 0.0025,
      "step": 27590
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.22426670789718628,
      "learning_rate": 1.32e-05,
      "loss": 0.0027,
      "step": 27600
    },
    {
      "epoch": 1.4725333333333332,
      "grad_norm": 0.36359819769859314,
      "learning_rate": 1.3186666666666667e-05,
      "loss": 0.0019,
      "step": 27610
    },
    {
      "epoch": 1.4730666666666667,
      "grad_norm": 0.4101548492908478,
      "learning_rate": 1.3173333333333335e-05,
      "loss": 0.0024,
      "step": 27620
    },
    {
      "epoch": 1.4736,
      "grad_norm": 0.5223154425621033,
      "learning_rate": 1.316e-05,
      "loss": 0.0027,
      "step": 27630
    },
    {
      "epoch": 1.4741333333333333,
      "grad_norm": 0.18103839457035065,
      "learning_rate": 1.3146666666666669e-05,
      "loss": 0.002,
      "step": 27640
    },
    {
      "epoch": 1.4746666666666668,
      "grad_norm": 0.5307512283325195,
      "learning_rate": 1.3133333333333334e-05,
      "loss": 0.0021,
      "step": 27650
    },
    {
      "epoch": 1.4752,
      "grad_norm": 0.32118797302246094,
      "learning_rate": 1.3120000000000001e-05,
      "loss": 0.0023,
      "step": 27660
    },
    {
      "epoch": 1.4757333333333333,
      "grad_norm": 0.11432491987943649,
      "learning_rate": 1.3106666666666668e-05,
      "loss": 0.0022,
      "step": 27670
    },
    {
      "epoch": 1.4762666666666666,
      "grad_norm": 0.4106132388114929,
      "learning_rate": 1.3093333333333336e-05,
      "loss": 0.0029,
      "step": 27680
    },
    {
      "epoch": 1.4768,
      "grad_norm": 0.5181211233139038,
      "learning_rate": 1.308e-05,
      "loss": 0.0021,
      "step": 27690
    },
    {
      "epoch": 1.4773333333333334,
      "grad_norm": 0.34712159633636475,
      "learning_rate": 1.3066666666666666e-05,
      "loss": 0.0019,
      "step": 27700
    },
    {
      "epoch": 1.4778666666666667,
      "grad_norm": 0.26484131813049316,
      "learning_rate": 1.3053333333333334e-05,
      "loss": 0.0021,
      "step": 27710
    },
    {
      "epoch": 1.4784,
      "grad_norm": 0.18749836087226868,
      "learning_rate": 1.3039999999999999e-05,
      "loss": 0.0023,
      "step": 27720
    },
    {
      "epoch": 1.4789333333333334,
      "grad_norm": 0.1638784557580948,
      "learning_rate": 1.3026666666666667e-05,
      "loss": 0.0026,
      "step": 27730
    },
    {
      "epoch": 1.4794666666666667,
      "grad_norm": 0.6971157193183899,
      "learning_rate": 1.3013333333333333e-05,
      "loss": 0.0017,
      "step": 27740
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.3928540050983429,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.0021,
      "step": 27750
    },
    {
      "epoch": 1.4805333333333333,
      "grad_norm": 0.4573948085308075,
      "learning_rate": 1.2986666666666667e-05,
      "loss": 0.0019,
      "step": 27760
    },
    {
      "epoch": 1.4810666666666665,
      "grad_norm": 0.17339904606342316,
      "learning_rate": 1.2973333333333335e-05,
      "loss": 0.0035,
      "step": 27770
    },
    {
      "epoch": 1.4816,
      "grad_norm": 0.3433365821838379,
      "learning_rate": 1.296e-05,
      "loss": 0.003,
      "step": 27780
    },
    {
      "epoch": 1.4821333333333333,
      "grad_norm": 0.5049671530723572,
      "learning_rate": 1.2946666666666668e-05,
      "loss": 0.0022,
      "step": 27790
    },
    {
      "epoch": 1.4826666666666668,
      "grad_norm": 0.08784016966819763,
      "learning_rate": 1.2933333333333334e-05,
      "loss": 0.0023,
      "step": 27800
    },
    {
      "epoch": 1.4832,
      "grad_norm": 0.3517409861087799,
      "learning_rate": 1.2920000000000002e-05,
      "loss": 0.0023,
      "step": 27810
    },
    {
      "epoch": 1.4837333333333333,
      "grad_norm": 0.27144861221313477,
      "learning_rate": 1.2906666666666666e-05,
      "loss": 0.0019,
      "step": 27820
    },
    {
      "epoch": 1.4842666666666666,
      "grad_norm": 0.14567182958126068,
      "learning_rate": 1.2893333333333336e-05,
      "loss": 0.0032,
      "step": 27830
    },
    {
      "epoch": 1.4848,
      "grad_norm": 0.337455689907074,
      "learning_rate": 1.288e-05,
      "loss": 0.0023,
      "step": 27840
    },
    {
      "epoch": 1.4853333333333334,
      "grad_norm": 0.1734938770532608,
      "learning_rate": 1.2866666666666668e-05,
      "loss": 0.0017,
      "step": 27850
    },
    {
      "epoch": 1.4858666666666667,
      "grad_norm": 0.42378631234169006,
      "learning_rate": 1.2853333333333335e-05,
      "loss": 0.0021,
      "step": 27860
    },
    {
      "epoch": 1.4864,
      "grad_norm": 0.18155527114868164,
      "learning_rate": 1.2839999999999999e-05,
      "loss": 0.0018,
      "step": 27870
    },
    {
      "epoch": 1.4869333333333334,
      "grad_norm": 0.09094152599573135,
      "learning_rate": 1.2826666666666667e-05,
      "loss": 0.0018,
      "step": 27880
    },
    {
      "epoch": 1.4874666666666667,
      "grad_norm": 0.3584176003932953,
      "learning_rate": 1.2813333333333333e-05,
      "loss": 0.0018,
      "step": 27890
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.08933216333389282,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 0.0022,
      "step": 27900
    },
    {
      "epoch": 1.4885333333333333,
      "grad_norm": 0.11493223905563354,
      "learning_rate": 1.2786666666666666e-05,
      "loss": 0.0027,
      "step": 27910
    },
    {
      "epoch": 1.4890666666666665,
      "grad_norm": 0.3630387485027313,
      "learning_rate": 1.2773333333333334e-05,
      "loss": 0.0023,
      "step": 27920
    },
    {
      "epoch": 1.4896,
      "grad_norm": 0.362333208322525,
      "learning_rate": 1.276e-05,
      "loss": 0.0033,
      "step": 27930
    },
    {
      "epoch": 1.4901333333333333,
      "grad_norm": 0.332832932472229,
      "learning_rate": 1.2746666666666668e-05,
      "loss": 0.0017,
      "step": 27940
    },
    {
      "epoch": 1.4906666666666666,
      "grad_norm": 0.32610243558883667,
      "learning_rate": 1.2733333333333334e-05,
      "loss": 0.0028,
      "step": 27950
    },
    {
      "epoch": 1.4912,
      "grad_norm": 0.41120442748069763,
      "learning_rate": 1.2720000000000002e-05,
      "loss": 0.005,
      "step": 27960
    },
    {
      "epoch": 1.4917333333333334,
      "grad_norm": 0.3744717240333557,
      "learning_rate": 1.2706666666666666e-05,
      "loss": 0.0028,
      "step": 27970
    },
    {
      "epoch": 1.4922666666666666,
      "grad_norm": 0.17337210476398468,
      "learning_rate": 1.2693333333333334e-05,
      "loss": 0.0026,
      "step": 27980
    },
    {
      "epoch": 1.4928,
      "grad_norm": 0.20307855308055878,
      "learning_rate": 1.268e-05,
      "loss": 0.0026,
      "step": 27990
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 0.12275410443544388,
      "learning_rate": 1.2666666666666668e-05,
      "loss": 0.0024,
      "step": 28000
    },
    {
      "epoch": 1.4938666666666667,
      "grad_norm": 0.1406611204147339,
      "learning_rate": 1.2653333333333333e-05,
      "loss": 0.0018,
      "step": 28010
    },
    {
      "epoch": 1.4944,
      "grad_norm": 0.2984565198421478,
      "learning_rate": 1.2640000000000003e-05,
      "loss": 0.0019,
      "step": 28020
    },
    {
      "epoch": 1.4949333333333334,
      "grad_norm": 0.38139134645462036,
      "learning_rate": 1.2626666666666667e-05,
      "loss": 0.0043,
      "step": 28030
    },
    {
      "epoch": 1.4954666666666667,
      "grad_norm": 0.21752679347991943,
      "learning_rate": 1.2613333333333332e-05,
      "loss": 0.0026,
      "step": 28040
    },
    {
      "epoch": 1.496,
      "grad_norm": 0.22746840119361877,
      "learning_rate": 1.2600000000000001e-05,
      "loss": 0.0019,
      "step": 28050
    },
    {
      "epoch": 1.4965333333333333,
      "grad_norm": 0.21926264464855194,
      "learning_rate": 1.2586666666666666e-05,
      "loss": 0.0027,
      "step": 28060
    },
    {
      "epoch": 1.4970666666666665,
      "grad_norm": 0.166436105966568,
      "learning_rate": 1.2573333333333334e-05,
      "loss": 0.0036,
      "step": 28070
    },
    {
      "epoch": 1.4976,
      "grad_norm": 0.2976562976837158,
      "learning_rate": 1.256e-05,
      "loss": 0.0021,
      "step": 28080
    },
    {
      "epoch": 1.4981333333333333,
      "grad_norm": 0.42955508828163147,
      "learning_rate": 1.2546666666666668e-05,
      "loss": 0.0021,
      "step": 28090
    },
    {
      "epoch": 1.4986666666666666,
      "grad_norm": 0.1192828118801117,
      "learning_rate": 1.2533333333333332e-05,
      "loss": 0.002,
      "step": 28100
    },
    {
      "epoch": 1.4992,
      "grad_norm": 0.2138148695230484,
      "learning_rate": 1.252e-05,
      "loss": 0.0022,
      "step": 28110
    },
    {
      "epoch": 1.4997333333333334,
      "grad_norm": 0.297272652387619,
      "learning_rate": 1.2506666666666667e-05,
      "loss": 0.0023,
      "step": 28120
    },
    {
      "epoch": 1.5002666666666666,
      "grad_norm": 0.3242250680923462,
      "learning_rate": 1.2493333333333333e-05,
      "loss": 0.0029,
      "step": 28130
    },
    {
      "epoch": 1.5008,
      "grad_norm": 0.3100275695323944,
      "learning_rate": 1.248e-05,
      "loss": 0.0021,
      "step": 28140
    },
    {
      "epoch": 1.5013333333333332,
      "grad_norm": 0.21438057720661163,
      "learning_rate": 1.2466666666666667e-05,
      "loss": 0.0018,
      "step": 28150
    },
    {
      "epoch": 1.5018666666666667,
      "grad_norm": 0.10016295313835144,
      "learning_rate": 1.2453333333333333e-05,
      "loss": 0.0022,
      "step": 28160
    },
    {
      "epoch": 1.5024,
      "grad_norm": 0.06525041162967682,
      "learning_rate": 1.244e-05,
      "loss": 0.0021,
      "step": 28170
    },
    {
      "epoch": 1.5029333333333335,
      "grad_norm": 0.09071166068315506,
      "learning_rate": 1.2426666666666667e-05,
      "loss": 0.0022,
      "step": 28180
    },
    {
      "epoch": 1.5034666666666667,
      "grad_norm": 0.22644275426864624,
      "learning_rate": 1.2413333333333334e-05,
      "loss": 0.0031,
      "step": 28190
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.391558974981308,
      "learning_rate": 1.24e-05,
      "loss": 0.0017,
      "step": 28200
    },
    {
      "epoch": 1.5045333333333333,
      "grad_norm": 0.1898128241300583,
      "learning_rate": 1.2386666666666668e-05,
      "loss": 0.0023,
      "step": 28210
    },
    {
      "epoch": 1.5050666666666666,
      "grad_norm": 0.229576975107193,
      "learning_rate": 1.2373333333333334e-05,
      "loss": 0.0025,
      "step": 28220
    },
    {
      "epoch": 1.5056,
      "grad_norm": 0.5896382927894592,
      "learning_rate": 1.236e-05,
      "loss": 0.0026,
      "step": 28230
    },
    {
      "epoch": 1.5061333333333333,
      "grad_norm": 0.0965440571308136,
      "learning_rate": 1.2346666666666668e-05,
      "loss": 0.0041,
      "step": 28240
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 0.23753440380096436,
      "learning_rate": 1.2333333333333334e-05,
      "loss": 0.0021,
      "step": 28250
    },
    {
      "epoch": 1.5072,
      "grad_norm": 0.5990500450134277,
      "learning_rate": 1.232e-05,
      "loss": 0.002,
      "step": 28260
    },
    {
      "epoch": 1.5077333333333334,
      "grad_norm": 0.6123724579811096,
      "learning_rate": 1.2306666666666669e-05,
      "loss": 0.0019,
      "step": 28270
    },
    {
      "epoch": 1.5082666666666666,
      "grad_norm": 0.14891593158245087,
      "learning_rate": 1.2293333333333335e-05,
      "loss": 0.0022,
      "step": 28280
    },
    {
      "epoch": 1.5088,
      "grad_norm": 0.2659778296947479,
      "learning_rate": 1.2280000000000001e-05,
      "loss": 0.0024,
      "step": 28290
    },
    {
      "epoch": 1.5093333333333332,
      "grad_norm": 0.11336227506399155,
      "learning_rate": 1.2266666666666667e-05,
      "loss": 0.003,
      "step": 28300
    },
    {
      "epoch": 1.5098666666666667,
      "grad_norm": 0.3942398726940155,
      "learning_rate": 1.2253333333333333e-05,
      "loss": 0.002,
      "step": 28310
    },
    {
      "epoch": 1.5104,
      "grad_norm": 0.21874454617500305,
      "learning_rate": 1.224e-05,
      "loss": 0.0032,
      "step": 28320
    },
    {
      "epoch": 1.5109333333333335,
      "grad_norm": 0.32610195875167847,
      "learning_rate": 1.2226666666666668e-05,
      "loss": 0.0029,
      "step": 28330
    },
    {
      "epoch": 1.5114666666666667,
      "grad_norm": 0.1610783338546753,
      "learning_rate": 1.2213333333333334e-05,
      "loss": 0.0022,
      "step": 28340
    },
    {
      "epoch": 1.512,
      "grad_norm": 0.4169769883155823,
      "learning_rate": 1.22e-05,
      "loss": 0.0028,
      "step": 28350
    },
    {
      "epoch": 1.5125333333333333,
      "grad_norm": 0.19614598155021667,
      "learning_rate": 1.2186666666666666e-05,
      "loss": 0.0033,
      "step": 28360
    },
    {
      "epoch": 1.5130666666666666,
      "grad_norm": 0.10962191224098206,
      "learning_rate": 1.2173333333333334e-05,
      "loss": 0.0042,
      "step": 28370
    },
    {
      "epoch": 1.5135999999999998,
      "grad_norm": 0.29786351323127747,
      "learning_rate": 1.216e-05,
      "loss": 0.0021,
      "step": 28380
    },
    {
      "epoch": 1.5141333333333333,
      "grad_norm": 0.32190999388694763,
      "learning_rate": 1.2146666666666667e-05,
      "loss": 0.0036,
      "step": 28390
    },
    {
      "epoch": 1.5146666666666668,
      "grad_norm": 0.27631163597106934,
      "learning_rate": 1.2133333333333335e-05,
      "loss": 0.0052,
      "step": 28400
    },
    {
      "epoch": 1.5152,
      "grad_norm": 0.3593563139438629,
      "learning_rate": 1.2120000000000001e-05,
      "loss": 0.0018,
      "step": 28410
    },
    {
      "epoch": 1.5157333333333334,
      "grad_norm": 0.5801987648010254,
      "learning_rate": 1.2106666666666667e-05,
      "loss": 0.0021,
      "step": 28420
    },
    {
      "epoch": 1.5162666666666667,
      "grad_norm": 0.5002005696296692,
      "learning_rate": 1.2093333333333335e-05,
      "loss": 0.0019,
      "step": 28430
    },
    {
      "epoch": 1.5168,
      "grad_norm": 0.17475558817386627,
      "learning_rate": 1.2080000000000001e-05,
      "loss": 0.0023,
      "step": 28440
    },
    {
      "epoch": 1.5173333333333332,
      "grad_norm": 0.36508068442344666,
      "learning_rate": 1.2066666666666667e-05,
      "loss": 0.0025,
      "step": 28450
    },
    {
      "epoch": 1.5178666666666667,
      "grad_norm": 0.19104330241680145,
      "learning_rate": 1.2053333333333334e-05,
      "loss": 0.0028,
      "step": 28460
    },
    {
      "epoch": 1.5184,
      "grad_norm": 0.36094939708709717,
      "learning_rate": 1.204e-05,
      "loss": 0.0029,
      "step": 28470
    },
    {
      "epoch": 1.5189333333333335,
      "grad_norm": 0.647443950176239,
      "learning_rate": 1.2026666666666666e-05,
      "loss": 0.0032,
      "step": 28480
    },
    {
      "epoch": 1.5194666666666667,
      "grad_norm": 0.11980783939361572,
      "learning_rate": 1.2013333333333334e-05,
      "loss": 0.0018,
      "step": 28490
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.6713298559188843,
      "learning_rate": 1.2e-05,
      "loss": 0.003,
      "step": 28500
    },
    {
      "epoch": 1.5205333333333333,
      "grad_norm": 0.40442249178886414,
      "learning_rate": 1.1986666666666667e-05,
      "loss": 0.0022,
      "step": 28510
    },
    {
      "epoch": 1.5210666666666666,
      "grad_norm": 0.8944718837738037,
      "learning_rate": 1.1973333333333334e-05,
      "loss": 0.0022,
      "step": 28520
    },
    {
      "epoch": 1.5215999999999998,
      "grad_norm": 0.30196714401245117,
      "learning_rate": 1.196e-05,
      "loss": 0.0025,
      "step": 28530
    },
    {
      "epoch": 1.5221333333333333,
      "grad_norm": 0.5289252400398254,
      "learning_rate": 1.1946666666666667e-05,
      "loss": 0.0022,
      "step": 28540
    },
    {
      "epoch": 1.5226666666666666,
      "grad_norm": 0.8906448483467102,
      "learning_rate": 1.1933333333333333e-05,
      "loss": 0.0032,
      "step": 28550
    },
    {
      "epoch": 1.5232,
      "grad_norm": 1.085928201675415,
      "learning_rate": 1.1920000000000001e-05,
      "loss": 0.0022,
      "step": 28560
    },
    {
      "epoch": 1.5237333333333334,
      "grad_norm": 0.31330883502960205,
      "learning_rate": 1.1906666666666667e-05,
      "loss": 0.002,
      "step": 28570
    },
    {
      "epoch": 1.5242666666666667,
      "grad_norm": 0.5992354154586792,
      "learning_rate": 1.1893333333333334e-05,
      "loss": 0.0022,
      "step": 28580
    },
    {
      "epoch": 1.5248,
      "grad_norm": 0.43913841247558594,
      "learning_rate": 1.1880000000000001e-05,
      "loss": 0.0018,
      "step": 28590
    },
    {
      "epoch": 1.5253333333333332,
      "grad_norm": 0.14267350733280182,
      "learning_rate": 1.1866666666666668e-05,
      "loss": 0.0018,
      "step": 28600
    },
    {
      "epoch": 1.5258666666666667,
      "grad_norm": 0.20569604635238647,
      "learning_rate": 1.1853333333333334e-05,
      "loss": 0.0028,
      "step": 28610
    },
    {
      "epoch": 1.5264,
      "grad_norm": 0.37782084941864014,
      "learning_rate": 1.1840000000000002e-05,
      "loss": 0.0026,
      "step": 28620
    },
    {
      "epoch": 1.5269333333333335,
      "grad_norm": 0.34472283720970154,
      "learning_rate": 1.1826666666666668e-05,
      "loss": 0.0019,
      "step": 28630
    },
    {
      "epoch": 1.5274666666666668,
      "grad_norm": 0.1130773201584816,
      "learning_rate": 1.1813333333333334e-05,
      "loss": 0.0017,
      "step": 28640
    },
    {
      "epoch": 1.528,
      "grad_norm": 0.49530428647994995,
      "learning_rate": 1.18e-05,
      "loss": 0.0015,
      "step": 28650
    },
    {
      "epoch": 1.5285333333333333,
      "grad_norm": 0.08657463639974594,
      "learning_rate": 1.1786666666666667e-05,
      "loss": 0.0034,
      "step": 28660
    },
    {
      "epoch": 1.5290666666666666,
      "grad_norm": 0.26260077953338623,
      "learning_rate": 1.1773333333333333e-05,
      "loss": 0.0025,
      "step": 28670
    },
    {
      "epoch": 1.5295999999999998,
      "grad_norm": 0.24562175571918488,
      "learning_rate": 1.1760000000000001e-05,
      "loss": 0.0025,
      "step": 28680
    },
    {
      "epoch": 1.5301333333333333,
      "grad_norm": 0.32583385705947876,
      "learning_rate": 1.1746666666666667e-05,
      "loss": 0.0031,
      "step": 28690
    },
    {
      "epoch": 1.5306666666666666,
      "grad_norm": 0.23202233016490936,
      "learning_rate": 1.1733333333333333e-05,
      "loss": 0.0023,
      "step": 28700
    },
    {
      "epoch": 1.5312000000000001,
      "grad_norm": 0.43419042229652405,
      "learning_rate": 1.172e-05,
      "loss": 0.002,
      "step": 28710
    },
    {
      "epoch": 1.5317333333333334,
      "grad_norm": 0.5088838338851929,
      "learning_rate": 1.1706666666666668e-05,
      "loss": 0.0028,
      "step": 28720
    },
    {
      "epoch": 1.5322666666666667,
      "grad_norm": 0.5886844992637634,
      "learning_rate": 1.1693333333333334e-05,
      "loss": 0.0021,
      "step": 28730
    },
    {
      "epoch": 1.5328,
      "grad_norm": 0.21705710887908936,
      "learning_rate": 1.168e-05,
      "loss": 0.0023,
      "step": 28740
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.3493143320083618,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 0.0041,
      "step": 28750
    },
    {
      "epoch": 1.5338666666666667,
      "grad_norm": 0.19897817075252533,
      "learning_rate": 1.1653333333333334e-05,
      "loss": 0.0018,
      "step": 28760
    },
    {
      "epoch": 1.5344,
      "grad_norm": 0.1466827094554901,
      "learning_rate": 1.164e-05,
      "loss": 0.0039,
      "step": 28770
    },
    {
      "epoch": 1.5349333333333335,
      "grad_norm": 0.31148475408554077,
      "learning_rate": 1.1626666666666668e-05,
      "loss": 0.0022,
      "step": 28780
    },
    {
      "epoch": 1.5354666666666668,
      "grad_norm": 0.1652618646621704,
      "learning_rate": 1.1613333333333335e-05,
      "loss": 0.0017,
      "step": 28790
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.44098204374313354,
      "learning_rate": 1.16e-05,
      "loss": 0.0026,
      "step": 28800
    },
    {
      "epoch": 1.5365333333333333,
      "grad_norm": 0.7595064043998718,
      "learning_rate": 1.1586666666666669e-05,
      "loss": 0.0022,
      "step": 28810
    },
    {
      "epoch": 1.5370666666666666,
      "grad_norm": 0.09403590112924576,
      "learning_rate": 1.1573333333333333e-05,
      "loss": 0.0022,
      "step": 28820
    },
    {
      "epoch": 1.5375999999999999,
      "grad_norm": 0.21063075959682465,
      "learning_rate": 1.156e-05,
      "loss": 0.0021,
      "step": 28830
    },
    {
      "epoch": 1.5381333333333334,
      "grad_norm": 0.1656998097896576,
      "learning_rate": 1.1546666666666667e-05,
      "loss": 0.0022,
      "step": 28840
    },
    {
      "epoch": 1.5386666666666666,
      "grad_norm": 0.561284601688385,
      "learning_rate": 1.1533333333333334e-05,
      "loss": 0.0035,
      "step": 28850
    },
    {
      "epoch": 1.5392000000000001,
      "grad_norm": 0.09112269431352615,
      "learning_rate": 1.152e-05,
      "loss": 0.0019,
      "step": 28860
    },
    {
      "epoch": 1.5397333333333334,
      "grad_norm": 0.7841687202453613,
      "learning_rate": 1.1506666666666668e-05,
      "loss": 0.002,
      "step": 28870
    },
    {
      "epoch": 1.5402666666666667,
      "grad_norm": 0.2747747004032135,
      "learning_rate": 1.1493333333333334e-05,
      "loss": 0.0019,
      "step": 28880
    },
    {
      "epoch": 1.5408,
      "grad_norm": 0.06060048192739487,
      "learning_rate": 1.148e-05,
      "loss": 0.0029,
      "step": 28890
    },
    {
      "epoch": 1.5413333333333332,
      "grad_norm": 0.31459900736808777,
      "learning_rate": 1.1466666666666666e-05,
      "loss": 0.0019,
      "step": 28900
    },
    {
      "epoch": 1.5418666666666667,
      "grad_norm": 0.20314756035804749,
      "learning_rate": 1.1453333333333334e-05,
      "loss": 0.0028,
      "step": 28910
    },
    {
      "epoch": 1.5424,
      "grad_norm": 0.3811253607273102,
      "learning_rate": 1.144e-05,
      "loss": 0.0018,
      "step": 28920
    },
    {
      "epoch": 1.5429333333333335,
      "grad_norm": 0.08856645971536636,
      "learning_rate": 1.1426666666666667e-05,
      "loss": 0.002,
      "step": 28930
    },
    {
      "epoch": 1.5434666666666668,
      "grad_norm": 0.09181061387062073,
      "learning_rate": 1.1413333333333335e-05,
      "loss": 0.0021,
      "step": 28940
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.10472242534160614,
      "learning_rate": 1.1400000000000001e-05,
      "loss": 0.002,
      "step": 28950
    },
    {
      "epoch": 1.5445333333333333,
      "grad_norm": 0.09879428893327713,
      "learning_rate": 1.1386666666666667e-05,
      "loss": 0.002,
      "step": 28960
    },
    {
      "epoch": 1.5450666666666666,
      "grad_norm": 0.06556453555822372,
      "learning_rate": 1.1373333333333335e-05,
      "loss": 0.0016,
      "step": 28970
    },
    {
      "epoch": 1.5455999999999999,
      "grad_norm": 0.10426943004131317,
      "learning_rate": 1.1360000000000001e-05,
      "loss": 0.003,
      "step": 28980
    },
    {
      "epoch": 1.5461333333333334,
      "grad_norm": 0.4925178289413452,
      "learning_rate": 1.1346666666666666e-05,
      "loss": 0.0027,
      "step": 28990
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 0.25149667263031006,
      "learning_rate": 1.1333333333333334e-05,
      "loss": 0.0026,
      "step": 29000
    },
    {
      "epoch": 1.5472000000000001,
      "grad_norm": 0.6443496346473694,
      "learning_rate": 1.132e-05,
      "loss": 0.003,
      "step": 29010
    },
    {
      "epoch": 1.5477333333333334,
      "grad_norm": 0.2620876729488373,
      "learning_rate": 1.1306666666666666e-05,
      "loss": 0.002,
      "step": 29020
    },
    {
      "epoch": 1.5482666666666667,
      "grad_norm": 0.30568447709083557,
      "learning_rate": 1.1293333333333334e-05,
      "loss": 0.0022,
      "step": 29030
    },
    {
      "epoch": 1.5488,
      "grad_norm": 0.2230764925479889,
      "learning_rate": 1.128e-05,
      "loss": 0.0025,
      "step": 29040
    },
    {
      "epoch": 1.5493333333333332,
      "grad_norm": 0.24913464486598969,
      "learning_rate": 1.1266666666666667e-05,
      "loss": 0.0019,
      "step": 29050
    },
    {
      "epoch": 1.5498666666666665,
      "grad_norm": 0.1723455786705017,
      "learning_rate": 1.1253333333333335e-05,
      "loss": 0.0019,
      "step": 29060
    },
    {
      "epoch": 1.5504,
      "grad_norm": 0.17145198583602905,
      "learning_rate": 1.124e-05,
      "loss": 0.0026,
      "step": 29070
    },
    {
      "epoch": 1.5509333333333335,
      "grad_norm": 0.3705679476261139,
      "learning_rate": 1.1226666666666667e-05,
      "loss": 0.0035,
      "step": 29080
    },
    {
      "epoch": 1.5514666666666668,
      "grad_norm": 0.22327150404453278,
      "learning_rate": 1.1213333333333333e-05,
      "loss": 0.0027,
      "step": 29090
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.31923025846481323,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 0.0018,
      "step": 29100
    },
    {
      "epoch": 1.5525333333333333,
      "grad_norm": 0.32500943541526794,
      "learning_rate": 1.1186666666666667e-05,
      "loss": 0.002,
      "step": 29110
    },
    {
      "epoch": 1.5530666666666666,
      "grad_norm": 0.1491907387971878,
      "learning_rate": 1.1173333333333334e-05,
      "loss": 0.002,
      "step": 29120
    },
    {
      "epoch": 1.5535999999999999,
      "grad_norm": 0.46809646487236023,
      "learning_rate": 1.1160000000000002e-05,
      "loss": 0.0021,
      "step": 29130
    },
    {
      "epoch": 1.5541333333333334,
      "grad_norm": 0.24393519759178162,
      "learning_rate": 1.1146666666666668e-05,
      "loss": 0.002,
      "step": 29140
    },
    {
      "epoch": 1.5546666666666666,
      "grad_norm": 0.2585757374763489,
      "learning_rate": 1.1133333333333334e-05,
      "loss": 0.0039,
      "step": 29150
    },
    {
      "epoch": 1.5552000000000001,
      "grad_norm": 0.12970030307769775,
      "learning_rate": 1.112e-05,
      "loss": 0.0026,
      "step": 29160
    },
    {
      "epoch": 1.5557333333333334,
      "grad_norm": 0.16849137842655182,
      "learning_rate": 1.1106666666666666e-05,
      "loss": 0.0038,
      "step": 29170
    },
    {
      "epoch": 1.5562666666666667,
      "grad_norm": 0.7542933821678162,
      "learning_rate": 1.1093333333333333e-05,
      "loss": 0.0028,
      "step": 29180
    },
    {
      "epoch": 1.5568,
      "grad_norm": 0.25176429748535156,
      "learning_rate": 1.108e-05,
      "loss": 0.0019,
      "step": 29190
    },
    {
      "epoch": 1.5573333333333332,
      "grad_norm": 0.4167853593826294,
      "learning_rate": 1.1066666666666667e-05,
      "loss": 0.002,
      "step": 29200
    },
    {
      "epoch": 1.5578666666666665,
      "grad_norm": 0.09090188890695572,
      "learning_rate": 1.1053333333333333e-05,
      "loss": 0.0024,
      "step": 29210
    },
    {
      "epoch": 1.5584,
      "grad_norm": 0.12847958505153656,
      "learning_rate": 1.1040000000000001e-05,
      "loss": 0.0018,
      "step": 29220
    },
    {
      "epoch": 1.5589333333333333,
      "grad_norm": 0.5541468262672424,
      "learning_rate": 1.1026666666666667e-05,
      "loss": 0.004,
      "step": 29230
    },
    {
      "epoch": 1.5594666666666668,
      "grad_norm": 0.10226840525865555,
      "learning_rate": 1.1013333333333333e-05,
      "loss": 0.0027,
      "step": 29240
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.08501368761062622,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.0021,
      "step": 29250
    },
    {
      "epoch": 1.5605333333333333,
      "grad_norm": 0.23448942601680756,
      "learning_rate": 1.0986666666666668e-05,
      "loss": 0.0017,
      "step": 29260
    },
    {
      "epoch": 1.5610666666666666,
      "grad_norm": 0.2888320982456207,
      "learning_rate": 1.0973333333333334e-05,
      "loss": 0.0029,
      "step": 29270
    },
    {
      "epoch": 1.5615999999999999,
      "grad_norm": 0.3672139644622803,
      "learning_rate": 1.096e-05,
      "loss": 0.0034,
      "step": 29280
    },
    {
      "epoch": 1.5621333333333334,
      "grad_norm": 0.36003413796424866,
      "learning_rate": 1.0946666666666668e-05,
      "loss": 0.0025,
      "step": 29290
    },
    {
      "epoch": 1.5626666666666666,
      "grad_norm": 0.18460482358932495,
      "learning_rate": 1.0933333333333334e-05,
      "loss": 0.0026,
      "step": 29300
    },
    {
      "epoch": 1.5632000000000001,
      "grad_norm": 0.48047345876693726,
      "learning_rate": 1.092e-05,
      "loss": 0.0029,
      "step": 29310
    },
    {
      "epoch": 1.5637333333333334,
      "grad_norm": 0.1770387589931488,
      "learning_rate": 1.0906666666666668e-05,
      "loss": 0.0021,
      "step": 29320
    },
    {
      "epoch": 1.5642666666666667,
      "grad_norm": 0.45525306463241577,
      "learning_rate": 1.0893333333333333e-05,
      "loss": 0.0028,
      "step": 29330
    },
    {
      "epoch": 1.5648,
      "grad_norm": 0.10955410450696945,
      "learning_rate": 1.088e-05,
      "loss": 0.0028,
      "step": 29340
    },
    {
      "epoch": 1.5653333333333332,
      "grad_norm": 0.10110735893249512,
      "learning_rate": 1.0866666666666667e-05,
      "loss": 0.0025,
      "step": 29350
    },
    {
      "epoch": 1.5658666666666665,
      "grad_norm": 0.3108200430870056,
      "learning_rate": 1.0853333333333333e-05,
      "loss": 0.0019,
      "step": 29360
    },
    {
      "epoch": 1.5664,
      "grad_norm": 0.5397818684577942,
      "learning_rate": 1.084e-05,
      "loss": 0.0026,
      "step": 29370
    },
    {
      "epoch": 1.5669333333333333,
      "grad_norm": 0.11088152229785919,
      "learning_rate": 1.0826666666666667e-05,
      "loss": 0.0032,
      "step": 29380
    },
    {
      "epoch": 1.5674666666666668,
      "grad_norm": 0.48727157711982727,
      "learning_rate": 1.0813333333333334e-05,
      "loss": 0.0027,
      "step": 29390
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.2123180329799652,
      "learning_rate": 1.08e-05,
      "loss": 0.0017,
      "step": 29400
    },
    {
      "epoch": 1.5685333333333333,
      "grad_norm": 0.20825080573558807,
      "learning_rate": 1.0786666666666668e-05,
      "loss": 0.002,
      "step": 29410
    },
    {
      "epoch": 1.5690666666666666,
      "grad_norm": 0.10517671704292297,
      "learning_rate": 1.0773333333333334e-05,
      "loss": 0.0021,
      "step": 29420
    },
    {
      "epoch": 1.5695999999999999,
      "grad_norm": 0.08328809589147568,
      "learning_rate": 1.076e-05,
      "loss": 0.0022,
      "step": 29430
    },
    {
      "epoch": 1.5701333333333334,
      "grad_norm": 0.38620269298553467,
      "learning_rate": 1.0746666666666667e-05,
      "loss": 0.0021,
      "step": 29440
    },
    {
      "epoch": 1.5706666666666667,
      "grad_norm": 0.13113002479076385,
      "learning_rate": 1.0733333333333334e-05,
      "loss": 0.0024,
      "step": 29450
    },
    {
      "epoch": 1.5712000000000002,
      "grad_norm": 0.4902915358543396,
      "learning_rate": 1.072e-05,
      "loss": 0.003,
      "step": 29460
    },
    {
      "epoch": 1.5717333333333334,
      "grad_norm": 0.4852544963359833,
      "learning_rate": 1.0706666666666667e-05,
      "loss": 0.0026,
      "step": 29470
    },
    {
      "epoch": 1.5722666666666667,
      "grad_norm": 0.3508484363555908,
      "learning_rate": 1.0693333333333335e-05,
      "loss": 0.0025,
      "step": 29480
    },
    {
      "epoch": 1.5728,
      "grad_norm": 0.1310531049966812,
      "learning_rate": 1.0680000000000001e-05,
      "loss": 0.0027,
      "step": 29490
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 0.1575043648481369,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 0.0017,
      "step": 29500
    },
    {
      "epoch": 1.5738666666666665,
      "grad_norm": 0.36405304074287415,
      "learning_rate": 1.0653333333333334e-05,
      "loss": 0.0022,
      "step": 29510
    },
    {
      "epoch": 1.5744,
      "grad_norm": 0.3582783043384552,
      "learning_rate": 1.064e-05,
      "loss": 0.0028,
      "step": 29520
    },
    {
      "epoch": 1.5749333333333333,
      "grad_norm": 0.2451915442943573,
      "learning_rate": 1.0626666666666666e-05,
      "loss": 0.0031,
      "step": 29530
    },
    {
      "epoch": 1.5754666666666668,
      "grad_norm": 0.10859400033950806,
      "learning_rate": 1.0613333333333334e-05,
      "loss": 0.0025,
      "step": 29540
    },
    {
      "epoch": 1.576,
      "grad_norm": 0.2224256843328476,
      "learning_rate": 1.06e-05,
      "loss": 0.0032,
      "step": 29550
    },
    {
      "epoch": 1.5765333333333333,
      "grad_norm": 0.2503620982170105,
      "learning_rate": 1.0586666666666666e-05,
      "loss": 0.0017,
      "step": 29560
    },
    {
      "epoch": 1.5770666666666666,
      "grad_norm": 0.21485166251659393,
      "learning_rate": 1.0573333333333334e-05,
      "loss": 0.0019,
      "step": 29570
    },
    {
      "epoch": 1.5776,
      "grad_norm": 0.08740578591823578,
      "learning_rate": 1.056e-05,
      "loss": 0.0018,
      "step": 29580
    },
    {
      "epoch": 1.5781333333333334,
      "grad_norm": 0.1769062876701355,
      "learning_rate": 1.0546666666666667e-05,
      "loss": 0.0023,
      "step": 29590
    },
    {
      "epoch": 1.5786666666666667,
      "grad_norm": 0.35515984892845154,
      "learning_rate": 1.0533333333333335e-05,
      "loss": 0.0021,
      "step": 29600
    },
    {
      "epoch": 1.5792000000000002,
      "grad_norm": 0.18626615405082703,
      "learning_rate": 1.0520000000000001e-05,
      "loss": 0.0019,
      "step": 29610
    },
    {
      "epoch": 1.5797333333333334,
      "grad_norm": 0.09781188517808914,
      "learning_rate": 1.0506666666666667e-05,
      "loss": 0.0028,
      "step": 29620
    },
    {
      "epoch": 1.5802666666666667,
      "grad_norm": 0.3288194239139557,
      "learning_rate": 1.0493333333333333e-05,
      "loss": 0.0018,
      "step": 29630
    },
    {
      "epoch": 1.5808,
      "grad_norm": 0.152261421084404,
      "learning_rate": 1.0480000000000001e-05,
      "loss": 0.0019,
      "step": 29640
    },
    {
      "epoch": 1.5813333333333333,
      "grad_norm": 0.6643354296684265,
      "learning_rate": 1.0466666666666668e-05,
      "loss": 0.0033,
      "step": 29650
    },
    {
      "epoch": 1.5818666666666665,
      "grad_norm": 0.13581836223602295,
      "learning_rate": 1.0453333333333334e-05,
      "loss": 0.0028,
      "step": 29660
    },
    {
      "epoch": 1.5824,
      "grad_norm": 0.41797003149986267,
      "learning_rate": 1.0440000000000002e-05,
      "loss": 0.0023,
      "step": 29670
    },
    {
      "epoch": 1.5829333333333333,
      "grad_norm": 0.3236972689628601,
      "learning_rate": 1.0426666666666666e-05,
      "loss": 0.0021,
      "step": 29680
    },
    {
      "epoch": 1.5834666666666668,
      "grad_norm": 0.3368678092956543,
      "learning_rate": 1.0413333333333332e-05,
      "loss": 0.0021,
      "step": 29690
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.3843645453453064,
      "learning_rate": 1.04e-05,
      "loss": 0.0028,
      "step": 29700
    },
    {
      "epoch": 1.5845333333333333,
      "grad_norm": 0.0708908885717392,
      "learning_rate": 1.0386666666666667e-05,
      "loss": 0.0031,
      "step": 29710
    },
    {
      "epoch": 1.5850666666666666,
      "grad_norm": 0.5938105583190918,
      "learning_rate": 1.0373333333333333e-05,
      "loss": 0.0018,
      "step": 29720
    },
    {
      "epoch": 1.5856,
      "grad_norm": 0.08860193938016891,
      "learning_rate": 1.036e-05,
      "loss": 0.0018,
      "step": 29730
    },
    {
      "epoch": 1.5861333333333332,
      "grad_norm": 0.23039408028125763,
      "learning_rate": 1.0346666666666667e-05,
      "loss": 0.0034,
      "step": 29740
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 0.22465352714061737,
      "learning_rate": 1.0333333333333333e-05,
      "loss": 0.0023,
      "step": 29750
    },
    {
      "epoch": 1.5872000000000002,
      "grad_norm": 0.1370939016342163,
      "learning_rate": 1.0320000000000001e-05,
      "loss": 0.0024,
      "step": 29760
    },
    {
      "epoch": 1.5877333333333334,
      "grad_norm": 0.22127263247966766,
      "learning_rate": 1.0306666666666667e-05,
      "loss": 0.0021,
      "step": 29770
    },
    {
      "epoch": 1.5882666666666667,
      "grad_norm": 0.38859739899635315,
      "learning_rate": 1.0293333333333334e-05,
      "loss": 0.0022,
      "step": 29780
    },
    {
      "epoch": 1.5888,
      "grad_norm": 0.7053607702255249,
      "learning_rate": 1.0280000000000002e-05,
      "loss": 0.0018,
      "step": 29790
    },
    {
      "epoch": 1.5893333333333333,
      "grad_norm": 0.23558154702186584,
      "learning_rate": 1.0266666666666668e-05,
      "loss": 0.003,
      "step": 29800
    },
    {
      "epoch": 1.5898666666666665,
      "grad_norm": 0.109200619161129,
      "learning_rate": 1.0253333333333334e-05,
      "loss": 0.0042,
      "step": 29810
    },
    {
      "epoch": 1.5904,
      "grad_norm": 0.32233983278274536,
      "learning_rate": 1.024e-05,
      "loss": 0.0018,
      "step": 29820
    },
    {
      "epoch": 1.5909333333333333,
      "grad_norm": 0.22942712903022766,
      "learning_rate": 1.0226666666666668e-05,
      "loss": 0.002,
      "step": 29830
    },
    {
      "epoch": 1.5914666666666668,
      "grad_norm": 0.10951200872659683,
      "learning_rate": 1.0213333333333334e-05,
      "loss": 0.003,
      "step": 29840
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.09225119650363922,
      "learning_rate": 1.02e-05,
      "loss": 0.0023,
      "step": 29850
    },
    {
      "epoch": 1.5925333333333334,
      "grad_norm": 0.09964480996131897,
      "learning_rate": 1.0186666666666667e-05,
      "loss": 0.0021,
      "step": 29860
    },
    {
      "epoch": 1.5930666666666666,
      "grad_norm": 0.2571634352207184,
      "learning_rate": 1.0173333333333333e-05,
      "loss": 0.004,
      "step": 29870
    },
    {
      "epoch": 1.5936,
      "grad_norm": 0.13133591413497925,
      "learning_rate": 1.016e-05,
      "loss": 0.0031,
      "step": 29880
    },
    {
      "epoch": 1.5941333333333332,
      "grad_norm": 0.16765408217906952,
      "learning_rate": 1.0146666666666667e-05,
      "loss": 0.0025,
      "step": 29890
    },
    {
      "epoch": 1.5946666666666667,
      "grad_norm": 0.220648854970932,
      "learning_rate": 1.0133333333333333e-05,
      "loss": 0.0019,
      "step": 29900
    },
    {
      "epoch": 1.5952,
      "grad_norm": 0.6393432021141052,
      "learning_rate": 1.012e-05,
      "loss": 0.0018,
      "step": 29910
    },
    {
      "epoch": 1.5957333333333334,
      "grad_norm": 0.5426463484764099,
      "learning_rate": 1.0106666666666668e-05,
      "loss": 0.0022,
      "step": 29920
    },
    {
      "epoch": 1.5962666666666667,
      "grad_norm": 0.11478029191493988,
      "learning_rate": 1.0093333333333334e-05,
      "loss": 0.0033,
      "step": 29930
    },
    {
      "epoch": 1.5968,
      "grad_norm": 0.44344255328178406,
      "learning_rate": 1.008e-05,
      "loss": 0.0026,
      "step": 29940
    },
    {
      "epoch": 1.5973333333333333,
      "grad_norm": 0.13323421776294708,
      "learning_rate": 1.0066666666666668e-05,
      "loss": 0.0032,
      "step": 29950
    },
    {
      "epoch": 1.5978666666666665,
      "grad_norm": 0.12853989005088806,
      "learning_rate": 1.0053333333333334e-05,
      "loss": 0.0028,
      "step": 29960
    },
    {
      "epoch": 1.5984,
      "grad_norm": 0.08015512675046921,
      "learning_rate": 1.004e-05,
      "loss": 0.0044,
      "step": 29970
    },
    {
      "epoch": 1.5989333333333333,
      "grad_norm": 0.17535239458084106,
      "learning_rate": 1.0026666666666668e-05,
      "loss": 0.0021,
      "step": 29980
    },
    {
      "epoch": 1.5994666666666668,
      "grad_norm": 0.31568431854248047,
      "learning_rate": 1.0013333333333335e-05,
      "loss": 0.003,
      "step": 29990
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.23302963376045227,
      "learning_rate": 1e-05,
      "loss": 0.0031,
      "step": 30000
    },
    {
      "epoch": 1.6005333333333334,
      "grad_norm": 0.32953402400016785,
      "learning_rate": 9.986666666666667e-06,
      "loss": 0.0021,
      "step": 30010
    },
    {
      "epoch": 1.6010666666666666,
      "grad_norm": 0.09747694432735443,
      "learning_rate": 9.973333333333333e-06,
      "loss": 0.0025,
      "step": 30020
    },
    {
      "epoch": 1.6016,
      "grad_norm": 0.2567223012447357,
      "learning_rate": 9.96e-06,
      "loss": 0.0033,
      "step": 30030
    },
    {
      "epoch": 1.6021333333333332,
      "grad_norm": 0.5173062682151794,
      "learning_rate": 9.946666666666667e-06,
      "loss": 0.0024,
      "step": 30040
    },
    {
      "epoch": 1.6026666666666667,
      "grad_norm": 0.5251274108886719,
      "learning_rate": 9.933333333333334e-06,
      "loss": 0.0024,
      "step": 30050
    },
    {
      "epoch": 1.6032,
      "grad_norm": 0.12514863908290863,
      "learning_rate": 9.92e-06,
      "loss": 0.0026,
      "step": 30060
    },
    {
      "epoch": 1.6037333333333335,
      "grad_norm": 0.07799994200468063,
      "learning_rate": 9.906666666666666e-06,
      "loss": 0.002,
      "step": 30070
    },
    {
      "epoch": 1.6042666666666667,
      "grad_norm": 0.4553593695163727,
      "learning_rate": 9.893333333333334e-06,
      "loss": 0.0027,
      "step": 30080
    },
    {
      "epoch": 1.6048,
      "grad_norm": 0.44776758551597595,
      "learning_rate": 9.88e-06,
      "loss": 0.0023,
      "step": 30090
    },
    {
      "epoch": 1.6053333333333333,
      "grad_norm": 0.18229585886001587,
      "learning_rate": 9.866666666666667e-06,
      "loss": 0.0026,
      "step": 30100
    },
    {
      "epoch": 1.6058666666666666,
      "grad_norm": 0.22182483971118927,
      "learning_rate": 9.853333333333334e-06,
      "loss": 0.0015,
      "step": 30110
    },
    {
      "epoch": 1.6064,
      "grad_norm": 0.2614724040031433,
      "learning_rate": 9.84e-06,
      "loss": 0.0022,
      "step": 30120
    },
    {
      "epoch": 1.6069333333333333,
      "grad_norm": 0.6477232575416565,
      "learning_rate": 9.826666666666667e-06,
      "loss": 0.0018,
      "step": 30130
    },
    {
      "epoch": 1.6074666666666668,
      "grad_norm": 0.36344683170318604,
      "learning_rate": 9.813333333333335e-06,
      "loss": 0.0017,
      "step": 30140
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.5672528743743896,
      "learning_rate": 9.800000000000001e-06,
      "loss": 0.0019,
      "step": 30150
    },
    {
      "epoch": 1.6085333333333334,
      "grad_norm": 0.44329920411109924,
      "learning_rate": 9.786666666666667e-06,
      "loss": 0.0036,
      "step": 30160
    },
    {
      "epoch": 1.6090666666666666,
      "grad_norm": 0.12824931740760803,
      "learning_rate": 9.773333333333333e-06,
      "loss": 0.002,
      "step": 30170
    },
    {
      "epoch": 1.6096,
      "grad_norm": 0.27181294560432434,
      "learning_rate": 9.760000000000001e-06,
      "loss": 0.0023,
      "step": 30180
    },
    {
      "epoch": 1.6101333333333332,
      "grad_norm": 0.46274298429489136,
      "learning_rate": 9.746666666666666e-06,
      "loss": 0.0024,
      "step": 30190
    },
    {
      "epoch": 1.6106666666666667,
      "grad_norm": 0.07857150584459305,
      "learning_rate": 9.733333333333334e-06,
      "loss": 0.0033,
      "step": 30200
    },
    {
      "epoch": 1.6112,
      "grad_norm": 0.3326305150985718,
      "learning_rate": 9.72e-06,
      "loss": 0.0024,
      "step": 30210
    },
    {
      "epoch": 1.6117333333333335,
      "grad_norm": 0.47548213601112366,
      "learning_rate": 9.706666666666666e-06,
      "loss": 0.0017,
      "step": 30220
    },
    {
      "epoch": 1.6122666666666667,
      "grad_norm": 0.15261708199977875,
      "learning_rate": 9.693333333333334e-06,
      "loss": 0.0022,
      "step": 30230
    },
    {
      "epoch": 1.6128,
      "grad_norm": 0.18930931389331818,
      "learning_rate": 9.68e-06,
      "loss": 0.0021,
      "step": 30240
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.09318233281373978,
      "learning_rate": 9.666666666666667e-06,
      "loss": 0.0029,
      "step": 30250
    },
    {
      "epoch": 1.6138666666666666,
      "grad_norm": 0.3604084849357605,
      "learning_rate": 9.653333333333333e-06,
      "loss": 0.0019,
      "step": 30260
    },
    {
      "epoch": 1.6143999999999998,
      "grad_norm": 0.09885057806968689,
      "learning_rate": 9.640000000000001e-06,
      "loss": 0.0019,
      "step": 30270
    },
    {
      "epoch": 1.6149333333333333,
      "grad_norm": 0.36755555868148804,
      "learning_rate": 9.626666666666667e-06,
      "loss": 0.0021,
      "step": 30280
    },
    {
      "epoch": 1.6154666666666668,
      "grad_norm": 0.1606874018907547,
      "learning_rate": 9.613333333333333e-06,
      "loss": 0.0021,
      "step": 30290
    },
    {
      "epoch": 1.616,
      "grad_norm": 0.31266355514526367,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.004,
      "step": 30300
    },
    {
      "epoch": 1.6165333333333334,
      "grad_norm": 0.18553310632705688,
      "learning_rate": 9.586666666666667e-06,
      "loss": 0.0027,
      "step": 30310
    },
    {
      "epoch": 1.6170666666666667,
      "grad_norm": 0.5350586175918579,
      "learning_rate": 9.573333333333334e-06,
      "loss": 0.0017,
      "step": 30320
    },
    {
      "epoch": 1.6176,
      "grad_norm": 0.34172698855400085,
      "learning_rate": 9.560000000000002e-06,
      "loss": 0.002,
      "step": 30330
    },
    {
      "epoch": 1.6181333333333332,
      "grad_norm": 0.32320237159729004,
      "learning_rate": 9.546666666666668e-06,
      "loss": 0.0029,
      "step": 30340
    },
    {
      "epoch": 1.6186666666666667,
      "grad_norm": 0.2677636444568634,
      "learning_rate": 9.533333333333334e-06,
      "loss": 0.0022,
      "step": 30350
    },
    {
      "epoch": 1.6192,
      "grad_norm": 0.3268047571182251,
      "learning_rate": 9.52e-06,
      "loss": 0.0019,
      "step": 30360
    },
    {
      "epoch": 1.6197333333333335,
      "grad_norm": 0.11449680477380753,
      "learning_rate": 9.506666666666667e-06,
      "loss": 0.0028,
      "step": 30370
    },
    {
      "epoch": 1.6202666666666667,
      "grad_norm": 0.3750990629196167,
      "learning_rate": 9.493333333333333e-06,
      "loss": 0.0037,
      "step": 30380
    },
    {
      "epoch": 1.6208,
      "grad_norm": 0.4328393042087555,
      "learning_rate": 9.48e-06,
      "loss": 0.0021,
      "step": 30390
    },
    {
      "epoch": 1.6213333333333333,
      "grad_norm": 0.10510120540857315,
      "learning_rate": 9.466666666666667e-06,
      "loss": 0.0025,
      "step": 30400
    },
    {
      "epoch": 1.6218666666666666,
      "grad_norm": 0.19658441841602325,
      "learning_rate": 9.453333333333333e-06,
      "loss": 0.0017,
      "step": 30410
    },
    {
      "epoch": 1.6223999999999998,
      "grad_norm": 0.14291496574878693,
      "learning_rate": 9.44e-06,
      "loss": 0.002,
      "step": 30420
    },
    {
      "epoch": 1.6229333333333333,
      "grad_norm": 0.25970229506492615,
      "learning_rate": 9.426666666666667e-06,
      "loss": 0.002,
      "step": 30430
    },
    {
      "epoch": 1.6234666666666666,
      "grad_norm": 0.2420741319656372,
      "learning_rate": 9.413333333333334e-06,
      "loss": 0.0022,
      "step": 30440
    },
    {
      "epoch": 1.624,
      "grad_norm": 0.31300345063209534,
      "learning_rate": 9.4e-06,
      "loss": 0.0019,
      "step": 30450
    },
    {
      "epoch": 1.6245333333333334,
      "grad_norm": 0.4998696744441986,
      "learning_rate": 9.386666666666668e-06,
      "loss": 0.0021,
      "step": 30460
    },
    {
      "epoch": 1.6250666666666667,
      "grad_norm": 0.2582909166812897,
      "learning_rate": 9.373333333333334e-06,
      "loss": 0.0019,
      "step": 30470
    },
    {
      "epoch": 1.6256,
      "grad_norm": 0.2719155550003052,
      "learning_rate": 9.36e-06,
      "loss": 0.002,
      "step": 30480
    },
    {
      "epoch": 1.6261333333333332,
      "grad_norm": 0.2907949686050415,
      "learning_rate": 9.346666666666668e-06,
      "loss": 0.003,
      "step": 30490
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 0.29210060834884644,
      "learning_rate": 9.333333333333334e-06,
      "loss": 0.002,
      "step": 30500
    },
    {
      "epoch": 1.6272,
      "grad_norm": 0.1607728749513626,
      "learning_rate": 9.32e-06,
      "loss": 0.0022,
      "step": 30510
    },
    {
      "epoch": 1.6277333333333335,
      "grad_norm": 0.12831126153469086,
      "learning_rate": 9.306666666666668e-06,
      "loss": 0.0029,
      "step": 30520
    },
    {
      "epoch": 1.6282666666666668,
      "grad_norm": 0.27669650316238403,
      "learning_rate": 9.293333333333335e-06,
      "loss": 0.0026,
      "step": 30530
    },
    {
      "epoch": 1.6288,
      "grad_norm": 0.0734173059463501,
      "learning_rate": 9.28e-06,
      "loss": 0.0026,
      "step": 30540
    },
    {
      "epoch": 1.6293333333333333,
      "grad_norm": 0.4053077697753906,
      "learning_rate": 9.266666666666667e-06,
      "loss": 0.0027,
      "step": 30550
    },
    {
      "epoch": 1.6298666666666666,
      "grad_norm": 0.17985479533672333,
      "learning_rate": 9.253333333333333e-06,
      "loss": 0.0021,
      "step": 30560
    },
    {
      "epoch": 1.6303999999999998,
      "grad_norm": 0.1839277148246765,
      "learning_rate": 9.24e-06,
      "loss": 0.0038,
      "step": 30570
    },
    {
      "epoch": 1.6309333333333333,
      "grad_norm": 0.3952455520629883,
      "learning_rate": 9.226666666666668e-06,
      "loss": 0.0036,
      "step": 30580
    },
    {
      "epoch": 1.6314666666666666,
      "grad_norm": 0.16399456560611725,
      "learning_rate": 9.213333333333334e-06,
      "loss": 0.0018,
      "step": 30590
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.18142908811569214,
      "learning_rate": 9.2e-06,
      "loss": 0.0021,
      "step": 30600
    },
    {
      "epoch": 1.6325333333333334,
      "grad_norm": 0.3247603476047516,
      "learning_rate": 9.186666666666666e-06,
      "loss": 0.0019,
      "step": 30610
    },
    {
      "epoch": 1.6330666666666667,
      "grad_norm": 0.2025468349456787,
      "learning_rate": 9.173333333333334e-06,
      "loss": 0.0025,
      "step": 30620
    },
    {
      "epoch": 1.6336,
      "grad_norm": 0.1390477418899536,
      "learning_rate": 9.16e-06,
      "loss": 0.0023,
      "step": 30630
    },
    {
      "epoch": 1.6341333333333332,
      "grad_norm": 0.6591586470603943,
      "learning_rate": 9.146666666666667e-06,
      "loss": 0.0018,
      "step": 30640
    },
    {
      "epoch": 1.6346666666666667,
      "grad_norm": 0.06834679841995239,
      "learning_rate": 9.133333333333335e-06,
      "loss": 0.002,
      "step": 30650
    },
    {
      "epoch": 1.6352,
      "grad_norm": 0.24974088370800018,
      "learning_rate": 9.12e-06,
      "loss": 0.0029,
      "step": 30660
    },
    {
      "epoch": 1.6357333333333335,
      "grad_norm": 0.12131665647029877,
      "learning_rate": 9.106666666666667e-06,
      "loss": 0.0024,
      "step": 30670
    },
    {
      "epoch": 1.6362666666666668,
      "grad_norm": 0.25537386536598206,
      "learning_rate": 9.093333333333335e-06,
      "loss": 0.0025,
      "step": 30680
    },
    {
      "epoch": 1.6368,
      "grad_norm": 0.3121544420719147,
      "learning_rate": 9.080000000000001e-06,
      "loss": 0.0026,
      "step": 30690
    },
    {
      "epoch": 1.6373333333333333,
      "grad_norm": 0.11392839252948761,
      "learning_rate": 9.066666666666667e-06,
      "loss": 0.0023,
      "step": 30700
    },
    {
      "epoch": 1.6378666666666666,
      "grad_norm": 0.1433435082435608,
      "learning_rate": 9.053333333333334e-06,
      "loss": 0.002,
      "step": 30710
    },
    {
      "epoch": 1.6383999999999999,
      "grad_norm": 0.3242529332637787,
      "learning_rate": 9.04e-06,
      "loss": 0.0027,
      "step": 30720
    },
    {
      "epoch": 1.6389333333333334,
      "grad_norm": 0.6311861276626587,
      "learning_rate": 9.026666666666666e-06,
      "loss": 0.0025,
      "step": 30730
    },
    {
      "epoch": 1.6394666666666666,
      "grad_norm": 0.19809363782405853,
      "learning_rate": 9.013333333333334e-06,
      "loss": 0.0022,
      "step": 30740
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.0952540785074234,
      "learning_rate": 9e-06,
      "loss": 0.0018,
      "step": 30750
    },
    {
      "epoch": 1.6405333333333334,
      "grad_norm": 0.19728045165538788,
      "learning_rate": 8.986666666666666e-06,
      "loss": 0.0042,
      "step": 30760
    },
    {
      "epoch": 1.6410666666666667,
      "grad_norm": 0.07398390024900436,
      "learning_rate": 8.973333333333334e-06,
      "loss": 0.0024,
      "step": 30770
    },
    {
      "epoch": 1.6416,
      "grad_norm": 0.22579604387283325,
      "learning_rate": 8.96e-06,
      "loss": 0.0021,
      "step": 30780
    },
    {
      "epoch": 1.6421333333333332,
      "grad_norm": 0.1862582415342331,
      "learning_rate": 8.946666666666667e-06,
      "loss": 0.0023,
      "step": 30790
    },
    {
      "epoch": 1.6426666666666667,
      "grad_norm": 0.11227930337190628,
      "learning_rate": 8.933333333333333e-06,
      "loss": 0.0018,
      "step": 30800
    },
    {
      "epoch": 1.6432,
      "grad_norm": 0.2928963899612427,
      "learning_rate": 8.920000000000001e-06,
      "loss": 0.0026,
      "step": 30810
    },
    {
      "epoch": 1.6437333333333335,
      "grad_norm": 0.6194403767585754,
      "learning_rate": 8.906666666666667e-06,
      "loss": 0.0026,
      "step": 30820
    },
    {
      "epoch": 1.6442666666666668,
      "grad_norm": 0.11641503125429153,
      "learning_rate": 8.893333333333333e-06,
      "loss": 0.0027,
      "step": 30830
    },
    {
      "epoch": 1.6448,
      "grad_norm": 0.4026907980442047,
      "learning_rate": 8.880000000000001e-06,
      "loss": 0.0018,
      "step": 30840
    },
    {
      "epoch": 1.6453333333333333,
      "grad_norm": 0.19737961888313293,
      "learning_rate": 8.866666666666668e-06,
      "loss": 0.0018,
      "step": 30850
    },
    {
      "epoch": 1.6458666666666666,
      "grad_norm": 0.1540636420249939,
      "learning_rate": 8.853333333333334e-06,
      "loss": 0.0018,
      "step": 30860
    },
    {
      "epoch": 1.6463999999999999,
      "grad_norm": 0.10631931573152542,
      "learning_rate": 8.840000000000002e-06,
      "loss": 0.0021,
      "step": 30870
    },
    {
      "epoch": 1.6469333333333334,
      "grad_norm": 0.41359609365463257,
      "learning_rate": 8.826666666666666e-06,
      "loss": 0.0024,
      "step": 30880
    },
    {
      "epoch": 1.6474666666666666,
      "grad_norm": 0.19972679018974304,
      "learning_rate": 8.813333333333333e-06,
      "loss": 0.0016,
      "step": 30890
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.08720344305038452,
      "learning_rate": 8.8e-06,
      "loss": 0.0019,
      "step": 30900
    },
    {
      "epoch": 1.6485333333333334,
      "grad_norm": 0.2674214541912079,
      "learning_rate": 8.786666666666667e-06,
      "loss": 0.0026,
      "step": 30910
    },
    {
      "epoch": 1.6490666666666667,
      "grad_norm": 0.75576251745224,
      "learning_rate": 8.773333333333333e-06,
      "loss": 0.0018,
      "step": 30920
    },
    {
      "epoch": 1.6496,
      "grad_norm": 0.340285062789917,
      "learning_rate": 8.76e-06,
      "loss": 0.0021,
      "step": 30930
    },
    {
      "epoch": 1.6501333333333332,
      "grad_norm": 0.6913840174674988,
      "learning_rate": 8.746666666666667e-06,
      "loss": 0.0026,
      "step": 30940
    },
    {
      "epoch": 1.6506666666666665,
      "grad_norm": 0.5084782838821411,
      "learning_rate": 8.733333333333333e-06,
      "loss": 0.0036,
      "step": 30950
    },
    {
      "epoch": 1.6512,
      "grad_norm": 0.475578248500824,
      "learning_rate": 8.720000000000001e-06,
      "loss": 0.0018,
      "step": 30960
    },
    {
      "epoch": 1.6517333333333335,
      "grad_norm": 0.13711796700954437,
      "learning_rate": 8.706666666666667e-06,
      "loss": 0.0019,
      "step": 30970
    },
    {
      "epoch": 1.6522666666666668,
      "grad_norm": 0.27935171127319336,
      "learning_rate": 8.693333333333334e-06,
      "loss": 0.0019,
      "step": 30980
    },
    {
      "epoch": 1.6528,
      "grad_norm": 0.13732154667377472,
      "learning_rate": 8.68e-06,
      "loss": 0.003,
      "step": 30990
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 0.17213565111160278,
      "learning_rate": 8.666666666666668e-06,
      "loss": 0.0025,
      "step": 31000
    },
    {
      "epoch": 1.6538666666666666,
      "grad_norm": 0.40519601106643677,
      "learning_rate": 8.653333333333334e-06,
      "loss": 0.0024,
      "step": 31010
    },
    {
      "epoch": 1.6543999999999999,
      "grad_norm": 0.16199693083763123,
      "learning_rate": 8.64e-06,
      "loss": 0.0025,
      "step": 31020
    },
    {
      "epoch": 1.6549333333333334,
      "grad_norm": 0.11971323937177658,
      "learning_rate": 8.626666666666668e-06,
      "loss": 0.0021,
      "step": 31030
    },
    {
      "epoch": 1.6554666666666666,
      "grad_norm": 0.630887508392334,
      "learning_rate": 8.613333333333334e-06,
      "loss": 0.0017,
      "step": 31040
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 0.543074905872345,
      "learning_rate": 8.599999999999999e-06,
      "loss": 0.0034,
      "step": 31050
    },
    {
      "epoch": 1.6565333333333334,
      "grad_norm": 0.2571239769458771,
      "learning_rate": 8.586666666666667e-06,
      "loss": 0.0021,
      "step": 31060
    },
    {
      "epoch": 1.6570666666666667,
      "grad_norm": 0.09028486907482147,
      "learning_rate": 8.573333333333333e-06,
      "loss": 0.0022,
      "step": 31070
    },
    {
      "epoch": 1.6576,
      "grad_norm": 0.37848231196403503,
      "learning_rate": 8.56e-06,
      "loss": 0.0026,
      "step": 31080
    },
    {
      "epoch": 1.6581333333333332,
      "grad_norm": 0.4654243588447571,
      "learning_rate": 8.546666666666667e-06,
      "loss": 0.0021,
      "step": 31090
    },
    {
      "epoch": 1.6586666666666665,
      "grad_norm": 0.22123293578624725,
      "learning_rate": 8.533333333333334e-06,
      "loss": 0.0021,
      "step": 31100
    },
    {
      "epoch": 1.6592,
      "grad_norm": 0.31133168935775757,
      "learning_rate": 8.52e-06,
      "loss": 0.003,
      "step": 31110
    },
    {
      "epoch": 1.6597333333333333,
      "grad_norm": 0.41101664304733276,
      "learning_rate": 8.506666666666668e-06,
      "loss": 0.0021,
      "step": 31120
    },
    {
      "epoch": 1.6602666666666668,
      "grad_norm": 0.5584912300109863,
      "learning_rate": 8.493333333333334e-06,
      "loss": 0.0032,
      "step": 31130
    },
    {
      "epoch": 1.6608,
      "grad_norm": 0.43585672974586487,
      "learning_rate": 8.48e-06,
      "loss": 0.0033,
      "step": 31140
    },
    {
      "epoch": 1.6613333333333333,
      "grad_norm": 0.08911910653114319,
      "learning_rate": 8.466666666666666e-06,
      "loss": 0.0018,
      "step": 31150
    },
    {
      "epoch": 1.6618666666666666,
      "grad_norm": 0.4471035897731781,
      "learning_rate": 8.453333333333334e-06,
      "loss": 0.0017,
      "step": 31160
    },
    {
      "epoch": 1.6623999999999999,
      "grad_norm": 0.12046004086732864,
      "learning_rate": 8.44e-06,
      "loss": 0.0021,
      "step": 31170
    },
    {
      "epoch": 1.6629333333333334,
      "grad_norm": 0.18039049208164215,
      "learning_rate": 8.426666666666667e-06,
      "loss": 0.0033,
      "step": 31180
    },
    {
      "epoch": 1.6634666666666666,
      "grad_norm": 0.10343650728464127,
      "learning_rate": 8.413333333333335e-06,
      "loss": 0.0021,
      "step": 31190
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.26354914903640747,
      "learning_rate": 8.400000000000001e-06,
      "loss": 0.0018,
      "step": 31200
    },
    {
      "epoch": 1.6645333333333334,
      "grad_norm": 0.18387068808078766,
      "learning_rate": 8.386666666666667e-06,
      "loss": 0.0021,
      "step": 31210
    },
    {
      "epoch": 1.6650666666666667,
      "grad_norm": 0.3078712224960327,
      "learning_rate": 8.373333333333335e-06,
      "loss": 0.0026,
      "step": 31220
    },
    {
      "epoch": 1.6656,
      "grad_norm": 0.6479585766792297,
      "learning_rate": 8.36e-06,
      "loss": 0.0023,
      "step": 31230
    },
    {
      "epoch": 1.6661333333333332,
      "grad_norm": 0.10217147320508957,
      "learning_rate": 8.346666666666666e-06,
      "loss": 0.002,
      "step": 31240
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.12352550774812698,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.0027,
      "step": 31250
    },
    {
      "epoch": 1.6672,
      "grad_norm": 0.26240074634552,
      "learning_rate": 8.32e-06,
      "loss": 0.0027,
      "step": 31260
    },
    {
      "epoch": 1.6677333333333333,
      "grad_norm": 0.14617909491062164,
      "learning_rate": 8.306666666666666e-06,
      "loss": 0.0027,
      "step": 31270
    },
    {
      "epoch": 1.6682666666666668,
      "grad_norm": 0.39825087785720825,
      "learning_rate": 8.293333333333334e-06,
      "loss": 0.0022,
      "step": 31280
    },
    {
      "epoch": 1.6688,
      "grad_norm": 0.07452328503131866,
      "learning_rate": 8.28e-06,
      "loss": 0.0019,
      "step": 31290
    },
    {
      "epoch": 1.6693333333333333,
      "grad_norm": 0.7795172929763794,
      "learning_rate": 8.266666666666667e-06,
      "loss": 0.0036,
      "step": 31300
    },
    {
      "epoch": 1.6698666666666666,
      "grad_norm": 0.3887309730052948,
      "learning_rate": 8.253333333333334e-06,
      "loss": 0.002,
      "step": 31310
    },
    {
      "epoch": 1.6703999999999999,
      "grad_norm": 0.5199441313743591,
      "learning_rate": 8.24e-06,
      "loss": 0.0025,
      "step": 31320
    },
    {
      "epoch": 1.6709333333333334,
      "grad_norm": 0.33552291989326477,
      "learning_rate": 8.226666666666667e-06,
      "loss": 0.0029,
      "step": 31330
    },
    {
      "epoch": 1.6714666666666667,
      "grad_norm": 0.13519808650016785,
      "learning_rate": 8.213333333333333e-06,
      "loss": 0.0019,
      "step": 31340
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.2148197740316391,
      "learning_rate": 8.200000000000001e-06,
      "loss": 0.0026,
      "step": 31350
    },
    {
      "epoch": 1.6725333333333334,
      "grad_norm": 0.17544133961200714,
      "learning_rate": 8.186666666666667e-06,
      "loss": 0.0018,
      "step": 31360
    },
    {
      "epoch": 1.6730666666666667,
      "grad_norm": 0.25384873151779175,
      "learning_rate": 8.173333333333334e-06,
      "loss": 0.0031,
      "step": 31370
    },
    {
      "epoch": 1.6736,
      "grad_norm": 0.3104363679885864,
      "learning_rate": 8.160000000000001e-06,
      "loss": 0.0018,
      "step": 31380
    },
    {
      "epoch": 1.6741333333333333,
      "grad_norm": 0.1534791737794876,
      "learning_rate": 8.146666666666668e-06,
      "loss": 0.0025,
      "step": 31390
    },
    {
      "epoch": 1.6746666666666665,
      "grad_norm": 0.5884956121444702,
      "learning_rate": 8.133333333333332e-06,
      "loss": 0.0024,
      "step": 31400
    },
    {
      "epoch": 1.6752,
      "grad_norm": 0.335670530796051,
      "learning_rate": 8.12e-06,
      "loss": 0.003,
      "step": 31410
    },
    {
      "epoch": 1.6757333333333333,
      "grad_norm": 0.3220759332180023,
      "learning_rate": 8.106666666666666e-06,
      "loss": 0.0027,
      "step": 31420
    },
    {
      "epoch": 1.6762666666666668,
      "grad_norm": 0.1810550093650818,
      "learning_rate": 8.093333333333333e-06,
      "loss": 0.0021,
      "step": 31430
    },
    {
      "epoch": 1.6768,
      "grad_norm": 0.33738434314727783,
      "learning_rate": 8.08e-06,
      "loss": 0.0024,
      "step": 31440
    },
    {
      "epoch": 1.6773333333333333,
      "grad_norm": 0.30502334237098694,
      "learning_rate": 8.066666666666667e-06,
      "loss": 0.0023,
      "step": 31450
    },
    {
      "epoch": 1.6778666666666666,
      "grad_norm": 0.08461663872003555,
      "learning_rate": 8.053333333333333e-06,
      "loss": 0.003,
      "step": 31460
    },
    {
      "epoch": 1.6784,
      "grad_norm": 0.1980697065591812,
      "learning_rate": 8.040000000000001e-06,
      "loss": 0.0024,
      "step": 31470
    },
    {
      "epoch": 1.6789333333333334,
      "grad_norm": 0.43944114446640015,
      "learning_rate": 8.026666666666667e-06,
      "loss": 0.0021,
      "step": 31480
    },
    {
      "epoch": 1.6794666666666667,
      "grad_norm": 0.5828818678855896,
      "learning_rate": 8.013333333333333e-06,
      "loss": 0.0019,
      "step": 31490
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.28597044944763184,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.0029,
      "step": 31500
    },
    {
      "epoch": 1.6805333333333334,
      "grad_norm": 0.311052143573761,
      "learning_rate": 7.986666666666668e-06,
      "loss": 0.0019,
      "step": 31510
    },
    {
      "epoch": 1.6810666666666667,
      "grad_norm": 0.09328656643629074,
      "learning_rate": 7.973333333333334e-06,
      "loss": 0.0022,
      "step": 31520
    },
    {
      "epoch": 1.6816,
      "grad_norm": 0.0916270837187767,
      "learning_rate": 7.96e-06,
      "loss": 0.002,
      "step": 31530
    },
    {
      "epoch": 1.6821333333333333,
      "grad_norm": 0.2607037127017975,
      "learning_rate": 7.946666666666668e-06,
      "loss": 0.0017,
      "step": 31540
    },
    {
      "epoch": 1.6826666666666665,
      "grad_norm": 0.3004850745201111,
      "learning_rate": 7.933333333333334e-06,
      "loss": 0.0027,
      "step": 31550
    },
    {
      "epoch": 1.6832,
      "grad_norm": 0.5257346034049988,
      "learning_rate": 7.92e-06,
      "loss": 0.002,
      "step": 31560
    },
    {
      "epoch": 1.6837333333333333,
      "grad_norm": 0.4606393277645111,
      "learning_rate": 7.906666666666667e-06,
      "loss": 0.0021,
      "step": 31570
    },
    {
      "epoch": 1.6842666666666668,
      "grad_norm": 0.20737428963184357,
      "learning_rate": 7.893333333333333e-06,
      "loss": 0.0023,
      "step": 31580
    },
    {
      "epoch": 1.6848,
      "grad_norm": 0.304286926984787,
      "learning_rate": 7.879999999999999e-06,
      "loss": 0.0019,
      "step": 31590
    },
    {
      "epoch": 1.6853333333333333,
      "grad_norm": 0.5268946290016174,
      "learning_rate": 7.866666666666667e-06,
      "loss": 0.0031,
      "step": 31600
    },
    {
      "epoch": 1.6858666666666666,
      "grad_norm": 0.20712141692638397,
      "learning_rate": 7.853333333333333e-06,
      "loss": 0.002,
      "step": 31610
    },
    {
      "epoch": 1.6864,
      "grad_norm": 0.2734276354312897,
      "learning_rate": 7.84e-06,
      "loss": 0.0019,
      "step": 31620
    },
    {
      "epoch": 1.6869333333333332,
      "grad_norm": 0.5569555759429932,
      "learning_rate": 7.826666666666667e-06,
      "loss": 0.0018,
      "step": 31630
    },
    {
      "epoch": 1.6874666666666667,
      "grad_norm": 0.14342761039733887,
      "learning_rate": 7.813333333333334e-06,
      "loss": 0.002,
      "step": 31640
    },
    {
      "epoch": 1.688,
      "grad_norm": 0.17508339881896973,
      "learning_rate": 7.8e-06,
      "loss": 0.0027,
      "step": 31650
    },
    {
      "epoch": 1.6885333333333334,
      "grad_norm": 0.48164117336273193,
      "learning_rate": 7.786666666666668e-06,
      "loss": 0.0021,
      "step": 31660
    },
    {
      "epoch": 1.6890666666666667,
      "grad_norm": 0.10251043736934662,
      "learning_rate": 7.773333333333334e-06,
      "loss": 0.0028,
      "step": 31670
    },
    {
      "epoch": 1.6896,
      "grad_norm": 0.5531519055366516,
      "learning_rate": 7.76e-06,
      "loss": 0.0024,
      "step": 31680
    },
    {
      "epoch": 1.6901333333333333,
      "grad_norm": 0.17681697010993958,
      "learning_rate": 7.746666666666668e-06,
      "loss": 0.0026,
      "step": 31690
    },
    {
      "epoch": 1.6906666666666665,
      "grad_norm": 0.09882905334234238,
      "learning_rate": 7.733333333333334e-06,
      "loss": 0.0017,
      "step": 31700
    },
    {
      "epoch": 1.6912,
      "grad_norm": 0.2755391597747803,
      "learning_rate": 7.72e-06,
      "loss": 0.0016,
      "step": 31710
    },
    {
      "epoch": 1.6917333333333333,
      "grad_norm": 0.19521912932395935,
      "learning_rate": 7.706666666666667e-06,
      "loss": 0.0028,
      "step": 31720
    },
    {
      "epoch": 1.6922666666666668,
      "grad_norm": 0.3563472628593445,
      "learning_rate": 7.693333333333335e-06,
      "loss": 0.0028,
      "step": 31730
    },
    {
      "epoch": 1.6928,
      "grad_norm": 0.4905730187892914,
      "learning_rate": 7.68e-06,
      "loss": 0.0024,
      "step": 31740
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.5656347870826721,
      "learning_rate": 7.666666666666667e-06,
      "loss": 0.0019,
      "step": 31750
    },
    {
      "epoch": 1.6938666666666666,
      "grad_norm": 0.36088696122169495,
      "learning_rate": 7.653333333333333e-06,
      "loss": 0.0018,
      "step": 31760
    },
    {
      "epoch": 1.6944,
      "grad_norm": 0.1315227746963501,
      "learning_rate": 7.64e-06,
      "loss": 0.0028,
      "step": 31770
    },
    {
      "epoch": 1.6949333333333332,
      "grad_norm": 0.33941522240638733,
      "learning_rate": 7.626666666666667e-06,
      "loss": 0.0018,
      "step": 31780
    },
    {
      "epoch": 1.6954666666666667,
      "grad_norm": 0.5140495896339417,
      "learning_rate": 7.613333333333334e-06,
      "loss": 0.0025,
      "step": 31790
    },
    {
      "epoch": 1.696,
      "grad_norm": 0.18153342604637146,
      "learning_rate": 7.6e-06,
      "loss": 0.0022,
      "step": 31800
    },
    {
      "epoch": 1.6965333333333334,
      "grad_norm": 0.3469454348087311,
      "learning_rate": 7.586666666666667e-06,
      "loss": 0.0016,
      "step": 31810
    },
    {
      "epoch": 1.6970666666666667,
      "grad_norm": 0.45448800921440125,
      "learning_rate": 7.573333333333333e-06,
      "loss": 0.0016,
      "step": 31820
    },
    {
      "epoch": 1.6976,
      "grad_norm": 0.21953994035720825,
      "learning_rate": 7.5600000000000005e-06,
      "loss": 0.0023,
      "step": 31830
    },
    {
      "epoch": 1.6981333333333333,
      "grad_norm": 0.17962531745433807,
      "learning_rate": 7.5466666666666675e-06,
      "loss": 0.0017,
      "step": 31840
    },
    {
      "epoch": 1.6986666666666665,
      "grad_norm": 0.12374129146337509,
      "learning_rate": 7.533333333333334e-06,
      "loss": 0.0028,
      "step": 31850
    },
    {
      "epoch": 1.6992,
      "grad_norm": 0.2560548484325409,
      "learning_rate": 7.520000000000001e-06,
      "loss": 0.0034,
      "step": 31860
    },
    {
      "epoch": 1.6997333333333333,
      "grad_norm": 0.15699711441993713,
      "learning_rate": 7.506666666666667e-06,
      "loss": 0.0032,
      "step": 31870
    },
    {
      "epoch": 1.7002666666666668,
      "grad_norm": 0.20195217430591583,
      "learning_rate": 7.493333333333334e-06,
      "loss": 0.0033,
      "step": 31880
    },
    {
      "epoch": 1.7008,
      "grad_norm": 0.12847913801670074,
      "learning_rate": 7.480000000000001e-06,
      "loss": 0.0025,
      "step": 31890
    },
    {
      "epoch": 1.7013333333333334,
      "grad_norm": 0.16832335293293,
      "learning_rate": 7.4666666666666675e-06,
      "loss": 0.0042,
      "step": 31900
    },
    {
      "epoch": 1.7018666666666666,
      "grad_norm": 0.2955622375011444,
      "learning_rate": 7.453333333333333e-06,
      "loss": 0.0024,
      "step": 31910
    },
    {
      "epoch": 1.7024,
      "grad_norm": 0.2582089602947235,
      "learning_rate": 7.44e-06,
      "loss": 0.0019,
      "step": 31920
    },
    {
      "epoch": 1.7029333333333332,
      "grad_norm": 0.42045122385025024,
      "learning_rate": 7.426666666666666e-06,
      "loss": 0.0017,
      "step": 31930
    },
    {
      "epoch": 1.7034666666666667,
      "grad_norm": 0.18770278990268707,
      "learning_rate": 7.413333333333333e-06,
      "loss": 0.0017,
      "step": 31940
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.15492750704288483,
      "learning_rate": 7.4e-06,
      "loss": 0.0028,
      "step": 31950
    },
    {
      "epoch": 1.7045333333333335,
      "grad_norm": 0.09837841242551804,
      "learning_rate": 7.3866666666666665e-06,
      "loss": 0.0019,
      "step": 31960
    },
    {
      "epoch": 1.7050666666666667,
      "grad_norm": 0.5617350339889526,
      "learning_rate": 7.373333333333334e-06,
      "loss": 0.0021,
      "step": 31970
    },
    {
      "epoch": 1.7056,
      "grad_norm": 0.21248655021190643,
      "learning_rate": 7.36e-06,
      "loss": 0.0035,
      "step": 31980
    },
    {
      "epoch": 1.7061333333333333,
      "grad_norm": 0.1317663937807083,
      "learning_rate": 7.346666666666667e-06,
      "loss": 0.0029,
      "step": 31990
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.11181878298521042,
      "learning_rate": 7.333333333333334e-06,
      "loss": 0.0017,
      "step": 32000
    },
    {
      "epoch": 1.7072,
      "grad_norm": 0.622649610042572,
      "learning_rate": 7.32e-06,
      "loss": 0.0035,
      "step": 32010
    },
    {
      "epoch": 1.7077333333333333,
      "grad_norm": 0.09375128149986267,
      "learning_rate": 7.306666666666667e-06,
      "loss": 0.0021,
      "step": 32020
    },
    {
      "epoch": 1.7082666666666668,
      "grad_norm": 0.10273286700248718,
      "learning_rate": 7.293333333333334e-06,
      "loss": 0.0039,
      "step": 32030
    },
    {
      "epoch": 1.7088,
      "grad_norm": 0.42618778347969055,
      "learning_rate": 7.280000000000001e-06,
      "loss": 0.0024,
      "step": 32040
    },
    {
      "epoch": 1.7093333333333334,
      "grad_norm": 0.33961403369903564,
      "learning_rate": 7.266666666666668e-06,
      "loss": 0.0027,
      "step": 32050
    },
    {
      "epoch": 1.7098666666666666,
      "grad_norm": 0.1377435326576233,
      "learning_rate": 7.253333333333334e-06,
      "loss": 0.0023,
      "step": 32060
    },
    {
      "epoch": 1.7104,
      "grad_norm": 0.10566404461860657,
      "learning_rate": 7.240000000000001e-06,
      "loss": 0.002,
      "step": 32070
    },
    {
      "epoch": 1.7109333333333332,
      "grad_norm": 0.2653310298919678,
      "learning_rate": 7.226666666666668e-06,
      "loss": 0.0035,
      "step": 32080
    },
    {
      "epoch": 1.7114666666666667,
      "grad_norm": 0.22301718592643738,
      "learning_rate": 7.2133333333333334e-06,
      "loss": 0.0026,
      "step": 32090
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.08743395656347275,
      "learning_rate": 7.2e-06,
      "loss": 0.0025,
      "step": 32100
    },
    {
      "epoch": 1.7125333333333335,
      "grad_norm": 0.42514845728874207,
      "learning_rate": 7.186666666666667e-06,
      "loss": 0.0022,
      "step": 32110
    },
    {
      "epoch": 1.7130666666666667,
      "grad_norm": 0.12718160450458527,
      "learning_rate": 7.173333333333333e-06,
      "loss": 0.0024,
      "step": 32120
    },
    {
      "epoch": 1.7136,
      "grad_norm": 0.1479949951171875,
      "learning_rate": 7.16e-06,
      "loss": 0.0019,
      "step": 32130
    },
    {
      "epoch": 1.7141333333333333,
      "grad_norm": 0.24363234639167786,
      "learning_rate": 7.146666666666667e-06,
      "loss": 0.0019,
      "step": 32140
    },
    {
      "epoch": 1.7146666666666666,
      "grad_norm": 0.3025468587875366,
      "learning_rate": 7.133333333333333e-06,
      "loss": 0.0044,
      "step": 32150
    },
    {
      "epoch": 1.7151999999999998,
      "grad_norm": 0.4031003713607788,
      "learning_rate": 7.1200000000000004e-06,
      "loss": 0.002,
      "step": 32160
    },
    {
      "epoch": 1.7157333333333333,
      "grad_norm": 0.3638271689414978,
      "learning_rate": 7.106666666666667e-06,
      "loss": 0.002,
      "step": 32170
    },
    {
      "epoch": 1.7162666666666668,
      "grad_norm": 0.36349040269851685,
      "learning_rate": 7.093333333333334e-06,
      "loss": 0.0018,
      "step": 32180
    },
    {
      "epoch": 1.7168,
      "grad_norm": 0.36831098794937134,
      "learning_rate": 7.080000000000001e-06,
      "loss": 0.0019,
      "step": 32190
    },
    {
      "epoch": 1.7173333333333334,
      "grad_norm": 0.7831447720527649,
      "learning_rate": 7.066666666666667e-06,
      "loss": 0.002,
      "step": 32200
    },
    {
      "epoch": 1.7178666666666667,
      "grad_norm": 0.0900794044137001,
      "learning_rate": 7.053333333333334e-06,
      "loss": 0.0018,
      "step": 32210
    },
    {
      "epoch": 1.7184,
      "grad_norm": 0.26777780055999756,
      "learning_rate": 7.04e-06,
      "loss": 0.0019,
      "step": 32220
    },
    {
      "epoch": 1.7189333333333332,
      "grad_norm": 0.13780634105205536,
      "learning_rate": 7.0266666666666674e-06,
      "loss": 0.0029,
      "step": 32230
    },
    {
      "epoch": 1.7194666666666667,
      "grad_norm": 0.37702974677085876,
      "learning_rate": 7.0133333333333345e-06,
      "loss": 0.0018,
      "step": 32240
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.11614812910556793,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.0026,
      "step": 32250
    },
    {
      "epoch": 1.7205333333333335,
      "grad_norm": 0.3456439673900604,
      "learning_rate": 6.986666666666666e-06,
      "loss": 0.0018,
      "step": 32260
    },
    {
      "epoch": 1.7210666666666667,
      "grad_norm": 0.16303788125514984,
      "learning_rate": 6.973333333333333e-06,
      "loss": 0.0026,
      "step": 32270
    },
    {
      "epoch": 1.7216,
      "grad_norm": 0.2943784296512604,
      "learning_rate": 6.9599999999999994e-06,
      "loss": 0.0017,
      "step": 32280
    },
    {
      "epoch": 1.7221333333333333,
      "grad_norm": 0.2314240038394928,
      "learning_rate": 6.9466666666666665e-06,
      "loss": 0.0023,
      "step": 32290
    },
    {
      "epoch": 1.7226666666666666,
      "grad_norm": 0.37923502922058105,
      "learning_rate": 6.933333333333334e-06,
      "loss": 0.0033,
      "step": 32300
    },
    {
      "epoch": 1.7231999999999998,
      "grad_norm": 0.16954201459884644,
      "learning_rate": 6.92e-06,
      "loss": 0.0017,
      "step": 32310
    },
    {
      "epoch": 1.7237333333333333,
      "grad_norm": 0.7005305886268616,
      "learning_rate": 6.906666666666667e-06,
      "loss": 0.0018,
      "step": 32320
    },
    {
      "epoch": 1.7242666666666666,
      "grad_norm": 0.4305666983127594,
      "learning_rate": 6.893333333333334e-06,
      "loss": 0.0022,
      "step": 32330
    },
    {
      "epoch": 1.7248,
      "grad_norm": 0.26920992136001587,
      "learning_rate": 6.88e-06,
      "loss": 0.0025,
      "step": 32340
    },
    {
      "epoch": 1.7253333333333334,
      "grad_norm": 0.19967788457870483,
      "learning_rate": 6.866666666666667e-06,
      "loss": 0.0019,
      "step": 32350
    },
    {
      "epoch": 1.7258666666666667,
      "grad_norm": 0.18106171488761902,
      "learning_rate": 6.8533333333333335e-06,
      "loss": 0.0029,
      "step": 32360
    },
    {
      "epoch": 1.7264,
      "grad_norm": 0.09808801859617233,
      "learning_rate": 6.840000000000001e-06,
      "loss": 0.0028,
      "step": 32370
    },
    {
      "epoch": 1.7269333333333332,
      "grad_norm": 0.25550395250320435,
      "learning_rate": 6.826666666666668e-06,
      "loss": 0.0017,
      "step": 32380
    },
    {
      "epoch": 1.7274666666666667,
      "grad_norm": 0.4104098081588745,
      "learning_rate": 6.813333333333334e-06,
      "loss": 0.0026,
      "step": 32390
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.12948200106620789,
      "learning_rate": 6.800000000000001e-06,
      "loss": 0.0018,
      "step": 32400
    },
    {
      "epoch": 1.7285333333333335,
      "grad_norm": 0.2650153338909149,
      "learning_rate": 6.786666666666667e-06,
      "loss": 0.0021,
      "step": 32410
    },
    {
      "epoch": 1.7290666666666668,
      "grad_norm": 0.41435834765434265,
      "learning_rate": 6.773333333333334e-06,
      "loss": 0.002,
      "step": 32420
    },
    {
      "epoch": 1.7296,
      "grad_norm": 0.07365069538354874,
      "learning_rate": 6.76e-06,
      "loss": 0.0041,
      "step": 32430
    },
    {
      "epoch": 1.7301333333333333,
      "grad_norm": 0.2305782288312912,
      "learning_rate": 6.746666666666667e-06,
      "loss": 0.002,
      "step": 32440
    },
    {
      "epoch": 1.7306666666666666,
      "grad_norm": 0.4966828227043152,
      "learning_rate": 6.733333333333333e-06,
      "loss": 0.0019,
      "step": 32450
    },
    {
      "epoch": 1.7311999999999999,
      "grad_norm": 0.2663828134536743,
      "learning_rate": 6.72e-06,
      "loss": 0.0035,
      "step": 32460
    },
    {
      "epoch": 1.7317333333333333,
      "grad_norm": 0.12360286712646484,
      "learning_rate": 6.706666666666666e-06,
      "loss": 0.0022,
      "step": 32470
    },
    {
      "epoch": 1.7322666666666666,
      "grad_norm": 0.269741952419281,
      "learning_rate": 6.693333333333333e-06,
      "loss": 0.0019,
      "step": 32480
    },
    {
      "epoch": 1.7328000000000001,
      "grad_norm": 0.23263783752918243,
      "learning_rate": 6.68e-06,
      "loss": 0.0031,
      "step": 32490
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.2459518313407898,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.0034,
      "step": 32500
    },
    {
      "epoch": 1.7338666666666667,
      "grad_norm": 0.13265332579612732,
      "learning_rate": 6.653333333333334e-06,
      "loss": 0.002,
      "step": 32510
    },
    {
      "epoch": 1.7344,
      "grad_norm": 0.1287389099597931,
      "learning_rate": 6.640000000000001e-06,
      "loss": 0.0025,
      "step": 32520
    },
    {
      "epoch": 1.7349333333333332,
      "grad_norm": 0.3541049659252167,
      "learning_rate": 6.626666666666667e-06,
      "loss": 0.002,
      "step": 32530
    },
    {
      "epoch": 1.7354666666666667,
      "grad_norm": 0.1336304098367691,
      "learning_rate": 6.613333333333334e-06,
      "loss": 0.0017,
      "step": 32540
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.5107132196426392,
      "learning_rate": 6.6e-06,
      "loss": 0.0019,
      "step": 32550
    },
    {
      "epoch": 1.7365333333333335,
      "grad_norm": 0.3794657588005066,
      "learning_rate": 6.586666666666667e-06,
      "loss": 0.0016,
      "step": 32560
    },
    {
      "epoch": 1.7370666666666668,
      "grad_norm": 0.2471616566181183,
      "learning_rate": 6.5733333333333345e-06,
      "loss": 0.0019,
      "step": 32570
    },
    {
      "epoch": 1.7376,
      "grad_norm": 0.2280529886484146,
      "learning_rate": 6.560000000000001e-06,
      "loss": 0.0024,
      "step": 32580
    },
    {
      "epoch": 1.7381333333333333,
      "grad_norm": 0.15169410407543182,
      "learning_rate": 6.546666666666668e-06,
      "loss": 0.0023,
      "step": 32590
    },
    {
      "epoch": 1.7386666666666666,
      "grad_norm": 0.2870476245880127,
      "learning_rate": 6.533333333333333e-06,
      "loss": 0.0027,
      "step": 32600
    },
    {
      "epoch": 1.7391999999999999,
      "grad_norm": 0.17024798691272736,
      "learning_rate": 6.519999999999999e-06,
      "loss": 0.0032,
      "step": 32610
    },
    {
      "epoch": 1.7397333333333334,
      "grad_norm": 0.08268602192401886,
      "learning_rate": 6.5066666666666665e-06,
      "loss": 0.0017,
      "step": 32620
    },
    {
      "epoch": 1.7402666666666666,
      "grad_norm": 0.2565597593784332,
      "learning_rate": 6.4933333333333336e-06,
      "loss": 0.0021,
      "step": 32630
    },
    {
      "epoch": 1.7408000000000001,
      "grad_norm": 0.2675568461418152,
      "learning_rate": 6.48e-06,
      "loss": 0.0022,
      "step": 32640
    },
    {
      "epoch": 1.7413333333333334,
      "grad_norm": 0.10746728628873825,
      "learning_rate": 6.466666666666667e-06,
      "loss": 0.0037,
      "step": 32650
    },
    {
      "epoch": 1.7418666666666667,
      "grad_norm": 0.17529965937137604,
      "learning_rate": 6.453333333333333e-06,
      "loss": 0.0019,
      "step": 32660
    },
    {
      "epoch": 1.7424,
      "grad_norm": 0.2951553463935852,
      "learning_rate": 6.44e-06,
      "loss": 0.0018,
      "step": 32670
    },
    {
      "epoch": 1.7429333333333332,
      "grad_norm": 0.4013806879520416,
      "learning_rate": 6.426666666666667e-06,
      "loss": 0.0026,
      "step": 32680
    },
    {
      "epoch": 1.7434666666666667,
      "grad_norm": 0.271062970161438,
      "learning_rate": 6.4133333333333335e-06,
      "loss": 0.0024,
      "step": 32690
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.12995024025440216,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 0.0023,
      "step": 32700
    },
    {
      "epoch": 1.7445333333333335,
      "grad_norm": 0.45165908336639404,
      "learning_rate": 6.386666666666667e-06,
      "loss": 0.0021,
      "step": 32710
    },
    {
      "epoch": 1.7450666666666668,
      "grad_norm": 0.10611052066087723,
      "learning_rate": 6.373333333333334e-06,
      "loss": 0.0021,
      "step": 32720
    },
    {
      "epoch": 1.7456,
      "grad_norm": 0.2648813724517822,
      "learning_rate": 6.360000000000001e-06,
      "loss": 0.0025,
      "step": 32730
    },
    {
      "epoch": 1.7461333333333333,
      "grad_norm": 0.13063904643058777,
      "learning_rate": 6.346666666666667e-06,
      "loss": 0.0018,
      "step": 32740
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 0.10637959092855453,
      "learning_rate": 6.333333333333334e-06,
      "loss": 0.0018,
      "step": 32750
    },
    {
      "epoch": 1.7471999999999999,
      "grad_norm": 0.17385098338127136,
      "learning_rate": 6.320000000000001e-06,
      "loss": 0.0019,
      "step": 32760
    },
    {
      "epoch": 1.7477333333333334,
      "grad_norm": 0.4540870785713196,
      "learning_rate": 6.306666666666666e-06,
      "loss": 0.0034,
      "step": 32770
    },
    {
      "epoch": 1.7482666666666666,
      "grad_norm": 0.11397440731525421,
      "learning_rate": 6.293333333333333e-06,
      "loss": 0.003,
      "step": 32780
    },
    {
      "epoch": 1.7488000000000001,
      "grad_norm": 0.36351847648620605,
      "learning_rate": 6.28e-06,
      "loss": 0.003,
      "step": 32790
    },
    {
      "epoch": 1.7493333333333334,
      "grad_norm": 0.4051750898361206,
      "learning_rate": 6.266666666666666e-06,
      "loss": 0.003,
      "step": 32800
    },
    {
      "epoch": 1.7498666666666667,
      "grad_norm": 0.20652925968170166,
      "learning_rate": 6.253333333333333e-06,
      "loss": 0.0026,
      "step": 32810
    },
    {
      "epoch": 1.7504,
      "grad_norm": 0.12862446904182434,
      "learning_rate": 6.24e-06,
      "loss": 0.0032,
      "step": 32820
    },
    {
      "epoch": 1.7509333333333332,
      "grad_norm": 0.42746442556381226,
      "learning_rate": 6.226666666666667e-06,
      "loss": 0.0021,
      "step": 32830
    },
    {
      "epoch": 1.7514666666666665,
      "grad_norm": 0.22171130776405334,
      "learning_rate": 6.213333333333334e-06,
      "loss": 0.0019,
      "step": 32840
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.3903026282787323,
      "learning_rate": 6.2e-06,
      "loss": 0.0026,
      "step": 32850
    },
    {
      "epoch": 1.7525333333333335,
      "grad_norm": 0.20846569538116455,
      "learning_rate": 6.186666666666667e-06,
      "loss": 0.0044,
      "step": 32860
    },
    {
      "epoch": 1.7530666666666668,
      "grad_norm": 0.13281813263893127,
      "learning_rate": 6.173333333333334e-06,
      "loss": 0.0025,
      "step": 32870
    },
    {
      "epoch": 1.7536,
      "grad_norm": 0.2195098102092743,
      "learning_rate": 6.16e-06,
      "loss": 0.0021,
      "step": 32880
    },
    {
      "epoch": 1.7541333333333333,
      "grad_norm": 0.11469978094100952,
      "learning_rate": 6.146666666666667e-06,
      "loss": 0.002,
      "step": 32890
    },
    {
      "epoch": 1.7546666666666666,
      "grad_norm": 0.40613842010498047,
      "learning_rate": 6.133333333333334e-06,
      "loss": 0.0025,
      "step": 32900
    },
    {
      "epoch": 1.7551999999999999,
      "grad_norm": 0.3171360194683075,
      "learning_rate": 6.12e-06,
      "loss": 0.0022,
      "step": 32910
    },
    {
      "epoch": 1.7557333333333334,
      "grad_norm": 0.5438893437385559,
      "learning_rate": 6.106666666666667e-06,
      "loss": 0.0021,
      "step": 32920
    },
    {
      "epoch": 1.7562666666666666,
      "grad_norm": 0.25245076417922974,
      "learning_rate": 6.093333333333333e-06,
      "loss": 0.0018,
      "step": 32930
    },
    {
      "epoch": 1.7568000000000001,
      "grad_norm": 0.2113284021615982,
      "learning_rate": 6.08e-06,
      "loss": 0.002,
      "step": 32940
    },
    {
      "epoch": 1.7573333333333334,
      "grad_norm": 0.1332809031009674,
      "learning_rate": 6.066666666666667e-06,
      "loss": 0.0019,
      "step": 32950
    },
    {
      "epoch": 1.7578666666666667,
      "grad_norm": 0.4362897276878357,
      "learning_rate": 6.0533333333333335e-06,
      "loss": 0.0016,
      "step": 32960
    },
    {
      "epoch": 1.7584,
      "grad_norm": 0.09479738771915436,
      "learning_rate": 6.040000000000001e-06,
      "loss": 0.0016,
      "step": 32970
    },
    {
      "epoch": 1.7589333333333332,
      "grad_norm": 0.08890578895807266,
      "learning_rate": 6.026666666666667e-06,
      "loss": 0.0019,
      "step": 32980
    },
    {
      "epoch": 1.7594666666666665,
      "grad_norm": 0.138211190700531,
      "learning_rate": 6.013333333333333e-06,
      "loss": 0.002,
      "step": 32990
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.48883679509162903,
      "learning_rate": 6e-06,
      "loss": 0.0018,
      "step": 33000
    },
    {
      "epoch": 1.7605333333333333,
      "grad_norm": 0.3226560950279236,
      "learning_rate": 5.986666666666667e-06,
      "loss": 0.0028,
      "step": 33010
    },
    {
      "epoch": 1.7610666666666668,
      "grad_norm": 0.25867268443107605,
      "learning_rate": 5.9733333333333335e-06,
      "loss": 0.0019,
      "step": 33020
    },
    {
      "epoch": 1.7616,
      "grad_norm": 0.4339631497859955,
      "learning_rate": 5.9600000000000005e-06,
      "loss": 0.0027,
      "step": 33030
    },
    {
      "epoch": 1.7621333333333333,
      "grad_norm": 0.17302213609218597,
      "learning_rate": 5.946666666666667e-06,
      "loss": 0.0019,
      "step": 33040
    },
    {
      "epoch": 1.7626666666666666,
      "grad_norm": 0.1547463834285736,
      "learning_rate": 5.933333333333334e-06,
      "loss": 0.002,
      "step": 33050
    },
    {
      "epoch": 1.7631999999999999,
      "grad_norm": 0.5426605939865112,
      "learning_rate": 5.920000000000001e-06,
      "loss": 0.0021,
      "step": 33060
    },
    {
      "epoch": 1.7637333333333334,
      "grad_norm": 0.18977445363998413,
      "learning_rate": 5.906666666666667e-06,
      "loss": 0.0035,
      "step": 33070
    },
    {
      "epoch": 1.7642666666666666,
      "grad_norm": 0.13413076102733612,
      "learning_rate": 5.893333333333333e-06,
      "loss": 0.0019,
      "step": 33080
    },
    {
      "epoch": 1.7648000000000001,
      "grad_norm": 0.3218059539794922,
      "learning_rate": 5.8800000000000005e-06,
      "loss": 0.0027,
      "step": 33090
    },
    {
      "epoch": 1.7653333333333334,
      "grad_norm": 0.4026048481464386,
      "learning_rate": 5.866666666666667e-06,
      "loss": 0.002,
      "step": 33100
    },
    {
      "epoch": 1.7658666666666667,
      "grad_norm": 0.13182580471038818,
      "learning_rate": 5.853333333333334e-06,
      "loss": 0.0031,
      "step": 33110
    },
    {
      "epoch": 1.7664,
      "grad_norm": 0.3460161089897156,
      "learning_rate": 5.84e-06,
      "loss": 0.0031,
      "step": 33120
    },
    {
      "epoch": 1.7669333333333332,
      "grad_norm": 0.38634058833122253,
      "learning_rate": 5.826666666666667e-06,
      "loss": 0.0018,
      "step": 33130
    },
    {
      "epoch": 1.7674666666666665,
      "grad_norm": 0.2007904052734375,
      "learning_rate": 5.813333333333334e-06,
      "loss": 0.002,
      "step": 33140
    },
    {
      "epoch": 1.768,
      "grad_norm": 0.23478062450885773,
      "learning_rate": 5.8e-06,
      "loss": 0.0025,
      "step": 33150
    },
    {
      "epoch": 1.7685333333333333,
      "grad_norm": 0.3836681544780731,
      "learning_rate": 5.786666666666667e-06,
      "loss": 0.0021,
      "step": 33160
    },
    {
      "epoch": 1.7690666666666668,
      "grad_norm": 0.2938516139984131,
      "learning_rate": 5.773333333333334e-06,
      "loss": 0.0033,
      "step": 33170
    },
    {
      "epoch": 1.7696,
      "grad_norm": 0.06526293605566025,
      "learning_rate": 5.76e-06,
      "loss": 0.0019,
      "step": 33180
    },
    {
      "epoch": 1.7701333333333333,
      "grad_norm": 0.21116843819618225,
      "learning_rate": 5.746666666666667e-06,
      "loss": 0.0019,
      "step": 33190
    },
    {
      "epoch": 1.7706666666666666,
      "grad_norm": 0.13623031973838806,
      "learning_rate": 5.733333333333333e-06,
      "loss": 0.0019,
      "step": 33200
    },
    {
      "epoch": 1.7711999999999999,
      "grad_norm": 0.2002478688955307,
      "learning_rate": 5.72e-06,
      "loss": 0.0017,
      "step": 33210
    },
    {
      "epoch": 1.7717333333333334,
      "grad_norm": 0.10089689493179321,
      "learning_rate": 5.706666666666667e-06,
      "loss": 0.0025,
      "step": 33220
    },
    {
      "epoch": 1.7722666666666667,
      "grad_norm": 0.28075122833251953,
      "learning_rate": 5.693333333333334e-06,
      "loss": 0.0018,
      "step": 33230
    },
    {
      "epoch": 1.7728000000000002,
      "grad_norm": 0.6656832098960876,
      "learning_rate": 5.680000000000001e-06,
      "loss": 0.0021,
      "step": 33240
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 0.2960423231124878,
      "learning_rate": 5.666666666666667e-06,
      "loss": 0.0039,
      "step": 33250
    },
    {
      "epoch": 1.7738666666666667,
      "grad_norm": 0.21881406009197235,
      "learning_rate": 5.653333333333333e-06,
      "loss": 0.0026,
      "step": 33260
    },
    {
      "epoch": 1.7744,
      "grad_norm": 0.10766728967428207,
      "learning_rate": 5.64e-06,
      "loss": 0.0019,
      "step": 33270
    },
    {
      "epoch": 1.7749333333333333,
      "grad_norm": 0.19118352234363556,
      "learning_rate": 5.626666666666667e-06,
      "loss": 0.0022,
      "step": 33280
    },
    {
      "epoch": 1.7754666666666665,
      "grad_norm": 0.12732002139091492,
      "learning_rate": 5.6133333333333335e-06,
      "loss": 0.0029,
      "step": 33290
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.11925190687179565,
      "learning_rate": 5.600000000000001e-06,
      "loss": 0.0019,
      "step": 33300
    },
    {
      "epoch": 1.7765333333333333,
      "grad_norm": 0.16033844649791718,
      "learning_rate": 5.586666666666667e-06,
      "loss": 0.0026,
      "step": 33310
    },
    {
      "epoch": 1.7770666666666668,
      "grad_norm": 0.08462011069059372,
      "learning_rate": 5.573333333333334e-06,
      "loss": 0.003,
      "step": 33320
    },
    {
      "epoch": 1.7776,
      "grad_norm": 0.43855908513069153,
      "learning_rate": 5.56e-06,
      "loss": 0.0018,
      "step": 33330
    },
    {
      "epoch": 1.7781333333333333,
      "grad_norm": 0.13742585480213165,
      "learning_rate": 5.546666666666666e-06,
      "loss": 0.002,
      "step": 33340
    },
    {
      "epoch": 1.7786666666666666,
      "grad_norm": 0.07821409404277802,
      "learning_rate": 5.5333333333333334e-06,
      "loss": 0.0019,
      "step": 33350
    },
    {
      "epoch": 1.7792,
      "grad_norm": 0.1414390355348587,
      "learning_rate": 5.5200000000000005e-06,
      "loss": 0.0019,
      "step": 33360
    },
    {
      "epoch": 1.7797333333333332,
      "grad_norm": 0.30340203642845154,
      "learning_rate": 5.506666666666667e-06,
      "loss": 0.002,
      "step": 33370
    },
    {
      "epoch": 1.7802666666666667,
      "grad_norm": 0.2313862144947052,
      "learning_rate": 5.493333333333334e-06,
      "loss": 0.0022,
      "step": 33380
    },
    {
      "epoch": 1.7808000000000002,
      "grad_norm": 0.16644108295440674,
      "learning_rate": 5.48e-06,
      "loss": 0.0024,
      "step": 33390
    },
    {
      "epoch": 1.7813333333333334,
      "grad_norm": 0.3287666141986847,
      "learning_rate": 5.466666666666667e-06,
      "loss": 0.0019,
      "step": 33400
    },
    {
      "epoch": 1.7818666666666667,
      "grad_norm": 0.2901138961315155,
      "learning_rate": 5.453333333333334e-06,
      "loss": 0.0019,
      "step": 33410
    },
    {
      "epoch": 1.7824,
      "grad_norm": 0.3747108280658722,
      "learning_rate": 5.44e-06,
      "loss": 0.0021,
      "step": 33420
    },
    {
      "epoch": 1.7829333333333333,
      "grad_norm": 0.46500328183174133,
      "learning_rate": 5.426666666666667e-06,
      "loss": 0.0028,
      "step": 33430
    },
    {
      "epoch": 1.7834666666666665,
      "grad_norm": 0.7511125206947327,
      "learning_rate": 5.413333333333334e-06,
      "loss": 0.0019,
      "step": 33440
    },
    {
      "epoch": 1.784,
      "grad_norm": 0.3192969858646393,
      "learning_rate": 5.4e-06,
      "loss": 0.0038,
      "step": 33450
    },
    {
      "epoch": 1.7845333333333333,
      "grad_norm": 0.33569440245628357,
      "learning_rate": 5.386666666666667e-06,
      "loss": 0.0028,
      "step": 33460
    },
    {
      "epoch": 1.7850666666666668,
      "grad_norm": 0.2944435775279999,
      "learning_rate": 5.373333333333333e-06,
      "loss": 0.0018,
      "step": 33470
    },
    {
      "epoch": 1.7856,
      "grad_norm": 0.2563248872756958,
      "learning_rate": 5.36e-06,
      "loss": 0.0018,
      "step": 33480
    },
    {
      "epoch": 1.7861333333333334,
      "grad_norm": 0.10236797481775284,
      "learning_rate": 5.3466666666666674e-06,
      "loss": 0.0031,
      "step": 33490
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 0.09560926258563995,
      "learning_rate": 5.333333333333334e-06,
      "loss": 0.0017,
      "step": 33500
    },
    {
      "epoch": 1.7872,
      "grad_norm": 0.23551084101200104,
      "learning_rate": 5.32e-06,
      "loss": 0.0031,
      "step": 33510
    },
    {
      "epoch": 1.7877333333333332,
      "grad_norm": 0.1251484900712967,
      "learning_rate": 5.306666666666667e-06,
      "loss": 0.0019,
      "step": 33520
    },
    {
      "epoch": 1.7882666666666667,
      "grad_norm": 0.20285151898860931,
      "learning_rate": 5.293333333333333e-06,
      "loss": 0.0022,
      "step": 33530
    },
    {
      "epoch": 1.7888,
      "grad_norm": 0.2458055019378662,
      "learning_rate": 5.28e-06,
      "loss": 0.0018,
      "step": 33540
    },
    {
      "epoch": 1.7893333333333334,
      "grad_norm": 0.2540552318096161,
      "learning_rate": 5.266666666666667e-06,
      "loss": 0.0017,
      "step": 33550
    },
    {
      "epoch": 1.7898666666666667,
      "grad_norm": 0.1742374747991562,
      "learning_rate": 5.2533333333333336e-06,
      "loss": 0.0021,
      "step": 33560
    },
    {
      "epoch": 1.7904,
      "grad_norm": 0.19224895536899567,
      "learning_rate": 5.240000000000001e-06,
      "loss": 0.0018,
      "step": 33570
    },
    {
      "epoch": 1.7909333333333333,
      "grad_norm": 0.13467083871364594,
      "learning_rate": 5.226666666666667e-06,
      "loss": 0.0022,
      "step": 33580
    },
    {
      "epoch": 1.7914666666666665,
      "grad_norm": 0.1276334971189499,
      "learning_rate": 5.213333333333333e-06,
      "loss": 0.0025,
      "step": 33590
    },
    {
      "epoch": 1.792,
      "grad_norm": 0.11210349947214127,
      "learning_rate": 5.2e-06,
      "loss": 0.0018,
      "step": 33600
    },
    {
      "epoch": 1.7925333333333333,
      "grad_norm": 0.35512423515319824,
      "learning_rate": 5.186666666666666e-06,
      "loss": 0.0018,
      "step": 33610
    },
    {
      "epoch": 1.7930666666666668,
      "grad_norm": 0.17966648936271667,
      "learning_rate": 5.1733333333333335e-06,
      "loss": 0.0018,
      "step": 33620
    },
    {
      "epoch": 1.7936,
      "grad_norm": 0.23282156884670258,
      "learning_rate": 5.1600000000000006e-06,
      "loss": 0.0022,
      "step": 33630
    },
    {
      "epoch": 1.7941333333333334,
      "grad_norm": 0.1714698076248169,
      "learning_rate": 5.146666666666667e-06,
      "loss": 0.0018,
      "step": 33640
    },
    {
      "epoch": 1.7946666666666666,
      "grad_norm": 0.42903468012809753,
      "learning_rate": 5.133333333333334e-06,
      "loss": 0.0021,
      "step": 33650
    },
    {
      "epoch": 1.7952,
      "grad_norm": 0.28024277091026306,
      "learning_rate": 5.12e-06,
      "loss": 0.0019,
      "step": 33660
    },
    {
      "epoch": 1.7957333333333332,
      "grad_norm": 0.09647338092327118,
      "learning_rate": 5.106666666666667e-06,
      "loss": 0.002,
      "step": 33670
    },
    {
      "epoch": 1.7962666666666667,
      "grad_norm": 0.16316884756088257,
      "learning_rate": 5.093333333333333e-06,
      "loss": 0.0028,
      "step": 33680
    },
    {
      "epoch": 1.7968,
      "grad_norm": 0.25486499071121216,
      "learning_rate": 5.08e-06,
      "loss": 0.002,
      "step": 33690
    },
    {
      "epoch": 1.7973333333333334,
      "grad_norm": 0.5120261907577515,
      "learning_rate": 5.066666666666667e-06,
      "loss": 0.0027,
      "step": 33700
    },
    {
      "epoch": 1.7978666666666667,
      "grad_norm": 0.0762895867228508,
      "learning_rate": 5.053333333333334e-06,
      "loss": 0.003,
      "step": 33710
    },
    {
      "epoch": 1.7984,
      "grad_norm": 0.614956259727478,
      "learning_rate": 5.04e-06,
      "loss": 0.0018,
      "step": 33720
    },
    {
      "epoch": 1.7989333333333333,
      "grad_norm": 0.3994019627571106,
      "learning_rate": 5.026666666666667e-06,
      "loss": 0.0038,
      "step": 33730
    },
    {
      "epoch": 1.7994666666666665,
      "grad_norm": 0.3464788496494293,
      "learning_rate": 5.013333333333334e-06,
      "loss": 0.0025,
      "step": 33740
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.29468637704849243,
      "learning_rate": 5e-06,
      "loss": 0.0018,
      "step": 33750
    },
    {
      "epoch": 1.8005333333333333,
      "grad_norm": 0.15117207169532776,
      "learning_rate": 4.986666666666667e-06,
      "loss": 0.0017,
      "step": 33760
    },
    {
      "epoch": 1.8010666666666668,
      "grad_norm": 0.12763665616512299,
      "learning_rate": 4.973333333333334e-06,
      "loss": 0.0026,
      "step": 33770
    },
    {
      "epoch": 1.8016,
      "grad_norm": 0.38636839389801025,
      "learning_rate": 4.96e-06,
      "loss": 0.0034,
      "step": 33780
    },
    {
      "epoch": 1.8021333333333334,
      "grad_norm": 0.3896653354167938,
      "learning_rate": 4.946666666666667e-06,
      "loss": 0.0027,
      "step": 33790
    },
    {
      "epoch": 1.8026666666666666,
      "grad_norm": 0.45610758662223816,
      "learning_rate": 4.933333333333333e-06,
      "loss": 0.0025,
      "step": 33800
    },
    {
      "epoch": 1.8032,
      "grad_norm": 0.20270106196403503,
      "learning_rate": 4.92e-06,
      "loss": 0.0018,
      "step": 33810
    },
    {
      "epoch": 1.8037333333333332,
      "grad_norm": 0.1860252022743225,
      "learning_rate": 4.906666666666667e-06,
      "loss": 0.0026,
      "step": 33820
    },
    {
      "epoch": 1.8042666666666667,
      "grad_norm": 0.11601441353559494,
      "learning_rate": 4.893333333333334e-06,
      "loss": 0.0029,
      "step": 33830
    },
    {
      "epoch": 1.8048,
      "grad_norm": 0.1312038153409958,
      "learning_rate": 4.880000000000001e-06,
      "loss": 0.0019,
      "step": 33840
    },
    {
      "epoch": 1.8053333333333335,
      "grad_norm": 0.1948196142911911,
      "learning_rate": 4.866666666666667e-06,
      "loss": 0.0028,
      "step": 33850
    },
    {
      "epoch": 1.8058666666666667,
      "grad_norm": 0.09465397149324417,
      "learning_rate": 4.853333333333333e-06,
      "loss": 0.0027,
      "step": 33860
    },
    {
      "epoch": 1.8064,
      "grad_norm": 0.2876697778701782,
      "learning_rate": 4.84e-06,
      "loss": 0.0028,
      "step": 33870
    },
    {
      "epoch": 1.8069333333333333,
      "grad_norm": 0.17454315721988678,
      "learning_rate": 4.8266666666666665e-06,
      "loss": 0.0017,
      "step": 33880
    },
    {
      "epoch": 1.8074666666666666,
      "grad_norm": 0.4422492980957031,
      "learning_rate": 4.8133333333333336e-06,
      "loss": 0.0019,
      "step": 33890
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.7156563997268677,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.0021,
      "step": 33900
    },
    {
      "epoch": 1.8085333333333333,
      "grad_norm": 0.17702610790729523,
      "learning_rate": 4.786666666666667e-06,
      "loss": 0.0032,
      "step": 33910
    },
    {
      "epoch": 1.8090666666666668,
      "grad_norm": 0.1974894404411316,
      "learning_rate": 4.773333333333334e-06,
      "loss": 0.0017,
      "step": 33920
    },
    {
      "epoch": 1.8096,
      "grad_norm": 0.6263587474822998,
      "learning_rate": 4.76e-06,
      "loss": 0.0029,
      "step": 33930
    },
    {
      "epoch": 1.8101333333333334,
      "grad_norm": 0.06804095208644867,
      "learning_rate": 4.746666666666666e-06,
      "loss": 0.002,
      "step": 33940
    },
    {
      "epoch": 1.8106666666666666,
      "grad_norm": 0.14334110915660858,
      "learning_rate": 4.7333333333333335e-06,
      "loss": 0.0024,
      "step": 33950
    },
    {
      "epoch": 1.8112,
      "grad_norm": 0.13104946911334991,
      "learning_rate": 4.72e-06,
      "loss": 0.0019,
      "step": 33960
    },
    {
      "epoch": 1.8117333333333332,
      "grad_norm": 0.19008071720600128,
      "learning_rate": 4.706666666666667e-06,
      "loss": 0.0028,
      "step": 33970
    },
    {
      "epoch": 1.8122666666666667,
      "grad_norm": 0.3323129117488861,
      "learning_rate": 4.693333333333334e-06,
      "loss": 0.0017,
      "step": 33980
    },
    {
      "epoch": 1.8128,
      "grad_norm": 0.18577684462070465,
      "learning_rate": 4.68e-06,
      "loss": 0.0037,
      "step": 33990
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 0.1580338329076767,
      "learning_rate": 4.666666666666667e-06,
      "loss": 0.0017,
      "step": 34000
    },
    {
      "epoch": 1.8138666666666667,
      "grad_norm": 0.28483691811561584,
      "learning_rate": 4.653333333333334e-06,
      "loss": 0.0027,
      "step": 34010
    },
    {
      "epoch": 1.8144,
      "grad_norm": 0.23476488888263702,
      "learning_rate": 4.64e-06,
      "loss": 0.0029,
      "step": 34020
    },
    {
      "epoch": 1.8149333333333333,
      "grad_norm": 0.15277229249477386,
      "learning_rate": 4.626666666666667e-06,
      "loss": 0.0017,
      "step": 34030
    },
    {
      "epoch": 1.8154666666666666,
      "grad_norm": 0.10256147384643555,
      "learning_rate": 4.613333333333334e-06,
      "loss": 0.004,
      "step": 34040
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 0.1284039318561554,
      "learning_rate": 4.6e-06,
      "loss": 0.0016,
      "step": 34050
    },
    {
      "epoch": 1.8165333333333333,
      "grad_norm": 0.1642337143421173,
      "learning_rate": 4.586666666666667e-06,
      "loss": 0.0034,
      "step": 34060
    },
    {
      "epoch": 1.8170666666666668,
      "grad_norm": 0.0734504982829094,
      "learning_rate": 4.573333333333333e-06,
      "loss": 0.0018,
      "step": 34070
    },
    {
      "epoch": 1.8176,
      "grad_norm": 0.24069687724113464,
      "learning_rate": 4.56e-06,
      "loss": 0.0025,
      "step": 34080
    },
    {
      "epoch": 1.8181333333333334,
      "grad_norm": 0.11607320606708527,
      "learning_rate": 4.5466666666666675e-06,
      "loss": 0.0025,
      "step": 34090
    },
    {
      "epoch": 1.8186666666666667,
      "grad_norm": 0.15445104241371155,
      "learning_rate": 4.533333333333334e-06,
      "loss": 0.0026,
      "step": 34100
    },
    {
      "epoch": 1.8192,
      "grad_norm": 0.08415734022855759,
      "learning_rate": 4.52e-06,
      "loss": 0.0018,
      "step": 34110
    },
    {
      "epoch": 1.8197333333333332,
      "grad_norm": 0.3675153851509094,
      "learning_rate": 4.506666666666667e-06,
      "loss": 0.0045,
      "step": 34120
    },
    {
      "epoch": 1.8202666666666667,
      "grad_norm": 0.34124886989593506,
      "learning_rate": 4.493333333333333e-06,
      "loss": 0.002,
      "step": 34130
    },
    {
      "epoch": 1.8208,
      "grad_norm": 0.1753903478384018,
      "learning_rate": 4.48e-06,
      "loss": 0.0035,
      "step": 34140
    },
    {
      "epoch": 1.8213333333333335,
      "grad_norm": 0.1851806938648224,
      "learning_rate": 4.4666666666666665e-06,
      "loss": 0.0022,
      "step": 34150
    },
    {
      "epoch": 1.8218666666666667,
      "grad_norm": 0.2495003491640091,
      "learning_rate": 4.453333333333334e-06,
      "loss": 0.0026,
      "step": 34160
    },
    {
      "epoch": 1.8224,
      "grad_norm": 0.2034713327884674,
      "learning_rate": 4.440000000000001e-06,
      "loss": 0.0018,
      "step": 34170
    },
    {
      "epoch": 1.8229333333333333,
      "grad_norm": 0.2035445272922516,
      "learning_rate": 4.426666666666667e-06,
      "loss": 0.0016,
      "step": 34180
    },
    {
      "epoch": 1.8234666666666666,
      "grad_norm": 0.19267217814922333,
      "learning_rate": 4.413333333333333e-06,
      "loss": 0.0026,
      "step": 34190
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.1931033879518509,
      "learning_rate": 4.4e-06,
      "loss": 0.0017,
      "step": 34200
    },
    {
      "epoch": 1.8245333333333333,
      "grad_norm": 0.21509727835655212,
      "learning_rate": 4.3866666666666665e-06,
      "loss": 0.0024,
      "step": 34210
    },
    {
      "epoch": 1.8250666666666666,
      "grad_norm": 0.1512286216020584,
      "learning_rate": 4.3733333333333335e-06,
      "loss": 0.0022,
      "step": 34220
    },
    {
      "epoch": 1.8256000000000001,
      "grad_norm": 0.1957906186580658,
      "learning_rate": 4.360000000000001e-06,
      "loss": 0.0019,
      "step": 34230
    },
    {
      "epoch": 1.8261333333333334,
      "grad_norm": 0.4934360384941101,
      "learning_rate": 4.346666666666667e-06,
      "loss": 0.0026,
      "step": 34240
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 0.12721332907676697,
      "learning_rate": 4.333333333333334e-06,
      "loss": 0.0016,
      "step": 34250
    },
    {
      "epoch": 1.8272,
      "grad_norm": 0.27739736437797546,
      "learning_rate": 4.32e-06,
      "loss": 0.0018,
      "step": 34260
    },
    {
      "epoch": 1.8277333333333332,
      "grad_norm": 0.8363727331161499,
      "learning_rate": 4.306666666666667e-06,
      "loss": 0.0032,
      "step": 34270
    },
    {
      "epoch": 1.8282666666666667,
      "grad_norm": 0.17946162819862366,
      "learning_rate": 4.2933333333333334e-06,
      "loss": 0.0027,
      "step": 34280
    },
    {
      "epoch": 1.8288,
      "grad_norm": 0.10401909053325653,
      "learning_rate": 4.28e-06,
      "loss": 0.002,
      "step": 34290
    },
    {
      "epoch": 1.8293333333333335,
      "grad_norm": 0.1496000736951828,
      "learning_rate": 4.266666666666667e-06,
      "loss": 0.0021,
      "step": 34300
    },
    {
      "epoch": 1.8298666666666668,
      "grad_norm": 0.2195993810892105,
      "learning_rate": 4.253333333333334e-06,
      "loss": 0.0018,
      "step": 34310
    },
    {
      "epoch": 1.8304,
      "grad_norm": 0.554344892501831,
      "learning_rate": 4.24e-06,
      "loss": 0.0018,
      "step": 34320
    },
    {
      "epoch": 1.8309333333333333,
      "grad_norm": 0.3550344407558441,
      "learning_rate": 4.226666666666667e-06,
      "loss": 0.004,
      "step": 34330
    },
    {
      "epoch": 1.8314666666666666,
      "grad_norm": 0.2740073502063751,
      "learning_rate": 4.213333333333333e-06,
      "loss": 0.0021,
      "step": 34340
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.31267592310905457,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 0.0019,
      "step": 34350
    },
    {
      "epoch": 1.8325333333333333,
      "grad_norm": 0.14137791097164154,
      "learning_rate": 4.1866666666666675e-06,
      "loss": 0.0023,
      "step": 34360
    },
    {
      "epoch": 1.8330666666666666,
      "grad_norm": 0.24154818058013916,
      "learning_rate": 4.173333333333333e-06,
      "loss": 0.0027,
      "step": 34370
    },
    {
      "epoch": 1.8336000000000001,
      "grad_norm": 0.42031335830688477,
      "learning_rate": 4.16e-06,
      "loss": 0.0026,
      "step": 34380
    },
    {
      "epoch": 1.8341333333333334,
      "grad_norm": 0.10483228415250778,
      "learning_rate": 4.146666666666667e-06,
      "loss": 0.0018,
      "step": 34390
    },
    {
      "epoch": 1.8346666666666667,
      "grad_norm": 0.7183429002761841,
      "learning_rate": 4.133333333333333e-06,
      "loss": 0.0019,
      "step": 34400
    },
    {
      "epoch": 1.8352,
      "grad_norm": 0.40854328870773315,
      "learning_rate": 4.12e-06,
      "loss": 0.0017,
      "step": 34410
    },
    {
      "epoch": 1.8357333333333332,
      "grad_norm": 0.09222868829965591,
      "learning_rate": 4.106666666666667e-06,
      "loss": 0.0019,
      "step": 34420
    },
    {
      "epoch": 1.8362666666666667,
      "grad_norm": 0.4621140956878662,
      "learning_rate": 4.093333333333334e-06,
      "loss": 0.0018,
      "step": 34430
    },
    {
      "epoch": 1.8368,
      "grad_norm": 0.5468786954879761,
      "learning_rate": 4.080000000000001e-06,
      "loss": 0.0023,
      "step": 34440
    },
    {
      "epoch": 1.8373333333333335,
      "grad_norm": 0.18873710930347443,
      "learning_rate": 4.066666666666666e-06,
      "loss": 0.0018,
      "step": 34450
    },
    {
      "epoch": 1.8378666666666668,
      "grad_norm": 0.050617415457963943,
      "learning_rate": 4.053333333333333e-06,
      "loss": 0.0026,
      "step": 34460
    },
    {
      "epoch": 1.8384,
      "grad_norm": 0.4788118600845337,
      "learning_rate": 4.04e-06,
      "loss": 0.0017,
      "step": 34470
    },
    {
      "epoch": 1.8389333333333333,
      "grad_norm": 0.3477959632873535,
      "learning_rate": 4.0266666666666665e-06,
      "loss": 0.0021,
      "step": 34480
    },
    {
      "epoch": 1.8394666666666666,
      "grad_norm": 0.30181798338890076,
      "learning_rate": 4.013333333333334e-06,
      "loss": 0.0026,
      "step": 34490
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.1879405379295349,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.0021,
      "step": 34500
    },
    {
      "epoch": 1.8405333333333334,
      "grad_norm": 0.13991235196590424,
      "learning_rate": 3.986666666666667e-06,
      "loss": 0.0032,
      "step": 34510
    },
    {
      "epoch": 1.8410666666666666,
      "grad_norm": 0.23855984210968018,
      "learning_rate": 3.973333333333334e-06,
      "loss": 0.0025,
      "step": 34520
    },
    {
      "epoch": 1.8416000000000001,
      "grad_norm": 0.35149088501930237,
      "learning_rate": 3.96e-06,
      "loss": 0.0018,
      "step": 34530
    },
    {
      "epoch": 1.8421333333333334,
      "grad_norm": 0.1721632480621338,
      "learning_rate": 3.9466666666666664e-06,
      "loss": 0.0035,
      "step": 34540
    },
    {
      "epoch": 1.8426666666666667,
      "grad_norm": 0.13186626136302948,
      "learning_rate": 3.9333333333333335e-06,
      "loss": 0.0018,
      "step": 34550
    },
    {
      "epoch": 1.8432,
      "grad_norm": 0.25294795632362366,
      "learning_rate": 3.92e-06,
      "loss": 0.0019,
      "step": 34560
    },
    {
      "epoch": 1.8437333333333332,
      "grad_norm": 0.29104578495025635,
      "learning_rate": 3.906666666666667e-06,
      "loss": 0.002,
      "step": 34570
    },
    {
      "epoch": 1.8442666666666667,
      "grad_norm": 0.09502055495977402,
      "learning_rate": 3.893333333333334e-06,
      "loss": 0.0016,
      "step": 34580
    },
    {
      "epoch": 1.8448,
      "grad_norm": 0.06730057299137115,
      "learning_rate": 3.88e-06,
      "loss": 0.0025,
      "step": 34590
    },
    {
      "epoch": 1.8453333333333335,
      "grad_norm": 0.16680407524108887,
      "learning_rate": 3.866666666666667e-06,
      "loss": 0.0023,
      "step": 34600
    },
    {
      "epoch": 1.8458666666666668,
      "grad_norm": 0.32203349471092224,
      "learning_rate": 3.8533333333333334e-06,
      "loss": 0.0019,
      "step": 34610
    },
    {
      "epoch": 1.8464,
      "grad_norm": 0.10066356509923935,
      "learning_rate": 3.84e-06,
      "loss": 0.0024,
      "step": 34620
    },
    {
      "epoch": 1.8469333333333333,
      "grad_norm": 0.24572719633579254,
      "learning_rate": 3.826666666666667e-06,
      "loss": 0.002,
      "step": 34630
    },
    {
      "epoch": 1.8474666666666666,
      "grad_norm": 0.23679403960704803,
      "learning_rate": 3.8133333333333334e-06,
      "loss": 0.0024,
      "step": 34640
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 0.5375846028327942,
      "learning_rate": 3.8e-06,
      "loss": 0.0028,
      "step": 34650
    },
    {
      "epoch": 1.8485333333333334,
      "grad_norm": 0.6458426713943481,
      "learning_rate": 3.7866666666666667e-06,
      "loss": 0.0026,
      "step": 34660
    },
    {
      "epoch": 1.8490666666666666,
      "grad_norm": 0.2714674770832062,
      "learning_rate": 3.7733333333333338e-06,
      "loss": 0.002,
      "step": 34670
    },
    {
      "epoch": 1.8496000000000001,
      "grad_norm": 0.08792753517627716,
      "learning_rate": 3.7600000000000004e-06,
      "loss": 0.0033,
      "step": 34680
    },
    {
      "epoch": 1.8501333333333334,
      "grad_norm": 0.36064422130584717,
      "learning_rate": 3.746666666666667e-06,
      "loss": 0.0018,
      "step": 34690
    },
    {
      "epoch": 1.8506666666666667,
      "grad_norm": 0.3786994218826294,
      "learning_rate": 3.7333333333333337e-06,
      "loss": 0.0029,
      "step": 34700
    },
    {
      "epoch": 1.8512,
      "grad_norm": 0.2548427879810333,
      "learning_rate": 3.72e-06,
      "loss": 0.0039,
      "step": 34710
    },
    {
      "epoch": 1.8517333333333332,
      "grad_norm": 0.13706909120082855,
      "learning_rate": 3.7066666666666666e-06,
      "loss": 0.0016,
      "step": 34720
    },
    {
      "epoch": 1.8522666666666665,
      "grad_norm": 0.22096142172813416,
      "learning_rate": 3.6933333333333333e-06,
      "loss": 0.0024,
      "step": 34730
    },
    {
      "epoch": 1.8528,
      "grad_norm": 0.21383173763751984,
      "learning_rate": 3.68e-06,
      "loss": 0.0025,
      "step": 34740
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 0.1721702516078949,
      "learning_rate": 3.666666666666667e-06,
      "loss": 0.0025,
      "step": 34750
    },
    {
      "epoch": 1.8538666666666668,
      "grad_norm": 0.3402128517627716,
      "learning_rate": 3.6533333333333336e-06,
      "loss": 0.0018,
      "step": 34760
    },
    {
      "epoch": 1.8544,
      "grad_norm": 0.15307064354419708,
      "learning_rate": 3.6400000000000003e-06,
      "loss": 0.0016,
      "step": 34770
    },
    {
      "epoch": 1.8549333333333333,
      "grad_norm": 0.10340307652950287,
      "learning_rate": 3.626666666666667e-06,
      "loss": 0.0017,
      "step": 34780
    },
    {
      "epoch": 1.8554666666666666,
      "grad_norm": 0.19272203743457794,
      "learning_rate": 3.613333333333334e-06,
      "loss": 0.0019,
      "step": 34790
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.3203461468219757,
      "learning_rate": 3.6e-06,
      "loss": 0.0019,
      "step": 34800
    },
    {
      "epoch": 1.8565333333333334,
      "grad_norm": 0.1273360252380371,
      "learning_rate": 3.5866666666666665e-06,
      "loss": 0.0021,
      "step": 34810
    },
    {
      "epoch": 1.8570666666666666,
      "grad_norm": 0.4245959520339966,
      "learning_rate": 3.5733333333333336e-06,
      "loss": 0.0032,
      "step": 34820
    },
    {
      "epoch": 1.8576000000000001,
      "grad_norm": 0.12578701972961426,
      "learning_rate": 3.5600000000000002e-06,
      "loss": 0.0019,
      "step": 34830
    },
    {
      "epoch": 1.8581333333333334,
      "grad_norm": 0.2036968469619751,
      "learning_rate": 3.546666666666667e-06,
      "loss": 0.0026,
      "step": 34840
    },
    {
      "epoch": 1.8586666666666667,
      "grad_norm": 0.17768198251724243,
      "learning_rate": 3.5333333333333335e-06,
      "loss": 0.0035,
      "step": 34850
    },
    {
      "epoch": 1.8592,
      "grad_norm": 0.294903963804245,
      "learning_rate": 3.52e-06,
      "loss": 0.0017,
      "step": 34860
    },
    {
      "epoch": 1.8597333333333332,
      "grad_norm": 0.31328099966049194,
      "learning_rate": 3.5066666666666673e-06,
      "loss": 0.002,
      "step": 34870
    },
    {
      "epoch": 1.8602666666666665,
      "grad_norm": 0.16128256916999817,
      "learning_rate": 3.493333333333333e-06,
      "loss": 0.0027,
      "step": 34880
    },
    {
      "epoch": 1.8608,
      "grad_norm": 0.3698875606060028,
      "learning_rate": 3.4799999999999997e-06,
      "loss": 0.0019,
      "step": 34890
    },
    {
      "epoch": 1.8613333333333333,
      "grad_norm": 0.0942021906375885,
      "learning_rate": 3.466666666666667e-06,
      "loss": 0.0025,
      "step": 34900
    },
    {
      "epoch": 1.8618666666666668,
      "grad_norm": 0.4709250032901764,
      "learning_rate": 3.4533333333333334e-06,
      "loss": 0.0022,
      "step": 34910
    },
    {
      "epoch": 1.8624,
      "grad_norm": 0.10371684283018112,
      "learning_rate": 3.44e-06,
      "loss": 0.0018,
      "step": 34920
    },
    {
      "epoch": 1.8629333333333333,
      "grad_norm": 0.1908310502767563,
      "learning_rate": 3.4266666666666668e-06,
      "loss": 0.0023,
      "step": 34930
    },
    {
      "epoch": 1.8634666666666666,
      "grad_norm": 0.2424483299255371,
      "learning_rate": 3.413333333333334e-06,
      "loss": 0.0037,
      "step": 34940
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.6359508037567139,
      "learning_rate": 3.4000000000000005e-06,
      "loss": 0.0029,
      "step": 34950
    },
    {
      "epoch": 1.8645333333333334,
      "grad_norm": 0.18400102853775024,
      "learning_rate": 3.386666666666667e-06,
      "loss": 0.0017,
      "step": 34960
    },
    {
      "epoch": 1.8650666666666667,
      "grad_norm": 0.09073901176452637,
      "learning_rate": 3.3733333333333334e-06,
      "loss": 0.0025,
      "step": 34970
    },
    {
      "epoch": 1.8656000000000001,
      "grad_norm": 0.2121742069721222,
      "learning_rate": 3.36e-06,
      "loss": 0.0023,
      "step": 34980
    },
    {
      "epoch": 1.8661333333333334,
      "grad_norm": 0.13913969695568085,
      "learning_rate": 3.3466666666666667e-06,
      "loss": 0.0026,
      "step": 34990
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.26277756690979004,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.0025,
      "step": 35000
    },
    {
      "epoch": 1.8672,
      "grad_norm": 0.1897698938846588,
      "learning_rate": 3.3200000000000004e-06,
      "loss": 0.0025,
      "step": 35010
    },
    {
      "epoch": 1.8677333333333332,
      "grad_norm": 0.467556893825531,
      "learning_rate": 3.306666666666667e-06,
      "loss": 0.0021,
      "step": 35020
    },
    {
      "epoch": 1.8682666666666665,
      "grad_norm": 0.22955451905727386,
      "learning_rate": 3.2933333333333337e-06,
      "loss": 0.0029,
      "step": 35030
    },
    {
      "epoch": 1.8688,
      "grad_norm": 0.1463353931903839,
      "learning_rate": 3.2800000000000004e-06,
      "loss": 0.0019,
      "step": 35040
    },
    {
      "epoch": 1.8693333333333333,
      "grad_norm": 0.23998118937015533,
      "learning_rate": 3.2666666666666666e-06,
      "loss": 0.0024,
      "step": 35050
    },
    {
      "epoch": 1.8698666666666668,
      "grad_norm": 0.6848852038383484,
      "learning_rate": 3.2533333333333332e-06,
      "loss": 0.0018,
      "step": 35060
    },
    {
      "epoch": 1.8704,
      "grad_norm": 0.14054545760154724,
      "learning_rate": 3.24e-06,
      "loss": 0.0019,
      "step": 35070
    },
    {
      "epoch": 1.8709333333333333,
      "grad_norm": 0.14728137850761414,
      "learning_rate": 3.2266666666666665e-06,
      "loss": 0.0035,
      "step": 35080
    },
    {
      "epoch": 1.8714666666666666,
      "grad_norm": 0.2086266577243805,
      "learning_rate": 3.2133333333333336e-06,
      "loss": 0.0025,
      "step": 35090
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.3383742570877075,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 0.0024,
      "step": 35100
    },
    {
      "epoch": 1.8725333333333334,
      "grad_norm": 0.29619303345680237,
      "learning_rate": 3.186666666666667e-06,
      "loss": 0.0018,
      "step": 35110
    },
    {
      "epoch": 1.8730666666666667,
      "grad_norm": 0.12346871942281723,
      "learning_rate": 3.1733333333333336e-06,
      "loss": 0.0027,
      "step": 35120
    },
    {
      "epoch": 1.8736000000000002,
      "grad_norm": 0.38944754004478455,
      "learning_rate": 3.1600000000000007e-06,
      "loss": 0.0032,
      "step": 35130
    },
    {
      "epoch": 1.8741333333333334,
      "grad_norm": 0.30751270055770874,
      "learning_rate": 3.1466666666666665e-06,
      "loss": 0.0025,
      "step": 35140
    },
    {
      "epoch": 1.8746666666666667,
      "grad_norm": 0.552440881729126,
      "learning_rate": 3.133333333333333e-06,
      "loss": 0.002,
      "step": 35150
    },
    {
      "epoch": 1.8752,
      "grad_norm": 0.19910548627376556,
      "learning_rate": 3.12e-06,
      "loss": 0.0017,
      "step": 35160
    },
    {
      "epoch": 1.8757333333333333,
      "grad_norm": 0.09746532142162323,
      "learning_rate": 3.106666666666667e-06,
      "loss": 0.0026,
      "step": 35170
    },
    {
      "epoch": 1.8762666666666665,
      "grad_norm": 0.3025364279747009,
      "learning_rate": 3.0933333333333335e-06,
      "loss": 0.0018,
      "step": 35180
    },
    {
      "epoch": 1.8768,
      "grad_norm": 0.09815344214439392,
      "learning_rate": 3.08e-06,
      "loss": 0.0023,
      "step": 35190
    },
    {
      "epoch": 1.8773333333333333,
      "grad_norm": 0.3266242742538452,
      "learning_rate": 3.066666666666667e-06,
      "loss": 0.0018,
      "step": 35200
    },
    {
      "epoch": 1.8778666666666668,
      "grad_norm": 0.438818097114563,
      "learning_rate": 3.0533333333333335e-06,
      "loss": 0.0024,
      "step": 35210
    },
    {
      "epoch": 1.8784,
      "grad_norm": 0.43422627449035645,
      "learning_rate": 3.04e-06,
      "loss": 0.0016,
      "step": 35220
    },
    {
      "epoch": 1.8789333333333333,
      "grad_norm": 0.18803350627422333,
      "learning_rate": 3.0266666666666668e-06,
      "loss": 0.0032,
      "step": 35230
    },
    {
      "epoch": 1.8794666666666666,
      "grad_norm": 0.2976700961589813,
      "learning_rate": 3.0133333333333334e-06,
      "loss": 0.0033,
      "step": 35240
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.38911738991737366,
      "learning_rate": 3e-06,
      "loss": 0.0023,
      "step": 35250
    },
    {
      "epoch": 1.8805333333333332,
      "grad_norm": 0.21594901382923126,
      "learning_rate": 2.9866666666666667e-06,
      "loss": 0.0021,
      "step": 35260
    },
    {
      "epoch": 1.8810666666666667,
      "grad_norm": 0.38528940081596375,
      "learning_rate": 2.9733333333333334e-06,
      "loss": 0.0022,
      "step": 35270
    },
    {
      "epoch": 1.8816000000000002,
      "grad_norm": 0.08404004573822021,
      "learning_rate": 2.9600000000000005e-06,
      "loss": 0.0017,
      "step": 35280
    },
    {
      "epoch": 1.8821333333333334,
      "grad_norm": 0.3616267740726471,
      "learning_rate": 2.9466666666666667e-06,
      "loss": 0.0031,
      "step": 35290
    },
    {
      "epoch": 1.8826666666666667,
      "grad_norm": 0.106298066675663,
      "learning_rate": 2.9333333333333333e-06,
      "loss": 0.0025,
      "step": 35300
    },
    {
      "epoch": 1.8832,
      "grad_norm": 0.3385410010814667,
      "learning_rate": 2.92e-06,
      "loss": 0.0028,
      "step": 35310
    },
    {
      "epoch": 1.8837333333333333,
      "grad_norm": 0.3117479681968689,
      "learning_rate": 2.906666666666667e-06,
      "loss": 0.002,
      "step": 35320
    },
    {
      "epoch": 1.8842666666666665,
      "grad_norm": 0.23204238712787628,
      "learning_rate": 2.8933333333333333e-06,
      "loss": 0.0033,
      "step": 35330
    },
    {
      "epoch": 1.8848,
      "grad_norm": 0.20043139159679413,
      "learning_rate": 2.88e-06,
      "loss": 0.0022,
      "step": 35340
    },
    {
      "epoch": 1.8853333333333333,
      "grad_norm": 0.35420140624046326,
      "learning_rate": 2.8666666666666666e-06,
      "loss": 0.0033,
      "step": 35350
    },
    {
      "epoch": 1.8858666666666668,
      "grad_norm": 0.3566088080406189,
      "learning_rate": 2.8533333333333337e-06,
      "loss": 0.0016,
      "step": 35360
    },
    {
      "epoch": 1.8864,
      "grad_norm": 0.795385479927063,
      "learning_rate": 2.8400000000000003e-06,
      "loss": 0.0019,
      "step": 35370
    },
    {
      "epoch": 1.8869333333333334,
      "grad_norm": 0.4741120934486389,
      "learning_rate": 2.8266666666666666e-06,
      "loss": 0.0018,
      "step": 35380
    },
    {
      "epoch": 1.8874666666666666,
      "grad_norm": 0.0907939150929451,
      "learning_rate": 2.8133333333333336e-06,
      "loss": 0.0031,
      "step": 35390
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.05559851974248886,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 0.0025,
      "step": 35400
    },
    {
      "epoch": 1.8885333333333332,
      "grad_norm": 0.07755283266305923,
      "learning_rate": 2.786666666666667e-06,
      "loss": 0.0018,
      "step": 35410
    },
    {
      "epoch": 1.8890666666666667,
      "grad_norm": 0.23702672123908997,
      "learning_rate": 2.773333333333333e-06,
      "loss": 0.0025,
      "step": 35420
    },
    {
      "epoch": 1.8896,
      "grad_norm": 0.12097174674272537,
      "learning_rate": 2.7600000000000003e-06,
      "loss": 0.0017,
      "step": 35430
    },
    {
      "epoch": 1.8901333333333334,
      "grad_norm": 0.5034347176551819,
      "learning_rate": 2.746666666666667e-06,
      "loss": 0.0019,
      "step": 35440
    },
    {
      "epoch": 1.8906666666666667,
      "grad_norm": 0.22180773317813873,
      "learning_rate": 2.7333333333333336e-06,
      "loss": 0.0017,
      "step": 35450
    },
    {
      "epoch": 1.8912,
      "grad_norm": 0.16952933371067047,
      "learning_rate": 2.72e-06,
      "loss": 0.0018,
      "step": 35460
    },
    {
      "epoch": 1.8917333333333333,
      "grad_norm": 0.40818125009536743,
      "learning_rate": 2.706666666666667e-06,
      "loss": 0.0018,
      "step": 35470
    },
    {
      "epoch": 1.8922666666666665,
      "grad_norm": 0.09776370227336884,
      "learning_rate": 2.6933333333333335e-06,
      "loss": 0.0025,
      "step": 35480
    },
    {
      "epoch": 1.8928,
      "grad_norm": 0.11044065654277802,
      "learning_rate": 2.68e-06,
      "loss": 0.0028,
      "step": 35490
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 0.26298630237579346,
      "learning_rate": 2.666666666666667e-06,
      "loss": 0.0017,
      "step": 35500
    },
    {
      "epoch": 1.8938666666666668,
      "grad_norm": 0.7344979643821716,
      "learning_rate": 2.6533333333333335e-06,
      "loss": 0.0019,
      "step": 35510
    },
    {
      "epoch": 1.8944,
      "grad_norm": 0.07896818220615387,
      "learning_rate": 2.64e-06,
      "loss": 0.0017,
      "step": 35520
    },
    {
      "epoch": 1.8949333333333334,
      "grad_norm": 0.20726273953914642,
      "learning_rate": 2.6266666666666668e-06,
      "loss": 0.0018,
      "step": 35530
    },
    {
      "epoch": 1.8954666666666666,
      "grad_norm": 0.6900879144668579,
      "learning_rate": 2.6133333333333334e-06,
      "loss": 0.0023,
      "step": 35540
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.20528480410575867,
      "learning_rate": 2.6e-06,
      "loss": 0.0025,
      "step": 35550
    },
    {
      "epoch": 1.8965333333333332,
      "grad_norm": 0.22780773043632507,
      "learning_rate": 2.5866666666666667e-06,
      "loss": 0.0019,
      "step": 35560
    },
    {
      "epoch": 1.8970666666666667,
      "grad_norm": 0.17628376185894012,
      "learning_rate": 2.5733333333333334e-06,
      "loss": 0.0026,
      "step": 35570
    },
    {
      "epoch": 1.8976,
      "grad_norm": 0.19013546407222748,
      "learning_rate": 2.56e-06,
      "loss": 0.0024,
      "step": 35580
    },
    {
      "epoch": 1.8981333333333335,
      "grad_norm": 0.3791393041610718,
      "learning_rate": 2.5466666666666667e-06,
      "loss": 0.0015,
      "step": 35590
    },
    {
      "epoch": 1.8986666666666667,
      "grad_norm": 0.41777801513671875,
      "learning_rate": 2.5333333333333334e-06,
      "loss": 0.0021,
      "step": 35600
    },
    {
      "epoch": 1.8992,
      "grad_norm": 0.1073833778500557,
      "learning_rate": 2.52e-06,
      "loss": 0.0033,
      "step": 35610
    },
    {
      "epoch": 1.8997333333333333,
      "grad_norm": 0.12778784334659576,
      "learning_rate": 2.506666666666667e-06,
      "loss": 0.0018,
      "step": 35620
    },
    {
      "epoch": 1.9002666666666665,
      "grad_norm": 0.1835341602563858,
      "learning_rate": 2.4933333333333333e-06,
      "loss": 0.0029,
      "step": 35630
    },
    {
      "epoch": 1.9008,
      "grad_norm": 0.31017693877220154,
      "learning_rate": 2.48e-06,
      "loss": 0.0023,
      "step": 35640
    },
    {
      "epoch": 1.9013333333333333,
      "grad_norm": 0.08606091141700745,
      "learning_rate": 2.4666666666666666e-06,
      "loss": 0.0022,
      "step": 35650
    },
    {
      "epoch": 1.9018666666666668,
      "grad_norm": 0.4229741394519806,
      "learning_rate": 2.4533333333333337e-06,
      "loss": 0.0023,
      "step": 35660
    },
    {
      "epoch": 1.9024,
      "grad_norm": 0.14249888062477112,
      "learning_rate": 2.4400000000000004e-06,
      "loss": 0.0017,
      "step": 35670
    },
    {
      "epoch": 1.9029333333333334,
      "grad_norm": 0.15613263845443726,
      "learning_rate": 2.4266666666666666e-06,
      "loss": 0.0024,
      "step": 35680
    },
    {
      "epoch": 1.9034666666666666,
      "grad_norm": 0.405362606048584,
      "learning_rate": 2.4133333333333332e-06,
      "loss": 0.0018,
      "step": 35690
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.39998340606689453,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 0.0021,
      "step": 35700
    },
    {
      "epoch": 1.9045333333333332,
      "grad_norm": 0.44851166009902954,
      "learning_rate": 2.386666666666667e-06,
      "loss": 0.0025,
      "step": 35710
    },
    {
      "epoch": 1.9050666666666667,
      "grad_norm": 0.7171863317489624,
      "learning_rate": 2.373333333333333e-06,
      "loss": 0.0018,
      "step": 35720
    },
    {
      "epoch": 1.9056,
      "grad_norm": 0.18458499014377594,
      "learning_rate": 2.36e-06,
      "loss": 0.0023,
      "step": 35730
    },
    {
      "epoch": 1.9061333333333335,
      "grad_norm": 0.11391931027173996,
      "learning_rate": 2.346666666666667e-06,
      "loss": 0.0016,
      "step": 35740
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.1577051728963852,
      "learning_rate": 2.3333333333333336e-06,
      "loss": 0.0019,
      "step": 35750
    },
    {
      "epoch": 1.9072,
      "grad_norm": 0.4281773567199707,
      "learning_rate": 2.32e-06,
      "loss": 0.0027,
      "step": 35760
    },
    {
      "epoch": 1.9077333333333333,
      "grad_norm": 0.23008985817432404,
      "learning_rate": 2.306666666666667e-06,
      "loss": 0.002,
      "step": 35770
    },
    {
      "epoch": 1.9082666666666666,
      "grad_norm": 0.09056680649518967,
      "learning_rate": 2.2933333333333335e-06,
      "loss": 0.0023,
      "step": 35780
    },
    {
      "epoch": 1.9088,
      "grad_norm": 0.09963326901197433,
      "learning_rate": 2.28e-06,
      "loss": 0.0019,
      "step": 35790
    },
    {
      "epoch": 1.9093333333333333,
      "grad_norm": 0.10977491736412048,
      "learning_rate": 2.266666666666667e-06,
      "loss": 0.0041,
      "step": 35800
    },
    {
      "epoch": 1.9098666666666668,
      "grad_norm": 0.18445159494876862,
      "learning_rate": 2.2533333333333335e-06,
      "loss": 0.0028,
      "step": 35810
    },
    {
      "epoch": 1.9104,
      "grad_norm": 0.16934466361999512,
      "learning_rate": 2.24e-06,
      "loss": 0.0028,
      "step": 35820
    },
    {
      "epoch": 1.9109333333333334,
      "grad_norm": 0.07011164724826813,
      "learning_rate": 2.226666666666667e-06,
      "loss": 0.0035,
      "step": 35830
    },
    {
      "epoch": 1.9114666666666666,
      "grad_norm": 0.17259852588176727,
      "learning_rate": 2.2133333333333335e-06,
      "loss": 0.0034,
      "step": 35840
    },
    {
      "epoch": 1.912,
      "grad_norm": 0.4963303804397583,
      "learning_rate": 2.2e-06,
      "loss": 0.0019,
      "step": 35850
    },
    {
      "epoch": 1.9125333333333332,
      "grad_norm": 0.4373633861541748,
      "learning_rate": 2.1866666666666668e-06,
      "loss": 0.0025,
      "step": 35860
    },
    {
      "epoch": 1.9130666666666667,
      "grad_norm": 0.5588570237159729,
      "learning_rate": 2.1733333333333334e-06,
      "loss": 0.0017,
      "step": 35870
    },
    {
      "epoch": 1.9136,
      "grad_norm": 0.23585011065006256,
      "learning_rate": 2.16e-06,
      "loss": 0.0024,
      "step": 35880
    },
    {
      "epoch": 1.9141333333333335,
      "grad_norm": 0.394284725189209,
      "learning_rate": 2.1466666666666667e-06,
      "loss": 0.0025,
      "step": 35890
    },
    {
      "epoch": 1.9146666666666667,
      "grad_norm": 0.11307331174612045,
      "learning_rate": 2.1333333333333334e-06,
      "loss": 0.0024,
      "step": 35900
    },
    {
      "epoch": 1.9152,
      "grad_norm": 0.31782785058021545,
      "learning_rate": 2.12e-06,
      "loss": 0.0019,
      "step": 35910
    },
    {
      "epoch": 1.9157333333333333,
      "grad_norm": 0.23571361601352692,
      "learning_rate": 2.1066666666666667e-06,
      "loss": 0.0015,
      "step": 35920
    },
    {
      "epoch": 1.9162666666666666,
      "grad_norm": 0.27226734161376953,
      "learning_rate": 2.0933333333333338e-06,
      "loss": 0.0035,
      "step": 35930
    },
    {
      "epoch": 1.9167999999999998,
      "grad_norm": 0.38878333568573,
      "learning_rate": 2.08e-06,
      "loss": 0.0022,
      "step": 35940
    },
    {
      "epoch": 1.9173333333333333,
      "grad_norm": 0.155399352312088,
      "learning_rate": 2.0666666666666666e-06,
      "loss": 0.0026,
      "step": 35950
    },
    {
      "epoch": 1.9178666666666668,
      "grad_norm": 0.15098494291305542,
      "learning_rate": 2.0533333333333333e-06,
      "loss": 0.002,
      "step": 35960
    },
    {
      "epoch": 1.9184,
      "grad_norm": 0.2246168851852417,
      "learning_rate": 2.0400000000000004e-06,
      "loss": 0.0026,
      "step": 35970
    },
    {
      "epoch": 1.9189333333333334,
      "grad_norm": 0.48023638129234314,
      "learning_rate": 2.0266666666666666e-06,
      "loss": 0.002,
      "step": 35980
    },
    {
      "epoch": 1.9194666666666667,
      "grad_norm": 0.21382904052734375,
      "learning_rate": 2.0133333333333333e-06,
      "loss": 0.0019,
      "step": 35990
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.2763879895210266,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.0026,
      "step": 36000
    },
    {
      "epoch": 1.9205333333333332,
      "grad_norm": 0.12827461957931519,
      "learning_rate": 1.986666666666667e-06,
      "loss": 0.002,
      "step": 36010
    },
    {
      "epoch": 1.9210666666666667,
      "grad_norm": 0.2864465117454529,
      "learning_rate": 1.9733333333333332e-06,
      "loss": 0.0024,
      "step": 36020
    },
    {
      "epoch": 1.9216,
      "grad_norm": 0.24041467905044556,
      "learning_rate": 1.96e-06,
      "loss": 0.0035,
      "step": 36030
    },
    {
      "epoch": 1.9221333333333335,
      "grad_norm": 0.36661720275878906,
      "learning_rate": 1.946666666666667e-06,
      "loss": 0.0027,
      "step": 36040
    },
    {
      "epoch": 1.9226666666666667,
      "grad_norm": 0.20214338600635529,
      "learning_rate": 1.9333333333333336e-06,
      "loss": 0.0025,
      "step": 36050
    },
    {
      "epoch": 1.9232,
      "grad_norm": 0.2598075568675995,
      "learning_rate": 1.92e-06,
      "loss": 0.0024,
      "step": 36060
    },
    {
      "epoch": 1.9237333333333333,
      "grad_norm": 0.24059893190860748,
      "learning_rate": 1.9066666666666667e-06,
      "loss": 0.0023,
      "step": 36070
    },
    {
      "epoch": 1.9242666666666666,
      "grad_norm": 0.1718810647726059,
      "learning_rate": 1.8933333333333333e-06,
      "loss": 0.0025,
      "step": 36080
    },
    {
      "epoch": 1.9247999999999998,
      "grad_norm": 0.31279411911964417,
      "learning_rate": 1.8800000000000002e-06,
      "loss": 0.0026,
      "step": 36090
    },
    {
      "epoch": 1.9253333333333333,
      "grad_norm": 0.4318263530731201,
      "learning_rate": 1.8666666666666669e-06,
      "loss": 0.0023,
      "step": 36100
    },
    {
      "epoch": 1.9258666666666666,
      "grad_norm": 0.25803104043006897,
      "learning_rate": 1.8533333333333333e-06,
      "loss": 0.0018,
      "step": 36110
    },
    {
      "epoch": 1.9264000000000001,
      "grad_norm": 0.1365937739610672,
      "learning_rate": 1.84e-06,
      "loss": 0.002,
      "step": 36120
    },
    {
      "epoch": 1.9269333333333334,
      "grad_norm": 0.12957663834095,
      "learning_rate": 1.8266666666666668e-06,
      "loss": 0.0031,
      "step": 36130
    },
    {
      "epoch": 1.9274666666666667,
      "grad_norm": 0.3263046443462372,
      "learning_rate": 1.8133333333333335e-06,
      "loss": 0.002,
      "step": 36140
    },
    {
      "epoch": 1.928,
      "grad_norm": 0.14314132928848267,
      "learning_rate": 1.8e-06,
      "loss": 0.0032,
      "step": 36150
    },
    {
      "epoch": 1.9285333333333332,
      "grad_norm": 0.3613826632499695,
      "learning_rate": 1.7866666666666668e-06,
      "loss": 0.0021,
      "step": 36160
    },
    {
      "epoch": 1.9290666666666667,
      "grad_norm": 0.16238264739513397,
      "learning_rate": 1.7733333333333334e-06,
      "loss": 0.0024,
      "step": 36170
    },
    {
      "epoch": 1.9296,
      "grad_norm": 0.24642127752304077,
      "learning_rate": 1.76e-06,
      "loss": 0.0017,
      "step": 36180
    },
    {
      "epoch": 1.9301333333333335,
      "grad_norm": 0.4398951232433319,
      "learning_rate": 1.7466666666666665e-06,
      "loss": 0.0031,
      "step": 36190
    },
    {
      "epoch": 1.9306666666666668,
      "grad_norm": 0.15727148950099945,
      "learning_rate": 1.7333333333333334e-06,
      "loss": 0.0031,
      "step": 36200
    },
    {
      "epoch": 1.9312,
      "grad_norm": 0.14040008187294006,
      "learning_rate": 1.72e-06,
      "loss": 0.0034,
      "step": 36210
    },
    {
      "epoch": 1.9317333333333333,
      "grad_norm": 0.4427020847797394,
      "learning_rate": 1.706666666666667e-06,
      "loss": 0.0017,
      "step": 36220
    },
    {
      "epoch": 1.9322666666666666,
      "grad_norm": 0.10938946157693863,
      "learning_rate": 1.6933333333333336e-06,
      "loss": 0.0028,
      "step": 36230
    },
    {
      "epoch": 1.9327999999999999,
      "grad_norm": 0.10982701927423477,
      "learning_rate": 1.68e-06,
      "loss": 0.0032,
      "step": 36240
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.12028893083333969,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 0.0027,
      "step": 36250
    },
    {
      "epoch": 1.9338666666666666,
      "grad_norm": 0.38708779215812683,
      "learning_rate": 1.6533333333333335e-06,
      "loss": 0.002,
      "step": 36260
    },
    {
      "epoch": 1.9344000000000001,
      "grad_norm": 0.06656164675951004,
      "learning_rate": 1.6400000000000002e-06,
      "loss": 0.0023,
      "step": 36270
    },
    {
      "epoch": 1.9349333333333334,
      "grad_norm": 0.4638946056365967,
      "learning_rate": 1.6266666666666666e-06,
      "loss": 0.0029,
      "step": 36280
    },
    {
      "epoch": 1.9354666666666667,
      "grad_norm": 0.5176472663879395,
      "learning_rate": 1.6133333333333333e-06,
      "loss": 0.0027,
      "step": 36290
    },
    {
      "epoch": 1.936,
      "grad_norm": 0.1045808419585228,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 0.0022,
      "step": 36300
    },
    {
      "epoch": 1.9365333333333332,
      "grad_norm": 0.2268795371055603,
      "learning_rate": 1.5866666666666668e-06,
      "loss": 0.0019,
      "step": 36310
    },
    {
      "epoch": 1.9370666666666667,
      "grad_norm": 0.17323237657546997,
      "learning_rate": 1.5733333333333332e-06,
      "loss": 0.0017,
      "step": 36320
    },
    {
      "epoch": 1.9376,
      "grad_norm": 0.15245680510997772,
      "learning_rate": 1.56e-06,
      "loss": 0.0017,
      "step": 36330
    },
    {
      "epoch": 1.9381333333333335,
      "grad_norm": 0.26004528999328613,
      "learning_rate": 1.5466666666666668e-06,
      "loss": 0.0018,
      "step": 36340
    },
    {
      "epoch": 1.9386666666666668,
      "grad_norm": 0.19559027254581451,
      "learning_rate": 1.5333333333333334e-06,
      "loss": 0.0019,
      "step": 36350
    },
    {
      "epoch": 1.9392,
      "grad_norm": 0.4181225895881653,
      "learning_rate": 1.52e-06,
      "loss": 0.0021,
      "step": 36360
    },
    {
      "epoch": 1.9397333333333333,
      "grad_norm": 0.4062214493751526,
      "learning_rate": 1.5066666666666667e-06,
      "loss": 0.0019,
      "step": 36370
    },
    {
      "epoch": 1.9402666666666666,
      "grad_norm": 0.4622107744216919,
      "learning_rate": 1.4933333333333334e-06,
      "loss": 0.0022,
      "step": 36380
    },
    {
      "epoch": 1.9407999999999999,
      "grad_norm": 0.6103036403656006,
      "learning_rate": 1.4800000000000002e-06,
      "loss": 0.002,
      "step": 36390
    },
    {
      "epoch": 1.9413333333333334,
      "grad_norm": 0.1967339813709259,
      "learning_rate": 1.4666666666666667e-06,
      "loss": 0.0027,
      "step": 36400
    },
    {
      "epoch": 1.9418666666666666,
      "grad_norm": 0.09424202889204025,
      "learning_rate": 1.4533333333333335e-06,
      "loss": 0.0018,
      "step": 36410
    },
    {
      "epoch": 1.9424000000000001,
      "grad_norm": 0.4107667803764343,
      "learning_rate": 1.44e-06,
      "loss": 0.002,
      "step": 36420
    },
    {
      "epoch": 1.9429333333333334,
      "grad_norm": 0.0878930315375328,
      "learning_rate": 1.4266666666666668e-06,
      "loss": 0.0017,
      "step": 36430
    },
    {
      "epoch": 1.9434666666666667,
      "grad_norm": 0.4167480170726776,
      "learning_rate": 1.4133333333333333e-06,
      "loss": 0.0026,
      "step": 36440
    },
    {
      "epoch": 1.944,
      "grad_norm": 0.2039402723312378,
      "learning_rate": 1.4000000000000001e-06,
      "loss": 0.004,
      "step": 36450
    },
    {
      "epoch": 1.9445333333333332,
      "grad_norm": 0.15997090935707092,
      "learning_rate": 1.3866666666666666e-06,
      "loss": 0.0017,
      "step": 36460
    },
    {
      "epoch": 1.9450666666666667,
      "grad_norm": 0.21484054625034332,
      "learning_rate": 1.3733333333333335e-06,
      "loss": 0.0017,
      "step": 36470
    },
    {
      "epoch": 1.9456,
      "grad_norm": 0.2592310607433319,
      "learning_rate": 1.36e-06,
      "loss": 0.0018,
      "step": 36480
    },
    {
      "epoch": 1.9461333333333335,
      "grad_norm": 0.20116454362869263,
      "learning_rate": 1.3466666666666668e-06,
      "loss": 0.0025,
      "step": 36490
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 0.24018189311027527,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 0.0024,
      "step": 36500
    },
    {
      "epoch": 1.9472,
      "grad_norm": 0.31811878085136414,
      "learning_rate": 1.32e-06,
      "loss": 0.0024,
      "step": 36510
    },
    {
      "epoch": 1.9477333333333333,
      "grad_norm": 0.19527563452720642,
      "learning_rate": 1.3066666666666667e-06,
      "loss": 0.0019,
      "step": 36520
    },
    {
      "epoch": 1.9482666666666666,
      "grad_norm": 0.1458238661289215,
      "learning_rate": 1.2933333333333334e-06,
      "loss": 0.0017,
      "step": 36530
    },
    {
      "epoch": 1.9487999999999999,
      "grad_norm": 0.37458014488220215,
      "learning_rate": 1.28e-06,
      "loss": 0.0028,
      "step": 36540
    },
    {
      "epoch": 1.9493333333333334,
      "grad_norm": 0.07016344368457794,
      "learning_rate": 1.2666666666666667e-06,
      "loss": 0.0016,
      "step": 36550
    },
    {
      "epoch": 1.9498666666666666,
      "grad_norm": 0.2512170374393463,
      "learning_rate": 1.2533333333333335e-06,
      "loss": 0.0018,
      "step": 36560
    },
    {
      "epoch": 1.9504000000000001,
      "grad_norm": 0.2966085970401764,
      "learning_rate": 1.24e-06,
      "loss": 0.0017,
      "step": 36570
    },
    {
      "epoch": 1.9509333333333334,
      "grad_norm": 0.272716760635376,
      "learning_rate": 1.2266666666666669e-06,
      "loss": 0.0024,
      "step": 36580
    },
    {
      "epoch": 1.9514666666666667,
      "grad_norm": 0.2534407079219818,
      "learning_rate": 1.2133333333333333e-06,
      "loss": 0.003,
      "step": 36590
    },
    {
      "epoch": 1.952,
      "grad_norm": 0.23622490465641022,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 0.0027,
      "step": 36600
    },
    {
      "epoch": 1.9525333333333332,
      "grad_norm": 0.34965598583221436,
      "learning_rate": 1.1866666666666666e-06,
      "loss": 0.0026,
      "step": 36610
    },
    {
      "epoch": 1.9530666666666665,
      "grad_norm": 0.36068111658096313,
      "learning_rate": 1.1733333333333335e-06,
      "loss": 0.0023,
      "step": 36620
    },
    {
      "epoch": 1.9536,
      "grad_norm": 0.3064669966697693,
      "learning_rate": 1.16e-06,
      "loss": 0.0017,
      "step": 36630
    },
    {
      "epoch": 1.9541333333333335,
      "grad_norm": 0.1462744176387787,
      "learning_rate": 1.1466666666666668e-06,
      "loss": 0.0029,
      "step": 36640
    },
    {
      "epoch": 1.9546666666666668,
      "grad_norm": 0.33881285786628723,
      "learning_rate": 1.1333333333333334e-06,
      "loss": 0.0025,
      "step": 36650
    },
    {
      "epoch": 1.9552,
      "grad_norm": 0.29719388484954834,
      "learning_rate": 1.12e-06,
      "loss": 0.0016,
      "step": 36660
    },
    {
      "epoch": 1.9557333333333333,
      "grad_norm": 0.2732833921909332,
      "learning_rate": 1.1066666666666667e-06,
      "loss": 0.002,
      "step": 36670
    },
    {
      "epoch": 1.9562666666666666,
      "grad_norm": 0.09842131286859512,
      "learning_rate": 1.0933333333333334e-06,
      "loss": 0.0023,
      "step": 36680
    },
    {
      "epoch": 1.9567999999999999,
      "grad_norm": 0.11049424111843109,
      "learning_rate": 1.08e-06,
      "loss": 0.0025,
      "step": 36690
    },
    {
      "epoch": 1.9573333333333334,
      "grad_norm": 0.11929962784051895,
      "learning_rate": 1.0666666666666667e-06,
      "loss": 0.0017,
      "step": 36700
    },
    {
      "epoch": 1.9578666666666666,
      "grad_norm": 0.10934627056121826,
      "learning_rate": 1.0533333333333333e-06,
      "loss": 0.0023,
      "step": 36710
    },
    {
      "epoch": 1.9584000000000001,
      "grad_norm": 0.12936151027679443,
      "learning_rate": 1.04e-06,
      "loss": 0.002,
      "step": 36720
    },
    {
      "epoch": 1.9589333333333334,
      "grad_norm": 0.13651171326637268,
      "learning_rate": 1.0266666666666666e-06,
      "loss": 0.0034,
      "step": 36730
    },
    {
      "epoch": 1.9594666666666667,
      "grad_norm": 0.23560909926891327,
      "learning_rate": 1.0133333333333333e-06,
      "loss": 0.0014,
      "step": 36740
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.6750084757804871,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.0041,
      "step": 36750
    },
    {
      "epoch": 1.9605333333333332,
      "grad_norm": 0.35349419713020325,
      "learning_rate": 9.866666666666666e-07,
      "loss": 0.0043,
      "step": 36760
    },
    {
      "epoch": 1.9610666666666665,
      "grad_norm": 0.1572045236825943,
      "learning_rate": 9.733333333333335e-07,
      "loss": 0.0021,
      "step": 36770
    },
    {
      "epoch": 1.9616,
      "grad_norm": 0.1649276316165924,
      "learning_rate": 9.6e-07,
      "loss": 0.0016,
      "step": 36780
    },
    {
      "epoch": 1.9621333333333333,
      "grad_norm": 0.2930774390697479,
      "learning_rate": 9.466666666666667e-07,
      "loss": 0.0026,
      "step": 36790
    },
    {
      "epoch": 1.9626666666666668,
      "grad_norm": 0.10248157382011414,
      "learning_rate": 9.333333333333334e-07,
      "loss": 0.0016,
      "step": 36800
    },
    {
      "epoch": 1.9632,
      "grad_norm": 0.18471325933933258,
      "learning_rate": 9.2e-07,
      "loss": 0.0024,
      "step": 36810
    },
    {
      "epoch": 1.9637333333333333,
      "grad_norm": 0.5495976209640503,
      "learning_rate": 9.066666666666667e-07,
      "loss": 0.0018,
      "step": 36820
    },
    {
      "epoch": 1.9642666666666666,
      "grad_norm": 0.14326971769332886,
      "learning_rate": 8.933333333333334e-07,
      "loss": 0.0025,
      "step": 36830
    },
    {
      "epoch": 1.9647999999999999,
      "grad_norm": 0.31256213784217834,
      "learning_rate": 8.8e-07,
      "loss": 0.0022,
      "step": 36840
    },
    {
      "epoch": 1.9653333333333334,
      "grad_norm": 0.1783999502658844,
      "learning_rate": 8.666666666666667e-07,
      "loss": 0.0019,
      "step": 36850
    },
    {
      "epoch": 1.9658666666666667,
      "grad_norm": 0.28194233775138855,
      "learning_rate": 8.533333333333335e-07,
      "loss": 0.002,
      "step": 36860
    },
    {
      "epoch": 1.9664000000000001,
      "grad_norm": 0.09820949286222458,
      "learning_rate": 8.4e-07,
      "loss": 0.0017,
      "step": 36870
    },
    {
      "epoch": 1.9669333333333334,
      "grad_norm": 0.24753275513648987,
      "learning_rate": 8.266666666666668e-07,
      "loss": 0.0017,
      "step": 36880
    },
    {
      "epoch": 1.9674666666666667,
      "grad_norm": 0.46775463223457336,
      "learning_rate": 8.133333333333333e-07,
      "loss": 0.0019,
      "step": 36890
    },
    {
      "epoch": 1.968,
      "grad_norm": 0.2699959874153137,
      "learning_rate": 8.000000000000001e-07,
      "loss": 0.002,
      "step": 36900
    },
    {
      "epoch": 1.9685333333333332,
      "grad_norm": 0.6600353717803955,
      "learning_rate": 7.866666666666666e-07,
      "loss": 0.0017,
      "step": 36910
    },
    {
      "epoch": 1.9690666666666665,
      "grad_norm": 0.13189345598220825,
      "learning_rate": 7.733333333333334e-07,
      "loss": 0.0025,
      "step": 36920
    },
    {
      "epoch": 1.9696,
      "grad_norm": 0.3828924894332886,
      "learning_rate": 7.6e-07,
      "loss": 0.0019,
      "step": 36930
    },
    {
      "epoch": 1.9701333333333333,
      "grad_norm": 0.2766270339488983,
      "learning_rate": 7.466666666666667e-07,
      "loss": 0.0025,
      "step": 36940
    },
    {
      "epoch": 1.9706666666666668,
      "grad_norm": 0.18963995575904846,
      "learning_rate": 7.333333333333333e-07,
      "loss": 0.0018,
      "step": 36950
    },
    {
      "epoch": 1.9712,
      "grad_norm": 0.2860282063484192,
      "learning_rate": 7.2e-07,
      "loss": 0.0016,
      "step": 36960
    },
    {
      "epoch": 1.9717333333333333,
      "grad_norm": 0.12104756385087967,
      "learning_rate": 7.066666666666666e-07,
      "loss": 0.0027,
      "step": 36970
    },
    {
      "epoch": 1.9722666666666666,
      "grad_norm": 0.4261019229888916,
      "learning_rate": 6.933333333333333e-07,
      "loss": 0.0025,
      "step": 36980
    },
    {
      "epoch": 1.9727999999999999,
      "grad_norm": 0.40019991993904114,
      "learning_rate": 6.8e-07,
      "loss": 0.002,
      "step": 36990
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 0.16017912328243256,
      "learning_rate": 6.666666666666667e-07,
      "loss": 0.0032,
      "step": 37000
    }
  ],
  "logging_steps": 10,
  "max_steps": 37500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 40,
  "trial_name": null,
  "trial_params": null
}

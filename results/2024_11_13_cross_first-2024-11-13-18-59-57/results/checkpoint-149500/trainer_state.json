{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 7.973333333333334,
  "eval_steps": 500,
  "global_step": 149500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0005333333333333334,
      "grad_norm": 0.6731148362159729,
      "learning_rate": 4.999666666666667e-05,
      "loss": 0.0183,
      "step": 10
    },
    {
      "epoch": 0.0010666666666666667,
      "grad_norm": 0.7922601103782654,
      "learning_rate": 4.9993333333333335e-05,
      "loss": 0.0082,
      "step": 20
    },
    {
      "epoch": 0.0016,
      "grad_norm": 0.12218088656663895,
      "learning_rate": 4.999e-05,
      "loss": 0.0069,
      "step": 30
    },
    {
      "epoch": 0.0021333333333333334,
      "grad_norm": 0.24234633147716522,
      "learning_rate": 4.9986666666666674e-05,
      "loss": 0.0071,
      "step": 40
    },
    {
      "epoch": 0.0026666666666666666,
      "grad_norm": 0.060814812779426575,
      "learning_rate": 4.998333333333334e-05,
      "loss": 0.0057,
      "step": 50
    },
    {
      "epoch": 0.0032,
      "grad_norm": 0.2415844202041626,
      "learning_rate": 4.9980000000000006e-05,
      "loss": 0.0088,
      "step": 60
    },
    {
      "epoch": 0.0037333333333333333,
      "grad_norm": 0.30283230543136597,
      "learning_rate": 4.997666666666667e-05,
      "loss": 0.0074,
      "step": 70
    },
    {
      "epoch": 0.004266666666666667,
      "grad_norm": 0.1814224272966385,
      "learning_rate": 4.997333333333333e-05,
      "loss": 0.0073,
      "step": 80
    },
    {
      "epoch": 0.0048,
      "grad_norm": 0.060970257967710495,
      "learning_rate": 4.997e-05,
      "loss": 0.0075,
      "step": 90
    },
    {
      "epoch": 0.005333333333333333,
      "grad_norm": 0.2421507090330124,
      "learning_rate": 4.996666666666667e-05,
      "loss": 0.0064,
      "step": 100
    },
    {
      "epoch": 0.005866666666666667,
      "grad_norm": 0.6058993339538574,
      "learning_rate": 4.996333333333334e-05,
      "loss": 0.0055,
      "step": 110
    },
    {
      "epoch": 0.0064,
      "grad_norm": 0.2425614446401596,
      "learning_rate": 4.996e-05,
      "loss": 0.0065,
      "step": 120
    },
    {
      "epoch": 0.006933333333333333,
      "grad_norm": 0.1819499433040619,
      "learning_rate": 4.995666666666667e-05,
      "loss": 0.0076,
      "step": 130
    },
    {
      "epoch": 0.007466666666666667,
      "grad_norm": 0.3030049800872803,
      "learning_rate": 4.9953333333333335e-05,
      "loss": 0.0061,
      "step": 140
    },
    {
      "epoch": 0.008,
      "grad_norm": 0.12087484449148178,
      "learning_rate": 4.995e-05,
      "loss": 0.0051,
      "step": 150
    },
    {
      "epoch": 0.008533333333333334,
      "grad_norm": 0.0609058141708374,
      "learning_rate": 4.994666666666667e-05,
      "loss": 0.0068,
      "step": 160
    },
    {
      "epoch": 0.009066666666666667,
      "grad_norm": 0.4234069883823395,
      "learning_rate": 4.9943333333333333e-05,
      "loss": 0.0049,
      "step": 170
    },
    {
      "epoch": 0.0096,
      "grad_norm": 0.48335617780685425,
      "learning_rate": 4.9940000000000006e-05,
      "loss": 0.0087,
      "step": 180
    },
    {
      "epoch": 0.010133333333333333,
      "grad_norm": 0.24164457619190216,
      "learning_rate": 4.993666666666667e-05,
      "loss": 0.0075,
      "step": 190
    },
    {
      "epoch": 0.010666666666666666,
      "grad_norm": 0.12066775560379028,
      "learning_rate": 4.993333333333334e-05,
      "loss": 0.0078,
      "step": 200
    },
    {
      "epoch": 0.0112,
      "grad_norm": 0.18097048997879028,
      "learning_rate": 4.9930000000000005e-05,
      "loss": 0.0061,
      "step": 210
    },
    {
      "epoch": 0.011733333333333333,
      "grad_norm": 0.011477204039692879,
      "learning_rate": 4.992666666666667e-05,
      "loss": 0.0056,
      "step": 220
    },
    {
      "epoch": 0.012266666666666667,
      "grad_norm": 0.5420020222663879,
      "learning_rate": 4.992333333333333e-05,
      "loss": 0.0064,
      "step": 230
    },
    {
      "epoch": 0.0128,
      "grad_norm": 0.180402010679245,
      "learning_rate": 4.992e-05,
      "loss": 0.0073,
      "step": 240
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 0.005624322686344385,
      "learning_rate": 4.991666666666667e-05,
      "loss": 0.0053,
      "step": 250
    },
    {
      "epoch": 0.013866666666666666,
      "grad_norm": 0.42128610610961914,
      "learning_rate": 4.9913333333333335e-05,
      "loss": 0.0066,
      "step": 260
    },
    {
      "epoch": 0.0144,
      "grad_norm": 0.24058423936367035,
      "learning_rate": 4.991e-05,
      "loss": 0.0063,
      "step": 270
    },
    {
      "epoch": 0.014933333333333333,
      "grad_norm": 0.301069438457489,
      "learning_rate": 4.990666666666667e-05,
      "loss": 0.0058,
      "step": 280
    },
    {
      "epoch": 0.015466666666666667,
      "grad_norm": 0.06067077815532684,
      "learning_rate": 4.9903333333333334e-05,
      "loss": 0.0068,
      "step": 290
    },
    {
      "epoch": 0.016,
      "grad_norm": 0.6016381978988647,
      "learning_rate": 4.99e-05,
      "loss": 0.0058,
      "step": 300
    },
    {
      "epoch": 0.016533333333333334,
      "grad_norm": 0.2404160052537918,
      "learning_rate": 4.9896666666666666e-05,
      "loss": 0.0058,
      "step": 310
    },
    {
      "epoch": 0.017066666666666667,
      "grad_norm": 0.060671329498291016,
      "learning_rate": 4.989333333333334e-05,
      "loss": 0.0065,
      "step": 320
    },
    {
      "epoch": 0.0176,
      "grad_norm": 0.36089396476745605,
      "learning_rate": 4.9890000000000005e-05,
      "loss": 0.007,
      "step": 330
    },
    {
      "epoch": 0.018133333333333335,
      "grad_norm": 0.30047497153282166,
      "learning_rate": 4.988666666666667e-05,
      "loss": 0.0064,
      "step": 340
    },
    {
      "epoch": 0.018666666666666668,
      "grad_norm": 0.24047057330608368,
      "learning_rate": 4.988333333333334e-05,
      "loss": 0.0071,
      "step": 350
    },
    {
      "epoch": 0.0192,
      "grad_norm": 0.0176559928804636,
      "learning_rate": 4.9880000000000004e-05,
      "loss": 0.0078,
      "step": 360
    },
    {
      "epoch": 0.019733333333333332,
      "grad_norm": 0.12013883888721466,
      "learning_rate": 4.987666666666667e-05,
      "loss": 0.007,
      "step": 370
    },
    {
      "epoch": 0.020266666666666665,
      "grad_norm": 0.18016768991947174,
      "learning_rate": 4.9873333333333336e-05,
      "loss": 0.0067,
      "step": 380
    },
    {
      "epoch": 0.0208,
      "grad_norm": 0.180001363158226,
      "learning_rate": 4.987e-05,
      "loss": 0.0069,
      "step": 390
    },
    {
      "epoch": 0.021333333333333333,
      "grad_norm": 0.42071324586868286,
      "learning_rate": 4.986666666666667e-05,
      "loss": 0.0068,
      "step": 400
    },
    {
      "epoch": 0.021866666666666666,
      "grad_norm": 0.00758581655099988,
      "learning_rate": 4.9863333333333334e-05,
      "loss": 0.0047,
      "step": 410
    },
    {
      "epoch": 0.0224,
      "grad_norm": 0.1800602525472641,
      "learning_rate": 4.986e-05,
      "loss": 0.006,
      "step": 420
    },
    {
      "epoch": 0.022933333333333333,
      "grad_norm": 0.42060261964797974,
      "learning_rate": 4.9856666666666666e-05,
      "loss": 0.0063,
      "step": 430
    },
    {
      "epoch": 0.023466666666666667,
      "grad_norm": 0.12018182873725891,
      "learning_rate": 4.985333333333333e-05,
      "loss": 0.0055,
      "step": 440
    },
    {
      "epoch": 0.024,
      "grad_norm": 0.24034994840621948,
      "learning_rate": 4.9850000000000006e-05,
      "loss": 0.0054,
      "step": 450
    },
    {
      "epoch": 0.024533333333333334,
      "grad_norm": 0.3004055917263031,
      "learning_rate": 4.984666666666667e-05,
      "loss": 0.0049,
      "step": 460
    },
    {
      "epoch": 0.025066666666666668,
      "grad_norm": 0.06026364117860794,
      "learning_rate": 4.984333333333334e-05,
      "loss": 0.0053,
      "step": 470
    },
    {
      "epoch": 0.0256,
      "grad_norm": 0.6008027791976929,
      "learning_rate": 4.9840000000000004e-05,
      "loss": 0.0063,
      "step": 480
    },
    {
      "epoch": 0.026133333333333335,
      "grad_norm": 0.17983128130435944,
      "learning_rate": 4.983666666666667e-05,
      "loss": 0.0058,
      "step": 490
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.06017757207155228,
      "learning_rate": 4.9833333333333336e-05,
      "loss": 0.0068,
      "step": 500
    },
    {
      "epoch": 0.0272,
      "grad_norm": 0.6007944345474243,
      "learning_rate": 4.983e-05,
      "loss": 0.0072,
      "step": 510
    },
    {
      "epoch": 0.027733333333333332,
      "grad_norm": 0.7815698981285095,
      "learning_rate": 4.982666666666667e-05,
      "loss": 0.006,
      "step": 520
    },
    {
      "epoch": 0.028266666666666666,
      "grad_norm": 0.060088854283094406,
      "learning_rate": 4.9823333333333335e-05,
      "loss": 0.0059,
      "step": 530
    },
    {
      "epoch": 0.0288,
      "grad_norm": 0.06053613871335983,
      "learning_rate": 4.982e-05,
      "loss": 0.0075,
      "step": 540
    },
    {
      "epoch": 0.029333333333333333,
      "grad_norm": 0.2998497188091278,
      "learning_rate": 4.981666666666667e-05,
      "loss": 0.0071,
      "step": 550
    },
    {
      "epoch": 0.029866666666666666,
      "grad_norm": 0.42058271169662476,
      "learning_rate": 4.981333333333333e-05,
      "loss": 0.0068,
      "step": 560
    },
    {
      "epoch": 0.0304,
      "grad_norm": 0.539027214050293,
      "learning_rate": 4.981e-05,
      "loss": 0.0075,
      "step": 570
    },
    {
      "epoch": 0.030933333333333334,
      "grad_norm": 0.12008491158485413,
      "learning_rate": 4.9806666666666665e-05,
      "loss": 0.007,
      "step": 580
    },
    {
      "epoch": 0.031466666666666664,
      "grad_norm": 0.6587716341018677,
      "learning_rate": 4.980333333333334e-05,
      "loss": 0.0052,
      "step": 590
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.35892727971076965,
      "learning_rate": 4.9800000000000004e-05,
      "loss": 0.0065,
      "step": 600
    },
    {
      "epoch": 0.03253333333333333,
      "grad_norm": 0.8970822691917419,
      "learning_rate": 4.979666666666667e-05,
      "loss": 0.0076,
      "step": 610
    },
    {
      "epoch": 0.03306666666666667,
      "grad_norm": 0.35940808057785034,
      "learning_rate": 4.9793333333333337e-05,
      "loss": 0.0069,
      "step": 620
    },
    {
      "epoch": 0.0336,
      "grad_norm": 0.17917822301387787,
      "learning_rate": 4.979e-05,
      "loss": 0.0071,
      "step": 630
    },
    {
      "epoch": 0.034133333333333335,
      "grad_norm": 0.2389836311340332,
      "learning_rate": 4.978666666666667e-05,
      "loss": 0.0065,
      "step": 640
    },
    {
      "epoch": 0.034666666666666665,
      "grad_norm": 0.2995621860027313,
      "learning_rate": 4.9783333333333335e-05,
      "loss": 0.0073,
      "step": 650
    },
    {
      "epoch": 0.0352,
      "grad_norm": 0.179153710603714,
      "learning_rate": 4.978e-05,
      "loss": 0.0061,
      "step": 660
    },
    {
      "epoch": 0.03573333333333333,
      "grad_norm": 0.3595544397830963,
      "learning_rate": 4.9776666666666674e-05,
      "loss": 0.0062,
      "step": 670
    },
    {
      "epoch": 0.03626666666666667,
      "grad_norm": 0.5987527370452881,
      "learning_rate": 4.977333333333334e-05,
      "loss": 0.005,
      "step": 680
    },
    {
      "epoch": 0.0368,
      "grad_norm": 0.7181268930435181,
      "learning_rate": 4.977e-05,
      "loss": 0.0074,
      "step": 690
    },
    {
      "epoch": 0.037333333333333336,
      "grad_norm": 0.5983134508132935,
      "learning_rate": 4.9766666666666666e-05,
      "loss": 0.0078,
      "step": 700
    },
    {
      "epoch": 0.037866666666666667,
      "grad_norm": 0.35885027050971985,
      "learning_rate": 4.976333333333333e-05,
      "loss": 0.008,
      "step": 710
    },
    {
      "epoch": 0.0384,
      "grad_norm": 0.12149112671613693,
      "learning_rate": 4.976e-05,
      "loss": 0.0043,
      "step": 720
    },
    {
      "epoch": 0.038933333333333334,
      "grad_norm": 0.3010692000389099,
      "learning_rate": 4.975666666666667e-05,
      "loss": 0.0074,
      "step": 730
    },
    {
      "epoch": 0.039466666666666664,
      "grad_norm": 0.41991886496543884,
      "learning_rate": 4.975333333333334e-05,
      "loss": 0.0044,
      "step": 740
    },
    {
      "epoch": 0.04,
      "grad_norm": 0.06425132602453232,
      "learning_rate": 4.975e-05,
      "loss": 0.0054,
      "step": 750
    },
    {
      "epoch": 0.04053333333333333,
      "grad_norm": 0.6023046970367432,
      "learning_rate": 4.974666666666667e-05,
      "loss": 0.0055,
      "step": 760
    },
    {
      "epoch": 0.04106666666666667,
      "grad_norm": 0.01417798362672329,
      "learning_rate": 4.9743333333333335e-05,
      "loss": 0.0053,
      "step": 770
    },
    {
      "epoch": 0.0416,
      "grad_norm": 0.36280909180641174,
      "learning_rate": 4.974e-05,
      "loss": 0.0075,
      "step": 780
    },
    {
      "epoch": 0.042133333333333335,
      "grad_norm": 0.3093503713607788,
      "learning_rate": 4.973666666666667e-05,
      "loss": 0.0055,
      "step": 790
    },
    {
      "epoch": 0.042666666666666665,
      "grad_norm": 0.12128933519124985,
      "learning_rate": 4.973333333333334e-05,
      "loss": 0.0056,
      "step": 800
    },
    {
      "epoch": 0.0432,
      "grad_norm": 0.24853791296482086,
      "learning_rate": 4.973000000000001e-05,
      "loss": 0.0085,
      "step": 810
    },
    {
      "epoch": 0.04373333333333333,
      "grad_norm": 0.3681086599826813,
      "learning_rate": 4.972666666666667e-05,
      "loss": 0.0062,
      "step": 820
    },
    {
      "epoch": 0.04426666666666667,
      "grad_norm": 0.2511760890483856,
      "learning_rate": 4.972333333333334e-05,
      "loss": 0.0045,
      "step": 830
    },
    {
      "epoch": 0.0448,
      "grad_norm": 0.28214484453201294,
      "learning_rate": 4.972e-05,
      "loss": 0.005,
      "step": 840
    },
    {
      "epoch": 0.04533333333333334,
      "grad_norm": 0.5539833903312683,
      "learning_rate": 4.9716666666666664e-05,
      "loss": 0.0065,
      "step": 850
    },
    {
      "epoch": 0.04586666666666667,
      "grad_norm": 0.3770247995853424,
      "learning_rate": 4.971333333333334e-05,
      "loss": 0.007,
      "step": 860
    },
    {
      "epoch": 0.0464,
      "grad_norm": 0.3130215108394623,
      "learning_rate": 4.9710000000000003e-05,
      "loss": 0.0037,
      "step": 870
    },
    {
      "epoch": 0.046933333333333334,
      "grad_norm": 0.2331652045249939,
      "learning_rate": 4.970666666666667e-05,
      "loss": 0.0052,
      "step": 880
    },
    {
      "epoch": 0.047466666666666664,
      "grad_norm": 0.8158845901489258,
      "learning_rate": 4.9703333333333336e-05,
      "loss": 0.005,
      "step": 890
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.21765980124473572,
      "learning_rate": 4.97e-05,
      "loss": 0.005,
      "step": 900
    },
    {
      "epoch": 0.04853333333333333,
      "grad_norm": 0.02240701951086521,
      "learning_rate": 4.969666666666667e-05,
      "loss": 0.0037,
      "step": 910
    },
    {
      "epoch": 0.04906666666666667,
      "grad_norm": 0.012878244742751122,
      "learning_rate": 4.9693333333333334e-05,
      "loss": 0.0075,
      "step": 920
    },
    {
      "epoch": 0.0496,
      "grad_norm": 0.12768340110778809,
      "learning_rate": 4.969e-05,
      "loss": 0.0047,
      "step": 930
    },
    {
      "epoch": 0.050133333333333335,
      "grad_norm": 0.31124866008758545,
      "learning_rate": 4.968666666666667e-05,
      "loss": 0.0056,
      "step": 940
    },
    {
      "epoch": 0.050666666666666665,
      "grad_norm": 0.5044259428977966,
      "learning_rate": 4.968333333333334e-05,
      "loss": 0.0038,
      "step": 950
    },
    {
      "epoch": 0.0512,
      "grad_norm": 0.3112633228302002,
      "learning_rate": 4.9680000000000005e-05,
      "loss": 0.005,
      "step": 960
    },
    {
      "epoch": 0.05173333333333333,
      "grad_norm": 0.8283177018165588,
      "learning_rate": 4.967666666666667e-05,
      "loss": 0.0053,
      "step": 970
    },
    {
      "epoch": 0.05226666666666667,
      "grad_norm": 1.0392570495605469,
      "learning_rate": 4.967333333333334e-05,
      "loss": 0.0047,
      "step": 980
    },
    {
      "epoch": 0.0528,
      "grad_norm": 0.5472145080566406,
      "learning_rate": 4.967e-05,
      "loss": 0.0042,
      "step": 990
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.18246635794639587,
      "learning_rate": 4.966666666666667e-05,
      "loss": 0.0062,
      "step": 1000
    },
    {
      "epoch": 0.05386666666666667,
      "grad_norm": 0.4343021512031555,
      "learning_rate": 4.9663333333333336e-05,
      "loss": 0.0047,
      "step": 1010
    },
    {
      "epoch": 0.0544,
      "grad_norm": 0.1298825740814209,
      "learning_rate": 4.966e-05,
      "loss": 0.0053,
      "step": 1020
    },
    {
      "epoch": 0.054933333333333334,
      "grad_norm": 0.06465639919042587,
      "learning_rate": 4.965666666666667e-05,
      "loss": 0.0043,
      "step": 1030
    },
    {
      "epoch": 0.055466666666666664,
      "grad_norm": 0.12360607832670212,
      "learning_rate": 4.9653333333333335e-05,
      "loss": 0.006,
      "step": 1040
    },
    {
      "epoch": 0.056,
      "grad_norm": 0.560873806476593,
      "learning_rate": 4.965e-05,
      "loss": 0.0041,
      "step": 1050
    },
    {
      "epoch": 0.05653333333333333,
      "grad_norm": 0.7588992118835449,
      "learning_rate": 4.964666666666667e-05,
      "loss": 0.0048,
      "step": 1060
    },
    {
      "epoch": 0.05706666666666667,
      "grad_norm": 0.3733832538127899,
      "learning_rate": 4.964333333333333e-05,
      "loss": 0.0032,
      "step": 1070
    },
    {
      "epoch": 0.0576,
      "grad_norm": 0.3442462980747223,
      "learning_rate": 4.9640000000000006e-05,
      "loss": 0.0039,
      "step": 1080
    },
    {
      "epoch": 0.058133333333333335,
      "grad_norm": 0.3744776248931885,
      "learning_rate": 4.963666666666667e-05,
      "loss": 0.006,
      "step": 1090
    },
    {
      "epoch": 0.058666666666666666,
      "grad_norm": 0.2831810414791107,
      "learning_rate": 4.963333333333334e-05,
      "loss": 0.0049,
      "step": 1100
    },
    {
      "epoch": 0.0592,
      "grad_norm": 0.7573944330215454,
      "learning_rate": 4.9630000000000004e-05,
      "loss": 0.0048,
      "step": 1110
    },
    {
      "epoch": 0.05973333333333333,
      "grad_norm": 0.17974193394184113,
      "learning_rate": 4.962666666666667e-05,
      "loss": 0.0059,
      "step": 1120
    },
    {
      "epoch": 0.06026666666666667,
      "grad_norm": 0.07407457381486893,
      "learning_rate": 4.9623333333333337e-05,
      "loss": 0.0047,
      "step": 1130
    },
    {
      "epoch": 0.0608,
      "grad_norm": 0.4406491816043854,
      "learning_rate": 4.962e-05,
      "loss": 0.0042,
      "step": 1140
    },
    {
      "epoch": 0.06133333333333333,
      "grad_norm": 0.31406959891319275,
      "learning_rate": 4.961666666666667e-05,
      "loss": 0.0041,
      "step": 1150
    },
    {
      "epoch": 0.06186666666666667,
      "grad_norm": 0.2486831694841385,
      "learning_rate": 4.9613333333333335e-05,
      "loss": 0.0058,
      "step": 1160
    },
    {
      "epoch": 0.0624,
      "grad_norm": 0.12404389679431915,
      "learning_rate": 4.961e-05,
      "loss": 0.0051,
      "step": 1170
    },
    {
      "epoch": 0.06293333333333333,
      "grad_norm": 0.27599048614501953,
      "learning_rate": 4.960666666666667e-05,
      "loss": 0.0035,
      "step": 1180
    },
    {
      "epoch": 0.06346666666666667,
      "grad_norm": 0.6144094467163086,
      "learning_rate": 4.960333333333333e-05,
      "loss": 0.0042,
      "step": 1190
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.18917426466941833,
      "learning_rate": 4.96e-05,
      "loss": 0.0048,
      "step": 1200
    },
    {
      "epoch": 0.06453333333333333,
      "grad_norm": 0.21402619779109955,
      "learning_rate": 4.959666666666667e-05,
      "loss": 0.0029,
      "step": 1210
    },
    {
      "epoch": 0.06506666666666666,
      "grad_norm": 0.49419426918029785,
      "learning_rate": 4.959333333333334e-05,
      "loss": 0.004,
      "step": 1220
    },
    {
      "epoch": 0.0656,
      "grad_norm": 0.24174058437347412,
      "learning_rate": 4.9590000000000005e-05,
      "loss": 0.0052,
      "step": 1230
    },
    {
      "epoch": 0.06613333333333334,
      "grad_norm": 0.27950015664100647,
      "learning_rate": 4.958666666666667e-05,
      "loss": 0.0054,
      "step": 1240
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 0.5232154130935669,
      "learning_rate": 4.958333333333334e-05,
      "loss": 0.0035,
      "step": 1250
    },
    {
      "epoch": 0.0672,
      "grad_norm": 0.12312953919172287,
      "learning_rate": 4.958e-05,
      "loss": 0.005,
      "step": 1260
    },
    {
      "epoch": 0.06773333333333334,
      "grad_norm": 0.3423604667186737,
      "learning_rate": 4.957666666666667e-05,
      "loss": 0.0051,
      "step": 1270
    },
    {
      "epoch": 0.06826666666666667,
      "grad_norm": 0.09941406548023224,
      "learning_rate": 4.9573333333333335e-05,
      "loss": 0.0055,
      "step": 1280
    },
    {
      "epoch": 0.0688,
      "grad_norm": 0.33877110481262207,
      "learning_rate": 4.957e-05,
      "loss": 0.0054,
      "step": 1290
    },
    {
      "epoch": 0.06933333333333333,
      "grad_norm": 0.27459603548049927,
      "learning_rate": 4.956666666666667e-05,
      "loss": 0.0054,
      "step": 1300
    },
    {
      "epoch": 0.06986666666666666,
      "grad_norm": 0.5564568042755127,
      "learning_rate": 4.9563333333333334e-05,
      "loss": 0.0038,
      "step": 1310
    },
    {
      "epoch": 0.0704,
      "grad_norm": 0.31167498230934143,
      "learning_rate": 4.956e-05,
      "loss": 0.0056,
      "step": 1320
    },
    {
      "epoch": 0.07093333333333333,
      "grad_norm": 0.27445271611213684,
      "learning_rate": 4.9556666666666666e-05,
      "loss": 0.0024,
      "step": 1330
    },
    {
      "epoch": 0.07146666666666666,
      "grad_norm": 0.3382008969783783,
      "learning_rate": 4.955333333333333e-05,
      "loss": 0.0036,
      "step": 1340
    },
    {
      "epoch": 0.072,
      "grad_norm": 0.3013398349285126,
      "learning_rate": 4.9550000000000005e-05,
      "loss": 0.0022,
      "step": 1350
    },
    {
      "epoch": 0.07253333333333334,
      "grad_norm": 0.3652442395687103,
      "learning_rate": 4.954666666666667e-05,
      "loss": 0.0041,
      "step": 1360
    },
    {
      "epoch": 0.07306666666666667,
      "grad_norm": 0.06333420425653458,
      "learning_rate": 4.954333333333334e-05,
      "loss": 0.0041,
      "step": 1370
    },
    {
      "epoch": 0.0736,
      "grad_norm": 0.3963996171951294,
      "learning_rate": 4.9540000000000003e-05,
      "loss": 0.0037,
      "step": 1380
    },
    {
      "epoch": 0.07413333333333333,
      "grad_norm": 0.2576858103275299,
      "learning_rate": 4.953666666666667e-05,
      "loss": 0.0044,
      "step": 1390
    },
    {
      "epoch": 0.07466666666666667,
      "grad_norm": 0.3083459734916687,
      "learning_rate": 4.9533333333333336e-05,
      "loss": 0.0043,
      "step": 1400
    },
    {
      "epoch": 0.0752,
      "grad_norm": 0.5941767692565918,
      "learning_rate": 4.953e-05,
      "loss": 0.0057,
      "step": 1410
    },
    {
      "epoch": 0.07573333333333333,
      "grad_norm": 0.21049588918685913,
      "learning_rate": 4.952666666666667e-05,
      "loss": 0.0049,
      "step": 1420
    },
    {
      "epoch": 0.07626666666666666,
      "grad_norm": 0.11996743828058243,
      "learning_rate": 4.952333333333334e-05,
      "loss": 0.0049,
      "step": 1430
    },
    {
      "epoch": 0.0768,
      "grad_norm": 0.14987318217754364,
      "learning_rate": 4.952e-05,
      "loss": 0.0048,
      "step": 1440
    },
    {
      "epoch": 0.07733333333333334,
      "grad_norm": 0.30196458101272583,
      "learning_rate": 4.9516666666666666e-05,
      "loss": 0.0039,
      "step": 1450
    },
    {
      "epoch": 0.07786666666666667,
      "grad_norm": 0.21247920393943787,
      "learning_rate": 4.951333333333333e-05,
      "loss": 0.0031,
      "step": 1460
    },
    {
      "epoch": 0.0784,
      "grad_norm": 0.12156373262405396,
      "learning_rate": 4.951e-05,
      "loss": 0.0038,
      "step": 1470
    },
    {
      "epoch": 0.07893333333333333,
      "grad_norm": 0.6178957223892212,
      "learning_rate": 4.9506666666666665e-05,
      "loss": 0.0041,
      "step": 1480
    },
    {
      "epoch": 0.07946666666666667,
      "grad_norm": 0.4051819443702698,
      "learning_rate": 4.950333333333334e-05,
      "loss": 0.0048,
      "step": 1490
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.7490835785865784,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 0.0027,
      "step": 1500
    },
    {
      "epoch": 0.08053333333333333,
      "grad_norm": 0.5468097925186157,
      "learning_rate": 4.949666666666667e-05,
      "loss": 0.0029,
      "step": 1510
    },
    {
      "epoch": 0.08106666666666666,
      "grad_norm": 0.3107493221759796,
      "learning_rate": 4.9493333333333336e-05,
      "loss": 0.0045,
      "step": 1520
    },
    {
      "epoch": 0.0816,
      "grad_norm": 0.15135648846626282,
      "learning_rate": 4.949e-05,
      "loss": 0.0045,
      "step": 1530
    },
    {
      "epoch": 0.08213333333333334,
      "grad_norm": 0.03271966800093651,
      "learning_rate": 4.948666666666667e-05,
      "loss": 0.0047,
      "step": 1540
    },
    {
      "epoch": 0.08266666666666667,
      "grad_norm": 0.6028484106063843,
      "learning_rate": 4.9483333333333334e-05,
      "loss": 0.0056,
      "step": 1550
    },
    {
      "epoch": 0.0832,
      "grad_norm": 0.03129097446799278,
      "learning_rate": 4.948000000000001e-05,
      "loss": 0.0052,
      "step": 1560
    },
    {
      "epoch": 0.08373333333333334,
      "grad_norm": 0.3144543468952179,
      "learning_rate": 4.9476666666666674e-05,
      "loss": 0.0052,
      "step": 1570
    },
    {
      "epoch": 0.08426666666666667,
      "grad_norm": 0.18656767904758453,
      "learning_rate": 4.947333333333334e-05,
      "loss": 0.004,
      "step": 1580
    },
    {
      "epoch": 0.0848,
      "grad_norm": 0.43563854694366455,
      "learning_rate": 4.947e-05,
      "loss": 0.0056,
      "step": 1590
    },
    {
      "epoch": 0.08533333333333333,
      "grad_norm": 0.3380143940448761,
      "learning_rate": 4.9466666666666665e-05,
      "loss": 0.0044,
      "step": 1600
    },
    {
      "epoch": 0.08586666666666666,
      "grad_norm": 0.9198216199874878,
      "learning_rate": 4.946333333333333e-05,
      "loss": 0.0039,
      "step": 1610
    },
    {
      "epoch": 0.0864,
      "grad_norm": 0.09462462365627289,
      "learning_rate": 4.946e-05,
      "loss": 0.006,
      "step": 1620
    },
    {
      "epoch": 0.08693333333333333,
      "grad_norm": 0.46079859137535095,
      "learning_rate": 4.945666666666667e-05,
      "loss": 0.0044,
      "step": 1630
    },
    {
      "epoch": 0.08746666666666666,
      "grad_norm": 0.12444023787975311,
      "learning_rate": 4.9453333333333336e-05,
      "loss": 0.0032,
      "step": 1640
    },
    {
      "epoch": 0.088,
      "grad_norm": 0.18449123203754425,
      "learning_rate": 4.945e-05,
      "loss": 0.0049,
      "step": 1650
    },
    {
      "epoch": 0.08853333333333334,
      "grad_norm": 0.47418323159217834,
      "learning_rate": 4.944666666666667e-05,
      "loss": 0.0035,
      "step": 1660
    },
    {
      "epoch": 0.08906666666666667,
      "grad_norm": 0.12534253299236298,
      "learning_rate": 4.9443333333333335e-05,
      "loss": 0.0043,
      "step": 1670
    },
    {
      "epoch": 0.0896,
      "grad_norm": 0.06346601992845535,
      "learning_rate": 4.944e-05,
      "loss": 0.0039,
      "step": 1680
    },
    {
      "epoch": 0.09013333333333333,
      "grad_norm": 0.3944627046585083,
      "learning_rate": 4.943666666666667e-05,
      "loss": 0.0043,
      "step": 1690
    },
    {
      "epoch": 0.09066666666666667,
      "grad_norm": 0.6043752431869507,
      "learning_rate": 4.943333333333334e-05,
      "loss": 0.003,
      "step": 1700
    },
    {
      "epoch": 0.0912,
      "grad_norm": 0.42596691846847534,
      "learning_rate": 4.9430000000000006e-05,
      "loss": 0.0039,
      "step": 1710
    },
    {
      "epoch": 0.09173333333333333,
      "grad_norm": 0.2424117773771286,
      "learning_rate": 4.942666666666667e-05,
      "loss": 0.0031,
      "step": 1720
    },
    {
      "epoch": 0.09226666666666666,
      "grad_norm": 0.4632391333580017,
      "learning_rate": 4.942333333333334e-05,
      "loss": 0.0037,
      "step": 1730
    },
    {
      "epoch": 0.0928,
      "grad_norm": 0.09284757822751999,
      "learning_rate": 4.942e-05,
      "loss": 0.0031,
      "step": 1740
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 0.5572561025619507,
      "learning_rate": 4.9416666666666664e-05,
      "loss": 0.0031,
      "step": 1750
    },
    {
      "epoch": 0.09386666666666667,
      "grad_norm": 0.4345894157886505,
      "learning_rate": 4.941333333333334e-05,
      "loss": 0.0051,
      "step": 1760
    },
    {
      "epoch": 0.0944,
      "grad_norm": 0.3119674623012543,
      "learning_rate": 4.941e-05,
      "loss": 0.0028,
      "step": 1770
    },
    {
      "epoch": 0.09493333333333333,
      "grad_norm": 0.061414167284965515,
      "learning_rate": 4.940666666666667e-05,
      "loss": 0.0048,
      "step": 1780
    },
    {
      "epoch": 0.09546666666666667,
      "grad_norm": 0.12524881958961487,
      "learning_rate": 4.9403333333333335e-05,
      "loss": 0.0039,
      "step": 1790
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.3873693645000458,
      "learning_rate": 4.94e-05,
      "loss": 0.005,
      "step": 1800
    },
    {
      "epoch": 0.09653333333333333,
      "grad_norm": 0.5592594742774963,
      "learning_rate": 4.939666666666667e-05,
      "loss": 0.0053,
      "step": 1810
    },
    {
      "epoch": 0.09706666666666666,
      "grad_norm": 0.8351309299468994,
      "learning_rate": 4.9393333333333334e-05,
      "loss": 0.0038,
      "step": 1820
    },
    {
      "epoch": 0.0976,
      "grad_norm": 0.2510877549648285,
      "learning_rate": 4.939e-05,
      "loss": 0.0037,
      "step": 1830
    },
    {
      "epoch": 0.09813333333333334,
      "grad_norm": 0.4236324727535248,
      "learning_rate": 4.938666666666667e-05,
      "loss": 0.0029,
      "step": 1840
    },
    {
      "epoch": 0.09866666666666667,
      "grad_norm": 0.21224790811538696,
      "learning_rate": 4.938333333333334e-05,
      "loss": 0.0038,
      "step": 1850
    },
    {
      "epoch": 0.0992,
      "grad_norm": 0.2747144103050232,
      "learning_rate": 4.9380000000000005e-05,
      "loss": 0.0041,
      "step": 1860
    },
    {
      "epoch": 0.09973333333333333,
      "grad_norm": 0.2439025342464447,
      "learning_rate": 4.937666666666667e-05,
      "loss": 0.0043,
      "step": 1870
    },
    {
      "epoch": 0.10026666666666667,
      "grad_norm": 0.18379247188568115,
      "learning_rate": 4.937333333333334e-05,
      "loss": 0.0028,
      "step": 1880
    },
    {
      "epoch": 0.1008,
      "grad_norm": 0.15542466938495636,
      "learning_rate": 4.937e-05,
      "loss": 0.0034,
      "step": 1890
    },
    {
      "epoch": 0.10133333333333333,
      "grad_norm": 0.33639970421791077,
      "learning_rate": 4.936666666666667e-05,
      "loss": 0.0047,
      "step": 1900
    },
    {
      "epoch": 0.10186666666666666,
      "grad_norm": 0.30960288643836975,
      "learning_rate": 4.9363333333333336e-05,
      "loss": 0.0035,
      "step": 1910
    },
    {
      "epoch": 0.1024,
      "grad_norm": 0.2461477667093277,
      "learning_rate": 4.936e-05,
      "loss": 0.0056,
      "step": 1920
    },
    {
      "epoch": 0.10293333333333334,
      "grad_norm": 0.5597054958343506,
      "learning_rate": 4.935666666666667e-05,
      "loss": 0.0063,
      "step": 1930
    },
    {
      "epoch": 0.10346666666666667,
      "grad_norm": 0.5545540452003479,
      "learning_rate": 4.9353333333333334e-05,
      "loss": 0.0039,
      "step": 1940
    },
    {
      "epoch": 0.104,
      "grad_norm": 0.12177547812461853,
      "learning_rate": 4.935e-05,
      "loss": 0.0025,
      "step": 1950
    },
    {
      "epoch": 0.10453333333333334,
      "grad_norm": 0.24328263103961945,
      "learning_rate": 4.9346666666666666e-05,
      "loss": 0.0031,
      "step": 1960
    },
    {
      "epoch": 0.10506666666666667,
      "grad_norm": 0.033503759652376175,
      "learning_rate": 4.934333333333334e-05,
      "loss": 0.004,
      "step": 1970
    },
    {
      "epoch": 0.1056,
      "grad_norm": 0.42696475982666016,
      "learning_rate": 4.9340000000000005e-05,
      "loss": 0.0034,
      "step": 1980
    },
    {
      "epoch": 0.10613333333333333,
      "grad_norm": 0.24844080209732056,
      "learning_rate": 4.933666666666667e-05,
      "loss": 0.0042,
      "step": 1990
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.3045562207698822,
      "learning_rate": 4.933333333333334e-05,
      "loss": 0.0052,
      "step": 2000
    },
    {
      "epoch": 0.1072,
      "grad_norm": 0.06645259261131287,
      "learning_rate": 4.9330000000000004e-05,
      "loss": 0.0034,
      "step": 2010
    },
    {
      "epoch": 0.10773333333333333,
      "grad_norm": 0.03336377814412117,
      "learning_rate": 4.932666666666667e-05,
      "loss": 0.0038,
      "step": 2020
    },
    {
      "epoch": 0.10826666666666666,
      "grad_norm": 0.29844555258750916,
      "learning_rate": 4.9323333333333336e-05,
      "loss": 0.0035,
      "step": 2030
    },
    {
      "epoch": 0.1088,
      "grad_norm": 0.12149020284414291,
      "learning_rate": 4.932e-05,
      "loss": 0.0042,
      "step": 2040
    },
    {
      "epoch": 0.10933333333333334,
      "grad_norm": 0.48313117027282715,
      "learning_rate": 4.931666666666667e-05,
      "loss": 0.0036,
      "step": 2050
    },
    {
      "epoch": 0.10986666666666667,
      "grad_norm": 0.28074365854263306,
      "learning_rate": 4.9313333333333334e-05,
      "loss": 0.0027,
      "step": 2060
    },
    {
      "epoch": 0.1104,
      "grad_norm": 0.43047720193862915,
      "learning_rate": 4.931e-05,
      "loss": 0.0045,
      "step": 2070
    },
    {
      "epoch": 0.11093333333333333,
      "grad_norm": 0.35881584882736206,
      "learning_rate": 4.930666666666667e-05,
      "loss": 0.0022,
      "step": 2080
    },
    {
      "epoch": 0.11146666666666667,
      "grad_norm": 0.5524104237556458,
      "learning_rate": 4.930333333333333e-05,
      "loss": 0.0038,
      "step": 2090
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.06398741155862808,
      "learning_rate": 4.93e-05,
      "loss": 0.0041,
      "step": 2100
    },
    {
      "epoch": 0.11253333333333333,
      "grad_norm": 0.06644928455352783,
      "learning_rate": 4.929666666666667e-05,
      "loss": 0.0034,
      "step": 2110
    },
    {
      "epoch": 0.11306666666666666,
      "grad_norm": 0.36990687251091003,
      "learning_rate": 4.929333333333334e-05,
      "loss": 0.0047,
      "step": 2120
    },
    {
      "epoch": 0.1136,
      "grad_norm": 0.030304372310638428,
      "learning_rate": 4.9290000000000004e-05,
      "loss": 0.0041,
      "step": 2130
    },
    {
      "epoch": 0.11413333333333334,
      "grad_norm": 0.03709119185805321,
      "learning_rate": 4.928666666666667e-05,
      "loss": 0.0042,
      "step": 2140
    },
    {
      "epoch": 0.11466666666666667,
      "grad_norm": 0.37041303515434265,
      "learning_rate": 4.9283333333333336e-05,
      "loss": 0.0038,
      "step": 2150
    },
    {
      "epoch": 0.1152,
      "grad_norm": 0.3639872372150421,
      "learning_rate": 4.928e-05,
      "loss": 0.0053,
      "step": 2160
    },
    {
      "epoch": 0.11573333333333333,
      "grad_norm": 0.03136374428868294,
      "learning_rate": 4.927666666666667e-05,
      "loss": 0.0049,
      "step": 2170
    },
    {
      "epoch": 0.11626666666666667,
      "grad_norm": 0.5605145692825317,
      "learning_rate": 4.9273333333333335e-05,
      "loss": 0.0029,
      "step": 2180
    },
    {
      "epoch": 0.1168,
      "grad_norm": 0.6166796088218689,
      "learning_rate": 4.927000000000001e-05,
      "loss": 0.0034,
      "step": 2190
    },
    {
      "epoch": 0.11733333333333333,
      "grad_norm": 0.6385692358016968,
      "learning_rate": 4.926666666666667e-05,
      "loss": 0.0033,
      "step": 2200
    },
    {
      "epoch": 0.11786666666666666,
      "grad_norm": 0.3116786777973175,
      "learning_rate": 4.926333333333333e-05,
      "loss": 0.0036,
      "step": 2210
    },
    {
      "epoch": 0.1184,
      "grad_norm": 0.18635427951812744,
      "learning_rate": 4.926e-05,
      "loss": 0.0043,
      "step": 2220
    },
    {
      "epoch": 0.11893333333333334,
      "grad_norm": 0.43257665634155273,
      "learning_rate": 4.9256666666666665e-05,
      "loss": 0.005,
      "step": 2230
    },
    {
      "epoch": 0.11946666666666667,
      "grad_norm": 0.8669288158416748,
      "learning_rate": 4.925333333333333e-05,
      "loss": 0.0044,
      "step": 2240
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.30761709809303284,
      "learning_rate": 4.9250000000000004e-05,
      "loss": 0.0038,
      "step": 2250
    },
    {
      "epoch": 0.12053333333333334,
      "grad_norm": 0.06545573472976685,
      "learning_rate": 4.924666666666667e-05,
      "loss": 0.0041,
      "step": 2260
    },
    {
      "epoch": 0.12106666666666667,
      "grad_norm": 0.5582100749015808,
      "learning_rate": 4.924333333333334e-05,
      "loss": 0.0039,
      "step": 2270
    },
    {
      "epoch": 0.1216,
      "grad_norm": 0.8417907953262329,
      "learning_rate": 4.924e-05,
      "loss": 0.0032,
      "step": 2280
    },
    {
      "epoch": 0.12213333333333333,
      "grad_norm": 0.4977801442146301,
      "learning_rate": 4.923666666666667e-05,
      "loss": 0.0055,
      "step": 2290
    },
    {
      "epoch": 0.12266666666666666,
      "grad_norm": 0.21874301135540009,
      "learning_rate": 4.9233333333333335e-05,
      "loss": 0.0039,
      "step": 2300
    },
    {
      "epoch": 0.1232,
      "grad_norm": 0.15491671860218048,
      "learning_rate": 4.923e-05,
      "loss": 0.0042,
      "step": 2310
    },
    {
      "epoch": 0.12373333333333333,
      "grad_norm": 0.46842721104621887,
      "learning_rate": 4.9226666666666674e-05,
      "loss": 0.0041,
      "step": 2320
    },
    {
      "epoch": 0.12426666666666666,
      "grad_norm": 0.6045752763748169,
      "learning_rate": 4.922333333333334e-05,
      "loss": 0.0041,
      "step": 2330
    },
    {
      "epoch": 0.1248,
      "grad_norm": 0.4332546889781952,
      "learning_rate": 4.9220000000000006e-05,
      "loss": 0.0053,
      "step": 2340
    },
    {
      "epoch": 0.12533333333333332,
      "grad_norm": 0.09116587787866592,
      "learning_rate": 4.9216666666666666e-05,
      "loss": 0.0036,
      "step": 2350
    },
    {
      "epoch": 0.12586666666666665,
      "grad_norm": 0.5472914576530457,
      "learning_rate": 4.921333333333333e-05,
      "loss": 0.0034,
      "step": 2360
    },
    {
      "epoch": 0.1264,
      "grad_norm": 0.09260374307632446,
      "learning_rate": 4.921e-05,
      "loss": 0.0041,
      "step": 2370
    },
    {
      "epoch": 0.12693333333333334,
      "grad_norm": 0.21896617114543915,
      "learning_rate": 4.9206666666666664e-05,
      "loss": 0.0045,
      "step": 2380
    },
    {
      "epoch": 0.12746666666666667,
      "grad_norm": 0.18837322294712067,
      "learning_rate": 4.920333333333334e-05,
      "loss": 0.0032,
      "step": 2390
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.18037813901901245,
      "learning_rate": 4.92e-05,
      "loss": 0.0027,
      "step": 2400
    },
    {
      "epoch": 0.12853333333333333,
      "grad_norm": 0.4035467803478241,
      "learning_rate": 4.919666666666667e-05,
      "loss": 0.0045,
      "step": 2410
    },
    {
      "epoch": 0.12906666666666666,
      "grad_norm": 0.3615427613258362,
      "learning_rate": 4.9193333333333336e-05,
      "loss": 0.0029,
      "step": 2420
    },
    {
      "epoch": 0.1296,
      "grad_norm": 0.06076787784695625,
      "learning_rate": 4.919e-05,
      "loss": 0.0039,
      "step": 2430
    },
    {
      "epoch": 0.13013333333333332,
      "grad_norm": 0.41634413599967957,
      "learning_rate": 4.918666666666667e-05,
      "loss": 0.004,
      "step": 2440
    },
    {
      "epoch": 0.13066666666666665,
      "grad_norm": 0.041011616587638855,
      "learning_rate": 4.9183333333333334e-05,
      "loss": 0.0026,
      "step": 2450
    },
    {
      "epoch": 0.1312,
      "grad_norm": 0.27344560623168945,
      "learning_rate": 4.918000000000001e-05,
      "loss": 0.0038,
      "step": 2460
    },
    {
      "epoch": 0.13173333333333334,
      "grad_norm": 0.06173329055309296,
      "learning_rate": 4.917666666666667e-05,
      "loss": 0.0039,
      "step": 2470
    },
    {
      "epoch": 0.13226666666666667,
      "grad_norm": 0.24125270545482635,
      "learning_rate": 4.917333333333334e-05,
      "loss": 0.0038,
      "step": 2480
    },
    {
      "epoch": 0.1328,
      "grad_norm": 0.07850325107574463,
      "learning_rate": 4.9170000000000005e-05,
      "loss": 0.003,
      "step": 2490
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.18097446858882904,
      "learning_rate": 4.9166666666666665e-05,
      "loss": 0.0033,
      "step": 2500
    },
    {
      "epoch": 0.13386666666666666,
      "grad_norm": 0.7065542340278625,
      "learning_rate": 4.916333333333333e-05,
      "loss": 0.0049,
      "step": 2510
    },
    {
      "epoch": 0.1344,
      "grad_norm": 0.12041378766298294,
      "learning_rate": 4.9160000000000004e-05,
      "loss": 0.0046,
      "step": 2520
    },
    {
      "epoch": 0.13493333333333332,
      "grad_norm": 0.030573487281799316,
      "learning_rate": 4.915666666666667e-05,
      "loss": 0.007,
      "step": 2530
    },
    {
      "epoch": 0.13546666666666668,
      "grad_norm": 0.060306843370199203,
      "learning_rate": 4.9153333333333336e-05,
      "loss": 0.0036,
      "step": 2540
    },
    {
      "epoch": 0.136,
      "grad_norm": 0.3706536591053009,
      "learning_rate": 4.915e-05,
      "loss": 0.0041,
      "step": 2550
    },
    {
      "epoch": 0.13653333333333334,
      "grad_norm": 0.5888859629631042,
      "learning_rate": 4.914666666666667e-05,
      "loss": 0.0036,
      "step": 2560
    },
    {
      "epoch": 0.13706666666666667,
      "grad_norm": 0.1888168454170227,
      "learning_rate": 4.9143333333333334e-05,
      "loss": 0.0042,
      "step": 2570
    },
    {
      "epoch": 0.1376,
      "grad_norm": 0.06309842318296432,
      "learning_rate": 4.914e-05,
      "loss": 0.0047,
      "step": 2580
    },
    {
      "epoch": 0.13813333333333333,
      "grad_norm": 0.09789734333753586,
      "learning_rate": 4.9136666666666667e-05,
      "loss": 0.0033,
      "step": 2590
    },
    {
      "epoch": 0.13866666666666666,
      "grad_norm": 0.42204082012176514,
      "learning_rate": 4.913333333333334e-05,
      "loss": 0.0045,
      "step": 2600
    },
    {
      "epoch": 0.1392,
      "grad_norm": 0.30839425325393677,
      "learning_rate": 4.9130000000000006e-05,
      "loss": 0.0036,
      "step": 2610
    },
    {
      "epoch": 0.13973333333333332,
      "grad_norm": 0.24912714958190918,
      "learning_rate": 4.912666666666667e-05,
      "loss": 0.0035,
      "step": 2620
    },
    {
      "epoch": 0.14026666666666668,
      "grad_norm": 0.15177780389785767,
      "learning_rate": 4.912333333333334e-05,
      "loss": 0.0038,
      "step": 2630
    },
    {
      "epoch": 0.1408,
      "grad_norm": 0.2723580300807953,
      "learning_rate": 4.9120000000000004e-05,
      "loss": 0.0042,
      "step": 2640
    },
    {
      "epoch": 0.14133333333333334,
      "grad_norm": 0.18770769238471985,
      "learning_rate": 4.9116666666666663e-05,
      "loss": 0.0051,
      "step": 2650
    },
    {
      "epoch": 0.14186666666666667,
      "grad_norm": 0.28317028284072876,
      "learning_rate": 4.9113333333333336e-05,
      "loss": 0.0045,
      "step": 2660
    },
    {
      "epoch": 0.1424,
      "grad_norm": 0.49603214859962463,
      "learning_rate": 4.911e-05,
      "loss": 0.0057,
      "step": 2670
    },
    {
      "epoch": 0.14293333333333333,
      "grad_norm": 0.30523812770843506,
      "learning_rate": 4.910666666666667e-05,
      "loss": 0.0042,
      "step": 2680
    },
    {
      "epoch": 0.14346666666666666,
      "grad_norm": 0.12410331517457962,
      "learning_rate": 4.9103333333333335e-05,
      "loss": 0.004,
      "step": 2690
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.12037862092256546,
      "learning_rate": 4.91e-05,
      "loss": 0.0043,
      "step": 2700
    },
    {
      "epoch": 0.14453333333333335,
      "grad_norm": 0.2471521943807602,
      "learning_rate": 4.909666666666667e-05,
      "loss": 0.0057,
      "step": 2710
    },
    {
      "epoch": 0.14506666666666668,
      "grad_norm": 0.27262264490127563,
      "learning_rate": 4.909333333333333e-05,
      "loss": 0.003,
      "step": 2720
    },
    {
      "epoch": 0.1456,
      "grad_norm": 0.39983779191970825,
      "learning_rate": 4.9090000000000006e-05,
      "loss": 0.0059,
      "step": 2730
    },
    {
      "epoch": 0.14613333333333334,
      "grad_norm": 0.030674856156110764,
      "learning_rate": 4.908666666666667e-05,
      "loss": 0.0039,
      "step": 2740
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 0.21384604275226593,
      "learning_rate": 4.908333333333334e-05,
      "loss": 0.004,
      "step": 2750
    },
    {
      "epoch": 0.1472,
      "grad_norm": 0.45838627219200134,
      "learning_rate": 4.9080000000000004e-05,
      "loss": 0.0043,
      "step": 2760
    },
    {
      "epoch": 0.14773333333333333,
      "grad_norm": 0.01072155125439167,
      "learning_rate": 4.907666666666667e-05,
      "loss": 0.0033,
      "step": 2770
    },
    {
      "epoch": 0.14826666666666666,
      "grad_norm": 0.7656282186508179,
      "learning_rate": 4.907333333333334e-05,
      "loss": 0.0043,
      "step": 2780
    },
    {
      "epoch": 0.1488,
      "grad_norm": 0.02992396056652069,
      "learning_rate": 4.907e-05,
      "loss": 0.0031,
      "step": 2790
    },
    {
      "epoch": 0.14933333333333335,
      "grad_norm": 0.5185503363609314,
      "learning_rate": 4.906666666666667e-05,
      "loss": 0.0051,
      "step": 2800
    },
    {
      "epoch": 0.14986666666666668,
      "grad_norm": 0.18342965841293335,
      "learning_rate": 4.9063333333333335e-05,
      "loss": 0.0053,
      "step": 2810
    },
    {
      "epoch": 0.1504,
      "grad_norm": 0.18567906320095062,
      "learning_rate": 4.906e-05,
      "loss": 0.0034,
      "step": 2820
    },
    {
      "epoch": 0.15093333333333334,
      "grad_norm": 0.21422387659549713,
      "learning_rate": 4.905666666666667e-05,
      "loss": 0.004,
      "step": 2830
    },
    {
      "epoch": 0.15146666666666667,
      "grad_norm": 0.10007517039775848,
      "learning_rate": 4.9053333333333333e-05,
      "loss": 0.0031,
      "step": 2840
    },
    {
      "epoch": 0.152,
      "grad_norm": 0.3061617612838745,
      "learning_rate": 4.905e-05,
      "loss": 0.005,
      "step": 2850
    },
    {
      "epoch": 0.15253333333333333,
      "grad_norm": 0.49093368649482727,
      "learning_rate": 4.9046666666666666e-05,
      "loss": 0.0061,
      "step": 2860
    },
    {
      "epoch": 0.15306666666666666,
      "grad_norm": 0.5510103702545166,
      "learning_rate": 4.904333333333334e-05,
      "loss": 0.0027,
      "step": 2870
    },
    {
      "epoch": 0.1536,
      "grad_norm": 0.523903489112854,
      "learning_rate": 4.9040000000000005e-05,
      "loss": 0.0054,
      "step": 2880
    },
    {
      "epoch": 0.15413333333333334,
      "grad_norm": 0.4575856924057007,
      "learning_rate": 4.903666666666667e-05,
      "loss": 0.0047,
      "step": 2890
    },
    {
      "epoch": 0.15466666666666667,
      "grad_norm": 0.5166348814964294,
      "learning_rate": 4.903333333333334e-05,
      "loss": 0.0041,
      "step": 2900
    },
    {
      "epoch": 0.1552,
      "grad_norm": 0.013602375984191895,
      "learning_rate": 4.903e-05,
      "loss": 0.0039,
      "step": 2910
    },
    {
      "epoch": 0.15573333333333333,
      "grad_norm": 0.4802461266517639,
      "learning_rate": 4.902666666666667e-05,
      "loss": 0.0044,
      "step": 2920
    },
    {
      "epoch": 0.15626666666666666,
      "grad_norm": 0.27523988485336304,
      "learning_rate": 4.9023333333333335e-05,
      "loss": 0.0025,
      "step": 2930
    },
    {
      "epoch": 0.1568,
      "grad_norm": 0.5769439935684204,
      "learning_rate": 4.902e-05,
      "loss": 0.003,
      "step": 2940
    },
    {
      "epoch": 0.15733333333333333,
      "grad_norm": 0.2725667953491211,
      "learning_rate": 4.901666666666667e-05,
      "loss": 0.0051,
      "step": 2950
    },
    {
      "epoch": 0.15786666666666666,
      "grad_norm": 0.4741979241371155,
      "learning_rate": 4.9013333333333334e-05,
      "loss": 0.0044,
      "step": 2960
    },
    {
      "epoch": 0.1584,
      "grad_norm": 0.11994264274835587,
      "learning_rate": 4.901e-05,
      "loss": 0.003,
      "step": 2970
    },
    {
      "epoch": 0.15893333333333334,
      "grad_norm": 0.4483047127723694,
      "learning_rate": 4.9006666666666666e-05,
      "loss": 0.0024,
      "step": 2980
    },
    {
      "epoch": 0.15946666666666667,
      "grad_norm": 0.5875251293182373,
      "learning_rate": 4.900333333333333e-05,
      "loss": 0.0038,
      "step": 2990
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.6770642399787903,
      "learning_rate": 4.9e-05,
      "loss": 0.0042,
      "step": 3000
    },
    {
      "epoch": 0.16053333333333333,
      "grad_norm": 0.2789936065673828,
      "learning_rate": 4.899666666666667e-05,
      "loss": 0.0036,
      "step": 3010
    },
    {
      "epoch": 0.16106666666666666,
      "grad_norm": 0.18367356061935425,
      "learning_rate": 4.899333333333334e-05,
      "loss": 0.0045,
      "step": 3020
    },
    {
      "epoch": 0.1616,
      "grad_norm": 0.36881527304649353,
      "learning_rate": 4.8990000000000004e-05,
      "loss": 0.0044,
      "step": 3030
    },
    {
      "epoch": 0.16213333333333332,
      "grad_norm": 0.6234911680221558,
      "learning_rate": 4.898666666666667e-05,
      "loss": 0.0046,
      "step": 3040
    },
    {
      "epoch": 0.16266666666666665,
      "grad_norm": 0.06277990341186523,
      "learning_rate": 4.8983333333333336e-05,
      "loss": 0.0046,
      "step": 3050
    },
    {
      "epoch": 0.1632,
      "grad_norm": 0.6766066551208496,
      "learning_rate": 4.898e-05,
      "loss": 0.0042,
      "step": 3060
    },
    {
      "epoch": 0.16373333333333334,
      "grad_norm": 0.12297426164150238,
      "learning_rate": 4.897666666666667e-05,
      "loss": 0.0045,
      "step": 3070
    },
    {
      "epoch": 0.16426666666666667,
      "grad_norm": 0.4931813180446625,
      "learning_rate": 4.897333333333334e-05,
      "loss": 0.0037,
      "step": 3080
    },
    {
      "epoch": 0.1648,
      "grad_norm": 0.06786227971315384,
      "learning_rate": 4.897000000000001e-05,
      "loss": 0.0038,
      "step": 3090
    },
    {
      "epoch": 0.16533333333333333,
      "grad_norm": 0.11979550868272781,
      "learning_rate": 4.8966666666666667e-05,
      "loss": 0.0043,
      "step": 3100
    },
    {
      "epoch": 0.16586666666666666,
      "grad_norm": 0.1613757312297821,
      "learning_rate": 4.896333333333333e-05,
      "loss": 0.0056,
      "step": 3110
    },
    {
      "epoch": 0.1664,
      "grad_norm": 0.5562595129013062,
      "learning_rate": 4.896e-05,
      "loss": 0.0049,
      "step": 3120
    },
    {
      "epoch": 0.16693333333333332,
      "grad_norm": 0.4869650900363922,
      "learning_rate": 4.8956666666666665e-05,
      "loss": 0.0044,
      "step": 3130
    },
    {
      "epoch": 0.16746666666666668,
      "grad_norm": 0.48804062604904175,
      "learning_rate": 4.895333333333333e-05,
      "loss": 0.0037,
      "step": 3140
    },
    {
      "epoch": 0.168,
      "grad_norm": 0.006729585584253073,
      "learning_rate": 4.8950000000000004e-05,
      "loss": 0.0052,
      "step": 3150
    },
    {
      "epoch": 0.16853333333333334,
      "grad_norm": 0.12369106709957123,
      "learning_rate": 4.894666666666667e-05,
      "loss": 0.004,
      "step": 3160
    },
    {
      "epoch": 0.16906666666666667,
      "grad_norm": 0.12154705077409744,
      "learning_rate": 4.8943333333333336e-05,
      "loss": 0.0038,
      "step": 3170
    },
    {
      "epoch": 0.1696,
      "grad_norm": 0.06507305800914764,
      "learning_rate": 4.894e-05,
      "loss": 0.0034,
      "step": 3180
    },
    {
      "epoch": 0.17013333333333333,
      "grad_norm": 0.6442355513572693,
      "learning_rate": 4.893666666666667e-05,
      "loss": 0.005,
      "step": 3190
    },
    {
      "epoch": 0.17066666666666666,
      "grad_norm": 0.5556140542030334,
      "learning_rate": 4.8933333333333335e-05,
      "loss": 0.005,
      "step": 3200
    },
    {
      "epoch": 0.1712,
      "grad_norm": 0.24684476852416992,
      "learning_rate": 4.893e-05,
      "loss": 0.0047,
      "step": 3210
    },
    {
      "epoch": 0.17173333333333332,
      "grad_norm": 0.1541629433631897,
      "learning_rate": 4.8926666666666674e-05,
      "loss": 0.0058,
      "step": 3220
    },
    {
      "epoch": 0.17226666666666668,
      "grad_norm": 0.5291772484779358,
      "learning_rate": 4.892333333333334e-05,
      "loss": 0.0038,
      "step": 3230
    },
    {
      "epoch": 0.1728,
      "grad_norm": 0.09601937979459763,
      "learning_rate": 4.8920000000000006e-05,
      "loss": 0.0036,
      "step": 3240
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 0.9362759590148926,
      "learning_rate": 4.891666666666667e-05,
      "loss": 0.0047,
      "step": 3250
    },
    {
      "epoch": 0.17386666666666667,
      "grad_norm": 0.8351048231124878,
      "learning_rate": 4.891333333333333e-05,
      "loss": 0.0041,
      "step": 3260
    },
    {
      "epoch": 0.1744,
      "grad_norm": 0.3635914921760559,
      "learning_rate": 4.891e-05,
      "loss": 0.0049,
      "step": 3270
    },
    {
      "epoch": 0.17493333333333333,
      "grad_norm": 0.005759476218372583,
      "learning_rate": 4.890666666666667e-05,
      "loss": 0.004,
      "step": 3280
    },
    {
      "epoch": 0.17546666666666666,
      "grad_norm": 0.30419668555259705,
      "learning_rate": 4.890333333333334e-05,
      "loss": 0.0047,
      "step": 3290
    },
    {
      "epoch": 0.176,
      "grad_norm": 0.3909950852394104,
      "learning_rate": 4.89e-05,
      "loss": 0.0027,
      "step": 3300
    },
    {
      "epoch": 0.17653333333333332,
      "grad_norm": 0.4491715729236603,
      "learning_rate": 4.889666666666667e-05,
      "loss": 0.0046,
      "step": 3310
    },
    {
      "epoch": 0.17706666666666668,
      "grad_norm": 0.21177665889263153,
      "learning_rate": 4.8893333333333335e-05,
      "loss": 0.0052,
      "step": 3320
    },
    {
      "epoch": 0.1776,
      "grad_norm": 0.306650310754776,
      "learning_rate": 4.889e-05,
      "loss": 0.0045,
      "step": 3330
    },
    {
      "epoch": 0.17813333333333334,
      "grad_norm": 0.15533150732517242,
      "learning_rate": 4.888666666666667e-05,
      "loss": 0.0043,
      "step": 3340
    },
    {
      "epoch": 0.17866666666666667,
      "grad_norm": 0.37055182456970215,
      "learning_rate": 4.8883333333333333e-05,
      "loss": 0.0046,
      "step": 3350
    },
    {
      "epoch": 0.1792,
      "grad_norm": 0.3105093836784363,
      "learning_rate": 4.8880000000000006e-05,
      "loss": 0.0049,
      "step": 3360
    },
    {
      "epoch": 0.17973333333333333,
      "grad_norm": 0.5253923535346985,
      "learning_rate": 4.887666666666667e-05,
      "loss": 0.0045,
      "step": 3370
    },
    {
      "epoch": 0.18026666666666666,
      "grad_norm": 0.39745521545410156,
      "learning_rate": 4.887333333333334e-05,
      "loss": 0.0039,
      "step": 3380
    },
    {
      "epoch": 0.1808,
      "grad_norm": 0.3055535554885864,
      "learning_rate": 4.8870000000000005e-05,
      "loss": 0.0038,
      "step": 3390
    },
    {
      "epoch": 0.18133333333333335,
      "grad_norm": 0.9851035475730896,
      "learning_rate": 4.886666666666667e-05,
      "loss": 0.0022,
      "step": 3400
    },
    {
      "epoch": 0.18186666666666668,
      "grad_norm": 0.4492206275463104,
      "learning_rate": 4.886333333333333e-05,
      "loss": 0.0067,
      "step": 3410
    },
    {
      "epoch": 0.1824,
      "grad_norm": 0.30234506726264954,
      "learning_rate": 4.886e-05,
      "loss": 0.0044,
      "step": 3420
    },
    {
      "epoch": 0.18293333333333334,
      "grad_norm": 0.11967209726572037,
      "learning_rate": 4.885666666666667e-05,
      "loss": 0.0038,
      "step": 3430
    },
    {
      "epoch": 0.18346666666666667,
      "grad_norm": 0.03027794137597084,
      "learning_rate": 4.8853333333333335e-05,
      "loss": 0.0034,
      "step": 3440
    },
    {
      "epoch": 0.184,
      "grad_norm": 0.09096688777208328,
      "learning_rate": 4.885e-05,
      "loss": 0.0041,
      "step": 3450
    },
    {
      "epoch": 0.18453333333333333,
      "grad_norm": 0.3641011416912079,
      "learning_rate": 4.884666666666667e-05,
      "loss": 0.0036,
      "step": 3460
    },
    {
      "epoch": 0.18506666666666666,
      "grad_norm": 0.809080958366394,
      "learning_rate": 4.8843333333333334e-05,
      "loss": 0.0031,
      "step": 3470
    },
    {
      "epoch": 0.1856,
      "grad_norm": 0.4229167103767395,
      "learning_rate": 4.884e-05,
      "loss": 0.0039,
      "step": 3480
    },
    {
      "epoch": 0.18613333333333335,
      "grad_norm": 0.07396593689918518,
      "learning_rate": 4.8836666666666666e-05,
      "loss": 0.0048,
      "step": 3490
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.18184569478034973,
      "learning_rate": 4.883333333333334e-05,
      "loss": 0.0031,
      "step": 3500
    },
    {
      "epoch": 0.1872,
      "grad_norm": 0.43180030584335327,
      "learning_rate": 4.8830000000000005e-05,
      "loss": 0.0034,
      "step": 3510
    },
    {
      "epoch": 0.18773333333333334,
      "grad_norm": 0.5232066512107849,
      "learning_rate": 4.882666666666667e-05,
      "loss": 0.0041,
      "step": 3520
    },
    {
      "epoch": 0.18826666666666667,
      "grad_norm": 0.40011975169181824,
      "learning_rate": 4.882333333333334e-05,
      "loss": 0.0034,
      "step": 3530
    },
    {
      "epoch": 0.1888,
      "grad_norm": 0.40136656165122986,
      "learning_rate": 4.8820000000000004e-05,
      "loss": 0.0034,
      "step": 3540
    },
    {
      "epoch": 0.18933333333333333,
      "grad_norm": 0.09154451638460159,
      "learning_rate": 4.881666666666667e-05,
      "loss": 0.005,
      "step": 3550
    },
    {
      "epoch": 0.18986666666666666,
      "grad_norm": 0.5525688529014587,
      "learning_rate": 4.8813333333333336e-05,
      "loss": 0.0039,
      "step": 3560
    },
    {
      "epoch": 0.1904,
      "grad_norm": 0.18828175961971283,
      "learning_rate": 4.881e-05,
      "loss": 0.0031,
      "step": 3570
    },
    {
      "epoch": 0.19093333333333334,
      "grad_norm": 0.03130870312452316,
      "learning_rate": 4.880666666666667e-05,
      "loss": 0.0039,
      "step": 3580
    },
    {
      "epoch": 0.19146666666666667,
      "grad_norm": 0.3046833574771881,
      "learning_rate": 4.8803333333333334e-05,
      "loss": 0.0039,
      "step": 3590
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.031594619154930115,
      "learning_rate": 4.88e-05,
      "loss": 0.0046,
      "step": 3600
    },
    {
      "epoch": 0.19253333333333333,
      "grad_norm": 0.1921442598104477,
      "learning_rate": 4.8796666666666666e-05,
      "loss": 0.0038,
      "step": 3610
    },
    {
      "epoch": 0.19306666666666666,
      "grad_norm": 0.39935529232025146,
      "learning_rate": 4.879333333333333e-05,
      "loss": 0.0034,
      "step": 3620
    },
    {
      "epoch": 0.1936,
      "grad_norm": 0.5489963889122009,
      "learning_rate": 4.8790000000000006e-05,
      "loss": 0.004,
      "step": 3630
    },
    {
      "epoch": 0.19413333333333332,
      "grad_norm": 0.1839723289012909,
      "learning_rate": 4.878666666666667e-05,
      "loss": 0.0049,
      "step": 3640
    },
    {
      "epoch": 0.19466666666666665,
      "grad_norm": 0.06216820329427719,
      "learning_rate": 4.878333333333334e-05,
      "loss": 0.0039,
      "step": 3650
    },
    {
      "epoch": 0.1952,
      "grad_norm": 0.5501338839530945,
      "learning_rate": 4.8780000000000004e-05,
      "loss": 0.0042,
      "step": 3660
    },
    {
      "epoch": 0.19573333333333334,
      "grad_norm": 0.09581877291202545,
      "learning_rate": 4.877666666666667e-05,
      "loss": 0.0036,
      "step": 3670
    },
    {
      "epoch": 0.19626666666666667,
      "grad_norm": 0.015042873099446297,
      "learning_rate": 4.8773333333333336e-05,
      "loss": 0.0032,
      "step": 3680
    },
    {
      "epoch": 0.1968,
      "grad_norm": 0.5728619694709778,
      "learning_rate": 4.877e-05,
      "loss": 0.0058,
      "step": 3690
    },
    {
      "epoch": 0.19733333333333333,
      "grad_norm": 0.17876283824443817,
      "learning_rate": 4.876666666666667e-05,
      "loss": 0.0034,
      "step": 3700
    },
    {
      "epoch": 0.19786666666666666,
      "grad_norm": 0.4997778832912445,
      "learning_rate": 4.8763333333333335e-05,
      "loss": 0.004,
      "step": 3710
    },
    {
      "epoch": 0.1984,
      "grad_norm": 0.4257580041885376,
      "learning_rate": 4.876e-05,
      "loss": 0.0039,
      "step": 3720
    },
    {
      "epoch": 0.19893333333333332,
      "grad_norm": 0.7889503240585327,
      "learning_rate": 4.875666666666667e-05,
      "loss": 0.0038,
      "step": 3730
    },
    {
      "epoch": 0.19946666666666665,
      "grad_norm": 0.11890686303377151,
      "learning_rate": 4.875333333333333e-05,
      "loss": 0.0049,
      "step": 3740
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.030740967020392418,
      "learning_rate": 4.875e-05,
      "loss": 0.0052,
      "step": 3750
    },
    {
      "epoch": 0.20053333333333334,
      "grad_norm": 0.21340884268283844,
      "learning_rate": 4.8746666666666665e-05,
      "loss": 0.0041,
      "step": 3760
    },
    {
      "epoch": 0.20106666666666667,
      "grad_norm": 0.5493481755256653,
      "learning_rate": 4.874333333333334e-05,
      "loss": 0.0062,
      "step": 3770
    },
    {
      "epoch": 0.2016,
      "grad_norm": 0.33794617652893066,
      "learning_rate": 4.8740000000000004e-05,
      "loss": 0.0049,
      "step": 3780
    },
    {
      "epoch": 0.20213333333333333,
      "grad_norm": 0.18483182787895203,
      "learning_rate": 4.873666666666667e-05,
      "loss": 0.0046,
      "step": 3790
    },
    {
      "epoch": 0.20266666666666666,
      "grad_norm": 0.1854092925786972,
      "learning_rate": 4.8733333333333337e-05,
      "loss": 0.0046,
      "step": 3800
    },
    {
      "epoch": 0.2032,
      "grad_norm": 0.5515545010566711,
      "learning_rate": 4.873e-05,
      "loss": 0.0059,
      "step": 3810
    },
    {
      "epoch": 0.20373333333333332,
      "grad_norm": 0.4000055193901062,
      "learning_rate": 4.872666666666667e-05,
      "loss": 0.0051,
      "step": 3820
    },
    {
      "epoch": 0.20426666666666668,
      "grad_norm": 0.5451192259788513,
      "learning_rate": 4.8723333333333335e-05,
      "loss": 0.0036,
      "step": 3830
    },
    {
      "epoch": 0.2048,
      "grad_norm": 0.24110957980155945,
      "learning_rate": 4.872000000000001e-05,
      "loss": 0.0034,
      "step": 3840
    },
    {
      "epoch": 0.20533333333333334,
      "grad_norm": 0.244215190410614,
      "learning_rate": 4.8716666666666674e-05,
      "loss": 0.0039,
      "step": 3850
    },
    {
      "epoch": 0.20586666666666667,
      "grad_norm": 0.2103666067123413,
      "learning_rate": 4.871333333333333e-05,
      "loss": 0.0025,
      "step": 3860
    },
    {
      "epoch": 0.2064,
      "grad_norm": 0.15030236542224884,
      "learning_rate": 4.871e-05,
      "loss": 0.0034,
      "step": 3870
    },
    {
      "epoch": 0.20693333333333333,
      "grad_norm": 0.4814237356185913,
      "learning_rate": 4.8706666666666666e-05,
      "loss": 0.0042,
      "step": 3880
    },
    {
      "epoch": 0.20746666666666666,
      "grad_norm": 0.1506136655807495,
      "learning_rate": 4.870333333333333e-05,
      "loss": 0.0038,
      "step": 3890
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.15111961960792542,
      "learning_rate": 4.87e-05,
      "loss": 0.0027,
      "step": 3900
    },
    {
      "epoch": 0.20853333333333332,
      "grad_norm": 0.037439871579408646,
      "learning_rate": 4.869666666666667e-05,
      "loss": 0.003,
      "step": 3910
    },
    {
      "epoch": 0.20906666666666668,
      "grad_norm": 0.2730398178100586,
      "learning_rate": 4.869333333333334e-05,
      "loss": 0.0037,
      "step": 3920
    },
    {
      "epoch": 0.2096,
      "grad_norm": 0.26993557810783386,
      "learning_rate": 4.869e-05,
      "loss": 0.0032,
      "step": 3930
    },
    {
      "epoch": 0.21013333333333334,
      "grad_norm": 0.14974302053451538,
      "learning_rate": 4.868666666666667e-05,
      "loss": 0.0035,
      "step": 3940
    },
    {
      "epoch": 0.21066666666666667,
      "grad_norm": 0.2740679383277893,
      "learning_rate": 4.8683333333333335e-05,
      "loss": 0.0035,
      "step": 3950
    },
    {
      "epoch": 0.2112,
      "grad_norm": 1.368322730064392,
      "learning_rate": 4.868e-05,
      "loss": 0.005,
      "step": 3960
    },
    {
      "epoch": 0.21173333333333333,
      "grad_norm": 0.241810142993927,
      "learning_rate": 4.867666666666667e-05,
      "loss": 0.0046,
      "step": 3970
    },
    {
      "epoch": 0.21226666666666666,
      "grad_norm": 0.012378472834825516,
      "learning_rate": 4.867333333333334e-05,
      "loss": 0.003,
      "step": 3980
    },
    {
      "epoch": 0.2128,
      "grad_norm": 0.14978711307048798,
      "learning_rate": 4.867000000000001e-05,
      "loss": 0.0043,
      "step": 3990
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.45161497592926025,
      "learning_rate": 4.866666666666667e-05,
      "loss": 0.003,
      "step": 4000
    },
    {
      "epoch": 0.21386666666666668,
      "grad_norm": 0.09016568958759308,
      "learning_rate": 4.866333333333333e-05,
      "loss": 0.0044,
      "step": 4010
    },
    {
      "epoch": 0.2144,
      "grad_norm": 0.15090537071228027,
      "learning_rate": 4.866e-05,
      "loss": 0.0035,
      "step": 4020
    },
    {
      "epoch": 0.21493333333333334,
      "grad_norm": 0.17974834144115448,
      "learning_rate": 4.8656666666666664e-05,
      "loss": 0.0021,
      "step": 4030
    },
    {
      "epoch": 0.21546666666666667,
      "grad_norm": 0.1739041805267334,
      "learning_rate": 4.865333333333334e-05,
      "loss": 0.0053,
      "step": 4040
    },
    {
      "epoch": 0.216,
      "grad_norm": 0.03386807441711426,
      "learning_rate": 4.8650000000000003e-05,
      "loss": 0.005,
      "step": 4050
    },
    {
      "epoch": 0.21653333333333333,
      "grad_norm": 0.3940468728542328,
      "learning_rate": 4.864666666666667e-05,
      "loss": 0.005,
      "step": 4060
    },
    {
      "epoch": 0.21706666666666666,
      "grad_norm": 0.6508032083511353,
      "learning_rate": 4.8643333333333336e-05,
      "loss": 0.0043,
      "step": 4070
    },
    {
      "epoch": 0.2176,
      "grad_norm": 0.2241877317428589,
      "learning_rate": 4.864e-05,
      "loss": 0.0035,
      "step": 4080
    },
    {
      "epoch": 0.21813333333333335,
      "grad_norm": 0.06470364332199097,
      "learning_rate": 4.863666666666667e-05,
      "loss": 0.0033,
      "step": 4090
    },
    {
      "epoch": 0.21866666666666668,
      "grad_norm": 0.430100679397583,
      "learning_rate": 4.8633333333333334e-05,
      "loss": 0.0039,
      "step": 4100
    },
    {
      "epoch": 0.2192,
      "grad_norm": 0.6408259868621826,
      "learning_rate": 4.863e-05,
      "loss": 0.0035,
      "step": 4110
    },
    {
      "epoch": 0.21973333333333334,
      "grad_norm": 0.12422912567853928,
      "learning_rate": 4.862666666666667e-05,
      "loss": 0.0038,
      "step": 4120
    },
    {
      "epoch": 0.22026666666666667,
      "grad_norm": 0.03485896438360214,
      "learning_rate": 4.862333333333334e-05,
      "loss": 0.0057,
      "step": 4130
    },
    {
      "epoch": 0.2208,
      "grad_norm": 0.42462533712387085,
      "learning_rate": 4.8620000000000005e-05,
      "loss": 0.0051,
      "step": 4140
    },
    {
      "epoch": 0.22133333333333333,
      "grad_norm": 0.06656116247177124,
      "learning_rate": 4.861666666666667e-05,
      "loss": 0.0051,
      "step": 4150
    },
    {
      "epoch": 0.22186666666666666,
      "grad_norm": 0.0929684191942215,
      "learning_rate": 4.861333333333333e-05,
      "loss": 0.004,
      "step": 4160
    },
    {
      "epoch": 0.2224,
      "grad_norm": 0.15534521639347076,
      "learning_rate": 4.861e-05,
      "loss": 0.0047,
      "step": 4170
    },
    {
      "epoch": 0.22293333333333334,
      "grad_norm": 0.060940101742744446,
      "learning_rate": 4.860666666666667e-05,
      "loss": 0.0045,
      "step": 4180
    },
    {
      "epoch": 0.22346666666666667,
      "grad_norm": 0.004982853773981333,
      "learning_rate": 4.8603333333333336e-05,
      "loss": 0.0052,
      "step": 4190
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.6753860116004944,
      "learning_rate": 4.86e-05,
      "loss": 0.0045,
      "step": 4200
    },
    {
      "epoch": 0.22453333333333333,
      "grad_norm": 0.12299960851669312,
      "learning_rate": 4.859666666666667e-05,
      "loss": 0.004,
      "step": 4210
    },
    {
      "epoch": 0.22506666666666666,
      "grad_norm": 0.1217334195971489,
      "learning_rate": 4.8593333333333335e-05,
      "loss": 0.0053,
      "step": 4220
    },
    {
      "epoch": 0.2256,
      "grad_norm": 0.18394900858402252,
      "learning_rate": 4.859e-05,
      "loss": 0.0059,
      "step": 4230
    },
    {
      "epoch": 0.22613333333333333,
      "grad_norm": 0.36346712708473206,
      "learning_rate": 4.858666666666667e-05,
      "loss": 0.0031,
      "step": 4240
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 0.30253905057907104,
      "learning_rate": 4.858333333333333e-05,
      "loss": 0.004,
      "step": 4250
    },
    {
      "epoch": 0.2272,
      "grad_norm": 0.514894425868988,
      "learning_rate": 4.8580000000000006e-05,
      "loss": 0.0031,
      "step": 4260
    },
    {
      "epoch": 0.22773333333333334,
      "grad_norm": 0.6542497873306274,
      "learning_rate": 4.857666666666667e-05,
      "loss": 0.0048,
      "step": 4270
    },
    {
      "epoch": 0.22826666666666667,
      "grad_norm": 0.23874062299728394,
      "learning_rate": 4.857333333333334e-05,
      "loss": 0.0038,
      "step": 4280
    },
    {
      "epoch": 0.2288,
      "grad_norm": 0.036857303231954575,
      "learning_rate": 4.8570000000000004e-05,
      "loss": 0.0018,
      "step": 4290
    },
    {
      "epoch": 0.22933333333333333,
      "grad_norm": 0.31195512413978577,
      "learning_rate": 4.856666666666667e-05,
      "loss": 0.0028,
      "step": 4300
    },
    {
      "epoch": 0.22986666666666666,
      "grad_norm": 0.5539449453353882,
      "learning_rate": 4.856333333333333e-05,
      "loss": 0.0038,
      "step": 4310
    },
    {
      "epoch": 0.2304,
      "grad_norm": 0.38757219910621643,
      "learning_rate": 4.856e-05,
      "loss": 0.0038,
      "step": 4320
    },
    {
      "epoch": 0.23093333333333332,
      "grad_norm": 0.29809117317199707,
      "learning_rate": 4.855666666666667e-05,
      "loss": 0.0055,
      "step": 4330
    },
    {
      "epoch": 0.23146666666666665,
      "grad_norm": 0.060555048286914825,
      "learning_rate": 4.8553333333333335e-05,
      "loss": 0.0037,
      "step": 4340
    },
    {
      "epoch": 0.232,
      "grad_norm": 0.08952269703149796,
      "learning_rate": 4.855e-05,
      "loss": 0.0025,
      "step": 4350
    },
    {
      "epoch": 0.23253333333333334,
      "grad_norm": 0.359699010848999,
      "learning_rate": 4.854666666666667e-05,
      "loss": 0.0046,
      "step": 4360
    },
    {
      "epoch": 0.23306666666666667,
      "grad_norm": 0.42165008187294006,
      "learning_rate": 4.854333333333333e-05,
      "loss": 0.0034,
      "step": 4370
    },
    {
      "epoch": 0.2336,
      "grad_norm": 0.09097705781459808,
      "learning_rate": 4.854e-05,
      "loss": 0.0039,
      "step": 4380
    },
    {
      "epoch": 0.23413333333333333,
      "grad_norm": 0.3282032012939453,
      "learning_rate": 4.853666666666667e-05,
      "loss": 0.0029,
      "step": 4390
    },
    {
      "epoch": 0.23466666666666666,
      "grad_norm": 0.03320543095469475,
      "learning_rate": 4.853333333333334e-05,
      "loss": 0.0035,
      "step": 4400
    },
    {
      "epoch": 0.2352,
      "grad_norm": 0.14954783022403717,
      "learning_rate": 4.8530000000000005e-05,
      "loss": 0.005,
      "step": 4410
    },
    {
      "epoch": 0.23573333333333332,
      "grad_norm": 0.015850955620408058,
      "learning_rate": 4.852666666666667e-05,
      "loss": 0.003,
      "step": 4420
    },
    {
      "epoch": 0.23626666666666668,
      "grad_norm": 0.11900578439235687,
      "learning_rate": 4.852333333333334e-05,
      "loss": 0.0025,
      "step": 4430
    },
    {
      "epoch": 0.2368,
      "grad_norm": 0.06227613240480423,
      "learning_rate": 4.852e-05,
      "loss": 0.0027,
      "step": 4440
    },
    {
      "epoch": 0.23733333333333334,
      "grad_norm": 0.23981362581253052,
      "learning_rate": 4.851666666666667e-05,
      "loss": 0.0028,
      "step": 4450
    },
    {
      "epoch": 0.23786666666666667,
      "grad_norm": 0.26878654956817627,
      "learning_rate": 4.8513333333333335e-05,
      "loss": 0.0037,
      "step": 4460
    },
    {
      "epoch": 0.2384,
      "grad_norm": 0.2998022139072418,
      "learning_rate": 4.851e-05,
      "loss": 0.0029,
      "step": 4470
    },
    {
      "epoch": 0.23893333333333333,
      "grad_norm": 0.12384703010320663,
      "learning_rate": 4.850666666666667e-05,
      "loss": 0.0052,
      "step": 4480
    },
    {
      "epoch": 0.23946666666666666,
      "grad_norm": 0.08936332166194916,
      "learning_rate": 4.8503333333333334e-05,
      "loss": 0.004,
      "step": 4490
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.12292421609163284,
      "learning_rate": 4.85e-05,
      "loss": 0.0028,
      "step": 4500
    },
    {
      "epoch": 0.24053333333333332,
      "grad_norm": 0.9005885124206543,
      "learning_rate": 4.8496666666666666e-05,
      "loss": 0.0066,
      "step": 4510
    },
    {
      "epoch": 0.24106666666666668,
      "grad_norm": 0.18348723649978638,
      "learning_rate": 4.849333333333333e-05,
      "loss": 0.0035,
      "step": 4520
    },
    {
      "epoch": 0.2416,
      "grad_norm": 0.2334701120853424,
      "learning_rate": 4.8490000000000005e-05,
      "loss": 0.0039,
      "step": 4530
    },
    {
      "epoch": 0.24213333333333334,
      "grad_norm": 0.8880963325500488,
      "learning_rate": 4.848666666666667e-05,
      "loss": 0.0045,
      "step": 4540
    },
    {
      "epoch": 0.24266666666666667,
      "grad_norm": 0.3020189106464386,
      "learning_rate": 4.848333333333334e-05,
      "loss": 0.0035,
      "step": 4550
    },
    {
      "epoch": 0.2432,
      "grad_norm": 0.06115134060382843,
      "learning_rate": 4.8480000000000003e-05,
      "loss": 0.0032,
      "step": 4560
    },
    {
      "epoch": 0.24373333333333333,
      "grad_norm": 0.1516837626695633,
      "learning_rate": 4.847666666666667e-05,
      "loss": 0.0021,
      "step": 4570
    },
    {
      "epoch": 0.24426666666666666,
      "grad_norm": 0.17951980233192444,
      "learning_rate": 4.8473333333333336e-05,
      "loss": 0.0043,
      "step": 4580
    },
    {
      "epoch": 0.2448,
      "grad_norm": 0.24124212563037872,
      "learning_rate": 4.847e-05,
      "loss": 0.0042,
      "step": 4590
    },
    {
      "epoch": 0.24533333333333332,
      "grad_norm": 0.09288936108350754,
      "learning_rate": 4.8466666666666675e-05,
      "loss": 0.0018,
      "step": 4600
    },
    {
      "epoch": 0.24586666666666668,
      "grad_norm": 0.12046770006418228,
      "learning_rate": 4.846333333333334e-05,
      "loss": 0.0052,
      "step": 4610
    },
    {
      "epoch": 0.2464,
      "grad_norm": 0.05937661975622177,
      "learning_rate": 4.846e-05,
      "loss": 0.0027,
      "step": 4620
    },
    {
      "epoch": 0.24693333333333334,
      "grad_norm": 0.12165945768356323,
      "learning_rate": 4.8456666666666666e-05,
      "loss": 0.0054,
      "step": 4630
    },
    {
      "epoch": 0.24746666666666667,
      "grad_norm": 0.42380809783935547,
      "learning_rate": 4.845333333333333e-05,
      "loss": 0.0029,
      "step": 4640
    },
    {
      "epoch": 0.248,
      "grad_norm": 0.4800454080104828,
      "learning_rate": 4.845e-05,
      "loss": 0.0039,
      "step": 4650
    },
    {
      "epoch": 0.24853333333333333,
      "grad_norm": 0.09609854966402054,
      "learning_rate": 4.8446666666666665e-05,
      "loss": 0.004,
      "step": 4660
    },
    {
      "epoch": 0.24906666666666666,
      "grad_norm": 0.4777941405773163,
      "learning_rate": 4.844333333333334e-05,
      "loss": 0.0033,
      "step": 4670
    },
    {
      "epoch": 0.2496,
      "grad_norm": 0.8689841628074646,
      "learning_rate": 4.8440000000000004e-05,
      "loss": 0.0044,
      "step": 4680
    },
    {
      "epoch": 0.2501333333333333,
      "grad_norm": 0.542961061000824,
      "learning_rate": 4.843666666666667e-05,
      "loss": 0.0032,
      "step": 4690
    },
    {
      "epoch": 0.25066666666666665,
      "grad_norm": 0.34129756689071655,
      "learning_rate": 4.8433333333333336e-05,
      "loss": 0.0042,
      "step": 4700
    },
    {
      "epoch": 0.2512,
      "grad_norm": 0.12273669987916946,
      "learning_rate": 4.843e-05,
      "loss": 0.0037,
      "step": 4710
    },
    {
      "epoch": 0.2517333333333333,
      "grad_norm": 0.1196286529302597,
      "learning_rate": 4.842666666666667e-05,
      "loss": 0.0022,
      "step": 4720
    },
    {
      "epoch": 0.25226666666666664,
      "grad_norm": 0.36596137285232544,
      "learning_rate": 4.8423333333333334e-05,
      "loss": 0.0031,
      "step": 4730
    },
    {
      "epoch": 0.2528,
      "grad_norm": 0.24416105449199677,
      "learning_rate": 4.842000000000001e-05,
      "loss": 0.0057,
      "step": 4740
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.005779535975307226,
      "learning_rate": 4.8416666666666673e-05,
      "loss": 0.0055,
      "step": 4750
    },
    {
      "epoch": 0.2538666666666667,
      "grad_norm": 0.6056473255157471,
      "learning_rate": 4.841333333333334e-05,
      "loss": 0.0032,
      "step": 4760
    },
    {
      "epoch": 0.2544,
      "grad_norm": 0.6647024750709534,
      "learning_rate": 4.841e-05,
      "loss": 0.0045,
      "step": 4770
    },
    {
      "epoch": 0.25493333333333335,
      "grad_norm": 0.2400125414133072,
      "learning_rate": 4.8406666666666665e-05,
      "loss": 0.0046,
      "step": 4780
    },
    {
      "epoch": 0.2554666666666667,
      "grad_norm": 0.12444163113832474,
      "learning_rate": 4.840333333333333e-05,
      "loss": 0.0042,
      "step": 4790
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.15337173640727997,
      "learning_rate": 4.8400000000000004e-05,
      "loss": 0.0041,
      "step": 4800
    },
    {
      "epoch": 0.25653333333333334,
      "grad_norm": 0.21023645997047424,
      "learning_rate": 4.839666666666667e-05,
      "loss": 0.0043,
      "step": 4810
    },
    {
      "epoch": 0.25706666666666667,
      "grad_norm": 0.238093763589859,
      "learning_rate": 4.8393333333333336e-05,
      "loss": 0.0033,
      "step": 4820
    },
    {
      "epoch": 0.2576,
      "grad_norm": 0.031156770884990692,
      "learning_rate": 4.839e-05,
      "loss": 0.0041,
      "step": 4830
    },
    {
      "epoch": 0.2581333333333333,
      "grad_norm": 0.20855097472667694,
      "learning_rate": 4.838666666666667e-05,
      "loss": 0.0028,
      "step": 4840
    },
    {
      "epoch": 0.25866666666666666,
      "grad_norm": 0.1806529313325882,
      "learning_rate": 4.8383333333333335e-05,
      "loss": 0.0029,
      "step": 4850
    },
    {
      "epoch": 0.2592,
      "grad_norm": 0.26821327209472656,
      "learning_rate": 4.838e-05,
      "loss": 0.0036,
      "step": 4860
    },
    {
      "epoch": 0.2597333333333333,
      "grad_norm": 0.33145028352737427,
      "learning_rate": 4.837666666666667e-05,
      "loss": 0.0038,
      "step": 4870
    },
    {
      "epoch": 0.26026666666666665,
      "grad_norm": 0.18230420351028442,
      "learning_rate": 4.837333333333334e-05,
      "loss": 0.004,
      "step": 4880
    },
    {
      "epoch": 0.2608,
      "grad_norm": 0.24086721241474152,
      "learning_rate": 4.8370000000000006e-05,
      "loss": 0.0035,
      "step": 4890
    },
    {
      "epoch": 0.2613333333333333,
      "grad_norm": 0.39033666253089905,
      "learning_rate": 4.836666666666667e-05,
      "loss": 0.0027,
      "step": 4900
    },
    {
      "epoch": 0.2618666666666667,
      "grad_norm": 0.5703493356704712,
      "learning_rate": 4.836333333333334e-05,
      "loss": 0.0062,
      "step": 4910
    },
    {
      "epoch": 0.2624,
      "grad_norm": 0.8287363648414612,
      "learning_rate": 4.836e-05,
      "loss": 0.0036,
      "step": 4920
    },
    {
      "epoch": 0.26293333333333335,
      "grad_norm": 0.2400655746459961,
      "learning_rate": 4.8356666666666664e-05,
      "loss": 0.0027,
      "step": 4930
    },
    {
      "epoch": 0.2634666666666667,
      "grad_norm": 0.0895446315407753,
      "learning_rate": 4.835333333333334e-05,
      "loss": 0.0042,
      "step": 4940
    },
    {
      "epoch": 0.264,
      "grad_norm": 0.11995240300893784,
      "learning_rate": 4.835e-05,
      "loss": 0.0037,
      "step": 4950
    },
    {
      "epoch": 0.26453333333333334,
      "grad_norm": 0.2387748658657074,
      "learning_rate": 4.834666666666667e-05,
      "loss": 0.0046,
      "step": 4960
    },
    {
      "epoch": 0.2650666666666667,
      "grad_norm": 0.27184736728668213,
      "learning_rate": 4.8343333333333335e-05,
      "loss": 0.0025,
      "step": 4970
    },
    {
      "epoch": 0.2656,
      "grad_norm": 0.4793192446231842,
      "learning_rate": 4.834e-05,
      "loss": 0.0029,
      "step": 4980
    },
    {
      "epoch": 0.26613333333333333,
      "grad_norm": 0.05942985787987709,
      "learning_rate": 4.833666666666667e-05,
      "loss": 0.003,
      "step": 4990
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.7231912612915039,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 0.0041,
      "step": 5000
    },
    {
      "epoch": 0.2672,
      "grad_norm": 0.5531700849533081,
      "learning_rate": 4.833e-05,
      "loss": 0.0043,
      "step": 5010
    },
    {
      "epoch": 0.2677333333333333,
      "grad_norm": 0.09142894297838211,
      "learning_rate": 4.832666666666667e-05,
      "loss": 0.0069,
      "step": 5020
    },
    {
      "epoch": 0.26826666666666665,
      "grad_norm": 0.27644866704940796,
      "learning_rate": 4.832333333333334e-05,
      "loss": 0.0031,
      "step": 5030
    },
    {
      "epoch": 0.2688,
      "grad_norm": 0.2548454701900482,
      "learning_rate": 4.8320000000000005e-05,
      "loss": 0.0042,
      "step": 5040
    },
    {
      "epoch": 0.2693333333333333,
      "grad_norm": 0.1825907677412033,
      "learning_rate": 4.831666666666667e-05,
      "loss": 0.0048,
      "step": 5050
    },
    {
      "epoch": 0.26986666666666664,
      "grad_norm": 0.15336285531520844,
      "learning_rate": 4.831333333333334e-05,
      "loss": 0.0032,
      "step": 5060
    },
    {
      "epoch": 0.2704,
      "grad_norm": 0.3620186746120453,
      "learning_rate": 4.8309999999999997e-05,
      "loss": 0.0035,
      "step": 5070
    },
    {
      "epoch": 0.27093333333333336,
      "grad_norm": 0.12144085764884949,
      "learning_rate": 4.830666666666667e-05,
      "loss": 0.0033,
      "step": 5080
    },
    {
      "epoch": 0.2714666666666667,
      "grad_norm": 0.3915339708328247,
      "learning_rate": 4.8303333333333336e-05,
      "loss": 0.0033,
      "step": 5090
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.05974139645695686,
      "learning_rate": 4.83e-05,
      "loss": 0.0033,
      "step": 5100
    },
    {
      "epoch": 0.27253333333333335,
      "grad_norm": 0.810298502445221,
      "learning_rate": 4.829666666666667e-05,
      "loss": 0.003,
      "step": 5110
    },
    {
      "epoch": 0.2730666666666667,
      "grad_norm": 0.2693127691745758,
      "learning_rate": 4.8293333333333334e-05,
      "loss": 0.0037,
      "step": 5120
    },
    {
      "epoch": 0.2736,
      "grad_norm": 0.006182441487908363,
      "learning_rate": 4.829e-05,
      "loss": 0.0033,
      "step": 5130
    },
    {
      "epoch": 0.27413333333333334,
      "grad_norm": 0.26928526163101196,
      "learning_rate": 4.8286666666666666e-05,
      "loss": 0.0044,
      "step": 5140
    },
    {
      "epoch": 0.27466666666666667,
      "grad_norm": 0.2672770321369171,
      "learning_rate": 4.828333333333334e-05,
      "loss": 0.0033,
      "step": 5150
    },
    {
      "epoch": 0.2752,
      "grad_norm": 0.011412433348596096,
      "learning_rate": 4.8280000000000005e-05,
      "loss": 0.0033,
      "step": 5160
    },
    {
      "epoch": 0.27573333333333333,
      "grad_norm": 0.35864654183387756,
      "learning_rate": 4.827666666666667e-05,
      "loss": 0.0027,
      "step": 5170
    },
    {
      "epoch": 0.27626666666666666,
      "grad_norm": 0.00848044827580452,
      "learning_rate": 4.827333333333334e-05,
      "loss": 0.0031,
      "step": 5180
    },
    {
      "epoch": 0.2768,
      "grad_norm": 0.17924360930919647,
      "learning_rate": 4.8270000000000004e-05,
      "loss": 0.0024,
      "step": 5190
    },
    {
      "epoch": 0.2773333333333333,
      "grad_norm": 0.4188111126422882,
      "learning_rate": 4.826666666666667e-05,
      "loss": 0.0036,
      "step": 5200
    },
    {
      "epoch": 0.27786666666666665,
      "grad_norm": 0.1784973442554474,
      "learning_rate": 4.8263333333333336e-05,
      "loss": 0.0031,
      "step": 5210
    },
    {
      "epoch": 0.2784,
      "grad_norm": 1.263916015625,
      "learning_rate": 4.826e-05,
      "loss": 0.0052,
      "step": 5220
    },
    {
      "epoch": 0.2789333333333333,
      "grad_norm": 0.12062850594520569,
      "learning_rate": 4.825666666666667e-05,
      "loss": 0.0029,
      "step": 5230
    },
    {
      "epoch": 0.27946666666666664,
      "grad_norm": 0.8498030304908752,
      "learning_rate": 4.8253333333333334e-05,
      "loss": 0.0043,
      "step": 5240
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.5145391821861267,
      "learning_rate": 4.825e-05,
      "loss": 0.0036,
      "step": 5250
    },
    {
      "epoch": 0.28053333333333336,
      "grad_norm": 0.27475234866142273,
      "learning_rate": 4.824666666666667e-05,
      "loss": 0.0049,
      "step": 5260
    },
    {
      "epoch": 0.2810666666666667,
      "grad_norm": 0.8863757848739624,
      "learning_rate": 4.824333333333333e-05,
      "loss": 0.0033,
      "step": 5270
    },
    {
      "epoch": 0.2816,
      "grad_norm": 0.3664187788963318,
      "learning_rate": 4.824e-05,
      "loss": 0.0036,
      "step": 5280
    },
    {
      "epoch": 0.28213333333333335,
      "grad_norm": 0.09071827679872513,
      "learning_rate": 4.823666666666667e-05,
      "loss": 0.0059,
      "step": 5290
    },
    {
      "epoch": 0.2826666666666667,
      "grad_norm": 0.45300108194351196,
      "learning_rate": 4.823333333333334e-05,
      "loss": 0.0055,
      "step": 5300
    },
    {
      "epoch": 0.2832,
      "grad_norm": 0.4823293685913086,
      "learning_rate": 4.8230000000000004e-05,
      "loss": 0.0045,
      "step": 5310
    },
    {
      "epoch": 0.28373333333333334,
      "grad_norm": 0.7524759769439697,
      "learning_rate": 4.822666666666667e-05,
      "loss": 0.0033,
      "step": 5320
    },
    {
      "epoch": 0.28426666666666667,
      "grad_norm": 0.12087634205818176,
      "learning_rate": 4.8223333333333336e-05,
      "loss": 0.0043,
      "step": 5330
    },
    {
      "epoch": 0.2848,
      "grad_norm": 0.3574528992176056,
      "learning_rate": 4.822e-05,
      "loss": 0.0035,
      "step": 5340
    },
    {
      "epoch": 0.2853333333333333,
      "grad_norm": 0.030554786324501038,
      "learning_rate": 4.821666666666667e-05,
      "loss": 0.0054,
      "step": 5350
    },
    {
      "epoch": 0.28586666666666666,
      "grad_norm": 0.8104315400123596,
      "learning_rate": 4.8213333333333335e-05,
      "loss": 0.0052,
      "step": 5360
    },
    {
      "epoch": 0.2864,
      "grad_norm": 0.14891208708286285,
      "learning_rate": 4.821e-05,
      "loss": 0.0035,
      "step": 5370
    },
    {
      "epoch": 0.2869333333333333,
      "grad_norm": 0.08943266421556473,
      "learning_rate": 4.820666666666667e-05,
      "loss": 0.005,
      "step": 5380
    },
    {
      "epoch": 0.28746666666666665,
      "grad_norm": 0.41671687364578247,
      "learning_rate": 4.820333333333333e-05,
      "loss": 0.0037,
      "step": 5390
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.03057711198925972,
      "learning_rate": 4.82e-05,
      "loss": 0.0037,
      "step": 5400
    },
    {
      "epoch": 0.2885333333333333,
      "grad_norm": 0.17861056327819824,
      "learning_rate": 4.8196666666666665e-05,
      "loss": 0.0035,
      "step": 5410
    },
    {
      "epoch": 0.2890666666666667,
      "grad_norm": 0.2979474365711212,
      "learning_rate": 4.819333333333333e-05,
      "loss": 0.0069,
      "step": 5420
    },
    {
      "epoch": 0.2896,
      "grad_norm": 0.6840008497238159,
      "learning_rate": 4.8190000000000004e-05,
      "loss": 0.0044,
      "step": 5430
    },
    {
      "epoch": 0.29013333333333335,
      "grad_norm": 0.533976674079895,
      "learning_rate": 4.818666666666667e-05,
      "loss": 0.0029,
      "step": 5440
    },
    {
      "epoch": 0.2906666666666667,
      "grad_norm": 0.05924030765891075,
      "learning_rate": 4.818333333333334e-05,
      "loss": 0.0037,
      "step": 5450
    },
    {
      "epoch": 0.2912,
      "grad_norm": 0.20899733901023865,
      "learning_rate": 4.818e-05,
      "loss": 0.0022,
      "step": 5460
    },
    {
      "epoch": 0.29173333333333334,
      "grad_norm": 0.23934362828731537,
      "learning_rate": 4.817666666666667e-05,
      "loss": 0.0033,
      "step": 5470
    },
    {
      "epoch": 0.2922666666666667,
      "grad_norm": 0.17816314101219177,
      "learning_rate": 4.8173333333333335e-05,
      "loss": 0.0035,
      "step": 5480
    },
    {
      "epoch": 0.2928,
      "grad_norm": 0.4751589596271515,
      "learning_rate": 4.817e-05,
      "loss": 0.0044,
      "step": 5490
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.05940914526581764,
      "learning_rate": 4.8166666666666674e-05,
      "loss": 0.0046,
      "step": 5500
    },
    {
      "epoch": 0.29386666666666666,
      "grad_norm": 0.47783681750297546,
      "learning_rate": 4.816333333333334e-05,
      "loss": 0.0045,
      "step": 5510
    },
    {
      "epoch": 0.2944,
      "grad_norm": 0.21144790947437286,
      "learning_rate": 4.816e-05,
      "loss": 0.0034,
      "step": 5520
    },
    {
      "epoch": 0.2949333333333333,
      "grad_norm": 0.060674067586660385,
      "learning_rate": 4.8156666666666666e-05,
      "loss": 0.0036,
      "step": 5530
    },
    {
      "epoch": 0.29546666666666666,
      "grad_norm": 0.03028986230492592,
      "learning_rate": 4.815333333333333e-05,
      "loss": 0.0037,
      "step": 5540
    },
    {
      "epoch": 0.296,
      "grad_norm": 0.2977994978427887,
      "learning_rate": 4.815e-05,
      "loss": 0.0042,
      "step": 5550
    },
    {
      "epoch": 0.2965333333333333,
      "grad_norm": 0.6532251238822937,
      "learning_rate": 4.814666666666667e-05,
      "loss": 0.0033,
      "step": 5560
    },
    {
      "epoch": 0.29706666666666665,
      "grad_norm": 0.11874816566705704,
      "learning_rate": 4.814333333333334e-05,
      "loss": 0.0038,
      "step": 5570
    },
    {
      "epoch": 0.2976,
      "grad_norm": 0.4762735664844513,
      "learning_rate": 4.814e-05,
      "loss": 0.003,
      "step": 5580
    },
    {
      "epoch": 0.2981333333333333,
      "grad_norm": 0.5351427793502808,
      "learning_rate": 4.813666666666667e-05,
      "loss": 0.0039,
      "step": 5590
    },
    {
      "epoch": 0.2986666666666667,
      "grad_norm": 0.5343365669250488,
      "learning_rate": 4.8133333333333336e-05,
      "loss": 0.0039,
      "step": 5600
    },
    {
      "epoch": 0.2992,
      "grad_norm": 0.2971360385417938,
      "learning_rate": 4.813e-05,
      "loss": 0.0059,
      "step": 5610
    },
    {
      "epoch": 0.29973333333333335,
      "grad_norm": 0.3266390562057495,
      "learning_rate": 4.812666666666667e-05,
      "loss": 0.0039,
      "step": 5620
    },
    {
      "epoch": 0.3002666666666667,
      "grad_norm": 0.6118447780609131,
      "learning_rate": 4.8123333333333334e-05,
      "loss": 0.0046,
      "step": 5630
    },
    {
      "epoch": 0.3008,
      "grad_norm": 0.08906079828739166,
      "learning_rate": 4.812000000000001e-05,
      "loss": 0.0025,
      "step": 5640
    },
    {
      "epoch": 0.30133333333333334,
      "grad_norm": 0.8049119710922241,
      "learning_rate": 4.811666666666667e-05,
      "loss": 0.0026,
      "step": 5650
    },
    {
      "epoch": 0.30186666666666667,
      "grad_norm": 0.00481753284111619,
      "learning_rate": 4.811333333333334e-05,
      "loss": 0.0042,
      "step": 5660
    },
    {
      "epoch": 0.3024,
      "grad_norm": 0.29804158210754395,
      "learning_rate": 4.8110000000000005e-05,
      "loss": 0.0031,
      "step": 5670
    },
    {
      "epoch": 0.30293333333333333,
      "grad_norm": 0.0591752789914608,
      "learning_rate": 4.8106666666666665e-05,
      "loss": 0.0032,
      "step": 5680
    },
    {
      "epoch": 0.30346666666666666,
      "grad_norm": 0.0035476309712976217,
      "learning_rate": 4.810333333333333e-05,
      "loss": 0.0028,
      "step": 5690
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.059269100427627563,
      "learning_rate": 4.8100000000000004e-05,
      "loss": 0.0042,
      "step": 5700
    },
    {
      "epoch": 0.3045333333333333,
      "grad_norm": 0.3557116985321045,
      "learning_rate": 4.809666666666667e-05,
      "loss": 0.0023,
      "step": 5710
    },
    {
      "epoch": 0.30506666666666665,
      "grad_norm": 0.5624822974205017,
      "learning_rate": 4.8093333333333336e-05,
      "loss": 0.0024,
      "step": 5720
    },
    {
      "epoch": 0.3056,
      "grad_norm": 0.7108390927314758,
      "learning_rate": 4.809e-05,
      "loss": 0.0033,
      "step": 5730
    },
    {
      "epoch": 0.3061333333333333,
      "grad_norm": 0.2965880334377289,
      "learning_rate": 4.808666666666667e-05,
      "loss": 0.0023,
      "step": 5740
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.03084620088338852,
      "learning_rate": 4.8083333333333334e-05,
      "loss": 0.0025,
      "step": 5750
    },
    {
      "epoch": 0.3072,
      "grad_norm": 0.32978126406669617,
      "learning_rate": 4.808e-05,
      "loss": 0.0038,
      "step": 5760
    },
    {
      "epoch": 0.30773333333333336,
      "grad_norm": 0.00575128523632884,
      "learning_rate": 4.8076666666666667e-05,
      "loss": 0.0022,
      "step": 5770
    },
    {
      "epoch": 0.3082666666666667,
      "grad_norm": 0.6523951292037964,
      "learning_rate": 4.807333333333334e-05,
      "loss": 0.004,
      "step": 5780
    },
    {
      "epoch": 0.3088,
      "grad_norm": 0.6592652201652527,
      "learning_rate": 4.8070000000000006e-05,
      "loss": 0.0045,
      "step": 5790
    },
    {
      "epoch": 0.30933333333333335,
      "grad_norm": 0.4580100476741791,
      "learning_rate": 4.806666666666667e-05,
      "loss": 0.0037,
      "step": 5800
    },
    {
      "epoch": 0.3098666666666667,
      "grad_norm": 0.17881406843662262,
      "learning_rate": 4.806333333333334e-05,
      "loss": 0.0039,
      "step": 5810
    },
    {
      "epoch": 0.3104,
      "grad_norm": 0.029866410419344902,
      "learning_rate": 4.8060000000000004e-05,
      "loss": 0.0038,
      "step": 5820
    },
    {
      "epoch": 0.31093333333333334,
      "grad_norm": 0.030254537239670753,
      "learning_rate": 4.805666666666666e-05,
      "loss": 0.0033,
      "step": 5830
    },
    {
      "epoch": 0.31146666666666667,
      "grad_norm": 0.11871486902236938,
      "learning_rate": 4.8053333333333336e-05,
      "loss": 0.0044,
      "step": 5840
    },
    {
      "epoch": 0.312,
      "grad_norm": 0.2088414430618286,
      "learning_rate": 4.805e-05,
      "loss": 0.0033,
      "step": 5850
    },
    {
      "epoch": 0.31253333333333333,
      "grad_norm": 0.32773199677467346,
      "learning_rate": 4.804666666666667e-05,
      "loss": 0.003,
      "step": 5860
    },
    {
      "epoch": 0.31306666666666666,
      "grad_norm": 0.3281387388706207,
      "learning_rate": 4.8043333333333335e-05,
      "loss": 0.003,
      "step": 5870
    },
    {
      "epoch": 0.3136,
      "grad_norm": 0.5342221856117249,
      "learning_rate": 4.804e-05,
      "loss": 0.0036,
      "step": 5880
    },
    {
      "epoch": 0.3141333333333333,
      "grad_norm": 0.6525266766548157,
      "learning_rate": 4.803666666666667e-05,
      "loss": 0.0019,
      "step": 5890
    },
    {
      "epoch": 0.31466666666666665,
      "grad_norm": 0.030318638309836388,
      "learning_rate": 4.803333333333333e-05,
      "loss": 0.0042,
      "step": 5900
    },
    {
      "epoch": 0.3152,
      "grad_norm": 0.23886440694332123,
      "learning_rate": 4.8030000000000006e-05,
      "loss": 0.0038,
      "step": 5910
    },
    {
      "epoch": 0.3157333333333333,
      "grad_norm": 0.20778650045394897,
      "learning_rate": 4.802666666666667e-05,
      "loss": 0.0041,
      "step": 5920
    },
    {
      "epoch": 0.31626666666666664,
      "grad_norm": 0.08953394740819931,
      "learning_rate": 4.802333333333334e-05,
      "loss": 0.0028,
      "step": 5930
    },
    {
      "epoch": 0.3168,
      "grad_norm": 0.20770251750946045,
      "learning_rate": 4.8020000000000004e-05,
      "loss": 0.0035,
      "step": 5940
    },
    {
      "epoch": 0.31733333333333336,
      "grad_norm": 0.26695385575294495,
      "learning_rate": 4.801666666666667e-05,
      "loss": 0.0029,
      "step": 5950
    },
    {
      "epoch": 0.3178666666666667,
      "grad_norm": 0.059637121856212616,
      "learning_rate": 4.801333333333334e-05,
      "loss": 0.0023,
      "step": 5960
    },
    {
      "epoch": 0.3184,
      "grad_norm": 0.029833894222974777,
      "learning_rate": 4.801e-05,
      "loss": 0.0034,
      "step": 5970
    },
    {
      "epoch": 0.31893333333333335,
      "grad_norm": 0.00802100170403719,
      "learning_rate": 4.800666666666667e-05,
      "loss": 0.0039,
      "step": 5980
    },
    {
      "epoch": 0.3194666666666667,
      "grad_norm": 0.06066673994064331,
      "learning_rate": 4.8003333333333335e-05,
      "loss": 0.0034,
      "step": 5990
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.41665828227996826,
      "learning_rate": 4.8e-05,
      "loss": 0.0031,
      "step": 6000
    },
    {
      "epoch": 0.32053333333333334,
      "grad_norm": 0.20924068987369537,
      "learning_rate": 4.799666666666667e-05,
      "loss": 0.0035,
      "step": 6010
    },
    {
      "epoch": 0.32106666666666667,
      "grad_norm": 0.11935514956712723,
      "learning_rate": 4.7993333333333333e-05,
      "loss": 0.0039,
      "step": 6020
    },
    {
      "epoch": 0.3216,
      "grad_norm": 0.05967840924859047,
      "learning_rate": 4.799e-05,
      "loss": 0.004,
      "step": 6030
    },
    {
      "epoch": 0.3221333333333333,
      "grad_norm": 0.5039476752281189,
      "learning_rate": 4.7986666666666666e-05,
      "loss": 0.0041,
      "step": 6040
    },
    {
      "epoch": 0.32266666666666666,
      "grad_norm": 0.08871130645275116,
      "learning_rate": 4.798333333333334e-05,
      "loss": 0.0037,
      "step": 6050
    },
    {
      "epoch": 0.3232,
      "grad_norm": 0.20847797393798828,
      "learning_rate": 4.7980000000000005e-05,
      "loss": 0.0037,
      "step": 6060
    },
    {
      "epoch": 0.3237333333333333,
      "grad_norm": 0.148057758808136,
      "learning_rate": 4.797666666666667e-05,
      "loss": 0.0036,
      "step": 6070
    },
    {
      "epoch": 0.32426666666666665,
      "grad_norm": 0.5337173342704773,
      "learning_rate": 4.797333333333334e-05,
      "loss": 0.0036,
      "step": 6080
    },
    {
      "epoch": 0.3248,
      "grad_norm": 0.35715270042419434,
      "learning_rate": 4.797e-05,
      "loss": 0.0035,
      "step": 6090
    },
    {
      "epoch": 0.3253333333333333,
      "grad_norm": 0.11830917000770569,
      "learning_rate": 4.796666666666667e-05,
      "loss": 0.0042,
      "step": 6100
    },
    {
      "epoch": 0.3258666666666667,
      "grad_norm": 0.2661994397640228,
      "learning_rate": 4.7963333333333335e-05,
      "loss": 0.0033,
      "step": 6110
    },
    {
      "epoch": 0.3264,
      "grad_norm": 0.1772691011428833,
      "learning_rate": 4.796e-05,
      "loss": 0.0037,
      "step": 6120
    },
    {
      "epoch": 0.32693333333333335,
      "grad_norm": 0.088945172727108,
      "learning_rate": 4.795666666666667e-05,
      "loss": 0.0042,
      "step": 6130
    },
    {
      "epoch": 0.3274666666666667,
      "grad_norm": 0.05926153436303139,
      "learning_rate": 4.7953333333333334e-05,
      "loss": 0.0029,
      "step": 6140
    },
    {
      "epoch": 0.328,
      "grad_norm": 0.23672538995742798,
      "learning_rate": 4.795e-05,
      "loss": 0.003,
      "step": 6150
    },
    {
      "epoch": 0.32853333333333334,
      "grad_norm": 0.0308262649923563,
      "learning_rate": 4.7946666666666666e-05,
      "loss": 0.0035,
      "step": 6160
    },
    {
      "epoch": 0.3290666666666667,
      "grad_norm": 0.2790316939353943,
      "learning_rate": 4.794333333333333e-05,
      "loss": 0.0037,
      "step": 6170
    },
    {
      "epoch": 0.3296,
      "grad_norm": 0.2968140244483948,
      "learning_rate": 4.794e-05,
      "loss": 0.0041,
      "step": 6180
    },
    {
      "epoch": 0.33013333333333333,
      "grad_norm": 0.8512683510780334,
      "learning_rate": 4.793666666666667e-05,
      "loss": 0.0038,
      "step": 6190
    },
    {
      "epoch": 0.33066666666666666,
      "grad_norm": 0.20757117867469788,
      "learning_rate": 4.793333333333334e-05,
      "loss": 0.0041,
      "step": 6200
    },
    {
      "epoch": 0.3312,
      "grad_norm": 0.030832739546895027,
      "learning_rate": 4.7930000000000004e-05,
      "loss": 0.0036,
      "step": 6210
    },
    {
      "epoch": 0.3317333333333333,
      "grad_norm": 0.14808876812458038,
      "learning_rate": 4.792666666666667e-05,
      "loss": 0.0033,
      "step": 6220
    },
    {
      "epoch": 0.33226666666666665,
      "grad_norm": 0.059406355023384094,
      "learning_rate": 4.7923333333333336e-05,
      "loss": 0.0028,
      "step": 6230
    },
    {
      "epoch": 0.3328,
      "grad_norm": 0.47340795397758484,
      "learning_rate": 4.792e-05,
      "loss": 0.0025,
      "step": 6240
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.47370001673698425,
      "learning_rate": 4.791666666666667e-05,
      "loss": 0.0026,
      "step": 6250
    },
    {
      "epoch": 0.33386666666666664,
      "grad_norm": 0.03021513670682907,
      "learning_rate": 4.791333333333334e-05,
      "loss": 0.0036,
      "step": 6260
    },
    {
      "epoch": 0.3344,
      "grad_norm": 0.1492161750793457,
      "learning_rate": 4.791000000000001e-05,
      "loss": 0.0042,
      "step": 6270
    },
    {
      "epoch": 0.33493333333333336,
      "grad_norm": 0.11815650016069412,
      "learning_rate": 4.7906666666666667e-05,
      "loss": 0.003,
      "step": 6280
    },
    {
      "epoch": 0.3354666666666667,
      "grad_norm": 0.14760929346084595,
      "learning_rate": 4.790333333333333e-05,
      "loss": 0.0039,
      "step": 6290
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.531915009021759,
      "learning_rate": 4.79e-05,
      "loss": 0.0038,
      "step": 6300
    },
    {
      "epoch": 0.33653333333333335,
      "grad_norm": 0.23717047274112701,
      "learning_rate": 4.7896666666666665e-05,
      "loss": 0.0035,
      "step": 6310
    },
    {
      "epoch": 0.3370666666666667,
      "grad_norm": 0.0885380282998085,
      "learning_rate": 4.789333333333334e-05,
      "loss": 0.0029,
      "step": 6320
    },
    {
      "epoch": 0.3376,
      "grad_norm": 0.38461148738861084,
      "learning_rate": 4.7890000000000004e-05,
      "loss": 0.0039,
      "step": 6330
    },
    {
      "epoch": 0.33813333333333334,
      "grad_norm": 0.1205550953745842,
      "learning_rate": 4.788666666666667e-05,
      "loss": 0.0037,
      "step": 6340
    },
    {
      "epoch": 0.33866666666666667,
      "grad_norm": 0.23615989089012146,
      "learning_rate": 4.7883333333333336e-05,
      "loss": 0.0031,
      "step": 6350
    },
    {
      "epoch": 0.3392,
      "grad_norm": 0.574021577835083,
      "learning_rate": 4.788e-05,
      "loss": 0.0025,
      "step": 6360
    },
    {
      "epoch": 0.33973333333333333,
      "grad_norm": 0.2066543996334076,
      "learning_rate": 4.787666666666667e-05,
      "loss": 0.0021,
      "step": 6370
    },
    {
      "epoch": 0.34026666666666666,
      "grad_norm": 0.532258152961731,
      "learning_rate": 4.7873333333333335e-05,
      "loss": 0.0043,
      "step": 6380
    },
    {
      "epoch": 0.3408,
      "grad_norm": 0.08920348435640335,
      "learning_rate": 4.787e-05,
      "loss": 0.0042,
      "step": 6390
    },
    {
      "epoch": 0.3413333333333333,
      "grad_norm": 0.47187790274620056,
      "learning_rate": 4.7866666666666674e-05,
      "loss": 0.003,
      "step": 6400
    },
    {
      "epoch": 0.34186666666666665,
      "grad_norm": 0.5310149788856506,
      "learning_rate": 4.786333333333334e-05,
      "loss": 0.0036,
      "step": 6410
    },
    {
      "epoch": 0.3424,
      "grad_norm": 0.03075280226767063,
      "learning_rate": 4.7860000000000006e-05,
      "loss": 0.0047,
      "step": 6420
    },
    {
      "epoch": 0.3429333333333333,
      "grad_norm": 0.5313122272491455,
      "learning_rate": 4.7856666666666665e-05,
      "loss": 0.0044,
      "step": 6430
    },
    {
      "epoch": 0.34346666666666664,
      "grad_norm": 0.4130555987358093,
      "learning_rate": 4.785333333333333e-05,
      "loss": 0.0034,
      "step": 6440
    },
    {
      "epoch": 0.344,
      "grad_norm": 0.2361004650592804,
      "learning_rate": 4.785e-05,
      "loss": 0.004,
      "step": 6450
    },
    {
      "epoch": 0.34453333333333336,
      "grad_norm": 0.38440558314323425,
      "learning_rate": 4.784666666666667e-05,
      "loss": 0.0034,
      "step": 6460
    },
    {
      "epoch": 0.3450666666666667,
      "grad_norm": 0.1182907223701477,
      "learning_rate": 4.784333333333334e-05,
      "loss": 0.0032,
      "step": 6470
    },
    {
      "epoch": 0.3456,
      "grad_norm": 0.5927764773368835,
      "learning_rate": 4.784e-05,
      "loss": 0.0038,
      "step": 6480
    },
    {
      "epoch": 0.34613333333333335,
      "grad_norm": 0.23650145530700684,
      "learning_rate": 4.783666666666667e-05,
      "loss": 0.0029,
      "step": 6490
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.23685921728610992,
      "learning_rate": 4.7833333333333335e-05,
      "loss": 0.0036,
      "step": 6500
    },
    {
      "epoch": 0.3472,
      "grad_norm": 0.9439063668251038,
      "learning_rate": 4.783e-05,
      "loss": 0.004,
      "step": 6510
    },
    {
      "epoch": 0.34773333333333334,
      "grad_norm": 0.11805932968854904,
      "learning_rate": 4.782666666666667e-05,
      "loss": 0.004,
      "step": 6520
    },
    {
      "epoch": 0.34826666666666667,
      "grad_norm": 0.26548680663108826,
      "learning_rate": 4.7823333333333333e-05,
      "loss": 0.0028,
      "step": 6530
    },
    {
      "epoch": 0.3488,
      "grad_norm": 0.20719757676124573,
      "learning_rate": 4.7820000000000006e-05,
      "loss": 0.0041,
      "step": 6540
    },
    {
      "epoch": 0.34933333333333333,
      "grad_norm": 0.7978096604347229,
      "learning_rate": 4.781666666666667e-05,
      "loss": 0.0034,
      "step": 6550
    },
    {
      "epoch": 0.34986666666666666,
      "grad_norm": 0.13274821639060974,
      "learning_rate": 4.781333333333334e-05,
      "loss": 0.0042,
      "step": 6560
    },
    {
      "epoch": 0.3504,
      "grad_norm": 0.35461336374282837,
      "learning_rate": 4.7810000000000005e-05,
      "loss": 0.0032,
      "step": 6570
    },
    {
      "epoch": 0.3509333333333333,
      "grad_norm": 0.14773236215114594,
      "learning_rate": 4.7806666666666664e-05,
      "loss": 0.0027,
      "step": 6580
    },
    {
      "epoch": 0.35146666666666665,
      "grad_norm": 0.23726701736450195,
      "learning_rate": 4.780333333333333e-05,
      "loss": 0.0038,
      "step": 6590
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.08891937136650085,
      "learning_rate": 4.78e-05,
      "loss": 0.0034,
      "step": 6600
    },
    {
      "epoch": 0.3525333333333333,
      "grad_norm": 0.11798924952745438,
      "learning_rate": 4.779666666666667e-05,
      "loss": 0.0059,
      "step": 6610
    },
    {
      "epoch": 0.35306666666666664,
      "grad_norm": 0.14781978726387024,
      "learning_rate": 4.7793333333333335e-05,
      "loss": 0.0041,
      "step": 6620
    },
    {
      "epoch": 0.3536,
      "grad_norm": 0.1496022492647171,
      "learning_rate": 4.779e-05,
      "loss": 0.0038,
      "step": 6630
    },
    {
      "epoch": 0.35413333333333336,
      "grad_norm": 0.17946438491344452,
      "learning_rate": 4.778666666666667e-05,
      "loss": 0.0035,
      "step": 6640
    },
    {
      "epoch": 0.3546666666666667,
      "grad_norm": 0.3277381360530853,
      "learning_rate": 4.7783333333333334e-05,
      "loss": 0.0058,
      "step": 6650
    },
    {
      "epoch": 0.3552,
      "grad_norm": 0.23822277784347534,
      "learning_rate": 4.778e-05,
      "loss": 0.0048,
      "step": 6660
    },
    {
      "epoch": 0.35573333333333335,
      "grad_norm": 0.17933402955532074,
      "learning_rate": 4.777666666666667e-05,
      "loss": 0.0035,
      "step": 6670
    },
    {
      "epoch": 0.3562666666666667,
      "grad_norm": 0.35667428374290466,
      "learning_rate": 4.777333333333334e-05,
      "loss": 0.0044,
      "step": 6680
    },
    {
      "epoch": 0.3568,
      "grad_norm": 0.05926427245140076,
      "learning_rate": 4.7770000000000005e-05,
      "loss": 0.005,
      "step": 6690
    },
    {
      "epoch": 0.35733333333333334,
      "grad_norm": 0.08872834593057632,
      "learning_rate": 4.776666666666667e-05,
      "loss": 0.0039,
      "step": 6700
    },
    {
      "epoch": 0.35786666666666667,
      "grad_norm": 0.08880376815795898,
      "learning_rate": 4.776333333333334e-05,
      "loss": 0.005,
      "step": 6710
    },
    {
      "epoch": 0.3584,
      "grad_norm": 0.8138129711151123,
      "learning_rate": 4.7760000000000004e-05,
      "loss": 0.0038,
      "step": 6720
    },
    {
      "epoch": 0.3589333333333333,
      "grad_norm": 0.7074970006942749,
      "learning_rate": 4.775666666666666e-05,
      "loss": 0.0022,
      "step": 6730
    },
    {
      "epoch": 0.35946666666666666,
      "grad_norm": 0.4127733111381531,
      "learning_rate": 4.7753333333333336e-05,
      "loss": 0.0042,
      "step": 6740
    },
    {
      "epoch": 0.36,
      "grad_norm": 0.14782387018203735,
      "learning_rate": 4.775e-05,
      "loss": 0.0038,
      "step": 6750
    },
    {
      "epoch": 0.3605333333333333,
      "grad_norm": 0.20635804533958435,
      "learning_rate": 4.774666666666667e-05,
      "loss": 0.0043,
      "step": 6760
    },
    {
      "epoch": 0.36106666666666665,
      "grad_norm": 0.20637406408786774,
      "learning_rate": 4.7743333333333334e-05,
      "loss": 0.003,
      "step": 6770
    },
    {
      "epoch": 0.3616,
      "grad_norm": 0.05961800739169121,
      "learning_rate": 4.774e-05,
      "loss": 0.004,
      "step": 6780
    },
    {
      "epoch": 0.3621333333333333,
      "grad_norm": 0.26552513241767883,
      "learning_rate": 4.7736666666666666e-05,
      "loss": 0.005,
      "step": 6790
    },
    {
      "epoch": 0.3626666666666667,
      "grad_norm": 0.003400667104870081,
      "learning_rate": 4.773333333333333e-05,
      "loss": 0.0054,
      "step": 6800
    },
    {
      "epoch": 0.3632,
      "grad_norm": 0.004315750207751989,
      "learning_rate": 4.7730000000000005e-05,
      "loss": 0.0039,
      "step": 6810
    },
    {
      "epoch": 0.36373333333333335,
      "grad_norm": 0.5029419660568237,
      "learning_rate": 4.772666666666667e-05,
      "loss": 0.0042,
      "step": 6820
    },
    {
      "epoch": 0.3642666666666667,
      "grad_norm": 0.05947245657444,
      "learning_rate": 4.772333333333334e-05,
      "loss": 0.0031,
      "step": 6830
    },
    {
      "epoch": 0.3648,
      "grad_norm": 0.08913052827119827,
      "learning_rate": 4.7720000000000004e-05,
      "loss": 0.0028,
      "step": 6840
    },
    {
      "epoch": 0.36533333333333334,
      "grad_norm": 0.709199070930481,
      "learning_rate": 4.771666666666667e-05,
      "loss": 0.0044,
      "step": 6850
    },
    {
      "epoch": 0.3658666666666667,
      "grad_norm": 0.1187017410993576,
      "learning_rate": 4.7713333333333336e-05,
      "loss": 0.0024,
      "step": 6860
    },
    {
      "epoch": 0.3664,
      "grad_norm": 0.11934717744588852,
      "learning_rate": 4.771e-05,
      "loss": 0.0034,
      "step": 6870
    },
    {
      "epoch": 0.36693333333333333,
      "grad_norm": 0.0597185380756855,
      "learning_rate": 4.770666666666667e-05,
      "loss": 0.0049,
      "step": 6880
    },
    {
      "epoch": 0.36746666666666666,
      "grad_norm": 0.7089001536369324,
      "learning_rate": 4.7703333333333335e-05,
      "loss": 0.0038,
      "step": 6890
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.14727731049060822,
      "learning_rate": 4.77e-05,
      "loss": 0.0047,
      "step": 6900
    },
    {
      "epoch": 0.3685333333333333,
      "grad_norm": 0.2654443085193634,
      "learning_rate": 4.769666666666667e-05,
      "loss": 0.0028,
      "step": 6910
    },
    {
      "epoch": 0.36906666666666665,
      "grad_norm": 0.32444337010383606,
      "learning_rate": 4.769333333333333e-05,
      "loss": 0.004,
      "step": 6920
    },
    {
      "epoch": 0.3696,
      "grad_norm": 0.05883089452981949,
      "learning_rate": 4.769e-05,
      "loss": 0.0026,
      "step": 6930
    },
    {
      "epoch": 0.3701333333333333,
      "grad_norm": 0.0883210301399231,
      "learning_rate": 4.7686666666666665e-05,
      "loss": 0.0028,
      "step": 6940
    },
    {
      "epoch": 0.37066666666666664,
      "grad_norm": 0.20637905597686768,
      "learning_rate": 4.768333333333334e-05,
      "loss": 0.0029,
      "step": 6950
    },
    {
      "epoch": 0.3712,
      "grad_norm": 0.08824749290943146,
      "learning_rate": 4.7680000000000004e-05,
      "loss": 0.0032,
      "step": 6960
    },
    {
      "epoch": 0.37173333333333336,
      "grad_norm": 0.17655760049819946,
      "learning_rate": 4.767666666666667e-05,
      "loss": 0.003,
      "step": 6970
    },
    {
      "epoch": 0.3722666666666667,
      "grad_norm": 0.36427631974220276,
      "learning_rate": 4.7673333333333337e-05,
      "loss": 0.0034,
      "step": 6980
    },
    {
      "epoch": 0.3728,
      "grad_norm": 0.9329870343208313,
      "learning_rate": 4.767e-05,
      "loss": 0.0048,
      "step": 6990
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.11810694634914398,
      "learning_rate": 4.766666666666667e-05,
      "loss": 0.005,
      "step": 7000
    },
    {
      "epoch": 0.3738666666666667,
      "grad_norm": 0.3829641044139862,
      "learning_rate": 4.7663333333333335e-05,
      "loss": 0.0043,
      "step": 7010
    },
    {
      "epoch": 0.3744,
      "grad_norm": 0.030929572880268097,
      "learning_rate": 4.766000000000001e-05,
      "loss": 0.004,
      "step": 7020
    },
    {
      "epoch": 0.37493333333333334,
      "grad_norm": 0.17657490074634552,
      "learning_rate": 4.7656666666666674e-05,
      "loss": 0.003,
      "step": 7030
    },
    {
      "epoch": 0.37546666666666667,
      "grad_norm": 0.1771216243505478,
      "learning_rate": 4.765333333333333e-05,
      "loss": 0.0035,
      "step": 7040
    },
    {
      "epoch": 0.376,
      "grad_norm": 0.11796348541975021,
      "learning_rate": 4.765e-05,
      "loss": 0.0037,
      "step": 7050
    },
    {
      "epoch": 0.37653333333333333,
      "grad_norm": 0.4722394049167633,
      "learning_rate": 4.7646666666666666e-05,
      "loss": 0.0054,
      "step": 7060
    },
    {
      "epoch": 0.37706666666666666,
      "grad_norm": 0.2061222344636917,
      "learning_rate": 4.764333333333333e-05,
      "loss": 0.0036,
      "step": 7070
    },
    {
      "epoch": 0.3776,
      "grad_norm": 0.1773630976676941,
      "learning_rate": 4.7640000000000005e-05,
      "loss": 0.0032,
      "step": 7080
    },
    {
      "epoch": 0.3781333333333333,
      "grad_norm": 0.08884382247924805,
      "learning_rate": 4.763666666666667e-05,
      "loss": 0.0037,
      "step": 7090
    },
    {
      "epoch": 0.37866666666666665,
      "grad_norm": 1.0021153688430786,
      "learning_rate": 4.763333333333334e-05,
      "loss": 0.0041,
      "step": 7100
    },
    {
      "epoch": 0.3792,
      "grad_norm": 0.2660015821456909,
      "learning_rate": 4.763e-05,
      "loss": 0.0025,
      "step": 7110
    },
    {
      "epoch": 0.3797333333333333,
      "grad_norm": 0.826493501663208,
      "learning_rate": 4.762666666666667e-05,
      "loss": 0.0052,
      "step": 7120
    },
    {
      "epoch": 0.38026666666666664,
      "grad_norm": 0.5595168471336365,
      "learning_rate": 4.7623333333333335e-05,
      "loss": 0.0043,
      "step": 7130
    },
    {
      "epoch": 0.3808,
      "grad_norm": 0.3832337558269501,
      "learning_rate": 4.762e-05,
      "loss": 0.0046,
      "step": 7140
    },
    {
      "epoch": 0.38133333333333336,
      "grad_norm": 0.3239229619503021,
      "learning_rate": 4.761666666666667e-05,
      "loss": 0.0051,
      "step": 7150
    },
    {
      "epoch": 0.3818666666666667,
      "grad_norm": 0.3236420154571533,
      "learning_rate": 4.761333333333334e-05,
      "loss": 0.0032,
      "step": 7160
    },
    {
      "epoch": 0.3824,
      "grad_norm": 0.118045374751091,
      "learning_rate": 4.761000000000001e-05,
      "loss": 0.0047,
      "step": 7170
    },
    {
      "epoch": 0.38293333333333335,
      "grad_norm": 0.32366326451301575,
      "learning_rate": 4.760666666666667e-05,
      "loss": 0.0032,
      "step": 7180
    },
    {
      "epoch": 0.3834666666666667,
      "grad_norm": 0.20633356273174286,
      "learning_rate": 4.760333333333333e-05,
      "loss": 0.0033,
      "step": 7190
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.0588361918926239,
      "learning_rate": 4.76e-05,
      "loss": 0.0045,
      "step": 7200
    },
    {
      "epoch": 0.38453333333333334,
      "grad_norm": 0.029610250145196915,
      "learning_rate": 4.7596666666666664e-05,
      "loss": 0.0029,
      "step": 7210
    },
    {
      "epoch": 0.38506666666666667,
      "grad_norm": 0.11822899430990219,
      "learning_rate": 4.759333333333334e-05,
      "loss": 0.0031,
      "step": 7220
    },
    {
      "epoch": 0.3856,
      "grad_norm": 0.11798837780952454,
      "learning_rate": 4.7590000000000003e-05,
      "loss": 0.0033,
      "step": 7230
    },
    {
      "epoch": 0.38613333333333333,
      "grad_norm": 0.14719827473163605,
      "learning_rate": 4.758666666666667e-05,
      "loss": 0.0049,
      "step": 7240
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.2653510272502899,
      "learning_rate": 4.7583333333333336e-05,
      "loss": 0.005,
      "step": 7250
    },
    {
      "epoch": 0.3872,
      "grad_norm": 0.4703061580657959,
      "learning_rate": 4.758e-05,
      "loss": 0.0028,
      "step": 7260
    },
    {
      "epoch": 0.3877333333333333,
      "grad_norm": 0.6176801323890686,
      "learning_rate": 4.757666666666667e-05,
      "loss": 0.0039,
      "step": 7270
    },
    {
      "epoch": 0.38826666666666665,
      "grad_norm": 0.6165218353271484,
      "learning_rate": 4.7573333333333334e-05,
      "loss": 0.0042,
      "step": 7280
    },
    {
      "epoch": 0.3888,
      "grad_norm": 0.29382577538490295,
      "learning_rate": 4.757e-05,
      "loss": 0.0042,
      "step": 7290
    },
    {
      "epoch": 0.3893333333333333,
      "grad_norm": 0.03233148530125618,
      "learning_rate": 4.756666666666667e-05,
      "loss": 0.0034,
      "step": 7300
    },
    {
      "epoch": 0.38986666666666664,
      "grad_norm": 0.006848089396953583,
      "learning_rate": 4.756333333333334e-05,
      "loss": 0.0032,
      "step": 7310
    },
    {
      "epoch": 0.3904,
      "grad_norm": 0.2644696533679962,
      "learning_rate": 4.7560000000000005e-05,
      "loss": 0.0031,
      "step": 7320
    },
    {
      "epoch": 0.39093333333333335,
      "grad_norm": 0.2640480101108551,
      "learning_rate": 4.755666666666667e-05,
      "loss": 0.0041,
      "step": 7330
    },
    {
      "epoch": 0.3914666666666667,
      "grad_norm": 0.41081997752189636,
      "learning_rate": 4.755333333333333e-05,
      "loss": 0.0035,
      "step": 7340
    },
    {
      "epoch": 0.392,
      "grad_norm": 0.17626357078552246,
      "learning_rate": 4.755e-05,
      "loss": 0.004,
      "step": 7350
    },
    {
      "epoch": 0.39253333333333335,
      "grad_norm": 0.44120487570762634,
      "learning_rate": 4.754666666666667e-05,
      "loss": 0.0052,
      "step": 7360
    },
    {
      "epoch": 0.3930666666666667,
      "grad_norm": 0.005610471125692129,
      "learning_rate": 4.7543333333333336e-05,
      "loss": 0.0041,
      "step": 7370
    },
    {
      "epoch": 0.3936,
      "grad_norm": 0.05879773944616318,
      "learning_rate": 4.754e-05,
      "loss": 0.0044,
      "step": 7380
    },
    {
      "epoch": 0.39413333333333334,
      "grad_norm": 0.009225831367075443,
      "learning_rate": 4.753666666666667e-05,
      "loss": 0.0044,
      "step": 7390
    },
    {
      "epoch": 0.39466666666666667,
      "grad_norm": 0.14700908958911896,
      "learning_rate": 4.7533333333333334e-05,
      "loss": 0.0036,
      "step": 7400
    },
    {
      "epoch": 0.3952,
      "grad_norm": 0.4714001715183258,
      "learning_rate": 4.753e-05,
      "loss": 0.0032,
      "step": 7410
    },
    {
      "epoch": 0.3957333333333333,
      "grad_norm": 0.1767367422580719,
      "learning_rate": 4.752666666666667e-05,
      "loss": 0.0037,
      "step": 7420
    },
    {
      "epoch": 0.39626666666666666,
      "grad_norm": 0.0587836317718029,
      "learning_rate": 4.752333333333334e-05,
      "loss": 0.0044,
      "step": 7430
    },
    {
      "epoch": 0.3968,
      "grad_norm": 0.030581871047616005,
      "learning_rate": 4.7520000000000006e-05,
      "loss": 0.0043,
      "step": 7440
    },
    {
      "epoch": 0.3973333333333333,
      "grad_norm": 0.2052515149116516,
      "learning_rate": 4.751666666666667e-05,
      "loss": 0.0038,
      "step": 7450
    },
    {
      "epoch": 0.39786666666666665,
      "grad_norm": 0.08855802565813065,
      "learning_rate": 4.751333333333334e-05,
      "loss": 0.0042,
      "step": 7460
    },
    {
      "epoch": 0.3984,
      "grad_norm": 0.35303395986557007,
      "learning_rate": 4.7510000000000004e-05,
      "loss": 0.0043,
      "step": 7470
    },
    {
      "epoch": 0.3989333333333333,
      "grad_norm": 0.29430317878723145,
      "learning_rate": 4.750666666666667e-05,
      "loss": 0.0042,
      "step": 7480
    },
    {
      "epoch": 0.3994666666666667,
      "grad_norm": 0.05903052166104317,
      "learning_rate": 4.750333333333333e-05,
      "loss": 0.0036,
      "step": 7490
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.23524783551692963,
      "learning_rate": 4.75e-05,
      "loss": 0.0027,
      "step": 7500
    },
    {
      "epoch": 0.40053333333333335,
      "grad_norm": 0.058883052319288254,
      "learning_rate": 4.749666666666667e-05,
      "loss": 0.0038,
      "step": 7510
    },
    {
      "epoch": 0.4010666666666667,
      "grad_norm": 0.05886785313487053,
      "learning_rate": 4.7493333333333335e-05,
      "loss": 0.0039,
      "step": 7520
    },
    {
      "epoch": 0.4016,
      "grad_norm": 0.23545028269290924,
      "learning_rate": 4.749e-05,
      "loss": 0.0038,
      "step": 7530
    },
    {
      "epoch": 0.40213333333333334,
      "grad_norm": 0.35350316762924194,
      "learning_rate": 4.748666666666667e-05,
      "loss": 0.0041,
      "step": 7540
    },
    {
      "epoch": 0.4026666666666667,
      "grad_norm": 0.4417973458766937,
      "learning_rate": 4.748333333333333e-05,
      "loss": 0.0036,
      "step": 7550
    },
    {
      "epoch": 0.4032,
      "grad_norm": 0.015113301575183868,
      "learning_rate": 4.748e-05,
      "loss": 0.0028,
      "step": 7560
    },
    {
      "epoch": 0.40373333333333333,
      "grad_norm": 0.059636205434799194,
      "learning_rate": 4.747666666666667e-05,
      "loss": 0.0048,
      "step": 7570
    },
    {
      "epoch": 0.40426666666666666,
      "grad_norm": 0.032779380679130554,
      "learning_rate": 4.747333333333334e-05,
      "loss": 0.0045,
      "step": 7580
    },
    {
      "epoch": 0.4048,
      "grad_norm": 0.7386816740036011,
      "learning_rate": 4.7470000000000005e-05,
      "loss": 0.0036,
      "step": 7590
    },
    {
      "epoch": 0.4053333333333333,
      "grad_norm": 0.23649269342422485,
      "learning_rate": 4.746666666666667e-05,
      "loss": 0.0047,
      "step": 7600
    },
    {
      "epoch": 0.40586666666666665,
      "grad_norm": 0.3843216001987457,
      "learning_rate": 4.746333333333334e-05,
      "loss": 0.0043,
      "step": 7610
    },
    {
      "epoch": 0.4064,
      "grad_norm": 0.23599201440811157,
      "learning_rate": 4.746e-05,
      "loss": 0.0045,
      "step": 7620
    },
    {
      "epoch": 0.4069333333333333,
      "grad_norm": 0.1768895983695984,
      "learning_rate": 4.745666666666667e-05,
      "loss": 0.0049,
      "step": 7630
    },
    {
      "epoch": 0.40746666666666664,
      "grad_norm": 0.1766088604927063,
      "learning_rate": 4.7453333333333335e-05,
      "loss": 0.0033,
      "step": 7640
    },
    {
      "epoch": 0.408,
      "grad_norm": 0.05884260684251785,
      "learning_rate": 4.745e-05,
      "loss": 0.0058,
      "step": 7650
    },
    {
      "epoch": 0.40853333333333336,
      "grad_norm": 0.38240399956703186,
      "learning_rate": 4.744666666666667e-05,
      "loss": 0.0042,
      "step": 7660
    },
    {
      "epoch": 0.4090666666666667,
      "grad_norm": 0.5882204174995422,
      "learning_rate": 4.7443333333333334e-05,
      "loss": 0.0038,
      "step": 7670
    },
    {
      "epoch": 0.4096,
      "grad_norm": 0.2644632160663605,
      "learning_rate": 4.744e-05,
      "loss": 0.0033,
      "step": 7680
    },
    {
      "epoch": 0.41013333333333335,
      "grad_norm": 0.26514482498168945,
      "learning_rate": 4.7436666666666666e-05,
      "loss": 0.0044,
      "step": 7690
    },
    {
      "epoch": 0.4106666666666667,
      "grad_norm": 0.1483144760131836,
      "learning_rate": 4.743333333333333e-05,
      "loss": 0.0044,
      "step": 7700
    },
    {
      "epoch": 0.4112,
      "grad_norm": 0.1471453309059143,
      "learning_rate": 4.7430000000000005e-05,
      "loss": 0.0036,
      "step": 7710
    },
    {
      "epoch": 0.41173333333333334,
      "grad_norm": 0.49954357743263245,
      "learning_rate": 4.742666666666667e-05,
      "loss": 0.0034,
      "step": 7720
    },
    {
      "epoch": 0.41226666666666667,
      "grad_norm": 0.11770013719797134,
      "learning_rate": 4.742333333333334e-05,
      "loss": 0.0048,
      "step": 7730
    },
    {
      "epoch": 0.4128,
      "grad_norm": 0.2056274563074112,
      "learning_rate": 4.742e-05,
      "loss": 0.005,
      "step": 7740
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.32350069284439087,
      "learning_rate": 4.741666666666667e-05,
      "loss": 0.0032,
      "step": 7750
    },
    {
      "epoch": 0.41386666666666666,
      "grad_norm": 0.14696870744228363,
      "learning_rate": 4.7413333333333336e-05,
      "loss": 0.0034,
      "step": 7760
    },
    {
      "epoch": 0.4144,
      "grad_norm": 0.1763223260641098,
      "learning_rate": 4.741e-05,
      "loss": 0.0038,
      "step": 7770
    },
    {
      "epoch": 0.4149333333333333,
      "grad_norm": 0.08813634514808655,
      "learning_rate": 4.7406666666666675e-05,
      "loss": 0.0042,
      "step": 7780
    },
    {
      "epoch": 0.41546666666666665,
      "grad_norm": 0.8309366106987,
      "learning_rate": 4.7403333333333334e-05,
      "loss": 0.0043,
      "step": 7790
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.1176895722746849,
      "learning_rate": 4.74e-05,
      "loss": 0.0044,
      "step": 7800
    },
    {
      "epoch": 0.4165333333333333,
      "grad_norm": 0.05897669866681099,
      "learning_rate": 4.7396666666666666e-05,
      "loss": 0.0044,
      "step": 7810
    },
    {
      "epoch": 0.41706666666666664,
      "grad_norm": 0.08873109519481659,
      "learning_rate": 4.739333333333333e-05,
      "loss": 0.0051,
      "step": 7820
    },
    {
      "epoch": 0.4176,
      "grad_norm": 0.1762440949678421,
      "learning_rate": 4.739e-05,
      "loss": 0.0036,
      "step": 7830
    },
    {
      "epoch": 0.41813333333333336,
      "grad_norm": 0.14676809310913086,
      "learning_rate": 4.7386666666666665e-05,
      "loss": 0.003,
      "step": 7840
    },
    {
      "epoch": 0.4186666666666667,
      "grad_norm": 0.08843546360731125,
      "learning_rate": 4.738333333333334e-05,
      "loss": 0.0038,
      "step": 7850
    },
    {
      "epoch": 0.4192,
      "grad_norm": 0.02980062924325466,
      "learning_rate": 4.7380000000000004e-05,
      "loss": 0.0029,
      "step": 7860
    },
    {
      "epoch": 0.41973333333333335,
      "grad_norm": 1.5437597036361694,
      "learning_rate": 4.737666666666667e-05,
      "loss": 0.0037,
      "step": 7870
    },
    {
      "epoch": 0.4202666666666667,
      "grad_norm": 0.3820052742958069,
      "learning_rate": 4.7373333333333336e-05,
      "loss": 0.0023,
      "step": 7880
    },
    {
      "epoch": 0.4208,
      "grad_norm": 0.20602306723594666,
      "learning_rate": 4.737e-05,
      "loss": 0.0028,
      "step": 7890
    },
    {
      "epoch": 0.42133333333333334,
      "grad_norm": 0.0024088993668556213,
      "learning_rate": 4.736666666666667e-05,
      "loss": 0.0043,
      "step": 7900
    },
    {
      "epoch": 0.42186666666666667,
      "grad_norm": 0.0026031211018562317,
      "learning_rate": 4.7363333333333334e-05,
      "loss": 0.0045,
      "step": 7910
    },
    {
      "epoch": 0.4224,
      "grad_norm": 0.2057258039712906,
      "learning_rate": 4.736000000000001e-05,
      "loss": 0.0032,
      "step": 7920
    },
    {
      "epoch": 0.42293333333333333,
      "grad_norm": 0.38201892375946045,
      "learning_rate": 4.7356666666666673e-05,
      "loss": 0.0024,
      "step": 7930
    },
    {
      "epoch": 0.42346666666666666,
      "grad_norm": 0.029396409168839455,
      "learning_rate": 4.735333333333333e-05,
      "loss": 0.0041,
      "step": 7940
    },
    {
      "epoch": 0.424,
      "grad_norm": 0.4112200140953064,
      "learning_rate": 4.735e-05,
      "loss": 0.0038,
      "step": 7950
    },
    {
      "epoch": 0.4245333333333333,
      "grad_norm": 0.030891962349414825,
      "learning_rate": 4.7346666666666665e-05,
      "loss": 0.0036,
      "step": 7960
    },
    {
      "epoch": 0.42506666666666665,
      "grad_norm": 0.26441290974617004,
      "learning_rate": 4.734333333333333e-05,
      "loss": 0.0031,
      "step": 7970
    },
    {
      "epoch": 0.4256,
      "grad_norm": 0.49931108951568604,
      "learning_rate": 4.7340000000000004e-05,
      "loss": 0.004,
      "step": 7980
    },
    {
      "epoch": 0.4261333333333333,
      "grad_norm": 0.05860741809010506,
      "learning_rate": 4.733666666666667e-05,
      "loss": 0.0029,
      "step": 7990
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.17624598741531372,
      "learning_rate": 4.7333333333333336e-05,
      "loss": 0.0033,
      "step": 8000
    },
    {
      "epoch": 0.4272,
      "grad_norm": 0.1761828362941742,
      "learning_rate": 4.733e-05,
      "loss": 0.0033,
      "step": 8010
    },
    {
      "epoch": 0.42773333333333335,
      "grad_norm": 0.20581284165382385,
      "learning_rate": 4.732666666666667e-05,
      "loss": 0.0041,
      "step": 8020
    },
    {
      "epoch": 0.4282666666666667,
      "grad_norm": 0.12608866393566132,
      "learning_rate": 4.7323333333333335e-05,
      "loss": 0.0044,
      "step": 8030
    },
    {
      "epoch": 0.4288,
      "grad_norm": 1.5642403364181519,
      "learning_rate": 4.732e-05,
      "loss": 0.0036,
      "step": 8040
    },
    {
      "epoch": 0.42933333333333334,
      "grad_norm": 0.20541591942310333,
      "learning_rate": 4.731666666666667e-05,
      "loss": 0.0037,
      "step": 8050
    },
    {
      "epoch": 0.4298666666666667,
      "grad_norm": 0.08765215426683426,
      "learning_rate": 4.731333333333334e-05,
      "loss": 0.0029,
      "step": 8060
    },
    {
      "epoch": 0.4304,
      "grad_norm": 0.23267333209514618,
      "learning_rate": 4.7310000000000006e-05,
      "loss": 0.0043,
      "step": 8070
    },
    {
      "epoch": 0.43093333333333333,
      "grad_norm": 0.6710450649261475,
      "learning_rate": 4.730666666666667e-05,
      "loss": 0.0039,
      "step": 8080
    },
    {
      "epoch": 0.43146666666666667,
      "grad_norm": 0.497010201215744,
      "learning_rate": 4.730333333333333e-05,
      "loss": 0.0032,
      "step": 8090
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.38143017888069153,
      "learning_rate": 4.73e-05,
      "loss": 0.0031,
      "step": 8100
    },
    {
      "epoch": 0.4325333333333333,
      "grad_norm": 0.20543812215328217,
      "learning_rate": 4.7296666666666664e-05,
      "loss": 0.0041,
      "step": 8110
    },
    {
      "epoch": 0.43306666666666666,
      "grad_norm": 0.004454333335161209,
      "learning_rate": 4.729333333333334e-05,
      "loss": 0.0047,
      "step": 8120
    },
    {
      "epoch": 0.4336,
      "grad_norm": 0.05953991413116455,
      "learning_rate": 4.729e-05,
      "loss": 0.0047,
      "step": 8130
    },
    {
      "epoch": 0.4341333333333333,
      "grad_norm": 0.05890009552240372,
      "learning_rate": 4.728666666666667e-05,
      "loss": 0.0056,
      "step": 8140
    },
    {
      "epoch": 0.43466666666666665,
      "grad_norm": 0.058466628193855286,
      "learning_rate": 4.7283333333333335e-05,
      "loss": 0.0042,
      "step": 8150
    },
    {
      "epoch": 0.4352,
      "grad_norm": 0.0015692211454734206,
      "learning_rate": 4.728e-05,
      "loss": 0.0029,
      "step": 8160
    },
    {
      "epoch": 0.4357333333333333,
      "grad_norm": 0.08817023038864136,
      "learning_rate": 4.727666666666667e-05,
      "loss": 0.0033,
      "step": 8170
    },
    {
      "epoch": 0.4362666666666667,
      "grad_norm": 0.44023048877716064,
      "learning_rate": 4.7273333333333334e-05,
      "loss": 0.0049,
      "step": 8180
    },
    {
      "epoch": 0.4368,
      "grad_norm": 0.20545567572116852,
      "learning_rate": 4.7270000000000007e-05,
      "loss": 0.005,
      "step": 8190
    },
    {
      "epoch": 0.43733333333333335,
      "grad_norm": 0.35241013765335083,
      "learning_rate": 4.726666666666667e-05,
      "loss": 0.0051,
      "step": 8200
    },
    {
      "epoch": 0.4378666666666667,
      "grad_norm": 0.6461320519447327,
      "learning_rate": 4.726333333333334e-05,
      "loss": 0.0047,
      "step": 8210
    },
    {
      "epoch": 0.4384,
      "grad_norm": 0.11741376668214798,
      "learning_rate": 4.7260000000000005e-05,
      "loss": 0.0033,
      "step": 8220
    },
    {
      "epoch": 0.43893333333333334,
      "grad_norm": 0.17623890936374664,
      "learning_rate": 4.725666666666667e-05,
      "loss": 0.0046,
      "step": 8230
    },
    {
      "epoch": 0.43946666666666667,
      "grad_norm": 0.11740794777870178,
      "learning_rate": 4.725333333333334e-05,
      "loss": 0.0029,
      "step": 8240
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.6457788944244385,
      "learning_rate": 4.7249999999999997e-05,
      "loss": 0.0047,
      "step": 8250
    },
    {
      "epoch": 0.44053333333333333,
      "grad_norm": 0.264425128698349,
      "learning_rate": 4.724666666666667e-05,
      "loss": 0.0032,
      "step": 8260
    },
    {
      "epoch": 0.44106666666666666,
      "grad_norm": 0.3517078459262848,
      "learning_rate": 4.7243333333333336e-05,
      "loss": 0.0049,
      "step": 8270
    },
    {
      "epoch": 0.4416,
      "grad_norm": 0.13308851420879364,
      "learning_rate": 4.724e-05,
      "loss": 0.004,
      "step": 8280
    },
    {
      "epoch": 0.4421333333333333,
      "grad_norm": 0.17607881128787994,
      "learning_rate": 4.723666666666667e-05,
      "loss": 0.004,
      "step": 8290
    },
    {
      "epoch": 0.44266666666666665,
      "grad_norm": 0.2937091588973999,
      "learning_rate": 4.7233333333333334e-05,
      "loss": 0.0043,
      "step": 8300
    },
    {
      "epoch": 0.4432,
      "grad_norm": 0.32329124212265015,
      "learning_rate": 4.723e-05,
      "loss": 0.0044,
      "step": 8310
    },
    {
      "epoch": 0.4437333333333333,
      "grad_norm": 0.3231433928012848,
      "learning_rate": 4.7226666666666666e-05,
      "loss": 0.0042,
      "step": 8320
    },
    {
      "epoch": 0.44426666666666664,
      "grad_norm": 0.6174162030220032,
      "learning_rate": 4.722333333333334e-05,
      "loss": 0.0039,
      "step": 8330
    },
    {
      "epoch": 0.4448,
      "grad_norm": 0.4403308629989624,
      "learning_rate": 4.7220000000000005e-05,
      "loss": 0.0051,
      "step": 8340
    },
    {
      "epoch": 0.44533333333333336,
      "grad_norm": 0.23456577956676483,
      "learning_rate": 4.721666666666667e-05,
      "loss": 0.0026,
      "step": 8350
    },
    {
      "epoch": 0.4458666666666667,
      "grad_norm": 0.2634904682636261,
      "learning_rate": 4.721333333333334e-05,
      "loss": 0.002,
      "step": 8360
    },
    {
      "epoch": 0.4464,
      "grad_norm": 0.23457026481628418,
      "learning_rate": 4.7210000000000004e-05,
      "loss": 0.0043,
      "step": 8370
    },
    {
      "epoch": 0.44693333333333335,
      "grad_norm": 0.059124670922756195,
      "learning_rate": 4.720666666666667e-05,
      "loss": 0.0032,
      "step": 8380
    },
    {
      "epoch": 0.4474666666666667,
      "grad_norm": 0.439646452665329,
      "learning_rate": 4.7203333333333336e-05,
      "loss": 0.0026,
      "step": 8390
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.20521633327007294,
      "learning_rate": 4.72e-05,
      "loss": 0.005,
      "step": 8400
    },
    {
      "epoch": 0.44853333333333334,
      "grad_norm": 0.35121458768844604,
      "learning_rate": 4.719666666666667e-05,
      "loss": 0.0027,
      "step": 8410
    },
    {
      "epoch": 0.44906666666666667,
      "grad_norm": 0.05863117426633835,
      "learning_rate": 4.7193333333333334e-05,
      "loss": 0.0036,
      "step": 8420
    },
    {
      "epoch": 0.4496,
      "grad_norm": 0.4393977224826813,
      "learning_rate": 4.719e-05,
      "loss": 0.0033,
      "step": 8430
    },
    {
      "epoch": 0.45013333333333333,
      "grad_norm": 0.23429691791534424,
      "learning_rate": 4.718666666666667e-05,
      "loss": 0.0037,
      "step": 8440
    },
    {
      "epoch": 0.45066666666666666,
      "grad_norm": 0.2344173640012741,
      "learning_rate": 4.718333333333333e-05,
      "loss": 0.003,
      "step": 8450
    },
    {
      "epoch": 0.4512,
      "grad_norm": 0.4099564552307129,
      "learning_rate": 4.718e-05,
      "loss": 0.0038,
      "step": 8460
    },
    {
      "epoch": 0.4517333333333333,
      "grad_norm": 0.08862036466598511,
      "learning_rate": 4.717666666666667e-05,
      "loss": 0.0032,
      "step": 8470
    },
    {
      "epoch": 0.45226666666666665,
      "grad_norm": 0.05871494114398956,
      "learning_rate": 4.717333333333334e-05,
      "loss": 0.0029,
      "step": 8480
    },
    {
      "epoch": 0.4528,
      "grad_norm": 0.4097757339477539,
      "learning_rate": 4.7170000000000004e-05,
      "loss": 0.0041,
      "step": 8490
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.26323676109313965,
      "learning_rate": 4.716666666666667e-05,
      "loss": 0.0038,
      "step": 8500
    },
    {
      "epoch": 0.45386666666666664,
      "grad_norm": 0.11704232543706894,
      "learning_rate": 4.7163333333333336e-05,
      "loss": 0.0041,
      "step": 8510
    },
    {
      "epoch": 0.4544,
      "grad_norm": 0.058966007083654404,
      "learning_rate": 4.716e-05,
      "loss": 0.0054,
      "step": 8520
    },
    {
      "epoch": 0.45493333333333336,
      "grad_norm": 0.11727911978960037,
      "learning_rate": 4.715666666666667e-05,
      "loss": 0.0044,
      "step": 8530
    },
    {
      "epoch": 0.4554666666666667,
      "grad_norm": 0.059007491916418076,
      "learning_rate": 4.715333333333334e-05,
      "loss": 0.0028,
      "step": 8540
    },
    {
      "epoch": 0.456,
      "grad_norm": 0.14655941724777222,
      "learning_rate": 4.715e-05,
      "loss": 0.0021,
      "step": 8550
    },
    {
      "epoch": 0.45653333333333335,
      "grad_norm": 0.4680914580821991,
      "learning_rate": 4.714666666666667e-05,
      "loss": 0.0049,
      "step": 8560
    },
    {
      "epoch": 0.4570666666666667,
      "grad_norm": 0.1461452692747116,
      "learning_rate": 4.714333333333333e-05,
      "loss": 0.0037,
      "step": 8570
    },
    {
      "epoch": 0.4576,
      "grad_norm": 0.17590710520744324,
      "learning_rate": 4.714e-05,
      "loss": 0.0032,
      "step": 8580
    },
    {
      "epoch": 0.45813333333333334,
      "grad_norm": 0.1571136713027954,
      "learning_rate": 4.7136666666666665e-05,
      "loss": 0.0025,
      "step": 8590
    },
    {
      "epoch": 0.45866666666666667,
      "grad_norm": 0.29260867834091187,
      "learning_rate": 4.713333333333333e-05,
      "loss": 0.0036,
      "step": 8600
    },
    {
      "epoch": 0.4592,
      "grad_norm": 0.2341553270816803,
      "learning_rate": 4.7130000000000004e-05,
      "loss": 0.0037,
      "step": 8610
    },
    {
      "epoch": 0.4597333333333333,
      "grad_norm": 0.1463053673505783,
      "learning_rate": 4.712666666666667e-05,
      "loss": 0.0044,
      "step": 8620
    },
    {
      "epoch": 0.46026666666666666,
      "grad_norm": 0.3512320816516876,
      "learning_rate": 4.712333333333334e-05,
      "loss": 0.0028,
      "step": 8630
    },
    {
      "epoch": 0.4608,
      "grad_norm": 0.11711442470550537,
      "learning_rate": 4.712e-05,
      "loss": 0.004,
      "step": 8640
    },
    {
      "epoch": 0.4613333333333333,
      "grad_norm": 0.3509310483932495,
      "learning_rate": 4.711666666666667e-05,
      "loss": 0.0038,
      "step": 8650
    },
    {
      "epoch": 0.46186666666666665,
      "grad_norm": 0.003944049589335918,
      "learning_rate": 4.7113333333333335e-05,
      "loss": 0.0038,
      "step": 8660
    },
    {
      "epoch": 0.4624,
      "grad_norm": 0.05904217064380646,
      "learning_rate": 4.711e-05,
      "loss": 0.0024,
      "step": 8670
    },
    {
      "epoch": 0.4629333333333333,
      "grad_norm": 0.0032312029507011175,
      "learning_rate": 4.7106666666666674e-05,
      "loss": 0.0048,
      "step": 8680
    },
    {
      "epoch": 0.4634666666666667,
      "grad_norm": 0.292654812335968,
      "learning_rate": 4.710333333333334e-05,
      "loss": 0.0024,
      "step": 8690
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.4094434380531311,
      "learning_rate": 4.71e-05,
      "loss": 0.0045,
      "step": 8700
    },
    {
      "epoch": 0.46453333333333335,
      "grad_norm": 0.029456250369548798,
      "learning_rate": 4.7096666666666666e-05,
      "loss": 0.0043,
      "step": 8710
    },
    {
      "epoch": 0.4650666666666667,
      "grad_norm": 0.05840584635734558,
      "learning_rate": 4.709333333333333e-05,
      "loss": 0.0039,
      "step": 8720
    },
    {
      "epoch": 0.4656,
      "grad_norm": 0.23379038274288177,
      "learning_rate": 4.709e-05,
      "loss": 0.0032,
      "step": 8730
    },
    {
      "epoch": 0.46613333333333334,
      "grad_norm": 0.17578130960464478,
      "learning_rate": 4.708666666666667e-05,
      "loss": 0.0044,
      "step": 8740
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.1758679449558258,
      "learning_rate": 4.708333333333334e-05,
      "loss": 0.0027,
      "step": 8750
    },
    {
      "epoch": 0.4672,
      "grad_norm": 0.6160942912101746,
      "learning_rate": 4.708e-05,
      "loss": 0.0031,
      "step": 8760
    },
    {
      "epoch": 0.46773333333333333,
      "grad_norm": 0.20484136044979095,
      "learning_rate": 4.707666666666667e-05,
      "loss": 0.0018,
      "step": 8770
    },
    {
      "epoch": 0.46826666666666666,
      "grad_norm": 0.5144007205963135,
      "learning_rate": 4.7073333333333336e-05,
      "loss": 0.0039,
      "step": 8780
    },
    {
      "epoch": 0.4688,
      "grad_norm": 0.7521672248840332,
      "learning_rate": 4.707e-05,
      "loss": 0.0081,
      "step": 8790
    },
    {
      "epoch": 0.4693333333333333,
      "grad_norm": 0.32235923409461975,
      "learning_rate": 4.706666666666667e-05,
      "loss": 0.0034,
      "step": 8800
    },
    {
      "epoch": 0.46986666666666665,
      "grad_norm": 0.2344837784767151,
      "learning_rate": 4.7063333333333334e-05,
      "loss": 0.0046,
      "step": 8810
    },
    {
      "epoch": 0.4704,
      "grad_norm": 0.4106557071208954,
      "learning_rate": 4.706000000000001e-05,
      "loss": 0.0052,
      "step": 8820
    },
    {
      "epoch": 0.4709333333333333,
      "grad_norm": 0.6447245478630066,
      "learning_rate": 4.705666666666667e-05,
      "loss": 0.0035,
      "step": 8830
    },
    {
      "epoch": 0.47146666666666665,
      "grad_norm": 0.2633737325668335,
      "learning_rate": 4.705333333333334e-05,
      "loss": 0.0043,
      "step": 8840
    },
    {
      "epoch": 0.472,
      "grad_norm": 0.058952391147613525,
      "learning_rate": 4.705e-05,
      "loss": 0.0038,
      "step": 8850
    },
    {
      "epoch": 0.47253333333333336,
      "grad_norm": 0.3514203429222107,
      "learning_rate": 4.7046666666666665e-05,
      "loss": 0.0061,
      "step": 8860
    },
    {
      "epoch": 0.4730666666666667,
      "grad_norm": 0.05893649533390999,
      "learning_rate": 4.704333333333333e-05,
      "loss": 0.0033,
      "step": 8870
    },
    {
      "epoch": 0.4736,
      "grad_norm": 0.3838149905204773,
      "learning_rate": 4.7040000000000004e-05,
      "loss": 0.0034,
      "step": 8880
    },
    {
      "epoch": 0.47413333333333335,
      "grad_norm": 0.08797317743301392,
      "learning_rate": 4.703666666666667e-05,
      "loss": 0.0039,
      "step": 8890
    },
    {
      "epoch": 0.4746666666666667,
      "grad_norm": 0.2934962809085846,
      "learning_rate": 4.7033333333333336e-05,
      "loss": 0.0031,
      "step": 8900
    },
    {
      "epoch": 0.4752,
      "grad_norm": 0.029408887028694153,
      "learning_rate": 4.703e-05,
      "loss": 0.0036,
      "step": 8910
    },
    {
      "epoch": 0.47573333333333334,
      "grad_norm": 0.23429907858371735,
      "learning_rate": 4.702666666666667e-05,
      "loss": 0.005,
      "step": 8920
    },
    {
      "epoch": 0.47626666666666667,
      "grad_norm": 0.08776978403329849,
      "learning_rate": 4.7023333333333334e-05,
      "loss": 0.005,
      "step": 8930
    },
    {
      "epoch": 0.4768,
      "grad_norm": 0.3420850932598114,
      "learning_rate": 4.702e-05,
      "loss": 0.0049,
      "step": 8940
    },
    {
      "epoch": 0.47733333333333333,
      "grad_norm": 0.5560960173606873,
      "learning_rate": 4.701666666666667e-05,
      "loss": 0.0038,
      "step": 8950
    },
    {
      "epoch": 0.47786666666666666,
      "grad_norm": 0.23392567038536072,
      "learning_rate": 4.701333333333334e-05,
      "loss": 0.0031,
      "step": 8960
    },
    {
      "epoch": 0.4784,
      "grad_norm": 0.2922901213169098,
      "learning_rate": 4.7010000000000006e-05,
      "loss": 0.0027,
      "step": 8970
    },
    {
      "epoch": 0.4789333333333333,
      "grad_norm": 0.1462986022233963,
      "learning_rate": 4.700666666666667e-05,
      "loss": 0.0041,
      "step": 8980
    },
    {
      "epoch": 0.47946666666666665,
      "grad_norm": 0.030135706067085266,
      "learning_rate": 4.700333333333334e-05,
      "loss": 0.0041,
      "step": 8990
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.05855210870504379,
      "learning_rate": 4.7e-05,
      "loss": 0.0032,
      "step": 9000
    },
    {
      "epoch": 0.4805333333333333,
      "grad_norm": 0.5850507616996765,
      "learning_rate": 4.699666666666666e-05,
      "loss": 0.0025,
      "step": 9010
    },
    {
      "epoch": 0.48106666666666664,
      "grad_norm": 0.23366057872772217,
      "learning_rate": 4.6993333333333336e-05,
      "loss": 0.0034,
      "step": 9020
    },
    {
      "epoch": 0.4816,
      "grad_norm": 0.1460975557565689,
      "learning_rate": 4.699e-05,
      "loss": 0.004,
      "step": 9030
    },
    {
      "epoch": 0.48213333333333336,
      "grad_norm": 0.08768987655639648,
      "learning_rate": 4.698666666666667e-05,
      "loss": 0.0023,
      "step": 9040
    },
    {
      "epoch": 0.4826666666666667,
      "grad_norm": 0.029254762455821037,
      "learning_rate": 4.6983333333333335e-05,
      "loss": 0.0043,
      "step": 9050
    },
    {
      "epoch": 0.4832,
      "grad_norm": 0.0035835502203553915,
      "learning_rate": 4.698e-05,
      "loss": 0.0028,
      "step": 9060
    },
    {
      "epoch": 0.48373333333333335,
      "grad_norm": 0.23377372324466705,
      "learning_rate": 4.697666666666667e-05,
      "loss": 0.0041,
      "step": 9070
    },
    {
      "epoch": 0.4842666666666667,
      "grad_norm": 0.17537645995616913,
      "learning_rate": 4.697333333333333e-05,
      "loss": 0.004,
      "step": 9080
    },
    {
      "epoch": 0.4848,
      "grad_norm": 0.05842026323080063,
      "learning_rate": 4.6970000000000006e-05,
      "loss": 0.0048,
      "step": 9090
    },
    {
      "epoch": 0.48533333333333334,
      "grad_norm": 0.32119476795196533,
      "learning_rate": 4.696666666666667e-05,
      "loss": 0.0035,
      "step": 9100
    },
    {
      "epoch": 0.48586666666666667,
      "grad_norm": 0.292178213596344,
      "learning_rate": 4.696333333333334e-05,
      "loss": 0.0043,
      "step": 9110
    },
    {
      "epoch": 0.4864,
      "grad_norm": 0.2920129597187042,
      "learning_rate": 4.6960000000000004e-05,
      "loss": 0.0051,
      "step": 9120
    },
    {
      "epoch": 0.48693333333333333,
      "grad_norm": 0.37947118282318115,
      "learning_rate": 4.695666666666667e-05,
      "loss": 0.0043,
      "step": 9130
    },
    {
      "epoch": 0.48746666666666666,
      "grad_norm": 0.0876353532075882,
      "learning_rate": 4.695333333333334e-05,
      "loss": 0.0032,
      "step": 9140
    },
    {
      "epoch": 0.488,
      "grad_norm": 0.1753104329109192,
      "learning_rate": 4.695e-05,
      "loss": 0.004,
      "step": 9150
    },
    {
      "epoch": 0.4885333333333333,
      "grad_norm": 0.08762216567993164,
      "learning_rate": 4.694666666666667e-05,
      "loss": 0.0024,
      "step": 9160
    },
    {
      "epoch": 0.48906666666666665,
      "grad_norm": 0.2638130486011505,
      "learning_rate": 4.6943333333333335e-05,
      "loss": 0.0028,
      "step": 9170
    },
    {
      "epoch": 0.4896,
      "grad_norm": 0.20555351674556732,
      "learning_rate": 4.694e-05,
      "loss": 0.0031,
      "step": 9180
    },
    {
      "epoch": 0.4901333333333333,
      "grad_norm": 0.14669008553028107,
      "learning_rate": 4.693666666666667e-05,
      "loss": 0.0038,
      "step": 9190
    },
    {
      "epoch": 0.49066666666666664,
      "grad_norm": 0.05880240350961685,
      "learning_rate": 4.6933333333333333e-05,
      "loss": 0.0049,
      "step": 9200
    },
    {
      "epoch": 0.4912,
      "grad_norm": 0.3225419223308563,
      "learning_rate": 4.693e-05,
      "loss": 0.0037,
      "step": 9210
    },
    {
      "epoch": 0.49173333333333336,
      "grad_norm": 0.23463034629821777,
      "learning_rate": 4.6926666666666666e-05,
      "loss": 0.0034,
      "step": 9220
    },
    {
      "epoch": 0.4922666666666667,
      "grad_norm": 0.26421260833740234,
      "learning_rate": 4.692333333333334e-05,
      "loss": 0.0035,
      "step": 9230
    },
    {
      "epoch": 0.4928,
      "grad_norm": 0.1763654351234436,
      "learning_rate": 4.6920000000000005e-05,
      "loss": 0.0064,
      "step": 9240
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.029710792005062103,
      "learning_rate": 4.691666666666667e-05,
      "loss": 0.003,
      "step": 9250
    },
    {
      "epoch": 0.4938666666666667,
      "grad_norm": 0.2931346297264099,
      "learning_rate": 4.691333333333334e-05,
      "loss": 0.0033,
      "step": 9260
    },
    {
      "epoch": 0.4944,
      "grad_norm": 0.20499302446842194,
      "learning_rate": 4.691e-05,
      "loss": 0.0025,
      "step": 9270
    },
    {
      "epoch": 0.49493333333333334,
      "grad_norm": 0.058768585324287415,
      "learning_rate": 4.690666666666667e-05,
      "loss": 0.0027,
      "step": 9280
    },
    {
      "epoch": 0.49546666666666667,
      "grad_norm": 0.38011208176612854,
      "learning_rate": 4.6903333333333335e-05,
      "loss": 0.0034,
      "step": 9290
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.17493587732315063,
      "learning_rate": 4.69e-05,
      "loss": 0.0029,
      "step": 9300
    },
    {
      "epoch": 0.4965333333333333,
      "grad_norm": 0.058953363448381424,
      "learning_rate": 4.689666666666667e-05,
      "loss": 0.0031,
      "step": 9310
    },
    {
      "epoch": 0.49706666666666666,
      "grad_norm": 0.5543060302734375,
      "learning_rate": 4.6893333333333334e-05,
      "loss": 0.0049,
      "step": 9320
    },
    {
      "epoch": 0.4976,
      "grad_norm": 0.35049208998680115,
      "learning_rate": 4.689e-05,
      "loss": 0.0033,
      "step": 9330
    },
    {
      "epoch": 0.4981333333333333,
      "grad_norm": 0.17588846385478973,
      "learning_rate": 4.6886666666666666e-05,
      "loss": 0.0044,
      "step": 9340
    },
    {
      "epoch": 0.49866666666666665,
      "grad_norm": 0.003761048661544919,
      "learning_rate": 4.688333333333333e-05,
      "loss": 0.0032,
      "step": 9350
    },
    {
      "epoch": 0.4992,
      "grad_norm": 0.02975332736968994,
      "learning_rate": 4.688e-05,
      "loss": 0.0023,
      "step": 9360
    },
    {
      "epoch": 0.4997333333333333,
      "grad_norm": 0.030886711552739143,
      "learning_rate": 4.687666666666667e-05,
      "loss": 0.004,
      "step": 9370
    },
    {
      "epoch": 0.5002666666666666,
      "grad_norm": 0.4683702290058136,
      "learning_rate": 4.687333333333334e-05,
      "loss": 0.0042,
      "step": 9380
    },
    {
      "epoch": 0.5008,
      "grad_norm": 0.321301132440567,
      "learning_rate": 4.6870000000000004e-05,
      "loss": 0.0045,
      "step": 9390
    },
    {
      "epoch": 0.5013333333333333,
      "grad_norm": 0.17526018619537354,
      "learning_rate": 4.686666666666667e-05,
      "loss": 0.0031,
      "step": 9400
    },
    {
      "epoch": 0.5018666666666667,
      "grad_norm": 0.35053420066833496,
      "learning_rate": 4.6863333333333336e-05,
      "loss": 0.0032,
      "step": 9410
    },
    {
      "epoch": 0.5024,
      "grad_norm": 0.06043079495429993,
      "learning_rate": 4.686e-05,
      "loss": 0.003,
      "step": 9420
    },
    {
      "epoch": 0.5029333333333333,
      "grad_norm": 0.05891682207584381,
      "learning_rate": 4.685666666666667e-05,
      "loss": 0.0018,
      "step": 9430
    },
    {
      "epoch": 0.5034666666666666,
      "grad_norm": 0.2914060950279236,
      "learning_rate": 4.685333333333334e-05,
      "loss": 0.0039,
      "step": 9440
    },
    {
      "epoch": 0.504,
      "grad_norm": 0.4948463439941406,
      "learning_rate": 4.685000000000001e-05,
      "loss": 0.0033,
      "step": 9450
    },
    {
      "epoch": 0.5045333333333333,
      "grad_norm": 0.1454860270023346,
      "learning_rate": 4.6846666666666667e-05,
      "loss": 0.0032,
      "step": 9460
    },
    {
      "epoch": 0.5050666666666667,
      "grad_norm": 0.0029958579689264297,
      "learning_rate": 4.684333333333333e-05,
      "loss": 0.004,
      "step": 9470
    },
    {
      "epoch": 0.5056,
      "grad_norm": 0.05842690169811249,
      "learning_rate": 4.684e-05,
      "loss": 0.0029,
      "step": 9480
    },
    {
      "epoch": 0.5061333333333333,
      "grad_norm": 0.2912825047969818,
      "learning_rate": 4.6836666666666665e-05,
      "loss": 0.0023,
      "step": 9490
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.37915483117103577,
      "learning_rate": 4.683333333333334e-05,
      "loss": 0.0048,
      "step": 9500
    },
    {
      "epoch": 0.5072,
      "grad_norm": 0.43749746680259705,
      "learning_rate": 4.6830000000000004e-05,
      "loss": 0.0029,
      "step": 9510
    },
    {
      "epoch": 0.5077333333333334,
      "grad_norm": 0.29153162240982056,
      "learning_rate": 4.682666666666667e-05,
      "loss": 0.0021,
      "step": 9520
    },
    {
      "epoch": 0.5082666666666666,
      "grad_norm": 0.11666151881217957,
      "learning_rate": 4.6823333333333336e-05,
      "loss": 0.0045,
      "step": 9530
    },
    {
      "epoch": 0.5088,
      "grad_norm": 0.2912476360797882,
      "learning_rate": 4.682e-05,
      "loss": 0.0034,
      "step": 9540
    },
    {
      "epoch": 0.5093333333333333,
      "grad_norm": 0.05837895721197128,
      "learning_rate": 4.681666666666667e-05,
      "loss": 0.0022,
      "step": 9550
    },
    {
      "epoch": 0.5098666666666667,
      "grad_norm": 0.14634379744529724,
      "learning_rate": 4.6813333333333335e-05,
      "loss": 0.002,
      "step": 9560
    },
    {
      "epoch": 0.5104,
      "grad_norm": 0.17516271770000458,
      "learning_rate": 4.681e-05,
      "loss": 0.0038,
      "step": 9570
    },
    {
      "epoch": 0.5109333333333334,
      "grad_norm": 0.23377223312854767,
      "learning_rate": 4.6806666666666674e-05,
      "loss": 0.0036,
      "step": 9580
    },
    {
      "epoch": 0.5114666666666666,
      "grad_norm": 0.20474639534950256,
      "learning_rate": 4.680333333333334e-05,
      "loss": 0.0039,
      "step": 9590
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.20380061864852905,
      "learning_rate": 4.6800000000000006e-05,
      "loss": 0.0032,
      "step": 9600
    },
    {
      "epoch": 0.5125333333333333,
      "grad_norm": 0.37848103046417236,
      "learning_rate": 4.6796666666666665e-05,
      "loss": 0.0025,
      "step": 9610
    },
    {
      "epoch": 0.5130666666666667,
      "grad_norm": 0.11705388873815536,
      "learning_rate": 4.679333333333333e-05,
      "loss": 0.0032,
      "step": 9620
    },
    {
      "epoch": 0.5136,
      "grad_norm": 0.08877492696046829,
      "learning_rate": 4.679e-05,
      "loss": 0.0048,
      "step": 9630
    },
    {
      "epoch": 0.5141333333333333,
      "grad_norm": 0.2627106308937073,
      "learning_rate": 4.678666666666667e-05,
      "loss": 0.0042,
      "step": 9640
    },
    {
      "epoch": 0.5146666666666667,
      "grad_norm": 0.29183876514434814,
      "learning_rate": 4.6783333333333337e-05,
      "loss": 0.0033,
      "step": 9650
    },
    {
      "epoch": 0.5152,
      "grad_norm": 0.058715034276247025,
      "learning_rate": 4.678e-05,
      "loss": 0.0043,
      "step": 9660
    },
    {
      "epoch": 0.5157333333333334,
      "grad_norm": 0.14625534415245056,
      "learning_rate": 4.677666666666667e-05,
      "loss": 0.0045,
      "step": 9670
    },
    {
      "epoch": 0.5162666666666667,
      "grad_norm": 0.14608871936798096,
      "learning_rate": 4.6773333333333335e-05,
      "loss": 0.0032,
      "step": 9680
    },
    {
      "epoch": 0.5168,
      "grad_norm": 0.4664872884750366,
      "learning_rate": 4.677e-05,
      "loss": 0.003,
      "step": 9690
    },
    {
      "epoch": 0.5173333333333333,
      "grad_norm": 0.17518644034862518,
      "learning_rate": 4.676666666666667e-05,
      "loss": 0.004,
      "step": 9700
    },
    {
      "epoch": 0.5178666666666667,
      "grad_norm": 0.08753518015146255,
      "learning_rate": 4.6763333333333333e-05,
      "loss": 0.0047,
      "step": 9710
    },
    {
      "epoch": 0.5184,
      "grad_norm": 0.029269130900502205,
      "learning_rate": 4.6760000000000006e-05,
      "loss": 0.0039,
      "step": 9720
    },
    {
      "epoch": 0.5189333333333334,
      "grad_norm": 0.35021811723709106,
      "learning_rate": 4.675666666666667e-05,
      "loss": 0.0042,
      "step": 9730
    },
    {
      "epoch": 0.5194666666666666,
      "grad_norm": 0.058315109461545944,
      "learning_rate": 4.675333333333334e-05,
      "loss": 0.0034,
      "step": 9740
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.05865855515003204,
      "learning_rate": 4.6750000000000005e-05,
      "loss": 0.0032,
      "step": 9750
    },
    {
      "epoch": 0.5205333333333333,
      "grad_norm": 0.29183387756347656,
      "learning_rate": 4.6746666666666664e-05,
      "loss": 0.0051,
      "step": 9760
    },
    {
      "epoch": 0.5210666666666667,
      "grad_norm": 0.17502327263355255,
      "learning_rate": 4.674333333333333e-05,
      "loss": 0.004,
      "step": 9770
    },
    {
      "epoch": 0.5216,
      "grad_norm": 0.17514176666736603,
      "learning_rate": 4.674e-05,
      "loss": 0.0044,
      "step": 9780
    },
    {
      "epoch": 0.5221333333333333,
      "grad_norm": 0.1755417138338089,
      "learning_rate": 4.673666666666667e-05,
      "loss": 0.0039,
      "step": 9790
    },
    {
      "epoch": 0.5226666666666666,
      "grad_norm": 0.008884260430932045,
      "learning_rate": 4.6733333333333335e-05,
      "loss": 0.0034,
      "step": 9800
    },
    {
      "epoch": 0.5232,
      "grad_norm": 0.11707239598035812,
      "learning_rate": 4.673e-05,
      "loss": 0.0052,
      "step": 9810
    },
    {
      "epoch": 0.5237333333333334,
      "grad_norm": 0.14651593565940857,
      "learning_rate": 4.672666666666667e-05,
      "loss": 0.0033,
      "step": 9820
    },
    {
      "epoch": 0.5242666666666667,
      "grad_norm": 0.11699529737234116,
      "learning_rate": 4.6723333333333334e-05,
      "loss": 0.0032,
      "step": 9830
    },
    {
      "epoch": 0.5248,
      "grad_norm": 0.5552049279212952,
      "learning_rate": 4.672e-05,
      "loss": 0.0032,
      "step": 9840
    },
    {
      "epoch": 0.5253333333333333,
      "grad_norm": 0.3799898326396942,
      "learning_rate": 4.671666666666667e-05,
      "loss": 0.0042,
      "step": 9850
    },
    {
      "epoch": 0.5258666666666667,
      "grad_norm": 0.5835450291633606,
      "learning_rate": 4.671333333333334e-05,
      "loss": 0.0029,
      "step": 9860
    },
    {
      "epoch": 0.5264,
      "grad_norm": 0.17453141510486603,
      "learning_rate": 4.6710000000000005e-05,
      "loss": 0.0042,
      "step": 9870
    },
    {
      "epoch": 0.5269333333333334,
      "grad_norm": 0.11702627688646317,
      "learning_rate": 4.670666666666667e-05,
      "loss": 0.0032,
      "step": 9880
    },
    {
      "epoch": 0.5274666666666666,
      "grad_norm": 0.029615262523293495,
      "learning_rate": 4.670333333333334e-05,
      "loss": 0.0031,
      "step": 9890
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.059683702886104584,
      "learning_rate": 4.6700000000000003e-05,
      "loss": 0.003,
      "step": 9900
    },
    {
      "epoch": 0.5285333333333333,
      "grad_norm": 0.29166361689567566,
      "learning_rate": 4.669666666666667e-05,
      "loss": 0.0045,
      "step": 9910
    },
    {
      "epoch": 0.5290666666666667,
      "grad_norm": 0.38054153323173523,
      "learning_rate": 4.6693333333333336e-05,
      "loss": 0.0035,
      "step": 9920
    },
    {
      "epoch": 0.5296,
      "grad_norm": 0.47650158405303955,
      "learning_rate": 4.669e-05,
      "loss": 0.0042,
      "step": 9930
    },
    {
      "epoch": 0.5301333333333333,
      "grad_norm": 0.009187608025968075,
      "learning_rate": 4.668666666666667e-05,
      "loss": 0.0033,
      "step": 9940
    },
    {
      "epoch": 0.5306666666666666,
      "grad_norm": 0.37848255038261414,
      "learning_rate": 4.6683333333333334e-05,
      "loss": 0.0038,
      "step": 9950
    },
    {
      "epoch": 0.5312,
      "grad_norm": 0.08756420016288757,
      "learning_rate": 4.668e-05,
      "loss": 0.0036,
      "step": 9960
    },
    {
      "epoch": 0.5317333333333333,
      "grad_norm": 0.11666766554117203,
      "learning_rate": 4.6676666666666666e-05,
      "loss": 0.0019,
      "step": 9970
    },
    {
      "epoch": 0.5322666666666667,
      "grad_norm": 0.26215147972106934,
      "learning_rate": 4.667333333333333e-05,
      "loss": 0.004,
      "step": 9980
    },
    {
      "epoch": 0.5328,
      "grad_norm": 0.17479324340820312,
      "learning_rate": 4.6670000000000005e-05,
      "loss": 0.0039,
      "step": 9990
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.1165267825126648,
      "learning_rate": 4.666666666666667e-05,
      "loss": 0.0034,
      "step": 10000
    },
    {
      "epoch": 0.5338666666666667,
      "grad_norm": 0.1746853142976761,
      "learning_rate": 4.666333333333334e-05,
      "loss": 0.0054,
      "step": 10010
    },
    {
      "epoch": 0.5344,
      "grad_norm": 0.5534343123435974,
      "learning_rate": 4.6660000000000004e-05,
      "loss": 0.0036,
      "step": 10020
    },
    {
      "epoch": 0.5349333333333334,
      "grad_norm": 0.17445777356624603,
      "learning_rate": 4.665666666666667e-05,
      "loss": 0.0049,
      "step": 10030
    },
    {
      "epoch": 0.5354666666666666,
      "grad_norm": 0.9942752718925476,
      "learning_rate": 4.6653333333333336e-05,
      "loss": 0.0035,
      "step": 10040
    },
    {
      "epoch": 0.536,
      "grad_norm": 0.030575837939977646,
      "learning_rate": 4.665e-05,
      "loss": 0.0024,
      "step": 10050
    },
    {
      "epoch": 0.5365333333333333,
      "grad_norm": 0.05829429626464844,
      "learning_rate": 4.664666666666667e-05,
      "loss": 0.0059,
      "step": 10060
    },
    {
      "epoch": 0.5370666666666667,
      "grad_norm": 0.34979966282844543,
      "learning_rate": 4.6643333333333335e-05,
      "loss": 0.0034,
      "step": 10070
    },
    {
      "epoch": 0.5376,
      "grad_norm": 0.058399539440870285,
      "learning_rate": 4.664e-05,
      "loss": 0.0052,
      "step": 10080
    },
    {
      "epoch": 0.5381333333333334,
      "grad_norm": 0.20401808619499207,
      "learning_rate": 4.663666666666667e-05,
      "loss": 0.0037,
      "step": 10090
    },
    {
      "epoch": 0.5386666666666666,
      "grad_norm": 0.2622210383415222,
      "learning_rate": 4.663333333333333e-05,
      "loss": 0.004,
      "step": 10100
    },
    {
      "epoch": 0.5392,
      "grad_norm": 0.2620665729045868,
      "learning_rate": 4.663e-05,
      "loss": 0.0046,
      "step": 10110
    },
    {
      "epoch": 0.5397333333333333,
      "grad_norm": 0.058152809739112854,
      "learning_rate": 4.6626666666666665e-05,
      "loss": 0.0043,
      "step": 10120
    },
    {
      "epoch": 0.5402666666666667,
      "grad_norm": 0.4375564754009247,
      "learning_rate": 4.662333333333334e-05,
      "loss": 0.0027,
      "step": 10130
    },
    {
      "epoch": 0.5408,
      "grad_norm": 0.40837788581848145,
      "learning_rate": 4.6620000000000004e-05,
      "loss": 0.0037,
      "step": 10140
    },
    {
      "epoch": 0.5413333333333333,
      "grad_norm": 0.006900295149534941,
      "learning_rate": 4.661666666666667e-05,
      "loss": 0.0075,
      "step": 10150
    },
    {
      "epoch": 0.5418666666666667,
      "grad_norm": 0.14703911542892456,
      "learning_rate": 4.6613333333333337e-05,
      "loss": 0.0046,
      "step": 10160
    },
    {
      "epoch": 0.5424,
      "grad_norm": 0.32309794425964355,
      "learning_rate": 4.661e-05,
      "loss": 0.0035,
      "step": 10170
    },
    {
      "epoch": 0.5429333333333334,
      "grad_norm": 0.08858376741409302,
      "learning_rate": 4.660666666666667e-05,
      "loss": 0.0039,
      "step": 10180
    },
    {
      "epoch": 0.5434666666666667,
      "grad_norm": 0.20489509403705597,
      "learning_rate": 4.6603333333333335e-05,
      "loss": 0.0041,
      "step": 10190
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.058562736958265305,
      "learning_rate": 4.660000000000001e-05,
      "loss": 0.0042,
      "step": 10200
    },
    {
      "epoch": 0.5445333333333333,
      "grad_norm": 1.4940465688705444,
      "learning_rate": 4.659666666666667e-05,
      "loss": 0.0025,
      "step": 10210
    },
    {
      "epoch": 0.5450666666666667,
      "grad_norm": 0.20527790486812592,
      "learning_rate": 4.659333333333333e-05,
      "loss": 0.005,
      "step": 10220
    },
    {
      "epoch": 0.5456,
      "grad_norm": 0.004211551044136286,
      "learning_rate": 4.659e-05,
      "loss": 0.0045,
      "step": 10230
    },
    {
      "epoch": 0.5461333333333334,
      "grad_norm": 0.0035872855223715305,
      "learning_rate": 4.6586666666666666e-05,
      "loss": 0.004,
      "step": 10240
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.05848418548703194,
      "learning_rate": 4.658333333333333e-05,
      "loss": 0.0044,
      "step": 10250
    },
    {
      "epoch": 0.5472,
      "grad_norm": 0.08786272257566452,
      "learning_rate": 4.6580000000000005e-05,
      "loss": 0.0026,
      "step": 10260
    },
    {
      "epoch": 0.5477333333333333,
      "grad_norm": 0.05892433226108551,
      "learning_rate": 4.657666666666667e-05,
      "loss": 0.0041,
      "step": 10270
    },
    {
      "epoch": 0.5482666666666667,
      "grad_norm": 0.14622174203395844,
      "learning_rate": 4.657333333333334e-05,
      "loss": 0.0037,
      "step": 10280
    },
    {
      "epoch": 0.5488,
      "grad_norm": 0.08783126622438431,
      "learning_rate": 4.657e-05,
      "loss": 0.0033,
      "step": 10290
    },
    {
      "epoch": 0.5493333333333333,
      "grad_norm": 0.2926514148712158,
      "learning_rate": 4.656666666666667e-05,
      "loss": 0.0042,
      "step": 10300
    },
    {
      "epoch": 0.5498666666666666,
      "grad_norm": 0.2045173943042755,
      "learning_rate": 4.6563333333333335e-05,
      "loss": 0.0031,
      "step": 10310
    },
    {
      "epoch": 0.5504,
      "grad_norm": 0.23345476388931274,
      "learning_rate": 4.656e-05,
      "loss": 0.0038,
      "step": 10320
    },
    {
      "epoch": 0.5509333333333334,
      "grad_norm": 0.17524424195289612,
      "learning_rate": 4.655666666666667e-05,
      "loss": 0.0033,
      "step": 10330
    },
    {
      "epoch": 0.5514666666666667,
      "grad_norm": 0.37926340103149414,
      "learning_rate": 4.655333333333334e-05,
      "loss": 0.0043,
      "step": 10340
    },
    {
      "epoch": 0.552,
      "grad_norm": 0.2041447013616562,
      "learning_rate": 4.655000000000001e-05,
      "loss": 0.0026,
      "step": 10350
    },
    {
      "epoch": 0.5525333333333333,
      "grad_norm": 0.002976099494844675,
      "learning_rate": 4.6546666666666666e-05,
      "loss": 0.0032,
      "step": 10360
    },
    {
      "epoch": 0.5530666666666667,
      "grad_norm": 0.3206765353679657,
      "learning_rate": 4.654333333333333e-05,
      "loss": 0.0039,
      "step": 10370
    },
    {
      "epoch": 0.5536,
      "grad_norm": 0.233097642660141,
      "learning_rate": 4.654e-05,
      "loss": 0.0048,
      "step": 10380
    },
    {
      "epoch": 0.5541333333333334,
      "grad_norm": 0.29120934009552,
      "learning_rate": 4.6536666666666664e-05,
      "loss": 0.0048,
      "step": 10390
    },
    {
      "epoch": 0.5546666666666666,
      "grad_norm": 0.08729079365730286,
      "learning_rate": 4.653333333333334e-05,
      "loss": 0.0034,
      "step": 10400
    },
    {
      "epoch": 0.5552,
      "grad_norm": 0.14555899798870087,
      "learning_rate": 4.6530000000000003e-05,
      "loss": 0.0027,
      "step": 10410
    },
    {
      "epoch": 0.5557333333333333,
      "grad_norm": 0.291494220495224,
      "learning_rate": 4.652666666666667e-05,
      "loss": 0.0037,
      "step": 10420
    },
    {
      "epoch": 0.5562666666666667,
      "grad_norm": 1.936457872390747,
      "learning_rate": 4.6523333333333336e-05,
      "loss": 0.0033,
      "step": 10430
    },
    {
      "epoch": 0.5568,
      "grad_norm": 0.23379912972450256,
      "learning_rate": 4.652e-05,
      "loss": 0.0027,
      "step": 10440
    },
    {
      "epoch": 0.5573333333333333,
      "grad_norm": 1.5848841667175293,
      "learning_rate": 4.651666666666667e-05,
      "loss": 0.0048,
      "step": 10450
    },
    {
      "epoch": 0.5578666666666666,
      "grad_norm": 0.5545252561569214,
      "learning_rate": 4.6513333333333334e-05,
      "loss": 0.0043,
      "step": 10460
    },
    {
      "epoch": 0.5584,
      "grad_norm": 0.1753130406141281,
      "learning_rate": 4.651e-05,
      "loss": 0.0048,
      "step": 10470
    },
    {
      "epoch": 0.5589333333333333,
      "grad_norm": 0.016114892438054085,
      "learning_rate": 4.650666666666667e-05,
      "loss": 0.0028,
      "step": 10480
    },
    {
      "epoch": 0.5594666666666667,
      "grad_norm": 0.2918420732021332,
      "learning_rate": 4.650333333333334e-05,
      "loss": 0.004,
      "step": 10490
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.003814652096480131,
      "learning_rate": 4.6500000000000005e-05,
      "loss": 0.0041,
      "step": 10500
    },
    {
      "epoch": 0.5605333333333333,
      "grad_norm": 0.11682586371898651,
      "learning_rate": 4.6496666666666665e-05,
      "loss": 0.0034,
      "step": 10510
    },
    {
      "epoch": 0.5610666666666667,
      "grad_norm": 0.29212072491645813,
      "learning_rate": 4.649333333333333e-05,
      "loss": 0.0024,
      "step": 10520
    },
    {
      "epoch": 0.5616,
      "grad_norm": 0.11638888716697693,
      "learning_rate": 4.649e-05,
      "loss": 0.0042,
      "step": 10530
    },
    {
      "epoch": 0.5621333333333334,
      "grad_norm": 0.3785918354988098,
      "learning_rate": 4.648666666666667e-05,
      "loss": 0.003,
      "step": 10540
    },
    {
      "epoch": 0.5626666666666666,
      "grad_norm": 0.11661666631698608,
      "learning_rate": 4.6483333333333336e-05,
      "loss": 0.0036,
      "step": 10550
    },
    {
      "epoch": 0.5632,
      "grad_norm": 0.7279708385467529,
      "learning_rate": 4.648e-05,
      "loss": 0.0036,
      "step": 10560
    },
    {
      "epoch": 0.5637333333333333,
      "grad_norm": 0.1455630511045456,
      "learning_rate": 4.647666666666667e-05,
      "loss": 0.0024,
      "step": 10570
    },
    {
      "epoch": 0.5642666666666667,
      "grad_norm": 0.002730999607592821,
      "learning_rate": 4.6473333333333334e-05,
      "loss": 0.0052,
      "step": 10580
    },
    {
      "epoch": 0.5648,
      "grad_norm": 0.11704675108194351,
      "learning_rate": 4.647e-05,
      "loss": 0.0024,
      "step": 10590
    },
    {
      "epoch": 0.5653333333333334,
      "grad_norm": 0.14571687579154968,
      "learning_rate": 4.646666666666667e-05,
      "loss": 0.0043,
      "step": 10600
    },
    {
      "epoch": 0.5658666666666666,
      "grad_norm": 0.05833595618605614,
      "learning_rate": 4.646333333333334e-05,
      "loss": 0.0041,
      "step": 10610
    },
    {
      "epoch": 0.5664,
      "grad_norm": 0.11652547866106033,
      "learning_rate": 4.6460000000000006e-05,
      "loss": 0.0034,
      "step": 10620
    },
    {
      "epoch": 0.5669333333333333,
      "grad_norm": 0.46649232506752014,
      "learning_rate": 4.645666666666667e-05,
      "loss": 0.0041,
      "step": 10630
    },
    {
      "epoch": 0.5674666666666667,
      "grad_norm": 0.34984132647514343,
      "learning_rate": 4.645333333333334e-05,
      "loss": 0.0043,
      "step": 10640
    },
    {
      "epoch": 0.568,
      "grad_norm": 0.029644474387168884,
      "learning_rate": 4.6450000000000004e-05,
      "loss": 0.0047,
      "step": 10650
    },
    {
      "epoch": 0.5685333333333333,
      "grad_norm": 0.26287391781806946,
      "learning_rate": 4.644666666666667e-05,
      "loss": 0.0025,
      "step": 10660
    },
    {
      "epoch": 0.5690666666666667,
      "grad_norm": 0.02980610355734825,
      "learning_rate": 4.6443333333333336e-05,
      "loss": 0.0048,
      "step": 10670
    },
    {
      "epoch": 0.5696,
      "grad_norm": 0.1458306461572647,
      "learning_rate": 4.644e-05,
      "loss": 0.0039,
      "step": 10680
    },
    {
      "epoch": 0.5701333333333334,
      "grad_norm": 0.17485952377319336,
      "learning_rate": 4.643666666666667e-05,
      "loss": 0.0024,
      "step": 10690
    },
    {
      "epoch": 0.5706666666666667,
      "grad_norm": 0.05822702497243881,
      "learning_rate": 4.6433333333333335e-05,
      "loss": 0.004,
      "step": 10700
    },
    {
      "epoch": 0.5712,
      "grad_norm": 0.40804144740104675,
      "learning_rate": 4.643e-05,
      "loss": 0.0029,
      "step": 10710
    },
    {
      "epoch": 0.5717333333333333,
      "grad_norm": 0.11648191511631012,
      "learning_rate": 4.642666666666667e-05,
      "loss": 0.0039,
      "step": 10720
    },
    {
      "epoch": 0.5722666666666667,
      "grad_norm": 0.6118109822273254,
      "learning_rate": 4.642333333333333e-05,
      "loss": 0.0039,
      "step": 10730
    },
    {
      "epoch": 0.5728,
      "grad_norm": 0.46578970551490784,
      "learning_rate": 4.642e-05,
      "loss": 0.0027,
      "step": 10740
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.37826281785964966,
      "learning_rate": 4.641666666666667e-05,
      "loss": 0.0038,
      "step": 10750
    },
    {
      "epoch": 0.5738666666666666,
      "grad_norm": 0.6400175094604492,
      "learning_rate": 4.641333333333334e-05,
      "loss": 0.0035,
      "step": 10760
    },
    {
      "epoch": 0.5744,
      "grad_norm": 0.0028867207001894712,
      "learning_rate": 4.6410000000000005e-05,
      "loss": 0.0033,
      "step": 10770
    },
    {
      "epoch": 0.5749333333333333,
      "grad_norm": 0.2325505018234253,
      "learning_rate": 4.640666666666667e-05,
      "loss": 0.0033,
      "step": 10780
    },
    {
      "epoch": 0.5754666666666667,
      "grad_norm": 0.08749379962682724,
      "learning_rate": 4.640333333333334e-05,
      "loss": 0.0037,
      "step": 10790
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.029116064310073853,
      "learning_rate": 4.64e-05,
      "loss": 0.0052,
      "step": 10800
    },
    {
      "epoch": 0.5765333333333333,
      "grad_norm": 0.26160234212875366,
      "learning_rate": 4.639666666666667e-05,
      "loss": 0.0035,
      "step": 10810
    },
    {
      "epoch": 0.5770666666666666,
      "grad_norm": 0.20352351665496826,
      "learning_rate": 4.6393333333333335e-05,
      "loss": 0.0036,
      "step": 10820
    },
    {
      "epoch": 0.5776,
      "grad_norm": 0.11652752757072449,
      "learning_rate": 4.639e-05,
      "loss": 0.0027,
      "step": 10830
    },
    {
      "epoch": 0.5781333333333334,
      "grad_norm": 0.5525431632995605,
      "learning_rate": 4.638666666666667e-05,
      "loss": 0.0027,
      "step": 10840
    },
    {
      "epoch": 0.5786666666666667,
      "grad_norm": 0.058690570294857025,
      "learning_rate": 4.6383333333333334e-05,
      "loss": 0.0045,
      "step": 10850
    },
    {
      "epoch": 0.5792,
      "grad_norm": 0.004959050565958023,
      "learning_rate": 4.638e-05,
      "loss": 0.0045,
      "step": 10860
    },
    {
      "epoch": 0.5797333333333333,
      "grad_norm": 0.43725791573524475,
      "learning_rate": 4.6376666666666666e-05,
      "loss": 0.003,
      "step": 10870
    },
    {
      "epoch": 0.5802666666666667,
      "grad_norm": 0.49559104442596436,
      "learning_rate": 4.637333333333333e-05,
      "loss": 0.0045,
      "step": 10880
    },
    {
      "epoch": 0.5808,
      "grad_norm": 0.2914589047431946,
      "learning_rate": 4.6370000000000005e-05,
      "loss": 0.0029,
      "step": 10890
    },
    {
      "epoch": 0.5813333333333334,
      "grad_norm": 0.4078613221645355,
      "learning_rate": 4.636666666666667e-05,
      "loss": 0.0025,
      "step": 10900
    },
    {
      "epoch": 0.5818666666666666,
      "grad_norm": 0.11665371805429459,
      "learning_rate": 4.636333333333334e-05,
      "loss": 0.003,
      "step": 10910
    },
    {
      "epoch": 0.5824,
      "grad_norm": 0.08750614523887634,
      "learning_rate": 4.636e-05,
      "loss": 0.0034,
      "step": 10920
    },
    {
      "epoch": 0.5829333333333333,
      "grad_norm": 0.1458003669977188,
      "learning_rate": 4.635666666666667e-05,
      "loss": 0.0038,
      "step": 10930
    },
    {
      "epoch": 0.5834666666666667,
      "grad_norm": 0.23303790390491486,
      "learning_rate": 4.6353333333333336e-05,
      "loss": 0.0028,
      "step": 10940
    },
    {
      "epoch": 0.584,
      "grad_norm": 0.5241982936859131,
      "learning_rate": 4.635e-05,
      "loss": 0.0045,
      "step": 10950
    },
    {
      "epoch": 0.5845333333333333,
      "grad_norm": 0.3203568756580353,
      "learning_rate": 4.6346666666666675e-05,
      "loss": 0.0034,
      "step": 10960
    },
    {
      "epoch": 0.5850666666666666,
      "grad_norm": 0.14568141102790833,
      "learning_rate": 4.6343333333333334e-05,
      "loss": 0.0036,
      "step": 10970
    },
    {
      "epoch": 0.5856,
      "grad_norm": 0.20376989245414734,
      "learning_rate": 4.634e-05,
      "loss": 0.0027,
      "step": 10980
    },
    {
      "epoch": 0.5861333333333333,
      "grad_norm": 0.764783501625061,
      "learning_rate": 4.6336666666666666e-05,
      "loss": 0.0033,
      "step": 10990
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.17451968789100647,
      "learning_rate": 4.633333333333333e-05,
      "loss": 0.0029,
      "step": 11000
    },
    {
      "epoch": 0.5872,
      "grad_norm": 0.20398709177970886,
      "learning_rate": 4.633e-05,
      "loss": 0.0035,
      "step": 11010
    },
    {
      "epoch": 0.5877333333333333,
      "grad_norm": 0.17574141919612885,
      "learning_rate": 4.632666666666667e-05,
      "loss": 0.0037,
      "step": 11020
    },
    {
      "epoch": 0.5882666666666667,
      "grad_norm": 0.11629652976989746,
      "learning_rate": 4.632333333333334e-05,
      "loss": 0.003,
      "step": 11030
    },
    {
      "epoch": 0.5888,
      "grad_norm": 0.37819698452949524,
      "learning_rate": 4.6320000000000004e-05,
      "loss": 0.0033,
      "step": 11040
    },
    {
      "epoch": 0.5893333333333334,
      "grad_norm": 0.029096202924847603,
      "learning_rate": 4.631666666666667e-05,
      "loss": 0.0032,
      "step": 11050
    },
    {
      "epoch": 0.5898666666666667,
      "grad_norm": 0.34919941425323486,
      "learning_rate": 4.6313333333333336e-05,
      "loss": 0.0026,
      "step": 11060
    },
    {
      "epoch": 0.5904,
      "grad_norm": 0.08791392296552658,
      "learning_rate": 4.631e-05,
      "loss": 0.003,
      "step": 11070
    },
    {
      "epoch": 0.5909333333333333,
      "grad_norm": 0.23318788409233093,
      "learning_rate": 4.630666666666667e-05,
      "loss": 0.0028,
      "step": 11080
    },
    {
      "epoch": 0.5914666666666667,
      "grad_norm": 0.2324812412261963,
      "learning_rate": 4.6303333333333334e-05,
      "loss": 0.0041,
      "step": 11090
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.02923710085451603,
      "learning_rate": 4.630000000000001e-05,
      "loss": 0.0036,
      "step": 11100
    },
    {
      "epoch": 0.5925333333333334,
      "grad_norm": 0.08738914877176285,
      "learning_rate": 4.6296666666666673e-05,
      "loss": 0.0028,
      "step": 11110
    },
    {
      "epoch": 0.5930666666666666,
      "grad_norm": 0.37091097235679626,
      "learning_rate": 4.629333333333333e-05,
      "loss": 0.0034,
      "step": 11120
    },
    {
      "epoch": 0.5936,
      "grad_norm": 0.14536039531230927,
      "learning_rate": 4.629e-05,
      "loss": 0.0044,
      "step": 11130
    },
    {
      "epoch": 0.5941333333333333,
      "grad_norm": 0.6371329426765442,
      "learning_rate": 4.6286666666666665e-05,
      "loss": 0.0042,
      "step": 11140
    },
    {
      "epoch": 0.5946666666666667,
      "grad_norm": 0.11644600331783295,
      "learning_rate": 4.628333333333333e-05,
      "loss": 0.0035,
      "step": 11150
    },
    {
      "epoch": 0.5952,
      "grad_norm": 0.1745246946811676,
      "learning_rate": 4.6280000000000004e-05,
      "loss": 0.0034,
      "step": 11160
    },
    {
      "epoch": 0.5957333333333333,
      "grad_norm": 0.1745656579732895,
      "learning_rate": 4.627666666666667e-05,
      "loss": 0.0028,
      "step": 11170
    },
    {
      "epoch": 0.5962666666666666,
      "grad_norm": 0.23280814290046692,
      "learning_rate": 4.6273333333333336e-05,
      "loss": 0.0032,
      "step": 11180
    },
    {
      "epoch": 0.5968,
      "grad_norm": 0.34915491938591003,
      "learning_rate": 4.627e-05,
      "loss": 0.0044,
      "step": 11190
    },
    {
      "epoch": 0.5973333333333334,
      "grad_norm": 0.46568483114242554,
      "learning_rate": 4.626666666666667e-05,
      "loss": 0.0032,
      "step": 11200
    },
    {
      "epoch": 0.5978666666666667,
      "grad_norm": 0.407362699508667,
      "learning_rate": 4.6263333333333335e-05,
      "loss": 0.0024,
      "step": 11210
    },
    {
      "epoch": 0.5984,
      "grad_norm": 0.14555513858795166,
      "learning_rate": 4.626e-05,
      "loss": 0.003,
      "step": 11220
    },
    {
      "epoch": 0.5989333333333333,
      "grad_norm": 0.11636706441640854,
      "learning_rate": 4.625666666666667e-05,
      "loss": 0.0039,
      "step": 11230
    },
    {
      "epoch": 0.5994666666666667,
      "grad_norm": 0.2037656158208847,
      "learning_rate": 4.625333333333334e-05,
      "loss": 0.0041,
      "step": 11240
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.0038425587117671967,
      "learning_rate": 4.6250000000000006e-05,
      "loss": 0.0042,
      "step": 11250
    },
    {
      "epoch": 0.6005333333333334,
      "grad_norm": 0.058193955570459366,
      "learning_rate": 4.624666666666667e-05,
      "loss": 0.0037,
      "step": 11260
    },
    {
      "epoch": 0.6010666666666666,
      "grad_norm": 0.058256231248378754,
      "learning_rate": 4.624333333333333e-05,
      "loss": 0.0024,
      "step": 11270
    },
    {
      "epoch": 0.6016,
      "grad_norm": 0.08719175308942795,
      "learning_rate": 4.624e-05,
      "loss": 0.0044,
      "step": 11280
    },
    {
      "epoch": 0.6021333333333333,
      "grad_norm": 0.3488157093524933,
      "learning_rate": 4.6236666666666664e-05,
      "loss": 0.0054,
      "step": 11290
    },
    {
      "epoch": 0.6026666666666667,
      "grad_norm": 0.11636130511760712,
      "learning_rate": 4.623333333333334e-05,
      "loss": 0.0036,
      "step": 11300
    },
    {
      "epoch": 0.6032,
      "grad_norm": 0.11635902523994446,
      "learning_rate": 4.623e-05,
      "loss": 0.0046,
      "step": 11310
    },
    {
      "epoch": 0.6037333333333333,
      "grad_norm": 0.319513201713562,
      "learning_rate": 4.622666666666667e-05,
      "loss": 0.0045,
      "step": 11320
    },
    {
      "epoch": 0.6042666666666666,
      "grad_norm": 0.2032560110092163,
      "learning_rate": 4.6223333333333335e-05,
      "loss": 0.0044,
      "step": 11330
    },
    {
      "epoch": 0.6048,
      "grad_norm": 0.43561744689941406,
      "learning_rate": 4.622e-05,
      "loss": 0.0028,
      "step": 11340
    },
    {
      "epoch": 0.6053333333333333,
      "grad_norm": 0.08750026673078537,
      "learning_rate": 4.621666666666667e-05,
      "loss": 0.0032,
      "step": 11350
    },
    {
      "epoch": 0.6058666666666667,
      "grad_norm": 0.059872817248106,
      "learning_rate": 4.6213333333333334e-05,
      "loss": 0.0044,
      "step": 11360
    },
    {
      "epoch": 0.6064,
      "grad_norm": 1.7813043594360352,
      "learning_rate": 4.6210000000000006e-05,
      "loss": 0.0026,
      "step": 11370
    },
    {
      "epoch": 0.6069333333333333,
      "grad_norm": 0.1741819977760315,
      "learning_rate": 4.620666666666667e-05,
      "loss": 0.0027,
      "step": 11380
    },
    {
      "epoch": 0.6074666666666667,
      "grad_norm": 0.2906913459300995,
      "learning_rate": 4.620333333333334e-05,
      "loss": 0.0032,
      "step": 11390
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.11604302376508713,
      "learning_rate": 4.6200000000000005e-05,
      "loss": 0.0019,
      "step": 11400
    },
    {
      "epoch": 0.6085333333333334,
      "grad_norm": 0.6392766237258911,
      "learning_rate": 4.619666666666667e-05,
      "loss": 0.0034,
      "step": 11410
    },
    {
      "epoch": 0.6090666666666666,
      "grad_norm": 1.7242635488510132,
      "learning_rate": 4.619333333333333e-05,
      "loss": 0.0033,
      "step": 11420
    },
    {
      "epoch": 0.6096,
      "grad_norm": 0.003830099944025278,
      "learning_rate": 4.619e-05,
      "loss": 0.0049,
      "step": 11430
    },
    {
      "epoch": 0.6101333333333333,
      "grad_norm": 0.7261692881584167,
      "learning_rate": 4.618666666666667e-05,
      "loss": 0.0044,
      "step": 11440
    },
    {
      "epoch": 0.6106666666666667,
      "grad_norm": 0.08727572858333588,
      "learning_rate": 4.6183333333333336e-05,
      "loss": 0.0045,
      "step": 11450
    },
    {
      "epoch": 0.6112,
      "grad_norm": 0.2032766491174698,
      "learning_rate": 4.618e-05,
      "loss": 0.0034,
      "step": 11460
    },
    {
      "epoch": 0.6117333333333334,
      "grad_norm": 0.0870220735669136,
      "learning_rate": 4.617666666666667e-05,
      "loss": 0.004,
      "step": 11470
    },
    {
      "epoch": 0.6122666666666666,
      "grad_norm": 0.02897649258375168,
      "learning_rate": 4.6173333333333334e-05,
      "loss": 0.0038,
      "step": 11480
    },
    {
      "epoch": 0.6128,
      "grad_norm": 0.3482620418071747,
      "learning_rate": 4.617e-05,
      "loss": 0.0026,
      "step": 11490
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.05814933404326439,
      "learning_rate": 4.6166666666666666e-05,
      "loss": 0.0043,
      "step": 11500
    },
    {
      "epoch": 0.6138666666666667,
      "grad_norm": 0.5804190635681152,
      "learning_rate": 4.616333333333334e-05,
      "loss": 0.0054,
      "step": 11510
    },
    {
      "epoch": 0.6144,
      "grad_norm": 0.5226796269416809,
      "learning_rate": 4.6160000000000005e-05,
      "loss": 0.0036,
      "step": 11520
    },
    {
      "epoch": 0.6149333333333333,
      "grad_norm": 0.05804615095257759,
      "learning_rate": 4.615666666666667e-05,
      "loss": 0.0023,
      "step": 11530
    },
    {
      "epoch": 0.6154666666666667,
      "grad_norm": 0.17409008741378784,
      "learning_rate": 4.615333333333334e-05,
      "loss": 0.0039,
      "step": 11540
    },
    {
      "epoch": 0.616,
      "grad_norm": 0.2901969254016876,
      "learning_rate": 4.6150000000000004e-05,
      "loss": 0.0043,
      "step": 11550
    },
    {
      "epoch": 0.6165333333333334,
      "grad_norm": 0.20316921174526215,
      "learning_rate": 4.614666666666667e-05,
      "loss": 0.0032,
      "step": 11560
    },
    {
      "epoch": 0.6170666666666667,
      "grad_norm": 0.004189321771264076,
      "learning_rate": 4.6143333333333336e-05,
      "loss": 0.0049,
      "step": 11570
    },
    {
      "epoch": 0.6176,
      "grad_norm": 0.1162506565451622,
      "learning_rate": 4.614e-05,
      "loss": 0.004,
      "step": 11580
    },
    {
      "epoch": 0.6181333333333333,
      "grad_norm": 0.05808348208665848,
      "learning_rate": 4.613666666666667e-05,
      "loss": 0.0035,
      "step": 11590
    },
    {
      "epoch": 0.6186666666666667,
      "grad_norm": 0.3196288049221039,
      "learning_rate": 4.6133333333333334e-05,
      "loss": 0.0031,
      "step": 11600
    },
    {
      "epoch": 0.6192,
      "grad_norm": 0.05817881226539612,
      "learning_rate": 4.613e-05,
      "loss": 0.004,
      "step": 11610
    },
    {
      "epoch": 0.6197333333333334,
      "grad_norm": 0.3485146164894104,
      "learning_rate": 4.612666666666667e-05,
      "loss": 0.0041,
      "step": 11620
    },
    {
      "epoch": 0.6202666666666666,
      "grad_norm": 0.49364182353019714,
      "learning_rate": 4.612333333333333e-05,
      "loss": 0.0044,
      "step": 11630
    },
    {
      "epoch": 0.6208,
      "grad_norm": 0.029348080977797508,
      "learning_rate": 4.612e-05,
      "loss": 0.0052,
      "step": 11640
    },
    {
      "epoch": 0.6213333333333333,
      "grad_norm": 0.08782262355089188,
      "learning_rate": 4.611666666666667e-05,
      "loss": 0.0029,
      "step": 11650
    },
    {
      "epoch": 0.6218666666666667,
      "grad_norm": 0.2907012701034546,
      "learning_rate": 4.611333333333334e-05,
      "loss": 0.0041,
      "step": 11660
    },
    {
      "epoch": 0.6224,
      "grad_norm": 0.3777623772621155,
      "learning_rate": 4.6110000000000004e-05,
      "loss": 0.0032,
      "step": 11670
    },
    {
      "epoch": 0.6229333333333333,
      "grad_norm": 0.1743350327014923,
      "learning_rate": 4.610666666666667e-05,
      "loss": 0.0042,
      "step": 11680
    },
    {
      "epoch": 0.6234666666666666,
      "grad_norm": 0.11627126485109329,
      "learning_rate": 4.6103333333333336e-05,
      "loss": 0.0038,
      "step": 11690
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.1745106726884842,
      "learning_rate": 4.61e-05,
      "loss": 0.004,
      "step": 11700
    },
    {
      "epoch": 0.6245333333333334,
      "grad_norm": 0.11645061522722244,
      "learning_rate": 4.609666666666667e-05,
      "loss": 0.0045,
      "step": 11710
    },
    {
      "epoch": 0.6250666666666667,
      "grad_norm": 0.3780018091201782,
      "learning_rate": 4.6093333333333335e-05,
      "loss": 0.0031,
      "step": 11720
    },
    {
      "epoch": 0.6256,
      "grad_norm": 0.08720589429140091,
      "learning_rate": 4.609e-05,
      "loss": 0.0027,
      "step": 11730
    },
    {
      "epoch": 0.6261333333333333,
      "grad_norm": 0.0581049770116806,
      "learning_rate": 4.608666666666667e-05,
      "loss": 0.0041,
      "step": 11740
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.5523080229759216,
      "learning_rate": 4.608333333333333e-05,
      "loss": 0.005,
      "step": 11750
    },
    {
      "epoch": 0.6272,
      "grad_norm": 0.34858840703964233,
      "learning_rate": 4.608e-05,
      "loss": 0.0056,
      "step": 11760
    },
    {
      "epoch": 0.6277333333333334,
      "grad_norm": 0.17418785393238068,
      "learning_rate": 4.6076666666666665e-05,
      "loss": 0.0032,
      "step": 11770
    },
    {
      "epoch": 0.6282666666666666,
      "grad_norm": 0.23258116841316223,
      "learning_rate": 4.607333333333334e-05,
      "loss": 0.0022,
      "step": 11780
    },
    {
      "epoch": 0.6288,
      "grad_norm": 0.029830478131771088,
      "learning_rate": 4.6070000000000004e-05,
      "loss": 0.0039,
      "step": 11790
    },
    {
      "epoch": 0.6293333333333333,
      "grad_norm": 0.3774872124195099,
      "learning_rate": 4.606666666666667e-05,
      "loss": 0.0022,
      "step": 11800
    },
    {
      "epoch": 0.6298666666666667,
      "grad_norm": 0.08766252547502518,
      "learning_rate": 4.606333333333334e-05,
      "loss": 0.0031,
      "step": 11810
    },
    {
      "epoch": 0.6304,
      "grad_norm": 0.23270031809806824,
      "learning_rate": 4.606e-05,
      "loss": 0.0038,
      "step": 11820
    },
    {
      "epoch": 0.6309333333333333,
      "grad_norm": 0.02916932851076126,
      "learning_rate": 4.605666666666667e-05,
      "loss": 0.0024,
      "step": 11830
    },
    {
      "epoch": 0.6314666666666666,
      "grad_norm": 0.08736290037631989,
      "learning_rate": 4.6053333333333335e-05,
      "loss": 0.0024,
      "step": 11840
    },
    {
      "epoch": 0.632,
      "grad_norm": 2.004321575164795,
      "learning_rate": 4.605e-05,
      "loss": 0.0031,
      "step": 11850
    },
    {
      "epoch": 0.6325333333333333,
      "grad_norm": 0.3783508539199829,
      "learning_rate": 4.6046666666666674e-05,
      "loss": 0.0034,
      "step": 11860
    },
    {
      "epoch": 0.6330666666666667,
      "grad_norm": 0.11621608585119247,
      "learning_rate": 4.6043333333333334e-05,
      "loss": 0.0038,
      "step": 11870
    },
    {
      "epoch": 0.6336,
      "grad_norm": 0.0870843231678009,
      "learning_rate": 4.604e-05,
      "loss": 0.0026,
      "step": 11880
    },
    {
      "epoch": 0.6341333333333333,
      "grad_norm": 0.002294090809300542,
      "learning_rate": 4.6036666666666666e-05,
      "loss": 0.0042,
      "step": 11890
    },
    {
      "epoch": 0.6346666666666667,
      "grad_norm": 0.6682834625244141,
      "learning_rate": 4.603333333333333e-05,
      "loss": 0.0044,
      "step": 11900
    },
    {
      "epoch": 0.6352,
      "grad_norm": 0.002244093921035528,
      "learning_rate": 4.603e-05,
      "loss": 0.0034,
      "step": 11910
    },
    {
      "epoch": 0.6357333333333334,
      "grad_norm": 0.05812172591686249,
      "learning_rate": 4.602666666666667e-05,
      "loss": 0.0036,
      "step": 11920
    },
    {
      "epoch": 0.6362666666666666,
      "grad_norm": 0.029243623837828636,
      "learning_rate": 4.602333333333334e-05,
      "loss": 0.0028,
      "step": 11930
    },
    {
      "epoch": 0.6368,
      "grad_norm": 0.0035558207891881466,
      "learning_rate": 4.602e-05,
      "loss": 0.0033,
      "step": 11940
    },
    {
      "epoch": 0.6373333333333333,
      "grad_norm": 2.026979923248291,
      "learning_rate": 4.601666666666667e-05,
      "loss": 0.0028,
      "step": 11950
    },
    {
      "epoch": 0.6378666666666667,
      "grad_norm": 0.17417185008525848,
      "learning_rate": 4.6013333333333336e-05,
      "loss": 0.0033,
      "step": 11960
    },
    {
      "epoch": 0.6384,
      "grad_norm": 0.2614518702030182,
      "learning_rate": 4.601e-05,
      "loss": 0.0031,
      "step": 11970
    },
    {
      "epoch": 0.6389333333333334,
      "grad_norm": 0.2031751424074173,
      "learning_rate": 4.600666666666667e-05,
      "loss": 0.0037,
      "step": 11980
    },
    {
      "epoch": 0.6394666666666666,
      "grad_norm": 0.3481992185115814,
      "learning_rate": 4.6003333333333334e-05,
      "loss": 0.0044,
      "step": 11990
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.290388822555542,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.0032,
      "step": 12000
    },
    {
      "epoch": 0.6405333333333333,
      "grad_norm": 0.20318126678466797,
      "learning_rate": 4.599666666666667e-05,
      "loss": 0.0055,
      "step": 12010
    },
    {
      "epoch": 0.6410666666666667,
      "grad_norm": 0.40617695450782776,
      "learning_rate": 4.599333333333334e-05,
      "loss": 0.0025,
      "step": 12020
    },
    {
      "epoch": 0.6416,
      "grad_norm": 0.029308047145605087,
      "learning_rate": 4.599e-05,
      "loss": 0.0035,
      "step": 12030
    },
    {
      "epoch": 0.6421333333333333,
      "grad_norm": 0.2316153198480606,
      "learning_rate": 4.5986666666666665e-05,
      "loss": 0.0036,
      "step": 12040
    },
    {
      "epoch": 0.6426666666666667,
      "grad_norm": 0.03138993680477142,
      "learning_rate": 4.598333333333333e-05,
      "loss": 0.0069,
      "step": 12050
    },
    {
      "epoch": 0.6432,
      "grad_norm": 0.34833985567092896,
      "learning_rate": 4.5980000000000004e-05,
      "loss": 0.0034,
      "step": 12060
    },
    {
      "epoch": 0.6437333333333334,
      "grad_norm": 0.23215791583061218,
      "learning_rate": 4.597666666666667e-05,
      "loss": 0.0039,
      "step": 12070
    },
    {
      "epoch": 0.6442666666666667,
      "grad_norm": 0.2612009048461914,
      "learning_rate": 4.5973333333333336e-05,
      "loss": 0.0033,
      "step": 12080
    },
    {
      "epoch": 0.6448,
      "grad_norm": 0.4643370509147644,
      "learning_rate": 4.597e-05,
      "loss": 0.0029,
      "step": 12090
    },
    {
      "epoch": 0.6453333333333333,
      "grad_norm": 0.29021552205085754,
      "learning_rate": 4.596666666666667e-05,
      "loss": 0.0043,
      "step": 12100
    },
    {
      "epoch": 0.6458666666666667,
      "grad_norm": 0.40640267729759216,
      "learning_rate": 4.5963333333333334e-05,
      "loss": 0.0035,
      "step": 12110
    },
    {
      "epoch": 0.6464,
      "grad_norm": 0.14522835612297058,
      "learning_rate": 4.596e-05,
      "loss": 0.003,
      "step": 12120
    },
    {
      "epoch": 0.6469333333333334,
      "grad_norm": 0.05810658633708954,
      "learning_rate": 4.595666666666667e-05,
      "loss": 0.0047,
      "step": 12130
    },
    {
      "epoch": 0.6474666666666666,
      "grad_norm": 0.0026023793034255505,
      "learning_rate": 4.595333333333334e-05,
      "loss": 0.0033,
      "step": 12140
    },
    {
      "epoch": 0.648,
      "grad_norm": 0.2917162775993347,
      "learning_rate": 4.5950000000000006e-05,
      "loss": 0.0043,
      "step": 12150
    },
    {
      "epoch": 0.6485333333333333,
      "grad_norm": 0.29053759574890137,
      "learning_rate": 4.594666666666667e-05,
      "loss": 0.0045,
      "step": 12160
    },
    {
      "epoch": 0.6490666666666667,
      "grad_norm": 0.1462363749742508,
      "learning_rate": 4.594333333333334e-05,
      "loss": 0.0025,
      "step": 12170
    },
    {
      "epoch": 0.6496,
      "grad_norm": 0.06039856746792793,
      "learning_rate": 4.594e-05,
      "loss": 0.0026,
      "step": 12180
    },
    {
      "epoch": 0.6501333333333333,
      "grad_norm": 0.3211832642555237,
      "learning_rate": 4.593666666666666e-05,
      "loss": 0.0047,
      "step": 12190
    },
    {
      "epoch": 0.6506666666666666,
      "grad_norm": 0.030189281329512596,
      "learning_rate": 4.5933333333333336e-05,
      "loss": 0.0027,
      "step": 12200
    },
    {
      "epoch": 0.6512,
      "grad_norm": 0.1460452377796173,
      "learning_rate": 4.593e-05,
      "loss": 0.0032,
      "step": 12210
    },
    {
      "epoch": 0.6517333333333334,
      "grad_norm": 0.17459240555763245,
      "learning_rate": 4.592666666666667e-05,
      "loss": 0.0029,
      "step": 12220
    },
    {
      "epoch": 0.6522666666666667,
      "grad_norm": 0.1455046683549881,
      "learning_rate": 4.5923333333333335e-05,
      "loss": 0.0029,
      "step": 12230
    },
    {
      "epoch": 0.6528,
      "grad_norm": 0.058146629482507706,
      "learning_rate": 4.592e-05,
      "loss": 0.0033,
      "step": 12240
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.030329540371894836,
      "learning_rate": 4.591666666666667e-05,
      "loss": 0.0036,
      "step": 12250
    },
    {
      "epoch": 0.6538666666666667,
      "grad_norm": 0.23398935794830322,
      "learning_rate": 4.591333333333333e-05,
      "loss": 0.0038,
      "step": 12260
    },
    {
      "epoch": 0.6544,
      "grad_norm": 0.17512662708759308,
      "learning_rate": 4.5910000000000006e-05,
      "loss": 0.0027,
      "step": 12270
    },
    {
      "epoch": 0.6549333333333334,
      "grad_norm": 0.06823020428419113,
      "learning_rate": 4.590666666666667e-05,
      "loss": 0.0029,
      "step": 12280
    },
    {
      "epoch": 0.6554666666666666,
      "grad_norm": 0.37688976526260376,
      "learning_rate": 4.590333333333334e-05,
      "loss": 0.0033,
      "step": 12290
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.40859904885292053,
      "learning_rate": 4.5900000000000004e-05,
      "loss": 0.0037,
      "step": 12300
    },
    {
      "epoch": 0.6565333333333333,
      "grad_norm": 0.058893803507089615,
      "learning_rate": 4.589666666666667e-05,
      "loss": 0.0029,
      "step": 12310
    },
    {
      "epoch": 0.6570666666666667,
      "grad_norm": 0.14558516442775726,
      "learning_rate": 4.589333333333334e-05,
      "loss": 0.0045,
      "step": 12320
    },
    {
      "epoch": 0.6576,
      "grad_norm": 0.030070094391703606,
      "learning_rate": 4.589e-05,
      "loss": 0.0028,
      "step": 12330
    },
    {
      "epoch": 0.6581333333333333,
      "grad_norm": 0.002133181318640709,
      "learning_rate": 4.588666666666667e-05,
      "loss": 0.0032,
      "step": 12340
    },
    {
      "epoch": 0.6586666666666666,
      "grad_norm": 0.6707847714424133,
      "learning_rate": 4.5883333333333335e-05,
      "loss": 0.0053,
      "step": 12350
    },
    {
      "epoch": 0.6592,
      "grad_norm": 0.14609354734420776,
      "learning_rate": 4.588e-05,
      "loss": 0.0047,
      "step": 12360
    },
    {
      "epoch": 0.6597333333333333,
      "grad_norm": 0.4081316888332367,
      "learning_rate": 4.587666666666667e-05,
      "loss": 0.0041,
      "step": 12370
    },
    {
      "epoch": 0.6602666666666667,
      "grad_norm": 0.20372886955738068,
      "learning_rate": 4.5873333333333333e-05,
      "loss": 0.003,
      "step": 12380
    },
    {
      "epoch": 0.6608,
      "grad_norm": 0.34907907247543335,
      "learning_rate": 4.587e-05,
      "loss": 0.0033,
      "step": 12390
    },
    {
      "epoch": 0.6613333333333333,
      "grad_norm": 0.17437948286533356,
      "learning_rate": 4.5866666666666666e-05,
      "loss": 0.0034,
      "step": 12400
    },
    {
      "epoch": 0.6618666666666667,
      "grad_norm": 0.002303079701960087,
      "learning_rate": 4.586333333333334e-05,
      "loss": 0.0036,
      "step": 12410
    },
    {
      "epoch": 0.6624,
      "grad_norm": 1.6291210651397705,
      "learning_rate": 4.5860000000000005e-05,
      "loss": 0.0041,
      "step": 12420
    },
    {
      "epoch": 0.6629333333333334,
      "grad_norm": 0.26207470893859863,
      "learning_rate": 4.585666666666667e-05,
      "loss": 0.0023,
      "step": 12430
    },
    {
      "epoch": 0.6634666666666666,
      "grad_norm": 0.029245173558592796,
      "learning_rate": 4.585333333333334e-05,
      "loss": 0.0039,
      "step": 12440
    },
    {
      "epoch": 0.664,
      "grad_norm": 0.20357434451580048,
      "learning_rate": 4.585e-05,
      "loss": 0.0062,
      "step": 12450
    },
    {
      "epoch": 0.6645333333333333,
      "grad_norm": 0.7558185458183289,
      "learning_rate": 4.584666666666667e-05,
      "loss": 0.0042,
      "step": 12460
    },
    {
      "epoch": 0.6650666666666667,
      "grad_norm": 0.34861940145492554,
      "learning_rate": 4.5843333333333335e-05,
      "loss": 0.0045,
      "step": 12470
    },
    {
      "epoch": 0.6656,
      "grad_norm": 0.3195210099220276,
      "learning_rate": 4.584e-05,
      "loss": 0.0049,
      "step": 12480
    },
    {
      "epoch": 0.6661333333333334,
      "grad_norm": 0.05800776928663254,
      "learning_rate": 4.583666666666667e-05,
      "loss": 0.0034,
      "step": 12490
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.0031707657035440207,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 0.0041,
      "step": 12500
    },
    {
      "epoch": 0.6672,
      "grad_norm": 0.17395685613155365,
      "learning_rate": 4.583e-05,
      "loss": 0.0036,
      "step": 12510
    },
    {
      "epoch": 0.6677333333333333,
      "grad_norm": 0.11613478511571884,
      "learning_rate": 4.5826666666666666e-05,
      "loss": 0.003,
      "step": 12520
    },
    {
      "epoch": 0.6682666666666667,
      "grad_norm": 0.006452604196965694,
      "learning_rate": 4.582333333333333e-05,
      "loss": 0.0034,
      "step": 12530
    },
    {
      "epoch": 0.6688,
      "grad_norm": 0.173868790268898,
      "learning_rate": 4.5820000000000005e-05,
      "loss": 0.0046,
      "step": 12540
    },
    {
      "epoch": 0.6693333333333333,
      "grad_norm": 0.05827014148235321,
      "learning_rate": 4.581666666666667e-05,
      "loss": 0.0039,
      "step": 12550
    },
    {
      "epoch": 0.6698666666666667,
      "grad_norm": 0.4061182141304016,
      "learning_rate": 4.581333333333334e-05,
      "loss": 0.0037,
      "step": 12560
    },
    {
      "epoch": 0.6704,
      "grad_norm": 0.2320929765701294,
      "learning_rate": 4.5810000000000004e-05,
      "loss": 0.0031,
      "step": 12570
    },
    {
      "epoch": 0.6709333333333334,
      "grad_norm": 0.1448592096567154,
      "learning_rate": 4.580666666666667e-05,
      "loss": 0.0043,
      "step": 12580
    },
    {
      "epoch": 0.6714666666666667,
      "grad_norm": 1.287933349609375,
      "learning_rate": 4.5803333333333336e-05,
      "loss": 0.0046,
      "step": 12590
    },
    {
      "epoch": 0.672,
      "grad_norm": 1.2055310010910034,
      "learning_rate": 4.58e-05,
      "loss": 0.0049,
      "step": 12600
    },
    {
      "epoch": 0.6725333333333333,
      "grad_norm": 0.463925838470459,
      "learning_rate": 4.579666666666667e-05,
      "loss": 0.0041,
      "step": 12610
    },
    {
      "epoch": 0.6730666666666667,
      "grad_norm": 0.05805397778749466,
      "learning_rate": 4.579333333333334e-05,
      "loss": 0.0029,
      "step": 12620
    },
    {
      "epoch": 0.6736,
      "grad_norm": 0.2609236240386963,
      "learning_rate": 4.579e-05,
      "loss": 0.0042,
      "step": 12630
    },
    {
      "epoch": 0.6741333333333334,
      "grad_norm": 0.521889865398407,
      "learning_rate": 4.5786666666666666e-05,
      "loss": 0.0026,
      "step": 12640
    },
    {
      "epoch": 0.6746666666666666,
      "grad_norm": 0.1450403928756714,
      "learning_rate": 4.578333333333333e-05,
      "loss": 0.0046,
      "step": 12650
    },
    {
      "epoch": 0.6752,
      "grad_norm": 0.11599855124950409,
      "learning_rate": 4.578e-05,
      "loss": 0.0039,
      "step": 12660
    },
    {
      "epoch": 0.6757333333333333,
      "grad_norm": 0.00841651763767004,
      "learning_rate": 4.5776666666666665e-05,
      "loss": 0.0036,
      "step": 12670
    },
    {
      "epoch": 0.6762666666666667,
      "grad_norm": 0.23207324743270874,
      "learning_rate": 4.577333333333334e-05,
      "loss": 0.0044,
      "step": 12680
    },
    {
      "epoch": 0.6768,
      "grad_norm": 0.17434224486351013,
      "learning_rate": 4.5770000000000004e-05,
      "loss": 0.0038,
      "step": 12690
    },
    {
      "epoch": 0.6773333333333333,
      "grad_norm": 0.029189521446824074,
      "learning_rate": 4.576666666666667e-05,
      "loss": 0.004,
      "step": 12700
    },
    {
      "epoch": 0.6778666666666666,
      "grad_norm": 0.23204782605171204,
      "learning_rate": 4.5763333333333336e-05,
      "loss": 0.0037,
      "step": 12710
    },
    {
      "epoch": 0.6784,
      "grad_norm": 0.34810107946395874,
      "learning_rate": 4.576e-05,
      "loss": 0.0033,
      "step": 12720
    },
    {
      "epoch": 0.6789333333333334,
      "grad_norm": 0.058473825454711914,
      "learning_rate": 4.575666666666667e-05,
      "loss": 0.0028,
      "step": 12730
    },
    {
      "epoch": 0.6794666666666667,
      "grad_norm": 0.31891509890556335,
      "learning_rate": 4.5753333333333335e-05,
      "loss": 0.003,
      "step": 12740
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.40581685304641724,
      "learning_rate": 4.575e-05,
      "loss": 0.0031,
      "step": 12750
    },
    {
      "epoch": 0.6805333333333333,
      "grad_norm": 0.058028444647789,
      "learning_rate": 4.5746666666666674e-05,
      "loss": 0.0032,
      "step": 12760
    },
    {
      "epoch": 0.6810666666666667,
      "grad_norm": 0.6085196137428284,
      "learning_rate": 4.574333333333334e-05,
      "loss": 0.0029,
      "step": 12770
    },
    {
      "epoch": 0.6816,
      "grad_norm": 0.28945374488830566,
      "learning_rate": 4.574e-05,
      "loss": 0.004,
      "step": 12780
    },
    {
      "epoch": 0.6821333333333334,
      "grad_norm": 0.05796098709106445,
      "learning_rate": 4.5736666666666665e-05,
      "loss": 0.0038,
      "step": 12790
    },
    {
      "epoch": 0.6826666666666666,
      "grad_norm": 0.029047884047031403,
      "learning_rate": 4.573333333333333e-05,
      "loss": 0.0031,
      "step": 12800
    },
    {
      "epoch": 0.6832,
      "grad_norm": 0.17385488748550415,
      "learning_rate": 4.573e-05,
      "loss": 0.0034,
      "step": 12810
    },
    {
      "epoch": 0.6837333333333333,
      "grad_norm": 0.20290091633796692,
      "learning_rate": 4.572666666666667e-05,
      "loss": 0.0035,
      "step": 12820
    },
    {
      "epoch": 0.6842666666666667,
      "grad_norm": 0.2317953258752823,
      "learning_rate": 4.5723333333333337e-05,
      "loss": 0.0042,
      "step": 12830
    },
    {
      "epoch": 0.6848,
      "grad_norm": 0.028976336121559143,
      "learning_rate": 4.572e-05,
      "loss": 0.0032,
      "step": 12840
    },
    {
      "epoch": 0.6853333333333333,
      "grad_norm": 0.2607177495956421,
      "learning_rate": 4.571666666666667e-05,
      "loss": 0.0046,
      "step": 12850
    },
    {
      "epoch": 0.6858666666666666,
      "grad_norm": 0.3186691403388977,
      "learning_rate": 4.5713333333333335e-05,
      "loss": 0.0044,
      "step": 12860
    },
    {
      "epoch": 0.6864,
      "grad_norm": 0.550219714641571,
      "learning_rate": 4.571e-05,
      "loss": 0.0028,
      "step": 12870
    },
    {
      "epoch": 0.6869333333333333,
      "grad_norm": 0.2609713673591614,
      "learning_rate": 4.570666666666667e-05,
      "loss": 0.0031,
      "step": 12880
    },
    {
      "epoch": 0.6874666666666667,
      "grad_norm": 0.2316160351037979,
      "learning_rate": 4.570333333333334e-05,
      "loss": 0.0049,
      "step": 12890
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.4343818724155426,
      "learning_rate": 4.5700000000000006e-05,
      "loss": 0.0025,
      "step": 12900
    },
    {
      "epoch": 0.6885333333333333,
      "grad_norm": 0.05796846002340317,
      "learning_rate": 4.569666666666667e-05,
      "loss": 0.0041,
      "step": 12910
    },
    {
      "epoch": 0.6890666666666667,
      "grad_norm": 0.1447288691997528,
      "learning_rate": 4.569333333333334e-05,
      "loss": 0.0026,
      "step": 12920
    },
    {
      "epoch": 0.6896,
      "grad_norm": 0.2604774236679077,
      "learning_rate": 4.569e-05,
      "loss": 0.0036,
      "step": 12930
    },
    {
      "epoch": 0.6901333333333334,
      "grad_norm": 0.144723579287529,
      "learning_rate": 4.5686666666666664e-05,
      "loss": 0.0028,
      "step": 12940
    },
    {
      "epoch": 0.6906666666666667,
      "grad_norm": 0.17351366579532623,
      "learning_rate": 4.568333333333333e-05,
      "loss": 0.0016,
      "step": 12950
    },
    {
      "epoch": 0.6912,
      "grad_norm": 0.34735095500946045,
      "learning_rate": 4.568e-05,
      "loss": 0.0042,
      "step": 12960
    },
    {
      "epoch": 0.6917333333333333,
      "grad_norm": 0.057925548404455185,
      "learning_rate": 4.567666666666667e-05,
      "loss": 0.0047,
      "step": 12970
    },
    {
      "epoch": 0.6922666666666667,
      "grad_norm": 0.0014603862073272467,
      "learning_rate": 4.5673333333333335e-05,
      "loss": 0.0032,
      "step": 12980
    },
    {
      "epoch": 0.6928,
      "grad_norm": 0.11575721949338913,
      "learning_rate": 4.567e-05,
      "loss": 0.0037,
      "step": 12990
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.34815993905067444,
      "learning_rate": 4.566666666666667e-05,
      "loss": 0.0048,
      "step": 13000
    },
    {
      "epoch": 0.6938666666666666,
      "grad_norm": 0.058087315410375595,
      "learning_rate": 4.5663333333333334e-05,
      "loss": 0.0039,
      "step": 13010
    },
    {
      "epoch": 0.6944,
      "grad_norm": 0.29024064540863037,
      "learning_rate": 4.566e-05,
      "loss": 0.0028,
      "step": 13020
    },
    {
      "epoch": 0.6949333333333333,
      "grad_norm": 0.14507095515727997,
      "learning_rate": 4.565666666666667e-05,
      "loss": 0.0024,
      "step": 13030
    },
    {
      "epoch": 0.6954666666666667,
      "grad_norm": 0.3484242856502533,
      "learning_rate": 4.565333333333334e-05,
      "loss": 0.0037,
      "step": 13040
    },
    {
      "epoch": 0.696,
      "grad_norm": 0.29018092155456543,
      "learning_rate": 4.5650000000000005e-05,
      "loss": 0.0034,
      "step": 13050
    },
    {
      "epoch": 0.6965333333333333,
      "grad_norm": 0.02906137891113758,
      "learning_rate": 4.564666666666667e-05,
      "loss": 0.0037,
      "step": 13060
    },
    {
      "epoch": 0.6970666666666666,
      "grad_norm": 0.23210550844669342,
      "learning_rate": 4.564333333333334e-05,
      "loss": 0.0032,
      "step": 13070
    },
    {
      "epoch": 0.6976,
      "grad_norm": 0.17407581210136414,
      "learning_rate": 4.564e-05,
      "loss": 0.0029,
      "step": 13080
    },
    {
      "epoch": 0.6981333333333334,
      "grad_norm": 0.17402052879333496,
      "learning_rate": 4.563666666666667e-05,
      "loss": 0.0032,
      "step": 13090
    },
    {
      "epoch": 0.6986666666666667,
      "grad_norm": 0.11593189835548401,
      "learning_rate": 4.5633333333333336e-05,
      "loss": 0.0039,
      "step": 13100
    },
    {
      "epoch": 0.6992,
      "grad_norm": 0.0870094746351242,
      "learning_rate": 4.563e-05,
      "loss": 0.0026,
      "step": 13110
    },
    {
      "epoch": 0.6997333333333333,
      "grad_norm": 0.08688165247440338,
      "learning_rate": 4.562666666666667e-05,
      "loss": 0.0025,
      "step": 13120
    },
    {
      "epoch": 0.7002666666666667,
      "grad_norm": 0.14478906989097595,
      "learning_rate": 4.5623333333333334e-05,
      "loss": 0.0018,
      "step": 13130
    },
    {
      "epoch": 0.7008,
      "grad_norm": 0.31847280263900757,
      "learning_rate": 4.562e-05,
      "loss": 0.0028,
      "step": 13140
    },
    {
      "epoch": 0.7013333333333334,
      "grad_norm": 0.08689043670892715,
      "learning_rate": 4.5616666666666666e-05,
      "loss": 0.0031,
      "step": 13150
    },
    {
      "epoch": 0.7018666666666666,
      "grad_norm": 0.0016771783120930195,
      "learning_rate": 4.561333333333333e-05,
      "loss": 0.0025,
      "step": 13160
    },
    {
      "epoch": 0.7024,
      "grad_norm": 0.11583463102579117,
      "learning_rate": 4.5610000000000005e-05,
      "loss": 0.0037,
      "step": 13170
    },
    {
      "epoch": 0.7029333333333333,
      "grad_norm": 0.0019587846472859383,
      "learning_rate": 4.560666666666667e-05,
      "loss": 0.0036,
      "step": 13180
    },
    {
      "epoch": 0.7034666666666667,
      "grad_norm": 0.23161764442920685,
      "learning_rate": 4.560333333333334e-05,
      "loss": 0.0046,
      "step": 13190
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.17374639213085175,
      "learning_rate": 4.5600000000000004e-05,
      "loss": 0.0036,
      "step": 13200
    },
    {
      "epoch": 0.7045333333333333,
      "grad_norm": 0.08699256926774979,
      "learning_rate": 4.559666666666667e-05,
      "loss": 0.0031,
      "step": 13210
    },
    {
      "epoch": 0.7050666666666666,
      "grad_norm": 0.001247937441803515,
      "learning_rate": 4.5593333333333336e-05,
      "loss": 0.0024,
      "step": 13220
    },
    {
      "epoch": 0.7056,
      "grad_norm": 0.14481393992900848,
      "learning_rate": 4.559e-05,
      "loss": 0.0034,
      "step": 13230
    },
    {
      "epoch": 0.7061333333333333,
      "grad_norm": 0.40500086545944214,
      "learning_rate": 4.558666666666667e-05,
      "loss": 0.0016,
      "step": 13240
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.029286110773682594,
      "learning_rate": 4.5583333333333335e-05,
      "loss": 0.0024,
      "step": 13250
    },
    {
      "epoch": 0.7072,
      "grad_norm": 0.2894757390022278,
      "learning_rate": 4.558e-05,
      "loss": 0.0037,
      "step": 13260
    },
    {
      "epoch": 0.7077333333333333,
      "grad_norm": 0.20266856253147125,
      "learning_rate": 4.557666666666667e-05,
      "loss": 0.0028,
      "step": 13270
    },
    {
      "epoch": 0.7082666666666667,
      "grad_norm": 0.3764197826385498,
      "learning_rate": 4.557333333333333e-05,
      "loss": 0.0046,
      "step": 13280
    },
    {
      "epoch": 0.7088,
      "grad_norm": 0.20271438360214233,
      "learning_rate": 4.557e-05,
      "loss": 0.0042,
      "step": 13290
    },
    {
      "epoch": 0.7093333333333334,
      "grad_norm": 0.1737881898880005,
      "learning_rate": 4.556666666666667e-05,
      "loss": 0.0033,
      "step": 13300
    },
    {
      "epoch": 0.7098666666666666,
      "grad_norm": 0.058017976582050323,
      "learning_rate": 4.556333333333334e-05,
      "loss": 0.0032,
      "step": 13310
    },
    {
      "epoch": 0.7104,
      "grad_norm": 0.05792400613427162,
      "learning_rate": 4.5560000000000004e-05,
      "loss": 0.0041,
      "step": 13320
    },
    {
      "epoch": 0.7109333333333333,
      "grad_norm": 0.20268522202968597,
      "learning_rate": 4.555666666666667e-05,
      "loss": 0.0029,
      "step": 13330
    },
    {
      "epoch": 0.7114666666666667,
      "grad_norm": 0.029028985649347305,
      "learning_rate": 4.5553333333333337e-05,
      "loss": 0.0037,
      "step": 13340
    },
    {
      "epoch": 0.712,
      "grad_norm": 0.37593382596969604,
      "learning_rate": 4.555e-05,
      "loss": 0.0037,
      "step": 13350
    },
    {
      "epoch": 0.7125333333333334,
      "grad_norm": 0.4046533405780792,
      "learning_rate": 4.554666666666667e-05,
      "loss": 0.0039,
      "step": 13360
    },
    {
      "epoch": 0.7130666666666666,
      "grad_norm": 0.17379508912563324,
      "learning_rate": 4.5543333333333335e-05,
      "loss": 0.0035,
      "step": 13370
    },
    {
      "epoch": 0.7136,
      "grad_norm": 0.05804828554391861,
      "learning_rate": 4.554000000000001e-05,
      "loss": 0.0041,
      "step": 13380
    },
    {
      "epoch": 0.7141333333333333,
      "grad_norm": 0.15134212374687195,
      "learning_rate": 4.553666666666667e-05,
      "loss": 0.0045,
      "step": 13390
    },
    {
      "epoch": 0.7146666666666667,
      "grad_norm": 0.26061150431632996,
      "learning_rate": 4.553333333333333e-05,
      "loss": 0.0028,
      "step": 13400
    },
    {
      "epoch": 0.7152,
      "grad_norm": 0.463428258895874,
      "learning_rate": 4.553e-05,
      "loss": 0.0053,
      "step": 13410
    },
    {
      "epoch": 0.7157333333333333,
      "grad_norm": 0.14478643238544464,
      "learning_rate": 4.5526666666666666e-05,
      "loss": 0.004,
      "step": 13420
    },
    {
      "epoch": 0.7162666666666667,
      "grad_norm": 0.2896057069301605,
      "learning_rate": 4.552333333333333e-05,
      "loss": 0.0027,
      "step": 13430
    },
    {
      "epoch": 0.7168,
      "grad_norm": 0.3764168620109558,
      "learning_rate": 4.5520000000000005e-05,
      "loss": 0.0038,
      "step": 13440
    },
    {
      "epoch": 0.7173333333333334,
      "grad_norm": 0.2894341051578522,
      "learning_rate": 4.551666666666667e-05,
      "loss": 0.0035,
      "step": 13450
    },
    {
      "epoch": 0.7178666666666667,
      "grad_norm": 0.14478282630443573,
      "learning_rate": 4.551333333333334e-05,
      "loss": 0.0026,
      "step": 13460
    },
    {
      "epoch": 0.7184,
      "grad_norm": 0.31832462549209595,
      "learning_rate": 4.551e-05,
      "loss": 0.0022,
      "step": 13470
    },
    {
      "epoch": 0.7189333333333333,
      "grad_norm": 0.2893759608268738,
      "learning_rate": 4.550666666666667e-05,
      "loss": 0.0043,
      "step": 13480
    },
    {
      "epoch": 0.7194666666666667,
      "grad_norm": 0.17363528907299042,
      "learning_rate": 4.5503333333333335e-05,
      "loss": 0.0039,
      "step": 13490
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.2892712354660034,
      "learning_rate": 4.55e-05,
      "loss": 0.0028,
      "step": 13500
    },
    {
      "epoch": 0.7205333333333334,
      "grad_norm": 0.20256678760051727,
      "learning_rate": 4.549666666666667e-05,
      "loss": 0.0038,
      "step": 13510
    },
    {
      "epoch": 0.7210666666666666,
      "grad_norm": 0.17337928712368011,
      "learning_rate": 4.549333333333334e-05,
      "loss": 0.0049,
      "step": 13520
    },
    {
      "epoch": 0.7216,
      "grad_norm": 0.23161618411540985,
      "learning_rate": 4.549000000000001e-05,
      "loss": 0.002,
      "step": 13530
    },
    {
      "epoch": 0.7221333333333333,
      "grad_norm": 0.08689762651920319,
      "learning_rate": 4.5486666666666666e-05,
      "loss": 0.0041,
      "step": 13540
    },
    {
      "epoch": 0.7226666666666667,
      "grad_norm": 0.08683543652296066,
      "learning_rate": 4.548333333333333e-05,
      "loss": 0.0029,
      "step": 13550
    },
    {
      "epoch": 0.7232,
      "grad_norm": 0.4065999686717987,
      "learning_rate": 4.548e-05,
      "loss": 0.003,
      "step": 13560
    },
    {
      "epoch": 0.7237333333333333,
      "grad_norm": 0.23168008029460907,
      "learning_rate": 4.5476666666666664e-05,
      "loss": 0.0043,
      "step": 13570
    },
    {
      "epoch": 0.7242666666666666,
      "grad_norm": 0.2605106830596924,
      "learning_rate": 4.547333333333334e-05,
      "loss": 0.0042,
      "step": 13580
    },
    {
      "epoch": 0.7248,
      "grad_norm": 0.001167052541859448,
      "learning_rate": 4.5470000000000003e-05,
      "loss": 0.0046,
      "step": 13590
    },
    {
      "epoch": 0.7253333333333334,
      "grad_norm": 0.11579030007123947,
      "learning_rate": 4.546666666666667e-05,
      "loss": 0.0024,
      "step": 13600
    },
    {
      "epoch": 0.7258666666666667,
      "grad_norm": 0.17365729808807373,
      "learning_rate": 4.5463333333333336e-05,
      "loss": 0.0034,
      "step": 13610
    },
    {
      "epoch": 0.7264,
      "grad_norm": 0.05793208256363869,
      "learning_rate": 4.546e-05,
      "loss": 0.0038,
      "step": 13620
    },
    {
      "epoch": 0.7269333333333333,
      "grad_norm": 0.34734025597572327,
      "learning_rate": 4.545666666666667e-05,
      "loss": 0.003,
      "step": 13630
    },
    {
      "epoch": 0.7274666666666667,
      "grad_norm": 0.43433794379234314,
      "learning_rate": 4.5453333333333334e-05,
      "loss": 0.0024,
      "step": 13640
    },
    {
      "epoch": 0.728,
      "grad_norm": 0.2605585753917694,
      "learning_rate": 4.545000000000001e-05,
      "loss": 0.0032,
      "step": 13650
    },
    {
      "epoch": 0.7285333333333334,
      "grad_norm": 0.3184159994125366,
      "learning_rate": 4.544666666666667e-05,
      "loss": 0.0032,
      "step": 13660
    },
    {
      "epoch": 0.7290666666666666,
      "grad_norm": 0.34735408425331116,
      "learning_rate": 4.544333333333334e-05,
      "loss": 0.0039,
      "step": 13670
    },
    {
      "epoch": 0.7296,
      "grad_norm": 0.31837350130081177,
      "learning_rate": 4.5440000000000005e-05,
      "loss": 0.0046,
      "step": 13680
    },
    {
      "epoch": 0.7301333333333333,
      "grad_norm": 0.3473203480243683,
      "learning_rate": 4.5436666666666665e-05,
      "loss": 0.003,
      "step": 13690
    },
    {
      "epoch": 0.7306666666666667,
      "grad_norm": 0.31825974583625793,
      "learning_rate": 4.543333333333333e-05,
      "loss": 0.003,
      "step": 13700
    },
    {
      "epoch": 0.7312,
      "grad_norm": 0.3471280634403229,
      "learning_rate": 4.543e-05,
      "loss": 0.0018,
      "step": 13710
    },
    {
      "epoch": 0.7317333333333333,
      "grad_norm": 0.0016336728585883975,
      "learning_rate": 4.542666666666667e-05,
      "loss": 0.0025,
      "step": 13720
    },
    {
      "epoch": 0.7322666666666666,
      "grad_norm": 0.31806424260139465,
      "learning_rate": 4.5423333333333336e-05,
      "loss": 0.0017,
      "step": 13730
    },
    {
      "epoch": 0.7328,
      "grad_norm": 0.4911487400531769,
      "learning_rate": 4.542e-05,
      "loss": 0.0034,
      "step": 13740
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.2885283827781677,
      "learning_rate": 4.541666666666667e-05,
      "loss": 0.0037,
      "step": 13750
    },
    {
      "epoch": 0.7338666666666667,
      "grad_norm": 0.2025010734796524,
      "learning_rate": 4.5413333333333334e-05,
      "loss": 0.0034,
      "step": 13760
    },
    {
      "epoch": 0.7344,
      "grad_norm": 0.05797190964221954,
      "learning_rate": 4.541e-05,
      "loss": 0.003,
      "step": 13770
    },
    {
      "epoch": 0.7349333333333333,
      "grad_norm": 0.11575503647327423,
      "learning_rate": 4.540666666666667e-05,
      "loss": 0.0022,
      "step": 13780
    },
    {
      "epoch": 0.7354666666666667,
      "grad_norm": 0.23119011521339417,
      "learning_rate": 4.540333333333334e-05,
      "loss": 0.0031,
      "step": 13790
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.029054727405309677,
      "learning_rate": 4.5400000000000006e-05,
      "loss": 0.004,
      "step": 13800
    },
    {
      "epoch": 0.7365333333333334,
      "grad_norm": 0.3758257329463959,
      "learning_rate": 4.539666666666667e-05,
      "loss": 0.0038,
      "step": 13810
    },
    {
      "epoch": 0.7370666666666666,
      "grad_norm": 0.11587783694267273,
      "learning_rate": 4.539333333333334e-05,
      "loss": 0.0039,
      "step": 13820
    },
    {
      "epoch": 0.7376,
      "grad_norm": 0.37667250633239746,
      "learning_rate": 4.5390000000000004e-05,
      "loss": 0.0029,
      "step": 13830
    },
    {
      "epoch": 0.7381333333333333,
      "grad_norm": 0.1160968765616417,
      "learning_rate": 4.5386666666666664e-05,
      "loss": 0.0024,
      "step": 13840
    },
    {
      "epoch": 0.7386666666666667,
      "grad_norm": 0.14485213160514832,
      "learning_rate": 4.5383333333333336e-05,
      "loss": 0.0023,
      "step": 13850
    },
    {
      "epoch": 0.7392,
      "grad_norm": 0.2026887685060501,
      "learning_rate": 4.538e-05,
      "loss": 0.0029,
      "step": 13860
    },
    {
      "epoch": 0.7397333333333334,
      "grad_norm": 0.0869378000497818,
      "learning_rate": 4.537666666666667e-05,
      "loss": 0.0023,
      "step": 13870
    },
    {
      "epoch": 0.7402666666666666,
      "grad_norm": 0.05796174705028534,
      "learning_rate": 4.5373333333333335e-05,
      "loss": 0.004,
      "step": 13880
    },
    {
      "epoch": 0.7408,
      "grad_norm": 0.34761661291122437,
      "learning_rate": 4.537e-05,
      "loss": 0.0028,
      "step": 13890
    },
    {
      "epoch": 0.7413333333333333,
      "grad_norm": 0.17376630008220673,
      "learning_rate": 4.536666666666667e-05,
      "loss": 0.004,
      "step": 13900
    },
    {
      "epoch": 0.7418666666666667,
      "grad_norm": 0.3184775710105896,
      "learning_rate": 4.536333333333333e-05,
      "loss": 0.0038,
      "step": 13910
    },
    {
      "epoch": 0.7424,
      "grad_norm": 0.029031328856945038,
      "learning_rate": 4.536e-05,
      "loss": 0.0027,
      "step": 13920
    },
    {
      "epoch": 0.7429333333333333,
      "grad_norm": 0.05784434825181961,
      "learning_rate": 4.535666666666667e-05,
      "loss": 0.0028,
      "step": 13930
    },
    {
      "epoch": 0.7434666666666667,
      "grad_norm": 0.0018577674636617303,
      "learning_rate": 4.535333333333334e-05,
      "loss": 0.0027,
      "step": 13940
    },
    {
      "epoch": 0.744,
      "grad_norm": 0.08674917370080948,
      "learning_rate": 4.5350000000000005e-05,
      "loss": 0.0024,
      "step": 13950
    },
    {
      "epoch": 0.7445333333333334,
      "grad_norm": 0.3469628691673279,
      "learning_rate": 4.534666666666667e-05,
      "loss": 0.0027,
      "step": 13960
    },
    {
      "epoch": 0.7450666666666667,
      "grad_norm": 0.549340546131134,
      "learning_rate": 4.534333333333334e-05,
      "loss": 0.0031,
      "step": 13970
    },
    {
      "epoch": 0.7456,
      "grad_norm": 0.11565425992012024,
      "learning_rate": 4.534e-05,
      "loss": 0.0038,
      "step": 13980
    },
    {
      "epoch": 0.7461333333333333,
      "grad_norm": 0.34686580300331116,
      "learning_rate": 4.533666666666667e-05,
      "loss": 0.0038,
      "step": 13990
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.02913505770266056,
      "learning_rate": 4.5333333333333335e-05,
      "loss": 0.004,
      "step": 14000
    },
    {
      "epoch": 0.7472,
      "grad_norm": 0.05781780928373337,
      "learning_rate": 4.533e-05,
      "loss": 0.0028,
      "step": 14010
    },
    {
      "epoch": 0.7477333333333334,
      "grad_norm": 0.1735166311264038,
      "learning_rate": 4.532666666666667e-05,
      "loss": 0.0032,
      "step": 14020
    },
    {
      "epoch": 0.7482666666666666,
      "grad_norm": 0.17292365431785583,
      "learning_rate": 4.5323333333333334e-05,
      "loss": 0.0026,
      "step": 14030
    },
    {
      "epoch": 0.7488,
      "grad_norm": 0.11533642560243607,
      "learning_rate": 4.532e-05,
      "loss": 0.0029,
      "step": 14040
    },
    {
      "epoch": 0.7493333333333333,
      "grad_norm": 0.3750619888305664,
      "learning_rate": 4.5316666666666666e-05,
      "loss": 0.0039,
      "step": 14050
    },
    {
      "epoch": 0.7498666666666667,
      "grad_norm": 0.23227345943450928,
      "learning_rate": 4.531333333333333e-05,
      "loss": 0.0038,
      "step": 14060
    },
    {
      "epoch": 0.7504,
      "grad_norm": 0.40793266892433167,
      "learning_rate": 4.5310000000000005e-05,
      "loss": 0.0029,
      "step": 14070
    },
    {
      "epoch": 0.7509333333333333,
      "grad_norm": 0.08725826442241669,
      "learning_rate": 4.530666666666667e-05,
      "loss": 0.0034,
      "step": 14080
    },
    {
      "epoch": 0.7514666666666666,
      "grad_norm": 0.1172296330332756,
      "learning_rate": 4.530333333333334e-05,
      "loss": 0.0032,
      "step": 14090
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.23312720656394958,
      "learning_rate": 4.53e-05,
      "loss": 0.0039,
      "step": 14100
    },
    {
      "epoch": 0.7525333333333334,
      "grad_norm": 0.029139215126633644,
      "learning_rate": 4.529666666666667e-05,
      "loss": 0.005,
      "step": 14110
    },
    {
      "epoch": 0.7530666666666667,
      "grad_norm": 0.14621996879577637,
      "learning_rate": 4.5293333333333336e-05,
      "loss": 0.0041,
      "step": 14120
    },
    {
      "epoch": 0.7536,
      "grad_norm": 0.23261994123458862,
      "learning_rate": 4.529e-05,
      "loss": 0.0047,
      "step": 14130
    },
    {
      "epoch": 0.7541333333333333,
      "grad_norm": 0.31959253549575806,
      "learning_rate": 4.528666666666667e-05,
      "loss": 0.0027,
      "step": 14140
    },
    {
      "epoch": 0.7546666666666667,
      "grad_norm": 0.26135557889938354,
      "learning_rate": 4.5283333333333334e-05,
      "loss": 0.004,
      "step": 14150
    },
    {
      "epoch": 0.7552,
      "grad_norm": 0.46462374925613403,
      "learning_rate": 4.528e-05,
      "loss": 0.0034,
      "step": 14160
    },
    {
      "epoch": 0.7557333333333334,
      "grad_norm": 0.2323361337184906,
      "learning_rate": 4.5276666666666666e-05,
      "loss": 0.0035,
      "step": 14170
    },
    {
      "epoch": 0.7562666666666666,
      "grad_norm": 0.6668410897254944,
      "learning_rate": 4.527333333333333e-05,
      "loss": 0.0022,
      "step": 14180
    },
    {
      "epoch": 0.7568,
      "grad_norm": 0.029212718829512596,
      "learning_rate": 4.527e-05,
      "loss": 0.0042,
      "step": 14190
    },
    {
      "epoch": 0.7573333333333333,
      "grad_norm": 0.2895473837852478,
      "learning_rate": 4.526666666666667e-05,
      "loss": 0.005,
      "step": 14200
    },
    {
      "epoch": 0.7578666666666667,
      "grad_norm": 0.3764307498931885,
      "learning_rate": 4.526333333333334e-05,
      "loss": 0.0041,
      "step": 14210
    },
    {
      "epoch": 0.7584,
      "grad_norm": 0.2609056830406189,
      "learning_rate": 4.5260000000000004e-05,
      "loss": 0.0032,
      "step": 14220
    },
    {
      "epoch": 0.7589333333333333,
      "grad_norm": 0.2603837251663208,
      "learning_rate": 4.525666666666667e-05,
      "loss": 0.0038,
      "step": 14230
    },
    {
      "epoch": 0.7594666666666666,
      "grad_norm": 0.23160676658153534,
      "learning_rate": 4.5253333333333336e-05,
      "loss": 0.0027,
      "step": 14240
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.17354637384414673,
      "learning_rate": 4.525e-05,
      "loss": 0.0042,
      "step": 14250
    },
    {
      "epoch": 0.7605333333333333,
      "grad_norm": 0.1269473135471344,
      "learning_rate": 4.524666666666667e-05,
      "loss": 0.0038,
      "step": 14260
    },
    {
      "epoch": 0.7610666666666667,
      "grad_norm": 0.23153488337993622,
      "learning_rate": 4.5243333333333334e-05,
      "loss": 0.0024,
      "step": 14270
    },
    {
      "epoch": 0.7616,
      "grad_norm": 0.08678428083658218,
      "learning_rate": 4.524000000000001e-05,
      "loss": 0.0025,
      "step": 14280
    },
    {
      "epoch": 0.7621333333333333,
      "grad_norm": 0.7788169980049133,
      "learning_rate": 4.523666666666667e-05,
      "loss": 0.0022,
      "step": 14290
    },
    {
      "epoch": 0.7626666666666667,
      "grad_norm": 0.02903306484222412,
      "learning_rate": 4.523333333333333e-05,
      "loss": 0.0035,
      "step": 14300
    },
    {
      "epoch": 0.7632,
      "grad_norm": 0.46297457814216614,
      "learning_rate": 4.523e-05,
      "loss": 0.0032,
      "step": 14310
    },
    {
      "epoch": 0.7637333333333334,
      "grad_norm": 0.28934961557388306,
      "learning_rate": 4.5226666666666665e-05,
      "loss": 0.003,
      "step": 14320
    },
    {
      "epoch": 0.7642666666666666,
      "grad_norm": 0.13933773338794708,
      "learning_rate": 4.522333333333333e-05,
      "loss": 0.0031,
      "step": 14330
    },
    {
      "epoch": 0.7648,
      "grad_norm": 0.029014626517891884,
      "learning_rate": 4.5220000000000004e-05,
      "loss": 0.0033,
      "step": 14340
    },
    {
      "epoch": 0.7653333333333333,
      "grad_norm": 0.11564235389232635,
      "learning_rate": 4.521666666666667e-05,
      "loss": 0.0046,
      "step": 14350
    },
    {
      "epoch": 0.7658666666666667,
      "grad_norm": 0.11578846722841263,
      "learning_rate": 4.5213333333333336e-05,
      "loss": 0.0036,
      "step": 14360
    },
    {
      "epoch": 0.7664,
      "grad_norm": 0.08679265528917313,
      "learning_rate": 4.521e-05,
      "loss": 0.0057,
      "step": 14370
    },
    {
      "epoch": 0.7669333333333334,
      "grad_norm": 0.08701357245445251,
      "learning_rate": 4.520666666666667e-05,
      "loss": 0.0038,
      "step": 14380
    },
    {
      "epoch": 0.7674666666666666,
      "grad_norm": 0.14476393163204193,
      "learning_rate": 4.5203333333333335e-05,
      "loss": 0.0033,
      "step": 14390
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.08690948784351349,
      "learning_rate": 4.52e-05,
      "loss": 0.0029,
      "step": 14400
    },
    {
      "epoch": 0.7685333333333333,
      "grad_norm": 0.1446686089038849,
      "learning_rate": 4.5196666666666674e-05,
      "loss": 0.0041,
      "step": 14410
    },
    {
      "epoch": 0.7690666666666667,
      "grad_norm": 0.0578952431678772,
      "learning_rate": 4.519333333333334e-05,
      "loss": 0.0035,
      "step": 14420
    },
    {
      "epoch": 0.7696,
      "grad_norm": 0.43399810791015625,
      "learning_rate": 4.5190000000000006e-05,
      "loss": 0.003,
      "step": 14430
    },
    {
      "epoch": 0.7701333333333333,
      "grad_norm": 0.08684488385915756,
      "learning_rate": 4.518666666666667e-05,
      "loss": 0.0032,
      "step": 14440
    },
    {
      "epoch": 0.7706666666666667,
      "grad_norm": 0.46294230222702026,
      "learning_rate": 4.518333333333333e-05,
      "loss": 0.0033,
      "step": 14450
    },
    {
      "epoch": 0.7712,
      "grad_norm": 0.18533813953399658,
      "learning_rate": 4.518e-05,
      "loss": 0.0028,
      "step": 14460
    },
    {
      "epoch": 0.7717333333333334,
      "grad_norm": 0.08662653714418411,
      "learning_rate": 4.5176666666666664e-05,
      "loss": 0.0034,
      "step": 14470
    },
    {
      "epoch": 0.7722666666666667,
      "grad_norm": 0.23111993074417114,
      "learning_rate": 4.517333333333334e-05,
      "loss": 0.0024,
      "step": 14480
    },
    {
      "epoch": 0.7728,
      "grad_norm": 0.01945021189749241,
      "learning_rate": 4.517e-05,
      "loss": 0.0033,
      "step": 14490
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.1156625896692276,
      "learning_rate": 4.516666666666667e-05,
      "loss": 0.0041,
      "step": 14500
    },
    {
      "epoch": 0.7738666666666667,
      "grad_norm": 0.6940590739250183,
      "learning_rate": 4.5163333333333335e-05,
      "loss": 0.004,
      "step": 14510
    },
    {
      "epoch": 0.7744,
      "grad_norm": 0.6939812302589417,
      "learning_rate": 4.516e-05,
      "loss": 0.0028,
      "step": 14520
    },
    {
      "epoch": 0.7749333333333334,
      "grad_norm": 0.6321308612823486,
      "learning_rate": 4.515666666666667e-05,
      "loss": 0.0039,
      "step": 14530
    },
    {
      "epoch": 0.7754666666666666,
      "grad_norm": 0.4326326251029968,
      "learning_rate": 4.5153333333333334e-05,
      "loss": 0.0041,
      "step": 14540
    },
    {
      "epoch": 0.776,
      "grad_norm": 0.5502304434776306,
      "learning_rate": 4.5150000000000006e-05,
      "loss": 0.0046,
      "step": 14550
    },
    {
      "epoch": 0.7765333333333333,
      "grad_norm": 0.1447066366672516,
      "learning_rate": 4.514666666666667e-05,
      "loss": 0.0042,
      "step": 14560
    },
    {
      "epoch": 0.7770666666666667,
      "grad_norm": 0.08714685589075089,
      "learning_rate": 4.514333333333334e-05,
      "loss": 0.0044,
      "step": 14570
    },
    {
      "epoch": 0.7776,
      "grad_norm": 0.1160123273730278,
      "learning_rate": 4.5140000000000005e-05,
      "loss": 0.0035,
      "step": 14580
    },
    {
      "epoch": 0.7781333333333333,
      "grad_norm": 0.1740887612104416,
      "learning_rate": 4.513666666666667e-05,
      "loss": 0.0036,
      "step": 14590
    },
    {
      "epoch": 0.7786666666666666,
      "grad_norm": 0.0300259031355381,
      "learning_rate": 4.513333333333333e-05,
      "loss": 0.0036,
      "step": 14600
    },
    {
      "epoch": 0.7792,
      "grad_norm": 0.40632733702659607,
      "learning_rate": 4.513e-05,
      "loss": 0.0035,
      "step": 14610
    },
    {
      "epoch": 0.7797333333333333,
      "grad_norm": 0.20277602970600128,
      "learning_rate": 4.512666666666667e-05,
      "loss": 0.0043,
      "step": 14620
    },
    {
      "epoch": 0.7802666666666667,
      "grad_norm": 0.2892796993255615,
      "learning_rate": 4.5123333333333336e-05,
      "loss": 0.0047,
      "step": 14630
    },
    {
      "epoch": 0.7808,
      "grad_norm": 0.029518278315663338,
      "learning_rate": 4.512e-05,
      "loss": 0.0032,
      "step": 14640
    },
    {
      "epoch": 0.7813333333333333,
      "grad_norm": 0.26249417662620544,
      "learning_rate": 4.511666666666667e-05,
      "loss": 0.0031,
      "step": 14650
    },
    {
      "epoch": 0.7818666666666667,
      "grad_norm": 0.2898885905742645,
      "learning_rate": 4.5113333333333334e-05,
      "loss": 0.0022,
      "step": 14660
    },
    {
      "epoch": 0.7824,
      "grad_norm": 0.08697566390037537,
      "learning_rate": 4.511e-05,
      "loss": 0.0022,
      "step": 14670
    },
    {
      "epoch": 0.7829333333333334,
      "grad_norm": 0.058210138231515884,
      "learning_rate": 4.5106666666666666e-05,
      "loss": 0.0035,
      "step": 14680
    },
    {
      "epoch": 0.7834666666666666,
      "grad_norm": 0.4053647518157959,
      "learning_rate": 4.510333333333334e-05,
      "loss": 0.0038,
      "step": 14690
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.6366443634033203,
      "learning_rate": 4.5100000000000005e-05,
      "loss": 0.0028,
      "step": 14700
    },
    {
      "epoch": 0.7845333333333333,
      "grad_norm": 0.08676613122224808,
      "learning_rate": 4.509666666666667e-05,
      "loss": 0.0034,
      "step": 14710
    },
    {
      "epoch": 0.7850666666666667,
      "grad_norm": 0.11602335423231125,
      "learning_rate": 4.509333333333334e-05,
      "loss": 0.0053,
      "step": 14720
    },
    {
      "epoch": 0.7856,
      "grad_norm": 0.14476673305034637,
      "learning_rate": 4.5090000000000004e-05,
      "loss": 0.0029,
      "step": 14730
    },
    {
      "epoch": 0.7861333333333334,
      "grad_norm": 0.0017934608040377498,
      "learning_rate": 4.508666666666667e-05,
      "loss": 0.0035,
      "step": 14740
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.028956541791558266,
      "learning_rate": 4.5083333333333336e-05,
      "loss": 0.0042,
      "step": 14750
    },
    {
      "epoch": 0.7872,
      "grad_norm": 0.08683627098798752,
      "learning_rate": 4.508e-05,
      "loss": 0.0033,
      "step": 14760
    },
    {
      "epoch": 0.7877333333333333,
      "grad_norm": 0.2895481586456299,
      "learning_rate": 4.507666666666667e-05,
      "loss": 0.0038,
      "step": 14770
    },
    {
      "epoch": 0.7882666666666667,
      "grad_norm": 0.23148034512996674,
      "learning_rate": 4.5073333333333334e-05,
      "loss": 0.0048,
      "step": 14780
    },
    {
      "epoch": 0.7888,
      "grad_norm": 0.37596529722213745,
      "learning_rate": 4.507e-05,
      "loss": 0.0045,
      "step": 14790
    },
    {
      "epoch": 0.7893333333333333,
      "grad_norm": 0.28943124413490295,
      "learning_rate": 4.5066666666666667e-05,
      "loss": 0.0049,
      "step": 14800
    },
    {
      "epoch": 0.7898666666666667,
      "grad_norm": 0.3758414685726166,
      "learning_rate": 4.506333333333333e-05,
      "loss": 0.0039,
      "step": 14810
    },
    {
      "epoch": 0.7904,
      "grad_norm": 0.02889382652938366,
      "learning_rate": 4.506e-05,
      "loss": 0.0025,
      "step": 14820
    },
    {
      "epoch": 0.7909333333333334,
      "grad_norm": 0.3468579947948456,
      "learning_rate": 4.505666666666667e-05,
      "loss": 0.0038,
      "step": 14830
    },
    {
      "epoch": 0.7914666666666667,
      "grad_norm": 0.31801432371139526,
      "learning_rate": 4.505333333333334e-05,
      "loss": 0.0027,
      "step": 14840
    },
    {
      "epoch": 0.792,
      "grad_norm": 0.11569733172655106,
      "learning_rate": 4.5050000000000004e-05,
      "loss": 0.0029,
      "step": 14850
    },
    {
      "epoch": 0.7925333333333333,
      "grad_norm": 0.1733856350183487,
      "learning_rate": 4.504666666666667e-05,
      "loss": 0.0045,
      "step": 14860
    },
    {
      "epoch": 0.7930666666666667,
      "grad_norm": 0.26010575890541077,
      "learning_rate": 4.5043333333333336e-05,
      "loss": 0.0051,
      "step": 14870
    },
    {
      "epoch": 0.7936,
      "grad_norm": 0.173325315117836,
      "learning_rate": 4.504e-05,
      "loss": 0.0036,
      "step": 14880
    },
    {
      "epoch": 0.7941333333333334,
      "grad_norm": 0.7800928950309753,
      "learning_rate": 4.503666666666667e-05,
      "loss": 0.0032,
      "step": 14890
    },
    {
      "epoch": 0.7946666666666666,
      "grad_norm": 0.1443421095609665,
      "learning_rate": 4.5033333333333335e-05,
      "loss": 0.0047,
      "step": 14900
    },
    {
      "epoch": 0.7952,
      "grad_norm": 0.26003870368003845,
      "learning_rate": 4.503e-05,
      "loss": 0.0041,
      "step": 14910
    },
    {
      "epoch": 0.7957333333333333,
      "grad_norm": 0.17353622615337372,
      "learning_rate": 4.502666666666667e-05,
      "loss": 0.0025,
      "step": 14920
    },
    {
      "epoch": 0.7962666666666667,
      "grad_norm": 0.1155971884727478,
      "learning_rate": 4.502333333333333e-05,
      "loss": 0.0039,
      "step": 14930
    },
    {
      "epoch": 0.7968,
      "grad_norm": 0.20229315757751465,
      "learning_rate": 4.502e-05,
      "loss": 0.0032,
      "step": 14940
    },
    {
      "epoch": 0.7973333333333333,
      "grad_norm": 0.6936236619949341,
      "learning_rate": 4.5016666666666665e-05,
      "loss": 0.0029,
      "step": 14950
    },
    {
      "epoch": 0.7978666666666666,
      "grad_norm": 0.2021879255771637,
      "learning_rate": 4.501333333333334e-05,
      "loss": 0.0036,
      "step": 14960
    },
    {
      "epoch": 0.7984,
      "grad_norm": 0.26110953092575073,
      "learning_rate": 4.5010000000000004e-05,
      "loss": 0.0027,
      "step": 14970
    },
    {
      "epoch": 0.7989333333333334,
      "grad_norm": 0.3195366859436035,
      "learning_rate": 4.500666666666667e-05,
      "loss": 0.0031,
      "step": 14980
    },
    {
      "epoch": 0.7994666666666667,
      "grad_norm": 0.6392650604248047,
      "learning_rate": 4.500333333333334e-05,
      "loss": 0.0023,
      "step": 14990
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.29002973437309265,
      "learning_rate": 4.5e-05,
      "loss": 0.0049,
      "step": 15000
    },
    {
      "epoch": 0.8005333333333333,
      "grad_norm": 0.08694402128458023,
      "learning_rate": 4.499666666666667e-05,
      "loss": 0.0042,
      "step": 15010
    },
    {
      "epoch": 0.8010666666666667,
      "grad_norm": 0.3769475221633911,
      "learning_rate": 4.4993333333333335e-05,
      "loss": 0.0026,
      "step": 15020
    },
    {
      "epoch": 0.8016,
      "grad_norm": 0.11570020765066147,
      "learning_rate": 4.499e-05,
      "loss": 0.0025,
      "step": 15030
    },
    {
      "epoch": 0.8021333333333334,
      "grad_norm": 0.3481138348579407,
      "learning_rate": 4.4986666666666674e-05,
      "loss": 0.0037,
      "step": 15040
    },
    {
      "epoch": 0.8026666666666666,
      "grad_norm": 0.14527851343154907,
      "learning_rate": 4.4983333333333334e-05,
      "loss": 0.0052,
      "step": 15050
    },
    {
      "epoch": 0.8032,
      "grad_norm": 0.2899599075317383,
      "learning_rate": 4.498e-05,
      "loss": 0.0039,
      "step": 15060
    },
    {
      "epoch": 0.8037333333333333,
      "grad_norm": 0.40564846992492676,
      "learning_rate": 4.4976666666666666e-05,
      "loss": 0.0047,
      "step": 15070
    },
    {
      "epoch": 0.8042666666666667,
      "grad_norm": 0.11574027687311172,
      "learning_rate": 4.497333333333333e-05,
      "loss": 0.0035,
      "step": 15080
    },
    {
      "epoch": 0.8048,
      "grad_norm": 0.028989965096116066,
      "learning_rate": 4.497e-05,
      "loss": 0.0032,
      "step": 15090
    },
    {
      "epoch": 0.8053333333333333,
      "grad_norm": 0.31833362579345703,
      "learning_rate": 4.496666666666667e-05,
      "loss": 0.0033,
      "step": 15100
    },
    {
      "epoch": 0.8058666666666666,
      "grad_norm": 0.46324753761291504,
      "learning_rate": 4.496333333333334e-05,
      "loss": 0.0044,
      "step": 15110
    },
    {
      "epoch": 0.8064,
      "grad_norm": 0.17321522533893585,
      "learning_rate": 4.496e-05,
      "loss": 0.0027,
      "step": 15120
    },
    {
      "epoch": 0.8069333333333333,
      "grad_norm": 0.0015619727782905102,
      "learning_rate": 4.495666666666667e-05,
      "loss": 0.003,
      "step": 15130
    },
    {
      "epoch": 0.8074666666666667,
      "grad_norm": 0.11574538052082062,
      "learning_rate": 4.4953333333333335e-05,
      "loss": 0.0046,
      "step": 15140
    },
    {
      "epoch": 0.808,
      "grad_norm": 0.028992289677262306,
      "learning_rate": 4.495e-05,
      "loss": 0.0027,
      "step": 15150
    },
    {
      "epoch": 0.8085333333333333,
      "grad_norm": 0.17375361919403076,
      "learning_rate": 4.494666666666667e-05,
      "loss": 0.0034,
      "step": 15160
    },
    {
      "epoch": 0.8090666666666667,
      "grad_norm": 0.26055318117141724,
      "learning_rate": 4.494333333333334e-05,
      "loss": 0.0024,
      "step": 15170
    },
    {
      "epoch": 0.8096,
      "grad_norm": 0.05786292254924774,
      "learning_rate": 4.494000000000001e-05,
      "loss": 0.0025,
      "step": 15180
    },
    {
      "epoch": 0.8101333333333334,
      "grad_norm": 0.08676847815513611,
      "learning_rate": 4.493666666666667e-05,
      "loss": 0.0034,
      "step": 15190
    },
    {
      "epoch": 0.8106666666666666,
      "grad_norm": 0.11589989811182022,
      "learning_rate": 4.493333333333333e-05,
      "loss": 0.0034,
      "step": 15200
    },
    {
      "epoch": 0.8112,
      "grad_norm": 0.376313716173172,
      "learning_rate": 4.493e-05,
      "loss": 0.0028,
      "step": 15210
    },
    {
      "epoch": 0.8117333333333333,
      "grad_norm": 0.028938358649611473,
      "learning_rate": 4.4926666666666665e-05,
      "loss": 0.0041,
      "step": 15220
    },
    {
      "epoch": 0.8122666666666667,
      "grad_norm": 0.289554238319397,
      "learning_rate": 4.492333333333333e-05,
      "loss": 0.0028,
      "step": 15230
    },
    {
      "epoch": 0.8128,
      "grad_norm": 0.5790107250213623,
      "learning_rate": 4.4920000000000004e-05,
      "loss": 0.0036,
      "step": 15240
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.3350163400173187,
      "learning_rate": 4.491666666666667e-05,
      "loss": 0.0038,
      "step": 15250
    },
    {
      "epoch": 0.8138666666666666,
      "grad_norm": 0.4625599682331085,
      "learning_rate": 4.4913333333333336e-05,
      "loss": 0.0042,
      "step": 15260
    },
    {
      "epoch": 0.8144,
      "grad_norm": 0.6358983516693115,
      "learning_rate": 4.491e-05,
      "loss": 0.0031,
      "step": 15270
    },
    {
      "epoch": 0.8149333333333333,
      "grad_norm": 0.2876262068748474,
      "learning_rate": 4.490666666666667e-05,
      "loss": 0.0038,
      "step": 15280
    },
    {
      "epoch": 0.8154666666666667,
      "grad_norm": 0.004728253465145826,
      "learning_rate": 4.4903333333333334e-05,
      "loss": 0.0033,
      "step": 15290
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.2306576520204544,
      "learning_rate": 4.49e-05,
      "loss": 0.003,
      "step": 15300
    },
    {
      "epoch": 0.8165333333333333,
      "grad_norm": 0.086473748087883,
      "learning_rate": 4.489666666666667e-05,
      "loss": 0.0036,
      "step": 15310
    },
    {
      "epoch": 0.8170666666666667,
      "grad_norm": 0.23111878335475922,
      "learning_rate": 4.489333333333334e-05,
      "loss": 0.0029,
      "step": 15320
    },
    {
      "epoch": 0.8176,
      "grad_norm": 0.31800687313079834,
      "learning_rate": 4.4890000000000006e-05,
      "loss": 0.0025,
      "step": 15330
    },
    {
      "epoch": 0.8181333333333334,
      "grad_norm": 0.02904069609940052,
      "learning_rate": 4.488666666666667e-05,
      "loss": 0.0034,
      "step": 15340
    },
    {
      "epoch": 0.8186666666666667,
      "grad_norm": 0.05779588967561722,
      "learning_rate": 4.488333333333333e-05,
      "loss": 0.0037,
      "step": 15350
    },
    {
      "epoch": 0.8192,
      "grad_norm": 0.4338092505931854,
      "learning_rate": 4.488e-05,
      "loss": 0.0042,
      "step": 15360
    },
    {
      "epoch": 0.8197333333333333,
      "grad_norm": 0.1443585455417633,
      "learning_rate": 4.487666666666667e-05,
      "loss": 0.0039,
      "step": 15370
    },
    {
      "epoch": 0.8202666666666667,
      "grad_norm": 0.17332163453102112,
      "learning_rate": 4.4873333333333336e-05,
      "loss": 0.0039,
      "step": 15380
    },
    {
      "epoch": 0.8208,
      "grad_norm": 0.029455427080392838,
      "learning_rate": 4.487e-05,
      "loss": 0.002,
      "step": 15390
    },
    {
      "epoch": 0.8213333333333334,
      "grad_norm": 0.057741157710552216,
      "learning_rate": 4.486666666666667e-05,
      "loss": 0.0042,
      "step": 15400
    },
    {
      "epoch": 0.8218666666666666,
      "grad_norm": 0.5487074255943298,
      "learning_rate": 4.4863333333333335e-05,
      "loss": 0.0041,
      "step": 15410
    },
    {
      "epoch": 0.8224,
      "grad_norm": 0.173786461353302,
      "learning_rate": 4.486e-05,
      "loss": 0.0023,
      "step": 15420
    },
    {
      "epoch": 0.8229333333333333,
      "grad_norm": 0.23105698823928833,
      "learning_rate": 4.485666666666667e-05,
      "loss": 0.0029,
      "step": 15430
    },
    {
      "epoch": 0.8234666666666667,
      "grad_norm": 0.2306479513645172,
      "learning_rate": 4.485333333333333e-05,
      "loss": 0.0032,
      "step": 15440
    },
    {
      "epoch": 0.824,
      "grad_norm": 0.5824099779129028,
      "learning_rate": 4.4850000000000006e-05,
      "loss": 0.0033,
      "step": 15450
    },
    {
      "epoch": 0.8245333333333333,
      "grad_norm": 0.4342782497406006,
      "learning_rate": 4.484666666666667e-05,
      "loss": 0.0035,
      "step": 15460
    },
    {
      "epoch": 0.8250666666666666,
      "grad_norm": 0.40502795577049255,
      "learning_rate": 4.484333333333334e-05,
      "loss": 0.004,
      "step": 15470
    },
    {
      "epoch": 0.8256,
      "grad_norm": 0.11532576382160187,
      "learning_rate": 4.4840000000000004e-05,
      "loss": 0.0033,
      "step": 15480
    },
    {
      "epoch": 0.8261333333333334,
      "grad_norm": 0.08769003301858902,
      "learning_rate": 4.483666666666667e-05,
      "loss": 0.0046,
      "step": 15490
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 2.174551010131836,
      "learning_rate": 4.483333333333333e-05,
      "loss": 0.0024,
      "step": 15500
    },
    {
      "epoch": 0.8272,
      "grad_norm": 0.1736367642879486,
      "learning_rate": 4.483e-05,
      "loss": 0.0024,
      "step": 15510
    },
    {
      "epoch": 0.8277333333333333,
      "grad_norm": 0.05775986239314079,
      "learning_rate": 4.482666666666667e-05,
      "loss": 0.0038,
      "step": 15520
    },
    {
      "epoch": 0.8282666666666667,
      "grad_norm": 0.11589820683002472,
      "learning_rate": 4.4823333333333335e-05,
      "loss": 0.0031,
      "step": 15530
    },
    {
      "epoch": 0.8288,
      "grad_norm": 0.0866062194108963,
      "learning_rate": 4.482e-05,
      "loss": 0.0033,
      "step": 15540
    },
    {
      "epoch": 0.8293333333333334,
      "grad_norm": 0.34877410531044006,
      "learning_rate": 4.481666666666667e-05,
      "loss": 0.0035,
      "step": 15550
    },
    {
      "epoch": 0.8298666666666666,
      "grad_norm": 0.030040955170989037,
      "learning_rate": 4.4813333333333333e-05,
      "loss": 0.0039,
      "step": 15560
    },
    {
      "epoch": 0.8304,
      "grad_norm": 0.20315402746200562,
      "learning_rate": 4.481e-05,
      "loss": 0.0038,
      "step": 15570
    },
    {
      "epoch": 0.8309333333333333,
      "grad_norm": 0.029113803058862686,
      "learning_rate": 4.4806666666666666e-05,
      "loss": 0.0037,
      "step": 15580
    },
    {
      "epoch": 0.8314666666666667,
      "grad_norm": 0.46195095777511597,
      "learning_rate": 4.480333333333334e-05,
      "loss": 0.0024,
      "step": 15590
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.634706437587738,
      "learning_rate": 4.4800000000000005e-05,
      "loss": 0.002,
      "step": 15600
    },
    {
      "epoch": 0.8325333333333333,
      "grad_norm": 0.23108211159706116,
      "learning_rate": 4.479666666666667e-05,
      "loss": 0.0035,
      "step": 15610
    },
    {
      "epoch": 0.8330666666666666,
      "grad_norm": 0.029383748769760132,
      "learning_rate": 4.479333333333334e-05,
      "loss": 0.003,
      "step": 15620
    },
    {
      "epoch": 0.8336,
      "grad_norm": 0.17356349527835846,
      "learning_rate": 4.479e-05,
      "loss": 0.0029,
      "step": 15630
    },
    {
      "epoch": 0.8341333333333333,
      "grad_norm": 0.28873151540756226,
      "learning_rate": 4.478666666666667e-05,
      "loss": 0.0044,
      "step": 15640
    },
    {
      "epoch": 0.8346666666666667,
      "grad_norm": 0.17353713512420654,
      "learning_rate": 4.4783333333333335e-05,
      "loss": 0.0028,
      "step": 15650
    },
    {
      "epoch": 0.8352,
      "grad_norm": 0.2883906960487366,
      "learning_rate": 4.478e-05,
      "loss": 0.0017,
      "step": 15660
    },
    {
      "epoch": 0.8357333333333333,
      "grad_norm": 0.11556612700223923,
      "learning_rate": 4.477666666666667e-05,
      "loss": 0.0035,
      "step": 15670
    },
    {
      "epoch": 0.8362666666666667,
      "grad_norm": 0.0578199177980423,
      "learning_rate": 4.4773333333333334e-05,
      "loss": 0.0027,
      "step": 15680
    },
    {
      "epoch": 0.8368,
      "grad_norm": 0.0865197628736496,
      "learning_rate": 4.477e-05,
      "loss": 0.0027,
      "step": 15690
    },
    {
      "epoch": 0.8373333333333334,
      "grad_norm": 0.23092518746852875,
      "learning_rate": 4.4766666666666666e-05,
      "loss": 0.0024,
      "step": 15700
    },
    {
      "epoch": 0.8378666666666666,
      "grad_norm": 0.05784960836172104,
      "learning_rate": 4.476333333333333e-05,
      "loss": 0.004,
      "step": 15710
    },
    {
      "epoch": 0.8384,
      "grad_norm": 0.0020779867190867662,
      "learning_rate": 4.4760000000000005e-05,
      "loss": 0.0036,
      "step": 15720
    },
    {
      "epoch": 0.8389333333333333,
      "grad_norm": 0.5477285385131836,
      "learning_rate": 4.475666666666667e-05,
      "loss": 0.0038,
      "step": 15730
    },
    {
      "epoch": 0.8394666666666667,
      "grad_norm": 0.4032348096370697,
      "learning_rate": 4.475333333333334e-05,
      "loss": 0.003,
      "step": 15740
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.2888164520263672,
      "learning_rate": 4.4750000000000004e-05,
      "loss": 0.0038,
      "step": 15750
    },
    {
      "epoch": 0.8405333333333334,
      "grad_norm": 0.0864962562918663,
      "learning_rate": 4.474666666666667e-05,
      "loss": 0.0049,
      "step": 15760
    },
    {
      "epoch": 0.8410666666666666,
      "grad_norm": 0.14437681436538696,
      "learning_rate": 4.4743333333333336e-05,
      "loss": 0.0049,
      "step": 15770
    },
    {
      "epoch": 0.8416,
      "grad_norm": 0.5197373032569885,
      "learning_rate": 4.474e-05,
      "loss": 0.0044,
      "step": 15780
    },
    {
      "epoch": 0.8421333333333333,
      "grad_norm": 0.25954410433769226,
      "learning_rate": 4.473666666666667e-05,
      "loss": 0.0058,
      "step": 15790
    },
    {
      "epoch": 0.8426666666666667,
      "grad_norm": 0.4319060742855072,
      "learning_rate": 4.473333333333334e-05,
      "loss": 0.0044,
      "step": 15800
    },
    {
      "epoch": 0.8432,
      "grad_norm": 0.11523457616567612,
      "learning_rate": 4.473e-05,
      "loss": 0.0057,
      "step": 15810
    },
    {
      "epoch": 0.8437333333333333,
      "grad_norm": 0.4320993423461914,
      "learning_rate": 4.4726666666666666e-05,
      "loss": 0.0041,
      "step": 15820
    },
    {
      "epoch": 0.8442666666666667,
      "grad_norm": 0.0866880863904953,
      "learning_rate": 4.472333333333333e-05,
      "loss": 0.0036,
      "step": 15830
    },
    {
      "epoch": 0.8448,
      "grad_norm": 0.08645398914813995,
      "learning_rate": 4.472e-05,
      "loss": 0.0044,
      "step": 15840
    },
    {
      "epoch": 0.8453333333333334,
      "grad_norm": 0.05783671513199806,
      "learning_rate": 4.4716666666666665e-05,
      "loss": 0.0048,
      "step": 15850
    },
    {
      "epoch": 0.8458666666666667,
      "grad_norm": 0.14404729008674622,
      "learning_rate": 4.471333333333334e-05,
      "loss": 0.0027,
      "step": 15860
    },
    {
      "epoch": 0.8464,
      "grad_norm": 0.08674635738134384,
      "learning_rate": 4.4710000000000004e-05,
      "loss": 0.0033,
      "step": 15870
    },
    {
      "epoch": 0.8469333333333333,
      "grad_norm": 0.2594704031944275,
      "learning_rate": 4.470666666666667e-05,
      "loss": 0.0056,
      "step": 15880
    },
    {
      "epoch": 0.8474666666666667,
      "grad_norm": 0.4607125520706177,
      "learning_rate": 4.4703333333333336e-05,
      "loss": 0.0043,
      "step": 15890
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.17276151478290558,
      "learning_rate": 4.47e-05,
      "loss": 0.0045,
      "step": 15900
    },
    {
      "epoch": 0.8485333333333334,
      "grad_norm": 0.34581324458122253,
      "learning_rate": 4.469666666666667e-05,
      "loss": 0.0038,
      "step": 15910
    },
    {
      "epoch": 0.8490666666666666,
      "grad_norm": 0.3171476125717163,
      "learning_rate": 4.4693333333333335e-05,
      "loss": 0.0033,
      "step": 15920
    },
    {
      "epoch": 0.8496,
      "grad_norm": 0.23062071204185486,
      "learning_rate": 4.469e-05,
      "loss": 0.0027,
      "step": 15930
    },
    {
      "epoch": 0.8501333333333333,
      "grad_norm": 0.17329086363315582,
      "learning_rate": 4.4686666666666674e-05,
      "loss": 0.0072,
      "step": 15940
    },
    {
      "epoch": 0.8506666666666667,
      "grad_norm": 0.28843143582344055,
      "learning_rate": 4.468333333333334e-05,
      "loss": 0.0039,
      "step": 15950
    },
    {
      "epoch": 0.8512,
      "grad_norm": 1.9024029970169067,
      "learning_rate": 4.468e-05,
      "loss": 0.0041,
      "step": 15960
    },
    {
      "epoch": 0.8517333333333333,
      "grad_norm": 0.3176185190677643,
      "learning_rate": 4.4676666666666665e-05,
      "loss": 0.0031,
      "step": 15970
    },
    {
      "epoch": 0.8522666666666666,
      "grad_norm": 0.1441352814435959,
      "learning_rate": 4.467333333333333e-05,
      "loss": 0.0022,
      "step": 15980
    },
    {
      "epoch": 0.8528,
      "grad_norm": 0.11544907838106155,
      "learning_rate": 4.467e-05,
      "loss": 0.0041,
      "step": 15990
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.2017902433872223,
      "learning_rate": 4.466666666666667e-05,
      "loss": 0.0045,
      "step": 16000
    },
    {
      "epoch": 0.8538666666666667,
      "grad_norm": 0.37543123960494995,
      "learning_rate": 4.4663333333333337e-05,
      "loss": 0.0041,
      "step": 16010
    },
    {
      "epoch": 0.8544,
      "grad_norm": 0.43245700001716614,
      "learning_rate": 4.466e-05,
      "loss": 0.0042,
      "step": 16020
    },
    {
      "epoch": 0.8549333333333333,
      "grad_norm": 0.05771792680025101,
      "learning_rate": 4.465666666666667e-05,
      "loss": 0.0037,
      "step": 16030
    },
    {
      "epoch": 0.8554666666666667,
      "grad_norm": 0.057856932282447815,
      "learning_rate": 4.4653333333333335e-05,
      "loss": 0.0027,
      "step": 16040
    },
    {
      "epoch": 0.856,
      "grad_norm": 0.058682605624198914,
      "learning_rate": 4.465e-05,
      "loss": 0.0041,
      "step": 16050
    },
    {
      "epoch": 0.8565333333333334,
      "grad_norm": 0.05805693939328194,
      "learning_rate": 4.464666666666667e-05,
      "loss": 0.0026,
      "step": 16060
    },
    {
      "epoch": 0.8570666666666666,
      "grad_norm": 0.028998717665672302,
      "learning_rate": 4.464333333333334e-05,
      "loss": 0.0028,
      "step": 16070
    },
    {
      "epoch": 0.8576,
      "grad_norm": 0.08644641935825348,
      "learning_rate": 4.4640000000000006e-05,
      "loss": 0.0022,
      "step": 16080
    },
    {
      "epoch": 0.8581333333333333,
      "grad_norm": 0.48969772458076477,
      "learning_rate": 4.463666666666667e-05,
      "loss": 0.0032,
      "step": 16090
    },
    {
      "epoch": 0.8586666666666667,
      "grad_norm": 0.7634400129318237,
      "learning_rate": 4.463333333333334e-05,
      "loss": 0.0027,
      "step": 16100
    },
    {
      "epoch": 0.8592,
      "grad_norm": 0.31714117527008057,
      "learning_rate": 4.463e-05,
      "loss": 0.0033,
      "step": 16110
    },
    {
      "epoch": 0.8597333333333333,
      "grad_norm": 0.31709063053131104,
      "learning_rate": 4.4626666666666664e-05,
      "loss": 0.0075,
      "step": 16120
    },
    {
      "epoch": 0.8602666666666666,
      "grad_norm": 0.3465222716331482,
      "learning_rate": 4.462333333333334e-05,
      "loss": 0.0058,
      "step": 16130
    },
    {
      "epoch": 0.8608,
      "grad_norm": 0.43349549174308777,
      "learning_rate": 4.462e-05,
      "loss": 0.0031,
      "step": 16140
    },
    {
      "epoch": 0.8613333333333333,
      "grad_norm": 0.17336232960224152,
      "learning_rate": 4.461666666666667e-05,
      "loss": 0.0037,
      "step": 16150
    },
    {
      "epoch": 0.8618666666666667,
      "grad_norm": 0.2888627350330353,
      "learning_rate": 4.4613333333333335e-05,
      "loss": 0.0032,
      "step": 16160
    },
    {
      "epoch": 0.8624,
      "grad_norm": 0.2602558135986328,
      "learning_rate": 4.461e-05,
      "loss": 0.0044,
      "step": 16170
    },
    {
      "epoch": 0.8629333333333333,
      "grad_norm": 0.20222589373588562,
      "learning_rate": 4.460666666666667e-05,
      "loss": 0.0042,
      "step": 16180
    },
    {
      "epoch": 0.8634666666666667,
      "grad_norm": 0.4047797620296478,
      "learning_rate": 4.4603333333333334e-05,
      "loss": 0.0036,
      "step": 16190
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.20202572643756866,
      "learning_rate": 4.46e-05,
      "loss": 0.0036,
      "step": 16200
    },
    {
      "epoch": 0.8645333333333334,
      "grad_norm": 0.2025647908449173,
      "learning_rate": 4.459666666666667e-05,
      "loss": 0.0034,
      "step": 16210
    },
    {
      "epoch": 0.8650666666666667,
      "grad_norm": 0.3176129460334778,
      "learning_rate": 4.459333333333334e-05,
      "loss": 0.0034,
      "step": 16220
    },
    {
      "epoch": 0.8656,
      "grad_norm": 0.057860102504491806,
      "learning_rate": 4.4590000000000005e-05,
      "loss": 0.0025,
      "step": 16230
    },
    {
      "epoch": 0.8661333333333333,
      "grad_norm": 0.11547563970088959,
      "learning_rate": 4.458666666666667e-05,
      "loss": 0.0044,
      "step": 16240
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.3464169204235077,
      "learning_rate": 4.458333333333334e-05,
      "loss": 0.0023,
      "step": 16250
    },
    {
      "epoch": 0.8672,
      "grad_norm": 0.1442137211561203,
      "learning_rate": 4.458e-05,
      "loss": 0.0047,
      "step": 16260
    },
    {
      "epoch": 0.8677333333333334,
      "grad_norm": 0.05764785408973694,
      "learning_rate": 4.457666666666667e-05,
      "loss": 0.0039,
      "step": 16270
    },
    {
      "epoch": 0.8682666666666666,
      "grad_norm": 0.20220234990119934,
      "learning_rate": 4.4573333333333336e-05,
      "loss": 0.0036,
      "step": 16280
    },
    {
      "epoch": 0.8688,
      "grad_norm": 0.029120730236172676,
      "learning_rate": 4.457e-05,
      "loss": 0.0032,
      "step": 16290
    },
    {
      "epoch": 0.8693333333333333,
      "grad_norm": 0.0024109298828989267,
      "learning_rate": 4.456666666666667e-05,
      "loss": 0.0033,
      "step": 16300
    },
    {
      "epoch": 0.8698666666666667,
      "grad_norm": 0.11573771387338638,
      "learning_rate": 4.4563333333333334e-05,
      "loss": 0.0041,
      "step": 16310
    },
    {
      "epoch": 0.8704,
      "grad_norm": 0.20283353328704834,
      "learning_rate": 4.456e-05,
      "loss": 0.0037,
      "step": 16320
    },
    {
      "epoch": 0.8709333333333333,
      "grad_norm": 0.11589525640010834,
      "learning_rate": 4.4556666666666666e-05,
      "loss": 0.0047,
      "step": 16330
    },
    {
      "epoch": 0.8714666666666666,
      "grad_norm": 0.08673673868179321,
      "learning_rate": 4.455333333333333e-05,
      "loss": 0.0039,
      "step": 16340
    },
    {
      "epoch": 0.872,
      "grad_norm": 0.1442813128232956,
      "learning_rate": 4.4550000000000005e-05,
      "loss": 0.0043,
      "step": 16350
    },
    {
      "epoch": 0.8725333333333334,
      "grad_norm": 0.028859447687864304,
      "learning_rate": 4.454666666666667e-05,
      "loss": 0.0035,
      "step": 16360
    },
    {
      "epoch": 0.8730666666666667,
      "grad_norm": 0.028855521231889725,
      "learning_rate": 4.454333333333334e-05,
      "loss": 0.0027,
      "step": 16370
    },
    {
      "epoch": 0.8736,
      "grad_norm": 0.40443170070648193,
      "learning_rate": 4.4540000000000004e-05,
      "loss": 0.0027,
      "step": 16380
    },
    {
      "epoch": 0.8741333333333333,
      "grad_norm": 0.029090523719787598,
      "learning_rate": 4.453666666666667e-05,
      "loss": 0.0034,
      "step": 16390
    },
    {
      "epoch": 0.8746666666666667,
      "grad_norm": 0.2888864576816559,
      "learning_rate": 4.4533333333333336e-05,
      "loss": 0.0023,
      "step": 16400
    },
    {
      "epoch": 0.8752,
      "grad_norm": 0.029011482372879982,
      "learning_rate": 4.453e-05,
      "loss": 0.0036,
      "step": 16410
    },
    {
      "epoch": 0.8757333333333334,
      "grad_norm": 0.2597711682319641,
      "learning_rate": 4.452666666666667e-05,
      "loss": 0.004,
      "step": 16420
    },
    {
      "epoch": 0.8762666666666666,
      "grad_norm": 0.2884957194328308,
      "learning_rate": 4.4523333333333335e-05,
      "loss": 0.0041,
      "step": 16430
    },
    {
      "epoch": 0.8768,
      "grad_norm": 0.1444830745458603,
      "learning_rate": 4.452e-05,
      "loss": 0.0033,
      "step": 16440
    },
    {
      "epoch": 0.8773333333333333,
      "grad_norm": 0.28838592767715454,
      "learning_rate": 4.451666666666667e-05,
      "loss": 0.0039,
      "step": 16450
    },
    {
      "epoch": 0.8778666666666667,
      "grad_norm": 0.6918784379959106,
      "learning_rate": 4.451333333333333e-05,
      "loss": 0.0031,
      "step": 16460
    },
    {
      "epoch": 0.8784,
      "grad_norm": 0.3453933596611023,
      "learning_rate": 4.451e-05,
      "loss": 0.0032,
      "step": 16470
    },
    {
      "epoch": 0.8789333333333333,
      "grad_norm": 0.028895659372210503,
      "learning_rate": 4.450666666666667e-05,
      "loss": 0.0041,
      "step": 16480
    },
    {
      "epoch": 0.8794666666666666,
      "grad_norm": 0.2307535856962204,
      "learning_rate": 4.450333333333334e-05,
      "loss": 0.0026,
      "step": 16490
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.05777527019381523,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 0.0031,
      "step": 16500
    },
    {
      "epoch": 0.8805333333333333,
      "grad_norm": 0.14454874396324158,
      "learning_rate": 4.449666666666667e-05,
      "loss": 0.0048,
      "step": 16510
    },
    {
      "epoch": 0.8810666666666667,
      "grad_norm": 0.37578800320625305,
      "learning_rate": 4.4493333333333337e-05,
      "loss": 0.0041,
      "step": 16520
    },
    {
      "epoch": 0.8816,
      "grad_norm": 0.25989100337028503,
      "learning_rate": 4.449e-05,
      "loss": 0.0017,
      "step": 16530
    },
    {
      "epoch": 0.8821333333333333,
      "grad_norm": 0.11571286618709564,
      "learning_rate": 4.448666666666667e-05,
      "loss": 0.0028,
      "step": 16540
    },
    {
      "epoch": 0.8826666666666667,
      "grad_norm": 0.2597399353981018,
      "learning_rate": 4.4483333333333335e-05,
      "loss": 0.0041,
      "step": 16550
    },
    {
      "epoch": 0.8832,
      "grad_norm": 0.057693347334861755,
      "learning_rate": 4.448e-05,
      "loss": 0.003,
      "step": 16560
    },
    {
      "epoch": 0.8837333333333334,
      "grad_norm": 0.0015211583813652396,
      "learning_rate": 4.447666666666667e-05,
      "loss": 0.0029,
      "step": 16570
    },
    {
      "epoch": 0.8842666666666666,
      "grad_norm": 0.3462716042995453,
      "learning_rate": 4.447333333333333e-05,
      "loss": 0.0023,
      "step": 16580
    },
    {
      "epoch": 0.8848,
      "grad_norm": 0.057802025228738785,
      "learning_rate": 4.447e-05,
      "loss": 0.0034,
      "step": 16590
    },
    {
      "epoch": 0.8853333333333333,
      "grad_norm": 0.25960248708724976,
      "learning_rate": 4.4466666666666666e-05,
      "loss": 0.0028,
      "step": 16600
    },
    {
      "epoch": 0.8858666666666667,
      "grad_norm": 0.11512439697980881,
      "learning_rate": 4.446333333333333e-05,
      "loss": 0.0034,
      "step": 16610
    },
    {
      "epoch": 0.8864,
      "grad_norm": 2.3203125,
      "learning_rate": 4.4460000000000005e-05,
      "loss": 0.0059,
      "step": 16620
    },
    {
      "epoch": 0.8869333333333334,
      "grad_norm": 0.8356680274009705,
      "learning_rate": 4.445666666666667e-05,
      "loss": 0.0047,
      "step": 16630
    },
    {
      "epoch": 0.8874666666666666,
      "grad_norm": 0.17317020893096924,
      "learning_rate": 4.445333333333334e-05,
      "loss": 0.0027,
      "step": 16640
    },
    {
      "epoch": 0.888,
      "grad_norm": 0.11584239453077316,
      "learning_rate": 4.445e-05,
      "loss": 0.0042,
      "step": 16650
    },
    {
      "epoch": 0.8885333333333333,
      "grad_norm": 0.17338283360004425,
      "learning_rate": 4.444666666666667e-05,
      "loss": 0.0033,
      "step": 16660
    },
    {
      "epoch": 0.8890666666666667,
      "grad_norm": 0.05791642516851425,
      "learning_rate": 4.4443333333333335e-05,
      "loss": 0.0035,
      "step": 16670
    },
    {
      "epoch": 0.8896,
      "grad_norm": 0.3464987277984619,
      "learning_rate": 4.444e-05,
      "loss": 0.0037,
      "step": 16680
    },
    {
      "epoch": 0.8901333333333333,
      "grad_norm": 0.17412708699703217,
      "learning_rate": 4.443666666666667e-05,
      "loss": 0.004,
      "step": 16690
    },
    {
      "epoch": 0.8906666666666667,
      "grad_norm": 0.2030164748430252,
      "learning_rate": 4.443333333333334e-05,
      "loss": 0.004,
      "step": 16700
    },
    {
      "epoch": 0.8912,
      "grad_norm": 0.057918354868888855,
      "learning_rate": 4.443e-05,
      "loss": 0.0048,
      "step": 16710
    },
    {
      "epoch": 0.8917333333333334,
      "grad_norm": 0.3471411168575287,
      "learning_rate": 4.4426666666666666e-05,
      "loss": 0.0031,
      "step": 16720
    },
    {
      "epoch": 0.8922666666666667,
      "grad_norm": 0.6079724431037903,
      "learning_rate": 4.442333333333333e-05,
      "loss": 0.0032,
      "step": 16730
    },
    {
      "epoch": 0.8928,
      "grad_norm": 0.11604917794466019,
      "learning_rate": 4.442e-05,
      "loss": 0.0061,
      "step": 16740
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.14473986625671387,
      "learning_rate": 4.4416666666666664e-05,
      "loss": 0.0032,
      "step": 16750
    },
    {
      "epoch": 0.8938666666666667,
      "grad_norm": 0.11544779688119888,
      "learning_rate": 4.441333333333334e-05,
      "loss": 0.0051,
      "step": 16760
    },
    {
      "epoch": 0.8944,
      "grad_norm": 0.14435258507728577,
      "learning_rate": 4.4410000000000003e-05,
      "loss": 0.0021,
      "step": 16770
    },
    {
      "epoch": 0.8949333333333334,
      "grad_norm": 0.34722116589546204,
      "learning_rate": 4.440666666666667e-05,
      "loss": 0.0027,
      "step": 16780
    },
    {
      "epoch": 0.8954666666666666,
      "grad_norm": 0.0577499158680439,
      "learning_rate": 4.4403333333333336e-05,
      "loss": 0.0031,
      "step": 16790
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.08667347580194473,
      "learning_rate": 4.44e-05,
      "loss": 0.0029,
      "step": 16800
    },
    {
      "epoch": 0.8965333333333333,
      "grad_norm": 0.3180169463157654,
      "learning_rate": 4.439666666666667e-05,
      "loss": 0.0028,
      "step": 16810
    },
    {
      "epoch": 0.8970666666666667,
      "grad_norm": 0.20259711146354675,
      "learning_rate": 4.4393333333333334e-05,
      "loss": 0.0029,
      "step": 16820
    },
    {
      "epoch": 0.8976,
      "grad_norm": 0.0018661359790712595,
      "learning_rate": 4.439000000000001e-05,
      "loss": 0.0033,
      "step": 16830
    },
    {
      "epoch": 0.8981333333333333,
      "grad_norm": 0.11535229533910751,
      "learning_rate": 4.438666666666667e-05,
      "loss": 0.003,
      "step": 16840
    },
    {
      "epoch": 0.8986666666666666,
      "grad_norm": 0.14473922550678253,
      "learning_rate": 4.438333333333334e-05,
      "loss": 0.0037,
      "step": 16850
    },
    {
      "epoch": 0.8992,
      "grad_norm": 0.3751283288002014,
      "learning_rate": 4.438e-05,
      "loss": 0.0039,
      "step": 16860
    },
    {
      "epoch": 0.8997333333333334,
      "grad_norm": 0.2886907160282135,
      "learning_rate": 4.4376666666666665e-05,
      "loss": 0.0036,
      "step": 16870
    },
    {
      "epoch": 0.9002666666666667,
      "grad_norm": 0.1154385581612587,
      "learning_rate": 4.437333333333333e-05,
      "loss": 0.0025,
      "step": 16880
    },
    {
      "epoch": 0.9008,
      "grad_norm": 0.4617859423160553,
      "learning_rate": 4.4370000000000004e-05,
      "loss": 0.003,
      "step": 16890
    },
    {
      "epoch": 0.9013333333333333,
      "grad_norm": 0.14415891468524933,
      "learning_rate": 4.436666666666667e-05,
      "loss": 0.0032,
      "step": 16900
    },
    {
      "epoch": 0.9018666666666667,
      "grad_norm": 0.46155470609664917,
      "learning_rate": 4.4363333333333336e-05,
      "loss": 0.0036,
      "step": 16910
    },
    {
      "epoch": 0.9024,
      "grad_norm": 0.3747219145298004,
      "learning_rate": 4.436e-05,
      "loss": 0.0037,
      "step": 16920
    },
    {
      "epoch": 0.9029333333333334,
      "grad_norm": 0.4321712851524353,
      "learning_rate": 4.435666666666667e-05,
      "loss": 0.0041,
      "step": 16930
    },
    {
      "epoch": 0.9034666666666666,
      "grad_norm": 0.08643915504217148,
      "learning_rate": 4.4353333333333334e-05,
      "loss": 0.0036,
      "step": 16940
    },
    {
      "epoch": 0.904,
      "grad_norm": 0.05765847861766815,
      "learning_rate": 4.435e-05,
      "loss": 0.0027,
      "step": 16950
    },
    {
      "epoch": 0.9045333333333333,
      "grad_norm": 0.02897815592586994,
      "learning_rate": 4.434666666666667e-05,
      "loss": 0.0033,
      "step": 16960
    },
    {
      "epoch": 0.9050666666666667,
      "grad_norm": 0.029046300798654556,
      "learning_rate": 4.434333333333334e-05,
      "loss": 0.0039,
      "step": 16970
    },
    {
      "epoch": 0.9056,
      "grad_norm": 0.2305537611246109,
      "learning_rate": 4.4340000000000006e-05,
      "loss": 0.0037,
      "step": 16980
    },
    {
      "epoch": 0.9061333333333333,
      "grad_norm": 0.3747268319129944,
      "learning_rate": 4.433666666666667e-05,
      "loss": 0.0025,
      "step": 16990
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.028936278074979782,
      "learning_rate": 4.433333333333334e-05,
      "loss": 0.0032,
      "step": 17000
    },
    {
      "epoch": 0.9072,
      "grad_norm": 0.17276203632354736,
      "learning_rate": 4.4330000000000004e-05,
      "loss": 0.0029,
      "step": 17010
    },
    {
      "epoch": 0.9077333333333333,
      "grad_norm": 0.2306661307811737,
      "learning_rate": 4.4326666666666664e-05,
      "loss": 0.0032,
      "step": 17020
    },
    {
      "epoch": 0.9082666666666667,
      "grad_norm": 0.2017468363046646,
      "learning_rate": 4.4323333333333336e-05,
      "loss": 0.0032,
      "step": 17030
    },
    {
      "epoch": 0.9088,
      "grad_norm": 0.028827715665102005,
      "learning_rate": 4.432e-05,
      "loss": 0.004,
      "step": 17040
    },
    {
      "epoch": 0.9093333333333333,
      "grad_norm": 0.5766363739967346,
      "learning_rate": 4.431666666666667e-05,
      "loss": 0.0023,
      "step": 17050
    },
    {
      "epoch": 0.9098666666666667,
      "grad_norm": 0.2596230208873749,
      "learning_rate": 4.4313333333333335e-05,
      "loss": 0.0027,
      "step": 17060
    },
    {
      "epoch": 0.9104,
      "grad_norm": 0.2019806057214737,
      "learning_rate": 4.431e-05,
      "loss": 0.0026,
      "step": 17070
    },
    {
      "epoch": 0.9109333333333334,
      "grad_norm": 0.0864473432302475,
      "learning_rate": 4.430666666666667e-05,
      "loss": 0.0036,
      "step": 17080
    },
    {
      "epoch": 0.9114666666666666,
      "grad_norm": 0.17312419414520264,
      "learning_rate": 4.430333333333333e-05,
      "loss": 0.0037,
      "step": 17090
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.3170776665210724,
      "learning_rate": 4.43e-05,
      "loss": 0.0051,
      "step": 17100
    },
    {
      "epoch": 0.9125333333333333,
      "grad_norm": 0.20187807083129883,
      "learning_rate": 4.429666666666667e-05,
      "loss": 0.0041,
      "step": 17110
    },
    {
      "epoch": 0.9130666666666667,
      "grad_norm": 0.28823184967041016,
      "learning_rate": 4.429333333333334e-05,
      "loss": 0.0041,
      "step": 17120
    },
    {
      "epoch": 0.9136,
      "grad_norm": 0.3459104001522064,
      "learning_rate": 4.4290000000000005e-05,
      "loss": 0.0046,
      "step": 17130
    },
    {
      "epoch": 0.9141333333333334,
      "grad_norm": 0.5190069079399109,
      "learning_rate": 4.428666666666667e-05,
      "loss": 0.0043,
      "step": 17140
    },
    {
      "epoch": 0.9146666666666666,
      "grad_norm": 0.057672951370477676,
      "learning_rate": 4.428333333333334e-05,
      "loss": 0.0025,
      "step": 17150
    },
    {
      "epoch": 0.9152,
      "grad_norm": 0.28830859065055847,
      "learning_rate": 4.428e-05,
      "loss": 0.0043,
      "step": 17160
    },
    {
      "epoch": 0.9157333333333333,
      "grad_norm": 0.17313584685325623,
      "learning_rate": 4.427666666666667e-05,
      "loss": 0.0025,
      "step": 17170
    },
    {
      "epoch": 0.9162666666666667,
      "grad_norm": 0.28833824396133423,
      "learning_rate": 4.4273333333333335e-05,
      "loss": 0.0041,
      "step": 17180
    },
    {
      "epoch": 0.9168,
      "grad_norm": 0.001478622667491436,
      "learning_rate": 4.427e-05,
      "loss": 0.0026,
      "step": 17190
    },
    {
      "epoch": 0.9173333333333333,
      "grad_norm": 0.25961700081825256,
      "learning_rate": 4.426666666666667e-05,
      "loss": 0.0027,
      "step": 17200
    },
    {
      "epoch": 0.9178666666666667,
      "grad_norm": 0.057884953916072845,
      "learning_rate": 4.4263333333333334e-05,
      "loss": 0.0025,
      "step": 17210
    },
    {
      "epoch": 0.9184,
      "grad_norm": 0.02883979305624962,
      "learning_rate": 4.426e-05,
      "loss": 0.0044,
      "step": 17220
    },
    {
      "epoch": 0.9189333333333334,
      "grad_norm": 2.3714725971221924,
      "learning_rate": 4.4256666666666666e-05,
      "loss": 0.0039,
      "step": 17230
    },
    {
      "epoch": 0.9194666666666667,
      "grad_norm": 0.11563236266374588,
      "learning_rate": 4.425333333333334e-05,
      "loss": 0.0025,
      "step": 17240
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.5187283158302307,
      "learning_rate": 4.4250000000000005e-05,
      "loss": 0.0055,
      "step": 17250
    },
    {
      "epoch": 0.9205333333333333,
      "grad_norm": 0.1439570039510727,
      "learning_rate": 4.424666666666667e-05,
      "loss": 0.0049,
      "step": 17260
    },
    {
      "epoch": 0.9210666666666667,
      "grad_norm": 0.17275774478912354,
      "learning_rate": 4.424333333333334e-05,
      "loss": 0.003,
      "step": 17270
    },
    {
      "epoch": 0.9216,
      "grad_norm": 0.37456023693084717,
      "learning_rate": 4.424e-05,
      "loss": 0.0039,
      "step": 17280
    },
    {
      "epoch": 0.9221333333333334,
      "grad_norm": 0.11536277085542679,
      "learning_rate": 4.423666666666667e-05,
      "loss": 0.0046,
      "step": 17290
    },
    {
      "epoch": 0.9226666666666666,
      "grad_norm": 0.004631761461496353,
      "learning_rate": 4.4233333333333336e-05,
      "loss": 0.0038,
      "step": 17300
    },
    {
      "epoch": 0.9232,
      "grad_norm": 0.3171129822731018,
      "learning_rate": 4.423e-05,
      "loss": 0.0045,
      "step": 17310
    },
    {
      "epoch": 0.9237333333333333,
      "grad_norm": 0.08690648525953293,
      "learning_rate": 4.422666666666667e-05,
      "loss": 0.0029,
      "step": 17320
    },
    {
      "epoch": 0.9242666666666667,
      "grad_norm": 0.25935664772987366,
      "learning_rate": 4.4223333333333334e-05,
      "loss": 0.0032,
      "step": 17330
    },
    {
      "epoch": 0.9248,
      "grad_norm": 0.17335018515586853,
      "learning_rate": 4.422e-05,
      "loss": 0.0055,
      "step": 17340
    },
    {
      "epoch": 0.9253333333333333,
      "grad_norm": 0.08658603578805923,
      "learning_rate": 4.4216666666666666e-05,
      "loss": 0.0028,
      "step": 17350
    },
    {
      "epoch": 0.9258666666666666,
      "grad_norm": 0.2016679346561432,
      "learning_rate": 4.421333333333333e-05,
      "loss": 0.0046,
      "step": 17360
    },
    {
      "epoch": 0.9264,
      "grad_norm": 0.317886084318161,
      "learning_rate": 4.421e-05,
      "loss": 0.0024,
      "step": 17370
    },
    {
      "epoch": 0.9269333333333334,
      "grad_norm": 0.779841423034668,
      "learning_rate": 4.420666666666667e-05,
      "loss": 0.0038,
      "step": 17380
    },
    {
      "epoch": 0.9274666666666667,
      "grad_norm": 0.20299771428108215,
      "learning_rate": 4.420333333333334e-05,
      "loss": 0.0032,
      "step": 17390
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.23269155621528625,
      "learning_rate": 4.4200000000000004e-05,
      "loss": 0.0038,
      "step": 17400
    },
    {
      "epoch": 0.9285333333333333,
      "grad_norm": 0.23388458788394928,
      "learning_rate": 4.419666666666667e-05,
      "loss": 0.0032,
      "step": 17410
    },
    {
      "epoch": 0.9290666666666667,
      "grad_norm": 0.11651144176721573,
      "learning_rate": 4.4193333333333336e-05,
      "loss": 0.0049,
      "step": 17420
    },
    {
      "epoch": 0.9296,
      "grad_norm": 0.2037540078163147,
      "learning_rate": 4.419e-05,
      "loss": 0.0028,
      "step": 17430
    },
    {
      "epoch": 0.9301333333333334,
      "grad_norm": 0.3205212652683258,
      "learning_rate": 4.418666666666667e-05,
      "loss": 0.0043,
      "step": 17440
    },
    {
      "epoch": 0.9306666666666666,
      "grad_norm": 0.4046279788017273,
      "learning_rate": 4.4183333333333334e-05,
      "loss": 0.0029,
      "step": 17450
    },
    {
      "epoch": 0.9312,
      "grad_norm": 0.1746850609779358,
      "learning_rate": 4.418000000000001e-05,
      "loss": 0.0034,
      "step": 17460
    },
    {
      "epoch": 0.9317333333333333,
      "grad_norm": 0.05959611013531685,
      "learning_rate": 4.417666666666667e-05,
      "loss": 0.0051,
      "step": 17470
    },
    {
      "epoch": 0.9322666666666667,
      "grad_norm": 0.5200554132461548,
      "learning_rate": 4.417333333333333e-05,
      "loss": 0.0038,
      "step": 17480
    },
    {
      "epoch": 0.9328,
      "grad_norm": 0.058234721422195435,
      "learning_rate": 4.417e-05,
      "loss": 0.004,
      "step": 17490
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.20220538973808289,
      "learning_rate": 4.4166666666666665e-05,
      "loss": 0.0035,
      "step": 17500
    },
    {
      "epoch": 0.9338666666666666,
      "grad_norm": 0.011040883138775826,
      "learning_rate": 4.416333333333333e-05,
      "loss": 0.0038,
      "step": 17510
    },
    {
      "epoch": 0.9344,
      "grad_norm": 0.2108345925807953,
      "learning_rate": 4.4160000000000004e-05,
      "loss": 0.0046,
      "step": 17520
    },
    {
      "epoch": 0.9349333333333333,
      "grad_norm": 0.23147819936275482,
      "learning_rate": 4.415666666666667e-05,
      "loss": 0.0045,
      "step": 17530
    },
    {
      "epoch": 0.9354666666666667,
      "grad_norm": 0.061179134994745255,
      "learning_rate": 4.4153333333333336e-05,
      "loss": 0.0062,
      "step": 17540
    },
    {
      "epoch": 0.936,
      "grad_norm": 0.07408556342124939,
      "learning_rate": 4.415e-05,
      "loss": 0.0038,
      "step": 17550
    },
    {
      "epoch": 0.9365333333333333,
      "grad_norm": 0.37850382924079895,
      "learning_rate": 4.414666666666667e-05,
      "loss": 0.0026,
      "step": 17560
    },
    {
      "epoch": 0.9370666666666667,
      "grad_norm": 0.26171571016311646,
      "learning_rate": 4.4143333333333335e-05,
      "loss": 0.0038,
      "step": 17570
    },
    {
      "epoch": 0.9376,
      "grad_norm": 0.21562060713768005,
      "learning_rate": 4.414e-05,
      "loss": 0.0041,
      "step": 17580
    },
    {
      "epoch": 0.9381333333333334,
      "grad_norm": 0.4069063663482666,
      "learning_rate": 4.4136666666666674e-05,
      "loss": 0.0032,
      "step": 17590
    },
    {
      "epoch": 0.9386666666666666,
      "grad_norm": 0.44289615750312805,
      "learning_rate": 4.413333333333334e-05,
      "loss": 0.0037,
      "step": 17600
    },
    {
      "epoch": 0.9392,
      "grad_norm": 0.029587557539343834,
      "learning_rate": 4.4130000000000006e-05,
      "loss": 0.0036,
      "step": 17610
    },
    {
      "epoch": 0.9397333333333333,
      "grad_norm": 0.31933173537254333,
      "learning_rate": 4.4126666666666665e-05,
      "loss": 0.0036,
      "step": 17620
    },
    {
      "epoch": 0.9402666666666667,
      "grad_norm": 0.0870388001203537,
      "learning_rate": 4.412333333333333e-05,
      "loss": 0.0041,
      "step": 17630
    },
    {
      "epoch": 0.9408,
      "grad_norm": 0.6203227639198303,
      "learning_rate": 4.412e-05,
      "loss": 0.0026,
      "step": 17640
    },
    {
      "epoch": 0.9413333333333334,
      "grad_norm": 0.3480958044528961,
      "learning_rate": 4.411666666666667e-05,
      "loss": 0.0035,
      "step": 17650
    },
    {
      "epoch": 0.9418666666666666,
      "grad_norm": 0.2781159579753876,
      "learning_rate": 4.411333333333334e-05,
      "loss": 0.0027,
      "step": 17660
    },
    {
      "epoch": 0.9424,
      "grad_norm": 0.3727177083492279,
      "learning_rate": 4.411e-05,
      "loss": 0.004,
      "step": 17670
    },
    {
      "epoch": 0.9429333333333333,
      "grad_norm": 0.31982746720314026,
      "learning_rate": 4.410666666666667e-05,
      "loss": 0.003,
      "step": 17680
    },
    {
      "epoch": 0.9434666666666667,
      "grad_norm": 0.030832266435027122,
      "learning_rate": 4.4103333333333335e-05,
      "loss": 0.0028,
      "step": 17690
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.25520434975624084,
      "learning_rate": 4.41e-05,
      "loss": 0.0027,
      "step": 17700
    },
    {
      "epoch": 0.9445333333333333,
      "grad_norm": 0.1160658746957779,
      "learning_rate": 4.409666666666667e-05,
      "loss": 0.0034,
      "step": 17710
    },
    {
      "epoch": 0.9450666666666667,
      "grad_norm": 0.2612244784832001,
      "learning_rate": 4.4093333333333334e-05,
      "loss": 0.0025,
      "step": 17720
    },
    {
      "epoch": 0.9456,
      "grad_norm": 0.23209567368030548,
      "learning_rate": 4.4090000000000006e-05,
      "loss": 0.004,
      "step": 17730
    },
    {
      "epoch": 0.9461333333333334,
      "grad_norm": 0.032531436532735825,
      "learning_rate": 4.408666666666667e-05,
      "loss": 0.0037,
      "step": 17740
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 0.2318568229675293,
      "learning_rate": 4.408333333333334e-05,
      "loss": 0.003,
      "step": 17750
    },
    {
      "epoch": 0.9472,
      "grad_norm": 0.19793517887592316,
      "learning_rate": 4.4080000000000005e-05,
      "loss": 0.0039,
      "step": 17760
    },
    {
      "epoch": 0.9477333333333333,
      "grad_norm": 0.43539661169052124,
      "learning_rate": 4.4076666666666664e-05,
      "loss": 0.0037,
      "step": 17770
    },
    {
      "epoch": 0.9482666666666667,
      "grad_norm": 0.32465365529060364,
      "learning_rate": 4.407333333333333e-05,
      "loss": 0.0022,
      "step": 17780
    },
    {
      "epoch": 0.9488,
      "grad_norm": 0.2704089879989624,
      "learning_rate": 4.407e-05,
      "loss": 0.0031,
      "step": 17790
    },
    {
      "epoch": 0.9493333333333334,
      "grad_norm": 0.34481605887413025,
      "learning_rate": 4.406666666666667e-05,
      "loss": 0.0033,
      "step": 17800
    },
    {
      "epoch": 0.9498666666666666,
      "grad_norm": 0.14663976430892944,
      "learning_rate": 4.4063333333333336e-05,
      "loss": 0.0026,
      "step": 17810
    },
    {
      "epoch": 0.9504,
      "grad_norm": 0.08909384161233902,
      "learning_rate": 4.406e-05,
      "loss": 0.0028,
      "step": 17820
    },
    {
      "epoch": 0.9509333333333333,
      "grad_norm": 0.1755029857158661,
      "learning_rate": 4.405666666666667e-05,
      "loss": 0.0023,
      "step": 17830
    },
    {
      "epoch": 0.9514666666666667,
      "grad_norm": 0.059065643697977066,
      "learning_rate": 4.4053333333333334e-05,
      "loss": 0.0029,
      "step": 17840
    },
    {
      "epoch": 0.952,
      "grad_norm": 0.4347788393497467,
      "learning_rate": 4.405e-05,
      "loss": 0.0037,
      "step": 17850
    },
    {
      "epoch": 0.9525333333333333,
      "grad_norm": 0.08649275451898575,
      "learning_rate": 4.4046666666666666e-05,
      "loss": 0.0028,
      "step": 17860
    },
    {
      "epoch": 0.9530666666666666,
      "grad_norm": 0.28979790210723877,
      "learning_rate": 4.404333333333334e-05,
      "loss": 0.0031,
      "step": 17870
    },
    {
      "epoch": 0.9536,
      "grad_norm": 0.17357994616031647,
      "learning_rate": 4.4040000000000005e-05,
      "loss": 0.0031,
      "step": 17880
    },
    {
      "epoch": 0.9541333333333334,
      "grad_norm": 0.6397886872291565,
      "learning_rate": 4.403666666666667e-05,
      "loss": 0.005,
      "step": 17890
    },
    {
      "epoch": 0.9546666666666667,
      "grad_norm": 0.3768976926803589,
      "learning_rate": 4.403333333333334e-05,
      "loss": 0.0045,
      "step": 17900
    },
    {
      "epoch": 0.9552,
      "grad_norm": 0.08685200661420822,
      "learning_rate": 4.4030000000000004e-05,
      "loss": 0.0037,
      "step": 17910
    },
    {
      "epoch": 0.9557333333333333,
      "grad_norm": 0.1726938635110855,
      "learning_rate": 4.402666666666666e-05,
      "loss": 0.0031,
      "step": 17920
    },
    {
      "epoch": 0.9562666666666667,
      "grad_norm": 0.37810033559799194,
      "learning_rate": 4.4023333333333336e-05,
      "loss": 0.0044,
      "step": 17930
    },
    {
      "epoch": 0.9568,
      "grad_norm": 0.5006416440010071,
      "learning_rate": 4.402e-05,
      "loss": 0.0049,
      "step": 17940
    },
    {
      "epoch": 0.9573333333333334,
      "grad_norm": 0.46888983249664307,
      "learning_rate": 4.401666666666667e-05,
      "loss": 0.0019,
      "step": 17950
    },
    {
      "epoch": 0.9578666666666666,
      "grad_norm": 0.17568722367286682,
      "learning_rate": 4.4013333333333334e-05,
      "loss": 0.0029,
      "step": 17960
    },
    {
      "epoch": 0.9584,
      "grad_norm": 0.23466090857982635,
      "learning_rate": 4.401e-05,
      "loss": 0.0029,
      "step": 17970
    },
    {
      "epoch": 0.9589333333333333,
      "grad_norm": 0.00807094108313322,
      "learning_rate": 4.4006666666666667e-05,
      "loss": 0.0035,
      "step": 17980
    },
    {
      "epoch": 0.9594666666666667,
      "grad_norm": 0.20676131546497345,
      "learning_rate": 4.400333333333333e-05,
      "loss": 0.0053,
      "step": 17990
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.030122103169560432,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.0036,
      "step": 18000
    },
    {
      "epoch": 0.9605333333333334,
      "grad_norm": 0.028776206076145172,
      "learning_rate": 4.399666666666667e-05,
      "loss": 0.0024,
      "step": 18010
    },
    {
      "epoch": 0.9610666666666666,
      "grad_norm": 0.5458551645278931,
      "learning_rate": 4.399333333333334e-05,
      "loss": 0.0036,
      "step": 18020
    },
    {
      "epoch": 0.9616,
      "grad_norm": 0.48407819867134094,
      "learning_rate": 4.3990000000000004e-05,
      "loss": 0.0037,
      "step": 18030
    },
    {
      "epoch": 0.9621333333333333,
      "grad_norm": 0.23310382664203644,
      "learning_rate": 4.398666666666667e-05,
      "loss": 0.0026,
      "step": 18040
    },
    {
      "epoch": 0.9626666666666667,
      "grad_norm": 0.05772634595632553,
      "learning_rate": 4.3983333333333336e-05,
      "loss": 0.0025,
      "step": 18050
    },
    {
      "epoch": 0.9632,
      "grad_norm": 0.38597050309181213,
      "learning_rate": 4.398e-05,
      "loss": 0.004,
      "step": 18060
    },
    {
      "epoch": 0.9637333333333333,
      "grad_norm": 0.0294363833963871,
      "learning_rate": 4.397666666666667e-05,
      "loss": 0.0035,
      "step": 18070
    },
    {
      "epoch": 0.9642666666666667,
      "grad_norm": 0.1746130883693695,
      "learning_rate": 4.3973333333333335e-05,
      "loss": 0.0034,
      "step": 18080
    },
    {
      "epoch": 0.9648,
      "grad_norm": 0.321039617061615,
      "learning_rate": 4.397e-05,
      "loss": 0.0035,
      "step": 18090
    },
    {
      "epoch": 0.9653333333333334,
      "grad_norm": 0.14450427889823914,
      "learning_rate": 4.396666666666667e-05,
      "loss": 0.0034,
      "step": 18100
    },
    {
      "epoch": 0.9658666666666667,
      "grad_norm": 0.06047594174742699,
      "learning_rate": 4.396333333333333e-05,
      "loss": 0.0025,
      "step": 18110
    },
    {
      "epoch": 0.9664,
      "grad_norm": 0.2606164216995239,
      "learning_rate": 4.396e-05,
      "loss": 0.0036,
      "step": 18120
    },
    {
      "epoch": 0.9669333333333333,
      "grad_norm": 0.2607182562351227,
      "learning_rate": 4.3956666666666665e-05,
      "loss": 0.0039,
      "step": 18130
    },
    {
      "epoch": 0.9674666666666667,
      "grad_norm": 0.4919246733188629,
      "learning_rate": 4.395333333333334e-05,
      "loss": 0.003,
      "step": 18140
    },
    {
      "epoch": 0.968,
      "grad_norm": 0.1733388751745224,
      "learning_rate": 4.3950000000000004e-05,
      "loss": 0.003,
      "step": 18150
    },
    {
      "epoch": 0.9685333333333334,
      "grad_norm": 0.2705090045928955,
      "learning_rate": 4.394666666666667e-05,
      "loss": 0.0026,
      "step": 18160
    },
    {
      "epoch": 0.9690666666666666,
      "grad_norm": 0.49173012375831604,
      "learning_rate": 4.394333333333334e-05,
      "loss": 0.0025,
      "step": 18170
    },
    {
      "epoch": 0.9696,
      "grad_norm": 0.11682301759719849,
      "learning_rate": 4.394e-05,
      "loss": 0.0038,
      "step": 18180
    },
    {
      "epoch": 0.9701333333333333,
      "grad_norm": 0.060205161571502686,
      "learning_rate": 4.393666666666667e-05,
      "loss": 0.0032,
      "step": 18190
    },
    {
      "epoch": 0.9706666666666667,
      "grad_norm": 0.05832577124238014,
      "learning_rate": 4.3933333333333335e-05,
      "loss": 0.0037,
      "step": 18200
    },
    {
      "epoch": 0.9712,
      "grad_norm": 0.11873514950275421,
      "learning_rate": 4.393e-05,
      "loss": 0.0029,
      "step": 18210
    },
    {
      "epoch": 0.9717333333333333,
      "grad_norm": 0.059046532958745956,
      "learning_rate": 4.3926666666666674e-05,
      "loss": 0.0033,
      "step": 18220
    },
    {
      "epoch": 0.9722666666666666,
      "grad_norm": 0.08708854019641876,
      "learning_rate": 4.3923333333333333e-05,
      "loss": 0.0032,
      "step": 18230
    },
    {
      "epoch": 0.9728,
      "grad_norm": 0.11622397601604462,
      "learning_rate": 4.392e-05,
      "loss": 0.003,
      "step": 18240
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 0.028968632221221924,
      "learning_rate": 4.3916666666666666e-05,
      "loss": 0.0034,
      "step": 18250
    },
    {
      "epoch": 0.9738666666666667,
      "grad_norm": 0.029515359550714493,
      "learning_rate": 4.391333333333333e-05,
      "loss": 0.0022,
      "step": 18260
    },
    {
      "epoch": 0.9744,
      "grad_norm": 0.4060104787349701,
      "learning_rate": 4.391e-05,
      "loss": 0.0041,
      "step": 18270
    },
    {
      "epoch": 0.9749333333333333,
      "grad_norm": 0.4074322581291199,
      "learning_rate": 4.390666666666667e-05,
      "loss": 0.0029,
      "step": 18280
    },
    {
      "epoch": 0.9754666666666667,
      "grad_norm": 0.40778684616088867,
      "learning_rate": 4.390333333333334e-05,
      "loss": 0.0038,
      "step": 18290
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.003791800234466791,
      "learning_rate": 4.39e-05,
      "loss": 0.0047,
      "step": 18300
    },
    {
      "epoch": 0.9765333333333334,
      "grad_norm": 0.030804643407464027,
      "learning_rate": 4.389666666666667e-05,
      "loss": 0.0048,
      "step": 18310
    },
    {
      "epoch": 0.9770666666666666,
      "grad_norm": 0.11610706150531769,
      "learning_rate": 4.3893333333333335e-05,
      "loss": 0.0041,
      "step": 18320
    },
    {
      "epoch": 0.9776,
      "grad_norm": 0.05775853246450424,
      "learning_rate": 4.389e-05,
      "loss": 0.0033,
      "step": 18330
    },
    {
      "epoch": 0.9781333333333333,
      "grad_norm": 0.11503788083791733,
      "learning_rate": 4.388666666666667e-05,
      "loss": 0.003,
      "step": 18340
    },
    {
      "epoch": 0.9786666666666667,
      "grad_norm": 0.02961382269859314,
      "learning_rate": 4.388333333333334e-05,
      "loss": 0.0025,
      "step": 18350
    },
    {
      "epoch": 0.9792,
      "grad_norm": 0.4814220666885376,
      "learning_rate": 4.388000000000001e-05,
      "loss": 0.0028,
      "step": 18360
    },
    {
      "epoch": 0.9797333333333333,
      "grad_norm": 0.08709987998008728,
      "learning_rate": 4.387666666666667e-05,
      "loss": 0.0029,
      "step": 18370
    },
    {
      "epoch": 0.9802666666666666,
      "grad_norm": 0.6369826197624207,
      "learning_rate": 4.387333333333333e-05,
      "loss": 0.0026,
      "step": 18380
    },
    {
      "epoch": 0.9808,
      "grad_norm": 0.005061723757535219,
      "learning_rate": 4.387e-05,
      "loss": 0.0029,
      "step": 18390
    },
    {
      "epoch": 0.9813333333333333,
      "grad_norm": 0.1755487322807312,
      "learning_rate": 4.3866666666666665e-05,
      "loss": 0.0027,
      "step": 18400
    },
    {
      "epoch": 0.9818666666666667,
      "grad_norm": 0.2601531445980072,
      "learning_rate": 4.386333333333333e-05,
      "loss": 0.0029,
      "step": 18410
    },
    {
      "epoch": 0.9824,
      "grad_norm": 0.2895989716053009,
      "learning_rate": 4.3860000000000004e-05,
      "loss": 0.004,
      "step": 18420
    },
    {
      "epoch": 0.9829333333333333,
      "grad_norm": 0.11645959317684174,
      "learning_rate": 4.385666666666667e-05,
      "loss": 0.0022,
      "step": 18430
    },
    {
      "epoch": 0.9834666666666667,
      "grad_norm": 0.0870305523276329,
      "learning_rate": 4.3853333333333336e-05,
      "loss": 0.0038,
      "step": 18440
    },
    {
      "epoch": 0.984,
      "grad_norm": 0.08726923167705536,
      "learning_rate": 4.385e-05,
      "loss": 0.0025,
      "step": 18450
    },
    {
      "epoch": 0.9845333333333334,
      "grad_norm": 0.1449713259935379,
      "learning_rate": 4.384666666666667e-05,
      "loss": 0.0041,
      "step": 18460
    },
    {
      "epoch": 0.9850666666666666,
      "grad_norm": 0.31782156229019165,
      "learning_rate": 4.3843333333333334e-05,
      "loss": 0.0036,
      "step": 18470
    },
    {
      "epoch": 0.9856,
      "grad_norm": 0.14413976669311523,
      "learning_rate": 4.384e-05,
      "loss": 0.0028,
      "step": 18480
    },
    {
      "epoch": 0.9861333333333333,
      "grad_norm": 0.14458921551704407,
      "learning_rate": 4.383666666666667e-05,
      "loss": 0.0033,
      "step": 18490
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.8270143866539001,
      "learning_rate": 4.383333333333334e-05,
      "loss": 0.0017,
      "step": 18500
    },
    {
      "epoch": 0.9872,
      "grad_norm": 0.08772886544466019,
      "learning_rate": 4.3830000000000006e-05,
      "loss": 0.0048,
      "step": 18510
    },
    {
      "epoch": 0.9877333333333334,
      "grad_norm": 0.08924179524183273,
      "learning_rate": 4.382666666666667e-05,
      "loss": 0.0037,
      "step": 18520
    },
    {
      "epoch": 0.9882666666666666,
      "grad_norm": 0.32128965854644775,
      "learning_rate": 4.382333333333333e-05,
      "loss": 0.0032,
      "step": 18530
    },
    {
      "epoch": 0.9888,
      "grad_norm": 0.14414583146572113,
      "learning_rate": 4.382e-05,
      "loss": 0.0033,
      "step": 18540
    },
    {
      "epoch": 0.9893333333333333,
      "grad_norm": 0.02878672257065773,
      "learning_rate": 4.381666666666667e-05,
      "loss": 0.0031,
      "step": 18550
    },
    {
      "epoch": 0.9898666666666667,
      "grad_norm": 0.007997328415513039,
      "learning_rate": 4.3813333333333336e-05,
      "loss": 0.003,
      "step": 18560
    },
    {
      "epoch": 0.9904,
      "grad_norm": 0.32011401653289795,
      "learning_rate": 4.381e-05,
      "loss": 0.0029,
      "step": 18570
    },
    {
      "epoch": 0.9909333333333333,
      "grad_norm": 0.5229014754295349,
      "learning_rate": 4.380666666666667e-05,
      "loss": 0.004,
      "step": 18580
    },
    {
      "epoch": 0.9914666666666667,
      "grad_norm": 0.23378001153469086,
      "learning_rate": 4.3803333333333335e-05,
      "loss": 0.0041,
      "step": 18590
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.12016943842172623,
      "learning_rate": 4.38e-05,
      "loss": 0.0033,
      "step": 18600
    },
    {
      "epoch": 0.9925333333333334,
      "grad_norm": 0.1156257912516594,
      "learning_rate": 4.379666666666667e-05,
      "loss": 0.0026,
      "step": 18610
    },
    {
      "epoch": 0.9930666666666667,
      "grad_norm": 0.20240667462348938,
      "learning_rate": 4.379333333333333e-05,
      "loss": 0.0039,
      "step": 18620
    },
    {
      "epoch": 0.9936,
      "grad_norm": 0.3198835253715515,
      "learning_rate": 4.3790000000000006e-05,
      "loss": 0.0022,
      "step": 18630
    },
    {
      "epoch": 0.9941333333333333,
      "grad_norm": 0.23061124980449677,
      "learning_rate": 4.378666666666667e-05,
      "loss": 0.0029,
      "step": 18640
    },
    {
      "epoch": 0.9946666666666667,
      "grad_norm": 0.23117922246456146,
      "learning_rate": 4.378333333333334e-05,
      "loss": 0.0033,
      "step": 18650
    },
    {
      "epoch": 0.9952,
      "grad_norm": 0.2026589959859848,
      "learning_rate": 4.3780000000000004e-05,
      "loss": 0.0041,
      "step": 18660
    },
    {
      "epoch": 0.9957333333333334,
      "grad_norm": 0.17247647047042847,
      "learning_rate": 4.377666666666667e-05,
      "loss": 0.0044,
      "step": 18670
    },
    {
      "epoch": 0.9962666666666666,
      "grad_norm": 0.2911953628063202,
      "learning_rate": 4.377333333333333e-05,
      "loss": 0.0032,
      "step": 18680
    },
    {
      "epoch": 0.9968,
      "grad_norm": 0.05778910964727402,
      "learning_rate": 4.377e-05,
      "loss": 0.0042,
      "step": 18690
    },
    {
      "epoch": 0.9973333333333333,
      "grad_norm": 0.00410950044170022,
      "learning_rate": 4.376666666666667e-05,
      "loss": 0.0033,
      "step": 18700
    },
    {
      "epoch": 0.9978666666666667,
      "grad_norm": 0.2602742314338684,
      "learning_rate": 4.3763333333333335e-05,
      "loss": 0.0035,
      "step": 18710
    },
    {
      "epoch": 0.9984,
      "grad_norm": 0.0032730011735111475,
      "learning_rate": 4.376e-05,
      "loss": 0.003,
      "step": 18720
    },
    {
      "epoch": 0.9989333333333333,
      "grad_norm": 0.0864851251244545,
      "learning_rate": 4.375666666666667e-05,
      "loss": 0.0047,
      "step": 18730
    },
    {
      "epoch": 0.9994666666666666,
      "grad_norm": 0.5222657322883606,
      "learning_rate": 4.3753333333333333e-05,
      "loss": 0.0035,
      "step": 18740
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.05961570888757706,
      "learning_rate": 4.375e-05,
      "loss": 0.0031,
      "step": 18750
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.004160473123192787,
      "eval_runtime": 169.7276,
      "eval_samples_per_second": 1472.948,
      "eval_steps_per_second": 36.824,
      "step": 18750
    },
    {
      "epoch": 1.0005333333333333,
      "grad_norm": 0.34863024950027466,
      "learning_rate": 4.374666666666667e-05,
      "loss": 0.0045,
      "step": 18760
    },
    {
      "epoch": 1.0010666666666668,
      "grad_norm": 0.31931203603744507,
      "learning_rate": 4.374333333333334e-05,
      "loss": 0.0036,
      "step": 18770
    },
    {
      "epoch": 1.0016,
      "grad_norm": 0.10520172119140625,
      "learning_rate": 4.3740000000000005e-05,
      "loss": 0.0038,
      "step": 18780
    },
    {
      "epoch": 1.0021333333333333,
      "grad_norm": 0.029643818736076355,
      "learning_rate": 4.373666666666667e-05,
      "loss": 0.003,
      "step": 18790
    },
    {
      "epoch": 1.0026666666666666,
      "grad_norm": 0.0865369439125061,
      "learning_rate": 4.373333333333334e-05,
      "loss": 0.0024,
      "step": 18800
    },
    {
      "epoch": 1.0032,
      "grad_norm": 0.013561271131038666,
      "learning_rate": 4.373e-05,
      "loss": 0.0038,
      "step": 18810
    },
    {
      "epoch": 1.0037333333333334,
      "grad_norm": 0.02915349043905735,
      "learning_rate": 4.372666666666667e-05,
      "loss": 0.003,
      "step": 18820
    },
    {
      "epoch": 1.0042666666666666,
      "grad_norm": 0.08746315538883209,
      "learning_rate": 4.3723333333333335e-05,
      "loss": 0.0031,
      "step": 18830
    },
    {
      "epoch": 1.0048,
      "grad_norm": 0.23235781490802765,
      "learning_rate": 4.372e-05,
      "loss": 0.0043,
      "step": 18840
    },
    {
      "epoch": 1.0053333333333334,
      "grad_norm": 0.13211578130722046,
      "learning_rate": 4.371666666666667e-05,
      "loss": 0.0037,
      "step": 18850
    },
    {
      "epoch": 1.0058666666666667,
      "grad_norm": 0.3769146502017975,
      "learning_rate": 4.3713333333333334e-05,
      "loss": 0.0049,
      "step": 18860
    },
    {
      "epoch": 1.0064,
      "grad_norm": 0.23268812894821167,
      "learning_rate": 4.371e-05,
      "loss": 0.0028,
      "step": 18870
    },
    {
      "epoch": 1.0069333333333332,
      "grad_norm": 0.46318820118904114,
      "learning_rate": 4.3706666666666666e-05,
      "loss": 0.0039,
      "step": 18880
    },
    {
      "epoch": 1.0074666666666667,
      "grad_norm": 0.6120728254318237,
      "learning_rate": 4.370333333333333e-05,
      "loss": 0.0049,
      "step": 18890
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.3470284044742584,
      "learning_rate": 4.3700000000000005e-05,
      "loss": 0.0032,
      "step": 18900
    },
    {
      "epoch": 1.0085333333333333,
      "grad_norm": 0.1436796486377716,
      "learning_rate": 4.369666666666667e-05,
      "loss": 0.0035,
      "step": 18910
    },
    {
      "epoch": 1.0090666666666666,
      "grad_norm": 0.2029629945755005,
      "learning_rate": 4.369333333333334e-05,
      "loss": 0.0038,
      "step": 18920
    },
    {
      "epoch": 1.0096,
      "grad_norm": 0.20156365633010864,
      "learning_rate": 4.3690000000000004e-05,
      "loss": 0.0037,
      "step": 18930
    },
    {
      "epoch": 1.0101333333333333,
      "grad_norm": 0.20200082659721375,
      "learning_rate": 4.368666666666667e-05,
      "loss": 0.0028,
      "step": 18940
    },
    {
      "epoch": 1.0106666666666666,
      "grad_norm": 0.11667697876691818,
      "learning_rate": 4.3683333333333336e-05,
      "loss": 0.0044,
      "step": 18950
    },
    {
      "epoch": 1.0112,
      "grad_norm": 0.0865853875875473,
      "learning_rate": 4.368e-05,
      "loss": 0.0031,
      "step": 18960
    },
    {
      "epoch": 1.0117333333333334,
      "grad_norm": 0.2589949369430542,
      "learning_rate": 4.367666666666667e-05,
      "loss": 0.0034,
      "step": 18970
    },
    {
      "epoch": 1.0122666666666666,
      "grad_norm": 0.05843532085418701,
      "learning_rate": 4.3673333333333334e-05,
      "loss": 0.0035,
      "step": 18980
    },
    {
      "epoch": 1.0128,
      "grad_norm": 0.08700980991125107,
      "learning_rate": 4.367e-05,
      "loss": 0.0033,
      "step": 18990
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.3999485969543457,
      "learning_rate": 4.3666666666666666e-05,
      "loss": 0.0026,
      "step": 19000
    },
    {
      "epoch": 1.0138666666666667,
      "grad_norm": 0.17247839272022247,
      "learning_rate": 4.366333333333333e-05,
      "loss": 0.0025,
      "step": 19010
    },
    {
      "epoch": 1.0144,
      "grad_norm": 0.11579328775405884,
      "learning_rate": 4.366e-05,
      "loss": 0.0031,
      "step": 19020
    },
    {
      "epoch": 1.0149333333333332,
      "grad_norm": 0.11576423048973083,
      "learning_rate": 4.3656666666666665e-05,
      "loss": 0.0042,
      "step": 19030
    },
    {
      "epoch": 1.0154666666666667,
      "grad_norm": 0.1445743292570114,
      "learning_rate": 4.365333333333334e-05,
      "loss": 0.0034,
      "step": 19040
    },
    {
      "epoch": 1.016,
      "grad_norm": 0.05925243720412254,
      "learning_rate": 4.3650000000000004e-05,
      "loss": 0.0042,
      "step": 19050
    },
    {
      "epoch": 1.0165333333333333,
      "grad_norm": 0.6634201407432556,
      "learning_rate": 4.364666666666667e-05,
      "loss": 0.0024,
      "step": 19060
    },
    {
      "epoch": 1.0170666666666666,
      "grad_norm": 0.17332221567630768,
      "learning_rate": 4.3643333333333336e-05,
      "loss": 0.003,
      "step": 19070
    },
    {
      "epoch": 1.0176,
      "grad_norm": 0.11538729071617126,
      "learning_rate": 4.364e-05,
      "loss": 0.0028,
      "step": 19080
    },
    {
      "epoch": 1.0181333333333333,
      "grad_norm": 0.05779648199677467,
      "learning_rate": 4.363666666666667e-05,
      "loss": 0.0041,
      "step": 19090
    },
    {
      "epoch": 1.0186666666666666,
      "grad_norm": 0.6034820079803467,
      "learning_rate": 4.3633333333333335e-05,
      "loss": 0.0024,
      "step": 19100
    },
    {
      "epoch": 1.0192,
      "grad_norm": 0.25967782735824585,
      "learning_rate": 4.363000000000001e-05,
      "loss": 0.0042,
      "step": 19110
    },
    {
      "epoch": 1.0197333333333334,
      "grad_norm": 0.2301887720823288,
      "learning_rate": 4.3626666666666674e-05,
      "loss": 0.0033,
      "step": 19120
    },
    {
      "epoch": 1.0202666666666667,
      "grad_norm": 0.3742956519126892,
      "learning_rate": 4.362333333333333e-05,
      "loss": 0.0031,
      "step": 19130
    },
    {
      "epoch": 1.0208,
      "grad_norm": 0.005946112331002951,
      "learning_rate": 4.362e-05,
      "loss": 0.0046,
      "step": 19140
    },
    {
      "epoch": 1.0213333333333334,
      "grad_norm": 0.31608790159225464,
      "learning_rate": 4.3616666666666665e-05,
      "loss": 0.0034,
      "step": 19150
    },
    {
      "epoch": 1.0218666666666667,
      "grad_norm": 0.05774824321269989,
      "learning_rate": 4.361333333333333e-05,
      "loss": 0.0019,
      "step": 19160
    },
    {
      "epoch": 1.0224,
      "grad_norm": 0.08641772717237473,
      "learning_rate": 4.361e-05,
      "loss": 0.0031,
      "step": 19170
    },
    {
      "epoch": 1.0229333333333333,
      "grad_norm": 0.28794628381729126,
      "learning_rate": 4.360666666666667e-05,
      "loss": 0.0045,
      "step": 19180
    },
    {
      "epoch": 1.0234666666666667,
      "grad_norm": 0.8572759628295898,
      "learning_rate": 4.3603333333333337e-05,
      "loss": 0.0036,
      "step": 19190
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.005106298252940178,
      "learning_rate": 4.36e-05,
      "loss": 0.0039,
      "step": 19200
    },
    {
      "epoch": 1.0245333333333333,
      "grad_norm": 0.5187921524047852,
      "learning_rate": 4.359666666666667e-05,
      "loss": 0.0032,
      "step": 19210
    },
    {
      "epoch": 1.0250666666666666,
      "grad_norm": 0.2883853316307068,
      "learning_rate": 4.3593333333333335e-05,
      "loss": 0.005,
      "step": 19220
    },
    {
      "epoch": 1.0256,
      "grad_norm": 0.2878236472606659,
      "learning_rate": 4.359e-05,
      "loss": 0.0031,
      "step": 19230
    },
    {
      "epoch": 1.0261333333333333,
      "grad_norm": 0.5493596196174622,
      "learning_rate": 4.358666666666667e-05,
      "loss": 0.0033,
      "step": 19240
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 0.17387661337852478,
      "learning_rate": 4.358333333333334e-05,
      "loss": 0.0038,
      "step": 19250
    },
    {
      "epoch": 1.0272,
      "grad_norm": 0.05805040895938873,
      "learning_rate": 4.3580000000000006e-05,
      "loss": 0.0034,
      "step": 19260
    },
    {
      "epoch": 1.0277333333333334,
      "grad_norm": 0.23263221979141235,
      "learning_rate": 4.357666666666667e-05,
      "loss": 0.0034,
      "step": 19270
    },
    {
      "epoch": 1.0282666666666667,
      "grad_norm": 0.004935379605740309,
      "learning_rate": 4.357333333333333e-05,
      "loss": 0.0034,
      "step": 19280
    },
    {
      "epoch": 1.0288,
      "grad_norm": 0.05784783139824867,
      "learning_rate": 4.357e-05,
      "loss": 0.0038,
      "step": 19290
    },
    {
      "epoch": 1.0293333333333334,
      "grad_norm": 0.02894805371761322,
      "learning_rate": 4.3566666666666664e-05,
      "loss": 0.0035,
      "step": 19300
    },
    {
      "epoch": 1.0298666666666667,
      "grad_norm": 0.3469466269016266,
      "learning_rate": 4.356333333333334e-05,
      "loss": 0.004,
      "step": 19310
    },
    {
      "epoch": 1.0304,
      "grad_norm": 0.23103371262550354,
      "learning_rate": 4.356e-05,
      "loss": 0.0034,
      "step": 19320
    },
    {
      "epoch": 1.0309333333333333,
      "grad_norm": 0.058581650257110596,
      "learning_rate": 4.355666666666667e-05,
      "loss": 0.0028,
      "step": 19330
    },
    {
      "epoch": 1.0314666666666668,
      "grad_norm": 0.3462851941585541,
      "learning_rate": 4.3553333333333335e-05,
      "loss": 0.0033,
      "step": 19340
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.14499974250793457,
      "learning_rate": 4.355e-05,
      "loss": 0.0045,
      "step": 19350
    },
    {
      "epoch": 1.0325333333333333,
      "grad_norm": 0.34572628140449524,
      "learning_rate": 4.354666666666667e-05,
      "loss": 0.0048,
      "step": 19360
    },
    {
      "epoch": 1.0330666666666666,
      "grad_norm": 0.08630376309156418,
      "learning_rate": 4.3543333333333334e-05,
      "loss": 0.0031,
      "step": 19370
    },
    {
      "epoch": 1.0336,
      "grad_norm": 0.05758705362677574,
      "learning_rate": 4.354e-05,
      "loss": 0.003,
      "step": 19380
    },
    {
      "epoch": 1.0341333333333333,
      "grad_norm": 0.1445050835609436,
      "learning_rate": 4.353666666666667e-05,
      "loss": 0.0033,
      "step": 19390
    },
    {
      "epoch": 1.0346666666666666,
      "grad_norm": 0.28696775436401367,
      "learning_rate": 4.353333333333334e-05,
      "loss": 0.0044,
      "step": 19400
    },
    {
      "epoch": 1.0352,
      "grad_norm": 0.11485996842384338,
      "learning_rate": 4.3530000000000005e-05,
      "loss": 0.0038,
      "step": 19410
    },
    {
      "epoch": 1.0357333333333334,
      "grad_norm": 0.08658695220947266,
      "learning_rate": 4.352666666666667e-05,
      "loss": 0.0037,
      "step": 19420
    },
    {
      "epoch": 1.0362666666666667,
      "grad_norm": 0.7954204082489014,
      "learning_rate": 4.352333333333334e-05,
      "loss": 0.002,
      "step": 19430
    },
    {
      "epoch": 1.0368,
      "grad_norm": 0.30846139788627625,
      "learning_rate": 4.352e-05,
      "loss": 0.0035,
      "step": 19440
    },
    {
      "epoch": 1.0373333333333334,
      "grad_norm": 0.1442566066980362,
      "learning_rate": 4.351666666666667e-05,
      "loss": 0.0085,
      "step": 19450
    },
    {
      "epoch": 1.0378666666666667,
      "grad_norm": 0.34465035796165466,
      "learning_rate": 4.3513333333333336e-05,
      "loss": 0.0036,
      "step": 19460
    },
    {
      "epoch": 1.0384,
      "grad_norm": 0.2310747504234314,
      "learning_rate": 4.351e-05,
      "loss": 0.0042,
      "step": 19470
    },
    {
      "epoch": 1.0389333333333333,
      "grad_norm": 0.23033766448497772,
      "learning_rate": 4.350666666666667e-05,
      "loss": 0.0031,
      "step": 19480
    },
    {
      "epoch": 1.0394666666666668,
      "grad_norm": 0.0053096674382686615,
      "learning_rate": 4.3503333333333334e-05,
      "loss": 0.0032,
      "step": 19490
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.20155715942382812,
      "learning_rate": 4.35e-05,
      "loss": 0.0024,
      "step": 19500
    },
    {
      "epoch": 1.0405333333333333,
      "grad_norm": 0.20158155262470245,
      "learning_rate": 4.3496666666666666e-05,
      "loss": 0.0037,
      "step": 19510
    },
    {
      "epoch": 1.0410666666666666,
      "grad_norm": 0.08591561019420624,
      "learning_rate": 4.349333333333334e-05,
      "loss": 0.0027,
      "step": 19520
    },
    {
      "epoch": 1.0416,
      "grad_norm": 0.05756772309541702,
      "learning_rate": 4.3490000000000005e-05,
      "loss": 0.0036,
      "step": 19530
    },
    {
      "epoch": 1.0421333333333334,
      "grad_norm": 0.5985615849494934,
      "learning_rate": 4.348666666666667e-05,
      "loss": 0.0042,
      "step": 19540
    },
    {
      "epoch": 1.0426666666666666,
      "grad_norm": 0.28563565015792847,
      "learning_rate": 4.348333333333334e-05,
      "loss": 0.0035,
      "step": 19550
    },
    {
      "epoch": 1.0432,
      "grad_norm": 0.02880854345858097,
      "learning_rate": 4.3480000000000004e-05,
      "loss": 0.0039,
      "step": 19560
    },
    {
      "epoch": 1.0437333333333334,
      "grad_norm": 0.46011796593666077,
      "learning_rate": 4.347666666666667e-05,
      "loss": 0.0047,
      "step": 19570
    },
    {
      "epoch": 1.0442666666666667,
      "grad_norm": 0.17263653874397278,
      "learning_rate": 4.3473333333333336e-05,
      "loss": 0.0025,
      "step": 19580
    },
    {
      "epoch": 1.0448,
      "grad_norm": 0.24302494525909424,
      "learning_rate": 4.347e-05,
      "loss": 0.0046,
      "step": 19590
    },
    {
      "epoch": 1.0453333333333332,
      "grad_norm": 0.08603989332914352,
      "learning_rate": 4.346666666666667e-05,
      "loss": 0.0034,
      "step": 19600
    },
    {
      "epoch": 1.0458666666666667,
      "grad_norm": 0.20098696649074554,
      "learning_rate": 4.3463333333333335e-05,
      "loss": 0.003,
      "step": 19610
    },
    {
      "epoch": 1.0464,
      "grad_norm": 0.17284837365150452,
      "learning_rate": 4.346e-05,
      "loss": 0.0032,
      "step": 19620
    },
    {
      "epoch": 1.0469333333333333,
      "grad_norm": 0.2879251539707184,
      "learning_rate": 4.345666666666667e-05,
      "loss": 0.0022,
      "step": 19630
    },
    {
      "epoch": 1.0474666666666668,
      "grad_norm": 0.6623277068138123,
      "learning_rate": 4.345333333333333e-05,
      "loss": 0.0029,
      "step": 19640
    },
    {
      "epoch": 1.048,
      "grad_norm": 0.08736942708492279,
      "learning_rate": 4.345e-05,
      "loss": 0.0042,
      "step": 19650
    },
    {
      "epoch": 1.0485333333333333,
      "grad_norm": 0.31590738892555237,
      "learning_rate": 4.344666666666667e-05,
      "loss": 0.0032,
      "step": 19660
    },
    {
      "epoch": 1.0490666666666666,
      "grad_norm": 0.6824616193771362,
      "learning_rate": 4.344333333333334e-05,
      "loss": 0.0039,
      "step": 19670
    },
    {
      "epoch": 1.0496,
      "grad_norm": 0.262667179107666,
      "learning_rate": 4.3440000000000004e-05,
      "loss": 0.0032,
      "step": 19680
    },
    {
      "epoch": 1.0501333333333334,
      "grad_norm": 0.5183894038200378,
      "learning_rate": 4.343666666666667e-05,
      "loss": 0.0058,
      "step": 19690
    },
    {
      "epoch": 1.0506666666666666,
      "grad_norm": 0.28941187262535095,
      "learning_rate": 4.3433333333333336e-05,
      "loss": 0.0041,
      "step": 19700
    },
    {
      "epoch": 1.0512,
      "grad_norm": 0.11589618027210236,
      "learning_rate": 4.343e-05,
      "loss": 0.0042,
      "step": 19710
    },
    {
      "epoch": 1.0517333333333334,
      "grad_norm": 0.17751164734363556,
      "learning_rate": 4.342666666666667e-05,
      "loss": 0.0039,
      "step": 19720
    },
    {
      "epoch": 1.0522666666666667,
      "grad_norm": 0.26195791363716125,
      "learning_rate": 4.3423333333333335e-05,
      "loss": 0.0039,
      "step": 19730
    },
    {
      "epoch": 1.0528,
      "grad_norm": 0.6358822584152222,
      "learning_rate": 4.342e-05,
      "loss": 0.0048,
      "step": 19740
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.12175413966178894,
      "learning_rate": 4.341666666666667e-05,
      "loss": 0.0026,
      "step": 19750
    },
    {
      "epoch": 1.0538666666666667,
      "grad_norm": 0.010395602323114872,
      "learning_rate": 4.341333333333333e-05,
      "loss": 0.0042,
      "step": 19760
    },
    {
      "epoch": 1.0544,
      "grad_norm": 0.17618751525878906,
      "learning_rate": 4.341e-05,
      "loss": 0.0049,
      "step": 19770
    },
    {
      "epoch": 1.0549333333333333,
      "grad_norm": 0.03085698001086712,
      "learning_rate": 4.3406666666666666e-05,
      "loss": 0.0044,
      "step": 19780
    },
    {
      "epoch": 1.0554666666666668,
      "grad_norm": 0.11730027943849564,
      "learning_rate": 4.340333333333333e-05,
      "loss": 0.0048,
      "step": 19790
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.1725795418024063,
      "learning_rate": 4.3400000000000005e-05,
      "loss": 0.0047,
      "step": 19800
    },
    {
      "epoch": 1.0565333333333333,
      "grad_norm": 0.11547847092151642,
      "learning_rate": 4.339666666666667e-05,
      "loss": 0.0029,
      "step": 19810
    },
    {
      "epoch": 1.0570666666666666,
      "grad_norm": 0.3468181788921356,
      "learning_rate": 4.339333333333334e-05,
      "loss": 0.0048,
      "step": 19820
    },
    {
      "epoch": 1.0576,
      "grad_norm": 0.6392664313316345,
      "learning_rate": 4.339e-05,
      "loss": 0.0036,
      "step": 19830
    },
    {
      "epoch": 1.0581333333333334,
      "grad_norm": 0.23223821818828583,
      "learning_rate": 4.338666666666667e-05,
      "loss": 0.0041,
      "step": 19840
    },
    {
      "epoch": 1.0586666666666666,
      "grad_norm": 0.1727127730846405,
      "learning_rate": 4.3383333333333335e-05,
      "loss": 0.0026,
      "step": 19850
    },
    {
      "epoch": 1.0592,
      "grad_norm": 0.25931504368782043,
      "learning_rate": 4.338e-05,
      "loss": 0.004,
      "step": 19860
    },
    {
      "epoch": 1.0597333333333334,
      "grad_norm": 0.20200785994529724,
      "learning_rate": 4.3376666666666674e-05,
      "loss": 0.0039,
      "step": 19870
    },
    {
      "epoch": 1.0602666666666667,
      "grad_norm": 0.11595369875431061,
      "learning_rate": 4.337333333333334e-05,
      "loss": 0.0039,
      "step": 19880
    },
    {
      "epoch": 1.0608,
      "grad_norm": 0.26137155294418335,
      "learning_rate": 4.337e-05,
      "loss": 0.0036,
      "step": 19890
    },
    {
      "epoch": 1.0613333333333332,
      "grad_norm": 0.6090059280395508,
      "learning_rate": 4.3366666666666666e-05,
      "loss": 0.0032,
      "step": 19900
    },
    {
      "epoch": 1.0618666666666667,
      "grad_norm": 0.20114439725875854,
      "learning_rate": 4.336333333333333e-05,
      "loss": 0.0042,
      "step": 19910
    },
    {
      "epoch": 1.0624,
      "grad_norm": 0.11660824716091156,
      "learning_rate": 4.336e-05,
      "loss": 0.0032,
      "step": 19920
    },
    {
      "epoch": 1.0629333333333333,
      "grad_norm": 0.1163584366440773,
      "learning_rate": 4.3356666666666664e-05,
      "loss": 0.0031,
      "step": 19930
    },
    {
      "epoch": 1.0634666666666668,
      "grad_norm": 0.17274349927902222,
      "learning_rate": 4.335333333333334e-05,
      "loss": 0.0039,
      "step": 19940
    },
    {
      "epoch": 1.064,
      "grad_norm": 0.173465758562088,
      "learning_rate": 4.335e-05,
      "loss": 0.004,
      "step": 19950
    },
    {
      "epoch": 1.0645333333333333,
      "grad_norm": 0.11818542331457138,
      "learning_rate": 4.334666666666667e-05,
      "loss": 0.0034,
      "step": 19960
    },
    {
      "epoch": 1.0650666666666666,
      "grad_norm": 0.20275843143463135,
      "learning_rate": 4.3343333333333336e-05,
      "loss": 0.0037,
      "step": 19970
    },
    {
      "epoch": 1.0656,
      "grad_norm": 0.31684577465057373,
      "learning_rate": 4.334e-05,
      "loss": 0.0045,
      "step": 19980
    },
    {
      "epoch": 1.0661333333333334,
      "grad_norm": 0.20177973806858063,
      "learning_rate": 4.333666666666667e-05,
      "loss": 0.0029,
      "step": 19990
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.009579755365848541,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 0.0027,
      "step": 20000
    },
    {
      "epoch": 1.0672,
      "grad_norm": 0.11517082154750824,
      "learning_rate": 4.333000000000001e-05,
      "loss": 0.0038,
      "step": 20010
    },
    {
      "epoch": 1.0677333333333334,
      "grad_norm": 0.05745185166597366,
      "learning_rate": 4.332666666666667e-05,
      "loss": 0.0035,
      "step": 20020
    },
    {
      "epoch": 1.0682666666666667,
      "grad_norm": 0.23126158118247986,
      "learning_rate": 4.332333333333334e-05,
      "loss": 0.003,
      "step": 20030
    },
    {
      "epoch": 1.0688,
      "grad_norm": 0.11482513695955276,
      "learning_rate": 4.332e-05,
      "loss": 0.0029,
      "step": 20040
    },
    {
      "epoch": 1.0693333333333332,
      "grad_norm": 0.2596680819988251,
      "learning_rate": 4.3316666666666665e-05,
      "loss": 0.0028,
      "step": 20050
    },
    {
      "epoch": 1.0698666666666667,
      "grad_norm": 0.05951329693198204,
      "learning_rate": 4.331333333333333e-05,
      "loss": 0.0032,
      "step": 20060
    },
    {
      "epoch": 1.0704,
      "grad_norm": 0.20395579934120178,
      "learning_rate": 4.3310000000000004e-05,
      "loss": 0.0037,
      "step": 20070
    },
    {
      "epoch": 1.0709333333333333,
      "grad_norm": 0.057558901607990265,
      "learning_rate": 4.330666666666667e-05,
      "loss": 0.0029,
      "step": 20080
    },
    {
      "epoch": 1.0714666666666666,
      "grad_norm": 0.038251712918281555,
      "learning_rate": 4.3303333333333336e-05,
      "loss": 0.0057,
      "step": 20090
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.013131873682141304,
      "learning_rate": 4.33e-05,
      "loss": 0.0036,
      "step": 20100
    },
    {
      "epoch": 1.0725333333333333,
      "grad_norm": 0.05843531712889671,
      "learning_rate": 4.329666666666667e-05,
      "loss": 0.0047,
      "step": 20110
    },
    {
      "epoch": 1.0730666666666666,
      "grad_norm": 0.03120245225727558,
      "learning_rate": 4.3293333333333334e-05,
      "loss": 0.004,
      "step": 20120
    },
    {
      "epoch": 1.0735999999999999,
      "grad_norm": 0.43576139211654663,
      "learning_rate": 4.329e-05,
      "loss": 0.0038,
      "step": 20130
    },
    {
      "epoch": 1.0741333333333334,
      "grad_norm": 0.3180106580257416,
      "learning_rate": 4.328666666666667e-05,
      "loss": 0.004,
      "step": 20140
    },
    {
      "epoch": 1.0746666666666667,
      "grad_norm": 0.3162989914417267,
      "learning_rate": 4.328333333333334e-05,
      "loss": 0.0022,
      "step": 20150
    },
    {
      "epoch": 1.0752,
      "grad_norm": 0.05766162648797035,
      "learning_rate": 4.3280000000000006e-05,
      "loss": 0.0029,
      "step": 20160
    },
    {
      "epoch": 1.0757333333333334,
      "grad_norm": 0.2873581051826477,
      "learning_rate": 4.327666666666667e-05,
      "loss": 0.0038,
      "step": 20170
    },
    {
      "epoch": 1.0762666666666667,
      "grad_norm": 0.08669286221265793,
      "learning_rate": 4.327333333333334e-05,
      "loss": 0.0033,
      "step": 20180
    },
    {
      "epoch": 1.0768,
      "grad_norm": 0.11650051921606064,
      "learning_rate": 4.327e-05,
      "loss": 0.0029,
      "step": 20190
    },
    {
      "epoch": 1.0773333333333333,
      "grad_norm": 0.08645697683095932,
      "learning_rate": 4.3266666666666664e-05,
      "loss": 0.004,
      "step": 20200
    },
    {
      "epoch": 1.0778666666666668,
      "grad_norm": 0.08613225072622299,
      "learning_rate": 4.3263333333333336e-05,
      "loss": 0.0024,
      "step": 20210
    },
    {
      "epoch": 1.0784,
      "grad_norm": 0.28953543305397034,
      "learning_rate": 4.326e-05,
      "loss": 0.0028,
      "step": 20220
    },
    {
      "epoch": 1.0789333333333333,
      "grad_norm": 0.17389744520187378,
      "learning_rate": 4.325666666666667e-05,
      "loss": 0.0036,
      "step": 20230
    },
    {
      "epoch": 1.0794666666666666,
      "grad_norm": 0.11509259790182114,
      "learning_rate": 4.3253333333333335e-05,
      "loss": 0.0032,
      "step": 20240
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.49015140533447266,
      "learning_rate": 4.325e-05,
      "loss": 0.0036,
      "step": 20250
    },
    {
      "epoch": 1.0805333333333333,
      "grad_norm": 0.25845468044281006,
      "learning_rate": 4.324666666666667e-05,
      "loss": 0.0032,
      "step": 20260
    },
    {
      "epoch": 1.0810666666666666,
      "grad_norm": 0.402547687292099,
      "learning_rate": 4.324333333333333e-05,
      "loss": 0.0025,
      "step": 20270
    },
    {
      "epoch": 1.0816,
      "grad_norm": 0.11451086401939392,
      "learning_rate": 4.324e-05,
      "loss": 0.0035,
      "step": 20280
    },
    {
      "epoch": 1.0821333333333334,
      "grad_norm": 0.22934743762016296,
      "learning_rate": 4.323666666666667e-05,
      "loss": 0.0034,
      "step": 20290
    },
    {
      "epoch": 1.0826666666666667,
      "grad_norm": 0.9879325032234192,
      "learning_rate": 4.323333333333334e-05,
      "loss": 0.0041,
      "step": 20300
    },
    {
      "epoch": 1.0832,
      "grad_norm": 0.057693563401699066,
      "learning_rate": 4.3230000000000005e-05,
      "loss": 0.0031,
      "step": 20310
    },
    {
      "epoch": 1.0837333333333334,
      "grad_norm": 0.05715933442115784,
      "learning_rate": 4.322666666666667e-05,
      "loss": 0.0041,
      "step": 20320
    },
    {
      "epoch": 1.0842666666666667,
      "grad_norm": 0.22952628135681152,
      "learning_rate": 4.322333333333334e-05,
      "loss": 0.0039,
      "step": 20330
    },
    {
      "epoch": 1.0848,
      "grad_norm": 0.31555721163749695,
      "learning_rate": 4.3219999999999996e-05,
      "loss": 0.0025,
      "step": 20340
    },
    {
      "epoch": 1.0853333333333333,
      "grad_norm": 0.6605441570281982,
      "learning_rate": 4.321666666666667e-05,
      "loss": 0.0039,
      "step": 20350
    },
    {
      "epoch": 1.0858666666666668,
      "grad_norm": 0.201619952917099,
      "learning_rate": 4.3213333333333335e-05,
      "loss": 0.0038,
      "step": 20360
    },
    {
      "epoch": 1.0864,
      "grad_norm": 0.028845027089118958,
      "learning_rate": 4.321e-05,
      "loss": 0.0026,
      "step": 20370
    },
    {
      "epoch": 1.0869333333333333,
      "grad_norm": 0.3458538353443146,
      "learning_rate": 4.320666666666667e-05,
      "loss": 0.0041,
      "step": 20380
    },
    {
      "epoch": 1.0874666666666666,
      "grad_norm": 0.5164316892623901,
      "learning_rate": 4.3203333333333334e-05,
      "loss": 0.0031,
      "step": 20390
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.08661304414272308,
      "learning_rate": 4.32e-05,
      "loss": 0.005,
      "step": 20400
    },
    {
      "epoch": 1.0885333333333334,
      "grad_norm": 0.08770979940891266,
      "learning_rate": 4.3196666666666666e-05,
      "loss": 0.0033,
      "step": 20410
    },
    {
      "epoch": 1.0890666666666666,
      "grad_norm": 0.2291349321603775,
      "learning_rate": 4.319333333333334e-05,
      "loss": 0.004,
      "step": 20420
    },
    {
      "epoch": 1.0896,
      "grad_norm": 0.20082995295524597,
      "learning_rate": 4.3190000000000005e-05,
      "loss": 0.0027,
      "step": 20430
    },
    {
      "epoch": 1.0901333333333334,
      "grad_norm": 0.8287000060081482,
      "learning_rate": 4.318666666666667e-05,
      "loss": 0.0024,
      "step": 20440
    },
    {
      "epoch": 1.0906666666666667,
      "grad_norm": 0.28727322816848755,
      "learning_rate": 4.318333333333334e-05,
      "loss": 0.0029,
      "step": 20450
    },
    {
      "epoch": 1.0912,
      "grad_norm": 0.08571968227624893,
      "learning_rate": 4.318e-05,
      "loss": 0.0038,
      "step": 20460
    },
    {
      "epoch": 1.0917333333333334,
      "grad_norm": 0.05738398805260658,
      "learning_rate": 4.317666666666667e-05,
      "loss": 0.0044,
      "step": 20470
    },
    {
      "epoch": 1.0922666666666667,
      "grad_norm": 0.3439735770225525,
      "learning_rate": 4.3173333333333336e-05,
      "loss": 0.003,
      "step": 20480
    },
    {
      "epoch": 1.0928,
      "grad_norm": 0.05768747627735138,
      "learning_rate": 4.317e-05,
      "loss": 0.0021,
      "step": 20490
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 0.20176266133785248,
      "learning_rate": 4.316666666666667e-05,
      "loss": 0.0029,
      "step": 20500
    },
    {
      "epoch": 1.0938666666666668,
      "grad_norm": 0.3739112615585327,
      "learning_rate": 4.3163333333333334e-05,
      "loss": 0.0041,
      "step": 20510
    },
    {
      "epoch": 1.0944,
      "grad_norm": 0.34427085518836975,
      "learning_rate": 4.316e-05,
      "loss": 0.0052,
      "step": 20520
    },
    {
      "epoch": 1.0949333333333333,
      "grad_norm": 0.14408224821090698,
      "learning_rate": 4.3156666666666666e-05,
      "loss": 0.004,
      "step": 20530
    },
    {
      "epoch": 1.0954666666666666,
      "grad_norm": 0.3449152112007141,
      "learning_rate": 4.315333333333333e-05,
      "loss": 0.0027,
      "step": 20540
    },
    {
      "epoch": 1.096,
      "grad_norm": 0.48852604627609253,
      "learning_rate": 4.315e-05,
      "loss": 0.003,
      "step": 20550
    },
    {
      "epoch": 1.0965333333333334,
      "grad_norm": 0.1447347104549408,
      "learning_rate": 4.314666666666667e-05,
      "loss": 0.0032,
      "step": 20560
    },
    {
      "epoch": 1.0970666666666666,
      "grad_norm": 0.029133794829249382,
      "learning_rate": 4.314333333333334e-05,
      "loss": 0.0044,
      "step": 20570
    },
    {
      "epoch": 1.0976,
      "grad_norm": 0.2867768704891205,
      "learning_rate": 4.3140000000000004e-05,
      "loss": 0.0041,
      "step": 20580
    },
    {
      "epoch": 1.0981333333333334,
      "grad_norm": 0.4588302671909332,
      "learning_rate": 4.313666666666667e-05,
      "loss": 0.0055,
      "step": 20590
    },
    {
      "epoch": 1.0986666666666667,
      "grad_norm": 0.11453811079263687,
      "learning_rate": 4.3133333333333336e-05,
      "loss": 0.0023,
      "step": 20600
    },
    {
      "epoch": 1.0992,
      "grad_norm": 0.6598779559135437,
      "learning_rate": 4.313e-05,
      "loss": 0.0033,
      "step": 20610
    },
    {
      "epoch": 1.0997333333333332,
      "grad_norm": 0.20127393305301666,
      "learning_rate": 4.312666666666667e-05,
      "loss": 0.0036,
      "step": 20620
    },
    {
      "epoch": 1.1002666666666667,
      "grad_norm": 0.20177723467350006,
      "learning_rate": 4.312333333333334e-05,
      "loss": 0.0024,
      "step": 20630
    },
    {
      "epoch": 1.1008,
      "grad_norm": 0.08691289275884628,
      "learning_rate": 4.312000000000001e-05,
      "loss": 0.0048,
      "step": 20640
    },
    {
      "epoch": 1.1013333333333333,
      "grad_norm": 0.172615185379982,
      "learning_rate": 4.311666666666667e-05,
      "loss": 0.0036,
      "step": 20650
    },
    {
      "epoch": 1.1018666666666665,
      "grad_norm": 0.402313232421875,
      "learning_rate": 4.311333333333333e-05,
      "loss": 0.0037,
      "step": 20660
    },
    {
      "epoch": 1.1024,
      "grad_norm": 0.37365832924842834,
      "learning_rate": 4.311e-05,
      "loss": 0.0036,
      "step": 20670
    },
    {
      "epoch": 1.1029333333333333,
      "grad_norm": 0.9855782389640808,
      "learning_rate": 4.3106666666666665e-05,
      "loss": 0.0039,
      "step": 20680
    },
    {
      "epoch": 1.1034666666666666,
      "grad_norm": 0.22934556007385254,
      "learning_rate": 4.310333333333333e-05,
      "loss": 0.0023,
      "step": 20690
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.20250917971134186,
      "learning_rate": 4.3100000000000004e-05,
      "loss": 0.0024,
      "step": 20700
    },
    {
      "epoch": 1.1045333333333334,
      "grad_norm": 0.2593955993652344,
      "learning_rate": 4.309666666666667e-05,
      "loss": 0.0033,
      "step": 20710
    },
    {
      "epoch": 1.1050666666666666,
      "grad_norm": 0.08679498732089996,
      "learning_rate": 4.3093333333333336e-05,
      "loss": 0.0031,
      "step": 20720
    },
    {
      "epoch": 1.1056,
      "grad_norm": 0.48967477679252625,
      "learning_rate": 4.309e-05,
      "loss": 0.0043,
      "step": 20730
    },
    {
      "epoch": 1.1061333333333334,
      "grad_norm": 0.11495885998010635,
      "learning_rate": 4.308666666666667e-05,
      "loss": 0.0027,
      "step": 20740
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 0.05771889165043831,
      "learning_rate": 4.3083333333333335e-05,
      "loss": 0.003,
      "step": 20750
    },
    {
      "epoch": 1.1072,
      "grad_norm": 0.34452828764915466,
      "learning_rate": 4.308e-05,
      "loss": 0.003,
      "step": 20760
    },
    {
      "epoch": 1.1077333333333332,
      "grad_norm": 0.08606128394603729,
      "learning_rate": 4.3076666666666674e-05,
      "loss": 0.0027,
      "step": 20770
    },
    {
      "epoch": 1.1082666666666667,
      "grad_norm": 0.1145748496055603,
      "learning_rate": 4.307333333333334e-05,
      "loss": 0.0028,
      "step": 20780
    },
    {
      "epoch": 1.1088,
      "grad_norm": 0.2013482004404068,
      "learning_rate": 4.3070000000000006e-05,
      "loss": 0.0043,
      "step": 20790
    },
    {
      "epoch": 1.1093333333333333,
      "grad_norm": 0.029013844206929207,
      "learning_rate": 4.3066666666666665e-05,
      "loss": 0.003,
      "step": 20800
    },
    {
      "epoch": 1.1098666666666666,
      "grad_norm": 0.030155902728438377,
      "learning_rate": 4.306333333333333e-05,
      "loss": 0.0043,
      "step": 20810
    },
    {
      "epoch": 1.1104,
      "grad_norm": 0.2006254941225052,
      "learning_rate": 4.306e-05,
      "loss": 0.0033,
      "step": 20820
    },
    {
      "epoch": 1.1109333333333333,
      "grad_norm": 0.057486020028591156,
      "learning_rate": 4.305666666666667e-05,
      "loss": 0.0028,
      "step": 20830
    },
    {
      "epoch": 1.1114666666666666,
      "grad_norm": 0.3432515561580658,
      "learning_rate": 4.305333333333334e-05,
      "loss": 0.0027,
      "step": 20840
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.02881811186671257,
      "learning_rate": 4.305e-05,
      "loss": 0.0021,
      "step": 20850
    },
    {
      "epoch": 1.1125333333333334,
      "grad_norm": 0.20024673640727997,
      "learning_rate": 4.304666666666667e-05,
      "loss": 0.0026,
      "step": 20860
    },
    {
      "epoch": 1.1130666666666666,
      "grad_norm": 0.029334845021367073,
      "learning_rate": 4.3043333333333335e-05,
      "loss": 0.0019,
      "step": 20870
    },
    {
      "epoch": 1.1136,
      "grad_norm": 0.11473985016345978,
      "learning_rate": 4.304e-05,
      "loss": 0.003,
      "step": 20880
    },
    {
      "epoch": 1.1141333333333334,
      "grad_norm": 0.029912231490015984,
      "learning_rate": 4.303666666666667e-05,
      "loss": 0.0026,
      "step": 20890
    },
    {
      "epoch": 1.1146666666666667,
      "grad_norm": 0.3440099060535431,
      "learning_rate": 4.3033333333333334e-05,
      "loss": 0.0027,
      "step": 20900
    },
    {
      "epoch": 1.1152,
      "grad_norm": 1.0279793739318848,
      "learning_rate": 4.3030000000000006e-05,
      "loss": 0.003,
      "step": 20910
    },
    {
      "epoch": 1.1157333333333332,
      "grad_norm": 0.21813145279884338,
      "learning_rate": 4.302666666666667e-05,
      "loss": 0.0032,
      "step": 20920
    },
    {
      "epoch": 1.1162666666666667,
      "grad_norm": 0.9757739305496216,
      "learning_rate": 4.302333333333334e-05,
      "loss": 0.0035,
      "step": 20930
    },
    {
      "epoch": 1.1168,
      "grad_norm": 0.11420947313308716,
      "learning_rate": 4.3020000000000005e-05,
      "loss": 0.0022,
      "step": 20940
    },
    {
      "epoch": 1.1173333333333333,
      "grad_norm": 0.17147044837474823,
      "learning_rate": 4.3016666666666664e-05,
      "loss": 0.0037,
      "step": 20950
    },
    {
      "epoch": 1.1178666666666666,
      "grad_norm": 0.6479079127311707,
      "learning_rate": 4.301333333333333e-05,
      "loss": 0.003,
      "step": 20960
    },
    {
      "epoch": 1.1184,
      "grad_norm": 0.1718541532754898,
      "learning_rate": 4.301e-05,
      "loss": 0.005,
      "step": 20970
    },
    {
      "epoch": 1.1189333333333333,
      "grad_norm": 0.571520984172821,
      "learning_rate": 4.300666666666667e-05,
      "loss": 0.0042,
      "step": 20980
    },
    {
      "epoch": 1.1194666666666666,
      "grad_norm": 0.17163923382759094,
      "learning_rate": 4.3003333333333336e-05,
      "loss": 0.004,
      "step": 20990
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.20020921528339386,
      "learning_rate": 4.3e-05,
      "loss": 0.0027,
      "step": 21000
    },
    {
      "epoch": 1.1205333333333334,
      "grad_norm": 0.1714448630809784,
      "learning_rate": 4.299666666666667e-05,
      "loss": 0.0048,
      "step": 21010
    },
    {
      "epoch": 1.1210666666666667,
      "grad_norm": 0.003437815001234412,
      "learning_rate": 4.2993333333333334e-05,
      "loss": 0.0035,
      "step": 21020
    },
    {
      "epoch": 1.1216,
      "grad_norm": 0.3433483839035034,
      "learning_rate": 4.299e-05,
      "loss": 0.0037,
      "step": 21030
    },
    {
      "epoch": 1.1221333333333334,
      "grad_norm": 0.11454389244318008,
      "learning_rate": 4.2986666666666666e-05,
      "loss": 0.003,
      "step": 21040
    },
    {
      "epoch": 1.1226666666666667,
      "grad_norm": 0.006516890600323677,
      "learning_rate": 4.298333333333334e-05,
      "loss": 0.0031,
      "step": 21050
    },
    {
      "epoch": 1.1232,
      "grad_norm": 0.029285486787557602,
      "learning_rate": 4.2980000000000005e-05,
      "loss": 0.0036,
      "step": 21060
    },
    {
      "epoch": 1.1237333333333333,
      "grad_norm": 0.3726016581058502,
      "learning_rate": 4.297666666666667e-05,
      "loss": 0.0039,
      "step": 21070
    },
    {
      "epoch": 1.1242666666666667,
      "grad_norm": 0.05726078525185585,
      "learning_rate": 4.297333333333334e-05,
      "loss": 0.0024,
      "step": 21080
    },
    {
      "epoch": 1.1248,
      "grad_norm": 0.20059673488140106,
      "learning_rate": 4.2970000000000004e-05,
      "loss": 0.0041,
      "step": 21090
    },
    {
      "epoch": 1.1253333333333333,
      "grad_norm": 0.14404712617397308,
      "learning_rate": 4.296666666666666e-05,
      "loss": 0.0038,
      "step": 21100
    },
    {
      "epoch": 1.1258666666666666,
      "grad_norm": 0.8776955604553223,
      "learning_rate": 4.2963333333333336e-05,
      "loss": 0.0046,
      "step": 21110
    },
    {
      "epoch": 1.1264,
      "grad_norm": 0.17306546866893768,
      "learning_rate": 4.296e-05,
      "loss": 0.0036,
      "step": 21120
    },
    {
      "epoch": 1.1269333333333333,
      "grad_norm": 0.8182936310768127,
      "learning_rate": 4.295666666666667e-05,
      "loss": 0.0045,
      "step": 21130
    },
    {
      "epoch": 1.1274666666666666,
      "grad_norm": 0.5821223855018616,
      "learning_rate": 4.2953333333333334e-05,
      "loss": 0.0042,
      "step": 21140
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 0.23219063878059387,
      "learning_rate": 4.295e-05,
      "loss": 0.0037,
      "step": 21150
    },
    {
      "epoch": 1.1285333333333334,
      "grad_norm": 0.4697699248790741,
      "learning_rate": 4.2946666666666667e-05,
      "loss": 0.0045,
      "step": 21160
    },
    {
      "epoch": 1.1290666666666667,
      "grad_norm": 0.2894308865070343,
      "learning_rate": 4.294333333333333e-05,
      "loss": 0.0035,
      "step": 21170
    },
    {
      "epoch": 1.1296,
      "grad_norm": 0.2879435420036316,
      "learning_rate": 4.2940000000000006e-05,
      "loss": 0.0024,
      "step": 21180
    },
    {
      "epoch": 1.1301333333333332,
      "grad_norm": 0.7165715098381042,
      "learning_rate": 4.293666666666667e-05,
      "loss": 0.0028,
      "step": 21190
    },
    {
      "epoch": 1.1306666666666667,
      "grad_norm": 0.6301265954971313,
      "learning_rate": 4.293333333333334e-05,
      "loss": 0.0037,
      "step": 21200
    },
    {
      "epoch": 1.1312,
      "grad_norm": 0.11599572002887726,
      "learning_rate": 4.2930000000000004e-05,
      "loss": 0.0036,
      "step": 21210
    },
    {
      "epoch": 1.1317333333333333,
      "grad_norm": 0.20043878257274628,
      "learning_rate": 4.292666666666667e-05,
      "loss": 0.0044,
      "step": 21220
    },
    {
      "epoch": 1.1322666666666668,
      "grad_norm": 0.3434259593486786,
      "learning_rate": 4.2923333333333336e-05,
      "loss": 0.0029,
      "step": 21230
    },
    {
      "epoch": 1.1328,
      "grad_norm": 0.4860391616821289,
      "learning_rate": 4.292e-05,
      "loss": 0.0049,
      "step": 21240
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.5185582041740417,
      "learning_rate": 4.291666666666667e-05,
      "loss": 0.0038,
      "step": 21250
    },
    {
      "epoch": 1.1338666666666666,
      "grad_norm": 0.40252113342285156,
      "learning_rate": 4.2913333333333335e-05,
      "loss": 0.0041,
      "step": 21260
    },
    {
      "epoch": 1.1344,
      "grad_norm": 0.3153059184551239,
      "learning_rate": 4.291e-05,
      "loss": 0.0026,
      "step": 21270
    },
    {
      "epoch": 1.1349333333333333,
      "grad_norm": 0.14523649215698242,
      "learning_rate": 4.290666666666667e-05,
      "loss": 0.0039,
      "step": 21280
    },
    {
      "epoch": 1.1354666666666666,
      "grad_norm": 0.2623896300792694,
      "learning_rate": 4.290333333333333e-05,
      "loss": 0.0031,
      "step": 21290
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 0.23033267259597778,
      "learning_rate": 4.29e-05,
      "loss": 0.0035,
      "step": 21300
    },
    {
      "epoch": 1.1365333333333334,
      "grad_norm": 0.0038870491553097963,
      "learning_rate": 4.2896666666666665e-05,
      "loss": 0.0024,
      "step": 21310
    },
    {
      "epoch": 1.1370666666666667,
      "grad_norm": 0.11515775322914124,
      "learning_rate": 4.289333333333334e-05,
      "loss": 0.0035,
      "step": 21320
    },
    {
      "epoch": 1.1376,
      "grad_norm": 0.2586537003517151,
      "learning_rate": 4.2890000000000004e-05,
      "loss": 0.0033,
      "step": 21330
    },
    {
      "epoch": 1.1381333333333332,
      "grad_norm": 0.14350755512714386,
      "learning_rate": 4.288666666666667e-05,
      "loss": 0.0036,
      "step": 21340
    },
    {
      "epoch": 1.1386666666666667,
      "grad_norm": 0.20137622952461243,
      "learning_rate": 4.288333333333334e-05,
      "loss": 0.0032,
      "step": 21350
    },
    {
      "epoch": 1.1392,
      "grad_norm": 0.2582243084907532,
      "learning_rate": 4.288e-05,
      "loss": 0.0043,
      "step": 21360
    },
    {
      "epoch": 1.1397333333333333,
      "grad_norm": 0.11509551852941513,
      "learning_rate": 4.287666666666667e-05,
      "loss": 0.0032,
      "step": 21370
    },
    {
      "epoch": 1.1402666666666668,
      "grad_norm": 0.029614752158522606,
      "learning_rate": 4.2873333333333335e-05,
      "loss": 0.0031,
      "step": 21380
    },
    {
      "epoch": 1.1408,
      "grad_norm": 0.009959318675100803,
      "learning_rate": 4.287000000000001e-05,
      "loss": 0.0021,
      "step": 21390
    },
    {
      "epoch": 1.1413333333333333,
      "grad_norm": 0.31559568643569946,
      "learning_rate": 4.286666666666667e-05,
      "loss": 0.002,
      "step": 21400
    },
    {
      "epoch": 1.1418666666666666,
      "grad_norm": 0.05772113427519798,
      "learning_rate": 4.2863333333333333e-05,
      "loss": 0.0027,
      "step": 21410
    },
    {
      "epoch": 1.1424,
      "grad_norm": 0.08591674268245697,
      "learning_rate": 4.286e-05,
      "loss": 0.003,
      "step": 21420
    },
    {
      "epoch": 1.1429333333333334,
      "grad_norm": 0.2012181282043457,
      "learning_rate": 4.2856666666666666e-05,
      "loss": 0.0048,
      "step": 21430
    },
    {
      "epoch": 1.1434666666666666,
      "grad_norm": 0.0027427589520812035,
      "learning_rate": 4.285333333333333e-05,
      "loss": 0.0058,
      "step": 21440
    },
    {
      "epoch": 1.144,
      "grad_norm": 0.004640460945665836,
      "learning_rate": 4.285e-05,
      "loss": 0.0041,
      "step": 21450
    },
    {
      "epoch": 1.1445333333333334,
      "grad_norm": 0.08691708743572235,
      "learning_rate": 4.284666666666667e-05,
      "loss": 0.0043,
      "step": 21460
    },
    {
      "epoch": 1.1450666666666667,
      "grad_norm": 0.2579383850097656,
      "learning_rate": 4.284333333333334e-05,
      "loss": 0.0025,
      "step": 21470
    },
    {
      "epoch": 1.1456,
      "grad_norm": 0.4583573341369629,
      "learning_rate": 4.284e-05,
      "loss": 0.0046,
      "step": 21480
    },
    {
      "epoch": 1.1461333333333332,
      "grad_norm": 0.14313915371894836,
      "learning_rate": 4.283666666666667e-05,
      "loss": 0.0034,
      "step": 21490
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 0.057583995163440704,
      "learning_rate": 4.2833333333333335e-05,
      "loss": 0.0038,
      "step": 21500
    },
    {
      "epoch": 1.1472,
      "grad_norm": 0.5439629554748535,
      "learning_rate": 4.283e-05,
      "loss": 0.0038,
      "step": 21510
    },
    {
      "epoch": 1.1477333333333333,
      "grad_norm": 0.08591286092996597,
      "learning_rate": 4.282666666666667e-05,
      "loss": 0.0023,
      "step": 21520
    },
    {
      "epoch": 1.1482666666666668,
      "grad_norm": 0.11440186947584152,
      "learning_rate": 4.282333333333334e-05,
      "loss": 0.0048,
      "step": 21530
    },
    {
      "epoch": 1.1488,
      "grad_norm": 0.08569394797086716,
      "learning_rate": 4.282000000000001e-05,
      "loss": 0.0044,
      "step": 21540
    },
    {
      "epoch": 1.1493333333333333,
      "grad_norm": 0.22892849147319794,
      "learning_rate": 4.2816666666666666e-05,
      "loss": 0.0028,
      "step": 21550
    },
    {
      "epoch": 1.1498666666666666,
      "grad_norm": 0.057367272675037384,
      "learning_rate": 4.281333333333333e-05,
      "loss": 0.0032,
      "step": 21560
    },
    {
      "epoch": 1.1504,
      "grad_norm": 0.05779704451560974,
      "learning_rate": 4.281e-05,
      "loss": 0.0047,
      "step": 21570
    },
    {
      "epoch": 1.1509333333333334,
      "grad_norm": 0.3145577013492584,
      "learning_rate": 4.2806666666666665e-05,
      "loss": 0.0046,
      "step": 21580
    },
    {
      "epoch": 1.1514666666666666,
      "grad_norm": 0.1438348889350891,
      "learning_rate": 4.280333333333334e-05,
      "loss": 0.0035,
      "step": 21590
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.2860350012779236,
      "learning_rate": 4.2800000000000004e-05,
      "loss": 0.003,
      "step": 21600
    },
    {
      "epoch": 1.1525333333333334,
      "grad_norm": 0.05816804990172386,
      "learning_rate": 4.279666666666667e-05,
      "loss": 0.0032,
      "step": 21610
    },
    {
      "epoch": 1.1530666666666667,
      "grad_norm": 0.17173494398593903,
      "learning_rate": 4.2793333333333336e-05,
      "loss": 0.0038,
      "step": 21620
    },
    {
      "epoch": 1.1536,
      "grad_norm": 0.4010031223297119,
      "learning_rate": 4.279e-05,
      "loss": 0.0037,
      "step": 21630
    },
    {
      "epoch": 1.1541333333333332,
      "grad_norm": 0.02857949025928974,
      "learning_rate": 4.278666666666667e-05,
      "loss": 0.0037,
      "step": 21640
    },
    {
      "epoch": 1.1546666666666667,
      "grad_norm": 0.22934940457344055,
      "learning_rate": 4.2783333333333334e-05,
      "loss": 0.0026,
      "step": 21650
    },
    {
      "epoch": 1.1552,
      "grad_norm": 0.22944477200508118,
      "learning_rate": 4.278e-05,
      "loss": 0.0042,
      "step": 21660
    },
    {
      "epoch": 1.1557333333333333,
      "grad_norm": 0.14299172163009644,
      "learning_rate": 4.277666666666667e-05,
      "loss": 0.0032,
      "step": 21670
    },
    {
      "epoch": 1.1562666666666668,
      "grad_norm": 0.08577151596546173,
      "learning_rate": 4.277333333333334e-05,
      "loss": 0.0029,
      "step": 21680
    },
    {
      "epoch": 1.1568,
      "grad_norm": 0.11514304578304291,
      "learning_rate": 4.2770000000000006e-05,
      "loss": 0.0035,
      "step": 21690
    },
    {
      "epoch": 1.1573333333333333,
      "grad_norm": 0.0030083926394581795,
      "learning_rate": 4.2766666666666665e-05,
      "loss": 0.0038,
      "step": 21700
    },
    {
      "epoch": 1.1578666666666666,
      "grad_norm": 0.515138566493988,
      "learning_rate": 4.276333333333333e-05,
      "loss": 0.0059,
      "step": 21710
    },
    {
      "epoch": 1.1584,
      "grad_norm": 0.2287472039461136,
      "learning_rate": 4.276e-05,
      "loss": 0.0034,
      "step": 21720
    },
    {
      "epoch": 1.1589333333333334,
      "grad_norm": 0.0062887161038815975,
      "learning_rate": 4.275666666666667e-05,
      "loss": 0.0045,
      "step": 21730
    },
    {
      "epoch": 1.1594666666666666,
      "grad_norm": 0.2585065960884094,
      "learning_rate": 4.2753333333333336e-05,
      "loss": 0.0037,
      "step": 21740
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.2295171022415161,
      "learning_rate": 4.275e-05,
      "loss": 0.0028,
      "step": 21750
    },
    {
      "epoch": 1.1605333333333334,
      "grad_norm": 0.25890910625457764,
      "learning_rate": 4.274666666666667e-05,
      "loss": 0.0037,
      "step": 21760
    },
    {
      "epoch": 1.1610666666666667,
      "grad_norm": 0.08632107824087143,
      "learning_rate": 4.2743333333333335e-05,
      "loss": 0.0038,
      "step": 21770
    },
    {
      "epoch": 1.1616,
      "grad_norm": 0.25837141275405884,
      "learning_rate": 4.274e-05,
      "loss": 0.0043,
      "step": 21780
    },
    {
      "epoch": 1.1621333333333332,
      "grad_norm": 0.028955193236470222,
      "learning_rate": 4.273666666666667e-05,
      "loss": 0.003,
      "step": 21790
    },
    {
      "epoch": 1.1626666666666667,
      "grad_norm": 0.08563654869794846,
      "learning_rate": 4.273333333333333e-05,
      "loss": 0.0041,
      "step": 21800
    },
    {
      "epoch": 1.1632,
      "grad_norm": 0.057534243911504745,
      "learning_rate": 4.2730000000000006e-05,
      "loss": 0.0032,
      "step": 21810
    },
    {
      "epoch": 1.1637333333333333,
      "grad_norm": 0.08743011951446533,
      "learning_rate": 4.272666666666667e-05,
      "loss": 0.0033,
      "step": 21820
    },
    {
      "epoch": 1.1642666666666668,
      "grad_norm": 0.48773831129074097,
      "learning_rate": 4.272333333333334e-05,
      "loss": 0.0027,
      "step": 21830
    },
    {
      "epoch": 1.1648,
      "grad_norm": 0.17263010144233704,
      "learning_rate": 4.2720000000000004e-05,
      "loss": 0.0038,
      "step": 21840
    },
    {
      "epoch": 1.1653333333333333,
      "grad_norm": 0.11554862558841705,
      "learning_rate": 4.2716666666666664e-05,
      "loss": 0.0034,
      "step": 21850
    },
    {
      "epoch": 1.1658666666666666,
      "grad_norm": 0.11529143899679184,
      "learning_rate": 4.271333333333333e-05,
      "loss": 0.0026,
      "step": 21860
    },
    {
      "epoch": 1.1663999999999999,
      "grad_norm": 0.02861602045595646,
      "learning_rate": 4.271e-05,
      "loss": 0.003,
      "step": 21870
    },
    {
      "epoch": 1.1669333333333334,
      "grad_norm": 0.030199632048606873,
      "learning_rate": 4.270666666666667e-05,
      "loss": 0.003,
      "step": 21880
    },
    {
      "epoch": 1.1674666666666667,
      "grad_norm": 0.4021783173084259,
      "learning_rate": 4.2703333333333335e-05,
      "loss": 0.0024,
      "step": 21890
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.8727509379386902,
      "learning_rate": 4.27e-05,
      "loss": 0.0044,
      "step": 21900
    },
    {
      "epoch": 1.1685333333333334,
      "grad_norm": 0.11495218425989151,
      "learning_rate": 4.269666666666667e-05,
      "loss": 0.0046,
      "step": 21910
    },
    {
      "epoch": 1.1690666666666667,
      "grad_norm": 0.22962389886379242,
      "learning_rate": 4.2693333333333333e-05,
      "loss": 0.0035,
      "step": 21920
    },
    {
      "epoch": 1.1696,
      "grad_norm": 0.25777196884155273,
      "learning_rate": 4.269e-05,
      "loss": 0.0026,
      "step": 21930
    },
    {
      "epoch": 1.1701333333333332,
      "grad_norm": 0.2004198431968689,
      "learning_rate": 4.268666666666667e-05,
      "loss": 0.0027,
      "step": 21940
    },
    {
      "epoch": 1.1706666666666667,
      "grad_norm": 0.2862585783004761,
      "learning_rate": 4.268333333333334e-05,
      "loss": 0.0029,
      "step": 21950
    },
    {
      "epoch": 1.1712,
      "grad_norm": 0.17314082384109497,
      "learning_rate": 4.2680000000000005e-05,
      "loss": 0.0024,
      "step": 21960
    },
    {
      "epoch": 1.1717333333333333,
      "grad_norm": 0.1724785715341568,
      "learning_rate": 4.267666666666667e-05,
      "loss": 0.0031,
      "step": 21970
    },
    {
      "epoch": 1.1722666666666668,
      "grad_norm": 0.05734952911734581,
      "learning_rate": 4.267333333333334e-05,
      "loss": 0.0039,
      "step": 21980
    },
    {
      "epoch": 1.1728,
      "grad_norm": 0.31568095088005066,
      "learning_rate": 4.267e-05,
      "loss": 0.0027,
      "step": 21990
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 0.05769360065460205,
      "learning_rate": 4.266666666666667e-05,
      "loss": 0.0026,
      "step": 22000
    },
    {
      "epoch": 1.1738666666666666,
      "grad_norm": 0.11409781873226166,
      "learning_rate": 4.2663333333333335e-05,
      "loss": 0.003,
      "step": 22010
    },
    {
      "epoch": 1.1743999999999999,
      "grad_norm": 0.1152074784040451,
      "learning_rate": 4.266e-05,
      "loss": 0.0046,
      "step": 22020
    },
    {
      "epoch": 1.1749333333333334,
      "grad_norm": 0.23026566207408905,
      "learning_rate": 4.265666666666667e-05,
      "loss": 0.0031,
      "step": 22030
    },
    {
      "epoch": 1.1754666666666667,
      "grad_norm": 0.08662545680999756,
      "learning_rate": 4.2653333333333334e-05,
      "loss": 0.0049,
      "step": 22040
    },
    {
      "epoch": 1.176,
      "grad_norm": 0.11531110852956772,
      "learning_rate": 4.265e-05,
      "loss": 0.0044,
      "step": 22050
    },
    {
      "epoch": 1.1765333333333334,
      "grad_norm": 0.08580587804317474,
      "learning_rate": 4.2646666666666666e-05,
      "loss": 0.004,
      "step": 22060
    },
    {
      "epoch": 1.1770666666666667,
      "grad_norm": 0.17154428362846375,
      "learning_rate": 4.264333333333333e-05,
      "loss": 0.0043,
      "step": 22070
    },
    {
      "epoch": 1.1776,
      "grad_norm": 0.5466115474700928,
      "learning_rate": 4.2640000000000005e-05,
      "loss": 0.0029,
      "step": 22080
    },
    {
      "epoch": 1.1781333333333333,
      "grad_norm": 0.2626023590564728,
      "learning_rate": 4.263666666666667e-05,
      "loss": 0.0032,
      "step": 22090
    },
    {
      "epoch": 1.1786666666666668,
      "grad_norm": 0.4364142417907715,
      "learning_rate": 4.263333333333334e-05,
      "loss": 0.0032,
      "step": 22100
    },
    {
      "epoch": 1.1792,
      "grad_norm": 0.49799129366874695,
      "learning_rate": 4.2630000000000004e-05,
      "loss": 0.0038,
      "step": 22110
    },
    {
      "epoch": 1.1797333333333333,
      "grad_norm": 0.1232457235455513,
      "learning_rate": 4.262666666666667e-05,
      "loss": 0.0042,
      "step": 22120
    },
    {
      "epoch": 1.1802666666666666,
      "grad_norm": 0.08980584144592285,
      "learning_rate": 4.2623333333333336e-05,
      "loss": 0.0031,
      "step": 22130
    },
    {
      "epoch": 1.1808,
      "grad_norm": 0.1469097137451172,
      "learning_rate": 4.262e-05,
      "loss": 0.0046,
      "step": 22140
    },
    {
      "epoch": 1.1813333333333333,
      "grad_norm": 0.17448386549949646,
      "learning_rate": 4.261666666666667e-05,
      "loss": 0.0043,
      "step": 22150
    },
    {
      "epoch": 1.1818666666666666,
      "grad_norm": 0.40513765811920166,
      "learning_rate": 4.2613333333333334e-05,
      "loss": 0.0033,
      "step": 22160
    },
    {
      "epoch": 1.1824,
      "grad_norm": 0.3764285743236542,
      "learning_rate": 4.261e-05,
      "loss": 0.003,
      "step": 22170
    },
    {
      "epoch": 1.1829333333333334,
      "grad_norm": 0.14482247829437256,
      "learning_rate": 4.2606666666666666e-05,
      "loss": 0.0031,
      "step": 22180
    },
    {
      "epoch": 1.1834666666666667,
      "grad_norm": 0.2889031767845154,
      "learning_rate": 4.260333333333333e-05,
      "loss": 0.0035,
      "step": 22190
    },
    {
      "epoch": 1.184,
      "grad_norm": 0.029008831828832626,
      "learning_rate": 4.26e-05,
      "loss": 0.0026,
      "step": 22200
    },
    {
      "epoch": 1.1845333333333334,
      "grad_norm": 0.05779186636209488,
      "learning_rate": 4.2596666666666665e-05,
      "loss": 0.0053,
      "step": 22210
    },
    {
      "epoch": 1.1850666666666667,
      "grad_norm": 0.4039556682109833,
      "learning_rate": 4.259333333333334e-05,
      "loss": 0.0024,
      "step": 22220
    },
    {
      "epoch": 1.1856,
      "grad_norm": 0.08640535175800323,
      "learning_rate": 4.2590000000000004e-05,
      "loss": 0.0041,
      "step": 22230
    },
    {
      "epoch": 1.1861333333333333,
      "grad_norm": 0.3454713821411133,
      "learning_rate": 4.258666666666667e-05,
      "loss": 0.0032,
      "step": 22240
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 0.3173712491989136,
      "learning_rate": 4.2583333333333336e-05,
      "loss": 0.0025,
      "step": 22250
    },
    {
      "epoch": 1.1872,
      "grad_norm": 0.2300965040922165,
      "learning_rate": 4.258e-05,
      "loss": 0.0035,
      "step": 22260
    },
    {
      "epoch": 1.1877333333333333,
      "grad_norm": 0.34409236907958984,
      "learning_rate": 4.257666666666667e-05,
      "loss": 0.0027,
      "step": 22270
    },
    {
      "epoch": 1.1882666666666666,
      "grad_norm": 0.0891151875257492,
      "learning_rate": 4.2573333333333335e-05,
      "loss": 0.0037,
      "step": 22280
    },
    {
      "epoch": 1.1888,
      "grad_norm": 0.3447863459587097,
      "learning_rate": 4.257000000000001e-05,
      "loss": 0.0047,
      "step": 22290
    },
    {
      "epoch": 1.1893333333333334,
      "grad_norm": 0.05724100023508072,
      "learning_rate": 4.2566666666666674e-05,
      "loss": 0.0037,
      "step": 22300
    },
    {
      "epoch": 1.1898666666666666,
      "grad_norm": 0.11469556391239166,
      "learning_rate": 4.256333333333333e-05,
      "loss": 0.0038,
      "step": 22310
    },
    {
      "epoch": 1.1904,
      "grad_norm": 0.3752601146697998,
      "learning_rate": 4.256e-05,
      "loss": 0.0036,
      "step": 22320
    },
    {
      "epoch": 1.1909333333333334,
      "grad_norm": 0.043230090290308,
      "learning_rate": 4.2556666666666665e-05,
      "loss": 0.0024,
      "step": 22330
    },
    {
      "epoch": 1.1914666666666667,
      "grad_norm": 0.4345267117023468,
      "learning_rate": 4.255333333333333e-05,
      "loss": 0.0035,
      "step": 22340
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.011324002407491207,
      "learning_rate": 4.2550000000000004e-05,
      "loss": 0.0036,
      "step": 22350
    },
    {
      "epoch": 1.1925333333333334,
      "grad_norm": 0.08876407146453857,
      "learning_rate": 4.254666666666667e-05,
      "loss": 0.004,
      "step": 22360
    },
    {
      "epoch": 1.1930666666666667,
      "grad_norm": 0.20206964015960693,
      "learning_rate": 4.2543333333333337e-05,
      "loss": 0.0034,
      "step": 22370
    },
    {
      "epoch": 1.1936,
      "grad_norm": 0.031033098697662354,
      "learning_rate": 4.254e-05,
      "loss": 0.0028,
      "step": 22380
    },
    {
      "epoch": 1.1941333333333333,
      "grad_norm": 0.4648074507713318,
      "learning_rate": 4.253666666666667e-05,
      "loss": 0.0032,
      "step": 22390
    },
    {
      "epoch": 1.1946666666666665,
      "grad_norm": 1.099221110343933,
      "learning_rate": 4.2533333333333335e-05,
      "loss": 0.0042,
      "step": 22400
    },
    {
      "epoch": 1.1952,
      "grad_norm": 0.11564445495605469,
      "learning_rate": 4.253e-05,
      "loss": 0.0043,
      "step": 22410
    },
    {
      "epoch": 1.1957333333333333,
      "grad_norm": 0.20385390520095825,
      "learning_rate": 4.252666666666667e-05,
      "loss": 0.0052,
      "step": 22420
    },
    {
      "epoch": 1.1962666666666666,
      "grad_norm": 0.5875281691551208,
      "learning_rate": 4.252333333333334e-05,
      "loss": 0.0041,
      "step": 22430
    },
    {
      "epoch": 1.1968,
      "grad_norm": 0.1163257360458374,
      "learning_rate": 4.2520000000000006e-05,
      "loss": 0.0042,
      "step": 22440
    },
    {
      "epoch": 1.1973333333333334,
      "grad_norm": 0.3489842712879181,
      "learning_rate": 4.251666666666667e-05,
      "loss": 0.0032,
      "step": 22450
    },
    {
      "epoch": 1.1978666666666666,
      "grad_norm": 0.2906658947467804,
      "learning_rate": 4.251333333333333e-05,
      "loss": 0.004,
      "step": 22460
    },
    {
      "epoch": 1.1984,
      "grad_norm": 0.20345154404640198,
      "learning_rate": 4.251e-05,
      "loss": 0.0035,
      "step": 22470
    },
    {
      "epoch": 1.1989333333333334,
      "grad_norm": 0.08732590079307556,
      "learning_rate": 4.2506666666666664e-05,
      "loss": 0.0034,
      "step": 22480
    },
    {
      "epoch": 1.1994666666666667,
      "grad_norm": 0.03060668334364891,
      "learning_rate": 4.250333333333334e-05,
      "loss": 0.0028,
      "step": 22490
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.009639987722039223,
      "learning_rate": 4.25e-05,
      "loss": 0.0041,
      "step": 22500
    },
    {
      "epoch": 1.2005333333333335,
      "grad_norm": 0.2646462917327881,
      "learning_rate": 4.249666666666667e-05,
      "loss": 0.004,
      "step": 22510
    },
    {
      "epoch": 1.2010666666666667,
      "grad_norm": 0.0868082344532013,
      "learning_rate": 4.2493333333333335e-05,
      "loss": 0.004,
      "step": 22520
    },
    {
      "epoch": 1.2016,
      "grad_norm": 0.14542710781097412,
      "learning_rate": 4.249e-05,
      "loss": 0.0037,
      "step": 22530
    },
    {
      "epoch": 1.2021333333333333,
      "grad_norm": 0.2610163688659668,
      "learning_rate": 4.248666666666667e-05,
      "loss": 0.0038,
      "step": 22540
    },
    {
      "epoch": 1.2026666666666666,
      "grad_norm": 0.1751118004322052,
      "learning_rate": 4.2483333333333334e-05,
      "loss": 0.0029,
      "step": 22550
    },
    {
      "epoch": 1.2032,
      "grad_norm": 0.17430905997753143,
      "learning_rate": 4.248e-05,
      "loss": 0.0046,
      "step": 22560
    },
    {
      "epoch": 1.2037333333333333,
      "grad_norm": 0.3495011329650879,
      "learning_rate": 4.247666666666667e-05,
      "loss": 0.0036,
      "step": 22570
    },
    {
      "epoch": 1.2042666666666666,
      "grad_norm": 0.08749104291200638,
      "learning_rate": 4.247333333333334e-05,
      "loss": 0.0032,
      "step": 22580
    },
    {
      "epoch": 1.2048,
      "grad_norm": 0.17483529448509216,
      "learning_rate": 4.2470000000000005e-05,
      "loss": 0.0023,
      "step": 22590
    },
    {
      "epoch": 1.2053333333333334,
      "grad_norm": 0.0576624795794487,
      "learning_rate": 4.246666666666667e-05,
      "loss": 0.0022,
      "step": 22600
    },
    {
      "epoch": 1.2058666666666666,
      "grad_norm": 0.1735525131225586,
      "learning_rate": 4.246333333333333e-05,
      "loss": 0.0023,
      "step": 22610
    },
    {
      "epoch": 1.2064,
      "grad_norm": 0.05876948684453964,
      "learning_rate": 4.246e-05,
      "loss": 0.0033,
      "step": 22620
    },
    {
      "epoch": 1.2069333333333334,
      "grad_norm": 0.00512390723451972,
      "learning_rate": 4.245666666666667e-05,
      "loss": 0.0036,
      "step": 22630
    },
    {
      "epoch": 1.2074666666666667,
      "grad_norm": 0.2869285047054291,
      "learning_rate": 4.2453333333333336e-05,
      "loss": 0.0041,
      "step": 22640
    },
    {
      "epoch": 1.208,
      "grad_norm": 0.9229827523231506,
      "learning_rate": 4.245e-05,
      "loss": 0.0033,
      "step": 22650
    },
    {
      "epoch": 1.2085333333333332,
      "grad_norm": 0.20023538172245026,
      "learning_rate": 4.244666666666667e-05,
      "loss": 0.0021,
      "step": 22660
    },
    {
      "epoch": 1.2090666666666667,
      "grad_norm": 0.05773815140128136,
      "learning_rate": 4.2443333333333334e-05,
      "loss": 0.0032,
      "step": 22670
    },
    {
      "epoch": 1.2096,
      "grad_norm": 0.1722627878189087,
      "learning_rate": 4.244e-05,
      "loss": 0.0031,
      "step": 22680
    },
    {
      "epoch": 1.2101333333333333,
      "grad_norm": 0.02882605604827404,
      "learning_rate": 4.2436666666666666e-05,
      "loss": 0.0041,
      "step": 22690
    },
    {
      "epoch": 1.2106666666666666,
      "grad_norm": 0.2871396541595459,
      "learning_rate": 4.243333333333334e-05,
      "loss": 0.0038,
      "step": 22700
    },
    {
      "epoch": 1.2112,
      "grad_norm": 0.14408493041992188,
      "learning_rate": 4.2430000000000005e-05,
      "loss": 0.0023,
      "step": 22710
    },
    {
      "epoch": 1.2117333333333333,
      "grad_norm": 0.31580349802970886,
      "learning_rate": 4.242666666666667e-05,
      "loss": 0.0034,
      "step": 22720
    },
    {
      "epoch": 1.2122666666666666,
      "grad_norm": 0.030801299959421158,
      "learning_rate": 4.242333333333334e-05,
      "loss": 0.0047,
      "step": 22730
    },
    {
      "epoch": 1.2128,
      "grad_norm": 0.11624231189489365,
      "learning_rate": 4.2420000000000004e-05,
      "loss": 0.0047,
      "step": 22740
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.012252653017640114,
      "learning_rate": 4.241666666666667e-05,
      "loss": 0.002,
      "step": 22750
    },
    {
      "epoch": 1.2138666666666666,
      "grad_norm": 0.22936230897903442,
      "learning_rate": 4.241333333333333e-05,
      "loss": 0.0024,
      "step": 22760
    },
    {
      "epoch": 1.2144,
      "grad_norm": 0.40317341685295105,
      "learning_rate": 4.241e-05,
      "loss": 0.0035,
      "step": 22770
    },
    {
      "epoch": 1.2149333333333334,
      "grad_norm": 0.2016989290714264,
      "learning_rate": 4.240666666666667e-05,
      "loss": 0.0042,
      "step": 22780
    },
    {
      "epoch": 1.2154666666666667,
      "grad_norm": 0.060006361454725266,
      "learning_rate": 4.2403333333333334e-05,
      "loss": 0.0032,
      "step": 22790
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.17314931750297546,
      "learning_rate": 4.24e-05,
      "loss": 0.0045,
      "step": 22800
    },
    {
      "epoch": 1.2165333333333332,
      "grad_norm": 0.05733887478709221,
      "learning_rate": 4.239666666666667e-05,
      "loss": 0.0032,
      "step": 22810
    },
    {
      "epoch": 1.2170666666666667,
      "grad_norm": 0.28797435760498047,
      "learning_rate": 4.239333333333333e-05,
      "loss": 0.0019,
      "step": 22820
    },
    {
      "epoch": 1.2176,
      "grad_norm": 0.11577215045690536,
      "learning_rate": 4.239e-05,
      "loss": 0.0017,
      "step": 22830
    },
    {
      "epoch": 1.2181333333333333,
      "grad_norm": 0.17210106551647186,
      "learning_rate": 4.238666666666667e-05,
      "loss": 0.0027,
      "step": 22840
    },
    {
      "epoch": 1.2186666666666666,
      "grad_norm": 0.31600287556648254,
      "learning_rate": 4.238333333333334e-05,
      "loss": 0.0031,
      "step": 22850
    },
    {
      "epoch": 1.2192,
      "grad_norm": 0.20074790716171265,
      "learning_rate": 4.2380000000000004e-05,
      "loss": 0.0017,
      "step": 22860
    },
    {
      "epoch": 1.2197333333333333,
      "grad_norm": 0.2589223384857178,
      "learning_rate": 4.237666666666667e-05,
      "loss": 0.004,
      "step": 22870
    },
    {
      "epoch": 1.2202666666666666,
      "grad_norm": 0.11475739628076553,
      "learning_rate": 4.2373333333333336e-05,
      "loss": 0.0038,
      "step": 22880
    },
    {
      "epoch": 1.2208,
      "grad_norm": 0.1439986675977707,
      "learning_rate": 4.237e-05,
      "loss": 0.0037,
      "step": 22890
    },
    {
      "epoch": 1.2213333333333334,
      "grad_norm": 0.3730660378932953,
      "learning_rate": 4.236666666666667e-05,
      "loss": 0.0034,
      "step": 22900
    },
    {
      "epoch": 1.2218666666666667,
      "grad_norm": 0.17308072745800018,
      "learning_rate": 4.2363333333333335e-05,
      "loss": 0.0047,
      "step": 22910
    },
    {
      "epoch": 1.2224,
      "grad_norm": 0.17233452200889587,
      "learning_rate": 4.236e-05,
      "loss": 0.0036,
      "step": 22920
    },
    {
      "epoch": 1.2229333333333334,
      "grad_norm": 0.029260428622364998,
      "learning_rate": 4.235666666666667e-05,
      "loss": 0.0039,
      "step": 22930
    },
    {
      "epoch": 1.2234666666666667,
      "grad_norm": 0.25769051909446716,
      "learning_rate": 4.235333333333333e-05,
      "loss": 0.0027,
      "step": 22940
    },
    {
      "epoch": 1.224,
      "grad_norm": 0.029130589216947556,
      "learning_rate": 4.235e-05,
      "loss": 0.0033,
      "step": 22950
    },
    {
      "epoch": 1.2245333333333333,
      "grad_norm": 0.05735271796584129,
      "learning_rate": 4.2346666666666666e-05,
      "loss": 0.0032,
      "step": 22960
    },
    {
      "epoch": 1.2250666666666667,
      "grad_norm": 0.08631213009357452,
      "learning_rate": 4.234333333333333e-05,
      "loss": 0.0031,
      "step": 22970
    },
    {
      "epoch": 1.2256,
      "grad_norm": 0.2581300735473633,
      "learning_rate": 4.2340000000000005e-05,
      "loss": 0.0026,
      "step": 22980
    },
    {
      "epoch": 1.2261333333333333,
      "grad_norm": 0.028752287849783897,
      "learning_rate": 4.233666666666667e-05,
      "loss": 0.0038,
      "step": 22990
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 0.08630836009979248,
      "learning_rate": 4.233333333333334e-05,
      "loss": 0.0028,
      "step": 23000
    },
    {
      "epoch": 1.2272,
      "grad_norm": 0.43231016397476196,
      "learning_rate": 4.233e-05,
      "loss": 0.0024,
      "step": 23010
    },
    {
      "epoch": 1.2277333333333333,
      "grad_norm": 0.00419287895783782,
      "learning_rate": 4.232666666666667e-05,
      "loss": 0.0027,
      "step": 23020
    },
    {
      "epoch": 1.2282666666666666,
      "grad_norm": 0.05839495733380318,
      "learning_rate": 4.2323333333333335e-05,
      "loss": 0.0037,
      "step": 23030
    },
    {
      "epoch": 1.2288000000000001,
      "grad_norm": 0.3158191442489624,
      "learning_rate": 4.232e-05,
      "loss": 0.0023,
      "step": 23040
    },
    {
      "epoch": 1.2293333333333334,
      "grad_norm": 0.05727734416723251,
      "learning_rate": 4.2316666666666674e-05,
      "loss": 0.0027,
      "step": 23050
    },
    {
      "epoch": 1.2298666666666667,
      "grad_norm": 0.2865109443664551,
      "learning_rate": 4.2313333333333334e-05,
      "loss": 0.0026,
      "step": 23060
    },
    {
      "epoch": 1.2304,
      "grad_norm": 0.17163479328155518,
      "learning_rate": 4.231e-05,
      "loss": 0.0043,
      "step": 23070
    },
    {
      "epoch": 1.2309333333333332,
      "grad_norm": 0.5737985372543335,
      "learning_rate": 4.2306666666666666e-05,
      "loss": 0.0018,
      "step": 23080
    },
    {
      "epoch": 1.2314666666666667,
      "grad_norm": 0.028970591723918915,
      "learning_rate": 4.230333333333333e-05,
      "loss": 0.0033,
      "step": 23090
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.08587388694286346,
      "learning_rate": 4.23e-05,
      "loss": 0.003,
      "step": 23100
    },
    {
      "epoch": 1.2325333333333333,
      "grad_norm": 0.08654093742370605,
      "learning_rate": 4.229666666666667e-05,
      "loss": 0.0045,
      "step": 23110
    },
    {
      "epoch": 1.2330666666666668,
      "grad_norm": 0.1720133274793625,
      "learning_rate": 4.229333333333334e-05,
      "loss": 0.0027,
      "step": 23120
    },
    {
      "epoch": 1.2336,
      "grad_norm": 0.1442992091178894,
      "learning_rate": 4.229e-05,
      "loss": 0.0027,
      "step": 23130
    },
    {
      "epoch": 1.2341333333333333,
      "grad_norm": 0.14563871920108795,
      "learning_rate": 4.228666666666667e-05,
      "loss": 0.003,
      "step": 23140
    },
    {
      "epoch": 1.2346666666666666,
      "grad_norm": 0.08664820343255997,
      "learning_rate": 4.2283333333333336e-05,
      "loss": 0.0036,
      "step": 23150
    },
    {
      "epoch": 1.2352,
      "grad_norm": 0.11641484498977661,
      "learning_rate": 4.228e-05,
      "loss": 0.0022,
      "step": 23160
    },
    {
      "epoch": 1.2357333333333334,
      "grad_norm": 0.05896766483783722,
      "learning_rate": 4.227666666666667e-05,
      "loss": 0.0034,
      "step": 23170
    },
    {
      "epoch": 1.2362666666666666,
      "grad_norm": 0.11499886959791183,
      "learning_rate": 4.2273333333333334e-05,
      "loss": 0.0025,
      "step": 23180
    },
    {
      "epoch": 1.2368000000000001,
      "grad_norm": 0.43007272481918335,
      "learning_rate": 4.227000000000001e-05,
      "loss": 0.0042,
      "step": 23190
    },
    {
      "epoch": 1.2373333333333334,
      "grad_norm": 0.028733590617775917,
      "learning_rate": 4.226666666666667e-05,
      "loss": 0.0052,
      "step": 23200
    },
    {
      "epoch": 1.2378666666666667,
      "grad_norm": 0.5739597678184509,
      "learning_rate": 4.226333333333334e-05,
      "loss": 0.0038,
      "step": 23210
    },
    {
      "epoch": 1.2384,
      "grad_norm": 0.02932065725326538,
      "learning_rate": 4.226e-05,
      "loss": 0.004,
      "step": 23220
    },
    {
      "epoch": 1.2389333333333332,
      "grad_norm": 0.11589137464761734,
      "learning_rate": 4.2256666666666665e-05,
      "loss": 0.0022,
      "step": 23230
    },
    {
      "epoch": 1.2394666666666667,
      "grad_norm": 0.3906399607658386,
      "learning_rate": 4.225333333333333e-05,
      "loss": 0.0025,
      "step": 23240
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.14379575848579407,
      "learning_rate": 4.2250000000000004e-05,
      "loss": 0.0041,
      "step": 23250
    },
    {
      "epoch": 1.2405333333333333,
      "grad_norm": 0.08780869096517563,
      "learning_rate": 4.224666666666667e-05,
      "loss": 0.0045,
      "step": 23260
    },
    {
      "epoch": 1.2410666666666668,
      "grad_norm": 0.11509107798337936,
      "learning_rate": 4.2243333333333336e-05,
      "loss": 0.0026,
      "step": 23270
    },
    {
      "epoch": 1.2416,
      "grad_norm": 0.005876763258129358,
      "learning_rate": 4.224e-05,
      "loss": 0.0038,
      "step": 23280
    },
    {
      "epoch": 1.2421333333333333,
      "grad_norm": 0.11489013582468033,
      "learning_rate": 4.223666666666667e-05,
      "loss": 0.0041,
      "step": 23290
    },
    {
      "epoch": 1.2426666666666666,
      "grad_norm": 0.20163269340991974,
      "learning_rate": 4.2233333333333334e-05,
      "loss": 0.003,
      "step": 23300
    },
    {
      "epoch": 1.2432,
      "grad_norm": 0.029028093442320824,
      "learning_rate": 4.223e-05,
      "loss": 0.0034,
      "step": 23310
    },
    {
      "epoch": 1.2437333333333334,
      "grad_norm": 0.2580101191997528,
      "learning_rate": 4.222666666666667e-05,
      "loss": 0.0038,
      "step": 23320
    },
    {
      "epoch": 1.2442666666666666,
      "grad_norm": 0.029997991397976875,
      "learning_rate": 4.222333333333334e-05,
      "loss": 0.0046,
      "step": 23330
    },
    {
      "epoch": 1.2448,
      "grad_norm": 0.48926493525505066,
      "learning_rate": 4.2220000000000006e-05,
      "loss": 0.0032,
      "step": 23340
    },
    {
      "epoch": 1.2453333333333334,
      "grad_norm": 0.05762123689055443,
      "learning_rate": 4.221666666666667e-05,
      "loss": 0.0033,
      "step": 23350
    },
    {
      "epoch": 1.2458666666666667,
      "grad_norm": 0.028596948832273483,
      "learning_rate": 4.221333333333334e-05,
      "loss": 0.0024,
      "step": 23360
    },
    {
      "epoch": 1.2464,
      "grad_norm": 0.08649735897779465,
      "learning_rate": 4.221e-05,
      "loss": 0.0033,
      "step": 23370
    },
    {
      "epoch": 1.2469333333333332,
      "grad_norm": 0.028749840334057808,
      "learning_rate": 4.2206666666666663e-05,
      "loss": 0.0023,
      "step": 23380
    },
    {
      "epoch": 1.2474666666666667,
      "grad_norm": 0.40144452452659607,
      "learning_rate": 4.2203333333333336e-05,
      "loss": 0.0023,
      "step": 23390
    },
    {
      "epoch": 1.248,
      "grad_norm": 0.34453248977661133,
      "learning_rate": 4.22e-05,
      "loss": 0.0043,
      "step": 23400
    },
    {
      "epoch": 1.2485333333333333,
      "grad_norm": 0.25855347514152527,
      "learning_rate": 4.219666666666667e-05,
      "loss": 0.0034,
      "step": 23410
    },
    {
      "epoch": 1.2490666666666668,
      "grad_norm": 0.14383353292942047,
      "learning_rate": 4.2193333333333335e-05,
      "loss": 0.0029,
      "step": 23420
    },
    {
      "epoch": 1.2496,
      "grad_norm": 0.08675719052553177,
      "learning_rate": 4.219e-05,
      "loss": 0.0015,
      "step": 23430
    },
    {
      "epoch": 1.2501333333333333,
      "grad_norm": 0.43005669116973877,
      "learning_rate": 4.218666666666667e-05,
      "loss": 0.0037,
      "step": 23440
    },
    {
      "epoch": 1.2506666666666666,
      "grad_norm": 0.11489470303058624,
      "learning_rate": 4.218333333333333e-05,
      "loss": 0.0057,
      "step": 23450
    },
    {
      "epoch": 1.2511999999999999,
      "grad_norm": 0.058333855122327805,
      "learning_rate": 4.2180000000000006e-05,
      "loss": 0.0044,
      "step": 23460
    },
    {
      "epoch": 1.2517333333333334,
      "grad_norm": 0.32021281123161316,
      "learning_rate": 4.217666666666667e-05,
      "loss": 0.0039,
      "step": 23470
    },
    {
      "epoch": 1.2522666666666666,
      "grad_norm": 0.29042842984199524,
      "learning_rate": 4.217333333333334e-05,
      "loss": 0.0033,
      "step": 23480
    },
    {
      "epoch": 1.2528000000000001,
      "grad_norm": 0.23351886868476868,
      "learning_rate": 4.2170000000000005e-05,
      "loss": 0.0031,
      "step": 23490
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 0.08688116073608398,
      "learning_rate": 4.216666666666667e-05,
      "loss": 0.0032,
      "step": 23500
    },
    {
      "epoch": 1.2538666666666667,
      "grad_norm": 0.2609323263168335,
      "learning_rate": 4.216333333333334e-05,
      "loss": 0.0035,
      "step": 23510
    },
    {
      "epoch": 1.2544,
      "grad_norm": 0.35052555799484253,
      "learning_rate": 4.2159999999999996e-05,
      "loss": 0.003,
      "step": 23520
    },
    {
      "epoch": 1.2549333333333332,
      "grad_norm": 0.616927444934845,
      "learning_rate": 4.215666666666667e-05,
      "loss": 0.0034,
      "step": 23530
    },
    {
      "epoch": 1.2554666666666667,
      "grad_norm": 0.05986570194363594,
      "learning_rate": 4.2153333333333335e-05,
      "loss": 0.0033,
      "step": 23540
    },
    {
      "epoch": 1.256,
      "grad_norm": 0.14333388209342957,
      "learning_rate": 4.215e-05,
      "loss": 0.004,
      "step": 23550
    },
    {
      "epoch": 1.2565333333333333,
      "grad_norm": 0.6619560122489929,
      "learning_rate": 4.214666666666667e-05,
      "loss": 0.0032,
      "step": 23560
    },
    {
      "epoch": 1.2570666666666668,
      "grad_norm": 0.4045974612236023,
      "learning_rate": 4.2143333333333334e-05,
      "loss": 0.0043,
      "step": 23570
    },
    {
      "epoch": 1.2576,
      "grad_norm": 0.35947540402412415,
      "learning_rate": 4.214e-05,
      "loss": 0.0044,
      "step": 23580
    },
    {
      "epoch": 1.2581333333333333,
      "grad_norm": 0.32013097405433655,
      "learning_rate": 4.2136666666666666e-05,
      "loss": 0.0033,
      "step": 23590
    },
    {
      "epoch": 1.2586666666666666,
      "grad_norm": 0.4903971552848816,
      "learning_rate": 4.213333333333334e-05,
      "loss": 0.0044,
      "step": 23600
    },
    {
      "epoch": 1.2591999999999999,
      "grad_norm": 0.23125317692756653,
      "learning_rate": 4.2130000000000005e-05,
      "loss": 0.004,
      "step": 23610
    },
    {
      "epoch": 1.2597333333333334,
      "grad_norm": 0.11562056094408035,
      "learning_rate": 4.212666666666667e-05,
      "loss": 0.0023,
      "step": 23620
    },
    {
      "epoch": 1.2602666666666666,
      "grad_norm": 0.46459245681762695,
      "learning_rate": 4.212333333333334e-05,
      "loss": 0.0025,
      "step": 23630
    },
    {
      "epoch": 1.2608,
      "grad_norm": 0.20307445526123047,
      "learning_rate": 4.212e-05,
      "loss": 0.003,
      "step": 23640
    },
    {
      "epoch": 1.2613333333333334,
      "grad_norm": 0.11618483811616898,
      "learning_rate": 4.211666666666667e-05,
      "loss": 0.0033,
      "step": 23650
    },
    {
      "epoch": 1.2618666666666667,
      "grad_norm": 0.1473190188407898,
      "learning_rate": 4.2113333333333336e-05,
      "loss": 0.0044,
      "step": 23660
    },
    {
      "epoch": 1.2624,
      "grad_norm": 1.7084720134735107,
      "learning_rate": 4.211e-05,
      "loss": 0.0036,
      "step": 23670
    },
    {
      "epoch": 1.2629333333333332,
      "grad_norm": 0.028987208381295204,
      "learning_rate": 4.210666666666667e-05,
      "loss": 0.0027,
      "step": 23680
    },
    {
      "epoch": 1.2634666666666667,
      "grad_norm": 0.39312776923179626,
      "learning_rate": 4.2103333333333334e-05,
      "loss": 0.0045,
      "step": 23690
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.20180195569992065,
      "learning_rate": 4.21e-05,
      "loss": 0.0032,
      "step": 23700
    },
    {
      "epoch": 1.2645333333333333,
      "grad_norm": 0.08656633645296097,
      "learning_rate": 4.2096666666666666e-05,
      "loss": 0.0042,
      "step": 23710
    },
    {
      "epoch": 1.2650666666666668,
      "grad_norm": 1.9477108716964722,
      "learning_rate": 4.209333333333333e-05,
      "loss": 0.0028,
      "step": 23720
    },
    {
      "epoch": 1.2656,
      "grad_norm": 0.9393738508224487,
      "learning_rate": 4.209e-05,
      "loss": 0.0054,
      "step": 23730
    },
    {
      "epoch": 1.2661333333333333,
      "grad_norm": 0.34594956040382385,
      "learning_rate": 4.208666666666667e-05,
      "loss": 0.0035,
      "step": 23740
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.03377127647399902,
      "learning_rate": 4.208333333333334e-05,
      "loss": 0.0036,
      "step": 23750
    },
    {
      "epoch": 1.2671999999999999,
      "grad_norm": 0.3759010434150696,
      "learning_rate": 4.2080000000000004e-05,
      "loss": 0.0024,
      "step": 23760
    },
    {
      "epoch": 1.2677333333333334,
      "grad_norm": 0.14470982551574707,
      "learning_rate": 4.207666666666667e-05,
      "loss": 0.0034,
      "step": 23770
    },
    {
      "epoch": 1.2682666666666667,
      "grad_norm": 0.0572834238409996,
      "learning_rate": 4.2073333333333336e-05,
      "loss": 0.0057,
      "step": 23780
    },
    {
      "epoch": 1.2688,
      "grad_norm": 0.5240551829338074,
      "learning_rate": 4.207e-05,
      "loss": 0.004,
      "step": 23790
    },
    {
      "epoch": 1.2693333333333334,
      "grad_norm": 0.057353291660547256,
      "learning_rate": 4.206666666666667e-05,
      "loss": 0.0028,
      "step": 23800
    },
    {
      "epoch": 1.2698666666666667,
      "grad_norm": 0.14668318629264832,
      "learning_rate": 4.206333333333334e-05,
      "loss": 0.0048,
      "step": 23810
    },
    {
      "epoch": 1.2704,
      "grad_norm": 0.028837326914072037,
      "learning_rate": 4.206e-05,
      "loss": 0.0055,
      "step": 23820
    },
    {
      "epoch": 1.2709333333333332,
      "grad_norm": 0.14488810300827026,
      "learning_rate": 4.205666666666667e-05,
      "loss": 0.0034,
      "step": 23830
    },
    {
      "epoch": 1.2714666666666667,
      "grad_norm": 0.17299622297286987,
      "learning_rate": 4.205333333333333e-05,
      "loss": 0.0032,
      "step": 23840
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.05816899612545967,
      "learning_rate": 4.205e-05,
      "loss": 0.0037,
      "step": 23850
    },
    {
      "epoch": 1.2725333333333333,
      "grad_norm": 0.46022459864616394,
      "learning_rate": 4.2046666666666665e-05,
      "loss": 0.0031,
      "step": 23860
    },
    {
      "epoch": 1.2730666666666668,
      "grad_norm": 0.17494739592075348,
      "learning_rate": 4.204333333333334e-05,
      "loss": 0.003,
      "step": 23870
    },
    {
      "epoch": 1.2736,
      "grad_norm": 0.20396114885807037,
      "learning_rate": 4.2040000000000004e-05,
      "loss": 0.0039,
      "step": 23880
    },
    {
      "epoch": 1.2741333333333333,
      "grad_norm": 0.5817197561264038,
      "learning_rate": 4.203666666666667e-05,
      "loss": 0.0022,
      "step": 23890
    },
    {
      "epoch": 1.2746666666666666,
      "grad_norm": 0.14587286114692688,
      "learning_rate": 4.2033333333333336e-05,
      "loss": 0.0047,
      "step": 23900
    },
    {
      "epoch": 1.2752,
      "grad_norm": 0.23145055770874023,
      "learning_rate": 4.203e-05,
      "loss": 0.0039,
      "step": 23910
    },
    {
      "epoch": 1.2757333333333334,
      "grad_norm": 0.11559615284204483,
      "learning_rate": 4.202666666666667e-05,
      "loss": 0.0029,
      "step": 23920
    },
    {
      "epoch": 1.2762666666666667,
      "grad_norm": 0.4032007157802582,
      "learning_rate": 4.2023333333333335e-05,
      "loss": 0.0035,
      "step": 23930
    },
    {
      "epoch": 1.2768,
      "grad_norm": 0.05799444764852524,
      "learning_rate": 4.202e-05,
      "loss": 0.003,
      "step": 23940
    },
    {
      "epoch": 1.2773333333333334,
      "grad_norm": 0.6421920657157898,
      "learning_rate": 4.2016666666666674e-05,
      "loss": 0.005,
      "step": 23950
    },
    {
      "epoch": 1.2778666666666667,
      "grad_norm": 0.03265559673309326,
      "learning_rate": 4.201333333333334e-05,
      "loss": 0.0035,
      "step": 23960
    },
    {
      "epoch": 1.2784,
      "grad_norm": 0.346838116645813,
      "learning_rate": 4.201e-05,
      "loss": 0.003,
      "step": 23970
    },
    {
      "epoch": 1.2789333333333333,
      "grad_norm": 0.05750473588705063,
      "learning_rate": 4.2006666666666665e-05,
      "loss": 0.0031,
      "step": 23980
    },
    {
      "epoch": 1.2794666666666665,
      "grad_norm": 0.05743912607431412,
      "learning_rate": 4.200333333333333e-05,
      "loss": 0.0026,
      "step": 23990
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.030167458578944206,
      "learning_rate": 4.2e-05,
      "loss": 0.0036,
      "step": 24000
    },
    {
      "epoch": 1.2805333333333333,
      "grad_norm": 0.1162075623869896,
      "learning_rate": 4.199666666666667e-05,
      "loss": 0.0026,
      "step": 24010
    },
    {
      "epoch": 1.2810666666666668,
      "grad_norm": 0.1727413684129715,
      "learning_rate": 4.199333333333334e-05,
      "loss": 0.003,
      "step": 24020
    },
    {
      "epoch": 1.2816,
      "grad_norm": 0.14489170908927917,
      "learning_rate": 4.199e-05,
      "loss": 0.0029,
      "step": 24030
    },
    {
      "epoch": 1.2821333333333333,
      "grad_norm": 0.34692975878715515,
      "learning_rate": 4.198666666666667e-05,
      "loss": 0.0024,
      "step": 24040
    },
    {
      "epoch": 1.2826666666666666,
      "grad_norm": 0.17291152477264404,
      "learning_rate": 4.1983333333333335e-05,
      "loss": 0.0019,
      "step": 24050
    },
    {
      "epoch": 1.2832,
      "grad_norm": 0.3158910572528839,
      "learning_rate": 4.198e-05,
      "loss": 0.002,
      "step": 24060
    },
    {
      "epoch": 1.2837333333333334,
      "grad_norm": 0.05733053758740425,
      "learning_rate": 4.197666666666667e-05,
      "loss": 0.0039,
      "step": 24070
    },
    {
      "epoch": 1.2842666666666667,
      "grad_norm": 0.37638863921165466,
      "learning_rate": 4.1973333333333334e-05,
      "loss": 0.0024,
      "step": 24080
    },
    {
      "epoch": 1.2848,
      "grad_norm": 0.06037410348653793,
      "learning_rate": 4.1970000000000006e-05,
      "loss": 0.0034,
      "step": 24090
    },
    {
      "epoch": 1.2853333333333334,
      "grad_norm": 0.1735469251871109,
      "learning_rate": 4.196666666666667e-05,
      "loss": 0.0036,
      "step": 24100
    },
    {
      "epoch": 1.2858666666666667,
      "grad_norm": 0.02910648100078106,
      "learning_rate": 4.196333333333334e-05,
      "loss": 0.0028,
      "step": 24110
    },
    {
      "epoch": 1.2864,
      "grad_norm": 0.05726871266961098,
      "learning_rate": 4.196e-05,
      "loss": 0.003,
      "step": 24120
    },
    {
      "epoch": 1.2869333333333333,
      "grad_norm": 0.2019929736852646,
      "learning_rate": 4.1956666666666664e-05,
      "loss": 0.0032,
      "step": 24130
    },
    {
      "epoch": 1.2874666666666665,
      "grad_norm": 0.202453151345253,
      "learning_rate": 4.195333333333333e-05,
      "loss": 0.0025,
      "step": 24140
    },
    {
      "epoch": 1.288,
      "grad_norm": 0.05884413793683052,
      "learning_rate": 4.195e-05,
      "loss": 0.0037,
      "step": 24150
    },
    {
      "epoch": 1.2885333333333333,
      "grad_norm": 0.11498744040727615,
      "learning_rate": 4.194666666666667e-05,
      "loss": 0.0025,
      "step": 24160
    },
    {
      "epoch": 1.2890666666666668,
      "grad_norm": 0.2020130604505539,
      "learning_rate": 4.1943333333333336e-05,
      "loss": 0.0027,
      "step": 24170
    },
    {
      "epoch": 1.2896,
      "grad_norm": 0.404601514339447,
      "learning_rate": 4.194e-05,
      "loss": 0.0043,
      "step": 24180
    },
    {
      "epoch": 1.2901333333333334,
      "grad_norm": 0.1441388577222824,
      "learning_rate": 4.193666666666667e-05,
      "loss": 0.0036,
      "step": 24190
    },
    {
      "epoch": 1.2906666666666666,
      "grad_norm": 0.4039413630962372,
      "learning_rate": 4.1933333333333334e-05,
      "loss": 0.0049,
      "step": 24200
    },
    {
      "epoch": 1.2912,
      "grad_norm": 0.08639877289533615,
      "learning_rate": 4.193e-05,
      "loss": 0.0035,
      "step": 24210
    },
    {
      "epoch": 1.2917333333333334,
      "grad_norm": 0.029262162744998932,
      "learning_rate": 4.192666666666667e-05,
      "loss": 0.0028,
      "step": 24220
    },
    {
      "epoch": 1.2922666666666667,
      "grad_norm": 0.08649476617574692,
      "learning_rate": 4.192333333333334e-05,
      "loss": 0.0035,
      "step": 24230
    },
    {
      "epoch": 1.2928,
      "grad_norm": 0.34619417786598206,
      "learning_rate": 4.1920000000000005e-05,
      "loss": 0.0037,
      "step": 24240
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 0.2882925570011139,
      "learning_rate": 4.191666666666667e-05,
      "loss": 0.0046,
      "step": 24250
    },
    {
      "epoch": 1.2938666666666667,
      "grad_norm": 0.2318112552165985,
      "learning_rate": 4.191333333333334e-05,
      "loss": 0.005,
      "step": 24260
    },
    {
      "epoch": 1.2944,
      "grad_norm": 0.31943631172180176,
      "learning_rate": 4.191e-05,
      "loss": 0.0046,
      "step": 24270
    },
    {
      "epoch": 1.2949333333333333,
      "grad_norm": 0.3477502167224884,
      "learning_rate": 4.190666666666666e-05,
      "loss": 0.0033,
      "step": 24280
    },
    {
      "epoch": 1.2954666666666665,
      "grad_norm": 0.05925033241510391,
      "learning_rate": 4.1903333333333336e-05,
      "loss": 0.0028,
      "step": 24290
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.4030124545097351,
      "learning_rate": 4.19e-05,
      "loss": 0.0037,
      "step": 24300
    },
    {
      "epoch": 1.2965333333333333,
      "grad_norm": 0.1720713973045349,
      "learning_rate": 4.189666666666667e-05,
      "loss": 0.0036,
      "step": 24310
    },
    {
      "epoch": 1.2970666666666666,
      "grad_norm": 0.400648832321167,
      "learning_rate": 4.1893333333333334e-05,
      "loss": 0.0046,
      "step": 24320
    },
    {
      "epoch": 1.2976,
      "grad_norm": 0.14363797008991241,
      "learning_rate": 4.189e-05,
      "loss": 0.004,
      "step": 24330
    },
    {
      "epoch": 1.2981333333333334,
      "grad_norm": 0.2581722140312195,
      "learning_rate": 4.1886666666666667e-05,
      "loss": 0.0035,
      "step": 24340
    },
    {
      "epoch": 1.2986666666666666,
      "grad_norm": 0.40253761410713196,
      "learning_rate": 4.188333333333333e-05,
      "loss": 0.0034,
      "step": 24350
    },
    {
      "epoch": 1.2992,
      "grad_norm": 0.34694939851760864,
      "learning_rate": 4.1880000000000006e-05,
      "loss": 0.0039,
      "step": 24360
    },
    {
      "epoch": 1.2997333333333334,
      "grad_norm": 0.11481871455907822,
      "learning_rate": 4.187666666666667e-05,
      "loss": 0.0054,
      "step": 24370
    },
    {
      "epoch": 1.3002666666666667,
      "grad_norm": 0.23120994865894318,
      "learning_rate": 4.187333333333334e-05,
      "loss": 0.0028,
      "step": 24380
    },
    {
      "epoch": 1.3008,
      "grad_norm": 0.2900581657886505,
      "learning_rate": 4.1870000000000004e-05,
      "loss": 0.0059,
      "step": 24390
    },
    {
      "epoch": 1.3013333333333335,
      "grad_norm": 0.28810685873031616,
      "learning_rate": 4.186666666666667e-05,
      "loss": 0.0042,
      "step": 24400
    },
    {
      "epoch": 1.3018666666666667,
      "grad_norm": 0.23002178966999054,
      "learning_rate": 4.1863333333333336e-05,
      "loss": 0.0042,
      "step": 24410
    },
    {
      "epoch": 1.3024,
      "grad_norm": 0.0033215670846402645,
      "learning_rate": 4.186e-05,
      "loss": 0.003,
      "step": 24420
    },
    {
      "epoch": 1.3029333333333333,
      "grad_norm": 0.057530954480171204,
      "learning_rate": 4.185666666666667e-05,
      "loss": 0.0039,
      "step": 24430
    },
    {
      "epoch": 1.3034666666666666,
      "grad_norm": 0.23275013267993927,
      "learning_rate": 4.1853333333333335e-05,
      "loss": 0.0044,
      "step": 24440
    },
    {
      "epoch": 1.304,
      "grad_norm": 0.11691915988922119,
      "learning_rate": 4.185e-05,
      "loss": 0.0042,
      "step": 24450
    },
    {
      "epoch": 1.3045333333333333,
      "grad_norm": 0.058489616960287094,
      "learning_rate": 4.184666666666667e-05,
      "loss": 0.0029,
      "step": 24460
    },
    {
      "epoch": 1.3050666666666666,
      "grad_norm": 0.2328021377325058,
      "learning_rate": 4.184333333333333e-05,
      "loss": 0.0041,
      "step": 24470
    },
    {
      "epoch": 1.3056,
      "grad_norm": 0.37769103050231934,
      "learning_rate": 4.184e-05,
      "loss": 0.0027,
      "step": 24480
    },
    {
      "epoch": 1.3061333333333334,
      "grad_norm": 0.20407351851463318,
      "learning_rate": 4.1836666666666665e-05,
      "loss": 0.0038,
      "step": 24490
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 0.3208317458629608,
      "learning_rate": 4.183333333333334e-05,
      "loss": 0.0028,
      "step": 24500
    },
    {
      "epoch": 1.3072,
      "grad_norm": 0.29222312569618225,
      "learning_rate": 4.1830000000000004e-05,
      "loss": 0.0028,
      "step": 24510
    },
    {
      "epoch": 1.3077333333333334,
      "grad_norm": 0.23253343999385834,
      "learning_rate": 4.182666666666667e-05,
      "loss": 0.0027,
      "step": 24520
    },
    {
      "epoch": 1.3082666666666667,
      "grad_norm": 0.03253588825464249,
      "learning_rate": 4.182333333333334e-05,
      "loss": 0.0051,
      "step": 24530
    },
    {
      "epoch": 1.3088,
      "grad_norm": 0.17379987239837646,
      "learning_rate": 4.182e-05,
      "loss": 0.0026,
      "step": 24540
    },
    {
      "epoch": 1.3093333333333335,
      "grad_norm": 0.08915286511182785,
      "learning_rate": 4.181666666666667e-05,
      "loss": 0.0033,
      "step": 24550
    },
    {
      "epoch": 1.3098666666666667,
      "grad_norm": 0.34739434719085693,
      "learning_rate": 4.1813333333333335e-05,
      "loss": 0.0037,
      "step": 24560
    },
    {
      "epoch": 1.3104,
      "grad_norm": 0.1444406509399414,
      "learning_rate": 4.181000000000001e-05,
      "loss": 0.0031,
      "step": 24570
    },
    {
      "epoch": 1.3109333333333333,
      "grad_norm": 0.1441732496023178,
      "learning_rate": 4.180666666666667e-05,
      "loss": 0.0028,
      "step": 24580
    },
    {
      "epoch": 1.3114666666666666,
      "grad_norm": 0.029654666781425476,
      "learning_rate": 4.1803333333333333e-05,
      "loss": 0.0046,
      "step": 24590
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.1723473072052002,
      "learning_rate": 4.18e-05,
      "loss": 0.0031,
      "step": 24600
    },
    {
      "epoch": 1.3125333333333333,
      "grad_norm": 0.2020048052072525,
      "learning_rate": 4.1796666666666666e-05,
      "loss": 0.0036,
      "step": 24610
    },
    {
      "epoch": 1.3130666666666666,
      "grad_norm": 0.5185691118240356,
      "learning_rate": 4.179333333333333e-05,
      "loss": 0.0028,
      "step": 24620
    },
    {
      "epoch": 1.3136,
      "grad_norm": 0.14463278651237488,
      "learning_rate": 4.179e-05,
      "loss": 0.0025,
      "step": 24630
    },
    {
      "epoch": 1.3141333333333334,
      "grad_norm": 0.17291128635406494,
      "learning_rate": 4.178666666666667e-05,
      "loss": 0.0026,
      "step": 24640
    },
    {
      "epoch": 1.3146666666666667,
      "grad_norm": 0.05723046511411667,
      "learning_rate": 4.178333333333334e-05,
      "loss": 0.0032,
      "step": 24650
    },
    {
      "epoch": 1.3152,
      "grad_norm": 0.02929259091615677,
      "learning_rate": 4.178e-05,
      "loss": 0.0026,
      "step": 24660
    },
    {
      "epoch": 1.3157333333333332,
      "grad_norm": 0.1720476895570755,
      "learning_rate": 4.177666666666667e-05,
      "loss": 0.0031,
      "step": 24670
    },
    {
      "epoch": 1.3162666666666667,
      "grad_norm": 0.20089015364646912,
      "learning_rate": 4.1773333333333335e-05,
      "loss": 0.0024,
      "step": 24680
    },
    {
      "epoch": 1.3168,
      "grad_norm": 0.4593520760536194,
      "learning_rate": 4.177e-05,
      "loss": 0.0042,
      "step": 24690
    },
    {
      "epoch": 1.3173333333333335,
      "grad_norm": 0.14368601143360138,
      "learning_rate": 4.176666666666667e-05,
      "loss": 0.0035,
      "step": 24700
    },
    {
      "epoch": 1.3178666666666667,
      "grad_norm": 0.25945010781288147,
      "learning_rate": 4.176333333333334e-05,
      "loss": 0.0044,
      "step": 24710
    },
    {
      "epoch": 1.3184,
      "grad_norm": 0.05749896913766861,
      "learning_rate": 4.176000000000001e-05,
      "loss": 0.004,
      "step": 24720
    },
    {
      "epoch": 1.3189333333333333,
      "grad_norm": 0.22999106347560883,
      "learning_rate": 4.1756666666666666e-05,
      "loss": 0.0024,
      "step": 24730
    },
    {
      "epoch": 1.3194666666666666,
      "grad_norm": 0.40477079153060913,
      "learning_rate": 4.175333333333333e-05,
      "loss": 0.0041,
      "step": 24740
    },
    {
      "epoch": 1.32,
      "grad_norm": 0.17211423814296722,
      "learning_rate": 4.175e-05,
      "loss": 0.003,
      "step": 24750
    },
    {
      "epoch": 1.3205333333333333,
      "grad_norm": 0.11532893776893616,
      "learning_rate": 4.1746666666666665e-05,
      "loss": 0.0031,
      "step": 24760
    },
    {
      "epoch": 1.3210666666666666,
      "grad_norm": 0.3475151062011719,
      "learning_rate": 4.174333333333334e-05,
      "loss": 0.004,
      "step": 24770
    },
    {
      "epoch": 1.3216,
      "grad_norm": 0.11650890856981277,
      "learning_rate": 4.1740000000000004e-05,
      "loss": 0.0035,
      "step": 24780
    },
    {
      "epoch": 1.3221333333333334,
      "grad_norm": 0.17362822592258453,
      "learning_rate": 4.173666666666667e-05,
      "loss": 0.0037,
      "step": 24790
    },
    {
      "epoch": 1.3226666666666667,
      "grad_norm": 0.3455912470817566,
      "learning_rate": 4.1733333333333336e-05,
      "loss": 0.0043,
      "step": 24800
    },
    {
      "epoch": 1.3232,
      "grad_norm": 0.11529184132814407,
      "learning_rate": 4.173e-05,
      "loss": 0.0045,
      "step": 24810
    },
    {
      "epoch": 1.3237333333333332,
      "grad_norm": 0.2616862952709198,
      "learning_rate": 4.172666666666667e-05,
      "loss": 0.0052,
      "step": 24820
    },
    {
      "epoch": 1.3242666666666667,
      "grad_norm": 0.08606959134340286,
      "learning_rate": 4.1723333333333334e-05,
      "loss": 0.0039,
      "step": 24830
    },
    {
      "epoch": 1.3248,
      "grad_norm": 0.17339198291301727,
      "learning_rate": 4.172e-05,
      "loss": 0.0037,
      "step": 24840
    },
    {
      "epoch": 1.3253333333333333,
      "grad_norm": 0.05761251226067543,
      "learning_rate": 4.171666666666667e-05,
      "loss": 0.0026,
      "step": 24850
    },
    {
      "epoch": 1.3258666666666667,
      "grad_norm": 0.11601089686155319,
      "learning_rate": 4.171333333333334e-05,
      "loss": 0.0052,
      "step": 24860
    },
    {
      "epoch": 1.3264,
      "grad_norm": 0.11703024804592133,
      "learning_rate": 4.1710000000000006e-05,
      "loss": 0.0103,
      "step": 24870
    },
    {
      "epoch": 1.3269333333333333,
      "grad_norm": 0.08574454486370087,
      "learning_rate": 4.1706666666666665e-05,
      "loss": 0.008,
      "step": 24880
    },
    {
      "epoch": 1.3274666666666666,
      "grad_norm": 0.14935415983200073,
      "learning_rate": 4.170333333333333e-05,
      "loss": 0.0051,
      "step": 24890
    },
    {
      "epoch": 1.328,
      "grad_norm": 1.4295121431350708,
      "learning_rate": 4.17e-05,
      "loss": 0.0072,
      "step": 24900
    },
    {
      "epoch": 1.3285333333333333,
      "grad_norm": 0.6043741703033447,
      "learning_rate": 4.169666666666667e-05,
      "loss": 0.0047,
      "step": 24910
    },
    {
      "epoch": 1.3290666666666666,
      "grad_norm": 0.5768274664878845,
      "learning_rate": 4.1693333333333336e-05,
      "loss": 0.0029,
      "step": 24920
    },
    {
      "epoch": 1.3296000000000001,
      "grad_norm": 0.1718544065952301,
      "learning_rate": 4.169e-05,
      "loss": 0.0034,
      "step": 24930
    },
    {
      "epoch": 1.3301333333333334,
      "grad_norm": 0.14421211183071136,
      "learning_rate": 4.168666666666667e-05,
      "loss": 0.0034,
      "step": 24940
    },
    {
      "epoch": 1.3306666666666667,
      "grad_norm": 0.2883131802082062,
      "learning_rate": 4.1683333333333335e-05,
      "loss": 0.003,
      "step": 24950
    },
    {
      "epoch": 1.3312,
      "grad_norm": 0.28764137625694275,
      "learning_rate": 4.168e-05,
      "loss": 0.0036,
      "step": 24960
    },
    {
      "epoch": 1.3317333333333332,
      "grad_norm": 0.49019312858581543,
      "learning_rate": 4.167666666666667e-05,
      "loss": 0.003,
      "step": 24970
    },
    {
      "epoch": 1.3322666666666667,
      "grad_norm": 0.14609239995479584,
      "learning_rate": 4.167333333333334e-05,
      "loss": 0.002,
      "step": 24980
    },
    {
      "epoch": 1.3328,
      "grad_norm": 0.4056589603424072,
      "learning_rate": 4.1670000000000006e-05,
      "loss": 0.0049,
      "step": 24990
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.17342609167099,
      "learning_rate": 4.166666666666667e-05,
      "loss": 0.0028,
      "step": 25000
    },
    {
      "epoch": 1.3338666666666668,
      "grad_norm": 0.34701260924339294,
      "learning_rate": 4.166333333333334e-05,
      "loss": 0.0041,
      "step": 25010
    },
    {
      "epoch": 1.3344,
      "grad_norm": 0.17359112203121185,
      "learning_rate": 4.1660000000000004e-05,
      "loss": 0.0026,
      "step": 25020
    },
    {
      "epoch": 1.3349333333333333,
      "grad_norm": 0.11528747528791428,
      "learning_rate": 4.1656666666666664e-05,
      "loss": 0.0023,
      "step": 25030
    },
    {
      "epoch": 1.3354666666666666,
      "grad_norm": 0.05766873061656952,
      "learning_rate": 4.165333333333333e-05,
      "loss": 0.0049,
      "step": 25040
    },
    {
      "epoch": 1.336,
      "grad_norm": 0.20117324590682983,
      "learning_rate": 4.165e-05,
      "loss": 0.0031,
      "step": 25050
    },
    {
      "epoch": 1.3365333333333334,
      "grad_norm": 0.05745382606983185,
      "learning_rate": 4.164666666666667e-05,
      "loss": 0.0041,
      "step": 25060
    },
    {
      "epoch": 1.3370666666666666,
      "grad_norm": 0.08642305433750153,
      "learning_rate": 4.1643333333333335e-05,
      "loss": 0.0026,
      "step": 25070
    },
    {
      "epoch": 1.3376000000000001,
      "grad_norm": 0.14424854516983032,
      "learning_rate": 4.164e-05,
      "loss": 0.0027,
      "step": 25080
    },
    {
      "epoch": 1.3381333333333334,
      "grad_norm": 0.08665231615304947,
      "learning_rate": 4.163666666666667e-05,
      "loss": 0.0027,
      "step": 25090
    },
    {
      "epoch": 1.3386666666666667,
      "grad_norm": 0.14449483156204224,
      "learning_rate": 4.1633333333333333e-05,
      "loss": 0.0038,
      "step": 25100
    },
    {
      "epoch": 1.3392,
      "grad_norm": 0.288025438785553,
      "learning_rate": 4.163e-05,
      "loss": 0.0027,
      "step": 25110
    },
    {
      "epoch": 1.3397333333333332,
      "grad_norm": 0.20096388459205627,
      "learning_rate": 4.162666666666667e-05,
      "loss": 0.0045,
      "step": 25120
    },
    {
      "epoch": 1.3402666666666667,
      "grad_norm": 0.2004648894071579,
      "learning_rate": 4.162333333333334e-05,
      "loss": 0.0054,
      "step": 25130
    },
    {
      "epoch": 1.3408,
      "grad_norm": 0.057644397020339966,
      "learning_rate": 4.1620000000000005e-05,
      "loss": 0.0027,
      "step": 25140
    },
    {
      "epoch": 1.3413333333333333,
      "grad_norm": 0.029619872570037842,
      "learning_rate": 4.161666666666667e-05,
      "loss": 0.0031,
      "step": 25150
    },
    {
      "epoch": 1.3418666666666668,
      "grad_norm": 0.08716404438018799,
      "learning_rate": 4.161333333333334e-05,
      "loss": 0.004,
      "step": 25160
    },
    {
      "epoch": 1.3424,
      "grad_norm": 0.028858330100774765,
      "learning_rate": 4.161e-05,
      "loss": 0.0027,
      "step": 25170
    },
    {
      "epoch": 1.3429333333333333,
      "grad_norm": 0.007725076284259558,
      "learning_rate": 4.160666666666667e-05,
      "loss": 0.0038,
      "step": 25180
    },
    {
      "epoch": 1.3434666666666666,
      "grad_norm": 0.23194795846939087,
      "learning_rate": 4.1603333333333335e-05,
      "loss": 0.0034,
      "step": 25190
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.5486563444137573,
      "learning_rate": 4.16e-05,
      "loss": 0.0031,
      "step": 25200
    },
    {
      "epoch": 1.3445333333333334,
      "grad_norm": 0.11478321254253387,
      "learning_rate": 4.159666666666667e-05,
      "loss": 0.0038,
      "step": 25210
    },
    {
      "epoch": 1.3450666666666666,
      "grad_norm": 0.20156458020210266,
      "learning_rate": 4.1593333333333334e-05,
      "loss": 0.0035,
      "step": 25220
    },
    {
      "epoch": 1.3456000000000001,
      "grad_norm": 0.005709860939532518,
      "learning_rate": 4.159e-05,
      "loss": 0.0028,
      "step": 25230
    },
    {
      "epoch": 1.3461333333333334,
      "grad_norm": 0.20143629610538483,
      "learning_rate": 4.1586666666666666e-05,
      "loss": 0.0031,
      "step": 25240
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.4321059286594391,
      "learning_rate": 4.158333333333333e-05,
      "loss": 0.0042,
      "step": 25250
    },
    {
      "epoch": 1.3472,
      "grad_norm": 0.05858824774622917,
      "learning_rate": 4.1580000000000005e-05,
      "loss": 0.0045,
      "step": 25260
    },
    {
      "epoch": 1.3477333333333332,
      "grad_norm": 0.17331381142139435,
      "learning_rate": 4.157666666666667e-05,
      "loss": 0.0031,
      "step": 25270
    },
    {
      "epoch": 1.3482666666666667,
      "grad_norm": 0.2299782782793045,
      "learning_rate": 4.157333333333334e-05,
      "loss": 0.0033,
      "step": 25280
    },
    {
      "epoch": 1.3488,
      "grad_norm": 0.08639492094516754,
      "learning_rate": 4.1570000000000003e-05,
      "loss": 0.003,
      "step": 25290
    },
    {
      "epoch": 1.3493333333333333,
      "grad_norm": 0.11546846479177475,
      "learning_rate": 4.156666666666667e-05,
      "loss": 0.0026,
      "step": 25300
    },
    {
      "epoch": 1.3498666666666668,
      "grad_norm": 0.14451204240322113,
      "learning_rate": 4.1563333333333336e-05,
      "loss": 0.0024,
      "step": 25310
    },
    {
      "epoch": 1.3504,
      "grad_norm": 0.1725088208913803,
      "learning_rate": 4.156e-05,
      "loss": 0.0046,
      "step": 25320
    },
    {
      "epoch": 1.3509333333333333,
      "grad_norm": 0.17327193915843964,
      "learning_rate": 4.155666666666667e-05,
      "loss": 0.0044,
      "step": 25330
    },
    {
      "epoch": 1.3514666666666666,
      "grad_norm": 0.23008739948272705,
      "learning_rate": 4.1553333333333334e-05,
      "loss": 0.0028,
      "step": 25340
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.26128602027893066,
      "learning_rate": 4.155e-05,
      "loss": 0.0036,
      "step": 25350
    },
    {
      "epoch": 1.3525333333333334,
      "grad_norm": 0.6076620817184448,
      "learning_rate": 4.1546666666666666e-05,
      "loss": 0.0026,
      "step": 25360
    },
    {
      "epoch": 1.3530666666666666,
      "grad_norm": 0.2882618010044098,
      "learning_rate": 4.154333333333333e-05,
      "loss": 0.0039,
      "step": 25370
    },
    {
      "epoch": 1.3536000000000001,
      "grad_norm": 0.17228543758392334,
      "learning_rate": 4.154e-05,
      "loss": 0.0025,
      "step": 25380
    },
    {
      "epoch": 1.3541333333333334,
      "grad_norm": 0.20122085511684418,
      "learning_rate": 4.1536666666666665e-05,
      "loss": 0.0026,
      "step": 25390
    },
    {
      "epoch": 1.3546666666666667,
      "grad_norm": 0.05748499929904938,
      "learning_rate": 4.153333333333334e-05,
      "loss": 0.002,
      "step": 25400
    },
    {
      "epoch": 1.3552,
      "grad_norm": 0.3730212450027466,
      "learning_rate": 4.1530000000000004e-05,
      "loss": 0.0024,
      "step": 25410
    },
    {
      "epoch": 1.3557333333333332,
      "grad_norm": 0.029287798330187798,
      "learning_rate": 4.152666666666667e-05,
      "loss": 0.0029,
      "step": 25420
    },
    {
      "epoch": 1.3562666666666667,
      "grad_norm": 0.3166842758655548,
      "learning_rate": 4.1523333333333336e-05,
      "loss": 0.0031,
      "step": 25430
    },
    {
      "epoch": 1.3568,
      "grad_norm": 0.11509662121534348,
      "learning_rate": 4.152e-05,
      "loss": 0.002,
      "step": 25440
    },
    {
      "epoch": 1.3573333333333333,
      "grad_norm": 0.5750117897987366,
      "learning_rate": 4.151666666666667e-05,
      "loss": 0.0026,
      "step": 25450
    },
    {
      "epoch": 1.3578666666666668,
      "grad_norm": 0.028962789103388786,
      "learning_rate": 4.1513333333333335e-05,
      "loss": 0.0038,
      "step": 25460
    },
    {
      "epoch": 1.3584,
      "grad_norm": 0.40155044198036194,
      "learning_rate": 4.151000000000001e-05,
      "loss": 0.0029,
      "step": 25470
    },
    {
      "epoch": 1.3589333333333333,
      "grad_norm": 0.029123125597834587,
      "learning_rate": 4.150666666666667e-05,
      "loss": 0.0042,
      "step": 25480
    },
    {
      "epoch": 1.3594666666666666,
      "grad_norm": 0.5159375667572021,
      "learning_rate": 4.150333333333333e-05,
      "loss": 0.0027,
      "step": 25490
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.26006078720092773,
      "learning_rate": 4.15e-05,
      "loss": 0.0037,
      "step": 25500
    },
    {
      "epoch": 1.3605333333333334,
      "grad_norm": 0.26083528995513916,
      "learning_rate": 4.1496666666666665e-05,
      "loss": 0.003,
      "step": 25510
    },
    {
      "epoch": 1.3610666666666666,
      "grad_norm": 0.4364623427391052,
      "learning_rate": 4.149333333333333e-05,
      "loss": 0.0026,
      "step": 25520
    },
    {
      "epoch": 1.3616,
      "grad_norm": 0.2033189982175827,
      "learning_rate": 4.1490000000000004e-05,
      "loss": 0.004,
      "step": 25530
    },
    {
      "epoch": 1.3621333333333334,
      "grad_norm": 0.2602624297142029,
      "learning_rate": 4.148666666666667e-05,
      "loss": 0.0043,
      "step": 25540
    },
    {
      "epoch": 1.3626666666666667,
      "grad_norm": 0.4369940459728241,
      "learning_rate": 4.1483333333333337e-05,
      "loss": 0.0024,
      "step": 25550
    },
    {
      "epoch": 1.3632,
      "grad_norm": 0.00637917360290885,
      "learning_rate": 4.148e-05,
      "loss": 0.0051,
      "step": 25560
    },
    {
      "epoch": 1.3637333333333332,
      "grad_norm": 0.23111820220947266,
      "learning_rate": 4.147666666666667e-05,
      "loss": 0.0038,
      "step": 25570
    },
    {
      "epoch": 1.3642666666666667,
      "grad_norm": 0.08848778158426285,
      "learning_rate": 4.1473333333333335e-05,
      "loss": 0.0033,
      "step": 25580
    },
    {
      "epoch": 1.3648,
      "grad_norm": 0.08671104162931442,
      "learning_rate": 4.147e-05,
      "loss": 0.0018,
      "step": 25590
    },
    {
      "epoch": 1.3653333333333333,
      "grad_norm": 0.031052241101861,
      "learning_rate": 4.146666666666667e-05,
      "loss": 0.0026,
      "step": 25600
    },
    {
      "epoch": 1.3658666666666668,
      "grad_norm": 0.14418815076351166,
      "learning_rate": 4.146333333333334e-05,
      "loss": 0.0038,
      "step": 25610
    },
    {
      "epoch": 1.3664,
      "grad_norm": 0.11573588848114014,
      "learning_rate": 4.1460000000000006e-05,
      "loss": 0.004,
      "step": 25620
    },
    {
      "epoch": 1.3669333333333333,
      "grad_norm": 0.1728924959897995,
      "learning_rate": 4.145666666666667e-05,
      "loss": 0.0031,
      "step": 25630
    },
    {
      "epoch": 1.3674666666666666,
      "grad_norm": 0.2027749866247177,
      "learning_rate": 4.145333333333333e-05,
      "loss": 0.0041,
      "step": 25640
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 0.2323298305273056,
      "learning_rate": 4.145e-05,
      "loss": 0.0022,
      "step": 25650
    },
    {
      "epoch": 1.3685333333333334,
      "grad_norm": 0.46594929695129395,
      "learning_rate": 4.1446666666666664e-05,
      "loss": 0.0034,
      "step": 25660
    },
    {
      "epoch": 1.3690666666666667,
      "grad_norm": 0.11635378748178482,
      "learning_rate": 4.144333333333334e-05,
      "loss": 0.0035,
      "step": 25670
    },
    {
      "epoch": 1.3696,
      "grad_norm": 0.14664588868618011,
      "learning_rate": 4.144e-05,
      "loss": 0.0029,
      "step": 25680
    },
    {
      "epoch": 1.3701333333333334,
      "grad_norm": 0.2915988862514496,
      "learning_rate": 4.143666666666667e-05,
      "loss": 0.003,
      "step": 25690
    },
    {
      "epoch": 1.3706666666666667,
      "grad_norm": 0.23292258381843567,
      "learning_rate": 4.1433333333333335e-05,
      "loss": 0.0033,
      "step": 25700
    },
    {
      "epoch": 1.3712,
      "grad_norm": 0.26110100746154785,
      "learning_rate": 4.143e-05,
      "loss": 0.0037,
      "step": 25710
    },
    {
      "epoch": 1.3717333333333332,
      "grad_norm": 0.2021225243806839,
      "learning_rate": 4.142666666666667e-05,
      "loss": 0.0039,
      "step": 25720
    },
    {
      "epoch": 1.3722666666666667,
      "grad_norm": 0.08649259060621262,
      "learning_rate": 4.1423333333333334e-05,
      "loss": 0.0035,
      "step": 25730
    },
    {
      "epoch": 1.3728,
      "grad_norm": 0.28919968008995056,
      "learning_rate": 4.142000000000001e-05,
      "loss": 0.0033,
      "step": 25740
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.08672717213630676,
      "learning_rate": 4.141666666666667e-05,
      "loss": 0.0037,
      "step": 25750
    },
    {
      "epoch": 1.3738666666666668,
      "grad_norm": 0.3471923768520355,
      "learning_rate": 4.141333333333334e-05,
      "loss": 0.0027,
      "step": 25760
    },
    {
      "epoch": 1.3744,
      "grad_norm": 0.058218490332365036,
      "learning_rate": 4.1410000000000005e-05,
      "loss": 0.0036,
      "step": 25770
    },
    {
      "epoch": 1.3749333333333333,
      "grad_norm": 0.08892981708049774,
      "learning_rate": 4.140666666666667e-05,
      "loss": 0.0035,
      "step": 25780
    },
    {
      "epoch": 1.3754666666666666,
      "grad_norm": 0.08727152645587921,
      "learning_rate": 4.140333333333333e-05,
      "loss": 0.003,
      "step": 25790
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.17370009422302246,
      "learning_rate": 4.14e-05,
      "loss": 0.002,
      "step": 25800
    },
    {
      "epoch": 1.3765333333333334,
      "grad_norm": 0.5787240266799927,
      "learning_rate": 4.139666666666667e-05,
      "loss": 0.0021,
      "step": 25810
    },
    {
      "epoch": 1.3770666666666667,
      "grad_norm": 0.3182619512081146,
      "learning_rate": 4.1393333333333336e-05,
      "loss": 0.0024,
      "step": 25820
    },
    {
      "epoch": 1.3776,
      "grad_norm": 0.14513444900512695,
      "learning_rate": 4.139e-05,
      "loss": 0.0039,
      "step": 25830
    },
    {
      "epoch": 1.3781333333333334,
      "grad_norm": 0.1438150405883789,
      "learning_rate": 4.138666666666667e-05,
      "loss": 0.0027,
      "step": 25840
    },
    {
      "epoch": 1.3786666666666667,
      "grad_norm": 0.20182956755161285,
      "learning_rate": 4.1383333333333334e-05,
      "loss": 0.0044,
      "step": 25850
    },
    {
      "epoch": 1.3792,
      "grad_norm": 0.20207779109477997,
      "learning_rate": 4.138e-05,
      "loss": 0.0032,
      "step": 25860
    },
    {
      "epoch": 1.3797333333333333,
      "grad_norm": 0.20240962505340576,
      "learning_rate": 4.1376666666666666e-05,
      "loss": 0.0024,
      "step": 25870
    },
    {
      "epoch": 1.3802666666666665,
      "grad_norm": 0.26049262285232544,
      "learning_rate": 4.137333333333334e-05,
      "loss": 0.0035,
      "step": 25880
    },
    {
      "epoch": 1.3808,
      "grad_norm": 0.2325752079486847,
      "learning_rate": 4.1370000000000005e-05,
      "loss": 0.0026,
      "step": 25890
    },
    {
      "epoch": 1.3813333333333333,
      "grad_norm": 0.5501649379730225,
      "learning_rate": 4.136666666666667e-05,
      "loss": 0.0024,
      "step": 25900
    },
    {
      "epoch": 1.3818666666666668,
      "grad_norm": 0.11577940732240677,
      "learning_rate": 4.136333333333334e-05,
      "loss": 0.0038,
      "step": 25910
    },
    {
      "epoch": 1.3824,
      "grad_norm": 0.14456868171691895,
      "learning_rate": 4.1360000000000004e-05,
      "loss": 0.0043,
      "step": 25920
    },
    {
      "epoch": 1.3829333333333333,
      "grad_norm": 0.17228351533412933,
      "learning_rate": 4.135666666666667e-05,
      "loss": 0.0049,
      "step": 25930
    },
    {
      "epoch": 1.3834666666666666,
      "grad_norm": 0.20045168697834015,
      "learning_rate": 4.1353333333333336e-05,
      "loss": 0.0031,
      "step": 25940
    },
    {
      "epoch": 1.384,
      "grad_norm": 0.5176500082015991,
      "learning_rate": 4.135e-05,
      "loss": 0.0034,
      "step": 25950
    },
    {
      "epoch": 1.3845333333333334,
      "grad_norm": 0.4651451110839844,
      "learning_rate": 4.134666666666667e-05,
      "loss": 0.003,
      "step": 25960
    },
    {
      "epoch": 1.3850666666666667,
      "grad_norm": 0.34686848521232605,
      "learning_rate": 4.1343333333333334e-05,
      "loss": 0.0036,
      "step": 25970
    },
    {
      "epoch": 1.3856,
      "grad_norm": 0.08718635141849518,
      "learning_rate": 4.134e-05,
      "loss": 0.0046,
      "step": 25980
    },
    {
      "epoch": 1.3861333333333334,
      "grad_norm": 0.4059242308139801,
      "learning_rate": 4.133666666666667e-05,
      "loss": 0.003,
      "step": 25990
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.1446840465068817,
      "learning_rate": 4.133333333333333e-05,
      "loss": 0.0032,
      "step": 26000
    },
    {
      "epoch": 1.3872,
      "grad_norm": 0.11579007655382156,
      "learning_rate": 4.133e-05,
      "loss": 0.0034,
      "step": 26010
    },
    {
      "epoch": 1.3877333333333333,
      "grad_norm": 0.1740478128194809,
      "learning_rate": 4.132666666666667e-05,
      "loss": 0.0034,
      "step": 26020
    },
    {
      "epoch": 1.3882666666666665,
      "grad_norm": 0.11655634641647339,
      "learning_rate": 4.132333333333334e-05,
      "loss": 0.0035,
      "step": 26030
    },
    {
      "epoch": 1.3888,
      "grad_norm": 0.5212188959121704,
      "learning_rate": 4.1320000000000004e-05,
      "loss": 0.0056,
      "step": 26040
    },
    {
      "epoch": 1.3893333333333333,
      "grad_norm": 0.08707895129919052,
      "learning_rate": 4.131666666666667e-05,
      "loss": 0.0042,
      "step": 26050
    },
    {
      "epoch": 1.3898666666666666,
      "grad_norm": 0.0031919837929308414,
      "learning_rate": 4.1313333333333336e-05,
      "loss": 0.0025,
      "step": 26060
    },
    {
      "epoch": 1.3904,
      "grad_norm": 0.31764110922813416,
      "learning_rate": 4.131e-05,
      "loss": 0.0043,
      "step": 26070
    },
    {
      "epoch": 1.3909333333333334,
      "grad_norm": 0.5779387354850769,
      "learning_rate": 4.130666666666667e-05,
      "loss": 0.0038,
      "step": 26080
    },
    {
      "epoch": 1.3914666666666666,
      "grad_norm": 0.029213616624474525,
      "learning_rate": 4.1303333333333335e-05,
      "loss": 0.0052,
      "step": 26090
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.2319093495607376,
      "learning_rate": 4.13e-05,
      "loss": 0.0048,
      "step": 26100
    },
    {
      "epoch": 1.3925333333333334,
      "grad_norm": 0.11471832543611526,
      "learning_rate": 4.129666666666667e-05,
      "loss": 0.003,
      "step": 26110
    },
    {
      "epoch": 1.3930666666666667,
      "grad_norm": 0.40403270721435547,
      "learning_rate": 4.129333333333333e-05,
      "loss": 0.0033,
      "step": 26120
    },
    {
      "epoch": 1.3936,
      "grad_norm": 0.08834980428218842,
      "learning_rate": 4.129e-05,
      "loss": 0.0028,
      "step": 26130
    },
    {
      "epoch": 1.3941333333333334,
      "grad_norm": 0.030560147017240524,
      "learning_rate": 4.1286666666666666e-05,
      "loss": 0.0029,
      "step": 26140
    },
    {
      "epoch": 1.3946666666666667,
      "grad_norm": 0.031982239335775375,
      "learning_rate": 4.128333333333333e-05,
      "loss": 0.0041,
      "step": 26150
    },
    {
      "epoch": 1.3952,
      "grad_norm": 0.2890951633453369,
      "learning_rate": 4.1280000000000005e-05,
      "loss": 0.0027,
      "step": 26160
    },
    {
      "epoch": 1.3957333333333333,
      "grad_norm": 0.20222492516040802,
      "learning_rate": 4.127666666666667e-05,
      "loss": 0.0036,
      "step": 26170
    },
    {
      "epoch": 1.3962666666666665,
      "grad_norm": 0.05841062217950821,
      "learning_rate": 4.127333333333334e-05,
      "loss": 0.0029,
      "step": 26180
    },
    {
      "epoch": 1.3968,
      "grad_norm": 0.028928032144904137,
      "learning_rate": 4.127e-05,
      "loss": 0.0032,
      "step": 26190
    },
    {
      "epoch": 1.3973333333333333,
      "grad_norm": 0.03172711282968521,
      "learning_rate": 4.126666666666667e-05,
      "loss": 0.0052,
      "step": 26200
    },
    {
      "epoch": 1.3978666666666666,
      "grad_norm": 0.3750725984573364,
      "learning_rate": 4.1263333333333335e-05,
      "loss": 0.005,
      "step": 26210
    },
    {
      "epoch": 1.3984,
      "grad_norm": 0.17362792789936066,
      "learning_rate": 4.126e-05,
      "loss": 0.0034,
      "step": 26220
    },
    {
      "epoch": 1.3989333333333334,
      "grad_norm": 0.23143157362937927,
      "learning_rate": 4.1256666666666674e-05,
      "loss": 0.0037,
      "step": 26230
    },
    {
      "epoch": 1.3994666666666666,
      "grad_norm": 0.2322988510131836,
      "learning_rate": 4.1253333333333334e-05,
      "loss": 0.004,
      "step": 26240
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.029286589473485947,
      "learning_rate": 4.125e-05,
      "loss": 0.0029,
      "step": 26250
    },
    {
      "epoch": 1.4005333333333334,
      "grad_norm": 0.46421289443969727,
      "learning_rate": 4.1246666666666666e-05,
      "loss": 0.0046,
      "step": 26260
    },
    {
      "epoch": 1.4010666666666667,
      "grad_norm": 0.43355846405029297,
      "learning_rate": 4.124333333333333e-05,
      "loss": 0.0029,
      "step": 26270
    },
    {
      "epoch": 1.4016,
      "grad_norm": 0.2601686418056488,
      "learning_rate": 4.124e-05,
      "loss": 0.0048,
      "step": 26280
    },
    {
      "epoch": 1.4021333333333335,
      "grad_norm": 0.2604665458202362,
      "learning_rate": 4.123666666666667e-05,
      "loss": 0.0034,
      "step": 26290
    },
    {
      "epoch": 1.4026666666666667,
      "grad_norm": 0.23243647813796997,
      "learning_rate": 4.123333333333334e-05,
      "loss": 0.0039,
      "step": 26300
    },
    {
      "epoch": 1.4032,
      "grad_norm": 0.49259915947914124,
      "learning_rate": 4.123e-05,
      "loss": 0.0035,
      "step": 26310
    },
    {
      "epoch": 1.4037333333333333,
      "grad_norm": 0.23115302622318268,
      "learning_rate": 4.122666666666667e-05,
      "loss": 0.0032,
      "step": 26320
    },
    {
      "epoch": 1.4042666666666666,
      "grad_norm": 0.11537390947341919,
      "learning_rate": 4.1223333333333336e-05,
      "loss": 0.003,
      "step": 26330
    },
    {
      "epoch": 1.4048,
      "grad_norm": 0.28960034251213074,
      "learning_rate": 4.122e-05,
      "loss": 0.0031,
      "step": 26340
    },
    {
      "epoch": 1.4053333333333333,
      "grad_norm": 0.17440146207809448,
      "learning_rate": 4.121666666666667e-05,
      "loss": 0.0033,
      "step": 26350
    },
    {
      "epoch": 1.4058666666666666,
      "grad_norm": 0.145380437374115,
      "learning_rate": 4.1213333333333334e-05,
      "loss": 0.0048,
      "step": 26360
    },
    {
      "epoch": 1.4064,
      "grad_norm": 0.11568400263786316,
      "learning_rate": 4.121000000000001e-05,
      "loss": 0.003,
      "step": 26370
    },
    {
      "epoch": 1.4069333333333334,
      "grad_norm": 3.062403917312622,
      "learning_rate": 4.120666666666667e-05,
      "loss": 0.0037,
      "step": 26380
    },
    {
      "epoch": 1.4074666666666666,
      "grad_norm": 0.5127068758010864,
      "learning_rate": 4.120333333333333e-05,
      "loss": 0.005,
      "step": 26390
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.34561821818351746,
      "learning_rate": 4.12e-05,
      "loss": 0.0084,
      "step": 26400
    },
    {
      "epoch": 1.4085333333333334,
      "grad_norm": 0.1440265029668808,
      "learning_rate": 4.1196666666666665e-05,
      "loss": 0.0044,
      "step": 26410
    },
    {
      "epoch": 1.4090666666666667,
      "grad_norm": 0.14418458938598633,
      "learning_rate": 4.119333333333333e-05,
      "loss": 0.0043,
      "step": 26420
    },
    {
      "epoch": 1.4096,
      "grad_norm": 0.20408959686756134,
      "learning_rate": 4.1190000000000004e-05,
      "loss": 0.0062,
      "step": 26430
    },
    {
      "epoch": 1.4101333333333335,
      "grad_norm": 0.11637287586927414,
      "learning_rate": 4.118666666666667e-05,
      "loss": 0.0045,
      "step": 26440
    },
    {
      "epoch": 1.4106666666666667,
      "grad_norm": 0.08679812401533127,
      "learning_rate": 4.1183333333333336e-05,
      "loss": 0.0049,
      "step": 26450
    },
    {
      "epoch": 1.4112,
      "grad_norm": 0.17439165711402893,
      "learning_rate": 4.118e-05,
      "loss": 0.0027,
      "step": 26460
    },
    {
      "epoch": 1.4117333333333333,
      "grad_norm": 0.43579956889152527,
      "learning_rate": 4.117666666666667e-05,
      "loss": 0.0038,
      "step": 26470
    },
    {
      "epoch": 1.4122666666666666,
      "grad_norm": 0.11574187129735947,
      "learning_rate": 4.1173333333333334e-05,
      "loss": 0.0026,
      "step": 26480
    },
    {
      "epoch": 1.4128,
      "grad_norm": 0.08668027073144913,
      "learning_rate": 4.117e-05,
      "loss": 0.0045,
      "step": 26490
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 0.05778928101062775,
      "learning_rate": 4.116666666666667e-05,
      "loss": 0.0053,
      "step": 26500
    },
    {
      "epoch": 1.4138666666666666,
      "grad_norm": 0.3195927143096924,
      "learning_rate": 4.116333333333334e-05,
      "loss": 0.0039,
      "step": 26510
    },
    {
      "epoch": 1.4144,
      "grad_norm": 0.11568312346935272,
      "learning_rate": 4.1160000000000006e-05,
      "loss": 0.0046,
      "step": 26520
    },
    {
      "epoch": 1.4149333333333334,
      "grad_norm": 0.02883128449320793,
      "learning_rate": 4.115666666666667e-05,
      "loss": 0.0045,
      "step": 26530
    },
    {
      "epoch": 1.4154666666666667,
      "grad_norm": 0.17300185561180115,
      "learning_rate": 4.115333333333333e-05,
      "loss": 0.0046,
      "step": 26540
    },
    {
      "epoch": 1.416,
      "grad_norm": 0.17268191277980804,
      "learning_rate": 4.115e-05,
      "loss": 0.0037,
      "step": 26550
    },
    {
      "epoch": 1.4165333333333332,
      "grad_norm": 0.05813697725534439,
      "learning_rate": 4.1146666666666663e-05,
      "loss": 0.0042,
      "step": 26560
    },
    {
      "epoch": 1.4170666666666667,
      "grad_norm": 1.9149645566940308,
      "learning_rate": 4.1143333333333336e-05,
      "loss": 0.0033,
      "step": 26570
    },
    {
      "epoch": 1.4176,
      "grad_norm": 0.28812476992607117,
      "learning_rate": 4.114e-05,
      "loss": 0.0026,
      "step": 26580
    },
    {
      "epoch": 1.4181333333333335,
      "grad_norm": 0.05715307593345642,
      "learning_rate": 4.113666666666667e-05,
      "loss": 0.0037,
      "step": 26590
    },
    {
      "epoch": 1.4186666666666667,
      "grad_norm": 0.28747332096099854,
      "learning_rate": 4.1133333333333335e-05,
      "loss": 0.003,
      "step": 26600
    },
    {
      "epoch": 1.4192,
      "grad_norm": 0.22923356294631958,
      "learning_rate": 4.113e-05,
      "loss": 0.004,
      "step": 26610
    },
    {
      "epoch": 1.4197333333333333,
      "grad_norm": 0.11480491608381271,
      "learning_rate": 4.112666666666667e-05,
      "loss": 0.0041,
      "step": 26620
    },
    {
      "epoch": 1.4202666666666666,
      "grad_norm": 0.14357218146324158,
      "learning_rate": 4.112333333333333e-05,
      "loss": 0.0043,
      "step": 26630
    },
    {
      "epoch": 1.4208,
      "grad_norm": 0.08622995018959045,
      "learning_rate": 4.1120000000000006e-05,
      "loss": 0.0035,
      "step": 26640
    },
    {
      "epoch": 1.4213333333333333,
      "grad_norm": 0.34425410628318787,
      "learning_rate": 4.111666666666667e-05,
      "loss": 0.002,
      "step": 26650
    },
    {
      "epoch": 1.4218666666666666,
      "grad_norm": 0.25882619619369507,
      "learning_rate": 4.111333333333334e-05,
      "loss": 0.0033,
      "step": 26660
    },
    {
      "epoch": 1.4224,
      "grad_norm": 0.11591852456331253,
      "learning_rate": 4.1110000000000005e-05,
      "loss": 0.0046,
      "step": 26670
    },
    {
      "epoch": 1.4229333333333334,
      "grad_norm": 0.4913134276866913,
      "learning_rate": 4.110666666666667e-05,
      "loss": 0.0028,
      "step": 26680
    },
    {
      "epoch": 1.4234666666666667,
      "grad_norm": 0.003954625688493252,
      "learning_rate": 4.110333333333333e-05,
      "loss": 0.0043,
      "step": 26690
    },
    {
      "epoch": 1.424,
      "grad_norm": 0.1447591930627823,
      "learning_rate": 4.11e-05,
      "loss": 0.0037,
      "step": 26700
    },
    {
      "epoch": 1.4245333333333332,
      "grad_norm": 0.0027104259934276342,
      "learning_rate": 4.109666666666667e-05,
      "loss": 0.004,
      "step": 26710
    },
    {
      "epoch": 1.4250666666666667,
      "grad_norm": 0.31785374879837036,
      "learning_rate": 4.1093333333333335e-05,
      "loss": 0.0035,
      "step": 26720
    },
    {
      "epoch": 1.4256,
      "grad_norm": 0.722823977470398,
      "learning_rate": 4.109e-05,
      "loss": 0.0042,
      "step": 26730
    },
    {
      "epoch": 1.4261333333333333,
      "grad_norm": 0.08640531450510025,
      "learning_rate": 4.108666666666667e-05,
      "loss": 0.0023,
      "step": 26740
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.3755246698856354,
      "learning_rate": 4.1083333333333334e-05,
      "loss": 0.0032,
      "step": 26750
    },
    {
      "epoch": 1.4272,
      "grad_norm": 0.2888522744178772,
      "learning_rate": 4.108e-05,
      "loss": 0.0043,
      "step": 26760
    },
    {
      "epoch": 1.4277333333333333,
      "grad_norm": 0.08702677488327026,
      "learning_rate": 4.1076666666666666e-05,
      "loss": 0.0031,
      "step": 26770
    },
    {
      "epoch": 1.4282666666666666,
      "grad_norm": 0.2887180745601654,
      "learning_rate": 4.107333333333334e-05,
      "loss": 0.0035,
      "step": 26780
    },
    {
      "epoch": 1.4288,
      "grad_norm": 0.05770377442240715,
      "learning_rate": 4.1070000000000005e-05,
      "loss": 0.0039,
      "step": 26790
    },
    {
      "epoch": 1.4293333333333333,
      "grad_norm": 0.028857024386525154,
      "learning_rate": 4.106666666666667e-05,
      "loss": 0.0036,
      "step": 26800
    },
    {
      "epoch": 1.4298666666666666,
      "grad_norm": 0.5768618583679199,
      "learning_rate": 4.106333333333334e-05,
      "loss": 0.0045,
      "step": 26810
    },
    {
      "epoch": 1.4304000000000001,
      "grad_norm": 0.1721557080745697,
      "learning_rate": 4.106e-05,
      "loss": 0.0036,
      "step": 26820
    },
    {
      "epoch": 1.4309333333333334,
      "grad_norm": 0.5735815167427063,
      "learning_rate": 4.105666666666667e-05,
      "loss": 0.0036,
      "step": 26830
    },
    {
      "epoch": 1.4314666666666667,
      "grad_norm": 0.08708672225475311,
      "learning_rate": 4.1053333333333336e-05,
      "loss": 0.0034,
      "step": 26840
    },
    {
      "epoch": 1.432,
      "grad_norm": 0.402540922164917,
      "learning_rate": 4.105e-05,
      "loss": 0.0021,
      "step": 26850
    },
    {
      "epoch": 1.4325333333333332,
      "grad_norm": 0.17234843969345093,
      "learning_rate": 4.104666666666667e-05,
      "loss": 0.0027,
      "step": 26860
    },
    {
      "epoch": 1.4330666666666667,
      "grad_norm": 0.20335760712623596,
      "learning_rate": 4.1043333333333334e-05,
      "loss": 0.0019,
      "step": 26870
    },
    {
      "epoch": 1.4336,
      "grad_norm": 0.0288479495793581,
      "learning_rate": 4.104e-05,
      "loss": 0.0036,
      "step": 26880
    },
    {
      "epoch": 1.4341333333333333,
      "grad_norm": 0.028635382652282715,
      "learning_rate": 4.1036666666666666e-05,
      "loss": 0.0034,
      "step": 26890
    },
    {
      "epoch": 1.4346666666666668,
      "grad_norm": 0.43034595251083374,
      "learning_rate": 4.103333333333333e-05,
      "loss": 0.0026,
      "step": 26900
    },
    {
      "epoch": 1.4352,
      "grad_norm": 0.4019923210144043,
      "learning_rate": 4.103e-05,
      "loss": 0.0038,
      "step": 26910
    },
    {
      "epoch": 1.4357333333333333,
      "grad_norm": 0.28873980045318604,
      "learning_rate": 4.102666666666667e-05,
      "loss": 0.0029,
      "step": 26920
    },
    {
      "epoch": 1.4362666666666666,
      "grad_norm": 0.028817707672715187,
      "learning_rate": 4.102333333333334e-05,
      "loss": 0.0032,
      "step": 26930
    },
    {
      "epoch": 1.4368,
      "grad_norm": 0.02896483987569809,
      "learning_rate": 4.1020000000000004e-05,
      "loss": 0.0023,
      "step": 26940
    },
    {
      "epoch": 1.4373333333333334,
      "grad_norm": 0.23073692619800568,
      "learning_rate": 4.101666666666667e-05,
      "loss": 0.0036,
      "step": 26950
    },
    {
      "epoch": 1.4378666666666666,
      "grad_norm": 0.11527985334396362,
      "learning_rate": 4.1013333333333336e-05,
      "loss": 0.0029,
      "step": 26960
    },
    {
      "epoch": 1.4384000000000001,
      "grad_norm": 0.05734717473387718,
      "learning_rate": 4.101e-05,
      "loss": 0.0029,
      "step": 26970
    },
    {
      "epoch": 1.4389333333333334,
      "grad_norm": 0.1446666270494461,
      "learning_rate": 4.100666666666667e-05,
      "loss": 0.0038,
      "step": 26980
    },
    {
      "epoch": 1.4394666666666667,
      "grad_norm": 0.46030452847480774,
      "learning_rate": 4.100333333333334e-05,
      "loss": 0.0028,
      "step": 26990
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.02901715226471424,
      "learning_rate": 4.1e-05,
      "loss": 0.0022,
      "step": 27000
    },
    {
      "epoch": 1.4405333333333332,
      "grad_norm": 0.46028363704681396,
      "learning_rate": 4.0996666666666667e-05,
      "loss": 0.0032,
      "step": 27010
    },
    {
      "epoch": 1.4410666666666667,
      "grad_norm": 0.3447023630142212,
      "learning_rate": 4.099333333333333e-05,
      "loss": 0.004,
      "step": 27020
    },
    {
      "epoch": 1.4416,
      "grad_norm": 0.0024224163498729467,
      "learning_rate": 4.099e-05,
      "loss": 0.0027,
      "step": 27030
    },
    {
      "epoch": 1.4421333333333333,
      "grad_norm": 0.14401912689208984,
      "learning_rate": 4.0986666666666665e-05,
      "loss": 0.0032,
      "step": 27040
    },
    {
      "epoch": 1.4426666666666668,
      "grad_norm": 0.029735011979937553,
      "learning_rate": 4.098333333333334e-05,
      "loss": 0.0043,
      "step": 27050
    },
    {
      "epoch": 1.4432,
      "grad_norm": 0.40136075019836426,
      "learning_rate": 4.0980000000000004e-05,
      "loss": 0.004,
      "step": 27060
    },
    {
      "epoch": 1.4437333333333333,
      "grad_norm": 0.1722409427165985,
      "learning_rate": 4.097666666666667e-05,
      "loss": 0.0028,
      "step": 27070
    },
    {
      "epoch": 1.4442666666666666,
      "grad_norm": 0.14346443116664886,
      "learning_rate": 4.0973333333333336e-05,
      "loss": 0.003,
      "step": 27080
    },
    {
      "epoch": 1.4447999999999999,
      "grad_norm": 0.14366364479064941,
      "learning_rate": 4.097e-05,
      "loss": 0.0035,
      "step": 27090
    },
    {
      "epoch": 1.4453333333333334,
      "grad_norm": 0.11487026512622833,
      "learning_rate": 4.096666666666667e-05,
      "loss": 0.0029,
      "step": 27100
    },
    {
      "epoch": 1.4458666666666666,
      "grad_norm": 0.057825781404972076,
      "learning_rate": 4.0963333333333335e-05,
      "loss": 0.0031,
      "step": 27110
    },
    {
      "epoch": 1.4464000000000001,
      "grad_norm": 0.05765475332736969,
      "learning_rate": 4.096e-05,
      "loss": 0.0028,
      "step": 27120
    },
    {
      "epoch": 1.4469333333333334,
      "grad_norm": 0.2888663113117218,
      "learning_rate": 4.0956666666666674e-05,
      "loss": 0.0031,
      "step": 27130
    },
    {
      "epoch": 1.4474666666666667,
      "grad_norm": 0.17322294414043427,
      "learning_rate": 4.095333333333334e-05,
      "loss": 0.0027,
      "step": 27140
    },
    {
      "epoch": 1.448,
      "grad_norm": 0.4606728255748749,
      "learning_rate": 4.095e-05,
      "loss": 0.0029,
      "step": 27150
    },
    {
      "epoch": 1.4485333333333332,
      "grad_norm": 0.0867997407913208,
      "learning_rate": 4.0946666666666665e-05,
      "loss": 0.0041,
      "step": 27160
    },
    {
      "epoch": 1.4490666666666667,
      "grad_norm": 0.17196357250213623,
      "learning_rate": 4.094333333333333e-05,
      "loss": 0.0039,
      "step": 27170
    },
    {
      "epoch": 1.4496,
      "grad_norm": 0.2585241496562958,
      "learning_rate": 4.094e-05,
      "loss": 0.0033,
      "step": 27180
    },
    {
      "epoch": 1.4501333333333333,
      "grad_norm": 0.012352251447737217,
      "learning_rate": 4.093666666666667e-05,
      "loss": 0.0027,
      "step": 27190
    },
    {
      "epoch": 1.4506666666666668,
      "grad_norm": 0.0578276626765728,
      "learning_rate": 4.093333333333334e-05,
      "loss": 0.0053,
      "step": 27200
    },
    {
      "epoch": 1.4512,
      "grad_norm": 0.1727200299501419,
      "learning_rate": 4.093e-05,
      "loss": 0.003,
      "step": 27210
    },
    {
      "epoch": 1.4517333333333333,
      "grad_norm": 0.02881843037903309,
      "learning_rate": 4.092666666666667e-05,
      "loss": 0.0025,
      "step": 27220
    },
    {
      "epoch": 1.4522666666666666,
      "grad_norm": 0.11488235741853714,
      "learning_rate": 4.0923333333333335e-05,
      "loss": 0.0028,
      "step": 27230
    },
    {
      "epoch": 1.4527999999999999,
      "grad_norm": 0.05895967781543732,
      "learning_rate": 4.092e-05,
      "loss": 0.0024,
      "step": 27240
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 0.17401844263076782,
      "learning_rate": 4.091666666666667e-05,
      "loss": 0.0034,
      "step": 27250
    },
    {
      "epoch": 1.4538666666666666,
      "grad_norm": 0.28860366344451904,
      "learning_rate": 4.0913333333333334e-05,
      "loss": 0.0035,
      "step": 27260
    },
    {
      "epoch": 1.4544000000000001,
      "grad_norm": 0.1440732181072235,
      "learning_rate": 4.0910000000000006e-05,
      "loss": 0.0023,
      "step": 27270
    },
    {
      "epoch": 1.4549333333333334,
      "grad_norm": 0.4319908916950226,
      "learning_rate": 4.090666666666667e-05,
      "loss": 0.0049,
      "step": 27280
    },
    {
      "epoch": 1.4554666666666667,
      "grad_norm": 0.28782981634140015,
      "learning_rate": 4.090333333333334e-05,
      "loss": 0.003,
      "step": 27290
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.02875974029302597,
      "learning_rate": 4.09e-05,
      "loss": 0.0034,
      "step": 27300
    },
    {
      "epoch": 1.4565333333333332,
      "grad_norm": 0.029436733573675156,
      "learning_rate": 4.0896666666666664e-05,
      "loss": 0.0023,
      "step": 27310
    },
    {
      "epoch": 1.4570666666666667,
      "grad_norm": 0.23086772859096527,
      "learning_rate": 4.089333333333333e-05,
      "loss": 0.0027,
      "step": 27320
    },
    {
      "epoch": 1.4576,
      "grad_norm": 0.1149311512708664,
      "learning_rate": 4.089e-05,
      "loss": 0.0035,
      "step": 27330
    },
    {
      "epoch": 1.4581333333333333,
      "grad_norm": 0.029384957626461983,
      "learning_rate": 4.088666666666667e-05,
      "loss": 0.0036,
      "step": 27340
    },
    {
      "epoch": 1.4586666666666668,
      "grad_norm": 0.25934866070747375,
      "learning_rate": 4.0883333333333335e-05,
      "loss": 0.0031,
      "step": 27350
    },
    {
      "epoch": 1.4592,
      "grad_norm": 0.11639545857906342,
      "learning_rate": 4.088e-05,
      "loss": 0.0037,
      "step": 27360
    },
    {
      "epoch": 1.4597333333333333,
      "grad_norm": 0.0029455176554620266,
      "learning_rate": 4.087666666666667e-05,
      "loss": 0.0031,
      "step": 27370
    },
    {
      "epoch": 1.4602666666666666,
      "grad_norm": 0.20213323831558228,
      "learning_rate": 4.0873333333333334e-05,
      "loss": 0.004,
      "step": 27380
    },
    {
      "epoch": 1.4607999999999999,
      "grad_norm": 0.11623913794755936,
      "learning_rate": 4.087e-05,
      "loss": 0.0026,
      "step": 27390
    },
    {
      "epoch": 1.4613333333333334,
      "grad_norm": 0.028861384838819504,
      "learning_rate": 4.086666666666667e-05,
      "loss": 0.0044,
      "step": 27400
    },
    {
      "epoch": 1.4618666666666666,
      "grad_norm": 0.346439391374588,
      "learning_rate": 4.086333333333334e-05,
      "loss": 0.0035,
      "step": 27410
    },
    {
      "epoch": 1.4624,
      "grad_norm": 0.17258581519126892,
      "learning_rate": 4.0860000000000005e-05,
      "loss": 0.0025,
      "step": 27420
    },
    {
      "epoch": 1.4629333333333334,
      "grad_norm": 0.05749274790287018,
      "learning_rate": 4.085666666666667e-05,
      "loss": 0.0046,
      "step": 27430
    },
    {
      "epoch": 1.4634666666666667,
      "grad_norm": 0.11574586480855942,
      "learning_rate": 4.085333333333334e-05,
      "loss": 0.0032,
      "step": 27440
    },
    {
      "epoch": 1.464,
      "grad_norm": 0.14468351006507874,
      "learning_rate": 4.085e-05,
      "loss": 0.0041,
      "step": 27450
    },
    {
      "epoch": 1.4645333333333332,
      "grad_norm": 0.08640846610069275,
      "learning_rate": 4.084666666666667e-05,
      "loss": 0.0029,
      "step": 27460
    },
    {
      "epoch": 1.4650666666666667,
      "grad_norm": 0.6326050758361816,
      "learning_rate": 4.0843333333333336e-05,
      "loss": 0.0031,
      "step": 27470
    },
    {
      "epoch": 1.4656,
      "grad_norm": 0.03016173467040062,
      "learning_rate": 4.084e-05,
      "loss": 0.0035,
      "step": 27480
    },
    {
      "epoch": 1.4661333333333333,
      "grad_norm": 0.058014508336782455,
      "learning_rate": 4.083666666666667e-05,
      "loss": 0.0029,
      "step": 27490
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.11467072367668152,
      "learning_rate": 4.0833333333333334e-05,
      "loss": 0.0035,
      "step": 27500
    },
    {
      "epoch": 1.4672,
      "grad_norm": 0.23022092878818512,
      "learning_rate": 4.083e-05,
      "loss": 0.0024,
      "step": 27510
    },
    {
      "epoch": 1.4677333333333333,
      "grad_norm": 0.058292828500270844,
      "learning_rate": 4.0826666666666667e-05,
      "loss": 0.0021,
      "step": 27520
    },
    {
      "epoch": 1.4682666666666666,
      "grad_norm": 0.20245139300823212,
      "learning_rate": 4.082333333333333e-05,
      "loss": 0.0032,
      "step": 27530
    },
    {
      "epoch": 1.4687999999999999,
      "grad_norm": 0.08675796538591385,
      "learning_rate": 4.0820000000000006e-05,
      "loss": 0.0027,
      "step": 27540
    },
    {
      "epoch": 1.4693333333333334,
      "grad_norm": 0.1448749601840973,
      "learning_rate": 4.081666666666667e-05,
      "loss": 0.0028,
      "step": 27550
    },
    {
      "epoch": 1.4698666666666667,
      "grad_norm": 0.20211417973041534,
      "learning_rate": 4.081333333333334e-05,
      "loss": 0.0036,
      "step": 27560
    },
    {
      "epoch": 1.4704,
      "grad_norm": 0.02981489710509777,
      "learning_rate": 4.0810000000000004e-05,
      "loss": 0.0019,
      "step": 27570
    },
    {
      "epoch": 1.4709333333333334,
      "grad_norm": 0.11570357531309128,
      "learning_rate": 4.080666666666667e-05,
      "loss": 0.0035,
      "step": 27580
    },
    {
      "epoch": 1.4714666666666667,
      "grad_norm": 0.172820582985878,
      "learning_rate": 4.0803333333333336e-05,
      "loss": 0.0036,
      "step": 27590
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.23048949241638184,
      "learning_rate": 4.08e-05,
      "loss": 0.0045,
      "step": 27600
    },
    {
      "epoch": 1.4725333333333332,
      "grad_norm": 0.23054082691669464,
      "learning_rate": 4.079666666666667e-05,
      "loss": 0.0073,
      "step": 27610
    },
    {
      "epoch": 1.4730666666666667,
      "grad_norm": 0.3474780321121216,
      "learning_rate": 4.0793333333333335e-05,
      "loss": 0.0027,
      "step": 27620
    },
    {
      "epoch": 1.4736,
      "grad_norm": 1.158218264579773,
      "learning_rate": 4.079e-05,
      "loss": 0.0039,
      "step": 27630
    },
    {
      "epoch": 1.4741333333333333,
      "grad_norm": 0.28839024901390076,
      "learning_rate": 4.078666666666667e-05,
      "loss": 0.0033,
      "step": 27640
    },
    {
      "epoch": 1.4746666666666668,
      "grad_norm": 0.20181822776794434,
      "learning_rate": 4.078333333333333e-05,
      "loss": 0.0029,
      "step": 27650
    },
    {
      "epoch": 1.4752,
      "grad_norm": 0.1439589411020279,
      "learning_rate": 4.078e-05,
      "loss": 0.0042,
      "step": 27660
    },
    {
      "epoch": 1.4757333333333333,
      "grad_norm": 0.3455166518688202,
      "learning_rate": 4.0776666666666665e-05,
      "loss": 0.0033,
      "step": 27670
    },
    {
      "epoch": 1.4762666666666666,
      "grad_norm": 0.028860770165920258,
      "learning_rate": 4.077333333333334e-05,
      "loss": 0.0039,
      "step": 27680
    },
    {
      "epoch": 1.4768,
      "grad_norm": 0.34516051411628723,
      "learning_rate": 4.0770000000000004e-05,
      "loss": 0.0028,
      "step": 27690
    },
    {
      "epoch": 1.4773333333333334,
      "grad_norm": 0.02895159088075161,
      "learning_rate": 4.076666666666667e-05,
      "loss": 0.0026,
      "step": 27700
    },
    {
      "epoch": 1.4778666666666667,
      "grad_norm": 0.08608882874250412,
      "learning_rate": 4.076333333333334e-05,
      "loss": 0.0032,
      "step": 27710
    },
    {
      "epoch": 1.4784,
      "grad_norm": 0.028671568259596825,
      "learning_rate": 4.076e-05,
      "loss": 0.0051,
      "step": 27720
    },
    {
      "epoch": 1.4789333333333334,
      "grad_norm": 0.3171119689941406,
      "learning_rate": 4.075666666666667e-05,
      "loss": 0.0044,
      "step": 27730
    },
    {
      "epoch": 1.4794666666666667,
      "grad_norm": 0.28879427909851074,
      "learning_rate": 4.0753333333333335e-05,
      "loss": 0.0027,
      "step": 27740
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.6038025617599487,
      "learning_rate": 4.075e-05,
      "loss": 0.0026,
      "step": 27750
    },
    {
      "epoch": 1.4805333333333333,
      "grad_norm": 0.2596094012260437,
      "learning_rate": 4.074666666666667e-05,
      "loss": 0.0035,
      "step": 27760
    },
    {
      "epoch": 1.4810666666666665,
      "grad_norm": 0.0574638806283474,
      "learning_rate": 4.0743333333333333e-05,
      "loss": 0.004,
      "step": 27770
    },
    {
      "epoch": 1.4816,
      "grad_norm": 0.20251387357711792,
      "learning_rate": 4.074e-05,
      "loss": 0.0034,
      "step": 27780
    },
    {
      "epoch": 1.4821333333333333,
      "grad_norm": 0.2018410861492157,
      "learning_rate": 4.0736666666666666e-05,
      "loss": 0.0036,
      "step": 27790
    },
    {
      "epoch": 1.4826666666666668,
      "grad_norm": 0.5767166018486023,
      "learning_rate": 4.073333333333333e-05,
      "loss": 0.0038,
      "step": 27800
    },
    {
      "epoch": 1.4832,
      "grad_norm": 0.1451830118894577,
      "learning_rate": 4.0730000000000005e-05,
      "loss": 0.0031,
      "step": 27810
    },
    {
      "epoch": 1.4837333333333333,
      "grad_norm": 0.14928779006004333,
      "learning_rate": 4.072666666666667e-05,
      "loss": 0.0035,
      "step": 27820
    },
    {
      "epoch": 1.4842666666666666,
      "grad_norm": 0.2601442337036133,
      "learning_rate": 4.072333333333334e-05,
      "loss": 0.0042,
      "step": 27830
    },
    {
      "epoch": 1.4848,
      "grad_norm": 0.05795585736632347,
      "learning_rate": 4.072e-05,
      "loss": 0.0039,
      "step": 27840
    },
    {
      "epoch": 1.4853333333333334,
      "grad_norm": 0.0586160309612751,
      "learning_rate": 4.071666666666667e-05,
      "loss": 0.0039,
      "step": 27850
    },
    {
      "epoch": 1.4858666666666667,
      "grad_norm": 0.25906845927238464,
      "learning_rate": 4.0713333333333335e-05,
      "loss": 0.0031,
      "step": 27860
    },
    {
      "epoch": 1.4864,
      "grad_norm": 0.1437048614025116,
      "learning_rate": 4.071e-05,
      "loss": 0.0045,
      "step": 27870
    },
    {
      "epoch": 1.4869333333333334,
      "grad_norm": 0.02863011695444584,
      "learning_rate": 4.070666666666667e-05,
      "loss": 0.0034,
      "step": 27880
    },
    {
      "epoch": 1.4874666666666667,
      "grad_norm": 0.028971200808882713,
      "learning_rate": 4.070333333333334e-05,
      "loss": 0.0026,
      "step": 27890
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.058756984770298004,
      "learning_rate": 4.07e-05,
      "loss": 0.0022,
      "step": 27900
    },
    {
      "epoch": 1.4885333333333333,
      "grad_norm": 0.05963856726884842,
      "learning_rate": 4.0696666666666666e-05,
      "loss": 0.0035,
      "step": 27910
    },
    {
      "epoch": 1.4890666666666665,
      "grad_norm": 0.5175701975822449,
      "learning_rate": 4.069333333333333e-05,
      "loss": 0.003,
      "step": 27920
    },
    {
      "epoch": 1.4896,
      "grad_norm": 0.007761381566524506,
      "learning_rate": 4.069e-05,
      "loss": 0.0032,
      "step": 27930
    },
    {
      "epoch": 1.4901333333333333,
      "grad_norm": 0.1725211888551712,
      "learning_rate": 4.0686666666666664e-05,
      "loss": 0.0028,
      "step": 27940
    },
    {
      "epoch": 1.4906666666666666,
      "grad_norm": 0.17232646048069,
      "learning_rate": 4.068333333333334e-05,
      "loss": 0.004,
      "step": 27950
    },
    {
      "epoch": 1.4912,
      "grad_norm": 0.11463399231433868,
      "learning_rate": 4.0680000000000004e-05,
      "loss": 0.004,
      "step": 27960
    },
    {
      "epoch": 1.4917333333333334,
      "grad_norm": 0.11525894701480865,
      "learning_rate": 4.067666666666667e-05,
      "loss": 0.003,
      "step": 27970
    },
    {
      "epoch": 1.4922666666666666,
      "grad_norm": 0.08603141456842422,
      "learning_rate": 4.0673333333333336e-05,
      "loss": 0.0031,
      "step": 27980
    },
    {
      "epoch": 1.4928,
      "grad_norm": 0.057451412081718445,
      "learning_rate": 4.067e-05,
      "loss": 0.0025,
      "step": 27990
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 0.11497573554515839,
      "learning_rate": 4.066666666666667e-05,
      "loss": 0.0038,
      "step": 28000
    },
    {
      "epoch": 1.4938666666666667,
      "grad_norm": 0.4004535675048828,
      "learning_rate": 4.0663333333333334e-05,
      "loss": 0.003,
      "step": 28010
    },
    {
      "epoch": 1.4944,
      "grad_norm": 0.22860944271087646,
      "learning_rate": 4.066e-05,
      "loss": 0.0021,
      "step": 28020
    },
    {
      "epoch": 1.4949333333333334,
      "grad_norm": 0.25779616832733154,
      "learning_rate": 4.065666666666667e-05,
      "loss": 0.0038,
      "step": 28030
    },
    {
      "epoch": 1.4954666666666667,
      "grad_norm": 0.432105153799057,
      "learning_rate": 4.065333333333334e-05,
      "loss": 0.0036,
      "step": 28040
    },
    {
      "epoch": 1.496,
      "grad_norm": 0.14509980380535126,
      "learning_rate": 4.065e-05,
      "loss": 0.0036,
      "step": 28050
    },
    {
      "epoch": 1.4965333333333333,
      "grad_norm": 0.03215324506163597,
      "learning_rate": 4.0646666666666665e-05,
      "loss": 0.0038,
      "step": 28060
    },
    {
      "epoch": 1.4970666666666665,
      "grad_norm": 0.05853730067610741,
      "learning_rate": 4.064333333333333e-05,
      "loss": 0.0037,
      "step": 28070
    },
    {
      "epoch": 1.4976,
      "grad_norm": 0.004538445267826319,
      "learning_rate": 4.064e-05,
      "loss": 0.0044,
      "step": 28080
    },
    {
      "epoch": 1.4981333333333333,
      "grad_norm": 0.2025531828403473,
      "learning_rate": 4.063666666666667e-05,
      "loss": 0.0035,
      "step": 28090
    },
    {
      "epoch": 1.4986666666666666,
      "grad_norm": 0.02891373261809349,
      "learning_rate": 4.0633333333333336e-05,
      "loss": 0.0041,
      "step": 28100
    },
    {
      "epoch": 1.4992,
      "grad_norm": 0.31572723388671875,
      "learning_rate": 4.063e-05,
      "loss": 0.0029,
      "step": 28110
    },
    {
      "epoch": 1.4997333333333334,
      "grad_norm": 0.02878841571509838,
      "learning_rate": 4.062666666666667e-05,
      "loss": 0.0026,
      "step": 28120
    },
    {
      "epoch": 1.5002666666666666,
      "grad_norm": 0.0572194941341877,
      "learning_rate": 4.0623333333333335e-05,
      "loss": 0.0035,
      "step": 28130
    },
    {
      "epoch": 1.5008,
      "grad_norm": 0.0032079967204481363,
      "learning_rate": 4.062e-05,
      "loss": 0.0026,
      "step": 28140
    },
    {
      "epoch": 1.5013333333333332,
      "grad_norm": 0.28641268610954285,
      "learning_rate": 4.061666666666667e-05,
      "loss": 0.0041,
      "step": 28150
    },
    {
      "epoch": 1.5018666666666667,
      "grad_norm": 0.17152374982833862,
      "learning_rate": 4.061333333333334e-05,
      "loss": 0.0037,
      "step": 28160
    },
    {
      "epoch": 1.5024,
      "grad_norm": 0.03137384355068207,
      "learning_rate": 4.0610000000000006e-05,
      "loss": 0.0043,
      "step": 28170
    },
    {
      "epoch": 1.5029333333333335,
      "grad_norm": 0.007383192889392376,
      "learning_rate": 4.060666666666667e-05,
      "loss": 0.0027,
      "step": 28180
    },
    {
      "epoch": 1.5034666666666667,
      "grad_norm": 0.0287932138890028,
      "learning_rate": 4.060333333333334e-05,
      "loss": 0.0038,
      "step": 28190
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.029088851064443588,
      "learning_rate": 4.0600000000000004e-05,
      "loss": 0.0025,
      "step": 28200
    },
    {
      "epoch": 1.5045333333333333,
      "grad_norm": 0.05827205628156662,
      "learning_rate": 4.0596666666666664e-05,
      "loss": 0.0028,
      "step": 28210
    },
    {
      "epoch": 1.5050666666666666,
      "grad_norm": 0.25811830163002014,
      "learning_rate": 4.0593333333333337e-05,
      "loss": 0.0036,
      "step": 28220
    },
    {
      "epoch": 1.5056,
      "grad_norm": 0.05726657435297966,
      "learning_rate": 4.059e-05,
      "loss": 0.004,
      "step": 28230
    },
    {
      "epoch": 1.5061333333333333,
      "grad_norm": 0.02921379916369915,
      "learning_rate": 4.058666666666667e-05,
      "loss": 0.0067,
      "step": 28240
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 0.1721869558095932,
      "learning_rate": 4.0583333333333335e-05,
      "loss": 0.0024,
      "step": 28250
    },
    {
      "epoch": 1.5072,
      "grad_norm": 0.05737675726413727,
      "learning_rate": 4.058e-05,
      "loss": 0.0024,
      "step": 28260
    },
    {
      "epoch": 1.5077333333333334,
      "grad_norm": 0.4011353552341461,
      "learning_rate": 4.057666666666667e-05,
      "loss": 0.0031,
      "step": 28270
    },
    {
      "epoch": 1.5082666666666666,
      "grad_norm": 0.08597851544618607,
      "learning_rate": 4.057333333333333e-05,
      "loss": 0.0035,
      "step": 28280
    },
    {
      "epoch": 1.5088,
      "grad_norm": 0.14332549273967743,
      "learning_rate": 4.057e-05,
      "loss": 0.0038,
      "step": 28290
    },
    {
      "epoch": 1.5093333333333332,
      "grad_norm": 0.0860794261097908,
      "learning_rate": 4.056666666666667e-05,
      "loss": 0.0026,
      "step": 28300
    },
    {
      "epoch": 1.5098666666666667,
      "grad_norm": 0.22903190553188324,
      "learning_rate": 4.056333333333334e-05,
      "loss": 0.0041,
      "step": 28310
    },
    {
      "epoch": 1.5104,
      "grad_norm": 0.057674143463373184,
      "learning_rate": 4.0560000000000005e-05,
      "loss": 0.0042,
      "step": 28320
    },
    {
      "epoch": 1.5109333333333335,
      "grad_norm": 0.029142146930098534,
      "learning_rate": 4.055666666666667e-05,
      "loss": 0.0034,
      "step": 28330
    },
    {
      "epoch": 1.5114666666666667,
      "grad_norm": 0.3455211818218231,
      "learning_rate": 4.055333333333334e-05,
      "loss": 0.0042,
      "step": 28340
    },
    {
      "epoch": 1.512,
      "grad_norm": 0.3456513285636902,
      "learning_rate": 4.055e-05,
      "loss": 0.0033,
      "step": 28350
    },
    {
      "epoch": 1.5125333333333333,
      "grad_norm": 0.17335812747478485,
      "learning_rate": 4.054666666666667e-05,
      "loss": 0.0042,
      "step": 28360
    },
    {
      "epoch": 1.5130666666666666,
      "grad_norm": 0.08574612438678741,
      "learning_rate": 4.0543333333333335e-05,
      "loss": 0.0034,
      "step": 28370
    },
    {
      "epoch": 1.5135999999999998,
      "grad_norm": 0.4604748785495758,
      "learning_rate": 4.054e-05,
      "loss": 0.0032,
      "step": 28380
    },
    {
      "epoch": 1.5141333333333333,
      "grad_norm": 0.05789096653461456,
      "learning_rate": 4.053666666666667e-05,
      "loss": 0.0038,
      "step": 28390
    },
    {
      "epoch": 1.5146666666666668,
      "grad_norm": 0.2016550898551941,
      "learning_rate": 4.0533333333333334e-05,
      "loss": 0.0056,
      "step": 28400
    },
    {
      "epoch": 1.5152,
      "grad_norm": 0.3456619381904602,
      "learning_rate": 4.053e-05,
      "loss": 0.0033,
      "step": 28410
    },
    {
      "epoch": 1.5157333333333334,
      "grad_norm": 0.28747984766960144,
      "learning_rate": 4.0526666666666666e-05,
      "loss": 0.0024,
      "step": 28420
    },
    {
      "epoch": 1.5162666666666667,
      "grad_norm": 0.1151421070098877,
      "learning_rate": 4.052333333333333e-05,
      "loss": 0.004,
      "step": 28430
    },
    {
      "epoch": 1.5168,
      "grad_norm": 0.14332148432731628,
      "learning_rate": 4.0520000000000005e-05,
      "loss": 0.0024,
      "step": 28440
    },
    {
      "epoch": 1.5173333333333332,
      "grad_norm": 0.057607293128967285,
      "learning_rate": 4.051666666666667e-05,
      "loss": 0.0036,
      "step": 28450
    },
    {
      "epoch": 1.5178666666666667,
      "grad_norm": 0.0037306942977011204,
      "learning_rate": 4.051333333333334e-05,
      "loss": 0.0044,
      "step": 28460
    },
    {
      "epoch": 1.5184,
      "grad_norm": 0.28742361068725586,
      "learning_rate": 4.0510000000000003e-05,
      "loss": 0.0041,
      "step": 28470
    },
    {
      "epoch": 1.5189333333333335,
      "grad_norm": 0.3748573064804077,
      "learning_rate": 4.050666666666667e-05,
      "loss": 0.0039,
      "step": 28480
    },
    {
      "epoch": 1.5194666666666667,
      "grad_norm": 0.05910066142678261,
      "learning_rate": 4.0503333333333336e-05,
      "loss": 0.0037,
      "step": 28490
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.5777019262313843,
      "learning_rate": 4.05e-05,
      "loss": 0.0039,
      "step": 28500
    },
    {
      "epoch": 1.5205333333333333,
      "grad_norm": 0.11563829332590103,
      "learning_rate": 4.049666666666667e-05,
      "loss": 0.0029,
      "step": 28510
    },
    {
      "epoch": 1.5210666666666666,
      "grad_norm": 0.2596266269683838,
      "learning_rate": 4.0493333333333334e-05,
      "loss": 0.0027,
      "step": 28520
    },
    {
      "epoch": 1.5215999999999998,
      "grad_norm": 0.1723470836877823,
      "learning_rate": 4.049e-05,
      "loss": 0.0025,
      "step": 28530
    },
    {
      "epoch": 1.5221333333333333,
      "grad_norm": 0.17248502373695374,
      "learning_rate": 4.0486666666666666e-05,
      "loss": 0.0028,
      "step": 28540
    },
    {
      "epoch": 1.5226666666666666,
      "grad_norm": 0.2879941463470459,
      "learning_rate": 4.048333333333333e-05,
      "loss": 0.0034,
      "step": 28550
    },
    {
      "epoch": 1.5232,
      "grad_norm": 0.43136778473854065,
      "learning_rate": 4.048e-05,
      "loss": 0.0027,
      "step": 28560
    },
    {
      "epoch": 1.5237333333333334,
      "grad_norm": 0.057851798832416534,
      "learning_rate": 4.047666666666667e-05,
      "loss": 0.0027,
      "step": 28570
    },
    {
      "epoch": 1.5242666666666667,
      "grad_norm": 0.2595210373401642,
      "learning_rate": 4.047333333333334e-05,
      "loss": 0.0022,
      "step": 28580
    },
    {
      "epoch": 1.5248,
      "grad_norm": 0.2589028775691986,
      "learning_rate": 4.0470000000000004e-05,
      "loss": 0.004,
      "step": 28590
    },
    {
      "epoch": 1.5253333333333332,
      "grad_norm": 0.1721421778202057,
      "learning_rate": 4.046666666666667e-05,
      "loss": 0.003,
      "step": 28600
    },
    {
      "epoch": 1.5258666666666667,
      "grad_norm": 0.2582811415195465,
      "learning_rate": 4.0463333333333336e-05,
      "loss": 0.0031,
      "step": 28610
    },
    {
      "epoch": 1.5264,
      "grad_norm": 0.172347754240036,
      "learning_rate": 4.046e-05,
      "loss": 0.0025,
      "step": 28620
    },
    {
      "epoch": 1.5269333333333335,
      "grad_norm": 0.40137237310409546,
      "learning_rate": 4.045666666666667e-05,
      "loss": 0.0035,
      "step": 28630
    },
    {
      "epoch": 1.5274666666666668,
      "grad_norm": 0.009075100533664227,
      "learning_rate": 4.0453333333333335e-05,
      "loss": 0.0037,
      "step": 28640
    },
    {
      "epoch": 1.528,
      "grad_norm": 0.14322714507579803,
      "learning_rate": 4.045000000000001e-05,
      "loss": 0.0042,
      "step": 28650
    },
    {
      "epoch": 1.5285333333333333,
      "grad_norm": 0.3537508547306061,
      "learning_rate": 4.044666666666667e-05,
      "loss": 0.0055,
      "step": 28660
    },
    {
      "epoch": 1.5290666666666666,
      "grad_norm": 0.08600764721632004,
      "learning_rate": 4.044333333333333e-05,
      "loss": 0.0041,
      "step": 28670
    },
    {
      "epoch": 1.5295999999999998,
      "grad_norm": 0.028692733496427536,
      "learning_rate": 4.044e-05,
      "loss": 0.003,
      "step": 28680
    },
    {
      "epoch": 1.5301333333333333,
      "grad_norm": 0.11450197547674179,
      "learning_rate": 4.0436666666666665e-05,
      "loss": 0.0029,
      "step": 28690
    },
    {
      "epoch": 1.5306666666666666,
      "grad_norm": 0.37440499663352966,
      "learning_rate": 4.043333333333333e-05,
      "loss": 0.0023,
      "step": 28700
    },
    {
      "epoch": 1.5312000000000001,
      "grad_norm": 0.317070335149765,
      "learning_rate": 4.0430000000000004e-05,
      "loss": 0.0026,
      "step": 28710
    },
    {
      "epoch": 1.5317333333333334,
      "grad_norm": 0.374234139919281,
      "learning_rate": 4.042666666666667e-05,
      "loss": 0.0029,
      "step": 28720
    },
    {
      "epoch": 1.5322666666666667,
      "grad_norm": 0.08674705773591995,
      "learning_rate": 4.0423333333333337e-05,
      "loss": 0.0023,
      "step": 28730
    },
    {
      "epoch": 1.5328,
      "grad_norm": 0.46010860800743103,
      "learning_rate": 4.042e-05,
      "loss": 0.0035,
      "step": 28740
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.004839157219976187,
      "learning_rate": 4.041666666666667e-05,
      "loss": 0.0051,
      "step": 28750
    },
    {
      "epoch": 1.5338666666666667,
      "grad_norm": 0.029575984925031662,
      "learning_rate": 4.0413333333333335e-05,
      "loss": 0.0029,
      "step": 28760
    },
    {
      "epoch": 1.5344,
      "grad_norm": 0.2586449086666107,
      "learning_rate": 4.041e-05,
      "loss": 0.0045,
      "step": 28770
    },
    {
      "epoch": 1.5349333333333335,
      "grad_norm": 0.40318313241004944,
      "learning_rate": 4.040666666666667e-05,
      "loss": 0.0041,
      "step": 28780
    },
    {
      "epoch": 1.5354666666666668,
      "grad_norm": 0.34533360600471497,
      "learning_rate": 4.040333333333334e-05,
      "loss": 0.002,
      "step": 28790
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.20152685046195984,
      "learning_rate": 4.0400000000000006e-05,
      "loss": 0.0021,
      "step": 28800
    },
    {
      "epoch": 1.5365333333333333,
      "grad_norm": 0.31629452109336853,
      "learning_rate": 4.0396666666666666e-05,
      "loss": 0.0026,
      "step": 28810
    },
    {
      "epoch": 1.5370666666666666,
      "grad_norm": 0.2879178524017334,
      "learning_rate": 4.039333333333333e-05,
      "loss": 0.0017,
      "step": 28820
    },
    {
      "epoch": 1.5375999999999999,
      "grad_norm": 0.1721273809671402,
      "learning_rate": 4.039e-05,
      "loss": 0.0024,
      "step": 28830
    },
    {
      "epoch": 1.5381333333333334,
      "grad_norm": 2.7298099994659424,
      "learning_rate": 4.0386666666666664e-05,
      "loss": 0.0035,
      "step": 28840
    },
    {
      "epoch": 1.5386666666666666,
      "grad_norm": 0.22936591506004333,
      "learning_rate": 4.038333333333334e-05,
      "loss": 0.0042,
      "step": 28850
    },
    {
      "epoch": 1.5392000000000001,
      "grad_norm": 0.028660820797085762,
      "learning_rate": 4.038e-05,
      "loss": 0.0029,
      "step": 28860
    },
    {
      "epoch": 1.5397333333333334,
      "grad_norm": 0.4010763168334961,
      "learning_rate": 4.037666666666667e-05,
      "loss": 0.0024,
      "step": 28870
    },
    {
      "epoch": 1.5402666666666667,
      "grad_norm": 0.14338363707065582,
      "learning_rate": 4.0373333333333335e-05,
      "loss": 0.0027,
      "step": 28880
    },
    {
      "epoch": 1.5408,
      "grad_norm": 0.14322710037231445,
      "learning_rate": 4.037e-05,
      "loss": 0.0039,
      "step": 28890
    },
    {
      "epoch": 1.5413333333333332,
      "grad_norm": 0.05733279883861542,
      "learning_rate": 4.036666666666667e-05,
      "loss": 0.0032,
      "step": 28900
    },
    {
      "epoch": 1.5418666666666667,
      "grad_norm": 0.17183944582939148,
      "learning_rate": 4.0363333333333334e-05,
      "loss": 0.003,
      "step": 28910
    },
    {
      "epoch": 1.5424,
      "grad_norm": 0.20059311389923096,
      "learning_rate": 4.0360000000000007e-05,
      "loss": 0.0028,
      "step": 28920
    },
    {
      "epoch": 1.5429333333333335,
      "grad_norm": 0.02887359820306301,
      "learning_rate": 4.035666666666667e-05,
      "loss": 0.0028,
      "step": 28930
    },
    {
      "epoch": 1.5434666666666668,
      "grad_norm": 0.2577189803123474,
      "learning_rate": 4.035333333333334e-05,
      "loss": 0.0029,
      "step": 28940
    },
    {
      "epoch": 1.544,
      "grad_norm": 0.02879277803003788,
      "learning_rate": 4.0350000000000005e-05,
      "loss": 0.0039,
      "step": 28950
    },
    {
      "epoch": 1.5445333333333333,
      "grad_norm": 0.02860712818801403,
      "learning_rate": 4.0346666666666664e-05,
      "loss": 0.0031,
      "step": 28960
    },
    {
      "epoch": 1.5450666666666666,
      "grad_norm": 0.11450262367725372,
      "learning_rate": 4.034333333333333e-05,
      "loss": 0.0039,
      "step": 28970
    },
    {
      "epoch": 1.5455999999999999,
      "grad_norm": 0.028657708317041397,
      "learning_rate": 4.034e-05,
      "loss": 0.0045,
      "step": 28980
    },
    {
      "epoch": 1.5461333333333334,
      "grad_norm": 0.5733635425567627,
      "learning_rate": 4.033666666666667e-05,
      "loss": 0.0022,
      "step": 28990
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 0.25817403197288513,
      "learning_rate": 4.0333333333333336e-05,
      "loss": 0.0029,
      "step": 29000
    },
    {
      "epoch": 1.5472000000000001,
      "grad_norm": 0.6312552690505981,
      "learning_rate": 4.033e-05,
      "loss": 0.0032,
      "step": 29010
    },
    {
      "epoch": 1.5477333333333334,
      "grad_norm": 0.2007106989622116,
      "learning_rate": 4.032666666666667e-05,
      "loss": 0.0031,
      "step": 29020
    },
    {
      "epoch": 1.5482666666666667,
      "grad_norm": 0.14316685497760773,
      "learning_rate": 4.0323333333333334e-05,
      "loss": 0.0042,
      "step": 29030
    },
    {
      "epoch": 1.5488,
      "grad_norm": 0.5453991293907166,
      "learning_rate": 4.032e-05,
      "loss": 0.003,
      "step": 29040
    },
    {
      "epoch": 1.5493333333333332,
      "grad_norm": 0.08617711067199707,
      "learning_rate": 4.0316666666666666e-05,
      "loss": 0.0034,
      "step": 29050
    },
    {
      "epoch": 1.5498666666666665,
      "grad_norm": 0.05751952528953552,
      "learning_rate": 4.031333333333334e-05,
      "loss": 0.0031,
      "step": 29060
    },
    {
      "epoch": 1.5504,
      "grad_norm": 0.11461277306079865,
      "learning_rate": 4.0310000000000005e-05,
      "loss": 0.003,
      "step": 29070
    },
    {
      "epoch": 1.5509333333333335,
      "grad_norm": 0.4307540953159332,
      "learning_rate": 4.030666666666667e-05,
      "loss": 0.004,
      "step": 29080
    },
    {
      "epoch": 1.5514666666666668,
      "grad_norm": 0.20114736258983612,
      "learning_rate": 4.030333333333334e-05,
      "loss": 0.004,
      "step": 29090
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.20221459865570068,
      "learning_rate": 4.0300000000000004e-05,
      "loss": 0.0033,
      "step": 29100
    },
    {
      "epoch": 1.5525333333333333,
      "grad_norm": 0.031625352799892426,
      "learning_rate": 4.029666666666666e-05,
      "loss": 0.0028,
      "step": 29110
    },
    {
      "epoch": 1.5530666666666666,
      "grad_norm": 0.08587688952684402,
      "learning_rate": 4.0293333333333336e-05,
      "loss": 0.0024,
      "step": 29120
    },
    {
      "epoch": 1.5535999999999999,
      "grad_norm": 0.17264579236507416,
      "learning_rate": 4.029e-05,
      "loss": 0.0027,
      "step": 29130
    },
    {
      "epoch": 1.5541333333333334,
      "grad_norm": 0.25862765312194824,
      "learning_rate": 4.028666666666667e-05,
      "loss": 0.0032,
      "step": 29140
    },
    {
      "epoch": 1.5546666666666666,
      "grad_norm": 0.4022176265716553,
      "learning_rate": 4.0283333333333334e-05,
      "loss": 0.0037,
      "step": 29150
    },
    {
      "epoch": 1.5552000000000001,
      "grad_norm": 0.28758376836776733,
      "learning_rate": 4.028e-05,
      "loss": 0.0036,
      "step": 29160
    },
    {
      "epoch": 1.5557333333333334,
      "grad_norm": 0.11465369164943695,
      "learning_rate": 4.027666666666667e-05,
      "loss": 0.0042,
      "step": 29170
    },
    {
      "epoch": 1.5562666666666667,
      "grad_norm": 0.5163583159446716,
      "learning_rate": 4.027333333333333e-05,
      "loss": 0.0036,
      "step": 29180
    },
    {
      "epoch": 1.5568,
      "grad_norm": 0.0576668381690979,
      "learning_rate": 4.027e-05,
      "loss": 0.0032,
      "step": 29190
    },
    {
      "epoch": 1.5573333333333332,
      "grad_norm": 0.2292570024728775,
      "learning_rate": 4.026666666666667e-05,
      "loss": 0.0034,
      "step": 29200
    },
    {
      "epoch": 1.5578666666666665,
      "grad_norm": 0.17168615758419037,
      "learning_rate": 4.026333333333334e-05,
      "loss": 0.0035,
      "step": 29210
    },
    {
      "epoch": 1.5584,
      "grad_norm": 0.17144158482551575,
      "learning_rate": 4.0260000000000004e-05,
      "loss": 0.003,
      "step": 29220
    },
    {
      "epoch": 1.5589333333333333,
      "grad_norm": 0.37110447883605957,
      "learning_rate": 4.025666666666667e-05,
      "loss": 0.0071,
      "step": 29230
    },
    {
      "epoch": 1.5594666666666668,
      "grad_norm": 0.08693180978298187,
      "learning_rate": 4.0253333333333336e-05,
      "loss": 0.0096,
      "step": 29240
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.1429171860218048,
      "learning_rate": 4.025e-05,
      "loss": 0.0028,
      "step": 29250
    },
    {
      "epoch": 1.5605333333333333,
      "grad_norm": 0.17138101160526276,
      "learning_rate": 4.024666666666667e-05,
      "loss": 0.0035,
      "step": 29260
    },
    {
      "epoch": 1.5610666666666666,
      "grad_norm": 0.34317922592163086,
      "learning_rate": 4.0243333333333335e-05,
      "loss": 0.0039,
      "step": 29270
    },
    {
      "epoch": 1.5615999999999999,
      "grad_norm": 0.343011736869812,
      "learning_rate": 4.024e-05,
      "loss": 0.0038,
      "step": 29280
    },
    {
      "epoch": 1.5621333333333334,
      "grad_norm": 0.2855651080608368,
      "learning_rate": 4.023666666666667e-05,
      "loss": 0.0031,
      "step": 29290
    },
    {
      "epoch": 1.5626666666666666,
      "grad_norm": 0.030284805223345757,
      "learning_rate": 4.023333333333333e-05,
      "loss": 0.0034,
      "step": 29300
    },
    {
      "epoch": 1.5632000000000001,
      "grad_norm": 0.1721363514661789,
      "learning_rate": 4.023e-05,
      "loss": 0.0037,
      "step": 29310
    },
    {
      "epoch": 1.5637333333333334,
      "grad_norm": 0.11466450244188309,
      "learning_rate": 4.0226666666666666e-05,
      "loss": 0.0027,
      "step": 29320
    },
    {
      "epoch": 1.5642666666666667,
      "grad_norm": 0.23003771901130676,
      "learning_rate": 4.022333333333334e-05,
      "loss": 0.0045,
      "step": 29330
    },
    {
      "epoch": 1.5648,
      "grad_norm": 0.11557687073945999,
      "learning_rate": 4.0220000000000005e-05,
      "loss": 0.0038,
      "step": 29340
    },
    {
      "epoch": 1.5653333333333332,
      "grad_norm": 0.029677484184503555,
      "learning_rate": 4.021666666666667e-05,
      "loss": 0.0039,
      "step": 29350
    },
    {
      "epoch": 1.5658666666666665,
      "grad_norm": 0.1151527464389801,
      "learning_rate": 4.021333333333334e-05,
      "loss": 0.0027,
      "step": 29360
    },
    {
      "epoch": 1.5664,
      "grad_norm": 0.5166611075401306,
      "learning_rate": 4.021e-05,
      "loss": 0.0027,
      "step": 29370
    },
    {
      "epoch": 1.5669333333333333,
      "grad_norm": 0.05716659501194954,
      "learning_rate": 4.020666666666667e-05,
      "loss": 0.0041,
      "step": 29380
    },
    {
      "epoch": 1.5674666666666668,
      "grad_norm": 0.4299008250236511,
      "learning_rate": 4.0203333333333335e-05,
      "loss": 0.0038,
      "step": 29390
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.11446390300989151,
      "learning_rate": 4.02e-05,
      "loss": 0.0021,
      "step": 29400
    },
    {
      "epoch": 1.5685333333333333,
      "grad_norm": 0.02885894849896431,
      "learning_rate": 4.0196666666666674e-05,
      "loss": 0.0034,
      "step": 29410
    },
    {
      "epoch": 1.5690666666666666,
      "grad_norm": 0.028648896142840385,
      "learning_rate": 4.0193333333333334e-05,
      "loss": 0.0028,
      "step": 29420
    },
    {
      "epoch": 1.5695999999999999,
      "grad_norm": 0.08633892983198166,
      "learning_rate": 4.019e-05,
      "loss": 0.0037,
      "step": 29430
    },
    {
      "epoch": 1.5701333333333334,
      "grad_norm": 0.08680789917707443,
      "learning_rate": 4.0186666666666666e-05,
      "loss": 0.0028,
      "step": 29440
    },
    {
      "epoch": 1.5706666666666667,
      "grad_norm": 0.4873557686805725,
      "learning_rate": 4.018333333333333e-05,
      "loss": 0.0031,
      "step": 29450
    },
    {
      "epoch": 1.5712000000000002,
      "grad_norm": 0.31446805596351624,
      "learning_rate": 4.018e-05,
      "loss": 0.0052,
      "step": 29460
    },
    {
      "epoch": 1.5717333333333334,
      "grad_norm": 0.1144060268998146,
      "learning_rate": 4.017666666666667e-05,
      "loss": 0.0045,
      "step": 29470
    },
    {
      "epoch": 1.5722666666666667,
      "grad_norm": 0.513677716255188,
      "learning_rate": 4.017333333333334e-05,
      "loss": 0.0024,
      "step": 29480
    },
    {
      "epoch": 1.5728,
      "grad_norm": 0.20065882802009583,
      "learning_rate": 4.017e-05,
      "loss": 0.0042,
      "step": 29490
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 0.028964893892407417,
      "learning_rate": 4.016666666666667e-05,
      "loss": 0.0023,
      "step": 29500
    },
    {
      "epoch": 1.5738666666666665,
      "grad_norm": 0.42916634678840637,
      "learning_rate": 4.0163333333333336e-05,
      "loss": 0.003,
      "step": 29510
    },
    {
      "epoch": 1.5744,
      "grad_norm": 0.28599581122398376,
      "learning_rate": 4.016e-05,
      "loss": 0.0029,
      "step": 29520
    },
    {
      "epoch": 1.5749333333333333,
      "grad_norm": 0.25745806097984314,
      "learning_rate": 4.015666666666667e-05,
      "loss": 0.0033,
      "step": 29530
    },
    {
      "epoch": 1.5754666666666668,
      "grad_norm": 0.11446162313222885,
      "learning_rate": 4.0153333333333334e-05,
      "loss": 0.0039,
      "step": 29540
    },
    {
      "epoch": 1.576,
      "grad_norm": 0.057306014001369476,
      "learning_rate": 4.015000000000001e-05,
      "loss": 0.0031,
      "step": 29550
    },
    {
      "epoch": 1.5765333333333333,
      "grad_norm": 0.20063434541225433,
      "learning_rate": 4.014666666666667e-05,
      "loss": 0.0017,
      "step": 29560
    },
    {
      "epoch": 1.5770666666666666,
      "grad_norm": 0.029296521097421646,
      "learning_rate": 4.014333333333333e-05,
      "loss": 0.0036,
      "step": 29570
    },
    {
      "epoch": 1.5776,
      "grad_norm": 0.4853040277957916,
      "learning_rate": 4.014e-05,
      "loss": 0.0033,
      "step": 29580
    },
    {
      "epoch": 1.5781333333333334,
      "grad_norm": 0.37140196561813354,
      "learning_rate": 4.0136666666666665e-05,
      "loss": 0.0039,
      "step": 29590
    },
    {
      "epoch": 1.5786666666666667,
      "grad_norm": 0.3143658936023712,
      "learning_rate": 4.013333333333333e-05,
      "loss": 0.0027,
      "step": 29600
    },
    {
      "epoch": 1.5792000000000002,
      "grad_norm": 0.2289900928735733,
      "learning_rate": 4.0130000000000004e-05,
      "loss": 0.0039,
      "step": 29610
    },
    {
      "epoch": 1.5797333333333334,
      "grad_norm": 0.31437528133392334,
      "learning_rate": 4.012666666666667e-05,
      "loss": 0.0039,
      "step": 29620
    },
    {
      "epoch": 1.5802666666666667,
      "grad_norm": 0.2283525913953781,
      "learning_rate": 4.0123333333333336e-05,
      "loss": 0.0037,
      "step": 29630
    },
    {
      "epoch": 1.5808,
      "grad_norm": 0.08593502640724182,
      "learning_rate": 4.012e-05,
      "loss": 0.0031,
      "step": 29640
    },
    {
      "epoch": 1.5813333333333333,
      "grad_norm": 0.17154903709888458,
      "learning_rate": 4.011666666666667e-05,
      "loss": 0.0042,
      "step": 29650
    },
    {
      "epoch": 1.5818666666666665,
      "grad_norm": 0.599773645401001,
      "learning_rate": 4.0113333333333334e-05,
      "loss": 0.0022,
      "step": 29660
    },
    {
      "epoch": 1.5824,
      "grad_norm": 0.37112921476364136,
      "learning_rate": 4.011e-05,
      "loss": 0.0025,
      "step": 29670
    },
    {
      "epoch": 1.5829333333333333,
      "grad_norm": 0.25722232460975647,
      "learning_rate": 4.0106666666666673e-05,
      "loss": 0.0032,
      "step": 29680
    },
    {
      "epoch": 1.5834666666666668,
      "grad_norm": 0.25713032484054565,
      "learning_rate": 4.010333333333334e-05,
      "loss": 0.0021,
      "step": 29690
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.0866333544254303,
      "learning_rate": 4.0100000000000006e-05,
      "loss": 0.0028,
      "step": 29700
    },
    {
      "epoch": 1.5845333333333333,
      "grad_norm": 3.224797487258911,
      "learning_rate": 4.009666666666667e-05,
      "loss": 0.0042,
      "step": 29710
    },
    {
      "epoch": 1.5850666666666666,
      "grad_norm": 0.48550668358802795,
      "learning_rate": 4.009333333333333e-05,
      "loss": 0.0029,
      "step": 29720
    },
    {
      "epoch": 1.5856,
      "grad_norm": 0.1425311267375946,
      "learning_rate": 4.009e-05,
      "loss": 0.003,
      "step": 29730
    },
    {
      "epoch": 1.5861333333333332,
      "grad_norm": 0.0298765916377306,
      "learning_rate": 4.0086666666666663e-05,
      "loss": 0.0035,
      "step": 29740
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 0.0017470067832618952,
      "learning_rate": 4.0083333333333336e-05,
      "loss": 0.0022,
      "step": 29750
    },
    {
      "epoch": 1.5872000000000002,
      "grad_norm": 0.17246942222118378,
      "learning_rate": 4.008e-05,
      "loss": 0.0027,
      "step": 29760
    },
    {
      "epoch": 1.5877333333333334,
      "grad_norm": 0.030609747394919395,
      "learning_rate": 4.007666666666667e-05,
      "loss": 0.0029,
      "step": 29770
    },
    {
      "epoch": 1.5882666666666667,
      "grad_norm": 0.11457664519548416,
      "learning_rate": 4.0073333333333335e-05,
      "loss": 0.0025,
      "step": 29780
    },
    {
      "epoch": 1.5888,
      "grad_norm": 0.4572638273239136,
      "learning_rate": 4.007e-05,
      "loss": 0.0029,
      "step": 29790
    },
    {
      "epoch": 1.5893333333333333,
      "grad_norm": 0.31384316086769104,
      "learning_rate": 4.006666666666667e-05,
      "loss": 0.0033,
      "step": 29800
    },
    {
      "epoch": 1.5898666666666665,
      "grad_norm": 0.02867932803928852,
      "learning_rate": 4.006333333333333e-05,
      "loss": 0.0031,
      "step": 29810
    },
    {
      "epoch": 1.5904,
      "grad_norm": 0.17148654162883759,
      "learning_rate": 4.0060000000000006e-05,
      "loss": 0.003,
      "step": 29820
    },
    {
      "epoch": 1.5909333333333333,
      "grad_norm": 0.2291758507490158,
      "learning_rate": 4.005666666666667e-05,
      "loss": 0.0024,
      "step": 29830
    },
    {
      "epoch": 1.5914666666666668,
      "grad_norm": 0.11466943472623825,
      "learning_rate": 4.005333333333334e-05,
      "loss": 0.0037,
      "step": 29840
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.058233603835105896,
      "learning_rate": 4.0050000000000004e-05,
      "loss": 0.0021,
      "step": 29850
    },
    {
      "epoch": 1.5925333333333334,
      "grad_norm": 0.003665426280349493,
      "learning_rate": 4.004666666666667e-05,
      "loss": 0.0024,
      "step": 29860
    },
    {
      "epoch": 1.5930666666666666,
      "grad_norm": 0.3733164966106415,
      "learning_rate": 4.004333333333333e-05,
      "loss": 0.0046,
      "step": 29870
    },
    {
      "epoch": 1.5936,
      "grad_norm": 0.23010800778865814,
      "learning_rate": 4.004e-05,
      "loss": 0.005,
      "step": 29880
    },
    {
      "epoch": 1.5941333333333332,
      "grad_norm": 0.057509999722242355,
      "learning_rate": 4.003666666666667e-05,
      "loss": 0.0035,
      "step": 29890
    },
    {
      "epoch": 1.5946666666666667,
      "grad_norm": 0.08557312935590744,
      "learning_rate": 4.0033333333333335e-05,
      "loss": 0.0031,
      "step": 29900
    },
    {
      "epoch": 1.5952,
      "grad_norm": 0.31385156512260437,
      "learning_rate": 4.003e-05,
      "loss": 0.0029,
      "step": 29910
    },
    {
      "epoch": 1.5957333333333334,
      "grad_norm": 0.25740447640419006,
      "learning_rate": 4.002666666666667e-05,
      "loss": 0.0037,
      "step": 29920
    },
    {
      "epoch": 1.5962666666666667,
      "grad_norm": 0.14299188554286957,
      "learning_rate": 4.0023333333333334e-05,
      "loss": 0.0045,
      "step": 29930
    },
    {
      "epoch": 1.5968,
      "grad_norm": 0.028977220878005028,
      "learning_rate": 4.002e-05,
      "loss": 0.0034,
      "step": 29940
    },
    {
      "epoch": 1.5973333333333333,
      "grad_norm": 0.1426675170660019,
      "learning_rate": 4.0016666666666666e-05,
      "loss": 0.0043,
      "step": 29950
    },
    {
      "epoch": 1.5978666666666665,
      "grad_norm": 0.028594154864549637,
      "learning_rate": 4.001333333333334e-05,
      "loss": 0.0027,
      "step": 29960
    },
    {
      "epoch": 1.5984,
      "grad_norm": 0.14300519227981567,
      "learning_rate": 4.0010000000000005e-05,
      "loss": 0.0051,
      "step": 29970
    },
    {
      "epoch": 1.5989333333333333,
      "grad_norm": 0.02879475988447666,
      "learning_rate": 4.000666666666667e-05,
      "loss": 0.0037,
      "step": 29980
    },
    {
      "epoch": 1.5994666666666668,
      "grad_norm": 0.028651662170886993,
      "learning_rate": 4.000333333333334e-05,
      "loss": 0.0037,
      "step": 29990
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.28574761748313904,
      "learning_rate": 4e-05,
      "loss": 0.0039,
      "step": 30000
    },
    {
      "epoch": 1.6005333333333334,
      "grad_norm": 0.25694945454597473,
      "learning_rate": 3.999666666666667e-05,
      "loss": 0.0025,
      "step": 30010
    },
    {
      "epoch": 1.6010666666666666,
      "grad_norm": 0.39950376749038696,
      "learning_rate": 3.9993333333333336e-05,
      "loss": 0.0035,
      "step": 30020
    },
    {
      "epoch": 1.6016,
      "grad_norm": 0.25698670744895935,
      "learning_rate": 3.999e-05,
      "loss": 0.004,
      "step": 30030
    },
    {
      "epoch": 1.6021333333333332,
      "grad_norm": 0.14270290732383728,
      "learning_rate": 3.998666666666667e-05,
      "loss": 0.0034,
      "step": 30040
    },
    {
      "epoch": 1.6026666666666667,
      "grad_norm": 0.14265847206115723,
      "learning_rate": 3.9983333333333334e-05,
      "loss": 0.0024,
      "step": 30050
    },
    {
      "epoch": 1.6032,
      "grad_norm": 0.057106517255306244,
      "learning_rate": 3.998e-05,
      "loss": 0.0034,
      "step": 30060
    },
    {
      "epoch": 1.6037333333333335,
      "grad_norm": 0.1997838169336319,
      "learning_rate": 3.9976666666666666e-05,
      "loss": 0.0034,
      "step": 30070
    },
    {
      "epoch": 1.6042666666666667,
      "grad_norm": 0.3711325526237488,
      "learning_rate": 3.997333333333333e-05,
      "loss": 0.0025,
      "step": 30080
    },
    {
      "epoch": 1.6048,
      "grad_norm": 0.20007352530956268,
      "learning_rate": 3.9970000000000005e-05,
      "loss": 0.0027,
      "step": 30090
    },
    {
      "epoch": 1.6053333333333333,
      "grad_norm": 0.002470726380124688,
      "learning_rate": 3.996666666666667e-05,
      "loss": 0.0031,
      "step": 30100
    },
    {
      "epoch": 1.6058666666666666,
      "grad_norm": 0.05754850059747696,
      "learning_rate": 3.996333333333334e-05,
      "loss": 0.0027,
      "step": 30110
    },
    {
      "epoch": 1.6064,
      "grad_norm": 0.2854614853858948,
      "learning_rate": 3.9960000000000004e-05,
      "loss": 0.0036,
      "step": 30120
    },
    {
      "epoch": 1.6069333333333333,
      "grad_norm": 0.31385594606399536,
      "learning_rate": 3.995666666666667e-05,
      "loss": 0.003,
      "step": 30130
    },
    {
      "epoch": 1.6074666666666668,
      "grad_norm": 0.11421367526054382,
      "learning_rate": 3.9953333333333336e-05,
      "loss": 0.005,
      "step": 30140
    },
    {
      "epoch": 1.608,
      "grad_norm": 0.22909806668758392,
      "learning_rate": 3.995e-05,
      "loss": 0.0029,
      "step": 30150
    },
    {
      "epoch": 1.6085333333333334,
      "grad_norm": 0.2573195993900299,
      "learning_rate": 3.994666666666667e-05,
      "loss": 0.0052,
      "step": 30160
    },
    {
      "epoch": 1.6090666666666666,
      "grad_norm": 0.1144837737083435,
      "learning_rate": 3.9943333333333334e-05,
      "loss": 0.0026,
      "step": 30170
    },
    {
      "epoch": 1.6096,
      "grad_norm": 0.20046350359916687,
      "learning_rate": 3.994e-05,
      "loss": 0.0035,
      "step": 30180
    },
    {
      "epoch": 1.6101333333333332,
      "grad_norm": 0.20099587738513947,
      "learning_rate": 3.9936666666666667e-05,
      "loss": 0.0033,
      "step": 30190
    },
    {
      "epoch": 1.6106666666666667,
      "grad_norm": 0.3166917562484741,
      "learning_rate": 3.993333333333333e-05,
      "loss": 0.0054,
      "step": 30200
    },
    {
      "epoch": 1.6112,
      "grad_norm": 0.20203612744808197,
      "learning_rate": 3.993e-05,
      "loss": 0.0037,
      "step": 30210
    },
    {
      "epoch": 1.6117333333333335,
      "grad_norm": 0.1167963296175003,
      "learning_rate": 3.9926666666666665e-05,
      "loss": 0.0034,
      "step": 30220
    },
    {
      "epoch": 1.6122666666666667,
      "grad_norm": 0.5782390236854553,
      "learning_rate": 3.992333333333334e-05,
      "loss": 0.005,
      "step": 30230
    },
    {
      "epoch": 1.6128,
      "grad_norm": 0.48783648014068604,
      "learning_rate": 3.9920000000000004e-05,
      "loss": 0.0045,
      "step": 30240
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.11520551145076752,
      "learning_rate": 3.991666666666667e-05,
      "loss": 0.0044,
      "step": 30250
    },
    {
      "epoch": 1.6138666666666666,
      "grad_norm": 0.11584056168794632,
      "learning_rate": 3.9913333333333336e-05,
      "loss": 0.0031,
      "step": 30260
    },
    {
      "epoch": 1.6143999999999998,
      "grad_norm": 0.43106362223625183,
      "learning_rate": 3.991e-05,
      "loss": 0.0035,
      "step": 30270
    },
    {
      "epoch": 1.6149333333333333,
      "grad_norm": 0.22881746292114258,
      "learning_rate": 3.990666666666667e-05,
      "loss": 0.0043,
      "step": 30280
    },
    {
      "epoch": 1.6154666666666668,
      "grad_norm": 0.11465447396039963,
      "learning_rate": 3.9903333333333335e-05,
      "loss": 0.004,
      "step": 30290
    },
    {
      "epoch": 1.616,
      "grad_norm": 0.02876070886850357,
      "learning_rate": 3.99e-05,
      "loss": 0.0035,
      "step": 30300
    },
    {
      "epoch": 1.6165333333333334,
      "grad_norm": 0.17178882658481598,
      "learning_rate": 3.9896666666666674e-05,
      "loss": 0.0036,
      "step": 30310
    },
    {
      "epoch": 1.6170666666666667,
      "grad_norm": 0.3456883728504181,
      "learning_rate": 3.989333333333333e-05,
      "loss": 0.0039,
      "step": 30320
    },
    {
      "epoch": 1.6176,
      "grad_norm": 0.3160761296749115,
      "learning_rate": 3.989e-05,
      "loss": 0.0033,
      "step": 30330
    },
    {
      "epoch": 1.6181333333333332,
      "grad_norm": 0.4302271902561188,
      "learning_rate": 3.9886666666666665e-05,
      "loss": 0.0039,
      "step": 30340
    },
    {
      "epoch": 1.6186666666666667,
      "grad_norm": 0.3162255585193634,
      "learning_rate": 3.988333333333333e-05,
      "loss": 0.0038,
      "step": 30350
    },
    {
      "epoch": 1.6192,
      "grad_norm": 0.02916993387043476,
      "learning_rate": 3.988e-05,
      "loss": 0.0036,
      "step": 30360
    },
    {
      "epoch": 1.6197333333333335,
      "grad_norm": 0.02871175855398178,
      "learning_rate": 3.987666666666667e-05,
      "loss": 0.0045,
      "step": 30370
    },
    {
      "epoch": 1.6202666666666667,
      "grad_norm": 0.3154142498970032,
      "learning_rate": 3.987333333333334e-05,
      "loss": 0.0052,
      "step": 30380
    },
    {
      "epoch": 1.6208,
      "grad_norm": 0.20069783926010132,
      "learning_rate": 3.987e-05,
      "loss": 0.0035,
      "step": 30390
    },
    {
      "epoch": 1.6213333333333333,
      "grad_norm": 0.11405577510595322,
      "learning_rate": 3.986666666666667e-05,
      "loss": 0.0044,
      "step": 30400
    },
    {
      "epoch": 1.6218666666666666,
      "grad_norm": 0.5421274900436401,
      "learning_rate": 3.9863333333333335e-05,
      "loss": 0.0042,
      "step": 30410
    },
    {
      "epoch": 1.6223999999999998,
      "grad_norm": 0.11405806243419647,
      "learning_rate": 3.986e-05,
      "loss": 0.0051,
      "step": 30420
    },
    {
      "epoch": 1.6229333333333333,
      "grad_norm": 0.25690987706184387,
      "learning_rate": 3.985666666666667e-05,
      "loss": 0.004,
      "step": 30430
    },
    {
      "epoch": 1.6234666666666666,
      "grad_norm": 0.2854617238044739,
      "learning_rate": 3.985333333333334e-05,
      "loss": 0.0044,
      "step": 30440
    },
    {
      "epoch": 1.624,
      "grad_norm": 0.08584905415773392,
      "learning_rate": 3.9850000000000006e-05,
      "loss": 0.0035,
      "step": 30450
    },
    {
      "epoch": 1.6245333333333334,
      "grad_norm": 0.19984571635723114,
      "learning_rate": 3.984666666666667e-05,
      "loss": 0.0025,
      "step": 30460
    },
    {
      "epoch": 1.6250666666666667,
      "grad_norm": 0.11418446898460388,
      "learning_rate": 3.984333333333333e-05,
      "loss": 0.003,
      "step": 30470
    },
    {
      "epoch": 1.6256,
      "grad_norm": 0.2855784296989441,
      "learning_rate": 3.984e-05,
      "loss": 0.0027,
      "step": 30480
    },
    {
      "epoch": 1.6261333333333332,
      "grad_norm": 0.029199913144111633,
      "learning_rate": 3.9836666666666664e-05,
      "loss": 0.0049,
      "step": 30490
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 0.08566056191921234,
      "learning_rate": 3.983333333333333e-05,
      "loss": 0.0039,
      "step": 30500
    },
    {
      "epoch": 1.6272,
      "grad_norm": 0.14264945685863495,
      "learning_rate": 3.983e-05,
      "loss": 0.0041,
      "step": 30510
    },
    {
      "epoch": 1.6277333333333335,
      "grad_norm": 0.17128103971481323,
      "learning_rate": 3.982666666666667e-05,
      "loss": 0.0037,
      "step": 30520
    },
    {
      "epoch": 1.6282666666666668,
      "grad_norm": 2.7548327445983887,
      "learning_rate": 3.9823333333333335e-05,
      "loss": 0.0042,
      "step": 30530
    },
    {
      "epoch": 1.6288,
      "grad_norm": 0.11407789587974548,
      "learning_rate": 3.982e-05,
      "loss": 0.0027,
      "step": 30540
    },
    {
      "epoch": 1.6293333333333333,
      "grad_norm": 0.31427040696144104,
      "learning_rate": 3.981666666666667e-05,
      "loss": 0.004,
      "step": 30550
    },
    {
      "epoch": 1.6298666666666666,
      "grad_norm": 0.003912586718797684,
      "learning_rate": 3.9813333333333334e-05,
      "loss": 0.0043,
      "step": 30560
    },
    {
      "epoch": 1.6303999999999998,
      "grad_norm": 0.3137837052345276,
      "learning_rate": 3.981e-05,
      "loss": 0.005,
      "step": 30570
    },
    {
      "epoch": 1.6309333333333333,
      "grad_norm": 0.3136231005191803,
      "learning_rate": 3.980666666666667e-05,
      "loss": 0.0048,
      "step": 30580
    },
    {
      "epoch": 1.6314666666666666,
      "grad_norm": 3.372338056564331,
      "learning_rate": 3.980333333333334e-05,
      "loss": 0.0051,
      "step": 30590
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.14289140701293945,
      "learning_rate": 3.9800000000000005e-05,
      "loss": 0.0028,
      "step": 30600
    },
    {
      "epoch": 1.6325333333333334,
      "grad_norm": 0.1432444006204605,
      "learning_rate": 3.979666666666667e-05,
      "loss": 0.0041,
      "step": 30610
    },
    {
      "epoch": 1.6330666666666667,
      "grad_norm": 0.00611454900354147,
      "learning_rate": 3.979333333333333e-05,
      "loss": 0.0035,
      "step": 30620
    },
    {
      "epoch": 1.6336,
      "grad_norm": 0.1428132951259613,
      "learning_rate": 3.979e-05,
      "loss": 0.0029,
      "step": 30630
    },
    {
      "epoch": 1.6341333333333332,
      "grad_norm": 0.34363770484924316,
      "learning_rate": 3.978666666666667e-05,
      "loss": 0.0042,
      "step": 30640
    },
    {
      "epoch": 1.6346666666666667,
      "grad_norm": 0.1428476721048355,
      "learning_rate": 3.9783333333333336e-05,
      "loss": 0.0024,
      "step": 30650
    },
    {
      "epoch": 1.6352,
      "grad_norm": 0.14303846657276154,
      "learning_rate": 3.978e-05,
      "loss": 0.0033,
      "step": 30660
    },
    {
      "epoch": 1.6357333333333335,
      "grad_norm": 0.05788835883140564,
      "learning_rate": 3.977666666666667e-05,
      "loss": 0.0039,
      "step": 30670
    },
    {
      "epoch": 1.6362666666666668,
      "grad_norm": 0.14288127422332764,
      "learning_rate": 3.9773333333333334e-05,
      "loss": 0.0027,
      "step": 30680
    },
    {
      "epoch": 1.6368,
      "grad_norm": 0.3715777099132538,
      "learning_rate": 3.977e-05,
      "loss": 0.0047,
      "step": 30690
    },
    {
      "epoch": 1.6373333333333333,
      "grad_norm": 0.20016765594482422,
      "learning_rate": 3.9766666666666667e-05,
      "loss": 0.0031,
      "step": 30700
    },
    {
      "epoch": 1.6378666666666666,
      "grad_norm": 0.11420592665672302,
      "learning_rate": 3.976333333333333e-05,
      "loss": 0.0036,
      "step": 30710
    },
    {
      "epoch": 1.6383999999999999,
      "grad_norm": 0.42850133776664734,
      "learning_rate": 3.9760000000000006e-05,
      "loss": 0.0051,
      "step": 30720
    },
    {
      "epoch": 1.6389333333333334,
      "grad_norm": 0.2286154180765152,
      "learning_rate": 3.975666666666667e-05,
      "loss": 0.004,
      "step": 30730
    },
    {
      "epoch": 1.6394666666666666,
      "grad_norm": 0.028834016993641853,
      "learning_rate": 3.975333333333334e-05,
      "loss": 0.004,
      "step": 30740
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.029004277661442757,
      "learning_rate": 3.9750000000000004e-05,
      "loss": 0.004,
      "step": 30750
    },
    {
      "epoch": 1.6405333333333334,
      "grad_norm": 0.48892831802368164,
      "learning_rate": 3.974666666666667e-05,
      "loss": 0.0043,
      "step": 30760
    },
    {
      "epoch": 1.6410666666666667,
      "grad_norm": 0.4000149369239807,
      "learning_rate": 3.9743333333333336e-05,
      "loss": 0.0035,
      "step": 30770
    },
    {
      "epoch": 1.6416,
      "grad_norm": 0.05701727047562599,
      "learning_rate": 3.974e-05,
      "loss": 0.0034,
      "step": 30780
    },
    {
      "epoch": 1.6421333333333332,
      "grad_norm": 0.08572854846715927,
      "learning_rate": 3.973666666666667e-05,
      "loss": 0.0036,
      "step": 30790
    },
    {
      "epoch": 1.6426666666666667,
      "grad_norm": 0.17107003927230835,
      "learning_rate": 3.9733333333333335e-05,
      "loss": 0.0024,
      "step": 30800
    },
    {
      "epoch": 1.6432,
      "grad_norm": 0.028595808893442154,
      "learning_rate": 3.973e-05,
      "loss": 0.0051,
      "step": 30810
    },
    {
      "epoch": 1.6437333333333335,
      "grad_norm": 0.31460005044937134,
      "learning_rate": 3.972666666666667e-05,
      "loss": 0.0045,
      "step": 30820
    },
    {
      "epoch": 1.6442666666666668,
      "grad_norm": 0.028735142201185226,
      "learning_rate": 3.972333333333333e-05,
      "loss": 0.004,
      "step": 30830
    },
    {
      "epoch": 1.6448,
      "grad_norm": 0.14508242905139923,
      "learning_rate": 3.972e-05,
      "loss": 0.0036,
      "step": 30840
    },
    {
      "epoch": 1.6453333333333333,
      "grad_norm": 0.17262762784957886,
      "learning_rate": 3.9716666666666665e-05,
      "loss": 0.0016,
      "step": 30850
    },
    {
      "epoch": 1.6458666666666666,
      "grad_norm": 0.11538190394639969,
      "learning_rate": 3.971333333333334e-05,
      "loss": 0.0038,
      "step": 30860
    },
    {
      "epoch": 1.6463999999999999,
      "grad_norm": 0.08580507338047028,
      "learning_rate": 3.9710000000000004e-05,
      "loss": 0.004,
      "step": 30870
    },
    {
      "epoch": 1.6469333333333334,
      "grad_norm": 0.25666216015815735,
      "learning_rate": 3.970666666666667e-05,
      "loss": 0.0036,
      "step": 30880
    },
    {
      "epoch": 1.6474666666666666,
      "grad_norm": 0.057110220193862915,
      "learning_rate": 3.970333333333334e-05,
      "loss": 0.0023,
      "step": 30890
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.11413302272558212,
      "learning_rate": 3.97e-05,
      "loss": 0.0028,
      "step": 30900
    },
    {
      "epoch": 1.6485333333333334,
      "grad_norm": 0.28531304001808167,
      "learning_rate": 3.969666666666667e-05,
      "loss": 0.0035,
      "step": 30910
    },
    {
      "epoch": 1.6490666666666667,
      "grad_norm": 0.4278165400028229,
      "learning_rate": 3.9693333333333335e-05,
      "loss": 0.0035,
      "step": 30920
    },
    {
      "epoch": 1.6496,
      "grad_norm": 0.1426255851984024,
      "learning_rate": 3.969e-05,
      "loss": 0.0036,
      "step": 30930
    },
    {
      "epoch": 1.6501333333333332,
      "grad_norm": 0.3135959804058075,
      "learning_rate": 3.968666666666667e-05,
      "loss": 0.0037,
      "step": 30940
    },
    {
      "epoch": 1.6506666666666665,
      "grad_norm": 0.05709199979901314,
      "learning_rate": 3.9683333333333333e-05,
      "loss": 0.0053,
      "step": 30950
    },
    {
      "epoch": 1.6512,
      "grad_norm": 0.22835072875022888,
      "learning_rate": 3.968e-05,
      "loss": 0.0041,
      "step": 30960
    },
    {
      "epoch": 1.6517333333333335,
      "grad_norm": 0.11416160315275192,
      "learning_rate": 3.9676666666666666e-05,
      "loss": 0.0028,
      "step": 30970
    },
    {
      "epoch": 1.6522666666666668,
      "grad_norm": 0.19951413571834564,
      "learning_rate": 3.967333333333333e-05,
      "loss": 0.0029,
      "step": 30980
    },
    {
      "epoch": 1.6528,
      "grad_norm": 0.1425168216228485,
      "learning_rate": 3.9670000000000005e-05,
      "loss": 0.0036,
      "step": 30990
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 0.0003402403090149164,
      "learning_rate": 3.966666666666667e-05,
      "loss": 0.0025,
      "step": 31000
    },
    {
      "epoch": 1.6538666666666666,
      "grad_norm": 0.25678300857543945,
      "learning_rate": 3.966333333333334e-05,
      "loss": 0.0031,
      "step": 31010
    },
    {
      "epoch": 1.6543999999999999,
      "grad_norm": 0.028566939756274223,
      "learning_rate": 3.966e-05,
      "loss": 0.0028,
      "step": 31020
    },
    {
      "epoch": 1.6549333333333334,
      "grad_norm": 0.08567298203706741,
      "learning_rate": 3.965666666666667e-05,
      "loss": 0.003,
      "step": 31030
    },
    {
      "epoch": 1.6554666666666666,
      "grad_norm": 0.25685346126556396,
      "learning_rate": 3.9653333333333335e-05,
      "loss": 0.0019,
      "step": 31040
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 0.3421259820461273,
      "learning_rate": 3.965e-05,
      "loss": 0.0037,
      "step": 31050
    },
    {
      "epoch": 1.6565333333333334,
      "grad_norm": 0.028698457404971123,
      "learning_rate": 3.964666666666667e-05,
      "loss": 0.0041,
      "step": 31060
    },
    {
      "epoch": 1.6570666666666667,
      "grad_norm": 0.1426205337047577,
      "learning_rate": 3.964333333333334e-05,
      "loss": 0.0024,
      "step": 31070
    },
    {
      "epoch": 1.6576,
      "grad_norm": 0.19966550171375275,
      "learning_rate": 3.964e-05,
      "loss": 0.0033,
      "step": 31080
    },
    {
      "epoch": 1.6581333333333332,
      "grad_norm": 0.3709275722503662,
      "learning_rate": 3.9636666666666666e-05,
      "loss": 0.0036,
      "step": 31090
    },
    {
      "epoch": 1.6586666666666665,
      "grad_norm": 0.05703841149806976,
      "learning_rate": 3.963333333333333e-05,
      "loss": 0.0032,
      "step": 31100
    },
    {
      "epoch": 1.6592,
      "grad_norm": 4.584784507751465,
      "learning_rate": 3.963e-05,
      "loss": 0.0056,
      "step": 31110
    },
    {
      "epoch": 1.6597333333333333,
      "grad_norm": 0.11398302763700485,
      "learning_rate": 3.9626666666666664e-05,
      "loss": 0.0039,
      "step": 31120
    },
    {
      "epoch": 1.6602666666666668,
      "grad_norm": 0.14282114803791046,
      "learning_rate": 3.962333333333334e-05,
      "loss": 0.0049,
      "step": 31130
    },
    {
      "epoch": 1.6608,
      "grad_norm": 0.1995820254087448,
      "learning_rate": 3.9620000000000004e-05,
      "loss": 0.004,
      "step": 31140
    },
    {
      "epoch": 1.6613333333333333,
      "grad_norm": 0.11534536629915237,
      "learning_rate": 3.961666666666667e-05,
      "loss": 0.0029,
      "step": 31150
    },
    {
      "epoch": 1.6618666666666666,
      "grad_norm": 0.2857244908809662,
      "learning_rate": 3.9613333333333336e-05,
      "loss": 0.0035,
      "step": 31160
    },
    {
      "epoch": 1.6623999999999999,
      "grad_norm": 0.0042617060244083405,
      "learning_rate": 3.961e-05,
      "loss": 0.0032,
      "step": 31170
    },
    {
      "epoch": 1.6629333333333334,
      "grad_norm": 0.08571752160787582,
      "learning_rate": 3.960666666666667e-05,
      "loss": 0.0051,
      "step": 31180
    },
    {
      "epoch": 1.6634666666666666,
      "grad_norm": 0.028741028159856796,
      "learning_rate": 3.9603333333333334e-05,
      "loss": 0.003,
      "step": 31190
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.14257284998893738,
      "learning_rate": 3.960000000000001e-05,
      "loss": 0.0024,
      "step": 31200
    },
    {
      "epoch": 1.6645333333333334,
      "grad_norm": 0.11397474259138107,
      "learning_rate": 3.959666666666667e-05,
      "loss": 0.0036,
      "step": 31210
    },
    {
      "epoch": 1.6650666666666667,
      "grad_norm": 0.19963006675243378,
      "learning_rate": 3.959333333333334e-05,
      "loss": 0.0037,
      "step": 31220
    },
    {
      "epoch": 1.6656,
      "grad_norm": 0.1425604373216629,
      "learning_rate": 3.959e-05,
      "loss": 0.0041,
      "step": 31230
    },
    {
      "epoch": 1.6661333333333332,
      "grad_norm": 0.11417747288942337,
      "learning_rate": 3.9586666666666665e-05,
      "loss": 0.0028,
      "step": 31240
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.057022396475076675,
      "learning_rate": 3.958333333333333e-05,
      "loss": 0.0026,
      "step": 31250
    },
    {
      "epoch": 1.6672,
      "grad_norm": 0.34206125140190125,
      "learning_rate": 3.958e-05,
      "loss": 0.0041,
      "step": 31260
    },
    {
      "epoch": 1.6677333333333333,
      "grad_norm": 0.19956521689891815,
      "learning_rate": 3.957666666666667e-05,
      "loss": 0.0027,
      "step": 31270
    },
    {
      "epoch": 1.6682666666666668,
      "grad_norm": 0.3422045409679413,
      "learning_rate": 3.9573333333333336e-05,
      "loss": 0.0028,
      "step": 31280
    },
    {
      "epoch": 1.6688,
      "grad_norm": 0.19997434318065643,
      "learning_rate": 3.957e-05,
      "loss": 0.0034,
      "step": 31290
    },
    {
      "epoch": 1.6693333333333333,
      "grad_norm": 3.832669258117676,
      "learning_rate": 3.956666666666667e-05,
      "loss": 0.0042,
      "step": 31300
    },
    {
      "epoch": 1.6698666666666666,
      "grad_norm": 0.17109617590904236,
      "learning_rate": 3.9563333333333335e-05,
      "loss": 0.0052,
      "step": 31310
    },
    {
      "epoch": 1.6703999999999999,
      "grad_norm": 0.5706326365470886,
      "learning_rate": 3.956e-05,
      "loss": 0.0028,
      "step": 31320
    },
    {
      "epoch": 1.6709333333333334,
      "grad_norm": 0.057225629687309265,
      "learning_rate": 3.955666666666667e-05,
      "loss": 0.0031,
      "step": 31330
    },
    {
      "epoch": 1.6714666666666667,
      "grad_norm": 0.08612328767776489,
      "learning_rate": 3.955333333333334e-05,
      "loss": 0.0033,
      "step": 31340
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 1.626233696937561,
      "learning_rate": 3.9550000000000006e-05,
      "loss": 0.0042,
      "step": 31350
    },
    {
      "epoch": 1.6725333333333334,
      "grad_norm": 0.0039328644052147865,
      "learning_rate": 3.954666666666667e-05,
      "loss": 0.0027,
      "step": 31360
    },
    {
      "epoch": 1.6730666666666667,
      "grad_norm": 0.08683156222105026,
      "learning_rate": 3.954333333333334e-05,
      "loss": 0.004,
      "step": 31370
    },
    {
      "epoch": 1.6736,
      "grad_norm": 0.1717069149017334,
      "learning_rate": 3.954e-05,
      "loss": 0.0034,
      "step": 31380
    },
    {
      "epoch": 1.6741333333333333,
      "grad_norm": 0.005467572715133429,
      "learning_rate": 3.9536666666666664e-05,
      "loss": 0.0043,
      "step": 31390
    },
    {
      "epoch": 1.6746666666666665,
      "grad_norm": 3.8420262336730957,
      "learning_rate": 3.9533333333333337e-05,
      "loss": 0.0025,
      "step": 31400
    },
    {
      "epoch": 1.6752,
      "grad_norm": 0.2858494520187378,
      "learning_rate": 3.953e-05,
      "loss": 0.0034,
      "step": 31410
    },
    {
      "epoch": 1.6757333333333333,
      "grad_norm": 0.3150034546852112,
      "learning_rate": 3.952666666666667e-05,
      "loss": 0.0042,
      "step": 31420
    },
    {
      "epoch": 1.6762666666666668,
      "grad_norm": 0.17152021825313568,
      "learning_rate": 3.9523333333333335e-05,
      "loss": 0.0047,
      "step": 31430
    },
    {
      "epoch": 1.6768,
      "grad_norm": 0.39972254633903503,
      "learning_rate": 3.952e-05,
      "loss": 0.0042,
      "step": 31440
    },
    {
      "epoch": 1.6773333333333333,
      "grad_norm": 3.6175713539123535,
      "learning_rate": 3.951666666666667e-05,
      "loss": 0.0042,
      "step": 31450
    },
    {
      "epoch": 1.6778666666666666,
      "grad_norm": 0.05781412497162819,
      "learning_rate": 3.951333333333333e-05,
      "loss": 0.004,
      "step": 31460
    },
    {
      "epoch": 1.6784,
      "grad_norm": 0.08624084293842316,
      "learning_rate": 3.951e-05,
      "loss": 0.003,
      "step": 31470
    },
    {
      "epoch": 1.6789333333333334,
      "grad_norm": 0.20105184614658356,
      "learning_rate": 3.950666666666667e-05,
      "loss": 0.0033,
      "step": 31480
    },
    {
      "epoch": 1.6794666666666667,
      "grad_norm": 0.3429180383682251,
      "learning_rate": 3.950333333333334e-05,
      "loss": 0.0042,
      "step": 31490
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.057008046656847,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 0.0042,
      "step": 31500
    },
    {
      "epoch": 1.6805333333333334,
      "grad_norm": 0.372226744890213,
      "learning_rate": 3.949666666666667e-05,
      "loss": 0.0028,
      "step": 31510
    },
    {
      "epoch": 1.6810666666666667,
      "grad_norm": 0.11461049318313599,
      "learning_rate": 3.949333333333334e-05,
      "loss": 0.0025,
      "step": 31520
    },
    {
      "epoch": 1.6816,
      "grad_norm": 0.02868540771305561,
      "learning_rate": 3.9489999999999996e-05,
      "loss": 0.0034,
      "step": 31530
    },
    {
      "epoch": 1.6821333333333333,
      "grad_norm": 0.08727789670228958,
      "learning_rate": 3.948666666666667e-05,
      "loss": 0.0028,
      "step": 31540
    },
    {
      "epoch": 1.6826666666666665,
      "grad_norm": 0.40157872438430786,
      "learning_rate": 3.9483333333333335e-05,
      "loss": 0.0036,
      "step": 31550
    },
    {
      "epoch": 1.6832,
      "grad_norm": 0.11542002111673355,
      "learning_rate": 3.948e-05,
      "loss": 0.0023,
      "step": 31560
    },
    {
      "epoch": 1.6837333333333333,
      "grad_norm": 0.1720847487449646,
      "learning_rate": 3.947666666666667e-05,
      "loss": 0.0039,
      "step": 31570
    },
    {
      "epoch": 1.6842666666666668,
      "grad_norm": 0.1438368856906891,
      "learning_rate": 3.9473333333333334e-05,
      "loss": 0.0035,
      "step": 31580
    },
    {
      "epoch": 1.6848,
      "grad_norm": 0.17172767221927643,
      "learning_rate": 3.947e-05,
      "loss": 0.0033,
      "step": 31590
    },
    {
      "epoch": 1.6853333333333333,
      "grad_norm": 0.1150723323225975,
      "learning_rate": 3.9466666666666666e-05,
      "loss": 0.0041,
      "step": 31600
    },
    {
      "epoch": 1.6858666666666666,
      "grad_norm": 0.11536652594804764,
      "learning_rate": 3.946333333333333e-05,
      "loss": 0.0032,
      "step": 31610
    },
    {
      "epoch": 1.6864,
      "grad_norm": 0.4024096727371216,
      "learning_rate": 3.9460000000000005e-05,
      "loss": 0.003,
      "step": 31620
    },
    {
      "epoch": 1.6869333333333332,
      "grad_norm": 0.008787925355136395,
      "learning_rate": 3.945666666666667e-05,
      "loss": 0.0032,
      "step": 31630
    },
    {
      "epoch": 1.6874666666666667,
      "grad_norm": 0.0864708349108696,
      "learning_rate": 3.945333333333334e-05,
      "loss": 0.0027,
      "step": 31640
    },
    {
      "epoch": 1.688,
      "grad_norm": 0.08702649176120758,
      "learning_rate": 3.9450000000000003e-05,
      "loss": 0.0045,
      "step": 31650
    },
    {
      "epoch": 1.6885333333333334,
      "grad_norm": 0.3466857075691223,
      "learning_rate": 3.944666666666667e-05,
      "loss": 0.0034,
      "step": 31660
    },
    {
      "epoch": 1.6890666666666667,
      "grad_norm": 0.287629097700119,
      "learning_rate": 3.9443333333333336e-05,
      "loss": 0.0047,
      "step": 31670
    },
    {
      "epoch": 1.6896,
      "grad_norm": 0.057386793196201324,
      "learning_rate": 3.944e-05,
      "loss": 0.0043,
      "step": 31680
    },
    {
      "epoch": 1.6901333333333333,
      "grad_norm": 0.08630911260843277,
      "learning_rate": 3.943666666666667e-05,
      "loss": 0.0041,
      "step": 31690
    },
    {
      "epoch": 1.6906666666666665,
      "grad_norm": 0.029451804235577583,
      "learning_rate": 3.9433333333333334e-05,
      "loss": 0.003,
      "step": 31700
    },
    {
      "epoch": 1.6912,
      "grad_norm": 0.11454824358224869,
      "learning_rate": 3.943e-05,
      "loss": 0.0026,
      "step": 31710
    },
    {
      "epoch": 1.6917333333333333,
      "grad_norm": 0.11614397168159485,
      "learning_rate": 3.9426666666666666e-05,
      "loss": 0.0038,
      "step": 31720
    },
    {
      "epoch": 1.6922666666666668,
      "grad_norm": 0.37566933035850525,
      "learning_rate": 3.942333333333333e-05,
      "loss": 0.0039,
      "step": 31730
    },
    {
      "epoch": 1.6928,
      "grad_norm": 0.6929575204849243,
      "learning_rate": 3.942e-05,
      "loss": 0.0041,
      "step": 31740
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.14360085129737854,
      "learning_rate": 3.941666666666667e-05,
      "loss": 0.0029,
      "step": 31750
    },
    {
      "epoch": 1.6938666666666666,
      "grad_norm": 0.11421799659729004,
      "learning_rate": 3.941333333333334e-05,
      "loss": 0.0032,
      "step": 31760
    },
    {
      "epoch": 1.6944,
      "grad_norm": 0.11497392505407333,
      "learning_rate": 3.9410000000000004e-05,
      "loss": 0.004,
      "step": 31770
    },
    {
      "epoch": 1.6949333333333332,
      "grad_norm": 0.40169158577919006,
      "learning_rate": 3.940666666666667e-05,
      "loss": 0.0036,
      "step": 31780
    },
    {
      "epoch": 1.6954666666666667,
      "grad_norm": 0.488690048456192,
      "learning_rate": 3.9403333333333336e-05,
      "loss": 0.0038,
      "step": 31790
    },
    {
      "epoch": 1.696,
      "grad_norm": 0.11585839092731476,
      "learning_rate": 3.94e-05,
      "loss": 0.0035,
      "step": 31800
    },
    {
      "epoch": 1.6965333333333334,
      "grad_norm": 0.2580012083053589,
      "learning_rate": 3.939666666666667e-05,
      "loss": 0.0038,
      "step": 31810
    },
    {
      "epoch": 1.6970666666666667,
      "grad_norm": 0.31530335545539856,
      "learning_rate": 3.9393333333333335e-05,
      "loss": 0.0029,
      "step": 31820
    },
    {
      "epoch": 1.6976,
      "grad_norm": 0.28638672828674316,
      "learning_rate": 3.939e-05,
      "loss": 0.0038,
      "step": 31830
    },
    {
      "epoch": 1.6981333333333333,
      "grad_norm": 0.1430351287126541,
      "learning_rate": 3.938666666666667e-05,
      "loss": 0.0032,
      "step": 31840
    },
    {
      "epoch": 1.6986666666666665,
      "grad_norm": 0.057632025331258774,
      "learning_rate": 3.938333333333333e-05,
      "loss": 0.0057,
      "step": 31850
    },
    {
      "epoch": 1.6992,
      "grad_norm": 0.08575529605150223,
      "learning_rate": 3.938e-05,
      "loss": 0.0081,
      "step": 31860
    },
    {
      "epoch": 1.6997333333333333,
      "grad_norm": 0.08592015504837036,
      "learning_rate": 3.9376666666666665e-05,
      "loss": 0.0047,
      "step": 31870
    },
    {
      "epoch": 1.7002666666666668,
      "grad_norm": 0.029627030715346336,
      "learning_rate": 3.937333333333333e-05,
      "loss": 0.0046,
      "step": 31880
    },
    {
      "epoch": 1.7008,
      "grad_norm": 0.11634299904108047,
      "learning_rate": 3.9370000000000004e-05,
      "loss": 0.0032,
      "step": 31890
    },
    {
      "epoch": 1.7013333333333334,
      "grad_norm": 0.1718192994594574,
      "learning_rate": 3.936666666666667e-05,
      "loss": 0.0053,
      "step": 31900
    },
    {
      "epoch": 1.7018666666666666,
      "grad_norm": 0.14411082863807678,
      "learning_rate": 3.9363333333333336e-05,
      "loss": 0.0033,
      "step": 31910
    },
    {
      "epoch": 1.7024,
      "grad_norm": 0.029224712401628494,
      "learning_rate": 3.936e-05,
      "loss": 0.0033,
      "step": 31920
    },
    {
      "epoch": 1.7029333333333332,
      "grad_norm": 0.1159069836139679,
      "learning_rate": 3.935666666666667e-05,
      "loss": 0.0021,
      "step": 31930
    },
    {
      "epoch": 1.7034666666666667,
      "grad_norm": 0.11443667858839035,
      "learning_rate": 3.9353333333333335e-05,
      "loss": 0.0032,
      "step": 31940
    },
    {
      "epoch": 1.704,
      "grad_norm": 0.05781835317611694,
      "learning_rate": 3.935e-05,
      "loss": 0.0041,
      "step": 31950
    },
    {
      "epoch": 1.7045333333333335,
      "grad_norm": 0.08768712729215622,
      "learning_rate": 3.9346666666666674e-05,
      "loss": 0.0035,
      "step": 31960
    },
    {
      "epoch": 1.7050666666666667,
      "grad_norm": 0.3444375693798065,
      "learning_rate": 3.934333333333334e-05,
      "loss": 0.0023,
      "step": 31970
    },
    {
      "epoch": 1.7056,
      "grad_norm": 0.028856612741947174,
      "learning_rate": 3.9340000000000006e-05,
      "loss": 0.0037,
      "step": 31980
    },
    {
      "epoch": 1.7061333333333333,
      "grad_norm": 0.23067818582057953,
      "learning_rate": 3.9336666666666666e-05,
      "loss": 0.004,
      "step": 31990
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.028555843979120255,
      "learning_rate": 3.933333333333333e-05,
      "loss": 0.0019,
      "step": 32000
    },
    {
      "epoch": 1.7072,
      "grad_norm": 0.28542718291282654,
      "learning_rate": 3.933e-05,
      "loss": 0.0045,
      "step": 32010
    },
    {
      "epoch": 1.7077333333333333,
      "grad_norm": 0.34342965483665466,
      "learning_rate": 3.9326666666666664e-05,
      "loss": 0.0025,
      "step": 32020
    },
    {
      "epoch": 1.7082666666666668,
      "grad_norm": 0.34242692589759827,
      "learning_rate": 3.932333333333334e-05,
      "loss": 0.0038,
      "step": 32030
    },
    {
      "epoch": 1.7088,
      "grad_norm": 0.14272452890872955,
      "learning_rate": 3.932e-05,
      "loss": 0.0028,
      "step": 32040
    },
    {
      "epoch": 1.7093333333333334,
      "grad_norm": 0.2287611961364746,
      "learning_rate": 3.931666666666667e-05,
      "loss": 0.0039,
      "step": 32050
    },
    {
      "epoch": 1.7098666666666666,
      "grad_norm": 0.11421576887369156,
      "learning_rate": 3.9313333333333335e-05,
      "loss": 0.0029,
      "step": 32060
    },
    {
      "epoch": 1.7104,
      "grad_norm": 0.25644630193710327,
      "learning_rate": 3.931e-05,
      "loss": 0.002,
      "step": 32070
    },
    {
      "epoch": 1.7109333333333332,
      "grad_norm": 0.3135083019733429,
      "learning_rate": 3.930666666666667e-05,
      "loss": 0.0043,
      "step": 32080
    },
    {
      "epoch": 1.7114666666666667,
      "grad_norm": 0.11403582990169525,
      "learning_rate": 3.9303333333333334e-05,
      "loss": 0.0031,
      "step": 32090
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.2566964328289032,
      "learning_rate": 3.9300000000000007e-05,
      "loss": 0.0037,
      "step": 32100
    },
    {
      "epoch": 1.7125333333333335,
      "grad_norm": 0.14249064028263092,
      "learning_rate": 3.929666666666667e-05,
      "loss": 0.0029,
      "step": 32110
    },
    {
      "epoch": 1.7130666666666667,
      "grad_norm": 0.08548137545585632,
      "learning_rate": 3.929333333333334e-05,
      "loss": 0.0027,
      "step": 32120
    },
    {
      "epoch": 1.7136,
      "grad_norm": 0.028499750420451164,
      "learning_rate": 3.9290000000000005e-05,
      "loss": 0.0026,
      "step": 32130
    },
    {
      "epoch": 1.7141333333333333,
      "grad_norm": 0.31351563334465027,
      "learning_rate": 3.9286666666666664e-05,
      "loss": 0.0038,
      "step": 32140
    },
    {
      "epoch": 1.7146666666666666,
      "grad_norm": 0.3418639004230499,
      "learning_rate": 3.928333333333333e-05,
      "loss": 0.0043,
      "step": 32150
    },
    {
      "epoch": 1.7151999999999998,
      "grad_norm": 0.20050571858882904,
      "learning_rate": 3.9280000000000003e-05,
      "loss": 0.0036,
      "step": 32160
    },
    {
      "epoch": 1.7157333333333333,
      "grad_norm": 0.17113207280635834,
      "learning_rate": 3.927666666666667e-05,
      "loss": 0.0044,
      "step": 32170
    },
    {
      "epoch": 1.7162666666666668,
      "grad_norm": 0.08557286858558655,
      "learning_rate": 3.9273333333333336e-05,
      "loss": 0.0038,
      "step": 32180
    },
    {
      "epoch": 1.7168,
      "grad_norm": 0.37034693360328674,
      "learning_rate": 3.927e-05,
      "loss": 0.0036,
      "step": 32190
    },
    {
      "epoch": 1.7173333333333334,
      "grad_norm": 0.2852059006690979,
      "learning_rate": 3.926666666666667e-05,
      "loss": 0.0025,
      "step": 32200
    },
    {
      "epoch": 1.7178666666666667,
      "grad_norm": 0.3708510100841522,
      "learning_rate": 3.9263333333333334e-05,
      "loss": 0.0025,
      "step": 32210
    },
    {
      "epoch": 1.7184,
      "grad_norm": 0.14292673766613007,
      "learning_rate": 3.926e-05,
      "loss": 0.0027,
      "step": 32220
    },
    {
      "epoch": 1.7189333333333332,
      "grad_norm": 0.17118504643440247,
      "learning_rate": 3.9256666666666666e-05,
      "loss": 0.0033,
      "step": 32230
    },
    {
      "epoch": 1.7194666666666667,
      "grad_norm": 0.22795036435127258,
      "learning_rate": 3.925333333333334e-05,
      "loss": 0.0027,
      "step": 32240
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.22799940407276154,
      "learning_rate": 3.9250000000000005e-05,
      "loss": 0.0048,
      "step": 32250
    },
    {
      "epoch": 1.7205333333333335,
      "grad_norm": 0.4843161404132843,
      "learning_rate": 3.924666666666667e-05,
      "loss": 0.0027,
      "step": 32260
    },
    {
      "epoch": 1.7210666666666667,
      "grad_norm": 0.19992253184318542,
      "learning_rate": 3.924333333333334e-05,
      "loss": 0.0054,
      "step": 32270
    },
    {
      "epoch": 1.7216,
      "grad_norm": 0.14268231391906738,
      "learning_rate": 3.9240000000000004e-05,
      "loss": 0.0036,
      "step": 32280
    },
    {
      "epoch": 1.7221333333333333,
      "grad_norm": 0.0856851115822792,
      "learning_rate": 3.923666666666666e-05,
      "loss": 0.0025,
      "step": 32290
    },
    {
      "epoch": 1.7226666666666666,
      "grad_norm": 0.31362470984458923,
      "learning_rate": 3.9233333333333336e-05,
      "loss": 0.0042,
      "step": 32300
    },
    {
      "epoch": 1.7231999999999998,
      "grad_norm": 0.008436198346316814,
      "learning_rate": 3.923e-05,
      "loss": 0.0031,
      "step": 32310
    },
    {
      "epoch": 1.7237333333333333,
      "grad_norm": 0.5423217415809631,
      "learning_rate": 3.922666666666667e-05,
      "loss": 0.0035,
      "step": 32320
    },
    {
      "epoch": 1.7242666666666666,
      "grad_norm": 0.1426861435174942,
      "learning_rate": 3.9223333333333334e-05,
      "loss": 0.0031,
      "step": 32330
    },
    {
      "epoch": 1.7248,
      "grad_norm": 0.3135412037372589,
      "learning_rate": 3.922e-05,
      "loss": 0.0044,
      "step": 32340
    },
    {
      "epoch": 1.7253333333333334,
      "grad_norm": 0.003226339351385832,
      "learning_rate": 3.921666666666667e-05,
      "loss": 0.0025,
      "step": 32350
    },
    {
      "epoch": 1.7258666666666667,
      "grad_norm": 0.05698084831237793,
      "learning_rate": 3.921333333333333e-05,
      "loss": 0.0037,
      "step": 32360
    },
    {
      "epoch": 1.7264,
      "grad_norm": 0.0862547978758812,
      "learning_rate": 3.921e-05,
      "loss": 0.0022,
      "step": 32370
    },
    {
      "epoch": 1.7269333333333332,
      "grad_norm": 0.17098955810070038,
      "learning_rate": 3.920666666666667e-05,
      "loss": 0.0028,
      "step": 32380
    },
    {
      "epoch": 1.7274666666666667,
      "grad_norm": 0.3135454058647156,
      "learning_rate": 3.920333333333334e-05,
      "loss": 0.0028,
      "step": 32390
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.02867658995091915,
      "learning_rate": 3.9200000000000004e-05,
      "loss": 0.0028,
      "step": 32400
    },
    {
      "epoch": 1.7285333333333335,
      "grad_norm": 0.11389719694852829,
      "learning_rate": 3.919666666666667e-05,
      "loss": 0.0027,
      "step": 32410
    },
    {
      "epoch": 1.7290666666666668,
      "grad_norm": 0.31403928995132446,
      "learning_rate": 3.9193333333333336e-05,
      "loss": 0.0047,
      "step": 32420
    },
    {
      "epoch": 1.7296,
      "grad_norm": 0.08568881452083588,
      "learning_rate": 3.919e-05,
      "loss": 0.0036,
      "step": 32430
    },
    {
      "epoch": 1.7301333333333333,
      "grad_norm": 0.0016550563741475344,
      "learning_rate": 3.918666666666667e-05,
      "loss": 0.0035,
      "step": 32440
    },
    {
      "epoch": 1.7306666666666666,
      "grad_norm": 0.11401102691888809,
      "learning_rate": 3.9183333333333335e-05,
      "loss": 0.0041,
      "step": 32450
    },
    {
      "epoch": 1.7311999999999999,
      "grad_norm": 0.37045615911483765,
      "learning_rate": 3.918e-05,
      "loss": 0.0039,
      "step": 32460
    },
    {
      "epoch": 1.7317333333333333,
      "grad_norm": 0.05698899179697037,
      "learning_rate": 3.917666666666667e-05,
      "loss": 0.0029,
      "step": 32470
    },
    {
      "epoch": 1.7322666666666666,
      "grad_norm": 0.0573071725666523,
      "learning_rate": 3.917333333333333e-05,
      "loss": 0.0032,
      "step": 32480
    },
    {
      "epoch": 1.7328000000000001,
      "grad_norm": 0.39921465516090393,
      "learning_rate": 3.917e-05,
      "loss": 0.0039,
      "step": 32490
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.19951188564300537,
      "learning_rate": 3.9166666666666665e-05,
      "loss": 0.0034,
      "step": 32500
    },
    {
      "epoch": 1.7338666666666667,
      "grad_norm": 0.11400164663791656,
      "learning_rate": 3.916333333333334e-05,
      "loss": 0.0033,
      "step": 32510
    },
    {
      "epoch": 1.7344,
      "grad_norm": 0.028454717248678207,
      "learning_rate": 3.9160000000000005e-05,
      "loss": 0.0032,
      "step": 32520
    },
    {
      "epoch": 1.7349333333333332,
      "grad_norm": 0.256445974111557,
      "learning_rate": 3.915666666666667e-05,
      "loss": 0.0028,
      "step": 32530
    },
    {
      "epoch": 1.7354666666666667,
      "grad_norm": 0.08553076535463333,
      "learning_rate": 3.915333333333334e-05,
      "loss": 0.0035,
      "step": 32540
    },
    {
      "epoch": 1.736,
      "grad_norm": 0.14326773583889008,
      "learning_rate": 3.915e-05,
      "loss": 0.0031,
      "step": 32550
    },
    {
      "epoch": 1.7365333333333335,
      "grad_norm": 0.1998070776462555,
      "learning_rate": 3.914666666666667e-05,
      "loss": 0.0031,
      "step": 32560
    },
    {
      "epoch": 1.7370666666666668,
      "grad_norm": 0.08576373755931854,
      "learning_rate": 3.9143333333333335e-05,
      "loss": 0.0031,
      "step": 32570
    },
    {
      "epoch": 1.7376,
      "grad_norm": 0.005338861607015133,
      "learning_rate": 3.914e-05,
      "loss": 0.0037,
      "step": 32580
    },
    {
      "epoch": 1.7381333333333333,
      "grad_norm": 0.343434602022171,
      "learning_rate": 3.913666666666667e-05,
      "loss": 0.0048,
      "step": 32590
    },
    {
      "epoch": 1.7386666666666666,
      "grad_norm": 0.2003568559885025,
      "learning_rate": 3.9133333333333334e-05,
      "loss": 0.0082,
      "step": 32600
    },
    {
      "epoch": 1.7391999999999999,
      "grad_norm": 0.02870512567460537,
      "learning_rate": 3.913e-05,
      "loss": 0.0029,
      "step": 32610
    },
    {
      "epoch": 1.7397333333333334,
      "grad_norm": 0.05722915753722191,
      "learning_rate": 3.9126666666666666e-05,
      "loss": 0.0021,
      "step": 32620
    },
    {
      "epoch": 1.7402666666666666,
      "grad_norm": 0.057315342128276825,
      "learning_rate": 3.912333333333333e-05,
      "loss": 0.003,
      "step": 32630
    },
    {
      "epoch": 1.7408000000000001,
      "grad_norm": 0.05718127638101578,
      "learning_rate": 3.912e-05,
      "loss": 0.0031,
      "step": 32640
    },
    {
      "epoch": 1.7413333333333334,
      "grad_norm": 0.08579053729772568,
      "learning_rate": 3.911666666666667e-05,
      "loss": 0.004,
      "step": 32650
    },
    {
      "epoch": 1.7418666666666667,
      "grad_norm": 0.08558245003223419,
      "learning_rate": 3.911333333333334e-05,
      "loss": 0.0023,
      "step": 32660
    },
    {
      "epoch": 1.7424,
      "grad_norm": 0.1144006997346878,
      "learning_rate": 3.911e-05,
      "loss": 0.0025,
      "step": 32670
    },
    {
      "epoch": 1.7429333333333332,
      "grad_norm": 0.3137962222099304,
      "learning_rate": 3.910666666666667e-05,
      "loss": 0.0026,
      "step": 32680
    },
    {
      "epoch": 1.7434666666666667,
      "grad_norm": 0.3722788393497467,
      "learning_rate": 3.9103333333333336e-05,
      "loss": 0.0031,
      "step": 32690
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.171505868434906,
      "learning_rate": 3.91e-05,
      "loss": 0.0036,
      "step": 32700
    },
    {
      "epoch": 1.7445333333333335,
      "grad_norm": 0.2853161096572876,
      "learning_rate": 3.909666666666667e-05,
      "loss": 0.0034,
      "step": 32710
    },
    {
      "epoch": 1.7450666666666668,
      "grad_norm": 0.1142207458615303,
      "learning_rate": 3.9093333333333334e-05,
      "loss": 0.0025,
      "step": 32720
    },
    {
      "epoch": 1.7456,
      "grad_norm": 0.39909544587135315,
      "learning_rate": 3.909000000000001e-05,
      "loss": 0.0025,
      "step": 32730
    },
    {
      "epoch": 1.7461333333333333,
      "grad_norm": 0.028838086873292923,
      "learning_rate": 3.9086666666666666e-05,
      "loss": 0.0024,
      "step": 32740
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 0.28516384959220886,
      "learning_rate": 3.908333333333333e-05,
      "loss": 0.0024,
      "step": 32750
    },
    {
      "epoch": 1.7471999999999999,
      "grad_norm": 0.05692461505532265,
      "learning_rate": 3.908e-05,
      "loss": 0.0033,
      "step": 32760
    },
    {
      "epoch": 1.7477333333333334,
      "grad_norm": 0.11409761011600494,
      "learning_rate": 3.9076666666666665e-05,
      "loss": 0.0043,
      "step": 32770
    },
    {
      "epoch": 1.7482666666666666,
      "grad_norm": 0.17107459902763367,
      "learning_rate": 3.907333333333333e-05,
      "loss": 0.0039,
      "step": 32780
    },
    {
      "epoch": 1.7488000000000001,
      "grad_norm": 0.17126311361789703,
      "learning_rate": 3.9070000000000004e-05,
      "loss": 0.0047,
      "step": 32790
    },
    {
      "epoch": 1.7493333333333334,
      "grad_norm": 2.5104031562805176,
      "learning_rate": 3.906666666666667e-05,
      "loss": 0.0038,
      "step": 32800
    },
    {
      "epoch": 1.7498666666666667,
      "grad_norm": 0.0858808159828186,
      "learning_rate": 3.9063333333333336e-05,
      "loss": 0.006,
      "step": 32810
    },
    {
      "epoch": 1.7504,
      "grad_norm": 0.085873082280159,
      "learning_rate": 3.906e-05,
      "loss": 0.0044,
      "step": 32820
    },
    {
      "epoch": 1.7509333333333332,
      "grad_norm": 0.34290894865989685,
      "learning_rate": 3.905666666666667e-05,
      "loss": 0.0035,
      "step": 32830
    },
    {
      "epoch": 1.7514666666666665,
      "grad_norm": 0.17119143903255463,
      "learning_rate": 3.9053333333333334e-05,
      "loss": 0.0021,
      "step": 32840
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.17114852368831635,
      "learning_rate": 3.905e-05,
      "loss": 0.0034,
      "step": 32850
    },
    {
      "epoch": 1.7525333333333335,
      "grad_norm": 0.1426112949848175,
      "learning_rate": 3.9046666666666673e-05,
      "loss": 0.0038,
      "step": 32860
    },
    {
      "epoch": 1.7530666666666668,
      "grad_norm": 0.22809554636478424,
      "learning_rate": 3.904333333333334e-05,
      "loss": 0.0024,
      "step": 32870
    },
    {
      "epoch": 1.7536,
      "grad_norm": 0.028836997225880623,
      "learning_rate": 3.9040000000000006e-05,
      "loss": 0.0029,
      "step": 32880
    },
    {
      "epoch": 1.7541333333333333,
      "grad_norm": 0.19973354041576385,
      "learning_rate": 3.9036666666666665e-05,
      "loss": 0.0026,
      "step": 32890
    },
    {
      "epoch": 1.7546666666666666,
      "grad_norm": 0.28484293818473816,
      "learning_rate": 3.903333333333333e-05,
      "loss": 0.0027,
      "step": 32900
    },
    {
      "epoch": 1.7551999999999999,
      "grad_norm": 0.11410640925168991,
      "learning_rate": 3.903e-05,
      "loss": 0.0029,
      "step": 32910
    },
    {
      "epoch": 1.7557333333333334,
      "grad_norm": 0.3424345850944519,
      "learning_rate": 3.902666666666667e-05,
      "loss": 0.0022,
      "step": 32920
    },
    {
      "epoch": 1.7562666666666666,
      "grad_norm": 0.05731600150465965,
      "learning_rate": 3.9023333333333336e-05,
      "loss": 0.003,
      "step": 32930
    },
    {
      "epoch": 1.7568000000000001,
      "grad_norm": 0.028621826320886612,
      "learning_rate": 3.902e-05,
      "loss": 0.0032,
      "step": 32940
    },
    {
      "epoch": 1.7573333333333334,
      "grad_norm": 0.11408892273902893,
      "learning_rate": 3.901666666666667e-05,
      "loss": 0.0034,
      "step": 32950
    },
    {
      "epoch": 1.7578666666666667,
      "grad_norm": 0.057119619101285934,
      "learning_rate": 3.9013333333333335e-05,
      "loss": 0.0031,
      "step": 32960
    },
    {
      "epoch": 1.7584,
      "grad_norm": 0.02952391654253006,
      "learning_rate": 3.901e-05,
      "loss": 0.0017,
      "step": 32970
    },
    {
      "epoch": 1.7589333333333332,
      "grad_norm": 0.31334033608436584,
      "learning_rate": 3.900666666666667e-05,
      "loss": 0.0028,
      "step": 32980
    },
    {
      "epoch": 1.7594666666666665,
      "grad_norm": 0.11394284665584564,
      "learning_rate": 3.900333333333333e-05,
      "loss": 0.0029,
      "step": 32990
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.5698580741882324,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.0027,
      "step": 33000
    },
    {
      "epoch": 1.7605333333333333,
      "grad_norm": 0.19954678416252136,
      "learning_rate": 3.899666666666667e-05,
      "loss": 0.0039,
      "step": 33010
    },
    {
      "epoch": 1.7610666666666668,
      "grad_norm": 0.3429233431816101,
      "learning_rate": 3.899333333333334e-05,
      "loss": 0.003,
      "step": 33020
    },
    {
      "epoch": 1.7616,
      "grad_norm": 0.005895392503589392,
      "learning_rate": 3.8990000000000004e-05,
      "loss": 0.004,
      "step": 33030
    },
    {
      "epoch": 1.7621333333333333,
      "grad_norm": 0.1426125317811966,
      "learning_rate": 3.8986666666666664e-05,
      "loss": 0.0042,
      "step": 33040
    },
    {
      "epoch": 1.7626666666666666,
      "grad_norm": 0.01231134682893753,
      "learning_rate": 3.898333333333333e-05,
      "loss": 0.0026,
      "step": 33050
    },
    {
      "epoch": 1.7631999999999999,
      "grad_norm": 0.11401700973510742,
      "learning_rate": 3.898e-05,
      "loss": 0.004,
      "step": 33060
    },
    {
      "epoch": 1.7637333333333334,
      "grad_norm": 0.14356181025505066,
      "learning_rate": 3.897666666666667e-05,
      "loss": 0.0043,
      "step": 33070
    },
    {
      "epoch": 1.7642666666666666,
      "grad_norm": 0.4285062849521637,
      "learning_rate": 3.8973333333333335e-05,
      "loss": 0.0035,
      "step": 33080
    },
    {
      "epoch": 1.7648000000000001,
      "grad_norm": 0.2568371295928955,
      "learning_rate": 3.897e-05,
      "loss": 0.0042,
      "step": 33090
    },
    {
      "epoch": 1.7653333333333334,
      "grad_norm": 0.20009194314479828,
      "learning_rate": 3.896666666666667e-05,
      "loss": 0.0028,
      "step": 33100
    },
    {
      "epoch": 1.7658666666666667,
      "grad_norm": 0.17144136130809784,
      "learning_rate": 3.8963333333333334e-05,
      "loss": 0.005,
      "step": 33110
    },
    {
      "epoch": 1.7664,
      "grad_norm": 0.05713696777820587,
      "learning_rate": 3.896e-05,
      "loss": 0.0029,
      "step": 33120
    },
    {
      "epoch": 1.7669333333333332,
      "grad_norm": 0.31453582644462585,
      "learning_rate": 3.8956666666666666e-05,
      "loss": 0.002,
      "step": 33130
    },
    {
      "epoch": 1.7674666666666665,
      "grad_norm": 0.1143772155046463,
      "learning_rate": 3.895333333333334e-05,
      "loss": 0.0034,
      "step": 33140
    },
    {
      "epoch": 1.768,
      "grad_norm": 0.14300180971622467,
      "learning_rate": 3.8950000000000005e-05,
      "loss": 0.0031,
      "step": 33150
    },
    {
      "epoch": 1.7685333333333333,
      "grad_norm": 0.1431596875190735,
      "learning_rate": 3.894666666666667e-05,
      "loss": 0.0031,
      "step": 33160
    },
    {
      "epoch": 1.7690666666666668,
      "grad_norm": 0.057450905442237854,
      "learning_rate": 3.894333333333334e-05,
      "loss": 0.0033,
      "step": 33170
    },
    {
      "epoch": 1.7696,
      "grad_norm": 0.05700746178627014,
      "learning_rate": 3.894e-05,
      "loss": 0.0038,
      "step": 33180
    },
    {
      "epoch": 1.7701333333333333,
      "grad_norm": 0.08527624607086182,
      "learning_rate": 3.893666666666667e-05,
      "loss": 0.0028,
      "step": 33190
    },
    {
      "epoch": 1.7706666666666666,
      "grad_norm": 0.08542589843273163,
      "learning_rate": 3.8933333333333336e-05,
      "loss": 0.0025,
      "step": 33200
    },
    {
      "epoch": 1.7711999999999999,
      "grad_norm": 0.1429741233587265,
      "learning_rate": 3.893e-05,
      "loss": 0.0039,
      "step": 33210
    },
    {
      "epoch": 1.7717333333333334,
      "grad_norm": 0.17145077884197235,
      "learning_rate": 3.892666666666667e-05,
      "loss": 0.003,
      "step": 33220
    },
    {
      "epoch": 1.7722666666666667,
      "grad_norm": 0.11432259529829025,
      "learning_rate": 3.8923333333333334e-05,
      "loss": 0.0028,
      "step": 33230
    },
    {
      "epoch": 1.7728000000000002,
      "grad_norm": 0.3703230917453766,
      "learning_rate": 3.892e-05,
      "loss": 0.0033,
      "step": 33240
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 0.1996183544397354,
      "learning_rate": 3.8916666666666666e-05,
      "loss": 0.0043,
      "step": 33250
    },
    {
      "epoch": 1.7738666666666667,
      "grad_norm": 0.0855129137635231,
      "learning_rate": 3.891333333333333e-05,
      "loss": 0.0039,
      "step": 33260
    },
    {
      "epoch": 1.7744,
      "grad_norm": 0.11406071484088898,
      "learning_rate": 3.8910000000000005e-05,
      "loss": 0.0039,
      "step": 33270
    },
    {
      "epoch": 1.7749333333333333,
      "grad_norm": 0.14334458112716675,
      "learning_rate": 3.890666666666667e-05,
      "loss": 0.0038,
      "step": 33280
    },
    {
      "epoch": 1.7754666666666665,
      "grad_norm": 0.1432896852493286,
      "learning_rate": 3.890333333333334e-05,
      "loss": 0.0042,
      "step": 33290
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.08570795506238937,
      "learning_rate": 3.8900000000000004e-05,
      "loss": 0.0034,
      "step": 33300
    },
    {
      "epoch": 1.7765333333333333,
      "grad_norm": 0.028468957170844078,
      "learning_rate": 3.889666666666667e-05,
      "loss": 0.0036,
      "step": 33310
    },
    {
      "epoch": 1.7770666666666668,
      "grad_norm": 0.028642958030104637,
      "learning_rate": 3.8893333333333336e-05,
      "loss": 0.0034,
      "step": 33320
    },
    {
      "epoch": 1.7776,
      "grad_norm": 0.08549869805574417,
      "learning_rate": 3.889e-05,
      "loss": 0.0024,
      "step": 33330
    },
    {
      "epoch": 1.7781333333333333,
      "grad_norm": 0.22846990823745728,
      "learning_rate": 3.888666666666667e-05,
      "loss": 0.0035,
      "step": 33340
    },
    {
      "epoch": 1.7786666666666666,
      "grad_norm": 0.05690063163638115,
      "learning_rate": 3.8883333333333334e-05,
      "loss": 0.0027,
      "step": 33350
    },
    {
      "epoch": 1.7792,
      "grad_norm": 0.3707718253135681,
      "learning_rate": 3.888e-05,
      "loss": 0.002,
      "step": 33360
    },
    {
      "epoch": 1.7797333333333332,
      "grad_norm": 0.19948434829711914,
      "learning_rate": 3.8876666666666667e-05,
      "loss": 0.003,
      "step": 33370
    },
    {
      "epoch": 1.7802666666666667,
      "grad_norm": 0.057112108916044235,
      "learning_rate": 3.887333333333333e-05,
      "loss": 0.0019,
      "step": 33380
    },
    {
      "epoch": 1.7808000000000002,
      "grad_norm": 0.0031129533890634775,
      "learning_rate": 3.887e-05,
      "loss": 0.0032,
      "step": 33390
    },
    {
      "epoch": 1.7813333333333334,
      "grad_norm": 0.14213545620441437,
      "learning_rate": 3.8866666666666665e-05,
      "loss": 0.0034,
      "step": 33400
    },
    {
      "epoch": 1.7818666666666667,
      "grad_norm": 0.0574459470808506,
      "learning_rate": 3.886333333333334e-05,
      "loss": 0.0034,
      "step": 33410
    },
    {
      "epoch": 1.7824,
      "grad_norm": 0.1427404284477234,
      "learning_rate": 3.8860000000000004e-05,
      "loss": 0.003,
      "step": 33420
    },
    {
      "epoch": 1.7829333333333333,
      "grad_norm": 0.313955157995224,
      "learning_rate": 3.885666666666667e-05,
      "loss": 0.0026,
      "step": 33430
    },
    {
      "epoch": 1.7834666666666665,
      "grad_norm": 0.14240700006484985,
      "learning_rate": 3.8853333333333336e-05,
      "loss": 0.0028,
      "step": 33440
    },
    {
      "epoch": 1.784,
      "grad_norm": 0.4845157861709595,
      "learning_rate": 3.885e-05,
      "loss": 0.0032,
      "step": 33450
    },
    {
      "epoch": 1.7845333333333333,
      "grad_norm": 0.19955088198184967,
      "learning_rate": 3.884666666666667e-05,
      "loss": 0.0032,
      "step": 33460
    },
    {
      "epoch": 1.7850666666666668,
      "grad_norm": 0.3705821633338928,
      "learning_rate": 3.8843333333333335e-05,
      "loss": 0.0028,
      "step": 33470
    },
    {
      "epoch": 1.7856,
      "grad_norm": 0.227889284491539,
      "learning_rate": 3.884e-05,
      "loss": 0.0026,
      "step": 33480
    },
    {
      "epoch": 1.7861333333333334,
      "grad_norm": 0.057377807796001434,
      "learning_rate": 3.8836666666666674e-05,
      "loss": 0.0029,
      "step": 33490
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 0.08608364313840866,
      "learning_rate": 3.883333333333333e-05,
      "loss": 0.0026,
      "step": 33500
    },
    {
      "epoch": 1.7872,
      "grad_norm": 0.5983860492706299,
      "learning_rate": 3.883e-05,
      "loss": 0.0043,
      "step": 33510
    },
    {
      "epoch": 1.7877333333333332,
      "grad_norm": 0.11401337385177612,
      "learning_rate": 3.8826666666666665e-05,
      "loss": 0.0017,
      "step": 33520
    },
    {
      "epoch": 1.7882666666666667,
      "grad_norm": 0.1427665650844574,
      "learning_rate": 3.882333333333333e-05,
      "loss": 0.0027,
      "step": 33530
    },
    {
      "epoch": 1.7888,
      "grad_norm": 0.28510811924934387,
      "learning_rate": 3.882e-05,
      "loss": 0.0043,
      "step": 33540
    },
    {
      "epoch": 1.7893333333333334,
      "grad_norm": 0.11496831476688385,
      "learning_rate": 3.881666666666667e-05,
      "loss": 0.0093,
      "step": 33550
    },
    {
      "epoch": 1.7898666666666667,
      "grad_norm": 0.16419218480587006,
      "learning_rate": 3.881333333333334e-05,
      "loss": 0.0154,
      "step": 33560
    },
    {
      "epoch": 1.7904,
      "grad_norm": 0.17125307023525238,
      "learning_rate": 3.881e-05,
      "loss": 0.006,
      "step": 33570
    },
    {
      "epoch": 1.7909333333333333,
      "grad_norm": 0.28497350215911865,
      "learning_rate": 3.880666666666667e-05,
      "loss": 0.0048,
      "step": 33580
    },
    {
      "epoch": 1.7914666666666665,
      "grad_norm": 0.028576292097568512,
      "learning_rate": 3.8803333333333335e-05,
      "loss": 0.0043,
      "step": 33590
    },
    {
      "epoch": 1.792,
      "grad_norm": 0.3988508880138397,
      "learning_rate": 3.88e-05,
      "loss": 0.0034,
      "step": 33600
    },
    {
      "epoch": 1.7925333333333333,
      "grad_norm": 0.0578022301197052,
      "learning_rate": 3.879666666666667e-05,
      "loss": 0.0034,
      "step": 33610
    },
    {
      "epoch": 1.7930666666666668,
      "grad_norm": 0.11432062089443207,
      "learning_rate": 3.879333333333334e-05,
      "loss": 0.003,
      "step": 33620
    },
    {
      "epoch": 1.7936,
      "grad_norm": 0.05696549639105797,
      "learning_rate": 3.8790000000000006e-05,
      "loss": 0.0029,
      "step": 33630
    },
    {
      "epoch": 1.7941333333333334,
      "grad_norm": 0.2568492591381073,
      "learning_rate": 3.878666666666667e-05,
      "loss": 0.0035,
      "step": 33640
    },
    {
      "epoch": 1.7946666666666666,
      "grad_norm": 0.19967547059059143,
      "learning_rate": 3.878333333333333e-05,
      "loss": 0.0029,
      "step": 33650
    },
    {
      "epoch": 1.7952,
      "grad_norm": 0.14268679916858673,
      "learning_rate": 3.878e-05,
      "loss": 0.0039,
      "step": 33660
    },
    {
      "epoch": 1.7957333333333332,
      "grad_norm": 0.028814587742090225,
      "learning_rate": 3.8776666666666664e-05,
      "loss": 0.0027,
      "step": 33670
    },
    {
      "epoch": 1.7962666666666667,
      "grad_norm": 0.11409162729978561,
      "learning_rate": 3.877333333333334e-05,
      "loss": 0.0037,
      "step": 33680
    },
    {
      "epoch": 1.7968,
      "grad_norm": 0.2563595175743103,
      "learning_rate": 3.877e-05,
      "loss": 0.0033,
      "step": 33690
    },
    {
      "epoch": 1.7973333333333334,
      "grad_norm": 0.4838784337043762,
      "learning_rate": 3.876666666666667e-05,
      "loss": 0.0033,
      "step": 33700
    },
    {
      "epoch": 1.7978666666666667,
      "grad_norm": 0.0570627860724926,
      "learning_rate": 3.8763333333333335e-05,
      "loss": 0.0038,
      "step": 33710
    },
    {
      "epoch": 1.7984,
      "grad_norm": 0.3416804075241089,
      "learning_rate": 3.876e-05,
      "loss": 0.0022,
      "step": 33720
    },
    {
      "epoch": 1.7989333333333333,
      "grad_norm": 0.015873398631811142,
      "learning_rate": 3.875666666666667e-05,
      "loss": 0.0033,
      "step": 33730
    },
    {
      "epoch": 1.7994666666666665,
      "grad_norm": 0.028704218566417694,
      "learning_rate": 3.8753333333333334e-05,
      "loss": 0.0027,
      "step": 33740
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.1709267646074295,
      "learning_rate": 3.875e-05,
      "loss": 0.0023,
      "step": 33750
    },
    {
      "epoch": 1.8005333333333333,
      "grad_norm": 0.17107100784778595,
      "learning_rate": 3.874666666666667e-05,
      "loss": 0.0016,
      "step": 33760
    },
    {
      "epoch": 1.8010666666666668,
      "grad_norm": 3.0623881816864014,
      "learning_rate": 3.874333333333334e-05,
      "loss": 0.0046,
      "step": 33770
    },
    {
      "epoch": 1.8016,
      "grad_norm": 0.2848873436450958,
      "learning_rate": 3.8740000000000005e-05,
      "loss": 0.0052,
      "step": 33780
    },
    {
      "epoch": 1.8021333333333334,
      "grad_norm": 0.08647559583187103,
      "learning_rate": 3.873666666666667e-05,
      "loss": 0.0042,
      "step": 33790
    },
    {
      "epoch": 1.8026666666666666,
      "grad_norm": 0.19924063980579376,
      "learning_rate": 3.873333333333333e-05,
      "loss": 0.0029,
      "step": 33800
    },
    {
      "epoch": 1.8032,
      "grad_norm": 0.3703240752220154,
      "learning_rate": 3.873e-05,
      "loss": 0.0042,
      "step": 33810
    },
    {
      "epoch": 1.8037333333333332,
      "grad_norm": 0.4556290805339813,
      "learning_rate": 3.872666666666667e-05,
      "loss": 0.0041,
      "step": 33820
    },
    {
      "epoch": 1.8042666666666667,
      "grad_norm": 0.4277641475200653,
      "learning_rate": 3.8723333333333336e-05,
      "loss": 0.0027,
      "step": 33830
    },
    {
      "epoch": 1.8048,
      "grad_norm": 0.6549797058105469,
      "learning_rate": 3.872e-05,
      "loss": 0.0041,
      "step": 33840
    },
    {
      "epoch": 1.8053333333333335,
      "grad_norm": 0.057134877890348434,
      "learning_rate": 3.871666666666667e-05,
      "loss": 0.0048,
      "step": 33850
    },
    {
      "epoch": 1.8058666666666667,
      "grad_norm": 0.17080475389957428,
      "learning_rate": 3.8713333333333334e-05,
      "loss": 0.0037,
      "step": 33860
    },
    {
      "epoch": 1.8064,
      "grad_norm": 0.085611991584301,
      "learning_rate": 3.871e-05,
      "loss": 0.0033,
      "step": 33870
    },
    {
      "epoch": 1.8069333333333333,
      "grad_norm": 0.22762644290924072,
      "learning_rate": 3.8706666666666667e-05,
      "loss": 0.0029,
      "step": 33880
    },
    {
      "epoch": 1.8074666666666666,
      "grad_norm": 0.11399337649345398,
      "learning_rate": 3.870333333333333e-05,
      "loss": 0.0031,
      "step": 33890
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.22775742411613464,
      "learning_rate": 3.8700000000000006e-05,
      "loss": 0.0047,
      "step": 33900
    },
    {
      "epoch": 1.8085333333333333,
      "grad_norm": 0.029094701632857323,
      "learning_rate": 3.869666666666667e-05,
      "loss": 0.0047,
      "step": 33910
    },
    {
      "epoch": 1.8090666666666668,
      "grad_norm": 0.228036031126976,
      "learning_rate": 3.869333333333334e-05,
      "loss": 0.0026,
      "step": 33920
    },
    {
      "epoch": 1.8096,
      "grad_norm": 0.4268696904182434,
      "learning_rate": 3.8690000000000004e-05,
      "loss": 0.0036,
      "step": 33930
    },
    {
      "epoch": 1.8101333333333334,
      "grad_norm": 0.14260216057300568,
      "learning_rate": 3.868666666666667e-05,
      "loss": 0.0026,
      "step": 33940
    },
    {
      "epoch": 1.8106666666666666,
      "grad_norm": 0.11483489722013474,
      "learning_rate": 3.868333333333333e-05,
      "loss": 0.0032,
      "step": 33950
    },
    {
      "epoch": 1.8112,
      "grad_norm": 0.02903083525598049,
      "learning_rate": 3.868e-05,
      "loss": 0.0032,
      "step": 33960
    },
    {
      "epoch": 1.8117333333333332,
      "grad_norm": 0.08568266779184341,
      "learning_rate": 3.867666666666667e-05,
      "loss": 0.0029,
      "step": 33970
    },
    {
      "epoch": 1.8122666666666667,
      "grad_norm": 0.14445914328098297,
      "learning_rate": 3.8673333333333335e-05,
      "loss": 0.0026,
      "step": 33980
    },
    {
      "epoch": 1.8128,
      "grad_norm": 0.1138162612915039,
      "learning_rate": 3.867e-05,
      "loss": 0.0042,
      "step": 33990
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 0.11386030167341232,
      "learning_rate": 3.866666666666667e-05,
      "loss": 0.0036,
      "step": 34000
    },
    {
      "epoch": 1.8138666666666667,
      "grad_norm": 1.0032788515090942,
      "learning_rate": 3.866333333333333e-05,
      "loss": 0.0031,
      "step": 34010
    },
    {
      "epoch": 1.8144,
      "grad_norm": 1.907174825668335,
      "learning_rate": 3.866e-05,
      "loss": 0.0043,
      "step": 34020
    },
    {
      "epoch": 1.8149333333333333,
      "grad_norm": 0.256300151348114,
      "learning_rate": 3.865666666666667e-05,
      "loss": 0.003,
      "step": 34030
    },
    {
      "epoch": 1.8154666666666666,
      "grad_norm": 0.17236660420894623,
      "learning_rate": 3.865333333333334e-05,
      "loss": 0.0046,
      "step": 34040
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 0.25665342807769775,
      "learning_rate": 3.8650000000000004e-05,
      "loss": 0.0026,
      "step": 34050
    },
    {
      "epoch": 1.8165333333333333,
      "grad_norm": 0.0852014347910881,
      "learning_rate": 3.864666666666667e-05,
      "loss": 0.005,
      "step": 34060
    },
    {
      "epoch": 1.8170666666666668,
      "grad_norm": 0.14195837080478668,
      "learning_rate": 3.8643333333333337e-05,
      "loss": 0.0034,
      "step": 34070
    },
    {
      "epoch": 1.8176,
      "grad_norm": 0.3977244794368744,
      "learning_rate": 3.864e-05,
      "loss": 0.0025,
      "step": 34080
    },
    {
      "epoch": 1.8181333333333334,
      "grad_norm": 0.0021425841841846704,
      "learning_rate": 3.863666666666667e-05,
      "loss": 0.0039,
      "step": 34090
    },
    {
      "epoch": 1.8186666666666667,
      "grad_norm": 0.028448795899748802,
      "learning_rate": 3.8633333333333335e-05,
      "loss": 0.0048,
      "step": 34100
    },
    {
      "epoch": 1.8192,
      "grad_norm": 0.1138186827301979,
      "learning_rate": 3.863e-05,
      "loss": 0.0033,
      "step": 34110
    },
    {
      "epoch": 1.8197333333333332,
      "grad_norm": 0.17067593336105347,
      "learning_rate": 3.862666666666667e-05,
      "loss": 0.0044,
      "step": 34120
    },
    {
      "epoch": 1.8202666666666667,
      "grad_norm": 0.14253243803977966,
      "learning_rate": 3.8623333333333333e-05,
      "loss": 0.003,
      "step": 34130
    },
    {
      "epoch": 1.8208,
      "grad_norm": 0.0288494024425745,
      "learning_rate": 3.862e-05,
      "loss": 0.0044,
      "step": 34140
    },
    {
      "epoch": 1.8213333333333335,
      "grad_norm": 0.02890567108988762,
      "learning_rate": 3.8616666666666666e-05,
      "loss": 0.0037,
      "step": 34150
    },
    {
      "epoch": 1.8218666666666667,
      "grad_norm": 0.2562204599380493,
      "learning_rate": 3.861333333333333e-05,
      "loss": 0.0042,
      "step": 34160
    },
    {
      "epoch": 1.8224,
      "grad_norm": 0.03015752136707306,
      "learning_rate": 3.8610000000000005e-05,
      "loss": 0.0023,
      "step": 34170
    },
    {
      "epoch": 1.8229333333333333,
      "grad_norm": 0.31386032700538635,
      "learning_rate": 3.860666666666667e-05,
      "loss": 0.0029,
      "step": 34180
    },
    {
      "epoch": 1.8234666666666666,
      "grad_norm": 0.05706595629453659,
      "learning_rate": 3.860333333333334e-05,
      "loss": 0.0026,
      "step": 34190
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.1711835116147995,
      "learning_rate": 3.86e-05,
      "loss": 0.0031,
      "step": 34200
    },
    {
      "epoch": 1.8245333333333333,
      "grad_norm": 0.08561134338378906,
      "learning_rate": 3.859666666666667e-05,
      "loss": 0.0027,
      "step": 34210
    },
    {
      "epoch": 1.8250666666666666,
      "grad_norm": 0.08564389497041702,
      "learning_rate": 3.8593333333333335e-05,
      "loss": 0.0035,
      "step": 34220
    },
    {
      "epoch": 1.8256000000000001,
      "grad_norm": 0.05714501067996025,
      "learning_rate": 3.859e-05,
      "loss": 0.003,
      "step": 34230
    },
    {
      "epoch": 1.8261333333333334,
      "grad_norm": 0.1997586041688919,
      "learning_rate": 3.858666666666667e-05,
      "loss": 0.003,
      "step": 34240
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 0.11416265368461609,
      "learning_rate": 3.8583333333333334e-05,
      "loss": 0.0036,
      "step": 34250
    },
    {
      "epoch": 1.8272,
      "grad_norm": 0.028596432879567146,
      "learning_rate": 3.858e-05,
      "loss": 0.0034,
      "step": 34260
    },
    {
      "epoch": 1.8277333333333332,
      "grad_norm": 0.5421474575996399,
      "learning_rate": 3.8576666666666666e-05,
      "loss": 0.0041,
      "step": 34270
    },
    {
      "epoch": 1.8282666666666667,
      "grad_norm": 7.927894330350682e-05,
      "learning_rate": 3.857333333333333e-05,
      "loss": 0.0052,
      "step": 34280
    },
    {
      "epoch": 1.8288,
      "grad_norm": 0.028533868491649628,
      "learning_rate": 3.857e-05,
      "loss": 0.0032,
      "step": 34290
    },
    {
      "epoch": 1.8293333333333335,
      "grad_norm": 0.11420617997646332,
      "learning_rate": 3.8566666666666664e-05,
      "loss": 0.0032,
      "step": 34300
    },
    {
      "epoch": 1.8298666666666668,
      "grad_norm": 0.08567200601100922,
      "learning_rate": 3.856333333333334e-05,
      "loss": 0.0035,
      "step": 34310
    },
    {
      "epoch": 1.8304,
      "grad_norm": 1.4704965353012085,
      "learning_rate": 3.8560000000000004e-05,
      "loss": 0.0032,
      "step": 34320
    },
    {
      "epoch": 1.8309333333333333,
      "grad_norm": 0.11412546783685684,
      "learning_rate": 3.855666666666667e-05,
      "loss": 0.0039,
      "step": 34330
    },
    {
      "epoch": 1.8314666666666666,
      "grad_norm": 0.31384727358818054,
      "learning_rate": 3.8553333333333336e-05,
      "loss": 0.0032,
      "step": 34340
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.17118364572525024,
      "learning_rate": 3.855e-05,
      "loss": 0.0036,
      "step": 34350
    },
    {
      "epoch": 1.8325333333333333,
      "grad_norm": 0.1713022142648697,
      "learning_rate": 3.854666666666667e-05,
      "loss": 0.004,
      "step": 34360
    },
    {
      "epoch": 1.8330666666666666,
      "grad_norm": 0.1711903214454651,
      "learning_rate": 3.8543333333333334e-05,
      "loss": 0.0033,
      "step": 34370
    },
    {
      "epoch": 1.8336000000000001,
      "grad_norm": 0.3994353711605072,
      "learning_rate": 3.854000000000001e-05,
      "loss": 0.004,
      "step": 34380
    },
    {
      "epoch": 1.8341333333333334,
      "grad_norm": 0.1712079495191574,
      "learning_rate": 3.853666666666667e-05,
      "loss": 0.0022,
      "step": 34390
    },
    {
      "epoch": 1.8346666666666667,
      "grad_norm": 0.19973671436309814,
      "learning_rate": 3.853333333333334e-05,
      "loss": 0.0029,
      "step": 34400
    },
    {
      "epoch": 1.8352,
      "grad_norm": 0.02854081243276596,
      "learning_rate": 3.853e-05,
      "loss": 0.0028,
      "step": 34410
    },
    {
      "epoch": 1.8357333333333332,
      "grad_norm": 0.4565041661262512,
      "learning_rate": 3.8526666666666665e-05,
      "loss": 0.0016,
      "step": 34420
    },
    {
      "epoch": 1.8362666666666667,
      "grad_norm": 0.3709539771080017,
      "learning_rate": 3.852333333333333e-05,
      "loss": 0.0032,
      "step": 34430
    },
    {
      "epoch": 1.8368,
      "grad_norm": 0.542149543762207,
      "learning_rate": 3.8520000000000004e-05,
      "loss": 0.0036,
      "step": 34440
    },
    {
      "epoch": 1.8373333333333335,
      "grad_norm": 0.02854195423424244,
      "learning_rate": 3.851666666666667e-05,
      "loss": 0.0033,
      "step": 34450
    },
    {
      "epoch": 1.8378666666666668,
      "grad_norm": 0.002667132066562772,
      "learning_rate": 3.8513333333333336e-05,
      "loss": 0.0037,
      "step": 34460
    },
    {
      "epoch": 1.8384,
      "grad_norm": 0.05707956850528717,
      "learning_rate": 3.851e-05,
      "loss": 0.0025,
      "step": 34470
    },
    {
      "epoch": 1.8389333333333333,
      "grad_norm": 0.2853592336177826,
      "learning_rate": 3.850666666666667e-05,
      "loss": 0.0033,
      "step": 34480
    },
    {
      "epoch": 1.8394666666666666,
      "grad_norm": 0.37100479006767273,
      "learning_rate": 3.8503333333333335e-05,
      "loss": 0.0023,
      "step": 34490
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.028783241286873817,
      "learning_rate": 3.85e-05,
      "loss": 0.0033,
      "step": 34500
    },
    {
      "epoch": 1.8405333333333334,
      "grad_norm": 0.19975228607654572,
      "learning_rate": 3.849666666666667e-05,
      "loss": 0.0037,
      "step": 34510
    },
    {
      "epoch": 1.8410666666666666,
      "grad_norm": 0.45673391222953796,
      "learning_rate": 3.849333333333334e-05,
      "loss": 0.0036,
      "step": 34520
    },
    {
      "epoch": 1.8416000000000001,
      "grad_norm": 0.22829675674438477,
      "learning_rate": 3.8490000000000006e-05,
      "loss": 0.0031,
      "step": 34530
    },
    {
      "epoch": 1.8421333333333334,
      "grad_norm": 0.11412307620048523,
      "learning_rate": 3.848666666666667e-05,
      "loss": 0.0042,
      "step": 34540
    },
    {
      "epoch": 1.8426666666666667,
      "grad_norm": 0.08558976650238037,
      "learning_rate": 3.848333333333334e-05,
      "loss": 0.002,
      "step": 34550
    },
    {
      "epoch": 1.8432,
      "grad_norm": 0.2853432297706604,
      "learning_rate": 3.848e-05,
      "loss": 0.0029,
      "step": 34560
    },
    {
      "epoch": 1.8437333333333332,
      "grad_norm": 0.14265850186347961,
      "learning_rate": 3.8476666666666664e-05,
      "loss": 0.0022,
      "step": 34570
    },
    {
      "epoch": 1.8442666666666667,
      "grad_norm": 0.19976994395256042,
      "learning_rate": 3.8473333333333337e-05,
      "loss": 0.0019,
      "step": 34580
    },
    {
      "epoch": 1.8448,
      "grad_norm": 0.0004970569862052798,
      "learning_rate": 3.847e-05,
      "loss": 0.0036,
      "step": 34590
    },
    {
      "epoch": 1.8453333333333335,
      "grad_norm": 0.11413346230983734,
      "learning_rate": 3.846666666666667e-05,
      "loss": 0.0029,
      "step": 34600
    },
    {
      "epoch": 1.8458666666666668,
      "grad_norm": 0.0856163501739502,
      "learning_rate": 3.8463333333333335e-05,
      "loss": 0.0028,
      "step": 34610
    },
    {
      "epoch": 1.8464,
      "grad_norm": 0.08559049665927887,
      "learning_rate": 3.846e-05,
      "loss": 0.0031,
      "step": 34620
    },
    {
      "epoch": 1.8469333333333333,
      "grad_norm": 0.1711776703596115,
      "learning_rate": 3.845666666666667e-05,
      "loss": 0.004,
      "step": 34630
    },
    {
      "epoch": 1.8474666666666666,
      "grad_norm": 0.17118462920188904,
      "learning_rate": 3.845333333333333e-05,
      "loss": 0.0038,
      "step": 34640
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 0.05740132927894592,
      "learning_rate": 3.845e-05,
      "loss": 0.004,
      "step": 34650
    },
    {
      "epoch": 1.8485333333333334,
      "grad_norm": 0.48513564467430115,
      "learning_rate": 3.844666666666667e-05,
      "loss": 0.0034,
      "step": 34660
    },
    {
      "epoch": 1.8490666666666666,
      "grad_norm": 0.02943592518568039,
      "learning_rate": 3.844333333333334e-05,
      "loss": 0.0028,
      "step": 34670
    },
    {
      "epoch": 1.8496000000000001,
      "grad_norm": 0.006861330941319466,
      "learning_rate": 3.8440000000000005e-05,
      "loss": 0.0033,
      "step": 34680
    },
    {
      "epoch": 1.8501333333333334,
      "grad_norm": 0.05706704780459404,
      "learning_rate": 3.843666666666667e-05,
      "loss": 0.0022,
      "step": 34690
    },
    {
      "epoch": 1.8506666666666667,
      "grad_norm": 0.14267612993717194,
      "learning_rate": 3.843333333333334e-05,
      "loss": 0.0033,
      "step": 34700
    },
    {
      "epoch": 1.8512,
      "grad_norm": 0.25683602690696716,
      "learning_rate": 3.8429999999999996e-05,
      "loss": 0.0039,
      "step": 34710
    },
    {
      "epoch": 1.8517333333333332,
      "grad_norm": 0.3424140214920044,
      "learning_rate": 3.842666666666667e-05,
      "loss": 0.0033,
      "step": 34720
    },
    {
      "epoch": 1.8522666666666665,
      "grad_norm": 1.123982548713684,
      "learning_rate": 3.8423333333333335e-05,
      "loss": 0.0034,
      "step": 34730
    },
    {
      "epoch": 1.8528,
      "grad_norm": 0.028530001640319824,
      "learning_rate": 3.842e-05,
      "loss": 0.0027,
      "step": 34740
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 0.17119796574115753,
      "learning_rate": 3.841666666666667e-05,
      "loss": 0.0029,
      "step": 34750
    },
    {
      "epoch": 1.8538666666666668,
      "grad_norm": 0.2567761540412903,
      "learning_rate": 3.8413333333333334e-05,
      "loss": 0.0039,
      "step": 34760
    },
    {
      "epoch": 1.8544,
      "grad_norm": 0.34237056970596313,
      "learning_rate": 3.841e-05,
      "loss": 0.0033,
      "step": 34770
    },
    {
      "epoch": 1.8549333333333333,
      "grad_norm": 0.14265389740467072,
      "learning_rate": 3.8406666666666666e-05,
      "loss": 0.0038,
      "step": 34780
    },
    {
      "epoch": 1.8554666666666666,
      "grad_norm": 0.08689170330762863,
      "learning_rate": 3.840333333333334e-05,
      "loss": 0.0035,
      "step": 34790
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.05715686082839966,
      "learning_rate": 3.8400000000000005e-05,
      "loss": 0.0029,
      "step": 34800
    },
    {
      "epoch": 1.8565333333333334,
      "grad_norm": 0.1996888667345047,
      "learning_rate": 3.839666666666667e-05,
      "loss": 0.0038,
      "step": 34810
    },
    {
      "epoch": 1.8570666666666666,
      "grad_norm": 0.25674888491630554,
      "learning_rate": 3.839333333333334e-05,
      "loss": 0.0035,
      "step": 34820
    },
    {
      "epoch": 1.8576000000000001,
      "grad_norm": 0.12492013722658157,
      "learning_rate": 3.8390000000000003e-05,
      "loss": 0.0031,
      "step": 34830
    },
    {
      "epoch": 1.8581333333333334,
      "grad_norm": 0.05842990800738335,
      "learning_rate": 3.838666666666667e-05,
      "loss": 0.0031,
      "step": 34840
    },
    {
      "epoch": 1.8586666666666667,
      "grad_norm": 0.14263248443603516,
      "learning_rate": 3.8383333333333336e-05,
      "loss": 0.0035,
      "step": 34850
    },
    {
      "epoch": 1.8592,
      "grad_norm": 0.02855311520397663,
      "learning_rate": 3.838e-05,
      "loss": 0.0032,
      "step": 34860
    },
    {
      "epoch": 1.8597333333333332,
      "grad_norm": 0.3423279821872711,
      "learning_rate": 3.837666666666667e-05,
      "loss": 0.0026,
      "step": 34870
    },
    {
      "epoch": 1.8602666666666665,
      "grad_norm": 0.4280139207839966,
      "learning_rate": 3.8373333333333334e-05,
      "loss": 0.0046,
      "step": 34880
    },
    {
      "epoch": 1.8608,
      "grad_norm": 0.11412674188613892,
      "learning_rate": 3.837e-05,
      "loss": 0.0029,
      "step": 34890
    },
    {
      "epoch": 1.8613333333333333,
      "grad_norm": 0.057643741369247437,
      "learning_rate": 3.8366666666666666e-05,
      "loss": 0.004,
      "step": 34900
    },
    {
      "epoch": 1.8618666666666668,
      "grad_norm": 0.25670942664146423,
      "learning_rate": 3.836333333333333e-05,
      "loss": 0.0029,
      "step": 34910
    },
    {
      "epoch": 1.8624,
      "grad_norm": 0.12603779137134552,
      "learning_rate": 3.836e-05,
      "loss": 0.0033,
      "step": 34920
    },
    {
      "epoch": 1.8629333333333333,
      "grad_norm": 0.11426407843828201,
      "learning_rate": 3.835666666666667e-05,
      "loss": 0.0032,
      "step": 34930
    },
    {
      "epoch": 1.8634666666666666,
      "grad_norm": 0.34261566400527954,
      "learning_rate": 3.835333333333334e-05,
      "loss": 0.004,
      "step": 34940
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 0.3706541061401367,
      "learning_rate": 3.8350000000000004e-05,
      "loss": 0.0042,
      "step": 34950
    },
    {
      "epoch": 1.8645333333333334,
      "grad_norm": 0.17090503871440887,
      "learning_rate": 3.834666666666667e-05,
      "loss": 0.0024,
      "step": 34960
    },
    {
      "epoch": 1.8650666666666667,
      "grad_norm": 0.028532564640045166,
      "learning_rate": 3.8343333333333336e-05,
      "loss": 0.003,
      "step": 34970
    },
    {
      "epoch": 1.8656000000000001,
      "grad_norm": 0.05706116929650307,
      "learning_rate": 3.834e-05,
      "loss": 0.0038,
      "step": 34980
    },
    {
      "epoch": 1.8661333333333334,
      "grad_norm": 0.08558251708745956,
      "learning_rate": 3.833666666666667e-05,
      "loss": 0.0024,
      "step": 34990
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.1426350474357605,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 0.003,
      "step": 35000
    },
    {
      "epoch": 1.8672,
      "grad_norm": 0.11409901827573776,
      "learning_rate": 3.833e-05,
      "loss": 0.0034,
      "step": 35010
    },
    {
      "epoch": 1.8677333333333332,
      "grad_norm": 0.2567330598831177,
      "learning_rate": 3.832666666666667e-05,
      "loss": 0.0033,
      "step": 35020
    },
    {
      "epoch": 1.8682666666666665,
      "grad_norm": 0.3230454623699188,
      "learning_rate": 3.832333333333333e-05,
      "loss": 0.0036,
      "step": 35030
    },
    {
      "epoch": 1.8688,
      "grad_norm": 0.02855702117085457,
      "learning_rate": 3.832e-05,
      "loss": 0.0035,
      "step": 35040
    },
    {
      "epoch": 1.8693333333333333,
      "grad_norm": 0.11411940306425095,
      "learning_rate": 3.8316666666666665e-05,
      "loss": 0.0035,
      "step": 35050
    },
    {
      "epoch": 1.8698666666666668,
      "grad_norm": 0.3993891775608063,
      "learning_rate": 3.831333333333333e-05,
      "loss": 0.0033,
      "step": 35060
    },
    {
      "epoch": 1.8704,
      "grad_norm": 0.0570756196975708,
      "learning_rate": 3.8310000000000004e-05,
      "loss": 0.0031,
      "step": 35070
    },
    {
      "epoch": 1.8709333333333333,
      "grad_norm": 0.14395782351493835,
      "learning_rate": 3.830666666666667e-05,
      "loss": 0.0036,
      "step": 35080
    },
    {
      "epoch": 1.8714666666666666,
      "grad_norm": 0.17117831110954285,
      "learning_rate": 3.8303333333333336e-05,
      "loss": 0.0028,
      "step": 35090
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.05706169083714485,
      "learning_rate": 3.83e-05,
      "loss": 0.0027,
      "step": 35100
    },
    {
      "epoch": 1.8725333333333334,
      "grad_norm": 0.11411622911691666,
      "learning_rate": 3.829666666666667e-05,
      "loss": 0.0031,
      "step": 35110
    },
    {
      "epoch": 1.8730666666666667,
      "grad_norm": 0.11606130748987198,
      "learning_rate": 3.8293333333333335e-05,
      "loss": 0.0032,
      "step": 35120
    },
    {
      "epoch": 1.8736000000000002,
      "grad_norm": 0.34234702587127686,
      "learning_rate": 3.829e-05,
      "loss": 0.0034,
      "step": 35130
    },
    {
      "epoch": 1.8741333333333334,
      "grad_norm": 0.570489227771759,
      "learning_rate": 3.8286666666666674e-05,
      "loss": 0.0025,
      "step": 35140
    },
    {
      "epoch": 1.8746666666666667,
      "grad_norm": 0.28543445467948914,
      "learning_rate": 3.828333333333334e-05,
      "loss": 0.0035,
      "step": 35150
    },
    {
      "epoch": 1.8752,
      "grad_norm": 0.004349957685917616,
      "learning_rate": 3.828e-05,
      "loss": 0.0026,
      "step": 35160
    },
    {
      "epoch": 1.8757333333333333,
      "grad_norm": 0.08557772636413574,
      "learning_rate": 3.8276666666666666e-05,
      "loss": 0.0031,
      "step": 35170
    },
    {
      "epoch": 1.8762666666666665,
      "grad_norm": 0.1711602360010147,
      "learning_rate": 3.827333333333333e-05,
      "loss": 0.0032,
      "step": 35180
    },
    {
      "epoch": 1.8768,
      "grad_norm": 0.4564719796180725,
      "learning_rate": 3.827e-05,
      "loss": 0.0027,
      "step": 35190
    },
    {
      "epoch": 1.8773333333333333,
      "grad_norm": 0.5134608745574951,
      "learning_rate": 3.8266666666666664e-05,
      "loss": 0.0023,
      "step": 35200
    },
    {
      "epoch": 1.8778666666666668,
      "grad_norm": 0.4564489424228668,
      "learning_rate": 3.826333333333334e-05,
      "loss": 0.0034,
      "step": 35210
    },
    {
      "epoch": 1.8784,
      "grad_norm": 0.14263424277305603,
      "learning_rate": 3.826e-05,
      "loss": 0.0027,
      "step": 35220
    },
    {
      "epoch": 1.8789333333333333,
      "grad_norm": 0.00023838250490371138,
      "learning_rate": 3.825666666666667e-05,
      "loss": 0.0042,
      "step": 35230
    },
    {
      "epoch": 1.8794666666666666,
      "grad_norm": 0.17112404108047485,
      "learning_rate": 3.8253333333333335e-05,
      "loss": 0.0033,
      "step": 35240
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.1426270604133606,
      "learning_rate": 3.825e-05,
      "loss": 0.0038,
      "step": 35250
    },
    {
      "epoch": 1.8805333333333332,
      "grad_norm": 0.057060372084379196,
      "learning_rate": 3.824666666666667e-05,
      "loss": 0.0028,
      "step": 35260
    },
    {
      "epoch": 1.8810666666666667,
      "grad_norm": 0.22834476828575134,
      "learning_rate": 3.8243333333333334e-05,
      "loss": 0.0044,
      "step": 35270
    },
    {
      "epoch": 1.8816000000000002,
      "grad_norm": 0.14284855127334595,
      "learning_rate": 3.8240000000000007e-05,
      "loss": 0.0026,
      "step": 35280
    },
    {
      "epoch": 1.8821333333333334,
      "grad_norm": 0.25670865178108215,
      "learning_rate": 3.823666666666667e-05,
      "loss": 0.0035,
      "step": 35290
    },
    {
      "epoch": 1.8826666666666667,
      "grad_norm": 0.08559183031320572,
      "learning_rate": 3.823333333333334e-05,
      "loss": 0.0033,
      "step": 35300
    },
    {
      "epoch": 1.8832,
      "grad_norm": 0.11385772377252579,
      "learning_rate": 3.823e-05,
      "loss": 0.005,
      "step": 35310
    },
    {
      "epoch": 1.8837333333333333,
      "grad_norm": 1.1276614665985107,
      "learning_rate": 3.8226666666666664e-05,
      "loss": 0.0038,
      "step": 35320
    },
    {
      "epoch": 1.8842666666666665,
      "grad_norm": 0.31367620825767517,
      "learning_rate": 3.822333333333333e-05,
      "loss": 0.0028,
      "step": 35330
    },
    {
      "epoch": 1.8848,
      "grad_norm": 0.1711292415857315,
      "learning_rate": 3.822e-05,
      "loss": 0.0036,
      "step": 35340
    },
    {
      "epoch": 1.8853333333333333,
      "grad_norm": 0.11407400667667389,
      "learning_rate": 3.821666666666667e-05,
      "loss": 0.0039,
      "step": 35350
    },
    {
      "epoch": 1.8858666666666668,
      "grad_norm": 0.14259575307369232,
      "learning_rate": 3.8213333333333336e-05,
      "loss": 0.0038,
      "step": 35360
    },
    {
      "epoch": 1.8864,
      "grad_norm": 0.5133453607559204,
      "learning_rate": 3.821e-05,
      "loss": 0.0036,
      "step": 35370
    },
    {
      "epoch": 1.8869333333333334,
      "grad_norm": 0.5989168882369995,
      "learning_rate": 3.820666666666667e-05,
      "loss": 0.0033,
      "step": 35380
    },
    {
      "epoch": 1.8874666666666666,
      "grad_norm": 0.11405935883522034,
      "learning_rate": 3.8203333333333334e-05,
      "loss": 0.0051,
      "step": 35390
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.1425982564687729,
      "learning_rate": 3.82e-05,
      "loss": 0.0049,
      "step": 35400
    },
    {
      "epoch": 1.8885333333333332,
      "grad_norm": 0.11407467722892761,
      "learning_rate": 3.8196666666666666e-05,
      "loss": 0.0026,
      "step": 35410
    },
    {
      "epoch": 1.8890666666666667,
      "grad_norm": 0.11407431960105896,
      "learning_rate": 3.819333333333334e-05,
      "loss": 0.0047,
      "step": 35420
    },
    {
      "epoch": 1.8896,
      "grad_norm": 0.11408784985542297,
      "learning_rate": 3.8190000000000005e-05,
      "loss": 0.0036,
      "step": 35430
    },
    {
      "epoch": 1.8901333333333334,
      "grad_norm": 0.3422468304634094,
      "learning_rate": 3.818666666666667e-05,
      "loss": 0.0048,
      "step": 35440
    },
    {
      "epoch": 1.8906666666666667,
      "grad_norm": 0.14259883761405945,
      "learning_rate": 3.818333333333334e-05,
      "loss": 0.003,
      "step": 35450
    },
    {
      "epoch": 1.8912,
      "grad_norm": 0.028522536158561707,
      "learning_rate": 3.818e-05,
      "loss": 0.0029,
      "step": 35460
    },
    {
      "epoch": 1.8917333333333333,
      "grad_norm": 0.4563564956188202,
      "learning_rate": 3.817666666666666e-05,
      "loss": 0.0037,
      "step": 35470
    },
    {
      "epoch": 1.8922666666666665,
      "grad_norm": 0.057059064507484436,
      "learning_rate": 3.8173333333333336e-05,
      "loss": 0.0034,
      "step": 35480
    },
    {
      "epoch": 1.8928,
      "grad_norm": 0.2281896471977234,
      "learning_rate": 3.817e-05,
      "loss": 0.0028,
      "step": 35490
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 0.22830885648727417,
      "learning_rate": 3.816666666666667e-05,
      "loss": 0.0031,
      "step": 35500
    },
    {
      "epoch": 1.8938666666666668,
      "grad_norm": 0.22821174561977386,
      "learning_rate": 3.8163333333333334e-05,
      "loss": 0.0026,
      "step": 35510
    },
    {
      "epoch": 1.8944,
      "grad_norm": 0.1438872516155243,
      "learning_rate": 3.816e-05,
      "loss": 0.0021,
      "step": 35520
    },
    {
      "epoch": 1.8949333333333334,
      "grad_norm": 0.0570211187005043,
      "learning_rate": 3.815666666666667e-05,
      "loss": 0.0028,
      "step": 35530
    },
    {
      "epoch": 1.8954666666666666,
      "grad_norm": 0.3990234136581421,
      "learning_rate": 3.815333333333333e-05,
      "loss": 0.0034,
      "step": 35540
    },
    {
      "epoch": 1.896,
      "grad_norm": 0.22811362147331238,
      "learning_rate": 3.8150000000000006e-05,
      "loss": 0.0038,
      "step": 35550
    },
    {
      "epoch": 1.8965333333333332,
      "grad_norm": 0.2844800353050232,
      "learning_rate": 3.814666666666667e-05,
      "loss": 0.0024,
      "step": 35560
    },
    {
      "epoch": 1.8970666666666667,
      "grad_norm": 0.3986923396587372,
      "learning_rate": 3.814333333333334e-05,
      "loss": 0.0037,
      "step": 35570
    },
    {
      "epoch": 1.8976,
      "grad_norm": 0.11406109482049942,
      "learning_rate": 3.8140000000000004e-05,
      "loss": 0.0036,
      "step": 35580
    },
    {
      "epoch": 1.8981333333333335,
      "grad_norm": 2.32175874710083,
      "learning_rate": 3.813666666666667e-05,
      "loss": 0.0025,
      "step": 35590
    },
    {
      "epoch": 1.8986666666666667,
      "grad_norm": 0.25729793310165405,
      "learning_rate": 3.8133333333333336e-05,
      "loss": 0.0033,
      "step": 35600
    },
    {
      "epoch": 1.8992,
      "grad_norm": 0.17132529616355896,
      "learning_rate": 3.8129999999999996e-05,
      "loss": 0.0034,
      "step": 35610
    },
    {
      "epoch": 1.8997333333333333,
      "grad_norm": 0.03221746161580086,
      "learning_rate": 3.812666666666667e-05,
      "loss": 0.003,
      "step": 35620
    },
    {
      "epoch": 1.9002666666666665,
      "grad_norm": 0.11380497366189957,
      "learning_rate": 3.8123333333333335e-05,
      "loss": 0.0039,
      "step": 35630
    },
    {
      "epoch": 1.9008,
      "grad_norm": 0.08594363927841187,
      "learning_rate": 3.812e-05,
      "loss": 0.0034,
      "step": 35640
    },
    {
      "epoch": 1.9013333333333333,
      "grad_norm": 0.284868985414505,
      "learning_rate": 3.811666666666667e-05,
      "loss": 0.0024,
      "step": 35650
    },
    {
      "epoch": 1.9018666666666668,
      "grad_norm": 0.11377536505460739,
      "learning_rate": 3.811333333333333e-05,
      "loss": 0.0023,
      "step": 35660
    },
    {
      "epoch": 1.9024,
      "grad_norm": 0.08543878048658371,
      "learning_rate": 3.811e-05,
      "loss": 0.0019,
      "step": 35670
    },
    {
      "epoch": 1.9029333333333334,
      "grad_norm": 0.19912117719650269,
      "learning_rate": 3.8106666666666665e-05,
      "loss": 0.0042,
      "step": 35680
    },
    {
      "epoch": 1.9034666666666666,
      "grad_norm": 0.22788268327713013,
      "learning_rate": 3.810333333333334e-05,
      "loss": 0.0023,
      "step": 35690
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.4551185071468353,
      "learning_rate": 3.8100000000000005e-05,
      "loss": 0.0029,
      "step": 35700
    },
    {
      "epoch": 1.9045333333333332,
      "grad_norm": 0.7111356854438782,
      "learning_rate": 3.809666666666667e-05,
      "loss": 0.003,
      "step": 35710
    },
    {
      "epoch": 1.9050666666666667,
      "grad_norm": 0.3129190504550934,
      "learning_rate": 3.809333333333334e-05,
      "loss": 0.0027,
      "step": 35720
    },
    {
      "epoch": 1.9056,
      "grad_norm": 0.14240111410617828,
      "learning_rate": 3.809e-05,
      "loss": 0.0029,
      "step": 35730
    },
    {
      "epoch": 1.9061333333333335,
      "grad_norm": 0.11404705047607422,
      "learning_rate": 3.808666666666667e-05,
      "loss": 0.0015,
      "step": 35740
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.02925381436944008,
      "learning_rate": 3.8083333333333335e-05,
      "loss": 0.0024,
      "step": 35750
    },
    {
      "epoch": 1.9072,
      "grad_norm": 0.19971150159835815,
      "learning_rate": 3.808e-05,
      "loss": 0.0035,
      "step": 35760
    },
    {
      "epoch": 1.9077333333333333,
      "grad_norm": 0.08635113388299942,
      "learning_rate": 3.807666666666667e-05,
      "loss": 0.0029,
      "step": 35770
    },
    {
      "epoch": 1.9082666666666666,
      "grad_norm": 0.057169653475284576,
      "learning_rate": 3.8073333333333334e-05,
      "loss": 0.0039,
      "step": 35780
    },
    {
      "epoch": 1.9088,
      "grad_norm": 0.08524197340011597,
      "learning_rate": 3.807e-05,
      "loss": 0.0043,
      "step": 35790
    },
    {
      "epoch": 1.9093333333333333,
      "grad_norm": 0.08513335883617401,
      "learning_rate": 3.8066666666666666e-05,
      "loss": 0.0038,
      "step": 35800
    },
    {
      "epoch": 1.9098666666666668,
      "grad_norm": 0.1985400915145874,
      "learning_rate": 3.806333333333333e-05,
      "loss": 0.0027,
      "step": 35810
    },
    {
      "epoch": 1.9104,
      "grad_norm": 0.05909022316336632,
      "learning_rate": 3.806e-05,
      "loss": 0.0033,
      "step": 35820
    },
    {
      "epoch": 1.9109333333333334,
      "grad_norm": 0.2560812830924988,
      "learning_rate": 3.805666666666667e-05,
      "loss": 0.0044,
      "step": 35830
    },
    {
      "epoch": 1.9114666666666666,
      "grad_norm": 0.22743858397006989,
      "learning_rate": 3.805333333333334e-05,
      "loss": 0.0043,
      "step": 35840
    },
    {
      "epoch": 1.912,
      "grad_norm": 0.8232771754264832,
      "learning_rate": 3.805e-05,
      "loss": 0.0031,
      "step": 35850
    },
    {
      "epoch": 1.9125333333333332,
      "grad_norm": 0.5395590662956238,
      "learning_rate": 3.804666666666667e-05,
      "loss": 0.0027,
      "step": 35860
    },
    {
      "epoch": 1.9130666666666667,
      "grad_norm": 0.19873587787151337,
      "learning_rate": 3.8043333333333336e-05,
      "loss": 0.0031,
      "step": 35870
    },
    {
      "epoch": 1.9136,
      "grad_norm": 0.11457392573356628,
      "learning_rate": 3.804e-05,
      "loss": 0.0044,
      "step": 35880
    },
    {
      "epoch": 1.9141333333333335,
      "grad_norm": 0.22807352244853973,
      "learning_rate": 3.803666666666667e-05,
      "loss": 0.0035,
      "step": 35890
    },
    {
      "epoch": 1.9146666666666667,
      "grad_norm": 0.11414657533168793,
      "learning_rate": 3.803333333333334e-05,
      "loss": 0.0042,
      "step": 35900
    },
    {
      "epoch": 1.9152,
      "grad_norm": 0.4289853274822235,
      "learning_rate": 3.803000000000001e-05,
      "loss": 0.0041,
      "step": 35910
    },
    {
      "epoch": 1.9157333333333333,
      "grad_norm": 0.05689413100481033,
      "learning_rate": 3.8026666666666666e-05,
      "loss": 0.0017,
      "step": 35920
    },
    {
      "epoch": 1.9162666666666666,
      "grad_norm": 0.05778055265545845,
      "learning_rate": 3.802333333333333e-05,
      "loss": 0.0047,
      "step": 35930
    },
    {
      "epoch": 1.9167999999999998,
      "grad_norm": 0.3127582371234894,
      "learning_rate": 3.802e-05,
      "loss": 0.0031,
      "step": 35940
    },
    {
      "epoch": 1.9173333333333333,
      "grad_norm": 0.25592389702796936,
      "learning_rate": 3.8016666666666665e-05,
      "loss": 0.0039,
      "step": 35950
    },
    {
      "epoch": 1.9178666666666668,
      "grad_norm": 0.2279016077518463,
      "learning_rate": 3.801333333333333e-05,
      "loss": 0.0035,
      "step": 35960
    },
    {
      "epoch": 1.9184,
      "grad_norm": 0.39869144558906555,
      "learning_rate": 3.8010000000000004e-05,
      "loss": 0.0044,
      "step": 35970
    },
    {
      "epoch": 1.9189333333333334,
      "grad_norm": 0.028537632897496223,
      "learning_rate": 3.800666666666667e-05,
      "loss": 0.003,
      "step": 35980
    },
    {
      "epoch": 1.9194666666666667,
      "grad_norm": 0.05704961344599724,
      "learning_rate": 3.8003333333333336e-05,
      "loss": 0.0038,
      "step": 35990
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.2571958005428314,
      "learning_rate": 3.8e-05,
      "loss": 0.0032,
      "step": 36000
    },
    {
      "epoch": 1.9205333333333332,
      "grad_norm": 0.08598634600639343,
      "learning_rate": 3.799666666666667e-05,
      "loss": 0.0022,
      "step": 36010
    },
    {
      "epoch": 1.9210666666666667,
      "grad_norm": 0.5417059063911438,
      "learning_rate": 3.7993333333333334e-05,
      "loss": 0.0022,
      "step": 36020
    },
    {
      "epoch": 1.9216,
      "grad_norm": 1.0844557285308838,
      "learning_rate": 3.799e-05,
      "loss": 0.0049,
      "step": 36030
    },
    {
      "epoch": 1.9221333333333335,
      "grad_norm": 0.1992889642715454,
      "learning_rate": 3.7986666666666673e-05,
      "loss": 0.0037,
      "step": 36040
    },
    {
      "epoch": 1.9226666666666667,
      "grad_norm": 0.2562108337879181,
      "learning_rate": 3.798333333333334e-05,
      "loss": 0.0033,
      "step": 36050
    },
    {
      "epoch": 1.9232,
      "grad_norm": 0.028709791600704193,
      "learning_rate": 3.7980000000000006e-05,
      "loss": 0.0025,
      "step": 36060
    },
    {
      "epoch": 1.9237333333333333,
      "grad_norm": 0.2847609221935272,
      "learning_rate": 3.7976666666666665e-05,
      "loss": 0.0047,
      "step": 36070
    },
    {
      "epoch": 1.9242666666666666,
      "grad_norm": 0.22761231660842896,
      "learning_rate": 3.797333333333333e-05,
      "loss": 0.0033,
      "step": 36080
    },
    {
      "epoch": 1.9247999999999998,
      "grad_norm": 0.05766056850552559,
      "learning_rate": 3.797e-05,
      "loss": 0.0072,
      "step": 36090
    },
    {
      "epoch": 1.9253333333333333,
      "grad_norm": 0.3133452534675598,
      "learning_rate": 3.796666666666667e-05,
      "loss": 0.0055,
      "step": 36100
    },
    {
      "epoch": 1.9258666666666666,
      "grad_norm": 0.11368109285831451,
      "learning_rate": 3.7963333333333336e-05,
      "loss": 0.0083,
      "step": 36110
    },
    {
      "epoch": 1.9264000000000001,
      "grad_norm": 0.0038982441183179617,
      "learning_rate": 3.796e-05,
      "loss": 0.0037,
      "step": 36120
    },
    {
      "epoch": 1.9269333333333334,
      "grad_norm": 0.22851699590682983,
      "learning_rate": 3.795666666666667e-05,
      "loss": 0.0034,
      "step": 36130
    },
    {
      "epoch": 1.9274666666666667,
      "grad_norm": 0.34119656682014465,
      "learning_rate": 3.7953333333333335e-05,
      "loss": 0.0031,
      "step": 36140
    },
    {
      "epoch": 1.928,
      "grad_norm": 0.08618412911891937,
      "learning_rate": 3.795e-05,
      "loss": 0.0041,
      "step": 36150
    },
    {
      "epoch": 1.9285333333333332,
      "grad_norm": 0.2557058036327362,
      "learning_rate": 3.794666666666667e-05,
      "loss": 0.0031,
      "step": 36160
    },
    {
      "epoch": 1.9290666666666667,
      "grad_norm": 0.08547233045101166,
      "learning_rate": 3.794333333333333e-05,
      "loss": 0.004,
      "step": 36170
    },
    {
      "epoch": 1.9296,
      "grad_norm": 0.05709794908761978,
      "learning_rate": 3.7940000000000006e-05,
      "loss": 0.0044,
      "step": 36180
    },
    {
      "epoch": 1.9301333333333335,
      "grad_norm": 0.001974459271878004,
      "learning_rate": 3.793666666666667e-05,
      "loss": 0.0026,
      "step": 36190
    },
    {
      "epoch": 1.9306666666666668,
      "grad_norm": 0.11395794153213501,
      "learning_rate": 3.793333333333334e-05,
      "loss": 0.0033,
      "step": 36200
    },
    {
      "epoch": 1.9312,
      "grad_norm": 0.028487123548984528,
      "learning_rate": 3.7930000000000004e-05,
      "loss": 0.0035,
      "step": 36210
    },
    {
      "epoch": 1.9317333333333333,
      "grad_norm": 0.2278846949338913,
      "learning_rate": 3.7926666666666664e-05,
      "loss": 0.0025,
      "step": 36220
    },
    {
      "epoch": 1.9322666666666666,
      "grad_norm": 0.057103753089904785,
      "learning_rate": 3.792333333333333e-05,
      "loss": 0.0038,
      "step": 36230
    },
    {
      "epoch": 1.9327999999999999,
      "grad_norm": 0.14274556934833527,
      "learning_rate": 3.792e-05,
      "loss": 0.0044,
      "step": 36240
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.057102642953395844,
      "learning_rate": 3.791666666666667e-05,
      "loss": 0.004,
      "step": 36250
    },
    {
      "epoch": 1.9338666666666666,
      "grad_norm": 0.08564794063568115,
      "learning_rate": 3.7913333333333335e-05,
      "loss": 0.003,
      "step": 36260
    },
    {
      "epoch": 1.9344000000000001,
      "grad_norm": 0.42822352051734924,
      "learning_rate": 3.791e-05,
      "loss": 0.0026,
      "step": 36270
    },
    {
      "epoch": 1.9349333333333334,
      "grad_norm": 0.22839760780334473,
      "learning_rate": 3.790666666666667e-05,
      "loss": 0.0034,
      "step": 36280
    },
    {
      "epoch": 1.9354666666666667,
      "grad_norm": 0.028550760820508003,
      "learning_rate": 3.7903333333333334e-05,
      "loss": 0.0038,
      "step": 36290
    },
    {
      "epoch": 1.936,
      "grad_norm": 0.25692713260650635,
      "learning_rate": 3.79e-05,
      "loss": 0.0025,
      "step": 36300
    },
    {
      "epoch": 1.9365333333333332,
      "grad_norm": 0.39977914094924927,
      "learning_rate": 3.789666666666667e-05,
      "loss": 0.0022,
      "step": 36310
    },
    {
      "epoch": 1.9370666666666667,
      "grad_norm": 0.45678213238716125,
      "learning_rate": 3.789333333333334e-05,
      "loss": 0.0035,
      "step": 36320
    },
    {
      "epoch": 1.9376,
      "grad_norm": 0.28553399443626404,
      "learning_rate": 3.7890000000000005e-05,
      "loss": 0.0025,
      "step": 36330
    },
    {
      "epoch": 1.9381333333333335,
      "grad_norm": 0.39970025420188904,
      "learning_rate": 3.788666666666667e-05,
      "loss": 0.003,
      "step": 36340
    },
    {
      "epoch": 1.9386666666666668,
      "grad_norm": 0.08568286895751953,
      "learning_rate": 3.788333333333334e-05,
      "loss": 0.004,
      "step": 36350
    },
    {
      "epoch": 1.9392,
      "grad_norm": 0.25698113441467285,
      "learning_rate": 3.788e-05,
      "loss": 0.0045,
      "step": 36360
    },
    {
      "epoch": 1.9397333333333333,
      "grad_norm": 0.028593197464942932,
      "learning_rate": 3.787666666666666e-05,
      "loss": 0.0029,
      "step": 36370
    },
    {
      "epoch": 1.9402666666666666,
      "grad_norm": 0.22842013835906982,
      "learning_rate": 3.7873333333333336e-05,
      "loss": 0.0025,
      "step": 36380
    },
    {
      "epoch": 1.9407999999999999,
      "grad_norm": 0.5138898491859436,
      "learning_rate": 3.787e-05,
      "loss": 0.0027,
      "step": 36390
    },
    {
      "epoch": 1.9413333333333334,
      "grad_norm": 0.028601858764886856,
      "learning_rate": 3.786666666666667e-05,
      "loss": 0.0035,
      "step": 36400
    },
    {
      "epoch": 1.9418666666666666,
      "grad_norm": 0.028620855882763863,
      "learning_rate": 3.7863333333333334e-05,
      "loss": 0.0028,
      "step": 36410
    },
    {
      "epoch": 1.9424000000000001,
      "grad_norm": 0.22844120860099792,
      "learning_rate": 3.786e-05,
      "loss": 0.0026,
      "step": 36420
    },
    {
      "epoch": 1.9429333333333334,
      "grad_norm": 0.05718723312020302,
      "learning_rate": 3.7856666666666666e-05,
      "loss": 0.0026,
      "step": 36430
    },
    {
      "epoch": 1.9434666666666667,
      "grad_norm": 2.554814100265503,
      "learning_rate": 3.785333333333333e-05,
      "loss": 0.0039,
      "step": 36440
    },
    {
      "epoch": 1.944,
      "grad_norm": 0.1141873449087143,
      "learning_rate": 3.7850000000000005e-05,
      "loss": 0.0046,
      "step": 36450
    },
    {
      "epoch": 1.9445333333333332,
      "grad_norm": 0.08567863702774048,
      "learning_rate": 3.784666666666667e-05,
      "loss": 0.0027,
      "step": 36460
    },
    {
      "epoch": 1.9450666666666667,
      "grad_norm": 0.11421086639165878,
      "learning_rate": 3.784333333333334e-05,
      "loss": 0.0043,
      "step": 36470
    },
    {
      "epoch": 1.9456,
      "grad_norm": 0.08570031076669693,
      "learning_rate": 3.7840000000000004e-05,
      "loss": 0.0026,
      "step": 36480
    },
    {
      "epoch": 1.9461333333333335,
      "grad_norm": 0.2855844795703888,
      "learning_rate": 3.783666666666667e-05,
      "loss": 0.0039,
      "step": 36490
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 0.08568182587623596,
      "learning_rate": 3.7833333333333336e-05,
      "loss": 0.0026,
      "step": 36500
    },
    {
      "epoch": 1.9472,
      "grad_norm": 0.028672024607658386,
      "learning_rate": 3.783e-05,
      "loss": 0.0032,
      "step": 36510
    },
    {
      "epoch": 1.9477333333333333,
      "grad_norm": 0.25699126720428467,
      "learning_rate": 3.782666666666667e-05,
      "loss": 0.0025,
      "step": 36520
    },
    {
      "epoch": 1.9482666666666666,
      "grad_norm": 0.17130881547927856,
      "learning_rate": 3.7823333333333334e-05,
      "loss": 0.0025,
      "step": 36530
    },
    {
      "epoch": 1.9487999999999999,
      "grad_norm": 0.002259243745356798,
      "learning_rate": 3.782e-05,
      "loss": 0.0038,
      "step": 36540
    },
    {
      "epoch": 1.9493333333333334,
      "grad_norm": 0.028551075607538223,
      "learning_rate": 3.7816666666666667e-05,
      "loss": 0.0022,
      "step": 36550
    },
    {
      "epoch": 1.9498666666666666,
      "grad_norm": 0.11427830904722214,
      "learning_rate": 3.781333333333333e-05,
      "loss": 0.0025,
      "step": 36560
    },
    {
      "epoch": 1.9504000000000001,
      "grad_norm": 0.05724356696009636,
      "learning_rate": 3.781e-05,
      "loss": 0.0042,
      "step": 36570
    },
    {
      "epoch": 1.9509333333333334,
      "grad_norm": 0.19983920454978943,
      "learning_rate": 3.7806666666666665e-05,
      "loss": 0.0028,
      "step": 36580
    },
    {
      "epoch": 1.9514666666666667,
      "grad_norm": 0.00044470748980529606,
      "learning_rate": 3.780333333333334e-05,
      "loss": 0.0034,
      "step": 36590
    },
    {
      "epoch": 1.952,
      "grad_norm": 1.162526249885559,
      "learning_rate": 3.7800000000000004e-05,
      "loss": 0.0033,
      "step": 36600
    },
    {
      "epoch": 1.9525333333333332,
      "grad_norm": 0.1712866872549057,
      "learning_rate": 3.779666666666667e-05,
      "loss": 0.003,
      "step": 36610
    },
    {
      "epoch": 1.9530666666666665,
      "grad_norm": 0.40400031208992004,
      "learning_rate": 3.7793333333333336e-05,
      "loss": 0.0038,
      "step": 36620
    },
    {
      "epoch": 1.9536,
      "grad_norm": 0.1712683141231537,
      "learning_rate": 3.779e-05,
      "loss": 0.0022,
      "step": 36630
    },
    {
      "epoch": 1.9541333333333335,
      "grad_norm": 0.05711556226015091,
      "learning_rate": 3.778666666666667e-05,
      "loss": 0.0048,
      "step": 36640
    },
    {
      "epoch": 1.9546666666666668,
      "grad_norm": 0.08564054220914841,
      "learning_rate": 3.7783333333333335e-05,
      "loss": 0.0025,
      "step": 36650
    },
    {
      "epoch": 1.9552,
      "grad_norm": 0.6566303372383118,
      "learning_rate": 3.778000000000001e-05,
      "loss": 0.003,
      "step": 36660
    },
    {
      "epoch": 1.9557333333333333,
      "grad_norm": 0.28543245792388916,
      "learning_rate": 3.777666666666667e-05,
      "loss": 0.003,
      "step": 36670
    },
    {
      "epoch": 1.9562666666666666,
      "grad_norm": 0.11435317248106003,
      "learning_rate": 3.777333333333333e-05,
      "loss": 0.0025,
      "step": 36680
    },
    {
      "epoch": 1.9567999999999999,
      "grad_norm": 0.17328506708145142,
      "learning_rate": 3.777e-05,
      "loss": 0.0033,
      "step": 36690
    },
    {
      "epoch": 1.9573333333333334,
      "grad_norm": 2.2131507396698,
      "learning_rate": 3.7766666666666665e-05,
      "loss": 0.0047,
      "step": 36700
    },
    {
      "epoch": 1.9578666666666666,
      "grad_norm": 0.08747490495443344,
      "learning_rate": 3.776333333333333e-05,
      "loss": 0.0045,
      "step": 36710
    },
    {
      "epoch": 1.9584000000000001,
      "grad_norm": 0.11421128362417221,
      "learning_rate": 3.776e-05,
      "loss": 0.0036,
      "step": 36720
    },
    {
      "epoch": 1.9589333333333334,
      "grad_norm": 0.0030390953179448843,
      "learning_rate": 3.775666666666667e-05,
      "loss": 0.0049,
      "step": 36730
    },
    {
      "epoch": 1.9594666666666667,
      "grad_norm": 0.05713186785578728,
      "learning_rate": 3.775333333333334e-05,
      "loss": 0.0038,
      "step": 36740
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.456672340631485,
      "learning_rate": 3.775e-05,
      "loss": 0.0043,
      "step": 36750
    },
    {
      "epoch": 1.9605333333333332,
      "grad_norm": 4.3694748878479,
      "learning_rate": 3.774666666666667e-05,
      "loss": 0.0046,
      "step": 36760
    },
    {
      "epoch": 1.9610666666666665,
      "grad_norm": 0.17123770713806152,
      "learning_rate": 3.7743333333333335e-05,
      "loss": 0.0031,
      "step": 36770
    },
    {
      "epoch": 1.9616,
      "grad_norm": 0.19976989924907684,
      "learning_rate": 3.774e-05,
      "loss": 0.0031,
      "step": 36780
    },
    {
      "epoch": 1.9621333333333333,
      "grad_norm": 0.11415774375200272,
      "learning_rate": 3.773666666666667e-05,
      "loss": 0.0049,
      "step": 36790
    },
    {
      "epoch": 1.9626666666666668,
      "grad_norm": 0.028592457994818687,
      "learning_rate": 3.773333333333334e-05,
      "loss": 0.0053,
      "step": 36800
    },
    {
      "epoch": 1.9632,
      "grad_norm": 0.1712401658296585,
      "learning_rate": 3.7730000000000006e-05,
      "loss": 0.0049,
      "step": 36810
    },
    {
      "epoch": 1.9637333333333333,
      "grad_norm": 0.6564072966575623,
      "learning_rate": 3.7726666666666666e-05,
      "loss": 0.0025,
      "step": 36820
    },
    {
      "epoch": 1.9642666666666666,
      "grad_norm": 0.1427149772644043,
      "learning_rate": 3.772333333333333e-05,
      "loss": 0.0024,
      "step": 36830
    },
    {
      "epoch": 1.9647999999999999,
      "grad_norm": 0.14270223677158356,
      "learning_rate": 3.772e-05,
      "loss": 0.0021,
      "step": 36840
    },
    {
      "epoch": 1.9653333333333334,
      "grad_norm": 0.28538140654563904,
      "learning_rate": 3.7716666666666664e-05,
      "loss": 0.0026,
      "step": 36850
    },
    {
      "epoch": 1.9658666666666667,
      "grad_norm": 0.08562348037958145,
      "learning_rate": 3.771333333333334e-05,
      "loss": 0.0031,
      "step": 36860
    },
    {
      "epoch": 1.9664000000000001,
      "grad_norm": 0.42074695229530334,
      "learning_rate": 3.771e-05,
      "loss": 0.003,
      "step": 36870
    },
    {
      "epoch": 1.9669333333333334,
      "grad_norm": 0.11416575312614441,
      "learning_rate": 3.770666666666667e-05,
      "loss": 0.0028,
      "step": 36880
    },
    {
      "epoch": 1.9674666666666667,
      "grad_norm": 0.5137563943862915,
      "learning_rate": 3.7703333333333335e-05,
      "loss": 0.0034,
      "step": 36890
    },
    {
      "epoch": 1.968,
      "grad_norm": 0.5422306656837463,
      "learning_rate": 3.77e-05,
      "loss": 0.0042,
      "step": 36900
    },
    {
      "epoch": 1.9685333333333332,
      "grad_norm": 0.3424697518348694,
      "learning_rate": 3.769666666666667e-05,
      "loss": 0.0024,
      "step": 36910
    },
    {
      "epoch": 1.9690666666666665,
      "grad_norm": 0.1712382435798645,
      "learning_rate": 3.7693333333333334e-05,
      "loss": 0.0028,
      "step": 36920
    },
    {
      "epoch": 1.9696,
      "grad_norm": 0.17123831808567047,
      "learning_rate": 3.769e-05,
      "loss": 0.0018,
      "step": 36930
    },
    {
      "epoch": 1.9701333333333333,
      "grad_norm": 0.05708824470639229,
      "learning_rate": 3.768666666666667e-05,
      "loss": 0.0031,
      "step": 36940
    },
    {
      "epoch": 1.9706666666666668,
      "grad_norm": 0.1427190601825714,
      "learning_rate": 3.768333333333334e-05,
      "loss": 0.0027,
      "step": 36950
    },
    {
      "epoch": 1.9712,
      "grad_norm": 0.22834832966327667,
      "learning_rate": 3.7680000000000005e-05,
      "loss": 0.0044,
      "step": 36960
    },
    {
      "epoch": 1.9717333333333333,
      "grad_norm": 0.02885497361421585,
      "learning_rate": 3.767666666666667e-05,
      "loss": 0.0042,
      "step": 36970
    },
    {
      "epoch": 1.9722666666666666,
      "grad_norm": 0.42811161279678345,
      "learning_rate": 3.767333333333333e-05,
      "loss": 0.0033,
      "step": 36980
    },
    {
      "epoch": 1.9727999999999999,
      "grad_norm": 0.5707285404205322,
      "learning_rate": 3.767e-05,
      "loss": 0.0032,
      "step": 36990
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 0.17123384773731232,
      "learning_rate": 3.766666666666667e-05,
      "loss": 0.0041,
      "step": 37000
    },
    {
      "epoch": 1.9738666666666667,
      "grad_norm": 0.2282869666814804,
      "learning_rate": 3.7663333333333336e-05,
      "loss": 0.002,
      "step": 37010
    },
    {
      "epoch": 1.9744000000000002,
      "grad_norm": 0.057074543088674545,
      "learning_rate": 3.766e-05,
      "loss": 0.0032,
      "step": 37020
    },
    {
      "epoch": 1.9749333333333334,
      "grad_norm": 0.48510071635246277,
      "learning_rate": 3.765666666666667e-05,
      "loss": 0.0031,
      "step": 37030
    },
    {
      "epoch": 1.9754666666666667,
      "grad_norm": 0.19978150725364685,
      "learning_rate": 3.7653333333333334e-05,
      "loss": 0.0032,
      "step": 37040
    },
    {
      "epoch": 1.976,
      "grad_norm": 0.22831350564956665,
      "learning_rate": 3.765e-05,
      "loss": 0.0024,
      "step": 37050
    },
    {
      "epoch": 1.9765333333333333,
      "grad_norm": 0.2283090054988861,
      "learning_rate": 3.7646666666666666e-05,
      "loss": 0.0022,
      "step": 37060
    },
    {
      "epoch": 1.9770666666666665,
      "grad_norm": 0.02854534424841404,
      "learning_rate": 3.764333333333333e-05,
      "loss": 0.004,
      "step": 37070
    },
    {
      "epoch": 1.9776,
      "grad_norm": 0.05712902173399925,
      "learning_rate": 3.7640000000000006e-05,
      "loss": 0.0023,
      "step": 37080
    },
    {
      "epoch": 1.9781333333333333,
      "grad_norm": 0.313942015171051,
      "learning_rate": 3.763666666666667e-05,
      "loss": 0.0027,
      "step": 37090
    },
    {
      "epoch": 1.9786666666666668,
      "grad_norm": 0.17121565341949463,
      "learning_rate": 3.763333333333334e-05,
      "loss": 0.003,
      "step": 37100
    },
    {
      "epoch": 1.9792,
      "grad_norm": 0.028539815917611122,
      "learning_rate": 3.7630000000000004e-05,
      "loss": 0.003,
      "step": 37110
    },
    {
      "epoch": 1.9797333333333333,
      "grad_norm": 0.0570722296833992,
      "learning_rate": 3.762666666666667e-05,
      "loss": 0.0027,
      "step": 37120
    },
    {
      "epoch": 1.9802666666666666,
      "grad_norm": 0.05708615854382515,
      "learning_rate": 3.762333333333333e-05,
      "loss": 0.0044,
      "step": 37130
    },
    {
      "epoch": 1.9808,
      "grad_norm": 0.1141434982419014,
      "learning_rate": 3.762e-05,
      "loss": 0.0048,
      "step": 37140
    },
    {
      "epoch": 1.9813333333333332,
      "grad_norm": 0.5191099047660828,
      "learning_rate": 3.761666666666667e-05,
      "loss": 0.0028,
      "step": 37150
    },
    {
      "epoch": 1.9818666666666667,
      "grad_norm": 0.571158230304718,
      "learning_rate": 3.7613333333333335e-05,
      "loss": 0.0028,
      "step": 37160
    },
    {
      "epoch": 1.9824000000000002,
      "grad_norm": 0.17122484743595123,
      "learning_rate": 3.761e-05,
      "loss": 0.0025,
      "step": 37170
    },
    {
      "epoch": 1.9829333333333334,
      "grad_norm": 0.1997421234846115,
      "learning_rate": 3.760666666666667e-05,
      "loss": 0.0024,
      "step": 37180
    },
    {
      "epoch": 1.9834666666666667,
      "grad_norm": 0.1712077409029007,
      "learning_rate": 3.760333333333333e-05,
      "loss": 0.0033,
      "step": 37190
    },
    {
      "epoch": 1.984,
      "grad_norm": 0.028536951169371605,
      "learning_rate": 3.76e-05,
      "loss": 0.004,
      "step": 37200
    },
    {
      "epoch": 1.9845333333333333,
      "grad_norm": 0.3994791507720947,
      "learning_rate": 3.759666666666667e-05,
      "loss": 0.003,
      "step": 37210
    },
    {
      "epoch": 1.9850666666666665,
      "grad_norm": 0.11566223949193954,
      "learning_rate": 3.759333333333334e-05,
      "loss": 0.0032,
      "step": 37220
    },
    {
      "epoch": 1.9856,
      "grad_norm": 0.11414982378482819,
      "learning_rate": 3.7590000000000004e-05,
      "loss": 0.0051,
      "step": 37230
    },
    {
      "epoch": 1.9861333333333333,
      "grad_norm": 0.028582805767655373,
      "learning_rate": 3.758666666666667e-05,
      "loss": 0.003,
      "step": 37240
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 0.028539778664708138,
      "learning_rate": 3.7583333333333337e-05,
      "loss": 0.0025,
      "step": 37250
    },
    {
      "epoch": 1.9872,
      "grad_norm": 0.057066213339567184,
      "learning_rate": 3.758e-05,
      "loss": 0.0025,
      "step": 37260
    },
    {
      "epoch": 1.9877333333333334,
      "grad_norm": 0.0007939683273434639,
      "learning_rate": 3.757666666666667e-05,
      "loss": 0.0035,
      "step": 37270
    },
    {
      "epoch": 1.9882666666666666,
      "grad_norm": 0.028569381684064865,
      "learning_rate": 3.7573333333333335e-05,
      "loss": 0.0019,
      "step": 37280
    },
    {
      "epoch": 1.9888,
      "grad_norm": 0.2853311002254486,
      "learning_rate": 3.757e-05,
      "loss": 0.0034,
      "step": 37290
    },
    {
      "epoch": 1.9893333333333332,
      "grad_norm": 0.19978909194469452,
      "learning_rate": 3.756666666666667e-05,
      "loss": 0.005,
      "step": 37300
    },
    {
      "epoch": 1.9898666666666667,
      "grad_norm": 0.15619854629039764,
      "learning_rate": 3.7563333333333333e-05,
      "loss": 0.0048,
      "step": 37310
    },
    {
      "epoch": 1.9904,
      "grad_norm": 0.37106603384017944,
      "learning_rate": 3.756e-05,
      "loss": 0.0035,
      "step": 37320
    },
    {
      "epoch": 1.9909333333333334,
      "grad_norm": 0.19974006712436676,
      "learning_rate": 3.7556666666666666e-05,
      "loss": 0.0038,
      "step": 37330
    },
    {
      "epoch": 1.9914666666666667,
      "grad_norm": 0.3423818349838257,
      "learning_rate": 3.755333333333333e-05,
      "loss": 0.0028,
      "step": 37340
    },
    {
      "epoch": 1.992,
      "grad_norm": 0.028534047305583954,
      "learning_rate": 3.7550000000000005e-05,
      "loss": 0.0018,
      "step": 37350
    },
    {
      "epoch": 1.9925333333333333,
      "grad_norm": 0.19974957406520844,
      "learning_rate": 3.754666666666667e-05,
      "loss": 0.0021,
      "step": 37360
    },
    {
      "epoch": 1.9930666666666665,
      "grad_norm": 0.07995573431253433,
      "learning_rate": 3.754333333333334e-05,
      "loss": 0.0026,
      "step": 37370
    },
    {
      "epoch": 1.9936,
      "grad_norm": 0.1426617056131363,
      "learning_rate": 3.754e-05,
      "loss": 0.0029,
      "step": 37380
    },
    {
      "epoch": 1.9941333333333333,
      "grad_norm": 0.1711985468864441,
      "learning_rate": 3.753666666666667e-05,
      "loss": 0.0027,
      "step": 37390
    },
    {
      "epoch": 1.9946666666666668,
      "grad_norm": 0.14265510439872742,
      "learning_rate": 3.7533333333333335e-05,
      "loss": 0.0023,
      "step": 37400
    },
    {
      "epoch": 1.9952,
      "grad_norm": 0.1141292005777359,
      "learning_rate": 3.753e-05,
      "loss": 0.0029,
      "step": 37410
    },
    {
      "epoch": 1.9957333333333334,
      "grad_norm": 0.14322379231452942,
      "learning_rate": 3.7526666666666674e-05,
      "loss": 0.0036,
      "step": 37420
    },
    {
      "epoch": 1.9962666666666666,
      "grad_norm": 0.14265678822994232,
      "learning_rate": 3.7523333333333334e-05,
      "loss": 0.0031,
      "step": 37430
    },
    {
      "epoch": 1.9968,
      "grad_norm": 0.2282477617263794,
      "learning_rate": 3.752e-05,
      "loss": 0.003,
      "step": 37440
    },
    {
      "epoch": 1.9973333333333332,
      "grad_norm": 0.3425213396549225,
      "learning_rate": 3.7516666666666666e-05,
      "loss": 0.0036,
      "step": 37450
    },
    {
      "epoch": 1.9978666666666667,
      "grad_norm": 0.08568675071001053,
      "learning_rate": 3.751333333333333e-05,
      "loss": 0.0033,
      "step": 37460
    },
    {
      "epoch": 1.9984,
      "grad_norm": 0.14265227317810059,
      "learning_rate": 3.751e-05,
      "loss": 0.0032,
      "step": 37470
    },
    {
      "epoch": 1.9989333333333335,
      "grad_norm": 0.08559384942054749,
      "learning_rate": 3.7506666666666664e-05,
      "loss": 0.0027,
      "step": 37480
    },
    {
      "epoch": 1.9994666666666667,
      "grad_norm": 0.02853284403681755,
      "learning_rate": 3.750333333333334e-05,
      "loss": 0.0031,
      "step": 37490
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.000321144238114357,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 0.0028,
      "step": 37500
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.0032997415401041508,
      "eval_runtime": 169.1196,
      "eval_samples_per_second": 1478.244,
      "eval_steps_per_second": 36.956,
      "step": 37500
    },
    {
      "epoch": 2.0005333333333333,
      "grad_norm": 0.08559174090623856,
      "learning_rate": 3.749666666666667e-05,
      "loss": 0.0025,
      "step": 37510
    },
    {
      "epoch": 2.0010666666666665,
      "grad_norm": 0.05706758052110672,
      "learning_rate": 3.7493333333333336e-05,
      "loss": 0.0033,
      "step": 37520
    },
    {
      "epoch": 2.0016,
      "grad_norm": 0.02853216417133808,
      "learning_rate": 3.749e-05,
      "loss": 0.0035,
      "step": 37530
    },
    {
      "epoch": 2.0021333333333335,
      "grad_norm": 0.45647749304771423,
      "learning_rate": 3.748666666666667e-05,
      "loss": 0.0046,
      "step": 37540
    },
    {
      "epoch": 2.002666666666667,
      "grad_norm": 3.682918071746826,
      "learning_rate": 3.7483333333333334e-05,
      "loss": 0.0042,
      "step": 37550
    },
    {
      "epoch": 2.0032,
      "grad_norm": 0.2853260338306427,
      "learning_rate": 3.748000000000001e-05,
      "loss": 0.0041,
      "step": 37560
    },
    {
      "epoch": 2.0037333333333334,
      "grad_norm": 0.05708394944667816,
      "learning_rate": 3.747666666666667e-05,
      "loss": 0.0031,
      "step": 37570
    },
    {
      "epoch": 2.0042666666666666,
      "grad_norm": 0.313849538564682,
      "learning_rate": 3.747333333333333e-05,
      "loss": 0.0028,
      "step": 37580
    },
    {
      "epoch": 2.0048,
      "grad_norm": 0.02860458381474018,
      "learning_rate": 3.747e-05,
      "loss": 0.0042,
      "step": 37590
    },
    {
      "epoch": 2.005333333333333,
      "grad_norm": 0.39947769045829773,
      "learning_rate": 3.7466666666666665e-05,
      "loss": 0.0024,
      "step": 37600
    },
    {
      "epoch": 2.0058666666666665,
      "grad_norm": 0.028615768998861313,
      "learning_rate": 3.746333333333333e-05,
      "loss": 0.0035,
      "step": 37610
    },
    {
      "epoch": 2.0064,
      "grad_norm": 0.39945852756500244,
      "learning_rate": 3.7460000000000004e-05,
      "loss": 0.0029,
      "step": 37620
    },
    {
      "epoch": 2.0069333333333335,
      "grad_norm": 0.3993757665157318,
      "learning_rate": 3.745666666666667e-05,
      "loss": 0.0027,
      "step": 37630
    },
    {
      "epoch": 2.0074666666666667,
      "grad_norm": 0.2282673418521881,
      "learning_rate": 3.7453333333333336e-05,
      "loss": 0.0032,
      "step": 37640
    },
    {
      "epoch": 2.008,
      "grad_norm": 2.9086391925811768,
      "learning_rate": 3.745e-05,
      "loss": 0.0026,
      "step": 37650
    },
    {
      "epoch": 2.0085333333333333,
      "grad_norm": 0.08562622219324112,
      "learning_rate": 3.744666666666667e-05,
      "loss": 0.0022,
      "step": 37660
    },
    {
      "epoch": 2.0090666666666666,
      "grad_norm": 0.0856415331363678,
      "learning_rate": 3.7443333333333335e-05,
      "loss": 0.0025,
      "step": 37670
    },
    {
      "epoch": 2.0096,
      "grad_norm": 0.22824373841285706,
      "learning_rate": 3.744e-05,
      "loss": 0.0037,
      "step": 37680
    },
    {
      "epoch": 2.0101333333333335,
      "grad_norm": 0.028536593541502953,
      "learning_rate": 3.743666666666667e-05,
      "loss": 0.0028,
      "step": 37690
    },
    {
      "epoch": 2.010666666666667,
      "grad_norm": 0.1712132841348648,
      "learning_rate": 3.743333333333334e-05,
      "loss": 0.003,
      "step": 37700
    },
    {
      "epoch": 2.0112,
      "grad_norm": 0.09038876742124557,
      "learning_rate": 3.7430000000000006e-05,
      "loss": 0.0045,
      "step": 37710
    },
    {
      "epoch": 2.0117333333333334,
      "grad_norm": 0.05729932337999344,
      "learning_rate": 3.742666666666667e-05,
      "loss": 0.0026,
      "step": 37720
    },
    {
      "epoch": 2.0122666666666666,
      "grad_norm": 0.42799246311187744,
      "learning_rate": 3.742333333333333e-05,
      "loss": 0.0035,
      "step": 37730
    },
    {
      "epoch": 2.0128,
      "grad_norm": 0.05706394091248512,
      "learning_rate": 3.742e-05,
      "loss": 0.0025,
      "step": 37740
    },
    {
      "epoch": 2.013333333333333,
      "grad_norm": 0.08567440509796143,
      "learning_rate": 3.7416666666666664e-05,
      "loss": 0.0035,
      "step": 37750
    },
    {
      "epoch": 2.0138666666666665,
      "grad_norm": 0.14281593263149261,
      "learning_rate": 3.7413333333333337e-05,
      "loss": 0.0029,
      "step": 37760
    },
    {
      "epoch": 2.0144,
      "grad_norm": 0.37085703015327454,
      "learning_rate": 3.741e-05,
      "loss": 0.0024,
      "step": 37770
    },
    {
      "epoch": 2.0149333333333335,
      "grad_norm": 0.28529974818229675,
      "learning_rate": 3.740666666666667e-05,
      "loss": 0.0031,
      "step": 37780
    },
    {
      "epoch": 2.0154666666666667,
      "grad_norm": 0.19970159232616425,
      "learning_rate": 3.7403333333333335e-05,
      "loss": 0.0031,
      "step": 37790
    },
    {
      "epoch": 2.016,
      "grad_norm": 0.17117325961589813,
      "learning_rate": 3.74e-05,
      "loss": 0.0022,
      "step": 37800
    },
    {
      "epoch": 2.0165333333333333,
      "grad_norm": 0.08559060841798782,
      "learning_rate": 3.739666666666667e-05,
      "loss": 0.0028,
      "step": 37810
    },
    {
      "epoch": 2.0170666666666666,
      "grad_norm": 0.5135212540626526,
      "learning_rate": 3.739333333333333e-05,
      "loss": 0.0035,
      "step": 37820
    },
    {
      "epoch": 2.0176,
      "grad_norm": 0.057126205414533615,
      "learning_rate": 3.739e-05,
      "loss": 0.0038,
      "step": 37830
    },
    {
      "epoch": 2.018133333333333,
      "grad_norm": 0.17158827185630798,
      "learning_rate": 3.738666666666667e-05,
      "loss": 0.002,
      "step": 37840
    },
    {
      "epoch": 2.018666666666667,
      "grad_norm": 0.31381434202194214,
      "learning_rate": 3.738333333333334e-05,
      "loss": 0.0041,
      "step": 37850
    },
    {
      "epoch": 2.0192,
      "grad_norm": 0.1144518181681633,
      "learning_rate": 3.7380000000000005e-05,
      "loss": 0.0051,
      "step": 37860
    },
    {
      "epoch": 2.0197333333333334,
      "grad_norm": 0.22827331721782684,
      "learning_rate": 3.737666666666667e-05,
      "loss": 0.0035,
      "step": 37870
    },
    {
      "epoch": 2.0202666666666667,
      "grad_norm": 1.2199172973632812,
      "learning_rate": 3.737333333333333e-05,
      "loss": 0.0049,
      "step": 37880
    },
    {
      "epoch": 2.0208,
      "grad_norm": 0.08557834476232529,
      "learning_rate": 3.7369999999999996e-05,
      "loss": 0.0028,
      "step": 37890
    },
    {
      "epoch": 2.021333333333333,
      "grad_norm": 0.00022004861966706812,
      "learning_rate": 3.736666666666667e-05,
      "loss": 0.0021,
      "step": 37900
    },
    {
      "epoch": 2.0218666666666665,
      "grad_norm": 0.0010655989171937108,
      "learning_rate": 3.7363333333333335e-05,
      "loss": 0.0039,
      "step": 37910
    },
    {
      "epoch": 2.0224,
      "grad_norm": 0.0570480115711689,
      "learning_rate": 3.736e-05,
      "loss": 0.0029,
      "step": 37920
    },
    {
      "epoch": 2.0229333333333335,
      "grad_norm": 3.2399699687957764,
      "learning_rate": 3.735666666666667e-05,
      "loss": 0.0076,
      "step": 37930
    },
    {
      "epoch": 2.0234666666666667,
      "grad_norm": 0.0571177676320076,
      "learning_rate": 3.7353333333333334e-05,
      "loss": 0.0037,
      "step": 37940
    },
    {
      "epoch": 2.024,
      "grad_norm": 0.1711331158876419,
      "learning_rate": 3.735e-05,
      "loss": 0.0025,
      "step": 37950
    },
    {
      "epoch": 2.0245333333333333,
      "grad_norm": 0.02852204442024231,
      "learning_rate": 3.7346666666666666e-05,
      "loss": 0.0031,
      "step": 37960
    },
    {
      "epoch": 2.0250666666666666,
      "grad_norm": 0.14261789619922638,
      "learning_rate": 3.734333333333334e-05,
      "loss": 0.0019,
      "step": 37970
    },
    {
      "epoch": 2.0256,
      "grad_norm": 0.39936313033103943,
      "learning_rate": 3.7340000000000005e-05,
      "loss": 0.0034,
      "step": 37980
    },
    {
      "epoch": 2.026133333333333,
      "grad_norm": 0.1996508687734604,
      "learning_rate": 3.733666666666667e-05,
      "loss": 0.0031,
      "step": 37990
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 0.05704682692885399,
      "learning_rate": 3.733333333333334e-05,
      "loss": 0.0043,
      "step": 38000
    },
    {
      "epoch": 2.0272,
      "grad_norm": 0.08600860089063644,
      "learning_rate": 3.7330000000000003e-05,
      "loss": 0.0031,
      "step": 38010
    },
    {
      "epoch": 2.0277333333333334,
      "grad_norm": 0.1996651440858841,
      "learning_rate": 3.732666666666667e-05,
      "loss": 0.0059,
      "step": 38020
    },
    {
      "epoch": 2.0282666666666667,
      "grad_norm": 0.19968175888061523,
      "learning_rate": 3.7323333333333336e-05,
      "loss": 0.0038,
      "step": 38030
    },
    {
      "epoch": 2.0288,
      "grad_norm": 0.1996673345565796,
      "learning_rate": 3.732e-05,
      "loss": 0.0031,
      "step": 38040
    },
    {
      "epoch": 2.029333333333333,
      "grad_norm": 0.22819510102272034,
      "learning_rate": 3.731666666666667e-05,
      "loss": 0.0034,
      "step": 38050
    },
    {
      "epoch": 2.0298666666666665,
      "grad_norm": 0.3992921710014343,
      "learning_rate": 3.7313333333333334e-05,
      "loss": 0.0028,
      "step": 38060
    },
    {
      "epoch": 2.0304,
      "grad_norm": 0.11408963799476624,
      "learning_rate": 3.731e-05,
      "loss": 0.0026,
      "step": 38070
    },
    {
      "epoch": 2.0309333333333335,
      "grad_norm": 0.11409156769514084,
      "learning_rate": 3.7306666666666666e-05,
      "loss": 0.0039,
      "step": 38080
    },
    {
      "epoch": 2.0314666666666668,
      "grad_norm": 0.08555919677019119,
      "learning_rate": 3.730333333333333e-05,
      "loss": 0.0025,
      "step": 38090
    },
    {
      "epoch": 2.032,
      "grad_norm": 1.025527000427246,
      "learning_rate": 3.73e-05,
      "loss": 0.0032,
      "step": 38100
    },
    {
      "epoch": 2.0325333333333333,
      "grad_norm": 0.028523346409201622,
      "learning_rate": 3.729666666666667e-05,
      "loss": 0.0045,
      "step": 38110
    },
    {
      "epoch": 2.0330666666666666,
      "grad_norm": 0.5704805850982666,
      "learning_rate": 3.729333333333334e-05,
      "loss": 0.0031,
      "step": 38120
    },
    {
      "epoch": 2.0336,
      "grad_norm": 0.1996772140264511,
      "learning_rate": 3.7290000000000004e-05,
      "loss": 0.0034,
      "step": 38130
    },
    {
      "epoch": 2.034133333333333,
      "grad_norm": 0.08556585013866425,
      "learning_rate": 3.728666666666667e-05,
      "loss": 0.002,
      "step": 38140
    },
    {
      "epoch": 2.034666666666667,
      "grad_norm": 0.02852274291217327,
      "learning_rate": 3.7283333333333336e-05,
      "loss": 0.0034,
      "step": 38150
    },
    {
      "epoch": 2.0352,
      "grad_norm": 0.39931926131248474,
      "learning_rate": 3.728e-05,
      "loss": 0.0049,
      "step": 38160
    },
    {
      "epoch": 2.0357333333333334,
      "grad_norm": 0.11428727209568024,
      "learning_rate": 3.727666666666667e-05,
      "loss": 0.003,
      "step": 38170
    },
    {
      "epoch": 2.0362666666666667,
      "grad_norm": 0.028530912473797798,
      "learning_rate": 3.727333333333334e-05,
      "loss": 0.0024,
      "step": 38180
    },
    {
      "epoch": 2.0368,
      "grad_norm": 2.2014920711517334,
      "learning_rate": 3.727e-05,
      "loss": 0.0035,
      "step": 38190
    },
    {
      "epoch": 2.037333333333333,
      "grad_norm": 0.057039957493543625,
      "learning_rate": 3.726666666666667e-05,
      "loss": 0.0027,
      "step": 38200
    },
    {
      "epoch": 2.0378666666666665,
      "grad_norm": 0.570371150970459,
      "learning_rate": 3.726333333333333e-05,
      "loss": 0.0037,
      "step": 38210
    },
    {
      "epoch": 2.0384,
      "grad_norm": 0.14259102940559387,
      "learning_rate": 3.726e-05,
      "loss": 0.0036,
      "step": 38220
    },
    {
      "epoch": 2.0389333333333335,
      "grad_norm": 0.028530513867735863,
      "learning_rate": 3.7256666666666665e-05,
      "loss": 0.0033,
      "step": 38230
    },
    {
      "epoch": 2.0394666666666668,
      "grad_norm": 0.11410386860370636,
      "learning_rate": 3.725333333333333e-05,
      "loss": 0.0024,
      "step": 38240
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.31371229887008667,
      "learning_rate": 3.7250000000000004e-05,
      "loss": 0.0033,
      "step": 38250
    },
    {
      "epoch": 2.0405333333333333,
      "grad_norm": 0.3708173930644989,
      "learning_rate": 3.724666666666667e-05,
      "loss": 0.0027,
      "step": 38260
    },
    {
      "epoch": 2.0410666666666666,
      "grad_norm": 0.02852017991244793,
      "learning_rate": 3.7243333333333336e-05,
      "loss": 0.0019,
      "step": 38270
    },
    {
      "epoch": 2.0416,
      "grad_norm": 0.14261245727539062,
      "learning_rate": 3.724e-05,
      "loss": 0.0021,
      "step": 38280
    },
    {
      "epoch": 2.042133333333333,
      "grad_norm": 0.31379449367523193,
      "learning_rate": 3.723666666666667e-05,
      "loss": 0.0034,
      "step": 38290
    },
    {
      "epoch": 2.042666666666667,
      "grad_norm": 0.05704578012228012,
      "learning_rate": 3.7233333333333335e-05,
      "loss": 0.0028,
      "step": 38300
    },
    {
      "epoch": 2.0432,
      "grad_norm": 0.028521306812763214,
      "learning_rate": 3.723e-05,
      "loss": 0.0032,
      "step": 38310
    },
    {
      "epoch": 2.0437333333333334,
      "grad_norm": 0.25668153166770935,
      "learning_rate": 3.7226666666666674e-05,
      "loss": 0.0024,
      "step": 38320
    },
    {
      "epoch": 2.0442666666666667,
      "grad_norm": 0.2566690146923065,
      "learning_rate": 3.722333333333334e-05,
      "loss": 0.0032,
      "step": 38330
    },
    {
      "epoch": 2.0448,
      "grad_norm": 0.05704019218683243,
      "learning_rate": 3.722e-05,
      "loss": 0.0024,
      "step": 38340
    },
    {
      "epoch": 2.0453333333333332,
      "grad_norm": 0.3992432951927185,
      "learning_rate": 3.7216666666666666e-05,
      "loss": 0.0036,
      "step": 38350
    },
    {
      "epoch": 2.0458666666666665,
      "grad_norm": 0.45635390281677246,
      "learning_rate": 3.721333333333333e-05,
      "loss": 0.0037,
      "step": 38360
    },
    {
      "epoch": 2.0464,
      "grad_norm": 0.08555185049772263,
      "learning_rate": 3.721e-05,
      "loss": 0.0015,
      "step": 38370
    },
    {
      "epoch": 2.0469333333333335,
      "grad_norm": 0.3422255516052246,
      "learning_rate": 3.720666666666667e-05,
      "loss": 0.0031,
      "step": 38380
    },
    {
      "epoch": 2.0474666666666668,
      "grad_norm": 0.19962544739246368,
      "learning_rate": 3.720333333333334e-05,
      "loss": 0.0035,
      "step": 38390
    },
    {
      "epoch": 2.048,
      "grad_norm": 0.00031897122971713543,
      "learning_rate": 3.72e-05,
      "loss": 0.0027,
      "step": 38400
    },
    {
      "epoch": 2.0485333333333333,
      "grad_norm": 0.5133605599403381,
      "learning_rate": 3.719666666666667e-05,
      "loss": 0.0025,
      "step": 38410
    },
    {
      "epoch": 2.0490666666666666,
      "grad_norm": 0.11407283693552017,
      "learning_rate": 3.7193333333333335e-05,
      "loss": 0.0031,
      "step": 38420
    },
    {
      "epoch": 2.0496,
      "grad_norm": 0.05704237520694733,
      "learning_rate": 3.719e-05,
      "loss": 0.0027,
      "step": 38430
    },
    {
      "epoch": 2.050133333333333,
      "grad_norm": 0.14260904490947723,
      "learning_rate": 3.718666666666667e-05,
      "loss": 0.005,
      "step": 38440
    },
    {
      "epoch": 2.050666666666667,
      "grad_norm": 0.22821155190467834,
      "learning_rate": 3.7183333333333334e-05,
      "loss": 0.0042,
      "step": 38450
    },
    {
      "epoch": 2.0512,
      "grad_norm": 0.6844983100891113,
      "learning_rate": 3.7180000000000007e-05,
      "loss": 0.003,
      "step": 38460
    },
    {
      "epoch": 2.0517333333333334,
      "grad_norm": 0.6274462938308716,
      "learning_rate": 3.717666666666667e-05,
      "loss": 0.0034,
      "step": 38470
    },
    {
      "epoch": 2.0522666666666667,
      "grad_norm": 0.08555523306131363,
      "learning_rate": 3.717333333333334e-05,
      "loss": 0.0027,
      "step": 38480
    },
    {
      "epoch": 2.0528,
      "grad_norm": 0.3421921730041504,
      "learning_rate": 3.717e-05,
      "loss": 0.0019,
      "step": 38490
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 0.08555937558412552,
      "learning_rate": 3.7166666666666664e-05,
      "loss": 0.0029,
      "step": 38500
    },
    {
      "epoch": 2.0538666666666665,
      "grad_norm": 0.05704817548394203,
      "learning_rate": 3.716333333333333e-05,
      "loss": 0.0025,
      "step": 38510
    },
    {
      "epoch": 2.0544,
      "grad_norm": 0.057052113115787506,
      "learning_rate": 3.716e-05,
      "loss": 0.003,
      "step": 38520
    },
    {
      "epoch": 2.0549333333333335,
      "grad_norm": 0.19965481758117676,
      "learning_rate": 3.715666666666667e-05,
      "loss": 0.0033,
      "step": 38530
    },
    {
      "epoch": 2.0554666666666668,
      "grad_norm": 0.02860722877085209,
      "learning_rate": 3.7153333333333336e-05,
      "loss": 0.0024,
      "step": 38540
    },
    {
      "epoch": 2.056,
      "grad_norm": 0.05704513564705849,
      "learning_rate": 3.715e-05,
      "loss": 0.002,
      "step": 38550
    },
    {
      "epoch": 2.0565333333333333,
      "grad_norm": 0.48474574089050293,
      "learning_rate": 3.714666666666667e-05,
      "loss": 0.0047,
      "step": 38560
    },
    {
      "epoch": 2.0570666666666666,
      "grad_norm": 0.19961927831172943,
      "learning_rate": 3.7143333333333334e-05,
      "loss": 0.0032,
      "step": 38570
    },
    {
      "epoch": 2.0576,
      "grad_norm": 0.11406383663415909,
      "learning_rate": 3.714e-05,
      "loss": 0.0028,
      "step": 38580
    },
    {
      "epoch": 2.058133333333333,
      "grad_norm": 0.11406346410512924,
      "learning_rate": 3.7136666666666666e-05,
      "loss": 0.0029,
      "step": 38590
    },
    {
      "epoch": 2.058666666666667,
      "grad_norm": 0.0006064178887754679,
      "learning_rate": 3.713333333333334e-05,
      "loss": 0.0024,
      "step": 38600
    },
    {
      "epoch": 2.0592,
      "grad_norm": 0.0006884023896418512,
      "learning_rate": 3.7130000000000005e-05,
      "loss": 0.0037,
      "step": 38610
    },
    {
      "epoch": 2.0597333333333334,
      "grad_norm": 0.513251543045044,
      "learning_rate": 3.712666666666667e-05,
      "loss": 0.0025,
      "step": 38620
    },
    {
      "epoch": 2.0602666666666667,
      "grad_norm": 0.3136786222457886,
      "learning_rate": 3.712333333333334e-05,
      "loss": 0.0019,
      "step": 38630
    },
    {
      "epoch": 2.0608,
      "grad_norm": 0.11409258842468262,
      "learning_rate": 3.712e-05,
      "loss": 0.0026,
      "step": 38640
    },
    {
      "epoch": 2.0613333333333332,
      "grad_norm": 0.057038068771362305,
      "learning_rate": 3.711666666666666e-05,
      "loss": 0.0031,
      "step": 38650
    },
    {
      "epoch": 2.0618666666666665,
      "grad_norm": 0.19965152442455292,
      "learning_rate": 3.7113333333333336e-05,
      "loss": 0.003,
      "step": 38660
    },
    {
      "epoch": 2.0624,
      "grad_norm": 0.028538992628455162,
      "learning_rate": 3.711e-05,
      "loss": 0.0023,
      "step": 38670
    },
    {
      "epoch": 2.0629333333333335,
      "grad_norm": 0.17108342051506042,
      "learning_rate": 3.710666666666667e-05,
      "loss": 0.0037,
      "step": 38680
    },
    {
      "epoch": 2.063466666666667,
      "grad_norm": 0.22811874747276306,
      "learning_rate": 3.7103333333333334e-05,
      "loss": 0.0027,
      "step": 38690
    },
    {
      "epoch": 2.064,
      "grad_norm": 0.11405118554830551,
      "learning_rate": 3.71e-05,
      "loss": 0.0041,
      "step": 38700
    },
    {
      "epoch": 2.0645333333333333,
      "grad_norm": 0.028528233990073204,
      "learning_rate": 3.709666666666667e-05,
      "loss": 0.0031,
      "step": 38710
    },
    {
      "epoch": 2.0650666666666666,
      "grad_norm": 0.17109450697898865,
      "learning_rate": 3.709333333333333e-05,
      "loss": 0.0054,
      "step": 38720
    },
    {
      "epoch": 2.0656,
      "grad_norm": 0.1996118575334549,
      "learning_rate": 3.7090000000000006e-05,
      "loss": 0.0048,
      "step": 38730
    },
    {
      "epoch": 2.066133333333333,
      "grad_norm": 0.42772793769836426,
      "learning_rate": 3.708666666666667e-05,
      "loss": 0.0018,
      "step": 38740
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 0.2002263367176056,
      "learning_rate": 3.708333333333334e-05,
      "loss": 0.0031,
      "step": 38750
    },
    {
      "epoch": 2.0672,
      "grad_norm": 0.25661569833755493,
      "learning_rate": 3.7080000000000004e-05,
      "loss": 0.003,
      "step": 38760
    },
    {
      "epoch": 2.0677333333333334,
      "grad_norm": 0.028515156358480453,
      "learning_rate": 3.707666666666667e-05,
      "loss": 0.0045,
      "step": 38770
    },
    {
      "epoch": 2.0682666666666667,
      "grad_norm": 0.057030100375413895,
      "learning_rate": 3.7073333333333336e-05,
      "loss": 0.0033,
      "step": 38780
    },
    {
      "epoch": 2.0688,
      "grad_norm": 0.14259280264377594,
      "learning_rate": 3.707e-05,
      "loss": 0.0026,
      "step": 38790
    },
    {
      "epoch": 2.0693333333333332,
      "grad_norm": 0.22813530266284943,
      "learning_rate": 3.706666666666667e-05,
      "loss": 0.0034,
      "step": 38800
    },
    {
      "epoch": 2.0698666666666665,
      "grad_norm": 0.4277198016643524,
      "learning_rate": 3.7063333333333335e-05,
      "loss": 0.0032,
      "step": 38810
    },
    {
      "epoch": 2.0704,
      "grad_norm": 0.05725136026740074,
      "learning_rate": 3.706e-05,
      "loss": 0.0027,
      "step": 38820
    },
    {
      "epoch": 2.0709333333333335,
      "grad_norm": 0.0016312490915879607,
      "learning_rate": 3.705666666666667e-05,
      "loss": 0.0038,
      "step": 38830
    },
    {
      "epoch": 2.071466666666667,
      "grad_norm": 0.17109403014183044,
      "learning_rate": 3.705333333333333e-05,
      "loss": 0.0024,
      "step": 38840
    },
    {
      "epoch": 2.072,
      "grad_norm": 0.3421599268913269,
      "learning_rate": 3.705e-05,
      "loss": 0.0042,
      "step": 38850
    },
    {
      "epoch": 2.0725333333333333,
      "grad_norm": 1.7951010677919044e-09,
      "learning_rate": 3.7046666666666665e-05,
      "loss": 0.0034,
      "step": 38860
    },
    {
      "epoch": 2.0730666666666666,
      "grad_norm": 0.05704240873456001,
      "learning_rate": 3.704333333333334e-05,
      "loss": 0.0026,
      "step": 38870
    },
    {
      "epoch": 2.0736,
      "grad_norm": 0.28515318036079407,
      "learning_rate": 3.7040000000000005e-05,
      "loss": 0.0018,
      "step": 38880
    },
    {
      "epoch": 2.074133333333333,
      "grad_norm": 0.17117846012115479,
      "learning_rate": 3.703666666666667e-05,
      "loss": 0.0034,
      "step": 38890
    },
    {
      "epoch": 2.074666666666667,
      "grad_norm": 0.22809647023677826,
      "learning_rate": 3.703333333333334e-05,
      "loss": 0.0039,
      "step": 38900
    },
    {
      "epoch": 2.0752,
      "grad_norm": 0.1425844430923462,
      "learning_rate": 3.703e-05,
      "loss": 0.0027,
      "step": 38910
    },
    {
      "epoch": 2.0757333333333334,
      "grad_norm": 0.4562176465988159,
      "learning_rate": 3.702666666666667e-05,
      "loss": 0.0032,
      "step": 38920
    },
    {
      "epoch": 2.0762666666666667,
      "grad_norm": 0.057031650096178055,
      "learning_rate": 3.7023333333333335e-05,
      "loss": 0.0016,
      "step": 38930
    },
    {
      "epoch": 2.0768,
      "grad_norm": 0.0855848491191864,
      "learning_rate": 3.702e-05,
      "loss": 0.0045,
      "step": 38940
    },
    {
      "epoch": 2.0773333333333333,
      "grad_norm": 0.31400805711746216,
      "learning_rate": 3.701666666666667e-05,
      "loss": 0.0023,
      "step": 38950
    },
    {
      "epoch": 2.0778666666666665,
      "grad_norm": 0.22927674651145935,
      "learning_rate": 3.7013333333333334e-05,
      "loss": 0.0021,
      "step": 38960
    },
    {
      "epoch": 2.0784,
      "grad_norm": 0.17106860876083374,
      "learning_rate": 3.701e-05,
      "loss": 0.0026,
      "step": 38970
    },
    {
      "epoch": 2.0789333333333335,
      "grad_norm": 0.02851088158786297,
      "learning_rate": 3.7006666666666666e-05,
      "loss": 0.0026,
      "step": 38980
    },
    {
      "epoch": 2.079466666666667,
      "grad_norm": 0.11449434608221054,
      "learning_rate": 3.700333333333333e-05,
      "loss": 0.0036,
      "step": 38990
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.17106589674949646,
      "learning_rate": 3.7e-05,
      "loss": 0.0032,
      "step": 39000
    },
    {
      "epoch": 2.0805333333333333,
      "grad_norm": 0.02851160801947117,
      "learning_rate": 3.699666666666667e-05,
      "loss": 0.0023,
      "step": 39010
    },
    {
      "epoch": 2.0810666666666666,
      "grad_norm": 0.1710747927427292,
      "learning_rate": 3.699333333333334e-05,
      "loss": 0.0028,
      "step": 39020
    },
    {
      "epoch": 2.0816,
      "grad_norm": 0.11404257267713547,
      "learning_rate": 3.699e-05,
      "loss": 0.003,
      "step": 39030
    },
    {
      "epoch": 2.082133333333333,
      "grad_norm": 0.19956675171852112,
      "learning_rate": 3.698666666666667e-05,
      "loss": 0.0027,
      "step": 39040
    },
    {
      "epoch": 2.0826666666666664,
      "grad_norm": 0.1995914876461029,
      "learning_rate": 3.6983333333333336e-05,
      "loss": 0.0038,
      "step": 39050
    },
    {
      "epoch": 2.0832,
      "grad_norm": 0.6272146105766296,
      "learning_rate": 3.698e-05,
      "loss": 0.002,
      "step": 39060
    },
    {
      "epoch": 2.0837333333333334,
      "grad_norm": 0.08556655049324036,
      "learning_rate": 3.697666666666667e-05,
      "loss": 0.0032,
      "step": 39070
    },
    {
      "epoch": 2.0842666666666667,
      "grad_norm": 0.753119707107544,
      "learning_rate": 3.697333333333334e-05,
      "loss": 0.0031,
      "step": 39080
    },
    {
      "epoch": 2.0848,
      "grad_norm": 0.11408895999193192,
      "learning_rate": 3.697e-05,
      "loss": 0.0024,
      "step": 39090
    },
    {
      "epoch": 2.0853333333333333,
      "grad_norm": 0.25656983256340027,
      "learning_rate": 3.6966666666666666e-05,
      "loss": 0.0037,
      "step": 39100
    },
    {
      "epoch": 2.0858666666666665,
      "grad_norm": 0.2287723869085312,
      "learning_rate": 3.696333333333333e-05,
      "loss": 0.0034,
      "step": 39110
    },
    {
      "epoch": 2.0864,
      "grad_norm": 0.05702629312872887,
      "learning_rate": 3.696e-05,
      "loss": 0.0027,
      "step": 39120
    },
    {
      "epoch": 2.0869333333333335,
      "grad_norm": 0.398369163274765,
      "learning_rate": 3.6956666666666665e-05,
      "loss": 0.0039,
      "step": 39130
    },
    {
      "epoch": 2.087466666666667,
      "grad_norm": 0.05705345422029495,
      "learning_rate": 3.695333333333334e-05,
      "loss": 0.0021,
      "step": 39140
    },
    {
      "epoch": 2.088,
      "grad_norm": 0.22760245203971863,
      "learning_rate": 3.6950000000000004e-05,
      "loss": 0.0031,
      "step": 39150
    },
    {
      "epoch": 2.0885333333333334,
      "grad_norm": 0.05689424276351929,
      "learning_rate": 3.694666666666667e-05,
      "loss": 0.0024,
      "step": 39160
    },
    {
      "epoch": 2.0890666666666666,
      "grad_norm": 0.05692446231842041,
      "learning_rate": 3.6943333333333336e-05,
      "loss": 0.0044,
      "step": 39170
    },
    {
      "epoch": 2.0896,
      "grad_norm": 0.25605374574661255,
      "learning_rate": 3.694e-05,
      "loss": 0.0032,
      "step": 39180
    },
    {
      "epoch": 2.090133333333333,
      "grad_norm": 0.05693749710917473,
      "learning_rate": 3.693666666666667e-05,
      "loss": 0.0032,
      "step": 39190
    },
    {
      "epoch": 2.0906666666666665,
      "grad_norm": 0.2844635844230652,
      "learning_rate": 3.6933333333333334e-05,
      "loss": 0.0049,
      "step": 39200
    },
    {
      "epoch": 2.0912,
      "grad_norm": 0.14232324063777924,
      "learning_rate": 3.693e-05,
      "loss": 0.003,
      "step": 39210
    },
    {
      "epoch": 2.0917333333333334,
      "grad_norm": 0.05693253502249718,
      "learning_rate": 3.6926666666666673e-05,
      "loss": 0.0032,
      "step": 39220
    },
    {
      "epoch": 2.0922666666666667,
      "grad_norm": 0.00030268909176811576,
      "learning_rate": 3.692333333333334e-05,
      "loss": 0.0033,
      "step": 39230
    },
    {
      "epoch": 2.0928,
      "grad_norm": 0.05689295008778572,
      "learning_rate": 3.692e-05,
      "loss": 0.004,
      "step": 39240
    },
    {
      "epoch": 2.0933333333333333,
      "grad_norm": 0.02870875969529152,
      "learning_rate": 3.6916666666666665e-05,
      "loss": 0.0031,
      "step": 39250
    },
    {
      "epoch": 2.0938666666666665,
      "grad_norm": 0.1422417014837265,
      "learning_rate": 3.691333333333333e-05,
      "loss": 0.0021,
      "step": 39260
    },
    {
      "epoch": 2.0944,
      "grad_norm": 0.22758467495441437,
      "learning_rate": 3.691e-05,
      "loss": 0.0033,
      "step": 39270
    },
    {
      "epoch": 2.0949333333333335,
      "grad_norm": 0.48525485396385193,
      "learning_rate": 3.690666666666667e-05,
      "loss": 0.0022,
      "step": 39280
    },
    {
      "epoch": 2.095466666666667,
      "grad_norm": 0.028739379718899727,
      "learning_rate": 3.6903333333333336e-05,
      "loss": 0.0027,
      "step": 39290
    },
    {
      "epoch": 2.096,
      "grad_norm": 0.3129047155380249,
      "learning_rate": 3.69e-05,
      "loss": 0.0027,
      "step": 39300
    },
    {
      "epoch": 2.0965333333333334,
      "grad_norm": 0.00024393852800130844,
      "learning_rate": 3.689666666666667e-05,
      "loss": 0.0035,
      "step": 39310
    },
    {
      "epoch": 2.0970666666666666,
      "grad_norm": 0.25602802634239197,
      "learning_rate": 3.6893333333333335e-05,
      "loss": 0.0028,
      "step": 39320
    },
    {
      "epoch": 2.0976,
      "grad_norm": 0.36990463733673096,
      "learning_rate": 3.689e-05,
      "loss": 0.0031,
      "step": 39330
    },
    {
      "epoch": 2.098133333333333,
      "grad_norm": 0.2564108073711395,
      "learning_rate": 3.688666666666667e-05,
      "loss": 0.0038,
      "step": 39340
    },
    {
      "epoch": 2.0986666666666665,
      "grad_norm": 0.22828127443790436,
      "learning_rate": 3.688333333333333e-05,
      "loss": 0.0032,
      "step": 39350
    },
    {
      "epoch": 2.0992,
      "grad_norm": 0.312922865152359,
      "learning_rate": 3.6880000000000006e-05,
      "loss": 0.0035,
      "step": 39360
    },
    {
      "epoch": 2.0997333333333335,
      "grad_norm": 0.2560509741306305,
      "learning_rate": 3.687666666666667e-05,
      "loss": 0.0028,
      "step": 39370
    },
    {
      "epoch": 2.1002666666666667,
      "grad_norm": 0.11440823972225189,
      "learning_rate": 3.687333333333334e-05,
      "loss": 0.003,
      "step": 39380
    },
    {
      "epoch": 2.1008,
      "grad_norm": 0.14224673807621002,
      "learning_rate": 3.6870000000000004e-05,
      "loss": 0.0024,
      "step": 39390
    },
    {
      "epoch": 2.1013333333333333,
      "grad_norm": 0.05692724883556366,
      "learning_rate": 3.6866666666666664e-05,
      "loss": 0.0032,
      "step": 39400
    },
    {
      "epoch": 2.1018666666666665,
      "grad_norm": 0.11378378421068192,
      "learning_rate": 3.686333333333333e-05,
      "loss": 0.003,
      "step": 39410
    },
    {
      "epoch": 2.1024,
      "grad_norm": 0.22756242752075195,
      "learning_rate": 3.686e-05,
      "loss": 0.0056,
      "step": 39420
    },
    {
      "epoch": 2.1029333333333335,
      "grad_norm": 0.08534812927246094,
      "learning_rate": 3.685666666666667e-05,
      "loss": 0.0036,
      "step": 39430
    },
    {
      "epoch": 2.103466666666667,
      "grad_norm": 0.11378244310617447,
      "learning_rate": 3.6853333333333335e-05,
      "loss": 0.0028,
      "step": 39440
    },
    {
      "epoch": 2.104,
      "grad_norm": 0.1708996295928955,
      "learning_rate": 3.685e-05,
      "loss": 0.0022,
      "step": 39450
    },
    {
      "epoch": 2.1045333333333334,
      "grad_norm": 0.11376802623271942,
      "learning_rate": 3.684666666666667e-05,
      "loss": 0.0029,
      "step": 39460
    },
    {
      "epoch": 2.1050666666666666,
      "grad_norm": 0.2559911012649536,
      "learning_rate": 3.6843333333333334e-05,
      "loss": 0.0029,
      "step": 39470
    },
    {
      "epoch": 2.1056,
      "grad_norm": 0.0568901002407074,
      "learning_rate": 3.684e-05,
      "loss": 0.0029,
      "step": 39480
    },
    {
      "epoch": 2.106133333333333,
      "grad_norm": 0.25599685311317444,
      "learning_rate": 3.683666666666667e-05,
      "loss": 0.004,
      "step": 39490
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 0.028444774448871613,
      "learning_rate": 3.683333333333334e-05,
      "loss": 0.0029,
      "step": 39500
    },
    {
      "epoch": 2.1072,
      "grad_norm": 0.00042846606811508536,
      "learning_rate": 3.6830000000000005e-05,
      "loss": 0.0016,
      "step": 39510
    },
    {
      "epoch": 2.1077333333333335,
      "grad_norm": 0.3697528839111328,
      "learning_rate": 3.682666666666667e-05,
      "loss": 0.0029,
      "step": 39520
    },
    {
      "epoch": 2.1082666666666667,
      "grad_norm": 0.2560151219367981,
      "learning_rate": 3.682333333333334e-05,
      "loss": 0.0035,
      "step": 39530
    },
    {
      "epoch": 2.1088,
      "grad_norm": 0.17067496478557587,
      "learning_rate": 3.682e-05,
      "loss": 0.0026,
      "step": 39540
    },
    {
      "epoch": 2.1093333333333333,
      "grad_norm": 0.14222800731658936,
      "learning_rate": 3.681666666666667e-05,
      "loss": 0.0035,
      "step": 39550
    },
    {
      "epoch": 2.1098666666666666,
      "grad_norm": 0.05705062299966812,
      "learning_rate": 3.6813333333333335e-05,
      "loss": 0.0031,
      "step": 39560
    },
    {
      "epoch": 2.1104,
      "grad_norm": 0.255977064371109,
      "learning_rate": 3.681e-05,
      "loss": 0.0026,
      "step": 39570
    },
    {
      "epoch": 2.1109333333333336,
      "grad_norm": 0.341371089220047,
      "learning_rate": 3.680666666666667e-05,
      "loss": 0.0035,
      "step": 39580
    },
    {
      "epoch": 2.111466666666667,
      "grad_norm": 0.11376877874135971,
      "learning_rate": 3.6803333333333334e-05,
      "loss": 0.0037,
      "step": 39590
    },
    {
      "epoch": 2.112,
      "grad_norm": 0.11385547369718552,
      "learning_rate": 3.68e-05,
      "loss": 0.0026,
      "step": 39600
    },
    {
      "epoch": 2.1125333333333334,
      "grad_norm": 0.1706668734550476,
      "learning_rate": 3.6796666666666666e-05,
      "loss": 0.0026,
      "step": 39610
    },
    {
      "epoch": 2.1130666666666666,
      "grad_norm": 0.1706724911928177,
      "learning_rate": 3.679333333333333e-05,
      "loss": 0.0033,
      "step": 39620
    },
    {
      "epoch": 2.1136,
      "grad_norm": 0.05692970007658005,
      "learning_rate": 3.6790000000000005e-05,
      "loss": 0.0033,
      "step": 39630
    },
    {
      "epoch": 2.114133333333333,
      "grad_norm": 0.31285616755485535,
      "learning_rate": 3.678666666666667e-05,
      "loss": 0.0032,
      "step": 39640
    },
    {
      "epoch": 2.1146666666666665,
      "grad_norm": 0.28443971276283264,
      "learning_rate": 3.678333333333334e-05,
      "loss": 0.0033,
      "step": 39650
    },
    {
      "epoch": 2.1152,
      "grad_norm": 0.08534584194421768,
      "learning_rate": 3.6780000000000004e-05,
      "loss": 0.0032,
      "step": 39660
    },
    {
      "epoch": 2.1157333333333335,
      "grad_norm": 0.08532397449016571,
      "learning_rate": 3.677666666666667e-05,
      "loss": 0.0023,
      "step": 39670
    },
    {
      "epoch": 2.1162666666666667,
      "grad_norm": 0.08532457053661346,
      "learning_rate": 3.6773333333333336e-05,
      "loss": 0.0043,
      "step": 39680
    },
    {
      "epoch": 2.1168,
      "grad_norm": 0.05726293474435806,
      "learning_rate": 3.677e-05,
      "loss": 0.0037,
      "step": 39690
    },
    {
      "epoch": 2.1173333333333333,
      "grad_norm": 0.1139642670750618,
      "learning_rate": 3.676666666666667e-05,
      "loss": 0.0035,
      "step": 39700
    },
    {
      "epoch": 2.1178666666666666,
      "grad_norm": 0.36975330114364624,
      "learning_rate": 3.6763333333333334e-05,
      "loss": 0.0023,
      "step": 39710
    },
    {
      "epoch": 2.1184,
      "grad_norm": 0.028446370735764503,
      "learning_rate": 3.676e-05,
      "loss": 0.0026,
      "step": 39720
    },
    {
      "epoch": 2.1189333333333336,
      "grad_norm": 0.25599318742752075,
      "learning_rate": 3.6756666666666667e-05,
      "loss": 0.0024,
      "step": 39730
    },
    {
      "epoch": 2.119466666666667,
      "grad_norm": 0.142205148935318,
      "learning_rate": 3.675333333333333e-05,
      "loss": 0.0023,
      "step": 39740
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.11376596242189407,
      "learning_rate": 3.675e-05,
      "loss": 0.0027,
      "step": 39750
    },
    {
      "epoch": 2.1205333333333334,
      "grad_norm": 0.11376939713954926,
      "learning_rate": 3.6746666666666665e-05,
      "loss": 0.0049,
      "step": 39760
    },
    {
      "epoch": 2.1210666666666667,
      "grad_norm": 0.3412850499153137,
      "learning_rate": 3.674333333333334e-05,
      "loss": 0.002,
      "step": 39770
    },
    {
      "epoch": 2.1216,
      "grad_norm": 0.14220459759235382,
      "learning_rate": 3.6740000000000004e-05,
      "loss": 0.0029,
      "step": 39780
    },
    {
      "epoch": 2.122133333333333,
      "grad_norm": 0.14220380783081055,
      "learning_rate": 3.673666666666667e-05,
      "loss": 0.0034,
      "step": 39790
    },
    {
      "epoch": 2.1226666666666665,
      "grad_norm": 0.4550625681877136,
      "learning_rate": 3.6733333333333336e-05,
      "loss": 0.0034,
      "step": 39800
    },
    {
      "epoch": 2.1232,
      "grad_norm": 0.08532551676034927,
      "learning_rate": 3.673e-05,
      "loss": 0.0029,
      "step": 39810
    },
    {
      "epoch": 2.1237333333333335,
      "grad_norm": 0.05688063055276871,
      "learning_rate": 3.672666666666667e-05,
      "loss": 0.0038,
      "step": 39820
    },
    {
      "epoch": 2.1242666666666667,
      "grad_norm": 0.14220091700553894,
      "learning_rate": 3.6723333333333335e-05,
      "loss": 0.0022,
      "step": 39830
    },
    {
      "epoch": 2.1248,
      "grad_norm": 0.39816904067993164,
      "learning_rate": 3.672000000000001e-05,
      "loss": 0.0038,
      "step": 39840
    },
    {
      "epoch": 2.1253333333333333,
      "grad_norm": 0.05688002333045006,
      "learning_rate": 3.671666666666667e-05,
      "loss": 0.0031,
      "step": 39850
    },
    {
      "epoch": 2.1258666666666666,
      "grad_norm": 0.19909824430942535,
      "learning_rate": 3.671333333333333e-05,
      "loss": 0.0023,
      "step": 39860
    },
    {
      "epoch": 2.1264,
      "grad_norm": 0.22752448916435242,
      "learning_rate": 3.671e-05,
      "loss": 0.0026,
      "step": 39870
    },
    {
      "epoch": 2.1269333333333336,
      "grad_norm": 0.3128334581851959,
      "learning_rate": 3.6706666666666665e-05,
      "loss": 0.003,
      "step": 39880
    },
    {
      "epoch": 2.127466666666667,
      "grad_norm": 0.14220498502254486,
      "learning_rate": 3.670333333333333e-05,
      "loss": 0.0028,
      "step": 39890
    },
    {
      "epoch": 2.128,
      "grad_norm": 0.398150235414505,
      "learning_rate": 3.6700000000000004e-05,
      "loss": 0.0034,
      "step": 39900
    },
    {
      "epoch": 2.1285333333333334,
      "grad_norm": 0.22751474380493164,
      "learning_rate": 3.669666666666667e-05,
      "loss": 0.0047,
      "step": 39910
    },
    {
      "epoch": 2.1290666666666667,
      "grad_norm": 1.238404045977859e-09,
      "learning_rate": 3.669333333333334e-05,
      "loss": 0.0038,
      "step": 39920
    },
    {
      "epoch": 2.1296,
      "grad_norm": 0.1421969085931778,
      "learning_rate": 3.669e-05,
      "loss": 0.004,
      "step": 39930
    },
    {
      "epoch": 2.130133333333333,
      "grad_norm": 0.08531417697668076,
      "learning_rate": 3.668666666666667e-05,
      "loss": 0.0023,
      "step": 39940
    },
    {
      "epoch": 2.1306666666666665,
      "grad_norm": 0.426615446805954,
      "learning_rate": 3.6683333333333335e-05,
      "loss": 0.0026,
      "step": 39950
    },
    {
      "epoch": 2.1312,
      "grad_norm": 0.19907285273075104,
      "learning_rate": 3.668e-05,
      "loss": 0.0025,
      "step": 39960
    },
    {
      "epoch": 2.1317333333333335,
      "grad_norm": 0.08536794036626816,
      "learning_rate": 3.667666666666667e-05,
      "loss": 0.0031,
      "step": 39970
    },
    {
      "epoch": 2.1322666666666668,
      "grad_norm": 0.2559603154659271,
      "learning_rate": 3.667333333333334e-05,
      "loss": 0.0031,
      "step": 39980
    },
    {
      "epoch": 2.1328,
      "grad_norm": 0.028562944382429123,
      "learning_rate": 3.6670000000000006e-05,
      "loss": 0.0029,
      "step": 39990
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.2559659481048584,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 0.0025,
      "step": 40000
    },
    {
      "epoch": 2.1338666666666666,
      "grad_norm": 0.08534632623195648,
      "learning_rate": 3.666333333333333e-05,
      "loss": 0.0023,
      "step": 40010
    },
    {
      "epoch": 2.1344,
      "grad_norm": 0.2844085693359375,
      "learning_rate": 3.666e-05,
      "loss": 0.0033,
      "step": 40020
    },
    {
      "epoch": 2.134933333333333,
      "grad_norm": 0.056897033005952835,
      "learning_rate": 3.6656666666666664e-05,
      "loss": 0.004,
      "step": 40030
    },
    {
      "epoch": 2.135466666666667,
      "grad_norm": 0.1706468015909195,
      "learning_rate": 3.665333333333334e-05,
      "loss": 0.0047,
      "step": 40040
    },
    {
      "epoch": 2.136,
      "grad_norm": 0.11472050845623016,
      "learning_rate": 3.665e-05,
      "loss": 0.0037,
      "step": 40050
    },
    {
      "epoch": 2.1365333333333334,
      "grad_norm": 0.08534672856330872,
      "learning_rate": 3.664666666666667e-05,
      "loss": 0.0039,
      "step": 40060
    },
    {
      "epoch": 2.1370666666666667,
      "grad_norm": 0.08531711250543594,
      "learning_rate": 3.6643333333333335e-05,
      "loss": 0.004,
      "step": 40070
    },
    {
      "epoch": 2.1376,
      "grad_norm": 0.1137598380446434,
      "learning_rate": 3.664e-05,
      "loss": 0.004,
      "step": 40080
    },
    {
      "epoch": 2.138133333333333,
      "grad_norm": 0.1137508749961853,
      "learning_rate": 3.663666666666667e-05,
      "loss": 0.0039,
      "step": 40090
    },
    {
      "epoch": 2.1386666666666665,
      "grad_norm": 0.08532337099313736,
      "learning_rate": 3.6633333333333334e-05,
      "loss": 0.0044,
      "step": 40100
    },
    {
      "epoch": 2.1391999999999998,
      "grad_norm": 0.05723625048995018,
      "learning_rate": 3.663e-05,
      "loss": 0.0029,
      "step": 40110
    },
    {
      "epoch": 2.1397333333333335,
      "grad_norm": 0.1421952098608017,
      "learning_rate": 3.662666666666667e-05,
      "loss": 0.0026,
      "step": 40120
    },
    {
      "epoch": 2.1402666666666668,
      "grad_norm": 0.02845093421638012,
      "learning_rate": 3.662333333333334e-05,
      "loss": 0.0031,
      "step": 40130
    },
    {
      "epoch": 2.1408,
      "grad_norm": 0.19906488060951233,
      "learning_rate": 3.6620000000000005e-05,
      "loss": 0.0032,
      "step": 40140
    },
    {
      "epoch": 2.1413333333333333,
      "grad_norm": 0.1137634664773941,
      "learning_rate": 3.6616666666666664e-05,
      "loss": 0.0022,
      "step": 40150
    },
    {
      "epoch": 2.1418666666666666,
      "grad_norm": 0.028746431693434715,
      "learning_rate": 3.661333333333333e-05,
      "loss": 0.0032,
      "step": 40160
    },
    {
      "epoch": 2.1424,
      "grad_norm": 0.11425016075372696,
      "learning_rate": 3.661e-05,
      "loss": 0.0021,
      "step": 40170
    },
    {
      "epoch": 2.142933333333333,
      "grad_norm": 0.08535958081483841,
      "learning_rate": 3.660666666666667e-05,
      "loss": 0.0032,
      "step": 40180
    },
    {
      "epoch": 2.143466666666667,
      "grad_norm": 0.0002942644350696355,
      "learning_rate": 3.6603333333333336e-05,
      "loss": 0.0043,
      "step": 40190
    },
    {
      "epoch": 2.144,
      "grad_norm": 0.31280452013015747,
      "learning_rate": 3.66e-05,
      "loss": 0.0037,
      "step": 40200
    },
    {
      "epoch": 2.1445333333333334,
      "grad_norm": 0.170626699924469,
      "learning_rate": 3.659666666666667e-05,
      "loss": 0.0049,
      "step": 40210
    },
    {
      "epoch": 2.1450666666666667,
      "grad_norm": 0.05688069015741348,
      "learning_rate": 3.6593333333333334e-05,
      "loss": 0.0041,
      "step": 40220
    },
    {
      "epoch": 2.1456,
      "grad_norm": 0.2843717932701111,
      "learning_rate": 3.659e-05,
      "loss": 0.0031,
      "step": 40230
    },
    {
      "epoch": 2.1461333333333332,
      "grad_norm": 0.2274981588125229,
      "learning_rate": 3.6586666666666666e-05,
      "loss": 0.0033,
      "step": 40240
    },
    {
      "epoch": 2.1466666666666665,
      "grad_norm": 0.02846512943506241,
      "learning_rate": 3.658333333333334e-05,
      "loss": 0.0038,
      "step": 40250
    },
    {
      "epoch": 2.1471999999999998,
      "grad_norm": 0.029402224346995354,
      "learning_rate": 3.6580000000000006e-05,
      "loss": 0.0042,
      "step": 40260
    },
    {
      "epoch": 2.1477333333333335,
      "grad_norm": 0.028437510132789612,
      "learning_rate": 3.657666666666667e-05,
      "loss": 0.0037,
      "step": 40270
    },
    {
      "epoch": 2.1482666666666668,
      "grad_norm": 0.11374484747648239,
      "learning_rate": 3.657333333333334e-05,
      "loss": 0.0026,
      "step": 40280
    },
    {
      "epoch": 2.1488,
      "grad_norm": 0.028435369953513145,
      "learning_rate": 3.6570000000000004e-05,
      "loss": 0.0027,
      "step": 40290
    },
    {
      "epoch": 2.1493333333333333,
      "grad_norm": 0.08531133085489273,
      "learning_rate": 3.656666666666666e-05,
      "loss": 0.0036,
      "step": 40300
    },
    {
      "epoch": 2.1498666666666666,
      "grad_norm": 0.4911915957927704,
      "learning_rate": 3.656333333333333e-05,
      "loss": 0.0026,
      "step": 40310
    },
    {
      "epoch": 2.1504,
      "grad_norm": 0.19905170798301697,
      "learning_rate": 3.656e-05,
      "loss": 0.0023,
      "step": 40320
    },
    {
      "epoch": 2.150933333333333,
      "grad_norm": 0.22749947011470795,
      "learning_rate": 3.655666666666667e-05,
      "loss": 0.004,
      "step": 40330
    },
    {
      "epoch": 2.151466666666667,
      "grad_norm": 0.1421724408864975,
      "learning_rate": 3.6553333333333335e-05,
      "loss": 0.0017,
      "step": 40340
    },
    {
      "epoch": 2.152,
      "grad_norm": 0.05687396973371506,
      "learning_rate": 3.655e-05,
      "loss": 0.0038,
      "step": 40350
    },
    {
      "epoch": 2.1525333333333334,
      "grad_norm": 0.11373629420995712,
      "learning_rate": 3.654666666666667e-05,
      "loss": 0.0026,
      "step": 40360
    },
    {
      "epoch": 2.1530666666666667,
      "grad_norm": 0.028434159234166145,
      "learning_rate": 3.654333333333333e-05,
      "loss": 0.0028,
      "step": 40370
    },
    {
      "epoch": 2.1536,
      "grad_norm": 0.19904479384422302,
      "learning_rate": 3.654e-05,
      "loss": 0.0031,
      "step": 40380
    },
    {
      "epoch": 2.1541333333333332,
      "grad_norm": 0.1990516483783722,
      "learning_rate": 3.653666666666667e-05,
      "loss": 0.0034,
      "step": 40390
    },
    {
      "epoch": 2.1546666666666665,
      "grad_norm": 0.22747522592544556,
      "learning_rate": 3.653333333333334e-05,
      "loss": 0.0019,
      "step": 40400
    },
    {
      "epoch": 2.1552,
      "grad_norm": 0.02844124101102352,
      "learning_rate": 3.6530000000000004e-05,
      "loss": 0.0015,
      "step": 40410
    },
    {
      "epoch": 2.1557333333333335,
      "grad_norm": 0.12045455724000931,
      "learning_rate": 3.652666666666667e-05,
      "loss": 0.003,
      "step": 40420
    },
    {
      "epoch": 2.1562666666666668,
      "grad_norm": 0.2559090256690979,
      "learning_rate": 3.6523333333333337e-05,
      "loss": 0.0032,
      "step": 40430
    },
    {
      "epoch": 2.1568,
      "grad_norm": 0.19905482232570648,
      "learning_rate": 3.652e-05,
      "loss": 0.002,
      "step": 40440
    },
    {
      "epoch": 2.1573333333333333,
      "grad_norm": 0.25591805577278137,
      "learning_rate": 3.651666666666667e-05,
      "loss": 0.0034,
      "step": 40450
    },
    {
      "epoch": 2.1578666666666666,
      "grad_norm": 0.1706312894821167,
      "learning_rate": 3.6513333333333335e-05,
      "loss": 0.0035,
      "step": 40460
    },
    {
      "epoch": 2.1584,
      "grad_norm": 0.1706850826740265,
      "learning_rate": 3.651e-05,
      "loss": 0.0021,
      "step": 40470
    },
    {
      "epoch": 2.158933333333333,
      "grad_norm": 0.028529809787869453,
      "learning_rate": 3.650666666666667e-05,
      "loss": 0.0034,
      "step": 40480
    },
    {
      "epoch": 2.159466666666667,
      "grad_norm": 0.2843603789806366,
      "learning_rate": 3.650333333333333e-05,
      "loss": 0.004,
      "step": 40490
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.14216259121894836,
      "learning_rate": 3.65e-05,
      "loss": 0.003,
      "step": 40500
    },
    {
      "epoch": 2.1605333333333334,
      "grad_norm": 0.4834045469760895,
      "learning_rate": 3.6496666666666666e-05,
      "loss": 0.0026,
      "step": 40510
    },
    {
      "epoch": 2.1610666666666667,
      "grad_norm": 0.14216160774230957,
      "learning_rate": 3.649333333333333e-05,
      "loss": 0.0019,
      "step": 40520
    },
    {
      "epoch": 2.1616,
      "grad_norm": 0.08530358970165253,
      "learning_rate": 3.6490000000000005e-05,
      "loss": 0.0031,
      "step": 40530
    },
    {
      "epoch": 2.1621333333333332,
      "grad_norm": 0.45491766929626465,
      "learning_rate": 3.648666666666667e-05,
      "loss": 0.004,
      "step": 40540
    },
    {
      "epoch": 2.1626666666666665,
      "grad_norm": 0.11373015493154526,
      "learning_rate": 3.648333333333334e-05,
      "loss": 0.0032,
      "step": 40550
    },
    {
      "epoch": 2.1632,
      "grad_norm": 0.4552180767059326,
      "learning_rate": 3.648e-05,
      "loss": 0.0026,
      "step": 40560
    },
    {
      "epoch": 2.1637333333333335,
      "grad_norm": 0.056874558329582214,
      "learning_rate": 3.647666666666667e-05,
      "loss": 0.005,
      "step": 40570
    },
    {
      "epoch": 2.164266666666667,
      "grad_norm": 0.11373729258775711,
      "learning_rate": 3.6473333333333335e-05,
      "loss": 0.0033,
      "step": 40580
    },
    {
      "epoch": 2.1648,
      "grad_norm": 0.17059914767742157,
      "learning_rate": 3.647e-05,
      "loss": 0.0032,
      "step": 40590
    },
    {
      "epoch": 2.1653333333333333,
      "grad_norm": 0.5686121582984924,
      "learning_rate": 3.646666666666667e-05,
      "loss": 0.0041,
      "step": 40600
    },
    {
      "epoch": 2.1658666666666666,
      "grad_norm": 0.05686194822192192,
      "learning_rate": 3.6463333333333334e-05,
      "loss": 0.0031,
      "step": 40610
    },
    {
      "epoch": 2.1664,
      "grad_norm": 0.25590434670448303,
      "learning_rate": 3.646e-05,
      "loss": 0.0031,
      "step": 40620
    },
    {
      "epoch": 2.166933333333333,
      "grad_norm": 0.0852978304028511,
      "learning_rate": 3.6456666666666666e-05,
      "loss": 0.0045,
      "step": 40630
    },
    {
      "epoch": 2.167466666666667,
      "grad_norm": 0.653866708278656,
      "learning_rate": 3.645333333333333e-05,
      "loss": 0.0035,
      "step": 40640
    },
    {
      "epoch": 2.168,
      "grad_norm": 0.25589874386787415,
      "learning_rate": 3.645e-05,
      "loss": 0.0039,
      "step": 40650
    },
    {
      "epoch": 2.1685333333333334,
      "grad_norm": 0.3127301037311554,
      "learning_rate": 3.644666666666667e-05,
      "loss": 0.002,
      "step": 40660
    },
    {
      "epoch": 2.1690666666666667,
      "grad_norm": 0.17057651281356812,
      "learning_rate": 3.644333333333334e-05,
      "loss": 0.0045,
      "step": 40670
    },
    {
      "epoch": 2.1696,
      "grad_norm": 0.28430068492889404,
      "learning_rate": 3.6440000000000003e-05,
      "loss": 0.0044,
      "step": 40680
    },
    {
      "epoch": 2.1701333333333332,
      "grad_norm": 2.7244155820227434e-09,
      "learning_rate": 3.643666666666667e-05,
      "loss": 0.0032,
      "step": 40690
    },
    {
      "epoch": 2.1706666666666665,
      "grad_norm": 0.028498735278844833,
      "learning_rate": 3.6433333333333336e-05,
      "loss": 0.0034,
      "step": 40700
    },
    {
      "epoch": 2.1712,
      "grad_norm": 0.08529409766197205,
      "learning_rate": 3.643e-05,
      "loss": 0.0017,
      "step": 40710
    },
    {
      "epoch": 2.1717333333333335,
      "grad_norm": 0.11375757306814194,
      "learning_rate": 3.642666666666667e-05,
      "loss": 0.0045,
      "step": 40720
    },
    {
      "epoch": 2.172266666666667,
      "grad_norm": 0.2842942774295807,
      "learning_rate": 3.6423333333333334e-05,
      "loss": 0.0026,
      "step": 40730
    },
    {
      "epoch": 2.1728,
      "grad_norm": 0.00037373838131316006,
      "learning_rate": 3.642000000000001e-05,
      "loss": 0.0032,
      "step": 40740
    },
    {
      "epoch": 2.1733333333333333,
      "grad_norm": 0.5970012545585632,
      "learning_rate": 3.641666666666667e-05,
      "loss": 0.0023,
      "step": 40750
    },
    {
      "epoch": 2.1738666666666666,
      "grad_norm": 0.11371919512748718,
      "learning_rate": 3.641333333333333e-05,
      "loss": 0.0029,
      "step": 40760
    },
    {
      "epoch": 2.1744,
      "grad_norm": 0.08528932929039001,
      "learning_rate": 3.641e-05,
      "loss": 0.002,
      "step": 40770
    },
    {
      "epoch": 2.174933333333333,
      "grad_norm": 0.1989966183900833,
      "learning_rate": 3.6406666666666665e-05,
      "loss": 0.0028,
      "step": 40780
    },
    {
      "epoch": 2.175466666666667,
      "grad_norm": 0.22744785249233246,
      "learning_rate": 3.640333333333333e-05,
      "loss": 0.0035,
      "step": 40790
    },
    {
      "epoch": 2.176,
      "grad_norm": 0.11371437460184097,
      "learning_rate": 3.6400000000000004e-05,
      "loss": 0.0029,
      "step": 40800
    },
    {
      "epoch": 2.1765333333333334,
      "grad_norm": 0.14214621484279633,
      "learning_rate": 3.639666666666667e-05,
      "loss": 0.003,
      "step": 40810
    },
    {
      "epoch": 2.1770666666666667,
      "grad_norm": 0.3127160966396332,
      "learning_rate": 3.6393333333333336e-05,
      "loss": 0.0037,
      "step": 40820
    },
    {
      "epoch": 2.1776,
      "grad_norm": 0.36957359313964844,
      "learning_rate": 3.639e-05,
      "loss": 0.0021,
      "step": 40830
    },
    {
      "epoch": 2.1781333333333333,
      "grad_norm": 0.36958059668540955,
      "learning_rate": 3.638666666666667e-05,
      "loss": 0.0028,
      "step": 40840
    },
    {
      "epoch": 2.1786666666666665,
      "grad_norm": 0.056859713047742844,
      "learning_rate": 3.6383333333333335e-05,
      "loss": 0.0034,
      "step": 40850
    },
    {
      "epoch": 2.1792,
      "grad_norm": 0.3127089738845825,
      "learning_rate": 3.638e-05,
      "loss": 0.0032,
      "step": 40860
    },
    {
      "epoch": 2.1797333333333335,
      "grad_norm": 0.3696875274181366,
      "learning_rate": 3.637666666666667e-05,
      "loss": 0.0046,
      "step": 40870
    },
    {
      "epoch": 2.180266666666667,
      "grad_norm": 0.4548335373401642,
      "learning_rate": 3.637333333333334e-05,
      "loss": 0.0026,
      "step": 40880
    },
    {
      "epoch": 2.1808,
      "grad_norm": 0.02844197489321232,
      "learning_rate": 3.6370000000000006e-05,
      "loss": 0.0025,
      "step": 40890
    },
    {
      "epoch": 2.1813333333333333,
      "grad_norm": 0.1989990770816803,
      "learning_rate": 3.636666666666667e-05,
      "loss": 0.0033,
      "step": 40900
    },
    {
      "epoch": 2.1818666666666666,
      "grad_norm": 0.17056851089000702,
      "learning_rate": 3.636333333333333e-05,
      "loss": 0.0033,
      "step": 40910
    },
    {
      "epoch": 2.1824,
      "grad_norm": 0.02842664159834385,
      "learning_rate": 3.636e-05,
      "loss": 0.0035,
      "step": 40920
    },
    {
      "epoch": 2.182933333333333,
      "grad_norm": 0.08528262376785278,
      "learning_rate": 3.6356666666666664e-05,
      "loss": 0.0036,
      "step": 40930
    },
    {
      "epoch": 2.183466666666667,
      "grad_norm": 0.170576274394989,
      "learning_rate": 3.6353333333333337e-05,
      "loss": 0.0036,
      "step": 40940
    },
    {
      "epoch": 2.184,
      "grad_norm": 0.17060722410678864,
      "learning_rate": 3.635e-05,
      "loss": 0.0025,
      "step": 40950
    },
    {
      "epoch": 2.1845333333333334,
      "grad_norm": 0.028427070006728172,
      "learning_rate": 3.634666666666667e-05,
      "loss": 0.0034,
      "step": 40960
    },
    {
      "epoch": 2.1850666666666667,
      "grad_norm": 0.2558556795120239,
      "learning_rate": 3.6343333333333335e-05,
      "loss": 0.0031,
      "step": 40970
    },
    {
      "epoch": 2.1856,
      "grad_norm": 0.028426017612218857,
      "learning_rate": 3.634e-05,
      "loss": 0.0029,
      "step": 40980
    },
    {
      "epoch": 2.1861333333333333,
      "grad_norm": 0.22742612659931183,
      "learning_rate": 3.633666666666667e-05,
      "loss": 0.003,
      "step": 40990
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 0.14213447272777557,
      "learning_rate": 3.633333333333333e-05,
      "loss": 0.0027,
      "step": 41000
    },
    {
      "epoch": 2.1872,
      "grad_norm": 0.5117208957672119,
      "learning_rate": 3.6330000000000006e-05,
      "loss": 0.0038,
      "step": 41010
    },
    {
      "epoch": 2.1877333333333335,
      "grad_norm": 0.7106128931045532,
      "learning_rate": 3.632666666666667e-05,
      "loss": 0.0039,
      "step": 41020
    },
    {
      "epoch": 2.188266666666667,
      "grad_norm": 0.08528207987546921,
      "learning_rate": 3.632333333333334e-05,
      "loss": 0.0023,
      "step": 41030
    },
    {
      "epoch": 2.1888,
      "grad_norm": 0.11370222270488739,
      "learning_rate": 3.6320000000000005e-05,
      "loss": 0.0035,
      "step": 41040
    },
    {
      "epoch": 2.1893333333333334,
      "grad_norm": 0.17055052518844604,
      "learning_rate": 3.631666666666667e-05,
      "loss": 0.0035,
      "step": 41050
    },
    {
      "epoch": 2.1898666666666666,
      "grad_norm": 0.14213038980960846,
      "learning_rate": 3.631333333333333e-05,
      "loss": 0.003,
      "step": 41060
    },
    {
      "epoch": 2.1904,
      "grad_norm": 0.11369545012712479,
      "learning_rate": 3.6309999999999996e-05,
      "loss": 0.0037,
      "step": 41070
    },
    {
      "epoch": 2.190933333333333,
      "grad_norm": 0.36956071853637695,
      "learning_rate": 3.630666666666667e-05,
      "loss": 0.0044,
      "step": 41080
    },
    {
      "epoch": 2.191466666666667,
      "grad_norm": 0.19896109402179718,
      "learning_rate": 3.6303333333333335e-05,
      "loss": 0.0039,
      "step": 41090
    },
    {
      "epoch": 2.192,
      "grad_norm": 3.3145717281968246e-09,
      "learning_rate": 3.63e-05,
      "loss": 0.0026,
      "step": 41100
    },
    {
      "epoch": 2.1925333333333334,
      "grad_norm": 0.056846797466278076,
      "learning_rate": 3.629666666666667e-05,
      "loss": 0.0045,
      "step": 41110
    },
    {
      "epoch": 2.1930666666666667,
      "grad_norm": 0.25581079721450806,
      "learning_rate": 3.6293333333333334e-05,
      "loss": 0.0037,
      "step": 41120
    },
    {
      "epoch": 2.1936,
      "grad_norm": 0.17053872346878052,
      "learning_rate": 3.629e-05,
      "loss": 0.0028,
      "step": 41130
    },
    {
      "epoch": 2.1941333333333333,
      "grad_norm": 0.14212997257709503,
      "learning_rate": 3.6286666666666666e-05,
      "loss": 0.0027,
      "step": 41140
    },
    {
      "epoch": 2.1946666666666665,
      "grad_norm": 0.739098072052002,
      "learning_rate": 3.628333333333334e-05,
      "loss": 0.0045,
      "step": 41150
    },
    {
      "epoch": 2.1952,
      "grad_norm": 3.2959772688911926e-09,
      "learning_rate": 3.6280000000000005e-05,
      "loss": 0.0027,
      "step": 41160
    },
    {
      "epoch": 2.1957333333333335,
      "grad_norm": 0.25580304861068726,
      "learning_rate": 3.627666666666667e-05,
      "loss": 0.0037,
      "step": 41170
    },
    {
      "epoch": 2.196266666666667,
      "grad_norm": 0.19899053871631622,
      "learning_rate": 3.627333333333334e-05,
      "loss": 0.0024,
      "step": 41180
    },
    {
      "epoch": 2.1968,
      "grad_norm": 0.22738151252269745,
      "learning_rate": 3.6270000000000003e-05,
      "loss": 0.0025,
      "step": 41190
    },
    {
      "epoch": 2.1973333333333334,
      "grad_norm": 0.17053371667861938,
      "learning_rate": 3.626666666666667e-05,
      "loss": 0.0026,
      "step": 41200
    },
    {
      "epoch": 2.1978666666666666,
      "grad_norm": 0.2558099031448364,
      "learning_rate": 3.6263333333333336e-05,
      "loss": 0.0034,
      "step": 41210
    },
    {
      "epoch": 2.1984,
      "grad_norm": 0.05684283375740051,
      "learning_rate": 3.626e-05,
      "loss": 0.004,
      "step": 41220
    },
    {
      "epoch": 2.198933333333333,
      "grad_norm": 0.14211364090442657,
      "learning_rate": 3.625666666666667e-05,
      "loss": 0.0035,
      "step": 41230
    },
    {
      "epoch": 2.1994666666666665,
      "grad_norm": 0.028430664911866188,
      "learning_rate": 3.6253333333333334e-05,
      "loss": 0.0034,
      "step": 41240
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.22791887819766998,
      "learning_rate": 3.625e-05,
      "loss": 0.0033,
      "step": 41250
    },
    {
      "epoch": 2.2005333333333335,
      "grad_norm": 0.085296630859375,
      "learning_rate": 3.6246666666666666e-05,
      "loss": 0.0037,
      "step": 41260
    },
    {
      "epoch": 2.2010666666666667,
      "grad_norm": 0.05688885971903801,
      "learning_rate": 3.624333333333333e-05,
      "loss": 0.0036,
      "step": 41270
    },
    {
      "epoch": 2.2016,
      "grad_norm": 0.028422052040696144,
      "learning_rate": 3.624e-05,
      "loss": 0.0039,
      "step": 41280
    },
    {
      "epoch": 2.2021333333333333,
      "grad_norm": 0.2273726761341095,
      "learning_rate": 3.623666666666667e-05,
      "loss": 0.0032,
      "step": 41290
    },
    {
      "epoch": 2.2026666666666666,
      "grad_norm": 0.028431257233023643,
      "learning_rate": 3.623333333333334e-05,
      "loss": 0.003,
      "step": 41300
    },
    {
      "epoch": 2.2032,
      "grad_norm": 0.22739413380622864,
      "learning_rate": 3.6230000000000004e-05,
      "loss": 0.0032,
      "step": 41310
    },
    {
      "epoch": 2.203733333333333,
      "grad_norm": 0.0569039061665535,
      "learning_rate": 3.622666666666667e-05,
      "loss": 0.0031,
      "step": 41320
    },
    {
      "epoch": 2.204266666666667,
      "grad_norm": 0.11374178528785706,
      "learning_rate": 3.6223333333333336e-05,
      "loss": 0.0035,
      "step": 41330
    },
    {
      "epoch": 2.2048,
      "grad_norm": 0.08527174592018127,
      "learning_rate": 3.622e-05,
      "loss": 0.0038,
      "step": 41340
    },
    {
      "epoch": 2.2053333333333334,
      "grad_norm": 0.02850477397441864,
      "learning_rate": 3.621666666666667e-05,
      "loss": 0.0031,
      "step": 41350
    },
    {
      "epoch": 2.2058666666666666,
      "grad_norm": 0.39793699979782104,
      "learning_rate": 3.6213333333333334e-05,
      "loss": 0.0028,
      "step": 41360
    },
    {
      "epoch": 2.2064,
      "grad_norm": 0.4547049403190613,
      "learning_rate": 3.621e-05,
      "loss": 0.0036,
      "step": 41370
    },
    {
      "epoch": 2.206933333333333,
      "grad_norm": 0.05684344097971916,
      "learning_rate": 3.620666666666667e-05,
      "loss": 0.0035,
      "step": 41380
    },
    {
      "epoch": 2.2074666666666665,
      "grad_norm": 0.14210304617881775,
      "learning_rate": 3.620333333333333e-05,
      "loss": 0.0025,
      "step": 41390
    },
    {
      "epoch": 2.208,
      "grad_norm": 0.4547092914581299,
      "learning_rate": 3.62e-05,
      "loss": 0.0028,
      "step": 41400
    },
    {
      "epoch": 2.2085333333333335,
      "grad_norm": 0.4255509078502655,
      "learning_rate": 3.6196666666666665e-05,
      "loss": 0.0032,
      "step": 41410
    },
    {
      "epoch": 2.2090666666666667,
      "grad_norm": 0.14183767139911652,
      "learning_rate": 3.619333333333333e-05,
      "loss": 0.003,
      "step": 41420
    },
    {
      "epoch": 2.2096,
      "grad_norm": 0.11347203701734543,
      "learning_rate": 3.6190000000000004e-05,
      "loss": 0.002,
      "step": 41430
    },
    {
      "epoch": 2.2101333333333333,
      "grad_norm": 0.1985669583082199,
      "learning_rate": 3.618666666666667e-05,
      "loss": 0.0038,
      "step": 41440
    },
    {
      "epoch": 2.2106666666666666,
      "grad_norm": 0.11346644163131714,
      "learning_rate": 3.6183333333333336e-05,
      "loss": 0.0039,
      "step": 41450
    },
    {
      "epoch": 2.2112,
      "grad_norm": 0.11347217112779617,
      "learning_rate": 3.618e-05,
      "loss": 0.0027,
      "step": 41460
    },
    {
      "epoch": 2.211733333333333,
      "grad_norm": 0.255293071269989,
      "learning_rate": 3.617666666666667e-05,
      "loss": 0.0043,
      "step": 41470
    },
    {
      "epoch": 2.212266666666667,
      "grad_norm": 0.1985715627670288,
      "learning_rate": 3.6173333333333335e-05,
      "loss": 0.0034,
      "step": 41480
    },
    {
      "epoch": 2.2128,
      "grad_norm": 0.08509781956672668,
      "learning_rate": 3.617e-05,
      "loss": 0.0035,
      "step": 41490
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 0.05673561245203018,
      "learning_rate": 3.6166666666666674e-05,
      "loss": 0.0037,
      "step": 41500
    },
    {
      "epoch": 2.2138666666666666,
      "grad_norm": 0.028367560356855392,
      "learning_rate": 3.616333333333333e-05,
      "loss": 0.0018,
      "step": 41510
    },
    {
      "epoch": 2.2144,
      "grad_norm": 0.08510025590658188,
      "learning_rate": 3.616e-05,
      "loss": 0.0032,
      "step": 41520
    },
    {
      "epoch": 2.214933333333333,
      "grad_norm": 0.17019464075565338,
      "learning_rate": 3.6156666666666666e-05,
      "loss": 0.0043,
      "step": 41530
    },
    {
      "epoch": 2.2154666666666665,
      "grad_norm": 0.08509837836027145,
      "learning_rate": 3.615333333333333e-05,
      "loss": 0.0034,
      "step": 41540
    },
    {
      "epoch": 2.216,
      "grad_norm": 0.36876794695854187,
      "learning_rate": 3.615e-05,
      "loss": 0.002,
      "step": 41550
    },
    {
      "epoch": 2.2165333333333335,
      "grad_norm": 0.2553142309188843,
      "learning_rate": 3.614666666666667e-05,
      "loss": 0.0043,
      "step": 41560
    },
    {
      "epoch": 2.2170666666666667,
      "grad_norm": 0.028366485610604286,
      "learning_rate": 3.614333333333334e-05,
      "loss": 0.0037,
      "step": 41570
    },
    {
      "epoch": 2.2176,
      "grad_norm": 0.36876222491264343,
      "learning_rate": 3.614e-05,
      "loss": 0.0045,
      "step": 41580
    },
    {
      "epoch": 2.2181333333333333,
      "grad_norm": 0.17019249498844147,
      "learning_rate": 3.613666666666667e-05,
      "loss": 0.0036,
      "step": 41590
    },
    {
      "epoch": 2.2186666666666666,
      "grad_norm": 0.22694124281406403,
      "learning_rate": 3.6133333333333335e-05,
      "loss": 0.0036,
      "step": 41600
    },
    {
      "epoch": 2.2192,
      "grad_norm": 0.028364788740873337,
      "learning_rate": 3.613e-05,
      "loss": 0.0037,
      "step": 41610
    },
    {
      "epoch": 2.219733333333333,
      "grad_norm": 0.1702028214931488,
      "learning_rate": 3.612666666666667e-05,
      "loss": 0.0028,
      "step": 41620
    },
    {
      "epoch": 2.220266666666667,
      "grad_norm": 0.19856226444244385,
      "learning_rate": 3.6123333333333334e-05,
      "loss": 0.0041,
      "step": 41630
    },
    {
      "epoch": 2.2208,
      "grad_norm": 0.510597288608551,
      "learning_rate": 3.6120000000000007e-05,
      "loss": 0.0025,
      "step": 41640
    },
    {
      "epoch": 2.2213333333333334,
      "grad_norm": 0.34040024876594543,
      "learning_rate": 3.611666666666667e-05,
      "loss": 0.0034,
      "step": 41650
    },
    {
      "epoch": 2.2218666666666667,
      "grad_norm": 0.11367533355951309,
      "learning_rate": 3.611333333333333e-05,
      "loss": 0.003,
      "step": 41660
    },
    {
      "epoch": 2.2224,
      "grad_norm": 0.08510041236877441,
      "learning_rate": 3.611e-05,
      "loss": 0.0033,
      "step": 41670
    },
    {
      "epoch": 2.222933333333333,
      "grad_norm": 0.31202107667922974,
      "learning_rate": 3.6106666666666664e-05,
      "loss": 0.0026,
      "step": 41680
    },
    {
      "epoch": 2.2234666666666665,
      "grad_norm": 0.25528156757354736,
      "learning_rate": 3.610333333333333e-05,
      "loss": 0.0037,
      "step": 41690
    },
    {
      "epoch": 2.224,
      "grad_norm": 0.08510330319404602,
      "learning_rate": 3.61e-05,
      "loss": 0.0028,
      "step": 41700
    },
    {
      "epoch": 2.2245333333333335,
      "grad_norm": 0.25529658794403076,
      "learning_rate": 3.609666666666667e-05,
      "loss": 0.0042,
      "step": 41710
    },
    {
      "epoch": 2.2250666666666667,
      "grad_norm": 0.25579461455345154,
      "learning_rate": 3.6093333333333336e-05,
      "loss": 0.0031,
      "step": 41720
    },
    {
      "epoch": 2.2256,
      "grad_norm": 0.11368124932050705,
      "learning_rate": 3.609e-05,
      "loss": 0.0039,
      "step": 41730
    },
    {
      "epoch": 2.2261333333333333,
      "grad_norm": 0.9615908265113831,
      "learning_rate": 3.608666666666667e-05,
      "loss": 0.0038,
      "step": 41740
    },
    {
      "epoch": 2.2266666666666666,
      "grad_norm": 0.028603797778487206,
      "learning_rate": 3.6083333333333334e-05,
      "loss": 0.0037,
      "step": 41750
    },
    {
      "epoch": 2.2272,
      "grad_norm": 0.17051658034324646,
      "learning_rate": 3.608e-05,
      "loss": 0.003,
      "step": 41760
    },
    {
      "epoch": 2.227733333333333,
      "grad_norm": 0.19892510771751404,
      "learning_rate": 3.607666666666667e-05,
      "loss": 0.0036,
      "step": 41770
    },
    {
      "epoch": 2.228266666666667,
      "grad_norm": 0.2841984033584595,
      "learning_rate": 3.607333333333334e-05,
      "loss": 0.0034,
      "step": 41780
    },
    {
      "epoch": 2.2288,
      "grad_norm": 0.08525523543357849,
      "learning_rate": 3.6070000000000005e-05,
      "loss": 0.0036,
      "step": 41790
    },
    {
      "epoch": 2.2293333333333334,
      "grad_norm": 0.3979029655456543,
      "learning_rate": 3.606666666666667e-05,
      "loss": 0.003,
      "step": 41800
    },
    {
      "epoch": 2.2298666666666667,
      "grad_norm": 0.1136775016784668,
      "learning_rate": 3.606333333333333e-05,
      "loss": 0.0034,
      "step": 41810
    },
    {
      "epoch": 2.2304,
      "grad_norm": 0.02841954678297043,
      "learning_rate": 3.606e-05,
      "loss": 0.0028,
      "step": 41820
    },
    {
      "epoch": 2.230933333333333,
      "grad_norm": 0.31259211897850037,
      "learning_rate": 3.605666666666666e-05,
      "loss": 0.0032,
      "step": 41830
    },
    {
      "epoch": 2.2314666666666665,
      "grad_norm": 0.08525904268026352,
      "learning_rate": 3.6053333333333336e-05,
      "loss": 0.0025,
      "step": 41840
    },
    {
      "epoch": 2.232,
      "grad_norm": 0.22735744714736938,
      "learning_rate": 3.605e-05,
      "loss": 0.0029,
      "step": 41850
    },
    {
      "epoch": 2.2325333333333335,
      "grad_norm": 0.1705220490694046,
      "learning_rate": 3.604666666666667e-05,
      "loss": 0.0038,
      "step": 41860
    },
    {
      "epoch": 2.2330666666666668,
      "grad_norm": 0.625178337097168,
      "learning_rate": 3.6043333333333334e-05,
      "loss": 0.0029,
      "step": 41870
    },
    {
      "epoch": 2.2336,
      "grad_norm": 0.14209753274917603,
      "learning_rate": 3.604e-05,
      "loss": 0.0038,
      "step": 41880
    },
    {
      "epoch": 2.2341333333333333,
      "grad_norm": 0.14456240832805634,
      "learning_rate": 3.603666666666667e-05,
      "loss": 0.0029,
      "step": 41890
    },
    {
      "epoch": 2.2346666666666666,
      "grad_norm": 0.17050425708293915,
      "learning_rate": 3.603333333333333e-05,
      "loss": 0.0023,
      "step": 41900
    },
    {
      "epoch": 2.2352,
      "grad_norm": 0.3979792892932892,
      "learning_rate": 3.6030000000000006e-05,
      "loss": 0.0039,
      "step": 41910
    },
    {
      "epoch": 2.235733333333333,
      "grad_norm": 0.08551464229822159,
      "learning_rate": 3.602666666666667e-05,
      "loss": 0.0037,
      "step": 41920
    },
    {
      "epoch": 2.236266666666667,
      "grad_norm": 0.08541028946638107,
      "learning_rate": 3.602333333333334e-05,
      "loss": 0.0033,
      "step": 41930
    },
    {
      "epoch": 2.2368,
      "grad_norm": 0.198935404419899,
      "learning_rate": 3.6020000000000004e-05,
      "loss": 0.0027,
      "step": 41940
    },
    {
      "epoch": 2.2373333333333334,
      "grad_norm": 0.05699286237359047,
      "learning_rate": 3.601666666666667e-05,
      "loss": 0.0035,
      "step": 41950
    },
    {
      "epoch": 2.2378666666666667,
      "grad_norm": 4.1636232239739e-09,
      "learning_rate": 3.6013333333333336e-05,
      "loss": 0.0028,
      "step": 41960
    },
    {
      "epoch": 2.2384,
      "grad_norm": 0.19892318546772003,
      "learning_rate": 3.601e-05,
      "loss": 0.0028,
      "step": 41970
    },
    {
      "epoch": 2.238933333333333,
      "grad_norm": 0.2841518223285675,
      "learning_rate": 3.600666666666667e-05,
      "loss": 0.0025,
      "step": 41980
    },
    {
      "epoch": 2.2394666666666665,
      "grad_norm": 0.3410247266292572,
      "learning_rate": 3.6003333333333335e-05,
      "loss": 0.0037,
      "step": 41990
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.22732959687709808,
      "learning_rate": 3.6e-05,
      "loss": 0.0026,
      "step": 42000
    },
    {
      "epoch": 2.2405333333333335,
      "grad_norm": 0.4546419680118561,
      "learning_rate": 3.599666666666667e-05,
      "loss": 0.002,
      "step": 42010
    },
    {
      "epoch": 2.2410666666666668,
      "grad_norm": 0.1704903244972229,
      "learning_rate": 3.599333333333333e-05,
      "loss": 0.0043,
      "step": 42020
    },
    {
      "epoch": 2.2416,
      "grad_norm": 0.22732317447662354,
      "learning_rate": 3.599e-05,
      "loss": 0.0034,
      "step": 42030
    },
    {
      "epoch": 2.2421333333333333,
      "grad_norm": 0.14208227396011353,
      "learning_rate": 3.5986666666666665e-05,
      "loss": 0.0032,
      "step": 42040
    },
    {
      "epoch": 2.2426666666666666,
      "grad_norm": 0.03110724873840809,
      "learning_rate": 3.598333333333334e-05,
      "loss": 0.0038,
      "step": 42050
    },
    {
      "epoch": 2.2432,
      "grad_norm": 0.42626717686653137,
      "learning_rate": 3.5980000000000004e-05,
      "loss": 0.0033,
      "step": 42060
    },
    {
      "epoch": 2.243733333333333,
      "grad_norm": 0.5115199685096741,
      "learning_rate": 3.597666666666667e-05,
      "loss": 0.0025,
      "step": 42070
    },
    {
      "epoch": 2.244266666666667,
      "grad_norm": 0.2561376690864563,
      "learning_rate": 3.597333333333334e-05,
      "loss": 0.0031,
      "step": 42080
    },
    {
      "epoch": 2.2448,
      "grad_norm": 0.19892559945583344,
      "learning_rate": 3.597e-05,
      "loss": 0.0039,
      "step": 42090
    },
    {
      "epoch": 2.2453333333333334,
      "grad_norm": 2.739436233412107e-09,
      "learning_rate": 3.596666666666667e-05,
      "loss": 0.0028,
      "step": 42100
    },
    {
      "epoch": 2.2458666666666667,
      "grad_norm": 0.14208458364009857,
      "learning_rate": 3.5963333333333335e-05,
      "loss": 0.0026,
      "step": 42110
    },
    {
      "epoch": 2.2464,
      "grad_norm": 0.1997041553258896,
      "learning_rate": 3.596e-05,
      "loss": 0.0049,
      "step": 42120
    },
    {
      "epoch": 2.2469333333333332,
      "grad_norm": 0.17049263417720795,
      "learning_rate": 3.595666666666667e-05,
      "loss": 0.0032,
      "step": 42130
    },
    {
      "epoch": 2.2474666666666665,
      "grad_norm": 0.2557355761528015,
      "learning_rate": 3.5953333333333334e-05,
      "loss": 0.0047,
      "step": 42140
    },
    {
      "epoch": 2.248,
      "grad_norm": 0.0585695318877697,
      "learning_rate": 3.595e-05,
      "loss": 0.0039,
      "step": 42150
    },
    {
      "epoch": 2.2485333333333335,
      "grad_norm": 0.028415532782673836,
      "learning_rate": 3.5946666666666666e-05,
      "loss": 0.0046,
      "step": 42160
    },
    {
      "epoch": 2.2490666666666668,
      "grad_norm": 0.17048560082912445,
      "learning_rate": 3.594333333333333e-05,
      "loss": 0.0041,
      "step": 42170
    },
    {
      "epoch": 2.2496,
      "grad_norm": 0.14207901060581207,
      "learning_rate": 3.594e-05,
      "loss": 0.0031,
      "step": 42180
    },
    {
      "epoch": 2.2501333333333333,
      "grad_norm": 0.3693820536136627,
      "learning_rate": 3.593666666666667e-05,
      "loss": 0.0032,
      "step": 42190
    },
    {
      "epoch": 2.2506666666666666,
      "grad_norm": 0.14207147061824799,
      "learning_rate": 3.593333333333334e-05,
      "loss": 0.0019,
      "step": 42200
    },
    {
      "epoch": 2.2512,
      "grad_norm": 0.028413936495780945,
      "learning_rate": 3.593e-05,
      "loss": 0.0026,
      "step": 42210
    },
    {
      "epoch": 2.251733333333333,
      "grad_norm": 0.05683004483580589,
      "learning_rate": 3.592666666666667e-05,
      "loss": 0.004,
      "step": 42220
    },
    {
      "epoch": 2.2522666666666664,
      "grad_norm": 0.22730454802513123,
      "learning_rate": 3.5923333333333336e-05,
      "loss": 0.0039,
      "step": 42230
    },
    {
      "epoch": 2.2528,
      "grad_norm": 0.22731386125087738,
      "learning_rate": 3.592e-05,
      "loss": 0.0023,
      "step": 42240
    },
    {
      "epoch": 2.2533333333333334,
      "grad_norm": 0.08524027466773987,
      "learning_rate": 3.591666666666667e-05,
      "loss": 0.0032,
      "step": 42250
    },
    {
      "epoch": 2.2538666666666667,
      "grad_norm": 0.05682642385363579,
      "learning_rate": 3.591333333333334e-05,
      "loss": 0.0025,
      "step": 42260
    },
    {
      "epoch": 2.2544,
      "grad_norm": 0.11366097629070282,
      "learning_rate": 3.591e-05,
      "loss": 0.0027,
      "step": 42270
    },
    {
      "epoch": 2.2549333333333332,
      "grad_norm": 0.19888882339000702,
      "learning_rate": 3.5906666666666666e-05,
      "loss": 0.0036,
      "step": 42280
    },
    {
      "epoch": 2.2554666666666665,
      "grad_norm": 0.22732125222682953,
      "learning_rate": 3.590333333333333e-05,
      "loss": 0.0035,
      "step": 42290
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 0.14206448197364807,
      "learning_rate": 3.59e-05,
      "loss": 0.0026,
      "step": 42300
    },
    {
      "epoch": 2.2565333333333335,
      "grad_norm": 0.1420673429965973,
      "learning_rate": 3.5896666666666665e-05,
      "loss": 0.004,
      "step": 42310
    },
    {
      "epoch": 2.2570666666666668,
      "grad_norm": 0.05682571977376938,
      "learning_rate": 3.589333333333334e-05,
      "loss": 0.0022,
      "step": 42320
    },
    {
      "epoch": 2.2576,
      "grad_norm": 0.17048543691635132,
      "learning_rate": 3.5890000000000004e-05,
      "loss": 0.0023,
      "step": 42330
    },
    {
      "epoch": 2.2581333333333333,
      "grad_norm": 3.673138238013962e-09,
      "learning_rate": 3.588666666666667e-05,
      "loss": 0.003,
      "step": 42340
    },
    {
      "epoch": 2.2586666666666666,
      "grad_norm": 0.4261999726295471,
      "learning_rate": 3.5883333333333336e-05,
      "loss": 0.0028,
      "step": 42350
    },
    {
      "epoch": 2.2592,
      "grad_norm": 0.05682525038719177,
      "learning_rate": 3.588e-05,
      "loss": 0.0035,
      "step": 42360
    },
    {
      "epoch": 2.259733333333333,
      "grad_norm": 0.19890357553958893,
      "learning_rate": 3.587666666666667e-05,
      "loss": 0.004,
      "step": 42370
    },
    {
      "epoch": 2.2602666666666664,
      "grad_norm": 0.1832629293203354,
      "learning_rate": 3.5873333333333334e-05,
      "loss": 0.0026,
      "step": 42380
    },
    {
      "epoch": 2.2608,
      "grad_norm": 0.11425387859344482,
      "learning_rate": 3.587e-05,
      "loss": 0.0032,
      "step": 42390
    },
    {
      "epoch": 2.2613333333333334,
      "grad_norm": 0.14206165075302124,
      "learning_rate": 3.586666666666667e-05,
      "loss": 0.0045,
      "step": 42400
    },
    {
      "epoch": 2.2618666666666667,
      "grad_norm": 3.413478388836211e-09,
      "learning_rate": 3.586333333333334e-05,
      "loss": 0.0037,
      "step": 42410
    },
    {
      "epoch": 2.2624,
      "grad_norm": 0.31253886222839355,
      "learning_rate": 3.586e-05,
      "loss": 0.0026,
      "step": 42420
    },
    {
      "epoch": 2.2629333333333332,
      "grad_norm": 0.14207087457180023,
      "learning_rate": 3.5856666666666665e-05,
      "loss": 0.0031,
      "step": 42430
    },
    {
      "epoch": 2.2634666666666665,
      "grad_norm": 0.02862466312944889,
      "learning_rate": 3.585333333333333e-05,
      "loss": 0.0022,
      "step": 42440
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 0.06558133661746979,
      "learning_rate": 3.585e-05,
      "loss": 0.0023,
      "step": 42450
    },
    {
      "epoch": 2.2645333333333335,
      "grad_norm": 0.5683997869491577,
      "learning_rate": 3.584666666666667e-05,
      "loss": 0.0028,
      "step": 42460
    },
    {
      "epoch": 2.265066666666667,
      "grad_norm": 0.5124832391738892,
      "learning_rate": 3.5843333333333336e-05,
      "loss": 0.0033,
      "step": 42470
    },
    {
      "epoch": 2.2656,
      "grad_norm": 1.463677845592315e-09,
      "learning_rate": 3.584e-05,
      "loss": 0.0035,
      "step": 42480
    },
    {
      "epoch": 2.2661333333333333,
      "grad_norm": 0.5682595372200012,
      "learning_rate": 3.583666666666667e-05,
      "loss": 0.0038,
      "step": 42490
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.5113316774368286,
      "learning_rate": 3.5833333333333335e-05,
      "loss": 0.0039,
      "step": 42500
    },
    {
      "epoch": 2.2672,
      "grad_norm": 0.08522854000329971,
      "learning_rate": 3.583e-05,
      "loss": 0.0036,
      "step": 42510
    },
    {
      "epoch": 2.267733333333333,
      "grad_norm": 0.05681818351149559,
      "learning_rate": 3.582666666666667e-05,
      "loss": 0.0047,
      "step": 42520
    },
    {
      "epoch": 2.2682666666666664,
      "grad_norm": 0.1136375218629837,
      "learning_rate": 3.582333333333334e-05,
      "loss": 0.0034,
      "step": 42530
    },
    {
      "epoch": 2.2688,
      "grad_norm": 0.2557033896446228,
      "learning_rate": 3.5820000000000006e-05,
      "loss": 0.0038,
      "step": 42540
    },
    {
      "epoch": 2.2693333333333334,
      "grad_norm": 0.05681895837187767,
      "learning_rate": 3.581666666666667e-05,
      "loss": 0.0038,
      "step": 42550
    },
    {
      "epoch": 2.2698666666666667,
      "grad_norm": 0.5397204160690308,
      "learning_rate": 3.581333333333334e-05,
      "loss": 0.0036,
      "step": 42560
    },
    {
      "epoch": 2.2704,
      "grad_norm": 0.39773303270339966,
      "learning_rate": 3.581e-05,
      "loss": 0.0048,
      "step": 42570
    },
    {
      "epoch": 2.2709333333333332,
      "grad_norm": 0.028407279402017593,
      "learning_rate": 3.5806666666666664e-05,
      "loss": 0.0028,
      "step": 42580
    },
    {
      "epoch": 2.2714666666666665,
      "grad_norm": 0.028407413512468338,
      "learning_rate": 3.580333333333333e-05,
      "loss": 0.0036,
      "step": 42590
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 0.0284077450633049,
      "learning_rate": 3.58e-05,
      "loss": 0.0025,
      "step": 42600
    },
    {
      "epoch": 2.2725333333333335,
      "grad_norm": 0.4545515179634094,
      "learning_rate": 3.579666666666667e-05,
      "loss": 0.002,
      "step": 42610
    },
    {
      "epoch": 2.273066666666667,
      "grad_norm": 0.02840651012957096,
      "learning_rate": 3.5793333333333335e-05,
      "loss": 0.0029,
      "step": 42620
    },
    {
      "epoch": 2.2736,
      "grad_norm": 0.08522111922502518,
      "learning_rate": 3.579e-05,
      "loss": 0.004,
      "step": 42630
    },
    {
      "epoch": 2.2741333333333333,
      "grad_norm": 0.4261132776737213,
      "learning_rate": 3.578666666666667e-05,
      "loss": 0.0027,
      "step": 42640
    },
    {
      "epoch": 2.2746666666666666,
      "grad_norm": 0.028406783938407898,
      "learning_rate": 3.5783333333333333e-05,
      "loss": 0.002,
      "step": 42650
    },
    {
      "epoch": 2.2752,
      "grad_norm": 0.05681367963552475,
      "learning_rate": 3.578e-05,
      "loss": 0.0037,
      "step": 42660
    },
    {
      "epoch": 2.275733333333333,
      "grad_norm": 0.14235693216323853,
      "learning_rate": 3.577666666666667e-05,
      "loss": 0.0032,
      "step": 42670
    },
    {
      "epoch": 2.2762666666666664,
      "grad_norm": 0.14234620332717896,
      "learning_rate": 3.577333333333334e-05,
      "loss": 0.0021,
      "step": 42680
    },
    {
      "epoch": 2.2768,
      "grad_norm": 0.028468647971749306,
      "learning_rate": 3.5770000000000005e-05,
      "loss": 0.0027,
      "step": 42690
    },
    {
      "epoch": 2.2773333333333334,
      "grad_norm": 0.28470978140830994,
      "learning_rate": 3.576666666666667e-05,
      "loss": 0.003,
      "step": 42700
    },
    {
      "epoch": 2.2778666666666667,
      "grad_norm": 1.4295438166556096e-09,
      "learning_rate": 3.576333333333334e-05,
      "loss": 0.005,
      "step": 42710
    },
    {
      "epoch": 2.2784,
      "grad_norm": 0.2562369108200073,
      "learning_rate": 3.5759999999999996e-05,
      "loss": 0.0031,
      "step": 42720
    },
    {
      "epoch": 2.2789333333333333,
      "grad_norm": 0.08540233969688416,
      "learning_rate": 3.575666666666667e-05,
      "loss": 0.0032,
      "step": 42730
    },
    {
      "epoch": 2.2794666666666665,
      "grad_norm": 0.37011033296585083,
      "learning_rate": 3.5753333333333335e-05,
      "loss": 0.0019,
      "step": 42740
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.3131403625011444,
      "learning_rate": 3.575e-05,
      "loss": 0.0028,
      "step": 42750
    },
    {
      "epoch": 2.2805333333333335,
      "grad_norm": 0.08540269732475281,
      "learning_rate": 3.574666666666667e-05,
      "loss": 0.0037,
      "step": 42760
    },
    {
      "epoch": 2.281066666666667,
      "grad_norm": 0.05693240463733673,
      "learning_rate": 3.5743333333333334e-05,
      "loss": 0.0041,
      "step": 42770
    },
    {
      "epoch": 2.2816,
      "grad_norm": 0.284684419631958,
      "learning_rate": 3.574e-05,
      "loss": 0.0031,
      "step": 42780
    },
    {
      "epoch": 2.2821333333333333,
      "grad_norm": 0.1423274129629135,
      "learning_rate": 3.5736666666666666e-05,
      "loss": 0.003,
      "step": 42790
    },
    {
      "epoch": 2.2826666666666666,
      "grad_norm": 0.540903627872467,
      "learning_rate": 3.573333333333333e-05,
      "loss": 0.0042,
      "step": 42800
    },
    {
      "epoch": 2.2832,
      "grad_norm": 0.17079035937786102,
      "learning_rate": 3.5730000000000005e-05,
      "loss": 0.0041,
      "step": 42810
    },
    {
      "epoch": 2.283733333333333,
      "grad_norm": 0.0853944793343544,
      "learning_rate": 3.572666666666667e-05,
      "loss": 0.0029,
      "step": 42820
    },
    {
      "epoch": 2.2842666666666664,
      "grad_norm": 0.11385975778102875,
      "learning_rate": 3.572333333333334e-05,
      "loss": 0.0042,
      "step": 42830
    },
    {
      "epoch": 2.2848,
      "grad_norm": 0.4839320778846741,
      "learning_rate": 3.5720000000000004e-05,
      "loss": 0.0027,
      "step": 42840
    },
    {
      "epoch": 2.2853333333333334,
      "grad_norm": 0.0042299688793718815,
      "learning_rate": 3.571666666666667e-05,
      "loss": 0.0039,
      "step": 42850
    },
    {
      "epoch": 2.2858666666666667,
      "grad_norm": 0.1424277275800705,
      "learning_rate": 3.5713333333333336e-05,
      "loss": 0.0044,
      "step": 42860
    },
    {
      "epoch": 2.2864,
      "grad_norm": 0.14428122341632843,
      "learning_rate": 3.571e-05,
      "loss": 0.0072,
      "step": 42870
    },
    {
      "epoch": 2.2869333333333333,
      "grad_norm": 0.0019843620248138905,
      "learning_rate": 3.570666666666667e-05,
      "loss": 0.005,
      "step": 42880
    },
    {
      "epoch": 2.2874666666666665,
      "grad_norm": 0.056916579604148865,
      "learning_rate": 3.5703333333333334e-05,
      "loss": 0.0023,
      "step": 42890
    },
    {
      "epoch": 2.288,
      "grad_norm": 0.3699897527694702,
      "learning_rate": 3.57e-05,
      "loss": 0.0031,
      "step": 42900
    },
    {
      "epoch": 2.2885333333333335,
      "grad_norm": 0.17074990272521973,
      "learning_rate": 3.5696666666666667e-05,
      "loss": 0.0033,
      "step": 42910
    },
    {
      "epoch": 2.289066666666667,
      "grad_norm": 0.11383403837680817,
      "learning_rate": 3.569333333333333e-05,
      "loss": 0.0041,
      "step": 42920
    },
    {
      "epoch": 2.2896,
      "grad_norm": 0.11383042484521866,
      "learning_rate": 3.569e-05,
      "loss": 0.0041,
      "step": 42930
    },
    {
      "epoch": 2.2901333333333334,
      "grad_norm": 0.11383882164955139,
      "learning_rate": 3.5686666666666665e-05,
      "loss": 0.003,
      "step": 42940
    },
    {
      "epoch": 2.2906666666666666,
      "grad_norm": 0.48377057909965515,
      "learning_rate": 3.568333333333334e-05,
      "loss": 0.0047,
      "step": 42950
    },
    {
      "epoch": 2.2912,
      "grad_norm": 2.1333492750130745e-09,
      "learning_rate": 3.5680000000000004e-05,
      "loss": 0.0035,
      "step": 42960
    },
    {
      "epoch": 2.291733333333333,
      "grad_norm": 0.11383616179227829,
      "learning_rate": 3.567666666666667e-05,
      "loss": 0.0029,
      "step": 42970
    },
    {
      "epoch": 2.2922666666666665,
      "grad_norm": 0.3130209743976593,
      "learning_rate": 3.5673333333333336e-05,
      "loss": 0.0029,
      "step": 42980
    },
    {
      "epoch": 2.2928,
      "grad_norm": 0.07222864776849747,
      "learning_rate": 3.567e-05,
      "loss": 0.0041,
      "step": 42990
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 0.4268433153629303,
      "learning_rate": 3.566666666666667e-05,
      "loss": 0.0041,
      "step": 43000
    },
    {
      "epoch": 2.2938666666666667,
      "grad_norm": 0.5122616291046143,
      "learning_rate": 3.5663333333333335e-05,
      "loss": 0.0035,
      "step": 43010
    },
    {
      "epoch": 2.2944,
      "grad_norm": 0.05691352114081383,
      "learning_rate": 3.566e-05,
      "loss": 0.0034,
      "step": 43020
    },
    {
      "epoch": 2.2949333333333333,
      "grad_norm": 0.31304723024368286,
      "learning_rate": 3.565666666666667e-05,
      "loss": 0.0035,
      "step": 43030
    },
    {
      "epoch": 2.2954666666666665,
      "grad_norm": 0.17074325680732727,
      "learning_rate": 3.565333333333333e-05,
      "loss": 0.004,
      "step": 43040
    },
    {
      "epoch": 2.296,
      "grad_norm": 0.17074470221996307,
      "learning_rate": 3.565e-05,
      "loss": 0.0036,
      "step": 43050
    },
    {
      "epoch": 2.2965333333333335,
      "grad_norm": 0.34149202704429626,
      "learning_rate": 3.5646666666666665e-05,
      "loss": 0.0032,
      "step": 43060
    },
    {
      "epoch": 2.297066666666667,
      "grad_norm": 0.3130435645580292,
      "learning_rate": 3.564333333333333e-05,
      "loss": 0.004,
      "step": 43070
    },
    {
      "epoch": 2.2976,
      "grad_norm": 0.1992175132036209,
      "learning_rate": 3.5640000000000004e-05,
      "loss": 0.0031,
      "step": 43080
    },
    {
      "epoch": 2.2981333333333334,
      "grad_norm": 0.028459472581744194,
      "learning_rate": 3.563666666666667e-05,
      "loss": 0.0035,
      "step": 43090
    },
    {
      "epoch": 2.2986666666666666,
      "grad_norm": 0.056961867958307266,
      "learning_rate": 3.563333333333334e-05,
      "loss": 0.0028,
      "step": 43100
    },
    {
      "epoch": 2.2992,
      "grad_norm": 0.1138305589556694,
      "learning_rate": 3.563e-05,
      "loss": 0.0026,
      "step": 43110
    },
    {
      "epoch": 2.299733333333333,
      "grad_norm": 0.22768959403038025,
      "learning_rate": 3.562666666666667e-05,
      "loss": 0.0029,
      "step": 43120
    },
    {
      "epoch": 2.3002666666666665,
      "grad_norm": 0.056915923953056335,
      "learning_rate": 3.5623333333333335e-05,
      "loss": 0.0024,
      "step": 43130
    },
    {
      "epoch": 2.3008,
      "grad_norm": 0.2276800274848938,
      "learning_rate": 3.562e-05,
      "loss": 0.0039,
      "step": 43140
    },
    {
      "epoch": 2.3013333333333335,
      "grad_norm": 0.1992025375366211,
      "learning_rate": 3.561666666666667e-05,
      "loss": 0.0027,
      "step": 43150
    },
    {
      "epoch": 2.3018666666666667,
      "grad_norm": 0.22768093645572662,
      "learning_rate": 3.561333333333334e-05,
      "loss": 0.0039,
      "step": 43160
    },
    {
      "epoch": 2.3024,
      "grad_norm": 0.6829017996788025,
      "learning_rate": 3.5610000000000006e-05,
      "loss": 0.0041,
      "step": 43170
    },
    {
      "epoch": 2.3029333333333333,
      "grad_norm": 0.02845742553472519,
      "learning_rate": 3.5606666666666666e-05,
      "loss": 0.0023,
      "step": 43180
    },
    {
      "epoch": 2.3034666666666666,
      "grad_norm": 0.17074419558048248,
      "learning_rate": 3.560333333333333e-05,
      "loss": 0.0031,
      "step": 43190
    },
    {
      "epoch": 2.304,
      "grad_norm": 0.08547497540712357,
      "learning_rate": 3.56e-05,
      "loss": 0.0033,
      "step": 43200
    },
    {
      "epoch": 2.3045333333333335,
      "grad_norm": 0.0029356232844293118,
      "learning_rate": 3.5596666666666664e-05,
      "loss": 0.0027,
      "step": 43210
    },
    {
      "epoch": 2.305066666666667,
      "grad_norm": 0.056920815259218216,
      "learning_rate": 3.559333333333334e-05,
      "loss": 0.0027,
      "step": 43220
    },
    {
      "epoch": 2.3056,
      "grad_norm": 0.17076106369495392,
      "learning_rate": 3.559e-05,
      "loss": 0.0044,
      "step": 43230
    },
    {
      "epoch": 2.3061333333333334,
      "grad_norm": 0.19919142127037048,
      "learning_rate": 3.558666666666667e-05,
      "loss": 0.0028,
      "step": 43240
    },
    {
      "epoch": 2.3066666666666666,
      "grad_norm": 0.36990687251091003,
      "learning_rate": 3.5583333333333335e-05,
      "loss": 0.0026,
      "step": 43250
    },
    {
      "epoch": 2.3072,
      "grad_norm": 0.08547654002904892,
      "learning_rate": 3.558e-05,
      "loss": 0.0041,
      "step": 43260
    },
    {
      "epoch": 2.307733333333333,
      "grad_norm": 0.3414597809314728,
      "learning_rate": 3.557666666666667e-05,
      "loss": 0.0033,
      "step": 43270
    },
    {
      "epoch": 2.3082666666666665,
      "grad_norm": 0.1422979086637497,
      "learning_rate": 3.5573333333333334e-05,
      "loss": 0.0036,
      "step": 43280
    },
    {
      "epoch": 2.3088,
      "grad_norm": 0.14308218657970428,
      "learning_rate": 3.557e-05,
      "loss": 0.0031,
      "step": 43290
    },
    {
      "epoch": 2.3093333333333335,
      "grad_norm": 0.05874200910329819,
      "learning_rate": 3.556666666666667e-05,
      "loss": 0.0043,
      "step": 43300
    },
    {
      "epoch": 2.3098666666666667,
      "grad_norm": 0.2276403307914734,
      "learning_rate": 3.556333333333334e-05,
      "loss": 0.0025,
      "step": 43310
    },
    {
      "epoch": 2.3104,
      "grad_norm": 0.003640457522124052,
      "learning_rate": 3.5560000000000005e-05,
      "loss": 0.003,
      "step": 43320
    },
    {
      "epoch": 2.3109333333333333,
      "grad_norm": 0.08539464324712753,
      "learning_rate": 3.5556666666666664e-05,
      "loss": 0.0026,
      "step": 43330
    },
    {
      "epoch": 2.3114666666666666,
      "grad_norm": 0.2276199907064438,
      "learning_rate": 3.555333333333333e-05,
      "loss": 0.003,
      "step": 43340
    },
    {
      "epoch": 2.312,
      "grad_norm": 0.25607553124427795,
      "learning_rate": 3.555e-05,
      "loss": 0.0031,
      "step": 43350
    },
    {
      "epoch": 2.3125333333333336,
      "grad_norm": 0.028452672064304352,
      "learning_rate": 3.554666666666667e-05,
      "loss": 0.0028,
      "step": 43360
    },
    {
      "epoch": 2.313066666666667,
      "grad_norm": 0.20039594173431396,
      "learning_rate": 3.5543333333333336e-05,
      "loss": 0.0032,
      "step": 43370
    },
    {
      "epoch": 2.3136,
      "grad_norm": 0.19916553795337677,
      "learning_rate": 3.554e-05,
      "loss": 0.0026,
      "step": 43380
    },
    {
      "epoch": 2.3141333333333334,
      "grad_norm": 0.14225898683071136,
      "learning_rate": 3.553666666666667e-05,
      "loss": 0.0032,
      "step": 43390
    },
    {
      "epoch": 2.3146666666666667,
      "grad_norm": 0.227641761302948,
      "learning_rate": 3.5533333333333334e-05,
      "loss": 0.0044,
      "step": 43400
    },
    {
      "epoch": 2.3152,
      "grad_norm": 0.1171727105975151,
      "learning_rate": 3.553e-05,
      "loss": 0.0039,
      "step": 43410
    },
    {
      "epoch": 2.315733333333333,
      "grad_norm": 0.028452524915337563,
      "learning_rate": 3.5526666666666666e-05,
      "loss": 0.0043,
      "step": 43420
    },
    {
      "epoch": 2.3162666666666665,
      "grad_norm": 0.056905560195446014,
      "learning_rate": 3.552333333333334e-05,
      "loss": 0.0028,
      "step": 43430
    },
    {
      "epoch": 2.3168,
      "grad_norm": 0.22761502861976624,
      "learning_rate": 3.5520000000000006e-05,
      "loss": 0.0047,
      "step": 43440
    },
    {
      "epoch": 2.3173333333333335,
      "grad_norm": 0.39871203899383545,
      "learning_rate": 3.551666666666667e-05,
      "loss": 0.0027,
      "step": 43450
    },
    {
      "epoch": 2.3178666666666667,
      "grad_norm": 0.05690870061516762,
      "learning_rate": 3.551333333333334e-05,
      "loss": 0.0048,
      "step": 43460
    },
    {
      "epoch": 2.3184,
      "grad_norm": 0.5405588150024414,
      "learning_rate": 3.5510000000000004e-05,
      "loss": 0.0034,
      "step": 43470
    },
    {
      "epoch": 2.3189333333333333,
      "grad_norm": 0.056909140199422836,
      "learning_rate": 3.550666666666666e-05,
      "loss": 0.004,
      "step": 43480
    },
    {
      "epoch": 2.3194666666666666,
      "grad_norm": 0.20073546469211578,
      "learning_rate": 3.5503333333333336e-05,
      "loss": 0.0046,
      "step": 43490
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.3137713372707367,
      "learning_rate": 3.55e-05,
      "loss": 0.0057,
      "step": 43500
    },
    {
      "epoch": 2.3205333333333336,
      "grad_norm": 0.2845018208026886,
      "learning_rate": 3.549666666666667e-05,
      "loss": 0.0025,
      "step": 43510
    },
    {
      "epoch": 2.321066666666667,
      "grad_norm": 0.3129609525203705,
      "learning_rate": 3.5493333333333335e-05,
      "loss": 0.0036,
      "step": 43520
    },
    {
      "epoch": 2.3216,
      "grad_norm": 0.028449397534132004,
      "learning_rate": 3.549e-05,
      "loss": 0.0045,
      "step": 43530
    },
    {
      "epoch": 2.3221333333333334,
      "grad_norm": 0.19914685189723969,
      "learning_rate": 3.548666666666667e-05,
      "loss": 0.0044,
      "step": 43540
    },
    {
      "epoch": 2.3226666666666667,
      "grad_norm": 0.1706947684288025,
      "learning_rate": 3.548333333333333e-05,
      "loss": 0.0038,
      "step": 43550
    },
    {
      "epoch": 2.3232,
      "grad_norm": 0.14224408566951752,
      "learning_rate": 3.548e-05,
      "loss": 0.0043,
      "step": 43560
    },
    {
      "epoch": 2.323733333333333,
      "grad_norm": 0.11379434168338776,
      "learning_rate": 3.547666666666667e-05,
      "loss": 0.0071,
      "step": 43570
    },
    {
      "epoch": 2.3242666666666665,
      "grad_norm": 0.3129398226737976,
      "learning_rate": 3.547333333333334e-05,
      "loss": 0.004,
      "step": 43580
    },
    {
      "epoch": 2.3247999999999998,
      "grad_norm": 0.19914725422859192,
      "learning_rate": 3.5470000000000004e-05,
      "loss": 0.0046,
      "step": 43590
    },
    {
      "epoch": 2.3253333333333335,
      "grad_norm": 0.08534649014472961,
      "learning_rate": 3.546666666666667e-05,
      "loss": 0.0037,
      "step": 43600
    },
    {
      "epoch": 2.3258666666666667,
      "grad_norm": 0.19913864135742188,
      "learning_rate": 3.5463333333333337e-05,
      "loss": 0.0038,
      "step": 43610
    },
    {
      "epoch": 2.3264,
      "grad_norm": 0.08534640073776245,
      "learning_rate": 3.546e-05,
      "loss": 0.0065,
      "step": 43620
    },
    {
      "epoch": 2.3269333333333333,
      "grad_norm": 0.25605323910713196,
      "learning_rate": 3.545666666666667e-05,
      "loss": 0.0053,
      "step": 43630
    },
    {
      "epoch": 2.3274666666666666,
      "grad_norm": 0.36982089281082153,
      "learning_rate": 3.5453333333333335e-05,
      "loss": 0.0037,
      "step": 43640
    },
    {
      "epoch": 2.328,
      "grad_norm": 0.19915013015270233,
      "learning_rate": 3.545e-05,
      "loss": 0.0035,
      "step": 43650
    },
    {
      "epoch": 2.3285333333333336,
      "grad_norm": 0.056896451860666275,
      "learning_rate": 3.544666666666667e-05,
      "loss": 0.0027,
      "step": 43660
    },
    {
      "epoch": 2.329066666666667,
      "grad_norm": 0.05690125375986099,
      "learning_rate": 3.544333333333333e-05,
      "loss": 0.0043,
      "step": 43670
    },
    {
      "epoch": 2.3296,
      "grad_norm": 0.2560196816921234,
      "learning_rate": 3.544e-05,
      "loss": 0.0025,
      "step": 43680
    },
    {
      "epoch": 2.3301333333333334,
      "grad_norm": 0.3698568642139435,
      "learning_rate": 3.5436666666666666e-05,
      "loss": 0.0041,
      "step": 43690
    },
    {
      "epoch": 2.3306666666666667,
      "grad_norm": 0.568932831287384,
      "learning_rate": 3.543333333333333e-05,
      "loss": 0.0033,
      "step": 43700
    },
    {
      "epoch": 2.3312,
      "grad_norm": 0.11362157762050629,
      "learning_rate": 3.5430000000000005e-05,
      "loss": 0.0034,
      "step": 43710
    },
    {
      "epoch": 2.331733333333333,
      "grad_norm": 3.013836622238159,
      "learning_rate": 3.542666666666667e-05,
      "loss": 0.0024,
      "step": 43720
    },
    {
      "epoch": 2.3322666666666665,
      "grad_norm": 0.170426145195961,
      "learning_rate": 3.542333333333334e-05,
      "loss": 0.0032,
      "step": 43730
    },
    {
      "epoch": 2.3327999999999998,
      "grad_norm": 0.14202125370502472,
      "learning_rate": 3.542e-05,
      "loss": 0.0021,
      "step": 43740
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.45443859696388245,
      "learning_rate": 3.541666666666667e-05,
      "loss": 0.0023,
      "step": 43750
    },
    {
      "epoch": 2.3338666666666668,
      "grad_norm": 0.056812580674886703,
      "learning_rate": 3.5413333333333335e-05,
      "loss": 0.0032,
      "step": 43760
    },
    {
      "epoch": 2.3344,
      "grad_norm": 0.3408580720424652,
      "learning_rate": 3.541e-05,
      "loss": 0.0048,
      "step": 43770
    },
    {
      "epoch": 2.3349333333333333,
      "grad_norm": 0.28403717279434204,
      "learning_rate": 3.540666666666667e-05,
      "loss": 0.0044,
      "step": 43780
    },
    {
      "epoch": 2.3354666666666666,
      "grad_norm": 0.05680655315518379,
      "learning_rate": 3.5403333333333334e-05,
      "loss": 0.0038,
      "step": 43790
    },
    {
      "epoch": 2.336,
      "grad_norm": 0.028403064236044884,
      "learning_rate": 3.54e-05,
      "loss": 0.0029,
      "step": 43800
    },
    {
      "epoch": 2.3365333333333336,
      "grad_norm": 0.2556457221508026,
      "learning_rate": 3.5396666666666666e-05,
      "loss": 0.0022,
      "step": 43810
    },
    {
      "epoch": 2.337066666666667,
      "grad_norm": 0.11361253261566162,
      "learning_rate": 3.539333333333333e-05,
      "loss": 0.0034,
      "step": 43820
    },
    {
      "epoch": 2.3376,
      "grad_norm": 0.17042025923728943,
      "learning_rate": 3.539e-05,
      "loss": 0.0041,
      "step": 43830
    },
    {
      "epoch": 2.3381333333333334,
      "grad_norm": 0.0852108970284462,
      "learning_rate": 3.538666666666667e-05,
      "loss": 0.0039,
      "step": 43840
    },
    {
      "epoch": 2.3386666666666667,
      "grad_norm": 0.056808169931173325,
      "learning_rate": 3.538333333333334e-05,
      "loss": 0.0033,
      "step": 43850
    },
    {
      "epoch": 2.3392,
      "grad_norm": 0.14202432334423065,
      "learning_rate": 3.5380000000000003e-05,
      "loss": 0.0035,
      "step": 43860
    },
    {
      "epoch": 2.339733333333333,
      "grad_norm": 0.14207936823368073,
      "learning_rate": 3.537666666666667e-05,
      "loss": 0.0038,
      "step": 43870
    },
    {
      "epoch": 2.3402666666666665,
      "grad_norm": 0.3124247193336487,
      "learning_rate": 3.5373333333333336e-05,
      "loss": 0.0032,
      "step": 43880
    },
    {
      "epoch": 2.3407999999999998,
      "grad_norm": 0.08521389961242676,
      "learning_rate": 3.537e-05,
      "loss": 0.0042,
      "step": 43890
    },
    {
      "epoch": 2.3413333333333335,
      "grad_norm": 0.05695153400301933,
      "learning_rate": 3.536666666666667e-05,
      "loss": 0.0035,
      "step": 43900
    },
    {
      "epoch": 2.3418666666666668,
      "grad_norm": 0.08534429967403412,
      "learning_rate": 3.5363333333333334e-05,
      "loss": 0.0028,
      "step": 43910
    },
    {
      "epoch": 2.3424,
      "grad_norm": 0.1991194188594818,
      "learning_rate": 3.536000000000001e-05,
      "loss": 0.0045,
      "step": 43920
    },
    {
      "epoch": 2.3429333333333333,
      "grad_norm": 0.1706889569759369,
      "learning_rate": 3.5356666666666666e-05,
      "loss": 0.0031,
      "step": 43930
    },
    {
      "epoch": 2.3434666666666666,
      "grad_norm": 0.056896258145570755,
      "learning_rate": 3.535333333333333e-05,
      "loss": 0.0032,
      "step": 43940
    },
    {
      "epoch": 2.344,
      "grad_norm": 0.08534552156925201,
      "learning_rate": 3.535e-05,
      "loss": 0.0041,
      "step": 43950
    },
    {
      "epoch": 2.3445333333333336,
      "grad_norm": 0.2560369074344635,
      "learning_rate": 3.5346666666666665e-05,
      "loss": 0.0033,
      "step": 43960
    },
    {
      "epoch": 2.345066666666667,
      "grad_norm": 0.14225006103515625,
      "learning_rate": 3.534333333333333e-05,
      "loss": 0.002,
      "step": 43970
    },
    {
      "epoch": 2.3456,
      "grad_norm": 0.4266972839832306,
      "learning_rate": 3.5340000000000004e-05,
      "loss": 0.0013,
      "step": 43980
    },
    {
      "epoch": 2.3461333333333334,
      "grad_norm": 0.02844884619116783,
      "learning_rate": 3.533666666666667e-05,
      "loss": 0.0037,
      "step": 43990
    },
    {
      "epoch": 2.3466666666666667,
      "grad_norm": 0.17069046199321747,
      "learning_rate": 3.5333333333333336e-05,
      "loss": 0.0028,
      "step": 44000
    },
    {
      "epoch": 2.3472,
      "grad_norm": 0.2560230791568756,
      "learning_rate": 3.533e-05,
      "loss": 0.0046,
      "step": 44010
    },
    {
      "epoch": 2.3477333333333332,
      "grad_norm": 0.11379627138376236,
      "learning_rate": 3.532666666666667e-05,
      "loss": 0.0031,
      "step": 44020
    },
    {
      "epoch": 2.3482666666666665,
      "grad_norm": 0.08534464240074158,
      "learning_rate": 3.5323333333333335e-05,
      "loss": 0.0036,
      "step": 44030
    },
    {
      "epoch": 2.3487999999999998,
      "grad_norm": 0.17068956792354584,
      "learning_rate": 3.532e-05,
      "loss": 0.0025,
      "step": 44040
    },
    {
      "epoch": 2.3493333333333335,
      "grad_norm": 0.398293137550354,
      "learning_rate": 3.531666666666667e-05,
      "loss": 0.004,
      "step": 44050
    },
    {
      "epoch": 2.3498666666666668,
      "grad_norm": 0.028447046875953674,
      "learning_rate": 3.531333333333334e-05,
      "loss": 0.0037,
      "step": 44060
    },
    {
      "epoch": 2.3504,
      "grad_norm": 0.057275690138339996,
      "learning_rate": 3.5310000000000006e-05,
      "loss": 0.0039,
      "step": 44070
    },
    {
      "epoch": 2.3509333333333333,
      "grad_norm": 0.05689486861228943,
      "learning_rate": 3.5306666666666665e-05,
      "loss": 0.003,
      "step": 44080
    },
    {
      "epoch": 2.3514666666666666,
      "grad_norm": 0.1710372418165207,
      "learning_rate": 3.530333333333333e-05,
      "loss": 0.0033,
      "step": 44090
    },
    {
      "epoch": 2.352,
      "grad_norm": 2.0446955240061016e-09,
      "learning_rate": 3.53e-05,
      "loss": 0.003,
      "step": 44100
    },
    {
      "epoch": 2.352533333333333,
      "grad_norm": 0.14215287566184998,
      "learning_rate": 3.5296666666666664e-05,
      "loss": 0.0028,
      "step": 44110
    },
    {
      "epoch": 2.353066666666667,
      "grad_norm": 0.028450332581996918,
      "learning_rate": 3.5293333333333336e-05,
      "loss": 0.0045,
      "step": 44120
    },
    {
      "epoch": 2.3536,
      "grad_norm": 0.37079209089279175,
      "learning_rate": 3.529e-05,
      "loss": 0.0025,
      "step": 44130
    },
    {
      "epoch": 2.3541333333333334,
      "grad_norm": 0.4552404582500458,
      "learning_rate": 3.528666666666667e-05,
      "loss": 0.0023,
      "step": 44140
    },
    {
      "epoch": 2.3546666666666667,
      "grad_norm": 0.11446993798017502,
      "learning_rate": 3.5283333333333335e-05,
      "loss": 0.0028,
      "step": 44150
    },
    {
      "epoch": 2.3552,
      "grad_norm": 0.028664259240031242,
      "learning_rate": 3.528e-05,
      "loss": 0.0023,
      "step": 44160
    },
    {
      "epoch": 2.3557333333333332,
      "grad_norm": 0.043390244245529175,
      "learning_rate": 3.527666666666667e-05,
      "loss": 0.003,
      "step": 44170
    },
    {
      "epoch": 2.3562666666666665,
      "grad_norm": 0.11382444947957993,
      "learning_rate": 3.527333333333333e-05,
      "loss": 0.0031,
      "step": 44180
    },
    {
      "epoch": 2.3568,
      "grad_norm": 0.6260160207748413,
      "learning_rate": 3.5270000000000006e-05,
      "loss": 0.0031,
      "step": 44190
    },
    {
      "epoch": 2.3573333333333335,
      "grad_norm": 0.08550769090652466,
      "learning_rate": 3.526666666666667e-05,
      "loss": 0.0032,
      "step": 44200
    },
    {
      "epoch": 2.3578666666666668,
      "grad_norm": 0.48871946334838867,
      "learning_rate": 3.526333333333334e-05,
      "loss": 0.004,
      "step": 44210
    },
    {
      "epoch": 2.3584,
      "grad_norm": 0.1268092840909958,
      "learning_rate": 3.5260000000000005e-05,
      "loss": 0.0032,
      "step": 44220
    },
    {
      "epoch": 2.3589333333333333,
      "grad_norm": 0.2576088607311249,
      "learning_rate": 3.5256666666666664e-05,
      "loss": 0.0035,
      "step": 44230
    },
    {
      "epoch": 2.3594666666666666,
      "grad_norm": 0.08535563200712204,
      "learning_rate": 3.525333333333333e-05,
      "loss": 0.003,
      "step": 44240
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.08535163104534149,
      "learning_rate": 3.525e-05,
      "loss": 0.0024,
      "step": 44250
    },
    {
      "epoch": 2.360533333333333,
      "grad_norm": 0.10046997666358948,
      "learning_rate": 3.524666666666667e-05,
      "loss": 0.0019,
      "step": 44260
    },
    {
      "epoch": 2.361066666666667,
      "grad_norm": 0.3245513439178467,
      "learning_rate": 3.5243333333333335e-05,
      "loss": 0.0035,
      "step": 44270
    },
    {
      "epoch": 2.3616,
      "grad_norm": 0.3698819875717163,
      "learning_rate": 3.524e-05,
      "loss": 0.0029,
      "step": 44280
    },
    {
      "epoch": 2.3621333333333334,
      "grad_norm": 0.1595957726240158,
      "learning_rate": 3.523666666666667e-05,
      "loss": 0.0032,
      "step": 44290
    },
    {
      "epoch": 2.3626666666666667,
      "grad_norm": 0.10959446430206299,
      "learning_rate": 3.5233333333333334e-05,
      "loss": 0.0029,
      "step": 44300
    },
    {
      "epoch": 2.3632,
      "grad_norm": 0.056904107332229614,
      "learning_rate": 3.523e-05,
      "loss": 0.004,
      "step": 44310
    },
    {
      "epoch": 2.3637333333333332,
      "grad_norm": 0.23595817387104034,
      "learning_rate": 3.5226666666666666e-05,
      "loss": 0.0028,
      "step": 44320
    },
    {
      "epoch": 2.3642666666666665,
      "grad_norm": 0.056902676820755005,
      "learning_rate": 3.522333333333334e-05,
      "loss": 0.0025,
      "step": 44330
    },
    {
      "epoch": 2.3648,
      "grad_norm": 0.2845377027988434,
      "learning_rate": 3.5220000000000005e-05,
      "loss": 0.0029,
      "step": 44340
    },
    {
      "epoch": 2.3653333333333335,
      "grad_norm": 0.028451651334762573,
      "learning_rate": 3.521666666666667e-05,
      "loss": 0.0023,
      "step": 44350
    },
    {
      "epoch": 2.365866666666667,
      "grad_norm": 0.056906312704086304,
      "learning_rate": 3.521333333333334e-05,
      "loss": 0.0035,
      "step": 44360
    },
    {
      "epoch": 2.3664,
      "grad_norm": 0.17071902751922607,
      "learning_rate": 3.5210000000000003e-05,
      "loss": 0.0021,
      "step": 44370
    },
    {
      "epoch": 2.3669333333333333,
      "grad_norm": 0.0854460671544075,
      "learning_rate": 3.520666666666667e-05,
      "loss": 0.0033,
      "step": 44380
    },
    {
      "epoch": 2.3674666666666666,
      "grad_norm": 0.14241580665111542,
      "learning_rate": 3.5203333333333336e-05,
      "loss": 0.0026,
      "step": 44390
    },
    {
      "epoch": 2.368,
      "grad_norm": 0.4382861852645874,
      "learning_rate": 3.52e-05,
      "loss": 0.0031,
      "step": 44400
    },
    {
      "epoch": 2.368533333333333,
      "grad_norm": 0.45578089356422424,
      "learning_rate": 3.519666666666667e-05,
      "loss": 0.0026,
      "step": 44410
    },
    {
      "epoch": 2.369066666666667,
      "grad_norm": 0.17743316292762756,
      "learning_rate": 3.5193333333333334e-05,
      "loss": 0.0053,
      "step": 44420
    },
    {
      "epoch": 2.3696,
      "grad_norm": 0.16103939712047577,
      "learning_rate": 3.519e-05,
      "loss": 0.0031,
      "step": 44430
    },
    {
      "epoch": 2.3701333333333334,
      "grad_norm": 0.05697581544518471,
      "learning_rate": 3.5186666666666666e-05,
      "loss": 0.0034,
      "step": 44440
    },
    {
      "epoch": 2.3706666666666667,
      "grad_norm": 0.1424294114112854,
      "learning_rate": 3.518333333333333e-05,
      "loss": 0.0034,
      "step": 44450
    },
    {
      "epoch": 2.3712,
      "grad_norm": 0.056968897581100464,
      "learning_rate": 3.518e-05,
      "loss": 0.0028,
      "step": 44460
    },
    {
      "epoch": 2.3717333333333332,
      "grad_norm": 0.11393653601408005,
      "learning_rate": 3.517666666666667e-05,
      "loss": 0.0032,
      "step": 44470
    },
    {
      "epoch": 2.3722666666666665,
      "grad_norm": 0.08545240014791489,
      "learning_rate": 3.517333333333334e-05,
      "loss": 0.0025,
      "step": 44480
    },
    {
      "epoch": 2.3728,
      "grad_norm": 0.14245717227458954,
      "learning_rate": 3.5170000000000004e-05,
      "loss": 0.0031,
      "step": 44490
    },
    {
      "epoch": 2.3733333333333335,
      "grad_norm": 0.11393808573484421,
      "learning_rate": 3.516666666666667e-05,
      "loss": 0.0025,
      "step": 44500
    },
    {
      "epoch": 2.373866666666667,
      "grad_norm": 0.05716748163104057,
      "learning_rate": 3.5163333333333336e-05,
      "loss": 0.0036,
      "step": 44510
    },
    {
      "epoch": 2.3744,
      "grad_norm": 1.0460502464226806e-09,
      "learning_rate": 3.516e-05,
      "loss": 0.0023,
      "step": 44520
    },
    {
      "epoch": 2.3749333333333333,
      "grad_norm": 0.11393868923187256,
      "learning_rate": 3.515666666666667e-05,
      "loss": 0.0032,
      "step": 44530
    },
    {
      "epoch": 2.3754666666666666,
      "grad_norm": 0.028483306989073753,
      "learning_rate": 3.5153333333333334e-05,
      "loss": 0.0024,
      "step": 44540
    },
    {
      "epoch": 2.376,
      "grad_norm": 0.17090317606925964,
      "learning_rate": 3.515e-05,
      "loss": 0.0044,
      "step": 44550
    },
    {
      "epoch": 2.376533333333333,
      "grad_norm": 0.31333649158477783,
      "learning_rate": 3.514666666666667e-05,
      "loss": 0.0037,
      "step": 44560
    },
    {
      "epoch": 2.377066666666667,
      "grad_norm": 0.8939721584320068,
      "learning_rate": 3.514333333333333e-05,
      "loss": 0.0027,
      "step": 44570
    },
    {
      "epoch": 2.3776,
      "grad_norm": 0.6900438666343689,
      "learning_rate": 3.514e-05,
      "loss": 0.0028,
      "step": 44580
    },
    {
      "epoch": 2.3781333333333334,
      "grad_norm": 0.11393896490335464,
      "learning_rate": 3.5136666666666665e-05,
      "loss": 0.0031,
      "step": 44590
    },
    {
      "epoch": 2.3786666666666667,
      "grad_norm": 0.028486065566539764,
      "learning_rate": 3.513333333333334e-05,
      "loss": 0.0027,
      "step": 44600
    },
    {
      "epoch": 2.3792,
      "grad_norm": 0.028502082452178,
      "learning_rate": 3.5130000000000004e-05,
      "loss": 0.0036,
      "step": 44610
    },
    {
      "epoch": 2.3797333333333333,
      "grad_norm": 0.14242394268512726,
      "learning_rate": 3.512666666666667e-05,
      "loss": 0.0037,
      "step": 44620
    },
    {
      "epoch": 2.3802666666666665,
      "grad_norm": 0.028484653681516647,
      "learning_rate": 3.5123333333333336e-05,
      "loss": 0.0028,
      "step": 44630
    },
    {
      "epoch": 2.3808,
      "grad_norm": 0.1424194723367691,
      "learning_rate": 3.512e-05,
      "loss": 0.003,
      "step": 44640
    },
    {
      "epoch": 2.3813333333333335,
      "grad_norm": 0.05697038397192955,
      "learning_rate": 3.511666666666667e-05,
      "loss": 0.0028,
      "step": 44650
    },
    {
      "epoch": 2.381866666666667,
      "grad_norm": 0.028484657406806946,
      "learning_rate": 3.5113333333333335e-05,
      "loss": 0.0034,
      "step": 44660
    },
    {
      "epoch": 2.3824,
      "grad_norm": 0.18036609888076782,
      "learning_rate": 3.511e-05,
      "loss": 0.0039,
      "step": 44670
    },
    {
      "epoch": 2.3829333333333333,
      "grad_norm": 0.19939762353897095,
      "learning_rate": 3.5106666666666674e-05,
      "loss": 0.0042,
      "step": 44680
    },
    {
      "epoch": 2.3834666666666666,
      "grad_norm": 0.42724618315696716,
      "learning_rate": 3.510333333333333e-05,
      "loss": 0.0034,
      "step": 44690
    },
    {
      "epoch": 2.384,
      "grad_norm": 0.11393482983112335,
      "learning_rate": 3.51e-05,
      "loss": 0.0029,
      "step": 44700
    },
    {
      "epoch": 2.384533333333333,
      "grad_norm": 0.17088735103607178,
      "learning_rate": 3.5096666666666665e-05,
      "loss": 0.0031,
      "step": 44710
    },
    {
      "epoch": 2.385066666666667,
      "grad_norm": 0.2563551366329193,
      "learning_rate": 3.509333333333333e-05,
      "loss": 0.0043,
      "step": 44720
    },
    {
      "epoch": 2.3856,
      "grad_norm": 0.1994079351425171,
      "learning_rate": 3.509e-05,
      "loss": 0.002,
      "step": 44730
    },
    {
      "epoch": 2.3861333333333334,
      "grad_norm": 0.1424131691455841,
      "learning_rate": 3.508666666666667e-05,
      "loss": 0.0035,
      "step": 44740
    },
    {
      "epoch": 2.3866666666666667,
      "grad_norm": 0.08544740825891495,
      "learning_rate": 3.508333333333334e-05,
      "loss": 0.0023,
      "step": 44750
    },
    {
      "epoch": 2.3872,
      "grad_norm": 0.34178969264030457,
      "learning_rate": 3.508e-05,
      "loss": 0.0028,
      "step": 44760
    },
    {
      "epoch": 2.3877333333333333,
      "grad_norm": 0.19937850534915924,
      "learning_rate": 3.507666666666667e-05,
      "loss": 0.0044,
      "step": 44770
    },
    {
      "epoch": 2.3882666666666665,
      "grad_norm": 0.1708952635526657,
      "learning_rate": 3.5073333333333335e-05,
      "loss": 0.0037,
      "step": 44780
    },
    {
      "epoch": 2.3888,
      "grad_norm": 0.11392633616924286,
      "learning_rate": 3.507e-05,
      "loss": 0.0025,
      "step": 44790
    },
    {
      "epoch": 2.389333333333333,
      "grad_norm": 0.17090383172035217,
      "learning_rate": 3.506666666666667e-05,
      "loss": 0.003,
      "step": 44800
    },
    {
      "epoch": 2.389866666666667,
      "grad_norm": 0.14241942763328552,
      "learning_rate": 3.5063333333333334e-05,
      "loss": 0.0028,
      "step": 44810
    },
    {
      "epoch": 2.3904,
      "grad_norm": 0.028481807559728622,
      "learning_rate": 3.5060000000000007e-05,
      "loss": 0.0025,
      "step": 44820
    },
    {
      "epoch": 2.3909333333333334,
      "grad_norm": 3.2587856857446695e-09,
      "learning_rate": 3.505666666666667e-05,
      "loss": 0.0022,
      "step": 44830
    },
    {
      "epoch": 2.3914666666666666,
      "grad_norm": 0.05696482956409454,
      "learning_rate": 3.505333333333333e-05,
      "loss": 0.0032,
      "step": 44840
    },
    {
      "epoch": 2.392,
      "grad_norm": 0.19936753809452057,
      "learning_rate": 3.505e-05,
      "loss": 0.0035,
      "step": 44850
    },
    {
      "epoch": 2.392533333333333,
      "grad_norm": 0.02848096378147602,
      "learning_rate": 3.5046666666666664e-05,
      "loss": 0.0041,
      "step": 44860
    },
    {
      "epoch": 2.393066666666667,
      "grad_norm": 0.14240185916423798,
      "learning_rate": 3.504333333333333e-05,
      "loss": 0.0028,
      "step": 44870
    },
    {
      "epoch": 2.3936,
      "grad_norm": 0.1708911806344986,
      "learning_rate": 3.504e-05,
      "loss": 0.0044,
      "step": 44880
    },
    {
      "epoch": 2.3941333333333334,
      "grad_norm": 0.05696417763829231,
      "learning_rate": 3.503666666666667e-05,
      "loss": 0.0036,
      "step": 44890
    },
    {
      "epoch": 2.3946666666666667,
      "grad_norm": 0.19937311112880707,
      "learning_rate": 3.5033333333333336e-05,
      "loss": 0.0022,
      "step": 44900
    },
    {
      "epoch": 2.3952,
      "grad_norm": 0.1424126923084259,
      "learning_rate": 3.503e-05,
      "loss": 0.0027,
      "step": 44910
    },
    {
      "epoch": 2.3957333333333333,
      "grad_norm": 0.028483230620622635,
      "learning_rate": 3.502666666666667e-05,
      "loss": 0.0044,
      "step": 44920
    },
    {
      "epoch": 2.3962666666666665,
      "grad_norm": 0.08545049279928207,
      "learning_rate": 3.5023333333333334e-05,
      "loss": 0.0042,
      "step": 44930
    },
    {
      "epoch": 2.3968,
      "grad_norm": 4.011039500539937e-09,
      "learning_rate": 3.502e-05,
      "loss": 0.0024,
      "step": 44940
    },
    {
      "epoch": 2.397333333333333,
      "grad_norm": 0.34176570177078247,
      "learning_rate": 3.501666666666667e-05,
      "loss": 0.0022,
      "step": 44950
    },
    {
      "epoch": 2.397866666666667,
      "grad_norm": 0.08544592559337616,
      "learning_rate": 3.501333333333334e-05,
      "loss": 0.0034,
      "step": 44960
    },
    {
      "epoch": 2.3984,
      "grad_norm": 0.11392557621002197,
      "learning_rate": 3.5010000000000005e-05,
      "loss": 0.0027,
      "step": 44970
    },
    {
      "epoch": 2.3989333333333334,
      "grad_norm": 0.028481513261795044,
      "learning_rate": 3.500666666666667e-05,
      "loss": 0.0033,
      "step": 44980
    },
    {
      "epoch": 2.3994666666666666,
      "grad_norm": 0.14240247011184692,
      "learning_rate": 3.500333333333333e-05,
      "loss": 0.0035,
      "step": 44990
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.22784994542598724,
      "learning_rate": 3.5e-05,
      "loss": 0.003,
      "step": 45000
    },
    {
      "epoch": 2.400533333333333,
      "grad_norm": 0.28481024503707886,
      "learning_rate": 3.499666666666667e-05,
      "loss": 0.0022,
      "step": 45010
    },
    {
      "epoch": 2.401066666666667,
      "grad_norm": 0.22784891724586487,
      "learning_rate": 3.4993333333333336e-05,
      "loss": 0.0036,
      "step": 45020
    },
    {
      "epoch": 2.4016,
      "grad_norm": 0.17088446021080017,
      "learning_rate": 3.499e-05,
      "loss": 0.0036,
      "step": 45030
    },
    {
      "epoch": 2.4021333333333335,
      "grad_norm": 0.22783732414245605,
      "learning_rate": 3.498666666666667e-05,
      "loss": 0.002,
      "step": 45040
    },
    {
      "epoch": 2.4026666666666667,
      "grad_norm": 0.08544277399778366,
      "learning_rate": 3.4983333333333334e-05,
      "loss": 0.003,
      "step": 45050
    },
    {
      "epoch": 2.4032,
      "grad_norm": 0.11391979455947876,
      "learning_rate": 3.498e-05,
      "loss": 0.0023,
      "step": 45060
    },
    {
      "epoch": 2.4037333333333333,
      "grad_norm": 0.22662344574928284,
      "learning_rate": 3.497666666666667e-05,
      "loss": 0.0036,
      "step": 45070
    },
    {
      "epoch": 2.4042666666666666,
      "grad_norm": 0.2847883999347687,
      "learning_rate": 3.497333333333333e-05,
      "loss": 0.0024,
      "step": 45080
    },
    {
      "epoch": 2.4048,
      "grad_norm": 0.1139129251241684,
      "learning_rate": 3.4970000000000006e-05,
      "loss": 0.0028,
      "step": 45090
    },
    {
      "epoch": 2.405333333333333,
      "grad_norm": 0.05695776641368866,
      "learning_rate": 3.496666666666667e-05,
      "loss": 0.0036,
      "step": 45100
    },
    {
      "epoch": 2.405866666666667,
      "grad_norm": 0.05695603787899017,
      "learning_rate": 3.496333333333334e-05,
      "loss": 0.0033,
      "step": 45110
    },
    {
      "epoch": 2.4064,
      "grad_norm": 0.028479348868131638,
      "learning_rate": 3.4960000000000004e-05,
      "loss": 0.0037,
      "step": 45120
    },
    {
      "epoch": 2.4069333333333334,
      "grad_norm": 0.19934329390525818,
      "learning_rate": 3.495666666666667e-05,
      "loss": 0.0048,
      "step": 45130
    },
    {
      "epoch": 2.4074666666666666,
      "grad_norm": 0.05696040764451027,
      "learning_rate": 3.495333333333333e-05,
      "loss": 0.0034,
      "step": 45140
    },
    {
      "epoch": 2.408,
      "grad_norm": 0.17086957395076752,
      "learning_rate": 3.495e-05,
      "loss": 0.0024,
      "step": 45150
    },
    {
      "epoch": 2.408533333333333,
      "grad_norm": 0.512651801109314,
      "learning_rate": 3.494666666666667e-05,
      "loss": 0.0027,
      "step": 45160
    },
    {
      "epoch": 2.409066666666667,
      "grad_norm": 0.1423865556716919,
      "learning_rate": 3.4943333333333335e-05,
      "loss": 0.002,
      "step": 45170
    },
    {
      "epoch": 2.4096,
      "grad_norm": 0.05695491284132004,
      "learning_rate": 3.494e-05,
      "loss": 0.0017,
      "step": 45180
    },
    {
      "epoch": 2.4101333333333335,
      "grad_norm": 0.14238779246807098,
      "learning_rate": 3.493666666666667e-05,
      "loss": 0.0036,
      "step": 45190
    },
    {
      "epoch": 2.4106666666666667,
      "grad_norm": 7.144762004251959e-10,
      "learning_rate": 3.493333333333333e-05,
      "loss": 0.0035,
      "step": 45200
    },
    {
      "epoch": 2.4112,
      "grad_norm": 0.1139044314622879,
      "learning_rate": 3.493e-05,
      "loss": 0.0029,
      "step": 45210
    },
    {
      "epoch": 2.4117333333333333,
      "grad_norm": 0.0854310467839241,
      "learning_rate": 3.4926666666666665e-05,
      "loss": 0.0033,
      "step": 45220
    },
    {
      "epoch": 2.4122666666666666,
      "grad_norm": 0.17084841430187225,
      "learning_rate": 3.492333333333334e-05,
      "loss": 0.0023,
      "step": 45230
    },
    {
      "epoch": 2.4128,
      "grad_norm": 0.9196593165397644,
      "learning_rate": 3.4920000000000004e-05,
      "loss": 0.0038,
      "step": 45240
    },
    {
      "epoch": 2.413333333333333,
      "grad_norm": 1.000540852546692,
      "learning_rate": 3.491666666666667e-05,
      "loss": 0.0039,
      "step": 45250
    },
    {
      "epoch": 2.413866666666667,
      "grad_norm": 0.7119602560997009,
      "learning_rate": 3.491333333333334e-05,
      "loss": 0.0036,
      "step": 45260
    },
    {
      "epoch": 2.4144,
      "grad_norm": 0.22783781588077545,
      "learning_rate": 3.491e-05,
      "loss": 0.002,
      "step": 45270
    },
    {
      "epoch": 2.4149333333333334,
      "grad_norm": 0.6264472603797913,
      "learning_rate": 3.490666666666667e-05,
      "loss": 0.0024,
      "step": 45280
    },
    {
      "epoch": 2.4154666666666667,
      "grad_norm": 0.056950781494379044,
      "learning_rate": 3.4903333333333335e-05,
      "loss": 0.0032,
      "step": 45290
    },
    {
      "epoch": 2.416,
      "grad_norm": 0.14237834513187408,
      "learning_rate": 3.49e-05,
      "loss": 0.0031,
      "step": 45300
    },
    {
      "epoch": 2.416533333333333,
      "grad_norm": 0.2278037816286087,
      "learning_rate": 3.489666666666667e-05,
      "loss": 0.0026,
      "step": 45310
    },
    {
      "epoch": 2.4170666666666665,
      "grad_norm": 0.31323742866516113,
      "learning_rate": 3.4893333333333334e-05,
      "loss": 0.0027,
      "step": 45320
    },
    {
      "epoch": 2.4176,
      "grad_norm": 0.056954361498355865,
      "learning_rate": 3.489e-05,
      "loss": 0.0028,
      "step": 45330
    },
    {
      "epoch": 2.4181333333333335,
      "grad_norm": 0.4840465784072876,
      "learning_rate": 3.4886666666666666e-05,
      "loss": 0.0022,
      "step": 45340
    },
    {
      "epoch": 2.4186666666666667,
      "grad_norm": 0.4556320607662201,
      "learning_rate": 3.488333333333333e-05,
      "loss": 0.0034,
      "step": 45350
    },
    {
      "epoch": 2.4192,
      "grad_norm": 0.28473061323165894,
      "learning_rate": 3.4880000000000005e-05,
      "loss": 0.0044,
      "step": 45360
    },
    {
      "epoch": 2.4197333333333333,
      "grad_norm": 0.08542432636022568,
      "learning_rate": 3.487666666666667e-05,
      "loss": 0.0019,
      "step": 45370
    },
    {
      "epoch": 2.4202666666666666,
      "grad_norm": 0.31319659948349,
      "learning_rate": 3.487333333333334e-05,
      "loss": 0.0037,
      "step": 45380
    },
    {
      "epoch": 2.4208,
      "grad_norm": 0.11389550566673279,
      "learning_rate": 3.487e-05,
      "loss": 0.0035,
      "step": 45390
    },
    {
      "epoch": 2.421333333333333,
      "grad_norm": 0.05694686993956566,
      "learning_rate": 3.486666666666667e-05,
      "loss": 0.0027,
      "step": 45400
    },
    {
      "epoch": 2.421866666666667,
      "grad_norm": 0.2847285568714142,
      "learning_rate": 3.4863333333333336e-05,
      "loss": 0.0031,
      "step": 45410
    },
    {
      "epoch": 2.4224,
      "grad_norm": 0.08542263507843018,
      "learning_rate": 3.486e-05,
      "loss": 0.0039,
      "step": 45420
    },
    {
      "epoch": 2.4229333333333334,
      "grad_norm": 0.34168317914009094,
      "learning_rate": 3.485666666666667e-05,
      "loss": 0.003,
      "step": 45430
    },
    {
      "epoch": 2.4234666666666667,
      "grad_norm": 0.056948937475681305,
      "learning_rate": 3.4853333333333334e-05,
      "loss": 0.0033,
      "step": 45440
    },
    {
      "epoch": 2.424,
      "grad_norm": 0.05694560706615448,
      "learning_rate": 3.485e-05,
      "loss": 0.0028,
      "step": 45450
    },
    {
      "epoch": 2.424533333333333,
      "grad_norm": 0.056946150958538055,
      "learning_rate": 3.4846666666666666e-05,
      "loss": 0.0033,
      "step": 45460
    },
    {
      "epoch": 2.4250666666666665,
      "grad_norm": 0.029637116938829422,
      "learning_rate": 3.484333333333333e-05,
      "loss": 0.0032,
      "step": 45470
    },
    {
      "epoch": 2.4256,
      "grad_norm": 0.14237134158611298,
      "learning_rate": 3.484e-05,
      "loss": 0.0027,
      "step": 45480
    },
    {
      "epoch": 2.4261333333333335,
      "grad_norm": 0.8868601322174072,
      "learning_rate": 3.4836666666666665e-05,
      "loss": 0.0027,
      "step": 45490
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 0.08542165905237198,
      "learning_rate": 3.483333333333334e-05,
      "loss": 0.0026,
      "step": 45500
    },
    {
      "epoch": 2.4272,
      "grad_norm": 0.14236797392368317,
      "learning_rate": 3.4830000000000004e-05,
      "loss": 0.0036,
      "step": 45510
    },
    {
      "epoch": 2.4277333333333333,
      "grad_norm": 0.14236314594745636,
      "learning_rate": 3.482666666666667e-05,
      "loss": 0.0023,
      "step": 45520
    },
    {
      "epoch": 2.4282666666666666,
      "grad_norm": 0.056948307901620865,
      "learning_rate": 3.4823333333333336e-05,
      "loss": 0.0024,
      "step": 45530
    },
    {
      "epoch": 2.4288,
      "grad_norm": 1.4682067783766684e-09,
      "learning_rate": 3.482e-05,
      "loss": 0.0029,
      "step": 45540
    },
    {
      "epoch": 2.429333333333333,
      "grad_norm": 0.08541841059923172,
      "learning_rate": 3.481666666666667e-05,
      "loss": 0.0021,
      "step": 45550
    },
    {
      "epoch": 2.429866666666667,
      "grad_norm": 0.1708395928144455,
      "learning_rate": 3.4813333333333334e-05,
      "loss": 0.0019,
      "step": 45560
    },
    {
      "epoch": 2.4304,
      "grad_norm": 3.460230102447781e-09,
      "learning_rate": 3.481e-05,
      "loss": 0.0034,
      "step": 45570
    },
    {
      "epoch": 2.4309333333333334,
      "grad_norm": 0.22778303921222687,
      "learning_rate": 3.480666666666667e-05,
      "loss": 0.0029,
      "step": 45580
    },
    {
      "epoch": 2.4314666666666667,
      "grad_norm": 0.05694491043686867,
      "learning_rate": 3.480333333333333e-05,
      "loss": 0.0027,
      "step": 45590
    },
    {
      "epoch": 2.432,
      "grad_norm": 0.05694374814629555,
      "learning_rate": 3.48e-05,
      "loss": 0.0037,
      "step": 45600
    },
    {
      "epoch": 2.432533333333333,
      "grad_norm": 0.14236468076705933,
      "learning_rate": 3.4796666666666665e-05,
      "loss": 0.0031,
      "step": 45610
    },
    {
      "epoch": 2.4330666666666665,
      "grad_norm": 0.14236105978488922,
      "learning_rate": 3.479333333333333e-05,
      "loss": 0.002,
      "step": 45620
    },
    {
      "epoch": 2.4336,
      "grad_norm": 0.3416661024093628,
      "learning_rate": 3.479e-05,
      "loss": 0.0032,
      "step": 45630
    },
    {
      "epoch": 2.4341333333333335,
      "grad_norm": 0.2562495470046997,
      "learning_rate": 3.478666666666667e-05,
      "loss": 0.0031,
      "step": 45640
    },
    {
      "epoch": 2.4346666666666668,
      "grad_norm": 0.3131870925426483,
      "learning_rate": 3.4783333333333336e-05,
      "loss": 0.003,
      "step": 45650
    },
    {
      "epoch": 2.4352,
      "grad_norm": 0.39863651990890503,
      "learning_rate": 3.478e-05,
      "loss": 0.0018,
      "step": 45660
    },
    {
      "epoch": 2.4357333333333333,
      "grad_norm": 0.030457941815257072,
      "learning_rate": 3.477666666666667e-05,
      "loss": 0.0036,
      "step": 45670
    },
    {
      "epoch": 2.4362666666666666,
      "grad_norm": 0.2562500834465027,
      "learning_rate": 3.4773333333333335e-05,
      "loss": 0.0038,
      "step": 45680
    },
    {
      "epoch": 2.4368,
      "grad_norm": 9.045627580306359e-10,
      "learning_rate": 3.477e-05,
      "loss": 0.0028,
      "step": 45690
    },
    {
      "epoch": 2.437333333333333,
      "grad_norm": 0.1138865053653717,
      "learning_rate": 3.476666666666667e-05,
      "loss": 0.0019,
      "step": 45700
    },
    {
      "epoch": 2.437866666666667,
      "grad_norm": 0.3986402750015259,
      "learning_rate": 3.476333333333334e-05,
      "loss": 0.0021,
      "step": 45710
    },
    {
      "epoch": 2.4384,
      "grad_norm": 0.34165728092193604,
      "learning_rate": 3.4760000000000006e-05,
      "loss": 0.0025,
      "step": 45720
    },
    {
      "epoch": 2.4389333333333334,
      "grad_norm": 0.22776682674884796,
      "learning_rate": 3.475666666666667e-05,
      "loss": 0.0031,
      "step": 45730
    },
    {
      "epoch": 2.4394666666666667,
      "grad_norm": 0.08541858941316605,
      "learning_rate": 3.475333333333334e-05,
      "loss": 0.0048,
      "step": 45740
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.14236219227313995,
      "learning_rate": 3.475e-05,
      "loss": 0.0034,
      "step": 45750
    },
    {
      "epoch": 2.440533333333333,
      "grad_norm": 0.05694367364048958,
      "learning_rate": 3.4746666666666664e-05,
      "loss": 0.0027,
      "step": 45760
    },
    {
      "epoch": 2.4410666666666665,
      "grad_norm": 0.056942835450172424,
      "learning_rate": 3.474333333333334e-05,
      "loss": 0.0038,
      "step": 45770
    },
    {
      "epoch": 2.4416,
      "grad_norm": 0.0854155495762825,
      "learning_rate": 3.474e-05,
      "loss": 0.0023,
      "step": 45780
    },
    {
      "epoch": 2.4421333333333335,
      "grad_norm": 0.028470607474446297,
      "learning_rate": 3.473666666666667e-05,
      "loss": 0.0046,
      "step": 45790
    },
    {
      "epoch": 2.4426666666666668,
      "grad_norm": 0.3986164629459381,
      "learning_rate": 3.4733333333333335e-05,
      "loss": 0.0035,
      "step": 45800
    },
    {
      "epoch": 2.4432,
      "grad_norm": 0.4839877188205719,
      "learning_rate": 3.473e-05,
      "loss": 0.0026,
      "step": 45810
    },
    {
      "epoch": 2.4437333333333333,
      "grad_norm": 0.02847115881741047,
      "learning_rate": 3.472666666666667e-05,
      "loss": 0.0043,
      "step": 45820
    },
    {
      "epoch": 2.4442666666666666,
      "grad_norm": 0.056944340467453,
      "learning_rate": 3.4723333333333333e-05,
      "loss": 0.0032,
      "step": 45830
    },
    {
      "epoch": 2.4448,
      "grad_norm": 0.11388061195611954,
      "learning_rate": 3.472e-05,
      "loss": 0.0033,
      "step": 45840
    },
    {
      "epoch": 2.445333333333333,
      "grad_norm": 0.05693693459033966,
      "learning_rate": 3.471666666666667e-05,
      "loss": 0.0029,
      "step": 45850
    },
    {
      "epoch": 2.445866666666667,
      "grad_norm": 0.2846827805042267,
      "learning_rate": 3.471333333333334e-05,
      "loss": 0.0035,
      "step": 45860
    },
    {
      "epoch": 2.4464,
      "grad_norm": 0.11387471109628677,
      "learning_rate": 3.4710000000000005e-05,
      "loss": 0.0026,
      "step": 45870
    },
    {
      "epoch": 2.4469333333333334,
      "grad_norm": 0.25623270869255066,
      "learning_rate": 3.470666666666667e-05,
      "loss": 0.0033,
      "step": 45880
    },
    {
      "epoch": 2.4474666666666667,
      "grad_norm": 0.02846972830593586,
      "learning_rate": 3.470333333333334e-05,
      "loss": 0.0031,
      "step": 45890
    },
    {
      "epoch": 2.448,
      "grad_norm": 0.22775357961654663,
      "learning_rate": 3.4699999999999996e-05,
      "loss": 0.0032,
      "step": 45900
    },
    {
      "epoch": 2.4485333333333332,
      "grad_norm": 2.6041087064498925e-09,
      "learning_rate": 3.469666666666667e-05,
      "loss": 0.0034,
      "step": 45910
    },
    {
      "epoch": 2.4490666666666665,
      "grad_norm": 0.02846904657781124,
      "learning_rate": 3.4693333333333335e-05,
      "loss": 0.0031,
      "step": 45920
    },
    {
      "epoch": 2.4496,
      "grad_norm": 0.11388111114501953,
      "learning_rate": 3.469e-05,
      "loss": 0.0035,
      "step": 45930
    },
    {
      "epoch": 2.4501333333333335,
      "grad_norm": 0.2562066614627838,
      "learning_rate": 3.468666666666667e-05,
      "loss": 0.0035,
      "step": 45940
    },
    {
      "epoch": 2.4506666666666668,
      "grad_norm": 0.14234888553619385,
      "learning_rate": 3.4683333333333334e-05,
      "loss": 0.0029,
      "step": 45950
    },
    {
      "epoch": 2.4512,
      "grad_norm": 3.45739015195079e-09,
      "learning_rate": 3.468e-05,
      "loss": 0.0031,
      "step": 45960
    },
    {
      "epoch": 2.4517333333333333,
      "grad_norm": 0.14234650135040283,
      "learning_rate": 3.4676666666666666e-05,
      "loss": 0.0032,
      "step": 45970
    },
    {
      "epoch": 2.4522666666666666,
      "grad_norm": 0.11386868357658386,
      "learning_rate": 3.467333333333333e-05,
      "loss": 0.004,
      "step": 45980
    },
    {
      "epoch": 2.4528,
      "grad_norm": 0.3131755590438843,
      "learning_rate": 3.4670000000000005e-05,
      "loss": 0.0044,
      "step": 45990
    },
    {
      "epoch": 2.453333333333333,
      "grad_norm": 0.028466761112213135,
      "learning_rate": 3.466666666666667e-05,
      "loss": 0.0031,
      "step": 46000
    },
    {
      "epoch": 2.4538666666666664,
      "grad_norm": 0.2846987545490265,
      "learning_rate": 3.466333333333334e-05,
      "loss": 0.0032,
      "step": 46010
    },
    {
      "epoch": 2.4544,
      "grad_norm": 0.08540204912424088,
      "learning_rate": 3.4660000000000004e-05,
      "loss": 0.0026,
      "step": 46020
    },
    {
      "epoch": 2.4549333333333334,
      "grad_norm": 0.22774219512939453,
      "learning_rate": 3.465666666666667e-05,
      "loss": 0.0028,
      "step": 46030
    },
    {
      "epoch": 2.4554666666666667,
      "grad_norm": 1.3615169525146484,
      "learning_rate": 3.4653333333333336e-05,
      "loss": 0.0028,
      "step": 46040
    },
    {
      "epoch": 2.456,
      "grad_norm": 0.4270482361316681,
      "learning_rate": 3.465e-05,
      "loss": 0.0026,
      "step": 46050
    },
    {
      "epoch": 2.4565333333333332,
      "grad_norm": 0.22773802280426025,
      "learning_rate": 3.464666666666667e-05,
      "loss": 0.0029,
      "step": 46060
    },
    {
      "epoch": 2.4570666666666665,
      "grad_norm": 0.39855730533599854,
      "learning_rate": 3.4643333333333334e-05,
      "loss": 0.0031,
      "step": 46070
    },
    {
      "epoch": 2.4576000000000002,
      "grad_norm": 0.5693970322608948,
      "learning_rate": 3.464e-05,
      "loss": 0.0031,
      "step": 46080
    },
    {
      "epoch": 2.4581333333333335,
      "grad_norm": 1.397412657737732,
      "learning_rate": 3.4636666666666667e-05,
      "loss": 0.0024,
      "step": 46090
    },
    {
      "epoch": 2.458666666666667,
      "grad_norm": 0.056934140622615814,
      "learning_rate": 3.463333333333333e-05,
      "loss": 0.0024,
      "step": 46100
    },
    {
      "epoch": 2.4592,
      "grad_norm": 0.22775332629680634,
      "learning_rate": 3.463e-05,
      "loss": 0.0052,
      "step": 46110
    },
    {
      "epoch": 2.4597333333333333,
      "grad_norm": 0.1423247903585434,
      "learning_rate": 3.462666666666667e-05,
      "loss": 0.0039,
      "step": 46120
    },
    {
      "epoch": 2.4602666666666666,
      "grad_norm": 0.6035927534103394,
      "learning_rate": 3.462333333333334e-05,
      "loss": 0.0026,
      "step": 46130
    },
    {
      "epoch": 2.4608,
      "grad_norm": 0.25619208812713623,
      "learning_rate": 3.4620000000000004e-05,
      "loss": 0.0054,
      "step": 46140
    },
    {
      "epoch": 2.461333333333333,
      "grad_norm": 0.02846471406519413,
      "learning_rate": 3.461666666666667e-05,
      "loss": 0.0019,
      "step": 46150
    },
    {
      "epoch": 2.4618666666666664,
      "grad_norm": 0.05692831426858902,
      "learning_rate": 3.4613333333333336e-05,
      "loss": 0.0024,
      "step": 46160
    },
    {
      "epoch": 2.4624,
      "grad_norm": 0.19925332069396973,
      "learning_rate": 3.461e-05,
      "loss": 0.0021,
      "step": 46170
    },
    {
      "epoch": 2.4629333333333334,
      "grad_norm": 0.028464704751968384,
      "learning_rate": 3.460666666666667e-05,
      "loss": 0.0027,
      "step": 46180
    },
    {
      "epoch": 2.4634666666666667,
      "grad_norm": 0.31309211254119873,
      "learning_rate": 3.4603333333333335e-05,
      "loss": 0.0039,
      "step": 46190
    },
    {
      "epoch": 2.464,
      "grad_norm": 0.05692702904343605,
      "learning_rate": 3.46e-05,
      "loss": 0.0041,
      "step": 46200
    },
    {
      "epoch": 2.4645333333333332,
      "grad_norm": 0.25617825984954834,
      "learning_rate": 3.459666666666667e-05,
      "loss": 0.0037,
      "step": 46210
    },
    {
      "epoch": 2.4650666666666665,
      "grad_norm": 0.14295752346515656,
      "learning_rate": 3.459333333333333e-05,
      "loss": 0.0027,
      "step": 46220
    },
    {
      "epoch": 2.4656000000000002,
      "grad_norm": 2.7805804325709005e-09,
      "learning_rate": 3.459e-05,
      "loss": 0.003,
      "step": 46230
    },
    {
      "epoch": 2.4661333333333335,
      "grad_norm": 0.08539287745952606,
      "learning_rate": 3.4586666666666665e-05,
      "loss": 0.002,
      "step": 46240
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 3.153858285642741e-09,
      "learning_rate": 3.458333333333333e-05,
      "loss": 0.0024,
      "step": 46250
    },
    {
      "epoch": 2.4672,
      "grad_norm": 2.306606461388583e-09,
      "learning_rate": 3.4580000000000004e-05,
      "loss": 0.0021,
      "step": 46260
    },
    {
      "epoch": 2.4677333333333333,
      "grad_norm": 0.1423240453004837,
      "learning_rate": 3.457666666666667e-05,
      "loss": 0.0028,
      "step": 46270
    },
    {
      "epoch": 2.4682666666666666,
      "grad_norm": 0.3415413796901703,
      "learning_rate": 3.4573333333333337e-05,
      "loss": 0.0024,
      "step": 46280
    },
    {
      "epoch": 2.4688,
      "grad_norm": 0.05692768841981888,
      "learning_rate": 3.457e-05,
      "loss": 0.0031,
      "step": 46290
    },
    {
      "epoch": 2.469333333333333,
      "grad_norm": 0.17077775299549103,
      "learning_rate": 3.456666666666667e-05,
      "loss": 0.003,
      "step": 46300
    },
    {
      "epoch": 2.4698666666666664,
      "grad_norm": 0.31307902932167053,
      "learning_rate": 3.4563333333333335e-05,
      "loss": 0.0016,
      "step": 46310
    },
    {
      "epoch": 2.4704,
      "grad_norm": 0.39846423268318176,
      "learning_rate": 3.456e-05,
      "loss": 0.0024,
      "step": 46320
    },
    {
      "epoch": 2.4709333333333334,
      "grad_norm": 0.19925184547901154,
      "learning_rate": 3.455666666666667e-05,
      "loss": 0.0023,
      "step": 46330
    },
    {
      "epoch": 2.4714666666666667,
      "grad_norm": 0.256147563457489,
      "learning_rate": 3.455333333333334e-05,
      "loss": 0.0029,
      "step": 46340
    },
    {
      "epoch": 2.472,
      "grad_norm": 0.028463758528232574,
      "learning_rate": 3.455e-05,
      "loss": 0.0034,
      "step": 46350
    },
    {
      "epoch": 2.4725333333333332,
      "grad_norm": 0.08538349717855453,
      "learning_rate": 3.4546666666666666e-05,
      "loss": 0.0028,
      "step": 46360
    },
    {
      "epoch": 2.4730666666666665,
      "grad_norm": 1.420415896014049e-09,
      "learning_rate": 3.454333333333333e-05,
      "loss": 0.0021,
      "step": 46370
    },
    {
      "epoch": 2.4736000000000002,
      "grad_norm": 0.19922760128974915,
      "learning_rate": 3.454e-05,
      "loss": 0.0021,
      "step": 46380
    },
    {
      "epoch": 2.4741333333333335,
      "grad_norm": 0.17077764868736267,
      "learning_rate": 3.4536666666666664e-05,
      "loss": 0.002,
      "step": 46390
    },
    {
      "epoch": 2.474666666666667,
      "grad_norm": 0.2561400830745697,
      "learning_rate": 3.453333333333334e-05,
      "loss": 0.0025,
      "step": 46400
    },
    {
      "epoch": 2.4752,
      "grad_norm": 0.4554116427898407,
      "learning_rate": 3.453e-05,
      "loss": 0.0031,
      "step": 46410
    },
    {
      "epoch": 2.4757333333333333,
      "grad_norm": 0.08551963418722153,
      "learning_rate": 3.452666666666667e-05,
      "loss": 0.0031,
      "step": 46420
    },
    {
      "epoch": 2.4762666666666666,
      "grad_norm": 0.4553649127483368,
      "learning_rate": 3.4523333333333335e-05,
      "loss": 0.0025,
      "step": 46430
    },
    {
      "epoch": 2.4768,
      "grad_norm": 0.05691853538155556,
      "learning_rate": 3.452e-05,
      "loss": 0.0029,
      "step": 46440
    },
    {
      "epoch": 2.477333333333333,
      "grad_norm": 0.2846105992794037,
      "learning_rate": 3.451666666666667e-05,
      "loss": 0.0037,
      "step": 46450
    },
    {
      "epoch": 2.4778666666666664,
      "grad_norm": 0.2846040427684784,
      "learning_rate": 3.4513333333333334e-05,
      "loss": 0.0031,
      "step": 46460
    },
    {
      "epoch": 2.4784,
      "grad_norm": 0.14229902625083923,
      "learning_rate": 3.451000000000001e-05,
      "loss": 0.0034,
      "step": 46470
    },
    {
      "epoch": 2.4789333333333334,
      "grad_norm": 0.08537586033344269,
      "learning_rate": 3.450666666666667e-05,
      "loss": 0.0038,
      "step": 46480
    },
    {
      "epoch": 2.4794666666666667,
      "grad_norm": 0.056921154260635376,
      "learning_rate": 3.450333333333334e-05,
      "loss": 0.0028,
      "step": 46490
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.17075839638710022,
      "learning_rate": 3.45e-05,
      "loss": 0.0032,
      "step": 46500
    },
    {
      "epoch": 2.4805333333333333,
      "grad_norm": 0.028459614142775536,
      "learning_rate": 3.4496666666666664e-05,
      "loss": 0.0015,
      "step": 46510
    },
    {
      "epoch": 2.4810666666666665,
      "grad_norm": 0.39846622943878174,
      "learning_rate": 3.449333333333333e-05,
      "loss": 0.003,
      "step": 46520
    },
    {
      "epoch": 2.4816,
      "grad_norm": 0.08537985384464264,
      "learning_rate": 3.449e-05,
      "loss": 0.0027,
      "step": 46530
    },
    {
      "epoch": 2.4821333333333335,
      "grad_norm": 0.1992255598306656,
      "learning_rate": 3.448666666666667e-05,
      "loss": 0.002,
      "step": 46540
    },
    {
      "epoch": 2.482666666666667,
      "grad_norm": 0.11384464800357819,
      "learning_rate": 3.4483333333333336e-05,
      "loss": 0.0032,
      "step": 46550
    },
    {
      "epoch": 2.4832,
      "grad_norm": 0.17075373232364655,
      "learning_rate": 3.448e-05,
      "loss": 0.0025,
      "step": 46560
    },
    {
      "epoch": 2.4837333333333333,
      "grad_norm": 0.02846211940050125,
      "learning_rate": 3.447666666666667e-05,
      "loss": 0.0039,
      "step": 46570
    },
    {
      "epoch": 2.4842666666666666,
      "grad_norm": 0.3984186053276062,
      "learning_rate": 3.4473333333333334e-05,
      "loss": 0.0038,
      "step": 46580
    },
    {
      "epoch": 2.4848,
      "grad_norm": 0.056919556111097336,
      "learning_rate": 3.447e-05,
      "loss": 0.004,
      "step": 46590
    },
    {
      "epoch": 2.485333333333333,
      "grad_norm": 0.142287477850914,
      "learning_rate": 3.4466666666666666e-05,
      "loss": 0.0035,
      "step": 46600
    },
    {
      "epoch": 2.4858666666666664,
      "grad_norm": 0.28459179401397705,
      "learning_rate": 3.446333333333334e-05,
      "loss": 0.0027,
      "step": 46610
    },
    {
      "epoch": 2.4864,
      "grad_norm": 0.0853751078248024,
      "learning_rate": 3.4460000000000005e-05,
      "loss": 0.0036,
      "step": 46620
    },
    {
      "epoch": 2.4869333333333334,
      "grad_norm": 0.11382795870304108,
      "learning_rate": 3.445666666666667e-05,
      "loss": 0.0039,
      "step": 46630
    },
    {
      "epoch": 2.4874666666666667,
      "grad_norm": 0.11383365839719772,
      "learning_rate": 3.445333333333334e-05,
      "loss": 0.0031,
      "step": 46640
    },
    {
      "epoch": 2.488,
      "grad_norm": 0.05691702663898468,
      "learning_rate": 3.445e-05,
      "loss": 0.0026,
      "step": 46650
    },
    {
      "epoch": 2.4885333333333333,
      "grad_norm": 0.5122735500335693,
      "learning_rate": 3.444666666666666e-05,
      "loss": 0.0028,
      "step": 46660
    },
    {
      "epoch": 2.4890666666666665,
      "grad_norm": 0.2276579886674881,
      "learning_rate": 3.4443333333333336e-05,
      "loss": 0.0025,
      "step": 46670
    },
    {
      "epoch": 2.4896,
      "grad_norm": 0.11383018642663956,
      "learning_rate": 3.444e-05,
      "loss": 0.0034,
      "step": 46680
    },
    {
      "epoch": 2.4901333333333335,
      "grad_norm": 0.14228594303131104,
      "learning_rate": 3.443666666666667e-05,
      "loss": 0.003,
      "step": 46690
    },
    {
      "epoch": 2.490666666666667,
      "grad_norm": 0.3984169661998749,
      "learning_rate": 3.4433333333333335e-05,
      "loss": 0.0041,
      "step": 46700
    },
    {
      "epoch": 2.4912,
      "grad_norm": 0.19920143485069275,
      "learning_rate": 3.443e-05,
      "loss": 0.0028,
      "step": 46710
    },
    {
      "epoch": 2.4917333333333334,
      "grad_norm": 0.17073695361614227,
      "learning_rate": 3.442666666666667e-05,
      "loss": 0.0033,
      "step": 46720
    },
    {
      "epoch": 2.4922666666666666,
      "grad_norm": 0.08537755906581879,
      "learning_rate": 3.442333333333333e-05,
      "loss": 0.0036,
      "step": 46730
    },
    {
      "epoch": 2.4928,
      "grad_norm": 0.14227880537509918,
      "learning_rate": 3.442e-05,
      "loss": 0.002,
      "step": 46740
    },
    {
      "epoch": 2.493333333333333,
      "grad_norm": 0.028456462547183037,
      "learning_rate": 3.441666666666667e-05,
      "loss": 0.0046,
      "step": 46750
    },
    {
      "epoch": 2.4938666666666665,
      "grad_norm": 0.14228764176368713,
      "learning_rate": 3.441333333333334e-05,
      "loss": 0.0034,
      "step": 46760
    },
    {
      "epoch": 2.4944,
      "grad_norm": 0.2845463752746582,
      "learning_rate": 3.4410000000000004e-05,
      "loss": 0.0032,
      "step": 46770
    },
    {
      "epoch": 2.4949333333333334,
      "grad_norm": 0.0853651836514473,
      "learning_rate": 3.440666666666667e-05,
      "loss": 0.0033,
      "step": 46780
    },
    {
      "epoch": 2.4954666666666667,
      "grad_norm": 0.05690930411219597,
      "learning_rate": 3.4403333333333337e-05,
      "loss": 0.0036,
      "step": 46790
    },
    {
      "epoch": 2.496,
      "grad_norm": 0.19918543100357056,
      "learning_rate": 3.4399999999999996e-05,
      "loss": 0.0029,
      "step": 46800
    },
    {
      "epoch": 2.4965333333333333,
      "grad_norm": 0.08536243438720703,
      "learning_rate": 3.439666666666667e-05,
      "loss": 0.0036,
      "step": 46810
    },
    {
      "epoch": 2.4970666666666665,
      "grad_norm": 0.5614519119262695,
      "learning_rate": 3.4393333333333335e-05,
      "loss": 0.0028,
      "step": 46820
    },
    {
      "epoch": 2.4976,
      "grad_norm": 0.037059955298900604,
      "learning_rate": 3.439e-05,
      "loss": 0.0033,
      "step": 46830
    },
    {
      "epoch": 2.4981333333333335,
      "grad_norm": 0.05690545588731766,
      "learning_rate": 3.438666666666667e-05,
      "loss": 0.0026,
      "step": 46840
    },
    {
      "epoch": 2.498666666666667,
      "grad_norm": 0.14225846529006958,
      "learning_rate": 3.438333333333333e-05,
      "loss": 0.0031,
      "step": 46850
    },
    {
      "epoch": 2.4992,
      "grad_norm": 0.028451336547732353,
      "learning_rate": 3.438e-05,
      "loss": 0.004,
      "step": 46860
    },
    {
      "epoch": 2.4997333333333334,
      "grad_norm": 0.17071513831615448,
      "learning_rate": 3.4376666666666666e-05,
      "loss": 0.0027,
      "step": 46870
    },
    {
      "epoch": 2.5002666666666666,
      "grad_norm": 0.3129624128341675,
      "learning_rate": 3.437333333333334e-05,
      "loss": 0.0043,
      "step": 46880
    },
    {
      "epoch": 2.5008,
      "grad_norm": 0.3129779100418091,
      "learning_rate": 3.4370000000000005e-05,
      "loss": 0.0029,
      "step": 46890
    },
    {
      "epoch": 2.501333333333333,
      "grad_norm": 0.08534707874059677,
      "learning_rate": 3.436666666666667e-05,
      "loss": 0.0027,
      "step": 46900
    },
    {
      "epoch": 2.5018666666666665,
      "grad_norm": 0.09721106290817261,
      "learning_rate": 3.436333333333334e-05,
      "loss": 0.0025,
      "step": 46910
    },
    {
      "epoch": 2.5023999999999997,
      "grad_norm": 0.31293848156929016,
      "learning_rate": 3.436e-05,
      "loss": 0.0029,
      "step": 46920
    },
    {
      "epoch": 2.5029333333333335,
      "grad_norm": 0.17070956528186798,
      "learning_rate": 3.435666666666667e-05,
      "loss": 0.002,
      "step": 46930
    },
    {
      "epoch": 2.5034666666666667,
      "grad_norm": 0.05690191313624382,
      "learning_rate": 3.4353333333333335e-05,
      "loss": 0.0035,
      "step": 46940
    },
    {
      "epoch": 2.504,
      "grad_norm": 0.11379696428775787,
      "learning_rate": 3.435e-05,
      "loss": 0.0029,
      "step": 46950
    },
    {
      "epoch": 2.5045333333333333,
      "grad_norm": 0.31294935941696167,
      "learning_rate": 3.434666666666667e-05,
      "loss": 0.0037,
      "step": 46960
    },
    {
      "epoch": 2.5050666666666666,
      "grad_norm": 1.5463561542361504e-09,
      "learning_rate": 3.4343333333333334e-05,
      "loss": 0.003,
      "step": 46970
    },
    {
      "epoch": 2.5056000000000003,
      "grad_norm": 0.6543713212013245,
      "learning_rate": 3.434e-05,
      "loss": 0.0034,
      "step": 46980
    },
    {
      "epoch": 2.5061333333333335,
      "grad_norm": 0.36983153223991394,
      "learning_rate": 3.4336666666666666e-05,
      "loss": 0.0039,
      "step": 46990
    },
    {
      "epoch": 2.506666666666667,
      "grad_norm": 0.19913357496261597,
      "learning_rate": 3.433333333333333e-05,
      "loss": 0.0028,
      "step": 47000
    },
    {
      "epoch": 2.5072,
      "grad_norm": 0.11379391700029373,
      "learning_rate": 3.433e-05,
      "loss": 0.0038,
      "step": 47010
    },
    {
      "epoch": 2.5077333333333334,
      "grad_norm": 0.19914557039737701,
      "learning_rate": 3.432666666666667e-05,
      "loss": 0.0042,
      "step": 47020
    },
    {
      "epoch": 2.5082666666666666,
      "grad_norm": 0.11380403488874435,
      "learning_rate": 3.432333333333334e-05,
      "loss": 0.0041,
      "step": 47030
    },
    {
      "epoch": 2.5088,
      "grad_norm": 0.11380410194396973,
      "learning_rate": 3.4320000000000003e-05,
      "loss": 0.0025,
      "step": 47040
    },
    {
      "epoch": 2.509333333333333,
      "grad_norm": 0.34138303995132446,
      "learning_rate": 3.431666666666667e-05,
      "loss": 0.0025,
      "step": 47050
    },
    {
      "epoch": 2.5098666666666665,
      "grad_norm": 0.1707097738981247,
      "learning_rate": 3.4313333333333336e-05,
      "loss": 0.0029,
      "step": 47060
    },
    {
      "epoch": 2.5103999999999997,
      "grad_norm": 0.05689651519060135,
      "learning_rate": 3.431e-05,
      "loss": 0.0026,
      "step": 47070
    },
    {
      "epoch": 2.5109333333333335,
      "grad_norm": 0.06820889562368393,
      "learning_rate": 3.430666666666667e-05,
      "loss": 0.0052,
      "step": 47080
    },
    {
      "epoch": 2.5114666666666667,
      "grad_norm": 0.22760461270809174,
      "learning_rate": 3.4303333333333334e-05,
      "loss": 0.0034,
      "step": 47090
    },
    {
      "epoch": 2.512,
      "grad_norm": 1.065644383430481,
      "learning_rate": 3.430000000000001e-05,
      "loss": 0.0029,
      "step": 47100
    },
    {
      "epoch": 2.5125333333333333,
      "grad_norm": 0.1712646335363388,
      "learning_rate": 3.4296666666666666e-05,
      "loss": 0.0024,
      "step": 47110
    },
    {
      "epoch": 2.5130666666666666,
      "grad_norm": 0.28453317284584045,
      "learning_rate": 3.429333333333333e-05,
      "loss": 0.0047,
      "step": 47120
    },
    {
      "epoch": 2.5136,
      "grad_norm": 0.11379854381084442,
      "learning_rate": 3.429e-05,
      "loss": 0.007,
      "step": 47130
    },
    {
      "epoch": 2.5141333333333336,
      "grad_norm": 0.17067991197109222,
      "learning_rate": 3.4286666666666665e-05,
      "loss": 0.003,
      "step": 47140
    },
    {
      "epoch": 2.514666666666667,
      "grad_norm": 0.08533602207899094,
      "learning_rate": 3.428333333333333e-05,
      "loss": 0.0028,
      "step": 47150
    },
    {
      "epoch": 2.5152,
      "grad_norm": 0.028445333242416382,
      "learning_rate": 3.4280000000000004e-05,
      "loss": 0.0027,
      "step": 47160
    },
    {
      "epoch": 2.5157333333333334,
      "grad_norm": 3.842740792237009e-09,
      "learning_rate": 3.427666666666667e-05,
      "loss": 0.004,
      "step": 47170
    },
    {
      "epoch": 2.5162666666666667,
      "grad_norm": 0.19913548231124878,
      "learning_rate": 3.4273333333333336e-05,
      "loss": 0.0016,
      "step": 47180
    },
    {
      "epoch": 2.5168,
      "grad_norm": 0.08533596992492676,
      "learning_rate": 3.427e-05,
      "loss": 0.0044,
      "step": 47190
    },
    {
      "epoch": 2.517333333333333,
      "grad_norm": 0.11378613859415054,
      "learning_rate": 3.426666666666667e-05,
      "loss": 0.0034,
      "step": 47200
    },
    {
      "epoch": 2.5178666666666665,
      "grad_norm": 0.05689512565732002,
      "learning_rate": 3.4263333333333334e-05,
      "loss": 0.0036,
      "step": 47210
    },
    {
      "epoch": 2.5183999999999997,
      "grad_norm": 0.08534841984510422,
      "learning_rate": 3.426e-05,
      "loss": 0.0033,
      "step": 47220
    },
    {
      "epoch": 2.5189333333333335,
      "grad_norm": 1.942979110935994e-09,
      "learning_rate": 3.4256666666666674e-05,
      "loss": 0.0021,
      "step": 47230
    },
    {
      "epoch": 2.5194666666666667,
      "grad_norm": 3.6119334190232166e-09,
      "learning_rate": 3.425333333333334e-05,
      "loss": 0.0044,
      "step": 47240
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.056896355003118515,
      "learning_rate": 3.4250000000000006e-05,
      "loss": 0.003,
      "step": 47250
    },
    {
      "epoch": 2.5205333333333333,
      "grad_norm": 0.39825356006622314,
      "learning_rate": 3.4246666666666665e-05,
      "loss": 0.0052,
      "step": 47260
    },
    {
      "epoch": 2.5210666666666666,
      "grad_norm": 0.028446976095438004,
      "learning_rate": 3.424333333333333e-05,
      "loss": 0.0023,
      "step": 47270
    },
    {
      "epoch": 2.5216,
      "grad_norm": 0.3431580066680908,
      "learning_rate": 3.424e-05,
      "loss": 0.0048,
      "step": 47280
    },
    {
      "epoch": 2.5221333333333336,
      "grad_norm": 0.17068113386631012,
      "learning_rate": 3.4236666666666664e-05,
      "loss": 0.003,
      "step": 47290
    },
    {
      "epoch": 2.522666666666667,
      "grad_norm": 0.1706787496805191,
      "learning_rate": 3.4233333333333336e-05,
      "loss": 0.0024,
      "step": 47300
    },
    {
      "epoch": 2.5232,
      "grad_norm": 0.1422342211008072,
      "learning_rate": 3.423e-05,
      "loss": 0.0034,
      "step": 47310
    },
    {
      "epoch": 2.5237333333333334,
      "grad_norm": 0.14224031567573547,
      "learning_rate": 3.422666666666667e-05,
      "loss": 0.0035,
      "step": 47320
    },
    {
      "epoch": 2.5242666666666667,
      "grad_norm": 0.08533840626478195,
      "learning_rate": 3.4223333333333335e-05,
      "loss": 0.0024,
      "step": 47330
    },
    {
      "epoch": 2.5248,
      "grad_norm": 0.1422395259141922,
      "learning_rate": 3.422e-05,
      "loss": 0.0022,
      "step": 47340
    },
    {
      "epoch": 2.525333333333333,
      "grad_norm": 0.028444234281778336,
      "learning_rate": 3.421666666666667e-05,
      "loss": 0.0029,
      "step": 47350
    },
    {
      "epoch": 2.5258666666666665,
      "grad_norm": 0.512061357498169,
      "learning_rate": 3.421333333333333e-05,
      "loss": 0.0028,
      "step": 47360
    },
    {
      "epoch": 2.5263999999999998,
      "grad_norm": 0.11378256231546402,
      "learning_rate": 3.4210000000000006e-05,
      "loss": 0.0022,
      "step": 47370
    },
    {
      "epoch": 2.5269333333333335,
      "grad_norm": 0.02844797447323799,
      "learning_rate": 3.420666666666667e-05,
      "loss": 0.0028,
      "step": 47380
    },
    {
      "epoch": 2.5274666666666668,
      "grad_norm": 0.1435873955488205,
      "learning_rate": 3.420333333333334e-05,
      "loss": 0.0041,
      "step": 47390
    },
    {
      "epoch": 2.528,
      "grad_norm": 0.028447028249502182,
      "learning_rate": 3.4200000000000005e-05,
      "loss": 0.004,
      "step": 47400
    },
    {
      "epoch": 2.5285333333333333,
      "grad_norm": 0.028445111587643623,
      "learning_rate": 3.4196666666666664e-05,
      "loss": 0.004,
      "step": 47410
    },
    {
      "epoch": 2.5290666666666666,
      "grad_norm": 0.0568942129611969,
      "learning_rate": 3.419333333333333e-05,
      "loss": 0.0028,
      "step": 47420
    },
    {
      "epoch": 2.5296,
      "grad_norm": 0.1706804633140564,
      "learning_rate": 3.419e-05,
      "loss": 0.0023,
      "step": 47430
    },
    {
      "epoch": 2.5301333333333336,
      "grad_norm": 0.25599223375320435,
      "learning_rate": 3.418666666666667e-05,
      "loss": 0.0036,
      "step": 47440
    },
    {
      "epoch": 2.530666666666667,
      "grad_norm": 0.17068208754062653,
      "learning_rate": 3.4183333333333335e-05,
      "loss": 0.0025,
      "step": 47450
    },
    {
      "epoch": 2.5312,
      "grad_norm": 0.22756630182266235,
      "learning_rate": 3.418e-05,
      "loss": 0.0024,
      "step": 47460
    },
    {
      "epoch": 2.5317333333333334,
      "grad_norm": 0.08533904701471329,
      "learning_rate": 3.417666666666667e-05,
      "loss": 0.0031,
      "step": 47470
    },
    {
      "epoch": 2.5322666666666667,
      "grad_norm": 0.17067347466945648,
      "learning_rate": 3.4173333333333334e-05,
      "loss": 0.0036,
      "step": 47480
    },
    {
      "epoch": 2.5328,
      "grad_norm": 0.028446068987250328,
      "learning_rate": 3.417e-05,
      "loss": 0.0047,
      "step": 47490
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 1.7532914009521505e-09,
      "learning_rate": 3.4166666666666666e-05,
      "loss": 0.0029,
      "step": 47500
    },
    {
      "epoch": 2.5338666666666665,
      "grad_norm": 0.4551529586315155,
      "learning_rate": 3.416333333333334e-05,
      "loss": 0.0024,
      "step": 47510
    },
    {
      "epoch": 2.5343999999999998,
      "grad_norm": 0.6257564425468445,
      "learning_rate": 3.4160000000000005e-05,
      "loss": 0.0023,
      "step": 47520
    },
    {
      "epoch": 2.5349333333333335,
      "grad_norm": 0.08533040434122086,
      "learning_rate": 3.415666666666667e-05,
      "loss": 0.0031,
      "step": 47530
    },
    {
      "epoch": 2.5354666666666668,
      "grad_norm": 0.19910968840122223,
      "learning_rate": 3.415333333333334e-05,
      "loss": 0.0035,
      "step": 47540
    },
    {
      "epoch": 2.536,
      "grad_norm": 0.19910557568073273,
      "learning_rate": 3.415e-05,
      "loss": 0.0029,
      "step": 47550
    },
    {
      "epoch": 2.5365333333333333,
      "grad_norm": 0.08533080667257309,
      "learning_rate": 3.414666666666666e-05,
      "loss": 0.0035,
      "step": 47560
    },
    {
      "epoch": 2.5370666666666666,
      "grad_norm": 0.08533734083175659,
      "learning_rate": 3.4143333333333336e-05,
      "loss": 0.0028,
      "step": 47570
    },
    {
      "epoch": 2.5376,
      "grad_norm": 0.08533408492803574,
      "learning_rate": 3.414e-05,
      "loss": 0.003,
      "step": 47580
    },
    {
      "epoch": 2.5381333333333336,
      "grad_norm": 4.652667584537085e-09,
      "learning_rate": 3.413666666666667e-05,
      "loss": 0.003,
      "step": 47590
    },
    {
      "epoch": 2.538666666666667,
      "grad_norm": 0.08532807976007462,
      "learning_rate": 3.4133333333333334e-05,
      "loss": 0.0031,
      "step": 47600
    },
    {
      "epoch": 2.5392,
      "grad_norm": 0.19911368191242218,
      "learning_rate": 3.413e-05,
      "loss": 0.0029,
      "step": 47610
    },
    {
      "epoch": 2.5397333333333334,
      "grad_norm": 0.2844267785549164,
      "learning_rate": 3.4126666666666666e-05,
      "loss": 0.0024,
      "step": 47620
    },
    {
      "epoch": 2.5402666666666667,
      "grad_norm": 0.02844412997364998,
      "learning_rate": 3.412333333333333e-05,
      "loss": 0.0032,
      "step": 47630
    },
    {
      "epoch": 2.5408,
      "grad_norm": 0.08532846719026566,
      "learning_rate": 3.412e-05,
      "loss": 0.0042,
      "step": 47640
    },
    {
      "epoch": 2.541333333333333,
      "grad_norm": 0.02844197303056717,
      "learning_rate": 3.411666666666667e-05,
      "loss": 0.0021,
      "step": 47650
    },
    {
      "epoch": 2.5418666666666665,
      "grad_norm": 5.870403274599312e-09,
      "learning_rate": 3.411333333333334e-05,
      "loss": 0.0021,
      "step": 47660
    },
    {
      "epoch": 2.5423999999999998,
      "grad_norm": 0.28441253304481506,
      "learning_rate": 3.4110000000000004e-05,
      "loss": 0.0027,
      "step": 47670
    },
    {
      "epoch": 2.5429333333333335,
      "grad_norm": 0.6308513283729553,
      "learning_rate": 3.410666666666667e-05,
      "loss": 0.0032,
      "step": 47680
    },
    {
      "epoch": 2.5434666666666668,
      "grad_norm": 0.06062672287225723,
      "learning_rate": 3.4103333333333336e-05,
      "loss": 0.0068,
      "step": 47690
    },
    {
      "epoch": 2.544,
      "grad_norm": 0.17063774168491364,
      "learning_rate": 3.41e-05,
      "loss": 0.0052,
      "step": 47700
    },
    {
      "epoch": 2.5445333333333333,
      "grad_norm": 0.3696519732475281,
      "learning_rate": 3.409666666666667e-05,
      "loss": 0.0022,
      "step": 47710
    },
    {
      "epoch": 2.5450666666666666,
      "grad_norm": 0.19906117022037506,
      "learning_rate": 3.4093333333333334e-05,
      "loss": 0.0035,
      "step": 47720
    },
    {
      "epoch": 2.5456,
      "grad_norm": 0.2274780571460724,
      "learning_rate": 3.409e-05,
      "loss": 0.0033,
      "step": 47730
    },
    {
      "epoch": 2.5461333333333336,
      "grad_norm": 0.3696650564670563,
      "learning_rate": 3.408666666666667e-05,
      "loss": 0.004,
      "step": 47740
    },
    {
      "epoch": 2.546666666666667,
      "grad_norm": 0.028435654938220978,
      "learning_rate": 3.408333333333333e-05,
      "loss": 0.0027,
      "step": 47750
    },
    {
      "epoch": 2.5472,
      "grad_norm": 0.14217408001422882,
      "learning_rate": 3.408e-05,
      "loss": 0.0029,
      "step": 47760
    },
    {
      "epoch": 2.5477333333333334,
      "grad_norm": 1.6239271038998027e-09,
      "learning_rate": 3.4076666666666665e-05,
      "loss": 0.0035,
      "step": 47770
    },
    {
      "epoch": 2.5482666666666667,
      "grad_norm": 0.45493438839912415,
      "learning_rate": 3.407333333333334e-05,
      "loss": 0.0032,
      "step": 47780
    },
    {
      "epoch": 2.5488,
      "grad_norm": 0.02839684672653675,
      "learning_rate": 3.4070000000000004e-05,
      "loss": 0.003,
      "step": 47790
    },
    {
      "epoch": 2.5493333333333332,
      "grad_norm": 0.2271522730588913,
      "learning_rate": 3.406666666666667e-05,
      "loss": 0.0038,
      "step": 47800
    },
    {
      "epoch": 2.5498666666666665,
      "grad_norm": 0.19877173006534576,
      "learning_rate": 3.4063333333333336e-05,
      "loss": 0.0036,
      "step": 47810
    },
    {
      "epoch": 2.5504,
      "grad_norm": 0.5678626298904419,
      "learning_rate": 3.406e-05,
      "loss": 0.0023,
      "step": 47820
    },
    {
      "epoch": 2.5509333333333335,
      "grad_norm": 0.4778730869293213,
      "learning_rate": 3.405666666666667e-05,
      "loss": 0.0046,
      "step": 47830
    },
    {
      "epoch": 2.5514666666666668,
      "grad_norm": 0.2271539866924286,
      "learning_rate": 3.4053333333333335e-05,
      "loss": 0.0027,
      "step": 47840
    },
    {
      "epoch": 2.552,
      "grad_norm": 0.08518116176128387,
      "learning_rate": 3.405e-05,
      "loss": 0.0033,
      "step": 47850
    },
    {
      "epoch": 2.5525333333333333,
      "grad_norm": 0.1987556368112564,
      "learning_rate": 3.404666666666667e-05,
      "loss": 0.0026,
      "step": 47860
    },
    {
      "epoch": 2.5530666666666666,
      "grad_norm": 0.028396040201187134,
      "learning_rate": 3.404333333333333e-05,
      "loss": 0.0037,
      "step": 47870
    },
    {
      "epoch": 2.5536,
      "grad_norm": 0.028395064175128937,
      "learning_rate": 3.404e-05,
      "loss": 0.0032,
      "step": 47880
    },
    {
      "epoch": 2.5541333333333336,
      "grad_norm": 0.17036104202270508,
      "learning_rate": 3.4036666666666665e-05,
      "loss": 0.002,
      "step": 47890
    },
    {
      "epoch": 2.554666666666667,
      "grad_norm": 0.25554266571998596,
      "learning_rate": 3.403333333333333e-05,
      "loss": 0.0028,
      "step": 47900
    },
    {
      "epoch": 2.5552,
      "grad_norm": 0.2555365562438965,
      "learning_rate": 3.403e-05,
      "loss": 0.0034,
      "step": 47910
    },
    {
      "epoch": 2.5557333333333334,
      "grad_norm": 0.056786391884088516,
      "learning_rate": 3.402666666666667e-05,
      "loss": 0.0035,
      "step": 47920
    },
    {
      "epoch": 2.5562666666666667,
      "grad_norm": 0.05678853765130043,
      "learning_rate": 3.402333333333334e-05,
      "loss": 0.0033,
      "step": 47930
    },
    {
      "epoch": 2.5568,
      "grad_norm": 0.11357622593641281,
      "learning_rate": 3.402e-05,
      "loss": 0.0037,
      "step": 47940
    },
    {
      "epoch": 2.5573333333333332,
      "grad_norm": 3.4783220748124677e-09,
      "learning_rate": 3.401666666666667e-05,
      "loss": 0.0017,
      "step": 47950
    },
    {
      "epoch": 2.5578666666666665,
      "grad_norm": 0.22714775800704956,
      "learning_rate": 3.4013333333333335e-05,
      "loss": 0.0024,
      "step": 47960
    },
    {
      "epoch": 2.5584,
      "grad_norm": 0.11357397586107254,
      "learning_rate": 3.401e-05,
      "loss": 0.0025,
      "step": 47970
    },
    {
      "epoch": 2.558933333333333,
      "grad_norm": 0.05678689479827881,
      "learning_rate": 3.400666666666667e-05,
      "loss": 0.0035,
      "step": 47980
    },
    {
      "epoch": 2.559466666666667,
      "grad_norm": 0.4282822906970978,
      "learning_rate": 3.400333333333334e-05,
      "loss": 0.0031,
      "step": 47990
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.1987539678812027,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.0032,
      "step": 48000
    },
    {
      "epoch": 2.5605333333333333,
      "grad_norm": 0.22714650630950928,
      "learning_rate": 3.3996666666666666e-05,
      "loss": 0.0028,
      "step": 48010
    },
    {
      "epoch": 2.5610666666666666,
      "grad_norm": 0.31231045722961426,
      "learning_rate": 3.399333333333333e-05,
      "loss": 0.0031,
      "step": 48020
    },
    {
      "epoch": 2.5616,
      "grad_norm": 0.05678766220808029,
      "learning_rate": 3.399e-05,
      "loss": 0.0023,
      "step": 48030
    },
    {
      "epoch": 2.5621333333333336,
      "grad_norm": 0.1703493744134903,
      "learning_rate": 3.3986666666666664e-05,
      "loss": 0.003,
      "step": 48040
    },
    {
      "epoch": 2.562666666666667,
      "grad_norm": 0.056789614260196686,
      "learning_rate": 3.398333333333333e-05,
      "loss": 0.002,
      "step": 48050
    },
    {
      "epoch": 2.5632,
      "grad_norm": 0.08517249673604965,
      "learning_rate": 3.398e-05,
      "loss": 0.0027,
      "step": 48060
    },
    {
      "epoch": 2.5637333333333334,
      "grad_norm": 0.5679129362106323,
      "learning_rate": 3.397666666666667e-05,
      "loss": 0.0036,
      "step": 48070
    },
    {
      "epoch": 2.5642666666666667,
      "grad_norm": 0.40317022800445557,
      "learning_rate": 3.3973333333333336e-05,
      "loss": 0.0031,
      "step": 48080
    },
    {
      "epoch": 2.5648,
      "grad_norm": 0.11356974393129349,
      "learning_rate": 3.397e-05,
      "loss": 0.0017,
      "step": 48090
    },
    {
      "epoch": 2.5653333333333332,
      "grad_norm": 0.08517330884933472,
      "learning_rate": 3.396666666666667e-05,
      "loss": 0.0034,
      "step": 48100
    },
    {
      "epoch": 2.5658666666666665,
      "grad_norm": 0.028391076251864433,
      "learning_rate": 3.3963333333333334e-05,
      "loss": 0.0019,
      "step": 48110
    },
    {
      "epoch": 2.5664,
      "grad_norm": 0.2839062809944153,
      "learning_rate": 3.396e-05,
      "loss": 0.0041,
      "step": 48120
    },
    {
      "epoch": 2.566933333333333,
      "grad_norm": 0.4440234303474426,
      "learning_rate": 3.395666666666667e-05,
      "loss": 0.0032,
      "step": 48130
    },
    {
      "epoch": 2.567466666666667,
      "grad_norm": 0.14195211231708527,
      "learning_rate": 3.395333333333334e-05,
      "loss": 0.002,
      "step": 48140
    },
    {
      "epoch": 2.568,
      "grad_norm": 0.6448331475257874,
      "learning_rate": 3.3950000000000005e-05,
      "loss": 0.0025,
      "step": 48150
    },
    {
      "epoch": 2.5685333333333333,
      "grad_norm": 0.08517380058765411,
      "learning_rate": 3.394666666666667e-05,
      "loss": 0.0027,
      "step": 48160
    },
    {
      "epoch": 2.5690666666666666,
      "grad_norm": 0.22711817920207977,
      "learning_rate": 3.394333333333333e-05,
      "loss": 0.0032,
      "step": 48170
    },
    {
      "epoch": 2.5696,
      "grad_norm": 0.28391554951667786,
      "learning_rate": 3.394e-05,
      "loss": 0.0026,
      "step": 48180
    },
    {
      "epoch": 2.5701333333333336,
      "grad_norm": 0.19872035086154938,
      "learning_rate": 3.393666666666667e-05,
      "loss": 0.0033,
      "step": 48190
    },
    {
      "epoch": 2.570666666666667,
      "grad_norm": 0.4542701840400696,
      "learning_rate": 3.3933333333333336e-05,
      "loss": 0.0024,
      "step": 48200
    },
    {
      "epoch": 2.5712,
      "grad_norm": 0.1419399529695511,
      "learning_rate": 3.393e-05,
      "loss": 0.0022,
      "step": 48210
    },
    {
      "epoch": 2.5717333333333334,
      "grad_norm": 0.28390926122665405,
      "learning_rate": 3.392666666666667e-05,
      "loss": 0.0026,
      "step": 48220
    },
    {
      "epoch": 2.5722666666666667,
      "grad_norm": 0.113557830452919,
      "learning_rate": 3.3923333333333334e-05,
      "loss": 0.0025,
      "step": 48230
    },
    {
      "epoch": 2.5728,
      "grad_norm": 0.02838815003633499,
      "learning_rate": 3.392e-05,
      "loss": 0.0026,
      "step": 48240
    },
    {
      "epoch": 2.5733333333333333,
      "grad_norm": 0.3122909665107727,
      "learning_rate": 3.391666666666667e-05,
      "loss": 0.0035,
      "step": 48250
    },
    {
      "epoch": 2.5738666666666665,
      "grad_norm": 0.2554895281791687,
      "learning_rate": 3.391333333333333e-05,
      "loss": 0.0039,
      "step": 48260
    },
    {
      "epoch": 2.5744,
      "grad_norm": 1.3210043059430632e-09,
      "learning_rate": 3.3910000000000006e-05,
      "loss": 0.002,
      "step": 48270
    },
    {
      "epoch": 2.574933333333333,
      "grad_norm": 0.056777313351631165,
      "learning_rate": 3.390666666666667e-05,
      "loss": 0.0022,
      "step": 48280
    },
    {
      "epoch": 2.575466666666667,
      "grad_norm": 0.1987239420413971,
      "learning_rate": 3.390333333333334e-05,
      "loss": 0.0024,
      "step": 48290
    },
    {
      "epoch": 2.576,
      "grad_norm": 0.2554941773414612,
      "learning_rate": 3.3900000000000004e-05,
      "loss": 0.0022,
      "step": 48300
    },
    {
      "epoch": 2.5765333333333333,
      "grad_norm": 0.02838827297091484,
      "learning_rate": 3.389666666666667e-05,
      "loss": 0.0034,
      "step": 48310
    },
    {
      "epoch": 2.5770666666666666,
      "grad_norm": 0.08516796678304672,
      "learning_rate": 3.389333333333333e-05,
      "loss": 0.0023,
      "step": 48320
    },
    {
      "epoch": 2.5776,
      "grad_norm": 0.056776195764541626,
      "learning_rate": 3.389e-05,
      "loss": 0.0023,
      "step": 48330
    },
    {
      "epoch": 2.5781333333333336,
      "grad_norm": 0.227113738656044,
      "learning_rate": 3.388666666666667e-05,
      "loss": 0.0023,
      "step": 48340
    },
    {
      "epoch": 2.578666666666667,
      "grad_norm": 0.1703147292137146,
      "learning_rate": 3.3883333333333335e-05,
      "loss": 0.003,
      "step": 48350
    },
    {
      "epoch": 2.5792,
      "grad_norm": 0.1703333854675293,
      "learning_rate": 3.388e-05,
      "loss": 0.0036,
      "step": 48360
    },
    {
      "epoch": 2.5797333333333334,
      "grad_norm": 0.34063515067100525,
      "learning_rate": 3.387666666666667e-05,
      "loss": 0.0035,
      "step": 48370
    },
    {
      "epoch": 2.5802666666666667,
      "grad_norm": 0.22709907591342926,
      "learning_rate": 3.387333333333333e-05,
      "loss": 0.0024,
      "step": 48380
    },
    {
      "epoch": 2.5808,
      "grad_norm": 0.056776054203510284,
      "learning_rate": 3.387e-05,
      "loss": 0.0029,
      "step": 48390
    },
    {
      "epoch": 2.5813333333333333,
      "grad_norm": 0.11354885250329971,
      "learning_rate": 3.3866666666666665e-05,
      "loss": 0.003,
      "step": 48400
    },
    {
      "epoch": 2.5818666666666665,
      "grad_norm": 0.1703149378299713,
      "learning_rate": 3.386333333333334e-05,
      "loss": 0.003,
      "step": 48410
    },
    {
      "epoch": 2.5824,
      "grad_norm": 0.3122568726539612,
      "learning_rate": 3.3860000000000004e-05,
      "loss": 0.0028,
      "step": 48420
    },
    {
      "epoch": 2.582933333333333,
      "grad_norm": 0.028386348858475685,
      "learning_rate": 3.385666666666667e-05,
      "loss": 0.0026,
      "step": 48430
    },
    {
      "epoch": 2.583466666666667,
      "grad_norm": 0.0567721351981163,
      "learning_rate": 3.385333333333334e-05,
      "loss": 0.0036,
      "step": 48440
    },
    {
      "epoch": 2.584,
      "grad_norm": 0.19870005548000336,
      "learning_rate": 3.385e-05,
      "loss": 0.0016,
      "step": 48450
    },
    {
      "epoch": 2.5845333333333333,
      "grad_norm": 0.14192847907543182,
      "learning_rate": 3.384666666666667e-05,
      "loss": 0.0026,
      "step": 48460
    },
    {
      "epoch": 2.5850666666666666,
      "grad_norm": 0.3406188189983368,
      "learning_rate": 3.3843333333333335e-05,
      "loss": 0.0017,
      "step": 48470
    },
    {
      "epoch": 2.5856,
      "grad_norm": 1.1008066680417983e-09,
      "learning_rate": 3.384e-05,
      "loss": 0.004,
      "step": 48480
    },
    {
      "epoch": 2.586133333333333,
      "grad_norm": 0.2554899752140045,
      "learning_rate": 3.383666666666667e-05,
      "loss": 0.0039,
      "step": 48490
    },
    {
      "epoch": 2.586666666666667,
      "grad_norm": 0.3122337758541107,
      "learning_rate": 3.3833333333333334e-05,
      "loss": 0.0031,
      "step": 48500
    },
    {
      "epoch": 2.5872,
      "grad_norm": 0.028385749086737633,
      "learning_rate": 3.383e-05,
      "loss": 0.0033,
      "step": 48510
    },
    {
      "epoch": 2.5877333333333334,
      "grad_norm": 0.11354051530361176,
      "learning_rate": 3.3826666666666666e-05,
      "loss": 0.0038,
      "step": 48520
    },
    {
      "epoch": 2.5882666666666667,
      "grad_norm": 0.22707368433475494,
      "learning_rate": 3.382333333333333e-05,
      "loss": 0.0034,
      "step": 48530
    },
    {
      "epoch": 2.5888,
      "grad_norm": 0.14193087816238403,
      "learning_rate": 3.3820000000000005e-05,
      "loss": 0.0027,
      "step": 48540
    },
    {
      "epoch": 2.5893333333333333,
      "grad_norm": 0.4541410207748413,
      "learning_rate": 3.381666666666667e-05,
      "loss": 0.0045,
      "step": 48550
    },
    {
      "epoch": 2.5898666666666665,
      "grad_norm": 0.3690167963504791,
      "learning_rate": 3.381333333333334e-05,
      "loss": 0.0042,
      "step": 48560
    },
    {
      "epoch": 2.5904,
      "grad_norm": 0.11353779584169388,
      "learning_rate": 3.381e-05,
      "loss": 0.0032,
      "step": 48570
    },
    {
      "epoch": 2.590933333333333,
      "grad_norm": 0.22709061205387115,
      "learning_rate": 3.380666666666667e-05,
      "loss": 0.0036,
      "step": 48580
    },
    {
      "epoch": 2.591466666666667,
      "grad_norm": 0.22707077860832214,
      "learning_rate": 3.3803333333333336e-05,
      "loss": 0.0022,
      "step": 48590
    },
    {
      "epoch": 2.592,
      "grad_norm": 0.5393633246421814,
      "learning_rate": 3.38e-05,
      "loss": 0.0029,
      "step": 48600
    },
    {
      "epoch": 2.5925333333333334,
      "grad_norm": 0.056769419461488724,
      "learning_rate": 3.379666666666667e-05,
      "loss": 0.0049,
      "step": 48610
    },
    {
      "epoch": 2.5930666666666666,
      "grad_norm": 1.7016795750279812e-09,
      "learning_rate": 3.3793333333333334e-05,
      "loss": 0.0025,
      "step": 48620
    },
    {
      "epoch": 2.5936,
      "grad_norm": 0.056769635528326035,
      "learning_rate": 3.379e-05,
      "loss": 0.0021,
      "step": 48630
    },
    {
      "epoch": 2.594133333333333,
      "grad_norm": 4.727274127702685e-09,
      "learning_rate": 3.3786666666666666e-05,
      "loss": 0.0025,
      "step": 48640
    },
    {
      "epoch": 2.594666666666667,
      "grad_norm": 0.056767698377370834,
      "learning_rate": 3.378333333333333e-05,
      "loss": 0.0025,
      "step": 48650
    },
    {
      "epoch": 2.5952,
      "grad_norm": 0.028384262695908546,
      "learning_rate": 3.378e-05,
      "loss": 0.0038,
      "step": 48660
    },
    {
      "epoch": 2.5957333333333334,
      "grad_norm": 0.14192581176757812,
      "learning_rate": 3.3776666666666665e-05,
      "loss": 0.0025,
      "step": 48670
    },
    {
      "epoch": 2.5962666666666667,
      "grad_norm": 0.14192549884319305,
      "learning_rate": 3.377333333333334e-05,
      "loss": 0.0044,
      "step": 48680
    },
    {
      "epoch": 2.5968,
      "grad_norm": 0.11354216933250427,
      "learning_rate": 3.3770000000000004e-05,
      "loss": 0.0031,
      "step": 48690
    },
    {
      "epoch": 2.5973333333333333,
      "grad_norm": 0.19867856800556183,
      "learning_rate": 3.376666666666667e-05,
      "loss": 0.0024,
      "step": 48700
    },
    {
      "epoch": 2.5978666666666665,
      "grad_norm": 0.08515922725200653,
      "learning_rate": 3.3763333333333336e-05,
      "loss": 0.0032,
      "step": 48710
    },
    {
      "epoch": 2.5984,
      "grad_norm": 0.34059369564056396,
      "learning_rate": 3.376e-05,
      "loss": 0.0039,
      "step": 48720
    },
    {
      "epoch": 2.598933333333333,
      "grad_norm": 0.08515460044145584,
      "learning_rate": 3.375666666666667e-05,
      "loss": 0.0033,
      "step": 48730
    },
    {
      "epoch": 2.599466666666667,
      "grad_norm": 0.02838345803320408,
      "learning_rate": 3.3753333333333334e-05,
      "loss": 0.0031,
      "step": 48740
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.1135353371500969,
      "learning_rate": 3.375000000000001e-05,
      "loss": 0.0031,
      "step": 48750
    },
    {
      "epoch": 2.6005333333333334,
      "grad_norm": 0.05676649883389473,
      "learning_rate": 3.374666666666667e-05,
      "loss": 0.0034,
      "step": 48760
    },
    {
      "epoch": 2.6010666666666666,
      "grad_norm": 0.08515719324350357,
      "learning_rate": 3.374333333333333e-05,
      "loss": 0.0018,
      "step": 48770
    },
    {
      "epoch": 2.6016,
      "grad_norm": 0.7095283269882202,
      "learning_rate": 3.374e-05,
      "loss": 0.0025,
      "step": 48780
    },
    {
      "epoch": 2.602133333333333,
      "grad_norm": 0.05676732212305069,
      "learning_rate": 3.3736666666666665e-05,
      "loss": 0.003,
      "step": 48790
    },
    {
      "epoch": 2.602666666666667,
      "grad_norm": 0.22705838084220886,
      "learning_rate": 3.373333333333333e-05,
      "loss": 0.0026,
      "step": 48800
    },
    {
      "epoch": 2.6032,
      "grad_norm": 0.2554608881473541,
      "learning_rate": 3.373e-05,
      "loss": 0.0026,
      "step": 48810
    },
    {
      "epoch": 2.6037333333333335,
      "grad_norm": 0.3122040033340454,
      "learning_rate": 3.372666666666667e-05,
      "loss": 0.0028,
      "step": 48820
    },
    {
      "epoch": 2.6042666666666667,
      "grad_norm": 0.02838139422237873,
      "learning_rate": 3.3723333333333336e-05,
      "loss": 0.0023,
      "step": 48830
    },
    {
      "epoch": 2.6048,
      "grad_norm": 0.17029397189617157,
      "learning_rate": 3.372e-05,
      "loss": 0.0032,
      "step": 48840
    },
    {
      "epoch": 2.6053333333333333,
      "grad_norm": 0.14191745221614838,
      "learning_rate": 3.371666666666667e-05,
      "loss": 0.0033,
      "step": 48850
    },
    {
      "epoch": 2.6058666666666666,
      "grad_norm": 0.17029330134391785,
      "learning_rate": 3.3713333333333335e-05,
      "loss": 0.0029,
      "step": 48860
    },
    {
      "epoch": 2.6064,
      "grad_norm": 0.3122253715991974,
      "learning_rate": 3.371e-05,
      "loss": 0.0038,
      "step": 48870
    },
    {
      "epoch": 2.606933333333333,
      "grad_norm": 0.22704827785491943,
      "learning_rate": 3.370666666666667e-05,
      "loss": 0.0029,
      "step": 48880
    },
    {
      "epoch": 2.607466666666667,
      "grad_norm": 0.48253947496414185,
      "learning_rate": 3.370333333333334e-05,
      "loss": 0.0038,
      "step": 48890
    },
    {
      "epoch": 2.608,
      "grad_norm": 0.28381651639938354,
      "learning_rate": 3.3700000000000006e-05,
      "loss": 0.0031,
      "step": 48900
    },
    {
      "epoch": 2.6085333333333334,
      "grad_norm": 0.028382522985339165,
      "learning_rate": 3.369666666666667e-05,
      "loss": 0.0033,
      "step": 48910
    },
    {
      "epoch": 2.6090666666666666,
      "grad_norm": 0.3122043311595917,
      "learning_rate": 3.369333333333333e-05,
      "loss": 0.0025,
      "step": 48920
    },
    {
      "epoch": 2.6096,
      "grad_norm": 0.028382280841469765,
      "learning_rate": 3.369e-05,
      "loss": 0.0019,
      "step": 48930
    },
    {
      "epoch": 2.610133333333333,
      "grad_norm": 0.17029306292533875,
      "learning_rate": 3.3686666666666664e-05,
      "loss": 0.0034,
      "step": 48940
    },
    {
      "epoch": 2.610666666666667,
      "grad_norm": 0.19867022335529327,
      "learning_rate": 3.368333333333334e-05,
      "loss": 0.0027,
      "step": 48950
    },
    {
      "epoch": 2.6112,
      "grad_norm": 3.1176037307290017e-09,
      "learning_rate": 3.368e-05,
      "loss": 0.0042,
      "step": 48960
    },
    {
      "epoch": 2.6117333333333335,
      "grad_norm": 0.19865666329860687,
      "learning_rate": 3.367666666666667e-05,
      "loss": 0.0036,
      "step": 48970
    },
    {
      "epoch": 2.6122666666666667,
      "grad_norm": 0.5883125066757202,
      "learning_rate": 3.3673333333333335e-05,
      "loss": 0.0036,
      "step": 48980
    },
    {
      "epoch": 2.6128,
      "grad_norm": 0.5959464907646179,
      "learning_rate": 3.367e-05,
      "loss": 0.0027,
      "step": 48990
    },
    {
      "epoch": 2.6133333333333333,
      "grad_norm": 0.36896052956581116,
      "learning_rate": 3.366666666666667e-05,
      "loss": 0.0031,
      "step": 49000
    },
    {
      "epoch": 2.6138666666666666,
      "grad_norm": 0.056761547923088074,
      "learning_rate": 3.3663333333333333e-05,
      "loss": 0.0033,
      "step": 49010
    },
    {
      "epoch": 2.6144,
      "grad_norm": 0.17027722299098969,
      "learning_rate": 3.366e-05,
      "loss": 0.0022,
      "step": 49020
    },
    {
      "epoch": 2.614933333333333,
      "grad_norm": 0.1702793538570404,
      "learning_rate": 3.365666666666667e-05,
      "loss": 0.0021,
      "step": 49030
    },
    {
      "epoch": 2.615466666666667,
      "grad_norm": 0.25541311502456665,
      "learning_rate": 3.365333333333334e-05,
      "loss": 0.0027,
      "step": 49040
    },
    {
      "epoch": 2.616,
      "grad_norm": 0.14189022779464722,
      "learning_rate": 3.3650000000000005e-05,
      "loss": 0.002,
      "step": 49050
    },
    {
      "epoch": 2.6165333333333334,
      "grad_norm": 0.028378458693623543,
      "learning_rate": 3.364666666666667e-05,
      "loss": 0.0021,
      "step": 49060
    },
    {
      "epoch": 2.6170666666666667,
      "grad_norm": 0.2838042676448822,
      "learning_rate": 3.364333333333333e-05,
      "loss": 0.0043,
      "step": 49070
    },
    {
      "epoch": 2.6176,
      "grad_norm": 0.17026422917842865,
      "learning_rate": 3.3639999999999996e-05,
      "loss": 0.0028,
      "step": 49080
    },
    {
      "epoch": 2.618133333333333,
      "grad_norm": 0.3972742259502411,
      "learning_rate": 3.363666666666667e-05,
      "loss": 0.0046,
      "step": 49090
    },
    {
      "epoch": 2.618666666666667,
      "grad_norm": 0.1418992578983307,
      "learning_rate": 3.3633333333333335e-05,
      "loss": 0.0021,
      "step": 49100
    },
    {
      "epoch": 2.6192,
      "grad_norm": 0.19864606857299805,
      "learning_rate": 3.363e-05,
      "loss": 0.0033,
      "step": 49110
    },
    {
      "epoch": 2.6197333333333335,
      "grad_norm": 0.08513669669628143,
      "learning_rate": 3.362666666666667e-05,
      "loss": 0.0024,
      "step": 49120
    },
    {
      "epoch": 2.6202666666666667,
      "grad_norm": 4.630441363673299e-09,
      "learning_rate": 3.3623333333333334e-05,
      "loss": 0.0034,
      "step": 49130
    },
    {
      "epoch": 2.6208,
      "grad_norm": 0.1418875902891159,
      "learning_rate": 3.362e-05,
      "loss": 0.0027,
      "step": 49140
    },
    {
      "epoch": 2.6213333333333333,
      "grad_norm": 1.299278018507266e-09,
      "learning_rate": 3.3616666666666666e-05,
      "loss": 0.0044,
      "step": 49150
    },
    {
      "epoch": 2.6218666666666666,
      "grad_norm": 2.7115543144162757e-09,
      "learning_rate": 3.361333333333333e-05,
      "loss": 0.003,
      "step": 49160
    },
    {
      "epoch": 2.6224,
      "grad_norm": 0.05675320327281952,
      "learning_rate": 3.3610000000000005e-05,
      "loss": 0.0038,
      "step": 49170
    },
    {
      "epoch": 2.622933333333333,
      "grad_norm": 0.36893436312675476,
      "learning_rate": 3.360666666666667e-05,
      "loss": 0.0021,
      "step": 49180
    },
    {
      "epoch": 2.6234666666666664,
      "grad_norm": 0.34051716327667236,
      "learning_rate": 3.360333333333334e-05,
      "loss": 0.0041,
      "step": 49190
    },
    {
      "epoch": 2.624,
      "grad_norm": 0.08513043075799942,
      "learning_rate": 3.3600000000000004e-05,
      "loss": 0.0035,
      "step": 49200
    },
    {
      "epoch": 2.6245333333333334,
      "grad_norm": 0.01086930837482214,
      "learning_rate": 3.359666666666667e-05,
      "loss": 0.0028,
      "step": 49210
    },
    {
      "epoch": 2.6250666666666667,
      "grad_norm": 0.1702597737312317,
      "learning_rate": 3.359333333333333e-05,
      "loss": 0.0032,
      "step": 49220
    },
    {
      "epoch": 2.6256,
      "grad_norm": 0.28379619121551514,
      "learning_rate": 3.359e-05,
      "loss": 0.0026,
      "step": 49230
    },
    {
      "epoch": 2.626133333333333,
      "grad_norm": 0.283765971660614,
      "learning_rate": 3.358666666666667e-05,
      "loss": 0.0028,
      "step": 49240
    },
    {
      "epoch": 2.626666666666667,
      "grad_norm": 0.16748537123203278,
      "learning_rate": 3.3583333333333334e-05,
      "loss": 0.002,
      "step": 49250
    },
    {
      "epoch": 2.6272,
      "grad_norm": 0.25541213154792786,
      "learning_rate": 3.358e-05,
      "loss": 0.0017,
      "step": 49260
    },
    {
      "epoch": 2.6277333333333335,
      "grad_norm": 2.146821165283086e-09,
      "learning_rate": 3.3576666666666666e-05,
      "loss": 0.0039,
      "step": 49270
    },
    {
      "epoch": 2.6282666666666668,
      "grad_norm": 0.142221137881279,
      "learning_rate": 3.357333333333333e-05,
      "loss": 0.0032,
      "step": 49280
    },
    {
      "epoch": 2.6288,
      "grad_norm": 0.14188258349895477,
      "learning_rate": 3.357e-05,
      "loss": 0.0026,
      "step": 49290
    },
    {
      "epoch": 2.6293333333333333,
      "grad_norm": 0.3973236083984375,
      "learning_rate": 3.356666666666667e-05,
      "loss": 0.0025,
      "step": 49300
    },
    {
      "epoch": 2.6298666666666666,
      "grad_norm": 0.3405197262763977,
      "learning_rate": 3.356333333333334e-05,
      "loss": 0.0032,
      "step": 49310
    },
    {
      "epoch": 2.6304,
      "grad_norm": 0.17027439177036285,
      "learning_rate": 3.3560000000000004e-05,
      "loss": 0.0042,
      "step": 49320
    },
    {
      "epoch": 2.630933333333333,
      "grad_norm": 0.2837595045566559,
      "learning_rate": 3.355666666666667e-05,
      "loss": 0.0019,
      "step": 49330
    },
    {
      "epoch": 2.6314666666666664,
      "grad_norm": 0.28379061818122864,
      "learning_rate": 3.3553333333333336e-05,
      "loss": 0.0027,
      "step": 49340
    },
    {
      "epoch": 2.632,
      "grad_norm": 0.11351427435874939,
      "learning_rate": 3.355e-05,
      "loss": 0.0031,
      "step": 49350
    },
    {
      "epoch": 2.6325333333333334,
      "grad_norm": 0.08537527918815613,
      "learning_rate": 3.354666666666667e-05,
      "loss": 0.0029,
      "step": 49360
    },
    {
      "epoch": 2.6330666666666667,
      "grad_norm": 0.19886957108974457,
      "learning_rate": 3.3543333333333335e-05,
      "loss": 0.0026,
      "step": 49370
    },
    {
      "epoch": 2.6336,
      "grad_norm": 0.3405398428440094,
      "learning_rate": 3.354e-05,
      "loss": 0.0024,
      "step": 49380
    },
    {
      "epoch": 2.634133333333333,
      "grad_norm": 0.3121366500854492,
      "learning_rate": 3.353666666666667e-05,
      "loss": 0.003,
      "step": 49390
    },
    {
      "epoch": 2.634666666666667,
      "grad_norm": 0.11350453644990921,
      "learning_rate": 3.353333333333333e-05,
      "loss": 0.004,
      "step": 49400
    },
    {
      "epoch": 2.6352,
      "grad_norm": 0.1702520251274109,
      "learning_rate": 3.353e-05,
      "loss": 0.0032,
      "step": 49410
    },
    {
      "epoch": 2.6357333333333335,
      "grad_norm": 0.28374621272087097,
      "learning_rate": 3.3526666666666665e-05,
      "loss": 0.003,
      "step": 49420
    },
    {
      "epoch": 2.6362666666666668,
      "grad_norm": 0.22701333463191986,
      "learning_rate": 3.352333333333333e-05,
      "loss": 0.003,
      "step": 49430
    },
    {
      "epoch": 2.6368,
      "grad_norm": 0.14187897741794586,
      "learning_rate": 3.3520000000000004e-05,
      "loss": 0.003,
      "step": 49440
    },
    {
      "epoch": 2.6373333333333333,
      "grad_norm": 0.2269953042268753,
      "learning_rate": 3.351666666666667e-05,
      "loss": 0.0023,
      "step": 49450
    },
    {
      "epoch": 2.6378666666666666,
      "grad_norm": 0.11349981278181076,
      "learning_rate": 3.3513333333333337e-05,
      "loss": 0.0041,
      "step": 49460
    },
    {
      "epoch": 2.6384,
      "grad_norm": 0.14187566936016083,
      "learning_rate": 3.351e-05,
      "loss": 0.004,
      "step": 49470
    },
    {
      "epoch": 2.638933333333333,
      "grad_norm": 0.02837531641125679,
      "learning_rate": 3.350666666666667e-05,
      "loss": 0.0019,
      "step": 49480
    },
    {
      "epoch": 2.6394666666666664,
      "grad_norm": 0.08512361347675323,
      "learning_rate": 3.3503333333333335e-05,
      "loss": 0.0033,
      "step": 49490
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.19862912595272064,
      "learning_rate": 3.35e-05,
      "loss": 0.0024,
      "step": 49500
    },
    {
      "epoch": 2.6405333333333334,
      "grad_norm": 0.31211692094802856,
      "learning_rate": 3.349666666666667e-05,
      "loss": 0.004,
      "step": 49510
    },
    {
      "epoch": 2.6410666666666667,
      "grad_norm": 0.05674990266561508,
      "learning_rate": 3.349333333333334e-05,
      "loss": 0.004,
      "step": 49520
    },
    {
      "epoch": 2.6416,
      "grad_norm": 0.11420078575611115,
      "learning_rate": 3.349e-05,
      "loss": 0.0053,
      "step": 49530
    },
    {
      "epoch": 2.6421333333333332,
      "grad_norm": 0.17024418711662292,
      "learning_rate": 3.3486666666666666e-05,
      "loss": 0.0025,
      "step": 49540
    },
    {
      "epoch": 2.642666666666667,
      "grad_norm": 0.2553766965866089,
      "learning_rate": 3.348333333333333e-05,
      "loss": 0.003,
      "step": 49550
    },
    {
      "epoch": 2.6432,
      "grad_norm": 0.1986163854598999,
      "learning_rate": 3.348e-05,
      "loss": 0.0032,
      "step": 49560
    },
    {
      "epoch": 2.6437333333333335,
      "grad_norm": 0.05674975365400314,
      "learning_rate": 3.3476666666666664e-05,
      "loss": 0.0024,
      "step": 49570
    },
    {
      "epoch": 2.6442666666666668,
      "grad_norm": 0.05674649775028229,
      "learning_rate": 3.347333333333334e-05,
      "loss": 0.0019,
      "step": 49580
    },
    {
      "epoch": 2.6448,
      "grad_norm": 0.22700706124305725,
      "learning_rate": 3.347e-05,
      "loss": 0.0034,
      "step": 49590
    },
    {
      "epoch": 2.6453333333333333,
      "grad_norm": 0.14186856150627136,
      "learning_rate": 3.346666666666667e-05,
      "loss": 0.005,
      "step": 49600
    },
    {
      "epoch": 2.6458666666666666,
      "grad_norm": 0.1418764293193817,
      "learning_rate": 3.3463333333333335e-05,
      "loss": 0.0023,
      "step": 49610
    },
    {
      "epoch": 2.6464,
      "grad_norm": 0.08511954545974731,
      "learning_rate": 3.346e-05,
      "loss": 0.0027,
      "step": 49620
    },
    {
      "epoch": 2.646933333333333,
      "grad_norm": 0.056754108518362045,
      "learning_rate": 3.345666666666667e-05,
      "loss": 0.0031,
      "step": 49630
    },
    {
      "epoch": 2.6474666666666664,
      "grad_norm": 0.19861455261707306,
      "learning_rate": 3.3453333333333334e-05,
      "loss": 0.0025,
      "step": 49640
    },
    {
      "epoch": 2.648,
      "grad_norm": 0.028373958542943,
      "learning_rate": 3.345000000000001e-05,
      "loss": 0.0026,
      "step": 49650
    },
    {
      "epoch": 2.6485333333333334,
      "grad_norm": 0.1986159384250641,
      "learning_rate": 3.344666666666667e-05,
      "loss": 0.0019,
      "step": 49660
    },
    {
      "epoch": 2.6490666666666667,
      "grad_norm": 0.4823477566242218,
      "learning_rate": 3.344333333333334e-05,
      "loss": 0.0024,
      "step": 49670
    },
    {
      "epoch": 2.6496,
      "grad_norm": 0.05675157159566879,
      "learning_rate": 3.344e-05,
      "loss": 0.0031,
      "step": 49680
    },
    {
      "epoch": 2.6501333333333332,
      "grad_norm": 0.3972140848636627,
      "learning_rate": 3.3436666666666664e-05,
      "loss": 0.003,
      "step": 49690
    },
    {
      "epoch": 2.6506666666666665,
      "grad_norm": 0.05674951896071434,
      "learning_rate": 3.343333333333333e-05,
      "loss": 0.0027,
      "step": 49700
    },
    {
      "epoch": 2.6512000000000002,
      "grad_norm": 5.2108193315802964e-09,
      "learning_rate": 3.3430000000000003e-05,
      "loss": 0.0031,
      "step": 49710
    },
    {
      "epoch": 2.6517333333333335,
      "grad_norm": 0.51069176197052,
      "learning_rate": 3.342666666666667e-05,
      "loss": 0.0025,
      "step": 49720
    },
    {
      "epoch": 2.6522666666666668,
      "grad_norm": 0.5107611417770386,
      "learning_rate": 3.3423333333333336e-05,
      "loss": 0.0032,
      "step": 49730
    },
    {
      "epoch": 2.6528,
      "grad_norm": 0.39721158146858215,
      "learning_rate": 3.342e-05,
      "loss": 0.0021,
      "step": 49740
    },
    {
      "epoch": 2.6533333333333333,
      "grad_norm": 0.08511944115161896,
      "learning_rate": 3.341666666666667e-05,
      "loss": 0.0024,
      "step": 49750
    },
    {
      "epoch": 2.6538666666666666,
      "grad_norm": 0.1702442169189453,
      "learning_rate": 3.3413333333333334e-05,
      "loss": 0.0019,
      "step": 49760
    },
    {
      "epoch": 2.6544,
      "grad_norm": 0.25565704703330994,
      "learning_rate": 3.341e-05,
      "loss": 0.0023,
      "step": 49770
    },
    {
      "epoch": 2.654933333333333,
      "grad_norm": 0.05674663558602333,
      "learning_rate": 3.3406666666666666e-05,
      "loss": 0.0023,
      "step": 49780
    },
    {
      "epoch": 2.6554666666666664,
      "grad_norm": 0.5390632152557373,
      "learning_rate": 3.340333333333334e-05,
      "loss": 0.0039,
      "step": 49790
    },
    {
      "epoch": 2.656,
      "grad_norm": 0.2837485373020172,
      "learning_rate": 3.3400000000000005e-05,
      "loss": 0.0034,
      "step": 49800
    },
    {
      "epoch": 2.6565333333333334,
      "grad_norm": 1.8473158558407476e-09,
      "learning_rate": 3.339666666666667e-05,
      "loss": 0.0031,
      "step": 49810
    },
    {
      "epoch": 2.6570666666666667,
      "grad_norm": 0.05674659460783005,
      "learning_rate": 3.339333333333334e-05,
      "loss": 0.0026,
      "step": 49820
    },
    {
      "epoch": 2.6576,
      "grad_norm": 0.39722031354904175,
      "learning_rate": 3.339e-05,
      "loss": 0.0035,
      "step": 49830
    },
    {
      "epoch": 2.6581333333333332,
      "grad_norm": 0.2837311029434204,
      "learning_rate": 3.338666666666666e-05,
      "loss": 0.0033,
      "step": 49840
    },
    {
      "epoch": 2.6586666666666665,
      "grad_norm": 6.196495316856954e-09,
      "learning_rate": 3.3383333333333336e-05,
      "loss": 0.0042,
      "step": 49850
    },
    {
      "epoch": 2.6592000000000002,
      "grad_norm": 4.765727812383602e-09,
      "learning_rate": 3.338e-05,
      "loss": 0.0041,
      "step": 49860
    },
    {
      "epoch": 2.6597333333333335,
      "grad_norm": 0.11404707282781601,
      "learning_rate": 3.337666666666667e-05,
      "loss": 0.0038,
      "step": 49870
    },
    {
      "epoch": 2.660266666666667,
      "grad_norm": 0.22698315978050232,
      "learning_rate": 3.3373333333333335e-05,
      "loss": 0.002,
      "step": 49880
    },
    {
      "epoch": 2.6608,
      "grad_norm": 2.2046824366128703e-09,
      "learning_rate": 3.337e-05,
      "loss": 0.0021,
      "step": 49890
    },
    {
      "epoch": 2.6613333333333333,
      "grad_norm": 0.05674445629119873,
      "learning_rate": 3.336666666666667e-05,
      "loss": 0.0025,
      "step": 49900
    },
    {
      "epoch": 2.6618666666666666,
      "grad_norm": 0.0410112701356411,
      "learning_rate": 3.336333333333333e-05,
      "loss": 0.0025,
      "step": 49910
    },
    {
      "epoch": 2.6624,
      "grad_norm": 0.0567467026412487,
      "learning_rate": 3.336e-05,
      "loss": 0.0025,
      "step": 49920
    },
    {
      "epoch": 2.662933333333333,
      "grad_norm": 0.05674894526600838,
      "learning_rate": 3.335666666666667e-05,
      "loss": 0.0034,
      "step": 49930
    },
    {
      "epoch": 2.6634666666666664,
      "grad_norm": 0.8227396011352539,
      "learning_rate": 3.335333333333334e-05,
      "loss": 0.0026,
      "step": 49940
    },
    {
      "epoch": 2.664,
      "grad_norm": 0.4259520471096039,
      "learning_rate": 3.3350000000000004e-05,
      "loss": 0.0031,
      "step": 49950
    },
    {
      "epoch": 2.6645333333333334,
      "grad_norm": 0.48231375217437744,
      "learning_rate": 3.334666666666667e-05,
      "loss": 0.0035,
      "step": 49960
    },
    {
      "epoch": 2.6650666666666667,
      "grad_norm": 0.42553362250328064,
      "learning_rate": 3.3343333333333337e-05,
      "loss": 0.002,
      "step": 49970
    },
    {
      "epoch": 2.6656,
      "grad_norm": 0.4255739748477936,
      "learning_rate": 3.3339999999999996e-05,
      "loss": 0.0021,
      "step": 49980
    },
    {
      "epoch": 2.6661333333333332,
      "grad_norm": 0.14184290170669556,
      "learning_rate": 3.333666666666667e-05,
      "loss": 0.0023,
      "step": 49990
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.08510985225439072,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.0029,
      "step": 50000
    },
    {
      "epoch": 2.6672000000000002,
      "grad_norm": 0.36885228753089905,
      "learning_rate": 3.333e-05,
      "loss": 0.0032,
      "step": 50010
    },
    {
      "epoch": 2.6677333333333335,
      "grad_norm": 0.0567396804690361,
      "learning_rate": 3.332666666666667e-05,
      "loss": 0.0031,
      "step": 50020
    },
    {
      "epoch": 2.668266666666667,
      "grad_norm": 0.19870693981647491,
      "learning_rate": 3.332333333333333e-05,
      "loss": 0.0025,
      "step": 50030
    },
    {
      "epoch": 2.6688,
      "grad_norm": 0.08511082082986832,
      "learning_rate": 3.332e-05,
      "loss": 0.0023,
      "step": 50040
    },
    {
      "epoch": 2.6693333333333333,
      "grad_norm": 0.7375263571739197,
      "learning_rate": 3.3316666666666666e-05,
      "loss": 0.0031,
      "step": 50050
    },
    {
      "epoch": 2.6698666666666666,
      "grad_norm": 0.17022168636322021,
      "learning_rate": 3.331333333333334e-05,
      "loss": 0.0024,
      "step": 50060
    },
    {
      "epoch": 2.6704,
      "grad_norm": 0.17021657526493073,
      "learning_rate": 3.3310000000000005e-05,
      "loss": 0.0036,
      "step": 50070
    },
    {
      "epoch": 2.670933333333333,
      "grad_norm": 0.11347393691539764,
      "learning_rate": 3.330666666666667e-05,
      "loss": 0.0019,
      "step": 50080
    },
    {
      "epoch": 2.6714666666666664,
      "grad_norm": 0.11385156959295273,
      "learning_rate": 3.330333333333334e-05,
      "loss": 0.003,
      "step": 50090
    },
    {
      "epoch": 2.672,
      "grad_norm": 0.31203633546829224,
      "learning_rate": 3.33e-05,
      "loss": 0.0024,
      "step": 50100
    },
    {
      "epoch": 2.6725333333333334,
      "grad_norm": 0.34042420983314514,
      "learning_rate": 3.329666666666667e-05,
      "loss": 0.0025,
      "step": 50110
    },
    {
      "epoch": 2.6730666666666667,
      "grad_norm": 0.1985623687505722,
      "learning_rate": 3.3293333333333335e-05,
      "loss": 0.0023,
      "step": 50120
    },
    {
      "epoch": 2.6736,
      "grad_norm": 0.2553069591522217,
      "learning_rate": 3.329e-05,
      "loss": 0.0034,
      "step": 50130
    },
    {
      "epoch": 2.6741333333333333,
      "grad_norm": 0.28366661071777344,
      "learning_rate": 3.328666666666667e-05,
      "loss": 0.0026,
      "step": 50140
    },
    {
      "epoch": 2.6746666666666665,
      "grad_norm": 0.05673196539282799,
      "learning_rate": 3.3283333333333334e-05,
      "loss": 0.0032,
      "step": 50150
    },
    {
      "epoch": 2.6752000000000002,
      "grad_norm": 0.34039509296417236,
      "learning_rate": 3.328e-05,
      "loss": 0.0041,
      "step": 50160
    },
    {
      "epoch": 2.6757333333333335,
      "grad_norm": 0.2836490869522095,
      "learning_rate": 3.3276666666666666e-05,
      "loss": 0.0021,
      "step": 50170
    },
    {
      "epoch": 2.676266666666667,
      "grad_norm": 0.2836739122867584,
      "learning_rate": 3.327333333333333e-05,
      "loss": 0.003,
      "step": 50180
    },
    {
      "epoch": 2.6768,
      "grad_norm": 0.028365517035126686,
      "learning_rate": 3.327e-05,
      "loss": 0.0024,
      "step": 50190
    },
    {
      "epoch": 2.6773333333333333,
      "grad_norm": 0.11346755921840668,
      "learning_rate": 3.326666666666667e-05,
      "loss": 0.0032,
      "step": 50200
    },
    {
      "epoch": 2.6778666666666666,
      "grad_norm": 0.22693026065826416,
      "learning_rate": 3.326333333333334e-05,
      "loss": 0.0026,
      "step": 50210
    },
    {
      "epoch": 2.6784,
      "grad_norm": 0.17019890248775482,
      "learning_rate": 3.3260000000000003e-05,
      "loss": 0.0033,
      "step": 50220
    },
    {
      "epoch": 2.678933333333333,
      "grad_norm": 0.028367899358272552,
      "learning_rate": 3.325666666666667e-05,
      "loss": 0.0029,
      "step": 50230
    },
    {
      "epoch": 2.6794666666666664,
      "grad_norm": 0.36874496936798096,
      "learning_rate": 3.3253333333333336e-05,
      "loss": 0.0024,
      "step": 50240
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.11346882581710815,
      "learning_rate": 3.325e-05,
      "loss": 0.0025,
      "step": 50250
    },
    {
      "epoch": 2.6805333333333334,
      "grad_norm": 0.2552938461303711,
      "learning_rate": 3.324666666666667e-05,
      "loss": 0.0033,
      "step": 50260
    },
    {
      "epoch": 2.6810666666666667,
      "grad_norm": 0.08510376513004303,
      "learning_rate": 3.3243333333333334e-05,
      "loss": 0.0029,
      "step": 50270
    },
    {
      "epoch": 2.6816,
      "grad_norm": 0.3120068609714508,
      "learning_rate": 3.324e-05,
      "loss": 0.0033,
      "step": 50280
    },
    {
      "epoch": 2.6821333333333333,
      "grad_norm": 0.028366973623633385,
      "learning_rate": 3.3236666666666666e-05,
      "loss": 0.0026,
      "step": 50290
    },
    {
      "epoch": 2.6826666666666665,
      "grad_norm": 0.19856370985507965,
      "learning_rate": 3.323333333333333e-05,
      "loss": 0.0025,
      "step": 50300
    },
    {
      "epoch": 2.6832000000000003,
      "grad_norm": 0.08510014414787292,
      "learning_rate": 3.323e-05,
      "loss": 0.0038,
      "step": 50310
    },
    {
      "epoch": 2.6837333333333335,
      "grad_norm": 1.4666758918480127e-09,
      "learning_rate": 3.3226666666666665e-05,
      "loss": 0.0023,
      "step": 50320
    },
    {
      "epoch": 2.684266666666667,
      "grad_norm": 0.028365395963191986,
      "learning_rate": 3.322333333333333e-05,
      "loss": 0.0024,
      "step": 50330
    },
    {
      "epoch": 2.6848,
      "grad_norm": 0.3687504231929779,
      "learning_rate": 3.3220000000000004e-05,
      "loss": 0.0031,
      "step": 50340
    },
    {
      "epoch": 2.6853333333333333,
      "grad_norm": 0.36874067783355713,
      "learning_rate": 3.321666666666667e-05,
      "loss": 0.004,
      "step": 50350
    },
    {
      "epoch": 2.6858666666666666,
      "grad_norm": 0.1418251246213913,
      "learning_rate": 3.3213333333333336e-05,
      "loss": 0.0019,
      "step": 50360
    },
    {
      "epoch": 2.6864,
      "grad_norm": 0.22692465782165527,
      "learning_rate": 3.321e-05,
      "loss": 0.0023,
      "step": 50370
    },
    {
      "epoch": 2.686933333333333,
      "grad_norm": 0.1702028065919876,
      "learning_rate": 3.320666666666667e-05,
      "loss": 0.0043,
      "step": 50380
    },
    {
      "epoch": 2.6874666666666664,
      "grad_norm": 0.08509422838687897,
      "learning_rate": 3.3203333333333334e-05,
      "loss": 0.0034,
      "step": 50390
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 0.22693924605846405,
      "learning_rate": 3.32e-05,
      "loss": 0.0027,
      "step": 50400
    },
    {
      "epoch": 2.6885333333333334,
      "grad_norm": 0.9597288370132446,
      "learning_rate": 3.3196666666666674e-05,
      "loss": 0.0028,
      "step": 50410
    },
    {
      "epoch": 2.6890666666666667,
      "grad_norm": 0.05673246458172798,
      "learning_rate": 3.319333333333334e-05,
      "loss": 0.002,
      "step": 50420
    },
    {
      "epoch": 2.6896,
      "grad_norm": 0.3119852542877197,
      "learning_rate": 3.319e-05,
      "loss": 0.004,
      "step": 50430
    },
    {
      "epoch": 2.6901333333333333,
      "grad_norm": 0.11346309632062912,
      "learning_rate": 3.3186666666666665e-05,
      "loss": 0.0021,
      "step": 50440
    },
    {
      "epoch": 2.6906666666666665,
      "grad_norm": 0.48217064142227173,
      "learning_rate": 3.318333333333333e-05,
      "loss": 0.0027,
      "step": 50450
    },
    {
      "epoch": 2.6912000000000003,
      "grad_norm": 3.878502852217025e-09,
      "learning_rate": 3.318e-05,
      "loss": 0.0033,
      "step": 50460
    },
    {
      "epoch": 2.6917333333333335,
      "grad_norm": 0.11345354467630386,
      "learning_rate": 3.317666666666667e-05,
      "loss": 0.0034,
      "step": 50470
    },
    {
      "epoch": 2.692266666666667,
      "grad_norm": 0.17018671333789825,
      "learning_rate": 3.3173333333333336e-05,
      "loss": 0.0031,
      "step": 50480
    },
    {
      "epoch": 2.6928,
      "grad_norm": 0.3687216639518738,
      "learning_rate": 3.317e-05,
      "loss": 0.0028,
      "step": 50490
    },
    {
      "epoch": 2.6933333333333334,
      "grad_norm": 0.19854825735092163,
      "learning_rate": 3.316666666666667e-05,
      "loss": 0.0038,
      "step": 50500
    },
    {
      "epoch": 2.6938666666666666,
      "grad_norm": 0.17025652527809143,
      "learning_rate": 3.3163333333333335e-05,
      "loss": 0.001,
      "step": 50510
    },
    {
      "epoch": 2.6944,
      "grad_norm": 0.19853807985782623,
      "learning_rate": 3.316e-05,
      "loss": 0.0015,
      "step": 50520
    },
    {
      "epoch": 2.694933333333333,
      "grad_norm": 0.14204871654510498,
      "learning_rate": 3.315666666666667e-05,
      "loss": 0.0027,
      "step": 50530
    },
    {
      "epoch": 2.6954666666666665,
      "grad_norm": 0.11450111120939255,
      "learning_rate": 3.315333333333333e-05,
      "loss": 0.0023,
      "step": 50540
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 0.5389463305473328,
      "learning_rate": 3.3150000000000006e-05,
      "loss": 0.0026,
      "step": 50550
    },
    {
      "epoch": 2.6965333333333334,
      "grad_norm": 0.0567258782684803,
      "learning_rate": 3.314666666666667e-05,
      "loss": 0.0036,
      "step": 50560
    },
    {
      "epoch": 2.6970666666666667,
      "grad_norm": 0.1701814979314804,
      "learning_rate": 3.314333333333334e-05,
      "loss": 0.0026,
      "step": 50570
    },
    {
      "epoch": 2.6976,
      "grad_norm": 0.311997652053833,
      "learning_rate": 3.314e-05,
      "loss": 0.0027,
      "step": 50580
    },
    {
      "epoch": 2.6981333333333333,
      "grad_norm": 0.36870700120925903,
      "learning_rate": 3.3136666666666664e-05,
      "loss": 0.0017,
      "step": 50590
    },
    {
      "epoch": 2.6986666666666665,
      "grad_norm": 0.19854040443897247,
      "learning_rate": 3.313333333333333e-05,
      "loss": 0.0035,
      "step": 50600
    },
    {
      "epoch": 2.6992000000000003,
      "grad_norm": 0.22689871490001678,
      "learning_rate": 3.313e-05,
      "loss": 0.0028,
      "step": 50610
    },
    {
      "epoch": 2.6997333333333335,
      "grad_norm": 0.08508973568677902,
      "learning_rate": 3.312666666666667e-05,
      "loss": 0.0032,
      "step": 50620
    },
    {
      "epoch": 2.700266666666667,
      "grad_norm": 0.5105099081993103,
      "learning_rate": 3.3123333333333335e-05,
      "loss": 0.0021,
      "step": 50630
    },
    {
      "epoch": 2.7008,
      "grad_norm": 0.11345848441123962,
      "learning_rate": 3.312e-05,
      "loss": 0.0032,
      "step": 50640
    },
    {
      "epoch": 2.7013333333333334,
      "grad_norm": 0.05770593881607056,
      "learning_rate": 3.311666666666667e-05,
      "loss": 0.0026,
      "step": 50650
    },
    {
      "epoch": 2.7018666666666666,
      "grad_norm": 7.968032456062701e-09,
      "learning_rate": 3.3113333333333334e-05,
      "loss": 0.0027,
      "step": 50660
    },
    {
      "epoch": 2.7024,
      "grad_norm": 0.2836131453514099,
      "learning_rate": 3.311e-05,
      "loss": 0.0019,
      "step": 50670
    },
    {
      "epoch": 2.702933333333333,
      "grad_norm": 0.05672961473464966,
      "learning_rate": 3.3106666666666666e-05,
      "loss": 0.004,
      "step": 50680
    },
    {
      "epoch": 2.7034666666666665,
      "grad_norm": 0.19852910935878754,
      "learning_rate": 3.310333333333334e-05,
      "loss": 0.0034,
      "step": 50690
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 0.25526073575019836,
      "learning_rate": 3.3100000000000005e-05,
      "loss": 0.0031,
      "step": 50700
    },
    {
      "epoch": 2.7045333333333335,
      "grad_norm": 0.05672585964202881,
      "learning_rate": 3.309666666666667e-05,
      "loss": 0.0035,
      "step": 50710
    },
    {
      "epoch": 2.7050666666666667,
      "grad_norm": 0.39709511399269104,
      "learning_rate": 3.309333333333334e-05,
      "loss": 0.0035,
      "step": 50720
    },
    {
      "epoch": 2.7056,
      "grad_norm": 0.08508526533842087,
      "learning_rate": 3.309e-05,
      "loss": 0.0029,
      "step": 50730
    },
    {
      "epoch": 2.7061333333333333,
      "grad_norm": 0.028362596407532692,
      "learning_rate": 3.308666666666666e-05,
      "loss": 0.0025,
      "step": 50740
    },
    {
      "epoch": 2.7066666666666666,
      "grad_norm": 0.08508840948343277,
      "learning_rate": 3.3083333333333336e-05,
      "loss": 0.0027,
      "step": 50750
    },
    {
      "epoch": 2.7072000000000003,
      "grad_norm": 0.0850893184542656,
      "learning_rate": 3.308e-05,
      "loss": 0.0036,
      "step": 50760
    },
    {
      "epoch": 2.7077333333333335,
      "grad_norm": 0.14181478321552277,
      "learning_rate": 3.307666666666667e-05,
      "loss": 0.0029,
      "step": 50770
    },
    {
      "epoch": 2.708266666666667,
      "grad_norm": 0.1985277533531189,
      "learning_rate": 3.3073333333333334e-05,
      "loss": 0.0038,
      "step": 50780
    },
    {
      "epoch": 2.7088,
      "grad_norm": 0.056727081537246704,
      "learning_rate": 3.307e-05,
      "loss": 0.0035,
      "step": 50790
    },
    {
      "epoch": 2.7093333333333334,
      "grad_norm": 0.2268957495689392,
      "learning_rate": 3.3066666666666666e-05,
      "loss": 0.0032,
      "step": 50800
    },
    {
      "epoch": 2.7098666666666666,
      "grad_norm": 0.05672396346926689,
      "learning_rate": 3.306333333333333e-05,
      "loss": 0.0032,
      "step": 50810
    },
    {
      "epoch": 2.7104,
      "grad_norm": 3.951672322699551e-09,
      "learning_rate": 3.3060000000000005e-05,
      "loss": 0.0027,
      "step": 50820
    },
    {
      "epoch": 2.710933333333333,
      "grad_norm": 0.14181524515151978,
      "learning_rate": 3.305666666666667e-05,
      "loss": 0.004,
      "step": 50830
    },
    {
      "epoch": 2.7114666666666665,
      "grad_norm": 0.14181587100028992,
      "learning_rate": 3.305333333333334e-05,
      "loss": 0.0029,
      "step": 50840
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 0.2552509605884552,
      "learning_rate": 3.3050000000000004e-05,
      "loss": 0.0023,
      "step": 50850
    },
    {
      "epoch": 2.7125333333333335,
      "grad_norm": 0.1138424426317215,
      "learning_rate": 3.304666666666667e-05,
      "loss": 0.0037,
      "step": 50860
    },
    {
      "epoch": 2.7130666666666667,
      "grad_norm": 0.08508335798978806,
      "learning_rate": 3.3043333333333336e-05,
      "loss": 0.0034,
      "step": 50870
    },
    {
      "epoch": 2.7136,
      "grad_norm": 0.08508360385894775,
      "learning_rate": 3.304e-05,
      "loss": 0.0031,
      "step": 50880
    },
    {
      "epoch": 2.7141333333333333,
      "grad_norm": 0.3119727075099945,
      "learning_rate": 3.303666666666667e-05,
      "loss": 0.0019,
      "step": 50890
    },
    {
      "epoch": 2.7146666666666666,
      "grad_norm": 0.19852903485298157,
      "learning_rate": 3.3033333333333334e-05,
      "loss": 0.0025,
      "step": 50900
    },
    {
      "epoch": 2.7152,
      "grad_norm": 0.17016719281673431,
      "learning_rate": 3.303e-05,
      "loss": 0.0026,
      "step": 50910
    },
    {
      "epoch": 2.7157333333333336,
      "grad_norm": 0.14180375635623932,
      "learning_rate": 3.302666666666667e-05,
      "loss": 0.0028,
      "step": 50920
    },
    {
      "epoch": 2.716266666666667,
      "grad_norm": 2.5426647454196427e-09,
      "learning_rate": 3.302333333333333e-05,
      "loss": 0.0025,
      "step": 50930
    },
    {
      "epoch": 2.7168,
      "grad_norm": 0.31199705600738525,
      "learning_rate": 3.302e-05,
      "loss": 0.0017,
      "step": 50940
    },
    {
      "epoch": 2.7173333333333334,
      "grad_norm": 0.36867663264274597,
      "learning_rate": 3.3016666666666665e-05,
      "loss": 0.0027,
      "step": 50950
    },
    {
      "epoch": 2.7178666666666667,
      "grad_norm": 0.08508926630020142,
      "learning_rate": 3.301333333333334e-05,
      "loss": 0.0023,
      "step": 50960
    },
    {
      "epoch": 2.7184,
      "grad_norm": 0.02836025319993496,
      "learning_rate": 3.3010000000000004e-05,
      "loss": 0.0036,
      "step": 50970
    },
    {
      "epoch": 2.718933333333333,
      "grad_norm": 0.17059798538684845,
      "learning_rate": 3.300666666666667e-05,
      "loss": 0.0036,
      "step": 50980
    },
    {
      "epoch": 2.7194666666666665,
      "grad_norm": 0.17017565667629242,
      "learning_rate": 3.3003333333333336e-05,
      "loss": 0.0028,
      "step": 50990
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.11344706267118454,
      "learning_rate": 3.3e-05,
      "loss": 0.0028,
      "step": 51000
    },
    {
      "epoch": 2.7205333333333335,
      "grad_norm": 0.1417965441942215,
      "learning_rate": 3.299666666666667e-05,
      "loss": 0.0031,
      "step": 51010
    },
    {
      "epoch": 2.7210666666666667,
      "grad_norm": 0.2835956811904907,
      "learning_rate": 3.2993333333333335e-05,
      "loss": 0.0034,
      "step": 51020
    },
    {
      "epoch": 2.7216,
      "grad_norm": 0.283598393201828,
      "learning_rate": 3.299e-05,
      "loss": 0.0029,
      "step": 51030
    },
    {
      "epoch": 2.7221333333333333,
      "grad_norm": 0.48208993673324585,
      "learning_rate": 3.298666666666667e-05,
      "loss": 0.0021,
      "step": 51040
    },
    {
      "epoch": 2.7226666666666666,
      "grad_norm": 1.316615343093872,
      "learning_rate": 3.298333333333333e-05,
      "loss": 0.0033,
      "step": 51050
    },
    {
      "epoch": 2.7232,
      "grad_norm": 2.300312385017378e-09,
      "learning_rate": 3.298e-05,
      "loss": 0.0025,
      "step": 51060
    },
    {
      "epoch": 2.7237333333333336,
      "grad_norm": 0.19851025938987732,
      "learning_rate": 3.2976666666666665e-05,
      "loss": 0.0035,
      "step": 51070
    },
    {
      "epoch": 2.724266666666667,
      "grad_norm": 0.3119616210460663,
      "learning_rate": 3.297333333333333e-05,
      "loss": 0.0023,
      "step": 51080
    },
    {
      "epoch": 2.7248,
      "grad_norm": 0.22687485814094543,
      "learning_rate": 3.297e-05,
      "loss": 0.0024,
      "step": 51090
    },
    {
      "epoch": 2.7253333333333334,
      "grad_norm": 0.08507859706878662,
      "learning_rate": 3.296666666666667e-05,
      "loss": 0.0027,
      "step": 51100
    },
    {
      "epoch": 2.7258666666666667,
      "grad_norm": 0.028359100222587585,
      "learning_rate": 3.296333333333334e-05,
      "loss": 0.0033,
      "step": 51110
    },
    {
      "epoch": 2.7264,
      "grad_norm": 0.453716516494751,
      "learning_rate": 3.296e-05,
      "loss": 0.0046,
      "step": 51120
    },
    {
      "epoch": 2.726933333333333,
      "grad_norm": 0.028359409421682358,
      "learning_rate": 3.295666666666667e-05,
      "loss": 0.0036,
      "step": 51130
    },
    {
      "epoch": 2.7274666666666665,
      "grad_norm": 0.22686731815338135,
      "learning_rate": 3.2953333333333335e-05,
      "loss": 0.0035,
      "step": 51140
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 4.761393501695466e-09,
      "learning_rate": 3.295e-05,
      "loss": 0.0033,
      "step": 51150
    },
    {
      "epoch": 2.7285333333333335,
      "grad_norm": 0.02835875190794468,
      "learning_rate": 3.294666666666667e-05,
      "loss": 0.004,
      "step": 51160
    },
    {
      "epoch": 2.7290666666666668,
      "grad_norm": 0.34032872319221497,
      "learning_rate": 3.294333333333334e-05,
      "loss": 0.0031,
      "step": 51170
    },
    {
      "epoch": 2.7296,
      "grad_norm": 0.05671508237719536,
      "learning_rate": 3.2940000000000006e-05,
      "loss": 0.0029,
      "step": 51180
    },
    {
      "epoch": 2.7301333333333333,
      "grad_norm": 0.6081781387329102,
      "learning_rate": 3.2936666666666666e-05,
      "loss": 0.0033,
      "step": 51190
    },
    {
      "epoch": 2.7306666666666666,
      "grad_norm": 0.028358908370137215,
      "learning_rate": 3.293333333333333e-05,
      "loss": 0.003,
      "step": 51200
    },
    {
      "epoch": 2.7312,
      "grad_norm": 7.500205789767733e-09,
      "learning_rate": 3.293e-05,
      "loss": 0.0032,
      "step": 51210
    },
    {
      "epoch": 2.7317333333333336,
      "grad_norm": 0.17015573382377625,
      "learning_rate": 3.2926666666666664e-05,
      "loss": 0.003,
      "step": 51220
    },
    {
      "epoch": 2.732266666666667,
      "grad_norm": 0.11342930048704147,
      "learning_rate": 3.292333333333334e-05,
      "loss": 0.003,
      "step": 51230
    },
    {
      "epoch": 2.7328,
      "grad_norm": 0.1985127329826355,
      "learning_rate": 3.292e-05,
      "loss": 0.0038,
      "step": 51240
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 0.2554170489311218,
      "learning_rate": 3.291666666666667e-05,
      "loss": 0.0037,
      "step": 51250
    },
    {
      "epoch": 2.7338666666666667,
      "grad_norm": 0.28357359766960144,
      "learning_rate": 3.2913333333333336e-05,
      "loss": 0.0028,
      "step": 51260
    },
    {
      "epoch": 2.7344,
      "grad_norm": 0.02931300736963749,
      "learning_rate": 3.291e-05,
      "loss": 0.0025,
      "step": 51270
    },
    {
      "epoch": 2.734933333333333,
      "grad_norm": 0.08507140725851059,
      "learning_rate": 3.290666666666667e-05,
      "loss": 0.0036,
      "step": 51280
    },
    {
      "epoch": 2.7354666666666665,
      "grad_norm": 0.5103768706321716,
      "learning_rate": 3.2903333333333334e-05,
      "loss": 0.0032,
      "step": 51290
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 0.19849736988544464,
      "learning_rate": 3.29e-05,
      "loss": 0.003,
      "step": 51300
    },
    {
      "epoch": 2.7365333333333335,
      "grad_norm": 0.28356698155403137,
      "learning_rate": 3.289666666666667e-05,
      "loss": 0.0032,
      "step": 51310
    },
    {
      "epoch": 2.7370666666666668,
      "grad_norm": 0.3685988187789917,
      "learning_rate": 3.289333333333334e-05,
      "loss": 0.0032,
      "step": 51320
    },
    {
      "epoch": 2.7376,
      "grad_norm": 1.5163500677317643e-08,
      "learning_rate": 3.2890000000000005e-05,
      "loss": 0.0041,
      "step": 51330
    },
    {
      "epoch": 2.7381333333333333,
      "grad_norm": 4.20231599518317e-10,
      "learning_rate": 3.2886666666666665e-05,
      "loss": 0.0037,
      "step": 51340
    },
    {
      "epoch": 2.7386666666666666,
      "grad_norm": 0.0573677197098732,
      "learning_rate": 3.288333333333333e-05,
      "loss": 0.0024,
      "step": 51350
    },
    {
      "epoch": 2.7392,
      "grad_norm": 0.19860659539699554,
      "learning_rate": 3.288e-05,
      "loss": 0.0029,
      "step": 51360
    },
    {
      "epoch": 2.7397333333333336,
      "grad_norm": 0.02835647203028202,
      "learning_rate": 3.287666666666667e-05,
      "loss": 0.0041,
      "step": 51370
    },
    {
      "epoch": 2.740266666666667,
      "grad_norm": 0.028354762122035027,
      "learning_rate": 3.2873333333333336e-05,
      "loss": 0.004,
      "step": 51380
    },
    {
      "epoch": 2.7408,
      "grad_norm": 0.14178112149238586,
      "learning_rate": 3.287e-05,
      "loss": 0.0055,
      "step": 51390
    },
    {
      "epoch": 2.7413333333333334,
      "grad_norm": 0.6521039009094238,
      "learning_rate": 3.286666666666667e-05,
      "loss": 0.0029,
      "step": 51400
    },
    {
      "epoch": 2.7418666666666667,
      "grad_norm": 0.14176470041275024,
      "learning_rate": 3.2863333333333334e-05,
      "loss": 0.004,
      "step": 51410
    },
    {
      "epoch": 2.7424,
      "grad_norm": 0.2551949918270111,
      "learning_rate": 3.286e-05,
      "loss": 0.0016,
      "step": 51420
    },
    {
      "epoch": 2.7429333333333332,
      "grad_norm": 0.11341451853513718,
      "learning_rate": 3.285666666666667e-05,
      "loss": 0.0038,
      "step": 51430
    },
    {
      "epoch": 2.7434666666666665,
      "grad_norm": 0.283567875623703,
      "learning_rate": 3.285333333333333e-05,
      "loss": 0.003,
      "step": 51440
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 0.595390260219574,
      "learning_rate": 3.2850000000000006e-05,
      "loss": 0.0035,
      "step": 51450
    },
    {
      "epoch": 2.7445333333333335,
      "grad_norm": 0.17013514041900635,
      "learning_rate": 3.284666666666667e-05,
      "loss": 0.0033,
      "step": 51460
    },
    {
      "epoch": 2.7450666666666668,
      "grad_norm": 0.2551698386669159,
      "learning_rate": 3.284333333333334e-05,
      "loss": 0.0028,
      "step": 51470
    },
    {
      "epoch": 2.7456,
      "grad_norm": 0.08505954593420029,
      "learning_rate": 3.2840000000000004e-05,
      "loss": 0.0025,
      "step": 51480
    },
    {
      "epoch": 2.7461333333333333,
      "grad_norm": 0.31188705563545227,
      "learning_rate": 3.2836666666666663e-05,
      "loss": 0.0022,
      "step": 51490
    },
    {
      "epoch": 2.7466666666666666,
      "grad_norm": 0.05670903995633125,
      "learning_rate": 3.283333333333333e-05,
      "loss": 0.0034,
      "step": 51500
    },
    {
      "epoch": 2.7472,
      "grad_norm": 0.368582546710968,
      "learning_rate": 3.283e-05,
      "loss": 0.0022,
      "step": 51510
    },
    {
      "epoch": 2.7477333333333336,
      "grad_norm": 0.25518330931663513,
      "learning_rate": 3.282666666666667e-05,
      "loss": 0.004,
      "step": 51520
    },
    {
      "epoch": 2.748266666666667,
      "grad_norm": 0.11376989632844925,
      "learning_rate": 3.2823333333333335e-05,
      "loss": 0.0036,
      "step": 51530
    },
    {
      "epoch": 2.7488,
      "grad_norm": 0.3969389796257019,
      "learning_rate": 3.282e-05,
      "loss": 0.004,
      "step": 51540
    },
    {
      "epoch": 2.7493333333333334,
      "grad_norm": 0.08505646139383316,
      "learning_rate": 3.281666666666667e-05,
      "loss": 0.0033,
      "step": 51550
    },
    {
      "epoch": 2.7498666666666667,
      "grad_norm": 0.08506352454423904,
      "learning_rate": 3.281333333333333e-05,
      "loss": 0.004,
      "step": 51560
    },
    {
      "epoch": 2.7504,
      "grad_norm": 0.05670478567481041,
      "learning_rate": 3.281e-05,
      "loss": 0.003,
      "step": 51570
    },
    {
      "epoch": 2.7509333333333332,
      "grad_norm": 2.8838180732293495e-09,
      "learning_rate": 3.280666666666667e-05,
      "loss": 0.0039,
      "step": 51580
    },
    {
      "epoch": 2.7514666666666665,
      "grad_norm": 0.22682341933250427,
      "learning_rate": 3.280333333333334e-05,
      "loss": 0.0032,
      "step": 51590
    },
    {
      "epoch": 2.752,
      "grad_norm": 1.1381099224090576,
      "learning_rate": 3.2800000000000004e-05,
      "loss": 0.0018,
      "step": 51600
    },
    {
      "epoch": 2.7525333333333335,
      "grad_norm": 0.08505495637655258,
      "learning_rate": 3.279666666666667e-05,
      "loss": 0.0036,
      "step": 51610
    },
    {
      "epoch": 2.7530666666666668,
      "grad_norm": 0.17036548256874084,
      "learning_rate": 3.279333333333334e-05,
      "loss": 0.0029,
      "step": 51620
    },
    {
      "epoch": 2.7536,
      "grad_norm": 0.22680778801441193,
      "learning_rate": 3.279e-05,
      "loss": 0.0024,
      "step": 51630
    },
    {
      "epoch": 2.7541333333333333,
      "grad_norm": 0.028351861983537674,
      "learning_rate": 3.278666666666666e-05,
      "loss": 0.0035,
      "step": 51640
    },
    {
      "epoch": 2.7546666666666666,
      "grad_norm": 0.2551598846912384,
      "learning_rate": 3.2783333333333335e-05,
      "loss": 0.0032,
      "step": 51650
    },
    {
      "epoch": 2.7552,
      "grad_norm": 0.0850549265742302,
      "learning_rate": 3.278e-05,
      "loss": 0.0023,
      "step": 51660
    },
    {
      "epoch": 2.7557333333333336,
      "grad_norm": 0.056704044342041016,
      "learning_rate": 3.277666666666667e-05,
      "loss": 0.0026,
      "step": 51670
    },
    {
      "epoch": 2.756266666666667,
      "grad_norm": 0.5670077800750732,
      "learning_rate": 3.2773333333333334e-05,
      "loss": 0.0025,
      "step": 51680
    },
    {
      "epoch": 2.7568,
      "grad_norm": 0.3969002366065979,
      "learning_rate": 3.277e-05,
      "loss": 0.0021,
      "step": 51690
    },
    {
      "epoch": 2.7573333333333334,
      "grad_norm": 0.056705307215452194,
      "learning_rate": 3.2766666666666666e-05,
      "loss": 0.002,
      "step": 51700
    },
    {
      "epoch": 2.7578666666666667,
      "grad_norm": 0.056700199842453,
      "learning_rate": 3.276333333333333e-05,
      "loss": 0.0028,
      "step": 51710
    },
    {
      "epoch": 2.7584,
      "grad_norm": 1.501288537930634e-09,
      "learning_rate": 3.2760000000000005e-05,
      "loss": 0.005,
      "step": 51720
    },
    {
      "epoch": 2.7589333333333332,
      "grad_norm": 0.1984523981809616,
      "learning_rate": 3.275666666666667e-05,
      "loss": 0.0041,
      "step": 51730
    },
    {
      "epoch": 2.7594666666666665,
      "grad_norm": 0.14175660908222198,
      "learning_rate": 3.275333333333334e-05,
      "loss": 0.0022,
      "step": 51740
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.311860591173172,
      "learning_rate": 3.275e-05,
      "loss": 0.0024,
      "step": 51750
    },
    {
      "epoch": 2.760533333333333,
      "grad_norm": 0.02987663261592388,
      "learning_rate": 3.274666666666667e-05,
      "loss": 0.0039,
      "step": 51760
    },
    {
      "epoch": 2.761066666666667,
      "grad_norm": 0.05670497193932533,
      "learning_rate": 3.2743333333333335e-05,
      "loss": 0.0034,
      "step": 51770
    },
    {
      "epoch": 2.7616,
      "grad_norm": 0.4252639710903168,
      "learning_rate": 3.274e-05,
      "loss": 0.0024,
      "step": 51780
    },
    {
      "epoch": 2.7621333333333333,
      "grad_norm": 0.11341234296560287,
      "learning_rate": 3.273666666666667e-05,
      "loss": 0.0041,
      "step": 51790
    },
    {
      "epoch": 2.7626666666666666,
      "grad_norm": 0.48207995295524597,
      "learning_rate": 3.2733333333333334e-05,
      "loss": 0.0026,
      "step": 51800
    },
    {
      "epoch": 2.7632,
      "grad_norm": 0.17010918259620667,
      "learning_rate": 3.273e-05,
      "loss": 0.0025,
      "step": 51810
    },
    {
      "epoch": 2.7637333333333336,
      "grad_norm": 0.05670167878270149,
      "learning_rate": 3.2726666666666666e-05,
      "loss": 0.0021,
      "step": 51820
    },
    {
      "epoch": 2.764266666666667,
      "grad_norm": 0.14174865186214447,
      "learning_rate": 3.272333333333333e-05,
      "loss": 0.0029,
      "step": 51830
    },
    {
      "epoch": 2.7648,
      "grad_norm": 0.1984645128250122,
      "learning_rate": 3.272e-05,
      "loss": 0.0032,
      "step": 51840
    },
    {
      "epoch": 2.7653333333333334,
      "grad_norm": 0.283511221408844,
      "learning_rate": 3.2716666666666665e-05,
      "loss": 0.0039,
      "step": 51850
    },
    {
      "epoch": 2.7658666666666667,
      "grad_norm": 0.056701503694057465,
      "learning_rate": 3.271333333333334e-05,
      "loss": 0.0036,
      "step": 51860
    },
    {
      "epoch": 2.7664,
      "grad_norm": 0.0072062076069414616,
      "learning_rate": 3.2710000000000004e-05,
      "loss": 0.0025,
      "step": 51870
    },
    {
      "epoch": 2.7669333333333332,
      "grad_norm": 1.2926100190213674e-09,
      "learning_rate": 3.270666666666667e-05,
      "loss": 0.0031,
      "step": 51880
    },
    {
      "epoch": 2.7674666666666665,
      "grad_norm": 0.028349990025162697,
      "learning_rate": 3.2703333333333336e-05,
      "loss": 0.0029,
      "step": 51890
    },
    {
      "epoch": 2.768,
      "grad_norm": 0.19856423139572144,
      "learning_rate": 3.27e-05,
      "loss": 0.0047,
      "step": 51900
    },
    {
      "epoch": 2.768533333333333,
      "grad_norm": 0.056704405695199966,
      "learning_rate": 3.269666666666667e-05,
      "loss": 0.0021,
      "step": 51910
    },
    {
      "epoch": 2.769066666666667,
      "grad_norm": 0.34029898047447205,
      "learning_rate": 3.2693333333333334e-05,
      "loss": 0.0031,
      "step": 51920
    },
    {
      "epoch": 2.7696,
      "grad_norm": 0.36857983469963074,
      "learning_rate": 3.269000000000001e-05,
      "loss": 0.0017,
      "step": 51930
    },
    {
      "epoch": 2.7701333333333333,
      "grad_norm": 0.08505459129810333,
      "learning_rate": 3.268666666666667e-05,
      "loss": 0.0024,
      "step": 51940
    },
    {
      "epoch": 2.7706666666666666,
      "grad_norm": 0.005342856515198946,
      "learning_rate": 3.268333333333333e-05,
      "loss": 0.0025,
      "step": 51950
    },
    {
      "epoch": 2.7712,
      "grad_norm": 0.17030936479568481,
      "learning_rate": 3.268e-05,
      "loss": 0.003,
      "step": 51960
    },
    {
      "epoch": 2.7717333333333336,
      "grad_norm": 0.1704905480146408,
      "learning_rate": 3.2676666666666665e-05,
      "loss": 0.0029,
      "step": 51970
    },
    {
      "epoch": 2.772266666666667,
      "grad_norm": 0.17010679841041565,
      "learning_rate": 3.267333333333333e-05,
      "loss": 0.0031,
      "step": 51980
    },
    {
      "epoch": 2.7728,
      "grad_norm": 0.39686185121536255,
      "learning_rate": 3.267e-05,
      "loss": 0.0036,
      "step": 51990
    },
    {
      "epoch": 2.7733333333333334,
      "grad_norm": 0.5386644005775452,
      "learning_rate": 3.266666666666667e-05,
      "loss": 0.003,
      "step": 52000
    },
    {
      "epoch": 2.7738666666666667,
      "grad_norm": 0.11339327692985535,
      "learning_rate": 3.2663333333333336e-05,
      "loss": 0.0028,
      "step": 52010
    },
    {
      "epoch": 2.7744,
      "grad_norm": 0.08504852652549744,
      "learning_rate": 3.266e-05,
      "loss": 0.0032,
      "step": 52020
    },
    {
      "epoch": 2.7749333333333333,
      "grad_norm": 0.028347741812467575,
      "learning_rate": 3.265666666666667e-05,
      "loss": 0.0032,
      "step": 52030
    },
    {
      "epoch": 2.7754666666666665,
      "grad_norm": 0.7356992363929749,
      "learning_rate": 3.2653333333333335e-05,
      "loss": 0.0035,
      "step": 52040
    },
    {
      "epoch": 2.776,
      "grad_norm": 0.11339602619409561,
      "learning_rate": 3.265e-05,
      "loss": 0.0023,
      "step": 52050
    },
    {
      "epoch": 2.776533333333333,
      "grad_norm": 0.02834792248904705,
      "learning_rate": 3.264666666666667e-05,
      "loss": 0.0048,
      "step": 52060
    },
    {
      "epoch": 2.777066666666667,
      "grad_norm": 0.11338988691568375,
      "learning_rate": 3.264333333333334e-05,
      "loss": 0.0051,
      "step": 52070
    },
    {
      "epoch": 2.7776,
      "grad_norm": 0.17008455097675323,
      "learning_rate": 3.2640000000000006e-05,
      "loss": 0.0022,
      "step": 52080
    },
    {
      "epoch": 2.7781333333333333,
      "grad_norm": 0.2551479935646057,
      "learning_rate": 3.263666666666667e-05,
      "loss": 0.0015,
      "step": 52090
    },
    {
      "epoch": 2.7786666666666666,
      "grad_norm": 0.1704210788011551,
      "learning_rate": 3.263333333333333e-05,
      "loss": 0.0025,
      "step": 52100
    },
    {
      "epoch": 2.7792,
      "grad_norm": 0.19844284653663635,
      "learning_rate": 3.263e-05,
      "loss": 0.0035,
      "step": 52110
    },
    {
      "epoch": 2.779733333333333,
      "grad_norm": 1.0274745498861648e-09,
      "learning_rate": 3.2626666666666664e-05,
      "loss": 0.0024,
      "step": 52120
    },
    {
      "epoch": 2.780266666666667,
      "grad_norm": 0.08504471182823181,
      "learning_rate": 3.262333333333334e-05,
      "loss": 0.0025,
      "step": 52130
    },
    {
      "epoch": 2.7808,
      "grad_norm": 0.05669713020324707,
      "learning_rate": 3.262e-05,
      "loss": 0.0037,
      "step": 52140
    },
    {
      "epoch": 2.7813333333333334,
      "grad_norm": 0.1133980080485344,
      "learning_rate": 3.261666666666667e-05,
      "loss": 0.0032,
      "step": 52150
    },
    {
      "epoch": 2.7818666666666667,
      "grad_norm": 0.05669788271188736,
      "learning_rate": 3.2613333333333335e-05,
      "loss": 0.0028,
      "step": 52160
    },
    {
      "epoch": 2.7824,
      "grad_norm": 0.5952691435813904,
      "learning_rate": 3.261e-05,
      "loss": 0.0035,
      "step": 52170
    },
    {
      "epoch": 2.7829333333333333,
      "grad_norm": 0.4252490699291229,
      "learning_rate": 3.260666666666667e-05,
      "loss": 0.0033,
      "step": 52180
    },
    {
      "epoch": 2.7834666666666665,
      "grad_norm": 0.11338819563388824,
      "learning_rate": 3.2603333333333333e-05,
      "loss": 0.003,
      "step": 52190
    },
    {
      "epoch": 2.784,
      "grad_norm": 0.11339657753705978,
      "learning_rate": 3.26e-05,
      "loss": 0.003,
      "step": 52200
    },
    {
      "epoch": 2.784533333333333,
      "grad_norm": 0.3968481123447418,
      "learning_rate": 3.259666666666667e-05,
      "loss": 0.0036,
      "step": 52210
    },
    {
      "epoch": 2.785066666666667,
      "grad_norm": 0.17008398473262787,
      "learning_rate": 3.259333333333334e-05,
      "loss": 0.0033,
      "step": 52220
    },
    {
      "epoch": 2.7856,
      "grad_norm": 0.17008240520954132,
      "learning_rate": 3.2590000000000005e-05,
      "loss": 0.0031,
      "step": 52230
    },
    {
      "epoch": 2.7861333333333334,
      "grad_norm": 3.4487288580464792e-09,
      "learning_rate": 3.258666666666667e-05,
      "loss": 0.0028,
      "step": 52240
    },
    {
      "epoch": 2.7866666666666666,
      "grad_norm": 0.14173926413059235,
      "learning_rate": 3.258333333333333e-05,
      "loss": 0.0032,
      "step": 52250
    },
    {
      "epoch": 2.7872,
      "grad_norm": 0.19843246042728424,
      "learning_rate": 3.2579999999999996e-05,
      "loss": 0.003,
      "step": 52260
    },
    {
      "epoch": 2.787733333333333,
      "grad_norm": 0.25512370467185974,
      "learning_rate": 3.257666666666667e-05,
      "loss": 0.0027,
      "step": 52270
    },
    {
      "epoch": 2.788266666666667,
      "grad_norm": 0.48189231753349304,
      "learning_rate": 3.2573333333333335e-05,
      "loss": 0.0023,
      "step": 52280
    },
    {
      "epoch": 2.7888,
      "grad_norm": 0.25512605905532837,
      "learning_rate": 3.257e-05,
      "loss": 0.0032,
      "step": 52290
    },
    {
      "epoch": 2.7893333333333334,
      "grad_norm": 0.1133880466222763,
      "learning_rate": 3.256666666666667e-05,
      "loss": 0.0018,
      "step": 52300
    },
    {
      "epoch": 2.7898666666666667,
      "grad_norm": 1.6757785159526861e-09,
      "learning_rate": 3.2563333333333334e-05,
      "loss": 0.0031,
      "step": 52310
    },
    {
      "epoch": 2.7904,
      "grad_norm": 0.5385628342628479,
      "learning_rate": 3.256e-05,
      "loss": 0.0025,
      "step": 52320
    },
    {
      "epoch": 2.7909333333333333,
      "grad_norm": 0.3685358166694641,
      "learning_rate": 3.2556666666666666e-05,
      "loss": 0.0023,
      "step": 52330
    },
    {
      "epoch": 2.7914666666666665,
      "grad_norm": 0.3401508927345276,
      "learning_rate": 3.255333333333334e-05,
      "loss": 0.0041,
      "step": 52340
    },
    {
      "epoch": 2.792,
      "grad_norm": 0.39687928557395935,
      "learning_rate": 3.2550000000000005e-05,
      "loss": 0.0025,
      "step": 52350
    },
    {
      "epoch": 2.792533333333333,
      "grad_norm": 0.22675758600234985,
      "learning_rate": 3.254666666666667e-05,
      "loss": 0.0032,
      "step": 52360
    },
    {
      "epoch": 2.793066666666667,
      "grad_norm": 3.659096359243108e-09,
      "learning_rate": 3.254333333333334e-05,
      "loss": 0.002,
      "step": 52370
    },
    {
      "epoch": 2.7936,
      "grad_norm": 0.05669103562831879,
      "learning_rate": 3.2540000000000004e-05,
      "loss": 0.003,
      "step": 52380
    },
    {
      "epoch": 2.7941333333333334,
      "grad_norm": 0.08657611906528473,
      "learning_rate": 3.253666666666667e-05,
      "loss": 0.0031,
      "step": 52390
    },
    {
      "epoch": 2.7946666666666666,
      "grad_norm": 0.14172618091106415,
      "learning_rate": 3.253333333333333e-05,
      "loss": 0.0029,
      "step": 52400
    },
    {
      "epoch": 2.7952,
      "grad_norm": 0.14172683656215668,
      "learning_rate": 3.253e-05,
      "loss": 0.0024,
      "step": 52410
    },
    {
      "epoch": 2.795733333333333,
      "grad_norm": 0.2834589183330536,
      "learning_rate": 3.252666666666667e-05,
      "loss": 0.0049,
      "step": 52420
    },
    {
      "epoch": 2.796266666666667,
      "grad_norm": 0.028345920145511627,
      "learning_rate": 3.2523333333333334e-05,
      "loss": 0.003,
      "step": 52430
    },
    {
      "epoch": 2.7968,
      "grad_norm": 0.14172427356243134,
      "learning_rate": 3.252e-05,
      "loss": 0.0034,
      "step": 52440
    },
    {
      "epoch": 2.7973333333333334,
      "grad_norm": 0.02834613248705864,
      "learning_rate": 3.2516666666666666e-05,
      "loss": 0.0038,
      "step": 52450
    },
    {
      "epoch": 2.7978666666666667,
      "grad_norm": 0.08504237979650497,
      "learning_rate": 3.251333333333333e-05,
      "loss": 0.003,
      "step": 52460
    },
    {
      "epoch": 2.7984,
      "grad_norm": 0.19827470183372498,
      "learning_rate": 3.251e-05,
      "loss": 0.0038,
      "step": 52470
    },
    {
      "epoch": 2.7989333333333333,
      "grad_norm": 0.08490655571222305,
      "learning_rate": 3.250666666666667e-05,
      "loss": 0.0028,
      "step": 52480
    },
    {
      "epoch": 2.7994666666666665,
      "grad_norm": 0.226413756608963,
      "learning_rate": 3.250333333333334e-05,
      "loss": 0.0022,
      "step": 52490
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.1698099821805954,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 0.0019,
      "step": 52500
    },
    {
      "epoch": 2.800533333333333,
      "grad_norm": 0.39633142948150635,
      "learning_rate": 3.249666666666667e-05,
      "loss": 0.0021,
      "step": 52510
    },
    {
      "epoch": 2.801066666666667,
      "grad_norm": 0.11347317695617676,
      "learning_rate": 3.2493333333333336e-05,
      "loss": 0.0031,
      "step": 52520
    },
    {
      "epoch": 2.8016,
      "grad_norm": 0.33965349197387695,
      "learning_rate": 3.249e-05,
      "loss": 0.0037,
      "step": 52530
    },
    {
      "epoch": 2.8021333333333334,
      "grad_norm": 0.16980791091918945,
      "learning_rate": 3.248666666666667e-05,
      "loss": 0.0034,
      "step": 52540
    },
    {
      "epoch": 2.8026666666666666,
      "grad_norm": 0.17033152282238007,
      "learning_rate": 3.2483333333333335e-05,
      "loss": 0.0023,
      "step": 52550
    },
    {
      "epoch": 2.8032,
      "grad_norm": 0.05660117417573929,
      "learning_rate": 3.248e-05,
      "loss": 0.0033,
      "step": 52560
    },
    {
      "epoch": 2.803733333333333,
      "grad_norm": 0.17009535431861877,
      "learning_rate": 3.247666666666667e-05,
      "loss": 0.004,
      "step": 52570
    },
    {
      "epoch": 2.804266666666667,
      "grad_norm": 0.028300559148192406,
      "learning_rate": 3.247333333333333e-05,
      "loss": 0.0032,
      "step": 52580
    },
    {
      "epoch": 2.8048,
      "grad_norm": 0.028302408754825592,
      "learning_rate": 3.247e-05,
      "loss": 0.0026,
      "step": 52590
    },
    {
      "epoch": 2.8053333333333335,
      "grad_norm": 0.3961942791938782,
      "learning_rate": 3.2466666666666665e-05,
      "loss": 0.0022,
      "step": 52600
    },
    {
      "epoch": 2.8058666666666667,
      "grad_norm": 0.11320512741804123,
      "learning_rate": 3.246333333333333e-05,
      "loss": 0.0024,
      "step": 52610
    },
    {
      "epoch": 2.8064,
      "grad_norm": 0.028300896286964417,
      "learning_rate": 3.2460000000000004e-05,
      "loss": 0.0026,
      "step": 52620
    },
    {
      "epoch": 2.8069333333333333,
      "grad_norm": 0.283004492521286,
      "learning_rate": 3.245666666666667e-05,
      "loss": 0.0025,
      "step": 52630
    },
    {
      "epoch": 2.8074666666666666,
      "grad_norm": 0.028301144018769264,
      "learning_rate": 3.2453333333333337e-05,
      "loss": 0.0027,
      "step": 52640
    },
    {
      "epoch": 2.808,
      "grad_norm": 0.14150097966194153,
      "learning_rate": 3.245e-05,
      "loss": 0.003,
      "step": 52650
    },
    {
      "epoch": 2.808533333333333,
      "grad_norm": 0.16981898248195648,
      "learning_rate": 3.244666666666667e-05,
      "loss": 0.0045,
      "step": 52660
    },
    {
      "epoch": 2.809066666666667,
      "grad_norm": 0.4244903326034546,
      "learning_rate": 3.2443333333333335e-05,
      "loss": 0.0033,
      "step": 52670
    },
    {
      "epoch": 2.8096,
      "grad_norm": 0.16980081796646118,
      "learning_rate": 3.244e-05,
      "loss": 0.0027,
      "step": 52680
    },
    {
      "epoch": 2.8101333333333334,
      "grad_norm": 0.1981048583984375,
      "learning_rate": 3.2436666666666674e-05,
      "loss": 0.0023,
      "step": 52690
    },
    {
      "epoch": 2.8106666666666666,
      "grad_norm": 0.14169688522815704,
      "learning_rate": 3.243333333333333e-05,
      "loss": 0.0033,
      "step": 52700
    },
    {
      "epoch": 2.8112,
      "grad_norm": 0.22642318904399872,
      "learning_rate": 3.243e-05,
      "loss": 0.0021,
      "step": 52710
    },
    {
      "epoch": 2.811733333333333,
      "grad_norm": 0.0849013403058052,
      "learning_rate": 3.2426666666666666e-05,
      "loss": 0.0053,
      "step": 52720
    },
    {
      "epoch": 2.812266666666667,
      "grad_norm": 0.1698092818260193,
      "learning_rate": 3.242333333333333e-05,
      "loss": 0.0045,
      "step": 52730
    },
    {
      "epoch": 2.8128,
      "grad_norm": 0.11319279670715332,
      "learning_rate": 3.242e-05,
      "loss": 0.0027,
      "step": 52740
    },
    {
      "epoch": 2.8133333333333335,
      "grad_norm": 0.11319675296545029,
      "learning_rate": 3.2416666666666664e-05,
      "loss": 0.0031,
      "step": 52750
    },
    {
      "epoch": 2.8138666666666667,
      "grad_norm": 0.028300277888774872,
      "learning_rate": 3.241333333333334e-05,
      "loss": 0.0037,
      "step": 52760
    },
    {
      "epoch": 2.8144,
      "grad_norm": 0.5093649625778198,
      "learning_rate": 3.241e-05,
      "loss": 0.0025,
      "step": 52770
    },
    {
      "epoch": 2.8149333333333333,
      "grad_norm": 0.16979140043258667,
      "learning_rate": 3.240666666666667e-05,
      "loss": 0.0032,
      "step": 52780
    },
    {
      "epoch": 2.8154666666666666,
      "grad_norm": 0.3679288327693939,
      "learning_rate": 3.2403333333333335e-05,
      "loss": 0.0032,
      "step": 52790
    },
    {
      "epoch": 2.816,
      "grad_norm": 0.11319117248058319,
      "learning_rate": 3.24e-05,
      "loss": 0.0019,
      "step": 52800
    },
    {
      "epoch": 2.816533333333333,
      "grad_norm": 0.3113081753253937,
      "learning_rate": 3.239666666666667e-05,
      "loss": 0.003,
      "step": 52810
    },
    {
      "epoch": 2.817066666666667,
      "grad_norm": 0.16978557407855988,
      "learning_rate": 3.2393333333333334e-05,
      "loss": 0.0024,
      "step": 52820
    },
    {
      "epoch": 2.8176,
      "grad_norm": 0.0848982110619545,
      "learning_rate": 3.239000000000001e-05,
      "loss": 0.0044,
      "step": 52830
    },
    {
      "epoch": 2.8181333333333334,
      "grad_norm": 2.0022636881833478e-09,
      "learning_rate": 3.238666666666667e-05,
      "loss": 0.0038,
      "step": 52840
    },
    {
      "epoch": 2.8186666666666667,
      "grad_norm": 0.084896020591259,
      "learning_rate": 3.238333333333333e-05,
      "loss": 0.0034,
      "step": 52850
    },
    {
      "epoch": 2.8192,
      "grad_norm": 0.16980049014091492,
      "learning_rate": 3.238e-05,
      "loss": 0.0025,
      "step": 52860
    },
    {
      "epoch": 2.819733333333333,
      "grad_norm": 0.08490130305290222,
      "learning_rate": 3.2376666666666664e-05,
      "loss": 0.005,
      "step": 52870
    },
    {
      "epoch": 2.820266666666667,
      "grad_norm": 0.05659528821706772,
      "learning_rate": 3.237333333333333e-05,
      "loss": 0.0042,
      "step": 52880
    },
    {
      "epoch": 2.8208,
      "grad_norm": 0.08489309996366501,
      "learning_rate": 3.2370000000000003e-05,
      "loss": 0.0041,
      "step": 52890
    },
    {
      "epoch": 2.8213333333333335,
      "grad_norm": 0.05659814924001694,
      "learning_rate": 3.236666666666667e-05,
      "loss": 0.0036,
      "step": 52900
    },
    {
      "epoch": 2.8218666666666667,
      "grad_norm": 0.42446598410606384,
      "learning_rate": 3.2363333333333336e-05,
      "loss": 0.003,
      "step": 52910
    },
    {
      "epoch": 2.8224,
      "grad_norm": 0.05659785121679306,
      "learning_rate": 3.236e-05,
      "loss": 0.0024,
      "step": 52920
    },
    {
      "epoch": 2.8229333333333333,
      "grad_norm": 0.11319540441036224,
      "learning_rate": 3.235666666666667e-05,
      "loss": 0.0034,
      "step": 52930
    },
    {
      "epoch": 2.8234666666666666,
      "grad_norm": 0.339566707611084,
      "learning_rate": 3.2353333333333334e-05,
      "loss": 0.0026,
      "step": 52940
    },
    {
      "epoch": 2.824,
      "grad_norm": 0.05659981444478035,
      "learning_rate": 3.235e-05,
      "loss": 0.0033,
      "step": 52950
    },
    {
      "epoch": 2.824533333333333,
      "grad_norm": 2.6841400213584166e-09,
      "learning_rate": 3.2346666666666666e-05,
      "loss": 0.0031,
      "step": 52960
    },
    {
      "epoch": 2.8250666666666664,
      "grad_norm": 0.05659690871834755,
      "learning_rate": 3.234333333333334e-05,
      "loss": 0.0029,
      "step": 52970
    },
    {
      "epoch": 2.8256,
      "grad_norm": 0.3961532413959503,
      "learning_rate": 3.2340000000000005e-05,
      "loss": 0.0028,
      "step": 52980
    },
    {
      "epoch": 2.8261333333333334,
      "grad_norm": 0.08489383012056351,
      "learning_rate": 3.233666666666667e-05,
      "loss": 0.0036,
      "step": 52990
    },
    {
      "epoch": 2.8266666666666667,
      "grad_norm": 0.25467491149902344,
      "learning_rate": 3.233333333333333e-05,
      "loss": 0.0038,
      "step": 53000
    },
    {
      "epoch": 2.8272,
      "grad_norm": 0.14149510860443115,
      "learning_rate": 3.233e-05,
      "loss": 0.0026,
      "step": 53010
    },
    {
      "epoch": 2.827733333333333,
      "grad_norm": 0.05659349635243416,
      "learning_rate": 3.232666666666666e-05,
      "loss": 0.0035,
      "step": 53020
    },
    {
      "epoch": 2.828266666666667,
      "grad_norm": 0.4245043396949768,
      "learning_rate": 3.2323333333333336e-05,
      "loss": 0.0041,
      "step": 53030
    },
    {
      "epoch": 2.8288,
      "grad_norm": 0.2546699345111847,
      "learning_rate": 3.232e-05,
      "loss": 0.0027,
      "step": 53040
    },
    {
      "epoch": 2.8293333333333335,
      "grad_norm": 0.14149032533168793,
      "learning_rate": 3.231666666666667e-05,
      "loss": 0.0023,
      "step": 53050
    },
    {
      "epoch": 2.8298666666666668,
      "grad_norm": 0.11319438368082047,
      "learning_rate": 3.2313333333333335e-05,
      "loss": 0.0015,
      "step": 53060
    },
    {
      "epoch": 2.8304,
      "grad_norm": 0.05659294128417969,
      "learning_rate": 3.231e-05,
      "loss": 0.002,
      "step": 53070
    },
    {
      "epoch": 2.8309333333333333,
      "grad_norm": 0.0565950907766819,
      "learning_rate": 3.230666666666667e-05,
      "loss": 0.0031,
      "step": 53080
    },
    {
      "epoch": 2.8314666666666666,
      "grad_norm": 0.05659608542919159,
      "learning_rate": 3.230333333333333e-05,
      "loss": 0.0028,
      "step": 53090
    },
    {
      "epoch": 2.832,
      "grad_norm": 0.028297390788793564,
      "learning_rate": 3.2300000000000006e-05,
      "loss": 0.0025,
      "step": 53100
    },
    {
      "epoch": 2.832533333333333,
      "grad_norm": 0.5659735798835754,
      "learning_rate": 3.229666666666667e-05,
      "loss": 0.0028,
      "step": 53110
    },
    {
      "epoch": 2.8330666666666664,
      "grad_norm": 0.2829601466655731,
      "learning_rate": 3.229333333333334e-05,
      "loss": 0.0033,
      "step": 53120
    },
    {
      "epoch": 2.8336,
      "grad_norm": 0.057607557624578476,
      "learning_rate": 3.2290000000000004e-05,
      "loss": 0.0035,
      "step": 53130
    },
    {
      "epoch": 2.8341333333333334,
      "grad_norm": 0.3395439386367798,
      "learning_rate": 3.228666666666667e-05,
      "loss": 0.0036,
      "step": 53140
    },
    {
      "epoch": 2.8346666666666667,
      "grad_norm": 0.08489315211772919,
      "learning_rate": 3.2283333333333337e-05,
      "loss": 0.0027,
      "step": 53150
    },
    {
      "epoch": 2.8352,
      "grad_norm": 0.396140456199646,
      "learning_rate": 3.2279999999999996e-05,
      "loss": 0.0031,
      "step": 53160
    },
    {
      "epoch": 2.835733333333333,
      "grad_norm": 0.3112630248069763,
      "learning_rate": 3.227666666666667e-05,
      "loss": 0.0029,
      "step": 53170
    },
    {
      "epoch": 2.836266666666667,
      "grad_norm": 0.02829720266163349,
      "learning_rate": 3.2273333333333335e-05,
      "loss": 0.0026,
      "step": 53180
    },
    {
      "epoch": 2.8368,
      "grad_norm": 0.19808053970336914,
      "learning_rate": 3.227e-05,
      "loss": 0.0036,
      "step": 53190
    },
    {
      "epoch": 2.8373333333333335,
      "grad_norm": 0.28294637799263,
      "learning_rate": 3.226666666666667e-05,
      "loss": 0.0024,
      "step": 53200
    },
    {
      "epoch": 2.8378666666666668,
      "grad_norm": 2.189832981613904e-09,
      "learning_rate": 3.226333333333333e-05,
      "loss": 0.0035,
      "step": 53210
    },
    {
      "epoch": 2.8384,
      "grad_norm": 0.1980801820755005,
      "learning_rate": 3.226e-05,
      "loss": 0.0039,
      "step": 53220
    },
    {
      "epoch": 2.8389333333333333,
      "grad_norm": 0.056592170149087906,
      "learning_rate": 3.2256666666666666e-05,
      "loss": 0.0038,
      "step": 53230
    },
    {
      "epoch": 2.8394666666666666,
      "grad_norm": 0.08488909900188446,
      "learning_rate": 3.225333333333334e-05,
      "loss": 0.0025,
      "step": 53240
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.08488644659519196,
      "learning_rate": 3.2250000000000005e-05,
      "loss": 0.0028,
      "step": 53250
    },
    {
      "epoch": 2.840533333333333,
      "grad_norm": 0.1983039528131485,
      "learning_rate": 3.224666666666667e-05,
      "loss": 0.003,
      "step": 53260
    },
    {
      "epoch": 2.8410666666666664,
      "grad_norm": 0.16977187991142273,
      "learning_rate": 3.224333333333334e-05,
      "loss": 0.0038,
      "step": 53270
    },
    {
      "epoch": 2.8416,
      "grad_norm": 0.3961280882358551,
      "learning_rate": 3.224e-05,
      "loss": 0.0032,
      "step": 53280
    },
    {
      "epoch": 2.8421333333333334,
      "grad_norm": 0.028297509998083115,
      "learning_rate": 3.223666666666667e-05,
      "loss": 0.0032,
      "step": 53290
    },
    {
      "epoch": 2.8426666666666667,
      "grad_norm": 1.01665198802948,
      "learning_rate": 3.2233333333333335e-05,
      "loss": 0.0017,
      "step": 53300
    },
    {
      "epoch": 2.8432,
      "grad_norm": 0.4245278239250183,
      "learning_rate": 3.223e-05,
      "loss": 0.0025,
      "step": 53310
    },
    {
      "epoch": 2.8437333333333332,
      "grad_norm": 0.7851316928863525,
      "learning_rate": 3.222666666666667e-05,
      "loss": 0.0051,
      "step": 53320
    },
    {
      "epoch": 2.844266666666667,
      "grad_norm": 0.39611610770225525,
      "learning_rate": 3.2223333333333334e-05,
      "loss": 0.0051,
      "step": 53330
    },
    {
      "epoch": 2.8448,
      "grad_norm": 0.19805701076984406,
      "learning_rate": 3.222e-05,
      "loss": 0.0024,
      "step": 53340
    },
    {
      "epoch": 2.8453333333333335,
      "grad_norm": 0.05659142881631851,
      "learning_rate": 3.2216666666666666e-05,
      "loss": 0.0034,
      "step": 53350
    },
    {
      "epoch": 2.8458666666666668,
      "grad_norm": 0.11398489773273468,
      "learning_rate": 3.221333333333333e-05,
      "loss": 0.0038,
      "step": 53360
    },
    {
      "epoch": 2.8464,
      "grad_norm": 0.1414710432291031,
      "learning_rate": 3.221e-05,
      "loss": 0.0044,
      "step": 53370
    },
    {
      "epoch": 2.8469333333333333,
      "grad_norm": 0.056592460721731186,
      "learning_rate": 3.220666666666667e-05,
      "loss": 0.0025,
      "step": 53380
    },
    {
      "epoch": 2.8474666666666666,
      "grad_norm": 0.028293773531913757,
      "learning_rate": 3.220333333333334e-05,
      "loss": 0.0022,
      "step": 53390
    },
    {
      "epoch": 2.848,
      "grad_norm": 0.02829650230705738,
      "learning_rate": 3.2200000000000003e-05,
      "loss": 0.0046,
      "step": 53400
    },
    {
      "epoch": 2.848533333333333,
      "grad_norm": 0.11316894739866257,
      "learning_rate": 3.219666666666667e-05,
      "loss": 0.0036,
      "step": 53410
    },
    {
      "epoch": 2.8490666666666664,
      "grad_norm": 0.22637276351451874,
      "learning_rate": 3.2193333333333336e-05,
      "loss": 0.0045,
      "step": 53420
    },
    {
      "epoch": 2.8496,
      "grad_norm": 0.05658657103776932,
      "learning_rate": 3.219e-05,
      "loss": 0.0029,
      "step": 53430
    },
    {
      "epoch": 2.8501333333333334,
      "grad_norm": 0.010577019304037094,
      "learning_rate": 3.218666666666667e-05,
      "loss": 0.0037,
      "step": 53440
    },
    {
      "epoch": 2.8506666666666667,
      "grad_norm": 0.11317654699087143,
      "learning_rate": 3.218333333333334e-05,
      "loss": 0.0032,
      "step": 53450
    },
    {
      "epoch": 2.8512,
      "grad_norm": 0.08488181233406067,
      "learning_rate": 3.218e-05,
      "loss": 0.0018,
      "step": 53460
    },
    {
      "epoch": 2.8517333333333332,
      "grad_norm": 0.22634176909923553,
      "learning_rate": 3.2176666666666666e-05,
      "loss": 0.0025,
      "step": 53470
    },
    {
      "epoch": 2.8522666666666665,
      "grad_norm": 0.08488401025533676,
      "learning_rate": 3.217333333333333e-05,
      "loss": 0.0029,
      "step": 53480
    },
    {
      "epoch": 2.8528000000000002,
      "grad_norm": 0.22634585201740265,
      "learning_rate": 3.217e-05,
      "loss": 0.003,
      "step": 53490
    },
    {
      "epoch": 2.8533333333333335,
      "grad_norm": 0.22635871171951294,
      "learning_rate": 3.2166666666666665e-05,
      "loss": 0.0024,
      "step": 53500
    },
    {
      "epoch": 2.8538666666666668,
      "grad_norm": 0.028292670845985413,
      "learning_rate": 3.216333333333333e-05,
      "loss": 0.0029,
      "step": 53510
    },
    {
      "epoch": 2.8544,
      "grad_norm": 0.3112511932849884,
      "learning_rate": 3.2160000000000004e-05,
      "loss": 0.0032,
      "step": 53520
    },
    {
      "epoch": 2.8549333333333333,
      "grad_norm": 0.056585296988487244,
      "learning_rate": 3.215666666666667e-05,
      "loss": 0.003,
      "step": 53530
    },
    {
      "epoch": 2.8554666666666666,
      "grad_norm": 0.19805969297885895,
      "learning_rate": 3.2153333333333336e-05,
      "loss": 0.0034,
      "step": 53540
    },
    {
      "epoch": 2.856,
      "grad_norm": 0.05658479034900665,
      "learning_rate": 3.215e-05,
      "loss": 0.0017,
      "step": 53550
    },
    {
      "epoch": 2.856533333333333,
      "grad_norm": 0.45272722840309143,
      "learning_rate": 3.214666666666667e-05,
      "loss": 0.0027,
      "step": 53560
    },
    {
      "epoch": 2.8570666666666664,
      "grad_norm": 0.11316903680562973,
      "learning_rate": 3.2143333333333334e-05,
      "loss": 0.0042,
      "step": 53570
    },
    {
      "epoch": 2.8576,
      "grad_norm": 1.5064937075592866e-09,
      "learning_rate": 3.214e-05,
      "loss": 0.0031,
      "step": 53580
    },
    {
      "epoch": 2.8581333333333334,
      "grad_norm": 0.2546350657939911,
      "learning_rate": 3.2136666666666674e-05,
      "loss": 0.003,
      "step": 53590
    },
    {
      "epoch": 2.8586666666666667,
      "grad_norm": 0.028292588889598846,
      "learning_rate": 3.213333333333334e-05,
      "loss": 0.0031,
      "step": 53600
    },
    {
      "epoch": 2.8592,
      "grad_norm": 0.33950620889663696,
      "learning_rate": 3.213e-05,
      "loss": 0.0033,
      "step": 53610
    },
    {
      "epoch": 2.8597333333333332,
      "grad_norm": 0.25463229417800903,
      "learning_rate": 3.2126666666666665e-05,
      "loss": 0.0031,
      "step": 53620
    },
    {
      "epoch": 2.8602666666666665,
      "grad_norm": 0.0854884460568428,
      "learning_rate": 3.212333333333333e-05,
      "loss": 0.0048,
      "step": 53630
    },
    {
      "epoch": 2.8608000000000002,
      "grad_norm": 0.2263539880514145,
      "learning_rate": 3.212e-05,
      "loss": 0.0035,
      "step": 53640
    },
    {
      "epoch": 2.8613333333333335,
      "grad_norm": 0.2546233832836151,
      "learning_rate": 3.211666666666667e-05,
      "loss": 0.0025,
      "step": 53650
    },
    {
      "epoch": 2.861866666666667,
      "grad_norm": 0.02829386107623577,
      "learning_rate": 3.2113333333333336e-05,
      "loss": 0.0025,
      "step": 53660
    },
    {
      "epoch": 2.8624,
      "grad_norm": 0.1980362832546234,
      "learning_rate": 3.211e-05,
      "loss": 0.0035,
      "step": 53670
    },
    {
      "epoch": 2.8629333333333333,
      "grad_norm": 0.028292933478951454,
      "learning_rate": 3.210666666666667e-05,
      "loss": 0.0036,
      "step": 53680
    },
    {
      "epoch": 2.8634666666666666,
      "grad_norm": 0.19805845618247986,
      "learning_rate": 3.2103333333333335e-05,
      "loss": 0.0034,
      "step": 53690
    },
    {
      "epoch": 2.864,
      "grad_norm": 0.3677883446216583,
      "learning_rate": 3.21e-05,
      "loss": 0.0032,
      "step": 53700
    },
    {
      "epoch": 2.864533333333333,
      "grad_norm": 0.028293997049331665,
      "learning_rate": 3.209666666666667e-05,
      "loss": 0.0022,
      "step": 53710
    },
    {
      "epoch": 2.8650666666666664,
      "grad_norm": 0.33948758244514465,
      "learning_rate": 3.209333333333333e-05,
      "loss": 0.0041,
      "step": 53720
    },
    {
      "epoch": 2.8656,
      "grad_norm": 0.14146773517131805,
      "learning_rate": 3.2090000000000006e-05,
      "loss": 0.0036,
      "step": 53730
    },
    {
      "epoch": 2.8661333333333334,
      "grad_norm": 0.282903790473938,
      "learning_rate": 3.208666666666667e-05,
      "loss": 0.0042,
      "step": 53740
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 0.1414581537246704,
      "learning_rate": 3.208333333333334e-05,
      "loss": 0.0024,
      "step": 53750
    },
    {
      "epoch": 2.8672,
      "grad_norm": 0.11316538602113724,
      "learning_rate": 3.208e-05,
      "loss": 0.0035,
      "step": 53760
    },
    {
      "epoch": 2.8677333333333332,
      "grad_norm": 0.11316198110580444,
      "learning_rate": 3.2076666666666664e-05,
      "loss": 0.0033,
      "step": 53770
    },
    {
      "epoch": 2.8682666666666665,
      "grad_norm": 0.11317188292741776,
      "learning_rate": 3.207333333333333e-05,
      "loss": 0.0034,
      "step": 53780
    },
    {
      "epoch": 2.8688000000000002,
      "grad_norm": 2.752660543947627e-09,
      "learning_rate": 3.207e-05,
      "loss": 0.0036,
      "step": 53790
    },
    {
      "epoch": 2.8693333333333335,
      "grad_norm": 0.22633029520511627,
      "learning_rate": 3.206666666666667e-05,
      "loss": 0.0042,
      "step": 53800
    },
    {
      "epoch": 2.869866666666667,
      "grad_norm": 0.3112184703350067,
      "learning_rate": 3.2063333333333335e-05,
      "loss": 0.0023,
      "step": 53810
    },
    {
      "epoch": 2.8704,
      "grad_norm": 0.5375051498413086,
      "learning_rate": 3.206e-05,
      "loss": 0.0026,
      "step": 53820
    },
    {
      "epoch": 2.8709333333333333,
      "grad_norm": 0.11317533254623413,
      "learning_rate": 3.205666666666667e-05,
      "loss": 0.0018,
      "step": 53830
    },
    {
      "epoch": 2.8714666666666666,
      "grad_norm": 0.3960612118244171,
      "learning_rate": 3.2053333333333334e-05,
      "loss": 0.0032,
      "step": 53840
    },
    {
      "epoch": 2.872,
      "grad_norm": 0.11316053569316864,
      "learning_rate": 3.205e-05,
      "loss": 0.0042,
      "step": 53850
    },
    {
      "epoch": 2.872533333333333,
      "grad_norm": 0.14145518839359283,
      "learning_rate": 3.2046666666666666e-05,
      "loss": 0.0027,
      "step": 53860
    },
    {
      "epoch": 2.8730666666666664,
      "grad_norm": 0.22632767260074615,
      "learning_rate": 3.204333333333334e-05,
      "loss": 0.004,
      "step": 53870
    },
    {
      "epoch": 2.8736,
      "grad_norm": 0.028290390968322754,
      "learning_rate": 3.2040000000000005e-05,
      "loss": 0.0033,
      "step": 53880
    },
    {
      "epoch": 2.8741333333333334,
      "grad_norm": 0.3111954629421234,
      "learning_rate": 3.203666666666667e-05,
      "loss": 0.0036,
      "step": 53890
    },
    {
      "epoch": 2.8746666666666667,
      "grad_norm": 0.5658851265907288,
      "learning_rate": 3.203333333333334e-05,
      "loss": 0.0028,
      "step": 53900
    },
    {
      "epoch": 2.8752,
      "grad_norm": 0.05658045783638954,
      "learning_rate": 3.2029999999999997e-05,
      "loss": 0.0036,
      "step": 53910
    },
    {
      "epoch": 2.8757333333333333,
      "grad_norm": 0.16974222660064697,
      "learning_rate": 3.202666666666666e-05,
      "loss": 0.0038,
      "step": 53920
    },
    {
      "epoch": 2.8762666666666665,
      "grad_norm": 0.1414511352777481,
      "learning_rate": 3.2023333333333336e-05,
      "loss": 0.0035,
      "step": 53930
    },
    {
      "epoch": 2.8768000000000002,
      "grad_norm": 0.25461164116859436,
      "learning_rate": 3.202e-05,
      "loss": 0.0032,
      "step": 53940
    },
    {
      "epoch": 2.8773333333333335,
      "grad_norm": 0.3395029604434967,
      "learning_rate": 3.201666666666667e-05,
      "loss": 0.0056,
      "step": 53950
    },
    {
      "epoch": 2.877866666666667,
      "grad_norm": 0.05658036097884178,
      "learning_rate": 3.2013333333333334e-05,
      "loss": 0.0029,
      "step": 53960
    },
    {
      "epoch": 2.8784,
      "grad_norm": 0.05658057704567909,
      "learning_rate": 3.201e-05,
      "loss": 0.0035,
      "step": 53970
    },
    {
      "epoch": 2.8789333333333333,
      "grad_norm": 0.16974641382694244,
      "learning_rate": 3.2006666666666666e-05,
      "loss": 0.0036,
      "step": 53980
    },
    {
      "epoch": 2.8794666666666666,
      "grad_norm": 0.11315817385911942,
      "learning_rate": 3.200333333333333e-05,
      "loss": 0.0032,
      "step": 53990
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.028290413320064545,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.0026,
      "step": 54000
    },
    {
      "epoch": 2.880533333333333,
      "grad_norm": 0.05658101290464401,
      "learning_rate": 3.199666666666667e-05,
      "loss": 0.0027,
      "step": 54010
    },
    {
      "epoch": 2.8810666666666664,
      "grad_norm": 0.056579720228910446,
      "learning_rate": 3.199333333333334e-05,
      "loss": 0.0048,
      "step": 54020
    },
    {
      "epoch": 2.8816,
      "grad_norm": 0.3394842743873596,
      "learning_rate": 3.1990000000000004e-05,
      "loss": 0.0031,
      "step": 54030
    },
    {
      "epoch": 2.8821333333333334,
      "grad_norm": 0.1131611242890358,
      "learning_rate": 3.198666666666667e-05,
      "loss": 0.0047,
      "step": 54040
    },
    {
      "epoch": 2.8826666666666667,
      "grad_norm": 0.11315561830997467,
      "learning_rate": 3.1983333333333336e-05,
      "loss": 0.0024,
      "step": 54050
    },
    {
      "epoch": 2.8832,
      "grad_norm": 0.2546122670173645,
      "learning_rate": 3.198e-05,
      "loss": 0.0062,
      "step": 54060
    },
    {
      "epoch": 2.8837333333333333,
      "grad_norm": 0.5657557845115662,
      "learning_rate": 3.197666666666667e-05,
      "loss": 0.0027,
      "step": 54070
    },
    {
      "epoch": 2.8842666666666665,
      "grad_norm": 0.22633060812950134,
      "learning_rate": 3.1973333333333334e-05,
      "loss": 0.0021,
      "step": 54080
    },
    {
      "epoch": 2.8848000000000003,
      "grad_norm": 0.1697346419095993,
      "learning_rate": 3.197e-05,
      "loss": 0.0032,
      "step": 54090
    },
    {
      "epoch": 2.8853333333333335,
      "grad_norm": 0.1414525955915451,
      "learning_rate": 3.196666666666667e-05,
      "loss": 0.0032,
      "step": 54100
    },
    {
      "epoch": 2.885866666666667,
      "grad_norm": 0.1414439082145691,
      "learning_rate": 3.196333333333333e-05,
      "loss": 0.0038,
      "step": 54110
    },
    {
      "epoch": 2.8864,
      "grad_norm": 0.19802148640155792,
      "learning_rate": 3.196e-05,
      "loss": 0.0026,
      "step": 54120
    },
    {
      "epoch": 2.8869333333333334,
      "grad_norm": 0.0565788708627224,
      "learning_rate": 3.1956666666666665e-05,
      "loss": 0.0025,
      "step": 54130
    },
    {
      "epoch": 2.8874666666666666,
      "grad_norm": 0.056577008217573166,
      "learning_rate": 3.195333333333334e-05,
      "loss": 0.0039,
      "step": 54140
    },
    {
      "epoch": 2.888,
      "grad_norm": 0.22632041573524475,
      "learning_rate": 3.1950000000000004e-05,
      "loss": 0.0029,
      "step": 54150
    },
    {
      "epoch": 2.888533333333333,
      "grad_norm": 0.14143884181976318,
      "learning_rate": 3.194666666666667e-05,
      "loss": 0.0026,
      "step": 54160
    },
    {
      "epoch": 2.8890666666666664,
      "grad_norm": 0.198029026389122,
      "learning_rate": 3.1943333333333336e-05,
      "loss": 0.0025,
      "step": 54170
    },
    {
      "epoch": 2.8895999999999997,
      "grad_norm": 0.0848643034696579,
      "learning_rate": 3.194e-05,
      "loss": 0.0019,
      "step": 54180
    },
    {
      "epoch": 2.8901333333333334,
      "grad_norm": 0.6223986148834229,
      "learning_rate": 3.193666666666667e-05,
      "loss": 0.0031,
      "step": 54190
    },
    {
      "epoch": 2.8906666666666667,
      "grad_norm": 0.25458139181137085,
      "learning_rate": 3.1933333333333335e-05,
      "loss": 0.0037,
      "step": 54200
    },
    {
      "epoch": 2.8912,
      "grad_norm": 0.19802618026733398,
      "learning_rate": 3.193e-05,
      "loss": 0.0032,
      "step": 54210
    },
    {
      "epoch": 2.8917333333333333,
      "grad_norm": 0.028287138789892197,
      "learning_rate": 3.192666666666667e-05,
      "loss": 0.0035,
      "step": 54220
    },
    {
      "epoch": 2.8922666666666665,
      "grad_norm": 0.11314906179904938,
      "learning_rate": 3.192333333333333e-05,
      "loss": 0.0019,
      "step": 54230
    },
    {
      "epoch": 2.8928000000000003,
      "grad_norm": 0.11315110325813293,
      "learning_rate": 3.192e-05,
      "loss": 0.0042,
      "step": 54240
    },
    {
      "epoch": 2.8933333333333335,
      "grad_norm": 0.11315670609474182,
      "learning_rate": 3.1916666666666665e-05,
      "loss": 0.0025,
      "step": 54250
    },
    {
      "epoch": 2.893866666666667,
      "grad_norm": 0.1414375752210617,
      "learning_rate": 3.191333333333333e-05,
      "loss": 0.0038,
      "step": 54260
    },
    {
      "epoch": 2.8944,
      "grad_norm": 0.37060827016830444,
      "learning_rate": 3.191e-05,
      "loss": 0.0027,
      "step": 54270
    },
    {
      "epoch": 2.8949333333333334,
      "grad_norm": 0.311169296503067,
      "learning_rate": 3.190666666666667e-05,
      "loss": 0.0034,
      "step": 54280
    },
    {
      "epoch": 2.8954666666666666,
      "grad_norm": 0.028288235887885094,
      "learning_rate": 3.190333333333334e-05,
      "loss": 0.005,
      "step": 54290
    },
    {
      "epoch": 2.896,
      "grad_norm": 0.169722780585289,
      "learning_rate": 3.19e-05,
      "loss": 0.0027,
      "step": 54300
    },
    {
      "epoch": 2.896533333333333,
      "grad_norm": 0.22630468010902405,
      "learning_rate": 3.189666666666667e-05,
      "loss": 0.0034,
      "step": 54310
    },
    {
      "epoch": 2.8970666666666665,
      "grad_norm": 0.08486846089363098,
      "learning_rate": 3.1893333333333335e-05,
      "loss": 0.0028,
      "step": 54320
    },
    {
      "epoch": 2.8975999999999997,
      "grad_norm": 0.11314944177865982,
      "learning_rate": 3.189e-05,
      "loss": 0.0046,
      "step": 54330
    },
    {
      "epoch": 2.8981333333333335,
      "grad_norm": 0.3960362374782562,
      "learning_rate": 3.188666666666667e-05,
      "loss": 0.004,
      "step": 54340
    },
    {
      "epoch": 2.8986666666666667,
      "grad_norm": 0.6505831480026245,
      "learning_rate": 3.188333333333334e-05,
      "loss": 0.0034,
      "step": 54350
    },
    {
      "epoch": 2.8992,
      "grad_norm": 4.825861932289399e-09,
      "learning_rate": 3.188e-05,
      "loss": 0.0032,
      "step": 54360
    },
    {
      "epoch": 2.8997333333333333,
      "grad_norm": 0.11315284669399261,
      "learning_rate": 3.1876666666666666e-05,
      "loss": 0.003,
      "step": 54370
    },
    {
      "epoch": 2.9002666666666665,
      "grad_norm": 0.028288092464208603,
      "learning_rate": 3.187333333333333e-05,
      "loss": 0.0028,
      "step": 54380
    },
    {
      "epoch": 2.9008000000000003,
      "grad_norm": 0.11314541846513748,
      "learning_rate": 3.187e-05,
      "loss": 0.003,
      "step": 54390
    },
    {
      "epoch": 2.9013333333333335,
      "grad_norm": 0.05657433718442917,
      "learning_rate": 3.1866666666666664e-05,
      "loss": 0.0042,
      "step": 54400
    },
    {
      "epoch": 2.901866666666667,
      "grad_norm": 0.25458768010139465,
      "learning_rate": 3.186333333333334e-05,
      "loss": 0.0035,
      "step": 54410
    },
    {
      "epoch": 2.9024,
      "grad_norm": 0.1697162389755249,
      "learning_rate": 3.186e-05,
      "loss": 0.0038,
      "step": 54420
    },
    {
      "epoch": 2.9029333333333334,
      "grad_norm": 0.08486253768205643,
      "learning_rate": 3.185666666666667e-05,
      "loss": 0.003,
      "step": 54430
    },
    {
      "epoch": 2.9034666666666666,
      "grad_norm": 0.08486024290323257,
      "learning_rate": 3.1853333333333336e-05,
      "loss": 0.0032,
      "step": 54440
    },
    {
      "epoch": 2.904,
      "grad_norm": 0.33942532539367676,
      "learning_rate": 3.185e-05,
      "loss": 0.0034,
      "step": 54450
    },
    {
      "epoch": 2.904533333333333,
      "grad_norm": 0.4808643162250519,
      "learning_rate": 3.184666666666667e-05,
      "loss": 0.0034,
      "step": 54460
    },
    {
      "epoch": 2.9050666666666665,
      "grad_norm": 0.11314871907234192,
      "learning_rate": 3.1843333333333334e-05,
      "loss": 0.0027,
      "step": 54470
    },
    {
      "epoch": 2.9055999999999997,
      "grad_norm": 0.1697137951850891,
      "learning_rate": 3.184e-05,
      "loss": 0.0032,
      "step": 54480
    },
    {
      "epoch": 2.9061333333333335,
      "grad_norm": 0.25459274649620056,
      "learning_rate": 3.183666666666667e-05,
      "loss": 0.0034,
      "step": 54490
    },
    {
      "epoch": 2.9066666666666667,
      "grad_norm": 0.056573156267404556,
      "learning_rate": 3.183333333333334e-05,
      "loss": 0.0027,
      "step": 54500
    },
    {
      "epoch": 2.9072,
      "grad_norm": 0.6223601698875427,
      "learning_rate": 3.1830000000000005e-05,
      "loss": 0.0031,
      "step": 54510
    },
    {
      "epoch": 2.9077333333333333,
      "grad_norm": 0.31113407015800476,
      "learning_rate": 3.1826666666666665e-05,
      "loss": 0.0028,
      "step": 54520
    },
    {
      "epoch": 2.9082666666666666,
      "grad_norm": 0.28287938237190247,
      "learning_rate": 3.182333333333333e-05,
      "loss": 0.0048,
      "step": 54530
    },
    {
      "epoch": 2.9088000000000003,
      "grad_norm": 0.311130553483963,
      "learning_rate": 3.182e-05,
      "loss": 0.0038,
      "step": 54540
    },
    {
      "epoch": 2.9093333333333335,
      "grad_norm": 3.4901790618896484,
      "learning_rate": 3.181666666666667e-05,
      "loss": 0.0039,
      "step": 54550
    },
    {
      "epoch": 2.909866666666667,
      "grad_norm": 0.11314539611339569,
      "learning_rate": 3.1813333333333336e-05,
      "loss": 0.0024,
      "step": 54560
    },
    {
      "epoch": 2.9104,
      "grad_norm": 0.028284985572099686,
      "learning_rate": 3.181e-05,
      "loss": 0.0028,
      "step": 54570
    },
    {
      "epoch": 2.9109333333333334,
      "grad_norm": 0.05657081678509712,
      "learning_rate": 3.180666666666667e-05,
      "loss": 0.0027,
      "step": 54580
    },
    {
      "epoch": 2.9114666666666666,
      "grad_norm": 0.05656969174742699,
      "learning_rate": 3.1803333333333334e-05,
      "loss": 0.0037,
      "step": 54590
    },
    {
      "epoch": 2.912,
      "grad_norm": 0.5374822020530701,
      "learning_rate": 3.18e-05,
      "loss": 0.0025,
      "step": 54600
    },
    {
      "epoch": 2.912533333333333,
      "grad_norm": 0.7626683712005615,
      "learning_rate": 3.1796666666666667e-05,
      "loss": 0.0026,
      "step": 54610
    },
    {
      "epoch": 2.9130666666666665,
      "grad_norm": 0.2545677125453949,
      "learning_rate": 3.179333333333333e-05,
      "loss": 0.0038,
      "step": 54620
    },
    {
      "epoch": 2.9135999999999997,
      "grad_norm": 0.028284931555390358,
      "learning_rate": 3.1790000000000006e-05,
      "loss": 0.0028,
      "step": 54630
    },
    {
      "epoch": 2.9141333333333335,
      "grad_norm": 0.02828577347099781,
      "learning_rate": 3.178666666666667e-05,
      "loss": 0.0038,
      "step": 54640
    },
    {
      "epoch": 2.9146666666666667,
      "grad_norm": 3.785810775980281e-09,
      "learning_rate": 3.178333333333334e-05,
      "loss": 0.0039,
      "step": 54650
    },
    {
      "epoch": 2.9152,
      "grad_norm": 0.11314001679420471,
      "learning_rate": 3.1780000000000004e-05,
      "loss": 0.0033,
      "step": 54660
    },
    {
      "epoch": 2.9157333333333333,
      "grad_norm": 0.3111276626586914,
      "learning_rate": 3.1776666666666663e-05,
      "loss": 0.003,
      "step": 54670
    },
    {
      "epoch": 2.9162666666666666,
      "grad_norm": 0.05657300353050232,
      "learning_rate": 3.177333333333333e-05,
      "loss": 0.003,
      "step": 54680
    },
    {
      "epoch": 2.9168,
      "grad_norm": 0.16971002519130707,
      "learning_rate": 3.177e-05,
      "loss": 0.003,
      "step": 54690
    },
    {
      "epoch": 2.9173333333333336,
      "grad_norm": 0.16972298920154572,
      "learning_rate": 3.176666666666667e-05,
      "loss": 0.0045,
      "step": 54700
    },
    {
      "epoch": 2.917866666666667,
      "grad_norm": 0.11314252763986588,
      "learning_rate": 3.1763333333333335e-05,
      "loss": 0.0036,
      "step": 54710
    },
    {
      "epoch": 2.9184,
      "grad_norm": 0.08485899865627289,
      "learning_rate": 3.176e-05,
      "loss": 0.0033,
      "step": 54720
    },
    {
      "epoch": 2.9189333333333334,
      "grad_norm": 0.3111160099506378,
      "learning_rate": 3.175666666666667e-05,
      "loss": 0.0029,
      "step": 54730
    },
    {
      "epoch": 2.9194666666666667,
      "grad_norm": 0.08485858887434006,
      "learning_rate": 3.175333333333333e-05,
      "loss": 0.0031,
      "step": 54740
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.1979825496673584,
      "learning_rate": 3.175e-05,
      "loss": 0.0034,
      "step": 54750
    },
    {
      "epoch": 2.920533333333333,
      "grad_norm": 0.16971981525421143,
      "learning_rate": 3.174666666666667e-05,
      "loss": 0.0036,
      "step": 54760
    },
    {
      "epoch": 2.9210666666666665,
      "grad_norm": 0.42422208189964294,
      "learning_rate": 3.174333333333334e-05,
      "loss": 0.0037,
      "step": 54770
    },
    {
      "epoch": 2.9215999999999998,
      "grad_norm": 0.2545894384384155,
      "learning_rate": 3.1740000000000004e-05,
      "loss": 0.0035,
      "step": 54780
    },
    {
      "epoch": 2.9221333333333335,
      "grad_norm": 0.5090644359588623,
      "learning_rate": 3.173666666666667e-05,
      "loss": 0.003,
      "step": 54790
    },
    {
      "epoch": 2.9226666666666667,
      "grad_norm": 0.5374346971511841,
      "learning_rate": 3.173333333333334e-05,
      "loss": 0.0037,
      "step": 54800
    },
    {
      "epoch": 2.9232,
      "grad_norm": 0.08537738025188446,
      "learning_rate": 3.173e-05,
      "loss": 0.0036,
      "step": 54810
    },
    {
      "epoch": 2.9237333333333333,
      "grad_norm": 3.1453557536309518e-09,
      "learning_rate": 3.172666666666667e-05,
      "loss": 0.007,
      "step": 54820
    },
    {
      "epoch": 2.9242666666666666,
      "grad_norm": 2.2359676332683875e-09,
      "learning_rate": 3.1723333333333335e-05,
      "loss": 0.0029,
      "step": 54830
    },
    {
      "epoch": 2.9248,
      "grad_norm": 0.5090637803077698,
      "learning_rate": 3.172e-05,
      "loss": 0.0028,
      "step": 54840
    },
    {
      "epoch": 2.9253333333333336,
      "grad_norm": 0.36768853664398193,
      "learning_rate": 3.171666666666667e-05,
      "loss": 0.003,
      "step": 54850
    },
    {
      "epoch": 2.925866666666667,
      "grad_norm": 0.2545333206653595,
      "learning_rate": 3.1713333333333334e-05,
      "loss": 0.0038,
      "step": 54860
    },
    {
      "epoch": 2.9264,
      "grad_norm": 0.08484827727079391,
      "learning_rate": 3.171e-05,
      "loss": 0.0037,
      "step": 54870
    },
    {
      "epoch": 2.9269333333333334,
      "grad_norm": 0.11312651634216309,
      "learning_rate": 3.1706666666666666e-05,
      "loss": 0.0031,
      "step": 54880
    },
    {
      "epoch": 2.9274666666666667,
      "grad_norm": 0.05656496807932854,
      "learning_rate": 3.170333333333333e-05,
      "loss": 0.0025,
      "step": 54890
    },
    {
      "epoch": 2.928,
      "grad_norm": 0.45249542593955994,
      "learning_rate": 3.1700000000000005e-05,
      "loss": 0.0025,
      "step": 54900
    },
    {
      "epoch": 2.928533333333333,
      "grad_norm": 0.14141233265399933,
      "learning_rate": 3.169666666666667e-05,
      "loss": 0.0031,
      "step": 54910
    },
    {
      "epoch": 2.9290666666666665,
      "grad_norm": 0.14141036570072174,
      "learning_rate": 3.169333333333334e-05,
      "loss": 0.0031,
      "step": 54920
    },
    {
      "epoch": 2.9295999999999998,
      "grad_norm": 0.25455549359321594,
      "learning_rate": 3.169e-05,
      "loss": 0.0042,
      "step": 54930
    },
    {
      "epoch": 2.9301333333333335,
      "grad_norm": 0.11312565952539444,
      "learning_rate": 3.168666666666667e-05,
      "loss": 0.0035,
      "step": 54940
    },
    {
      "epoch": 2.9306666666666668,
      "grad_norm": 0.028283121064305305,
      "learning_rate": 3.1683333333333335e-05,
      "loss": 0.0036,
      "step": 54950
    },
    {
      "epoch": 2.9312,
      "grad_norm": 0.11312248557806015,
      "learning_rate": 3.168e-05,
      "loss": 0.0033,
      "step": 54960
    },
    {
      "epoch": 2.9317333333333333,
      "grad_norm": 0.056562867015600204,
      "learning_rate": 3.167666666666667e-05,
      "loss": 0.0025,
      "step": 54970
    },
    {
      "epoch": 2.9322666666666666,
      "grad_norm": 0.2828201651573181,
      "learning_rate": 3.1673333333333334e-05,
      "loss": 0.0044,
      "step": 54980
    },
    {
      "epoch": 2.9328,
      "grad_norm": 0.3393951952457428,
      "learning_rate": 3.167e-05,
      "loss": 0.0022,
      "step": 54990
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 0.22625687718391418,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 0.0032,
      "step": 55000
    },
    {
      "epoch": 2.933866666666667,
      "grad_norm": 0.05656274035573006,
      "learning_rate": 3.166333333333333e-05,
      "loss": 0.0024,
      "step": 55010
    },
    {
      "epoch": 2.9344,
      "grad_norm": 0.14140932261943817,
      "learning_rate": 3.166e-05,
      "loss": 0.0019,
      "step": 55020
    },
    {
      "epoch": 2.9349333333333334,
      "grad_norm": 0.14141325652599335,
      "learning_rate": 3.1656666666666665e-05,
      "loss": 0.0028,
      "step": 55030
    },
    {
      "epoch": 2.9354666666666667,
      "grad_norm": 0.650431752204895,
      "learning_rate": 3.165333333333334e-05,
      "loss": 0.0034,
      "step": 55040
    },
    {
      "epoch": 2.936,
      "grad_norm": 0.11313124746084213,
      "learning_rate": 3.1650000000000004e-05,
      "loss": 0.0048,
      "step": 55050
    },
    {
      "epoch": 2.936533333333333,
      "grad_norm": 0.0848410576581955,
      "learning_rate": 3.164666666666667e-05,
      "loss": 0.0036,
      "step": 55060
    },
    {
      "epoch": 2.9370666666666665,
      "grad_norm": 0.22626656293869019,
      "learning_rate": 3.1643333333333336e-05,
      "loss": 0.0017,
      "step": 55070
    },
    {
      "epoch": 2.9375999999999998,
      "grad_norm": 0.2828090190887451,
      "learning_rate": 3.164e-05,
      "loss": 0.0029,
      "step": 55080
    },
    {
      "epoch": 2.9381333333333335,
      "grad_norm": 0.5090594291687012,
      "learning_rate": 3.163666666666667e-05,
      "loss": 0.003,
      "step": 55090
    },
    {
      "epoch": 2.9386666666666668,
      "grad_norm": 0.19796189665794373,
      "learning_rate": 3.1633333333333334e-05,
      "loss": 0.003,
      "step": 55100
    },
    {
      "epoch": 2.9392,
      "grad_norm": 0.22624363005161285,
      "learning_rate": 3.163000000000001e-05,
      "loss": 0.0029,
      "step": 55110
    },
    {
      "epoch": 2.9397333333333333,
      "grad_norm": 0.14140057563781738,
      "learning_rate": 3.1626666666666667e-05,
      "loss": 0.004,
      "step": 55120
    },
    {
      "epoch": 2.9402666666666666,
      "grad_norm": 0.14140191674232483,
      "learning_rate": 3.162333333333333e-05,
      "loss": 0.0018,
      "step": 55130
    },
    {
      "epoch": 2.9408,
      "grad_norm": 0.22624525427818298,
      "learning_rate": 3.162e-05,
      "loss": 0.0023,
      "step": 55140
    },
    {
      "epoch": 2.9413333333333336,
      "grad_norm": 0.028280936181545258,
      "learning_rate": 3.1616666666666665e-05,
      "loss": 0.0034,
      "step": 55150
    },
    {
      "epoch": 2.941866666666667,
      "grad_norm": 0.11311954259872437,
      "learning_rate": 3.161333333333333e-05,
      "loss": 0.0024,
      "step": 55160
    },
    {
      "epoch": 2.9424,
      "grad_norm": 0.1131233498454094,
      "learning_rate": 3.1610000000000004e-05,
      "loss": 0.0027,
      "step": 55170
    },
    {
      "epoch": 2.9429333333333334,
      "grad_norm": 0.22625505924224854,
      "learning_rate": 3.160666666666667e-05,
      "loss": 0.0029,
      "step": 55180
    },
    {
      "epoch": 2.9434666666666667,
      "grad_norm": 0.45247986912727356,
      "learning_rate": 3.1603333333333336e-05,
      "loss": 0.003,
      "step": 55190
    },
    {
      "epoch": 2.944,
      "grad_norm": 0.282802551984787,
      "learning_rate": 3.16e-05,
      "loss": 0.0021,
      "step": 55200
    },
    {
      "epoch": 2.9445333333333332,
      "grad_norm": 0.7693712711334229,
      "learning_rate": 3.159666666666667e-05,
      "loss": 0.0042,
      "step": 55210
    },
    {
      "epoch": 2.9450666666666665,
      "grad_norm": 0.004983179271221161,
      "learning_rate": 3.1593333333333335e-05,
      "loss": 0.003,
      "step": 55220
    },
    {
      "epoch": 2.9455999999999998,
      "grad_norm": 0.16976165771484375,
      "learning_rate": 3.159e-05,
      "loss": 0.0036,
      "step": 55230
    },
    {
      "epoch": 2.9461333333333335,
      "grad_norm": 0.42418771982192993,
      "learning_rate": 3.158666666666667e-05,
      "loss": 0.0047,
      "step": 55240
    },
    {
      "epoch": 2.9466666666666668,
      "grad_norm": 0.19796757400035858,
      "learning_rate": 3.158333333333334e-05,
      "loss": 0.0041,
      "step": 55250
    },
    {
      "epoch": 2.9472,
      "grad_norm": 0.42418280243873596,
      "learning_rate": 3.1580000000000006e-05,
      "loss": 0.0032,
      "step": 55260
    },
    {
      "epoch": 2.9477333333333333,
      "grad_norm": 0.19795528054237366,
      "learning_rate": 3.1576666666666665e-05,
      "loss": 0.0037,
      "step": 55270
    },
    {
      "epoch": 2.9482666666666666,
      "grad_norm": 0.19795599579811096,
      "learning_rate": 3.157333333333333e-05,
      "loss": 0.0027,
      "step": 55280
    },
    {
      "epoch": 2.9488,
      "grad_norm": 0.3393597900867462,
      "learning_rate": 3.157e-05,
      "loss": 0.0021,
      "step": 55290
    },
    {
      "epoch": 2.9493333333333336,
      "grad_norm": 0.3393581807613373,
      "learning_rate": 3.1566666666666664e-05,
      "loss": 0.0021,
      "step": 55300
    },
    {
      "epoch": 2.949866666666667,
      "grad_norm": 0.056562840938568115,
      "learning_rate": 3.156333333333334e-05,
      "loss": 0.0027,
      "step": 55310
    },
    {
      "epoch": 2.9504,
      "grad_norm": 0.7831809520721436,
      "learning_rate": 3.156e-05,
      "loss": 0.0027,
      "step": 55320
    },
    {
      "epoch": 2.9509333333333334,
      "grad_norm": 0.11311903595924377,
      "learning_rate": 3.155666666666667e-05,
      "loss": 0.003,
      "step": 55330
    },
    {
      "epoch": 2.9514666666666667,
      "grad_norm": 0.029974564909934998,
      "learning_rate": 3.1553333333333335e-05,
      "loss": 0.0027,
      "step": 55340
    },
    {
      "epoch": 2.952,
      "grad_norm": 0.08483803272247314,
      "learning_rate": 3.155e-05,
      "loss": 0.0032,
      "step": 55350
    },
    {
      "epoch": 2.9525333333333332,
      "grad_norm": 0.22658132016658783,
      "learning_rate": 3.154666666666667e-05,
      "loss": 0.0022,
      "step": 55360
    },
    {
      "epoch": 2.9530666666666665,
      "grad_norm": 0.2545284628868103,
      "learning_rate": 3.1543333333333333e-05,
      "loss": 0.0022,
      "step": 55370
    },
    {
      "epoch": 2.9536,
      "grad_norm": 0.11311410367488861,
      "learning_rate": 3.154e-05,
      "loss": 0.0028,
      "step": 55380
    },
    {
      "epoch": 2.9541333333333335,
      "grad_norm": 0.028280433267354965,
      "learning_rate": 3.153666666666667e-05,
      "loss": 0.0037,
      "step": 55390
    },
    {
      "epoch": 2.9546666666666668,
      "grad_norm": 0.05655777081847191,
      "learning_rate": 3.153333333333334e-05,
      "loss": 0.0035,
      "step": 55400
    },
    {
      "epoch": 2.9552,
      "grad_norm": 0.16967663168907166,
      "learning_rate": 3.1530000000000005e-05,
      "loss": 0.0039,
      "step": 55410
    },
    {
      "epoch": 2.9557333333333333,
      "grad_norm": 0.08618006110191345,
      "learning_rate": 3.1526666666666664e-05,
      "loss": 0.0024,
      "step": 55420
    },
    {
      "epoch": 2.9562666666666666,
      "grad_norm": 0.1131175085902214,
      "learning_rate": 3.152333333333333e-05,
      "loss": 0.003,
      "step": 55430
    },
    {
      "epoch": 2.9568,
      "grad_norm": 2.942543098072292e-09,
      "learning_rate": 3.1519999999999996e-05,
      "loss": 0.0019,
      "step": 55440
    },
    {
      "epoch": 2.9573333333333336,
      "grad_norm": 0.05655956268310547,
      "learning_rate": 3.151666666666667e-05,
      "loss": 0.0046,
      "step": 55450
    },
    {
      "epoch": 2.957866666666667,
      "grad_norm": 0.33935943245887756,
      "learning_rate": 3.1513333333333335e-05,
      "loss": 0.0031,
      "step": 55460
    },
    {
      "epoch": 2.9584,
      "grad_norm": 0.14139828085899353,
      "learning_rate": 3.151e-05,
      "loss": 0.0031,
      "step": 55470
    },
    {
      "epoch": 2.9589333333333334,
      "grad_norm": 0.05655796453356743,
      "learning_rate": 3.150666666666667e-05,
      "loss": 0.0016,
      "step": 55480
    },
    {
      "epoch": 2.9594666666666667,
      "grad_norm": 0.16967181861400604,
      "learning_rate": 3.1503333333333334e-05,
      "loss": 0.0031,
      "step": 55490
    },
    {
      "epoch": 2.96,
      "grad_norm": 3.1968709901519787e-09,
      "learning_rate": 3.15e-05,
      "loss": 0.0029,
      "step": 55500
    },
    {
      "epoch": 2.9605333333333332,
      "grad_norm": 0.08484268188476562,
      "learning_rate": 3.1496666666666666e-05,
      "loss": 0.0021,
      "step": 55510
    },
    {
      "epoch": 2.9610666666666665,
      "grad_norm": 0.11311891674995422,
      "learning_rate": 3.149333333333334e-05,
      "loss": 0.0051,
      "step": 55520
    },
    {
      "epoch": 2.9616,
      "grad_norm": 8.036057472229004,
      "learning_rate": 3.1490000000000005e-05,
      "loss": 0.0035,
      "step": 55530
    },
    {
      "epoch": 2.962133333333333,
      "grad_norm": 0.22624896466732025,
      "learning_rate": 3.148666666666667e-05,
      "loss": 0.0027,
      "step": 55540
    },
    {
      "epoch": 2.962666666666667,
      "grad_norm": 0.05655514821410179,
      "learning_rate": 3.148333333333334e-05,
      "loss": 0.0035,
      "step": 55550
    },
    {
      "epoch": 2.9632,
      "grad_norm": 0.2828064262866974,
      "learning_rate": 3.1480000000000004e-05,
      "loss": 0.003,
      "step": 55560
    },
    {
      "epoch": 2.9637333333333333,
      "grad_norm": 0.3110572397708893,
      "learning_rate": 3.147666666666666e-05,
      "loss": 0.0029,
      "step": 55570
    },
    {
      "epoch": 2.9642666666666666,
      "grad_norm": 0.1696845442056656,
      "learning_rate": 3.1473333333333336e-05,
      "loss": 0.0019,
      "step": 55580
    },
    {
      "epoch": 2.9648,
      "grad_norm": 0.28278547525405884,
      "learning_rate": 3.147e-05,
      "loss": 0.0033,
      "step": 55590
    },
    {
      "epoch": 2.9653333333333336,
      "grad_norm": 0.14139887690544128,
      "learning_rate": 3.146666666666667e-05,
      "loss": 0.0027,
      "step": 55600
    },
    {
      "epoch": 2.965866666666667,
      "grad_norm": 0.39591383934020996,
      "learning_rate": 3.1463333333333334e-05,
      "loss": 0.0031,
      "step": 55610
    },
    {
      "epoch": 2.9664,
      "grad_norm": 0.16967445611953735,
      "learning_rate": 3.146e-05,
      "loss": 0.0043,
      "step": 55620
    },
    {
      "epoch": 2.9669333333333334,
      "grad_norm": 0.1696670800447464,
      "learning_rate": 3.1456666666666666e-05,
      "loss": 0.0043,
      "step": 55630
    },
    {
      "epoch": 2.9674666666666667,
      "grad_norm": 0.05655952915549278,
      "learning_rate": 3.145333333333333e-05,
      "loss": 0.0036,
      "step": 55640
    },
    {
      "epoch": 2.968,
      "grad_norm": 0.1131189838051796,
      "learning_rate": 3.145e-05,
      "loss": 0.0033,
      "step": 55650
    },
    {
      "epoch": 2.9685333333333332,
      "grad_norm": 0.16966135799884796,
      "learning_rate": 3.144666666666667e-05,
      "loss": 0.0032,
      "step": 55660
    },
    {
      "epoch": 2.9690666666666665,
      "grad_norm": 0.22623313963413239,
      "learning_rate": 3.144333333333334e-05,
      "loss": 0.0025,
      "step": 55670
    },
    {
      "epoch": 2.9696,
      "grad_norm": 0.1696787178516388,
      "learning_rate": 3.1440000000000004e-05,
      "loss": 0.0026,
      "step": 55680
    },
    {
      "epoch": 2.970133333333333,
      "grad_norm": 0.08483700454235077,
      "learning_rate": 3.143666666666667e-05,
      "loss": 0.0024,
      "step": 55690
    },
    {
      "epoch": 2.970666666666667,
      "grad_norm": 0.056559816002845764,
      "learning_rate": 3.1433333333333336e-05,
      "loss": 0.0033,
      "step": 55700
    },
    {
      "epoch": 2.9712,
      "grad_norm": 0.36760053038597107,
      "learning_rate": 3.143e-05,
      "loss": 0.0042,
      "step": 55710
    },
    {
      "epoch": 2.9717333333333333,
      "grad_norm": 0.028278762474656105,
      "learning_rate": 3.142666666666667e-05,
      "loss": 0.0031,
      "step": 55720
    },
    {
      "epoch": 2.9722666666666666,
      "grad_norm": 0.08483127504587173,
      "learning_rate": 3.1423333333333335e-05,
      "loss": 0.0025,
      "step": 55730
    },
    {
      "epoch": 2.9728,
      "grad_norm": 0.33934149146080017,
      "learning_rate": 3.142e-05,
      "loss": 0.0027,
      "step": 55740
    },
    {
      "epoch": 2.9733333333333336,
      "grad_norm": 0.05655519291758537,
      "learning_rate": 3.141666666666667e-05,
      "loss": 0.0028,
      "step": 55750
    },
    {
      "epoch": 2.973866666666667,
      "grad_norm": 0.14138802886009216,
      "learning_rate": 3.141333333333333e-05,
      "loss": 0.0028,
      "step": 55760
    },
    {
      "epoch": 2.9744,
      "grad_norm": 0.3958800435066223,
      "learning_rate": 3.141e-05,
      "loss": 0.0021,
      "step": 55770
    },
    {
      "epoch": 2.9749333333333334,
      "grad_norm": 0.08484018594026566,
      "learning_rate": 3.1406666666666665e-05,
      "loss": 0.0031,
      "step": 55780
    },
    {
      "epoch": 2.9754666666666667,
      "grad_norm": 0.11311265826225281,
      "learning_rate": 3.140333333333333e-05,
      "loss": 0.0037,
      "step": 55790
    },
    {
      "epoch": 2.976,
      "grad_norm": 0.05655604600906372,
      "learning_rate": 3.1400000000000004e-05,
      "loss": 0.0034,
      "step": 55800
    },
    {
      "epoch": 2.9765333333333333,
      "grad_norm": 0.08483237773180008,
      "learning_rate": 3.139666666666667e-05,
      "loss": 0.0033,
      "step": 55810
    },
    {
      "epoch": 2.9770666666666665,
      "grad_norm": 0.2827772796154022,
      "learning_rate": 3.1393333333333337e-05,
      "loss": 0.003,
      "step": 55820
    },
    {
      "epoch": 2.9776,
      "grad_norm": 0.11311812698841095,
      "learning_rate": 3.139e-05,
      "loss": 0.0017,
      "step": 55830
    },
    {
      "epoch": 2.978133333333333,
      "grad_norm": 0.19793595373630524,
      "learning_rate": 3.138666666666667e-05,
      "loss": 0.0052,
      "step": 55840
    },
    {
      "epoch": 2.978666666666667,
      "grad_norm": 0.14138968288898468,
      "learning_rate": 3.1383333333333335e-05,
      "loss": 0.0021,
      "step": 55850
    },
    {
      "epoch": 2.9792,
      "grad_norm": 3.349807320418563e-09,
      "learning_rate": 3.138e-05,
      "loss": 0.0019,
      "step": 55860
    },
    {
      "epoch": 2.9797333333333333,
      "grad_norm": 0.14138709008693695,
      "learning_rate": 3.1376666666666674e-05,
      "loss": 0.0025,
      "step": 55870
    },
    {
      "epoch": 2.9802666666666666,
      "grad_norm": 0.05655483901500702,
      "learning_rate": 3.137333333333333e-05,
      "loss": 0.0043,
      "step": 55880
    },
    {
      "epoch": 2.9808,
      "grad_norm": 0.1696694940328598,
      "learning_rate": 3.137e-05,
      "loss": 0.003,
      "step": 55890
    },
    {
      "epoch": 2.981333333333333,
      "grad_norm": 0.16966810822486877,
      "learning_rate": 3.1366666666666666e-05,
      "loss": 0.0026,
      "step": 55900
    },
    {
      "epoch": 2.981866666666667,
      "grad_norm": 0.25449931621551514,
      "learning_rate": 3.136333333333333e-05,
      "loss": 0.0034,
      "step": 55910
    },
    {
      "epoch": 2.9824,
      "grad_norm": 0.028279142454266548,
      "learning_rate": 3.136e-05,
      "loss": 0.0036,
      "step": 55920
    },
    {
      "epoch": 2.9829333333333334,
      "grad_norm": 0.3110378086566925,
      "learning_rate": 3.135666666666667e-05,
      "loss": 0.003,
      "step": 55930
    },
    {
      "epoch": 2.9834666666666667,
      "grad_norm": 0.08483795076608658,
      "learning_rate": 3.135333333333334e-05,
      "loss": 0.0034,
      "step": 55940
    },
    {
      "epoch": 2.984,
      "grad_norm": 0.14138604700565338,
      "learning_rate": 3.135e-05,
      "loss": 0.0033,
      "step": 55950
    },
    {
      "epoch": 2.9845333333333333,
      "grad_norm": 0.05655542016029358,
      "learning_rate": 3.134666666666667e-05,
      "loss": 0.002,
      "step": 55960
    },
    {
      "epoch": 2.9850666666666665,
      "grad_norm": 0.14138449728488922,
      "learning_rate": 3.1343333333333335e-05,
      "loss": 0.002,
      "step": 55970
    },
    {
      "epoch": 2.9856,
      "grad_norm": 0.45244407653808594,
      "learning_rate": 3.134e-05,
      "loss": 0.0036,
      "step": 55980
    },
    {
      "epoch": 2.986133333333333,
      "grad_norm": 0.19793374836444855,
      "learning_rate": 3.133666666666667e-05,
      "loss": 0.0023,
      "step": 55990
    },
    {
      "epoch": 2.986666666666667,
      "grad_norm": 0.14138424396514893,
      "learning_rate": 3.1333333333333334e-05,
      "loss": 0.0018,
      "step": 56000
    },
    {
      "epoch": 2.9872,
      "grad_norm": 0.28275686502456665,
      "learning_rate": 3.133000000000001e-05,
      "loss": 0.0026,
      "step": 56010
    },
    {
      "epoch": 2.9877333333333334,
      "grad_norm": 0.254487544298172,
      "learning_rate": 3.132666666666667e-05,
      "loss": 0.0029,
      "step": 56020
    },
    {
      "epoch": 2.9882666666666666,
      "grad_norm": 0.25448498129844666,
      "learning_rate": 3.132333333333333e-05,
      "loss": 0.0033,
      "step": 56030
    },
    {
      "epoch": 2.9888,
      "grad_norm": 0.0282763484865427,
      "learning_rate": 3.132e-05,
      "loss": 0.0032,
      "step": 56040
    },
    {
      "epoch": 2.989333333333333,
      "grad_norm": 0.282757967710495,
      "learning_rate": 3.1316666666666664e-05,
      "loss": 0.0024,
      "step": 56050
    },
    {
      "epoch": 2.989866666666667,
      "grad_norm": 0.11310464888811111,
      "learning_rate": 3.131333333333333e-05,
      "loss": 0.0031,
      "step": 56060
    },
    {
      "epoch": 2.9904,
      "grad_norm": 0.22621318697929382,
      "learning_rate": 3.1310000000000003e-05,
      "loss": 0.0038,
      "step": 56070
    },
    {
      "epoch": 2.9909333333333334,
      "grad_norm": 0.3676087558269501,
      "learning_rate": 3.130666666666667e-05,
      "loss": 0.0021,
      "step": 56080
    },
    {
      "epoch": 2.9914666666666667,
      "grad_norm": 0.19793391227722168,
      "learning_rate": 3.1303333333333336e-05,
      "loss": 0.0036,
      "step": 56090
    },
    {
      "epoch": 2.992,
      "grad_norm": 0.254485547542572,
      "learning_rate": 3.13e-05,
      "loss": 0.0038,
      "step": 56100
    },
    {
      "epoch": 2.9925333333333333,
      "grad_norm": 0.11310488730669022,
      "learning_rate": 3.129666666666667e-05,
      "loss": 0.0036,
      "step": 56110
    },
    {
      "epoch": 2.9930666666666665,
      "grad_norm": 0.05655212700366974,
      "learning_rate": 3.1293333333333334e-05,
      "loss": 0.0034,
      "step": 56120
    },
    {
      "epoch": 2.9936,
      "grad_norm": 0.056554511189460754,
      "learning_rate": 3.129e-05,
      "loss": 0.0045,
      "step": 56130
    },
    {
      "epoch": 2.994133333333333,
      "grad_norm": 0.14137858152389526,
      "learning_rate": 3.1286666666666666e-05,
      "loss": 0.0029,
      "step": 56140
    },
    {
      "epoch": 2.994666666666667,
      "grad_norm": 0.02827737294137478,
      "learning_rate": 3.128333333333334e-05,
      "loss": 0.0033,
      "step": 56150
    },
    {
      "epoch": 2.9952,
      "grad_norm": 0.028276946395635605,
      "learning_rate": 3.1280000000000005e-05,
      "loss": 0.0029,
      "step": 56160
    },
    {
      "epoch": 2.9957333333333334,
      "grad_norm": 0.056550875306129456,
      "learning_rate": 3.127666666666667e-05,
      "loss": 0.004,
      "step": 56170
    },
    {
      "epoch": 2.9962666666666666,
      "grad_norm": 0.22620409727096558,
      "learning_rate": 3.127333333333333e-05,
      "loss": 0.0028,
      "step": 56180
    },
    {
      "epoch": 2.9968,
      "grad_norm": 0.05655131861567497,
      "learning_rate": 3.127e-05,
      "loss": 0.0029,
      "step": 56190
    },
    {
      "epoch": 2.997333333333333,
      "grad_norm": 0.2544831931591034,
      "learning_rate": 3.126666666666666e-05,
      "loss": 0.0024,
      "step": 56200
    },
    {
      "epoch": 2.997866666666667,
      "grad_norm": 0.0565539225935936,
      "learning_rate": 3.1263333333333336e-05,
      "loss": 0.0029,
      "step": 56210
    },
    {
      "epoch": 2.9984,
      "grad_norm": 0.16966374218463898,
      "learning_rate": 3.126e-05,
      "loss": 0.0038,
      "step": 56220
    },
    {
      "epoch": 2.9989333333333335,
      "grad_norm": 0.056551460176706314,
      "learning_rate": 3.125666666666667e-05,
      "loss": 0.0023,
      "step": 56230
    },
    {
      "epoch": 2.9994666666666667,
      "grad_norm": 0.08482734858989716,
      "learning_rate": 3.1253333333333335e-05,
      "loss": 0.0025,
      "step": 56240
    },
    {
      "epoch": 3.0,
      "grad_norm": 3.4347760191622e-09,
      "learning_rate": 3.125e-05,
      "loss": 0.0038,
      "step": 56250
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.003302820725366473,
      "eval_runtime": 164.6459,
      "eval_samples_per_second": 1518.411,
      "eval_steps_per_second": 37.96,
      "step": 56250
    },
    {
      "epoch": 3.0005333333333333,
      "grad_norm": 0.11310239136219025,
      "learning_rate": 3.124666666666667e-05,
      "loss": 0.0029,
      "step": 56260
    },
    {
      "epoch": 3.0010666666666665,
      "grad_norm": 1.0893789529800415,
      "learning_rate": 3.124333333333333e-05,
      "loss": 0.003,
      "step": 56270
    },
    {
      "epoch": 3.0016,
      "grad_norm": 0.2262103408575058,
      "learning_rate": 3.1240000000000006e-05,
      "loss": 0.0032,
      "step": 56280
    },
    {
      "epoch": 3.0021333333333335,
      "grad_norm": 0.339316725730896,
      "learning_rate": 3.123666666666667e-05,
      "loss": 0.0028,
      "step": 56290
    },
    {
      "epoch": 3.002666666666667,
      "grad_norm": 0.3393224775791168,
      "learning_rate": 3.123333333333334e-05,
      "loss": 0.0023,
      "step": 56300
    },
    {
      "epoch": 3.0032,
      "grad_norm": 0.508935272693634,
      "learning_rate": 3.1230000000000004e-05,
      "loss": 0.0028,
      "step": 56310
    },
    {
      "epoch": 3.0037333333333334,
      "grad_norm": 0.22620069980621338,
      "learning_rate": 3.122666666666667e-05,
      "loss": 0.0034,
      "step": 56320
    },
    {
      "epoch": 3.0042666666666666,
      "grad_norm": 0.3675951361656189,
      "learning_rate": 3.122333333333333e-05,
      "loss": 0.0026,
      "step": 56330
    },
    {
      "epoch": 3.0048,
      "grad_norm": 0.19792720675468445,
      "learning_rate": 3.122e-05,
      "loss": 0.0033,
      "step": 56340
    },
    {
      "epoch": 3.005333333333333,
      "grad_norm": 0.14138883352279663,
      "learning_rate": 3.121666666666667e-05,
      "loss": 0.0026,
      "step": 56350
    },
    {
      "epoch": 3.0058666666666665,
      "grad_norm": 0.1413784772157669,
      "learning_rate": 3.1213333333333335e-05,
      "loss": 0.0023,
      "step": 56360
    },
    {
      "epoch": 3.0064,
      "grad_norm": 0.11309988051652908,
      "learning_rate": 3.121e-05,
      "loss": 0.0023,
      "step": 56370
    },
    {
      "epoch": 3.0069333333333335,
      "grad_norm": 0.05655224621295929,
      "learning_rate": 3.120666666666667e-05,
      "loss": 0.0023,
      "step": 56380
    },
    {
      "epoch": 3.0074666666666667,
      "grad_norm": 0.05655159428715706,
      "learning_rate": 3.120333333333333e-05,
      "loss": 0.0037,
      "step": 56390
    },
    {
      "epoch": 3.008,
      "grad_norm": 3.6602851860578767e-09,
      "learning_rate": 3.12e-05,
      "loss": 0.0025,
      "step": 56400
    },
    {
      "epoch": 3.0085333333333333,
      "grad_norm": 0.08482810109853745,
      "learning_rate": 3.1196666666666666e-05,
      "loss": 0.0036,
      "step": 56410
    },
    {
      "epoch": 3.0090666666666666,
      "grad_norm": 0.16965335607528687,
      "learning_rate": 3.119333333333334e-05,
      "loss": 0.0024,
      "step": 56420
    },
    {
      "epoch": 3.0096,
      "grad_norm": 0.17069238424301147,
      "learning_rate": 3.1190000000000005e-05,
      "loss": 0.0038,
      "step": 56430
    },
    {
      "epoch": 3.0101333333333335,
      "grad_norm": 0.1413830667734146,
      "learning_rate": 3.118666666666667e-05,
      "loss": 0.0025,
      "step": 56440
    },
    {
      "epoch": 3.010666666666667,
      "grad_norm": 0.16964134573936462,
      "learning_rate": 3.118333333333334e-05,
      "loss": 0.0048,
      "step": 56450
    },
    {
      "epoch": 3.0112,
      "grad_norm": 0.19792135059833527,
      "learning_rate": 3.118e-05,
      "loss": 0.0027,
      "step": 56460
    },
    {
      "epoch": 3.0117333333333334,
      "grad_norm": 0.14137880504131317,
      "learning_rate": 3.117666666666667e-05,
      "loss": 0.0031,
      "step": 56470
    },
    {
      "epoch": 3.0122666666666666,
      "grad_norm": 3.2641647162989784e-09,
      "learning_rate": 3.1173333333333335e-05,
      "loss": 0.0032,
      "step": 56480
    },
    {
      "epoch": 3.0128,
      "grad_norm": 0.08482766151428223,
      "learning_rate": 3.117e-05,
      "loss": 0.0024,
      "step": 56490
    },
    {
      "epoch": 3.013333333333333,
      "grad_norm": 0.08482442796230316,
      "learning_rate": 3.116666666666667e-05,
      "loss": 0.004,
      "step": 56500
    },
    {
      "epoch": 3.0138666666666665,
      "grad_norm": 0.33930233120918274,
      "learning_rate": 3.1163333333333334e-05,
      "loss": 0.0025,
      "step": 56510
    },
    {
      "epoch": 3.0144,
      "grad_norm": 0.6221014857292175,
      "learning_rate": 3.116e-05,
      "loss": 0.0035,
      "step": 56520
    },
    {
      "epoch": 3.0149333333333335,
      "grad_norm": 0.19792130589485168,
      "learning_rate": 3.1156666666666666e-05,
      "loss": 0.0019,
      "step": 56530
    },
    {
      "epoch": 3.0154666666666667,
      "grad_norm": 0.11309564113616943,
      "learning_rate": 3.115333333333333e-05,
      "loss": 0.0016,
      "step": 56540
    },
    {
      "epoch": 3.016,
      "grad_norm": 0.22620542347431183,
      "learning_rate": 3.115e-05,
      "loss": 0.0026,
      "step": 56550
    },
    {
      "epoch": 3.0165333333333333,
      "grad_norm": 0.45235297083854675,
      "learning_rate": 3.114666666666667e-05,
      "loss": 0.0033,
      "step": 56560
    },
    {
      "epoch": 3.0170666666666666,
      "grad_norm": 0.1413784921169281,
      "learning_rate": 3.114333333333334e-05,
      "loss": 0.0026,
      "step": 56570
    },
    {
      "epoch": 3.0176,
      "grad_norm": 0.3110208809375763,
      "learning_rate": 3.1140000000000003e-05,
      "loss": 0.0035,
      "step": 56580
    },
    {
      "epoch": 3.018133333333333,
      "grad_norm": 0.02827349677681923,
      "learning_rate": 3.113666666666667e-05,
      "loss": 0.0018,
      "step": 56590
    },
    {
      "epoch": 3.018666666666667,
      "grad_norm": 0.11309940367937088,
      "learning_rate": 3.1133333333333336e-05,
      "loss": 0.0028,
      "step": 56600
    },
    {
      "epoch": 3.0192,
      "grad_norm": 0.3110058605670929,
      "learning_rate": 3.113e-05,
      "loss": 0.0034,
      "step": 56610
    },
    {
      "epoch": 3.0197333333333334,
      "grad_norm": 0.31102821230888367,
      "learning_rate": 3.112666666666667e-05,
      "loss": 0.0018,
      "step": 56620
    },
    {
      "epoch": 3.0202666666666667,
      "grad_norm": 0.28273123502731323,
      "learning_rate": 3.1123333333333334e-05,
      "loss": 0.0037,
      "step": 56630
    },
    {
      "epoch": 3.0208,
      "grad_norm": 0.16965393722057343,
      "learning_rate": 3.112e-05,
      "loss": 0.0032,
      "step": 56640
    },
    {
      "epoch": 3.021333333333333,
      "grad_norm": 0.11309085041284561,
      "learning_rate": 3.1116666666666666e-05,
      "loss": 0.0044,
      "step": 56650
    },
    {
      "epoch": 3.0218666666666665,
      "grad_norm": 0.028275184333324432,
      "learning_rate": 3.111333333333333e-05,
      "loss": 0.0036,
      "step": 56660
    },
    {
      "epoch": 3.0224,
      "grad_norm": 0.14136233925819397,
      "learning_rate": 3.111e-05,
      "loss": 0.0043,
      "step": 56670
    },
    {
      "epoch": 3.0229333333333335,
      "grad_norm": 0.028275245800614357,
      "learning_rate": 3.1106666666666665e-05,
      "loss": 0.0038,
      "step": 56680
    },
    {
      "epoch": 3.0234666666666667,
      "grad_norm": 0.05654941499233246,
      "learning_rate": 3.110333333333334e-05,
      "loss": 0.0048,
      "step": 56690
    },
    {
      "epoch": 3.024,
      "grad_norm": 3.4177469743212896e-09,
      "learning_rate": 3.1100000000000004e-05,
      "loss": 0.003,
      "step": 56700
    },
    {
      "epoch": 3.0245333333333333,
      "grad_norm": 8.854264543778356e-10,
      "learning_rate": 3.109666666666667e-05,
      "loss": 0.0029,
      "step": 56710
    },
    {
      "epoch": 3.0250666666666666,
      "grad_norm": 0.028272762894630432,
      "learning_rate": 3.1093333333333336e-05,
      "loss": 0.0052,
      "step": 56720
    },
    {
      "epoch": 3.0256,
      "grad_norm": 0.056547265499830246,
      "learning_rate": 3.109e-05,
      "loss": 0.0019,
      "step": 56730
    },
    {
      "epoch": 3.026133333333333,
      "grad_norm": 0.028274640440940857,
      "learning_rate": 3.108666666666667e-05,
      "loss": 0.0037,
      "step": 56740
    },
    {
      "epoch": 3.026666666666667,
      "grad_norm": 0.05654546618461609,
      "learning_rate": 3.1083333333333334e-05,
      "loss": 0.0027,
      "step": 56750
    },
    {
      "epoch": 3.0272,
      "grad_norm": 0.2261776477098465,
      "learning_rate": 3.108e-05,
      "loss": 0.0047,
      "step": 56760
    },
    {
      "epoch": 3.0277333333333334,
      "grad_norm": 0.11309604346752167,
      "learning_rate": 3.1076666666666673e-05,
      "loss": 0.0032,
      "step": 56770
    },
    {
      "epoch": 3.0282666666666667,
      "grad_norm": 0.11309260129928589,
      "learning_rate": 3.107333333333333e-05,
      "loss": 0.0031,
      "step": 56780
    },
    {
      "epoch": 3.0288,
      "grad_norm": 0.197922483086586,
      "learning_rate": 3.107e-05,
      "loss": 0.003,
      "step": 56790
    },
    {
      "epoch": 3.029333333333333,
      "grad_norm": 0.33926576375961304,
      "learning_rate": 3.1066666666666665e-05,
      "loss": 0.003,
      "step": 56800
    },
    {
      "epoch": 3.0298666666666665,
      "grad_norm": 0.14136594533920288,
      "learning_rate": 3.106333333333333e-05,
      "loss": 0.0052,
      "step": 56810
    },
    {
      "epoch": 3.0304,
      "grad_norm": 3.0882163493117787e-09,
      "learning_rate": 3.106e-05,
      "loss": 0.003,
      "step": 56820
    },
    {
      "epoch": 3.0309333333333335,
      "grad_norm": 0.05654432997107506,
      "learning_rate": 3.105666666666667e-05,
      "loss": 0.0023,
      "step": 56830
    },
    {
      "epoch": 3.0314666666666668,
      "grad_norm": 0.05654872581362724,
      "learning_rate": 3.1053333333333336e-05,
      "loss": 0.0044,
      "step": 56840
    },
    {
      "epoch": 3.032,
      "grad_norm": 1.930509974101824e-09,
      "learning_rate": 3.105e-05,
      "loss": 0.0032,
      "step": 56850
    },
    {
      "epoch": 3.0325333333333333,
      "grad_norm": 0.02827381156384945,
      "learning_rate": 3.104666666666667e-05,
      "loss": 0.0034,
      "step": 56860
    },
    {
      "epoch": 3.0330666666666666,
      "grad_norm": 0.0848195031285286,
      "learning_rate": 3.1043333333333335e-05,
      "loss": 0.0037,
      "step": 56870
    },
    {
      "epoch": 3.0336,
      "grad_norm": 4.031097955436991e-10,
      "learning_rate": 3.104e-05,
      "loss": 0.0027,
      "step": 56880
    },
    {
      "epoch": 3.034133333333333,
      "grad_norm": 0.36756113171577454,
      "learning_rate": 3.103666666666667e-05,
      "loss": 0.0039,
      "step": 56890
    },
    {
      "epoch": 3.034666666666667,
      "grad_norm": 0.5371448397636414,
      "learning_rate": 3.103333333333333e-05,
      "loss": 0.0041,
      "step": 56900
    },
    {
      "epoch": 3.0352,
      "grad_norm": 0.1413651704788208,
      "learning_rate": 3.1030000000000006e-05,
      "loss": 0.0034,
      "step": 56910
    },
    {
      "epoch": 3.0357333333333334,
      "grad_norm": 0.0282739344984293,
      "learning_rate": 3.102666666666667e-05,
      "loss": 0.0026,
      "step": 56920
    },
    {
      "epoch": 3.0362666666666667,
      "grad_norm": 0.36754316091537476,
      "learning_rate": 3.102333333333334e-05,
      "loss": 0.0023,
      "step": 56930
    },
    {
      "epoch": 3.0368,
      "grad_norm": 0.1421150267124176,
      "learning_rate": 3.102e-05,
      "loss": 0.0031,
      "step": 56940
    },
    {
      "epoch": 3.037333333333333,
      "grad_norm": 0.4806269705295563,
      "learning_rate": 3.1016666666666664e-05,
      "loss": 0.0034,
      "step": 56950
    },
    {
      "epoch": 3.0378666666666665,
      "grad_norm": 0.2544553279876709,
      "learning_rate": 3.101333333333333e-05,
      "loss": 0.0031,
      "step": 56960
    },
    {
      "epoch": 3.0384,
      "grad_norm": 0.22616557776927948,
      "learning_rate": 3.101e-05,
      "loss": 0.0028,
      "step": 56970
    },
    {
      "epoch": 3.0389333333333335,
      "grad_norm": 0.310991108417511,
      "learning_rate": 3.100666666666667e-05,
      "loss": 0.003,
      "step": 56980
    },
    {
      "epoch": 3.0394666666666668,
      "grad_norm": 0.11308992654085159,
      "learning_rate": 3.1003333333333335e-05,
      "loss": 0.0043,
      "step": 56990
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.16962862014770508,
      "learning_rate": 3.1e-05,
      "loss": 0.0028,
      "step": 57000
    },
    {
      "epoch": 3.0405333333333333,
      "grad_norm": 0.3109753727912903,
      "learning_rate": 3.099666666666667e-05,
      "loss": 0.0036,
      "step": 57010
    },
    {
      "epoch": 3.0410666666666666,
      "grad_norm": 0.08481282740831375,
      "learning_rate": 3.0993333333333334e-05,
      "loss": 0.0031,
      "step": 57020
    },
    {
      "epoch": 3.0416,
      "grad_norm": 0.056544508785009384,
      "learning_rate": 3.099e-05,
      "loss": 0.0033,
      "step": 57030
    },
    {
      "epoch": 3.042133333333333,
      "grad_norm": 3.5223408634266207e-09,
      "learning_rate": 3.098666666666667e-05,
      "loss": 0.0025,
      "step": 57040
    },
    {
      "epoch": 3.042666666666667,
      "grad_norm": 0.3958170413970947,
      "learning_rate": 3.098333333333334e-05,
      "loss": 0.0034,
      "step": 57050
    },
    {
      "epoch": 3.0432,
      "grad_norm": 0.1413637399673462,
      "learning_rate": 3.0980000000000005e-05,
      "loss": 0.0028,
      "step": 57060
    },
    {
      "epoch": 3.0437333333333334,
      "grad_norm": 0.16962817311286926,
      "learning_rate": 3.097666666666667e-05,
      "loss": 0.0032,
      "step": 57070
    },
    {
      "epoch": 3.0442666666666667,
      "grad_norm": 0.19790293276309967,
      "learning_rate": 3.097333333333334e-05,
      "loss": 0.0035,
      "step": 57080
    },
    {
      "epoch": 3.0448,
      "grad_norm": 0.1130891889333725,
      "learning_rate": 3.0969999999999997e-05,
      "loss": 0.0022,
      "step": 57090
    },
    {
      "epoch": 3.0453333333333332,
      "grad_norm": 0.25443172454833984,
      "learning_rate": 3.096666666666666e-05,
      "loss": 0.0031,
      "step": 57100
    },
    {
      "epoch": 3.0458666666666665,
      "grad_norm": 0.3109871745109558,
      "learning_rate": 3.0963333333333336e-05,
      "loss": 0.0028,
      "step": 57110
    },
    {
      "epoch": 3.0464,
      "grad_norm": 0.08481288701295853,
      "learning_rate": 3.096e-05,
      "loss": 0.0019,
      "step": 57120
    },
    {
      "epoch": 3.0469333333333335,
      "grad_norm": 0.02831302583217621,
      "learning_rate": 3.095666666666667e-05,
      "loss": 0.0039,
      "step": 57130
    },
    {
      "epoch": 3.0474666666666668,
      "grad_norm": 0.1981888711452484,
      "learning_rate": 3.0953333333333334e-05,
      "loss": 0.0023,
      "step": 57140
    },
    {
      "epoch": 3.048,
      "grad_norm": 0.2831485867500305,
      "learning_rate": 3.095e-05,
      "loss": 0.0032,
      "step": 57150
    },
    {
      "epoch": 3.0485333333333333,
      "grad_norm": 0.3680393695831299,
      "learning_rate": 3.0946666666666666e-05,
      "loss": 0.0033,
      "step": 57160
    },
    {
      "epoch": 3.0490666666666666,
      "grad_norm": 0.42469632625579834,
      "learning_rate": 3.094333333333333e-05,
      "loss": 0.0038,
      "step": 57170
    },
    {
      "epoch": 3.0496,
      "grad_norm": 0.028311021625995636,
      "learning_rate": 3.0940000000000005e-05,
      "loss": 0.0029,
      "step": 57180
    },
    {
      "epoch": 3.050133333333333,
      "grad_norm": 0.11324236541986465,
      "learning_rate": 3.093666666666667e-05,
      "loss": 0.0026,
      "step": 57190
    },
    {
      "epoch": 3.050666666666667,
      "grad_norm": 0.028310826048254967,
      "learning_rate": 3.093333333333334e-05,
      "loss": 0.0029,
      "step": 57200
    },
    {
      "epoch": 3.0512,
      "grad_norm": 0.1415596604347229,
      "learning_rate": 3.0930000000000004e-05,
      "loss": 0.0038,
      "step": 57210
    },
    {
      "epoch": 3.0517333333333334,
      "grad_norm": 0.14561721682548523,
      "learning_rate": 3.092666666666667e-05,
      "loss": 0.0029,
      "step": 57220
    },
    {
      "epoch": 3.0522666666666667,
      "grad_norm": 0.42465993762016296,
      "learning_rate": 3.0923333333333336e-05,
      "loss": 0.0027,
      "step": 57230
    },
    {
      "epoch": 3.0528,
      "grad_norm": 0.08492868393659592,
      "learning_rate": 3.092e-05,
      "loss": 0.0037,
      "step": 57240
    },
    {
      "epoch": 3.0533333333333332,
      "grad_norm": 0.05661827325820923,
      "learning_rate": 3.091666666666667e-05,
      "loss": 0.005,
      "step": 57250
    },
    {
      "epoch": 3.0538666666666665,
      "grad_norm": 0.19815759360790253,
      "learning_rate": 3.0913333333333334e-05,
      "loss": 0.002,
      "step": 57260
    },
    {
      "epoch": 3.0544,
      "grad_norm": 0.16985926032066345,
      "learning_rate": 3.091e-05,
      "loss": 0.0026,
      "step": 57270
    },
    {
      "epoch": 3.0549333333333335,
      "grad_norm": 0.22646452486515045,
      "learning_rate": 3.090666666666667e-05,
      "loss": 0.0023,
      "step": 57280
    },
    {
      "epoch": 3.0554666666666668,
      "grad_norm": 0.028309188783168793,
      "learning_rate": 3.090333333333333e-05,
      "loss": 0.0024,
      "step": 57290
    },
    {
      "epoch": 3.056,
      "grad_norm": 0.0849241092801094,
      "learning_rate": 3.09e-05,
      "loss": 0.0031,
      "step": 57300
    },
    {
      "epoch": 3.0565333333333333,
      "grad_norm": 0.1415480524301529,
      "learning_rate": 3.0896666666666665e-05,
      "loss": 0.0029,
      "step": 57310
    },
    {
      "epoch": 3.0570666666666666,
      "grad_norm": 0.339701384305954,
      "learning_rate": 3.089333333333334e-05,
      "loss": 0.0022,
      "step": 57320
    },
    {
      "epoch": 3.0576,
      "grad_norm": 0.19815921783447266,
      "learning_rate": 3.0890000000000004e-05,
      "loss": 0.0028,
      "step": 57330
    },
    {
      "epoch": 3.058133333333333,
      "grad_norm": 0.3113904595375061,
      "learning_rate": 3.088666666666667e-05,
      "loss": 0.0039,
      "step": 57340
    },
    {
      "epoch": 3.058666666666667,
      "grad_norm": 0.2264646589756012,
      "learning_rate": 3.0883333333333336e-05,
      "loss": 0.0033,
      "step": 57350
    },
    {
      "epoch": 3.0592,
      "grad_norm": 0.22647088766098022,
      "learning_rate": 3.088e-05,
      "loss": 0.0029,
      "step": 57360
    },
    {
      "epoch": 3.0597333333333334,
      "grad_norm": 0.1698462814092636,
      "learning_rate": 3.087666666666667e-05,
      "loss": 0.0042,
      "step": 57370
    },
    {
      "epoch": 3.0602666666666667,
      "grad_norm": 0.283079594373703,
      "learning_rate": 3.0873333333333335e-05,
      "loss": 0.0026,
      "step": 57380
    },
    {
      "epoch": 3.0608,
      "grad_norm": 0.25476977229118347,
      "learning_rate": 3.087e-05,
      "loss": 0.0034,
      "step": 57390
    },
    {
      "epoch": 3.0613333333333332,
      "grad_norm": 0.3113974332809448,
      "learning_rate": 3.086666666666667e-05,
      "loss": 0.0041,
      "step": 57400
    },
    {
      "epoch": 3.0618666666666665,
      "grad_norm": 0.11323419213294983,
      "learning_rate": 3.086333333333333e-05,
      "loss": 0.0025,
      "step": 57410
    },
    {
      "epoch": 3.0624,
      "grad_norm": 0.028308339416980743,
      "learning_rate": 3.086e-05,
      "loss": 0.0022,
      "step": 57420
    },
    {
      "epoch": 3.0629333333333335,
      "grad_norm": 0.11323024332523346,
      "learning_rate": 3.0856666666666665e-05,
      "loss": 0.0035,
      "step": 57430
    },
    {
      "epoch": 3.063466666666667,
      "grad_norm": 0.3963428735733032,
      "learning_rate": 3.085333333333333e-05,
      "loss": 0.0027,
      "step": 57440
    },
    {
      "epoch": 3.064,
      "grad_norm": 0.396366149187088,
      "learning_rate": 3.0850000000000004e-05,
      "loss": 0.0037,
      "step": 57450
    },
    {
      "epoch": 3.0645333333333333,
      "grad_norm": 0.11323051899671555,
      "learning_rate": 3.084666666666667e-05,
      "loss": 0.0037,
      "step": 57460
    },
    {
      "epoch": 3.0650666666666666,
      "grad_norm": 0.08492709696292877,
      "learning_rate": 3.084333333333334e-05,
      "loss": 0.0032,
      "step": 57470
    },
    {
      "epoch": 3.0656,
      "grad_norm": 0.11340481787919998,
      "learning_rate": 3.084e-05,
      "loss": 0.0028,
      "step": 57480
    },
    {
      "epoch": 3.066133333333333,
      "grad_norm": 2.5255519897626755e-09,
      "learning_rate": 3.083666666666667e-05,
      "loss": 0.0022,
      "step": 57490
    },
    {
      "epoch": 3.066666666666667,
      "grad_norm": 0.05661081150174141,
      "learning_rate": 3.0833333333333335e-05,
      "loss": 0.0024,
      "step": 57500
    },
    {
      "epoch": 3.0672,
      "grad_norm": 0.1415240317583084,
      "learning_rate": 3.083e-05,
      "loss": 0.0024,
      "step": 57510
    },
    {
      "epoch": 3.0677333333333334,
      "grad_norm": 0.16982489824295044,
      "learning_rate": 3.082666666666667e-05,
      "loss": 0.0022,
      "step": 57520
    },
    {
      "epoch": 3.0682666666666667,
      "grad_norm": 0.02830474264919758,
      "learning_rate": 3.082333333333334e-05,
      "loss": 0.0031,
      "step": 57530
    },
    {
      "epoch": 3.0688,
      "grad_norm": 0.19813482463359833,
      "learning_rate": 3.082e-05,
      "loss": 0.0024,
      "step": 57540
    },
    {
      "epoch": 3.0693333333333332,
      "grad_norm": 0.19813741743564606,
      "learning_rate": 3.0816666666666666e-05,
      "loss": 0.0036,
      "step": 57550
    },
    {
      "epoch": 3.0698666666666665,
      "grad_norm": 0.05661122128367424,
      "learning_rate": 3.081333333333333e-05,
      "loss": 0.0027,
      "step": 57560
    },
    {
      "epoch": 3.0704,
      "grad_norm": 0.2830374538898468,
      "learning_rate": 3.081e-05,
      "loss": 0.0033,
      "step": 57570
    },
    {
      "epoch": 3.0709333333333335,
      "grad_norm": 0.02830534614622593,
      "learning_rate": 3.0806666666666664e-05,
      "loss": 0.0033,
      "step": 57580
    },
    {
      "epoch": 3.071466666666667,
      "grad_norm": 0.1698242723941803,
      "learning_rate": 3.080333333333334e-05,
      "loss": 0.0033,
      "step": 57590
    },
    {
      "epoch": 3.072,
      "grad_norm": 0.11322610080242157,
      "learning_rate": 3.08e-05,
      "loss": 0.0036,
      "step": 57600
    },
    {
      "epoch": 3.0725333333333333,
      "grad_norm": 0.11321833729743958,
      "learning_rate": 3.079666666666667e-05,
      "loss": 0.0027,
      "step": 57610
    },
    {
      "epoch": 3.0730666666666666,
      "grad_norm": 0.05661031976342201,
      "learning_rate": 3.0793333333333336e-05,
      "loss": 0.0038,
      "step": 57620
    },
    {
      "epoch": 3.0736,
      "grad_norm": 3.1438782688297806e-09,
      "learning_rate": 3.079e-05,
      "loss": 0.0024,
      "step": 57630
    },
    {
      "epoch": 3.074133333333333,
      "grad_norm": 0.2264329493045807,
      "learning_rate": 3.078666666666667e-05,
      "loss": 0.0029,
      "step": 57640
    },
    {
      "epoch": 3.074666666666667,
      "grad_norm": 0.1698271930217743,
      "learning_rate": 3.0783333333333334e-05,
      "loss": 0.0027,
      "step": 57650
    },
    {
      "epoch": 3.0752,
      "grad_norm": 0.14152194559574127,
      "learning_rate": 3.078e-05,
      "loss": 0.0039,
      "step": 57660
    },
    {
      "epoch": 3.0757333333333334,
      "grad_norm": 0.11322615295648575,
      "learning_rate": 3.077666666666667e-05,
      "loss": 0.0024,
      "step": 57670
    },
    {
      "epoch": 3.0762666666666667,
      "grad_norm": 0.0849113017320633,
      "learning_rate": 3.077333333333334e-05,
      "loss": 0.0024,
      "step": 57680
    },
    {
      "epoch": 3.0768,
      "grad_norm": 0.25475603342056274,
      "learning_rate": 3.077e-05,
      "loss": 0.0036,
      "step": 57690
    },
    {
      "epoch": 3.0773333333333333,
      "grad_norm": 0.22643080353736877,
      "learning_rate": 3.0766666666666665e-05,
      "loss": 0.0021,
      "step": 57700
    },
    {
      "epoch": 3.0778666666666665,
      "grad_norm": 0.22644218802452087,
      "learning_rate": 3.076333333333333e-05,
      "loss": 0.0031,
      "step": 57710
    },
    {
      "epoch": 3.0784,
      "grad_norm": 0.1698279231786728,
      "learning_rate": 3.076e-05,
      "loss": 0.003,
      "step": 57720
    },
    {
      "epoch": 3.0789333333333335,
      "grad_norm": 0.19812552630901337,
      "learning_rate": 3.075666666666667e-05,
      "loss": 0.0036,
      "step": 57730
    },
    {
      "epoch": 3.079466666666667,
      "grad_norm": 0.11321377754211426,
      "learning_rate": 3.0753333333333336e-05,
      "loss": 0.0034,
      "step": 57740
    },
    {
      "epoch": 3.08,
      "grad_norm": 0.31135430932044983,
      "learning_rate": 3.075e-05,
      "loss": 0.003,
      "step": 57750
    },
    {
      "epoch": 3.0805333333333333,
      "grad_norm": 0.2830340266227722,
      "learning_rate": 3.074666666666667e-05,
      "loss": 0.0029,
      "step": 57760
    },
    {
      "epoch": 3.0810666666666666,
      "grad_norm": 0.283034086227417,
      "learning_rate": 3.0743333333333334e-05,
      "loss": 0.0038,
      "step": 57770
    },
    {
      "epoch": 3.0816,
      "grad_norm": 0.16982394456863403,
      "learning_rate": 3.074e-05,
      "loss": 0.003,
      "step": 57780
    },
    {
      "epoch": 3.082133333333333,
      "grad_norm": 0.311330646276474,
      "learning_rate": 3.0736666666666667e-05,
      "loss": 0.0035,
      "step": 57790
    },
    {
      "epoch": 3.0826666666666664,
      "grad_norm": 0.22642455995082855,
      "learning_rate": 3.073333333333334e-05,
      "loss": 0.0035,
      "step": 57800
    },
    {
      "epoch": 3.0832,
      "grad_norm": 0.2830337584018707,
      "learning_rate": 3.0730000000000006e-05,
      "loss": 0.0033,
      "step": 57810
    },
    {
      "epoch": 3.0837333333333334,
      "grad_norm": 0.22643864154815674,
      "learning_rate": 3.072666666666667e-05,
      "loss": 0.0048,
      "step": 57820
    },
    {
      "epoch": 3.0842666666666667,
      "grad_norm": 0.22641876339912415,
      "learning_rate": 3.072333333333334e-05,
      "loss": 0.0035,
      "step": 57830
    },
    {
      "epoch": 3.0848,
      "grad_norm": 0.28305763006210327,
      "learning_rate": 3.072e-05,
      "loss": 0.0041,
      "step": 57840
    },
    {
      "epoch": 3.0853333333333333,
      "grad_norm": 0.5660352110862732,
      "learning_rate": 3.0716666666666663e-05,
      "loss": 0.0046,
      "step": 57850
    },
    {
      "epoch": 3.0858666666666665,
      "grad_norm": 0.0849166214466095,
      "learning_rate": 3.071333333333333e-05,
      "loss": 0.0037,
      "step": 57860
    },
    {
      "epoch": 3.0864,
      "grad_norm": 0.16981850564479828,
      "learning_rate": 3.071e-05,
      "loss": 0.0028,
      "step": 57870
    },
    {
      "epoch": 3.0869333333333335,
      "grad_norm": 0.22642673552036285,
      "learning_rate": 3.070666666666667e-05,
      "loss": 0.0035,
      "step": 57880
    },
    {
      "epoch": 3.087466666666667,
      "grad_norm": 0.36794590950012207,
      "learning_rate": 3.0703333333333335e-05,
      "loss": 0.0028,
      "step": 57890
    },
    {
      "epoch": 3.088,
      "grad_norm": 0.1415146440267563,
      "learning_rate": 3.07e-05,
      "loss": 0.0024,
      "step": 57900
    },
    {
      "epoch": 3.0885333333333334,
      "grad_norm": 0.05660365894436836,
      "learning_rate": 3.069666666666667e-05,
      "loss": 0.0032,
      "step": 57910
    },
    {
      "epoch": 3.0890666666666666,
      "grad_norm": 0.16982319951057434,
      "learning_rate": 3.069333333333333e-05,
      "loss": 0.0026,
      "step": 57920
    },
    {
      "epoch": 3.0896,
      "grad_norm": 0.016550106927752495,
      "learning_rate": 3.069e-05,
      "loss": 0.0054,
      "step": 57930
    },
    {
      "epoch": 3.090133333333333,
      "grad_norm": 0.14150969684123993,
      "learning_rate": 3.068666666666667e-05,
      "loss": 0.0068,
      "step": 57940
    },
    {
      "epoch": 3.0906666666666665,
      "grad_norm": 0.028303341940045357,
      "learning_rate": 3.068333333333334e-05,
      "loss": 0.0018,
      "step": 57950
    },
    {
      "epoch": 3.0912,
      "grad_norm": 0.28301429748535156,
      "learning_rate": 3.0680000000000004e-05,
      "loss": 0.0025,
      "step": 57960
    },
    {
      "epoch": 3.0917333333333334,
      "grad_norm": 0.2264290601015091,
      "learning_rate": 3.067666666666667e-05,
      "loss": 0.0019,
      "step": 57970
    },
    {
      "epoch": 3.0922666666666667,
      "grad_norm": 0.08490459620952606,
      "learning_rate": 3.067333333333334e-05,
      "loss": 0.0026,
      "step": 57980
    },
    {
      "epoch": 3.0928,
      "grad_norm": 0.056608058512210846,
      "learning_rate": 3.0669999999999996e-05,
      "loss": 0.0028,
      "step": 57990
    },
    {
      "epoch": 3.0933333333333333,
      "grad_norm": 1.5330627434195776e-08,
      "learning_rate": 3.066666666666667e-05,
      "loss": 0.0033,
      "step": 58000
    },
    {
      "epoch": 3.0938666666666665,
      "grad_norm": 0.1135622188448906,
      "learning_rate": 3.0663333333333335e-05,
      "loss": 0.003,
      "step": 58010
    },
    {
      "epoch": 3.0944,
      "grad_norm": 6.037359714508057,
      "learning_rate": 3.066e-05,
      "loss": 0.0066,
      "step": 58020
    },
    {
      "epoch": 3.0949333333333335,
      "grad_norm": 0.08490350842475891,
      "learning_rate": 3.065666666666667e-05,
      "loss": 0.0027,
      "step": 58030
    },
    {
      "epoch": 3.095466666666667,
      "grad_norm": 0.11320330202579498,
      "learning_rate": 3.0653333333333333e-05,
      "loss": 0.0034,
      "step": 58040
    },
    {
      "epoch": 3.096,
      "grad_norm": 0.11321136355400085,
      "learning_rate": 3.065e-05,
      "loss": 0.0039,
      "step": 58050
    },
    {
      "epoch": 3.0965333333333334,
      "grad_norm": 0.056603021919727325,
      "learning_rate": 3.0646666666666666e-05,
      "loss": 0.004,
      "step": 58060
    },
    {
      "epoch": 3.0970666666666666,
      "grad_norm": 0.22641173005104065,
      "learning_rate": 3.064333333333333e-05,
      "loss": 0.0042,
      "step": 58070
    },
    {
      "epoch": 3.0976,
      "grad_norm": 0.08490347862243652,
      "learning_rate": 3.0640000000000005e-05,
      "loss": 0.0034,
      "step": 58080
    },
    {
      "epoch": 3.098133333333333,
      "grad_norm": 0.028301173821091652,
      "learning_rate": 3.063666666666667e-05,
      "loss": 0.0036,
      "step": 58090
    },
    {
      "epoch": 3.0986666666666665,
      "grad_norm": 0.1132051944732666,
      "learning_rate": 3.063333333333334e-05,
      "loss": 0.0025,
      "step": 58100
    },
    {
      "epoch": 3.0992,
      "grad_norm": 0.22640445828437805,
      "learning_rate": 3.063e-05,
      "loss": 0.0051,
      "step": 58110
    },
    {
      "epoch": 3.0997333333333335,
      "grad_norm": 0.2547026574611664,
      "learning_rate": 3.062666666666667e-05,
      "loss": 0.0034,
      "step": 58120
    },
    {
      "epoch": 3.1002666666666667,
      "grad_norm": 1.6133446800736806e-09,
      "learning_rate": 3.0623333333333335e-05,
      "loss": 0.0029,
      "step": 58130
    },
    {
      "epoch": 3.1008,
      "grad_norm": 0.4810950756072998,
      "learning_rate": 3.062e-05,
      "loss": 0.0043,
      "step": 58140
    },
    {
      "epoch": 3.1013333333333333,
      "grad_norm": 0.11320778727531433,
      "learning_rate": 3.061666666666667e-05,
      "loss": 0.0044,
      "step": 58150
    },
    {
      "epoch": 3.1018666666666665,
      "grad_norm": 0.056600525975227356,
      "learning_rate": 3.0613333333333334e-05,
      "loss": 0.0029,
      "step": 58160
    },
    {
      "epoch": 3.1024,
      "grad_norm": 0.08490492403507233,
      "learning_rate": 3.061e-05,
      "loss": 0.0024,
      "step": 58170
    },
    {
      "epoch": 3.1029333333333335,
      "grad_norm": 0.08490020781755447,
      "learning_rate": 3.0606666666666666e-05,
      "loss": 0.003,
      "step": 58180
    },
    {
      "epoch": 3.103466666666667,
      "grad_norm": 0.11320455372333527,
      "learning_rate": 3.060333333333333e-05,
      "loss": 0.0027,
      "step": 58190
    },
    {
      "epoch": 3.104,
      "grad_norm": 0.19810716807842255,
      "learning_rate": 3.06e-05,
      "loss": 0.0029,
      "step": 58200
    },
    {
      "epoch": 3.1045333333333334,
      "grad_norm": 2.005689614392736e-09,
      "learning_rate": 3.0596666666666665e-05,
      "loss": 0.0036,
      "step": 58210
    },
    {
      "epoch": 3.1050666666666666,
      "grad_norm": 0.11319971084594727,
      "learning_rate": 3.059333333333334e-05,
      "loss": 0.0032,
      "step": 58220
    },
    {
      "epoch": 3.1056,
      "grad_norm": 0.02829938940703869,
      "learning_rate": 3.0590000000000004e-05,
      "loss": 0.0033,
      "step": 58230
    },
    {
      "epoch": 3.106133333333333,
      "grad_norm": 0.1698070615530014,
      "learning_rate": 3.058666666666667e-05,
      "loss": 0.002,
      "step": 58240
    },
    {
      "epoch": 3.1066666666666665,
      "grad_norm": 0.028301848098635674,
      "learning_rate": 3.0583333333333336e-05,
      "loss": 0.0043,
      "step": 58250
    },
    {
      "epoch": 3.1072,
      "grad_norm": 0.028300287202000618,
      "learning_rate": 3.058e-05,
      "loss": 0.0029,
      "step": 58260
    },
    {
      "epoch": 3.1077333333333335,
      "grad_norm": 0.19810406863689423,
      "learning_rate": 3.057666666666667e-05,
      "loss": 0.0031,
      "step": 58270
    },
    {
      "epoch": 3.1082666666666667,
      "grad_norm": 0.16979742050170898,
      "learning_rate": 3.0573333333333334e-05,
      "loss": 0.0032,
      "step": 58280
    },
    {
      "epoch": 3.1088,
      "grad_norm": 0.012334232218563557,
      "learning_rate": 3.057000000000001e-05,
      "loss": 0.0033,
      "step": 58290
    },
    {
      "epoch": 3.1093333333333333,
      "grad_norm": 0.11320171505212784,
      "learning_rate": 3.0566666666666667e-05,
      "loss": 0.0039,
      "step": 58300
    },
    {
      "epoch": 3.1098666666666666,
      "grad_norm": 0.31130319833755493,
      "learning_rate": 3.056333333333333e-05,
      "loss": 0.0028,
      "step": 58310
    },
    {
      "epoch": 3.1104,
      "grad_norm": 0.05660054087638855,
      "learning_rate": 3.056e-05,
      "loss": 0.0035,
      "step": 58320
    },
    {
      "epoch": 3.1109333333333336,
      "grad_norm": 0.3112960755825043,
      "learning_rate": 3.0556666666666665e-05,
      "loss": 0.0031,
      "step": 58330
    },
    {
      "epoch": 3.111466666666667,
      "grad_norm": 0.19811169803142548,
      "learning_rate": 3.055333333333333e-05,
      "loss": 0.0031,
      "step": 58340
    },
    {
      "epoch": 3.112,
      "grad_norm": 0.22639583051204681,
      "learning_rate": 3.0550000000000004e-05,
      "loss": 0.0045,
      "step": 58350
    },
    {
      "epoch": 3.1125333333333334,
      "grad_norm": 0.11319886147975922,
      "learning_rate": 3.054666666666667e-05,
      "loss": 0.0027,
      "step": 58360
    },
    {
      "epoch": 3.1130666666666666,
      "grad_norm": 0.22639907896518707,
      "learning_rate": 3.0543333333333336e-05,
      "loss": 0.0023,
      "step": 58370
    },
    {
      "epoch": 3.1136,
      "grad_norm": 0.11319494992494583,
      "learning_rate": 3.054e-05,
      "loss": 0.0031,
      "step": 58380
    },
    {
      "epoch": 3.114133333333333,
      "grad_norm": 0.3113095760345459,
      "learning_rate": 3.053666666666667e-05,
      "loss": 0.0027,
      "step": 58390
    },
    {
      "epoch": 3.1146666666666665,
      "grad_norm": 2.890406136657475e-09,
      "learning_rate": 3.0533333333333335e-05,
      "loss": 0.0026,
      "step": 58400
    },
    {
      "epoch": 3.1152,
      "grad_norm": 0.08489842712879181,
      "learning_rate": 3.053e-05,
      "loss": 0.0022,
      "step": 58410
    },
    {
      "epoch": 3.1157333333333335,
      "grad_norm": 0.22638490796089172,
      "learning_rate": 3.052666666666667e-05,
      "loss": 0.0028,
      "step": 58420
    },
    {
      "epoch": 3.1162666666666667,
      "grad_norm": 0.08490081876516342,
      "learning_rate": 3.052333333333334e-05,
      "loss": 0.0042,
      "step": 58430
    },
    {
      "epoch": 3.1168,
      "grad_norm": 2.7426483631134033,
      "learning_rate": 3.0520000000000006e-05,
      "loss": 0.003,
      "step": 58440
    },
    {
      "epoch": 3.1173333333333333,
      "grad_norm": 0.2830010950565338,
      "learning_rate": 3.0516666666666665e-05,
      "loss": 0.003,
      "step": 58450
    },
    {
      "epoch": 3.1178666666666666,
      "grad_norm": 0.16979017853736877,
      "learning_rate": 3.051333333333333e-05,
      "loss": 0.0046,
      "step": 58460
    },
    {
      "epoch": 3.1184,
      "grad_norm": 0.11319523304700851,
      "learning_rate": 3.051e-05,
      "loss": 0.0046,
      "step": 58470
    },
    {
      "epoch": 3.1189333333333336,
      "grad_norm": 0.14150045812129974,
      "learning_rate": 3.0506666666666667e-05,
      "loss": 0.0028,
      "step": 58480
    },
    {
      "epoch": 3.119466666666667,
      "grad_norm": 0.22638870775699615,
      "learning_rate": 3.0503333333333333e-05,
      "loss": 0.0032,
      "step": 58490
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.056598469614982605,
      "learning_rate": 3.05e-05,
      "loss": 0.0031,
      "step": 58500
    },
    {
      "epoch": 3.1205333333333334,
      "grad_norm": 0.2829855680465698,
      "learning_rate": 3.049666666666667e-05,
      "loss": 0.0041,
      "step": 58510
    },
    {
      "epoch": 3.1210666666666667,
      "grad_norm": 0.05659676343202591,
      "learning_rate": 3.0493333333333335e-05,
      "loss": 0.003,
      "step": 58520
    },
    {
      "epoch": 3.1216,
      "grad_norm": 0.16979803144931793,
      "learning_rate": 3.049e-05,
      "loss": 0.0044,
      "step": 58530
    },
    {
      "epoch": 3.122133333333333,
      "grad_norm": 0.22638006508350372,
      "learning_rate": 3.0486666666666667e-05,
      "loss": 0.005,
      "step": 58540
    },
    {
      "epoch": 3.1226666666666665,
      "grad_norm": 0.08489613234996796,
      "learning_rate": 3.0483333333333337e-05,
      "loss": 0.0051,
      "step": 58550
    },
    {
      "epoch": 3.1232,
      "grad_norm": 0.0848923921585083,
      "learning_rate": 3.0480000000000003e-05,
      "loss": 0.003,
      "step": 58560
    },
    {
      "epoch": 3.1237333333333335,
      "grad_norm": 0.11319208890199661,
      "learning_rate": 3.047666666666667e-05,
      "loss": 0.0032,
      "step": 58570
    },
    {
      "epoch": 3.1242666666666667,
      "grad_norm": 0.19809193909168243,
      "learning_rate": 3.047333333333334e-05,
      "loss": 0.003,
      "step": 58580
    },
    {
      "epoch": 3.1248,
      "grad_norm": 0.11319663375616074,
      "learning_rate": 3.0470000000000005e-05,
      "loss": 0.0022,
      "step": 58590
    },
    {
      "epoch": 3.1253333333333333,
      "grad_norm": 0.14148719608783722,
      "learning_rate": 3.0466666666666664e-05,
      "loss": 0.0035,
      "step": 58600
    },
    {
      "epoch": 3.1258666666666666,
      "grad_norm": 0.08489885926246643,
      "learning_rate": 3.0463333333333334e-05,
      "loss": 0.0035,
      "step": 58610
    },
    {
      "epoch": 3.1264,
      "grad_norm": 0.14148299396038055,
      "learning_rate": 3.046e-05,
      "loss": 0.0023,
      "step": 58620
    },
    {
      "epoch": 3.1269333333333336,
      "grad_norm": 0.1980951577425003,
      "learning_rate": 3.0456666666666666e-05,
      "loss": 0.0032,
      "step": 58630
    },
    {
      "epoch": 3.127466666666667,
      "grad_norm": 0.16977792978286743,
      "learning_rate": 3.0453333333333335e-05,
      "loss": 0.0041,
      "step": 58640
    },
    {
      "epoch": 3.128,
      "grad_norm": 0.05659950524568558,
      "learning_rate": 3.045e-05,
      "loss": 0.0026,
      "step": 58650
    },
    {
      "epoch": 3.1285333333333334,
      "grad_norm": 0.19807520508766174,
      "learning_rate": 3.0446666666666668e-05,
      "loss": 0.0035,
      "step": 58660
    },
    {
      "epoch": 3.1290666666666667,
      "grad_norm": 0.19809062778949738,
      "learning_rate": 3.0443333333333334e-05,
      "loss": 0.0023,
      "step": 58670
    },
    {
      "epoch": 3.1296,
      "grad_norm": 0.16977807879447937,
      "learning_rate": 3.0440000000000003e-05,
      "loss": 0.0041,
      "step": 58680
    },
    {
      "epoch": 3.130133333333333,
      "grad_norm": 0.39619624614715576,
      "learning_rate": 3.043666666666667e-05,
      "loss": 0.004,
      "step": 58690
    },
    {
      "epoch": 3.1306666666666665,
      "grad_norm": 0.3678458333015442,
      "learning_rate": 3.0433333333333336e-05,
      "loss": 0.0035,
      "step": 58700
    },
    {
      "epoch": 3.1312,
      "grad_norm": 0.05659374222159386,
      "learning_rate": 3.0430000000000002e-05,
      "loss": 0.0026,
      "step": 58710
    },
    {
      "epoch": 3.1317333333333335,
      "grad_norm": 0.16977854073047638,
      "learning_rate": 3.042666666666667e-05,
      "loss": 0.0036,
      "step": 58720
    },
    {
      "epoch": 3.1322666666666668,
      "grad_norm": 0.4244435131549835,
      "learning_rate": 3.0423333333333337e-05,
      "loss": 0.0051,
      "step": 58730
    },
    {
      "epoch": 3.1328,
      "grad_norm": 0.0282971803098917,
      "learning_rate": 3.0420000000000004e-05,
      "loss": 0.0032,
      "step": 58740
    },
    {
      "epoch": 3.1333333333333333,
      "grad_norm": 0.25466495752334595,
      "learning_rate": 3.0416666666666666e-05,
      "loss": 0.0029,
      "step": 58750
    },
    {
      "epoch": 3.1338666666666666,
      "grad_norm": 0.19808006286621094,
      "learning_rate": 3.0413333333333332e-05,
      "loss": 0.0036,
      "step": 58760
    },
    {
      "epoch": 3.1344,
      "grad_norm": 0.2546541094779968,
      "learning_rate": 3.041e-05,
      "loss": 0.0042,
      "step": 58770
    },
    {
      "epoch": 3.134933333333333,
      "grad_norm": 0.22636628150939941,
      "learning_rate": 3.0406666666666668e-05,
      "loss": 0.0042,
      "step": 58780
    },
    {
      "epoch": 3.135466666666667,
      "grad_norm": 0.0282954890280962,
      "learning_rate": 3.0403333333333334e-05,
      "loss": 0.0033,
      "step": 58790
    },
    {
      "epoch": 3.136,
      "grad_norm": 7.867460460886377e-09,
      "learning_rate": 3.04e-05,
      "loss": 0.003,
      "step": 58800
    },
    {
      "epoch": 3.1365333333333334,
      "grad_norm": 0.33957618474960327,
      "learning_rate": 3.0396666666666666e-05,
      "loss": 0.0023,
      "step": 58810
    },
    {
      "epoch": 3.1370666666666667,
      "grad_norm": 0.11318858712911606,
      "learning_rate": 3.0393333333333336e-05,
      "loss": 0.0037,
      "step": 58820
    },
    {
      "epoch": 3.1376,
      "grad_norm": 0.11318787187337875,
      "learning_rate": 3.0390000000000002e-05,
      "loss": 0.0037,
      "step": 58830
    },
    {
      "epoch": 3.138133333333333,
      "grad_norm": 0.2546774446964264,
      "learning_rate": 3.0386666666666668e-05,
      "loss": 0.0033,
      "step": 58840
    },
    {
      "epoch": 3.1386666666666665,
      "grad_norm": 0.056595250964164734,
      "learning_rate": 3.0383333333333334e-05,
      "loss": 0.0027,
      "step": 58850
    },
    {
      "epoch": 3.1391999999999998,
      "grad_norm": 0.31152448058128357,
      "learning_rate": 3.0380000000000004e-05,
      "loss": 0.003,
      "step": 58860
    },
    {
      "epoch": 3.1397333333333335,
      "grad_norm": 0.02829740010201931,
      "learning_rate": 3.037666666666667e-05,
      "loss": 0.0041,
      "step": 58870
    },
    {
      "epoch": 3.1402666666666668,
      "grad_norm": 0.2829591631889343,
      "learning_rate": 3.0373333333333336e-05,
      "loss": 0.0054,
      "step": 58880
    },
    {
      "epoch": 3.1408,
      "grad_norm": 0.14147815108299255,
      "learning_rate": 3.0370000000000006e-05,
      "loss": 0.0031,
      "step": 58890
    },
    {
      "epoch": 3.1413333333333333,
      "grad_norm": 0.1980658322572708,
      "learning_rate": 3.0366666666666665e-05,
      "loss": 0.0047,
      "step": 58900
    },
    {
      "epoch": 3.1418666666666666,
      "grad_norm": 0.1131889671087265,
      "learning_rate": 3.036333333333333e-05,
      "loss": 0.0028,
      "step": 58910
    },
    {
      "epoch": 3.1424,
      "grad_norm": 0.14148186147212982,
      "learning_rate": 3.036e-05,
      "loss": 0.0034,
      "step": 58920
    },
    {
      "epoch": 3.142933333333333,
      "grad_norm": 0.05659148097038269,
      "learning_rate": 3.0356666666666667e-05,
      "loss": 0.0035,
      "step": 58930
    },
    {
      "epoch": 3.143466666666667,
      "grad_norm": 0.45270979404449463,
      "learning_rate": 3.0353333333333333e-05,
      "loss": 0.0039,
      "step": 58940
    },
    {
      "epoch": 3.144,
      "grad_norm": 0.19807372987270355,
      "learning_rate": 3.035e-05,
      "loss": 0.0036,
      "step": 58950
    },
    {
      "epoch": 3.1445333333333334,
      "grad_norm": 0.1697719544172287,
      "learning_rate": 3.034666666666667e-05,
      "loss": 0.0041,
      "step": 58960
    },
    {
      "epoch": 3.1450666666666667,
      "grad_norm": 0.3961581289768219,
      "learning_rate": 3.0343333333333335e-05,
      "loss": 0.0049,
      "step": 58970
    },
    {
      "epoch": 3.1456,
      "grad_norm": 0.2546449601650238,
      "learning_rate": 3.034e-05,
      "loss": 0.0034,
      "step": 58980
    },
    {
      "epoch": 3.1461333333333332,
      "grad_norm": 0.33955997228622437,
      "learning_rate": 3.033666666666667e-05,
      "loss": 0.0032,
      "step": 58990
    },
    {
      "epoch": 3.1466666666666665,
      "grad_norm": 0.19806517660617828,
      "learning_rate": 3.0333333333333337e-05,
      "loss": 0.0043,
      "step": 59000
    },
    {
      "epoch": 3.1471999999999998,
      "grad_norm": 0.16977404057979584,
      "learning_rate": 3.0330000000000003e-05,
      "loss": 0.0029,
      "step": 59010
    },
    {
      "epoch": 3.1477333333333335,
      "grad_norm": 0.16977199912071228,
      "learning_rate": 3.032666666666667e-05,
      "loss": 0.0031,
      "step": 59020
    },
    {
      "epoch": 3.1482666666666668,
      "grad_norm": 3.414825755498896e-09,
      "learning_rate": 3.032333333333334e-05,
      "loss": 0.0032,
      "step": 59030
    },
    {
      "epoch": 3.1488,
      "grad_norm": 0.5375815629959106,
      "learning_rate": 3.0320000000000004e-05,
      "loss": 0.0038,
      "step": 59040
    },
    {
      "epoch": 3.1493333333333333,
      "grad_norm": 0.1697777807712555,
      "learning_rate": 3.0316666666666664e-05,
      "loss": 0.003,
      "step": 59050
    },
    {
      "epoch": 3.1498666666666666,
      "grad_norm": 0.36781689524650574,
      "learning_rate": 3.0313333333333333e-05,
      "loss": 0.0031,
      "step": 59060
    },
    {
      "epoch": 3.1504,
      "grad_norm": 0.08488541096448898,
      "learning_rate": 3.031e-05,
      "loss": 0.0033,
      "step": 59070
    },
    {
      "epoch": 3.150933333333333,
      "grad_norm": 0.08488313853740692,
      "learning_rate": 3.0306666666666666e-05,
      "loss": 0.0039,
      "step": 59080
    },
    {
      "epoch": 3.151466666666667,
      "grad_norm": 0.1980614811182022,
      "learning_rate": 3.0303333333333335e-05,
      "loss": 0.0036,
      "step": 59090
    },
    {
      "epoch": 3.152,
      "grad_norm": 0.11318063735961914,
      "learning_rate": 3.03e-05,
      "loss": 0.0026,
      "step": 59100
    },
    {
      "epoch": 3.1525333333333334,
      "grad_norm": 0.16977490484714508,
      "learning_rate": 3.0296666666666667e-05,
      "loss": 0.0035,
      "step": 59110
    },
    {
      "epoch": 3.1530666666666667,
      "grad_norm": 0.2829347550868988,
      "learning_rate": 3.0293333333333334e-05,
      "loss": 0.0035,
      "step": 59120
    },
    {
      "epoch": 3.1536,
      "grad_norm": 0.2546525001525879,
      "learning_rate": 3.0290000000000003e-05,
      "loss": 0.0036,
      "step": 59130
    },
    {
      "epoch": 3.1541333333333332,
      "grad_norm": 0.16976825892925262,
      "learning_rate": 3.028666666666667e-05,
      "loss": 0.0025,
      "step": 59140
    },
    {
      "epoch": 3.1546666666666665,
      "grad_norm": 0.14147460460662842,
      "learning_rate": 3.0283333333333335e-05,
      "loss": 0.0023,
      "step": 59150
    },
    {
      "epoch": 3.1552,
      "grad_norm": 0.14147260785102844,
      "learning_rate": 3.028e-05,
      "loss": 0.0033,
      "step": 59160
    },
    {
      "epoch": 3.1557333333333335,
      "grad_norm": 0.08488395065069199,
      "learning_rate": 3.027666666666667e-05,
      "loss": 0.003,
      "step": 59170
    },
    {
      "epoch": 3.1562666666666668,
      "grad_norm": 0.19807006418704987,
      "learning_rate": 3.0273333333333337e-05,
      "loss": 0.0026,
      "step": 59180
    },
    {
      "epoch": 3.1568,
      "grad_norm": 0.08488167822360992,
      "learning_rate": 3.0270000000000003e-05,
      "loss": 0.0036,
      "step": 59190
    },
    {
      "epoch": 3.1573333333333333,
      "grad_norm": 0.25465184450149536,
      "learning_rate": 3.0266666666666666e-05,
      "loss": 0.0041,
      "step": 59200
    },
    {
      "epoch": 3.1578666666666666,
      "grad_norm": 0.05659019201993942,
      "learning_rate": 3.0263333333333332e-05,
      "loss": 0.0041,
      "step": 59210
    },
    {
      "epoch": 3.1584,
      "grad_norm": 0.480977863073349,
      "learning_rate": 3.0259999999999998e-05,
      "loss": 0.0031,
      "step": 59220
    },
    {
      "epoch": 3.158933333333333,
      "grad_norm": 1.6736320107568758e-09,
      "learning_rate": 3.0256666666666668e-05,
      "loss": 0.0027,
      "step": 59230
    },
    {
      "epoch": 3.159466666666667,
      "grad_norm": 0.25463783740997314,
      "learning_rate": 3.0253333333333334e-05,
      "loss": 0.0032,
      "step": 59240
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.14148056507110596,
      "learning_rate": 3.025e-05,
      "loss": 0.0029,
      "step": 59250
    },
    {
      "epoch": 3.1605333333333334,
      "grad_norm": 0.3677978515625,
      "learning_rate": 3.0246666666666666e-05,
      "loss": 0.003,
      "step": 59260
    },
    {
      "epoch": 3.1610666666666667,
      "grad_norm": 0.05659040808677673,
      "learning_rate": 3.0243333333333336e-05,
      "loss": 0.0038,
      "step": 59270
    },
    {
      "epoch": 3.1616,
      "grad_norm": 0.31121498346328735,
      "learning_rate": 3.0240000000000002e-05,
      "loss": 0.0032,
      "step": 59280
    },
    {
      "epoch": 3.1621333333333332,
      "grad_norm": 0.08488263934850693,
      "learning_rate": 3.0236666666666668e-05,
      "loss": 0.0035,
      "step": 59290
    },
    {
      "epoch": 3.1626666666666665,
      "grad_norm": 0.22633549571037292,
      "learning_rate": 3.0233333333333334e-05,
      "loss": 0.0046,
      "step": 59300
    },
    {
      "epoch": 3.1632,
      "grad_norm": 0.05658774822950363,
      "learning_rate": 3.0230000000000004e-05,
      "loss": 0.007,
      "step": 59310
    },
    {
      "epoch": 3.1637333333333335,
      "grad_norm": 1.4690086924673551e-09,
      "learning_rate": 3.022666666666667e-05,
      "loss": 0.0036,
      "step": 59320
    },
    {
      "epoch": 3.164266666666667,
      "grad_norm": 0.05658595636487007,
      "learning_rate": 3.0223333333333336e-05,
      "loss": 0.0025,
      "step": 59330
    },
    {
      "epoch": 3.1648,
      "grad_norm": 0.028295287862420082,
      "learning_rate": 3.0220000000000005e-05,
      "loss": 0.0035,
      "step": 59340
    },
    {
      "epoch": 3.1653333333333333,
      "grad_norm": 0.1705019176006317,
      "learning_rate": 3.0216666666666665e-05,
      "loss": 0.0033,
      "step": 59350
    },
    {
      "epoch": 3.1658666666666666,
      "grad_norm": 0.3395434617996216,
      "learning_rate": 3.021333333333333e-05,
      "loss": 0.0032,
      "step": 59360
    },
    {
      "epoch": 3.1664,
      "grad_norm": 0.3960770070552826,
      "learning_rate": 3.021e-05,
      "loss": 0.0034,
      "step": 59370
    },
    {
      "epoch": 3.166933333333333,
      "grad_norm": 0.3112412989139557,
      "learning_rate": 3.0206666666666667e-05,
      "loss": 0.0027,
      "step": 59380
    },
    {
      "epoch": 3.167466666666667,
      "grad_norm": 0.08487845957279205,
      "learning_rate": 3.0203333333333333e-05,
      "loss": 0.0043,
      "step": 59390
    },
    {
      "epoch": 3.168,
      "grad_norm": 0.11316630989313126,
      "learning_rate": 3.02e-05,
      "loss": 0.0038,
      "step": 59400
    },
    {
      "epoch": 3.1685333333333334,
      "grad_norm": 0.08487915247678757,
      "learning_rate": 3.019666666666667e-05,
      "loss": 0.0036,
      "step": 59410
    },
    {
      "epoch": 3.1690666666666667,
      "grad_norm": 0.05658392980694771,
      "learning_rate": 3.0193333333333335e-05,
      "loss": 0.0037,
      "step": 59420
    },
    {
      "epoch": 3.1696,
      "grad_norm": 0.05658642575144768,
      "learning_rate": 3.019e-05,
      "loss": 0.0039,
      "step": 59430
    },
    {
      "epoch": 3.1701333333333332,
      "grad_norm": 0.22635135054588318,
      "learning_rate": 3.018666666666667e-05,
      "loss": 0.0023,
      "step": 59440
    },
    {
      "epoch": 3.1706666666666665,
      "grad_norm": 0.11317312717437744,
      "learning_rate": 3.0183333333333336e-05,
      "loss": 0.0026,
      "step": 59450
    },
    {
      "epoch": 3.1712,
      "grad_norm": 0.029628461226820946,
      "learning_rate": 3.0180000000000002e-05,
      "loss": 0.0029,
      "step": 59460
    },
    {
      "epoch": 3.1717333333333335,
      "grad_norm": 0.14145898818969727,
      "learning_rate": 3.017666666666667e-05,
      "loss": 0.0022,
      "step": 59470
    },
    {
      "epoch": 3.172266666666667,
      "grad_norm": 0.028292642906308174,
      "learning_rate": 3.0173333333333338e-05,
      "loss": 0.0036,
      "step": 59480
    },
    {
      "epoch": 3.1728,
      "grad_norm": 0.08487896621227264,
      "learning_rate": 3.0170000000000004e-05,
      "loss": 0.0029,
      "step": 59490
    },
    {
      "epoch": 3.1733333333333333,
      "grad_norm": 0.028291692957282066,
      "learning_rate": 3.016666666666667e-05,
      "loss": 0.0028,
      "step": 59500
    },
    {
      "epoch": 3.1738666666666666,
      "grad_norm": 0.08538736402988434,
      "learning_rate": 3.0163333333333333e-05,
      "loss": 0.003,
      "step": 59510
    },
    {
      "epoch": 3.1744,
      "grad_norm": 0.05658286437392235,
      "learning_rate": 3.016e-05,
      "loss": 0.003,
      "step": 59520
    },
    {
      "epoch": 3.174933333333333,
      "grad_norm": 0.08474228531122208,
      "learning_rate": 3.0156666666666665e-05,
      "loss": 0.003,
      "step": 59530
    },
    {
      "epoch": 3.175466666666667,
      "grad_norm": 0.08487620204687119,
      "learning_rate": 3.0153333333333335e-05,
      "loss": 0.0034,
      "step": 59540
    },
    {
      "epoch": 3.176,
      "grad_norm": 0.08487376570701599,
      "learning_rate": 3.015e-05,
      "loss": 0.0034,
      "step": 59550
    },
    {
      "epoch": 3.1765333333333334,
      "grad_norm": 0.16974982619285583,
      "learning_rate": 3.0146666666666667e-05,
      "loss": 0.003,
      "step": 59560
    },
    {
      "epoch": 3.1770666666666667,
      "grad_norm": 0.14182136952877045,
      "learning_rate": 3.0143333333333333e-05,
      "loss": 0.004,
      "step": 59570
    },
    {
      "epoch": 3.1776,
      "grad_norm": 0.02829190343618393,
      "learning_rate": 3.0140000000000003e-05,
      "loss": 0.0036,
      "step": 59580
    },
    {
      "epoch": 3.1781333333333333,
      "grad_norm": 0.1697533279657364,
      "learning_rate": 3.013666666666667e-05,
      "loss": 0.0027,
      "step": 59590
    },
    {
      "epoch": 3.1786666666666665,
      "grad_norm": 0.1697533130645752,
      "learning_rate": 3.0133333333333335e-05,
      "loss": 0.0031,
      "step": 59600
    },
    {
      "epoch": 3.1792,
      "grad_norm": 0.11316496878862381,
      "learning_rate": 3.013e-05,
      "loss": 0.0027,
      "step": 59610
    },
    {
      "epoch": 3.1797333333333335,
      "grad_norm": 0.16979549825191498,
      "learning_rate": 3.012666666666667e-05,
      "loss": 0.0026,
      "step": 59620
    },
    {
      "epoch": 3.180266666666667,
      "grad_norm": 0.02829139493405819,
      "learning_rate": 3.0123333333333337e-05,
      "loss": 0.0037,
      "step": 59630
    },
    {
      "epoch": 3.1808,
      "grad_norm": 0.08487820625305176,
      "learning_rate": 3.0120000000000003e-05,
      "loss": 0.0038,
      "step": 59640
    },
    {
      "epoch": 3.1813333333333333,
      "grad_norm": 0.396071195602417,
      "learning_rate": 3.011666666666667e-05,
      "loss": 0.0028,
      "step": 59650
    },
    {
      "epoch": 3.1818666666666666,
      "grad_norm": 0.39610975980758667,
      "learning_rate": 3.0113333333333332e-05,
      "loss": 0.0017,
      "step": 59660
    },
    {
      "epoch": 3.1824,
      "grad_norm": 0.11316465586423874,
      "learning_rate": 3.0109999999999998e-05,
      "loss": 0.002,
      "step": 59670
    },
    {
      "epoch": 3.182933333333333,
      "grad_norm": 0.056929029524326324,
      "learning_rate": 3.0106666666666668e-05,
      "loss": 0.002,
      "step": 59680
    },
    {
      "epoch": 3.183466666666667,
      "grad_norm": 0.028292760252952576,
      "learning_rate": 3.0103333333333334e-05,
      "loss": 0.0031,
      "step": 59690
    },
    {
      "epoch": 3.184,
      "grad_norm": 0.11316531896591187,
      "learning_rate": 3.01e-05,
      "loss": 0.0027,
      "step": 59700
    },
    {
      "epoch": 3.1845333333333334,
      "grad_norm": 0.28290265798568726,
      "learning_rate": 3.0096666666666666e-05,
      "loss": 0.0027,
      "step": 59710
    },
    {
      "epoch": 3.1850666666666667,
      "grad_norm": 5.555551574332185e-10,
      "learning_rate": 3.0093333333333335e-05,
      "loss": 0.0039,
      "step": 59720
    },
    {
      "epoch": 3.1856,
      "grad_norm": 0.2546294331550598,
      "learning_rate": 3.009e-05,
      "loss": 0.0019,
      "step": 59730
    },
    {
      "epoch": 3.1861333333333333,
      "grad_norm": 0.05658378824591637,
      "learning_rate": 3.0086666666666668e-05,
      "loss": 0.0043,
      "step": 59740
    },
    {
      "epoch": 3.1866666666666665,
      "grad_norm": 0.14145565032958984,
      "learning_rate": 3.0083333333333337e-05,
      "loss": 0.0034,
      "step": 59750
    },
    {
      "epoch": 3.1872,
      "grad_norm": 0.19803981482982635,
      "learning_rate": 3.0080000000000003e-05,
      "loss": 0.0022,
      "step": 59760
    },
    {
      "epoch": 3.1877333333333335,
      "grad_norm": 0.0848732441663742,
      "learning_rate": 3.007666666666667e-05,
      "loss": 0.0019,
      "step": 59770
    },
    {
      "epoch": 3.188266666666667,
      "grad_norm": 0.36778560280799866,
      "learning_rate": 3.0073333333333336e-05,
      "loss": 0.0022,
      "step": 59780
    },
    {
      "epoch": 3.1888,
      "grad_norm": 0.19803433120250702,
      "learning_rate": 3.0070000000000005e-05,
      "loss": 0.0033,
      "step": 59790
    },
    {
      "epoch": 3.1893333333333334,
      "grad_norm": 0.08487017452716827,
      "learning_rate": 3.006666666666667e-05,
      "loss": 0.0021,
      "step": 59800
    },
    {
      "epoch": 3.1898666666666666,
      "grad_norm": 0.23340442776679993,
      "learning_rate": 3.006333333333333e-05,
      "loss": 0.0041,
      "step": 59810
    },
    {
      "epoch": 3.1904,
      "grad_norm": 0.02829120121896267,
      "learning_rate": 3.006e-05,
      "loss": 0.0031,
      "step": 59820
    },
    {
      "epoch": 3.190933333333333,
      "grad_norm": 0.254618376493454,
      "learning_rate": 3.0056666666666666e-05,
      "loss": 0.0019,
      "step": 59830
    },
    {
      "epoch": 3.191466666666667,
      "grad_norm": 0.2829063832759857,
      "learning_rate": 3.0053333333333332e-05,
      "loss": 0.0044,
      "step": 59840
    },
    {
      "epoch": 3.192,
      "grad_norm": 0.1131611242890358,
      "learning_rate": 3.0050000000000002e-05,
      "loss": 0.0022,
      "step": 59850
    },
    {
      "epoch": 3.1925333333333334,
      "grad_norm": 0.20037251710891724,
      "learning_rate": 3.0046666666666668e-05,
      "loss": 0.0071,
      "step": 59860
    },
    {
      "epoch": 3.1930666666666667,
      "grad_norm": 3.3665705778673782e-09,
      "learning_rate": 3.0043333333333334e-05,
      "loss": 0.0078,
      "step": 59870
    },
    {
      "epoch": 3.1936,
      "grad_norm": 4.29527613476921e-09,
      "learning_rate": 3.004e-05,
      "loss": 0.0028,
      "step": 59880
    },
    {
      "epoch": 3.1941333333333333,
      "grad_norm": 2.4569235534954714e-09,
      "learning_rate": 3.003666666666667e-05,
      "loss": 0.0035,
      "step": 59890
    },
    {
      "epoch": 3.1946666666666665,
      "grad_norm": 0.22787602245807648,
      "learning_rate": 3.0033333333333336e-05,
      "loss": 0.0046,
      "step": 59900
    },
    {
      "epoch": 3.1952,
      "grad_norm": 0.05657990276813507,
      "learning_rate": 3.0030000000000002e-05,
      "loss": 0.0032,
      "step": 59910
    },
    {
      "epoch": 3.1957333333333335,
      "grad_norm": 0.028289614245295525,
      "learning_rate": 3.0026666666666668e-05,
      "loss": 0.0031,
      "step": 59920
    },
    {
      "epoch": 3.196266666666667,
      "grad_norm": 0.3111986517906189,
      "learning_rate": 3.0023333333333338e-05,
      "loss": 0.0038,
      "step": 59930
    },
    {
      "epoch": 3.1968,
      "grad_norm": 0.5091678500175476,
      "learning_rate": 3.0020000000000004e-05,
      "loss": 0.0044,
      "step": 59940
    },
    {
      "epoch": 3.1973333333333334,
      "grad_norm": 0.19803820550441742,
      "learning_rate": 3.001666666666667e-05,
      "loss": 0.0038,
      "step": 59950
    },
    {
      "epoch": 3.1978666666666666,
      "grad_norm": 0.4243033826351166,
      "learning_rate": 3.0013333333333333e-05,
      "loss": 0.0028,
      "step": 59960
    },
    {
      "epoch": 3.1984,
      "grad_norm": 0.4526468813419342,
      "learning_rate": 3.001e-05,
      "loss": 0.0028,
      "step": 59970
    },
    {
      "epoch": 3.198933333333333,
      "grad_norm": 0.14143915474414825,
      "learning_rate": 3.0006666666666665e-05,
      "loss": 0.0027,
      "step": 59980
    },
    {
      "epoch": 3.1994666666666665,
      "grad_norm": 0.1414506733417511,
      "learning_rate": 3.0003333333333335e-05,
      "loss": 0.0022,
      "step": 59990
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.19800947606563568,
      "learning_rate": 3e-05,
      "loss": 0.0032,
      "step": 60000
    },
    {
      "epoch": 3.2005333333333335,
      "grad_norm": 0.16972918808460236,
      "learning_rate": 2.9996666666666667e-05,
      "loss": 0.0022,
      "step": 60010
    },
    {
      "epoch": 3.2010666666666667,
      "grad_norm": 0.14143399894237518,
      "learning_rate": 2.9993333333333333e-05,
      "loss": 0.0027,
      "step": 60020
    },
    {
      "epoch": 3.2016,
      "grad_norm": 0.2828706204891205,
      "learning_rate": 2.9990000000000003e-05,
      "loss": 0.0029,
      "step": 60030
    },
    {
      "epoch": 3.2021333333333333,
      "grad_norm": 1.1390217542648315,
      "learning_rate": 2.998666666666667e-05,
      "loss": 0.003,
      "step": 60040
    },
    {
      "epoch": 3.2026666666666666,
      "grad_norm": 0.08486250787973404,
      "learning_rate": 2.9983333333333335e-05,
      "loss": 0.0041,
      "step": 60050
    },
    {
      "epoch": 3.2032,
      "grad_norm": 0.14143936336040497,
      "learning_rate": 2.998e-05,
      "loss": 0.0026,
      "step": 60060
    },
    {
      "epoch": 3.203733333333333,
      "grad_norm": 0.14143633842468262,
      "learning_rate": 2.997666666666667e-05,
      "loss": 0.0042,
      "step": 60070
    },
    {
      "epoch": 3.204266666666667,
      "grad_norm": 0.02828741818666458,
      "learning_rate": 2.9973333333333337e-05,
      "loss": 0.0035,
      "step": 60080
    },
    {
      "epoch": 3.2048,
      "grad_norm": 0.3111679255962372,
      "learning_rate": 2.9970000000000003e-05,
      "loss": 0.0036,
      "step": 60090
    },
    {
      "epoch": 3.2053333333333334,
      "grad_norm": 0.08486492931842804,
      "learning_rate": 2.9966666666666672e-05,
      "loss": 0.0033,
      "step": 60100
    },
    {
      "epoch": 3.2058666666666666,
      "grad_norm": 0.31114163994789124,
      "learning_rate": 2.996333333333333e-05,
      "loss": 0.0041,
      "step": 60110
    },
    {
      "epoch": 3.2064,
      "grad_norm": 0.25460460782051086,
      "learning_rate": 2.9959999999999998e-05,
      "loss": 0.0025,
      "step": 60120
    },
    {
      "epoch": 3.206933333333333,
      "grad_norm": 0.11314382404088974,
      "learning_rate": 2.9956666666666667e-05,
      "loss": 0.0027,
      "step": 60130
    },
    {
      "epoch": 3.2074666666666665,
      "grad_norm": 0.1697237640619278,
      "learning_rate": 2.9953333333333333e-05,
      "loss": 0.0033,
      "step": 60140
    },
    {
      "epoch": 3.208,
      "grad_norm": 0.056575898081064224,
      "learning_rate": 2.995e-05,
      "loss": 0.0031,
      "step": 60150
    },
    {
      "epoch": 3.2085333333333335,
      "grad_norm": 6.4483787198810205e-09,
      "learning_rate": 2.9946666666666666e-05,
      "loss": 0.0014,
      "step": 60160
    },
    {
      "epoch": 3.2090666666666667,
      "grad_norm": 0.3111568093299866,
      "learning_rate": 2.9943333333333335e-05,
      "loss": 0.0024,
      "step": 60170
    },
    {
      "epoch": 3.2096,
      "grad_norm": 0.14143751561641693,
      "learning_rate": 2.994e-05,
      "loss": 0.0037,
      "step": 60180
    },
    {
      "epoch": 3.2101333333333333,
      "grad_norm": 0.2545642852783203,
      "learning_rate": 2.9936666666666667e-05,
      "loss": 0.0034,
      "step": 60190
    },
    {
      "epoch": 3.2106666666666666,
      "grad_norm": 0.08486376702785492,
      "learning_rate": 2.9933333333333337e-05,
      "loss": 0.0034,
      "step": 60200
    },
    {
      "epoch": 3.2112,
      "grad_norm": 0.4525723457336426,
      "learning_rate": 2.9930000000000003e-05,
      "loss": 0.002,
      "step": 60210
    },
    {
      "epoch": 3.211733333333333,
      "grad_norm": 0.2262774556875229,
      "learning_rate": 2.992666666666667e-05,
      "loss": 0.0036,
      "step": 60220
    },
    {
      "epoch": 3.212266666666667,
      "grad_norm": 0.367726594209671,
      "learning_rate": 2.9923333333333335e-05,
      "loss": 0.0044,
      "step": 60230
    },
    {
      "epoch": 3.2128,
      "grad_norm": 0.056571926921606064,
      "learning_rate": 2.9920000000000005e-05,
      "loss": 0.0029,
      "step": 60240
    },
    {
      "epoch": 3.2133333333333334,
      "grad_norm": 0.08485633134841919,
      "learning_rate": 2.991666666666667e-05,
      "loss": 0.0031,
      "step": 60250
    },
    {
      "epoch": 3.2138666666666666,
      "grad_norm": 0.39600589871406555,
      "learning_rate": 2.991333333333333e-05,
      "loss": 0.0039,
      "step": 60260
    },
    {
      "epoch": 3.2144,
      "grad_norm": 0.1697172373533249,
      "learning_rate": 2.991e-05,
      "loss": 0.0023,
      "step": 60270
    },
    {
      "epoch": 3.214933333333333,
      "grad_norm": 0.2262973040342331,
      "learning_rate": 2.9906666666666666e-05,
      "loss": 0.0017,
      "step": 60280
    },
    {
      "epoch": 3.2154666666666665,
      "grad_norm": 0.2262754738330841,
      "learning_rate": 2.9903333333333332e-05,
      "loss": 0.0037,
      "step": 60290
    },
    {
      "epoch": 3.216,
      "grad_norm": 0.14143867790699005,
      "learning_rate": 2.9900000000000002e-05,
      "loss": 0.0019,
      "step": 60300
    },
    {
      "epoch": 3.2165333333333335,
      "grad_norm": 0.0848594456911087,
      "learning_rate": 2.9896666666666668e-05,
      "loss": 0.0018,
      "step": 60310
    },
    {
      "epoch": 3.2170666666666667,
      "grad_norm": 0.056572284549474716,
      "learning_rate": 2.9893333333333334e-05,
      "loss": 0.0024,
      "step": 60320
    },
    {
      "epoch": 3.2176,
      "grad_norm": 0.08529751747846603,
      "learning_rate": 2.989e-05,
      "loss": 0.0023,
      "step": 60330
    },
    {
      "epoch": 3.2181333333333333,
      "grad_norm": 0.8048643469810486,
      "learning_rate": 2.988666666666667e-05,
      "loss": 0.0027,
      "step": 60340
    },
    {
      "epoch": 3.2186666666666666,
      "grad_norm": 0.6787809729576111,
      "learning_rate": 2.9883333333333336e-05,
      "loss": 0.0036,
      "step": 60350
    },
    {
      "epoch": 3.2192,
      "grad_norm": 0.3394488990306854,
      "learning_rate": 2.9880000000000002e-05,
      "loss": 0.0031,
      "step": 60360
    },
    {
      "epoch": 3.219733333333333,
      "grad_norm": 0.11313378810882568,
      "learning_rate": 2.9876666666666668e-05,
      "loss": 0.0038,
      "step": 60370
    },
    {
      "epoch": 3.220266666666667,
      "grad_norm": 0.25457096099853516,
      "learning_rate": 2.9873333333333338e-05,
      "loss": 0.0043,
      "step": 60380
    },
    {
      "epoch": 3.2208,
      "grad_norm": 0.22627423703670502,
      "learning_rate": 2.9870000000000004e-05,
      "loss": 0.0029,
      "step": 60390
    },
    {
      "epoch": 3.2213333333333334,
      "grad_norm": 0.05691669136285782,
      "learning_rate": 2.986666666666667e-05,
      "loss": 0.0022,
      "step": 60400
    },
    {
      "epoch": 3.2218666666666667,
      "grad_norm": 2.014255207072324e-09,
      "learning_rate": 2.9863333333333333e-05,
      "loss": 0.0025,
      "step": 60410
    },
    {
      "epoch": 3.2224,
      "grad_norm": 0.05715572088956833,
      "learning_rate": 2.986e-05,
      "loss": 0.0031,
      "step": 60420
    },
    {
      "epoch": 3.222933333333333,
      "grad_norm": 0.02828436717391014,
      "learning_rate": 2.9856666666666665e-05,
      "loss": 0.0042,
      "step": 60430
    },
    {
      "epoch": 3.2234666666666665,
      "grad_norm": 0.08485564589500427,
      "learning_rate": 2.9853333333333334e-05,
      "loss": 0.0016,
      "step": 60440
    },
    {
      "epoch": 3.224,
      "grad_norm": 0.08484803885221481,
      "learning_rate": 2.985e-05,
      "loss": 0.0031,
      "step": 60450
    },
    {
      "epoch": 3.2245333333333335,
      "grad_norm": 0.1979924738407135,
      "learning_rate": 2.9846666666666667e-05,
      "loss": 0.0036,
      "step": 60460
    },
    {
      "epoch": 3.2250666666666667,
      "grad_norm": 0.11313444375991821,
      "learning_rate": 2.9843333333333333e-05,
      "loss": 0.0027,
      "step": 60470
    },
    {
      "epoch": 3.2256,
      "grad_norm": 0.028284067288041115,
      "learning_rate": 2.9840000000000002e-05,
      "loss": 0.0016,
      "step": 60480
    },
    {
      "epoch": 3.2261333333333333,
      "grad_norm": 0.11313270777463913,
      "learning_rate": 2.983666666666667e-05,
      "loss": 0.0035,
      "step": 60490
    },
    {
      "epoch": 3.2266666666666666,
      "grad_norm": 0.16969601809978485,
      "learning_rate": 2.9833333333333335e-05,
      "loss": 0.0029,
      "step": 60500
    },
    {
      "epoch": 3.2272,
      "grad_norm": 0.226278617978096,
      "learning_rate": 2.9830000000000004e-05,
      "loss": 0.0043,
      "step": 60510
    },
    {
      "epoch": 3.227733333333333,
      "grad_norm": 0.02828260324895382,
      "learning_rate": 2.982666666666667e-05,
      "loss": 0.0033,
      "step": 60520
    },
    {
      "epoch": 3.228266666666667,
      "grad_norm": 9.728435834688298e-10,
      "learning_rate": 2.9823333333333336e-05,
      "loss": 0.002,
      "step": 60530
    },
    {
      "epoch": 3.2288,
      "grad_norm": 0.16970698535442352,
      "learning_rate": 2.9820000000000002e-05,
      "loss": 0.0028,
      "step": 60540
    },
    {
      "epoch": 3.2293333333333334,
      "grad_norm": 0.22625604271888733,
      "learning_rate": 2.9816666666666672e-05,
      "loss": 0.004,
      "step": 60550
    },
    {
      "epoch": 3.2298666666666667,
      "grad_norm": 0.36767804622650146,
      "learning_rate": 2.981333333333333e-05,
      "loss": 0.0013,
      "step": 60560
    },
    {
      "epoch": 3.2304,
      "grad_norm": 0.08484829217195511,
      "learning_rate": 2.9809999999999997e-05,
      "loss": 0.0035,
      "step": 60570
    },
    {
      "epoch": 3.230933333333333,
      "grad_norm": 0.2828391194343567,
      "learning_rate": 2.9806666666666667e-05,
      "loss": 0.0023,
      "step": 60580
    },
    {
      "epoch": 3.2314666666666665,
      "grad_norm": 0.08484777063131332,
      "learning_rate": 2.9803333333333333e-05,
      "loss": 0.0027,
      "step": 60590
    },
    {
      "epoch": 3.232,
      "grad_norm": 0.19797515869140625,
      "learning_rate": 2.98e-05,
      "loss": 0.0035,
      "step": 60600
    },
    {
      "epoch": 3.2325333333333335,
      "grad_norm": 0.22625692188739777,
      "learning_rate": 2.979666666666667e-05,
      "loss": 0.0035,
      "step": 60610
    },
    {
      "epoch": 3.2330666666666668,
      "grad_norm": 1.971815155599188e-09,
      "learning_rate": 2.9793333333333335e-05,
      "loss": 0.003,
      "step": 60620
    },
    {
      "epoch": 3.2336,
      "grad_norm": 0.05806060507893562,
      "learning_rate": 2.979e-05,
      "loss": 0.0026,
      "step": 60630
    },
    {
      "epoch": 3.2341333333333333,
      "grad_norm": 0.02828303724527359,
      "learning_rate": 2.9786666666666667e-05,
      "loss": 0.0028,
      "step": 60640
    },
    {
      "epoch": 3.2346666666666666,
      "grad_norm": 0.3393753170967102,
      "learning_rate": 2.9783333333333337e-05,
      "loss": 0.0038,
      "step": 60650
    },
    {
      "epoch": 3.2352,
      "grad_norm": 0.08484847843647003,
      "learning_rate": 2.9780000000000003e-05,
      "loss": 0.0034,
      "step": 60660
    },
    {
      "epoch": 3.235733333333333,
      "grad_norm": 2.8804834073525853e-09,
      "learning_rate": 2.977666666666667e-05,
      "loss": 0.0023,
      "step": 60670
    },
    {
      "epoch": 3.236266666666667,
      "grad_norm": 0.2262701541185379,
      "learning_rate": 2.9773333333333335e-05,
      "loss": 0.0025,
      "step": 60680
    },
    {
      "epoch": 3.2368,
      "grad_norm": 0.3959663212299347,
      "learning_rate": 2.9770000000000005e-05,
      "loss": 0.0032,
      "step": 60690
    },
    {
      "epoch": 3.2373333333333334,
      "grad_norm": 0.02828248031437397,
      "learning_rate": 2.976666666666667e-05,
      "loss": 0.0034,
      "step": 60700
    },
    {
      "epoch": 3.2378666666666667,
      "grad_norm": 0.11313150078058243,
      "learning_rate": 2.9763333333333337e-05,
      "loss": 0.004,
      "step": 60710
    },
    {
      "epoch": 3.2384,
      "grad_norm": 0.22624363005161285,
      "learning_rate": 2.976e-05,
      "loss": 0.0022,
      "step": 60720
    },
    {
      "epoch": 3.238933333333333,
      "grad_norm": 0.4525372385978699,
      "learning_rate": 2.9756666666666666e-05,
      "loss": 0.0038,
      "step": 60730
    },
    {
      "epoch": 3.2394666666666665,
      "grad_norm": 0.22624528408050537,
      "learning_rate": 2.9753333333333332e-05,
      "loss": 0.0025,
      "step": 60740
    },
    {
      "epoch": 3.24,
      "grad_norm": 0.056566204875707626,
      "learning_rate": 2.975e-05,
      "loss": 0.0033,
      "step": 60750
    },
    {
      "epoch": 3.2405333333333335,
      "grad_norm": 0.2262396663427353,
      "learning_rate": 2.9746666666666668e-05,
      "loss": 0.0021,
      "step": 60760
    },
    {
      "epoch": 3.2410666666666668,
      "grad_norm": 0.028282223269343376,
      "learning_rate": 2.9743333333333334e-05,
      "loss": 0.0017,
      "step": 60770
    },
    {
      "epoch": 3.2416,
      "grad_norm": 0.14139875769615173,
      "learning_rate": 2.974e-05,
      "loss": 0.0034,
      "step": 60780
    },
    {
      "epoch": 3.2421333333333333,
      "grad_norm": 0.22625823318958282,
      "learning_rate": 2.973666666666667e-05,
      "loss": 0.0031,
      "step": 60790
    },
    {
      "epoch": 3.2426666666666666,
      "grad_norm": 0.1414099782705307,
      "learning_rate": 2.9733333333333336e-05,
      "loss": 0.0034,
      "step": 60800
    },
    {
      "epoch": 3.2432,
      "grad_norm": 0.11311976611614227,
      "learning_rate": 2.973e-05,
      "loss": 0.0027,
      "step": 60810
    },
    {
      "epoch": 3.243733333333333,
      "grad_norm": 0.14140726625919342,
      "learning_rate": 2.9726666666666668e-05,
      "loss": 0.0028,
      "step": 60820
    },
    {
      "epoch": 3.244266666666667,
      "grad_norm": 0.395902544260025,
      "learning_rate": 2.9723333333333337e-05,
      "loss": 0.0037,
      "step": 60830
    },
    {
      "epoch": 3.2448,
      "grad_norm": 0.22625046968460083,
      "learning_rate": 2.9720000000000003e-05,
      "loss": 0.0029,
      "step": 60840
    },
    {
      "epoch": 3.2453333333333334,
      "grad_norm": 3.171120255274218e-09,
      "learning_rate": 2.971666666666667e-05,
      "loss": 0.0022,
      "step": 60850
    },
    {
      "epoch": 3.2458666666666667,
      "grad_norm": 0.02827998250722885,
      "learning_rate": 2.971333333333334e-05,
      "loss": 0.0025,
      "step": 60860
    },
    {
      "epoch": 3.2464,
      "grad_norm": 0.08484500646591187,
      "learning_rate": 2.971e-05,
      "loss": 0.0025,
      "step": 60870
    },
    {
      "epoch": 3.2469333333333332,
      "grad_norm": 0.1987900286912918,
      "learning_rate": 2.9706666666666665e-05,
      "loss": 0.0024,
      "step": 60880
    },
    {
      "epoch": 3.2474666666666665,
      "grad_norm": 0.08484338223934174,
      "learning_rate": 2.9703333333333334e-05,
      "loss": 0.004,
      "step": 60890
    },
    {
      "epoch": 3.248,
      "grad_norm": 0.02828000672161579,
      "learning_rate": 2.97e-05,
      "loss": 0.0028,
      "step": 60900
    },
    {
      "epoch": 3.2485333333333335,
      "grad_norm": 0.11437910050153732,
      "learning_rate": 2.9696666666666666e-05,
      "loss": 0.0031,
      "step": 60910
    },
    {
      "epoch": 3.2490666666666668,
      "grad_norm": 0.11564783751964569,
      "learning_rate": 2.9693333333333333e-05,
      "loss": 0.0032,
      "step": 60920
    },
    {
      "epoch": 3.2496,
      "grad_norm": 0.16968300938606262,
      "learning_rate": 2.9690000000000002e-05,
      "loss": 0.0028,
      "step": 60930
    },
    {
      "epoch": 3.2501333333333333,
      "grad_norm": 0.05655861645936966,
      "learning_rate": 2.9686666666666668e-05,
      "loss": 0.003,
      "step": 60940
    },
    {
      "epoch": 3.2506666666666666,
      "grad_norm": 0.19858890771865845,
      "learning_rate": 2.9683333333333334e-05,
      "loss": 0.0027,
      "step": 60950
    },
    {
      "epoch": 3.2512,
      "grad_norm": 0.08483942598104477,
      "learning_rate": 2.9680000000000004e-05,
      "loss": 0.0024,
      "step": 60960
    },
    {
      "epoch": 3.251733333333333,
      "grad_norm": 7.945510027695946e-09,
      "learning_rate": 2.967666666666667e-05,
      "loss": 0.0022,
      "step": 60970
    },
    {
      "epoch": 3.2522666666666664,
      "grad_norm": 0.08484024554491043,
      "learning_rate": 2.9673333333333336e-05,
      "loss": 0.0037,
      "step": 60980
    },
    {
      "epoch": 3.2528,
      "grad_norm": 0.16967280209064484,
      "learning_rate": 2.9670000000000002e-05,
      "loss": 0.0032,
      "step": 60990
    },
    {
      "epoch": 3.2533333333333334,
      "grad_norm": 0.33935296535491943,
      "learning_rate": 2.9666666666666672e-05,
      "loss": 0.0031,
      "step": 61000
    },
    {
      "epoch": 3.2538666666666667,
      "grad_norm": 0.08659844845533371,
      "learning_rate": 2.9663333333333338e-05,
      "loss": 0.0038,
      "step": 61010
    },
    {
      "epoch": 3.2544,
      "grad_norm": 0.254518061876297,
      "learning_rate": 2.9659999999999997e-05,
      "loss": 0.004,
      "step": 61020
    },
    {
      "epoch": 3.2549333333333332,
      "grad_norm": 0.2545149028301239,
      "learning_rate": 2.9656666666666667e-05,
      "loss": 0.0033,
      "step": 61030
    },
    {
      "epoch": 3.2554666666666665,
      "grad_norm": 0.197963148355484,
      "learning_rate": 2.9653333333333333e-05,
      "loss": 0.0029,
      "step": 61040
    },
    {
      "epoch": 3.2560000000000002,
      "grad_norm": 0.11312004923820496,
      "learning_rate": 2.965e-05,
      "loss": 0.0046,
      "step": 61050
    },
    {
      "epoch": 3.2565333333333335,
      "grad_norm": 0.11498584598302841,
      "learning_rate": 2.964666666666667e-05,
      "loss": 0.0022,
      "step": 61060
    },
    {
      "epoch": 3.2570666666666668,
      "grad_norm": 1.8616378438807146e-09,
      "learning_rate": 2.9643333333333335e-05,
      "loss": 0.0033,
      "step": 61070
    },
    {
      "epoch": 3.2576,
      "grad_norm": 0.056560907512903214,
      "learning_rate": 2.964e-05,
      "loss": 0.0033,
      "step": 61080
    },
    {
      "epoch": 3.2581333333333333,
      "grad_norm": 0.08483511209487915,
      "learning_rate": 2.9636666666666667e-05,
      "loss": 0.0034,
      "step": 61090
    },
    {
      "epoch": 3.2586666666666666,
      "grad_norm": 0.08483708649873734,
      "learning_rate": 2.9633333333333336e-05,
      "loss": 0.0021,
      "step": 61100
    },
    {
      "epoch": 3.2592,
      "grad_norm": 0.028280213475227356,
      "learning_rate": 2.9630000000000003e-05,
      "loss": 0.0033,
      "step": 61110
    },
    {
      "epoch": 3.259733333333333,
      "grad_norm": 0.2545195519924164,
      "learning_rate": 2.962666666666667e-05,
      "loss": 0.0035,
      "step": 61120
    },
    {
      "epoch": 3.2602666666666664,
      "grad_norm": 0.39588189125061035,
      "learning_rate": 2.9623333333333335e-05,
      "loss": 0.0029,
      "step": 61130
    },
    {
      "epoch": 3.2608,
      "grad_norm": 0.14138545095920563,
      "learning_rate": 2.9620000000000004e-05,
      "loss": 0.0024,
      "step": 61140
    },
    {
      "epoch": 3.2613333333333334,
      "grad_norm": 0.05655837059020996,
      "learning_rate": 2.961666666666667e-05,
      "loss": 0.0024,
      "step": 61150
    },
    {
      "epoch": 3.2618666666666667,
      "grad_norm": 0.16966812312602997,
      "learning_rate": 2.9613333333333337e-05,
      "loss": 0.0044,
      "step": 61160
    },
    {
      "epoch": 3.2624,
      "grad_norm": 0.05655806511640549,
      "learning_rate": 2.961e-05,
      "loss": 0.0036,
      "step": 61170
    },
    {
      "epoch": 3.2629333333333332,
      "grad_norm": 0.14138251543045044,
      "learning_rate": 2.9606666666666666e-05,
      "loss": 0.0038,
      "step": 61180
    },
    {
      "epoch": 3.2634666666666665,
      "grad_norm": 0.14138871431350708,
      "learning_rate": 2.960333333333333e-05,
      "loss": 0.0032,
      "step": 61190
    },
    {
      "epoch": 3.2640000000000002,
      "grad_norm": 0.05655729025602341,
      "learning_rate": 2.96e-05,
      "loss": 0.0018,
      "step": 61200
    },
    {
      "epoch": 3.2645333333333335,
      "grad_norm": 0.028276558965444565,
      "learning_rate": 2.9596666666666667e-05,
      "loss": 0.0039,
      "step": 61210
    },
    {
      "epoch": 3.265066666666667,
      "grad_norm": 0.028278427198529243,
      "learning_rate": 2.9593333333333333e-05,
      "loss": 0.004,
      "step": 61220
    },
    {
      "epoch": 3.2656,
      "grad_norm": 0.22622400522232056,
      "learning_rate": 2.959e-05,
      "loss": 0.0033,
      "step": 61230
    },
    {
      "epoch": 3.2661333333333333,
      "grad_norm": 0.16966785490512848,
      "learning_rate": 2.958666666666667e-05,
      "loss": 0.0028,
      "step": 61240
    },
    {
      "epoch": 3.2666666666666666,
      "grad_norm": 0.3397699296474457,
      "learning_rate": 2.9583333333333335e-05,
      "loss": 0.0025,
      "step": 61250
    },
    {
      "epoch": 3.2672,
      "grad_norm": 0.028277430683374405,
      "learning_rate": 2.958e-05,
      "loss": 0.0026,
      "step": 61260
    },
    {
      "epoch": 3.267733333333333,
      "grad_norm": 0.16965658962726593,
      "learning_rate": 2.9576666666666668e-05,
      "loss": 0.0042,
      "step": 61270
    },
    {
      "epoch": 3.2682666666666664,
      "grad_norm": 0.19903956353664398,
      "learning_rate": 2.9573333333333337e-05,
      "loss": 0.0026,
      "step": 61280
    },
    {
      "epoch": 3.2688,
      "grad_norm": 0.1131049320101738,
      "learning_rate": 2.9570000000000003e-05,
      "loss": 0.0044,
      "step": 61290
    },
    {
      "epoch": 3.2693333333333334,
      "grad_norm": 0.14138394594192505,
      "learning_rate": 2.956666666666667e-05,
      "loss": 0.0028,
      "step": 61300
    },
    {
      "epoch": 3.2698666666666667,
      "grad_norm": 0.028277216479182243,
      "learning_rate": 2.956333333333334e-05,
      "loss": 0.0024,
      "step": 61310
    },
    {
      "epoch": 3.2704,
      "grad_norm": 0.1979292780160904,
      "learning_rate": 2.9559999999999998e-05,
      "loss": 0.002,
      "step": 61320
    },
    {
      "epoch": 3.2709333333333332,
      "grad_norm": 0.056555088609457016,
      "learning_rate": 2.9556666666666664e-05,
      "loss": 0.0028,
      "step": 61330
    },
    {
      "epoch": 3.2714666666666665,
      "grad_norm": 0.395877480506897,
      "learning_rate": 2.9553333333333334e-05,
      "loss": 0.0029,
      "step": 61340
    },
    {
      "epoch": 3.2720000000000002,
      "grad_norm": 0.16966256499290466,
      "learning_rate": 2.955e-05,
      "loss": 0.0031,
      "step": 61350
    },
    {
      "epoch": 3.2725333333333335,
      "grad_norm": 0.1413811594247818,
      "learning_rate": 2.9546666666666666e-05,
      "loss": 0.0028,
      "step": 61360
    },
    {
      "epoch": 3.273066666666667,
      "grad_norm": 7.525733813906754e-09,
      "learning_rate": 2.9543333333333336e-05,
      "loss": 0.0033,
      "step": 61370
    },
    {
      "epoch": 3.2736,
      "grad_norm": 0.28277087211608887,
      "learning_rate": 2.9540000000000002e-05,
      "loss": 0.0023,
      "step": 61380
    },
    {
      "epoch": 3.2741333333333333,
      "grad_norm": 0.08482640236616135,
      "learning_rate": 2.9536666666666668e-05,
      "loss": 0.0035,
      "step": 61390
    },
    {
      "epoch": 3.2746666666666666,
      "grad_norm": 0.02827766351401806,
      "learning_rate": 2.9533333333333334e-05,
      "loss": 0.0025,
      "step": 61400
    },
    {
      "epoch": 3.2752,
      "grad_norm": 0.08483227342367172,
      "learning_rate": 2.9530000000000004e-05,
      "loss": 0.0019,
      "step": 61410
    },
    {
      "epoch": 3.275733333333333,
      "grad_norm": 0.22621402144432068,
      "learning_rate": 2.952666666666667e-05,
      "loss": 0.0024,
      "step": 61420
    },
    {
      "epoch": 3.2762666666666664,
      "grad_norm": 0.14138157665729523,
      "learning_rate": 2.9523333333333336e-05,
      "loss": 0.0017,
      "step": 61430
    },
    {
      "epoch": 3.2768,
      "grad_norm": 2.7590860707249476e-09,
      "learning_rate": 2.9520000000000002e-05,
      "loss": 0.0042,
      "step": 61440
    },
    {
      "epoch": 3.2773333333333334,
      "grad_norm": 0.2544819414615631,
      "learning_rate": 2.951666666666667e-05,
      "loss": 0.0039,
      "step": 61450
    },
    {
      "epoch": 3.2778666666666667,
      "grad_norm": 0.452399343252182,
      "learning_rate": 2.9513333333333338e-05,
      "loss": 0.0029,
      "step": 61460
    },
    {
      "epoch": 3.2784,
      "grad_norm": 0.16966518759727478,
      "learning_rate": 2.951e-05,
      "loss": 0.0027,
      "step": 61470
    },
    {
      "epoch": 3.2789333333333333,
      "grad_norm": 0.1131095215678215,
      "learning_rate": 2.9506666666666667e-05,
      "loss": 0.003,
      "step": 61480
    },
    {
      "epoch": 3.2794666666666665,
      "grad_norm": 0.3675888478755951,
      "learning_rate": 2.9503333333333333e-05,
      "loss": 0.0025,
      "step": 61490
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 0.16966034471988678,
      "learning_rate": 2.95e-05,
      "loss": 0.0046,
      "step": 61500
    },
    {
      "epoch": 3.2805333333333335,
      "grad_norm": 0.2262091487646103,
      "learning_rate": 2.9496666666666668e-05,
      "loss": 0.0025,
      "step": 61510
    },
    {
      "epoch": 3.281066666666667,
      "grad_norm": 0.7127587795257568,
      "learning_rate": 2.9493333333333334e-05,
      "loss": 0.0076,
      "step": 61520
    },
    {
      "epoch": 3.2816,
      "grad_norm": 0.08482823520898819,
      "learning_rate": 2.949e-05,
      "loss": 0.0054,
      "step": 61530
    },
    {
      "epoch": 3.2821333333333333,
      "grad_norm": 0.1979188770055771,
      "learning_rate": 2.9486666666666667e-05,
      "loss": 0.0047,
      "step": 61540
    },
    {
      "epoch": 3.2826666666666666,
      "grad_norm": 0.28274407982826233,
      "learning_rate": 2.9483333333333336e-05,
      "loss": 0.0033,
      "step": 61550
    },
    {
      "epoch": 3.2832,
      "grad_norm": 0.05654822289943695,
      "learning_rate": 2.9480000000000002e-05,
      "loss": 0.0031,
      "step": 61560
    },
    {
      "epoch": 3.283733333333333,
      "grad_norm": 0.1413748860359192,
      "learning_rate": 2.947666666666667e-05,
      "loss": 0.0023,
      "step": 61570
    },
    {
      "epoch": 3.2842666666666664,
      "grad_norm": 0.08482962101697922,
      "learning_rate": 2.9473333333333335e-05,
      "loss": 0.0029,
      "step": 61580
    },
    {
      "epoch": 3.2848,
      "grad_norm": 0.16964535415172577,
      "learning_rate": 2.9470000000000004e-05,
      "loss": 0.0028,
      "step": 61590
    },
    {
      "epoch": 3.2853333333333334,
      "grad_norm": 0.1696547269821167,
      "learning_rate": 2.946666666666667e-05,
      "loss": 0.0035,
      "step": 61600
    },
    {
      "epoch": 3.2858666666666667,
      "grad_norm": 0.08482044190168381,
      "learning_rate": 2.9463333333333336e-05,
      "loss": 0.0042,
      "step": 61610
    },
    {
      "epoch": 3.2864,
      "grad_norm": 0.1696581244468689,
      "learning_rate": 2.946e-05,
      "loss": 0.0018,
      "step": 61620
    },
    {
      "epoch": 3.2869333333333333,
      "grad_norm": 0.08664396405220032,
      "learning_rate": 2.9456666666666665e-05,
      "loss": 0.0035,
      "step": 61630
    },
    {
      "epoch": 3.2874666666666665,
      "grad_norm": 0.14137515425682068,
      "learning_rate": 2.945333333333333e-05,
      "loss": 0.0032,
      "step": 61640
    },
    {
      "epoch": 3.288,
      "grad_norm": 0.22620020806789398,
      "learning_rate": 2.945e-05,
      "loss": 0.0035,
      "step": 61650
    },
    {
      "epoch": 3.2885333333333335,
      "grad_norm": 0.0848206877708435,
      "learning_rate": 2.9446666666666667e-05,
      "loss": 0.0023,
      "step": 61660
    },
    {
      "epoch": 3.289066666666667,
      "grad_norm": 1.823290074476347e-09,
      "learning_rate": 2.9443333333333333e-05,
      "loss": 0.0034,
      "step": 61670
    },
    {
      "epoch": 3.2896,
      "grad_norm": 0.3115769326686859,
      "learning_rate": 2.944e-05,
      "loss": 0.0034,
      "step": 61680
    },
    {
      "epoch": 3.2901333333333334,
      "grad_norm": 0.11309937387704849,
      "learning_rate": 2.943666666666667e-05,
      "loss": 0.003,
      "step": 61690
    },
    {
      "epoch": 3.2906666666666666,
      "grad_norm": 0.1131000965833664,
      "learning_rate": 2.9433333333333335e-05,
      "loss": 0.0035,
      "step": 61700
    },
    {
      "epoch": 3.2912,
      "grad_norm": 0.08482006192207336,
      "learning_rate": 2.943e-05,
      "loss": 0.0017,
      "step": 61710
    },
    {
      "epoch": 3.291733333333333,
      "grad_norm": 0.05654790252447128,
      "learning_rate": 2.942666666666667e-05,
      "loss": 0.0023,
      "step": 61720
    },
    {
      "epoch": 3.2922666666666665,
      "grad_norm": 0.22619856894016266,
      "learning_rate": 2.9423333333333337e-05,
      "loss": 0.0032,
      "step": 61730
    },
    {
      "epoch": 3.2928,
      "grad_norm": 0.08481896668672562,
      "learning_rate": 2.9420000000000003e-05,
      "loss": 0.0036,
      "step": 61740
    },
    {
      "epoch": 3.2933333333333334,
      "grad_norm": 0.19792914390563965,
      "learning_rate": 2.941666666666667e-05,
      "loss": 0.0037,
      "step": 61750
    },
    {
      "epoch": 3.2938666666666667,
      "grad_norm": 0.0848207175731659,
      "learning_rate": 2.941333333333334e-05,
      "loss": 0.0037,
      "step": 61760
    },
    {
      "epoch": 3.2944,
      "grad_norm": 0.31099483370780945,
      "learning_rate": 2.9409999999999998e-05,
      "loss": 0.004,
      "step": 61770
    },
    {
      "epoch": 3.2949333333333333,
      "grad_norm": 0.42410755157470703,
      "learning_rate": 2.9406666666666664e-05,
      "loss": 0.0039,
      "step": 61780
    },
    {
      "epoch": 3.2954666666666665,
      "grad_norm": 0.31101641058921814,
      "learning_rate": 2.9403333333333334e-05,
      "loss": 0.0047,
      "step": 61790
    },
    {
      "epoch": 3.296,
      "grad_norm": 0.14137183129787445,
      "learning_rate": 2.94e-05,
      "loss": 0.003,
      "step": 61800
    },
    {
      "epoch": 3.2965333333333335,
      "grad_norm": 0.5936902761459351,
      "learning_rate": 2.9396666666666666e-05,
      "loss": 0.0041,
      "step": 61810
    },
    {
      "epoch": 3.297066666666667,
      "grad_norm": 0.05654893070459366,
      "learning_rate": 2.9393333333333335e-05,
      "loss": 0.0043,
      "step": 61820
    },
    {
      "epoch": 3.2976,
      "grad_norm": 0.04170730337500572,
      "learning_rate": 2.939e-05,
      "loss": 0.0039,
      "step": 61830
    },
    {
      "epoch": 3.2981333333333334,
      "grad_norm": 0.2544638514518738,
      "learning_rate": 2.9386666666666668e-05,
      "loss": 0.0026,
      "step": 61840
    },
    {
      "epoch": 3.2986666666666666,
      "grad_norm": 0.16963405907154083,
      "learning_rate": 2.9383333333333334e-05,
      "loss": 0.003,
      "step": 61850
    },
    {
      "epoch": 3.2992,
      "grad_norm": 0.16963355243206024,
      "learning_rate": 2.9380000000000003e-05,
      "loss": 0.0031,
      "step": 61860
    },
    {
      "epoch": 3.299733333333333,
      "grad_norm": 0.2544451057910919,
      "learning_rate": 2.937666666666667e-05,
      "loss": 0.0042,
      "step": 61870
    },
    {
      "epoch": 3.3002666666666665,
      "grad_norm": 0.16963370144367218,
      "learning_rate": 2.9373333333333336e-05,
      "loss": 0.0036,
      "step": 61880
    },
    {
      "epoch": 3.3008,
      "grad_norm": 0.1978989541530609,
      "learning_rate": 2.9370000000000002e-05,
      "loss": 0.0037,
      "step": 61890
    },
    {
      "epoch": 3.3013333333333335,
      "grad_norm": 0.19790174067020416,
      "learning_rate": 2.936666666666667e-05,
      "loss": 0.0029,
      "step": 61900
    },
    {
      "epoch": 3.3018666666666667,
      "grad_norm": 0.16963186860084534,
      "learning_rate": 2.9363333333333337e-05,
      "loss": 0.0035,
      "step": 61910
    },
    {
      "epoch": 3.3024,
      "grad_norm": 0.05654502660036087,
      "learning_rate": 2.9360000000000003e-05,
      "loss": 0.0038,
      "step": 61920
    },
    {
      "epoch": 3.3029333333333333,
      "grad_norm": 0.056546539068222046,
      "learning_rate": 2.9356666666666666e-05,
      "loss": 0.0032,
      "step": 61930
    },
    {
      "epoch": 3.3034666666666666,
      "grad_norm": 0.2261820286512375,
      "learning_rate": 2.9353333333333332e-05,
      "loss": 0.0039,
      "step": 61940
    },
    {
      "epoch": 3.304,
      "grad_norm": 0.0321926511824131,
      "learning_rate": 2.935e-05,
      "loss": 0.0029,
      "step": 61950
    },
    {
      "epoch": 3.3045333333333335,
      "grad_norm": 0.33925655484199524,
      "learning_rate": 2.9346666666666668e-05,
      "loss": 0.0034,
      "step": 61960
    },
    {
      "epoch": 3.305066666666667,
      "grad_norm": 2.664309439737167e-09,
      "learning_rate": 2.9343333333333334e-05,
      "loss": 0.0024,
      "step": 61970
    },
    {
      "epoch": 3.3056,
      "grad_norm": 0.02827289327979088,
      "learning_rate": 2.934e-05,
      "loss": 0.0032,
      "step": 61980
    },
    {
      "epoch": 3.3061333333333334,
      "grad_norm": 0.028272047638893127,
      "learning_rate": 2.9336666666666666e-05,
      "loss": 0.0031,
      "step": 61990
    },
    {
      "epoch": 3.3066666666666666,
      "grad_norm": 0.19790148735046387,
      "learning_rate": 2.9333333333333336e-05,
      "loss": 0.0025,
      "step": 62000
    },
    {
      "epoch": 3.3072,
      "grad_norm": 0.3392753601074219,
      "learning_rate": 2.9330000000000002e-05,
      "loss": 0.0039,
      "step": 62010
    },
    {
      "epoch": 3.307733333333333,
      "grad_norm": 0.08481607586145401,
      "learning_rate": 2.9326666666666668e-05,
      "loss": 0.0027,
      "step": 62020
    },
    {
      "epoch": 3.3082666666666665,
      "grad_norm": 0.5088866353034973,
      "learning_rate": 2.9323333333333334e-05,
      "loss": 0.0023,
      "step": 62030
    },
    {
      "epoch": 3.3088,
      "grad_norm": 0.05654308944940567,
      "learning_rate": 2.9320000000000004e-05,
      "loss": 0.0041,
      "step": 62040
    },
    {
      "epoch": 3.3093333333333335,
      "grad_norm": 0.45233744382858276,
      "learning_rate": 2.931666666666667e-05,
      "loss": 0.0033,
      "step": 62050
    },
    {
      "epoch": 3.3098666666666667,
      "grad_norm": 0.25443577766418457,
      "learning_rate": 2.9313333333333336e-05,
      "loss": 0.0039,
      "step": 62060
    },
    {
      "epoch": 3.3104,
      "grad_norm": 0.22616475820541382,
      "learning_rate": 2.9310000000000006e-05,
      "loss": 0.0032,
      "step": 62070
    },
    {
      "epoch": 3.3109333333333333,
      "grad_norm": 0.28269803524017334,
      "learning_rate": 2.9306666666666665e-05,
      "loss": 0.003,
      "step": 62080
    },
    {
      "epoch": 3.3114666666666666,
      "grad_norm": 0.05654534697532654,
      "learning_rate": 2.930333333333333e-05,
      "loss": 0.0021,
      "step": 62090
    },
    {
      "epoch": 3.312,
      "grad_norm": 0.08481229841709137,
      "learning_rate": 2.93e-05,
      "loss": 0.0035,
      "step": 62100
    },
    {
      "epoch": 3.3125333333333336,
      "grad_norm": 0.169619619846344,
      "learning_rate": 2.9296666666666667e-05,
      "loss": 0.0031,
      "step": 62110
    },
    {
      "epoch": 3.313066666666667,
      "grad_norm": 0.028269818052649498,
      "learning_rate": 2.9293333333333333e-05,
      "loss": 0.0018,
      "step": 62120
    },
    {
      "epoch": 3.3136,
      "grad_norm": 0.11307786405086517,
      "learning_rate": 2.929e-05,
      "loss": 0.002,
      "step": 62130
    },
    {
      "epoch": 3.3141333333333334,
      "grad_norm": 0.6785245537757874,
      "learning_rate": 2.928666666666667e-05,
      "loss": 0.0035,
      "step": 62140
    },
    {
      "epoch": 3.3146666666666667,
      "grad_norm": 0.0848078802227974,
      "learning_rate": 2.9283333333333335e-05,
      "loss": 0.0041,
      "step": 62150
    },
    {
      "epoch": 3.3152,
      "grad_norm": 0.11308286339044571,
      "learning_rate": 2.928e-05,
      "loss": 0.0021,
      "step": 62160
    },
    {
      "epoch": 3.315733333333333,
      "grad_norm": 0.36870548129081726,
      "learning_rate": 2.927666666666667e-05,
      "loss": 0.0036,
      "step": 62170
    },
    {
      "epoch": 3.3162666666666665,
      "grad_norm": 0.11307964473962784,
      "learning_rate": 2.9273333333333337e-05,
      "loss": 0.0023,
      "step": 62180
    },
    {
      "epoch": 3.3168,
      "grad_norm": 0.3392447829246521,
      "learning_rate": 2.9270000000000003e-05,
      "loss": 0.004,
      "step": 62190
    },
    {
      "epoch": 3.3173333333333335,
      "grad_norm": 0.14135459065437317,
      "learning_rate": 2.926666666666667e-05,
      "loss": 0.0022,
      "step": 62200
    },
    {
      "epoch": 3.3178666666666667,
      "grad_norm": 0.395768940448761,
      "learning_rate": 2.926333333333334e-05,
      "loss": 0.0045,
      "step": 62210
    },
    {
      "epoch": 3.3184,
      "grad_norm": 0.028269929811358452,
      "learning_rate": 2.9260000000000004e-05,
      "loss": 0.0037,
      "step": 62220
    },
    {
      "epoch": 3.3189333333333333,
      "grad_norm": 0.3675241470336914,
      "learning_rate": 2.9256666666666667e-05,
      "loss": 0.0031,
      "step": 62230
    },
    {
      "epoch": 3.3194666666666666,
      "grad_norm": 3.745477261674068e-09,
      "learning_rate": 2.9253333333333333e-05,
      "loss": 0.0026,
      "step": 62240
    },
    {
      "epoch": 3.32,
      "grad_norm": 0.19790607690811157,
      "learning_rate": 2.925e-05,
      "loss": 0.0028,
      "step": 62250
    },
    {
      "epoch": 3.3205333333333336,
      "grad_norm": 0.39575520157814026,
      "learning_rate": 2.9246666666666666e-05,
      "loss": 0.0028,
      "step": 62260
    },
    {
      "epoch": 3.321066666666667,
      "grad_norm": 2.90145396597552e-09,
      "learning_rate": 2.9243333333333335e-05,
      "loss": 0.0039,
      "step": 62270
    },
    {
      "epoch": 3.3216,
      "grad_norm": 0.028268588706851006,
      "learning_rate": 2.924e-05,
      "loss": 0.0036,
      "step": 62280
    },
    {
      "epoch": 3.3221333333333334,
      "grad_norm": 4.2457806159745815e-09,
      "learning_rate": 2.9236666666666667e-05,
      "loss": 0.0033,
      "step": 62290
    },
    {
      "epoch": 3.3226666666666667,
      "grad_norm": 0.33921682834625244,
      "learning_rate": 2.9233333333333334e-05,
      "loss": 0.0034,
      "step": 62300
    },
    {
      "epoch": 3.3232,
      "grad_norm": 0.0565374381840229,
      "learning_rate": 2.9230000000000003e-05,
      "loss": 0.0031,
      "step": 62310
    },
    {
      "epoch": 3.323733333333333,
      "grad_norm": 0.4805445969104767,
      "learning_rate": 2.922666666666667e-05,
      "loss": 0.0032,
      "step": 62320
    },
    {
      "epoch": 3.3242666666666665,
      "grad_norm": 0.226164773106575,
      "learning_rate": 2.9223333333333335e-05,
      "loss": 0.0033,
      "step": 62330
    },
    {
      "epoch": 3.3247999999999998,
      "grad_norm": 0.31094449758529663,
      "learning_rate": 2.922e-05,
      "loss": 0.0022,
      "step": 62340
    },
    {
      "epoch": 3.3253333333333335,
      "grad_norm": 0.39577406644821167,
      "learning_rate": 2.921666666666667e-05,
      "loss": 0.0037,
      "step": 62350
    },
    {
      "epoch": 3.3258666666666667,
      "grad_norm": 0.1130698174238205,
      "learning_rate": 2.9213333333333337e-05,
      "loss": 0.0039,
      "step": 62360
    },
    {
      "epoch": 3.3264,
      "grad_norm": 0.14134037494659424,
      "learning_rate": 2.9210000000000003e-05,
      "loss": 0.0028,
      "step": 62370
    },
    {
      "epoch": 3.3269333333333333,
      "grad_norm": 0.16960734128952026,
      "learning_rate": 2.9206666666666666e-05,
      "loss": 0.0028,
      "step": 62380
    },
    {
      "epoch": 3.3274666666666666,
      "grad_norm": 0.02826751209795475,
      "learning_rate": 2.9203333333333332e-05,
      "loss": 0.0032,
      "step": 62390
    },
    {
      "epoch": 3.328,
      "grad_norm": 0.084803007543087,
      "learning_rate": 2.9199999999999998e-05,
      "loss": 0.003,
      "step": 62400
    },
    {
      "epoch": 3.3285333333333336,
      "grad_norm": 0.16960826516151428,
      "learning_rate": 2.9196666666666668e-05,
      "loss": 0.0043,
      "step": 62410
    },
    {
      "epoch": 3.329066666666667,
      "grad_norm": 0.028267282992601395,
      "learning_rate": 2.9193333333333334e-05,
      "loss": 0.0026,
      "step": 62420
    },
    {
      "epoch": 3.3296,
      "grad_norm": 0.22614575922489166,
      "learning_rate": 2.919e-05,
      "loss": 0.0042,
      "step": 62430
    },
    {
      "epoch": 3.3301333333333334,
      "grad_norm": 0.028267497196793556,
      "learning_rate": 2.9186666666666666e-05,
      "loss": 0.0037,
      "step": 62440
    },
    {
      "epoch": 3.3306666666666667,
      "grad_norm": 0.2261383980512619,
      "learning_rate": 2.9183333333333336e-05,
      "loss": 0.0044,
      "step": 62450
    },
    {
      "epoch": 3.3312,
      "grad_norm": 0.22614052891731262,
      "learning_rate": 2.9180000000000002e-05,
      "loss": 0.0047,
      "step": 62460
    },
    {
      "epoch": 3.331733333333333,
      "grad_norm": 0.08480207622051239,
      "learning_rate": 2.9176666666666668e-05,
      "loss": 0.0045,
      "step": 62470
    },
    {
      "epoch": 3.3322666666666665,
      "grad_norm": 0.11307605355978012,
      "learning_rate": 2.9173333333333337e-05,
      "loss": 0.0026,
      "step": 62480
    },
    {
      "epoch": 3.3327999999999998,
      "grad_norm": 0.22613559663295746,
      "learning_rate": 2.9170000000000004e-05,
      "loss": 0.0023,
      "step": 62490
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.056534819304943085,
      "learning_rate": 2.916666666666667e-05,
      "loss": 0.0028,
      "step": 62500
    },
    {
      "epoch": 3.3338666666666668,
      "grad_norm": 0.02826617658138275,
      "learning_rate": 2.9163333333333336e-05,
      "loss": 0.0027,
      "step": 62510
    },
    {
      "epoch": 3.3344,
      "grad_norm": 0.22613102197647095,
      "learning_rate": 2.9160000000000005e-05,
      "loss": 0.0031,
      "step": 62520
    },
    {
      "epoch": 3.3349333333333333,
      "grad_norm": 4.098002381880406e-09,
      "learning_rate": 2.9156666666666665e-05,
      "loss": 0.0029,
      "step": 62530
    },
    {
      "epoch": 3.3354666666666666,
      "grad_norm": 0.056532274931669235,
      "learning_rate": 2.915333333333333e-05,
      "loss": 0.0028,
      "step": 62540
    },
    {
      "epoch": 3.336,
      "grad_norm": 0.05653620511293411,
      "learning_rate": 2.915e-05,
      "loss": 0.0031,
      "step": 62550
    },
    {
      "epoch": 3.3365333333333336,
      "grad_norm": 0.3391827940940857,
      "learning_rate": 2.9146666666666667e-05,
      "loss": 0.003,
      "step": 62560
    },
    {
      "epoch": 3.337066666666667,
      "grad_norm": 0.45229458808898926,
      "learning_rate": 2.9143333333333333e-05,
      "loss": 0.0034,
      "step": 62570
    },
    {
      "epoch": 3.3376,
      "grad_norm": 0.22613202035427094,
      "learning_rate": 2.9140000000000002e-05,
      "loss": 0.0031,
      "step": 62580
    },
    {
      "epoch": 3.3381333333333334,
      "grad_norm": 0.25438928604125977,
      "learning_rate": 2.913666666666667e-05,
      "loss": 0.003,
      "step": 62590
    },
    {
      "epoch": 3.3386666666666667,
      "grad_norm": 0.0282661821693182,
      "learning_rate": 2.9133333333333334e-05,
      "loss": 0.0026,
      "step": 62600
    },
    {
      "epoch": 3.3392,
      "grad_norm": 0.19786866009235382,
      "learning_rate": 2.913e-05,
      "loss": 0.0022,
      "step": 62610
    },
    {
      "epoch": 3.339733333333333,
      "grad_norm": 0.1413302719593048,
      "learning_rate": 2.912666666666667e-05,
      "loss": 0.0034,
      "step": 62620
    },
    {
      "epoch": 3.3402666666666665,
      "grad_norm": 0.2543816566467285,
      "learning_rate": 2.9123333333333336e-05,
      "loss": 0.0041,
      "step": 62630
    },
    {
      "epoch": 3.3407999999999998,
      "grad_norm": 0.05653155595064163,
      "learning_rate": 2.9120000000000002e-05,
      "loss": 0.004,
      "step": 62640
    },
    {
      "epoch": 3.3413333333333335,
      "grad_norm": 0.11306972056627274,
      "learning_rate": 2.911666666666667e-05,
      "loss": 0.0032,
      "step": 62650
    },
    {
      "epoch": 3.3418666666666668,
      "grad_norm": 0.0847981721162796,
      "learning_rate": 2.9113333333333338e-05,
      "loss": 0.0042,
      "step": 62660
    },
    {
      "epoch": 3.3424,
      "grad_norm": 0.3957156836986542,
      "learning_rate": 2.9110000000000004e-05,
      "loss": 0.002,
      "step": 62670
    },
    {
      "epoch": 3.3429333333333333,
      "grad_norm": 0.1130625307559967,
      "learning_rate": 2.9106666666666667e-05,
      "loss": 0.0039,
      "step": 62680
    },
    {
      "epoch": 3.3434666666666666,
      "grad_norm": 0.39573991298675537,
      "learning_rate": 2.9103333333333333e-05,
      "loss": 0.003,
      "step": 62690
    },
    {
      "epoch": 3.344,
      "grad_norm": 0.028264617547392845,
      "learning_rate": 2.91e-05,
      "loss": 0.0042,
      "step": 62700
    },
    {
      "epoch": 3.3445333333333336,
      "grad_norm": 0.08479718118906021,
      "learning_rate": 2.9096666666666665e-05,
      "loss": 0.0045,
      "step": 62710
    },
    {
      "epoch": 3.345066666666667,
      "grad_norm": 0.19785110652446747,
      "learning_rate": 2.9093333333333335e-05,
      "loss": 0.0031,
      "step": 62720
    },
    {
      "epoch": 3.3456,
      "grad_norm": 0.056533683091402054,
      "learning_rate": 2.909e-05,
      "loss": 0.0035,
      "step": 62730
    },
    {
      "epoch": 3.3461333333333334,
      "grad_norm": 0.3956865966320038,
      "learning_rate": 2.9086666666666667e-05,
      "loss": 0.0036,
      "step": 62740
    },
    {
      "epoch": 3.3466666666666667,
      "grad_norm": 0.28266674280166626,
      "learning_rate": 2.9083333333333333e-05,
      "loss": 0.0045,
      "step": 62750
    },
    {
      "epoch": 3.3472,
      "grad_norm": 0.08479480445384979,
      "learning_rate": 2.9080000000000003e-05,
      "loss": 0.0039,
      "step": 62760
    },
    {
      "epoch": 3.3477333333333332,
      "grad_norm": 0.0565272681415081,
      "learning_rate": 2.907666666666667e-05,
      "loss": 0.0023,
      "step": 62770
    },
    {
      "epoch": 3.3482666666666665,
      "grad_norm": 0.16958566009998322,
      "learning_rate": 2.9073333333333335e-05,
      "loss": 0.0036,
      "step": 62780
    },
    {
      "epoch": 3.3487999999999998,
      "grad_norm": 0.11305777728557587,
      "learning_rate": 2.907e-05,
      "loss": 0.0036,
      "step": 62790
    },
    {
      "epoch": 3.3493333333333335,
      "grad_norm": 0.14132100343704224,
      "learning_rate": 2.906666666666667e-05,
      "loss": 0.0047,
      "step": 62800
    },
    {
      "epoch": 3.3498666666666668,
      "grad_norm": 0.11305824667215347,
      "learning_rate": 2.9063333333333337e-05,
      "loss": 0.0027,
      "step": 62810
    },
    {
      "epoch": 3.3504,
      "grad_norm": 0.2543705403804779,
      "learning_rate": 2.9060000000000003e-05,
      "loss": 0.0031,
      "step": 62820
    },
    {
      "epoch": 3.3509333333333333,
      "grad_norm": 0.11305991560220718,
      "learning_rate": 2.9056666666666666e-05,
      "loss": 0.0036,
      "step": 62830
    },
    {
      "epoch": 3.3514666666666666,
      "grad_norm": 0.14131934940814972,
      "learning_rate": 2.9053333333333332e-05,
      "loss": 0.0057,
      "step": 62840
    },
    {
      "epoch": 3.352,
      "grad_norm": 0.25437724590301514,
      "learning_rate": 2.9049999999999998e-05,
      "loss": 0.0042,
      "step": 62850
    },
    {
      "epoch": 3.352533333333333,
      "grad_norm": 2.0962529490020643e-09,
      "learning_rate": 2.9046666666666668e-05,
      "loss": 0.0037,
      "step": 62860
    },
    {
      "epoch": 3.353066666666667,
      "grad_norm": 0.254377543926239,
      "learning_rate": 2.9043333333333334e-05,
      "loss": 0.0031,
      "step": 62870
    },
    {
      "epoch": 3.3536,
      "grad_norm": 0.22611218690872192,
      "learning_rate": 2.904e-05,
      "loss": 0.0029,
      "step": 62880
    },
    {
      "epoch": 3.3541333333333334,
      "grad_norm": 0.14131510257720947,
      "learning_rate": 2.9036666666666666e-05,
      "loss": 0.0029,
      "step": 62890
    },
    {
      "epoch": 3.3546666666666667,
      "grad_norm": 0.028264151886105537,
      "learning_rate": 2.9033333333333335e-05,
      "loss": 0.0035,
      "step": 62900
    },
    {
      "epoch": 3.3552,
      "grad_norm": 0.0847899541258812,
      "learning_rate": 2.903e-05,
      "loss": 0.0043,
      "step": 62910
    },
    {
      "epoch": 3.3557333333333332,
      "grad_norm": 0.028262874111533165,
      "learning_rate": 2.9026666666666668e-05,
      "loss": 0.0031,
      "step": 62920
    },
    {
      "epoch": 3.3562666666666665,
      "grad_norm": 0.14132316410541534,
      "learning_rate": 2.9023333333333337e-05,
      "loss": 0.0033,
      "step": 62930
    },
    {
      "epoch": 3.3568,
      "grad_norm": 0.2543618977069855,
      "learning_rate": 2.9020000000000003e-05,
      "loss": 0.0043,
      "step": 62940
    },
    {
      "epoch": 3.3573333333333335,
      "grad_norm": 0.16959208250045776,
      "learning_rate": 2.901666666666667e-05,
      "loss": 0.0034,
      "step": 62950
    },
    {
      "epoch": 3.3578666666666668,
      "grad_norm": 0.14179496467113495,
      "learning_rate": 2.9013333333333336e-05,
      "loss": 0.0027,
      "step": 62960
    },
    {
      "epoch": 3.3584,
      "grad_norm": 0.08478912711143494,
      "learning_rate": 2.9010000000000005e-05,
      "loss": 0.0032,
      "step": 62970
    },
    {
      "epoch": 3.3589333333333333,
      "grad_norm": 0.11305511742830276,
      "learning_rate": 2.9006666666666665e-05,
      "loss": 0.0027,
      "step": 62980
    },
    {
      "epoch": 3.3594666666666666,
      "grad_norm": 0.028263743966817856,
      "learning_rate": 2.9003333333333334e-05,
      "loss": 0.0024,
      "step": 62990
    },
    {
      "epoch": 3.36,
      "grad_norm": 0.0565260648727417,
      "learning_rate": 2.9e-05,
      "loss": 0.0026,
      "step": 63000
    },
    {
      "epoch": 3.360533333333333,
      "grad_norm": 0.22610844671726227,
      "learning_rate": 2.8996666666666666e-05,
      "loss": 0.0034,
      "step": 63010
    },
    {
      "epoch": 3.361066666666667,
      "grad_norm": 0.1978413462638855,
      "learning_rate": 2.8993333333333332e-05,
      "loss": 0.003,
      "step": 63020
    },
    {
      "epoch": 3.3616,
      "grad_norm": 0.11305166780948639,
      "learning_rate": 2.8990000000000002e-05,
      "loss": 0.0027,
      "step": 63030
    },
    {
      "epoch": 3.3621333333333334,
      "grad_norm": 0.028263362124562263,
      "learning_rate": 2.8986666666666668e-05,
      "loss": 0.0039,
      "step": 63040
    },
    {
      "epoch": 3.3626666666666667,
      "grad_norm": 3.777280710437481e-09,
      "learning_rate": 2.8983333333333334e-05,
      "loss": 0.0035,
      "step": 63050
    },
    {
      "epoch": 3.3632,
      "grad_norm": 0.0847872942686081,
      "learning_rate": 2.898e-05,
      "loss": 0.0036,
      "step": 63060
    },
    {
      "epoch": 3.3637333333333332,
      "grad_norm": 0.2543601393699646,
      "learning_rate": 2.897666666666667e-05,
      "loss": 0.0037,
      "step": 63070
    },
    {
      "epoch": 3.3642666666666665,
      "grad_norm": 0.11305497586727142,
      "learning_rate": 2.8973333333333336e-05,
      "loss": 0.0032,
      "step": 63080
    },
    {
      "epoch": 3.3648,
      "grad_norm": 0.25446993112564087,
      "learning_rate": 2.8970000000000002e-05,
      "loss": 0.0033,
      "step": 63090
    },
    {
      "epoch": 3.3653333333333335,
      "grad_norm": 0.2543739676475525,
      "learning_rate": 2.8966666666666668e-05,
      "loss": 0.0031,
      "step": 63100
    },
    {
      "epoch": 3.365866666666667,
      "grad_norm": 0.08660638332366943,
      "learning_rate": 2.8963333333333338e-05,
      "loss": 0.0031,
      "step": 63110
    },
    {
      "epoch": 3.3664,
      "grad_norm": 0.5370044112205505,
      "learning_rate": 2.8960000000000004e-05,
      "loss": 0.0033,
      "step": 63120
    },
    {
      "epoch": 3.3669333333333333,
      "grad_norm": 0.25435546040534973,
      "learning_rate": 2.895666666666667e-05,
      "loss": 0.0019,
      "step": 63130
    },
    {
      "epoch": 3.3674666666666666,
      "grad_norm": 0.028262360021471977,
      "learning_rate": 2.8953333333333333e-05,
      "loss": 0.0032,
      "step": 63140
    },
    {
      "epoch": 3.368,
      "grad_norm": 0.02826189436018467,
      "learning_rate": 2.895e-05,
      "loss": 0.0023,
      "step": 63150
    },
    {
      "epoch": 3.368533333333333,
      "grad_norm": 0.14131569862365723,
      "learning_rate": 2.8946666666666665e-05,
      "loss": 0.0036,
      "step": 63160
    },
    {
      "epoch": 3.369066666666667,
      "grad_norm": 0.11304598301649094,
      "learning_rate": 2.8943333333333335e-05,
      "loss": 0.0037,
      "step": 63170
    },
    {
      "epoch": 3.3696,
      "grad_norm": 0.11305856704711914,
      "learning_rate": 2.894e-05,
      "loss": 0.004,
      "step": 63180
    },
    {
      "epoch": 3.3701333333333334,
      "grad_norm": 0.22609351575374603,
      "learning_rate": 2.8936666666666667e-05,
      "loss": 0.0027,
      "step": 63190
    },
    {
      "epoch": 3.3706666666666667,
      "grad_norm": 0.06141938641667366,
      "learning_rate": 2.8933333333333333e-05,
      "loss": 0.0028,
      "step": 63200
    },
    {
      "epoch": 3.3712,
      "grad_norm": 0.19782760739326477,
      "learning_rate": 2.8930000000000003e-05,
      "loss": 0.0026,
      "step": 63210
    },
    {
      "epoch": 3.3717333333333332,
      "grad_norm": 0.1695796698331833,
      "learning_rate": 2.892666666666667e-05,
      "loss": 0.0026,
      "step": 63220
    },
    {
      "epoch": 3.3722666666666665,
      "grad_norm": 0.0565251000225544,
      "learning_rate": 2.8923333333333335e-05,
      "loss": 0.0028,
      "step": 63230
    },
    {
      "epoch": 3.3728,
      "grad_norm": 0.05652230232954025,
      "learning_rate": 2.8920000000000004e-05,
      "loss": 0.0033,
      "step": 63240
    },
    {
      "epoch": 3.3733333333333335,
      "grad_norm": 0.0565248504281044,
      "learning_rate": 2.891666666666667e-05,
      "loss": 0.0024,
      "step": 63250
    },
    {
      "epoch": 3.373866666666667,
      "grad_norm": 0.45221182703971863,
      "learning_rate": 2.8913333333333337e-05,
      "loss": 0.0037,
      "step": 63260
    },
    {
      "epoch": 3.3744,
      "grad_norm": 0.3115489184856415,
      "learning_rate": 2.8910000000000003e-05,
      "loss": 0.0031,
      "step": 63270
    },
    {
      "epoch": 3.3749333333333333,
      "grad_norm": 0.037104327231645584,
      "learning_rate": 2.8906666666666672e-05,
      "loss": 0.0034,
      "step": 63280
    },
    {
      "epoch": 3.3754666666666666,
      "grad_norm": 0.14131616055965424,
      "learning_rate": 2.890333333333333e-05,
      "loss": 0.0033,
      "step": 63290
    },
    {
      "epoch": 3.376,
      "grad_norm": 0.08604293316602707,
      "learning_rate": 2.8899999999999998e-05,
      "loss": 0.0024,
      "step": 63300
    },
    {
      "epoch": 3.376533333333333,
      "grad_norm": 0.08478821069002151,
      "learning_rate": 2.8896666666666667e-05,
      "loss": 0.003,
      "step": 63310
    },
    {
      "epoch": 3.377066666666667,
      "grad_norm": 0.339150607585907,
      "learning_rate": 2.8893333333333333e-05,
      "loss": 0.0034,
      "step": 63320
    },
    {
      "epoch": 3.3776,
      "grad_norm": 0.14221566915512085,
      "learning_rate": 2.889e-05,
      "loss": 0.003,
      "step": 63330
    },
    {
      "epoch": 3.3781333333333334,
      "grad_norm": 0.05652749165892601,
      "learning_rate": 2.888666666666667e-05,
      "loss": 0.0035,
      "step": 63340
    },
    {
      "epoch": 3.3786666666666667,
      "grad_norm": 0.25434410572052,
      "learning_rate": 2.8883333333333335e-05,
      "loss": 0.003,
      "step": 63350
    },
    {
      "epoch": 3.3792,
      "grad_norm": 0.31088703870773315,
      "learning_rate": 2.888e-05,
      "loss": 0.0028,
      "step": 63360
    },
    {
      "epoch": 3.3797333333333333,
      "grad_norm": 0.17157655954360962,
      "learning_rate": 2.8876666666666667e-05,
      "loss": 0.0037,
      "step": 63370
    },
    {
      "epoch": 3.3802666666666665,
      "grad_norm": 0.08492209762334824,
      "learning_rate": 2.8873333333333337e-05,
      "loss": 0.0028,
      "step": 63380
    },
    {
      "epoch": 3.3808,
      "grad_norm": 0.25434014201164246,
      "learning_rate": 2.8870000000000003e-05,
      "loss": 0.0034,
      "step": 63390
    },
    {
      "epoch": 3.3813333333333335,
      "grad_norm": 0.19783668220043182,
      "learning_rate": 2.886666666666667e-05,
      "loss": 0.0032,
      "step": 63400
    },
    {
      "epoch": 3.381866666666667,
      "grad_norm": 0.593434751033783,
      "learning_rate": 2.8863333333333335e-05,
      "loss": 0.0037,
      "step": 63410
    },
    {
      "epoch": 3.3824,
      "grad_norm": 0.11304939538240433,
      "learning_rate": 2.8860000000000005e-05,
      "loss": 0.0029,
      "step": 63420
    },
    {
      "epoch": 3.3829333333333333,
      "grad_norm": 0.08477608859539032,
      "learning_rate": 2.885666666666667e-05,
      "loss": 0.0017,
      "step": 63430
    },
    {
      "epoch": 3.3834666666666666,
      "grad_norm": 0.3674042522907257,
      "learning_rate": 2.8853333333333334e-05,
      "loss": 0.0024,
      "step": 63440
    },
    {
      "epoch": 3.384,
      "grad_norm": 0.28259047865867615,
      "learning_rate": 2.885e-05,
      "loss": 0.003,
      "step": 63450
    },
    {
      "epoch": 3.384533333333333,
      "grad_norm": 0.14129990339279175,
      "learning_rate": 2.8846666666666666e-05,
      "loss": 0.0029,
      "step": 63460
    },
    {
      "epoch": 3.385066666666667,
      "grad_norm": 0.19782309234142303,
      "learning_rate": 2.8843333333333332e-05,
      "loss": 0.003,
      "step": 63470
    },
    {
      "epoch": 3.3856,
      "grad_norm": 0.2750672399997711,
      "learning_rate": 2.8840000000000002e-05,
      "loss": 0.0029,
      "step": 63480
    },
    {
      "epoch": 3.3861333333333334,
      "grad_norm": 0.19782976806163788,
      "learning_rate": 2.8836666666666668e-05,
      "loss": 0.0037,
      "step": 63490
    },
    {
      "epoch": 3.3866666666666667,
      "grad_norm": 0.31084391474723816,
      "learning_rate": 2.8833333333333334e-05,
      "loss": 0.0041,
      "step": 63500
    },
    {
      "epoch": 3.3872,
      "grad_norm": 0.05652177333831787,
      "learning_rate": 2.883e-05,
      "loss": 0.0035,
      "step": 63510
    },
    {
      "epoch": 3.3877333333333333,
      "grad_norm": 0.4526975452899933,
      "learning_rate": 2.882666666666667e-05,
      "loss": 0.0037,
      "step": 63520
    },
    {
      "epoch": 3.3882666666666665,
      "grad_norm": 0.05652143806219101,
      "learning_rate": 2.8823333333333336e-05,
      "loss": 0.0037,
      "step": 63530
    },
    {
      "epoch": 3.3888,
      "grad_norm": 0.19781233370304108,
      "learning_rate": 2.8820000000000002e-05,
      "loss": 0.0026,
      "step": 63540
    },
    {
      "epoch": 3.389333333333333,
      "grad_norm": 0.19892176985740662,
      "learning_rate": 2.8816666666666668e-05,
      "loss": 0.0024,
      "step": 63550
    },
    {
      "epoch": 3.389866666666667,
      "grad_norm": 0.11303828656673431,
      "learning_rate": 2.8813333333333338e-05,
      "loss": 0.0028,
      "step": 63560
    },
    {
      "epoch": 3.3904,
      "grad_norm": 0.11303362250328064,
      "learning_rate": 2.8810000000000004e-05,
      "loss": 0.0035,
      "step": 63570
    },
    {
      "epoch": 3.3909333333333334,
      "grad_norm": 0.19780848920345306,
      "learning_rate": 2.880666666666667e-05,
      "loss": 0.0023,
      "step": 63580
    },
    {
      "epoch": 3.3914666666666666,
      "grad_norm": 0.05651674047112465,
      "learning_rate": 2.8803333333333333e-05,
      "loss": 0.002,
      "step": 63590
    },
    {
      "epoch": 3.392,
      "grad_norm": 0.028257910162210464,
      "learning_rate": 2.88e-05,
      "loss": 0.0029,
      "step": 63600
    },
    {
      "epoch": 3.392533333333333,
      "grad_norm": 0.22607599198818207,
      "learning_rate": 2.8796666666666665e-05,
      "loss": 0.003,
      "step": 63610
    },
    {
      "epoch": 3.393066666666667,
      "grad_norm": 0.3390827178955078,
      "learning_rate": 2.8793333333333334e-05,
      "loss": 0.0029,
      "step": 63620
    },
    {
      "epoch": 3.3936,
      "grad_norm": 0.11304113268852234,
      "learning_rate": 2.879e-05,
      "loss": 0.0026,
      "step": 63630
    },
    {
      "epoch": 3.3941333333333334,
      "grad_norm": 0.14128603041172028,
      "learning_rate": 2.8786666666666667e-05,
      "loss": 0.0026,
      "step": 63640
    },
    {
      "epoch": 3.3946666666666667,
      "grad_norm": 0.1412910670042038,
      "learning_rate": 2.8783333333333333e-05,
      "loss": 0.0026,
      "step": 63650
    },
    {
      "epoch": 3.3952,
      "grad_norm": 0.16954442858695984,
      "learning_rate": 2.8780000000000002e-05,
      "loss": 0.0024,
      "step": 63660
    },
    {
      "epoch": 3.3957333333333333,
      "grad_norm": 0.08477307856082916,
      "learning_rate": 2.877666666666667e-05,
      "loss": 0.0018,
      "step": 63670
    },
    {
      "epoch": 3.3962666666666665,
      "grad_norm": 0.028257397934794426,
      "learning_rate": 2.8773333333333335e-05,
      "loss": 0.0029,
      "step": 63680
    },
    {
      "epoch": 3.3968,
      "grad_norm": 0.14129069447517395,
      "learning_rate": 2.8770000000000004e-05,
      "loss": 0.0027,
      "step": 63690
    },
    {
      "epoch": 3.397333333333333,
      "grad_norm": 0.028258007019758224,
      "learning_rate": 2.876666666666667e-05,
      "loss": 0.0021,
      "step": 63700
    },
    {
      "epoch": 3.397866666666667,
      "grad_norm": 0.028257131576538086,
      "learning_rate": 2.8763333333333336e-05,
      "loss": 0.0015,
      "step": 63710
    },
    {
      "epoch": 3.3984,
      "grad_norm": 0.226072758436203,
      "learning_rate": 2.8760000000000002e-05,
      "loss": 0.004,
      "step": 63720
    },
    {
      "epoch": 3.3989333333333334,
      "grad_norm": 0.028257597237825394,
      "learning_rate": 2.8756666666666672e-05,
      "loss": 0.0033,
      "step": 63730
    },
    {
      "epoch": 3.3994666666666666,
      "grad_norm": 0.042685654014348984,
      "learning_rate": 2.875333333333333e-05,
      "loss": 0.0026,
      "step": 63740
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.028258075937628746,
      "learning_rate": 2.8749999999999997e-05,
      "loss": 0.0028,
      "step": 63750
    },
    {
      "epoch": 3.400533333333333,
      "grad_norm": 0.08477412164211273,
      "learning_rate": 2.8746666666666667e-05,
      "loss": 0.0024,
      "step": 63760
    },
    {
      "epoch": 3.401066666666667,
      "grad_norm": 0.197795569896698,
      "learning_rate": 2.8743333333333333e-05,
      "loss": 0.0029,
      "step": 63770
    },
    {
      "epoch": 3.4016,
      "grad_norm": 0.25431323051452637,
      "learning_rate": 2.874e-05,
      "loss": 0.0029,
      "step": 63780
    },
    {
      "epoch": 3.4021333333333335,
      "grad_norm": 0.33908841013908386,
      "learning_rate": 2.873666666666667e-05,
      "loss": 0.0039,
      "step": 63790
    },
    {
      "epoch": 3.4026666666666667,
      "grad_norm": 0.5368555784225464,
      "learning_rate": 2.8733333333333335e-05,
      "loss": 0.0028,
      "step": 63800
    },
    {
      "epoch": 3.4032,
      "grad_norm": 0.11303526908159256,
      "learning_rate": 2.873e-05,
      "loss": 0.003,
      "step": 63810
    },
    {
      "epoch": 3.4037333333333333,
      "grad_norm": 0.2543070912361145,
      "learning_rate": 2.8726666666666667e-05,
      "loss": 0.0034,
      "step": 63820
    },
    {
      "epoch": 3.4042666666666666,
      "grad_norm": 0.1130266785621643,
      "learning_rate": 2.8723333333333337e-05,
      "loss": 0.0022,
      "step": 63830
    },
    {
      "epoch": 3.4048,
      "grad_norm": 0.19780641794204712,
      "learning_rate": 2.8720000000000003e-05,
      "loss": 0.0036,
      "step": 63840
    },
    {
      "epoch": 3.405333333333333,
      "grad_norm": 0.08477642387151718,
      "learning_rate": 2.871666666666667e-05,
      "loss": 0.0035,
      "step": 63850
    },
    {
      "epoch": 3.405866666666667,
      "grad_norm": 0.11302416771650314,
      "learning_rate": 2.8713333333333335e-05,
      "loss": 0.0038,
      "step": 63860
    },
    {
      "epoch": 3.4064,
      "grad_norm": 2.1128743199483324e-09,
      "learning_rate": 2.8710000000000005e-05,
      "loss": 0.0015,
      "step": 63870
    },
    {
      "epoch": 3.4069333333333334,
      "grad_norm": 0.1695379614830017,
      "learning_rate": 2.870666666666667e-05,
      "loss": 0.0023,
      "step": 63880
    },
    {
      "epoch": 3.4074666666666666,
      "grad_norm": 0.16953638195991516,
      "learning_rate": 2.8703333333333334e-05,
      "loss": 0.005,
      "step": 63890
    },
    {
      "epoch": 3.408,
      "grad_norm": 0.14129073917865753,
      "learning_rate": 2.87e-05,
      "loss": 0.0029,
      "step": 63900
    },
    {
      "epoch": 3.408533333333333,
      "grad_norm": 0.33905649185180664,
      "learning_rate": 2.8696666666666666e-05,
      "loss": 0.0027,
      "step": 63910
    },
    {
      "epoch": 3.409066666666667,
      "grad_norm": 0.6782260537147522,
      "learning_rate": 2.8693333333333332e-05,
      "loss": 0.0019,
      "step": 63920
    },
    {
      "epoch": 3.4096,
      "grad_norm": 0.4803314507007599,
      "learning_rate": 2.869e-05,
      "loss": 0.0013,
      "step": 63930
    },
    {
      "epoch": 3.4101333333333335,
      "grad_norm": 0.08477127552032471,
      "learning_rate": 2.8686666666666668e-05,
      "loss": 0.0029,
      "step": 63940
    },
    {
      "epoch": 3.4106666666666667,
      "grad_norm": 0.31079915165901184,
      "learning_rate": 2.8683333333333334e-05,
      "loss": 0.0022,
      "step": 63950
    },
    {
      "epoch": 3.4112,
      "grad_norm": 0.0847647413611412,
      "learning_rate": 2.868e-05,
      "loss": 0.003,
      "step": 63960
    },
    {
      "epoch": 3.4117333333333333,
      "grad_norm": 0.1412726640701294,
      "learning_rate": 2.867666666666667e-05,
      "loss": 0.003,
      "step": 63970
    },
    {
      "epoch": 3.4122666666666666,
      "grad_norm": 0.05650951713323593,
      "learning_rate": 2.8673333333333336e-05,
      "loss": 0.0029,
      "step": 63980
    },
    {
      "epoch": 3.4128,
      "grad_norm": 0.22603681683540344,
      "learning_rate": 2.867e-05,
      "loss": 0.0022,
      "step": 63990
    },
    {
      "epoch": 3.413333333333333,
      "grad_norm": 0.31081530451774597,
      "learning_rate": 2.8666666666666668e-05,
      "loss": 0.0036,
      "step": 64000
    },
    {
      "epoch": 3.413866666666667,
      "grad_norm": 4.937713349306705e-09,
      "learning_rate": 2.8663333333333337e-05,
      "loss": 0.0029,
      "step": 64010
    },
    {
      "epoch": 3.4144,
      "grad_norm": 0.1695275902748108,
      "learning_rate": 2.8660000000000003e-05,
      "loss": 0.0029,
      "step": 64020
    },
    {
      "epoch": 3.4149333333333334,
      "grad_norm": 0.1695387214422226,
      "learning_rate": 2.865666666666667e-05,
      "loss": 0.003,
      "step": 64030
    },
    {
      "epoch": 3.4154666666666667,
      "grad_norm": 0.19777585566043854,
      "learning_rate": 2.8653333333333332e-05,
      "loss": 0.0038,
      "step": 64040
    },
    {
      "epoch": 3.416,
      "grad_norm": 0.33908194303512573,
      "learning_rate": 2.865e-05,
      "loss": 0.0018,
      "step": 64050
    },
    {
      "epoch": 3.416533333333333,
      "grad_norm": 0.16951799392700195,
      "learning_rate": 2.8646666666666665e-05,
      "loss": 0.0027,
      "step": 64060
    },
    {
      "epoch": 3.4170666666666665,
      "grad_norm": 3.377884860711333e-09,
      "learning_rate": 2.8643333333333334e-05,
      "loss": 0.0028,
      "step": 64070
    },
    {
      "epoch": 3.4176,
      "grad_norm": 0.028255075216293335,
      "learning_rate": 2.864e-05,
      "loss": 0.0022,
      "step": 64080
    },
    {
      "epoch": 3.4181333333333335,
      "grad_norm": 0.3390391170978546,
      "learning_rate": 2.8636666666666666e-05,
      "loss": 0.0031,
      "step": 64090
    },
    {
      "epoch": 3.4186666666666667,
      "grad_norm": 0.16953475773334503,
      "learning_rate": 2.8633333333333336e-05,
      "loss": 0.004,
      "step": 64100
    },
    {
      "epoch": 3.4192,
      "grad_norm": 0.16952189803123474,
      "learning_rate": 2.8630000000000002e-05,
      "loss": 0.0034,
      "step": 64110
    },
    {
      "epoch": 3.4197333333333333,
      "grad_norm": 0.08476036787033081,
      "learning_rate": 2.8626666666666668e-05,
      "loss": 0.003,
      "step": 64120
    },
    {
      "epoch": 3.4202666666666666,
      "grad_norm": 0.16952268779277802,
      "learning_rate": 2.8623333333333334e-05,
      "loss": 0.0038,
      "step": 64130
    },
    {
      "epoch": 3.4208,
      "grad_norm": 0.08476311713457108,
      "learning_rate": 2.8620000000000004e-05,
      "loss": 0.0033,
      "step": 64140
    },
    {
      "epoch": 3.421333333333333,
      "grad_norm": 0.08475805819034576,
      "learning_rate": 2.861666666666667e-05,
      "loss": 0.0037,
      "step": 64150
    },
    {
      "epoch": 3.421866666666667,
      "grad_norm": 4.082196358723422e-09,
      "learning_rate": 2.8613333333333336e-05,
      "loss": 0.0041,
      "step": 64160
    },
    {
      "epoch": 3.4224,
      "grad_norm": 0.16951912641525269,
      "learning_rate": 2.8610000000000002e-05,
      "loss": 0.0042,
      "step": 64170
    },
    {
      "epoch": 3.4229333333333334,
      "grad_norm": 0.36729422211647034,
      "learning_rate": 2.8606666666666672e-05,
      "loss": 0.0036,
      "step": 64180
    },
    {
      "epoch": 3.4234666666666667,
      "grad_norm": 0.28253161907196045,
      "learning_rate": 2.860333333333333e-05,
      "loss": 0.0032,
      "step": 64190
    },
    {
      "epoch": 3.424,
      "grad_norm": 0.11301466077566147,
      "learning_rate": 2.86e-05,
      "loss": 0.0038,
      "step": 64200
    },
    {
      "epoch": 3.424533333333333,
      "grad_norm": 0.11301267892122269,
      "learning_rate": 2.8596666666666667e-05,
      "loss": 0.0034,
      "step": 64210
    },
    {
      "epoch": 3.4250666666666665,
      "grad_norm": 0.31077033281326294,
      "learning_rate": 2.8593333333333333e-05,
      "loss": 0.0034,
      "step": 64220
    },
    {
      "epoch": 3.4256,
      "grad_norm": 0.028253603726625443,
      "learning_rate": 2.859e-05,
      "loss": 0.0032,
      "step": 64230
    },
    {
      "epoch": 3.4261333333333335,
      "grad_norm": 0.16952069103717804,
      "learning_rate": 2.858666666666667e-05,
      "loss": 0.0026,
      "step": 64240
    },
    {
      "epoch": 3.4266666666666667,
      "grad_norm": 0.056504350155591965,
      "learning_rate": 2.8583333333333335e-05,
      "loss": 0.0038,
      "step": 64250
    },
    {
      "epoch": 3.4272,
      "grad_norm": 0.6780372262001038,
      "learning_rate": 2.858e-05,
      "loss": 0.004,
      "step": 64260
    },
    {
      "epoch": 3.4277333333333333,
      "grad_norm": 0.3673085868358612,
      "learning_rate": 2.8576666666666667e-05,
      "loss": 0.0018,
      "step": 64270
    },
    {
      "epoch": 3.4282666666666666,
      "grad_norm": 0.45203089714050293,
      "learning_rate": 2.8573333333333336e-05,
      "loss": 0.0014,
      "step": 64280
    },
    {
      "epoch": 3.4288,
      "grad_norm": 0.5368317365646362,
      "learning_rate": 2.8570000000000003e-05,
      "loss": 0.0033,
      "step": 64290
    },
    {
      "epoch": 3.429333333333333,
      "grad_norm": 0.05650264397263527,
      "learning_rate": 2.856666666666667e-05,
      "loss": 0.0041,
      "step": 64300
    },
    {
      "epoch": 3.429866666666667,
      "grad_norm": 0.14126265048980713,
      "learning_rate": 2.8563333333333335e-05,
      "loss": 0.0033,
      "step": 64310
    },
    {
      "epoch": 3.4304,
      "grad_norm": 0.22601936757564545,
      "learning_rate": 2.8560000000000004e-05,
      "loss": 0.002,
      "step": 64320
    },
    {
      "epoch": 3.4309333333333334,
      "grad_norm": 0.02825426682829857,
      "learning_rate": 2.855666666666667e-05,
      "loss": 0.0029,
      "step": 64330
    },
    {
      "epoch": 3.4314666666666667,
      "grad_norm": 0.19775693118572235,
      "learning_rate": 2.8553333333333333e-05,
      "loss": 0.0028,
      "step": 64340
    },
    {
      "epoch": 3.432,
      "grad_norm": 0.5370404720306396,
      "learning_rate": 2.855e-05,
      "loss": 0.0038,
      "step": 64350
    },
    {
      "epoch": 3.432533333333333,
      "grad_norm": 0.05815904587507248,
      "learning_rate": 2.8546666666666666e-05,
      "loss": 0.0032,
      "step": 64360
    },
    {
      "epoch": 3.4330666666666665,
      "grad_norm": 0.423765629529953,
      "learning_rate": 2.854333333333333e-05,
      "loss": 0.0026,
      "step": 64370
    },
    {
      "epoch": 3.4336,
      "grad_norm": 0.056504447013139725,
      "learning_rate": 2.854e-05,
      "loss": 0.0034,
      "step": 64380
    },
    {
      "epoch": 3.4341333333333335,
      "grad_norm": 0.056499578058719635,
      "learning_rate": 2.8536666666666667e-05,
      "loss": 0.0026,
      "step": 64390
    },
    {
      "epoch": 3.4346666666666668,
      "grad_norm": 0.22603262960910797,
      "learning_rate": 2.8533333333333333e-05,
      "loss": 0.0038,
      "step": 64400
    },
    {
      "epoch": 3.4352,
      "grad_norm": 0.5649780631065369,
      "learning_rate": 2.853e-05,
      "loss": 0.0026,
      "step": 64410
    },
    {
      "epoch": 3.4357333333333333,
      "grad_norm": 0.3672979772090912,
      "learning_rate": 2.852666666666667e-05,
      "loss": 0.0048,
      "step": 64420
    },
    {
      "epoch": 3.4362666666666666,
      "grad_norm": 0.5932289361953735,
      "learning_rate": 2.8523333333333335e-05,
      "loss": 0.0035,
      "step": 64430
    },
    {
      "epoch": 3.4368,
      "grad_norm": 0.5085431933403015,
      "learning_rate": 2.852e-05,
      "loss": 0.0051,
      "step": 64440
    },
    {
      "epoch": 3.437333333333333,
      "grad_norm": 0.5932098031044006,
      "learning_rate": 2.851666666666667e-05,
      "loss": 0.0043,
      "step": 64450
    },
    {
      "epoch": 3.437866666666667,
      "grad_norm": 0.08475258201360703,
      "learning_rate": 2.8513333333333337e-05,
      "loss": 0.0044,
      "step": 64460
    },
    {
      "epoch": 3.4384,
      "grad_norm": 0.3672245740890503,
      "learning_rate": 2.8510000000000003e-05,
      "loss": 0.0042,
      "step": 64470
    },
    {
      "epoch": 3.4389333333333334,
      "grad_norm": 1.7269646823692142e-09,
      "learning_rate": 2.850666666666667e-05,
      "loss": 0.003,
      "step": 64480
    },
    {
      "epoch": 3.4394666666666667,
      "grad_norm": 3.897343336944914e-09,
      "learning_rate": 2.850333333333334e-05,
      "loss": 0.0028,
      "step": 64490
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.11299435049295425,
      "learning_rate": 2.8499999999999998e-05,
      "loss": 0.0037,
      "step": 64500
    },
    {
      "epoch": 3.440533333333333,
      "grad_norm": 0.028248557820916176,
      "learning_rate": 2.8496666666666664e-05,
      "loss": 0.0026,
      "step": 64510
    },
    {
      "epoch": 3.4410666666666665,
      "grad_norm": 0.14125049114227295,
      "learning_rate": 2.8493333333333334e-05,
      "loss": 0.0045,
      "step": 64520
    },
    {
      "epoch": 3.4416,
      "grad_norm": 0.08474455773830414,
      "learning_rate": 2.849e-05,
      "loss": 0.0028,
      "step": 64530
    },
    {
      "epoch": 3.4421333333333335,
      "grad_norm": 0.1694875955581665,
      "learning_rate": 2.8486666666666666e-05,
      "loss": 0.004,
      "step": 64540
    },
    {
      "epoch": 3.4426666666666668,
      "grad_norm": 0.3954639136791229,
      "learning_rate": 2.8483333333333336e-05,
      "loss": 0.0027,
      "step": 64550
    },
    {
      "epoch": 3.4432,
      "grad_norm": 4.1009111662049236e-09,
      "learning_rate": 2.8480000000000002e-05,
      "loss": 0.004,
      "step": 64560
    },
    {
      "epoch": 3.4437333333333333,
      "grad_norm": 0.02824840322136879,
      "learning_rate": 2.8476666666666668e-05,
      "loss": 0.0039,
      "step": 64570
    },
    {
      "epoch": 3.4442666666666666,
      "grad_norm": 2.3015778172208456e-09,
      "learning_rate": 2.8473333333333334e-05,
      "loss": 0.0033,
      "step": 64580
    },
    {
      "epoch": 3.4448,
      "grad_norm": 0.2259933203458786,
      "learning_rate": 2.8470000000000004e-05,
      "loss": 0.0042,
      "step": 64590
    },
    {
      "epoch": 3.445333333333333,
      "grad_norm": 0.028249839320778847,
      "learning_rate": 2.846666666666667e-05,
      "loss": 0.0046,
      "step": 64600
    },
    {
      "epoch": 3.445866666666667,
      "grad_norm": 0.08474769443273544,
      "learning_rate": 2.8463333333333336e-05,
      "loss": 0.0028,
      "step": 64610
    },
    {
      "epoch": 3.4464,
      "grad_norm": 0.33900079131126404,
      "learning_rate": 2.8460000000000002e-05,
      "loss": 0.0029,
      "step": 64620
    },
    {
      "epoch": 3.4469333333333334,
      "grad_norm": 0.2542342245578766,
      "learning_rate": 2.845666666666667e-05,
      "loss": 0.0029,
      "step": 64630
    },
    {
      "epoch": 3.4474666666666667,
      "grad_norm": 0.028247464448213577,
      "learning_rate": 2.8453333333333338e-05,
      "loss": 0.0038,
      "step": 64640
    },
    {
      "epoch": 3.448,
      "grad_norm": 0.22599047422409058,
      "learning_rate": 2.845e-05,
      "loss": 0.0026,
      "step": 64650
    },
    {
      "epoch": 3.4485333333333332,
      "grad_norm": 0.2824672758579254,
      "learning_rate": 2.8446666666666666e-05,
      "loss": 0.003,
      "step": 64660
    },
    {
      "epoch": 3.4490666666666665,
      "grad_norm": 0.4519760310649872,
      "learning_rate": 2.8443333333333333e-05,
      "loss": 0.0026,
      "step": 64670
    },
    {
      "epoch": 3.4496,
      "grad_norm": 0.11299878358840942,
      "learning_rate": 2.844e-05,
      "loss": 0.004,
      "step": 64680
    },
    {
      "epoch": 3.4501333333333335,
      "grad_norm": 0.11298693716526031,
      "learning_rate": 2.8436666666666668e-05,
      "loss": 0.0036,
      "step": 64690
    },
    {
      "epoch": 3.4506666666666668,
      "grad_norm": 0.08474457263946533,
      "learning_rate": 2.8433333333333334e-05,
      "loss": 0.0036,
      "step": 64700
    },
    {
      "epoch": 3.4512,
      "grad_norm": 0.2542327344417572,
      "learning_rate": 2.843e-05,
      "loss": 0.0032,
      "step": 64710
    },
    {
      "epoch": 3.4517333333333333,
      "grad_norm": 0.02824767306447029,
      "learning_rate": 2.8426666666666667e-05,
      "loss": 0.003,
      "step": 64720
    },
    {
      "epoch": 3.4522666666666666,
      "grad_norm": 0.11299878358840942,
      "learning_rate": 2.8423333333333336e-05,
      "loss": 0.0034,
      "step": 64730
    },
    {
      "epoch": 3.4528,
      "grad_norm": 0.028246797621250153,
      "learning_rate": 2.8420000000000002e-05,
      "loss": 0.003,
      "step": 64740
    },
    {
      "epoch": 3.453333333333333,
      "grad_norm": 0.0847451239824295,
      "learning_rate": 2.841666666666667e-05,
      "loss": 0.0025,
      "step": 64750
    },
    {
      "epoch": 3.4538666666666664,
      "grad_norm": 0.08474183827638626,
      "learning_rate": 2.8413333333333335e-05,
      "loss": 0.0028,
      "step": 64760
    },
    {
      "epoch": 3.4544,
      "grad_norm": 0.254226952791214,
      "learning_rate": 2.8410000000000004e-05,
      "loss": 0.0043,
      "step": 64770
    },
    {
      "epoch": 3.4549333333333334,
      "grad_norm": 0.08474204689264297,
      "learning_rate": 2.840666666666667e-05,
      "loss": 0.0026,
      "step": 64780
    },
    {
      "epoch": 3.4554666666666667,
      "grad_norm": 0.02824702486395836,
      "learning_rate": 2.8403333333333336e-05,
      "loss": 0.0031,
      "step": 64790
    },
    {
      "epoch": 3.456,
      "grad_norm": 0.16947431862354279,
      "learning_rate": 2.84e-05,
      "loss": 0.0031,
      "step": 64800
    },
    {
      "epoch": 3.4565333333333332,
      "grad_norm": 0.05649678781628609,
      "learning_rate": 2.8396666666666665e-05,
      "loss": 0.0024,
      "step": 64810
    },
    {
      "epoch": 3.4570666666666665,
      "grad_norm": 0.028244640678167343,
      "learning_rate": 2.839333333333333e-05,
      "loss": 0.0034,
      "step": 64820
    },
    {
      "epoch": 3.4576000000000002,
      "grad_norm": 0.2259940356016159,
      "learning_rate": 2.839e-05,
      "loss": 0.0064,
      "step": 64830
    },
    {
      "epoch": 3.4581333333333335,
      "grad_norm": 0.19754262268543243,
      "learning_rate": 2.8386666666666667e-05,
      "loss": 0.0037,
      "step": 64840
    },
    {
      "epoch": 3.458666666666667,
      "grad_norm": 0.2257719188928604,
      "learning_rate": 2.8383333333333333e-05,
      "loss": 0.0029,
      "step": 64850
    },
    {
      "epoch": 3.4592,
      "grad_norm": 0.2822306156158447,
      "learning_rate": 2.8380000000000003e-05,
      "loss": 0.0024,
      "step": 64860
    },
    {
      "epoch": 3.4597333333333333,
      "grad_norm": 0.16932162642478943,
      "learning_rate": 2.837666666666667e-05,
      "loss": 0.0037,
      "step": 64870
    },
    {
      "epoch": 3.4602666666666666,
      "grad_norm": 0.16932788491249084,
      "learning_rate": 2.8373333333333335e-05,
      "loss": 0.0034,
      "step": 64880
    },
    {
      "epoch": 3.4608,
      "grad_norm": 0.45151370763778687,
      "learning_rate": 2.837e-05,
      "loss": 0.002,
      "step": 64890
    },
    {
      "epoch": 3.461333333333333,
      "grad_norm": 0.1145196408033371,
      "learning_rate": 2.836666666666667e-05,
      "loss": 0.0041,
      "step": 64900
    },
    {
      "epoch": 3.4618666666666664,
      "grad_norm": 0.02822163514792919,
      "learning_rate": 2.8363333333333337e-05,
      "loss": 0.0034,
      "step": 64910
    },
    {
      "epoch": 3.4624,
      "grad_norm": 0.08465815335512161,
      "learning_rate": 2.8360000000000003e-05,
      "loss": 0.0026,
      "step": 64920
    },
    {
      "epoch": 3.4629333333333334,
      "grad_norm": 0.28220921754837036,
      "learning_rate": 2.835666666666667e-05,
      "loss": 0.0021,
      "step": 64930
    },
    {
      "epoch": 3.4634666666666667,
      "grad_norm": 0.5926756858825684,
      "learning_rate": 2.835333333333334e-05,
      "loss": 0.0028,
      "step": 64940
    },
    {
      "epoch": 3.464,
      "grad_norm": 0.2821870446205139,
      "learning_rate": 2.8349999999999998e-05,
      "loss": 0.0021,
      "step": 64950
    },
    {
      "epoch": 3.4645333333333332,
      "grad_norm": 0.056443218141794205,
      "learning_rate": 2.8346666666666667e-05,
      "loss": 0.0032,
      "step": 64960
    },
    {
      "epoch": 3.4650666666666665,
      "grad_norm": 0.02821929007768631,
      "learning_rate": 2.8343333333333334e-05,
      "loss": 0.002,
      "step": 64970
    },
    {
      "epoch": 3.4656000000000002,
      "grad_norm": 0.31042465567588806,
      "learning_rate": 2.834e-05,
      "loss": 0.0041,
      "step": 64980
    },
    {
      "epoch": 3.4661333333333335,
      "grad_norm": 0.08466437458992004,
      "learning_rate": 2.8336666666666666e-05,
      "loss": 0.0021,
      "step": 64990
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 0.3673197329044342,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 0.0029,
      "step": 65000
    },
    {
      "epoch": 3.4672,
      "grad_norm": 0.3111192584037781,
      "learning_rate": 2.833e-05,
      "loss": 0.0028,
      "step": 65010
    },
    {
      "epoch": 3.4677333333333333,
      "grad_norm": 0.31039854884147644,
      "learning_rate": 2.8326666666666668e-05,
      "loss": 0.0016,
      "step": 65020
    },
    {
      "epoch": 3.4682666666666666,
      "grad_norm": 0.39511197805404663,
      "learning_rate": 2.8323333333333334e-05,
      "loss": 0.0031,
      "step": 65030
    },
    {
      "epoch": 3.4688,
      "grad_norm": 0.11287609487771988,
      "learning_rate": 2.8320000000000003e-05,
      "loss": 0.002,
      "step": 65040
    },
    {
      "epoch": 3.469333333333333,
      "grad_norm": 0.11288061738014221,
      "learning_rate": 2.831666666666667e-05,
      "loss": 0.0038,
      "step": 65050
    },
    {
      "epoch": 3.4698666666666664,
      "grad_norm": 0.11288093775510788,
      "learning_rate": 2.8313333333333336e-05,
      "loss": 0.0033,
      "step": 65060
    },
    {
      "epoch": 3.4704,
      "grad_norm": 0.02822089195251465,
      "learning_rate": 2.8310000000000002e-05,
      "loss": 0.0032,
      "step": 65070
    },
    {
      "epoch": 3.4709333333333334,
      "grad_norm": 0.11287131905555725,
      "learning_rate": 2.830666666666667e-05,
      "loss": 0.0026,
      "step": 65080
    },
    {
      "epoch": 3.4714666666666667,
      "grad_norm": 0.056439921259880066,
      "learning_rate": 2.8303333333333337e-05,
      "loss": 0.0021,
      "step": 65090
    },
    {
      "epoch": 3.472,
      "grad_norm": 0.08465444296598434,
      "learning_rate": 2.83e-05,
      "loss": 0.0027,
      "step": 65100
    },
    {
      "epoch": 3.4725333333333332,
      "grad_norm": 0.22575002908706665,
      "learning_rate": 2.8296666666666666e-05,
      "loss": 0.0023,
      "step": 65110
    },
    {
      "epoch": 3.4730666666666665,
      "grad_norm": 0.02821929194033146,
      "learning_rate": 2.8293333333333332e-05,
      "loss": 0.0028,
      "step": 65120
    },
    {
      "epoch": 3.4736000000000002,
      "grad_norm": 0.36687424778938293,
      "learning_rate": 2.829e-05,
      "loss": 0.0027,
      "step": 65130
    },
    {
      "epoch": 3.4741333333333335,
      "grad_norm": 0.2821715474128723,
      "learning_rate": 2.8286666666666668e-05,
      "loss": 0.0028,
      "step": 65140
    },
    {
      "epoch": 3.474666666666667,
      "grad_norm": 0.11287782341241837,
      "learning_rate": 2.8283333333333334e-05,
      "loss": 0.0042,
      "step": 65150
    },
    {
      "epoch": 3.4752,
      "grad_norm": 0.1693149209022522,
      "learning_rate": 2.828e-05,
      "loss": 0.0027,
      "step": 65160
    },
    {
      "epoch": 3.4757333333333333,
      "grad_norm": 0.08465498685836792,
      "learning_rate": 2.8276666666666666e-05,
      "loss": 0.0031,
      "step": 65170
    },
    {
      "epoch": 3.4762666666666666,
      "grad_norm": 0.11287269741296768,
      "learning_rate": 2.8273333333333336e-05,
      "loss": 0.0027,
      "step": 65180
    },
    {
      "epoch": 3.4768,
      "grad_norm": 0.05643472447991371,
      "learning_rate": 2.8270000000000002e-05,
      "loss": 0.0032,
      "step": 65190
    },
    {
      "epoch": 3.477333333333333,
      "grad_norm": 0.3386245369911194,
      "learning_rate": 2.8266666666666668e-05,
      "loss": 0.0031,
      "step": 65200
    },
    {
      "epoch": 3.4778666666666664,
      "grad_norm": 0.11287058144807816,
      "learning_rate": 2.8263333333333338e-05,
      "loss": 0.0028,
      "step": 65210
    },
    {
      "epoch": 3.4784,
      "grad_norm": 0.14109337329864502,
      "learning_rate": 2.8260000000000004e-05,
      "loss": 0.0025,
      "step": 65220
    },
    {
      "epoch": 3.4789333333333334,
      "grad_norm": 0.1410873681306839,
      "learning_rate": 2.825666666666667e-05,
      "loss": 0.0031,
      "step": 65230
    },
    {
      "epoch": 3.4794666666666667,
      "grad_norm": 0.1410907804965973,
      "learning_rate": 2.8253333333333336e-05,
      "loss": 0.0025,
      "step": 65240
    },
    {
      "epoch": 3.48,
      "grad_norm": 0.4514647126197815,
      "learning_rate": 2.825e-05,
      "loss": 0.0026,
      "step": 65250
    },
    {
      "epoch": 3.4805333333333333,
      "grad_norm": 0.05643654242157936,
      "learning_rate": 2.8246666666666665e-05,
      "loss": 0.003,
      "step": 65260
    },
    {
      "epoch": 3.4810666666666665,
      "grad_norm": 0.08675873279571533,
      "learning_rate": 2.824333333333333e-05,
      "loss": 0.0025,
      "step": 65270
    },
    {
      "epoch": 3.4816,
      "grad_norm": 0.11286690086126328,
      "learning_rate": 2.824e-05,
      "loss": 0.0027,
      "step": 65280
    },
    {
      "epoch": 3.4821333333333335,
      "grad_norm": 0.0564347542822361,
      "learning_rate": 2.8236666666666667e-05,
      "loss": 0.0032,
      "step": 65290
    },
    {
      "epoch": 3.482666666666667,
      "grad_norm": 0.22573989629745483,
      "learning_rate": 2.8233333333333333e-05,
      "loss": 0.0033,
      "step": 65300
    },
    {
      "epoch": 3.4832,
      "grad_norm": 0.19751760363578796,
      "learning_rate": 2.8230000000000002e-05,
      "loss": 0.0031,
      "step": 65310
    },
    {
      "epoch": 3.4837333333333333,
      "grad_norm": 0.2257394641637802,
      "learning_rate": 2.822666666666667e-05,
      "loss": 0.0023,
      "step": 65320
    },
    {
      "epoch": 3.4842666666666666,
      "grad_norm": 0.11393726617097855,
      "learning_rate": 2.8223333333333335e-05,
      "loss": 0.0038,
      "step": 65330
    },
    {
      "epoch": 3.4848,
      "grad_norm": 0.08166415989398956,
      "learning_rate": 2.822e-05,
      "loss": 0.0059,
      "step": 65340
    },
    {
      "epoch": 3.485333333333333,
      "grad_norm": 0.42327168583869934,
      "learning_rate": 2.821666666666667e-05,
      "loss": 0.0034,
      "step": 65350
    },
    {
      "epoch": 3.4858666666666664,
      "grad_norm": 0.2257317453622818,
      "learning_rate": 2.8213333333333337e-05,
      "loss": 0.0032,
      "step": 65360
    },
    {
      "epoch": 3.4864,
      "grad_norm": 0.19751587510108948,
      "learning_rate": 2.8210000000000003e-05,
      "loss": 0.004,
      "step": 65370
    },
    {
      "epoch": 3.4869333333333334,
      "grad_norm": 0.11287076026201248,
      "learning_rate": 2.820666666666667e-05,
      "loss": 0.0033,
      "step": 65380
    },
    {
      "epoch": 3.4874666666666667,
      "grad_norm": 0.16929784417152405,
      "learning_rate": 2.820333333333334e-05,
      "loss": 0.0028,
      "step": 65390
    },
    {
      "epoch": 3.488,
      "grad_norm": 0.11287032812833786,
      "learning_rate": 2.8199999999999998e-05,
      "loss": 0.0031,
      "step": 65400
    },
    {
      "epoch": 3.4885333333333333,
      "grad_norm": 0.5078831315040588,
      "learning_rate": 2.8196666666666667e-05,
      "loss": 0.0045,
      "step": 65410
    },
    {
      "epoch": 3.4890666666666665,
      "grad_norm": 0.45148351788520813,
      "learning_rate": 2.8193333333333333e-05,
      "loss": 0.0044,
      "step": 65420
    },
    {
      "epoch": 3.4896,
      "grad_norm": 0.16930949687957764,
      "learning_rate": 2.819e-05,
      "loss": 0.0028,
      "step": 65430
    },
    {
      "epoch": 3.4901333333333335,
      "grad_norm": 0.028216570615768433,
      "learning_rate": 2.8186666666666666e-05,
      "loss": 0.0028,
      "step": 65440
    },
    {
      "epoch": 3.490666666666667,
      "grad_norm": 0.16929738223552704,
      "learning_rate": 2.8183333333333335e-05,
      "loss": 0.0044,
      "step": 65450
    },
    {
      "epoch": 3.4912,
      "grad_norm": 6.242761863006763e-09,
      "learning_rate": 2.818e-05,
      "loss": 0.0041,
      "step": 65460
    },
    {
      "epoch": 3.4917333333333334,
      "grad_norm": 0.05643457919359207,
      "learning_rate": 2.8176666666666667e-05,
      "loss": 0.0039,
      "step": 65470
    },
    {
      "epoch": 3.4922666666666666,
      "grad_norm": 0.1692935675382614,
      "learning_rate": 2.8173333333333334e-05,
      "loss": 0.0032,
      "step": 65480
    },
    {
      "epoch": 3.4928,
      "grad_norm": 0.28217700123786926,
      "learning_rate": 2.8170000000000003e-05,
      "loss": 0.0026,
      "step": 65490
    },
    {
      "epoch": 3.493333333333333,
      "grad_norm": 0.14108142256736755,
      "learning_rate": 2.816666666666667e-05,
      "loss": 0.0042,
      "step": 65500
    },
    {
      "epoch": 3.4938666666666665,
      "grad_norm": 0.19751989841461182,
      "learning_rate": 2.8163333333333335e-05,
      "loss": 0.0029,
      "step": 65510
    },
    {
      "epoch": 3.4944,
      "grad_norm": 0.028216807171702385,
      "learning_rate": 2.816e-05,
      "loss": 0.0034,
      "step": 65520
    },
    {
      "epoch": 3.4949333333333334,
      "grad_norm": 0.1975100189447403,
      "learning_rate": 2.815666666666667e-05,
      "loss": 0.0026,
      "step": 65530
    },
    {
      "epoch": 3.4954666666666667,
      "grad_norm": 0.1410764455795288,
      "learning_rate": 2.8153333333333337e-05,
      "loss": 0.002,
      "step": 65540
    },
    {
      "epoch": 3.496,
      "grad_norm": 0.05643303692340851,
      "learning_rate": 2.815e-05,
      "loss": 0.0021,
      "step": 65550
    },
    {
      "epoch": 3.4965333333333333,
      "grad_norm": 0.08465012162923813,
      "learning_rate": 2.8146666666666666e-05,
      "loss": 0.0032,
      "step": 65560
    },
    {
      "epoch": 3.4970666666666665,
      "grad_norm": 0.14107541739940643,
      "learning_rate": 2.8143333333333332e-05,
      "loss": 0.0021,
      "step": 65570
    },
    {
      "epoch": 3.4976,
      "grad_norm": 0.3668266236782074,
      "learning_rate": 2.8139999999999998e-05,
      "loss": 0.0032,
      "step": 65580
    },
    {
      "epoch": 3.4981333333333335,
      "grad_norm": 2.1298043328954464e-09,
      "learning_rate": 2.8136666666666668e-05,
      "loss": 0.002,
      "step": 65590
    },
    {
      "epoch": 3.498666666666667,
      "grad_norm": 0.16928952932357788,
      "learning_rate": 2.8133333333333334e-05,
      "loss": 0.0037,
      "step": 65600
    },
    {
      "epoch": 3.4992,
      "grad_norm": 0.06045513227581978,
      "learning_rate": 2.813e-05,
      "loss": 0.0035,
      "step": 65610
    },
    {
      "epoch": 3.4997333333333334,
      "grad_norm": 0.11285905539989471,
      "learning_rate": 2.8126666666666666e-05,
      "loss": 0.0023,
      "step": 65620
    },
    {
      "epoch": 3.5002666666666666,
      "grad_norm": 0.22572682797908783,
      "learning_rate": 2.8123333333333336e-05,
      "loss": 0.0047,
      "step": 65630
    },
    {
      "epoch": 3.5008,
      "grad_norm": 0.14107520878314972,
      "learning_rate": 2.8120000000000002e-05,
      "loss": 0.0036,
      "step": 65640
    },
    {
      "epoch": 3.501333333333333,
      "grad_norm": 0.056434471160173416,
      "learning_rate": 2.8116666666666668e-05,
      "loss": 0.0024,
      "step": 65650
    },
    {
      "epoch": 3.5018666666666665,
      "grad_norm": 0.28214213252067566,
      "learning_rate": 2.8113333333333337e-05,
      "loss": 0.0028,
      "step": 65660
    },
    {
      "epoch": 3.5023999999999997,
      "grad_norm": 0.2539556920528412,
      "learning_rate": 2.8110000000000004e-05,
      "loss": 0.0025,
      "step": 65670
    },
    {
      "epoch": 3.5029333333333335,
      "grad_norm": 0.08464915305376053,
      "learning_rate": 2.810666666666667e-05,
      "loss": 0.0021,
      "step": 65680
    },
    {
      "epoch": 3.5034666666666667,
      "grad_norm": 0.14108119904994965,
      "learning_rate": 2.8103333333333336e-05,
      "loss": 0.0021,
      "step": 65690
    },
    {
      "epoch": 3.504,
      "grad_norm": 0.056429896503686905,
      "learning_rate": 2.8100000000000005e-05,
      "loss": 0.0032,
      "step": 65700
    },
    {
      "epoch": 3.5045333333333333,
      "grad_norm": 0.05643104389309883,
      "learning_rate": 2.8096666666666665e-05,
      "loss": 0.004,
      "step": 65710
    },
    {
      "epoch": 3.5050666666666666,
      "grad_norm": 0.42319804430007935,
      "learning_rate": 2.8093333333333334e-05,
      "loss": 0.0038,
      "step": 65720
    },
    {
      "epoch": 3.5056000000000003,
      "grad_norm": 0.02821645885705948,
      "learning_rate": 2.809e-05,
      "loss": 0.0029,
      "step": 65730
    },
    {
      "epoch": 3.5061333333333335,
      "grad_norm": 0.1410793513059616,
      "learning_rate": 2.8086666666666667e-05,
      "loss": 0.0023,
      "step": 65740
    },
    {
      "epoch": 3.506666666666667,
      "grad_norm": 4.5140184923297966e-09,
      "learning_rate": 2.8083333333333333e-05,
      "loss": 0.0024,
      "step": 65750
    },
    {
      "epoch": 3.5072,
      "grad_norm": 0.02821575663983822,
      "learning_rate": 2.8080000000000002e-05,
      "loss": 0.0031,
      "step": 65760
    },
    {
      "epoch": 3.5077333333333334,
      "grad_norm": 0.1692887246608734,
      "learning_rate": 2.807666666666667e-05,
      "loss": 0.0017,
      "step": 65770
    },
    {
      "epoch": 3.5082666666666666,
      "grad_norm": 0.16928309202194214,
      "learning_rate": 2.8073333333333334e-05,
      "loss": 0.0028,
      "step": 65780
    },
    {
      "epoch": 3.5088,
      "grad_norm": 0.08464489132165909,
      "learning_rate": 2.807e-05,
      "loss": 0.0029,
      "step": 65790
    },
    {
      "epoch": 3.509333333333333,
      "grad_norm": 3.002100124049889e-09,
      "learning_rate": 2.806666666666667e-05,
      "loss": 0.0022,
      "step": 65800
    },
    {
      "epoch": 3.5098666666666665,
      "grad_norm": 0.14107286930084229,
      "learning_rate": 2.8063333333333336e-05,
      "loss": 0.0037,
      "step": 65810
    },
    {
      "epoch": 3.5103999999999997,
      "grad_norm": 0.05642641335725784,
      "learning_rate": 2.8060000000000002e-05,
      "loss": 0.0033,
      "step": 65820
    },
    {
      "epoch": 3.5109333333333335,
      "grad_norm": 0.3103697597980499,
      "learning_rate": 2.805666666666667e-05,
      "loss": 0.0035,
      "step": 65830
    },
    {
      "epoch": 3.5114666666666667,
      "grad_norm": 0.16928230226039886,
      "learning_rate": 2.8053333333333338e-05,
      "loss": 0.0026,
      "step": 65840
    },
    {
      "epoch": 3.512,
      "grad_norm": 2.935613307997187e-09,
      "learning_rate": 2.8050000000000004e-05,
      "loss": 0.0031,
      "step": 65850
    },
    {
      "epoch": 3.5125333333333333,
      "grad_norm": 0.08464206010103226,
      "learning_rate": 2.8046666666666667e-05,
      "loss": 0.0018,
      "step": 65860
    },
    {
      "epoch": 3.5130666666666666,
      "grad_norm": 0.11285462975502014,
      "learning_rate": 2.8043333333333333e-05,
      "loss": 0.0031,
      "step": 65870
    },
    {
      "epoch": 3.5136,
      "grad_norm": 0.056430328637361526,
      "learning_rate": 2.804e-05,
      "loss": 0.0033,
      "step": 65880
    },
    {
      "epoch": 3.5141333333333336,
      "grad_norm": 0.2257070392370224,
      "learning_rate": 2.8036666666666665e-05,
      "loss": 0.003,
      "step": 65890
    },
    {
      "epoch": 3.514666666666667,
      "grad_norm": 0.19749559462070465,
      "learning_rate": 2.8033333333333335e-05,
      "loss": 0.0021,
      "step": 65900
    },
    {
      "epoch": 3.5152,
      "grad_norm": 0.08463811874389648,
      "learning_rate": 2.803e-05,
      "loss": 0.0028,
      "step": 65910
    },
    {
      "epoch": 3.5157333333333334,
      "grad_norm": 0.08464027941226959,
      "learning_rate": 2.8026666666666667e-05,
      "loss": 0.0033,
      "step": 65920
    },
    {
      "epoch": 3.5162666666666667,
      "grad_norm": 0.11285104602575302,
      "learning_rate": 2.8023333333333333e-05,
      "loss": 0.0019,
      "step": 65930
    },
    {
      "epoch": 3.5168,
      "grad_norm": 0.056425899267196655,
      "learning_rate": 2.8020000000000003e-05,
      "loss": 0.0039,
      "step": 65940
    },
    {
      "epoch": 3.517333333333333,
      "grad_norm": 0.056428179144859314,
      "learning_rate": 2.801666666666667e-05,
      "loss": 0.0031,
      "step": 65950
    },
    {
      "epoch": 3.5178666666666665,
      "grad_norm": 0.3949984312057495,
      "learning_rate": 2.8013333333333335e-05,
      "loss": 0.0037,
      "step": 65960
    },
    {
      "epoch": 3.5183999999999997,
      "grad_norm": 0.028212612494826317,
      "learning_rate": 2.8010000000000005e-05,
      "loss": 0.0029,
      "step": 65970
    },
    {
      "epoch": 3.5189333333333335,
      "grad_norm": 0.056425273418426514,
      "learning_rate": 2.800666666666667e-05,
      "loss": 0.0035,
      "step": 65980
    },
    {
      "epoch": 3.5194666666666667,
      "grad_norm": 0.028212465345859528,
      "learning_rate": 2.8003333333333337e-05,
      "loss": 0.0037,
      "step": 65990
    },
    {
      "epoch": 3.52,
      "grad_norm": 0.11285819113254547,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.0036,
      "step": 66000
    },
    {
      "epoch": 3.5205333333333333,
      "grad_norm": 0.1974935233592987,
      "learning_rate": 2.7996666666666666e-05,
      "loss": 0.0029,
      "step": 66010
    },
    {
      "epoch": 3.5210666666666666,
      "grad_norm": 0.02821267396211624,
      "learning_rate": 2.7993333333333332e-05,
      "loss": 0.0028,
      "step": 66020
    },
    {
      "epoch": 3.5216,
      "grad_norm": 0.2821241617202759,
      "learning_rate": 2.7989999999999998e-05,
      "loss": 0.0022,
      "step": 66030
    },
    {
      "epoch": 3.5221333333333336,
      "grad_norm": 0.2821398377418518,
      "learning_rate": 2.7986666666666668e-05,
      "loss": 0.0025,
      "step": 66040
    },
    {
      "epoch": 3.522666666666667,
      "grad_norm": 0.08464010059833527,
      "learning_rate": 2.7983333333333334e-05,
      "loss": 0.0038,
      "step": 66050
    },
    {
      "epoch": 3.5232,
      "grad_norm": 0.14106272161006927,
      "learning_rate": 2.798e-05,
      "loss": 0.0023,
      "step": 66060
    },
    {
      "epoch": 3.5237333333333334,
      "grad_norm": 0.028212569653987885,
      "learning_rate": 2.797666666666667e-05,
      "loss": 0.0046,
      "step": 66070
    },
    {
      "epoch": 3.5242666666666667,
      "grad_norm": 0.14106442034244537,
      "learning_rate": 2.7973333333333335e-05,
      "loss": 0.0039,
      "step": 66080
    },
    {
      "epoch": 3.5248,
      "grad_norm": 0.25391289591789246,
      "learning_rate": 2.797e-05,
      "loss": 0.0034,
      "step": 66090
    },
    {
      "epoch": 3.525333333333333,
      "grad_norm": 0.1974823772907257,
      "learning_rate": 2.7966666666666668e-05,
      "loss": 0.0025,
      "step": 66100
    },
    {
      "epoch": 3.5258666666666665,
      "grad_norm": 0.36675918102264404,
      "learning_rate": 2.7963333333333337e-05,
      "loss": 0.0038,
      "step": 66110
    },
    {
      "epoch": 3.5263999999999998,
      "grad_norm": 0.05642768740653992,
      "learning_rate": 2.7960000000000003e-05,
      "loss": 0.003,
      "step": 66120
    },
    {
      "epoch": 3.5269333333333335,
      "grad_norm": 0.19749854505062103,
      "learning_rate": 2.795666666666667e-05,
      "loss": 0.002,
      "step": 66130
    },
    {
      "epoch": 3.5274666666666668,
      "grad_norm": 0.1692732721567154,
      "learning_rate": 2.7953333333333336e-05,
      "loss": 0.0033,
      "step": 66140
    },
    {
      "epoch": 3.528,
      "grad_norm": 0.08463452011346817,
      "learning_rate": 2.7950000000000005e-05,
      "loss": 0.003,
      "step": 66150
    },
    {
      "epoch": 3.5285333333333333,
      "grad_norm": 0.05642222240567207,
      "learning_rate": 2.7946666666666664e-05,
      "loss": 0.0036,
      "step": 66160
    },
    {
      "epoch": 3.5290666666666666,
      "grad_norm": 0.22568993270397186,
      "learning_rate": 2.7943333333333334e-05,
      "loss": 0.0038,
      "step": 66170
    },
    {
      "epoch": 3.5296,
      "grad_norm": 0.4513753652572632,
      "learning_rate": 2.794e-05,
      "loss": 0.0032,
      "step": 66180
    },
    {
      "epoch": 3.5301333333333336,
      "grad_norm": 0.3949960172176361,
      "learning_rate": 2.7936666666666666e-05,
      "loss": 0.0032,
      "step": 66190
    },
    {
      "epoch": 3.530666666666667,
      "grad_norm": 0.1974785178899765,
      "learning_rate": 2.7933333333333332e-05,
      "loss": 0.0036,
      "step": 66200
    },
    {
      "epoch": 3.5312,
      "grad_norm": 3.320686614571855e-09,
      "learning_rate": 2.7930000000000002e-05,
      "loss": 0.0034,
      "step": 66210
    },
    {
      "epoch": 3.5317333333333334,
      "grad_norm": 0.33853238821029663,
      "learning_rate": 2.7926666666666668e-05,
      "loss": 0.0028,
      "step": 66220
    },
    {
      "epoch": 3.5322666666666667,
      "grad_norm": 0.056423500180244446,
      "learning_rate": 2.7923333333333334e-05,
      "loss": 0.0026,
      "step": 66230
    },
    {
      "epoch": 3.5328,
      "grad_norm": 0.028211399912834167,
      "learning_rate": 2.792e-05,
      "loss": 0.0037,
      "step": 66240
    },
    {
      "epoch": 3.533333333333333,
      "grad_norm": 0.056422144174575806,
      "learning_rate": 2.791666666666667e-05,
      "loss": 0.0036,
      "step": 66250
    },
    {
      "epoch": 3.5338666666666665,
      "grad_norm": 0.14106644690036774,
      "learning_rate": 2.7913333333333336e-05,
      "loss": 0.0027,
      "step": 66260
    },
    {
      "epoch": 3.5343999999999998,
      "grad_norm": 0.3949376940727234,
      "learning_rate": 2.7910000000000002e-05,
      "loss": 0.0035,
      "step": 66270
    },
    {
      "epoch": 3.5349333333333335,
      "grad_norm": 0.11284707486629486,
      "learning_rate": 2.7906666666666668e-05,
      "loss": 0.0026,
      "step": 66280
    },
    {
      "epoch": 3.5354666666666668,
      "grad_norm": 0.28212863206863403,
      "learning_rate": 2.7903333333333338e-05,
      "loss": 0.0034,
      "step": 66290
    },
    {
      "epoch": 3.536,
      "grad_norm": 0.22569191455841064,
      "learning_rate": 2.7900000000000004e-05,
      "loss": 0.0021,
      "step": 66300
    },
    {
      "epoch": 3.5365333333333333,
      "grad_norm": 0.22569146752357483,
      "learning_rate": 2.7896666666666667e-05,
      "loss": 0.004,
      "step": 66310
    },
    {
      "epoch": 3.5370666666666666,
      "grad_norm": 3.408634929868981e-09,
      "learning_rate": 2.7893333333333333e-05,
      "loss": 0.0029,
      "step": 66320
    },
    {
      "epoch": 3.5376,
      "grad_norm": 0.1410539150238037,
      "learning_rate": 2.789e-05,
      "loss": 0.0042,
      "step": 66330
    },
    {
      "epoch": 3.5381333333333336,
      "grad_norm": 0.25390949845314026,
      "learning_rate": 2.7886666666666665e-05,
      "loss": 0.0035,
      "step": 66340
    },
    {
      "epoch": 3.538666666666667,
      "grad_norm": 0.08463232219219208,
      "learning_rate": 2.7883333333333335e-05,
      "loss": 0.0022,
      "step": 66350
    },
    {
      "epoch": 3.5392,
      "grad_norm": 0.1410524994134903,
      "learning_rate": 2.788e-05,
      "loss": 0.002,
      "step": 66360
    },
    {
      "epoch": 3.5397333333333334,
      "grad_norm": 0.08463428914546967,
      "learning_rate": 2.7876666666666667e-05,
      "loss": 0.0028,
      "step": 66370
    },
    {
      "epoch": 3.5402666666666667,
      "grad_norm": 0.16926029324531555,
      "learning_rate": 2.7873333333333333e-05,
      "loss": 0.0021,
      "step": 66380
    },
    {
      "epoch": 3.5408,
      "grad_norm": 0.05642171949148178,
      "learning_rate": 2.7870000000000003e-05,
      "loss": 0.0035,
      "step": 66390
    },
    {
      "epoch": 3.541333333333333,
      "grad_norm": 0.1692630499601364,
      "learning_rate": 2.786666666666667e-05,
      "loss": 0.0019,
      "step": 66400
    },
    {
      "epoch": 3.5418666666666665,
      "grad_norm": 0.1128411591053009,
      "learning_rate": 2.7863333333333335e-05,
      "loss": 0.003,
      "step": 66410
    },
    {
      "epoch": 3.5423999999999998,
      "grad_norm": 0.11284711211919785,
      "learning_rate": 2.7860000000000004e-05,
      "loss": 0.0031,
      "step": 66420
    },
    {
      "epoch": 3.5429333333333335,
      "grad_norm": 0.028210731223225594,
      "learning_rate": 2.785666666666667e-05,
      "loss": 0.0018,
      "step": 66430
    },
    {
      "epoch": 3.5434666666666668,
      "grad_norm": 0.1692655086517334,
      "learning_rate": 2.7853333333333337e-05,
      "loss": 0.0031,
      "step": 66440
    },
    {
      "epoch": 3.544,
      "grad_norm": 0.14370949566364288,
      "learning_rate": 2.7850000000000003e-05,
      "loss": 0.0046,
      "step": 66450
    },
    {
      "epoch": 3.5445333333333333,
      "grad_norm": 0.22567175328731537,
      "learning_rate": 2.7846666666666665e-05,
      "loss": 0.0034,
      "step": 66460
    },
    {
      "epoch": 3.5450666666666666,
      "grad_norm": 0.22568835318088531,
      "learning_rate": 2.784333333333333e-05,
      "loss": 0.0028,
      "step": 66470
    },
    {
      "epoch": 3.5456,
      "grad_norm": 0.16925175487995148,
      "learning_rate": 2.7839999999999998e-05,
      "loss": 0.002,
      "step": 66480
    },
    {
      "epoch": 3.5461333333333336,
      "grad_norm": 0.11284621059894562,
      "learning_rate": 2.7836666666666667e-05,
      "loss": 0.0021,
      "step": 66490
    },
    {
      "epoch": 3.546666666666667,
      "grad_norm": 0.19746948778629303,
      "learning_rate": 2.7833333333333333e-05,
      "loss": 0.0022,
      "step": 66500
    },
    {
      "epoch": 3.5472,
      "grad_norm": 0.056418851017951965,
      "learning_rate": 2.783e-05,
      "loss": 0.0039,
      "step": 66510
    },
    {
      "epoch": 3.5477333333333334,
      "grad_norm": 0.22568169236183167,
      "learning_rate": 2.782666666666667e-05,
      "loss": 0.0028,
      "step": 66520
    },
    {
      "epoch": 3.5482666666666667,
      "grad_norm": 0.22568517923355103,
      "learning_rate": 2.7823333333333335e-05,
      "loss": 0.0042,
      "step": 66530
    },
    {
      "epoch": 3.5488,
      "grad_norm": 0.08462784439325333,
      "learning_rate": 2.782e-05,
      "loss": 0.0036,
      "step": 66540
    },
    {
      "epoch": 3.5493333333333332,
      "grad_norm": 0.11284617334604263,
      "learning_rate": 2.7816666666666667e-05,
      "loss": 0.0023,
      "step": 66550
    },
    {
      "epoch": 3.5498666666666665,
      "grad_norm": 0.19765087962150574,
      "learning_rate": 2.7813333333333337e-05,
      "loss": 0.0035,
      "step": 66560
    },
    {
      "epoch": 3.5504,
      "grad_norm": 1.205629825592041,
      "learning_rate": 2.7810000000000003e-05,
      "loss": 0.003,
      "step": 66570
    },
    {
      "epoch": 3.5509333333333335,
      "grad_norm": 0.344849556684494,
      "learning_rate": 2.780666666666667e-05,
      "loss": 0.0029,
      "step": 66580
    },
    {
      "epoch": 3.5514666666666668,
      "grad_norm": 0.2258750945329666,
      "learning_rate": 2.7803333333333335e-05,
      "loss": 0.0047,
      "step": 66590
    },
    {
      "epoch": 3.552,
      "grad_norm": 0.028235450387001038,
      "learning_rate": 2.7800000000000005e-05,
      "loss": 0.0033,
      "step": 66600
    },
    {
      "epoch": 3.5525333333333333,
      "grad_norm": 0.33850640058517456,
      "learning_rate": 2.7796666666666664e-05,
      "loss": 0.0036,
      "step": 66610
    },
    {
      "epoch": 3.5530666666666666,
      "grad_norm": 0.14104785025119781,
      "learning_rate": 2.7793333333333334e-05,
      "loss": 0.0039,
      "step": 66620
    },
    {
      "epoch": 3.5536,
      "grad_norm": 0.17179253697395325,
      "learning_rate": 2.779e-05,
      "loss": 0.0039,
      "step": 66630
    },
    {
      "epoch": 3.5541333333333336,
      "grad_norm": 0.14104825258255005,
      "learning_rate": 2.7786666666666666e-05,
      "loss": 0.0028,
      "step": 66640
    },
    {
      "epoch": 3.554666666666667,
      "grad_norm": 0.05642025172710419,
      "learning_rate": 2.7783333333333332e-05,
      "loss": 0.0044,
      "step": 66650
    },
    {
      "epoch": 3.5552,
      "grad_norm": 0.05642140284180641,
      "learning_rate": 2.778e-05,
      "loss": 0.0034,
      "step": 66660
    },
    {
      "epoch": 3.5557333333333334,
      "grad_norm": 0.3949584662914276,
      "learning_rate": 2.7776666666666668e-05,
      "loss": 0.0035,
      "step": 66670
    },
    {
      "epoch": 3.5562666666666667,
      "grad_norm": 0.366720050573349,
      "learning_rate": 2.7773333333333334e-05,
      "loss": 0.0043,
      "step": 66680
    },
    {
      "epoch": 3.5568,
      "grad_norm": 0.03734619542956352,
      "learning_rate": 2.777e-05,
      "loss": 0.0035,
      "step": 66690
    },
    {
      "epoch": 3.5573333333333332,
      "grad_norm": 0.062406256794929504,
      "learning_rate": 2.776666666666667e-05,
      "loss": 0.0048,
      "step": 66700
    },
    {
      "epoch": 3.5578666666666665,
      "grad_norm": 0.11283958703279495,
      "learning_rate": 2.7763333333333336e-05,
      "loss": 0.0033,
      "step": 66710
    },
    {
      "epoch": 3.5584,
      "grad_norm": 0.02821045182645321,
      "learning_rate": 2.7760000000000002e-05,
      "loss": 0.0033,
      "step": 66720
    },
    {
      "epoch": 3.558933333333333,
      "grad_norm": 0.8564149141311646,
      "learning_rate": 2.7756666666666668e-05,
      "loss": 0.0047,
      "step": 66730
    },
    {
      "epoch": 3.559466666666667,
      "grad_norm": 0.3385103642940521,
      "learning_rate": 2.7753333333333338e-05,
      "loss": 0.0036,
      "step": 66740
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.2256758064031601,
      "learning_rate": 2.7750000000000004e-05,
      "loss": 0.0024,
      "step": 66750
    },
    {
      "epoch": 3.5605333333333333,
      "grad_norm": 0.05641574785113335,
      "learning_rate": 2.7746666666666666e-05,
      "loss": 0.0036,
      "step": 66760
    },
    {
      "epoch": 3.5610666666666666,
      "grad_norm": 0.25388509035110474,
      "learning_rate": 2.7743333333333333e-05,
      "loss": 0.0034,
      "step": 66770
    },
    {
      "epoch": 3.5616,
      "grad_norm": 0.16924801468849182,
      "learning_rate": 2.774e-05,
      "loss": 0.0033,
      "step": 66780
    },
    {
      "epoch": 3.5621333333333336,
      "grad_norm": 0.11283237487077713,
      "learning_rate": 2.7736666666666665e-05,
      "loss": 0.0025,
      "step": 66790
    },
    {
      "epoch": 3.562666666666667,
      "grad_norm": 0.08462968468666077,
      "learning_rate": 2.7733333333333334e-05,
      "loss": 0.0034,
      "step": 66800
    },
    {
      "epoch": 3.5632,
      "grad_norm": 0.42314326763153076,
      "learning_rate": 2.773e-05,
      "loss": 0.0045,
      "step": 66810
    },
    {
      "epoch": 3.5637333333333334,
      "grad_norm": 0.08462812751531601,
      "learning_rate": 2.7726666666666667e-05,
      "loss": 0.0033,
      "step": 66820
    },
    {
      "epoch": 3.5642666666666667,
      "grad_norm": 0.1646931916475296,
      "learning_rate": 2.7723333333333336e-05,
      "loss": 0.0042,
      "step": 66830
    },
    {
      "epoch": 3.5648,
      "grad_norm": 0.056415751576423645,
      "learning_rate": 2.7720000000000002e-05,
      "loss": 0.0028,
      "step": 66840
    },
    {
      "epoch": 3.5653333333333332,
      "grad_norm": 0.19746848940849304,
      "learning_rate": 2.771666666666667e-05,
      "loss": 0.0033,
      "step": 66850
    },
    {
      "epoch": 3.5658666666666665,
      "grad_norm": 0.08462018519639969,
      "learning_rate": 2.7713333333333335e-05,
      "loss": 0.0032,
      "step": 66860
    },
    {
      "epoch": 3.5664,
      "grad_norm": 2.5339050857553502e-09,
      "learning_rate": 2.7710000000000004e-05,
      "loss": 0.0019,
      "step": 66870
    },
    {
      "epoch": 3.566933333333333,
      "grad_norm": 0.33850160241127014,
      "learning_rate": 2.770666666666667e-05,
      "loss": 0.0031,
      "step": 66880
    },
    {
      "epoch": 3.567466666666667,
      "grad_norm": 0.056414324790239334,
      "learning_rate": 2.7703333333333336e-05,
      "loss": 0.0037,
      "step": 66890
    },
    {
      "epoch": 3.568,
      "grad_norm": 0.22566963732242584,
      "learning_rate": 2.7700000000000002e-05,
      "loss": 0.0038,
      "step": 66900
    },
    {
      "epoch": 3.5685333333333333,
      "grad_norm": 0.2538619041442871,
      "learning_rate": 2.7696666666666672e-05,
      "loss": 0.0022,
      "step": 66910
    },
    {
      "epoch": 3.5690666666666666,
      "grad_norm": 0.028207242488861084,
      "learning_rate": 2.769333333333333e-05,
      "loss": 0.0025,
      "step": 66920
    },
    {
      "epoch": 3.5696,
      "grad_norm": 0.25388050079345703,
      "learning_rate": 2.769e-05,
      "loss": 0.0036,
      "step": 66930
    },
    {
      "epoch": 3.5701333333333336,
      "grad_norm": 0.028206737712025642,
      "learning_rate": 2.7686666666666667e-05,
      "loss": 0.0026,
      "step": 66940
    },
    {
      "epoch": 3.570666666666667,
      "grad_norm": 0.1974564492702484,
      "learning_rate": 2.7683333333333333e-05,
      "loss": 0.0024,
      "step": 66950
    },
    {
      "epoch": 3.5712,
      "grad_norm": 0.5077645778656006,
      "learning_rate": 2.768e-05,
      "loss": 0.0023,
      "step": 66960
    },
    {
      "epoch": 3.5717333333333334,
      "grad_norm": 0.028205949813127518,
      "learning_rate": 2.767666666666667e-05,
      "loss": 0.0022,
      "step": 66970
    },
    {
      "epoch": 3.5722666666666667,
      "grad_norm": 0.1410389244556427,
      "learning_rate": 2.7673333333333335e-05,
      "loss": 0.0024,
      "step": 66980
    },
    {
      "epoch": 3.5728,
      "grad_norm": 0.05641130730509758,
      "learning_rate": 2.767e-05,
      "loss": 0.0032,
      "step": 66990
    },
    {
      "epoch": 3.5733333333333333,
      "grad_norm": 0.42312759160995483,
      "learning_rate": 2.7666666666666667e-05,
      "loss": 0.005,
      "step": 67000
    },
    {
      "epoch": 3.5738666666666665,
      "grad_norm": 0.056413207203149796,
      "learning_rate": 2.7663333333333337e-05,
      "loss": 0.0034,
      "step": 67010
    },
    {
      "epoch": 3.5744,
      "grad_norm": 0.22564098238945007,
      "learning_rate": 2.7660000000000003e-05,
      "loss": 0.0023,
      "step": 67020
    },
    {
      "epoch": 3.574933333333333,
      "grad_norm": 0.22566039860248566,
      "learning_rate": 2.765666666666667e-05,
      "loss": 0.0021,
      "step": 67030
    },
    {
      "epoch": 3.575466666666667,
      "grad_norm": 0.16923962533473969,
      "learning_rate": 2.7653333333333335e-05,
      "loss": 0.003,
      "step": 67040
    },
    {
      "epoch": 3.576,
      "grad_norm": 0.11282870173454285,
      "learning_rate": 2.7650000000000005e-05,
      "loss": 0.0029,
      "step": 67050
    },
    {
      "epoch": 3.5765333333333333,
      "grad_norm": 0.14103449881076813,
      "learning_rate": 2.764666666666667e-05,
      "loss": 0.0037,
      "step": 67060
    },
    {
      "epoch": 3.5770666666666666,
      "grad_norm": 0.2820691466331482,
      "learning_rate": 2.7643333333333334e-05,
      "loss": 0.0031,
      "step": 67070
    },
    {
      "epoch": 3.5776,
      "grad_norm": 0.08462353050708771,
      "learning_rate": 2.764e-05,
      "loss": 0.0036,
      "step": 67080
    },
    {
      "epoch": 3.5781333333333336,
      "grad_norm": 0.14102530479431152,
      "learning_rate": 2.7636666666666666e-05,
      "loss": 0.0031,
      "step": 67090
    },
    {
      "epoch": 3.578666666666667,
      "grad_norm": 2.667003951017932e-09,
      "learning_rate": 2.7633333333333332e-05,
      "loss": 0.0031,
      "step": 67100
    },
    {
      "epoch": 3.5792,
      "grad_norm": 0.45126989483833313,
      "learning_rate": 2.763e-05,
      "loss": 0.0027,
      "step": 67110
    },
    {
      "epoch": 3.5797333333333334,
      "grad_norm": 0.08461853861808777,
      "learning_rate": 2.7626666666666668e-05,
      "loss": 0.0032,
      "step": 67120
    },
    {
      "epoch": 3.5802666666666667,
      "grad_norm": 0.33845365047454834,
      "learning_rate": 2.7623333333333334e-05,
      "loss": 0.0029,
      "step": 67130
    },
    {
      "epoch": 3.5808,
      "grad_norm": 4.159170341466734e-09,
      "learning_rate": 2.762e-05,
      "loss": 0.0026,
      "step": 67140
    },
    {
      "epoch": 3.5813333333333333,
      "grad_norm": 0.02820412628352642,
      "learning_rate": 2.761666666666667e-05,
      "loss": 0.0024,
      "step": 67150
    },
    {
      "epoch": 3.5818666666666665,
      "grad_norm": 0.28206315636634827,
      "learning_rate": 2.7613333333333335e-05,
      "loss": 0.0029,
      "step": 67160
    },
    {
      "epoch": 3.5824,
      "grad_norm": 0.5922648310661316,
      "learning_rate": 2.761e-05,
      "loss": 0.0027,
      "step": 67170
    },
    {
      "epoch": 3.582933333333333,
      "grad_norm": 4.303269296457302e-09,
      "learning_rate": 2.760666666666667e-05,
      "loss": 0.0022,
      "step": 67180
    },
    {
      "epoch": 3.583466666666667,
      "grad_norm": 0.02820521406829357,
      "learning_rate": 2.7603333333333337e-05,
      "loss": 0.0043,
      "step": 67190
    },
    {
      "epoch": 3.584,
      "grad_norm": 0.05641124024987221,
      "learning_rate": 2.7600000000000003e-05,
      "loss": 0.0036,
      "step": 67200
    },
    {
      "epoch": 3.5845333333333333,
      "grad_norm": 0.25382739305496216,
      "learning_rate": 2.759666666666667e-05,
      "loss": 0.0014,
      "step": 67210
    },
    {
      "epoch": 3.5850666666666666,
      "grad_norm": 0.08461899310350418,
      "learning_rate": 2.7593333333333332e-05,
      "loss": 0.0034,
      "step": 67220
    },
    {
      "epoch": 3.5856,
      "grad_norm": 0.05640793591737747,
      "learning_rate": 2.759e-05,
      "loss": 0.0023,
      "step": 67230
    },
    {
      "epoch": 3.586133333333333,
      "grad_norm": 0.08461648225784302,
      "learning_rate": 2.7586666666666665e-05,
      "loss": 0.0039,
      "step": 67240
    },
    {
      "epoch": 3.586666666666667,
      "grad_norm": 0.16923244297504425,
      "learning_rate": 2.7583333333333334e-05,
      "loss": 0.0023,
      "step": 67250
    },
    {
      "epoch": 3.5872,
      "grad_norm": 0.0564102828502655,
      "learning_rate": 2.758e-05,
      "loss": 0.003,
      "step": 67260
    },
    {
      "epoch": 3.5877333333333334,
      "grad_norm": 0.05640894174575806,
      "learning_rate": 2.7576666666666666e-05,
      "loss": 0.002,
      "step": 67270
    },
    {
      "epoch": 3.5882666666666667,
      "grad_norm": 0.3102336823940277,
      "learning_rate": 2.7573333333333336e-05,
      "loss": 0.0026,
      "step": 67280
    },
    {
      "epoch": 3.5888,
      "grad_norm": 0.25383493304252625,
      "learning_rate": 2.7570000000000002e-05,
      "loss": 0.0026,
      "step": 67290
    },
    {
      "epoch": 3.5893333333333333,
      "grad_norm": 0.028204290196299553,
      "learning_rate": 2.7566666666666668e-05,
      "loss": 0.0023,
      "step": 67300
    },
    {
      "epoch": 3.5898666666666665,
      "grad_norm": 0.14102426171302795,
      "learning_rate": 2.7563333333333334e-05,
      "loss": 0.0024,
      "step": 67310
    },
    {
      "epoch": 3.5904,
      "grad_norm": 0.028203941881656647,
      "learning_rate": 2.7560000000000004e-05,
      "loss": 0.0017,
      "step": 67320
    },
    {
      "epoch": 3.590933333333333,
      "grad_norm": 0.25384846329689026,
      "learning_rate": 2.755666666666667e-05,
      "loss": 0.0031,
      "step": 67330
    },
    {
      "epoch": 3.591466666666667,
      "grad_norm": 0.3102301061153412,
      "learning_rate": 2.7553333333333336e-05,
      "loss": 0.0028,
      "step": 67340
    },
    {
      "epoch": 3.592,
      "grad_norm": 0.1128171756863594,
      "learning_rate": 2.7550000000000002e-05,
      "loss": 0.0023,
      "step": 67350
    },
    {
      "epoch": 3.5925333333333334,
      "grad_norm": 0.1692219376564026,
      "learning_rate": 2.7546666666666672e-05,
      "loss": 0.0032,
      "step": 67360
    },
    {
      "epoch": 3.5930666666666666,
      "grad_norm": 0.11281204968690872,
      "learning_rate": 2.754333333333333e-05,
      "loss": 0.0029,
      "step": 67370
    },
    {
      "epoch": 3.5936,
      "grad_norm": 0.028205150738358498,
      "learning_rate": 2.754e-05,
      "loss": 0.0036,
      "step": 67380
    },
    {
      "epoch": 3.594133333333333,
      "grad_norm": 0.1692180186510086,
      "learning_rate": 2.7536666666666667e-05,
      "loss": 0.0034,
      "step": 67390
    },
    {
      "epoch": 3.594666666666667,
      "grad_norm": 0.08461254835128784,
      "learning_rate": 2.7533333333333333e-05,
      "loss": 0.0023,
      "step": 67400
    },
    {
      "epoch": 3.5952,
      "grad_norm": 0.253850519657135,
      "learning_rate": 2.753e-05,
      "loss": 0.0033,
      "step": 67410
    },
    {
      "epoch": 3.5957333333333334,
      "grad_norm": 0.2552091181278229,
      "learning_rate": 2.752666666666667e-05,
      "loss": 0.0029,
      "step": 67420
    },
    {
      "epoch": 3.5962666666666667,
      "grad_norm": 0.05640634149312973,
      "learning_rate": 2.7523333333333335e-05,
      "loss": 0.0025,
      "step": 67430
    },
    {
      "epoch": 3.5968,
      "grad_norm": 0.05640852078795433,
      "learning_rate": 2.752e-05,
      "loss": 0.0031,
      "step": 67440
    },
    {
      "epoch": 3.5973333333333333,
      "grad_norm": 0.17042237520217896,
      "learning_rate": 2.7516666666666667e-05,
      "loss": 0.0022,
      "step": 67450
    },
    {
      "epoch": 3.5978666666666665,
      "grad_norm": 0.03636692836880684,
      "learning_rate": 2.7513333333333336e-05,
      "loss": 0.0025,
      "step": 67460
    },
    {
      "epoch": 3.5984,
      "grad_norm": 0.028202839195728302,
      "learning_rate": 2.7510000000000003e-05,
      "loss": 0.0016,
      "step": 67470
    },
    {
      "epoch": 3.598933333333333,
      "grad_norm": 0.28206247091293335,
      "learning_rate": 2.750666666666667e-05,
      "loss": 0.0029,
      "step": 67480
    },
    {
      "epoch": 3.599466666666667,
      "grad_norm": 0.31021976470947266,
      "learning_rate": 2.7503333333333335e-05,
      "loss": 0.0029,
      "step": 67490
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.028204210102558136,
      "learning_rate": 2.7500000000000004e-05,
      "loss": 0.0023,
      "step": 67500
    },
    {
      "epoch": 3.6005333333333334,
      "grad_norm": 0.1974141001701355,
      "learning_rate": 2.749666666666667e-05,
      "loss": 0.004,
      "step": 67510
    },
    {
      "epoch": 3.6010666666666666,
      "grad_norm": 0.028204262256622314,
      "learning_rate": 2.7493333333333333e-05,
      "loss": 0.0038,
      "step": 67520
    },
    {
      "epoch": 3.6016,
      "grad_norm": 0.28201791644096375,
      "learning_rate": 2.749e-05,
      "loss": 0.0028,
      "step": 67530
    },
    {
      "epoch": 3.602133333333333,
      "grad_norm": 0.2820352017879486,
      "learning_rate": 2.7486666666666666e-05,
      "loss": 0.0025,
      "step": 67540
    },
    {
      "epoch": 3.602666666666667,
      "grad_norm": 0.33841443061828613,
      "learning_rate": 2.748333333333333e-05,
      "loss": 0.0019,
      "step": 67550
    },
    {
      "epoch": 3.6032,
      "grad_norm": 0.028203709051012993,
      "learning_rate": 2.748e-05,
      "loss": 0.0027,
      "step": 67560
    },
    {
      "epoch": 3.6037333333333335,
      "grad_norm": 0.12364465743303299,
      "learning_rate": 2.7476666666666667e-05,
      "loss": 0.0021,
      "step": 67570
    },
    {
      "epoch": 3.6042666666666667,
      "grad_norm": 0.02820197306573391,
      "learning_rate": 2.7473333333333333e-05,
      "loss": 0.0027,
      "step": 67580
    },
    {
      "epoch": 3.6048,
      "grad_norm": 0.11281008273363113,
      "learning_rate": 2.7470000000000003e-05,
      "loss": 0.0043,
      "step": 67590
    },
    {
      "epoch": 3.6053333333333333,
      "grad_norm": 0.056406114250421524,
      "learning_rate": 2.746666666666667e-05,
      "loss": 0.0017,
      "step": 67600
    },
    {
      "epoch": 3.6058666666666666,
      "grad_norm": 0.19740943610668182,
      "learning_rate": 2.7463333333333335e-05,
      "loss": 0.0028,
      "step": 67610
    },
    {
      "epoch": 3.6064,
      "grad_norm": 0.3384143114089966,
      "learning_rate": 2.746e-05,
      "loss": 0.0028,
      "step": 67620
    },
    {
      "epoch": 3.606933333333333,
      "grad_norm": 0.0846099779009819,
      "learning_rate": 2.745666666666667e-05,
      "loss": 0.0034,
      "step": 67630
    },
    {
      "epoch": 3.607466666666667,
      "grad_norm": 0.16920767724514008,
      "learning_rate": 2.7453333333333337e-05,
      "loss": 0.0033,
      "step": 67640
    },
    {
      "epoch": 3.608,
      "grad_norm": 0.36664560437202454,
      "learning_rate": 2.7450000000000003e-05,
      "loss": 0.0043,
      "step": 67650
    },
    {
      "epoch": 3.6085333333333334,
      "grad_norm": 0.1410093605518341,
      "learning_rate": 2.744666666666667e-05,
      "loss": 0.0032,
      "step": 67660
    },
    {
      "epoch": 3.6090666666666666,
      "grad_norm": 0.19740846753120422,
      "learning_rate": 2.7443333333333332e-05,
      "loss": 0.0048,
      "step": 67670
    },
    {
      "epoch": 3.6096,
      "grad_norm": 0.16920968890190125,
      "learning_rate": 2.7439999999999998e-05,
      "loss": 0.0029,
      "step": 67680
    },
    {
      "epoch": 3.610133333333333,
      "grad_norm": 2.544973787266258e-09,
      "learning_rate": 2.7436666666666668e-05,
      "loss": 0.0025,
      "step": 67690
    },
    {
      "epoch": 3.610666666666667,
      "grad_norm": 0.028202621266245842,
      "learning_rate": 2.7433333333333334e-05,
      "loss": 0.0024,
      "step": 67700
    },
    {
      "epoch": 3.6112,
      "grad_norm": 0.25380152463912964,
      "learning_rate": 2.743e-05,
      "loss": 0.0029,
      "step": 67710
    },
    {
      "epoch": 3.6117333333333335,
      "grad_norm": 0.42301759123802185,
      "learning_rate": 2.7426666666666666e-05,
      "loss": 0.0027,
      "step": 67720
    },
    {
      "epoch": 3.6122666666666667,
      "grad_norm": 0.028201788663864136,
      "learning_rate": 2.7423333333333336e-05,
      "loss": 0.0035,
      "step": 67730
    },
    {
      "epoch": 3.6128,
      "grad_norm": 0.028201205655932426,
      "learning_rate": 2.7420000000000002e-05,
      "loss": 0.003,
      "step": 67740
    },
    {
      "epoch": 3.6133333333333333,
      "grad_norm": 0.8697483539581299,
      "learning_rate": 2.7416666666666668e-05,
      "loss": 0.0038,
      "step": 67750
    },
    {
      "epoch": 3.6138666666666666,
      "grad_norm": 0.16921135783195496,
      "learning_rate": 2.7413333333333334e-05,
      "loss": 0.0027,
      "step": 67760
    },
    {
      "epoch": 3.6144,
      "grad_norm": 0.028200773522257805,
      "learning_rate": 2.7410000000000004e-05,
      "loss": 0.0027,
      "step": 67770
    },
    {
      "epoch": 3.614933333333333,
      "grad_norm": 0.14100679755210876,
      "learning_rate": 2.740666666666667e-05,
      "loss": 0.0027,
      "step": 67780
    },
    {
      "epoch": 3.615466666666667,
      "grad_norm": 0.22560939192771912,
      "learning_rate": 2.7403333333333336e-05,
      "loss": 0.0026,
      "step": 67790
    },
    {
      "epoch": 3.616,
      "grad_norm": 0.28202712535858154,
      "learning_rate": 2.7400000000000002e-05,
      "loss": 0.003,
      "step": 67800
    },
    {
      "epoch": 3.6165333333333334,
      "grad_norm": 0.062024038285017014,
      "learning_rate": 2.739666666666667e-05,
      "loss": 0.0026,
      "step": 67810
    },
    {
      "epoch": 3.6170666666666667,
      "grad_norm": 0.1410132646560669,
      "learning_rate": 2.739333333333333e-05,
      "loss": 0.0027,
      "step": 67820
    },
    {
      "epoch": 3.6176,
      "grad_norm": 1.498692725476758e-09,
      "learning_rate": 2.739e-05,
      "loss": 0.0037,
      "step": 67830
    },
    {
      "epoch": 3.618133333333333,
      "grad_norm": 0.028200499713420868,
      "learning_rate": 2.7386666666666666e-05,
      "loss": 0.003,
      "step": 67840
    },
    {
      "epoch": 3.618666666666667,
      "grad_norm": 0.05640138313174248,
      "learning_rate": 2.7383333333333333e-05,
      "loss": 0.0024,
      "step": 67850
    },
    {
      "epoch": 3.6192,
      "grad_norm": 0.05640173703432083,
      "learning_rate": 2.738e-05,
      "loss": 0.0024,
      "step": 67860
    },
    {
      "epoch": 3.6197333333333335,
      "grad_norm": 0.11280843615531921,
      "learning_rate": 2.7376666666666668e-05,
      "loss": 0.0031,
      "step": 67870
    },
    {
      "epoch": 3.6202666666666667,
      "grad_norm": 1.6280430337189955e-09,
      "learning_rate": 2.7373333333333334e-05,
      "loss": 0.0041,
      "step": 67880
    },
    {
      "epoch": 3.6208,
      "grad_norm": 0.3665943145751953,
      "learning_rate": 2.737e-05,
      "loss": 0.0029,
      "step": 67890
    },
    {
      "epoch": 3.6213333333333333,
      "grad_norm": 0.11281048506498337,
      "learning_rate": 2.7366666666666667e-05,
      "loss": 0.0021,
      "step": 67900
    },
    {
      "epoch": 3.6218666666666666,
      "grad_norm": 0.08460500836372375,
      "learning_rate": 2.7363333333333336e-05,
      "loss": 0.0027,
      "step": 67910
    },
    {
      "epoch": 3.6224,
      "grad_norm": 0.11280366033315659,
      "learning_rate": 2.7360000000000002e-05,
      "loss": 0.0029,
      "step": 67920
    },
    {
      "epoch": 3.622933333333333,
      "grad_norm": 1.6809852398935732e-09,
      "learning_rate": 2.735666666666667e-05,
      "loss": 0.0019,
      "step": 67930
    },
    {
      "epoch": 3.6234666666666664,
      "grad_norm": 0.11280164122581482,
      "learning_rate": 2.7353333333333338e-05,
      "loss": 0.0023,
      "step": 67940
    },
    {
      "epoch": 3.624,
      "grad_norm": 0.22560764849185944,
      "learning_rate": 2.7350000000000004e-05,
      "loss": 0.0016,
      "step": 67950
    },
    {
      "epoch": 3.6245333333333334,
      "grad_norm": 0.31020721793174744,
      "learning_rate": 2.734666666666667e-05,
      "loss": 0.0024,
      "step": 67960
    },
    {
      "epoch": 3.6250666666666667,
      "grad_norm": 0.19740986824035645,
      "learning_rate": 2.7343333333333333e-05,
      "loss": 0.0021,
      "step": 67970
    },
    {
      "epoch": 3.6256,
      "grad_norm": 0.3102193772792816,
      "learning_rate": 2.734e-05,
      "loss": 0.0026,
      "step": 67980
    },
    {
      "epoch": 3.626133333333333,
      "grad_norm": 0.4512084126472473,
      "learning_rate": 2.7336666666666665e-05,
      "loss": 0.0028,
      "step": 67990
    },
    {
      "epoch": 3.626666666666667,
      "grad_norm": 0.08460083603858948,
      "learning_rate": 2.733333333333333e-05,
      "loss": 0.0024,
      "step": 68000
    },
    {
      "epoch": 3.6272,
      "grad_norm": 0.25381746888160706,
      "learning_rate": 2.733e-05,
      "loss": 0.0031,
      "step": 68010
    },
    {
      "epoch": 3.6277333333333335,
      "grad_norm": 0.056399453431367874,
      "learning_rate": 2.7326666666666667e-05,
      "loss": 0.0032,
      "step": 68020
    },
    {
      "epoch": 3.6282666666666668,
      "grad_norm": 0.19741502404212952,
      "learning_rate": 2.7323333333333333e-05,
      "loss": 0.0024,
      "step": 68030
    },
    {
      "epoch": 3.6288,
      "grad_norm": 0.25379636883735657,
      "learning_rate": 2.7320000000000003e-05,
      "loss": 0.0027,
      "step": 68040
    },
    {
      "epoch": 3.6293333333333333,
      "grad_norm": 0.25382161140441895,
      "learning_rate": 2.731666666666667e-05,
      "loss": 0.0023,
      "step": 68050
    },
    {
      "epoch": 3.6298666666666666,
      "grad_norm": 0.22559770941734314,
      "learning_rate": 2.7313333333333335e-05,
      "loss": 0.0056,
      "step": 68060
    },
    {
      "epoch": 3.6304,
      "grad_norm": 0.22560492157936096,
      "learning_rate": 2.731e-05,
      "loss": 0.006,
      "step": 68070
    },
    {
      "epoch": 3.630933333333333,
      "grad_norm": 0.02820051833987236,
      "learning_rate": 2.730666666666667e-05,
      "loss": 0.0037,
      "step": 68080
    },
    {
      "epoch": 3.6314666666666664,
      "grad_norm": 0.05639803409576416,
      "learning_rate": 2.7303333333333337e-05,
      "loss": 0.0041,
      "step": 68090
    },
    {
      "epoch": 3.632,
      "grad_norm": 0.3666031062602997,
      "learning_rate": 2.7300000000000003e-05,
      "loss": 0.0016,
      "step": 68100
    },
    {
      "epoch": 3.6325333333333334,
      "grad_norm": 0.028199132531881332,
      "learning_rate": 2.729666666666667e-05,
      "loss": 0.0041,
      "step": 68110
    },
    {
      "epoch": 3.6330666666666667,
      "grad_norm": 0.05639995262026787,
      "learning_rate": 2.7293333333333332e-05,
      "loss": 0.0036,
      "step": 68120
    },
    {
      "epoch": 3.6336,
      "grad_norm": 0.08459804207086563,
      "learning_rate": 2.7289999999999998e-05,
      "loss": 0.002,
      "step": 68130
    },
    {
      "epoch": 3.634133333333333,
      "grad_norm": 0.2820073366165161,
      "learning_rate": 2.7286666666666667e-05,
      "loss": 0.0036,
      "step": 68140
    },
    {
      "epoch": 3.634666666666667,
      "grad_norm": 0.3383868932723999,
      "learning_rate": 2.7283333333333334e-05,
      "loss": 0.002,
      "step": 68150
    },
    {
      "epoch": 3.6352,
      "grad_norm": 0.19740523397922516,
      "learning_rate": 2.728e-05,
      "loss": 0.0022,
      "step": 68160
    },
    {
      "epoch": 3.6357333333333335,
      "grad_norm": 0.05639702081680298,
      "learning_rate": 2.7276666666666666e-05,
      "loss": 0.0032,
      "step": 68170
    },
    {
      "epoch": 3.6362666666666668,
      "grad_norm": 0.22559352219104767,
      "learning_rate": 2.7273333333333335e-05,
      "loss": 0.0024,
      "step": 68180
    },
    {
      "epoch": 3.6368,
      "grad_norm": 0.05639676749706268,
      "learning_rate": 2.727e-05,
      "loss": 0.0031,
      "step": 68190
    },
    {
      "epoch": 3.6373333333333333,
      "grad_norm": 0.14100120961666107,
      "learning_rate": 2.7266666666666668e-05,
      "loss": 0.0023,
      "step": 68200
    },
    {
      "epoch": 3.6378666666666666,
      "grad_norm": 0.14100311696529388,
      "learning_rate": 2.7263333333333334e-05,
      "loss": 0.0031,
      "step": 68210
    },
    {
      "epoch": 3.6384,
      "grad_norm": 0.4229549169540405,
      "learning_rate": 2.7260000000000003e-05,
      "loss": 0.0033,
      "step": 68220
    },
    {
      "epoch": 3.638933333333333,
      "grad_norm": 0.22560849785804749,
      "learning_rate": 2.725666666666667e-05,
      "loss": 0.0024,
      "step": 68230
    },
    {
      "epoch": 3.6394666666666664,
      "grad_norm": 0.2255820780992508,
      "learning_rate": 2.7253333333333336e-05,
      "loss": 0.0025,
      "step": 68240
    },
    {
      "epoch": 3.64,
      "grad_norm": 0.338406503200531,
      "learning_rate": 2.725e-05,
      "loss": 0.0022,
      "step": 68250
    },
    {
      "epoch": 3.6405333333333334,
      "grad_norm": 0.31016772985458374,
      "learning_rate": 2.724666666666667e-05,
      "loss": 0.0035,
      "step": 68260
    },
    {
      "epoch": 3.6410666666666667,
      "grad_norm": 0.08459392935037613,
      "learning_rate": 2.7243333333333337e-05,
      "loss": 0.002,
      "step": 68270
    },
    {
      "epoch": 3.6416,
      "grad_norm": 0.1409926563501358,
      "learning_rate": 2.724e-05,
      "loss": 0.0027,
      "step": 68280
    },
    {
      "epoch": 3.6421333333333332,
      "grad_norm": 0.31018179655075073,
      "learning_rate": 2.7236666666666666e-05,
      "loss": 0.0028,
      "step": 68290
    },
    {
      "epoch": 3.642666666666667,
      "grad_norm": 0.22557608783245087,
      "learning_rate": 2.7233333333333332e-05,
      "loss": 0.0025,
      "step": 68300
    },
    {
      "epoch": 3.6432,
      "grad_norm": 0.08459470421075821,
      "learning_rate": 2.723e-05,
      "loss": 0.0032,
      "step": 68310
    },
    {
      "epoch": 3.6437333333333335,
      "grad_norm": 0.22557418048381805,
      "learning_rate": 2.7226666666666668e-05,
      "loss": 0.0023,
      "step": 68320
    },
    {
      "epoch": 3.6442666666666668,
      "grad_norm": 0.08459601551294327,
      "learning_rate": 2.7223333333333334e-05,
      "loss": 0.003,
      "step": 68330
    },
    {
      "epoch": 3.6448,
      "grad_norm": 0.19737890362739563,
      "learning_rate": 2.722e-05,
      "loss": 0.0028,
      "step": 68340
    },
    {
      "epoch": 3.6453333333333333,
      "grad_norm": 0.3101785480976105,
      "learning_rate": 2.7216666666666666e-05,
      "loss": 0.0033,
      "step": 68350
    },
    {
      "epoch": 3.6458666666666666,
      "grad_norm": 0.28199175000190735,
      "learning_rate": 2.7213333333333336e-05,
      "loss": 0.0031,
      "step": 68360
    },
    {
      "epoch": 3.6464,
      "grad_norm": 0.25376901030540466,
      "learning_rate": 2.7210000000000002e-05,
      "loss": 0.0024,
      "step": 68370
    },
    {
      "epoch": 3.646933333333333,
      "grad_norm": 0.2255832403898239,
      "learning_rate": 2.7206666666666668e-05,
      "loss": 0.0021,
      "step": 68380
    },
    {
      "epoch": 3.6474666666666664,
      "grad_norm": 0.39475324749946594,
      "learning_rate": 2.7203333333333338e-05,
      "loss": 0.003,
      "step": 68390
    },
    {
      "epoch": 3.648,
      "grad_norm": 0.056393902748823166,
      "learning_rate": 2.7200000000000004e-05,
      "loss": 0.0021,
      "step": 68400
    },
    {
      "epoch": 3.6485333333333334,
      "grad_norm": 0.05639571323990822,
      "learning_rate": 2.719666666666667e-05,
      "loss": 0.0035,
      "step": 68410
    },
    {
      "epoch": 3.6490666666666667,
      "grad_norm": 0.16919425129890442,
      "learning_rate": 2.7193333333333336e-05,
      "loss": 0.0039,
      "step": 68420
    },
    {
      "epoch": 3.6496,
      "grad_norm": 0.42293548583984375,
      "learning_rate": 2.719e-05,
      "loss": 0.0027,
      "step": 68430
    },
    {
      "epoch": 3.6501333333333332,
      "grad_norm": 0.05639418587088585,
      "learning_rate": 2.7186666666666665e-05,
      "loss": 0.0039,
      "step": 68440
    },
    {
      "epoch": 3.6506666666666665,
      "grad_norm": 0.02819676324725151,
      "learning_rate": 2.7183333333333335e-05,
      "loss": 0.0036,
      "step": 68450
    },
    {
      "epoch": 3.6512000000000002,
      "grad_norm": 0.22557300329208374,
      "learning_rate": 2.718e-05,
      "loss": 0.0024,
      "step": 68460
    },
    {
      "epoch": 3.6517333333333335,
      "grad_norm": 0.25376245379447937,
      "learning_rate": 2.7176666666666667e-05,
      "loss": 0.002,
      "step": 68470
    },
    {
      "epoch": 3.6522666666666668,
      "grad_norm": 0.028195949271321297,
      "learning_rate": 2.7173333333333333e-05,
      "loss": 0.0029,
      "step": 68480
    },
    {
      "epoch": 3.6528,
      "grad_norm": 0.253787100315094,
      "learning_rate": 2.7170000000000002e-05,
      "loss": 0.0029,
      "step": 68490
    },
    {
      "epoch": 3.6533333333333333,
      "grad_norm": 0.25376155972480774,
      "learning_rate": 2.716666666666667e-05,
      "loss": 0.0037,
      "step": 68500
    },
    {
      "epoch": 3.6538666666666666,
      "grad_norm": 0.36657872796058655,
      "learning_rate": 2.7163333333333335e-05,
      "loss": 0.0027,
      "step": 68510
    },
    {
      "epoch": 3.6544,
      "grad_norm": 0.08459241688251495,
      "learning_rate": 2.716e-05,
      "loss": 0.0028,
      "step": 68520
    },
    {
      "epoch": 3.654933333333333,
      "grad_norm": 0.05639062076807022,
      "learning_rate": 2.715666666666667e-05,
      "loss": 0.0045,
      "step": 68530
    },
    {
      "epoch": 3.6554666666666664,
      "grad_norm": 0.05639380216598511,
      "learning_rate": 2.7153333333333337e-05,
      "loss": 0.0038,
      "step": 68540
    },
    {
      "epoch": 3.656,
      "grad_norm": 0.11278093606233597,
      "learning_rate": 2.7150000000000003e-05,
      "loss": 0.0039,
      "step": 68550
    },
    {
      "epoch": 3.6565333333333334,
      "grad_norm": 0.16917604207992554,
      "learning_rate": 2.714666666666667e-05,
      "loss": 0.0028,
      "step": 68560
    },
    {
      "epoch": 3.6570666666666667,
      "grad_norm": 0.08458694815635681,
      "learning_rate": 2.7143333333333338e-05,
      "loss": 0.0042,
      "step": 68570
    },
    {
      "epoch": 3.6576,
      "grad_norm": 0.2255716174840927,
      "learning_rate": 2.7139999999999998e-05,
      "loss": 0.0022,
      "step": 68580
    },
    {
      "epoch": 3.6581333333333332,
      "grad_norm": 0.3101714551448822,
      "learning_rate": 2.7136666666666667e-05,
      "loss": 0.0029,
      "step": 68590
    },
    {
      "epoch": 3.6586666666666665,
      "grad_norm": 0.14097699522972107,
      "learning_rate": 2.7133333333333333e-05,
      "loss": 0.0035,
      "step": 68600
    },
    {
      "epoch": 3.6592000000000002,
      "grad_norm": 0.1973714977502823,
      "learning_rate": 2.713e-05,
      "loss": 0.0021,
      "step": 68610
    },
    {
      "epoch": 3.6597333333333335,
      "grad_norm": 0.11278902739286423,
      "learning_rate": 2.7126666666666666e-05,
      "loss": 0.0031,
      "step": 68620
    },
    {
      "epoch": 3.660266666666667,
      "grad_norm": 0.19735658168792725,
      "learning_rate": 2.7123333333333335e-05,
      "loss": 0.0022,
      "step": 68630
    },
    {
      "epoch": 3.6608,
      "grad_norm": 0.19737330079078674,
      "learning_rate": 2.712e-05,
      "loss": 0.0033,
      "step": 68640
    },
    {
      "epoch": 3.6613333333333333,
      "grad_norm": 0.16916850209236145,
      "learning_rate": 2.7116666666666667e-05,
      "loss": 0.0041,
      "step": 68650
    },
    {
      "epoch": 3.6618666666666666,
      "grad_norm": 0.19736135005950928,
      "learning_rate": 2.7113333333333333e-05,
      "loss": 0.0034,
      "step": 68660
    },
    {
      "epoch": 3.6624,
      "grad_norm": 0.05638829246163368,
      "learning_rate": 2.7110000000000003e-05,
      "loss": 0.0027,
      "step": 68670
    },
    {
      "epoch": 3.662933333333333,
      "grad_norm": 0.028195617720484734,
      "learning_rate": 2.710666666666667e-05,
      "loss": 0.0022,
      "step": 68680
    },
    {
      "epoch": 3.6634666666666664,
      "grad_norm": 0.02819489873945713,
      "learning_rate": 2.7103333333333335e-05,
      "loss": 0.0029,
      "step": 68690
    },
    {
      "epoch": 3.664,
      "grad_norm": 0.25377464294433594,
      "learning_rate": 2.7100000000000005e-05,
      "loss": 0.0031,
      "step": 68700
    },
    {
      "epoch": 3.6645333333333334,
      "grad_norm": 0.22555126249790192,
      "learning_rate": 2.709666666666667e-05,
      "loss": 0.0036,
      "step": 68710
    },
    {
      "epoch": 3.6650666666666667,
      "grad_norm": 0.14098216593265533,
      "learning_rate": 2.7093333333333337e-05,
      "loss": 0.0026,
      "step": 68720
    },
    {
      "epoch": 3.6656,
      "grad_norm": 0.056387145072221756,
      "learning_rate": 2.709e-05,
      "loss": 0.0019,
      "step": 68730
    },
    {
      "epoch": 3.6661333333333332,
      "grad_norm": 3.4119991276782002e-09,
      "learning_rate": 2.7086666666666666e-05,
      "loss": 0.0037,
      "step": 68740
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 0.1973562091588974,
      "learning_rate": 2.7083333333333332e-05,
      "loss": 0.0026,
      "step": 68750
    },
    {
      "epoch": 3.6672000000000002,
      "grad_norm": 0.3101416230201721,
      "learning_rate": 2.7079999999999998e-05,
      "loss": 0.0024,
      "step": 68760
    },
    {
      "epoch": 3.6677333333333335,
      "grad_norm": 0.11277991533279419,
      "learning_rate": 2.7076666666666668e-05,
      "loss": 0.0024,
      "step": 68770
    },
    {
      "epoch": 3.668266666666667,
      "grad_norm": 0.11277937889099121,
      "learning_rate": 2.7073333333333334e-05,
      "loss": 0.0027,
      "step": 68780
    },
    {
      "epoch": 3.6688,
      "grad_norm": 1.9904373704804357e-09,
      "learning_rate": 2.707e-05,
      "loss": 0.0038,
      "step": 68790
    },
    {
      "epoch": 3.6693333333333333,
      "grad_norm": 0.3665335774421692,
      "learning_rate": 2.706666666666667e-05,
      "loss": 0.0022,
      "step": 68800
    },
    {
      "epoch": 3.6698666666666666,
      "grad_norm": 3.787564040180769e-09,
      "learning_rate": 2.7063333333333336e-05,
      "loss": 0.0032,
      "step": 68810
    },
    {
      "epoch": 3.6704,
      "grad_norm": 0.056385986506938934,
      "learning_rate": 2.7060000000000002e-05,
      "loss": 0.0026,
      "step": 68820
    },
    {
      "epoch": 3.670933333333333,
      "grad_norm": 0.3665319085121155,
      "learning_rate": 2.7056666666666668e-05,
      "loss": 0.0029,
      "step": 68830
    },
    {
      "epoch": 3.6714666666666664,
      "grad_norm": 0.05638695880770683,
      "learning_rate": 2.7053333333333337e-05,
      "loss": 0.0029,
      "step": 68840
    },
    {
      "epoch": 3.672,
      "grad_norm": 0.22555065155029297,
      "learning_rate": 2.7050000000000004e-05,
      "loss": 0.0023,
      "step": 68850
    },
    {
      "epoch": 3.6725333333333334,
      "grad_norm": 0.39471641182899475,
      "learning_rate": 2.704666666666667e-05,
      "loss": 0.0035,
      "step": 68860
    },
    {
      "epoch": 3.6730666666666667,
      "grad_norm": 0.36653169989585876,
      "learning_rate": 2.7043333333333336e-05,
      "loss": 0.0027,
      "step": 68870
    },
    {
      "epoch": 3.6736,
      "grad_norm": 0.16915318369865417,
      "learning_rate": 2.704e-05,
      "loss": 0.0024,
      "step": 68880
    },
    {
      "epoch": 3.6741333333333333,
      "grad_norm": 0.19735203683376312,
      "learning_rate": 2.7036666666666665e-05,
      "loss": 0.0017,
      "step": 68890
    },
    {
      "epoch": 3.6746666666666665,
      "grad_norm": 0.1409604251384735,
      "learning_rate": 2.7033333333333334e-05,
      "loss": 0.0027,
      "step": 68900
    },
    {
      "epoch": 3.6752000000000002,
      "grad_norm": 0.11277235299348831,
      "learning_rate": 2.703e-05,
      "loss": 0.0042,
      "step": 68910
    },
    {
      "epoch": 3.6757333333333335,
      "grad_norm": 0.02819289080798626,
      "learning_rate": 2.7026666666666667e-05,
      "loss": 0.0029,
      "step": 68920
    },
    {
      "epoch": 3.676266666666667,
      "grad_norm": 0.14183858036994934,
      "learning_rate": 2.7023333333333333e-05,
      "loss": 0.0021,
      "step": 68930
    },
    {
      "epoch": 3.6768,
      "grad_norm": 0.08458380401134491,
      "learning_rate": 2.7020000000000002e-05,
      "loss": 0.0047,
      "step": 68940
    },
    {
      "epoch": 3.6773333333333333,
      "grad_norm": 0.22553284466266632,
      "learning_rate": 2.701666666666667e-05,
      "loss": 0.0023,
      "step": 68950
    },
    {
      "epoch": 3.6778666666666666,
      "grad_norm": 0.25372251868247986,
      "learning_rate": 2.7013333333333334e-05,
      "loss": 0.0035,
      "step": 68960
    },
    {
      "epoch": 3.6784,
      "grad_norm": 0.11277325451374054,
      "learning_rate": 2.701e-05,
      "loss": 0.0026,
      "step": 68970
    },
    {
      "epoch": 3.678933333333333,
      "grad_norm": 0.028191419318318367,
      "learning_rate": 2.700666666666667e-05,
      "loss": 0.0025,
      "step": 68980
    },
    {
      "epoch": 3.6794666666666664,
      "grad_norm": 0.19735373556613922,
      "learning_rate": 2.7003333333333336e-05,
      "loss": 0.0035,
      "step": 68990
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.3101227581501007,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.0029,
      "step": 69000
    },
    {
      "epoch": 3.6805333333333334,
      "grad_norm": 0.2537365257740021,
      "learning_rate": 2.699666666666667e-05,
      "loss": 0.0027,
      "step": 69010
    },
    {
      "epoch": 3.6810666666666667,
      "grad_norm": 0.14096274971961975,
      "learning_rate": 2.6993333333333338e-05,
      "loss": 0.0045,
      "step": 69020
    },
    {
      "epoch": 3.6816,
      "grad_norm": 0.11276917904615402,
      "learning_rate": 2.6989999999999997e-05,
      "loss": 0.0025,
      "step": 69030
    },
    {
      "epoch": 3.6821333333333333,
      "grad_norm": 0.11276875436306,
      "learning_rate": 2.6986666666666667e-05,
      "loss": 0.0026,
      "step": 69040
    },
    {
      "epoch": 3.6826666666666665,
      "grad_norm": 0.16915875673294067,
      "learning_rate": 2.6983333333333333e-05,
      "loss": 0.0034,
      "step": 69050
    },
    {
      "epoch": 3.6832000000000003,
      "grad_norm": 0.05638191103935242,
      "learning_rate": 2.698e-05,
      "loss": 0.0032,
      "step": 69060
    },
    {
      "epoch": 3.6837333333333335,
      "grad_norm": 0.19734808802604675,
      "learning_rate": 2.6976666666666665e-05,
      "loss": 0.0029,
      "step": 69070
    },
    {
      "epoch": 3.684266666666667,
      "grad_norm": 0.11397043615579605,
      "learning_rate": 2.6973333333333335e-05,
      "loss": 0.0029,
      "step": 69080
    },
    {
      "epoch": 3.6848,
      "grad_norm": 0.7209453582763672,
      "learning_rate": 2.697e-05,
      "loss": 0.0033,
      "step": 69090
    },
    {
      "epoch": 3.6853333333333333,
      "grad_norm": 0.11276992410421371,
      "learning_rate": 2.6966666666666667e-05,
      "loss": 0.0036,
      "step": 69100
    },
    {
      "epoch": 3.6858666666666666,
      "grad_norm": 0.3100832402706146,
      "learning_rate": 2.6963333333333333e-05,
      "loss": 0.0022,
      "step": 69110
    },
    {
      "epoch": 3.6864,
      "grad_norm": 0.14095844328403473,
      "learning_rate": 2.6960000000000003e-05,
      "loss": 0.0036,
      "step": 69120
    },
    {
      "epoch": 3.686933333333333,
      "grad_norm": 1.2331794474462754e-09,
      "learning_rate": 2.695666666666667e-05,
      "loss": 0.0043,
      "step": 69130
    },
    {
      "epoch": 3.6874666666666664,
      "grad_norm": 2.474828564302811e-09,
      "learning_rate": 2.6953333333333335e-05,
      "loss": 0.0026,
      "step": 69140
    },
    {
      "epoch": 3.6879999999999997,
      "grad_norm": 0.05638238415122032,
      "learning_rate": 2.6950000000000005e-05,
      "loss": 0.002,
      "step": 69150
    },
    {
      "epoch": 3.6885333333333334,
      "grad_norm": 0.19732660055160522,
      "learning_rate": 2.694666666666667e-05,
      "loss": 0.0029,
      "step": 69160
    },
    {
      "epoch": 3.6890666666666667,
      "grad_norm": 0.4512675404548645,
      "learning_rate": 2.6943333333333337e-05,
      "loss": 0.0037,
      "step": 69170
    },
    {
      "epoch": 3.6896,
      "grad_norm": 0.11275944858789444,
      "learning_rate": 2.694e-05,
      "loss": 0.0037,
      "step": 69180
    },
    {
      "epoch": 3.6901333333333333,
      "grad_norm": 0.16914062201976776,
      "learning_rate": 2.6936666666666666e-05,
      "loss": 0.0029,
      "step": 69190
    },
    {
      "epoch": 3.6906666666666665,
      "grad_norm": 0.2537097632884979,
      "learning_rate": 2.6933333333333332e-05,
      "loss": 0.0028,
      "step": 69200
    },
    {
      "epoch": 3.6912000000000003,
      "grad_norm": 0.2255236655473709,
      "learning_rate": 2.693e-05,
      "loss": 0.0028,
      "step": 69210
    },
    {
      "epoch": 3.6917333333333335,
      "grad_norm": 0.14095285534858704,
      "learning_rate": 2.6926666666666667e-05,
      "loss": 0.0032,
      "step": 69220
    },
    {
      "epoch": 3.692266666666667,
      "grad_norm": 4.880122084216509e-09,
      "learning_rate": 2.6923333333333334e-05,
      "loss": 0.0028,
      "step": 69230
    },
    {
      "epoch": 3.6928,
      "grad_norm": 0.0563797689974308,
      "learning_rate": 2.692e-05,
      "loss": 0.0026,
      "step": 69240
    },
    {
      "epoch": 3.6933333333333334,
      "grad_norm": 0.028191300109028816,
      "learning_rate": 2.691666666666667e-05,
      "loss": 0.0031,
      "step": 69250
    },
    {
      "epoch": 3.6938666666666666,
      "grad_norm": 0.08456659317016602,
      "learning_rate": 2.6913333333333335e-05,
      "loss": 0.0021,
      "step": 69260
    },
    {
      "epoch": 3.6944,
      "grad_norm": 0.36648574471473694,
      "learning_rate": 2.691e-05,
      "loss": 0.0029,
      "step": 69270
    },
    {
      "epoch": 3.694933333333333,
      "grad_norm": 0.11275467276573181,
      "learning_rate": 2.6906666666666668e-05,
      "loss": 0.0029,
      "step": 69280
    },
    {
      "epoch": 3.6954666666666665,
      "grad_norm": 0.1691385954618454,
      "learning_rate": 2.6903333333333337e-05,
      "loss": 0.0035,
      "step": 69290
    },
    {
      "epoch": 3.6959999999999997,
      "grad_norm": 0.1973266750574112,
      "learning_rate": 2.6900000000000003e-05,
      "loss": 0.003,
      "step": 69300
    },
    {
      "epoch": 3.6965333333333334,
      "grad_norm": 0.08456858992576599,
      "learning_rate": 2.689666666666667e-05,
      "loss": 0.0027,
      "step": 69310
    },
    {
      "epoch": 3.6970666666666667,
      "grad_norm": 0.33826714754104614,
      "learning_rate": 2.6893333333333336e-05,
      "loss": 0.0033,
      "step": 69320
    },
    {
      "epoch": 3.6976,
      "grad_norm": 3.3967086920938527e-09,
      "learning_rate": 2.689e-05,
      "loss": 0.0032,
      "step": 69330
    },
    {
      "epoch": 3.6981333333333333,
      "grad_norm": 0.25369057059288025,
      "learning_rate": 2.6886666666666664e-05,
      "loss": 0.0023,
      "step": 69340
    },
    {
      "epoch": 3.6986666666666665,
      "grad_norm": 0.19732631742954254,
      "learning_rate": 2.6883333333333334e-05,
      "loss": 0.0051,
      "step": 69350
    },
    {
      "epoch": 3.6992000000000003,
      "grad_norm": 0.056381236761808395,
      "learning_rate": 2.688e-05,
      "loss": 0.0021,
      "step": 69360
    },
    {
      "epoch": 3.6997333333333335,
      "grad_norm": 0.31009307503700256,
      "learning_rate": 2.6876666666666666e-05,
      "loss": 0.0015,
      "step": 69370
    },
    {
      "epoch": 3.700266666666667,
      "grad_norm": 0.028188174590468407,
      "learning_rate": 2.6873333333333332e-05,
      "loss": 0.003,
      "step": 69380
    },
    {
      "epoch": 3.7008,
      "grad_norm": 0.4510243833065033,
      "learning_rate": 2.6870000000000002e-05,
      "loss": 0.0041,
      "step": 69390
    },
    {
      "epoch": 3.7013333333333334,
      "grad_norm": 0.11275596171617508,
      "learning_rate": 2.6866666666666668e-05,
      "loss": 0.0056,
      "step": 69400
    },
    {
      "epoch": 3.7018666666666666,
      "grad_norm": 2.041057323154405e-09,
      "learning_rate": 2.6863333333333334e-05,
      "loss": 0.0036,
      "step": 69410
    },
    {
      "epoch": 3.7024,
      "grad_norm": 0.11275051534175873,
      "learning_rate": 2.686e-05,
      "loss": 0.0017,
      "step": 69420
    },
    {
      "epoch": 3.702933333333333,
      "grad_norm": 0.33827662467956543,
      "learning_rate": 2.685666666666667e-05,
      "loss": 0.0025,
      "step": 69430
    },
    {
      "epoch": 3.7034666666666665,
      "grad_norm": 0.028189245611429214,
      "learning_rate": 2.6853333333333336e-05,
      "loss": 0.0028,
      "step": 69440
    },
    {
      "epoch": 3.7039999999999997,
      "grad_norm": 0.05637688189744949,
      "learning_rate": 2.6850000000000002e-05,
      "loss": 0.0049,
      "step": 69450
    },
    {
      "epoch": 3.7045333333333335,
      "grad_norm": 0.02818841114640236,
      "learning_rate": 2.684666666666667e-05,
      "loss": 0.002,
      "step": 69460
    },
    {
      "epoch": 3.7050666666666667,
      "grad_norm": 0.1409381628036499,
      "learning_rate": 2.6843333333333338e-05,
      "loss": 0.003,
      "step": 69470
    },
    {
      "epoch": 3.7056,
      "grad_norm": 0.028188582509756088,
      "learning_rate": 2.6840000000000004e-05,
      "loss": 0.0025,
      "step": 69480
    },
    {
      "epoch": 3.7061333333333333,
      "grad_norm": 0.08456568419933319,
      "learning_rate": 2.6836666666666667e-05,
      "loss": 0.0029,
      "step": 69490
    },
    {
      "epoch": 3.7066666666666666,
      "grad_norm": 0.16912837326526642,
      "learning_rate": 2.6833333333333333e-05,
      "loss": 0.0028,
      "step": 69500
    },
    {
      "epoch": 3.7072000000000003,
      "grad_norm": 0.11275885254144669,
      "learning_rate": 2.683e-05,
      "loss": 0.0021,
      "step": 69510
    },
    {
      "epoch": 3.7077333333333335,
      "grad_norm": 0.2818809151649475,
      "learning_rate": 2.6826666666666665e-05,
      "loss": 0.0038,
      "step": 69520
    },
    {
      "epoch": 3.708266666666667,
      "grad_norm": 0.19732904434204102,
      "learning_rate": 2.6823333333333335e-05,
      "loss": 0.0026,
      "step": 69530
    },
    {
      "epoch": 3.7088,
      "grad_norm": 0.058476485311985016,
      "learning_rate": 2.682e-05,
      "loss": 0.0037,
      "step": 69540
    },
    {
      "epoch": 3.7093333333333334,
      "grad_norm": 0.08456479012966156,
      "learning_rate": 2.6816666666666667e-05,
      "loss": 0.0035,
      "step": 69550
    },
    {
      "epoch": 3.7098666666666666,
      "grad_norm": 0.2536833882331848,
      "learning_rate": 2.6813333333333336e-05,
      "loss": 0.0036,
      "step": 69560
    },
    {
      "epoch": 3.7104,
      "grad_norm": 0.19731344282627106,
      "learning_rate": 2.6810000000000003e-05,
      "loss": 0.0033,
      "step": 69570
    },
    {
      "epoch": 3.710933333333333,
      "grad_norm": 0.16912636160850525,
      "learning_rate": 2.680666666666667e-05,
      "loss": 0.0032,
      "step": 69580
    },
    {
      "epoch": 3.7114666666666665,
      "grad_norm": 0.08456096053123474,
      "learning_rate": 2.6803333333333335e-05,
      "loss": 0.0032,
      "step": 69590
    },
    {
      "epoch": 3.7119999999999997,
      "grad_norm": 0.056376758962869644,
      "learning_rate": 2.6800000000000004e-05,
      "loss": 0.0027,
      "step": 69600
    },
    {
      "epoch": 3.7125333333333335,
      "grad_norm": 0.33824026584625244,
      "learning_rate": 2.679666666666667e-05,
      "loss": 0.0023,
      "step": 69610
    },
    {
      "epoch": 3.7130666666666667,
      "grad_norm": 0.1409398913383484,
      "learning_rate": 2.6793333333333337e-05,
      "loss": 0.0028,
      "step": 69620
    },
    {
      "epoch": 3.7136,
      "grad_norm": 0.3100499212741852,
      "learning_rate": 2.6790000000000003e-05,
      "loss": 0.0024,
      "step": 69630
    },
    {
      "epoch": 3.7141333333333333,
      "grad_norm": 0.08456405252218246,
      "learning_rate": 2.6786666666666665e-05,
      "loss": 0.0038,
      "step": 69640
    },
    {
      "epoch": 3.7146666666666666,
      "grad_norm": 0.056372400373220444,
      "learning_rate": 2.678333333333333e-05,
      "loss": 0.004,
      "step": 69650
    },
    {
      "epoch": 3.7152,
      "grad_norm": 0.1973072588443756,
      "learning_rate": 2.678e-05,
      "loss": 0.0035,
      "step": 69660
    },
    {
      "epoch": 3.7157333333333336,
      "grad_norm": 1.4999992359321368e-09,
      "learning_rate": 2.6776666666666667e-05,
      "loss": 0.0025,
      "step": 69670
    },
    {
      "epoch": 3.716266666666667,
      "grad_norm": 0.2536843717098236,
      "learning_rate": 2.6773333333333333e-05,
      "loss": 0.0025,
      "step": 69680
    },
    {
      "epoch": 3.7168,
      "grad_norm": 0.14093366265296936,
      "learning_rate": 2.677e-05,
      "loss": 0.0017,
      "step": 69690
    },
    {
      "epoch": 3.7173333333333334,
      "grad_norm": 0.16913099586963654,
      "learning_rate": 2.676666666666667e-05,
      "loss": 0.0035,
      "step": 69700
    },
    {
      "epoch": 3.7178666666666667,
      "grad_norm": 0.22549690306186676,
      "learning_rate": 2.6763333333333335e-05,
      "loss": 0.0024,
      "step": 69710
    },
    {
      "epoch": 3.7184,
      "grad_norm": 1.9829418107519814e-09,
      "learning_rate": 2.676e-05,
      "loss": 0.0021,
      "step": 69720
    },
    {
      "epoch": 3.718933333333333,
      "grad_norm": 0.11274649947881699,
      "learning_rate": 2.6756666666666667e-05,
      "loss": 0.003,
      "step": 69730
    },
    {
      "epoch": 3.7194666666666665,
      "grad_norm": 0.16911858320236206,
      "learning_rate": 2.6753333333333337e-05,
      "loss": 0.0027,
      "step": 69740
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 0.310057669878006,
      "learning_rate": 2.6750000000000003e-05,
      "loss": 0.0056,
      "step": 69750
    },
    {
      "epoch": 3.7205333333333335,
      "grad_norm": 0.16911932826042175,
      "learning_rate": 2.674666666666667e-05,
      "loss": 0.004,
      "step": 69760
    },
    {
      "epoch": 3.7210666666666667,
      "grad_norm": 0.05637352913618088,
      "learning_rate": 2.6743333333333335e-05,
      "loss": 0.0032,
      "step": 69770
    },
    {
      "epoch": 3.7216,
      "grad_norm": 0.2818606197834015,
      "learning_rate": 2.6740000000000005e-05,
      "loss": 0.0025,
      "step": 69780
    },
    {
      "epoch": 3.7221333333333333,
      "grad_norm": 0.028186818584799767,
      "learning_rate": 2.6736666666666664e-05,
      "loss": 0.003,
      "step": 69790
    },
    {
      "epoch": 3.7226666666666666,
      "grad_norm": 0.31005820631980896,
      "learning_rate": 2.6733333333333334e-05,
      "loss": 0.003,
      "step": 69800
    },
    {
      "epoch": 3.7232,
      "grad_norm": 0.05637282505631447,
      "learning_rate": 2.673e-05,
      "loss": 0.0025,
      "step": 69810
    },
    {
      "epoch": 3.7237333333333336,
      "grad_norm": 0.1973172426223755,
      "learning_rate": 2.6726666666666666e-05,
      "loss": 0.0046,
      "step": 69820
    },
    {
      "epoch": 3.724266666666667,
      "grad_norm": 0.22547926008701324,
      "learning_rate": 2.6723333333333332e-05,
      "loss": 0.0037,
      "step": 69830
    },
    {
      "epoch": 3.7248,
      "grad_norm": 0.028187325224280357,
      "learning_rate": 2.672e-05,
      "loss": 0.0042,
      "step": 69840
    },
    {
      "epoch": 3.7253333333333334,
      "grad_norm": 7.876705510057036e-09,
      "learning_rate": 2.6716666666666668e-05,
      "loss": 0.0028,
      "step": 69850
    },
    {
      "epoch": 3.7258666666666667,
      "grad_norm": 0.028184983879327774,
      "learning_rate": 2.6713333333333334e-05,
      "loss": 0.0027,
      "step": 69860
    },
    {
      "epoch": 3.7264,
      "grad_norm": 0.11274993419647217,
      "learning_rate": 2.671e-05,
      "loss": 0.0041,
      "step": 69870
    },
    {
      "epoch": 3.726933333333333,
      "grad_norm": 0.11274238675832748,
      "learning_rate": 2.670666666666667e-05,
      "loss": 0.0044,
      "step": 69880
    },
    {
      "epoch": 3.7274666666666665,
      "grad_norm": 0.05637060105800629,
      "learning_rate": 2.6703333333333336e-05,
      "loss": 0.0032,
      "step": 69890
    },
    {
      "epoch": 3.7279999999999998,
      "grad_norm": 0.05637089163064957,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 0.0034,
      "step": 69900
    },
    {
      "epoch": 3.7285333333333335,
      "grad_norm": 0.05637042596936226,
      "learning_rate": 2.669666666666667e-05,
      "loss": 0.0035,
      "step": 69910
    },
    {
      "epoch": 3.7290666666666668,
      "grad_norm": 0.11274172365665436,
      "learning_rate": 2.6693333333333338e-05,
      "loss": 0.0025,
      "step": 69920
    },
    {
      "epoch": 3.7296,
      "grad_norm": 0.22548043727874756,
      "learning_rate": 2.6690000000000004e-05,
      "loss": 0.004,
      "step": 69930
    },
    {
      "epoch": 3.7301333333333333,
      "grad_norm": 0.08455505222082138,
      "learning_rate": 2.6686666666666666e-05,
      "loss": 0.0032,
      "step": 69940
    },
    {
      "epoch": 3.7306666666666666,
      "grad_norm": 0.22548054158687592,
      "learning_rate": 2.6683333333333333e-05,
      "loss": 0.002,
      "step": 69950
    },
    {
      "epoch": 3.7312,
      "grad_norm": 0.2818647623062134,
      "learning_rate": 2.668e-05,
      "loss": 0.0023,
      "step": 69960
    },
    {
      "epoch": 3.7317333333333336,
      "grad_norm": 0.028185196220874786,
      "learning_rate": 2.6676666666666665e-05,
      "loss": 0.0027,
      "step": 69970
    },
    {
      "epoch": 3.732266666666667,
      "grad_norm": 0.08455286920070648,
      "learning_rate": 2.6673333333333334e-05,
      "loss": 0.0027,
      "step": 69980
    },
    {
      "epoch": 3.7328,
      "grad_norm": 0.19730331003665924,
      "learning_rate": 2.667e-05,
      "loss": 0.0027,
      "step": 69990
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 0.0563686303794384,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.0027,
      "step": 70000
    },
    {
      "epoch": 3.7338666666666667,
      "grad_norm": 0.2818506360054016,
      "learning_rate": 2.6663333333333336e-05,
      "loss": 0.0027,
      "step": 70010
    },
    {
      "epoch": 3.7344,
      "grad_norm": 0.05636958032846451,
      "learning_rate": 2.6660000000000002e-05,
      "loss": 0.0037,
      "step": 70020
    },
    {
      "epoch": 3.734933333333333,
      "grad_norm": 0.0281855296343565,
      "learning_rate": 2.665666666666667e-05,
      "loss": 0.0025,
      "step": 70030
    },
    {
      "epoch": 3.7354666666666665,
      "grad_norm": 0.028184058144688606,
      "learning_rate": 2.6653333333333335e-05,
      "loss": 0.0019,
      "step": 70040
    },
    {
      "epoch": 3.7359999999999998,
      "grad_norm": 0.22549040615558624,
      "learning_rate": 2.6650000000000004e-05,
      "loss": 0.0032,
      "step": 70050
    },
    {
      "epoch": 3.7365333333333335,
      "grad_norm": 0.2536557912826538,
      "learning_rate": 2.664666666666667e-05,
      "loss": 0.0036,
      "step": 70060
    },
    {
      "epoch": 3.7370666666666668,
      "grad_norm": 0.08455296605825424,
      "learning_rate": 2.6643333333333336e-05,
      "loss": 0.0028,
      "step": 70070
    },
    {
      "epoch": 3.7376,
      "grad_norm": 0.2254704236984253,
      "learning_rate": 2.6640000000000002e-05,
      "loss": 0.0035,
      "step": 70080
    },
    {
      "epoch": 3.7381333333333333,
      "grad_norm": 0.1691063791513443,
      "learning_rate": 2.6636666666666665e-05,
      "loss": 0.0029,
      "step": 70090
    },
    {
      "epoch": 3.7386666666666666,
      "grad_norm": 0.028184887021780014,
      "learning_rate": 2.663333333333333e-05,
      "loss": 0.003,
      "step": 70100
    },
    {
      "epoch": 3.7392,
      "grad_norm": 0.16910405457019806,
      "learning_rate": 2.663e-05,
      "loss": 0.0038,
      "step": 70110
    },
    {
      "epoch": 3.7397333333333336,
      "grad_norm": 0.11273224651813507,
      "learning_rate": 2.6626666666666667e-05,
      "loss": 0.003,
      "step": 70120
    },
    {
      "epoch": 3.740266666666667,
      "grad_norm": 3.557627303862887e-09,
      "learning_rate": 2.6623333333333333e-05,
      "loss": 0.0022,
      "step": 70130
    },
    {
      "epoch": 3.7408,
      "grad_norm": 0.19728340208530426,
      "learning_rate": 2.662e-05,
      "loss": 0.0018,
      "step": 70140
    },
    {
      "epoch": 3.7413333333333334,
      "grad_norm": 0.3382177948951721,
      "learning_rate": 2.661666666666667e-05,
      "loss": 0.0021,
      "step": 70150
    },
    {
      "epoch": 3.7418666666666667,
      "grad_norm": 0.3100123703479767,
      "learning_rate": 2.6613333333333335e-05,
      "loss": 0.0026,
      "step": 70160
    },
    {
      "epoch": 3.7424,
      "grad_norm": 0.14092287421226501,
      "learning_rate": 2.661e-05,
      "loss": 0.0031,
      "step": 70170
    },
    {
      "epoch": 3.7429333333333332,
      "grad_norm": 0.19728516042232513,
      "learning_rate": 2.6606666666666667e-05,
      "loss": 0.0029,
      "step": 70180
    },
    {
      "epoch": 3.7434666666666665,
      "grad_norm": 2.6608331094024607e-09,
      "learning_rate": 2.6603333333333337e-05,
      "loss": 0.0019,
      "step": 70190
    },
    {
      "epoch": 3.7439999999999998,
      "grad_norm": 0.6974452137947083,
      "learning_rate": 2.6600000000000003e-05,
      "loss": 0.0039,
      "step": 70200
    },
    {
      "epoch": 3.7445333333333335,
      "grad_norm": 0.2818373441696167,
      "learning_rate": 2.659666666666667e-05,
      "loss": 0.0022,
      "step": 70210
    },
    {
      "epoch": 3.7450666666666668,
      "grad_norm": 0.08455369621515274,
      "learning_rate": 2.6593333333333335e-05,
      "loss": 0.0024,
      "step": 70220
    },
    {
      "epoch": 3.7456,
      "grad_norm": 0.16910883784294128,
      "learning_rate": 2.6590000000000005e-05,
      "loss": 0.0041,
      "step": 70230
    },
    {
      "epoch": 3.7461333333333333,
      "grad_norm": 0.14092357456684113,
      "learning_rate": 2.6586666666666664e-05,
      "loss": 0.0023,
      "step": 70240
    },
    {
      "epoch": 3.7466666666666666,
      "grad_norm": 0.22545836865901947,
      "learning_rate": 2.6583333333333333e-05,
      "loss": 0.0032,
      "step": 70250
    },
    {
      "epoch": 3.7472,
      "grad_norm": 0.1691129505634308,
      "learning_rate": 2.658e-05,
      "loss": 0.0029,
      "step": 70260
    },
    {
      "epoch": 3.7477333333333336,
      "grad_norm": 0.19728350639343262,
      "learning_rate": 2.6576666666666666e-05,
      "loss": 0.004,
      "step": 70270
    },
    {
      "epoch": 3.748266666666667,
      "grad_norm": 0.25364765524864197,
      "learning_rate": 2.6573333333333332e-05,
      "loss": 0.002,
      "step": 70280
    },
    {
      "epoch": 3.7488,
      "grad_norm": 0.3100424110889435,
      "learning_rate": 2.657e-05,
      "loss": 0.0028,
      "step": 70290
    },
    {
      "epoch": 3.7493333333333334,
      "grad_norm": 0.3663637638092041,
      "learning_rate": 2.6566666666666668e-05,
      "loss": 0.0025,
      "step": 70300
    },
    {
      "epoch": 3.7498666666666667,
      "grad_norm": 1.0966403340972875e-09,
      "learning_rate": 2.6563333333333334e-05,
      "loss": 0.0031,
      "step": 70310
    },
    {
      "epoch": 3.7504,
      "grad_norm": 0.4509970545768738,
      "learning_rate": 2.6560000000000003e-05,
      "loss": 0.0028,
      "step": 70320
    },
    {
      "epoch": 3.7509333333333332,
      "grad_norm": 0.19729481637477875,
      "learning_rate": 2.655666666666667e-05,
      "loss": 0.0026,
      "step": 70330
    },
    {
      "epoch": 3.7514666666666665,
      "grad_norm": 0.3663581311702728,
      "learning_rate": 2.6553333333333335e-05,
      "loss": 0.0024,
      "step": 70340
    },
    {
      "epoch": 3.752,
      "grad_norm": 0.1127345860004425,
      "learning_rate": 2.655e-05,
      "loss": 0.0028,
      "step": 70350
    },
    {
      "epoch": 3.7525333333333335,
      "grad_norm": 0.11273816972970963,
      "learning_rate": 2.654666666666667e-05,
      "loss": 0.002,
      "step": 70360
    },
    {
      "epoch": 3.7530666666666668,
      "grad_norm": 0.25396251678466797,
      "learning_rate": 2.6543333333333337e-05,
      "loss": 0.0034,
      "step": 70370
    },
    {
      "epoch": 3.7536,
      "grad_norm": 0.1696283519268036,
      "learning_rate": 2.6540000000000003e-05,
      "loss": 0.0027,
      "step": 70380
    },
    {
      "epoch": 3.7541333333333333,
      "grad_norm": 0.05636410042643547,
      "learning_rate": 2.6536666666666666e-05,
      "loss": 0.0029,
      "step": 70390
    },
    {
      "epoch": 3.7546666666666666,
      "grad_norm": 0.2818234860897064,
      "learning_rate": 2.6533333333333332e-05,
      "loss": 0.0035,
      "step": 70400
    },
    {
      "epoch": 3.7552,
      "grad_norm": 0.3104172646999359,
      "learning_rate": 2.653e-05,
      "loss": 0.0032,
      "step": 70410
    },
    {
      "epoch": 3.7557333333333336,
      "grad_norm": 0.28184565901756287,
      "learning_rate": 2.6526666666666668e-05,
      "loss": 0.0023,
      "step": 70420
    },
    {
      "epoch": 3.756266666666667,
      "grad_norm": 0.22545737028121948,
      "learning_rate": 2.6523333333333334e-05,
      "loss": 0.0026,
      "step": 70430
    },
    {
      "epoch": 3.7568,
      "grad_norm": 0.11275941878557205,
      "learning_rate": 2.652e-05,
      "loss": 0.0033,
      "step": 70440
    },
    {
      "epoch": 3.7573333333333334,
      "grad_norm": 0.08455111086368561,
      "learning_rate": 2.6516666666666666e-05,
      "loss": 0.0032,
      "step": 70450
    },
    {
      "epoch": 3.7578666666666667,
      "grad_norm": 0.08455123007297516,
      "learning_rate": 2.6513333333333336e-05,
      "loss": 0.0035,
      "step": 70460
    },
    {
      "epoch": 3.7584,
      "grad_norm": 0.14090372622013092,
      "learning_rate": 2.6510000000000002e-05,
      "loss": 0.0021,
      "step": 70470
    },
    {
      "epoch": 3.7589333333333332,
      "grad_norm": 0.22545728087425232,
      "learning_rate": 2.6506666666666668e-05,
      "loss": 0.0034,
      "step": 70480
    },
    {
      "epoch": 3.7594666666666665,
      "grad_norm": 0.2818055748939514,
      "learning_rate": 2.6503333333333334e-05,
      "loss": 0.003,
      "step": 70490
    },
    {
      "epoch": 3.76,
      "grad_norm": 0.1409120410680771,
      "learning_rate": 2.6500000000000004e-05,
      "loss": 0.0027,
      "step": 70500
    },
    {
      "epoch": 3.760533333333333,
      "grad_norm": 0.05636240541934967,
      "learning_rate": 2.649666666666667e-05,
      "loss": 0.002,
      "step": 70510
    },
    {
      "epoch": 3.761066666666667,
      "grad_norm": 0.1127283126115799,
      "learning_rate": 2.6493333333333336e-05,
      "loss": 0.0031,
      "step": 70520
    },
    {
      "epoch": 3.7616,
      "grad_norm": 0.1409066766500473,
      "learning_rate": 2.6490000000000002e-05,
      "loss": 0.0029,
      "step": 70530
    },
    {
      "epoch": 3.7621333333333333,
      "grad_norm": 0.02818196639418602,
      "learning_rate": 2.6486666666666665e-05,
      "loss": 0.0032,
      "step": 70540
    },
    {
      "epoch": 3.7626666666666666,
      "grad_norm": 0.11273308843374252,
      "learning_rate": 2.648333333333333e-05,
      "loss": 0.0015,
      "step": 70550
    },
    {
      "epoch": 3.7632,
      "grad_norm": 0.28179818391799927,
      "learning_rate": 2.648e-05,
      "loss": 0.003,
      "step": 70560
    },
    {
      "epoch": 3.7637333333333336,
      "grad_norm": 0.08454939723014832,
      "learning_rate": 2.6476666666666667e-05,
      "loss": 0.0036,
      "step": 70570
    },
    {
      "epoch": 3.764266666666667,
      "grad_norm": 0.30998143553733826,
      "learning_rate": 2.6473333333333333e-05,
      "loss": 0.0025,
      "step": 70580
    },
    {
      "epoch": 3.7648,
      "grad_norm": 3.849733865024518e-09,
      "learning_rate": 2.647e-05,
      "loss": 0.0021,
      "step": 70590
    },
    {
      "epoch": 3.7653333333333334,
      "grad_norm": 0.11272484809160233,
      "learning_rate": 2.646666666666667e-05,
      "loss": 0.0023,
      "step": 70600
    },
    {
      "epoch": 3.7658666666666667,
      "grad_norm": 0.4227096736431122,
      "learning_rate": 2.6463333333333335e-05,
      "loss": 0.0033,
      "step": 70610
    },
    {
      "epoch": 3.7664,
      "grad_norm": 0.19727526605129242,
      "learning_rate": 2.646e-05,
      "loss": 0.0023,
      "step": 70620
    },
    {
      "epoch": 3.7669333333333332,
      "grad_norm": 0.16908970475196838,
      "learning_rate": 2.6456666666666667e-05,
      "loss": 0.0024,
      "step": 70630
    },
    {
      "epoch": 3.7674666666666665,
      "grad_norm": 0.3663509786128998,
      "learning_rate": 2.6453333333333336e-05,
      "loss": 0.0031,
      "step": 70640
    },
    {
      "epoch": 3.768,
      "grad_norm": 0.14090730249881744,
      "learning_rate": 2.6450000000000003e-05,
      "loss": 0.0035,
      "step": 70650
    },
    {
      "epoch": 3.768533333333333,
      "grad_norm": 0.3100005090236664,
      "learning_rate": 2.644666666666667e-05,
      "loss": 0.0028,
      "step": 70660
    },
    {
      "epoch": 3.769066666666667,
      "grad_norm": 0.2254549264907837,
      "learning_rate": 2.6443333333333338e-05,
      "loss": 0.0032,
      "step": 70670
    },
    {
      "epoch": 3.7696,
      "grad_norm": 0.16907821595668793,
      "learning_rate": 2.6440000000000004e-05,
      "loss": 0.0038,
      "step": 70680
    },
    {
      "epoch": 3.7701333333333333,
      "grad_norm": 0.1972745954990387,
      "learning_rate": 2.643666666666667e-05,
      "loss": 0.0023,
      "step": 70690
    },
    {
      "epoch": 3.7706666666666666,
      "grad_norm": 0.1409047544002533,
      "learning_rate": 2.6433333333333333e-05,
      "loss": 0.0046,
      "step": 70700
    },
    {
      "epoch": 3.7712,
      "grad_norm": 0.1707940697669983,
      "learning_rate": 2.643e-05,
      "loss": 0.0048,
      "step": 70710
    },
    {
      "epoch": 3.7717333333333336,
      "grad_norm": 0.16908665001392365,
      "learning_rate": 2.6426666666666665e-05,
      "loss": 0.0029,
      "step": 70720
    },
    {
      "epoch": 3.772266666666667,
      "grad_norm": 0.08453936874866486,
      "learning_rate": 2.642333333333333e-05,
      "loss": 0.0043,
      "step": 70730
    },
    {
      "epoch": 3.7728,
      "grad_norm": 0.028181036934256554,
      "learning_rate": 2.642e-05,
      "loss": 0.0034,
      "step": 70740
    },
    {
      "epoch": 3.7733333333333334,
      "grad_norm": 0.08453942835330963,
      "learning_rate": 2.6416666666666667e-05,
      "loss": 0.0023,
      "step": 70750
    },
    {
      "epoch": 3.7738666666666667,
      "grad_norm": 0.11272328346967697,
      "learning_rate": 2.6413333333333333e-05,
      "loss": 0.0036,
      "step": 70760
    },
    {
      "epoch": 3.7744,
      "grad_norm": 0.028179291635751724,
      "learning_rate": 2.6410000000000003e-05,
      "loss": 0.0026,
      "step": 70770
    },
    {
      "epoch": 3.7749333333333333,
      "grad_norm": 0.2818053364753723,
      "learning_rate": 2.640666666666667e-05,
      "loss": 0.0039,
      "step": 70780
    },
    {
      "epoch": 3.7754666666666665,
      "grad_norm": 0.2536254823207855,
      "learning_rate": 2.6403333333333335e-05,
      "loss": 0.0041,
      "step": 70790
    },
    {
      "epoch": 3.776,
      "grad_norm": 0.1408955305814743,
      "learning_rate": 2.64e-05,
      "loss": 0.004,
      "step": 70800
    },
    {
      "epoch": 3.776533333333333,
      "grad_norm": 0.2818076014518738,
      "learning_rate": 2.639666666666667e-05,
      "loss": 0.0027,
      "step": 70810
    },
    {
      "epoch": 3.777066666666667,
      "grad_norm": 0.22544120252132416,
      "learning_rate": 2.6393333333333337e-05,
      "loss": 0.0028,
      "step": 70820
    },
    {
      "epoch": 3.7776,
      "grad_norm": 0.4508967399597168,
      "learning_rate": 2.6390000000000003e-05,
      "loss": 0.0027,
      "step": 70830
    },
    {
      "epoch": 3.7781333333333333,
      "grad_norm": 0.22702601552009583,
      "learning_rate": 2.638666666666667e-05,
      "loss": 0.0023,
      "step": 70840
    },
    {
      "epoch": 3.7786666666666666,
      "grad_norm": 2.743556271056491e-09,
      "learning_rate": 2.6383333333333332e-05,
      "loss": 0.0026,
      "step": 70850
    },
    {
      "epoch": 3.7792,
      "grad_norm": 0.0845421850681305,
      "learning_rate": 2.6379999999999998e-05,
      "loss": 0.0041,
      "step": 70860
    },
    {
      "epoch": 3.779733333333333,
      "grad_norm": 0.05635882914066315,
      "learning_rate": 2.6376666666666668e-05,
      "loss": 0.0033,
      "step": 70870
    },
    {
      "epoch": 3.780266666666667,
      "grad_norm": 0.05635780841112137,
      "learning_rate": 2.6373333333333334e-05,
      "loss": 0.0028,
      "step": 70880
    },
    {
      "epoch": 3.7808,
      "grad_norm": 0.1127169206738472,
      "learning_rate": 2.637e-05,
      "loss": 0.0035,
      "step": 70890
    },
    {
      "epoch": 3.7813333333333334,
      "grad_norm": 2.0855484006432334e-09,
      "learning_rate": 2.6366666666666666e-05,
      "loss": 0.0039,
      "step": 70900
    },
    {
      "epoch": 3.7818666666666667,
      "grad_norm": 0.11272129416465759,
      "learning_rate": 2.6363333333333336e-05,
      "loss": 0.0029,
      "step": 70910
    },
    {
      "epoch": 3.7824,
      "grad_norm": 0.11271606385707855,
      "learning_rate": 2.6360000000000002e-05,
      "loss": 0.0025,
      "step": 70920
    },
    {
      "epoch": 3.7829333333333333,
      "grad_norm": 0.14089615643024445,
      "learning_rate": 2.6356666666666668e-05,
      "loss": 0.0031,
      "step": 70930
    },
    {
      "epoch": 3.7834666666666665,
      "grad_norm": 0.1408984661102295,
      "learning_rate": 2.6353333333333334e-05,
      "loss": 0.0022,
      "step": 70940
    },
    {
      "epoch": 3.784,
      "grad_norm": 0.11271785944700241,
      "learning_rate": 2.6350000000000004e-05,
      "loss": 0.0023,
      "step": 70950
    },
    {
      "epoch": 3.784533333333333,
      "grad_norm": 0.16907455027103424,
      "learning_rate": 2.634666666666667e-05,
      "loss": 0.0035,
      "step": 70960
    },
    {
      "epoch": 3.785066666666667,
      "grad_norm": 0.05636020377278328,
      "learning_rate": 2.6343333333333336e-05,
      "loss": 0.0031,
      "step": 70970
    },
    {
      "epoch": 3.7856,
      "grad_norm": 0.19725264608860016,
      "learning_rate": 2.6340000000000002e-05,
      "loss": 0.003,
      "step": 70980
    },
    {
      "epoch": 3.7861333333333334,
      "grad_norm": 0.22543074190616608,
      "learning_rate": 2.633666666666667e-05,
      "loss": 0.0022,
      "step": 70990
    },
    {
      "epoch": 3.7866666666666666,
      "grad_norm": 0.08453667908906937,
      "learning_rate": 2.633333333333333e-05,
      "loss": 0.0025,
      "step": 71000
    },
    {
      "epoch": 3.7872,
      "grad_norm": 0.11271599680185318,
      "learning_rate": 2.633e-05,
      "loss": 0.0046,
      "step": 71010
    },
    {
      "epoch": 3.787733333333333,
      "grad_norm": 0.3948778808116913,
      "learning_rate": 2.6326666666666666e-05,
      "loss": 0.0035,
      "step": 71020
    },
    {
      "epoch": 3.788266666666667,
      "grad_norm": 0.30997976660728455,
      "learning_rate": 2.6323333333333333e-05,
      "loss": 0.0022,
      "step": 71030
    },
    {
      "epoch": 3.7888,
      "grad_norm": 0.11271391808986664,
      "learning_rate": 2.632e-05,
      "loss": 0.0027,
      "step": 71040
    },
    {
      "epoch": 3.7893333333333334,
      "grad_norm": 0.3381589949131012,
      "learning_rate": 2.6316666666666668e-05,
      "loss": 0.0036,
      "step": 71050
    },
    {
      "epoch": 3.7898666666666667,
      "grad_norm": 0.16908389329910278,
      "learning_rate": 2.6313333333333334e-05,
      "loss": 0.0026,
      "step": 71060
    },
    {
      "epoch": 3.7904,
      "grad_norm": 0.056355856359004974,
      "learning_rate": 2.631e-05,
      "loss": 0.0041,
      "step": 71070
    },
    {
      "epoch": 3.7909333333333333,
      "grad_norm": 0.05635983496904373,
      "learning_rate": 2.630666666666667e-05,
      "loss": 0.0028,
      "step": 71080
    },
    {
      "epoch": 3.7914666666666665,
      "grad_norm": 0.25359898805618286,
      "learning_rate": 2.6303333333333336e-05,
      "loss": 0.0024,
      "step": 71090
    },
    {
      "epoch": 3.792,
      "grad_norm": 0.2817991077899933,
      "learning_rate": 2.6300000000000002e-05,
      "loss": 0.0024,
      "step": 71100
    },
    {
      "epoch": 3.792533333333333,
      "grad_norm": 0.11271331459283829,
      "learning_rate": 2.629666666666667e-05,
      "loss": 0.0022,
      "step": 71110
    },
    {
      "epoch": 3.793066666666667,
      "grad_norm": 0.1417110115289688,
      "learning_rate": 2.6293333333333338e-05,
      "loss": 0.004,
      "step": 71120
    },
    {
      "epoch": 3.7936,
      "grad_norm": 0.05635730177164078,
      "learning_rate": 2.6290000000000004e-05,
      "loss": 0.004,
      "step": 71130
    },
    {
      "epoch": 3.7941333333333334,
      "grad_norm": 0.197243332862854,
      "learning_rate": 2.628666666666667e-05,
      "loss": 0.0032,
      "step": 71140
    },
    {
      "epoch": 3.7946666666666666,
      "grad_norm": 0.05635816231369972,
      "learning_rate": 2.6283333333333333e-05,
      "loss": 0.002,
      "step": 71150
    },
    {
      "epoch": 3.7952,
      "grad_norm": 0.3381447494029999,
      "learning_rate": 2.628e-05,
      "loss": 0.0034,
      "step": 71160
    },
    {
      "epoch": 3.795733333333333,
      "grad_norm": 0.3381156921386719,
      "learning_rate": 2.6276666666666665e-05,
      "loss": 0.0029,
      "step": 71170
    },
    {
      "epoch": 3.796266666666667,
      "grad_norm": 0.309979647397995,
      "learning_rate": 2.6273333333333335e-05,
      "loss": 0.0019,
      "step": 71180
    },
    {
      "epoch": 3.7968,
      "grad_norm": 0.14088647067546844,
      "learning_rate": 2.627e-05,
      "loss": 0.0031,
      "step": 71190
    },
    {
      "epoch": 3.7973333333333334,
      "grad_norm": 0.028178194537758827,
      "learning_rate": 2.6266666666666667e-05,
      "loss": 0.0035,
      "step": 71200
    },
    {
      "epoch": 3.7978666666666667,
      "grad_norm": 1.6890571163941104e-09,
      "learning_rate": 2.6263333333333333e-05,
      "loss": 0.003,
      "step": 71210
    },
    {
      "epoch": 3.7984,
      "grad_norm": 0.36629703640937805,
      "learning_rate": 2.6260000000000003e-05,
      "loss": 0.004,
      "step": 71220
    },
    {
      "epoch": 3.7989333333333333,
      "grad_norm": 0.028178319334983826,
      "learning_rate": 2.625666666666667e-05,
      "loss": 0.0045,
      "step": 71230
    },
    {
      "epoch": 3.7994666666666665,
      "grad_norm": 0.39451101422309875,
      "learning_rate": 2.6253333333333335e-05,
      "loss": 0.0034,
      "step": 71240
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.4508090317249298,
      "learning_rate": 2.625e-05,
      "loss": 0.0023,
      "step": 71250
    },
    {
      "epoch": 3.800533333333333,
      "grad_norm": 0.45088574290275574,
      "learning_rate": 2.624666666666667e-05,
      "loss": 0.0021,
      "step": 71260
    },
    {
      "epoch": 3.801066666666667,
      "grad_norm": 0.3944488763809204,
      "learning_rate": 2.6243333333333337e-05,
      "loss": 0.0029,
      "step": 71270
    },
    {
      "epoch": 3.8016,
      "grad_norm": 0.22542917728424072,
      "learning_rate": 2.6240000000000003e-05,
      "loss": 0.0024,
      "step": 71280
    },
    {
      "epoch": 3.8021333333333334,
      "grad_norm": 0.056355070322752,
      "learning_rate": 2.623666666666667e-05,
      "loss": 0.0035,
      "step": 71290
    },
    {
      "epoch": 3.8026666666666666,
      "grad_norm": 0.28176265954971313,
      "learning_rate": 2.6233333333333332e-05,
      "loss": 0.0031,
      "step": 71300
    },
    {
      "epoch": 3.8032,
      "grad_norm": 2.1452615239780926e-09,
      "learning_rate": 2.6229999999999998e-05,
      "loss": 0.0023,
      "step": 71310
    },
    {
      "epoch": 3.803733333333333,
      "grad_norm": 0.19722962379455566,
      "learning_rate": 2.6226666666666667e-05,
      "loss": 0.0023,
      "step": 71320
    },
    {
      "epoch": 3.804266666666667,
      "grad_norm": 0.08452863246202469,
      "learning_rate": 2.6223333333333334e-05,
      "loss": 0.002,
      "step": 71330
    },
    {
      "epoch": 3.8048,
      "grad_norm": 0.254315048456192,
      "learning_rate": 2.622e-05,
      "loss": 0.0035,
      "step": 71340
    },
    {
      "epoch": 3.8053333333333335,
      "grad_norm": 5.616103138095241e-09,
      "learning_rate": 2.6216666666666666e-05,
      "loss": 0.0038,
      "step": 71350
    },
    {
      "epoch": 3.8058666666666667,
      "grad_norm": 0.30993226170539856,
      "learning_rate": 2.6213333333333335e-05,
      "loss": 0.0037,
      "step": 71360
    },
    {
      "epoch": 3.8064,
      "grad_norm": 0.1972290724515915,
      "learning_rate": 2.621e-05,
      "loss": 0.0035,
      "step": 71370
    },
    {
      "epoch": 3.8069333333333333,
      "grad_norm": 0.1127064898610115,
      "learning_rate": 2.6206666666666668e-05,
      "loss": 0.0037,
      "step": 71380
    },
    {
      "epoch": 3.8074666666666666,
      "grad_norm": 0.3944464325904846,
      "learning_rate": 2.6203333333333334e-05,
      "loss": 0.0026,
      "step": 71390
    },
    {
      "epoch": 3.808,
      "grad_norm": 0.028176357969641685,
      "learning_rate": 2.6200000000000003e-05,
      "loss": 0.0034,
      "step": 71400
    },
    {
      "epoch": 3.808533333333333,
      "grad_norm": 0.08453186601400375,
      "learning_rate": 2.619666666666667e-05,
      "loss": 0.0035,
      "step": 71410
    },
    {
      "epoch": 3.809066666666667,
      "grad_norm": 0.2817712128162384,
      "learning_rate": 2.6193333333333336e-05,
      "loss": 0.0022,
      "step": 71420
    },
    {
      "epoch": 3.8096,
      "grad_norm": 0.22539477050304413,
      "learning_rate": 2.6190000000000005e-05,
      "loss": 0.0027,
      "step": 71430
    },
    {
      "epoch": 3.8101333333333334,
      "grad_norm": 0.08453210443258286,
      "learning_rate": 2.618666666666667e-05,
      "loss": 0.0029,
      "step": 71440
    },
    {
      "epoch": 3.8106666666666666,
      "grad_norm": 0.0845312848687172,
      "learning_rate": 2.618333333333333e-05,
      "loss": 0.0028,
      "step": 71450
    },
    {
      "epoch": 3.8112,
      "grad_norm": 0.4226250648498535,
      "learning_rate": 2.618e-05,
      "loss": 0.0035,
      "step": 71460
    },
    {
      "epoch": 3.811733333333333,
      "grad_norm": 0.16906815767288208,
      "learning_rate": 2.6176666666666666e-05,
      "loss": 0.0026,
      "step": 71470
    },
    {
      "epoch": 3.812266666666667,
      "grad_norm": 0.16905109584331512,
      "learning_rate": 2.6173333333333332e-05,
      "loss": 0.0025,
      "step": 71480
    },
    {
      "epoch": 3.8128,
      "grad_norm": 0.14088387787342072,
      "learning_rate": 2.617e-05,
      "loss": 0.0023,
      "step": 71490
    },
    {
      "epoch": 3.8133333333333335,
      "grad_norm": 0.08452914655208588,
      "learning_rate": 2.6166666666666668e-05,
      "loss": 0.0037,
      "step": 71500
    },
    {
      "epoch": 3.8138666666666667,
      "grad_norm": 0.14087359607219696,
      "learning_rate": 2.6163333333333334e-05,
      "loss": 0.003,
      "step": 71510
    },
    {
      "epoch": 3.8144,
      "grad_norm": 0.05635125935077667,
      "learning_rate": 2.616e-05,
      "loss": 0.0035,
      "step": 71520
    },
    {
      "epoch": 3.8149333333333333,
      "grad_norm": 0.08452541381120682,
      "learning_rate": 2.615666666666667e-05,
      "loss": 0.0038,
      "step": 71530
    },
    {
      "epoch": 3.8154666666666666,
      "grad_norm": 0.140875443816185,
      "learning_rate": 2.6153333333333336e-05,
      "loss": 0.0032,
      "step": 71540
    },
    {
      "epoch": 3.816,
      "grad_norm": 0.05635223910212517,
      "learning_rate": 2.6150000000000002e-05,
      "loss": 0.0027,
      "step": 71550
    },
    {
      "epoch": 3.816533333333333,
      "grad_norm": 3.387339075899831e-09,
      "learning_rate": 2.6146666666666668e-05,
      "loss": 0.0023,
      "step": 71560
    },
    {
      "epoch": 3.817066666666667,
      "grad_norm": 0.36626365780830383,
      "learning_rate": 2.6143333333333338e-05,
      "loss": 0.0029,
      "step": 71570
    },
    {
      "epoch": 3.8176,
      "grad_norm": 0.1972411423921585,
      "learning_rate": 2.6140000000000004e-05,
      "loss": 0.0025,
      "step": 71580
    },
    {
      "epoch": 3.8181333333333334,
      "grad_norm": 0.5916316509246826,
      "learning_rate": 2.613666666666667e-05,
      "loss": 0.0033,
      "step": 71590
    },
    {
      "epoch": 3.8186666666666667,
      "grad_norm": 0.19723641872406006,
      "learning_rate": 2.6133333333333333e-05,
      "loss": 0.003,
      "step": 71600
    },
    {
      "epoch": 3.8192,
      "grad_norm": 0.028174886479973793,
      "learning_rate": 2.613e-05,
      "loss": 0.0042,
      "step": 71610
    },
    {
      "epoch": 3.819733333333333,
      "grad_norm": 0.16904163360595703,
      "learning_rate": 2.6126666666666665e-05,
      "loss": 0.0033,
      "step": 71620
    },
    {
      "epoch": 3.820266666666667,
      "grad_norm": 0.11270119249820709,
      "learning_rate": 2.6123333333333335e-05,
      "loss": 0.0029,
      "step": 71630
    },
    {
      "epoch": 3.8208,
      "grad_norm": 0.2535701096057892,
      "learning_rate": 2.612e-05,
      "loss": 0.0027,
      "step": 71640
    },
    {
      "epoch": 3.8213333333333335,
      "grad_norm": 0.1972258985042572,
      "learning_rate": 2.6116666666666667e-05,
      "loss": 0.0041,
      "step": 71650
    },
    {
      "epoch": 3.8218666666666667,
      "grad_norm": 0.05634737014770508,
      "learning_rate": 2.6113333333333333e-05,
      "loss": 0.0042,
      "step": 71660
    },
    {
      "epoch": 3.8224,
      "grad_norm": 5.256409529863504e-09,
      "learning_rate": 2.6110000000000002e-05,
      "loss": 0.0021,
      "step": 71670
    },
    {
      "epoch": 3.8229333333333333,
      "grad_norm": 0.1690392792224884,
      "learning_rate": 2.610666666666667e-05,
      "loss": 0.0036,
      "step": 71680
    },
    {
      "epoch": 3.8234666666666666,
      "grad_norm": 0.02817613072693348,
      "learning_rate": 2.6103333333333335e-05,
      "loss": 0.003,
      "step": 71690
    },
    {
      "epoch": 3.824,
      "grad_norm": 1.123019455206986e-09,
      "learning_rate": 2.61e-05,
      "loss": 0.005,
      "step": 71700
    },
    {
      "epoch": 3.824533333333333,
      "grad_norm": 0.2535702884197235,
      "learning_rate": 2.609666666666667e-05,
      "loss": 0.0021,
      "step": 71710
    },
    {
      "epoch": 3.8250666666666664,
      "grad_norm": 0.08452629297971725,
      "learning_rate": 2.6093333333333336e-05,
      "loss": 0.0038,
      "step": 71720
    },
    {
      "epoch": 3.8256,
      "grad_norm": 0.3380657434463501,
      "learning_rate": 2.6090000000000003e-05,
      "loss": 0.0036,
      "step": 71730
    },
    {
      "epoch": 3.8261333333333334,
      "grad_norm": 0.05634957179427147,
      "learning_rate": 2.608666666666667e-05,
      "loss": 0.0034,
      "step": 71740
    },
    {
      "epoch": 3.8266666666666667,
      "grad_norm": 0.11269039660692215,
      "learning_rate": 2.608333333333333e-05,
      "loss": 0.0028,
      "step": 71750
    },
    {
      "epoch": 3.8272,
      "grad_norm": 0.08452069014310837,
      "learning_rate": 2.6079999999999998e-05,
      "loss": 0.0025,
      "step": 71760
    },
    {
      "epoch": 3.827733333333333,
      "grad_norm": 0.33809760212898254,
      "learning_rate": 2.6076666666666667e-05,
      "loss": 0.0024,
      "step": 71770
    },
    {
      "epoch": 3.828266666666667,
      "grad_norm": 0.05634516850113869,
      "learning_rate": 2.6073333333333333e-05,
      "loss": 0.0026,
      "step": 71780
    },
    {
      "epoch": 3.8288,
      "grad_norm": 0.2535565197467804,
      "learning_rate": 2.607e-05,
      "loss": 0.0021,
      "step": 71790
    },
    {
      "epoch": 3.8293333333333335,
      "grad_norm": 0.11269016563892365,
      "learning_rate": 2.6066666666666666e-05,
      "loss": 0.0025,
      "step": 71800
    },
    {
      "epoch": 3.8298666666666668,
      "grad_norm": 0.36625906825065613,
      "learning_rate": 2.6063333333333335e-05,
      "loss": 0.0043,
      "step": 71810
    },
    {
      "epoch": 3.8304,
      "grad_norm": 0.16903340816497803,
      "learning_rate": 2.606e-05,
      "loss": 0.0027,
      "step": 71820
    },
    {
      "epoch": 3.8309333333333333,
      "grad_norm": 0.3099064826965332,
      "learning_rate": 2.6056666666666667e-05,
      "loss": 0.0033,
      "step": 71830
    },
    {
      "epoch": 3.8314666666666666,
      "grad_norm": 0.028174158185720444,
      "learning_rate": 2.6053333333333333e-05,
      "loss": 0.0037,
      "step": 71840
    },
    {
      "epoch": 3.832,
      "grad_norm": 0.11269019544124603,
      "learning_rate": 2.6050000000000003e-05,
      "loss": 0.0024,
      "step": 71850
    },
    {
      "epoch": 3.832533333333333,
      "grad_norm": 0.19720754027366638,
      "learning_rate": 2.604666666666667e-05,
      "loss": 0.0026,
      "step": 71860
    },
    {
      "epoch": 3.8330666666666664,
      "grad_norm": 0.22540363669395447,
      "learning_rate": 2.6043333333333335e-05,
      "loss": 0.0039,
      "step": 71870
    },
    {
      "epoch": 3.8336,
      "grad_norm": 0.19720277190208435,
      "learning_rate": 2.6040000000000005e-05,
      "loss": 0.0033,
      "step": 71880
    },
    {
      "epoch": 3.8341333333333334,
      "grad_norm": 0.16905103623867035,
      "learning_rate": 2.603666666666667e-05,
      "loss": 0.0034,
      "step": 71890
    },
    {
      "epoch": 3.8346666666666667,
      "grad_norm": 0.056343741714954376,
      "learning_rate": 2.6033333333333337e-05,
      "loss": 0.0035,
      "step": 71900
    },
    {
      "epoch": 3.8352,
      "grad_norm": 0.16986967623233795,
      "learning_rate": 2.603e-05,
      "loss": 0.0025,
      "step": 71910
    },
    {
      "epoch": 3.835733333333333,
      "grad_norm": 0.14086145162582397,
      "learning_rate": 2.6026666666666666e-05,
      "loss": 0.0029,
      "step": 71920
    },
    {
      "epoch": 3.836266666666667,
      "grad_norm": 0.30988797545433044,
      "learning_rate": 2.6023333333333332e-05,
      "loss": 0.0029,
      "step": 71930
    },
    {
      "epoch": 3.8368,
      "grad_norm": 0.42258691787719727,
      "learning_rate": 2.602e-05,
      "loss": 0.005,
      "step": 71940
    },
    {
      "epoch": 3.8373333333333335,
      "grad_norm": 0.16996337473392487,
      "learning_rate": 2.6016666666666668e-05,
      "loss": 0.0016,
      "step": 71950
    },
    {
      "epoch": 3.8378666666666668,
      "grad_norm": 0.3380679488182068,
      "learning_rate": 2.6013333333333334e-05,
      "loss": 0.0021,
      "step": 71960
    },
    {
      "epoch": 3.8384,
      "grad_norm": 0.056343965232372284,
      "learning_rate": 2.601e-05,
      "loss": 0.0036,
      "step": 71970
    },
    {
      "epoch": 3.8389333333333333,
      "grad_norm": 0.1690412312746048,
      "learning_rate": 2.600666666666667e-05,
      "loss": 0.0032,
      "step": 71980
    },
    {
      "epoch": 3.8394666666666666,
      "grad_norm": 0.39438632130622864,
      "learning_rate": 2.6003333333333336e-05,
      "loss": 0.0024,
      "step": 71990
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.1126922070980072,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.0016,
      "step": 72000
    },
    {
      "epoch": 3.840533333333333,
      "grad_norm": 0.3662080764770508,
      "learning_rate": 2.5996666666666668e-05,
      "loss": 0.0038,
      "step": 72010
    },
    {
      "epoch": 3.8410666666666664,
      "grad_norm": 0.17119023203849792,
      "learning_rate": 2.5993333333333337e-05,
      "loss": 0.0044,
      "step": 72020
    },
    {
      "epoch": 3.8416,
      "grad_norm": 0.16902504861354828,
      "learning_rate": 2.5990000000000004e-05,
      "loss": 0.0042,
      "step": 72030
    },
    {
      "epoch": 3.8421333333333334,
      "grad_norm": 0.25353527069091797,
      "learning_rate": 2.598666666666667e-05,
      "loss": 0.0038,
      "step": 72040
    },
    {
      "epoch": 3.8426666666666667,
      "grad_norm": 0.0845126211643219,
      "learning_rate": 2.5983333333333336e-05,
      "loss": 0.0045,
      "step": 72050
    },
    {
      "epoch": 3.8432,
      "grad_norm": 0.14360322058200836,
      "learning_rate": 2.598e-05,
      "loss": 0.0029,
      "step": 72060
    },
    {
      "epoch": 3.8437333333333332,
      "grad_norm": 0.22538481652736664,
      "learning_rate": 2.5976666666666665e-05,
      "loss": 0.0038,
      "step": 72070
    },
    {
      "epoch": 3.844266666666667,
      "grad_norm": 0.11391313374042511,
      "learning_rate": 2.5973333333333334e-05,
      "loss": 0.0023,
      "step": 72080
    },
    {
      "epoch": 3.8448,
      "grad_norm": 0.19720230996608734,
      "learning_rate": 2.597e-05,
      "loss": 0.0016,
      "step": 72090
    },
    {
      "epoch": 3.8453333333333335,
      "grad_norm": 0.02817186899483204,
      "learning_rate": 2.5966666666666667e-05,
      "loss": 0.0027,
      "step": 72100
    },
    {
      "epoch": 3.8458666666666668,
      "grad_norm": 0.056344062089920044,
      "learning_rate": 2.5963333333333333e-05,
      "loss": 0.0024,
      "step": 72110
    },
    {
      "epoch": 3.8464,
      "grad_norm": 0.2540864646434784,
      "learning_rate": 2.5960000000000002e-05,
      "loss": 0.003,
      "step": 72120
    },
    {
      "epoch": 3.8469333333333333,
      "grad_norm": 0.08451452851295471,
      "learning_rate": 2.5956666666666668e-05,
      "loss": 0.0027,
      "step": 72130
    },
    {
      "epoch": 3.8474666666666666,
      "grad_norm": 0.22582782804965973,
      "learning_rate": 2.5953333333333334e-05,
      "loss": 0.0029,
      "step": 72140
    },
    {
      "epoch": 3.848,
      "grad_norm": 0.2253628969192505,
      "learning_rate": 2.595e-05,
      "loss": 0.0027,
      "step": 72150
    },
    {
      "epoch": 3.848533333333333,
      "grad_norm": 4.640220652163407e-09,
      "learning_rate": 2.594666666666667e-05,
      "loss": 0.002,
      "step": 72160
    },
    {
      "epoch": 3.8490666666666664,
      "grad_norm": 0.02817164920270443,
      "learning_rate": 2.5943333333333336e-05,
      "loss": 0.0026,
      "step": 72170
    },
    {
      "epoch": 3.8496,
      "grad_norm": 0.08451075106859207,
      "learning_rate": 2.5940000000000002e-05,
      "loss": 0.0034,
      "step": 72180
    },
    {
      "epoch": 3.8501333333333334,
      "grad_norm": 0.4507094621658325,
      "learning_rate": 2.5936666666666672e-05,
      "loss": 0.0034,
      "step": 72190
    },
    {
      "epoch": 3.8506666666666667,
      "grad_norm": 0.33805620670318604,
      "learning_rate": 2.5933333333333338e-05,
      "loss": 0.0031,
      "step": 72200
    },
    {
      "epoch": 3.8512,
      "grad_norm": 0.16902059316635132,
      "learning_rate": 2.5929999999999997e-05,
      "loss": 0.002,
      "step": 72210
    },
    {
      "epoch": 3.8517333333333332,
      "grad_norm": 0.19719500839710236,
      "learning_rate": 2.5926666666666667e-05,
      "loss": 0.0025,
      "step": 72220
    },
    {
      "epoch": 3.8522666666666665,
      "grad_norm": 0.33806583285331726,
      "learning_rate": 2.5923333333333333e-05,
      "loss": 0.0031,
      "step": 72230
    },
    {
      "epoch": 3.8528000000000002,
      "grad_norm": 0.19718702137470245,
      "learning_rate": 2.592e-05,
      "loss": 0.0048,
      "step": 72240
    },
    {
      "epoch": 3.8533333333333335,
      "grad_norm": 0.11268279701471329,
      "learning_rate": 2.5916666666666665e-05,
      "loss": 0.0021,
      "step": 72250
    },
    {
      "epoch": 3.8538666666666668,
      "grad_norm": 0.19719097018241882,
      "learning_rate": 2.5913333333333335e-05,
      "loss": 0.002,
      "step": 72260
    },
    {
      "epoch": 3.8544,
      "grad_norm": 0.3380623161792755,
      "learning_rate": 2.591e-05,
      "loss": 0.0022,
      "step": 72270
    },
    {
      "epoch": 3.8549333333333333,
      "grad_norm": 0.338032990694046,
      "learning_rate": 2.5906666666666667e-05,
      "loss": 0.0028,
      "step": 72280
    },
    {
      "epoch": 3.8554666666666666,
      "grad_norm": 0.02817104570567608,
      "learning_rate": 2.5903333333333337e-05,
      "loss": 0.0028,
      "step": 72290
    },
    {
      "epoch": 3.856,
      "grad_norm": 0.22607873380184174,
      "learning_rate": 2.5900000000000003e-05,
      "loss": 0.0023,
      "step": 72300
    },
    {
      "epoch": 3.856533333333333,
      "grad_norm": 0.4506995379924774,
      "learning_rate": 2.589666666666667e-05,
      "loss": 0.0026,
      "step": 72310
    },
    {
      "epoch": 3.8570666666666664,
      "grad_norm": 0.3380412459373474,
      "learning_rate": 2.5893333333333335e-05,
      "loss": 0.0023,
      "step": 72320
    },
    {
      "epoch": 3.8576,
      "grad_norm": 0.25351816415786743,
      "learning_rate": 2.5890000000000005e-05,
      "loss": 0.0033,
      "step": 72330
    },
    {
      "epoch": 3.8581333333333334,
      "grad_norm": 1.9499646425247192,
      "learning_rate": 2.588666666666667e-05,
      "loss": 0.0028,
      "step": 72340
    },
    {
      "epoch": 3.8586666666666667,
      "grad_norm": 0.25352489948272705,
      "learning_rate": 2.5883333333333337e-05,
      "loss": 0.0031,
      "step": 72350
    },
    {
      "epoch": 3.8592,
      "grad_norm": 0.14084291458129883,
      "learning_rate": 2.588e-05,
      "loss": 0.0029,
      "step": 72360
    },
    {
      "epoch": 3.8597333333333332,
      "grad_norm": 0.14084966480731964,
      "learning_rate": 2.5876666666666666e-05,
      "loss": 0.0025,
      "step": 72370
    },
    {
      "epoch": 3.8602666666666665,
      "grad_norm": 0.25352349877357483,
      "learning_rate": 2.5873333333333332e-05,
      "loss": 0.0026,
      "step": 72380
    },
    {
      "epoch": 3.8608000000000002,
      "grad_norm": 2.2443771285907133e-09,
      "learning_rate": 2.587e-05,
      "loss": 0.0027,
      "step": 72390
    },
    {
      "epoch": 3.8613333333333335,
      "grad_norm": 0.11268250644207001,
      "learning_rate": 2.5866666666666667e-05,
      "loss": 0.0027,
      "step": 72400
    },
    {
      "epoch": 3.861866666666667,
      "grad_norm": 0.02816922776401043,
      "learning_rate": 2.5863333333333334e-05,
      "loss": 0.0015,
      "step": 72410
    },
    {
      "epoch": 3.8624,
      "grad_norm": 0.028169266879558563,
      "learning_rate": 2.586e-05,
      "loss": 0.0026,
      "step": 72420
    },
    {
      "epoch": 3.8629333333333333,
      "grad_norm": 0.14084665477275848,
      "learning_rate": 2.585666666666667e-05,
      "loss": 0.0035,
      "step": 72430
    },
    {
      "epoch": 3.8634666666666666,
      "grad_norm": 0.30987608432769775,
      "learning_rate": 2.5853333333333335e-05,
      "loss": 0.0033,
      "step": 72440
    },
    {
      "epoch": 3.864,
      "grad_norm": 0.11267483234405518,
      "learning_rate": 2.585e-05,
      "loss": 0.0037,
      "step": 72450
    },
    {
      "epoch": 3.864533333333333,
      "grad_norm": 0.19719144701957703,
      "learning_rate": 2.5846666666666668e-05,
      "loss": 0.0028,
      "step": 72460
    },
    {
      "epoch": 3.8650666666666664,
      "grad_norm": 0.0845075249671936,
      "learning_rate": 2.5843333333333337e-05,
      "loss": 0.0023,
      "step": 72470
    },
    {
      "epoch": 3.8656,
      "grad_norm": 0.14084848761558533,
      "learning_rate": 2.5840000000000003e-05,
      "loss": 0.0025,
      "step": 72480
    },
    {
      "epoch": 3.8661333333333334,
      "grad_norm": 0.22535933554172516,
      "learning_rate": 2.583666666666667e-05,
      "loss": 0.0036,
      "step": 72490
    },
    {
      "epoch": 3.8666666666666667,
      "grad_norm": 0.02816816233098507,
      "learning_rate": 2.5833333333333336e-05,
      "loss": 0.0022,
      "step": 72500
    },
    {
      "epoch": 3.8672,
      "grad_norm": 0.1690216064453125,
      "learning_rate": 2.583e-05,
      "loss": 0.0041,
      "step": 72510
    },
    {
      "epoch": 3.8677333333333332,
      "grad_norm": 0.028168033808469772,
      "learning_rate": 2.5826666666666664e-05,
      "loss": 0.0033,
      "step": 72520
    },
    {
      "epoch": 3.8682666666666665,
      "grad_norm": 0.2535291910171509,
      "learning_rate": 2.5823333333333334e-05,
      "loss": 0.0032,
      "step": 72530
    },
    {
      "epoch": 3.8688000000000002,
      "grad_norm": 0.2253330945968628,
      "learning_rate": 2.582e-05,
      "loss": 0.0023,
      "step": 72540
    },
    {
      "epoch": 3.8693333333333335,
      "grad_norm": 2.599782833456743e-09,
      "learning_rate": 2.5816666666666666e-05,
      "loss": 0.0021,
      "step": 72550
    },
    {
      "epoch": 3.869866666666667,
      "grad_norm": 1.1060781478881836,
      "learning_rate": 2.5813333333333332e-05,
      "loss": 0.0026,
      "step": 72560
    },
    {
      "epoch": 3.8704,
      "grad_norm": 0.14194943010807037,
      "learning_rate": 2.5810000000000002e-05,
      "loss": 0.0061,
      "step": 72570
    },
    {
      "epoch": 3.8709333333333333,
      "grad_norm": 0.11267489939928055,
      "learning_rate": 2.5806666666666668e-05,
      "loss": 0.0052,
      "step": 72580
    },
    {
      "epoch": 3.8714666666666666,
      "grad_norm": 0.16900216042995453,
      "learning_rate": 2.5803333333333334e-05,
      "loss": 0.0038,
      "step": 72590
    },
    {
      "epoch": 3.872,
      "grad_norm": 0.19717937707901,
      "learning_rate": 2.58e-05,
      "loss": 0.0018,
      "step": 72600
    },
    {
      "epoch": 3.872533333333333,
      "grad_norm": 0.11267725378274918,
      "learning_rate": 2.579666666666667e-05,
      "loss": 0.0047,
      "step": 72610
    },
    {
      "epoch": 3.8730666666666664,
      "grad_norm": 0.25351089239120483,
      "learning_rate": 2.5793333333333336e-05,
      "loss": 0.0031,
      "step": 72620
    },
    {
      "epoch": 3.8736,
      "grad_norm": 0.05633853003382683,
      "learning_rate": 2.5790000000000002e-05,
      "loss": 0.0037,
      "step": 72630
    },
    {
      "epoch": 3.8741333333333334,
      "grad_norm": 0.4788415729999542,
      "learning_rate": 2.578666666666667e-05,
      "loss": 0.0039,
      "step": 72640
    },
    {
      "epoch": 3.8746666666666667,
      "grad_norm": 0.2816922068595886,
      "learning_rate": 2.5783333333333338e-05,
      "loss": 0.0031,
      "step": 72650
    },
    {
      "epoch": 3.8752,
      "grad_norm": 0.22534659504890442,
      "learning_rate": 2.5779999999999997e-05,
      "loss": 0.0035,
      "step": 72660
    },
    {
      "epoch": 3.8757333333333333,
      "grad_norm": 0.0281674861907959,
      "learning_rate": 2.5776666666666667e-05,
      "loss": 0.0025,
      "step": 72670
    },
    {
      "epoch": 3.8762666666666665,
      "grad_norm": 0.16901394724845886,
      "learning_rate": 2.5773333333333333e-05,
      "loss": 0.0027,
      "step": 72680
    },
    {
      "epoch": 3.8768000000000002,
      "grad_norm": 0.2816886901855469,
      "learning_rate": 2.577e-05,
      "loss": 0.0035,
      "step": 72690
    },
    {
      "epoch": 3.8773333333333335,
      "grad_norm": 0.22533740103244781,
      "learning_rate": 2.5766666666666665e-05,
      "loss": 0.0027,
      "step": 72700
    },
    {
      "epoch": 3.877866666666667,
      "grad_norm": 0.056335482746362686,
      "learning_rate": 2.5763333333333335e-05,
      "loss": 0.0034,
      "step": 72710
    },
    {
      "epoch": 3.8784,
      "grad_norm": 0.30986538529396057,
      "learning_rate": 2.576e-05,
      "loss": 0.003,
      "step": 72720
    },
    {
      "epoch": 3.8789333333333333,
      "grad_norm": 0.056334201246500015,
      "learning_rate": 2.5756666666666667e-05,
      "loss": 0.0027,
      "step": 72730
    },
    {
      "epoch": 3.8794666666666666,
      "grad_norm": 0.25350651144981384,
      "learning_rate": 2.5753333333333336e-05,
      "loss": 0.0019,
      "step": 72740
    },
    {
      "epoch": 3.88,
      "grad_norm": 0.309829980134964,
      "learning_rate": 2.5750000000000002e-05,
      "loss": 0.0036,
      "step": 72750
    },
    {
      "epoch": 3.880533333333333,
      "grad_norm": 0.16900700330734253,
      "learning_rate": 2.574666666666667e-05,
      "loss": 0.0033,
      "step": 72760
    },
    {
      "epoch": 3.8810666666666664,
      "grad_norm": 0.08450045436620712,
      "learning_rate": 2.5743333333333335e-05,
      "loss": 0.0031,
      "step": 72770
    },
    {
      "epoch": 3.8816,
      "grad_norm": 0.02816706895828247,
      "learning_rate": 2.5740000000000004e-05,
      "loss": 0.0018,
      "step": 72780
    },
    {
      "epoch": 3.8821333333333334,
      "grad_norm": 0.028166498988866806,
      "learning_rate": 2.573666666666667e-05,
      "loss": 0.0026,
      "step": 72790
    },
    {
      "epoch": 3.8826666666666667,
      "grad_norm": 0.08450207114219666,
      "learning_rate": 2.5733333333333337e-05,
      "loss": 0.0042,
      "step": 72800
    },
    {
      "epoch": 3.8832,
      "grad_norm": 0.22534367442131042,
      "learning_rate": 2.573e-05,
      "loss": 0.003,
      "step": 72810
    },
    {
      "epoch": 3.8837333333333333,
      "grad_norm": 0.16900523006916046,
      "learning_rate": 2.5726666666666665e-05,
      "loss": 0.0035,
      "step": 72820
    },
    {
      "epoch": 3.8842666666666665,
      "grad_norm": 0.08450376242399216,
      "learning_rate": 2.572333333333333e-05,
      "loss": 0.0028,
      "step": 72830
    },
    {
      "epoch": 3.8848000000000003,
      "grad_norm": 0.028166597709059715,
      "learning_rate": 2.572e-05,
      "loss": 0.0037,
      "step": 72840
    },
    {
      "epoch": 3.8853333333333335,
      "grad_norm": 0.11267267912626266,
      "learning_rate": 2.5716666666666667e-05,
      "loss": 0.0021,
      "step": 72850
    },
    {
      "epoch": 3.885866666666667,
      "grad_norm": 0.1689956933259964,
      "learning_rate": 2.5713333333333333e-05,
      "loss": 0.0026,
      "step": 72860
    },
    {
      "epoch": 3.8864,
      "grad_norm": 0.3111245036125183,
      "learning_rate": 2.571e-05,
      "loss": 0.0022,
      "step": 72870
    },
    {
      "epoch": 3.8869333333333334,
      "grad_norm": 0.3943031430244446,
      "learning_rate": 2.570666666666667e-05,
      "loss": 0.0037,
      "step": 72880
    },
    {
      "epoch": 3.8874666666666666,
      "grad_norm": 0.1690146028995514,
      "learning_rate": 2.5703333333333335e-05,
      "loss": 0.0032,
      "step": 72890
    },
    {
      "epoch": 3.888,
      "grad_norm": 0.14083035290241241,
      "learning_rate": 2.57e-05,
      "loss": 0.003,
      "step": 72900
    },
    {
      "epoch": 3.888533333333333,
      "grad_norm": 0.05633172020316124,
      "learning_rate": 2.5696666666666667e-05,
      "loss": 0.0018,
      "step": 72910
    },
    {
      "epoch": 3.8890666666666664,
      "grad_norm": 0.28168267011642456,
      "learning_rate": 2.5693333333333337e-05,
      "loss": 0.0033,
      "step": 72920
    },
    {
      "epoch": 3.8895999999999997,
      "grad_norm": 0.02816668711602688,
      "learning_rate": 2.5690000000000003e-05,
      "loss": 0.0026,
      "step": 72930
    },
    {
      "epoch": 3.8901333333333334,
      "grad_norm": 0.028166303411126137,
      "learning_rate": 2.568666666666667e-05,
      "loss": 0.0023,
      "step": 72940
    },
    {
      "epoch": 3.8906666666666667,
      "grad_norm": 0.28167301416397095,
      "learning_rate": 2.5683333333333335e-05,
      "loss": 0.0037,
      "step": 72950
    },
    {
      "epoch": 3.8912,
      "grad_norm": 0.028165003284811974,
      "learning_rate": 2.5679999999999998e-05,
      "loss": 0.0032,
      "step": 72960
    },
    {
      "epoch": 3.8917333333333333,
      "grad_norm": 0.3661893308162689,
      "learning_rate": 2.5676666666666664e-05,
      "loss": 0.0016,
      "step": 72970
    },
    {
      "epoch": 3.8922666666666665,
      "grad_norm": 0.30980217456817627,
      "learning_rate": 2.5673333333333334e-05,
      "loss": 0.0021,
      "step": 72980
    },
    {
      "epoch": 3.8928000000000003,
      "grad_norm": 0.05633394792675972,
      "learning_rate": 2.567e-05,
      "loss": 0.0021,
      "step": 72990
    },
    {
      "epoch": 3.8933333333333335,
      "grad_norm": 2.4510200535843296e-09,
      "learning_rate": 2.5666666666666666e-05,
      "loss": 0.0025,
      "step": 73000
    },
    {
      "epoch": 3.893866666666667,
      "grad_norm": 0.6759288907051086,
      "learning_rate": 2.5663333333333332e-05,
      "loss": 0.0032,
      "step": 73010
    },
    {
      "epoch": 3.8944,
      "grad_norm": 0.14083005487918854,
      "learning_rate": 2.566e-05,
      "loss": 0.0031,
      "step": 73020
    },
    {
      "epoch": 3.8949333333333334,
      "grad_norm": 0.253478467464447,
      "learning_rate": 2.5656666666666668e-05,
      "loss": 0.0021,
      "step": 73030
    },
    {
      "epoch": 3.8954666666666666,
      "grad_norm": 0.08449613302946091,
      "learning_rate": 2.5653333333333334e-05,
      "loss": 0.0022,
      "step": 73040
    },
    {
      "epoch": 3.896,
      "grad_norm": 2.958540745723326e-09,
      "learning_rate": 2.5650000000000003e-05,
      "loss": 0.0026,
      "step": 73050
    },
    {
      "epoch": 3.896533333333333,
      "grad_norm": 0.3379553556442261,
      "learning_rate": 2.564666666666667e-05,
      "loss": 0.0034,
      "step": 73060
    },
    {
      "epoch": 3.8970666666666665,
      "grad_norm": 0.1690063327550888,
      "learning_rate": 2.5643333333333336e-05,
      "loss": 0.0026,
      "step": 73070
    },
    {
      "epoch": 3.8975999999999997,
      "grad_norm": 0.05632540211081505,
      "learning_rate": 2.5640000000000002e-05,
      "loss": 0.0024,
      "step": 73080
    },
    {
      "epoch": 3.8981333333333335,
      "grad_norm": 0.140828937292099,
      "learning_rate": 2.563666666666667e-05,
      "loss": 0.0033,
      "step": 73090
    },
    {
      "epoch": 3.8986666666666667,
      "grad_norm": 0.22532065212726593,
      "learning_rate": 2.5633333333333338e-05,
      "loss": 0.0028,
      "step": 73100
    },
    {
      "epoch": 3.8992,
      "grad_norm": 0.19714349508285522,
      "learning_rate": 2.5629999999999997e-05,
      "loss": 0.0033,
      "step": 73110
    },
    {
      "epoch": 3.8997333333333333,
      "grad_norm": 0.3379756212234497,
      "learning_rate": 2.5626666666666666e-05,
      "loss": 0.0045,
      "step": 73120
    },
    {
      "epoch": 3.9002666666666665,
      "grad_norm": 0.30978715419769287,
      "learning_rate": 2.5623333333333333e-05,
      "loss": 0.0021,
      "step": 73130
    },
    {
      "epoch": 3.9008000000000003,
      "grad_norm": 0.0281645767390728,
      "learning_rate": 2.562e-05,
      "loss": 0.0032,
      "step": 73140
    },
    {
      "epoch": 3.9013333333333335,
      "grad_norm": 0.11265628784894943,
      "learning_rate": 2.5616666666666668e-05,
      "loss": 0.0042,
      "step": 73150
    },
    {
      "epoch": 3.901866666666667,
      "grad_norm": 0.11265745013952255,
      "learning_rate": 2.5613333333333334e-05,
      "loss": 0.0026,
      "step": 73160
    },
    {
      "epoch": 3.9024,
      "grad_norm": 0.3097834587097168,
      "learning_rate": 2.561e-05,
      "loss": 0.0031,
      "step": 73170
    },
    {
      "epoch": 3.9029333333333334,
      "grad_norm": 0.028164660558104515,
      "learning_rate": 2.5606666666666667e-05,
      "loss": 0.0031,
      "step": 73180
    },
    {
      "epoch": 3.9034666666666666,
      "grad_norm": 0.05632851645350456,
      "learning_rate": 2.5603333333333336e-05,
      "loss": 0.0026,
      "step": 73190
    },
    {
      "epoch": 3.904,
      "grad_norm": 0.6758947968482971,
      "learning_rate": 2.5600000000000002e-05,
      "loss": 0.0038,
      "step": 73200
    },
    {
      "epoch": 3.904533333333333,
      "grad_norm": 0.02816256694495678,
      "learning_rate": 2.559666666666667e-05,
      "loss": 0.0046,
      "step": 73210
    },
    {
      "epoch": 3.9050666666666665,
      "grad_norm": 1.889894241102752e-09,
      "learning_rate": 2.5593333333333334e-05,
      "loss": 0.0031,
      "step": 73220
    },
    {
      "epoch": 3.9055999999999997,
      "grad_norm": 0.28162914514541626,
      "learning_rate": 2.5590000000000004e-05,
      "loss": 0.0041,
      "step": 73230
    },
    {
      "epoch": 3.9061333333333335,
      "grad_norm": 0.19715306162834167,
      "learning_rate": 2.558666666666667e-05,
      "loss": 0.0036,
      "step": 73240
    },
    {
      "epoch": 3.9066666666666667,
      "grad_norm": 0.028162704780697823,
      "learning_rate": 2.5583333333333336e-05,
      "loss": 0.0034,
      "step": 73250
    },
    {
      "epoch": 3.9072,
      "grad_norm": 0.16897857189178467,
      "learning_rate": 2.5580000000000002e-05,
      "loss": 0.0037,
      "step": 73260
    },
    {
      "epoch": 3.9077333333333333,
      "grad_norm": 0.28162962198257446,
      "learning_rate": 2.5576666666666665e-05,
      "loss": 0.0021,
      "step": 73270
    },
    {
      "epoch": 3.9082666666666666,
      "grad_norm": 3.244247093192598e-09,
      "learning_rate": 2.557333333333333e-05,
      "loss": 0.0033,
      "step": 73280
    },
    {
      "epoch": 3.9088000000000003,
      "grad_norm": 0.1126556396484375,
      "learning_rate": 2.557e-05,
      "loss": 0.0025,
      "step": 73290
    },
    {
      "epoch": 3.9093333333333335,
      "grad_norm": 0.1408093422651291,
      "learning_rate": 2.5566666666666667e-05,
      "loss": 0.0037,
      "step": 73300
    },
    {
      "epoch": 3.909866666666667,
      "grad_norm": 0.5632917881011963,
      "learning_rate": 2.5563333333333333e-05,
      "loss": 0.0046,
      "step": 73310
    },
    {
      "epoch": 3.9104,
      "grad_norm": 0.22529612481594086,
      "learning_rate": 2.556e-05,
      "loss": 0.0033,
      "step": 73320
    },
    {
      "epoch": 3.9109333333333334,
      "grad_norm": 0.08448819816112518,
      "learning_rate": 2.555666666666667e-05,
      "loss": 0.0025,
      "step": 73330
    },
    {
      "epoch": 3.9114666666666666,
      "grad_norm": 0.1978347897529602,
      "learning_rate": 2.5553333333333335e-05,
      "loss": 0.003,
      "step": 73340
    },
    {
      "epoch": 3.912,
      "grad_norm": 0.0586562305688858,
      "learning_rate": 2.555e-05,
      "loss": 0.0038,
      "step": 73350
    },
    {
      "epoch": 3.912533333333333,
      "grad_norm": 0.16897614300251007,
      "learning_rate": 2.5546666666666667e-05,
      "loss": 0.0027,
      "step": 73360
    },
    {
      "epoch": 3.9130666666666665,
      "grad_norm": 0.25346216559410095,
      "learning_rate": 2.5543333333333337e-05,
      "loss": 0.0029,
      "step": 73370
    },
    {
      "epoch": 3.9135999999999997,
      "grad_norm": 0.28164052963256836,
      "learning_rate": 2.5540000000000003e-05,
      "loss": 0.0029,
      "step": 73380
    },
    {
      "epoch": 3.9141333333333335,
      "grad_norm": 0.5068991780281067,
      "learning_rate": 2.553666666666667e-05,
      "loss": 0.0033,
      "step": 73390
    },
    {
      "epoch": 3.9146666666666667,
      "grad_norm": 0.19714713096618652,
      "learning_rate": 2.553333333333334e-05,
      "loss": 0.0022,
      "step": 73400
    },
    {
      "epoch": 3.9152,
      "grad_norm": 0.11264806985855103,
      "learning_rate": 2.5530000000000005e-05,
      "loss": 0.0021,
      "step": 73410
    },
    {
      "epoch": 3.9157333333333333,
      "grad_norm": 0.056323062628507614,
      "learning_rate": 2.5526666666666664e-05,
      "loss": 0.0035,
      "step": 73420
    },
    {
      "epoch": 3.9162666666666666,
      "grad_norm": 0.08448636531829834,
      "learning_rate": 2.5523333333333333e-05,
      "loss": 0.0023,
      "step": 73430
    },
    {
      "epoch": 3.9168,
      "grad_norm": 0.33794036507606506,
      "learning_rate": 2.552e-05,
      "loss": 0.0022,
      "step": 73440
    },
    {
      "epoch": 3.9173333333333336,
      "grad_norm": 0.25345903635025024,
      "learning_rate": 2.5516666666666666e-05,
      "loss": 0.0029,
      "step": 73450
    },
    {
      "epoch": 3.917866666666667,
      "grad_norm": 0.3097706735134125,
      "learning_rate": 2.5513333333333332e-05,
      "loss": 0.003,
      "step": 73460
    },
    {
      "epoch": 3.9184,
      "grad_norm": 0.3942996859550476,
      "learning_rate": 2.551e-05,
      "loss": 0.0032,
      "step": 73470
    },
    {
      "epoch": 3.9189333333333334,
      "grad_norm": 0.028161486610770226,
      "learning_rate": 2.5506666666666668e-05,
      "loss": 0.003,
      "step": 73480
    },
    {
      "epoch": 3.9194666666666667,
      "grad_norm": 0.056915223598480225,
      "learning_rate": 2.5503333333333334e-05,
      "loss": 0.0017,
      "step": 73490
    },
    {
      "epoch": 3.92,
      "grad_norm": 0.1689809113740921,
      "learning_rate": 2.5500000000000003e-05,
      "loss": 0.0027,
      "step": 73500
    },
    {
      "epoch": 3.920533333333333,
      "grad_norm": 0.28161028027534485,
      "learning_rate": 2.549666666666667e-05,
      "loss": 0.003,
      "step": 73510
    },
    {
      "epoch": 3.9210666666666665,
      "grad_norm": 0.25345394015312195,
      "learning_rate": 2.5493333333333335e-05,
      "loss": 0.0025,
      "step": 73520
    },
    {
      "epoch": 3.9215999999999998,
      "grad_norm": 0.0036551060620695353,
      "learning_rate": 2.549e-05,
      "loss": 0.0034,
      "step": 73530
    },
    {
      "epoch": 3.9221333333333335,
      "grad_norm": 0.11265043914318085,
      "learning_rate": 2.548666666666667e-05,
      "loss": 0.0026,
      "step": 73540
    },
    {
      "epoch": 3.9226666666666667,
      "grad_norm": 1.2070501265171174e-09,
      "learning_rate": 2.5483333333333337e-05,
      "loss": 0.002,
      "step": 73550
    },
    {
      "epoch": 3.9232,
      "grad_norm": 0.33796361088752747,
      "learning_rate": 2.5480000000000003e-05,
      "loss": 0.0036,
      "step": 73560
    },
    {
      "epoch": 3.9237333333333333,
      "grad_norm": 0.16896037757396698,
      "learning_rate": 2.5476666666666666e-05,
      "loss": 0.0036,
      "step": 73570
    },
    {
      "epoch": 3.9242666666666666,
      "grad_norm": 0.14081218838691711,
      "learning_rate": 2.5473333333333332e-05,
      "loss": 0.0031,
      "step": 73580
    },
    {
      "epoch": 3.9248,
      "grad_norm": 0.14081090688705444,
      "learning_rate": 2.547e-05,
      "loss": 0.0023,
      "step": 73590
    },
    {
      "epoch": 3.9253333333333336,
      "grad_norm": 0.2534439265727997,
      "learning_rate": 2.5466666666666668e-05,
      "loss": 0.0023,
      "step": 73600
    },
    {
      "epoch": 3.925866666666667,
      "grad_norm": 0.47877955436706543,
      "learning_rate": 2.5463333333333334e-05,
      "loss": 0.003,
      "step": 73610
    },
    {
      "epoch": 3.9264,
      "grad_norm": 0.6194927096366882,
      "learning_rate": 2.546e-05,
      "loss": 0.002,
      "step": 73620
    },
    {
      "epoch": 3.9269333333333334,
      "grad_norm": 0.05632231384515762,
      "learning_rate": 2.5456666666666666e-05,
      "loss": 0.0023,
      "step": 73630
    },
    {
      "epoch": 3.9274666666666667,
      "grad_norm": 0.16895751655101776,
      "learning_rate": 2.5453333333333336e-05,
      "loss": 0.0021,
      "step": 73640
    },
    {
      "epoch": 3.928,
      "grad_norm": 0.11264652013778687,
      "learning_rate": 2.5450000000000002e-05,
      "loss": 0.003,
      "step": 73650
    },
    {
      "epoch": 3.928533333333333,
      "grad_norm": 0.16896525025367737,
      "learning_rate": 2.5446666666666668e-05,
      "loss": 0.0024,
      "step": 73660
    },
    {
      "epoch": 3.9290666666666665,
      "grad_norm": 0.22527910768985748,
      "learning_rate": 2.5443333333333334e-05,
      "loss": 0.0029,
      "step": 73670
    },
    {
      "epoch": 3.9295999999999998,
      "grad_norm": 3.811061244363145e-09,
      "learning_rate": 2.5440000000000004e-05,
      "loss": 0.0027,
      "step": 73680
    },
    {
      "epoch": 3.9301333333333335,
      "grad_norm": 0.028160985559225082,
      "learning_rate": 2.543666666666667e-05,
      "loss": 0.0026,
      "step": 73690
    },
    {
      "epoch": 3.9306666666666668,
      "grad_norm": 0.14080993831157684,
      "learning_rate": 2.5433333333333336e-05,
      "loss": 0.0049,
      "step": 73700
    },
    {
      "epoch": 3.9312,
      "grad_norm": 0.25343427062034607,
      "learning_rate": 2.5430000000000002e-05,
      "loss": 0.0031,
      "step": 73710
    },
    {
      "epoch": 3.9317333333333333,
      "grad_norm": 0.08448302000761032,
      "learning_rate": 2.5426666666666665e-05,
      "loss": 0.0028,
      "step": 73720
    },
    {
      "epoch": 3.9322666666666666,
      "grad_norm": 0.2252911776304245,
      "learning_rate": 2.542333333333333e-05,
      "loss": 0.003,
      "step": 73730
    },
    {
      "epoch": 3.9328,
      "grad_norm": 0.198146253824234,
      "learning_rate": 2.542e-05,
      "loss": 0.0018,
      "step": 73740
    },
    {
      "epoch": 3.9333333333333336,
      "grad_norm": 0.4328251779079437,
      "learning_rate": 2.5416666666666667e-05,
      "loss": 0.002,
      "step": 73750
    },
    {
      "epoch": 3.933866666666667,
      "grad_norm": 0.22528666257858276,
      "learning_rate": 2.5413333333333333e-05,
      "loss": 0.0023,
      "step": 73760
    },
    {
      "epoch": 3.9344,
      "grad_norm": 0.28159451484680176,
      "learning_rate": 2.541e-05,
      "loss": 0.002,
      "step": 73770
    },
    {
      "epoch": 3.9349333333333334,
      "grad_norm": 0.028160084038972855,
      "learning_rate": 2.540666666666667e-05,
      "loss": 0.0044,
      "step": 73780
    },
    {
      "epoch": 3.9354666666666667,
      "grad_norm": 0.2540719211101532,
      "learning_rate": 2.5403333333333335e-05,
      "loss": 0.0035,
      "step": 73790
    },
    {
      "epoch": 3.936,
      "grad_norm": 0.14080002903938293,
      "learning_rate": 2.54e-05,
      "loss": 0.0026,
      "step": 73800
    },
    {
      "epoch": 3.936533333333333,
      "grad_norm": 0.05631782487034798,
      "learning_rate": 2.539666666666667e-05,
      "loss": 0.003,
      "step": 73810
    },
    {
      "epoch": 3.9370666666666665,
      "grad_norm": 7.506028687487287e-09,
      "learning_rate": 2.5393333333333336e-05,
      "loss": 0.0028,
      "step": 73820
    },
    {
      "epoch": 3.9375999999999998,
      "grad_norm": 0.05631863325834274,
      "learning_rate": 2.5390000000000003e-05,
      "loss": 0.0029,
      "step": 73830
    },
    {
      "epoch": 3.9381333333333335,
      "grad_norm": 0.056318335235118866,
      "learning_rate": 2.538666666666667e-05,
      "loss": 0.0023,
      "step": 73840
    },
    {
      "epoch": 3.9386666666666668,
      "grad_norm": 0.19711102545261383,
      "learning_rate": 2.5383333333333338e-05,
      "loss": 0.0025,
      "step": 73850
    },
    {
      "epoch": 3.9392,
      "grad_norm": 0.3379048705101013,
      "learning_rate": 2.5380000000000004e-05,
      "loss": 0.0028,
      "step": 73860
    },
    {
      "epoch": 3.9397333333333333,
      "grad_norm": 0.1407977193593979,
      "learning_rate": 2.5376666666666664e-05,
      "loss": 0.0026,
      "step": 73870
    },
    {
      "epoch": 3.9402666666666666,
      "grad_norm": 2.668402609984355e-09,
      "learning_rate": 2.5373333333333333e-05,
      "loss": 0.003,
      "step": 73880
    },
    {
      "epoch": 3.9408,
      "grad_norm": 0.3942719101905823,
      "learning_rate": 2.537e-05,
      "loss": 0.003,
      "step": 73890
    },
    {
      "epoch": 3.9413333333333336,
      "grad_norm": 0.11393799632787704,
      "learning_rate": 2.5366666666666665e-05,
      "loss": 0.0024,
      "step": 73900
    },
    {
      "epoch": 3.941866666666667,
      "grad_norm": 0.19712203741073608,
      "learning_rate": 2.5363333333333335e-05,
      "loss": 0.0029,
      "step": 73910
    },
    {
      "epoch": 3.9424,
      "grad_norm": 0.1408025175333023,
      "learning_rate": 2.536e-05,
      "loss": 0.0025,
      "step": 73920
    },
    {
      "epoch": 3.9429333333333334,
      "grad_norm": 0.16894975304603577,
      "learning_rate": 2.5356666666666667e-05,
      "loss": 0.0031,
      "step": 73930
    },
    {
      "epoch": 3.9434666666666667,
      "grad_norm": 0.2816094756126404,
      "learning_rate": 2.5353333333333333e-05,
      "loss": 0.0025,
      "step": 73940
    },
    {
      "epoch": 3.944,
      "grad_norm": 0.33789652585983276,
      "learning_rate": 2.5350000000000003e-05,
      "loss": 0.0038,
      "step": 73950
    },
    {
      "epoch": 3.9445333333333332,
      "grad_norm": 0.19711613655090332,
      "learning_rate": 2.534666666666667e-05,
      "loss": 0.0022,
      "step": 73960
    },
    {
      "epoch": 3.9450666666666665,
      "grad_norm": 0.28158456087112427,
      "learning_rate": 2.5343333333333335e-05,
      "loss": 0.0018,
      "step": 73970
    },
    {
      "epoch": 3.9455999999999998,
      "grad_norm": 0.16894960403442383,
      "learning_rate": 2.534e-05,
      "loss": 0.0025,
      "step": 73980
    },
    {
      "epoch": 3.9461333333333335,
      "grad_norm": 0.16895370185375214,
      "learning_rate": 2.533666666666667e-05,
      "loss": 0.0024,
      "step": 73990
    },
    {
      "epoch": 3.9466666666666668,
      "grad_norm": 0.2534339427947998,
      "learning_rate": 2.5333333333333337e-05,
      "loss": 0.0027,
      "step": 74000
    },
    {
      "epoch": 3.9472,
      "grad_norm": 0.028158411383628845,
      "learning_rate": 2.5330000000000003e-05,
      "loss": 0.0028,
      "step": 74010
    },
    {
      "epoch": 3.9477333333333333,
      "grad_norm": 0.1407967358827591,
      "learning_rate": 2.5326666666666666e-05,
      "loss": 0.0021,
      "step": 74020
    },
    {
      "epoch": 3.9482666666666666,
      "grad_norm": 0.11263468861579895,
      "learning_rate": 2.5323333333333332e-05,
      "loss": 0.0032,
      "step": 74030
    },
    {
      "epoch": 3.9488,
      "grad_norm": 0.33791133761405945,
      "learning_rate": 2.5319999999999998e-05,
      "loss": 0.0029,
      "step": 74040
    },
    {
      "epoch": 3.9493333333333336,
      "grad_norm": 0.14080139994621277,
      "learning_rate": 2.5316666666666668e-05,
      "loss": 0.0033,
      "step": 74050
    },
    {
      "epoch": 3.949866666666667,
      "grad_norm": 0.28159284591674805,
      "learning_rate": 2.5313333333333334e-05,
      "loss": 0.0022,
      "step": 74060
    },
    {
      "epoch": 3.9504,
      "grad_norm": 0.7614930868148804,
      "learning_rate": 2.531e-05,
      "loss": 0.0034,
      "step": 74070
    },
    {
      "epoch": 3.9509333333333334,
      "grad_norm": 0.08447485417127609,
      "learning_rate": 2.5306666666666666e-05,
      "loss": 0.0036,
      "step": 74080
    },
    {
      "epoch": 3.9514666666666667,
      "grad_norm": 0.2252529412508011,
      "learning_rate": 2.5303333333333336e-05,
      "loss": 0.0034,
      "step": 74090
    },
    {
      "epoch": 3.952,
      "grad_norm": 0.05631532520055771,
      "learning_rate": 2.5300000000000002e-05,
      "loss": 0.004,
      "step": 74100
    },
    {
      "epoch": 3.9525333333333332,
      "grad_norm": 0.11262790858745575,
      "learning_rate": 2.5296666666666668e-05,
      "loss": 0.003,
      "step": 74110
    },
    {
      "epoch": 3.9530666666666665,
      "grad_norm": 0.14079056680202484,
      "learning_rate": 2.5293333333333334e-05,
      "loss": 0.004,
      "step": 74120
    },
    {
      "epoch": 3.9536,
      "grad_norm": 4.049692137186867e-09,
      "learning_rate": 2.5290000000000004e-05,
      "loss": 0.0028,
      "step": 74130
    },
    {
      "epoch": 3.9541333333333335,
      "grad_norm": 0.08455069363117218,
      "learning_rate": 2.528666666666667e-05,
      "loss": 0.0029,
      "step": 74140
    },
    {
      "epoch": 3.9546666666666668,
      "grad_norm": 0.3100203275680542,
      "learning_rate": 2.5283333333333336e-05,
      "loss": 0.003,
      "step": 74150
    },
    {
      "epoch": 3.9552,
      "grad_norm": 0.11273360997438431,
      "learning_rate": 2.5280000000000005e-05,
      "loss": 0.0028,
      "step": 74160
    },
    {
      "epoch": 3.9557333333333333,
      "grad_norm": 0.1409139633178711,
      "learning_rate": 2.5276666666666665e-05,
      "loss": 0.0032,
      "step": 74170
    },
    {
      "epoch": 3.9562666666666666,
      "grad_norm": 0.3100016117095947,
      "learning_rate": 2.527333333333333e-05,
      "loss": 0.0036,
      "step": 74180
    },
    {
      "epoch": 3.9568,
      "grad_norm": 0.31001609563827515,
      "learning_rate": 2.527e-05,
      "loss": 0.0022,
      "step": 74190
    },
    {
      "epoch": 3.9573333333333336,
      "grad_norm": 0.19727745652198792,
      "learning_rate": 2.5266666666666666e-05,
      "loss": 0.0018,
      "step": 74200
    },
    {
      "epoch": 3.957866666666667,
      "grad_norm": 0.47910863161087036,
      "learning_rate": 2.5263333333333333e-05,
      "loss": 0.0038,
      "step": 74210
    },
    {
      "epoch": 3.9584,
      "grad_norm": 0.028182610869407654,
      "learning_rate": 2.526e-05,
      "loss": 0.0036,
      "step": 74220
    },
    {
      "epoch": 3.9589333333333334,
      "grad_norm": 0.3100050687789917,
      "learning_rate": 2.5256666666666668e-05,
      "loss": 0.0038,
      "step": 74230
    },
    {
      "epoch": 3.9594666666666667,
      "grad_norm": 0.19728022813796997,
      "learning_rate": 2.5253333333333334e-05,
      "loss": 0.0033,
      "step": 74240
    },
    {
      "epoch": 3.96,
      "grad_norm": 0.2536323368549347,
      "learning_rate": 2.525e-05,
      "loss": 0.0034,
      "step": 74250
    },
    {
      "epoch": 3.9605333333333332,
      "grad_norm": 0.1691025197505951,
      "learning_rate": 2.524666666666667e-05,
      "loss": 0.0031,
      "step": 74260
    },
    {
      "epoch": 3.9610666666666665,
      "grad_norm": 0.11272822320461273,
      "learning_rate": 2.5243333333333336e-05,
      "loss": 0.0023,
      "step": 74270
    },
    {
      "epoch": 3.9616,
      "grad_norm": 0.02818293496966362,
      "learning_rate": 2.5240000000000002e-05,
      "loss": 0.0029,
      "step": 74280
    },
    {
      "epoch": 3.962133333333333,
      "grad_norm": 0.14210158586502075,
      "learning_rate": 2.523666666666667e-05,
      "loss": 0.0036,
      "step": 74290
    },
    {
      "epoch": 3.962666666666667,
      "grad_norm": 0.253653347492218,
      "learning_rate": 2.5233333333333338e-05,
      "loss": 0.0033,
      "step": 74300
    },
    {
      "epoch": 3.9632,
      "grad_norm": 0.19727475941181183,
      "learning_rate": 2.5230000000000004e-05,
      "loss": 0.003,
      "step": 74310
    },
    {
      "epoch": 3.9637333333333333,
      "grad_norm": 0.05636486038565636,
      "learning_rate": 2.5226666666666663e-05,
      "loss": 0.0023,
      "step": 74320
    },
    {
      "epoch": 3.9642666666666666,
      "grad_norm": 0.031432367861270905,
      "learning_rate": 2.5223333333333333e-05,
      "loss": 0.005,
      "step": 74330
    },
    {
      "epoch": 3.9648,
      "grad_norm": 0.22546428442001343,
      "learning_rate": 2.522e-05,
      "loss": 0.005,
      "step": 74340
    },
    {
      "epoch": 3.9653333333333336,
      "grad_norm": 0.04346730187535286,
      "learning_rate": 2.5216666666666665e-05,
      "loss": 0.0029,
      "step": 74350
    },
    {
      "epoch": 3.965866666666667,
      "grad_norm": 0.4231759309768677,
      "learning_rate": 2.5213333333333335e-05,
      "loss": 0.0028,
      "step": 74360
    },
    {
      "epoch": 3.9664,
      "grad_norm": 0.22546052932739258,
      "learning_rate": 2.521e-05,
      "loss": 0.0028,
      "step": 74370
    },
    {
      "epoch": 3.9669333333333334,
      "grad_norm": 0.22583729028701782,
      "learning_rate": 2.5206666666666667e-05,
      "loss": 0.0034,
      "step": 74380
    },
    {
      "epoch": 3.9674666666666667,
      "grad_norm": 0.02818330191075802,
      "learning_rate": 2.5203333333333333e-05,
      "loss": 0.0016,
      "step": 74390
    },
    {
      "epoch": 3.968,
      "grad_norm": 0.2818349599838257,
      "learning_rate": 2.5200000000000003e-05,
      "loss": 0.0032,
      "step": 74400
    },
    {
      "epoch": 3.9685333333333332,
      "grad_norm": 0.08454422652721405,
      "learning_rate": 2.519666666666667e-05,
      "loss": 0.0024,
      "step": 74410
    },
    {
      "epoch": 3.9690666666666665,
      "grad_norm": 0.1972905844449997,
      "learning_rate": 2.5193333333333335e-05,
      "loss": 0.0039,
      "step": 74420
    },
    {
      "epoch": 3.9696,
      "grad_norm": 0.3663480281829834,
      "learning_rate": 2.519e-05,
      "loss": 0.0033,
      "step": 74430
    },
    {
      "epoch": 3.970133333333333,
      "grad_norm": 0.16979922354221344,
      "learning_rate": 2.518666666666667e-05,
      "loss": 0.0034,
      "step": 74440
    },
    {
      "epoch": 3.970666666666667,
      "grad_norm": 0.11485356092453003,
      "learning_rate": 2.5183333333333337e-05,
      "loss": 0.0032,
      "step": 74450
    },
    {
      "epoch": 3.9712,
      "grad_norm": 0.056366682052612305,
      "learning_rate": 2.5180000000000003e-05,
      "loss": 0.0025,
      "step": 74460
    },
    {
      "epoch": 3.9717333333333333,
      "grad_norm": 0.08455198258161545,
      "learning_rate": 2.517666666666667e-05,
      "loss": 0.0028,
      "step": 74470
    },
    {
      "epoch": 3.9722666666666666,
      "grad_norm": 0.05636743828654289,
      "learning_rate": 2.5173333333333332e-05,
      "loss": 0.0025,
      "step": 74480
    },
    {
      "epoch": 3.9728,
      "grad_norm": 0.22545279562473297,
      "learning_rate": 2.5169999999999998e-05,
      "loss": 0.002,
      "step": 74490
    },
    {
      "epoch": 3.9733333333333336,
      "grad_norm": 0.31059467792510986,
      "learning_rate": 2.5166666666666667e-05,
      "loss": 0.0026,
      "step": 74500
    },
    {
      "epoch": 3.973866666666667,
      "grad_norm": 0.18123449385166168,
      "learning_rate": 2.5163333333333334e-05,
      "loss": 0.0024,
      "step": 74510
    },
    {
      "epoch": 3.9744,
      "grad_norm": 0.11273879557847977,
      "learning_rate": 2.516e-05,
      "loss": 0.0026,
      "step": 74520
    },
    {
      "epoch": 3.9749333333333334,
      "grad_norm": 0.19727720320224762,
      "learning_rate": 2.5156666666666666e-05,
      "loss": 0.0029,
      "step": 74530
    },
    {
      "epoch": 3.9754666666666667,
      "grad_norm": 0.028182758018374443,
      "learning_rate": 2.5153333333333335e-05,
      "loss": 0.0019,
      "step": 74540
    },
    {
      "epoch": 3.976,
      "grad_norm": 0.25365006923675537,
      "learning_rate": 2.515e-05,
      "loss": 0.0029,
      "step": 74550
    },
    {
      "epoch": 3.9765333333333333,
      "grad_norm": 0.16909271478652954,
      "learning_rate": 2.5146666666666668e-05,
      "loss": 0.0015,
      "step": 74560
    },
    {
      "epoch": 3.9770666666666665,
      "grad_norm": 0.14091551303863525,
      "learning_rate": 2.5143333333333334e-05,
      "loss": 0.002,
      "step": 74570
    },
    {
      "epoch": 3.9776,
      "grad_norm": 2.553746547562241e-09,
      "learning_rate": 2.5140000000000003e-05,
      "loss": 0.003,
      "step": 74580
    },
    {
      "epoch": 3.978133333333333,
      "grad_norm": 0.1409185230731964,
      "learning_rate": 2.513666666666667e-05,
      "loss": 0.0021,
      "step": 74590
    },
    {
      "epoch": 3.978666666666667,
      "grad_norm": 0.19729214906692505,
      "learning_rate": 2.5133333333333336e-05,
      "loss": 0.002,
      "step": 74600
    },
    {
      "epoch": 3.9792,
      "grad_norm": 0.05636262521147728,
      "learning_rate": 2.5130000000000005e-05,
      "loss": 0.0021,
      "step": 74610
    },
    {
      "epoch": 3.9797333333333333,
      "grad_norm": 0.08454855531454086,
      "learning_rate": 2.512666666666667e-05,
      "loss": 0.0016,
      "step": 74620
    },
    {
      "epoch": 3.9802666666666666,
      "grad_norm": 0.1972782164812088,
      "learning_rate": 2.512333333333333e-05,
      "loss": 0.0028,
      "step": 74630
    },
    {
      "epoch": 3.9808,
      "grad_norm": 0.1409190446138382,
      "learning_rate": 2.512e-05,
      "loss": 0.0033,
      "step": 74640
    },
    {
      "epoch": 3.981333333333333,
      "grad_norm": 0.028181802481412888,
      "learning_rate": 2.5116666666666666e-05,
      "loss": 0.0033,
      "step": 74650
    },
    {
      "epoch": 3.981866666666667,
      "grad_norm": 0.11272753030061722,
      "learning_rate": 2.5113333333333332e-05,
      "loss": 0.0032,
      "step": 74660
    },
    {
      "epoch": 3.9824,
      "grad_norm": 0.08454965054988861,
      "learning_rate": 2.5110000000000002e-05,
      "loss": 0.0025,
      "step": 74670
    },
    {
      "epoch": 3.9829333333333334,
      "grad_norm": 0.3945674002170563,
      "learning_rate": 2.5106666666666668e-05,
      "loss": 0.002,
      "step": 74680
    },
    {
      "epoch": 3.9834666666666667,
      "grad_norm": 0.19726914167404175,
      "learning_rate": 2.5103333333333334e-05,
      "loss": 0.0031,
      "step": 74690
    },
    {
      "epoch": 3.984,
      "grad_norm": 0.05636429414153099,
      "learning_rate": 2.51e-05,
      "loss": 0.0018,
      "step": 74700
    },
    {
      "epoch": 3.9845333333333333,
      "grad_norm": 0.39455467462539673,
      "learning_rate": 2.509666666666667e-05,
      "loss": 0.0022,
      "step": 74710
    },
    {
      "epoch": 3.9850666666666665,
      "grad_norm": 0.36635822057724,
      "learning_rate": 2.5093333333333336e-05,
      "loss": 0.0027,
      "step": 74720
    },
    {
      "epoch": 3.9856,
      "grad_norm": 0.2254500538110733,
      "learning_rate": 2.5090000000000002e-05,
      "loss": 0.0028,
      "step": 74730
    },
    {
      "epoch": 3.986133333333333,
      "grad_norm": 0.28182628750801086,
      "learning_rate": 2.5086666666666668e-05,
      "loss": 0.0023,
      "step": 74740
    },
    {
      "epoch": 3.986666666666667,
      "grad_norm": 0.2536587715148926,
      "learning_rate": 2.5083333333333338e-05,
      "loss": 0.002,
      "step": 74750
    },
    {
      "epoch": 3.9872,
      "grad_norm": 0.2728014886379242,
      "learning_rate": 2.5080000000000004e-05,
      "loss": 0.0028,
      "step": 74760
    },
    {
      "epoch": 3.9877333333333334,
      "grad_norm": 0.2598894536495209,
      "learning_rate": 2.507666666666667e-05,
      "loss": 0.0033,
      "step": 74770
    },
    {
      "epoch": 3.9882666666666666,
      "grad_norm": 0.0563642755150795,
      "learning_rate": 2.5073333333333333e-05,
      "loss": 0.0023,
      "step": 74780
    },
    {
      "epoch": 3.9888,
      "grad_norm": 0.19728121161460876,
      "learning_rate": 2.507e-05,
      "loss": 0.0022,
      "step": 74790
    },
    {
      "epoch": 3.989333333333333,
      "grad_norm": 0.05636313185095787,
      "learning_rate": 2.5066666666666665e-05,
      "loss": 0.0035,
      "step": 74800
    },
    {
      "epoch": 3.989866666666667,
      "grad_norm": 0.16908468306064606,
      "learning_rate": 2.5063333333333334e-05,
      "loss": 0.0028,
      "step": 74810
    },
    {
      "epoch": 3.9904,
      "grad_norm": 0.39454740285873413,
      "learning_rate": 2.506e-05,
      "loss": 0.0022,
      "step": 74820
    },
    {
      "epoch": 3.9909333333333334,
      "grad_norm": 0.028182903304696083,
      "learning_rate": 2.5056666666666667e-05,
      "loss": 0.003,
      "step": 74830
    },
    {
      "epoch": 3.9914666666666667,
      "grad_norm": 0.2536304295063019,
      "learning_rate": 2.5053333333333333e-05,
      "loss": 0.0025,
      "step": 74840
    },
    {
      "epoch": 3.992,
      "grad_norm": 0.21741332113742828,
      "learning_rate": 2.5050000000000002e-05,
      "loss": 0.0039,
      "step": 74850
    },
    {
      "epoch": 3.9925333333333333,
      "grad_norm": 0.19726671278476715,
      "learning_rate": 2.504666666666667e-05,
      "loss": 0.0024,
      "step": 74860
    },
    {
      "epoch": 3.9930666666666665,
      "grad_norm": 0.22544685006141663,
      "learning_rate": 2.5043333333333335e-05,
      "loss": 0.002,
      "step": 74870
    },
    {
      "epoch": 3.9936,
      "grad_norm": 0.28181448578834534,
      "learning_rate": 2.504e-05,
      "loss": 0.0031,
      "step": 74880
    },
    {
      "epoch": 3.994133333333333,
      "grad_norm": 0.05636133998632431,
      "learning_rate": 2.503666666666667e-05,
      "loss": 0.0027,
      "step": 74890
    },
    {
      "epoch": 3.994666666666667,
      "grad_norm": 0.14090484380722046,
      "learning_rate": 2.5033333333333336e-05,
      "loss": 0.0033,
      "step": 74900
    },
    {
      "epoch": 3.9952,
      "grad_norm": 0.2536223232746124,
      "learning_rate": 2.5030000000000003e-05,
      "loss": 0.0028,
      "step": 74910
    },
    {
      "epoch": 3.9957333333333334,
      "grad_norm": 0.16909226775169373,
      "learning_rate": 2.5026666666666672e-05,
      "loss": 0.0025,
      "step": 74920
    },
    {
      "epoch": 3.9962666666666666,
      "grad_norm": 0.14091216027736664,
      "learning_rate": 2.502333333333333e-05,
      "loss": 0.0037,
      "step": 74930
    },
    {
      "epoch": 3.9968,
      "grad_norm": 0.14090844988822937,
      "learning_rate": 2.5019999999999998e-05,
      "loss": 0.0033,
      "step": 74940
    },
    {
      "epoch": 3.997333333333333,
      "grad_norm": 0.2254423201084137,
      "learning_rate": 2.5016666666666667e-05,
      "loss": 0.0034,
      "step": 74950
    },
    {
      "epoch": 3.997866666666667,
      "grad_norm": 0.31000250577926636,
      "learning_rate": 2.5013333333333333e-05,
      "loss": 0.0035,
      "step": 74960
    },
    {
      "epoch": 3.9984,
      "grad_norm": 0.14090240001678467,
      "learning_rate": 2.501e-05,
      "loss": 0.0026,
      "step": 74970
    },
    {
      "epoch": 3.9989333333333335,
      "grad_norm": 0.14090967178344727,
      "learning_rate": 2.5006666666666666e-05,
      "loss": 0.0027,
      "step": 74980
    },
    {
      "epoch": 3.9994666666666667,
      "grad_norm": 0.1408996880054474,
      "learning_rate": 2.5003333333333335e-05,
      "loss": 0.0029,
      "step": 74990
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.11272888630628586,
      "learning_rate": 2.5e-05,
      "loss": 0.0018,
      "step": 75000
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.002995244227349758,
      "eval_runtime": 168.1741,
      "eval_samples_per_second": 1486.555,
      "eval_steps_per_second": 37.164,
      "step": 75000
    },
    {
      "epoch": 4.000533333333333,
      "grad_norm": 0.4226841628551483,
      "learning_rate": 2.4996666666666667e-05,
      "loss": 0.0023,
      "step": 75010
    },
    {
      "epoch": 4.0010666666666665,
      "grad_norm": 0.25363728404045105,
      "learning_rate": 2.4993333333333337e-05,
      "loss": 0.0031,
      "step": 75020
    },
    {
      "epoch": 4.0016,
      "grad_norm": 0.169075146317482,
      "learning_rate": 2.4990000000000003e-05,
      "loss": 0.0021,
      "step": 75030
    },
    {
      "epoch": 4.002133333333333,
      "grad_norm": 0.14091017842292786,
      "learning_rate": 2.4986666666666666e-05,
      "loss": 0.002,
      "step": 75040
    },
    {
      "epoch": 4.002666666666666,
      "grad_norm": 0.5354135632514954,
      "learning_rate": 2.4983333333333335e-05,
      "loss": 0.0027,
      "step": 75050
    },
    {
      "epoch": 4.0032,
      "grad_norm": 0.028180204331874847,
      "learning_rate": 2.498e-05,
      "loss": 0.0043,
      "step": 75060
    },
    {
      "epoch": 4.003733333333333,
      "grad_norm": 0.2818049490451813,
      "learning_rate": 2.4976666666666668e-05,
      "loss": 0.0032,
      "step": 75070
    },
    {
      "epoch": 4.004266666666667,
      "grad_norm": 0.42267364263534546,
      "learning_rate": 2.4973333333333334e-05,
      "loss": 0.0037,
      "step": 75080
    },
    {
      "epoch": 4.0048,
      "grad_norm": 0.05636047199368477,
      "learning_rate": 2.4970000000000003e-05,
      "loss": 0.0023,
      "step": 75090
    },
    {
      "epoch": 4.005333333333334,
      "grad_norm": 0.1972566694021225,
      "learning_rate": 2.496666666666667e-05,
      "loss": 0.0033,
      "step": 75100
    },
    {
      "epoch": 4.005866666666667,
      "grad_norm": 0.1408994495868683,
      "learning_rate": 2.4963333333333335e-05,
      "loss": 0.0038,
      "step": 75110
    },
    {
      "epoch": 4.0064,
      "grad_norm": 0.19725890457630157,
      "learning_rate": 2.496e-05,
      "loss": 0.0034,
      "step": 75120
    },
    {
      "epoch": 4.0069333333333335,
      "grad_norm": 0.20680062472820282,
      "learning_rate": 2.4956666666666668e-05,
      "loss": 0.0032,
      "step": 75130
    },
    {
      "epoch": 4.007466666666667,
      "grad_norm": 0.28402093052864075,
      "learning_rate": 2.4953333333333334e-05,
      "loss": 0.0034,
      "step": 75140
    },
    {
      "epoch": 4.008,
      "grad_norm": 0.05635697394609451,
      "learning_rate": 2.495e-05,
      "loss": 0.0031,
      "step": 75150
    },
    {
      "epoch": 4.008533333333333,
      "grad_norm": 0.5353867411613464,
      "learning_rate": 2.494666666666667e-05,
      "loss": 0.0027,
      "step": 75160
    },
    {
      "epoch": 4.009066666666667,
      "grad_norm": 0.11902609467506409,
      "learning_rate": 2.4943333333333336e-05,
      "loss": 0.0026,
      "step": 75170
    },
    {
      "epoch": 4.0096,
      "grad_norm": 0.05635829642415047,
      "learning_rate": 2.4940000000000002e-05,
      "loss": 0.0017,
      "step": 75180
    },
    {
      "epoch": 4.010133333333333,
      "grad_norm": 0.028178611770272255,
      "learning_rate": 2.4936666666666668e-05,
      "loss": 0.0024,
      "step": 75190
    },
    {
      "epoch": 4.010666666666666,
      "grad_norm": 0.39450550079345703,
      "learning_rate": 2.4933333333333334e-05,
      "loss": 0.0038,
      "step": 75200
    },
    {
      "epoch": 4.0112,
      "grad_norm": 0.05635915696620941,
      "learning_rate": 2.493e-05,
      "loss": 0.0029,
      "step": 75210
    },
    {
      "epoch": 4.011733333333333,
      "grad_norm": 0.14089597761631012,
      "learning_rate": 2.4926666666666666e-05,
      "loss": 0.0032,
      "step": 75220
    },
    {
      "epoch": 4.012266666666667,
      "grad_norm": 0.11602751910686493,
      "learning_rate": 2.4923333333333336e-05,
      "loss": 0.003,
      "step": 75230
    },
    {
      "epoch": 4.0128,
      "grad_norm": 0.028179820626974106,
      "learning_rate": 2.4920000000000002e-05,
      "loss": 0.002,
      "step": 75240
    },
    {
      "epoch": 4.013333333333334,
      "grad_norm": 0.08454107493162155,
      "learning_rate": 2.4916666666666668e-05,
      "loss": 0.0024,
      "step": 75250
    },
    {
      "epoch": 4.013866666666667,
      "grad_norm": 0.1408914029598236,
      "learning_rate": 2.4913333333333334e-05,
      "loss": 0.0025,
      "step": 75260
    },
    {
      "epoch": 4.0144,
      "grad_norm": 0.0845390036702156,
      "learning_rate": 2.491e-05,
      "loss": 0.002,
      "step": 75270
    },
    {
      "epoch": 4.0149333333333335,
      "grad_norm": 0.16906480491161346,
      "learning_rate": 2.4906666666666666e-05,
      "loss": 0.0027,
      "step": 75280
    },
    {
      "epoch": 4.015466666666667,
      "grad_norm": 0.19724754989147186,
      "learning_rate": 2.4903333333333333e-05,
      "loss": 0.0017,
      "step": 75290
    },
    {
      "epoch": 4.016,
      "grad_norm": 0.16906797885894775,
      "learning_rate": 2.4900000000000002e-05,
      "loss": 0.0033,
      "step": 75300
    },
    {
      "epoch": 4.016533333333333,
      "grad_norm": 0.08454091101884842,
      "learning_rate": 2.4896666666666668e-05,
      "loss": 0.0029,
      "step": 75310
    },
    {
      "epoch": 4.017066666666667,
      "grad_norm": 0.08453335613012314,
      "learning_rate": 2.4893333333333334e-05,
      "loss": 0.0032,
      "step": 75320
    },
    {
      "epoch": 4.0176,
      "grad_norm": 0.05635512247681618,
      "learning_rate": 2.489e-05,
      "loss": 0.0035,
      "step": 75330
    },
    {
      "epoch": 4.018133333333333,
      "grad_norm": 0.30996057391166687,
      "learning_rate": 2.488666666666667e-05,
      "loss": 0.002,
      "step": 75340
    },
    {
      "epoch": 4.018666666666666,
      "grad_norm": 0.08453253656625748,
      "learning_rate": 2.4883333333333333e-05,
      "loss": 0.0033,
      "step": 75350
    },
    {
      "epoch": 4.0192,
      "grad_norm": 0.30995646119117737,
      "learning_rate": 2.488e-05,
      "loss": 0.0032,
      "step": 75360
    },
    {
      "epoch": 4.019733333333333,
      "grad_norm": 0.08453446626663208,
      "learning_rate": 2.487666666666667e-05,
      "loss": 0.0033,
      "step": 75370
    },
    {
      "epoch": 4.020266666666667,
      "grad_norm": 0.33813363313674927,
      "learning_rate": 2.4873333333333335e-05,
      "loss": 0.0026,
      "step": 75380
    },
    {
      "epoch": 4.0208,
      "grad_norm": 0.1408909261226654,
      "learning_rate": 2.487e-05,
      "loss": 0.003,
      "step": 75390
    },
    {
      "epoch": 4.021333333333334,
      "grad_norm": 0.11271172761917114,
      "learning_rate": 2.486666666666667e-05,
      "loss": 0.0023,
      "step": 75400
    },
    {
      "epoch": 4.021866666666667,
      "grad_norm": 0.2254161685705185,
      "learning_rate": 2.4863333333333336e-05,
      "loss": 0.0024,
      "step": 75410
    },
    {
      "epoch": 4.0224,
      "grad_norm": 0.08453142642974854,
      "learning_rate": 2.486e-05,
      "loss": 0.0027,
      "step": 75420
    },
    {
      "epoch": 4.0229333333333335,
      "grad_norm": 0.1408882737159729,
      "learning_rate": 2.485666666666667e-05,
      "loss": 0.0048,
      "step": 75430
    },
    {
      "epoch": 4.023466666666667,
      "grad_norm": 0.3099428415298462,
      "learning_rate": 2.4853333333333335e-05,
      "loss": 0.0027,
      "step": 75440
    },
    {
      "epoch": 4.024,
      "grad_norm": 0.1127169281244278,
      "learning_rate": 2.485e-05,
      "loss": 0.0024,
      "step": 75450
    },
    {
      "epoch": 4.024533333333333,
      "grad_norm": 3.716256857799749e-09,
      "learning_rate": 2.4846666666666667e-05,
      "loss": 0.003,
      "step": 75460
    },
    {
      "epoch": 4.025066666666667,
      "grad_norm": 0.05635401979088783,
      "learning_rate": 2.4843333333333337e-05,
      "loss": 0.0025,
      "step": 75470
    },
    {
      "epoch": 4.0256,
      "grad_norm": 0.1408889889717102,
      "learning_rate": 2.4840000000000003e-05,
      "loss": 0.0032,
      "step": 75480
    },
    {
      "epoch": 4.026133333333333,
      "grad_norm": 0.36631307005882263,
      "learning_rate": 2.483666666666667e-05,
      "loss": 0.0029,
      "step": 75490
    },
    {
      "epoch": 4.026666666666666,
      "grad_norm": 1.7831294536590576,
      "learning_rate": 2.4833333333333335e-05,
      "loss": 0.0033,
      "step": 75500
    },
    {
      "epoch": 4.0272,
      "grad_norm": 0.14200462400913239,
      "learning_rate": 2.483e-05,
      "loss": 0.0033,
      "step": 75510
    },
    {
      "epoch": 4.027733333333333,
      "grad_norm": 0.4226398766040802,
      "learning_rate": 2.4826666666666667e-05,
      "loss": 0.0028,
      "step": 75520
    },
    {
      "epoch": 4.028266666666667,
      "grad_norm": 0.3381507694721222,
      "learning_rate": 2.4823333333333333e-05,
      "loss": 0.0045,
      "step": 75530
    },
    {
      "epoch": 4.0288,
      "grad_norm": 0.14087943732738495,
      "learning_rate": 2.4820000000000003e-05,
      "loss": 0.0028,
      "step": 75540
    },
    {
      "epoch": 4.029333333333334,
      "grad_norm": 1.6469110519778951e-09,
      "learning_rate": 2.481666666666667e-05,
      "loss": 0.0039,
      "step": 75550
    },
    {
      "epoch": 4.029866666666667,
      "grad_norm": 0.028177041560411453,
      "learning_rate": 2.4813333333333335e-05,
      "loss": 0.003,
      "step": 75560
    },
    {
      "epoch": 4.0304,
      "grad_norm": 0.30993449687957764,
      "learning_rate": 2.481e-05,
      "loss": 0.0022,
      "step": 75570
    },
    {
      "epoch": 4.0309333333333335,
      "grad_norm": 0.3381006419658661,
      "learning_rate": 2.4806666666666667e-05,
      "loss": 0.0028,
      "step": 75580
    },
    {
      "epoch": 4.031466666666667,
      "grad_norm": 0.08453270047903061,
      "learning_rate": 2.4803333333333334e-05,
      "loss": 0.0025,
      "step": 75590
    },
    {
      "epoch": 4.032,
      "grad_norm": 0.05635182186961174,
      "learning_rate": 2.48e-05,
      "loss": 0.0022,
      "step": 75600
    },
    {
      "epoch": 4.032533333333333,
      "grad_norm": 0.2817564308643341,
      "learning_rate": 2.479666666666667e-05,
      "loss": 0.0024,
      "step": 75610
    },
    {
      "epoch": 4.033066666666667,
      "grad_norm": 0.05635049566626549,
      "learning_rate": 2.4793333333333335e-05,
      "loss": 0.0022,
      "step": 75620
    },
    {
      "epoch": 4.0336,
      "grad_norm": 0.479002445936203,
      "learning_rate": 2.479e-05,
      "loss": 0.0035,
      "step": 75630
    },
    {
      "epoch": 4.034133333333333,
      "grad_norm": 0.1408754140138626,
      "learning_rate": 2.4786666666666668e-05,
      "loss": 0.0029,
      "step": 75640
    },
    {
      "epoch": 4.034666666666666,
      "grad_norm": 6.843517974530755e-10,
      "learning_rate": 2.4783333333333334e-05,
      "loss": 0.002,
      "step": 75650
    },
    {
      "epoch": 4.0352,
      "grad_norm": 0.3099410831928253,
      "learning_rate": 2.478e-05,
      "loss": 0.0026,
      "step": 75660
    },
    {
      "epoch": 4.035733333333333,
      "grad_norm": 0.14087948203086853,
      "learning_rate": 2.4776666666666666e-05,
      "loss": 0.0029,
      "step": 75670
    },
    {
      "epoch": 4.036266666666666,
      "grad_norm": 0.3381248414516449,
      "learning_rate": 2.4773333333333336e-05,
      "loss": 0.0023,
      "step": 75680
    },
    {
      "epoch": 4.0368,
      "grad_norm": 0.3380949795246124,
      "learning_rate": 2.4770000000000002e-05,
      "loss": 0.0023,
      "step": 75690
    },
    {
      "epoch": 4.037333333333334,
      "grad_norm": 0.08452551066875458,
      "learning_rate": 2.4766666666666668e-05,
      "loss": 0.0028,
      "step": 75700
    },
    {
      "epoch": 4.037866666666667,
      "grad_norm": 0.16905224323272705,
      "learning_rate": 2.4763333333333334e-05,
      "loss": 0.0022,
      "step": 75710
    },
    {
      "epoch": 4.0384,
      "grad_norm": 2.7664968094143205e-09,
      "learning_rate": 2.476e-05,
      "loss": 0.004,
      "step": 75720
    },
    {
      "epoch": 4.0389333333333335,
      "grad_norm": 0.028176510706543922,
      "learning_rate": 2.4756666666666666e-05,
      "loss": 0.0027,
      "step": 75730
    },
    {
      "epoch": 4.039466666666667,
      "grad_norm": 0.11270551383495331,
      "learning_rate": 2.4753333333333332e-05,
      "loss": 0.0029,
      "step": 75740
    },
    {
      "epoch": 4.04,
      "grad_norm": 0.25356611609458923,
      "learning_rate": 2.4750000000000002e-05,
      "loss": 0.0024,
      "step": 75750
    },
    {
      "epoch": 4.040533333333333,
      "grad_norm": 0.05635296553373337,
      "learning_rate": 2.4746666666666668e-05,
      "loss": 0.0032,
      "step": 75760
    },
    {
      "epoch": 4.041066666666667,
      "grad_norm": 0.3944585621356964,
      "learning_rate": 2.4743333333333334e-05,
      "loss": 0.0024,
      "step": 75770
    },
    {
      "epoch": 4.0416,
      "grad_norm": 0.14087410271167755,
      "learning_rate": 2.4740000000000004e-05,
      "loss": 0.0029,
      "step": 75780
    },
    {
      "epoch": 4.042133333333333,
      "grad_norm": 0.1972247213125229,
      "learning_rate": 2.473666666666667e-05,
      "loss": 0.0026,
      "step": 75790
    },
    {
      "epoch": 4.042666666666666,
      "grad_norm": 0.1972179263830185,
      "learning_rate": 2.4733333333333333e-05,
      "loss": 0.0022,
      "step": 75800
    },
    {
      "epoch": 4.0432,
      "grad_norm": 0.02817620150744915,
      "learning_rate": 2.473e-05,
      "loss": 0.0029,
      "step": 75810
    },
    {
      "epoch": 4.043733333333333,
      "grad_norm": 0.08889113366603851,
      "learning_rate": 2.4726666666666668e-05,
      "loss": 0.0032,
      "step": 75820
    },
    {
      "epoch": 4.044266666666666,
      "grad_norm": 1.6744241548849459e-09,
      "learning_rate": 2.4723333333333334e-05,
      "loss": 0.0025,
      "step": 75830
    },
    {
      "epoch": 4.0448,
      "grad_norm": 0.47900277376174927,
      "learning_rate": 2.472e-05,
      "loss": 0.0029,
      "step": 75840
    },
    {
      "epoch": 4.045333333333334,
      "grad_norm": 0.16904518008232117,
      "learning_rate": 2.471666666666667e-05,
      "loss": 0.0028,
      "step": 75850
    },
    {
      "epoch": 4.045866666666667,
      "grad_norm": 0.11269619315862656,
      "learning_rate": 2.4713333333333336e-05,
      "loss": 0.0033,
      "step": 75860
    },
    {
      "epoch": 4.0464,
      "grad_norm": 0.02817472442984581,
      "learning_rate": 2.471e-05,
      "loss": 0.0026,
      "step": 75870
    },
    {
      "epoch": 4.0469333333333335,
      "grad_norm": 0.1126975491642952,
      "learning_rate": 2.470666666666667e-05,
      "loss": 0.0036,
      "step": 75880
    },
    {
      "epoch": 4.047466666666667,
      "grad_norm": 0.1408727765083313,
      "learning_rate": 2.4703333333333335e-05,
      "loss": 0.0026,
      "step": 75890
    },
    {
      "epoch": 4.048,
      "grad_norm": 0.11269690096378326,
      "learning_rate": 2.47e-05,
      "loss": 0.002,
      "step": 75900
    },
    {
      "epoch": 4.048533333333333,
      "grad_norm": 1.1640743924345998e-09,
      "learning_rate": 2.4696666666666667e-05,
      "loss": 0.0029,
      "step": 75910
    },
    {
      "epoch": 4.049066666666667,
      "grad_norm": 0.2535713016986847,
      "learning_rate": 2.4693333333333336e-05,
      "loss": 0.0027,
      "step": 75920
    },
    {
      "epoch": 4.0496,
      "grad_norm": 0.22540459036827087,
      "learning_rate": 2.4690000000000002e-05,
      "loss": 0.0026,
      "step": 75930
    },
    {
      "epoch": 4.050133333333333,
      "grad_norm": 0.2684469521045685,
      "learning_rate": 2.468666666666667e-05,
      "loss": 0.0021,
      "step": 75940
    },
    {
      "epoch": 4.050666666666666,
      "grad_norm": 0.19723030924797058,
      "learning_rate": 2.4683333333333335e-05,
      "loss": 0.0063,
      "step": 75950
    },
    {
      "epoch": 4.0512,
      "grad_norm": 3.813244386918768e-09,
      "learning_rate": 2.468e-05,
      "loss": 0.0036,
      "step": 75960
    },
    {
      "epoch": 4.051733333333333,
      "grad_norm": 0.07943341135978699,
      "learning_rate": 2.4676666666666667e-05,
      "loss": 0.0028,
      "step": 75970
    },
    {
      "epoch": 4.052266666666666,
      "grad_norm": 0.5071700215339661,
      "learning_rate": 2.4673333333333333e-05,
      "loss": 0.0041,
      "step": 75980
    },
    {
      "epoch": 4.0528,
      "grad_norm": 0.16903775930404663,
      "learning_rate": 2.4670000000000003e-05,
      "loss": 0.0029,
      "step": 75990
    },
    {
      "epoch": 4.053333333333334,
      "grad_norm": 0.05634899437427521,
      "learning_rate": 2.466666666666667e-05,
      "loss": 0.0023,
      "step": 76000
    },
    {
      "epoch": 4.053866666666667,
      "grad_norm": 0.056350257247686386,
      "learning_rate": 2.4663333333333335e-05,
      "loss": 0.0027,
      "step": 76010
    },
    {
      "epoch": 4.0544,
      "grad_norm": 0.22539609670639038,
      "learning_rate": 2.466e-05,
      "loss": 0.0026,
      "step": 76020
    },
    {
      "epoch": 4.0549333333333335,
      "grad_norm": 0.05634972080588341,
      "learning_rate": 2.4656666666666667e-05,
      "loss": 0.0028,
      "step": 76030
    },
    {
      "epoch": 4.055466666666667,
      "grad_norm": 0.05634554848074913,
      "learning_rate": 2.4653333333333333e-05,
      "loss": 0.0042,
      "step": 76040
    },
    {
      "epoch": 4.056,
      "grad_norm": 0.0845225602388382,
      "learning_rate": 2.465e-05,
      "loss": 0.0031,
      "step": 76050
    },
    {
      "epoch": 4.056533333333333,
      "grad_norm": 0.02817426063120365,
      "learning_rate": 2.464666666666667e-05,
      "loss": 0.0026,
      "step": 76060
    },
    {
      "epoch": 4.057066666666667,
      "grad_norm": 0.05634554848074913,
      "learning_rate": 2.4643333333333335e-05,
      "loss": 0.0025,
      "step": 76070
    },
    {
      "epoch": 4.0576,
      "grad_norm": 0.028172550722956657,
      "learning_rate": 2.464e-05,
      "loss": 0.0025,
      "step": 76080
    },
    {
      "epoch": 4.058133333333333,
      "grad_norm": 0.3944198489189148,
      "learning_rate": 2.4636666666666667e-05,
      "loss": 0.0032,
      "step": 76090
    },
    {
      "epoch": 4.058666666666666,
      "grad_norm": 0.11269214749336243,
      "learning_rate": 2.4633333333333334e-05,
      "loss": 0.0034,
      "step": 76100
    },
    {
      "epoch": 4.0592,
      "grad_norm": 0.1972218155860901,
      "learning_rate": 2.463e-05,
      "loss": 0.004,
      "step": 76110
    },
    {
      "epoch": 4.059733333333333,
      "grad_norm": 0.2253820151090622,
      "learning_rate": 2.4626666666666666e-05,
      "loss": 0.0035,
      "step": 76120
    },
    {
      "epoch": 4.060266666666666,
      "grad_norm": 0.08452170342206955,
      "learning_rate": 2.4623333333333335e-05,
      "loss": 0.0024,
      "step": 76130
    },
    {
      "epoch": 4.0608,
      "grad_norm": 0.1690409779548645,
      "learning_rate": 2.462e-05,
      "loss": 0.004,
      "step": 76140
    },
    {
      "epoch": 4.061333333333334,
      "grad_norm": 0.338059663772583,
      "learning_rate": 2.4616666666666668e-05,
      "loss": 0.0022,
      "step": 76150
    },
    {
      "epoch": 4.061866666666667,
      "grad_norm": 0.1690356284379959,
      "learning_rate": 2.4613333333333337e-05,
      "loss": 0.0024,
      "step": 76160
    },
    {
      "epoch": 4.0624,
      "grad_norm": 0.028172161430120468,
      "learning_rate": 2.4610000000000003e-05,
      "loss": 0.0018,
      "step": 76170
    },
    {
      "epoch": 4.0629333333333335,
      "grad_norm": 0.19720827043056488,
      "learning_rate": 2.4606666666666666e-05,
      "loss": 0.0023,
      "step": 76180
    },
    {
      "epoch": 4.063466666666667,
      "grad_norm": 0.028171885758638382,
      "learning_rate": 2.4603333333333332e-05,
      "loss": 0.0031,
      "step": 76190
    },
    {
      "epoch": 4.064,
      "grad_norm": 0.6198205351829529,
      "learning_rate": 2.46e-05,
      "loss": 0.0025,
      "step": 76200
    },
    {
      "epoch": 4.064533333333333,
      "grad_norm": 0.14085720479488373,
      "learning_rate": 2.4596666666666668e-05,
      "loss": 0.0023,
      "step": 76210
    },
    {
      "epoch": 4.065066666666667,
      "grad_norm": 0.1408672034740448,
      "learning_rate": 2.4593333333333334e-05,
      "loss": 0.002,
      "step": 76220
    },
    {
      "epoch": 4.0656,
      "grad_norm": 0.14085859060287476,
      "learning_rate": 2.4590000000000003e-05,
      "loss": 0.0042,
      "step": 76230
    },
    {
      "epoch": 4.066133333333333,
      "grad_norm": 0.36623936891555786,
      "learning_rate": 2.458666666666667e-05,
      "loss": 0.0028,
      "step": 76240
    },
    {
      "epoch": 4.066666666666666,
      "grad_norm": 0.1408635824918747,
      "learning_rate": 2.4583333333333332e-05,
      "loss": 0.0028,
      "step": 76250
    },
    {
      "epoch": 4.0672,
      "grad_norm": 0.19720344245433807,
      "learning_rate": 2.4580000000000002e-05,
      "loss": 0.0024,
      "step": 76260
    },
    {
      "epoch": 4.067733333333333,
      "grad_norm": 0.16903646290302277,
      "learning_rate": 2.4576666666666668e-05,
      "loss": 0.0025,
      "step": 76270
    },
    {
      "epoch": 4.068266666666666,
      "grad_norm": 2.2627040241474106e-09,
      "learning_rate": 2.4573333333333334e-05,
      "loss": 0.0024,
      "step": 76280
    },
    {
      "epoch": 4.0688,
      "grad_norm": 0.19720745086669922,
      "learning_rate": 2.457e-05,
      "loss": 0.0018,
      "step": 76290
    },
    {
      "epoch": 4.069333333333334,
      "grad_norm": 0.08451627194881439,
      "learning_rate": 2.456666666666667e-05,
      "loss": 0.0036,
      "step": 76300
    },
    {
      "epoch": 4.069866666666667,
      "grad_norm": 0.22537057101726532,
      "learning_rate": 2.4563333333333336e-05,
      "loss": 0.0027,
      "step": 76310
    },
    {
      "epoch": 4.0704,
      "grad_norm": 0.3099078834056854,
      "learning_rate": 2.4560000000000002e-05,
      "loss": 0.0024,
      "step": 76320
    },
    {
      "epoch": 4.0709333333333335,
      "grad_norm": 4.751031124072824e-09,
      "learning_rate": 2.4556666666666668e-05,
      "loss": 0.0032,
      "step": 76330
    },
    {
      "epoch": 4.071466666666667,
      "grad_norm": 0.3098871409893036,
      "learning_rate": 2.4553333333333334e-05,
      "loss": 0.0045,
      "step": 76340
    },
    {
      "epoch": 4.072,
      "grad_norm": 0.08452185243368149,
      "learning_rate": 2.455e-05,
      "loss": 0.0039,
      "step": 76350
    },
    {
      "epoch": 4.072533333333333,
      "grad_norm": 0.3380596339702606,
      "learning_rate": 2.4546666666666667e-05,
      "loss": 0.0026,
      "step": 76360
    },
    {
      "epoch": 4.073066666666667,
      "grad_norm": 0.22537536919116974,
      "learning_rate": 2.4543333333333336e-05,
      "loss": 0.0019,
      "step": 76370
    },
    {
      "epoch": 4.0736,
      "grad_norm": 0.3944337069988251,
      "learning_rate": 2.4540000000000002e-05,
      "loss": 0.0028,
      "step": 76380
    },
    {
      "epoch": 4.074133333333333,
      "grad_norm": 0.14085480570793152,
      "learning_rate": 2.453666666666667e-05,
      "loss": 0.0023,
      "step": 76390
    },
    {
      "epoch": 4.074666666666666,
      "grad_norm": 0.4225594401359558,
      "learning_rate": 2.4533333333333334e-05,
      "loss": 0.0034,
      "step": 76400
    },
    {
      "epoch": 4.0752,
      "grad_norm": 0.5352506637573242,
      "learning_rate": 2.453e-05,
      "loss": 0.0028,
      "step": 76410
    },
    {
      "epoch": 4.075733333333333,
      "grad_norm": 0.11269548535346985,
      "learning_rate": 2.4526666666666667e-05,
      "loss": 0.003,
      "step": 76420
    },
    {
      "epoch": 4.076266666666666,
      "grad_norm": 0.2817023992538452,
      "learning_rate": 2.4523333333333333e-05,
      "loss": 0.0031,
      "step": 76430
    },
    {
      "epoch": 4.0768,
      "grad_norm": 0.22537966072559357,
      "learning_rate": 2.4520000000000002e-05,
      "loss": 0.0036,
      "step": 76440
    },
    {
      "epoch": 4.077333333333334,
      "grad_norm": 2.033189058303833,
      "learning_rate": 2.451666666666667e-05,
      "loss": 0.0033,
      "step": 76450
    },
    {
      "epoch": 4.077866666666667,
      "grad_norm": 2.023346424102783,
      "learning_rate": 2.4513333333333335e-05,
      "loss": 0.0039,
      "step": 76460
    },
    {
      "epoch": 4.0784,
      "grad_norm": 0.056343983858823776,
      "learning_rate": 2.451e-05,
      "loss": 0.0023,
      "step": 76470
    },
    {
      "epoch": 4.0789333333333335,
      "grad_norm": 0.19719144701957703,
      "learning_rate": 2.4506666666666667e-05,
      "loss": 0.0037,
      "step": 76480
    },
    {
      "epoch": 4.079466666666667,
      "grad_norm": 0.02817199006676674,
      "learning_rate": 2.4503333333333333e-05,
      "loss": 0.0034,
      "step": 76490
    },
    {
      "epoch": 4.08,
      "grad_norm": 0.1971982717514038,
      "learning_rate": 2.45e-05,
      "loss": 0.003,
      "step": 76500
    },
    {
      "epoch": 4.080533333333333,
      "grad_norm": 0.11268101632595062,
      "learning_rate": 2.449666666666667e-05,
      "loss": 0.0022,
      "step": 76510
    },
    {
      "epoch": 4.081066666666667,
      "grad_norm": 0.16902042925357819,
      "learning_rate": 2.4493333333333335e-05,
      "loss": 0.0024,
      "step": 76520
    },
    {
      "epoch": 4.0816,
      "grad_norm": 0.2253604680299759,
      "learning_rate": 2.449e-05,
      "loss": 0.0033,
      "step": 76530
    },
    {
      "epoch": 4.082133333333333,
      "grad_norm": 0.30989378690719604,
      "learning_rate": 2.448666666666667e-05,
      "loss": 0.0036,
      "step": 76540
    },
    {
      "epoch": 4.082666666666666,
      "grad_norm": 0.28170526027679443,
      "learning_rate": 2.4483333333333333e-05,
      "loss": 0.0022,
      "step": 76550
    },
    {
      "epoch": 4.0832,
      "grad_norm": 0.05634235218167305,
      "learning_rate": 2.448e-05,
      "loss": 0.0024,
      "step": 76560
    },
    {
      "epoch": 4.083733333333333,
      "grad_norm": 0.1690174788236618,
      "learning_rate": 2.4476666666666666e-05,
      "loss": 0.0031,
      "step": 76570
    },
    {
      "epoch": 4.084266666666666,
      "grad_norm": 7.976490579153506e-09,
      "learning_rate": 2.4473333333333335e-05,
      "loss": 0.0047,
      "step": 76580
    },
    {
      "epoch": 4.0848,
      "grad_norm": 0.1971891224384308,
      "learning_rate": 2.447e-05,
      "loss": 0.0028,
      "step": 76590
    },
    {
      "epoch": 4.085333333333334,
      "grad_norm": 0.1408567875623703,
      "learning_rate": 2.4466666666666667e-05,
      "loss": 0.0034,
      "step": 76600
    },
    {
      "epoch": 4.085866666666667,
      "grad_norm": 0.08451486378908157,
      "learning_rate": 2.4463333333333337e-05,
      "loss": 0.0031,
      "step": 76610
    },
    {
      "epoch": 4.0864,
      "grad_norm": 0.2817048132419586,
      "learning_rate": 2.4460000000000003e-05,
      "loss": 0.0029,
      "step": 76620
    },
    {
      "epoch": 4.0869333333333335,
      "grad_norm": 0.08451592922210693,
      "learning_rate": 2.4456666666666666e-05,
      "loss": 0.003,
      "step": 76630
    },
    {
      "epoch": 4.087466666666667,
      "grad_norm": 0.30985885858535767,
      "learning_rate": 2.4453333333333335e-05,
      "loss": 0.0023,
      "step": 76640
    },
    {
      "epoch": 4.088,
      "grad_norm": 0.11268974095582962,
      "learning_rate": 2.445e-05,
      "loss": 0.0028,
      "step": 76650
    },
    {
      "epoch": 4.088533333333333,
      "grad_norm": 0.028169969096779823,
      "learning_rate": 2.4446666666666668e-05,
      "loss": 0.0026,
      "step": 76660
    },
    {
      "epoch": 4.089066666666667,
      "grad_norm": 0.16902297735214233,
      "learning_rate": 2.4443333333333334e-05,
      "loss": 0.0032,
      "step": 76670
    },
    {
      "epoch": 4.0896,
      "grad_norm": 0.31594929099082947,
      "learning_rate": 2.4440000000000003e-05,
      "loss": 0.0028,
      "step": 76680
    },
    {
      "epoch": 4.090133333333333,
      "grad_norm": 0.3380337655544281,
      "learning_rate": 2.443666666666667e-05,
      "loss": 0.0034,
      "step": 76690
    },
    {
      "epoch": 4.0906666666666665,
      "grad_norm": 0.14084765315055847,
      "learning_rate": 2.4433333333333335e-05,
      "loss": 0.0037,
      "step": 76700
    },
    {
      "epoch": 4.0912,
      "grad_norm": 0.0845077708363533,
      "learning_rate": 2.443e-05,
      "loss": 0.0025,
      "step": 76710
    },
    {
      "epoch": 4.091733333333333,
      "grad_norm": 0.16902782022953033,
      "learning_rate": 2.4426666666666668e-05,
      "loss": 0.0024,
      "step": 76720
    },
    {
      "epoch": 4.092266666666666,
      "grad_norm": 0.11268224567174911,
      "learning_rate": 2.4423333333333334e-05,
      "loss": 0.0022,
      "step": 76730
    },
    {
      "epoch": 4.0928,
      "grad_norm": 4.300730882533799e-09,
      "learning_rate": 2.442e-05,
      "loss": 0.0037,
      "step": 76740
    },
    {
      "epoch": 4.093333333333334,
      "grad_norm": 0.08450944721698761,
      "learning_rate": 2.441666666666667e-05,
      "loss": 0.0024,
      "step": 76750
    },
    {
      "epoch": 4.093866666666667,
      "grad_norm": 0.1408458799123764,
      "learning_rate": 2.4413333333333336e-05,
      "loss": 0.0029,
      "step": 76760
    },
    {
      "epoch": 4.0944,
      "grad_norm": 0.2816949784755707,
      "learning_rate": 2.4410000000000002e-05,
      "loss": 0.0018,
      "step": 76770
    },
    {
      "epoch": 4.0949333333333335,
      "grad_norm": 0.1690174639225006,
      "learning_rate": 2.4406666666666668e-05,
      "loss": 0.0025,
      "step": 76780
    },
    {
      "epoch": 4.095466666666667,
      "grad_norm": 0.22535404562950134,
      "learning_rate": 2.4403333333333334e-05,
      "loss": 0.0033,
      "step": 76790
    },
    {
      "epoch": 4.096,
      "grad_norm": 0.056337036192417145,
      "learning_rate": 2.44e-05,
      "loss": 0.003,
      "step": 76800
    },
    {
      "epoch": 4.096533333333333,
      "grad_norm": 0.1971893012523651,
      "learning_rate": 2.4396666666666666e-05,
      "loss": 0.0028,
      "step": 76810
    },
    {
      "epoch": 4.097066666666667,
      "grad_norm": 0.22535496950149536,
      "learning_rate": 2.4393333333333336e-05,
      "loss": 0.0027,
      "step": 76820
    },
    {
      "epoch": 4.0976,
      "grad_norm": 0.05634016543626785,
      "learning_rate": 2.4390000000000002e-05,
      "loss": 0.0028,
      "step": 76830
    },
    {
      "epoch": 4.098133333333333,
      "grad_norm": 0.2816862165927887,
      "learning_rate": 2.4386666666666668e-05,
      "loss": 0.003,
      "step": 76840
    },
    {
      "epoch": 4.0986666666666665,
      "grad_norm": 0.028170514851808548,
      "learning_rate": 2.4383333333333334e-05,
      "loss": 0.0027,
      "step": 76850
    },
    {
      "epoch": 4.0992,
      "grad_norm": 0.4788791537284851,
      "learning_rate": 2.438e-05,
      "loss": 0.0025,
      "step": 76860
    },
    {
      "epoch": 4.099733333333333,
      "grad_norm": 0.20540592074394226,
      "learning_rate": 2.4376666666666666e-05,
      "loss": 0.0042,
      "step": 76870
    },
    {
      "epoch": 4.100266666666666,
      "grad_norm": 0.11267732828855515,
      "learning_rate": 2.4373333333333333e-05,
      "loss": 0.003,
      "step": 76880
    },
    {
      "epoch": 4.1008,
      "grad_norm": 0.02816910669207573,
      "learning_rate": 2.4370000000000002e-05,
      "loss": 0.0027,
      "step": 76890
    },
    {
      "epoch": 4.101333333333334,
      "grad_norm": 0.14084558188915253,
      "learning_rate": 2.4366666666666668e-05,
      "loss": 0.0023,
      "step": 76900
    },
    {
      "epoch": 4.101866666666667,
      "grad_norm": 0.028170211240649223,
      "learning_rate": 2.4363333333333334e-05,
      "loss": 0.0028,
      "step": 76910
    },
    {
      "epoch": 4.1024,
      "grad_norm": 0.08450745046138763,
      "learning_rate": 2.4360000000000004e-05,
      "loss": 0.0039,
      "step": 76920
    },
    {
      "epoch": 4.1029333333333335,
      "grad_norm": 0.140845388174057,
      "learning_rate": 2.4356666666666667e-05,
      "loss": 0.0037,
      "step": 76930
    },
    {
      "epoch": 4.103466666666667,
      "grad_norm": 0.08450614660978317,
      "learning_rate": 2.4353333333333333e-05,
      "loss": 0.0044,
      "step": 76940
    },
    {
      "epoch": 4.104,
      "grad_norm": 0.056342534720897675,
      "learning_rate": 2.435e-05,
      "loss": 0.0042,
      "step": 76950
    },
    {
      "epoch": 4.104533333333333,
      "grad_norm": 0.1971742808818817,
      "learning_rate": 2.434666666666667e-05,
      "loss": 0.003,
      "step": 76960
    },
    {
      "epoch": 4.105066666666667,
      "grad_norm": 1.4186805064042574e-09,
      "learning_rate": 2.4343333333333335e-05,
      "loss": 0.0019,
      "step": 76970
    },
    {
      "epoch": 4.1056,
      "grad_norm": 0.0563385933637619,
      "learning_rate": 2.434e-05,
      "loss": 0.0035,
      "step": 76980
    },
    {
      "epoch": 4.106133333333333,
      "grad_norm": 0.14084143936634064,
      "learning_rate": 2.433666666666667e-05,
      "loss": 0.0032,
      "step": 76990
    },
    {
      "epoch": 4.1066666666666665,
      "grad_norm": 0.47890156507492065,
      "learning_rate": 2.4333333333333336e-05,
      "loss": 0.0029,
      "step": 77000
    },
    {
      "epoch": 4.1072,
      "grad_norm": 0.14084534347057343,
      "learning_rate": 2.433e-05,
      "loss": 0.0027,
      "step": 77010
    },
    {
      "epoch": 4.107733333333333,
      "grad_norm": 0.11267619580030441,
      "learning_rate": 2.432666666666667e-05,
      "loss": 0.0025,
      "step": 77020
    },
    {
      "epoch": 4.108266666666666,
      "grad_norm": 0.30987414717674255,
      "learning_rate": 2.4323333333333335e-05,
      "loss": 0.0035,
      "step": 77030
    },
    {
      "epoch": 4.1088,
      "grad_norm": 0.22533588111400604,
      "learning_rate": 2.432e-05,
      "loss": 0.0024,
      "step": 77040
    },
    {
      "epoch": 4.109333333333334,
      "grad_norm": 0.28170305490493774,
      "learning_rate": 2.4316666666666667e-05,
      "loss": 0.0041,
      "step": 77050
    },
    {
      "epoch": 4.109866666666667,
      "grad_norm": 0.22534975409507751,
      "learning_rate": 2.4313333333333337e-05,
      "loss": 0.0029,
      "step": 77060
    },
    {
      "epoch": 4.1104,
      "grad_norm": 0.05633501708507538,
      "learning_rate": 2.4310000000000003e-05,
      "loss": 0.0031,
      "step": 77070
    },
    {
      "epoch": 4.1109333333333336,
      "grad_norm": 0.08450532704591751,
      "learning_rate": 2.4306666666666665e-05,
      "loss": 0.0021,
      "step": 77080
    },
    {
      "epoch": 4.111466666666667,
      "grad_norm": 0.42254406213760376,
      "learning_rate": 2.4303333333333335e-05,
      "loss": 0.003,
      "step": 77090
    },
    {
      "epoch": 4.112,
      "grad_norm": 0.39433369040489197,
      "learning_rate": 2.43e-05,
      "loss": 0.0021,
      "step": 77100
    },
    {
      "epoch": 4.112533333333333,
      "grad_norm": 0.19718162715435028,
      "learning_rate": 2.4296666666666667e-05,
      "loss": 0.003,
      "step": 77110
    },
    {
      "epoch": 4.113066666666667,
      "grad_norm": 0.19717615842819214,
      "learning_rate": 2.4293333333333333e-05,
      "loss": 0.0041,
      "step": 77120
    },
    {
      "epoch": 4.1136,
      "grad_norm": 0.3380023241043091,
      "learning_rate": 2.4290000000000003e-05,
      "loss": 0.0039,
      "step": 77130
    },
    {
      "epoch": 4.114133333333333,
      "grad_norm": 0.36620137095451355,
      "learning_rate": 2.428666666666667e-05,
      "loss": 0.0027,
      "step": 77140
    },
    {
      "epoch": 4.1146666666666665,
      "grad_norm": 0.6760048866271973,
      "learning_rate": 2.4283333333333335e-05,
      "loss": 0.0029,
      "step": 77150
    },
    {
      "epoch": 4.1152,
      "grad_norm": 0.02816767618060112,
      "learning_rate": 2.428e-05,
      "loss": 0.0025,
      "step": 77160
    },
    {
      "epoch": 4.115733333333333,
      "grad_norm": 0.19716526567935944,
      "learning_rate": 2.4276666666666667e-05,
      "loss": 0.0037,
      "step": 77170
    },
    {
      "epoch": 4.116266666666666,
      "grad_norm": 0.08450667560100555,
      "learning_rate": 2.4273333333333334e-05,
      "loss": 0.0031,
      "step": 77180
    },
    {
      "epoch": 4.1168,
      "grad_norm": 0.14083388447761536,
      "learning_rate": 2.427e-05,
      "loss": 0.0027,
      "step": 77190
    },
    {
      "epoch": 4.117333333333334,
      "grad_norm": 0.2253335863351822,
      "learning_rate": 2.426666666666667e-05,
      "loss": 0.0025,
      "step": 77200
    },
    {
      "epoch": 4.117866666666667,
      "grad_norm": 0.3380138576030731,
      "learning_rate": 2.4263333333333335e-05,
      "loss": 0.002,
      "step": 77210
    },
    {
      "epoch": 4.1184,
      "grad_norm": 0.11267366260290146,
      "learning_rate": 2.426e-05,
      "loss": 0.0021,
      "step": 77220
    },
    {
      "epoch": 4.118933333333334,
      "grad_norm": 3.184403407630043e-09,
      "learning_rate": 2.4256666666666668e-05,
      "loss": 0.0026,
      "step": 77230
    },
    {
      "epoch": 4.119466666666667,
      "grad_norm": 0.05633419007062912,
      "learning_rate": 2.4253333333333334e-05,
      "loss": 0.0035,
      "step": 77240
    },
    {
      "epoch": 4.12,
      "grad_norm": 0.28167593479156494,
      "learning_rate": 2.425e-05,
      "loss": 0.005,
      "step": 77250
    },
    {
      "epoch": 4.120533333333333,
      "grad_norm": 0.11267077177762985,
      "learning_rate": 2.4246666666666666e-05,
      "loss": 0.0031,
      "step": 77260
    },
    {
      "epoch": 4.121066666666667,
      "grad_norm": 0.08449952304363251,
      "learning_rate": 2.4243333333333336e-05,
      "loss": 0.0019,
      "step": 77270
    },
    {
      "epoch": 4.1216,
      "grad_norm": 0.028166839852929115,
      "learning_rate": 2.4240000000000002e-05,
      "loss": 0.0025,
      "step": 77280
    },
    {
      "epoch": 4.122133333333333,
      "grad_norm": 0.02816825918853283,
      "learning_rate": 2.4236666666666668e-05,
      "loss": 0.0025,
      "step": 77290
    },
    {
      "epoch": 4.1226666666666665,
      "grad_norm": 0.3380075693130493,
      "learning_rate": 2.4233333333333337e-05,
      "loss": 0.0024,
      "step": 77300
    },
    {
      "epoch": 4.1232,
      "grad_norm": 0.056334175169467926,
      "learning_rate": 2.423e-05,
      "loss": 0.0035,
      "step": 77310
    },
    {
      "epoch": 4.123733333333333,
      "grad_norm": 0.16901198029518127,
      "learning_rate": 2.4226666666666666e-05,
      "loss": 0.0031,
      "step": 77320
    },
    {
      "epoch": 4.124266666666666,
      "grad_norm": 0.05633312463760376,
      "learning_rate": 2.4223333333333332e-05,
      "loss": 0.0022,
      "step": 77330
    },
    {
      "epoch": 4.1248,
      "grad_norm": 0.05633237585425377,
      "learning_rate": 2.4220000000000002e-05,
      "loss": 0.0044,
      "step": 77340
    },
    {
      "epoch": 4.125333333333334,
      "grad_norm": 0.08450078964233398,
      "learning_rate": 2.4216666666666668e-05,
      "loss": 0.0036,
      "step": 77350
    },
    {
      "epoch": 4.125866666666667,
      "grad_norm": 0.21066822111606598,
      "learning_rate": 2.4213333333333334e-05,
      "loss": 0.0048,
      "step": 77360
    },
    {
      "epoch": 4.1264,
      "grad_norm": 1.8235573051583742e-09,
      "learning_rate": 2.4210000000000004e-05,
      "loss": 0.0029,
      "step": 77370
    },
    {
      "epoch": 4.126933333333334,
      "grad_norm": 0.19716623425483704,
      "learning_rate": 2.420666666666667e-05,
      "loss": 0.0028,
      "step": 77380
    },
    {
      "epoch": 4.127466666666667,
      "grad_norm": 0.05633239820599556,
      "learning_rate": 2.4203333333333333e-05,
      "loss": 0.0029,
      "step": 77390
    },
    {
      "epoch": 4.128,
      "grad_norm": 0.2816779613494873,
      "learning_rate": 2.4200000000000002e-05,
      "loss": 0.0024,
      "step": 77400
    },
    {
      "epoch": 4.128533333333333,
      "grad_norm": 0.2253248244524002,
      "learning_rate": 2.4196666666666668e-05,
      "loss": 0.0039,
      "step": 77410
    },
    {
      "epoch": 4.129066666666667,
      "grad_norm": 0.19717144966125488,
      "learning_rate": 2.4193333333333334e-05,
      "loss": 0.0035,
      "step": 77420
    },
    {
      "epoch": 4.1296,
      "grad_norm": 0.16899950802326202,
      "learning_rate": 2.419e-05,
      "loss": 0.003,
      "step": 77430
    },
    {
      "epoch": 4.130133333333333,
      "grad_norm": 2.4648556529172083e-09,
      "learning_rate": 2.418666666666667e-05,
      "loss": 0.0039,
      "step": 77440
    },
    {
      "epoch": 4.1306666666666665,
      "grad_norm": 0.11266984790563583,
      "learning_rate": 2.4183333333333336e-05,
      "loss": 0.0022,
      "step": 77450
    },
    {
      "epoch": 4.1312,
      "grad_norm": 0.028167085722088814,
      "learning_rate": 2.418e-05,
      "loss": 0.0031,
      "step": 77460
    },
    {
      "epoch": 4.131733333333333,
      "grad_norm": 0.16899430751800537,
      "learning_rate": 2.417666666666667e-05,
      "loss": 0.0029,
      "step": 77470
    },
    {
      "epoch": 4.132266666666666,
      "grad_norm": 5.853247220244384e-09,
      "learning_rate": 2.4173333333333335e-05,
      "loss": 0.0033,
      "step": 77480
    },
    {
      "epoch": 4.1328,
      "grad_norm": 0.11266332119703293,
      "learning_rate": 2.417e-05,
      "loss": 0.0023,
      "step": 77490
    },
    {
      "epoch": 4.133333333333334,
      "grad_norm": 0.22533081471920013,
      "learning_rate": 2.4166666666666667e-05,
      "loss": 0.0035,
      "step": 77500
    },
    {
      "epoch": 4.133866666666667,
      "grad_norm": 0.16899457573890686,
      "learning_rate": 2.4163333333333336e-05,
      "loss": 0.0036,
      "step": 77510
    },
    {
      "epoch": 4.1344,
      "grad_norm": 0.19716015458106995,
      "learning_rate": 2.4160000000000002e-05,
      "loss": 0.002,
      "step": 77520
    },
    {
      "epoch": 4.134933333333334,
      "grad_norm": 0.14083677530288696,
      "learning_rate": 2.415666666666667e-05,
      "loss": 0.0031,
      "step": 77530
    },
    {
      "epoch": 4.135466666666667,
      "grad_norm": 0.08449802547693253,
      "learning_rate": 2.4153333333333335e-05,
      "loss": 0.004,
      "step": 77540
    },
    {
      "epoch": 4.136,
      "grad_norm": 0.02816593274474144,
      "learning_rate": 2.415e-05,
      "loss": 0.0037,
      "step": 77550
    },
    {
      "epoch": 4.136533333333333,
      "grad_norm": 0.4506942629814148,
      "learning_rate": 2.4146666666666667e-05,
      "loss": 0.0025,
      "step": 77560
    },
    {
      "epoch": 4.137066666666667,
      "grad_norm": 0.1689932644367218,
      "learning_rate": 2.4143333333333333e-05,
      "loss": 0.0027,
      "step": 77570
    },
    {
      "epoch": 4.1376,
      "grad_norm": 0.05632983148097992,
      "learning_rate": 2.4140000000000003e-05,
      "loss": 0.003,
      "step": 77580
    },
    {
      "epoch": 4.138133333333333,
      "grad_norm": 0.2816609740257263,
      "learning_rate": 2.413666666666667e-05,
      "loss": 0.0043,
      "step": 77590
    },
    {
      "epoch": 4.1386666666666665,
      "grad_norm": 0.028165945783257484,
      "learning_rate": 2.4133333333333335e-05,
      "loss": 0.0036,
      "step": 77600
    },
    {
      "epoch": 4.1392,
      "grad_norm": 0.05633057281374931,
      "learning_rate": 2.413e-05,
      "loss": 0.0028,
      "step": 77610
    },
    {
      "epoch": 4.139733333333333,
      "grad_norm": 0.19716767966747284,
      "learning_rate": 2.4126666666666667e-05,
      "loss": 0.0042,
      "step": 77620
    },
    {
      "epoch": 4.140266666666666,
      "grad_norm": 0.11266149580478668,
      "learning_rate": 2.4123333333333333e-05,
      "loss": 0.0025,
      "step": 77630
    },
    {
      "epoch": 4.1408,
      "grad_norm": 0.22532376646995544,
      "learning_rate": 2.412e-05,
      "loss": 0.0034,
      "step": 77640
    },
    {
      "epoch": 4.141333333333334,
      "grad_norm": 0.33797380328178406,
      "learning_rate": 2.411666666666667e-05,
      "loss": 0.0018,
      "step": 77650
    },
    {
      "epoch": 4.141866666666667,
      "grad_norm": 0.169002965092659,
      "learning_rate": 2.4113333333333335e-05,
      "loss": 0.0032,
      "step": 77660
    },
    {
      "epoch": 4.1424,
      "grad_norm": 0.1126585602760315,
      "learning_rate": 2.411e-05,
      "loss": 0.0028,
      "step": 77670
    },
    {
      "epoch": 4.142933333333334,
      "grad_norm": 0.028164846822619438,
      "learning_rate": 2.4106666666666667e-05,
      "loss": 0.0022,
      "step": 77680
    },
    {
      "epoch": 4.143466666666667,
      "grad_norm": 0.19715750217437744,
      "learning_rate": 2.4103333333333334e-05,
      "loss": 0.002,
      "step": 77690
    },
    {
      "epoch": 4.144,
      "grad_norm": 0.28167763352394104,
      "learning_rate": 2.41e-05,
      "loss": 0.0049,
      "step": 77700
    },
    {
      "epoch": 4.144533333333333,
      "grad_norm": 0.25347402691841125,
      "learning_rate": 2.4096666666666666e-05,
      "loss": 0.0024,
      "step": 77710
    },
    {
      "epoch": 4.145066666666667,
      "grad_norm": 0.16899605095386505,
      "learning_rate": 2.4093333333333335e-05,
      "loss": 0.0033,
      "step": 77720
    },
    {
      "epoch": 4.1456,
      "grad_norm": 0.0844922587275505,
      "learning_rate": 2.409e-05,
      "loss": 0.0027,
      "step": 77730
    },
    {
      "epoch": 4.146133333333333,
      "grad_norm": 0.059125807136297226,
      "learning_rate": 2.4086666666666668e-05,
      "loss": 0.0022,
      "step": 77740
    },
    {
      "epoch": 4.1466666666666665,
      "grad_norm": 5.157747118289535e-09,
      "learning_rate": 2.4083333333333337e-05,
      "loss": 0.0024,
      "step": 77750
    },
    {
      "epoch": 4.1472,
      "grad_norm": 0.16898301243782043,
      "learning_rate": 2.408e-05,
      "loss": 0.0022,
      "step": 77760
    },
    {
      "epoch": 4.147733333333333,
      "grad_norm": 0.08449424803256989,
      "learning_rate": 2.4076666666666666e-05,
      "loss": 0.0033,
      "step": 77770
    },
    {
      "epoch": 4.148266666666666,
      "grad_norm": 1.2712427377700806,
      "learning_rate": 2.4073333333333335e-05,
      "loss": 0.0023,
      "step": 77780
    },
    {
      "epoch": 4.1488,
      "grad_norm": 0.028164159506559372,
      "learning_rate": 2.407e-05,
      "loss": 0.0037,
      "step": 77790
    },
    {
      "epoch": 4.149333333333334,
      "grad_norm": 0.45062270760536194,
      "learning_rate": 2.4066666666666668e-05,
      "loss": 0.0038,
      "step": 77800
    },
    {
      "epoch": 4.149866666666667,
      "grad_norm": 0.05632869526743889,
      "learning_rate": 2.4063333333333334e-05,
      "loss": 0.003,
      "step": 77810
    },
    {
      "epoch": 4.1504,
      "grad_norm": 0.5914284586906433,
      "learning_rate": 2.4060000000000003e-05,
      "loss": 0.0026,
      "step": 77820
    },
    {
      "epoch": 4.150933333333334,
      "grad_norm": 0.11265874654054642,
      "learning_rate": 2.405666666666667e-05,
      "loss": 0.0017,
      "step": 77830
    },
    {
      "epoch": 4.151466666666667,
      "grad_norm": 0.2816382646560669,
      "learning_rate": 2.4053333333333332e-05,
      "loss": 0.0023,
      "step": 77840
    },
    {
      "epoch": 4.152,
      "grad_norm": 0.11265328526496887,
      "learning_rate": 2.4050000000000002e-05,
      "loss": 0.0031,
      "step": 77850
    },
    {
      "epoch": 4.152533333333333,
      "grad_norm": 0.028165224939584732,
      "learning_rate": 2.4046666666666668e-05,
      "loss": 0.0024,
      "step": 77860
    },
    {
      "epoch": 4.153066666666667,
      "grad_norm": 0.05633100867271423,
      "learning_rate": 2.4043333333333334e-05,
      "loss": 0.0041,
      "step": 77870
    },
    {
      "epoch": 4.1536,
      "grad_norm": 0.1408156305551529,
      "learning_rate": 2.404e-05,
      "loss": 0.003,
      "step": 77880
    },
    {
      "epoch": 4.154133333333333,
      "grad_norm": 0.1971496045589447,
      "learning_rate": 2.403666666666667e-05,
      "loss": 0.0035,
      "step": 77890
    },
    {
      "epoch": 4.1546666666666665,
      "grad_norm": 0.22531694173812866,
      "learning_rate": 2.4033333333333336e-05,
      "loss": 0.0028,
      "step": 77900
    },
    {
      "epoch": 4.1552,
      "grad_norm": 0.16899102926254272,
      "learning_rate": 2.4030000000000002e-05,
      "loss": 0.0036,
      "step": 77910
    },
    {
      "epoch": 4.155733333333333,
      "grad_norm": 0.08449731767177582,
      "learning_rate": 2.4026666666666668e-05,
      "loss": 0.0028,
      "step": 77920
    },
    {
      "epoch": 4.156266666666666,
      "grad_norm": 0.30982130765914917,
      "learning_rate": 2.4023333333333334e-05,
      "loss": 0.003,
      "step": 77930
    },
    {
      "epoch": 4.1568,
      "grad_norm": 0.19713708758354187,
      "learning_rate": 2.402e-05,
      "loss": 0.0021,
      "step": 77940
    },
    {
      "epoch": 4.157333333333334,
      "grad_norm": 0.16898922622203827,
      "learning_rate": 2.4016666666666667e-05,
      "loss": 0.0031,
      "step": 77950
    },
    {
      "epoch": 4.157866666666667,
      "grad_norm": 0.16897661983966827,
      "learning_rate": 2.4013333333333336e-05,
      "loss": 0.0036,
      "step": 77960
    },
    {
      "epoch": 4.1584,
      "grad_norm": 0.1408260017633438,
      "learning_rate": 2.4010000000000002e-05,
      "loss": 0.0037,
      "step": 77970
    },
    {
      "epoch": 4.158933333333334,
      "grad_norm": 0.05632711574435234,
      "learning_rate": 2.400666666666667e-05,
      "loss": 0.0033,
      "step": 77980
    },
    {
      "epoch": 4.159466666666667,
      "grad_norm": 0.08448898792266846,
      "learning_rate": 2.4003333333333334e-05,
      "loss": 0.0032,
      "step": 77990
    },
    {
      "epoch": 4.16,
      "grad_norm": 0.28163519501686096,
      "learning_rate": 2.4e-05,
      "loss": 0.0035,
      "step": 78000
    },
    {
      "epoch": 4.160533333333333,
      "grad_norm": 0.22530516982078552,
      "learning_rate": 2.3996666666666667e-05,
      "loss": 0.0034,
      "step": 78010
    },
    {
      "epoch": 4.161066666666667,
      "grad_norm": 0.05632714927196503,
      "learning_rate": 2.3993333333333333e-05,
      "loss": 0.0025,
      "step": 78020
    },
    {
      "epoch": 4.1616,
      "grad_norm": 0.05632869526743889,
      "learning_rate": 2.3990000000000002e-05,
      "loss": 0.0021,
      "step": 78030
    },
    {
      "epoch": 4.162133333333333,
      "grad_norm": 0.33794838190078735,
      "learning_rate": 2.398666666666667e-05,
      "loss": 0.0028,
      "step": 78040
    },
    {
      "epoch": 4.1626666666666665,
      "grad_norm": 0.14081254601478577,
      "learning_rate": 2.3983333333333335e-05,
      "loss": 0.0035,
      "step": 78050
    },
    {
      "epoch": 4.1632,
      "grad_norm": 0.47875726222991943,
      "learning_rate": 2.398e-05,
      "loss": 0.0038,
      "step": 78060
    },
    {
      "epoch": 4.163733333333333,
      "grad_norm": 0.028165820986032486,
      "learning_rate": 2.3976666666666667e-05,
      "loss": 0.0039,
      "step": 78070
    },
    {
      "epoch": 4.164266666666666,
      "grad_norm": 0.19713300466537476,
      "learning_rate": 2.3973333333333333e-05,
      "loss": 0.0043,
      "step": 78080
    },
    {
      "epoch": 4.1648,
      "grad_norm": 0.028164809569716454,
      "learning_rate": 2.397e-05,
      "loss": 0.0036,
      "step": 78090
    },
    {
      "epoch": 4.165333333333333,
      "grad_norm": 0.08448737859725952,
      "learning_rate": 2.396666666666667e-05,
      "loss": 0.0023,
      "step": 78100
    },
    {
      "epoch": 4.165866666666667,
      "grad_norm": 0.3097926080226898,
      "learning_rate": 2.3963333333333335e-05,
      "loss": 0.004,
      "step": 78110
    },
    {
      "epoch": 4.1664,
      "grad_norm": 3.4259926007251806e-09,
      "learning_rate": 2.396e-05,
      "loss": 0.0034,
      "step": 78120
    },
    {
      "epoch": 4.166933333333334,
      "grad_norm": 0.0844852551817894,
      "learning_rate": 2.395666666666667e-05,
      "loss": 0.0029,
      "step": 78130
    },
    {
      "epoch": 4.167466666666667,
      "grad_norm": 2.1117074755494514e-09,
      "learning_rate": 2.3953333333333333e-05,
      "loss": 0.0028,
      "step": 78140
    },
    {
      "epoch": 4.168,
      "grad_norm": 0.12626755237579346,
      "learning_rate": 2.395e-05,
      "loss": 0.0028,
      "step": 78150
    },
    {
      "epoch": 4.168533333333333,
      "grad_norm": 0.4224225580692291,
      "learning_rate": 2.394666666666667e-05,
      "loss": 0.003,
      "step": 78160
    },
    {
      "epoch": 4.169066666666667,
      "grad_norm": 3.300370693206787,
      "learning_rate": 2.3943333333333335e-05,
      "loss": 0.0027,
      "step": 78170
    },
    {
      "epoch": 4.1696,
      "grad_norm": 0.5069290399551392,
      "learning_rate": 2.394e-05,
      "loss": 0.0041,
      "step": 78180
    },
    {
      "epoch": 4.170133333333333,
      "grad_norm": 0.08448521047830582,
      "learning_rate": 2.3936666666666667e-05,
      "loss": 0.0034,
      "step": 78190
    },
    {
      "epoch": 4.1706666666666665,
      "grad_norm": 0.19713349640369415,
      "learning_rate": 2.3933333333333337e-05,
      "loss": 0.0038,
      "step": 78200
    },
    {
      "epoch": 4.1712,
      "grad_norm": 3.156646055657575e-09,
      "learning_rate": 2.3930000000000003e-05,
      "loss": 0.0045,
      "step": 78210
    },
    {
      "epoch": 4.171733333333333,
      "grad_norm": 0.1971365511417389,
      "learning_rate": 2.3926666666666666e-05,
      "loss": 0.0033,
      "step": 78220
    },
    {
      "epoch": 4.172266666666666,
      "grad_norm": 0.17206574976444244,
      "learning_rate": 2.3923333333333335e-05,
      "loss": 0.0034,
      "step": 78230
    },
    {
      "epoch": 4.1728,
      "grad_norm": 0.16897228360176086,
      "learning_rate": 2.392e-05,
      "loss": 0.0026,
      "step": 78240
    },
    {
      "epoch": 4.173333333333334,
      "grad_norm": 0.2255955934524536,
      "learning_rate": 2.3916666666666668e-05,
      "loss": 0.0034,
      "step": 78250
    },
    {
      "epoch": 4.173866666666667,
      "grad_norm": 0.14080710709095,
      "learning_rate": 2.3913333333333334e-05,
      "loss": 0.0035,
      "step": 78260
    },
    {
      "epoch": 4.1744,
      "grad_norm": 0.056324552744627,
      "learning_rate": 2.3910000000000003e-05,
      "loss": 0.0045,
      "step": 78270
    },
    {
      "epoch": 4.174933333333334,
      "grad_norm": 0.2534586191177368,
      "learning_rate": 2.390666666666667e-05,
      "loss": 0.0039,
      "step": 78280
    },
    {
      "epoch": 4.175466666666667,
      "grad_norm": 0.05632593855261803,
      "learning_rate": 2.3903333333333332e-05,
      "loss": 0.0018,
      "step": 78290
    },
    {
      "epoch": 4.176,
      "grad_norm": 0.25408750772476196,
      "learning_rate": 2.39e-05,
      "loss": 0.0047,
      "step": 78300
    },
    {
      "epoch": 4.176533333333333,
      "grad_norm": 0.45059871673583984,
      "learning_rate": 2.3896666666666668e-05,
      "loss": 0.0024,
      "step": 78310
    },
    {
      "epoch": 4.177066666666667,
      "grad_norm": 0.16896851360797882,
      "learning_rate": 2.3893333333333334e-05,
      "loss": 0.003,
      "step": 78320
    },
    {
      "epoch": 4.1776,
      "grad_norm": 0.16896876692771912,
      "learning_rate": 2.389e-05,
      "loss": 0.0032,
      "step": 78330
    },
    {
      "epoch": 4.178133333333333,
      "grad_norm": 0.05632608011364937,
      "learning_rate": 2.388666666666667e-05,
      "loss": 0.0037,
      "step": 78340
    },
    {
      "epoch": 4.1786666666666665,
      "grad_norm": 0.11264676600694656,
      "learning_rate": 2.3883333333333336e-05,
      "loss": 0.0033,
      "step": 78350
    },
    {
      "epoch": 4.1792,
      "grad_norm": 0.11264974623918533,
      "learning_rate": 2.3880000000000002e-05,
      "loss": 0.002,
      "step": 78360
    },
    {
      "epoch": 4.179733333333333,
      "grad_norm": 0.1971244066953659,
      "learning_rate": 2.3876666666666668e-05,
      "loss": 0.0032,
      "step": 78370
    },
    {
      "epoch": 4.180266666666666,
      "grad_norm": 0.3379594087600708,
      "learning_rate": 2.3873333333333334e-05,
      "loss": 0.0038,
      "step": 78380
    },
    {
      "epoch": 4.1808,
      "grad_norm": 0.36611345410346985,
      "learning_rate": 2.387e-05,
      "loss": 0.0025,
      "step": 78390
    },
    {
      "epoch": 4.181333333333333,
      "grad_norm": 0.04505991190671921,
      "learning_rate": 2.3866666666666666e-05,
      "loss": 0.003,
      "step": 78400
    },
    {
      "epoch": 4.181866666666667,
      "grad_norm": 0.3379732072353363,
      "learning_rate": 2.3863333333333336e-05,
      "loss": 0.003,
      "step": 78410
    },
    {
      "epoch": 4.1824,
      "grad_norm": 1.201950192451477,
      "learning_rate": 2.3860000000000002e-05,
      "loss": 0.0019,
      "step": 78420
    },
    {
      "epoch": 4.182933333333334,
      "grad_norm": 0.028162430971860886,
      "learning_rate": 2.3856666666666668e-05,
      "loss": 0.0031,
      "step": 78430
    },
    {
      "epoch": 4.183466666666667,
      "grad_norm": 0.08449103683233261,
      "learning_rate": 2.3853333333333334e-05,
      "loss": 0.0034,
      "step": 78440
    },
    {
      "epoch": 4.184,
      "grad_norm": 0.08448607474565506,
      "learning_rate": 2.385e-05,
      "loss": 0.0026,
      "step": 78450
    },
    {
      "epoch": 4.184533333333333,
      "grad_norm": 0.3862396478652954,
      "learning_rate": 2.3846666666666666e-05,
      "loss": 0.0034,
      "step": 78460
    },
    {
      "epoch": 4.185066666666667,
      "grad_norm": 0.08448615670204163,
      "learning_rate": 2.3843333333333333e-05,
      "loss": 0.0034,
      "step": 78470
    },
    {
      "epoch": 4.1856,
      "grad_norm": 0.5069317817687988,
      "learning_rate": 2.3840000000000002e-05,
      "loss": 0.0033,
      "step": 78480
    },
    {
      "epoch": 4.186133333333333,
      "grad_norm": 0.02817799150943756,
      "learning_rate": 2.3836666666666668e-05,
      "loss": 0.0016,
      "step": 78490
    },
    {
      "epoch": 4.1866666666666665,
      "grad_norm": 0.05632201209664345,
      "learning_rate": 2.3833333333333334e-05,
      "loss": 0.0042,
      "step": 78500
    },
    {
      "epoch": 4.1872,
      "grad_norm": 0.028161106631159782,
      "learning_rate": 2.3830000000000004e-05,
      "loss": 0.0028,
      "step": 78510
    },
    {
      "epoch": 4.187733333333333,
      "grad_norm": 6.089028835296631,
      "learning_rate": 2.3826666666666667e-05,
      "loss": 0.0019,
      "step": 78520
    },
    {
      "epoch": 4.188266666666666,
      "grad_norm": 0.11264139413833618,
      "learning_rate": 2.3823333333333333e-05,
      "loss": 0.0029,
      "step": 78530
    },
    {
      "epoch": 4.1888,
      "grad_norm": 0.08448342233896255,
      "learning_rate": 2.3820000000000002e-05,
      "loss": 0.003,
      "step": 78540
    },
    {
      "epoch": 4.189333333333333,
      "grad_norm": 0.2816130816936493,
      "learning_rate": 2.381666666666667e-05,
      "loss": 0.006,
      "step": 78550
    },
    {
      "epoch": 4.189866666666667,
      "grad_norm": 0.3379284143447876,
      "learning_rate": 2.3813333333333335e-05,
      "loss": 0.0033,
      "step": 78560
    },
    {
      "epoch": 4.1904,
      "grad_norm": 0.05632201209664345,
      "learning_rate": 2.381e-05,
      "loss": 0.0034,
      "step": 78570
    },
    {
      "epoch": 4.190933333333334,
      "grad_norm": 0.028161246329545975,
      "learning_rate": 2.380666666666667e-05,
      "loss": 0.0042,
      "step": 78580
    },
    {
      "epoch": 4.191466666666667,
      "grad_norm": 0.30978861451148987,
      "learning_rate": 2.3803333333333336e-05,
      "loss": 0.0034,
      "step": 78590
    },
    {
      "epoch": 4.192,
      "grad_norm": 0.3942369818687439,
      "learning_rate": 2.38e-05,
      "loss": 0.0032,
      "step": 78600
    },
    {
      "epoch": 4.1925333333333334,
      "grad_norm": 0.028161561116576195,
      "learning_rate": 2.379666666666667e-05,
      "loss": 0.0032,
      "step": 78610
    },
    {
      "epoch": 4.193066666666667,
      "grad_norm": 0.16896140575408936,
      "learning_rate": 2.3793333333333335e-05,
      "loss": 0.0039,
      "step": 78620
    },
    {
      "epoch": 4.1936,
      "grad_norm": 7.55334816915365e-09,
      "learning_rate": 2.379e-05,
      "loss": 0.0033,
      "step": 78630
    },
    {
      "epoch": 4.194133333333333,
      "grad_norm": 0.0451941192150116,
      "learning_rate": 2.3786666666666667e-05,
      "loss": 0.0021,
      "step": 78640
    },
    {
      "epoch": 4.1946666666666665,
      "grad_norm": 0.05632155016064644,
      "learning_rate": 2.3783333333333337e-05,
      "loss": 0.0028,
      "step": 78650
    },
    {
      "epoch": 4.1952,
      "grad_norm": 0.16897137463092804,
      "learning_rate": 2.3780000000000003e-05,
      "loss": 0.0025,
      "step": 78660
    },
    {
      "epoch": 4.195733333333333,
      "grad_norm": 0.30760958790779114,
      "learning_rate": 2.3776666666666665e-05,
      "loss": 0.0036,
      "step": 78670
    },
    {
      "epoch": 4.196266666666666,
      "grad_norm": 0.19712485373020172,
      "learning_rate": 2.3773333333333335e-05,
      "loss": 0.0026,
      "step": 78680
    },
    {
      "epoch": 4.1968,
      "grad_norm": 0.10914652794599533,
      "learning_rate": 2.377e-05,
      "loss": 0.0034,
      "step": 78690
    },
    {
      "epoch": 4.197333333333333,
      "grad_norm": 0.19712777435779572,
      "learning_rate": 2.3766666666666667e-05,
      "loss": 0.0023,
      "step": 78700
    },
    {
      "epoch": 4.197866666666667,
      "grad_norm": 0.16896450519561768,
      "learning_rate": 2.3763333333333333e-05,
      "loss": 0.002,
      "step": 78710
    },
    {
      "epoch": 4.1984,
      "grad_norm": 0.3942418396472931,
      "learning_rate": 2.3760000000000003e-05,
      "loss": 0.0024,
      "step": 78720
    },
    {
      "epoch": 4.198933333333334,
      "grad_norm": 0.422420471906662,
      "learning_rate": 2.375666666666667e-05,
      "loss": 0.0027,
      "step": 78730
    },
    {
      "epoch": 4.199466666666667,
      "grad_norm": 0.0844852402806282,
      "learning_rate": 2.3753333333333335e-05,
      "loss": 0.0029,
      "step": 78740
    },
    {
      "epoch": 4.2,
      "grad_norm": 0.08448200672864914,
      "learning_rate": 2.375e-05,
      "loss": 0.0035,
      "step": 78750
    },
    {
      "epoch": 4.2005333333333335,
      "grad_norm": 0.08448601514101028,
      "learning_rate": 2.3746666666666667e-05,
      "loss": 0.0021,
      "step": 78760
    },
    {
      "epoch": 4.201066666666667,
      "grad_norm": 0.3660791218280792,
      "learning_rate": 2.3743333333333334e-05,
      "loss": 0.0021,
      "step": 78770
    },
    {
      "epoch": 4.2016,
      "grad_norm": 0.19712358713150024,
      "learning_rate": 2.374e-05,
      "loss": 0.0032,
      "step": 78780
    },
    {
      "epoch": 4.202133333333333,
      "grad_norm": 0.02815978229045868,
      "learning_rate": 2.373666666666667e-05,
      "loss": 0.0034,
      "step": 78790
    },
    {
      "epoch": 4.2026666666666666,
      "grad_norm": 0.2816304862499237,
      "learning_rate": 2.3733333333333335e-05,
      "loss": 0.0022,
      "step": 78800
    },
    {
      "epoch": 4.2032,
      "grad_norm": 0.08448103815317154,
      "learning_rate": 2.373e-05,
      "loss": 0.0022,
      "step": 78810
    },
    {
      "epoch": 4.203733333333333,
      "grad_norm": 0.28161144256591797,
      "learning_rate": 2.3726666666666668e-05,
      "loss": 0.003,
      "step": 78820
    },
    {
      "epoch": 4.204266666666666,
      "grad_norm": 0.02816174365580082,
      "learning_rate": 2.3723333333333334e-05,
      "loss": 0.0024,
      "step": 78830
    },
    {
      "epoch": 4.2048,
      "grad_norm": 0.22528326511383057,
      "learning_rate": 2.372e-05,
      "loss": 0.0025,
      "step": 78840
    },
    {
      "epoch": 4.205333333333333,
      "grad_norm": 0.1408083289861679,
      "learning_rate": 2.3716666666666666e-05,
      "loss": 0.0017,
      "step": 78850
    },
    {
      "epoch": 4.205866666666667,
      "grad_norm": 0.22528092563152313,
      "learning_rate": 2.3713333333333336e-05,
      "loss": 0.0037,
      "step": 78860
    },
    {
      "epoch": 4.2064,
      "grad_norm": 0.056320760399103165,
      "learning_rate": 2.371e-05,
      "loss": 0.0025,
      "step": 78870
    },
    {
      "epoch": 4.206933333333334,
      "grad_norm": 3.0649480731170797e-09,
      "learning_rate": 2.3706666666666668e-05,
      "loss": 0.0021,
      "step": 78880
    },
    {
      "epoch": 4.207466666666667,
      "grad_norm": 0.14080692827701569,
      "learning_rate": 2.3703333333333337e-05,
      "loss": 0.0025,
      "step": 78890
    },
    {
      "epoch": 4.208,
      "grad_norm": 0.08448095619678497,
      "learning_rate": 2.37e-05,
      "loss": 0.0022,
      "step": 78900
    },
    {
      "epoch": 4.2085333333333335,
      "grad_norm": 0.16896307468414307,
      "learning_rate": 2.3696666666666666e-05,
      "loss": 0.0024,
      "step": 78910
    },
    {
      "epoch": 4.209066666666667,
      "grad_norm": 0.028159335255622864,
      "learning_rate": 2.3693333333333332e-05,
      "loss": 0.0033,
      "step": 78920
    },
    {
      "epoch": 4.2096,
      "grad_norm": 0.2816014885902405,
      "learning_rate": 2.3690000000000002e-05,
      "loss": 0.0036,
      "step": 78930
    },
    {
      "epoch": 4.210133333333333,
      "grad_norm": 0.112644724547863,
      "learning_rate": 2.3686666666666668e-05,
      "loss": 0.0036,
      "step": 78940
    },
    {
      "epoch": 4.210666666666667,
      "grad_norm": 0.19711461663246155,
      "learning_rate": 2.3683333333333334e-05,
      "loss": 0.0031,
      "step": 78950
    },
    {
      "epoch": 4.2112,
      "grad_norm": 1.7808119423534663e-09,
      "learning_rate": 2.3680000000000004e-05,
      "loss": 0.0019,
      "step": 78960
    },
    {
      "epoch": 4.211733333333333,
      "grad_norm": 1.3864355087280273,
      "learning_rate": 2.3676666666666666e-05,
      "loss": 0.0029,
      "step": 78970
    },
    {
      "epoch": 4.212266666666666,
      "grad_norm": 0.4505476653575897,
      "learning_rate": 2.3673333333333333e-05,
      "loss": 0.0033,
      "step": 78980
    },
    {
      "epoch": 4.2128,
      "grad_norm": 0.2816219925880432,
      "learning_rate": 2.3670000000000002e-05,
      "loss": 0.0024,
      "step": 78990
    },
    {
      "epoch": 4.213333333333333,
      "grad_norm": 0.5631712675094604,
      "learning_rate": 2.3666666666666668e-05,
      "loss": 0.0044,
      "step": 79000
    },
    {
      "epoch": 4.213866666666667,
      "grad_norm": 0.14079593122005463,
      "learning_rate": 2.3663333333333334e-05,
      "loss": 0.0045,
      "step": 79010
    },
    {
      "epoch": 4.2144,
      "grad_norm": 0.28159740567207336,
      "learning_rate": 2.366e-05,
      "loss": 0.003,
      "step": 79020
    },
    {
      "epoch": 4.214933333333334,
      "grad_norm": 0.06752325594425201,
      "learning_rate": 2.365666666666667e-05,
      "loss": 0.003,
      "step": 79030
    },
    {
      "epoch": 4.215466666666667,
      "grad_norm": 0.14079420268535614,
      "learning_rate": 2.3653333333333336e-05,
      "loss": 0.0034,
      "step": 79040
    },
    {
      "epoch": 4.216,
      "grad_norm": 0.19712834060192108,
      "learning_rate": 2.365e-05,
      "loss": 0.0029,
      "step": 79050
    },
    {
      "epoch": 4.2165333333333335,
      "grad_norm": 0.281602144241333,
      "learning_rate": 2.364666666666667e-05,
      "loss": 0.003,
      "step": 79060
    },
    {
      "epoch": 4.217066666666667,
      "grad_norm": 0.3942181468009949,
      "learning_rate": 2.3643333333333335e-05,
      "loss": 0.0028,
      "step": 79070
    },
    {
      "epoch": 4.2176,
      "grad_norm": 0.20743632316589355,
      "learning_rate": 2.364e-05,
      "loss": 0.0024,
      "step": 79080
    },
    {
      "epoch": 4.218133333333333,
      "grad_norm": 7.6632629131268e-09,
      "learning_rate": 2.3636666666666667e-05,
      "loss": 0.0042,
      "step": 79090
    },
    {
      "epoch": 4.218666666666667,
      "grad_norm": 0.028159871697425842,
      "learning_rate": 2.3633333333333336e-05,
      "loss": 0.0037,
      "step": 79100
    },
    {
      "epoch": 4.2192,
      "grad_norm": 0.36605164408683777,
      "learning_rate": 2.3630000000000002e-05,
      "loss": 0.0037,
      "step": 79110
    },
    {
      "epoch": 4.219733333333333,
      "grad_norm": 0.1971120685338974,
      "learning_rate": 2.362666666666667e-05,
      "loss": 0.0032,
      "step": 79120
    },
    {
      "epoch": 4.220266666666666,
      "grad_norm": 0.2252718210220337,
      "learning_rate": 2.3623333333333335e-05,
      "loss": 0.002,
      "step": 79130
    },
    {
      "epoch": 4.2208,
      "grad_norm": 0.16895535588264465,
      "learning_rate": 2.362e-05,
      "loss": 0.003,
      "step": 79140
    },
    {
      "epoch": 4.221333333333333,
      "grad_norm": 0.14079181849956512,
      "learning_rate": 2.3616666666666667e-05,
      "loss": 0.0016,
      "step": 79150
    },
    {
      "epoch": 4.221866666666667,
      "grad_norm": 0.16895022988319397,
      "learning_rate": 2.3613333333333333e-05,
      "loss": 0.0024,
      "step": 79160
    },
    {
      "epoch": 4.2224,
      "grad_norm": 0.2252768874168396,
      "learning_rate": 2.3610000000000003e-05,
      "loss": 0.0043,
      "step": 79170
    },
    {
      "epoch": 4.222933333333334,
      "grad_norm": 0.2534329891204834,
      "learning_rate": 2.360666666666667e-05,
      "loss": 0.0023,
      "step": 79180
    },
    {
      "epoch": 4.223466666666667,
      "grad_norm": 0.028158104047179222,
      "learning_rate": 2.3603333333333335e-05,
      "loss": 0.0019,
      "step": 79190
    },
    {
      "epoch": 4.224,
      "grad_norm": 0.22526566684246063,
      "learning_rate": 2.36e-05,
      "loss": 0.0019,
      "step": 79200
    },
    {
      "epoch": 4.2245333333333335,
      "grad_norm": 3.106733315050292e-09,
      "learning_rate": 2.3596666666666667e-05,
      "loss": 0.002,
      "step": 79210
    },
    {
      "epoch": 4.225066666666667,
      "grad_norm": 0.11373067647218704,
      "learning_rate": 2.3593333333333333e-05,
      "loss": 0.0033,
      "step": 79220
    },
    {
      "epoch": 4.2256,
      "grad_norm": 0.16896195709705353,
      "learning_rate": 2.359e-05,
      "loss": 0.0024,
      "step": 79230
    },
    {
      "epoch": 4.226133333333333,
      "grad_norm": 0.02815876714885235,
      "learning_rate": 2.358666666666667e-05,
      "loss": 0.0029,
      "step": 79240
    },
    {
      "epoch": 4.226666666666667,
      "grad_norm": 0.08447565138339996,
      "learning_rate": 2.3583333333333335e-05,
      "loss": 0.0038,
      "step": 79250
    },
    {
      "epoch": 4.2272,
      "grad_norm": 0.25341856479644775,
      "learning_rate": 2.358e-05,
      "loss": 0.0038,
      "step": 79260
    },
    {
      "epoch": 4.227733333333333,
      "grad_norm": 0.16895346343517303,
      "learning_rate": 2.357666666666667e-05,
      "loss": 0.003,
      "step": 79270
    },
    {
      "epoch": 4.228266666666666,
      "grad_norm": 0.5631570816040039,
      "learning_rate": 2.3573333333333334e-05,
      "loss": 0.0027,
      "step": 79280
    },
    {
      "epoch": 4.2288,
      "grad_norm": 0.08447805047035217,
      "learning_rate": 2.357e-05,
      "loss": 0.0028,
      "step": 79290
    },
    {
      "epoch": 4.229333333333333,
      "grad_norm": 0.11262999475002289,
      "learning_rate": 2.3566666666666666e-05,
      "loss": 0.0024,
      "step": 79300
    },
    {
      "epoch": 4.229866666666666,
      "grad_norm": 0.22527548670768738,
      "learning_rate": 2.3563333333333335e-05,
      "loss": 0.0021,
      "step": 79310
    },
    {
      "epoch": 4.2304,
      "grad_norm": 0.056316908448934555,
      "learning_rate": 2.356e-05,
      "loss": 0.0035,
      "step": 79320
    },
    {
      "epoch": 4.230933333333334,
      "grad_norm": 0.05631688982248306,
      "learning_rate": 2.3556666666666668e-05,
      "loss": 0.0018,
      "step": 79330
    },
    {
      "epoch": 4.231466666666667,
      "grad_norm": 0.11263701319694519,
      "learning_rate": 2.3553333333333337e-05,
      "loss": 0.0032,
      "step": 79340
    },
    {
      "epoch": 4.232,
      "grad_norm": 0.6194472908973694,
      "learning_rate": 2.355e-05,
      "loss": 0.003,
      "step": 79350
    },
    {
      "epoch": 4.2325333333333335,
      "grad_norm": 0.028158441185951233,
      "learning_rate": 2.3546666666666666e-05,
      "loss": 0.0028,
      "step": 79360
    },
    {
      "epoch": 4.233066666666667,
      "grad_norm": 0.4256679117679596,
      "learning_rate": 2.3543333333333335e-05,
      "loss": 0.0031,
      "step": 79370
    },
    {
      "epoch": 4.2336,
      "grad_norm": 9.223379282552457e-10,
      "learning_rate": 2.354e-05,
      "loss": 0.0027,
      "step": 79380
    },
    {
      "epoch": 4.234133333333333,
      "grad_norm": 0.1689552217721939,
      "learning_rate": 2.3536666666666668e-05,
      "loss": 0.0034,
      "step": 79390
    },
    {
      "epoch": 4.234666666666667,
      "grad_norm": 0.0281569454818964,
      "learning_rate": 2.3533333333333334e-05,
      "loss": 0.0025,
      "step": 79400
    },
    {
      "epoch": 4.2352,
      "grad_norm": 0.11262789368629456,
      "learning_rate": 2.3530000000000003e-05,
      "loss": 0.0013,
      "step": 79410
    },
    {
      "epoch": 4.235733333333333,
      "grad_norm": 0.08447510004043579,
      "learning_rate": 2.352666666666667e-05,
      "loss": 0.0044,
      "step": 79420
    },
    {
      "epoch": 4.236266666666666,
      "grad_norm": 0.02815781719982624,
      "learning_rate": 2.3523333333333332e-05,
      "loss": 0.003,
      "step": 79430
    },
    {
      "epoch": 4.2368,
      "grad_norm": 1.495622992515564,
      "learning_rate": 2.3520000000000002e-05,
      "loss": 0.003,
      "step": 79440
    },
    {
      "epoch": 4.237333333333333,
      "grad_norm": 0.2815732955932617,
      "learning_rate": 2.3516666666666668e-05,
      "loss": 0.005,
      "step": 79450
    },
    {
      "epoch": 4.237866666666667,
      "grad_norm": 2.976323631997957e-09,
      "learning_rate": 2.3513333333333334e-05,
      "loss": 0.0035,
      "step": 79460
    },
    {
      "epoch": 4.2384,
      "grad_norm": 0.14078988134860992,
      "learning_rate": 2.351e-05,
      "loss": 0.0039,
      "step": 79470
    },
    {
      "epoch": 4.238933333333334,
      "grad_norm": 0.19710184633731842,
      "learning_rate": 2.350666666666667e-05,
      "loss": 0.0031,
      "step": 79480
    },
    {
      "epoch": 4.239466666666667,
      "grad_norm": 0.08447112143039703,
      "learning_rate": 2.3503333333333336e-05,
      "loss": 0.0027,
      "step": 79490
    },
    {
      "epoch": 4.24,
      "grad_norm": 0.49432316422462463,
      "learning_rate": 2.35e-05,
      "loss": 0.003,
      "step": 79500
    },
    {
      "epoch": 4.2405333333333335,
      "grad_norm": 0.14079344272613525,
      "learning_rate": 2.3496666666666668e-05,
      "loss": 0.0029,
      "step": 79510
    },
    {
      "epoch": 4.241066666666667,
      "grad_norm": 0.33786311745643616,
      "learning_rate": 2.3493333333333334e-05,
      "loss": 0.003,
      "step": 79520
    },
    {
      "epoch": 4.2416,
      "grad_norm": 0.3097318112850189,
      "learning_rate": 2.349e-05,
      "loss": 0.0023,
      "step": 79530
    },
    {
      "epoch": 4.242133333333333,
      "grad_norm": 0.19709369540214539,
      "learning_rate": 2.3486666666666667e-05,
      "loss": 0.0029,
      "step": 79540
    },
    {
      "epoch": 4.242666666666667,
      "grad_norm": 0.1126270443201065,
      "learning_rate": 2.3483333333333336e-05,
      "loss": 0.0032,
      "step": 79550
    },
    {
      "epoch": 4.2432,
      "grad_norm": 0.25342074036598206,
      "learning_rate": 2.3480000000000002e-05,
      "loss": 0.0019,
      "step": 79560
    },
    {
      "epoch": 4.243733333333333,
      "grad_norm": 0.11263149976730347,
      "learning_rate": 2.347666666666667e-05,
      "loss": 0.003,
      "step": 79570
    },
    {
      "epoch": 4.244266666666666,
      "grad_norm": 0.2533973157405853,
      "learning_rate": 2.3473333333333334e-05,
      "loss": 0.0035,
      "step": 79580
    },
    {
      "epoch": 4.2448,
      "grad_norm": 0.1407817155122757,
      "learning_rate": 2.347e-05,
      "loss": 0.002,
      "step": 79590
    },
    {
      "epoch": 4.245333333333333,
      "grad_norm": 0.0844738632440567,
      "learning_rate": 2.3466666666666667e-05,
      "loss": 0.0022,
      "step": 79600
    },
    {
      "epoch": 4.245866666666666,
      "grad_norm": 1.7808629274368286,
      "learning_rate": 2.3463333333333333e-05,
      "loss": 0.0034,
      "step": 79610
    },
    {
      "epoch": 4.2464,
      "grad_norm": 0.056311704218387604,
      "learning_rate": 2.3460000000000002e-05,
      "loss": 0.0039,
      "step": 79620
    },
    {
      "epoch": 4.246933333333334,
      "grad_norm": 0.2534152865409851,
      "learning_rate": 2.345666666666667e-05,
      "loss": 0.0031,
      "step": 79630
    },
    {
      "epoch": 4.247466666666667,
      "grad_norm": 0.14078739285469055,
      "learning_rate": 2.3453333333333335e-05,
      "loss": 0.0045,
      "step": 79640
    },
    {
      "epoch": 4.248,
      "grad_norm": 0.2252628058195114,
      "learning_rate": 2.345e-05,
      "loss": 0.0021,
      "step": 79650
    },
    {
      "epoch": 4.2485333333333335,
      "grad_norm": 0.02815585769712925,
      "learning_rate": 2.3446666666666667e-05,
      "loss": 0.0028,
      "step": 79660
    },
    {
      "epoch": 4.249066666666667,
      "grad_norm": 0.36604082584381104,
      "learning_rate": 2.3443333333333333e-05,
      "loss": 0.0029,
      "step": 79670
    },
    {
      "epoch": 4.2496,
      "grad_norm": 0.19708643853664398,
      "learning_rate": 2.344e-05,
      "loss": 0.0034,
      "step": 79680
    },
    {
      "epoch": 4.250133333333333,
      "grad_norm": 0.3378824293613434,
      "learning_rate": 2.343666666666667e-05,
      "loss": 0.0027,
      "step": 79690
    },
    {
      "epoch": 4.250666666666667,
      "grad_norm": 0.36600354313850403,
      "learning_rate": 2.3433333333333335e-05,
      "loss": 0.0023,
      "step": 79700
    },
    {
      "epoch": 4.2512,
      "grad_norm": 0.14078517258167267,
      "learning_rate": 2.343e-05,
      "loss": 0.0017,
      "step": 79710
    },
    {
      "epoch": 4.251733333333333,
      "grad_norm": 0.39419159293174744,
      "learning_rate": 2.342666666666667e-05,
      "loss": 0.0031,
      "step": 79720
    },
    {
      "epoch": 4.252266666666666,
      "grad_norm": 0.028154661878943443,
      "learning_rate": 2.3423333333333333e-05,
      "loss": 0.0023,
      "step": 79730
    },
    {
      "epoch": 4.2528,
      "grad_norm": 0.11262974143028259,
      "learning_rate": 2.342e-05,
      "loss": 0.0027,
      "step": 79740
    },
    {
      "epoch": 4.253333333333333,
      "grad_norm": 0.30969569087028503,
      "learning_rate": 2.341666666666667e-05,
      "loss": 0.0023,
      "step": 79750
    },
    {
      "epoch": 4.253866666666667,
      "grad_norm": 0.16893912851810455,
      "learning_rate": 2.3413333333333335e-05,
      "loss": 0.0029,
      "step": 79760
    },
    {
      "epoch": 4.2544,
      "grad_norm": 0.2533821165561676,
      "learning_rate": 2.341e-05,
      "loss": 0.0034,
      "step": 79770
    },
    {
      "epoch": 4.254933333333334,
      "grad_norm": 0.08447323739528656,
      "learning_rate": 2.3406666666666667e-05,
      "loss": 0.0028,
      "step": 79780
    },
    {
      "epoch": 4.255466666666667,
      "grad_norm": 0.3378417193889618,
      "learning_rate": 2.3403333333333337e-05,
      "loss": 0.0033,
      "step": 79790
    },
    {
      "epoch": 4.256,
      "grad_norm": 0.14077872037887573,
      "learning_rate": 2.3400000000000003e-05,
      "loss": 0.0023,
      "step": 79800
    },
    {
      "epoch": 4.2565333333333335,
      "grad_norm": 0.19708915054798126,
      "learning_rate": 2.3396666666666666e-05,
      "loss": 0.0025,
      "step": 79810
    },
    {
      "epoch": 4.257066666666667,
      "grad_norm": 0.22523558139801025,
      "learning_rate": 2.3393333333333335e-05,
      "loss": 0.0031,
      "step": 79820
    },
    {
      "epoch": 4.2576,
      "grad_norm": 0.08446729183197021,
      "learning_rate": 2.339e-05,
      "loss": 0.0035,
      "step": 79830
    },
    {
      "epoch": 4.258133333333333,
      "grad_norm": 0.16893061995506287,
      "learning_rate": 2.3386666666666668e-05,
      "loss": 0.0031,
      "step": 79840
    },
    {
      "epoch": 4.258666666666667,
      "grad_norm": 0.19708774983882904,
      "learning_rate": 2.3383333333333334e-05,
      "loss": 0.0024,
      "step": 79850
    },
    {
      "epoch": 4.2592,
      "grad_norm": 0.028153900057077408,
      "learning_rate": 2.3380000000000003e-05,
      "loss": 0.0034,
      "step": 79860
    },
    {
      "epoch": 4.259733333333333,
      "grad_norm": 0.11261753737926483,
      "learning_rate": 2.337666666666667e-05,
      "loss": 0.0029,
      "step": 79870
    },
    {
      "epoch": 4.260266666666666,
      "grad_norm": 0.0563075952231884,
      "learning_rate": 2.3373333333333332e-05,
      "loss": 0.0023,
      "step": 79880
    },
    {
      "epoch": 4.2608,
      "grad_norm": 0.05631069839000702,
      "learning_rate": 2.337e-05,
      "loss": 0.0019,
      "step": 79890
    },
    {
      "epoch": 4.261333333333333,
      "grad_norm": 0.05631019175052643,
      "learning_rate": 2.3366666666666668e-05,
      "loss": 0.0016,
      "step": 79900
    },
    {
      "epoch": 4.261866666666666,
      "grad_norm": 0.33784371614456177,
      "learning_rate": 2.3363333333333334e-05,
      "loss": 0.0028,
      "step": 79910
    },
    {
      "epoch": 4.2624,
      "grad_norm": 0.11262399703264236,
      "learning_rate": 2.336e-05,
      "loss": 0.0028,
      "step": 79920
    },
    {
      "epoch": 4.262933333333334,
      "grad_norm": 0.11261653900146484,
      "learning_rate": 2.335666666666667e-05,
      "loss": 0.0022,
      "step": 79930
    },
    {
      "epoch": 4.263466666666667,
      "grad_norm": 0.4504574239253998,
      "learning_rate": 2.3353333333333336e-05,
      "loss": 0.0024,
      "step": 79940
    },
    {
      "epoch": 4.264,
      "grad_norm": 0.14077158272266388,
      "learning_rate": 2.3350000000000002e-05,
      "loss": 0.0018,
      "step": 79950
    },
    {
      "epoch": 4.2645333333333335,
      "grad_norm": 0.028153520077466965,
      "learning_rate": 2.3346666666666668e-05,
      "loss": 0.0023,
      "step": 79960
    },
    {
      "epoch": 4.265066666666667,
      "grad_norm": 0.16892485320568085,
      "learning_rate": 2.3343333333333334e-05,
      "loss": 0.0029,
      "step": 79970
    },
    {
      "epoch": 4.2656,
      "grad_norm": 0.1126134991645813,
      "learning_rate": 2.334e-05,
      "loss": 0.0018,
      "step": 79980
    },
    {
      "epoch": 4.266133333333333,
      "grad_norm": 1.5319180488586426,
      "learning_rate": 2.3336666666666666e-05,
      "loss": 0.0028,
      "step": 79990
    },
    {
      "epoch": 4.266666666666667,
      "grad_norm": 0.11261434108018875,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 0.0027,
      "step": 80000
    },
    {
      "epoch": 4.2672,
      "grad_norm": 0.19708509743213654,
      "learning_rate": 2.3330000000000002e-05,
      "loss": 0.0038,
      "step": 80010
    },
    {
      "epoch": 4.267733333333333,
      "grad_norm": 0.5446634888648987,
      "learning_rate": 2.3326666666666668e-05,
      "loss": 0.0045,
      "step": 80020
    },
    {
      "epoch": 4.268266666666666,
      "grad_norm": 0.19708506762981415,
      "learning_rate": 2.3323333333333334e-05,
      "loss": 0.0027,
      "step": 80030
    },
    {
      "epoch": 4.2688,
      "grad_norm": 4.872787950915836e-09,
      "learning_rate": 2.332e-05,
      "loss": 0.0036,
      "step": 80040
    },
    {
      "epoch": 4.269333333333333,
      "grad_norm": 0.056311383843421936,
      "learning_rate": 2.3316666666666666e-05,
      "loss": 0.0034,
      "step": 80050
    },
    {
      "epoch": 4.269866666666666,
      "grad_norm": 0.05630844086408615,
      "learning_rate": 2.3313333333333333e-05,
      "loss": 0.0036,
      "step": 80060
    },
    {
      "epoch": 4.2704,
      "grad_norm": 0.08446112275123596,
      "learning_rate": 2.3310000000000002e-05,
      "loss": 0.0037,
      "step": 80070
    },
    {
      "epoch": 4.270933333333334,
      "grad_norm": 0.16893763840198517,
      "learning_rate": 2.3306666666666668e-05,
      "loss": 0.0036,
      "step": 80080
    },
    {
      "epoch": 4.271466666666667,
      "grad_norm": 0.2815224230289459,
      "learning_rate": 2.3303333333333334e-05,
      "loss": 0.0024,
      "step": 80090
    },
    {
      "epoch": 4.272,
      "grad_norm": 0.056309375911951065,
      "learning_rate": 2.3300000000000004e-05,
      "loss": 0.0028,
      "step": 80100
    },
    {
      "epoch": 4.2725333333333335,
      "grad_norm": 0.2533927857875824,
      "learning_rate": 2.3296666666666667e-05,
      "loss": 0.0023,
      "step": 80110
    },
    {
      "epoch": 4.273066666666667,
      "grad_norm": 2.8647739735987443e-09,
      "learning_rate": 2.3293333333333333e-05,
      "loss": 0.0033,
      "step": 80120
    },
    {
      "epoch": 4.2736,
      "grad_norm": 0.36599200963974,
      "learning_rate": 2.3290000000000002e-05,
      "loss": 0.0028,
      "step": 80130
    },
    {
      "epoch": 4.274133333333333,
      "grad_norm": 0.11260954290628433,
      "learning_rate": 2.328666666666667e-05,
      "loss": 0.0035,
      "step": 80140
    },
    {
      "epoch": 4.274666666666667,
      "grad_norm": 0.19707198441028595,
      "learning_rate": 2.3283333333333335e-05,
      "loss": 0.0032,
      "step": 80150
    },
    {
      "epoch": 4.2752,
      "grad_norm": 0.14076581597328186,
      "learning_rate": 2.328e-05,
      "loss": 0.0023,
      "step": 80160
    },
    {
      "epoch": 4.275733333333333,
      "grad_norm": 0.056306175887584686,
      "learning_rate": 2.327666666666667e-05,
      "loss": 0.0021,
      "step": 80170
    },
    {
      "epoch": 4.276266666666666,
      "grad_norm": 0.28154727816581726,
      "learning_rate": 2.3273333333333333e-05,
      "loss": 0.0033,
      "step": 80180
    },
    {
      "epoch": 4.2768,
      "grad_norm": 0.08445926010608673,
      "learning_rate": 2.327e-05,
      "loss": 0.0041,
      "step": 80190
    },
    {
      "epoch": 4.277333333333333,
      "grad_norm": 0.08445776253938675,
      "learning_rate": 2.326666666666667e-05,
      "loss": 0.0025,
      "step": 80200
    },
    {
      "epoch": 4.277866666666666,
      "grad_norm": 0.16891539096832275,
      "learning_rate": 2.3263333333333335e-05,
      "loss": 0.0028,
      "step": 80210
    },
    {
      "epoch": 4.2783999999999995,
      "grad_norm": 0.056306686252355576,
      "learning_rate": 2.326e-05,
      "loss": 0.0022,
      "step": 80220
    },
    {
      "epoch": 4.278933333333334,
      "grad_norm": 0.11261562258005142,
      "learning_rate": 2.3256666666666667e-05,
      "loss": 0.0026,
      "step": 80230
    },
    {
      "epoch": 4.279466666666667,
      "grad_norm": 0.1407596915960312,
      "learning_rate": 2.3253333333333337e-05,
      "loss": 0.0046,
      "step": 80240
    },
    {
      "epoch": 4.28,
      "grad_norm": 0.5067691802978516,
      "learning_rate": 2.3250000000000003e-05,
      "loss": 0.0033,
      "step": 80250
    },
    {
      "epoch": 4.2805333333333335,
      "grad_norm": 0.11261332780122757,
      "learning_rate": 2.3246666666666665e-05,
      "loss": 0.0033,
      "step": 80260
    },
    {
      "epoch": 4.281066666666667,
      "grad_norm": 0.028153005987405777,
      "learning_rate": 2.3243333333333335e-05,
      "loss": 0.0027,
      "step": 80270
    },
    {
      "epoch": 4.2816,
      "grad_norm": 0.02815241739153862,
      "learning_rate": 2.324e-05,
      "loss": 0.0037,
      "step": 80280
    },
    {
      "epoch": 4.282133333333333,
      "grad_norm": 0.1126110628247261,
      "learning_rate": 2.3236666666666667e-05,
      "loss": 0.0022,
      "step": 80290
    },
    {
      "epoch": 4.282666666666667,
      "grad_norm": 0.45047980546951294,
      "learning_rate": 2.3233333333333333e-05,
      "loss": 0.0034,
      "step": 80300
    },
    {
      "epoch": 4.2832,
      "grad_norm": 0.08445568382740021,
      "learning_rate": 2.3230000000000003e-05,
      "loss": 0.0026,
      "step": 80310
    },
    {
      "epoch": 4.283733333333333,
      "grad_norm": 0.22523057460784912,
      "learning_rate": 2.322666666666667e-05,
      "loss": 0.0041,
      "step": 80320
    },
    {
      "epoch": 4.2842666666666664,
      "grad_norm": 2.9177962268533975e-09,
      "learning_rate": 2.3223333333333335e-05,
      "loss": 0.0032,
      "step": 80330
    },
    {
      "epoch": 4.2848,
      "grad_norm": 0.08445903658866882,
      "learning_rate": 2.322e-05,
      "loss": 0.0032,
      "step": 80340
    },
    {
      "epoch": 4.285333333333333,
      "grad_norm": 0.22522065043449402,
      "learning_rate": 2.3216666666666667e-05,
      "loss": 0.0018,
      "step": 80350
    },
    {
      "epoch": 4.285866666666666,
      "grad_norm": 0.563059389591217,
      "learning_rate": 2.3213333333333334e-05,
      "loss": 0.0033,
      "step": 80360
    },
    {
      "epoch": 4.2864,
      "grad_norm": 0.028152206912636757,
      "learning_rate": 2.321e-05,
      "loss": 0.0035,
      "step": 80370
    },
    {
      "epoch": 4.286933333333334,
      "grad_norm": 0.11261289566755295,
      "learning_rate": 2.320666666666667e-05,
      "loss": 0.0037,
      "step": 80380
    },
    {
      "epoch": 4.287466666666667,
      "grad_norm": 0.16891871392726898,
      "learning_rate": 2.3203333333333335e-05,
      "loss": 0.002,
      "step": 80390
    },
    {
      "epoch": 4.288,
      "grad_norm": 0.11260508000850677,
      "learning_rate": 2.32e-05,
      "loss": 0.0028,
      "step": 80400
    },
    {
      "epoch": 4.2885333333333335,
      "grad_norm": 0.0563049241900444,
      "learning_rate": 2.3196666666666668e-05,
      "loss": 0.0029,
      "step": 80410
    },
    {
      "epoch": 4.289066666666667,
      "grad_norm": 2.0926737785339355,
      "learning_rate": 2.3193333333333334e-05,
      "loss": 0.0043,
      "step": 80420
    },
    {
      "epoch": 4.2896,
      "grad_norm": 0.19706840813159943,
      "learning_rate": 2.319e-05,
      "loss": 0.0035,
      "step": 80430
    },
    {
      "epoch": 4.290133333333333,
      "grad_norm": 0.1407579481601715,
      "learning_rate": 2.3186666666666666e-05,
      "loss": 0.0033,
      "step": 80440
    },
    {
      "epoch": 4.290666666666667,
      "grad_norm": 0.2533706724643707,
      "learning_rate": 2.3183333333333336e-05,
      "loss": 0.0034,
      "step": 80450
    },
    {
      "epoch": 4.2912,
      "grad_norm": 0.02815171889960766,
      "learning_rate": 2.318e-05,
      "loss": 0.0036,
      "step": 80460
    },
    {
      "epoch": 4.291733333333333,
      "grad_norm": 0.05630454793572426,
      "learning_rate": 2.3176666666666668e-05,
      "loss": 0.0023,
      "step": 80470
    },
    {
      "epoch": 4.2922666666666665,
      "grad_norm": 0.02815212495625019,
      "learning_rate": 2.3173333333333337e-05,
      "loss": 0.0029,
      "step": 80480
    },
    {
      "epoch": 4.2928,
      "grad_norm": 0.028151176869869232,
      "learning_rate": 2.317e-05,
      "loss": 0.003,
      "step": 80490
    },
    {
      "epoch": 4.293333333333333,
      "grad_norm": 0.11260325461626053,
      "learning_rate": 2.3166666666666666e-05,
      "loss": 0.0036,
      "step": 80500
    },
    {
      "epoch": 4.293866666666666,
      "grad_norm": 0.05630460008978844,
      "learning_rate": 2.3163333333333336e-05,
      "loss": 0.0029,
      "step": 80510
    },
    {
      "epoch": 4.2943999999999996,
      "grad_norm": 0.11261267215013504,
      "learning_rate": 2.3160000000000002e-05,
      "loss": 0.0032,
      "step": 80520
    },
    {
      "epoch": 4.294933333333334,
      "grad_norm": 0.0844539925456047,
      "learning_rate": 2.3156666666666668e-05,
      "loss": 0.0039,
      "step": 80530
    },
    {
      "epoch": 4.295466666666667,
      "grad_norm": 0.056303564459085464,
      "learning_rate": 2.3153333333333334e-05,
      "loss": 0.0026,
      "step": 80540
    },
    {
      "epoch": 4.296,
      "grad_norm": 0.16892051696777344,
      "learning_rate": 2.3150000000000004e-05,
      "loss": 0.0032,
      "step": 80550
    },
    {
      "epoch": 4.2965333333333335,
      "grad_norm": 0.11260976642370224,
      "learning_rate": 2.3146666666666666e-05,
      "loss": 0.0035,
      "step": 80560
    },
    {
      "epoch": 4.297066666666667,
      "grad_norm": 0.11260664463043213,
      "learning_rate": 2.3143333333333333e-05,
      "loss": 0.0033,
      "step": 80570
    },
    {
      "epoch": 4.2976,
      "grad_norm": 0.028152158483862877,
      "learning_rate": 2.3140000000000002e-05,
      "loss": 0.0029,
      "step": 80580
    },
    {
      "epoch": 4.298133333333333,
      "grad_norm": 0.0844513326883316,
      "learning_rate": 2.3136666666666668e-05,
      "loss": 0.0023,
      "step": 80590
    },
    {
      "epoch": 4.298666666666667,
      "grad_norm": 0.168910950422287,
      "learning_rate": 2.3133333333333334e-05,
      "loss": 0.0027,
      "step": 80600
    },
    {
      "epoch": 4.2992,
      "grad_norm": 0.02815134823322296,
      "learning_rate": 2.313e-05,
      "loss": 0.0024,
      "step": 80610
    },
    {
      "epoch": 4.299733333333333,
      "grad_norm": 0.11260371655225754,
      "learning_rate": 2.312666666666667e-05,
      "loss": 0.0042,
      "step": 80620
    },
    {
      "epoch": 4.3002666666666665,
      "grad_norm": 0.05630422756075859,
      "learning_rate": 2.3123333333333336e-05,
      "loss": 0.0029,
      "step": 80630
    },
    {
      "epoch": 4.3008,
      "grad_norm": 0.4504636824131012,
      "learning_rate": 2.312e-05,
      "loss": 0.002,
      "step": 80640
    },
    {
      "epoch": 4.301333333333333,
      "grad_norm": 0.1126011535525322,
      "learning_rate": 2.311666666666667e-05,
      "loss": 0.0039,
      "step": 80650
    },
    {
      "epoch": 4.301866666666666,
      "grad_norm": 0.2252253144979477,
      "learning_rate": 2.3113333333333335e-05,
      "loss": 0.0031,
      "step": 80660
    },
    {
      "epoch": 4.3024000000000004,
      "grad_norm": 0.11260241270065308,
      "learning_rate": 2.311e-05,
      "loss": 0.0027,
      "step": 80670
    },
    {
      "epoch": 4.302933333333334,
      "grad_norm": 0.08445578813552856,
      "learning_rate": 2.3106666666666667e-05,
      "loss": 0.0017,
      "step": 80680
    },
    {
      "epoch": 4.303466666666667,
      "grad_norm": 0.1970614790916443,
      "learning_rate": 2.3103333333333336e-05,
      "loss": 0.0026,
      "step": 80690
    },
    {
      "epoch": 4.304,
      "grad_norm": 4.91862328644288e-09,
      "learning_rate": 2.3100000000000002e-05,
      "loss": 0.0038,
      "step": 80700
    },
    {
      "epoch": 4.3045333333333335,
      "grad_norm": 2.435442736370419e-09,
      "learning_rate": 2.3096666666666665e-05,
      "loss": 0.0021,
      "step": 80710
    },
    {
      "epoch": 4.305066666666667,
      "grad_norm": 0.08445076644420624,
      "learning_rate": 2.3093333333333335e-05,
      "loss": 0.0017,
      "step": 80720
    },
    {
      "epoch": 4.3056,
      "grad_norm": 4.188343005750994e-09,
      "learning_rate": 2.309e-05,
      "loss": 0.0022,
      "step": 80730
    },
    {
      "epoch": 4.306133333333333,
      "grad_norm": 0.19704917073249817,
      "learning_rate": 2.3086666666666667e-05,
      "loss": 0.0025,
      "step": 80740
    },
    {
      "epoch": 4.306666666666667,
      "grad_norm": 0.2815055549144745,
      "learning_rate": 2.3083333333333333e-05,
      "loss": 0.0039,
      "step": 80750
    },
    {
      "epoch": 4.3072,
      "grad_norm": 0.028150174766778946,
      "learning_rate": 2.3080000000000003e-05,
      "loss": 0.0027,
      "step": 80760
    },
    {
      "epoch": 4.307733333333333,
      "grad_norm": 0.08445262908935547,
      "learning_rate": 2.307666666666667e-05,
      "loss": 0.003,
      "step": 80770
    },
    {
      "epoch": 4.3082666666666665,
      "grad_norm": 0.1407521516084671,
      "learning_rate": 2.3073333333333335e-05,
      "loss": 0.0033,
      "step": 80780
    },
    {
      "epoch": 4.3088,
      "grad_norm": 0.14075049757957458,
      "learning_rate": 2.307e-05,
      "loss": 0.003,
      "step": 80790
    },
    {
      "epoch": 4.309333333333333,
      "grad_norm": 0.28150004148483276,
      "learning_rate": 2.3066666666666667e-05,
      "loss": 0.0034,
      "step": 80800
    },
    {
      "epoch": 4.309866666666666,
      "grad_norm": 0.1970461905002594,
      "learning_rate": 2.3063333333333333e-05,
      "loss": 0.0047,
      "step": 80810
    },
    {
      "epoch": 4.3104,
      "grad_norm": 0.028150754049420357,
      "learning_rate": 2.306e-05,
      "loss": 0.0028,
      "step": 80820
    },
    {
      "epoch": 4.310933333333334,
      "grad_norm": 0.0569341741502285,
      "learning_rate": 2.305666666666667e-05,
      "loss": 0.0025,
      "step": 80830
    },
    {
      "epoch": 4.311466666666667,
      "grad_norm": 0.14075113832950592,
      "learning_rate": 2.3053333333333335e-05,
      "loss": 0.0026,
      "step": 80840
    },
    {
      "epoch": 4.312,
      "grad_norm": 0.35645994544029236,
      "learning_rate": 2.305e-05,
      "loss": 0.003,
      "step": 80850
    },
    {
      "epoch": 4.3125333333333336,
      "grad_norm": 0.08444852381944656,
      "learning_rate": 2.3046666666666667e-05,
      "loss": 0.0028,
      "step": 80860
    },
    {
      "epoch": 4.313066666666667,
      "grad_norm": 0.2815164625644684,
      "learning_rate": 2.3043333333333334e-05,
      "loss": 0.0029,
      "step": 80870
    },
    {
      "epoch": 4.3136,
      "grad_norm": 0.30964288115501404,
      "learning_rate": 2.304e-05,
      "loss": 0.0022,
      "step": 80880
    },
    {
      "epoch": 4.314133333333333,
      "grad_norm": 0.08445703238248825,
      "learning_rate": 2.303666666666667e-05,
      "loss": 0.0027,
      "step": 80890
    },
    {
      "epoch": 4.314666666666667,
      "grad_norm": 0.16890837252140045,
      "learning_rate": 2.3033333333333335e-05,
      "loss": 0.0029,
      "step": 80900
    },
    {
      "epoch": 4.3152,
      "grad_norm": 0.3433530628681183,
      "learning_rate": 2.303e-05,
      "loss": 0.0036,
      "step": 80910
    },
    {
      "epoch": 4.315733333333333,
      "grad_norm": 0.1407582014799118,
      "learning_rate": 2.3026666666666668e-05,
      "loss": 0.0036,
      "step": 80920
    },
    {
      "epoch": 4.3162666666666665,
      "grad_norm": 7.764501571655273,
      "learning_rate": 2.3023333333333337e-05,
      "loss": 0.0034,
      "step": 80930
    },
    {
      "epoch": 4.3168,
      "grad_norm": 0.1970517337322235,
      "learning_rate": 2.302e-05,
      "loss": 0.0038,
      "step": 80940
    },
    {
      "epoch": 4.317333333333333,
      "grad_norm": 0.028150200843811035,
      "learning_rate": 2.3016666666666666e-05,
      "loss": 0.0021,
      "step": 80950
    },
    {
      "epoch": 4.317866666666666,
      "grad_norm": 0.3096396327018738,
      "learning_rate": 2.3013333333333335e-05,
      "loss": 0.0032,
      "step": 80960
    },
    {
      "epoch": 4.3184000000000005,
      "grad_norm": 0.14075452089309692,
      "learning_rate": 2.301e-05,
      "loss": 0.0027,
      "step": 80970
    },
    {
      "epoch": 4.318933333333334,
      "grad_norm": 2.1327302146545435e-09,
      "learning_rate": 2.3006666666666668e-05,
      "loss": 0.0018,
      "step": 80980
    },
    {
      "epoch": 4.319466666666667,
      "grad_norm": 3.939371495675914e-09,
      "learning_rate": 2.3003333333333334e-05,
      "loss": 0.0032,
      "step": 80990
    },
    {
      "epoch": 4.32,
      "grad_norm": 0.08445219695568085,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.0033,
      "step": 81000
    },
    {
      "epoch": 4.320533333333334,
      "grad_norm": 0.16889478266239166,
      "learning_rate": 2.299666666666667e-05,
      "loss": 0.0026,
      "step": 81010
    },
    {
      "epoch": 4.321066666666667,
      "grad_norm": 0.2533625364303589,
      "learning_rate": 2.2993333333333332e-05,
      "loss": 0.0025,
      "step": 81020
    },
    {
      "epoch": 4.3216,
      "grad_norm": 0.2815089821815491,
      "learning_rate": 2.2990000000000002e-05,
      "loss": 0.0033,
      "step": 81030
    },
    {
      "epoch": 4.322133333333333,
      "grad_norm": 0.28148165345191956,
      "learning_rate": 2.2986666666666668e-05,
      "loss": 0.0023,
      "step": 81040
    },
    {
      "epoch": 4.322666666666667,
      "grad_norm": 0.3378000557422638,
      "learning_rate": 2.2983333333333334e-05,
      "loss": 0.0021,
      "step": 81050
    },
    {
      "epoch": 4.3232,
      "grad_norm": 0.02815137803554535,
      "learning_rate": 2.298e-05,
      "loss": 0.0032,
      "step": 81060
    },
    {
      "epoch": 4.323733333333333,
      "grad_norm": 0.2252061665058136,
      "learning_rate": 2.297666666666667e-05,
      "loss": 0.003,
      "step": 81070
    },
    {
      "epoch": 4.3242666666666665,
      "grad_norm": 0.1125938668847084,
      "learning_rate": 2.2973333333333336e-05,
      "loss": 0.0033,
      "step": 81080
    },
    {
      "epoch": 4.3248,
      "grad_norm": 0.33781611919403076,
      "learning_rate": 2.297e-05,
      "loss": 0.0035,
      "step": 81090
    },
    {
      "epoch": 4.325333333333333,
      "grad_norm": 0.1688893735408783,
      "learning_rate": 2.2966666666666668e-05,
      "loss": 0.0026,
      "step": 81100
    },
    {
      "epoch": 4.325866666666666,
      "grad_norm": 0.30965927243232727,
      "learning_rate": 2.2963333333333334e-05,
      "loss": 0.0019,
      "step": 81110
    },
    {
      "epoch": 4.3264,
      "grad_norm": 0.22518295049667358,
      "learning_rate": 2.296e-05,
      "loss": 0.0031,
      "step": 81120
    },
    {
      "epoch": 4.326933333333334,
      "grad_norm": 0.3377956748008728,
      "learning_rate": 2.2956666666666667e-05,
      "loss": 0.0027,
      "step": 81130
    },
    {
      "epoch": 4.327466666666667,
      "grad_norm": 0.33778145909309387,
      "learning_rate": 2.2953333333333336e-05,
      "loss": 0.003,
      "step": 81140
    },
    {
      "epoch": 4.328,
      "grad_norm": 0.0562974251806736,
      "learning_rate": 2.2950000000000002e-05,
      "loss": 0.0033,
      "step": 81150
    },
    {
      "epoch": 4.328533333333334,
      "grad_norm": 0.1125943511724472,
      "learning_rate": 2.294666666666667e-05,
      "loss": 0.0026,
      "step": 81160
    },
    {
      "epoch": 4.329066666666667,
      "grad_norm": 0.5066791772842407,
      "learning_rate": 2.2943333333333334e-05,
      "loss": 0.003,
      "step": 81170
    },
    {
      "epoch": 4.3296,
      "grad_norm": 0.22519850730895996,
      "learning_rate": 2.294e-05,
      "loss": 0.0042,
      "step": 81180
    },
    {
      "epoch": 4.330133333333333,
      "grad_norm": 0.1125965490937233,
      "learning_rate": 2.2936666666666667e-05,
      "loss": 0.0025,
      "step": 81190
    },
    {
      "epoch": 4.330666666666667,
      "grad_norm": 0.05629684403538704,
      "learning_rate": 2.2933333333333333e-05,
      "loss": 0.0022,
      "step": 81200
    },
    {
      "epoch": 4.3312,
      "grad_norm": 0.25333333015441895,
      "learning_rate": 2.2930000000000002e-05,
      "loss": 0.0027,
      "step": 81210
    },
    {
      "epoch": 4.331733333333333,
      "grad_norm": 0.3096296787261963,
      "learning_rate": 2.292666666666667e-05,
      "loss": 0.0029,
      "step": 81220
    },
    {
      "epoch": 4.3322666666666665,
      "grad_norm": 2.757891470750451e-09,
      "learning_rate": 2.2923333333333335e-05,
      "loss": 0.003,
      "step": 81230
    },
    {
      "epoch": 4.3328,
      "grad_norm": 0.11259696632623672,
      "learning_rate": 2.292e-05,
      "loss": 0.0028,
      "step": 81240
    },
    {
      "epoch": 4.333333333333333,
      "grad_norm": 0.4909477233886719,
      "learning_rate": 2.2916666666666667e-05,
      "loss": 0.0022,
      "step": 81250
    },
    {
      "epoch": 4.333866666666666,
      "grad_norm": 0.056297220289707184,
      "learning_rate": 2.2913333333333333e-05,
      "loss": 0.0032,
      "step": 81260
    },
    {
      "epoch": 4.3344,
      "grad_norm": 0.14074355363845825,
      "learning_rate": 2.2910000000000003e-05,
      "loss": 0.0029,
      "step": 81270
    },
    {
      "epoch": 4.334933333333334,
      "grad_norm": 0.08444210141897202,
      "learning_rate": 2.290666666666667e-05,
      "loss": 0.0027,
      "step": 81280
    },
    {
      "epoch": 4.335466666666667,
      "grad_norm": 0.056295666843652725,
      "learning_rate": 2.2903333333333335e-05,
      "loss": 0.003,
      "step": 81290
    },
    {
      "epoch": 4.336,
      "grad_norm": 0.16888967156410217,
      "learning_rate": 2.29e-05,
      "loss": 0.0025,
      "step": 81300
    },
    {
      "epoch": 4.336533333333334,
      "grad_norm": 0.1125950813293457,
      "learning_rate": 2.289666666666667e-05,
      "loss": 0.0036,
      "step": 81310
    },
    {
      "epoch": 4.337066666666667,
      "grad_norm": 0.14074623584747314,
      "learning_rate": 2.2893333333333333e-05,
      "loss": 0.0023,
      "step": 81320
    },
    {
      "epoch": 4.3376,
      "grad_norm": 0.02814766950905323,
      "learning_rate": 2.289e-05,
      "loss": 0.0018,
      "step": 81330
    },
    {
      "epoch": 4.338133333333333,
      "grad_norm": 0.08444860577583313,
      "learning_rate": 2.288666666666667e-05,
      "loss": 0.0039,
      "step": 81340
    },
    {
      "epoch": 4.338666666666667,
      "grad_norm": 0.1407439112663269,
      "learning_rate": 2.2883333333333335e-05,
      "loss": 0.0025,
      "step": 81350
    },
    {
      "epoch": 4.3392,
      "grad_norm": 0.028147375211119652,
      "learning_rate": 2.288e-05,
      "loss": 0.0028,
      "step": 81360
    },
    {
      "epoch": 4.339733333333333,
      "grad_norm": 0.28148990869522095,
      "learning_rate": 2.2876666666666667e-05,
      "loss": 0.0039,
      "step": 81370
    },
    {
      "epoch": 4.3402666666666665,
      "grad_norm": 0.19704218208789825,
      "learning_rate": 2.2873333333333337e-05,
      "loss": 0.0043,
      "step": 81380
    },
    {
      "epoch": 4.3408,
      "grad_norm": 0.2814716696739197,
      "learning_rate": 2.287e-05,
      "loss": 0.0028,
      "step": 81390
    },
    {
      "epoch": 4.341333333333333,
      "grad_norm": 0.450361967086792,
      "learning_rate": 2.2866666666666666e-05,
      "loss": 0.003,
      "step": 81400
    },
    {
      "epoch": 4.341866666666666,
      "grad_norm": 0.08444632589817047,
      "learning_rate": 2.2863333333333335e-05,
      "loss": 0.0022,
      "step": 81410
    },
    {
      "epoch": 4.3424,
      "grad_norm": 0.028147637844085693,
      "learning_rate": 2.286e-05,
      "loss": 0.0021,
      "step": 81420
    },
    {
      "epoch": 4.342933333333333,
      "grad_norm": 0.05629405006766319,
      "learning_rate": 2.2856666666666667e-05,
      "loss": 0.0031,
      "step": 81430
    },
    {
      "epoch": 4.343466666666667,
      "grad_norm": 0.08444709330797195,
      "learning_rate": 2.2853333333333334e-05,
      "loss": 0.0018,
      "step": 81440
    },
    {
      "epoch": 4.344,
      "grad_norm": 0.19703306257724762,
      "learning_rate": 2.2850000000000003e-05,
      "loss": 0.0016,
      "step": 81450
    },
    {
      "epoch": 4.344533333333334,
      "grad_norm": 0.3011854887008667,
      "learning_rate": 2.284666666666667e-05,
      "loss": 0.0026,
      "step": 81460
    },
    {
      "epoch": 4.345066666666667,
      "grad_norm": 0.14073967933654785,
      "learning_rate": 2.2843333333333332e-05,
      "loss": 0.0026,
      "step": 81470
    },
    {
      "epoch": 4.3456,
      "grad_norm": 0.1688862144947052,
      "learning_rate": 2.284e-05,
      "loss": 0.0025,
      "step": 81480
    },
    {
      "epoch": 4.346133333333333,
      "grad_norm": 0.028146915137767792,
      "learning_rate": 2.2836666666666668e-05,
      "loss": 0.0029,
      "step": 81490
    },
    {
      "epoch": 4.346666666666667,
      "grad_norm": 0.30964726209640503,
      "learning_rate": 2.2833333333333334e-05,
      "loss": 0.0029,
      "step": 81500
    },
    {
      "epoch": 4.3472,
      "grad_norm": 0.028146762400865555,
      "learning_rate": 2.283e-05,
      "loss": 0.003,
      "step": 81510
    },
    {
      "epoch": 4.347733333333333,
      "grad_norm": 0.36591753363609314,
      "learning_rate": 2.282666666666667e-05,
      "loss": 0.0023,
      "step": 81520
    },
    {
      "epoch": 4.3482666666666665,
      "grad_norm": 0.0844469889998436,
      "learning_rate": 2.2823333333333336e-05,
      "loss": 0.0026,
      "step": 81530
    },
    {
      "epoch": 4.3488,
      "grad_norm": 0.1970306634902954,
      "learning_rate": 2.282e-05,
      "loss": 0.0045,
      "step": 81540
    },
    {
      "epoch": 4.349333333333333,
      "grad_norm": 0.028146784752607346,
      "learning_rate": 2.2816666666666668e-05,
      "loss": 0.0021,
      "step": 81550
    },
    {
      "epoch": 4.349866666666666,
      "grad_norm": 0.2533436119556427,
      "learning_rate": 2.2813333333333334e-05,
      "loss": 0.0027,
      "step": 81560
    },
    {
      "epoch": 4.3504,
      "grad_norm": 0.5629235506057739,
      "learning_rate": 2.281e-05,
      "loss": 0.0038,
      "step": 81570
    },
    {
      "epoch": 4.350933333333334,
      "grad_norm": 0.2814714014530182,
      "learning_rate": 2.2806666666666666e-05,
      "loss": 0.0032,
      "step": 81580
    },
    {
      "epoch": 4.351466666666667,
      "grad_norm": 0.028147559612989426,
      "learning_rate": 2.2803333333333336e-05,
      "loss": 0.0031,
      "step": 81590
    },
    {
      "epoch": 4.352,
      "grad_norm": 0.11258727312088013,
      "learning_rate": 2.2800000000000002e-05,
      "loss": 0.0024,
      "step": 81600
    },
    {
      "epoch": 4.352533333333334,
      "grad_norm": 0.28147226572036743,
      "learning_rate": 2.2796666666666668e-05,
      "loss": 0.0041,
      "step": 81610
    },
    {
      "epoch": 4.353066666666667,
      "grad_norm": 0.11258846521377563,
      "learning_rate": 2.2793333333333334e-05,
      "loss": 0.0031,
      "step": 81620
    },
    {
      "epoch": 4.3536,
      "grad_norm": 0.3658839166164398,
      "learning_rate": 2.279e-05,
      "loss": 0.0042,
      "step": 81630
    },
    {
      "epoch": 4.354133333333333,
      "grad_norm": 0.028147900477051735,
      "learning_rate": 2.2786666666666666e-05,
      "loss": 0.0027,
      "step": 81640
    },
    {
      "epoch": 4.354666666666667,
      "grad_norm": 0.7036235928535461,
      "learning_rate": 2.2783333333333336e-05,
      "loss": 0.003,
      "step": 81650
    },
    {
      "epoch": 4.3552,
      "grad_norm": 4.3916918990305476e-09,
      "learning_rate": 2.2780000000000002e-05,
      "loss": 0.0026,
      "step": 81660
    },
    {
      "epoch": 4.355733333333333,
      "grad_norm": 0.08444186300039291,
      "learning_rate": 2.2776666666666668e-05,
      "loss": 0.0039,
      "step": 81670
    },
    {
      "epoch": 4.3562666666666665,
      "grad_norm": 0.028144804760813713,
      "learning_rate": 2.2773333333333334e-05,
      "loss": 0.0027,
      "step": 81680
    },
    {
      "epoch": 4.3568,
      "grad_norm": 0.028147749602794647,
      "learning_rate": 2.2770000000000004e-05,
      "loss": 0.0024,
      "step": 81690
    },
    {
      "epoch": 4.357333333333333,
      "grad_norm": 8.158715480988121e-09,
      "learning_rate": 2.2766666666666667e-05,
      "loss": 0.0038,
      "step": 81700
    },
    {
      "epoch": 4.357866666666666,
      "grad_norm": 0.2533135712146759,
      "learning_rate": 2.2763333333333333e-05,
      "loss": 0.0048,
      "step": 81710
    },
    {
      "epoch": 4.3584,
      "grad_norm": 0.5628972053527832,
      "learning_rate": 2.2760000000000002e-05,
      "loss": 0.0019,
      "step": 81720
    },
    {
      "epoch": 4.358933333333333,
      "grad_norm": 0.39406636357307434,
      "learning_rate": 2.275666666666667e-05,
      "loss": 0.0021,
      "step": 81730
    },
    {
      "epoch": 4.359466666666667,
      "grad_norm": 0.11258038878440857,
      "learning_rate": 2.2753333333333335e-05,
      "loss": 0.0036,
      "step": 81740
    },
    {
      "epoch": 4.36,
      "grad_norm": 0.1688757836818695,
      "learning_rate": 2.275e-05,
      "loss": 0.003,
      "step": 81750
    },
    {
      "epoch": 4.360533333333334,
      "grad_norm": 0.11258520185947418,
      "learning_rate": 2.274666666666667e-05,
      "loss": 0.004,
      "step": 81760
    },
    {
      "epoch": 4.361066666666667,
      "grad_norm": 0.05629084259271622,
      "learning_rate": 2.2743333333333333e-05,
      "loss": 0.0027,
      "step": 81770
    },
    {
      "epoch": 4.3616,
      "grad_norm": 0.28146713972091675,
      "learning_rate": 2.274e-05,
      "loss": 0.004,
      "step": 81780
    },
    {
      "epoch": 4.362133333333333,
      "grad_norm": 2.098848650433638e-09,
      "learning_rate": 2.273666666666667e-05,
      "loss": 0.0027,
      "step": 81790
    },
    {
      "epoch": 4.362666666666667,
      "grad_norm": 0.3658749461174011,
      "learning_rate": 2.2733333333333335e-05,
      "loss": 0.0025,
      "step": 81800
    },
    {
      "epoch": 4.3632,
      "grad_norm": 0.056293316185474396,
      "learning_rate": 2.273e-05,
      "loss": 0.0038,
      "step": 81810
    },
    {
      "epoch": 4.363733333333333,
      "grad_norm": 0.08444028347730637,
      "learning_rate": 2.2726666666666667e-05,
      "loss": 0.0027,
      "step": 81820
    },
    {
      "epoch": 4.3642666666666665,
      "grad_norm": 0.16888019442558289,
      "learning_rate": 2.2723333333333337e-05,
      "loss": 0.0036,
      "step": 81830
    },
    {
      "epoch": 4.3648,
      "grad_norm": 0.30959171056747437,
      "learning_rate": 2.2720000000000003e-05,
      "loss": 0.0029,
      "step": 81840
    },
    {
      "epoch": 4.365333333333333,
      "grad_norm": 0.05629124492406845,
      "learning_rate": 2.2716666666666665e-05,
      "loss": 0.0025,
      "step": 81850
    },
    {
      "epoch": 4.365866666666666,
      "grad_norm": 0.11258906871080399,
      "learning_rate": 2.2713333333333335e-05,
      "loss": 0.0032,
      "step": 81860
    },
    {
      "epoch": 4.3664,
      "grad_norm": 0.16887269914150238,
      "learning_rate": 2.271e-05,
      "loss": 0.0029,
      "step": 81870
    },
    {
      "epoch": 4.366933333333334,
      "grad_norm": 0.028144801035523415,
      "learning_rate": 2.2706666666666667e-05,
      "loss": 0.0039,
      "step": 81880
    },
    {
      "epoch": 4.367466666666667,
      "grad_norm": 0.05628931149840355,
      "learning_rate": 2.2703333333333333e-05,
      "loss": 0.0024,
      "step": 81890
    },
    {
      "epoch": 4.368,
      "grad_norm": 0.19702018797397614,
      "learning_rate": 2.2700000000000003e-05,
      "loss": 0.0024,
      "step": 81900
    },
    {
      "epoch": 4.368533333333334,
      "grad_norm": 0.3377637565135956,
      "learning_rate": 2.269666666666667e-05,
      "loss": 0.0029,
      "step": 81910
    },
    {
      "epoch": 4.369066666666667,
      "grad_norm": 0.19700933992862701,
      "learning_rate": 2.2693333333333332e-05,
      "loss": 0.0018,
      "step": 81920
    },
    {
      "epoch": 4.3696,
      "grad_norm": 0.16886958479881287,
      "learning_rate": 2.269e-05,
      "loss": 0.0028,
      "step": 81930
    },
    {
      "epoch": 4.370133333333333,
      "grad_norm": 0.028144851326942444,
      "learning_rate": 2.2686666666666667e-05,
      "loss": 0.0031,
      "step": 81940
    },
    {
      "epoch": 4.370666666666667,
      "grad_norm": 2.8984312727686756e-09,
      "learning_rate": 2.2683333333333334e-05,
      "loss": 0.0041,
      "step": 81950
    },
    {
      "epoch": 4.3712,
      "grad_norm": 0.0844321921467781,
      "learning_rate": 2.268e-05,
      "loss": 0.003,
      "step": 81960
    },
    {
      "epoch": 4.371733333333333,
      "grad_norm": 0.0281454399228096,
      "learning_rate": 2.267666666666667e-05,
      "loss": 0.0027,
      "step": 81970
    },
    {
      "epoch": 4.3722666666666665,
      "grad_norm": 0.08444128930568695,
      "learning_rate": 2.2673333333333335e-05,
      "loss": 0.0035,
      "step": 81980
    },
    {
      "epoch": 4.3728,
      "grad_norm": 0.02814602293074131,
      "learning_rate": 2.267e-05,
      "loss": 0.0016,
      "step": 81990
    },
    {
      "epoch": 4.373333333333333,
      "grad_norm": 0.1688718944787979,
      "learning_rate": 2.2666666666666668e-05,
      "loss": 0.0033,
      "step": 82000
    },
    {
      "epoch": 4.373866666666666,
      "grad_norm": 0.11257865279912949,
      "learning_rate": 2.2663333333333334e-05,
      "loss": 0.0024,
      "step": 82010
    },
    {
      "epoch": 4.3744,
      "grad_norm": 0.11258399486541748,
      "learning_rate": 2.266e-05,
      "loss": 0.0022,
      "step": 82020
    },
    {
      "epoch": 4.374933333333333,
      "grad_norm": 0.16886399686336517,
      "learning_rate": 2.2656666666666666e-05,
      "loss": 0.002,
      "step": 82030
    },
    {
      "epoch": 4.375466666666667,
      "grad_norm": 0.16887272894382477,
      "learning_rate": 2.2653333333333336e-05,
      "loss": 0.003,
      "step": 82040
    },
    {
      "epoch": 4.376,
      "grad_norm": 0.028143690899014473,
      "learning_rate": 2.265e-05,
      "loss": 0.0024,
      "step": 82050
    },
    {
      "epoch": 4.376533333333334,
      "grad_norm": 0.28145483136177063,
      "learning_rate": 2.2646666666666668e-05,
      "loss": 0.0023,
      "step": 82060
    },
    {
      "epoch": 4.377066666666667,
      "grad_norm": 0.22515177726745605,
      "learning_rate": 2.2643333333333334e-05,
      "loss": 0.0025,
      "step": 82070
    },
    {
      "epoch": 4.3776,
      "grad_norm": 0.056288499385118484,
      "learning_rate": 2.264e-05,
      "loss": 0.0033,
      "step": 82080
    },
    {
      "epoch": 4.378133333333333,
      "grad_norm": 0.02814614772796631,
      "learning_rate": 2.2636666666666666e-05,
      "loss": 0.0031,
      "step": 82090
    },
    {
      "epoch": 4.378666666666667,
      "grad_norm": 2.3807449345270015e-09,
      "learning_rate": 2.2633333333333336e-05,
      "loss": 0.0033,
      "step": 82100
    },
    {
      "epoch": 4.3792,
      "grad_norm": 0.14072437584400177,
      "learning_rate": 2.2630000000000002e-05,
      "loss": 0.0035,
      "step": 82110
    },
    {
      "epoch": 4.379733333333333,
      "grad_norm": 0.14071568846702576,
      "learning_rate": 2.2626666666666668e-05,
      "loss": 0.003,
      "step": 82120
    },
    {
      "epoch": 4.3802666666666665,
      "grad_norm": 0.19702374935150146,
      "learning_rate": 2.2623333333333334e-05,
      "loss": 0.0025,
      "step": 82130
    },
    {
      "epoch": 4.3808,
      "grad_norm": 0.02814486064016819,
      "learning_rate": 2.2620000000000004e-05,
      "loss": 0.0027,
      "step": 82140
    },
    {
      "epoch": 4.381333333333333,
      "grad_norm": 0.16886860132217407,
      "learning_rate": 2.2616666666666666e-05,
      "loss": 0.003,
      "step": 82150
    },
    {
      "epoch": 4.381866666666666,
      "grad_norm": 0.14072853326797485,
      "learning_rate": 2.2613333333333333e-05,
      "loss": 0.0023,
      "step": 82160
    },
    {
      "epoch": 4.3824,
      "grad_norm": 0.056288085877895355,
      "learning_rate": 2.2610000000000002e-05,
      "loss": 0.002,
      "step": 82170
    },
    {
      "epoch": 4.382933333333334,
      "grad_norm": 0.05628756806254387,
      "learning_rate": 2.2606666666666668e-05,
      "loss": 0.0036,
      "step": 82180
    },
    {
      "epoch": 4.383466666666667,
      "grad_norm": 0.05122872069478035,
      "learning_rate": 2.2603333333333334e-05,
      "loss": 0.0029,
      "step": 82190
    },
    {
      "epoch": 4.384,
      "grad_norm": 0.19700965285301208,
      "learning_rate": 2.26e-05,
      "loss": 0.002,
      "step": 82200
    },
    {
      "epoch": 4.384533333333334,
      "grad_norm": 0.3377492427825928,
      "learning_rate": 2.259666666666667e-05,
      "loss": 0.0032,
      "step": 82210
    },
    {
      "epoch": 4.385066666666667,
      "grad_norm": 0.14071771502494812,
      "learning_rate": 2.2593333333333336e-05,
      "loss": 0.0028,
      "step": 82220
    },
    {
      "epoch": 4.3856,
      "grad_norm": 0.1688736528158188,
      "learning_rate": 2.259e-05,
      "loss": 0.003,
      "step": 82230
    },
    {
      "epoch": 4.386133333333333,
      "grad_norm": 0.08443035930395126,
      "learning_rate": 2.258666666666667e-05,
      "loss": 0.0019,
      "step": 82240
    },
    {
      "epoch": 4.386666666666667,
      "grad_norm": 0.16885758936405182,
      "learning_rate": 2.2583333333333335e-05,
      "loss": 0.0037,
      "step": 82250
    },
    {
      "epoch": 4.3872,
      "grad_norm": 0.08443228900432587,
      "learning_rate": 2.258e-05,
      "loss": 0.003,
      "step": 82260
    },
    {
      "epoch": 4.387733333333333,
      "grad_norm": 0.19700253009796143,
      "learning_rate": 2.2576666666666667e-05,
      "loss": 0.0029,
      "step": 82270
    },
    {
      "epoch": 4.3882666666666665,
      "grad_norm": 0.1407141089439392,
      "learning_rate": 2.2573333333333336e-05,
      "loss": 0.0028,
      "step": 82280
    },
    {
      "epoch": 4.3888,
      "grad_norm": 0.16886430978775024,
      "learning_rate": 2.2570000000000002e-05,
      "loss": 0.0035,
      "step": 82290
    },
    {
      "epoch": 4.389333333333333,
      "grad_norm": 0.1970091611146927,
      "learning_rate": 2.2566666666666665e-05,
      "loss": 0.0026,
      "step": 82300
    },
    {
      "epoch": 4.389866666666666,
      "grad_norm": 0.1688610315322876,
      "learning_rate": 2.2563333333333335e-05,
      "loss": 0.0021,
      "step": 82310
    },
    {
      "epoch": 4.3904,
      "grad_norm": 0.028143076226115227,
      "learning_rate": 2.256e-05,
      "loss": 0.0032,
      "step": 82320
    },
    {
      "epoch": 4.390933333333333,
      "grad_norm": 0.14071904122829437,
      "learning_rate": 2.2556666666666667e-05,
      "loss": 0.0031,
      "step": 82330
    },
    {
      "epoch": 4.391466666666667,
      "grad_norm": 0.28142568469047546,
      "learning_rate": 2.2553333333333333e-05,
      "loss": 0.0028,
      "step": 82340
    },
    {
      "epoch": 4.392,
      "grad_norm": 0.056290462613105774,
      "learning_rate": 2.2550000000000003e-05,
      "loss": 0.0033,
      "step": 82350
    },
    {
      "epoch": 4.392533333333334,
      "grad_norm": 0.14070816338062286,
      "learning_rate": 2.254666666666667e-05,
      "loss": 0.0021,
      "step": 82360
    },
    {
      "epoch": 4.393066666666667,
      "grad_norm": 0.05628655478358269,
      "learning_rate": 2.2543333333333335e-05,
      "loss": 0.0028,
      "step": 82370
    },
    {
      "epoch": 4.3936,
      "grad_norm": 0.5066078901290894,
      "learning_rate": 2.254e-05,
      "loss": 0.0029,
      "step": 82380
    },
    {
      "epoch": 4.3941333333333334,
      "grad_norm": 0.16884903609752655,
      "learning_rate": 2.2536666666666667e-05,
      "loss": 0.0029,
      "step": 82390
    },
    {
      "epoch": 4.394666666666667,
      "grad_norm": 0.337728887796402,
      "learning_rate": 2.2533333333333333e-05,
      "loss": 0.0024,
      "step": 82400
    },
    {
      "epoch": 4.3952,
      "grad_norm": 0.05628571659326553,
      "learning_rate": 2.253e-05,
      "loss": 0.004,
      "step": 82410
    },
    {
      "epoch": 4.395733333333333,
      "grad_norm": 0.450279176235199,
      "learning_rate": 2.252666666666667e-05,
      "loss": 0.0024,
      "step": 82420
    },
    {
      "epoch": 4.3962666666666665,
      "grad_norm": 0.14071300625801086,
      "learning_rate": 2.2523333333333335e-05,
      "loss": 0.0024,
      "step": 82430
    },
    {
      "epoch": 4.3968,
      "grad_norm": 0.1407119184732437,
      "learning_rate": 2.252e-05,
      "loss": 0.0026,
      "step": 82440
    },
    {
      "epoch": 4.397333333333333,
      "grad_norm": 0.39581233263015747,
      "learning_rate": 2.2516666666666667e-05,
      "loss": 0.0029,
      "step": 82450
    },
    {
      "epoch": 4.397866666666666,
      "grad_norm": 0.14312931895256042,
      "learning_rate": 2.2513333333333333e-05,
      "loss": 0.0023,
      "step": 82460
    },
    {
      "epoch": 4.3984,
      "grad_norm": 0.08442723751068115,
      "learning_rate": 2.251e-05,
      "loss": 0.0031,
      "step": 82470
    },
    {
      "epoch": 4.398933333333333,
      "grad_norm": 0.16885477304458618,
      "learning_rate": 2.250666666666667e-05,
      "loss": 0.0033,
      "step": 82480
    },
    {
      "epoch": 4.399466666666667,
      "grad_norm": 0.11257193982601166,
      "learning_rate": 2.2503333333333335e-05,
      "loss": 0.004,
      "step": 82490
    },
    {
      "epoch": 4.4,
      "grad_norm": 0.21358561515808105,
      "learning_rate": 2.25e-05,
      "loss": 0.0028,
      "step": 82500
    },
    {
      "epoch": 4.400533333333334,
      "grad_norm": 0.1969936192035675,
      "learning_rate": 2.2496666666666668e-05,
      "loss": 0.0035,
      "step": 82510
    },
    {
      "epoch": 4.401066666666667,
      "grad_norm": 0.28142249584198,
      "learning_rate": 2.2493333333333337e-05,
      "loss": 0.0025,
      "step": 82520
    },
    {
      "epoch": 4.4016,
      "grad_norm": 0.05628485977649689,
      "learning_rate": 2.249e-05,
      "loss": 0.003,
      "step": 82530
    },
    {
      "epoch": 4.4021333333333335,
      "grad_norm": 0.22515515983104706,
      "learning_rate": 2.2486666666666666e-05,
      "loss": 0.0027,
      "step": 82540
    },
    {
      "epoch": 4.402666666666667,
      "grad_norm": 0.02814200520515442,
      "learning_rate": 2.2483333333333335e-05,
      "loss": 0.0018,
      "step": 82550
    },
    {
      "epoch": 4.4032,
      "grad_norm": 0.11257157474756241,
      "learning_rate": 2.248e-05,
      "loss": 0.0036,
      "step": 82560
    },
    {
      "epoch": 4.403733333333333,
      "grad_norm": 0.11257490515708923,
      "learning_rate": 2.2476666666666668e-05,
      "loss": 0.0025,
      "step": 82570
    },
    {
      "epoch": 4.4042666666666666,
      "grad_norm": 0.19698719680309296,
      "learning_rate": 2.2473333333333334e-05,
      "loss": 0.0028,
      "step": 82580
    },
    {
      "epoch": 4.4048,
      "grad_norm": 0.056285373866558075,
      "learning_rate": 2.2470000000000003e-05,
      "loss": 0.0024,
      "step": 82590
    },
    {
      "epoch": 4.405333333333333,
      "grad_norm": 0.19699619710445404,
      "learning_rate": 2.2466666666666666e-05,
      "loss": 0.0029,
      "step": 82600
    },
    {
      "epoch": 4.405866666666666,
      "grad_norm": 0.056283336132764816,
      "learning_rate": 2.2463333333333332e-05,
      "loss": 0.0033,
      "step": 82610
    },
    {
      "epoch": 4.4064,
      "grad_norm": 0.1125708743929863,
      "learning_rate": 2.2460000000000002e-05,
      "loss": 0.0037,
      "step": 82620
    },
    {
      "epoch": 4.406933333333333,
      "grad_norm": 0.25328242778778076,
      "learning_rate": 2.2456666666666668e-05,
      "loss": 0.0023,
      "step": 82630
    },
    {
      "epoch": 4.407466666666666,
      "grad_norm": 0.028142696246504784,
      "learning_rate": 2.2453333333333334e-05,
      "loss": 0.0029,
      "step": 82640
    },
    {
      "epoch": 4.408,
      "grad_norm": 0.08443049341440201,
      "learning_rate": 2.245e-05,
      "loss": 0.0029,
      "step": 82650
    },
    {
      "epoch": 4.408533333333334,
      "grad_norm": 0.14070630073547363,
      "learning_rate": 2.244666666666667e-05,
      "loss": 0.0031,
      "step": 82660
    },
    {
      "epoch": 4.409066666666667,
      "grad_norm": 5.046817630471878e-09,
      "learning_rate": 2.2443333333333336e-05,
      "loss": 0.0034,
      "step": 82670
    },
    {
      "epoch": 4.4096,
      "grad_norm": 0.4221274256706238,
      "learning_rate": 2.244e-05,
      "loss": 0.0036,
      "step": 82680
    },
    {
      "epoch": 4.4101333333333335,
      "grad_norm": 0.08442866057157516,
      "learning_rate": 2.2436666666666668e-05,
      "loss": 0.002,
      "step": 82690
    },
    {
      "epoch": 4.410666666666667,
      "grad_norm": 0.3376959264278412,
      "learning_rate": 2.2433333333333334e-05,
      "loss": 0.002,
      "step": 82700
    },
    {
      "epoch": 4.4112,
      "grad_norm": 0.05628432705998421,
      "learning_rate": 2.243e-05,
      "loss": 0.002,
      "step": 82710
    },
    {
      "epoch": 4.411733333333333,
      "grad_norm": 3.6075864517925993e-09,
      "learning_rate": 2.2426666666666667e-05,
      "loss": 0.0018,
      "step": 82720
    },
    {
      "epoch": 4.412266666666667,
      "grad_norm": 1.7022729892346433e-09,
      "learning_rate": 2.2423333333333336e-05,
      "loss": 0.0021,
      "step": 82730
    },
    {
      "epoch": 4.4128,
      "grad_norm": 0.05628448724746704,
      "learning_rate": 2.2420000000000002e-05,
      "loss": 0.0035,
      "step": 82740
    },
    {
      "epoch": 4.413333333333333,
      "grad_norm": 2.7529545310045478e-09,
      "learning_rate": 2.2416666666666665e-05,
      "loss": 0.0024,
      "step": 82750
    },
    {
      "epoch": 4.413866666666666,
      "grad_norm": 0.11255991458892822,
      "learning_rate": 2.2413333333333334e-05,
      "loss": 0.0019,
      "step": 82760
    },
    {
      "epoch": 4.4144,
      "grad_norm": 0.056283075362443924,
      "learning_rate": 2.241e-05,
      "loss": 0.0027,
      "step": 82770
    },
    {
      "epoch": 4.414933333333333,
      "grad_norm": 0.25327396392822266,
      "learning_rate": 2.2406666666666667e-05,
      "loss": 0.003,
      "step": 82780
    },
    {
      "epoch": 4.415466666666667,
      "grad_norm": 0.2814103662967682,
      "learning_rate": 2.2403333333333333e-05,
      "loss": 0.0022,
      "step": 82790
    },
    {
      "epoch": 4.416,
      "grad_norm": 0.14071425795555115,
      "learning_rate": 2.2400000000000002e-05,
      "loss": 0.0033,
      "step": 82800
    },
    {
      "epoch": 4.416533333333334,
      "grad_norm": 0.42209741473197937,
      "learning_rate": 2.239666666666667e-05,
      "loss": 0.0025,
      "step": 82810
    },
    {
      "epoch": 4.417066666666667,
      "grad_norm": 0.20354682207107544,
      "learning_rate": 2.2393333333333335e-05,
      "loss": 0.0037,
      "step": 82820
    },
    {
      "epoch": 4.4176,
      "grad_norm": 0.11257237941026688,
      "learning_rate": 2.239e-05,
      "loss": 0.0034,
      "step": 82830
    },
    {
      "epoch": 4.4181333333333335,
      "grad_norm": 0.3095370829105377,
      "learning_rate": 2.2386666666666667e-05,
      "loss": 0.0036,
      "step": 82840
    },
    {
      "epoch": 4.418666666666667,
      "grad_norm": 0.056284185498952866,
      "learning_rate": 2.2383333333333333e-05,
      "loss": 0.0017,
      "step": 82850
    },
    {
      "epoch": 4.4192,
      "grad_norm": 0.16884039342403412,
      "learning_rate": 2.2380000000000003e-05,
      "loss": 0.0025,
      "step": 82860
    },
    {
      "epoch": 4.419733333333333,
      "grad_norm": 0.19698330760002136,
      "learning_rate": 2.237666666666667e-05,
      "loss": 0.0029,
      "step": 82870
    },
    {
      "epoch": 4.420266666666667,
      "grad_norm": 0.08442217856645584,
      "learning_rate": 2.2373333333333335e-05,
      "loss": 0.0021,
      "step": 82880
    },
    {
      "epoch": 4.4208,
      "grad_norm": 0.028141234070062637,
      "learning_rate": 2.237e-05,
      "loss": 0.0042,
      "step": 82890
    },
    {
      "epoch": 4.421333333333333,
      "grad_norm": 0.1125670075416565,
      "learning_rate": 2.236666666666667e-05,
      "loss": 0.005,
      "step": 82900
    },
    {
      "epoch": 4.421866666666666,
      "grad_norm": 0.05627933889627457,
      "learning_rate": 2.2363333333333333e-05,
      "loss": 0.0022,
      "step": 82910
    },
    {
      "epoch": 4.4224,
      "grad_norm": 0.16885389387607574,
      "learning_rate": 2.236e-05,
      "loss": 0.0024,
      "step": 82920
    },
    {
      "epoch": 4.422933333333333,
      "grad_norm": 4.689115047454834,
      "learning_rate": 2.235666666666667e-05,
      "loss": 0.0025,
      "step": 82930
    },
    {
      "epoch": 4.423466666666666,
      "grad_norm": 0.20665384829044342,
      "learning_rate": 2.2353333333333335e-05,
      "loss": 0.0047,
      "step": 82940
    },
    {
      "epoch": 4.424,
      "grad_norm": 0.02814112789928913,
      "learning_rate": 2.235e-05,
      "loss": 0.0036,
      "step": 82950
    },
    {
      "epoch": 4.424533333333334,
      "grad_norm": 0.16884075105190277,
      "learning_rate": 2.2346666666666667e-05,
      "loss": 0.0029,
      "step": 82960
    },
    {
      "epoch": 4.425066666666667,
      "grad_norm": 0.02814003825187683,
      "learning_rate": 2.2343333333333337e-05,
      "loss": 0.0026,
      "step": 82970
    },
    {
      "epoch": 4.4256,
      "grad_norm": 0.14070336520671844,
      "learning_rate": 2.234e-05,
      "loss": 0.0042,
      "step": 82980
    },
    {
      "epoch": 4.4261333333333335,
      "grad_norm": 0.11256755143404007,
      "learning_rate": 2.2336666666666666e-05,
      "loss": 0.0038,
      "step": 82990
    },
    {
      "epoch": 4.426666666666667,
      "grad_norm": 2.552674960298873e-09,
      "learning_rate": 2.2333333333333335e-05,
      "loss": 0.0024,
      "step": 83000
    },
    {
      "epoch": 4.4272,
      "grad_norm": 0.1969754695892334,
      "learning_rate": 2.233e-05,
      "loss": 0.0039,
      "step": 83010
    },
    {
      "epoch": 4.427733333333333,
      "grad_norm": 0.11255806684494019,
      "learning_rate": 2.2326666666666667e-05,
      "loss": 0.0024,
      "step": 83020
    },
    {
      "epoch": 4.428266666666667,
      "grad_norm": 0.056279443204402924,
      "learning_rate": 2.2323333333333334e-05,
      "loss": 0.0038,
      "step": 83030
    },
    {
      "epoch": 4.4288,
      "grad_norm": 0.2626459300518036,
      "learning_rate": 2.2320000000000003e-05,
      "loss": 0.003,
      "step": 83040
    },
    {
      "epoch": 4.429333333333333,
      "grad_norm": 4.6645802775913126e-09,
      "learning_rate": 2.231666666666667e-05,
      "loss": 0.0028,
      "step": 83050
    },
    {
      "epoch": 4.429866666666666,
      "grad_norm": 0.11255661398172379,
      "learning_rate": 2.2313333333333332e-05,
      "loss": 0.0022,
      "step": 83060
    },
    {
      "epoch": 4.4304,
      "grad_norm": 0.028141964226961136,
      "learning_rate": 2.231e-05,
      "loss": 0.0032,
      "step": 83070
    },
    {
      "epoch": 4.430933333333333,
      "grad_norm": 0.4220716655254364,
      "learning_rate": 2.2306666666666668e-05,
      "loss": 0.0021,
      "step": 83080
    },
    {
      "epoch": 4.431466666666667,
      "grad_norm": 0.1688387542963028,
      "learning_rate": 2.2303333333333334e-05,
      "loss": 0.0024,
      "step": 83090
    },
    {
      "epoch": 4.432,
      "grad_norm": 0.28139984607696533,
      "learning_rate": 2.23e-05,
      "loss": 0.0022,
      "step": 83100
    },
    {
      "epoch": 4.432533333333334,
      "grad_norm": 0.0844147801399231,
      "learning_rate": 2.229666666666667e-05,
      "loss": 0.0033,
      "step": 83110
    },
    {
      "epoch": 4.433066666666667,
      "grad_norm": 0.07588985562324524,
      "learning_rate": 2.2293333333333336e-05,
      "loss": 0.0046,
      "step": 83120
    },
    {
      "epoch": 4.4336,
      "grad_norm": 0.08441770076751709,
      "learning_rate": 2.229e-05,
      "loss": 0.0025,
      "step": 83130
    },
    {
      "epoch": 4.4341333333333335,
      "grad_norm": 0.14070335030555725,
      "learning_rate": 2.2286666666666668e-05,
      "loss": 0.0026,
      "step": 83140
    },
    {
      "epoch": 4.434666666666667,
      "grad_norm": 0.028139781206846237,
      "learning_rate": 2.2283333333333334e-05,
      "loss": 0.0027,
      "step": 83150
    },
    {
      "epoch": 4.4352,
      "grad_norm": 0.11162657290697098,
      "learning_rate": 2.228e-05,
      "loss": 0.0023,
      "step": 83160
    },
    {
      "epoch": 4.435733333333333,
      "grad_norm": 0.11255744844675064,
      "learning_rate": 2.2276666666666666e-05,
      "loss": 0.0026,
      "step": 83170
    },
    {
      "epoch": 4.436266666666667,
      "grad_norm": 0.30951425433158875,
      "learning_rate": 2.2273333333333336e-05,
      "loss": 0.003,
      "step": 83180
    },
    {
      "epoch": 4.4368,
      "grad_norm": 0.33768418431282043,
      "learning_rate": 2.2270000000000002e-05,
      "loss": 0.0019,
      "step": 83190
    },
    {
      "epoch": 4.437333333333333,
      "grad_norm": 0.5627982020378113,
      "learning_rate": 2.2266666666666668e-05,
      "loss": 0.0032,
      "step": 83200
    },
    {
      "epoch": 4.437866666666666,
      "grad_norm": 3.242665469471717e-09,
      "learning_rate": 2.2263333333333334e-05,
      "loss": 0.0039,
      "step": 83210
    },
    {
      "epoch": 4.4384,
      "grad_norm": 0.11255565285682678,
      "learning_rate": 2.226e-05,
      "loss": 0.0021,
      "step": 83220
    },
    {
      "epoch": 4.438933333333333,
      "grad_norm": 0.08441804349422455,
      "learning_rate": 2.2256666666666666e-05,
      "loss": 0.0031,
      "step": 83230
    },
    {
      "epoch": 4.439466666666666,
      "grad_norm": 0.5346282720565796,
      "learning_rate": 2.2253333333333336e-05,
      "loss": 0.0025,
      "step": 83240
    },
    {
      "epoch": 4.44,
      "grad_norm": 0.2532638609409332,
      "learning_rate": 2.2250000000000002e-05,
      "loss": 0.0031,
      "step": 83250
    },
    {
      "epoch": 4.440533333333334,
      "grad_norm": 0.3939618170261383,
      "learning_rate": 2.2246666666666668e-05,
      "loss": 0.0027,
      "step": 83260
    },
    {
      "epoch": 4.441066666666667,
      "grad_norm": 0.11255401372909546,
      "learning_rate": 2.2243333333333334e-05,
      "loss": 0.0022,
      "step": 83270
    },
    {
      "epoch": 4.4416,
      "grad_norm": 0.25326189398765564,
      "learning_rate": 2.224e-05,
      "loss": 0.0033,
      "step": 83280
    },
    {
      "epoch": 4.4421333333333335,
      "grad_norm": 0.19697444140911102,
      "learning_rate": 2.2236666666666667e-05,
      "loss": 0.0039,
      "step": 83290
    },
    {
      "epoch": 4.442666666666667,
      "grad_norm": 0.1969635933637619,
      "learning_rate": 2.2233333333333333e-05,
      "loss": 0.0023,
      "step": 83300
    },
    {
      "epoch": 4.4432,
      "grad_norm": 0.05627790093421936,
      "learning_rate": 2.2230000000000002e-05,
      "loss": 0.0028,
      "step": 83310
    },
    {
      "epoch": 4.443733333333333,
      "grad_norm": 0.2813819348812103,
      "learning_rate": 2.222666666666667e-05,
      "loss": 0.0023,
      "step": 83320
    },
    {
      "epoch": 4.444266666666667,
      "grad_norm": 0.16882570087909698,
      "learning_rate": 2.2223333333333335e-05,
      "loss": 0.0025,
      "step": 83330
    },
    {
      "epoch": 4.4448,
      "grad_norm": 0.8684255480766296,
      "learning_rate": 2.222e-05,
      "loss": 0.0022,
      "step": 83340
    },
    {
      "epoch": 4.445333333333333,
      "grad_norm": 0.5380352735519409,
      "learning_rate": 2.221666666666667e-05,
      "loss": 0.0033,
      "step": 83350
    },
    {
      "epoch": 4.445866666666666,
      "grad_norm": 0.0812707170844078,
      "learning_rate": 2.2213333333333333e-05,
      "loss": 0.0034,
      "step": 83360
    },
    {
      "epoch": 4.4464,
      "grad_norm": 0.14069277048110962,
      "learning_rate": 2.221e-05,
      "loss": 0.0024,
      "step": 83370
    },
    {
      "epoch": 4.446933333333333,
      "grad_norm": 0.1688261479139328,
      "learning_rate": 2.220666666666667e-05,
      "loss": 0.0026,
      "step": 83380
    },
    {
      "epoch": 4.447466666666667,
      "grad_norm": 0.22510254383087158,
      "learning_rate": 2.2203333333333335e-05,
      "loss": 0.0026,
      "step": 83390
    },
    {
      "epoch": 4.448,
      "grad_norm": 0.11255491524934769,
      "learning_rate": 2.22e-05,
      "loss": 0.002,
      "step": 83400
    },
    {
      "epoch": 4.448533333333334,
      "grad_norm": 0.14069169759750366,
      "learning_rate": 2.2196666666666667e-05,
      "loss": 0.0033,
      "step": 83410
    },
    {
      "epoch": 4.449066666666667,
      "grad_norm": 0.14068983495235443,
      "learning_rate": 2.2193333333333337e-05,
      "loss": 0.0026,
      "step": 83420
    },
    {
      "epoch": 4.4496,
      "grad_norm": 0.028137514367699623,
      "learning_rate": 2.219e-05,
      "loss": 0.0028,
      "step": 83430
    },
    {
      "epoch": 4.4501333333333335,
      "grad_norm": 0.14069585502147675,
      "learning_rate": 2.2186666666666665e-05,
      "loss": 0.0029,
      "step": 83440
    },
    {
      "epoch": 4.450666666666667,
      "grad_norm": 0.1125577911734581,
      "learning_rate": 2.2183333333333335e-05,
      "loss": 0.0042,
      "step": 83450
    },
    {
      "epoch": 4.4512,
      "grad_norm": 0.25325003266334534,
      "learning_rate": 2.218e-05,
      "loss": 0.0021,
      "step": 83460
    },
    {
      "epoch": 4.451733333333333,
      "grad_norm": 0.08441229164600372,
      "learning_rate": 2.2176666666666667e-05,
      "loss": 0.0033,
      "step": 83470
    },
    {
      "epoch": 4.452266666666667,
      "grad_norm": 0.11255500465631485,
      "learning_rate": 2.2173333333333333e-05,
      "loss": 0.0027,
      "step": 83480
    },
    {
      "epoch": 4.4528,
      "grad_norm": 0.1688215583562851,
      "learning_rate": 2.2170000000000003e-05,
      "loss": 0.0024,
      "step": 83490
    },
    {
      "epoch": 4.453333333333333,
      "grad_norm": 0.05627725273370743,
      "learning_rate": 2.216666666666667e-05,
      "loss": 0.0035,
      "step": 83500
    },
    {
      "epoch": 4.453866666666666,
      "grad_norm": 0.1683933436870575,
      "learning_rate": 2.2163333333333332e-05,
      "loss": 0.0036,
      "step": 83510
    },
    {
      "epoch": 4.4544,
      "grad_norm": 0.08441604673862457,
      "learning_rate": 2.216e-05,
      "loss": 0.0032,
      "step": 83520
    },
    {
      "epoch": 4.454933333333333,
      "grad_norm": 0.5277286171913147,
      "learning_rate": 2.2156666666666667e-05,
      "loss": 0.0041,
      "step": 83530
    },
    {
      "epoch": 4.455466666666666,
      "grad_norm": 0.12205041199922562,
      "learning_rate": 2.2153333333333334e-05,
      "loss": 0.005,
      "step": 83540
    },
    {
      "epoch": 4.456,
      "grad_norm": 0.25322824716567993,
      "learning_rate": 2.215e-05,
      "loss": 0.0032,
      "step": 83550
    },
    {
      "epoch": 4.456533333333334,
      "grad_norm": 0.4783889949321747,
      "learning_rate": 2.214666666666667e-05,
      "loss": 0.0035,
      "step": 83560
    },
    {
      "epoch": 4.457066666666667,
      "grad_norm": 0.19696639478206635,
      "learning_rate": 2.2143333333333335e-05,
      "loss": 0.0041,
      "step": 83570
    },
    {
      "epoch": 4.4576,
      "grad_norm": 0.1688256710767746,
      "learning_rate": 2.214e-05,
      "loss": 0.0029,
      "step": 83580
    },
    {
      "epoch": 4.4581333333333335,
      "grad_norm": 0.08441215008497238,
      "learning_rate": 2.2136666666666668e-05,
      "loss": 0.0029,
      "step": 83590
    },
    {
      "epoch": 4.458666666666667,
      "grad_norm": 0.028137298300862312,
      "learning_rate": 2.2133333333333334e-05,
      "loss": 0.0026,
      "step": 83600
    },
    {
      "epoch": 4.4592,
      "grad_norm": 0.2250930219888687,
      "learning_rate": 2.213e-05,
      "loss": 0.0022,
      "step": 83610
    },
    {
      "epoch": 4.459733333333333,
      "grad_norm": 0.05627310276031494,
      "learning_rate": 2.212666666666667e-05,
      "loss": 0.0033,
      "step": 83620
    },
    {
      "epoch": 4.460266666666667,
      "grad_norm": 0.056273508816957474,
      "learning_rate": 2.2123333333333336e-05,
      "loss": 0.0043,
      "step": 83630
    },
    {
      "epoch": 4.4608,
      "grad_norm": 2.6742206227226006e-09,
      "learning_rate": 2.212e-05,
      "loss": 0.0017,
      "step": 83640
    },
    {
      "epoch": 4.461333333333333,
      "grad_norm": 0.2532253861427307,
      "learning_rate": 2.2116666666666668e-05,
      "loss": 0.0022,
      "step": 83650
    },
    {
      "epoch": 4.461866666666666,
      "grad_norm": 0.36580872535705566,
      "learning_rate": 2.2113333333333334e-05,
      "loss": 0.0022,
      "step": 83660
    },
    {
      "epoch": 4.4624,
      "grad_norm": 0.08441172540187836,
      "learning_rate": 2.211e-05,
      "loss": 0.0024,
      "step": 83670
    },
    {
      "epoch": 4.462933333333333,
      "grad_norm": 0.14068220555782318,
      "learning_rate": 2.2106666666666666e-05,
      "loss": 0.0051,
      "step": 83680
    },
    {
      "epoch": 4.463466666666667,
      "grad_norm": 0.1688220202922821,
      "learning_rate": 2.2103333333333336e-05,
      "loss": 0.0038,
      "step": 83690
    },
    {
      "epoch": 4.464,
      "grad_norm": 0.33764293789863586,
      "learning_rate": 2.2100000000000002e-05,
      "loss": 0.0032,
      "step": 83700
    },
    {
      "epoch": 4.464533333333334,
      "grad_norm": 0.11254855990409851,
      "learning_rate": 2.2096666666666668e-05,
      "loss": 0.0025,
      "step": 83710
    },
    {
      "epoch": 4.465066666666667,
      "grad_norm": 0.4783169627189636,
      "learning_rate": 2.2093333333333334e-05,
      "loss": 0.002,
      "step": 83720
    },
    {
      "epoch": 4.4656,
      "grad_norm": 0.36577463150024414,
      "learning_rate": 2.2090000000000004e-05,
      "loss": 0.0021,
      "step": 83730
    },
    {
      "epoch": 4.4661333333333335,
      "grad_norm": 0.1125517338514328,
      "learning_rate": 2.2086666666666666e-05,
      "loss": 0.0023,
      "step": 83740
    },
    {
      "epoch": 4.466666666666667,
      "grad_norm": 0.05627620220184326,
      "learning_rate": 2.2083333333333333e-05,
      "loss": 0.0035,
      "step": 83750
    },
    {
      "epoch": 4.4672,
      "grad_norm": 0.14068104326725006,
      "learning_rate": 2.2080000000000002e-05,
      "loss": 0.0033,
      "step": 83760
    },
    {
      "epoch": 4.467733333333333,
      "grad_norm": 0.16882359981536865,
      "learning_rate": 2.2076666666666668e-05,
      "loss": 0.0035,
      "step": 83770
    },
    {
      "epoch": 4.468266666666667,
      "grad_norm": 0.236053466796875,
      "learning_rate": 2.2073333333333334e-05,
      "loss": 0.0026,
      "step": 83780
    },
    {
      "epoch": 4.4688,
      "grad_norm": 0.1406867951154709,
      "learning_rate": 2.207e-05,
      "loss": 0.0022,
      "step": 83790
    },
    {
      "epoch": 4.469333333333333,
      "grad_norm": 0.16882748901844025,
      "learning_rate": 2.206666666666667e-05,
      "loss": 0.0029,
      "step": 83800
    },
    {
      "epoch": 4.469866666666666,
      "grad_norm": 0.36577108502388,
      "learning_rate": 2.2063333333333333e-05,
      "loss": 0.002,
      "step": 83810
    },
    {
      "epoch": 4.4704,
      "grad_norm": 0.1688212901353836,
      "learning_rate": 2.206e-05,
      "loss": 0.0026,
      "step": 83820
    },
    {
      "epoch": 4.470933333333333,
      "grad_norm": 0.08440964668989182,
      "learning_rate": 2.205666666666667e-05,
      "loss": 0.0034,
      "step": 83830
    },
    {
      "epoch": 4.471466666666666,
      "grad_norm": 0.2250911295413971,
      "learning_rate": 2.2053333333333335e-05,
      "loss": 0.0036,
      "step": 83840
    },
    {
      "epoch": 4.4719999999999995,
      "grad_norm": 0.30950915813446045,
      "learning_rate": 2.205e-05,
      "loss": 0.0041,
      "step": 83850
    },
    {
      "epoch": 4.472533333333334,
      "grad_norm": 0.14068204164505005,
      "learning_rate": 2.2046666666666667e-05,
      "loss": 0.002,
      "step": 83860
    },
    {
      "epoch": 4.473066666666667,
      "grad_norm": 0.028137505054473877,
      "learning_rate": 2.2043333333333336e-05,
      "loss": 0.0024,
      "step": 83870
    },
    {
      "epoch": 4.4736,
      "grad_norm": 0.1125447154045105,
      "learning_rate": 2.2040000000000002e-05,
      "loss": 0.0031,
      "step": 83880
    },
    {
      "epoch": 4.4741333333333335,
      "grad_norm": 0.2813844680786133,
      "learning_rate": 2.2036666666666665e-05,
      "loss": 0.0043,
      "step": 83890
    },
    {
      "epoch": 4.474666666666667,
      "grad_norm": 0.028137311339378357,
      "learning_rate": 2.2033333333333335e-05,
      "loss": 0.0028,
      "step": 83900
    },
    {
      "epoch": 4.4752,
      "grad_norm": 0.1125403493642807,
      "learning_rate": 2.203e-05,
      "loss": 0.0028,
      "step": 83910
    },
    {
      "epoch": 4.475733333333333,
      "grad_norm": 0.25323230028152466,
      "learning_rate": 2.2026666666666667e-05,
      "loss": 0.0022,
      "step": 83920
    },
    {
      "epoch": 4.476266666666667,
      "grad_norm": 0.14068137109279633,
      "learning_rate": 2.2023333333333333e-05,
      "loss": 0.002,
      "step": 83930
    },
    {
      "epoch": 4.4768,
      "grad_norm": 0.14068414270877838,
      "learning_rate": 2.2020000000000003e-05,
      "loss": 0.0033,
      "step": 83940
    },
    {
      "epoch": 4.477333333333333,
      "grad_norm": 0.08440826088190079,
      "learning_rate": 2.201666666666667e-05,
      "loss": 0.0019,
      "step": 83950
    },
    {
      "epoch": 4.477866666666666,
      "grad_norm": 0.0844128355383873,
      "learning_rate": 2.201333333333333e-05,
      "loss": 0.0033,
      "step": 83960
    },
    {
      "epoch": 4.4784,
      "grad_norm": 0.4220482409000397,
      "learning_rate": 2.201e-05,
      "loss": 0.0022,
      "step": 83970
    },
    {
      "epoch": 4.478933333333333,
      "grad_norm": 0.056273240596055984,
      "learning_rate": 2.2006666666666667e-05,
      "loss": 0.0033,
      "step": 83980
    },
    {
      "epoch": 4.479466666666666,
      "grad_norm": 0.11266496032476425,
      "learning_rate": 2.2003333333333333e-05,
      "loss": 0.004,
      "step": 83990
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.7315049767494202,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.003,
      "step": 84000
    },
    {
      "epoch": 4.480533333333334,
      "grad_norm": 0.056273698806762695,
      "learning_rate": 2.199666666666667e-05,
      "loss": 0.003,
      "step": 84010
    },
    {
      "epoch": 4.481066666666667,
      "grad_norm": 0.6189756393432617,
      "learning_rate": 2.1993333333333335e-05,
      "loss": 0.0018,
      "step": 84020
    },
    {
      "epoch": 4.4816,
      "grad_norm": 0.0562722273170948,
      "learning_rate": 2.199e-05,
      "loss": 0.0038,
      "step": 84030
    },
    {
      "epoch": 4.4821333333333335,
      "grad_norm": 0.14067701995372772,
      "learning_rate": 2.1986666666666667e-05,
      "loss": 0.0026,
      "step": 84040
    },
    {
      "epoch": 4.482666666666667,
      "grad_norm": 0.2532367706298828,
      "learning_rate": 2.1983333333333333e-05,
      "loss": 0.0022,
      "step": 84050
    },
    {
      "epoch": 4.4832,
      "grad_norm": 0.14067597687244415,
      "learning_rate": 2.198e-05,
      "loss": 0.0041,
      "step": 84060
    },
    {
      "epoch": 4.483733333333333,
      "grad_norm": 0.3657551407814026,
      "learning_rate": 2.197666666666667e-05,
      "loss": 0.0019,
      "step": 84070
    },
    {
      "epoch": 4.484266666666667,
      "grad_norm": 0.11254604905843735,
      "learning_rate": 2.1973333333333335e-05,
      "loss": 0.0034,
      "step": 84080
    },
    {
      "epoch": 4.4848,
      "grad_norm": 0.22508065402507782,
      "learning_rate": 2.197e-05,
      "loss": 0.0029,
      "step": 84090
    },
    {
      "epoch": 4.485333333333333,
      "grad_norm": 0.02813624031841755,
      "learning_rate": 2.1966666666666668e-05,
      "loss": 0.0031,
      "step": 84100
    },
    {
      "epoch": 4.4858666666666664,
      "grad_norm": 0.08441197872161865,
      "learning_rate": 2.1963333333333337e-05,
      "loss": 0.0023,
      "step": 84110
    },
    {
      "epoch": 4.4864,
      "grad_norm": 0.1406809538602829,
      "learning_rate": 2.196e-05,
      "loss": 0.0032,
      "step": 84120
    },
    {
      "epoch": 4.486933333333333,
      "grad_norm": 3.2754816636781925e-09,
      "learning_rate": 2.1956666666666666e-05,
      "loss": 0.0033,
      "step": 84130
    },
    {
      "epoch": 4.487466666666666,
      "grad_norm": 0.5345562100410461,
      "learning_rate": 2.1953333333333335e-05,
      "loss": 0.0029,
      "step": 84140
    },
    {
      "epoch": 4.4879999999999995,
      "grad_norm": 0.16882511973381042,
      "learning_rate": 2.195e-05,
      "loss": 0.0022,
      "step": 84150
    },
    {
      "epoch": 4.488533333333334,
      "grad_norm": 0.2532022297382355,
      "learning_rate": 2.1946666666666668e-05,
      "loss": 0.0024,
      "step": 84160
    },
    {
      "epoch": 4.489066666666667,
      "grad_norm": 0.14068226516246796,
      "learning_rate": 2.1943333333333334e-05,
      "loss": 0.0027,
      "step": 84170
    },
    {
      "epoch": 4.4896,
      "grad_norm": 0.11254134774208069,
      "learning_rate": 2.1940000000000003e-05,
      "loss": 0.0029,
      "step": 84180
    },
    {
      "epoch": 4.4901333333333335,
      "grad_norm": 0.08440491557121277,
      "learning_rate": 2.1936666666666666e-05,
      "loss": 0.0036,
      "step": 84190
    },
    {
      "epoch": 4.490666666666667,
      "grad_norm": 0.05626939609646797,
      "learning_rate": 2.1933333333333332e-05,
      "loss": 0.0028,
      "step": 84200
    },
    {
      "epoch": 4.4912,
      "grad_norm": 0.05626997724175453,
      "learning_rate": 2.1930000000000002e-05,
      "loss": 0.0028,
      "step": 84210
    },
    {
      "epoch": 4.491733333333333,
      "grad_norm": 0.08440759032964706,
      "learning_rate": 2.1926666666666668e-05,
      "loss": 0.0018,
      "step": 84220
    },
    {
      "epoch": 4.492266666666667,
      "grad_norm": 0.05626831576228142,
      "learning_rate": 2.1923333333333334e-05,
      "loss": 0.0025,
      "step": 84230
    },
    {
      "epoch": 4.4928,
      "grad_norm": 0.2532027065753937,
      "learning_rate": 2.192e-05,
      "loss": 0.0023,
      "step": 84240
    },
    {
      "epoch": 4.493333333333333,
      "grad_norm": 0.1406758427619934,
      "learning_rate": 2.191666666666667e-05,
      "loss": 0.0027,
      "step": 84250
    },
    {
      "epoch": 4.4938666666666665,
      "grad_norm": 0.19693735241889954,
      "learning_rate": 2.1913333333333336e-05,
      "loss": 0.004,
      "step": 84260
    },
    {
      "epoch": 4.4944,
      "grad_norm": 0.08440268039703369,
      "learning_rate": 2.191e-05,
      "loss": 0.0028,
      "step": 84270
    },
    {
      "epoch": 4.494933333333333,
      "grad_norm": 0.11253593862056732,
      "learning_rate": 2.1906666666666668e-05,
      "loss": 0.003,
      "step": 84280
    },
    {
      "epoch": 4.495466666666666,
      "grad_norm": 0.05626983940601349,
      "learning_rate": 2.1903333333333334e-05,
      "loss": 0.0028,
      "step": 84290
    },
    {
      "epoch": 4.496,
      "grad_norm": 0.08440297842025757,
      "learning_rate": 2.19e-05,
      "loss": 0.0025,
      "step": 84300
    },
    {
      "epoch": 4.496533333333334,
      "grad_norm": 0.02813560701906681,
      "learning_rate": 2.1896666666666667e-05,
      "loss": 0.002,
      "step": 84310
    },
    {
      "epoch": 4.497066666666667,
      "grad_norm": 0.0281353909522295,
      "learning_rate": 2.1893333333333336e-05,
      "loss": 0.0027,
      "step": 84320
    },
    {
      "epoch": 4.4976,
      "grad_norm": 0.0844002515077591,
      "learning_rate": 2.1890000000000002e-05,
      "loss": 0.0024,
      "step": 84330
    },
    {
      "epoch": 4.4981333333333335,
      "grad_norm": 0.16881108283996582,
      "learning_rate": 2.1886666666666665e-05,
      "loss": 0.0036,
      "step": 84340
    },
    {
      "epoch": 4.498666666666667,
      "grad_norm": 0.1969510167837143,
      "learning_rate": 2.1883333333333334e-05,
      "loss": 0.0035,
      "step": 84350
    },
    {
      "epoch": 4.4992,
      "grad_norm": 0.056267499923706055,
      "learning_rate": 2.188e-05,
      "loss": 0.0015,
      "step": 84360
    },
    {
      "epoch": 4.499733333333333,
      "grad_norm": 0.2813270390033722,
      "learning_rate": 2.1876666666666667e-05,
      "loss": 0.0027,
      "step": 84370
    },
    {
      "epoch": 4.500266666666667,
      "grad_norm": 0.22507794201374054,
      "learning_rate": 2.1873333333333336e-05,
      "loss": 0.0026,
      "step": 84380
    },
    {
      "epoch": 4.5008,
      "grad_norm": 0.084406279027462,
      "learning_rate": 2.1870000000000002e-05,
      "loss": 0.0043,
      "step": 84390
    },
    {
      "epoch": 4.501333333333333,
      "grad_norm": 0.3375893235206604,
      "learning_rate": 2.186666666666667e-05,
      "loss": 0.0028,
      "step": 84400
    },
    {
      "epoch": 4.5018666666666665,
      "grad_norm": 0.05627182871103287,
      "learning_rate": 2.1863333333333335e-05,
      "loss": 0.0024,
      "step": 84410
    },
    {
      "epoch": 4.5024,
      "grad_norm": 0.11253416538238525,
      "learning_rate": 2.186e-05,
      "loss": 0.0037,
      "step": 84420
    },
    {
      "epoch": 4.502933333333333,
      "grad_norm": 0.05626530945301056,
      "learning_rate": 2.1856666666666667e-05,
      "loss": 0.0028,
      "step": 84430
    },
    {
      "epoch": 4.503466666666666,
      "grad_norm": 0.11253178119659424,
      "learning_rate": 2.1853333333333333e-05,
      "loss": 0.0029,
      "step": 84440
    },
    {
      "epoch": 4.504,
      "grad_norm": 0.1406763643026352,
      "learning_rate": 2.1850000000000003e-05,
      "loss": 0.0037,
      "step": 84450
    },
    {
      "epoch": 4.504533333333333,
      "grad_norm": 0.05626656860113144,
      "learning_rate": 2.184666666666667e-05,
      "loss": 0.0033,
      "step": 84460
    },
    {
      "epoch": 4.505066666666667,
      "grad_norm": 0.056265246123075485,
      "learning_rate": 2.1843333333333335e-05,
      "loss": 0.0026,
      "step": 84470
    },
    {
      "epoch": 4.5056,
      "grad_norm": 0.11252837628126144,
      "learning_rate": 2.184e-05,
      "loss": 0.0017,
      "step": 84480
    },
    {
      "epoch": 4.5061333333333335,
      "grad_norm": 0.05626869574189186,
      "learning_rate": 2.1836666666666667e-05,
      "loss": 0.0043,
      "step": 84490
    },
    {
      "epoch": 4.506666666666667,
      "grad_norm": 0.11253689974546432,
      "learning_rate": 2.1833333333333333e-05,
      "loss": 0.0018,
      "step": 84500
    },
    {
      "epoch": 4.5072,
      "grad_norm": 0.3376118242740631,
      "learning_rate": 2.183e-05,
      "loss": 0.0031,
      "step": 84510
    },
    {
      "epoch": 4.507733333333333,
      "grad_norm": 0.028134092688560486,
      "learning_rate": 2.182666666666667e-05,
      "loss": 0.003,
      "step": 84520
    },
    {
      "epoch": 4.508266666666667,
      "grad_norm": 0.0562654584646225,
      "learning_rate": 2.1823333333333335e-05,
      "loss": 0.0031,
      "step": 84530
    },
    {
      "epoch": 4.5088,
      "grad_norm": 0.056266289204359055,
      "learning_rate": 2.182e-05,
      "loss": 0.0035,
      "step": 84540
    },
    {
      "epoch": 4.509333333333333,
      "grad_norm": 0.14066651463508606,
      "learning_rate": 2.1816666666666667e-05,
      "loss": 0.003,
      "step": 84550
    },
    {
      "epoch": 4.5098666666666665,
      "grad_norm": 0.1688080132007599,
      "learning_rate": 2.1813333333333337e-05,
      "loss": 0.0033,
      "step": 84560
    },
    {
      "epoch": 4.5104,
      "grad_norm": 0.02813291922211647,
      "learning_rate": 2.181e-05,
      "loss": 0.0024,
      "step": 84570
    },
    {
      "epoch": 4.510933333333333,
      "grad_norm": 0.30945226550102234,
      "learning_rate": 2.1806666666666666e-05,
      "loss": 0.0029,
      "step": 84580
    },
    {
      "epoch": 4.511466666666666,
      "grad_norm": 0.39386996626853943,
      "learning_rate": 2.1803333333333335e-05,
      "loss": 0.0017,
      "step": 84590
    },
    {
      "epoch": 4.5120000000000005,
      "grad_norm": 0.02813262678682804,
      "learning_rate": 2.18e-05,
      "loss": 0.0017,
      "step": 84600
    },
    {
      "epoch": 4.512533333333334,
      "grad_norm": 0.19693025946617126,
      "learning_rate": 2.1796666666666667e-05,
      "loss": 0.0045,
      "step": 84610
    },
    {
      "epoch": 4.513066666666667,
      "grad_norm": 0.05626785010099411,
      "learning_rate": 2.1793333333333334e-05,
      "loss": 0.0025,
      "step": 84620
    },
    {
      "epoch": 4.5136,
      "grad_norm": 0.14066101610660553,
      "learning_rate": 2.1790000000000003e-05,
      "loss": 0.0034,
      "step": 84630
    },
    {
      "epoch": 4.5141333333333336,
      "grad_norm": 0.056267157196998596,
      "learning_rate": 2.1786666666666666e-05,
      "loss": 0.0022,
      "step": 84640
    },
    {
      "epoch": 4.514666666666667,
      "grad_norm": 0.08439971506595612,
      "learning_rate": 2.1783333333333332e-05,
      "loss": 0.0022,
      "step": 84650
    },
    {
      "epoch": 4.5152,
      "grad_norm": 0.39385414123535156,
      "learning_rate": 2.178e-05,
      "loss": 0.0026,
      "step": 84660
    },
    {
      "epoch": 4.515733333333333,
      "grad_norm": 0.056264881044626236,
      "learning_rate": 2.1776666666666668e-05,
      "loss": 0.0023,
      "step": 84670
    },
    {
      "epoch": 4.516266666666667,
      "grad_norm": 0.2813098728656769,
      "learning_rate": 2.1773333333333334e-05,
      "loss": 0.0028,
      "step": 84680
    },
    {
      "epoch": 4.5168,
      "grad_norm": 0.08439937978982925,
      "learning_rate": 2.177e-05,
      "loss": 0.0027,
      "step": 84690
    },
    {
      "epoch": 4.517333333333333,
      "grad_norm": 0.08439622819423676,
      "learning_rate": 2.176666666666667e-05,
      "loss": 0.0037,
      "step": 84700
    },
    {
      "epoch": 4.5178666666666665,
      "grad_norm": 0.22504882514476776,
      "learning_rate": 2.1763333333333336e-05,
      "loss": 0.0025,
      "step": 84710
    },
    {
      "epoch": 4.5184,
      "grad_norm": 0.02813287079334259,
      "learning_rate": 2.176e-05,
      "loss": 0.0018,
      "step": 84720
    },
    {
      "epoch": 4.518933333333333,
      "grad_norm": 0.16878879070281982,
      "learning_rate": 2.1756666666666668e-05,
      "loss": 0.0026,
      "step": 84730
    },
    {
      "epoch": 4.519466666666666,
      "grad_norm": 0.056264203041791916,
      "learning_rate": 2.1753333333333334e-05,
      "loss": 0.0025,
      "step": 84740
    },
    {
      "epoch": 4.52,
      "grad_norm": 0.08439546823501587,
      "learning_rate": 2.175e-05,
      "loss": 0.0021,
      "step": 84750
    },
    {
      "epoch": 4.520533333333333,
      "grad_norm": 0.028131557628512383,
      "learning_rate": 2.174666666666667e-05,
      "loss": 0.0025,
      "step": 84760
    },
    {
      "epoch": 4.521066666666667,
      "grad_norm": 0.3094608783721924,
      "learning_rate": 2.1743333333333336e-05,
      "loss": 0.0023,
      "step": 84770
    },
    {
      "epoch": 4.5216,
      "grad_norm": 0.11252784729003906,
      "learning_rate": 2.1740000000000002e-05,
      "loss": 0.0023,
      "step": 84780
    },
    {
      "epoch": 4.522133333333334,
      "grad_norm": 0.25318706035614014,
      "learning_rate": 2.1736666666666668e-05,
      "loss": 0.0029,
      "step": 84790
    },
    {
      "epoch": 4.522666666666667,
      "grad_norm": 0.19692181050777435,
      "learning_rate": 2.1733333333333334e-05,
      "loss": 0.0033,
      "step": 84800
    },
    {
      "epoch": 4.5232,
      "grad_norm": 0.19691820442676544,
      "learning_rate": 2.173e-05,
      "loss": 0.0035,
      "step": 84810
    },
    {
      "epoch": 4.523733333333333,
      "grad_norm": 0.22505968809127808,
      "learning_rate": 2.1726666666666666e-05,
      "loss": 0.0019,
      "step": 84820
    },
    {
      "epoch": 4.524266666666667,
      "grad_norm": 0.16879260540008545,
      "learning_rate": 2.1723333333333336e-05,
      "loss": 0.0038,
      "step": 84830
    },
    {
      "epoch": 4.5248,
      "grad_norm": 0.22505585849285126,
      "learning_rate": 2.1720000000000002e-05,
      "loss": 0.0027,
      "step": 84840
    },
    {
      "epoch": 4.525333333333333,
      "grad_norm": 0.08439555019140244,
      "learning_rate": 2.1716666666666668e-05,
      "loss": 0.0017,
      "step": 84850
    },
    {
      "epoch": 4.5258666666666665,
      "grad_norm": 0.3094695806503296,
      "learning_rate": 2.1713333333333334e-05,
      "loss": 0.0025,
      "step": 84860
    },
    {
      "epoch": 4.5264,
      "grad_norm": 0.4219602346420288,
      "learning_rate": 2.171e-05,
      "loss": 0.0026,
      "step": 84870
    },
    {
      "epoch": 4.526933333333333,
      "grad_norm": 0.2532051205635071,
      "learning_rate": 2.1706666666666667e-05,
      "loss": 0.0031,
      "step": 84880
    },
    {
      "epoch": 4.527466666666666,
      "grad_norm": 0.11252779513597488,
      "learning_rate": 2.1703333333333333e-05,
      "loss": 0.0014,
      "step": 84890
    },
    {
      "epoch": 4.5280000000000005,
      "grad_norm": 0.028131183236837387,
      "learning_rate": 2.1700000000000002e-05,
      "loss": 0.003,
      "step": 84900
    },
    {
      "epoch": 4.528533333333334,
      "grad_norm": 0.196921244263649,
      "learning_rate": 2.169666666666667e-05,
      "loss": 0.0023,
      "step": 84910
    },
    {
      "epoch": 4.529066666666667,
      "grad_norm": 0.450125515460968,
      "learning_rate": 2.1693333333333335e-05,
      "loss": 0.0023,
      "step": 84920
    },
    {
      "epoch": 4.5296,
      "grad_norm": 0.47821301221847534,
      "learning_rate": 2.169e-05,
      "loss": 0.0023,
      "step": 84930
    },
    {
      "epoch": 4.530133333333334,
      "grad_norm": 0.3657149076461792,
      "learning_rate": 2.168666666666667e-05,
      "loss": 0.0031,
      "step": 84940
    },
    {
      "epoch": 4.530666666666667,
      "grad_norm": 0.22504927217960358,
      "learning_rate": 2.1683333333333333e-05,
      "loss": 0.0031,
      "step": 84950
    },
    {
      "epoch": 4.5312,
      "grad_norm": 0.08439486473798752,
      "learning_rate": 2.168e-05,
      "loss": 0.0029,
      "step": 84960
    },
    {
      "epoch": 4.531733333333333,
      "grad_norm": 0.028130490332841873,
      "learning_rate": 2.167666666666667e-05,
      "loss": 0.0023,
      "step": 84970
    },
    {
      "epoch": 4.532266666666667,
      "grad_norm": 0.05626245588064194,
      "learning_rate": 2.1673333333333335e-05,
      "loss": 0.0024,
      "step": 84980
    },
    {
      "epoch": 4.5328,
      "grad_norm": 0.36570316553115845,
      "learning_rate": 2.167e-05,
      "loss": 0.0022,
      "step": 84990
    },
    {
      "epoch": 4.533333333333333,
      "grad_norm": 0.1969192624092102,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 0.0036,
      "step": 85000
    },
    {
      "epoch": 4.5338666666666665,
      "grad_norm": 0.3938460350036621,
      "learning_rate": 2.1663333333333337e-05,
      "loss": 0.0028,
      "step": 85010
    },
    {
      "epoch": 4.5344,
      "grad_norm": 0.05626072734594345,
      "learning_rate": 2.166e-05,
      "loss": 0.0023,
      "step": 85020
    },
    {
      "epoch": 4.534933333333333,
      "grad_norm": 0.28132274746894836,
      "learning_rate": 2.1656666666666665e-05,
      "loss": 0.0027,
      "step": 85030
    },
    {
      "epoch": 4.535466666666666,
      "grad_norm": 0.4782260060310364,
      "learning_rate": 2.1653333333333335e-05,
      "loss": 0.0034,
      "step": 85040
    },
    {
      "epoch": 4.536,
      "grad_norm": 0.25318893790245056,
      "learning_rate": 2.165e-05,
      "loss": 0.0031,
      "step": 85050
    },
    {
      "epoch": 4.536533333333333,
      "grad_norm": 0.1406562477350235,
      "learning_rate": 2.1646666666666667e-05,
      "loss": 0.0021,
      "step": 85060
    },
    {
      "epoch": 4.537066666666667,
      "grad_norm": 0.16878250241279602,
      "learning_rate": 2.1643333333333333e-05,
      "loss": 0.0025,
      "step": 85070
    },
    {
      "epoch": 4.5376,
      "grad_norm": 0.08439473807811737,
      "learning_rate": 2.1640000000000003e-05,
      "loss": 0.0019,
      "step": 85080
    },
    {
      "epoch": 4.538133333333334,
      "grad_norm": 0.25316354632377625,
      "learning_rate": 2.163666666666667e-05,
      "loss": 0.0015,
      "step": 85090
    },
    {
      "epoch": 4.538666666666667,
      "grad_norm": 0.1687927544116974,
      "learning_rate": 2.1633333333333332e-05,
      "loss": 0.0036,
      "step": 85100
    },
    {
      "epoch": 4.5392,
      "grad_norm": 0.05626164749264717,
      "learning_rate": 2.163e-05,
      "loss": 0.002,
      "step": 85110
    },
    {
      "epoch": 4.539733333333333,
      "grad_norm": 0.11252370476722717,
      "learning_rate": 2.1626666666666667e-05,
      "loss": 0.0024,
      "step": 85120
    },
    {
      "epoch": 4.540266666666667,
      "grad_norm": 0.028130661696195602,
      "learning_rate": 2.1623333333333334e-05,
      "loss": 0.0024,
      "step": 85130
    },
    {
      "epoch": 4.5408,
      "grad_norm": 4.751753990284158e-10,
      "learning_rate": 2.162e-05,
      "loss": 0.0019,
      "step": 85140
    },
    {
      "epoch": 4.541333333333333,
      "grad_norm": 0.16877779364585876,
      "learning_rate": 2.161666666666667e-05,
      "loss": 0.0026,
      "step": 85150
    },
    {
      "epoch": 4.5418666666666665,
      "grad_norm": 0.1687859445810318,
      "learning_rate": 2.1613333333333335e-05,
      "loss": 0.0033,
      "step": 85160
    },
    {
      "epoch": 4.5424,
      "grad_norm": 0.11252780258655548,
      "learning_rate": 2.1609999999999998e-05,
      "loss": 0.0021,
      "step": 85170
    },
    {
      "epoch": 4.542933333333333,
      "grad_norm": 0.0281301811337471,
      "learning_rate": 2.1606666666666668e-05,
      "loss": 0.004,
      "step": 85180
    },
    {
      "epoch": 4.543466666666666,
      "grad_norm": 2.897233342125105e-09,
      "learning_rate": 2.1603333333333334e-05,
      "loss": 0.0037,
      "step": 85190
    },
    {
      "epoch": 4.5440000000000005,
      "grad_norm": 0.2531849145889282,
      "learning_rate": 2.16e-05,
      "loss": 0.0034,
      "step": 85200
    },
    {
      "epoch": 4.544533333333334,
      "grad_norm": 0.16877464950084686,
      "learning_rate": 2.159666666666667e-05,
      "loss": 0.0016,
      "step": 85210
    },
    {
      "epoch": 4.545066666666667,
      "grad_norm": 0.08439359813928604,
      "learning_rate": 2.1593333333333336e-05,
      "loss": 0.002,
      "step": 85220
    },
    {
      "epoch": 4.5456,
      "grad_norm": 0.08438912779092789,
      "learning_rate": 2.159e-05,
      "loss": 0.002,
      "step": 85230
    },
    {
      "epoch": 4.546133333333334,
      "grad_norm": 0.36569640040397644,
      "learning_rate": 2.1586666666666668e-05,
      "loss": 0.0026,
      "step": 85240
    },
    {
      "epoch": 4.546666666666667,
      "grad_norm": 0.28131136298179626,
      "learning_rate": 2.1583333333333334e-05,
      "loss": 0.0029,
      "step": 85250
    },
    {
      "epoch": 4.5472,
      "grad_norm": 0.16877497732639313,
      "learning_rate": 2.158e-05,
      "loss": 0.0027,
      "step": 85260
    },
    {
      "epoch": 4.547733333333333,
      "grad_norm": 0.028130142018198967,
      "learning_rate": 2.1576666666666666e-05,
      "loss": 0.0024,
      "step": 85270
    },
    {
      "epoch": 4.548266666666667,
      "grad_norm": 0.0562596321105957,
      "learning_rate": 2.1573333333333336e-05,
      "loss": 0.002,
      "step": 85280
    },
    {
      "epoch": 4.5488,
      "grad_norm": 0.0562596432864666,
      "learning_rate": 2.1570000000000002e-05,
      "loss": 0.0043,
      "step": 85290
    },
    {
      "epoch": 4.549333333333333,
      "grad_norm": 0.056257136166095734,
      "learning_rate": 2.1566666666666668e-05,
      "loss": 0.0029,
      "step": 85300
    },
    {
      "epoch": 4.5498666666666665,
      "grad_norm": 7.599185281037535e-09,
      "learning_rate": 2.1563333333333334e-05,
      "loss": 0.0027,
      "step": 85310
    },
    {
      "epoch": 4.5504,
      "grad_norm": 0.14065170288085938,
      "learning_rate": 2.1560000000000004e-05,
      "loss": 0.0026,
      "step": 85320
    },
    {
      "epoch": 4.550933333333333,
      "grad_norm": 0.08438927680253983,
      "learning_rate": 2.1556666666666666e-05,
      "loss": 0.0023,
      "step": 85330
    },
    {
      "epoch": 4.551466666666666,
      "grad_norm": 0.05625760182738304,
      "learning_rate": 2.1553333333333333e-05,
      "loss": 0.0027,
      "step": 85340
    },
    {
      "epoch": 4.552,
      "grad_norm": 0.7724156975746155,
      "learning_rate": 2.1550000000000002e-05,
      "loss": 0.0028,
      "step": 85350
    },
    {
      "epoch": 4.552533333333333,
      "grad_norm": 0.08438757061958313,
      "learning_rate": 2.1546666666666668e-05,
      "loss": 0.0018,
      "step": 85360
    },
    {
      "epoch": 4.553066666666667,
      "grad_norm": 0.056259721517562866,
      "learning_rate": 2.1543333333333334e-05,
      "loss": 0.0039,
      "step": 85370
    },
    {
      "epoch": 4.5536,
      "grad_norm": 0.056261688470840454,
      "learning_rate": 2.154e-05,
      "loss": 0.0044,
      "step": 85380
    },
    {
      "epoch": 4.554133333333334,
      "grad_norm": 0.19690434634685516,
      "learning_rate": 2.153666666666667e-05,
      "loss": 0.0024,
      "step": 85390
    },
    {
      "epoch": 4.554666666666667,
      "grad_norm": 0.11251376569271088,
      "learning_rate": 2.1533333333333333e-05,
      "loss": 0.0029,
      "step": 85400
    },
    {
      "epoch": 4.5552,
      "grad_norm": 1.315483570098877,
      "learning_rate": 2.153e-05,
      "loss": 0.0039,
      "step": 85410
    },
    {
      "epoch": 4.555733333333333,
      "grad_norm": 0.05625678971409798,
      "learning_rate": 2.152666666666667e-05,
      "loss": 0.004,
      "step": 85420
    },
    {
      "epoch": 4.556266666666667,
      "grad_norm": 0.11251360923051834,
      "learning_rate": 2.1523333333333335e-05,
      "loss": 0.0041,
      "step": 85430
    },
    {
      "epoch": 4.5568,
      "grad_norm": 0.28144288063049316,
      "learning_rate": 2.152e-05,
      "loss": 0.0024,
      "step": 85440
    },
    {
      "epoch": 4.557333333333333,
      "grad_norm": 0.16876962780952454,
      "learning_rate": 2.1516666666666667e-05,
      "loss": 0.0036,
      "step": 85450
    },
    {
      "epoch": 4.5578666666666665,
      "grad_norm": 0.08438611775636673,
      "learning_rate": 2.1513333333333336e-05,
      "loss": 0.0035,
      "step": 85460
    },
    {
      "epoch": 4.5584,
      "grad_norm": 0.2531501352787018,
      "learning_rate": 2.1510000000000002e-05,
      "loss": 0.0031,
      "step": 85470
    },
    {
      "epoch": 4.558933333333333,
      "grad_norm": 0.1687721461057663,
      "learning_rate": 2.1506666666666665e-05,
      "loss": 0.0026,
      "step": 85480
    },
    {
      "epoch": 4.559466666666666,
      "grad_norm": 0.4219207465648651,
      "learning_rate": 2.1503333333333335e-05,
      "loss": 0.0031,
      "step": 85490
    },
    {
      "epoch": 4.5600000000000005,
      "grad_norm": 0.3656874895095825,
      "learning_rate": 2.15e-05,
      "loss": 0.0036,
      "step": 85500
    },
    {
      "epoch": 4.560533333333334,
      "grad_norm": 0.05625597760081291,
      "learning_rate": 2.1496666666666667e-05,
      "loss": 0.0022,
      "step": 85510
    },
    {
      "epoch": 4.561066666666667,
      "grad_norm": 0.08438393473625183,
      "learning_rate": 2.1493333333333333e-05,
      "loss": 0.0033,
      "step": 85520
    },
    {
      "epoch": 4.5616,
      "grad_norm": 0.18320979177951813,
      "learning_rate": 2.1490000000000003e-05,
      "loss": 0.0022,
      "step": 85530
    },
    {
      "epoch": 4.562133333333334,
      "grad_norm": 0.028128042817115784,
      "learning_rate": 2.148666666666667e-05,
      "loss": 0.0029,
      "step": 85540
    },
    {
      "epoch": 4.562666666666667,
      "grad_norm": 0.08438929170370102,
      "learning_rate": 2.148333333333333e-05,
      "loss": 0.0031,
      "step": 85550
    },
    {
      "epoch": 4.5632,
      "grad_norm": 0.028129592537879944,
      "learning_rate": 2.148e-05,
      "loss": 0.0036,
      "step": 85560
    },
    {
      "epoch": 4.563733333333333,
      "grad_norm": 0.11250893026590347,
      "learning_rate": 2.1476666666666667e-05,
      "loss": 0.0027,
      "step": 85570
    },
    {
      "epoch": 4.564266666666667,
      "grad_norm": 0.056257303804159164,
      "learning_rate": 2.1473333333333333e-05,
      "loss": 0.0028,
      "step": 85580
    },
    {
      "epoch": 4.5648,
      "grad_norm": 0.020665384829044342,
      "learning_rate": 2.1470000000000003e-05,
      "loss": 0.0025,
      "step": 85590
    },
    {
      "epoch": 4.565333333333333,
      "grad_norm": 0.1125195175409317,
      "learning_rate": 2.146666666666667e-05,
      "loss": 0.0048,
      "step": 85600
    },
    {
      "epoch": 4.5658666666666665,
      "grad_norm": 0.1687716245651245,
      "learning_rate": 2.1463333333333335e-05,
      "loss": 0.003,
      "step": 85610
    },
    {
      "epoch": 4.5664,
      "grad_norm": 0.33753281831741333,
      "learning_rate": 2.146e-05,
      "loss": 0.0034,
      "step": 85620
    },
    {
      "epoch": 4.566933333333333,
      "grad_norm": 0.0562555231153965,
      "learning_rate": 2.1456666666666667e-05,
      "loss": 0.0021,
      "step": 85630
    },
    {
      "epoch": 4.567466666666666,
      "grad_norm": 0.028127901256084442,
      "learning_rate": 2.1453333333333333e-05,
      "loss": 0.0035,
      "step": 85640
    },
    {
      "epoch": 4.568,
      "grad_norm": 0.1406472623348236,
      "learning_rate": 2.145e-05,
      "loss": 0.0028,
      "step": 85650
    },
    {
      "epoch": 4.568533333333333,
      "grad_norm": 0.30942219495773315,
      "learning_rate": 2.144666666666667e-05,
      "loss": 0.0027,
      "step": 85660
    },
    {
      "epoch": 4.569066666666667,
      "grad_norm": 1.4177508354187012,
      "learning_rate": 2.1443333333333335e-05,
      "loss": 0.0032,
      "step": 85670
    },
    {
      "epoch": 4.5696,
      "grad_norm": 0.2531478703022003,
      "learning_rate": 2.144e-05,
      "loss": 0.003,
      "step": 85680
    },
    {
      "epoch": 4.570133333333334,
      "grad_norm": 0.28130730986595154,
      "learning_rate": 2.1436666666666668e-05,
      "loss": 0.0033,
      "step": 85690
    },
    {
      "epoch": 4.570666666666667,
      "grad_norm": 0.4219138026237488,
      "learning_rate": 2.1433333333333334e-05,
      "loss": 0.0029,
      "step": 85700
    },
    {
      "epoch": 4.5712,
      "grad_norm": 0.11250931769609451,
      "learning_rate": 2.143e-05,
      "loss": 0.0029,
      "step": 85710
    },
    {
      "epoch": 4.571733333333333,
      "grad_norm": 0.08438479155302048,
      "learning_rate": 2.1426666666666666e-05,
      "loss": 0.0042,
      "step": 85720
    },
    {
      "epoch": 4.572266666666667,
      "grad_norm": 0.08438955247402191,
      "learning_rate": 2.1423333333333335e-05,
      "loss": 0.0019,
      "step": 85730
    },
    {
      "epoch": 4.5728,
      "grad_norm": 4.0768286524439645e-09,
      "learning_rate": 2.142e-05,
      "loss": 0.0032,
      "step": 85740
    },
    {
      "epoch": 4.573333333333333,
      "grad_norm": 0.22502216696739197,
      "learning_rate": 2.1416666666666668e-05,
      "loss": 0.0027,
      "step": 85750
    },
    {
      "epoch": 4.5738666666666665,
      "grad_norm": 0.2531597912311554,
      "learning_rate": 2.1413333333333334e-05,
      "loss": 0.0036,
      "step": 85760
    },
    {
      "epoch": 4.5744,
      "grad_norm": 3.3722951098269505e-09,
      "learning_rate": 2.1410000000000003e-05,
      "loss": 0.0033,
      "step": 85770
    },
    {
      "epoch": 4.574933333333333,
      "grad_norm": 0.028128163889050484,
      "learning_rate": 2.1406666666666666e-05,
      "loss": 0.0031,
      "step": 85780
    },
    {
      "epoch": 4.575466666666666,
      "grad_norm": 0.5380727052688599,
      "learning_rate": 2.1403333333333332e-05,
      "loss": 0.0024,
      "step": 85790
    },
    {
      "epoch": 4.576,
      "grad_norm": 1.674463152885437,
      "learning_rate": 2.1400000000000002e-05,
      "loss": 0.0028,
      "step": 85800
    },
    {
      "epoch": 4.576533333333334,
      "grad_norm": 0.16876602172851562,
      "learning_rate": 2.1396666666666668e-05,
      "loss": 0.0027,
      "step": 85810
    },
    {
      "epoch": 4.577066666666667,
      "grad_norm": 0.1968974769115448,
      "learning_rate": 2.1393333333333334e-05,
      "loss": 0.0025,
      "step": 85820
    },
    {
      "epoch": 4.5776,
      "grad_norm": 0.02812878042459488,
      "learning_rate": 2.139e-05,
      "loss": 0.0024,
      "step": 85830
    },
    {
      "epoch": 4.578133333333334,
      "grad_norm": 0.11251064389944077,
      "learning_rate": 2.138666666666667e-05,
      "loss": 0.0034,
      "step": 85840
    },
    {
      "epoch": 4.578666666666667,
      "grad_norm": 0.3937881290912628,
      "learning_rate": 2.1383333333333332e-05,
      "loss": 0.0033,
      "step": 85850
    },
    {
      "epoch": 4.5792,
      "grad_norm": 0.02812732756137848,
      "learning_rate": 2.138e-05,
      "loss": 0.0024,
      "step": 85860
    },
    {
      "epoch": 4.579733333333333,
      "grad_norm": 0.1687636822462082,
      "learning_rate": 2.1376666666666668e-05,
      "loss": 0.0033,
      "step": 85870
    },
    {
      "epoch": 4.580266666666667,
      "grad_norm": 0.33755776286125183,
      "learning_rate": 2.1373333333333334e-05,
      "loss": 0.0024,
      "step": 85880
    },
    {
      "epoch": 4.5808,
      "grad_norm": 0.16876280307769775,
      "learning_rate": 2.137e-05,
      "loss": 0.0025,
      "step": 85890
    },
    {
      "epoch": 4.581333333333333,
      "grad_norm": 0.14063534140586853,
      "learning_rate": 2.1366666666666667e-05,
      "loss": 0.0032,
      "step": 85900
    },
    {
      "epoch": 4.5818666666666665,
      "grad_norm": 0.05625668168067932,
      "learning_rate": 2.1363333333333336e-05,
      "loss": 0.0027,
      "step": 85910
    },
    {
      "epoch": 4.5824,
      "grad_norm": 0.281286358833313,
      "learning_rate": 2.1360000000000002e-05,
      "loss": 0.0032,
      "step": 85920
    },
    {
      "epoch": 4.582933333333333,
      "grad_norm": 0.30940529704093933,
      "learning_rate": 2.1356666666666665e-05,
      "loss": 0.0037,
      "step": 85930
    },
    {
      "epoch": 4.583466666666666,
      "grad_norm": 0.1687590330839157,
      "learning_rate": 2.1353333333333334e-05,
      "loss": 0.0033,
      "step": 85940
    },
    {
      "epoch": 4.584,
      "grad_norm": 0.3094102442264557,
      "learning_rate": 2.135e-05,
      "loss": 0.0026,
      "step": 85950
    },
    {
      "epoch": 4.584533333333333,
      "grad_norm": 0.028127586469054222,
      "learning_rate": 2.1346666666666667e-05,
      "loss": 0.0033,
      "step": 85960
    },
    {
      "epoch": 4.585066666666666,
      "grad_norm": 0.11251208186149597,
      "learning_rate": 2.1343333333333336e-05,
      "loss": 0.0028,
      "step": 85970
    },
    {
      "epoch": 4.5856,
      "grad_norm": 0.14063242077827454,
      "learning_rate": 2.1340000000000002e-05,
      "loss": 0.0028,
      "step": 85980
    },
    {
      "epoch": 4.586133333333334,
      "grad_norm": 0.028128845617175102,
      "learning_rate": 2.133666666666667e-05,
      "loss": 0.0028,
      "step": 85990
    },
    {
      "epoch": 4.586666666666667,
      "grad_norm": 0.14064352214336395,
      "learning_rate": 2.1333333333333335e-05,
      "loss": 0.0024,
      "step": 86000
    },
    {
      "epoch": 4.5872,
      "grad_norm": 0.3094196617603302,
      "learning_rate": 2.133e-05,
      "loss": 0.0016,
      "step": 86010
    },
    {
      "epoch": 4.587733333333333,
      "grad_norm": 0.056254807859659195,
      "learning_rate": 2.1326666666666667e-05,
      "loss": 0.0023,
      "step": 86020
    },
    {
      "epoch": 4.588266666666667,
      "grad_norm": 0.19688934087753296,
      "learning_rate": 2.1323333333333333e-05,
      "loss": 0.0029,
      "step": 86030
    },
    {
      "epoch": 4.5888,
      "grad_norm": 7.875843976989927e-09,
      "learning_rate": 2.1320000000000003e-05,
      "loss": 0.0033,
      "step": 86040
    },
    {
      "epoch": 4.589333333333333,
      "grad_norm": 0.33752354979515076,
      "learning_rate": 2.131666666666667e-05,
      "loss": 0.0039,
      "step": 86050
    },
    {
      "epoch": 4.5898666666666665,
      "grad_norm": 0.05625494569540024,
      "learning_rate": 2.1313333333333335e-05,
      "loss": 0.0021,
      "step": 86060
    },
    {
      "epoch": 4.5904,
      "grad_norm": 0.08437880873680115,
      "learning_rate": 2.131e-05,
      "loss": 0.0021,
      "step": 86070
    },
    {
      "epoch": 4.590933333333333,
      "grad_norm": 0.05625689774751663,
      "learning_rate": 2.1306666666666667e-05,
      "loss": 0.0019,
      "step": 86080
    },
    {
      "epoch": 4.591466666666666,
      "grad_norm": 0.08438480645418167,
      "learning_rate": 2.1303333333333333e-05,
      "loss": 0.0027,
      "step": 86090
    },
    {
      "epoch": 4.592,
      "grad_norm": 0.16876335442066193,
      "learning_rate": 2.13e-05,
      "loss": 0.0026,
      "step": 86100
    },
    {
      "epoch": 4.592533333333334,
      "grad_norm": 1.9870056711113193e-09,
      "learning_rate": 2.129666666666667e-05,
      "loss": 0.0032,
      "step": 86110
    },
    {
      "epoch": 4.593066666666667,
      "grad_norm": 0.11251073330640793,
      "learning_rate": 2.1293333333333335e-05,
      "loss": 0.003,
      "step": 86120
    },
    {
      "epoch": 4.5936,
      "grad_norm": 0.22501438856124878,
      "learning_rate": 2.129e-05,
      "loss": 0.0042,
      "step": 86130
    },
    {
      "epoch": 4.594133333333334,
      "grad_norm": 0.08500558137893677,
      "learning_rate": 2.1286666666666667e-05,
      "loss": 0.0049,
      "step": 86140
    },
    {
      "epoch": 4.594666666666667,
      "grad_norm": 0.3093821108341217,
      "learning_rate": 2.1283333333333337e-05,
      "loss": 0.0035,
      "step": 86150
    },
    {
      "epoch": 4.5952,
      "grad_norm": 0.084383025765419,
      "learning_rate": 2.128e-05,
      "loss": 0.0032,
      "step": 86160
    },
    {
      "epoch": 4.5957333333333334,
      "grad_norm": 0.19746799767017365,
      "learning_rate": 2.1276666666666666e-05,
      "loss": 0.0031,
      "step": 86170
    },
    {
      "epoch": 4.596266666666667,
      "grad_norm": 0.30941298604011536,
      "learning_rate": 2.1273333333333335e-05,
      "loss": 0.0036,
      "step": 86180
    },
    {
      "epoch": 4.5968,
      "grad_norm": 0.0843781903386116,
      "learning_rate": 2.127e-05,
      "loss": 0.0031,
      "step": 86190
    },
    {
      "epoch": 4.597333333333333,
      "grad_norm": 0.3375215530395508,
      "learning_rate": 2.1266666666666667e-05,
      "loss": 0.0026,
      "step": 86200
    },
    {
      "epoch": 4.5978666666666665,
      "grad_norm": 3.3512774777477716e-09,
      "learning_rate": 2.1263333333333334e-05,
      "loss": 0.0032,
      "step": 86210
    },
    {
      "epoch": 4.5984,
      "grad_norm": 0.16875672340393066,
      "learning_rate": 2.1260000000000003e-05,
      "loss": 0.0045,
      "step": 86220
    },
    {
      "epoch": 4.598933333333333,
      "grad_norm": 0.0562528632581234,
      "learning_rate": 2.1256666666666666e-05,
      "loss": 0.0036,
      "step": 86230
    },
    {
      "epoch": 4.599466666666666,
      "grad_norm": 0.7032078504562378,
      "learning_rate": 2.1253333333333332e-05,
      "loss": 0.0041,
      "step": 86240
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.14062950015068054,
      "learning_rate": 2.125e-05,
      "loss": 0.0027,
      "step": 86250
    },
    {
      "epoch": 4.600533333333333,
      "grad_norm": 0.19688266515731812,
      "learning_rate": 2.1246666666666668e-05,
      "loss": 0.0016,
      "step": 86260
    },
    {
      "epoch": 4.601066666666666,
      "grad_norm": 0.08438348770141602,
      "learning_rate": 2.1243333333333334e-05,
      "loss": 0.0026,
      "step": 86270
    },
    {
      "epoch": 4.6016,
      "grad_norm": 0.19688235223293304,
      "learning_rate": 2.124e-05,
      "loss": 0.0033,
      "step": 86280
    },
    {
      "epoch": 4.602133333333334,
      "grad_norm": 0.5343682765960693,
      "learning_rate": 2.123666666666667e-05,
      "loss": 0.0037,
      "step": 86290
    },
    {
      "epoch": 4.602666666666667,
      "grad_norm": 0.11812403053045273,
      "learning_rate": 2.1233333333333336e-05,
      "loss": 0.0043,
      "step": 86300
    },
    {
      "epoch": 4.6032,
      "grad_norm": 0.02812584675848484,
      "learning_rate": 2.123e-05,
      "loss": 0.003,
      "step": 86310
    },
    {
      "epoch": 4.6037333333333335,
      "grad_norm": 0.08437690883874893,
      "learning_rate": 2.1226666666666668e-05,
      "loss": 0.0029,
      "step": 86320
    },
    {
      "epoch": 4.604266666666667,
      "grad_norm": 0.14076577126979828,
      "learning_rate": 2.1223333333333334e-05,
      "loss": 0.0021,
      "step": 86330
    },
    {
      "epoch": 4.6048,
      "grad_norm": 0.02812710590660572,
      "learning_rate": 2.122e-05,
      "loss": 0.0026,
      "step": 86340
    },
    {
      "epoch": 4.605333333333333,
      "grad_norm": 0.11250464618206024,
      "learning_rate": 2.121666666666667e-05,
      "loss": 0.0026,
      "step": 86350
    },
    {
      "epoch": 4.6058666666666666,
      "grad_norm": 0.33771175146102905,
      "learning_rate": 2.1213333333333336e-05,
      "loss": 0.0021,
      "step": 86360
    },
    {
      "epoch": 4.6064,
      "grad_norm": 0.16875801980495453,
      "learning_rate": 2.1210000000000002e-05,
      "loss": 0.0021,
      "step": 86370
    },
    {
      "epoch": 4.606933333333333,
      "grad_norm": 0.056248925626277924,
      "learning_rate": 2.1206666666666665e-05,
      "loss": 0.0023,
      "step": 86380
    },
    {
      "epoch": 4.607466666666666,
      "grad_norm": 0.14062762260437012,
      "learning_rate": 2.1203333333333334e-05,
      "loss": 0.0038,
      "step": 86390
    },
    {
      "epoch": 4.608,
      "grad_norm": 0.16874882578849792,
      "learning_rate": 2.12e-05,
      "loss": 0.0023,
      "step": 86400
    },
    {
      "epoch": 4.608533333333334,
      "grad_norm": 0.2531247138977051,
      "learning_rate": 2.1196666666666666e-05,
      "loss": 0.0015,
      "step": 86410
    },
    {
      "epoch": 4.609066666666667,
      "grad_norm": 0.028126850724220276,
      "learning_rate": 2.1193333333333336e-05,
      "loss": 0.0026,
      "step": 86420
    },
    {
      "epoch": 4.6096,
      "grad_norm": 0.42188629508018494,
      "learning_rate": 2.1190000000000002e-05,
      "loss": 0.0025,
      "step": 86430
    },
    {
      "epoch": 4.610133333333334,
      "grad_norm": 0.084376260638237,
      "learning_rate": 2.1186666666666668e-05,
      "loss": 0.0024,
      "step": 86440
    },
    {
      "epoch": 4.610666666666667,
      "grad_norm": 0.08437837660312653,
      "learning_rate": 2.1183333333333334e-05,
      "loss": 0.0019,
      "step": 86450
    },
    {
      "epoch": 4.6112,
      "grad_norm": 0.39372313022613525,
      "learning_rate": 2.118e-05,
      "loss": 0.0028,
      "step": 86460
    },
    {
      "epoch": 4.6117333333333335,
      "grad_norm": 0.19689308106899261,
      "learning_rate": 2.1176666666666667e-05,
      "loss": 0.0037,
      "step": 86470
    },
    {
      "epoch": 4.612266666666667,
      "grad_norm": 0.16874349117279053,
      "learning_rate": 2.1173333333333333e-05,
      "loss": 0.0015,
      "step": 86480
    },
    {
      "epoch": 4.6128,
      "grad_norm": 0.08437581360340118,
      "learning_rate": 2.1170000000000002e-05,
      "loss": 0.0017,
      "step": 86490
    },
    {
      "epoch": 4.613333333333333,
      "grad_norm": 0.11250445991754532,
      "learning_rate": 2.116666666666667e-05,
      "loss": 0.0035,
      "step": 86500
    },
    {
      "epoch": 4.613866666666667,
      "grad_norm": 0.1124972477555275,
      "learning_rate": 2.1163333333333335e-05,
      "loss": 0.003,
      "step": 86510
    },
    {
      "epoch": 4.6144,
      "grad_norm": 0.1406252235174179,
      "learning_rate": 2.116e-05,
      "loss": 0.0017,
      "step": 86520
    },
    {
      "epoch": 4.614933333333333,
      "grad_norm": 1.928679660423427e-09,
      "learning_rate": 2.1156666666666667e-05,
      "loss": 0.0021,
      "step": 86530
    },
    {
      "epoch": 4.615466666666666,
      "grad_norm": 0.056246720254421234,
      "learning_rate": 2.1153333333333333e-05,
      "loss": 0.0028,
      "step": 86540
    },
    {
      "epoch": 4.616,
      "grad_norm": 0.08437313139438629,
      "learning_rate": 2.115e-05,
      "loss": 0.0029,
      "step": 86550
    },
    {
      "epoch": 4.616533333333333,
      "grad_norm": 0.06107484549283981,
      "learning_rate": 2.114666666666667e-05,
      "loss": 0.0031,
      "step": 86560
    },
    {
      "epoch": 4.617066666666666,
      "grad_norm": 0.14063043892383575,
      "learning_rate": 2.1143333333333335e-05,
      "loss": 0.0026,
      "step": 86570
    },
    {
      "epoch": 4.6176,
      "grad_norm": 0.36563077569007874,
      "learning_rate": 2.114e-05,
      "loss": 0.0025,
      "step": 86580
    },
    {
      "epoch": 4.618133333333334,
      "grad_norm": 0.028124120086431503,
      "learning_rate": 2.1136666666666667e-05,
      "loss": 0.0026,
      "step": 86590
    },
    {
      "epoch": 4.618666666666667,
      "grad_norm": 0.11250655353069305,
      "learning_rate": 2.1133333333333337e-05,
      "loss": 0.0037,
      "step": 86600
    },
    {
      "epoch": 4.6192,
      "grad_norm": 0.1124996542930603,
      "learning_rate": 2.113e-05,
      "loss": 0.0025,
      "step": 86610
    },
    {
      "epoch": 4.6197333333333335,
      "grad_norm": 0.16874049603939056,
      "learning_rate": 2.1126666666666665e-05,
      "loss": 0.0033,
      "step": 86620
    },
    {
      "epoch": 4.620266666666667,
      "grad_norm": 0.11250109225511551,
      "learning_rate": 2.1123333333333335e-05,
      "loss": 0.0049,
      "step": 86630
    },
    {
      "epoch": 4.6208,
      "grad_norm": 0.028124727308750153,
      "learning_rate": 2.112e-05,
      "loss": 0.0021,
      "step": 86640
    },
    {
      "epoch": 4.621333333333333,
      "grad_norm": 0.1968655288219452,
      "learning_rate": 2.1116666666666667e-05,
      "loss": 0.0025,
      "step": 86650
    },
    {
      "epoch": 4.621866666666667,
      "grad_norm": 0.08437543362379074,
      "learning_rate": 2.1113333333333333e-05,
      "loss": 0.0027,
      "step": 86660
    },
    {
      "epoch": 4.6224,
      "grad_norm": 0.1968652606010437,
      "learning_rate": 2.1110000000000003e-05,
      "loss": 0.0019,
      "step": 86670
    },
    {
      "epoch": 4.622933333333333,
      "grad_norm": 0.1687377542257309,
      "learning_rate": 2.110666666666667e-05,
      "loss": 0.0042,
      "step": 86680
    },
    {
      "epoch": 4.623466666666666,
      "grad_norm": 0.28347131609916687,
      "learning_rate": 2.1103333333333332e-05,
      "loss": 0.0045,
      "step": 86690
    },
    {
      "epoch": 4.624,
      "grad_norm": 0.5062462091445923,
      "learning_rate": 2.11e-05,
      "loss": 0.0023,
      "step": 86700
    },
    {
      "epoch": 4.624533333333334,
      "grad_norm": 0.16873987019062042,
      "learning_rate": 2.1096666666666667e-05,
      "loss": 0.0033,
      "step": 86710
    },
    {
      "epoch": 4.625066666666667,
      "grad_norm": 0.028123607859015465,
      "learning_rate": 2.1093333333333334e-05,
      "loss": 0.0029,
      "step": 86720
    },
    {
      "epoch": 4.6256,
      "grad_norm": 0.08437436819076538,
      "learning_rate": 2.1090000000000003e-05,
      "loss": 0.0033,
      "step": 86730
    },
    {
      "epoch": 4.626133333333334,
      "grad_norm": 0.2812309265136719,
      "learning_rate": 2.108666666666667e-05,
      "loss": 0.0035,
      "step": 86740
    },
    {
      "epoch": 4.626666666666667,
      "grad_norm": 0.08437114208936691,
      "learning_rate": 2.1083333333333335e-05,
      "loss": 0.0026,
      "step": 86750
    },
    {
      "epoch": 4.6272,
      "grad_norm": 0.11478672921657562,
      "learning_rate": 2.1079999999999998e-05,
      "loss": 0.0029,
      "step": 86760
    },
    {
      "epoch": 4.6277333333333335,
      "grad_norm": 0.1125517413020134,
      "learning_rate": 2.1076666666666668e-05,
      "loss": 0.0022,
      "step": 86770
    },
    {
      "epoch": 4.628266666666667,
      "grad_norm": 0.028137853369116783,
      "learning_rate": 2.1073333333333334e-05,
      "loss": 0.0023,
      "step": 86780
    },
    {
      "epoch": 4.6288,
      "grad_norm": 0.11255018413066864,
      "learning_rate": 2.107e-05,
      "loss": 0.002,
      "step": 86790
    },
    {
      "epoch": 4.629333333333333,
      "grad_norm": 0.14062009751796722,
      "learning_rate": 2.106666666666667e-05,
      "loss": 0.0039,
      "step": 86800
    },
    {
      "epoch": 4.629866666666667,
      "grad_norm": 0.11473330855369568,
      "learning_rate": 2.1063333333333336e-05,
      "loss": 0.0025,
      "step": 86810
    },
    {
      "epoch": 4.6304,
      "grad_norm": 0.056248776614665985,
      "learning_rate": 2.106e-05,
      "loss": 0.003,
      "step": 86820
    },
    {
      "epoch": 4.630933333333333,
      "grad_norm": 0.028124766424298286,
      "learning_rate": 2.1056666666666668e-05,
      "loss": 0.0027,
      "step": 86830
    },
    {
      "epoch": 4.631466666666666,
      "grad_norm": 0.05900618061423302,
      "learning_rate": 2.1053333333333334e-05,
      "loss": 0.0035,
      "step": 86840
    },
    {
      "epoch": 4.632,
      "grad_norm": 0.17052488029003143,
      "learning_rate": 2.105e-05,
      "loss": 0.0047,
      "step": 86850
    },
    {
      "epoch": 4.632533333333333,
      "grad_norm": 0.028124330565333366,
      "learning_rate": 2.1046666666666666e-05,
      "loss": 0.0028,
      "step": 86860
    },
    {
      "epoch": 4.633066666666666,
      "grad_norm": 0.05624556541442871,
      "learning_rate": 2.1043333333333336e-05,
      "loss": 0.0028,
      "step": 86870
    },
    {
      "epoch": 4.6336,
      "grad_norm": 0.22498472034931183,
      "learning_rate": 2.1040000000000002e-05,
      "loss": 0.0025,
      "step": 86880
    },
    {
      "epoch": 4.634133333333334,
      "grad_norm": 3.400534520636711e-09,
      "learning_rate": 2.1036666666666668e-05,
      "loss": 0.0021,
      "step": 86890
    },
    {
      "epoch": 4.634666666666667,
      "grad_norm": 0.5768277049064636,
      "learning_rate": 2.1033333333333334e-05,
      "loss": 0.0022,
      "step": 86900
    },
    {
      "epoch": 4.6352,
      "grad_norm": 0.2531093955039978,
      "learning_rate": 2.103e-05,
      "loss": 0.0027,
      "step": 86910
    },
    {
      "epoch": 4.6357333333333335,
      "grad_norm": 0.2249787598848343,
      "learning_rate": 2.1026666666666666e-05,
      "loss": 0.0028,
      "step": 86920
    },
    {
      "epoch": 4.636266666666667,
      "grad_norm": 0.028122249990701675,
      "learning_rate": 2.1023333333333333e-05,
      "loss": 0.0039,
      "step": 86930
    },
    {
      "epoch": 4.6368,
      "grad_norm": 0.1406136155128479,
      "learning_rate": 2.1020000000000002e-05,
      "loss": 0.0029,
      "step": 86940
    },
    {
      "epoch": 4.637333333333333,
      "grad_norm": 0.19685399532318115,
      "learning_rate": 2.1016666666666668e-05,
      "loss": 0.0018,
      "step": 86950
    },
    {
      "epoch": 4.637866666666667,
      "grad_norm": 0.16873513162136078,
      "learning_rate": 2.1013333333333334e-05,
      "loss": 0.0029,
      "step": 86960
    },
    {
      "epoch": 4.6384,
      "grad_norm": 2.0162169711568367e-09,
      "learning_rate": 2.101e-05,
      "loss": 0.0036,
      "step": 86970
    },
    {
      "epoch": 4.638933333333333,
      "grad_norm": 0.16874411702156067,
      "learning_rate": 2.100666666666667e-05,
      "loss": 0.0024,
      "step": 86980
    },
    {
      "epoch": 4.639466666666666,
      "grad_norm": 0.02812337689101696,
      "learning_rate": 2.1003333333333333e-05,
      "loss": 0.0035,
      "step": 86990
    },
    {
      "epoch": 4.64,
      "grad_norm": 0.11249330639839172,
      "learning_rate": 2.1e-05,
      "loss": 0.0024,
      "step": 87000
    },
    {
      "epoch": 4.640533333333333,
      "grad_norm": 0.08437056094408035,
      "learning_rate": 2.099666666666667e-05,
      "loss": 0.0023,
      "step": 87010
    },
    {
      "epoch": 4.641066666666667,
      "grad_norm": 0.11249599605798721,
      "learning_rate": 2.0993333333333334e-05,
      "loss": 0.0023,
      "step": 87020
    },
    {
      "epoch": 4.6416,
      "grad_norm": 0.1406119167804718,
      "learning_rate": 2.099e-05,
      "loss": 0.0039,
      "step": 87030
    },
    {
      "epoch": 4.642133333333334,
      "grad_norm": 0.25310537219047546,
      "learning_rate": 2.0986666666666667e-05,
      "loss": 0.003,
      "step": 87040
    },
    {
      "epoch": 4.642666666666667,
      "grad_norm": 0.05624361336231232,
      "learning_rate": 2.0983333333333336e-05,
      "loss": 0.0016,
      "step": 87050
    },
    {
      "epoch": 4.6432,
      "grad_norm": 0.14061376452445984,
      "learning_rate": 2.098e-05,
      "loss": 0.0028,
      "step": 87060
    },
    {
      "epoch": 4.6437333333333335,
      "grad_norm": 0.0562446191906929,
      "learning_rate": 2.0976666666666665e-05,
      "loss": 0.0033,
      "step": 87070
    },
    {
      "epoch": 4.644266666666667,
      "grad_norm": 0.1968608945608139,
      "learning_rate": 2.0973333333333335e-05,
      "loss": 0.0029,
      "step": 87080
    },
    {
      "epoch": 4.6448,
      "grad_norm": 0.08436864614486694,
      "learning_rate": 2.097e-05,
      "loss": 0.0031,
      "step": 87090
    },
    {
      "epoch": 4.645333333333333,
      "grad_norm": 8.839376453018133e-10,
      "learning_rate": 2.0966666666666667e-05,
      "loss": 0.0019,
      "step": 87100
    },
    {
      "epoch": 4.645866666666667,
      "grad_norm": 0.02812168560922146,
      "learning_rate": 2.0963333333333336e-05,
      "loss": 0.0029,
      "step": 87110
    },
    {
      "epoch": 4.6464,
      "grad_norm": 0.3093548119068146,
      "learning_rate": 2.0960000000000003e-05,
      "loss": 0.0024,
      "step": 87120
    },
    {
      "epoch": 4.646933333333333,
      "grad_norm": 0.11248896270990372,
      "learning_rate": 2.095666666666667e-05,
      "loss": 0.0036,
      "step": 87130
    },
    {
      "epoch": 4.647466666666666,
      "grad_norm": 0.1968577802181244,
      "learning_rate": 2.095333333333333e-05,
      "loss": 0.0028,
      "step": 87140
    },
    {
      "epoch": 4.648,
      "grad_norm": 0.2530946731567383,
      "learning_rate": 2.095e-05,
      "loss": 0.0028,
      "step": 87150
    },
    {
      "epoch": 4.648533333333333,
      "grad_norm": 1.976803165604224e-09,
      "learning_rate": 2.0946666666666667e-05,
      "loss": 0.0026,
      "step": 87160
    },
    {
      "epoch": 4.649066666666666,
      "grad_norm": 0.2530946433544159,
      "learning_rate": 2.0943333333333333e-05,
      "loss": 0.0025,
      "step": 87170
    },
    {
      "epoch": 4.6495999999999995,
      "grad_norm": 0.4218579828739166,
      "learning_rate": 2.0940000000000003e-05,
      "loss": 0.0023,
      "step": 87180
    },
    {
      "epoch": 4.650133333333334,
      "grad_norm": 0.19685766100883484,
      "learning_rate": 2.093666666666667e-05,
      "loss": 0.0024,
      "step": 87190
    },
    {
      "epoch": 4.650666666666667,
      "grad_norm": 0.11249363422393799,
      "learning_rate": 2.0933333333333335e-05,
      "loss": 0.002,
      "step": 87200
    },
    {
      "epoch": 4.6512,
      "grad_norm": 0.14061729609966278,
      "learning_rate": 2.093e-05,
      "loss": 0.002,
      "step": 87210
    },
    {
      "epoch": 4.6517333333333335,
      "grad_norm": 0.22496794164180756,
      "learning_rate": 2.0926666666666667e-05,
      "loss": 0.0028,
      "step": 87220
    },
    {
      "epoch": 4.652266666666667,
      "grad_norm": 0.05624610558152199,
      "learning_rate": 2.0923333333333333e-05,
      "loss": 0.0025,
      "step": 87230
    },
    {
      "epoch": 4.6528,
      "grad_norm": 0.05624525249004364,
      "learning_rate": 2.092e-05,
      "loss": 0.0038,
      "step": 87240
    },
    {
      "epoch": 4.653333333333333,
      "grad_norm": 0.1406073421239853,
      "learning_rate": 2.091666666666667e-05,
      "loss": 0.0027,
      "step": 87250
    },
    {
      "epoch": 4.653866666666667,
      "grad_norm": 0.028122927993535995,
      "learning_rate": 2.0913333333333335e-05,
      "loss": 0.002,
      "step": 87260
    },
    {
      "epoch": 4.6544,
      "grad_norm": 0.4780702292919159,
      "learning_rate": 2.091e-05,
      "loss": 0.0038,
      "step": 87270
    },
    {
      "epoch": 4.654933333333333,
      "grad_norm": 0.02812253125011921,
      "learning_rate": 2.0906666666666668e-05,
      "loss": 0.002,
      "step": 87280
    },
    {
      "epoch": 4.655466666666666,
      "grad_norm": 0.2531082034111023,
      "learning_rate": 2.0903333333333334e-05,
      "loss": 0.0041,
      "step": 87290
    },
    {
      "epoch": 4.656,
      "grad_norm": 0.3936774432659149,
      "learning_rate": 2.09e-05,
      "loss": 0.0034,
      "step": 87300
    },
    {
      "epoch": 4.656533333333333,
      "grad_norm": 0.2812208831310272,
      "learning_rate": 2.0896666666666666e-05,
      "loss": 0.0032,
      "step": 87310
    },
    {
      "epoch": 4.657066666666667,
      "grad_norm": 0.0843614861369133,
      "learning_rate": 2.0893333333333335e-05,
      "loss": 0.0025,
      "step": 87320
    },
    {
      "epoch": 4.6576,
      "grad_norm": 0.16873383522033691,
      "learning_rate": 2.089e-05,
      "loss": 0.0017,
      "step": 87330
    },
    {
      "epoch": 4.658133333333334,
      "grad_norm": 0.028120459988713264,
      "learning_rate": 2.0886666666666668e-05,
      "loss": 0.0026,
      "step": 87340
    },
    {
      "epoch": 4.658666666666667,
      "grad_norm": 0.02812160924077034,
      "learning_rate": 2.0883333333333334e-05,
      "loss": 0.0029,
      "step": 87350
    },
    {
      "epoch": 4.6592,
      "grad_norm": 0.2530840337276459,
      "learning_rate": 2.0880000000000003e-05,
      "loss": 0.0035,
      "step": 87360
    },
    {
      "epoch": 4.6597333333333335,
      "grad_norm": 0.08436626940965652,
      "learning_rate": 2.0876666666666666e-05,
      "loss": 0.0029,
      "step": 87370
    },
    {
      "epoch": 4.660266666666667,
      "grad_norm": 0.19684116542339325,
      "learning_rate": 2.0873333333333332e-05,
      "loss": 0.0027,
      "step": 87380
    },
    {
      "epoch": 4.6608,
      "grad_norm": 0.14060702919960022,
      "learning_rate": 2.0870000000000002e-05,
      "loss": 0.003,
      "step": 87390
    },
    {
      "epoch": 4.661333333333333,
      "grad_norm": 0.14060233533382416,
      "learning_rate": 2.0866666666666668e-05,
      "loss": 0.0035,
      "step": 87400
    },
    {
      "epoch": 4.661866666666667,
      "grad_norm": 0.056242283433675766,
      "learning_rate": 2.0863333333333334e-05,
      "loss": 0.0019,
      "step": 87410
    },
    {
      "epoch": 4.6624,
      "grad_norm": 0.16873277723789215,
      "learning_rate": 2.086e-05,
      "loss": 0.0031,
      "step": 87420
    },
    {
      "epoch": 4.662933333333333,
      "grad_norm": 0.19684600830078125,
      "learning_rate": 2.085666666666667e-05,
      "loss": 0.0032,
      "step": 87430
    },
    {
      "epoch": 4.663466666666666,
      "grad_norm": 0.16872525215148926,
      "learning_rate": 2.0853333333333332e-05,
      "loss": 0.0023,
      "step": 87440
    },
    {
      "epoch": 4.664,
      "grad_norm": 0.3937062919139862,
      "learning_rate": 2.085e-05,
      "loss": 0.0037,
      "step": 87450
    },
    {
      "epoch": 4.664533333333333,
      "grad_norm": 0.2249557077884674,
      "learning_rate": 2.0846666666666668e-05,
      "loss": 0.0033,
      "step": 87460
    },
    {
      "epoch": 4.665066666666666,
      "grad_norm": 0.14060817658901215,
      "learning_rate": 2.0843333333333334e-05,
      "loss": 0.0019,
      "step": 87470
    },
    {
      "epoch": 4.6655999999999995,
      "grad_norm": 0.2812243402004242,
      "learning_rate": 2.084e-05,
      "loss": 0.0033,
      "step": 87480
    },
    {
      "epoch": 4.666133333333334,
      "grad_norm": 0.16873390972614288,
      "learning_rate": 2.083666666666667e-05,
      "loss": 0.0036,
      "step": 87490
    },
    {
      "epoch": 4.666666666666667,
      "grad_norm": 0.140597403049469,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 0.0041,
      "step": 87500
    },
    {
      "epoch": 4.6672,
      "grad_norm": 0.0843619704246521,
      "learning_rate": 2.0830000000000002e-05,
      "loss": 0.0032,
      "step": 87510
    },
    {
      "epoch": 4.6677333333333335,
      "grad_norm": 0.08436539024114609,
      "learning_rate": 2.0826666666666665e-05,
      "loss": 0.0029,
      "step": 87520
    },
    {
      "epoch": 4.668266666666667,
      "grad_norm": 0.1968497484922409,
      "learning_rate": 2.0823333333333334e-05,
      "loss": 0.0022,
      "step": 87530
    },
    {
      "epoch": 4.6688,
      "grad_norm": 0.16872304677963257,
      "learning_rate": 2.082e-05,
      "loss": 0.0022,
      "step": 87540
    },
    {
      "epoch": 4.669333333333333,
      "grad_norm": 0.16872982680797577,
      "learning_rate": 2.0816666666666667e-05,
      "loss": 0.0043,
      "step": 87550
    },
    {
      "epoch": 4.669866666666667,
      "grad_norm": 0.16872338950634003,
      "learning_rate": 2.0813333333333336e-05,
      "loss": 0.0039,
      "step": 87560
    },
    {
      "epoch": 4.6704,
      "grad_norm": 0.16871508955955505,
      "learning_rate": 2.0810000000000002e-05,
      "loss": 0.0032,
      "step": 87570
    },
    {
      "epoch": 4.670933333333333,
      "grad_norm": 0.3655833899974823,
      "learning_rate": 2.080666666666667e-05,
      "loss": 0.0019,
      "step": 87580
    },
    {
      "epoch": 4.671466666666666,
      "grad_norm": 0.22495931386947632,
      "learning_rate": 2.0803333333333335e-05,
      "loss": 0.0033,
      "step": 87590
    },
    {
      "epoch": 4.672,
      "grad_norm": 0.14059871435165405,
      "learning_rate": 2.08e-05,
      "loss": 0.0021,
      "step": 87600
    },
    {
      "epoch": 4.672533333333333,
      "grad_norm": 1.4190355557275325e-09,
      "learning_rate": 2.0796666666666667e-05,
      "loss": 0.0034,
      "step": 87610
    },
    {
      "epoch": 4.673066666666667,
      "grad_norm": 0.2531002163887024,
      "learning_rate": 2.0793333333333333e-05,
      "loss": 0.0027,
      "step": 87620
    },
    {
      "epoch": 4.6736,
      "grad_norm": 0.22496674954891205,
      "learning_rate": 2.0790000000000003e-05,
      "loss": 0.0035,
      "step": 87630
    },
    {
      "epoch": 4.674133333333334,
      "grad_norm": 0.14059653878211975,
      "learning_rate": 2.078666666666667e-05,
      "loss": 0.0031,
      "step": 87640
    },
    {
      "epoch": 4.674666666666667,
      "grad_norm": 0.08436286449432373,
      "learning_rate": 2.0783333333333335e-05,
      "loss": 0.0022,
      "step": 87650
    },
    {
      "epoch": 4.6752,
      "grad_norm": 0.05623960122466087,
      "learning_rate": 2.078e-05,
      "loss": 0.0018,
      "step": 87660
    },
    {
      "epoch": 4.6757333333333335,
      "grad_norm": 0.02812035195529461,
      "learning_rate": 2.0776666666666667e-05,
      "loss": 0.0019,
      "step": 87670
    },
    {
      "epoch": 4.676266666666667,
      "grad_norm": 0.11247887462377548,
      "learning_rate": 2.0773333333333333e-05,
      "loss": 0.0035,
      "step": 87680
    },
    {
      "epoch": 4.6768,
      "grad_norm": 0.08435879647731781,
      "learning_rate": 2.077e-05,
      "loss": 0.0037,
      "step": 87690
    },
    {
      "epoch": 4.677333333333333,
      "grad_norm": 0.11247781664133072,
      "learning_rate": 2.076666666666667e-05,
      "loss": 0.0038,
      "step": 87700
    },
    {
      "epoch": 4.677866666666667,
      "grad_norm": 0.08435822278261185,
      "learning_rate": 2.0763333333333335e-05,
      "loss": 0.0024,
      "step": 87710
    },
    {
      "epoch": 4.6784,
      "grad_norm": 0.11248374730348587,
      "learning_rate": 2.076e-05,
      "loss": 0.0028,
      "step": 87720
    },
    {
      "epoch": 4.678933333333333,
      "grad_norm": 0.056241996586322784,
      "learning_rate": 2.0756666666666667e-05,
      "loss": 0.0023,
      "step": 87730
    },
    {
      "epoch": 4.679466666666666,
      "grad_norm": 0.1687164455652237,
      "learning_rate": 2.0753333333333333e-05,
      "loss": 0.0017,
      "step": 87740
    },
    {
      "epoch": 4.68,
      "grad_norm": 0.056241169571876526,
      "learning_rate": 2.075e-05,
      "loss": 0.0034,
      "step": 87750
    },
    {
      "epoch": 4.680533333333333,
      "grad_norm": 0.3093160390853882,
      "learning_rate": 2.0746666666666666e-05,
      "loss": 0.0025,
      "step": 87760
    },
    {
      "epoch": 4.681066666666666,
      "grad_norm": 0.05624181777238846,
      "learning_rate": 2.0743333333333335e-05,
      "loss": 0.0032,
      "step": 87770
    },
    {
      "epoch": 4.6815999999999995,
      "grad_norm": 0.19683893024921417,
      "learning_rate": 2.074e-05,
      "loss": 0.0026,
      "step": 87780
    },
    {
      "epoch": 4.682133333333334,
      "grad_norm": 1.8822388092587516e-09,
      "learning_rate": 2.0736666666666667e-05,
      "loss": 0.0031,
      "step": 87790
    },
    {
      "epoch": 4.682666666666667,
      "grad_norm": 0.16871801018714905,
      "learning_rate": 2.0733333333333334e-05,
      "loss": 0.0018,
      "step": 87800
    },
    {
      "epoch": 4.6832,
      "grad_norm": 0.19683195650577545,
      "learning_rate": 2.0730000000000003e-05,
      "loss": 0.0015,
      "step": 87810
    },
    {
      "epoch": 4.6837333333333335,
      "grad_norm": 0.22496871650218964,
      "learning_rate": 2.0726666666666666e-05,
      "loss": 0.0034,
      "step": 87820
    },
    {
      "epoch": 4.684266666666667,
      "grad_norm": 0.3374199867248535,
      "learning_rate": 2.0723333333333332e-05,
      "loss": 0.002,
      "step": 87830
    },
    {
      "epoch": 4.6848,
      "grad_norm": 0.19684064388275146,
      "learning_rate": 2.072e-05,
      "loss": 0.0022,
      "step": 87840
    },
    {
      "epoch": 4.685333333333333,
      "grad_norm": 0.08435710519552231,
      "learning_rate": 2.0716666666666668e-05,
      "loss": 0.004,
      "step": 87850
    },
    {
      "epoch": 4.685866666666667,
      "grad_norm": 0.2812078893184662,
      "learning_rate": 2.0713333333333334e-05,
      "loss": 0.0031,
      "step": 87860
    },
    {
      "epoch": 4.6864,
      "grad_norm": 0.1405985802412033,
      "learning_rate": 2.0710000000000003e-05,
      "loss": 0.0019,
      "step": 87870
    },
    {
      "epoch": 4.686933333333333,
      "grad_norm": 0.5623511672019958,
      "learning_rate": 2.070666666666667e-05,
      "loss": 0.0022,
      "step": 87880
    },
    {
      "epoch": 4.6874666666666664,
      "grad_norm": 0.1687159687280655,
      "learning_rate": 2.0703333333333336e-05,
      "loss": 0.0026,
      "step": 87890
    },
    {
      "epoch": 4.688,
      "grad_norm": 3.0919309335075695e-09,
      "learning_rate": 2.07e-05,
      "loss": 0.0045,
      "step": 87900
    },
    {
      "epoch": 4.688533333333333,
      "grad_norm": 0.1687231808900833,
      "learning_rate": 2.0696666666666668e-05,
      "loss": 0.0037,
      "step": 87910
    },
    {
      "epoch": 4.689066666666667,
      "grad_norm": 0.4217686057090759,
      "learning_rate": 2.0693333333333334e-05,
      "loss": 0.0023,
      "step": 87920
    },
    {
      "epoch": 4.6896,
      "grad_norm": 0.4218190312385559,
      "learning_rate": 2.069e-05,
      "loss": 0.003,
      "step": 87930
    },
    {
      "epoch": 4.690133333333334,
      "grad_norm": 0.22494666278362274,
      "learning_rate": 2.068666666666667e-05,
      "loss": 0.0029,
      "step": 87940
    },
    {
      "epoch": 4.690666666666667,
      "grad_norm": 0.4217822849750519,
      "learning_rate": 2.0683333333333336e-05,
      "loss": 0.0022,
      "step": 87950
    },
    {
      "epoch": 4.6912,
      "grad_norm": 4.661465435873424e-09,
      "learning_rate": 2.0680000000000002e-05,
      "loss": 0.0042,
      "step": 87960
    },
    {
      "epoch": 4.6917333333333335,
      "grad_norm": 0.1414588838815689,
      "learning_rate": 2.0676666666666668e-05,
      "loss": 0.0038,
      "step": 87970
    },
    {
      "epoch": 4.692266666666667,
      "grad_norm": 0.0843583270907402,
      "learning_rate": 2.0673333333333334e-05,
      "loss": 0.0027,
      "step": 87980
    },
    {
      "epoch": 4.6928,
      "grad_norm": 0.25305357575416565,
      "learning_rate": 2.067e-05,
      "loss": 0.0028,
      "step": 87990
    },
    {
      "epoch": 4.693333333333333,
      "grad_norm": 1.1453304290771484,
      "learning_rate": 2.0666666666666666e-05,
      "loss": 0.0033,
      "step": 88000
    },
    {
      "epoch": 4.693866666666667,
      "grad_norm": 0.11247269064188004,
      "learning_rate": 2.0663333333333336e-05,
      "loss": 0.0035,
      "step": 88010
    },
    {
      "epoch": 4.6944,
      "grad_norm": 0.028118204325437546,
      "learning_rate": 2.0660000000000002e-05,
      "loss": 0.0031,
      "step": 88020
    },
    {
      "epoch": 4.694933333333333,
      "grad_norm": 0.2812044322490692,
      "learning_rate": 2.0656666666666668e-05,
      "loss": 0.0027,
      "step": 88030
    },
    {
      "epoch": 4.6954666666666665,
      "grad_norm": 0.4498577117919922,
      "learning_rate": 2.0653333333333334e-05,
      "loss": 0.0017,
      "step": 88040
    },
    {
      "epoch": 4.696,
      "grad_norm": 0.14059481024742126,
      "learning_rate": 2.065e-05,
      "loss": 0.0023,
      "step": 88050
    },
    {
      "epoch": 4.696533333333333,
      "grad_norm": 0.056234005838632584,
      "learning_rate": 2.0646666666666667e-05,
      "loss": 0.0027,
      "step": 88060
    },
    {
      "epoch": 4.697066666666666,
      "grad_norm": 0.056815698742866516,
      "learning_rate": 2.0643333333333333e-05,
      "loss": 0.0036,
      "step": 88070
    },
    {
      "epoch": 4.6975999999999996,
      "grad_norm": 0.30928701162338257,
      "learning_rate": 2.0640000000000002e-05,
      "loss": 0.0029,
      "step": 88080
    },
    {
      "epoch": 4.698133333333334,
      "grad_norm": 0.1687065064907074,
      "learning_rate": 2.063666666666667e-05,
      "loss": 0.0026,
      "step": 88090
    },
    {
      "epoch": 4.698666666666667,
      "grad_norm": 0.2530600130558014,
      "learning_rate": 2.0633333333333335e-05,
      "loss": 0.0032,
      "step": 88100
    },
    {
      "epoch": 4.6992,
      "grad_norm": 0.6466896533966064,
      "learning_rate": 2.063e-05,
      "loss": 0.0025,
      "step": 88110
    },
    {
      "epoch": 4.6997333333333335,
      "grad_norm": 0.16871310770511627,
      "learning_rate": 2.0626666666666667e-05,
      "loss": 0.003,
      "step": 88120
    },
    {
      "epoch": 4.700266666666667,
      "grad_norm": 0.16871508955955505,
      "learning_rate": 2.0623333333333333e-05,
      "loss": 0.0028,
      "step": 88130
    },
    {
      "epoch": 4.7008,
      "grad_norm": 0.05623527616262436,
      "learning_rate": 2.062e-05,
      "loss": 0.0022,
      "step": 88140
    },
    {
      "epoch": 4.701333333333333,
      "grad_norm": 0.39693111181259155,
      "learning_rate": 2.061666666666667e-05,
      "loss": 0.0028,
      "step": 88150
    },
    {
      "epoch": 4.701866666666667,
      "grad_norm": 2.386089770212152e-09,
      "learning_rate": 2.0613333333333335e-05,
      "loss": 0.0034,
      "step": 88160
    },
    {
      "epoch": 4.7024,
      "grad_norm": 0.08435483276844025,
      "learning_rate": 2.061e-05,
      "loss": 0.0025,
      "step": 88170
    },
    {
      "epoch": 4.702933333333333,
      "grad_norm": 0.08435121178627014,
      "learning_rate": 2.0606666666666667e-05,
      "loss": 0.0029,
      "step": 88180
    },
    {
      "epoch": 4.7034666666666665,
      "grad_norm": 0.16870278120040894,
      "learning_rate": 2.0603333333333337e-05,
      "loss": 0.0041,
      "step": 88190
    },
    {
      "epoch": 4.704,
      "grad_norm": 0.08435050398111343,
      "learning_rate": 2.06e-05,
      "loss": 0.0032,
      "step": 88200
    },
    {
      "epoch": 4.704533333333333,
      "grad_norm": 0.16870254278182983,
      "learning_rate": 2.0596666666666665e-05,
      "loss": 0.0018,
      "step": 88210
    },
    {
      "epoch": 4.705066666666666,
      "grad_norm": 2.030192680635423e-09,
      "learning_rate": 2.0593333333333335e-05,
      "loss": 0.0036,
      "step": 88220
    },
    {
      "epoch": 4.7056000000000004,
      "grad_norm": 0.22493453323841095,
      "learning_rate": 2.059e-05,
      "loss": 0.0033,
      "step": 88230
    },
    {
      "epoch": 4.706133333333334,
      "grad_norm": 0.19681531190872192,
      "learning_rate": 2.0586666666666667e-05,
      "loss": 0.0019,
      "step": 88240
    },
    {
      "epoch": 4.706666666666667,
      "grad_norm": 0.16870257258415222,
      "learning_rate": 2.0583333333333333e-05,
      "loss": 0.0026,
      "step": 88250
    },
    {
      "epoch": 4.7072,
      "grad_norm": 0.08434779196977615,
      "learning_rate": 2.0580000000000003e-05,
      "loss": 0.0037,
      "step": 88260
    },
    {
      "epoch": 4.7077333333333335,
      "grad_norm": 0.028116978704929352,
      "learning_rate": 2.0576666666666666e-05,
      "loss": 0.0028,
      "step": 88270
    },
    {
      "epoch": 4.708266666666667,
      "grad_norm": 0.11246877908706665,
      "learning_rate": 2.0573333333333332e-05,
      "loss": 0.0025,
      "step": 88280
    },
    {
      "epoch": 4.7088,
      "grad_norm": 0.16870199143886566,
      "learning_rate": 2.057e-05,
      "loss": 0.0019,
      "step": 88290
    },
    {
      "epoch": 4.709333333333333,
      "grad_norm": 0.22492529451847076,
      "learning_rate": 2.0566666666666667e-05,
      "loss": 0.0028,
      "step": 88300
    },
    {
      "epoch": 4.709866666666667,
      "grad_norm": 1.126874566078186,
      "learning_rate": 2.0563333333333334e-05,
      "loss": 0.0022,
      "step": 88310
    },
    {
      "epoch": 4.7104,
      "grad_norm": 3.0416920093756517e-09,
      "learning_rate": 2.0560000000000003e-05,
      "loss": 0.0028,
      "step": 88320
    },
    {
      "epoch": 4.710933333333333,
      "grad_norm": 0.08435053378343582,
      "learning_rate": 2.055666666666667e-05,
      "loss": 0.0034,
      "step": 88330
    },
    {
      "epoch": 4.7114666666666665,
      "grad_norm": 0.42174169421195984,
      "learning_rate": 2.0553333333333335e-05,
      "loss": 0.0019,
      "step": 88340
    },
    {
      "epoch": 4.712,
      "grad_norm": 0.19682633876800537,
      "learning_rate": 2.055e-05,
      "loss": 0.0028,
      "step": 88350
    },
    {
      "epoch": 4.712533333333333,
      "grad_norm": 0.22493523359298706,
      "learning_rate": 2.0546666666666668e-05,
      "loss": 0.0025,
      "step": 88360
    },
    {
      "epoch": 4.713066666666666,
      "grad_norm": 0.05623192712664604,
      "learning_rate": 2.0543333333333334e-05,
      "loss": 0.0024,
      "step": 88370
    },
    {
      "epoch": 4.7136,
      "grad_norm": 0.06596551090478897,
      "learning_rate": 2.054e-05,
      "loss": 0.0033,
      "step": 88380
    },
    {
      "epoch": 4.714133333333333,
      "grad_norm": 0.47797006368637085,
      "learning_rate": 2.053666666666667e-05,
      "loss": 0.0029,
      "step": 88390
    },
    {
      "epoch": 4.714666666666667,
      "grad_norm": 0.039435096085071564,
      "learning_rate": 2.0533333333333336e-05,
      "loss": 0.0026,
      "step": 88400
    },
    {
      "epoch": 4.7152,
      "grad_norm": 0.0281167384237051,
      "learning_rate": 2.053e-05,
      "loss": 0.0023,
      "step": 88410
    },
    {
      "epoch": 4.7157333333333336,
      "grad_norm": 0.08435273170471191,
      "learning_rate": 2.0526666666666668e-05,
      "loss": 0.0018,
      "step": 88420
    },
    {
      "epoch": 4.716266666666667,
      "grad_norm": 0.08434708416461945,
      "learning_rate": 2.0523333333333334e-05,
      "loss": 0.0026,
      "step": 88430
    },
    {
      "epoch": 4.7168,
      "grad_norm": 0.08434920758008957,
      "learning_rate": 2.052e-05,
      "loss": 0.0029,
      "step": 88440
    },
    {
      "epoch": 4.717333333333333,
      "grad_norm": 0.33898553252220154,
      "learning_rate": 2.0516666666666666e-05,
      "loss": 0.0026,
      "step": 88450
    },
    {
      "epoch": 4.717866666666667,
      "grad_norm": 0.08434896171092987,
      "learning_rate": 2.0513333333333336e-05,
      "loss": 0.0023,
      "step": 88460
    },
    {
      "epoch": 4.7184,
      "grad_norm": 0.28115254640579224,
      "learning_rate": 2.0510000000000002e-05,
      "loss": 0.0017,
      "step": 88470
    },
    {
      "epoch": 4.718933333333333,
      "grad_norm": 0.05623271316289902,
      "learning_rate": 2.0506666666666668e-05,
      "loss": 0.0035,
      "step": 88480
    },
    {
      "epoch": 4.7194666666666665,
      "grad_norm": 0.19682027399539948,
      "learning_rate": 2.0503333333333334e-05,
      "loss": 0.0024,
      "step": 88490
    },
    {
      "epoch": 4.72,
      "grad_norm": 0.028116481378674507,
      "learning_rate": 2.05e-05,
      "loss": 0.0037,
      "step": 88500
    },
    {
      "epoch": 4.720533333333333,
      "grad_norm": 0.140577033162117,
      "learning_rate": 2.0496666666666666e-05,
      "loss": 0.0017,
      "step": 88510
    },
    {
      "epoch": 4.721066666666666,
      "grad_norm": 1.155027151107788,
      "learning_rate": 2.0493333333333333e-05,
      "loss": 0.0036,
      "step": 88520
    },
    {
      "epoch": 4.7216000000000005,
      "grad_norm": 0.44986692070961,
      "learning_rate": 2.0490000000000002e-05,
      "loss": 0.0031,
      "step": 88530
    },
    {
      "epoch": 4.722133333333334,
      "grad_norm": 0.2811630368232727,
      "learning_rate": 2.0486666666666668e-05,
      "loss": 0.0033,
      "step": 88540
    },
    {
      "epoch": 4.722666666666667,
      "grad_norm": 0.028115006163716316,
      "learning_rate": 2.0483333333333334e-05,
      "loss": 0.0032,
      "step": 88550
    },
    {
      "epoch": 4.7232,
      "grad_norm": 4.506583994867697e-09,
      "learning_rate": 2.048e-05,
      "loss": 0.0033,
      "step": 88560
    },
    {
      "epoch": 4.723733333333334,
      "grad_norm": 0.028116879984736443,
      "learning_rate": 2.047666666666667e-05,
      "loss": 0.0033,
      "step": 88570
    },
    {
      "epoch": 4.724266666666667,
      "grad_norm": 0.14057740569114685,
      "learning_rate": 2.0473333333333333e-05,
      "loss": 0.0025,
      "step": 88580
    },
    {
      "epoch": 4.7248,
      "grad_norm": 0.028114669024944305,
      "learning_rate": 2.047e-05,
      "loss": 0.0027,
      "step": 88590
    },
    {
      "epoch": 4.725333333333333,
      "grad_norm": 0.19680191576480865,
      "learning_rate": 2.046666666666667e-05,
      "loss": 0.003,
      "step": 88600
    },
    {
      "epoch": 4.725866666666667,
      "grad_norm": 0.11246295273303986,
      "learning_rate": 2.0463333333333334e-05,
      "loss": 0.0016,
      "step": 88610
    },
    {
      "epoch": 4.7264,
      "grad_norm": 0.08434496074914932,
      "learning_rate": 2.046e-05,
      "loss": 0.0033,
      "step": 88620
    },
    {
      "epoch": 4.726933333333333,
      "grad_norm": 0.1686898171901703,
      "learning_rate": 2.0456666666666667e-05,
      "loss": 0.0029,
      "step": 88630
    },
    {
      "epoch": 4.7274666666666665,
      "grad_norm": 0.30925196409225464,
      "learning_rate": 2.0453333333333336e-05,
      "loss": 0.0022,
      "step": 88640
    },
    {
      "epoch": 4.728,
      "grad_norm": 0.028116174042224884,
      "learning_rate": 2.045e-05,
      "loss": 0.0027,
      "step": 88650
    },
    {
      "epoch": 4.728533333333333,
      "grad_norm": 0.11245673894882202,
      "learning_rate": 2.0446666666666665e-05,
      "loss": 0.0032,
      "step": 88660
    },
    {
      "epoch": 4.729066666666666,
      "grad_norm": 0.19680893421173096,
      "learning_rate": 2.0443333333333335e-05,
      "loss": 0.0037,
      "step": 88670
    },
    {
      "epoch": 4.7296,
      "grad_norm": 0.16869260370731354,
      "learning_rate": 2.044e-05,
      "loss": 0.0017,
      "step": 88680
    },
    {
      "epoch": 4.730133333333333,
      "grad_norm": 0.08434059470891953,
      "learning_rate": 2.0436666666666667e-05,
      "loss": 0.0033,
      "step": 88690
    },
    {
      "epoch": 4.730666666666667,
      "grad_norm": 0.22492440044879913,
      "learning_rate": 2.0433333333333336e-05,
      "loss": 0.0023,
      "step": 88700
    },
    {
      "epoch": 4.7312,
      "grad_norm": 0.02811582386493683,
      "learning_rate": 2.0430000000000003e-05,
      "loss": 0.0023,
      "step": 88710
    },
    {
      "epoch": 4.731733333333334,
      "grad_norm": 0.05622708797454834,
      "learning_rate": 2.042666666666667e-05,
      "loss": 0.0027,
      "step": 88720
    },
    {
      "epoch": 4.732266666666667,
      "grad_norm": 0.08434253931045532,
      "learning_rate": 2.0423333333333335e-05,
      "loss": 0.0028,
      "step": 88730
    },
    {
      "epoch": 4.7328,
      "grad_norm": 0.4498637914657593,
      "learning_rate": 2.042e-05,
      "loss": 0.0022,
      "step": 88740
    },
    {
      "epoch": 4.733333333333333,
      "grad_norm": 0.5622628331184387,
      "learning_rate": 2.0416666666666667e-05,
      "loss": 0.0024,
      "step": 88750
    },
    {
      "epoch": 4.733866666666667,
      "grad_norm": 0.16869831085205078,
      "learning_rate": 2.0413333333333333e-05,
      "loss": 0.0029,
      "step": 88760
    },
    {
      "epoch": 4.7344,
      "grad_norm": 0.05622824281454086,
      "learning_rate": 2.0410000000000003e-05,
      "loss": 0.0019,
      "step": 88770
    },
    {
      "epoch": 4.734933333333333,
      "grad_norm": 0.11245930939912796,
      "learning_rate": 2.040666666666667e-05,
      "loss": 0.0029,
      "step": 88780
    },
    {
      "epoch": 4.7354666666666665,
      "grad_norm": 0.2530340552330017,
      "learning_rate": 2.0403333333333335e-05,
      "loss": 0.002,
      "step": 88790
    },
    {
      "epoch": 4.736,
      "grad_norm": 4.159999011932314e-09,
      "learning_rate": 2.04e-05,
      "loss": 0.0028,
      "step": 88800
    },
    {
      "epoch": 4.736533333333333,
      "grad_norm": 4.60209337305173e-09,
      "learning_rate": 2.0396666666666667e-05,
      "loss": 0.0019,
      "step": 88810
    },
    {
      "epoch": 4.737066666666666,
      "grad_norm": 0.33736780285835266,
      "learning_rate": 2.0393333333333333e-05,
      "loss": 0.0025,
      "step": 88820
    },
    {
      "epoch": 4.7376000000000005,
      "grad_norm": 0.11245511472225189,
      "learning_rate": 2.039e-05,
      "loss": 0.0034,
      "step": 88830
    },
    {
      "epoch": 4.738133333333334,
      "grad_norm": 0.19679850339889526,
      "learning_rate": 2.038666666666667e-05,
      "loss": 0.0035,
      "step": 88840
    },
    {
      "epoch": 4.738666666666667,
      "grad_norm": 0.22491680085659027,
      "learning_rate": 2.0383333333333335e-05,
      "loss": 0.0016,
      "step": 88850
    },
    {
      "epoch": 4.7392,
      "grad_norm": 0.22492638230323792,
      "learning_rate": 2.038e-05,
      "loss": 0.0026,
      "step": 88860
    },
    {
      "epoch": 4.739733333333334,
      "grad_norm": 0.19680365920066833,
      "learning_rate": 2.0376666666666668e-05,
      "loss": 0.0027,
      "step": 88870
    },
    {
      "epoch": 4.740266666666667,
      "grad_norm": 0.028113681823015213,
      "learning_rate": 2.0373333333333334e-05,
      "loss": 0.0026,
      "step": 88880
    },
    {
      "epoch": 4.7408,
      "grad_norm": 0.11246143281459808,
      "learning_rate": 2.037e-05,
      "loss": 0.0035,
      "step": 88890
    },
    {
      "epoch": 4.741333333333333,
      "grad_norm": 0.14057648181915283,
      "learning_rate": 2.0366666666666666e-05,
      "loss": 0.0017,
      "step": 88900
    },
    {
      "epoch": 4.741866666666667,
      "grad_norm": 0.1124604269862175,
      "learning_rate": 2.0363333333333335e-05,
      "loss": 0.0024,
      "step": 88910
    },
    {
      "epoch": 4.7424,
      "grad_norm": 0.0562313050031662,
      "learning_rate": 2.036e-05,
      "loss": 0.0024,
      "step": 88920
    },
    {
      "epoch": 4.742933333333333,
      "grad_norm": 3.155389949327514e-09,
      "learning_rate": 2.0356666666666668e-05,
      "loss": 0.0022,
      "step": 88930
    },
    {
      "epoch": 4.7434666666666665,
      "grad_norm": 0.05622781440615654,
      "learning_rate": 2.0353333333333334e-05,
      "loss": 0.0033,
      "step": 88940
    },
    {
      "epoch": 4.744,
      "grad_norm": 0.33736032247543335,
      "learning_rate": 2.035e-05,
      "loss": 0.0027,
      "step": 88950
    },
    {
      "epoch": 4.744533333333333,
      "grad_norm": 0.08434652537107468,
      "learning_rate": 2.0346666666666666e-05,
      "loss": 0.003,
      "step": 88960
    },
    {
      "epoch": 4.745066666666666,
      "grad_norm": 0.1124604120850563,
      "learning_rate": 2.0343333333333332e-05,
      "loss": 0.0032,
      "step": 88970
    },
    {
      "epoch": 4.7456,
      "grad_norm": 0.19680078327655792,
      "learning_rate": 2.0340000000000002e-05,
      "loss": 0.0035,
      "step": 88980
    },
    {
      "epoch": 4.746133333333333,
      "grad_norm": 0.16868877410888672,
      "learning_rate": 2.0336666666666668e-05,
      "loss": 0.0017,
      "step": 88990
    },
    {
      "epoch": 4.746666666666667,
      "grad_norm": 0.1405726969242096,
      "learning_rate": 2.0333333333333334e-05,
      "loss": 0.0027,
      "step": 89000
    },
    {
      "epoch": 4.7472,
      "grad_norm": 0.028113210573792458,
      "learning_rate": 2.033e-05,
      "loss": 0.0039,
      "step": 89010
    },
    {
      "epoch": 4.747733333333334,
      "grad_norm": 2.549034316956522e-09,
      "learning_rate": 2.032666666666667e-05,
      "loss": 0.0031,
      "step": 89020
    },
    {
      "epoch": 4.748266666666667,
      "grad_norm": 0.22490528225898743,
      "learning_rate": 2.0323333333333332e-05,
      "loss": 0.0029,
      "step": 89030
    },
    {
      "epoch": 4.7488,
      "grad_norm": 0.056228239089250565,
      "learning_rate": 2.032e-05,
      "loss": 0.0018,
      "step": 89040
    },
    {
      "epoch": 4.749333333333333,
      "grad_norm": 0.08434072881937027,
      "learning_rate": 2.0316666666666668e-05,
      "loss": 0.0042,
      "step": 89050
    },
    {
      "epoch": 4.749866666666667,
      "grad_norm": 0.14057192206382751,
      "learning_rate": 2.0313333333333334e-05,
      "loss": 0.0029,
      "step": 89060
    },
    {
      "epoch": 4.7504,
      "grad_norm": 0.3373715877532959,
      "learning_rate": 2.031e-05,
      "loss": 0.0036,
      "step": 89070
    },
    {
      "epoch": 4.750933333333333,
      "grad_norm": 0.19678589701652527,
      "learning_rate": 2.030666666666667e-05,
      "loss": 0.0018,
      "step": 89080
    },
    {
      "epoch": 4.7514666666666665,
      "grad_norm": 0.22492262721061707,
      "learning_rate": 2.0303333333333336e-05,
      "loss": 0.0044,
      "step": 89090
    },
    {
      "epoch": 4.752,
      "grad_norm": 0.19679389894008636,
      "learning_rate": 2.0300000000000002e-05,
      "loss": 0.0027,
      "step": 89100
    },
    {
      "epoch": 4.752533333333333,
      "grad_norm": 0.05622701346874237,
      "learning_rate": 2.0296666666666668e-05,
      "loss": 0.0026,
      "step": 89110
    },
    {
      "epoch": 4.753066666666666,
      "grad_norm": 1.956128716468811,
      "learning_rate": 2.0293333333333334e-05,
      "loss": 0.0025,
      "step": 89120
    },
    {
      "epoch": 4.7536000000000005,
      "grad_norm": 0.14056773483753204,
      "learning_rate": 2.029e-05,
      "loss": 0.0027,
      "step": 89130
    },
    {
      "epoch": 4.754133333333334,
      "grad_norm": 0.3935593366622925,
      "learning_rate": 2.0286666666666667e-05,
      "loss": 0.0035,
      "step": 89140
    },
    {
      "epoch": 4.754666666666667,
      "grad_norm": 0.02811296284198761,
      "learning_rate": 2.0283333333333336e-05,
      "loss": 0.0017,
      "step": 89150
    },
    {
      "epoch": 4.7552,
      "grad_norm": 0.2530348598957062,
      "learning_rate": 2.0280000000000002e-05,
      "loss": 0.003,
      "step": 89160
    },
    {
      "epoch": 4.755733333333334,
      "grad_norm": 0.19678114354610443,
      "learning_rate": 2.027666666666667e-05,
      "loss": 0.002,
      "step": 89170
    },
    {
      "epoch": 4.756266666666667,
      "grad_norm": 2.327126047418915e-09,
      "learning_rate": 2.0273333333333335e-05,
      "loss": 0.0034,
      "step": 89180
    },
    {
      "epoch": 4.7568,
      "grad_norm": 0.12712301313877106,
      "learning_rate": 2.027e-05,
      "loss": 0.003,
      "step": 89190
    },
    {
      "epoch": 4.757333333333333,
      "grad_norm": 0.3373441994190216,
      "learning_rate": 2.0266666666666667e-05,
      "loss": 0.0025,
      "step": 89200
    },
    {
      "epoch": 4.757866666666667,
      "grad_norm": 0.14056396484375,
      "learning_rate": 2.0263333333333333e-05,
      "loss": 0.0022,
      "step": 89210
    },
    {
      "epoch": 4.7584,
      "grad_norm": 0.028111524879932404,
      "learning_rate": 2.0260000000000003e-05,
      "loss": 0.0028,
      "step": 89220
    },
    {
      "epoch": 4.758933333333333,
      "grad_norm": 0.05622488260269165,
      "learning_rate": 2.025666666666667e-05,
      "loss": 0.0034,
      "step": 89230
    },
    {
      "epoch": 4.7594666666666665,
      "grad_norm": 0.3654584586620331,
      "learning_rate": 2.0253333333333335e-05,
      "loss": 0.0039,
      "step": 89240
    },
    {
      "epoch": 4.76,
      "grad_norm": 0.2811451554298401,
      "learning_rate": 2.025e-05,
      "loss": 0.0028,
      "step": 89250
    },
    {
      "epoch": 4.760533333333333,
      "grad_norm": 0.028112124651670456,
      "learning_rate": 2.0246666666666667e-05,
      "loss": 0.003,
      "step": 89260
    },
    {
      "epoch": 4.761066666666666,
      "grad_norm": 0.16867037117481232,
      "learning_rate": 2.0243333333333333e-05,
      "loss": 0.0043,
      "step": 89270
    },
    {
      "epoch": 4.7616,
      "grad_norm": 0.1686743199825287,
      "learning_rate": 2.024e-05,
      "loss": 0.0023,
      "step": 89280
    },
    {
      "epoch": 4.762133333333333,
      "grad_norm": 0.39355480670928955,
      "learning_rate": 2.023666666666667e-05,
      "loss": 0.0029,
      "step": 89290
    },
    {
      "epoch": 4.762666666666667,
      "grad_norm": 0.9157068729400635,
      "learning_rate": 2.0233333333333335e-05,
      "loss": 0.002,
      "step": 89300
    },
    {
      "epoch": 4.7632,
      "grad_norm": 0.4778861403465271,
      "learning_rate": 2.023e-05,
      "loss": 0.0033,
      "step": 89310
    },
    {
      "epoch": 4.763733333333334,
      "grad_norm": 0.08434131741523743,
      "learning_rate": 2.0226666666666667e-05,
      "loss": 0.0024,
      "step": 89320
    },
    {
      "epoch": 4.764266666666667,
      "grad_norm": 0.19679468870162964,
      "learning_rate": 2.0223333333333333e-05,
      "loss": 0.0038,
      "step": 89330
    },
    {
      "epoch": 4.7648,
      "grad_norm": 0.16866865754127502,
      "learning_rate": 2.022e-05,
      "loss": 0.0032,
      "step": 89340
    },
    {
      "epoch": 4.765333333333333,
      "grad_norm": 0.14056408405303955,
      "learning_rate": 2.0216666666666666e-05,
      "loss": 0.0018,
      "step": 89350
    },
    {
      "epoch": 4.765866666666667,
      "grad_norm": 0.16866791248321533,
      "learning_rate": 2.0213333333333335e-05,
      "loss": 0.0034,
      "step": 89360
    },
    {
      "epoch": 4.7664,
      "grad_norm": 0.2530037462711334,
      "learning_rate": 2.021e-05,
      "loss": 0.0024,
      "step": 89370
    },
    {
      "epoch": 4.766933333333333,
      "grad_norm": 0.08433442562818527,
      "learning_rate": 2.0206666666666667e-05,
      "loss": 0.0027,
      "step": 89380
    },
    {
      "epoch": 4.7674666666666665,
      "grad_norm": 0.05622311681509018,
      "learning_rate": 2.0203333333333334e-05,
      "loss": 0.0034,
      "step": 89390
    },
    {
      "epoch": 4.768,
      "grad_norm": 0.365454763174057,
      "learning_rate": 2.0200000000000003e-05,
      "loss": 0.0037,
      "step": 89400
    },
    {
      "epoch": 4.768533333333333,
      "grad_norm": 0.09224744886159897,
      "learning_rate": 2.0196666666666666e-05,
      "loss": 0.0039,
      "step": 89410
    },
    {
      "epoch": 4.769066666666666,
      "grad_norm": 0.08433671295642853,
      "learning_rate": 2.0193333333333332e-05,
      "loss": 0.0037,
      "step": 89420
    },
    {
      "epoch": 4.7696,
      "grad_norm": 0.14056067168712616,
      "learning_rate": 2.019e-05,
      "loss": 0.0027,
      "step": 89430
    },
    {
      "epoch": 4.770133333333334,
      "grad_norm": 0.1405581831932068,
      "learning_rate": 2.0186666666666668e-05,
      "loss": 0.0021,
      "step": 89440
    },
    {
      "epoch": 4.770666666666667,
      "grad_norm": 0.3935566842556,
      "learning_rate": 2.0183333333333334e-05,
      "loss": 0.0028,
      "step": 89450
    },
    {
      "epoch": 4.7712,
      "grad_norm": 0.2248978614807129,
      "learning_rate": 2.0180000000000003e-05,
      "loss": 0.003,
      "step": 89460
    },
    {
      "epoch": 4.771733333333334,
      "grad_norm": 0.36544641852378845,
      "learning_rate": 2.017666666666667e-05,
      "loss": 0.0029,
      "step": 89470
    },
    {
      "epoch": 4.772266666666667,
      "grad_norm": 0.11244593560695648,
      "learning_rate": 2.0173333333333332e-05,
      "loss": 0.0019,
      "step": 89480
    },
    {
      "epoch": 4.7728,
      "grad_norm": 0.44979655742645264,
      "learning_rate": 2.017e-05,
      "loss": 0.003,
      "step": 89490
    },
    {
      "epoch": 4.773333333333333,
      "grad_norm": 0.16866670548915863,
      "learning_rate": 2.0166666666666668e-05,
      "loss": 0.0027,
      "step": 89500
    },
    {
      "epoch": 4.773866666666667,
      "grad_norm": 0.14056751132011414,
      "learning_rate": 2.0163333333333334e-05,
      "loss": 0.0035,
      "step": 89510
    },
    {
      "epoch": 4.7744,
      "grad_norm": 0.05622284859418869,
      "learning_rate": 2.016e-05,
      "loss": 0.0028,
      "step": 89520
    },
    {
      "epoch": 4.774933333333333,
      "grad_norm": 0.19677704572677612,
      "learning_rate": 2.015666666666667e-05,
      "loss": 0.0025,
      "step": 89530
    },
    {
      "epoch": 4.7754666666666665,
      "grad_norm": 0.14055897295475006,
      "learning_rate": 2.0153333333333336e-05,
      "loss": 0.0029,
      "step": 89540
    },
    {
      "epoch": 4.776,
      "grad_norm": 0.1692633479833603,
      "learning_rate": 2.0150000000000002e-05,
      "loss": 0.0025,
      "step": 89550
    },
    {
      "epoch": 4.776533333333333,
      "grad_norm": 0.16866564750671387,
      "learning_rate": 2.0146666666666668e-05,
      "loss": 0.0029,
      "step": 89560
    },
    {
      "epoch": 4.777066666666666,
      "grad_norm": 0.33734071254730225,
      "learning_rate": 2.0143333333333334e-05,
      "loss": 0.0034,
      "step": 89570
    },
    {
      "epoch": 4.7776,
      "grad_norm": 0.3373558521270752,
      "learning_rate": 2.014e-05,
      "loss": 0.0024,
      "step": 89580
    },
    {
      "epoch": 4.778133333333333,
      "grad_norm": 0.0803014263510704,
      "learning_rate": 2.0136666666666666e-05,
      "loss": 0.0031,
      "step": 89590
    },
    {
      "epoch": 4.778666666666666,
      "grad_norm": 0.028111083433032036,
      "learning_rate": 2.0133333333333336e-05,
      "loss": 0.0019,
      "step": 89600
    },
    {
      "epoch": 4.7792,
      "grad_norm": 0.11244942992925644,
      "learning_rate": 2.0130000000000002e-05,
      "loss": 0.0026,
      "step": 89610
    },
    {
      "epoch": 4.779733333333334,
      "grad_norm": 0.05622345209121704,
      "learning_rate": 2.0126666666666668e-05,
      "loss": 0.0034,
      "step": 89620
    },
    {
      "epoch": 4.780266666666667,
      "grad_norm": 0.11244173347949982,
      "learning_rate": 2.0123333333333334e-05,
      "loss": 0.0035,
      "step": 89630
    },
    {
      "epoch": 4.7808,
      "grad_norm": 1.8296238968318335e-09,
      "learning_rate": 2.012e-05,
      "loss": 0.0035,
      "step": 89640
    },
    {
      "epoch": 4.781333333333333,
      "grad_norm": 0.1124521866440773,
      "learning_rate": 2.0116666666666667e-05,
      "loss": 0.0037,
      "step": 89650
    },
    {
      "epoch": 4.781866666666667,
      "grad_norm": 0.3654560446739197,
      "learning_rate": 2.0113333333333333e-05,
      "loss": 0.0026,
      "step": 89660
    },
    {
      "epoch": 4.7824,
      "grad_norm": 0.03711161017417908,
      "learning_rate": 2.0110000000000002e-05,
      "loss": 0.0027,
      "step": 89670
    },
    {
      "epoch": 4.782933333333333,
      "grad_norm": 0.02811104990541935,
      "learning_rate": 2.010666666666667e-05,
      "loss": 0.0021,
      "step": 89680
    },
    {
      "epoch": 4.7834666666666665,
      "grad_norm": 0.14056000113487244,
      "learning_rate": 2.0103333333333335e-05,
      "loss": 0.0024,
      "step": 89690
    },
    {
      "epoch": 4.784,
      "grad_norm": 0.05622471123933792,
      "learning_rate": 2.01e-05,
      "loss": 0.0021,
      "step": 89700
    },
    {
      "epoch": 4.784533333333333,
      "grad_norm": 0.08433137089014053,
      "learning_rate": 2.0096666666666667e-05,
      "loss": 0.0029,
      "step": 89710
    },
    {
      "epoch": 4.785066666666666,
      "grad_norm": 0.1784791648387909,
      "learning_rate": 2.0093333333333333e-05,
      "loss": 0.0022,
      "step": 89720
    },
    {
      "epoch": 4.7856,
      "grad_norm": 0.3654625415802002,
      "learning_rate": 2.009e-05,
      "loss": 0.0029,
      "step": 89730
    },
    {
      "epoch": 4.786133333333334,
      "grad_norm": 0.08433236181735992,
      "learning_rate": 2.008666666666667e-05,
      "loss": 0.0026,
      "step": 89740
    },
    {
      "epoch": 4.786666666666667,
      "grad_norm": 0.16866780817508698,
      "learning_rate": 2.0083333333333335e-05,
      "loss": 0.0025,
      "step": 89750
    },
    {
      "epoch": 4.7872,
      "grad_norm": 5.503312916488312e-09,
      "learning_rate": 2.008e-05,
      "loss": 0.0018,
      "step": 89760
    },
    {
      "epoch": 4.787733333333334,
      "grad_norm": 0.1405528336763382,
      "learning_rate": 2.0076666666666667e-05,
      "loss": 0.0044,
      "step": 89770
    },
    {
      "epoch": 4.788266666666667,
      "grad_norm": 0.1405552476644516,
      "learning_rate": 2.0073333333333337e-05,
      "loss": 0.0028,
      "step": 89780
    },
    {
      "epoch": 4.7888,
      "grad_norm": 0.11244125664234161,
      "learning_rate": 2.007e-05,
      "loss": 0.0026,
      "step": 89790
    },
    {
      "epoch": 4.789333333333333,
      "grad_norm": 1.1194216664733858e-09,
      "learning_rate": 2.0066666666666665e-05,
      "loss": 0.0032,
      "step": 89800
    },
    {
      "epoch": 4.789866666666667,
      "grad_norm": 0.11244206875562668,
      "learning_rate": 2.0063333333333335e-05,
      "loss": 0.002,
      "step": 89810
    },
    {
      "epoch": 4.7904,
      "grad_norm": 0.42168524861335754,
      "learning_rate": 2.006e-05,
      "loss": 0.0029,
      "step": 89820
    },
    {
      "epoch": 4.790933333333333,
      "grad_norm": 0.2529960572719574,
      "learning_rate": 2.0056666666666667e-05,
      "loss": 0.0023,
      "step": 89830
    },
    {
      "epoch": 4.7914666666666665,
      "grad_norm": 0.02811054140329361,
      "learning_rate": 2.0053333333333337e-05,
      "loss": 0.0029,
      "step": 89840
    },
    {
      "epoch": 4.792,
      "grad_norm": 0.08433043211698532,
      "learning_rate": 2.0050000000000003e-05,
      "loss": 0.0021,
      "step": 89850
    },
    {
      "epoch": 4.792533333333333,
      "grad_norm": 5.619655851774041e-10,
      "learning_rate": 2.0046666666666666e-05,
      "loss": 0.0031,
      "step": 89860
    },
    {
      "epoch": 4.793066666666666,
      "grad_norm": 0.22488923370838165,
      "learning_rate": 2.0043333333333332e-05,
      "loss": 0.003,
      "step": 89870
    },
    {
      "epoch": 4.7936,
      "grad_norm": 0.19676809012889862,
      "learning_rate": 2.004e-05,
      "loss": 0.0014,
      "step": 89880
    },
    {
      "epoch": 4.794133333333333,
      "grad_norm": 0.2811041474342346,
      "learning_rate": 2.0036666666666667e-05,
      "loss": 0.0036,
      "step": 89890
    },
    {
      "epoch": 4.794666666666666,
      "grad_norm": 0.11244266480207443,
      "learning_rate": 2.0033333333333334e-05,
      "loss": 0.0026,
      "step": 89900
    },
    {
      "epoch": 4.7952,
      "grad_norm": 0.028111204504966736,
      "learning_rate": 2.0030000000000003e-05,
      "loss": 0.0024,
      "step": 89910
    },
    {
      "epoch": 4.795733333333334,
      "grad_norm": 0.2811039984226227,
      "learning_rate": 2.002666666666667e-05,
      "loss": 0.0023,
      "step": 89920
    },
    {
      "epoch": 4.796266666666667,
      "grad_norm": 0.05622031167149544,
      "learning_rate": 2.0023333333333335e-05,
      "loss": 0.0014,
      "step": 89930
    },
    {
      "epoch": 4.7968,
      "grad_norm": 0.11244139075279236,
      "learning_rate": 2.002e-05,
      "loss": 0.0031,
      "step": 89940
    },
    {
      "epoch": 4.7973333333333334,
      "grad_norm": 0.028110302984714508,
      "learning_rate": 2.0016666666666668e-05,
      "loss": 0.0032,
      "step": 89950
    },
    {
      "epoch": 4.797866666666667,
      "grad_norm": 0.1124395877122879,
      "learning_rate": 2.0013333333333334e-05,
      "loss": 0.0024,
      "step": 89960
    },
    {
      "epoch": 4.7984,
      "grad_norm": 0.25299516320228577,
      "learning_rate": 2.001e-05,
      "loss": 0.0022,
      "step": 89970
    },
    {
      "epoch": 4.798933333333333,
      "grad_norm": 0.1967717707157135,
      "learning_rate": 2.000666666666667e-05,
      "loss": 0.0024,
      "step": 89980
    },
    {
      "epoch": 4.7994666666666665,
      "grad_norm": 0.056220654398202896,
      "learning_rate": 2.0003333333333336e-05,
      "loss": 0.0034,
      "step": 89990
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.028110351413488388,
      "learning_rate": 2e-05,
      "loss": 0.0039,
      "step": 90000
    },
    {
      "epoch": 4.800533333333333,
      "grad_norm": 0.22489356994628906,
      "learning_rate": 1.9996666666666668e-05,
      "loss": 0.0029,
      "step": 90010
    },
    {
      "epoch": 4.801066666666666,
      "grad_norm": 0.253008633852005,
      "learning_rate": 1.9993333333333334e-05,
      "loss": 0.0026,
      "step": 90020
    },
    {
      "epoch": 4.8016,
      "grad_norm": 0.2811078727245331,
      "learning_rate": 1.999e-05,
      "loss": 0.0041,
      "step": 90030
    },
    {
      "epoch": 4.802133333333334,
      "grad_norm": 0.05621984973549843,
      "learning_rate": 1.9986666666666666e-05,
      "loss": 0.0022,
      "step": 90040
    },
    {
      "epoch": 4.802666666666667,
      "grad_norm": 0.22489073872566223,
      "learning_rate": 1.9983333333333336e-05,
      "loss": 0.0046,
      "step": 90050
    },
    {
      "epoch": 4.8032,
      "grad_norm": 0.02811010368168354,
      "learning_rate": 1.9980000000000002e-05,
      "loss": 0.0019,
      "step": 90060
    },
    {
      "epoch": 4.803733333333334,
      "grad_norm": 0.22487999498844147,
      "learning_rate": 1.9976666666666668e-05,
      "loss": 0.0029,
      "step": 90070
    },
    {
      "epoch": 4.804266666666667,
      "grad_norm": 0.16865499317646027,
      "learning_rate": 1.9973333333333334e-05,
      "loss": 0.0028,
      "step": 90080
    },
    {
      "epoch": 4.8048,
      "grad_norm": 0.3092046082019806,
      "learning_rate": 1.997e-05,
      "loss": 0.003,
      "step": 90090
    },
    {
      "epoch": 4.8053333333333335,
      "grad_norm": 0.16866636276245117,
      "learning_rate": 1.9966666666666666e-05,
      "loss": 0.0027,
      "step": 90100
    },
    {
      "epoch": 4.805866666666667,
      "grad_norm": 0.056219421327114105,
      "learning_rate": 1.9963333333333332e-05,
      "loss": 0.0033,
      "step": 90110
    },
    {
      "epoch": 4.8064,
      "grad_norm": 0.08433344215154648,
      "learning_rate": 1.9960000000000002e-05,
      "loss": 0.0029,
      "step": 90120
    },
    {
      "epoch": 4.806933333333333,
      "grad_norm": 0.5642989873886108,
      "learning_rate": 1.9956666666666668e-05,
      "loss": 0.0047,
      "step": 90130
    },
    {
      "epoch": 4.8074666666666666,
      "grad_norm": 0.028109822422266006,
      "learning_rate": 1.9953333333333334e-05,
      "loss": 0.0049,
      "step": 90140
    },
    {
      "epoch": 4.808,
      "grad_norm": 0.056220076978206635,
      "learning_rate": 1.995e-05,
      "loss": 0.0029,
      "step": 90150
    },
    {
      "epoch": 4.808533333333333,
      "grad_norm": 0.028109772130846977,
      "learning_rate": 1.9946666666666667e-05,
      "loss": 0.0025,
      "step": 90160
    },
    {
      "epoch": 4.809066666666666,
      "grad_norm": 0.08433452248573303,
      "learning_rate": 1.9943333333333333e-05,
      "loss": 0.0036,
      "step": 90170
    },
    {
      "epoch": 4.8096,
      "grad_norm": 0.14054854214191437,
      "learning_rate": 1.994e-05,
      "loss": 0.0022,
      "step": 90180
    },
    {
      "epoch": 4.810133333333333,
      "grad_norm": 0.056220486760139465,
      "learning_rate": 1.993666666666667e-05,
      "loss": 0.0028,
      "step": 90190
    },
    {
      "epoch": 4.810666666666666,
      "grad_norm": 0.08433244377374649,
      "learning_rate": 1.9933333333333334e-05,
      "loss": 0.0023,
      "step": 90200
    },
    {
      "epoch": 4.8112,
      "grad_norm": 0.0843292772769928,
      "learning_rate": 1.993e-05,
      "loss": 0.0032,
      "step": 90210
    },
    {
      "epoch": 4.811733333333334,
      "grad_norm": 0.13028773665428162,
      "learning_rate": 1.992666666666667e-05,
      "loss": 0.0038,
      "step": 90220
    },
    {
      "epoch": 4.812266666666667,
      "grad_norm": 0.047823064029216766,
      "learning_rate": 1.9923333333333336e-05,
      "loss": 0.0041,
      "step": 90230
    },
    {
      "epoch": 4.8128,
      "grad_norm": 0.08432993292808533,
      "learning_rate": 1.992e-05,
      "loss": 0.0035,
      "step": 90240
    },
    {
      "epoch": 4.8133333333333335,
      "grad_norm": 0.05622078478336334,
      "learning_rate": 1.9916666666666665e-05,
      "loss": 0.0032,
      "step": 90250
    },
    {
      "epoch": 4.813866666666667,
      "grad_norm": 0.2529832124710083,
      "learning_rate": 1.9913333333333335e-05,
      "loss": 0.0037,
      "step": 90260
    },
    {
      "epoch": 4.8144,
      "grad_norm": 0.19676810503005981,
      "learning_rate": 1.991e-05,
      "loss": 0.0028,
      "step": 90270
    },
    {
      "epoch": 4.814933333333333,
      "grad_norm": 0.2810949683189392,
      "learning_rate": 1.9906666666666667e-05,
      "loss": 0.0026,
      "step": 90280
    },
    {
      "epoch": 4.815466666666667,
      "grad_norm": 0.1405434012413025,
      "learning_rate": 1.9903333333333336e-05,
      "loss": 0.0038,
      "step": 90290
    },
    {
      "epoch": 4.816,
      "grad_norm": 0.028109567239880562,
      "learning_rate": 1.9900000000000003e-05,
      "loss": 0.0014,
      "step": 90300
    },
    {
      "epoch": 4.816533333333333,
      "grad_norm": 0.08432599902153015,
      "learning_rate": 1.9896666666666665e-05,
      "loss": 0.0024,
      "step": 90310
    },
    {
      "epoch": 4.817066666666666,
      "grad_norm": 0.02810947597026825,
      "learning_rate": 1.9893333333333335e-05,
      "loss": 0.0031,
      "step": 90320
    },
    {
      "epoch": 4.8176,
      "grad_norm": 0.056221019476652145,
      "learning_rate": 1.989e-05,
      "loss": 0.0024,
      "step": 90330
    },
    {
      "epoch": 4.818133333333334,
      "grad_norm": 0.2529967129230499,
      "learning_rate": 1.9886666666666667e-05,
      "loss": 0.0039,
      "step": 90340
    },
    {
      "epoch": 4.818666666666667,
      "grad_norm": 0.25297582149505615,
      "learning_rate": 1.9883333333333333e-05,
      "loss": 0.0022,
      "step": 90350
    },
    {
      "epoch": 4.8192,
      "grad_norm": 0.08432931452989578,
      "learning_rate": 1.9880000000000003e-05,
      "loss": 0.0029,
      "step": 90360
    },
    {
      "epoch": 4.819733333333334,
      "grad_norm": 0.3935530185699463,
      "learning_rate": 1.987666666666667e-05,
      "loss": 0.0028,
      "step": 90370
    },
    {
      "epoch": 4.820266666666667,
      "grad_norm": 0.25296932458877563,
      "learning_rate": 1.9873333333333335e-05,
      "loss": 0.0026,
      "step": 90380
    },
    {
      "epoch": 4.8208,
      "grad_norm": 0.2248774617910385,
      "learning_rate": 1.987e-05,
      "loss": 0.0028,
      "step": 90390
    },
    {
      "epoch": 4.8213333333333335,
      "grad_norm": 0.14055001735687256,
      "learning_rate": 1.9866666666666667e-05,
      "loss": 0.0025,
      "step": 90400
    },
    {
      "epoch": 4.821866666666667,
      "grad_norm": 0.2248721718788147,
      "learning_rate": 1.9863333333333333e-05,
      "loss": 0.0021,
      "step": 90410
    },
    {
      "epoch": 4.8224,
      "grad_norm": 0.44977161288261414,
      "learning_rate": 1.986e-05,
      "loss": 0.0027,
      "step": 90420
    },
    {
      "epoch": 4.822933333333333,
      "grad_norm": 0.39351004362106323,
      "learning_rate": 1.985666666666667e-05,
      "loss": 0.0036,
      "step": 90430
    },
    {
      "epoch": 4.823466666666667,
      "grad_norm": 0.16865240037441254,
      "learning_rate": 1.9853333333333335e-05,
      "loss": 0.0038,
      "step": 90440
    },
    {
      "epoch": 4.824,
      "grad_norm": 0.16865378618240356,
      "learning_rate": 1.985e-05,
      "loss": 0.0028,
      "step": 90450
    },
    {
      "epoch": 4.824533333333333,
      "grad_norm": 0.3372977674007416,
      "learning_rate": 1.9846666666666668e-05,
      "loss": 0.0025,
      "step": 90460
    },
    {
      "epoch": 4.825066666666666,
      "grad_norm": 0.33731958270072937,
      "learning_rate": 1.9843333333333334e-05,
      "loss": 0.0031,
      "step": 90470
    },
    {
      "epoch": 4.8256,
      "grad_norm": 0.19676734507083893,
      "learning_rate": 1.984e-05,
      "loss": 0.0041,
      "step": 90480
    },
    {
      "epoch": 4.826133333333333,
      "grad_norm": 0.08432374894618988,
      "learning_rate": 1.9836666666666666e-05,
      "loss": 0.0023,
      "step": 90490
    },
    {
      "epoch": 4.826666666666666,
      "grad_norm": 0.14055036008358002,
      "learning_rate": 1.9833333333333335e-05,
      "loss": 0.0027,
      "step": 90500
    },
    {
      "epoch": 4.8272,
      "grad_norm": 0.08432958275079727,
      "learning_rate": 1.983e-05,
      "loss": 0.0016,
      "step": 90510
    },
    {
      "epoch": 4.827733333333334,
      "grad_norm": 3.679920812516002e-09,
      "learning_rate": 1.9826666666666668e-05,
      "loss": 0.002,
      "step": 90520
    },
    {
      "epoch": 4.828266666666667,
      "grad_norm": 0.028108034282922745,
      "learning_rate": 1.9823333333333334e-05,
      "loss": 0.004,
      "step": 90530
    },
    {
      "epoch": 4.8288,
      "grad_norm": 0.056215617805719376,
      "learning_rate": 1.982e-05,
      "loss": 0.0026,
      "step": 90540
    },
    {
      "epoch": 4.8293333333333335,
      "grad_norm": 0.16864751279354095,
      "learning_rate": 1.9816666666666666e-05,
      "loss": 0.0021,
      "step": 90550
    },
    {
      "epoch": 4.829866666666667,
      "grad_norm": 0.028108853846788406,
      "learning_rate": 1.9813333333333332e-05,
      "loss": 0.0034,
      "step": 90560
    },
    {
      "epoch": 4.8304,
      "grad_norm": 0.05621890351176262,
      "learning_rate": 1.9810000000000002e-05,
      "loss": 0.0022,
      "step": 90570
    },
    {
      "epoch": 4.830933333333333,
      "grad_norm": 0.1967533379793167,
      "learning_rate": 1.9806666666666668e-05,
      "loss": 0.0024,
      "step": 90580
    },
    {
      "epoch": 4.831466666666667,
      "grad_norm": 0.19676604866981506,
      "learning_rate": 1.9803333333333334e-05,
      "loss": 0.0023,
      "step": 90590
    },
    {
      "epoch": 4.832,
      "grad_norm": 0.14054569602012634,
      "learning_rate": 1.9800000000000004e-05,
      "loss": 0.0019,
      "step": 90600
    },
    {
      "epoch": 4.832533333333333,
      "grad_norm": 3.994417685504459e-09,
      "learning_rate": 1.979666666666667e-05,
      "loss": 0.0025,
      "step": 90610
    },
    {
      "epoch": 4.833066666666666,
      "grad_norm": 0.028108343482017517,
      "learning_rate": 1.9793333333333332e-05,
      "loss": 0.0023,
      "step": 90620
    },
    {
      "epoch": 4.8336,
      "grad_norm": 0.1405392438173294,
      "learning_rate": 1.979e-05,
      "loss": 0.0022,
      "step": 90630
    },
    {
      "epoch": 4.834133333333333,
      "grad_norm": 0.112433061003685,
      "learning_rate": 1.9786666666666668e-05,
      "loss": 0.0034,
      "step": 90640
    },
    {
      "epoch": 4.834666666666667,
      "grad_norm": 0.2529628574848175,
      "learning_rate": 1.9783333333333334e-05,
      "loss": 0.0036,
      "step": 90650
    },
    {
      "epoch": 4.8352,
      "grad_norm": 0.22487160563468933,
      "learning_rate": 1.978e-05,
      "loss": 0.0036,
      "step": 90660
    },
    {
      "epoch": 4.835733333333334,
      "grad_norm": 0.028109226375818253,
      "learning_rate": 1.977666666666667e-05,
      "loss": 0.0037,
      "step": 90670
    },
    {
      "epoch": 4.836266666666667,
      "grad_norm": 0.05621771514415741,
      "learning_rate": 1.9773333333333336e-05,
      "loss": 0.0041,
      "step": 90680
    },
    {
      "epoch": 4.8368,
      "grad_norm": 0.16864199936389923,
      "learning_rate": 1.977e-05,
      "loss": 0.0028,
      "step": 90690
    },
    {
      "epoch": 4.8373333333333335,
      "grad_norm": 0.16865381598472595,
      "learning_rate": 1.9766666666666668e-05,
      "loss": 0.0023,
      "step": 90700
    },
    {
      "epoch": 4.837866666666667,
      "grad_norm": 2.346427496746628e-09,
      "learning_rate": 1.9763333333333334e-05,
      "loss": 0.0029,
      "step": 90710
    },
    {
      "epoch": 4.8384,
      "grad_norm": 0.11242746561765671,
      "learning_rate": 1.976e-05,
      "loss": 0.0024,
      "step": 90720
    },
    {
      "epoch": 4.838933333333333,
      "grad_norm": 0.1686456948518753,
      "learning_rate": 1.9756666666666667e-05,
      "loss": 0.004,
      "step": 90730
    },
    {
      "epoch": 4.839466666666667,
      "grad_norm": 0.1686398833990097,
      "learning_rate": 1.9753333333333336e-05,
      "loss": 0.0034,
      "step": 90740
    },
    {
      "epoch": 4.84,
      "grad_norm": 0.16864793002605438,
      "learning_rate": 1.9750000000000002e-05,
      "loss": 0.0027,
      "step": 90750
    },
    {
      "epoch": 4.840533333333333,
      "grad_norm": 0.028107333928346634,
      "learning_rate": 1.974666666666667e-05,
      "loss": 0.0023,
      "step": 90760
    },
    {
      "epoch": 4.841066666666666,
      "grad_norm": 0.056213852018117905,
      "learning_rate": 1.9743333333333335e-05,
      "loss": 0.0027,
      "step": 90770
    },
    {
      "epoch": 4.8416,
      "grad_norm": 0.11242678016424179,
      "learning_rate": 1.974e-05,
      "loss": 0.003,
      "step": 90780
    },
    {
      "epoch": 4.842133333333333,
      "grad_norm": 0.08432190120220184,
      "learning_rate": 1.9736666666666667e-05,
      "loss": 0.0021,
      "step": 90790
    },
    {
      "epoch": 4.842666666666666,
      "grad_norm": 0.14053356647491455,
      "learning_rate": 1.9733333333333333e-05,
      "loss": 0.0035,
      "step": 90800
    },
    {
      "epoch": 4.8431999999999995,
      "grad_norm": 0.08432140946388245,
      "learning_rate": 1.9730000000000003e-05,
      "loss": 0.0029,
      "step": 90810
    },
    {
      "epoch": 4.843733333333334,
      "grad_norm": 0.028107542544603348,
      "learning_rate": 1.972666666666667e-05,
      "loss": 0.0032,
      "step": 90820
    },
    {
      "epoch": 4.844266666666667,
      "grad_norm": 0.028106870129704475,
      "learning_rate": 1.9723333333333335e-05,
      "loss": 0.0023,
      "step": 90830
    },
    {
      "epoch": 4.8448,
      "grad_norm": 0.19675856828689575,
      "learning_rate": 1.972e-05,
      "loss": 0.0022,
      "step": 90840
    },
    {
      "epoch": 4.8453333333333335,
      "grad_norm": 0.14054733514785767,
      "learning_rate": 1.9716666666666667e-05,
      "loss": 0.0036,
      "step": 90850
    },
    {
      "epoch": 4.845866666666667,
      "grad_norm": 0.4496988356113434,
      "learning_rate": 1.9713333333333333e-05,
      "loss": 0.0023,
      "step": 90860
    },
    {
      "epoch": 4.8464,
      "grad_norm": 5.6990829833125645e-09,
      "learning_rate": 1.971e-05,
      "loss": 0.0038,
      "step": 90870
    },
    {
      "epoch": 4.846933333333333,
      "grad_norm": 0.16863758862018585,
      "learning_rate": 1.970666666666667e-05,
      "loss": 0.0028,
      "step": 90880
    },
    {
      "epoch": 4.847466666666667,
      "grad_norm": 0.2810763120651245,
      "learning_rate": 1.9703333333333335e-05,
      "loss": 0.0025,
      "step": 90890
    },
    {
      "epoch": 4.848,
      "grad_norm": 0.028107265010476112,
      "learning_rate": 1.97e-05,
      "loss": 0.0029,
      "step": 90900
    },
    {
      "epoch": 4.848533333333333,
      "grad_norm": 2.2055348658511775e-09,
      "learning_rate": 1.9696666666666667e-05,
      "loss": 0.0024,
      "step": 90910
    },
    {
      "epoch": 4.849066666666666,
      "grad_norm": 0.4497216045856476,
      "learning_rate": 1.9693333333333333e-05,
      "loss": 0.0024,
      "step": 90920
    },
    {
      "epoch": 4.8496,
      "grad_norm": 0.19676196575164795,
      "learning_rate": 1.969e-05,
      "loss": 0.0016,
      "step": 90930
    },
    {
      "epoch": 4.850133333333333,
      "grad_norm": 0.1686387062072754,
      "learning_rate": 1.9686666666666666e-05,
      "loss": 0.0032,
      "step": 90940
    },
    {
      "epoch": 4.850666666666667,
      "grad_norm": 0.11243174225091934,
      "learning_rate": 1.9683333333333335e-05,
      "loss": 0.003,
      "step": 90950
    },
    {
      "epoch": 4.8512,
      "grad_norm": 0.19674475491046906,
      "learning_rate": 1.968e-05,
      "loss": 0.0032,
      "step": 90960
    },
    {
      "epoch": 4.851733333333334,
      "grad_norm": 0.421615332365036,
      "learning_rate": 1.9676666666666667e-05,
      "loss": 0.0034,
      "step": 90970
    },
    {
      "epoch": 4.852266666666667,
      "grad_norm": 0.1405336558818817,
      "learning_rate": 1.9673333333333337e-05,
      "loss": 0.0029,
      "step": 90980
    },
    {
      "epoch": 4.8528,
      "grad_norm": 0.056211575865745544,
      "learning_rate": 1.9670000000000003e-05,
      "loss": 0.0023,
      "step": 90990
    },
    {
      "epoch": 4.8533333333333335,
      "grad_norm": 0.16864296793937683,
      "learning_rate": 1.9666666666666666e-05,
      "loss": 0.0026,
      "step": 91000
    },
    {
      "epoch": 4.853866666666667,
      "grad_norm": 0.22484394907951355,
      "learning_rate": 1.9663333333333332e-05,
      "loss": 0.0034,
      "step": 91010
    },
    {
      "epoch": 4.8544,
      "grad_norm": 0.028106722980737686,
      "learning_rate": 1.966e-05,
      "loss": 0.0037,
      "step": 91020
    },
    {
      "epoch": 4.854933333333333,
      "grad_norm": 0.08431819081306458,
      "learning_rate": 1.9656666666666668e-05,
      "loss": 0.0031,
      "step": 91030
    },
    {
      "epoch": 4.855466666666667,
      "grad_norm": 0.056214217096567154,
      "learning_rate": 1.9653333333333334e-05,
      "loss": 0.002,
      "step": 91040
    },
    {
      "epoch": 4.856,
      "grad_norm": 0.08431882411241531,
      "learning_rate": 1.9650000000000003e-05,
      "loss": 0.0028,
      "step": 91050
    },
    {
      "epoch": 4.856533333333333,
      "grad_norm": 0.08431799709796906,
      "learning_rate": 1.964666666666667e-05,
      "loss": 0.0029,
      "step": 91060
    },
    {
      "epoch": 4.857066666666666,
      "grad_norm": 0.02810678444802761,
      "learning_rate": 1.9643333333333332e-05,
      "loss": 0.0036,
      "step": 91070
    },
    {
      "epoch": 4.8576,
      "grad_norm": 0.1686389148235321,
      "learning_rate": 1.9640000000000002e-05,
      "loss": 0.003,
      "step": 91080
    },
    {
      "epoch": 4.858133333333333,
      "grad_norm": 5.309594541813567e-09,
      "learning_rate": 1.9636666666666668e-05,
      "loss": 0.0033,
      "step": 91090
    },
    {
      "epoch": 4.858666666666666,
      "grad_norm": 0.05621391162276268,
      "learning_rate": 1.9633333333333334e-05,
      "loss": 0.0025,
      "step": 91100
    },
    {
      "epoch": 4.8591999999999995,
      "grad_norm": 0.14053872227668762,
      "learning_rate": 1.963e-05,
      "loss": 0.002,
      "step": 91110
    },
    {
      "epoch": 4.859733333333334,
      "grad_norm": 0.02810695394873619,
      "learning_rate": 1.962666666666667e-05,
      "loss": 0.0025,
      "step": 91120
    },
    {
      "epoch": 4.860266666666667,
      "grad_norm": 4.125566999135799e-09,
      "learning_rate": 1.9623333333333336e-05,
      "loss": 0.0038,
      "step": 91130
    },
    {
      "epoch": 4.8608,
      "grad_norm": 0.1967468112707138,
      "learning_rate": 1.9620000000000002e-05,
      "loss": 0.0026,
      "step": 91140
    },
    {
      "epoch": 4.8613333333333335,
      "grad_norm": 0.11242633312940598,
      "learning_rate": 1.9616666666666668e-05,
      "loss": 0.0021,
      "step": 91150
    },
    {
      "epoch": 4.861866666666667,
      "grad_norm": 0.056211818009614944,
      "learning_rate": 1.9613333333333334e-05,
      "loss": 0.0021,
      "step": 91160
    },
    {
      "epoch": 4.8624,
      "grad_norm": 0.2248515486717224,
      "learning_rate": 1.961e-05,
      "loss": 0.0019,
      "step": 91170
    },
    {
      "epoch": 4.862933333333333,
      "grad_norm": 0.14052899181842804,
      "learning_rate": 1.9606666666666666e-05,
      "loss": 0.0018,
      "step": 91180
    },
    {
      "epoch": 4.863466666666667,
      "grad_norm": 7.83060816189618e-09,
      "learning_rate": 1.9603333333333336e-05,
      "loss": 0.0032,
      "step": 91190
    },
    {
      "epoch": 4.864,
      "grad_norm": 0.39351001381874084,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 0.0028,
      "step": 91200
    },
    {
      "epoch": 4.864533333333333,
      "grad_norm": 0.19673527777194977,
      "learning_rate": 1.9596666666666668e-05,
      "loss": 0.0029,
      "step": 91210
    },
    {
      "epoch": 4.865066666666666,
      "grad_norm": 0.16864104568958282,
      "learning_rate": 1.9593333333333334e-05,
      "loss": 0.0023,
      "step": 91220
    },
    {
      "epoch": 4.8656,
      "grad_norm": 0.25295117497444153,
      "learning_rate": 1.959e-05,
      "loss": 0.0021,
      "step": 91230
    },
    {
      "epoch": 4.866133333333333,
      "grad_norm": 0.309180349111557,
      "learning_rate": 1.9586666666666667e-05,
      "loss": 0.0025,
      "step": 91240
    },
    {
      "epoch": 4.866666666666667,
      "grad_norm": 0.2810596525669098,
      "learning_rate": 1.9583333333333333e-05,
      "loss": 0.0029,
      "step": 91250
    },
    {
      "epoch": 4.8672,
      "grad_norm": 0.028104322031140327,
      "learning_rate": 1.9580000000000002e-05,
      "loss": 0.0033,
      "step": 91260
    },
    {
      "epoch": 4.867733333333334,
      "grad_norm": 0.028106089681386948,
      "learning_rate": 1.957666666666667e-05,
      "loss": 0.0023,
      "step": 91270
    },
    {
      "epoch": 4.868266666666667,
      "grad_norm": 1.7759760618209839,
      "learning_rate": 1.9573333333333335e-05,
      "loss": 0.0037,
      "step": 91280
    },
    {
      "epoch": 4.8688,
      "grad_norm": 0.28105026483535767,
      "learning_rate": 1.957e-05,
      "loss": 0.003,
      "step": 91290
    },
    {
      "epoch": 4.8693333333333335,
      "grad_norm": 0.09577180445194244,
      "learning_rate": 1.9566666666666667e-05,
      "loss": 0.0055,
      "step": 91300
    },
    {
      "epoch": 4.869866666666667,
      "grad_norm": 0.1686340868473053,
      "learning_rate": 1.9563333333333333e-05,
      "loss": 0.0041,
      "step": 91310
    },
    {
      "epoch": 4.8704,
      "grad_norm": 0.08431477844715118,
      "learning_rate": 1.956e-05,
      "loss": 0.0026,
      "step": 91320
    },
    {
      "epoch": 4.870933333333333,
      "grad_norm": 0.1967383176088333,
      "learning_rate": 1.955666666666667e-05,
      "loss": 0.0026,
      "step": 91330
    },
    {
      "epoch": 4.871466666666667,
      "grad_norm": 0.05621077120304108,
      "learning_rate": 1.9553333333333335e-05,
      "loss": 0.0036,
      "step": 91340
    },
    {
      "epoch": 4.872,
      "grad_norm": 3.931495573539223e-09,
      "learning_rate": 1.955e-05,
      "loss": 0.0032,
      "step": 91350
    },
    {
      "epoch": 4.872533333333333,
      "grad_norm": 0.028106383979320526,
      "learning_rate": 1.9546666666666667e-05,
      "loss": 0.0028,
      "step": 91360
    },
    {
      "epoch": 4.873066666666666,
      "grad_norm": 0.140527606010437,
      "learning_rate": 1.9543333333333333e-05,
      "loss": 0.0023,
      "step": 91370
    },
    {
      "epoch": 4.8736,
      "grad_norm": 0.11242310702800751,
      "learning_rate": 1.954e-05,
      "loss": 0.0029,
      "step": 91380
    },
    {
      "epoch": 4.874133333333333,
      "grad_norm": 0.028106532990932465,
      "learning_rate": 1.9536666666666665e-05,
      "loss": 0.0026,
      "step": 91390
    },
    {
      "epoch": 4.874666666666666,
      "grad_norm": 0.05620964616537094,
      "learning_rate": 1.9533333333333335e-05,
      "loss": 0.003,
      "step": 91400
    },
    {
      "epoch": 4.8751999999999995,
      "grad_norm": 0.05621001496911049,
      "learning_rate": 1.953e-05,
      "loss": 0.0016,
      "step": 91410
    },
    {
      "epoch": 4.875733333333334,
      "grad_norm": 0.3934580981731415,
      "learning_rate": 1.9526666666666667e-05,
      "loss": 0.0026,
      "step": 91420
    },
    {
      "epoch": 4.876266666666667,
      "grad_norm": 0.30917295813560486,
      "learning_rate": 1.9523333333333337e-05,
      "loss": 0.0039,
      "step": 91430
    },
    {
      "epoch": 4.8768,
      "grad_norm": 0.224838525056839,
      "learning_rate": 1.9520000000000003e-05,
      "loss": 0.0034,
      "step": 91440
    },
    {
      "epoch": 4.8773333333333335,
      "grad_norm": 0.1686292141675949,
      "learning_rate": 1.9516666666666666e-05,
      "loss": 0.0031,
      "step": 91450
    },
    {
      "epoch": 4.877866666666667,
      "grad_norm": 0.1825891137123108,
      "learning_rate": 1.9513333333333335e-05,
      "loss": 0.0025,
      "step": 91460
    },
    {
      "epoch": 4.8784,
      "grad_norm": 0.3091731667518616,
      "learning_rate": 1.951e-05,
      "loss": 0.0035,
      "step": 91470
    },
    {
      "epoch": 4.878933333333333,
      "grad_norm": 0.22483590245246887,
      "learning_rate": 1.9506666666666667e-05,
      "loss": 0.0028,
      "step": 91480
    },
    {
      "epoch": 4.879466666666667,
      "grad_norm": 4.29157598347274e-09,
      "learning_rate": 1.9503333333333334e-05,
      "loss": 0.003,
      "step": 91490
    },
    {
      "epoch": 4.88,
      "grad_norm": 0.06339725106954575,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 0.0025,
      "step": 91500
    },
    {
      "epoch": 4.880533333333333,
      "grad_norm": 0.15325585007667542,
      "learning_rate": 1.949666666666667e-05,
      "loss": 0.0016,
      "step": 91510
    },
    {
      "epoch": 4.881066666666666,
      "grad_norm": 0.1124250739812851,
      "learning_rate": 1.9493333333333332e-05,
      "loss": 0.0035,
      "step": 91520
    },
    {
      "epoch": 4.8816,
      "grad_norm": 0.056210484355688095,
      "learning_rate": 1.949e-05,
      "loss": 0.0027,
      "step": 91530
    },
    {
      "epoch": 4.882133333333333,
      "grad_norm": 0.05621054768562317,
      "learning_rate": 1.9486666666666668e-05,
      "loss": 0.0022,
      "step": 91540
    },
    {
      "epoch": 4.882666666666667,
      "grad_norm": 0.05620787292718887,
      "learning_rate": 1.9483333333333334e-05,
      "loss": 0.0018,
      "step": 91550
    },
    {
      "epoch": 4.8832,
      "grad_norm": 0.16863535344600677,
      "learning_rate": 1.948e-05,
      "loss": 0.0025,
      "step": 91560
    },
    {
      "epoch": 4.883733333333334,
      "grad_norm": 1.5539721731627765e-09,
      "learning_rate": 1.947666666666667e-05,
      "loss": 0.0036,
      "step": 91570
    },
    {
      "epoch": 4.884266666666667,
      "grad_norm": 0.11241542547941208,
      "learning_rate": 1.9473333333333335e-05,
      "loss": 0.0027,
      "step": 91580
    },
    {
      "epoch": 4.8848,
      "grad_norm": 0.14052876830101013,
      "learning_rate": 1.947e-05,
      "loss": 0.0022,
      "step": 91590
    },
    {
      "epoch": 4.8853333333333335,
      "grad_norm": 0.1686352789402008,
      "learning_rate": 1.9466666666666668e-05,
      "loss": 0.003,
      "step": 91600
    },
    {
      "epoch": 4.885866666666667,
      "grad_norm": 0.42155182361602783,
      "learning_rate": 1.9463333333333334e-05,
      "loss": 0.0035,
      "step": 91610
    },
    {
      "epoch": 4.8864,
      "grad_norm": 0.14052529633045197,
      "learning_rate": 1.946e-05,
      "loss": 0.0043,
      "step": 91620
    },
    {
      "epoch": 4.886933333333333,
      "grad_norm": 0.05620763823390007,
      "learning_rate": 1.9456666666666666e-05,
      "loss": 0.0021,
      "step": 91630
    },
    {
      "epoch": 4.887466666666667,
      "grad_norm": 0.3653453588485718,
      "learning_rate": 1.9453333333333336e-05,
      "loss": 0.0037,
      "step": 91640
    },
    {
      "epoch": 4.888,
      "grad_norm": 0.16863030195236206,
      "learning_rate": 1.9450000000000002e-05,
      "loss": 0.0021,
      "step": 91650
    },
    {
      "epoch": 4.888533333333333,
      "grad_norm": 0.056209366768598557,
      "learning_rate": 1.9446666666666668e-05,
      "loss": 0.0025,
      "step": 91660
    },
    {
      "epoch": 4.8890666666666664,
      "grad_norm": 9.663919664504306e-10,
      "learning_rate": 1.9443333333333334e-05,
      "loss": 0.0022,
      "step": 91670
    },
    {
      "epoch": 4.8896,
      "grad_norm": 0.16862790286540985,
      "learning_rate": 1.944e-05,
      "loss": 0.0034,
      "step": 91680
    },
    {
      "epoch": 4.890133333333333,
      "grad_norm": 0.11241832375526428,
      "learning_rate": 1.9436666666666666e-05,
      "loss": 0.0042,
      "step": 91690
    },
    {
      "epoch": 4.890666666666666,
      "grad_norm": 0.16862380504608154,
      "learning_rate": 1.9433333333333332e-05,
      "loss": 0.0015,
      "step": 91700
    },
    {
      "epoch": 4.8911999999999995,
      "grad_norm": 0.1405208259820938,
      "learning_rate": 1.9430000000000002e-05,
      "loss": 0.0039,
      "step": 91710
    },
    {
      "epoch": 4.891733333333334,
      "grad_norm": 0.19673509895801544,
      "learning_rate": 1.9426666666666668e-05,
      "loss": 0.0022,
      "step": 91720
    },
    {
      "epoch": 4.892266666666667,
      "grad_norm": 0.11242005974054337,
      "learning_rate": 1.9423333333333334e-05,
      "loss": 0.0027,
      "step": 91730
    },
    {
      "epoch": 4.8928,
      "grad_norm": 0.16862043738365173,
      "learning_rate": 1.942e-05,
      "loss": 0.0031,
      "step": 91740
    },
    {
      "epoch": 4.8933333333333335,
      "grad_norm": 0.08431026339530945,
      "learning_rate": 1.9416666666666667e-05,
      "loss": 0.0039,
      "step": 91750
    },
    {
      "epoch": 4.893866666666667,
      "grad_norm": 0.08431337028741837,
      "learning_rate": 1.9413333333333333e-05,
      "loss": 0.002,
      "step": 91760
    },
    {
      "epoch": 4.8944,
      "grad_norm": 0.19673128426074982,
      "learning_rate": 1.941e-05,
      "loss": 0.0035,
      "step": 91770
    },
    {
      "epoch": 4.894933333333333,
      "grad_norm": 0.1967196762561798,
      "learning_rate": 1.940666666666667e-05,
      "loss": 0.0027,
      "step": 91780
    },
    {
      "epoch": 4.895466666666667,
      "grad_norm": 0.11241980642080307,
      "learning_rate": 1.9403333333333334e-05,
      "loss": 0.0034,
      "step": 91790
    },
    {
      "epoch": 4.896,
      "grad_norm": 0.5058500170707703,
      "learning_rate": 1.94e-05,
      "loss": 0.0035,
      "step": 91800
    },
    {
      "epoch": 4.896533333333333,
      "grad_norm": 0.30916815996170044,
      "learning_rate": 1.939666666666667e-05,
      "loss": 0.0027,
      "step": 91810
    },
    {
      "epoch": 4.8970666666666665,
      "grad_norm": 0.11241637915372849,
      "learning_rate": 1.9393333333333336e-05,
      "loss": 0.0023,
      "step": 91820
    },
    {
      "epoch": 4.8976,
      "grad_norm": 0.19672368466854095,
      "learning_rate": 1.939e-05,
      "loss": 0.0036,
      "step": 91830
    },
    {
      "epoch": 4.898133333333333,
      "grad_norm": 0.056211505085229874,
      "learning_rate": 1.938666666666667e-05,
      "loss": 0.0022,
      "step": 91840
    },
    {
      "epoch": 4.898666666666666,
      "grad_norm": 0.3653321862220764,
      "learning_rate": 1.9383333333333335e-05,
      "loss": 0.0029,
      "step": 91850
    },
    {
      "epoch": 4.8992,
      "grad_norm": 0.19673125445842743,
      "learning_rate": 1.938e-05,
      "loss": 0.0022,
      "step": 91860
    },
    {
      "epoch": 4.899733333333334,
      "grad_norm": 0.11241307109594345,
      "learning_rate": 1.9376666666666667e-05,
      "loss": 0.0025,
      "step": 91870
    },
    {
      "epoch": 4.900266666666667,
      "grad_norm": 0.028103090822696686,
      "learning_rate": 1.9373333333333336e-05,
      "loss": 0.0025,
      "step": 91880
    },
    {
      "epoch": 4.9008,
      "grad_norm": 0.22482050955295563,
      "learning_rate": 1.9370000000000003e-05,
      "loss": 0.0018,
      "step": 91890
    },
    {
      "epoch": 4.9013333333333335,
      "grad_norm": 0.16862574219703674,
      "learning_rate": 1.9366666666666665e-05,
      "loss": 0.003,
      "step": 91900
    },
    {
      "epoch": 4.901866666666667,
      "grad_norm": 0.8148233294487,
      "learning_rate": 1.9363333333333335e-05,
      "loss": 0.0045,
      "step": 91910
    },
    {
      "epoch": 4.9024,
      "grad_norm": 0.14051556587219238,
      "learning_rate": 1.936e-05,
      "loss": 0.0024,
      "step": 91920
    },
    {
      "epoch": 4.902933333333333,
      "grad_norm": 0.02810249663889408,
      "learning_rate": 1.9356666666666667e-05,
      "loss": 0.0026,
      "step": 91930
    },
    {
      "epoch": 4.903466666666667,
      "grad_norm": 0.14051592350006104,
      "learning_rate": 1.9353333333333333e-05,
      "loss": 0.0035,
      "step": 91940
    },
    {
      "epoch": 4.904,
      "grad_norm": 7.681800973102781e-09,
      "learning_rate": 1.9350000000000003e-05,
      "loss": 0.0026,
      "step": 91950
    },
    {
      "epoch": 4.904533333333333,
      "grad_norm": 0.2248353213071823,
      "learning_rate": 1.934666666666667e-05,
      "loss": 0.0042,
      "step": 91960
    },
    {
      "epoch": 4.9050666666666665,
      "grad_norm": 0.028103474527597427,
      "learning_rate": 1.9343333333333335e-05,
      "loss": 0.0038,
      "step": 91970
    },
    {
      "epoch": 4.9056,
      "grad_norm": 0.3372530937194824,
      "learning_rate": 1.934e-05,
      "loss": 0.0026,
      "step": 91980
    },
    {
      "epoch": 4.906133333333333,
      "grad_norm": 0.08430926501750946,
      "learning_rate": 1.9336666666666667e-05,
      "loss": 0.0034,
      "step": 91990
    },
    {
      "epoch": 4.906666666666666,
      "grad_norm": 0.14051300287246704,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 0.0029,
      "step": 92000
    },
    {
      "epoch": 4.9072,
      "grad_norm": 0.4496431350708008,
      "learning_rate": 1.933e-05,
      "loss": 0.0028,
      "step": 92010
    },
    {
      "epoch": 4.907733333333333,
      "grad_norm": 0.028103383257985115,
      "learning_rate": 1.932666666666667e-05,
      "loss": 0.0029,
      "step": 92020
    },
    {
      "epoch": 4.908266666666667,
      "grad_norm": 0.25293195247650146,
      "learning_rate": 1.9323333333333335e-05,
      "loss": 0.0021,
      "step": 92030
    },
    {
      "epoch": 4.9088,
      "grad_norm": 0.02810295857489109,
      "learning_rate": 1.932e-05,
      "loss": 0.0042,
      "step": 92040
    },
    {
      "epoch": 4.9093333333333335,
      "grad_norm": 0.028102189302444458,
      "learning_rate": 1.9316666666666668e-05,
      "loss": 0.0029,
      "step": 92050
    },
    {
      "epoch": 4.909866666666667,
      "grad_norm": 0.22482843697071075,
      "learning_rate": 1.9313333333333334e-05,
      "loss": 0.0028,
      "step": 92060
    },
    {
      "epoch": 4.9104,
      "grad_norm": 0.42154356837272644,
      "learning_rate": 1.931e-05,
      "loss": 0.0023,
      "step": 92070
    },
    {
      "epoch": 4.910933333333333,
      "grad_norm": 0.11241427809000015,
      "learning_rate": 1.9306666666666666e-05,
      "loss": 0.0024,
      "step": 92080
    },
    {
      "epoch": 4.911466666666667,
      "grad_norm": 0.25291764736175537,
      "learning_rate": 1.9303333333333335e-05,
      "loss": 0.0031,
      "step": 92090
    },
    {
      "epoch": 4.912,
      "grad_norm": 0.056206487119197845,
      "learning_rate": 1.93e-05,
      "loss": 0.0035,
      "step": 92100
    },
    {
      "epoch": 4.912533333333333,
      "grad_norm": 0.3091107904911041,
      "learning_rate": 1.9296666666666668e-05,
      "loss": 0.0027,
      "step": 92110
    },
    {
      "epoch": 4.9130666666666665,
      "grad_norm": 0.33725813031196594,
      "learning_rate": 1.9293333333333334e-05,
      "loss": 0.0024,
      "step": 92120
    },
    {
      "epoch": 4.9136,
      "grad_norm": 0.028102772310376167,
      "learning_rate": 1.929e-05,
      "loss": 0.0028,
      "step": 92130
    },
    {
      "epoch": 4.914133333333333,
      "grad_norm": 0.056204576045274734,
      "learning_rate": 1.9286666666666666e-05,
      "loss": 0.0032,
      "step": 92140
    },
    {
      "epoch": 4.914666666666666,
      "grad_norm": 0.05620352923870087,
      "learning_rate": 1.9283333333333332e-05,
      "loss": 0.003,
      "step": 92150
    },
    {
      "epoch": 4.9152000000000005,
      "grad_norm": 0.14051729440689087,
      "learning_rate": 1.9280000000000002e-05,
      "loss": 0.003,
      "step": 92160
    },
    {
      "epoch": 4.915733333333334,
      "grad_norm": 5.468544960021973,
      "learning_rate": 1.9276666666666668e-05,
      "loss": 0.0031,
      "step": 92170
    },
    {
      "epoch": 4.916266666666667,
      "grad_norm": 0.08431272208690643,
      "learning_rate": 1.9273333333333334e-05,
      "loss": 0.0036,
      "step": 92180
    },
    {
      "epoch": 4.9168,
      "grad_norm": 0.3934350311756134,
      "learning_rate": 1.9270000000000004e-05,
      "loss": 0.0019,
      "step": 92190
    },
    {
      "epoch": 4.917333333333334,
      "grad_norm": 6.455823875484157e-09,
      "learning_rate": 1.926666666666667e-05,
      "loss": 0.0014,
      "step": 92200
    },
    {
      "epoch": 4.917866666666667,
      "grad_norm": 0.2529372572898865,
      "learning_rate": 1.9263333333333332e-05,
      "loss": 0.0036,
      "step": 92210
    },
    {
      "epoch": 4.9184,
      "grad_norm": 0.3934114873409271,
      "learning_rate": 1.9260000000000002e-05,
      "loss": 0.0024,
      "step": 92220
    },
    {
      "epoch": 4.918933333333333,
      "grad_norm": 0.28104355931282043,
      "learning_rate": 1.9256666666666668e-05,
      "loss": 0.0032,
      "step": 92230
    },
    {
      "epoch": 4.919466666666667,
      "grad_norm": 0.1405126303434372,
      "learning_rate": 1.9253333333333334e-05,
      "loss": 0.004,
      "step": 92240
    },
    {
      "epoch": 4.92,
      "grad_norm": 0.14051280915737152,
      "learning_rate": 1.925e-05,
      "loss": 0.0023,
      "step": 92250
    },
    {
      "epoch": 4.920533333333333,
      "grad_norm": 0.11240918189287186,
      "learning_rate": 1.924666666666667e-05,
      "loss": 0.0032,
      "step": 92260
    },
    {
      "epoch": 4.9210666666666665,
      "grad_norm": 0.2529280483722687,
      "learning_rate": 1.9243333333333336e-05,
      "loss": 0.003,
      "step": 92270
    },
    {
      "epoch": 4.9216,
      "grad_norm": 0.028101488947868347,
      "learning_rate": 1.924e-05,
      "loss": 0.004,
      "step": 92280
    },
    {
      "epoch": 4.922133333333333,
      "grad_norm": 0.11241032183170319,
      "learning_rate": 1.9236666666666668e-05,
      "loss": 0.0026,
      "step": 92290
    },
    {
      "epoch": 4.922666666666666,
      "grad_norm": 0.19670717418193817,
      "learning_rate": 1.9233333333333334e-05,
      "loss": 0.0043,
      "step": 92300
    },
    {
      "epoch": 4.9232,
      "grad_norm": 0.11240626871585846,
      "learning_rate": 1.923e-05,
      "loss": 0.003,
      "step": 92310
    },
    {
      "epoch": 4.923733333333333,
      "grad_norm": 0.11241377890110016,
      "learning_rate": 1.9226666666666667e-05,
      "loss": 0.002,
      "step": 92320
    },
    {
      "epoch": 4.924266666666667,
      "grad_norm": 0.19670994579792023,
      "learning_rate": 1.9223333333333336e-05,
      "loss": 0.0037,
      "step": 92330
    },
    {
      "epoch": 4.9248,
      "grad_norm": 0.14051206409931183,
      "learning_rate": 1.9220000000000002e-05,
      "loss": 0.0019,
      "step": 92340
    },
    {
      "epoch": 4.925333333333334,
      "grad_norm": 0.05620361492037773,
      "learning_rate": 1.921666666666667e-05,
      "loss": 0.0021,
      "step": 92350
    },
    {
      "epoch": 4.925866666666667,
      "grad_norm": 0.30910786986351013,
      "learning_rate": 1.9213333333333335e-05,
      "loss": 0.0032,
      "step": 92360
    },
    {
      "epoch": 4.9264,
      "grad_norm": 2.6153050836086322e-09,
      "learning_rate": 1.921e-05,
      "loss": 0.0028,
      "step": 92370
    },
    {
      "epoch": 4.926933333333333,
      "grad_norm": 0.20074795186519623,
      "learning_rate": 1.9206666666666667e-05,
      "loss": 0.002,
      "step": 92380
    },
    {
      "epoch": 4.927466666666667,
      "grad_norm": 0.02810097299516201,
      "learning_rate": 1.9203333333333333e-05,
      "loss": 0.0021,
      "step": 92390
    },
    {
      "epoch": 4.928,
      "grad_norm": 0.28101545572280884,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.0037,
      "step": 92400
    },
    {
      "epoch": 4.928533333333333,
      "grad_norm": 0.14051173627376556,
      "learning_rate": 1.919666666666667e-05,
      "loss": 0.0029,
      "step": 92410
    },
    {
      "epoch": 4.9290666666666665,
      "grad_norm": 0.5339102745056152,
      "learning_rate": 1.9193333333333335e-05,
      "loss": 0.0028,
      "step": 92420
    },
    {
      "epoch": 4.9296,
      "grad_norm": 0.2529105842113495,
      "learning_rate": 1.919e-05,
      "loss": 0.0021,
      "step": 92430
    },
    {
      "epoch": 4.930133333333333,
      "grad_norm": 0.3091045022010803,
      "learning_rate": 1.9186666666666667e-05,
      "loss": 0.0026,
      "step": 92440
    },
    {
      "epoch": 4.930666666666666,
      "grad_norm": 0.14051461219787598,
      "learning_rate": 1.9183333333333333e-05,
      "loss": 0.0027,
      "step": 92450
    },
    {
      "epoch": 4.9312000000000005,
      "grad_norm": 0.02810136042535305,
      "learning_rate": 1.918e-05,
      "loss": 0.0018,
      "step": 92460
    },
    {
      "epoch": 4.931733333333334,
      "grad_norm": 0.16860367357730865,
      "learning_rate": 1.917666666666667e-05,
      "loss": 0.0023,
      "step": 92470
    },
    {
      "epoch": 4.932266666666667,
      "grad_norm": 0.11240441352128983,
      "learning_rate": 1.9173333333333335e-05,
      "loss": 0.0023,
      "step": 92480
    },
    {
      "epoch": 4.9328,
      "grad_norm": 0.11240661889314651,
      "learning_rate": 1.917e-05,
      "loss": 0.0027,
      "step": 92490
    },
    {
      "epoch": 4.933333333333334,
      "grad_norm": 0.028102416545152664,
      "learning_rate": 1.9166666666666667e-05,
      "loss": 0.0022,
      "step": 92500
    },
    {
      "epoch": 4.933866666666667,
      "grad_norm": 1.934827853489196e-09,
      "learning_rate": 1.9163333333333333e-05,
      "loss": 0.0032,
      "step": 92510
    },
    {
      "epoch": 4.9344,
      "grad_norm": 0.16860900819301605,
      "learning_rate": 1.916e-05,
      "loss": 0.0031,
      "step": 92520
    },
    {
      "epoch": 4.934933333333333,
      "grad_norm": 0.16861729323863983,
      "learning_rate": 1.9156666666666666e-05,
      "loss": 0.0024,
      "step": 92530
    },
    {
      "epoch": 4.935466666666667,
      "grad_norm": 0.2529048025608063,
      "learning_rate": 1.9153333333333335e-05,
      "loss": 0.0024,
      "step": 92540
    },
    {
      "epoch": 4.936,
      "grad_norm": 0.08430544286966324,
      "learning_rate": 1.915e-05,
      "loss": 0.0033,
      "step": 92550
    },
    {
      "epoch": 4.936533333333333,
      "grad_norm": 0.1686057597398758,
      "learning_rate": 1.9146666666666667e-05,
      "loss": 0.0025,
      "step": 92560
    },
    {
      "epoch": 4.9370666666666665,
      "grad_norm": 0.22481168806552887,
      "learning_rate": 1.9143333333333337e-05,
      "loss": 0.0024,
      "step": 92570
    },
    {
      "epoch": 4.9376,
      "grad_norm": 0.281013160943985,
      "learning_rate": 1.914e-05,
      "loss": 0.0025,
      "step": 92580
    },
    {
      "epoch": 4.938133333333333,
      "grad_norm": 0.39340901374816895,
      "learning_rate": 1.9136666666666666e-05,
      "loss": 0.0028,
      "step": 92590
    },
    {
      "epoch": 4.938666666666666,
      "grad_norm": 0.05619995296001434,
      "learning_rate": 1.9133333333333332e-05,
      "loss": 0.0034,
      "step": 92600
    },
    {
      "epoch": 4.9392,
      "grad_norm": 0.2810119390487671,
      "learning_rate": 1.913e-05,
      "loss": 0.0025,
      "step": 92610
    },
    {
      "epoch": 4.939733333333333,
      "grad_norm": 0.05619914457201958,
      "learning_rate": 1.9126666666666668e-05,
      "loss": 0.0029,
      "step": 92620
    },
    {
      "epoch": 4.940266666666667,
      "grad_norm": 2.698930634537078e-09,
      "learning_rate": 1.9123333333333334e-05,
      "loss": 0.0035,
      "step": 92630
    },
    {
      "epoch": 4.9408,
      "grad_norm": 0.25289902091026306,
      "learning_rate": 1.9120000000000003e-05,
      "loss": 0.003,
      "step": 92640
    },
    {
      "epoch": 4.941333333333334,
      "grad_norm": 0.05620177462697029,
      "learning_rate": 1.911666666666667e-05,
      "loss": 0.0022,
      "step": 92650
    },
    {
      "epoch": 4.941866666666667,
      "grad_norm": 0.11240262538194656,
      "learning_rate": 1.9113333333333332e-05,
      "loss": 0.0025,
      "step": 92660
    },
    {
      "epoch": 4.9424,
      "grad_norm": 0.5619907975196838,
      "learning_rate": 1.911e-05,
      "loss": 0.0035,
      "step": 92670
    },
    {
      "epoch": 4.942933333333333,
      "grad_norm": 0.42152324318885803,
      "learning_rate": 1.9106666666666668e-05,
      "loss": 0.0021,
      "step": 92680
    },
    {
      "epoch": 4.943466666666667,
      "grad_norm": 0.12706254422664642,
      "learning_rate": 1.9103333333333334e-05,
      "loss": 0.0033,
      "step": 92690
    },
    {
      "epoch": 4.944,
      "grad_norm": 0.11240121722221375,
      "learning_rate": 1.91e-05,
      "loss": 0.0022,
      "step": 92700
    },
    {
      "epoch": 4.944533333333333,
      "grad_norm": 0.056200068444013596,
      "learning_rate": 1.909666666666667e-05,
      "loss": 0.0027,
      "step": 92710
    },
    {
      "epoch": 4.9450666666666665,
      "grad_norm": 0.22480282187461853,
      "learning_rate": 1.9093333333333336e-05,
      "loss": 0.0036,
      "step": 92720
    },
    {
      "epoch": 4.9456,
      "grad_norm": 0.14049676060676575,
      "learning_rate": 1.909e-05,
      "loss": 0.0031,
      "step": 92730
    },
    {
      "epoch": 4.946133333333333,
      "grad_norm": 0.028100166469812393,
      "learning_rate": 1.9086666666666668e-05,
      "loss": 0.0017,
      "step": 92740
    },
    {
      "epoch": 4.946666666666666,
      "grad_norm": 0.28099384903907776,
      "learning_rate": 1.9083333333333334e-05,
      "loss": 0.0033,
      "step": 92750
    },
    {
      "epoch": 4.9472000000000005,
      "grad_norm": 0.25291094183921814,
      "learning_rate": 1.908e-05,
      "loss": 0.002,
      "step": 92760
    },
    {
      "epoch": 4.947733333333334,
      "grad_norm": 0.05619984492659569,
      "learning_rate": 1.9076666666666666e-05,
      "loss": 0.0023,
      "step": 92770
    },
    {
      "epoch": 4.948266666666667,
      "grad_norm": 0.05620240047574043,
      "learning_rate": 1.9073333333333336e-05,
      "loss": 0.0026,
      "step": 92780
    },
    {
      "epoch": 4.9488,
      "grad_norm": 0.16860297322273254,
      "learning_rate": 1.9070000000000002e-05,
      "loss": 0.0025,
      "step": 92790
    },
    {
      "epoch": 4.949333333333334,
      "grad_norm": 0.11240001022815704,
      "learning_rate": 1.9066666666666668e-05,
      "loss": 0.0032,
      "step": 92800
    },
    {
      "epoch": 4.949866666666667,
      "grad_norm": 1.159192442893982,
      "learning_rate": 1.9063333333333334e-05,
      "loss": 0.0028,
      "step": 92810
    },
    {
      "epoch": 4.9504,
      "grad_norm": 0.05620058253407478,
      "learning_rate": 1.906e-05,
      "loss": 0.0032,
      "step": 92820
    },
    {
      "epoch": 4.950933333333333,
      "grad_norm": 0.3091109097003937,
      "learning_rate": 1.9056666666666667e-05,
      "loss": 0.0029,
      "step": 92830
    },
    {
      "epoch": 4.951466666666667,
      "grad_norm": 0.0561986044049263,
      "learning_rate": 1.9053333333333333e-05,
      "loss": 0.0031,
      "step": 92840
    },
    {
      "epoch": 4.952,
      "grad_norm": 0.0843031033873558,
      "learning_rate": 1.9050000000000002e-05,
      "loss": 0.0015,
      "step": 92850
    },
    {
      "epoch": 4.952533333333333,
      "grad_norm": 0.22481130063533783,
      "learning_rate": 1.904666666666667e-05,
      "loss": 0.0027,
      "step": 92860
    },
    {
      "epoch": 4.9530666666666665,
      "grad_norm": 0.4214847981929779,
      "learning_rate": 1.9043333333333335e-05,
      "loss": 0.0021,
      "step": 92870
    },
    {
      "epoch": 4.9536,
      "grad_norm": 0.1405041366815567,
      "learning_rate": 1.904e-05,
      "loss": 0.0021,
      "step": 92880
    },
    {
      "epoch": 4.954133333333333,
      "grad_norm": 0.1404978632926941,
      "learning_rate": 1.9036666666666667e-05,
      "loss": 0.0024,
      "step": 92890
    },
    {
      "epoch": 4.954666666666666,
      "grad_norm": 0.02809990756213665,
      "learning_rate": 1.9033333333333333e-05,
      "loss": 0.0033,
      "step": 92900
    },
    {
      "epoch": 4.9552,
      "grad_norm": 0.02810114249587059,
      "learning_rate": 1.903e-05,
      "loss": 0.0019,
      "step": 92910
    },
    {
      "epoch": 4.955733333333333,
      "grad_norm": 0.17704921960830688,
      "learning_rate": 1.902666666666667e-05,
      "loss": 0.0016,
      "step": 92920
    },
    {
      "epoch": 4.956266666666667,
      "grad_norm": 2.3108067512512207,
      "learning_rate": 1.9023333333333335e-05,
      "loss": 0.0032,
      "step": 92930
    },
    {
      "epoch": 4.9568,
      "grad_norm": 0.028099585324525833,
      "learning_rate": 1.902e-05,
      "loss": 0.0026,
      "step": 92940
    },
    {
      "epoch": 4.957333333333334,
      "grad_norm": 0.19669023156166077,
      "learning_rate": 1.901666666666667e-05,
      "loss": 0.0031,
      "step": 92950
    },
    {
      "epoch": 4.957866666666667,
      "grad_norm": 0.14049839973449707,
      "learning_rate": 1.9013333333333333e-05,
      "loss": 0.0037,
      "step": 92960
    },
    {
      "epoch": 4.9584,
      "grad_norm": 0.14050233364105225,
      "learning_rate": 1.901e-05,
      "loss": 0.0024,
      "step": 92970
    },
    {
      "epoch": 4.958933333333333,
      "grad_norm": 0.11239837110042572,
      "learning_rate": 1.9006666666666665e-05,
      "loss": 0.0038,
      "step": 92980
    },
    {
      "epoch": 4.959466666666667,
      "grad_norm": 0.11239761114120483,
      "learning_rate": 1.9003333333333335e-05,
      "loss": 0.0036,
      "step": 92990
    },
    {
      "epoch": 4.96,
      "grad_norm": 0.2528897523880005,
      "learning_rate": 1.9e-05,
      "loss": 0.0033,
      "step": 93000
    },
    {
      "epoch": 4.960533333333333,
      "grad_norm": 0.08429884165525436,
      "learning_rate": 1.8996666666666667e-05,
      "loss": 0.0035,
      "step": 93010
    },
    {
      "epoch": 4.9610666666666665,
      "grad_norm": 0.1967044621706009,
      "learning_rate": 1.8993333333333337e-05,
      "loss": 0.0033,
      "step": 93020
    },
    {
      "epoch": 4.9616,
      "grad_norm": 1.6071634023617776e-09,
      "learning_rate": 1.8990000000000003e-05,
      "loss": 0.0023,
      "step": 93030
    },
    {
      "epoch": 4.962133333333333,
      "grad_norm": 0.3371965289115906,
      "learning_rate": 1.8986666666666666e-05,
      "loss": 0.0032,
      "step": 93040
    },
    {
      "epoch": 4.962666666666666,
      "grad_norm": 0.19669204950332642,
      "learning_rate": 1.8983333333333335e-05,
      "loss": 0.0028,
      "step": 93050
    },
    {
      "epoch": 4.9632,
      "grad_norm": 0.3371867835521698,
      "learning_rate": 1.898e-05,
      "loss": 0.0023,
      "step": 93060
    },
    {
      "epoch": 4.963733333333334,
      "grad_norm": 0.08429786562919617,
      "learning_rate": 1.8976666666666667e-05,
      "loss": 0.003,
      "step": 93070
    },
    {
      "epoch": 4.964266666666667,
      "grad_norm": 0.30907294154167175,
      "learning_rate": 1.8973333333333334e-05,
      "loss": 0.003,
      "step": 93080
    },
    {
      "epoch": 4.9648,
      "grad_norm": 0.16859763860702515,
      "learning_rate": 1.8970000000000003e-05,
      "loss": 0.0031,
      "step": 93090
    },
    {
      "epoch": 4.965333333333334,
      "grad_norm": 0.0280995462089777,
      "learning_rate": 1.896666666666667e-05,
      "loss": 0.0022,
      "step": 93100
    },
    {
      "epoch": 4.965866666666667,
      "grad_norm": 0.4495704472064972,
      "learning_rate": 1.8963333333333332e-05,
      "loss": 0.0029,
      "step": 93110
    },
    {
      "epoch": 4.9664,
      "grad_norm": 1.5174337342216404e-09,
      "learning_rate": 1.896e-05,
      "loss": 0.0025,
      "step": 93120
    },
    {
      "epoch": 4.966933333333333,
      "grad_norm": 0.0561966747045517,
      "learning_rate": 1.8956666666666668e-05,
      "loss": 0.0028,
      "step": 93130
    },
    {
      "epoch": 4.967466666666667,
      "grad_norm": 0.0842972993850708,
      "learning_rate": 1.8953333333333334e-05,
      "loss": 0.0037,
      "step": 93140
    },
    {
      "epoch": 4.968,
      "grad_norm": 5.1416364499345946e-09,
      "learning_rate": 1.895e-05,
      "loss": 0.0025,
      "step": 93150
    },
    {
      "epoch": 4.968533333333333,
      "grad_norm": 0.1685885488986969,
      "learning_rate": 1.894666666666667e-05,
      "loss": 0.0024,
      "step": 93160
    },
    {
      "epoch": 4.9690666666666665,
      "grad_norm": 0.08429396897554398,
      "learning_rate": 1.8943333333333335e-05,
      "loss": 0.0027,
      "step": 93170
    },
    {
      "epoch": 4.9696,
      "grad_norm": 0.08429970592260361,
      "learning_rate": 1.894e-05,
      "loss": 0.0015,
      "step": 93180
    },
    {
      "epoch": 4.970133333333333,
      "grad_norm": 0.2247934192419052,
      "learning_rate": 1.8936666666666668e-05,
      "loss": 0.0022,
      "step": 93190
    },
    {
      "epoch": 4.970666666666666,
      "grad_norm": 0.4214637875556946,
      "learning_rate": 1.8933333333333334e-05,
      "loss": 0.0027,
      "step": 93200
    },
    {
      "epoch": 4.9712,
      "grad_norm": 0.1685967594385147,
      "learning_rate": 1.893e-05,
      "loss": 0.002,
      "step": 93210
    },
    {
      "epoch": 4.971733333333333,
      "grad_norm": 0.22479429841041565,
      "learning_rate": 1.8926666666666666e-05,
      "loss": 0.0027,
      "step": 93220
    },
    {
      "epoch": 4.972266666666666,
      "grad_norm": 0.1966814398765564,
      "learning_rate": 1.8923333333333336e-05,
      "loss": 0.0037,
      "step": 93230
    },
    {
      "epoch": 4.9728,
      "grad_norm": 0.3371824324131012,
      "learning_rate": 1.8920000000000002e-05,
      "loss": 0.0039,
      "step": 93240
    },
    {
      "epoch": 4.973333333333334,
      "grad_norm": 0.19668418169021606,
      "learning_rate": 1.8916666666666668e-05,
      "loss": 0.0027,
      "step": 93250
    },
    {
      "epoch": 4.973866666666667,
      "grad_norm": 0.028099969029426575,
      "learning_rate": 1.8913333333333334e-05,
      "loss": 0.0029,
      "step": 93260
    },
    {
      "epoch": 4.9744,
      "grad_norm": 0.22479377686977386,
      "learning_rate": 1.891e-05,
      "loss": 0.0019,
      "step": 93270
    },
    {
      "epoch": 4.974933333333333,
      "grad_norm": 0.05619581788778305,
      "learning_rate": 1.8906666666666666e-05,
      "loss": 0.002,
      "step": 93280
    },
    {
      "epoch": 4.975466666666667,
      "grad_norm": 2.432990031664417e-09,
      "learning_rate": 1.8903333333333332e-05,
      "loss": 0.0025,
      "step": 93290
    },
    {
      "epoch": 4.976,
      "grad_norm": 0.08429557085037231,
      "learning_rate": 1.8900000000000002e-05,
      "loss": 0.0022,
      "step": 93300
    },
    {
      "epoch": 4.976533333333333,
      "grad_norm": 0.14049014449119568,
      "learning_rate": 1.8896666666666668e-05,
      "loss": 0.0034,
      "step": 93310
    },
    {
      "epoch": 4.9770666666666665,
      "grad_norm": 0.14049041271209717,
      "learning_rate": 1.8893333333333334e-05,
      "loss": 0.0038,
      "step": 93320
    },
    {
      "epoch": 4.9776,
      "grad_norm": 0.05619559437036514,
      "learning_rate": 1.8890000000000004e-05,
      "loss": 0.0026,
      "step": 93330
    },
    {
      "epoch": 4.978133333333333,
      "grad_norm": 0.02809872478246689,
      "learning_rate": 1.8886666666666667e-05,
      "loss": 0.0034,
      "step": 93340
    },
    {
      "epoch": 4.978666666666666,
      "grad_norm": 0.084293432533741,
      "learning_rate": 1.8883333333333333e-05,
      "loss": 0.004,
      "step": 93350
    },
    {
      "epoch": 4.9792,
      "grad_norm": 0.028099583461880684,
      "learning_rate": 1.888e-05,
      "loss": 0.0019,
      "step": 93360
    },
    {
      "epoch": 4.979733333333334,
      "grad_norm": 0.11239288747310638,
      "learning_rate": 1.887666666666667e-05,
      "loss": 0.0023,
      "step": 93370
    },
    {
      "epoch": 4.980266666666667,
      "grad_norm": 0.05619589611887932,
      "learning_rate": 1.8873333333333334e-05,
      "loss": 0.002,
      "step": 93380
    },
    {
      "epoch": 4.9808,
      "grad_norm": 0.3090846836566925,
      "learning_rate": 1.887e-05,
      "loss": 0.0027,
      "step": 93390
    },
    {
      "epoch": 4.981333333333334,
      "grad_norm": 0.08429262042045593,
      "learning_rate": 1.886666666666667e-05,
      "loss": 0.0021,
      "step": 93400
    },
    {
      "epoch": 4.981866666666667,
      "grad_norm": 0.1685861051082611,
      "learning_rate": 1.8863333333333333e-05,
      "loss": 0.003,
      "step": 93410
    },
    {
      "epoch": 4.9824,
      "grad_norm": 0.140489399433136,
      "learning_rate": 1.886e-05,
      "loss": 0.0028,
      "step": 93420
    },
    {
      "epoch": 4.982933333333333,
      "grad_norm": 0.8043590188026428,
      "learning_rate": 1.885666666666667e-05,
      "loss": 0.0022,
      "step": 93430
    },
    {
      "epoch": 4.983466666666667,
      "grad_norm": 0.02809670940041542,
      "learning_rate": 1.8853333333333335e-05,
      "loss": 0.0025,
      "step": 93440
    },
    {
      "epoch": 4.984,
      "grad_norm": 0.3934016823768616,
      "learning_rate": 1.885e-05,
      "loss": 0.003,
      "step": 93450
    },
    {
      "epoch": 4.984533333333333,
      "grad_norm": 0.02809746563434601,
      "learning_rate": 1.8846666666666667e-05,
      "loss": 0.0025,
      "step": 93460
    },
    {
      "epoch": 4.9850666666666665,
      "grad_norm": 7.626708153907202e-09,
      "learning_rate": 1.8843333333333336e-05,
      "loss": 0.0021,
      "step": 93470
    },
    {
      "epoch": 4.9856,
      "grad_norm": 0.0561985969543457,
      "learning_rate": 1.8840000000000003e-05,
      "loss": 0.0027,
      "step": 93480
    },
    {
      "epoch": 4.986133333333333,
      "grad_norm": 0.14048589766025543,
      "learning_rate": 1.8836666666666665e-05,
      "loss": 0.004,
      "step": 93490
    },
    {
      "epoch": 4.986666666666666,
      "grad_norm": 0.16858528554439545,
      "learning_rate": 1.8833333333333335e-05,
      "loss": 0.003,
      "step": 93500
    },
    {
      "epoch": 4.9872,
      "grad_norm": 0.05619528144598007,
      "learning_rate": 1.883e-05,
      "loss": 0.0031,
      "step": 93510
    },
    {
      "epoch": 4.987733333333333,
      "grad_norm": 0.2528728246688843,
      "learning_rate": 1.8826666666666667e-05,
      "loss": 0.0033,
      "step": 93520
    },
    {
      "epoch": 4.988266666666666,
      "grad_norm": 0.16858388483524323,
      "learning_rate": 1.8823333333333333e-05,
      "loss": 0.0025,
      "step": 93530
    },
    {
      "epoch": 4.9888,
      "grad_norm": 0.08429019153118134,
      "learning_rate": 1.8820000000000003e-05,
      "loss": 0.0029,
      "step": 93540
    },
    {
      "epoch": 4.989333333333334,
      "grad_norm": 0.056194718927145004,
      "learning_rate": 1.881666666666667e-05,
      "loss": 0.0031,
      "step": 93550
    },
    {
      "epoch": 4.989866666666667,
      "grad_norm": 0.2528725266456604,
      "learning_rate": 1.8813333333333335e-05,
      "loss": 0.0025,
      "step": 93560
    },
    {
      "epoch": 4.9904,
      "grad_norm": 0.05619383975863457,
      "learning_rate": 1.881e-05,
      "loss": 0.0025,
      "step": 93570
    },
    {
      "epoch": 4.990933333333333,
      "grad_norm": 7.324697494506836,
      "learning_rate": 1.8806666666666667e-05,
      "loss": 0.0044,
      "step": 93580
    },
    {
      "epoch": 4.991466666666667,
      "grad_norm": 0.3371657729148865,
      "learning_rate": 1.8803333333333333e-05,
      "loss": 0.003,
      "step": 93590
    },
    {
      "epoch": 4.992,
      "grad_norm": 0.14055809378623962,
      "learning_rate": 1.88e-05,
      "loss": 0.0029,
      "step": 93600
    },
    {
      "epoch": 4.992533333333333,
      "grad_norm": 0.3092409074306488,
      "learning_rate": 1.879666666666667e-05,
      "loss": 0.0033,
      "step": 93610
    },
    {
      "epoch": 4.9930666666666665,
      "grad_norm": 0.11244617402553558,
      "learning_rate": 1.8793333333333335e-05,
      "loss": 0.0027,
      "step": 93620
    },
    {
      "epoch": 4.9936,
      "grad_norm": 0.056223589926958084,
      "learning_rate": 1.879e-05,
      "loss": 0.0023,
      "step": 93630
    },
    {
      "epoch": 4.994133333333333,
      "grad_norm": 0.056226395070552826,
      "learning_rate": 1.8786666666666667e-05,
      "loss": 0.0033,
      "step": 93640
    },
    {
      "epoch": 4.994666666666666,
      "grad_norm": 3.750155741499839e-09,
      "learning_rate": 1.8783333333333334e-05,
      "loss": 0.0027,
      "step": 93650
    },
    {
      "epoch": 4.9952,
      "grad_norm": 0.08433325588703156,
      "learning_rate": 1.878e-05,
      "loss": 0.0029,
      "step": 93660
    },
    {
      "epoch": 4.995733333333334,
      "grad_norm": 0.0843336209654808,
      "learning_rate": 1.8776666666666666e-05,
      "loss": 0.002,
      "step": 93670
    },
    {
      "epoch": 4.996266666666667,
      "grad_norm": 0.1686689555644989,
      "learning_rate": 1.8773333333333335e-05,
      "loss": 0.0032,
      "step": 93680
    },
    {
      "epoch": 4.9968,
      "grad_norm": 0.2529975473880768,
      "learning_rate": 1.877e-05,
      "loss": 0.0037,
      "step": 93690
    },
    {
      "epoch": 4.997333333333334,
      "grad_norm": 0.3373448848724365,
      "learning_rate": 1.8766666666666668e-05,
      "loss": 0.0041,
      "step": 93700
    },
    {
      "epoch": 4.997866666666667,
      "grad_norm": 0.3935377597808838,
      "learning_rate": 1.8763333333333337e-05,
      "loss": 0.0029,
      "step": 93710
    },
    {
      "epoch": 4.9984,
      "grad_norm": 0.2530098557472229,
      "learning_rate": 1.876e-05,
      "loss": 0.0034,
      "step": 93720
    },
    {
      "epoch": 4.9989333333333335,
      "grad_norm": 0.11244579404592514,
      "learning_rate": 1.8756666666666666e-05,
      "loss": 0.0036,
      "step": 93730
    },
    {
      "epoch": 4.999466666666667,
      "grad_norm": 0.05622175708413124,
      "learning_rate": 1.8753333333333332e-05,
      "loss": 0.0034,
      "step": 93740
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.11244408786296844,
      "learning_rate": 1.8750000000000002e-05,
      "loss": 0.0032,
      "step": 93750
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.002923607360571623,
      "eval_runtime": 172.5992,
      "eval_samples_per_second": 1448.442,
      "eval_steps_per_second": 36.211,
      "step": 93750
    },
    {
      "epoch": 5.000533333333333,
      "grad_norm": 4.863276892308477e-09,
      "learning_rate": 1.8746666666666668e-05,
      "loss": 0.003,
      "step": 93760
    },
    {
      "epoch": 5.0010666666666665,
      "grad_norm": 0.25614747405052185,
      "learning_rate": 1.8743333333333334e-05,
      "loss": 0.0022,
      "step": 93770
    },
    {
      "epoch": 5.0016,
      "grad_norm": 0.3935626149177551,
      "learning_rate": 1.8740000000000004e-05,
      "loss": 0.0034,
      "step": 93780
    },
    {
      "epoch": 5.002133333333333,
      "grad_norm": 0.14055797457695007,
      "learning_rate": 1.8736666666666666e-05,
      "loss": 0.0025,
      "step": 93790
    },
    {
      "epoch": 5.002666666666666,
      "grad_norm": 0.3935420513153076,
      "learning_rate": 1.8733333333333332e-05,
      "loss": 0.003,
      "step": 93800
    },
    {
      "epoch": 5.0032,
      "grad_norm": 0.14055457711219788,
      "learning_rate": 1.8730000000000002e-05,
      "loss": 0.0034,
      "step": 93810
    },
    {
      "epoch": 5.003733333333333,
      "grad_norm": 0.22488826513290405,
      "learning_rate": 1.8726666666666668e-05,
      "loss": 0.0022,
      "step": 93820
    },
    {
      "epoch": 5.004266666666667,
      "grad_norm": 0.2248765081167221,
      "learning_rate": 1.8723333333333334e-05,
      "loss": 0.0027,
      "step": 93830
    },
    {
      "epoch": 5.0048,
      "grad_norm": 0.05622177571058273,
      "learning_rate": 1.872e-05,
      "loss": 0.0035,
      "step": 93840
    },
    {
      "epoch": 5.005333333333334,
      "grad_norm": 0.16865797340869904,
      "learning_rate": 1.871666666666667e-05,
      "loss": 0.0035,
      "step": 93850
    },
    {
      "epoch": 5.005866666666667,
      "grad_norm": 2.179919578182421e-09,
      "learning_rate": 1.8713333333333336e-05,
      "loss": 0.0027,
      "step": 93860
    },
    {
      "epoch": 5.0064,
      "grad_norm": 0.02811279706656933,
      "learning_rate": 1.871e-05,
      "loss": 0.0024,
      "step": 93870
    },
    {
      "epoch": 5.0069333333333335,
      "grad_norm": 0.16866742074489594,
      "learning_rate": 1.8706666666666668e-05,
      "loss": 0.0029,
      "step": 93880
    },
    {
      "epoch": 5.007466666666667,
      "grad_norm": 3.247778046500116e-09,
      "learning_rate": 1.8703333333333334e-05,
      "loss": 0.0034,
      "step": 93890
    },
    {
      "epoch": 5.008,
      "grad_norm": 0.19677942991256714,
      "learning_rate": 1.87e-05,
      "loss": 0.0021,
      "step": 93900
    },
    {
      "epoch": 5.008533333333333,
      "grad_norm": 0.08433648198843002,
      "learning_rate": 1.8696666666666667e-05,
      "loss": 0.0032,
      "step": 93910
    },
    {
      "epoch": 5.009066666666667,
      "grad_norm": 0.14055433869361877,
      "learning_rate": 1.8693333333333336e-05,
      "loss": 0.0028,
      "step": 93920
    },
    {
      "epoch": 5.0096,
      "grad_norm": 0.30920541286468506,
      "learning_rate": 1.8690000000000002e-05,
      "loss": 0.0029,
      "step": 93930
    },
    {
      "epoch": 5.010133333333333,
      "grad_norm": 0.028110086917877197,
      "learning_rate": 1.8686666666666665e-05,
      "loss": 0.0029,
      "step": 93940
    },
    {
      "epoch": 5.010666666666666,
      "grad_norm": 0.11244378983974457,
      "learning_rate": 1.8683333333333335e-05,
      "loss": 0.0039,
      "step": 93950
    },
    {
      "epoch": 5.0112,
      "grad_norm": 0.06529168784618378,
      "learning_rate": 1.868e-05,
      "loss": 0.0037,
      "step": 93960
    },
    {
      "epoch": 5.011733333333333,
      "grad_norm": 0.14054830372333527,
      "learning_rate": 1.8676666666666667e-05,
      "loss": 0.004,
      "step": 93970
    },
    {
      "epoch": 5.012266666666667,
      "grad_norm": 0.1124396100640297,
      "learning_rate": 1.8673333333333333e-05,
      "loss": 0.0022,
      "step": 93980
    },
    {
      "epoch": 5.0128,
      "grad_norm": 0.16866545379161835,
      "learning_rate": 1.8670000000000003e-05,
      "loss": 0.0019,
      "step": 93990
    },
    {
      "epoch": 5.013333333333334,
      "grad_norm": 0.28111451864242554,
      "learning_rate": 1.866666666666667e-05,
      "loss": 0.0017,
      "step": 94000
    },
    {
      "epoch": 5.013866666666667,
      "grad_norm": 0.28902730345726013,
      "learning_rate": 1.8663333333333335e-05,
      "loss": 0.0021,
      "step": 94010
    },
    {
      "epoch": 5.0144,
      "grad_norm": 0.1686680018901825,
      "learning_rate": 1.866e-05,
      "loss": 0.0029,
      "step": 94020
    },
    {
      "epoch": 5.0149333333333335,
      "grad_norm": 0.4497741162776947,
      "learning_rate": 1.8656666666666667e-05,
      "loss": 0.0039,
      "step": 94030
    },
    {
      "epoch": 5.015466666666667,
      "grad_norm": 0.3373064696788788,
      "learning_rate": 1.8653333333333333e-05,
      "loss": 0.0026,
      "step": 94040
    },
    {
      "epoch": 5.016,
      "grad_norm": 0.05621917545795441,
      "learning_rate": 1.865e-05,
      "loss": 0.0025,
      "step": 94050
    },
    {
      "epoch": 5.016533333333333,
      "grad_norm": 0.028109446167945862,
      "learning_rate": 1.864666666666667e-05,
      "loss": 0.0026,
      "step": 94060
    },
    {
      "epoch": 5.017066666666667,
      "grad_norm": 0.3654504418373108,
      "learning_rate": 1.8643333333333335e-05,
      "loss": 0.0016,
      "step": 94070
    },
    {
      "epoch": 5.0176,
      "grad_norm": 0.056221604347229004,
      "learning_rate": 1.864e-05,
      "loss": 0.0026,
      "step": 94080
    },
    {
      "epoch": 5.018133333333333,
      "grad_norm": 0.05621906742453575,
      "learning_rate": 1.863666666666667e-05,
      "loss": 0.0018,
      "step": 94090
    },
    {
      "epoch": 5.018666666666666,
      "grad_norm": 2.117493291819983e-09,
      "learning_rate": 1.8633333333333333e-05,
      "loss": 0.0027,
      "step": 94100
    },
    {
      "epoch": 5.0192,
      "grad_norm": 0.16297830641269684,
      "learning_rate": 1.863e-05,
      "loss": 0.0038,
      "step": 94110
    },
    {
      "epoch": 5.019733333333333,
      "grad_norm": 0.14055553078651428,
      "learning_rate": 1.8626666666666666e-05,
      "loss": 0.0016,
      "step": 94120
    },
    {
      "epoch": 5.020266666666667,
      "grad_norm": 0.14054982364177704,
      "learning_rate": 1.8623333333333335e-05,
      "loss": 0.004,
      "step": 94130
    },
    {
      "epoch": 5.0208,
      "grad_norm": 0.16865405440330505,
      "learning_rate": 1.862e-05,
      "loss": 0.0031,
      "step": 94140
    },
    {
      "epoch": 5.021333333333334,
      "grad_norm": 0.14054952561855316,
      "learning_rate": 1.8616666666666667e-05,
      "loss": 0.0026,
      "step": 94150
    },
    {
      "epoch": 5.021866666666667,
      "grad_norm": 2.210335026120447e-09,
      "learning_rate": 1.8613333333333337e-05,
      "loss": 0.002,
      "step": 94160
    },
    {
      "epoch": 5.0224,
      "grad_norm": 0.39351513981819153,
      "learning_rate": 1.861e-05,
      "loss": 0.0033,
      "step": 94170
    },
    {
      "epoch": 5.0229333333333335,
      "grad_norm": 0.16716498136520386,
      "learning_rate": 1.8606666666666666e-05,
      "loss": 0.0024,
      "step": 94180
    },
    {
      "epoch": 5.023466666666667,
      "grad_norm": 0.1124410480260849,
      "learning_rate": 1.8603333333333335e-05,
      "loss": 0.0035,
      "step": 94190
    },
    {
      "epoch": 5.024,
      "grad_norm": 0.0562182292342186,
      "learning_rate": 1.86e-05,
      "loss": 0.0021,
      "step": 94200
    },
    {
      "epoch": 5.024533333333333,
      "grad_norm": 0.028109686449170113,
      "learning_rate": 1.8596666666666668e-05,
      "loss": 0.0022,
      "step": 94210
    },
    {
      "epoch": 5.025066666666667,
      "grad_norm": 0.140554279088974,
      "learning_rate": 1.8593333333333334e-05,
      "loss": 0.0031,
      "step": 94220
    },
    {
      "epoch": 5.0256,
      "grad_norm": 0.3373175263404846,
      "learning_rate": 1.8590000000000003e-05,
      "loss": 0.0026,
      "step": 94230
    },
    {
      "epoch": 5.026133333333333,
      "grad_norm": 0.028109995648264885,
      "learning_rate": 1.858666666666667e-05,
      "loss": 0.0015,
      "step": 94240
    },
    {
      "epoch": 5.026666666666666,
      "grad_norm": 0.02810855768620968,
      "learning_rate": 1.8583333333333332e-05,
      "loss": 0.0039,
      "step": 94250
    },
    {
      "epoch": 5.0272,
      "grad_norm": 0.05621921271085739,
      "learning_rate": 1.858e-05,
      "loss": 0.0025,
      "step": 94260
    },
    {
      "epoch": 5.027733333333333,
      "grad_norm": 0.1686626374721527,
      "learning_rate": 1.8576666666666668e-05,
      "loss": 0.0029,
      "step": 94270
    },
    {
      "epoch": 5.028266666666667,
      "grad_norm": 0.14054827392101288,
      "learning_rate": 1.8573333333333334e-05,
      "loss": 0.0032,
      "step": 94280
    },
    {
      "epoch": 5.0288,
      "grad_norm": 0.168654203414917,
      "learning_rate": 1.857e-05,
      "loss": 0.0026,
      "step": 94290
    },
    {
      "epoch": 5.029333333333334,
      "grad_norm": 0.14055024087429047,
      "learning_rate": 1.856666666666667e-05,
      "loss": 0.0026,
      "step": 94300
    },
    {
      "epoch": 5.029866666666667,
      "grad_norm": 3.6709815187663253e-09,
      "learning_rate": 1.8563333333333336e-05,
      "loss": 0.0022,
      "step": 94310
    },
    {
      "epoch": 5.0304,
      "grad_norm": 0.3092004954814911,
      "learning_rate": 1.856e-05,
      "loss": 0.0018,
      "step": 94320
    },
    {
      "epoch": 5.0309333333333335,
      "grad_norm": 0.08135942369699478,
      "learning_rate": 1.8556666666666668e-05,
      "loss": 0.0029,
      "step": 94330
    },
    {
      "epoch": 5.031466666666667,
      "grad_norm": 0.028109470382332802,
      "learning_rate": 1.8553333333333334e-05,
      "loss": 0.0028,
      "step": 94340
    },
    {
      "epoch": 5.032,
      "grad_norm": 0.281084269285202,
      "learning_rate": 1.855e-05,
      "loss": 0.0024,
      "step": 94350
    },
    {
      "epoch": 5.032533333333333,
      "grad_norm": 0.05622084066271782,
      "learning_rate": 1.8546666666666666e-05,
      "loss": 0.0039,
      "step": 94360
    },
    {
      "epoch": 5.033066666666667,
      "grad_norm": 0.28109002113342285,
      "learning_rate": 1.8543333333333336e-05,
      "loss": 0.0021,
      "step": 94370
    },
    {
      "epoch": 5.0336,
      "grad_norm": 0.2810773551464081,
      "learning_rate": 1.8540000000000002e-05,
      "loss": 0.0033,
      "step": 94380
    },
    {
      "epoch": 5.034133333333333,
      "grad_norm": 0.08433040231466293,
      "learning_rate": 1.8536666666666668e-05,
      "loss": 0.0039,
      "step": 94390
    },
    {
      "epoch": 5.034666666666666,
      "grad_norm": 0.08432988822460175,
      "learning_rate": 1.8533333333333334e-05,
      "loss": 0.0026,
      "step": 94400
    },
    {
      "epoch": 5.0352,
      "grad_norm": 0.22487089037895203,
      "learning_rate": 1.853e-05,
      "loss": 0.0021,
      "step": 94410
    },
    {
      "epoch": 5.035733333333333,
      "grad_norm": 0.4216321110725403,
      "learning_rate": 1.8526666666666667e-05,
      "loss": 0.0042,
      "step": 94420
    },
    {
      "epoch": 5.036266666666666,
      "grad_norm": 0.3373013436794281,
      "learning_rate": 1.8523333333333333e-05,
      "loss": 0.0018,
      "step": 94430
    },
    {
      "epoch": 5.0368,
      "grad_norm": 0.12352913618087769,
      "learning_rate": 1.8520000000000002e-05,
      "loss": 0.0034,
      "step": 94440
    },
    {
      "epoch": 5.037333333333334,
      "grad_norm": 0.14053936302661896,
      "learning_rate": 1.851666666666667e-05,
      "loss": 0.0028,
      "step": 94450
    },
    {
      "epoch": 5.037866666666667,
      "grad_norm": 0.056216154247522354,
      "learning_rate": 1.8513333333333335e-05,
      "loss": 0.0028,
      "step": 94460
    },
    {
      "epoch": 5.0384,
      "grad_norm": 0.14054352045059204,
      "learning_rate": 1.851e-05,
      "loss": 0.0018,
      "step": 94470
    },
    {
      "epoch": 5.0389333333333335,
      "grad_norm": 0.08432302623987198,
      "learning_rate": 1.8506666666666667e-05,
      "loss": 0.0024,
      "step": 94480
    },
    {
      "epoch": 5.039466666666667,
      "grad_norm": 0.2529790699481964,
      "learning_rate": 1.8503333333333333e-05,
      "loss": 0.0023,
      "step": 94490
    },
    {
      "epoch": 5.04,
      "grad_norm": 0.08432319760322571,
      "learning_rate": 1.85e-05,
      "loss": 0.0028,
      "step": 94500
    },
    {
      "epoch": 5.040533333333333,
      "grad_norm": 1.989592712803301e-09,
      "learning_rate": 1.849666666666667e-05,
      "loss": 0.0025,
      "step": 94510
    },
    {
      "epoch": 5.041066666666667,
      "grad_norm": 0.08432349562644958,
      "learning_rate": 1.8493333333333335e-05,
      "loss": 0.0029,
      "step": 94520
    },
    {
      "epoch": 5.0416,
      "grad_norm": 0.11243419349193573,
      "learning_rate": 1.849e-05,
      "loss": 0.003,
      "step": 94530
    },
    {
      "epoch": 5.042133333333333,
      "grad_norm": 0.19675464928150177,
      "learning_rate": 1.848666666666667e-05,
      "loss": 0.0037,
      "step": 94540
    },
    {
      "epoch": 5.042666666666666,
      "grad_norm": 0.25297486782073975,
      "learning_rate": 1.8483333333333333e-05,
      "loss": 0.0026,
      "step": 94550
    },
    {
      "epoch": 5.0432,
      "grad_norm": 0.05621645599603653,
      "learning_rate": 1.848e-05,
      "loss": 0.0028,
      "step": 94560
    },
    {
      "epoch": 5.043733333333333,
      "grad_norm": 0.0843222364783287,
      "learning_rate": 1.847666666666667e-05,
      "loss": 0.0042,
      "step": 94570
    },
    {
      "epoch": 5.044266666666666,
      "grad_norm": 0.056215789169073105,
      "learning_rate": 1.8473333333333335e-05,
      "loss": 0.0023,
      "step": 94580
    },
    {
      "epoch": 5.0448,
      "grad_norm": 0.028109939768910408,
      "learning_rate": 1.847e-05,
      "loss": 0.0021,
      "step": 94590
    },
    {
      "epoch": 5.045333333333334,
      "grad_norm": 0.196763813495636,
      "learning_rate": 1.8466666666666667e-05,
      "loss": 0.0025,
      "step": 94600
    },
    {
      "epoch": 5.045866666666667,
      "grad_norm": 0.11242692172527313,
      "learning_rate": 1.8463333333333337e-05,
      "loss": 0.0026,
      "step": 94610
    },
    {
      "epoch": 5.0464,
      "grad_norm": 0.08432293683290482,
      "learning_rate": 1.846e-05,
      "loss": 0.0053,
      "step": 94620
    },
    {
      "epoch": 5.0469333333333335,
      "grad_norm": 0.02810750901699066,
      "learning_rate": 1.8456666666666666e-05,
      "loss": 0.0019,
      "step": 94630
    },
    {
      "epoch": 5.047466666666667,
      "grad_norm": 0.19675399363040924,
      "learning_rate": 1.8453333333333335e-05,
      "loss": 0.0026,
      "step": 94640
    },
    {
      "epoch": 5.048,
      "grad_norm": 0.11242881417274475,
      "learning_rate": 1.845e-05,
      "loss": 0.0019,
      "step": 94650
    },
    {
      "epoch": 5.048533333333333,
      "grad_norm": 0.22486041486263275,
      "learning_rate": 1.8446666666666667e-05,
      "loss": 0.0033,
      "step": 94660
    },
    {
      "epoch": 5.049066666666667,
      "grad_norm": 0.05621696263551712,
      "learning_rate": 1.8443333333333333e-05,
      "loss": 0.0032,
      "step": 94670
    },
    {
      "epoch": 5.0496,
      "grad_norm": 0.11243493109941483,
      "learning_rate": 1.8440000000000003e-05,
      "loss": 0.0032,
      "step": 94680
    },
    {
      "epoch": 5.050133333333333,
      "grad_norm": 0.11242781579494476,
      "learning_rate": 1.843666666666667e-05,
      "loss": 0.003,
      "step": 94690
    },
    {
      "epoch": 5.050666666666666,
      "grad_norm": 0.30919864773750305,
      "learning_rate": 1.8433333333333332e-05,
      "loss": 0.0033,
      "step": 94700
    },
    {
      "epoch": 5.0512,
      "grad_norm": 0.055388838052749634,
      "learning_rate": 1.843e-05,
      "loss": 0.0024,
      "step": 94710
    },
    {
      "epoch": 5.051733333333333,
      "grad_norm": 0.028109077364206314,
      "learning_rate": 1.8426666666666668e-05,
      "loss": 0.0028,
      "step": 94720
    },
    {
      "epoch": 5.052266666666666,
      "grad_norm": 0.39348873496055603,
      "learning_rate": 1.8423333333333334e-05,
      "loss": 0.0028,
      "step": 94730
    },
    {
      "epoch": 5.0528,
      "grad_norm": 0.16864913702011108,
      "learning_rate": 1.842e-05,
      "loss": 0.0028,
      "step": 94740
    },
    {
      "epoch": 5.053333333333334,
      "grad_norm": 0.02810792811214924,
      "learning_rate": 1.841666666666667e-05,
      "loss": 0.0023,
      "step": 94750
    },
    {
      "epoch": 5.053866666666667,
      "grad_norm": 0.028106767684221268,
      "learning_rate": 1.8413333333333335e-05,
      "loss": 0.0025,
      "step": 94760
    },
    {
      "epoch": 5.0544,
      "grad_norm": 0.16864082217216492,
      "learning_rate": 1.841e-05,
      "loss": 0.0025,
      "step": 94770
    },
    {
      "epoch": 5.0549333333333335,
      "grad_norm": 0.0843205600976944,
      "learning_rate": 1.8406666666666668e-05,
      "loss": 0.0033,
      "step": 94780
    },
    {
      "epoch": 5.055466666666667,
      "grad_norm": 0.028107324615120888,
      "learning_rate": 1.8403333333333334e-05,
      "loss": 0.0052,
      "step": 94790
    },
    {
      "epoch": 5.056,
      "grad_norm": 0.1405341476202011,
      "learning_rate": 1.84e-05,
      "loss": 0.0025,
      "step": 94800
    },
    {
      "epoch": 5.056533333333333,
      "grad_norm": 0.11243382841348648,
      "learning_rate": 1.8396666666666666e-05,
      "loss": 0.0021,
      "step": 94810
    },
    {
      "epoch": 5.057066666666667,
      "grad_norm": 0.14054441452026367,
      "learning_rate": 1.8393333333333336e-05,
      "loss": 0.0031,
      "step": 94820
    },
    {
      "epoch": 5.0576,
      "grad_norm": 0.056213077157735825,
      "learning_rate": 1.8390000000000002e-05,
      "loss": 0.0016,
      "step": 94830
    },
    {
      "epoch": 5.058133333333333,
      "grad_norm": 0.22485685348510742,
      "learning_rate": 1.8386666666666668e-05,
      "loss": 0.003,
      "step": 94840
    },
    {
      "epoch": 5.058666666666666,
      "grad_norm": 0.1967625766992569,
      "learning_rate": 1.8383333333333334e-05,
      "loss": 0.0034,
      "step": 94850
    },
    {
      "epoch": 5.0592,
      "grad_norm": 1.4066725969314575,
      "learning_rate": 1.838e-05,
      "loss": 0.0032,
      "step": 94860
    },
    {
      "epoch": 5.059733333333333,
      "grad_norm": 0.08431974798440933,
      "learning_rate": 1.8376666666666666e-05,
      "loss": 0.0025,
      "step": 94870
    },
    {
      "epoch": 5.060266666666666,
      "grad_norm": 0.2529620826244354,
      "learning_rate": 1.8373333333333332e-05,
      "loss": 0.0015,
      "step": 94880
    },
    {
      "epoch": 5.0608,
      "grad_norm": 0.028105782344937325,
      "learning_rate": 1.8370000000000002e-05,
      "loss": 0.0026,
      "step": 94890
    },
    {
      "epoch": 5.061333333333334,
      "grad_norm": 0.056216493248939514,
      "learning_rate": 1.8366666666666668e-05,
      "loss": 0.0036,
      "step": 94900
    },
    {
      "epoch": 5.061866666666667,
      "grad_norm": 0.3934985399246216,
      "learning_rate": 1.8363333333333334e-05,
      "loss": 0.0034,
      "step": 94910
    },
    {
      "epoch": 5.0624,
      "grad_norm": 0.28106141090393066,
      "learning_rate": 1.8360000000000004e-05,
      "loss": 0.0027,
      "step": 94920
    },
    {
      "epoch": 5.0629333333333335,
      "grad_norm": 2.6447797285555907e-09,
      "learning_rate": 1.8356666666666667e-05,
      "loss": 0.0033,
      "step": 94930
    },
    {
      "epoch": 5.063466666666667,
      "grad_norm": 0.056215107440948486,
      "learning_rate": 1.8353333333333333e-05,
      "loss": 0.0023,
      "step": 94940
    },
    {
      "epoch": 5.064,
      "grad_norm": 0.3653736114501953,
      "learning_rate": 1.8350000000000002e-05,
      "loss": 0.0024,
      "step": 94950
    },
    {
      "epoch": 5.064533333333333,
      "grad_norm": 0.14053556323051453,
      "learning_rate": 1.834666666666667e-05,
      "loss": 0.0027,
      "step": 94960
    },
    {
      "epoch": 5.065066666666667,
      "grad_norm": 0.08431852608919144,
      "learning_rate": 1.8343333333333334e-05,
      "loss": 0.0037,
      "step": 94970
    },
    {
      "epoch": 5.0656,
      "grad_norm": 0.3372845947742462,
      "learning_rate": 1.834e-05,
      "loss": 0.0024,
      "step": 94980
    },
    {
      "epoch": 5.066133333333333,
      "grad_norm": 0.2248431146144867,
      "learning_rate": 1.833666666666667e-05,
      "loss": 0.003,
      "step": 94990
    },
    {
      "epoch": 5.066666666666666,
      "grad_norm": 0.3372933864593506,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 0.0022,
      "step": 95000
    },
    {
      "epoch": 5.0672,
      "grad_norm": 0.11242936551570892,
      "learning_rate": 1.833e-05,
      "loss": 0.0026,
      "step": 95010
    },
    {
      "epoch": 5.067733333333333,
      "grad_norm": 0.05621194466948509,
      "learning_rate": 1.832666666666667e-05,
      "loss": 0.0026,
      "step": 95020
    },
    {
      "epoch": 5.068266666666666,
      "grad_norm": 0.11242396384477615,
      "learning_rate": 1.8323333333333335e-05,
      "loss": 0.0026,
      "step": 95030
    },
    {
      "epoch": 5.0688,
      "grad_norm": 0.1124265193939209,
      "learning_rate": 1.832e-05,
      "loss": 0.0023,
      "step": 95040
    },
    {
      "epoch": 5.069333333333334,
      "grad_norm": 0.0281057171523571,
      "learning_rate": 1.8316666666666667e-05,
      "loss": 0.002,
      "step": 95050
    },
    {
      "epoch": 5.069866666666667,
      "grad_norm": 0.11242102086544037,
      "learning_rate": 1.8313333333333336e-05,
      "loss": 0.0031,
      "step": 95060
    },
    {
      "epoch": 5.0704,
      "grad_norm": 0.22485294938087463,
      "learning_rate": 1.8310000000000003e-05,
      "loss": 0.0041,
      "step": 95070
    },
    {
      "epoch": 5.0709333333333335,
      "grad_norm": 2.758573055267334,
      "learning_rate": 1.8306666666666665e-05,
      "loss": 0.0026,
      "step": 95080
    },
    {
      "epoch": 5.071466666666667,
      "grad_norm": 1.7259000895109011e-09,
      "learning_rate": 1.8303333333333335e-05,
      "loss": 0.0036,
      "step": 95090
    },
    {
      "epoch": 5.072,
      "grad_norm": 3.8661154277974674e-09,
      "learning_rate": 1.83e-05,
      "loss": 0.003,
      "step": 95100
    },
    {
      "epoch": 5.072533333333333,
      "grad_norm": 0.25296536087989807,
      "learning_rate": 1.8296666666666667e-05,
      "loss": 0.0025,
      "step": 95110
    },
    {
      "epoch": 5.073066666666667,
      "grad_norm": 0.14052468538284302,
      "learning_rate": 1.8293333333333333e-05,
      "loss": 0.0019,
      "step": 95120
    },
    {
      "epoch": 5.0736,
      "grad_norm": 0.3934866487979889,
      "learning_rate": 1.8290000000000003e-05,
      "loss": 0.0034,
      "step": 95130
    },
    {
      "epoch": 5.074133333333333,
      "grad_norm": 0.056215353310108185,
      "learning_rate": 1.828666666666667e-05,
      "loss": 0.0047,
      "step": 95140
    },
    {
      "epoch": 5.074666666666666,
      "grad_norm": 0.39345693588256836,
      "learning_rate": 1.828333333333333e-05,
      "loss": 0.0029,
      "step": 95150
    },
    {
      "epoch": 5.0752,
      "grad_norm": 0.08030474931001663,
      "learning_rate": 1.828e-05,
      "loss": 0.0038,
      "step": 95160
    },
    {
      "epoch": 5.075733333333333,
      "grad_norm": 0.1405356377363205,
      "learning_rate": 1.8276666666666667e-05,
      "loss": 0.0015,
      "step": 95170
    },
    {
      "epoch": 5.076266666666666,
      "grad_norm": 0.05620914697647095,
      "learning_rate": 1.8273333333333333e-05,
      "loss": 0.0029,
      "step": 95180
    },
    {
      "epoch": 5.0768,
      "grad_norm": 0.1686340719461441,
      "learning_rate": 1.827e-05,
      "loss": 0.002,
      "step": 95190
    },
    {
      "epoch": 5.077333333333334,
      "grad_norm": 0.05621155723929405,
      "learning_rate": 1.826666666666667e-05,
      "loss": 0.0025,
      "step": 95200
    },
    {
      "epoch": 5.077866666666667,
      "grad_norm": 8.2257467504121e-10,
      "learning_rate": 1.8263333333333335e-05,
      "loss": 0.004,
      "step": 95210
    },
    {
      "epoch": 5.0784,
      "grad_norm": 0.14052100479602814,
      "learning_rate": 1.826e-05,
      "loss": 0.003,
      "step": 95220
    },
    {
      "epoch": 5.0789333333333335,
      "grad_norm": 0.4216057360172272,
      "learning_rate": 1.8256666666666667e-05,
      "loss": 0.003,
      "step": 95230
    },
    {
      "epoch": 5.079466666666667,
      "grad_norm": 0.052496373653411865,
      "learning_rate": 1.8253333333333334e-05,
      "loss": 0.0018,
      "step": 95240
    },
    {
      "epoch": 5.08,
      "grad_norm": 0.056209396570920944,
      "learning_rate": 1.825e-05,
      "loss": 0.0029,
      "step": 95250
    },
    {
      "epoch": 5.080533333333333,
      "grad_norm": 0.02810371294617653,
      "learning_rate": 1.8246666666666666e-05,
      "loss": 0.003,
      "step": 95260
    },
    {
      "epoch": 5.081066666666667,
      "grad_norm": 0.14052513241767883,
      "learning_rate": 1.8243333333333335e-05,
      "loss": 0.0025,
      "step": 95270
    },
    {
      "epoch": 5.0816,
      "grad_norm": 2.8965356779764306e-09,
      "learning_rate": 1.824e-05,
      "loss": 0.003,
      "step": 95280
    },
    {
      "epoch": 5.082133333333333,
      "grad_norm": 0.05621004104614258,
      "learning_rate": 1.8236666666666668e-05,
      "loss": 0.0039,
      "step": 95290
    },
    {
      "epoch": 5.082666666666666,
      "grad_norm": 0.28103727102279663,
      "learning_rate": 1.8233333333333334e-05,
      "loss": 0.0027,
      "step": 95300
    },
    {
      "epoch": 5.0832,
      "grad_norm": 0.19673025608062744,
      "learning_rate": 1.823e-05,
      "loss": 0.0021,
      "step": 95310
    },
    {
      "epoch": 5.083733333333333,
      "grad_norm": 0.028104295954108238,
      "learning_rate": 1.8226666666666666e-05,
      "loss": 0.0038,
      "step": 95320
    },
    {
      "epoch": 5.084266666666666,
      "grad_norm": 0.08431249856948853,
      "learning_rate": 1.8223333333333336e-05,
      "loss": 0.004,
      "step": 95330
    },
    {
      "epoch": 5.0848,
      "grad_norm": 0.16862231492996216,
      "learning_rate": 1.8220000000000002e-05,
      "loss": 0.002,
      "step": 95340
    },
    {
      "epoch": 5.085333333333334,
      "grad_norm": 0.22482937574386597,
      "learning_rate": 1.8216666666666668e-05,
      "loss": 0.0022,
      "step": 95350
    },
    {
      "epoch": 5.085866666666667,
      "grad_norm": 0.08431348949670792,
      "learning_rate": 1.8213333333333334e-05,
      "loss": 0.0031,
      "step": 95360
    },
    {
      "epoch": 5.0864,
      "grad_norm": 0.14052222669124603,
      "learning_rate": 1.8210000000000004e-05,
      "loss": 0.0025,
      "step": 95370
    },
    {
      "epoch": 5.0869333333333335,
      "grad_norm": 0.3934546411037445,
      "learning_rate": 1.8206666666666666e-05,
      "loss": 0.0024,
      "step": 95380
    },
    {
      "epoch": 5.087466666666667,
      "grad_norm": 0.19673581421375275,
      "learning_rate": 1.8203333333333332e-05,
      "loss": 0.0017,
      "step": 95390
    },
    {
      "epoch": 5.088,
      "grad_norm": 0.16862107813358307,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 0.0027,
      "step": 95400
    },
    {
      "epoch": 5.088533333333333,
      "grad_norm": 0.05620912089943886,
      "learning_rate": 1.8196666666666668e-05,
      "loss": 0.0026,
      "step": 95410
    },
    {
      "epoch": 5.089066666666667,
      "grad_norm": 0.11241994053125381,
      "learning_rate": 1.8193333333333334e-05,
      "loss": 0.0031,
      "step": 95420
    },
    {
      "epoch": 5.0896,
      "grad_norm": 0.22483520209789276,
      "learning_rate": 1.819e-05,
      "loss": 0.0024,
      "step": 95430
    },
    {
      "epoch": 5.090133333333333,
      "grad_norm": 0.30912867188453674,
      "learning_rate": 1.818666666666667e-05,
      "loss": 0.0035,
      "step": 95440
    },
    {
      "epoch": 5.0906666666666665,
      "grad_norm": 0.5340198874473572,
      "learning_rate": 1.8183333333333336e-05,
      "loss": 0.0031,
      "step": 95450
    },
    {
      "epoch": 5.0912,
      "grad_norm": 0.05620645359158516,
      "learning_rate": 1.818e-05,
      "loss": 0.0032,
      "step": 95460
    },
    {
      "epoch": 5.091733333333333,
      "grad_norm": 0.05620718374848366,
      "learning_rate": 1.8176666666666668e-05,
      "loss": 0.004,
      "step": 95470
    },
    {
      "epoch": 5.092266666666666,
      "grad_norm": 0.12912426888942719,
      "learning_rate": 1.8173333333333334e-05,
      "loss": 0.0038,
      "step": 95480
    },
    {
      "epoch": 5.0928,
      "grad_norm": 0.14051716029644012,
      "learning_rate": 1.817e-05,
      "loss": 0.0024,
      "step": 95490
    },
    {
      "epoch": 5.093333333333334,
      "grad_norm": 0.028102891519665718,
      "learning_rate": 1.8166666666666667e-05,
      "loss": 0.0039,
      "step": 95500
    },
    {
      "epoch": 5.093866666666667,
      "grad_norm": 0.36536461114883423,
      "learning_rate": 1.8163333333333336e-05,
      "loss": 0.0022,
      "step": 95510
    },
    {
      "epoch": 5.0944,
      "grad_norm": 0.2248174101114273,
      "learning_rate": 1.8160000000000002e-05,
      "loss": 0.0023,
      "step": 95520
    },
    {
      "epoch": 5.0949333333333335,
      "grad_norm": 0.3091343641281128,
      "learning_rate": 1.8156666666666665e-05,
      "loss": 0.0026,
      "step": 95530
    },
    {
      "epoch": 5.095466666666667,
      "grad_norm": 0.14051470160484314,
      "learning_rate": 1.8153333333333335e-05,
      "loss": 0.0035,
      "step": 95540
    },
    {
      "epoch": 5.096,
      "grad_norm": 0.05620545148849487,
      "learning_rate": 1.815e-05,
      "loss": 0.0028,
      "step": 95550
    },
    {
      "epoch": 5.096533333333333,
      "grad_norm": 0.5339643955230713,
      "learning_rate": 1.8146666666666667e-05,
      "loss": 0.002,
      "step": 95560
    },
    {
      "epoch": 5.097066666666667,
      "grad_norm": 2.870035098467838e-09,
      "learning_rate": 1.8143333333333333e-05,
      "loss": 0.0028,
      "step": 95570
    },
    {
      "epoch": 5.0976,
      "grad_norm": 0.08430817723274231,
      "learning_rate": 1.8140000000000003e-05,
      "loss": 0.0023,
      "step": 95580
    },
    {
      "epoch": 5.098133333333333,
      "grad_norm": 0.056206610053777695,
      "learning_rate": 1.813666666666667e-05,
      "loss": 0.0026,
      "step": 95590
    },
    {
      "epoch": 5.0986666666666665,
      "grad_norm": 0.16861921548843384,
      "learning_rate": 1.8133333333333335e-05,
      "loss": 0.0023,
      "step": 95600
    },
    {
      "epoch": 5.0992,
      "grad_norm": 0.22482119500637054,
      "learning_rate": 1.813e-05,
      "loss": 0.0022,
      "step": 95610
    },
    {
      "epoch": 5.099733333333333,
      "grad_norm": 0.14051876962184906,
      "learning_rate": 1.8126666666666667e-05,
      "loss": 0.0033,
      "step": 95620
    },
    {
      "epoch": 5.100266666666666,
      "grad_norm": 0.22482793033123016,
      "learning_rate": 1.8123333333333333e-05,
      "loss": 0.0024,
      "step": 95630
    },
    {
      "epoch": 5.1008,
      "grad_norm": 0.14051532745361328,
      "learning_rate": 1.812e-05,
      "loss": 0.0031,
      "step": 95640
    },
    {
      "epoch": 5.101333333333334,
      "grad_norm": 5.209506034851074,
      "learning_rate": 1.811666666666667e-05,
      "loss": 0.0032,
      "step": 95650
    },
    {
      "epoch": 5.101866666666667,
      "grad_norm": 0.08431152999401093,
      "learning_rate": 1.8113333333333335e-05,
      "loss": 0.0025,
      "step": 95660
    },
    {
      "epoch": 5.1024,
      "grad_norm": 0.16861586272716522,
      "learning_rate": 1.811e-05,
      "loss": 0.0027,
      "step": 95670
    },
    {
      "epoch": 5.1029333333333335,
      "grad_norm": 0.19672252237796783,
      "learning_rate": 1.8106666666666667e-05,
      "loss": 0.003,
      "step": 95680
    },
    {
      "epoch": 5.103466666666667,
      "grad_norm": 0.16861476004123688,
      "learning_rate": 1.8103333333333333e-05,
      "loss": 0.0032,
      "step": 95690
    },
    {
      "epoch": 5.104,
      "grad_norm": 0.1686200052499771,
      "learning_rate": 1.81e-05,
      "loss": 0.0026,
      "step": 95700
    },
    {
      "epoch": 5.104533333333333,
      "grad_norm": 0.6744413375854492,
      "learning_rate": 1.8096666666666666e-05,
      "loss": 0.002,
      "step": 95710
    },
    {
      "epoch": 5.105066666666667,
      "grad_norm": 0.16861586272716522,
      "learning_rate": 1.8093333333333335e-05,
      "loss": 0.0039,
      "step": 95720
    },
    {
      "epoch": 5.1056,
      "grad_norm": 0.028101982548832893,
      "learning_rate": 1.809e-05,
      "loss": 0.0039,
      "step": 95730
    },
    {
      "epoch": 5.106133333333333,
      "grad_norm": 0.08431285619735718,
      "learning_rate": 1.8086666666666667e-05,
      "loss": 0.003,
      "step": 95740
    },
    {
      "epoch": 5.1066666666666665,
      "grad_norm": 0.08430778235197067,
      "learning_rate": 1.8083333333333337e-05,
      "loss": 0.0047,
      "step": 95750
    },
    {
      "epoch": 5.1072,
      "grad_norm": 0.4215535521507263,
      "learning_rate": 1.808e-05,
      "loss": 0.0032,
      "step": 95760
    },
    {
      "epoch": 5.107733333333333,
      "grad_norm": 0.3372427821159363,
      "learning_rate": 1.8076666666666666e-05,
      "loss": 0.0029,
      "step": 95770
    },
    {
      "epoch": 5.108266666666666,
      "grad_norm": 0.08430470526218414,
      "learning_rate": 1.8073333333333335e-05,
      "loss": 0.0031,
      "step": 95780
    },
    {
      "epoch": 5.1088,
      "grad_norm": 0.25292298197746277,
      "learning_rate": 1.807e-05,
      "loss": 0.003,
      "step": 95790
    },
    {
      "epoch": 5.109333333333334,
      "grad_norm": 0.028102273121476173,
      "learning_rate": 1.8066666666666668e-05,
      "loss": 0.0025,
      "step": 95800
    },
    {
      "epoch": 5.109866666666667,
      "grad_norm": 0.2529166638851166,
      "learning_rate": 1.8063333333333334e-05,
      "loss": 0.0019,
      "step": 95810
    },
    {
      "epoch": 5.1104,
      "grad_norm": 2.056799619509775e-09,
      "learning_rate": 1.8060000000000003e-05,
      "loss": 0.0024,
      "step": 95820
    },
    {
      "epoch": 5.1109333333333336,
      "grad_norm": 0.5058432817459106,
      "learning_rate": 1.8056666666666666e-05,
      "loss": 0.0028,
      "step": 95830
    },
    {
      "epoch": 5.111466666666667,
      "grad_norm": 0.05620555207133293,
      "learning_rate": 1.8053333333333332e-05,
      "loss": 0.0026,
      "step": 95840
    },
    {
      "epoch": 5.112,
      "grad_norm": 0.056205593049526215,
      "learning_rate": 1.805e-05,
      "loss": 0.0017,
      "step": 95850
    },
    {
      "epoch": 5.112533333333333,
      "grad_norm": 0.05620468780398369,
      "learning_rate": 1.8046666666666668e-05,
      "loss": 0.0032,
      "step": 95860
    },
    {
      "epoch": 5.113066666666667,
      "grad_norm": 0.08430728316307068,
      "learning_rate": 1.8043333333333334e-05,
      "loss": 0.0034,
      "step": 95870
    },
    {
      "epoch": 5.1136,
      "grad_norm": 0.16861742734909058,
      "learning_rate": 1.804e-05,
      "loss": 0.0021,
      "step": 95880
    },
    {
      "epoch": 5.114133333333333,
      "grad_norm": 0.11240848153829575,
      "learning_rate": 1.803666666666667e-05,
      "loss": 0.0017,
      "step": 95890
    },
    {
      "epoch": 5.1146666666666665,
      "grad_norm": 0.19671683013439178,
      "learning_rate": 1.8033333333333336e-05,
      "loss": 0.0025,
      "step": 95900
    },
    {
      "epoch": 5.1152,
      "grad_norm": 0.08430558443069458,
      "learning_rate": 1.803e-05,
      "loss": 0.0026,
      "step": 95910
    },
    {
      "epoch": 5.115733333333333,
      "grad_norm": 0.11241050064563751,
      "learning_rate": 1.8026666666666668e-05,
      "loss": 0.0021,
      "step": 95920
    },
    {
      "epoch": 5.116266666666666,
      "grad_norm": 0.14051587879657745,
      "learning_rate": 1.8023333333333334e-05,
      "loss": 0.0023,
      "step": 95930
    },
    {
      "epoch": 5.1168,
      "grad_norm": 0.08430497348308563,
      "learning_rate": 1.802e-05,
      "loss": 0.0031,
      "step": 95940
    },
    {
      "epoch": 5.117333333333334,
      "grad_norm": 0.3372270464897156,
      "learning_rate": 1.8016666666666666e-05,
      "loss": 0.0023,
      "step": 95950
    },
    {
      "epoch": 5.117866666666667,
      "grad_norm": 0.02810318022966385,
      "learning_rate": 1.8013333333333336e-05,
      "loss": 0.0029,
      "step": 95960
    },
    {
      "epoch": 5.1184,
      "grad_norm": 0.22482316195964813,
      "learning_rate": 1.8010000000000002e-05,
      "loss": 0.0029,
      "step": 95970
    },
    {
      "epoch": 5.118933333333334,
      "grad_norm": 0.0562032125890255,
      "learning_rate": 1.8006666666666668e-05,
      "loss": 0.0022,
      "step": 95980
    },
    {
      "epoch": 5.119466666666667,
      "grad_norm": 0.028103355318307877,
      "learning_rate": 1.8003333333333334e-05,
      "loss": 0.0029,
      "step": 95990
    },
    {
      "epoch": 5.12,
      "grad_norm": 0.25292304158210754,
      "learning_rate": 1.8e-05,
      "loss": 0.0029,
      "step": 96000
    },
    {
      "epoch": 5.120533333333333,
      "grad_norm": 0.33721446990966797,
      "learning_rate": 1.7996666666666667e-05,
      "loss": 0.0027,
      "step": 96010
    },
    {
      "epoch": 5.121066666666667,
      "grad_norm": 0.3653583526611328,
      "learning_rate": 1.7993333333333333e-05,
      "loss": 0.0038,
      "step": 96020
    },
    {
      "epoch": 5.1216,
      "grad_norm": 0.1686205267906189,
      "learning_rate": 1.7990000000000002e-05,
      "loss": 0.0039,
      "step": 96030
    },
    {
      "epoch": 5.122133333333333,
      "grad_norm": 0.2248237282037735,
      "learning_rate": 1.798666666666667e-05,
      "loss": 0.0029,
      "step": 96040
    },
    {
      "epoch": 5.1226666666666665,
      "grad_norm": 0.2529072165489197,
      "learning_rate": 1.7983333333333335e-05,
      "loss": 0.0021,
      "step": 96050
    },
    {
      "epoch": 5.1232,
      "grad_norm": 0.11240962892770767,
      "learning_rate": 1.798e-05,
      "loss": 0.0029,
      "step": 96060
    },
    {
      "epoch": 5.123733333333333,
      "grad_norm": 0.16862253844738007,
      "learning_rate": 1.7976666666666667e-05,
      "loss": 0.0023,
      "step": 96070
    },
    {
      "epoch": 5.124266666666666,
      "grad_norm": 5.1331467628479,
      "learning_rate": 1.7973333333333333e-05,
      "loss": 0.0035,
      "step": 96080
    },
    {
      "epoch": 5.1248,
      "grad_norm": 0.1405065506696701,
      "learning_rate": 1.797e-05,
      "loss": 0.0026,
      "step": 96090
    },
    {
      "epoch": 5.125333333333334,
      "grad_norm": 0.2248317450284958,
      "learning_rate": 1.796666666666667e-05,
      "loss": 0.0024,
      "step": 96100
    },
    {
      "epoch": 5.125866666666667,
      "grad_norm": 0.2528977692127228,
      "learning_rate": 1.7963333333333335e-05,
      "loss": 0.0023,
      "step": 96110
    },
    {
      "epoch": 5.1264,
      "grad_norm": 0.2810089886188507,
      "learning_rate": 1.796e-05,
      "loss": 0.0023,
      "step": 96120
    },
    {
      "epoch": 5.126933333333334,
      "grad_norm": 0.4496212899684906,
      "learning_rate": 1.795666666666667e-05,
      "loss": 0.0033,
      "step": 96130
    },
    {
      "epoch": 5.127466666666667,
      "grad_norm": 0.08430597931146622,
      "learning_rate": 1.7953333333333333e-05,
      "loss": 0.0029,
      "step": 96140
    },
    {
      "epoch": 5.128,
      "grad_norm": 0.14049862325191498,
      "learning_rate": 1.795e-05,
      "loss": 0.003,
      "step": 96150
    },
    {
      "epoch": 5.128533333333333,
      "grad_norm": 0.07282593846321106,
      "learning_rate": 1.794666666666667e-05,
      "loss": 0.0025,
      "step": 96160
    },
    {
      "epoch": 5.129066666666667,
      "grad_norm": 0.14051026105880737,
      "learning_rate": 1.7943333333333335e-05,
      "loss": 0.0024,
      "step": 96170
    },
    {
      "epoch": 5.1296,
      "grad_norm": 0.05620187893509865,
      "learning_rate": 1.794e-05,
      "loss": 0.0017,
      "step": 96180
    },
    {
      "epoch": 5.130133333333333,
      "grad_norm": 0.44959497451782227,
      "learning_rate": 1.7936666666666667e-05,
      "loss": 0.002,
      "step": 96190
    },
    {
      "epoch": 5.1306666666666665,
      "grad_norm": 0.3372253179550171,
      "learning_rate": 1.7933333333333337e-05,
      "loss": 0.003,
      "step": 96200
    },
    {
      "epoch": 5.1312,
      "grad_norm": 0.14050866663455963,
      "learning_rate": 1.793e-05,
      "loss": 0.0025,
      "step": 96210
    },
    {
      "epoch": 5.131733333333333,
      "grad_norm": 0.2529084384441376,
      "learning_rate": 1.7926666666666666e-05,
      "loss": 0.0023,
      "step": 96220
    },
    {
      "epoch": 5.132266666666666,
      "grad_norm": 0.14051014184951782,
      "learning_rate": 1.7923333333333335e-05,
      "loss": 0.002,
      "step": 96230
    },
    {
      "epoch": 5.1328,
      "grad_norm": 0.028100522235035896,
      "learning_rate": 1.792e-05,
      "loss": 0.0017,
      "step": 96240
    },
    {
      "epoch": 5.133333333333334,
      "grad_norm": 0.11240585148334503,
      "learning_rate": 1.7916666666666667e-05,
      "loss": 0.0031,
      "step": 96250
    },
    {
      "epoch": 5.133866666666667,
      "grad_norm": 0.11240093410015106,
      "learning_rate": 1.7913333333333333e-05,
      "loss": 0.0024,
      "step": 96260
    },
    {
      "epoch": 5.1344,
      "grad_norm": 0.02810157649219036,
      "learning_rate": 1.7910000000000003e-05,
      "loss": 0.0028,
      "step": 96270
    },
    {
      "epoch": 5.134933333333334,
      "grad_norm": 0.028102122247219086,
      "learning_rate": 1.790666666666667e-05,
      "loss": 0.0032,
      "step": 96280
    },
    {
      "epoch": 5.135466666666667,
      "grad_norm": 0.19670946896076202,
      "learning_rate": 1.7903333333333332e-05,
      "loss": 0.0031,
      "step": 96290
    },
    {
      "epoch": 5.136,
      "grad_norm": 0.11240001767873764,
      "learning_rate": 1.79e-05,
      "loss": 0.0018,
      "step": 96300
    },
    {
      "epoch": 5.136533333333333,
      "grad_norm": 0.11240121722221375,
      "learning_rate": 1.7896666666666668e-05,
      "loss": 0.0025,
      "step": 96310
    },
    {
      "epoch": 5.137066666666667,
      "grad_norm": 0.028100302442908287,
      "learning_rate": 1.7893333333333334e-05,
      "loss": 0.002,
      "step": 96320
    },
    {
      "epoch": 5.1376,
      "grad_norm": 0.3372000753879547,
      "learning_rate": 1.789e-05,
      "loss": 0.0034,
      "step": 96330
    },
    {
      "epoch": 5.138133333333333,
      "grad_norm": 0.16860942542552948,
      "learning_rate": 1.788666666666667e-05,
      "loss": 0.002,
      "step": 96340
    },
    {
      "epoch": 5.1386666666666665,
      "grad_norm": 0.39339393377304077,
      "learning_rate": 1.7883333333333335e-05,
      "loss": 0.003,
      "step": 96350
    },
    {
      "epoch": 5.1392,
      "grad_norm": 0.22481153905391693,
      "learning_rate": 1.7879999999999998e-05,
      "loss": 0.0019,
      "step": 96360
    },
    {
      "epoch": 5.139733333333333,
      "grad_norm": 0.11240420490503311,
      "learning_rate": 1.7876666666666668e-05,
      "loss": 0.0028,
      "step": 96370
    },
    {
      "epoch": 5.140266666666666,
      "grad_norm": 0.08430298417806625,
      "learning_rate": 1.7873333333333334e-05,
      "loss": 0.0015,
      "step": 96380
    },
    {
      "epoch": 5.1408,
      "grad_norm": 0.3090943992137909,
      "learning_rate": 1.787e-05,
      "loss": 0.0049,
      "step": 96390
    },
    {
      "epoch": 5.141333333333334,
      "grad_norm": 0.061588406562805176,
      "learning_rate": 1.7866666666666666e-05,
      "loss": 0.0025,
      "step": 96400
    },
    {
      "epoch": 5.141866666666667,
      "grad_norm": 0.19670039415359497,
      "learning_rate": 1.7863333333333336e-05,
      "loss": 0.0026,
      "step": 96410
    },
    {
      "epoch": 5.1424,
      "grad_norm": 0.505776047706604,
      "learning_rate": 1.7860000000000002e-05,
      "loss": 0.0032,
      "step": 96420
    },
    {
      "epoch": 5.142933333333334,
      "grad_norm": 3.0516339455388675e-10,
      "learning_rate": 1.7856666666666668e-05,
      "loss": 0.0041,
      "step": 96430
    },
    {
      "epoch": 5.143466666666667,
      "grad_norm": 0.08430449664592743,
      "learning_rate": 1.7853333333333334e-05,
      "loss": 0.0031,
      "step": 96440
    },
    {
      "epoch": 5.144,
      "grad_norm": 0.1966976523399353,
      "learning_rate": 1.785e-05,
      "loss": 0.002,
      "step": 96450
    },
    {
      "epoch": 5.144533333333333,
      "grad_norm": 1.613497224717264e-09,
      "learning_rate": 1.7846666666666666e-05,
      "loss": 0.0033,
      "step": 96460
    },
    {
      "epoch": 5.145066666666667,
      "grad_norm": 0.16860006749629974,
      "learning_rate": 1.7843333333333332e-05,
      "loss": 0.0035,
      "step": 96470
    },
    {
      "epoch": 5.1456,
      "grad_norm": 0.11239848285913467,
      "learning_rate": 1.7840000000000002e-05,
      "loss": 0.0025,
      "step": 96480
    },
    {
      "epoch": 5.146133333333333,
      "grad_norm": 0.2529069781303406,
      "learning_rate": 1.7836666666666668e-05,
      "loss": 0.0035,
      "step": 96490
    },
    {
      "epoch": 5.1466666666666665,
      "grad_norm": 0.3652792274951935,
      "learning_rate": 1.7833333333333334e-05,
      "loss": 0.0033,
      "step": 96500
    },
    {
      "epoch": 5.1472,
      "grad_norm": 0.47773635387420654,
      "learning_rate": 1.783e-05,
      "loss": 0.0026,
      "step": 96510
    },
    {
      "epoch": 5.147733333333333,
      "grad_norm": 0.252896785736084,
      "learning_rate": 1.7826666666666667e-05,
      "loss": 0.0034,
      "step": 96520
    },
    {
      "epoch": 5.148266666666666,
      "grad_norm": 0.25289595127105713,
      "learning_rate": 1.7823333333333333e-05,
      "loss": 0.0027,
      "step": 96530
    },
    {
      "epoch": 5.1488,
      "grad_norm": 0.22480173408985138,
      "learning_rate": 1.7820000000000002e-05,
      "loss": 0.0022,
      "step": 96540
    },
    {
      "epoch": 5.149333333333334,
      "grad_norm": 0.2847079038619995,
      "learning_rate": 1.781666666666667e-05,
      "loss": 0.0049,
      "step": 96550
    },
    {
      "epoch": 5.149866666666667,
      "grad_norm": 0.2528967559337616,
      "learning_rate": 1.7813333333333334e-05,
      "loss": 0.0021,
      "step": 96560
    },
    {
      "epoch": 5.1504,
      "grad_norm": 0.056199125945568085,
      "learning_rate": 1.781e-05,
      "loss": 0.003,
      "step": 96570
    },
    {
      "epoch": 5.150933333333334,
      "grad_norm": 0.08429590612649918,
      "learning_rate": 1.780666666666667e-05,
      "loss": 0.0028,
      "step": 96580
    },
    {
      "epoch": 5.151466666666667,
      "grad_norm": 0.477681040763855,
      "learning_rate": 1.7803333333333333e-05,
      "loss": 0.0023,
      "step": 96590
    },
    {
      "epoch": 5.152,
      "grad_norm": 0.1686066836118698,
      "learning_rate": 1.78e-05,
      "loss": 0.0019,
      "step": 96600
    },
    {
      "epoch": 5.152533333333333,
      "grad_norm": 0.08429927378892899,
      "learning_rate": 1.779666666666667e-05,
      "loss": 0.0045,
      "step": 96610
    },
    {
      "epoch": 5.153066666666667,
      "grad_norm": 0.22478729486465454,
      "learning_rate": 1.7793333333333335e-05,
      "loss": 0.0017,
      "step": 96620
    },
    {
      "epoch": 5.1536,
      "grad_norm": 0.14049363136291504,
      "learning_rate": 1.779e-05,
      "loss": 0.0025,
      "step": 96630
    },
    {
      "epoch": 5.154133333333333,
      "grad_norm": 0.2247946858406067,
      "learning_rate": 1.7786666666666667e-05,
      "loss": 0.0027,
      "step": 96640
    },
    {
      "epoch": 5.1546666666666665,
      "grad_norm": 0.25288811326026917,
      "learning_rate": 1.7783333333333336e-05,
      "loss": 0.0022,
      "step": 96650
    },
    {
      "epoch": 5.1552,
      "grad_norm": 0.2810039818286896,
      "learning_rate": 1.7780000000000003e-05,
      "loss": 0.0017,
      "step": 96660
    },
    {
      "epoch": 5.155733333333333,
      "grad_norm": 0.2809908390045166,
      "learning_rate": 1.7776666666666665e-05,
      "loss": 0.0034,
      "step": 96670
    },
    {
      "epoch": 5.156266666666666,
      "grad_norm": 0.3933732509613037,
      "learning_rate": 1.7773333333333335e-05,
      "loss": 0.0027,
      "step": 96680
    },
    {
      "epoch": 5.1568,
      "grad_norm": 0.02809876762330532,
      "learning_rate": 1.777e-05,
      "loss": 0.0034,
      "step": 96690
    },
    {
      "epoch": 5.157333333333334,
      "grad_norm": 0.02809819020330906,
      "learning_rate": 1.7766666666666667e-05,
      "loss": 0.0031,
      "step": 96700
    },
    {
      "epoch": 5.157866666666667,
      "grad_norm": 0.2528873682022095,
      "learning_rate": 1.7763333333333333e-05,
      "loss": 0.003,
      "step": 96710
    },
    {
      "epoch": 5.1584,
      "grad_norm": 0.17467032372951508,
      "learning_rate": 1.7760000000000003e-05,
      "loss": 0.0035,
      "step": 96720
    },
    {
      "epoch": 5.158933333333334,
      "grad_norm": 0.14049066603183746,
      "learning_rate": 1.775666666666667e-05,
      "loss": 0.0028,
      "step": 96730
    },
    {
      "epoch": 5.159466666666667,
      "grad_norm": 0.11239418387413025,
      "learning_rate": 1.775333333333333e-05,
      "loss": 0.0021,
      "step": 96740
    },
    {
      "epoch": 5.16,
      "grad_norm": 0.2247963696718216,
      "learning_rate": 1.775e-05,
      "loss": 0.0027,
      "step": 96750
    },
    {
      "epoch": 5.160533333333333,
      "grad_norm": 0.11240056157112122,
      "learning_rate": 1.7746666666666667e-05,
      "loss": 0.0028,
      "step": 96760
    },
    {
      "epoch": 5.161066666666667,
      "grad_norm": 0.47764530777931213,
      "learning_rate": 1.7743333333333333e-05,
      "loss": 0.0024,
      "step": 96770
    },
    {
      "epoch": 5.1616,
      "grad_norm": 0.22479426860809326,
      "learning_rate": 1.774e-05,
      "loss": 0.0016,
      "step": 96780
    },
    {
      "epoch": 5.162133333333333,
      "grad_norm": 0.028097694739699364,
      "learning_rate": 1.773666666666667e-05,
      "loss": 0.0029,
      "step": 96790
    },
    {
      "epoch": 5.1626666666666665,
      "grad_norm": 0.25289425253868103,
      "learning_rate": 1.7733333333333335e-05,
      "loss": 0.0031,
      "step": 96800
    },
    {
      "epoch": 5.1632,
      "grad_norm": 0.05619772523641586,
      "learning_rate": 1.773e-05,
      "loss": 0.0025,
      "step": 96810
    },
    {
      "epoch": 5.163733333333333,
      "grad_norm": 0.252885639667511,
      "learning_rate": 1.7726666666666667e-05,
      "loss": 0.0029,
      "step": 96820
    },
    {
      "epoch": 5.164266666666666,
      "grad_norm": 0.11238836497068405,
      "learning_rate": 1.7723333333333334e-05,
      "loss": 0.002,
      "step": 96830
    },
    {
      "epoch": 5.1648,
      "grad_norm": 0.16858920454978943,
      "learning_rate": 1.772e-05,
      "loss": 0.0022,
      "step": 96840
    },
    {
      "epoch": 5.165333333333333,
      "grad_norm": 0.25288888812065125,
      "learning_rate": 1.7716666666666666e-05,
      "loss": 0.0021,
      "step": 96850
    },
    {
      "epoch": 5.165866666666667,
      "grad_norm": 0.05619620159268379,
      "learning_rate": 1.7713333333333335e-05,
      "loss": 0.0016,
      "step": 96860
    },
    {
      "epoch": 5.1664,
      "grad_norm": 0.42147526144981384,
      "learning_rate": 1.771e-05,
      "loss": 0.0015,
      "step": 96870
    },
    {
      "epoch": 5.166933333333334,
      "grad_norm": 0.08429939299821854,
      "learning_rate": 1.7706666666666668e-05,
      "loss": 0.0037,
      "step": 96880
    },
    {
      "epoch": 5.167466666666667,
      "grad_norm": 0.22477687895298004,
      "learning_rate": 1.7703333333333334e-05,
      "loss": 0.0021,
      "step": 96890
    },
    {
      "epoch": 5.168,
      "grad_norm": 0.08429228514432907,
      "learning_rate": 1.77e-05,
      "loss": 0.0024,
      "step": 96900
    },
    {
      "epoch": 5.168533333333333,
      "grad_norm": 0.08429128676652908,
      "learning_rate": 1.7696666666666666e-05,
      "loss": 0.0024,
      "step": 96910
    },
    {
      "epoch": 5.169066666666667,
      "grad_norm": 0.05619559437036514,
      "learning_rate": 1.7693333333333336e-05,
      "loss": 0.0017,
      "step": 96920
    },
    {
      "epoch": 5.1696,
      "grad_norm": 0.028097273781895638,
      "learning_rate": 1.7690000000000002e-05,
      "loss": 0.003,
      "step": 96930
    },
    {
      "epoch": 5.170133333333333,
      "grad_norm": 0.14049139618873596,
      "learning_rate": 1.7686666666666668e-05,
      "loss": 0.0022,
      "step": 96940
    },
    {
      "epoch": 5.1706666666666665,
      "grad_norm": 2.721863845422945e-09,
      "learning_rate": 1.7683333333333334e-05,
      "loss": 0.0018,
      "step": 96950
    },
    {
      "epoch": 5.1712,
      "grad_norm": 0.16859449446201324,
      "learning_rate": 1.7680000000000004e-05,
      "loss": 0.0028,
      "step": 96960
    },
    {
      "epoch": 5.171733333333333,
      "grad_norm": 0.08429243415594101,
      "learning_rate": 1.7676666666666666e-05,
      "loss": 0.0038,
      "step": 96970
    },
    {
      "epoch": 5.172266666666666,
      "grad_norm": 0.08429108560085297,
      "learning_rate": 1.7673333333333332e-05,
      "loss": 0.0029,
      "step": 96980
    },
    {
      "epoch": 5.1728,
      "grad_norm": 0.2528766393661499,
      "learning_rate": 1.7670000000000002e-05,
      "loss": 0.0023,
      "step": 96990
    },
    {
      "epoch": 5.173333333333334,
      "grad_norm": 0.1123872697353363,
      "learning_rate": 1.7666666666666668e-05,
      "loss": 0.003,
      "step": 97000
    },
    {
      "epoch": 5.173866666666667,
      "grad_norm": 0.33716925978660583,
      "learning_rate": 1.7663333333333334e-05,
      "loss": 0.003,
      "step": 97010
    },
    {
      "epoch": 5.1744,
      "grad_norm": 0.08429159969091415,
      "learning_rate": 1.766e-05,
      "loss": 0.0031,
      "step": 97020
    },
    {
      "epoch": 5.174933333333334,
      "grad_norm": 0.5057464838027954,
      "learning_rate": 1.765666666666667e-05,
      "loss": 0.0035,
      "step": 97030
    },
    {
      "epoch": 5.175466666666667,
      "grad_norm": 0.39338576793670654,
      "learning_rate": 1.7653333333333333e-05,
      "loss": 0.0031,
      "step": 97040
    },
    {
      "epoch": 5.176,
      "grad_norm": 0.7304739952087402,
      "learning_rate": 1.765e-05,
      "loss": 0.005,
      "step": 97050
    },
    {
      "epoch": 5.176533333333333,
      "grad_norm": 0.2809906005859375,
      "learning_rate": 1.7646666666666668e-05,
      "loss": 0.0018,
      "step": 97060
    },
    {
      "epoch": 5.177066666666667,
      "grad_norm": 0.16858239471912384,
      "learning_rate": 1.7643333333333334e-05,
      "loss": 0.0029,
      "step": 97070
    },
    {
      "epoch": 5.1776,
      "grad_norm": 0.2809658348560333,
      "learning_rate": 1.764e-05,
      "loss": 0.0023,
      "step": 97080
    },
    {
      "epoch": 5.178133333333333,
      "grad_norm": 0.16858232021331787,
      "learning_rate": 1.7636666666666667e-05,
      "loss": 0.0031,
      "step": 97090
    },
    {
      "epoch": 5.1786666666666665,
      "grad_norm": 0.028097448870539665,
      "learning_rate": 1.7633333333333336e-05,
      "loss": 0.0023,
      "step": 97100
    },
    {
      "epoch": 5.1792,
      "grad_norm": 4.7669086455925935e-09,
      "learning_rate": 1.7630000000000002e-05,
      "loss": 0.0028,
      "step": 97110
    },
    {
      "epoch": 5.179733333333333,
      "grad_norm": 1.1529215360184253e-09,
      "learning_rate": 1.7626666666666665e-05,
      "loss": 0.0019,
      "step": 97120
    },
    {
      "epoch": 5.180266666666666,
      "grad_norm": 0.16858619451522827,
      "learning_rate": 1.7623333333333335e-05,
      "loss": 0.0038,
      "step": 97130
    },
    {
      "epoch": 5.1808,
      "grad_norm": 3.399644136428833,
      "learning_rate": 1.762e-05,
      "loss": 0.0019,
      "step": 97140
    },
    {
      "epoch": 5.181333333333333,
      "grad_norm": 0.1966797113418579,
      "learning_rate": 1.7616666666666667e-05,
      "loss": 0.0017,
      "step": 97150
    },
    {
      "epoch": 5.181866666666667,
      "grad_norm": 0.12434975057840347,
      "learning_rate": 1.7613333333333333e-05,
      "loss": 0.0022,
      "step": 97160
    },
    {
      "epoch": 5.1824,
      "grad_norm": 0.14048123359680176,
      "learning_rate": 1.7610000000000002e-05,
      "loss": 0.0031,
      "step": 97170
    },
    {
      "epoch": 5.182933333333334,
      "grad_norm": 0.028096312656998634,
      "learning_rate": 1.760666666666667e-05,
      "loss": 0.0042,
      "step": 97180
    },
    {
      "epoch": 5.183466666666667,
      "grad_norm": 0.42941293120384216,
      "learning_rate": 1.7603333333333335e-05,
      "loss": 0.0022,
      "step": 97190
    },
    {
      "epoch": 5.184,
      "grad_norm": 0.22793644666671753,
      "learning_rate": 1.76e-05,
      "loss": 0.0023,
      "step": 97200
    },
    {
      "epoch": 5.184533333333333,
      "grad_norm": 0.337144136428833,
      "learning_rate": 1.7596666666666667e-05,
      "loss": 0.0019,
      "step": 97210
    },
    {
      "epoch": 5.185066666666667,
      "grad_norm": 0.309074729681015,
      "learning_rate": 1.7593333333333333e-05,
      "loss": 0.0042,
      "step": 97220
    },
    {
      "epoch": 5.1856,
      "grad_norm": 0.11238839477300644,
      "learning_rate": 1.759e-05,
      "loss": 0.0034,
      "step": 97230
    },
    {
      "epoch": 5.186133333333333,
      "grad_norm": 0.1966673880815506,
      "learning_rate": 1.758666666666667e-05,
      "loss": 0.0024,
      "step": 97240
    },
    {
      "epoch": 5.1866666666666665,
      "grad_norm": 0.33717116713523865,
      "learning_rate": 1.7583333333333335e-05,
      "loss": 0.0017,
      "step": 97250
    },
    {
      "epoch": 5.1872,
      "grad_norm": 0.08429323136806488,
      "learning_rate": 1.758e-05,
      "loss": 0.0029,
      "step": 97260
    },
    {
      "epoch": 5.187733333333333,
      "grad_norm": 0.2528461515903473,
      "learning_rate": 1.7576666666666667e-05,
      "loss": 0.0034,
      "step": 97270
    },
    {
      "epoch": 5.188266666666666,
      "grad_norm": 0.22478768229484558,
      "learning_rate": 1.7573333333333333e-05,
      "loss": 0.0047,
      "step": 97280
    },
    {
      "epoch": 5.1888,
      "grad_norm": 0.056190986186265945,
      "learning_rate": 1.757e-05,
      "loss": 0.0027,
      "step": 97290
    },
    {
      "epoch": 5.189333333333333,
      "grad_norm": 3.903521950121558e-09,
      "learning_rate": 1.756666666666667e-05,
      "loss": 0.003,
      "step": 97300
    },
    {
      "epoch": 5.189866666666667,
      "grad_norm": 0.2528775930404663,
      "learning_rate": 1.7563333333333335e-05,
      "loss": 0.0023,
      "step": 97310
    },
    {
      "epoch": 5.1904,
      "grad_norm": 4.724455049398557e-09,
      "learning_rate": 1.756e-05,
      "loss": 0.0031,
      "step": 97320
    },
    {
      "epoch": 5.190933333333334,
      "grad_norm": 0.02809516340494156,
      "learning_rate": 1.7556666666666667e-05,
      "loss": 0.0022,
      "step": 97330
    },
    {
      "epoch": 5.191466666666667,
      "grad_norm": 0.25285059213638306,
      "learning_rate": 1.7553333333333337e-05,
      "loss": 0.0021,
      "step": 97340
    },
    {
      "epoch": 5.192,
      "grad_norm": 0.19666966795921326,
      "learning_rate": 1.755e-05,
      "loss": 0.0027,
      "step": 97350
    },
    {
      "epoch": 5.1925333333333334,
      "grad_norm": 0.05619032680988312,
      "learning_rate": 1.7546666666666666e-05,
      "loss": 0.0028,
      "step": 97360
    },
    {
      "epoch": 5.193066666666667,
      "grad_norm": 0.02809462510049343,
      "learning_rate": 1.7543333333333335e-05,
      "loss": 0.0018,
      "step": 97370
    },
    {
      "epoch": 5.1936,
      "grad_norm": 0.1966744363307953,
      "learning_rate": 1.754e-05,
      "loss": 0.0022,
      "step": 97380
    },
    {
      "epoch": 5.194133333333333,
      "grad_norm": 0.1404763162136078,
      "learning_rate": 1.7536666666666668e-05,
      "loss": 0.0029,
      "step": 97390
    },
    {
      "epoch": 5.1946666666666665,
      "grad_norm": 0.42140400409698486,
      "learning_rate": 1.7533333333333334e-05,
      "loss": 0.0044,
      "step": 97400
    },
    {
      "epoch": 5.1952,
      "grad_norm": 4.151801125118482e-09,
      "learning_rate": 1.7530000000000003e-05,
      "loss": 0.0025,
      "step": 97410
    },
    {
      "epoch": 5.195733333333333,
      "grad_norm": 0.22476881742477417,
      "learning_rate": 1.7526666666666666e-05,
      "loss": 0.0015,
      "step": 97420
    },
    {
      "epoch": 5.196266666666666,
      "grad_norm": 0.42142531275749207,
      "learning_rate": 1.7523333333333332e-05,
      "loss": 0.0036,
      "step": 97430
    },
    {
      "epoch": 5.1968,
      "grad_norm": 0.3090370297431946,
      "learning_rate": 1.752e-05,
      "loss": 0.0025,
      "step": 97440
    },
    {
      "epoch": 5.197333333333333,
      "grad_norm": 0.14047445356845856,
      "learning_rate": 1.7516666666666668e-05,
      "loss": 0.0028,
      "step": 97450
    },
    {
      "epoch": 5.197866666666667,
      "grad_norm": 0.02809477038681507,
      "learning_rate": 1.7513333333333334e-05,
      "loss": 0.0027,
      "step": 97460
    },
    {
      "epoch": 5.1984,
      "grad_norm": 0.47760170698165894,
      "learning_rate": 1.751e-05,
      "loss": 0.0021,
      "step": 97470
    },
    {
      "epoch": 5.198933333333334,
      "grad_norm": 0.19666101038455963,
      "learning_rate": 1.750666666666667e-05,
      "loss": 0.0015,
      "step": 97480
    },
    {
      "epoch": 5.199466666666667,
      "grad_norm": 0.056190039962530136,
      "learning_rate": 1.7503333333333336e-05,
      "loss": 0.0019,
      "step": 97490
    },
    {
      "epoch": 5.2,
      "grad_norm": 0.19667159020900726,
      "learning_rate": 1.75e-05,
      "loss": 0.0027,
      "step": 97500
    },
    {
      "epoch": 5.2005333333333335,
      "grad_norm": 0.056192558258771896,
      "learning_rate": 1.7496666666666668e-05,
      "loss": 0.0023,
      "step": 97510
    },
    {
      "epoch": 5.201066666666667,
      "grad_norm": 0.3090311884880066,
      "learning_rate": 1.7493333333333334e-05,
      "loss": 0.0019,
      "step": 97520
    },
    {
      "epoch": 5.2016,
      "grad_norm": 0.14048370718955994,
      "learning_rate": 1.749e-05,
      "loss": 0.0032,
      "step": 97530
    },
    {
      "epoch": 5.202133333333333,
      "grad_norm": 0.39332830905914307,
      "learning_rate": 1.7486666666666666e-05,
      "loss": 0.0039,
      "step": 97540
    },
    {
      "epoch": 5.2026666666666666,
      "grad_norm": 0.9014968276023865,
      "learning_rate": 1.7483333333333336e-05,
      "loss": 0.0024,
      "step": 97550
    },
    {
      "epoch": 5.2032,
      "grad_norm": 0.14047567546367645,
      "learning_rate": 1.7480000000000002e-05,
      "loss": 0.002,
      "step": 97560
    },
    {
      "epoch": 5.203733333333333,
      "grad_norm": 0.30903157591819763,
      "learning_rate": 1.7476666666666665e-05,
      "loss": 0.002,
      "step": 97570
    },
    {
      "epoch": 5.204266666666666,
      "grad_norm": 0.2528616189956665,
      "learning_rate": 1.7473333333333334e-05,
      "loss": 0.0036,
      "step": 97580
    },
    {
      "epoch": 5.2048,
      "grad_norm": 0.25285273790359497,
      "learning_rate": 1.747e-05,
      "loss": 0.002,
      "step": 97590
    },
    {
      "epoch": 5.205333333333333,
      "grad_norm": 0.4213927388191223,
      "learning_rate": 1.7466666666666667e-05,
      "loss": 0.0024,
      "step": 97600
    },
    {
      "epoch": 5.205866666666667,
      "grad_norm": 0.0842834934592247,
      "learning_rate": 1.7463333333333333e-05,
      "loss": 0.0036,
      "step": 97610
    },
    {
      "epoch": 5.2064,
      "grad_norm": 0.25284436345100403,
      "learning_rate": 1.7460000000000002e-05,
      "loss": 0.0027,
      "step": 97620
    },
    {
      "epoch": 5.206933333333334,
      "grad_norm": 0.2809382379055023,
      "learning_rate": 1.745666666666667e-05,
      "loss": 0.0015,
      "step": 97630
    },
    {
      "epoch": 5.207466666666667,
      "grad_norm": 0.19666101038455963,
      "learning_rate": 1.7453333333333335e-05,
      "loss": 0.0023,
      "step": 97640
    },
    {
      "epoch": 5.208,
      "grad_norm": 0.5057192444801331,
      "learning_rate": 1.745e-05,
      "loss": 0.0022,
      "step": 97650
    },
    {
      "epoch": 5.2085333333333335,
      "grad_norm": 0.08428109437227249,
      "learning_rate": 1.7446666666666667e-05,
      "loss": 0.0024,
      "step": 97660
    },
    {
      "epoch": 5.209066666666667,
      "grad_norm": 2.6176316669790367e-09,
      "learning_rate": 1.7443333333333333e-05,
      "loss": 0.0032,
      "step": 97670
    },
    {
      "epoch": 5.2096,
      "grad_norm": 0.028095055371522903,
      "learning_rate": 1.7440000000000002e-05,
      "loss": 0.0028,
      "step": 97680
    },
    {
      "epoch": 5.210133333333333,
      "grad_norm": 0.11237648129463196,
      "learning_rate": 1.743666666666667e-05,
      "loss": 0.0018,
      "step": 97690
    },
    {
      "epoch": 5.210666666666667,
      "grad_norm": 0.16856077313423157,
      "learning_rate": 1.7433333333333335e-05,
      "loss": 0.0033,
      "step": 97700
    },
    {
      "epoch": 5.2112,
      "grad_norm": 0.2528584897518158,
      "learning_rate": 1.743e-05,
      "loss": 0.0043,
      "step": 97710
    },
    {
      "epoch": 5.211733333333333,
      "grad_norm": 0.14047668874263763,
      "learning_rate": 1.7426666666666667e-05,
      "loss": 0.0028,
      "step": 97720
    },
    {
      "epoch": 5.212266666666666,
      "grad_norm": 0.0842830091714859,
      "learning_rate": 1.7423333333333333e-05,
      "loss": 0.0025,
      "step": 97730
    },
    {
      "epoch": 5.2128,
      "grad_norm": 0.3090194761753082,
      "learning_rate": 1.742e-05,
      "loss": 0.0037,
      "step": 97740
    },
    {
      "epoch": 5.213333333333333,
      "grad_norm": 0.08428460359573364,
      "learning_rate": 1.741666666666667e-05,
      "loss": 0.0026,
      "step": 97750
    },
    {
      "epoch": 5.213866666666667,
      "grad_norm": 0.08428438752889633,
      "learning_rate": 1.7413333333333335e-05,
      "loss": 0.0021,
      "step": 97760
    },
    {
      "epoch": 5.2144,
      "grad_norm": 0.08428343385457993,
      "learning_rate": 1.741e-05,
      "loss": 0.0028,
      "step": 97770
    },
    {
      "epoch": 5.214933333333334,
      "grad_norm": 0.08427879959344864,
      "learning_rate": 1.7406666666666667e-05,
      "loss": 0.0032,
      "step": 97780
    },
    {
      "epoch": 5.215466666666667,
      "grad_norm": 0.08428023010492325,
      "learning_rate": 1.7403333333333337e-05,
      "loss": 0.0039,
      "step": 97790
    },
    {
      "epoch": 5.216,
      "grad_norm": 0.16855555772781372,
      "learning_rate": 1.74e-05,
      "loss": 0.0023,
      "step": 97800
    },
    {
      "epoch": 5.2165333333333335,
      "grad_norm": 0.0561886765062809,
      "learning_rate": 1.7396666666666666e-05,
      "loss": 0.0028,
      "step": 97810
    },
    {
      "epoch": 5.217066666666667,
      "grad_norm": 0.25285494327545166,
      "learning_rate": 1.7393333333333335e-05,
      "loss": 0.0029,
      "step": 97820
    },
    {
      "epoch": 5.2176,
      "grad_norm": 0.16855771839618683,
      "learning_rate": 1.739e-05,
      "loss": 0.0036,
      "step": 97830
    },
    {
      "epoch": 5.218133333333333,
      "grad_norm": 0.0561857633292675,
      "learning_rate": 1.7386666666666667e-05,
      "loss": 0.0027,
      "step": 97840
    },
    {
      "epoch": 5.218666666666667,
      "grad_norm": 0.39330965280532837,
      "learning_rate": 1.7383333333333333e-05,
      "loss": 0.0026,
      "step": 97850
    },
    {
      "epoch": 5.2192,
      "grad_norm": 0.2809196412563324,
      "learning_rate": 1.7380000000000003e-05,
      "loss": 0.0031,
      "step": 97860
    },
    {
      "epoch": 5.219733333333333,
      "grad_norm": 0.11237984150648117,
      "learning_rate": 1.737666666666667e-05,
      "loss": 0.0031,
      "step": 97870
    },
    {
      "epoch": 5.220266666666666,
      "grad_norm": 0.084281325340271,
      "learning_rate": 1.7373333333333332e-05,
      "loss": 0.0021,
      "step": 97880
    },
    {
      "epoch": 5.2208,
      "grad_norm": 0.08427887409925461,
      "learning_rate": 1.737e-05,
      "loss": 0.0024,
      "step": 97890
    },
    {
      "epoch": 5.221333333333333,
      "grad_norm": 0.4776172339916229,
      "learning_rate": 1.7366666666666668e-05,
      "loss": 0.0035,
      "step": 97900
    },
    {
      "epoch": 5.221866666666667,
      "grad_norm": 0.4494524896144867,
      "learning_rate": 1.7363333333333334e-05,
      "loss": 0.0026,
      "step": 97910
    },
    {
      "epoch": 5.2224,
      "grad_norm": 0.5337944626808167,
      "learning_rate": 1.736e-05,
      "loss": 0.0022,
      "step": 97920
    },
    {
      "epoch": 5.222933333333334,
      "grad_norm": 0.25282564759254456,
      "learning_rate": 1.735666666666667e-05,
      "loss": 0.002,
      "step": 97930
    },
    {
      "epoch": 5.223466666666667,
      "grad_norm": 0.2809273600578308,
      "learning_rate": 1.7353333333333335e-05,
      "loss": 0.0037,
      "step": 97940
    },
    {
      "epoch": 5.224,
      "grad_norm": 0.1123729795217514,
      "learning_rate": 1.7349999999999998e-05,
      "loss": 0.0034,
      "step": 97950
    },
    {
      "epoch": 5.2245333333333335,
      "grad_norm": 0.05618486553430557,
      "learning_rate": 1.7346666666666668e-05,
      "loss": 0.0025,
      "step": 97960
    },
    {
      "epoch": 5.225066666666667,
      "grad_norm": 0.30901092290878296,
      "learning_rate": 1.7343333333333334e-05,
      "loss": 0.0026,
      "step": 97970
    },
    {
      "epoch": 5.2256,
      "grad_norm": 2.4100053064302074e-09,
      "learning_rate": 1.734e-05,
      "loss": 0.0025,
      "step": 97980
    },
    {
      "epoch": 5.226133333333333,
      "grad_norm": 0.22474241256713867,
      "learning_rate": 1.7336666666666666e-05,
      "loss": 0.0019,
      "step": 97990
    },
    {
      "epoch": 5.226666666666667,
      "grad_norm": 0.05618244409561157,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 0.0022,
      "step": 98000
    },
    {
      "epoch": 5.2272,
      "grad_norm": 0.08427491784095764,
      "learning_rate": 1.7330000000000002e-05,
      "loss": 0.0039,
      "step": 98010
    },
    {
      "epoch": 5.227733333333333,
      "grad_norm": 0.08427368849515915,
      "learning_rate": 1.7326666666666668e-05,
      "loss": 0.0024,
      "step": 98020
    },
    {
      "epoch": 5.228266666666666,
      "grad_norm": 0.056185562163591385,
      "learning_rate": 1.7323333333333334e-05,
      "loss": 0.0033,
      "step": 98030
    },
    {
      "epoch": 5.2288,
      "grad_norm": 0.1966487616300583,
      "learning_rate": 1.732e-05,
      "loss": 0.0016,
      "step": 98040
    },
    {
      "epoch": 5.229333333333333,
      "grad_norm": 0.30900073051452637,
      "learning_rate": 1.7316666666666666e-05,
      "loss": 0.0031,
      "step": 98050
    },
    {
      "epoch": 5.229866666666666,
      "grad_norm": 0.056186117231845856,
      "learning_rate": 1.7313333333333336e-05,
      "loss": 0.0019,
      "step": 98060
    },
    {
      "epoch": 5.2304,
      "grad_norm": 0.33710095286369324,
      "learning_rate": 1.7310000000000002e-05,
      "loss": 0.0026,
      "step": 98070
    },
    {
      "epoch": 5.230933333333334,
      "grad_norm": 0.08427482843399048,
      "learning_rate": 1.7306666666666668e-05,
      "loss": 0.0024,
      "step": 98080
    },
    {
      "epoch": 5.231466666666667,
      "grad_norm": 0.028090951964259148,
      "learning_rate": 1.7303333333333334e-05,
      "loss": 0.0026,
      "step": 98090
    },
    {
      "epoch": 5.232,
      "grad_norm": 0.05618259310722351,
      "learning_rate": 1.73e-05,
      "loss": 0.0027,
      "step": 98100
    },
    {
      "epoch": 5.2325333333333335,
      "grad_norm": 0.19663353264331818,
      "learning_rate": 1.7296666666666667e-05,
      "loss": 0.0031,
      "step": 98110
    },
    {
      "epoch": 5.233066666666667,
      "grad_norm": 0.4213750958442688,
      "learning_rate": 1.7293333333333333e-05,
      "loss": 0.0028,
      "step": 98120
    },
    {
      "epoch": 5.2336,
      "grad_norm": 0.22473475337028503,
      "learning_rate": 1.7290000000000002e-05,
      "loss": 0.0026,
      "step": 98130
    },
    {
      "epoch": 5.234133333333333,
      "grad_norm": 0.14045315980911255,
      "learning_rate": 1.7286666666666668e-05,
      "loss": 0.0031,
      "step": 98140
    },
    {
      "epoch": 5.234666666666667,
      "grad_norm": 0.08427376300096512,
      "learning_rate": 1.7283333333333334e-05,
      "loss": 0.0033,
      "step": 98150
    },
    {
      "epoch": 5.2352,
      "grad_norm": 0.22472503781318665,
      "learning_rate": 1.728e-05,
      "loss": 0.0022,
      "step": 98160
    },
    {
      "epoch": 5.235733333333333,
      "grad_norm": 0.05618147552013397,
      "learning_rate": 1.727666666666667e-05,
      "loss": 0.0031,
      "step": 98170
    },
    {
      "epoch": 5.236266666666666,
      "grad_norm": 0.14045800268650055,
      "learning_rate": 1.7273333333333333e-05,
      "loss": 0.003,
      "step": 98180
    },
    {
      "epoch": 5.2368,
      "grad_norm": 0.2528168559074402,
      "learning_rate": 1.727e-05,
      "loss": 0.003,
      "step": 98190
    },
    {
      "epoch": 5.237333333333333,
      "grad_norm": 0.14045579731464386,
      "learning_rate": 1.726666666666667e-05,
      "loss": 0.0026,
      "step": 98200
    },
    {
      "epoch": 5.237866666666667,
      "grad_norm": 0.028090978041291237,
      "learning_rate": 1.7263333333333335e-05,
      "loss": 0.0039,
      "step": 98210
    },
    {
      "epoch": 5.2384,
      "grad_norm": 3.345222543416071e-09,
      "learning_rate": 1.726e-05,
      "loss": 0.0019,
      "step": 98220
    },
    {
      "epoch": 5.238933333333334,
      "grad_norm": 0.05618428438901901,
      "learning_rate": 1.7256666666666667e-05,
      "loss": 0.0032,
      "step": 98230
    },
    {
      "epoch": 5.239466666666667,
      "grad_norm": 0.11236576735973358,
      "learning_rate": 1.7253333333333336e-05,
      "loss": 0.0023,
      "step": 98240
    },
    {
      "epoch": 5.24,
      "grad_norm": 0.2809062898159027,
      "learning_rate": 1.725e-05,
      "loss": 0.0031,
      "step": 98250
    },
    {
      "epoch": 5.2405333333333335,
      "grad_norm": 0.0842725932598114,
      "learning_rate": 1.7246666666666665e-05,
      "loss": 0.0041,
      "step": 98260
    },
    {
      "epoch": 5.241066666666667,
      "grad_norm": 0.028090588748455048,
      "learning_rate": 1.7243333333333335e-05,
      "loss": 0.0032,
      "step": 98270
    },
    {
      "epoch": 5.2416,
      "grad_norm": 0.22472809255123138,
      "learning_rate": 1.724e-05,
      "loss": 0.0023,
      "step": 98280
    },
    {
      "epoch": 5.242133333333333,
      "grad_norm": 0.11236727982759476,
      "learning_rate": 1.7236666666666667e-05,
      "loss": 0.0028,
      "step": 98290
    },
    {
      "epoch": 5.242666666666667,
      "grad_norm": 0.02809145674109459,
      "learning_rate": 1.7233333333333333e-05,
      "loss": 0.0027,
      "step": 98300
    },
    {
      "epoch": 5.2432,
      "grad_norm": 0.33708032965660095,
      "learning_rate": 1.7230000000000003e-05,
      "loss": 0.0027,
      "step": 98310
    },
    {
      "epoch": 5.243733333333333,
      "grad_norm": 0.4213868975639343,
      "learning_rate": 1.722666666666667e-05,
      "loss": 0.0038,
      "step": 98320
    },
    {
      "epoch": 5.244266666666666,
      "grad_norm": 0.2247219681739807,
      "learning_rate": 1.722333333333333e-05,
      "loss": 0.0022,
      "step": 98330
    },
    {
      "epoch": 5.2448,
      "grad_norm": 0.11236267536878586,
      "learning_rate": 1.722e-05,
      "loss": 0.0035,
      "step": 98340
    },
    {
      "epoch": 5.245333333333333,
      "grad_norm": 0.112358957529068,
      "learning_rate": 1.7216666666666667e-05,
      "loss": 0.0022,
      "step": 98350
    },
    {
      "epoch": 5.245866666666666,
      "grad_norm": 1.574259278491752e-09,
      "learning_rate": 1.7213333333333333e-05,
      "loss": 0.0024,
      "step": 98360
    },
    {
      "epoch": 5.2464,
      "grad_norm": 0.16854439675807953,
      "learning_rate": 1.721e-05,
      "loss": 0.0032,
      "step": 98370
    },
    {
      "epoch": 5.246933333333334,
      "grad_norm": 0.05618154630064964,
      "learning_rate": 1.720666666666667e-05,
      "loss": 0.0035,
      "step": 98380
    },
    {
      "epoch": 5.247466666666667,
      "grad_norm": 0.16855211555957794,
      "learning_rate": 1.7203333333333335e-05,
      "loss": 0.0037,
      "step": 98390
    },
    {
      "epoch": 5.248,
      "grad_norm": 0.28091511130332947,
      "learning_rate": 1.7199999999999998e-05,
      "loss": 0.0039,
      "step": 98400
    },
    {
      "epoch": 5.2485333333333335,
      "grad_norm": 0.39324548840522766,
      "learning_rate": 1.7196666666666667e-05,
      "loss": 0.0028,
      "step": 98410
    },
    {
      "epoch": 5.249066666666667,
      "grad_norm": 0.14045575261116028,
      "learning_rate": 1.7193333333333334e-05,
      "loss": 0.0023,
      "step": 98420
    },
    {
      "epoch": 5.2496,
      "grad_norm": 0.22473649680614471,
      "learning_rate": 1.719e-05,
      "loss": 0.0031,
      "step": 98430
    },
    {
      "epoch": 5.250133333333333,
      "grad_norm": 0.11236071586608887,
      "learning_rate": 1.718666666666667e-05,
      "loss": 0.0022,
      "step": 98440
    },
    {
      "epoch": 5.250666666666667,
      "grad_norm": 0.168534055352211,
      "learning_rate": 1.7183333333333335e-05,
      "loss": 0.0018,
      "step": 98450
    },
    {
      "epoch": 5.2512,
      "grad_norm": 0.19664344191551208,
      "learning_rate": 1.718e-05,
      "loss": 0.0034,
      "step": 98460
    },
    {
      "epoch": 5.251733333333333,
      "grad_norm": 0.028089916333556175,
      "learning_rate": 1.7176666666666668e-05,
      "loss": 0.0025,
      "step": 98470
    },
    {
      "epoch": 5.252266666666666,
      "grad_norm": 5.335440422804538e-10,
      "learning_rate": 1.7173333333333334e-05,
      "loss": 0.0023,
      "step": 98480
    },
    {
      "epoch": 5.2528,
      "grad_norm": 0.13398565351963043,
      "learning_rate": 1.717e-05,
      "loss": 0.0039,
      "step": 98490
    },
    {
      "epoch": 5.253333333333333,
      "grad_norm": 0.08857408910989761,
      "learning_rate": 1.7166666666666666e-05,
      "loss": 0.004,
      "step": 98500
    },
    {
      "epoch": 5.253866666666667,
      "grad_norm": 0.11235777288675308,
      "learning_rate": 1.7163333333333336e-05,
      "loss": 0.0035,
      "step": 98510
    },
    {
      "epoch": 5.2544,
      "grad_norm": 0.112363800406456,
      "learning_rate": 1.7160000000000002e-05,
      "loss": 0.0024,
      "step": 98520
    },
    {
      "epoch": 5.254933333333334,
      "grad_norm": 0.2809044122695923,
      "learning_rate": 1.7156666666666668e-05,
      "loss": 0.0021,
      "step": 98530
    },
    {
      "epoch": 5.255466666666667,
      "grad_norm": 0.3651503324508667,
      "learning_rate": 1.7153333333333334e-05,
      "loss": 0.0026,
      "step": 98540
    },
    {
      "epoch": 5.256,
      "grad_norm": 0.028090815991163254,
      "learning_rate": 1.7150000000000004e-05,
      "loss": 0.0042,
      "step": 98550
    },
    {
      "epoch": 5.2565333333333335,
      "grad_norm": 0.28090938925743103,
      "learning_rate": 1.7146666666666666e-05,
      "loss": 0.0023,
      "step": 98560
    },
    {
      "epoch": 5.257066666666667,
      "grad_norm": 0.19662359356880188,
      "learning_rate": 1.7143333333333332e-05,
      "loss": 0.0022,
      "step": 98570
    },
    {
      "epoch": 5.2576,
      "grad_norm": 0.1404455602169037,
      "learning_rate": 1.7140000000000002e-05,
      "loss": 0.0025,
      "step": 98580
    },
    {
      "epoch": 5.258133333333333,
      "grad_norm": 0.1404450386762619,
      "learning_rate": 1.7136666666666668e-05,
      "loss": 0.0029,
      "step": 98590
    },
    {
      "epoch": 5.258666666666667,
      "grad_norm": 0.056179456412792206,
      "learning_rate": 1.7133333333333334e-05,
      "loss": 0.0022,
      "step": 98600
    },
    {
      "epoch": 5.2592,
      "grad_norm": 0.28088733553886414,
      "learning_rate": 1.713e-05,
      "loss": 0.0021,
      "step": 98610
    },
    {
      "epoch": 5.259733333333333,
      "grad_norm": 0.28089314699172974,
      "learning_rate": 1.712666666666667e-05,
      "loss": 0.0045,
      "step": 98620
    },
    {
      "epoch": 5.260266666666666,
      "grad_norm": 0.16853569447994232,
      "learning_rate": 1.7123333333333333e-05,
      "loss": 0.0035,
      "step": 98630
    },
    {
      "epoch": 5.2608,
      "grad_norm": 0.22471068799495697,
      "learning_rate": 1.712e-05,
      "loss": 0.0018,
      "step": 98640
    },
    {
      "epoch": 5.261333333333333,
      "grad_norm": 0.11235743761062622,
      "learning_rate": 1.7116666666666668e-05,
      "loss": 0.0032,
      "step": 98650
    },
    {
      "epoch": 5.261866666666666,
      "grad_norm": 0.08426667749881744,
      "learning_rate": 1.7113333333333334e-05,
      "loss": 0.0029,
      "step": 98660
    },
    {
      "epoch": 5.2624,
      "grad_norm": 0.05617699772119522,
      "learning_rate": 1.711e-05,
      "loss": 0.0029,
      "step": 98670
    },
    {
      "epoch": 5.262933333333334,
      "grad_norm": 0.08426710963249207,
      "learning_rate": 1.7106666666666667e-05,
      "loss": 0.0024,
      "step": 98680
    },
    {
      "epoch": 5.263466666666667,
      "grad_norm": 0.25279662013053894,
      "learning_rate": 1.7103333333333336e-05,
      "loss": 0.0041,
      "step": 98690
    },
    {
      "epoch": 5.264,
      "grad_norm": 0.1404447704553604,
      "learning_rate": 1.7100000000000002e-05,
      "loss": 0.0035,
      "step": 98700
    },
    {
      "epoch": 5.2645333333333335,
      "grad_norm": 0.05617794767022133,
      "learning_rate": 1.7096666666666665e-05,
      "loss": 0.0031,
      "step": 98710
    },
    {
      "epoch": 5.265066666666667,
      "grad_norm": 0.11235775053501129,
      "learning_rate": 1.7093333333333335e-05,
      "loss": 0.003,
      "step": 98720
    },
    {
      "epoch": 5.2656,
      "grad_norm": 0.47750258445739746,
      "learning_rate": 1.709e-05,
      "loss": 0.0027,
      "step": 98730
    },
    {
      "epoch": 5.266133333333333,
      "grad_norm": 0.05617998167872429,
      "learning_rate": 1.7086666666666667e-05,
      "loss": 0.0029,
      "step": 98740
    },
    {
      "epoch": 5.266666666666667,
      "grad_norm": 0.16853705048561096,
      "learning_rate": 1.7083333333333333e-05,
      "loss": 0.0031,
      "step": 98750
    },
    {
      "epoch": 5.2672,
      "grad_norm": 0.028088966384530067,
      "learning_rate": 1.7080000000000002e-05,
      "loss": 0.0027,
      "step": 98760
    },
    {
      "epoch": 5.267733333333333,
      "grad_norm": 6.0931011347520325e-09,
      "learning_rate": 1.707666666666667e-05,
      "loss": 0.0026,
      "step": 98770
    },
    {
      "epoch": 5.268266666666666,
      "grad_norm": 0.2247055172920227,
      "learning_rate": 1.707333333333333e-05,
      "loss": 0.002,
      "step": 98780
    },
    {
      "epoch": 5.2688,
      "grad_norm": 0.08427049219608307,
      "learning_rate": 1.707e-05,
      "loss": 0.0033,
      "step": 98790
    },
    {
      "epoch": 5.269333333333333,
      "grad_norm": 0.30898749828338623,
      "learning_rate": 1.7066666666666667e-05,
      "loss": 0.003,
      "step": 98800
    },
    {
      "epoch": 5.269866666666666,
      "grad_norm": 0.25279122591018677,
      "learning_rate": 1.7063333333333333e-05,
      "loss": 0.0012,
      "step": 98810
    },
    {
      "epoch": 5.2704,
      "grad_norm": 2.945626409456281e-09,
      "learning_rate": 1.706e-05,
      "loss": 0.0038,
      "step": 98820
    },
    {
      "epoch": 5.270933333333334,
      "grad_norm": 0.19662614166736603,
      "learning_rate": 1.705666666666667e-05,
      "loss": 0.0034,
      "step": 98830
    },
    {
      "epoch": 5.271466666666667,
      "grad_norm": 0.3089803159236908,
      "learning_rate": 1.7053333333333335e-05,
      "loss": 0.0023,
      "step": 98840
    },
    {
      "epoch": 5.272,
      "grad_norm": 0.3932162821292877,
      "learning_rate": 1.705e-05,
      "loss": 0.0027,
      "step": 98850
    },
    {
      "epoch": 5.2725333333333335,
      "grad_norm": 0.14044463634490967,
      "learning_rate": 1.7046666666666667e-05,
      "loss": 0.0029,
      "step": 98860
    },
    {
      "epoch": 5.273066666666667,
      "grad_norm": 0.02808784693479538,
      "learning_rate": 1.7043333333333333e-05,
      "loss": 0.003,
      "step": 98870
    },
    {
      "epoch": 5.2736,
      "grad_norm": 0.3370560109615326,
      "learning_rate": 1.704e-05,
      "loss": 0.0036,
      "step": 98880
    },
    {
      "epoch": 5.274133333333333,
      "grad_norm": 0.22470371425151825,
      "learning_rate": 1.703666666666667e-05,
      "loss": 0.003,
      "step": 98890
    },
    {
      "epoch": 5.274666666666667,
      "grad_norm": 0.22470605373382568,
      "learning_rate": 1.7033333333333335e-05,
      "loss": 0.0027,
      "step": 98900
    },
    {
      "epoch": 5.2752,
      "grad_norm": 0.3370460867881775,
      "learning_rate": 1.703e-05,
      "loss": 0.0025,
      "step": 98910
    },
    {
      "epoch": 5.275733333333333,
      "grad_norm": 0.028088752180337906,
      "learning_rate": 1.7026666666666667e-05,
      "loss": 0.0023,
      "step": 98920
    },
    {
      "epoch": 5.276266666666666,
      "grad_norm": 0.056177809834480286,
      "learning_rate": 1.7023333333333334e-05,
      "loss": 0.0034,
      "step": 98930
    },
    {
      "epoch": 5.2768,
      "grad_norm": 0.02808738686144352,
      "learning_rate": 1.702e-05,
      "loss": 0.0032,
      "step": 98940
    },
    {
      "epoch": 5.277333333333333,
      "grad_norm": 0.05617856979370117,
      "learning_rate": 1.7016666666666666e-05,
      "loss": 0.002,
      "step": 98950
    },
    {
      "epoch": 5.277866666666666,
      "grad_norm": 0.11235109716653824,
      "learning_rate": 1.7013333333333335e-05,
      "loss": 0.0025,
      "step": 98960
    },
    {
      "epoch": 5.2783999999999995,
      "grad_norm": 0.22470155358314514,
      "learning_rate": 1.701e-05,
      "loss": 0.002,
      "step": 98970
    },
    {
      "epoch": 5.278933333333334,
      "grad_norm": 0.44944095611572266,
      "learning_rate": 1.7006666666666668e-05,
      "loss": 0.003,
      "step": 98980
    },
    {
      "epoch": 5.279466666666667,
      "grad_norm": 0.16853632032871246,
      "learning_rate": 1.7003333333333334e-05,
      "loss": 0.0033,
      "step": 98990
    },
    {
      "epoch": 5.28,
      "grad_norm": 0.08426249027252197,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.0039,
      "step": 99000
    },
    {
      "epoch": 5.2805333333333335,
      "grad_norm": 0.1404433399438858,
      "learning_rate": 1.6996666666666666e-05,
      "loss": 0.0022,
      "step": 99010
    },
    {
      "epoch": 5.281066666666667,
      "grad_norm": 0.08426493406295776,
      "learning_rate": 1.6993333333333332e-05,
      "loss": 0.0027,
      "step": 99020
    },
    {
      "epoch": 5.2816,
      "grad_norm": 0.19661520421504974,
      "learning_rate": 1.699e-05,
      "loss": 0.0026,
      "step": 99030
    },
    {
      "epoch": 5.282133333333333,
      "grad_norm": 0.1685367077589035,
      "learning_rate": 1.6986666666666668e-05,
      "loss": 0.0031,
      "step": 99040
    },
    {
      "epoch": 5.282666666666667,
      "grad_norm": 0.16853421926498413,
      "learning_rate": 1.6983333333333334e-05,
      "loss": 0.0025,
      "step": 99050
    },
    {
      "epoch": 5.2832,
      "grad_norm": 0.16853587329387665,
      "learning_rate": 1.698e-05,
      "loss": 0.0025,
      "step": 99060
    },
    {
      "epoch": 5.283733333333333,
      "grad_norm": 0.22469428181648254,
      "learning_rate": 1.697666666666667e-05,
      "loss": 0.0027,
      "step": 99070
    },
    {
      "epoch": 5.2842666666666664,
      "grad_norm": 0.1123528927564621,
      "learning_rate": 1.6973333333333336e-05,
      "loss": 0.0022,
      "step": 99080
    },
    {
      "epoch": 5.2848,
      "grad_norm": 0.19662635028362274,
      "learning_rate": 1.697e-05,
      "loss": 0.0027,
      "step": 99090
    },
    {
      "epoch": 5.285333333333333,
      "grad_norm": 0.11235053092241287,
      "learning_rate": 1.6966666666666668e-05,
      "loss": 0.0025,
      "step": 99100
    },
    {
      "epoch": 5.285866666666666,
      "grad_norm": 0.056173667311668396,
      "learning_rate": 1.6963333333333334e-05,
      "loss": 0.0042,
      "step": 99110
    },
    {
      "epoch": 5.2864,
      "grad_norm": 0.02808709815144539,
      "learning_rate": 1.696e-05,
      "loss": 0.0039,
      "step": 99120
    },
    {
      "epoch": 5.286933333333334,
      "grad_norm": 0.1404331773519516,
      "learning_rate": 1.6956666666666666e-05,
      "loss": 0.0022,
      "step": 99130
    },
    {
      "epoch": 5.287466666666667,
      "grad_norm": 0.14043359458446503,
      "learning_rate": 1.6953333333333336e-05,
      "loss": 0.0025,
      "step": 99140
    },
    {
      "epoch": 5.288,
      "grad_norm": 0.3370581865310669,
      "learning_rate": 1.6950000000000002e-05,
      "loss": 0.0036,
      "step": 99150
    },
    {
      "epoch": 5.2885333333333335,
      "grad_norm": 0.2808741629123688,
      "learning_rate": 1.6946666666666665e-05,
      "loss": 0.0021,
      "step": 99160
    },
    {
      "epoch": 5.289066666666667,
      "grad_norm": 0.19660963118076324,
      "learning_rate": 1.6943333333333334e-05,
      "loss": 0.0039,
      "step": 99170
    },
    {
      "epoch": 5.2896,
      "grad_norm": 0.056177571415901184,
      "learning_rate": 1.694e-05,
      "loss": 0.0016,
      "step": 99180
    },
    {
      "epoch": 5.290133333333333,
      "grad_norm": 0.25279733538627625,
      "learning_rate": 1.6936666666666667e-05,
      "loss": 0.0025,
      "step": 99190
    },
    {
      "epoch": 5.290666666666667,
      "grad_norm": 0.33703190088272095,
      "learning_rate": 1.6933333333333333e-05,
      "loss": 0.0037,
      "step": 99200
    },
    {
      "epoch": 5.2912,
      "grad_norm": 0.14043492078781128,
      "learning_rate": 1.6930000000000002e-05,
      "loss": 0.0029,
      "step": 99210
    },
    {
      "epoch": 5.291733333333333,
      "grad_norm": 0.16852378845214844,
      "learning_rate": 1.692666666666667e-05,
      "loss": 0.0028,
      "step": 99220
    },
    {
      "epoch": 5.2922666666666665,
      "grad_norm": 0.08426354080438614,
      "learning_rate": 1.6923333333333334e-05,
      "loss": 0.0022,
      "step": 99230
    },
    {
      "epoch": 5.2928,
      "grad_norm": 1.4703425144091398e-09,
      "learning_rate": 1.692e-05,
      "loss": 0.0032,
      "step": 99240
    },
    {
      "epoch": 5.293333333333333,
      "grad_norm": 0.19661067426204681,
      "learning_rate": 1.6916666666666667e-05,
      "loss": 0.0019,
      "step": 99250
    },
    {
      "epoch": 5.293866666666666,
      "grad_norm": 0.0842670202255249,
      "learning_rate": 1.6913333333333333e-05,
      "loss": 0.0027,
      "step": 99260
    },
    {
      "epoch": 5.2943999999999996,
      "grad_norm": 1.094172716140747,
      "learning_rate": 1.6910000000000002e-05,
      "loss": 0.0017,
      "step": 99270
    },
    {
      "epoch": 5.294933333333334,
      "grad_norm": 0.2808602750301361,
      "learning_rate": 1.690666666666667e-05,
      "loss": 0.0033,
      "step": 99280
    },
    {
      "epoch": 5.295466666666667,
      "grad_norm": 0.028087161481380463,
      "learning_rate": 1.6903333333333335e-05,
      "loss": 0.0019,
      "step": 99290
    },
    {
      "epoch": 5.296,
      "grad_norm": 0.3370380997657776,
      "learning_rate": 1.69e-05,
      "loss": 0.0025,
      "step": 99300
    },
    {
      "epoch": 5.2965333333333335,
      "grad_norm": 0.0842628926038742,
      "learning_rate": 1.6896666666666667e-05,
      "loss": 0.0033,
      "step": 99310
    },
    {
      "epoch": 5.297066666666667,
      "grad_norm": 0.16851453483104706,
      "learning_rate": 1.6893333333333333e-05,
      "loss": 0.0028,
      "step": 99320
    },
    {
      "epoch": 5.2976,
      "grad_norm": 0.08426381647586823,
      "learning_rate": 1.689e-05,
      "loss": 0.0028,
      "step": 99330
    },
    {
      "epoch": 5.298133333333333,
      "grad_norm": 3.347953692056649e-09,
      "learning_rate": 1.688666666666667e-05,
      "loss": 0.0016,
      "step": 99340
    },
    {
      "epoch": 5.298666666666667,
      "grad_norm": 0.2246953547000885,
      "learning_rate": 1.6883333333333335e-05,
      "loss": 0.0025,
      "step": 99350
    },
    {
      "epoch": 5.2992,
      "grad_norm": 0.28086990118026733,
      "learning_rate": 1.688e-05,
      "loss": 0.0028,
      "step": 99360
    },
    {
      "epoch": 5.299733333333333,
      "grad_norm": 0.08425678312778473,
      "learning_rate": 1.6876666666666667e-05,
      "loss": 0.0027,
      "step": 99370
    },
    {
      "epoch": 5.3002666666666665,
      "grad_norm": 0.33703750371932983,
      "learning_rate": 1.6873333333333337e-05,
      "loss": 0.0025,
      "step": 99380
    },
    {
      "epoch": 5.3008,
      "grad_norm": 0.33705517649650574,
      "learning_rate": 1.687e-05,
      "loss": 0.0024,
      "step": 99390
    },
    {
      "epoch": 5.301333333333333,
      "grad_norm": 0.05617152526974678,
      "learning_rate": 1.6866666666666666e-05,
      "loss": 0.0039,
      "step": 99400
    },
    {
      "epoch": 5.301866666666666,
      "grad_norm": 0.2246999591588974,
      "learning_rate": 1.6863333333333335e-05,
      "loss": 0.0029,
      "step": 99410
    },
    {
      "epoch": 5.3024000000000004,
      "grad_norm": 0.028087137266993523,
      "learning_rate": 1.686e-05,
      "loss": 0.0032,
      "step": 99420
    },
    {
      "epoch": 5.302933333333334,
      "grad_norm": 0.11234097182750702,
      "learning_rate": 1.6856666666666667e-05,
      "loss": 0.0023,
      "step": 99430
    },
    {
      "epoch": 5.303466666666667,
      "grad_norm": 0.028086796402931213,
      "learning_rate": 1.6853333333333333e-05,
      "loss": 0.0027,
      "step": 99440
    },
    {
      "epoch": 5.304,
      "grad_norm": 0.2808777093887329,
      "learning_rate": 1.6850000000000003e-05,
      "loss": 0.0035,
      "step": 99450
    },
    {
      "epoch": 5.3045333333333335,
      "grad_norm": 0.11234674602746964,
      "learning_rate": 1.6846666666666666e-05,
      "loss": 0.0025,
      "step": 99460
    },
    {
      "epoch": 5.305066666666667,
      "grad_norm": 0.47744324803352356,
      "learning_rate": 1.6843333333333332e-05,
      "loss": 0.0023,
      "step": 99470
    },
    {
      "epoch": 5.3056,
      "grad_norm": 0.14043541252613068,
      "learning_rate": 1.684e-05,
      "loss": 0.0037,
      "step": 99480
    },
    {
      "epoch": 5.306133333333333,
      "grad_norm": 4.264597563974348e-09,
      "learning_rate": 1.6836666666666668e-05,
      "loss": 0.002,
      "step": 99490
    },
    {
      "epoch": 5.306666666666667,
      "grad_norm": 0.05617058277130127,
      "learning_rate": 1.6833333333333334e-05,
      "loss": 0.0035,
      "step": 99500
    },
    {
      "epoch": 5.3072,
      "grad_norm": 0.08425622433423996,
      "learning_rate": 1.683e-05,
      "loss": 0.0014,
      "step": 99510
    },
    {
      "epoch": 5.307733333333333,
      "grad_norm": 0.14043258130550385,
      "learning_rate": 1.682666666666667e-05,
      "loss": 0.0031,
      "step": 99520
    },
    {
      "epoch": 5.3082666666666665,
      "grad_norm": 0.0842522382736206,
      "learning_rate": 1.6823333333333335e-05,
      "loss": 0.002,
      "step": 99530
    },
    {
      "epoch": 5.3088,
      "grad_norm": 0.3089481294155121,
      "learning_rate": 1.6819999999999998e-05,
      "loss": 0.0031,
      "step": 99540
    },
    {
      "epoch": 5.309333333333333,
      "grad_norm": 0.19659946858882904,
      "learning_rate": 1.6816666666666668e-05,
      "loss": 0.0021,
      "step": 99550
    },
    {
      "epoch": 5.309866666666666,
      "grad_norm": 0.1123410239815712,
      "learning_rate": 1.6813333333333334e-05,
      "loss": 0.0019,
      "step": 99560
    },
    {
      "epoch": 5.3104,
      "grad_norm": 0.05617040768265724,
      "learning_rate": 1.681e-05,
      "loss": 0.0027,
      "step": 99570
    },
    {
      "epoch": 5.310933333333334,
      "grad_norm": 0.5617261528968811,
      "learning_rate": 1.6806666666666666e-05,
      "loss": 0.0032,
      "step": 99580
    },
    {
      "epoch": 5.311466666666667,
      "grad_norm": 0.33701130747795105,
      "learning_rate": 1.6803333333333336e-05,
      "loss": 0.0026,
      "step": 99590
    },
    {
      "epoch": 5.312,
      "grad_norm": 0.2527795135974884,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 0.0027,
      "step": 99600
    },
    {
      "epoch": 5.3125333333333336,
      "grad_norm": 0.11234424263238907,
      "learning_rate": 1.6796666666666665e-05,
      "loss": 0.0043,
      "step": 99610
    },
    {
      "epoch": 5.313066666666667,
      "grad_norm": 0.1123376116156578,
      "learning_rate": 1.6793333333333334e-05,
      "loss": 0.0029,
      "step": 99620
    },
    {
      "epoch": 5.3136,
      "grad_norm": 0.0842573419213295,
      "learning_rate": 1.679e-05,
      "loss": 0.0024,
      "step": 99630
    },
    {
      "epoch": 5.314133333333333,
      "grad_norm": 0.05617000162601471,
      "learning_rate": 1.6786666666666666e-05,
      "loss": 0.0025,
      "step": 99640
    },
    {
      "epoch": 5.314666666666667,
      "grad_norm": 2.924701814066566e-09,
      "learning_rate": 1.6783333333333336e-05,
      "loss": 0.0023,
      "step": 99650
    },
    {
      "epoch": 5.3152,
      "grad_norm": 0.11234646290540695,
      "learning_rate": 1.6780000000000002e-05,
      "loss": 0.0023,
      "step": 99660
    },
    {
      "epoch": 5.315733333333333,
      "grad_norm": 0.05617138370871544,
      "learning_rate": 1.6776666666666668e-05,
      "loss": 0.0029,
      "step": 99670
    },
    {
      "epoch": 5.3162666666666665,
      "grad_norm": 0.4493430256843567,
      "learning_rate": 1.6773333333333334e-05,
      "loss": 0.0025,
      "step": 99680
    },
    {
      "epoch": 5.3168,
      "grad_norm": 1.1187232732772827,
      "learning_rate": 1.677e-05,
      "loss": 0.0028,
      "step": 99690
    },
    {
      "epoch": 5.317333333333333,
      "grad_norm": 0.08425810188055038,
      "learning_rate": 1.6766666666666667e-05,
      "loss": 0.0023,
      "step": 99700
    },
    {
      "epoch": 5.317866666666666,
      "grad_norm": 0.16850978136062622,
      "learning_rate": 1.6763333333333333e-05,
      "loss": 0.0024,
      "step": 99710
    },
    {
      "epoch": 5.3184000000000005,
      "grad_norm": 0.08425441384315491,
      "learning_rate": 1.6760000000000002e-05,
      "loss": 0.003,
      "step": 99720
    },
    {
      "epoch": 5.318933333333334,
      "grad_norm": 0.2808556854724884,
      "learning_rate": 1.6756666666666668e-05,
      "loss": 0.0024,
      "step": 99730
    },
    {
      "epoch": 5.319466666666667,
      "grad_norm": 7.138878821422168e-09,
      "learning_rate": 1.6753333333333334e-05,
      "loss": 0.0016,
      "step": 99740
    },
    {
      "epoch": 5.32,
      "grad_norm": 0.08425360918045044,
      "learning_rate": 1.675e-05,
      "loss": 0.0031,
      "step": 99750
    },
    {
      "epoch": 5.320533333333334,
      "grad_norm": 0.02808420918881893,
      "learning_rate": 1.674666666666667e-05,
      "loss": 0.0032,
      "step": 99760
    },
    {
      "epoch": 5.321066666666667,
      "grad_norm": 0.08425228297710419,
      "learning_rate": 1.6743333333333333e-05,
      "loss": 0.0026,
      "step": 99770
    },
    {
      "epoch": 5.3216,
      "grad_norm": 0.08425509929656982,
      "learning_rate": 1.674e-05,
      "loss": 0.0023,
      "step": 99780
    },
    {
      "epoch": 5.322133333333333,
      "grad_norm": 0.16850295662879944,
      "learning_rate": 1.673666666666667e-05,
      "loss": 0.0029,
      "step": 99790
    },
    {
      "epoch": 5.322666666666667,
      "grad_norm": 0.0842587798833847,
      "learning_rate": 1.6733333333333335e-05,
      "loss": 0.0039,
      "step": 99800
    },
    {
      "epoch": 5.3232,
      "grad_norm": 0.1965932548046112,
      "learning_rate": 1.673e-05,
      "loss": 0.0026,
      "step": 99810
    },
    {
      "epoch": 5.323733333333333,
      "grad_norm": 0.252756804227829,
      "learning_rate": 1.6726666666666667e-05,
      "loss": 0.0036,
      "step": 99820
    },
    {
      "epoch": 5.3242666666666665,
      "grad_norm": 0.11234257370233536,
      "learning_rate": 1.6723333333333336e-05,
      "loss": 0.0022,
      "step": 99830
    },
    {
      "epoch": 5.3248,
      "grad_norm": 0.1685035079717636,
      "learning_rate": 1.672e-05,
      "loss": 0.0036,
      "step": 99840
    },
    {
      "epoch": 5.325333333333333,
      "grad_norm": 0.02808523178100586,
      "learning_rate": 1.6716666666666665e-05,
      "loss": 0.0036,
      "step": 99850
    },
    {
      "epoch": 5.325866666666666,
      "grad_norm": 0.0842578336596489,
      "learning_rate": 1.6713333333333335e-05,
      "loss": 0.0033,
      "step": 99860
    },
    {
      "epoch": 5.3264,
      "grad_norm": 0.11233777552843094,
      "learning_rate": 1.671e-05,
      "loss": 0.002,
      "step": 99870
    },
    {
      "epoch": 5.326933333333334,
      "grad_norm": 0.3369968831539154,
      "learning_rate": 1.6706666666666667e-05,
      "loss": 0.0028,
      "step": 99880
    },
    {
      "epoch": 5.327466666666667,
      "grad_norm": 0.08425892144441605,
      "learning_rate": 1.6703333333333333e-05,
      "loss": 0.0028,
      "step": 99890
    },
    {
      "epoch": 5.328,
      "grad_norm": 0.22467434406280518,
      "learning_rate": 1.6700000000000003e-05,
      "loss": 0.0026,
      "step": 99900
    },
    {
      "epoch": 5.328533333333334,
      "grad_norm": 0.08425042778253555,
      "learning_rate": 1.669666666666667e-05,
      "loss": 0.0025,
      "step": 99910
    },
    {
      "epoch": 5.329066666666667,
      "grad_norm": 0.3089219629764557,
      "learning_rate": 1.669333333333333e-05,
      "loss": 0.0022,
      "step": 99920
    },
    {
      "epoch": 5.3296,
      "grad_norm": 0.2808362543582916,
      "learning_rate": 1.669e-05,
      "loss": 0.0038,
      "step": 99930
    },
    {
      "epoch": 5.330133333333333,
      "grad_norm": 0.16850095987319946,
      "learning_rate": 1.6686666666666667e-05,
      "loss": 0.0033,
      "step": 99940
    },
    {
      "epoch": 5.330666666666667,
      "grad_norm": 1.2703951597213745,
      "learning_rate": 1.6683333333333333e-05,
      "loss": 0.0034,
      "step": 99950
    },
    {
      "epoch": 5.3312,
      "grad_norm": 4.11614609063804e-09,
      "learning_rate": 1.668e-05,
      "loss": 0.002,
      "step": 99960
    },
    {
      "epoch": 5.331733333333333,
      "grad_norm": 0.22467336058616638,
      "learning_rate": 1.667666666666667e-05,
      "loss": 0.0021,
      "step": 99970
    },
    {
      "epoch": 5.3322666666666665,
      "grad_norm": 0.14041365683078766,
      "learning_rate": 1.6673333333333335e-05,
      "loss": 0.0024,
      "step": 99980
    },
    {
      "epoch": 5.3328,
      "grad_norm": 0.16850389540195465,
      "learning_rate": 1.6669999999999998e-05,
      "loss": 0.0031,
      "step": 99990
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 0.33700209856033325,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.0036,
      "step": 100000
    },
    {
      "epoch": 5.333866666666666,
      "grad_norm": 0.1123388335108757,
      "learning_rate": 1.6663333333333334e-05,
      "loss": 0.0023,
      "step": 100010
    },
    {
      "epoch": 5.3344,
      "grad_norm": 0.2527638375759125,
      "learning_rate": 1.666e-05,
      "loss": 0.0024,
      "step": 100020
    },
    {
      "epoch": 5.334933333333334,
      "grad_norm": 0.19658008217811584,
      "learning_rate": 1.665666666666667e-05,
      "loss": 0.0032,
      "step": 100030
    },
    {
      "epoch": 5.335466666666667,
      "grad_norm": 0.056167617440223694,
      "learning_rate": 1.6653333333333335e-05,
      "loss": 0.0026,
      "step": 100040
    },
    {
      "epoch": 5.336,
      "grad_norm": 0.20426535606384277,
      "learning_rate": 1.665e-05,
      "loss": 0.0028,
      "step": 100050
    },
    {
      "epoch": 5.336533333333334,
      "grad_norm": 0.16849282383918762,
      "learning_rate": 1.6646666666666668e-05,
      "loss": 0.0023,
      "step": 100060
    },
    {
      "epoch": 5.337066666666667,
      "grad_norm": 0.11233694851398468,
      "learning_rate": 1.6643333333333334e-05,
      "loss": 0.002,
      "step": 100070
    },
    {
      "epoch": 5.3376,
      "grad_norm": 0.1685047447681427,
      "learning_rate": 1.664e-05,
      "loss": 0.002,
      "step": 100080
    },
    {
      "epoch": 5.338133333333333,
      "grad_norm": 0.08424897491931915,
      "learning_rate": 1.6636666666666666e-05,
      "loss": 0.0019,
      "step": 100090
    },
    {
      "epoch": 5.338666666666667,
      "grad_norm": 0.08424872905015945,
      "learning_rate": 1.6633333333333336e-05,
      "loss": 0.0027,
      "step": 100100
    },
    {
      "epoch": 5.3392,
      "grad_norm": 0.28084084391593933,
      "learning_rate": 1.6630000000000002e-05,
      "loss": 0.0023,
      "step": 100110
    },
    {
      "epoch": 5.339733333333333,
      "grad_norm": 0.02808448299765587,
      "learning_rate": 1.6626666666666668e-05,
      "loss": 0.0032,
      "step": 100120
    },
    {
      "epoch": 5.3402666666666665,
      "grad_norm": 0.08424881845712662,
      "learning_rate": 1.6623333333333334e-05,
      "loss": 0.0022,
      "step": 100130
    },
    {
      "epoch": 5.3408,
      "grad_norm": 0.1925157606601715,
      "learning_rate": 1.662e-05,
      "loss": 0.0024,
      "step": 100140
    },
    {
      "epoch": 5.341333333333333,
      "grad_norm": 0.16851064562797546,
      "learning_rate": 1.6616666666666666e-05,
      "loss": 0.0019,
      "step": 100150
    },
    {
      "epoch": 5.341866666666666,
      "grad_norm": 0.33700206875801086,
      "learning_rate": 1.6613333333333332e-05,
      "loss": 0.003,
      "step": 100160
    },
    {
      "epoch": 5.3424,
      "grad_norm": 1.488032341003418,
      "learning_rate": 1.6610000000000002e-05,
      "loss": 0.0022,
      "step": 100170
    },
    {
      "epoch": 5.342933333333333,
      "grad_norm": 0.112330362200737,
      "learning_rate": 1.6606666666666668e-05,
      "loss": 0.0035,
      "step": 100180
    },
    {
      "epoch": 5.343466666666667,
      "grad_norm": 0.1684970259666443,
      "learning_rate": 1.6603333333333334e-05,
      "loss": 0.0027,
      "step": 100190
    },
    {
      "epoch": 5.344,
      "grad_norm": 0.02808305062353611,
      "learning_rate": 1.66e-05,
      "loss": 0.0021,
      "step": 100200
    },
    {
      "epoch": 5.344533333333334,
      "grad_norm": 0.2808299958705902,
      "learning_rate": 1.659666666666667e-05,
      "loss": 0.0019,
      "step": 100210
    },
    {
      "epoch": 5.345066666666667,
      "grad_norm": 0.056164566427469254,
      "learning_rate": 1.6593333333333333e-05,
      "loss": 0.003,
      "step": 100220
    },
    {
      "epoch": 5.3456,
      "grad_norm": 0.05616399273276329,
      "learning_rate": 1.659e-05,
      "loss": 0.0024,
      "step": 100230
    },
    {
      "epoch": 5.346133333333333,
      "grad_norm": 0.2527503967285156,
      "learning_rate": 1.6586666666666668e-05,
      "loss": 0.0025,
      "step": 100240
    },
    {
      "epoch": 5.346666666666667,
      "grad_norm": 0.05616584047675133,
      "learning_rate": 1.6583333333333334e-05,
      "loss": 0.0041,
      "step": 100250
    },
    {
      "epoch": 5.3472,
      "grad_norm": 0.3089216649532318,
      "learning_rate": 1.658e-05,
      "loss": 0.0025,
      "step": 100260
    },
    {
      "epoch": 5.347733333333333,
      "grad_norm": 0.3088989555835724,
      "learning_rate": 1.6576666666666667e-05,
      "loss": 0.0021,
      "step": 100270
    },
    {
      "epoch": 5.3482666666666665,
      "grad_norm": 0.05616483464837074,
      "learning_rate": 1.6573333333333336e-05,
      "loss": 0.0033,
      "step": 100280
    },
    {
      "epoch": 5.3488,
      "grad_norm": 0.05616435781121254,
      "learning_rate": 1.657e-05,
      "loss": 0.0038,
      "step": 100290
    },
    {
      "epoch": 5.349333333333333,
      "grad_norm": 0.11232729256153107,
      "learning_rate": 1.6566666666666665e-05,
      "loss": 0.0021,
      "step": 100300
    },
    {
      "epoch": 5.349866666666666,
      "grad_norm": 0.028081996366381645,
      "learning_rate": 1.6563333333333335e-05,
      "loss": 0.0022,
      "step": 100310
    },
    {
      "epoch": 5.3504,
      "grad_norm": 0.11233431100845337,
      "learning_rate": 1.656e-05,
      "loss": 0.0026,
      "step": 100320
    },
    {
      "epoch": 5.350933333333334,
      "grad_norm": 0.11233197897672653,
      "learning_rate": 1.6556666666666667e-05,
      "loss": 0.0025,
      "step": 100330
    },
    {
      "epoch": 5.351466666666667,
      "grad_norm": 0.028083017095923424,
      "learning_rate": 1.6553333333333333e-05,
      "loss": 0.0025,
      "step": 100340
    },
    {
      "epoch": 5.352,
      "grad_norm": 0.028084127232432365,
      "learning_rate": 1.6550000000000002e-05,
      "loss": 0.0037,
      "step": 100350
    },
    {
      "epoch": 5.352533333333334,
      "grad_norm": 0.14041773974895477,
      "learning_rate": 1.654666666666667e-05,
      "loss": 0.0025,
      "step": 100360
    },
    {
      "epoch": 5.353066666666667,
      "grad_norm": 0.056162796914577484,
      "learning_rate": 1.654333333333333e-05,
      "loss": 0.0039,
      "step": 100370
    },
    {
      "epoch": 5.3536,
      "grad_norm": 0.5616894960403442,
      "learning_rate": 1.654e-05,
      "loss": 0.004,
      "step": 100380
    },
    {
      "epoch": 5.354133333333333,
      "grad_norm": 0.3089101016521454,
      "learning_rate": 1.6536666666666667e-05,
      "loss": 0.0036,
      "step": 100390
    },
    {
      "epoch": 5.354666666666667,
      "grad_norm": 0.08424320816993713,
      "learning_rate": 1.6533333333333333e-05,
      "loss": 0.0036,
      "step": 100400
    },
    {
      "epoch": 5.3552,
      "grad_norm": 0.11232710629701614,
      "learning_rate": 1.6530000000000003e-05,
      "loss": 0.0037,
      "step": 100410
    },
    {
      "epoch": 5.355733333333333,
      "grad_norm": 1.8338890075683594,
      "learning_rate": 1.652666666666667e-05,
      "loss": 0.004,
      "step": 100420
    },
    {
      "epoch": 5.3562666666666665,
      "grad_norm": 0.28081387281417847,
      "learning_rate": 1.6523333333333335e-05,
      "loss": 0.0048,
      "step": 100430
    },
    {
      "epoch": 5.3568,
      "grad_norm": 0.2246491014957428,
      "learning_rate": 1.652e-05,
      "loss": 0.0016,
      "step": 100440
    },
    {
      "epoch": 5.357333333333333,
      "grad_norm": 3.0298417108554077e-09,
      "learning_rate": 1.6516666666666667e-05,
      "loss": 0.0035,
      "step": 100450
    },
    {
      "epoch": 5.357866666666666,
      "grad_norm": 1.5843338863064105e-09,
      "learning_rate": 1.6513333333333333e-05,
      "loss": 0.0032,
      "step": 100460
    },
    {
      "epoch": 5.3584,
      "grad_norm": 0.36505037546157837,
      "learning_rate": 1.651e-05,
      "loss": 0.0032,
      "step": 100470
    },
    {
      "epoch": 5.358933333333333,
      "grad_norm": 0.11232948303222656,
      "learning_rate": 1.650666666666667e-05,
      "loss": 0.0023,
      "step": 100480
    },
    {
      "epoch": 5.359466666666667,
      "grad_norm": 0.2808225750923157,
      "learning_rate": 1.6503333333333335e-05,
      "loss": 0.0026,
      "step": 100490
    },
    {
      "epoch": 5.36,
      "grad_norm": 0.22465667128562927,
      "learning_rate": 1.65e-05,
      "loss": 0.0021,
      "step": 100500
    },
    {
      "epoch": 5.360533333333334,
      "grad_norm": 0.3650515377521515,
      "learning_rate": 1.6496666666666667e-05,
      "loss": 0.0022,
      "step": 100510
    },
    {
      "epoch": 5.361066666666667,
      "grad_norm": 0.05616514012217522,
      "learning_rate": 1.6493333333333334e-05,
      "loss": 0.0032,
      "step": 100520
    },
    {
      "epoch": 5.3616,
      "grad_norm": 0.25274839997291565,
      "learning_rate": 1.649e-05,
      "loss": 0.0036,
      "step": 100530
    },
    {
      "epoch": 5.362133333333333,
      "grad_norm": 4.272720813751221,
      "learning_rate": 1.6486666666666666e-05,
      "loss": 0.0034,
      "step": 100540
    },
    {
      "epoch": 5.362666666666667,
      "grad_norm": 0.08424470573663712,
      "learning_rate": 1.6483333333333335e-05,
      "loss": 0.0023,
      "step": 100550
    },
    {
      "epoch": 5.3632,
      "grad_norm": 0.280806303024292,
      "learning_rate": 1.648e-05,
      "loss": 0.0037,
      "step": 100560
    },
    {
      "epoch": 5.363733333333333,
      "grad_norm": 0.08424587547779083,
      "learning_rate": 1.6476666666666668e-05,
      "loss": 0.0025,
      "step": 100570
    },
    {
      "epoch": 5.3642666666666665,
      "grad_norm": 0.22465547919273376,
      "learning_rate": 1.6473333333333334e-05,
      "loss": 0.0032,
      "step": 100580
    },
    {
      "epoch": 5.3648,
      "grad_norm": 0.0280806515365839,
      "learning_rate": 1.6470000000000003e-05,
      "loss": 0.0024,
      "step": 100590
    },
    {
      "epoch": 5.365333333333333,
      "grad_norm": 2.9596996307373047,
      "learning_rate": 1.6466666666666666e-05,
      "loss": 0.0017,
      "step": 100600
    },
    {
      "epoch": 5.365866666666666,
      "grad_norm": 0.08424355089664459,
      "learning_rate": 1.6463333333333332e-05,
      "loss": 0.0029,
      "step": 100610
    },
    {
      "epoch": 5.3664,
      "grad_norm": 0.22464890778064728,
      "learning_rate": 1.646e-05,
      "loss": 0.0022,
      "step": 100620
    },
    {
      "epoch": 5.366933333333334,
      "grad_norm": 0.02808084897696972,
      "learning_rate": 1.6456666666666668e-05,
      "loss": 0.0025,
      "step": 100630
    },
    {
      "epoch": 5.367466666666667,
      "grad_norm": 0.05616383254528046,
      "learning_rate": 1.6453333333333334e-05,
      "loss": 0.0015,
      "step": 100640
    },
    {
      "epoch": 5.368,
      "grad_norm": 0.02808244898915291,
      "learning_rate": 1.645e-05,
      "loss": 0.0028,
      "step": 100650
    },
    {
      "epoch": 5.368533333333334,
      "grad_norm": 0.05616474151611328,
      "learning_rate": 1.644666666666667e-05,
      "loss": 0.0033,
      "step": 100660
    },
    {
      "epoch": 5.369066666666667,
      "grad_norm": 0.6739168763160706,
      "learning_rate": 1.6443333333333332e-05,
      "loss": 0.0024,
      "step": 100670
    },
    {
      "epoch": 5.3696,
      "grad_norm": 0.028081396594643593,
      "learning_rate": 1.644e-05,
      "loss": 0.0037,
      "step": 100680
    },
    {
      "epoch": 5.370133333333333,
      "grad_norm": 0.11232543736696243,
      "learning_rate": 1.6436666666666668e-05,
      "loss": 0.0022,
      "step": 100690
    },
    {
      "epoch": 5.370666666666667,
      "grad_norm": 0.19655868411064148,
      "learning_rate": 1.6433333333333334e-05,
      "loss": 0.0041,
      "step": 100700
    },
    {
      "epoch": 5.3712,
      "grad_norm": 0.2527441382408142,
      "learning_rate": 1.643e-05,
      "loss": 0.0034,
      "step": 100710
    },
    {
      "epoch": 5.371733333333333,
      "grad_norm": 0.19656603038311005,
      "learning_rate": 1.6426666666666666e-05,
      "loss": 0.0021,
      "step": 100720
    },
    {
      "epoch": 5.3722666666666665,
      "grad_norm": 0.2527272701263428,
      "learning_rate": 1.6423333333333336e-05,
      "loss": 0.0019,
      "step": 100730
    },
    {
      "epoch": 5.3728,
      "grad_norm": 0.05615992844104767,
      "learning_rate": 1.6420000000000002e-05,
      "loss": 0.0024,
      "step": 100740
    },
    {
      "epoch": 5.373333333333333,
      "grad_norm": 0.1965619921684265,
      "learning_rate": 1.6416666666666665e-05,
      "loss": 0.0033,
      "step": 100750
    },
    {
      "epoch": 5.373866666666666,
      "grad_norm": 0.196569561958313,
      "learning_rate": 1.6413333333333334e-05,
      "loss": 0.003,
      "step": 100760
    },
    {
      "epoch": 5.3744,
      "grad_norm": 3.4399427750742007e-09,
      "learning_rate": 1.641e-05,
      "loss": 0.0033,
      "step": 100770
    },
    {
      "epoch": 5.374933333333333,
      "grad_norm": 0.16848120093345642,
      "learning_rate": 1.6406666666666667e-05,
      "loss": 0.0029,
      "step": 100780
    },
    {
      "epoch": 5.375466666666667,
      "grad_norm": 0.28081846237182617,
      "learning_rate": 1.6403333333333336e-05,
      "loss": 0.0032,
      "step": 100790
    },
    {
      "epoch": 5.376,
      "grad_norm": 0.02808070369064808,
      "learning_rate": 1.6400000000000002e-05,
      "loss": 0.0039,
      "step": 100800
    },
    {
      "epoch": 5.376533333333334,
      "grad_norm": 0.08423889428377151,
      "learning_rate": 1.639666666666667e-05,
      "loss": 0.0026,
      "step": 100810
    },
    {
      "epoch": 5.377066666666667,
      "grad_norm": 0.30888375639915466,
      "learning_rate": 1.639333333333333e-05,
      "loss": 0.0026,
      "step": 100820
    },
    {
      "epoch": 5.3776,
      "grad_norm": 0.11232004314661026,
      "learning_rate": 1.639e-05,
      "loss": 0.0016,
      "step": 100830
    },
    {
      "epoch": 5.378133333333333,
      "grad_norm": 0.08424226194620132,
      "learning_rate": 1.6386666666666667e-05,
      "loss": 0.0031,
      "step": 100840
    },
    {
      "epoch": 5.378666666666667,
      "grad_norm": 0.36502450704574585,
      "learning_rate": 1.6383333333333333e-05,
      "loss": 0.0024,
      "step": 100850
    },
    {
      "epoch": 5.3792,
      "grad_norm": 0.14040322601795197,
      "learning_rate": 1.6380000000000002e-05,
      "loss": 0.0033,
      "step": 100860
    },
    {
      "epoch": 5.379733333333333,
      "grad_norm": 0.1684960126876831,
      "learning_rate": 1.637666666666667e-05,
      "loss": 0.0028,
      "step": 100870
    },
    {
      "epoch": 5.3802666666666665,
      "grad_norm": 0.19655410945415497,
      "learning_rate": 1.6373333333333335e-05,
      "loss": 0.0023,
      "step": 100880
    },
    {
      "epoch": 5.3808,
      "grad_norm": 1.281450629234314,
      "learning_rate": 1.637e-05,
      "loss": 0.0044,
      "step": 100890
    },
    {
      "epoch": 5.381333333333333,
      "grad_norm": 0.028079941868782043,
      "learning_rate": 1.6366666666666667e-05,
      "loss": 0.0029,
      "step": 100900
    },
    {
      "epoch": 5.381866666666666,
      "grad_norm": 0.25271692872047424,
      "learning_rate": 1.6363333333333333e-05,
      "loss": 0.003,
      "step": 100910
    },
    {
      "epoch": 5.3824,
      "grad_norm": 0.33695217967033386,
      "learning_rate": 1.636e-05,
      "loss": 0.0026,
      "step": 100920
    },
    {
      "epoch": 5.382933333333334,
      "grad_norm": 0.4212149381637573,
      "learning_rate": 1.635666666666667e-05,
      "loss": 0.0019,
      "step": 100930
    },
    {
      "epoch": 5.383466666666667,
      "grad_norm": 0.028080489486455917,
      "learning_rate": 1.6353333333333335e-05,
      "loss": 0.0036,
      "step": 100940
    },
    {
      "epoch": 5.384,
      "grad_norm": 0.1403989940881729,
      "learning_rate": 1.635e-05,
      "loss": 0.0023,
      "step": 100950
    },
    {
      "epoch": 5.384533333333334,
      "grad_norm": 0.11231967061758041,
      "learning_rate": 1.6346666666666667e-05,
      "loss": 0.0037,
      "step": 100960
    },
    {
      "epoch": 5.385066666666667,
      "grad_norm": 0.16847632825374603,
      "learning_rate": 1.6343333333333337e-05,
      "loss": 0.0027,
      "step": 100970
    },
    {
      "epoch": 5.3856,
      "grad_norm": 2.191130610285086e-09,
      "learning_rate": 1.634e-05,
      "loss": 0.0031,
      "step": 100980
    },
    {
      "epoch": 5.386133333333333,
      "grad_norm": 0.22464922070503235,
      "learning_rate": 1.6336666666666666e-05,
      "loss": 0.0025,
      "step": 100990
    },
    {
      "epoch": 5.386666666666667,
      "grad_norm": 0.22464632987976074,
      "learning_rate": 1.6333333333333335e-05,
      "loss": 0.0033,
      "step": 101000
    },
    {
      "epoch": 5.3872,
      "grad_norm": 0.3930860757827759,
      "learning_rate": 1.633e-05,
      "loss": 0.0024,
      "step": 101010
    },
    {
      "epoch": 5.387733333333333,
      "grad_norm": 0.0842382162809372,
      "learning_rate": 1.6326666666666667e-05,
      "loss": 0.0016,
      "step": 101020
    },
    {
      "epoch": 5.3882666666666665,
      "grad_norm": 0.1965620070695877,
      "learning_rate": 1.6323333333333333e-05,
      "loss": 0.003,
      "step": 101030
    },
    {
      "epoch": 5.3888,
      "grad_norm": 0.028081024065613747,
      "learning_rate": 1.6320000000000003e-05,
      "loss": 0.0026,
      "step": 101040
    },
    {
      "epoch": 5.389333333333333,
      "grad_norm": 0.08423873037099838,
      "learning_rate": 1.6316666666666666e-05,
      "loss": 0.0037,
      "step": 101050
    },
    {
      "epoch": 5.389866666666666,
      "grad_norm": 0.3930962085723877,
      "learning_rate": 1.6313333333333332e-05,
      "loss": 0.0034,
      "step": 101060
    },
    {
      "epoch": 5.3904,
      "grad_norm": 0.08423992246389389,
      "learning_rate": 1.631e-05,
      "loss": 0.0021,
      "step": 101070
    },
    {
      "epoch": 5.390933333333333,
      "grad_norm": 0.028078321367502213,
      "learning_rate": 1.6306666666666668e-05,
      "loss": 0.0018,
      "step": 101080
    },
    {
      "epoch": 5.391466666666667,
      "grad_norm": 0.056159015744924545,
      "learning_rate": 1.6303333333333334e-05,
      "loss": 0.0026,
      "step": 101090
    },
    {
      "epoch": 5.392,
      "grad_norm": 0.14039255678653717,
      "learning_rate": 1.63e-05,
      "loss": 0.0024,
      "step": 101100
    },
    {
      "epoch": 5.392533333333334,
      "grad_norm": 0.05615687370300293,
      "learning_rate": 1.629666666666667e-05,
      "loss": 0.0033,
      "step": 101110
    },
    {
      "epoch": 5.393066666666667,
      "grad_norm": 0.11231434345245361,
      "learning_rate": 1.6293333333333335e-05,
      "loss": 0.002,
      "step": 101120
    },
    {
      "epoch": 5.3936,
      "grad_norm": 0.16846761107444763,
      "learning_rate": 1.6289999999999998e-05,
      "loss": 0.0034,
      "step": 101130
    },
    {
      "epoch": 5.3941333333333334,
      "grad_norm": 1.7675518989562988,
      "learning_rate": 1.6286666666666668e-05,
      "loss": 0.0025,
      "step": 101140
    },
    {
      "epoch": 5.394666666666667,
      "grad_norm": 0.28080207109451294,
      "learning_rate": 1.6283333333333334e-05,
      "loss": 0.003,
      "step": 101150
    },
    {
      "epoch": 5.3952,
      "grad_norm": 0.14039884507656097,
      "learning_rate": 1.628e-05,
      "loss": 0.0021,
      "step": 101160
    },
    {
      "epoch": 5.395733333333333,
      "grad_norm": 0.2319246381521225,
      "learning_rate": 1.627666666666667e-05,
      "loss": 0.0025,
      "step": 101170
    },
    {
      "epoch": 5.3962666666666665,
      "grad_norm": 0.05615765601396561,
      "learning_rate": 1.6273333333333336e-05,
      "loss": 0.0032,
      "step": 101180
    },
    {
      "epoch": 5.3968,
      "grad_norm": 0.0561576746404171,
      "learning_rate": 1.6270000000000002e-05,
      "loss": 0.0035,
      "step": 101190
    },
    {
      "epoch": 5.397333333333333,
      "grad_norm": 0.19655175507068634,
      "learning_rate": 1.6266666666666665e-05,
      "loss": 0.0027,
      "step": 101200
    },
    {
      "epoch": 5.397866666666666,
      "grad_norm": 0.19654391705989838,
      "learning_rate": 1.6263333333333334e-05,
      "loss": 0.003,
      "step": 101210
    },
    {
      "epoch": 5.3984,
      "grad_norm": 5.093791610732978e-09,
      "learning_rate": 1.626e-05,
      "loss": 0.0024,
      "step": 101220
    },
    {
      "epoch": 5.398933333333333,
      "grad_norm": 0.16846774518489838,
      "learning_rate": 1.6256666666666666e-05,
      "loss": 0.0022,
      "step": 101230
    },
    {
      "epoch": 5.399466666666667,
      "grad_norm": 0.19654610753059387,
      "learning_rate": 1.6253333333333336e-05,
      "loss": 0.0029,
      "step": 101240
    },
    {
      "epoch": 5.4,
      "grad_norm": 0.056155722588300705,
      "learning_rate": 1.6250000000000002e-05,
      "loss": 0.0033,
      "step": 101250
    },
    {
      "epoch": 5.400533333333334,
      "grad_norm": 0.11231113970279694,
      "learning_rate": 1.6246666666666668e-05,
      "loss": 0.003,
      "step": 101260
    },
    {
      "epoch": 5.401066666666667,
      "grad_norm": 0.11231357604265213,
      "learning_rate": 1.6243333333333334e-05,
      "loss": 0.0034,
      "step": 101270
    },
    {
      "epoch": 5.4016,
      "grad_norm": 0.1684769243001938,
      "learning_rate": 1.624e-05,
      "loss": 0.0016,
      "step": 101280
    },
    {
      "epoch": 5.4021333333333335,
      "grad_norm": 0.028078466653823853,
      "learning_rate": 1.6236666666666667e-05,
      "loss": 0.0026,
      "step": 101290
    },
    {
      "epoch": 5.402666666666667,
      "grad_norm": 0.22461849451065063,
      "learning_rate": 1.6233333333333333e-05,
      "loss": 0.0037,
      "step": 101300
    },
    {
      "epoch": 5.4032,
      "grad_norm": 0.08423618972301483,
      "learning_rate": 1.6230000000000002e-05,
      "loss": 0.0023,
      "step": 101310
    },
    {
      "epoch": 5.403733333333333,
      "grad_norm": 0.028078647330403328,
      "learning_rate": 1.6226666666666668e-05,
      "loss": 0.003,
      "step": 101320
    },
    {
      "epoch": 5.4042666666666666,
      "grad_norm": 0.44923463463783264,
      "learning_rate": 1.6223333333333334e-05,
      "loss": 0.0035,
      "step": 101330
    },
    {
      "epoch": 5.4048,
      "grad_norm": 3.058996389526669e-09,
      "learning_rate": 1.622e-05,
      "loss": 0.0038,
      "step": 101340
    },
    {
      "epoch": 5.405333333333333,
      "grad_norm": 1.2840608576425439e-09,
      "learning_rate": 1.6216666666666667e-05,
      "loss": 0.0028,
      "step": 101350
    },
    {
      "epoch": 5.405866666666666,
      "grad_norm": 0.08423151075839996,
      "learning_rate": 1.6213333333333333e-05,
      "loss": 0.0028,
      "step": 101360
    },
    {
      "epoch": 5.4064,
      "grad_norm": 0.30885934829711914,
      "learning_rate": 1.621e-05,
      "loss": 0.0026,
      "step": 101370
    },
    {
      "epoch": 5.406933333333333,
      "grad_norm": 0.056157760322093964,
      "learning_rate": 1.620666666666667e-05,
      "loss": 0.002,
      "step": 101380
    },
    {
      "epoch": 5.407466666666666,
      "grad_norm": 0.14038461446762085,
      "learning_rate": 1.6203333333333335e-05,
      "loss": 0.003,
      "step": 101390
    },
    {
      "epoch": 5.408,
      "grad_norm": 0.3930852711200714,
      "learning_rate": 1.62e-05,
      "loss": 0.0029,
      "step": 101400
    },
    {
      "epoch": 5.408533333333334,
      "grad_norm": 0.11943253129720688,
      "learning_rate": 1.6196666666666667e-05,
      "loss": 0.0032,
      "step": 101410
    },
    {
      "epoch": 5.409066666666667,
      "grad_norm": 0.08423256874084473,
      "learning_rate": 1.6193333333333336e-05,
      "loss": 0.0038,
      "step": 101420
    },
    {
      "epoch": 5.4096,
      "grad_norm": 0.08423811942338943,
      "learning_rate": 1.619e-05,
      "loss": 0.0033,
      "step": 101430
    },
    {
      "epoch": 5.4101333333333335,
      "grad_norm": 0.08423399925231934,
      "learning_rate": 1.6186666666666665e-05,
      "loss": 0.0027,
      "step": 101440
    },
    {
      "epoch": 5.410666666666667,
      "grad_norm": 0.44925326108932495,
      "learning_rate": 1.6183333333333335e-05,
      "loss": 0.002,
      "step": 101450
    },
    {
      "epoch": 5.4112,
      "grad_norm": 0.3088304400444031,
      "learning_rate": 1.618e-05,
      "loss": 0.004,
      "step": 101460
    },
    {
      "epoch": 5.411733333333333,
      "grad_norm": 0.16847588121891022,
      "learning_rate": 1.6176666666666667e-05,
      "loss": 0.0035,
      "step": 101470
    },
    {
      "epoch": 5.412266666666667,
      "grad_norm": 0.11230830103158951,
      "learning_rate": 1.6173333333333333e-05,
      "loss": 0.0025,
      "step": 101480
    },
    {
      "epoch": 5.4128,
      "grad_norm": 0.2246101200580597,
      "learning_rate": 1.6170000000000003e-05,
      "loss": 0.0034,
      "step": 101490
    },
    {
      "epoch": 5.413333333333333,
      "grad_norm": 0.14038753509521484,
      "learning_rate": 1.6166666666666665e-05,
      "loss": 0.0035,
      "step": 101500
    },
    {
      "epoch": 5.413866666666666,
      "grad_norm": 0.2807712256908417,
      "learning_rate": 1.616333333333333e-05,
      "loss": 0.003,
      "step": 101510
    },
    {
      "epoch": 5.4144,
      "grad_norm": 0.2807556986808777,
      "learning_rate": 1.616e-05,
      "loss": 0.0025,
      "step": 101520
    },
    {
      "epoch": 5.414933333333333,
      "grad_norm": 0.19653469324111938,
      "learning_rate": 1.6156666666666667e-05,
      "loss": 0.0036,
      "step": 101530
    },
    {
      "epoch": 5.415466666666667,
      "grad_norm": 0.2526980936527252,
      "learning_rate": 1.6153333333333333e-05,
      "loss": 0.0031,
      "step": 101540
    },
    {
      "epoch": 5.416,
      "grad_norm": 0.08422821015119553,
      "learning_rate": 1.6150000000000003e-05,
      "loss": 0.0028,
      "step": 101550
    },
    {
      "epoch": 5.416533333333334,
      "grad_norm": 0.028075620532035828,
      "learning_rate": 1.614666666666667e-05,
      "loss": 0.0019,
      "step": 101560
    },
    {
      "epoch": 5.417066666666667,
      "grad_norm": 0.08422909677028656,
      "learning_rate": 1.6143333333333335e-05,
      "loss": 0.0028,
      "step": 101570
    },
    {
      "epoch": 5.4176,
      "grad_norm": 0.05615243315696716,
      "learning_rate": 1.6139999999999998e-05,
      "loss": 0.0028,
      "step": 101580
    },
    {
      "epoch": 5.4181333333333335,
      "grad_norm": 0.02807609736919403,
      "learning_rate": 1.6136666666666667e-05,
      "loss": 0.003,
      "step": 101590
    },
    {
      "epoch": 5.418666666666667,
      "grad_norm": 0.19653058052062988,
      "learning_rate": 1.6133333333333334e-05,
      "loss": 0.0023,
      "step": 101600
    },
    {
      "epoch": 5.4192,
      "grad_norm": 0.25268489122390747,
      "learning_rate": 1.613e-05,
      "loss": 0.0034,
      "step": 101610
    },
    {
      "epoch": 5.419733333333333,
      "grad_norm": 0.16845376789569855,
      "learning_rate": 1.612666666666667e-05,
      "loss": 0.0024,
      "step": 101620
    },
    {
      "epoch": 5.420266666666667,
      "grad_norm": 0.056151967495679855,
      "learning_rate": 1.6123333333333335e-05,
      "loss": 0.003,
      "step": 101630
    },
    {
      "epoch": 5.4208,
      "grad_norm": 0.1965320110321045,
      "learning_rate": 1.612e-05,
      "loss": 0.0028,
      "step": 101640
    },
    {
      "epoch": 5.421333333333333,
      "grad_norm": 0.056151144206523895,
      "learning_rate": 1.6116666666666668e-05,
      "loss": 0.0024,
      "step": 101650
    },
    {
      "epoch": 5.421866666666666,
      "grad_norm": 0.19654050469398499,
      "learning_rate": 1.6113333333333334e-05,
      "loss": 0.0023,
      "step": 101660
    },
    {
      "epoch": 5.4224,
      "grad_norm": 0.14038868248462677,
      "learning_rate": 1.611e-05,
      "loss": 0.0022,
      "step": 101670
    },
    {
      "epoch": 5.422933333333333,
      "grad_norm": 0.08422720432281494,
      "learning_rate": 1.6106666666666666e-05,
      "loss": 0.0027,
      "step": 101680
    },
    {
      "epoch": 5.423466666666666,
      "grad_norm": 3.4695148976027212e-09,
      "learning_rate": 1.6103333333333336e-05,
      "loss": 0.0024,
      "step": 101690
    },
    {
      "epoch": 5.424,
      "grad_norm": 0.2526775300502777,
      "learning_rate": 1.6100000000000002e-05,
      "loss": 0.0025,
      "step": 101700
    },
    {
      "epoch": 5.424533333333334,
      "grad_norm": 0.14037933945655823,
      "learning_rate": 1.6096666666666668e-05,
      "loss": 0.0027,
      "step": 101710
    },
    {
      "epoch": 5.425066666666667,
      "grad_norm": 0.08422568440437317,
      "learning_rate": 1.6093333333333334e-05,
      "loss": 0.0023,
      "step": 101720
    },
    {
      "epoch": 5.4256,
      "grad_norm": 0.1965303272008896,
      "learning_rate": 1.609e-05,
      "loss": 0.0039,
      "step": 101730
    },
    {
      "epoch": 5.4261333333333335,
      "grad_norm": 0.11230842024087906,
      "learning_rate": 1.6086666666666666e-05,
      "loss": 0.0031,
      "step": 101740
    },
    {
      "epoch": 5.426666666666667,
      "grad_norm": 0.19654019176959991,
      "learning_rate": 1.6083333333333332e-05,
      "loss": 0.0035,
      "step": 101750
    },
    {
      "epoch": 5.4272,
      "grad_norm": 0.16844911873340607,
      "learning_rate": 1.6080000000000002e-05,
      "loss": 0.0027,
      "step": 101760
    },
    {
      "epoch": 5.427733333333333,
      "grad_norm": 0.45262545347213745,
      "learning_rate": 1.6076666666666668e-05,
      "loss": 0.0031,
      "step": 101770
    },
    {
      "epoch": 5.428266666666667,
      "grad_norm": 0.19654366374015808,
      "learning_rate": 1.6073333333333334e-05,
      "loss": 0.0047,
      "step": 101780
    },
    {
      "epoch": 5.4288,
      "grad_norm": 0.028076540678739548,
      "learning_rate": 1.607e-05,
      "loss": 0.0031,
      "step": 101790
    },
    {
      "epoch": 5.429333333333333,
      "grad_norm": 0.3930552303791046,
      "learning_rate": 1.606666666666667e-05,
      "loss": 0.0037,
      "step": 101800
    },
    {
      "epoch": 5.429866666666666,
      "grad_norm": 0.08422669768333435,
      "learning_rate": 1.6063333333333333e-05,
      "loss": 0.0026,
      "step": 101810
    },
    {
      "epoch": 5.4304,
      "grad_norm": 0.33689606189727783,
      "learning_rate": 1.606e-05,
      "loss": 0.0029,
      "step": 101820
    },
    {
      "epoch": 5.430933333333333,
      "grad_norm": 0.19652903079986572,
      "learning_rate": 1.6056666666666668e-05,
      "loss": 0.0027,
      "step": 101830
    },
    {
      "epoch": 5.431466666666667,
      "grad_norm": 0.08422502875328064,
      "learning_rate": 1.6053333333333334e-05,
      "loss": 0.0054,
      "step": 101840
    },
    {
      "epoch": 5.432,
      "grad_norm": 0.1403798907995224,
      "learning_rate": 1.605e-05,
      "loss": 0.0021,
      "step": 101850
    },
    {
      "epoch": 5.432533333333334,
      "grad_norm": 0.25268346071243286,
      "learning_rate": 1.6046666666666667e-05,
      "loss": 0.0026,
      "step": 101860
    },
    {
      "epoch": 5.433066666666667,
      "grad_norm": 0.028075972571969032,
      "learning_rate": 1.6043333333333336e-05,
      "loss": 0.0031,
      "step": 101870
    },
    {
      "epoch": 5.4336,
      "grad_norm": 0.5067621469497681,
      "learning_rate": 1.604e-05,
      "loss": 0.0029,
      "step": 101880
    },
    {
      "epoch": 5.4341333333333335,
      "grad_norm": 0.028075354173779488,
      "learning_rate": 1.6036666666666665e-05,
      "loss": 0.0035,
      "step": 101890
    },
    {
      "epoch": 5.434666666666667,
      "grad_norm": 0.0842319205403328,
      "learning_rate": 1.6033333333333335e-05,
      "loss": 0.0029,
      "step": 101900
    },
    {
      "epoch": 5.4352,
      "grad_norm": 1.004608154296875,
      "learning_rate": 1.603e-05,
      "loss": 0.002,
      "step": 101910
    },
    {
      "epoch": 5.435733333333333,
      "grad_norm": 0.1456255465745926,
      "learning_rate": 1.6026666666666667e-05,
      "loss": 0.0025,
      "step": 101920
    },
    {
      "epoch": 5.436266666666667,
      "grad_norm": 0.16845062375068665,
      "learning_rate": 1.6023333333333333e-05,
      "loss": 0.0044,
      "step": 101930
    },
    {
      "epoch": 5.4368,
      "grad_norm": 0.14037880301475525,
      "learning_rate": 1.6020000000000002e-05,
      "loss": 0.0015,
      "step": 101940
    },
    {
      "epoch": 5.437333333333333,
      "grad_norm": 0.1403832584619522,
      "learning_rate": 1.601666666666667e-05,
      "loss": 0.0034,
      "step": 101950
    },
    {
      "epoch": 5.437866666666666,
      "grad_norm": 0.02807587943971157,
      "learning_rate": 1.601333333333333e-05,
      "loss": 0.0023,
      "step": 101960
    },
    {
      "epoch": 5.4384,
      "grad_norm": 0.056149039417505264,
      "learning_rate": 1.601e-05,
      "loss": 0.0024,
      "step": 101970
    },
    {
      "epoch": 5.438933333333333,
      "grad_norm": 0.364994078874588,
      "learning_rate": 1.6006666666666667e-05,
      "loss": 0.0023,
      "step": 101980
    },
    {
      "epoch": 5.439466666666666,
      "grad_norm": 0.2526903748512268,
      "learning_rate": 1.6003333333333333e-05,
      "loss": 0.0021,
      "step": 101990
    },
    {
      "epoch": 5.44,
      "grad_norm": 0.028076421469449997,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.0017,
      "step": 102000
    },
    {
      "epoch": 5.440533333333334,
      "grad_norm": 0.5334281325340271,
      "learning_rate": 1.599666666666667e-05,
      "loss": 0.0018,
      "step": 102010
    },
    {
      "epoch": 5.441066666666667,
      "grad_norm": 0.05615095794200897,
      "learning_rate": 1.5993333333333335e-05,
      "loss": 0.0018,
      "step": 102020
    },
    {
      "epoch": 5.4416,
      "grad_norm": 0.056150831282138824,
      "learning_rate": 1.599e-05,
      "loss": 0.0039,
      "step": 102030
    },
    {
      "epoch": 5.4421333333333335,
      "grad_norm": 0.4491915702819824,
      "learning_rate": 1.5986666666666667e-05,
      "loss": 0.0024,
      "step": 102040
    },
    {
      "epoch": 5.442666666666667,
      "grad_norm": 0.028075575828552246,
      "learning_rate": 1.5983333333333333e-05,
      "loss": 0.0035,
      "step": 102050
    },
    {
      "epoch": 5.4432,
      "grad_norm": 0.11230254918336868,
      "learning_rate": 1.598e-05,
      "loss": 0.0027,
      "step": 102060
    },
    {
      "epoch": 5.443733333333333,
      "grad_norm": 0.05615290254354477,
      "learning_rate": 1.597666666666667e-05,
      "loss": 0.004,
      "step": 102070
    },
    {
      "epoch": 5.444266666666667,
      "grad_norm": 0.14038242399692535,
      "learning_rate": 1.5973333333333335e-05,
      "loss": 0.0025,
      "step": 102080
    },
    {
      "epoch": 5.4448,
      "grad_norm": 0.4772493243217468,
      "learning_rate": 1.597e-05,
      "loss": 0.0025,
      "step": 102090
    },
    {
      "epoch": 5.445333333333333,
      "grad_norm": 0.08422872424125671,
      "learning_rate": 1.5966666666666667e-05,
      "loss": 0.0019,
      "step": 102100
    },
    {
      "epoch": 5.445866666666666,
      "grad_norm": 0.028076132759451866,
      "learning_rate": 1.5963333333333334e-05,
      "loss": 0.0032,
      "step": 102110
    },
    {
      "epoch": 5.4464,
      "grad_norm": 0.36207354068756104,
      "learning_rate": 1.596e-05,
      "loss": 0.0025,
      "step": 102120
    },
    {
      "epoch": 5.446933333333333,
      "grad_norm": 0.11230532079935074,
      "learning_rate": 1.5956666666666666e-05,
      "loss": 0.0028,
      "step": 102130
    },
    {
      "epoch": 5.447466666666667,
      "grad_norm": 0.36498719453811646,
      "learning_rate": 1.5953333333333335e-05,
      "loss": 0.0032,
      "step": 102140
    },
    {
      "epoch": 5.448,
      "grad_norm": 0.2807493507862091,
      "learning_rate": 1.595e-05,
      "loss": 0.0043,
      "step": 102150
    },
    {
      "epoch": 5.448533333333334,
      "grad_norm": 0.08422673493623734,
      "learning_rate": 1.5946666666666668e-05,
      "loss": 0.0041,
      "step": 102160
    },
    {
      "epoch": 5.449066666666667,
      "grad_norm": 0.028074171394109726,
      "learning_rate": 1.5943333333333334e-05,
      "loss": 0.0035,
      "step": 102170
    },
    {
      "epoch": 5.4496,
      "grad_norm": 0.19652095437049866,
      "learning_rate": 1.594e-05,
      "loss": 0.005,
      "step": 102180
    },
    {
      "epoch": 5.4501333333333335,
      "grad_norm": 0.05614927038550377,
      "learning_rate": 1.5936666666666666e-05,
      "loss": 0.0017,
      "step": 102190
    },
    {
      "epoch": 5.450666666666667,
      "grad_norm": 0.028074992820620537,
      "learning_rate": 1.5933333333333332e-05,
      "loss": 0.0019,
      "step": 102200
    },
    {
      "epoch": 5.4512,
      "grad_norm": 0.05615096539258957,
      "learning_rate": 1.593e-05,
      "loss": 0.0021,
      "step": 102210
    },
    {
      "epoch": 5.451733333333333,
      "grad_norm": 0.05614812299609184,
      "learning_rate": 1.5926666666666668e-05,
      "loss": 0.0027,
      "step": 102220
    },
    {
      "epoch": 5.452266666666667,
      "grad_norm": 0.1403733491897583,
      "learning_rate": 1.5923333333333334e-05,
      "loss": 0.0019,
      "step": 102230
    },
    {
      "epoch": 5.4528,
      "grad_norm": 0.16845610737800598,
      "learning_rate": 1.592e-05,
      "loss": 0.0035,
      "step": 102240
    },
    {
      "epoch": 5.453333333333333,
      "grad_norm": 0.028076479211449623,
      "learning_rate": 1.591666666666667e-05,
      "loss": 0.0035,
      "step": 102250
    },
    {
      "epoch": 5.453866666666666,
      "grad_norm": 0.02807403728365898,
      "learning_rate": 1.5913333333333332e-05,
      "loss": 0.0032,
      "step": 102260
    },
    {
      "epoch": 5.4544,
      "grad_norm": 0.19652625918388367,
      "learning_rate": 1.591e-05,
      "loss": 0.0023,
      "step": 102270
    },
    {
      "epoch": 5.454933333333333,
      "grad_norm": 0.05614858865737915,
      "learning_rate": 1.5906666666666668e-05,
      "loss": 0.002,
      "step": 102280
    },
    {
      "epoch": 5.455466666666666,
      "grad_norm": 2.1001911321150146e-09,
      "learning_rate": 1.5903333333333334e-05,
      "loss": 0.0018,
      "step": 102290
    },
    {
      "epoch": 5.456,
      "grad_norm": 0.2526806890964508,
      "learning_rate": 1.59e-05,
      "loss": 0.0023,
      "step": 102300
    },
    {
      "epoch": 5.456533333333334,
      "grad_norm": 0.0280744768679142,
      "learning_rate": 1.5896666666666666e-05,
      "loss": 0.0014,
      "step": 102310
    },
    {
      "epoch": 5.457066666666667,
      "grad_norm": 0.028074612841010094,
      "learning_rate": 1.5893333333333336e-05,
      "loss": 0.0026,
      "step": 102320
    },
    {
      "epoch": 5.4576,
      "grad_norm": 0.02807386964559555,
      "learning_rate": 1.5890000000000002e-05,
      "loss": 0.0027,
      "step": 102330
    },
    {
      "epoch": 5.4581333333333335,
      "grad_norm": 0.08422113209962845,
      "learning_rate": 1.5886666666666665e-05,
      "loss": 0.0015,
      "step": 102340
    },
    {
      "epoch": 5.458666666666667,
      "grad_norm": 1.322796649994018e-09,
      "learning_rate": 1.5883333333333334e-05,
      "loss": 0.0019,
      "step": 102350
    },
    {
      "epoch": 5.4592,
      "grad_norm": 0.3088187873363495,
      "learning_rate": 1.588e-05,
      "loss": 0.0014,
      "step": 102360
    },
    {
      "epoch": 5.459733333333333,
      "grad_norm": 0.252663791179657,
      "learning_rate": 1.5876666666666667e-05,
      "loss": 0.0023,
      "step": 102370
    },
    {
      "epoch": 5.460266666666667,
      "grad_norm": 0.19651976227760315,
      "learning_rate": 1.5873333333333336e-05,
      "loss": 0.0023,
      "step": 102380
    },
    {
      "epoch": 5.4608,
      "grad_norm": 0.08422700315713882,
      "learning_rate": 1.5870000000000002e-05,
      "loss": 0.0028,
      "step": 102390
    },
    {
      "epoch": 5.461333333333333,
      "grad_norm": 0.22459213435649872,
      "learning_rate": 1.586666666666667e-05,
      "loss": 0.002,
      "step": 102400
    },
    {
      "epoch": 5.461866666666666,
      "grad_norm": 0.22458887100219727,
      "learning_rate": 1.5863333333333334e-05,
      "loss": 0.002,
      "step": 102410
    },
    {
      "epoch": 5.4624,
      "grad_norm": 0.25265955924987793,
      "learning_rate": 1.586e-05,
      "loss": 0.0026,
      "step": 102420
    },
    {
      "epoch": 5.462933333333333,
      "grad_norm": 0.054202787578105927,
      "learning_rate": 1.5856666666666667e-05,
      "loss": 0.0031,
      "step": 102430
    },
    {
      "epoch": 5.463466666666667,
      "grad_norm": 0.1122979074716568,
      "learning_rate": 1.5853333333333333e-05,
      "loss": 0.0023,
      "step": 102440
    },
    {
      "epoch": 5.464,
      "grad_norm": 0.112295962870121,
      "learning_rate": 1.5850000000000002e-05,
      "loss": 0.0024,
      "step": 102450
    },
    {
      "epoch": 5.464533333333334,
      "grad_norm": 0.11229440569877625,
      "learning_rate": 1.584666666666667e-05,
      "loss": 0.0021,
      "step": 102460
    },
    {
      "epoch": 5.465066666666667,
      "grad_norm": 0.2245924025774002,
      "learning_rate": 1.5843333333333335e-05,
      "loss": 0.0026,
      "step": 102470
    },
    {
      "epoch": 5.4656,
      "grad_norm": 0.2245916724205017,
      "learning_rate": 1.584e-05,
      "loss": 0.0028,
      "step": 102480
    },
    {
      "epoch": 5.4661333333333335,
      "grad_norm": 0.19651657342910767,
      "learning_rate": 1.5836666666666667e-05,
      "loss": 0.0039,
      "step": 102490
    },
    {
      "epoch": 5.466666666666667,
      "grad_norm": 0.36494016647338867,
      "learning_rate": 1.5833333333333333e-05,
      "loss": 0.0022,
      "step": 102500
    },
    {
      "epoch": 5.4672,
      "grad_norm": 7.853359740295218e-09,
      "learning_rate": 1.583e-05,
      "loss": 0.0047,
      "step": 102510
    },
    {
      "epoch": 5.467733333333333,
      "grad_norm": 0.056149840354919434,
      "learning_rate": 1.582666666666667e-05,
      "loss": 0.003,
      "step": 102520
    },
    {
      "epoch": 5.468266666666667,
      "grad_norm": 0.11229156702756882,
      "learning_rate": 1.5823333333333335e-05,
      "loss": 0.0032,
      "step": 102530
    },
    {
      "epoch": 5.4688,
      "grad_norm": 0.11229733377695084,
      "learning_rate": 1.582e-05,
      "loss": 0.0029,
      "step": 102540
    },
    {
      "epoch": 5.469333333333333,
      "grad_norm": 0.2807508707046509,
      "learning_rate": 1.5816666666666667e-05,
      "loss": 0.0033,
      "step": 102550
    },
    {
      "epoch": 5.469866666666666,
      "grad_norm": 0.19651511311531067,
      "learning_rate": 1.5813333333333333e-05,
      "loss": 0.0031,
      "step": 102560
    },
    {
      "epoch": 5.4704,
      "grad_norm": 2.9753557395650887e-09,
      "learning_rate": 1.581e-05,
      "loss": 0.0032,
      "step": 102570
    },
    {
      "epoch": 5.470933333333333,
      "grad_norm": 3.4087963562967616e-09,
      "learning_rate": 1.5806666666666666e-05,
      "loss": 0.0037,
      "step": 102580
    },
    {
      "epoch": 5.471466666666666,
      "grad_norm": 0.22459541261196136,
      "learning_rate": 1.5803333333333335e-05,
      "loss": 0.0041,
      "step": 102590
    },
    {
      "epoch": 5.4719999999999995,
      "grad_norm": 0.14036504924297333,
      "learning_rate": 1.58e-05,
      "loss": 0.0033,
      "step": 102600
    },
    {
      "epoch": 5.472533333333334,
      "grad_norm": 0.11229345947504044,
      "learning_rate": 1.5796666666666667e-05,
      "loss": 0.0036,
      "step": 102610
    },
    {
      "epoch": 5.473066666666667,
      "grad_norm": 0.08422292768955231,
      "learning_rate": 1.5793333333333333e-05,
      "loss": 0.003,
      "step": 102620
    },
    {
      "epoch": 5.4736,
      "grad_norm": 2.6262534369436707e-09,
      "learning_rate": 1.5790000000000003e-05,
      "loss": 0.0029,
      "step": 102630
    },
    {
      "epoch": 5.4741333333333335,
      "grad_norm": 0.4211511015892029,
      "learning_rate": 1.5786666666666666e-05,
      "loss": 0.0024,
      "step": 102640
    },
    {
      "epoch": 5.474666666666667,
      "grad_norm": 5.238820488528972e-10,
      "learning_rate": 1.5783333333333332e-05,
      "loss": 0.0036,
      "step": 102650
    },
    {
      "epoch": 5.4752,
      "grad_norm": 0.11229266971349716,
      "learning_rate": 1.578e-05,
      "loss": 0.0037,
      "step": 102660
    },
    {
      "epoch": 5.475733333333333,
      "grad_norm": 0.3930043876171112,
      "learning_rate": 1.5776666666666668e-05,
      "loss": 0.0027,
      "step": 102670
    },
    {
      "epoch": 5.476266666666667,
      "grad_norm": 0.028073400259017944,
      "learning_rate": 1.5773333333333334e-05,
      "loss": 0.0028,
      "step": 102680
    },
    {
      "epoch": 5.4768,
      "grad_norm": 0.028074180707335472,
      "learning_rate": 1.577e-05,
      "loss": 0.0019,
      "step": 102690
    },
    {
      "epoch": 5.477333333333333,
      "grad_norm": 0.14036771655082703,
      "learning_rate": 1.576666666666667e-05,
      "loss": 0.0033,
      "step": 102700
    },
    {
      "epoch": 5.477866666666666,
      "grad_norm": 0.11228972673416138,
      "learning_rate": 1.5763333333333332e-05,
      "loss": 0.0024,
      "step": 102710
    },
    {
      "epoch": 5.4784,
      "grad_norm": 0.1122930720448494,
      "learning_rate": 1.5759999999999998e-05,
      "loss": 0.0039,
      "step": 102720
    },
    {
      "epoch": 5.478933333333333,
      "grad_norm": 0.28071826696395874,
      "learning_rate": 1.5756666666666668e-05,
      "loss": 0.002,
      "step": 102730
    },
    {
      "epoch": 5.479466666666666,
      "grad_norm": 0.028073495253920555,
      "learning_rate": 1.5753333333333334e-05,
      "loss": 0.0034,
      "step": 102740
    },
    {
      "epoch": 5.48,
      "grad_norm": 0.22459302842617035,
      "learning_rate": 1.575e-05,
      "loss": 0.0027,
      "step": 102750
    },
    {
      "epoch": 5.480533333333334,
      "grad_norm": 0.08422145247459412,
      "learning_rate": 1.574666666666667e-05,
      "loss": 0.0034,
      "step": 102760
    },
    {
      "epoch": 5.481066666666667,
      "grad_norm": 0.05614553391933441,
      "learning_rate": 1.5743333333333336e-05,
      "loss": 0.0024,
      "step": 102770
    },
    {
      "epoch": 5.4816,
      "grad_norm": 0.056145619601011276,
      "learning_rate": 1.5740000000000002e-05,
      "loss": 0.0033,
      "step": 102780
    },
    {
      "epoch": 5.4821333333333335,
      "grad_norm": 0.19651105999946594,
      "learning_rate": 1.5736666666666668e-05,
      "loss": 0.003,
      "step": 102790
    },
    {
      "epoch": 5.482666666666667,
      "grad_norm": 0.028073081746697426,
      "learning_rate": 1.5733333333333334e-05,
      "loss": 0.0023,
      "step": 102800
    },
    {
      "epoch": 5.4832,
      "grad_norm": 0.06818675994873047,
      "learning_rate": 1.573e-05,
      "loss": 0.0019,
      "step": 102810
    },
    {
      "epoch": 5.483733333333333,
      "grad_norm": 0.1965102255344391,
      "learning_rate": 1.5726666666666666e-05,
      "loss": 0.0022,
      "step": 102820
    },
    {
      "epoch": 5.484266666666667,
      "grad_norm": 0.05614656209945679,
      "learning_rate": 1.5723333333333336e-05,
      "loss": 0.0038,
      "step": 102830
    },
    {
      "epoch": 5.4848,
      "grad_norm": 0.028074294328689575,
      "learning_rate": 1.5720000000000002e-05,
      "loss": 0.0018,
      "step": 102840
    },
    {
      "epoch": 5.485333333333333,
      "grad_norm": 0.11602486670017242,
      "learning_rate": 1.5716666666666668e-05,
      "loss": 0.0023,
      "step": 102850
    },
    {
      "epoch": 5.4858666666666664,
      "grad_norm": 3.8384366796151426e-09,
      "learning_rate": 1.5713333333333334e-05,
      "loss": 0.0027,
      "step": 102860
    },
    {
      "epoch": 5.4864,
      "grad_norm": 0.19650909304618835,
      "learning_rate": 1.571e-05,
      "loss": 0.0018,
      "step": 102870
    },
    {
      "epoch": 5.486933333333333,
      "grad_norm": 0.14037123322486877,
      "learning_rate": 1.5706666666666666e-05,
      "loss": 0.0025,
      "step": 102880
    },
    {
      "epoch": 5.487466666666666,
      "grad_norm": 0.028072919696569443,
      "learning_rate": 1.5703333333333333e-05,
      "loss": 0.0024,
      "step": 102890
    },
    {
      "epoch": 5.4879999999999995,
      "grad_norm": 0.0986286997795105,
      "learning_rate": 1.5700000000000002e-05,
      "loss": 0.0029,
      "step": 102900
    },
    {
      "epoch": 5.488533333333334,
      "grad_norm": 0.11229438334703445,
      "learning_rate": 1.5696666666666668e-05,
      "loss": 0.0024,
      "step": 102910
    },
    {
      "epoch": 5.489066666666667,
      "grad_norm": 0.08422233164310455,
      "learning_rate": 1.5693333333333334e-05,
      "loss": 0.0023,
      "step": 102920
    },
    {
      "epoch": 5.4896,
      "grad_norm": 0.2807522118091583,
      "learning_rate": 1.569e-05,
      "loss": 0.002,
      "step": 102930
    },
    {
      "epoch": 5.4901333333333335,
      "grad_norm": 0.05614761635661125,
      "learning_rate": 1.5686666666666667e-05,
      "loss": 0.0033,
      "step": 102940
    },
    {
      "epoch": 5.490666666666667,
      "grad_norm": 0.22458568215370178,
      "learning_rate": 1.5683333333333333e-05,
      "loss": 0.0024,
      "step": 102950
    },
    {
      "epoch": 5.4912,
      "grad_norm": 0.2526618242263794,
      "learning_rate": 1.568e-05,
      "loss": 0.0032,
      "step": 102960
    },
    {
      "epoch": 5.491733333333333,
      "grad_norm": 0.05614727735519409,
      "learning_rate": 1.567666666666667e-05,
      "loss": 0.0043,
      "step": 102970
    },
    {
      "epoch": 5.492266666666667,
      "grad_norm": 0.11229001730680466,
      "learning_rate": 1.5673333333333335e-05,
      "loss": 0.0027,
      "step": 102980
    },
    {
      "epoch": 5.4928,
      "grad_norm": 0.5895505547523499,
      "learning_rate": 1.567e-05,
      "loss": 0.0035,
      "step": 102990
    },
    {
      "epoch": 5.493333333333333,
      "grad_norm": 7.913039667961641e-10,
      "learning_rate": 1.5666666666666667e-05,
      "loss": 0.0028,
      "step": 103000
    },
    {
      "epoch": 5.4938666666666665,
      "grad_norm": 0.22457313537597656,
      "learning_rate": 1.5663333333333336e-05,
      "loss": 0.0024,
      "step": 103010
    },
    {
      "epoch": 5.4944,
      "grad_norm": 0.2807348668575287,
      "learning_rate": 1.566e-05,
      "loss": 0.0031,
      "step": 103020
    },
    {
      "epoch": 5.494933333333333,
      "grad_norm": 0.1684437245130539,
      "learning_rate": 1.5656666666666665e-05,
      "loss": 0.0025,
      "step": 103030
    },
    {
      "epoch": 5.495466666666666,
      "grad_norm": 0.2526639997959137,
      "learning_rate": 1.5653333333333335e-05,
      "loss": 0.0028,
      "step": 103040
    },
    {
      "epoch": 5.496,
      "grad_norm": 0.42108815908432007,
      "learning_rate": 1.565e-05,
      "loss": 0.0033,
      "step": 103050
    },
    {
      "epoch": 5.496533333333334,
      "grad_norm": 0.14035873115062714,
      "learning_rate": 1.5646666666666667e-05,
      "loss": 0.0035,
      "step": 103060
    },
    {
      "epoch": 5.497066666666667,
      "grad_norm": 0.11228464543819427,
      "learning_rate": 1.5643333333333333e-05,
      "loss": 0.0039,
      "step": 103070
    },
    {
      "epoch": 5.4976,
      "grad_norm": 0.028073051944375038,
      "learning_rate": 1.5640000000000003e-05,
      "loss": 0.0023,
      "step": 103080
    },
    {
      "epoch": 5.4981333333333335,
      "grad_norm": 0.11229148507118225,
      "learning_rate": 1.5636666666666665e-05,
      "loss": 0.0034,
      "step": 103090
    },
    {
      "epoch": 5.498666666666667,
      "grad_norm": 0.3087904453277588,
      "learning_rate": 1.563333333333333e-05,
      "loss": 0.0031,
      "step": 103100
    },
    {
      "epoch": 5.4992,
      "grad_norm": 0.14036037027835846,
      "learning_rate": 1.563e-05,
      "loss": 0.0034,
      "step": 103110
    },
    {
      "epoch": 5.499733333333333,
      "grad_norm": 0.3368852138519287,
      "learning_rate": 1.5626666666666667e-05,
      "loss": 0.0028,
      "step": 103120
    },
    {
      "epoch": 5.500266666666667,
      "grad_norm": 1.3468619585037231,
      "learning_rate": 1.5623333333333333e-05,
      "loss": 0.0035,
      "step": 103130
    },
    {
      "epoch": 5.5008,
      "grad_norm": 0.22456581890583038,
      "learning_rate": 1.5620000000000003e-05,
      "loss": 0.002,
      "step": 103140
    },
    {
      "epoch": 5.501333333333333,
      "grad_norm": 0.16843374073505402,
      "learning_rate": 1.561666666666667e-05,
      "loss": 0.0032,
      "step": 103150
    },
    {
      "epoch": 5.5018666666666665,
      "grad_norm": 0.08421731740236282,
      "learning_rate": 1.5613333333333335e-05,
      "loss": 0.0018,
      "step": 103160
    },
    {
      "epoch": 5.5024,
      "grad_norm": 0.08421492576599121,
      "learning_rate": 1.561e-05,
      "loss": 0.0028,
      "step": 103170
    },
    {
      "epoch": 5.502933333333333,
      "grad_norm": 0.11228232085704803,
      "learning_rate": 1.5606666666666667e-05,
      "loss": 0.0022,
      "step": 103180
    },
    {
      "epoch": 5.503466666666666,
      "grad_norm": 0.22457489371299744,
      "learning_rate": 1.5603333333333334e-05,
      "loss": 0.0042,
      "step": 103190
    },
    {
      "epoch": 5.504,
      "grad_norm": 0.14036118984222412,
      "learning_rate": 1.56e-05,
      "loss": 0.0026,
      "step": 103200
    },
    {
      "epoch": 5.504533333333333,
      "grad_norm": 0.1403542011976242,
      "learning_rate": 1.559666666666667e-05,
      "loss": 0.0038,
      "step": 103210
    },
    {
      "epoch": 5.505066666666667,
      "grad_norm": 0.19649633765220642,
      "learning_rate": 1.5593333333333335e-05,
      "loss": 0.004,
      "step": 103220
    },
    {
      "epoch": 5.5056,
      "grad_norm": 0.5614105463027954,
      "learning_rate": 1.559e-05,
      "loss": 0.0027,
      "step": 103230
    },
    {
      "epoch": 5.5061333333333335,
      "grad_norm": 0.5333411693572998,
      "learning_rate": 1.5586666666666668e-05,
      "loss": 0.0023,
      "step": 103240
    },
    {
      "epoch": 5.506666666666667,
      "grad_norm": 0.16843317449092865,
      "learning_rate": 1.5583333333333334e-05,
      "loss": 0.0029,
      "step": 103250
    },
    {
      "epoch": 5.5072,
      "grad_norm": 0.19650284945964813,
      "learning_rate": 1.558e-05,
      "loss": 0.0029,
      "step": 103260
    },
    {
      "epoch": 5.507733333333333,
      "grad_norm": 0.224565789103508,
      "learning_rate": 1.5576666666666666e-05,
      "loss": 0.0023,
      "step": 103270
    },
    {
      "epoch": 5.508266666666667,
      "grad_norm": 0.028071166947484016,
      "learning_rate": 1.5573333333333336e-05,
      "loss": 0.002,
      "step": 103280
    },
    {
      "epoch": 5.5088,
      "grad_norm": 0.22457143664360046,
      "learning_rate": 1.5570000000000002e-05,
      "loss": 0.0035,
      "step": 103290
    },
    {
      "epoch": 5.509333333333333,
      "grad_norm": 0.028071362525224686,
      "learning_rate": 1.5566666666666668e-05,
      "loss": 0.0029,
      "step": 103300
    },
    {
      "epoch": 5.5098666666666665,
      "grad_norm": 0.056141339242458344,
      "learning_rate": 1.5563333333333334e-05,
      "loss": 0.0022,
      "step": 103310
    },
    {
      "epoch": 5.5104,
      "grad_norm": 0.11228320002555847,
      "learning_rate": 1.556e-05,
      "loss": 0.0023,
      "step": 103320
    },
    {
      "epoch": 5.510933333333333,
      "grad_norm": 0.05614229291677475,
      "learning_rate": 1.5556666666666666e-05,
      "loss": 0.0027,
      "step": 103330
    },
    {
      "epoch": 5.511466666666666,
      "grad_norm": 0.2807123064994812,
      "learning_rate": 1.5553333333333332e-05,
      "loss": 0.0032,
      "step": 103340
    },
    {
      "epoch": 5.5120000000000005,
      "grad_norm": 0.028071973472833633,
      "learning_rate": 1.5550000000000002e-05,
      "loss": 0.0027,
      "step": 103350
    },
    {
      "epoch": 5.512533333333334,
      "grad_norm": 2.1342734246587725e-09,
      "learning_rate": 1.5546666666666668e-05,
      "loss": 0.0027,
      "step": 103360
    },
    {
      "epoch": 5.513066666666667,
      "grad_norm": 0.16842785477638245,
      "learning_rate": 1.5543333333333334e-05,
      "loss": 0.0022,
      "step": 103370
    },
    {
      "epoch": 5.5136,
      "grad_norm": 0.05614246428012848,
      "learning_rate": 1.554e-05,
      "loss": 0.0026,
      "step": 103380
    },
    {
      "epoch": 5.5141333333333336,
      "grad_norm": 0.25264137983322144,
      "learning_rate": 1.5536666666666666e-05,
      "loss": 0.0026,
      "step": 103390
    },
    {
      "epoch": 5.514666666666667,
      "grad_norm": 0.1964954137802124,
      "learning_rate": 1.5533333333333333e-05,
      "loss": 0.0024,
      "step": 103400
    },
    {
      "epoch": 5.5152,
      "grad_norm": 0.36493316292762756,
      "learning_rate": 1.553e-05,
      "loss": 0.0019,
      "step": 103410
    },
    {
      "epoch": 5.515733333333333,
      "grad_norm": 0.11228224635124207,
      "learning_rate": 1.5526666666666668e-05,
      "loss": 0.0021,
      "step": 103420
    },
    {
      "epoch": 5.516266666666667,
      "grad_norm": 2.1019570528579834e-09,
      "learning_rate": 1.5523333333333334e-05,
      "loss": 0.0021,
      "step": 103430
    },
    {
      "epoch": 5.5168,
      "grad_norm": 2.25524243724351e-09,
      "learning_rate": 1.552e-05,
      "loss": 0.0031,
      "step": 103440
    },
    {
      "epoch": 5.517333333333333,
      "grad_norm": 0.05614596977829933,
      "learning_rate": 1.5516666666666667e-05,
      "loss": 0.0043,
      "step": 103450
    },
    {
      "epoch": 5.5178666666666665,
      "grad_norm": 0.05614534765481949,
      "learning_rate": 1.5513333333333336e-05,
      "loss": 0.002,
      "step": 103460
    },
    {
      "epoch": 5.5184,
      "grad_norm": 8.489962621816005e-10,
      "learning_rate": 1.551e-05,
      "loss": 0.0037,
      "step": 103470
    },
    {
      "epoch": 5.518933333333333,
      "grad_norm": 0.7970273494720459,
      "learning_rate": 1.5506666666666665e-05,
      "loss": 0.0025,
      "step": 103480
    },
    {
      "epoch": 5.519466666666666,
      "grad_norm": 0.08421129733324051,
      "learning_rate": 1.5503333333333335e-05,
      "loss": 0.0027,
      "step": 103490
    },
    {
      "epoch": 5.52,
      "grad_norm": 0.25263839960098267,
      "learning_rate": 1.55e-05,
      "loss": 0.0041,
      "step": 103500
    },
    {
      "epoch": 5.520533333333333,
      "grad_norm": 0.05614204332232475,
      "learning_rate": 1.5496666666666667e-05,
      "loss": 0.0012,
      "step": 103510
    },
    {
      "epoch": 5.521066666666667,
      "grad_norm": 0.017771167680621147,
      "learning_rate": 1.5493333333333336e-05,
      "loss": 0.003,
      "step": 103520
    },
    {
      "epoch": 5.5216,
      "grad_norm": 0.14035457372665405,
      "learning_rate": 1.5490000000000002e-05,
      "loss": 0.0021,
      "step": 103530
    },
    {
      "epoch": 5.522133333333334,
      "grad_norm": 0.08421299606561661,
      "learning_rate": 1.548666666666667e-05,
      "loss": 0.003,
      "step": 103540
    },
    {
      "epoch": 5.522666666666667,
      "grad_norm": 0.05614352598786354,
      "learning_rate": 1.548333333333333e-05,
      "loss": 0.003,
      "step": 103550
    },
    {
      "epoch": 5.5232,
      "grad_norm": 0.22457291185855865,
      "learning_rate": 1.548e-05,
      "loss": 0.0028,
      "step": 103560
    },
    {
      "epoch": 5.523733333333333,
      "grad_norm": 0.028071140870451927,
      "learning_rate": 1.5476666666666667e-05,
      "loss": 0.0024,
      "step": 103570
    },
    {
      "epoch": 5.524266666666667,
      "grad_norm": 0.28191182017326355,
      "learning_rate": 1.5473333333333333e-05,
      "loss": 0.0028,
      "step": 103580
    },
    {
      "epoch": 5.5248,
      "grad_norm": 0.02807152085006237,
      "learning_rate": 1.5470000000000003e-05,
      "loss": 0.003,
      "step": 103590
    },
    {
      "epoch": 5.525333333333333,
      "grad_norm": 0.028071613982319832,
      "learning_rate": 1.546666666666667e-05,
      "loss": 0.0033,
      "step": 103600
    },
    {
      "epoch": 5.5258666666666665,
      "grad_norm": 0.05614223703742027,
      "learning_rate": 1.5463333333333335e-05,
      "loss": 0.0027,
      "step": 103610
    },
    {
      "epoch": 5.5264,
      "grad_norm": 0.25263354182243347,
      "learning_rate": 1.546e-05,
      "loss": 0.0026,
      "step": 103620
    },
    {
      "epoch": 5.526933333333333,
      "grad_norm": 0.11228129267692566,
      "learning_rate": 1.5456666666666667e-05,
      "loss": 0.0034,
      "step": 103630
    },
    {
      "epoch": 5.527466666666666,
      "grad_norm": 0.08851028978824615,
      "learning_rate": 1.5453333333333333e-05,
      "loss": 0.0021,
      "step": 103640
    },
    {
      "epoch": 5.5280000000000005,
      "grad_norm": 0.19058722257614136,
      "learning_rate": 1.545e-05,
      "loss": 0.0024,
      "step": 103650
    },
    {
      "epoch": 5.528533333333334,
      "grad_norm": 0.11228425800800323,
      "learning_rate": 1.544666666666667e-05,
      "loss": 0.0032,
      "step": 103660
    },
    {
      "epoch": 5.529066666666667,
      "grad_norm": 0.08421067893505096,
      "learning_rate": 1.5443333333333335e-05,
      "loss": 0.0036,
      "step": 103670
    },
    {
      "epoch": 5.5296,
      "grad_norm": 0.6066651344299316,
      "learning_rate": 1.544e-05,
      "loss": 0.0024,
      "step": 103680
    },
    {
      "epoch": 5.530133333333334,
      "grad_norm": 0.14035730063915253,
      "learning_rate": 1.5436666666666667e-05,
      "loss": 0.0034,
      "step": 103690
    },
    {
      "epoch": 5.530666666666667,
      "grad_norm": 0.08298556506633759,
      "learning_rate": 1.5433333333333334e-05,
      "loss": 0.0022,
      "step": 103700
    },
    {
      "epoch": 5.5312,
      "grad_norm": 0.05614132806658745,
      "learning_rate": 1.543e-05,
      "loss": 0.0039,
      "step": 103710
    },
    {
      "epoch": 5.531733333333333,
      "grad_norm": 0.05614317208528519,
      "learning_rate": 1.5426666666666666e-05,
      "loss": 0.0024,
      "step": 103720
    },
    {
      "epoch": 5.532266666666667,
      "grad_norm": 0.11242426931858063,
      "learning_rate": 1.5423333333333335e-05,
      "loss": 0.0036,
      "step": 103730
    },
    {
      "epoch": 5.5328,
      "grad_norm": 0.1964927613735199,
      "learning_rate": 1.542e-05,
      "loss": 0.0036,
      "step": 103740
    },
    {
      "epoch": 5.533333333333333,
      "grad_norm": 0.4210559129714966,
      "learning_rate": 1.5416666666666668e-05,
      "loss": 0.003,
      "step": 103750
    },
    {
      "epoch": 5.5338666666666665,
      "grad_norm": 0.2245684266090393,
      "learning_rate": 1.5413333333333334e-05,
      "loss": 0.0027,
      "step": 103760
    },
    {
      "epoch": 5.5344,
      "grad_norm": 0.1403573602437973,
      "learning_rate": 1.541e-05,
      "loss": 0.0022,
      "step": 103770
    },
    {
      "epoch": 5.534933333333333,
      "grad_norm": 0.11228697001934052,
      "learning_rate": 1.5406666666666666e-05,
      "loss": 0.002,
      "step": 103780
    },
    {
      "epoch": 5.535466666666666,
      "grad_norm": 0.08421128243207932,
      "learning_rate": 1.5403333333333332e-05,
      "loss": 0.0027,
      "step": 103790
    },
    {
      "epoch": 5.536,
      "grad_norm": 0.2245771735906601,
      "learning_rate": 1.54e-05,
      "loss": 0.0024,
      "step": 103800
    },
    {
      "epoch": 5.536533333333333,
      "grad_norm": 0.16843168437480927,
      "learning_rate": 1.5396666666666668e-05,
      "loss": 0.0033,
      "step": 103810
    },
    {
      "epoch": 5.537066666666667,
      "grad_norm": 0.11227957904338837,
      "learning_rate": 1.5393333333333334e-05,
      "loss": 0.0028,
      "step": 103820
    },
    {
      "epoch": 5.5376,
      "grad_norm": 0.028070352971553802,
      "learning_rate": 1.539e-05,
      "loss": 0.0038,
      "step": 103830
    },
    {
      "epoch": 5.538133333333334,
      "grad_norm": 0.14035069942474365,
      "learning_rate": 1.538666666666667e-05,
      "loss": 0.0022,
      "step": 103840
    },
    {
      "epoch": 5.538666666666667,
      "grad_norm": 0.14035123586654663,
      "learning_rate": 1.5383333333333332e-05,
      "loss": 0.0019,
      "step": 103850
    },
    {
      "epoch": 5.5392,
      "grad_norm": 0.5614165663719177,
      "learning_rate": 1.538e-05,
      "loss": 0.0024,
      "step": 103860
    },
    {
      "epoch": 5.539733333333333,
      "grad_norm": 0.056141793727874756,
      "learning_rate": 1.5376666666666668e-05,
      "loss": 0.0021,
      "step": 103870
    },
    {
      "epoch": 5.540266666666667,
      "grad_norm": 0.11244913190603256,
      "learning_rate": 1.5373333333333334e-05,
      "loss": 0.0027,
      "step": 103880
    },
    {
      "epoch": 5.5408,
      "grad_norm": 0.16841858625411987,
      "learning_rate": 1.537e-05,
      "loss": 0.0031,
      "step": 103890
    },
    {
      "epoch": 5.541333333333333,
      "grad_norm": 1.6656748202947824e-09,
      "learning_rate": 1.536666666666667e-05,
      "loss": 0.0025,
      "step": 103900
    },
    {
      "epoch": 5.5418666666666665,
      "grad_norm": 0.028069762513041496,
      "learning_rate": 1.5363333333333336e-05,
      "loss": 0.0028,
      "step": 103910
    },
    {
      "epoch": 5.5424,
      "grad_norm": 0.22455887496471405,
      "learning_rate": 1.536e-05,
      "loss": 0.0024,
      "step": 103920
    },
    {
      "epoch": 5.542933333333333,
      "grad_norm": 0.16842420399188995,
      "learning_rate": 1.5356666666666665e-05,
      "loss": 0.0023,
      "step": 103930
    },
    {
      "epoch": 5.543466666666666,
      "grad_norm": 0.25262650847435,
      "learning_rate": 1.5353333333333334e-05,
      "loss": 0.0022,
      "step": 103940
    },
    {
      "epoch": 5.5440000000000005,
      "grad_norm": 0.140353724360466,
      "learning_rate": 1.535e-05,
      "loss": 0.004,
      "step": 103950
    },
    {
      "epoch": 5.544533333333334,
      "grad_norm": 0.02807062491774559,
      "learning_rate": 1.5346666666666667e-05,
      "loss": 0.0033,
      "step": 103960
    },
    {
      "epoch": 5.545066666666667,
      "grad_norm": 0.14378021657466888,
      "learning_rate": 1.5343333333333336e-05,
      "loss": 0.002,
      "step": 103970
    },
    {
      "epoch": 5.5456,
      "grad_norm": 0.20073291659355164,
      "learning_rate": 1.5340000000000002e-05,
      "loss": 0.0028,
      "step": 103980
    },
    {
      "epoch": 5.546133333333334,
      "grad_norm": 0.02807033807039261,
      "learning_rate": 1.533666666666667e-05,
      "loss": 0.0046,
      "step": 103990
    },
    {
      "epoch": 5.546666666666667,
      "grad_norm": 0.1122819036245346,
      "learning_rate": 1.5333333333333334e-05,
      "loss": 0.0031,
      "step": 104000
    },
    {
      "epoch": 5.5472,
      "grad_norm": 0.16842234134674072,
      "learning_rate": 1.533e-05,
      "loss": 0.0019,
      "step": 104010
    },
    {
      "epoch": 5.547733333333333,
      "grad_norm": 0.22776105999946594,
      "learning_rate": 1.5326666666666667e-05,
      "loss": 0.0016,
      "step": 104020
    },
    {
      "epoch": 5.548266666666667,
      "grad_norm": 0.22457942366600037,
      "learning_rate": 1.5323333333333333e-05,
      "loss": 0.0022,
      "step": 104030
    },
    {
      "epoch": 5.5488,
      "grad_norm": 0.22456589341163635,
      "learning_rate": 1.5320000000000002e-05,
      "loss": 0.0023,
      "step": 104040
    },
    {
      "epoch": 5.549333333333333,
      "grad_norm": 2.98246072283348e-09,
      "learning_rate": 1.531666666666667e-05,
      "loss": 0.0037,
      "step": 104050
    },
    {
      "epoch": 5.5498666666666665,
      "grad_norm": 0.22456909716129303,
      "learning_rate": 1.5313333333333335e-05,
      "loss": 0.0027,
      "step": 104060
    },
    {
      "epoch": 5.5504,
      "grad_norm": 0.028070272877812386,
      "learning_rate": 1.531e-05,
      "loss": 0.0024,
      "step": 104070
    },
    {
      "epoch": 5.550933333333333,
      "grad_norm": 0.22455978393554688,
      "learning_rate": 1.5306666666666667e-05,
      "loss": 0.0025,
      "step": 104080
    },
    {
      "epoch": 5.551466666666666,
      "grad_norm": 0.028070462867617607,
      "learning_rate": 1.5303333333333333e-05,
      "loss": 0.0017,
      "step": 104090
    },
    {
      "epoch": 5.552,
      "grad_norm": 0.05614043027162552,
      "learning_rate": 1.53e-05,
      "loss": 0.0016,
      "step": 104100
    },
    {
      "epoch": 5.552533333333333,
      "grad_norm": 0.2245648205280304,
      "learning_rate": 1.529666666666667e-05,
      "loss": 0.0024,
      "step": 104110
    },
    {
      "epoch": 5.553066666666667,
      "grad_norm": 0.3087671101093292,
      "learning_rate": 1.5293333333333335e-05,
      "loss": 0.0019,
      "step": 104120
    },
    {
      "epoch": 5.5536,
      "grad_norm": 0.05614156275987625,
      "learning_rate": 1.529e-05,
      "loss": 0.0034,
      "step": 104130
    },
    {
      "epoch": 5.554133333333334,
      "grad_norm": 0.2235003113746643,
      "learning_rate": 1.5286666666666667e-05,
      "loss": 0.0026,
      "step": 104140
    },
    {
      "epoch": 5.554666666666667,
      "grad_norm": 0.028070181608200073,
      "learning_rate": 1.5283333333333333e-05,
      "loss": 0.0023,
      "step": 104150
    },
    {
      "epoch": 5.5552,
      "grad_norm": 0.16841734945774078,
      "learning_rate": 1.528e-05,
      "loss": 0.0028,
      "step": 104160
    },
    {
      "epoch": 5.555733333333333,
      "grad_norm": 0.28070133924484253,
      "learning_rate": 1.5276666666666666e-05,
      "loss": 0.0025,
      "step": 104170
    },
    {
      "epoch": 5.556266666666667,
      "grad_norm": 0.3087942600250244,
      "learning_rate": 1.5273333333333335e-05,
      "loss": 0.0024,
      "step": 104180
    },
    {
      "epoch": 5.5568,
      "grad_norm": 0.02807113155722618,
      "learning_rate": 1.527e-05,
      "loss": 0.0033,
      "step": 104190
    },
    {
      "epoch": 5.557333333333333,
      "grad_norm": 0.2806921601295471,
      "learning_rate": 1.5266666666666667e-05,
      "loss": 0.002,
      "step": 104200
    },
    {
      "epoch": 5.5578666666666665,
      "grad_norm": 0.28071120381355286,
      "learning_rate": 1.5263333333333333e-05,
      "loss": 0.0027,
      "step": 104210
    },
    {
      "epoch": 5.5584,
      "grad_norm": 0.1403546780347824,
      "learning_rate": 1.5260000000000003e-05,
      "loss": 0.0032,
      "step": 104220
    },
    {
      "epoch": 5.558933333333333,
      "grad_norm": 0.08421176671981812,
      "learning_rate": 1.5256666666666666e-05,
      "loss": 0.0029,
      "step": 104230
    },
    {
      "epoch": 5.559466666666666,
      "grad_norm": 0.33682942390441895,
      "learning_rate": 1.5253333333333334e-05,
      "loss": 0.0021,
      "step": 104240
    },
    {
      "epoch": 5.5600000000000005,
      "grad_norm": 0.1684253066778183,
      "learning_rate": 1.525e-05,
      "loss": 0.0015,
      "step": 104250
    },
    {
      "epoch": 5.560533333333334,
      "grad_norm": 0.1964939832687378,
      "learning_rate": 1.5246666666666668e-05,
      "loss": 0.0029,
      "step": 104260
    },
    {
      "epoch": 5.561066666666667,
      "grad_norm": 0.14034569263458252,
      "learning_rate": 1.5243333333333334e-05,
      "loss": 0.0023,
      "step": 104270
    },
    {
      "epoch": 5.5616,
      "grad_norm": 0.05614059418439865,
      "learning_rate": 1.5240000000000001e-05,
      "loss": 0.0035,
      "step": 104280
    },
    {
      "epoch": 5.562133333333334,
      "grad_norm": 0.11228293925523758,
      "learning_rate": 1.523666666666667e-05,
      "loss": 0.0019,
      "step": 104290
    },
    {
      "epoch": 5.562666666666667,
      "grad_norm": 0.05614107474684715,
      "learning_rate": 1.5233333333333332e-05,
      "loss": 0.0023,
      "step": 104300
    },
    {
      "epoch": 5.5632,
      "grad_norm": 0.4210277199745178,
      "learning_rate": 1.523e-05,
      "loss": 0.003,
      "step": 104310
    },
    {
      "epoch": 5.563733333333333,
      "grad_norm": 0.1122773066163063,
      "learning_rate": 1.5226666666666668e-05,
      "loss": 0.0034,
      "step": 104320
    },
    {
      "epoch": 5.564266666666667,
      "grad_norm": 8.901370751601689e-10,
      "learning_rate": 1.5223333333333334e-05,
      "loss": 0.0028,
      "step": 104330
    },
    {
      "epoch": 5.5648,
      "grad_norm": 0.084207683801651,
      "learning_rate": 1.5220000000000002e-05,
      "loss": 0.0022,
      "step": 104340
    },
    {
      "epoch": 5.565333333333333,
      "grad_norm": 0.2245495617389679,
      "learning_rate": 1.5216666666666668e-05,
      "loss": 0.0025,
      "step": 104350
    },
    {
      "epoch": 5.5658666666666665,
      "grad_norm": 0.33683252334594727,
      "learning_rate": 1.5213333333333336e-05,
      "loss": 0.0031,
      "step": 104360
    },
    {
      "epoch": 5.5664,
      "grad_norm": 0.16841493546962738,
      "learning_rate": 1.5210000000000002e-05,
      "loss": 0.0023,
      "step": 104370
    },
    {
      "epoch": 5.566933333333333,
      "grad_norm": 0.14035385847091675,
      "learning_rate": 1.5206666666666666e-05,
      "loss": 0.0025,
      "step": 104380
    },
    {
      "epoch": 5.567466666666666,
      "grad_norm": 0.08421254903078079,
      "learning_rate": 1.5203333333333334e-05,
      "loss": 0.0037,
      "step": 104390
    },
    {
      "epoch": 5.568,
      "grad_norm": 0.3368336260318756,
      "learning_rate": 1.52e-05,
      "loss": 0.0035,
      "step": 104400
    },
    {
      "epoch": 5.568533333333333,
      "grad_norm": 0.14034360647201538,
      "learning_rate": 1.5196666666666668e-05,
      "loss": 0.003,
      "step": 104410
    },
    {
      "epoch": 5.569066666666667,
      "grad_norm": 0.11227840930223465,
      "learning_rate": 1.5193333333333334e-05,
      "loss": 0.0025,
      "step": 104420
    },
    {
      "epoch": 5.5696,
      "grad_norm": 0.028068484738469124,
      "learning_rate": 1.5190000000000002e-05,
      "loss": 0.0026,
      "step": 104430
    },
    {
      "epoch": 5.570133333333334,
      "grad_norm": 0.02806909941136837,
      "learning_rate": 1.5186666666666668e-05,
      "loss": 0.0036,
      "step": 104440
    },
    {
      "epoch": 5.570666666666667,
      "grad_norm": 0.3368333876132965,
      "learning_rate": 1.5183333333333333e-05,
      "loss": 0.0028,
      "step": 104450
    },
    {
      "epoch": 5.5712,
      "grad_norm": 0.4210207164287567,
      "learning_rate": 1.518e-05,
      "loss": 0.0027,
      "step": 104460
    },
    {
      "epoch": 5.571733333333333,
      "grad_norm": 0.16842766106128693,
      "learning_rate": 1.5176666666666666e-05,
      "loss": 0.0035,
      "step": 104470
    },
    {
      "epoch": 5.572266666666667,
      "grad_norm": 0.14034704864025116,
      "learning_rate": 1.5173333333333334e-05,
      "loss": 0.0027,
      "step": 104480
    },
    {
      "epoch": 5.5728,
      "grad_norm": 4.163961175862596e-09,
      "learning_rate": 1.517e-05,
      "loss": 0.0033,
      "step": 104490
    },
    {
      "epoch": 5.573333333333333,
      "grad_norm": 0.028070099651813507,
      "learning_rate": 1.5166666666666668e-05,
      "loss": 0.0019,
      "step": 104500
    },
    {
      "epoch": 5.5738666666666665,
      "grad_norm": 0.22454530000686646,
      "learning_rate": 1.5163333333333334e-05,
      "loss": 0.0034,
      "step": 104510
    },
    {
      "epoch": 5.5744,
      "grad_norm": 1.5652784624009541e-09,
      "learning_rate": 1.5160000000000002e-05,
      "loss": 0.0025,
      "step": 104520
    },
    {
      "epoch": 5.574933333333333,
      "grad_norm": 0.05613795295357704,
      "learning_rate": 1.5156666666666667e-05,
      "loss": 0.0025,
      "step": 104530
    },
    {
      "epoch": 5.575466666666666,
      "grad_norm": 0.19648659229278564,
      "learning_rate": 1.5153333333333333e-05,
      "loss": 0.004,
      "step": 104540
    },
    {
      "epoch": 5.576,
      "grad_norm": 1.0884037017822266,
      "learning_rate": 1.515e-05,
      "loss": 0.0043,
      "step": 104550
    },
    {
      "epoch": 5.576533333333334,
      "grad_norm": 0.19648143649101257,
      "learning_rate": 1.5146666666666667e-05,
      "loss": 0.0026,
      "step": 104560
    },
    {
      "epoch": 5.577066666666667,
      "grad_norm": 0.19647343456745148,
      "learning_rate": 1.5143333333333335e-05,
      "loss": 0.0021,
      "step": 104570
    },
    {
      "epoch": 5.5776,
      "grad_norm": 0.1684207320213318,
      "learning_rate": 1.514e-05,
      "loss": 0.0031,
      "step": 104580
    },
    {
      "epoch": 5.578133333333334,
      "grad_norm": 0.028069227933883667,
      "learning_rate": 1.5136666666666669e-05,
      "loss": 0.0027,
      "step": 104590
    },
    {
      "epoch": 5.578666666666667,
      "grad_norm": 0.19647987186908722,
      "learning_rate": 1.5133333333333333e-05,
      "loss": 0.0018,
      "step": 104600
    },
    {
      "epoch": 5.5792,
      "grad_norm": 0.1684107780456543,
      "learning_rate": 1.5129999999999999e-05,
      "loss": 0.0035,
      "step": 104610
    },
    {
      "epoch": 5.579733333333333,
      "grad_norm": 0.1964752972126007,
      "learning_rate": 1.5126666666666667e-05,
      "loss": 0.0027,
      "step": 104620
    },
    {
      "epoch": 5.580266666666667,
      "grad_norm": 2.225984729875563e-09,
      "learning_rate": 1.5123333333333333e-05,
      "loss": 0.003,
      "step": 104630
    },
    {
      "epoch": 5.5808,
      "grad_norm": 0.47715112566947937,
      "learning_rate": 1.5120000000000001e-05,
      "loss": 0.0028,
      "step": 104640
    },
    {
      "epoch": 5.581333333333333,
      "grad_norm": 0.1684138923883438,
      "learning_rate": 1.5116666666666667e-05,
      "loss": 0.0035,
      "step": 104650
    },
    {
      "epoch": 5.5818666666666665,
      "grad_norm": 0.42105358839035034,
      "learning_rate": 1.5113333333333335e-05,
      "loss": 0.0028,
      "step": 104660
    },
    {
      "epoch": 5.5824,
      "grad_norm": 0.1684144139289856,
      "learning_rate": 1.5110000000000003e-05,
      "loss": 0.0024,
      "step": 104670
    },
    {
      "epoch": 5.582933333333333,
      "grad_norm": 0.22454436123371124,
      "learning_rate": 1.5106666666666665e-05,
      "loss": 0.003,
      "step": 104680
    },
    {
      "epoch": 5.583466666666666,
      "grad_norm": 0.16841329634189606,
      "learning_rate": 1.5103333333333333e-05,
      "loss": 0.0025,
      "step": 104690
    },
    {
      "epoch": 5.584,
      "grad_norm": 0.36487728357315063,
      "learning_rate": 1.51e-05,
      "loss": 0.002,
      "step": 104700
    },
    {
      "epoch": 5.584533333333333,
      "grad_norm": 0.2245507389307022,
      "learning_rate": 1.5096666666666667e-05,
      "loss": 0.0035,
      "step": 104710
    },
    {
      "epoch": 5.585066666666666,
      "grad_norm": 0.02806878834962845,
      "learning_rate": 1.5093333333333335e-05,
      "loss": 0.0046,
      "step": 104720
    },
    {
      "epoch": 5.5856,
      "grad_norm": 0.11227463185787201,
      "learning_rate": 1.5090000000000001e-05,
      "loss": 0.0031,
      "step": 104730
    },
    {
      "epoch": 5.586133333333334,
      "grad_norm": 0.22453534603118896,
      "learning_rate": 1.5086666666666669e-05,
      "loss": 0.0027,
      "step": 104740
    },
    {
      "epoch": 5.586666666666667,
      "grad_norm": 0.22453941404819489,
      "learning_rate": 1.5083333333333335e-05,
      "loss": 0.0029,
      "step": 104750
    },
    {
      "epoch": 5.5872,
      "grad_norm": 0.028068602085113525,
      "learning_rate": 1.508e-05,
      "loss": 0.0023,
      "step": 104760
    },
    {
      "epoch": 5.587733333333333,
      "grad_norm": 0.36487799882888794,
      "learning_rate": 1.5076666666666667e-05,
      "loss": 0.0032,
      "step": 104770
    },
    {
      "epoch": 5.588266666666667,
      "grad_norm": 0.08420248329639435,
      "learning_rate": 1.5073333333333334e-05,
      "loss": 0.0031,
      "step": 104780
    },
    {
      "epoch": 5.5888,
      "grad_norm": 0.22454452514648438,
      "learning_rate": 1.5070000000000001e-05,
      "loss": 0.0035,
      "step": 104790
    },
    {
      "epoch": 5.589333333333333,
      "grad_norm": 0.2526119649410248,
      "learning_rate": 1.5066666666666668e-05,
      "loss": 0.0019,
      "step": 104800
    },
    {
      "epoch": 5.5898666666666665,
      "grad_norm": 0.16840408742427826,
      "learning_rate": 1.5063333333333335e-05,
      "loss": 0.0022,
      "step": 104810
    },
    {
      "epoch": 5.5904,
      "grad_norm": 0.11227055639028549,
      "learning_rate": 1.5060000000000001e-05,
      "loss": 0.0026,
      "step": 104820
    },
    {
      "epoch": 5.590933333333333,
      "grad_norm": 0.0561356320977211,
      "learning_rate": 1.5056666666666666e-05,
      "loss": 0.0027,
      "step": 104830
    },
    {
      "epoch": 5.591466666666666,
      "grad_norm": 0.028067201375961304,
      "learning_rate": 1.5053333333333334e-05,
      "loss": 0.0031,
      "step": 104840
    },
    {
      "epoch": 5.592,
      "grad_norm": 0.08420458436012268,
      "learning_rate": 1.505e-05,
      "loss": 0.002,
      "step": 104850
    },
    {
      "epoch": 5.592533333333334,
      "grad_norm": 0.056136488914489746,
      "learning_rate": 1.5046666666666668e-05,
      "loss": 0.0037,
      "step": 104860
    },
    {
      "epoch": 5.593066666666667,
      "grad_norm": 0.22453376650810242,
      "learning_rate": 1.5043333333333334e-05,
      "loss": 0.0018,
      "step": 104870
    },
    {
      "epoch": 5.5936,
      "grad_norm": 0.1403409242630005,
      "learning_rate": 1.5040000000000002e-05,
      "loss": 0.0033,
      "step": 104880
    },
    {
      "epoch": 5.594133333333334,
      "grad_norm": 0.05613643303513527,
      "learning_rate": 1.5036666666666668e-05,
      "loss": 0.0029,
      "step": 104890
    },
    {
      "epoch": 5.594666666666667,
      "grad_norm": 0.11227427423000336,
      "learning_rate": 1.5033333333333336e-05,
      "loss": 0.0019,
      "step": 104900
    },
    {
      "epoch": 5.5952,
      "grad_norm": 0.3368179202079773,
      "learning_rate": 1.503e-05,
      "loss": 0.0022,
      "step": 104910
    },
    {
      "epoch": 5.5957333333333334,
      "grad_norm": 1.901897668838501,
      "learning_rate": 1.5026666666666666e-05,
      "loss": 0.0014,
      "step": 104920
    },
    {
      "epoch": 5.596266666666667,
      "grad_norm": 0.22455547749996185,
      "learning_rate": 1.5023333333333334e-05,
      "loss": 0.003,
      "step": 104930
    },
    {
      "epoch": 5.5968,
      "grad_norm": 0.14034418761730194,
      "learning_rate": 1.502e-05,
      "loss": 0.0021,
      "step": 104940
    },
    {
      "epoch": 5.597333333333333,
      "grad_norm": 0.05613516643643379,
      "learning_rate": 1.5016666666666668e-05,
      "loss": 0.0042,
      "step": 104950
    },
    {
      "epoch": 5.5978666666666665,
      "grad_norm": 0.05613826587796211,
      "learning_rate": 1.5013333333333334e-05,
      "loss": 0.002,
      "step": 104960
    },
    {
      "epoch": 5.5984,
      "grad_norm": 3.1490849927706677e-09,
      "learning_rate": 1.5010000000000002e-05,
      "loss": 0.0026,
      "step": 104970
    },
    {
      "epoch": 5.598933333333333,
      "grad_norm": 0.028067797422409058,
      "learning_rate": 1.5006666666666666e-05,
      "loss": 0.003,
      "step": 104980
    },
    {
      "epoch": 5.599466666666666,
      "grad_norm": 0.0842033326625824,
      "learning_rate": 1.5003333333333333e-05,
      "loss": 0.0019,
      "step": 104990
    },
    {
      "epoch": 5.6,
      "grad_norm": 0.14033924043178558,
      "learning_rate": 1.5e-05,
      "loss": 0.0027,
      "step": 105000
    },
    {
      "epoch": 5.600533333333333,
      "grad_norm": 0.2525995373725891,
      "learning_rate": 1.4996666666666667e-05,
      "loss": 0.0023,
      "step": 105010
    },
    {
      "epoch": 5.601066666666666,
      "grad_norm": 0.16840368509292603,
      "learning_rate": 1.4993333333333334e-05,
      "loss": 0.0021,
      "step": 105020
    },
    {
      "epoch": 5.6016,
      "grad_norm": 0.2245388925075531,
      "learning_rate": 1.499e-05,
      "loss": 0.0028,
      "step": 105030
    },
    {
      "epoch": 5.602133333333334,
      "grad_norm": 0.14033405482769012,
      "learning_rate": 1.4986666666666668e-05,
      "loss": 0.0026,
      "step": 105040
    },
    {
      "epoch": 5.602666666666667,
      "grad_norm": 0.14034515619277954,
      "learning_rate": 1.4983333333333336e-05,
      "loss": 0.0018,
      "step": 105050
    },
    {
      "epoch": 5.6032,
      "grad_norm": 1.2306026220321655,
      "learning_rate": 1.4979999999999999e-05,
      "loss": 0.0039,
      "step": 105060
    },
    {
      "epoch": 5.6037333333333335,
      "grad_norm": 0.028066817671060562,
      "learning_rate": 1.4976666666666667e-05,
      "loss": 0.0025,
      "step": 105070
    },
    {
      "epoch": 5.604266666666667,
      "grad_norm": 0.19647832214832306,
      "learning_rate": 1.4973333333333333e-05,
      "loss": 0.0032,
      "step": 105080
    },
    {
      "epoch": 5.6048,
      "grad_norm": 0.23628783226013184,
      "learning_rate": 1.497e-05,
      "loss": 0.0021,
      "step": 105090
    },
    {
      "epoch": 5.605333333333333,
      "grad_norm": 0.2526165544986725,
      "learning_rate": 1.4966666666666668e-05,
      "loss": 0.0026,
      "step": 105100
    },
    {
      "epoch": 5.6058666666666666,
      "grad_norm": 0.36484968662261963,
      "learning_rate": 1.4963333333333335e-05,
      "loss": 0.0024,
      "step": 105110
    },
    {
      "epoch": 5.6064,
      "grad_norm": 0.05613631010055542,
      "learning_rate": 1.4960000000000002e-05,
      "loss": 0.0035,
      "step": 105120
    },
    {
      "epoch": 5.606933333333333,
      "grad_norm": 0.05613277852535248,
      "learning_rate": 1.4956666666666665e-05,
      "loss": 0.0024,
      "step": 105130
    },
    {
      "epoch": 5.607466666666666,
      "grad_norm": 0.11226753890514374,
      "learning_rate": 1.4953333333333333e-05,
      "loss": 0.0015,
      "step": 105140
    },
    {
      "epoch": 5.608,
      "grad_norm": 0.056136783212423325,
      "learning_rate": 1.4950000000000001e-05,
      "loss": 0.0027,
      "step": 105150
    },
    {
      "epoch": 5.608533333333334,
      "grad_norm": 0.16840241849422455,
      "learning_rate": 1.4946666666666667e-05,
      "loss": 0.0024,
      "step": 105160
    },
    {
      "epoch": 5.609066666666667,
      "grad_norm": 0.05613129585981369,
      "learning_rate": 1.4943333333333335e-05,
      "loss": 0.002,
      "step": 105170
    },
    {
      "epoch": 5.6096,
      "grad_norm": 0.140339195728302,
      "learning_rate": 1.4940000000000001e-05,
      "loss": 0.0022,
      "step": 105180
    },
    {
      "epoch": 5.610133333333334,
      "grad_norm": 0.5644368529319763,
      "learning_rate": 1.4936666666666669e-05,
      "loss": 0.0033,
      "step": 105190
    },
    {
      "epoch": 5.610666666666667,
      "grad_norm": 1.251133680343628,
      "learning_rate": 1.4933333333333335e-05,
      "loss": 0.0023,
      "step": 105200
    },
    {
      "epoch": 5.6112,
      "grad_norm": 0.5051816701889038,
      "learning_rate": 1.493e-05,
      "loss": 0.0022,
      "step": 105210
    },
    {
      "epoch": 5.6117333333333335,
      "grad_norm": 0.33681443333625793,
      "learning_rate": 1.4926666666666667e-05,
      "loss": 0.0024,
      "step": 105220
    },
    {
      "epoch": 5.612266666666667,
      "grad_norm": 0.2526148855686188,
      "learning_rate": 1.4923333333333333e-05,
      "loss": 0.0023,
      "step": 105230
    },
    {
      "epoch": 5.6128,
      "grad_norm": 0.22452981770038605,
      "learning_rate": 1.4920000000000001e-05,
      "loss": 0.0046,
      "step": 105240
    },
    {
      "epoch": 5.613333333333333,
      "grad_norm": 0.02806604839861393,
      "learning_rate": 1.4916666666666667e-05,
      "loss": 0.0019,
      "step": 105250
    },
    {
      "epoch": 5.613866666666667,
      "grad_norm": 0.5051819086074829,
      "learning_rate": 1.4913333333333335e-05,
      "loss": 0.0038,
      "step": 105260
    },
    {
      "epoch": 5.6144,
      "grad_norm": 0.25259703397750854,
      "learning_rate": 1.4910000000000001e-05,
      "loss": 0.0022,
      "step": 105270
    },
    {
      "epoch": 5.614933333333333,
      "grad_norm": 0.2806604206562042,
      "learning_rate": 1.4906666666666666e-05,
      "loss": 0.0021,
      "step": 105280
    },
    {
      "epoch": 5.615466666666666,
      "grad_norm": 0.3367830216884613,
      "learning_rate": 1.4903333333333334e-05,
      "loss": 0.0022,
      "step": 105290
    },
    {
      "epoch": 5.616,
      "grad_norm": 0.11226408928632736,
      "learning_rate": 1.49e-05,
      "loss": 0.0019,
      "step": 105300
    },
    {
      "epoch": 5.616533333333333,
      "grad_norm": 0.1683943122625351,
      "learning_rate": 1.4896666666666667e-05,
      "loss": 0.0031,
      "step": 105310
    },
    {
      "epoch": 5.617066666666666,
      "grad_norm": 0.25260016322135925,
      "learning_rate": 1.4893333333333334e-05,
      "loss": 0.0026,
      "step": 105320
    },
    {
      "epoch": 5.6176,
      "grad_norm": 0.16840584576129913,
      "learning_rate": 1.4890000000000001e-05,
      "loss": 0.0024,
      "step": 105330
    },
    {
      "epoch": 5.618133333333334,
      "grad_norm": 0.028066182509064674,
      "learning_rate": 1.4886666666666668e-05,
      "loss": 0.0025,
      "step": 105340
    },
    {
      "epoch": 5.618666666666667,
      "grad_norm": 0.028064928948879242,
      "learning_rate": 1.4883333333333335e-05,
      "loss": 0.003,
      "step": 105350
    },
    {
      "epoch": 5.6192,
      "grad_norm": 0.22453099489212036,
      "learning_rate": 1.488e-05,
      "loss": 0.0026,
      "step": 105360
    },
    {
      "epoch": 5.6197333333333335,
      "grad_norm": 0.36486223340034485,
      "learning_rate": 1.4876666666666666e-05,
      "loss": 0.0034,
      "step": 105370
    },
    {
      "epoch": 5.620266666666667,
      "grad_norm": 0.11226308345794678,
      "learning_rate": 1.4873333333333334e-05,
      "loss": 0.0031,
      "step": 105380
    },
    {
      "epoch": 5.6208,
      "grad_norm": 0.28065523505210876,
      "learning_rate": 1.487e-05,
      "loss": 0.0032,
      "step": 105390
    },
    {
      "epoch": 5.621333333333333,
      "grad_norm": 0.1683906465768814,
      "learning_rate": 1.4866666666666668e-05,
      "loss": 0.0024,
      "step": 105400
    },
    {
      "epoch": 5.621866666666667,
      "grad_norm": 0.08419643342494965,
      "learning_rate": 1.4863333333333334e-05,
      "loss": 0.0036,
      "step": 105410
    },
    {
      "epoch": 5.6224,
      "grad_norm": 0.11226218938827515,
      "learning_rate": 1.4860000000000002e-05,
      "loss": 0.0025,
      "step": 105420
    },
    {
      "epoch": 5.622933333333333,
      "grad_norm": 0.08419511467218399,
      "learning_rate": 1.485666666666667e-05,
      "loss": 0.0012,
      "step": 105430
    },
    {
      "epoch": 5.623466666666666,
      "grad_norm": 0.02806512638926506,
      "learning_rate": 1.4853333333333332e-05,
      "loss": 0.0028,
      "step": 105440
    },
    {
      "epoch": 5.624,
      "grad_norm": 0.19645394384860992,
      "learning_rate": 1.485e-05,
      "loss": 0.0023,
      "step": 105450
    },
    {
      "epoch": 5.624533333333334,
      "grad_norm": 1.6517341136932373,
      "learning_rate": 1.4846666666666666e-05,
      "loss": 0.0025,
      "step": 105460
    },
    {
      "epoch": 5.625066666666667,
      "grad_norm": 0.1122620552778244,
      "learning_rate": 1.4843333333333334e-05,
      "loss": 0.0029,
      "step": 105470
    },
    {
      "epoch": 5.6256,
      "grad_norm": 0.08781716227531433,
      "learning_rate": 1.4840000000000002e-05,
      "loss": 0.0029,
      "step": 105480
    },
    {
      "epoch": 5.626133333333334,
      "grad_norm": 0.3176765739917755,
      "learning_rate": 1.4836666666666668e-05,
      "loss": 0.0033,
      "step": 105490
    },
    {
      "epoch": 5.626666666666667,
      "grad_norm": 0.3087144196033478,
      "learning_rate": 1.4833333333333336e-05,
      "loss": 0.0035,
      "step": 105500
    },
    {
      "epoch": 5.6272,
      "grad_norm": 0.16839298605918884,
      "learning_rate": 1.4829999999999999e-05,
      "loss": 0.002,
      "step": 105510
    },
    {
      "epoch": 5.6277333333333335,
      "grad_norm": 0.1964745968580246,
      "learning_rate": 1.4826666666666666e-05,
      "loss": 0.0043,
      "step": 105520
    },
    {
      "epoch": 5.628266666666667,
      "grad_norm": 0.22450852394104004,
      "learning_rate": 1.4823333333333334e-05,
      "loss": 0.0027,
      "step": 105530
    },
    {
      "epoch": 5.6288,
      "grad_norm": 0.2245282232761383,
      "learning_rate": 1.482e-05,
      "loss": 0.0025,
      "step": 105540
    },
    {
      "epoch": 5.629333333333333,
      "grad_norm": 0.11226287484169006,
      "learning_rate": 1.4816666666666668e-05,
      "loss": 0.0025,
      "step": 105550
    },
    {
      "epoch": 5.629866666666667,
      "grad_norm": 0.028064178302884102,
      "learning_rate": 1.4813333333333334e-05,
      "loss": 0.0023,
      "step": 105560
    },
    {
      "epoch": 5.6304,
      "grad_norm": 0.14032672345638275,
      "learning_rate": 1.4810000000000002e-05,
      "loss": 0.0016,
      "step": 105570
    },
    {
      "epoch": 5.630933333333333,
      "grad_norm": 0.056130699813365936,
      "learning_rate": 1.4806666666666668e-05,
      "loss": 0.0022,
      "step": 105580
    },
    {
      "epoch": 5.631466666666666,
      "grad_norm": 0.14032167196273804,
      "learning_rate": 1.4803333333333333e-05,
      "loss": 0.0032,
      "step": 105590
    },
    {
      "epoch": 5.632,
      "grad_norm": 0.14032310247421265,
      "learning_rate": 1.48e-05,
      "loss": 0.0023,
      "step": 105600
    },
    {
      "epoch": 5.632533333333333,
      "grad_norm": 0.08419486880302429,
      "learning_rate": 1.4796666666666667e-05,
      "loss": 0.002,
      "step": 105610
    },
    {
      "epoch": 5.633066666666666,
      "grad_norm": 0.0618571899831295,
      "learning_rate": 1.4793333333333335e-05,
      "loss": 0.0026,
      "step": 105620
    },
    {
      "epoch": 5.6336,
      "grad_norm": 0.2806486189365387,
      "learning_rate": 1.479e-05,
      "loss": 0.0021,
      "step": 105630
    },
    {
      "epoch": 5.634133333333334,
      "grad_norm": 0.08419852703809738,
      "learning_rate": 1.4786666666666669e-05,
      "loss": 0.0032,
      "step": 105640
    },
    {
      "epoch": 5.634666666666667,
      "grad_norm": 3.5985900925794567e-09,
      "learning_rate": 1.4783333333333335e-05,
      "loss": 0.0036,
      "step": 105650
    },
    {
      "epoch": 5.6352,
      "grad_norm": 0.16838663816452026,
      "learning_rate": 1.4779999999999999e-05,
      "loss": 0.0024,
      "step": 105660
    },
    {
      "epoch": 5.6357333333333335,
      "grad_norm": 0.22453397512435913,
      "learning_rate": 1.4776666666666667e-05,
      "loss": 0.0024,
      "step": 105670
    },
    {
      "epoch": 5.636266666666667,
      "grad_norm": 0.14032629132270813,
      "learning_rate": 1.4773333333333333e-05,
      "loss": 0.0028,
      "step": 105680
    },
    {
      "epoch": 5.6368,
      "grad_norm": 0.028064580634236336,
      "learning_rate": 1.4770000000000001e-05,
      "loss": 0.0038,
      "step": 105690
    },
    {
      "epoch": 5.637333333333333,
      "grad_norm": 0.19644835591316223,
      "learning_rate": 1.4766666666666667e-05,
      "loss": 0.0028,
      "step": 105700
    },
    {
      "epoch": 5.637866666666667,
      "grad_norm": 0.44903451204299927,
      "learning_rate": 1.4763333333333335e-05,
      "loss": 0.003,
      "step": 105710
    },
    {
      "epoch": 5.6384,
      "grad_norm": 0.36482736468315125,
      "learning_rate": 1.4760000000000001e-05,
      "loss": 0.0021,
      "step": 105720
    },
    {
      "epoch": 5.638933333333333,
      "grad_norm": 0.3367787301540375,
      "learning_rate": 1.4756666666666669e-05,
      "loss": 0.0025,
      "step": 105730
    },
    {
      "epoch": 5.639466666666666,
      "grad_norm": 3.4014420169370396e-09,
      "learning_rate": 1.4753333333333333e-05,
      "loss": 0.0026,
      "step": 105740
    },
    {
      "epoch": 5.64,
      "grad_norm": 0.1122574433684349,
      "learning_rate": 1.475e-05,
      "loss": 0.0027,
      "step": 105750
    },
    {
      "epoch": 5.640533333333333,
      "grad_norm": 0.19645187258720398,
      "learning_rate": 1.4746666666666667e-05,
      "loss": 0.0023,
      "step": 105760
    },
    {
      "epoch": 5.641066666666667,
      "grad_norm": 0.02806464396417141,
      "learning_rate": 1.4743333333333333e-05,
      "loss": 0.004,
      "step": 105770
    },
    {
      "epoch": 5.6416,
      "grad_norm": 0.11225598305463791,
      "learning_rate": 1.4740000000000001e-05,
      "loss": 0.0023,
      "step": 105780
    },
    {
      "epoch": 5.642133333333334,
      "grad_norm": 0.028064247220754623,
      "learning_rate": 1.4736666666666667e-05,
      "loss": 0.0024,
      "step": 105790
    },
    {
      "epoch": 5.642666666666667,
      "grad_norm": 0.36483606696128845,
      "learning_rate": 1.4733333333333335e-05,
      "loss": 0.0022,
      "step": 105800
    },
    {
      "epoch": 5.6432,
      "grad_norm": 2.853759673016043e-09,
      "learning_rate": 1.473e-05,
      "loss": 0.0023,
      "step": 105810
    },
    {
      "epoch": 5.6437333333333335,
      "grad_norm": 0.11225684732198715,
      "learning_rate": 1.4726666666666666e-05,
      "loss": 0.0018,
      "step": 105820
    },
    {
      "epoch": 5.644266666666667,
      "grad_norm": 0.056128982454538345,
      "learning_rate": 1.4723333333333334e-05,
      "loss": 0.0032,
      "step": 105830
    },
    {
      "epoch": 5.6448,
      "grad_norm": 0.3367590010166168,
      "learning_rate": 1.472e-05,
      "loss": 0.0023,
      "step": 105840
    },
    {
      "epoch": 5.645333333333333,
      "grad_norm": 0.056127406656742096,
      "learning_rate": 1.4716666666666668e-05,
      "loss": 0.0022,
      "step": 105850
    },
    {
      "epoch": 5.645866666666667,
      "grad_norm": 0.420956015586853,
      "learning_rate": 1.4713333333333335e-05,
      "loss": 0.0031,
      "step": 105860
    },
    {
      "epoch": 5.6464,
      "grad_norm": 0.02806331031024456,
      "learning_rate": 1.4710000000000001e-05,
      "loss": 0.002,
      "step": 105870
    },
    {
      "epoch": 5.646933333333333,
      "grad_norm": 0.9744582772254944,
      "learning_rate": 1.470666666666667e-05,
      "loss": 0.0019,
      "step": 105880
    },
    {
      "epoch": 5.647466666666666,
      "grad_norm": 0.5612574219703674,
      "learning_rate": 1.4703333333333332e-05,
      "loss": 0.0025,
      "step": 105890
    },
    {
      "epoch": 5.648,
      "grad_norm": 0.14032022655010223,
      "learning_rate": 1.47e-05,
      "loss": 0.0027,
      "step": 105900
    },
    {
      "epoch": 5.648533333333333,
      "grad_norm": 0.028063444420695305,
      "learning_rate": 1.4696666666666668e-05,
      "loss": 0.0015,
      "step": 105910
    },
    {
      "epoch": 5.649066666666666,
      "grad_norm": 0.3367655575275421,
      "learning_rate": 1.4693333333333334e-05,
      "loss": 0.0033,
      "step": 105920
    },
    {
      "epoch": 5.6495999999999995,
      "grad_norm": 0.11225700378417969,
      "learning_rate": 1.4690000000000002e-05,
      "loss": 0.0025,
      "step": 105930
    },
    {
      "epoch": 5.650133333333334,
      "grad_norm": 0.1122530922293663,
      "learning_rate": 1.4686666666666668e-05,
      "loss": 0.0024,
      "step": 105940
    },
    {
      "epoch": 5.650666666666667,
      "grad_norm": 0.22451096773147583,
      "learning_rate": 1.4683333333333336e-05,
      "loss": 0.0023,
      "step": 105950
    },
    {
      "epoch": 5.6512,
      "grad_norm": 0.22451019287109375,
      "learning_rate": 1.4680000000000002e-05,
      "loss": 0.0031,
      "step": 105960
    },
    {
      "epoch": 5.6517333333333335,
      "grad_norm": 0.2525656223297119,
      "learning_rate": 1.4676666666666666e-05,
      "loss": 0.0023,
      "step": 105970
    },
    {
      "epoch": 5.652266666666667,
      "grad_norm": 0.08419224619865417,
      "learning_rate": 1.4673333333333334e-05,
      "loss": 0.0023,
      "step": 105980
    },
    {
      "epoch": 5.6528,
      "grad_norm": 0.0280635803937912,
      "learning_rate": 1.467e-05,
      "loss": 0.0021,
      "step": 105990
    },
    {
      "epoch": 5.653333333333333,
      "grad_norm": 0.22451019287109375,
      "learning_rate": 1.4666666666666668e-05,
      "loss": 0.0022,
      "step": 106000
    },
    {
      "epoch": 5.653866666666667,
      "grad_norm": 0.25258734822273254,
      "learning_rate": 1.4663333333333334e-05,
      "loss": 0.0034,
      "step": 106010
    },
    {
      "epoch": 5.6544,
      "grad_norm": 0.5051701664924622,
      "learning_rate": 1.4660000000000002e-05,
      "loss": 0.0021,
      "step": 106020
    },
    {
      "epoch": 5.654933333333333,
      "grad_norm": 0.056125886738300323,
      "learning_rate": 1.4656666666666668e-05,
      "loss": 0.0029,
      "step": 106030
    },
    {
      "epoch": 5.655466666666666,
      "grad_norm": 0.08419134467840195,
      "learning_rate": 1.4653333333333333e-05,
      "loss": 0.003,
      "step": 106040
    },
    {
      "epoch": 5.656,
      "grad_norm": 0.19644436240196228,
      "learning_rate": 1.465e-05,
      "loss": 0.0025,
      "step": 106050
    },
    {
      "epoch": 5.656533333333333,
      "grad_norm": 0.30870023369789124,
      "learning_rate": 1.4646666666666666e-05,
      "loss": 0.0025,
      "step": 106060
    },
    {
      "epoch": 5.657066666666667,
      "grad_norm": 1.2292853593826294,
      "learning_rate": 1.4643333333333334e-05,
      "loss": 0.0034,
      "step": 106070
    },
    {
      "epoch": 5.6576,
      "grad_norm": 3.454366348520921e-09,
      "learning_rate": 1.464e-05,
      "loss": 0.0012,
      "step": 106080
    },
    {
      "epoch": 5.658133333333334,
      "grad_norm": 0.1403132826089859,
      "learning_rate": 1.4636666666666668e-05,
      "loss": 0.0026,
      "step": 106090
    },
    {
      "epoch": 5.658666666666667,
      "grad_norm": 0.2806467115879059,
      "learning_rate": 1.4633333333333334e-05,
      "loss": 0.0029,
      "step": 106100
    },
    {
      "epoch": 5.6592,
      "grad_norm": 0.16838471591472626,
      "learning_rate": 1.4630000000000002e-05,
      "loss": 0.0035,
      "step": 106110
    },
    {
      "epoch": 5.6597333333333335,
      "grad_norm": 0.1964411437511444,
      "learning_rate": 1.4626666666666667e-05,
      "loss": 0.0022,
      "step": 106120
    },
    {
      "epoch": 5.660266666666667,
      "grad_norm": 0.2525683641433716,
      "learning_rate": 1.4623333333333333e-05,
      "loss": 0.0036,
      "step": 106130
    },
    {
      "epoch": 5.6608,
      "grad_norm": 0.05612929165363312,
      "learning_rate": 1.462e-05,
      "loss": 0.0027,
      "step": 106140
    },
    {
      "epoch": 5.661333333333333,
      "grad_norm": 0.15208055078983307,
      "learning_rate": 1.4616666666666667e-05,
      "loss": 0.0022,
      "step": 106150
    },
    {
      "epoch": 5.661866666666667,
      "grad_norm": 0.39563989639282227,
      "learning_rate": 1.4613333333333335e-05,
      "loss": 0.003,
      "step": 106160
    },
    {
      "epoch": 5.6624,
      "grad_norm": 0.11225921660661697,
      "learning_rate": 1.461e-05,
      "loss": 0.0025,
      "step": 106170
    },
    {
      "epoch": 5.662933333333333,
      "grad_norm": 0.2806550860404968,
      "learning_rate": 1.4606666666666669e-05,
      "loss": 0.0029,
      "step": 106180
    },
    {
      "epoch": 5.663466666666666,
      "grad_norm": 0.02806307002902031,
      "learning_rate": 1.4603333333333333e-05,
      "loss": 0.0016,
      "step": 106190
    },
    {
      "epoch": 5.664,
      "grad_norm": 0.2806328237056732,
      "learning_rate": 1.4599999999999999e-05,
      "loss": 0.0027,
      "step": 106200
    },
    {
      "epoch": 5.664533333333333,
      "grad_norm": 0.36482205986976624,
      "learning_rate": 1.4596666666666667e-05,
      "loss": 0.0025,
      "step": 106210
    },
    {
      "epoch": 5.665066666666666,
      "grad_norm": 0.3367564082145691,
      "learning_rate": 1.4593333333333333e-05,
      "loss": 0.0018,
      "step": 106220
    },
    {
      "epoch": 5.6655999999999995,
      "grad_norm": 0.05612841621041298,
      "learning_rate": 1.4590000000000001e-05,
      "loss": 0.0016,
      "step": 106230
    },
    {
      "epoch": 5.666133333333334,
      "grad_norm": 0.02806326188147068,
      "learning_rate": 1.4586666666666669e-05,
      "loss": 0.0016,
      "step": 106240
    },
    {
      "epoch": 5.666666666666667,
      "grad_norm": 0.22450360655784607,
      "learning_rate": 1.4583333333333335e-05,
      "loss": 0.0025,
      "step": 106250
    },
    {
      "epoch": 5.6672,
      "grad_norm": 0.36481940746307373,
      "learning_rate": 1.4580000000000003e-05,
      "loss": 0.0027,
      "step": 106260
    },
    {
      "epoch": 5.6677333333333335,
      "grad_norm": 0.11225371807813644,
      "learning_rate": 1.4576666666666665e-05,
      "loss": 0.0026,
      "step": 106270
    },
    {
      "epoch": 5.668266666666667,
      "grad_norm": 0.33675163984298706,
      "learning_rate": 1.4573333333333333e-05,
      "loss": 0.0031,
      "step": 106280
    },
    {
      "epoch": 5.6688,
      "grad_norm": 0.056126706302165985,
      "learning_rate": 1.4570000000000001e-05,
      "loss": 0.0027,
      "step": 106290
    },
    {
      "epoch": 5.669333333333333,
      "grad_norm": 0.05612655729055405,
      "learning_rate": 1.4566666666666667e-05,
      "loss": 0.0028,
      "step": 106300
    },
    {
      "epoch": 5.669866666666667,
      "grad_norm": 0.08419177681207657,
      "learning_rate": 1.4563333333333335e-05,
      "loss": 0.0034,
      "step": 106310
    },
    {
      "epoch": 5.6704,
      "grad_norm": 0.11225578933954239,
      "learning_rate": 1.4560000000000001e-05,
      "loss": 0.0038,
      "step": 106320
    },
    {
      "epoch": 5.670933333333333,
      "grad_norm": 0.0280629750341177,
      "learning_rate": 1.4556666666666669e-05,
      "loss": 0.0043,
      "step": 106330
    },
    {
      "epoch": 5.671466666666666,
      "grad_norm": 0.40515846014022827,
      "learning_rate": 1.4553333333333333e-05,
      "loss": 0.0024,
      "step": 106340
    },
    {
      "epoch": 5.672,
      "grad_norm": 0.25257059931755066,
      "learning_rate": 1.455e-05,
      "loss": 0.0025,
      "step": 106350
    },
    {
      "epoch": 5.672533333333333,
      "grad_norm": 0.14464306831359863,
      "learning_rate": 1.4546666666666667e-05,
      "loss": 0.0028,
      "step": 106360
    },
    {
      "epoch": 5.673066666666667,
      "grad_norm": 0.08419127017259598,
      "learning_rate": 1.4543333333333334e-05,
      "loss": 0.0022,
      "step": 106370
    },
    {
      "epoch": 5.6736,
      "grad_norm": 0.25257524847984314,
      "learning_rate": 1.4540000000000001e-05,
      "loss": 0.0024,
      "step": 106380
    },
    {
      "epoch": 5.674133333333334,
      "grad_norm": 0.16837754845619202,
      "learning_rate": 1.4536666666666668e-05,
      "loss": 0.0026,
      "step": 106390
    },
    {
      "epoch": 5.674666666666667,
      "grad_norm": 0.25258132815361023,
      "learning_rate": 1.4533333333333335e-05,
      "loss": 0.002,
      "step": 106400
    },
    {
      "epoch": 5.6752,
      "grad_norm": 0.11225273460149765,
      "learning_rate": 1.4530000000000001e-05,
      "loss": 0.0024,
      "step": 106410
    },
    {
      "epoch": 5.6757333333333335,
      "grad_norm": 0.11225065588951111,
      "learning_rate": 1.4526666666666666e-05,
      "loss": 0.0026,
      "step": 106420
    },
    {
      "epoch": 5.676266666666667,
      "grad_norm": 0.19645117223262787,
      "learning_rate": 1.4523333333333334e-05,
      "loss": 0.0035,
      "step": 106430
    },
    {
      "epoch": 5.6768,
      "grad_norm": 0.36483100056648254,
      "learning_rate": 1.452e-05,
      "loss": 0.003,
      "step": 106440
    },
    {
      "epoch": 5.677333333333333,
      "grad_norm": 0.37037360668182373,
      "learning_rate": 1.4516666666666668e-05,
      "loss": 0.0024,
      "step": 106450
    },
    {
      "epoch": 5.677866666666667,
      "grad_norm": 0.1586717665195465,
      "learning_rate": 1.4513333333333334e-05,
      "loss": 0.0026,
      "step": 106460
    },
    {
      "epoch": 5.6784,
      "grad_norm": 0.028063485398888588,
      "learning_rate": 1.4510000000000002e-05,
      "loss": 0.0037,
      "step": 106470
    },
    {
      "epoch": 5.678933333333333,
      "grad_norm": 0.22450774908065796,
      "learning_rate": 1.4506666666666668e-05,
      "loss": 0.0025,
      "step": 106480
    },
    {
      "epoch": 5.679466666666666,
      "grad_norm": 0.028063176199793816,
      "learning_rate": 1.4503333333333332e-05,
      "loss": 0.0029,
      "step": 106490
    },
    {
      "epoch": 5.68,
      "grad_norm": 0.1683775931596756,
      "learning_rate": 1.45e-05,
      "loss": 0.0032,
      "step": 106500
    },
    {
      "epoch": 5.680533333333333,
      "grad_norm": 0.05613020434975624,
      "learning_rate": 1.4496666666666666e-05,
      "loss": 0.0019,
      "step": 106510
    },
    {
      "epoch": 5.681066666666666,
      "grad_norm": 0.05612904578447342,
      "learning_rate": 1.4493333333333334e-05,
      "loss": 0.0036,
      "step": 106520
    },
    {
      "epoch": 5.6815999999999995,
      "grad_norm": 0.1683836132287979,
      "learning_rate": 1.449e-05,
      "loss": 0.0034,
      "step": 106530
    },
    {
      "epoch": 5.682133333333334,
      "grad_norm": 0.02806343510746956,
      "learning_rate": 1.4486666666666668e-05,
      "loss": 0.0034,
      "step": 106540
    },
    {
      "epoch": 5.682666666666667,
      "grad_norm": 0.08418828994035721,
      "learning_rate": 1.4483333333333334e-05,
      "loss": 0.0016,
      "step": 106550
    },
    {
      "epoch": 5.6832,
      "grad_norm": 0.14031870663166046,
      "learning_rate": 1.4480000000000002e-05,
      "loss": 0.0032,
      "step": 106560
    },
    {
      "epoch": 5.6837333333333335,
      "grad_norm": 0.11225484311580658,
      "learning_rate": 1.4476666666666666e-05,
      "loss": 0.0022,
      "step": 106570
    },
    {
      "epoch": 5.684266666666667,
      "grad_norm": 0.028061922639608383,
      "learning_rate": 1.4473333333333333e-05,
      "loss": 0.0025,
      "step": 106580
    },
    {
      "epoch": 5.6848,
      "grad_norm": 0.25256502628326416,
      "learning_rate": 1.447e-05,
      "loss": 0.0032,
      "step": 106590
    },
    {
      "epoch": 5.685333333333333,
      "grad_norm": 0.056125741451978683,
      "learning_rate": 1.4466666666666667e-05,
      "loss": 0.0032,
      "step": 106600
    },
    {
      "epoch": 5.685866666666667,
      "grad_norm": 0.1683730036020279,
      "learning_rate": 1.4463333333333334e-05,
      "loss": 0.0032,
      "step": 106610
    },
    {
      "epoch": 5.6864,
      "grad_norm": 0.1403147429227829,
      "learning_rate": 1.4460000000000002e-05,
      "loss": 0.0029,
      "step": 106620
    },
    {
      "epoch": 5.686933333333333,
      "grad_norm": 0.30869320034980774,
      "learning_rate": 1.4456666666666668e-05,
      "loss": 0.0029,
      "step": 106630
    },
    {
      "epoch": 5.6874666666666664,
      "grad_norm": 0.22449249029159546,
      "learning_rate": 1.4453333333333336e-05,
      "loss": 0.0034,
      "step": 106640
    },
    {
      "epoch": 5.688,
      "grad_norm": 0.02806365303695202,
      "learning_rate": 1.4449999999999999e-05,
      "loss": 0.0031,
      "step": 106650
    },
    {
      "epoch": 5.688533333333333,
      "grad_norm": 0.3648279011249542,
      "learning_rate": 1.4446666666666667e-05,
      "loss": 0.0019,
      "step": 106660
    },
    {
      "epoch": 5.689066666666667,
      "grad_norm": 0.1683700978755951,
      "learning_rate": 1.4443333333333335e-05,
      "loss": 0.0022,
      "step": 106670
    },
    {
      "epoch": 5.6896,
      "grad_norm": 0.16837452352046967,
      "learning_rate": 1.444e-05,
      "loss": 0.0025,
      "step": 106680
    },
    {
      "epoch": 5.690133333333334,
      "grad_norm": 0.22449494898319244,
      "learning_rate": 1.4436666666666668e-05,
      "loss": 0.0024,
      "step": 106690
    },
    {
      "epoch": 5.690666666666667,
      "grad_norm": 0.19643542170524597,
      "learning_rate": 1.4433333333333335e-05,
      "loss": 0.0034,
      "step": 106700
    },
    {
      "epoch": 5.6912,
      "grad_norm": 0.11225339770317078,
      "learning_rate": 1.4430000000000002e-05,
      "loss": 0.0025,
      "step": 106710
    },
    {
      "epoch": 5.6917333333333335,
      "grad_norm": 0.08418826013803482,
      "learning_rate": 1.4426666666666667e-05,
      "loss": 0.0044,
      "step": 106720
    },
    {
      "epoch": 5.692266666666667,
      "grad_norm": 0.36479294300079346,
      "learning_rate": 1.4423333333333333e-05,
      "loss": 0.0028,
      "step": 106730
    },
    {
      "epoch": 5.6928,
      "grad_norm": 0.05612403154373169,
      "learning_rate": 1.4420000000000001e-05,
      "loss": 0.0027,
      "step": 106740
    },
    {
      "epoch": 5.693333333333333,
      "grad_norm": 0.28061720728874207,
      "learning_rate": 1.4416666666666667e-05,
      "loss": 0.0026,
      "step": 106750
    },
    {
      "epoch": 5.693866666666667,
      "grad_norm": 0.3928600549697876,
      "learning_rate": 1.4413333333333335e-05,
      "loss": 0.003,
      "step": 106760
    },
    {
      "epoch": 5.6944,
      "grad_norm": 0.1403081864118576,
      "learning_rate": 1.4410000000000001e-05,
      "loss": 0.0034,
      "step": 106770
    },
    {
      "epoch": 5.694933333333333,
      "grad_norm": 0.19643644988536835,
      "learning_rate": 1.4406666666666669e-05,
      "loss": 0.0026,
      "step": 106780
    },
    {
      "epoch": 5.6954666666666665,
      "grad_norm": 0.1964275985956192,
      "learning_rate": 1.4403333333333335e-05,
      "loss": 0.0029,
      "step": 106790
    },
    {
      "epoch": 5.696,
      "grad_norm": 0.3086898624897003,
      "learning_rate": 1.44e-05,
      "loss": 0.0022,
      "step": 106800
    },
    {
      "epoch": 5.696533333333333,
      "grad_norm": 0.16838181018829346,
      "learning_rate": 1.4396666666666667e-05,
      "loss": 0.0028,
      "step": 106810
    },
    {
      "epoch": 5.697066666666666,
      "grad_norm": 0.0280607920140028,
      "learning_rate": 1.4393333333333333e-05,
      "loss": 0.0021,
      "step": 106820
    },
    {
      "epoch": 5.6975999999999996,
      "grad_norm": 0.14030727744102478,
      "learning_rate": 1.4390000000000001e-05,
      "loss": 0.002,
      "step": 106830
    },
    {
      "epoch": 5.698133333333334,
      "grad_norm": 3.6513767565082844e-09,
      "learning_rate": 1.4386666666666667e-05,
      "loss": 0.0035,
      "step": 106840
    },
    {
      "epoch": 5.698666666666667,
      "grad_norm": 0.30867233872413635,
      "learning_rate": 1.4383333333333335e-05,
      "loss": 0.0037,
      "step": 106850
    },
    {
      "epoch": 5.6992,
      "grad_norm": 0.16836771368980408,
      "learning_rate": 1.4380000000000001e-05,
      "loss": 0.0024,
      "step": 106860
    },
    {
      "epoch": 5.6997333333333335,
      "grad_norm": 0.33673667907714844,
      "learning_rate": 1.4376666666666666e-05,
      "loss": 0.0027,
      "step": 106870
    },
    {
      "epoch": 5.700266666666667,
      "grad_norm": 0.056122295558452606,
      "learning_rate": 1.4373333333333334e-05,
      "loss": 0.0036,
      "step": 106880
    },
    {
      "epoch": 5.7008,
      "grad_norm": 0.08418292552232742,
      "learning_rate": 1.437e-05,
      "loss": 0.0031,
      "step": 106890
    },
    {
      "epoch": 5.701333333333333,
      "grad_norm": 0.1964268684387207,
      "learning_rate": 1.4366666666666667e-05,
      "loss": 0.0028,
      "step": 106900
    },
    {
      "epoch": 5.701866666666667,
      "grad_norm": 0.16837891936302185,
      "learning_rate": 1.4363333333333334e-05,
      "loss": 0.0027,
      "step": 106910
    },
    {
      "epoch": 5.7024,
      "grad_norm": 0.06980586796998978,
      "learning_rate": 1.4360000000000001e-05,
      "loss": 0.0027,
      "step": 106920
    },
    {
      "epoch": 5.702933333333333,
      "grad_norm": 0.028061145916581154,
      "learning_rate": 1.4356666666666668e-05,
      "loss": 0.0029,
      "step": 106930
    },
    {
      "epoch": 5.7034666666666665,
      "grad_norm": 0.2525531053543091,
      "learning_rate": 1.4353333333333335e-05,
      "loss": 0.0026,
      "step": 106940
    },
    {
      "epoch": 5.704,
      "grad_norm": 0.3367283344268799,
      "learning_rate": 1.435e-05,
      "loss": 0.0022,
      "step": 106950
    },
    {
      "epoch": 5.704533333333333,
      "grad_norm": 3.2791565018897018e-09,
      "learning_rate": 1.4346666666666666e-05,
      "loss": 0.0024,
      "step": 106960
    },
    {
      "epoch": 5.705066666666666,
      "grad_norm": 0.02806144766509533,
      "learning_rate": 1.4343333333333334e-05,
      "loss": 0.0034,
      "step": 106970
    },
    {
      "epoch": 5.7056000000000004,
      "grad_norm": 0.056124087423086166,
      "learning_rate": 1.434e-05,
      "loss": 0.003,
      "step": 106980
    },
    {
      "epoch": 5.706133333333334,
      "grad_norm": 0.14030595123767853,
      "learning_rate": 1.4336666666666668e-05,
      "loss": 0.0026,
      "step": 106990
    },
    {
      "epoch": 5.706666666666667,
      "grad_norm": 0.02806108258664608,
      "learning_rate": 1.4333333333333334e-05,
      "loss": 0.0022,
      "step": 107000
    },
    {
      "epoch": 5.7072,
      "grad_norm": 7.682826819177535e-09,
      "learning_rate": 1.4330000000000002e-05,
      "loss": 0.0037,
      "step": 107010
    },
    {
      "epoch": 5.7077333333333335,
      "grad_norm": 0.18818248808383942,
      "learning_rate": 1.4326666666666666e-05,
      "loss": 0.0034,
      "step": 107020
    },
    {
      "epoch": 5.708266666666667,
      "grad_norm": 0.0280627328902483,
      "learning_rate": 1.4323333333333332e-05,
      "loss": 0.0035,
      "step": 107030
    },
    {
      "epoch": 5.7088,
      "grad_norm": 0.028062444180250168,
      "learning_rate": 1.432e-05,
      "loss": 0.0033,
      "step": 107040
    },
    {
      "epoch": 5.709333333333333,
      "grad_norm": 3.2006695072084312e-09,
      "learning_rate": 1.4316666666666668e-05,
      "loss": 0.0024,
      "step": 107050
    },
    {
      "epoch": 5.709866666666667,
      "grad_norm": 0.42093297839164734,
      "learning_rate": 1.4313333333333334e-05,
      "loss": 0.0032,
      "step": 107060
    },
    {
      "epoch": 5.7104,
      "grad_norm": 0.16836632788181305,
      "learning_rate": 1.4310000000000002e-05,
      "loss": 0.0021,
      "step": 107070
    },
    {
      "epoch": 5.710933333333333,
      "grad_norm": 0.05612240359187126,
      "learning_rate": 1.4306666666666668e-05,
      "loss": 0.0026,
      "step": 107080
    },
    {
      "epoch": 5.7114666666666665,
      "grad_norm": 0.3086795508861542,
      "learning_rate": 1.4303333333333336e-05,
      "loss": 0.0026,
      "step": 107090
    },
    {
      "epoch": 5.712,
      "grad_norm": 0.1964273452758789,
      "learning_rate": 1.43e-05,
      "loss": 0.002,
      "step": 107100
    },
    {
      "epoch": 5.712533333333333,
      "grad_norm": 7.990792916245937e-09,
      "learning_rate": 1.4296666666666666e-05,
      "loss": 0.0029,
      "step": 107110
    },
    {
      "epoch": 5.713066666666666,
      "grad_norm": 0.08418597280979156,
      "learning_rate": 1.4293333333333334e-05,
      "loss": 0.0027,
      "step": 107120
    },
    {
      "epoch": 5.7136,
      "grad_norm": 0.5051054358482361,
      "learning_rate": 1.429e-05,
      "loss": 0.0026,
      "step": 107130
    },
    {
      "epoch": 5.714133333333333,
      "grad_norm": 0.028062710538506508,
      "learning_rate": 1.4286666666666668e-05,
      "loss": 0.0027,
      "step": 107140
    },
    {
      "epoch": 5.714666666666667,
      "grad_norm": 0.28062501549720764,
      "learning_rate": 1.4283333333333334e-05,
      "loss": 0.0024,
      "step": 107150
    },
    {
      "epoch": 5.7152,
      "grad_norm": 0.3367352783679962,
      "learning_rate": 1.4280000000000002e-05,
      "loss": 0.0026,
      "step": 107160
    },
    {
      "epoch": 5.7157333333333336,
      "grad_norm": 0.28062117099761963,
      "learning_rate": 1.4276666666666667e-05,
      "loss": 0.0031,
      "step": 107170
    },
    {
      "epoch": 5.716266666666667,
      "grad_norm": 0.05612112209200859,
      "learning_rate": 1.4273333333333333e-05,
      "loss": 0.0029,
      "step": 107180
    },
    {
      "epoch": 5.7168,
      "grad_norm": 0.19642861187458038,
      "learning_rate": 1.427e-05,
      "loss": 0.0026,
      "step": 107190
    },
    {
      "epoch": 5.717333333333333,
      "grad_norm": 0.08418440818786621,
      "learning_rate": 1.4266666666666667e-05,
      "loss": 0.0031,
      "step": 107200
    },
    {
      "epoch": 5.717866666666667,
      "grad_norm": 0.16837111115455627,
      "learning_rate": 1.4263333333333335e-05,
      "loss": 0.0024,
      "step": 107210
    },
    {
      "epoch": 5.7184,
      "grad_norm": 0.08418339490890503,
      "learning_rate": 1.426e-05,
      "loss": 0.002,
      "step": 107220
    },
    {
      "epoch": 5.718933333333333,
      "grad_norm": 0.08418615162372589,
      "learning_rate": 1.4256666666666669e-05,
      "loss": 0.0033,
      "step": 107230
    },
    {
      "epoch": 5.7194666666666665,
      "grad_norm": 0.08418356627225876,
      "learning_rate": 1.4253333333333335e-05,
      "loss": 0.0039,
      "step": 107240
    },
    {
      "epoch": 5.72,
      "grad_norm": 0.16836965084075928,
      "learning_rate": 1.4249999999999999e-05,
      "loss": 0.0029,
      "step": 107250
    },
    {
      "epoch": 5.720533333333333,
      "grad_norm": 0.028061477467417717,
      "learning_rate": 1.4246666666666667e-05,
      "loss": 0.0028,
      "step": 107260
    },
    {
      "epoch": 5.721066666666666,
      "grad_norm": 0.05612199380993843,
      "learning_rate": 1.4243333333333333e-05,
      "loss": 0.0031,
      "step": 107270
    },
    {
      "epoch": 5.7216000000000005,
      "grad_norm": 0.1403062641620636,
      "learning_rate": 1.4240000000000001e-05,
      "loss": 0.002,
      "step": 107280
    },
    {
      "epoch": 5.722133333333334,
      "grad_norm": 0.28062164783477783,
      "learning_rate": 1.4236666666666667e-05,
      "loss": 0.003,
      "step": 107290
    },
    {
      "epoch": 5.722666666666667,
      "grad_norm": 0.196433424949646,
      "learning_rate": 1.4233333333333335e-05,
      "loss": 0.0027,
      "step": 107300
    },
    {
      "epoch": 5.7232,
      "grad_norm": 0.3647860586643219,
      "learning_rate": 1.4230000000000001e-05,
      "loss": 0.0025,
      "step": 107310
    },
    {
      "epoch": 5.723733333333334,
      "grad_norm": 0.0841854065656662,
      "learning_rate": 1.4226666666666669e-05,
      "loss": 0.0032,
      "step": 107320
    },
    {
      "epoch": 5.724266666666667,
      "grad_norm": 1.7652664885403624e-09,
      "learning_rate": 1.4223333333333333e-05,
      "loss": 0.0022,
      "step": 107330
    },
    {
      "epoch": 5.7248,
      "grad_norm": 0.28061622381210327,
      "learning_rate": 1.422e-05,
      "loss": 0.0035,
      "step": 107340
    },
    {
      "epoch": 5.725333333333333,
      "grad_norm": 0.19642095267772675,
      "learning_rate": 1.4216666666666667e-05,
      "loss": 0.0032,
      "step": 107350
    },
    {
      "epoch": 5.725866666666667,
      "grad_norm": 0.02806093916296959,
      "learning_rate": 1.4213333333333333e-05,
      "loss": 0.0022,
      "step": 107360
    },
    {
      "epoch": 5.7264,
      "grad_norm": 0.08418106287717819,
      "learning_rate": 1.4210000000000001e-05,
      "loss": 0.0027,
      "step": 107370
    },
    {
      "epoch": 5.726933333333333,
      "grad_norm": 0.1964298039674759,
      "learning_rate": 1.4206666666666667e-05,
      "loss": 0.0022,
      "step": 107380
    },
    {
      "epoch": 5.7274666666666665,
      "grad_norm": 0.1122460812330246,
      "learning_rate": 1.4203333333333335e-05,
      "loss": 0.0026,
      "step": 107390
    },
    {
      "epoch": 5.728,
      "grad_norm": 0.11224498599767685,
      "learning_rate": 1.42e-05,
      "loss": 0.002,
      "step": 107400
    },
    {
      "epoch": 5.728533333333333,
      "grad_norm": 0.028060544282197952,
      "learning_rate": 1.4196666666666666e-05,
      "loss": 0.0025,
      "step": 107410
    },
    {
      "epoch": 5.729066666666666,
      "grad_norm": 0.36477747559547424,
      "learning_rate": 1.4193333333333334e-05,
      "loss": 0.0021,
      "step": 107420
    },
    {
      "epoch": 5.7296,
      "grad_norm": 0.19642460346221924,
      "learning_rate": 1.4190000000000001e-05,
      "loss": 0.0028,
      "step": 107430
    },
    {
      "epoch": 5.730133333333333,
      "grad_norm": 0.5331575274467468,
      "learning_rate": 1.4186666666666667e-05,
      "loss": 0.0017,
      "step": 107440
    },
    {
      "epoch": 5.730666666666667,
      "grad_norm": 0.16835947334766388,
      "learning_rate": 1.4183333333333335e-05,
      "loss": 0.0023,
      "step": 107450
    },
    {
      "epoch": 5.7312,
      "grad_norm": 0.28059303760528564,
      "learning_rate": 1.4180000000000001e-05,
      "loss": 0.0033,
      "step": 107460
    },
    {
      "epoch": 5.731733333333334,
      "grad_norm": 0.11708910763263702,
      "learning_rate": 1.417666666666667e-05,
      "loss": 0.0033,
      "step": 107470
    },
    {
      "epoch": 5.732266666666667,
      "grad_norm": 0.1964220255613327,
      "learning_rate": 1.4173333333333334e-05,
      "loss": 0.0032,
      "step": 107480
    },
    {
      "epoch": 5.7328,
      "grad_norm": 0.2244751900434494,
      "learning_rate": 1.417e-05,
      "loss": 0.0019,
      "step": 107490
    },
    {
      "epoch": 5.733333333333333,
      "grad_norm": 0.14030013978481293,
      "learning_rate": 1.4166666666666668e-05,
      "loss": 0.0031,
      "step": 107500
    },
    {
      "epoch": 5.733866666666667,
      "grad_norm": 0.25254321098327637,
      "learning_rate": 1.4163333333333334e-05,
      "loss": 0.0022,
      "step": 107510
    },
    {
      "epoch": 5.7344,
      "grad_norm": 1.4551740923351986e-09,
      "learning_rate": 1.4160000000000002e-05,
      "loss": 0.0029,
      "step": 107520
    },
    {
      "epoch": 5.734933333333333,
      "grad_norm": 0.11224067211151123,
      "learning_rate": 1.4156666666666668e-05,
      "loss": 0.0022,
      "step": 107530
    },
    {
      "epoch": 5.7354666666666665,
      "grad_norm": 0.19642645120620728,
      "learning_rate": 1.4153333333333336e-05,
      "loss": 0.0027,
      "step": 107540
    },
    {
      "epoch": 5.736,
      "grad_norm": 0.02806081995368004,
      "learning_rate": 1.415e-05,
      "loss": 0.0032,
      "step": 107550
    },
    {
      "epoch": 5.736533333333333,
      "grad_norm": 0.17553873360157013,
      "learning_rate": 1.4146666666666666e-05,
      "loss": 0.0029,
      "step": 107560
    },
    {
      "epoch": 5.737066666666666,
      "grad_norm": 0.2806116044521332,
      "learning_rate": 1.4143333333333334e-05,
      "loss": 0.0026,
      "step": 107570
    },
    {
      "epoch": 5.7376000000000005,
      "grad_norm": 0.5331224203109741,
      "learning_rate": 1.414e-05,
      "loss": 0.0037,
      "step": 107580
    },
    {
      "epoch": 5.738133333333334,
      "grad_norm": 0.42090320587158203,
      "learning_rate": 1.4136666666666668e-05,
      "loss": 0.0023,
      "step": 107590
    },
    {
      "epoch": 5.738666666666667,
      "grad_norm": 0.20230986177921295,
      "learning_rate": 1.4133333333333334e-05,
      "loss": 0.0031,
      "step": 107600
    },
    {
      "epoch": 5.7392,
      "grad_norm": 0.02805997058749199,
      "learning_rate": 1.4130000000000002e-05,
      "loss": 0.0033,
      "step": 107610
    },
    {
      "epoch": 5.739733333333334,
      "grad_norm": 0.28059789538383484,
      "learning_rate": 1.4126666666666668e-05,
      "loss": 0.002,
      "step": 107620
    },
    {
      "epoch": 5.740266666666667,
      "grad_norm": 0.028060343116521835,
      "learning_rate": 1.4123333333333333e-05,
      "loss": 0.0032,
      "step": 107630
    },
    {
      "epoch": 5.7408,
      "grad_norm": 0.11224232614040375,
      "learning_rate": 1.412e-05,
      "loss": 0.0023,
      "step": 107640
    },
    {
      "epoch": 5.741333333333333,
      "grad_norm": 0.19641287624835968,
      "learning_rate": 1.4116666666666666e-05,
      "loss": 0.0038,
      "step": 107650
    },
    {
      "epoch": 5.741866666666667,
      "grad_norm": 0.056118689477443695,
      "learning_rate": 1.4113333333333334e-05,
      "loss": 0.0021,
      "step": 107660
    },
    {
      "epoch": 5.7424,
      "grad_norm": 1.1330863237380981,
      "learning_rate": 1.411e-05,
      "loss": 0.0032,
      "step": 107670
    },
    {
      "epoch": 5.742933333333333,
      "grad_norm": 0.1691860407590866,
      "learning_rate": 1.4106666666666668e-05,
      "loss": 0.0021,
      "step": 107680
    },
    {
      "epoch": 5.7434666666666665,
      "grad_norm": 0.05612342432141304,
      "learning_rate": 1.4103333333333334e-05,
      "loss": 0.0023,
      "step": 107690
    },
    {
      "epoch": 5.744,
      "grad_norm": 0.1683552861213684,
      "learning_rate": 1.4099999999999999e-05,
      "loss": 0.0024,
      "step": 107700
    },
    {
      "epoch": 5.744533333333333,
      "grad_norm": 0.39283323287963867,
      "learning_rate": 1.4096666666666667e-05,
      "loss": 0.0021,
      "step": 107710
    },
    {
      "epoch": 5.745066666666666,
      "grad_norm": 0.08418416976928711,
      "learning_rate": 1.4093333333333333e-05,
      "loss": 0.0027,
      "step": 107720
    },
    {
      "epoch": 5.7456,
      "grad_norm": 0.5003349781036377,
      "learning_rate": 1.409e-05,
      "loss": 0.003,
      "step": 107730
    },
    {
      "epoch": 5.746133333333333,
      "grad_norm": 0.02806018851697445,
      "learning_rate": 1.4086666666666667e-05,
      "loss": 0.0038,
      "step": 107740
    },
    {
      "epoch": 5.746666666666667,
      "grad_norm": 0.30865103006362915,
      "learning_rate": 1.4083333333333335e-05,
      "loss": 0.0026,
      "step": 107750
    },
    {
      "epoch": 5.7472,
      "grad_norm": 0.22448208928108215,
      "learning_rate": 1.408e-05,
      "loss": 0.0038,
      "step": 107760
    },
    {
      "epoch": 5.747733333333334,
      "grad_norm": 0.2244768887758255,
      "learning_rate": 1.4076666666666669e-05,
      "loss": 0.0017,
      "step": 107770
    },
    {
      "epoch": 5.748266666666667,
      "grad_norm": 0.2525446116924286,
      "learning_rate": 1.4073333333333333e-05,
      "loss": 0.0029,
      "step": 107780
    },
    {
      "epoch": 5.7488,
      "grad_norm": 0.14030231535434723,
      "learning_rate": 1.4069999999999999e-05,
      "loss": 0.0045,
      "step": 107790
    },
    {
      "epoch": 5.749333333333333,
      "grad_norm": 0.4770245850086212,
      "learning_rate": 1.4066666666666667e-05,
      "loss": 0.0036,
      "step": 107800
    },
    {
      "epoch": 5.749866666666667,
      "grad_norm": 0.05612028017640114,
      "learning_rate": 1.4063333333333333e-05,
      "loss": 0.0028,
      "step": 107810
    },
    {
      "epoch": 5.7504,
      "grad_norm": 0.05612107366323471,
      "learning_rate": 1.4060000000000001e-05,
      "loss": 0.0028,
      "step": 107820
    },
    {
      "epoch": 5.750933333333333,
      "grad_norm": 0.1683594137430191,
      "learning_rate": 1.4056666666666669e-05,
      "loss": 0.0023,
      "step": 107830
    },
    {
      "epoch": 5.7514666666666665,
      "grad_norm": 0.364788681268692,
      "learning_rate": 1.4053333333333335e-05,
      "loss": 0.0027,
      "step": 107840
    },
    {
      "epoch": 5.752,
      "grad_norm": 0.056121207773685455,
      "learning_rate": 1.4050000000000003e-05,
      "loss": 0.0013,
      "step": 107850
    },
    {
      "epoch": 5.752533333333333,
      "grad_norm": 0.05612083151936531,
      "learning_rate": 1.4046666666666667e-05,
      "loss": 0.0043,
      "step": 107860
    },
    {
      "epoch": 5.753066666666666,
      "grad_norm": 0.11223939061164856,
      "learning_rate": 1.4043333333333333e-05,
      "loss": 0.002,
      "step": 107870
    },
    {
      "epoch": 5.7536000000000005,
      "grad_norm": 0.056118398904800415,
      "learning_rate": 1.4040000000000001e-05,
      "loss": 0.0022,
      "step": 107880
    },
    {
      "epoch": 5.754133333333334,
      "grad_norm": 0.05611696466803551,
      "learning_rate": 1.4036666666666667e-05,
      "loss": 0.0028,
      "step": 107890
    },
    {
      "epoch": 5.754666666666667,
      "grad_norm": 0.16836154460906982,
      "learning_rate": 1.4033333333333335e-05,
      "loss": 0.0032,
      "step": 107900
    },
    {
      "epoch": 5.7552,
      "grad_norm": 0.112238310277462,
      "learning_rate": 1.4030000000000001e-05,
      "loss": 0.0031,
      "step": 107910
    },
    {
      "epoch": 5.755733333333334,
      "grad_norm": 0.028058690950274467,
      "learning_rate": 1.4026666666666669e-05,
      "loss": 0.0022,
      "step": 107920
    },
    {
      "epoch": 5.756266666666667,
      "grad_norm": 0.2298731654882431,
      "learning_rate": 1.4023333333333333e-05,
      "loss": 0.0022,
      "step": 107930
    },
    {
      "epoch": 5.7568,
      "grad_norm": 0.11223739385604858,
      "learning_rate": 1.402e-05,
      "loss": 0.0032,
      "step": 107940
    },
    {
      "epoch": 5.757333333333333,
      "grad_norm": 0.05611713230609894,
      "learning_rate": 1.4016666666666667e-05,
      "loss": 0.0042,
      "step": 107950
    },
    {
      "epoch": 5.757866666666667,
      "grad_norm": 0.2525363266468048,
      "learning_rate": 1.4013333333333334e-05,
      "loss": 0.0038,
      "step": 107960
    },
    {
      "epoch": 5.7584,
      "grad_norm": 0.2244882434606552,
      "learning_rate": 1.4010000000000001e-05,
      "loss": 0.003,
      "step": 107970
    },
    {
      "epoch": 5.758933333333333,
      "grad_norm": 0.19641615450382233,
      "learning_rate": 1.4006666666666668e-05,
      "loss": 0.0027,
      "step": 107980
    },
    {
      "epoch": 5.7594666666666665,
      "grad_norm": 0.14029356837272644,
      "learning_rate": 1.4003333333333335e-05,
      "loss": 0.0022,
      "step": 107990
    },
    {
      "epoch": 5.76,
      "grad_norm": 0.14029286801815033,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.0039,
      "step": 108000
    },
    {
      "epoch": 5.760533333333333,
      "grad_norm": 0.16835130751132965,
      "learning_rate": 1.3996666666666666e-05,
      "loss": 0.0022,
      "step": 108010
    },
    {
      "epoch": 5.761066666666666,
      "grad_norm": 2.74825406876289e-09,
      "learning_rate": 1.3993333333333334e-05,
      "loss": 0.0033,
      "step": 108020
    },
    {
      "epoch": 5.7616,
      "grad_norm": 0.38109228014945984,
      "learning_rate": 1.399e-05,
      "loss": 0.0029,
      "step": 108030
    },
    {
      "epoch": 5.762133333333333,
      "grad_norm": 0.14029686152935028,
      "learning_rate": 1.3986666666666668e-05,
      "loss": 0.0029,
      "step": 108040
    },
    {
      "epoch": 5.762666666666667,
      "grad_norm": 0.22448121011257172,
      "learning_rate": 1.3983333333333334e-05,
      "loss": 0.0033,
      "step": 108050
    },
    {
      "epoch": 5.7632,
      "grad_norm": 0.1402931958436966,
      "learning_rate": 1.3980000000000002e-05,
      "loss": 0.0042,
      "step": 108060
    },
    {
      "epoch": 5.763733333333334,
      "grad_norm": 0.22447912395000458,
      "learning_rate": 1.3976666666666668e-05,
      "loss": 0.0031,
      "step": 108070
    },
    {
      "epoch": 5.764266666666667,
      "grad_norm": 0.25253260135650635,
      "learning_rate": 1.3973333333333332e-05,
      "loss": 0.0026,
      "step": 108080
    },
    {
      "epoch": 5.7648,
      "grad_norm": 8.524576600166256e-09,
      "learning_rate": 1.397e-05,
      "loss": 0.0031,
      "step": 108090
    },
    {
      "epoch": 5.765333333333333,
      "grad_norm": 0.11223933845758438,
      "learning_rate": 1.3966666666666666e-05,
      "loss": 0.0021,
      "step": 108100
    },
    {
      "epoch": 5.765866666666667,
      "grad_norm": 0.05611800402402878,
      "learning_rate": 1.3963333333333334e-05,
      "loss": 0.0028,
      "step": 108110
    },
    {
      "epoch": 5.7664,
      "grad_norm": 0.056120049208402634,
      "learning_rate": 1.396e-05,
      "loss": 0.002,
      "step": 108120
    },
    {
      "epoch": 5.766933333333333,
      "grad_norm": 0.08418205380439758,
      "learning_rate": 1.3956666666666668e-05,
      "loss": 0.0027,
      "step": 108130
    },
    {
      "epoch": 5.7674666666666665,
      "grad_norm": 0.08417726308107376,
      "learning_rate": 1.3953333333333334e-05,
      "loss": 0.002,
      "step": 108140
    },
    {
      "epoch": 5.768,
      "grad_norm": 0.08417993038892746,
      "learning_rate": 1.3950000000000002e-05,
      "loss": 0.0029,
      "step": 108150
    },
    {
      "epoch": 5.768533333333333,
      "grad_norm": 0.11224626004695892,
      "learning_rate": 1.3946666666666666e-05,
      "loss": 0.0019,
      "step": 108160
    },
    {
      "epoch": 5.769066666666666,
      "grad_norm": 0.2806098759174347,
      "learning_rate": 1.3943333333333333e-05,
      "loss": 0.003,
      "step": 108170
    },
    {
      "epoch": 5.7696,
      "grad_norm": 0.364765465259552,
      "learning_rate": 1.394e-05,
      "loss": 0.003,
      "step": 108180
    },
    {
      "epoch": 5.770133333333334,
      "grad_norm": 0.11223649978637695,
      "learning_rate": 1.3936666666666666e-05,
      "loss": 0.0021,
      "step": 108190
    },
    {
      "epoch": 5.770666666666667,
      "grad_norm": 0.02805939130485058,
      "learning_rate": 1.3933333333333334e-05,
      "loss": 0.0038,
      "step": 108200
    },
    {
      "epoch": 5.7712,
      "grad_norm": 0.05611695349216461,
      "learning_rate": 1.3930000000000002e-05,
      "loss": 0.003,
      "step": 108210
    },
    {
      "epoch": 5.771733333333334,
      "grad_norm": 1.0512780646010356e-09,
      "learning_rate": 1.3926666666666668e-05,
      "loss": 0.0038,
      "step": 108220
    },
    {
      "epoch": 5.772266666666667,
      "grad_norm": 0.11224108189344406,
      "learning_rate": 1.3923333333333333e-05,
      "loss": 0.003,
      "step": 108230
    },
    {
      "epoch": 5.7728,
      "grad_norm": 0.1964128613471985,
      "learning_rate": 1.3919999999999999e-05,
      "loss": 0.0039,
      "step": 108240
    },
    {
      "epoch": 5.773333333333333,
      "grad_norm": 0.19640904664993286,
      "learning_rate": 1.3916666666666667e-05,
      "loss": 0.0032,
      "step": 108250
    },
    {
      "epoch": 5.773866666666667,
      "grad_norm": 0.028060631826519966,
      "learning_rate": 1.3913333333333335e-05,
      "loss": 0.003,
      "step": 108260
    },
    {
      "epoch": 5.7744,
      "grad_norm": 0.3086531162261963,
      "learning_rate": 1.391e-05,
      "loss": 0.0032,
      "step": 108270
    },
    {
      "epoch": 5.774933333333333,
      "grad_norm": 0.05611693486571312,
      "learning_rate": 1.3906666666666668e-05,
      "loss": 0.0021,
      "step": 108280
    },
    {
      "epoch": 5.7754666666666665,
      "grad_norm": 0.02806009165942669,
      "learning_rate": 1.3903333333333335e-05,
      "loss": 0.0022,
      "step": 108290
    },
    {
      "epoch": 5.776,
      "grad_norm": 4.544465248557117e-09,
      "learning_rate": 1.3900000000000002e-05,
      "loss": 0.0025,
      "step": 108300
    },
    {
      "epoch": 5.776533333333333,
      "grad_norm": 0.028059840202331543,
      "learning_rate": 1.3896666666666667e-05,
      "loss": 0.0025,
      "step": 108310
    },
    {
      "epoch": 5.777066666666666,
      "grad_norm": 0.028058355674147606,
      "learning_rate": 1.3893333333333333e-05,
      "loss": 0.0027,
      "step": 108320
    },
    {
      "epoch": 5.7776,
      "grad_norm": 0.08418021351099014,
      "learning_rate": 1.389e-05,
      "loss": 0.0027,
      "step": 108330
    },
    {
      "epoch": 5.778133333333333,
      "grad_norm": 0.16835041344165802,
      "learning_rate": 1.3886666666666667e-05,
      "loss": 0.0018,
      "step": 108340
    },
    {
      "epoch": 5.778666666666666,
      "grad_norm": 0.028058534488081932,
      "learning_rate": 1.3883333333333335e-05,
      "loss": 0.0024,
      "step": 108350
    },
    {
      "epoch": 5.7792,
      "grad_norm": 0.08418132364749908,
      "learning_rate": 1.3880000000000001e-05,
      "loss": 0.0039,
      "step": 108360
    },
    {
      "epoch": 5.779733333333334,
      "grad_norm": 0.1122402772307396,
      "learning_rate": 1.3876666666666669e-05,
      "loss": 0.0025,
      "step": 108370
    },
    {
      "epoch": 5.780266666666667,
      "grad_norm": 0.028059042990207672,
      "learning_rate": 1.3873333333333333e-05,
      "loss": 0.0019,
      "step": 108380
    },
    {
      "epoch": 5.7808,
      "grad_norm": 0.19640332460403442,
      "learning_rate": 1.387e-05,
      "loss": 0.0021,
      "step": 108390
    },
    {
      "epoch": 5.781333333333333,
      "grad_norm": 0.1964135468006134,
      "learning_rate": 1.3866666666666667e-05,
      "loss": 0.0026,
      "step": 108400
    },
    {
      "epoch": 5.781866666666667,
      "grad_norm": 0.05611537769436836,
      "learning_rate": 1.3863333333333333e-05,
      "loss": 0.0016,
      "step": 108410
    },
    {
      "epoch": 5.7824,
      "grad_norm": 0.056117262691259384,
      "learning_rate": 1.3860000000000001e-05,
      "loss": 0.0024,
      "step": 108420
    },
    {
      "epoch": 5.782933333333333,
      "grad_norm": 0.2244814932346344,
      "learning_rate": 1.3856666666666667e-05,
      "loss": 0.0039,
      "step": 108430
    },
    {
      "epoch": 5.7834666666666665,
      "grad_norm": 0.06417087465524673,
      "learning_rate": 1.3853333333333335e-05,
      "loss": 0.0033,
      "step": 108440
    },
    {
      "epoch": 5.784,
      "grad_norm": 0.08417396247386932,
      "learning_rate": 1.3850000000000001e-05,
      "loss": 0.0019,
      "step": 108450
    },
    {
      "epoch": 5.784533333333333,
      "grad_norm": 0.11223199963569641,
      "learning_rate": 1.3846666666666666e-05,
      "loss": 0.0018,
      "step": 108460
    },
    {
      "epoch": 5.785066666666666,
      "grad_norm": 0.25253912806510925,
      "learning_rate": 1.3843333333333333e-05,
      "loss": 0.0035,
      "step": 108470
    },
    {
      "epoch": 5.7856,
      "grad_norm": 0.3647478222846985,
      "learning_rate": 1.384e-05,
      "loss": 0.0025,
      "step": 108480
    },
    {
      "epoch": 5.786133333333334,
      "grad_norm": 0.02805820107460022,
      "learning_rate": 1.3836666666666667e-05,
      "loss": 0.0025,
      "step": 108490
    },
    {
      "epoch": 5.786666666666667,
      "grad_norm": 0.22447220981121063,
      "learning_rate": 1.3833333333333334e-05,
      "loss": 0.0025,
      "step": 108500
    },
    {
      "epoch": 5.7872,
      "grad_norm": 0.08417224884033203,
      "learning_rate": 1.3830000000000001e-05,
      "loss": 0.0017,
      "step": 108510
    },
    {
      "epoch": 5.787733333333334,
      "grad_norm": 0.14029265940189362,
      "learning_rate": 1.3826666666666668e-05,
      "loss": 0.0035,
      "step": 108520
    },
    {
      "epoch": 5.788266666666667,
      "grad_norm": 0.08417915552854538,
      "learning_rate": 1.3823333333333335e-05,
      "loss": 0.0019,
      "step": 108530
    },
    {
      "epoch": 5.7888,
      "grad_norm": 0.028057530522346497,
      "learning_rate": 1.382e-05,
      "loss": 0.0022,
      "step": 108540
    },
    {
      "epoch": 5.789333333333333,
      "grad_norm": 0.5330796241760254,
      "learning_rate": 1.3816666666666666e-05,
      "loss": 0.0038,
      "step": 108550
    },
    {
      "epoch": 5.789866666666667,
      "grad_norm": 0.02805926650762558,
      "learning_rate": 1.3813333333333334e-05,
      "loss": 0.0018,
      "step": 108560
    },
    {
      "epoch": 5.7904,
      "grad_norm": 0.19641494750976562,
      "learning_rate": 1.381e-05,
      "loss": 0.0032,
      "step": 108570
    },
    {
      "epoch": 5.790933333333333,
      "grad_norm": 0.1683599203824997,
      "learning_rate": 1.3806666666666668e-05,
      "loss": 0.0021,
      "step": 108580
    },
    {
      "epoch": 5.7914666666666665,
      "grad_norm": 0.16834963858127594,
      "learning_rate": 1.3803333333333336e-05,
      "loss": 0.0024,
      "step": 108590
    },
    {
      "epoch": 5.792,
      "grad_norm": 0.280567467212677,
      "learning_rate": 1.3800000000000002e-05,
      "loss": 0.0035,
      "step": 108600
    },
    {
      "epoch": 5.792533333333333,
      "grad_norm": 0.14029239118099213,
      "learning_rate": 1.3796666666666666e-05,
      "loss": 0.0034,
      "step": 108610
    },
    {
      "epoch": 5.793066666666666,
      "grad_norm": 0.30862951278686523,
      "learning_rate": 1.3793333333333332e-05,
      "loss": 0.0027,
      "step": 108620
    },
    {
      "epoch": 5.7936,
      "grad_norm": 0.33668968081474304,
      "learning_rate": 1.379e-05,
      "loss": 0.0026,
      "step": 108630
    },
    {
      "epoch": 5.794133333333333,
      "grad_norm": 0.19640134274959564,
      "learning_rate": 1.3786666666666668e-05,
      "loss": 0.0036,
      "step": 108640
    },
    {
      "epoch": 5.794666666666666,
      "grad_norm": 0.11222979426383972,
      "learning_rate": 1.3783333333333334e-05,
      "loss": 0.0024,
      "step": 108650
    },
    {
      "epoch": 5.7952,
      "grad_norm": 0.08417393267154694,
      "learning_rate": 1.3780000000000002e-05,
      "loss": 0.003,
      "step": 108660
    },
    {
      "epoch": 5.795733333333334,
      "grad_norm": 0.028058629482984543,
      "learning_rate": 1.3776666666666668e-05,
      "loss": 0.0035,
      "step": 108670
    },
    {
      "epoch": 5.796266666666667,
      "grad_norm": 0.16834881901741028,
      "learning_rate": 1.3773333333333336e-05,
      "loss": 0.0032,
      "step": 108680
    },
    {
      "epoch": 5.7968,
      "grad_norm": 0.14029309153556824,
      "learning_rate": 1.377e-05,
      "loss": 0.0027,
      "step": 108690
    },
    {
      "epoch": 5.7973333333333334,
      "grad_norm": 0.028057396411895752,
      "learning_rate": 1.3766666666666666e-05,
      "loss": 0.0027,
      "step": 108700
    },
    {
      "epoch": 5.797866666666667,
      "grad_norm": 0.16834579408168793,
      "learning_rate": 1.3763333333333334e-05,
      "loss": 0.0042,
      "step": 108710
    },
    {
      "epoch": 5.7984,
      "grad_norm": 0.05611443519592285,
      "learning_rate": 1.376e-05,
      "loss": 0.0024,
      "step": 108720
    },
    {
      "epoch": 5.798933333333333,
      "grad_norm": 0.02805701456964016,
      "learning_rate": 1.3756666666666668e-05,
      "loss": 0.0023,
      "step": 108730
    },
    {
      "epoch": 5.7994666666666665,
      "grad_norm": 0.08417589962482452,
      "learning_rate": 1.3753333333333334e-05,
      "loss": 0.0024,
      "step": 108740
    },
    {
      "epoch": 5.8,
      "grad_norm": 0.0841759443283081,
      "learning_rate": 1.3750000000000002e-05,
      "loss": 0.0026,
      "step": 108750
    },
    {
      "epoch": 5.800533333333333,
      "grad_norm": 0.16834235191345215,
      "learning_rate": 1.3746666666666667e-05,
      "loss": 0.0018,
      "step": 108760
    },
    {
      "epoch": 5.801066666666666,
      "grad_norm": 0.11222553253173828,
      "learning_rate": 1.3743333333333333e-05,
      "loss": 0.0027,
      "step": 108770
    },
    {
      "epoch": 5.8016,
      "grad_norm": 2.2496662310800275e-09,
      "learning_rate": 1.374e-05,
      "loss": 0.0027,
      "step": 108780
    },
    {
      "epoch": 5.802133333333334,
      "grad_norm": 0.19639462232589722,
      "learning_rate": 1.3736666666666667e-05,
      "loss": 0.0023,
      "step": 108790
    },
    {
      "epoch": 5.802666666666667,
      "grad_norm": 0.35153335332870483,
      "learning_rate": 1.3733333333333335e-05,
      "loss": 0.0033,
      "step": 108800
    },
    {
      "epoch": 5.8032,
      "grad_norm": 4.6524459840213694e-09,
      "learning_rate": 1.373e-05,
      "loss": 0.0029,
      "step": 108810
    },
    {
      "epoch": 5.803733333333334,
      "grad_norm": 0.16834333539009094,
      "learning_rate": 1.3726666666666669e-05,
      "loss": 0.0021,
      "step": 108820
    },
    {
      "epoch": 5.804266666666667,
      "grad_norm": 0.25250083208084106,
      "learning_rate": 1.3723333333333335e-05,
      "loss": 0.0022,
      "step": 108830
    },
    {
      "epoch": 5.8048,
      "grad_norm": 0.056112829595804214,
      "learning_rate": 1.3719999999999999e-05,
      "loss": 0.0022,
      "step": 108840
    },
    {
      "epoch": 5.8053333333333335,
      "grad_norm": 0.05611443147063255,
      "learning_rate": 1.3716666666666667e-05,
      "loss": 0.0023,
      "step": 108850
    },
    {
      "epoch": 5.805866666666667,
      "grad_norm": 0.08417201787233353,
      "learning_rate": 1.3713333333333333e-05,
      "loss": 0.0025,
      "step": 108860
    },
    {
      "epoch": 5.8064,
      "grad_norm": 0.08417161554098129,
      "learning_rate": 1.3710000000000001e-05,
      "loss": 0.0031,
      "step": 108870
    },
    {
      "epoch": 5.806933333333333,
      "grad_norm": 0.16834236681461334,
      "learning_rate": 1.3706666666666667e-05,
      "loss": 0.0021,
      "step": 108880
    },
    {
      "epoch": 5.8074666666666666,
      "grad_norm": 0.056112684309482574,
      "learning_rate": 1.3703333333333335e-05,
      "loss": 0.0027,
      "step": 108890
    },
    {
      "epoch": 5.808,
      "grad_norm": 0.08416856080293655,
      "learning_rate": 1.3700000000000001e-05,
      "loss": 0.003,
      "step": 108900
    },
    {
      "epoch": 5.808533333333333,
      "grad_norm": 0.30861154198646545,
      "learning_rate": 1.3696666666666665e-05,
      "loss": 0.0021,
      "step": 108910
    },
    {
      "epoch": 5.809066666666666,
      "grad_norm": 0.028056664392352104,
      "learning_rate": 1.3693333333333333e-05,
      "loss": 0.0028,
      "step": 108920
    },
    {
      "epoch": 5.8096,
      "grad_norm": 0.028056137263774872,
      "learning_rate": 1.369e-05,
      "loss": 0.002,
      "step": 108930
    },
    {
      "epoch": 5.810133333333333,
      "grad_norm": 0.22446110844612122,
      "learning_rate": 1.3686666666666667e-05,
      "loss": 0.004,
      "step": 108940
    },
    {
      "epoch": 5.810666666666666,
      "grad_norm": 0.08417476713657379,
      "learning_rate": 1.3683333333333333e-05,
      "loss": 0.0028,
      "step": 108950
    },
    {
      "epoch": 5.8112,
      "grad_norm": 0.1402803510427475,
      "learning_rate": 1.3680000000000001e-05,
      "loss": 0.0023,
      "step": 108960
    },
    {
      "epoch": 5.811733333333334,
      "grad_norm": 0.308610737323761,
      "learning_rate": 1.3676666666666669e-05,
      "loss": 0.0036,
      "step": 108970
    },
    {
      "epoch": 5.812266666666667,
      "grad_norm": 0.05611352249979973,
      "learning_rate": 1.3673333333333335e-05,
      "loss": 0.0021,
      "step": 108980
    },
    {
      "epoch": 5.8128,
      "grad_norm": 0.14027701318264008,
      "learning_rate": 1.367e-05,
      "loss": 0.0036,
      "step": 108990
    },
    {
      "epoch": 5.8133333333333335,
      "grad_norm": 0.23987187445163727,
      "learning_rate": 1.3666666666666666e-05,
      "loss": 0.0038,
      "step": 109000
    },
    {
      "epoch": 5.813866666666667,
      "grad_norm": 0.16834311187267303,
      "learning_rate": 1.3663333333333334e-05,
      "loss": 0.0022,
      "step": 109010
    },
    {
      "epoch": 5.8144,
      "grad_norm": 0.08416939526796341,
      "learning_rate": 1.3660000000000001e-05,
      "loss": 0.0033,
      "step": 109020
    },
    {
      "epoch": 5.814933333333333,
      "grad_norm": 0.028056461364030838,
      "learning_rate": 1.3656666666666667e-05,
      "loss": 0.0017,
      "step": 109030
    },
    {
      "epoch": 5.815466666666667,
      "grad_norm": 0.2244420349597931,
      "learning_rate": 1.3653333333333335e-05,
      "loss": 0.0015,
      "step": 109040
    },
    {
      "epoch": 5.816,
      "grad_norm": 0.09036797285079956,
      "learning_rate": 1.3650000000000001e-05,
      "loss": 0.0027,
      "step": 109050
    },
    {
      "epoch": 5.816533333333333,
      "grad_norm": 0.14028127491474152,
      "learning_rate": 1.3646666666666666e-05,
      "loss": 0.0027,
      "step": 109060
    },
    {
      "epoch": 5.817066666666666,
      "grad_norm": 0.11222673952579498,
      "learning_rate": 1.3643333333333334e-05,
      "loss": 0.0037,
      "step": 109070
    },
    {
      "epoch": 5.8176,
      "grad_norm": 0.08416702598333359,
      "learning_rate": 1.364e-05,
      "loss": 0.0018,
      "step": 109080
    },
    {
      "epoch": 5.818133333333334,
      "grad_norm": 0.028056148439645767,
      "learning_rate": 1.3636666666666668e-05,
      "loss": 0.0026,
      "step": 109090
    },
    {
      "epoch": 5.818666666666667,
      "grad_norm": 0.3086041808128357,
      "learning_rate": 1.3633333333333334e-05,
      "loss": 0.0013,
      "step": 109100
    },
    {
      "epoch": 5.8192,
      "grad_norm": 0.11222045868635178,
      "learning_rate": 1.3630000000000002e-05,
      "loss": 0.0027,
      "step": 109110
    },
    {
      "epoch": 5.819733333333334,
      "grad_norm": 0.14027465879917145,
      "learning_rate": 1.3626666666666668e-05,
      "loss": 0.0023,
      "step": 109120
    },
    {
      "epoch": 5.820266666666667,
      "grad_norm": 0.30860403180122375,
      "learning_rate": 1.3623333333333336e-05,
      "loss": 0.0024,
      "step": 109130
    },
    {
      "epoch": 5.8208,
      "grad_norm": 0.2244495302438736,
      "learning_rate": 1.362e-05,
      "loss": 0.0031,
      "step": 109140
    },
    {
      "epoch": 5.8213333333333335,
      "grad_norm": 0.33667314052581787,
      "learning_rate": 1.3616666666666666e-05,
      "loss": 0.0032,
      "step": 109150
    },
    {
      "epoch": 5.821866666666667,
      "grad_norm": 0.11221860349178314,
      "learning_rate": 1.3613333333333334e-05,
      "loss": 0.0033,
      "step": 109160
    },
    {
      "epoch": 5.8224,
      "grad_norm": 0.16834299266338348,
      "learning_rate": 1.361e-05,
      "loss": 0.0023,
      "step": 109170
    },
    {
      "epoch": 5.822933333333333,
      "grad_norm": 0.02805623970925808,
      "learning_rate": 1.3606666666666668e-05,
      "loss": 0.0035,
      "step": 109180
    },
    {
      "epoch": 5.823466666666667,
      "grad_norm": 0.11222507804632187,
      "learning_rate": 1.3603333333333334e-05,
      "loss": 0.0037,
      "step": 109190
    },
    {
      "epoch": 5.824,
      "grad_norm": 0.19638262689113617,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 0.0029,
      "step": 109200
    },
    {
      "epoch": 5.824533333333333,
      "grad_norm": 0.056109242141246796,
      "learning_rate": 1.3596666666666668e-05,
      "loss": 0.0032,
      "step": 109210
    },
    {
      "epoch": 5.825066666666666,
      "grad_norm": 0.19638651609420776,
      "learning_rate": 1.3593333333333332e-05,
      "loss": 0.0019,
      "step": 109220
    },
    {
      "epoch": 5.8256,
      "grad_norm": 0.1402701735496521,
      "learning_rate": 1.359e-05,
      "loss": 0.0029,
      "step": 109230
    },
    {
      "epoch": 5.826133333333333,
      "grad_norm": 0.20351248979568481,
      "learning_rate": 1.3586666666666666e-05,
      "loss": 0.0033,
      "step": 109240
    },
    {
      "epoch": 5.826666666666666,
      "grad_norm": 0.5050210356712341,
      "learning_rate": 1.3583333333333334e-05,
      "loss": 0.0033,
      "step": 109250
    },
    {
      "epoch": 5.8272,
      "grad_norm": 0.3927555978298187,
      "learning_rate": 1.358e-05,
      "loss": 0.0036,
      "step": 109260
    },
    {
      "epoch": 5.827733333333334,
      "grad_norm": 0.05610812455415726,
      "learning_rate": 1.3576666666666668e-05,
      "loss": 0.0032,
      "step": 109270
    },
    {
      "epoch": 5.828266666666667,
      "grad_norm": 0.1963813304901123,
      "learning_rate": 1.3573333333333334e-05,
      "loss": 0.0041,
      "step": 109280
    },
    {
      "epoch": 5.8288,
      "grad_norm": 1.4664973020553589,
      "learning_rate": 1.3569999999999999e-05,
      "loss": 0.0046,
      "step": 109290
    },
    {
      "epoch": 5.8293333333333335,
      "grad_norm": 0.08416272699832916,
      "learning_rate": 1.3566666666666667e-05,
      "loss": 0.0028,
      "step": 109300
    },
    {
      "epoch": 5.829866666666667,
      "grad_norm": 0.14027434587478638,
      "learning_rate": 1.3563333333333333e-05,
      "loss": 0.0024,
      "step": 109310
    },
    {
      "epoch": 5.8304,
      "grad_norm": 1.7204027091821672e-09,
      "learning_rate": 1.356e-05,
      "loss": 0.0017,
      "step": 109320
    },
    {
      "epoch": 5.830933333333333,
      "grad_norm": 0.25249356031417847,
      "learning_rate": 1.3556666666666667e-05,
      "loss": 0.003,
      "step": 109330
    },
    {
      "epoch": 5.831466666666667,
      "grad_norm": 0.3647159934043884,
      "learning_rate": 1.3553333333333335e-05,
      "loss": 0.0048,
      "step": 109340
    },
    {
      "epoch": 5.832,
      "grad_norm": 0.3927568197250366,
      "learning_rate": 1.3550000000000002e-05,
      "loss": 0.0022,
      "step": 109350
    },
    {
      "epoch": 5.832533333333333,
      "grad_norm": 0.1402747631072998,
      "learning_rate": 1.3546666666666669e-05,
      "loss": 0.0028,
      "step": 109360
    },
    {
      "epoch": 5.833066666666666,
      "grad_norm": 0.05610832944512367,
      "learning_rate": 1.3543333333333333e-05,
      "loss": 0.0027,
      "step": 109370
    },
    {
      "epoch": 5.8336,
      "grad_norm": 0.02805423177778721,
      "learning_rate": 1.3539999999999999e-05,
      "loss": 0.0041,
      "step": 109380
    },
    {
      "epoch": 5.834133333333333,
      "grad_norm": 0.2524898648262024,
      "learning_rate": 1.3536666666666667e-05,
      "loss": 0.0026,
      "step": 109390
    },
    {
      "epoch": 5.834666666666667,
      "grad_norm": 0.16833186149597168,
      "learning_rate": 1.3533333333333335e-05,
      "loss": 0.0024,
      "step": 109400
    },
    {
      "epoch": 5.8352,
      "grad_norm": 0.028056278824806213,
      "learning_rate": 1.3530000000000001e-05,
      "loss": 0.0022,
      "step": 109410
    },
    {
      "epoch": 5.835733333333334,
      "grad_norm": 0.19638660550117493,
      "learning_rate": 1.3526666666666669e-05,
      "loss": 0.0024,
      "step": 109420
    },
    {
      "epoch": 5.836266666666667,
      "grad_norm": 0.11221667379140854,
      "learning_rate": 1.3523333333333335e-05,
      "loss": 0.0026,
      "step": 109430
    },
    {
      "epoch": 5.8368,
      "grad_norm": 0.2805393934249878,
      "learning_rate": 1.352e-05,
      "loss": 0.0033,
      "step": 109440
    },
    {
      "epoch": 5.8373333333333335,
      "grad_norm": 0.028053948655724525,
      "learning_rate": 1.3516666666666667e-05,
      "loss": 0.0024,
      "step": 109450
    },
    {
      "epoch": 5.837866666666667,
      "grad_norm": 0.08416085690259933,
      "learning_rate": 1.3513333333333333e-05,
      "loss": 0.0021,
      "step": 109460
    },
    {
      "epoch": 5.8384,
      "grad_norm": 0.19637934863567352,
      "learning_rate": 1.3510000000000001e-05,
      "loss": 0.0021,
      "step": 109470
    },
    {
      "epoch": 5.838933333333333,
      "grad_norm": 0.05610894039273262,
      "learning_rate": 1.3506666666666667e-05,
      "loss": 0.003,
      "step": 109480
    },
    {
      "epoch": 5.839466666666667,
      "grad_norm": 0.08416717499494553,
      "learning_rate": 1.3503333333333335e-05,
      "loss": 0.0021,
      "step": 109490
    },
    {
      "epoch": 5.84,
      "grad_norm": 0.11222179979085922,
      "learning_rate": 1.3500000000000001e-05,
      "loss": 0.0027,
      "step": 109500
    },
    {
      "epoch": 5.840533333333333,
      "grad_norm": 0.36469539999961853,
      "learning_rate": 1.3496666666666669e-05,
      "loss": 0.0026,
      "step": 109510
    },
    {
      "epoch": 5.841066666666666,
      "grad_norm": 0.14027002453804016,
      "learning_rate": 1.3493333333333333e-05,
      "loss": 0.0029,
      "step": 109520
    },
    {
      "epoch": 5.8416,
      "grad_norm": 0.25248217582702637,
      "learning_rate": 1.349e-05,
      "loss": 0.0018,
      "step": 109530
    },
    {
      "epoch": 5.842133333333333,
      "grad_norm": 0.1683223694562912,
      "learning_rate": 1.3486666666666667e-05,
      "loss": 0.0027,
      "step": 109540
    },
    {
      "epoch": 5.842666666666666,
      "grad_norm": 0.3927563726902008,
      "learning_rate": 1.3483333333333334e-05,
      "loss": 0.0047,
      "step": 109550
    },
    {
      "epoch": 5.8431999999999995,
      "grad_norm": 0.16832320392131805,
      "learning_rate": 1.3480000000000001e-05,
      "loss": 0.0016,
      "step": 109560
    },
    {
      "epoch": 5.843733333333334,
      "grad_norm": 0.14026764035224915,
      "learning_rate": 1.3476666666666668e-05,
      "loss": 0.0037,
      "step": 109570
    },
    {
      "epoch": 5.844266666666667,
      "grad_norm": 0.14026544988155365,
      "learning_rate": 1.3473333333333335e-05,
      "loss": 0.0017,
      "step": 109580
    },
    {
      "epoch": 5.8448,
      "grad_norm": 0.22442582249641418,
      "learning_rate": 1.347e-05,
      "loss": 0.0038,
      "step": 109590
    },
    {
      "epoch": 5.8453333333333335,
      "grad_norm": 0.05610711872577667,
      "learning_rate": 1.3466666666666666e-05,
      "loss": 0.0023,
      "step": 109600
    },
    {
      "epoch": 5.845866666666667,
      "grad_norm": 0.08416151255369186,
      "learning_rate": 1.3463333333333334e-05,
      "loss": 0.003,
      "step": 109610
    },
    {
      "epoch": 5.8464,
      "grad_norm": 0.16831664741039276,
      "learning_rate": 1.346e-05,
      "loss": 0.0034,
      "step": 109620
    },
    {
      "epoch": 5.846933333333333,
      "grad_norm": 0.16535937786102295,
      "learning_rate": 1.3456666666666668e-05,
      "loss": 0.0026,
      "step": 109630
    },
    {
      "epoch": 5.847466666666667,
      "grad_norm": 0.11221595108509064,
      "learning_rate": 1.3453333333333334e-05,
      "loss": 0.0025,
      "step": 109640
    },
    {
      "epoch": 5.848,
      "grad_norm": 0.14027290046215057,
      "learning_rate": 1.3450000000000002e-05,
      "loss": 0.0022,
      "step": 109650
    },
    {
      "epoch": 5.848533333333333,
      "grad_norm": 0.19638365507125854,
      "learning_rate": 1.3446666666666668e-05,
      "loss": 0.0023,
      "step": 109660
    },
    {
      "epoch": 5.849066666666666,
      "grad_norm": 0.168319970369339,
      "learning_rate": 1.3443333333333332e-05,
      "loss": 0.0037,
      "step": 109670
    },
    {
      "epoch": 5.8496,
      "grad_norm": 0.02805411070585251,
      "learning_rate": 1.344e-05,
      "loss": 0.0033,
      "step": 109680
    },
    {
      "epoch": 5.850133333333333,
      "grad_norm": 4.101934791833628e-09,
      "learning_rate": 1.3436666666666666e-05,
      "loss": 0.0025,
      "step": 109690
    },
    {
      "epoch": 5.850666666666667,
      "grad_norm": 0.02805296890437603,
      "learning_rate": 1.3433333333333334e-05,
      "loss": 0.0025,
      "step": 109700
    },
    {
      "epoch": 5.8512,
      "grad_norm": 0.28053686022758484,
      "learning_rate": 1.343e-05,
      "loss": 0.0034,
      "step": 109710
    },
    {
      "epoch": 5.851733333333334,
      "grad_norm": 1.4911294421438015e-09,
      "learning_rate": 1.3426666666666668e-05,
      "loss": 0.0033,
      "step": 109720
    },
    {
      "epoch": 5.852266666666667,
      "grad_norm": 0.1122133657336235,
      "learning_rate": 1.3423333333333336e-05,
      "loss": 0.0015,
      "step": 109730
    },
    {
      "epoch": 5.8528,
      "grad_norm": 0.056105129420757294,
      "learning_rate": 1.3420000000000002e-05,
      "loss": 0.0032,
      "step": 109740
    },
    {
      "epoch": 5.8533333333333335,
      "grad_norm": 0.28726592659950256,
      "learning_rate": 1.3416666666666666e-05,
      "loss": 0.0032,
      "step": 109750
    },
    {
      "epoch": 5.853866666666667,
      "grad_norm": 0.028052765876054764,
      "learning_rate": 1.3413333333333333e-05,
      "loss": 0.0021,
      "step": 109760
    },
    {
      "epoch": 5.8544,
      "grad_norm": 0.1963702291250229,
      "learning_rate": 1.341e-05,
      "loss": 0.0028,
      "step": 109770
    },
    {
      "epoch": 5.854933333333333,
      "grad_norm": 0.16840095818042755,
      "learning_rate": 1.3406666666666668e-05,
      "loss": 0.0048,
      "step": 109780
    },
    {
      "epoch": 5.855466666666667,
      "grad_norm": 0.22801269590854645,
      "learning_rate": 1.3403333333333334e-05,
      "loss": 0.0022,
      "step": 109790
    },
    {
      "epoch": 5.856,
      "grad_norm": 0.19637824594974518,
      "learning_rate": 1.3400000000000002e-05,
      "loss": 0.0029,
      "step": 109800
    },
    {
      "epoch": 5.856533333333333,
      "grad_norm": 0.1402723491191864,
      "learning_rate": 1.3396666666666668e-05,
      "loss": 0.0037,
      "step": 109810
    },
    {
      "epoch": 5.857066666666666,
      "grad_norm": 0.25249040126800537,
      "learning_rate": 1.3393333333333333e-05,
      "loss": 0.0029,
      "step": 109820
    },
    {
      "epoch": 5.8576,
      "grad_norm": 0.14026379585266113,
      "learning_rate": 1.339e-05,
      "loss": 0.0023,
      "step": 109830
    },
    {
      "epoch": 5.858133333333333,
      "grad_norm": 0.028052417561411858,
      "learning_rate": 1.3386666666666667e-05,
      "loss": 0.0027,
      "step": 109840
    },
    {
      "epoch": 5.858666666666666,
      "grad_norm": 0.22442053258419037,
      "learning_rate": 1.3383333333333335e-05,
      "loss": 0.0042,
      "step": 109850
    },
    {
      "epoch": 5.8591999999999995,
      "grad_norm": 0.08415650576353073,
      "learning_rate": 1.338e-05,
      "loss": 0.0023,
      "step": 109860
    },
    {
      "epoch": 5.859733333333334,
      "grad_norm": 0.056105054914951324,
      "learning_rate": 1.3376666666666668e-05,
      "loss": 0.0034,
      "step": 109870
    },
    {
      "epoch": 5.860266666666667,
      "grad_norm": 0.02805272489786148,
      "learning_rate": 1.3373333333333335e-05,
      "loss": 0.0037,
      "step": 109880
    },
    {
      "epoch": 5.8608,
      "grad_norm": 0.1683141589164734,
      "learning_rate": 1.3370000000000002e-05,
      "loss": 0.0037,
      "step": 109890
    },
    {
      "epoch": 5.8613333333333335,
      "grad_norm": 1.6880608797073364,
      "learning_rate": 1.3366666666666667e-05,
      "loss": 0.0026,
      "step": 109900
    },
    {
      "epoch": 5.861866666666667,
      "grad_norm": 0.16831481456756592,
      "learning_rate": 1.3363333333333333e-05,
      "loss": 0.0022,
      "step": 109910
    },
    {
      "epoch": 5.8624,
      "grad_norm": 0.028052087873220444,
      "learning_rate": 1.336e-05,
      "loss": 0.0022,
      "step": 109920
    },
    {
      "epoch": 5.862933333333333,
      "grad_norm": 0.3085780441761017,
      "learning_rate": 1.3356666666666667e-05,
      "loss": 0.0026,
      "step": 109930
    },
    {
      "epoch": 5.863466666666667,
      "grad_norm": 0.05610612407326698,
      "learning_rate": 1.3353333333333335e-05,
      "loss": 0.0022,
      "step": 109940
    },
    {
      "epoch": 5.864,
      "grad_norm": 0.2805306315422058,
      "learning_rate": 1.3350000000000001e-05,
      "loss": 0.0031,
      "step": 109950
    },
    {
      "epoch": 5.864533333333333,
      "grad_norm": 0.16831250488758087,
      "learning_rate": 1.3346666666666669e-05,
      "loss": 0.004,
      "step": 109960
    },
    {
      "epoch": 5.865066666666666,
      "grad_norm": 0.16831561923027039,
      "learning_rate": 1.3343333333333333e-05,
      "loss": 0.0015,
      "step": 109970
    },
    {
      "epoch": 5.8656,
      "grad_norm": 0.05610502511262894,
      "learning_rate": 1.334e-05,
      "loss": 0.0022,
      "step": 109980
    },
    {
      "epoch": 5.866133333333333,
      "grad_norm": 0.08415696024894714,
      "learning_rate": 1.3336666666666667e-05,
      "loss": 0.002,
      "step": 109990
    },
    {
      "epoch": 5.866666666666667,
      "grad_norm": 0.3646751344203949,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.0027,
      "step": 110000
    },
    {
      "epoch": 5.8672,
      "grad_norm": 0.05610702186822891,
      "learning_rate": 1.3330000000000001e-05,
      "loss": 0.0027,
      "step": 110010
    },
    {
      "epoch": 5.867733333333334,
      "grad_norm": 0.3366466462612152,
      "learning_rate": 1.3326666666666667e-05,
      "loss": 0.0028,
      "step": 110020
    },
    {
      "epoch": 5.868266666666667,
      "grad_norm": 0.2805171310901642,
      "learning_rate": 1.3323333333333335e-05,
      "loss": 0.0032,
      "step": 110030
    },
    {
      "epoch": 5.8688,
      "grad_norm": 3.3085507666896774e-09,
      "learning_rate": 1.3320000000000001e-05,
      "loss": 0.0029,
      "step": 110040
    },
    {
      "epoch": 5.8693333333333335,
      "grad_norm": 0.3366643786430359,
      "learning_rate": 1.3316666666666666e-05,
      "loss": 0.0038,
      "step": 110050
    },
    {
      "epoch": 5.869866666666667,
      "grad_norm": 0.28052595257759094,
      "learning_rate": 1.3313333333333333e-05,
      "loss": 0.003,
      "step": 110060
    },
    {
      "epoch": 5.8704,
      "grad_norm": 0.19636796414852142,
      "learning_rate": 1.331e-05,
      "loss": 0.0021,
      "step": 110070
    },
    {
      "epoch": 5.870933333333333,
      "grad_norm": 0.3085973560810089,
      "learning_rate": 1.3306666666666667e-05,
      "loss": 0.0018,
      "step": 110080
    },
    {
      "epoch": 5.871466666666667,
      "grad_norm": 0.1963641196489334,
      "learning_rate": 1.3303333333333334e-05,
      "loss": 0.0024,
      "step": 110090
    },
    {
      "epoch": 5.872,
      "grad_norm": 0.19636626541614532,
      "learning_rate": 1.3300000000000001e-05,
      "loss": 0.0022,
      "step": 110100
    },
    {
      "epoch": 5.872533333333333,
      "grad_norm": 0.11220930516719818,
      "learning_rate": 1.3296666666666668e-05,
      "loss": 0.0036,
      "step": 110110
    },
    {
      "epoch": 5.873066666666666,
      "grad_norm": 0.08415386080741882,
      "learning_rate": 1.3293333333333332e-05,
      "loss": 0.0027,
      "step": 110120
    },
    {
      "epoch": 5.8736,
      "grad_norm": 4.122194585676198e-09,
      "learning_rate": 1.329e-05,
      "loss": 0.0028,
      "step": 110130
    },
    {
      "epoch": 5.874133333333333,
      "grad_norm": 0.4261057674884796,
      "learning_rate": 1.3286666666666666e-05,
      "loss": 0.0022,
      "step": 110140
    },
    {
      "epoch": 5.874666666666666,
      "grad_norm": 0.16831213235855103,
      "learning_rate": 1.3283333333333334e-05,
      "loss": 0.0024,
      "step": 110150
    },
    {
      "epoch": 5.8751999999999995,
      "grad_norm": 0.32619431614875793,
      "learning_rate": 1.3280000000000002e-05,
      "loss": 0.0024,
      "step": 110160
    },
    {
      "epoch": 5.875733333333334,
      "grad_norm": 0.3646901547908783,
      "learning_rate": 1.3276666666666668e-05,
      "loss": 0.0032,
      "step": 110170
    },
    {
      "epoch": 5.876266666666667,
      "grad_norm": 0.16831515729427338,
      "learning_rate": 1.3273333333333336e-05,
      "loss": 0.0036,
      "step": 110180
    },
    {
      "epoch": 5.8768,
      "grad_norm": 0.05610383674502373,
      "learning_rate": 1.3270000000000002e-05,
      "loss": 0.0032,
      "step": 110190
    },
    {
      "epoch": 5.8773333333333335,
      "grad_norm": 0.1122061237692833,
      "learning_rate": 1.3266666666666666e-05,
      "loss": 0.0018,
      "step": 110200
    },
    {
      "epoch": 5.877866666666667,
      "grad_norm": 0.11220624297857285,
      "learning_rate": 1.3263333333333334e-05,
      "loss": 0.0028,
      "step": 110210
    },
    {
      "epoch": 5.8784,
      "grad_norm": 0.5049305558204651,
      "learning_rate": 1.326e-05,
      "loss": 0.0047,
      "step": 110220
    },
    {
      "epoch": 5.878933333333333,
      "grad_norm": 0.25247132778167725,
      "learning_rate": 1.3256666666666668e-05,
      "loss": 0.0025,
      "step": 110230
    },
    {
      "epoch": 5.879466666666667,
      "grad_norm": 0.11221162974834442,
      "learning_rate": 1.3253333333333334e-05,
      "loss": 0.0032,
      "step": 110240
    },
    {
      "epoch": 5.88,
      "grad_norm": 0.028051743283867836,
      "learning_rate": 1.3250000000000002e-05,
      "loss": 0.0029,
      "step": 110250
    },
    {
      "epoch": 5.880533333333333,
      "grad_norm": 0.056104082614183426,
      "learning_rate": 1.3246666666666668e-05,
      "loss": 0.002,
      "step": 110260
    },
    {
      "epoch": 5.881066666666666,
      "grad_norm": 0.05610514432191849,
      "learning_rate": 1.3243333333333332e-05,
      "loss": 0.0027,
      "step": 110270
    },
    {
      "epoch": 5.8816,
      "grad_norm": 0.3927268981933594,
      "learning_rate": 1.324e-05,
      "loss": 0.0034,
      "step": 110280
    },
    {
      "epoch": 5.882133333333333,
      "grad_norm": 0.02805347926914692,
      "learning_rate": 1.3236666666666666e-05,
      "loss": 0.0031,
      "step": 110290
    },
    {
      "epoch": 5.882666666666667,
      "grad_norm": 0.08416246622800827,
      "learning_rate": 1.3233333333333334e-05,
      "loss": 0.0041,
      "step": 110300
    },
    {
      "epoch": 5.8832,
      "grad_norm": 0.3085692226886749,
      "learning_rate": 1.323e-05,
      "loss": 0.0021,
      "step": 110310
    },
    {
      "epoch": 5.883733333333334,
      "grad_norm": 0.056102607399225235,
      "learning_rate": 1.3226666666666668e-05,
      "loss": 0.0025,
      "step": 110320
    },
    {
      "epoch": 5.884266666666667,
      "grad_norm": 0.16831159591674805,
      "learning_rate": 1.3223333333333334e-05,
      "loss": 0.002,
      "step": 110330
    },
    {
      "epoch": 5.8848,
      "grad_norm": 0.1963641196489334,
      "learning_rate": 1.3220000000000002e-05,
      "loss": 0.0032,
      "step": 110340
    },
    {
      "epoch": 5.8853333333333335,
      "grad_norm": 0.14026862382888794,
      "learning_rate": 1.3216666666666667e-05,
      "loss": 0.0018,
      "step": 110350
    },
    {
      "epoch": 5.885866666666667,
      "grad_norm": 0.19636739790439606,
      "learning_rate": 1.3213333333333333e-05,
      "loss": 0.0034,
      "step": 110360
    },
    {
      "epoch": 5.8864,
      "grad_norm": 4.537863862452696e-09,
      "learning_rate": 1.321e-05,
      "loss": 0.0033,
      "step": 110370
    },
    {
      "epoch": 5.886933333333333,
      "grad_norm": 0.10352063924074173,
      "learning_rate": 1.3206666666666667e-05,
      "loss": 0.0035,
      "step": 110380
    },
    {
      "epoch": 5.887466666666667,
      "grad_norm": 0.11220899224281311,
      "learning_rate": 1.3203333333333335e-05,
      "loss": 0.0027,
      "step": 110390
    },
    {
      "epoch": 5.888,
      "grad_norm": 0.0280519537627697,
      "learning_rate": 1.32e-05,
      "loss": 0.0028,
      "step": 110400
    },
    {
      "epoch": 5.888533333333333,
      "grad_norm": 0.056106675416231155,
      "learning_rate": 1.3196666666666669e-05,
      "loss": 0.002,
      "step": 110410
    },
    {
      "epoch": 5.8890666666666664,
      "grad_norm": 0.3646933138370514,
      "learning_rate": 1.3193333333333335e-05,
      "loss": 0.0028,
      "step": 110420
    },
    {
      "epoch": 5.8896,
      "grad_norm": 0.33660659193992615,
      "learning_rate": 1.3189999999999999e-05,
      "loss": 0.0023,
      "step": 110430
    },
    {
      "epoch": 5.890133333333333,
      "grad_norm": 0.17854711413383484,
      "learning_rate": 1.3186666666666667e-05,
      "loss": 0.0014,
      "step": 110440
    },
    {
      "epoch": 5.890666666666666,
      "grad_norm": 0.25246530771255493,
      "learning_rate": 1.3183333333333333e-05,
      "loss": 0.0033,
      "step": 110450
    },
    {
      "epoch": 5.8911999999999995,
      "grad_norm": 0.16830995678901672,
      "learning_rate": 1.3180000000000001e-05,
      "loss": 0.0024,
      "step": 110460
    },
    {
      "epoch": 5.891733333333334,
      "grad_norm": 0.2524668872356415,
      "learning_rate": 1.3176666666666667e-05,
      "loss": 0.0022,
      "step": 110470
    },
    {
      "epoch": 5.892266666666667,
      "grad_norm": 0.16830958425998688,
      "learning_rate": 1.3173333333333335e-05,
      "loss": 0.0022,
      "step": 110480
    },
    {
      "epoch": 5.8928,
      "grad_norm": 0.30856597423553467,
      "learning_rate": 1.3170000000000001e-05,
      "loss": 0.0027,
      "step": 110490
    },
    {
      "epoch": 5.8933333333333335,
      "grad_norm": 0.11220463365316391,
      "learning_rate": 1.3166666666666665e-05,
      "loss": 0.0017,
      "step": 110500
    },
    {
      "epoch": 5.893866666666667,
      "grad_norm": 0.22441409528255463,
      "learning_rate": 1.3163333333333333e-05,
      "loss": 0.0026,
      "step": 110510
    },
    {
      "epoch": 5.8944,
      "grad_norm": 0.3085795044898987,
      "learning_rate": 1.316e-05,
      "loss": 0.0023,
      "step": 110520
    },
    {
      "epoch": 5.894933333333333,
      "grad_norm": 0.08415253460407257,
      "learning_rate": 1.3156666666666667e-05,
      "loss": 0.0025,
      "step": 110530
    },
    {
      "epoch": 5.895466666666667,
      "grad_norm": 0.11220773309469223,
      "learning_rate": 1.3153333333333335e-05,
      "loss": 0.0016,
      "step": 110540
    },
    {
      "epoch": 5.896,
      "grad_norm": 0.028052566573023796,
      "learning_rate": 1.3150000000000001e-05,
      "loss": 0.0019,
      "step": 110550
    },
    {
      "epoch": 5.896533333333333,
      "grad_norm": 0.08415250480175018,
      "learning_rate": 1.3146666666666669e-05,
      "loss": 0.0024,
      "step": 110560
    },
    {
      "epoch": 5.8970666666666665,
      "grad_norm": 0.14025525748729706,
      "learning_rate": 1.3143333333333335e-05,
      "loss": 0.0036,
      "step": 110570
    },
    {
      "epoch": 5.8976,
      "grad_norm": 0.23744884133338928,
      "learning_rate": 1.314e-05,
      "loss": 0.0022,
      "step": 110580
    },
    {
      "epoch": 5.898133333333333,
      "grad_norm": 0.08415289223194122,
      "learning_rate": 1.3136666666666667e-05,
      "loss": 0.002,
      "step": 110590
    },
    {
      "epoch": 5.898666666666666,
      "grad_norm": 0.08415456861257553,
      "learning_rate": 1.3133333333333334e-05,
      "loss": 0.0028,
      "step": 110600
    },
    {
      "epoch": 5.8992,
      "grad_norm": 0.05610322952270508,
      "learning_rate": 1.3130000000000001e-05,
      "loss": 0.0025,
      "step": 110610
    },
    {
      "epoch": 5.899733333333334,
      "grad_norm": 0.1683066189289093,
      "learning_rate": 1.3126666666666667e-05,
      "loss": 0.0031,
      "step": 110620
    },
    {
      "epoch": 5.900266666666667,
      "grad_norm": 0.14025773108005524,
      "learning_rate": 1.3123333333333335e-05,
      "loss": 0.0017,
      "step": 110630
    },
    {
      "epoch": 5.9008,
      "grad_norm": 0.08531828969717026,
      "learning_rate": 1.3120000000000001e-05,
      "loss": 0.0034,
      "step": 110640
    },
    {
      "epoch": 5.9013333333333335,
      "grad_norm": 0.05610306188464165,
      "learning_rate": 1.3116666666666666e-05,
      "loss": 0.0041,
      "step": 110650
    },
    {
      "epoch": 5.901866666666667,
      "grad_norm": 0.08415537327528,
      "learning_rate": 1.3113333333333334e-05,
      "loss": 0.003,
      "step": 110660
    },
    {
      "epoch": 5.9024,
      "grad_norm": 0.25246933102607727,
      "learning_rate": 1.311e-05,
      "loss": 0.0029,
      "step": 110670
    },
    {
      "epoch": 5.902933333333333,
      "grad_norm": 0.19635720551013947,
      "learning_rate": 1.3106666666666668e-05,
      "loss": 0.0018,
      "step": 110680
    },
    {
      "epoch": 5.903466666666667,
      "grad_norm": 0.22440898418426514,
      "learning_rate": 1.3103333333333334e-05,
      "loss": 0.0024,
      "step": 110690
    },
    {
      "epoch": 5.904,
      "grad_norm": 0.7689648270606995,
      "learning_rate": 1.3100000000000002e-05,
      "loss": 0.0027,
      "step": 110700
    },
    {
      "epoch": 5.904533333333333,
      "grad_norm": 0.2524738609790802,
      "learning_rate": 1.3096666666666668e-05,
      "loss": 0.0033,
      "step": 110710
    },
    {
      "epoch": 5.9050666666666665,
      "grad_norm": 0.02805170789361,
      "learning_rate": 1.3093333333333336e-05,
      "loss": 0.0033,
      "step": 110720
    },
    {
      "epoch": 5.9056,
      "grad_norm": 0.1683052033185959,
      "learning_rate": 1.309e-05,
      "loss": 0.0045,
      "step": 110730
    },
    {
      "epoch": 5.906133333333333,
      "grad_norm": 0.30856114625930786,
      "learning_rate": 1.3086666666666666e-05,
      "loss": 0.0016,
      "step": 110740
    },
    {
      "epoch": 5.906666666666666,
      "grad_norm": 0.14025725424289703,
      "learning_rate": 1.3083333333333334e-05,
      "loss": 0.0026,
      "step": 110750
    },
    {
      "epoch": 5.9072,
      "grad_norm": 0.11220937222242355,
      "learning_rate": 1.308e-05,
      "loss": 0.0019,
      "step": 110760
    },
    {
      "epoch": 5.907733333333333,
      "grad_norm": 0.11220842599868774,
      "learning_rate": 1.3076666666666668e-05,
      "loss": 0.003,
      "step": 110770
    },
    {
      "epoch": 5.908266666666667,
      "grad_norm": 0.6501497030258179,
      "learning_rate": 1.3073333333333334e-05,
      "loss": 0.0034,
      "step": 110780
    },
    {
      "epoch": 5.9088,
      "grad_norm": 0.05610359087586403,
      "learning_rate": 1.3070000000000002e-05,
      "loss": 0.0023,
      "step": 110790
    },
    {
      "epoch": 5.9093333333333335,
      "grad_norm": 0.028051961213350296,
      "learning_rate": 1.3066666666666666e-05,
      "loss": 0.0033,
      "step": 110800
    },
    {
      "epoch": 5.909866666666667,
      "grad_norm": 0.028051666915416718,
      "learning_rate": 1.3063333333333332e-05,
      "loss": 0.0021,
      "step": 110810
    },
    {
      "epoch": 5.9104,
      "grad_norm": 0.0561041384935379,
      "learning_rate": 1.306e-05,
      "loss": 0.0026,
      "step": 110820
    },
    {
      "epoch": 5.910933333333333,
      "grad_norm": 0.1963627189397812,
      "learning_rate": 1.3056666666666666e-05,
      "loss": 0.0032,
      "step": 110830
    },
    {
      "epoch": 5.911466666666667,
      "grad_norm": 0.05610347166657448,
      "learning_rate": 1.3053333333333334e-05,
      "loss": 0.0027,
      "step": 110840
    },
    {
      "epoch": 5.912,
      "grad_norm": 0.056103385984897614,
      "learning_rate": 1.305e-05,
      "loss": 0.0033,
      "step": 110850
    },
    {
      "epoch": 5.912533333333333,
      "grad_norm": 0.08415435254573822,
      "learning_rate": 1.3046666666666668e-05,
      "loss": 0.0028,
      "step": 110860
    },
    {
      "epoch": 5.9130666666666665,
      "grad_norm": 0.19635958969593048,
      "learning_rate": 1.3043333333333334e-05,
      "loss": 0.002,
      "step": 110870
    },
    {
      "epoch": 5.9136,
      "grad_norm": 3.0348548119008e-09,
      "learning_rate": 1.3039999999999999e-05,
      "loss": 0.0022,
      "step": 110880
    },
    {
      "epoch": 5.914133333333333,
      "grad_norm": 0.11220858246088028,
      "learning_rate": 1.3036666666666667e-05,
      "loss": 0.0035,
      "step": 110890
    },
    {
      "epoch": 5.914666666666666,
      "grad_norm": 0.02805273048579693,
      "learning_rate": 1.3033333333333333e-05,
      "loss": 0.0032,
      "step": 110900
    },
    {
      "epoch": 5.9152000000000005,
      "grad_norm": 0.1683160960674286,
      "learning_rate": 1.303e-05,
      "loss": 0.0025,
      "step": 110910
    },
    {
      "epoch": 5.915733333333334,
      "grad_norm": 0.2805282175540924,
      "learning_rate": 1.3026666666666667e-05,
      "loss": 0.0022,
      "step": 110920
    },
    {
      "epoch": 5.916266666666667,
      "grad_norm": 0.02805226854979992,
      "learning_rate": 1.3023333333333335e-05,
      "loss": 0.0035,
      "step": 110930
    },
    {
      "epoch": 5.9168,
      "grad_norm": 0.22441135346889496,
      "learning_rate": 1.3020000000000002e-05,
      "loss": 0.003,
      "step": 110940
    },
    {
      "epoch": 5.917333333333334,
      "grad_norm": 0.05610383301973343,
      "learning_rate": 1.3016666666666669e-05,
      "loss": 0.003,
      "step": 110950
    },
    {
      "epoch": 5.917866666666667,
      "grad_norm": 0.1122112050652504,
      "learning_rate": 1.3013333333333333e-05,
      "loss": 0.0025,
      "step": 110960
    },
    {
      "epoch": 5.9184,
      "grad_norm": 0.33661898970603943,
      "learning_rate": 1.301e-05,
      "loss": 0.0022,
      "step": 110970
    },
    {
      "epoch": 5.918933333333333,
      "grad_norm": 0.2244103103876114,
      "learning_rate": 1.3006666666666667e-05,
      "loss": 0.0026,
      "step": 110980
    },
    {
      "epoch": 5.919466666666667,
      "grad_norm": 0.05610429495573044,
      "learning_rate": 1.3003333333333335e-05,
      "loss": 0.0029,
      "step": 110990
    },
    {
      "epoch": 5.92,
      "grad_norm": 0.0841536894440651,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.0017,
      "step": 111000
    },
    {
      "epoch": 5.920533333333333,
      "grad_norm": 0.16830149292945862,
      "learning_rate": 1.2996666666666669e-05,
      "loss": 0.0023,
      "step": 111010
    },
    {
      "epoch": 5.9210666666666665,
      "grad_norm": 3.3840177326993626e-09,
      "learning_rate": 1.2993333333333335e-05,
      "loss": 0.003,
      "step": 111020
    },
    {
      "epoch": 5.9216,
      "grad_norm": 0.08415537327528,
      "learning_rate": 1.299e-05,
      "loss": 0.0038,
      "step": 111030
    },
    {
      "epoch": 5.922133333333333,
      "grad_norm": 0.22441262006759644,
      "learning_rate": 1.2986666666666667e-05,
      "loss": 0.0034,
      "step": 111040
    },
    {
      "epoch": 5.922666666666666,
      "grad_norm": 0.02805108018219471,
      "learning_rate": 1.2983333333333333e-05,
      "loss": 0.002,
      "step": 111050
    },
    {
      "epoch": 5.9232,
      "grad_norm": 0.3085503578186035,
      "learning_rate": 1.2980000000000001e-05,
      "loss": 0.0021,
      "step": 111060
    },
    {
      "epoch": 5.923733333333333,
      "grad_norm": 0.05610235780477524,
      "learning_rate": 1.2976666666666667e-05,
      "loss": 0.003,
      "step": 111070
    },
    {
      "epoch": 5.924266666666667,
      "grad_norm": 0.0841522067785263,
      "learning_rate": 1.2973333333333335e-05,
      "loss": 0.0021,
      "step": 111080
    },
    {
      "epoch": 5.9248,
      "grad_norm": 0.08415106683969498,
      "learning_rate": 1.2970000000000001e-05,
      "loss": 0.0021,
      "step": 111090
    },
    {
      "epoch": 5.925333333333334,
      "grad_norm": 0.36466747522354126,
      "learning_rate": 1.2966666666666669e-05,
      "loss": 0.0029,
      "step": 111100
    },
    {
      "epoch": 5.925866666666667,
      "grad_norm": 0.14026255905628204,
      "learning_rate": 1.2963333333333333e-05,
      "loss": 0.0039,
      "step": 111110
    },
    {
      "epoch": 5.9264,
      "grad_norm": 0.22439900040626526,
      "learning_rate": 1.296e-05,
      "loss": 0.0032,
      "step": 111120
    },
    {
      "epoch": 5.926933333333333,
      "grad_norm": 0.08412764966487885,
      "learning_rate": 1.2956666666666667e-05,
      "loss": 0.0032,
      "step": 111130
    },
    {
      "epoch": 5.927466666666667,
      "grad_norm": 0.11217419803142548,
      "learning_rate": 1.2953333333333334e-05,
      "loss": 0.0033,
      "step": 111140
    },
    {
      "epoch": 5.928,
      "grad_norm": 0.02804291993379593,
      "learning_rate": 1.2950000000000001e-05,
      "loss": 0.0028,
      "step": 111150
    },
    {
      "epoch": 5.928533333333333,
      "grad_norm": 0.12996351718902588,
      "learning_rate": 1.2946666666666668e-05,
      "loss": 0.0039,
      "step": 111160
    },
    {
      "epoch": 5.9290666666666665,
      "grad_norm": 0.42064934968948364,
      "learning_rate": 1.2943333333333335e-05,
      "loss": 0.002,
      "step": 111170
    },
    {
      "epoch": 5.9296,
      "grad_norm": 0.22433768212795258,
      "learning_rate": 1.294e-05,
      "loss": 0.0028,
      "step": 111180
    },
    {
      "epoch": 5.930133333333333,
      "grad_norm": 0.3084709942340851,
      "learning_rate": 1.2936666666666666e-05,
      "loss": 0.0017,
      "step": 111190
    },
    {
      "epoch": 5.930666666666666,
      "grad_norm": 0.05608554556965828,
      "learning_rate": 1.2933333333333334e-05,
      "loss": 0.0029,
      "step": 111200
    },
    {
      "epoch": 5.9312000000000005,
      "grad_norm": 0.056086961179971695,
      "learning_rate": 1.293e-05,
      "loss": 0.0031,
      "step": 111210
    },
    {
      "epoch": 5.931733333333334,
      "grad_norm": 0.25238901376724243,
      "learning_rate": 1.2926666666666668e-05,
      "loss": 0.0023,
      "step": 111220
    },
    {
      "epoch": 5.932266666666667,
      "grad_norm": 0.3365120589733124,
      "learning_rate": 1.2923333333333334e-05,
      "loss": 0.0019,
      "step": 111230
    },
    {
      "epoch": 5.9328,
      "grad_norm": 0.028042655438184738,
      "learning_rate": 1.2920000000000002e-05,
      "loss": 0.0018,
      "step": 111240
    },
    {
      "epoch": 5.933333333333334,
      "grad_norm": 0.1682533174753189,
      "learning_rate": 1.2916666666666668e-05,
      "loss": 0.0022,
      "step": 111250
    },
    {
      "epoch": 5.933866666666667,
      "grad_norm": 0.1682593673467636,
      "learning_rate": 1.2913333333333332e-05,
      "loss": 0.0026,
      "step": 111260
    },
    {
      "epoch": 5.9344,
      "grad_norm": 0.40086638927459717,
      "learning_rate": 1.291e-05,
      "loss": 0.0035,
      "step": 111270
    },
    {
      "epoch": 5.934933333333333,
      "grad_norm": 0.11217112094163895,
      "learning_rate": 1.2906666666666666e-05,
      "loss": 0.003,
      "step": 111280
    },
    {
      "epoch": 5.935466666666667,
      "grad_norm": 0.11217018216848373,
      "learning_rate": 1.2903333333333334e-05,
      "loss": 0.002,
      "step": 111290
    },
    {
      "epoch": 5.936,
      "grad_norm": 0.05608562007546425,
      "learning_rate": 1.29e-05,
      "loss": 0.0019,
      "step": 111300
    },
    {
      "epoch": 5.936533333333333,
      "grad_norm": 0.02804398722946644,
      "learning_rate": 1.2896666666666668e-05,
      "loss": 0.0016,
      "step": 111310
    },
    {
      "epoch": 5.9370666666666665,
      "grad_norm": 0.10133636742830276,
      "learning_rate": 1.2893333333333336e-05,
      "loss": 0.0036,
      "step": 111320
    },
    {
      "epoch": 5.9376,
      "grad_norm": 0.08412772417068481,
      "learning_rate": 1.2889999999999999e-05,
      "loss": 0.0015,
      "step": 111330
    },
    {
      "epoch": 5.938133333333333,
      "grad_norm": 0.3926384747028351,
      "learning_rate": 1.2886666666666666e-05,
      "loss": 0.0031,
      "step": 111340
    },
    {
      "epoch": 5.938666666666666,
      "grad_norm": 0.05608554556965828,
      "learning_rate": 1.2883333333333333e-05,
      "loss": 0.0034,
      "step": 111350
    },
    {
      "epoch": 5.9392,
      "grad_norm": 0.05608472600579262,
      "learning_rate": 1.288e-05,
      "loss": 0.0028,
      "step": 111360
    },
    {
      "epoch": 5.939733333333333,
      "grad_norm": 0.22433783113956451,
      "learning_rate": 1.2876666666666668e-05,
      "loss": 0.002,
      "step": 111370
    },
    {
      "epoch": 5.940266666666667,
      "grad_norm": 0.11217080801725388,
      "learning_rate": 1.2873333333333334e-05,
      "loss": 0.0029,
      "step": 111380
    },
    {
      "epoch": 5.9408,
      "grad_norm": 0.0841265618801117,
      "learning_rate": 1.2870000000000002e-05,
      "loss": 0.002,
      "step": 111390
    },
    {
      "epoch": 5.941333333333334,
      "grad_norm": 0.33650875091552734,
      "learning_rate": 1.2866666666666668e-05,
      "loss": 0.0026,
      "step": 111400
    },
    {
      "epoch": 5.941866666666667,
      "grad_norm": 0.08413232862949371,
      "learning_rate": 1.2863333333333333e-05,
      "loss": 0.0028,
      "step": 111410
    },
    {
      "epoch": 5.9424,
      "grad_norm": 0.11217395961284637,
      "learning_rate": 1.286e-05,
      "loss": 0.0031,
      "step": 111420
    },
    {
      "epoch": 5.942933333333333,
      "grad_norm": 0.028041820973157883,
      "learning_rate": 1.2856666666666667e-05,
      "loss": 0.0027,
      "step": 111430
    },
    {
      "epoch": 5.943466666666667,
      "grad_norm": 0.16825450956821442,
      "learning_rate": 1.2853333333333335e-05,
      "loss": 0.0021,
      "step": 111440
    },
    {
      "epoch": 5.944,
      "grad_norm": 0.1402108073234558,
      "learning_rate": 1.285e-05,
      "loss": 0.003,
      "step": 111450
    },
    {
      "epoch": 5.944533333333333,
      "grad_norm": 4.429723254872897e-09,
      "learning_rate": 1.2846666666666668e-05,
      "loss": 0.0017,
      "step": 111460
    },
    {
      "epoch": 5.9450666666666665,
      "grad_norm": 0.14021484553813934,
      "learning_rate": 1.2843333333333335e-05,
      "loss": 0.0032,
      "step": 111470
    },
    {
      "epoch": 5.9456,
      "grad_norm": 0.19630058109760284,
      "learning_rate": 1.2839999999999999e-05,
      "loss": 0.0022,
      "step": 111480
    },
    {
      "epoch": 5.946133333333333,
      "grad_norm": 0.28042036294937134,
      "learning_rate": 1.2836666666666667e-05,
      "loss": 0.003,
      "step": 111490
    },
    {
      "epoch": 5.946666666666666,
      "grad_norm": 0.1402149647474289,
      "learning_rate": 1.2833333333333333e-05,
      "loss": 0.0026,
      "step": 111500
    },
    {
      "epoch": 5.9472000000000005,
      "grad_norm": 0.280430406332016,
      "learning_rate": 1.283e-05,
      "loss": 0.0031,
      "step": 111510
    },
    {
      "epoch": 5.947733333333334,
      "grad_norm": 0.3645341396331787,
      "learning_rate": 1.2826666666666667e-05,
      "loss": 0.0034,
      "step": 111520
    },
    {
      "epoch": 5.948266666666667,
      "grad_norm": 0.056085046380758286,
      "learning_rate": 1.2823333333333335e-05,
      "loss": 0.0029,
      "step": 111530
    },
    {
      "epoch": 5.9488,
      "grad_norm": 0.19630402326583862,
      "learning_rate": 1.2820000000000001e-05,
      "loss": 0.0022,
      "step": 111540
    },
    {
      "epoch": 5.949333333333334,
      "grad_norm": 0.05608754605054855,
      "learning_rate": 1.2816666666666669e-05,
      "loss": 0.0019,
      "step": 111550
    },
    {
      "epoch": 5.949866666666667,
      "grad_norm": 0.22433888912200928,
      "learning_rate": 1.2813333333333333e-05,
      "loss": 0.0025,
      "step": 111560
    },
    {
      "epoch": 5.9504,
      "grad_norm": 0.11216778308153152,
      "learning_rate": 1.281e-05,
      "loss": 0.002,
      "step": 111570
    },
    {
      "epoch": 5.950933333333333,
      "grad_norm": 0.3925841152667999,
      "learning_rate": 1.2806666666666667e-05,
      "loss": 0.0021,
      "step": 111580
    },
    {
      "epoch": 5.951466666666667,
      "grad_norm": 0.02804211899638176,
      "learning_rate": 1.2803333333333333e-05,
      "loss": 0.0028,
      "step": 111590
    },
    {
      "epoch": 5.952,
      "grad_norm": 0.14021098613739014,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 0.0022,
      "step": 111600
    },
    {
      "epoch": 5.952533333333333,
      "grad_norm": 0.2804267108440399,
      "learning_rate": 1.2796666666666667e-05,
      "loss": 0.0025,
      "step": 111610
    },
    {
      "epoch": 5.9530666666666665,
      "grad_norm": 0.08412336558103561,
      "learning_rate": 1.2793333333333335e-05,
      "loss": 0.0022,
      "step": 111620
    },
    {
      "epoch": 5.9536,
      "grad_norm": 0.1121668592095375,
      "learning_rate": 1.2790000000000001e-05,
      "loss": 0.0036,
      "step": 111630
    },
    {
      "epoch": 5.954133333333333,
      "grad_norm": 0.11216770112514496,
      "learning_rate": 1.2786666666666666e-05,
      "loss": 0.0036,
      "step": 111640
    },
    {
      "epoch": 5.954666666666666,
      "grad_norm": 0.22433903813362122,
      "learning_rate": 1.2783333333333333e-05,
      "loss": 0.0042,
      "step": 111650
    },
    {
      "epoch": 5.9552,
      "grad_norm": 1.3751614291734882e-09,
      "learning_rate": 1.278e-05,
      "loss": 0.0023,
      "step": 111660
    },
    {
      "epoch": 5.955733333333333,
      "grad_norm": 0.16825297474861145,
      "learning_rate": 1.2776666666666667e-05,
      "loss": 0.0024,
      "step": 111670
    },
    {
      "epoch": 5.956266666666667,
      "grad_norm": 0.16825304925441742,
      "learning_rate": 1.2773333333333334e-05,
      "loss": 0.0029,
      "step": 111680
    },
    {
      "epoch": 5.9568,
      "grad_norm": 0.08412855863571167,
      "learning_rate": 1.2770000000000001e-05,
      "loss": 0.0028,
      "step": 111690
    },
    {
      "epoch": 5.957333333333334,
      "grad_norm": 0.16824762523174286,
      "learning_rate": 1.276666666666667e-05,
      "loss": 0.0017,
      "step": 111700
    },
    {
      "epoch": 5.957866666666667,
      "grad_norm": 0.2804175019264221,
      "learning_rate": 1.2763333333333332e-05,
      "loss": 0.0025,
      "step": 111710
    },
    {
      "epoch": 5.9584,
      "grad_norm": 0.05608757212758064,
      "learning_rate": 1.276e-05,
      "loss": 0.0025,
      "step": 111720
    },
    {
      "epoch": 5.958933333333333,
      "grad_norm": 0.25238385796546936,
      "learning_rate": 1.2756666666666666e-05,
      "loss": 0.0022,
      "step": 111730
    },
    {
      "epoch": 5.959466666666667,
      "grad_norm": 0.3645364046096802,
      "learning_rate": 1.2753333333333334e-05,
      "loss": 0.0029,
      "step": 111740
    },
    {
      "epoch": 5.96,
      "grad_norm": 0.11217059195041656,
      "learning_rate": 1.2750000000000002e-05,
      "loss": 0.0037,
      "step": 111750
    },
    {
      "epoch": 5.960533333333333,
      "grad_norm": 0.2804282307624817,
      "learning_rate": 1.2746666666666668e-05,
      "loss": 0.002,
      "step": 111760
    },
    {
      "epoch": 5.9610666666666665,
      "grad_norm": 0.05608221888542175,
      "learning_rate": 1.2743333333333336e-05,
      "loss": 0.0025,
      "step": 111770
    },
    {
      "epoch": 5.9616,
      "grad_norm": 0.1962904930114746,
      "learning_rate": 1.2740000000000002e-05,
      "loss": 0.0022,
      "step": 111780
    },
    {
      "epoch": 5.962133333333333,
      "grad_norm": 0.19629248976707458,
      "learning_rate": 1.2736666666666666e-05,
      "loss": 0.0034,
      "step": 111790
    },
    {
      "epoch": 5.962666666666666,
      "grad_norm": 0.11216519773006439,
      "learning_rate": 1.2733333333333334e-05,
      "loss": 0.0023,
      "step": 111800
    },
    {
      "epoch": 5.9632,
      "grad_norm": 0.1962905079126358,
      "learning_rate": 1.273e-05,
      "loss": 0.0035,
      "step": 111810
    },
    {
      "epoch": 5.963733333333334,
      "grad_norm": 0.22432376444339752,
      "learning_rate": 1.2726666666666668e-05,
      "loss": 0.003,
      "step": 111820
    },
    {
      "epoch": 5.964266666666667,
      "grad_norm": 0.1402091532945633,
      "learning_rate": 1.2723333333333334e-05,
      "loss": 0.0033,
      "step": 111830
    },
    {
      "epoch": 5.9648,
      "grad_norm": 0.16824953258037567,
      "learning_rate": 1.2720000000000002e-05,
      "loss": 0.0024,
      "step": 111840
    },
    {
      "epoch": 5.965333333333334,
      "grad_norm": 0.33648908138275146,
      "learning_rate": 1.2716666666666668e-05,
      "loss": 0.0026,
      "step": 111850
    },
    {
      "epoch": 5.965866666666667,
      "grad_norm": 0.11216652393341064,
      "learning_rate": 1.2713333333333332e-05,
      "loss": 0.0025,
      "step": 111860
    },
    {
      "epoch": 5.9664,
      "grad_norm": 0.028040830045938492,
      "learning_rate": 1.271e-05,
      "loss": 0.004,
      "step": 111870
    },
    {
      "epoch": 5.966933333333333,
      "grad_norm": 0.19629383087158203,
      "learning_rate": 1.2706666666666666e-05,
      "loss": 0.0028,
      "step": 111880
    },
    {
      "epoch": 5.967466666666667,
      "grad_norm": 0.28042882680892944,
      "learning_rate": 1.2703333333333334e-05,
      "loss": 0.0028,
      "step": 111890
    },
    {
      "epoch": 5.968,
      "grad_norm": 0.16825063526630402,
      "learning_rate": 1.27e-05,
      "loss": 0.0023,
      "step": 111900
    },
    {
      "epoch": 5.968533333333333,
      "grad_norm": 0.28040778636932373,
      "learning_rate": 1.2696666666666668e-05,
      "loss": 0.0026,
      "step": 111910
    },
    {
      "epoch": 5.9690666666666665,
      "grad_norm": 0.02804131619632244,
      "learning_rate": 1.2693333333333334e-05,
      "loss": 0.0031,
      "step": 111920
    },
    {
      "epoch": 5.9696,
      "grad_norm": 0.08432143926620483,
      "learning_rate": 1.2690000000000002e-05,
      "loss": 0.0021,
      "step": 111930
    },
    {
      "epoch": 5.970133333333333,
      "grad_norm": 0.028043042868375778,
      "learning_rate": 1.2686666666666667e-05,
      "loss": 0.0027,
      "step": 111940
    },
    {
      "epoch": 5.970666666666666,
      "grad_norm": 0.3645293712615967,
      "learning_rate": 1.2683333333333333e-05,
      "loss": 0.0029,
      "step": 111950
    },
    {
      "epoch": 5.9712,
      "grad_norm": 0.33649858832359314,
      "learning_rate": 1.268e-05,
      "loss": 0.0025,
      "step": 111960
    },
    {
      "epoch": 5.971733333333333,
      "grad_norm": 0.0560825951397419,
      "learning_rate": 1.2676666666666667e-05,
      "loss": 0.0025,
      "step": 111970
    },
    {
      "epoch": 5.972266666666666,
      "grad_norm": 0.08412603288888931,
      "learning_rate": 1.2673333333333335e-05,
      "loss": 0.003,
      "step": 111980
    },
    {
      "epoch": 5.9728,
      "grad_norm": 0.28040748834609985,
      "learning_rate": 1.267e-05,
      "loss": 0.0024,
      "step": 111990
    },
    {
      "epoch": 5.973333333333334,
      "grad_norm": 0.16824844479560852,
      "learning_rate": 1.2666666666666668e-05,
      "loss": 0.002,
      "step": 112000
    },
    {
      "epoch": 5.973866666666667,
      "grad_norm": 0.11216782033443451,
      "learning_rate": 1.2663333333333333e-05,
      "loss": 0.0031,
      "step": 112010
    },
    {
      "epoch": 5.9744,
      "grad_norm": 0.08412712067365646,
      "learning_rate": 1.2659999999999999e-05,
      "loss": 0.0029,
      "step": 112020
    },
    {
      "epoch": 5.974933333333333,
      "grad_norm": 0.08412548154592514,
      "learning_rate": 1.2656666666666667e-05,
      "loss": 0.0023,
      "step": 112030
    },
    {
      "epoch": 5.975466666666667,
      "grad_norm": 0.08412323892116547,
      "learning_rate": 1.2653333333333333e-05,
      "loss": 0.0029,
      "step": 112040
    },
    {
      "epoch": 5.976,
      "grad_norm": 0.08412035554647446,
      "learning_rate": 1.2650000000000001e-05,
      "loss": 0.0021,
      "step": 112050
    },
    {
      "epoch": 5.976533333333333,
      "grad_norm": 0.4486531615257263,
      "learning_rate": 1.2646666666666667e-05,
      "loss": 0.0024,
      "step": 112060
    },
    {
      "epoch": 5.9770666666666665,
      "grad_norm": 0.22432155907154083,
      "learning_rate": 1.2643333333333335e-05,
      "loss": 0.0019,
      "step": 112070
    },
    {
      "epoch": 5.9776,
      "grad_norm": 0.08412700146436691,
      "learning_rate": 1.2640000000000003e-05,
      "loss": 0.0034,
      "step": 112080
    },
    {
      "epoch": 5.978133333333333,
      "grad_norm": 0.30846840143203735,
      "learning_rate": 1.2636666666666665e-05,
      "loss": 0.002,
      "step": 112090
    },
    {
      "epoch": 5.978666666666666,
      "grad_norm": 0.05608506500720978,
      "learning_rate": 1.2633333333333333e-05,
      "loss": 0.0022,
      "step": 112100
    },
    {
      "epoch": 5.9792,
      "grad_norm": 0.22432291507720947,
      "learning_rate": 1.263e-05,
      "loss": 0.0045,
      "step": 112110
    },
    {
      "epoch": 5.979733333333334,
      "grad_norm": 0.13401928544044495,
      "learning_rate": 1.2626666666666667e-05,
      "loss": 0.0031,
      "step": 112120
    },
    {
      "epoch": 5.980266666666667,
      "grad_norm": 0.28041473031044006,
      "learning_rate": 1.2623333333333335e-05,
      "loss": 0.0032,
      "step": 112130
    },
    {
      "epoch": 5.9808,
      "grad_norm": 0.3084574043750763,
      "learning_rate": 1.2620000000000001e-05,
      "loss": 0.0029,
      "step": 112140
    },
    {
      "epoch": 5.981333333333334,
      "grad_norm": 0.0280399601906538,
      "learning_rate": 1.2616666666666669e-05,
      "loss": 0.0031,
      "step": 112150
    },
    {
      "epoch": 5.981866666666667,
      "grad_norm": 0.0841231495141983,
      "learning_rate": 1.2613333333333332e-05,
      "loss": 0.0023,
      "step": 112160
    },
    {
      "epoch": 5.9824,
      "grad_norm": 0.19629107415676117,
      "learning_rate": 1.261e-05,
      "loss": 0.0018,
      "step": 112170
    },
    {
      "epoch": 5.982933333333333,
      "grad_norm": 0.14020179212093353,
      "learning_rate": 1.2606666666666667e-05,
      "loss": 0.0022,
      "step": 112180
    },
    {
      "epoch": 5.983466666666667,
      "grad_norm": 0.25237104296684265,
      "learning_rate": 1.2603333333333334e-05,
      "loss": 0.0034,
      "step": 112190
    },
    {
      "epoch": 5.984,
      "grad_norm": 0.14020635187625885,
      "learning_rate": 1.2600000000000001e-05,
      "loss": 0.0026,
      "step": 112200
    },
    {
      "epoch": 5.984533333333333,
      "grad_norm": 0.12710973620414734,
      "learning_rate": 1.2596666666666667e-05,
      "loss": 0.0041,
      "step": 112210
    },
    {
      "epoch": 5.9850666666666665,
      "grad_norm": 0.19629041850566864,
      "learning_rate": 1.2593333333333335e-05,
      "loss": 0.0025,
      "step": 112220
    },
    {
      "epoch": 5.9856,
      "grad_norm": 0.16824761033058167,
      "learning_rate": 1.2590000000000001e-05,
      "loss": 0.0024,
      "step": 112230
    },
    {
      "epoch": 5.986133333333333,
      "grad_norm": 0.16824695467948914,
      "learning_rate": 1.2586666666666666e-05,
      "loss": 0.0022,
      "step": 112240
    },
    {
      "epoch": 5.986666666666666,
      "grad_norm": 0.08412116020917892,
      "learning_rate": 1.2583333333333334e-05,
      "loss": 0.0023,
      "step": 112250
    },
    {
      "epoch": 5.9872,
      "grad_norm": 0.14020442962646484,
      "learning_rate": 1.258e-05,
      "loss": 0.002,
      "step": 112260
    },
    {
      "epoch": 5.987733333333333,
      "grad_norm": 0.19627967476844788,
      "learning_rate": 1.2576666666666668e-05,
      "loss": 0.0033,
      "step": 112270
    },
    {
      "epoch": 5.988266666666666,
      "grad_norm": 0.08412440121173859,
      "learning_rate": 1.2573333333333334e-05,
      "loss": 0.0019,
      "step": 112280
    },
    {
      "epoch": 5.9888,
      "grad_norm": 0.28041210770606995,
      "learning_rate": 1.2570000000000002e-05,
      "loss": 0.0026,
      "step": 112290
    },
    {
      "epoch": 5.989333333333334,
      "grad_norm": 0.08412662148475647,
      "learning_rate": 1.2566666666666668e-05,
      "loss": 0.0025,
      "step": 112300
    },
    {
      "epoch": 5.989866666666667,
      "grad_norm": 0.08412604033946991,
      "learning_rate": 1.2563333333333336e-05,
      "loss": 0.0023,
      "step": 112310
    },
    {
      "epoch": 5.9904,
      "grad_norm": 0.5047096014022827,
      "learning_rate": 1.256e-05,
      "loss": 0.0017,
      "step": 112320
    },
    {
      "epoch": 5.990933333333333,
      "grad_norm": 0.18089820444583893,
      "learning_rate": 1.2556666666666666e-05,
      "loss": 0.003,
      "step": 112330
    },
    {
      "epoch": 5.991466666666667,
      "grad_norm": 0.05608338862657547,
      "learning_rate": 1.2553333333333334e-05,
      "loss": 0.0027,
      "step": 112340
    },
    {
      "epoch": 5.992,
      "grad_norm": 0.11216345429420471,
      "learning_rate": 1.255e-05,
      "loss": 0.0022,
      "step": 112350
    },
    {
      "epoch": 5.992533333333333,
      "grad_norm": 0.16823658347129822,
      "learning_rate": 1.2546666666666668e-05,
      "loss": 0.0041,
      "step": 112360
    },
    {
      "epoch": 5.9930666666666665,
      "grad_norm": 0.14020617306232452,
      "learning_rate": 1.2543333333333334e-05,
      "loss": 0.0025,
      "step": 112370
    },
    {
      "epoch": 5.9936,
      "grad_norm": 0.05608179420232773,
      "learning_rate": 1.2540000000000002e-05,
      "loss": 0.0028,
      "step": 112380
    },
    {
      "epoch": 5.994133333333333,
      "grad_norm": 0.2243209332227707,
      "learning_rate": 1.2536666666666666e-05,
      "loss": 0.0018,
      "step": 112390
    },
    {
      "epoch": 5.994666666666666,
      "grad_norm": 0.11216370761394501,
      "learning_rate": 1.2533333333333332e-05,
      "loss": 0.0027,
      "step": 112400
    },
    {
      "epoch": 5.9952,
      "grad_norm": 0.552771270275116,
      "learning_rate": 1.253e-05,
      "loss": 0.0023,
      "step": 112410
    },
    {
      "epoch": 5.995733333333334,
      "grad_norm": 0.3084467351436615,
      "learning_rate": 1.2526666666666666e-05,
      "loss": 0.0027,
      "step": 112420
    },
    {
      "epoch": 5.996266666666667,
      "grad_norm": 0.22432534396648407,
      "learning_rate": 1.2523333333333334e-05,
      "loss": 0.003,
      "step": 112430
    },
    {
      "epoch": 5.9968,
      "grad_norm": 0.39255765080451965,
      "learning_rate": 1.252e-05,
      "loss": 0.0028,
      "step": 112440
    },
    {
      "epoch": 5.997333333333334,
      "grad_norm": 0.4266802668571472,
      "learning_rate": 1.2516666666666668e-05,
      "loss": 0.0031,
      "step": 112450
    },
    {
      "epoch": 5.997866666666667,
      "grad_norm": 0.05607975646853447,
      "learning_rate": 1.2513333333333336e-05,
      "loss": 0.0021,
      "step": 112460
    },
    {
      "epoch": 5.9984,
      "grad_norm": 0.28040850162506104,
      "learning_rate": 1.2509999999999999e-05,
      "loss": 0.0037,
      "step": 112470
    },
    {
      "epoch": 5.9989333333333335,
      "grad_norm": 0.28041377663612366,
      "learning_rate": 1.2506666666666667e-05,
      "loss": 0.0019,
      "step": 112480
    },
    {
      "epoch": 5.999466666666667,
      "grad_norm": 0.14020803570747375,
      "learning_rate": 1.2503333333333333e-05,
      "loss": 0.0022,
      "step": 112490
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.11221494525671005,
      "learning_rate": 1.25e-05,
      "loss": 0.0024,
      "step": 112500
    },
    {
      "epoch": 6.0,
      "eval_loss": 0.002873950870707631,
      "eval_runtime": 171.5774,
      "eval_samples_per_second": 1457.068,
      "eval_steps_per_second": 36.427,
      "step": 112500
    },
    {
      "epoch": 6.000533333333333,
      "grad_norm": 0.05608152225613594,
      "learning_rate": 1.2496666666666668e-05,
      "loss": 0.0032,
      "step": 112510
    },
    {
      "epoch": 6.0010666666666665,
      "grad_norm": 2.9871383144808306e-09,
      "learning_rate": 1.2493333333333333e-05,
      "loss": 0.0028,
      "step": 112520
    },
    {
      "epoch": 6.0016,
      "grad_norm": 0.0841216966509819,
      "learning_rate": 1.249e-05,
      "loss": 0.0028,
      "step": 112530
    },
    {
      "epoch": 6.002133333333333,
      "grad_norm": 0.02804020419716835,
      "learning_rate": 1.2486666666666667e-05,
      "loss": 0.0024,
      "step": 112540
    },
    {
      "epoch": 6.002666666666666,
      "grad_norm": 0.14020352065563202,
      "learning_rate": 1.2483333333333335e-05,
      "loss": 0.0028,
      "step": 112550
    },
    {
      "epoch": 6.0032,
      "grad_norm": 0.23690302670001984,
      "learning_rate": 1.248e-05,
      "loss": 0.0032,
      "step": 112560
    },
    {
      "epoch": 6.003733333333333,
      "grad_norm": 0.36453261971473694,
      "learning_rate": 1.2476666666666667e-05,
      "loss": 0.0038,
      "step": 112570
    },
    {
      "epoch": 6.004266666666667,
      "grad_norm": 0.16824021935462952,
      "learning_rate": 1.2473333333333335e-05,
      "loss": 0.0024,
      "step": 112580
    },
    {
      "epoch": 6.0048,
      "grad_norm": 0.05608399957418442,
      "learning_rate": 1.2470000000000001e-05,
      "loss": 0.0034,
      "step": 112590
    },
    {
      "epoch": 6.005333333333334,
      "grad_norm": 0.14020897448062897,
      "learning_rate": 1.2466666666666667e-05,
      "loss": 0.0027,
      "step": 112600
    },
    {
      "epoch": 6.005866666666667,
      "grad_norm": 0.3645176589488983,
      "learning_rate": 1.2463333333333333e-05,
      "loss": 0.0022,
      "step": 112610
    },
    {
      "epoch": 6.0064,
      "grad_norm": 0.14020265638828278,
      "learning_rate": 1.2460000000000001e-05,
      "loss": 0.0029,
      "step": 112620
    },
    {
      "epoch": 6.0069333333333335,
      "grad_norm": 0.05608264356851578,
      "learning_rate": 1.2456666666666667e-05,
      "loss": 0.0027,
      "step": 112630
    },
    {
      "epoch": 6.007466666666667,
      "grad_norm": 1.7146462027994858e-09,
      "learning_rate": 1.2453333333333333e-05,
      "loss": 0.0024,
      "step": 112640
    },
    {
      "epoch": 6.008,
      "grad_norm": 0.05608150735497475,
      "learning_rate": 1.2450000000000001e-05,
      "loss": 0.0033,
      "step": 112650
    },
    {
      "epoch": 6.008533333333333,
      "grad_norm": 0.02803926356136799,
      "learning_rate": 1.2446666666666667e-05,
      "loss": 0.0027,
      "step": 112660
    },
    {
      "epoch": 6.009066666666667,
      "grad_norm": 0.1962825506925583,
      "learning_rate": 1.2443333333333335e-05,
      "loss": 0.0033,
      "step": 112670
    },
    {
      "epoch": 6.0096,
      "grad_norm": 0.22433257102966309,
      "learning_rate": 1.244e-05,
      "loss": 0.0023,
      "step": 112680
    },
    {
      "epoch": 6.010133333333333,
      "grad_norm": 0.08412119746208191,
      "learning_rate": 1.2436666666666667e-05,
      "loss": 0.0012,
      "step": 112690
    },
    {
      "epoch": 6.010666666666666,
      "grad_norm": 0.028039850294589996,
      "learning_rate": 1.2433333333333335e-05,
      "loss": 0.0027,
      "step": 112700
    },
    {
      "epoch": 6.0112,
      "grad_norm": 0.08412101119756699,
      "learning_rate": 1.243e-05,
      "loss": 0.0031,
      "step": 112710
    },
    {
      "epoch": 6.011733333333333,
      "grad_norm": 0.2803996801376343,
      "learning_rate": 1.2426666666666667e-05,
      "loss": 0.0025,
      "step": 112720
    },
    {
      "epoch": 6.012266666666667,
      "grad_norm": 0.47667011618614197,
      "learning_rate": 1.2423333333333334e-05,
      "loss": 0.0025,
      "step": 112730
    },
    {
      "epoch": 6.0128,
      "grad_norm": 0.11216665804386139,
      "learning_rate": 1.2420000000000001e-05,
      "loss": 0.0033,
      "step": 112740
    },
    {
      "epoch": 6.013333333333334,
      "grad_norm": 0.5607934594154358,
      "learning_rate": 1.2416666666666667e-05,
      "loss": 0.0026,
      "step": 112750
    },
    {
      "epoch": 6.013866666666667,
      "grad_norm": 0.05608300864696503,
      "learning_rate": 1.2413333333333334e-05,
      "loss": 0.0029,
      "step": 112760
    },
    {
      "epoch": 6.0144,
      "grad_norm": 3.265317793932354e-09,
      "learning_rate": 1.2410000000000001e-05,
      "loss": 0.0035,
      "step": 112770
    },
    {
      "epoch": 6.0149333333333335,
      "grad_norm": 0.08411857485771179,
      "learning_rate": 1.2406666666666668e-05,
      "loss": 0.0024,
      "step": 112780
    },
    {
      "epoch": 6.015466666666667,
      "grad_norm": 0.14020341634750366,
      "learning_rate": 1.2403333333333334e-05,
      "loss": 0.0026,
      "step": 112790
    },
    {
      "epoch": 6.016,
      "grad_norm": 0.19627884030342102,
      "learning_rate": 1.24e-05,
      "loss": 0.0033,
      "step": 112800
    },
    {
      "epoch": 6.016533333333333,
      "grad_norm": 0.30842673778533936,
      "learning_rate": 1.2396666666666668e-05,
      "loss": 0.003,
      "step": 112810
    },
    {
      "epoch": 6.017066666666667,
      "grad_norm": 0.19627775251865387,
      "learning_rate": 1.2393333333333334e-05,
      "loss": 0.0034,
      "step": 112820
    },
    {
      "epoch": 6.0176,
      "grad_norm": 1.382284842144088e-09,
      "learning_rate": 1.239e-05,
      "loss": 0.0033,
      "step": 112830
    },
    {
      "epoch": 6.018133333333333,
      "grad_norm": 0.19627802073955536,
      "learning_rate": 1.2386666666666668e-05,
      "loss": 0.0018,
      "step": 112840
    },
    {
      "epoch": 6.018666666666666,
      "grad_norm": 0.05607780069112778,
      "learning_rate": 1.2383333333333334e-05,
      "loss": 0.002,
      "step": 112850
    },
    {
      "epoch": 6.0192,
      "grad_norm": 0.05608184263110161,
      "learning_rate": 1.238e-05,
      "loss": 0.0034,
      "step": 112860
    },
    {
      "epoch": 6.019733333333333,
      "grad_norm": 0.2523730993270874,
      "learning_rate": 1.2376666666666666e-05,
      "loss": 0.0023,
      "step": 112870
    },
    {
      "epoch": 6.020266666666667,
      "grad_norm": 0.11216055601835251,
      "learning_rate": 1.2373333333333334e-05,
      "loss": 0.0036,
      "step": 112880
    },
    {
      "epoch": 6.0208,
      "grad_norm": 0.2804047167301178,
      "learning_rate": 1.2370000000000002e-05,
      "loss": 0.0022,
      "step": 112890
    },
    {
      "epoch": 6.021333333333334,
      "grad_norm": 0.14019879698753357,
      "learning_rate": 1.2366666666666666e-05,
      "loss": 0.0024,
      "step": 112900
    },
    {
      "epoch": 6.021866666666667,
      "grad_norm": 0.05607888847589493,
      "learning_rate": 1.2363333333333334e-05,
      "loss": 0.0026,
      "step": 112910
    },
    {
      "epoch": 6.0224,
      "grad_norm": 0.2803958058357239,
      "learning_rate": 1.236e-05,
      "loss": 0.0021,
      "step": 112920
    },
    {
      "epoch": 6.0229333333333335,
      "grad_norm": 0.16823431849479675,
      "learning_rate": 1.2356666666666668e-05,
      "loss": 0.0024,
      "step": 112930
    },
    {
      "epoch": 6.023466666666667,
      "grad_norm": 1.7159531573440745e-09,
      "learning_rate": 1.2353333333333334e-05,
      "loss": 0.003,
      "step": 112940
    },
    {
      "epoch": 6.024,
      "grad_norm": 0.1121595948934555,
      "learning_rate": 1.235e-05,
      "loss": 0.0017,
      "step": 112950
    },
    {
      "epoch": 6.024533333333333,
      "grad_norm": 0.28040027618408203,
      "learning_rate": 1.2346666666666668e-05,
      "loss": 0.0022,
      "step": 112960
    },
    {
      "epoch": 6.025066666666667,
      "grad_norm": 2.864561254867226e-09,
      "learning_rate": 1.2343333333333334e-05,
      "loss": 0.0026,
      "step": 112970
    },
    {
      "epoch": 6.0256,
      "grad_norm": 0.08411923050880432,
      "learning_rate": 1.234e-05,
      "loss": 0.003,
      "step": 112980
    },
    {
      "epoch": 6.026133333333333,
      "grad_norm": 0.1682373285293579,
      "learning_rate": 1.2336666666666667e-05,
      "loss": 0.0031,
      "step": 112990
    },
    {
      "epoch": 6.026666666666666,
      "grad_norm": 0.05607880279421806,
      "learning_rate": 1.2333333333333334e-05,
      "loss": 0.0039,
      "step": 113000
    },
    {
      "epoch": 6.0272,
      "grad_norm": 0.1682334989309311,
      "learning_rate": 1.233e-05,
      "loss": 0.0029,
      "step": 113010
    },
    {
      "epoch": 6.027733333333333,
      "grad_norm": 0.14019571244716644,
      "learning_rate": 1.2326666666666667e-05,
      "loss": 0.0022,
      "step": 113020
    },
    {
      "epoch": 6.028266666666667,
      "grad_norm": 0.11215829104185104,
      "learning_rate": 1.2323333333333334e-05,
      "loss": 0.003,
      "step": 113030
    },
    {
      "epoch": 6.0288,
      "grad_norm": 0.028039315715432167,
      "learning_rate": 1.232e-05,
      "loss": 0.0031,
      "step": 113040
    },
    {
      "epoch": 6.029333333333334,
      "grad_norm": 0.2803860604763031,
      "learning_rate": 1.2316666666666667e-05,
      "loss": 0.0026,
      "step": 113050
    },
    {
      "epoch": 6.029866666666667,
      "grad_norm": 0.44863247871398926,
      "learning_rate": 1.2313333333333333e-05,
      "loss": 0.0029,
      "step": 113060
    },
    {
      "epoch": 6.0304,
      "grad_norm": 0.028038974851369858,
      "learning_rate": 1.231e-05,
      "loss": 0.0026,
      "step": 113070
    },
    {
      "epoch": 6.0309333333333335,
      "grad_norm": 2.1259005069732666,
      "learning_rate": 1.2306666666666669e-05,
      "loss": 0.0024,
      "step": 113080
    },
    {
      "epoch": 6.031466666666667,
      "grad_norm": 0.02803945168852806,
      "learning_rate": 1.2303333333333333e-05,
      "loss": 0.003,
      "step": 113090
    },
    {
      "epoch": 6.032,
      "grad_norm": 0.11253292858600616,
      "learning_rate": 1.23e-05,
      "loss": 0.003,
      "step": 113100
    },
    {
      "epoch": 6.032533333333333,
      "grad_norm": 0.14019466936588287,
      "learning_rate": 1.2296666666666667e-05,
      "loss": 0.0022,
      "step": 113110
    },
    {
      "epoch": 6.033066666666667,
      "grad_norm": 0.08411689847707748,
      "learning_rate": 1.2293333333333335e-05,
      "loss": 0.0031,
      "step": 113120
    },
    {
      "epoch": 6.0336,
      "grad_norm": 0.33647388219833374,
      "learning_rate": 1.2290000000000001e-05,
      "loss": 0.0035,
      "step": 113130
    },
    {
      "epoch": 6.034133333333333,
      "grad_norm": 0.14019720256328583,
      "learning_rate": 1.2286666666666667e-05,
      "loss": 0.0025,
      "step": 113140
    },
    {
      "epoch": 6.034666666666666,
      "grad_norm": 0.05607841908931732,
      "learning_rate": 1.2283333333333335e-05,
      "loss": 0.0022,
      "step": 113150
    },
    {
      "epoch": 6.0352,
      "grad_norm": 0.14020155370235443,
      "learning_rate": 1.2280000000000001e-05,
      "loss": 0.0019,
      "step": 113160
    },
    {
      "epoch": 6.035733333333333,
      "grad_norm": 0.25236934423446655,
      "learning_rate": 1.2276666666666667e-05,
      "loss": 0.0026,
      "step": 113170
    },
    {
      "epoch": 6.036266666666666,
      "grad_norm": 0.08412036299705505,
      "learning_rate": 1.2273333333333333e-05,
      "loss": 0.0026,
      "step": 113180
    },
    {
      "epoch": 6.0368,
      "grad_norm": 0.25234803557395935,
      "learning_rate": 1.2270000000000001e-05,
      "loss": 0.0027,
      "step": 113190
    },
    {
      "epoch": 6.037333333333334,
      "grad_norm": 0.33648058772087097,
      "learning_rate": 1.2266666666666667e-05,
      "loss": 0.0028,
      "step": 113200
    },
    {
      "epoch": 6.037866666666667,
      "grad_norm": 0.3084382116794586,
      "learning_rate": 1.2263333333333333e-05,
      "loss": 0.0034,
      "step": 113210
    },
    {
      "epoch": 6.0384,
      "grad_norm": 0.05607917159795761,
      "learning_rate": 1.2260000000000001e-05,
      "loss": 0.0036,
      "step": 113220
    },
    {
      "epoch": 6.0389333333333335,
      "grad_norm": 1.833303842069256e-09,
      "learning_rate": 1.2256666666666667e-05,
      "loss": 0.0025,
      "step": 113230
    },
    {
      "epoch": 6.039466666666667,
      "grad_norm": 0.056077197194099426,
      "learning_rate": 1.2253333333333333e-05,
      "loss": 0.0022,
      "step": 113240
    },
    {
      "epoch": 6.04,
      "grad_norm": 0.05607815831899643,
      "learning_rate": 1.225e-05,
      "loss": 0.0028,
      "step": 113250
    },
    {
      "epoch": 6.040533333333333,
      "grad_norm": 4.277146636866291e-09,
      "learning_rate": 1.2246666666666667e-05,
      "loss": 0.0015,
      "step": 113260
    },
    {
      "epoch": 6.041066666666667,
      "grad_norm": 0.25234538316726685,
      "learning_rate": 1.2243333333333335e-05,
      "loss": 0.003,
      "step": 113270
    },
    {
      "epoch": 6.0416,
      "grad_norm": 0.16823674738407135,
      "learning_rate": 1.224e-05,
      "loss": 0.003,
      "step": 113280
    },
    {
      "epoch": 6.042133333333333,
      "grad_norm": 0.0841190293431282,
      "learning_rate": 1.2236666666666668e-05,
      "loss": 0.0037,
      "step": 113290
    },
    {
      "epoch": 6.042666666666666,
      "grad_norm": 0.028039170429110527,
      "learning_rate": 1.2233333333333334e-05,
      "loss": 0.0022,
      "step": 113300
    },
    {
      "epoch": 6.0432,
      "grad_norm": 0.28038710355758667,
      "learning_rate": 1.2230000000000001e-05,
      "loss": 0.0018,
      "step": 113310
    },
    {
      "epoch": 6.043733333333333,
      "grad_norm": 0.1682351976633072,
      "learning_rate": 1.2226666666666668e-05,
      "loss": 0.0023,
      "step": 113320
    },
    {
      "epoch": 6.044266666666666,
      "grad_norm": 0.05607917159795761,
      "learning_rate": 1.2223333333333334e-05,
      "loss": 0.0038,
      "step": 113330
    },
    {
      "epoch": 6.0448,
      "grad_norm": 0.1682298481464386,
      "learning_rate": 1.2220000000000002e-05,
      "loss": 0.0027,
      "step": 113340
    },
    {
      "epoch": 6.045333333333334,
      "grad_norm": 0.25234925746917725,
      "learning_rate": 1.2216666666666668e-05,
      "loss": 0.0033,
      "step": 113350
    },
    {
      "epoch": 6.045866666666667,
      "grad_norm": 0.16823048889636993,
      "learning_rate": 1.2213333333333334e-05,
      "loss": 0.0023,
      "step": 113360
    },
    {
      "epoch": 6.0464,
      "grad_norm": 0.1962716430425644,
      "learning_rate": 1.221e-05,
      "loss": 0.0028,
      "step": 113370
    },
    {
      "epoch": 6.0469333333333335,
      "grad_norm": 0.05608055368065834,
      "learning_rate": 1.2206666666666668e-05,
      "loss": 0.0021,
      "step": 113380
    },
    {
      "epoch": 6.047466666666667,
      "grad_norm": 0.1402006894350052,
      "learning_rate": 1.2203333333333334e-05,
      "loss": 0.0018,
      "step": 113390
    },
    {
      "epoch": 6.048,
      "grad_norm": 0.16823513805866241,
      "learning_rate": 1.22e-05,
      "loss": 0.0036,
      "step": 113400
    },
    {
      "epoch": 6.048533333333333,
      "grad_norm": 0.33645862340927124,
      "learning_rate": 1.2196666666666668e-05,
      "loss": 0.0029,
      "step": 113410
    },
    {
      "epoch": 6.049066666666667,
      "grad_norm": 0.14019669592380524,
      "learning_rate": 1.2193333333333334e-05,
      "loss": 0.0028,
      "step": 113420
    },
    {
      "epoch": 6.0496,
      "grad_norm": 0.3364666700363159,
      "learning_rate": 1.219e-05,
      "loss": 0.0024,
      "step": 113430
    },
    {
      "epoch": 6.050133333333333,
      "grad_norm": 0.19626545906066895,
      "learning_rate": 1.2186666666666666e-05,
      "loss": 0.0029,
      "step": 113440
    },
    {
      "epoch": 6.050666666666666,
      "grad_norm": 0.14019757509231567,
      "learning_rate": 1.2183333333333334e-05,
      "loss": 0.0031,
      "step": 113450
    },
    {
      "epoch": 6.0512,
      "grad_norm": 0.16823378205299377,
      "learning_rate": 1.2180000000000002e-05,
      "loss": 0.0027,
      "step": 113460
    },
    {
      "epoch": 6.051733333333333,
      "grad_norm": 0.448615700006485,
      "learning_rate": 1.2176666666666666e-05,
      "loss": 0.0025,
      "step": 113470
    },
    {
      "epoch": 6.052266666666666,
      "grad_norm": 0.11215672641992569,
      "learning_rate": 1.2173333333333334e-05,
      "loss": 0.0032,
      "step": 113480
    },
    {
      "epoch": 6.0528,
      "grad_norm": 0.14019358158111572,
      "learning_rate": 1.217e-05,
      "loss": 0.0018,
      "step": 113490
    },
    {
      "epoch": 6.053333333333334,
      "grad_norm": 0.08411572128534317,
      "learning_rate": 1.2166666666666668e-05,
      "loss": 0.0028,
      "step": 113500
    },
    {
      "epoch": 6.053866666666667,
      "grad_norm": 0.19626908004283905,
      "learning_rate": 1.2163333333333334e-05,
      "loss": 0.0035,
      "step": 113510
    },
    {
      "epoch": 6.0544,
      "grad_norm": 0.33646145462989807,
      "learning_rate": 1.216e-05,
      "loss": 0.0029,
      "step": 113520
    },
    {
      "epoch": 6.0549333333333335,
      "grad_norm": 0.056079134345054626,
      "learning_rate": 1.2156666666666668e-05,
      "loss": 0.003,
      "step": 113530
    },
    {
      "epoch": 6.055466666666667,
      "grad_norm": 0.05607981234788895,
      "learning_rate": 1.2153333333333333e-05,
      "loss": 0.0037,
      "step": 113540
    },
    {
      "epoch": 6.056,
      "grad_norm": 0.3084383010864258,
      "learning_rate": 1.215e-05,
      "loss": 0.0019,
      "step": 113550
    },
    {
      "epoch": 6.056533333333333,
      "grad_norm": 0.11215479671955109,
      "learning_rate": 1.2146666666666667e-05,
      "loss": 0.0021,
      "step": 113560
    },
    {
      "epoch": 6.057066666666667,
      "grad_norm": 0.19627436995506287,
      "learning_rate": 1.2143333333333335e-05,
      "loss": 0.0031,
      "step": 113570
    },
    {
      "epoch": 6.0576,
      "grad_norm": 0.1402018964290619,
      "learning_rate": 1.214e-05,
      "loss": 0.0032,
      "step": 113580
    },
    {
      "epoch": 6.058133333333333,
      "grad_norm": 0.11215708404779434,
      "learning_rate": 1.2136666666666667e-05,
      "loss": 0.0038,
      "step": 113590
    },
    {
      "epoch": 6.058666666666666,
      "grad_norm": 0.19626685976982117,
      "learning_rate": 1.2133333333333335e-05,
      "loss": 0.0025,
      "step": 113600
    },
    {
      "epoch": 6.0592,
      "grad_norm": 0.02803940512239933,
      "learning_rate": 1.213e-05,
      "loss": 0.0028,
      "step": 113610
    },
    {
      "epoch": 6.059733333333333,
      "grad_norm": 0.1402023583650589,
      "learning_rate": 1.2126666666666667e-05,
      "loss": 0.0021,
      "step": 113620
    },
    {
      "epoch": 6.060266666666666,
      "grad_norm": 0.1402013748884201,
      "learning_rate": 1.2123333333333333e-05,
      "loss": 0.0033,
      "step": 113630
    },
    {
      "epoch": 6.0608,
      "grad_norm": 0.11215640604496002,
      "learning_rate": 1.2120000000000001e-05,
      "loss": 0.0035,
      "step": 113640
    },
    {
      "epoch": 6.061333333333334,
      "grad_norm": 0.11215377599000931,
      "learning_rate": 1.2116666666666669e-05,
      "loss": 0.0029,
      "step": 113650
    },
    {
      "epoch": 6.061866666666667,
      "grad_norm": 0.05607684329152107,
      "learning_rate": 1.2113333333333333e-05,
      "loss": 0.0024,
      "step": 113660
    },
    {
      "epoch": 6.0624,
      "grad_norm": 0.378586083650589,
      "learning_rate": 1.2110000000000001e-05,
      "loss": 0.0028,
      "step": 113670
    },
    {
      "epoch": 6.0629333333333335,
      "grad_norm": 0.22432100772857666,
      "learning_rate": 1.2106666666666667e-05,
      "loss": 0.0028,
      "step": 113680
    },
    {
      "epoch": 6.063466666666667,
      "grad_norm": 0.1401960402727127,
      "learning_rate": 1.2103333333333335e-05,
      "loss": 0.0018,
      "step": 113690
    },
    {
      "epoch": 6.064,
      "grad_norm": 0.44860556721687317,
      "learning_rate": 1.2100000000000001e-05,
      "loss": 0.0032,
      "step": 113700
    },
    {
      "epoch": 6.064533333333333,
      "grad_norm": 0.028039872646331787,
      "learning_rate": 1.2096666666666667e-05,
      "loss": 0.0027,
      "step": 113710
    },
    {
      "epoch": 6.065066666666667,
      "grad_norm": 0.11215932667255402,
      "learning_rate": 1.2093333333333335e-05,
      "loss": 0.0028,
      "step": 113720
    },
    {
      "epoch": 6.0656,
      "grad_norm": 0.08412019163370132,
      "learning_rate": 1.209e-05,
      "loss": 0.0031,
      "step": 113730
    },
    {
      "epoch": 6.066133333333333,
      "grad_norm": 0.16823703050613403,
      "learning_rate": 1.2086666666666667e-05,
      "loss": 0.0025,
      "step": 113740
    },
    {
      "epoch": 6.066666666666666,
      "grad_norm": 0.2523441016674042,
      "learning_rate": 1.2083333333333333e-05,
      "loss": 0.0028,
      "step": 113750
    },
    {
      "epoch": 6.0672,
      "grad_norm": 1.3974460363388062,
      "learning_rate": 1.2080000000000001e-05,
      "loss": 0.0024,
      "step": 113760
    },
    {
      "epoch": 6.067733333333333,
      "grad_norm": 0.11215691268444061,
      "learning_rate": 1.2076666666666667e-05,
      "loss": 0.0022,
      "step": 113770
    },
    {
      "epoch": 6.068266666666666,
      "grad_norm": 0.33646640181541443,
      "learning_rate": 1.2073333333333333e-05,
      "loss": 0.0027,
      "step": 113780
    },
    {
      "epoch": 6.0688,
      "grad_norm": 0.0280732661485672,
      "learning_rate": 1.2070000000000001e-05,
      "loss": 0.002,
      "step": 113790
    },
    {
      "epoch": 6.069333333333334,
      "grad_norm": 0.02803904376924038,
      "learning_rate": 1.2066666666666667e-05,
      "loss": 0.0022,
      "step": 113800
    },
    {
      "epoch": 6.069866666666667,
      "grad_norm": 3.197006659405588e-09,
      "learning_rate": 1.2063333333333334e-05,
      "loss": 0.0038,
      "step": 113810
    },
    {
      "epoch": 6.0704,
      "grad_norm": 0.11215116828680038,
      "learning_rate": 1.206e-05,
      "loss": 0.0023,
      "step": 113820
    },
    {
      "epoch": 6.0709333333333335,
      "grad_norm": 0.0841154083609581,
      "learning_rate": 1.2056666666666668e-05,
      "loss": 0.0032,
      "step": 113830
    },
    {
      "epoch": 6.071466666666667,
      "grad_norm": 0.14019185304641724,
      "learning_rate": 1.2053333333333334e-05,
      "loss": 0.0034,
      "step": 113840
    },
    {
      "epoch": 6.072,
      "grad_norm": 1.5759851932525635,
      "learning_rate": 1.205e-05,
      "loss": 0.0024,
      "step": 113850
    },
    {
      "epoch": 6.072533333333333,
      "grad_norm": 0.33646348118782043,
      "learning_rate": 1.2046666666666668e-05,
      "loss": 0.0033,
      "step": 113860
    },
    {
      "epoch": 6.073066666666667,
      "grad_norm": 0.08411624282598495,
      "learning_rate": 1.2043333333333334e-05,
      "loss": 0.002,
      "step": 113870
    },
    {
      "epoch": 6.0736,
      "grad_norm": 0.05607627332210541,
      "learning_rate": 1.204e-05,
      "loss": 0.0025,
      "step": 113880
    },
    {
      "epoch": 6.074133333333333,
      "grad_norm": 0.05607513338327408,
      "learning_rate": 1.2036666666666668e-05,
      "loss": 0.0031,
      "step": 113890
    },
    {
      "epoch": 6.074666666666666,
      "grad_norm": 0.08411456644535065,
      "learning_rate": 1.2033333333333334e-05,
      "loss": 0.0031,
      "step": 113900
    },
    {
      "epoch": 6.0752,
      "grad_norm": 0.056076716631650925,
      "learning_rate": 1.2030000000000002e-05,
      "loss": 0.0025,
      "step": 113910
    },
    {
      "epoch": 6.075733333333333,
      "grad_norm": 0.1682262420654297,
      "learning_rate": 1.2026666666666666e-05,
      "loss": 0.0029,
      "step": 113920
    },
    {
      "epoch": 6.076266666666666,
      "grad_norm": 0.11215215176343918,
      "learning_rate": 1.2023333333333334e-05,
      "loss": 0.003,
      "step": 113930
    },
    {
      "epoch": 6.0768,
      "grad_norm": 0.19626742601394653,
      "learning_rate": 1.202e-05,
      "loss": 0.0025,
      "step": 113940
    },
    {
      "epoch": 6.077333333333334,
      "grad_norm": 0.028037941083312035,
      "learning_rate": 1.2016666666666668e-05,
      "loss": 0.0037,
      "step": 113950
    },
    {
      "epoch": 6.077866666666667,
      "grad_norm": 0.02803732268512249,
      "learning_rate": 1.2013333333333334e-05,
      "loss": 0.0033,
      "step": 113960
    },
    {
      "epoch": 6.0784,
      "grad_norm": 0.30841848254203796,
      "learning_rate": 1.201e-05,
      "loss": 0.0033,
      "step": 113970
    },
    {
      "epoch": 6.0789333333333335,
      "grad_norm": 0.14018858969211578,
      "learning_rate": 1.2006666666666668e-05,
      "loss": 0.0025,
      "step": 113980
    },
    {
      "epoch": 6.079466666666667,
      "grad_norm": 0.19626720249652863,
      "learning_rate": 1.2003333333333334e-05,
      "loss": 0.0028,
      "step": 113990
    },
    {
      "epoch": 6.08,
      "grad_norm": 0.11215429753065109,
      "learning_rate": 1.2e-05,
      "loss": 0.0026,
      "step": 114000
    },
    {
      "epoch": 6.080533333333333,
      "grad_norm": 0.14019325375556946,
      "learning_rate": 1.1996666666666666e-05,
      "loss": 0.0021,
      "step": 114010
    },
    {
      "epoch": 6.081066666666667,
      "grad_norm": 0.22429941594600677,
      "learning_rate": 1.1993333333333334e-05,
      "loss": 0.0031,
      "step": 114020
    },
    {
      "epoch": 6.0816,
      "grad_norm": 0.056075576692819595,
      "learning_rate": 1.199e-05,
      "loss": 0.0022,
      "step": 114030
    },
    {
      "epoch": 6.082133333333333,
      "grad_norm": 0.028038498014211655,
      "learning_rate": 1.1986666666666667e-05,
      "loss": 0.0033,
      "step": 114040
    },
    {
      "epoch": 6.082666666666666,
      "grad_norm": 2.36767294659046e-09,
      "learning_rate": 1.1983333333333334e-05,
      "loss": 0.0025,
      "step": 114050
    },
    {
      "epoch": 6.0832,
      "grad_norm": 0.08411512523889542,
      "learning_rate": 1.198e-05,
      "loss": 0.0031,
      "step": 114060
    },
    {
      "epoch": 6.083733333333333,
      "grad_norm": 0.3364446759223938,
      "learning_rate": 1.1976666666666667e-05,
      "loss": 0.0028,
      "step": 114070
    },
    {
      "epoch": 6.084266666666666,
      "grad_norm": 0.08411244302988052,
      "learning_rate": 1.1973333333333334e-05,
      "loss": 0.0018,
      "step": 114080
    },
    {
      "epoch": 6.0848,
      "grad_norm": 0.4485912322998047,
      "learning_rate": 1.197e-05,
      "loss": 0.0029,
      "step": 114090
    },
    {
      "epoch": 6.085333333333334,
      "grad_norm": 0.36450231075286865,
      "learning_rate": 1.1966666666666668e-05,
      "loss": 0.0028,
      "step": 114100
    },
    {
      "epoch": 6.085866666666667,
      "grad_norm": 0.02805865928530693,
      "learning_rate": 1.1963333333333333e-05,
      "loss": 0.003,
      "step": 114110
    },
    {
      "epoch": 6.0864,
      "grad_norm": 0.028038909658789635,
      "learning_rate": 1.196e-05,
      "loss": 0.0038,
      "step": 114120
    },
    {
      "epoch": 6.0869333333333335,
      "grad_norm": 0.19626940786838531,
      "learning_rate": 1.1956666666666667e-05,
      "loss": 0.0031,
      "step": 114130
    },
    {
      "epoch": 6.087466666666667,
      "grad_norm": 0.08411240577697754,
      "learning_rate": 1.1953333333333335e-05,
      "loss": 0.0024,
      "step": 114140
    },
    {
      "epoch": 6.088,
      "grad_norm": 0.028037311509251595,
      "learning_rate": 1.195e-05,
      "loss": 0.0022,
      "step": 114150
    },
    {
      "epoch": 6.088533333333333,
      "grad_norm": 0.14019159972667694,
      "learning_rate": 1.1946666666666667e-05,
      "loss": 0.0022,
      "step": 114160
    },
    {
      "epoch": 6.089066666666667,
      "grad_norm": 0.05607541278004646,
      "learning_rate": 1.1943333333333335e-05,
      "loss": 0.0031,
      "step": 114170
    },
    {
      "epoch": 6.0896,
      "grad_norm": 0.1962687075138092,
      "learning_rate": 1.1940000000000001e-05,
      "loss": 0.0024,
      "step": 114180
    },
    {
      "epoch": 6.090133333333333,
      "grad_norm": 2.4020743370056152,
      "learning_rate": 1.1936666666666667e-05,
      "loss": 0.0027,
      "step": 114190
    },
    {
      "epoch": 6.0906666666666665,
      "grad_norm": 0.14019781351089478,
      "learning_rate": 1.1933333333333333e-05,
      "loss": 0.0027,
      "step": 114200
    },
    {
      "epoch": 6.0912,
      "grad_norm": 0.4205748736858368,
      "learning_rate": 1.1930000000000001e-05,
      "loss": 0.0025,
      "step": 114210
    },
    {
      "epoch": 6.091733333333333,
      "grad_norm": 0.0560760498046875,
      "learning_rate": 1.1926666666666667e-05,
      "loss": 0.0026,
      "step": 114220
    },
    {
      "epoch": 6.092266666666666,
      "grad_norm": 0.08411096036434174,
      "learning_rate": 1.1923333333333333e-05,
      "loss": 0.0028,
      "step": 114230
    },
    {
      "epoch": 6.0928,
      "grad_norm": 0.2243005782365799,
      "learning_rate": 1.1920000000000001e-05,
      "loss": 0.002,
      "step": 114240
    },
    {
      "epoch": 6.093333333333334,
      "grad_norm": 0.19626930356025696,
      "learning_rate": 1.1916666666666667e-05,
      "loss": 0.0019,
      "step": 114250
    },
    {
      "epoch": 6.093866666666667,
      "grad_norm": 0.11214949935674667,
      "learning_rate": 1.1913333333333333e-05,
      "loss": 0.0027,
      "step": 114260
    },
    {
      "epoch": 6.0944,
      "grad_norm": 0.3364448845386505,
      "learning_rate": 1.1910000000000001e-05,
      "loss": 0.0024,
      "step": 114270
    },
    {
      "epoch": 6.0949333333333335,
      "grad_norm": 0.19626927375793457,
      "learning_rate": 1.1906666666666667e-05,
      "loss": 0.0026,
      "step": 114280
    },
    {
      "epoch": 6.095466666666667,
      "grad_norm": 0.02803889475762844,
      "learning_rate": 1.1903333333333335e-05,
      "loss": 0.0026,
      "step": 114290
    },
    {
      "epoch": 6.096,
      "grad_norm": 0.08411304652690887,
      "learning_rate": 1.19e-05,
      "loss": 0.0028,
      "step": 114300
    },
    {
      "epoch": 6.096533333333333,
      "grad_norm": 0.05607358366250992,
      "learning_rate": 1.1896666666666667e-05,
      "loss": 0.0022,
      "step": 114310
    },
    {
      "epoch": 6.097066666666667,
      "grad_norm": 0.1682276576757431,
      "learning_rate": 1.1893333333333334e-05,
      "loss": 0.0021,
      "step": 114320
    },
    {
      "epoch": 6.0976,
      "grad_norm": 0.28037741780281067,
      "learning_rate": 1.1890000000000001e-05,
      "loss": 0.0028,
      "step": 114330
    },
    {
      "epoch": 6.098133333333333,
      "grad_norm": 0.056076887995004654,
      "learning_rate": 1.1886666666666667e-05,
      "loss": 0.002,
      "step": 114340
    },
    {
      "epoch": 6.0986666666666665,
      "grad_norm": 0.5795788764953613,
      "learning_rate": 1.1883333333333334e-05,
      "loss": 0.0034,
      "step": 114350
    },
    {
      "epoch": 6.0992,
      "grad_norm": 7.07317260317808e-10,
      "learning_rate": 1.1880000000000001e-05,
      "loss": 0.0025,
      "step": 114360
    },
    {
      "epoch": 6.099733333333333,
      "grad_norm": 1.5735133196415063e-09,
      "learning_rate": 1.1876666666666668e-05,
      "loss": 0.0019,
      "step": 114370
    },
    {
      "epoch": 6.100266666666666,
      "grad_norm": 0.11214987933635712,
      "learning_rate": 1.1873333333333334e-05,
      "loss": 0.0027,
      "step": 114380
    },
    {
      "epoch": 6.1008,
      "grad_norm": 0.0280376598238945,
      "learning_rate": 1.187e-05,
      "loss": 0.0028,
      "step": 114390
    },
    {
      "epoch": 6.101333333333334,
      "grad_norm": 0.14018559455871582,
      "learning_rate": 1.1866666666666668e-05,
      "loss": 0.0024,
      "step": 114400
    },
    {
      "epoch": 6.101866666666667,
      "grad_norm": 0.3925217092037201,
      "learning_rate": 1.1863333333333334e-05,
      "loss": 0.0022,
      "step": 114410
    },
    {
      "epoch": 6.1024,
      "grad_norm": 0.028038274496793747,
      "learning_rate": 1.186e-05,
      "loss": 0.0016,
      "step": 114420
    },
    {
      "epoch": 6.1029333333333335,
      "grad_norm": 0.30842646956443787,
      "learning_rate": 1.1856666666666668e-05,
      "loss": 0.0022,
      "step": 114430
    },
    {
      "epoch": 6.103466666666667,
      "grad_norm": 0.11214838176965714,
      "learning_rate": 1.1853333333333334e-05,
      "loss": 0.0036,
      "step": 114440
    },
    {
      "epoch": 6.104,
      "grad_norm": 0.0560748390853405,
      "learning_rate": 1.185e-05,
      "loss": 0.004,
      "step": 114450
    },
    {
      "epoch": 6.104533333333333,
      "grad_norm": 0.05607479065656662,
      "learning_rate": 1.1846666666666666e-05,
      "loss": 0.0028,
      "step": 114460
    },
    {
      "epoch": 6.105066666666667,
      "grad_norm": 0.1121453121304512,
      "learning_rate": 1.1843333333333334e-05,
      "loss": 0.0029,
      "step": 114470
    },
    {
      "epoch": 6.1056,
      "grad_norm": 0.08411312848329544,
      "learning_rate": 1.1840000000000002e-05,
      "loss": 0.0021,
      "step": 114480
    },
    {
      "epoch": 6.106133333333333,
      "grad_norm": 0.056076060980558395,
      "learning_rate": 1.1836666666666666e-05,
      "loss": 0.0031,
      "step": 114490
    },
    {
      "epoch": 6.1066666666666665,
      "grad_norm": 3.92765420187402e-09,
      "learning_rate": 1.1833333333333334e-05,
      "loss": 0.0026,
      "step": 114500
    },
    {
      "epoch": 6.1072,
      "grad_norm": 1.3141843169250933e-09,
      "learning_rate": 1.183e-05,
      "loss": 0.0024,
      "step": 114510
    },
    {
      "epoch": 6.107733333333333,
      "grad_norm": 0.2523331344127655,
      "learning_rate": 1.1826666666666668e-05,
      "loss": 0.0023,
      "step": 114520
    },
    {
      "epoch": 6.108266666666666,
      "grad_norm": 0.05607381463050842,
      "learning_rate": 1.1823333333333334e-05,
      "loss": 0.0032,
      "step": 114530
    },
    {
      "epoch": 6.1088,
      "grad_norm": 0.056072428822517395,
      "learning_rate": 1.182e-05,
      "loss": 0.0021,
      "step": 114540
    },
    {
      "epoch": 6.109333333333334,
      "grad_norm": 0.1401873230934143,
      "learning_rate": 1.1816666666666668e-05,
      "loss": 0.0025,
      "step": 114550
    },
    {
      "epoch": 6.109866666666667,
      "grad_norm": 0.08411157131195068,
      "learning_rate": 1.1813333333333334e-05,
      "loss": 0.002,
      "step": 114560
    },
    {
      "epoch": 6.1104,
      "grad_norm": 0.16822263598442078,
      "learning_rate": 1.181e-05,
      "loss": 0.0026,
      "step": 114570
    },
    {
      "epoch": 6.1109333333333336,
      "grad_norm": 0.28038662672042847,
      "learning_rate": 1.1806666666666667e-05,
      "loss": 0.0036,
      "step": 114580
    },
    {
      "epoch": 6.111466666666667,
      "grad_norm": 0.1962634176015854,
      "learning_rate": 1.1803333333333334e-05,
      "loss": 0.0023,
      "step": 114590
    },
    {
      "epoch": 6.112,
      "grad_norm": 0.028036659583449364,
      "learning_rate": 1.18e-05,
      "loss": 0.0023,
      "step": 114600
    },
    {
      "epoch": 6.112533333333333,
      "grad_norm": 0.1682192087173462,
      "learning_rate": 1.1796666666666667e-05,
      "loss": 0.0032,
      "step": 114610
    },
    {
      "epoch": 6.113066666666667,
      "grad_norm": 0.02803746424615383,
      "learning_rate": 1.1793333333333334e-05,
      "loss": 0.0045,
      "step": 114620
    },
    {
      "epoch": 6.1136,
      "grad_norm": 0.14018556475639343,
      "learning_rate": 1.179e-05,
      "loss": 0.0029,
      "step": 114630
    },
    {
      "epoch": 6.114133333333333,
      "grad_norm": 0.14018286764621735,
      "learning_rate": 1.1786666666666667e-05,
      "loss": 0.0029,
      "step": 114640
    },
    {
      "epoch": 6.1146666666666665,
      "grad_norm": 0.16822564601898193,
      "learning_rate": 1.1783333333333333e-05,
      "loss": 0.002,
      "step": 114650
    },
    {
      "epoch": 6.1152,
      "grad_norm": 0.028038516640663147,
      "learning_rate": 1.178e-05,
      "loss": 0.0027,
      "step": 114660
    },
    {
      "epoch": 6.115733333333333,
      "grad_norm": 0.05607597157359123,
      "learning_rate": 1.1776666666666669e-05,
      "loss": 0.0022,
      "step": 114670
    },
    {
      "epoch": 6.116266666666666,
      "grad_norm": 4.965202915485634e-09,
      "learning_rate": 1.1773333333333333e-05,
      "loss": 0.0016,
      "step": 114680
    },
    {
      "epoch": 6.1168,
      "grad_norm": 0.11215122044086456,
      "learning_rate": 1.177e-05,
      "loss": 0.0039,
      "step": 114690
    },
    {
      "epoch": 6.117333333333334,
      "grad_norm": 0.056075699627399445,
      "learning_rate": 1.1766666666666667e-05,
      "loss": 0.0027,
      "step": 114700
    },
    {
      "epoch": 6.117866666666667,
      "grad_norm": 0.028036564588546753,
      "learning_rate": 1.1763333333333335e-05,
      "loss": 0.0019,
      "step": 114710
    },
    {
      "epoch": 6.1184,
      "grad_norm": 0.47661298513412476,
      "learning_rate": 1.1760000000000001e-05,
      "loss": 0.0032,
      "step": 114720
    },
    {
      "epoch": 6.118933333333334,
      "grad_norm": 0.30840355157852173,
      "learning_rate": 1.1756666666666667e-05,
      "loss": 0.002,
      "step": 114730
    },
    {
      "epoch": 6.119466666666667,
      "grad_norm": 0.02803666517138481,
      "learning_rate": 1.1753333333333335e-05,
      "loss": 0.0023,
      "step": 114740
    },
    {
      "epoch": 6.12,
      "grad_norm": 0.056072261184453964,
      "learning_rate": 1.175e-05,
      "loss": 0.0016,
      "step": 114750
    },
    {
      "epoch": 6.120533333333333,
      "grad_norm": 0.3925120532512665,
      "learning_rate": 1.1746666666666667e-05,
      "loss": 0.002,
      "step": 114760
    },
    {
      "epoch": 6.121066666666667,
      "grad_norm": 0.2803574204444885,
      "learning_rate": 1.1743333333333333e-05,
      "loss": 0.0034,
      "step": 114770
    },
    {
      "epoch": 6.1216,
      "grad_norm": 0.08411156386137009,
      "learning_rate": 1.1740000000000001e-05,
      "loss": 0.0029,
      "step": 114780
    },
    {
      "epoch": 6.122133333333333,
      "grad_norm": 0.16822072863578796,
      "learning_rate": 1.1736666666666667e-05,
      "loss": 0.0021,
      "step": 114790
    },
    {
      "epoch": 6.1226666666666665,
      "grad_norm": 0.05607258528470993,
      "learning_rate": 1.1733333333333333e-05,
      "loss": 0.0023,
      "step": 114800
    },
    {
      "epoch": 6.1232,
      "grad_norm": 0.11214515566825867,
      "learning_rate": 1.1730000000000001e-05,
      "loss": 0.0026,
      "step": 114810
    },
    {
      "epoch": 6.123733333333333,
      "grad_norm": 0.11214537173509598,
      "learning_rate": 1.1726666666666667e-05,
      "loss": 0.0028,
      "step": 114820
    },
    {
      "epoch": 6.124266666666666,
      "grad_norm": 0.05607399716973305,
      "learning_rate": 1.1723333333333333e-05,
      "loss": 0.003,
      "step": 114830
    },
    {
      "epoch": 6.1248,
      "grad_norm": 0.19626082479953766,
      "learning_rate": 1.172e-05,
      "loss": 0.0023,
      "step": 114840
    },
    {
      "epoch": 6.125333333333334,
      "grad_norm": 0.028036722913384438,
      "learning_rate": 1.1716666666666667e-05,
      "loss": 0.0027,
      "step": 114850
    },
    {
      "epoch": 6.125866666666667,
      "grad_norm": 0.05607451871037483,
      "learning_rate": 1.1713333333333335e-05,
      "loss": 0.0028,
      "step": 114860
    },
    {
      "epoch": 6.1264,
      "grad_norm": 0.05607364699244499,
      "learning_rate": 1.171e-05,
      "loss": 0.0023,
      "step": 114870
    },
    {
      "epoch": 6.126933333333334,
      "grad_norm": 0.1962563842535019,
      "learning_rate": 1.1706666666666668e-05,
      "loss": 0.0024,
      "step": 114880
    },
    {
      "epoch": 6.127466666666667,
      "grad_norm": 0.08410945534706116,
      "learning_rate": 1.1703333333333334e-05,
      "loss": 0.0014,
      "step": 114890
    },
    {
      "epoch": 6.128,
      "grad_norm": 0.028036559000611305,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 0.0039,
      "step": 114900
    },
    {
      "epoch": 6.128533333333333,
      "grad_norm": 0.19625462591648102,
      "learning_rate": 1.1696666666666668e-05,
      "loss": 0.0023,
      "step": 114910
    },
    {
      "epoch": 6.129066666666667,
      "grad_norm": 0.1682131290435791,
      "learning_rate": 1.1693333333333334e-05,
      "loss": 0.0028,
      "step": 114920
    },
    {
      "epoch": 6.1296,
      "grad_norm": 0.08410908281803131,
      "learning_rate": 1.1690000000000002e-05,
      "loss": 0.0021,
      "step": 114930
    },
    {
      "epoch": 6.130133333333333,
      "grad_norm": 1.1317267417907715,
      "learning_rate": 1.1686666666666666e-05,
      "loss": 0.0027,
      "step": 114940
    },
    {
      "epoch": 6.1306666666666665,
      "grad_norm": 0.1682162880897522,
      "learning_rate": 1.1683333333333334e-05,
      "loss": 0.0021,
      "step": 114950
    },
    {
      "epoch": 6.1312,
      "grad_norm": 2.571959978325822e-09,
      "learning_rate": 1.168e-05,
      "loss": 0.0021,
      "step": 114960
    },
    {
      "epoch": 6.131733333333333,
      "grad_norm": 0.2803657352924347,
      "learning_rate": 1.1676666666666668e-05,
      "loss": 0.0027,
      "step": 114970
    },
    {
      "epoch": 6.132266666666666,
      "grad_norm": 0.11214572936296463,
      "learning_rate": 1.1673333333333334e-05,
      "loss": 0.0026,
      "step": 114980
    },
    {
      "epoch": 6.1328,
      "grad_norm": 0.028036445379257202,
      "learning_rate": 1.167e-05,
      "loss": 0.0024,
      "step": 114990
    },
    {
      "epoch": 6.133333333333334,
      "grad_norm": 0.3084089159965515,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 0.0031,
      "step": 115000
    },
    {
      "epoch": 6.133866666666667,
      "grad_norm": 0.5606997013092041,
      "learning_rate": 1.1663333333333334e-05,
      "loss": 0.0027,
      "step": 115010
    },
    {
      "epoch": 6.1344,
      "grad_norm": 0.28037333488464355,
      "learning_rate": 1.166e-05,
      "loss": 0.0015,
      "step": 115020
    },
    {
      "epoch": 6.134933333333334,
      "grad_norm": 0.1682242602109909,
      "learning_rate": 1.1656666666666666e-05,
      "loss": 0.0033,
      "step": 115030
    },
    {
      "epoch": 6.135466666666667,
      "grad_norm": 0.08410865813493729,
      "learning_rate": 1.1653333333333334e-05,
      "loss": 0.0018,
      "step": 115040
    },
    {
      "epoch": 6.136,
      "grad_norm": 0.168216273188591,
      "learning_rate": 1.1650000000000002e-05,
      "loss": 0.0017,
      "step": 115050
    },
    {
      "epoch": 6.136533333333333,
      "grad_norm": 0.22428767383098602,
      "learning_rate": 1.1646666666666666e-05,
      "loss": 0.0031,
      "step": 115060
    },
    {
      "epoch": 6.137066666666667,
      "grad_norm": 0.2523229122161865,
      "learning_rate": 1.1643333333333334e-05,
      "loss": 0.0029,
      "step": 115070
    },
    {
      "epoch": 6.1376,
      "grad_norm": 0.1682177633047104,
      "learning_rate": 1.164e-05,
      "loss": 0.0019,
      "step": 115080
    },
    {
      "epoch": 6.138133333333333,
      "grad_norm": 0.028036361560225487,
      "learning_rate": 1.1636666666666666e-05,
      "loss": 0.0033,
      "step": 115090
    },
    {
      "epoch": 6.1386666666666665,
      "grad_norm": 0.0013080721255391836,
      "learning_rate": 1.1633333333333334e-05,
      "loss": 0.0016,
      "step": 115100
    },
    {
      "epoch": 6.1392,
      "grad_norm": 0.532681941986084,
      "learning_rate": 1.163e-05,
      "loss": 0.002,
      "step": 115110
    },
    {
      "epoch": 6.139733333333333,
      "grad_norm": 0.11214427649974823,
      "learning_rate": 1.1626666666666668e-05,
      "loss": 0.0023,
      "step": 115120
    },
    {
      "epoch": 6.140266666666666,
      "grad_norm": 0.1682189553976059,
      "learning_rate": 1.1623333333333333e-05,
      "loss": 0.0037,
      "step": 115130
    },
    {
      "epoch": 6.1408,
      "grad_norm": 0.14018109440803528,
      "learning_rate": 1.162e-05,
      "loss": 0.0025,
      "step": 115140
    },
    {
      "epoch": 6.141333333333334,
      "grad_norm": 0.14017923176288605,
      "learning_rate": 1.1616666666666667e-05,
      "loss": 0.0022,
      "step": 115150
    },
    {
      "epoch": 6.141866666666667,
      "grad_norm": 0.11214801669120789,
      "learning_rate": 1.1613333333333335e-05,
      "loss": 0.0024,
      "step": 115160
    },
    {
      "epoch": 6.1424,
      "grad_norm": 0.0560729093849659,
      "learning_rate": 1.161e-05,
      "loss": 0.0018,
      "step": 115170
    },
    {
      "epoch": 6.142933333333334,
      "grad_norm": 0.22428132593631744,
      "learning_rate": 1.1606666666666667e-05,
      "loss": 0.002,
      "step": 115180
    },
    {
      "epoch": 6.143466666666667,
      "grad_norm": 0.22429053485393524,
      "learning_rate": 1.1603333333333335e-05,
      "loss": 0.0025,
      "step": 115190
    },
    {
      "epoch": 6.144,
      "grad_norm": 2.6707638323131278e-09,
      "learning_rate": 1.16e-05,
      "loss": 0.0043,
      "step": 115200
    },
    {
      "epoch": 6.144533333333333,
      "grad_norm": 0.16822503507137299,
      "learning_rate": 1.1596666666666667e-05,
      "loss": 0.0021,
      "step": 115210
    },
    {
      "epoch": 6.145066666666667,
      "grad_norm": 6.395860729924152e-09,
      "learning_rate": 1.1593333333333333e-05,
      "loss": 0.0016,
      "step": 115220
    },
    {
      "epoch": 6.1456,
      "grad_norm": 0.05607281252741814,
      "learning_rate": 1.159e-05,
      "loss": 0.0018,
      "step": 115230
    },
    {
      "epoch": 6.146133333333333,
      "grad_norm": 0.028034958988428116,
      "learning_rate": 1.1586666666666669e-05,
      "loss": 0.0035,
      "step": 115240
    },
    {
      "epoch": 6.1466666666666665,
      "grad_norm": 0.22428816556930542,
      "learning_rate": 1.1583333333333333e-05,
      "loss": 0.0025,
      "step": 115250
    },
    {
      "epoch": 6.1472,
      "grad_norm": 0.056073859333992004,
      "learning_rate": 1.1580000000000001e-05,
      "loss": 0.0022,
      "step": 115260
    },
    {
      "epoch": 6.147733333333333,
      "grad_norm": 0.08411188423633575,
      "learning_rate": 1.1576666666666667e-05,
      "loss": 0.0026,
      "step": 115270
    },
    {
      "epoch": 6.148266666666666,
      "grad_norm": 0.22428885102272034,
      "learning_rate": 1.1573333333333333e-05,
      "loss": 0.0024,
      "step": 115280
    },
    {
      "epoch": 6.1488,
      "grad_norm": 0.08410631865262985,
      "learning_rate": 1.1570000000000001e-05,
      "loss": 0.0019,
      "step": 115290
    },
    {
      "epoch": 6.149333333333334,
      "grad_norm": 0.11214236915111542,
      "learning_rate": 1.1566666666666667e-05,
      "loss": 0.0038,
      "step": 115300
    },
    {
      "epoch": 6.149866666666667,
      "grad_norm": 0.1121443435549736,
      "learning_rate": 1.1563333333333335e-05,
      "loss": 0.0035,
      "step": 115310
    },
    {
      "epoch": 6.1504,
      "grad_norm": 2.7552449211043495e-09,
      "learning_rate": 1.156e-05,
      "loss": 0.0027,
      "step": 115320
    },
    {
      "epoch": 6.150933333333334,
      "grad_norm": 0.19624802470207214,
      "learning_rate": 1.1556666666666667e-05,
      "loss": 0.0029,
      "step": 115330
    },
    {
      "epoch": 6.151466666666667,
      "grad_norm": 0.1401807814836502,
      "learning_rate": 1.1553333333333333e-05,
      "loss": 0.003,
      "step": 115340
    },
    {
      "epoch": 6.152,
      "grad_norm": 0.05607357621192932,
      "learning_rate": 1.1550000000000001e-05,
      "loss": 0.0025,
      "step": 115350
    },
    {
      "epoch": 6.152533333333333,
      "grad_norm": 0.19625212252140045,
      "learning_rate": 1.1546666666666667e-05,
      "loss": 0.0032,
      "step": 115360
    },
    {
      "epoch": 6.153066666666667,
      "grad_norm": 0.2242823988199234,
      "learning_rate": 1.1543333333333333e-05,
      "loss": 0.0025,
      "step": 115370
    },
    {
      "epoch": 6.1536,
      "grad_norm": 0.11214584857225418,
      "learning_rate": 1.1540000000000001e-05,
      "loss": 0.002,
      "step": 115380
    },
    {
      "epoch": 6.154133333333333,
      "grad_norm": 0.1401776373386383,
      "learning_rate": 1.1536666666666667e-05,
      "loss": 0.0022,
      "step": 115390
    },
    {
      "epoch": 6.1546666666666665,
      "grad_norm": 0.11214273422956467,
      "learning_rate": 1.1533333333333334e-05,
      "loss": 0.0018,
      "step": 115400
    },
    {
      "epoch": 6.1552,
      "grad_norm": 0.07737577706575394,
      "learning_rate": 1.153e-05,
      "loss": 0.0032,
      "step": 115410
    },
    {
      "epoch": 6.155733333333333,
      "grad_norm": 0.1401769369840622,
      "learning_rate": 1.1526666666666668e-05,
      "loss": 0.0023,
      "step": 115420
    },
    {
      "epoch": 6.156266666666666,
      "grad_norm": 0.14018014073371887,
      "learning_rate": 1.1523333333333334e-05,
      "loss": 0.002,
      "step": 115430
    },
    {
      "epoch": 6.1568,
      "grad_norm": 0.36446553468704224,
      "learning_rate": 1.152e-05,
      "loss": 0.0026,
      "step": 115440
    },
    {
      "epoch": 6.157333333333334,
      "grad_norm": 0.3364141285419464,
      "learning_rate": 1.1516666666666668e-05,
      "loss": 0.0021,
      "step": 115450
    },
    {
      "epoch": 6.157866666666667,
      "grad_norm": 0.05607357248663902,
      "learning_rate": 1.1513333333333334e-05,
      "loss": 0.003,
      "step": 115460
    },
    {
      "epoch": 6.1584,
      "grad_norm": 0.2523289620876312,
      "learning_rate": 1.151e-05,
      "loss": 0.0027,
      "step": 115470
    },
    {
      "epoch": 6.158933333333334,
      "grad_norm": 0.19625107944011688,
      "learning_rate": 1.1506666666666668e-05,
      "loss": 0.0017,
      "step": 115480
    },
    {
      "epoch": 6.159466666666667,
      "grad_norm": 0.16821305453777313,
      "learning_rate": 1.1503333333333334e-05,
      "loss": 0.0011,
      "step": 115490
    },
    {
      "epoch": 6.16,
      "grad_norm": 0.1943640410900116,
      "learning_rate": 1.1500000000000002e-05,
      "loss": 0.0019,
      "step": 115500
    },
    {
      "epoch": 6.160533333333333,
      "grad_norm": 0.22429734468460083,
      "learning_rate": 1.1496666666666666e-05,
      "loss": 0.0016,
      "step": 115510
    },
    {
      "epoch": 6.161066666666667,
      "grad_norm": 0.1682123988866806,
      "learning_rate": 1.1493333333333334e-05,
      "loss": 0.0017,
      "step": 115520
    },
    {
      "epoch": 6.1616,
      "grad_norm": 0.05607038363814354,
      "learning_rate": 1.149e-05,
      "loss": 0.0026,
      "step": 115530
    },
    {
      "epoch": 6.162133333333333,
      "grad_norm": 0.056072596460580826,
      "learning_rate": 1.1486666666666668e-05,
      "loss": 0.0016,
      "step": 115540
    },
    {
      "epoch": 6.1626666666666665,
      "grad_norm": 0.08410879969596863,
      "learning_rate": 1.1483333333333334e-05,
      "loss": 0.0032,
      "step": 115550
    },
    {
      "epoch": 6.1632,
      "grad_norm": 0.16821573674678802,
      "learning_rate": 1.148e-05,
      "loss": 0.0015,
      "step": 115560
    },
    {
      "epoch": 6.163733333333333,
      "grad_norm": 0.56070476770401,
      "learning_rate": 1.1476666666666668e-05,
      "loss": 0.0025,
      "step": 115570
    },
    {
      "epoch": 6.164266666666666,
      "grad_norm": 0.3364339768886566,
      "learning_rate": 1.1473333333333334e-05,
      "loss": 0.0018,
      "step": 115580
    },
    {
      "epoch": 6.1648,
      "grad_norm": 0.11214812099933624,
      "learning_rate": 1.147e-05,
      "loss": 0.0028,
      "step": 115590
    },
    {
      "epoch": 6.165333333333333,
      "grad_norm": 2.3461868003948894e-09,
      "learning_rate": 1.1466666666666666e-05,
      "loss": 0.0026,
      "step": 115600
    },
    {
      "epoch": 6.165866666666667,
      "grad_norm": 0.3644771873950958,
      "learning_rate": 1.1463333333333334e-05,
      "loss": 0.0028,
      "step": 115610
    },
    {
      "epoch": 6.1664,
      "grad_norm": 2.1819650530829904e-09,
      "learning_rate": 1.146e-05,
      "loss": 0.0024,
      "step": 115620
    },
    {
      "epoch": 6.166933333333334,
      "grad_norm": 0.16821202635765076,
      "learning_rate": 1.1456666666666667e-05,
      "loss": 0.0031,
      "step": 115630
    },
    {
      "epoch": 6.167466666666667,
      "grad_norm": 0.08410748094320297,
      "learning_rate": 1.1453333333333334e-05,
      "loss": 0.0036,
      "step": 115640
    },
    {
      "epoch": 6.168,
      "grad_norm": 0.19625356793403625,
      "learning_rate": 1.145e-05,
      "loss": 0.002,
      "step": 115650
    },
    {
      "epoch": 6.168533333333333,
      "grad_norm": 0.28034693002700806,
      "learning_rate": 1.1446666666666667e-05,
      "loss": 0.0026,
      "step": 115660
    },
    {
      "epoch": 6.169066666666667,
      "grad_norm": 0.23579515516757965,
      "learning_rate": 1.1443333333333334e-05,
      "loss": 0.0028,
      "step": 115670
    },
    {
      "epoch": 6.1696,
      "grad_norm": 3.5160796496569446e-09,
      "learning_rate": 1.144e-05,
      "loss": 0.002,
      "step": 115680
    },
    {
      "epoch": 6.170133333333333,
      "grad_norm": 0.2242838591337204,
      "learning_rate": 1.1436666666666668e-05,
      "loss": 0.0035,
      "step": 115690
    },
    {
      "epoch": 6.1706666666666665,
      "grad_norm": 0.16821393370628357,
      "learning_rate": 1.1433333333333333e-05,
      "loss": 0.0017,
      "step": 115700
    },
    {
      "epoch": 6.1712,
      "grad_norm": 0.05607355013489723,
      "learning_rate": 1.143e-05,
      "loss": 0.0028,
      "step": 115710
    },
    {
      "epoch": 6.171733333333333,
      "grad_norm": 0.1401812881231308,
      "learning_rate": 1.1426666666666667e-05,
      "loss": 0.002,
      "step": 115720
    },
    {
      "epoch": 6.172266666666666,
      "grad_norm": 0.08410479873418808,
      "learning_rate": 1.1423333333333335e-05,
      "loss": 0.0025,
      "step": 115730
    },
    {
      "epoch": 6.1728,
      "grad_norm": 0.16821728646755219,
      "learning_rate": 1.142e-05,
      "loss": 0.003,
      "step": 115740
    },
    {
      "epoch": 6.173333333333334,
      "grad_norm": 0.02803504280745983,
      "learning_rate": 1.1416666666666667e-05,
      "loss": 0.0035,
      "step": 115750
    },
    {
      "epoch": 6.173866666666667,
      "grad_norm": 1.62505320311368e-09,
      "learning_rate": 1.1413333333333335e-05,
      "loss": 0.0024,
      "step": 115760
    },
    {
      "epoch": 6.1744,
      "grad_norm": 0.16821512579917908,
      "learning_rate": 1.141e-05,
      "loss": 0.0025,
      "step": 115770
    },
    {
      "epoch": 6.174933333333334,
      "grad_norm": 0.13234612345695496,
      "learning_rate": 1.1406666666666667e-05,
      "loss": 0.0033,
      "step": 115780
    },
    {
      "epoch": 6.175466666666667,
      "grad_norm": 0.028036674484610558,
      "learning_rate": 1.1403333333333333e-05,
      "loss": 0.0023,
      "step": 115790
    },
    {
      "epoch": 6.176,
      "grad_norm": 0.08410618454217911,
      "learning_rate": 1.1400000000000001e-05,
      "loss": 0.0032,
      "step": 115800
    },
    {
      "epoch": 6.176533333333333,
      "grad_norm": 0.028034966439008713,
      "learning_rate": 1.1396666666666667e-05,
      "loss": 0.0027,
      "step": 115810
    },
    {
      "epoch": 6.177066666666667,
      "grad_norm": 0.056071385741233826,
      "learning_rate": 1.1393333333333333e-05,
      "loss": 0.0022,
      "step": 115820
    },
    {
      "epoch": 6.1776,
      "grad_norm": 0.1121465265750885,
      "learning_rate": 1.1390000000000001e-05,
      "loss": 0.0034,
      "step": 115830
    },
    {
      "epoch": 6.178133333333333,
      "grad_norm": 0.19624988734722137,
      "learning_rate": 1.1386666666666667e-05,
      "loss": 0.0023,
      "step": 115840
    },
    {
      "epoch": 6.1786666666666665,
      "grad_norm": 0.3924768567085266,
      "learning_rate": 1.1383333333333333e-05,
      "loss": 0.0024,
      "step": 115850
    },
    {
      "epoch": 6.1792,
      "grad_norm": 0.1121448278427124,
      "learning_rate": 1.1380000000000001e-05,
      "loss": 0.0028,
      "step": 115860
    },
    {
      "epoch": 6.179733333333333,
      "grad_norm": 0.22430068254470825,
      "learning_rate": 1.1376666666666667e-05,
      "loss": 0.0021,
      "step": 115870
    },
    {
      "epoch": 6.180266666666666,
      "grad_norm": 0.08410315960645676,
      "learning_rate": 1.1373333333333335e-05,
      "loss": 0.0043,
      "step": 115880
    },
    {
      "epoch": 6.1808,
      "grad_norm": 0.02803485095500946,
      "learning_rate": 1.137e-05,
      "loss": 0.0029,
      "step": 115890
    },
    {
      "epoch": 6.181333333333333,
      "grad_norm": 0.3083932101726532,
      "learning_rate": 1.1366666666666667e-05,
      "loss": 0.0028,
      "step": 115900
    },
    {
      "epoch": 6.181866666666667,
      "grad_norm": 0.0560695044696331,
      "learning_rate": 1.1363333333333334e-05,
      "loss": 0.0021,
      "step": 115910
    },
    {
      "epoch": 6.1824,
      "grad_norm": 0.05606922507286072,
      "learning_rate": 1.1360000000000001e-05,
      "loss": 0.0037,
      "step": 115920
    },
    {
      "epoch": 6.182933333333334,
      "grad_norm": 0.36445707082748413,
      "learning_rate": 1.1356666666666667e-05,
      "loss": 0.0029,
      "step": 115930
    },
    {
      "epoch": 6.183466666666667,
      "grad_norm": 0.19930393993854523,
      "learning_rate": 1.1353333333333334e-05,
      "loss": 0.0024,
      "step": 115940
    },
    {
      "epoch": 6.184,
      "grad_norm": 0.05606811121106148,
      "learning_rate": 1.1350000000000001e-05,
      "loss": 0.0018,
      "step": 115950
    },
    {
      "epoch": 6.184533333333333,
      "grad_norm": 0.12081287801265717,
      "learning_rate": 1.1346666666666666e-05,
      "loss": 0.0023,
      "step": 115960
    },
    {
      "epoch": 6.185066666666667,
      "grad_norm": 0.2803390622138977,
      "learning_rate": 1.1343333333333334e-05,
      "loss": 0.0018,
      "step": 115970
    },
    {
      "epoch": 6.1856,
      "grad_norm": 0.08410511910915375,
      "learning_rate": 1.134e-05,
      "loss": 0.002,
      "step": 115980
    },
    {
      "epoch": 6.186133333333333,
      "grad_norm": 0.2803584933280945,
      "learning_rate": 1.1336666666666668e-05,
      "loss": 0.0021,
      "step": 115990
    },
    {
      "epoch": 6.1866666666666665,
      "grad_norm": 0.30837681889533997,
      "learning_rate": 1.1333333333333334e-05,
      "loss": 0.0025,
      "step": 116000
    },
    {
      "epoch": 6.1872,
      "grad_norm": 0.30838409066200256,
      "learning_rate": 1.133e-05,
      "loss": 0.0026,
      "step": 116010
    },
    {
      "epoch": 6.187733333333333,
      "grad_norm": 0.1401771456003189,
      "learning_rate": 1.1326666666666668e-05,
      "loss": 0.0029,
      "step": 116020
    },
    {
      "epoch": 6.188266666666666,
      "grad_norm": 0.08410753309726715,
      "learning_rate": 1.1323333333333334e-05,
      "loss": 0.0031,
      "step": 116030
    },
    {
      "epoch": 6.1888,
      "grad_norm": 0.08410729467868805,
      "learning_rate": 1.132e-05,
      "loss": 0.0028,
      "step": 116040
    },
    {
      "epoch": 6.189333333333333,
      "grad_norm": 0.336415559053421,
      "learning_rate": 1.1316666666666668e-05,
      "loss": 0.0016,
      "step": 116050
    },
    {
      "epoch": 6.189866666666667,
      "grad_norm": 0.28034788370132446,
      "learning_rate": 1.1313333333333334e-05,
      "loss": 0.0021,
      "step": 116060
    },
    {
      "epoch": 6.1904,
      "grad_norm": 0.05606924369931221,
      "learning_rate": 1.1310000000000002e-05,
      "loss": 0.0022,
      "step": 116070
    },
    {
      "epoch": 6.190933333333334,
      "grad_norm": 0.16820764541625977,
      "learning_rate": 1.1306666666666666e-05,
      "loss": 0.0026,
      "step": 116080
    },
    {
      "epoch": 6.191466666666667,
      "grad_norm": 2.561927336941494e-09,
      "learning_rate": 1.1303333333333334e-05,
      "loss": 0.0036,
      "step": 116090
    },
    {
      "epoch": 6.192,
      "grad_norm": 0.02803458645939827,
      "learning_rate": 1.13e-05,
      "loss": 0.0024,
      "step": 116100
    },
    {
      "epoch": 6.1925333333333334,
      "grad_norm": 0.028034618124365807,
      "learning_rate": 1.1296666666666668e-05,
      "loss": 0.0021,
      "step": 116110
    },
    {
      "epoch": 6.193066666666667,
      "grad_norm": 0.056071750819683075,
      "learning_rate": 1.1293333333333334e-05,
      "loss": 0.0018,
      "step": 116120
    },
    {
      "epoch": 6.1936,
      "grad_norm": 0.1121412143111229,
      "learning_rate": 1.129e-05,
      "loss": 0.0031,
      "step": 116130
    },
    {
      "epoch": 6.194133333333333,
      "grad_norm": 0.1962449699640274,
      "learning_rate": 1.1286666666666668e-05,
      "loss": 0.0035,
      "step": 116140
    },
    {
      "epoch": 6.1946666666666665,
      "grad_norm": 0.308375746011734,
      "learning_rate": 1.1283333333333333e-05,
      "loss": 0.0023,
      "step": 116150
    },
    {
      "epoch": 6.1952,
      "grad_norm": 0.11213621497154236,
      "learning_rate": 1.128e-05,
      "loss": 0.0034,
      "step": 116160
    },
    {
      "epoch": 6.195733333333333,
      "grad_norm": 0.30837205052375793,
      "learning_rate": 1.1276666666666667e-05,
      "loss": 0.0031,
      "step": 116170
    },
    {
      "epoch": 6.196266666666666,
      "grad_norm": 2.3462278786468005e-09,
      "learning_rate": 1.1273333333333334e-05,
      "loss": 0.0029,
      "step": 116180
    },
    {
      "epoch": 6.1968,
      "grad_norm": 0.19623617827892303,
      "learning_rate": 1.127e-05,
      "loss": 0.0019,
      "step": 116190
    },
    {
      "epoch": 6.197333333333333,
      "grad_norm": 0.05606864020228386,
      "learning_rate": 1.1266666666666667e-05,
      "loss": 0.0026,
      "step": 116200
    },
    {
      "epoch": 6.197866666666667,
      "grad_norm": 0.1682048738002777,
      "learning_rate": 1.1263333333333334e-05,
      "loss": 0.0033,
      "step": 116210
    },
    {
      "epoch": 6.1984,
      "grad_norm": 0.2523088753223419,
      "learning_rate": 1.126e-05,
      "loss": 0.0029,
      "step": 116220
    },
    {
      "epoch": 6.198933333333334,
      "grad_norm": 0.30837738513946533,
      "learning_rate": 1.1256666666666667e-05,
      "loss": 0.0032,
      "step": 116230
    },
    {
      "epoch": 6.199466666666667,
      "grad_norm": 0.08410339802503586,
      "learning_rate": 1.1253333333333335e-05,
      "loss": 0.0021,
      "step": 116240
    },
    {
      "epoch": 6.2,
      "grad_norm": 0.14017271995544434,
      "learning_rate": 1.125e-05,
      "loss": 0.0026,
      "step": 116250
    },
    {
      "epoch": 6.2005333333333335,
      "grad_norm": 0.11213599890470505,
      "learning_rate": 1.1246666666666669e-05,
      "loss": 0.0028,
      "step": 116260
    },
    {
      "epoch": 6.201066666666667,
      "grad_norm": 0.3924750089645386,
      "learning_rate": 1.1243333333333333e-05,
      "loss": 0.0017,
      "step": 116270
    },
    {
      "epoch": 6.2016,
      "grad_norm": 0.22442670166492462,
      "learning_rate": 1.124e-05,
      "loss": 0.0031,
      "step": 116280
    },
    {
      "epoch": 6.202133333333333,
      "grad_norm": 4.1137506734401086e-09,
      "learning_rate": 1.1236666666666667e-05,
      "loss": 0.0031,
      "step": 116290
    },
    {
      "epoch": 6.2026666666666666,
      "grad_norm": 0.4205165505409241,
      "learning_rate": 1.1233333333333333e-05,
      "loss": 0.0031,
      "step": 116300
    },
    {
      "epoch": 6.2032,
      "grad_norm": 0.34327182173728943,
      "learning_rate": 1.1230000000000001e-05,
      "loss": 0.0032,
      "step": 116310
    },
    {
      "epoch": 6.203733333333333,
      "grad_norm": 0.9708315134048462,
      "learning_rate": 1.1226666666666667e-05,
      "loss": 0.0032,
      "step": 116320
    },
    {
      "epoch": 6.204266666666666,
      "grad_norm": 2.0064101219177246,
      "learning_rate": 1.1223333333333335e-05,
      "loss": 0.0034,
      "step": 116330
    },
    {
      "epoch": 6.2048,
      "grad_norm": 0.14017152786254883,
      "learning_rate": 1.122e-05,
      "loss": 0.0034,
      "step": 116340
    },
    {
      "epoch": 6.205333333333333,
      "grad_norm": 0.028033969923853874,
      "learning_rate": 1.1216666666666667e-05,
      "loss": 0.0025,
      "step": 116350
    },
    {
      "epoch": 6.205866666666667,
      "grad_norm": 0.16820485889911652,
      "learning_rate": 1.1213333333333333e-05,
      "loss": 0.0033,
      "step": 116360
    },
    {
      "epoch": 6.2064,
      "grad_norm": 0.2803328335285187,
      "learning_rate": 1.1210000000000001e-05,
      "loss": 0.0016,
      "step": 116370
    },
    {
      "epoch": 6.206933333333334,
      "grad_norm": 0.05606939643621445,
      "learning_rate": 1.1206666666666667e-05,
      "loss": 0.0033,
      "step": 116380
    },
    {
      "epoch": 6.207466666666667,
      "grad_norm": 0.4205177128314972,
      "learning_rate": 1.1203333333333333e-05,
      "loss": 0.0025,
      "step": 116390
    },
    {
      "epoch": 6.208,
      "grad_norm": 0.19623441994190216,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 0.0023,
      "step": 116400
    },
    {
      "epoch": 6.2085333333333335,
      "grad_norm": 0.08410320430994034,
      "learning_rate": 1.1196666666666667e-05,
      "loss": 0.0019,
      "step": 116410
    },
    {
      "epoch": 6.209066666666667,
      "grad_norm": 0.028033819049596786,
      "learning_rate": 1.1193333333333333e-05,
      "loss": 0.0025,
      "step": 116420
    },
    {
      "epoch": 6.2096,
      "grad_norm": 0.36443451046943665,
      "learning_rate": 1.1190000000000001e-05,
      "loss": 0.0018,
      "step": 116430
    },
    {
      "epoch": 6.210133333333333,
      "grad_norm": 0.11213856935501099,
      "learning_rate": 1.1186666666666667e-05,
      "loss": 0.0025,
      "step": 116440
    },
    {
      "epoch": 6.210666666666667,
      "grad_norm": 0.08410240709781647,
      "learning_rate": 1.1183333333333335e-05,
      "loss": 0.0026,
      "step": 116450
    },
    {
      "epoch": 6.2112,
      "grad_norm": 0.16820502281188965,
      "learning_rate": 1.118e-05,
      "loss": 0.0018,
      "step": 116460
    },
    {
      "epoch": 6.211733333333333,
      "grad_norm": 0.02803419530391693,
      "learning_rate": 1.1176666666666668e-05,
      "loss": 0.0024,
      "step": 116470
    },
    {
      "epoch": 6.212266666666666,
      "grad_norm": 0.02803373709321022,
      "learning_rate": 1.1173333333333334e-05,
      "loss": 0.0031,
      "step": 116480
    },
    {
      "epoch": 6.2128,
      "grad_norm": 0.14017176628112793,
      "learning_rate": 1.117e-05,
      "loss": 0.0031,
      "step": 116490
    },
    {
      "epoch": 6.213333333333333,
      "grad_norm": 0.3364116847515106,
      "learning_rate": 1.1166666666666668e-05,
      "loss": 0.0033,
      "step": 116500
    },
    {
      "epoch": 6.213866666666667,
      "grad_norm": 0.02803407423198223,
      "learning_rate": 1.1163333333333334e-05,
      "loss": 0.002,
      "step": 116510
    },
    {
      "epoch": 6.2144,
      "grad_norm": 0.08410080522298813,
      "learning_rate": 1.1160000000000002e-05,
      "loss": 0.0029,
      "step": 116520
    },
    {
      "epoch": 6.214933333333334,
      "grad_norm": 0.11213544756174088,
      "learning_rate": 1.1156666666666666e-05,
      "loss": 0.0026,
      "step": 116530
    },
    {
      "epoch": 6.215466666666667,
      "grad_norm": 3.7044465273083915e-09,
      "learning_rate": 1.1153333333333334e-05,
      "loss": 0.0024,
      "step": 116540
    },
    {
      "epoch": 6.216,
      "grad_norm": 0.11213968694210052,
      "learning_rate": 1.115e-05,
      "loss": 0.004,
      "step": 116550
    },
    {
      "epoch": 6.2165333333333335,
      "grad_norm": 0.33641111850738525,
      "learning_rate": 1.1146666666666668e-05,
      "loss": 0.002,
      "step": 116560
    },
    {
      "epoch": 6.217066666666667,
      "grad_norm": 0.028034666553139687,
      "learning_rate": 1.1143333333333334e-05,
      "loss": 0.0028,
      "step": 116570
    },
    {
      "epoch": 6.2176,
      "grad_norm": 0.08410435169935226,
      "learning_rate": 1.114e-05,
      "loss": 0.0015,
      "step": 116580
    },
    {
      "epoch": 6.218133333333333,
      "grad_norm": 0.3083787262439728,
      "learning_rate": 1.1136666666666668e-05,
      "loss": 0.0017,
      "step": 116590
    },
    {
      "epoch": 6.218666666666667,
      "grad_norm": 0.42049694061279297,
      "learning_rate": 1.1133333333333334e-05,
      "loss": 0.0021,
      "step": 116600
    },
    {
      "epoch": 6.2192,
      "grad_norm": 0.14017169177532196,
      "learning_rate": 1.113e-05,
      "loss": 0.0025,
      "step": 116610
    },
    {
      "epoch": 6.219733333333333,
      "grad_norm": 5.319937379510975e-09,
      "learning_rate": 1.1126666666666668e-05,
      "loss": 0.002,
      "step": 116620
    },
    {
      "epoch": 6.220266666666666,
      "grad_norm": 0.11213912069797516,
      "learning_rate": 1.1123333333333334e-05,
      "loss": 0.0018,
      "step": 116630
    },
    {
      "epoch": 6.2208,
      "grad_norm": 0.22426460683345795,
      "learning_rate": 1.112e-05,
      "loss": 0.0035,
      "step": 116640
    },
    {
      "epoch": 6.221333333333333,
      "grad_norm": 0.08410231024026871,
      "learning_rate": 1.1116666666666666e-05,
      "loss": 0.0024,
      "step": 116650
    },
    {
      "epoch": 6.221866666666667,
      "grad_norm": 0.11214198917150497,
      "learning_rate": 1.1113333333333334e-05,
      "loss": 0.0033,
      "step": 116660
    },
    {
      "epoch": 6.2224,
      "grad_norm": 2.4618849181479163e-09,
      "learning_rate": 1.111e-05,
      "loss": 0.0039,
      "step": 116670
    },
    {
      "epoch": 6.222933333333334,
      "grad_norm": 0.028034457936882973,
      "learning_rate": 1.1106666666666666e-05,
      "loss": 0.0031,
      "step": 116680
    },
    {
      "epoch": 6.223466666666667,
      "grad_norm": 0.2523052394390106,
      "learning_rate": 1.1103333333333334e-05,
      "loss": 0.002,
      "step": 116690
    },
    {
      "epoch": 6.224,
      "grad_norm": 0.19624152779579163,
      "learning_rate": 1.11e-05,
      "loss": 0.0023,
      "step": 116700
    },
    {
      "epoch": 6.2245333333333335,
      "grad_norm": 0.16820578277111053,
      "learning_rate": 1.1096666666666668e-05,
      "loss": 0.0027,
      "step": 116710
    },
    {
      "epoch": 6.225066666666667,
      "grad_norm": 0.3363986015319824,
      "learning_rate": 1.1093333333333333e-05,
      "loss": 0.0039,
      "step": 116720
    },
    {
      "epoch": 6.2256,
      "grad_norm": 0.05606937035918236,
      "learning_rate": 1.109e-05,
      "loss": 0.0032,
      "step": 116730
    },
    {
      "epoch": 6.226133333333333,
      "grad_norm": 0.11213690042495728,
      "learning_rate": 1.1086666666666667e-05,
      "loss": 0.0036,
      "step": 116740
    },
    {
      "epoch": 6.226666666666667,
      "grad_norm": 0.2769976854324341,
      "learning_rate": 1.1083333333333335e-05,
      "loss": 0.0027,
      "step": 116750
    },
    {
      "epoch": 6.2272,
      "grad_norm": 0.6088456511497498,
      "learning_rate": 1.108e-05,
      "loss": 0.0037,
      "step": 116760
    },
    {
      "epoch": 6.227733333333333,
      "grad_norm": 0.4205408990383148,
      "learning_rate": 1.1076666666666667e-05,
      "loss": 0.0029,
      "step": 116770
    },
    {
      "epoch": 6.228266666666666,
      "grad_norm": 0.05606972053647041,
      "learning_rate": 1.1073333333333335e-05,
      "loss": 0.0025,
      "step": 116780
    },
    {
      "epoch": 6.2288,
      "grad_norm": 0.33640262484550476,
      "learning_rate": 1.107e-05,
      "loss": 0.0019,
      "step": 116790
    },
    {
      "epoch": 6.229333333333333,
      "grad_norm": 0.11213584244251251,
      "learning_rate": 1.1066666666666667e-05,
      "loss": 0.0036,
      "step": 116800
    },
    {
      "epoch": 6.229866666666666,
      "grad_norm": 0.14017190039157867,
      "learning_rate": 1.1063333333333335e-05,
      "loss": 0.002,
      "step": 116810
    },
    {
      "epoch": 6.2304,
      "grad_norm": 0.1962457150220871,
      "learning_rate": 1.106e-05,
      "loss": 0.0018,
      "step": 116820
    },
    {
      "epoch": 6.230933333333334,
      "grad_norm": 0.24253025650978088,
      "learning_rate": 1.1056666666666667e-05,
      "loss": 0.0024,
      "step": 116830
    },
    {
      "epoch": 6.231466666666667,
      "grad_norm": 0.30838340520858765,
      "learning_rate": 1.1053333333333333e-05,
      "loss": 0.0038,
      "step": 116840
    },
    {
      "epoch": 6.232,
      "grad_norm": 0.11213689297437668,
      "learning_rate": 1.1050000000000001e-05,
      "loss": 0.0017,
      "step": 116850
    },
    {
      "epoch": 6.2325333333333335,
      "grad_norm": 0.1682041436433792,
      "learning_rate": 1.1046666666666667e-05,
      "loss": 0.0021,
      "step": 116860
    },
    {
      "epoch": 6.233066666666667,
      "grad_norm": 0.22427189350128174,
      "learning_rate": 1.1043333333333333e-05,
      "loss": 0.0031,
      "step": 116870
    },
    {
      "epoch": 6.2336,
      "grad_norm": 0.028033625334501266,
      "learning_rate": 1.1040000000000001e-05,
      "loss": 0.0032,
      "step": 116880
    },
    {
      "epoch": 6.234133333333333,
      "grad_norm": 0.028033645823597908,
      "learning_rate": 1.1036666666666667e-05,
      "loss": 0.0042,
      "step": 116890
    },
    {
      "epoch": 6.234666666666667,
      "grad_norm": 0.028033845126628876,
      "learning_rate": 1.1033333333333335e-05,
      "loss": 0.0025,
      "step": 116900
    },
    {
      "epoch": 6.2352,
      "grad_norm": 0.05606928840279579,
      "learning_rate": 1.103e-05,
      "loss": 0.0023,
      "step": 116910
    },
    {
      "epoch": 6.235733333333333,
      "grad_norm": 0.08410260081291199,
      "learning_rate": 1.1026666666666667e-05,
      "loss": 0.0026,
      "step": 116920
    },
    {
      "epoch": 6.236266666666666,
      "grad_norm": 0.2523025572299957,
      "learning_rate": 1.1023333333333333e-05,
      "loss": 0.0025,
      "step": 116930
    },
    {
      "epoch": 6.2368,
      "grad_norm": 0.25230294466018677,
      "learning_rate": 1.1020000000000001e-05,
      "loss": 0.0039,
      "step": 116940
    },
    {
      "epoch": 6.237333333333333,
      "grad_norm": 0.1962389498949051,
      "learning_rate": 1.1016666666666667e-05,
      "loss": 0.0024,
      "step": 116950
    },
    {
      "epoch": 6.237866666666667,
      "grad_norm": 0.1962394118309021,
      "learning_rate": 1.1013333333333333e-05,
      "loss": 0.0022,
      "step": 116960
    },
    {
      "epoch": 6.2384,
      "grad_norm": 0.2803386449813843,
      "learning_rate": 1.1010000000000001e-05,
      "loss": 0.0016,
      "step": 116970
    },
    {
      "epoch": 6.238933333333334,
      "grad_norm": 0.22426554560661316,
      "learning_rate": 1.1006666666666666e-05,
      "loss": 0.002,
      "step": 116980
    },
    {
      "epoch": 6.239466666666667,
      "grad_norm": 0.028034577146172523,
      "learning_rate": 1.1003333333333334e-05,
      "loss": 0.0024,
      "step": 116990
    },
    {
      "epoch": 6.24,
      "grad_norm": 0.08410366624593735,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.0027,
      "step": 117000
    },
    {
      "epoch": 6.2405333333333335,
      "grad_norm": 0.05606668442487717,
      "learning_rate": 1.0996666666666668e-05,
      "loss": 0.0035,
      "step": 117010
    },
    {
      "epoch": 6.241066666666667,
      "grad_norm": 0.22427286207675934,
      "learning_rate": 1.0993333333333334e-05,
      "loss": 0.0028,
      "step": 117020
    },
    {
      "epoch": 6.2416,
      "grad_norm": 0.05607030540704727,
      "learning_rate": 1.099e-05,
      "loss": 0.003,
      "step": 117030
    },
    {
      "epoch": 6.242133333333333,
      "grad_norm": 0.08410567045211792,
      "learning_rate": 1.0986666666666668e-05,
      "loss": 0.0031,
      "step": 117040
    },
    {
      "epoch": 6.242666666666667,
      "grad_norm": 1.4373422452251816e-09,
      "learning_rate": 1.0983333333333334e-05,
      "loss": 0.0036,
      "step": 117050
    },
    {
      "epoch": 6.2432,
      "grad_norm": 0.1401662677526474,
      "learning_rate": 1.098e-05,
      "loss": 0.0017,
      "step": 117060
    },
    {
      "epoch": 6.243733333333333,
      "grad_norm": 0.028033675625920296,
      "learning_rate": 1.0976666666666668e-05,
      "loss": 0.0027,
      "step": 117070
    },
    {
      "epoch": 6.244266666666666,
      "grad_norm": 0.11213508248329163,
      "learning_rate": 1.0973333333333334e-05,
      "loss": 0.0031,
      "step": 117080
    },
    {
      "epoch": 6.2448,
      "grad_norm": 0.1401670128107071,
      "learning_rate": 1.0970000000000002e-05,
      "loss": 0.0023,
      "step": 117090
    },
    {
      "epoch": 6.245333333333333,
      "grad_norm": 0.22426959872245789,
      "learning_rate": 1.0966666666666666e-05,
      "loss": 0.0023,
      "step": 117100
    },
    {
      "epoch": 6.245866666666666,
      "grad_norm": 0.0841013714671135,
      "learning_rate": 1.0963333333333334e-05,
      "loss": 0.0021,
      "step": 117110
    },
    {
      "epoch": 6.2464,
      "grad_norm": 0.05606916919350624,
      "learning_rate": 1.096e-05,
      "loss": 0.0027,
      "step": 117120
    },
    {
      "epoch": 6.246933333333334,
      "grad_norm": 0.22427448630332947,
      "learning_rate": 1.0956666666666668e-05,
      "loss": 0.0035,
      "step": 117130
    },
    {
      "epoch": 6.247466666666667,
      "grad_norm": 4.373769346699419e-09,
      "learning_rate": 1.0953333333333334e-05,
      "loss": 0.0023,
      "step": 117140
    },
    {
      "epoch": 6.248,
      "grad_norm": 0.11213737726211548,
      "learning_rate": 1.095e-05,
      "loss": 0.0023,
      "step": 117150
    },
    {
      "epoch": 6.2485333333333335,
      "grad_norm": 0.33640575408935547,
      "learning_rate": 1.0946666666666668e-05,
      "loss": 0.0021,
      "step": 117160
    },
    {
      "epoch": 6.249066666666667,
      "grad_norm": 0.14016954600811005,
      "learning_rate": 1.0943333333333332e-05,
      "loss": 0.0026,
      "step": 117170
    },
    {
      "epoch": 6.2496,
      "grad_norm": 0.19623637199401855,
      "learning_rate": 1.094e-05,
      "loss": 0.0035,
      "step": 117180
    },
    {
      "epoch": 6.250133333333333,
      "grad_norm": 0.08409878611564636,
      "learning_rate": 1.0936666666666668e-05,
      "loss": 0.0022,
      "step": 117190
    },
    {
      "epoch": 6.250666666666667,
      "grad_norm": 0.16820605099201202,
      "learning_rate": 1.0933333333333334e-05,
      "loss": 0.0025,
      "step": 117200
    },
    {
      "epoch": 6.2512,
      "grad_norm": 0.11213483661413193,
      "learning_rate": 1.093e-05,
      "loss": 0.0026,
      "step": 117210
    },
    {
      "epoch": 6.251733333333333,
      "grad_norm": 0.05606702342629433,
      "learning_rate": 1.0926666666666667e-05,
      "loss": 0.0019,
      "step": 117220
    },
    {
      "epoch": 6.252266666666666,
      "grad_norm": 0.056067198514938354,
      "learning_rate": 1.0923333333333334e-05,
      "loss": 0.0025,
      "step": 117230
    },
    {
      "epoch": 6.2528,
      "grad_norm": 0.08410389721393585,
      "learning_rate": 1.092e-05,
      "loss": 0.0026,
      "step": 117240
    },
    {
      "epoch": 6.253333333333333,
      "grad_norm": 0.1401749849319458,
      "learning_rate": 1.0916666666666667e-05,
      "loss": 0.003,
      "step": 117250
    },
    {
      "epoch": 6.253866666666667,
      "grad_norm": 2.260194253977943e-09,
      "learning_rate": 1.0913333333333334e-05,
      "loss": 0.0017,
      "step": 117260
    },
    {
      "epoch": 6.2544,
      "grad_norm": 1.7017266484842253e-09,
      "learning_rate": 1.091e-05,
      "loss": 0.0027,
      "step": 117270
    },
    {
      "epoch": 6.254933333333334,
      "grad_norm": 1.969398200074579e-09,
      "learning_rate": 1.0906666666666668e-05,
      "loss": 0.0033,
      "step": 117280
    },
    {
      "epoch": 6.255466666666667,
      "grad_norm": 0.30838143825531006,
      "learning_rate": 1.0903333333333333e-05,
      "loss": 0.0018,
      "step": 117290
    },
    {
      "epoch": 6.256,
      "grad_norm": 0.19623546302318573,
      "learning_rate": 1.09e-05,
      "loss": 0.0031,
      "step": 117300
    },
    {
      "epoch": 6.2565333333333335,
      "grad_norm": 0.0841011106967926,
      "learning_rate": 1.0896666666666667e-05,
      "loss": 0.0029,
      "step": 117310
    },
    {
      "epoch": 6.257066666666667,
      "grad_norm": 9.47863121325554e-10,
      "learning_rate": 1.0893333333333333e-05,
      "loss": 0.003,
      "step": 117320
    },
    {
      "epoch": 6.2576,
      "grad_norm": 0.14016461372375488,
      "learning_rate": 1.089e-05,
      "loss": 0.0034,
      "step": 117330
    },
    {
      "epoch": 6.258133333333333,
      "grad_norm": 0.11213523149490356,
      "learning_rate": 1.0886666666666667e-05,
      "loss": 0.0019,
      "step": 117340
    },
    {
      "epoch": 6.258666666666667,
      "grad_norm": 0.16820241510868073,
      "learning_rate": 1.0883333333333335e-05,
      "loss": 0.0032,
      "step": 117350
    },
    {
      "epoch": 6.2592,
      "grad_norm": 0.11213363707065582,
      "learning_rate": 1.088e-05,
      "loss": 0.0026,
      "step": 117360
    },
    {
      "epoch": 6.259733333333333,
      "grad_norm": 0.14016561210155487,
      "learning_rate": 1.0876666666666667e-05,
      "loss": 0.0023,
      "step": 117370
    },
    {
      "epoch": 6.260266666666666,
      "grad_norm": 0.028033625334501266,
      "learning_rate": 1.0873333333333335e-05,
      "loss": 0.0017,
      "step": 117380
    },
    {
      "epoch": 6.2608,
      "grad_norm": 0.028033776208758354,
      "learning_rate": 1.0870000000000001e-05,
      "loss": 0.0022,
      "step": 117390
    },
    {
      "epoch": 6.261333333333333,
      "grad_norm": 0.2809292674064636,
      "learning_rate": 1.0866666666666667e-05,
      "loss": 0.0032,
      "step": 117400
    },
    {
      "epoch": 6.261866666666666,
      "grad_norm": 0.11213162541389465,
      "learning_rate": 1.0863333333333333e-05,
      "loss": 0.0022,
      "step": 117410
    },
    {
      "epoch": 6.2624,
      "grad_norm": 0.028033627197146416,
      "learning_rate": 1.0860000000000001e-05,
      "loss": 0.002,
      "step": 117420
    },
    {
      "epoch": 6.262933333333334,
      "grad_norm": 0.08410078287124634,
      "learning_rate": 1.0856666666666667e-05,
      "loss": 0.0022,
      "step": 117430
    },
    {
      "epoch": 6.263466666666667,
      "grad_norm": 2.7693376480897314e-09,
      "learning_rate": 1.0853333333333333e-05,
      "loss": 0.0026,
      "step": 117440
    },
    {
      "epoch": 6.264,
      "grad_norm": 0.2803356349468231,
      "learning_rate": 1.0850000000000001e-05,
      "loss": 0.002,
      "step": 117450
    },
    {
      "epoch": 6.2645333333333335,
      "grad_norm": 0.28034457564353943,
      "learning_rate": 1.0846666666666667e-05,
      "loss": 0.0028,
      "step": 117460
    },
    {
      "epoch": 6.265066666666667,
      "grad_norm": 2.0306381021129027e-09,
      "learning_rate": 1.0843333333333335e-05,
      "loss": 0.0023,
      "step": 117470
    },
    {
      "epoch": 6.2656,
      "grad_norm": 0.05606646463274956,
      "learning_rate": 1.084e-05,
      "loss": 0.0022,
      "step": 117480
    },
    {
      "epoch": 6.266133333333333,
      "grad_norm": 0.05606642737984657,
      "learning_rate": 1.0836666666666667e-05,
      "loss": 0.0032,
      "step": 117490
    },
    {
      "epoch": 6.266666666666667,
      "grad_norm": 0.5815305709838867,
      "learning_rate": 1.0833333333333334e-05,
      "loss": 0.0022,
      "step": 117500
    },
    {
      "epoch": 6.2672,
      "grad_norm": 0.05621565133333206,
      "learning_rate": 1.083e-05,
      "loss": 0.0021,
      "step": 117510
    },
    {
      "epoch": 6.267733333333333,
      "grad_norm": 0.08410132676362991,
      "learning_rate": 1.0826666666666667e-05,
      "loss": 0.0023,
      "step": 117520
    },
    {
      "epoch": 6.268266666666666,
      "grad_norm": 3.5202982751059153e-09,
      "learning_rate": 1.0823333333333334e-05,
      "loss": 0.0024,
      "step": 117530
    },
    {
      "epoch": 6.2688,
      "grad_norm": 1.704520344734192,
      "learning_rate": 1.0820000000000001e-05,
      "loss": 0.0016,
      "step": 117540
    },
    {
      "epoch": 6.269333333333333,
      "grad_norm": 0.2774341404438019,
      "learning_rate": 1.0816666666666666e-05,
      "loss": 0.0028,
      "step": 117550
    },
    {
      "epoch": 6.269866666666666,
      "grad_norm": 3.843915408197063e-09,
      "learning_rate": 1.0813333333333334e-05,
      "loss": 0.0043,
      "step": 117560
    },
    {
      "epoch": 6.2704,
      "grad_norm": 0.3364005982875824,
      "learning_rate": 1.081e-05,
      "loss": 0.0029,
      "step": 117570
    },
    {
      "epoch": 6.270933333333334,
      "grad_norm": 0.028033142909407616,
      "learning_rate": 1.0806666666666668e-05,
      "loss": 0.0022,
      "step": 117580
    },
    {
      "epoch": 6.271466666666667,
      "grad_norm": 0.2242712527513504,
      "learning_rate": 1.0803333333333334e-05,
      "loss": 0.0019,
      "step": 117590
    },
    {
      "epoch": 6.272,
      "grad_norm": 0.056069277226924896,
      "learning_rate": 1.08e-05,
      "loss": 0.0028,
      "step": 117600
    },
    {
      "epoch": 6.2725333333333335,
      "grad_norm": 0.1962364912033081,
      "learning_rate": 1.0796666666666668e-05,
      "loss": 0.0017,
      "step": 117610
    },
    {
      "epoch": 6.273066666666667,
      "grad_norm": 0.1962967813014984,
      "learning_rate": 1.0793333333333334e-05,
      "loss": 0.0015,
      "step": 117620
    },
    {
      "epoch": 6.2736,
      "grad_norm": 0.11216744035482407,
      "learning_rate": 1.079e-05,
      "loss": 0.0038,
      "step": 117630
    },
    {
      "epoch": 6.274133333333333,
      "grad_norm": 0.11213181167840958,
      "learning_rate": 1.0786666666666668e-05,
      "loss": 0.0039,
      "step": 117640
    },
    {
      "epoch": 6.274666666666667,
      "grad_norm": 0.11521739512681961,
      "learning_rate": 1.0783333333333334e-05,
      "loss": 0.0027,
      "step": 117650
    },
    {
      "epoch": 6.2752,
      "grad_norm": 0.11213486641645432,
      "learning_rate": 1.0780000000000002e-05,
      "loss": 0.0028,
      "step": 117660
    },
    {
      "epoch": 6.275733333333333,
      "grad_norm": 0.1681964248418808,
      "learning_rate": 1.0776666666666666e-05,
      "loss": 0.0024,
      "step": 117670
    },
    {
      "epoch": 6.276266666666666,
      "grad_norm": 0.33641934394836426,
      "learning_rate": 1.0773333333333334e-05,
      "loss": 0.003,
      "step": 117680
    },
    {
      "epoch": 6.2768,
      "grad_norm": 0.08410501480102539,
      "learning_rate": 1.077e-05,
      "loss": 0.0021,
      "step": 117690
    },
    {
      "epoch": 6.277333333333333,
      "grad_norm": 0.11213359981775284,
      "learning_rate": 1.0766666666666666e-05,
      "loss": 0.0042,
      "step": 117700
    },
    {
      "epoch": 6.277866666666666,
      "grad_norm": 0.1681983917951584,
      "learning_rate": 1.0763333333333334e-05,
      "loss": 0.003,
      "step": 117710
    },
    {
      "epoch": 6.2783999999999995,
      "grad_norm": 0.13057877123355865,
      "learning_rate": 1.076e-05,
      "loss": 0.0028,
      "step": 117720
    },
    {
      "epoch": 6.278933333333334,
      "grad_norm": 0.11213470995426178,
      "learning_rate": 1.0756666666666668e-05,
      "loss": 0.003,
      "step": 117730
    },
    {
      "epoch": 6.279466666666667,
      "grad_norm": 0.14016583561897278,
      "learning_rate": 1.0753333333333333e-05,
      "loss": 0.0024,
      "step": 117740
    },
    {
      "epoch": 6.28,
      "grad_norm": 0.11213479191064835,
      "learning_rate": 1.075e-05,
      "loss": 0.0031,
      "step": 117750
    },
    {
      "epoch": 6.2805333333333335,
      "grad_norm": 0.16820110380649567,
      "learning_rate": 1.0746666666666667e-05,
      "loss": 0.0021,
      "step": 117760
    },
    {
      "epoch": 6.281066666666667,
      "grad_norm": 0.16820365190505981,
      "learning_rate": 1.0743333333333334e-05,
      "loss": 0.0024,
      "step": 117770
    },
    {
      "epoch": 6.2816,
      "grad_norm": 0.4205074608325958,
      "learning_rate": 1.074e-05,
      "loss": 0.003,
      "step": 117780
    },
    {
      "epoch": 6.282133333333333,
      "grad_norm": 0.1682026982307434,
      "learning_rate": 1.0736666666666667e-05,
      "loss": 0.0032,
      "step": 117790
    },
    {
      "epoch": 6.282666666666667,
      "grad_norm": 0.22427047789096832,
      "learning_rate": 1.0733333333333334e-05,
      "loss": 0.0025,
      "step": 117800
    },
    {
      "epoch": 6.2832,
      "grad_norm": 0.14016735553741455,
      "learning_rate": 1.073e-05,
      "loss": 0.0022,
      "step": 117810
    },
    {
      "epoch": 6.283733333333333,
      "grad_norm": 0.4485231339931488,
      "learning_rate": 1.0726666666666667e-05,
      "loss": 0.0034,
      "step": 117820
    },
    {
      "epoch": 6.2842666666666664,
      "grad_norm": 0.08409839123487473,
      "learning_rate": 1.0723333333333335e-05,
      "loss": 0.0032,
      "step": 117830
    },
    {
      "epoch": 6.2848,
      "grad_norm": 0.16820110380649567,
      "learning_rate": 1.072e-05,
      "loss": 0.0032,
      "step": 117840
    },
    {
      "epoch": 6.285333333333333,
      "grad_norm": 0.2803318202495575,
      "learning_rate": 1.0716666666666667e-05,
      "loss": 0.0022,
      "step": 117850
    },
    {
      "epoch": 6.285866666666666,
      "grad_norm": 0.05606584995985031,
      "learning_rate": 1.0713333333333333e-05,
      "loss": 0.0017,
      "step": 117860
    },
    {
      "epoch": 6.2864,
      "grad_norm": 0.056067150086164474,
      "learning_rate": 1.071e-05,
      "loss": 0.0027,
      "step": 117870
    },
    {
      "epoch": 6.286933333333334,
      "grad_norm": 0.02803417481482029,
      "learning_rate": 1.0706666666666667e-05,
      "loss": 0.0034,
      "step": 117880
    },
    {
      "epoch": 6.287466666666667,
      "grad_norm": 0.2522963583469391,
      "learning_rate": 1.0703333333333333e-05,
      "loss": 0.0027,
      "step": 117890
    },
    {
      "epoch": 6.288,
      "grad_norm": 0.1088547483086586,
      "learning_rate": 1.0700000000000001e-05,
      "loss": 0.0025,
      "step": 117900
    },
    {
      "epoch": 6.2885333333333335,
      "grad_norm": 1.8725960782006723e-09,
      "learning_rate": 1.0696666666666667e-05,
      "loss": 0.003,
      "step": 117910
    },
    {
      "epoch": 6.289066666666667,
      "grad_norm": 0.056067369878292084,
      "learning_rate": 1.0693333333333335e-05,
      "loss": 0.0031,
      "step": 117920
    },
    {
      "epoch": 6.2896,
      "grad_norm": 0.028033170849084854,
      "learning_rate": 1.069e-05,
      "loss": 0.0017,
      "step": 117930
    },
    {
      "epoch": 6.290133333333333,
      "grad_norm": 0.1401679664850235,
      "learning_rate": 1.0686666666666667e-05,
      "loss": 0.0043,
      "step": 117940
    },
    {
      "epoch": 6.290666666666667,
      "grad_norm": 0.2523016035556793,
      "learning_rate": 1.0683333333333333e-05,
      "loss": 0.0032,
      "step": 117950
    },
    {
      "epoch": 6.2912,
      "grad_norm": 0.19625891745090485,
      "learning_rate": 1.0680000000000001e-05,
      "loss": 0.0031,
      "step": 117960
    },
    {
      "epoch": 6.291733333333333,
      "grad_norm": 0.1681993305683136,
      "learning_rate": 1.0676666666666667e-05,
      "loss": 0.0029,
      "step": 117970
    },
    {
      "epoch": 6.2922666666666665,
      "grad_norm": 0.11213580518960953,
      "learning_rate": 1.0673333333333333e-05,
      "loss": 0.0021,
      "step": 117980
    },
    {
      "epoch": 6.2928,
      "grad_norm": 0.3989551365375519,
      "learning_rate": 1.0670000000000001e-05,
      "loss": 0.0025,
      "step": 117990
    },
    {
      "epoch": 6.293333333333333,
      "grad_norm": 7.66601893076313e-09,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 0.003,
      "step": 118000
    },
    {
      "epoch": 6.293866666666666,
      "grad_norm": 0.19623221457004547,
      "learning_rate": 1.0663333333333333e-05,
      "loss": 0.0018,
      "step": 118010
    },
    {
      "epoch": 6.2943999999999996,
      "grad_norm": 0.16820642352104187,
      "learning_rate": 1.0660000000000001e-05,
      "loss": 0.0015,
      "step": 118020
    },
    {
      "epoch": 6.294933333333334,
      "grad_norm": 0.3364217281341553,
      "learning_rate": 1.0656666666666667e-05,
      "loss": 0.003,
      "step": 118030
    },
    {
      "epoch": 6.295466666666667,
      "grad_norm": 0.19623632729053497,
      "learning_rate": 1.0653333333333334e-05,
      "loss": 0.0015,
      "step": 118040
    },
    {
      "epoch": 6.296,
      "grad_norm": 2.145529975905447e-09,
      "learning_rate": 1.065e-05,
      "loss": 0.0015,
      "step": 118050
    },
    {
      "epoch": 6.2965333333333335,
      "grad_norm": 0.02803289145231247,
      "learning_rate": 1.0646666666666668e-05,
      "loss": 0.0022,
      "step": 118060
    },
    {
      "epoch": 6.297066666666667,
      "grad_norm": 0.22426657378673553,
      "learning_rate": 1.0643333333333334e-05,
      "loss": 0.0022,
      "step": 118070
    },
    {
      "epoch": 6.2976,
      "grad_norm": 4.362488148501598e-09,
      "learning_rate": 1.064e-05,
      "loss": 0.0022,
      "step": 118080
    },
    {
      "epoch": 6.298133333333333,
      "grad_norm": 2.4978299428823902e-09,
      "learning_rate": 1.0636666666666668e-05,
      "loss": 0.0033,
      "step": 118090
    },
    {
      "epoch": 6.298666666666667,
      "grad_norm": 0.1401674598455429,
      "learning_rate": 1.0633333333333334e-05,
      "loss": 0.0029,
      "step": 118100
    },
    {
      "epoch": 6.2992,
      "grad_norm": 1.8277636071317716e-09,
      "learning_rate": 1.0630000000000002e-05,
      "loss": 0.0027,
      "step": 118110
    },
    {
      "epoch": 6.299733333333333,
      "grad_norm": 0.1121295914053917,
      "learning_rate": 1.0626666666666666e-05,
      "loss": 0.0027,
      "step": 118120
    },
    {
      "epoch": 6.3002666666666665,
      "grad_norm": 0.05606609955430031,
      "learning_rate": 1.0623333333333334e-05,
      "loss": 0.0021,
      "step": 118130
    },
    {
      "epoch": 6.3008,
      "grad_norm": 0.028033005073666573,
      "learning_rate": 1.062e-05,
      "loss": 0.0043,
      "step": 118140
    },
    {
      "epoch": 6.301333333333333,
      "grad_norm": 0.14016340672969818,
      "learning_rate": 1.0616666666666668e-05,
      "loss": 0.0036,
      "step": 118150
    },
    {
      "epoch": 6.301866666666666,
      "grad_norm": 0.11212866008281708,
      "learning_rate": 1.0613333333333334e-05,
      "loss": 0.0023,
      "step": 118160
    },
    {
      "epoch": 6.3024000000000004,
      "grad_norm": 0.19624119997024536,
      "learning_rate": 1.061e-05,
      "loss": 0.0017,
      "step": 118170
    },
    {
      "epoch": 6.302933333333334,
      "grad_norm": 0.44856688380241394,
      "learning_rate": 1.0606666666666668e-05,
      "loss": 0.0034,
      "step": 118180
    },
    {
      "epoch": 6.303466666666667,
      "grad_norm": 0.14015884697437286,
      "learning_rate": 1.0603333333333332e-05,
      "loss": 0.0019,
      "step": 118190
    },
    {
      "epoch": 6.304,
      "grad_norm": 0.22426612675189972,
      "learning_rate": 1.06e-05,
      "loss": 0.0028,
      "step": 118200
    },
    {
      "epoch": 6.3045333333333335,
      "grad_norm": 0.16820333898067474,
      "learning_rate": 1.0596666666666668e-05,
      "loss": 0.0027,
      "step": 118210
    },
    {
      "epoch": 6.305066666666667,
      "grad_norm": 0.16819147765636444,
      "learning_rate": 1.0593333333333334e-05,
      "loss": 0.0023,
      "step": 118220
    },
    {
      "epoch": 6.3056,
      "grad_norm": 0.02803301252424717,
      "learning_rate": 1.059e-05,
      "loss": 0.0035,
      "step": 118230
    },
    {
      "epoch": 6.306133333333333,
      "grad_norm": 0.2242656946182251,
      "learning_rate": 1.0586666666666666e-05,
      "loss": 0.0028,
      "step": 118240
    },
    {
      "epoch": 6.306666666666667,
      "grad_norm": 0.1401616334915161,
      "learning_rate": 1.0583333333333334e-05,
      "loss": 0.0022,
      "step": 118250
    },
    {
      "epoch": 6.3072,
      "grad_norm": 0.1962244063615799,
      "learning_rate": 1.058e-05,
      "loss": 0.0024,
      "step": 118260
    },
    {
      "epoch": 6.307733333333333,
      "grad_norm": 0.1291256695985794,
      "learning_rate": 1.0576666666666666e-05,
      "loss": 0.0026,
      "step": 118270
    },
    {
      "epoch": 6.3082666666666665,
      "grad_norm": 4.312995738331438e-09,
      "learning_rate": 1.0573333333333334e-05,
      "loss": 0.0036,
      "step": 118280
    },
    {
      "epoch": 6.3088,
      "grad_norm": 0.05606682598590851,
      "learning_rate": 1.057e-05,
      "loss": 0.0032,
      "step": 118290
    },
    {
      "epoch": 6.309333333333333,
      "grad_norm": 0.14016380906105042,
      "learning_rate": 1.0566666666666668e-05,
      "loss": 0.0019,
      "step": 118300
    },
    {
      "epoch": 6.309866666666666,
      "grad_norm": 0.11212537437677383,
      "learning_rate": 1.0563333333333333e-05,
      "loss": 0.002,
      "step": 118310
    },
    {
      "epoch": 6.3104,
      "grad_norm": 0.056064847856760025,
      "learning_rate": 1.056e-05,
      "loss": 0.0024,
      "step": 118320
    },
    {
      "epoch": 6.310933333333334,
      "grad_norm": 0.028032902628183365,
      "learning_rate": 1.0556666666666667e-05,
      "loss": 0.0017,
      "step": 118330
    },
    {
      "epoch": 6.311466666666667,
      "grad_norm": 0.19622787833213806,
      "learning_rate": 1.0553333333333335e-05,
      "loss": 0.0027,
      "step": 118340
    },
    {
      "epoch": 6.312,
      "grad_norm": 1.2449175024187298e-09,
      "learning_rate": 1.055e-05,
      "loss": 0.0042,
      "step": 118350
    },
    {
      "epoch": 6.3125333333333336,
      "grad_norm": 0.08409573882818222,
      "learning_rate": 1.0546666666666667e-05,
      "loss": 0.0037,
      "step": 118360
    },
    {
      "epoch": 6.313066666666667,
      "grad_norm": 1.1051487502911073e-09,
      "learning_rate": 1.0543333333333335e-05,
      "loss": 0.0022,
      "step": 118370
    },
    {
      "epoch": 6.3136,
      "grad_norm": 0.3363759219646454,
      "learning_rate": 1.0539999999999999e-05,
      "loss": 0.0028,
      "step": 118380
    },
    {
      "epoch": 6.314133333333333,
      "grad_norm": 0.08409695327281952,
      "learning_rate": 1.0536666666666667e-05,
      "loss": 0.0023,
      "step": 118390
    },
    {
      "epoch": 6.314666666666667,
      "grad_norm": 2.0162351788144406e-09,
      "learning_rate": 1.0533333333333335e-05,
      "loss": 0.0021,
      "step": 118400
    },
    {
      "epoch": 6.3152,
      "grad_norm": 0.16819371283054352,
      "learning_rate": 1.053e-05,
      "loss": 0.0029,
      "step": 118410
    },
    {
      "epoch": 6.315733333333333,
      "grad_norm": 0.2803209125995636,
      "learning_rate": 1.0526666666666667e-05,
      "loss": 0.0027,
      "step": 118420
    },
    {
      "epoch": 6.3162666666666665,
      "grad_norm": 0.3083606958389282,
      "learning_rate": 1.0523333333333333e-05,
      "loss": 0.002,
      "step": 118430
    },
    {
      "epoch": 6.3168,
      "grad_norm": 0.028033312410116196,
      "learning_rate": 1.0520000000000001e-05,
      "loss": 0.0022,
      "step": 118440
    },
    {
      "epoch": 6.317333333333333,
      "grad_norm": 0.33639800548553467,
      "learning_rate": 1.0516666666666667e-05,
      "loss": 0.0025,
      "step": 118450
    },
    {
      "epoch": 6.317866666666666,
      "grad_norm": 0.08409324288368225,
      "learning_rate": 1.0513333333333333e-05,
      "loss": 0.0017,
      "step": 118460
    },
    {
      "epoch": 6.3184000000000005,
      "grad_norm": 0.14015665650367737,
      "learning_rate": 1.0510000000000001e-05,
      "loss": 0.0031,
      "step": 118470
    },
    {
      "epoch": 6.318933333333334,
      "grad_norm": 0.1962205320596695,
      "learning_rate": 1.0506666666666667e-05,
      "loss": 0.0032,
      "step": 118480
    },
    {
      "epoch": 6.319466666666667,
      "grad_norm": 0.08409635722637177,
      "learning_rate": 1.0503333333333335e-05,
      "loss": 0.0022,
      "step": 118490
    },
    {
      "epoch": 6.32,
      "grad_norm": 0.02803235873579979,
      "learning_rate": 1.05e-05,
      "loss": 0.0041,
      "step": 118500
    },
    {
      "epoch": 6.320533333333334,
      "grad_norm": 0.5239512324333191,
      "learning_rate": 1.0496666666666667e-05,
      "loss": 0.0018,
      "step": 118510
    },
    {
      "epoch": 6.321066666666667,
      "grad_norm": 0.05625343322753906,
      "learning_rate": 1.0493333333333333e-05,
      "loss": 0.002,
      "step": 118520
    },
    {
      "epoch": 6.3216,
      "grad_norm": 0.19621926546096802,
      "learning_rate": 1.049e-05,
      "loss": 0.0019,
      "step": 118530
    },
    {
      "epoch": 6.322133333333333,
      "grad_norm": 0.14016179740428925,
      "learning_rate": 1.0486666666666667e-05,
      "loss": 0.0025,
      "step": 118540
    },
    {
      "epoch": 6.322666666666667,
      "grad_norm": 0.14016093313694,
      "learning_rate": 1.0483333333333333e-05,
      "loss": 0.0023,
      "step": 118550
    },
    {
      "epoch": 6.3232,
      "grad_norm": 0.028031470254063606,
      "learning_rate": 1.0480000000000001e-05,
      "loss": 0.0032,
      "step": 118560
    },
    {
      "epoch": 6.323733333333333,
      "grad_norm": 0.05606269836425781,
      "learning_rate": 1.0476666666666666e-05,
      "loss": 0.0026,
      "step": 118570
    },
    {
      "epoch": 6.3242666666666665,
      "grad_norm": 0.02803257293999195,
      "learning_rate": 1.0473333333333334e-05,
      "loss": 0.0034,
      "step": 118580
    },
    {
      "epoch": 6.3248,
      "grad_norm": 0.08409629017114639,
      "learning_rate": 1.0470000000000001e-05,
      "loss": 0.0033,
      "step": 118590
    },
    {
      "epoch": 6.325333333333333,
      "grad_norm": 0.14015567302703857,
      "learning_rate": 1.0466666666666668e-05,
      "loss": 0.0036,
      "step": 118600
    },
    {
      "epoch": 6.325866666666666,
      "grad_norm": 0.1681879758834839,
      "learning_rate": 1.0463333333333334e-05,
      "loss": 0.0042,
      "step": 118610
    },
    {
      "epoch": 6.3264,
      "grad_norm": 0.05606263875961304,
      "learning_rate": 1.046e-05,
      "loss": 0.0021,
      "step": 118620
    },
    {
      "epoch": 6.326933333333334,
      "grad_norm": 0.05606178194284439,
      "learning_rate": 1.0456666666666668e-05,
      "loss": 0.0038,
      "step": 118630
    },
    {
      "epoch": 6.327466666666667,
      "grad_norm": 0.2522851526737213,
      "learning_rate": 1.0453333333333334e-05,
      "loss": 0.0018,
      "step": 118640
    },
    {
      "epoch": 6.328,
      "grad_norm": 0.19674152135849,
      "learning_rate": 1.045e-05,
      "loss": 0.0036,
      "step": 118650
    },
    {
      "epoch": 6.328533333333334,
      "grad_norm": 0.2522847652435303,
      "learning_rate": 1.0446666666666668e-05,
      "loss": 0.0022,
      "step": 118660
    },
    {
      "epoch": 6.329066666666667,
      "grad_norm": 0.28030794858932495,
      "learning_rate": 1.0443333333333334e-05,
      "loss": 0.0042,
      "step": 118670
    },
    {
      "epoch": 6.3296,
      "grad_norm": 0.22425158321857452,
      "learning_rate": 1.0440000000000002e-05,
      "loss": 0.0026,
      "step": 118680
    },
    {
      "epoch": 6.330133333333333,
      "grad_norm": 0.028031690046191216,
      "learning_rate": 1.0436666666666666e-05,
      "loss": 0.0037,
      "step": 118690
    },
    {
      "epoch": 6.330666666666667,
      "grad_norm": 0.028031332418322563,
      "learning_rate": 1.0433333333333334e-05,
      "loss": 0.0039,
      "step": 118700
    },
    {
      "epoch": 6.3312,
      "grad_norm": 0.05606213957071304,
      "learning_rate": 1.043e-05,
      "loss": 0.0023,
      "step": 118710
    },
    {
      "epoch": 6.331733333333333,
      "grad_norm": 0.14015553891658783,
      "learning_rate": 1.0426666666666666e-05,
      "loss": 0.0016,
      "step": 118720
    },
    {
      "epoch": 6.3322666666666665,
      "grad_norm": 0.19621612131595612,
      "learning_rate": 1.0423333333333334e-05,
      "loss": 0.002,
      "step": 118730
    },
    {
      "epoch": 6.3328,
      "grad_norm": 0.21369199454784393,
      "learning_rate": 1.042e-05,
      "loss": 0.0017,
      "step": 118740
    },
    {
      "epoch": 6.333333333333333,
      "grad_norm": 1.1203172206878662,
      "learning_rate": 1.0416666666666668e-05,
      "loss": 0.0025,
      "step": 118750
    },
    {
      "epoch": 6.333866666666666,
      "grad_norm": 0.11213158816099167,
      "learning_rate": 1.0413333333333332e-05,
      "loss": 0.0028,
      "step": 118760
    },
    {
      "epoch": 6.3344,
      "grad_norm": 0.25275567173957825,
      "learning_rate": 1.041e-05,
      "loss": 0.0035,
      "step": 118770
    },
    {
      "epoch": 6.334933333333334,
      "grad_norm": 0.1121247336268425,
      "learning_rate": 1.0406666666666668e-05,
      "loss": 0.0028,
      "step": 118780
    },
    {
      "epoch": 6.335466666666667,
      "grad_norm": 0.28031614422798157,
      "learning_rate": 1.0403333333333334e-05,
      "loss": 0.0025,
      "step": 118790
    },
    {
      "epoch": 6.336,
      "grad_norm": 0.28031882643699646,
      "learning_rate": 1.04e-05,
      "loss": 0.0025,
      "step": 118800
    },
    {
      "epoch": 6.336533333333334,
      "grad_norm": 0.16819369792938232,
      "learning_rate": 1.0396666666666667e-05,
      "loss": 0.002,
      "step": 118810
    },
    {
      "epoch": 6.337066666666667,
      "grad_norm": 0.11212924867868423,
      "learning_rate": 1.0393333333333334e-05,
      "loss": 0.0017,
      "step": 118820
    },
    {
      "epoch": 6.3376,
      "grad_norm": 0.05606222525238991,
      "learning_rate": 1.039e-05,
      "loss": 0.0036,
      "step": 118830
    },
    {
      "epoch": 6.338133333333333,
      "grad_norm": 0.11212193965911865,
      "learning_rate": 1.0386666666666667e-05,
      "loss": 0.0031,
      "step": 118840
    },
    {
      "epoch": 6.338666666666667,
      "grad_norm": 0.1121266558766365,
      "learning_rate": 1.0383333333333334e-05,
      "loss": 0.0027,
      "step": 118850
    },
    {
      "epoch": 6.3392,
      "grad_norm": 0.20387132465839386,
      "learning_rate": 1.038e-05,
      "loss": 0.0033,
      "step": 118860
    },
    {
      "epoch": 6.339733333333333,
      "grad_norm": 0.4765412509441376,
      "learning_rate": 1.0376666666666667e-05,
      "loss": 0.0024,
      "step": 118870
    },
    {
      "epoch": 6.3402666666666665,
      "grad_norm": 0.1681816577911377,
      "learning_rate": 1.0373333333333333e-05,
      "loss": 0.0023,
      "step": 118880
    },
    {
      "epoch": 6.3408,
      "grad_norm": 0.028030885383486748,
      "learning_rate": 1.037e-05,
      "loss": 0.0024,
      "step": 118890
    },
    {
      "epoch": 6.341333333333333,
      "grad_norm": 0.14015506207942963,
      "learning_rate": 1.0366666666666667e-05,
      "loss": 0.0027,
      "step": 118900
    },
    {
      "epoch": 6.341866666666666,
      "grad_norm": 3.1584457271804922e-09,
      "learning_rate": 1.0363333333333333e-05,
      "loss": 0.0031,
      "step": 118910
    },
    {
      "epoch": 6.3424,
      "grad_norm": 0.056060705333948135,
      "learning_rate": 1.036e-05,
      "loss": 0.0026,
      "step": 118920
    },
    {
      "epoch": 6.342933333333333,
      "grad_norm": 0.11212389171123505,
      "learning_rate": 1.0356666666666667e-05,
      "loss": 0.0032,
      "step": 118930
    },
    {
      "epoch": 6.343466666666667,
      "grad_norm": 0.25228163599967957,
      "learning_rate": 1.0353333333333335e-05,
      "loss": 0.0023,
      "step": 118940
    },
    {
      "epoch": 6.344,
      "grad_norm": 0.2242511361837387,
      "learning_rate": 1.035e-05,
      "loss": 0.0038,
      "step": 118950
    },
    {
      "epoch": 6.344533333333334,
      "grad_norm": 1.843751595842491e-09,
      "learning_rate": 1.0346666666666667e-05,
      "loss": 0.0019,
      "step": 118960
    },
    {
      "epoch": 6.345066666666667,
      "grad_norm": 0.05606301128864288,
      "learning_rate": 1.0343333333333335e-05,
      "loss": 0.0023,
      "step": 118970
    },
    {
      "epoch": 6.3456,
      "grad_norm": 0.02803187631070614,
      "learning_rate": 1.0340000000000001e-05,
      "loss": 0.0027,
      "step": 118980
    },
    {
      "epoch": 6.346133333333333,
      "grad_norm": 0.05606423318386078,
      "learning_rate": 1.0336666666666667e-05,
      "loss": 0.0029,
      "step": 118990
    },
    {
      "epoch": 6.346666666666667,
      "grad_norm": 0.028030090034008026,
      "learning_rate": 1.0333333333333333e-05,
      "loss": 0.0037,
      "step": 119000
    },
    {
      "epoch": 6.3472,
      "grad_norm": 0.19621549546718597,
      "learning_rate": 1.0330000000000001e-05,
      "loss": 0.0034,
      "step": 119010
    },
    {
      "epoch": 6.347733333333333,
      "grad_norm": 0.2803098261356354,
      "learning_rate": 1.0326666666666667e-05,
      "loss": 0.0017,
      "step": 119020
    },
    {
      "epoch": 6.3482666666666665,
      "grad_norm": 0.1962205320596695,
      "learning_rate": 1.0323333333333333e-05,
      "loss": 0.003,
      "step": 119030
    },
    {
      "epoch": 6.3488,
      "grad_norm": 0.1401548981666565,
      "learning_rate": 1.0320000000000001e-05,
      "loss": 0.0028,
      "step": 119040
    },
    {
      "epoch": 6.349333333333333,
      "grad_norm": 0.22424860298633575,
      "learning_rate": 1.0316666666666667e-05,
      "loss": 0.0021,
      "step": 119050
    },
    {
      "epoch": 6.349866666666666,
      "grad_norm": 0.25227975845336914,
      "learning_rate": 1.0313333333333333e-05,
      "loss": 0.0023,
      "step": 119060
    },
    {
      "epoch": 6.3504,
      "grad_norm": 0.5045623779296875,
      "learning_rate": 1.031e-05,
      "loss": 0.0025,
      "step": 119070
    },
    {
      "epoch": 6.350933333333334,
      "grad_norm": 0.16818535327911377,
      "learning_rate": 1.0306666666666667e-05,
      "loss": 0.0024,
      "step": 119080
    },
    {
      "epoch": 6.351466666666667,
      "grad_norm": 0.168181911110878,
      "learning_rate": 1.0303333333333334e-05,
      "loss": 0.0023,
      "step": 119090
    },
    {
      "epoch": 6.352,
      "grad_norm": 0.22424522042274475,
      "learning_rate": 1.03e-05,
      "loss": 0.0029,
      "step": 119100
    },
    {
      "epoch": 6.352533333333334,
      "grad_norm": 0.11212615668773651,
      "learning_rate": 1.0296666666666667e-05,
      "loss": 0.0033,
      "step": 119110
    },
    {
      "epoch": 6.353066666666667,
      "grad_norm": 0.02803247980773449,
      "learning_rate": 1.0293333333333334e-05,
      "loss": 0.0019,
      "step": 119120
    },
    {
      "epoch": 6.3536,
      "grad_norm": 0.0560637041926384,
      "learning_rate": 1.0290000000000001e-05,
      "loss": 0.003,
      "step": 119130
    },
    {
      "epoch": 6.354133333333333,
      "grad_norm": 0.028031591325998306,
      "learning_rate": 1.0286666666666666e-05,
      "loss": 0.0027,
      "step": 119140
    },
    {
      "epoch": 6.354666666666667,
      "grad_norm": 0.032883577048778534,
      "learning_rate": 1.0283333333333334e-05,
      "loss": 0.0032,
      "step": 119150
    },
    {
      "epoch": 6.3552,
      "grad_norm": 0.19621452689170837,
      "learning_rate": 1.0280000000000002e-05,
      "loss": 0.0024,
      "step": 119160
    },
    {
      "epoch": 6.355733333333333,
      "grad_norm": 0.07589960098266602,
      "learning_rate": 1.0276666666666668e-05,
      "loss": 0.0023,
      "step": 119170
    },
    {
      "epoch": 6.3562666666666665,
      "grad_norm": 0.1401531994342804,
      "learning_rate": 1.0273333333333334e-05,
      "loss": 0.0028,
      "step": 119180
    },
    {
      "epoch": 6.3568,
      "grad_norm": 0.1121225357055664,
      "learning_rate": 1.027e-05,
      "loss": 0.0023,
      "step": 119190
    },
    {
      "epoch": 6.357333333333333,
      "grad_norm": 0.6166825890541077,
      "learning_rate": 1.0266666666666668e-05,
      "loss": 0.0022,
      "step": 119200
    },
    {
      "epoch": 6.357866666666666,
      "grad_norm": 0.3083299398422241,
      "learning_rate": 1.0263333333333334e-05,
      "loss": 0.0028,
      "step": 119210
    },
    {
      "epoch": 6.3584,
      "grad_norm": 0.028031224384903908,
      "learning_rate": 1.026e-05,
      "loss": 0.0028,
      "step": 119220
    },
    {
      "epoch": 6.358933333333333,
      "grad_norm": 4.047209678503805e-09,
      "learning_rate": 1.0256666666666668e-05,
      "loss": 0.0018,
      "step": 119230
    },
    {
      "epoch": 6.359466666666667,
      "grad_norm": 0.11212647706270218,
      "learning_rate": 1.0253333333333334e-05,
      "loss": 0.0034,
      "step": 119240
    },
    {
      "epoch": 6.36,
      "grad_norm": 0.392423540353775,
      "learning_rate": 1.025e-05,
      "loss": 0.0021,
      "step": 119250
    },
    {
      "epoch": 6.360533333333334,
      "grad_norm": 0.0280299074947834,
      "learning_rate": 1.0246666666666666e-05,
      "loss": 0.0015,
      "step": 119260
    },
    {
      "epoch": 6.361066666666667,
      "grad_norm": 0.28030189871788025,
      "learning_rate": 1.0243333333333334e-05,
      "loss": 0.0025,
      "step": 119270
    },
    {
      "epoch": 6.3616,
      "grad_norm": 0.0840885192155838,
      "learning_rate": 1.024e-05,
      "loss": 0.0019,
      "step": 119280
    },
    {
      "epoch": 6.362133333333333,
      "grad_norm": 0.11211834102869034,
      "learning_rate": 1.0236666666666666e-05,
      "loss": 0.0023,
      "step": 119290
    },
    {
      "epoch": 6.362666666666667,
      "grad_norm": 0.11212080717086792,
      "learning_rate": 1.0233333333333334e-05,
      "loss": 0.0038,
      "step": 119300
    },
    {
      "epoch": 6.3632,
      "grad_norm": 0.08409375697374344,
      "learning_rate": 1.023e-05,
      "loss": 0.0019,
      "step": 119310
    },
    {
      "epoch": 6.363733333333333,
      "grad_norm": 0.16818025708198547,
      "learning_rate": 1.0226666666666668e-05,
      "loss": 0.0018,
      "step": 119320
    },
    {
      "epoch": 6.3642666666666665,
      "grad_norm": 0.02802976965904236,
      "learning_rate": 1.0223333333333333e-05,
      "loss": 0.0032,
      "step": 119330
    },
    {
      "epoch": 6.3648,
      "grad_norm": 0.3083361089229584,
      "learning_rate": 1.022e-05,
      "loss": 0.0036,
      "step": 119340
    },
    {
      "epoch": 6.365333333333333,
      "grad_norm": 0.08409322798252106,
      "learning_rate": 1.0216666666666668e-05,
      "loss": 0.0032,
      "step": 119350
    },
    {
      "epoch": 6.365866666666666,
      "grad_norm": 0.08409343659877777,
      "learning_rate": 1.0213333333333334e-05,
      "loss": 0.0033,
      "step": 119360
    },
    {
      "epoch": 6.3664,
      "grad_norm": 3.357341737952879e-09,
      "learning_rate": 1.021e-05,
      "loss": 0.0022,
      "step": 119370
    },
    {
      "epoch": 6.366933333333334,
      "grad_norm": 0.02803105115890503,
      "learning_rate": 1.0206666666666667e-05,
      "loss": 0.0034,
      "step": 119380
    },
    {
      "epoch": 6.367466666666667,
      "grad_norm": 0.08409283310174942,
      "learning_rate": 1.0203333333333334e-05,
      "loss": 0.0025,
      "step": 119390
    },
    {
      "epoch": 6.368,
      "grad_norm": 0.08409103751182556,
      "learning_rate": 1.02e-05,
      "loss": 0.0025,
      "step": 119400
    },
    {
      "epoch": 6.368533333333334,
      "grad_norm": 0.420451283454895,
      "learning_rate": 1.0196666666666667e-05,
      "loss": 0.002,
      "step": 119410
    },
    {
      "epoch": 6.369066666666667,
      "grad_norm": 0.112123504281044,
      "learning_rate": 1.0193333333333335e-05,
      "loss": 0.0018,
      "step": 119420
    },
    {
      "epoch": 6.3696,
      "grad_norm": 0.25228214263916016,
      "learning_rate": 1.019e-05,
      "loss": 0.0031,
      "step": 119430
    },
    {
      "epoch": 6.370133333333333,
      "grad_norm": 3.2452744935795863e-09,
      "learning_rate": 1.0186666666666667e-05,
      "loss": 0.0023,
      "step": 119440
    },
    {
      "epoch": 6.370666666666667,
      "grad_norm": 0.5326054692268372,
      "learning_rate": 1.0183333333333333e-05,
      "loss": 0.0019,
      "step": 119450
    },
    {
      "epoch": 6.3712,
      "grad_norm": 0.11212276667356491,
      "learning_rate": 1.018e-05,
      "loss": 0.0021,
      "step": 119460
    },
    {
      "epoch": 6.371733333333333,
      "grad_norm": 0.1962115615606308,
      "learning_rate": 1.0176666666666667e-05,
      "loss": 0.0022,
      "step": 119470
    },
    {
      "epoch": 6.3722666666666665,
      "grad_norm": 0.05606142804026604,
      "learning_rate": 1.0173333333333333e-05,
      "loss": 0.0017,
      "step": 119480
    },
    {
      "epoch": 6.3728,
      "grad_norm": 0.1962144523859024,
      "learning_rate": 1.0170000000000001e-05,
      "loss": 0.0025,
      "step": 119490
    },
    {
      "epoch": 6.373333333333333,
      "grad_norm": 0.22424766421318054,
      "learning_rate": 1.0166666666666667e-05,
      "loss": 0.0027,
      "step": 119500
    },
    {
      "epoch": 6.373866666666666,
      "grad_norm": 0.3363531827926636,
      "learning_rate": 1.0163333333333335e-05,
      "loss": 0.0025,
      "step": 119510
    },
    {
      "epoch": 6.3744,
      "grad_norm": 0.1962132304906845,
      "learning_rate": 1.016e-05,
      "loss": 0.0038,
      "step": 119520
    },
    {
      "epoch": 6.374933333333333,
      "grad_norm": 0.19622334837913513,
      "learning_rate": 1.0156666666666667e-05,
      "loss": 0.003,
      "step": 119530
    },
    {
      "epoch": 6.375466666666667,
      "grad_norm": 0.02803240716457367,
      "learning_rate": 1.0153333333333335e-05,
      "loss": 0.0035,
      "step": 119540
    },
    {
      "epoch": 6.376,
      "grad_norm": 2.2487244606018066,
      "learning_rate": 1.0150000000000001e-05,
      "loss": 0.0034,
      "step": 119550
    },
    {
      "epoch": 6.376533333333334,
      "grad_norm": 0.30832356214523315,
      "learning_rate": 1.0146666666666667e-05,
      "loss": 0.0031,
      "step": 119560
    },
    {
      "epoch": 6.377066666666667,
      "grad_norm": 0.36439526081085205,
      "learning_rate": 1.0143333333333333e-05,
      "loss": 0.0024,
      "step": 119570
    },
    {
      "epoch": 6.3776,
      "grad_norm": 0.16818009316921234,
      "learning_rate": 1.0140000000000001e-05,
      "loss": 0.0031,
      "step": 119580
    },
    {
      "epoch": 6.378133333333333,
      "grad_norm": 0.16817772388458252,
      "learning_rate": 1.0136666666666667e-05,
      "loss": 0.0023,
      "step": 119590
    },
    {
      "epoch": 6.378666666666667,
      "grad_norm": 0.08409012109041214,
      "learning_rate": 1.0133333333333333e-05,
      "loss": 0.0026,
      "step": 119600
    },
    {
      "epoch": 6.3792,
      "grad_norm": 0.16817931830883026,
      "learning_rate": 1.0130000000000001e-05,
      "loss": 0.0029,
      "step": 119610
    },
    {
      "epoch": 6.379733333333333,
      "grad_norm": 0.08409001678228378,
      "learning_rate": 1.0126666666666667e-05,
      "loss": 0.0026,
      "step": 119620
    },
    {
      "epoch": 6.3802666666666665,
      "grad_norm": 0.028030000627040863,
      "learning_rate": 1.0123333333333334e-05,
      "loss": 0.0035,
      "step": 119630
    },
    {
      "epoch": 6.3808,
      "grad_norm": 0.0840919092297554,
      "learning_rate": 1.012e-05,
      "loss": 0.0023,
      "step": 119640
    },
    {
      "epoch": 6.381333333333333,
      "grad_norm": 0.16818460822105408,
      "learning_rate": 1.0116666666666667e-05,
      "loss": 0.0022,
      "step": 119650
    },
    {
      "epoch": 6.381866666666666,
      "grad_norm": 0.2522647976875305,
      "learning_rate": 1.0113333333333334e-05,
      "loss": 0.0023,
      "step": 119660
    },
    {
      "epoch": 6.3824,
      "grad_norm": 0.3363550007343292,
      "learning_rate": 1.011e-05,
      "loss": 0.0029,
      "step": 119670
    },
    {
      "epoch": 6.382933333333334,
      "grad_norm": 0.2803024649620056,
      "learning_rate": 1.0106666666666668e-05,
      "loss": 0.0023,
      "step": 119680
    },
    {
      "epoch": 6.383466666666667,
      "grad_norm": 0.11212000995874405,
      "learning_rate": 1.0103333333333334e-05,
      "loss": 0.0038,
      "step": 119690
    },
    {
      "epoch": 6.384,
      "grad_norm": 0.11211928725242615,
      "learning_rate": 1.0100000000000002e-05,
      "loss": 0.0021,
      "step": 119700
    },
    {
      "epoch": 6.384533333333334,
      "grad_norm": 0.02802991308271885,
      "learning_rate": 1.0096666666666666e-05,
      "loss": 0.0029,
      "step": 119710
    },
    {
      "epoch": 6.385066666666667,
      "grad_norm": 2.6858379964522783e-09,
      "learning_rate": 1.0093333333333334e-05,
      "loss": 0.0025,
      "step": 119720
    },
    {
      "epoch": 6.3856,
      "grad_norm": 0.0840933546423912,
      "learning_rate": 1.0090000000000002e-05,
      "loss": 0.0017,
      "step": 119730
    },
    {
      "epoch": 6.386133333333333,
      "grad_norm": 0.028031358495354652,
      "learning_rate": 1.0086666666666666e-05,
      "loss": 0.0026,
      "step": 119740
    },
    {
      "epoch": 6.386666666666667,
      "grad_norm": 0.028031468391418457,
      "learning_rate": 1.0083333333333334e-05,
      "loss": 0.0024,
      "step": 119750
    },
    {
      "epoch": 6.3872,
      "grad_norm": 0.0560610294342041,
      "learning_rate": 1.008e-05,
      "loss": 0.0024,
      "step": 119760
    },
    {
      "epoch": 6.387733333333333,
      "grad_norm": 0.02802993729710579,
      "learning_rate": 1.0076666666666668e-05,
      "loss": 0.0034,
      "step": 119770
    },
    {
      "epoch": 6.3882666666666665,
      "grad_norm": 0.17450863122940063,
      "learning_rate": 1.0073333333333334e-05,
      "loss": 0.0027,
      "step": 119780
    },
    {
      "epoch": 6.3888,
      "grad_norm": 0.2803095579147339,
      "learning_rate": 1.007e-05,
      "loss": 0.0023,
      "step": 119790
    },
    {
      "epoch": 6.389333333333333,
      "grad_norm": 0.05919307842850685,
      "learning_rate": 1.0066666666666668e-05,
      "loss": 0.0035,
      "step": 119800
    },
    {
      "epoch": 6.389866666666666,
      "grad_norm": 0.056059759110212326,
      "learning_rate": 1.0063333333333334e-05,
      "loss": 0.0029,
      "step": 119810
    },
    {
      "epoch": 6.3904,
      "grad_norm": 0.16817717254161835,
      "learning_rate": 1.006e-05,
      "loss": 0.0038,
      "step": 119820
    },
    {
      "epoch": 6.390933333333333,
      "grad_norm": 0.05606040731072426,
      "learning_rate": 1.0056666666666666e-05,
      "loss": 0.0042,
      "step": 119830
    },
    {
      "epoch": 6.391466666666667,
      "grad_norm": 0.05606086179614067,
      "learning_rate": 1.0053333333333334e-05,
      "loss": 0.0032,
      "step": 119840
    },
    {
      "epoch": 6.392,
      "grad_norm": 1.2111433744430542,
      "learning_rate": 1.005e-05,
      "loss": 0.0031,
      "step": 119850
    },
    {
      "epoch": 6.392533333333334,
      "grad_norm": 0.2643609046936035,
      "learning_rate": 1.0046666666666666e-05,
      "loss": 0.0024,
      "step": 119860
    },
    {
      "epoch": 6.393066666666667,
      "grad_norm": 0.061483751982450485,
      "learning_rate": 1.0043333333333334e-05,
      "loss": 0.0021,
      "step": 119870
    },
    {
      "epoch": 6.3936,
      "grad_norm": 0.14014770090579987,
      "learning_rate": 1.004e-05,
      "loss": 0.0026,
      "step": 119880
    },
    {
      "epoch": 6.3941333333333334,
      "grad_norm": 0.28031352162361145,
      "learning_rate": 1.0036666666666668e-05,
      "loss": 0.0031,
      "step": 119890
    },
    {
      "epoch": 6.394666666666667,
      "grad_norm": 0.1121239960193634,
      "learning_rate": 1.0033333333333333e-05,
      "loss": 0.0021,
      "step": 119900
    },
    {
      "epoch": 6.3952,
      "grad_norm": 0.3363600969314575,
      "learning_rate": 1.003e-05,
      "loss": 0.0027,
      "step": 119910
    },
    {
      "epoch": 6.395733333333333,
      "grad_norm": 0.02802986092865467,
      "learning_rate": 1.0026666666666668e-05,
      "loss": 0.0017,
      "step": 119920
    },
    {
      "epoch": 6.3962666666666665,
      "grad_norm": 0.1401485651731491,
      "learning_rate": 1.0023333333333333e-05,
      "loss": 0.0042,
      "step": 119930
    },
    {
      "epoch": 6.3968,
      "grad_norm": 0.14014871418476105,
      "learning_rate": 1.002e-05,
      "loss": 0.0027,
      "step": 119940
    },
    {
      "epoch": 6.397333333333333,
      "grad_norm": 0.028030462563037872,
      "learning_rate": 1.0016666666666667e-05,
      "loss": 0.0022,
      "step": 119950
    },
    {
      "epoch": 6.397866666666666,
      "grad_norm": 0.280310720205307,
      "learning_rate": 1.0013333333333335e-05,
      "loss": 0.0021,
      "step": 119960
    },
    {
      "epoch": 6.3984,
      "grad_norm": 0.19621102511882782,
      "learning_rate": 1.001e-05,
      "loss": 0.0023,
      "step": 119970
    },
    {
      "epoch": 6.398933333333333,
      "grad_norm": 0.028029462322592735,
      "learning_rate": 1.0006666666666667e-05,
      "loss": 0.0038,
      "step": 119980
    },
    {
      "epoch": 6.399466666666667,
      "grad_norm": 0.1681811362504959,
      "learning_rate": 1.0003333333333335e-05,
      "loss": 0.0026,
      "step": 119990
    },
    {
      "epoch": 6.4,
      "grad_norm": 0.18245135247707367,
      "learning_rate": 1e-05,
      "loss": 0.0031,
      "step": 120000
    },
    {
      "epoch": 6.400533333333334,
      "grad_norm": 0.2522698938846588,
      "learning_rate": 9.996666666666667e-06,
      "loss": 0.0022,
      "step": 120010
    },
    {
      "epoch": 6.401066666666667,
      "grad_norm": 0.09926459938287735,
      "learning_rate": 9.993333333333333e-06,
      "loss": 0.0025,
      "step": 120020
    },
    {
      "epoch": 6.4016,
      "grad_norm": 0.0953834280371666,
      "learning_rate": 9.990000000000001e-06,
      "loss": 0.0023,
      "step": 120030
    },
    {
      "epoch": 6.4021333333333335,
      "grad_norm": 0.2522716820240021,
      "learning_rate": 9.986666666666667e-06,
      "loss": 0.0023,
      "step": 120040
    },
    {
      "epoch": 6.402666666666667,
      "grad_norm": 0.08524037897586823,
      "learning_rate": 9.983333333333333e-06,
      "loss": 0.0024,
      "step": 120050
    },
    {
      "epoch": 6.4032,
      "grad_norm": 0.1122175008058548,
      "learning_rate": 9.980000000000001e-06,
      "loss": 0.0027,
      "step": 120060
    },
    {
      "epoch": 6.403733333333333,
      "grad_norm": 0.11252912878990173,
      "learning_rate": 9.976666666666667e-06,
      "loss": 0.003,
      "step": 120070
    },
    {
      "epoch": 6.4042666666666666,
      "grad_norm": 0.1962149441242218,
      "learning_rate": 9.973333333333333e-06,
      "loss": 0.0027,
      "step": 120080
    },
    {
      "epoch": 6.4048,
      "grad_norm": 0.36438241600990295,
      "learning_rate": 9.97e-06,
      "loss": 0.0025,
      "step": 120090
    },
    {
      "epoch": 6.405333333333333,
      "grad_norm": 0.19620788097381592,
      "learning_rate": 9.966666666666667e-06,
      "loss": 0.0015,
      "step": 120100
    },
    {
      "epoch": 6.405866666666666,
      "grad_norm": 0.028029827401041985,
      "learning_rate": 9.963333333333335e-06,
      "loss": 0.003,
      "step": 120110
    },
    {
      "epoch": 6.4064,
      "grad_norm": 0.19620384275913239,
      "learning_rate": 9.96e-06,
      "loss": 0.0015,
      "step": 120120
    },
    {
      "epoch": 6.406933333333333,
      "grad_norm": 0.02803000807762146,
      "learning_rate": 9.956666666666667e-06,
      "loss": 0.0025,
      "step": 120130
    },
    {
      "epoch": 6.407466666666666,
      "grad_norm": 0.1964728832244873,
      "learning_rate": 9.953333333333333e-06,
      "loss": 0.0029,
      "step": 120140
    },
    {
      "epoch": 6.408,
      "grad_norm": 0.028030477464199066,
      "learning_rate": 9.950000000000001e-06,
      "loss": 0.0025,
      "step": 120150
    },
    {
      "epoch": 6.408533333333334,
      "grad_norm": 0.2802965044975281,
      "learning_rate": 9.946666666666667e-06,
      "loss": 0.0018,
      "step": 120160
    },
    {
      "epoch": 6.409066666666667,
      "grad_norm": 0.0840880498290062,
      "learning_rate": 9.943333333333334e-06,
      "loss": 0.0021,
      "step": 120170
    },
    {
      "epoch": 6.4096,
      "grad_norm": 0.11212120950222015,
      "learning_rate": 9.940000000000001e-06,
      "loss": 0.0029,
      "step": 120180
    },
    {
      "epoch": 6.4101333333333335,
      "grad_norm": 0.336456298828125,
      "learning_rate": 9.936666666666668e-06,
      "loss": 0.0024,
      "step": 120190
    },
    {
      "epoch": 6.410666666666667,
      "grad_norm": 0.03117247112095356,
      "learning_rate": 9.933333333333334e-06,
      "loss": 0.0031,
      "step": 120200
    },
    {
      "epoch": 6.4112,
      "grad_norm": 0.3363582193851471,
      "learning_rate": 9.93e-06,
      "loss": 0.0024,
      "step": 120210
    },
    {
      "epoch": 6.411733333333333,
      "grad_norm": 0.028031183406710625,
      "learning_rate": 9.926666666666668e-06,
      "loss": 0.0028,
      "step": 120220
    },
    {
      "epoch": 6.412266666666667,
      "grad_norm": 0.16817933320999146,
      "learning_rate": 9.923333333333334e-06,
      "loss": 0.0027,
      "step": 120230
    },
    {
      "epoch": 6.4128,
      "grad_norm": 0.05605887621641159,
      "learning_rate": 9.92e-06,
      "loss": 0.0024,
      "step": 120240
    },
    {
      "epoch": 6.413333333333333,
      "grad_norm": 0.2242421805858612,
      "learning_rate": 9.916666666666668e-06,
      "loss": 0.0044,
      "step": 120250
    },
    {
      "epoch": 6.413866666666666,
      "grad_norm": 0.11211695522069931,
      "learning_rate": 9.913333333333334e-06,
      "loss": 0.0024,
      "step": 120260
    },
    {
      "epoch": 6.4144,
      "grad_norm": 0.02803017757833004,
      "learning_rate": 9.91e-06,
      "loss": 0.0026,
      "step": 120270
    },
    {
      "epoch": 6.414933333333333,
      "grad_norm": 0.2522771954536438,
      "learning_rate": 9.906666666666666e-06,
      "loss": 0.0022,
      "step": 120280
    },
    {
      "epoch": 6.415466666666667,
      "grad_norm": 0.16818664968013763,
      "learning_rate": 9.903333333333334e-06,
      "loss": 0.0024,
      "step": 120290
    },
    {
      "epoch": 6.416,
      "grad_norm": 0.16817982494831085,
      "learning_rate": 9.900000000000002e-06,
      "loss": 0.0042,
      "step": 120300
    },
    {
      "epoch": 6.416533333333334,
      "grad_norm": 3.2107216885179923e-09,
      "learning_rate": 9.896666666666666e-06,
      "loss": 0.0023,
      "step": 120310
    },
    {
      "epoch": 6.417066666666667,
      "grad_norm": 0.08409187197685242,
      "learning_rate": 9.893333333333334e-06,
      "loss": 0.0029,
      "step": 120320
    },
    {
      "epoch": 6.4176,
      "grad_norm": 0.0280313640832901,
      "learning_rate": 9.89e-06,
      "loss": 0.0024,
      "step": 120330
    },
    {
      "epoch": 6.4181333333333335,
      "grad_norm": 0.2803129553794861,
      "learning_rate": 9.886666666666668e-06,
      "loss": 0.0021,
      "step": 120340
    },
    {
      "epoch": 6.418666666666667,
      "grad_norm": 0.2522704601287842,
      "learning_rate": 9.883333333333334e-06,
      "loss": 0.002,
      "step": 120350
    },
    {
      "epoch": 6.4192,
      "grad_norm": 0.19620783627033234,
      "learning_rate": 9.88e-06,
      "loss": 0.0019,
      "step": 120360
    },
    {
      "epoch": 6.419733333333333,
      "grad_norm": 0.1401502937078476,
      "learning_rate": 9.876666666666668e-06,
      "loss": 0.0022,
      "step": 120370
    },
    {
      "epoch": 6.420266666666667,
      "grad_norm": 0.1401505023241043,
      "learning_rate": 9.873333333333334e-06,
      "loss": 0.0023,
      "step": 120380
    },
    {
      "epoch": 6.4208,
      "grad_norm": 0.2802991271018982,
      "learning_rate": 9.87e-06,
      "loss": 0.0035,
      "step": 120390
    },
    {
      "epoch": 6.421333333333333,
      "grad_norm": 0.3364085555076599,
      "learning_rate": 9.866666666666667e-06,
      "loss": 0.0027,
      "step": 120400
    },
    {
      "epoch": 6.421866666666666,
      "grad_norm": 0.08500899374485016,
      "learning_rate": 9.863333333333334e-06,
      "loss": 0.002,
      "step": 120410
    },
    {
      "epoch": 6.4224,
      "grad_norm": 0.1962154656648636,
      "learning_rate": 9.86e-06,
      "loss": 0.0026,
      "step": 120420
    },
    {
      "epoch": 6.422933333333333,
      "grad_norm": 0.19620324671268463,
      "learning_rate": 9.856666666666667e-06,
      "loss": 0.0032,
      "step": 120430
    },
    {
      "epoch": 6.423466666666666,
      "grad_norm": 0.28029587864875793,
      "learning_rate": 9.853333333333334e-06,
      "loss": 0.0035,
      "step": 120440
    },
    {
      "epoch": 6.424,
      "grad_norm": 0.14014947414398193,
      "learning_rate": 9.85e-06,
      "loss": 0.0031,
      "step": 120450
    },
    {
      "epoch": 6.424533333333334,
      "grad_norm": 0.05606042593717575,
      "learning_rate": 9.846666666666667e-06,
      "loss": 0.0033,
      "step": 120460
    },
    {
      "epoch": 6.425066666666667,
      "grad_norm": 0.11211567372083664,
      "learning_rate": 9.843333333333333e-06,
      "loss": 0.0016,
      "step": 120470
    },
    {
      "epoch": 6.4256,
      "grad_norm": 0.11211778223514557,
      "learning_rate": 9.84e-06,
      "loss": 0.0023,
      "step": 120480
    },
    {
      "epoch": 6.4261333333333335,
      "grad_norm": 0.16817694902420044,
      "learning_rate": 9.836666666666668e-06,
      "loss": 0.0022,
      "step": 120490
    },
    {
      "epoch": 6.426666666666667,
      "grad_norm": 0.05605989694595337,
      "learning_rate": 9.833333333333333e-06,
      "loss": 0.0026,
      "step": 120500
    },
    {
      "epoch": 6.4272,
      "grad_norm": 0.2242337316274643,
      "learning_rate": 9.83e-06,
      "loss": 0.002,
      "step": 120510
    },
    {
      "epoch": 6.427733333333333,
      "grad_norm": 0.08408704400062561,
      "learning_rate": 9.826666666666667e-06,
      "loss": 0.0025,
      "step": 120520
    },
    {
      "epoch": 6.428266666666667,
      "grad_norm": 0.14014394581317902,
      "learning_rate": 9.823333333333335e-06,
      "loss": 0.0014,
      "step": 120530
    },
    {
      "epoch": 6.4288,
      "grad_norm": 0.056059714406728745,
      "learning_rate": 9.820000000000001e-06,
      "loss": 0.0026,
      "step": 120540
    },
    {
      "epoch": 6.429333333333333,
      "grad_norm": 1.6211234576957168e-09,
      "learning_rate": 9.816666666666667e-06,
      "loss": 0.0017,
      "step": 120550
    },
    {
      "epoch": 6.429866666666666,
      "grad_norm": 0.16817423701286316,
      "learning_rate": 9.813333333333335e-06,
      "loss": 0.0032,
      "step": 120560
    },
    {
      "epoch": 6.4304,
      "grad_norm": 0.19620683789253235,
      "learning_rate": 9.810000000000001e-06,
      "loss": 0.0031,
      "step": 120570
    },
    {
      "epoch": 6.430933333333333,
      "grad_norm": 0.19620674848556519,
      "learning_rate": 9.806666666666667e-06,
      "loss": 0.0022,
      "step": 120580
    },
    {
      "epoch": 6.431466666666667,
      "grad_norm": 0.05605952814221382,
      "learning_rate": 9.803333333333333e-06,
      "loss": 0.0017,
      "step": 120590
    },
    {
      "epoch": 6.432,
      "grad_norm": 1.8924253275542924e-09,
      "learning_rate": 9.800000000000001e-06,
      "loss": 0.0034,
      "step": 120600
    },
    {
      "epoch": 6.432533333333334,
      "grad_norm": 0.0840863585472107,
      "learning_rate": 9.796666666666667e-06,
      "loss": 0.0022,
      "step": 120610
    },
    {
      "epoch": 6.433066666666667,
      "grad_norm": 0.028029432520270348,
      "learning_rate": 9.793333333333333e-06,
      "loss": 0.0025,
      "step": 120620
    },
    {
      "epoch": 6.4336,
      "grad_norm": 0.33637258410453796,
      "learning_rate": 9.790000000000001e-06,
      "loss": 0.0022,
      "step": 120630
    },
    {
      "epoch": 6.4341333333333335,
      "grad_norm": 0.16817666590213776,
      "learning_rate": 9.786666666666667e-06,
      "loss": 0.0024,
      "step": 120640
    },
    {
      "epoch": 6.434666666666667,
      "grad_norm": 0.02802879549562931,
      "learning_rate": 9.783333333333333e-06,
      "loss": 0.0029,
      "step": 120650
    },
    {
      "epoch": 6.4352,
      "grad_norm": 0.11212095618247986,
      "learning_rate": 9.78e-06,
      "loss": 0.003,
      "step": 120660
    },
    {
      "epoch": 6.435733333333333,
      "grad_norm": 0.08408984541893005,
      "learning_rate": 9.776666666666667e-06,
      "loss": 0.0026,
      "step": 120670
    },
    {
      "epoch": 6.436266666666667,
      "grad_norm": 0.5045183897018433,
      "learning_rate": 9.773333333333333e-06,
      "loss": 0.0025,
      "step": 120680
    },
    {
      "epoch": 6.4368,
      "grad_norm": 0.16817520558834076,
      "learning_rate": 9.77e-06,
      "loss": 0.0022,
      "step": 120690
    },
    {
      "epoch": 6.437333333333333,
      "grad_norm": 0.16817356646060944,
      "learning_rate": 9.766666666666667e-06,
      "loss": 0.003,
      "step": 120700
    },
    {
      "epoch": 6.437866666666666,
      "grad_norm": 0.05605867877602577,
      "learning_rate": 9.763333333333334e-06,
      "loss": 0.0025,
      "step": 120710
    },
    {
      "epoch": 6.4384,
      "grad_norm": 0.08408932387828827,
      "learning_rate": 9.760000000000001e-06,
      "loss": 0.0021,
      "step": 120720
    },
    {
      "epoch": 6.438933333333333,
      "grad_norm": 0.11211755126714706,
      "learning_rate": 9.756666666666668e-06,
      "loss": 0.003,
      "step": 120730
    },
    {
      "epoch": 6.439466666666666,
      "grad_norm": 0.08408766239881516,
      "learning_rate": 9.753333333333334e-06,
      "loss": 0.0017,
      "step": 120740
    },
    {
      "epoch": 6.44,
      "grad_norm": 0.08408762514591217,
      "learning_rate": 9.750000000000002e-06,
      "loss": 0.0021,
      "step": 120750
    },
    {
      "epoch": 6.440533333333334,
      "grad_norm": 0.08408913016319275,
      "learning_rate": 9.746666666666666e-06,
      "loss": 0.0028,
      "step": 120760
    },
    {
      "epoch": 6.441066666666667,
      "grad_norm": 0.11212164163589478,
      "learning_rate": 9.743333333333334e-06,
      "loss": 0.0035,
      "step": 120770
    },
    {
      "epoch": 6.4416,
      "grad_norm": 0.2242429256439209,
      "learning_rate": 9.74e-06,
      "loss": 0.0029,
      "step": 120780
    },
    {
      "epoch": 6.4421333333333335,
      "grad_norm": 0.12399803102016449,
      "learning_rate": 9.736666666666668e-06,
      "loss": 0.0034,
      "step": 120790
    },
    {
      "epoch": 6.442666666666667,
      "grad_norm": 0.08408892899751663,
      "learning_rate": 9.733333333333334e-06,
      "loss": 0.0044,
      "step": 120800
    },
    {
      "epoch": 6.4432,
      "grad_norm": 0.14014354348182678,
      "learning_rate": 9.73e-06,
      "loss": 0.0027,
      "step": 120810
    },
    {
      "epoch": 6.443733333333333,
      "grad_norm": 0.1681729555130005,
      "learning_rate": 9.726666666666668e-06,
      "loss": 0.0034,
      "step": 120820
    },
    {
      "epoch": 6.444266666666667,
      "grad_norm": 0.05605795606970787,
      "learning_rate": 9.723333333333334e-06,
      "loss": 0.003,
      "step": 120830
    },
    {
      "epoch": 6.4448,
      "grad_norm": 0.252259761095047,
      "learning_rate": 9.72e-06,
      "loss": 0.0024,
      "step": 120840
    },
    {
      "epoch": 6.445333333333333,
      "grad_norm": 0.25225940346717834,
      "learning_rate": 9.716666666666666e-06,
      "loss": 0.003,
      "step": 120850
    },
    {
      "epoch": 6.445866666666666,
      "grad_norm": 0.25226691365242004,
      "learning_rate": 9.713333333333334e-06,
      "loss": 0.0019,
      "step": 120860
    },
    {
      "epoch": 6.4464,
      "grad_norm": 0.3643933832645416,
      "learning_rate": 9.71e-06,
      "loss": 0.0025,
      "step": 120870
    },
    {
      "epoch": 6.446933333333333,
      "grad_norm": 0.08408719301223755,
      "learning_rate": 9.706666666666666e-06,
      "loss": 0.003,
      "step": 120880
    },
    {
      "epoch": 6.447466666666667,
      "grad_norm": 0.19917629659175873,
      "learning_rate": 9.703333333333334e-06,
      "loss": 0.0026,
      "step": 120890
    },
    {
      "epoch": 6.448,
      "grad_norm": 0.19621163606643677,
      "learning_rate": 9.7e-06,
      "loss": 0.002,
      "step": 120900
    },
    {
      "epoch": 6.448533333333334,
      "grad_norm": 0.14014646410942078,
      "learning_rate": 9.696666666666668e-06,
      "loss": 0.0026,
      "step": 120910
    },
    {
      "epoch": 6.449066666666667,
      "grad_norm": 1.747770372872992e-09,
      "learning_rate": 9.693333333333334e-06,
      "loss": 0.0026,
      "step": 120920
    },
    {
      "epoch": 6.4496,
      "grad_norm": 0.14015161991119385,
      "learning_rate": 9.69e-06,
      "loss": 0.0029,
      "step": 120930
    },
    {
      "epoch": 6.4501333333333335,
      "grad_norm": 0.3083263635635376,
      "learning_rate": 9.686666666666668e-06,
      "loss": 0.0035,
      "step": 120940
    },
    {
      "epoch": 6.450666666666667,
      "grad_norm": 0.19620224833488464,
      "learning_rate": 9.683333333333333e-06,
      "loss": 0.0024,
      "step": 120950
    },
    {
      "epoch": 6.4512,
      "grad_norm": 0.3643817603588104,
      "learning_rate": 9.68e-06,
      "loss": 0.0029,
      "step": 120960
    },
    {
      "epoch": 6.451733333333333,
      "grad_norm": 0.2522561848163605,
      "learning_rate": 9.676666666666667e-06,
      "loss": 0.0037,
      "step": 120970
    },
    {
      "epoch": 6.452266666666667,
      "grad_norm": 0.11211629211902618,
      "learning_rate": 9.673333333333334e-06,
      "loss": 0.0027,
      "step": 120980
    },
    {
      "epoch": 6.4528,
      "grad_norm": 0.11212151497602463,
      "learning_rate": 9.67e-06,
      "loss": 0.0023,
      "step": 120990
    },
    {
      "epoch": 6.453333333333333,
      "grad_norm": 0.196214959025383,
      "learning_rate": 9.666666666666667e-06,
      "loss": 0.0024,
      "step": 121000
    },
    {
      "epoch": 6.453866666666666,
      "grad_norm": 0.14024780690670013,
      "learning_rate": 9.663333333333335e-06,
      "loss": 0.0026,
      "step": 121010
    },
    {
      "epoch": 6.4544,
      "grad_norm": 1.8060575257550227e-09,
      "learning_rate": 9.66e-06,
      "loss": 0.0028,
      "step": 121020
    },
    {
      "epoch": 6.454933333333333,
      "grad_norm": 3.050426855555344e-10,
      "learning_rate": 9.656666666666667e-06,
      "loss": 0.0029,
      "step": 121030
    },
    {
      "epoch": 6.455466666666666,
      "grad_norm": 0.11211735755205154,
      "learning_rate": 9.653333333333333e-06,
      "loss": 0.0033,
      "step": 121040
    },
    {
      "epoch": 6.456,
      "grad_norm": 0.11211439222097397,
      "learning_rate": 9.65e-06,
      "loss": 0.0025,
      "step": 121050
    },
    {
      "epoch": 6.456533333333334,
      "grad_norm": 0.11211645603179932,
      "learning_rate": 9.646666666666667e-06,
      "loss": 0.0021,
      "step": 121060
    },
    {
      "epoch": 6.457066666666667,
      "grad_norm": 0.11211638152599335,
      "learning_rate": 9.643333333333333e-06,
      "loss": 0.0022,
      "step": 121070
    },
    {
      "epoch": 6.4576,
      "grad_norm": 0.30831754207611084,
      "learning_rate": 9.640000000000001e-06,
      "loss": 0.002,
      "step": 121080
    },
    {
      "epoch": 6.4581333333333335,
      "grad_norm": 0.056058842688798904,
      "learning_rate": 9.636666666666667e-06,
      "loss": 0.0022,
      "step": 121090
    },
    {
      "epoch": 6.458666666666667,
      "grad_norm": 0.02802933380007744,
      "learning_rate": 9.633333333333335e-06,
      "loss": 0.0023,
      "step": 121100
    },
    {
      "epoch": 6.4592,
      "grad_norm": 0.22422337532043457,
      "learning_rate": 9.630000000000001e-06,
      "loss": 0.0033,
      "step": 121110
    },
    {
      "epoch": 6.459733333333333,
      "grad_norm": 0.02802915871143341,
      "learning_rate": 9.626666666666667e-06,
      "loss": 0.004,
      "step": 121120
    },
    {
      "epoch": 6.460266666666667,
      "grad_norm": 0.1962120682001114,
      "learning_rate": 9.623333333333335e-06,
      "loss": 0.0027,
      "step": 121130
    },
    {
      "epoch": 6.4608,
      "grad_norm": 0.2522631883621216,
      "learning_rate": 9.62e-06,
      "loss": 0.0036,
      "step": 121140
    },
    {
      "epoch": 6.461333333333333,
      "grad_norm": 0.2242291122674942,
      "learning_rate": 9.616666666666667e-06,
      "loss": 0.0018,
      "step": 121150
    },
    {
      "epoch": 6.461866666666666,
      "grad_norm": 1.3999205827713013,
      "learning_rate": 9.613333333333333e-06,
      "loss": 0.0022,
      "step": 121160
    },
    {
      "epoch": 6.4624,
      "grad_norm": 0.19619925320148468,
      "learning_rate": 9.610000000000001e-06,
      "loss": 0.0031,
      "step": 121170
    },
    {
      "epoch": 6.462933333333333,
      "grad_norm": 0.08409001678228378,
      "learning_rate": 9.606666666666667e-06,
      "loss": 0.003,
      "step": 121180
    },
    {
      "epoch": 6.463466666666667,
      "grad_norm": 0.1401471048593521,
      "learning_rate": 9.603333333333333e-06,
      "loss": 0.0028,
      "step": 121190
    },
    {
      "epoch": 6.464,
      "grad_norm": 0.2245962768793106,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.0022,
      "step": 121200
    },
    {
      "epoch": 6.464533333333334,
      "grad_norm": 0.056056778877973557,
      "learning_rate": 9.596666666666667e-06,
      "loss": 0.0022,
      "step": 121210
    },
    {
      "epoch": 6.465066666666667,
      "grad_norm": 0.4204317331314087,
      "learning_rate": 9.593333333333334e-06,
      "loss": 0.0038,
      "step": 121220
    },
    {
      "epoch": 6.4656,
      "grad_norm": 0.05605974793434143,
      "learning_rate": 9.59e-06,
      "loss": 0.0024,
      "step": 121230
    },
    {
      "epoch": 6.4661333333333335,
      "grad_norm": 0.11212173849344254,
      "learning_rate": 9.586666666666667e-06,
      "loss": 0.0024,
      "step": 121240
    },
    {
      "epoch": 6.466666666666667,
      "grad_norm": 0.14015215635299683,
      "learning_rate": 9.583333333333334e-06,
      "loss": 0.0027,
      "step": 121250
    },
    {
      "epoch": 6.4672,
      "grad_norm": 0.08409010618925095,
      "learning_rate": 9.58e-06,
      "loss": 0.0022,
      "step": 121260
    },
    {
      "epoch": 6.467733333333333,
      "grad_norm": 0.11211244761943817,
      "learning_rate": 9.576666666666668e-06,
      "loss": 0.003,
      "step": 121270
    },
    {
      "epoch": 6.468266666666667,
      "grad_norm": 0.11211191862821579,
      "learning_rate": 9.573333333333334e-06,
      "loss": 0.0037,
      "step": 121280
    },
    {
      "epoch": 6.4688,
      "grad_norm": 0.05605874955654144,
      "learning_rate": 9.57e-06,
      "loss": 0.003,
      "step": 121290
    },
    {
      "epoch": 6.469333333333333,
      "grad_norm": 0.22423222661018372,
      "learning_rate": 9.566666666666666e-06,
      "loss": 0.0021,
      "step": 121300
    },
    {
      "epoch": 6.469866666666666,
      "grad_norm": 3.739649478973206e-09,
      "learning_rate": 9.563333333333334e-06,
      "loss": 0.0026,
      "step": 121310
    },
    {
      "epoch": 6.4704,
      "grad_norm": 0.02802916429936886,
      "learning_rate": 9.560000000000002e-06,
      "loss": 0.0016,
      "step": 121320
    },
    {
      "epoch": 6.470933333333333,
      "grad_norm": 0.02803110145032406,
      "learning_rate": 9.556666666666666e-06,
      "loss": 0.0032,
      "step": 121330
    },
    {
      "epoch": 6.471466666666666,
      "grad_norm": 0.28029951453208923,
      "learning_rate": 9.553333333333334e-06,
      "loss": 0.0025,
      "step": 121340
    },
    {
      "epoch": 6.4719999999999995,
      "grad_norm": 0.02803018130362034,
      "learning_rate": 9.55e-06,
      "loss": 0.0019,
      "step": 121350
    },
    {
      "epoch": 6.472533333333334,
      "grad_norm": 0.2802961468696594,
      "learning_rate": 9.546666666666668e-06,
      "loss": 0.0022,
      "step": 121360
    },
    {
      "epoch": 6.473066666666667,
      "grad_norm": 0.19676966965198517,
      "learning_rate": 9.543333333333334e-06,
      "loss": 0.0023,
      "step": 121370
    },
    {
      "epoch": 6.4736,
      "grad_norm": 0.08409015834331512,
      "learning_rate": 9.54e-06,
      "loss": 0.003,
      "step": 121380
    },
    {
      "epoch": 6.4741333333333335,
      "grad_norm": 0.08408486843109131,
      "learning_rate": 9.536666666666668e-06,
      "loss": 0.0034,
      "step": 121390
    },
    {
      "epoch": 6.474666666666667,
      "grad_norm": 0.056926384568214417,
      "learning_rate": 9.533333333333334e-06,
      "loss": 0.0025,
      "step": 121400
    },
    {
      "epoch": 6.4752,
      "grad_norm": 0.056058984249830246,
      "learning_rate": 9.53e-06,
      "loss": 0.0027,
      "step": 121410
    },
    {
      "epoch": 6.475733333333333,
      "grad_norm": 0.05605755001306534,
      "learning_rate": 9.526666666666666e-06,
      "loss": 0.0026,
      "step": 121420
    },
    {
      "epoch": 6.476266666666667,
      "grad_norm": 0.2522486746311188,
      "learning_rate": 9.523333333333334e-06,
      "loss": 0.0019,
      "step": 121430
    },
    {
      "epoch": 6.4768,
      "grad_norm": 4.361346839232283e-09,
      "learning_rate": 9.52e-06,
      "loss": 0.0023,
      "step": 121440
    },
    {
      "epoch": 6.477333333333333,
      "grad_norm": 4.068152037461914e-09,
      "learning_rate": 9.516666666666666e-06,
      "loss": 0.0035,
      "step": 121450
    },
    {
      "epoch": 6.477866666666666,
      "grad_norm": 0.28027814626693726,
      "learning_rate": 9.513333333333334e-06,
      "loss": 0.0035,
      "step": 121460
    },
    {
      "epoch": 6.4784,
      "grad_norm": 0.1962062269449234,
      "learning_rate": 9.51e-06,
      "loss": 0.0015,
      "step": 121470
    },
    {
      "epoch": 6.478933333333333,
      "grad_norm": 0.11211348325014114,
      "learning_rate": 9.506666666666667e-06,
      "loss": 0.0021,
      "step": 121480
    },
    {
      "epoch": 6.479466666666666,
      "grad_norm": 0.168166384100914,
      "learning_rate": 9.503333333333333e-06,
      "loss": 0.0027,
      "step": 121490
    },
    {
      "epoch": 6.48,
      "grad_norm": 0.05605581775307655,
      "learning_rate": 9.5e-06,
      "loss": 0.0022,
      "step": 121500
    },
    {
      "epoch": 6.480533333333334,
      "grad_norm": 0.0840844139456749,
      "learning_rate": 9.496666666666668e-06,
      "loss": 0.0022,
      "step": 121510
    },
    {
      "epoch": 6.481066666666667,
      "grad_norm": 0.02802797220647335,
      "learning_rate": 9.493333333333333e-06,
      "loss": 0.0022,
      "step": 121520
    },
    {
      "epoch": 6.4816,
      "grad_norm": 1.792128556665773e-09,
      "learning_rate": 9.49e-06,
      "loss": 0.0033,
      "step": 121530
    },
    {
      "epoch": 6.4821333333333335,
      "grad_norm": 4.648029072740201e-09,
      "learning_rate": 9.486666666666667e-06,
      "loss": 0.0036,
      "step": 121540
    },
    {
      "epoch": 6.482666666666667,
      "grad_norm": 0.3363363742828369,
      "learning_rate": 9.483333333333335e-06,
      "loss": 0.0028,
      "step": 121550
    },
    {
      "epoch": 6.4832,
      "grad_norm": 0.14013628661632538,
      "learning_rate": 9.48e-06,
      "loss": 0.0037,
      "step": 121560
    },
    {
      "epoch": 6.483733333333333,
      "grad_norm": 0.28027546405792236,
      "learning_rate": 9.476666666666667e-06,
      "loss": 0.0032,
      "step": 121570
    },
    {
      "epoch": 6.484266666666667,
      "grad_norm": 0.08408524841070175,
      "learning_rate": 9.473333333333335e-06,
      "loss": 0.0022,
      "step": 121580
    },
    {
      "epoch": 6.4848,
      "grad_norm": 0.05605660751461983,
      "learning_rate": 9.47e-06,
      "loss": 0.0018,
      "step": 121590
    },
    {
      "epoch": 6.485333333333333,
      "grad_norm": 0.3363337814807892,
      "learning_rate": 9.466666666666667e-06,
      "loss": 0.0016,
      "step": 121600
    },
    {
      "epoch": 6.4858666666666664,
      "grad_norm": 0.11211004853248596,
      "learning_rate": 9.463333333333333e-06,
      "loss": 0.0024,
      "step": 121610
    },
    {
      "epoch": 6.4864,
      "grad_norm": 0.47647085785865784,
      "learning_rate": 9.460000000000001e-06,
      "loss": 0.0035,
      "step": 121620
    },
    {
      "epoch": 6.486933333333333,
      "grad_norm": 0.5045083165168762,
      "learning_rate": 9.456666666666667e-06,
      "loss": 0.0019,
      "step": 121630
    },
    {
      "epoch": 6.487466666666666,
      "grad_norm": 0.14013542234897614,
      "learning_rate": 9.453333333333333e-06,
      "loss": 0.0031,
      "step": 121640
    },
    {
      "epoch": 6.4879999999999995,
      "grad_norm": 0.08408784121274948,
      "learning_rate": 9.450000000000001e-06,
      "loss": 0.002,
      "step": 121650
    },
    {
      "epoch": 6.488533333333334,
      "grad_norm": 0.2242358922958374,
      "learning_rate": 9.446666666666667e-06,
      "loss": 0.0034,
      "step": 121660
    },
    {
      "epoch": 6.489066666666667,
      "grad_norm": 4.660283714486013e-09,
      "learning_rate": 9.443333333333333e-06,
      "loss": 0.0025,
      "step": 121670
    },
    {
      "epoch": 6.4896,
      "grad_norm": 0.08408468216657639,
      "learning_rate": 9.44e-06,
      "loss": 0.0036,
      "step": 121680
    },
    {
      "epoch": 6.4901333333333335,
      "grad_norm": 0.25224363803863525,
      "learning_rate": 9.436666666666667e-06,
      "loss": 0.0014,
      "step": 121690
    },
    {
      "epoch": 6.490666666666667,
      "grad_norm": 0.22422872483730316,
      "learning_rate": 9.433333333333335e-06,
      "loss": 0.0031,
      "step": 121700
    },
    {
      "epoch": 6.4912,
      "grad_norm": 0.028028616681694984,
      "learning_rate": 9.43e-06,
      "loss": 0.0031,
      "step": 121710
    },
    {
      "epoch": 6.491733333333333,
      "grad_norm": 0.0840849056839943,
      "learning_rate": 9.426666666666667e-06,
      "loss": 0.0029,
      "step": 121720
    },
    {
      "epoch": 6.492266666666667,
      "grad_norm": 0.168171688914299,
      "learning_rate": 9.423333333333333e-06,
      "loss": 0.0022,
      "step": 121730
    },
    {
      "epoch": 6.4928,
      "grad_norm": 0.8356245756149292,
      "learning_rate": 9.420000000000001e-06,
      "loss": 0.0021,
      "step": 121740
    },
    {
      "epoch": 6.493333333333333,
      "grad_norm": 0.05605535954236984,
      "learning_rate": 9.416666666666667e-06,
      "loss": 0.0027,
      "step": 121750
    },
    {
      "epoch": 6.4938666666666665,
      "grad_norm": 0.280274361371994,
      "learning_rate": 9.413333333333334e-06,
      "loss": 0.0037,
      "step": 121760
    },
    {
      "epoch": 6.4944,
      "grad_norm": 0.2802741527557373,
      "learning_rate": 9.410000000000001e-06,
      "loss": 0.0027,
      "step": 121770
    },
    {
      "epoch": 6.494933333333333,
      "grad_norm": 0.2522532641887665,
      "learning_rate": 9.406666666666668e-06,
      "loss": 0.0024,
      "step": 121780
    },
    {
      "epoch": 6.495466666666666,
      "grad_norm": 0.19619548320770264,
      "learning_rate": 9.403333333333334e-06,
      "loss": 0.0022,
      "step": 121790
    },
    {
      "epoch": 6.496,
      "grad_norm": 0.0840822234749794,
      "learning_rate": 9.4e-06,
      "loss": 0.0017,
      "step": 121800
    },
    {
      "epoch": 6.496533333333334,
      "grad_norm": 0.33632951974868774,
      "learning_rate": 9.396666666666668e-06,
      "loss": 0.0016,
      "step": 121810
    },
    {
      "epoch": 6.497066666666667,
      "grad_norm": 0.02802812121808529,
      "learning_rate": 9.393333333333334e-06,
      "loss": 0.0028,
      "step": 121820
    },
    {
      "epoch": 6.4976,
      "grad_norm": 0.14014571905136108,
      "learning_rate": 9.39e-06,
      "loss": 0.0018,
      "step": 121830
    },
    {
      "epoch": 6.4981333333333335,
      "grad_norm": 0.05605822056531906,
      "learning_rate": 9.386666666666668e-06,
      "loss": 0.0022,
      "step": 121840
    },
    {
      "epoch": 6.498666666666667,
      "grad_norm": 0.1401430368423462,
      "learning_rate": 9.383333333333334e-06,
      "loss": 0.0036,
      "step": 121850
    },
    {
      "epoch": 6.4992,
      "grad_norm": 0.1689416617155075,
      "learning_rate": 9.38e-06,
      "loss": 0.0021,
      "step": 121860
    },
    {
      "epoch": 6.499733333333333,
      "grad_norm": 0.39238619804382324,
      "learning_rate": 9.376666666666666e-06,
      "loss": 0.0014,
      "step": 121870
    },
    {
      "epoch": 6.500266666666667,
      "grad_norm": 0.028027597814798355,
      "learning_rate": 9.373333333333334e-06,
      "loss": 0.0029,
      "step": 121880
    },
    {
      "epoch": 6.5008,
      "grad_norm": 1.1206661154616882e-09,
      "learning_rate": 9.370000000000002e-06,
      "loss": 0.0025,
      "step": 121890
    },
    {
      "epoch": 6.501333333333333,
      "grad_norm": 0.08408326655626297,
      "learning_rate": 9.366666666666666e-06,
      "loss": 0.0026,
      "step": 121900
    },
    {
      "epoch": 6.5018666666666665,
      "grad_norm": 8.640441140350674e-10,
      "learning_rate": 9.363333333333334e-06,
      "loss": 0.0027,
      "step": 121910
    },
    {
      "epoch": 6.5024,
      "grad_norm": 0.08408185094594955,
      "learning_rate": 9.36e-06,
      "loss": 0.0022,
      "step": 121920
    },
    {
      "epoch": 6.502933333333333,
      "grad_norm": 0.08408407866954803,
      "learning_rate": 9.356666666666668e-06,
      "loss": 0.0031,
      "step": 121930
    },
    {
      "epoch": 6.503466666666666,
      "grad_norm": 0.30849727988243103,
      "learning_rate": 9.353333333333334e-06,
      "loss": 0.0041,
      "step": 121940
    },
    {
      "epoch": 6.504,
      "grad_norm": 0.1121155172586441,
      "learning_rate": 9.35e-06,
      "loss": 0.0018,
      "step": 121950
    },
    {
      "epoch": 6.504533333333333,
      "grad_norm": 0.084084153175354,
      "learning_rate": 9.346666666666668e-06,
      "loss": 0.0019,
      "step": 121960
    },
    {
      "epoch": 6.505066666666667,
      "grad_norm": 0.08408326655626297,
      "learning_rate": 9.343333333333333e-06,
      "loss": 0.0026,
      "step": 121970
    },
    {
      "epoch": 6.5056,
      "grad_norm": 0.08408597111701965,
      "learning_rate": 9.34e-06,
      "loss": 0.0023,
      "step": 121980
    },
    {
      "epoch": 6.5061333333333335,
      "grad_norm": 0.11211267858743668,
      "learning_rate": 9.336666666666666e-06,
      "loss": 0.0021,
      "step": 121990
    },
    {
      "epoch": 6.506666666666667,
      "grad_norm": 0.33632659912109375,
      "learning_rate": 9.333333333333334e-06,
      "loss": 0.0027,
      "step": 122000
    },
    {
      "epoch": 6.5072,
      "grad_norm": 0.19619712233543396,
      "learning_rate": 9.33e-06,
      "loss": 0.0018,
      "step": 122010
    },
    {
      "epoch": 6.507733333333333,
      "grad_norm": 0.03219184651970863,
      "learning_rate": 9.326666666666667e-06,
      "loss": 0.002,
      "step": 122020
    },
    {
      "epoch": 6.508266666666667,
      "grad_norm": 0.22423939406871796,
      "learning_rate": 9.323333333333334e-06,
      "loss": 0.0042,
      "step": 122030
    },
    {
      "epoch": 6.5088,
      "grad_norm": 0.3084142804145813,
      "learning_rate": 9.32e-06,
      "loss": 0.0024,
      "step": 122040
    },
    {
      "epoch": 6.509333333333333,
      "grad_norm": 0.16815786063671112,
      "learning_rate": 9.316666666666667e-06,
      "loss": 0.0021,
      "step": 122050
    },
    {
      "epoch": 6.5098666666666665,
      "grad_norm": 0.1401422917842865,
      "learning_rate": 9.313333333333333e-06,
      "loss": 0.0017,
      "step": 122060
    },
    {
      "epoch": 6.5104,
      "grad_norm": 1.0867116451263428,
      "learning_rate": 9.31e-06,
      "loss": 0.0031,
      "step": 122070
    },
    {
      "epoch": 6.510933333333333,
      "grad_norm": 0.252257376909256,
      "learning_rate": 9.306666666666668e-06,
      "loss": 0.0014,
      "step": 122080
    },
    {
      "epoch": 6.511466666666666,
      "grad_norm": 8.356940917053635e-09,
      "learning_rate": 9.303333333333333e-06,
      "loss": 0.0025,
      "step": 122090
    },
    {
      "epoch": 6.5120000000000005,
      "grad_norm": 3.594283315422331e-09,
      "learning_rate": 9.3e-06,
      "loss": 0.003,
      "step": 122100
    },
    {
      "epoch": 6.512533333333334,
      "grad_norm": 0.2522478997707367,
      "learning_rate": 9.296666666666667e-06,
      "loss": 0.0038,
      "step": 122110
    },
    {
      "epoch": 6.513066666666667,
      "grad_norm": 0.1401348114013672,
      "learning_rate": 9.293333333333335e-06,
      "loss": 0.0032,
      "step": 122120
    },
    {
      "epoch": 6.5136,
      "grad_norm": 2.342372740216092e-09,
      "learning_rate": 9.29e-06,
      "loss": 0.0032,
      "step": 122130
    },
    {
      "epoch": 6.5141333333333336,
      "grad_norm": 0.11210902780294418,
      "learning_rate": 9.286666666666667e-06,
      "loss": 0.0035,
      "step": 122140
    },
    {
      "epoch": 6.514666666666667,
      "grad_norm": 0.2802683413028717,
      "learning_rate": 9.283333333333335e-06,
      "loss": 0.0031,
      "step": 122150
    },
    {
      "epoch": 6.5152,
      "grad_norm": 0.08408515900373459,
      "learning_rate": 9.28e-06,
      "loss": 0.0039,
      "step": 122160
    },
    {
      "epoch": 6.515733333333333,
      "grad_norm": 0.47648969292640686,
      "learning_rate": 9.276666666666667e-06,
      "loss": 0.0029,
      "step": 122170
    },
    {
      "epoch": 6.516266666666667,
      "grad_norm": 0.05605378374457359,
      "learning_rate": 9.273333333333333e-06,
      "loss": 0.0026,
      "step": 122180
    },
    {
      "epoch": 6.5168,
      "grad_norm": 0.056052837520837784,
      "learning_rate": 9.270000000000001e-06,
      "loss": 0.0017,
      "step": 122190
    },
    {
      "epoch": 6.517333333333333,
      "grad_norm": 0.05605531111359596,
      "learning_rate": 9.266666666666667e-06,
      "loss": 0.0025,
      "step": 122200
    },
    {
      "epoch": 6.5178666666666665,
      "grad_norm": 0.47647082805633545,
      "learning_rate": 9.263333333333333e-06,
      "loss": 0.0029,
      "step": 122210
    },
    {
      "epoch": 6.5184,
      "grad_norm": 0.028028473258018494,
      "learning_rate": 9.260000000000001e-06,
      "loss": 0.0031,
      "step": 122220
    },
    {
      "epoch": 6.518933333333333,
      "grad_norm": 0.056055255234241486,
      "learning_rate": 9.256666666666667e-06,
      "loss": 0.0028,
      "step": 122230
    },
    {
      "epoch": 6.519466666666666,
      "grad_norm": 0.2522423267364502,
      "learning_rate": 9.253333333333333e-06,
      "loss": 0.0025,
      "step": 122240
    },
    {
      "epoch": 6.52,
      "grad_norm": 0.14013946056365967,
      "learning_rate": 9.25e-06,
      "loss": 0.0031,
      "step": 122250
    },
    {
      "epoch": 6.520533333333333,
      "grad_norm": 0.08408063650131226,
      "learning_rate": 9.246666666666667e-06,
      "loss": 0.0029,
      "step": 122260
    },
    {
      "epoch": 6.521066666666667,
      "grad_norm": 0.02802685648202896,
      "learning_rate": 9.243333333333335e-06,
      "loss": 0.0029,
      "step": 122270
    },
    {
      "epoch": 6.5216,
      "grad_norm": 0.14013440907001495,
      "learning_rate": 9.24e-06,
      "loss": 0.0021,
      "step": 122280
    },
    {
      "epoch": 6.522133333333334,
      "grad_norm": 0.05605398491024971,
      "learning_rate": 9.236666666666667e-06,
      "loss": 0.0026,
      "step": 122290
    },
    {
      "epoch": 6.522666666666667,
      "grad_norm": 0.1121094599366188,
      "learning_rate": 9.233333333333334e-06,
      "loss": 0.0028,
      "step": 122300
    },
    {
      "epoch": 6.5232,
      "grad_norm": 0.028027236461639404,
      "learning_rate": 9.23e-06,
      "loss": 0.003,
      "step": 122310
    },
    {
      "epoch": 6.523733333333333,
      "grad_norm": 0.2242128998041153,
      "learning_rate": 9.226666666666668e-06,
      "loss": 0.0021,
      "step": 122320
    },
    {
      "epoch": 6.524266666666667,
      "grad_norm": 0.11210715025663376,
      "learning_rate": 9.223333333333334e-06,
      "loss": 0.0022,
      "step": 122330
    },
    {
      "epoch": 6.5248,
      "grad_norm": 0.36435526609420776,
      "learning_rate": 9.220000000000002e-06,
      "loss": 0.0024,
      "step": 122340
    },
    {
      "epoch": 6.525333333333333,
      "grad_norm": 0.11211030185222626,
      "learning_rate": 9.216666666666666e-06,
      "loss": 0.0028,
      "step": 122350
    },
    {
      "epoch": 6.5258666666666665,
      "grad_norm": 0.16816909611225128,
      "learning_rate": 9.213333333333334e-06,
      "loss": 0.0025,
      "step": 122360
    },
    {
      "epoch": 6.5264,
      "grad_norm": 0.08408184349536896,
      "learning_rate": 9.21e-06,
      "loss": 0.0031,
      "step": 122370
    },
    {
      "epoch": 6.526933333333333,
      "grad_norm": 0.08408033102750778,
      "learning_rate": 9.206666666666668e-06,
      "loss": 0.0031,
      "step": 122380
    },
    {
      "epoch": 6.527466666666666,
      "grad_norm": 0.056055184453725815,
      "learning_rate": 9.203333333333334e-06,
      "loss": 0.0023,
      "step": 122390
    },
    {
      "epoch": 6.5280000000000005,
      "grad_norm": 0.14013801515102386,
      "learning_rate": 9.2e-06,
      "loss": 0.0022,
      "step": 122400
    },
    {
      "epoch": 6.528533333333334,
      "grad_norm": 0.2802780866622925,
      "learning_rate": 9.196666666666668e-06,
      "loss": 0.003,
      "step": 122410
    },
    {
      "epoch": 6.529066666666667,
      "grad_norm": 0.05605471506714821,
      "learning_rate": 9.193333333333334e-06,
      "loss": 0.0024,
      "step": 122420
    },
    {
      "epoch": 6.5296,
      "grad_norm": 0.19618988037109375,
      "learning_rate": 9.19e-06,
      "loss": 0.0023,
      "step": 122430
    },
    {
      "epoch": 6.530133333333334,
      "grad_norm": 0.08408429473638535,
      "learning_rate": 9.186666666666666e-06,
      "loss": 0.0029,
      "step": 122440
    },
    {
      "epoch": 6.530666666666667,
      "grad_norm": 0.1401394158601761,
      "learning_rate": 9.183333333333334e-06,
      "loss": 0.003,
      "step": 122450
    },
    {
      "epoch": 6.5312,
      "grad_norm": 0.11211232841014862,
      "learning_rate": 9.180000000000002e-06,
      "loss": 0.0018,
      "step": 122460
    },
    {
      "epoch": 6.531733333333333,
      "grad_norm": 0.2027318924665451,
      "learning_rate": 9.176666666666666e-06,
      "loss": 0.0024,
      "step": 122470
    },
    {
      "epoch": 6.532266666666667,
      "grad_norm": 0.02802724577486515,
      "learning_rate": 9.173333333333334e-06,
      "loss": 0.0023,
      "step": 122480
    },
    {
      "epoch": 6.5328,
      "grad_norm": 0.16816158592700958,
      "learning_rate": 9.17e-06,
      "loss": 0.0023,
      "step": 122490
    },
    {
      "epoch": 6.533333333333333,
      "grad_norm": 0.08408012241125107,
      "learning_rate": 9.166666666666666e-06,
      "loss": 0.003,
      "step": 122500
    },
    {
      "epoch": 6.5338666666666665,
      "grad_norm": 0.16816446185112,
      "learning_rate": 9.163333333333334e-06,
      "loss": 0.0025,
      "step": 122510
    },
    {
      "epoch": 6.5344,
      "grad_norm": 0.11210944503545761,
      "learning_rate": 9.16e-06,
      "loss": 0.0025,
      "step": 122520
    },
    {
      "epoch": 6.534933333333333,
      "grad_norm": 0.08407960832118988,
      "learning_rate": 9.156666666666668e-06,
      "loss": 0.0021,
      "step": 122530
    },
    {
      "epoch": 6.535466666666666,
      "grad_norm": 0.17481841146945953,
      "learning_rate": 9.153333333333333e-06,
      "loss": 0.0025,
      "step": 122540
    },
    {
      "epoch": 6.536,
      "grad_norm": 0.16816715896129608,
      "learning_rate": 9.15e-06,
      "loss": 0.0029,
      "step": 122550
    },
    {
      "epoch": 6.536533333333333,
      "grad_norm": 0.19619110226631165,
      "learning_rate": 9.146666666666667e-06,
      "loss": 0.0027,
      "step": 122560
    },
    {
      "epoch": 6.537066666666667,
      "grad_norm": 0.05605470761656761,
      "learning_rate": 9.143333333333334e-06,
      "loss": 0.0025,
      "step": 122570
    },
    {
      "epoch": 6.5376,
      "grad_norm": 0.224222794175148,
      "learning_rate": 9.14e-06,
      "loss": 0.0035,
      "step": 122580
    },
    {
      "epoch": 6.538133333333334,
      "grad_norm": 0.1486058533191681,
      "learning_rate": 9.136666666666667e-06,
      "loss": 0.0034,
      "step": 122590
    },
    {
      "epoch": 6.538666666666667,
      "grad_norm": 0.05605494976043701,
      "learning_rate": 9.133333333333335e-06,
      "loss": 0.0012,
      "step": 122600
    },
    {
      "epoch": 6.5392,
      "grad_norm": 0.3643576502799988,
      "learning_rate": 9.13e-06,
      "loss": 0.0019,
      "step": 122610
    },
    {
      "epoch": 6.539733333333333,
      "grad_norm": 0.11210688203573227,
      "learning_rate": 9.126666666666667e-06,
      "loss": 0.0026,
      "step": 122620
    },
    {
      "epoch": 6.540266666666667,
      "grad_norm": 0.19618624448776245,
      "learning_rate": 9.123333333333333e-06,
      "loss": 0.0027,
      "step": 122630
    },
    {
      "epoch": 6.5408,
      "grad_norm": 0.1401330530643463,
      "learning_rate": 9.12e-06,
      "loss": 0.0037,
      "step": 122640
    },
    {
      "epoch": 6.541333333333333,
      "grad_norm": 0.22421108186244965,
      "learning_rate": 9.116666666666667e-06,
      "loss": 0.002,
      "step": 122650
    },
    {
      "epoch": 6.5418666666666665,
      "grad_norm": 0.11210927367210388,
      "learning_rate": 9.113333333333333e-06,
      "loss": 0.002,
      "step": 122660
    },
    {
      "epoch": 6.5424,
      "grad_norm": 0.3363243341445923,
      "learning_rate": 9.110000000000001e-06,
      "loss": 0.0024,
      "step": 122670
    },
    {
      "epoch": 6.542933333333333,
      "grad_norm": 0.4203941226005554,
      "learning_rate": 9.106666666666667e-06,
      "loss": 0.0027,
      "step": 122680
    },
    {
      "epoch": 6.543466666666666,
      "grad_norm": 0.1121092438697815,
      "learning_rate": 9.103333333333333e-06,
      "loss": 0.0034,
      "step": 122690
    },
    {
      "epoch": 6.5440000000000005,
      "grad_norm": 0.1681615710258484,
      "learning_rate": 9.100000000000001e-06,
      "loss": 0.0034,
      "step": 122700
    },
    {
      "epoch": 6.544533333333334,
      "grad_norm": 0.16816264390945435,
      "learning_rate": 9.096666666666667e-06,
      "loss": 0.0028,
      "step": 122710
    },
    {
      "epoch": 6.545066666666667,
      "grad_norm": 0.19618752598762512,
      "learning_rate": 9.093333333333335e-06,
      "loss": 0.0021,
      "step": 122720
    },
    {
      "epoch": 6.5456,
      "grad_norm": 0.9562581181526184,
      "learning_rate": 9.09e-06,
      "loss": 0.0028,
      "step": 122730
    },
    {
      "epoch": 6.546133333333334,
      "grad_norm": 0.3643445670604706,
      "learning_rate": 9.086666666666667e-06,
      "loss": 0.0033,
      "step": 122740
    },
    {
      "epoch": 6.546666666666667,
      "grad_norm": 0.32751959562301636,
      "learning_rate": 9.083333333333333e-06,
      "loss": 0.0033,
      "step": 122750
    },
    {
      "epoch": 6.5472,
      "grad_norm": 0.08407730609178543,
      "learning_rate": 9.080000000000001e-06,
      "loss": 0.0025,
      "step": 122760
    },
    {
      "epoch": 6.547733333333333,
      "grad_norm": 0.028026700019836426,
      "learning_rate": 9.076666666666667e-06,
      "loss": 0.0019,
      "step": 122770
    },
    {
      "epoch": 6.548266666666667,
      "grad_norm": 0.16816407442092896,
      "learning_rate": 9.073333333333333e-06,
      "loss": 0.0025,
      "step": 122780
    },
    {
      "epoch": 6.5488,
      "grad_norm": 0.0840795636177063,
      "learning_rate": 9.070000000000001e-06,
      "loss": 0.0033,
      "step": 122790
    },
    {
      "epoch": 6.549333333333333,
      "grad_norm": 7.75590969226414e-09,
      "learning_rate": 9.066666666666667e-06,
      "loss": 0.0021,
      "step": 122800
    },
    {
      "epoch": 6.5498666666666665,
      "grad_norm": 0.14013081789016724,
      "learning_rate": 9.063333333333334e-06,
      "loss": 0.0019,
      "step": 122810
    },
    {
      "epoch": 6.5504,
      "grad_norm": 0.19421415030956268,
      "learning_rate": 9.06e-06,
      "loss": 0.0032,
      "step": 122820
    },
    {
      "epoch": 6.550933333333333,
      "grad_norm": 0.050651028752326965,
      "learning_rate": 9.056666666666667e-06,
      "loss": 0.003,
      "step": 122830
    },
    {
      "epoch": 6.551466666666666,
      "grad_norm": 0.08407764136791229,
      "learning_rate": 9.053333333333334e-06,
      "loss": 0.0032,
      "step": 122840
    },
    {
      "epoch": 6.552,
      "grad_norm": 0.14013119041919708,
      "learning_rate": 9.05e-06,
      "loss": 0.0024,
      "step": 122850
    },
    {
      "epoch": 6.552533333333333,
      "grad_norm": 0.19618253409862518,
      "learning_rate": 9.046666666666668e-06,
      "loss": 0.0023,
      "step": 122860
    },
    {
      "epoch": 6.553066666666667,
      "grad_norm": 0.028025172650814056,
      "learning_rate": 9.043333333333334e-06,
      "loss": 0.0032,
      "step": 122870
    },
    {
      "epoch": 6.5536,
      "grad_norm": 0.028024738654494286,
      "learning_rate": 9.04e-06,
      "loss": 0.002,
      "step": 122880
    },
    {
      "epoch": 6.554133333333334,
      "grad_norm": 0.028025394305586815,
      "learning_rate": 9.036666666666668e-06,
      "loss": 0.0036,
      "step": 122890
    },
    {
      "epoch": 6.554666666666667,
      "grad_norm": 0.05605272576212883,
      "learning_rate": 9.033333333333334e-06,
      "loss": 0.0028,
      "step": 122900
    },
    {
      "epoch": 6.5552,
      "grad_norm": 0.05605371668934822,
      "learning_rate": 9.030000000000002e-06,
      "loss": 0.0034,
      "step": 122910
    },
    {
      "epoch": 6.555733333333333,
      "grad_norm": 0.28025856614112854,
      "learning_rate": 9.026666666666666e-06,
      "loss": 0.002,
      "step": 122920
    },
    {
      "epoch": 6.556266666666667,
      "grad_norm": 0.02802491933107376,
      "learning_rate": 9.023333333333334e-06,
      "loss": 0.0023,
      "step": 122930
    },
    {
      "epoch": 6.5568,
      "grad_norm": 0.08407753705978394,
      "learning_rate": 9.02e-06,
      "loss": 0.0022,
      "step": 122940
    },
    {
      "epoch": 6.557333333333333,
      "grad_norm": 0.19618231058120728,
      "learning_rate": 9.016666666666668e-06,
      "loss": 0.0033,
      "step": 122950
    },
    {
      "epoch": 6.5578666666666665,
      "grad_norm": 0.05605075880885124,
      "learning_rate": 9.013333333333334e-06,
      "loss": 0.0019,
      "step": 122960
    },
    {
      "epoch": 6.5584,
      "grad_norm": 0.112106092274189,
      "learning_rate": 9.01e-06,
      "loss": 0.002,
      "step": 122970
    },
    {
      "epoch": 6.558933333333333,
      "grad_norm": 0.0560537651181221,
      "learning_rate": 9.006666666666668e-06,
      "loss": 0.0026,
      "step": 122980
    },
    {
      "epoch": 6.559466666666666,
      "grad_norm": 2.7176318972976787e-09,
      "learning_rate": 9.003333333333334e-06,
      "loss": 0.0025,
      "step": 122990
    },
    {
      "epoch": 6.5600000000000005,
      "grad_norm": 0.028026925399899483,
      "learning_rate": 9e-06,
      "loss": 0.0023,
      "step": 123000
    },
    {
      "epoch": 6.560533333333334,
      "grad_norm": 0.056052468717098236,
      "learning_rate": 8.996666666666666e-06,
      "loss": 0.0026,
      "step": 123010
    },
    {
      "epoch": 6.561066666666667,
      "grad_norm": 0.028025416657328606,
      "learning_rate": 8.993333333333334e-06,
      "loss": 0.0024,
      "step": 123020
    },
    {
      "epoch": 6.5616,
      "grad_norm": 0.056050654500722885,
      "learning_rate": 8.99e-06,
      "loss": 0.0022,
      "step": 123030
    },
    {
      "epoch": 6.562133333333334,
      "grad_norm": 0.05605249106884003,
      "learning_rate": 8.986666666666666e-06,
      "loss": 0.0023,
      "step": 123040
    },
    {
      "epoch": 6.562666666666667,
      "grad_norm": 0.3363121747970581,
      "learning_rate": 8.983333333333334e-06,
      "loss": 0.0033,
      "step": 123050
    },
    {
      "epoch": 6.5632,
      "grad_norm": 0.08407498151063919,
      "learning_rate": 8.98e-06,
      "loss": 0.004,
      "step": 123060
    },
    {
      "epoch": 6.563733333333333,
      "grad_norm": 0.6165668368339539,
      "learning_rate": 8.976666666666667e-06,
      "loss": 0.003,
      "step": 123070
    },
    {
      "epoch": 6.564266666666667,
      "grad_norm": 0.05605076253414154,
      "learning_rate": 8.973333333333334e-06,
      "loss": 0.0024,
      "step": 123080
    },
    {
      "epoch": 6.5648,
      "grad_norm": 0.02802567183971405,
      "learning_rate": 8.97e-06,
      "loss": 0.0029,
      "step": 123090
    },
    {
      "epoch": 6.565333333333333,
      "grad_norm": 0.16815868020057678,
      "learning_rate": 8.966666666666668e-06,
      "loss": 0.0022,
      "step": 123100
    },
    {
      "epoch": 6.5658666666666665,
      "grad_norm": 0.16815721988677979,
      "learning_rate": 8.963333333333333e-06,
      "loss": 0.0028,
      "step": 123110
    },
    {
      "epoch": 6.5664,
      "grad_norm": 0.02802550606429577,
      "learning_rate": 8.96e-06,
      "loss": 0.0028,
      "step": 123120
    },
    {
      "epoch": 6.566933333333333,
      "grad_norm": 0.2522273361682892,
      "learning_rate": 8.956666666666667e-06,
      "loss": 0.0019,
      "step": 123130
    },
    {
      "epoch": 6.567466666666666,
      "grad_norm": 2.7684841086283996e-09,
      "learning_rate": 8.953333333333335e-06,
      "loss": 0.0024,
      "step": 123140
    },
    {
      "epoch": 6.568,
      "grad_norm": 0.08407987654209137,
      "learning_rate": 8.95e-06,
      "loss": 0.002,
      "step": 123150
    },
    {
      "epoch": 6.568533333333333,
      "grad_norm": 0.08408267050981522,
      "learning_rate": 8.946666666666667e-06,
      "loss": 0.0025,
      "step": 123160
    },
    {
      "epoch": 6.569066666666667,
      "grad_norm": 0.2242111712694168,
      "learning_rate": 8.943333333333335e-06,
      "loss": 0.0021,
      "step": 123170
    },
    {
      "epoch": 6.5696,
      "grad_norm": 0.5044460892677307,
      "learning_rate": 8.939999999999999e-06,
      "loss": 0.0019,
      "step": 123180
    },
    {
      "epoch": 6.570133333333334,
      "grad_norm": 0.05605243891477585,
      "learning_rate": 8.936666666666667e-06,
      "loss": 0.002,
      "step": 123190
    },
    {
      "epoch": 6.570666666666667,
      "grad_norm": 0.19619400799274445,
      "learning_rate": 8.933333333333333e-06,
      "loss": 0.0038,
      "step": 123200
    },
    {
      "epoch": 6.5712,
      "grad_norm": 0.1401359587907791,
      "learning_rate": 8.930000000000001e-06,
      "loss": 0.0029,
      "step": 123210
    },
    {
      "epoch": 6.571733333333333,
      "grad_norm": 0.056051354855298996,
      "learning_rate": 8.926666666666667e-06,
      "loss": 0.0028,
      "step": 123220
    },
    {
      "epoch": 6.572266666666667,
      "grad_norm": 8.015410557504765e-09,
      "learning_rate": 8.923333333333333e-06,
      "loss": 0.0031,
      "step": 123230
    },
    {
      "epoch": 6.5728,
      "grad_norm": 0.11210238933563232,
      "learning_rate": 8.920000000000001e-06,
      "loss": 0.0018,
      "step": 123240
    },
    {
      "epoch": 6.573333333333333,
      "grad_norm": 0.02802559733390808,
      "learning_rate": 8.916666666666667e-06,
      "loss": 0.0037,
      "step": 123250
    },
    {
      "epoch": 6.5738666666666665,
      "grad_norm": 0.08407901227474213,
      "learning_rate": 8.913333333333333e-06,
      "loss": 0.0028,
      "step": 123260
    },
    {
      "epoch": 6.5744,
      "grad_norm": 0.19618697464466095,
      "learning_rate": 8.910000000000001e-06,
      "loss": 0.0029,
      "step": 123270
    },
    {
      "epoch": 6.574933333333333,
      "grad_norm": 2.542290156171134e-09,
      "learning_rate": 8.906666666666667e-06,
      "loss": 0.0029,
      "step": 123280
    },
    {
      "epoch": 6.575466666666666,
      "grad_norm": 0.140122652053833,
      "learning_rate": 8.903333333333335e-06,
      "loss": 0.0024,
      "step": 123290
    },
    {
      "epoch": 6.576,
      "grad_norm": 0.11210181564092636,
      "learning_rate": 8.9e-06,
      "loss": 0.0025,
      "step": 123300
    },
    {
      "epoch": 6.576533333333334,
      "grad_norm": 0.386260062456131,
      "learning_rate": 8.896666666666667e-06,
      "loss": 0.0031,
      "step": 123310
    },
    {
      "epoch": 6.577066666666667,
      "grad_norm": 0.19617579877376556,
      "learning_rate": 8.893333333333333e-06,
      "loss": 0.0022,
      "step": 123320
    },
    {
      "epoch": 6.5776,
      "grad_norm": 0.16815271973609924,
      "learning_rate": 8.890000000000001e-06,
      "loss": 0.0028,
      "step": 123330
    },
    {
      "epoch": 6.578133333333334,
      "grad_norm": 0.14012610912322998,
      "learning_rate": 8.886666666666667e-06,
      "loss": 0.0026,
      "step": 123340
    },
    {
      "epoch": 6.578666666666667,
      "grad_norm": 0.11210259795188904,
      "learning_rate": 8.883333333333334e-06,
      "loss": 0.0019,
      "step": 123350
    },
    {
      "epoch": 6.5792,
      "grad_norm": 0.14012789726257324,
      "learning_rate": 8.880000000000001e-06,
      "loss": 0.0017,
      "step": 123360
    },
    {
      "epoch": 6.579733333333333,
      "grad_norm": 0.05605103448033333,
      "learning_rate": 8.876666666666666e-06,
      "loss": 0.0033,
      "step": 123370
    },
    {
      "epoch": 6.580266666666667,
      "grad_norm": 0.19617436826229095,
      "learning_rate": 8.873333333333334e-06,
      "loss": 0.0025,
      "step": 123380
    },
    {
      "epoch": 6.5808,
      "grad_norm": 0.39235061407089233,
      "learning_rate": 8.87e-06,
      "loss": 0.0032,
      "step": 123390
    },
    {
      "epoch": 6.581333333333333,
      "grad_norm": 2.0917645393581097e-09,
      "learning_rate": 8.866666666666668e-06,
      "loss": 0.0019,
      "step": 123400
    },
    {
      "epoch": 6.5818666666666665,
      "grad_norm": 0.11210793256759644,
      "learning_rate": 8.863333333333334e-06,
      "loss": 0.0018,
      "step": 123410
    },
    {
      "epoch": 6.5824,
      "grad_norm": 0.16815948486328125,
      "learning_rate": 8.86e-06,
      "loss": 0.0016,
      "step": 123420
    },
    {
      "epoch": 6.582933333333333,
      "grad_norm": 0.056049834936857224,
      "learning_rate": 8.856666666666668e-06,
      "loss": 0.0035,
      "step": 123430
    },
    {
      "epoch": 6.583466666666666,
      "grad_norm": 0.11210061609745026,
      "learning_rate": 8.853333333333334e-06,
      "loss": 0.002,
      "step": 123440
    },
    {
      "epoch": 6.584,
      "grad_norm": 0.02802550606429577,
      "learning_rate": 8.85e-06,
      "loss": 0.0015,
      "step": 123450
    },
    {
      "epoch": 6.584533333333333,
      "grad_norm": 0.1401289701461792,
      "learning_rate": 8.846666666666668e-06,
      "loss": 0.0027,
      "step": 123460
    },
    {
      "epoch": 6.585066666666666,
      "grad_norm": 0.028025051578879356,
      "learning_rate": 8.843333333333334e-06,
      "loss": 0.0028,
      "step": 123470
    },
    {
      "epoch": 6.5856,
      "grad_norm": 0.1961740404367447,
      "learning_rate": 8.840000000000002e-06,
      "loss": 0.003,
      "step": 123480
    },
    {
      "epoch": 6.586133333333334,
      "grad_norm": 0.14012393355369568,
      "learning_rate": 8.836666666666666e-06,
      "loss": 0.0044,
      "step": 123490
    },
    {
      "epoch": 6.586666666666667,
      "grad_norm": 0.056051261723041534,
      "learning_rate": 8.833333333333334e-06,
      "loss": 0.0025,
      "step": 123500
    },
    {
      "epoch": 6.5872,
      "grad_norm": 0.028024788945913315,
      "learning_rate": 8.83e-06,
      "loss": 0.0022,
      "step": 123510
    },
    {
      "epoch": 6.587733333333333,
      "grad_norm": 0.05605021491646767,
      "learning_rate": 8.826666666666666e-06,
      "loss": 0.0018,
      "step": 123520
    },
    {
      "epoch": 6.588266666666667,
      "grad_norm": 0.0840756818652153,
      "learning_rate": 8.823333333333334e-06,
      "loss": 0.0018,
      "step": 123530
    },
    {
      "epoch": 6.5888,
      "grad_norm": 0.02803201973438263,
      "learning_rate": 8.82e-06,
      "loss": 0.0028,
      "step": 123540
    },
    {
      "epoch": 6.589333333333333,
      "grad_norm": 0.05605136603116989,
      "learning_rate": 8.816666666666668e-06,
      "loss": 0.0022,
      "step": 123550
    },
    {
      "epoch": 6.5898666666666665,
      "grad_norm": 0.22420164942741394,
      "learning_rate": 8.813333333333333e-06,
      "loss": 0.0019,
      "step": 123560
    },
    {
      "epoch": 6.5904,
      "grad_norm": 0.08326848596334457,
      "learning_rate": 8.81e-06,
      "loss": 0.0026,
      "step": 123570
    },
    {
      "epoch": 6.590933333333333,
      "grad_norm": 0.19617538154125214,
      "learning_rate": 8.806666666666666e-06,
      "loss": 0.0023,
      "step": 123580
    },
    {
      "epoch": 6.591466666666666,
      "grad_norm": 0.056050460785627365,
      "learning_rate": 8.803333333333334e-06,
      "loss": 0.0025,
      "step": 123590
    },
    {
      "epoch": 6.592,
      "grad_norm": 0.33629554510116577,
      "learning_rate": 8.8e-06,
      "loss": 0.0019,
      "step": 123600
    },
    {
      "epoch": 6.592533333333334,
      "grad_norm": 0.1401246339082718,
      "learning_rate": 8.796666666666667e-06,
      "loss": 0.0023,
      "step": 123610
    },
    {
      "epoch": 6.593066666666667,
      "grad_norm": 0.19617860019207,
      "learning_rate": 8.793333333333334e-06,
      "loss": 0.002,
      "step": 123620
    },
    {
      "epoch": 6.5936,
      "grad_norm": 0.05605003237724304,
      "learning_rate": 8.79e-06,
      "loss": 0.004,
      "step": 123630
    },
    {
      "epoch": 6.594133333333334,
      "grad_norm": 0.08407417684793472,
      "learning_rate": 8.786666666666667e-06,
      "loss": 0.0034,
      "step": 123640
    },
    {
      "epoch": 6.594666666666667,
      "grad_norm": 0.0840744897723198,
      "learning_rate": 8.783333333333335e-06,
      "loss": 0.0031,
      "step": 123650
    },
    {
      "epoch": 6.5952,
      "grad_norm": 0.08407693356275558,
      "learning_rate": 8.78e-06,
      "loss": 0.0028,
      "step": 123660
    },
    {
      "epoch": 6.5957333333333334,
      "grad_norm": 0.11210275441408157,
      "learning_rate": 8.776666666666668e-06,
      "loss": 0.002,
      "step": 123670
    },
    {
      "epoch": 6.596266666666667,
      "grad_norm": 0.11210247874259949,
      "learning_rate": 8.773333333333333e-06,
      "loss": 0.0034,
      "step": 123680
    },
    {
      "epoch": 6.5968,
      "grad_norm": 0.14012283086776733,
      "learning_rate": 8.77e-06,
      "loss": 0.0021,
      "step": 123690
    },
    {
      "epoch": 6.597333333333333,
      "grad_norm": 0.19617381691932678,
      "learning_rate": 8.766666666666667e-06,
      "loss": 0.0028,
      "step": 123700
    },
    {
      "epoch": 6.5978666666666665,
      "grad_norm": 0.2242008000612259,
      "learning_rate": 8.763333333333333e-06,
      "loss": 0.0028,
      "step": 123710
    },
    {
      "epoch": 6.5984,
      "grad_norm": 0.08407297730445862,
      "learning_rate": 8.76e-06,
      "loss": 0.0021,
      "step": 123720
    },
    {
      "epoch": 6.598933333333333,
      "grad_norm": 0.28025490045547485,
      "learning_rate": 8.756666666666667e-06,
      "loss": 0.0027,
      "step": 123730
    },
    {
      "epoch": 6.599466666666666,
      "grad_norm": 0.08407732099294662,
      "learning_rate": 8.753333333333335e-06,
      "loss": 0.0029,
      "step": 123740
    },
    {
      "epoch": 6.6,
      "grad_norm": 0.028025081381201744,
      "learning_rate": 8.75e-06,
      "loss": 0.0027,
      "step": 123750
    },
    {
      "epoch": 6.600533333333333,
      "grad_norm": 0.11210066825151443,
      "learning_rate": 8.746666666666667e-06,
      "loss": 0.0022,
      "step": 123760
    },
    {
      "epoch": 6.601066666666666,
      "grad_norm": 0.16815201938152313,
      "learning_rate": 8.743333333333333e-06,
      "loss": 0.0037,
      "step": 123770
    },
    {
      "epoch": 6.6016,
      "grad_norm": 0.05605020746588707,
      "learning_rate": 8.740000000000001e-06,
      "loss": 0.0015,
      "step": 123780
    },
    {
      "epoch": 6.602133333333334,
      "grad_norm": 0.05604874715209007,
      "learning_rate": 8.736666666666667e-06,
      "loss": 0.0023,
      "step": 123790
    },
    {
      "epoch": 6.602666666666667,
      "grad_norm": 0.28025656938552856,
      "learning_rate": 8.733333333333333e-06,
      "loss": 0.0028,
      "step": 123800
    },
    {
      "epoch": 6.6032,
      "grad_norm": 0.08407890051603317,
      "learning_rate": 8.730000000000001e-06,
      "loss": 0.0028,
      "step": 123810
    },
    {
      "epoch": 6.6037333333333335,
      "grad_norm": 0.02802509255707264,
      "learning_rate": 8.726666666666667e-06,
      "loss": 0.0034,
      "step": 123820
    },
    {
      "epoch": 6.604266666666667,
      "grad_norm": 0.028025107458233833,
      "learning_rate": 8.723333333333333e-06,
      "loss": 0.0019,
      "step": 123830
    },
    {
      "epoch": 6.6048,
      "grad_norm": 0.07702461630105972,
      "learning_rate": 8.720000000000001e-06,
      "loss": 0.0038,
      "step": 123840
    },
    {
      "epoch": 6.605333333333333,
      "grad_norm": 0.16815021634101868,
      "learning_rate": 8.716666666666667e-06,
      "loss": 0.0027,
      "step": 123850
    },
    {
      "epoch": 6.6058666666666666,
      "grad_norm": 0.05605120211839676,
      "learning_rate": 8.713333333333333e-06,
      "loss": 0.0032,
      "step": 123860
    },
    {
      "epoch": 6.6064,
      "grad_norm": 0.05605112388730049,
      "learning_rate": 8.71e-06,
      "loss": 0.0023,
      "step": 123870
    },
    {
      "epoch": 6.606933333333333,
      "grad_norm": 0.028025206178426743,
      "learning_rate": 8.706666666666667e-06,
      "loss": 0.0019,
      "step": 123880
    },
    {
      "epoch": 6.607466666666666,
      "grad_norm": 0.47643083333969116,
      "learning_rate": 8.703333333333334e-06,
      "loss": 0.0025,
      "step": 123890
    },
    {
      "epoch": 6.608,
      "grad_norm": 0.11210685968399048,
      "learning_rate": 8.7e-06,
      "loss": 0.0048,
      "step": 123900
    },
    {
      "epoch": 6.608533333333334,
      "grad_norm": 0.2242112010717392,
      "learning_rate": 8.696666666666668e-06,
      "loss": 0.0036,
      "step": 123910
    },
    {
      "epoch": 6.609066666666667,
      "grad_norm": 0.05605240538716316,
      "learning_rate": 8.693333333333334e-06,
      "loss": 0.0019,
      "step": 123920
    },
    {
      "epoch": 6.6096,
      "grad_norm": 0.28026843070983887,
      "learning_rate": 8.690000000000002e-06,
      "loss": 0.0028,
      "step": 123930
    },
    {
      "epoch": 6.610133333333334,
      "grad_norm": 0.224464550614357,
      "learning_rate": 8.686666666666666e-06,
      "loss": 0.0025,
      "step": 123940
    },
    {
      "epoch": 6.610666666666667,
      "grad_norm": 0.16814784705638885,
      "learning_rate": 8.683333333333334e-06,
      "loss": 0.0014,
      "step": 123950
    },
    {
      "epoch": 6.6112,
      "grad_norm": 0.2522307336330414,
      "learning_rate": 8.68e-06,
      "loss": 0.0025,
      "step": 123960
    },
    {
      "epoch": 6.6117333333333335,
      "grad_norm": 0.08408012241125107,
      "learning_rate": 8.676666666666668e-06,
      "loss": 0.0022,
      "step": 123970
    },
    {
      "epoch": 6.612266666666667,
      "grad_norm": 0.22421427071094513,
      "learning_rate": 8.673333333333334e-06,
      "loss": 0.0032,
      "step": 123980
    },
    {
      "epoch": 6.6128,
      "grad_norm": 0.11210201680660248,
      "learning_rate": 8.67e-06,
      "loss": 0.0027,
      "step": 123990
    },
    {
      "epoch": 6.613333333333333,
      "grad_norm": 0.028025684878230095,
      "learning_rate": 8.666666666666668e-06,
      "loss": 0.0019,
      "step": 124000
    },
    {
      "epoch": 6.613866666666667,
      "grad_norm": 0.25222763419151306,
      "learning_rate": 8.663333333333334e-06,
      "loss": 0.003,
      "step": 124010
    },
    {
      "epoch": 6.6144,
      "grad_norm": 0.08407525718212128,
      "learning_rate": 8.66e-06,
      "loss": 0.0024,
      "step": 124020
    },
    {
      "epoch": 6.614933333333333,
      "grad_norm": 0.05605006217956543,
      "learning_rate": 8.656666666666668e-06,
      "loss": 0.0015,
      "step": 124030
    },
    {
      "epoch": 6.615466666666666,
      "grad_norm": 0.1681593507528305,
      "learning_rate": 8.653333333333334e-06,
      "loss": 0.0019,
      "step": 124040
    },
    {
      "epoch": 6.616,
      "grad_norm": 0.252240926027298,
      "learning_rate": 8.65e-06,
      "loss": 0.0031,
      "step": 124050
    },
    {
      "epoch": 6.616533333333333,
      "grad_norm": 0.6445550322532654,
      "learning_rate": 8.646666666666666e-06,
      "loss": 0.0017,
      "step": 124060
    },
    {
      "epoch": 6.617066666666666,
      "grad_norm": 0.11210193485021591,
      "learning_rate": 8.643333333333334e-06,
      "loss": 0.0022,
      "step": 124070
    },
    {
      "epoch": 6.6176,
      "grad_norm": 0.08407891541719437,
      "learning_rate": 8.64e-06,
      "loss": 0.0045,
      "step": 124080
    },
    {
      "epoch": 6.618133333333334,
      "grad_norm": 0.028025247156620026,
      "learning_rate": 8.636666666666666e-06,
      "loss": 0.0028,
      "step": 124090
    },
    {
      "epoch": 6.618666666666667,
      "grad_norm": 3.988064545268344e-09,
      "learning_rate": 8.633333333333334e-06,
      "loss": 0.0027,
      "step": 124100
    },
    {
      "epoch": 6.6192,
      "grad_norm": 0.11210222542285919,
      "learning_rate": 8.63e-06,
      "loss": 0.0016,
      "step": 124110
    },
    {
      "epoch": 6.6197333333333335,
      "grad_norm": 0.3363110423088074,
      "learning_rate": 8.626666666666668e-06,
      "loss": 0.0022,
      "step": 124120
    },
    {
      "epoch": 6.620266666666667,
      "grad_norm": 0.08407610654830933,
      "learning_rate": 8.623333333333333e-06,
      "loss": 0.0021,
      "step": 124130
    },
    {
      "epoch": 6.6208,
      "grad_norm": 4.079966142711555e-09,
      "learning_rate": 8.62e-06,
      "loss": 0.0017,
      "step": 124140
    },
    {
      "epoch": 6.621333333333333,
      "grad_norm": 0.22420231997966766,
      "learning_rate": 8.616666666666667e-06,
      "loss": 0.0018,
      "step": 124150
    },
    {
      "epoch": 6.621866666666667,
      "grad_norm": 0.028025593608617783,
      "learning_rate": 8.613333333333334e-06,
      "loss": 0.0028,
      "step": 124160
    },
    {
      "epoch": 6.6224,
      "grad_norm": 0.1681521087884903,
      "learning_rate": 8.61e-06,
      "loss": 0.0028,
      "step": 124170
    },
    {
      "epoch": 6.622933333333333,
      "grad_norm": 0.14012673497200012,
      "learning_rate": 8.606666666666667e-06,
      "loss": 0.003,
      "step": 124180
    },
    {
      "epoch": 6.623466666666666,
      "grad_norm": 0.14012816548347473,
      "learning_rate": 8.603333333333335e-06,
      "loss": 0.0037,
      "step": 124190
    },
    {
      "epoch": 6.624,
      "grad_norm": 0.028024546802043915,
      "learning_rate": 8.599999999999999e-06,
      "loss": 0.0019,
      "step": 124200
    },
    {
      "epoch": 6.624533333333334,
      "grad_norm": 0.1681516021490097,
      "learning_rate": 8.596666666666667e-06,
      "loss": 0.0027,
      "step": 124210
    },
    {
      "epoch": 6.625066666666667,
      "grad_norm": 0.1681492030620575,
      "learning_rate": 8.593333333333335e-06,
      "loss": 0.0018,
      "step": 124220
    },
    {
      "epoch": 6.6256,
      "grad_norm": 0.028024572879076004,
      "learning_rate": 8.59e-06,
      "loss": 0.0029,
      "step": 124230
    },
    {
      "epoch": 6.626133333333334,
      "grad_norm": 0.14012764394283295,
      "learning_rate": 8.586666666666667e-06,
      "loss": 0.0021,
      "step": 124240
    },
    {
      "epoch": 6.626666666666667,
      "grad_norm": 0.11210093647241592,
      "learning_rate": 8.583333333333333e-06,
      "loss": 0.0024,
      "step": 124250
    },
    {
      "epoch": 6.6272,
      "grad_norm": 0.14012639224529266,
      "learning_rate": 8.580000000000001e-06,
      "loss": 0.0029,
      "step": 124260
    },
    {
      "epoch": 6.6277333333333335,
      "grad_norm": 0.4095386266708374,
      "learning_rate": 8.576666666666667e-06,
      "loss": 0.002,
      "step": 124270
    },
    {
      "epoch": 6.628266666666667,
      "grad_norm": 0.14012588560581207,
      "learning_rate": 8.573333333333333e-06,
      "loss": 0.0026,
      "step": 124280
    },
    {
      "epoch": 6.6288,
      "grad_norm": 0.05604948475956917,
      "learning_rate": 8.570000000000001e-06,
      "loss": 0.0016,
      "step": 124290
    },
    {
      "epoch": 6.629333333333333,
      "grad_norm": 0.19617019593715668,
      "learning_rate": 8.566666666666667e-06,
      "loss": 0.0019,
      "step": 124300
    },
    {
      "epoch": 6.629866666666667,
      "grad_norm": 0.02802431955933571,
      "learning_rate": 8.563333333333335e-06,
      "loss": 0.0019,
      "step": 124310
    },
    {
      "epoch": 6.6304,
      "grad_norm": 0.056050170212984085,
      "learning_rate": 8.56e-06,
      "loss": 0.0024,
      "step": 124320
    },
    {
      "epoch": 6.630933333333333,
      "grad_norm": 0.02802441455423832,
      "learning_rate": 8.556666666666667e-06,
      "loss": 0.0022,
      "step": 124330
    },
    {
      "epoch": 6.631466666666666,
      "grad_norm": 0.14012295007705688,
      "learning_rate": 8.553333333333333e-06,
      "loss": 0.003,
      "step": 124340
    },
    {
      "epoch": 6.632,
      "grad_norm": 0.05605040490627289,
      "learning_rate": 8.550000000000001e-06,
      "loss": 0.0024,
      "step": 124350
    },
    {
      "epoch": 6.632533333333333,
      "grad_norm": 0.02802533097565174,
      "learning_rate": 8.546666666666667e-06,
      "loss": 0.0021,
      "step": 124360
    },
    {
      "epoch": 6.633066666666666,
      "grad_norm": 0.08407663553953171,
      "learning_rate": 8.543333333333333e-06,
      "loss": 0.004,
      "step": 124370
    },
    {
      "epoch": 6.6336,
      "grad_norm": 0.14012335240840912,
      "learning_rate": 8.540000000000001e-06,
      "loss": 0.0033,
      "step": 124380
    },
    {
      "epoch": 6.634133333333334,
      "grad_norm": 0.028024934232234955,
      "learning_rate": 8.536666666666666e-06,
      "loss": 0.0029,
      "step": 124390
    },
    {
      "epoch": 6.634666666666667,
      "grad_norm": 0.0840737521648407,
      "learning_rate": 8.533333333333334e-06,
      "loss": 0.002,
      "step": 124400
    },
    {
      "epoch": 6.6352,
      "grad_norm": 0.056049685925245285,
      "learning_rate": 8.53e-06,
      "loss": 0.0022,
      "step": 124410
    },
    {
      "epoch": 6.6357333333333335,
      "grad_norm": 0.2118164598941803,
      "learning_rate": 8.526666666666667e-06,
      "loss": 0.0013,
      "step": 124420
    },
    {
      "epoch": 6.636266666666667,
      "grad_norm": 0.05605330690741539,
      "learning_rate": 8.523333333333334e-06,
      "loss": 0.0024,
      "step": 124430
    },
    {
      "epoch": 6.6368,
      "grad_norm": 0.14013177156448364,
      "learning_rate": 8.52e-06,
      "loss": 0.003,
      "step": 124440
    },
    {
      "epoch": 6.637333333333333,
      "grad_norm": 0.08407702296972275,
      "learning_rate": 8.516666666666668e-06,
      "loss": 0.0023,
      "step": 124450
    },
    {
      "epoch": 6.637866666666667,
      "grad_norm": 0.1681499481201172,
      "learning_rate": 8.513333333333334e-06,
      "loss": 0.0034,
      "step": 124460
    },
    {
      "epoch": 6.6384,
      "grad_norm": 0.25222209095954895,
      "learning_rate": 8.51e-06,
      "loss": 0.0028,
      "step": 124470
    },
    {
      "epoch": 6.638933333333333,
      "grad_norm": 0.05604955926537514,
      "learning_rate": 8.506666666666668e-06,
      "loss": 0.0033,
      "step": 124480
    },
    {
      "epoch": 6.639466666666666,
      "grad_norm": 0.11210031062364578,
      "learning_rate": 8.503333333333334e-06,
      "loss": 0.0023,
      "step": 124490
    },
    {
      "epoch": 6.64,
      "grad_norm": 0.14012973010540009,
      "learning_rate": 8.500000000000002e-06,
      "loss": 0.0017,
      "step": 124500
    },
    {
      "epoch": 6.640533333333333,
      "grad_norm": 0.30828937888145447,
      "learning_rate": 8.496666666666666e-06,
      "loss": 0.0027,
      "step": 124510
    },
    {
      "epoch": 6.641066666666667,
      "grad_norm": 0.2027224451303482,
      "learning_rate": 8.493333333333334e-06,
      "loss": 0.0036,
      "step": 124520
    },
    {
      "epoch": 6.6416,
      "grad_norm": 0.08407843858003616,
      "learning_rate": 8.49e-06,
      "loss": 0.0021,
      "step": 124530
    },
    {
      "epoch": 6.642133333333334,
      "grad_norm": 0.3363083004951477,
      "learning_rate": 8.486666666666668e-06,
      "loss": 0.0035,
      "step": 124540
    },
    {
      "epoch": 6.642666666666667,
      "grad_norm": 0.0280256737023592,
      "learning_rate": 8.483333333333334e-06,
      "loss": 0.0019,
      "step": 124550
    },
    {
      "epoch": 6.6432,
      "grad_norm": 0.30827027559280396,
      "learning_rate": 8.48e-06,
      "loss": 0.0023,
      "step": 124560
    },
    {
      "epoch": 6.6437333333333335,
      "grad_norm": 0.16815194487571716,
      "learning_rate": 8.476666666666668e-06,
      "loss": 0.0026,
      "step": 124570
    },
    {
      "epoch": 6.644266666666667,
      "grad_norm": 0.336296945810318,
      "learning_rate": 8.473333333333332e-06,
      "loss": 0.0027,
      "step": 124580
    },
    {
      "epoch": 6.6448,
      "grad_norm": 0.061832886189222336,
      "learning_rate": 8.47e-06,
      "loss": 0.002,
      "step": 124590
    },
    {
      "epoch": 6.645333333333333,
      "grad_norm": 0.36432668566703796,
      "learning_rate": 8.466666666666666e-06,
      "loss": 0.0019,
      "step": 124600
    },
    {
      "epoch": 6.645866666666667,
      "grad_norm": 0.36432746052742004,
      "learning_rate": 8.463333333333334e-06,
      "loss": 0.0034,
      "step": 124610
    },
    {
      "epoch": 6.6464,
      "grad_norm": 0.11209709942340851,
      "learning_rate": 8.46e-06,
      "loss": 0.0034,
      "step": 124620
    },
    {
      "epoch": 6.646933333333333,
      "grad_norm": 0.1401267647743225,
      "learning_rate": 8.456666666666666e-06,
      "loss": 0.0042,
      "step": 124630
    },
    {
      "epoch": 6.647466666666666,
      "grad_norm": 0.22464126348495483,
      "learning_rate": 8.453333333333334e-06,
      "loss": 0.003,
      "step": 124640
    },
    {
      "epoch": 6.648,
      "grad_norm": 0.028025390580296516,
      "learning_rate": 8.45e-06,
      "loss": 0.0028,
      "step": 124650
    },
    {
      "epoch": 6.648533333333333,
      "grad_norm": 0.028024448081851006,
      "learning_rate": 8.446666666666667e-06,
      "loss": 0.0036,
      "step": 124660
    },
    {
      "epoch": 6.649066666666666,
      "grad_norm": 0.05604817718267441,
      "learning_rate": 8.443333333333334e-06,
      "loss": 0.002,
      "step": 124670
    },
    {
      "epoch": 6.6495999999999995,
      "grad_norm": 0.08407574146986008,
      "learning_rate": 8.44e-06,
      "loss": 0.0031,
      "step": 124680
    },
    {
      "epoch": 6.650133333333334,
      "grad_norm": 0.22420287132263184,
      "learning_rate": 8.436666666666668e-06,
      "loss": 0.0031,
      "step": 124690
    },
    {
      "epoch": 6.650666666666667,
      "grad_norm": 0.08407147228717804,
      "learning_rate": 8.433333333333333e-06,
      "loss": 0.0018,
      "step": 124700
    },
    {
      "epoch": 6.6512,
      "grad_norm": 0.140117347240448,
      "learning_rate": 8.43e-06,
      "loss": 0.0035,
      "step": 124710
    },
    {
      "epoch": 6.6517333333333335,
      "grad_norm": 0.056049495935440063,
      "learning_rate": 8.426666666666667e-06,
      "loss": 0.0023,
      "step": 124720
    },
    {
      "epoch": 6.652266666666667,
      "grad_norm": 0.19617201387882233,
      "learning_rate": 8.423333333333333e-06,
      "loss": 0.0038,
      "step": 124730
    },
    {
      "epoch": 6.6528,
      "grad_norm": 0.19617103040218353,
      "learning_rate": 8.42e-06,
      "loss": 0.0022,
      "step": 124740
    },
    {
      "epoch": 6.653333333333333,
      "grad_norm": 0.1121014803647995,
      "learning_rate": 8.416666666666667e-06,
      "loss": 0.0028,
      "step": 124750
    },
    {
      "epoch": 6.653866666666667,
      "grad_norm": 0.1593492478132248,
      "learning_rate": 8.413333333333335e-06,
      "loss": 0.0029,
      "step": 124760
    },
    {
      "epoch": 6.6544,
      "grad_norm": 0.056048665195703506,
      "learning_rate": 8.409999999999999e-06,
      "loss": 0.0029,
      "step": 124770
    },
    {
      "epoch": 6.654933333333333,
      "grad_norm": 0.44837644696235657,
      "learning_rate": 8.406666666666667e-06,
      "loss": 0.0023,
      "step": 124780
    },
    {
      "epoch": 6.655466666666666,
      "grad_norm": 0.22419223189353943,
      "learning_rate": 8.403333333333333e-06,
      "loss": 0.0024,
      "step": 124790
    },
    {
      "epoch": 6.656,
      "grad_norm": 2.2140471678255835e-09,
      "learning_rate": 8.400000000000001e-06,
      "loss": 0.0023,
      "step": 124800
    },
    {
      "epoch": 6.656533333333333,
      "grad_norm": 0.05604761838912964,
      "learning_rate": 8.396666666666667e-06,
      "loss": 0.0023,
      "step": 124810
    },
    {
      "epoch": 6.657066666666667,
      "grad_norm": 0.11209489405155182,
      "learning_rate": 8.393333333333333e-06,
      "loss": 0.0029,
      "step": 124820
    },
    {
      "epoch": 6.6576,
      "grad_norm": 0.0840703696012497,
      "learning_rate": 8.390000000000001e-06,
      "loss": 0.0021,
      "step": 124830
    },
    {
      "epoch": 6.658133333333334,
      "grad_norm": 0.16814032196998596,
      "learning_rate": 8.386666666666667e-06,
      "loss": 0.0038,
      "step": 124840
    },
    {
      "epoch": 6.658666666666667,
      "grad_norm": 0.11209515482187271,
      "learning_rate": 8.383333333333333e-06,
      "loss": 0.0027,
      "step": 124850
    },
    {
      "epoch": 6.6592,
      "grad_norm": 0.1401182860136032,
      "learning_rate": 8.380000000000001e-06,
      "loss": 0.0018,
      "step": 124860
    },
    {
      "epoch": 6.6597333333333335,
      "grad_norm": 0.14011725783348083,
      "learning_rate": 8.376666666666667e-06,
      "loss": 0.0027,
      "step": 124870
    },
    {
      "epoch": 6.660266666666667,
      "grad_norm": 0.08407167345285416,
      "learning_rate": 8.373333333333335e-06,
      "loss": 0.0033,
      "step": 124880
    },
    {
      "epoch": 6.6608,
      "grad_norm": 0.11209945380687714,
      "learning_rate": 8.37e-06,
      "loss": 0.0028,
      "step": 124890
    },
    {
      "epoch": 6.661333333333333,
      "grad_norm": 0.16814272105693817,
      "learning_rate": 8.366666666666667e-06,
      "loss": 0.0016,
      "step": 124900
    },
    {
      "epoch": 6.661866666666667,
      "grad_norm": 0.140121728181839,
      "learning_rate": 8.363333333333333e-06,
      "loss": 0.0031,
      "step": 124910
    },
    {
      "epoch": 6.6624,
      "grad_norm": 3.790426639227462e-09,
      "learning_rate": 8.36e-06,
      "loss": 0.0025,
      "step": 124920
    },
    {
      "epoch": 6.662933333333333,
      "grad_norm": 0.283567875623703,
      "learning_rate": 8.356666666666667e-06,
      "loss": 0.0028,
      "step": 124930
    },
    {
      "epoch": 6.663466666666666,
      "grad_norm": 0.08407449722290039,
      "learning_rate": 8.353333333333334e-06,
      "loss": 0.0019,
      "step": 124940
    },
    {
      "epoch": 6.664,
      "grad_norm": 0.4203735888004303,
      "learning_rate": 8.350000000000001e-06,
      "loss": 0.0022,
      "step": 124950
    },
    {
      "epoch": 6.664533333333333,
      "grad_norm": 0.2802431881427765,
      "learning_rate": 8.346666666666666e-06,
      "loss": 0.0035,
      "step": 124960
    },
    {
      "epoch": 6.665066666666666,
      "grad_norm": 0.028023960068821907,
      "learning_rate": 8.343333333333334e-06,
      "loss": 0.0022,
      "step": 124970
    },
    {
      "epoch": 6.6655999999999995,
      "grad_norm": 0.196170374751091,
      "learning_rate": 8.34e-06,
      "loss": 0.0028,
      "step": 124980
    },
    {
      "epoch": 6.666133333333334,
      "grad_norm": 0.08407163619995117,
      "learning_rate": 8.336666666666668e-06,
      "loss": 0.0021,
      "step": 124990
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 0.336302250623703,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.003,
      "step": 125000
    },
    {
      "epoch": 6.6672,
      "grad_norm": 0.1401207447052002,
      "learning_rate": 8.33e-06,
      "loss": 0.0026,
      "step": 125010
    },
    {
      "epoch": 6.6677333333333335,
      "grad_norm": 0.28023236989974976,
      "learning_rate": 8.326666666666668e-06,
      "loss": 0.0038,
      "step": 125020
    },
    {
      "epoch": 6.668266666666667,
      "grad_norm": 0.42036375403404236,
      "learning_rate": 8.323333333333334e-06,
      "loss": 0.0038,
      "step": 125030
    },
    {
      "epoch": 6.6688,
      "grad_norm": 0.08407223224639893,
      "learning_rate": 8.32e-06,
      "loss": 0.0021,
      "step": 125040
    },
    {
      "epoch": 6.669333333333333,
      "grad_norm": 0.08407170325517654,
      "learning_rate": 8.316666666666668e-06,
      "loss": 0.0029,
      "step": 125050
    },
    {
      "epoch": 6.669866666666667,
      "grad_norm": 0.25221025943756104,
      "learning_rate": 8.313333333333334e-06,
      "loss": 0.0036,
      "step": 125060
    },
    {
      "epoch": 6.6704,
      "grad_norm": 0.2522122859954834,
      "learning_rate": 8.31e-06,
      "loss": 0.004,
      "step": 125070
    },
    {
      "epoch": 6.670933333333333,
      "grad_norm": 4.511444107180296e-09,
      "learning_rate": 8.306666666666666e-06,
      "loss": 0.0015,
      "step": 125080
    },
    {
      "epoch": 6.671466666666666,
      "grad_norm": 0.14012865722179413,
      "learning_rate": 8.303333333333334e-06,
      "loss": 0.0026,
      "step": 125090
    },
    {
      "epoch": 6.672,
      "grad_norm": 0.028024567291140556,
      "learning_rate": 8.3e-06,
      "loss": 0.0023,
      "step": 125100
    },
    {
      "epoch": 6.672533333333333,
      "grad_norm": 0.224190816283226,
      "learning_rate": 8.296666666666666e-06,
      "loss": 0.0023,
      "step": 125110
    },
    {
      "epoch": 6.673066666666667,
      "grad_norm": 0.11209473013877869,
      "learning_rate": 8.293333333333334e-06,
      "loss": 0.0024,
      "step": 125120
    },
    {
      "epoch": 6.6736,
      "grad_norm": 0.2802451252937317,
      "learning_rate": 8.29e-06,
      "loss": 0.0016,
      "step": 125130
    },
    {
      "epoch": 6.674133333333334,
      "grad_norm": 0.1681496798992157,
      "learning_rate": 8.286666666666668e-06,
      "loss": 0.0031,
      "step": 125140
    },
    {
      "epoch": 6.674666666666667,
      "grad_norm": 0.11209525913000107,
      "learning_rate": 8.283333333333333e-06,
      "loss": 0.0029,
      "step": 125150
    },
    {
      "epoch": 6.6752,
      "grad_norm": 0.1401209533214569,
      "learning_rate": 8.28e-06,
      "loss": 0.0024,
      "step": 125160
    },
    {
      "epoch": 6.6757333333333335,
      "grad_norm": 0.1681404411792755,
      "learning_rate": 8.276666666666666e-06,
      "loss": 0.0031,
      "step": 125170
    },
    {
      "epoch": 6.676266666666667,
      "grad_norm": 0.028024135157465935,
      "learning_rate": 8.273333333333334e-06,
      "loss": 0.002,
      "step": 125180
    },
    {
      "epoch": 6.6768,
      "grad_norm": 0.11210320144891739,
      "learning_rate": 8.27e-06,
      "loss": 0.0024,
      "step": 125190
    },
    {
      "epoch": 6.677333333333333,
      "grad_norm": 0.30828166007995605,
      "learning_rate": 8.266666666666667e-06,
      "loss": 0.0032,
      "step": 125200
    },
    {
      "epoch": 6.677866666666667,
      "grad_norm": 0.05604902654886246,
      "learning_rate": 8.263333333333334e-06,
      "loss": 0.0023,
      "step": 125210
    },
    {
      "epoch": 6.6784,
      "grad_norm": 0.14012040197849274,
      "learning_rate": 8.26e-06,
      "loss": 0.0028,
      "step": 125220
    },
    {
      "epoch": 6.678933333333333,
      "grad_norm": 0.39232906699180603,
      "learning_rate": 8.256666666666667e-06,
      "loss": 0.0024,
      "step": 125230
    },
    {
      "epoch": 6.679466666666666,
      "grad_norm": 0.19616109132766724,
      "learning_rate": 8.253333333333334e-06,
      "loss": 0.0029,
      "step": 125240
    },
    {
      "epoch": 6.68,
      "grad_norm": 0.08407012373209,
      "learning_rate": 8.25e-06,
      "loss": 0.0028,
      "step": 125250
    },
    {
      "epoch": 6.680533333333333,
      "grad_norm": 0.28023815155029297,
      "learning_rate": 8.246666666666667e-06,
      "loss": 0.0024,
      "step": 125260
    },
    {
      "epoch": 6.681066666666666,
      "grad_norm": 0.05604860559105873,
      "learning_rate": 8.243333333333333e-06,
      "loss": 0.0026,
      "step": 125270
    },
    {
      "epoch": 6.6815999999999995,
      "grad_norm": 0.25221362709999084,
      "learning_rate": 8.24e-06,
      "loss": 0.0025,
      "step": 125280
    },
    {
      "epoch": 6.682133333333334,
      "grad_norm": 1.7088231940576293e-09,
      "learning_rate": 8.236666666666667e-06,
      "loss": 0.0022,
      "step": 125290
    },
    {
      "epoch": 6.682666666666667,
      "grad_norm": 0.7756161093711853,
      "learning_rate": 8.233333333333333e-06,
      "loss": 0.0025,
      "step": 125300
    },
    {
      "epoch": 6.6832,
      "grad_norm": 0.11209561675786972,
      "learning_rate": 8.23e-06,
      "loss": 0.0024,
      "step": 125310
    },
    {
      "epoch": 6.6837333333333335,
      "grad_norm": 2.4374016138750676e-09,
      "learning_rate": 8.226666666666667e-06,
      "loss": 0.002,
      "step": 125320
    },
    {
      "epoch": 6.684266666666667,
      "grad_norm": 0.14093157649040222,
      "learning_rate": 8.223333333333335e-06,
      "loss": 0.0024,
      "step": 125330
    },
    {
      "epoch": 6.6848,
      "grad_norm": 0.08407367765903473,
      "learning_rate": 8.22e-06,
      "loss": 0.0034,
      "step": 125340
    },
    {
      "epoch": 6.685333333333333,
      "grad_norm": 0.08407308906316757,
      "learning_rate": 8.216666666666667e-06,
      "loss": 0.0022,
      "step": 125350
    },
    {
      "epoch": 6.685866666666667,
      "grad_norm": 0.16814012825489044,
      "learning_rate": 8.213333333333333e-06,
      "loss": 0.0046,
      "step": 125360
    },
    {
      "epoch": 6.6864,
      "grad_norm": 0.19616404175758362,
      "learning_rate": 8.210000000000001e-06,
      "loss": 0.0031,
      "step": 125370
    },
    {
      "epoch": 6.686933333333333,
      "grad_norm": 0.056048035621643066,
      "learning_rate": 8.206666666666667e-06,
      "loss": 0.0022,
      "step": 125380
    },
    {
      "epoch": 6.6874666666666664,
      "grad_norm": 3.838728002136804e-09,
      "learning_rate": 8.203333333333333e-06,
      "loss": 0.0025,
      "step": 125390
    },
    {
      "epoch": 6.688,
      "grad_norm": 0.22419661283493042,
      "learning_rate": 8.200000000000001e-06,
      "loss": 0.0029,
      "step": 125400
    },
    {
      "epoch": 6.688533333333333,
      "grad_norm": 0.08173216879367828,
      "learning_rate": 8.196666666666666e-06,
      "loss": 0.0021,
      "step": 125410
    },
    {
      "epoch": 6.689066666666667,
      "grad_norm": 0.4483775198459625,
      "learning_rate": 8.193333333333333e-06,
      "loss": 0.0031,
      "step": 125420
    },
    {
      "epoch": 6.6896,
      "grad_norm": 0.14011424779891968,
      "learning_rate": 8.190000000000001e-06,
      "loss": 0.0026,
      "step": 125430
    },
    {
      "epoch": 6.690133333333334,
      "grad_norm": 3.4658080849681028e-09,
      "learning_rate": 8.186666666666667e-06,
      "loss": 0.0029,
      "step": 125440
    },
    {
      "epoch": 6.690666666666667,
      "grad_norm": 0.08481947332620621,
      "learning_rate": 8.183333333333333e-06,
      "loss": 0.0027,
      "step": 125450
    },
    {
      "epoch": 6.6912,
      "grad_norm": 0.1681399941444397,
      "learning_rate": 8.18e-06,
      "loss": 0.0036,
      "step": 125460
    },
    {
      "epoch": 6.6917333333333335,
      "grad_norm": 4.690610122537464e-09,
      "learning_rate": 8.176666666666667e-06,
      "loss": 0.0034,
      "step": 125470
    },
    {
      "epoch": 6.692266666666667,
      "grad_norm": 0.22418150305747986,
      "learning_rate": 8.173333333333334e-06,
      "loss": 0.0031,
      "step": 125480
    },
    {
      "epoch": 6.6928,
      "grad_norm": 0.1120918020606041,
      "learning_rate": 8.17e-06,
      "loss": 0.0034,
      "step": 125490
    },
    {
      "epoch": 6.693333333333333,
      "grad_norm": 0.05604631081223488,
      "learning_rate": 8.166666666666668e-06,
      "loss": 0.0035,
      "step": 125500
    },
    {
      "epoch": 6.693866666666667,
      "grad_norm": 0.11580570787191391,
      "learning_rate": 8.163333333333334e-06,
      "loss": 0.0041,
      "step": 125510
    },
    {
      "epoch": 6.6944,
      "grad_norm": 0.056045494973659515,
      "learning_rate": 8.160000000000001e-06,
      "loss": 0.0024,
      "step": 125520
    },
    {
      "epoch": 6.694933333333333,
      "grad_norm": 0.08407168835401535,
      "learning_rate": 8.156666666666666e-06,
      "loss": 0.0022,
      "step": 125530
    },
    {
      "epoch": 6.6954666666666665,
      "grad_norm": 0.19616883993148804,
      "learning_rate": 8.153333333333334e-06,
      "loss": 0.0024,
      "step": 125540
    },
    {
      "epoch": 6.696,
      "grad_norm": 0.25221383571624756,
      "learning_rate": 8.15e-06,
      "loss": 0.0026,
      "step": 125550
    },
    {
      "epoch": 6.696533333333333,
      "grad_norm": 0.25220781564712524,
      "learning_rate": 8.146666666666668e-06,
      "loss": 0.0026,
      "step": 125560
    },
    {
      "epoch": 6.697066666666666,
      "grad_norm": 0.2523379921913147,
      "learning_rate": 8.143333333333334e-06,
      "loss": 0.0019,
      "step": 125570
    },
    {
      "epoch": 6.6975999999999996,
      "grad_norm": 0.22418738901615143,
      "learning_rate": 8.14e-06,
      "loss": 0.0018,
      "step": 125580
    },
    {
      "epoch": 6.698133333333334,
      "grad_norm": 0.0840689167380333,
      "learning_rate": 8.136666666666668e-06,
      "loss": 0.0023,
      "step": 125590
    },
    {
      "epoch": 6.698666666666667,
      "grad_norm": 0.22418761253356934,
      "learning_rate": 8.133333333333332e-06,
      "loss": 0.0022,
      "step": 125600
    },
    {
      "epoch": 6.6992,
      "grad_norm": 0.11271332204341888,
      "learning_rate": 8.13e-06,
      "loss": 0.0023,
      "step": 125610
    },
    {
      "epoch": 6.6997333333333335,
      "grad_norm": 0.3082594871520996,
      "learning_rate": 8.126666666666668e-06,
      "loss": 0.0037,
      "step": 125620
    },
    {
      "epoch": 6.700266666666667,
      "grad_norm": 0.14011415839195251,
      "learning_rate": 8.123333333333334e-06,
      "loss": 0.0035,
      "step": 125630
    },
    {
      "epoch": 6.7008,
      "grad_norm": 0.08406835794448853,
      "learning_rate": 8.12e-06,
      "loss": 0.0019,
      "step": 125640
    },
    {
      "epoch": 6.701333333333333,
      "grad_norm": 0.028022555634379387,
      "learning_rate": 8.116666666666666e-06,
      "loss": 0.0028,
      "step": 125650
    },
    {
      "epoch": 6.701866666666667,
      "grad_norm": 0.08406984806060791,
      "learning_rate": 8.113333333333334e-06,
      "loss": 0.0034,
      "step": 125660
    },
    {
      "epoch": 6.7024,
      "grad_norm": 1.0572741031646729,
      "learning_rate": 8.11e-06,
      "loss": 0.0022,
      "step": 125670
    },
    {
      "epoch": 6.702933333333333,
      "grad_norm": 0.42036116123199463,
      "learning_rate": 8.106666666666666e-06,
      "loss": 0.0021,
      "step": 125680
    },
    {
      "epoch": 6.7034666666666665,
      "grad_norm": 1.4254748492703584e-09,
      "learning_rate": 8.103333333333334e-06,
      "loss": 0.0029,
      "step": 125690
    },
    {
      "epoch": 6.704,
      "grad_norm": 0.11209405213594437,
      "learning_rate": 8.1e-06,
      "loss": 0.0024,
      "step": 125700
    },
    {
      "epoch": 6.704533333333333,
      "grad_norm": 0.19615541398525238,
      "learning_rate": 8.096666666666668e-06,
      "loss": 0.0051,
      "step": 125710
    },
    {
      "epoch": 6.705066666666666,
      "grad_norm": 0.08406901359558105,
      "learning_rate": 8.093333333333333e-06,
      "loss": 0.0035,
      "step": 125720
    },
    {
      "epoch": 6.7056000000000004,
      "grad_norm": 0.1120927557349205,
      "learning_rate": 8.09e-06,
      "loss": 0.0018,
      "step": 125730
    },
    {
      "epoch": 6.706133333333334,
      "grad_norm": 2.8459679057846188e-09,
      "learning_rate": 8.086666666666667e-06,
      "loss": 0.0026,
      "step": 125740
    },
    {
      "epoch": 6.706666666666667,
      "grad_norm": 0.16813598573207855,
      "learning_rate": 8.083333333333333e-06,
      "loss": 0.0025,
      "step": 125750
    },
    {
      "epoch": 6.7072,
      "grad_norm": 0.02802339754998684,
      "learning_rate": 8.08e-06,
      "loss": 0.0031,
      "step": 125760
    },
    {
      "epoch": 6.7077333333333335,
      "grad_norm": 0.11209286749362946,
      "learning_rate": 8.076666666666667e-06,
      "loss": 0.0022,
      "step": 125770
    },
    {
      "epoch": 6.708266666666667,
      "grad_norm": 0.05604489892721176,
      "learning_rate": 8.073333333333335e-06,
      "loss": 0.0022,
      "step": 125780
    },
    {
      "epoch": 6.7088,
      "grad_norm": 3.0647060444977114e-09,
      "learning_rate": 8.069999999999999e-06,
      "loss": 0.0028,
      "step": 125790
    },
    {
      "epoch": 6.709333333333333,
      "grad_norm": 3.1894162866308307e-09,
      "learning_rate": 8.066666666666667e-06,
      "loss": 0.0042,
      "step": 125800
    },
    {
      "epoch": 6.709866666666667,
      "grad_norm": 0.1681421548128128,
      "learning_rate": 8.063333333333335e-06,
      "loss": 0.0043,
      "step": 125810
    },
    {
      "epoch": 6.7104,
      "grad_norm": 0.19615980982780457,
      "learning_rate": 8.06e-06,
      "loss": 0.0024,
      "step": 125820
    },
    {
      "epoch": 6.710933333333333,
      "grad_norm": 3.9309999699810305e-09,
      "learning_rate": 8.056666666666667e-06,
      "loss": 0.002,
      "step": 125830
    },
    {
      "epoch": 6.7114666666666665,
      "grad_norm": 1.597056598079405e-09,
      "learning_rate": 8.053333333333333e-06,
      "loss": 0.0033,
      "step": 125840
    },
    {
      "epoch": 6.712,
      "grad_norm": 0.28023943305015564,
      "learning_rate": 8.050000000000001e-06,
      "loss": 0.0025,
      "step": 125850
    },
    {
      "epoch": 6.712533333333333,
      "grad_norm": 0.16814552247524261,
      "learning_rate": 8.046666666666667e-06,
      "loss": 0.0034,
      "step": 125860
    },
    {
      "epoch": 6.713066666666666,
      "grad_norm": 0.16814684867858887,
      "learning_rate": 8.043333333333333e-06,
      "loss": 0.0016,
      "step": 125870
    },
    {
      "epoch": 6.7136,
      "grad_norm": 0.05604938790202141,
      "learning_rate": 8.040000000000001e-06,
      "loss": 0.0017,
      "step": 125880
    },
    {
      "epoch": 6.714133333333333,
      "grad_norm": 0.25221720337867737,
      "learning_rate": 8.036666666666667e-06,
      "loss": 0.0026,
      "step": 125890
    },
    {
      "epoch": 6.714666666666667,
      "grad_norm": 0.08406827598810196,
      "learning_rate": 8.033333333333335e-06,
      "loss": 0.0025,
      "step": 125900
    },
    {
      "epoch": 6.7152,
      "grad_norm": 0.252211332321167,
      "learning_rate": 8.03e-06,
      "loss": 0.0028,
      "step": 125910
    },
    {
      "epoch": 6.7157333333333336,
      "grad_norm": 0.19616547226905823,
      "learning_rate": 8.026666666666667e-06,
      "loss": 0.0031,
      "step": 125920
    },
    {
      "epoch": 6.716266666666667,
      "grad_norm": 0.02802249789237976,
      "learning_rate": 8.023333333333333e-06,
      "loss": 0.0029,
      "step": 125930
    },
    {
      "epoch": 6.7168,
      "grad_norm": 0.08407077193260193,
      "learning_rate": 8.02e-06,
      "loss": 0.0033,
      "step": 125940
    },
    {
      "epoch": 6.717333333333333,
      "grad_norm": 0.028024140745401382,
      "learning_rate": 8.016666666666667e-06,
      "loss": 0.0032,
      "step": 125950
    },
    {
      "epoch": 6.717866666666667,
      "grad_norm": 0.1401197463274002,
      "learning_rate": 8.013333333333333e-06,
      "loss": 0.0012,
      "step": 125960
    },
    {
      "epoch": 6.7184,
      "grad_norm": 0.36432120203971863,
      "learning_rate": 8.010000000000001e-06,
      "loss": 0.003,
      "step": 125970
    },
    {
      "epoch": 6.718933333333333,
      "grad_norm": 0.11209762841463089,
      "learning_rate": 8.006666666666666e-06,
      "loss": 0.0025,
      "step": 125980
    },
    {
      "epoch": 6.7194666666666665,
      "grad_norm": 0.08406984806060791,
      "learning_rate": 8.003333333333334e-06,
      "loss": 0.0021,
      "step": 125990
    },
    {
      "epoch": 6.72,
      "grad_norm": 0.2802337408065796,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.0023,
      "step": 126000
    },
    {
      "epoch": 6.720533333333333,
      "grad_norm": 0.33626922965049744,
      "learning_rate": 7.996666666666667e-06,
      "loss": 0.0023,
      "step": 126010
    },
    {
      "epoch": 6.721066666666666,
      "grad_norm": 0.16813549399375916,
      "learning_rate": 7.993333333333334e-06,
      "loss": 0.0028,
      "step": 126020
    },
    {
      "epoch": 6.7216000000000005,
      "grad_norm": 0.028023116290569305,
      "learning_rate": 7.99e-06,
      "loss": 0.0019,
      "step": 126030
    },
    {
      "epoch": 6.722133333333334,
      "grad_norm": 0.3362749218940735,
      "learning_rate": 7.986666666666668e-06,
      "loss": 0.0024,
      "step": 126040
    },
    {
      "epoch": 6.722666666666667,
      "grad_norm": 0.05604526400566101,
      "learning_rate": 7.983333333333334e-06,
      "loss": 0.0036,
      "step": 126050
    },
    {
      "epoch": 6.7232,
      "grad_norm": 0.280225545167923,
      "learning_rate": 7.98e-06,
      "loss": 0.0028,
      "step": 126060
    },
    {
      "epoch": 6.723733333333334,
      "grad_norm": 0.0840708464384079,
      "learning_rate": 7.976666666666668e-06,
      "loss": 0.0018,
      "step": 126070
    },
    {
      "epoch": 6.724266666666667,
      "grad_norm": 0.7814032435417175,
      "learning_rate": 7.973333333333334e-06,
      "loss": 0.0026,
      "step": 126080
    },
    {
      "epoch": 6.7248,
      "grad_norm": 0.1681337058544159,
      "learning_rate": 7.97e-06,
      "loss": 0.0027,
      "step": 126090
    },
    {
      "epoch": 6.725333333333333,
      "grad_norm": 3.644056834062326e-09,
      "learning_rate": 7.966666666666666e-06,
      "loss": 0.0031,
      "step": 126100
    },
    {
      "epoch": 6.725866666666667,
      "grad_norm": 0.22418634593486786,
      "learning_rate": 7.963333333333334e-06,
      "loss": 0.0016,
      "step": 126110
    },
    {
      "epoch": 6.7264,
      "grad_norm": 0.08407313376665115,
      "learning_rate": 7.96e-06,
      "loss": 0.0029,
      "step": 126120
    },
    {
      "epoch": 6.726933333333333,
      "grad_norm": 0.1120961606502533,
      "learning_rate": 7.956666666666666e-06,
      "loss": 0.0039,
      "step": 126130
    },
    {
      "epoch": 6.7274666666666665,
      "grad_norm": 0.1681414097547531,
      "learning_rate": 7.953333333333334e-06,
      "loss": 0.0015,
      "step": 126140
    },
    {
      "epoch": 6.728,
      "grad_norm": 0.08406734466552734,
      "learning_rate": 7.95e-06,
      "loss": 0.0027,
      "step": 126150
    },
    {
      "epoch": 6.728533333333333,
      "grad_norm": 0.0840679481625557,
      "learning_rate": 7.946666666666668e-06,
      "loss": 0.0019,
      "step": 126160
    },
    {
      "epoch": 6.729066666666666,
      "grad_norm": 0.028022604063153267,
      "learning_rate": 7.943333333333332e-06,
      "loss": 0.0019,
      "step": 126170
    },
    {
      "epoch": 6.7296,
      "grad_norm": 0.05604464188218117,
      "learning_rate": 7.94e-06,
      "loss": 0.0027,
      "step": 126180
    },
    {
      "epoch": 6.730133333333333,
      "grad_norm": 0.08406656235456467,
      "learning_rate": 7.936666666666668e-06,
      "loss": 0.003,
      "step": 126190
    },
    {
      "epoch": 6.730666666666667,
      "grad_norm": 0.05604579672217369,
      "learning_rate": 7.933333333333334e-06,
      "loss": 0.0022,
      "step": 126200
    },
    {
      "epoch": 6.7312,
      "grad_norm": 0.16814082860946655,
      "learning_rate": 7.93e-06,
      "loss": 0.0028,
      "step": 126210
    },
    {
      "epoch": 6.731733333333334,
      "grad_norm": 5.139506598084154e-09,
      "learning_rate": 7.926666666666666e-06,
      "loss": 0.0025,
      "step": 126220
    },
    {
      "epoch": 6.732266666666667,
      "grad_norm": 0.19616417586803436,
      "learning_rate": 7.923333333333334e-06,
      "loss": 0.0022,
      "step": 126230
    },
    {
      "epoch": 6.7328,
      "grad_norm": 0.056044839322566986,
      "learning_rate": 7.92e-06,
      "loss": 0.0024,
      "step": 126240
    },
    {
      "epoch": 6.733333333333333,
      "grad_norm": 0.02802223153412342,
      "learning_rate": 7.916666666666667e-06,
      "loss": 0.0019,
      "step": 126250
    },
    {
      "epoch": 6.733866666666667,
      "grad_norm": 0.056045375764369965,
      "learning_rate": 7.913333333333334e-06,
      "loss": 0.0019,
      "step": 126260
    },
    {
      "epoch": 6.7344,
      "grad_norm": 0.22418086230754852,
      "learning_rate": 7.91e-06,
      "loss": 0.0026,
      "step": 126270
    },
    {
      "epoch": 6.734933333333333,
      "grad_norm": 0.1401137411594391,
      "learning_rate": 7.906666666666667e-06,
      "loss": 0.0025,
      "step": 126280
    },
    {
      "epoch": 6.7354666666666665,
      "grad_norm": 0.05604429543018341,
      "learning_rate": 7.903333333333333e-06,
      "loss": 0.0023,
      "step": 126290
    },
    {
      "epoch": 6.736,
      "grad_norm": 0.1120905727148056,
      "learning_rate": 7.9e-06,
      "loss": 0.0025,
      "step": 126300
    },
    {
      "epoch": 6.736533333333333,
      "grad_norm": 0.22417883574962616,
      "learning_rate": 7.896666666666667e-06,
      "loss": 0.0036,
      "step": 126310
    },
    {
      "epoch": 6.737066666666666,
      "grad_norm": 0.08406770974397659,
      "learning_rate": 7.893333333333333e-06,
      "loss": 0.0026,
      "step": 126320
    },
    {
      "epoch": 6.7376000000000005,
      "grad_norm": 0.16813795268535614,
      "learning_rate": 7.89e-06,
      "loss": 0.0035,
      "step": 126330
    },
    {
      "epoch": 6.738133333333334,
      "grad_norm": 0.05604584142565727,
      "learning_rate": 7.886666666666667e-06,
      "loss": 0.0027,
      "step": 126340
    },
    {
      "epoch": 6.738666666666667,
      "grad_norm": 0.11208787560462952,
      "learning_rate": 7.883333333333335e-06,
      "loss": 0.0031,
      "step": 126350
    },
    {
      "epoch": 6.7392,
      "grad_norm": 0.2521984875202179,
      "learning_rate": 7.879999999999999e-06,
      "loss": 0.0026,
      "step": 126360
    },
    {
      "epoch": 6.739733333333334,
      "grad_norm": 0.1681356430053711,
      "learning_rate": 7.876666666666667e-06,
      "loss": 0.002,
      "step": 126370
    },
    {
      "epoch": 6.740266666666667,
      "grad_norm": 0.056044965982437134,
      "learning_rate": 7.873333333333335e-06,
      "loss": 0.0015,
      "step": 126380
    },
    {
      "epoch": 6.7408,
      "grad_norm": 0.25219962000846863,
      "learning_rate": 7.870000000000001e-06,
      "loss": 0.0022,
      "step": 126390
    },
    {
      "epoch": 6.741333333333333,
      "grad_norm": 0.168134868144989,
      "learning_rate": 7.866666666666667e-06,
      "loss": 0.0041,
      "step": 126400
    },
    {
      "epoch": 6.741866666666667,
      "grad_norm": 0.19615662097930908,
      "learning_rate": 7.863333333333333e-06,
      "loss": 0.003,
      "step": 126410
    },
    {
      "epoch": 6.7424,
      "grad_norm": 2.1315607057204033e-09,
      "learning_rate": 7.860000000000001e-06,
      "loss": 0.0024,
      "step": 126420
    },
    {
      "epoch": 6.742933333333333,
      "grad_norm": 0.16813595592975616,
      "learning_rate": 7.856666666666667e-06,
      "loss": 0.0022,
      "step": 126430
    },
    {
      "epoch": 6.7434666666666665,
      "grad_norm": 0.08406822383403778,
      "learning_rate": 7.853333333333333e-06,
      "loss": 0.0027,
      "step": 126440
    },
    {
      "epoch": 6.744,
      "grad_norm": 0.492717444896698,
      "learning_rate": 7.850000000000001e-06,
      "loss": 0.0033,
      "step": 126450
    },
    {
      "epoch": 6.744533333333333,
      "grad_norm": 0.028022779151797295,
      "learning_rate": 7.846666666666667e-06,
      "loss": 0.0029,
      "step": 126460
    },
    {
      "epoch": 6.745066666666666,
      "grad_norm": 0.23640386760234833,
      "learning_rate": 7.843333333333333e-06,
      "loss": 0.0037,
      "step": 126470
    },
    {
      "epoch": 6.7456,
      "grad_norm": 1.3683345317840576,
      "learning_rate": 7.84e-06,
      "loss": 0.0027,
      "step": 126480
    },
    {
      "epoch": 6.746133333333333,
      "grad_norm": 0.08406836539506912,
      "learning_rate": 7.836666666666667e-06,
      "loss": 0.0036,
      "step": 126490
    },
    {
      "epoch": 6.746666666666667,
      "grad_norm": 0.2521963119506836,
      "learning_rate": 7.833333333333333e-06,
      "loss": 0.0023,
      "step": 126500
    },
    {
      "epoch": 6.7472,
      "grad_norm": 2.231745719909668,
      "learning_rate": 7.83e-06,
      "loss": 0.0021,
      "step": 126510
    },
    {
      "epoch": 6.747733333333334,
      "grad_norm": 0.11208923906087875,
      "learning_rate": 7.826666666666667e-06,
      "loss": 0.0027,
      "step": 126520
    },
    {
      "epoch": 6.748266666666667,
      "grad_norm": 0.11209134757518768,
      "learning_rate": 7.823333333333334e-06,
      "loss": 0.002,
      "step": 126530
    },
    {
      "epoch": 6.7488,
      "grad_norm": 0.39231857657432556,
      "learning_rate": 7.820000000000001e-06,
      "loss": 0.0023,
      "step": 126540
    },
    {
      "epoch": 6.749333333333333,
      "grad_norm": 0.2521935999393463,
      "learning_rate": 7.816666666666666e-06,
      "loss": 0.0037,
      "step": 126550
    },
    {
      "epoch": 6.749866666666667,
      "grad_norm": 0.14011353254318237,
      "learning_rate": 7.813333333333334e-06,
      "loss": 0.0042,
      "step": 126560
    },
    {
      "epoch": 6.7504,
      "grad_norm": 0.05604485049843788,
      "learning_rate": 7.810000000000001e-06,
      "loss": 0.0019,
      "step": 126570
    },
    {
      "epoch": 6.750933333333333,
      "grad_norm": 0.05604354292154312,
      "learning_rate": 7.806666666666668e-06,
      "loss": 0.0017,
      "step": 126580
    },
    {
      "epoch": 6.7514666666666665,
      "grad_norm": 0.22417756915092468,
      "learning_rate": 7.803333333333334e-06,
      "loss": 0.0023,
      "step": 126590
    },
    {
      "epoch": 6.752,
      "grad_norm": 0.16813382506370544,
      "learning_rate": 7.8e-06,
      "loss": 0.0028,
      "step": 126600
    },
    {
      "epoch": 6.752533333333333,
      "grad_norm": 0.056046586483716965,
      "learning_rate": 7.796666666666668e-06,
      "loss": 0.0031,
      "step": 126610
    },
    {
      "epoch": 6.753066666666666,
      "grad_norm": 0.16813924908638,
      "learning_rate": 7.793333333333334e-06,
      "loss": 0.0028,
      "step": 126620
    },
    {
      "epoch": 6.7536000000000005,
      "grad_norm": 0.0280228890478611,
      "learning_rate": 7.79e-06,
      "loss": 0.003,
      "step": 126630
    },
    {
      "epoch": 6.754133333333334,
      "grad_norm": 0.028022857382893562,
      "learning_rate": 7.786666666666668e-06,
      "loss": 0.0019,
      "step": 126640
    },
    {
      "epoch": 6.754666666666667,
      "grad_norm": 0.2241741269826889,
      "learning_rate": 7.783333333333334e-06,
      "loss": 0.0021,
      "step": 126650
    },
    {
      "epoch": 6.7552,
      "grad_norm": 0.11208836734294891,
      "learning_rate": 7.78e-06,
      "loss": 0.0024,
      "step": 126660
    },
    {
      "epoch": 6.755733333333334,
      "grad_norm": 0.05604483559727669,
      "learning_rate": 7.776666666666666e-06,
      "loss": 0.0023,
      "step": 126670
    },
    {
      "epoch": 6.756266666666667,
      "grad_norm": 0.028022069483995438,
      "learning_rate": 7.773333333333334e-06,
      "loss": 0.0031,
      "step": 126680
    },
    {
      "epoch": 6.7568,
      "grad_norm": 0.1442301869392395,
      "learning_rate": 7.77e-06,
      "loss": 0.0024,
      "step": 126690
    },
    {
      "epoch": 6.757333333333333,
      "grad_norm": 0.3020290434360504,
      "learning_rate": 7.766666666666666e-06,
      "loss": 0.0027,
      "step": 126700
    },
    {
      "epoch": 6.757866666666667,
      "grad_norm": 2.2512478548009085e-09,
      "learning_rate": 7.763333333333334e-06,
      "loss": 0.0029,
      "step": 126710
    },
    {
      "epoch": 6.7584,
      "grad_norm": 0.25220564007759094,
      "learning_rate": 7.76e-06,
      "loss": 0.0042,
      "step": 126720
    },
    {
      "epoch": 6.758933333333333,
      "grad_norm": 0.08406443148851395,
      "learning_rate": 7.756666666666668e-06,
      "loss": 0.0017,
      "step": 126730
    },
    {
      "epoch": 6.7594666666666665,
      "grad_norm": 0.028022585436701775,
      "learning_rate": 7.753333333333333e-06,
      "loss": 0.0029,
      "step": 126740
    },
    {
      "epoch": 6.76,
      "grad_norm": 0.14011318981647491,
      "learning_rate": 7.75e-06,
      "loss": 0.0022,
      "step": 126750
    },
    {
      "epoch": 6.760533333333333,
      "grad_norm": 0.056044045835733414,
      "learning_rate": 7.746666666666668e-06,
      "loss": 0.0028,
      "step": 126760
    },
    {
      "epoch": 6.761066666666666,
      "grad_norm": 0.3642825484275818,
      "learning_rate": 7.743333333333334e-06,
      "loss": 0.0034,
      "step": 126770
    },
    {
      "epoch": 6.7616,
      "grad_norm": 0.056044843047857285,
      "learning_rate": 7.74e-06,
      "loss": 0.0027,
      "step": 126780
    },
    {
      "epoch": 6.762133333333333,
      "grad_norm": 0.08406806737184525,
      "learning_rate": 7.736666666666667e-06,
      "loss": 0.0022,
      "step": 126790
    },
    {
      "epoch": 6.762666666666667,
      "grad_norm": 3.3765781282113494e-09,
      "learning_rate": 7.733333333333334e-06,
      "loss": 0.0022,
      "step": 126800
    },
    {
      "epoch": 6.7632,
      "grad_norm": 0.1681382656097412,
      "learning_rate": 7.73e-06,
      "loss": 0.0029,
      "step": 126810
    },
    {
      "epoch": 6.763733333333334,
      "grad_norm": 0.11209134757518768,
      "learning_rate": 7.726666666666667e-06,
      "loss": 0.002,
      "step": 126820
    },
    {
      "epoch": 6.764266666666667,
      "grad_norm": 0.25219935178756714,
      "learning_rate": 7.723333333333334e-06,
      "loss": 0.0024,
      "step": 126830
    },
    {
      "epoch": 6.7648,
      "grad_norm": 0.16813220083713531,
      "learning_rate": 7.72e-06,
      "loss": 0.002,
      "step": 126840
    },
    {
      "epoch": 6.765333333333333,
      "grad_norm": 0.2387569546699524,
      "learning_rate": 7.716666666666667e-06,
      "loss": 0.0045,
      "step": 126850
    },
    {
      "epoch": 6.765866666666667,
      "grad_norm": 0.28023025393486023,
      "learning_rate": 7.713333333333333e-06,
      "loss": 0.003,
      "step": 126860
    },
    {
      "epoch": 6.7664,
      "grad_norm": 0.1401154100894928,
      "learning_rate": 7.71e-06,
      "loss": 0.0031,
      "step": 126870
    },
    {
      "epoch": 6.766933333333333,
      "grad_norm": 2.575462731968514e-09,
      "learning_rate": 7.706666666666667e-06,
      "loss": 0.0018,
      "step": 126880
    },
    {
      "epoch": 6.7674666666666665,
      "grad_norm": 0.14010795950889587,
      "learning_rate": 7.703333333333333e-06,
      "loss": 0.002,
      "step": 126890
    },
    {
      "epoch": 6.768,
      "grad_norm": 0.0560440868139267,
      "learning_rate": 7.7e-06,
      "loss": 0.0029,
      "step": 126900
    },
    {
      "epoch": 6.768533333333333,
      "grad_norm": 3.778765744755219e-09,
      "learning_rate": 7.696666666666667e-06,
      "loss": 0.0037,
      "step": 126910
    },
    {
      "epoch": 6.769066666666666,
      "grad_norm": 0.25220662355422974,
      "learning_rate": 7.693333333333335e-06,
      "loss": 0.0021,
      "step": 126920
    },
    {
      "epoch": 6.7696,
      "grad_norm": 0.08406585454940796,
      "learning_rate": 7.69e-06,
      "loss": 0.0039,
      "step": 126930
    },
    {
      "epoch": 6.770133333333334,
      "grad_norm": 0.02802249602973461,
      "learning_rate": 7.686666666666667e-06,
      "loss": 0.0031,
      "step": 126940
    },
    {
      "epoch": 6.770666666666667,
      "grad_norm": 0.1401151716709137,
      "learning_rate": 7.683333333333335e-06,
      "loss": 0.0023,
      "step": 126950
    },
    {
      "epoch": 6.7712,
      "grad_norm": 0.28022152185440063,
      "learning_rate": 7.68e-06,
      "loss": 0.0016,
      "step": 126960
    },
    {
      "epoch": 6.771733333333334,
      "grad_norm": 0.1401132047176361,
      "learning_rate": 7.676666666666667e-06,
      "loss": 0.0023,
      "step": 126970
    },
    {
      "epoch": 6.772266666666667,
      "grad_norm": 0.6176205277442932,
      "learning_rate": 7.673333333333333e-06,
      "loss": 0.0022,
      "step": 126980
    },
    {
      "epoch": 6.7728,
      "grad_norm": 0.22419427335262299,
      "learning_rate": 7.670000000000001e-06,
      "loss": 0.002,
      "step": 126990
    },
    {
      "epoch": 6.773333333333333,
      "grad_norm": 0.08407207578420639,
      "learning_rate": 7.666666666666667e-06,
      "loss": 0.003,
      "step": 127000
    },
    {
      "epoch": 6.773866666666667,
      "grad_norm": 0.16813932359218597,
      "learning_rate": 7.663333333333333e-06,
      "loss": 0.0021,
      "step": 127010
    },
    {
      "epoch": 6.7744,
      "grad_norm": 0.19615407288074493,
      "learning_rate": 7.660000000000001e-06,
      "loss": 0.003,
      "step": 127020
    },
    {
      "epoch": 6.774933333333333,
      "grad_norm": 0.11209043860435486,
      "learning_rate": 7.656666666666667e-06,
      "loss": 0.002,
      "step": 127030
    },
    {
      "epoch": 6.7754666666666665,
      "grad_norm": 0.11208982020616531,
      "learning_rate": 7.653333333333333e-06,
      "loss": 0.003,
      "step": 127040
    },
    {
      "epoch": 6.776,
      "grad_norm": 0.3082490563392639,
      "learning_rate": 7.65e-06,
      "loss": 0.004,
      "step": 127050
    },
    {
      "epoch": 6.776533333333333,
      "grad_norm": 0.05604415014386177,
      "learning_rate": 7.646666666666667e-06,
      "loss": 0.0042,
      "step": 127060
    },
    {
      "epoch": 6.777066666666666,
      "grad_norm": 0.02802247367799282,
      "learning_rate": 7.643333333333334e-06,
      "loss": 0.0017,
      "step": 127070
    },
    {
      "epoch": 6.7776,
      "grad_norm": 0.14011365175247192,
      "learning_rate": 7.64e-06,
      "loss": 0.0026,
      "step": 127080
    },
    {
      "epoch": 6.778133333333333,
      "grad_norm": 0.08406467735767365,
      "learning_rate": 7.636666666666668e-06,
      "loss": 0.0027,
      "step": 127090
    },
    {
      "epoch": 6.778666666666666,
      "grad_norm": 0.2802138924598694,
      "learning_rate": 7.633333333333334e-06,
      "loss": 0.0035,
      "step": 127100
    },
    {
      "epoch": 6.7792,
      "grad_norm": 0.05604437366127968,
      "learning_rate": 7.630000000000001e-06,
      "loss": 0.0029,
      "step": 127110
    },
    {
      "epoch": 6.779733333333334,
      "grad_norm": 0.27419936656951904,
      "learning_rate": 7.626666666666667e-06,
      "loss": 0.0021,
      "step": 127120
    },
    {
      "epoch": 6.780266666666667,
      "grad_norm": 0.056042760610580444,
      "learning_rate": 7.623333333333334e-06,
      "loss": 0.0018,
      "step": 127130
    },
    {
      "epoch": 6.7808,
      "grad_norm": 0.02802197076380253,
      "learning_rate": 7.620000000000001e-06,
      "loss": 0.0035,
      "step": 127140
    },
    {
      "epoch": 6.781333333333333,
      "grad_norm": 0.30824583768844604,
      "learning_rate": 7.616666666666666e-06,
      "loss": 0.0024,
      "step": 127150
    },
    {
      "epoch": 6.781866666666667,
      "grad_norm": 0.11208539456129074,
      "learning_rate": 7.613333333333334e-06,
      "loss": 0.0039,
      "step": 127160
    },
    {
      "epoch": 6.7824,
      "grad_norm": 0.08406537026166916,
      "learning_rate": 7.610000000000001e-06,
      "loss": 0.0023,
      "step": 127170
    },
    {
      "epoch": 6.782933333333333,
      "grad_norm": 0.08406489342451096,
      "learning_rate": 7.606666666666668e-06,
      "loss": 0.0036,
      "step": 127180
    },
    {
      "epoch": 6.7834666666666665,
      "grad_norm": 0.39230191707611084,
      "learning_rate": 7.603333333333333e-06,
      "loss": 0.003,
      "step": 127190
    },
    {
      "epoch": 6.784,
      "grad_norm": 0.056043755263090134,
      "learning_rate": 7.6e-06,
      "loss": 0.0023,
      "step": 127200
    },
    {
      "epoch": 6.784533333333333,
      "grad_norm": 0.08406443893909454,
      "learning_rate": 7.596666666666667e-06,
      "loss": 0.003,
      "step": 127210
    },
    {
      "epoch": 6.785066666666666,
      "grad_norm": 0.02802123688161373,
      "learning_rate": 7.593333333333334e-06,
      "loss": 0.0031,
      "step": 127220
    },
    {
      "epoch": 6.7856,
      "grad_norm": 0.08406434953212738,
      "learning_rate": 7.59e-06,
      "loss": 0.0026,
      "step": 127230
    },
    {
      "epoch": 6.786133333333334,
      "grad_norm": 0.2522009015083313,
      "learning_rate": 7.586666666666667e-06,
      "loss": 0.0019,
      "step": 127240
    },
    {
      "epoch": 6.786666666666667,
      "grad_norm": 0.11208786070346832,
      "learning_rate": 7.583333333333334e-06,
      "loss": 0.0029,
      "step": 127250
    },
    {
      "epoch": 6.7872,
      "grad_norm": 4.368673423016389e-09,
      "learning_rate": 7.580000000000001e-06,
      "loss": 0.0019,
      "step": 127260
    },
    {
      "epoch": 6.787733333333334,
      "grad_norm": 0.028021272271871567,
      "learning_rate": 7.576666666666666e-06,
      "loss": 0.0025,
      "step": 127270
    },
    {
      "epoch": 6.788266666666667,
      "grad_norm": 0.08406512439250946,
      "learning_rate": 7.573333333333333e-06,
      "loss": 0.0017,
      "step": 127280
    },
    {
      "epoch": 6.7888,
      "grad_norm": 0.19614967703819275,
      "learning_rate": 7.57e-06,
      "loss": 0.0029,
      "step": 127290
    },
    {
      "epoch": 6.789333333333333,
      "grad_norm": 0.22417034208774567,
      "learning_rate": 7.5666666666666665e-06,
      "loss": 0.0029,
      "step": 127300
    },
    {
      "epoch": 6.789866666666667,
      "grad_norm": 3.424990513423154e-09,
      "learning_rate": 7.5633333333333335e-06,
      "loss": 0.0021,
      "step": 127310
    },
    {
      "epoch": 6.7904,
      "grad_norm": 1.531308191360381e-09,
      "learning_rate": 7.5600000000000005e-06,
      "loss": 0.002,
      "step": 127320
    },
    {
      "epoch": 6.790933333333333,
      "grad_norm": 0.22417129576206207,
      "learning_rate": 7.5566666666666674e-06,
      "loss": 0.0028,
      "step": 127330
    },
    {
      "epoch": 6.7914666666666665,
      "grad_norm": 0.08406608551740646,
      "learning_rate": 7.553333333333333e-06,
      "loss": 0.002,
      "step": 127340
    },
    {
      "epoch": 6.792,
      "grad_norm": 0.02802242897450924,
      "learning_rate": 7.55e-06,
      "loss": 0.003,
      "step": 127350
    },
    {
      "epoch": 6.792533333333333,
      "grad_norm": 0.028022799640893936,
      "learning_rate": 7.5466666666666675e-06,
      "loss": 0.0025,
      "step": 127360
    },
    {
      "epoch": 6.793066666666666,
      "grad_norm": 0.22417618334293365,
      "learning_rate": 7.5433333333333345e-06,
      "loss": 0.0026,
      "step": 127370
    },
    {
      "epoch": 6.7936,
      "grad_norm": 0.28022336959838867,
      "learning_rate": 7.54e-06,
      "loss": 0.0024,
      "step": 127380
    },
    {
      "epoch": 6.794133333333333,
      "grad_norm": 0.33626365661621094,
      "learning_rate": 7.536666666666667e-06,
      "loss": 0.0039,
      "step": 127390
    },
    {
      "epoch": 6.794666666666666,
      "grad_norm": 0.1401052176952362,
      "learning_rate": 7.533333333333334e-06,
      "loss": 0.0024,
      "step": 127400
    },
    {
      "epoch": 6.7952,
      "grad_norm": 0.14011067152023315,
      "learning_rate": 7.530000000000001e-06,
      "loss": 0.0026,
      "step": 127410
    },
    {
      "epoch": 6.795733333333334,
      "grad_norm": 0.05604434758424759,
      "learning_rate": 7.526666666666667e-06,
      "loss": 0.0026,
      "step": 127420
    },
    {
      "epoch": 6.796266666666667,
      "grad_norm": 0.2241782397031784,
      "learning_rate": 7.523333333333334e-06,
      "loss": 0.0028,
      "step": 127430
    },
    {
      "epoch": 6.7968,
      "grad_norm": 3.378971991097046e-09,
      "learning_rate": 7.520000000000001e-06,
      "loss": 0.0021,
      "step": 127440
    },
    {
      "epoch": 6.7973333333333334,
      "grad_norm": 0.2802175283432007,
      "learning_rate": 7.516666666666668e-06,
      "loss": 0.002,
      "step": 127450
    },
    {
      "epoch": 6.797866666666667,
      "grad_norm": 0.05604446306824684,
      "learning_rate": 7.513333333333333e-06,
      "loss": 0.0022,
      "step": 127460
    },
    {
      "epoch": 6.7984,
      "grad_norm": 0.22417688369750977,
      "learning_rate": 7.51e-06,
      "loss": 0.0027,
      "step": 127470
    },
    {
      "epoch": 6.798933333333333,
      "grad_norm": 0.1961515098810196,
      "learning_rate": 7.506666666666667e-06,
      "loss": 0.0019,
      "step": 127480
    },
    {
      "epoch": 6.7994666666666665,
      "grad_norm": 0.11208869516849518,
      "learning_rate": 7.503333333333333e-06,
      "loss": 0.003,
      "step": 127490
    },
    {
      "epoch": 6.8,
      "grad_norm": 0.0896858274936676,
      "learning_rate": 7.5e-06,
      "loss": 0.002,
      "step": 127500
    },
    {
      "epoch": 6.800533333333333,
      "grad_norm": 0.2241729348897934,
      "learning_rate": 7.496666666666667e-06,
      "loss": 0.0023,
      "step": 127510
    },
    {
      "epoch": 6.801066666666666,
      "grad_norm": 1.7600914059556771e-09,
      "learning_rate": 7.493333333333334e-06,
      "loss": 0.0018,
      "step": 127520
    },
    {
      "epoch": 6.8016,
      "grad_norm": 0.19615237414836884,
      "learning_rate": 7.4899999999999994e-06,
      "loss": 0.0021,
      "step": 127530
    },
    {
      "epoch": 6.802133333333334,
      "grad_norm": 0.14011117815971375,
      "learning_rate": 7.486666666666666e-06,
      "loss": 0.0028,
      "step": 127540
    },
    {
      "epoch": 6.802666666666667,
      "grad_norm": 0.16813381016254425,
      "learning_rate": 7.483333333333334e-06,
      "loss": 0.0033,
      "step": 127550
    },
    {
      "epoch": 6.8032,
      "grad_norm": 0.0560452863574028,
      "learning_rate": 7.480000000000001e-06,
      "loss": 0.0025,
      "step": 127560
    },
    {
      "epoch": 6.803733333333334,
      "grad_norm": 0.02802230231463909,
      "learning_rate": 7.4766666666666665e-06,
      "loss": 0.0029,
      "step": 127570
    },
    {
      "epoch": 6.804266666666667,
      "grad_norm": 0.28021612763404846,
      "learning_rate": 7.4733333333333335e-06,
      "loss": 0.0021,
      "step": 127580
    },
    {
      "epoch": 6.8048,
      "grad_norm": 0.16812683641910553,
      "learning_rate": 7.4700000000000005e-06,
      "loss": 0.0026,
      "step": 127590
    },
    {
      "epoch": 6.8053333333333335,
      "grad_norm": 0.2802216708660126,
      "learning_rate": 7.4666666666666675e-06,
      "loss": 0.0023,
      "step": 127600
    },
    {
      "epoch": 6.805866666666667,
      "grad_norm": 0.1401129812002182,
      "learning_rate": 7.463333333333334e-06,
      "loss": 0.0023,
      "step": 127610
    },
    {
      "epoch": 6.8064,
      "grad_norm": 0.11208870261907578,
      "learning_rate": 7.4600000000000006e-06,
      "loss": 0.0027,
      "step": 127620
    },
    {
      "epoch": 6.806933333333333,
      "grad_norm": 0.08406472206115723,
      "learning_rate": 7.4566666666666676e-06,
      "loss": 0.0018,
      "step": 127630
    },
    {
      "epoch": 6.8074666666666666,
      "grad_norm": 0.14010746777057648,
      "learning_rate": 7.453333333333333e-06,
      "loss": 0.0029,
      "step": 127640
    },
    {
      "epoch": 6.808,
      "grad_norm": 0.08406385034322739,
      "learning_rate": 7.45e-06,
      "loss": 0.0027,
      "step": 127650
    },
    {
      "epoch": 6.808533333333333,
      "grad_norm": 0.2521941065788269,
      "learning_rate": 7.446666666666667e-06,
      "loss": 0.003,
      "step": 127660
    },
    {
      "epoch": 6.809066666666666,
      "grad_norm": 0.34011951088905334,
      "learning_rate": 7.443333333333334e-06,
      "loss": 0.0025,
      "step": 127670
    },
    {
      "epoch": 6.8096,
      "grad_norm": 0.11208486557006836,
      "learning_rate": 7.44e-06,
      "loss": 0.0029,
      "step": 127680
    },
    {
      "epoch": 6.810133333333333,
      "grad_norm": 1.6758791208267212,
      "learning_rate": 7.436666666666667e-06,
      "loss": 0.0025,
      "step": 127690
    },
    {
      "epoch": 6.810666666666666,
      "grad_norm": 0.08406399190425873,
      "learning_rate": 7.433333333333334e-06,
      "loss": 0.003,
      "step": 127700
    },
    {
      "epoch": 6.8112,
      "grad_norm": 0.19614724814891815,
      "learning_rate": 7.430000000000001e-06,
      "loss": 0.0032,
      "step": 127710
    },
    {
      "epoch": 6.811733333333334,
      "grad_norm": 0.1401103138923645,
      "learning_rate": 7.426666666666666e-06,
      "loss": 0.0036,
      "step": 127720
    },
    {
      "epoch": 6.812266666666667,
      "grad_norm": 0.3923003673553467,
      "learning_rate": 7.423333333333333e-06,
      "loss": 0.0015,
      "step": 127730
    },
    {
      "epoch": 6.8128,
      "grad_norm": 0.36427414417266846,
      "learning_rate": 7.420000000000001e-06,
      "loss": 0.0022,
      "step": 127740
    },
    {
      "epoch": 6.8133333333333335,
      "grad_norm": 0.0840645357966423,
      "learning_rate": 7.416666666666668e-06,
      "loss": 0.0036,
      "step": 127750
    },
    {
      "epoch": 6.813866666666667,
      "grad_norm": 0.14010486006736755,
      "learning_rate": 7.413333333333333e-06,
      "loss": 0.0029,
      "step": 127760
    },
    {
      "epoch": 6.8144,
      "grad_norm": 2.435300849867872e-09,
      "learning_rate": 7.41e-06,
      "loss": 0.0029,
      "step": 127770
    },
    {
      "epoch": 6.814933333333333,
      "grad_norm": 0.25218892097473145,
      "learning_rate": 7.406666666666667e-06,
      "loss": 0.0025,
      "step": 127780
    },
    {
      "epoch": 6.815466666666667,
      "grad_norm": 0.14010584354400635,
      "learning_rate": 7.403333333333334e-06,
      "loss": 0.0024,
      "step": 127790
    },
    {
      "epoch": 6.816,
      "grad_norm": 0.05604275315999985,
      "learning_rate": 7.4e-06,
      "loss": 0.003,
      "step": 127800
    },
    {
      "epoch": 6.816533333333333,
      "grad_norm": 0.028021451085805893,
      "learning_rate": 7.396666666666667e-06,
      "loss": 0.003,
      "step": 127810
    },
    {
      "epoch": 6.817066666666666,
      "grad_norm": 9.744861584337627e-10,
      "learning_rate": 7.393333333333334e-06,
      "loss": 0.003,
      "step": 127820
    },
    {
      "epoch": 6.8176,
      "grad_norm": 0.11208605766296387,
      "learning_rate": 7.3899999999999995e-06,
      "loss": 0.0032,
      "step": 127830
    },
    {
      "epoch": 6.818133333333334,
      "grad_norm": 0.08406408876180649,
      "learning_rate": 7.3866666666666665e-06,
      "loss": 0.0038,
      "step": 127840
    },
    {
      "epoch": 6.818666666666667,
      "grad_norm": 0.2521957755088806,
      "learning_rate": 7.3833333333333335e-06,
      "loss": 0.004,
      "step": 127850
    },
    {
      "epoch": 6.8192,
      "grad_norm": 0.3082300126552582,
      "learning_rate": 7.3800000000000005e-06,
      "loss": 0.0033,
      "step": 127860
    },
    {
      "epoch": 6.819733333333334,
      "grad_norm": 0.16812820732593536,
      "learning_rate": 7.376666666666667e-06,
      "loss": 0.0036,
      "step": 127870
    },
    {
      "epoch": 6.820266666666667,
      "grad_norm": 0.1961551159620285,
      "learning_rate": 7.373333333333334e-06,
      "loss": 0.0021,
      "step": 127880
    },
    {
      "epoch": 6.8208,
      "grad_norm": 0.2521938681602478,
      "learning_rate": 7.370000000000001e-06,
      "loss": 0.0026,
      "step": 127890
    },
    {
      "epoch": 6.8213333333333335,
      "grad_norm": 0.2802095413208008,
      "learning_rate": 7.3666666666666676e-06,
      "loss": 0.0033,
      "step": 127900
    },
    {
      "epoch": 6.821866666666667,
      "grad_norm": 0.1401076763868332,
      "learning_rate": 7.363333333333333e-06,
      "loss": 0.0035,
      "step": 127910
    },
    {
      "epoch": 6.8224,
      "grad_norm": 0.25219228863716125,
      "learning_rate": 7.36e-06,
      "loss": 0.0028,
      "step": 127920
    },
    {
      "epoch": 6.822933333333333,
      "grad_norm": 0.028020834550261497,
      "learning_rate": 7.356666666666668e-06,
      "loss": 0.0023,
      "step": 127930
    },
    {
      "epoch": 6.823466666666667,
      "grad_norm": 0.1120864599943161,
      "learning_rate": 7.353333333333335e-06,
      "loss": 0.003,
      "step": 127940
    },
    {
      "epoch": 6.824,
      "grad_norm": 0.6179476380348206,
      "learning_rate": 7.35e-06,
      "loss": 0.0048,
      "step": 127950
    },
    {
      "epoch": 6.824533333333333,
      "grad_norm": 0.22417087852954865,
      "learning_rate": 7.346666666666667e-06,
      "loss": 0.003,
      "step": 127960
    },
    {
      "epoch": 6.825066666666666,
      "grad_norm": 4.22958379431293e-09,
      "learning_rate": 7.343333333333334e-06,
      "loss": 0.0024,
      "step": 127970
    },
    {
      "epoch": 6.8256,
      "grad_norm": 2.7463644691749778e-09,
      "learning_rate": 7.340000000000001e-06,
      "loss": 0.0027,
      "step": 127980
    },
    {
      "epoch": 6.826133333333333,
      "grad_norm": 0.16598601639270782,
      "learning_rate": 7.336666666666667e-06,
      "loss": 0.0019,
      "step": 127990
    },
    {
      "epoch": 6.826666666666666,
      "grad_norm": 0.08659281581640244,
      "learning_rate": 7.333333333333334e-06,
      "loss": 0.0021,
      "step": 128000
    },
    {
      "epoch": 6.8272,
      "grad_norm": 0.056044578552246094,
      "learning_rate": 7.330000000000001e-06,
      "loss": 0.0028,
      "step": 128010
    },
    {
      "epoch": 6.827733333333334,
      "grad_norm": 0.028022386133670807,
      "learning_rate": 7.326666666666666e-06,
      "loss": 0.002,
      "step": 128020
    },
    {
      "epoch": 6.828266666666667,
      "grad_norm": 8.254700922805114e-09,
      "learning_rate": 7.323333333333333e-06,
      "loss": 0.0024,
      "step": 128030
    },
    {
      "epoch": 6.8288,
      "grad_norm": 1.2576093673706055,
      "learning_rate": 7.32e-06,
      "loss": 0.0053,
      "step": 128040
    },
    {
      "epoch": 6.8293333333333335,
      "grad_norm": 0.11208464950323105,
      "learning_rate": 7.316666666666667e-06,
      "loss": 0.0017,
      "step": 128050
    },
    {
      "epoch": 6.829866666666667,
      "grad_norm": 0.05604204535484314,
      "learning_rate": 7.313333333333333e-06,
      "loss": 0.0024,
      "step": 128060
    },
    {
      "epoch": 6.8304,
      "grad_norm": 0.14010673761367798,
      "learning_rate": 7.31e-06,
      "loss": 0.0027,
      "step": 128070
    },
    {
      "epoch": 6.830933333333333,
      "grad_norm": 1.6959323945187066e-09,
      "learning_rate": 7.306666666666667e-06,
      "loss": 0.0029,
      "step": 128080
    },
    {
      "epoch": 6.831466666666667,
      "grad_norm": 0.056042976677417755,
      "learning_rate": 7.303333333333334e-06,
      "loss": 0.0023,
      "step": 128090
    },
    {
      "epoch": 6.832,
      "grad_norm": 0.1752256602048874,
      "learning_rate": 7.2999999999999996e-06,
      "loss": 0.0028,
      "step": 128100
    },
    {
      "epoch": 6.832533333333333,
      "grad_norm": 0.22417370975017548,
      "learning_rate": 7.2966666666666665e-06,
      "loss": 0.0029,
      "step": 128110
    },
    {
      "epoch": 6.833066666666666,
      "grad_norm": 0.19614899158477783,
      "learning_rate": 7.293333333333334e-06,
      "loss": 0.0021,
      "step": 128120
    },
    {
      "epoch": 6.8336,
      "grad_norm": 0.2802105247974396,
      "learning_rate": 7.290000000000001e-06,
      "loss": 0.0022,
      "step": 128130
    },
    {
      "epoch": 6.834133333333333,
      "grad_norm": 0.19614671170711517,
      "learning_rate": 7.286666666666667e-06,
      "loss": 0.0037,
      "step": 128140
    },
    {
      "epoch": 6.834666666666667,
      "grad_norm": 0.028020931407809258,
      "learning_rate": 7.283333333333334e-06,
      "loss": 0.0023,
      "step": 128150
    },
    {
      "epoch": 6.8352,
      "grad_norm": 0.02802104689180851,
      "learning_rate": 7.280000000000001e-06,
      "loss": 0.0027,
      "step": 128160
    },
    {
      "epoch": 6.835733333333334,
      "grad_norm": 0.22417566180229187,
      "learning_rate": 7.276666666666667e-06,
      "loss": 0.0019,
      "step": 128170
    },
    {
      "epoch": 6.836266666666667,
      "grad_norm": 0.14010711014270782,
      "learning_rate": 7.273333333333334e-06,
      "loss": 0.0024,
      "step": 128180
    },
    {
      "epoch": 6.8368,
      "grad_norm": 0.11208339780569077,
      "learning_rate": 7.270000000000001e-06,
      "loss": 0.0022,
      "step": 128190
    },
    {
      "epoch": 6.8373333333333335,
      "grad_norm": 0.14010243117809296,
      "learning_rate": 7.266666666666668e-06,
      "loss": 0.0023,
      "step": 128200
    },
    {
      "epoch": 6.837866666666667,
      "grad_norm": 0.16812650859355927,
      "learning_rate": 7.263333333333333e-06,
      "loss": 0.0019,
      "step": 128210
    },
    {
      "epoch": 6.8384,
      "grad_norm": 0.22416448593139648,
      "learning_rate": 7.26e-06,
      "loss": 0.0024,
      "step": 128220
    },
    {
      "epoch": 6.838933333333333,
      "grad_norm": 0.08406306058168411,
      "learning_rate": 7.256666666666667e-06,
      "loss": 0.0017,
      "step": 128230
    },
    {
      "epoch": 6.839466666666667,
      "grad_norm": 0.056044358760118484,
      "learning_rate": 7.253333333333334e-06,
      "loss": 0.0026,
      "step": 128240
    },
    {
      "epoch": 6.84,
      "grad_norm": 0.11208751797676086,
      "learning_rate": 7.25e-06,
      "loss": 0.0027,
      "step": 128250
    },
    {
      "epoch": 6.840533333333333,
      "grad_norm": 0.10904157161712646,
      "learning_rate": 7.246666666666667e-06,
      "loss": 0.0033,
      "step": 128260
    },
    {
      "epoch": 6.841066666666666,
      "grad_norm": 0.08406341820955276,
      "learning_rate": 7.243333333333334e-06,
      "loss": 0.0032,
      "step": 128270
    },
    {
      "epoch": 6.8416,
      "grad_norm": 0.30824345350265503,
      "learning_rate": 7.240000000000001e-06,
      "loss": 0.0025,
      "step": 128280
    },
    {
      "epoch": 6.842133333333333,
      "grad_norm": 0.05604523420333862,
      "learning_rate": 7.236666666666666e-06,
      "loss": 0.0033,
      "step": 128290
    },
    {
      "epoch": 6.842666666666666,
      "grad_norm": 0.05604458972811699,
      "learning_rate": 7.233333333333333e-06,
      "loss": 0.0029,
      "step": 128300
    },
    {
      "epoch": 6.8431999999999995,
      "grad_norm": 0.3923017382621765,
      "learning_rate": 7.230000000000001e-06,
      "loss": 0.002,
      "step": 128310
    },
    {
      "epoch": 6.843733333333334,
      "grad_norm": 0.33625391125679016,
      "learning_rate": 7.226666666666668e-06,
      "loss": 0.0025,
      "step": 128320
    },
    {
      "epoch": 6.844266666666667,
      "grad_norm": 0.25218909978866577,
      "learning_rate": 7.223333333333333e-06,
      "loss": 0.0029,
      "step": 128330
    },
    {
      "epoch": 6.8448,
      "grad_norm": 4.7575312578374e-09,
      "learning_rate": 7.22e-06,
      "loss": 0.0024,
      "step": 128340
    },
    {
      "epoch": 6.8453333333333335,
      "grad_norm": 0.19614365696907043,
      "learning_rate": 7.216666666666667e-06,
      "loss": 0.0022,
      "step": 128350
    },
    {
      "epoch": 6.845866666666667,
      "grad_norm": 0.08406449109315872,
      "learning_rate": 7.2133333333333334e-06,
      "loss": 0.0016,
      "step": 128360
    },
    {
      "epoch": 6.8464,
      "grad_norm": 0.08406617492437363,
      "learning_rate": 7.2100000000000004e-06,
      "loss": 0.0029,
      "step": 128370
    },
    {
      "epoch": 6.846933333333333,
      "grad_norm": 0.112083800137043,
      "learning_rate": 7.206666666666667e-06,
      "loss": 0.0035,
      "step": 128380
    },
    {
      "epoch": 6.847466666666667,
      "grad_norm": 0.22416840493679047,
      "learning_rate": 7.203333333333334e-06,
      "loss": 0.0024,
      "step": 128390
    },
    {
      "epoch": 6.848,
      "grad_norm": 0.2521887719631195,
      "learning_rate": 7.2e-06,
      "loss": 0.0035,
      "step": 128400
    },
    {
      "epoch": 6.848533333333333,
      "grad_norm": 0.39229363203048706,
      "learning_rate": 7.196666666666667e-06,
      "loss": 0.0037,
      "step": 128410
    },
    {
      "epoch": 6.849066666666666,
      "grad_norm": 0.16812962293624878,
      "learning_rate": 7.193333333333334e-06,
      "loss": 0.0023,
      "step": 128420
    },
    {
      "epoch": 6.8496,
      "grad_norm": 0.11208827048540115,
      "learning_rate": 7.190000000000001e-06,
      "loss": 0.0013,
      "step": 128430
    },
    {
      "epoch": 6.850133333333333,
      "grad_norm": 0.11209025233983994,
      "learning_rate": 7.186666666666667e-06,
      "loss": 0.0028,
      "step": 128440
    },
    {
      "epoch": 6.850666666666667,
      "grad_norm": 0.14011220633983612,
      "learning_rate": 7.183333333333334e-06,
      "loss": 0.0027,
      "step": 128450
    },
    {
      "epoch": 6.8512,
      "grad_norm": 0.05604465678334236,
      "learning_rate": 7.180000000000001e-06,
      "loss": 0.0045,
      "step": 128460
    },
    {
      "epoch": 6.851733333333334,
      "grad_norm": 0.3082410991191864,
      "learning_rate": 7.176666666666668e-06,
      "loss": 0.0028,
      "step": 128470
    },
    {
      "epoch": 6.852266666666667,
      "grad_norm": 0.11208499222993851,
      "learning_rate": 7.173333333333333e-06,
      "loss": 0.0029,
      "step": 128480
    },
    {
      "epoch": 6.8528,
      "grad_norm": 1.9143168926239014,
      "learning_rate": 7.17e-06,
      "loss": 0.0032,
      "step": 128490
    },
    {
      "epoch": 6.8533333333333335,
      "grad_norm": 0.056042373180389404,
      "learning_rate": 7.166666666666667e-06,
      "loss": 0.0038,
      "step": 128500
    },
    {
      "epoch": 6.853866666666667,
      "grad_norm": 0.1401050090789795,
      "learning_rate": 7.163333333333333e-06,
      "loss": 0.0032,
      "step": 128510
    },
    {
      "epoch": 6.8544,
      "grad_norm": 0.2522006630897522,
      "learning_rate": 7.16e-06,
      "loss": 0.0019,
      "step": 128520
    },
    {
      "epoch": 6.854933333333333,
      "grad_norm": 0.2802233397960663,
      "learning_rate": 7.156666666666667e-06,
      "loss": 0.004,
      "step": 128530
    },
    {
      "epoch": 6.855466666666667,
      "grad_norm": 0.05604173243045807,
      "learning_rate": 7.153333333333334e-06,
      "loss": 0.0015,
      "step": 128540
    },
    {
      "epoch": 6.856,
      "grad_norm": 0.08406419306993484,
      "learning_rate": 7.15e-06,
      "loss": 0.0023,
      "step": 128550
    },
    {
      "epoch": 6.856533333333333,
      "grad_norm": 0.1120905950665474,
      "learning_rate": 7.146666666666667e-06,
      "loss": 0.0017,
      "step": 128560
    },
    {
      "epoch": 6.857066666666666,
      "grad_norm": 0.05604612082242966,
      "learning_rate": 7.143333333333334e-06,
      "loss": 0.0014,
      "step": 128570
    },
    {
      "epoch": 6.8576,
      "grad_norm": 0.16813553869724274,
      "learning_rate": 7.140000000000001e-06,
      "loss": 0.0028,
      "step": 128580
    },
    {
      "epoch": 6.858133333333333,
      "grad_norm": 0.22417333722114563,
      "learning_rate": 7.136666666666666e-06,
      "loss": 0.002,
      "step": 128590
    },
    {
      "epoch": 6.858666666666666,
      "grad_norm": 0.05604061856865883,
      "learning_rate": 7.133333333333333e-06,
      "loss": 0.0018,
      "step": 128600
    },
    {
      "epoch": 6.8591999999999995,
      "grad_norm": 0.22416867315769196,
      "learning_rate": 7.13e-06,
      "loss": 0.0018,
      "step": 128610
    },
    {
      "epoch": 6.859733333333334,
      "grad_norm": 0.22417603433132172,
      "learning_rate": 7.126666666666667e-06,
      "loss": 0.0026,
      "step": 128620
    },
    {
      "epoch": 6.860266666666667,
      "grad_norm": 0.2802218496799469,
      "learning_rate": 7.1233333333333335e-06,
      "loss": 0.0031,
      "step": 128630
    },
    {
      "epoch": 6.8608,
      "grad_norm": 0.3362503945827484,
      "learning_rate": 7.1200000000000004e-06,
      "loss": 0.0041,
      "step": 128640
    },
    {
      "epoch": 6.8613333333333335,
      "grad_norm": 0.11208431422710419,
      "learning_rate": 7.116666666666667e-06,
      "loss": 0.0031,
      "step": 128650
    },
    {
      "epoch": 6.861866666666667,
      "grad_norm": 0.0560431182384491,
      "learning_rate": 7.113333333333334e-06,
      "loss": 0.0016,
      "step": 128660
    },
    {
      "epoch": 6.8624,
      "grad_norm": 0.22417165338993073,
      "learning_rate": 7.11e-06,
      "loss": 0.0029,
      "step": 128670
    },
    {
      "epoch": 6.862933333333333,
      "grad_norm": 0.028021955862641335,
      "learning_rate": 7.106666666666667e-06,
      "loss": 0.0022,
      "step": 128680
    },
    {
      "epoch": 6.863466666666667,
      "grad_norm": 0.14010635018348694,
      "learning_rate": 7.103333333333334e-06,
      "loss": 0.003,
      "step": 128690
    },
    {
      "epoch": 6.864,
      "grad_norm": 0.0840621292591095,
      "learning_rate": 7.1e-06,
      "loss": 0.0038,
      "step": 128700
    },
    {
      "epoch": 6.864533333333333,
      "grad_norm": 0.9169790148735046,
      "learning_rate": 7.096666666666667e-06,
      "loss": 0.0032,
      "step": 128710
    },
    {
      "epoch": 6.865066666666666,
      "grad_norm": 0.09147097170352936,
      "learning_rate": 7.093333333333334e-06,
      "loss": 0.0024,
      "step": 128720
    },
    {
      "epoch": 6.8656,
      "grad_norm": 1.5743448766869506e-09,
      "learning_rate": 7.090000000000001e-06,
      "loss": 0.0024,
      "step": 128730
    },
    {
      "epoch": 6.866133333333333,
      "grad_norm": 0.02802136354148388,
      "learning_rate": 7.086666666666667e-06,
      "loss": 0.0024,
      "step": 128740
    },
    {
      "epoch": 6.866666666666667,
      "grad_norm": 0.25218722224235535,
      "learning_rate": 7.083333333333334e-06,
      "loss": 0.0021,
      "step": 128750
    },
    {
      "epoch": 6.8672,
      "grad_norm": 0.25218939781188965,
      "learning_rate": 7.080000000000001e-06,
      "loss": 0.003,
      "step": 128760
    },
    {
      "epoch": 6.867733333333334,
      "grad_norm": 0.08406376093626022,
      "learning_rate": 7.076666666666668e-06,
      "loss": 0.0025,
      "step": 128770
    },
    {
      "epoch": 6.868266666666667,
      "grad_norm": 0.2521868944168091,
      "learning_rate": 7.073333333333333e-06,
      "loss": 0.0015,
      "step": 128780
    },
    {
      "epoch": 6.8688,
      "grad_norm": 0.11208397150039673,
      "learning_rate": 7.07e-06,
      "loss": 0.0025,
      "step": 128790
    },
    {
      "epoch": 6.8693333333333335,
      "grad_norm": 0.22417257726192474,
      "learning_rate": 7.066666666666667e-06,
      "loss": 0.003,
      "step": 128800
    },
    {
      "epoch": 6.869866666666667,
      "grad_norm": 0.28021520376205444,
      "learning_rate": 7.063333333333334e-06,
      "loss": 0.0032,
      "step": 128810
    },
    {
      "epoch": 6.8704,
      "grad_norm": 0.1681278795003891,
      "learning_rate": 7.06e-06,
      "loss": 0.0015,
      "step": 128820
    },
    {
      "epoch": 6.870933333333333,
      "grad_norm": 0.25219476222991943,
      "learning_rate": 7.056666666666667e-06,
      "loss": 0.0028,
      "step": 128830
    },
    {
      "epoch": 6.871466666666667,
      "grad_norm": 0.19615177810192108,
      "learning_rate": 7.053333333333334e-06,
      "loss": 0.0018,
      "step": 128840
    },
    {
      "epoch": 6.872,
      "grad_norm": 0.2521863877773285,
      "learning_rate": 7.049999999999999e-06,
      "loss": 0.0027,
      "step": 128850
    },
    {
      "epoch": 6.872533333333333,
      "grad_norm": 0.11208268254995346,
      "learning_rate": 7.046666666666666e-06,
      "loss": 0.0018,
      "step": 128860
    },
    {
      "epoch": 6.873066666666666,
      "grad_norm": 0.14010608196258545,
      "learning_rate": 7.043333333333333e-06,
      "loss": 0.0032,
      "step": 128870
    },
    {
      "epoch": 6.8736,
      "grad_norm": 0.1401049941778183,
      "learning_rate": 7.04e-06,
      "loss": 0.0039,
      "step": 128880
    },
    {
      "epoch": 6.874133333333333,
      "grad_norm": 0.11208326369524002,
      "learning_rate": 7.0366666666666665e-06,
      "loss": 0.0024,
      "step": 128890
    },
    {
      "epoch": 6.874666666666666,
      "grad_norm": 0.05604126676917076,
      "learning_rate": 7.0333333333333335e-06,
      "loss": 0.0023,
      "step": 128900
    },
    {
      "epoch": 6.8751999999999995,
      "grad_norm": 0.22416968643665314,
      "learning_rate": 7.0300000000000005e-06,
      "loss": 0.0027,
      "step": 128910
    },
    {
      "epoch": 6.875733333333334,
      "grad_norm": 0.14010533690452576,
      "learning_rate": 7.0266666666666674e-06,
      "loss": 0.0028,
      "step": 128920
    },
    {
      "epoch": 6.876266666666667,
      "grad_norm": 0.14010637998580933,
      "learning_rate": 7.0233333333333336e-06,
      "loss": 0.0039,
      "step": 128930
    },
    {
      "epoch": 6.8768,
      "grad_norm": 0.1401028335094452,
      "learning_rate": 7.0200000000000006e-06,
      "loss": 0.0027,
      "step": 128940
    },
    {
      "epoch": 6.8773333333333335,
      "grad_norm": 0.11208201944828033,
      "learning_rate": 7.0166666666666675e-06,
      "loss": 0.0026,
      "step": 128950
    },
    {
      "epoch": 6.877866666666667,
      "grad_norm": 0.19614745676517487,
      "learning_rate": 7.0133333333333345e-06,
      "loss": 0.0022,
      "step": 128960
    },
    {
      "epoch": 6.8784,
      "grad_norm": 0.25219088792800903,
      "learning_rate": 7.01e-06,
      "loss": 0.0025,
      "step": 128970
    },
    {
      "epoch": 6.878933333333333,
      "grad_norm": 0.11208062618970871,
      "learning_rate": 7.006666666666667e-06,
      "loss": 0.0024,
      "step": 128980
    },
    {
      "epoch": 6.879466666666667,
      "grad_norm": 0.14010021090507507,
      "learning_rate": 7.003333333333334e-06,
      "loss": 0.0024,
      "step": 128990
    },
    {
      "epoch": 6.88,
      "grad_norm": 0.168126180768013,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.0029,
      "step": 129000
    },
    {
      "epoch": 6.880533333333333,
      "grad_norm": 0.05604217201471329,
      "learning_rate": 6.996666666666667e-06,
      "loss": 0.0025,
      "step": 129010
    },
    {
      "epoch": 6.881066666666666,
      "grad_norm": 0.28020933270454407,
      "learning_rate": 6.993333333333334e-06,
      "loss": 0.0024,
      "step": 129020
    },
    {
      "epoch": 6.8816,
      "grad_norm": 0.22416354715824127,
      "learning_rate": 6.990000000000001e-06,
      "loss": 0.0029,
      "step": 129030
    },
    {
      "epoch": 6.882133333333333,
      "grad_norm": 0.11208205670118332,
      "learning_rate": 6.986666666666666e-06,
      "loss": 0.002,
      "step": 129040
    },
    {
      "epoch": 6.882666666666667,
      "grad_norm": 0.028020642697811127,
      "learning_rate": 6.983333333333333e-06,
      "loss": 0.0028,
      "step": 129050
    },
    {
      "epoch": 6.8832,
      "grad_norm": 0.2802157998085022,
      "learning_rate": 6.98e-06,
      "loss": 0.0032,
      "step": 129060
    },
    {
      "epoch": 6.883733333333334,
      "grad_norm": 0.4203243851661682,
      "learning_rate": 6.976666666666667e-06,
      "loss": 0.0032,
      "step": 129070
    },
    {
      "epoch": 6.884266666666667,
      "grad_norm": 0.028021303936839104,
      "learning_rate": 6.973333333333333e-06,
      "loss": 0.0015,
      "step": 129080
    },
    {
      "epoch": 6.8848,
      "grad_norm": 0.42030268907546997,
      "learning_rate": 6.97e-06,
      "loss": 0.0029,
      "step": 129090
    },
    {
      "epoch": 6.8853333333333335,
      "grad_norm": 0.028020422905683517,
      "learning_rate": 6.966666666666667e-06,
      "loss": 0.0033,
      "step": 129100
    },
    {
      "epoch": 6.885866666666667,
      "grad_norm": 0.5323870778083801,
      "learning_rate": 6.963333333333334e-06,
      "loss": 0.0027,
      "step": 129110
    },
    {
      "epoch": 6.8864,
      "grad_norm": 0.1961473524570465,
      "learning_rate": 6.9599999999999994e-06,
      "loss": 0.0019,
      "step": 129120
    },
    {
      "epoch": 6.886933333333333,
      "grad_norm": 0.16812607645988464,
      "learning_rate": 6.956666666666667e-06,
      "loss": 0.0032,
      "step": 129130
    },
    {
      "epoch": 6.887466666666667,
      "grad_norm": 0.19614006578922272,
      "learning_rate": 6.953333333333334e-06,
      "loss": 0.0038,
      "step": 129140
    },
    {
      "epoch": 6.888,
      "grad_norm": 0.19613690674304962,
      "learning_rate": 6.950000000000001e-06,
      "loss": 0.0035,
      "step": 129150
    },
    {
      "epoch": 6.888533333333333,
      "grad_norm": 0.2802068889141083,
      "learning_rate": 6.9466666666666665e-06,
      "loss": 0.0034,
      "step": 129160
    },
    {
      "epoch": 6.8890666666666664,
      "grad_norm": 0.05604046955704689,
      "learning_rate": 6.9433333333333335e-06,
      "loss": 0.0031,
      "step": 129170
    },
    {
      "epoch": 6.8896,
      "grad_norm": 0.05604107305407524,
      "learning_rate": 6.9400000000000005e-06,
      "loss": 0.003,
      "step": 129180
    },
    {
      "epoch": 6.890133333333333,
      "grad_norm": 0.33624693751335144,
      "learning_rate": 6.936666666666667e-06,
      "loss": 0.0025,
      "step": 129190
    },
    {
      "epoch": 6.890666666666666,
      "grad_norm": 0.2521778345108032,
      "learning_rate": 6.933333333333334e-06,
      "loss": 0.0019,
      "step": 129200
    },
    {
      "epoch": 6.8911999999999995,
      "grad_norm": 0.22415713965892792,
      "learning_rate": 6.9300000000000006e-06,
      "loss": 0.0023,
      "step": 129210
    },
    {
      "epoch": 6.891733333333334,
      "grad_norm": 0.08406306058168411,
      "learning_rate": 6.9266666666666675e-06,
      "loss": 0.0029,
      "step": 129220
    },
    {
      "epoch": 6.892266666666667,
      "grad_norm": 0.19615285098552704,
      "learning_rate": 6.923333333333333e-06,
      "loss": 0.0031,
      "step": 129230
    },
    {
      "epoch": 6.8928,
      "grad_norm": 3.2749754019789634e-09,
      "learning_rate": 6.92e-06,
      "loss": 0.0039,
      "step": 129240
    },
    {
      "epoch": 6.8933333333333335,
      "grad_norm": 0.16811954975128174,
      "learning_rate": 6.916666666666667e-06,
      "loss": 0.0028,
      "step": 129250
    },
    {
      "epoch": 6.893866666666667,
      "grad_norm": 0.028020083904266357,
      "learning_rate": 6.913333333333334e-06,
      "loss": 0.0024,
      "step": 129260
    },
    {
      "epoch": 6.8944,
      "grad_norm": 3.4563276685162236e-09,
      "learning_rate": 6.91e-06,
      "loss": 0.0022,
      "step": 129270
    },
    {
      "epoch": 6.894933333333333,
      "grad_norm": 0.11207913607358932,
      "learning_rate": 6.906666666666667e-06,
      "loss": 0.0029,
      "step": 129280
    },
    {
      "epoch": 6.895466666666667,
      "grad_norm": 0.22416159510612488,
      "learning_rate": 6.903333333333334e-06,
      "loss": 0.0021,
      "step": 129290
    },
    {
      "epoch": 6.896,
      "grad_norm": 0.1681198626756668,
      "learning_rate": 6.900000000000001e-06,
      "loss": 0.0029,
      "step": 129300
    },
    {
      "epoch": 6.896533333333333,
      "grad_norm": 0.05927883833646774,
      "learning_rate": 6.896666666666666e-06,
      "loss": 0.0039,
      "step": 129310
    },
    {
      "epoch": 6.8970666666666665,
      "grad_norm": 0.2801973819732666,
      "learning_rate": 6.893333333333334e-06,
      "loss": 0.0021,
      "step": 129320
    },
    {
      "epoch": 6.8976,
      "grad_norm": 0.1681201010942459,
      "learning_rate": 6.890000000000001e-06,
      "loss": 0.0027,
      "step": 129330
    },
    {
      "epoch": 6.898133333333333,
      "grad_norm": 0.29000991582870483,
      "learning_rate": 6.886666666666668e-06,
      "loss": 0.0029,
      "step": 129340
    },
    {
      "epoch": 6.898666666666666,
      "grad_norm": 0.08405935019254684,
      "learning_rate": 6.883333333333333e-06,
      "loss": 0.0026,
      "step": 129350
    },
    {
      "epoch": 6.8992,
      "grad_norm": 0.2802005410194397,
      "learning_rate": 6.88e-06,
      "loss": 0.0033,
      "step": 129360
    },
    {
      "epoch": 6.899733333333334,
      "grad_norm": 0.05604114755988121,
      "learning_rate": 6.876666666666667e-06,
      "loss": 0.0031,
      "step": 129370
    },
    {
      "epoch": 6.900266666666667,
      "grad_norm": 0.08405981212854385,
      "learning_rate": 6.873333333333333e-06,
      "loss": 0.003,
      "step": 129380
    },
    {
      "epoch": 6.9008,
      "grad_norm": 0.14009790122509003,
      "learning_rate": 6.87e-06,
      "loss": 0.0023,
      "step": 129390
    },
    {
      "epoch": 6.9013333333333335,
      "grad_norm": 0.3159504234790802,
      "learning_rate": 6.866666666666667e-06,
      "loss": 0.0037,
      "step": 129400
    },
    {
      "epoch": 6.901866666666667,
      "grad_norm": 0.4056418538093567,
      "learning_rate": 6.863333333333334e-06,
      "loss": 0.0028,
      "step": 129410
    },
    {
      "epoch": 6.9024,
      "grad_norm": 0.2521751821041107,
      "learning_rate": 6.8599999999999995e-06,
      "loss": 0.0024,
      "step": 129420
    },
    {
      "epoch": 6.902933333333333,
      "grad_norm": 0.22416451573371887,
      "learning_rate": 6.8566666666666665e-06,
      "loss": 0.0023,
      "step": 129430
    },
    {
      "epoch": 6.903466666666667,
      "grad_norm": 0.44832926988601685,
      "learning_rate": 6.8533333333333335e-06,
      "loss": 0.0019,
      "step": 129440
    },
    {
      "epoch": 6.904,
      "grad_norm": 0.11207994818687439,
      "learning_rate": 6.8500000000000005e-06,
      "loss": 0.0016,
      "step": 129450
    },
    {
      "epoch": 6.904533333333333,
      "grad_norm": 0.19929741322994232,
      "learning_rate": 6.846666666666667e-06,
      "loss": 0.0025,
      "step": 129460
    },
    {
      "epoch": 6.9050666666666665,
      "grad_norm": 0.6959212422370911,
      "learning_rate": 6.843333333333334e-06,
      "loss": 0.0015,
      "step": 129470
    },
    {
      "epoch": 6.9056,
      "grad_norm": 0.1401086300611496,
      "learning_rate": 6.840000000000001e-06,
      "loss": 0.0026,
      "step": 129480
    },
    {
      "epoch": 6.906133333333333,
      "grad_norm": 0.6507498025894165,
      "learning_rate": 6.8366666666666676e-06,
      "loss": 0.0025,
      "step": 129490
    },
    {
      "epoch": 6.906666666666666,
      "grad_norm": 0.2521734833717346,
      "learning_rate": 6.833333333333333e-06,
      "loss": 0.0028,
      "step": 129500
    },
    {
      "epoch": 6.9072,
      "grad_norm": 0.17580565810203552,
      "learning_rate": 6.830000000000001e-06,
      "loss": 0.0043,
      "step": 129510
    },
    {
      "epoch": 6.907733333333333,
      "grad_norm": 0.1681256741285324,
      "learning_rate": 6.826666666666668e-06,
      "loss": 0.0025,
      "step": 129520
    },
    {
      "epoch": 6.908266666666667,
      "grad_norm": 0.14010247588157654,
      "learning_rate": 6.823333333333333e-06,
      "loss": 0.0043,
      "step": 129530
    },
    {
      "epoch": 6.9088,
      "grad_norm": 0.2801959812641144,
      "learning_rate": 6.82e-06,
      "loss": 0.0022,
      "step": 129540
    },
    {
      "epoch": 6.9093333333333335,
      "grad_norm": 0.25217923521995544,
      "learning_rate": 6.816666666666667e-06,
      "loss": 0.0018,
      "step": 129550
    },
    {
      "epoch": 6.909866666666667,
      "grad_norm": 0.952156126499176,
      "learning_rate": 6.813333333333334e-06,
      "loss": 0.0035,
      "step": 129560
    },
    {
      "epoch": 6.9104,
      "grad_norm": 0.16812212765216827,
      "learning_rate": 6.81e-06,
      "loss": 0.0026,
      "step": 129570
    },
    {
      "epoch": 6.910933333333333,
      "grad_norm": 0.02801995724439621,
      "learning_rate": 6.806666666666667e-06,
      "loss": 0.0029,
      "step": 129580
    },
    {
      "epoch": 6.911466666666667,
      "grad_norm": 0.13828709721565247,
      "learning_rate": 6.803333333333334e-06,
      "loss": 0.0023,
      "step": 129590
    },
    {
      "epoch": 6.912,
      "grad_norm": 0.1961461901664734,
      "learning_rate": 6.800000000000001e-06,
      "loss": 0.0021,
      "step": 129600
    },
    {
      "epoch": 6.912533333333333,
      "grad_norm": 0.1961522102355957,
      "learning_rate": 6.796666666666666e-06,
      "loss": 0.0031,
      "step": 129610
    },
    {
      "epoch": 6.9130666666666665,
      "grad_norm": 0.05604104697704315,
      "learning_rate": 6.793333333333333e-06,
      "loss": 0.0025,
      "step": 129620
    },
    {
      "epoch": 6.9136,
      "grad_norm": 0.056041277945041656,
      "learning_rate": 6.79e-06,
      "loss": 0.0018,
      "step": 129630
    },
    {
      "epoch": 6.914133333333333,
      "grad_norm": 1.2602880001068115,
      "learning_rate": 6.786666666666667e-06,
      "loss": 0.0031,
      "step": 129640
    },
    {
      "epoch": 6.914666666666666,
      "grad_norm": 0.028020065277814865,
      "learning_rate": 6.783333333333333e-06,
      "loss": 0.0019,
      "step": 129650
    },
    {
      "epoch": 6.9152000000000005,
      "grad_norm": 0.056040532886981964,
      "learning_rate": 6.78e-06,
      "loss": 0.002,
      "step": 129660
    },
    {
      "epoch": 6.915733333333334,
      "grad_norm": 0.4484553337097168,
      "learning_rate": 6.776666666666667e-06,
      "loss": 0.0031,
      "step": 129670
    },
    {
      "epoch": 6.916266666666667,
      "grad_norm": 0.16812032461166382,
      "learning_rate": 6.773333333333334e-06,
      "loss": 0.0035,
      "step": 129680
    },
    {
      "epoch": 6.9168,
      "grad_norm": 0.392289400100708,
      "learning_rate": 6.7699999999999996e-06,
      "loss": 0.0024,
      "step": 129690
    },
    {
      "epoch": 6.917333333333334,
      "grad_norm": 0.3082183599472046,
      "learning_rate": 6.766666666666667e-06,
      "loss": 0.0018,
      "step": 129700
    },
    {
      "epoch": 6.917866666666667,
      "grad_norm": 0.11208242177963257,
      "learning_rate": 6.763333333333334e-06,
      "loss": 0.0025,
      "step": 129710
    },
    {
      "epoch": 6.9184,
      "grad_norm": 0.14010466635227203,
      "learning_rate": 6.76e-06,
      "loss": 0.0031,
      "step": 129720
    },
    {
      "epoch": 6.918933333333333,
      "grad_norm": 0.08406270295381546,
      "learning_rate": 6.756666666666667e-06,
      "loss": 0.0027,
      "step": 129730
    },
    {
      "epoch": 6.919466666666667,
      "grad_norm": 0.05604131519794464,
      "learning_rate": 6.753333333333334e-06,
      "loss": 0.0019,
      "step": 129740
    },
    {
      "epoch": 6.92,
      "grad_norm": 0.08406300097703934,
      "learning_rate": 6.750000000000001e-06,
      "loss": 0.0023,
      "step": 129750
    },
    {
      "epoch": 6.920533333333333,
      "grad_norm": 0.05604269728064537,
      "learning_rate": 6.746666666666667e-06,
      "loss": 0.0027,
      "step": 129760
    },
    {
      "epoch": 6.9210666666666665,
      "grad_norm": 0.056041695177555084,
      "learning_rate": 6.743333333333334e-06,
      "loss": 0.0031,
      "step": 129770
    },
    {
      "epoch": 6.9216,
      "grad_norm": 0.11208366602659225,
      "learning_rate": 6.740000000000001e-06,
      "loss": 0.0024,
      "step": 129780
    },
    {
      "epoch": 6.922133333333333,
      "grad_norm": 0.14010408520698547,
      "learning_rate": 6.736666666666668e-06,
      "loss": 0.0024,
      "step": 129790
    },
    {
      "epoch": 6.922666666666666,
      "grad_norm": 0.14010199904441833,
      "learning_rate": 6.733333333333333e-06,
      "loss": 0.0027,
      "step": 129800
    },
    {
      "epoch": 6.9232,
      "grad_norm": 0.05603946000337601,
      "learning_rate": 6.73e-06,
      "loss": 0.0028,
      "step": 129810
    },
    {
      "epoch": 6.923733333333333,
      "grad_norm": 0.1681230664253235,
      "learning_rate": 6.726666666666667e-06,
      "loss": 0.004,
      "step": 129820
    },
    {
      "epoch": 6.924266666666667,
      "grad_norm": 0.1961476355791092,
      "learning_rate": 6.723333333333334e-06,
      "loss": 0.0024,
      "step": 129830
    },
    {
      "epoch": 6.9248,
      "grad_norm": 0.2241692990064621,
      "learning_rate": 6.72e-06,
      "loss": 0.0031,
      "step": 129840
    },
    {
      "epoch": 6.925333333333334,
      "grad_norm": 0.14010272920131683,
      "learning_rate": 6.716666666666667e-06,
      "loss": 0.0038,
      "step": 129850
    },
    {
      "epoch": 6.925866666666667,
      "grad_norm": 0.2801995277404785,
      "learning_rate": 6.713333333333334e-06,
      "loss": 0.002,
      "step": 129860
    },
    {
      "epoch": 6.9264,
      "grad_norm": 0.22415955364704132,
      "learning_rate": 6.710000000000001e-06,
      "loss": 0.0024,
      "step": 129870
    },
    {
      "epoch": 6.926933333333333,
      "grad_norm": 0.11208071559667587,
      "learning_rate": 6.706666666666666e-06,
      "loss": 0.0029,
      "step": 129880
    },
    {
      "epoch": 6.927466666666667,
      "grad_norm": 0.08405894041061401,
      "learning_rate": 6.703333333333334e-06,
      "loss": 0.0032,
      "step": 129890
    },
    {
      "epoch": 6.928,
      "grad_norm": 0.11207807064056396,
      "learning_rate": 6.700000000000001e-06,
      "loss": 0.0022,
      "step": 129900
    },
    {
      "epoch": 6.928533333333333,
      "grad_norm": 0.056040797382593155,
      "learning_rate": 6.696666666666666e-06,
      "loss": 0.0025,
      "step": 129910
    },
    {
      "epoch": 6.9290666666666665,
      "grad_norm": 0.25218814611434937,
      "learning_rate": 6.693333333333333e-06,
      "loss": 0.0029,
      "step": 129920
    },
    {
      "epoch": 6.9296,
      "grad_norm": 0.02802009880542755,
      "learning_rate": 6.69e-06,
      "loss": 0.0015,
      "step": 129930
    },
    {
      "epoch": 6.930133333333333,
      "grad_norm": 0.05604018643498421,
      "learning_rate": 6.686666666666667e-06,
      "loss": 0.0024,
      "step": 129940
    },
    {
      "epoch": 6.930666666666666,
      "grad_norm": 0.08406253159046173,
      "learning_rate": 6.6833333333333334e-06,
      "loss": 0.0015,
      "step": 129950
    },
    {
      "epoch": 6.9312000000000005,
      "grad_norm": 0.11208198219537735,
      "learning_rate": 6.68e-06,
      "loss": 0.0025,
      "step": 129960
    },
    {
      "epoch": 6.931733333333334,
      "grad_norm": 0.028020137920975685,
      "learning_rate": 6.676666666666667e-06,
      "loss": 0.0027,
      "step": 129970
    },
    {
      "epoch": 6.932266666666667,
      "grad_norm": 0.22415907680988312,
      "learning_rate": 6.673333333333334e-06,
      "loss": 0.0041,
      "step": 129980
    },
    {
      "epoch": 6.9328,
      "grad_norm": 0.05604051798582077,
      "learning_rate": 6.67e-06,
      "loss": 0.0033,
      "step": 129990
    },
    {
      "epoch": 6.933333333333334,
      "grad_norm": 0.20066405832767487,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.0023,
      "step": 130000
    },
    {
      "epoch": 6.933866666666667,
      "grad_norm": 0.22415687143802643,
      "learning_rate": 6.663333333333334e-06,
      "loss": 0.0038,
      "step": 130010
    },
    {
      "epoch": 6.9344,
      "grad_norm": 0.33623847365379333,
      "learning_rate": 6.660000000000001e-06,
      "loss": 0.0017,
      "step": 130020
    },
    {
      "epoch": 6.934933333333333,
      "grad_norm": 0.05604171380400658,
      "learning_rate": 6.656666666666667e-06,
      "loss": 0.0031,
      "step": 130030
    },
    {
      "epoch": 6.935466666666667,
      "grad_norm": 0.08406352996826172,
      "learning_rate": 6.653333333333334e-06,
      "loss": 0.0025,
      "step": 130040
    },
    {
      "epoch": 6.936,
      "grad_norm": 0.22416843473911285,
      "learning_rate": 6.650000000000001e-06,
      "loss": 0.0018,
      "step": 130050
    },
    {
      "epoch": 6.936533333333333,
      "grad_norm": 0.22416958212852478,
      "learning_rate": 6.646666666666666e-06,
      "loss": 0.002,
      "step": 130060
    },
    {
      "epoch": 6.9370666666666665,
      "grad_norm": 0.28020575642585754,
      "learning_rate": 6.643333333333333e-06,
      "loss": 0.0033,
      "step": 130070
    },
    {
      "epoch": 6.9376,
      "grad_norm": 0.16811969876289368,
      "learning_rate": 6.640000000000001e-06,
      "loss": 0.0021,
      "step": 130080
    },
    {
      "epoch": 6.938133333333333,
      "grad_norm": 0.19613561034202576,
      "learning_rate": 6.636666666666668e-06,
      "loss": 0.0018,
      "step": 130090
    },
    {
      "epoch": 6.938666666666666,
      "grad_norm": 0.028019852936267853,
      "learning_rate": 6.633333333333333e-06,
      "loss": 0.0018,
      "step": 130100
    },
    {
      "epoch": 6.9392,
      "grad_norm": 0.2801973819732666,
      "learning_rate": 6.63e-06,
      "loss": 0.0027,
      "step": 130110
    },
    {
      "epoch": 6.939733333333333,
      "grad_norm": 0.2521834373474121,
      "learning_rate": 6.626666666666667e-06,
      "loss": 0.0016,
      "step": 130120
    },
    {
      "epoch": 6.940266666666667,
      "grad_norm": 0.3082157373428345,
      "learning_rate": 6.623333333333334e-06,
      "loss": 0.0032,
      "step": 130130
    },
    {
      "epoch": 6.9408,
      "grad_norm": 0.4212687313556671,
      "learning_rate": 6.62e-06,
      "loss": 0.0021,
      "step": 130140
    },
    {
      "epoch": 6.941333333333334,
      "grad_norm": 0.028019843623042107,
      "learning_rate": 6.616666666666667e-06,
      "loss": 0.0025,
      "step": 130150
    },
    {
      "epoch": 6.941866666666667,
      "grad_norm": 0.08406060934066772,
      "learning_rate": 6.613333333333334e-06,
      "loss": 0.0024,
      "step": 130160
    },
    {
      "epoch": 6.9424,
      "grad_norm": 0.05603966489434242,
      "learning_rate": 6.610000000000001e-06,
      "loss": 0.0031,
      "step": 130170
    },
    {
      "epoch": 6.942933333333333,
      "grad_norm": 0.05603951960802078,
      "learning_rate": 6.606666666666666e-06,
      "loss": 0.0038,
      "step": 130180
    },
    {
      "epoch": 6.943466666666667,
      "grad_norm": 6.870100044409355e-09,
      "learning_rate": 6.603333333333333e-06,
      "loss": 0.0019,
      "step": 130190
    },
    {
      "epoch": 6.944,
      "grad_norm": 0.08405928313732147,
      "learning_rate": 6.6e-06,
      "loss": 0.0032,
      "step": 130200
    },
    {
      "epoch": 6.944533333333333,
      "grad_norm": 0.056040581315755844,
      "learning_rate": 6.596666666666667e-06,
      "loss": 0.0033,
      "step": 130210
    },
    {
      "epoch": 6.9450666666666665,
      "grad_norm": 2.0918540233338945e-09,
      "learning_rate": 6.5933333333333335e-06,
      "loss": 0.0029,
      "step": 130220
    },
    {
      "epoch": 6.9456,
      "grad_norm": 0.11208220571279526,
      "learning_rate": 6.5900000000000004e-06,
      "loss": 0.0023,
      "step": 130230
    },
    {
      "epoch": 6.946133333333333,
      "grad_norm": 0.04684029892086983,
      "learning_rate": 6.586666666666667e-06,
      "loss": 0.002,
      "step": 130240
    },
    {
      "epoch": 6.946666666666666,
      "grad_norm": 0.05603950843214989,
      "learning_rate": 6.583333333333333e-06,
      "loss": 0.0028,
      "step": 130250
    },
    {
      "epoch": 6.9472000000000005,
      "grad_norm": 0.028020597994327545,
      "learning_rate": 6.58e-06,
      "loss": 0.0027,
      "step": 130260
    },
    {
      "epoch": 6.947733333333334,
      "grad_norm": 0.1120859757065773,
      "learning_rate": 6.5766666666666675e-06,
      "loss": 0.0028,
      "step": 130270
    },
    {
      "epoch": 6.948266666666667,
      "grad_norm": 0.05604187771677971,
      "learning_rate": 6.5733333333333345e-06,
      "loss": 0.0018,
      "step": 130280
    },
    {
      "epoch": 6.9488,
      "grad_norm": 0.19614529609680176,
      "learning_rate": 6.57e-06,
      "loss": 0.0022,
      "step": 130290
    },
    {
      "epoch": 6.949333333333334,
      "grad_norm": 0.2951931059360504,
      "learning_rate": 6.566666666666667e-06,
      "loss": 0.0031,
      "step": 130300
    },
    {
      "epoch": 6.949866666666667,
      "grad_norm": 0.056039124727249146,
      "learning_rate": 6.563333333333334e-06,
      "loss": 0.0025,
      "step": 130310
    },
    {
      "epoch": 6.9504,
      "grad_norm": 0.1681218147277832,
      "learning_rate": 6.560000000000001e-06,
      "loss": 0.003,
      "step": 130320
    },
    {
      "epoch": 6.950933333333333,
      "grad_norm": 0.0840630829334259,
      "learning_rate": 6.556666666666667e-06,
      "loss": 0.003,
      "step": 130330
    },
    {
      "epoch": 6.951466666666667,
      "grad_norm": 0.3148501217365265,
      "learning_rate": 6.553333333333334e-06,
      "loss": 0.0033,
      "step": 130340
    },
    {
      "epoch": 6.952,
      "grad_norm": 0.16812704503536224,
      "learning_rate": 6.550000000000001e-06,
      "loss": 0.0028,
      "step": 130350
    },
    {
      "epoch": 6.952533333333333,
      "grad_norm": 0.2802099883556366,
      "learning_rate": 6.546666666666668e-06,
      "loss": 0.0016,
      "step": 130360
    },
    {
      "epoch": 6.9530666666666665,
      "grad_norm": 0.25218093395233154,
      "learning_rate": 6.543333333333333e-06,
      "loss": 0.003,
      "step": 130370
    },
    {
      "epoch": 6.9536,
      "grad_norm": 0.19613851606845856,
      "learning_rate": 6.54e-06,
      "loss": 0.0017,
      "step": 130380
    },
    {
      "epoch": 6.954133333333333,
      "grad_norm": 0.08406133204698563,
      "learning_rate": 6.536666666666667e-06,
      "loss": 0.0031,
      "step": 130390
    },
    {
      "epoch": 6.954666666666666,
      "grad_norm": 0.08406168967485428,
      "learning_rate": 6.533333333333333e-06,
      "loss": 0.0016,
      "step": 130400
    },
    {
      "epoch": 6.9552,
      "grad_norm": 0.4203018248081207,
      "learning_rate": 6.53e-06,
      "loss": 0.0029,
      "step": 130410
    },
    {
      "epoch": 6.955733333333333,
      "grad_norm": 0.056039657443761826,
      "learning_rate": 6.526666666666667e-06,
      "loss": 0.003,
      "step": 130420
    },
    {
      "epoch": 6.956266666666667,
      "grad_norm": 0.11207856982946396,
      "learning_rate": 6.523333333333334e-06,
      "loss": 0.0028,
      "step": 130430
    },
    {
      "epoch": 6.9568,
      "grad_norm": 0.2836292088031769,
      "learning_rate": 6.519999999999999e-06,
      "loss": 0.0033,
      "step": 130440
    },
    {
      "epoch": 6.957333333333334,
      "grad_norm": 0.5568680763244629,
      "learning_rate": 6.516666666666666e-06,
      "loss": 0.0023,
      "step": 130450
    },
    {
      "epoch": 6.957866666666667,
      "grad_norm": 0.1330273449420929,
      "learning_rate": 6.513333333333333e-06,
      "loss": 0.0028,
      "step": 130460
    },
    {
      "epoch": 6.9584,
      "grad_norm": 0.19614796340465546,
      "learning_rate": 6.510000000000001e-06,
      "loss": 0.0021,
      "step": 130470
    },
    {
      "epoch": 6.958933333333333,
      "grad_norm": 0.12373047322034836,
      "learning_rate": 6.5066666666666665e-06,
      "loss": 0.0027,
      "step": 130480
    },
    {
      "epoch": 6.959466666666667,
      "grad_norm": 0.02801920846104622,
      "learning_rate": 6.5033333333333335e-06,
      "loss": 0.0028,
      "step": 130490
    },
    {
      "epoch": 6.96,
      "grad_norm": 0.08405815809965134,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 0.0024,
      "step": 130500
    },
    {
      "epoch": 6.960533333333333,
      "grad_norm": 0.11207784712314606,
      "learning_rate": 6.4966666666666674e-06,
      "loss": 0.0024,
      "step": 130510
    },
    {
      "epoch": 6.9610666666666665,
      "grad_norm": 0.11207982897758484,
      "learning_rate": 6.4933333333333336e-06,
      "loss": 0.0023,
      "step": 130520
    },
    {
      "epoch": 6.9616,
      "grad_norm": 2.9873927775980746e-09,
      "learning_rate": 6.4900000000000005e-06,
      "loss": 0.0028,
      "step": 130530
    },
    {
      "epoch": 6.962133333333333,
      "grad_norm": 0.3642513155937195,
      "learning_rate": 6.4866666666666675e-06,
      "loss": 0.0017,
      "step": 130540
    },
    {
      "epoch": 6.962666666666666,
      "grad_norm": 0.056038327515125275,
      "learning_rate": 6.4833333333333345e-06,
      "loss": 0.0018,
      "step": 130550
    },
    {
      "epoch": 6.9632,
      "grad_norm": 0.11307249218225479,
      "learning_rate": 6.48e-06,
      "loss": 0.0026,
      "step": 130560
    },
    {
      "epoch": 6.963733333333334,
      "grad_norm": 0.11207787692546844,
      "learning_rate": 6.476666666666667e-06,
      "loss": 0.0015,
      "step": 130570
    },
    {
      "epoch": 6.964266666666667,
      "grad_norm": 0.4202830493450165,
      "learning_rate": 6.473333333333334e-06,
      "loss": 0.0029,
      "step": 130580
    },
    {
      "epoch": 6.9648,
      "grad_norm": 0.28019437193870544,
      "learning_rate": 6.47e-06,
      "loss": 0.0025,
      "step": 130590
    },
    {
      "epoch": 6.965333333333334,
      "grad_norm": 0.12600955367088318,
      "learning_rate": 6.466666666666667e-06,
      "loss": 0.0028,
      "step": 130600
    },
    {
      "epoch": 6.965866666666667,
      "grad_norm": 0.14009778201580048,
      "learning_rate": 6.463333333333334e-06,
      "loss": 0.0031,
      "step": 130610
    },
    {
      "epoch": 6.9664,
      "grad_norm": 0.028020091354846954,
      "learning_rate": 6.460000000000001e-06,
      "loss": 0.0028,
      "step": 130620
    },
    {
      "epoch": 6.966933333333333,
      "grad_norm": 0.28019970655441284,
      "learning_rate": 6.456666666666666e-06,
      "loss": 0.0022,
      "step": 130630
    },
    {
      "epoch": 6.967466666666667,
      "grad_norm": 2.4523152397648573e-09,
      "learning_rate": 6.453333333333333e-06,
      "loss": 0.0018,
      "step": 130640
    },
    {
      "epoch": 6.968,
      "grad_norm": 0.21161715686321259,
      "learning_rate": 6.45e-06,
      "loss": 0.0026,
      "step": 130650
    },
    {
      "epoch": 6.968533333333333,
      "grad_norm": 0.14651617407798767,
      "learning_rate": 6.446666666666668e-06,
      "loss": 0.0036,
      "step": 130660
    },
    {
      "epoch": 6.9690666666666665,
      "grad_norm": 0.028019696474075317,
      "learning_rate": 6.443333333333333e-06,
      "loss": 0.0023,
      "step": 130670
    },
    {
      "epoch": 6.9696,
      "grad_norm": 0.02801922895014286,
      "learning_rate": 6.44e-06,
      "loss": 0.0019,
      "step": 130680
    },
    {
      "epoch": 6.970133333333333,
      "grad_norm": 0.1400994509458542,
      "learning_rate": 6.436666666666667e-06,
      "loss": 0.0023,
      "step": 130690
    },
    {
      "epoch": 6.970666666666666,
      "grad_norm": 0.36426424980163574,
      "learning_rate": 6.433333333333334e-06,
      "loss": 0.002,
      "step": 130700
    },
    {
      "epoch": 6.9712,
      "grad_norm": 0.25219106674194336,
      "learning_rate": 6.43e-06,
      "loss": 0.003,
      "step": 130710
    },
    {
      "epoch": 6.971733333333333,
      "grad_norm": 0.42029866576194763,
      "learning_rate": 6.426666666666667e-06,
      "loss": 0.0019,
      "step": 130720
    },
    {
      "epoch": 6.972266666666666,
      "grad_norm": 0.11207694560289383,
      "learning_rate": 6.423333333333334e-06,
      "loss": 0.0017,
      "step": 130730
    },
    {
      "epoch": 6.9728,
      "grad_norm": 0.028019271790981293,
      "learning_rate": 6.4199999999999995e-06,
      "loss": 0.0026,
      "step": 130740
    },
    {
      "epoch": 6.973333333333334,
      "grad_norm": 4.046389057155153e-10,
      "learning_rate": 6.4166666666666665e-06,
      "loss": 0.003,
      "step": 130750
    },
    {
      "epoch": 6.973866666666667,
      "grad_norm": 0.11208043247461319,
      "learning_rate": 6.4133333333333335e-06,
      "loss": 0.0022,
      "step": 130760
    },
    {
      "epoch": 6.9744,
      "grad_norm": 0.028020275756716728,
      "learning_rate": 6.4100000000000005e-06,
      "loss": 0.0025,
      "step": 130770
    },
    {
      "epoch": 6.974933333333333,
      "grad_norm": 0.056038666516542435,
      "learning_rate": 6.406666666666667e-06,
      "loss": 0.0021,
      "step": 130780
    },
    {
      "epoch": 6.975466666666667,
      "grad_norm": 0.028019040822982788,
      "learning_rate": 6.403333333333334e-06,
      "loss": 0.0028,
      "step": 130790
    },
    {
      "epoch": 6.976,
      "grad_norm": 0.14009353518486023,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 0.0019,
      "step": 130800
    },
    {
      "epoch": 6.976533333333333,
      "grad_norm": 0.3362264931201935,
      "learning_rate": 6.3966666666666675e-06,
      "loss": 0.0029,
      "step": 130810
    },
    {
      "epoch": 6.9770666666666665,
      "grad_norm": 0.2523168921470642,
      "learning_rate": 6.393333333333333e-06,
      "loss": 0.0036,
      "step": 130820
    },
    {
      "epoch": 6.9776,
      "grad_norm": 0.05603941157460213,
      "learning_rate": 6.39e-06,
      "loss": 0.0022,
      "step": 130830
    },
    {
      "epoch": 6.978133333333333,
      "grad_norm": 0.11207932978868484,
      "learning_rate": 6.386666666666667e-06,
      "loss": 0.0019,
      "step": 130840
    },
    {
      "epoch": 6.978666666666666,
      "grad_norm": 0.22415095567703247,
      "learning_rate": 6.383333333333335e-06,
      "loss": 0.0031,
      "step": 130850
    },
    {
      "epoch": 6.9792,
      "grad_norm": 0.08405724167823792,
      "learning_rate": 6.38e-06,
      "loss": 0.0019,
      "step": 130860
    },
    {
      "epoch": 6.979733333333334,
      "grad_norm": 0.05603838339447975,
      "learning_rate": 6.376666666666667e-06,
      "loss": 0.0029,
      "step": 130870
    },
    {
      "epoch": 6.980266666666667,
      "grad_norm": 0.196132630109787,
      "learning_rate": 6.373333333333334e-06,
      "loss": 0.0025,
      "step": 130880
    },
    {
      "epoch": 6.9808,
      "grad_norm": 0.08405745774507523,
      "learning_rate": 6.370000000000001e-06,
      "loss": 0.0021,
      "step": 130890
    },
    {
      "epoch": 6.981333333333334,
      "grad_norm": 0.11207842081785202,
      "learning_rate": 6.366666666666667e-06,
      "loss": 0.0027,
      "step": 130900
    },
    {
      "epoch": 6.981866666666667,
      "grad_norm": 0.252178817987442,
      "learning_rate": 6.363333333333334e-06,
      "loss": 0.0036,
      "step": 130910
    },
    {
      "epoch": 6.9824,
      "grad_norm": 0.028018999844789505,
      "learning_rate": 6.360000000000001e-06,
      "loss": 0.0021,
      "step": 130920
    },
    {
      "epoch": 6.982933333333333,
      "grad_norm": 0.05603831633925438,
      "learning_rate": 6.356666666666666e-06,
      "loss": 0.0024,
      "step": 130930
    },
    {
      "epoch": 6.983466666666667,
      "grad_norm": 0.028020130470395088,
      "learning_rate": 6.353333333333333e-06,
      "loss": 0.003,
      "step": 130940
    },
    {
      "epoch": 6.984,
      "grad_norm": 0.22415921092033386,
      "learning_rate": 6.35e-06,
      "loss": 0.0017,
      "step": 130950
    },
    {
      "epoch": 6.984533333333333,
      "grad_norm": 0.1120765283703804,
      "learning_rate": 6.346666666666667e-06,
      "loss": 0.0017,
      "step": 130960
    },
    {
      "epoch": 6.9850666666666665,
      "grad_norm": 0.308199942111969,
      "learning_rate": 6.343333333333333e-06,
      "loss": 0.0034,
      "step": 130970
    },
    {
      "epoch": 6.9856,
      "grad_norm": 0.14009755849838257,
      "learning_rate": 6.34e-06,
      "loss": 0.0029,
      "step": 130980
    },
    {
      "epoch": 6.986133333333333,
      "grad_norm": 0.19613641500473022,
      "learning_rate": 6.336666666666667e-06,
      "loss": 0.002,
      "step": 130990
    },
    {
      "epoch": 6.986666666666666,
      "grad_norm": 0.028018860146403313,
      "learning_rate": 6.333333333333334e-06,
      "loss": 0.0024,
      "step": 131000
    },
    {
      "epoch": 6.9872,
      "grad_norm": 0.4008261263370514,
      "learning_rate": 6.3299999999999995e-06,
      "loss": 0.0023,
      "step": 131010
    },
    {
      "epoch": 6.987733333333333,
      "grad_norm": 0.05603779852390289,
      "learning_rate": 6.3266666666666665e-06,
      "loss": 0.0025,
      "step": 131020
    },
    {
      "epoch": 6.988266666666666,
      "grad_norm": 0.11207662522792816,
      "learning_rate": 6.3233333333333335e-06,
      "loss": 0.0019,
      "step": 131030
    },
    {
      "epoch": 6.9888,
      "grad_norm": 0.3642590343952179,
      "learning_rate": 6.320000000000001e-06,
      "loss": 0.0022,
      "step": 131040
    },
    {
      "epoch": 6.989333333333334,
      "grad_norm": 0.22415956854820251,
      "learning_rate": 6.316666666666667e-06,
      "loss": 0.0027,
      "step": 131050
    },
    {
      "epoch": 6.989866666666667,
      "grad_norm": 0.11207924783229828,
      "learning_rate": 6.313333333333334e-06,
      "loss": 0.002,
      "step": 131060
    },
    {
      "epoch": 6.9904,
      "grad_norm": 0.2801911234855652,
      "learning_rate": 6.3100000000000006e-06,
      "loss": 0.0022,
      "step": 131070
    },
    {
      "epoch": 6.990933333333333,
      "grad_norm": 0.028019048273563385,
      "learning_rate": 6.306666666666666e-06,
      "loss": 0.0028,
      "step": 131080
    },
    {
      "epoch": 6.991466666666667,
      "grad_norm": 0.08405879139900208,
      "learning_rate": 6.303333333333334e-06,
      "loss": 0.002,
      "step": 131090
    },
    {
      "epoch": 6.992,
      "grad_norm": 0.14010077714920044,
      "learning_rate": 6.300000000000001e-06,
      "loss": 0.0035,
      "step": 131100
    },
    {
      "epoch": 6.992533333333333,
      "grad_norm": 0.11207982152700424,
      "learning_rate": 6.296666666666668e-06,
      "loss": 0.0024,
      "step": 131110
    },
    {
      "epoch": 6.9930666666666665,
      "grad_norm": 0.11207687109708786,
      "learning_rate": 6.293333333333333e-06,
      "loss": 0.0024,
      "step": 131120
    },
    {
      "epoch": 6.9936,
      "grad_norm": 0.11207254976034164,
      "learning_rate": 6.29e-06,
      "loss": 0.002,
      "step": 131130
    },
    {
      "epoch": 6.994133333333333,
      "grad_norm": 0.02801862359046936,
      "learning_rate": 6.286666666666667e-06,
      "loss": 0.0018,
      "step": 131140
    },
    {
      "epoch": 6.994666666666666,
      "grad_norm": 0.056036435067653656,
      "learning_rate": 6.283333333333334e-06,
      "loss": 0.0032,
      "step": 131150
    },
    {
      "epoch": 6.9952,
      "grad_norm": 0.0560365691781044,
      "learning_rate": 6.28e-06,
      "loss": 0.0026,
      "step": 131160
    },
    {
      "epoch": 6.995733333333334,
      "grad_norm": 0.1961328387260437,
      "learning_rate": 6.276666666666667e-06,
      "loss": 0.0032,
      "step": 131170
    },
    {
      "epoch": 6.996266666666667,
      "grad_norm": 0.560390055179596,
      "learning_rate": 6.273333333333334e-06,
      "loss": 0.0031,
      "step": 131180
    },
    {
      "epoch": 6.9968,
      "grad_norm": 0.16811540722846985,
      "learning_rate": 6.270000000000001e-06,
      "loss": 0.0034,
      "step": 131190
    },
    {
      "epoch": 6.997333333333334,
      "grad_norm": 0.05603747069835663,
      "learning_rate": 6.266666666666666e-06,
      "loss": 0.003,
      "step": 131200
    },
    {
      "epoch": 6.997866666666667,
      "grad_norm": 0.16810870170593262,
      "learning_rate": 6.263333333333333e-06,
      "loss": 0.003,
      "step": 131210
    },
    {
      "epoch": 6.9984,
      "grad_norm": 0.19612854719161987,
      "learning_rate": 6.26e-06,
      "loss": 0.0018,
      "step": 131220
    },
    {
      "epoch": 6.9989333333333335,
      "grad_norm": 0.02801837958395481,
      "learning_rate": 6.256666666666668e-06,
      "loss": 0.0039,
      "step": 131230
    },
    {
      "epoch": 6.999466666666667,
      "grad_norm": 0.11207355558872223,
      "learning_rate": 6.253333333333333e-06,
      "loss": 0.0027,
      "step": 131240
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.028018206357955933,
      "learning_rate": 6.25e-06,
      "loss": 0.0037,
      "step": 131250
    },
    {
      "epoch": 7.0,
      "eval_loss": 0.0027004729490727186,
      "eval_runtime": 172.6693,
      "eval_samples_per_second": 1447.854,
      "eval_steps_per_second": 36.196,
      "step": 131250
    },
    {
      "epoch": 7.000533333333333,
      "grad_norm": 0.028018372133374214,
      "learning_rate": 6.2466666666666664e-06,
      "loss": 0.0024,
      "step": 131260
    },
    {
      "epoch": 7.0010666666666665,
      "grad_norm": 0.2241506278514862,
      "learning_rate": 6.243333333333333e-06,
      "loss": 0.0053,
      "step": 131270
    },
    {
      "epoch": 7.0016,
      "grad_norm": 0.11207615584135056,
      "learning_rate": 6.24e-06,
      "loss": 0.0017,
      "step": 131280
    },
    {
      "epoch": 7.002133333333333,
      "grad_norm": 0.1681102216243744,
      "learning_rate": 6.236666666666667e-06,
      "loss": 0.0021,
      "step": 131290
    },
    {
      "epoch": 7.002666666666666,
      "grad_norm": 0.11207246780395508,
      "learning_rate": 6.2333333333333335e-06,
      "loss": 0.0019,
      "step": 131300
    },
    {
      "epoch": 7.0032,
      "grad_norm": 0.16810980439186096,
      "learning_rate": 6.2300000000000005e-06,
      "loss": 0.0018,
      "step": 131310
    },
    {
      "epoch": 7.003733333333333,
      "grad_norm": 0.05603798106312752,
      "learning_rate": 6.226666666666667e-06,
      "loss": 0.0024,
      "step": 131320
    },
    {
      "epoch": 7.004266666666667,
      "grad_norm": 0.05603867396712303,
      "learning_rate": 6.223333333333334e-06,
      "loss": 0.003,
      "step": 131330
    },
    {
      "epoch": 7.0048,
      "grad_norm": 0.0840577706694603,
      "learning_rate": 6.22e-06,
      "loss": 0.0029,
      "step": 131340
    },
    {
      "epoch": 7.005333333333334,
      "grad_norm": 0.028018441051244736,
      "learning_rate": 6.2166666666666676e-06,
      "loss": 0.0025,
      "step": 131350
    },
    {
      "epoch": 7.005866666666667,
      "grad_norm": 0.4202688932418823,
      "learning_rate": 6.213333333333334e-06,
      "loss": 0.0019,
      "step": 131360
    },
    {
      "epoch": 7.0064,
      "grad_norm": 0.14009414613246918,
      "learning_rate": 6.210000000000001e-06,
      "loss": 0.0033,
      "step": 131370
    },
    {
      "epoch": 7.0069333333333335,
      "grad_norm": 0.08405661582946777,
      "learning_rate": 6.206666666666667e-06,
      "loss": 0.0032,
      "step": 131380
    },
    {
      "epoch": 7.007466666666667,
      "grad_norm": 0.39225736260414124,
      "learning_rate": 6.203333333333334e-06,
      "loss": 0.0031,
      "step": 131390
    },
    {
      "epoch": 7.008,
      "grad_norm": 0.2801816165447235,
      "learning_rate": 6.2e-06,
      "loss": 0.0021,
      "step": 131400
    },
    {
      "epoch": 7.008533333333333,
      "grad_norm": 0.22414705157279968,
      "learning_rate": 6.196666666666667e-06,
      "loss": 0.0016,
      "step": 131410
    },
    {
      "epoch": 7.009066666666667,
      "grad_norm": 0.9808546304702759,
      "learning_rate": 6.193333333333334e-06,
      "loss": 0.0022,
      "step": 131420
    },
    {
      "epoch": 7.0096,
      "grad_norm": 3.2209506173330738e-09,
      "learning_rate": 6.19e-06,
      "loss": 0.0015,
      "step": 131430
    },
    {
      "epoch": 7.010133333333333,
      "grad_norm": 0.25216761231422424,
      "learning_rate": 6.186666666666667e-06,
      "loss": 0.0025,
      "step": 131440
    },
    {
      "epoch": 7.010666666666666,
      "grad_norm": 0.11207504570484161,
      "learning_rate": 6.183333333333333e-06,
      "loss": 0.0021,
      "step": 131450
    },
    {
      "epoch": 7.0112,
      "grad_norm": 0.196126788854599,
      "learning_rate": 6.18e-06,
      "loss": 0.0039,
      "step": 131460
    },
    {
      "epoch": 7.011733333333333,
      "grad_norm": 0.02801780216395855,
      "learning_rate": 6.176666666666667e-06,
      "loss": 0.0029,
      "step": 131470
    },
    {
      "epoch": 7.012266666666667,
      "grad_norm": 0.39225175976753235,
      "learning_rate": 6.173333333333334e-06,
      "loss": 0.0022,
      "step": 131480
    },
    {
      "epoch": 7.0128,
      "grad_norm": 0.3642449378967285,
      "learning_rate": 6.17e-06,
      "loss": 0.0033,
      "step": 131490
    },
    {
      "epoch": 7.013333333333334,
      "grad_norm": 0.08405746519565582,
      "learning_rate": 6.166666666666667e-06,
      "loss": 0.0025,
      "step": 131500
    },
    {
      "epoch": 7.013866666666667,
      "grad_norm": 0.056036993861198425,
      "learning_rate": 6.163333333333333e-06,
      "loss": 0.0024,
      "step": 131510
    },
    {
      "epoch": 7.0144,
      "grad_norm": 0.16811037063598633,
      "learning_rate": 6.16e-06,
      "loss": 0.003,
      "step": 131520
    },
    {
      "epoch": 7.0149333333333335,
      "grad_norm": 0.056036945432424545,
      "learning_rate": 6.1566666666666664e-06,
      "loss": 0.0029,
      "step": 131530
    },
    {
      "epoch": 7.015466666666667,
      "grad_norm": 0.05356890708208084,
      "learning_rate": 6.153333333333334e-06,
      "loss": 0.002,
      "step": 131540
    },
    {
      "epoch": 7.016,
      "grad_norm": 3.6293945626653112e-09,
      "learning_rate": 6.15e-06,
      "loss": 0.0023,
      "step": 131550
    },
    {
      "epoch": 7.016533333333333,
      "grad_norm": 0.05603650212287903,
      "learning_rate": 6.146666666666667e-06,
      "loss": 0.0025,
      "step": 131560
    },
    {
      "epoch": 7.017066666666667,
      "grad_norm": 0.11207153648138046,
      "learning_rate": 6.1433333333333335e-06,
      "loss": 0.006,
      "step": 131570
    },
    {
      "epoch": 7.0176,
      "grad_norm": 0.140090674161911,
      "learning_rate": 6.1400000000000005e-06,
      "loss": 0.0023,
      "step": 131580
    },
    {
      "epoch": 7.018133333333333,
      "grad_norm": 0.25216442346572876,
      "learning_rate": 6.136666666666667e-06,
      "loss": 0.003,
      "step": 131590
    },
    {
      "epoch": 7.018666666666666,
      "grad_norm": 0.03204730525612831,
      "learning_rate": 6.133333333333334e-06,
      "loss": 0.0034,
      "step": 131600
    },
    {
      "epoch": 7.0192,
      "grad_norm": 0.14009051024913788,
      "learning_rate": 6.130000000000001e-06,
      "loss": 0.0016,
      "step": 131610
    },
    {
      "epoch": 7.019733333333333,
      "grad_norm": 0.19438812136650085,
      "learning_rate": 6.126666666666667e-06,
      "loss": 0.0043,
      "step": 131620
    },
    {
      "epoch": 7.020266666666667,
      "grad_norm": 0.028018509969115257,
      "learning_rate": 6.123333333333334e-06,
      "loss": 0.0026,
      "step": 131630
    },
    {
      "epoch": 7.0208,
      "grad_norm": 0.05603621527552605,
      "learning_rate": 6.12e-06,
      "loss": 0.003,
      "step": 131640
    },
    {
      "epoch": 7.021333333333334,
      "grad_norm": 0.2241474837064743,
      "learning_rate": 6.116666666666667e-06,
      "loss": 0.0036,
      "step": 131650
    },
    {
      "epoch": 7.021866666666667,
      "grad_norm": 0.16811133921146393,
      "learning_rate": 6.113333333333334e-06,
      "loss": 0.0035,
      "step": 131660
    },
    {
      "epoch": 7.0224,
      "grad_norm": 0.02801828272640705,
      "learning_rate": 6.110000000000001e-06,
      "loss": 0.0031,
      "step": 131670
    },
    {
      "epoch": 7.0229333333333335,
      "grad_norm": 0.22414778172969818,
      "learning_rate": 6.106666666666667e-06,
      "loss": 0.0029,
      "step": 131680
    },
    {
      "epoch": 7.023466666666667,
      "grad_norm": 0.25216078758239746,
      "learning_rate": 6.103333333333334e-06,
      "loss": 0.0033,
      "step": 131690
    },
    {
      "epoch": 7.024,
      "grad_norm": 0.08405372500419617,
      "learning_rate": 6.1e-06,
      "loss": 0.0023,
      "step": 131700
    },
    {
      "epoch": 7.024533333333333,
      "grad_norm": 0.08405523747205734,
      "learning_rate": 6.096666666666667e-06,
      "loss": 0.0016,
      "step": 131710
    },
    {
      "epoch": 7.025066666666667,
      "grad_norm": 0.16810736060142517,
      "learning_rate": 6.093333333333333e-06,
      "loss": 0.0017,
      "step": 131720
    },
    {
      "epoch": 7.0256,
      "grad_norm": 0.05603625625371933,
      "learning_rate": 6.090000000000001e-06,
      "loss": 0.0029,
      "step": 131730
    },
    {
      "epoch": 7.026133333333333,
      "grad_norm": 0.16811087727546692,
      "learning_rate": 6.086666666666667e-06,
      "loss": 0.0029,
      "step": 131740
    },
    {
      "epoch": 7.026666666666666,
      "grad_norm": 0.14009472727775574,
      "learning_rate": 6.083333333333334e-06,
      "loss": 0.0018,
      "step": 131750
    },
    {
      "epoch": 7.0272,
      "grad_norm": 0.08405669033527374,
      "learning_rate": 6.08e-06,
      "loss": 0.0021,
      "step": 131760
    },
    {
      "epoch": 7.027733333333333,
      "grad_norm": 0.1400924175977707,
      "learning_rate": 6.076666666666666e-06,
      "loss": 0.0021,
      "step": 131770
    },
    {
      "epoch": 7.028266666666667,
      "grad_norm": 0.3922549784183502,
      "learning_rate": 6.073333333333333e-06,
      "loss": 0.0013,
      "step": 131780
    },
    {
      "epoch": 7.0288,
      "grad_norm": 0.16811281442642212,
      "learning_rate": 6.07e-06,
      "loss": 0.0027,
      "step": 131790
    },
    {
      "epoch": 7.029333333333334,
      "grad_norm": 0.11221912503242493,
      "learning_rate": 6.066666666666667e-06,
      "loss": 0.0033,
      "step": 131800
    },
    {
      "epoch": 7.029866666666667,
      "grad_norm": 0.3081924021244049,
      "learning_rate": 6.0633333333333334e-06,
      "loss": 0.0013,
      "step": 131810
    },
    {
      "epoch": 7.0304,
      "grad_norm": 0.0840543881058693,
      "learning_rate": 6.0600000000000004e-06,
      "loss": 0.0023,
      "step": 131820
    },
    {
      "epoch": 7.0309333333333335,
      "grad_norm": 0.2521732747554779,
      "learning_rate": 6.0566666666666666e-06,
      "loss": 0.0024,
      "step": 131830
    },
    {
      "epoch": 7.031466666666667,
      "grad_norm": 0.2521660029888153,
      "learning_rate": 6.0533333333333335e-06,
      "loss": 0.0023,
      "step": 131840
    },
    {
      "epoch": 7.032,
      "grad_norm": 0.25215810537338257,
      "learning_rate": 6.0500000000000005e-06,
      "loss": 0.0019,
      "step": 131850
    },
    {
      "epoch": 7.032533333333333,
      "grad_norm": 0.1961260586977005,
      "learning_rate": 6.0466666666666675e-06,
      "loss": 0.0023,
      "step": 131860
    },
    {
      "epoch": 7.033066666666667,
      "grad_norm": 0.3922517001628876,
      "learning_rate": 6.043333333333334e-06,
      "loss": 0.0027,
      "step": 131870
    },
    {
      "epoch": 7.0336,
      "grad_norm": 0.028018049895763397,
      "learning_rate": 6.040000000000001e-06,
      "loss": 0.0025,
      "step": 131880
    },
    {
      "epoch": 7.034133333333333,
      "grad_norm": 0.11207473278045654,
      "learning_rate": 6.036666666666667e-06,
      "loss": 0.0026,
      "step": 131890
    },
    {
      "epoch": 7.034666666666666,
      "grad_norm": 0.05699620023369789,
      "learning_rate": 6.033333333333334e-06,
      "loss": 0.0022,
      "step": 131900
    },
    {
      "epoch": 7.0352,
      "grad_norm": 0.25216540694236755,
      "learning_rate": 6.03e-06,
      "loss": 0.0019,
      "step": 131910
    },
    {
      "epoch": 7.035733333333333,
      "grad_norm": 2.3768684798142203e-09,
      "learning_rate": 6.026666666666667e-06,
      "loss": 0.0026,
      "step": 131920
    },
    {
      "epoch": 7.036266666666666,
      "grad_norm": 0.0840543881058693,
      "learning_rate": 6.023333333333334e-06,
      "loss": 0.004,
      "step": 131930
    },
    {
      "epoch": 7.0368,
      "grad_norm": 0.14008857309818268,
      "learning_rate": 6.02e-06,
      "loss": 0.004,
      "step": 131940
    },
    {
      "epoch": 7.037333333333334,
      "grad_norm": 0.028018198907375336,
      "learning_rate": 6.016666666666667e-06,
      "loss": 0.002,
      "step": 131950
    },
    {
      "epoch": 7.037866666666667,
      "grad_norm": 0.056037161499261856,
      "learning_rate": 6.013333333333333e-06,
      "loss": 0.0021,
      "step": 131960
    },
    {
      "epoch": 7.0384,
      "grad_norm": 0.0840548649430275,
      "learning_rate": 6.01e-06,
      "loss": 0.0026,
      "step": 131970
    },
    {
      "epoch": 7.0389333333333335,
      "grad_norm": 0.05603642761707306,
      "learning_rate": 6.006666666666667e-06,
      "loss": 0.0029,
      "step": 131980
    },
    {
      "epoch": 7.039466666666667,
      "grad_norm": 0.3922627866268158,
      "learning_rate": 6.003333333333334e-06,
      "loss": 0.0021,
      "step": 131990
    },
    {
      "epoch": 7.04,
      "grad_norm": 0.10294515639543533,
      "learning_rate": 6e-06,
      "loss": 0.0019,
      "step": 132000
    },
    {
      "epoch": 7.040533333333333,
      "grad_norm": 0.1961248517036438,
      "learning_rate": 5.996666666666667e-06,
      "loss": 0.0031,
      "step": 132010
    },
    {
      "epoch": 7.041066666666667,
      "grad_norm": 0.11207203567028046,
      "learning_rate": 5.993333333333333e-06,
      "loss": 0.0025,
      "step": 132020
    },
    {
      "epoch": 7.0416,
      "grad_norm": 0.08405479043722153,
      "learning_rate": 5.99e-06,
      "loss": 0.0027,
      "step": 132030
    },
    {
      "epoch": 7.042133333333333,
      "grad_norm": 0.11231721937656403,
      "learning_rate": 5.986666666666667e-06,
      "loss": 0.0017,
      "step": 132040
    },
    {
      "epoch": 7.042666666666666,
      "grad_norm": 0.1961345225572586,
      "learning_rate": 5.983333333333334e-06,
      "loss": 0.0031,
      "step": 132050
    },
    {
      "epoch": 7.0432,
      "grad_norm": 0.2108653038740158,
      "learning_rate": 5.98e-06,
      "loss": 0.003,
      "step": 132060
    },
    {
      "epoch": 7.043733333333333,
      "grad_norm": 0.11215630918741226,
      "learning_rate": 5.976666666666667e-06,
      "loss": 0.0022,
      "step": 132070
    },
    {
      "epoch": 7.044266666666666,
      "grad_norm": 0.39226996898651123,
      "learning_rate": 5.9733333333333335e-06,
      "loss": 0.002,
      "step": 132080
    },
    {
      "epoch": 7.0448,
      "grad_norm": 0.39225900173187256,
      "learning_rate": 5.9700000000000004e-06,
      "loss": 0.0024,
      "step": 132090
    },
    {
      "epoch": 7.045333333333334,
      "grad_norm": 6.197645951999675e-09,
      "learning_rate": 5.9666666666666666e-06,
      "loss": 0.0021,
      "step": 132100
    },
    {
      "epoch": 7.045866666666667,
      "grad_norm": 0.19633550941944122,
      "learning_rate": 5.9633333333333336e-06,
      "loss": 0.0026,
      "step": 132110
    },
    {
      "epoch": 7.0464,
      "grad_norm": 0.14008943736553192,
      "learning_rate": 5.9600000000000005e-06,
      "loss": 0.0018,
      "step": 132120
    },
    {
      "epoch": 7.0469333333333335,
      "grad_norm": 0.08405426889657974,
      "learning_rate": 5.956666666666667e-06,
      "loss": 0.0017,
      "step": 132130
    },
    {
      "epoch": 7.047466666666667,
      "grad_norm": 0.0560365654528141,
      "learning_rate": 5.953333333333334e-06,
      "loss": 0.0019,
      "step": 132140
    },
    {
      "epoch": 7.048,
      "grad_norm": 0.3642365336418152,
      "learning_rate": 5.95e-06,
      "loss": 0.0041,
      "step": 132150
    },
    {
      "epoch": 7.048533333333333,
      "grad_norm": 0.11340801417827606,
      "learning_rate": 5.946666666666667e-06,
      "loss": 0.0023,
      "step": 132160
    },
    {
      "epoch": 7.049066666666667,
      "grad_norm": 0.1400901973247528,
      "learning_rate": 5.943333333333334e-06,
      "loss": 0.0022,
      "step": 132170
    },
    {
      "epoch": 7.0496,
      "grad_norm": 0.028254589065909386,
      "learning_rate": 5.940000000000001e-06,
      "loss": 0.0024,
      "step": 132180
    },
    {
      "epoch": 7.050133333333333,
      "grad_norm": 0.140092134475708,
      "learning_rate": 5.936666666666667e-06,
      "loss": 0.0035,
      "step": 132190
    },
    {
      "epoch": 7.050666666666666,
      "grad_norm": 0.22414714097976685,
      "learning_rate": 5.933333333333334e-06,
      "loss": 0.0038,
      "step": 132200
    },
    {
      "epoch": 7.0512,
      "grad_norm": 0.16810648143291473,
      "learning_rate": 5.93e-06,
      "loss": 0.0018,
      "step": 132210
    },
    {
      "epoch": 7.051733333333333,
      "grad_norm": 0.47629907727241516,
      "learning_rate": 5.926666666666667e-06,
      "loss": 0.0031,
      "step": 132220
    },
    {
      "epoch": 7.052266666666666,
      "grad_norm": 0.05603553727269173,
      "learning_rate": 5.923333333333333e-06,
      "loss": 0.0017,
      "step": 132230
    },
    {
      "epoch": 7.0528,
      "grad_norm": 0.1120717003941536,
      "learning_rate": 5.920000000000001e-06,
      "loss": 0.0024,
      "step": 132240
    },
    {
      "epoch": 7.053333333333334,
      "grad_norm": 0.33621275424957275,
      "learning_rate": 5.916666666666667e-06,
      "loss": 0.0036,
      "step": 132250
    },
    {
      "epoch": 7.053866666666667,
      "grad_norm": 0.11207015812397003,
      "learning_rate": 5.913333333333334e-06,
      "loss": 0.0027,
      "step": 132260
    },
    {
      "epoch": 7.0544,
      "grad_norm": 0.36422744393348694,
      "learning_rate": 5.91e-06,
      "loss": 0.0024,
      "step": 132270
    },
    {
      "epoch": 7.0549333333333335,
      "grad_norm": 0.3081919252872467,
      "learning_rate": 5.906666666666667e-06,
      "loss": 0.0018,
      "step": 132280
    },
    {
      "epoch": 7.055466666666667,
      "grad_norm": 0.1120704784989357,
      "learning_rate": 5.903333333333333e-06,
      "loss": 0.0024,
      "step": 132290
    },
    {
      "epoch": 7.056,
      "grad_norm": 2.065705828613318e-09,
      "learning_rate": 5.9e-06,
      "loss": 0.0028,
      "step": 132300
    },
    {
      "epoch": 7.056533333333333,
      "grad_norm": 0.19612517952919006,
      "learning_rate": 5.896666666666667e-06,
      "loss": 0.0027,
      "step": 132310
    },
    {
      "epoch": 7.057066666666667,
      "grad_norm": 0.02814745157957077,
      "learning_rate": 5.893333333333333e-06,
      "loss": 0.0019,
      "step": 132320
    },
    {
      "epoch": 7.0576,
      "grad_norm": 0.0840524360537529,
      "learning_rate": 5.89e-06,
      "loss": 0.0032,
      "step": 132330
    },
    {
      "epoch": 7.058133333333333,
      "grad_norm": 0.16810551285743713,
      "learning_rate": 5.8866666666666665e-06,
      "loss": 0.0032,
      "step": 132340
    },
    {
      "epoch": 7.058666666666666,
      "grad_norm": 2.934855247715973e-09,
      "learning_rate": 5.8833333333333335e-06,
      "loss": 0.0033,
      "step": 132350
    },
    {
      "epoch": 7.0592,
      "grad_norm": 0.11207258701324463,
      "learning_rate": 5.8800000000000005e-06,
      "loss": 0.0023,
      "step": 132360
    },
    {
      "epoch": 7.059733333333333,
      "grad_norm": 0.19612598419189453,
      "learning_rate": 5.8766666666666674e-06,
      "loss": 0.0022,
      "step": 132370
    },
    {
      "epoch": 7.060266666666666,
      "grad_norm": 0.14008748531341553,
      "learning_rate": 5.8733333333333336e-06,
      "loss": 0.0023,
      "step": 132380
    },
    {
      "epoch": 7.0608,
      "grad_norm": 0.28017735481262207,
      "learning_rate": 5.8700000000000005e-06,
      "loss": 0.0044,
      "step": 132390
    },
    {
      "epoch": 7.061333333333334,
      "grad_norm": 0.08405445516109467,
      "learning_rate": 5.866666666666667e-06,
      "loss": 0.0025,
      "step": 132400
    },
    {
      "epoch": 7.061866666666667,
      "grad_norm": 0.11208463460206985,
      "learning_rate": 5.863333333333334e-06,
      "loss": 0.0026,
      "step": 132410
    },
    {
      "epoch": 7.0624,
      "grad_norm": 0.14009302854537964,
      "learning_rate": 5.86e-06,
      "loss": 0.0029,
      "step": 132420
    },
    {
      "epoch": 7.0629333333333335,
      "grad_norm": 0.028017964214086533,
      "learning_rate": 5.856666666666668e-06,
      "loss": 0.0024,
      "step": 132430
    },
    {
      "epoch": 7.063466666666667,
      "grad_norm": 0.08405403792858124,
      "learning_rate": 5.853333333333334e-06,
      "loss": 0.0023,
      "step": 132440
    },
    {
      "epoch": 7.064,
      "grad_norm": 0.0840541198849678,
      "learning_rate": 5.850000000000001e-06,
      "loss": 0.0021,
      "step": 132450
    },
    {
      "epoch": 7.064533333333333,
      "grad_norm": 0.14008696377277374,
      "learning_rate": 5.846666666666667e-06,
      "loss": 0.0027,
      "step": 132460
    },
    {
      "epoch": 7.065066666666667,
      "grad_norm": 0.30818870663642883,
      "learning_rate": 5.843333333333333e-06,
      "loss": 0.0031,
      "step": 132470
    },
    {
      "epoch": 7.0656,
      "grad_norm": 0.0840548425912857,
      "learning_rate": 5.84e-06,
      "loss": 0.0041,
      "step": 132480
    },
    {
      "epoch": 7.066133333333333,
      "grad_norm": 0.056035712361335754,
      "learning_rate": 5.836666666666667e-06,
      "loss": 0.0025,
      "step": 132490
    },
    {
      "epoch": 7.066666666666666,
      "grad_norm": 0.08405321091413498,
      "learning_rate": 5.833333333333334e-06,
      "loss": 0.0017,
      "step": 132500
    },
    {
      "epoch": 7.0672,
      "grad_norm": 0.02801787108182907,
      "learning_rate": 5.83e-06,
      "loss": 0.0029,
      "step": 132510
    },
    {
      "epoch": 7.067733333333333,
      "grad_norm": 1.2501647472381592,
      "learning_rate": 5.826666666666667e-06,
      "loss": 0.0021,
      "step": 132520
    },
    {
      "epoch": 7.068266666666666,
      "grad_norm": 0.1961270421743393,
      "learning_rate": 5.823333333333333e-06,
      "loss": 0.003,
      "step": 132530
    },
    {
      "epoch": 7.0688,
      "grad_norm": 0.028017938137054443,
      "learning_rate": 5.82e-06,
      "loss": 0.0042,
      "step": 132540
    },
    {
      "epoch": 7.069333333333334,
      "grad_norm": 0.06444352865219116,
      "learning_rate": 5.816666666666667e-06,
      "loss": 0.0045,
      "step": 132550
    },
    {
      "epoch": 7.069866666666667,
      "grad_norm": 0.1400911509990692,
      "learning_rate": 5.813333333333334e-06,
      "loss": 0.0019,
      "step": 132560
    },
    {
      "epoch": 7.0704,
      "grad_norm": 0.14008985459804535,
      "learning_rate": 5.81e-06,
      "loss": 0.0022,
      "step": 132570
    },
    {
      "epoch": 7.0709333333333335,
      "grad_norm": 0.1120707094669342,
      "learning_rate": 5.806666666666667e-06,
      "loss": 0.0026,
      "step": 132580
    },
    {
      "epoch": 7.071466666666667,
      "grad_norm": 0.19612668454647064,
      "learning_rate": 5.803333333333333e-06,
      "loss": 0.0025,
      "step": 132590
    },
    {
      "epoch": 7.072,
      "grad_norm": 0.05603516846895218,
      "learning_rate": 5.8e-06,
      "loss": 0.0031,
      "step": 132600
    },
    {
      "epoch": 7.072533333333333,
      "grad_norm": 0.19612228870391846,
      "learning_rate": 5.7966666666666665e-06,
      "loss": 0.0019,
      "step": 132610
    },
    {
      "epoch": 7.073066666666667,
      "grad_norm": 0.16810843348503113,
      "learning_rate": 5.793333333333334e-06,
      "loss": 0.0017,
      "step": 132620
    },
    {
      "epoch": 7.0736,
      "grad_norm": 0.33622831106185913,
      "learning_rate": 5.7900000000000005e-06,
      "loss": 0.0027,
      "step": 132630
    },
    {
      "epoch": 7.074133333333333,
      "grad_norm": 0.3922705054283142,
      "learning_rate": 5.786666666666667e-06,
      "loss": 0.0027,
      "step": 132640
    },
    {
      "epoch": 7.074666666666666,
      "grad_norm": 0.14009489119052887,
      "learning_rate": 5.783333333333334e-06,
      "loss": 0.0025,
      "step": 132650
    },
    {
      "epoch": 7.0752,
      "grad_norm": 0.2436612993478775,
      "learning_rate": 5.78e-06,
      "loss": 0.0023,
      "step": 132660
    },
    {
      "epoch": 7.075733333333333,
      "grad_norm": 0.02801796793937683,
      "learning_rate": 5.776666666666667e-06,
      "loss": 0.0013,
      "step": 132670
    },
    {
      "epoch": 7.076266666666666,
      "grad_norm": 3.5876859261207983e-09,
      "learning_rate": 5.773333333333334e-06,
      "loss": 0.0037,
      "step": 132680
    },
    {
      "epoch": 7.0768,
      "grad_norm": 0.25216054916381836,
      "learning_rate": 5.770000000000001e-06,
      "loss": 0.0013,
      "step": 132690
    },
    {
      "epoch": 7.077333333333334,
      "grad_norm": 0.02801736816763878,
      "learning_rate": 5.766666666666667e-06,
      "loss": 0.0034,
      "step": 132700
    },
    {
      "epoch": 7.077866666666667,
      "grad_norm": 0.02801809273660183,
      "learning_rate": 5.763333333333334e-06,
      "loss": 0.0043,
      "step": 132710
    },
    {
      "epoch": 7.0784,
      "grad_norm": 0.392261803150177,
      "learning_rate": 5.76e-06,
      "loss": 0.0024,
      "step": 132720
    },
    {
      "epoch": 7.0789333333333335,
      "grad_norm": 0.05603708699345589,
      "learning_rate": 5.756666666666667e-06,
      "loss": 0.0026,
      "step": 132730
    },
    {
      "epoch": 7.079466666666667,
      "grad_norm": 1.007161021232605,
      "learning_rate": 5.753333333333334e-06,
      "loss": 0.0025,
      "step": 132740
    },
    {
      "epoch": 7.08,
      "grad_norm": 0.14009293913841248,
      "learning_rate": 5.750000000000001e-06,
      "loss": 0.0028,
      "step": 132750
    },
    {
      "epoch": 7.080533333333333,
      "grad_norm": 0.05603604391217232,
      "learning_rate": 5.746666666666667e-06,
      "loss": 0.0019,
      "step": 132760
    },
    {
      "epoch": 7.081066666666667,
      "grad_norm": 0.3922610282897949,
      "learning_rate": 5.743333333333334e-06,
      "loss": 0.0017,
      "step": 132770
    },
    {
      "epoch": 7.0816,
      "grad_norm": 0.028019150719046593,
      "learning_rate": 5.74e-06,
      "loss": 0.0025,
      "step": 132780
    },
    {
      "epoch": 7.082133333333333,
      "grad_norm": 0.028018897399306297,
      "learning_rate": 5.736666666666667e-06,
      "loss": 0.0024,
      "step": 132790
    },
    {
      "epoch": 7.082666666666666,
      "grad_norm": 0.028018925338983536,
      "learning_rate": 5.733333333333333e-06,
      "loss": 0.0022,
      "step": 132800
    },
    {
      "epoch": 7.0832,
      "grad_norm": 2.158259571061194e-09,
      "learning_rate": 5.73e-06,
      "loss": 0.0015,
      "step": 132810
    },
    {
      "epoch": 7.083733333333333,
      "grad_norm": 0.07000817358493805,
      "learning_rate": 5.726666666666667e-06,
      "loss": 0.0024,
      "step": 132820
    },
    {
      "epoch": 7.084266666666666,
      "grad_norm": 0.2241421341896057,
      "learning_rate": 5.723333333333333e-06,
      "loss": 0.0027,
      "step": 132830
    },
    {
      "epoch": 7.0848,
      "grad_norm": 0.14008913934230804,
      "learning_rate": 5.72e-06,
      "loss": 0.0023,
      "step": 132840
    },
    {
      "epoch": 7.085333333333334,
      "grad_norm": 0.16810810565948486,
      "learning_rate": 5.7166666666666664e-06,
      "loss": 0.0027,
      "step": 132850
    },
    {
      "epoch": 7.085866666666667,
      "grad_norm": 0.3642316460609436,
      "learning_rate": 5.713333333333333e-06,
      "loss": 0.003,
      "step": 132860
    },
    {
      "epoch": 7.0864,
      "grad_norm": 1.996914189561494e-09,
      "learning_rate": 5.71e-06,
      "loss": 0.0018,
      "step": 132870
    },
    {
      "epoch": 7.0869333333333335,
      "grad_norm": 0.08405669778585434,
      "learning_rate": 5.706666666666667e-06,
      "loss": 0.0028,
      "step": 132880
    },
    {
      "epoch": 7.087466666666667,
      "grad_norm": 0.11207524687051773,
      "learning_rate": 5.7033333333333335e-06,
      "loss": 0.0028,
      "step": 132890
    },
    {
      "epoch": 7.088,
      "grad_norm": 0.11207303404808044,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 0.0021,
      "step": 132900
    },
    {
      "epoch": 7.088533333333333,
      "grad_norm": 0.28017935156822205,
      "learning_rate": 5.696666666666667e-06,
      "loss": 0.0035,
      "step": 132910
    },
    {
      "epoch": 7.089066666666667,
      "grad_norm": 0.05603674054145813,
      "learning_rate": 5.693333333333334e-06,
      "loss": 0.0024,
      "step": 132920
    },
    {
      "epoch": 7.0896,
      "grad_norm": 0.056037094444036484,
      "learning_rate": 5.690000000000001e-06,
      "loss": 0.0024,
      "step": 132930
    },
    {
      "epoch": 7.090133333333333,
      "grad_norm": 0.14009469747543335,
      "learning_rate": 5.6866666666666676e-06,
      "loss": 0.0028,
      "step": 132940
    },
    {
      "epoch": 7.0906666666666665,
      "grad_norm": 0.3082062005996704,
      "learning_rate": 5.683333333333334e-06,
      "loss": 0.0027,
      "step": 132950
    },
    {
      "epoch": 7.0912,
      "grad_norm": 0.42026665806770325,
      "learning_rate": 5.680000000000001e-06,
      "loss": 0.0028,
      "step": 132960
    },
    {
      "epoch": 7.091733333333333,
      "grad_norm": 0.08449526876211166,
      "learning_rate": 5.676666666666667e-06,
      "loss": 0.0024,
      "step": 132970
    },
    {
      "epoch": 7.092266666666666,
      "grad_norm": 0.8841750621795654,
      "learning_rate": 5.673333333333333e-06,
      "loss": 0.0025,
      "step": 132980
    },
    {
      "epoch": 7.0928,
      "grad_norm": 0.2807939052581787,
      "learning_rate": 5.67e-06,
      "loss": 0.0017,
      "step": 132990
    },
    {
      "epoch": 7.093333333333334,
      "grad_norm": 0.28020063042640686,
      "learning_rate": 5.666666666666667e-06,
      "loss": 0.0029,
      "step": 133000
    },
    {
      "epoch": 7.093866666666667,
      "grad_norm": 0.11207395046949387,
      "learning_rate": 5.663333333333334e-06,
      "loss": 0.0023,
      "step": 133010
    },
    {
      "epoch": 7.0944,
      "grad_norm": 0.11207190901041031,
      "learning_rate": 5.66e-06,
      "loss": 0.0024,
      "step": 133020
    },
    {
      "epoch": 7.0949333333333335,
      "grad_norm": 1.1034115552902222,
      "learning_rate": 5.656666666666667e-06,
      "loss": 0.0019,
      "step": 133030
    },
    {
      "epoch": 7.095466666666667,
      "grad_norm": 0.14009137451648712,
      "learning_rate": 5.653333333333333e-06,
      "loss": 0.0028,
      "step": 133040
    },
    {
      "epoch": 7.096,
      "grad_norm": 0.11207348108291626,
      "learning_rate": 5.65e-06,
      "loss": 0.0032,
      "step": 133050
    },
    {
      "epoch": 7.096533333333333,
      "grad_norm": 0.213693305850029,
      "learning_rate": 5.646666666666667e-06,
      "loss": 0.003,
      "step": 133060
    },
    {
      "epoch": 7.097066666666667,
      "grad_norm": 0.028018184006214142,
      "learning_rate": 5.643333333333334e-06,
      "loss": 0.0024,
      "step": 133070
    },
    {
      "epoch": 7.0976,
      "grad_norm": 0.196125328540802,
      "learning_rate": 5.64e-06,
      "loss": 0.0027,
      "step": 133080
    },
    {
      "epoch": 7.098133333333333,
      "grad_norm": 0.16810400784015656,
      "learning_rate": 5.636666666666667e-06,
      "loss": 0.003,
      "step": 133090
    },
    {
      "epoch": 7.0986666666666665,
      "grad_norm": 0.33621886372566223,
      "learning_rate": 5.633333333333333e-06,
      "loss": 0.0019,
      "step": 133100
    },
    {
      "epoch": 7.0992,
      "grad_norm": 0.2801915407180786,
      "learning_rate": 5.63e-06,
      "loss": 0.0029,
      "step": 133110
    },
    {
      "epoch": 7.099733333333333,
      "grad_norm": 0.02801818959414959,
      "learning_rate": 5.626666666666667e-06,
      "loss": 0.0023,
      "step": 133120
    },
    {
      "epoch": 7.100266666666666,
      "grad_norm": 0.05603542551398277,
      "learning_rate": 5.623333333333334e-06,
      "loss": 0.004,
      "step": 133130
    },
    {
      "epoch": 7.1008,
      "grad_norm": 0.08405447751283646,
      "learning_rate": 5.62e-06,
      "loss": 0.0017,
      "step": 133140
    },
    {
      "epoch": 7.101333333333334,
      "grad_norm": 0.05603668838739395,
      "learning_rate": 5.6166666666666665e-06,
      "loss": 0.0027,
      "step": 133150
    },
    {
      "epoch": 7.101866666666667,
      "grad_norm": 0.33622342348098755,
      "learning_rate": 5.6133333333333335e-06,
      "loss": 0.0025,
      "step": 133160
    },
    {
      "epoch": 7.1024,
      "grad_norm": 0.14008934795856476,
      "learning_rate": 5.61e-06,
      "loss": 0.0028,
      "step": 133170
    },
    {
      "epoch": 7.1029333333333335,
      "grad_norm": 2.9832560866083213e-09,
      "learning_rate": 5.606666666666667e-06,
      "loss": 0.0024,
      "step": 133180
    },
    {
      "epoch": 7.103466666666667,
      "grad_norm": 0.36423224210739136,
      "learning_rate": 5.603333333333334e-06,
      "loss": 0.0018,
      "step": 133190
    },
    {
      "epoch": 7.104,
      "grad_norm": 0.19612948596477509,
      "learning_rate": 5.600000000000001e-06,
      "loss": 0.0027,
      "step": 133200
    },
    {
      "epoch": 7.104533333333333,
      "grad_norm": 0.19612762331962585,
      "learning_rate": 5.596666666666667e-06,
      "loss": 0.0025,
      "step": 133210
    },
    {
      "epoch": 7.105066666666667,
      "grad_norm": 0.2241428643465042,
      "learning_rate": 5.593333333333334e-06,
      "loss": 0.0037,
      "step": 133220
    },
    {
      "epoch": 7.1056,
      "grad_norm": 0.16810671985149384,
      "learning_rate": 5.59e-06,
      "loss": 0.002,
      "step": 133230
    },
    {
      "epoch": 7.106133333333333,
      "grad_norm": 0.33621618151664734,
      "learning_rate": 5.586666666666667e-06,
      "loss": 0.0038,
      "step": 133240
    },
    {
      "epoch": 7.1066666666666665,
      "grad_norm": 0.02801845408976078,
      "learning_rate": 5.583333333333334e-06,
      "loss": 0.002,
      "step": 133250
    },
    {
      "epoch": 7.1072,
      "grad_norm": 0.028018567711114883,
      "learning_rate": 5.580000000000001e-06,
      "loss": 0.0027,
      "step": 133260
    },
    {
      "epoch": 7.107733333333333,
      "grad_norm": 0.028018061071634293,
      "learning_rate": 5.576666666666667e-06,
      "loss": 0.0027,
      "step": 133270
    },
    {
      "epoch": 7.108266666666666,
      "grad_norm": 0.028017807751893997,
      "learning_rate": 5.573333333333334e-06,
      "loss": 0.0024,
      "step": 133280
    },
    {
      "epoch": 7.1088,
      "grad_norm": 0.11207426339387894,
      "learning_rate": 5.57e-06,
      "loss": 0.0033,
      "step": 133290
    },
    {
      "epoch": 7.109333333333334,
      "grad_norm": 0.028018759563565254,
      "learning_rate": 5.566666666666667e-06,
      "loss": 0.0025,
      "step": 133300
    },
    {
      "epoch": 7.109866666666667,
      "grad_norm": 0.028019169345498085,
      "learning_rate": 5.563333333333334e-06,
      "loss": 0.0036,
      "step": 133310
    },
    {
      "epoch": 7.1104,
      "grad_norm": 0.08405479043722153,
      "learning_rate": 5.56e-06,
      "loss": 0.0023,
      "step": 133320
    },
    {
      "epoch": 7.1109333333333336,
      "grad_norm": 0.11207560449838638,
      "learning_rate": 5.556666666666667e-06,
      "loss": 0.0017,
      "step": 133330
    },
    {
      "epoch": 7.111466666666667,
      "grad_norm": 0.1681094765663147,
      "learning_rate": 5.553333333333333e-06,
      "loss": 0.0017,
      "step": 133340
    },
    {
      "epoch": 7.112,
      "grad_norm": 0.22414317727088928,
      "learning_rate": 5.55e-06,
      "loss": 0.0024,
      "step": 133350
    },
    {
      "epoch": 7.112533333333333,
      "grad_norm": 0.1400974690914154,
      "learning_rate": 5.546666666666666e-06,
      "loss": 0.0024,
      "step": 133360
    },
    {
      "epoch": 7.113066666666667,
      "grad_norm": 0.11207263916730881,
      "learning_rate": 5.543333333333333e-06,
      "loss": 0.0021,
      "step": 133370
    },
    {
      "epoch": 7.1136,
      "grad_norm": 0.08405443280935287,
      "learning_rate": 5.54e-06,
      "loss": 0.0032,
      "step": 133380
    },
    {
      "epoch": 7.114133333333333,
      "grad_norm": 0.28018563985824585,
      "learning_rate": 5.536666666666667e-06,
      "loss": 0.0023,
      "step": 133390
    },
    {
      "epoch": 7.1146666666666665,
      "grad_norm": 0.028018711134791374,
      "learning_rate": 5.5333333333333334e-06,
      "loss": 0.0021,
      "step": 133400
    },
    {
      "epoch": 7.1152,
      "grad_norm": 0.1681082844734192,
      "learning_rate": 5.53e-06,
      "loss": 0.0029,
      "step": 133410
    },
    {
      "epoch": 7.115733333333333,
      "grad_norm": 0.08405345678329468,
      "learning_rate": 5.5266666666666666e-06,
      "loss": 0.0022,
      "step": 133420
    },
    {
      "epoch": 7.116266666666666,
      "grad_norm": 0.3642341196537018,
      "learning_rate": 5.5233333333333335e-06,
      "loss": 0.0023,
      "step": 133430
    },
    {
      "epoch": 7.1168,
      "grad_norm": 0.05603783205151558,
      "learning_rate": 5.5200000000000005e-06,
      "loss": 0.0021,
      "step": 133440
    },
    {
      "epoch": 7.117333333333334,
      "grad_norm": 0.280185729265213,
      "learning_rate": 5.5166666666666675e-06,
      "loss": 0.0017,
      "step": 133450
    },
    {
      "epoch": 7.117866666666667,
      "grad_norm": 0.02801760472357273,
      "learning_rate": 5.513333333333334e-06,
      "loss": 0.0025,
      "step": 133460
    },
    {
      "epoch": 7.1184,
      "grad_norm": 0.1681065708398819,
      "learning_rate": 5.510000000000001e-06,
      "loss": 0.0036,
      "step": 133470
    },
    {
      "epoch": 7.118933333333334,
      "grad_norm": 0.02801753208041191,
      "learning_rate": 5.506666666666667e-06,
      "loss": 0.0022,
      "step": 133480
    },
    {
      "epoch": 7.119466666666667,
      "grad_norm": 0.08405404537916183,
      "learning_rate": 5.503333333333333e-06,
      "loss": 0.0028,
      "step": 133490
    },
    {
      "epoch": 7.12,
      "grad_norm": 0.19612610340118408,
      "learning_rate": 5.500000000000001e-06,
      "loss": 0.0019,
      "step": 133500
    },
    {
      "epoch": 7.120533333333333,
      "grad_norm": 0.056035369634628296,
      "learning_rate": 5.496666666666667e-06,
      "loss": 0.0025,
      "step": 133510
    },
    {
      "epoch": 7.121066666666667,
      "grad_norm": 0.1961272954940796,
      "learning_rate": 5.493333333333334e-06,
      "loss": 0.0029,
      "step": 133520
    },
    {
      "epoch": 7.1216,
      "grad_norm": 0.19612902402877808,
      "learning_rate": 5.49e-06,
      "loss": 0.0027,
      "step": 133530
    },
    {
      "epoch": 7.122133333333333,
      "grad_norm": 0.14009015262126923,
      "learning_rate": 5.486666666666667e-06,
      "loss": 0.0032,
      "step": 133540
    },
    {
      "epoch": 7.1226666666666665,
      "grad_norm": 0.05603590980172157,
      "learning_rate": 5.483333333333333e-06,
      "loss": 0.0034,
      "step": 133550
    },
    {
      "epoch": 7.1232,
      "grad_norm": 0.14008677005767822,
      "learning_rate": 5.48e-06,
      "loss": 0.0021,
      "step": 133560
    },
    {
      "epoch": 7.123733333333333,
      "grad_norm": 0.11206930875778198,
      "learning_rate": 5.476666666666667e-06,
      "loss": 0.0024,
      "step": 133570
    },
    {
      "epoch": 7.124266666666666,
      "grad_norm": 0.2521582245826721,
      "learning_rate": 5.473333333333334e-06,
      "loss": 0.0028,
      "step": 133580
    },
    {
      "epoch": 7.1248,
      "grad_norm": 0.3082045316696167,
      "learning_rate": 5.47e-06,
      "loss": 0.0016,
      "step": 133590
    },
    {
      "epoch": 7.125333333333334,
      "grad_norm": 0.08405465632677078,
      "learning_rate": 5.466666666666667e-06,
      "loss": 0.0022,
      "step": 133600
    },
    {
      "epoch": 7.125866666666667,
      "grad_norm": 0.02801794745028019,
      "learning_rate": 5.463333333333333e-06,
      "loss": 0.0032,
      "step": 133610
    },
    {
      "epoch": 7.1264,
      "grad_norm": 0.16810603439807892,
      "learning_rate": 5.46e-06,
      "loss": 0.003,
      "step": 133620
    },
    {
      "epoch": 7.126933333333334,
      "grad_norm": 0.14008662104606628,
      "learning_rate": 5.456666666666667e-06,
      "loss": 0.002,
      "step": 133630
    },
    {
      "epoch": 7.127466666666667,
      "grad_norm": 0.14008885622024536,
      "learning_rate": 5.453333333333334e-06,
      "loss": 0.0028,
      "step": 133640
    },
    {
      "epoch": 7.128,
      "grad_norm": 0.0280181672424078,
      "learning_rate": 5.45e-06,
      "loss": 0.0023,
      "step": 133650
    },
    {
      "epoch": 7.128533333333333,
      "grad_norm": 0.1962479054927826,
      "learning_rate": 5.4466666666666665e-06,
      "loss": 0.0025,
      "step": 133660
    },
    {
      "epoch": 7.129066666666667,
      "grad_norm": 0.028018629178404808,
      "learning_rate": 5.4433333333333335e-06,
      "loss": 0.0029,
      "step": 133670
    },
    {
      "epoch": 7.1296,
      "grad_norm": 0.028048783540725708,
      "learning_rate": 5.44e-06,
      "loss": 0.0021,
      "step": 133680
    },
    {
      "epoch": 7.130133333333333,
      "grad_norm": 0.16810938715934753,
      "learning_rate": 5.436666666666667e-06,
      "loss": 0.0023,
      "step": 133690
    },
    {
      "epoch": 7.1306666666666665,
      "grad_norm": 0.05603772774338722,
      "learning_rate": 5.4333333333333335e-06,
      "loss": 0.0022,
      "step": 133700
    },
    {
      "epoch": 7.1312,
      "grad_norm": 0.14009340107440948,
      "learning_rate": 5.4300000000000005e-06,
      "loss": 0.0027,
      "step": 133710
    },
    {
      "epoch": 7.131733333333333,
      "grad_norm": 0.08405301719903946,
      "learning_rate": 5.426666666666667e-06,
      "loss": 0.002,
      "step": 133720
    },
    {
      "epoch": 7.132266666666666,
      "grad_norm": 0.2521563768386841,
      "learning_rate": 5.423333333333334e-06,
      "loss": 0.0022,
      "step": 133730
    },
    {
      "epoch": 7.1328,
      "grad_norm": 0.224141463637352,
      "learning_rate": 5.42e-06,
      "loss": 0.0023,
      "step": 133740
    },
    {
      "epoch": 7.133333333333334,
      "grad_norm": 0.11207227408885956,
      "learning_rate": 5.416666666666667e-06,
      "loss": 0.0036,
      "step": 133750
    },
    {
      "epoch": 7.133866666666667,
      "grad_norm": 0.16811072826385498,
      "learning_rate": 5.413333333333334e-06,
      "loss": 0.0021,
      "step": 133760
    },
    {
      "epoch": 7.1344,
      "grad_norm": 0.028018562123179436,
      "learning_rate": 5.410000000000001e-06,
      "loss": 0.0031,
      "step": 133770
    },
    {
      "epoch": 7.134933333333334,
      "grad_norm": 0.448294997215271,
      "learning_rate": 5.406666666666667e-06,
      "loss": 0.0032,
      "step": 133780
    },
    {
      "epoch": 7.135466666666667,
      "grad_norm": 0.0840548425912857,
      "learning_rate": 5.403333333333334e-06,
      "loss": 0.0021,
      "step": 133790
    },
    {
      "epoch": 7.136,
      "grad_norm": 0.028017718344926834,
      "learning_rate": 5.4e-06,
      "loss": 0.0021,
      "step": 133800
    },
    {
      "epoch": 7.136533333333333,
      "grad_norm": 0.3081924617290497,
      "learning_rate": 5.396666666666667e-06,
      "loss": 0.0027,
      "step": 133810
    },
    {
      "epoch": 7.137066666666667,
      "grad_norm": 0.08405344933271408,
      "learning_rate": 5.393333333333334e-06,
      "loss": 0.002,
      "step": 133820
    },
    {
      "epoch": 7.1376,
      "grad_norm": 0.19612310826778412,
      "learning_rate": 5.390000000000001e-06,
      "loss": 0.0031,
      "step": 133830
    },
    {
      "epoch": 7.138133333333333,
      "grad_norm": 0.0840507447719574,
      "learning_rate": 5.386666666666667e-06,
      "loss": 0.0028,
      "step": 133840
    },
    {
      "epoch": 7.1386666666666665,
      "grad_norm": 0.2801726162433624,
      "learning_rate": 5.383333333333333e-06,
      "loss": 0.0024,
      "step": 133850
    },
    {
      "epoch": 7.1392,
      "grad_norm": 0.00018633261788636446,
      "learning_rate": 5.38e-06,
      "loss": 0.0026,
      "step": 133860
    },
    {
      "epoch": 7.139733333333333,
      "grad_norm": 0.392232209444046,
      "learning_rate": 5.376666666666666e-06,
      "loss": 0.0028,
      "step": 133870
    },
    {
      "epoch": 7.140266666666666,
      "grad_norm": 8.54589066179301e-10,
      "learning_rate": 5.373333333333333e-06,
      "loss": 0.0011,
      "step": 133880
    },
    {
      "epoch": 7.1408,
      "grad_norm": 8.653943339709258e-09,
      "learning_rate": 5.37e-06,
      "loss": 0.0034,
      "step": 133890
    },
    {
      "epoch": 7.141333333333334,
      "grad_norm": 0.16810829937458038,
      "learning_rate": 5.366666666666667e-06,
      "loss": 0.003,
      "step": 133900
    },
    {
      "epoch": 7.141866666666667,
      "grad_norm": 0.14009526371955872,
      "learning_rate": 5.363333333333333e-06,
      "loss": 0.0023,
      "step": 133910
    },
    {
      "epoch": 7.1424,
      "grad_norm": 0.05603431537747383,
      "learning_rate": 5.36e-06,
      "loss": 0.0037,
      "step": 133920
    },
    {
      "epoch": 7.142933333333334,
      "grad_norm": 0.5603371858596802,
      "learning_rate": 5.3566666666666665e-06,
      "loss": 0.0027,
      "step": 133930
    },
    {
      "epoch": 7.143466666666667,
      "grad_norm": 0.14008978009223938,
      "learning_rate": 5.3533333333333335e-06,
      "loss": 0.0026,
      "step": 133940
    },
    {
      "epoch": 7.144,
      "grad_norm": 0.056036338210105896,
      "learning_rate": 5.3500000000000004e-06,
      "loss": 0.0027,
      "step": 133950
    },
    {
      "epoch": 7.144533333333333,
      "grad_norm": 0.3642292320728302,
      "learning_rate": 5.3466666666666674e-06,
      "loss": 0.0028,
      "step": 133960
    },
    {
      "epoch": 7.145066666666667,
      "grad_norm": 0.08405113220214844,
      "learning_rate": 5.3433333333333336e-06,
      "loss": 0.0019,
      "step": 133970
    },
    {
      "epoch": 7.1456,
      "grad_norm": 0.14008821547031403,
      "learning_rate": 5.3400000000000005e-06,
      "loss": 0.0022,
      "step": 133980
    },
    {
      "epoch": 7.146133333333333,
      "grad_norm": 0.02801770530641079,
      "learning_rate": 5.336666666666667e-06,
      "loss": 0.0026,
      "step": 133990
    },
    {
      "epoch": 7.1466666666666665,
      "grad_norm": 0.16810663044452667,
      "learning_rate": 5.333333333333334e-06,
      "loss": 0.0018,
      "step": 134000
    },
    {
      "epoch": 7.1472,
      "grad_norm": 0.05603562295436859,
      "learning_rate": 5.330000000000001e-06,
      "loss": 0.003,
      "step": 134010
    },
    {
      "epoch": 7.147733333333333,
      "grad_norm": 0.14008884131908417,
      "learning_rate": 5.326666666666667e-06,
      "loss": 0.0024,
      "step": 134020
    },
    {
      "epoch": 7.148266666666666,
      "grad_norm": 0.16810664534568787,
      "learning_rate": 5.323333333333334e-06,
      "loss": 0.0022,
      "step": 134030
    },
    {
      "epoch": 7.1488,
      "grad_norm": 0.05603522062301636,
      "learning_rate": 5.32e-06,
      "loss": 0.003,
      "step": 134040
    },
    {
      "epoch": 7.149333333333334,
      "grad_norm": 0.25215473771095276,
      "learning_rate": 5.316666666666667e-06,
      "loss": 0.0023,
      "step": 134050
    },
    {
      "epoch": 7.149866666666667,
      "grad_norm": 1.2707729313277127e-09,
      "learning_rate": 5.313333333333333e-06,
      "loss": 0.0028,
      "step": 134060
    },
    {
      "epoch": 7.1504,
      "grad_norm": 0.08405299484729767,
      "learning_rate": 5.31e-06,
      "loss": 0.0022,
      "step": 134070
    },
    {
      "epoch": 7.150933333333334,
      "grad_norm": 2.2719244263669225e-09,
      "learning_rate": 5.306666666666667e-06,
      "loss": 0.0024,
      "step": 134080
    },
    {
      "epoch": 7.151466666666667,
      "grad_norm": 0.0012158254394307733,
      "learning_rate": 5.303333333333334e-06,
      "loss": 0.0022,
      "step": 134090
    },
    {
      "epoch": 7.152,
      "grad_norm": 0.05603533983230591,
      "learning_rate": 5.3e-06,
      "loss": 0.0022,
      "step": 134100
    },
    {
      "epoch": 7.152533333333333,
      "grad_norm": 0.11207030713558197,
      "learning_rate": 5.296666666666667e-06,
      "loss": 0.002,
      "step": 134110
    },
    {
      "epoch": 7.153066666666667,
      "grad_norm": 0.7672572135925293,
      "learning_rate": 5.293333333333333e-06,
      "loss": 0.0029,
      "step": 134120
    },
    {
      "epoch": 7.1536,
      "grad_norm": 0.08405232429504395,
      "learning_rate": 5.29e-06,
      "loss": 0.003,
      "step": 134130
    },
    {
      "epoch": 7.154133333333333,
      "grad_norm": 0.19612209498882294,
      "learning_rate": 5.286666666666667e-06,
      "loss": 0.0034,
      "step": 134140
    },
    {
      "epoch": 7.1546666666666665,
      "grad_norm": 0.028017446398735046,
      "learning_rate": 5.283333333333334e-06,
      "loss": 0.0023,
      "step": 134150
    },
    {
      "epoch": 7.1552,
      "grad_norm": 0.057340532541275024,
      "learning_rate": 5.28e-06,
      "loss": 0.0025,
      "step": 134160
    },
    {
      "epoch": 7.155733333333333,
      "grad_norm": 0.11220019310712814,
      "learning_rate": 5.276666666666667e-06,
      "loss": 0.0027,
      "step": 134170
    },
    {
      "epoch": 7.156266666666666,
      "grad_norm": 0.532368004322052,
      "learning_rate": 5.273333333333333e-06,
      "loss": 0.0016,
      "step": 134180
    },
    {
      "epoch": 7.1568,
      "grad_norm": 0.05603546276688576,
      "learning_rate": 5.2699999999999995e-06,
      "loss": 0.0027,
      "step": 134190
    },
    {
      "epoch": 7.157333333333334,
      "grad_norm": 0.11207054555416107,
      "learning_rate": 5.266666666666667e-06,
      "loss": 0.0026,
      "step": 134200
    },
    {
      "epoch": 7.157866666666667,
      "grad_norm": 0.19612227380275726,
      "learning_rate": 5.2633333333333335e-06,
      "loss": 0.0015,
      "step": 134210
    },
    {
      "epoch": 7.1584,
      "grad_norm": 0.19612301886081696,
      "learning_rate": 5.2600000000000005e-06,
      "loss": 0.0024,
      "step": 134220
    },
    {
      "epoch": 7.158933333333334,
      "grad_norm": 0.28018492460250854,
      "learning_rate": 5.256666666666667e-06,
      "loss": 0.0031,
      "step": 134230
    },
    {
      "epoch": 7.159466666666667,
      "grad_norm": 0.1400941163301468,
      "learning_rate": 5.2533333333333336e-06,
      "loss": 0.002,
      "step": 134240
    },
    {
      "epoch": 7.16,
      "grad_norm": 0.02801835536956787,
      "learning_rate": 5.25e-06,
      "loss": 0.0033,
      "step": 134250
    },
    {
      "epoch": 7.160533333333333,
      "grad_norm": 0.2521623373031616,
      "learning_rate": 5.246666666666667e-06,
      "loss": 0.0037,
      "step": 134260
    },
    {
      "epoch": 7.161066666666667,
      "grad_norm": 0.41674551367759705,
      "learning_rate": 5.243333333333334e-06,
      "loss": 0.003,
      "step": 134270
    },
    {
      "epoch": 7.1616,
      "grad_norm": 0.028298906981945038,
      "learning_rate": 5.240000000000001e-06,
      "loss": 0.0029,
      "step": 134280
    },
    {
      "epoch": 7.162133333333333,
      "grad_norm": 0.028017200529575348,
      "learning_rate": 5.236666666666667e-06,
      "loss": 0.002,
      "step": 134290
    },
    {
      "epoch": 7.1626666666666665,
      "grad_norm": 0.05603501573204994,
      "learning_rate": 5.233333333333334e-06,
      "loss": 0.0033,
      "step": 134300
    },
    {
      "epoch": 7.1632,
      "grad_norm": 0.08405367285013199,
      "learning_rate": 5.23e-06,
      "loss": 0.0026,
      "step": 134310
    },
    {
      "epoch": 7.163733333333333,
      "grad_norm": 0.028018038719892502,
      "learning_rate": 5.226666666666667e-06,
      "loss": 0.0024,
      "step": 134320
    },
    {
      "epoch": 7.164266666666666,
      "grad_norm": 0.11217635869979858,
      "learning_rate": 5.223333333333334e-06,
      "loss": 0.0029,
      "step": 134330
    },
    {
      "epoch": 7.1648,
      "grad_norm": 0.16810454428195953,
      "learning_rate": 5.220000000000001e-06,
      "loss": 0.0022,
      "step": 134340
    },
    {
      "epoch": 7.165333333333333,
      "grad_norm": 0.28017932176589966,
      "learning_rate": 5.216666666666667e-06,
      "loss": 0.002,
      "step": 134350
    },
    {
      "epoch": 7.165866666666667,
      "grad_norm": 0.05603623017668724,
      "learning_rate": 5.213333333333333e-06,
      "loss": 0.0022,
      "step": 134360
    },
    {
      "epoch": 7.1664,
      "grad_norm": 0.14008718729019165,
      "learning_rate": 5.21e-06,
      "loss": 0.0023,
      "step": 134370
    },
    {
      "epoch": 7.166933333333334,
      "grad_norm": 0.028017472475767136,
      "learning_rate": 5.206666666666666e-06,
      "loss": 0.0021,
      "step": 134380
    },
    {
      "epoch": 7.167466666666667,
      "grad_norm": 0.2801778316497803,
      "learning_rate": 5.203333333333334e-06,
      "loss": 0.0021,
      "step": 134390
    },
    {
      "epoch": 7.168,
      "grad_norm": 0.2045149803161621,
      "learning_rate": 5.2e-06,
      "loss": 0.0038,
      "step": 134400
    },
    {
      "epoch": 7.168533333333333,
      "grad_norm": 0.02801760844886303,
      "learning_rate": 5.196666666666667e-06,
      "loss": 0.003,
      "step": 134410
    },
    {
      "epoch": 7.169066666666667,
      "grad_norm": 0.05603472888469696,
      "learning_rate": 5.193333333333333e-06,
      "loss": 0.0038,
      "step": 134420
    },
    {
      "epoch": 7.1696,
      "grad_norm": 0.1681017279624939,
      "learning_rate": 5.19e-06,
      "loss": 0.0025,
      "step": 134430
    },
    {
      "epoch": 7.170133333333333,
      "grad_norm": 0.08404981344938278,
      "learning_rate": 5.186666666666666e-06,
      "loss": 0.0022,
      "step": 134440
    },
    {
      "epoch": 7.1706666666666665,
      "grad_norm": 0.47629019618034363,
      "learning_rate": 5.183333333333333e-06,
      "loss": 0.0024,
      "step": 134450
    },
    {
      "epoch": 7.1712,
      "grad_norm": 0.028017276898026466,
      "learning_rate": 5.18e-06,
      "loss": 0.0022,
      "step": 134460
    },
    {
      "epoch": 7.171733333333333,
      "grad_norm": 0.14008700847625732,
      "learning_rate": 5.176666666666667e-06,
      "loss": 0.0022,
      "step": 134470
    },
    {
      "epoch": 7.172266666666666,
      "grad_norm": 0.6546790599822998,
      "learning_rate": 5.1733333333333335e-06,
      "loss": 0.0017,
      "step": 134480
    },
    {
      "epoch": 7.1728,
      "grad_norm": 0.25216686725616455,
      "learning_rate": 5.1700000000000005e-06,
      "loss": 0.0024,
      "step": 134490
    },
    {
      "epoch": 7.173333333333334,
      "grad_norm": 5.5994382464064074e-09,
      "learning_rate": 5.166666666666667e-06,
      "loss": 0.0029,
      "step": 134500
    },
    {
      "epoch": 7.173866666666667,
      "grad_norm": 0.2521565854549408,
      "learning_rate": 5.163333333333334e-06,
      "loss": 0.0038,
      "step": 134510
    },
    {
      "epoch": 7.1744,
      "grad_norm": 0.39348915219306946,
      "learning_rate": 5.1600000000000006e-06,
      "loss": 0.0028,
      "step": 134520
    },
    {
      "epoch": 7.174933333333334,
      "grad_norm": 0.22413760423660278,
      "learning_rate": 5.156666666666667e-06,
      "loss": 0.003,
      "step": 134530
    },
    {
      "epoch": 7.175466666666667,
      "grad_norm": 0.08405017852783203,
      "learning_rate": 5.153333333333334e-06,
      "loss": 0.0026,
      "step": 134540
    },
    {
      "epoch": 7.176,
      "grad_norm": 0.2801685631275177,
      "learning_rate": 5.15e-06,
      "loss": 0.0018,
      "step": 134550
    },
    {
      "epoch": 7.176533333333333,
      "grad_norm": 0.11206939816474915,
      "learning_rate": 5.146666666666667e-06,
      "loss": 0.0038,
      "step": 134560
    },
    {
      "epoch": 7.177066666666667,
      "grad_norm": 1.8266477219697208e-09,
      "learning_rate": 5.143333333333333e-06,
      "loss": 0.002,
      "step": 134570
    },
    {
      "epoch": 7.1776,
      "grad_norm": 0.0280179213732481,
      "learning_rate": 5.140000000000001e-06,
      "loss": 0.0036,
      "step": 134580
    },
    {
      "epoch": 7.178133333333333,
      "grad_norm": 0.08405447751283646,
      "learning_rate": 5.136666666666667e-06,
      "loss": 0.0029,
      "step": 134590
    },
    {
      "epoch": 7.1786666666666665,
      "grad_norm": 0.0280184056609869,
      "learning_rate": 5.133333333333334e-06,
      "loss": 0.0023,
      "step": 134600
    },
    {
      "epoch": 7.1792,
      "grad_norm": 0.420269638299942,
      "learning_rate": 5.13e-06,
      "loss": 0.0029,
      "step": 134610
    },
    {
      "epoch": 7.179733333333333,
      "grad_norm": 0.028017275035381317,
      "learning_rate": 5.126666666666667e-06,
      "loss": 0.0028,
      "step": 134620
    },
    {
      "epoch": 7.180266666666666,
      "grad_norm": 0.3081810772418976,
      "learning_rate": 5.123333333333333e-06,
      "loss": 0.0041,
      "step": 134630
    },
    {
      "epoch": 7.1808,
      "grad_norm": 0.028016895055770874,
      "learning_rate": 5.12e-06,
      "loss": 0.0028,
      "step": 134640
    },
    {
      "epoch": 7.181333333333333,
      "grad_norm": 0.11207043379545212,
      "learning_rate": 5.116666666666667e-06,
      "loss": 0.0033,
      "step": 134650
    },
    {
      "epoch": 7.181866666666667,
      "grad_norm": 0.2801728844642639,
      "learning_rate": 5.113333333333334e-06,
      "loss": 0.003,
      "step": 134660
    },
    {
      "epoch": 7.1824,
      "grad_norm": 0.2801752984523773,
      "learning_rate": 5.11e-06,
      "loss": 0.003,
      "step": 134670
    },
    {
      "epoch": 7.182933333333334,
      "grad_norm": 0.25213107466697693,
      "learning_rate": 5.106666666666667e-06,
      "loss": 0.0026,
      "step": 134680
    },
    {
      "epoch": 7.183466666666667,
      "grad_norm": 0.02814825251698494,
      "learning_rate": 5.103333333333333e-06,
      "loss": 0.0027,
      "step": 134690
    },
    {
      "epoch": 7.184,
      "grad_norm": 0.1681058555841446,
      "learning_rate": 5.1e-06,
      "loss": 0.0031,
      "step": 134700
    },
    {
      "epoch": 7.184533333333333,
      "grad_norm": 0.11206931620836258,
      "learning_rate": 5.096666666666667e-06,
      "loss": 0.0018,
      "step": 134710
    },
    {
      "epoch": 7.185066666666667,
      "grad_norm": 0.11206918954849243,
      "learning_rate": 5.093333333333333e-06,
      "loss": 0.0022,
      "step": 134720
    },
    {
      "epoch": 7.1856,
      "grad_norm": 0.056035153567790985,
      "learning_rate": 5.09e-06,
      "loss": 0.0022,
      "step": 134730
    },
    {
      "epoch": 7.186133333333333,
      "grad_norm": 0.1681034415960312,
      "learning_rate": 5.0866666666666665e-06,
      "loss": 0.0023,
      "step": 134740
    },
    {
      "epoch": 7.1866666666666665,
      "grad_norm": 0.028016868978738785,
      "learning_rate": 5.0833333333333335e-06,
      "loss": 0.0024,
      "step": 134750
    },
    {
      "epoch": 7.1872,
      "grad_norm": 0.0054900082759559155,
      "learning_rate": 5.08e-06,
      "loss": 0.0028,
      "step": 134760
    },
    {
      "epoch": 7.187733333333333,
      "grad_norm": 0.2521520256996155,
      "learning_rate": 5.0766666666666675e-06,
      "loss": 0.0027,
      "step": 134770
    },
    {
      "epoch": 7.188266666666666,
      "grad_norm": 0.4763043224811554,
      "learning_rate": 5.073333333333334e-06,
      "loss": 0.0026,
      "step": 134780
    },
    {
      "epoch": 7.1888,
      "grad_norm": 0.48057103157043457,
      "learning_rate": 5.070000000000001e-06,
      "loss": 0.0028,
      "step": 134790
    },
    {
      "epoch": 7.189333333333333,
      "grad_norm": 0.1681048721075058,
      "learning_rate": 5.066666666666667e-06,
      "loss": 0.0026,
      "step": 134800
    },
    {
      "epoch": 7.189866666666667,
      "grad_norm": 0.11207128316164017,
      "learning_rate": 5.063333333333334e-06,
      "loss": 0.0027,
      "step": 134810
    },
    {
      "epoch": 7.1904,
      "grad_norm": 1.5992627143859863,
      "learning_rate": 5.06e-06,
      "loss": 0.0028,
      "step": 134820
    },
    {
      "epoch": 7.190933333333334,
      "grad_norm": 0.1961151659488678,
      "learning_rate": 5.056666666666667e-06,
      "loss": 0.0019,
      "step": 134830
    },
    {
      "epoch": 7.191466666666667,
      "grad_norm": 0.028016529977321625,
      "learning_rate": 5.053333333333334e-06,
      "loss": 0.0015,
      "step": 134840
    },
    {
      "epoch": 7.192,
      "grad_norm": 0.16810111701488495,
      "learning_rate": 5.050000000000001e-06,
      "loss": 0.0024,
      "step": 134850
    },
    {
      "epoch": 7.1925333333333334,
      "grad_norm": 0.05603338032960892,
      "learning_rate": 5.046666666666667e-06,
      "loss": 0.0026,
      "step": 134860
    },
    {
      "epoch": 7.193066666666667,
      "grad_norm": 0.02801653929054737,
      "learning_rate": 5.043333333333333e-06,
      "loss": 0.0022,
      "step": 134870
    },
    {
      "epoch": 7.1936,
      "grad_norm": 0.11206749826669693,
      "learning_rate": 5.04e-06,
      "loss": 0.0031,
      "step": 134880
    },
    {
      "epoch": 7.194133333333333,
      "grad_norm": 0.08405238389968872,
      "learning_rate": 5.036666666666667e-06,
      "loss": 0.0017,
      "step": 134890
    },
    {
      "epoch": 7.1946666666666665,
      "grad_norm": 0.3081875145435333,
      "learning_rate": 5.033333333333334e-06,
      "loss": 0.0025,
      "step": 134900
    },
    {
      "epoch": 7.1952,
      "grad_norm": 0.19611698389053345,
      "learning_rate": 5.03e-06,
      "loss": 0.0025,
      "step": 134910
    },
    {
      "epoch": 7.195733333333333,
      "grad_norm": 0.2241334170103073,
      "learning_rate": 5.026666666666667e-06,
      "loss": 0.0021,
      "step": 134920
    },
    {
      "epoch": 7.196266666666666,
      "grad_norm": 0.028016511350870132,
      "learning_rate": 5.023333333333333e-06,
      "loss": 0.0026,
      "step": 134930
    },
    {
      "epoch": 7.1968,
      "grad_norm": 0.14008501172065735,
      "learning_rate": 5.02e-06,
      "loss": 0.0022,
      "step": 134940
    },
    {
      "epoch": 7.197333333333333,
      "grad_norm": 0.1120687797665596,
      "learning_rate": 5.016666666666666e-06,
      "loss": 0.0021,
      "step": 134950
    },
    {
      "epoch": 7.197866666666667,
      "grad_norm": 0.0280168280005455,
      "learning_rate": 5.013333333333334e-06,
      "loss": 0.0024,
      "step": 134960
    },
    {
      "epoch": 7.1984,
      "grad_norm": 0.08405042439699173,
      "learning_rate": 5.01e-06,
      "loss": 0.0032,
      "step": 134970
    },
    {
      "epoch": 7.198933333333334,
      "grad_norm": 0.112066350877285,
      "learning_rate": 5.006666666666667e-06,
      "loss": 0.0025,
      "step": 134980
    },
    {
      "epoch": 7.199466666666667,
      "grad_norm": 0.11206629127264023,
      "learning_rate": 5.0033333333333334e-06,
      "loss": 0.0038,
      "step": 134990
    },
    {
      "epoch": 7.2,
      "grad_norm": 0.11206697672605515,
      "learning_rate": 5e-06,
      "loss": 0.0026,
      "step": 135000
    },
    {
      "epoch": 7.2005333333333335,
      "grad_norm": 0.028016841039061546,
      "learning_rate": 4.9966666666666665e-06,
      "loss": 0.003,
      "step": 135010
    },
    {
      "epoch": 7.201066666666667,
      "grad_norm": 0.11206386983394623,
      "learning_rate": 4.9933333333333335e-06,
      "loss": 0.0017,
      "step": 135020
    },
    {
      "epoch": 7.2016,
      "grad_norm": 0.028016870841383934,
      "learning_rate": 4.9900000000000005e-06,
      "loss": 0.0027,
      "step": 135030
    },
    {
      "epoch": 7.202133333333333,
      "grad_norm": 0.08405344933271408,
      "learning_rate": 4.986666666666667e-06,
      "loss": 0.0023,
      "step": 135040
    },
    {
      "epoch": 7.2026666666666666,
      "grad_norm": 1.6939974978313899e-09,
      "learning_rate": 4.983333333333334e-06,
      "loss": 0.0023,
      "step": 135050
    },
    {
      "epoch": 7.2032,
      "grad_norm": 0.2241406887769699,
      "learning_rate": 4.98e-06,
      "loss": 0.0027,
      "step": 135060
    },
    {
      "epoch": 7.203733333333333,
      "grad_norm": 0.006966999266296625,
      "learning_rate": 4.976666666666667e-06,
      "loss": 0.0021,
      "step": 135070
    },
    {
      "epoch": 7.204266666666666,
      "grad_norm": 0.11206703633069992,
      "learning_rate": 4.973333333333334e-06,
      "loss": 0.002,
      "step": 135080
    },
    {
      "epoch": 7.2048,
      "grad_norm": 0.1120666116476059,
      "learning_rate": 4.970000000000001e-06,
      "loss": 0.0019,
      "step": 135090
    },
    {
      "epoch": 7.205333333333333,
      "grad_norm": 0.19611477851867676,
      "learning_rate": 4.966666666666667e-06,
      "loss": 0.0021,
      "step": 135100
    },
    {
      "epoch": 7.205866666666667,
      "grad_norm": 0.05603291466832161,
      "learning_rate": 4.963333333333334e-06,
      "loss": 0.0025,
      "step": 135110
    },
    {
      "epoch": 7.2064,
      "grad_norm": 0.35470741987228394,
      "learning_rate": 4.96e-06,
      "loss": 0.0021,
      "step": 135120
    },
    {
      "epoch": 7.206933333333334,
      "grad_norm": 0.19611814618110657,
      "learning_rate": 4.956666666666667e-06,
      "loss": 0.0023,
      "step": 135130
    },
    {
      "epoch": 7.207466666666667,
      "grad_norm": 0.028016822412610054,
      "learning_rate": 4.953333333333333e-06,
      "loss": 0.0036,
      "step": 135140
    },
    {
      "epoch": 7.208,
      "grad_norm": 0.8268730044364929,
      "learning_rate": 4.950000000000001e-06,
      "loss": 0.0025,
      "step": 135150
    },
    {
      "epoch": 7.2085333333333335,
      "grad_norm": 0.25215786695480347,
      "learning_rate": 4.946666666666667e-06,
      "loss": 0.0024,
      "step": 135160
    },
    {
      "epoch": 7.209066666666667,
      "grad_norm": 0.05603424459695816,
      "learning_rate": 4.943333333333334e-06,
      "loss": 0.0031,
      "step": 135170
    },
    {
      "epoch": 7.2096,
      "grad_norm": 1.4000878234554648e-09,
      "learning_rate": 4.94e-06,
      "loss": 0.0023,
      "step": 135180
    },
    {
      "epoch": 7.210133333333333,
      "grad_norm": 0.2521555721759796,
      "learning_rate": 4.936666666666667e-06,
      "loss": 0.003,
      "step": 135190
    },
    {
      "epoch": 7.210666666666667,
      "grad_norm": 0.16911795735359192,
      "learning_rate": 4.933333333333333e-06,
      "loss": 0.0028,
      "step": 135200
    },
    {
      "epoch": 7.2112,
      "grad_norm": 0.14008226990699768,
      "learning_rate": 4.93e-06,
      "loss": 0.0023,
      "step": 135210
    },
    {
      "epoch": 7.211733333333333,
      "grad_norm": 0.1122138723731041,
      "learning_rate": 4.926666666666667e-06,
      "loss": 0.0024,
      "step": 135220
    },
    {
      "epoch": 7.212266666666666,
      "grad_norm": 0.4202549457550049,
      "learning_rate": 4.923333333333333e-06,
      "loss": 0.002,
      "step": 135230
    },
    {
      "epoch": 7.2128,
      "grad_norm": 0.1400817483663559,
      "learning_rate": 4.92e-06,
      "loss": 0.0026,
      "step": 135240
    },
    {
      "epoch": 7.213333333333333,
      "grad_norm": 0.4245535731315613,
      "learning_rate": 4.9166666666666665e-06,
      "loss": 0.0031,
      "step": 135250
    },
    {
      "epoch": 7.213866666666667,
      "grad_norm": 0.25215619802474976,
      "learning_rate": 4.9133333333333334e-06,
      "loss": 0.0018,
      "step": 135260
    },
    {
      "epoch": 7.2144,
      "grad_norm": 0.02801710180938244,
      "learning_rate": 4.9100000000000004e-06,
      "loss": 0.0031,
      "step": 135270
    },
    {
      "epoch": 7.214933333333334,
      "grad_norm": 0.33620572090148926,
      "learning_rate": 4.906666666666667e-06,
      "loss": 0.0024,
      "step": 135280
    },
    {
      "epoch": 7.215466666666667,
      "grad_norm": 1.1088323593139648,
      "learning_rate": 4.9033333333333335e-06,
      "loss": 0.0032,
      "step": 135290
    },
    {
      "epoch": 7.216,
      "grad_norm": 0.19611814618110657,
      "learning_rate": 4.9000000000000005e-06,
      "loss": 0.0017,
      "step": 135300
    },
    {
      "epoch": 7.2165333333333335,
      "grad_norm": 0.16810397803783417,
      "learning_rate": 4.896666666666667e-06,
      "loss": 0.0022,
      "step": 135310
    },
    {
      "epoch": 7.217066666666667,
      "grad_norm": 0.14008451998233795,
      "learning_rate": 4.893333333333334e-06,
      "loss": 0.003,
      "step": 135320
    },
    {
      "epoch": 7.2176,
      "grad_norm": 0.25214749574661255,
      "learning_rate": 4.89e-06,
      "loss": 0.0024,
      "step": 135330
    },
    {
      "epoch": 7.218133333333333,
      "grad_norm": 0.16810189187526703,
      "learning_rate": 4.886666666666667e-06,
      "loss": 0.0033,
      "step": 135340
    },
    {
      "epoch": 7.218666666666667,
      "grad_norm": 0.39017319679260254,
      "learning_rate": 4.883333333333334e-06,
      "loss": 0.0025,
      "step": 135350
    },
    {
      "epoch": 7.2192,
      "grad_norm": 0.28017404675483704,
      "learning_rate": 4.880000000000001e-06,
      "loss": 0.003,
      "step": 135360
    },
    {
      "epoch": 7.219733333333333,
      "grad_norm": 0.16809949278831482,
      "learning_rate": 4.876666666666667e-06,
      "loss": 0.0023,
      "step": 135370
    },
    {
      "epoch": 7.220266666666666,
      "grad_norm": 0.05603352561593056,
      "learning_rate": 4.873333333333333e-06,
      "loss": 0.0025,
      "step": 135380
    },
    {
      "epoch": 7.2208,
      "grad_norm": 0.2521587610244751,
      "learning_rate": 4.87e-06,
      "loss": 0.0029,
      "step": 135390
    },
    {
      "epoch": 7.221333333333333,
      "grad_norm": 4.21817958340398e-09,
      "learning_rate": 4.866666666666667e-06,
      "loss": 0.0028,
      "step": 135400
    },
    {
      "epoch": 7.221866666666667,
      "grad_norm": 0.08405385166406631,
      "learning_rate": 4.863333333333334e-06,
      "loss": 0.0015,
      "step": 135410
    },
    {
      "epoch": 7.2224,
      "grad_norm": 0.02801831252872944,
      "learning_rate": 4.86e-06,
      "loss": 0.0026,
      "step": 135420
    },
    {
      "epoch": 7.222933333333334,
      "grad_norm": 0.23105870187282562,
      "learning_rate": 4.856666666666667e-06,
      "loss": 0.0029,
      "step": 135430
    },
    {
      "epoch": 7.223466666666667,
      "grad_norm": 0.19612103700637817,
      "learning_rate": 4.853333333333333e-06,
      "loss": 0.003,
      "step": 135440
    },
    {
      "epoch": 7.224,
      "grad_norm": 0.16877976059913635,
      "learning_rate": 4.85e-06,
      "loss": 0.0019,
      "step": 135450
    },
    {
      "epoch": 7.2245333333333335,
      "grad_norm": 0.0840507447719574,
      "learning_rate": 4.846666666666667e-06,
      "loss": 0.0015,
      "step": 135460
    },
    {
      "epoch": 7.225066666666667,
      "grad_norm": 0.08404979854822159,
      "learning_rate": 4.843333333333334e-06,
      "loss": 0.0023,
      "step": 135470
    },
    {
      "epoch": 7.2256,
      "grad_norm": 0.47628253698349,
      "learning_rate": 4.84e-06,
      "loss": 0.0044,
      "step": 135480
    },
    {
      "epoch": 7.226133333333333,
      "grad_norm": 0.11206711828708649,
      "learning_rate": 4.836666666666667e-06,
      "loss": 0.0019,
      "step": 135490
    },
    {
      "epoch": 7.226666666666667,
      "grad_norm": 0.19612114131450653,
      "learning_rate": 4.833333333333333e-06,
      "loss": 0.0025,
      "step": 135500
    },
    {
      "epoch": 7.2272,
      "grad_norm": 0.02801806665956974,
      "learning_rate": 4.83e-06,
      "loss": 0.0029,
      "step": 135510
    },
    {
      "epoch": 7.227733333333333,
      "grad_norm": 0.3362131118774414,
      "learning_rate": 4.8266666666666665e-06,
      "loss": 0.0018,
      "step": 135520
    },
    {
      "epoch": 7.228266666666666,
      "grad_norm": 3.77991815625478e-09,
      "learning_rate": 4.8233333333333335e-06,
      "loss": 0.0024,
      "step": 135530
    },
    {
      "epoch": 7.2288,
      "grad_norm": 0.028017031028866768,
      "learning_rate": 4.8200000000000004e-06,
      "loss": 0.0031,
      "step": 135540
    },
    {
      "epoch": 7.229333333333333,
      "grad_norm": 0.05603325366973877,
      "learning_rate": 4.816666666666667e-06,
      "loss": 0.002,
      "step": 135550
    },
    {
      "epoch": 7.229866666666666,
      "grad_norm": 0.13058926165103912,
      "learning_rate": 4.8133333333333336e-06,
      "loss": 0.0028,
      "step": 135560
    },
    {
      "epoch": 7.2304,
      "grad_norm": 0.02801719680428505,
      "learning_rate": 4.81e-06,
      "loss": 0.0024,
      "step": 135570
    },
    {
      "epoch": 7.230933333333334,
      "grad_norm": 0.14008501172065735,
      "learning_rate": 4.806666666666667e-06,
      "loss": 0.003,
      "step": 135580
    },
    {
      "epoch": 7.231466666666667,
      "grad_norm": 0.3081836402416229,
      "learning_rate": 4.803333333333334e-06,
      "loss": 0.0019,
      "step": 135590
    },
    {
      "epoch": 7.232,
      "grad_norm": 3.289578387466463e-09,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.0034,
      "step": 135600
    },
    {
      "epoch": 7.2325333333333335,
      "grad_norm": 0.30818524956703186,
      "learning_rate": 4.796666666666667e-06,
      "loss": 0.003,
      "step": 135610
    },
    {
      "epoch": 7.233066666666667,
      "grad_norm": 0.056033678352832794,
      "learning_rate": 4.793333333333334e-06,
      "loss": 0.0037,
      "step": 135620
    },
    {
      "epoch": 7.2336,
      "grad_norm": 0.08405102789402008,
      "learning_rate": 4.79e-06,
      "loss": 0.0022,
      "step": 135630
    },
    {
      "epoch": 7.234133333333333,
      "grad_norm": 0.11206928640604019,
      "learning_rate": 4.786666666666667e-06,
      "loss": 0.0032,
      "step": 135640
    },
    {
      "epoch": 7.234666666666667,
      "grad_norm": 0.05474090576171875,
      "learning_rate": 4.783333333333333e-06,
      "loss": 0.0022,
      "step": 135650
    },
    {
      "epoch": 7.2352,
      "grad_norm": 0.16810359060764313,
      "learning_rate": 4.780000000000001e-06,
      "loss": 0.0023,
      "step": 135660
    },
    {
      "epoch": 7.235733333333333,
      "grad_norm": 4.345494186708265e-09,
      "learning_rate": 4.776666666666667e-06,
      "loss": 0.0023,
      "step": 135670
    },
    {
      "epoch": 7.236266666666666,
      "grad_norm": 1.1550490856170654,
      "learning_rate": 4.773333333333334e-06,
      "loss": 0.0027,
      "step": 135680
    },
    {
      "epoch": 7.2368,
      "grad_norm": 0.056034281849861145,
      "learning_rate": 4.77e-06,
      "loss": 0.003,
      "step": 135690
    },
    {
      "epoch": 7.237333333333333,
      "grad_norm": 0.11206705123186111,
      "learning_rate": 4.766666666666667e-06,
      "loss": 0.002,
      "step": 135700
    },
    {
      "epoch": 7.237866666666667,
      "grad_norm": 0.028016313910484314,
      "learning_rate": 4.763333333333333e-06,
      "loss": 0.002,
      "step": 135710
    },
    {
      "epoch": 7.2384,
      "grad_norm": 0.22413299977779388,
      "learning_rate": 4.76e-06,
      "loss": 0.0018,
      "step": 135720
    },
    {
      "epoch": 7.238933333333334,
      "grad_norm": 0.05603388324379921,
      "learning_rate": 4.756666666666667e-06,
      "loss": 0.0016,
      "step": 135730
    },
    {
      "epoch": 7.239466666666667,
      "grad_norm": 0.2521550953388214,
      "learning_rate": 4.753333333333333e-06,
      "loss": 0.0019,
      "step": 135740
    },
    {
      "epoch": 7.24,
      "grad_norm": 0.08405090868473053,
      "learning_rate": 4.75e-06,
      "loss": 0.0024,
      "step": 135750
    },
    {
      "epoch": 7.2405333333333335,
      "grad_norm": 0.0840517058968544,
      "learning_rate": 4.746666666666666e-06,
      "loss": 0.0027,
      "step": 135760
    },
    {
      "epoch": 7.241066666666667,
      "grad_norm": 0.1961183249950409,
      "learning_rate": 4.743333333333333e-06,
      "loss": 0.0021,
      "step": 135770
    },
    {
      "epoch": 7.2416,
      "grad_norm": 0.11206605285406113,
      "learning_rate": 4.74e-06,
      "loss": 0.0026,
      "step": 135780
    },
    {
      "epoch": 7.242133333333333,
      "grad_norm": 0.19611574709415436,
      "learning_rate": 4.736666666666667e-06,
      "loss": 0.0035,
      "step": 135790
    },
    {
      "epoch": 7.242666666666667,
      "grad_norm": 0.3922392725944519,
      "learning_rate": 4.7333333333333335e-06,
      "loss": 0.0026,
      "step": 135800
    },
    {
      "epoch": 7.2432,
      "grad_norm": 0.08405111730098724,
      "learning_rate": 4.7300000000000005e-06,
      "loss": 0.0025,
      "step": 135810
    },
    {
      "epoch": 7.243733333333333,
      "grad_norm": 0.16810056567192078,
      "learning_rate": 4.726666666666667e-06,
      "loss": 0.0019,
      "step": 135820
    },
    {
      "epoch": 7.244266666666666,
      "grad_norm": 0.19611714780330658,
      "learning_rate": 4.7233333333333336e-06,
      "loss": 0.0017,
      "step": 135830
    },
    {
      "epoch": 7.2448,
      "grad_norm": 0.02801770716905594,
      "learning_rate": 4.72e-06,
      "loss": 0.0029,
      "step": 135840
    },
    {
      "epoch": 7.245333333333333,
      "grad_norm": 0.028017498552799225,
      "learning_rate": 4.7166666666666675e-06,
      "loss": 0.0036,
      "step": 135850
    },
    {
      "epoch": 7.245866666666666,
      "grad_norm": 0.028017355129122734,
      "learning_rate": 4.713333333333334e-06,
      "loss": 0.002,
      "step": 135860
    },
    {
      "epoch": 7.2464,
      "grad_norm": 0.05603577196598053,
      "learning_rate": 4.710000000000001e-06,
      "loss": 0.0022,
      "step": 135870
    },
    {
      "epoch": 7.246933333333334,
      "grad_norm": 0.05603579431772232,
      "learning_rate": 4.706666666666667e-06,
      "loss": 0.0022,
      "step": 135880
    },
    {
      "epoch": 7.247466666666667,
      "grad_norm": 0.34236326813697815,
      "learning_rate": 4.703333333333334e-06,
      "loss": 0.0027,
      "step": 135890
    },
    {
      "epoch": 7.248,
      "grad_norm": 0.22413422167301178,
      "learning_rate": 4.7e-06,
      "loss": 0.0017,
      "step": 135900
    },
    {
      "epoch": 7.2485333333333335,
      "grad_norm": 0.14022156596183777,
      "learning_rate": 4.696666666666667e-06,
      "loss": 0.0026,
      "step": 135910
    },
    {
      "epoch": 7.249066666666667,
      "grad_norm": 0.11206653714179993,
      "learning_rate": 4.693333333333334e-06,
      "loss": 0.0028,
      "step": 135920
    },
    {
      "epoch": 7.2496,
      "grad_norm": 0.28016892075538635,
      "learning_rate": 4.69e-06,
      "loss": 0.0014,
      "step": 135930
    },
    {
      "epoch": 7.250133333333333,
      "grad_norm": 0.2801722288131714,
      "learning_rate": 4.686666666666667e-06,
      "loss": 0.0023,
      "step": 135940
    },
    {
      "epoch": 7.250666666666667,
      "grad_norm": 0.02801746502518654,
      "learning_rate": 4.683333333333333e-06,
      "loss": 0.0032,
      "step": 135950
    },
    {
      "epoch": 7.2512,
      "grad_norm": 0.22413760423660278,
      "learning_rate": 4.68e-06,
      "loss": 0.0028,
      "step": 135960
    },
    {
      "epoch": 7.251733333333333,
      "grad_norm": 0.1681000143289566,
      "learning_rate": 4.676666666666667e-06,
      "loss": 0.0027,
      "step": 135970
    },
    {
      "epoch": 7.252266666666666,
      "grad_norm": 0.1120678260922432,
      "learning_rate": 4.673333333333334e-06,
      "loss": 0.0032,
      "step": 135980
    },
    {
      "epoch": 7.2528,
      "grad_norm": 0.05603455379605293,
      "learning_rate": 4.67e-06,
      "loss": 0.0029,
      "step": 135990
    },
    {
      "epoch": 7.253333333333333,
      "grad_norm": 0.08405160903930664,
      "learning_rate": 4.666666666666667e-06,
      "loss": 0.0024,
      "step": 136000
    },
    {
      "epoch": 7.253866666666667,
      "grad_norm": 0.08405119925737381,
      "learning_rate": 4.663333333333333e-06,
      "loss": 0.002,
      "step": 136010
    },
    {
      "epoch": 7.2544,
      "grad_norm": 0.05603424087166786,
      "learning_rate": 4.66e-06,
      "loss": 0.0027,
      "step": 136020
    },
    {
      "epoch": 7.254933333333334,
      "grad_norm": 0.14008568227291107,
      "learning_rate": 4.656666666666666e-06,
      "loss": 0.0016,
      "step": 136030
    },
    {
      "epoch": 7.255466666666667,
      "grad_norm": 0.2241344451904297,
      "learning_rate": 4.653333333333334e-06,
      "loss": 0.0022,
      "step": 136040
    },
    {
      "epoch": 7.256,
      "grad_norm": 0.11206559836864471,
      "learning_rate": 4.65e-06,
      "loss": 0.0029,
      "step": 136050
    },
    {
      "epoch": 7.2565333333333335,
      "grad_norm": 0.28016597032546997,
      "learning_rate": 4.646666666666667e-06,
      "loss": 0.0025,
      "step": 136060
    },
    {
      "epoch": 7.257066666666667,
      "grad_norm": 0.11206752061843872,
      "learning_rate": 4.6433333333333335e-06,
      "loss": 0.0025,
      "step": 136070
    },
    {
      "epoch": 7.2576,
      "grad_norm": 0.11206819862127304,
      "learning_rate": 4.64e-06,
      "loss": 0.0024,
      "step": 136080
    },
    {
      "epoch": 7.258133333333333,
      "grad_norm": 0.21449649333953857,
      "learning_rate": 4.636666666666667e-06,
      "loss": 0.0023,
      "step": 136090
    },
    {
      "epoch": 7.258666666666667,
      "grad_norm": 0.28017765283584595,
      "learning_rate": 4.633333333333334e-06,
      "loss": 0.0024,
      "step": 136100
    },
    {
      "epoch": 7.2592,
      "grad_norm": 0.14008885622024536,
      "learning_rate": 4.6300000000000006e-06,
      "loss": 0.0023,
      "step": 136110
    },
    {
      "epoch": 7.259733333333333,
      "grad_norm": 0.028017284348607063,
      "learning_rate": 4.626666666666667e-06,
      "loss": 0.0021,
      "step": 136120
    },
    {
      "epoch": 7.260266666666666,
      "grad_norm": 0.5323242545127869,
      "learning_rate": 4.623333333333334e-06,
      "loss": 0.0024,
      "step": 136130
    },
    {
      "epoch": 7.2608,
      "grad_norm": 0.14008395373821259,
      "learning_rate": 4.62e-06,
      "loss": 0.0029,
      "step": 136140
    },
    {
      "epoch": 7.261333333333333,
      "grad_norm": 0.028016813099384308,
      "learning_rate": 4.616666666666667e-06,
      "loss": 0.0028,
      "step": 136150
    },
    {
      "epoch": 7.261866666666666,
      "grad_norm": 0.11206812411546707,
      "learning_rate": 4.613333333333334e-06,
      "loss": 0.0028,
      "step": 136160
    },
    {
      "epoch": 7.2624,
      "grad_norm": 0.05603368207812309,
      "learning_rate": 4.610000000000001e-06,
      "loss": 0.0024,
      "step": 136170
    },
    {
      "epoch": 7.262933333333334,
      "grad_norm": 0.16810117661952972,
      "learning_rate": 4.606666666666667e-06,
      "loss": 0.0024,
      "step": 136180
    },
    {
      "epoch": 7.263466666666667,
      "grad_norm": 0.11206802725791931,
      "learning_rate": 4.603333333333334e-06,
      "loss": 0.0026,
      "step": 136190
    },
    {
      "epoch": 7.264,
      "grad_norm": 0.16810134053230286,
      "learning_rate": 4.6e-06,
      "loss": 0.0028,
      "step": 136200
    },
    {
      "epoch": 7.2645333333333335,
      "grad_norm": 0.08569302409887314,
      "learning_rate": 4.596666666666667e-06,
      "loss": 0.0032,
      "step": 136210
    },
    {
      "epoch": 7.265066666666667,
      "grad_norm": 0.028016699478030205,
      "learning_rate": 4.593333333333333e-06,
      "loss": 0.0021,
      "step": 136220
    },
    {
      "epoch": 7.2656,
      "grad_norm": 0.17378972470760345,
      "learning_rate": 4.590000000000001e-06,
      "loss": 0.0024,
      "step": 136230
    },
    {
      "epoch": 7.266133333333333,
      "grad_norm": 0.11206752061843872,
      "learning_rate": 4.586666666666667e-06,
      "loss": 0.0028,
      "step": 136240
    },
    {
      "epoch": 7.266666666666667,
      "grad_norm": 3.44692052678397e-10,
      "learning_rate": 4.583333333333333e-06,
      "loss": 0.0026,
      "step": 136250
    },
    {
      "epoch": 7.2672,
      "grad_norm": 0.3081829845905304,
      "learning_rate": 4.58e-06,
      "loss": 0.0033,
      "step": 136260
    },
    {
      "epoch": 7.267733333333333,
      "grad_norm": 0.252151221036911,
      "learning_rate": 4.576666666666666e-06,
      "loss": 0.0028,
      "step": 136270
    },
    {
      "epoch": 7.268266666666666,
      "grad_norm": 0.9733424782752991,
      "learning_rate": 4.573333333333333e-06,
      "loss": 0.0022,
      "step": 136280
    },
    {
      "epoch": 7.2688,
      "grad_norm": 3.853501517880886e-09,
      "learning_rate": 4.57e-06,
      "loss": 0.0027,
      "step": 136290
    },
    {
      "epoch": 7.269333333333333,
      "grad_norm": 0.028016893193125725,
      "learning_rate": 4.566666666666667e-06,
      "loss": 0.0017,
      "step": 136300
    },
    {
      "epoch": 7.269866666666666,
      "grad_norm": 0.11206737905740738,
      "learning_rate": 4.563333333333333e-06,
      "loss": 0.0024,
      "step": 136310
    },
    {
      "epoch": 7.2704,
      "grad_norm": 0.5323051810264587,
      "learning_rate": 4.56e-06,
      "loss": 0.0031,
      "step": 136320
    },
    {
      "epoch": 7.270933333333334,
      "grad_norm": 0.028017135336995125,
      "learning_rate": 4.5566666666666665e-06,
      "loss": 0.0027,
      "step": 136330
    },
    {
      "epoch": 7.271466666666667,
      "grad_norm": 0.3368302285671234,
      "learning_rate": 4.5533333333333335e-06,
      "loss": 0.0025,
      "step": 136340
    },
    {
      "epoch": 7.272,
      "grad_norm": 0.3140345811843872,
      "learning_rate": 4.5500000000000005e-06,
      "loss": 0.0025,
      "step": 136350
    },
    {
      "epoch": 7.2725333333333335,
      "grad_norm": 0.11206743121147156,
      "learning_rate": 4.5466666666666675e-06,
      "loss": 0.0024,
      "step": 136360
    },
    {
      "epoch": 7.273066666666667,
      "grad_norm": 0.2241363227367401,
      "learning_rate": 4.543333333333334e-06,
      "loss": 0.0028,
      "step": 136370
    },
    {
      "epoch": 7.2736,
      "grad_norm": 0.11206823587417603,
      "learning_rate": 4.540000000000001e-06,
      "loss": 0.0034,
      "step": 136380
    },
    {
      "epoch": 7.274133333333333,
      "grad_norm": 0.028017519041895866,
      "learning_rate": 4.536666666666667e-06,
      "loss": 0.0034,
      "step": 136390
    },
    {
      "epoch": 7.274666666666667,
      "grad_norm": 0.36422884464263916,
      "learning_rate": 4.533333333333334e-06,
      "loss": 0.0026,
      "step": 136400
    },
    {
      "epoch": 7.2752,
      "grad_norm": 0.196124866604805,
      "learning_rate": 4.53e-06,
      "loss": 0.0026,
      "step": 136410
    },
    {
      "epoch": 7.275733333333333,
      "grad_norm": 0.16810420155525208,
      "learning_rate": 4.526666666666667e-06,
      "loss": 0.0022,
      "step": 136420
    },
    {
      "epoch": 7.276266666666666,
      "grad_norm": 0.11206888407468796,
      "learning_rate": 4.523333333333334e-06,
      "loss": 0.0019,
      "step": 136430
    },
    {
      "epoch": 7.2768,
      "grad_norm": 0.028017213568091393,
      "learning_rate": 4.52e-06,
      "loss": 0.0024,
      "step": 136440
    },
    {
      "epoch": 7.277333333333333,
      "grad_norm": 0.028017133474349976,
      "learning_rate": 4.516666666666667e-06,
      "loss": 0.0023,
      "step": 136450
    },
    {
      "epoch": 7.277866666666666,
      "grad_norm": 0.028016898781061172,
      "learning_rate": 4.513333333333333e-06,
      "loss": 0.0028,
      "step": 136460
    },
    {
      "epoch": 7.2783999999999995,
      "grad_norm": 1.9105368398442124e-09,
      "learning_rate": 4.51e-06,
      "loss": 0.003,
      "step": 136470
    },
    {
      "epoch": 7.278933333333334,
      "grad_norm": 0.2384330928325653,
      "learning_rate": 4.506666666666667e-06,
      "loss": 0.0024,
      "step": 136480
    },
    {
      "epoch": 7.279466666666667,
      "grad_norm": 0.08404912799596786,
      "learning_rate": 4.503333333333334e-06,
      "loss": 0.0019,
      "step": 136490
    },
    {
      "epoch": 7.28,
      "grad_norm": 0.1681005209684372,
      "learning_rate": 4.5e-06,
      "loss": 0.0028,
      "step": 136500
    },
    {
      "epoch": 7.2805333333333335,
      "grad_norm": 0.2521490454673767,
      "learning_rate": 4.496666666666667e-06,
      "loss": 0.0024,
      "step": 136510
    },
    {
      "epoch": 7.281066666666667,
      "grad_norm": 0.33619484305381775,
      "learning_rate": 4.493333333333333e-06,
      "loss": 0.002,
      "step": 136520
    },
    {
      "epoch": 7.2816,
      "grad_norm": 0.0840509831905365,
      "learning_rate": 4.49e-06,
      "loss": 0.0025,
      "step": 136530
    },
    {
      "epoch": 7.282133333333333,
      "grad_norm": 0.056034062057733536,
      "learning_rate": 4.486666666666667e-06,
      "loss": 0.0024,
      "step": 136540
    },
    {
      "epoch": 7.282666666666667,
      "grad_norm": 0.2270587831735611,
      "learning_rate": 4.483333333333334e-06,
      "loss": 0.002,
      "step": 136550
    },
    {
      "epoch": 7.2832,
      "grad_norm": 0.2241310179233551,
      "learning_rate": 4.48e-06,
      "loss": 0.0037,
      "step": 136560
    },
    {
      "epoch": 7.283733333333333,
      "grad_norm": 0.08405068516731262,
      "learning_rate": 4.476666666666667e-06,
      "loss": 0.0015,
      "step": 136570
    },
    {
      "epoch": 7.2842666666666664,
      "grad_norm": 0.25215423107147217,
      "learning_rate": 4.473333333333333e-06,
      "loss": 0.0021,
      "step": 136580
    },
    {
      "epoch": 7.2848,
      "grad_norm": 1.1675787003895266e-09,
      "learning_rate": 4.4699999999999996e-06,
      "loss": 0.0022,
      "step": 136590
    },
    {
      "epoch": 7.285333333333333,
      "grad_norm": 0.028017913922667503,
      "learning_rate": 4.4666666666666665e-06,
      "loss": 0.0022,
      "step": 136600
    },
    {
      "epoch": 7.285866666666666,
      "grad_norm": 0.30819717049598694,
      "learning_rate": 4.4633333333333335e-06,
      "loss": 0.0021,
      "step": 136610
    },
    {
      "epoch": 7.2864,
      "grad_norm": 0.05603506416082382,
      "learning_rate": 4.4600000000000005e-06,
      "loss": 0.0033,
      "step": 136620
    },
    {
      "epoch": 7.286933333333334,
      "grad_norm": 0.05615273863077164,
      "learning_rate": 4.456666666666667e-06,
      "loss": 0.0017,
      "step": 136630
    },
    {
      "epoch": 7.287466666666667,
      "grad_norm": 0.224136620759964,
      "learning_rate": 4.453333333333334e-06,
      "loss": 0.0023,
      "step": 136640
    },
    {
      "epoch": 7.288,
      "grad_norm": 0.19882243871688843,
      "learning_rate": 4.45e-06,
      "loss": 0.0034,
      "step": 136650
    },
    {
      "epoch": 7.2885333333333335,
      "grad_norm": 0.08404991775751114,
      "learning_rate": 4.446666666666667e-06,
      "loss": 0.0019,
      "step": 136660
    },
    {
      "epoch": 7.289066666666667,
      "grad_norm": 0.25214990973472595,
      "learning_rate": 4.443333333333334e-06,
      "loss": 0.0015,
      "step": 136670
    },
    {
      "epoch": 7.2896,
      "grad_norm": 0.11207008361816406,
      "learning_rate": 4.440000000000001e-06,
      "loss": 0.0031,
      "step": 136680
    },
    {
      "epoch": 7.290133333333333,
      "grad_norm": 1.7445489497447397e-09,
      "learning_rate": 4.436666666666667e-06,
      "loss": 0.0017,
      "step": 136690
    },
    {
      "epoch": 7.290666666666667,
      "grad_norm": 0.1681058406829834,
      "learning_rate": 4.433333333333334e-06,
      "loss": 0.0029,
      "step": 136700
    },
    {
      "epoch": 7.2912,
      "grad_norm": 0.05603459104895592,
      "learning_rate": 4.43e-06,
      "loss": 0.0027,
      "step": 136710
    },
    {
      "epoch": 7.291733333333333,
      "grad_norm": 1.110588732089468e-09,
      "learning_rate": 4.426666666666667e-06,
      "loss": 0.0039,
      "step": 136720
    },
    {
      "epoch": 7.2922666666666665,
      "grad_norm": 8.20470247298033e-10,
      "learning_rate": 4.423333333333334e-06,
      "loss": 0.0026,
      "step": 136730
    },
    {
      "epoch": 7.2928,
      "grad_norm": 0.028016570955514908,
      "learning_rate": 4.420000000000001e-06,
      "loss": 0.0029,
      "step": 136740
    },
    {
      "epoch": 7.293333333333333,
      "grad_norm": 0.02801685407757759,
      "learning_rate": 4.416666666666667e-06,
      "loss": 0.0024,
      "step": 136750
    },
    {
      "epoch": 7.293866666666666,
      "grad_norm": 0.22413678467273712,
      "learning_rate": 4.413333333333333e-06,
      "loss": 0.0033,
      "step": 136760
    },
    {
      "epoch": 7.2943999999999996,
      "grad_norm": 0.028017086908221245,
      "learning_rate": 4.41e-06,
      "loss": 0.0016,
      "step": 136770
    },
    {
      "epoch": 7.294933333333334,
      "grad_norm": 0.11207014322280884,
      "learning_rate": 4.406666666666666e-06,
      "loss": 0.0022,
      "step": 136780
    },
    {
      "epoch": 7.295466666666667,
      "grad_norm": 0.08405280858278275,
      "learning_rate": 4.403333333333333e-06,
      "loss": 0.0026,
      "step": 136790
    },
    {
      "epoch": 7.296,
      "grad_norm": 0.3642353415489197,
      "learning_rate": 4.4e-06,
      "loss": 0.0028,
      "step": 136800
    },
    {
      "epoch": 7.2965333333333335,
      "grad_norm": 0.45339348912239075,
      "learning_rate": 4.396666666666667e-06,
      "loss": 0.0022,
      "step": 136810
    },
    {
      "epoch": 7.297066666666667,
      "grad_norm": 0.14008963108062744,
      "learning_rate": 4.393333333333333e-06,
      "loss": 0.0024,
      "step": 136820
    },
    {
      "epoch": 7.2976,
      "grad_norm": 0.22414202988147736,
      "learning_rate": 4.39e-06,
      "loss": 0.0024,
      "step": 136830
    },
    {
      "epoch": 7.298133333333333,
      "grad_norm": 0.1120699793100357,
      "learning_rate": 4.3866666666666665e-06,
      "loss": 0.0028,
      "step": 136840
    },
    {
      "epoch": 7.298666666666667,
      "grad_norm": 0.11206737160682678,
      "learning_rate": 4.3833333333333334e-06,
      "loss": 0.0031,
      "step": 136850
    },
    {
      "epoch": 7.2992,
      "grad_norm": 0.1400831639766693,
      "learning_rate": 4.38e-06,
      "loss": 0.0016,
      "step": 136860
    },
    {
      "epoch": 7.299733333333333,
      "grad_norm": 0.02801649644970894,
      "learning_rate": 4.376666666666667e-06,
      "loss": 0.0025,
      "step": 136870
    },
    {
      "epoch": 7.3002666666666665,
      "grad_norm": 0.2241312861442566,
      "learning_rate": 4.3733333333333335e-06,
      "loss": 0.0019,
      "step": 136880
    },
    {
      "epoch": 7.3008,
      "grad_norm": 0.11206575483083725,
      "learning_rate": 4.3700000000000005e-06,
      "loss": 0.0023,
      "step": 136890
    },
    {
      "epoch": 7.301333333333333,
      "grad_norm": 0.28016626834869385,
      "learning_rate": 4.366666666666667e-06,
      "loss": 0.0032,
      "step": 136900
    },
    {
      "epoch": 7.301866666666666,
      "grad_norm": 0.30818450450897217,
      "learning_rate": 4.363333333333334e-06,
      "loss": 0.0025,
      "step": 136910
    },
    {
      "epoch": 7.3024000000000004,
      "grad_norm": 0.16810238361358643,
      "learning_rate": 4.360000000000001e-06,
      "loss": 0.0033,
      "step": 136920
    },
    {
      "epoch": 7.302933333333334,
      "grad_norm": 0.02801702357828617,
      "learning_rate": 4.356666666666667e-06,
      "loss": 0.0022,
      "step": 136930
    },
    {
      "epoch": 7.303466666666667,
      "grad_norm": 0.36421823501586914,
      "learning_rate": 4.353333333333334e-06,
      "loss": 0.0029,
      "step": 136940
    },
    {
      "epoch": 7.304,
      "grad_norm": 0.22413654625415802,
      "learning_rate": 4.35e-06,
      "loss": 0.0013,
      "step": 136950
    },
    {
      "epoch": 7.3045333333333335,
      "grad_norm": 0.08404996991157532,
      "learning_rate": 4.346666666666667e-06,
      "loss": 0.0033,
      "step": 136960
    },
    {
      "epoch": 7.305066666666667,
      "grad_norm": 0.2521459758281708,
      "learning_rate": 4.343333333333333e-06,
      "loss": 0.0036,
      "step": 136970
    },
    {
      "epoch": 7.3056,
      "grad_norm": 0.14008329808712006,
      "learning_rate": 4.34e-06,
      "loss": 0.002,
      "step": 136980
    },
    {
      "epoch": 7.306133333333333,
      "grad_norm": 0.1400839388370514,
      "learning_rate": 4.336666666666667e-06,
      "loss": 0.0038,
      "step": 136990
    },
    {
      "epoch": 7.306666666666667,
      "grad_norm": 0.0560324564576149,
      "learning_rate": 4.333333333333334e-06,
      "loss": 0.0035,
      "step": 137000
    },
    {
      "epoch": 7.3072,
      "grad_norm": 0.05603387951850891,
      "learning_rate": 4.33e-06,
      "loss": 0.0024,
      "step": 137010
    },
    {
      "epoch": 7.307733333333333,
      "grad_norm": 0.028017757460474968,
      "learning_rate": 4.326666666666667e-06,
      "loss": 0.0027,
      "step": 137020
    },
    {
      "epoch": 7.3082666666666665,
      "grad_norm": 3.706468243436234e-09,
      "learning_rate": 4.323333333333333e-06,
      "loss": 0.0036,
      "step": 137030
    },
    {
      "epoch": 7.3088,
      "grad_norm": 0.1014370322227478,
      "learning_rate": 4.32e-06,
      "loss": 0.0022,
      "step": 137040
    },
    {
      "epoch": 7.309333333333333,
      "grad_norm": 0.02801666222512722,
      "learning_rate": 4.316666666666667e-06,
      "loss": 0.0024,
      "step": 137050
    },
    {
      "epoch": 7.309866666666666,
      "grad_norm": 0.08404937386512756,
      "learning_rate": 4.313333333333334e-06,
      "loss": 0.0022,
      "step": 137060
    },
    {
      "epoch": 7.3104,
      "grad_norm": 0.1400829553604126,
      "learning_rate": 4.31e-06,
      "loss": 0.0025,
      "step": 137070
    },
    {
      "epoch": 7.310933333333334,
      "grad_norm": 0.3081803619861603,
      "learning_rate": 4.306666666666667e-06,
      "loss": 0.0013,
      "step": 137080
    },
    {
      "epoch": 7.311466666666667,
      "grad_norm": 0.02801695093512535,
      "learning_rate": 4.303333333333333e-06,
      "loss": 0.0022,
      "step": 137090
    },
    {
      "epoch": 7.312,
      "grad_norm": 0.0560334287583828,
      "learning_rate": 4.2999999999999995e-06,
      "loss": 0.0029,
      "step": 137100
    },
    {
      "epoch": 7.3125333333333336,
      "grad_norm": 3.1444169490413287e-09,
      "learning_rate": 4.296666666666667e-06,
      "loss": 0.0019,
      "step": 137110
    },
    {
      "epoch": 7.313066666666667,
      "grad_norm": 0.30819231271743774,
      "learning_rate": 4.2933333333333334e-06,
      "loss": 0.0024,
      "step": 137120
    },
    {
      "epoch": 7.3136,
      "grad_norm": 0.05603444576263428,
      "learning_rate": 4.2900000000000004e-06,
      "loss": 0.0026,
      "step": 137130
    },
    {
      "epoch": 7.314133333333333,
      "grad_norm": 0.11206710338592529,
      "learning_rate": 4.2866666666666666e-06,
      "loss": 0.0031,
      "step": 137140
    },
    {
      "epoch": 7.314666666666667,
      "grad_norm": 0.33619552850723267,
      "learning_rate": 4.2833333333333335e-06,
      "loss": 0.0018,
      "step": 137150
    },
    {
      "epoch": 7.3152,
      "grad_norm": 0.056033436208963394,
      "learning_rate": 4.28e-06,
      "loss": 0.0025,
      "step": 137160
    },
    {
      "epoch": 7.315733333333333,
      "grad_norm": 0.19625964760780334,
      "learning_rate": 4.276666666666667e-06,
      "loss": 0.0018,
      "step": 137170
    },
    {
      "epoch": 7.3162666666666665,
      "grad_norm": 0.11206603795289993,
      "learning_rate": 4.273333333333334e-06,
      "loss": 0.0033,
      "step": 137180
    },
    {
      "epoch": 7.3168,
      "grad_norm": 0.14008371531963348,
      "learning_rate": 4.270000000000001e-06,
      "loss": 0.0033,
      "step": 137190
    },
    {
      "epoch": 7.317333333333333,
      "grad_norm": 0.14008241891860962,
      "learning_rate": 4.266666666666667e-06,
      "loss": 0.0026,
      "step": 137200
    },
    {
      "epoch": 7.317866666666666,
      "grad_norm": 0.14008070528507233,
      "learning_rate": 4.263333333333334e-06,
      "loss": 0.002,
      "step": 137210
    },
    {
      "epoch": 7.3184000000000005,
      "grad_norm": 0.30818209052085876,
      "learning_rate": 4.26e-06,
      "loss": 0.0026,
      "step": 137220
    },
    {
      "epoch": 7.318933333333334,
      "grad_norm": 0.3081888258457184,
      "learning_rate": 4.256666666666667e-06,
      "loss": 0.0022,
      "step": 137230
    },
    {
      "epoch": 7.319466666666667,
      "grad_norm": 0.07306782901287079,
      "learning_rate": 4.253333333333334e-06,
      "loss": 0.0032,
      "step": 137240
    },
    {
      "epoch": 7.32,
      "grad_norm": 0.11207094043493271,
      "learning_rate": 4.250000000000001e-06,
      "loss": 0.0036,
      "step": 137250
    },
    {
      "epoch": 7.320533333333334,
      "grad_norm": 0.47445863485336304,
      "learning_rate": 4.246666666666667e-06,
      "loss": 0.0022,
      "step": 137260
    },
    {
      "epoch": 7.321066666666667,
      "grad_norm": 0.2801821827888489,
      "learning_rate": 4.243333333333334e-06,
      "loss": 0.0024,
      "step": 137270
    },
    {
      "epoch": 7.3216,
      "grad_norm": 0.3922421634197235,
      "learning_rate": 4.24e-06,
      "loss": 0.0022,
      "step": 137280
    },
    {
      "epoch": 7.322133333333333,
      "grad_norm": 0.19611868262290955,
      "learning_rate": 4.236666666666666e-06,
      "loss": 0.0021,
      "step": 137290
    },
    {
      "epoch": 7.322666666666667,
      "grad_norm": 0.16810238361358643,
      "learning_rate": 4.233333333333333e-06,
      "loss": 0.0021,
      "step": 137300
    },
    {
      "epoch": 7.3232,
      "grad_norm": 0.11206762492656708,
      "learning_rate": 4.23e-06,
      "loss": 0.002,
      "step": 137310
    },
    {
      "epoch": 7.323733333333333,
      "grad_norm": 0.05603368952870369,
      "learning_rate": 4.226666666666667e-06,
      "loss": 0.0026,
      "step": 137320
    },
    {
      "epoch": 7.3242666666666665,
      "grad_norm": 0.16810119152069092,
      "learning_rate": 4.223333333333333e-06,
      "loss": 0.0025,
      "step": 137330
    },
    {
      "epoch": 7.3248,
      "grad_norm": 0.056033287197351456,
      "learning_rate": 4.22e-06,
      "loss": 0.003,
      "step": 137340
    },
    {
      "epoch": 7.325333333333333,
      "grad_norm": 0.11206866800785065,
      "learning_rate": 4.216666666666666e-06,
      "loss": 0.0016,
      "step": 137350
    },
    {
      "epoch": 7.325866666666666,
      "grad_norm": 0.08405151218175888,
      "learning_rate": 4.213333333333333e-06,
      "loss": 0.0036,
      "step": 137360
    },
    {
      "epoch": 7.3264,
      "grad_norm": 0.08405325561761856,
      "learning_rate": 4.21e-06,
      "loss": 0.003,
      "step": 137370
    },
    {
      "epoch": 7.326933333333334,
      "grad_norm": 0.3642379343509674,
      "learning_rate": 4.206666666666667e-06,
      "loss": 0.0024,
      "step": 137380
    },
    {
      "epoch": 7.327466666666667,
      "grad_norm": 0.22414715588092804,
      "learning_rate": 4.2033333333333335e-06,
      "loss": 0.0015,
      "step": 137390
    },
    {
      "epoch": 7.328,
      "grad_norm": 0.16810709238052368,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 0.0013,
      "step": 137400
    },
    {
      "epoch": 7.328533333333334,
      "grad_norm": 0.11206898838281631,
      "learning_rate": 4.196666666666667e-06,
      "loss": 0.0021,
      "step": 137410
    },
    {
      "epoch": 7.329066666666667,
      "grad_norm": 0.11206666380167007,
      "learning_rate": 4.1933333333333336e-06,
      "loss": 0.0029,
      "step": 137420
    },
    {
      "epoch": 7.3296,
      "grad_norm": 2.779987129386541e-09,
      "learning_rate": 4.1900000000000005e-06,
      "loss": 0.0028,
      "step": 137430
    },
    {
      "epoch": 7.330133333333333,
      "grad_norm": 0.3361952602863312,
      "learning_rate": 4.1866666666666675e-06,
      "loss": 0.0017,
      "step": 137440
    },
    {
      "epoch": 7.330666666666667,
      "grad_norm": 0.056034088134765625,
      "learning_rate": 4.183333333333334e-06,
      "loss": 0.0018,
      "step": 137450
    },
    {
      "epoch": 7.3312,
      "grad_norm": 2.415086575169312e-09,
      "learning_rate": 4.18e-06,
      "loss": 0.0025,
      "step": 137460
    },
    {
      "epoch": 7.331733333333333,
      "grad_norm": 0.08405008167028427,
      "learning_rate": 4.176666666666667e-06,
      "loss": 0.0024,
      "step": 137470
    },
    {
      "epoch": 7.3322666666666665,
      "grad_norm": 0.28016427159309387,
      "learning_rate": 4.173333333333333e-06,
      "loss": 0.0028,
      "step": 137480
    },
    {
      "epoch": 7.3328,
      "grad_norm": 0.11206653714179993,
      "learning_rate": 4.17e-06,
      "loss": 0.0024,
      "step": 137490
    },
    {
      "epoch": 7.333333333333333,
      "grad_norm": 0.08405003696680069,
      "learning_rate": 4.166666666666667e-06,
      "loss": 0.002,
      "step": 137500
    },
    {
      "epoch": 7.333866666666666,
      "grad_norm": 0.16810214519500732,
      "learning_rate": 4.163333333333334e-06,
      "loss": 0.0028,
      "step": 137510
    },
    {
      "epoch": 7.3344,
      "grad_norm": 0.05603513494133949,
      "learning_rate": 4.16e-06,
      "loss": 0.0023,
      "step": 137520
    },
    {
      "epoch": 7.334933333333334,
      "grad_norm": 0.028017617762088776,
      "learning_rate": 4.156666666666667e-06,
      "loss": 0.0021,
      "step": 137530
    },
    {
      "epoch": 7.335466666666667,
      "grad_norm": 0.19612157344818115,
      "learning_rate": 4.153333333333333e-06,
      "loss": 0.0023,
      "step": 137540
    },
    {
      "epoch": 7.336,
      "grad_norm": 0.11206783354282379,
      "learning_rate": 4.15e-06,
      "loss": 0.0031,
      "step": 137550
    },
    {
      "epoch": 7.336533333333334,
      "grad_norm": 0.028016500174999237,
      "learning_rate": 4.146666666666667e-06,
      "loss": 0.0019,
      "step": 137560
    },
    {
      "epoch": 7.337066666666667,
      "grad_norm": 0.16809934377670288,
      "learning_rate": 4.143333333333334e-06,
      "loss": 0.0023,
      "step": 137570
    },
    {
      "epoch": 7.3376,
      "grad_norm": 0.16810287535190582,
      "learning_rate": 4.14e-06,
      "loss": 0.0036,
      "step": 137580
    },
    {
      "epoch": 7.338133333333333,
      "grad_norm": 0.028016608208417892,
      "learning_rate": 4.136666666666667e-06,
      "loss": 0.0026,
      "step": 137590
    },
    {
      "epoch": 7.338666666666667,
      "grad_norm": 0.19611236453056335,
      "learning_rate": 4.133333333333333e-06,
      "loss": 0.0017,
      "step": 137600
    },
    {
      "epoch": 7.3392,
      "grad_norm": 0.08404987305402756,
      "learning_rate": 4.13e-06,
      "loss": 0.0041,
      "step": 137610
    },
    {
      "epoch": 7.339733333333333,
      "grad_norm": 0.196114644408226,
      "learning_rate": 4.126666666666667e-06,
      "loss": 0.0025,
      "step": 137620
    },
    {
      "epoch": 7.3402666666666665,
      "grad_norm": 0.05603361874818802,
      "learning_rate": 4.123333333333333e-06,
      "loss": 0.0019,
      "step": 137630
    },
    {
      "epoch": 7.3408,
      "grad_norm": 0.25215527415275574,
      "learning_rate": 4.12e-06,
      "loss": 0.0027,
      "step": 137640
    },
    {
      "epoch": 7.341333333333333,
      "grad_norm": 0.11206797510385513,
      "learning_rate": 4.1166666666666665e-06,
      "loss": 0.002,
      "step": 137650
    },
    {
      "epoch": 7.341866666666666,
      "grad_norm": 0.3081815540790558,
      "learning_rate": 4.1133333333333335e-06,
      "loss": 0.0017,
      "step": 137660
    },
    {
      "epoch": 7.3424,
      "grad_norm": 0.2801682949066162,
      "learning_rate": 4.11e-06,
      "loss": 0.0029,
      "step": 137670
    },
    {
      "epoch": 7.342933333333333,
      "grad_norm": 0.16809822618961334,
      "learning_rate": 4.106666666666667e-06,
      "loss": 0.0034,
      "step": 137680
    },
    {
      "epoch": 7.343466666666667,
      "grad_norm": 0.2521458566188812,
      "learning_rate": 4.1033333333333336e-06,
      "loss": 0.0031,
      "step": 137690
    },
    {
      "epoch": 7.344,
      "grad_norm": 0.02801639772951603,
      "learning_rate": 4.1000000000000006e-06,
      "loss": 0.0027,
      "step": 137700
    },
    {
      "epoch": 7.344533333333334,
      "grad_norm": 0.30861905217170715,
      "learning_rate": 4.096666666666667e-06,
      "loss": 0.0026,
      "step": 137710
    },
    {
      "epoch": 7.345066666666667,
      "grad_norm": 0.16809745132923126,
      "learning_rate": 4.093333333333334e-06,
      "loss": 0.0018,
      "step": 137720
    },
    {
      "epoch": 7.3456,
      "grad_norm": 0.08405053615570068,
      "learning_rate": 4.09e-06,
      "loss": 0.0025,
      "step": 137730
    },
    {
      "epoch": 7.346133333333333,
      "grad_norm": 0.0983307883143425,
      "learning_rate": 4.086666666666667e-06,
      "loss": 0.0034,
      "step": 137740
    },
    {
      "epoch": 7.346666666666667,
      "grad_norm": 0.056033551692962646,
      "learning_rate": 4.083333333333334e-06,
      "loss": 0.0018,
      "step": 137750
    },
    {
      "epoch": 7.3472,
      "grad_norm": 0.05603316053748131,
      "learning_rate": 4.080000000000001e-06,
      "loss": 0.0036,
      "step": 137760
    },
    {
      "epoch": 7.347733333333333,
      "grad_norm": 0.05603262409567833,
      "learning_rate": 4.076666666666667e-06,
      "loss": 0.0024,
      "step": 137770
    },
    {
      "epoch": 7.3482666666666665,
      "grad_norm": 0.1961163729429245,
      "learning_rate": 4.073333333333334e-06,
      "loss": 0.0023,
      "step": 137780
    },
    {
      "epoch": 7.3488,
      "grad_norm": 0.056033905595541,
      "learning_rate": 4.07e-06,
      "loss": 0.0041,
      "step": 137790
    },
    {
      "epoch": 7.349333333333333,
      "grad_norm": 0.05603371560573578,
      "learning_rate": 4.066666666666666e-06,
      "loss": 0.0029,
      "step": 137800
    },
    {
      "epoch": 7.349866666666666,
      "grad_norm": 0.16809946298599243,
      "learning_rate": 4.063333333333334e-06,
      "loss": 0.002,
      "step": 137810
    },
    {
      "epoch": 7.3504,
      "grad_norm": 0.028016457334160805,
      "learning_rate": 4.06e-06,
      "loss": 0.0027,
      "step": 137820
    },
    {
      "epoch": 7.350933333333334,
      "grad_norm": 0.05603297799825668,
      "learning_rate": 4.056666666666667e-06,
      "loss": 0.0031,
      "step": 137830
    },
    {
      "epoch": 7.351466666666667,
      "grad_norm": 0.08405028283596039,
      "learning_rate": 4.053333333333333e-06,
      "loss": 0.0021,
      "step": 137840
    },
    {
      "epoch": 7.352,
      "grad_norm": 0.1681007444858551,
      "learning_rate": 4.05e-06,
      "loss": 0.0025,
      "step": 137850
    },
    {
      "epoch": 7.352533333333334,
      "grad_norm": 3.1734843641828547e-09,
      "learning_rate": 4.046666666666666e-06,
      "loss": 0.0024,
      "step": 137860
    },
    {
      "epoch": 7.353066666666667,
      "grad_norm": 0.11206945776939392,
      "learning_rate": 4.043333333333333e-06,
      "loss": 0.0024,
      "step": 137870
    },
    {
      "epoch": 7.3536,
      "grad_norm": 0.44827479124069214,
      "learning_rate": 4.04e-06,
      "loss": 0.0028,
      "step": 137880
    },
    {
      "epoch": 7.354133333333333,
      "grad_norm": 0.05603352189064026,
      "learning_rate": 4.036666666666667e-06,
      "loss": 0.003,
      "step": 137890
    },
    {
      "epoch": 7.354666666666667,
      "grad_norm": 0.2801668345928192,
      "learning_rate": 4.033333333333333e-06,
      "loss": 0.0024,
      "step": 137900
    },
    {
      "epoch": 7.3552,
      "grad_norm": 0.20251774787902832,
      "learning_rate": 4.03e-06,
      "loss": 0.0027,
      "step": 137910
    },
    {
      "epoch": 7.355733333333333,
      "grad_norm": 0.001719833118841052,
      "learning_rate": 4.0266666666666665e-06,
      "loss": 0.0033,
      "step": 137920
    },
    {
      "epoch": 7.3562666666666665,
      "grad_norm": 0.08434274792671204,
      "learning_rate": 4.0233333333333335e-06,
      "loss": 0.002,
      "step": 137930
    },
    {
      "epoch": 7.3568,
      "grad_norm": 0.25236615538597107,
      "learning_rate": 4.0200000000000005e-06,
      "loss": 0.0028,
      "step": 137940
    },
    {
      "epoch": 7.357333333333333,
      "grad_norm": 0.3398575186729431,
      "learning_rate": 4.0166666666666675e-06,
      "loss": 0.0034,
      "step": 137950
    },
    {
      "epoch": 7.357866666666666,
      "grad_norm": 0.11499772220849991,
      "learning_rate": 4.013333333333334e-06,
      "loss": 0.0025,
      "step": 137960
    },
    {
      "epoch": 7.3584,
      "grad_norm": 0.08405142277479172,
      "learning_rate": 4.01e-06,
      "loss": 0.0026,
      "step": 137970
    },
    {
      "epoch": 7.358933333333333,
      "grad_norm": 0.22413663566112518,
      "learning_rate": 4.006666666666667e-06,
      "loss": 0.0024,
      "step": 137980
    },
    {
      "epoch": 7.359466666666667,
      "grad_norm": 0.11206700652837753,
      "learning_rate": 4.003333333333333e-06,
      "loss": 0.0022,
      "step": 137990
    },
    {
      "epoch": 7.36,
      "grad_norm": 0.2801648676395416,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.0038,
      "step": 138000
    },
    {
      "epoch": 7.360533333333334,
      "grad_norm": 0.25214719772338867,
      "learning_rate": 3.996666666666667e-06,
      "loss": 0.0029,
      "step": 138010
    },
    {
      "epoch": 7.361066666666667,
      "grad_norm": 0.3922335207462311,
      "learning_rate": 3.993333333333334e-06,
      "loss": 0.0029,
      "step": 138020
    },
    {
      "epoch": 7.3616,
      "grad_norm": 0.2778013348579407,
      "learning_rate": 3.99e-06,
      "loss": 0.0025,
      "step": 138030
    },
    {
      "epoch": 7.362133333333333,
      "grad_norm": 0.22412827610969543,
      "learning_rate": 3.986666666666667e-06,
      "loss": 0.0027,
      "step": 138040
    },
    {
      "epoch": 7.362666666666667,
      "grad_norm": 0.11206468194723129,
      "learning_rate": 3.983333333333333e-06,
      "loss": 0.002,
      "step": 138050
    },
    {
      "epoch": 7.3632,
      "grad_norm": 0.11206650733947754,
      "learning_rate": 3.98e-06,
      "loss": 0.0039,
      "step": 138060
    },
    {
      "epoch": 7.363733333333333,
      "grad_norm": 0.08404901623725891,
      "learning_rate": 3.976666666666667e-06,
      "loss": 0.0017,
      "step": 138070
    },
    {
      "epoch": 7.3642666666666665,
      "grad_norm": 0.14008121192455292,
      "learning_rate": 3.973333333333334e-06,
      "loss": 0.004,
      "step": 138080
    },
    {
      "epoch": 7.3648,
      "grad_norm": 0.11206625401973724,
      "learning_rate": 3.97e-06,
      "loss": 0.0018,
      "step": 138090
    },
    {
      "epoch": 7.365333333333333,
      "grad_norm": 0.028016794472932816,
      "learning_rate": 3.966666666666667e-06,
      "loss": 0.0033,
      "step": 138100
    },
    {
      "epoch": 7.365866666666666,
      "grad_norm": 0.05603313073515892,
      "learning_rate": 3.963333333333333e-06,
      "loss": 0.0029,
      "step": 138110
    },
    {
      "epoch": 7.3664,
      "grad_norm": 0.028016600757837296,
      "learning_rate": 3.96e-06,
      "loss": 0.0023,
      "step": 138120
    },
    {
      "epoch": 7.366933333333334,
      "grad_norm": 0.08405059576034546,
      "learning_rate": 3.956666666666667e-06,
      "loss": 0.0024,
      "step": 138130
    },
    {
      "epoch": 7.367466666666667,
      "grad_norm": 0.056352268904447556,
      "learning_rate": 3.953333333333333e-06,
      "loss": 0.0018,
      "step": 138140
    },
    {
      "epoch": 7.368,
      "grad_norm": 0.30818840861320496,
      "learning_rate": 3.95e-06,
      "loss": 0.0019,
      "step": 138150
    },
    {
      "epoch": 7.368533333333334,
      "grad_norm": 0.21334055066108704,
      "learning_rate": 3.9466666666666664e-06,
      "loss": 0.0022,
      "step": 138160
    },
    {
      "epoch": 7.369066666666667,
      "grad_norm": 0.30818378925323486,
      "learning_rate": 3.943333333333333e-06,
      "loss": 0.0029,
      "step": 138170
    },
    {
      "epoch": 7.3696,
      "grad_norm": 0.1400805562734604,
      "learning_rate": 3.9399999999999995e-06,
      "loss": 0.0023,
      "step": 138180
    },
    {
      "epoch": 7.370133333333333,
      "grad_norm": 0.028016220778226852,
      "learning_rate": 3.936666666666667e-06,
      "loss": 0.0016,
      "step": 138190
    },
    {
      "epoch": 7.370666666666667,
      "grad_norm": 0.16809877753257751,
      "learning_rate": 3.9333333333333335e-06,
      "loss": 0.0032,
      "step": 138200
    },
    {
      "epoch": 7.3712,
      "grad_norm": 0.11206655949354172,
      "learning_rate": 3.9300000000000005e-06,
      "loss": 0.0028,
      "step": 138210
    },
    {
      "epoch": 7.371733333333333,
      "grad_norm": 0.4626780152320862,
      "learning_rate": 3.926666666666667e-06,
      "loss": 0.0028,
      "step": 138220
    },
    {
      "epoch": 7.3722666666666665,
      "grad_norm": 0.1400807648897171,
      "learning_rate": 3.923333333333334e-06,
      "loss": 0.0032,
      "step": 138230
    },
    {
      "epoch": 7.3728,
      "grad_norm": 0.13070926070213318,
      "learning_rate": 3.92e-06,
      "loss": 0.0027,
      "step": 138240
    },
    {
      "epoch": 7.373333333333333,
      "grad_norm": 1.923184944629952e-09,
      "learning_rate": 3.916666666666667e-06,
      "loss": 0.0031,
      "step": 138250
    },
    {
      "epoch": 7.373866666666666,
      "grad_norm": 0.1400831639766693,
      "learning_rate": 3.913333333333334e-06,
      "loss": 0.0021,
      "step": 138260
    },
    {
      "epoch": 7.3744,
      "grad_norm": 0.10422751307487488,
      "learning_rate": 3.910000000000001e-06,
      "loss": 0.0027,
      "step": 138270
    },
    {
      "epoch": 7.374933333333333,
      "grad_norm": 0.0840592235326767,
      "learning_rate": 3.906666666666667e-06,
      "loss": 0.0035,
      "step": 138280
    },
    {
      "epoch": 7.375466666666667,
      "grad_norm": 0.1889825016260147,
      "learning_rate": 3.903333333333334e-06,
      "loss": 0.0026,
      "step": 138290
    },
    {
      "epoch": 7.376,
      "grad_norm": 0.11206614226102829,
      "learning_rate": 3.9e-06,
      "loss": 0.0037,
      "step": 138300
    },
    {
      "epoch": 7.376533333333334,
      "grad_norm": 0.11206501722335815,
      "learning_rate": 3.896666666666667e-06,
      "loss": 0.0024,
      "step": 138310
    },
    {
      "epoch": 7.377066666666667,
      "grad_norm": 0.3082175850868225,
      "learning_rate": 3.893333333333334e-06,
      "loss": 0.0022,
      "step": 138320
    },
    {
      "epoch": 7.3776,
      "grad_norm": 0.5042840242385864,
      "learning_rate": 3.89e-06,
      "loss": 0.0031,
      "step": 138330
    },
    {
      "epoch": 7.378133333333333,
      "grad_norm": 0.3081776201725006,
      "learning_rate": 3.886666666666667e-06,
      "loss": 0.0023,
      "step": 138340
    },
    {
      "epoch": 7.378666666666667,
      "grad_norm": 0.0280165933072567,
      "learning_rate": 3.883333333333333e-06,
      "loss": 0.0037,
      "step": 138350
    },
    {
      "epoch": 7.3792,
      "grad_norm": 0.16812659800052643,
      "learning_rate": 3.88e-06,
      "loss": 0.0025,
      "step": 138360
    },
    {
      "epoch": 7.379733333333333,
      "grad_norm": 0.028016474097967148,
      "learning_rate": 3.876666666666666e-06,
      "loss": 0.0022,
      "step": 138370
    },
    {
      "epoch": 7.3802666666666665,
      "grad_norm": 0.02803850546479225,
      "learning_rate": 3.873333333333334e-06,
      "loss": 0.0021,
      "step": 138380
    },
    {
      "epoch": 7.3808,
      "grad_norm": 1.8019038438796997,
      "learning_rate": 3.87e-06,
      "loss": 0.0026,
      "step": 138390
    },
    {
      "epoch": 7.381333333333333,
      "grad_norm": 0.02801661565899849,
      "learning_rate": 3.866666666666667e-06,
      "loss": 0.0029,
      "step": 138400
    },
    {
      "epoch": 7.381866666666666,
      "grad_norm": 0.020054904744029045,
      "learning_rate": 3.863333333333333e-06,
      "loss": 0.0035,
      "step": 138410
    },
    {
      "epoch": 7.3824,
      "grad_norm": 0.14015166461467743,
      "learning_rate": 3.86e-06,
      "loss": 0.002,
      "step": 138420
    },
    {
      "epoch": 7.382933333333334,
      "grad_norm": 0.028016122058033943,
      "learning_rate": 3.8566666666666664e-06,
      "loss": 0.0022,
      "step": 138430
    },
    {
      "epoch": 7.383466666666667,
      "grad_norm": 0.3642120659351349,
      "learning_rate": 3.8533333333333334e-06,
      "loss": 0.0026,
      "step": 138440
    },
    {
      "epoch": 7.384,
      "grad_norm": 0.05603300407528877,
      "learning_rate": 3.85e-06,
      "loss": 0.0032,
      "step": 138450
    },
    {
      "epoch": 7.384533333333334,
      "grad_norm": 0.1120661050081253,
      "learning_rate": 3.846666666666667e-06,
      "loss": 0.0028,
      "step": 138460
    },
    {
      "epoch": 7.385066666666667,
      "grad_norm": 0.08404870331287384,
      "learning_rate": 3.8433333333333335e-06,
      "loss": 0.0019,
      "step": 138470
    },
    {
      "epoch": 7.3856,
      "grad_norm": 3.4880929256075888e-09,
      "learning_rate": 3.84e-06,
      "loss": 0.0021,
      "step": 138480
    },
    {
      "epoch": 7.386133333333333,
      "grad_norm": 0.22413131594657898,
      "learning_rate": 3.836666666666667e-06,
      "loss": 0.0031,
      "step": 138490
    },
    {
      "epoch": 7.386666666666667,
      "grad_norm": 0.028016336262226105,
      "learning_rate": 3.833333333333334e-06,
      "loss": 0.0025,
      "step": 138500
    },
    {
      "epoch": 7.3872,
      "grad_norm": 0.14008180797100067,
      "learning_rate": 3.830000000000001e-06,
      "loss": 0.0027,
      "step": 138510
    },
    {
      "epoch": 7.387733333333333,
      "grad_norm": 0.02801683358848095,
      "learning_rate": 3.826666666666667e-06,
      "loss": 0.0033,
      "step": 138520
    },
    {
      "epoch": 7.3882666666666665,
      "grad_norm": 0.028016826137900352,
      "learning_rate": 3.823333333333334e-06,
      "loss": 0.0023,
      "step": 138530
    },
    {
      "epoch": 7.3888,
      "grad_norm": 0.42025548219680786,
      "learning_rate": 3.82e-06,
      "loss": 0.003,
      "step": 138540
    },
    {
      "epoch": 7.389333333333333,
      "grad_norm": 0.11206890642642975,
      "learning_rate": 3.816666666666667e-06,
      "loss": 0.0025,
      "step": 138550
    },
    {
      "epoch": 7.389866666666666,
      "grad_norm": 0.3453613221645355,
      "learning_rate": 3.8133333333333334e-06,
      "loss": 0.0025,
      "step": 138560
    },
    {
      "epoch": 7.3904,
      "grad_norm": 0.14008423686027527,
      "learning_rate": 3.8100000000000004e-06,
      "loss": 0.0025,
      "step": 138570
    },
    {
      "epoch": 7.390933333333333,
      "grad_norm": 0.05603210628032684,
      "learning_rate": 3.806666666666667e-06,
      "loss": 0.002,
      "step": 138580
    },
    {
      "epoch": 7.391466666666667,
      "grad_norm": 0.22412681579589844,
      "learning_rate": 3.803333333333334e-06,
      "loss": 0.0028,
      "step": 138590
    },
    {
      "epoch": 7.392,
      "grad_norm": 0.47139739990234375,
      "learning_rate": 3.8e-06,
      "loss": 0.0026,
      "step": 138600
    },
    {
      "epoch": 7.392533333333334,
      "grad_norm": 0.22413378953933716,
      "learning_rate": 3.796666666666667e-06,
      "loss": 0.002,
      "step": 138610
    },
    {
      "epoch": 7.393066666666667,
      "grad_norm": 0.02801714837551117,
      "learning_rate": 3.7933333333333336e-06,
      "loss": 0.0031,
      "step": 138620
    },
    {
      "epoch": 7.3936,
      "grad_norm": 0.08405196666717529,
      "learning_rate": 3.7900000000000006e-06,
      "loss": 0.0021,
      "step": 138630
    },
    {
      "epoch": 7.3941333333333334,
      "grad_norm": 0.30818676948547363,
      "learning_rate": 3.7866666666666667e-06,
      "loss": 0.0023,
      "step": 138640
    },
    {
      "epoch": 7.394666666666667,
      "grad_norm": 0.05603343993425369,
      "learning_rate": 3.7833333333333333e-06,
      "loss": 0.0027,
      "step": 138650
    },
    {
      "epoch": 7.3952,
      "grad_norm": 0.22413088381290436,
      "learning_rate": 3.7800000000000002e-06,
      "loss": 0.0032,
      "step": 138660
    },
    {
      "epoch": 7.395733333333333,
      "grad_norm": 0.0560317263007164,
      "learning_rate": 3.7766666666666664e-06,
      "loss": 0.0022,
      "step": 138670
    },
    {
      "epoch": 7.3962666666666665,
      "grad_norm": 0.056032177060842514,
      "learning_rate": 3.7733333333333338e-06,
      "loss": 0.0036,
      "step": 138680
    },
    {
      "epoch": 7.3968,
      "grad_norm": 0.2521459758281708,
      "learning_rate": 3.77e-06,
      "loss": 0.0037,
      "step": 138690
    },
    {
      "epoch": 7.397333333333333,
      "grad_norm": 0.19611473381519318,
      "learning_rate": 3.766666666666667e-06,
      "loss": 0.0021,
      "step": 138700
    },
    {
      "epoch": 7.397866666666666,
      "grad_norm": 0.2801685035228729,
      "learning_rate": 3.7633333333333334e-06,
      "loss": 0.0019,
      "step": 138710
    },
    {
      "epoch": 7.3984,
      "grad_norm": 0.1681009978055954,
      "learning_rate": 3.7600000000000004e-06,
      "loss": 0.0019,
      "step": 138720
    },
    {
      "epoch": 7.398933333333333,
      "grad_norm": 0.14008325338363647,
      "learning_rate": 3.7566666666666666e-06,
      "loss": 0.0035,
      "step": 138730
    },
    {
      "epoch": 7.399466666666667,
      "grad_norm": 0.1400814652442932,
      "learning_rate": 3.7533333333333335e-06,
      "loss": 0.003,
      "step": 138740
    },
    {
      "epoch": 7.4,
      "grad_norm": 0.16809770464897156,
      "learning_rate": 3.75e-06,
      "loss": 0.0024,
      "step": 138750
    },
    {
      "epoch": 7.400533333333334,
      "grad_norm": 0.16809508204460144,
      "learning_rate": 3.746666666666667e-06,
      "loss": 0.0034,
      "step": 138760
    },
    {
      "epoch": 7.401066666666667,
      "grad_norm": 0.056032415479421616,
      "learning_rate": 3.743333333333333e-06,
      "loss": 0.003,
      "step": 138770
    },
    {
      "epoch": 7.4016,
      "grad_norm": 0.2801665961742401,
      "learning_rate": 3.7400000000000006e-06,
      "loss": 0.0021,
      "step": 138780
    },
    {
      "epoch": 7.4021333333333335,
      "grad_norm": 0.028016742318868637,
      "learning_rate": 3.7366666666666667e-06,
      "loss": 0.003,
      "step": 138790
    },
    {
      "epoch": 7.402666666666667,
      "grad_norm": 0.2241337150335312,
      "learning_rate": 3.7333333333333337e-06,
      "loss": 0.002,
      "step": 138800
    },
    {
      "epoch": 7.4032,
      "grad_norm": 0.1961144506931305,
      "learning_rate": 3.7300000000000003e-06,
      "loss": 0.0032,
      "step": 138810
    },
    {
      "epoch": 7.403733333333333,
      "grad_norm": 0.16809576749801636,
      "learning_rate": 3.7266666666666664e-06,
      "loss": 0.0025,
      "step": 138820
    },
    {
      "epoch": 7.4042666666666666,
      "grad_norm": 0.14007996022701263,
      "learning_rate": 3.7233333333333334e-06,
      "loss": 0.003,
      "step": 138830
    },
    {
      "epoch": 7.4048,
      "grad_norm": 0.028016170486807823,
      "learning_rate": 3.72e-06,
      "loss": 0.0023,
      "step": 138840
    },
    {
      "epoch": 7.405333333333333,
      "grad_norm": 0.056033238768577576,
      "learning_rate": 3.716666666666667e-06,
      "loss": 0.0024,
      "step": 138850
    },
    {
      "epoch": 7.405866666666666,
      "grad_norm": 0.280166894197464,
      "learning_rate": 3.713333333333333e-06,
      "loss": 0.0032,
      "step": 138860
    },
    {
      "epoch": 7.4064,
      "grad_norm": 0.3081818222999573,
      "learning_rate": 3.7100000000000005e-06,
      "loss": 0.0027,
      "step": 138870
    },
    {
      "epoch": 7.406933333333333,
      "grad_norm": 0.19611622393131256,
      "learning_rate": 3.7066666666666666e-06,
      "loss": 0.0023,
      "step": 138880
    },
    {
      "epoch": 7.407466666666666,
      "grad_norm": 0.11206664890050888,
      "learning_rate": 3.7033333333333336e-06,
      "loss": 0.0033,
      "step": 138890
    },
    {
      "epoch": 7.408,
      "grad_norm": 0.2521499693393707,
      "learning_rate": 3.7e-06,
      "loss": 0.0032,
      "step": 138900
    },
    {
      "epoch": 7.408533333333334,
      "grad_norm": 0.16809925436973572,
      "learning_rate": 3.696666666666667e-06,
      "loss": 0.0021,
      "step": 138910
    },
    {
      "epoch": 7.409066666666667,
      "grad_norm": 0.028015896677970886,
      "learning_rate": 3.6933333333333333e-06,
      "loss": 0.0028,
      "step": 138920
    },
    {
      "epoch": 7.4096,
      "grad_norm": 0.3922217786312103,
      "learning_rate": 3.6900000000000002e-06,
      "loss": 0.0026,
      "step": 138930
    },
    {
      "epoch": 7.4101333333333335,
      "grad_norm": 0.4367034137248993,
      "learning_rate": 3.686666666666667e-06,
      "loss": 0.0027,
      "step": 138940
    },
    {
      "epoch": 7.410666666666667,
      "grad_norm": 0.05603324621915817,
      "learning_rate": 3.6833333333333338e-06,
      "loss": 0.0034,
      "step": 138950
    },
    {
      "epoch": 7.4112,
      "grad_norm": 0.1400810331106186,
      "learning_rate": 3.68e-06,
      "loss": 0.0031,
      "step": 138960
    },
    {
      "epoch": 7.411733333333333,
      "grad_norm": 0.19611412286758423,
      "learning_rate": 3.6766666666666673e-06,
      "loss": 0.0037,
      "step": 138970
    },
    {
      "epoch": 7.412266666666667,
      "grad_norm": 0.08404773473739624,
      "learning_rate": 3.6733333333333335e-06,
      "loss": 0.0034,
      "step": 138980
    },
    {
      "epoch": 7.4128,
      "grad_norm": 4.395111830035603e-09,
      "learning_rate": 3.6700000000000004e-06,
      "loss": 0.0036,
      "step": 138990
    },
    {
      "epoch": 7.413333333333333,
      "grad_norm": 0.14007988572120667,
      "learning_rate": 3.666666666666667e-06,
      "loss": 0.0021,
      "step": 139000
    },
    {
      "epoch": 7.413866666666666,
      "grad_norm": 0.028016313910484314,
      "learning_rate": 3.663333333333333e-06,
      "loss": 0.0028,
      "step": 139010
    },
    {
      "epoch": 7.4144,
      "grad_norm": 0.11206652969121933,
      "learning_rate": 3.66e-06,
      "loss": 0.0044,
      "step": 139020
    },
    {
      "epoch": 7.414933333333333,
      "grad_norm": 0.05603363737463951,
      "learning_rate": 3.6566666666666667e-06,
      "loss": 0.0021,
      "step": 139030
    },
    {
      "epoch": 7.415466666666667,
      "grad_norm": 0.08405070006847382,
      "learning_rate": 3.6533333333333336e-06,
      "loss": 0.0018,
      "step": 139040
    },
    {
      "epoch": 7.416,
      "grad_norm": 0.1400844007730484,
      "learning_rate": 3.6499999999999998e-06,
      "loss": 0.002,
      "step": 139050
    },
    {
      "epoch": 7.416533333333334,
      "grad_norm": 0.14008253812789917,
      "learning_rate": 3.646666666666667e-06,
      "loss": 0.0014,
      "step": 139060
    },
    {
      "epoch": 7.417066666666667,
      "grad_norm": 0.05603213608264923,
      "learning_rate": 3.6433333333333333e-06,
      "loss": 0.0027,
      "step": 139070
    },
    {
      "epoch": 7.4176,
      "grad_norm": 0.336189329624176,
      "learning_rate": 3.6400000000000003e-06,
      "loss": 0.0043,
      "step": 139080
    },
    {
      "epoch": 7.4181333333333335,
      "grad_norm": 1.177369475364685,
      "learning_rate": 3.636666666666667e-06,
      "loss": 0.0038,
      "step": 139090
    },
    {
      "epoch": 7.418666666666667,
      "grad_norm": 0.19611486792564392,
      "learning_rate": 3.633333333333334e-06,
      "loss": 0.0021,
      "step": 139100
    },
    {
      "epoch": 7.4192,
      "grad_norm": 0.14008179306983948,
      "learning_rate": 3.63e-06,
      "loss": 0.0032,
      "step": 139110
    },
    {
      "epoch": 7.419733333333333,
      "grad_norm": 0.02801607735455036,
      "learning_rate": 3.626666666666667e-06,
      "loss": 0.0028,
      "step": 139120
    },
    {
      "epoch": 7.420266666666667,
      "grad_norm": 0.19610974192619324,
      "learning_rate": 3.6233333333333335e-06,
      "loss": 0.0026,
      "step": 139130
    },
    {
      "epoch": 7.4208,
      "grad_norm": 0.1961113065481186,
      "learning_rate": 3.6200000000000005e-06,
      "loss": 0.0025,
      "step": 139140
    },
    {
      "epoch": 7.421333333333333,
      "grad_norm": 0.02801584079861641,
      "learning_rate": 3.6166666666666666e-06,
      "loss": 0.002,
      "step": 139150
    },
    {
      "epoch": 7.421866666666666,
      "grad_norm": 0.14940086007118225,
      "learning_rate": 3.613333333333334e-06,
      "loss": 0.0023,
      "step": 139160
    },
    {
      "epoch": 7.4224,
      "grad_norm": 0.02801555022597313,
      "learning_rate": 3.61e-06,
      "loss": 0.0032,
      "step": 139170
    },
    {
      "epoch": 7.422933333333333,
      "grad_norm": 0.2801576852798462,
      "learning_rate": 3.6066666666666667e-06,
      "loss": 0.0024,
      "step": 139180
    },
    {
      "epoch": 7.423466666666666,
      "grad_norm": 0.11206448078155518,
      "learning_rate": 3.6033333333333337e-06,
      "loss": 0.0018,
      "step": 139190
    },
    {
      "epoch": 7.424,
      "grad_norm": 0.28015854954719543,
      "learning_rate": 3.6e-06,
      "loss": 0.0032,
      "step": 139200
    },
    {
      "epoch": 7.424533333333334,
      "grad_norm": 0.02801545523107052,
      "learning_rate": 3.596666666666667e-06,
      "loss": 0.0023,
      "step": 139210
    },
    {
      "epoch": 7.425066666666667,
      "grad_norm": 0.16809490323066711,
      "learning_rate": 3.5933333333333334e-06,
      "loss": 0.0031,
      "step": 139220
    },
    {
      "epoch": 7.4256,
      "grad_norm": 0.14007963240146637,
      "learning_rate": 3.5900000000000004e-06,
      "loss": 0.0025,
      "step": 139230
    },
    {
      "epoch": 7.4261333333333335,
      "grad_norm": 0.08404689282178879,
      "learning_rate": 3.5866666666666665e-06,
      "loss": 0.0038,
      "step": 139240
    },
    {
      "epoch": 7.426666666666667,
      "grad_norm": 2.863264070285254e-09,
      "learning_rate": 3.5833333333333335e-06,
      "loss": 0.0031,
      "step": 139250
    },
    {
      "epoch": 7.4272,
      "grad_norm": 0.3361831307411194,
      "learning_rate": 3.58e-06,
      "loss": 0.0019,
      "step": 139260
    },
    {
      "epoch": 7.427733333333333,
      "grad_norm": 0.08404791355133057,
      "learning_rate": 3.576666666666667e-06,
      "loss": 0.0026,
      "step": 139270
    },
    {
      "epoch": 7.428266666666667,
      "grad_norm": 0.3642108738422394,
      "learning_rate": 3.5733333333333336e-06,
      "loss": 0.0024,
      "step": 139280
    },
    {
      "epoch": 7.4288,
      "grad_norm": 0.02801583521068096,
      "learning_rate": 3.5700000000000005e-06,
      "loss": 0.002,
      "step": 139290
    },
    {
      "epoch": 7.429333333333333,
      "grad_norm": 0.16809329390525818,
      "learning_rate": 3.5666666666666667e-06,
      "loss": 0.002,
      "step": 139300
    },
    {
      "epoch": 7.429866666666666,
      "grad_norm": 0.4202377498149872,
      "learning_rate": 3.5633333333333337e-06,
      "loss": 0.0022,
      "step": 139310
    },
    {
      "epoch": 7.4304,
      "grad_norm": 0.05603215843439102,
      "learning_rate": 3.5600000000000002e-06,
      "loss": 0.0021,
      "step": 139320
    },
    {
      "epoch": 7.430933333333333,
      "grad_norm": 0.16809700429439545,
      "learning_rate": 3.556666666666667e-06,
      "loss": 0.0033,
      "step": 139330
    },
    {
      "epoch": 7.431466666666667,
      "grad_norm": 0.14007948338985443,
      "learning_rate": 3.5533333333333333e-06,
      "loss": 0.003,
      "step": 139340
    },
    {
      "epoch": 7.432,
      "grad_norm": 0.19611024856567383,
      "learning_rate": 3.55e-06,
      "loss": 0.0025,
      "step": 139350
    },
    {
      "epoch": 7.432533333333334,
      "grad_norm": 0.08404923230409622,
      "learning_rate": 3.546666666666667e-06,
      "loss": 0.0033,
      "step": 139360
    },
    {
      "epoch": 7.433066666666667,
      "grad_norm": 0.05603436380624771,
      "learning_rate": 3.5433333333333334e-06,
      "loss": 0.0025,
      "step": 139370
    },
    {
      "epoch": 7.4336,
      "grad_norm": 0.11206915974617004,
      "learning_rate": 3.5400000000000004e-06,
      "loss": 0.0024,
      "step": 139380
    },
    {
      "epoch": 7.4341333333333335,
      "grad_norm": 0.19611969590187073,
      "learning_rate": 3.5366666666666665e-06,
      "loss": 0.0028,
      "step": 139390
    },
    {
      "epoch": 7.434666666666667,
      "grad_norm": 0.02801659144461155,
      "learning_rate": 3.5333333333333335e-06,
      "loss": 0.0022,
      "step": 139400
    },
    {
      "epoch": 7.4352,
      "grad_norm": 0.0280157458037138,
      "learning_rate": 3.53e-06,
      "loss": 0.0029,
      "step": 139410
    },
    {
      "epoch": 7.435733333333333,
      "grad_norm": 3.328606945629531e-09,
      "learning_rate": 3.526666666666667e-06,
      "loss": 0.0027,
      "step": 139420
    },
    {
      "epoch": 7.436266666666667,
      "grad_norm": 3.801723824636838e-09,
      "learning_rate": 3.523333333333333e-06,
      "loss": 0.0023,
      "step": 139430
    },
    {
      "epoch": 7.4368,
      "grad_norm": 0.1120622456073761,
      "learning_rate": 3.52e-06,
      "loss": 0.0022,
      "step": 139440
    },
    {
      "epoch": 7.437333333333333,
      "grad_norm": 0.22412647306919098,
      "learning_rate": 3.5166666666666667e-06,
      "loss": 0.0033,
      "step": 139450
    },
    {
      "epoch": 7.437866666666666,
      "grad_norm": 0.16809536516666412,
      "learning_rate": 3.5133333333333337e-06,
      "loss": 0.0029,
      "step": 139460
    },
    {
      "epoch": 7.4384,
      "grad_norm": 1.7425012588500977,
      "learning_rate": 3.5100000000000003e-06,
      "loss": 0.0037,
      "step": 139470
    },
    {
      "epoch": 7.438933333333333,
      "grad_norm": 0.1120641753077507,
      "learning_rate": 3.5066666666666673e-06,
      "loss": 0.0023,
      "step": 139480
    },
    {
      "epoch": 7.439466666666666,
      "grad_norm": 0.08404812216758728,
      "learning_rate": 3.5033333333333334e-06,
      "loss": 0.0026,
      "step": 139490
    },
    {
      "epoch": 7.44,
      "grad_norm": 0.1680934578180313,
      "learning_rate": 3.5000000000000004e-06,
      "loss": 0.0025,
      "step": 139500
    },
    {
      "epoch": 7.440533333333334,
      "grad_norm": 0.14007599651813507,
      "learning_rate": 3.496666666666667e-06,
      "loss": 0.0033,
      "step": 139510
    },
    {
      "epoch": 7.441066666666667,
      "grad_norm": 0.459329754114151,
      "learning_rate": 3.493333333333333e-06,
      "loss": 0.0025,
      "step": 139520
    },
    {
      "epoch": 7.4416,
      "grad_norm": 0.08404599875211716,
      "learning_rate": 3.49e-06,
      "loss": 0.0022,
      "step": 139530
    },
    {
      "epoch": 7.4421333333333335,
      "grad_norm": 0.11206144094467163,
      "learning_rate": 3.4866666666666666e-06,
      "loss": 0.0016,
      "step": 139540
    },
    {
      "epoch": 7.442666666666667,
      "grad_norm": 0.11206000298261642,
      "learning_rate": 3.4833333333333336e-06,
      "loss": 0.003,
      "step": 139550
    },
    {
      "epoch": 7.4432,
      "grad_norm": 0.11206408590078354,
      "learning_rate": 3.4799999999999997e-06,
      "loss": 0.0017,
      "step": 139560
    },
    {
      "epoch": 7.443733333333333,
      "grad_norm": 0.4762882590293884,
      "learning_rate": 3.476666666666667e-06,
      "loss": 0.0025,
      "step": 139570
    },
    {
      "epoch": 7.444266666666667,
      "grad_norm": 5.706084049705851e-09,
      "learning_rate": 3.4733333333333333e-06,
      "loss": 0.0018,
      "step": 139580
    },
    {
      "epoch": 7.4448,
      "grad_norm": 0.11206340044736862,
      "learning_rate": 3.4700000000000002e-06,
      "loss": 0.0014,
      "step": 139590
    },
    {
      "epoch": 7.445333333333333,
      "grad_norm": 0.056031499058008194,
      "learning_rate": 3.466666666666667e-06,
      "loss": 0.0043,
      "step": 139600
    },
    {
      "epoch": 7.445866666666666,
      "grad_norm": 0.05618354678153992,
      "learning_rate": 3.4633333333333338e-06,
      "loss": 0.0025,
      "step": 139610
    },
    {
      "epoch": 7.4464,
      "grad_norm": 0.11205989122390747,
      "learning_rate": 3.46e-06,
      "loss": 0.003,
      "step": 139620
    },
    {
      "epoch": 7.446933333333333,
      "grad_norm": 0.19610854983329773,
      "learning_rate": 3.456666666666667e-06,
      "loss": 0.0024,
      "step": 139630
    },
    {
      "epoch": 7.447466666666667,
      "grad_norm": 0.0840473473072052,
      "learning_rate": 3.4533333333333334e-06,
      "loss": 0.0022,
      "step": 139640
    },
    {
      "epoch": 7.448,
      "grad_norm": 0.22412534058094025,
      "learning_rate": 3.4500000000000004e-06,
      "loss": 0.0027,
      "step": 139650
    },
    {
      "epoch": 7.448533333333334,
      "grad_norm": 0.42023301124572754,
      "learning_rate": 3.446666666666667e-06,
      "loss": 0.0023,
      "step": 139660
    },
    {
      "epoch": 7.449066666666667,
      "grad_norm": 0.2867792844772339,
      "learning_rate": 3.443333333333334e-06,
      "loss": 0.0033,
      "step": 139670
    },
    {
      "epoch": 7.4496,
      "grad_norm": 0.05603041499853134,
      "learning_rate": 3.44e-06,
      "loss": 0.0031,
      "step": 139680
    },
    {
      "epoch": 7.4501333333333335,
      "grad_norm": 2.8724054246254127e-09,
      "learning_rate": 3.4366666666666667e-06,
      "loss": 0.0028,
      "step": 139690
    },
    {
      "epoch": 7.450666666666667,
      "grad_norm": 0.15896917879581451,
      "learning_rate": 3.4333333333333336e-06,
      "loss": 0.0027,
      "step": 139700
    },
    {
      "epoch": 7.4512,
      "grad_norm": 0.05603080242872238,
      "learning_rate": 3.4299999999999998e-06,
      "loss": 0.0023,
      "step": 139710
    },
    {
      "epoch": 7.451733333333333,
      "grad_norm": 0.028024090453982353,
      "learning_rate": 3.4266666666666668e-06,
      "loss": 0.004,
      "step": 139720
    },
    {
      "epoch": 7.452266666666667,
      "grad_norm": 0.028015384450554848,
      "learning_rate": 3.4233333333333333e-06,
      "loss": 0.0022,
      "step": 139730
    },
    {
      "epoch": 7.4528,
      "grad_norm": 0.05603034049272537,
      "learning_rate": 3.4200000000000003e-06,
      "loss": 0.0021,
      "step": 139740
    },
    {
      "epoch": 7.453333333333333,
      "grad_norm": 0.0840451642870903,
      "learning_rate": 3.4166666666666664e-06,
      "loss": 0.0024,
      "step": 139750
    },
    {
      "epoch": 7.453866666666666,
      "grad_norm": 0.05603068321943283,
      "learning_rate": 3.413333333333334e-06,
      "loss": 0.0025,
      "step": 139760
    },
    {
      "epoch": 7.4544,
      "grad_norm": 0.02801540493965149,
      "learning_rate": 3.41e-06,
      "loss": 0.0026,
      "step": 139770
    },
    {
      "epoch": 7.454933333333333,
      "grad_norm": 0.16809020936489105,
      "learning_rate": 3.406666666666667e-06,
      "loss": 0.0021,
      "step": 139780
    },
    {
      "epoch": 7.455466666666666,
      "grad_norm": 0.3081599473953247,
      "learning_rate": 3.4033333333333335e-06,
      "loss": 0.0025,
      "step": 139790
    },
    {
      "epoch": 7.456,
      "grad_norm": 0.028015052899718285,
      "learning_rate": 3.4000000000000005e-06,
      "loss": 0.0037,
      "step": 139800
    },
    {
      "epoch": 7.456533333333334,
      "grad_norm": 1.0209029912948608,
      "learning_rate": 3.3966666666666666e-06,
      "loss": 0.0026,
      "step": 139810
    },
    {
      "epoch": 7.457066666666667,
      "grad_norm": 0.08404605090618134,
      "learning_rate": 3.3933333333333336e-06,
      "loss": 0.0031,
      "step": 139820
    },
    {
      "epoch": 7.4576,
      "grad_norm": 0.7672280669212341,
      "learning_rate": 3.39e-06,
      "loss": 0.0025,
      "step": 139830
    },
    {
      "epoch": 7.4581333333333335,
      "grad_norm": 0.28015223145484924,
      "learning_rate": 3.386666666666667e-06,
      "loss": 0.0025,
      "step": 139840
    },
    {
      "epoch": 7.458666666666667,
      "grad_norm": 0.4762618839740753,
      "learning_rate": 3.3833333333333337e-06,
      "loss": 0.0027,
      "step": 139850
    },
    {
      "epoch": 7.4592,
      "grad_norm": 0.08404615521430969,
      "learning_rate": 3.38e-06,
      "loss": 0.006,
      "step": 139860
    },
    {
      "epoch": 7.459733333333333,
      "grad_norm": 0.16809262335300446,
      "learning_rate": 3.376666666666667e-06,
      "loss": 0.0019,
      "step": 139870
    },
    {
      "epoch": 7.460266666666667,
      "grad_norm": 0.14007525146007538,
      "learning_rate": 3.3733333333333334e-06,
      "loss": 0.0024,
      "step": 139880
    },
    {
      "epoch": 7.4608,
      "grad_norm": 3.169105644573733e-09,
      "learning_rate": 3.3700000000000003e-06,
      "loss": 0.0036,
      "step": 139890
    },
    {
      "epoch": 7.461333333333333,
      "grad_norm": 0.1120622456073761,
      "learning_rate": 3.3666666666666665e-06,
      "loss": 0.0033,
      "step": 139900
    },
    {
      "epoch": 7.461866666666666,
      "grad_norm": 0.0560309924185276,
      "learning_rate": 3.3633333333333335e-06,
      "loss": 0.003,
      "step": 139910
    },
    {
      "epoch": 7.4624,
      "grad_norm": 0.19610919058322906,
      "learning_rate": 3.36e-06,
      "loss": 0.0025,
      "step": 139920
    },
    {
      "epoch": 7.462933333333333,
      "grad_norm": 0.1961076259613037,
      "learning_rate": 3.356666666666667e-06,
      "loss": 0.0027,
      "step": 139930
    },
    {
      "epoch": 7.463466666666667,
      "grad_norm": 0.19610579311847687,
      "learning_rate": 3.353333333333333e-06,
      "loss": 0.0024,
      "step": 139940
    },
    {
      "epoch": 7.464,
      "grad_norm": 3.905433754169962e-09,
      "learning_rate": 3.3500000000000005e-06,
      "loss": 0.0028,
      "step": 139950
    },
    {
      "epoch": 7.464533333333334,
      "grad_norm": 0.05603079870343208,
      "learning_rate": 3.3466666666666667e-06,
      "loss": 0.0027,
      "step": 139960
    },
    {
      "epoch": 7.465066666666667,
      "grad_norm": 0.028015494346618652,
      "learning_rate": 3.3433333333333337e-06,
      "loss": 0.0016,
      "step": 139970
    },
    {
      "epoch": 7.4656,
      "grad_norm": 0.028015656396746635,
      "learning_rate": 3.34e-06,
      "loss": 0.002,
      "step": 139980
    },
    {
      "epoch": 7.4661333333333335,
      "grad_norm": 0.0311916321516037,
      "learning_rate": 3.336666666666667e-06,
      "loss": 0.0038,
      "step": 139990
    },
    {
      "epoch": 7.466666666666667,
      "grad_norm": 0.056030984967947006,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.0016,
      "step": 140000
    },
    {
      "epoch": 7.4672,
      "grad_norm": 0.05603066459298134,
      "learning_rate": 3.3300000000000003e-06,
      "loss": 0.0024,
      "step": 140010
    },
    {
      "epoch": 7.467733333333333,
      "grad_norm": 0.11206214129924774,
      "learning_rate": 3.326666666666667e-06,
      "loss": 0.0022,
      "step": 140020
    },
    {
      "epoch": 7.468266666666667,
      "grad_norm": 0.403780072927475,
      "learning_rate": 3.323333333333333e-06,
      "loss": 0.0025,
      "step": 140030
    },
    {
      "epoch": 7.4688,
      "grad_norm": 0.11206122487783432,
      "learning_rate": 3.3200000000000004e-06,
      "loss": 0.0029,
      "step": 140040
    },
    {
      "epoch": 7.469333333333333,
      "grad_norm": 0.02801574394106865,
      "learning_rate": 3.3166666666666665e-06,
      "loss": 0.002,
      "step": 140050
    },
    {
      "epoch": 7.469866666666666,
      "grad_norm": 0.16809697449207306,
      "learning_rate": 3.3133333333333335e-06,
      "loss": 0.0018,
      "step": 140060
    },
    {
      "epoch": 7.4704,
      "grad_norm": 0.08404728025197983,
      "learning_rate": 3.31e-06,
      "loss": 0.0016,
      "step": 140070
    },
    {
      "epoch": 7.470933333333333,
      "grad_norm": 0.16809172928333282,
      "learning_rate": 3.306666666666667e-06,
      "loss": 0.0035,
      "step": 140080
    },
    {
      "epoch": 7.471466666666666,
      "grad_norm": 0.1680908203125,
      "learning_rate": 3.303333333333333e-06,
      "loss": 0.0018,
      "step": 140090
    },
    {
      "epoch": 7.4719999999999995,
      "grad_norm": 0.14007511734962463,
      "learning_rate": 3.3e-06,
      "loss": 0.0024,
      "step": 140100
    },
    {
      "epoch": 7.472533333333334,
      "grad_norm": 0.05603031441569328,
      "learning_rate": 3.2966666666666667e-06,
      "loss": 0.0024,
      "step": 140110
    },
    {
      "epoch": 7.473066666666667,
      "grad_norm": 0.2521379888057709,
      "learning_rate": 3.2933333333333337e-06,
      "loss": 0.0017,
      "step": 140120
    },
    {
      "epoch": 7.4736,
      "grad_norm": 0.42023104429244995,
      "learning_rate": 3.29e-06,
      "loss": 0.0042,
      "step": 140130
    },
    {
      "epoch": 7.4741333333333335,
      "grad_norm": 0.19611036777496338,
      "learning_rate": 3.2866666666666672e-06,
      "loss": 0.0025,
      "step": 140140
    },
    {
      "epoch": 7.474666666666667,
      "grad_norm": 0.4482473134994507,
      "learning_rate": 3.2833333333333334e-06,
      "loss": 0.0031,
      "step": 140150
    },
    {
      "epoch": 7.4752,
      "grad_norm": 0.4202304184436798,
      "learning_rate": 3.2800000000000004e-06,
      "loss": 0.0024,
      "step": 140160
    },
    {
      "epoch": 7.475733333333333,
      "grad_norm": 0.14007821679115295,
      "learning_rate": 3.276666666666667e-06,
      "loss": 0.0019,
      "step": 140170
    },
    {
      "epoch": 7.476266666666667,
      "grad_norm": 0.08404716849327087,
      "learning_rate": 3.273333333333334e-06,
      "loss": 0.002,
      "step": 140180
    },
    {
      "epoch": 7.4768,
      "grad_norm": 0.05603082478046417,
      "learning_rate": 3.27e-06,
      "loss": 0.0029,
      "step": 140190
    },
    {
      "epoch": 7.477333333333333,
      "grad_norm": 0.08404608815908432,
      "learning_rate": 3.2666666666666666e-06,
      "loss": 0.0025,
      "step": 140200
    },
    {
      "epoch": 7.477866666666666,
      "grad_norm": 0.140079066157341,
      "learning_rate": 3.2633333333333336e-06,
      "loss": 0.0018,
      "step": 140210
    },
    {
      "epoch": 7.4784,
      "grad_norm": 0.16809579730033875,
      "learning_rate": 3.2599999999999997e-06,
      "loss": 0.0021,
      "step": 140220
    },
    {
      "epoch": 7.478933333333333,
      "grad_norm": 0.14007921516895294,
      "learning_rate": 3.2566666666666667e-06,
      "loss": 0.0016,
      "step": 140230
    },
    {
      "epoch": 7.479466666666666,
      "grad_norm": 0.12531325221061707,
      "learning_rate": 3.2533333333333332e-06,
      "loss": 0.0029,
      "step": 140240
    },
    {
      "epoch": 7.48,
      "grad_norm": 0.25214120745658875,
      "learning_rate": 3.2500000000000002e-06,
      "loss": 0.0018,
      "step": 140250
    },
    {
      "epoch": 7.480533333333334,
      "grad_norm": 0.4482509195804596,
      "learning_rate": 3.2466666666666668e-06,
      "loss": 0.0025,
      "step": 140260
    },
    {
      "epoch": 7.481066666666667,
      "grad_norm": 0.19610720872879028,
      "learning_rate": 3.2433333333333338e-06,
      "loss": 0.0024,
      "step": 140270
    },
    {
      "epoch": 7.4816,
      "grad_norm": 0.42022648453712463,
      "learning_rate": 3.24e-06,
      "loss": 0.0023,
      "step": 140280
    },
    {
      "epoch": 7.4821333333333335,
      "grad_norm": 0.02867398038506508,
      "learning_rate": 3.236666666666667e-06,
      "loss": 0.0036,
      "step": 140290
    },
    {
      "epoch": 7.482666666666667,
      "grad_norm": 0.05603097751736641,
      "learning_rate": 3.2333333333333334e-06,
      "loss": 0.0022,
      "step": 140300
    },
    {
      "epoch": 7.4832,
      "grad_norm": 0.05603092908859253,
      "learning_rate": 3.2300000000000004e-06,
      "loss": 0.0027,
      "step": 140310
    },
    {
      "epoch": 7.483733333333333,
      "grad_norm": 0.08404647558927536,
      "learning_rate": 3.2266666666666665e-06,
      "loss": 0.0023,
      "step": 140320
    },
    {
      "epoch": 7.484266666666667,
      "grad_norm": 0.22412489354610443,
      "learning_rate": 3.223333333333334e-06,
      "loss": 0.0022,
      "step": 140330
    },
    {
      "epoch": 7.4848,
      "grad_norm": 0.3743080198764801,
      "learning_rate": 3.22e-06,
      "loss": 0.0025,
      "step": 140340
    },
    {
      "epoch": 7.485333333333333,
      "grad_norm": 0.11782056093215942,
      "learning_rate": 3.216666666666667e-06,
      "loss": 0.0023,
      "step": 140350
    },
    {
      "epoch": 7.4858666666666664,
      "grad_norm": 0.25215065479278564,
      "learning_rate": 3.2133333333333336e-06,
      "loss": 0.0029,
      "step": 140360
    },
    {
      "epoch": 7.4864,
      "grad_norm": 0.11206520348787308,
      "learning_rate": 3.2099999999999998e-06,
      "loss": 0.0019,
      "step": 140370
    },
    {
      "epoch": 7.486933333333333,
      "grad_norm": 0.23059265315532684,
      "learning_rate": 3.2066666666666667e-06,
      "loss": 0.0029,
      "step": 140380
    },
    {
      "epoch": 7.487466666666666,
      "grad_norm": 0.08404668420553207,
      "learning_rate": 3.2033333333333333e-06,
      "loss": 0.0023,
      "step": 140390
    },
    {
      "epoch": 7.4879999999999995,
      "grad_norm": 0.4482458233833313,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 0.0022,
      "step": 140400
    },
    {
      "epoch": 7.488533333333334,
      "grad_norm": 0.19610297679901123,
      "learning_rate": 3.1966666666666664e-06,
      "loss": 0.0025,
      "step": 140410
    },
    {
      "epoch": 7.489066666666667,
      "grad_norm": 0.22412024438381195,
      "learning_rate": 3.1933333333333334e-06,
      "loss": 0.0019,
      "step": 140420
    },
    {
      "epoch": 7.4896,
      "grad_norm": 0.028015797957777977,
      "learning_rate": 3.19e-06,
      "loss": 0.0023,
      "step": 140430
    },
    {
      "epoch": 7.4901333333333335,
      "grad_norm": 0.05604080110788345,
      "learning_rate": 3.186666666666667e-06,
      "loss": 0.0022,
      "step": 140440
    },
    {
      "epoch": 7.490666666666667,
      "grad_norm": 0.028015539050102234,
      "learning_rate": 3.1833333333333335e-06,
      "loss": 0.0024,
      "step": 140450
    },
    {
      "epoch": 7.4912,
      "grad_norm": 0.2801520526409149,
      "learning_rate": 3.1800000000000005e-06,
      "loss": 0.0038,
      "step": 140460
    },
    {
      "epoch": 7.491733333333333,
      "grad_norm": 0.08404625207185745,
      "learning_rate": 3.1766666666666666e-06,
      "loss": 0.0027,
      "step": 140470
    },
    {
      "epoch": 7.492266666666667,
      "grad_norm": 0.3081720769405365,
      "learning_rate": 3.1733333333333336e-06,
      "loss": 0.0016,
      "step": 140480
    },
    {
      "epoch": 7.4928,
      "grad_norm": 0.8416600227355957,
      "learning_rate": 3.17e-06,
      "loss": 0.0032,
      "step": 140490
    },
    {
      "epoch": 7.493333333333333,
      "grad_norm": 0.028015121817588806,
      "learning_rate": 3.166666666666667e-06,
      "loss": 0.0019,
      "step": 140500
    },
    {
      "epoch": 7.4938666666666665,
      "grad_norm": 0.0010029703844338655,
      "learning_rate": 3.1633333333333333e-06,
      "loss": 0.0033,
      "step": 140510
    },
    {
      "epoch": 7.4944,
      "grad_norm": 1.3853473663330078,
      "learning_rate": 3.1600000000000007e-06,
      "loss": 0.0022,
      "step": 140520
    },
    {
      "epoch": 7.494933333333333,
      "grad_norm": 0.23512977361679077,
      "learning_rate": 3.156666666666667e-06,
      "loss": 0.0038,
      "step": 140530
    },
    {
      "epoch": 7.495466666666666,
      "grad_norm": 0.3641957640647888,
      "learning_rate": 3.153333333333333e-06,
      "loss": 0.0023,
      "step": 140540
    },
    {
      "epoch": 7.496,
      "grad_norm": 0.15080946683883667,
      "learning_rate": 3.1500000000000003e-06,
      "loss": 0.0015,
      "step": 140550
    },
    {
      "epoch": 7.496533333333334,
      "grad_norm": 0.3924260437488556,
      "learning_rate": 3.1466666666666665e-06,
      "loss": 0.0022,
      "step": 140560
    },
    {
      "epoch": 7.497066666666667,
      "grad_norm": 0.25213637948036194,
      "learning_rate": 3.1433333333333334e-06,
      "loss": 0.0022,
      "step": 140570
    },
    {
      "epoch": 7.4976,
      "grad_norm": 0.05603007227182388,
      "learning_rate": 3.14e-06,
      "loss": 0.0038,
      "step": 140580
    },
    {
      "epoch": 7.4981333333333335,
      "grad_norm": 0.028014693409204483,
      "learning_rate": 3.136666666666667e-06,
      "loss": 0.0022,
      "step": 140590
    },
    {
      "epoch": 7.498666666666667,
      "grad_norm": 0.25688427686691284,
      "learning_rate": 3.133333333333333e-06,
      "loss": 0.0021,
      "step": 140600
    },
    {
      "epoch": 7.4992,
      "grad_norm": 0.02801506593823433,
      "learning_rate": 3.13e-06,
      "loss": 0.004,
      "step": 140610
    },
    {
      "epoch": 7.499733333333333,
      "grad_norm": 0.14007468521595,
      "learning_rate": 3.1266666666666667e-06,
      "loss": 0.0023,
      "step": 140620
    },
    {
      "epoch": 7.500266666666667,
      "grad_norm": 0.028014764189720154,
      "learning_rate": 3.1233333333333332e-06,
      "loss": 0.0018,
      "step": 140630
    },
    {
      "epoch": 7.5008,
      "grad_norm": 0.303477019071579,
      "learning_rate": 3.12e-06,
      "loss": 0.0016,
      "step": 140640
    },
    {
      "epoch": 7.501333333333333,
      "grad_norm": 0.19610677659511566,
      "learning_rate": 3.1166666666666668e-06,
      "loss": 0.0027,
      "step": 140650
    },
    {
      "epoch": 7.5018666666666665,
      "grad_norm": 0.056030482053756714,
      "learning_rate": 3.1133333333333333e-06,
      "loss": 0.002,
      "step": 140660
    },
    {
      "epoch": 7.5024,
      "grad_norm": 0.08404572308063507,
      "learning_rate": 3.11e-06,
      "loss": 0.0032,
      "step": 140670
    },
    {
      "epoch": 7.502933333333333,
      "grad_norm": 0.056030794978141785,
      "learning_rate": 3.106666666666667e-06,
      "loss": 0.0025,
      "step": 140680
    },
    {
      "epoch": 7.503466666666666,
      "grad_norm": 0.08405540138483047,
      "learning_rate": 3.1033333333333334e-06,
      "loss": 0.0028,
      "step": 140690
    },
    {
      "epoch": 7.504,
      "grad_norm": 0.25213339924812317,
      "learning_rate": 3.1e-06,
      "loss": 0.002,
      "step": 140700
    },
    {
      "epoch": 7.504533333333333,
      "grad_norm": 0.3641940951347351,
      "learning_rate": 3.096666666666667e-06,
      "loss": 0.0018,
      "step": 140710
    },
    {
      "epoch": 7.505066666666667,
      "grad_norm": 0.3880476951599121,
      "learning_rate": 3.0933333333333335e-06,
      "loss": 0.003,
      "step": 140720
    },
    {
      "epoch": 7.5056,
      "grad_norm": 0.2801481783390045,
      "learning_rate": 3.09e-06,
      "loss": 0.0023,
      "step": 140730
    },
    {
      "epoch": 7.5061333333333335,
      "grad_norm": 0.084046371281147,
      "learning_rate": 3.086666666666667e-06,
      "loss": 0.0017,
      "step": 140740
    },
    {
      "epoch": 7.506666666666667,
      "grad_norm": 0.02801539935171604,
      "learning_rate": 3.0833333333333336e-06,
      "loss": 0.0031,
      "step": 140750
    },
    {
      "epoch": 7.5072,
      "grad_norm": 0.14007477462291718,
      "learning_rate": 3.08e-06,
      "loss": 0.0028,
      "step": 140760
    },
    {
      "epoch": 7.507733333333333,
      "grad_norm": 0.24942980706691742,
      "learning_rate": 3.076666666666667e-06,
      "loss": 0.0026,
      "step": 140770
    },
    {
      "epoch": 7.508266666666667,
      "grad_norm": 0.1409987360239029,
      "learning_rate": 3.0733333333333337e-06,
      "loss": 0.0019,
      "step": 140780
    },
    {
      "epoch": 7.5088,
      "grad_norm": 0.11208423972129822,
      "learning_rate": 3.0700000000000003e-06,
      "loss": 0.0024,
      "step": 140790
    },
    {
      "epoch": 7.509333333333333,
      "grad_norm": 0.0285747479647398,
      "learning_rate": 3.066666666666667e-06,
      "loss": 0.0032,
      "step": 140800
    },
    {
      "epoch": 7.5098666666666665,
      "grad_norm": 0.02901856042444706,
      "learning_rate": 3.0633333333333334e-06,
      "loss": 0.0028,
      "step": 140810
    },
    {
      "epoch": 7.5104,
      "grad_norm": 0.16449929773807526,
      "learning_rate": 3.06e-06,
      "loss": 0.0024,
      "step": 140820
    },
    {
      "epoch": 7.510933333333333,
      "grad_norm": 0.11205890774726868,
      "learning_rate": 3.056666666666667e-06,
      "loss": 0.0015,
      "step": 140830
    },
    {
      "epoch": 7.511466666666666,
      "grad_norm": 0.05602934584021568,
      "learning_rate": 3.0533333333333335e-06,
      "loss": 0.0013,
      "step": 140840
    },
    {
      "epoch": 7.5120000000000005,
      "grad_norm": 0.11205898225307465,
      "learning_rate": 3.05e-06,
      "loss": 0.0032,
      "step": 140850
    },
    {
      "epoch": 7.512533333333334,
      "grad_norm": 0.42022785544395447,
      "learning_rate": 3.0466666666666666e-06,
      "loss": 0.0018,
      "step": 140860
    },
    {
      "epoch": 7.513066666666667,
      "grad_norm": 0.7517446279525757,
      "learning_rate": 3.0433333333333336e-06,
      "loss": 0.0035,
      "step": 140870
    },
    {
      "epoch": 7.5136,
      "grad_norm": 4.936196340565857e-09,
      "learning_rate": 3.04e-06,
      "loss": 0.002,
      "step": 140880
    },
    {
      "epoch": 7.5141333333333336,
      "grad_norm": 0.36419594287872314,
      "learning_rate": 3.0366666666666667e-06,
      "loss": 0.0021,
      "step": 140890
    },
    {
      "epoch": 7.514666666666667,
      "grad_norm": 0.11205808073282242,
      "learning_rate": 3.0333333333333337e-06,
      "loss": 0.003,
      "step": 140900
    },
    {
      "epoch": 7.5152,
      "grad_norm": 0.056029271334409714,
      "learning_rate": 3.0300000000000002e-06,
      "loss": 0.0026,
      "step": 140910
    },
    {
      "epoch": 7.515733333333333,
      "grad_norm": 0.3081638514995575,
      "learning_rate": 3.0266666666666668e-06,
      "loss": 0.0014,
      "step": 140920
    },
    {
      "epoch": 7.516266666666667,
      "grad_norm": 0.1120586171746254,
      "learning_rate": 3.0233333333333338e-06,
      "loss": 0.0029,
      "step": 140930
    },
    {
      "epoch": 7.5168,
      "grad_norm": 0.19610129296779633,
      "learning_rate": 3.0200000000000003e-06,
      "loss": 0.0029,
      "step": 140940
    },
    {
      "epoch": 7.517333333333333,
      "grad_norm": 0.4202214777469635,
      "learning_rate": 3.016666666666667e-06,
      "loss": 0.0015,
      "step": 140950
    },
    {
      "epoch": 7.5178666666666665,
      "grad_norm": 0.02801525592803955,
      "learning_rate": 3.0133333333333334e-06,
      "loss": 0.0018,
      "step": 140960
    },
    {
      "epoch": 7.5184,
      "grad_norm": 0.16809193789958954,
      "learning_rate": 3.01e-06,
      "loss": 0.0023,
      "step": 140970
    },
    {
      "epoch": 7.518933333333333,
      "grad_norm": 0.02801504172384739,
      "learning_rate": 3.0066666666666665e-06,
      "loss": 0.0028,
      "step": 140980
    },
    {
      "epoch": 7.519466666666666,
      "grad_norm": 0.4482375979423523,
      "learning_rate": 3.0033333333333335e-06,
      "loss": 0.0031,
      "step": 140990
    },
    {
      "epoch": 7.52,
      "grad_norm": 2.453770076016326e-09,
      "learning_rate": 3e-06,
      "loss": 0.0015,
      "step": 141000
    },
    {
      "epoch": 7.520533333333333,
      "grad_norm": 0.2801485061645508,
      "learning_rate": 2.9966666666666666e-06,
      "loss": 0.0023,
      "step": 141010
    },
    {
      "epoch": 7.521066666666667,
      "grad_norm": 0.11205977946519852,
      "learning_rate": 2.9933333333333336e-06,
      "loss": 0.0021,
      "step": 141020
    },
    {
      "epoch": 7.5216,
      "grad_norm": 0.1120595708489418,
      "learning_rate": 2.99e-06,
      "loss": 0.0032,
      "step": 141030
    },
    {
      "epoch": 7.522133333333334,
      "grad_norm": 0.028014836832880974,
      "learning_rate": 2.9866666666666667e-06,
      "loss": 0.0021,
      "step": 141040
    },
    {
      "epoch": 7.522666666666667,
      "grad_norm": 0.2801496386528015,
      "learning_rate": 2.9833333333333333e-06,
      "loss": 0.0022,
      "step": 141050
    },
    {
      "epoch": 7.5232,
      "grad_norm": 0.056029848754405975,
      "learning_rate": 2.9800000000000003e-06,
      "loss": 0.0021,
      "step": 141060
    },
    {
      "epoch": 7.523733333333333,
      "grad_norm": 0.4330240488052368,
      "learning_rate": 2.976666666666667e-06,
      "loss": 0.0026,
      "step": 141070
    },
    {
      "epoch": 7.524266666666667,
      "grad_norm": 0.1961045265197754,
      "learning_rate": 2.9733333333333334e-06,
      "loss": 0.0022,
      "step": 141080
    },
    {
      "epoch": 7.5248,
      "grad_norm": 0.3970535397529602,
      "learning_rate": 2.9700000000000004e-06,
      "loss": 0.0022,
      "step": 141090
    },
    {
      "epoch": 7.525333333333333,
      "grad_norm": 0.08404559642076492,
      "learning_rate": 2.966666666666667e-06,
      "loss": 0.0023,
      "step": 141100
    },
    {
      "epoch": 7.5258666666666665,
      "grad_norm": 0.2801518440246582,
      "learning_rate": 2.9633333333333335e-06,
      "loss": 0.0021,
      "step": 141110
    },
    {
      "epoch": 7.5264,
      "grad_norm": 0.056029971688985825,
      "learning_rate": 2.9600000000000005e-06,
      "loss": 0.0021,
      "step": 141120
    },
    {
      "epoch": 7.526933333333333,
      "grad_norm": 0.22412234544754028,
      "learning_rate": 2.956666666666667e-06,
      "loss": 0.0026,
      "step": 141130
    },
    {
      "epoch": 7.527466666666666,
      "grad_norm": 0.2241186797618866,
      "learning_rate": 2.9533333333333336e-06,
      "loss": 0.0031,
      "step": 141140
    },
    {
      "epoch": 7.5280000000000005,
      "grad_norm": 0.11206008493900299,
      "learning_rate": 2.95e-06,
      "loss": 0.0031,
      "step": 141150
    },
    {
      "epoch": 7.528533333333334,
      "grad_norm": 0.2801493704319,
      "learning_rate": 2.9466666666666667e-06,
      "loss": 0.0023,
      "step": 141160
    },
    {
      "epoch": 7.529066666666667,
      "grad_norm": 0.028014635667204857,
      "learning_rate": 2.9433333333333332e-06,
      "loss": 0.0024,
      "step": 141170
    },
    {
      "epoch": 7.5296,
      "grad_norm": 0.28014427423477173,
      "learning_rate": 2.9400000000000002e-06,
      "loss": 0.002,
      "step": 141180
    },
    {
      "epoch": 7.530133333333334,
      "grad_norm": 0.19610190391540527,
      "learning_rate": 2.9366666666666668e-06,
      "loss": 0.002,
      "step": 141190
    },
    {
      "epoch": 7.530666666666667,
      "grad_norm": 0.1120608001947403,
      "learning_rate": 2.9333333333333333e-06,
      "loss": 0.0021,
      "step": 141200
    },
    {
      "epoch": 7.5312,
      "grad_norm": 0.028014931827783585,
      "learning_rate": 2.93e-06,
      "loss": 0.0035,
      "step": 141210
    },
    {
      "epoch": 7.531733333333333,
      "grad_norm": 0.05602949857711792,
      "learning_rate": 2.926666666666667e-06,
      "loss": 0.0024,
      "step": 141220
    },
    {
      "epoch": 7.532266666666667,
      "grad_norm": 0.05602957308292389,
      "learning_rate": 2.9233333333333334e-06,
      "loss": 0.0038,
      "step": 141230
    },
    {
      "epoch": 7.5328,
      "grad_norm": 0.14008085429668427,
      "learning_rate": 2.92e-06,
      "loss": 0.0024,
      "step": 141240
    },
    {
      "epoch": 7.533333333333333,
      "grad_norm": 0.22411705553531647,
      "learning_rate": 2.916666666666667e-06,
      "loss": 0.0028,
      "step": 141250
    },
    {
      "epoch": 7.5338666666666665,
      "grad_norm": 0.08404453843832016,
      "learning_rate": 2.9133333333333335e-06,
      "loss": 0.0026,
      "step": 141260
    },
    {
      "epoch": 7.5344,
      "grad_norm": 0.2521357536315918,
      "learning_rate": 2.91e-06,
      "loss": 0.0024,
      "step": 141270
    },
    {
      "epoch": 7.534933333333333,
      "grad_norm": 0.1680905818939209,
      "learning_rate": 2.906666666666667e-06,
      "loss": 0.0022,
      "step": 141280
    },
    {
      "epoch": 7.535466666666666,
      "grad_norm": 0.05602969601750374,
      "learning_rate": 2.9033333333333336e-06,
      "loss": 0.0027,
      "step": 141290
    },
    {
      "epoch": 7.536,
      "grad_norm": 0.8325538635253906,
      "learning_rate": 2.9e-06,
      "loss": 0.0026,
      "step": 141300
    },
    {
      "epoch": 7.536533333333333,
      "grad_norm": 1.9756434266327005e-09,
      "learning_rate": 2.896666666666667e-06,
      "loss": 0.0024,
      "step": 141310
    },
    {
      "epoch": 7.537066666666667,
      "grad_norm": 0.19610315561294556,
      "learning_rate": 2.8933333333333333e-06,
      "loss": 0.0029,
      "step": 141320
    },
    {
      "epoch": 7.5376,
      "grad_norm": 0.11206912994384766,
      "learning_rate": 2.89e-06,
      "loss": 0.0027,
      "step": 141330
    },
    {
      "epoch": 7.538133333333334,
      "grad_norm": 0.11206132918596268,
      "learning_rate": 2.886666666666667e-06,
      "loss": 0.0029,
      "step": 141340
    },
    {
      "epoch": 7.538666666666667,
      "grad_norm": 0.19610589742660522,
      "learning_rate": 2.8833333333333334e-06,
      "loss": 0.0026,
      "step": 141350
    },
    {
      "epoch": 7.5392,
      "grad_norm": 0.08404453098773956,
      "learning_rate": 2.88e-06,
      "loss": 0.0027,
      "step": 141360
    },
    {
      "epoch": 7.539733333333333,
      "grad_norm": 0.08404478430747986,
      "learning_rate": 2.876666666666667e-06,
      "loss": 0.0029,
      "step": 141370
    },
    {
      "epoch": 7.540266666666667,
      "grad_norm": 7.310526073389667e-10,
      "learning_rate": 2.8733333333333335e-06,
      "loss": 0.0028,
      "step": 141380
    },
    {
      "epoch": 7.5408,
      "grad_norm": 0.11205771565437317,
      "learning_rate": 2.87e-06,
      "loss": 0.0026,
      "step": 141390
    },
    {
      "epoch": 7.541333333333333,
      "grad_norm": 0.08404342085123062,
      "learning_rate": 2.8666666666666666e-06,
      "loss": 0.0028,
      "step": 141400
    },
    {
      "epoch": 7.5418666666666665,
      "grad_norm": 0.1120588481426239,
      "learning_rate": 2.8633333333333336e-06,
      "loss": 0.0022,
      "step": 141410
    },
    {
      "epoch": 7.5424,
      "grad_norm": 0.19610238075256348,
      "learning_rate": 2.86e-06,
      "loss": 0.0013,
      "step": 141420
    },
    {
      "epoch": 7.542933333333333,
      "grad_norm": 0.19610349833965302,
      "learning_rate": 2.8566666666666667e-06,
      "loss": 0.0026,
      "step": 141430
    },
    {
      "epoch": 7.543466666666666,
      "grad_norm": 0.2521350085735321,
      "learning_rate": 2.8533333333333337e-06,
      "loss": 0.0026,
      "step": 141440
    },
    {
      "epoch": 7.5440000000000005,
      "grad_norm": 0.056030239909887314,
      "learning_rate": 2.8500000000000002e-06,
      "loss": 0.0033,
      "step": 141450
    },
    {
      "epoch": 7.544533333333334,
      "grad_norm": 0.08404573798179626,
      "learning_rate": 2.846666666666667e-06,
      "loss": 0.002,
      "step": 141460
    },
    {
      "epoch": 7.545066666666667,
      "grad_norm": 0.28015077114105225,
      "learning_rate": 2.8433333333333338e-06,
      "loss": 0.0023,
      "step": 141470
    },
    {
      "epoch": 7.5456,
      "grad_norm": 0.14117033779621124,
      "learning_rate": 2.8400000000000003e-06,
      "loss": 0.0025,
      "step": 141480
    },
    {
      "epoch": 7.546133333333334,
      "grad_norm": 0.22412055730819702,
      "learning_rate": 2.8366666666666665e-06,
      "loss": 0.0026,
      "step": 141490
    },
    {
      "epoch": 7.546666666666667,
      "grad_norm": 2.633725015854793e-09,
      "learning_rate": 2.8333333333333335e-06,
      "loss": 0.0022,
      "step": 141500
    },
    {
      "epoch": 7.5472,
      "grad_norm": 0.16809342801570892,
      "learning_rate": 2.83e-06,
      "loss": 0.0025,
      "step": 141510
    },
    {
      "epoch": 7.547733333333333,
      "grad_norm": 0.2521316707134247,
      "learning_rate": 2.8266666666666666e-06,
      "loss": 0.0028,
      "step": 141520
    },
    {
      "epoch": 7.548266666666667,
      "grad_norm": 0.11205805838108063,
      "learning_rate": 2.8233333333333335e-06,
      "loss": 0.0018,
      "step": 141530
    },
    {
      "epoch": 7.5488,
      "grad_norm": 0.1680881828069687,
      "learning_rate": 2.82e-06,
      "loss": 0.0021,
      "step": 141540
    },
    {
      "epoch": 7.549333333333333,
      "grad_norm": 0.028015103191137314,
      "learning_rate": 2.8166666666666667e-06,
      "loss": 0.0037,
      "step": 141550
    },
    {
      "epoch": 7.5498666666666665,
      "grad_norm": 0.08404459059238434,
      "learning_rate": 2.8133333333333336e-06,
      "loss": 0.0025,
      "step": 141560
    },
    {
      "epoch": 7.5504,
      "grad_norm": 0.14007426798343658,
      "learning_rate": 2.81e-06,
      "loss": 0.0019,
      "step": 141570
    },
    {
      "epoch": 7.550933333333333,
      "grad_norm": 0.08404402434825897,
      "learning_rate": 2.8066666666666668e-06,
      "loss": 0.0031,
      "step": 141580
    },
    {
      "epoch": 7.551466666666666,
      "grad_norm": 0.1680876761674881,
      "learning_rate": 2.8033333333333333e-06,
      "loss": 0.0029,
      "step": 141590
    },
    {
      "epoch": 7.552,
      "grad_norm": 0.2521322965621948,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 0.0025,
      "step": 141600
    },
    {
      "epoch": 7.552533333333333,
      "grad_norm": 0.22320514917373657,
      "learning_rate": 2.796666666666667e-06,
      "loss": 0.0023,
      "step": 141610
    },
    {
      "epoch": 7.553066666666667,
      "grad_norm": 0.056029513478279114,
      "learning_rate": 2.7933333333333334e-06,
      "loss": 0.003,
      "step": 141620
    },
    {
      "epoch": 7.5536,
      "grad_norm": 0.39221060276031494,
      "learning_rate": 2.7900000000000004e-06,
      "loss": 0.0023,
      "step": 141630
    },
    {
      "epoch": 7.554133333333334,
      "grad_norm": 0.11206597089767456,
      "learning_rate": 2.786666666666667e-06,
      "loss": 0.0023,
      "step": 141640
    },
    {
      "epoch": 7.554666666666667,
      "grad_norm": 0.0560292974114418,
      "learning_rate": 2.7833333333333335e-06,
      "loss": 0.0022,
      "step": 141650
    },
    {
      "epoch": 7.5552,
      "grad_norm": 0.028014419600367546,
      "learning_rate": 2.78e-06,
      "loss": 0.0033,
      "step": 141660
    },
    {
      "epoch": 7.555733333333333,
      "grad_norm": 0.13049162924289703,
      "learning_rate": 2.7766666666666666e-06,
      "loss": 0.0038,
      "step": 141670
    },
    {
      "epoch": 7.556266666666667,
      "grad_norm": 0.084043487906456,
      "learning_rate": 2.773333333333333e-06,
      "loss": 0.0032,
      "step": 141680
    },
    {
      "epoch": 7.5568,
      "grad_norm": 0.028014954179525375,
      "learning_rate": 2.77e-06,
      "loss": 0.0031,
      "step": 141690
    },
    {
      "epoch": 7.557333333333333,
      "grad_norm": 0.05775264650583267,
      "learning_rate": 2.7666666666666667e-06,
      "loss": 0.0031,
      "step": 141700
    },
    {
      "epoch": 7.5578666666666665,
      "grad_norm": 0.22412018477916718,
      "learning_rate": 2.7633333333333333e-06,
      "loss": 0.0022,
      "step": 141710
    },
    {
      "epoch": 7.5584,
      "grad_norm": 0.028014760464429855,
      "learning_rate": 2.7600000000000003e-06,
      "loss": 0.0042,
      "step": 141720
    },
    {
      "epoch": 7.558933333333333,
      "grad_norm": 0.08404330164194107,
      "learning_rate": 2.756666666666667e-06,
      "loss": 0.0027,
      "step": 141730
    },
    {
      "epoch": 7.559466666666666,
      "grad_norm": 0.19610147178173065,
      "learning_rate": 2.7533333333333334e-06,
      "loss": 0.0024,
      "step": 141740
    },
    {
      "epoch": 7.5600000000000005,
      "grad_norm": 0.2801460921764374,
      "learning_rate": 2.7500000000000004e-06,
      "loss": 0.0016,
      "step": 141750
    },
    {
      "epoch": 7.560533333333334,
      "grad_norm": 0.11205926537513733,
      "learning_rate": 2.746666666666667e-06,
      "loss": 0.0028,
      "step": 141760
    },
    {
      "epoch": 7.561066666666667,
      "grad_norm": 0.1042526438832283,
      "learning_rate": 2.7433333333333335e-06,
      "loss": 0.004,
      "step": 141770
    },
    {
      "epoch": 7.5616,
      "grad_norm": 0.1400756984949112,
      "learning_rate": 2.74e-06,
      "loss": 0.0017,
      "step": 141780
    },
    {
      "epoch": 7.562133333333334,
      "grad_norm": 0.08404438197612762,
      "learning_rate": 2.736666666666667e-06,
      "loss": 0.0026,
      "step": 141790
    },
    {
      "epoch": 7.562666666666667,
      "grad_norm": 0.056029994040727615,
      "learning_rate": 2.7333333333333336e-06,
      "loss": 0.0032,
      "step": 141800
    },
    {
      "epoch": 7.5632,
      "grad_norm": 0.1680884212255478,
      "learning_rate": 2.73e-06,
      "loss": 0.0027,
      "step": 141810
    },
    {
      "epoch": 7.563733333333333,
      "grad_norm": 0.22411668300628662,
      "learning_rate": 2.726666666666667e-06,
      "loss": 0.004,
      "step": 141820
    },
    {
      "epoch": 7.564266666666667,
      "grad_norm": 0.028014512732625008,
      "learning_rate": 2.7233333333333332e-06,
      "loss": 0.0042,
      "step": 141830
    },
    {
      "epoch": 7.5648,
      "grad_norm": 0.3081578314304352,
      "learning_rate": 2.72e-06,
      "loss": 0.003,
      "step": 141840
    },
    {
      "epoch": 7.565333333333333,
      "grad_norm": 0.02801433764398098,
      "learning_rate": 2.7166666666666668e-06,
      "loss": 0.0018,
      "step": 141850
    },
    {
      "epoch": 7.5658666666666665,
      "grad_norm": 0.1400720775127411,
      "learning_rate": 2.7133333333333333e-06,
      "loss": 0.0025,
      "step": 141860
    },
    {
      "epoch": 7.5664,
      "grad_norm": 0.0602017305791378,
      "learning_rate": 2.71e-06,
      "loss": 0.002,
      "step": 141870
    },
    {
      "epoch": 7.566933333333333,
      "grad_norm": 0.06063658744096756,
      "learning_rate": 2.706666666666667e-06,
      "loss": 0.0022,
      "step": 141880
    },
    {
      "epoch": 7.567466666666666,
      "grad_norm": 0.2801472246646881,
      "learning_rate": 2.7033333333333334e-06,
      "loss": 0.0023,
      "step": 141890
    },
    {
      "epoch": 7.568,
      "grad_norm": 0.112058624625206,
      "learning_rate": 2.7e-06,
      "loss": 0.0023,
      "step": 141900
    },
    {
      "epoch": 7.568533333333333,
      "grad_norm": 0.1961023509502411,
      "learning_rate": 2.696666666666667e-06,
      "loss": 0.0024,
      "step": 141910
    },
    {
      "epoch": 7.569066666666667,
      "grad_norm": 0.4202239513397217,
      "learning_rate": 2.6933333333333335e-06,
      "loss": 0.003,
      "step": 141920
    },
    {
      "epoch": 7.5696,
      "grad_norm": 0.11205869168043137,
      "learning_rate": 2.69e-06,
      "loss": 0.0026,
      "step": 141930
    },
    {
      "epoch": 7.570133333333334,
      "grad_norm": 0.05612910911440849,
      "learning_rate": 2.6866666666666666e-06,
      "loss": 0.0034,
      "step": 141940
    },
    {
      "epoch": 7.570666666666667,
      "grad_norm": 0.003493981435894966,
      "learning_rate": 2.6833333333333336e-06,
      "loss": 0.0021,
      "step": 141950
    },
    {
      "epoch": 7.5712,
      "grad_norm": 0.16808882355690002,
      "learning_rate": 2.68e-06,
      "loss": 0.0025,
      "step": 141960
    },
    {
      "epoch": 7.571733333333333,
      "grad_norm": 0.3641921579837799,
      "learning_rate": 2.6766666666666667e-06,
      "loss": 0.0026,
      "step": 141970
    },
    {
      "epoch": 7.572266666666667,
      "grad_norm": 0.05602947250008583,
      "learning_rate": 2.6733333333333337e-06,
      "loss": 0.0026,
      "step": 141980
    },
    {
      "epoch": 7.5728,
      "grad_norm": 0.1120586022734642,
      "learning_rate": 2.6700000000000003e-06,
      "loss": 0.002,
      "step": 141990
    },
    {
      "epoch": 7.573333333333333,
      "grad_norm": 0.25214219093322754,
      "learning_rate": 2.666666666666667e-06,
      "loss": 0.0033,
      "step": 142000
    },
    {
      "epoch": 7.5738666666666665,
      "grad_norm": 0.11205752938985825,
      "learning_rate": 2.6633333333333334e-06,
      "loss": 0.0034,
      "step": 142010
    },
    {
      "epoch": 7.5744,
      "grad_norm": 0.0280144065618515,
      "learning_rate": 2.66e-06,
      "loss": 0.003,
      "step": 142020
    },
    {
      "epoch": 7.574933333333333,
      "grad_norm": 0.05602947250008583,
      "learning_rate": 2.6566666666666665e-06,
      "loss": 0.0018,
      "step": 142030
    },
    {
      "epoch": 7.575466666666666,
      "grad_norm": 0.1120593473315239,
      "learning_rate": 2.6533333333333335e-06,
      "loss": 0.0026,
      "step": 142040
    },
    {
      "epoch": 7.576,
      "grad_norm": 0.39223429560661316,
      "learning_rate": 2.65e-06,
      "loss": 0.0018,
      "step": 142050
    },
    {
      "epoch": 7.576533333333334,
      "grad_norm": 0.5042662620544434,
      "learning_rate": 2.6466666666666666e-06,
      "loss": 0.0045,
      "step": 142060
    },
    {
      "epoch": 7.577066666666667,
      "grad_norm": 0.1680855005979538,
      "learning_rate": 2.6433333333333336e-06,
      "loss": 0.0022,
      "step": 142070
    },
    {
      "epoch": 7.5776,
      "grad_norm": 0.05602819100022316,
      "learning_rate": 2.64e-06,
      "loss": 0.0022,
      "step": 142080
    },
    {
      "epoch": 7.578133333333334,
      "grad_norm": 0.30815714597702026,
      "learning_rate": 2.6366666666666667e-06,
      "loss": 0.0027,
      "step": 142090
    },
    {
      "epoch": 7.578666666666667,
      "grad_norm": 0.05602926015853882,
      "learning_rate": 2.6333333333333337e-06,
      "loss": 0.002,
      "step": 142100
    },
    {
      "epoch": 7.5792,
      "grad_norm": 0.02801492065191269,
      "learning_rate": 2.6300000000000002e-06,
      "loss": 0.0024,
      "step": 142110
    },
    {
      "epoch": 7.579733333333333,
      "grad_norm": 0.028015047311782837,
      "learning_rate": 2.6266666666666668e-06,
      "loss": 0.003,
      "step": 142120
    },
    {
      "epoch": 7.580266666666667,
      "grad_norm": 0.05603006109595299,
      "learning_rate": 2.6233333333333333e-06,
      "loss": 0.0034,
      "step": 142130
    },
    {
      "epoch": 7.5808,
      "grad_norm": 0.28014951944351196,
      "learning_rate": 2.6200000000000003e-06,
      "loss": 0.0025,
      "step": 142140
    },
    {
      "epoch": 7.581333333333333,
      "grad_norm": 0.4482349157333374,
      "learning_rate": 2.616666666666667e-06,
      "loss": 0.0026,
      "step": 142150
    },
    {
      "epoch": 7.5818666666666665,
      "grad_norm": 0.4202198386192322,
      "learning_rate": 2.6133333333333334e-06,
      "loss": 0.0028,
      "step": 142160
    },
    {
      "epoch": 7.5824,
      "grad_norm": 0.47624775767326355,
      "learning_rate": 2.6100000000000004e-06,
      "loss": 0.0038,
      "step": 142170
    },
    {
      "epoch": 7.582933333333333,
      "grad_norm": 0.1680879443883896,
      "learning_rate": 2.6066666666666666e-06,
      "loss": 0.002,
      "step": 142180
    },
    {
      "epoch": 7.583466666666666,
      "grad_norm": 0.21279513835906982,
      "learning_rate": 2.603333333333333e-06,
      "loss": 0.0031,
      "step": 142190
    },
    {
      "epoch": 7.584,
      "grad_norm": 0.1916046142578125,
      "learning_rate": 2.6e-06,
      "loss": 0.0037,
      "step": 142200
    },
    {
      "epoch": 7.584533333333333,
      "grad_norm": 0.16808418929576874,
      "learning_rate": 2.5966666666666667e-06,
      "loss": 0.0028,
      "step": 142210
    },
    {
      "epoch": 7.585066666666666,
      "grad_norm": 0.16808657348155975,
      "learning_rate": 2.593333333333333e-06,
      "loss": 0.0035,
      "step": 142220
    },
    {
      "epoch": 7.5856,
      "grad_norm": 7.605841290114768e-09,
      "learning_rate": 2.59e-06,
      "loss": 0.0028,
      "step": 142230
    },
    {
      "epoch": 7.586133333333334,
      "grad_norm": 0.16810224950313568,
      "learning_rate": 2.5866666666666667e-06,
      "loss": 0.0029,
      "step": 142240
    },
    {
      "epoch": 7.586666666666667,
      "grad_norm": 0.33617720007896423,
      "learning_rate": 2.5833333333333333e-06,
      "loss": 0.0021,
      "step": 142250
    },
    {
      "epoch": 7.5872,
      "grad_norm": 0.05602862685918808,
      "learning_rate": 2.5800000000000003e-06,
      "loss": 0.0018,
      "step": 142260
    },
    {
      "epoch": 7.587733333333333,
      "grad_norm": 0.16808496415615082,
      "learning_rate": 2.576666666666667e-06,
      "loss": 0.0031,
      "step": 142270
    },
    {
      "epoch": 7.588266666666667,
      "grad_norm": 0.056028399616479874,
      "learning_rate": 2.5733333333333334e-06,
      "loss": 0.0019,
      "step": 142280
    },
    {
      "epoch": 7.5888,
      "grad_norm": 0.028039541095495224,
      "learning_rate": 2.5700000000000004e-06,
      "loss": 0.0024,
      "step": 142290
    },
    {
      "epoch": 7.589333333333333,
      "grad_norm": 0.08404385298490524,
      "learning_rate": 2.566666666666667e-06,
      "loss": 0.0025,
      "step": 142300
    },
    {
      "epoch": 7.5898666666666665,
      "grad_norm": 0.1961050033569336,
      "learning_rate": 2.5633333333333335e-06,
      "loss": 0.0028,
      "step": 142310
    },
    {
      "epoch": 7.5904,
      "grad_norm": 0.08404440432786942,
      "learning_rate": 2.56e-06,
      "loss": 0.0027,
      "step": 142320
    },
    {
      "epoch": 7.590933333333333,
      "grad_norm": 0.2480439692735672,
      "learning_rate": 2.556666666666667e-06,
      "loss": 0.0023,
      "step": 142330
    },
    {
      "epoch": 7.591466666666666,
      "grad_norm": 0.16808734834194183,
      "learning_rate": 2.5533333333333336e-06,
      "loss": 0.0026,
      "step": 142340
    },
    {
      "epoch": 7.592,
      "grad_norm": 0.028014354407787323,
      "learning_rate": 2.55e-06,
      "loss": 0.0034,
      "step": 142350
    },
    {
      "epoch": 7.592533333333334,
      "grad_norm": 0.05602940544486046,
      "learning_rate": 2.5466666666666667e-06,
      "loss": 0.0021,
      "step": 142360
    },
    {
      "epoch": 7.593066666666667,
      "grad_norm": 0.25213104486465454,
      "learning_rate": 2.5433333333333333e-06,
      "loss": 0.0025,
      "step": 142370
    },
    {
      "epoch": 7.5936,
      "grad_norm": 0.0840432196855545,
      "learning_rate": 2.54e-06,
      "loss": 0.0015,
      "step": 142380
    },
    {
      "epoch": 7.594133333333334,
      "grad_norm": 0.2365546077489853,
      "learning_rate": 2.536666666666667e-06,
      "loss": 0.0023,
      "step": 142390
    },
    {
      "epoch": 7.594666666666667,
      "grad_norm": 0.11205776780843735,
      "learning_rate": 2.5333333333333334e-06,
      "loss": 0.0019,
      "step": 142400
    },
    {
      "epoch": 7.5952,
      "grad_norm": 0.028014427050948143,
      "learning_rate": 2.53e-06,
      "loss": 0.0019,
      "step": 142410
    },
    {
      "epoch": 7.5957333333333334,
      "grad_norm": 0.1690274178981781,
      "learning_rate": 2.526666666666667e-06,
      "loss": 0.0026,
      "step": 142420
    },
    {
      "epoch": 7.596266666666667,
      "grad_norm": 0.3177691102027893,
      "learning_rate": 2.5233333333333335e-06,
      "loss": 0.0023,
      "step": 142430
    },
    {
      "epoch": 7.5968,
      "grad_norm": 0.1120605319738388,
      "learning_rate": 2.52e-06,
      "loss": 0.0023,
      "step": 142440
    },
    {
      "epoch": 7.597333333333333,
      "grad_norm": 0.028014976531267166,
      "learning_rate": 2.516666666666667e-06,
      "loss": 0.0021,
      "step": 142450
    },
    {
      "epoch": 7.5978666666666665,
      "grad_norm": 0.19610385596752167,
      "learning_rate": 2.5133333333333336e-06,
      "loss": 0.0033,
      "step": 142460
    },
    {
      "epoch": 7.5984,
      "grad_norm": 0.25671541690826416,
      "learning_rate": 2.51e-06,
      "loss": 0.0016,
      "step": 142470
    },
    {
      "epoch": 7.598933333333333,
      "grad_norm": 0.02801433764398098,
      "learning_rate": 2.506666666666667e-06,
      "loss": 0.0042,
      "step": 142480
    },
    {
      "epoch": 7.599466666666666,
      "grad_norm": 0.20337286591529846,
      "learning_rate": 2.5033333333333336e-06,
      "loss": 0.0024,
      "step": 142490
    },
    {
      "epoch": 7.6,
      "grad_norm": 0.14025820791721344,
      "learning_rate": 2.5e-06,
      "loss": 0.0033,
      "step": 142500
    },
    {
      "epoch": 7.600533333333333,
      "grad_norm": 0.11206097155809402,
      "learning_rate": 2.4966666666666668e-06,
      "loss": 0.0029,
      "step": 142510
    },
    {
      "epoch": 7.601066666666666,
      "grad_norm": 0.16809338331222534,
      "learning_rate": 2.4933333333333333e-06,
      "loss": 0.0033,
      "step": 142520
    },
    {
      "epoch": 7.6016,
      "grad_norm": 0.2282758206129074,
      "learning_rate": 2.49e-06,
      "loss": 0.0022,
      "step": 142530
    },
    {
      "epoch": 7.602133333333334,
      "grad_norm": 0.1400759071111679,
      "learning_rate": 2.486666666666667e-06,
      "loss": 0.0017,
      "step": 142540
    },
    {
      "epoch": 7.602666666666667,
      "grad_norm": 0.11205940693616867,
      "learning_rate": 2.4833333333333334e-06,
      "loss": 0.0028,
      "step": 142550
    },
    {
      "epoch": 7.6032,
      "grad_norm": 0.1961033046245575,
      "learning_rate": 2.48e-06,
      "loss": 0.0013,
      "step": 142560
    },
    {
      "epoch": 7.6037333333333335,
      "grad_norm": 0.08404426276683807,
      "learning_rate": 2.4766666666666665e-06,
      "loss": 0.0014,
      "step": 142570
    },
    {
      "epoch": 7.604266666666667,
      "grad_norm": 0.1961035281419754,
      "learning_rate": 2.4733333333333335e-06,
      "loss": 0.0029,
      "step": 142580
    },
    {
      "epoch": 7.6048,
      "grad_norm": 0.14007194340229034,
      "learning_rate": 2.47e-06,
      "loss": 0.0024,
      "step": 142590
    },
    {
      "epoch": 7.605333333333333,
      "grad_norm": 0.19610054790973663,
      "learning_rate": 2.4666666666666666e-06,
      "loss": 0.0023,
      "step": 142600
    },
    {
      "epoch": 7.6058666666666666,
      "grad_norm": 0.05602883920073509,
      "learning_rate": 2.4633333333333336e-06,
      "loss": 0.0028,
      "step": 142610
    },
    {
      "epoch": 7.6064,
      "grad_norm": 0.08404402434825897,
      "learning_rate": 2.46e-06,
      "loss": 0.0021,
      "step": 142620
    },
    {
      "epoch": 7.606933333333333,
      "grad_norm": 0.16808843612670898,
      "learning_rate": 2.4566666666666667e-06,
      "loss": 0.0021,
      "step": 142630
    },
    {
      "epoch": 7.607466666666666,
      "grad_norm": 0.14007237553596497,
      "learning_rate": 2.4533333333333337e-06,
      "loss": 0.003,
      "step": 142640
    },
    {
      "epoch": 7.608,
      "grad_norm": 2.250484687493781e-09,
      "learning_rate": 2.4500000000000003e-06,
      "loss": 0.0024,
      "step": 142650
    },
    {
      "epoch": 7.608533333333334,
      "grad_norm": 1.0447752475738525,
      "learning_rate": 2.446666666666667e-06,
      "loss": 0.0031,
      "step": 142660
    },
    {
      "epoch": 7.609066666666667,
      "grad_norm": 0.08404308557510376,
      "learning_rate": 2.4433333333333334e-06,
      "loss": 0.0022,
      "step": 142670
    },
    {
      "epoch": 7.6096,
      "grad_norm": 0.08404286205768585,
      "learning_rate": 2.4400000000000004e-06,
      "loss": 0.0021,
      "step": 142680
    },
    {
      "epoch": 7.610133333333334,
      "grad_norm": 0.05602879822254181,
      "learning_rate": 2.4366666666666665e-06,
      "loss": 0.0026,
      "step": 142690
    },
    {
      "epoch": 7.610666666666667,
      "grad_norm": 0.19610147178173065,
      "learning_rate": 2.4333333333333335e-06,
      "loss": 0.0035,
      "step": 142700
    },
    {
      "epoch": 7.6112,
      "grad_norm": 0.14007185399532318,
      "learning_rate": 2.43e-06,
      "loss": 0.0022,
      "step": 142710
    },
    {
      "epoch": 7.6117333333333335,
      "grad_norm": 1.7521001316467277e-09,
      "learning_rate": 2.4266666666666666e-06,
      "loss": 0.002,
      "step": 142720
    },
    {
      "epoch": 7.612266666666667,
      "grad_norm": 0.2241167575120926,
      "learning_rate": 2.4233333333333336e-06,
      "loss": 0.0029,
      "step": 142730
    },
    {
      "epoch": 7.6128,
      "grad_norm": 0.22411563992500305,
      "learning_rate": 2.42e-06,
      "loss": 0.002,
      "step": 142740
    },
    {
      "epoch": 7.613333333333333,
      "grad_norm": 0.05602920427918434,
      "learning_rate": 2.4166666666666667e-06,
      "loss": 0.0028,
      "step": 142750
    },
    {
      "epoch": 7.613866666666667,
      "grad_norm": 0.14007499814033508,
      "learning_rate": 2.4133333333333332e-06,
      "loss": 0.0019,
      "step": 142760
    },
    {
      "epoch": 7.6144,
      "grad_norm": 0.25213953852653503,
      "learning_rate": 2.4100000000000002e-06,
      "loss": 0.0031,
      "step": 142770
    },
    {
      "epoch": 7.614933333333333,
      "grad_norm": 0.19610707461833954,
      "learning_rate": 2.4066666666666668e-06,
      "loss": 0.0016,
      "step": 142780
    },
    {
      "epoch": 7.615466666666666,
      "grad_norm": 0.05603078380227089,
      "learning_rate": 2.4033333333333333e-06,
      "loss": 0.0027,
      "step": 142790
    },
    {
      "epoch": 7.616,
      "grad_norm": 0.16809110343456268,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 0.0031,
      "step": 142800
    },
    {
      "epoch": 7.616533333333333,
      "grad_norm": 0.1120591014623642,
      "learning_rate": 2.396666666666667e-06,
      "loss": 0.0041,
      "step": 142810
    },
    {
      "epoch": 7.617066666666666,
      "grad_norm": 0.991500973701477,
      "learning_rate": 2.3933333333333334e-06,
      "loss": 0.0042,
      "step": 142820
    },
    {
      "epoch": 7.6176,
      "grad_norm": 0.056028302758932114,
      "learning_rate": 2.3900000000000004e-06,
      "loss": 0.0014,
      "step": 142830
    },
    {
      "epoch": 7.618133333333334,
      "grad_norm": 0.39396098256111145,
      "learning_rate": 2.386666666666667e-06,
      "loss": 0.003,
      "step": 142840
    },
    {
      "epoch": 7.618666666666667,
      "grad_norm": 0.44822704792022705,
      "learning_rate": 2.3833333333333335e-06,
      "loss": 0.0028,
      "step": 142850
    },
    {
      "epoch": 7.6192,
      "grad_norm": 0.05602863430976868,
      "learning_rate": 2.38e-06,
      "loss": 0.0017,
      "step": 142860
    },
    {
      "epoch": 7.6197333333333335,
      "grad_norm": 0.056028589606285095,
      "learning_rate": 2.3766666666666666e-06,
      "loss": 0.0018,
      "step": 142870
    },
    {
      "epoch": 7.620266666666667,
      "grad_norm": 0.11206745356321335,
      "learning_rate": 2.373333333333333e-06,
      "loss": 0.0025,
      "step": 142880
    },
    {
      "epoch": 7.6208,
      "grad_norm": 0.3361758887767792,
      "learning_rate": 2.37e-06,
      "loss": 0.0017,
      "step": 142890
    },
    {
      "epoch": 7.621333333333333,
      "grad_norm": 0.08404381573200226,
      "learning_rate": 2.3666666666666667e-06,
      "loss": 0.0032,
      "step": 142900
    },
    {
      "epoch": 7.621866666666667,
      "grad_norm": 0.028014687821269035,
      "learning_rate": 2.3633333333333333e-06,
      "loss": 0.0027,
      "step": 142910
    },
    {
      "epoch": 7.6224,
      "grad_norm": 0.25213372707366943,
      "learning_rate": 2.36e-06,
      "loss": 0.0031,
      "step": 142920
    },
    {
      "epoch": 7.622933333333333,
      "grad_norm": 0.28014522790908813,
      "learning_rate": 2.356666666666667e-06,
      "loss": 0.0026,
      "step": 142930
    },
    {
      "epoch": 7.623466666666666,
      "grad_norm": 0.11205616593360901,
      "learning_rate": 2.3533333333333334e-06,
      "loss": 0.0026,
      "step": 142940
    },
    {
      "epoch": 7.624,
      "grad_norm": 0.1680852770805359,
      "learning_rate": 2.35e-06,
      "loss": 0.0042,
      "step": 142950
    },
    {
      "epoch": 7.624533333333334,
      "grad_norm": 0.05602888762950897,
      "learning_rate": 2.346666666666667e-06,
      "loss": 0.0026,
      "step": 142960
    },
    {
      "epoch": 7.625066666666667,
      "grad_norm": 0.084043949842453,
      "learning_rate": 2.3433333333333335e-06,
      "loss": 0.0028,
      "step": 142970
    },
    {
      "epoch": 7.6256,
      "grad_norm": 0.05602914094924927,
      "learning_rate": 2.34e-06,
      "loss": 0.0023,
      "step": 142980
    },
    {
      "epoch": 7.626133333333334,
      "grad_norm": 0.028014322742819786,
      "learning_rate": 2.336666666666667e-06,
      "loss": 0.0032,
      "step": 142990
    },
    {
      "epoch": 7.626666666666667,
      "grad_norm": 0.1120569035410881,
      "learning_rate": 2.3333333333333336e-06,
      "loss": 0.0023,
      "step": 143000
    },
    {
      "epoch": 7.6272,
      "grad_norm": 3.1186262461346814e-09,
      "learning_rate": 2.33e-06,
      "loss": 0.0024,
      "step": 143010
    },
    {
      "epoch": 7.6277333333333335,
      "grad_norm": 0.25212913751602173,
      "learning_rate": 2.326666666666667e-06,
      "loss": 0.0017,
      "step": 143020
    },
    {
      "epoch": 7.628266666666667,
      "grad_norm": 0.22411441802978516,
      "learning_rate": 2.3233333333333337e-06,
      "loss": 0.003,
      "step": 143030
    },
    {
      "epoch": 7.6288,
      "grad_norm": 0.19610297679901123,
      "learning_rate": 2.32e-06,
      "loss": 0.0028,
      "step": 143040
    },
    {
      "epoch": 7.629333333333333,
      "grad_norm": 0.25213438272476196,
      "learning_rate": 2.316666666666667e-06,
      "loss": 0.0029,
      "step": 143050
    },
    {
      "epoch": 7.629866666666667,
      "grad_norm": 0.22411713004112244,
      "learning_rate": 2.3133333333333333e-06,
      "loss": 0.0023,
      "step": 143060
    },
    {
      "epoch": 7.6304,
      "grad_norm": 0.028013944625854492,
      "learning_rate": 2.31e-06,
      "loss": 0.0025,
      "step": 143070
    },
    {
      "epoch": 7.630933333333333,
      "grad_norm": 0.028013775125145912,
      "learning_rate": 2.306666666666667e-06,
      "loss": 0.0031,
      "step": 143080
    },
    {
      "epoch": 7.631466666666666,
      "grad_norm": 0.14006926119327545,
      "learning_rate": 2.3033333333333334e-06,
      "loss": 0.0022,
      "step": 143090
    },
    {
      "epoch": 7.632,
      "grad_norm": 0.0840415209531784,
      "learning_rate": 2.3e-06,
      "loss": 0.0029,
      "step": 143100
    },
    {
      "epoch": 7.632533333333333,
      "grad_norm": 0.252127081155777,
      "learning_rate": 2.2966666666666666e-06,
      "loss": 0.0028,
      "step": 143110
    },
    {
      "epoch": 7.633066666666666,
      "grad_norm": 0.14007145166397095,
      "learning_rate": 2.2933333333333335e-06,
      "loss": 0.0027,
      "step": 143120
    },
    {
      "epoch": 7.6336,
      "grad_norm": 0.056029196828603745,
      "learning_rate": 2.29e-06,
      "loss": 0.0028,
      "step": 143130
    },
    {
      "epoch": 7.634133333333334,
      "grad_norm": 0.14007355272769928,
      "learning_rate": 2.2866666666666667e-06,
      "loss": 0.0031,
      "step": 143140
    },
    {
      "epoch": 7.634666666666667,
      "grad_norm": 2.222071859847574e-09,
      "learning_rate": 2.2833333333333336e-06,
      "loss": 0.0035,
      "step": 143150
    },
    {
      "epoch": 7.6352,
      "grad_norm": 0.22411884367465973,
      "learning_rate": 2.28e-06,
      "loss": 0.0016,
      "step": 143160
    },
    {
      "epoch": 7.6357333333333335,
      "grad_norm": 0.6619241833686829,
      "learning_rate": 2.2766666666666668e-06,
      "loss": 0.0034,
      "step": 143170
    },
    {
      "epoch": 7.636266666666667,
      "grad_norm": 0.28014543652534485,
      "learning_rate": 2.2733333333333337e-06,
      "loss": 0.0032,
      "step": 143180
    },
    {
      "epoch": 7.6368,
      "grad_norm": 0.1400705873966217,
      "learning_rate": 2.2700000000000003e-06,
      "loss": 0.0029,
      "step": 143190
    },
    {
      "epoch": 7.637333333333333,
      "grad_norm": 0.11213121563196182,
      "learning_rate": 2.266666666666667e-06,
      "loss": 0.0025,
      "step": 143200
    },
    {
      "epoch": 7.637866666666667,
      "grad_norm": 0.05602768808603287,
      "learning_rate": 2.2633333333333334e-06,
      "loss": 0.0025,
      "step": 143210
    },
    {
      "epoch": 7.6384,
      "grad_norm": 0.507328450679779,
      "learning_rate": 2.26e-06,
      "loss": 0.0022,
      "step": 143220
    },
    {
      "epoch": 7.638933333333333,
      "grad_norm": 1.4891693433938258e-09,
      "learning_rate": 2.2566666666666665e-06,
      "loss": 0.002,
      "step": 143230
    },
    {
      "epoch": 7.639466666666666,
      "grad_norm": 6.684522380062674e-10,
      "learning_rate": 2.2533333333333335e-06,
      "loss": 0.0021,
      "step": 143240
    },
    {
      "epoch": 7.64,
      "grad_norm": 0.05602776259183884,
      "learning_rate": 2.25e-06,
      "loss": 0.0031,
      "step": 143250
    },
    {
      "epoch": 7.640533333333333,
      "grad_norm": 0.22411200404167175,
      "learning_rate": 2.2466666666666666e-06,
      "loss": 0.0022,
      "step": 143260
    },
    {
      "epoch": 7.641066666666667,
      "grad_norm": 0.5076266527175903,
      "learning_rate": 2.2433333333333336e-06,
      "loss": 0.003,
      "step": 143270
    },
    {
      "epoch": 7.6416,
      "grad_norm": 0.1400696486234665,
      "learning_rate": 2.24e-06,
      "loss": 0.0023,
      "step": 143280
    },
    {
      "epoch": 7.642133333333334,
      "grad_norm": 0.3361675441265106,
      "learning_rate": 2.2366666666666667e-06,
      "loss": 0.0021,
      "step": 143290
    },
    {
      "epoch": 7.642666666666667,
      "grad_norm": 0.056028079241514206,
      "learning_rate": 2.2333333333333333e-06,
      "loss": 0.0034,
      "step": 143300
    },
    {
      "epoch": 7.6432,
      "grad_norm": 0.08404302597045898,
      "learning_rate": 2.2300000000000002e-06,
      "loss": 0.0012,
      "step": 143310
    },
    {
      "epoch": 7.6437333333333335,
      "grad_norm": 0.14007212221622467,
      "learning_rate": 2.226666666666667e-06,
      "loss": 0.0028,
      "step": 143320
    },
    {
      "epoch": 7.644266666666667,
      "grad_norm": 0.19610044360160828,
      "learning_rate": 2.2233333333333334e-06,
      "loss": 0.0017,
      "step": 143330
    },
    {
      "epoch": 7.6448,
      "grad_norm": 0.5042529106140137,
      "learning_rate": 2.2200000000000003e-06,
      "loss": 0.0021,
      "step": 143340
    },
    {
      "epoch": 7.645333333333333,
      "grad_norm": 0.056028369814157486,
      "learning_rate": 2.216666666666667e-06,
      "loss": 0.0023,
      "step": 143350
    },
    {
      "epoch": 7.645866666666667,
      "grad_norm": 0.05602840706706047,
      "learning_rate": 2.2133333333333335e-06,
      "loss": 0.0019,
      "step": 143360
    },
    {
      "epoch": 7.6464,
      "grad_norm": 0.14007049798965454,
      "learning_rate": 2.2100000000000004e-06,
      "loss": 0.0021,
      "step": 143370
    },
    {
      "epoch": 7.646933333333333,
      "grad_norm": 0.22411121428012848,
      "learning_rate": 2.2066666666666666e-06,
      "loss": 0.0031,
      "step": 143380
    },
    {
      "epoch": 7.647466666666666,
      "grad_norm": 0.3081531226634979,
      "learning_rate": 2.203333333333333e-06,
      "loss": 0.0028,
      "step": 143390
    },
    {
      "epoch": 7.648,
      "grad_norm": 0.14006951451301575,
      "learning_rate": 2.2e-06,
      "loss": 0.0022,
      "step": 143400
    },
    {
      "epoch": 7.648533333333333,
      "grad_norm": 0.25212234258651733,
      "learning_rate": 2.1966666666666667e-06,
      "loss": 0.0018,
      "step": 143410
    },
    {
      "epoch": 7.649066666666666,
      "grad_norm": 2.6466049352080745e-09,
      "learning_rate": 2.1933333333333332e-06,
      "loss": 0.0041,
      "step": 143420
    },
    {
      "epoch": 7.6495999999999995,
      "grad_norm": 1.889198131266312e-09,
      "learning_rate": 2.19e-06,
      "loss": 0.0025,
      "step": 143430
    },
    {
      "epoch": 7.650133333333334,
      "grad_norm": 0.11205840110778809,
      "learning_rate": 2.1866666666666668e-06,
      "loss": 0.0024,
      "step": 143440
    },
    {
      "epoch": 7.650666666666667,
      "grad_norm": 0.02801474556326866,
      "learning_rate": 2.1833333333333333e-06,
      "loss": 0.0027,
      "step": 143450
    },
    {
      "epoch": 7.6512,
      "grad_norm": 0.2801455557346344,
      "learning_rate": 2.1800000000000003e-06,
      "loss": 0.0025,
      "step": 143460
    },
    {
      "epoch": 7.6517333333333335,
      "grad_norm": 0.05602826923131943,
      "learning_rate": 2.176666666666667e-06,
      "loss": 0.0015,
      "step": 143470
    },
    {
      "epoch": 7.652266666666667,
      "grad_norm": 0.0840420126914978,
      "learning_rate": 2.1733333333333334e-06,
      "loss": 0.0024,
      "step": 143480
    },
    {
      "epoch": 7.6528,
      "grad_norm": 0.16808339953422546,
      "learning_rate": 2.17e-06,
      "loss": 0.0025,
      "step": 143490
    },
    {
      "epoch": 7.653333333333333,
      "grad_norm": 0.19609996676445007,
      "learning_rate": 2.166666666666667e-06,
      "loss": 0.002,
      "step": 143500
    },
    {
      "epoch": 7.653866666666667,
      "grad_norm": 0.05603012070059776,
      "learning_rate": 2.1633333333333335e-06,
      "loss": 0.003,
      "step": 143510
    },
    {
      "epoch": 7.6544,
      "grad_norm": 0.11205603927373886,
      "learning_rate": 2.16e-06,
      "loss": 0.0029,
      "step": 143520
    },
    {
      "epoch": 7.654933333333333,
      "grad_norm": 0.1388874650001526,
      "learning_rate": 2.156666666666667e-06,
      "loss": 0.0026,
      "step": 143530
    },
    {
      "epoch": 7.655466666666666,
      "grad_norm": 0.11205785721540451,
      "learning_rate": 2.1533333333333336e-06,
      "loss": 0.0035,
      "step": 143540
    },
    {
      "epoch": 7.656,
      "grad_norm": 0.08404387533664703,
      "learning_rate": 2.1499999999999997e-06,
      "loss": 0.0027,
      "step": 143550
    },
    {
      "epoch": 7.656533333333333,
      "grad_norm": 0.28014761209487915,
      "learning_rate": 2.1466666666666667e-06,
      "loss": 0.0023,
      "step": 143560
    },
    {
      "epoch": 7.657066666666667,
      "grad_norm": 0.39220428466796875,
      "learning_rate": 2.1433333333333333e-06,
      "loss": 0.0029,
      "step": 143570
    },
    {
      "epoch": 7.6576,
      "grad_norm": 0.056028835475444794,
      "learning_rate": 2.14e-06,
      "loss": 0.0017,
      "step": 143580
    },
    {
      "epoch": 7.658133333333334,
      "grad_norm": 0.028013993054628372,
      "learning_rate": 2.136666666666667e-06,
      "loss": 0.0026,
      "step": 143590
    },
    {
      "epoch": 7.658666666666667,
      "grad_norm": 0.028013713657855988,
      "learning_rate": 2.1333333333333334e-06,
      "loss": 0.0032,
      "step": 143600
    },
    {
      "epoch": 7.6592,
      "grad_norm": 0.3081539273262024,
      "learning_rate": 2.13e-06,
      "loss": 0.0025,
      "step": 143610
    },
    {
      "epoch": 7.6597333333333335,
      "grad_norm": 0.02801409550011158,
      "learning_rate": 2.126666666666667e-06,
      "loss": 0.0024,
      "step": 143620
    },
    {
      "epoch": 7.660266666666667,
      "grad_norm": 0.1120564192533493,
      "learning_rate": 2.1233333333333335e-06,
      "loss": 0.0025,
      "step": 143630
    },
    {
      "epoch": 7.6608,
      "grad_norm": 0.028014054521918297,
      "learning_rate": 2.12e-06,
      "loss": 0.0024,
      "step": 143640
    },
    {
      "epoch": 7.661333333333333,
      "grad_norm": 0.05602842941880226,
      "learning_rate": 2.1166666666666666e-06,
      "loss": 0.0033,
      "step": 143650
    },
    {
      "epoch": 7.661866666666667,
      "grad_norm": 0.16808658838272095,
      "learning_rate": 2.1133333333333336e-06,
      "loss": 0.0031,
      "step": 143660
    },
    {
      "epoch": 7.6624,
      "grad_norm": 9.240466170012951e-10,
      "learning_rate": 2.11e-06,
      "loss": 0.0026,
      "step": 143670
    },
    {
      "epoch": 7.662933333333333,
      "grad_norm": 0.08404286205768585,
      "learning_rate": 2.1066666666666667e-06,
      "loss": 0.0032,
      "step": 143680
    },
    {
      "epoch": 7.663466666666666,
      "grad_norm": 0.11205661296844482,
      "learning_rate": 2.1033333333333337e-06,
      "loss": 0.0017,
      "step": 143690
    },
    {
      "epoch": 7.664,
      "grad_norm": 0.028014158830046654,
      "learning_rate": 2.1000000000000002e-06,
      "loss": 0.0031,
      "step": 143700
    },
    {
      "epoch": 7.664533333333333,
      "grad_norm": 0.08404278010129929,
      "learning_rate": 2.0966666666666668e-06,
      "loss": 0.0023,
      "step": 143710
    },
    {
      "epoch": 7.665066666666666,
      "grad_norm": 0.028014162555336952,
      "learning_rate": 2.0933333333333338e-06,
      "loss": 0.0035,
      "step": 143720
    },
    {
      "epoch": 7.6655999999999995,
      "grad_norm": 0.2521299421787262,
      "learning_rate": 2.09e-06,
      "loss": 0.0034,
      "step": 143730
    },
    {
      "epoch": 7.666133333333334,
      "grad_norm": 0.19610266387462616,
      "learning_rate": 2.0866666666666665e-06,
      "loss": 0.0029,
      "step": 143740
    },
    {
      "epoch": 7.666666666666667,
      "grad_norm": 0.028014564886689186,
      "learning_rate": 2.0833333333333334e-06,
      "loss": 0.0026,
      "step": 143750
    },
    {
      "epoch": 7.6672,
      "grad_norm": 0.2521303594112396,
      "learning_rate": 2.08e-06,
      "loss": 0.0027,
      "step": 143760
    },
    {
      "epoch": 7.6677333333333335,
      "grad_norm": 0.028014278039336205,
      "learning_rate": 2.0766666666666665e-06,
      "loss": 0.0034,
      "step": 143770
    },
    {
      "epoch": 7.668266666666667,
      "grad_norm": 0.08404190093278885,
      "learning_rate": 2.0733333333333335e-06,
      "loss": 0.0037,
      "step": 143780
    },
    {
      "epoch": 7.6688,
      "grad_norm": 0.08404171466827393,
      "learning_rate": 2.07e-06,
      "loss": 0.0019,
      "step": 143790
    },
    {
      "epoch": 7.669333333333333,
      "grad_norm": 0.11205600947141647,
      "learning_rate": 2.0666666666666666e-06,
      "loss": 0.0024,
      "step": 143800
    },
    {
      "epoch": 7.669866666666667,
      "grad_norm": 0.11206026375293732,
      "learning_rate": 2.0633333333333336e-06,
      "loss": 0.0025,
      "step": 143810
    },
    {
      "epoch": 7.6704,
      "grad_norm": 0.33617183566093445,
      "learning_rate": 2.06e-06,
      "loss": 0.0014,
      "step": 143820
    },
    {
      "epoch": 7.670933333333333,
      "grad_norm": 0.168082132935524,
      "learning_rate": 2.0566666666666667e-06,
      "loss": 0.0027,
      "step": 143830
    },
    {
      "epoch": 7.671466666666666,
      "grad_norm": 0.22411277890205383,
      "learning_rate": 2.0533333333333333e-06,
      "loss": 0.0027,
      "step": 143840
    },
    {
      "epoch": 7.672,
      "grad_norm": 0.08404307067394257,
      "learning_rate": 2.0500000000000003e-06,
      "loss": 0.003,
      "step": 143850
    },
    {
      "epoch": 7.672533333333333,
      "grad_norm": 0.05602853372693062,
      "learning_rate": 2.046666666666667e-06,
      "loss": 0.0016,
      "step": 143860
    },
    {
      "epoch": 7.673066666666667,
      "grad_norm": 0.16808539628982544,
      "learning_rate": 2.0433333333333334e-06,
      "loss": 0.0018,
      "step": 143870
    },
    {
      "epoch": 7.6736,
      "grad_norm": 0.08404306322336197,
      "learning_rate": 2.0400000000000004e-06,
      "loss": 0.0023,
      "step": 143880
    },
    {
      "epoch": 7.674133333333334,
      "grad_norm": 0.056028977036476135,
      "learning_rate": 2.036666666666667e-06,
      "loss": 0.0024,
      "step": 143890
    },
    {
      "epoch": 7.674666666666667,
      "grad_norm": 0.08404528349637985,
      "learning_rate": 2.033333333333333e-06,
      "loss": 0.0019,
      "step": 143900
    },
    {
      "epoch": 7.6752,
      "grad_norm": 0.1680886298418045,
      "learning_rate": 2.03e-06,
      "loss": 0.0028,
      "step": 143910
    },
    {
      "epoch": 7.6757333333333335,
      "grad_norm": 0.4998907446861267,
      "learning_rate": 2.0266666666666666e-06,
      "loss": 0.0036,
      "step": 143920
    },
    {
      "epoch": 7.676266666666667,
      "grad_norm": 0.056031085550785065,
      "learning_rate": 2.023333333333333e-06,
      "loss": 0.0029,
      "step": 143930
    },
    {
      "epoch": 7.6768,
      "grad_norm": 0.08404435217380524,
      "learning_rate": 2.02e-06,
      "loss": 0.0029,
      "step": 143940
    },
    {
      "epoch": 7.677333333333333,
      "grad_norm": 0.08404356986284256,
      "learning_rate": 2.0166666666666667e-06,
      "loss": 0.0029,
      "step": 143950
    },
    {
      "epoch": 7.677866666666667,
      "grad_norm": 0.0840430036187172,
      "learning_rate": 2.0133333333333333e-06,
      "loss": 0.0022,
      "step": 143960
    },
    {
      "epoch": 7.6784,
      "grad_norm": 0.22411277890205383,
      "learning_rate": 2.0100000000000002e-06,
      "loss": 0.0027,
      "step": 143970
    },
    {
      "epoch": 7.678933333333333,
      "grad_norm": 0.028014199808239937,
      "learning_rate": 2.006666666666667e-06,
      "loss": 0.0016,
      "step": 143980
    },
    {
      "epoch": 7.679466666666666,
      "grad_norm": 0.028014380484819412,
      "learning_rate": 2.0033333333333334e-06,
      "loss": 0.002,
      "step": 143990
    },
    {
      "epoch": 7.68,
      "grad_norm": 0.48348039388656616,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.0014,
      "step": 144000
    },
    {
      "epoch": 7.680533333333333,
      "grad_norm": 0.30815622210502625,
      "learning_rate": 1.996666666666667e-06,
      "loss": 0.0023,
      "step": 144010
    },
    {
      "epoch": 7.681066666666666,
      "grad_norm": 0.3081558346748352,
      "learning_rate": 1.9933333333333334e-06,
      "loss": 0.0019,
      "step": 144020
    },
    {
      "epoch": 7.6815999999999995,
      "grad_norm": 0.11205749958753586,
      "learning_rate": 1.99e-06,
      "loss": 0.0037,
      "step": 144030
    },
    {
      "epoch": 7.682133333333334,
      "grad_norm": 0.08404345065355301,
      "learning_rate": 1.986666666666667e-06,
      "loss": 0.0029,
      "step": 144040
    },
    {
      "epoch": 7.682666666666667,
      "grad_norm": 0.028014544397592545,
      "learning_rate": 1.9833333333333335e-06,
      "loss": 0.0022,
      "step": 144050
    },
    {
      "epoch": 7.6832,
      "grad_norm": 0.11205803602933884,
      "learning_rate": 1.98e-06,
      "loss": 0.0021,
      "step": 144060
    },
    {
      "epoch": 7.6837333333333335,
      "grad_norm": 2.7455206996762627e-09,
      "learning_rate": 1.9766666666666667e-06,
      "loss": 0.0029,
      "step": 144070
    },
    {
      "epoch": 7.684266666666667,
      "grad_norm": 0.19632631540298462,
      "learning_rate": 1.9733333333333332e-06,
      "loss": 0.0035,
      "step": 144080
    },
    {
      "epoch": 7.6848,
      "grad_norm": 0.05602819845080376,
      "learning_rate": 1.9699999999999998e-06,
      "loss": 0.0025,
      "step": 144090
    },
    {
      "epoch": 7.685333333333333,
      "grad_norm": 0.14007127285003662,
      "learning_rate": 1.9666666666666668e-06,
      "loss": 0.0017,
      "step": 144100
    },
    {
      "epoch": 7.685866666666667,
      "grad_norm": 0.05602915212512016,
      "learning_rate": 1.9633333333333333e-06,
      "loss": 0.0019,
      "step": 144110
    },
    {
      "epoch": 7.6864,
      "grad_norm": 0.19610369205474854,
      "learning_rate": 1.96e-06,
      "loss": 0.0023,
      "step": 144120
    },
    {
      "epoch": 7.686933333333333,
      "grad_norm": 0.028014883399009705,
      "learning_rate": 1.956666666666667e-06,
      "loss": 0.0029,
      "step": 144130
    },
    {
      "epoch": 7.6874666666666664,
      "grad_norm": 0.44823768734931946,
      "learning_rate": 1.9533333333333334e-06,
      "loss": 0.0023,
      "step": 144140
    },
    {
      "epoch": 7.688,
      "grad_norm": 0.061326682567596436,
      "learning_rate": 1.95e-06,
      "loss": 0.0035,
      "step": 144150
    },
    {
      "epoch": 7.688533333333333,
      "grad_norm": 0.3366307020187378,
      "learning_rate": 1.946666666666667e-06,
      "loss": 0.0019,
      "step": 144160
    },
    {
      "epoch": 7.689066666666667,
      "grad_norm": 0.19636908173561096,
      "learning_rate": 1.9433333333333335e-06,
      "loss": 0.0037,
      "step": 144170
    },
    {
      "epoch": 7.6896,
      "grad_norm": 0.08469045907258987,
      "learning_rate": 1.94e-06,
      "loss": 0.0031,
      "step": 144180
    },
    {
      "epoch": 7.690133333333334,
      "grad_norm": 0.02808130718767643,
      "learning_rate": 1.936666666666667e-06,
      "loss": 0.0035,
      "step": 144190
    },
    {
      "epoch": 7.690666666666667,
      "grad_norm": 0.05602812021970749,
      "learning_rate": 1.9333333333333336e-06,
      "loss": 0.0025,
      "step": 144200
    },
    {
      "epoch": 7.6912,
      "grad_norm": 0.16808387637138367,
      "learning_rate": 1.93e-06,
      "loss": 0.0022,
      "step": 144210
    },
    {
      "epoch": 7.6917333333333335,
      "grad_norm": 0.168084979057312,
      "learning_rate": 1.9266666666666667e-06,
      "loss": 0.0021,
      "step": 144220
    },
    {
      "epoch": 7.692266666666667,
      "grad_norm": 0.19610027968883514,
      "learning_rate": 1.9233333333333337e-06,
      "loss": 0.0017,
      "step": 144230
    },
    {
      "epoch": 7.6928,
      "grad_norm": 0.5042598843574524,
      "learning_rate": 1.92e-06,
      "loss": 0.0023,
      "step": 144240
    },
    {
      "epoch": 7.693333333333333,
      "grad_norm": 2.3160136031208367e-09,
      "learning_rate": 1.916666666666667e-06,
      "loss": 0.0037,
      "step": 144250
    },
    {
      "epoch": 7.693866666666667,
      "grad_norm": 0.05602855980396271,
      "learning_rate": 1.9133333333333334e-06,
      "loss": 0.0026,
      "step": 144260
    },
    {
      "epoch": 7.6944,
      "grad_norm": 0.22414977848529816,
      "learning_rate": 1.91e-06,
      "loss": 0.0021,
      "step": 144270
    },
    {
      "epoch": 7.694933333333333,
      "grad_norm": 0.2801399230957031,
      "learning_rate": 1.9066666666666667e-06,
      "loss": 0.0023,
      "step": 144280
    },
    {
      "epoch": 7.6954666666666665,
      "grad_norm": 0.28014200925827026,
      "learning_rate": 1.9033333333333335e-06,
      "loss": 0.0034,
      "step": 144290
    },
    {
      "epoch": 7.696,
      "grad_norm": 0.028014183044433594,
      "learning_rate": 1.9e-06,
      "loss": 0.0026,
      "step": 144300
    },
    {
      "epoch": 7.696533333333333,
      "grad_norm": 0.28014320135116577,
      "learning_rate": 1.8966666666666668e-06,
      "loss": 0.0027,
      "step": 144310
    },
    {
      "epoch": 7.697066666666666,
      "grad_norm": 0.14007212221622467,
      "learning_rate": 1.8933333333333333e-06,
      "loss": 0.0034,
      "step": 144320
    },
    {
      "epoch": 7.6975999999999996,
      "grad_norm": 0.19610272347927094,
      "learning_rate": 1.8900000000000001e-06,
      "loss": 0.0019,
      "step": 144330
    },
    {
      "epoch": 7.698133333333334,
      "grad_norm": 4.019188093451476e-09,
      "learning_rate": 1.8866666666666669e-06,
      "loss": 0.002,
      "step": 144340
    },
    {
      "epoch": 7.698666666666667,
      "grad_norm": 0.11205907166004181,
      "learning_rate": 1.8833333333333334e-06,
      "loss": 0.0033,
      "step": 144350
    },
    {
      "epoch": 7.6992,
      "grad_norm": 5.2570268138651954e-09,
      "learning_rate": 1.8800000000000002e-06,
      "loss": 0.0019,
      "step": 144360
    },
    {
      "epoch": 7.6997333333333335,
      "grad_norm": 0.028014304116368294,
      "learning_rate": 1.8766666666666668e-06,
      "loss": 0.0022,
      "step": 144370
    },
    {
      "epoch": 7.700266666666667,
      "grad_norm": 0.07068178057670593,
      "learning_rate": 1.8733333333333335e-06,
      "loss": 0.003,
      "step": 144380
    },
    {
      "epoch": 7.7008,
      "grad_norm": 0.02801450341939926,
      "learning_rate": 1.8700000000000003e-06,
      "loss": 0.003,
      "step": 144390
    },
    {
      "epoch": 7.701333333333333,
      "grad_norm": 0.08404377847909927,
      "learning_rate": 1.8666666666666669e-06,
      "loss": 0.0028,
      "step": 144400
    },
    {
      "epoch": 7.701866666666667,
      "grad_norm": 0.3081594705581665,
      "learning_rate": 1.8633333333333332e-06,
      "loss": 0.003,
      "step": 144410
    },
    {
      "epoch": 7.7024,
      "grad_norm": 0.1680879145860672,
      "learning_rate": 1.86e-06,
      "loss": 0.002,
      "step": 144420
    },
    {
      "epoch": 7.702933333333333,
      "grad_norm": 0.11205974966287613,
      "learning_rate": 1.8566666666666665e-06,
      "loss": 0.0024,
      "step": 144430
    },
    {
      "epoch": 7.7034666666666665,
      "grad_norm": 0.02801494114100933,
      "learning_rate": 1.8533333333333333e-06,
      "loss": 0.0039,
      "step": 144440
    },
    {
      "epoch": 7.704,
      "grad_norm": 0.05602971836924553,
      "learning_rate": 1.85e-06,
      "loss": 0.0016,
      "step": 144450
    },
    {
      "epoch": 7.704533333333333,
      "grad_norm": 0.19610512256622314,
      "learning_rate": 1.8466666666666666e-06,
      "loss": 0.0021,
      "step": 144460
    },
    {
      "epoch": 7.705066666666666,
      "grad_norm": 0.14019647240638733,
      "learning_rate": 1.8433333333333334e-06,
      "loss": 0.0033,
      "step": 144470
    },
    {
      "epoch": 7.7056000000000004,
      "grad_norm": 0.08404473215341568,
      "learning_rate": 1.84e-06,
      "loss": 0.0019,
      "step": 144480
    },
    {
      "epoch": 7.706133333333334,
      "grad_norm": 0.0280146487057209,
      "learning_rate": 1.8366666666666667e-06,
      "loss": 0.0028,
      "step": 144490
    },
    {
      "epoch": 7.706666666666667,
      "grad_norm": 0.22411580383777618,
      "learning_rate": 1.8333333333333335e-06,
      "loss": 0.0031,
      "step": 144500
    },
    {
      "epoch": 7.7072,
      "grad_norm": 0.5086432099342346,
      "learning_rate": 1.83e-06,
      "loss": 0.0023,
      "step": 144510
    },
    {
      "epoch": 7.7077333333333335,
      "grad_norm": 1.6104635403024758e-09,
      "learning_rate": 1.8266666666666668e-06,
      "loss": 0.0032,
      "step": 144520
    },
    {
      "epoch": 7.708266666666667,
      "grad_norm": 0.30816036462783813,
      "learning_rate": 1.8233333333333336e-06,
      "loss": 0.0031,
      "step": 144530
    },
    {
      "epoch": 7.7088,
      "grad_norm": 0.11205802857875824,
      "learning_rate": 1.8200000000000002e-06,
      "loss": 0.0021,
      "step": 144540
    },
    {
      "epoch": 7.709333333333333,
      "grad_norm": 0.056028470396995544,
      "learning_rate": 1.816666666666667e-06,
      "loss": 0.0031,
      "step": 144550
    },
    {
      "epoch": 7.709866666666667,
      "grad_norm": 0.1960984766483307,
      "learning_rate": 1.8133333333333335e-06,
      "loss": 0.0023,
      "step": 144560
    },
    {
      "epoch": 7.7104,
      "grad_norm": 0.08404212445020676,
      "learning_rate": 1.8100000000000002e-06,
      "loss": 0.0023,
      "step": 144570
    },
    {
      "epoch": 7.710933333333333,
      "grad_norm": 0.05602800101041794,
      "learning_rate": 1.806666666666667e-06,
      "loss": 0.0031,
      "step": 144580
    },
    {
      "epoch": 7.7114666666666665,
      "grad_norm": 0.3082023859024048,
      "learning_rate": 1.8033333333333334e-06,
      "loss": 0.0022,
      "step": 144590
    },
    {
      "epoch": 7.712,
      "grad_norm": 0.08404333889484406,
      "learning_rate": 1.8e-06,
      "loss": 0.003,
      "step": 144600
    },
    {
      "epoch": 7.712533333333333,
      "grad_norm": 2.23858132031296e-09,
      "learning_rate": 1.7966666666666667e-06,
      "loss": 0.0021,
      "step": 144610
    },
    {
      "epoch": 7.713066666666666,
      "grad_norm": 0.02801453322172165,
      "learning_rate": 1.7933333333333332e-06,
      "loss": 0.0031,
      "step": 144620
    },
    {
      "epoch": 7.7136,
      "grad_norm": 0.08404318988323212,
      "learning_rate": 1.79e-06,
      "loss": 0.0026,
      "step": 144630
    },
    {
      "epoch": 7.714133333333333,
      "grad_norm": 0.11205670982599258,
      "learning_rate": 1.7866666666666668e-06,
      "loss": 0.0028,
      "step": 144640
    },
    {
      "epoch": 7.714666666666667,
      "grad_norm": 0.05602841451764107,
      "learning_rate": 1.7833333333333333e-06,
      "loss": 0.0026,
      "step": 144650
    },
    {
      "epoch": 7.7152,
      "grad_norm": 2.010657196294119e-09,
      "learning_rate": 1.7800000000000001e-06,
      "loss": 0.0028,
      "step": 144660
    },
    {
      "epoch": 7.7157333333333336,
      "grad_norm": 0.3081616461277008,
      "learning_rate": 1.7766666666666667e-06,
      "loss": 0.0036,
      "step": 144670
    },
    {
      "epoch": 7.716266666666667,
      "grad_norm": 0.08465699106454849,
      "learning_rate": 1.7733333333333334e-06,
      "loss": 0.0016,
      "step": 144680
    },
    {
      "epoch": 7.7168,
      "grad_norm": 0.11205734312534332,
      "learning_rate": 1.7700000000000002e-06,
      "loss": 0.0019,
      "step": 144690
    },
    {
      "epoch": 7.717333333333333,
      "grad_norm": 0.08404281735420227,
      "learning_rate": 1.7666666666666668e-06,
      "loss": 0.0031,
      "step": 144700
    },
    {
      "epoch": 7.717866666666667,
      "grad_norm": 0.02801421284675598,
      "learning_rate": 1.7633333333333335e-06,
      "loss": 0.0034,
      "step": 144710
    },
    {
      "epoch": 7.7184,
      "grad_norm": 0.14007145166397095,
      "learning_rate": 1.76e-06,
      "loss": 0.0021,
      "step": 144720
    },
    {
      "epoch": 7.718933333333333,
      "grad_norm": 0.057076502591371536,
      "learning_rate": 1.7566666666666669e-06,
      "loss": 0.0034,
      "step": 144730
    },
    {
      "epoch": 7.7194666666666665,
      "grad_norm": 0.08505173772573471,
      "learning_rate": 1.7533333333333336e-06,
      "loss": 0.0039,
      "step": 144740
    },
    {
      "epoch": 7.72,
      "grad_norm": 0.16811877489089966,
      "learning_rate": 1.7500000000000002e-06,
      "loss": 0.0018,
      "step": 144750
    },
    {
      "epoch": 7.720533333333333,
      "grad_norm": 0.22417089343070984,
      "learning_rate": 1.7466666666666665e-06,
      "loss": 0.0026,
      "step": 144760
    },
    {
      "epoch": 7.721066666666666,
      "grad_norm": 0.14007852971553802,
      "learning_rate": 1.7433333333333333e-06,
      "loss": 0.0019,
      "step": 144770
    },
    {
      "epoch": 7.7216000000000005,
      "grad_norm": 0.056074026972055435,
      "learning_rate": 1.7399999999999999e-06,
      "loss": 0.0032,
      "step": 144780
    },
    {
      "epoch": 7.722133333333334,
      "grad_norm": 0.16826578974723816,
      "learning_rate": 1.7366666666666666e-06,
      "loss": 0.0038,
      "step": 144790
    },
    {
      "epoch": 7.722666666666667,
      "grad_norm": 0.0857849046587944,
      "learning_rate": 1.7333333333333334e-06,
      "loss": 0.0027,
      "step": 144800
    },
    {
      "epoch": 7.7232,
      "grad_norm": 0.05602950602769852,
      "learning_rate": 1.73e-06,
      "loss": 0.0026,
      "step": 144810
    },
    {
      "epoch": 7.723733333333334,
      "grad_norm": 2.890605310668093e-09,
      "learning_rate": 1.7266666666666667e-06,
      "loss": 0.002,
      "step": 144820
    },
    {
      "epoch": 7.724266666666667,
      "grad_norm": 0.22411878407001495,
      "learning_rate": 1.7233333333333335e-06,
      "loss": 0.002,
      "step": 144830
    },
    {
      "epoch": 7.7248,
      "grad_norm": 0.05609482154250145,
      "learning_rate": 1.72e-06,
      "loss": 0.0017,
      "step": 144840
    },
    {
      "epoch": 7.725333333333333,
      "grad_norm": 0.19608989357948303,
      "learning_rate": 1.7166666666666668e-06,
      "loss": 0.0022,
      "step": 144850
    },
    {
      "epoch": 7.725866666666667,
      "grad_norm": 0.26439186930656433,
      "learning_rate": 1.7133333333333334e-06,
      "loss": 0.0018,
      "step": 144860
    },
    {
      "epoch": 7.7264,
      "grad_norm": 0.05910467356443405,
      "learning_rate": 1.7100000000000001e-06,
      "loss": 0.0022,
      "step": 144870
    },
    {
      "epoch": 7.726933333333333,
      "grad_norm": 0.6009305715560913,
      "learning_rate": 1.706666666666667e-06,
      "loss": 0.0026,
      "step": 144880
    },
    {
      "epoch": 7.7274666666666665,
      "grad_norm": 0.22432437539100647,
      "learning_rate": 1.7033333333333335e-06,
      "loss": 0.0029,
      "step": 144890
    },
    {
      "epoch": 7.728,
      "grad_norm": 0.02972187101840973,
      "learning_rate": 1.7000000000000002e-06,
      "loss": 0.0018,
      "step": 144900
    },
    {
      "epoch": 7.728533333333333,
      "grad_norm": 0.11205931007862091,
      "learning_rate": 1.6966666666666668e-06,
      "loss": 0.0023,
      "step": 144910
    },
    {
      "epoch": 7.729066666666666,
      "grad_norm": 0.2521342635154724,
      "learning_rate": 1.6933333333333336e-06,
      "loss": 0.0031,
      "step": 144920
    },
    {
      "epoch": 7.7296,
      "grad_norm": 9.199160877493284e-10,
      "learning_rate": 1.69e-06,
      "loss": 0.002,
      "step": 144930
    },
    {
      "epoch": 7.730133333333333,
      "grad_norm": 0.19139915704727173,
      "learning_rate": 1.6866666666666667e-06,
      "loss": 0.0021,
      "step": 144940
    },
    {
      "epoch": 7.730666666666667,
      "grad_norm": 0.4610968828201294,
      "learning_rate": 1.6833333333333332e-06,
      "loss": 0.0026,
      "step": 144950
    },
    {
      "epoch": 7.7312,
      "grad_norm": 0.19609978795051575,
      "learning_rate": 1.68e-06,
      "loss": 0.0025,
      "step": 144960
    },
    {
      "epoch": 7.731733333333334,
      "grad_norm": 0.2801460027694702,
      "learning_rate": 1.6766666666666666e-06,
      "loss": 0.0033,
      "step": 144970
    },
    {
      "epoch": 7.732266666666667,
      "grad_norm": 0.02801457606256008,
      "learning_rate": 1.6733333333333333e-06,
      "loss": 0.0023,
      "step": 144980
    },
    {
      "epoch": 7.7328,
      "grad_norm": 0.23424352705478668,
      "learning_rate": 1.67e-06,
      "loss": 0.003,
      "step": 144990
    },
    {
      "epoch": 7.733333333333333,
      "grad_norm": 2.7346787057069832e-09,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 0.0028,
      "step": 145000
    },
    {
      "epoch": 7.733866666666667,
      "grad_norm": 6.067659708008932e-09,
      "learning_rate": 1.6633333333333334e-06,
      "loss": 0.0027,
      "step": 145010
    },
    {
      "epoch": 7.7344,
      "grad_norm": 0.2801457941532135,
      "learning_rate": 1.6600000000000002e-06,
      "loss": 0.0022,
      "step": 145020
    },
    {
      "epoch": 7.734933333333333,
      "grad_norm": 0.4650596082210541,
      "learning_rate": 1.6566666666666668e-06,
      "loss": 0.0029,
      "step": 145030
    },
    {
      "epoch": 7.7354666666666665,
      "grad_norm": 0.2521323561668396,
      "learning_rate": 1.6533333333333335e-06,
      "loss": 0.0024,
      "step": 145040
    },
    {
      "epoch": 7.736,
      "grad_norm": 0.08404356986284256,
      "learning_rate": 1.65e-06,
      "loss": 0.0027,
      "step": 145050
    },
    {
      "epoch": 7.736533333333333,
      "grad_norm": 2.9768290055187663e-09,
      "learning_rate": 1.6466666666666669e-06,
      "loss": 0.0027,
      "step": 145060
    },
    {
      "epoch": 7.737066666666666,
      "grad_norm": 0.14014127850532532,
      "learning_rate": 1.6433333333333336e-06,
      "loss": 0.0026,
      "step": 145070
    },
    {
      "epoch": 7.7376000000000005,
      "grad_norm": 0.14007143676280975,
      "learning_rate": 1.6400000000000002e-06,
      "loss": 0.002,
      "step": 145080
    },
    {
      "epoch": 7.738133333333334,
      "grad_norm": 0.2521277964115143,
      "learning_rate": 1.636666666666667e-06,
      "loss": 0.0022,
      "step": 145090
    },
    {
      "epoch": 7.738666666666667,
      "grad_norm": 0.05602877587080002,
      "learning_rate": 1.6333333333333333e-06,
      "loss": 0.0033,
      "step": 145100
    },
    {
      "epoch": 7.7392,
      "grad_norm": 0.08405302464962006,
      "learning_rate": 1.6299999999999999e-06,
      "loss": 0.0025,
      "step": 145110
    },
    {
      "epoch": 7.739733333333334,
      "grad_norm": 0.0280145313590765,
      "learning_rate": 1.6266666666666666e-06,
      "loss": 0.002,
      "step": 145120
    },
    {
      "epoch": 7.740266666666667,
      "grad_norm": 0.16808824241161346,
      "learning_rate": 1.6233333333333334e-06,
      "loss": 0.0017,
      "step": 145130
    },
    {
      "epoch": 7.7408,
      "grad_norm": 0.1680901050567627,
      "learning_rate": 1.62e-06,
      "loss": 0.0024,
      "step": 145140
    },
    {
      "epoch": 7.741333333333333,
      "grad_norm": 0.07882839441299438,
      "learning_rate": 1.6166666666666667e-06,
      "loss": 0.002,
      "step": 145150
    },
    {
      "epoch": 7.741866666666667,
      "grad_norm": 0.40025246143341064,
      "learning_rate": 1.6133333333333333e-06,
      "loss": 0.0022,
      "step": 145160
    },
    {
      "epoch": 7.7424,
      "grad_norm": 0.05603937804698944,
      "learning_rate": 1.61e-06,
      "loss": 0.0029,
      "step": 145170
    },
    {
      "epoch": 7.742933333333333,
      "grad_norm": 0.1680888831615448,
      "learning_rate": 1.6066666666666668e-06,
      "loss": 0.0026,
      "step": 145180
    },
    {
      "epoch": 7.7434666666666665,
      "grad_norm": 0.2521325647830963,
      "learning_rate": 1.6033333333333334e-06,
      "loss": 0.0035,
      "step": 145190
    },
    {
      "epoch": 7.744,
      "grad_norm": 0.19090326130390167,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 0.0019,
      "step": 145200
    },
    {
      "epoch": 7.744533333333333,
      "grad_norm": 0.11205743253231049,
      "learning_rate": 1.5966666666666667e-06,
      "loss": 0.0034,
      "step": 145210
    },
    {
      "epoch": 7.745066666666666,
      "grad_norm": 0.22411414980888367,
      "learning_rate": 1.5933333333333335e-06,
      "loss": 0.0033,
      "step": 145220
    },
    {
      "epoch": 7.7456,
      "grad_norm": 0.11205722391605377,
      "learning_rate": 1.5900000000000002e-06,
      "loss": 0.0024,
      "step": 145230
    },
    {
      "epoch": 7.746133333333333,
      "grad_norm": 0.2801438570022583,
      "learning_rate": 1.5866666666666668e-06,
      "loss": 0.0022,
      "step": 145240
    },
    {
      "epoch": 7.746666666666667,
      "grad_norm": 0.2801460325717926,
      "learning_rate": 1.5833333333333336e-06,
      "loss": 0.0039,
      "step": 145250
    },
    {
      "epoch": 7.7472,
      "grad_norm": 0.28014683723449707,
      "learning_rate": 1.5800000000000003e-06,
      "loss": 0.0026,
      "step": 145260
    },
    {
      "epoch": 7.747733333333334,
      "grad_norm": 0.11205802112817764,
      "learning_rate": 1.5766666666666665e-06,
      "loss": 0.0027,
      "step": 145270
    },
    {
      "epoch": 7.748266666666667,
      "grad_norm": 0.16808676719665527,
      "learning_rate": 1.5733333333333332e-06,
      "loss": 0.0036,
      "step": 145280
    },
    {
      "epoch": 7.7488,
      "grad_norm": 0.1681397706270218,
      "learning_rate": 1.57e-06,
      "loss": 0.0031,
      "step": 145290
    },
    {
      "epoch": 7.749333333333333,
      "grad_norm": 0.028014490380883217,
      "learning_rate": 1.5666666666666666e-06,
      "loss": 0.0026,
      "step": 145300
    },
    {
      "epoch": 7.749866666666667,
      "grad_norm": 0.1400720626115799,
      "learning_rate": 1.5633333333333333e-06,
      "loss": 0.0019,
      "step": 145310
    },
    {
      "epoch": 7.7504,
      "grad_norm": 0.47624993324279785,
      "learning_rate": 1.56e-06,
      "loss": 0.0025,
      "step": 145320
    },
    {
      "epoch": 7.750933333333333,
      "grad_norm": 0.11205720901489258,
      "learning_rate": 1.5566666666666667e-06,
      "loss": 0.0012,
      "step": 145330
    },
    {
      "epoch": 7.7514666666666665,
      "grad_norm": 0.14007125794887543,
      "learning_rate": 1.5533333333333334e-06,
      "loss": 0.0027,
      "step": 145340
    },
    {
      "epoch": 7.752,
      "grad_norm": 0.280144602060318,
      "learning_rate": 1.55e-06,
      "loss": 0.0037,
      "step": 145350
    },
    {
      "epoch": 7.752533333333333,
      "grad_norm": 0.4202180504798889,
      "learning_rate": 1.5466666666666668e-06,
      "loss": 0.0025,
      "step": 145360
    },
    {
      "epoch": 7.753066666666666,
      "grad_norm": 0.14007295668125153,
      "learning_rate": 1.5433333333333335e-06,
      "loss": 0.0029,
      "step": 145370
    },
    {
      "epoch": 7.7536000000000005,
      "grad_norm": 0.11205817759037018,
      "learning_rate": 1.54e-06,
      "loss": 0.003,
      "step": 145380
    },
    {
      "epoch": 7.754133333333334,
      "grad_norm": 0.02801445871591568,
      "learning_rate": 1.5366666666666668e-06,
      "loss": 0.0019,
      "step": 145390
    },
    {
      "epoch": 7.754666666666667,
      "grad_norm": 0.08404326438903809,
      "learning_rate": 1.5333333333333334e-06,
      "loss": 0.002,
      "step": 145400
    },
    {
      "epoch": 7.7552,
      "grad_norm": 0.1680866777896881,
      "learning_rate": 1.53e-06,
      "loss": 0.003,
      "step": 145410
    },
    {
      "epoch": 7.755733333333334,
      "grad_norm": 0.22411532700061798,
      "learning_rate": 1.5266666666666667e-06,
      "loss": 0.0026,
      "step": 145420
    },
    {
      "epoch": 7.756266666666667,
      "grad_norm": 0.1400732547044754,
      "learning_rate": 1.5233333333333333e-06,
      "loss": 0.003,
      "step": 145430
    },
    {
      "epoch": 7.7568,
      "grad_norm": 0.06673044711351395,
      "learning_rate": 1.52e-06,
      "loss": 0.0017,
      "step": 145440
    },
    {
      "epoch": 7.757333333333333,
      "grad_norm": 0.39220118522644043,
      "learning_rate": 1.5166666666666668e-06,
      "loss": 0.0033,
      "step": 145450
    },
    {
      "epoch": 7.757866666666667,
      "grad_norm": 0.1400715559720993,
      "learning_rate": 1.5133333333333334e-06,
      "loss": 0.0029,
      "step": 145460
    },
    {
      "epoch": 7.7584,
      "grad_norm": 0.08404317498207092,
      "learning_rate": 1.5100000000000002e-06,
      "loss": 0.0015,
      "step": 145470
    },
    {
      "epoch": 7.758933333333333,
      "grad_norm": 0.11205797642469406,
      "learning_rate": 1.5066666666666667e-06,
      "loss": 0.0033,
      "step": 145480
    },
    {
      "epoch": 7.7594666666666665,
      "grad_norm": 0.09548576921224594,
      "learning_rate": 1.5033333333333333e-06,
      "loss": 0.0028,
      "step": 145490
    },
    {
      "epoch": 7.76,
      "grad_norm": 0.02801462449133396,
      "learning_rate": 1.5e-06,
      "loss": 0.0022,
      "step": 145500
    },
    {
      "epoch": 7.760533333333333,
      "grad_norm": 0.0280146524310112,
      "learning_rate": 1.4966666666666668e-06,
      "loss": 0.0032,
      "step": 145510
    },
    {
      "epoch": 7.761066666666666,
      "grad_norm": 1.4539617287923079e-09,
      "learning_rate": 1.4933333333333334e-06,
      "loss": 0.0028,
      "step": 145520
    },
    {
      "epoch": 7.7616,
      "grad_norm": 4.356249139192414e-09,
      "learning_rate": 1.4900000000000001e-06,
      "loss": 0.0041,
      "step": 145530
    },
    {
      "epoch": 7.762133333333333,
      "grad_norm": 0.05602994188666344,
      "learning_rate": 1.4866666666666667e-06,
      "loss": 0.0032,
      "step": 145540
    },
    {
      "epoch": 7.762666666666667,
      "grad_norm": 0.11205966770648956,
      "learning_rate": 1.4833333333333335e-06,
      "loss": 0.0025,
      "step": 145550
    },
    {
      "epoch": 7.7632,
      "grad_norm": 0.05602937191724777,
      "learning_rate": 1.4800000000000002e-06,
      "loss": 0.0029,
      "step": 145560
    },
    {
      "epoch": 7.763733333333334,
      "grad_norm": 0.2241162210702896,
      "learning_rate": 1.4766666666666668e-06,
      "loss": 0.0025,
      "step": 145570
    },
    {
      "epoch": 7.764266666666667,
      "grad_norm": 0.3641912341117859,
      "learning_rate": 1.4733333333333333e-06,
      "loss": 0.0036,
      "step": 145580
    },
    {
      "epoch": 7.7648,
      "grad_norm": 0.056029364466667175,
      "learning_rate": 1.4700000000000001e-06,
      "loss": 0.0021,
      "step": 145590
    },
    {
      "epoch": 7.765333333333333,
      "grad_norm": 0.16808819770812988,
      "learning_rate": 1.4666666666666667e-06,
      "loss": 0.0017,
      "step": 145600
    },
    {
      "epoch": 7.765866666666667,
      "grad_norm": 0.44823649525642395,
      "learning_rate": 1.4633333333333334e-06,
      "loss": 0.0024,
      "step": 145610
    },
    {
      "epoch": 7.7664,
      "grad_norm": 0.22411568462848663,
      "learning_rate": 1.46e-06,
      "loss": 0.0025,
      "step": 145620
    },
    {
      "epoch": 7.766933333333333,
      "grad_norm": 0.22411294281482697,
      "learning_rate": 1.4566666666666668e-06,
      "loss": 0.0031,
      "step": 145630
    },
    {
      "epoch": 7.7674666666666665,
      "grad_norm": 0.22411243617534637,
      "learning_rate": 1.4533333333333335e-06,
      "loss": 0.002,
      "step": 145640
    },
    {
      "epoch": 7.768,
      "grad_norm": 0.14007115364074707,
      "learning_rate": 1.45e-06,
      "loss": 0.0024,
      "step": 145650
    },
    {
      "epoch": 7.768533333333333,
      "grad_norm": 0.308157742023468,
      "learning_rate": 1.4466666666666667e-06,
      "loss": 0.0025,
      "step": 145660
    },
    {
      "epoch": 7.769066666666666,
      "grad_norm": 0.05602752044796944,
      "learning_rate": 1.4433333333333334e-06,
      "loss": 0.002,
      "step": 145670
    },
    {
      "epoch": 7.7696,
      "grad_norm": 1.9435795545578003,
      "learning_rate": 1.44e-06,
      "loss": 0.0021,
      "step": 145680
    },
    {
      "epoch": 7.770133333333334,
      "grad_norm": 0.11205781251192093,
      "learning_rate": 1.4366666666666667e-06,
      "loss": 0.0039,
      "step": 145690
    },
    {
      "epoch": 7.770666666666667,
      "grad_norm": 0.02801463194191456,
      "learning_rate": 1.4333333333333333e-06,
      "loss": 0.0029,
      "step": 145700
    },
    {
      "epoch": 7.7712,
      "grad_norm": 0.1961030513048172,
      "learning_rate": 1.43e-06,
      "loss": 0.0037,
      "step": 145710
    },
    {
      "epoch": 7.771733333333334,
      "grad_norm": 0.16809475421905518,
      "learning_rate": 1.4266666666666668e-06,
      "loss": 0.002,
      "step": 145720
    },
    {
      "epoch": 7.772266666666667,
      "grad_norm": 0.05602945387363434,
      "learning_rate": 1.4233333333333334e-06,
      "loss": 0.0032,
      "step": 145730
    },
    {
      "epoch": 7.7728,
      "grad_norm": 0.19610287249088287,
      "learning_rate": 1.4200000000000002e-06,
      "loss": 0.003,
      "step": 145740
    },
    {
      "epoch": 7.773333333333333,
      "grad_norm": 0.08404377102851868,
      "learning_rate": 1.4166666666666667e-06,
      "loss": 0.0026,
      "step": 145750
    },
    {
      "epoch": 7.773866666666667,
      "grad_norm": 0.00014703946362715214,
      "learning_rate": 1.4133333333333333e-06,
      "loss": 0.0024,
      "step": 145760
    },
    {
      "epoch": 7.7744,
      "grad_norm": 0.029881561174988747,
      "learning_rate": 1.41e-06,
      "loss": 0.0032,
      "step": 145770
    },
    {
      "epoch": 7.774933333333333,
      "grad_norm": 0.46305030584335327,
      "learning_rate": 1.4066666666666668e-06,
      "loss": 0.0024,
      "step": 145780
    },
    {
      "epoch": 7.7754666666666665,
      "grad_norm": 0.0840642899274826,
      "learning_rate": 1.4033333333333334e-06,
      "loss": 0.0026,
      "step": 145790
    },
    {
      "epoch": 7.776,
      "grad_norm": 0.11205814778804779,
      "learning_rate": 1.4000000000000001e-06,
      "loss": 0.0031,
      "step": 145800
    },
    {
      "epoch": 7.776533333333333,
      "grad_norm": 0.36447376012802124,
      "learning_rate": 1.3966666666666667e-06,
      "loss": 0.0026,
      "step": 145810
    },
    {
      "epoch": 7.777066666666666,
      "grad_norm": 0.05766971409320831,
      "learning_rate": 1.3933333333333335e-06,
      "loss": 0.0024,
      "step": 145820
    },
    {
      "epoch": 7.7776,
      "grad_norm": 0.04620123654603958,
      "learning_rate": 1.39e-06,
      "loss": 0.0023,
      "step": 145830
    },
    {
      "epoch": 7.778133333333333,
      "grad_norm": 0.11208312958478928,
      "learning_rate": 1.3866666666666666e-06,
      "loss": 0.0031,
      "step": 145840
    },
    {
      "epoch": 7.778666666666666,
      "grad_norm": 0.2524924874305725,
      "learning_rate": 1.3833333333333334e-06,
      "loss": 0.0022,
      "step": 145850
    },
    {
      "epoch": 7.7792,
      "grad_norm": 0.28902843594551086,
      "learning_rate": 1.3800000000000001e-06,
      "loss": 0.0027,
      "step": 145860
    },
    {
      "epoch": 7.779733333333334,
      "grad_norm": 0.1686062514781952,
      "learning_rate": 1.3766666666666667e-06,
      "loss": 0.0032,
      "step": 145870
    },
    {
      "epoch": 7.780266666666667,
      "grad_norm": 0.14792893826961517,
      "learning_rate": 1.3733333333333335e-06,
      "loss": 0.002,
      "step": 145880
    },
    {
      "epoch": 7.7808,
      "grad_norm": 0.3082183599472046,
      "learning_rate": 1.37e-06,
      "loss": 0.0034,
      "step": 145890
    },
    {
      "epoch": 7.781333333333333,
      "grad_norm": 0.42299067974090576,
      "learning_rate": 1.3666666666666668e-06,
      "loss": 0.0037,
      "step": 145900
    },
    {
      "epoch": 7.781866666666667,
      "grad_norm": 0.30858173966407776,
      "learning_rate": 1.3633333333333336e-06,
      "loss": 0.0024,
      "step": 145910
    },
    {
      "epoch": 7.7824,
      "grad_norm": 0.09438221901655197,
      "learning_rate": 1.36e-06,
      "loss": 0.004,
      "step": 145920
    },
    {
      "epoch": 7.782933333333333,
      "grad_norm": 0.6498758792877197,
      "learning_rate": 1.3566666666666667e-06,
      "loss": 0.0027,
      "step": 145930
    },
    {
      "epoch": 7.7834666666666665,
      "grad_norm": 0.19610291719436646,
      "learning_rate": 1.3533333333333334e-06,
      "loss": 0.0021,
      "step": 145940
    },
    {
      "epoch": 7.784,
      "grad_norm": 0.11216512322425842,
      "learning_rate": 1.35e-06,
      "loss": 0.0018,
      "step": 145950
    },
    {
      "epoch": 7.784533333333333,
      "grad_norm": 0.22411951422691345,
      "learning_rate": 1.3466666666666668e-06,
      "loss": 0.0021,
      "step": 145960
    },
    {
      "epoch": 7.785066666666666,
      "grad_norm": 0.22411949932575226,
      "learning_rate": 1.3433333333333333e-06,
      "loss": 0.0034,
      "step": 145970
    },
    {
      "epoch": 7.7856,
      "grad_norm": 0.05693913251161575,
      "learning_rate": 1.34e-06,
      "loss": 0.0018,
      "step": 145980
    },
    {
      "epoch": 7.786133333333334,
      "grad_norm": 0.05738504230976105,
      "learning_rate": 1.3366666666666669e-06,
      "loss": 0.0036,
      "step": 145990
    },
    {
      "epoch": 7.786666666666667,
      "grad_norm": 0.33621278405189514,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 0.0023,
      "step": 146000
    },
    {
      "epoch": 7.7872,
      "grad_norm": 0.19610311090946198,
      "learning_rate": 1.33e-06,
      "loss": 0.0019,
      "step": 146010
    },
    {
      "epoch": 7.787733333333334,
      "grad_norm": 0.1401223987340927,
      "learning_rate": 1.3266666666666667e-06,
      "loss": 0.0034,
      "step": 146020
    },
    {
      "epoch": 7.788266666666667,
      "grad_norm": 0.052610594779253006,
      "learning_rate": 1.3233333333333333e-06,
      "loss": 0.0017,
      "step": 146030
    },
    {
      "epoch": 7.7888,
      "grad_norm": 0.08414391428232193,
      "learning_rate": 1.32e-06,
      "loss": 0.0022,
      "step": 146040
    },
    {
      "epoch": 7.789333333333333,
      "grad_norm": 0.03165069967508316,
      "learning_rate": 1.3166666666666668e-06,
      "loss": 0.0033,
      "step": 146050
    },
    {
      "epoch": 7.789866666666667,
      "grad_norm": 0.47631368041038513,
      "learning_rate": 1.3133333333333334e-06,
      "loss": 0.0025,
      "step": 146060
    },
    {
      "epoch": 7.7904,
      "grad_norm": 0.07536647468805313,
      "learning_rate": 1.3100000000000002e-06,
      "loss": 0.0029,
      "step": 146070
    },
    {
      "epoch": 7.790933333333333,
      "grad_norm": 0.05605130270123482,
      "learning_rate": 1.3066666666666667e-06,
      "loss": 0.0028,
      "step": 146080
    },
    {
      "epoch": 7.7914666666666665,
      "grad_norm": 0.08515846729278564,
      "learning_rate": 1.3033333333333333e-06,
      "loss": 0.0018,
      "step": 146090
    },
    {
      "epoch": 7.792,
      "grad_norm": 0.0048019434325397015,
      "learning_rate": 1.3e-06,
      "loss": 0.0021,
      "step": 146100
    },
    {
      "epoch": 7.792533333333333,
      "grad_norm": 0.05602932721376419,
      "learning_rate": 1.2966666666666666e-06,
      "loss": 0.0018,
      "step": 146110
    },
    {
      "epoch": 7.793066666666666,
      "grad_norm": 0.22411957383155823,
      "learning_rate": 1.2933333333333334e-06,
      "loss": 0.0025,
      "step": 146120
    },
    {
      "epoch": 7.7936,
      "grad_norm": 0.3163587152957916,
      "learning_rate": 1.2900000000000001e-06,
      "loss": 0.0017,
      "step": 146130
    },
    {
      "epoch": 7.794133333333333,
      "grad_norm": 1.0656098127365112,
      "learning_rate": 1.2866666666666667e-06,
      "loss": 0.0036,
      "step": 146140
    },
    {
      "epoch": 7.794666666666666,
      "grad_norm": 0.22418253123760223,
      "learning_rate": 1.2833333333333335e-06,
      "loss": 0.0031,
      "step": 146150
    },
    {
      "epoch": 7.7952,
      "grad_norm": 0.05602915212512016,
      "learning_rate": 1.28e-06,
      "loss": 0.0026,
      "step": 146160
    },
    {
      "epoch": 7.795733333333334,
      "grad_norm": 0.5818532705307007,
      "learning_rate": 1.2766666666666668e-06,
      "loss": 0.0019,
      "step": 146170
    },
    {
      "epoch": 7.796266666666667,
      "grad_norm": 0.19610261917114258,
      "learning_rate": 1.2733333333333334e-06,
      "loss": 0.0024,
      "step": 146180
    },
    {
      "epoch": 7.7968,
      "grad_norm": 0.2801463007926941,
      "learning_rate": 1.27e-06,
      "loss": 0.0022,
      "step": 146190
    },
    {
      "epoch": 7.7973333333333334,
      "grad_norm": 0.05618949607014656,
      "learning_rate": 1.2666666666666667e-06,
      "loss": 0.0025,
      "step": 146200
    },
    {
      "epoch": 7.797866666666667,
      "grad_norm": 0.19722698628902435,
      "learning_rate": 1.2633333333333334e-06,
      "loss": 0.0032,
      "step": 146210
    },
    {
      "epoch": 7.7984,
      "grad_norm": 0.2529321312904358,
      "learning_rate": 1.26e-06,
      "loss": 0.0025,
      "step": 146220
    },
    {
      "epoch": 7.798933333333333,
      "grad_norm": 0.14007499814033508,
      "learning_rate": 1.2566666666666668e-06,
      "loss": 0.0024,
      "step": 146230
    },
    {
      "epoch": 7.7994666666666665,
      "grad_norm": 0.14219924807548523,
      "learning_rate": 1.2533333333333335e-06,
      "loss": 0.0029,
      "step": 146240
    },
    {
      "epoch": 7.8,
      "grad_norm": 0.1122247502207756,
      "learning_rate": 1.25e-06,
      "loss": 0.0012,
      "step": 146250
    },
    {
      "epoch": 7.800533333333333,
      "grad_norm": 0.013478514738380909,
      "learning_rate": 1.2466666666666667e-06,
      "loss": 0.0022,
      "step": 146260
    },
    {
      "epoch": 7.801066666666666,
      "grad_norm": 0.18430696427822113,
      "learning_rate": 1.2433333333333334e-06,
      "loss": 0.0017,
      "step": 146270
    },
    {
      "epoch": 7.8016,
      "grad_norm": 0.12877987325191498,
      "learning_rate": 1.24e-06,
      "loss": 0.0029,
      "step": 146280
    },
    {
      "epoch": 7.802133333333334,
      "grad_norm": 0.030641570687294006,
      "learning_rate": 1.2366666666666668e-06,
      "loss": 0.0016,
      "step": 146290
    },
    {
      "epoch": 7.802666666666667,
      "grad_norm": 0.19630475342273712,
      "learning_rate": 1.2333333333333333e-06,
      "loss": 0.0027,
      "step": 146300
    },
    {
      "epoch": 7.8032,
      "grad_norm": 0.05602307617664337,
      "learning_rate": 1.23e-06,
      "loss": 0.0024,
      "step": 146310
    },
    {
      "epoch": 7.803733333333334,
      "grad_norm": 0.02840813435614109,
      "learning_rate": 1.2266666666666669e-06,
      "loss": 0.0018,
      "step": 146320
    },
    {
      "epoch": 7.804266666666667,
      "grad_norm": 0.09306735545396805,
      "learning_rate": 1.2233333333333334e-06,
      "loss": 0.0023,
      "step": 146330
    },
    {
      "epoch": 7.8048,
      "grad_norm": 0.39494314789772034,
      "learning_rate": 1.2200000000000002e-06,
      "loss": 0.0035,
      "step": 146340
    },
    {
      "epoch": 7.8053333333333335,
      "grad_norm": 0.05612733215093613,
      "learning_rate": 1.2166666666666667e-06,
      "loss": 0.0026,
      "step": 146350
    },
    {
      "epoch": 7.805866666666667,
      "grad_norm": 0.13616694509983063,
      "learning_rate": 1.2133333333333333e-06,
      "loss": 0.0023,
      "step": 146360
    },
    {
      "epoch": 7.8064,
      "grad_norm": 0.14135734736919403,
      "learning_rate": 1.21e-06,
      "loss": 0.0017,
      "step": 146370
    },
    {
      "epoch": 7.806933333333333,
      "grad_norm": 0.19884487986564636,
      "learning_rate": 1.2066666666666666e-06,
      "loss": 0.0022,
      "step": 146380
    },
    {
      "epoch": 7.8074666666666666,
      "grad_norm": 0.14037202298641205,
      "learning_rate": 1.2033333333333334e-06,
      "loss": 0.0021,
      "step": 146390
    },
    {
      "epoch": 7.808,
      "grad_norm": 0.14011120796203613,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 0.0021,
      "step": 146400
    },
    {
      "epoch": 7.808533333333333,
      "grad_norm": 0.0280179213732481,
      "learning_rate": 1.1966666666666667e-06,
      "loss": 0.0032,
      "step": 146410
    },
    {
      "epoch": 7.809066666666666,
      "grad_norm": 0.029850350692868233,
      "learning_rate": 1.1933333333333335e-06,
      "loss": 0.002,
      "step": 146420
    },
    {
      "epoch": 7.8096,
      "grad_norm": 0.19685913622379303,
      "learning_rate": 1.19e-06,
      "loss": 0.0024,
      "step": 146430
    },
    {
      "epoch": 7.810133333333333,
      "grad_norm": 0.028027452528476715,
      "learning_rate": 1.1866666666666666e-06,
      "loss": 0.0034,
      "step": 146440
    },
    {
      "epoch": 7.810666666666666,
      "grad_norm": 0.14115719497203827,
      "learning_rate": 1.1833333333333334e-06,
      "loss": 0.0026,
      "step": 146450
    },
    {
      "epoch": 7.8112,
      "grad_norm": 0.3081655204296112,
      "learning_rate": 1.18e-06,
      "loss": 0.0025,
      "step": 146460
    },
    {
      "epoch": 7.811733333333334,
      "grad_norm": 0.16814184188842773,
      "learning_rate": 1.1766666666666667e-06,
      "loss": 0.0028,
      "step": 146470
    },
    {
      "epoch": 7.812266666666667,
      "grad_norm": 0.2805037498474121,
      "learning_rate": 1.1733333333333335e-06,
      "loss": 0.0034,
      "step": 146480
    },
    {
      "epoch": 7.8128,
      "grad_norm": 0.1965540200471878,
      "learning_rate": 1.17e-06,
      "loss": 0.0023,
      "step": 146490
    },
    {
      "epoch": 7.8133333333333335,
      "grad_norm": 0.2803976833820343,
      "learning_rate": 1.1666666666666668e-06,
      "loss": 0.0024,
      "step": 146500
    },
    {
      "epoch": 7.813866666666667,
      "grad_norm": 0.08564998209476471,
      "learning_rate": 1.1633333333333336e-06,
      "loss": 0.0027,
      "step": 146510
    },
    {
      "epoch": 7.8144,
      "grad_norm": 0.1473843902349472,
      "learning_rate": 1.16e-06,
      "loss": 0.0035,
      "step": 146520
    },
    {
      "epoch": 7.814933333333333,
      "grad_norm": 0.04073931276798248,
      "learning_rate": 1.1566666666666667e-06,
      "loss": 0.0021,
      "step": 146530
    },
    {
      "epoch": 7.815466666666667,
      "grad_norm": 0.11538872122764587,
      "learning_rate": 1.1533333333333334e-06,
      "loss": 0.002,
      "step": 146540
    },
    {
      "epoch": 7.816,
      "grad_norm": 0.3366551101207733,
      "learning_rate": 1.15e-06,
      "loss": 0.0022,
      "step": 146550
    },
    {
      "epoch": 7.816533333333333,
      "grad_norm": 0.06325124204158783,
      "learning_rate": 1.1466666666666668e-06,
      "loss": 0.0022,
      "step": 146560
    },
    {
      "epoch": 7.817066666666666,
      "grad_norm": 0.2802695333957672,
      "learning_rate": 1.1433333333333333e-06,
      "loss": 0.0023,
      "step": 146570
    },
    {
      "epoch": 7.8176,
      "grad_norm": 0.1961056888103485,
      "learning_rate": 1.14e-06,
      "loss": 0.0018,
      "step": 146580
    },
    {
      "epoch": 7.818133333333334,
      "grad_norm": 0.08404544740915298,
      "learning_rate": 1.1366666666666669e-06,
      "loss": 0.0021,
      "step": 146590
    },
    {
      "epoch": 7.818666666666667,
      "grad_norm": 0.05735310912132263,
      "learning_rate": 1.1333333333333334e-06,
      "loss": 0.0023,
      "step": 146600
    },
    {
      "epoch": 7.8192,
      "grad_norm": 0.196107417345047,
      "learning_rate": 1.13e-06,
      "loss": 0.0045,
      "step": 146610
    },
    {
      "epoch": 7.819733333333334,
      "grad_norm": 0.11207598447799683,
      "learning_rate": 1.1266666666666667e-06,
      "loss": 0.0027,
      "step": 146620
    },
    {
      "epoch": 7.820266666666667,
      "grad_norm": 0.08404459059238434,
      "learning_rate": 1.1233333333333333e-06,
      "loss": 0.0022,
      "step": 146630
    },
    {
      "epoch": 7.8208,
      "grad_norm": 0.02801479957997799,
      "learning_rate": 1.12e-06,
      "loss": 0.0023,
      "step": 146640
    },
    {
      "epoch": 7.8213333333333335,
      "grad_norm": 0.11205947399139404,
      "learning_rate": 1.1166666666666666e-06,
      "loss": 0.0017,
      "step": 146650
    },
    {
      "epoch": 7.821866666666667,
      "grad_norm": 0.4482397735118866,
      "learning_rate": 1.1133333333333334e-06,
      "loss": 0.0029,
      "step": 146660
    },
    {
      "epoch": 7.8224,
      "grad_norm": 1.1333844661712646,
      "learning_rate": 1.1100000000000002e-06,
      "loss": 0.0024,
      "step": 146670
    },
    {
      "epoch": 7.822933333333333,
      "grad_norm": 0.028015201911330223,
      "learning_rate": 1.1066666666666667e-06,
      "loss": 0.0022,
      "step": 146680
    },
    {
      "epoch": 7.823466666666667,
      "grad_norm": 0.33618196845054626,
      "learning_rate": 1.1033333333333333e-06,
      "loss": 0.001,
      "step": 146690
    },
    {
      "epoch": 7.824,
      "grad_norm": 0.3081662654876709,
      "learning_rate": 1.1e-06,
      "loss": 0.003,
      "step": 146700
    },
    {
      "epoch": 7.824533333333333,
      "grad_norm": 0.9690841436386108,
      "learning_rate": 1.0966666666666666e-06,
      "loss": 0.0027,
      "step": 146710
    },
    {
      "epoch": 7.825066666666666,
      "grad_norm": 0.05603038892149925,
      "learning_rate": 1.0933333333333334e-06,
      "loss": 0.0031,
      "step": 146720
    },
    {
      "epoch": 7.8256,
      "grad_norm": 0.28015121817588806,
      "learning_rate": 1.0900000000000002e-06,
      "loss": 0.0022,
      "step": 146730
    },
    {
      "epoch": 7.826133333333333,
      "grad_norm": 0.19610442221164703,
      "learning_rate": 1.0866666666666667e-06,
      "loss": 0.003,
      "step": 146740
    },
    {
      "epoch": 7.826666666666666,
      "grad_norm": 0.1961040496826172,
      "learning_rate": 1.0833333333333335e-06,
      "loss": 0.0021,
      "step": 146750
    },
    {
      "epoch": 7.8272,
      "grad_norm": 0.05602800101041794,
      "learning_rate": 1.08e-06,
      "loss": 0.0029,
      "step": 146760
    },
    {
      "epoch": 7.827733333333334,
      "grad_norm": 0.3641936480998993,
      "learning_rate": 1.0766666666666668e-06,
      "loss": 0.0025,
      "step": 146770
    },
    {
      "epoch": 7.828266666666667,
      "grad_norm": 0.056030742824077606,
      "learning_rate": 1.0733333333333334e-06,
      "loss": 0.0031,
      "step": 146780
    },
    {
      "epoch": 7.8288,
      "grad_norm": 0.09452971816062927,
      "learning_rate": 1.07e-06,
      "loss": 0.0026,
      "step": 146790
    },
    {
      "epoch": 7.8293333333333335,
      "grad_norm": 0.5042647123336792,
      "learning_rate": 1.0666666666666667e-06,
      "loss": 0.0031,
      "step": 146800
    },
    {
      "epoch": 7.829866666666667,
      "grad_norm": 0.22411806881427765,
      "learning_rate": 1.0633333333333335e-06,
      "loss": 0.0029,
      "step": 146810
    },
    {
      "epoch": 7.8304,
      "grad_norm": 0.028014782816171646,
      "learning_rate": 1.06e-06,
      "loss": 0.0037,
      "step": 146820
    },
    {
      "epoch": 7.830933333333333,
      "grad_norm": 0.06898674368858337,
      "learning_rate": 1.0566666666666668e-06,
      "loss": 0.0028,
      "step": 146830
    },
    {
      "epoch": 7.831466666666667,
      "grad_norm": 0.028014827519655228,
      "learning_rate": 1.0533333333333333e-06,
      "loss": 0.0026,
      "step": 146840
    },
    {
      "epoch": 7.832,
      "grad_norm": 0.11205874383449554,
      "learning_rate": 1.0500000000000001e-06,
      "loss": 0.0018,
      "step": 146850
    },
    {
      "epoch": 7.832533333333333,
      "grad_norm": 0.19610263407230377,
      "learning_rate": 1.0466666666666669e-06,
      "loss": 0.0018,
      "step": 146860
    },
    {
      "epoch": 7.833066666666666,
      "grad_norm": 0.14007404446601868,
      "learning_rate": 1.0433333333333332e-06,
      "loss": 0.003,
      "step": 146870
    },
    {
      "epoch": 7.8336,
      "grad_norm": 0.1680959016084671,
      "learning_rate": 1.04e-06,
      "loss": 0.0021,
      "step": 146880
    },
    {
      "epoch": 7.834133333333333,
      "grad_norm": 0.29573506116867065,
      "learning_rate": 1.0366666666666668e-06,
      "loss": 0.0018,
      "step": 146890
    },
    {
      "epoch": 7.834666666666667,
      "grad_norm": 0.05602952465415001,
      "learning_rate": 1.0333333333333333e-06,
      "loss": 0.0026,
      "step": 146900
    },
    {
      "epoch": 7.8352,
      "grad_norm": 0.16808857023715973,
      "learning_rate": 1.03e-06,
      "loss": 0.0021,
      "step": 146910
    },
    {
      "epoch": 7.835733333333334,
      "grad_norm": 0.22411854565143585,
      "learning_rate": 1.0266666666666666e-06,
      "loss": 0.0034,
      "step": 146920
    },
    {
      "epoch": 7.836266666666667,
      "grad_norm": 0.05602947250008583,
      "learning_rate": 1.0233333333333334e-06,
      "loss": 0.0027,
      "step": 146930
    },
    {
      "epoch": 7.8368,
      "grad_norm": 0.31223541498184204,
      "learning_rate": 1.0200000000000002e-06,
      "loss": 0.0026,
      "step": 146940
    },
    {
      "epoch": 7.8373333333333335,
      "grad_norm": 0.2521315813064575,
      "learning_rate": 1.0166666666666665e-06,
      "loss": 0.0017,
      "step": 146950
    },
    {
      "epoch": 7.837866666666667,
      "grad_norm": 0.02801469899713993,
      "learning_rate": 1.0133333333333333e-06,
      "loss": 0.0033,
      "step": 146960
    },
    {
      "epoch": 7.8384,
      "grad_norm": 0.36418893933296204,
      "learning_rate": 1.01e-06,
      "loss": 0.0025,
      "step": 146970
    },
    {
      "epoch": 7.838933333333333,
      "grad_norm": 0.08404500037431717,
      "learning_rate": 1.0066666666666666e-06,
      "loss": 0.0036,
      "step": 146980
    },
    {
      "epoch": 7.839466666666667,
      "grad_norm": 0.25213623046875,
      "learning_rate": 1.0033333333333334e-06,
      "loss": 0.0035,
      "step": 146990
    },
    {
      "epoch": 7.84,
      "grad_norm": 0.5322889089584351,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.0027,
      "step": 147000
    },
    {
      "epoch": 7.840533333333333,
      "grad_norm": 0.14007504284381866,
      "learning_rate": 9.966666666666667e-07,
      "loss": 0.003,
      "step": 147010
    },
    {
      "epoch": 7.841066666666666,
      "grad_norm": 0.3565191328525543,
      "learning_rate": 9.933333333333335e-07,
      "loss": 0.0041,
      "step": 147020
    },
    {
      "epoch": 7.8416,
      "grad_norm": 0.07957936078310013,
      "learning_rate": 9.9e-07,
      "loss": 0.0039,
      "step": 147030
    },
    {
      "epoch": 7.842133333333333,
      "grad_norm": 0.44823911786079407,
      "learning_rate": 9.866666666666666e-07,
      "loss": 0.0027,
      "step": 147040
    },
    {
      "epoch": 7.842666666666666,
      "grad_norm": 0.16808943450450897,
      "learning_rate": 9.833333333333334e-07,
      "loss": 0.0029,
      "step": 147050
    },
    {
      "epoch": 7.8431999999999995,
      "grad_norm": 0.07742348313331604,
      "learning_rate": 9.8e-07,
      "loss": 0.0028,
      "step": 147060
    },
    {
      "epoch": 7.843733333333334,
      "grad_norm": 0.16808851063251495,
      "learning_rate": 9.766666666666667e-07,
      "loss": 0.0017,
      "step": 147070
    },
    {
      "epoch": 7.844266666666667,
      "grad_norm": 0.1961030215024948,
      "learning_rate": 9.733333333333335e-07,
      "loss": 0.0021,
      "step": 147080
    },
    {
      "epoch": 7.8448,
      "grad_norm": 0.08474594354629517,
      "learning_rate": 9.7e-07,
      "loss": 0.0023,
      "step": 147090
    },
    {
      "epoch": 7.8453333333333335,
      "grad_norm": 0.25213390588760376,
      "learning_rate": 9.666666666666668e-07,
      "loss": 0.0029,
      "step": 147100
    },
    {
      "epoch": 7.845866666666667,
      "grad_norm": 3.6788709856239166e-09,
      "learning_rate": 9.633333333333334e-07,
      "loss": 0.0028,
      "step": 147110
    },
    {
      "epoch": 7.8464,
      "grad_norm": 0.19637706875801086,
      "learning_rate": 9.6e-07,
      "loss": 0.0024,
      "step": 147120
    },
    {
      "epoch": 7.846933333333333,
      "grad_norm": 0.11206614226102829,
      "learning_rate": 9.566666666666667e-07,
      "loss": 0.003,
      "step": 147130
    },
    {
      "epoch": 7.847466666666667,
      "grad_norm": 0.0869208425283432,
      "learning_rate": 9.533333333333333e-07,
      "loss": 0.0022,
      "step": 147140
    },
    {
      "epoch": 7.848,
      "grad_norm": 0.1963132619857788,
      "learning_rate": 9.5e-07,
      "loss": 0.0022,
      "step": 147150
    },
    {
      "epoch": 7.848533333333333,
      "grad_norm": 0.17502613365650177,
      "learning_rate": 9.466666666666667e-07,
      "loss": 0.0035,
      "step": 147160
    },
    {
      "epoch": 7.849066666666666,
      "grad_norm": 0.07568687945604324,
      "learning_rate": 9.433333333333334e-07,
      "loss": 0.0029,
      "step": 147170
    },
    {
      "epoch": 7.8496,
      "grad_norm": 0.029285620898008347,
      "learning_rate": 9.400000000000001e-07,
      "loss": 0.0032,
      "step": 147180
    },
    {
      "epoch": 7.850133333333333,
      "grad_norm": 0.056032419204711914,
      "learning_rate": 9.366666666666668e-07,
      "loss": 0.0024,
      "step": 147190
    },
    {
      "epoch": 7.850666666666667,
      "grad_norm": 0.19610634446144104,
      "learning_rate": 9.333333333333334e-07,
      "loss": 0.0035,
      "step": 147200
    },
    {
      "epoch": 7.8512,
      "grad_norm": 0.056030262261629105,
      "learning_rate": 9.3e-07,
      "loss": 0.0026,
      "step": 147210
    },
    {
      "epoch": 7.851733333333334,
      "grad_norm": 0.2521359622478485,
      "learning_rate": 9.266666666666667e-07,
      "loss": 0.0028,
      "step": 147220
    },
    {
      "epoch": 7.852266666666667,
      "grad_norm": 0.028015173971652985,
      "learning_rate": 9.233333333333333e-07,
      "loss": 0.0028,
      "step": 147230
    },
    {
      "epoch": 7.8528,
      "grad_norm": 0.2241215705871582,
      "learning_rate": 9.2e-07,
      "loss": 0.0029,
      "step": 147240
    },
    {
      "epoch": 7.8533333333333335,
      "grad_norm": 0.08404920995235443,
      "learning_rate": 9.166666666666667e-07,
      "loss": 0.0028,
      "step": 147250
    },
    {
      "epoch": 7.853866666666667,
      "grad_norm": 0.3093542158603668,
      "learning_rate": 9.133333333333334e-07,
      "loss": 0.0021,
      "step": 147260
    },
    {
      "epoch": 7.8544,
      "grad_norm": 0.3367600739002228,
      "learning_rate": 9.100000000000001e-07,
      "loss": 0.0035,
      "step": 147270
    },
    {
      "epoch": 7.854933333333333,
      "grad_norm": 0.16837143898010254,
      "learning_rate": 9.066666666666667e-07,
      "loss": 0.0035,
      "step": 147280
    },
    {
      "epoch": 7.855466666666667,
      "grad_norm": 0.28015217185020447,
      "learning_rate": 9.033333333333335e-07,
      "loss": 0.0025,
      "step": 147290
    },
    {
      "epoch": 7.856,
      "grad_norm": 0.0012512969551607966,
      "learning_rate": 9e-07,
      "loss": 0.0027,
      "step": 147300
    },
    {
      "epoch": 7.856533333333333,
      "grad_norm": 0.3641914129257202,
      "learning_rate": 8.966666666666666e-07,
      "loss": 0.0021,
      "step": 147310
    },
    {
      "epoch": 7.857066666666666,
      "grad_norm": 0.2523754835128784,
      "learning_rate": 8.933333333333334e-07,
      "loss": 0.0024,
      "step": 147320
    },
    {
      "epoch": 7.8576,
      "grad_norm": 5.973128103242686e-10,
      "learning_rate": 8.900000000000001e-07,
      "loss": 0.0019,
      "step": 147330
    },
    {
      "epoch": 7.858133333333333,
      "grad_norm": 0.0017700900789350271,
      "learning_rate": 8.866666666666667e-07,
      "loss": 0.0023,
      "step": 147340
    },
    {
      "epoch": 7.858666666666666,
      "grad_norm": 0.25211644172668457,
      "learning_rate": 8.833333333333334e-07,
      "loss": 0.0023,
      "step": 147350
    },
    {
      "epoch": 7.8591999999999995,
      "grad_norm": 0.0840437263250351,
      "learning_rate": 8.8e-07,
      "loss": 0.0035,
      "step": 147360
    },
    {
      "epoch": 7.859733333333334,
      "grad_norm": 0.3922027051448822,
      "learning_rate": 8.766666666666668e-07,
      "loss": 0.0024,
      "step": 147370
    },
    {
      "epoch": 7.860266666666667,
      "grad_norm": 0.08404333889484406,
      "learning_rate": 8.733333333333333e-07,
      "loss": 0.0033,
      "step": 147380
    },
    {
      "epoch": 7.8608,
      "grad_norm": 0.1126011535525322,
      "learning_rate": 8.699999999999999e-07,
      "loss": 0.002,
      "step": 147390
    },
    {
      "epoch": 7.8613333333333335,
      "grad_norm": 0.36599498987197876,
      "learning_rate": 8.666666666666667e-07,
      "loss": 0.0026,
      "step": 147400
    },
    {
      "epoch": 7.861866666666667,
      "grad_norm": 0.03227788209915161,
      "learning_rate": 8.633333333333334e-07,
      "loss": 0.0029,
      "step": 147410
    },
    {
      "epoch": 7.8624,
      "grad_norm": 0.22429026663303375,
      "learning_rate": 8.6e-07,
      "loss": 0.0035,
      "step": 147420
    },
    {
      "epoch": 7.862933333333333,
      "grad_norm": 0.05657929927110672,
      "learning_rate": 8.566666666666667e-07,
      "loss": 0.0029,
      "step": 147430
    },
    {
      "epoch": 7.863466666666667,
      "grad_norm": 0.02944391779601574,
      "learning_rate": 8.533333333333335e-07,
      "loss": 0.0025,
      "step": 147440
    },
    {
      "epoch": 7.864,
      "grad_norm": 0.3395225405693054,
      "learning_rate": 8.500000000000001e-07,
      "loss": 0.0031,
      "step": 147450
    },
    {
      "epoch": 7.864533333333333,
      "grad_norm": 0.02801497094333172,
      "learning_rate": 8.466666666666668e-07,
      "loss": 0.002,
      "step": 147460
    },
    {
      "epoch": 7.865066666666666,
      "grad_norm": 0.39221465587615967,
      "learning_rate": 8.433333333333333e-07,
      "loss": 0.0032,
      "step": 147470
    },
    {
      "epoch": 7.8656,
      "grad_norm": 0.0022370207589119673,
      "learning_rate": 8.4e-07,
      "loss": 0.0029,
      "step": 147480
    },
    {
      "epoch": 7.866133333333333,
      "grad_norm": 0.4921920895576477,
      "learning_rate": 8.366666666666667e-07,
      "loss": 0.0034,
      "step": 147490
    },
    {
      "epoch": 7.866666666666667,
      "grad_norm": 0.28030160069465637,
      "learning_rate": 8.333333333333333e-07,
      "loss": 0.0021,
      "step": 147500
    },
    {
      "epoch": 7.8672,
      "grad_norm": 0.11206048727035522,
      "learning_rate": 8.300000000000001e-07,
      "loss": 0.0028,
      "step": 147510
    },
    {
      "epoch": 7.867733333333334,
      "grad_norm": 0.0562746487557888,
      "learning_rate": 8.266666666666668e-07,
      "loss": 0.0026,
      "step": 147520
    },
    {
      "epoch": 7.868266666666667,
      "grad_norm": 0.18286430835723877,
      "learning_rate": 8.233333333333334e-07,
      "loss": 0.0019,
      "step": 147530
    },
    {
      "epoch": 7.8688,
      "grad_norm": 0.11243048310279846,
      "learning_rate": 8.200000000000001e-07,
      "loss": 0.0039,
      "step": 147540
    },
    {
      "epoch": 7.8693333333333335,
      "grad_norm": 0.056029513478279114,
      "learning_rate": 8.166666666666666e-07,
      "loss": 0.0022,
      "step": 147550
    },
    {
      "epoch": 7.869866666666667,
      "grad_norm": 0.056070223450660706,
      "learning_rate": 8.133333333333333e-07,
      "loss": 0.0036,
      "step": 147560
    },
    {
      "epoch": 7.8704,
      "grad_norm": 0.19617965817451477,
      "learning_rate": 8.1e-07,
      "loss": 0.0022,
      "step": 147570
    },
    {
      "epoch": 7.870933333333333,
      "grad_norm": 0.14058393239974976,
      "learning_rate": 8.066666666666666e-07,
      "loss": 0.0018,
      "step": 147580
    },
    {
      "epoch": 7.871466666666667,
      "grad_norm": 0.08421289920806885,
      "learning_rate": 8.033333333333334e-07,
      "loss": 0.0022,
      "step": 147590
    },
    {
      "epoch": 7.872,
      "grad_norm": 0.20209315419197083,
      "learning_rate": 8.000000000000001e-07,
      "loss": 0.0018,
      "step": 147600
    },
    {
      "epoch": 7.872533333333333,
      "grad_norm": 0.030030768364667892,
      "learning_rate": 7.966666666666667e-07,
      "loss": 0.0027,
      "step": 147610
    },
    {
      "epoch": 7.873066666666666,
      "grad_norm": 0.14007055759429932,
      "learning_rate": 7.933333333333334e-07,
      "loss": 0.0029,
      "step": 147620
    },
    {
      "epoch": 7.8736,
      "grad_norm": 0.0280148945748806,
      "learning_rate": 7.900000000000002e-07,
      "loss": 0.0025,
      "step": 147630
    },
    {
      "epoch": 7.874133333333333,
      "grad_norm": 0.14100675284862518,
      "learning_rate": 7.866666666666666e-07,
      "loss": 0.0031,
      "step": 147640
    },
    {
      "epoch": 7.874666666666666,
      "grad_norm": 0.11208811402320862,
      "learning_rate": 7.833333333333333e-07,
      "loss": 0.0026,
      "step": 147650
    },
    {
      "epoch": 7.8751999999999995,
      "grad_norm": 0.14028312265872955,
      "learning_rate": 7.8e-07,
      "loss": 0.0027,
      "step": 147660
    },
    {
      "epoch": 7.875733333333334,
      "grad_norm": 0.16807527840137482,
      "learning_rate": 7.766666666666667e-07,
      "loss": 0.0028,
      "step": 147670
    },
    {
      "epoch": 7.876266666666667,
      "grad_norm": 0.22414731979370117,
      "learning_rate": 7.733333333333334e-07,
      "loss": 0.0032,
      "step": 147680
    },
    {
      "epoch": 7.8768,
      "grad_norm": 0.057745568454265594,
      "learning_rate": 7.7e-07,
      "loss": 0.0027,
      "step": 147690
    },
    {
      "epoch": 7.8773333333333335,
      "grad_norm": 0.058449678122997284,
      "learning_rate": 7.666666666666667e-07,
      "loss": 0.0023,
      "step": 147700
    },
    {
      "epoch": 7.877866666666667,
      "grad_norm": 0.17045310139656067,
      "learning_rate": 7.633333333333334e-07,
      "loss": 0.0022,
      "step": 147710
    },
    {
      "epoch": 7.8784,
      "grad_norm": 0.08564843237400055,
      "learning_rate": 7.6e-07,
      "loss": 0.0023,
      "step": 147720
    },
    {
      "epoch": 7.878933333333333,
      "grad_norm": 0.05751277878880501,
      "learning_rate": 7.566666666666667e-07,
      "loss": 0.0019,
      "step": 147730
    },
    {
      "epoch": 7.879466666666667,
      "grad_norm": 0.3361837565898895,
      "learning_rate": 7.533333333333334e-07,
      "loss": 0.0017,
      "step": 147740
    },
    {
      "epoch": 7.88,
      "grad_norm": 0.14131419360637665,
      "learning_rate": 7.5e-07,
      "loss": 0.0027,
      "step": 147750
    },
    {
      "epoch": 7.880533333333333,
      "grad_norm": 0.36431992053985596,
      "learning_rate": 7.466666666666667e-07,
      "loss": 0.0017,
      "step": 147760
    },
    {
      "epoch": 7.881066666666666,
      "grad_norm": 0.08642520010471344,
      "learning_rate": 7.433333333333333e-07,
      "loss": 0.0027,
      "step": 147770
    },
    {
      "epoch": 7.8816,
      "grad_norm": 0.3084043860435486,
      "learning_rate": 7.400000000000001e-07,
      "loss": 0.0029,
      "step": 147780
    },
    {
      "epoch": 7.882133333333333,
      "grad_norm": 0.25247570872306824,
      "learning_rate": 7.366666666666667e-07,
      "loss": 0.0031,
      "step": 147790
    },
    {
      "epoch": 7.882666666666667,
      "grad_norm": 0.08452863246202469,
      "learning_rate": 7.333333333333333e-07,
      "loss": 0.0024,
      "step": 147800
    },
    {
      "epoch": 7.8832,
      "grad_norm": 0.22438618540763855,
      "learning_rate": 7.3e-07,
      "loss": 0.0022,
      "step": 147810
    },
    {
      "epoch": 7.883733333333334,
      "grad_norm": 0.14183369278907776,
      "learning_rate": 7.266666666666668e-07,
      "loss": 0.0023,
      "step": 147820
    },
    {
      "epoch": 7.884266666666667,
      "grad_norm": 0.08463355153799057,
      "learning_rate": 7.233333333333333e-07,
      "loss": 0.0026,
      "step": 147830
    },
    {
      "epoch": 7.8848,
      "grad_norm": 0.16983860731124878,
      "learning_rate": 7.2e-07,
      "loss": 0.0025,
      "step": 147840
    },
    {
      "epoch": 7.8853333333333335,
      "grad_norm": 0.22419889271259308,
      "learning_rate": 7.166666666666667e-07,
      "loss": 0.0019,
      "step": 147850
    },
    {
      "epoch": 7.885866666666667,
      "grad_norm": 0.08650917559862137,
      "learning_rate": 7.133333333333334e-07,
      "loss": 0.003,
      "step": 147860
    },
    {
      "epoch": 7.8864,
      "grad_norm": 0.2521663308143616,
      "learning_rate": 7.100000000000001e-07,
      "loss": 0.003,
      "step": 147870
    },
    {
      "epoch": 7.886933333333333,
      "grad_norm": 0.03307553008198738,
      "learning_rate": 7.066666666666666e-07,
      "loss": 0.0028,
      "step": 147880
    },
    {
      "epoch": 7.887466666666667,
      "grad_norm": 0.028015078976750374,
      "learning_rate": 7.033333333333334e-07,
      "loss": 0.002,
      "step": 147890
    },
    {
      "epoch": 7.888,
      "grad_norm": 0.056047532707452774,
      "learning_rate": 7.000000000000001e-07,
      "loss": 0.0023,
      "step": 147900
    },
    {
      "epoch": 7.888533333333333,
      "grad_norm": 0.029630906879901886,
      "learning_rate": 6.966666666666667e-07,
      "loss": 0.0022,
      "step": 147910
    },
    {
      "epoch": 7.8890666666666664,
      "grad_norm": 0.3113712966442108,
      "learning_rate": 6.933333333333333e-07,
      "loss": 0.003,
      "step": 147920
    },
    {
      "epoch": 7.8896,
      "grad_norm": 0.11347176134586334,
      "learning_rate": 6.900000000000001e-07,
      "loss": 0.0028,
      "step": 147930
    },
    {
      "epoch": 7.890133333333333,
      "grad_norm": 0.26768872141838074,
      "learning_rate": 6.866666666666667e-07,
      "loss": 0.0029,
      "step": 147940
    },
    {
      "epoch": 7.890666666666666,
      "grad_norm": 0.08514058589935303,
      "learning_rate": 6.833333333333334e-07,
      "loss": 0.0038,
      "step": 147950
    },
    {
      "epoch": 7.8911999999999995,
      "grad_norm": 0.14340990781784058,
      "learning_rate": 6.8e-07,
      "loss": 0.0023,
      "step": 147960
    },
    {
      "epoch": 7.891733333333334,
      "grad_norm": 0.252098947763443,
      "learning_rate": 6.766666666666667e-07,
      "loss": 0.0017,
      "step": 147970
    },
    {
      "epoch": 7.892266666666667,
      "grad_norm": 0.11109352111816406,
      "learning_rate": 6.733333333333334e-07,
      "loss": 0.0032,
      "step": 147980
    },
    {
      "epoch": 7.8928,
      "grad_norm": 0.2523403465747833,
      "learning_rate": 6.7e-07,
      "loss": 0.0022,
      "step": 147990
    },
    {
      "epoch": 7.8933333333333335,
      "grad_norm": 0.16928665339946747,
      "learning_rate": 6.666666666666667e-07,
      "loss": 0.002,
      "step": 148000
    },
    {
      "epoch": 7.893866666666667,
      "grad_norm": 0.11251702159643173,
      "learning_rate": 6.633333333333334e-07,
      "loss": 0.0019,
      "step": 148010
    },
    {
      "epoch": 7.8944,
      "grad_norm": 0.11261039972305298,
      "learning_rate": 6.6e-07,
      "loss": 0.0036,
      "step": 148020
    },
    {
      "epoch": 7.894933333333333,
      "grad_norm": 0.16851556301116943,
      "learning_rate": 6.566666666666667e-07,
      "loss": 0.0021,
      "step": 148030
    },
    {
      "epoch": 7.895466666666667,
      "grad_norm": 0.11218895763158798,
      "learning_rate": 6.533333333333334e-07,
      "loss": 0.0025,
      "step": 148040
    },
    {
      "epoch": 7.896,
      "grad_norm": 0.17071974277496338,
      "learning_rate": 6.5e-07,
      "loss": 0.003,
      "step": 148050
    },
    {
      "epoch": 7.896533333333333,
      "grad_norm": 0.05643027275800705,
      "learning_rate": 6.466666666666667e-07,
      "loss": 0.0027,
      "step": 148060
    },
    {
      "epoch": 7.8970666666666665,
      "grad_norm": 0.08457448333501816,
      "learning_rate": 6.433333333333334e-07,
      "loss": 0.0028,
      "step": 148070
    },
    {
      "epoch": 7.8976,
      "grad_norm": 0.05602939799427986,
      "learning_rate": 6.4e-07,
      "loss": 0.003,
      "step": 148080
    },
    {
      "epoch": 7.898133333333333,
      "grad_norm": 0.42649638652801514,
      "learning_rate": 6.366666666666667e-07,
      "loss": 0.0041,
      "step": 148090
    },
    {
      "epoch": 7.898666666666666,
      "grad_norm": 0.05714065954089165,
      "learning_rate": 6.333333333333333e-07,
      "loss": 0.0023,
      "step": 148100
    },
    {
      "epoch": 7.8992,
      "grad_norm": 0.21197867393493652,
      "learning_rate": 6.3e-07,
      "loss": 0.0021,
      "step": 148110
    },
    {
      "epoch": 7.899733333333334,
      "grad_norm": 0.28154879808425903,
      "learning_rate": 6.266666666666668e-07,
      "loss": 0.0018,
      "step": 148120
    },
    {
      "epoch": 7.900266666666667,
      "grad_norm": 0.018645141273736954,
      "learning_rate": 6.233333333333333e-07,
      "loss": 0.0022,
      "step": 148130
    },
    {
      "epoch": 7.9008,
      "grad_norm": 0.08556647598743439,
      "learning_rate": 6.2e-07,
      "loss": 0.0033,
      "step": 148140
    },
    {
      "epoch": 7.9013333333333335,
      "grad_norm": 0.1965671181678772,
      "learning_rate": 6.166666666666667e-07,
      "loss": 0.0017,
      "step": 148150
    },
    {
      "epoch": 7.901866666666667,
      "grad_norm": 0.3963068723678589,
      "learning_rate": 6.133333333333334e-07,
      "loss": 0.0025,
      "step": 148160
    },
    {
      "epoch": 7.9024,
      "grad_norm": 0.11498259752988815,
      "learning_rate": 6.100000000000001e-07,
      "loss": 0.0024,
      "step": 148170
    },
    {
      "epoch": 7.902933333333333,
      "grad_norm": 0.29969295859336853,
      "learning_rate": 6.066666666666666e-07,
      "loss": 0.0028,
      "step": 148180
    },
    {
      "epoch": 7.903466666666667,
      "grad_norm": 0.05622649937868118,
      "learning_rate": 6.033333333333333e-07,
      "loss": 0.0021,
      "step": 148190
    },
    {
      "epoch": 7.904,
      "grad_norm": 0.2631978392601013,
      "learning_rate": 6.000000000000001e-07,
      "loss": 0.0016,
      "step": 148200
    },
    {
      "epoch": 7.904533333333333,
      "grad_norm": 0.11246756464242935,
      "learning_rate": 5.966666666666667e-07,
      "loss": 0.0026,
      "step": 148210
    },
    {
      "epoch": 7.9050666666666665,
      "grad_norm": 0.30848661065101624,
      "learning_rate": 5.933333333333333e-07,
      "loss": 0.0026,
      "step": 148220
    },
    {
      "epoch": 7.9056,
      "grad_norm": 0.8023546934127808,
      "learning_rate": 5.9e-07,
      "loss": 0.003,
      "step": 148230
    },
    {
      "epoch": 7.906133333333333,
      "grad_norm": 0.11906655877828598,
      "learning_rate": 5.866666666666667e-07,
      "loss": 0.003,
      "step": 148240
    },
    {
      "epoch": 7.906666666666666,
      "grad_norm": 0.19644887745380402,
      "learning_rate": 5.833333333333334e-07,
      "loss": 0.002,
      "step": 148250
    },
    {
      "epoch": 7.9072,
      "grad_norm": 0.3387890160083771,
      "learning_rate": 5.8e-07,
      "loss": 0.0024,
      "step": 148260
    },
    {
      "epoch": 7.907733333333333,
      "grad_norm": 0.3117000460624695,
      "learning_rate": 5.766666666666667e-07,
      "loss": 0.0023,
      "step": 148270
    },
    {
      "epoch": 7.908266666666667,
      "grad_norm": 0.08833633363246918,
      "learning_rate": 5.733333333333334e-07,
      "loss": 0.0036,
      "step": 148280
    },
    {
      "epoch": 7.9088,
      "grad_norm": 0.05610992759466171,
      "learning_rate": 5.7e-07,
      "loss": 0.0041,
      "step": 148290
    },
    {
      "epoch": 7.9093333333333335,
      "grad_norm": 0.16972601413726807,
      "learning_rate": 5.666666666666667e-07,
      "loss": 0.0019,
      "step": 148300
    },
    {
      "epoch": 7.909866666666667,
      "grad_norm": 0.22927825152873993,
      "learning_rate": 5.633333333333334e-07,
      "loss": 0.0026,
      "step": 148310
    },
    {
      "epoch": 7.9104,
      "grad_norm": 0.14007432758808136,
      "learning_rate": 5.6e-07,
      "loss": 0.0025,
      "step": 148320
    },
    {
      "epoch": 7.910933333333333,
      "grad_norm": 0.11206021904945374,
      "learning_rate": 5.566666666666667e-07,
      "loss": 0.0042,
      "step": 148330
    },
    {
      "epoch": 7.911466666666667,
      "grad_norm": 0.060776665806770325,
      "learning_rate": 5.533333333333334e-07,
      "loss": 0.0026,
      "step": 148340
    },
    {
      "epoch": 7.912,
      "grad_norm": 0.08404424041509628,
      "learning_rate": 5.5e-07,
      "loss": 0.0026,
      "step": 148350
    },
    {
      "epoch": 7.912533333333333,
      "grad_norm": 0.05622526630759239,
      "learning_rate": 5.466666666666667e-07,
      "loss": 0.0024,
      "step": 148360
    },
    {
      "epoch": 7.9130666666666665,
      "grad_norm": 0.05532924085855484,
      "learning_rate": 5.433333333333334e-07,
      "loss": 0.0025,
      "step": 148370
    },
    {
      "epoch": 7.9136,
      "grad_norm": 0.02997904270887375,
      "learning_rate": 5.4e-07,
      "loss": 0.0018,
      "step": 148380
    },
    {
      "epoch": 7.914133333333333,
      "grad_norm": 0.4664405882358551,
      "learning_rate": 5.366666666666667e-07,
      "loss": 0.0017,
      "step": 148390
    },
    {
      "epoch": 7.914666666666666,
      "grad_norm": 0.11576742678880692,
      "learning_rate": 5.333333333333333e-07,
      "loss": 0.0029,
      "step": 148400
    },
    {
      "epoch": 7.9152000000000005,
      "grad_norm": 0.043834760785102844,
      "learning_rate": 5.3e-07,
      "loss": 0.0016,
      "step": 148410
    },
    {
      "epoch": 7.915733333333334,
      "grad_norm": 0.084908127784729,
      "learning_rate": 5.266666666666667e-07,
      "loss": 0.0021,
      "step": 148420
    },
    {
      "epoch": 7.916266666666667,
      "grad_norm": 0.2801758348941803,
      "learning_rate": 5.233333333333334e-07,
      "loss": 0.003,
      "step": 148430
    },
    {
      "epoch": 7.9168,
      "grad_norm": 0.16809161007404327,
      "learning_rate": 5.2e-07,
      "loss": 0.0025,
      "step": 148440
    },
    {
      "epoch": 7.917333333333334,
      "grad_norm": 0.19792968034744263,
      "learning_rate": 5.166666666666667e-07,
      "loss": 0.0039,
      "step": 148450
    },
    {
      "epoch": 7.917866666666667,
      "grad_norm": 0.009485061280429363,
      "learning_rate": 5.133333333333333e-07,
      "loss": 0.002,
      "step": 148460
    },
    {
      "epoch": 7.9184,
      "grad_norm": 0.2521919310092926,
      "learning_rate": 5.100000000000001e-07,
      "loss": 0.0025,
      "step": 148470
    },
    {
      "epoch": 7.918933333333333,
      "grad_norm": 0.3083457350730896,
      "learning_rate": 5.066666666666667e-07,
      "loss": 0.0025,
      "step": 148480
    },
    {
      "epoch": 7.919466666666667,
      "grad_norm": 0.22411863505840302,
      "learning_rate": 5.033333333333333e-07,
      "loss": 0.0021,
      "step": 148490
    },
    {
      "epoch": 7.92,
      "grad_norm": 0.9936433434486389,
      "learning_rate": 5.000000000000001e-07,
      "loss": 0.0021,
      "step": 148500
    },
    {
      "epoch": 7.920533333333333,
      "grad_norm": 0.05603375285863876,
      "learning_rate": 4.966666666666667e-07,
      "loss": 0.0019,
      "step": 148510
    },
    {
      "epoch": 7.9210666666666665,
      "grad_norm": 0.07540909945964813,
      "learning_rate": 4.933333333333333e-07,
      "loss": 0.0011,
      "step": 148520
    },
    {
      "epoch": 7.9216,
      "grad_norm": 0.028411436825990677,
      "learning_rate": 4.9e-07,
      "loss": 0.0016,
      "step": 148530
    },
    {
      "epoch": 7.922133333333333,
      "grad_norm": 0.05688793584704399,
      "learning_rate": 4.866666666666667e-07,
      "loss": 0.0023,
      "step": 148540
    },
    {
      "epoch": 7.922666666666666,
      "grad_norm": 0.08404593914747238,
      "learning_rate": 4.833333333333334e-07,
      "loss": 0.0035,
      "step": 148550
    },
    {
      "epoch": 7.9232,
      "grad_norm": 0.11364035308361053,
      "learning_rate": 4.8e-07,
      "loss": 0.0026,
      "step": 148560
    },
    {
      "epoch": 7.923733333333333,
      "grad_norm": 0.420988529920578,
      "learning_rate": 4.7666666666666667e-07,
      "loss": 0.0022,
      "step": 148570
    },
    {
      "epoch": 7.924266666666667,
      "grad_norm": 0.1681143343448639,
      "learning_rate": 4.7333333333333334e-07,
      "loss": 0.0016,
      "step": 148580
    },
    {
      "epoch": 7.9248,
      "grad_norm": 0.1975598782300949,
      "learning_rate": 4.7000000000000005e-07,
      "loss": 0.0027,
      "step": 148590
    },
    {
      "epoch": 7.925333333333334,
      "grad_norm": 0.3082828223705292,
      "learning_rate": 4.666666666666667e-07,
      "loss": 0.003,
      "step": 148600
    },
    {
      "epoch": 7.925866666666667,
      "grad_norm": 0.11281926929950714,
      "learning_rate": 4.6333333333333333e-07,
      "loss": 0.0023,
      "step": 148610
    },
    {
      "epoch": 7.9264,
      "grad_norm": 0.19628548622131348,
      "learning_rate": 4.6e-07,
      "loss": 0.0023,
      "step": 148620
    },
    {
      "epoch": 7.926933333333333,
      "grad_norm": 0.4207161068916321,
      "learning_rate": 4.566666666666667e-07,
      "loss": 0.0016,
      "step": 148630
    },
    {
      "epoch": 7.927466666666667,
      "grad_norm": 0.08095763623714447,
      "learning_rate": 4.5333333333333337e-07,
      "loss": 0.0029,
      "step": 148640
    },
    {
      "epoch": 7.928,
      "grad_norm": 0.2541314959526062,
      "learning_rate": 4.5e-07,
      "loss": 0.0024,
      "step": 148650
    },
    {
      "epoch": 7.928533333333333,
      "grad_norm": 0.057710666209459305,
      "learning_rate": 4.466666666666667e-07,
      "loss": 0.0032,
      "step": 148660
    },
    {
      "epoch": 7.9290666666666665,
      "grad_norm": 0.05602092295885086,
      "learning_rate": 4.4333333333333336e-07,
      "loss": 0.0029,
      "step": 148670
    },
    {
      "epoch": 7.9296,
      "grad_norm": 0.08488539606332779,
      "learning_rate": 4.4e-07,
      "loss": 0.0022,
      "step": 148680
    },
    {
      "epoch": 7.930133333333333,
      "grad_norm": 0.30836057662963867,
      "learning_rate": 4.3666666666666663e-07,
      "loss": 0.0024,
      "step": 148690
    },
    {
      "epoch": 7.930666666666666,
      "grad_norm": 0.08418793231248856,
      "learning_rate": 4.3333333333333335e-07,
      "loss": 0.0032,
      "step": 148700
    },
    {
      "epoch": 7.9312000000000005,
      "grad_norm": 0.19662202894687653,
      "learning_rate": 4.3e-07,
      "loss": 0.0028,
      "step": 148710
    },
    {
      "epoch": 7.931733333333334,
      "grad_norm": 0.0565187968313694,
      "learning_rate": 4.2666666666666673e-07,
      "loss": 0.0018,
      "step": 148720
    },
    {
      "epoch": 7.932266666666667,
      "grad_norm": 0.11217183619737625,
      "learning_rate": 4.233333333333334e-07,
      "loss": 0.0022,
      "step": 148730
    },
    {
      "epoch": 7.9328,
      "grad_norm": 0.029537411406636238,
      "learning_rate": 4.2e-07,
      "loss": 0.0023,
      "step": 148740
    },
    {
      "epoch": 7.933333333333334,
      "grad_norm": 0.006482989061623812,
      "learning_rate": 4.1666666666666667e-07,
      "loss": 0.0029,
      "step": 148750
    },
    {
      "epoch": 7.933866666666667,
      "grad_norm": 0.05674353986978531,
      "learning_rate": 4.133333333333334e-07,
      "loss": 0.0019,
      "step": 148760
    },
    {
      "epoch": 7.9344,
      "grad_norm": 0.05789833515882492,
      "learning_rate": 4.1000000000000004e-07,
      "loss": 0.0027,
      "step": 148770
    },
    {
      "epoch": 7.934933333333333,
      "grad_norm": 0.42030826210975647,
      "learning_rate": 4.0666666666666666e-07,
      "loss": 0.0018,
      "step": 148780
    },
    {
      "epoch": 7.935466666666667,
      "grad_norm": 0.08447816967964172,
      "learning_rate": 4.033333333333333e-07,
      "loss": 0.0026,
      "step": 148790
    },
    {
      "epoch": 7.936,
      "grad_norm": 0.1496533751487732,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 0.0029,
      "step": 148800
    },
    {
      "epoch": 7.936533333333333,
      "grad_norm": 0.05692632123827934,
      "learning_rate": 3.966666666666667e-07,
      "loss": 0.0028,
      "step": 148810
    },
    {
      "epoch": 7.9370666666666665,
      "grad_norm": 0.1587565392255783,
      "learning_rate": 3.933333333333333e-07,
      "loss": 0.0033,
      "step": 148820
    },
    {
      "epoch": 7.9376,
      "grad_norm": 0.16947375237941742,
      "learning_rate": 3.9e-07,
      "loss": 0.0025,
      "step": 148830
    },
    {
      "epoch": 7.938133333333333,
      "grad_norm": 0.03164089098572731,
      "learning_rate": 3.866666666666667e-07,
      "loss": 0.0026,
      "step": 148840
    },
    {
      "epoch": 7.938666666666666,
      "grad_norm": 0.05605892464518547,
      "learning_rate": 3.8333333333333335e-07,
      "loss": 0.0032,
      "step": 148850
    },
    {
      "epoch": 7.9392,
      "grad_norm": 0.08699624240398407,
      "learning_rate": 3.8e-07,
      "loss": 0.0023,
      "step": 148860
    },
    {
      "epoch": 7.939733333333333,
      "grad_norm": 0.08442622423171997,
      "learning_rate": 3.766666666666667e-07,
      "loss": 0.0019,
      "step": 148870
    },
    {
      "epoch": 7.940266666666667,
      "grad_norm": 0.19615916907787323,
      "learning_rate": 3.7333333333333334e-07,
      "loss": 0.0036,
      "step": 148880
    },
    {
      "epoch": 7.9408,
      "grad_norm": 0.3931705355644226,
      "learning_rate": 3.7000000000000006e-07,
      "loss": 0.002,
      "step": 148890
    },
    {
      "epoch": 7.941333333333334,
      "grad_norm": 0.08445323258638382,
      "learning_rate": 3.6666666666666667e-07,
      "loss": 0.0031,
      "step": 148900
    },
    {
      "epoch": 7.941866666666667,
      "grad_norm": 0.05782603099942207,
      "learning_rate": 3.633333333333334e-07,
      "loss": 0.0016,
      "step": 148910
    },
    {
      "epoch": 7.9424,
      "grad_norm": 0.25758272409439087,
      "learning_rate": 3.6e-07,
      "loss": 0.0033,
      "step": 148920
    },
    {
      "epoch": 7.942933333333333,
      "grad_norm": 0.08436798304319382,
      "learning_rate": 3.566666666666667e-07,
      "loss": 0.0024,
      "step": 148930
    },
    {
      "epoch": 7.943466666666667,
      "grad_norm": 0.168648824095726,
      "learning_rate": 3.533333333333333e-07,
      "loss": 0.0027,
      "step": 148940
    },
    {
      "epoch": 7.944,
      "grad_norm": 0.08516062051057816,
      "learning_rate": 3.5000000000000004e-07,
      "loss": 0.0022,
      "step": 148950
    },
    {
      "epoch": 7.944533333333333,
      "grad_norm": 0.05602959915995598,
      "learning_rate": 3.4666666666666665e-07,
      "loss": 0.0029,
      "step": 148960
    },
    {
      "epoch": 7.9450666666666665,
      "grad_norm": 0.16818732023239136,
      "learning_rate": 3.4333333333333336e-07,
      "loss": 0.0026,
      "step": 148970
    },
    {
      "epoch": 7.9456,
      "grad_norm": 0.059490371495485306,
      "learning_rate": 3.4e-07,
      "loss": 0.0025,
      "step": 148980
    },
    {
      "epoch": 7.946133333333333,
      "grad_norm": 0.03984453156590462,
      "learning_rate": 3.366666666666667e-07,
      "loss": 0.0031,
      "step": 148990
    },
    {
      "epoch": 7.946666666666666,
      "grad_norm": 0.07013888657093048,
      "learning_rate": 3.3333333333333335e-07,
      "loss": 0.0026,
      "step": 149000
    },
    {
      "epoch": 7.9472000000000005,
      "grad_norm": 0.1689557135105133,
      "learning_rate": 3.3e-07,
      "loss": 0.0026,
      "step": 149010
    },
    {
      "epoch": 7.947733333333334,
      "grad_norm": 0.046894095838069916,
      "learning_rate": 3.266666666666667e-07,
      "loss": 0.0019,
      "step": 149020
    },
    {
      "epoch": 7.948266666666667,
      "grad_norm": 0.056489862501621246,
      "learning_rate": 3.2333333333333334e-07,
      "loss": 0.0022,
      "step": 149030
    },
    {
      "epoch": 7.9488,
      "grad_norm": 0.08413559198379517,
      "learning_rate": 3.2e-07,
      "loss": 0.0032,
      "step": 149040
    },
    {
      "epoch": 7.949333333333334,
      "grad_norm": 0.02851051092147827,
      "learning_rate": 3.1666666666666667e-07,
      "loss": 0.0022,
      "step": 149050
    },
    {
      "epoch": 7.949866666666667,
      "grad_norm": 0.03236966207623482,
      "learning_rate": 3.133333333333334e-07,
      "loss": 0.0024,
      "step": 149060
    },
    {
      "epoch": 7.9504,
      "grad_norm": 0.08468060940504074,
      "learning_rate": 3.1e-07,
      "loss": 0.0022,
      "step": 149070
    },
    {
      "epoch": 7.950933333333333,
      "grad_norm": 0.0320306234061718,
      "learning_rate": 3.066666666666667e-07,
      "loss": 0.0029,
      "step": 149080
    },
    {
      "epoch": 7.951466666666667,
      "grad_norm": 0.22533835470676422,
      "learning_rate": 3.033333333333333e-07,
      "loss": 0.0023,
      "step": 149090
    },
    {
      "epoch": 7.952,
      "grad_norm": 0.20823726058006287,
      "learning_rate": 3.0000000000000004e-07,
      "loss": 0.0025,
      "step": 149100
    },
    {
      "epoch": 7.952533333333333,
      "grad_norm": 0.28015047311782837,
      "learning_rate": 2.9666666666666665e-07,
      "loss": 0.0034,
      "step": 149110
    },
    {
      "epoch": 7.9530666666666665,
      "grad_norm": 0.04806925356388092,
      "learning_rate": 2.9333333333333337e-07,
      "loss": 0.0017,
      "step": 149120
    },
    {
      "epoch": 7.9536,
      "grad_norm": 0.16959066689014435,
      "learning_rate": 2.9e-07,
      "loss": 0.0025,
      "step": 149130
    },
    {
      "epoch": 7.954133333333333,
      "grad_norm": 0.0308524239808321,
      "learning_rate": 2.866666666666667e-07,
      "loss": 0.0031,
      "step": 149140
    },
    {
      "epoch": 7.954666666666666,
      "grad_norm": 0.022419700399041176,
      "learning_rate": 2.8333333333333336e-07,
      "loss": 0.0027,
      "step": 149150
    },
    {
      "epoch": 7.9552,
      "grad_norm": 0.11218636482954025,
      "learning_rate": 2.8e-07,
      "loss": 0.0021,
      "step": 149160
    },
    {
      "epoch": 7.955733333333333,
      "grad_norm": 0.08479074388742447,
      "learning_rate": 2.766666666666667e-07,
      "loss": 0.0036,
      "step": 149170
    },
    {
      "epoch": 7.956266666666667,
      "grad_norm": 0.2524001896381378,
      "learning_rate": 2.7333333333333335e-07,
      "loss": 0.0022,
      "step": 149180
    },
    {
      "epoch": 7.9568,
      "grad_norm": 0.16870595514774323,
      "learning_rate": 2.7e-07,
      "loss": 0.0044,
      "step": 149190
    },
    {
      "epoch": 7.957333333333334,
      "grad_norm": 0.25230923295021057,
      "learning_rate": 2.6666666666666667e-07,
      "loss": 0.0017,
      "step": 149200
    },
    {
      "epoch": 7.957866666666667,
      "grad_norm": 0.08529229462146759,
      "learning_rate": 2.6333333333333334e-07,
      "loss": 0.0029,
      "step": 149210
    },
    {
      "epoch": 7.9584,
      "grad_norm": 0.4487696588039398,
      "learning_rate": 2.6e-07,
      "loss": 0.0034,
      "step": 149220
    },
    {
      "epoch": 7.958933333333333,
      "grad_norm": 0.1131761446595192,
      "learning_rate": 2.5666666666666666e-07,
      "loss": 0.0044,
      "step": 149230
    },
    {
      "epoch": 7.959466666666667,
      "grad_norm": 0.39241281151771545,
      "learning_rate": 2.533333333333333e-07,
      "loss": 0.0024,
      "step": 149240
    },
    {
      "epoch": 7.96,
      "grad_norm": 0.3923865854740143,
      "learning_rate": 2.5000000000000004e-07,
      "loss": 0.0031,
      "step": 149250
    },
    {
      "epoch": 7.960533333333333,
      "grad_norm": 0.22466400265693665,
      "learning_rate": 2.4666666666666665e-07,
      "loss": 0.0025,
      "step": 149260
    },
    {
      "epoch": 7.9610666666666665,
      "grad_norm": 0.057330384850502014,
      "learning_rate": 2.4333333333333337e-07,
      "loss": 0.003,
      "step": 149270
    },
    {
      "epoch": 7.9616,
      "grad_norm": 0.0907978042960167,
      "learning_rate": 2.4e-07,
      "loss": 0.0022,
      "step": 149280
    },
    {
      "epoch": 7.962133333333333,
      "grad_norm": 0.00013890395348425955,
      "learning_rate": 2.3666666666666667e-07,
      "loss": 0.0031,
      "step": 149290
    },
    {
      "epoch": 7.962666666666666,
      "grad_norm": 0.04656165838241577,
      "learning_rate": 2.3333333333333336e-07,
      "loss": 0.0021,
      "step": 149300
    },
    {
      "epoch": 7.9632,
      "grad_norm": 0.25441089272499084,
      "learning_rate": 2.3e-07,
      "loss": 0.0022,
      "step": 149310
    },
    {
      "epoch": 7.963733333333334,
      "grad_norm": 0.12517930567264557,
      "learning_rate": 2.2666666666666668e-07,
      "loss": 0.0034,
      "step": 149320
    },
    {
      "epoch": 7.964266666666667,
      "grad_norm": 0.1681491732597351,
      "learning_rate": 2.2333333333333335e-07,
      "loss": 0.0042,
      "step": 149330
    },
    {
      "epoch": 7.9648,
      "grad_norm": 0.18040280044078827,
      "learning_rate": 2.2e-07,
      "loss": 0.0018,
      "step": 149340
    },
    {
      "epoch": 7.965333333333334,
      "grad_norm": 5.518178802788043e-09,
      "learning_rate": 2.1666666666666667e-07,
      "loss": 0.0032,
      "step": 149350
    },
    {
      "epoch": 7.965866666666667,
      "grad_norm": 0.11259538680315018,
      "learning_rate": 2.1333333333333336e-07,
      "loss": 0.0017,
      "step": 149360
    },
    {
      "epoch": 7.9664,
      "grad_norm": 0.030851293355226517,
      "learning_rate": 2.1e-07,
      "loss": 0.0021,
      "step": 149370
    },
    {
      "epoch": 7.966933333333333,
      "grad_norm": 0.08500716090202332,
      "learning_rate": 2.066666666666667e-07,
      "loss": 0.0031,
      "step": 149380
    },
    {
      "epoch": 7.967466666666667,
      "grad_norm": 0.22414706647396088,
      "learning_rate": 2.0333333333333333e-07,
      "loss": 0.0032,
      "step": 149390
    },
    {
      "epoch": 7.968,
      "grad_norm": 0.28017809987068176,
      "learning_rate": 2.0000000000000002e-07,
      "loss": 0.0019,
      "step": 149400
    },
    {
      "epoch": 7.968533333333333,
      "grad_norm": 0.1684597283601761,
      "learning_rate": 1.9666666666666665e-07,
      "loss": 0.0022,
      "step": 149410
    },
    {
      "epoch": 7.9690666666666665,
      "grad_norm": 0.0049188993871212006,
      "learning_rate": 1.9333333333333334e-07,
      "loss": 0.0022,
      "step": 149420
    },
    {
      "epoch": 7.9696,
      "grad_norm": 0.056148480623960495,
      "learning_rate": 1.9e-07,
      "loss": 0.0017,
      "step": 149430
    },
    {
      "epoch": 7.970133333333333,
      "grad_norm": 0.05603349953889847,
      "learning_rate": 1.8666666666666667e-07,
      "loss": 0.0021,
      "step": 149440
    },
    {
      "epoch": 7.970666666666666,
      "grad_norm": 0.0519179068505764,
      "learning_rate": 1.8333333333333333e-07,
      "loss": 0.0019,
      "step": 149450
    },
    {
      "epoch": 7.9712,
      "grad_norm": 0.3364788889884949,
      "learning_rate": 1.8e-07,
      "loss": 0.0032,
      "step": 149460
    },
    {
      "epoch": 7.971733333333333,
      "grad_norm": 0.08404593914747238,
      "learning_rate": 1.7666666666666666e-07,
      "loss": 0.0027,
      "step": 149470
    },
    {
      "epoch": 7.972266666666666,
      "grad_norm": 0.2241698056459427,
      "learning_rate": 1.7333333333333332e-07,
      "loss": 0.0034,
      "step": 149480
    },
    {
      "epoch": 7.9728,
      "grad_norm": 0.1409022957086563,
      "learning_rate": 1.7e-07,
      "loss": 0.0026,
      "step": 149490
    },
    {
      "epoch": 7.973333333333334,
      "grad_norm": 0.16817042231559753,
      "learning_rate": 1.6666666666666668e-07,
      "loss": 0.0017,
      "step": 149500
    }
  ],
  "logging_steps": 10,
  "max_steps": 150000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 8,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 40,
  "trial_name": null,
  "trial_params": null
}
